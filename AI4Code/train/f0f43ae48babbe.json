{"cell_type":{"318b7813":"code","6bdabaf1":"code","287a86df":"code","291e64e1":"code","38351859":"code","dd79a196":"code","c573df91":"code","b5452600":"code","bb411980":"code","9985a01c":"code","e9a336e3":"code","747e2160":"code","6c396f8f":"code","8b19fb4c":"code","36b1e43a":"code","d3bddf91":"code","6705e7f1":"code","88e6cf92":"code","29583780":"code","f3ccbeff":"code","c00f50be":"code","d5317f19":"code","a7e5fe82":"code","ba1b22ea":"code","674ee440":"code","3e643fcd":"code","c4d3a466":"code","cb7b116a":"code","34d3086c":"code","80f94bcc":"code","5bd6a6bf":"code","e2d8cef0":"code","e5adc186":"code","f72e509b":"code","edb010c3":"code","ac3b0e2a":"code","6d54f300":"code","7f46c2e4":"code","b21faaf7":"code","52b0a668":"code","11d3b63b":"code","f424fec2":"code","b4412bcb":"code","5870f974":"code","909be9a5":"code","308233c1":"code","c9b5af9a":"code","01ca07db":"code","f1faa246":"code","321d8a36":"code","b747e1a4":"code","0d5fda8e":"code","e077957f":"code","4885c499":"code","2d57185a":"code","472b5784":"code","fbfc3d5c":"code","cb0dec6a":"code","97f6aeaf":"code","cf615b18":"code","d3f7a526":"code","27293d91":"code","57492e58":"code","be4cad06":"code","60e31df7":"code","3635f203":"code","e6981430":"code","2b06f090":"code","ed2038dc":"code","4dcf0316":"code","9f8c3917":"code","13920225":"code","251e6815":"code","dd9d1fe5":"code","6bdbb085":"code","2a9dee02":"code","b7503d4a":"code","6041bed7":"code","72e04792":"code","797527d7":"code","29cd3663":"code","9e7ad7fe":"code","4a6aff4c":"code","bd2e3fe3":"code","e615f81f":"code","ebe9b541":"code","ce27c73e":"code","a48bd887":"code","cb62a6d5":"code","915a22fe":"code","0ca2db3b":"code","fafb07ae":"code","453fc635":"code","d89fcd6a":"code","9b691a14":"code","6c680c8f":"code","49a282c9":"code","39e7c8c9":"code","03c948fc":"code","e58cd7ea":"code","08f1dfab":"code","10a5f17d":"code","005bff49":"code","f2690383":"code","029ff6bc":"code","4ea31f36":"code","e6af1be2":"code","c93a0220":"code","992590e5":"code","7298ac4b":"code","6a95777e":"markdown","5b64ec93":"markdown","6ea688d2":"markdown","aca8dcdc":"markdown","f6b340e8":"markdown","7542ce19":"markdown","a00905d2":"markdown","6e0a30d6":"markdown","fd00ebd9":"markdown","f21c87e7":"markdown","63b6fbd8":"markdown","92f6b006":"markdown","79674b81":"markdown","32e35576":"markdown","2efc15c6":"markdown","817e383d":"markdown","89f18063":"markdown","d971571a":"markdown","66283773":"markdown","4571fdf1":"markdown","8a10825a":"markdown","f7bb3339":"markdown","e342eb93":"markdown","c6e2f0ce":"markdown","1e195569":"markdown","0862c28a":"markdown","50607b22":"markdown","ff52b236":"markdown","9f729b1d":"markdown","e23ea031":"markdown","c6ba7a5a":"markdown","3f5674f4":"markdown","262ae1c7":"markdown","13c9847b":"markdown","a3276555":"markdown"},"source":{"318b7813":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom IPython.display import display_html \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# config pandas\npd.options.display.max_columns = 300\npd.options.display.max_rows = 300\n\n# config matplotlib\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n","6bdabaf1":"# set random_state\nRS=17\n\ndef df2_styler(df1, cap1, df2, cap2):\n    df1_styler = df1.style.set_table_attributes(\"style='display:inline'\").\\\n                 set_caption(cap1)\n    df2_styler = df2.style.set_table_attributes(\"style='display:inline'\").\\\n                 set_caption(cap2)\n    display_html(df1_styler._repr_html_()+df2_styler._repr_html_(), raw=True)\n\ndef df3_styler(df1, cap1, df2, cap2, df3, cap3):\n    df1_styler = df1.style.set_table_attributes(\"style='display:inline'\").\\\n                 set_caption(cap1)\n    df2_styler = df2.style.set_table_attributes(\"style='display:inline'\").\\\n                 set_caption(cap2)\n    df3_styler = df3.style.set_table_attributes(\"style='display:inline'\").\\\n                 set_caption(cap3)\n    display_html(df1_styler._repr_html_()+df2_styler._repr_html_()+df3_styler._repr_html_(), raw=True)\n    ","287a86df":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","291e64e1":"%%capture\n!pip install impyute","38351859":"# Loading datasets\ntrain = pd.read_csv('\/kaggle\/input\/task-04\/training_data.csv')\ntrain.info()","dd79a196":"train.shape","c573df91":"test = pd.read_csv('\/kaggle\/input\/task-04\/test_data_without_label.csv')\ntest.info()","b5452600":"test.shape","bb411980":"df2_styler(train.head(20), 'train_head', test.head(20), 'test_head')","9985a01c":"train = train.drop(['id'], axis=1)\ntest_id = test['id']\ntest = test.drop(['id'], axis=1)\n","e9a336e3":"train.isin([0]).sum()\n","747e2160":"train_zeros = train.isin([0])\ntrain_zeros.value_counts()","6c396f8f":"test.isin([0]).sum()\n","8b19fb4c":"test_zeros = test.isin([0])\ntest_zeros.value_counts()","36b1e43a":"target = train['sepse']\ntrain_notarget = train.drop(['sepse'], axis=1).replace({0:np.NaN})\ntrain = pd.concat([train_notarget, target], axis=1)\ntrain.isin([0]).sum()","d3bddf91":"test = test.replace({0:np.NaN})\ntest.isin([0]).sum()","6705e7f1":"train.iloc[:,1:-1].describe()","88e6cf92":"train[train < 0].dropna(axis=0, how='all')","29583780":"train.at[3837, 'temperatura'] = 35.0","f3ccbeff":"train_z = pd.DataFrame(train.drop(['num_atend', 'sepse'], axis=1).\\\n          apply(lambda x: stats.zscore(x, nan_policy='omit')))\n\ntr_z = train_z[ (train_z < -5) |  (train_z > 5) ].dropna(axis=0, how='all')\ntr_z_index = tr_z.index\ntr_z_melt = pd.melt(tr_z, ignore_index=False).dropna()\ntr_out_melt = pd.melt(train.iloc[tr_z_index,1:-1], ignore_index=False).dropna()\n\ndf_out = pd.merge(tr_z_melt, tr_out_melt, how='inner', left_index=True, right_index=True)\ndff_out = df_out[df_out['variable_x'] == df_out['variable_y']].drop(['variable_y'], axis=1)\ndff_out","c00f50be":"# A frequ\u00eancia card\u00edaca m\u00e1xima \u00e9 de 220 batimentos por minuto (bpm), ent\u00e3o\n# mantive o valor de pulso 200\n\ntr_rep_index = dff_out.index.tolist()\ntr_rep_var = dff_out['variable_x'].tolist()\ntr_rep_values = [35.6, 36.02, 100.0, 37.8, 36.02, 35.6, 84, 33.6, \n                 36.8, 200.0, 96, 36.8, 36.2, 84, 146, 88, 70]\n\nfor i in range(0, len(tr_rep_index)):\n    train.at[tr_rep_index[i], tr_rep_var[i]] = tr_rep_values[i]","d5317f19":"train.describe()","a7e5fe82":"train_z = pd.DataFrame(train.drop(['num_atend', 'sepse'], axis=1).\\\n          apply(lambda x: stats.zscore(x, nan_policy='omit')))\n\ntr_z = train_z[ (train_z < -3) |  (train_z > 3) ].dropna(axis=0, how='all')\ntr_z_index = tr_z.index\ntr_z_melt = pd.melt(tr_z, ignore_index=False).dropna()\ntr_out_melt = pd.melt(train.iloc[tr_z_index,1:-1], ignore_index=False).dropna()\n\ndf_out = pd.merge(tr_z_melt, tr_out_melt, how='inner', left_index=True, right_index=True)\ndff_out = df_out[df_out['variable_x'] == df_out['variable_y']].drop(['variable_y'], axis=1)\ndff_out.head(15)\n","ba1b22ea":"dff_out1 = dff_out[dff_out['variable_x'] == 'pa_min']\ndff_out1 = dff_out1.append(dff_out[dff_out['variable_x'] == 'pa_max'])\ndff_out1 = dff_out1.append(dff_out[dff_out['variable_x'] == 'temperatura'].query('value_y > 40'))\ndff_out1 = dff_out1.append(dff_out[dff_out['variable_x'] == 'temperatura'].query('value_y < 32'))\ndff_out1 = dff_out1.append(dff_out[dff_out['variable_x'] == 'pulso'].query('value_y < 60'))\n","674ee440":"tr_rep_index = dff_out1.index.tolist()\ntr_rep_var = dff_out1['variable_x'].tolist()\ntr_rep_values = [np.NaN for i in range(0, len(tr_rep_var))]\n\nfor i in range(0, len(tr_rep_index)):\n    train.at[tr_rep_index[i], tr_rep_var[i]] = tr_rep_values[i]\n","3e643fcd":"train.describe()","c4d3a466":"test.describe()","cb7b116a":"test[test < 0].dropna(axis=0, how='all')","34d3086c":"test.at[1588, 'temperatura'] = 35.0","80f94bcc":"\ntest_z = pd.DataFrame(test.drop(['num_atend'], axis=1).\\\n          apply(lambda x: stats.zscore(x, nan_policy='omit')))\n\nte_z = test_z[ (test_z < -5) |  (test_z > 5) ].dropna(axis=0, how='all')\nte_z_index = te_z.index\nte_z_melt = pd.melt(te_z, ignore_index=False).dropna()\nte_out_melt = pd.melt(test.iloc[te_z_index,1:], ignore_index=False).dropna()\n\ndf_out2 = pd.merge(te_z_melt, te_out_melt, how='inner', left_index=True, right_index=True)\ndff_out2 = df_out2[df_out2['variable_x'] == df_out2['variable_y']].drop(['variable_y'], axis=1)\ndff_out2\n","5bd6a6bf":"te_rep_index = dff_out2.index.tolist()\nte_rep_var = dff_out2['variable_x'].tolist()\nte_rep_values = [146.0, 37.8, 102.0, 102.0, 111.0, \n                 111.0, 74.0, 74.0, 88.0, 133.0]\n\nfor i in range(0, len(te_rep_index)):\n    test.at[te_rep_index[i], te_rep_var[i]] = te_rep_values[i]\n","e2d8cef0":"test_z = pd.DataFrame(test.drop(['num_atend'], axis=1).\\\n          apply(lambda x: stats.zscore(x, nan_policy='omit')))\n\nte_z = test_z[ (test_z < -3.5) |  (test_z > 3.5) ].dropna(axis=0, how='all')\nte_z_index = te_z.index\nte_z_melt = pd.melt(te_z, ignore_index=False).dropna()\nte_out_melt = pd.melt(test.iloc[te_z_index,1:], ignore_index=False).dropna()\n\ndf_out2 = pd.merge(te_z_melt, te_out_melt, how='inner', left_index=True, right_index=True)\ndff_out2 = df_out2[df_out2['variable_x'] == df_out2['variable_y']].drop(['variable_y'], axis=1)\ndff_out2.head(15)","e5adc186":"dff_out4 = dff_out2[dff_out2['variable_x'] == 'pulso'].query('value_y < 40')\ndff_out4 = dff_out4.append(dff_out2[dff_out2['variable_x'] == 'pa_min'].query('value_y < 21'))\ndff_out4 = dff_out4.append(dff_out2[dff_out2['variable_x'] == 'pa_max'].query('value_y < 21'))\n","f72e509b":"te_rep_index = dff_out4.index.tolist()\nte_rep_var = dff_out4['variable_x'].tolist()\nte_rep_values = [np.NaN for i in range(0, len(te_rep_var))]\n\nfor i in range(0, len(te_rep_index)):\n    test.at[te_rep_index[i], te_rep_var[i]] = te_rep_values[i]\n","edb010c3":"comp_idx = train[train['pa_min'] < train['pa_max']].index.tolist()\ntrain.iloc[comp_idx, 4] = np.NaN\ntrain.iloc[comp_idx, 5] = np.NaN\n\ntrain[train['pa_min'] < train['pa_max']]","ac3b0e2a":"comp_idx = test[test['pa_min'] < test['pa_max']].index.tolist()\ntest.iloc[comp_idx, 4] = np.NaN\ntest.iloc[comp_idx, 5] = np.NaN\n\ntest[test['pa_min'] < test['pa_max']]","6d54f300":"train_dup = train.duplicated(keep=False)\ntest_dup = test.duplicated(keep=False)\ndf2_styler(pd.DataFrame(train_dup.value_counts(), columns=['train']), '', \n           pd.DataFrame(test_dup.value_counts(), columns=['test']), '')","7f46c2e4":"train_dup2 = train.duplicated(keep=False, subset=['num_atend'])\ntest_dup2 = test.duplicated(keep=False, subset=['num_atend'])\ndf2_styler(pd.DataFrame(train_dup2.value_counts(), columns=['train']), '', \n           pd.DataFrame(test_dup2.value_counts(), columns=['test']), '')","b21faaf7":"train_nodup = train.drop_duplicates(keep='first', ignore_index=True)\ntest_nodup = test.drop_duplicates(keep='first', ignore_index=True)\n\ndf2_styler(pd.DataFrame(train_nodup.duplicated(keep=False).value_counts(), columns=['train']), '', \n           pd.DataFrame(test_nodup.duplicated(keep=False).value_counts(), columns=['test']), '')","52b0a668":"train_dup3 = train_nodup.drop(['sepse'], axis=1).duplicated(keep=False, subset=['num_atend'])\ntrain_dup3.value_counts()","11d3b63b":"train_nodup[train_dup3].sort_values(['num_atend']).head(10)","f424fec2":"train_dup3","b4412bcb":"def missing_percentage(df):\n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n               100)[(df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n                     100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n\ntrain_missing = missing_percentage(train)\ndisplay(train_missing.T.style.background_gradient(cmap='Reds', axis=1))","5870f974":"train_nodup_missing = missing_percentage(train_nodup)\ndisplay(train_nodup_missing.T.style.background_gradient(cmap='Reds', axis=1))","909be9a5":"test_missing = missing_percentage(test)\ndisplay(test_missing.T.style.background_gradient(cmap='Reds', axis=1))","308233c1":"test_nodup_missing = missing_percentage(test_nodup)\ndisplay(test_nodup_missing.T.style.background_gradient(cmap='Reds', axis=1))","c9b5af9a":"train_cut = train.shape[0]\ntrain_nodup_cut = train_nodup.shape[0]\n\ntarget = train['sepse'].astype('category')\ntarget_nodup = train_nodup['sepse'].astype('category')\n\ntr_num_atend = train['num_atend']\ntr_num_atend_nodup = train_nodup['num_atend']\nte_num_atend = test['num_atend']\nte_num_atend_nodup = test_nodup['num_atend']\n\nfeatures = pd.concat([train.drop(['sepse','num_atend'], axis=1), test.drop(['num_atend'], axis=1)])\nfeatures_nodup = pd.concat([train_nodup.drop(['sepse','num_atend'], axis=1), test_nodup.drop(['num_atend'], axis=1)])","01ca07db":"df3_styler(features.describe(), 'features', features_nodup.describe(), 'features_nodup', \n           features.describe() - features_nodup.describe(), 'dif_features_features_nodup')","f1faa246":"features['melt'] = 'feat'\nfeatures_nodup['melt'] = 'feat_nodup'\ndf_melt = pd.melt(features.append(features_nodup), id_vars='melt')\nfeatures = features.drop(['melt'], axis=1)\nfeatures_nodup = features_nodup.drop(['melt'], axis=1)\nfig, ax = plt.subplots(figsize=(9, 9))\nsns.boxplot(x='variable', y='value', hue='melt', data=df_melt, ax=ax)\n","321d8a36":"%%time\nfrom impyute.imputation.cs import mice\nfeatures_nodup = features_nodup.astype('float')\nfeat_list = features_nodup.columns.tolist()\nmice_data = mice(features_nodup.to_numpy())\nround_dict = {'temperatura':1, 'pulso':0, 'respiracao':0,\n              'pa_min':0, 'pa_max':0}\nmice_df = pd.DataFrame(mice_data, columns=feat_list).round(round_dict)\n","b747e1a4":"mice_df[mice_df <= 0.0].dropna()","0d5fda8e":"mice_df.isnull().sum().sum()","e077957f":"%%time\nfrom impyute.imputation.cs import em\nem_data = em(features_nodup.to_numpy()) \nem_df = pd.DataFrame(em_data, columns=feat_list).round(round_dict)","4885c499":"em_df[em_df <= 0.0].dropna()","2d57185a":"em_df.isnull().sum().sum()","472b5784":"%%time\nfrom impyute.imputation.cs import random\nrandom_data = random(features_nodup.to_numpy())\nrandom_df = pd.DataFrame(random_data, columns=feat_list).round(round_dict)","fbfc3d5c":"random_df[random_df <= 0.0].dropna()","cb0dec6a":"random_df.isnull().sum().sum()","97f6aeaf":"pd.DataFrame({'feat_vs_mice':(features.describe() - mice_df.describe()).abs().sum(axis=1),\n              'feat_vs_em':(features.describe() - em_df.describe()).abs().sum(axis=1),\n              'feat_vs_random':(features.describe() - random_df.describe()).abs().sum(axis=1),\n              'feat_nodup_vs_mice':(features_nodup.describe() - mice_df.describe()).abs().sum(axis=1),\n              'feat_nodup_vs_em':(features_nodup.describe() - em_df.describe()).abs().sum(axis=1),\n              'feat_nodup_vs_random':(features_nodup.describe() - random_df.describe()).abs().sum(axis=1)})","cf615b18":"df3_styler(mice_df.describe(), 'mice', \n           em_df.describe(), 'em', \n           random_df.describe(), 'random')","d3f7a526":"features['melt'] = 'feat'\nfeatures_nodup['melt'] = 'feat_nodup'\nmice_df['melt'] = 'mice'\nem_df['melt'] = 'em'\nrandom_df['melt'] = 'random'\ndfs = [features_nodup, mice_df, em_df, random_df]\ndf_melt = pd.melt(features.append(dfs), id_vars='melt')\nfeatures = features.drop(['melt'], axis=1)\nfeatures_nodup = features_nodup.drop(['melt'], axis=1)\nmice_df = mice_df.drop(['melt'], axis=1)\nem_df = em_df.drop(['melt'], axis=1)\nrandom_df = random_df.drop(['melt'], axis=1)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\nax[0].set_ylim([40, 250]) \nsns.boxplot(x='variable', y='value', hue='melt', data=df_melt, ax=ax[0])\nax[1].set_ylim([10, 40]) \nsns.boxplot(x='variable', y='value', hue='melt', data=df_melt, ax=ax[1])","27293d91":"num_features = mice_df","57492e58":"# https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK482408\/\n# Pulse Pressure = Systolic Blood Pressure (SBP) \u2013 Diastolic Blood Pressure(DBP)\nnum_features['pp'] = num_features['pa_min'] - num_features['pa_max']","be4cad06":"# https:\/\/www.nursingcenter.com\/ncblog\/december-2011\/calculating-the-map\n# Mean Arterial Pressure = (SBP + 2 * DBP) \/ 3\nnum_features['map'] = (num_features['pa_min'] + 2 * num_features['pa_max']) \/ 3","60e31df7":"# https:\/\/ccforum.biomedcentral.com\/articles\/10.1186\/cc9060\n# (a) hyperthermia (\u226538\u00b0C) or hypothermia (\u22645.6\u00b0C)\n# https:\/\/www.mdsaude.com\/doencas-infecciosas\/sepse\/\n# Temperatura corporal maior que 38\u00baC ou menor que 35\u00baC.\ndef cat_temp(x):\n    if (x > 38):\n        return 1\n    elif (x < 35):\n        return 2\n    else:\n        return 0\n    \nnum_features['cat_temp'] = np.NaN\nnum_features['cat_temp'] = num_features['temperatura'].apply(lambda x: cat_temp(x)).astype('int')\nnum_features['cat_temp'].isnull().sum()","3635f203":"num_features['cat_temp'].value_counts()","e6981430":"# (b) tachycardia (heart rate, \u226590 beats\/min)\ndef cat_pulso(x):\n    if x > 90:\n        return 1\n    else:\n        return 0\n\nnum_features['cat_pulso'] = np.NaN\nnum_features['cat_pulso'] = num_features['pulso'].apply(lambda x: cat_pulso(x)).astype('int')\nnum_features['cat_pulso'].isnull().sum()","2b06f090":"num_features['cat_pulso'].value_counts()","ed2038dc":"# (c) tachypnea (respiratory rate, \u226520 breaths\/min)\ndef cat_resp(x):\n    if x > 20:\n        return 1\n    else:\n        return 0\n\nnum_features['cat_resp'] = np.NaN\nnum_features['cat_resp'] = num_features['respiracao'].apply(lambda x: cat_resp(x)).astype('int')\nnum_features['cat_resp'].isnull().sum()","4dcf0316":"num_features['cat_resp'].value_counts()","9f8c3917":"# (d) leukocytosis (white blood cell count, \u226510 \u00d7 103 per microliter) or leucopenia (white blood cell count, \u22643 \u00d7 103 per microliter) \"n\u00e3o temos\"\n# and mean blood pressure < 60 mm Hg\ndef cat_map(x):\n    if x < 60:\n        return 1\n    else:\n        return 0\n\nnum_features['cat_map'] = np.NaN\nnum_features['cat_map'] = num_features['map'].apply(lambda x: cat_map(x))\nnum_features['cat_map'].isnull().sum()","13920225":"num_features['cat_map'].value_counts()","251e6815":"# https:\/\/www.mayoclinic.org\/diseases-conditions\/high-blood-pressure\/expert-answers\/pulse-pressure\/faq-20058189\n# Generally, a pulse pressure greater than 40 mm Hg is abnormal.\n\ndef cat_pp(x):\n    if x > 40:\n        return 1\n    else:\n        return 0\n\nnum_features['cat_pp'] = np.NaN\nnum_features['cat_pp'] = num_features['pp'].apply(lambda x: cat_pp(x))\nnum_features['cat_pp'].isnull().sum()","dd9d1fe5":"num_features['cat_pp'].value_counts()","6bdbb085":"# https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4677588\/\n# High blood pressure: Systolic blood pressure >140 or diastolic blood pressure >90\nnum_features['cat_pa'] = np.NaN\nnum_features['cat_pa'] = ((num_features['pa_min'] < 140.0) | (num_features['pa_max'] > 90.0)).astype('int')\nnum_features['cat_pa'].isnull().sum()\n","2a9dee02":"num_features['cat_pa'].value_counts()","b7503d4a":"num_features['cat_alteracoes'] = np.NaN\nnum_features['cat_alteracoes'] = num_features['cat_temp'] + num_features['cat_pulso'] + num_features['cat_resp'] + \\\n                                 num_features['cat_map'] + num_features['cat_pp'] + num_features['cat_pa'] \nnum_features['cat_alteracoes'].value_counts()\n","6041bed7":"n_train = pd.concat([tr_num_atend_nodup, num_features.iloc[0:train_nodup_cut,:]], axis=1)\nnew_train = n_train[~train_dup3].reset_index(drop=True)\nnew_train.shape","72e04792":"new_target = target_nodup[~train_dup3].reset_index(drop=True)","797527d7":"replace_test = pd.concat([te_num_atend_nodup.reset_index(drop=True), \n                          num_features.iloc[train_nodup_cut:,:].reset_index(drop=True)], axis=1)\nreplace_test['num_atend'] = replace_test['num_atend'].astype('int')\nreplace_test.shape","29cd3663":"new_test = test.copy(deep=True)\nnew_test['pp'] = 0\nnew_test['map'] = 0\ncat_names = ['cat_temp','cat_pulso','cat_resp','cat_pa', 'cat_map', 'cat_pp', 'cat_alteracoes']\nnew_test[cat_names] = 0.0\nnew_test.shape","9e7ad7fe":"for value in replace_test['num_atend']:\n    for row in new_test[new_test.iloc[:,0] == value].index.tolist():\n        new_test.iloc[row,:] = replace_test[replace_test.iloc[:,0] == value] \n","4a6aff4c":"new_test.describe()","bd2e3fe3":"new_train[new_train['pp'] < 0]","e615f81f":"new_test[new_test['pp'] < 0]","ebe9b541":"new_train[cat_names] = new_train[cat_names].astype('int').astype('category')\nnew_train['num_atend'] = new_train['num_atend'].astype('int')\nnew_test[cat_names] = new_test[cat_names].astype('int').astype('category')\nnew_test['num_atend'] = new_test['num_atend'].astype('int')","ce27c73e":"df2_styler(test.head(10), '', new_test.iloc[:,0:6].head(10), '')","a48bd887":"new_target.value_counts().plot(kind='bar', title='Count (target)')","cb62a6d5":"def count_annotate(plot):\n    for bar in plot.patches:\n        plot.annotate(format(bar.get_height(), '.0f'), (bar.get_x() + bar.get_width() \/ 2, \n                      bar.get_height()), ha='center', va='center', size=12, xytext=(0, 5),\n                      textcoords='offset points')\n        \ndf_cat = pd.concat([new_train[cat_names], new_target], axis=1)\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12,8))\n\ndf_melt_temp = df_cat[['cat_temp', 'sepse']].melt(id_vars='cat_temp', value_vars='sepse')\nplot1 = sns.countplot(x='value', hue='cat_temp', data=df_melt_temp, ax=axs[0,0])\ncount_annotate(plot1)\nplot1.set_xlabel(\"Sepse\", size=14)\nplot1.set_ylabel(\"Temperatura\", size=14)\n\ndf_melt_pulso = df_cat[['cat_pulso', 'sepse']].melt(id_vars='cat_pulso', value_vars='sepse')\nplot2 = sns.countplot(x='value', hue='cat_pulso', data=df_melt_pulso, ax=axs[0,1])\ncount_annotate(plot2)\nplot2.set_xlabel(\"Sepse\", size=14)\nplot2.set_ylabel(\"Pulso\", size=14)\n\ndf_melt_resp = df_cat[['cat_resp', 'sepse']].melt(id_vars='cat_resp', value_vars='sepse')\nplot3 = sns.countplot(x='value', hue='cat_resp', data=df_melt_resp, ax=axs[0,2])\ncount_annotate(plot3)\nplot3.set_xlabel(\"Sepse\", size=14)\nplot3.set_ylabel(\"Respira\u00e7\u00e3o\", size=14)\n\ndf_melt_pa = df_cat[['cat_pa', 'sepse']].melt(id_vars='cat_pa', value_vars='sepse')\nplot4 = sns.countplot(x='value', hue='cat_pa', data=df_melt_pa, ax=axs[1,0])\ncount_annotate(plot4)\nplot4.set_xlabel(\"Sepse\", size=14)\nplot4.set_ylabel(\"PA\", size=14)\n\ndf_melt_resp = df_cat[['cat_pp', 'sepse']].melt(id_vars='cat_pp', value_vars='sepse')\nplot5 = sns.countplot(x='value', hue='cat_pp', data=df_melt_resp, ax=axs[1,1])\ncount_annotate(plot5)\nplot5.set_xlabel(\"Sepse\", size=14)\nplot5.set_ylabel(\"PP\", size=14)\n\ndf_melt_pa = df_cat[['cat_map', 'sepse']].melt(id_vars='cat_map', value_vars='sepse')\nplot6 = sns.countplot(x='value', hue='cat_map', data=df_melt_pa, ax=axs[1,2])\ncount_annotate(plot6)\nplot6.set_xlabel(\"Sepse\", size=14)\nplot6.set_ylabel(\"MAP\", size=14)\n\nplt.show()","915a22fe":"df_melt_sint = df_cat[['cat_alteracoes', 'sepse']].melt(id_vars='cat_alteracoes', value_vars='sepse')\nplot7 = sns.countplot(x='value', hue='cat_alteracoes', data=df_melt_sint)\ncount_annotate(plot7)\nplot7.set_xlabel(\"Sepse\", size=14)\nplot7.set_ylabel(\"Altera\u00e7\u00f5es\", size=14)\nplt.show()","0ca2db3b":"num_names = ['temperatura', 'pulso', 'respiracao', 'pa_min', 'pa_max', 'pp', 'map']\n\nsns.set(font_scale=1.3)\ncorr_train = new_train[num_names].corr()\nmask = np.triu(corr_train.corr())\nplt.figure(figsize=(5, 5))\nsns.heatmap(corr_train, annot=True, fmt='.1f', cmap='coolwarm',\n            square=True, mask=mask, linewidths=1, cbar=False)\n","fafb07ae":"sns.pairplot(pd.concat([new_train[num_names], new_target], axis=1), hue=\"sepse\")","453fc635":"df_melt2 = pd.melt(pd.concat([new_train[num_names], new_target], axis=1), id_vars='sepse')\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.boxplot(x='variable', y='value', hue='sepse', data=df_melt2, ax=ax)","d89fcd6a":"from scipy.stats import pointbiserialr\n\ndef corr_calc(df, y):\n    return df.apply(lambda x: pointbiserialr(x, y)).T\n\ndf_cor = corr_calc(new_train[num_names], new_target).round(2)\ndf_cor.columns = ['rho', 'p-value']\ndf_cor.T","9b691a14":"y_strat = pd.concat([new_train[cat_names].drop(['cat_alteracoes', 'cat_map'], axis=1), new_target], axis=1)\ny_strat['cat_temp'] = y_strat['cat_temp'].replace({0:'A0', 1:'A1', 2:'A2'})\ny_strat['cat_pulso'] = y_strat['cat_pulso'].replace({0:'B0', 1:'B1'})\ny_strat['cat_resp'] = y_strat['cat_resp'].replace({0:'C0', 1:'C1'})\ny_strat['cat_pa'] = y_strat['cat_pa'].replace({0:'D0', 1:'D1'})\ny_strat['cat_pp'] = y_strat['cat_pp'].replace({0:'E0', 1:'E1'})\ny_strat['sepse'] = y_strat['sepse'].replace({0:'F0', 1:'F1'})\n\ny_strat['strat'] = y_strat['cat_temp'] + y_strat['cat_pulso'] + y_strat['cat_resp'] + \\\n                   y_strat['cat_pa'] + y_strat['cat_pp'] + y_strat['sepse'] \n\nothers_min = y_strat['strat'].value_counts()\nothers = others_min[others_min < 10].index.to_list()\n\nfor strat in others:\n    y_strat['strat'][y_strat['strat'] == strat] = 'Others'\n\ny_strat['strat'].value_counts()","6c680c8f":"from sklearn.utils.class_weight import compute_class_weight\n\n# funcao para calcular os pesos das classes \ndef calc_cw(col):\n    col_unique = list(set(col))\n    col_weight = compute_class_weight('balanced', col_unique, col)\n    col_dict = {col_unique[i]:col_weight[i] for i in range(0, len(col_unique))}\n    col_cw = col\n    col_cw = col_cw.replace(col_dict)\n    return col_dict, col_cw\n","49a282c9":"from sklearn.model_selection import cross_validate\nfrom sklearn.metrics import balanced_accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import brier_score_loss, log_loss, matthews_corrcoef, fbeta_score\nfrom IPython.display import display\n\nscore_names = ['tr_b-acc_me', 'tr_b-acc_sd', 'val_b-acc',\n               'tr_prec_me', 'tr_prec_sd', 'val_prec',\n               'tr_rec_me', 'tr_rec_sd', 'val_rec', \n               'tr_brier_me', 'tr_brier_sd', 'val_brier', \n               'tr_logloss_me', 'tr_logloss_sd', 'val_logloss',\n               '1(%)_val', '1(%)_pred', 'val_mcorrcoef', 'val_fbeta']\n    \n# fun\u00e7\u00e3o para avaliar modelos em v\u00e1rios ajustes\ndef model_fit(X_tr, X_val, y_tr, y_val, model, tdict, fpar=None, verb=None, name='None'):\n    if bool(fpar) == False:\n        score = cross_validate(model, X_tr, y_tr, cv=10, \n                               scoring=('balanced_accuracy','precision','recall', \n                                        'neg_brier_score', 'neg_log_loss'))\n        mfit = model.fit(X_tr, y_tr)\n        \n    else:\n        score = cross_validate(model, X_tr, y_tr, cv=10, \n                               scoring=('balanced_accuracy','precision','recall', \n                                        'neg_brier_score', 'neg_log_loss'), fit_params=fpar)\n        mfit = model.fit(X_tr, y_tr, **fpar)\n        \n    model_score = {}\n    for key in score:\n        model_score.update({str(key)+'_mean': np.mean(score[key])}) \n        model_score.update({str(key)+'_std': np.std(score[key])}) \n        \n    y_pred = mfit.predict(X_val)\n    prop_pred_val = np.count_nonzero(y_val == 1)\/len(y_val)\n    prop_pred_pred = np.count_nonzero(y_pred == 1)\/len(y_pred)\n    sw_val = y_val.replace(target_dict)\n    score_list = [np.round(model_score['test_balanced_accuracy_mean'],4),\n                  np.round(model_score['test_balanced_accuracy_std'],4),\n                  balanced_accuracy_score(y_val, y_pred, sample_weight=sw_val),\n                  np.round(model_score['test_precision_mean'],4),\n                  np.round(model_score['test_precision_std'],4),\n                  precision_score(y_val, y_pred, sample_weight=sw_val),\n                  np.round(model_score['test_recall_mean'],4),\n                  np.round(model_score['test_recall_std'],4),\n                  recall_score(y_val, y_pred, sample_weight=sw_val),\n                  -1*np.round(model_score['test_neg_brier_score_mean'],4),\n                  np.round(model_score['test_neg_brier_score_std'],4),\n                  brier_score_loss(y_val, y_pred, sample_weight=sw_val),\n                  -1*np.round(model_score['test_neg_log_loss_mean'],4),\n                  np.round(model_score['test_neg_log_loss_std'],4),\n                  log_loss(y_val, y_pred, sample_weight=sw_val),\n                  prop_pred_val,\n                  prop_pred_pred,\n                  matthews_corrcoef(y_val, y_pred, sample_weight=sw_val),\n                  fbeta_score(y_val, y_pred, 2, sample_weight=sw_val)]\n    if verb == True:\n        display(pd.DataFrame(score_list, index=score_names, columns=[name]).T)\n    else:\n        pass\n    return score_list","39e7c8c9":"from sklearn.model_selection import train_test_split\n\nfinal_train = new_train.drop(['num_atend', 'cat_alteracoes'], axis=1)\nX = pd.get_dummies(final_train)\ny = new_target\n\nX_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size = 0.3, random_state=RS, stratify=y_strat['strat'])\n\n# pesos das classes\ntarget_dict, target_cw = calc_cw(y)\ny_weights = y_tr.replace(target_dict)\nsw_par={'sample_weight':y_weights}\n","03c948fc":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nlr_pipe = Pipeline(steps=[('kbest_f', SelectKBest(f_classif)),\n                          ('lr', LogisticRegression(random_state=RS))])\n\nlr_params = {\n    'kbest_f__k': [5, 10, 15, 20],\n    'lr__C': np.linspace(0, 1, 6),\n    'lr__solver':['saga'],\n    'lr__penalty':['elasticnet'],\n    'lr__l1_ratio': np.linspace(0, 1, 6)\n}\nlr_grid = GridSearchCV(lr_pipe, lr_params, scoring='balanced_accuracy')\nlr_grid.fit(X_tr, y_tr)\nprint(lr_grid.best_params_)\n\nlr = make_pipeline(SelectKBest(f_classif, k=lr_grid.best_params_['kbest_f__k']), \n                   LogisticRegression(random_state=RS, C=lr_grid.best_params_['lr__C'], solver=lr_grid.best_params_['lr__solver'],\n                                      penalty=lr_grid.best_params_['lr__penalty'], l1_ratio=lr_grid.best_params_['lr__l1_ratio']))\nlr_par={'logisticregression__sample_weight':y_weights}\nscore1 = model_fit(X_tr, X_val, y_tr, y_val, lr, target_dict, fpar=lr_par, verb=True, name='None')\n","e58cd7ea":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda_pipe = Pipeline(steps=[('kbest_f', SelectKBest(f_classif)),\n                           ('lda', LinearDiscriminantAnalysis())])\n\nlda_params = {'kbest_f__k': [5, 10, 15, 20],\n              'lda__solver':['eigen'], \n              'lda__shrinkage':np.linspace(0.1,1,9) # penaliza\u00e7\u00e3o\n             }\n\nlda_grid = GridSearchCV(lda_pipe, lda_params, scoring='balanced_accuracy')\nlda_grid.fit(X_tr, y_tr)\nprint(lda_grid.best_params_)\n\nlda = make_pipeline(SelectKBest(f_classif, k=lda_grid.best_params_['kbest_f__k']), \n                    LinearDiscriminantAnalysis(solver=lda_grid.best_params_['lda__solver'], \n                                               shrinkage=lda_grid.best_params_['lda__shrinkage']))\n# fpar=None \/ lda n\u00e3o aceita sample_weight\nscore2 = model_fit(X_tr, X_val, y_tr, y_val, lda, target_dict, fpar=None, verb=True, name='None')\n","08f1dfab":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVC\n\nsvc_pipe = Pipeline(steps=[('rscaler', RobustScaler()),\n                           ('kbest_f', SelectKBest(f_classif)),\n                           ('svc', SVC(random_state=RS, probability=True))])\n\nsvc_grid = {\n    'rscaler__quantile_range':[(25.0,75.0),(20.0,80.0),(10.0, 90.0),(0.05, 0.95) ,(0.01, 0.99)],\n    'kbest_f__k': [5, 10, 15, 20],\n    'svc__kernel': ['rbf'],\n}\n\ngrid_svc = GridSearchCV(svc_pipe, svc_grid, scoring='balanced_accuracy', cv=5)\ngrid_svc.fit(X_tr, y_tr)\nprint(grid_svc.best_params_)\n\nsvc = make_pipeline(RobustScaler(quantile_range=grid_svc.best_params_['rscaler__quantile_range']), \n                    SelectKBest(f_classif, k=grid_svc.best_params_['kbest_f__k']), \n                    SVC(probability=True))\n\nsvc_par={'svc__sample_weight': y_weights}\nscore3 = model_fit(X_tr, X_val, y_tr, y_val, svc, target_dict, fpar=svc_par, verb=True, name='None')","10a5f17d":"from lightgbm.sklearn import LGBMClassifier\n\n\nlgbm_params = {'boosting_type':'gbdt',\n               'objective':'binary',\n               'metric':'binary_error',\n               #'is_unbalance': True, \n               'n_jobs':-1\n              }\n\nlgbm_clf0 = LGBMClassifier(random_state=RS, **lgbm_params)\nlgbm_clf0.fit(X_tr, y_tr, y_weights)\n\nlgbm_par={'sample_weight': y_weights}\nscore4 = model_fit(X_tr, X_val, y_tr, y_val, lgbm_clf0, target_dict, fpar=lgbm_par, verb=True, name='None')","005bff49":"grid_params = {'colsample_bytree': np.linspace(0, 1, 11), # colsample_bytree = feature_fraction = 1.0 \n               'subsample': np.linspace(0, 1, 11) # subsample = bagging_fraction = 1.0\n              } \n\nlgbm_clf1 = LGBMClassifier(random_state=RS, **lgbm_params)\ngrid_lgbm1 = GridSearchCV(estimator=lgbm_clf1, param_grid=grid_params, scoring='balanced_accuracy', \n                             cv=5, verbose=1, n_jobs=-1, refit=False)\ngrid_lgbm1.fit(X_tr, y_tr, y_weights)\nprint(grid_lgbm1.best_params_)\nscore5 = model_fit(X_tr, X_val, y_tr, y_val, lgbm_clf1, target_dict, fpar=lgbm_par, verb=True, name='None')","f2690383":"fixed_params = {\n    'colsample_bytree': grid_lgbm1.best_params_['colsample_bytree'],\n    'subsample': grid_lgbm1.best_params_['subsample']\n    }\n\ngrid_params2 = {'n_estimators': np.arange(20, 120, 20), # default = 100, single tuning = 20 \n                'max_depth': np.arange(4, 7, 1), # default = -1, single tuning = 4 \n                'num_leaves': np.arange(4, 20, 4), # default = 31, single tuning = 9\n                'learning_rate': np.arange(0, 0.05, 0.01), # default = 0.1, single tuning = 0.035\n                'min_split_gain': np.arange(0, 1, 0.25), # default = 0, single tuning = 0.65\n               }\n\nlgbm_clf2 = LGBMClassifier(random_state=RS, **lgbm_params, **fixed_params)\n\ngrid_lgbm2 = GridSearchCV(estimator=lgbm_clf2, param_grid=grid_params2, scoring=('balanced_accuracy'),\n                             cv=5, verbose=1, n_jobs=-1, refit=False)\n\ngrid_lgbm2.fit(X_tr, y_tr, y_weights)\nprint(grid_lgbm2.best_params_)\nscore6 = model_fit(X_tr, X_val, y_tr, y_val, lgbm_clf2, target_dict, fpar=lgbm_par, verb=True, name='None')","029ff6bc":"fixed_params.update(grid_lgbm2.best_params_)\n\ngrid_params3 = {'reg_alpha': np.linspace(0, 1, 11), # default = 0, single tuning = 0\n                'reg_lambda': np.linspace(0, 1, 11),\n               }\n\nlgbm_clf3 = LGBMClassifier(random_state=RS, **lgbm_params, **fixed_params)\n\ngrid_lgbm3 = GridSearchCV(estimator=lgbm_clf3, param_grid=grid_params3, scoring=('balanced_accuracy'),\n                             cv=5, verbose=1, n_jobs=-1, refit=False)\n\ngrid_lgbm3.fit(X_tr, y_tr, y_weights)\nprint(grid_lgbm3.best_params_)\nscore7 = model_fit(X_tr, X_val, y_tr, y_val, lgbm_clf3, target_dict, fpar=lgbm_par, verb=True, name='None')\n\nfixed_params.update(grid_lgbm3.best_params_)","4ea31f36":"from sklearn.ensemble import StackingClassifier\n\nlgbm = LGBMClassifier(random_state=RS, **lgbm_params, **fixed_params)\n\nest = [('lr', lr), ('lda', lda), ('svc', svc), ('lgbm', lgbm)]\nstacking_clf = StackingClassifier(estimators=est, final_estimator=LogisticRegression(random_state=RS, C=0.2))\nstacking_clf.fit(X_tr, y_tr)\n\nscore8 = model_fit(X_tr, X_val, y_tr, y_val, stacking_clf, target_dict, verb=True, name='None')","e6af1be2":"from sklearn.ensemble import VotingClassifier\n\nest = [('lr', lr), ('lda', lda), ('svc', svc), ('lgbm', lgbm)]\n# peso 2 para regress\u00e3o log\u00edstica porque os demais classificadores est\u00e3o com sobreajuste\nvoting_clf = VotingClassifier(est, voting='soft', weights=[2,1,1,1], flatten_transform=True)\nvoting_clf.fit(X_tr, y_tr)\n\nscore9 = model_fit(X_tr, X_val, y_tr, y_val, voting_clf, target_dict, verb=True, name='None')","c93a0220":"clfs = [lr, lda, svc, lgbm, stacking_clf, voting_clf]\nlr.fit(X_tr, y_tr)\nlda.fit(X_tr, y_tr)\nsvc.fit(X_tr, y_tr)\nlgbm.fit(X_tr, y_tr)\n\npd.DataFrame([score1, score2, score3, score6, score7, score8], \n             index=['lr','lda','svc','lgbm','stacking','voting'],\n             columns=score_names)","992590e5":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import plot_precision_recall_curve\n\nfig, axs = plt.subplots(nrows=2, ncols=len(clfs), figsize=(30, 15))\nfor i in range(len(clfs)):\n    plot_confusion_matrix(clfs[i], X_val, y_val, normalize='all', ax=axs[0,i]) \n    plot_precision_recall_curve(clfs[i], X_val, y_val, response_method='predict_proba', ax=axs[1,i])\n\nplt.tight_layout()","7298ac4b":"final_test = pd.get_dummies(new_test.drop(['num_atend', 'cat_alteracoes'], axis=1))\nfinal_train = pd.get_dummies(X)\ny_pred = pd.DataFrame(lr.fit(X_tr, y_tr).predict(final_test), columns=['sepse'])\nsub = pd.concat([test_id, y_pred], axis=1)\nsub.to_csv(\"submit_lr3.csv\", index=False)","6a95777e":"## Preencher dados faltantes\n\nA distribui\u00e7\u00e3o das vari\u00e1veis sofre poucas altera\u00e7\u00f5es ao retirar os duplicados.  ","5b64ec93":"## pa_max > pa_min (ou sist\u00f3lica < diast\u00f3lica)\nN\u00e3o fazem sentido (na verdade, o nome aparentemente est\u00e1 trocado, mas ser\u00e1 mantido pra evitar problemas).\n","6ea688d2":"## An\u00e1lise Explorat\u00f3ria\n\n### Vari\u00e1vel resposta","aca8dcdc":"## Modelo\n\n### Estratificar\n\nQueremos colocar todos os grupos de sepse\/altera\u00e7\u00f5es poss\u00edvel. Como alguns grupos ficariam com poucos indiv\u00edduos (gerando erro na hora do *data splitting*), eles ser\u00e3o somados em um estrato \"Outros\".","f6b340e8":"## Valores discrepantes\n\nQueremos remover apenas valores que s\u00e3o praticamente um erro de registro (temperatura negativa).","7542ce19":"Aparentemente, temos entradas duplicadas no treino com todas as medidas iguais e a mesma resposta, isto pode gerar problemas de desbalanceamento\/ajuste.","a00905d2":"### An\u00e1lise Discriminante Penalizada","6e0a30d6":"### Stacking","fd00ebd9":"Queremos diagnosticar sepse. Se algu\u00e9m inseriu dados de indiv\u00edduos que j\u00e1 estavam sem temperatura, pulso etc, estes n\u00e3o v\u00e3o ajudar no modelo. Acreditamos que os zeros s\u00e3o erros de preenchimento e vamos substitu\u00ed-los por NaN.","f21c87e7":"## Zeros\n\nS\u00e3o problem\u00e1ticos em algumas situa\u00e7\u00f5es (divis\u00e3o por zero) e tamb\u00e9m n\u00e3o fazem sentido para certos problemas, por exemplo:\n\nTemperatura = 0 -> \u00f3bito\n\nPA = 0 -> \u00f3bito\n\nRespira\u00e7\u00e3o = 0 -> \u00f3bito\n\nPulso = 0 -> \u00f3bito\n\nVamos investigar, mas a princ\u00edpio, substituir todos por NaN ou excluir do dataset.","63b6fbd8":"# Pacotes, dados etc","92f6b006":"### Fun\u00e7\u00f5es para registrar scores","79674b81":"### Vari\u00e1veis categ\u00f3ricas","32e35576":"### Regress\u00e3o Log\u00edstica\n\n","2efc15c6":"## Escolha do imputer\n\nA t\u00e9cnica 'mice' do 'impyute' parece ser a mais eficaz, por ter uma distribui\u00e7\u00e3o id\u00eantica aos dados originais. Como temos apenas os valores \u00fanicos de cada Id, basta completar no conjunto de teste original.","817e383d":"O gr\u00e1fico Sepse x Altera\u00e7\u00f5es indica a necessidade de utilizar amostragem estratificada, pois temos diversas situa\u00e7\u00f5es em que o paiente tem 1,2,3,... altera\u00e7\u00f5es e tem\/n\u00e3o tem sepse. Sem a estratifica\u00e7\u00e3o, o modelo ficar\u00e1 viesado para a parte que receber a maior quantidade de amostras e vai errar para o grupo que ficou faltando (que estar\u00e1 na valida\u00e7\u00e3o e afetar\u00e1 as m\u00e9tricas).","89f18063":"## Faltantes","d971571a":"Observa-se ainda temos duplicados com valores diferentes na vari\u00e1vel resposta. Como temos dados faltantes, podemos manter as linhas duplicadas para preservar ao m\u00e1ximo o conjunto de dados, contudo, elas dever\u00e3o ser retiradas antes de treinar o modelo, uma vez que n\u00e3o \u00e9 poss\u00edvel determinar se a observa\u00e7\u00e3o \u00e9 0 ou 1.\n\nQuanto ao teste, n\u00e3o podemos desconsiderar os duplicados, pois ser\u00e3o utilizados para previs\u00e3o. De qualquer forma, podemos retir\u00e1-los antes de fazer a engenharia de atributos, para que n\u00e3o coloquem vi\u00e9s na distribui\u00e7\u00e3o. Depois, usamos a coluna 'num_atend' para substituir os valores \u00fanicos no teste que est\u00e1 com valores duplicados.","66283773":"### Teste","4571fdf1":"Os par\u00e2metros 'majority' e n_neighbors = 10 aparentam ser est\u00e1veis, os outros geram problemas de ajuste.","8a10825a":"**Mean Arterial Pressure**","f7bb3339":"### Voting","e342eb93":"### Gradient Boosting (LGBM)","c6e2f0ce":"## Entrega","1e195569":"# Engenharia de atributos\n\nCuidados:\n* Preservar a distribui\u00e7\u00e3o dos dados iniciais;\n* Evitar gafes, como valores negativos ou fora do intervalo esperado (por exemplo, temperatura negativa).\n\n## Conferir as distribui\u00e7\u00f5es\n\nA ideia de combinar treino e teste para preenchimento dos dados, inicialmente, n\u00e3o \u00e9 correta (*data leakage*, neste caso, *data contamination*, pois informa\u00e7\u00f5es do teste s\u00e3o passadas para o treino. No entanto, este erro implica na falta de generaliza\u00e7\u00e3o do modelo (n\u00e3o ter\u00e1 a capacidade de prever o futuro a partir do presente) e o problema n\u00e3o informa se os dados de teste s\u00e3o uma parti\u00e7\u00e3o dos dados amostrados junto com o treino ou se s\u00e3o dados amostrados ap\u00f3s o teste. De qualquer forma, foram testadas as abordagens corretas e elas n\u00e3o obtiveram sucesso com a mesma modelagem (a regress\u00e3o log\u00edsitca n\u00e3o passou de 0.75 utilizando o procedimento correto).","0862c28a":"### Vari\u00e1veis categ\u00f3ricas","50607b22":"### SVC","ff52b236":"## Duplicados\n\nN\u00e3o s\u00e3o necessariamente um problema, j\u00e1 que o paciente pode ter sido examinado mais de uma vez. ","9f729b1d":"## Z-Score\n\n### Treino","e23ea031":"# Pr\u00e9-processamento\n\n","c6ba7a5a":"### Resultados","3f5674f4":"### Vari\u00e1veis num\u00e9ricas","262ae1c7":"## Criar novos atributos\n\n### PP \/ MAP\n\n**Pulse Pressure**","13c9847b":"## Reconstruir treino e teste\n\nComo n\u00e3o podemos ignorar os duplicados, vamos usar os valores \u00fanicos que foram completados para imputar o conjunto de teste baseado na coluna 'num_atend' que controla as observa\u00e7\u00f5es duplicadas (mesmo paciente).\n","a3276555":"Soma da diferen\u00e7a entre as estat\u00edsticas descritivas e os dados completados por mice, knn e em."}}