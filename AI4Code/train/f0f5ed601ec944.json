{"cell_type":{"8a948766":"code","d0f768cc":"code","754e8465":"code","25c36eee":"code","c6954d55":"code","03e96549":"code","39cfaa25":"code","021b5296":"code","5e6dc34d":"code","2254295b":"code","f2bb5ede":"code","11a29af8":"code","b355145a":"code","981606fc":"code","3fea6d03":"code","7b65ba58":"code","7effab68":"code","634cbac8":"code","75ce4417":"code","1b311794":"code","3dd2e346":"code","f35c53f6":"code","820ba436":"code","f8fb8495":"code","3dba126d":"code","5aaf8f59":"code","8b7bf54b":"code","4da22cbb":"code","cd359f08":"code","acf73e97":"code","b20b44eb":"code","3f61683c":"code","99e6b44f":"code","3e101d08":"code","63411f59":"markdown","2c80e18c":"markdown","1e386f85":"markdown","36b4cd4a":"markdown","bca8407a":"markdown","48e6868f":"markdown","e56c04cb":"markdown","f94d1304":"markdown","16bcc76c":"markdown","23c0ea8b":"markdown","1861fa11":"markdown","c0f182e0":"markdown","dead2fd5":"markdown","0a2e643c":"markdown","c2900153":"markdown","84b73596":"markdown","1f82b126":"markdown","a2518267":"markdown","cd0c5785":"markdown","908426a9":"markdown","274baa22":"markdown","877bdfc7":"markdown","71af8537":"markdown","548373f4":"markdown","c8d207e7":"markdown","ba4bea26":"markdown","c8d39fda":"markdown","85f35caf":"markdown","7ecaac3a":"markdown","ad9b7281":"markdown","2f00cb83":"markdown","b98fb44f":"markdown"},"source":{"8a948766":"#Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # data visualization\nsns.set(style=\"darkgrid\")\nfrom scipy.stats import norm\nfrom scipy import stats\nimport pylab ","d0f768cc":"# Read the CSV file into a DataFrame: data\ndata= pd.read_csv('\/kaggle\/input\/fitbitmc\/Fitbit.csv', parse_dates=['dateTime'], dayfirst=True)\n#change colunm name\ndata = data.rename(columns={'dateTime': 'Date'})","754e8465":"print(data.info())","25c36eee":"print(data.head(1).round(0))","c6954d55":"print(data.tail(1).round(0))","03e96549":"print(data.isnull().sum())","39cfaa25":"print(data.describe().round(0))","021b5296":"data.loc[data['Lightly active minutes'] == 0]","5e6dc34d":"#Checking to see if minutes add up to 1440. \ndata['Total minutes'] = data['Lightly active minutes'] +data['Moderately active minutes'] +data['Sedentary minutes'] +data['Very active minutes']\ndata['Missing'] = data['Total minutes'] - 1440\n\ndef complete(x):\n\n    if (x == 1440):\n        return 'Yes'\n    else:\n        return 'No'\n\ndata['complete'] = data['Total minutes'].apply(complete)\n\ndata['complete'].value_counts()","2254295b":"#First, address those 16 days. drop all values for those days. \nc = ['Calories', 'Lightly active minutes','Moderately active minutes','Sedentary minutes','Very active minutes', 'Distance','Steps']\ndata[c] = data[c].mask(data['Sedentary minutes'] == 1440)\n\n#Rerun this to reflect the changing of the 16 days.\ndata['Total minutes'] = data['Lightly active minutes'] +data['Moderately active minutes'] +data['Sedentary minutes'] +data['Very active minutes']\ndata['Missing'] = data['Total minutes'] - 1440\n\ndef complete(x):\n\n    if (x == 1440):\n        return 'Yes'\n    else:\n        return 'No'\n\ndata['complete'] = data['Total minutes'].apply(complete)\n\n#Investigating the days where each of the minute features\n#seperate these days into a datframe \ntotal = data['complete'] == 'Yes'\ntotal_1 = data.loc[total]\ntotal_1 = total_1.copy()\n\ntotal_1['sed_per'] = total_1['Sedentary minutes'] \/ 1440\n\ntotal_1['lig_per'] = total_1['Lightly active minutes'] \/ 1440\n\ntotal_1['mod_per'] = total_1['Moderately active minutes'] \/ 1440\n\ntotal_1['very_per'] = total_1['Very active minutes'] \/ 1440\n\nprint(total_1[['sed_per']].mean().round(4))\nprint(total_1[['lig_per']].mean().round(4)) \nprint(total_1[['mod_per']].mean().round(4)) \nprint(total_1[['very_per']].mean().round(4)) ","f2bb5ede":"data['sed_per'] = (data['Missing'] * -0.9071).round(0)\ndata['lig_per'] = (data['Missing'] * -0.0585).round(0)\ndata['mod_per'] = (data['Missing'] * -0.0073).round(0)\ndata['very_per'] = (data['Missing'] * -0.0271).round(0)\n\ndata['per_tot'] = data['sed_per']+ data['lig_per'] + data['mod_per']  +data['very_per'] \ndata['diff'] = data['per_tot'] + data['Missing']\n\n\n#Disperse these minutes to each minute feature\ndata['Lightly active minutes'] = data['Lightly active minutes'] + data['lig_per'] \ndata['Moderately active minutes'] = data['Moderately active minutes'] + data['mod_per']\ndata['Sedentary minutes'] = data['Sedentary minutes']  + data['sed_per']\ndata['Very active minutes'] = data['Very active minutes'] + data['very_per'] \ndata['Sedentary minutes'] = data['Sedentary minutes'] - data['diff']\ndata['recheck'] = data['Lightly active minutes'] +data['Moderately active minutes'] +data['Sedentary minutes'] +data['Very active minutes']\n#dropping \ndata=data.drop(['Missing','complete','Total minutes', 'per_tot' , 'sed_per', 'lig_per','mod_per','very_per','diff','recheck'],axis=1) \n\ndata.info()","11a29af8":"\n#Create a feature for each day of the week\ndata['Day of the week'] = data['Date'].dt.day_name()\ndata['Day of the week'] = data['Day of the week'].replace('Monday', 'Mon')\ndata['Day of the week'] = data['Day of the week'].replace('Tuesday', 'Tues')\ndata['Day of the week'] = data['Day of the week'].replace('Wednesday', 'Wed')\ndata['Day of the week'] = data['Day of the week'].replace('Thursday', 'Thurs')\ndata['Day of the week'] = data['Day of the week'].replace('Friday', 'Fri')\ndata['Day of the week'] = data['Day of the week'].replace(\"Saturday\", \"Sat\")\ndata['Day of the week'] = data['Day of the week'].replace(\"Sunday\", \"Sun\")\n#Create a feature for if its the lockdown\ndef lockdown(x):\n\n    if (x >= pd.to_datetime('2020-03-27')) & (x <=pd.to_datetime('2020-07-19')):\n        return 'Yes'\n    elif (x >= pd.to_datetime('2020-10-22')) & (x <=pd.to_datetime('2020-11-30')):\n        return 'Yes'\n    elif (x >=pd.to_datetime('2020-12-24')):\n        return 'Yes'\n    else:\n        return 'No'\n\ndata['In lockdown'] = data['Date'].apply(lockdown)","b355145a":"#Filling in missing values\ndata['Calories'] = data.groupby(['In lockdown','Day of the week'])['Calories'].apply(lambda x: x.fillna(x.mean()))\ndata['Lightly active minutes'] = data.groupby(['In lockdown','Day of the week'])['Lightly active minutes'].apply(lambda x: x.fillna(x.mean()))\ndata['Moderately active minutes'] = data.groupby(['In lockdown','Day of the week'])['Moderately active minutes'].apply(lambda x: x.fillna(x.mean()))\ndata['Sedentary minutes'] = data.groupby(['In lockdown','Day of the week'])['Sedentary minutes'].apply(lambda x: x.fillna(x.mean()))\ndata['Very active minutes'] = data.groupby(['In lockdown','Day of the week'])['Very active minutes'].apply(lambda x: x.fillna(x.mean()))\ndata['Steps'] = data.groupby(['In lockdown','Day of the week'])['Steps'].apply(lambda x: x.fillna(x.mean()))\ndata['Distance'] = data.groupby(['In lockdown','Day of the week'])['Distance'].apply(lambda x: x.fillna(x.mean()))\n\n#delete\ndel c, total, total_1","981606fc":"#Target Calories\n#histogram and normal probability plot\nsns.distplot(data['Calories'], fit=norm).set_title('Calories Burned Histogram (kcal)')","3fea6d03":"#stats\nprint(data['Calories'].describe().round(0))","7b65ba58":"#Scatter plot\n#Changing distance to km\ndef dist(x):\n    return (x)\/100000\ndata[['Distance']] = data[['Distance']].apply(dist)\n\nsns.set()\ncols = ['Calories', 'Distance', 'Steps', 'Sedentary minutes', 'Lightly active minutes', 'Moderately active minutes', 'Very active minutes']\nsns.pairplot(data[cols])\nplt.show()","7effab68":"Yes = data['In lockdown'].value_counts()[1]\nNo = data['In lockdown'].value_counts()[0]\nYes_per = Yes \/ data.shape[0] * 100\nNo_per = No \/ data.shape[0] * 100\n\nplt.figure(figsize=(10, 10))\nsns.countplot(data['In lockdown'])\nplt.ylabel('No. of days', size=15, labelpad=15)\nplt.xticks((0, 1), ['No ({0:.2f}%)'.format(No_per), 'Yes ({0:.2f}%)'.format(Yes_per)])\nplt.show()","634cbac8":"#Filter by Lockdown\nprint(data.groupby(['In lockdown']).mean().round(0))\nlock = data.groupby(['In lockdown']).mean().round(0)\n\norder = [\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\",\"Sun\"]","75ce4417":"swarmplot_1 = sns.catplot(x='Day of the week', y=\"Calories\", hue=\"In lockdown\", kind=\"swarm\", data=data, order=order)\nax = plt.gca()\nax.set_title('Calories Burned by Day and Lockdown Status')\nax.set_ylabel('Calories (kcal)')\nprint(swarmplot_1)","1b311794":"#bar plots minutes\nplt.figure(figsize=[12,12])\nplt.subplot(2,2,1)\nb1 = sns.barplot(x=\"Day of the week\", y=\"Sedentary minutes\", hue=\"In lockdown\",data=data, order=order)\nplt.subplot(2,2,2)\nb2=sns.barplot(x=\"Day of the week\", y=\"Lightly active minutes\", hue=\"In lockdown\",data=data, order=order)\nplt.subplot(2,2,3)\nb3=sns.barplot(x=\"Day of the week\", y=\"Moderately active minutes\", hue=\"In lockdown\",data=data, order=order)\nplt.subplot(2,2,4)\nb4 = sns.barplot(x=\"Day of the week\", y=\"Very active minutes\", hue=\"In lockdown\",data=data, order=order)","3dd2e346":"#Correlation \nheatmap = sns.heatmap(data.corr(), vmin=-1, vmax=1, annot=True, cmap='coolwarm_r')","f35c53f6":"#Create a feature for if its the weekend or weekday.\ndef week_status(x):\n    \n    if (x == 'Sat'):\n        return 'Weekend'\n    elif (x == 'Sun'):\n        return 'Weekend'\n    else:\n        return 'Weekday'\n\ndata['Week_status'] = data['Day of the week'].apply(week_status)\n#on fitbit dashboard very active and moderately active are added together and reported as Active minutes\ndata['Active minutes'] = data['Very active minutes'] + data['Moderately active minutes']\n#Avg steps per meter\ndata['Avg steps per km'] = data['Steps'] \/ (data['Distance'])\n\n#dropping\ndata=data.drop(['Very active minutes', 'Moderately active minutes'],axis=1)  \ndata=data.drop(['Steps', 'Distance'],axis=1)  ","820ba436":"#Correlation\nheatmap = sns.heatmap(data.corr(), vmin=-1, vmax=1, annot=True, cmap='coolwarm_r')","f8fb8495":"#index date\ndata = data.set_index('Date')\n# Print the col\n#Feature Transformation\n#scikit-learn does not accept non-numerical features.\n# Create dummy variables: data\ndata = pd.get_dummies(data, drop_first=True)\n\n# a list of the feature names for later\nfeature_names = ['Lightly active minutes', 'Sedentary minutes','Active minutes', 'Avg steps per km', 'Day of the week_Mon','Day of the week_Sat', 'Day of the week_Sun', 'Day of the week_Thurs', 'Day of the week_Tues', 'Day of the week_Wed', 'In lockdown_Yes','Week_status_Weekend'] ","3dba126d":"#Models\n# Import necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n\n# Create training and test sets\n# Create arrays for features and target variable\n\nX =  StandardScaler().fit_transform(data.drop(columns = 'Calories'))\ny = data['Calories'].values\n\nX_train, X_test = X[0:int(len(X)*0.8)], X[int(len(X)*0.8):]\ny_train, y_test = y[0:int(len(y)*0.8)], y[int(len(y)*0.8):]\n\n\n#LINEAR MODEL\n# Create the regressor: reg\nreg = LinearRegression()\n\n# Fit the regressor to the training data\nreg.fit(X_train, y_train)\n\n# Predict on the test data: y_pred\ny_pred_reg = reg.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(reg.score(X_test, y_test)))\nrmse_reg = np.sqrt(mean_squared_error(y_test, y_pred_reg))\nprint(\"Root Mean Squared Error: {}\".format(rmse_reg))\nprint(\"Mean absolute error: {}\".format(mean_absolute_error(y_test, y_pred_reg)))\n","5aaf8f59":"SEED = 42 #random seed\n#RANDOM FOREST Default\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create the random forest model and fit to the training data\nrfr = RandomForestRegressor(random_state= SEED)\nrfr.fit(X_train, y_train)\n\n# Predict on the test data: y_pred\ny_pred_rfr = rfr.predict(X_test)\n\n# Compute and print R^2, RMSE and MAE\nprint(\"R^2: {}\".format(rfr.score(X_test, y_test)))\nrmse_rfr = np.sqrt(mean_squared_error(y_test, y_pred_rfr))\nprint(\"Root Mean Squared Error: {}\".format(rmse_rfr))\nprint(\"Mean absolute error: {}\".format(mean_absolute_error(y_test, y_pred_rfr)))","8b7bf54b":"#TUNING\nfrom sklearn.model_selection import ParameterGrid\n\n# Create a dictionary of hyperparameters to search\ngrid = {'n_estimators':[100,200,300,500,1000], 'max_depth': [3,5,7], 'max_features': [4,8,12], 'random_state': [SEED] }\n\ntest_scores = []\n\n# Loop through the parameter grid, set the hyperparameters, and save the scores\nfor g in ParameterGrid(grid):\n    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n    rfr.fit(X_train, y_train)\n    test_scores.append(rfr.score(X_test, y_test))\n\n# Find best hyperparameters from the test score and print\nbest_idx = np.argmax(test_scores)\nprint(test_scores[best_idx], ParameterGrid(grid)[best_idx])","4da22cbb":"#Evaluate performance\n\n#Lastly, and as always, we want to evaluate performance of our best model to check \n#how well or poorly we are doing. \n\n# Create the random forest model and fit to the training data\nrfr_2 = RandomForestRegressor(n_estimators=200, max_depth=7,max_features=12, random_state= SEED)\nrfr_2.fit(X_train, y_train)\n\n# Predict on the test data: y_pred\ny_pred_rfr_2 = rfr_2.predict(X_test)\n\n# Compute and print R^2, RMSE and MAE\nprint(\"R^2: {}\".format(rfr_2.score(X_test, y_test)))\nrmse_rfr_2 = np.sqrt(mean_squared_error(y_test, y_pred_rfr_2))\nprint(\"Root Mean Squared Error: {}\".format(rmse_rfr_2))\nprint(\"Mean absolute error: {}\".format(mean_absolute_error(y_test, y_pred_rfr_2)))","cd359f08":"#Random forest feature importances\n# Get feature importances from our random forest model\nimportances = rfr_2.feature_importances_\n\n# Get the index of importances from greatest importance to least\nsorted_index = np.argsort(importances)[::-1]\nx = range(len(importances))\n\n# Create tick labels \nlabels = np.array(feature_names)[sorted_index]\nplt.bar(x, importances[sorted_index], tick_label=labels)\n\n# Rotate tick labels to vertical\nplt.xticks(rotation=90)\nplt.show()","acf73e97":"#A gradient boosting model\n#GRADIENT BOOSTING REGRESSOR DEFAULT\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Create GB model \ngbr = GradientBoostingRegressor(random_state= SEED)\ngbr.fit(X_train, y_train)\n# Predict on the test data: y_pred\ny_pred_gbr = gbr.predict(X_test)\n# Compute and print R^2, RMSE and MAE\nprint(\"R^2: {}\".format(gbr.score(X_test, y_test)))\nrmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))\nprint(\"Root Mean Squared Error: {}\".format(rmse_gbr))\nprint(\"Mean absolute error: {}\".format(mean_absolute_error(y_test, y_pred_gbr)))","b20b44eb":"#TUNING GRADIENT MODEL\nfrom sklearn.model_selection import ParameterGrid\n\n# Create a dictionary of hyperparameters to search\ngrid = {'n_estimators':[100,200,300,500,1000], 'learning_rate': [0.01,0.05,0.1], 'max_features': [4,8,12], 'max_depth':[3,5,7], 'random_state': [SEED] }\n\ntest_scores = []\n\n# Loop through the parameter grid, set the hyperparameters, and save the scores\nfor g in ParameterGrid(grid):\n    gbr.set_params(**g)  # ** is \"unpacking\" the dictionary\n    gbr.fit(X_train, y_train)\n    test_scores.append(gbr.score(X_test, y_test))\n\n# Find best hyperparameters from the test score and print\nbest_idx = np.argmax(test_scores)\nprint(test_scores[best_idx], ParameterGrid(grid)[best_idx])","3f61683c":"# Create GB model \ngbr_2 = GradientBoostingRegressor(n_estimators = 1000, learning_rate = 0.01, max_features = 8, max_depth = 5, random_state= SEED)\ngbr_2.fit(X_train, y_train)\n# Predict on the test data: y_pred\ny_pred_gbr_2 = gbr_2.predict(X_test)\n# Compute and print R^2, RMSE and MAE\nprint(\"R^2: {}\".format(gbr_2.score(X_test, y_test)))\nrmse_gbr_2 = np.sqrt(mean_squared_error(y_test, y_pred_gbr_2))\nprint(\"Root Mean Squared Error: {}\".format(rmse_gbr_2))\nprint(\"Mean absolute error: {}\".format(mean_absolute_error(y_test, y_pred_gbr_2)))","99e6b44f":"#GRADIENT BOOSTINGfeature importances\nfeatures = data[feature_names]\n\n# Extract feature importances from the fitted gradient boosting model\nfeature_importances = gbr_2.feature_importances_\n\n# Get the indices of the largest to smallest feature importances\nsorted_index = np.argsort(feature_importances)[::-1]\nx = range(len(feature_importances))\n\n# Create tick labels \nlabels = np.array(feature_names)[sorted_index]\nplt.bar(x, feature_importances[::-1], tick_label=labels)\n\n# Set the tick lables to be the feature names, according to the sorted feature_idx\nplt.xticks(rotation=90)\nplt.title('GBM Feature Importance')\nplt.show()","3e101d08":"#Plotting predictions against actual values\nY = data['Calories']\ny_index_train, y_index_test = Y[0:int(len(Y)*0.8)], Y[int(len(Y)*0.8):]\nperdict = pd.DataFrame(y_pred_rfr_2,  columns=['RF_predictions'],    index=y_index_test.index)\n\nresults  = pd.concat([y_index_test, perdict], axis=1)\n\nresults.plot(y=['Calories','RF_predictions'])\nplt.title('Calories Burned Actual v Predicted')\nplt.ylabel('Calories Burned (kcal)')\nplt.show()","63411f59":"## **2. Domain Understanding**\n\nThe tracker I use is the [Fitbit Charge](https:\/\/www.fitbit.com\/global\/ie\/products\/trackers\/charge4?sku=417BKBK). \nI bought my Fitbit in January 2020 and downloaded the data from my account. There have been several lockdowns since then. \n\nMy activity pre lockdown:\n* Cycled to work (7km).\n* Mix of cardio and resistance training at the gym.\n* More active at the weekends.\n\nProject objectives: \n* Visualise how lockdown impacted my health and fitness statistics. \n* Find a model that accurately predicts calories burned (KPI). \n* Create a custom fitness dashboard. ","2c80e18c":"Performs slightly better than MLR. I will tune it to see if I can increase performance. ","1e386f85":"Average calories burned over the period was 2,688.","36b4cd4a":"The scatterplot below illustrates the target and features relationships. I converted 'Distance' from cm to km to make the plot clearer.\nA clear positive linear relationship between calories and steps & distance.\nThe three active minutes are much more dispersed and positive. The sedentary is dispersed and negative. There is a strong linear relationship between `Distance` & `Steps`. ","bca8407a":"Missing values for each feature were filled with the average value based for that day of the week and if we were in lockdown or not. ","48e6868f":"Viewing the correlation matrix Calories is strngly coreelated with `Distance` & `Steps`. We saw in the scatterplot that these two are highly correlated. This could cause some issues with our models latter. ","e56c04cb":"Lockdown had the biggest impact on my moderately active minutes more than any other minute feature. ","f94d1304":"## **4. Data Preparation**\n\n### **4.1 Feature Engineering**\n\n\n\n\nTo fix the issues above:\n\n* create a new feature for average steps per meter.\n* feature for weekday or weekend.\n* combined very active and moderately active similar to the Fitbit dashboard.","16bcc76c":"### **3.2 EDA**\n\nThe target feature is calories burned, the plot below shows it is pretty close to a normal distribution.","23c0ea8b":"As seen above, there are missing values in the minute features. \nAlso, the minimum value is zero for several features.\nChecking the zero values first below. It looks like I wasn\u2019t wearing my Fitbit or something happened with syncing the device. \nI definitely took at least one step over two weeks.\n\n","1861fa11":"The perfromance is increase over the out of the box model and its better than MLR but the tuned RF. An interesting finding form this model is that days of the week feature is important in this model. ","c0f182e0":"Plotting the predicted calories over the actual calories in the test data below. It's close but could be better. ","dead2fd5":"## **6. Summary**\n\nFollowed the CRISP-DM method.\n\n* Formulated projects objectives.\n* Conducted exploratory data analysis.Answered first objective - health & fitness adversely affected by lockdown.\n* Prepared data for modelling stage\n* Predicted calories burned using several different models. RF model was the best based on evaluation metrics   \n* Project not at the deployment stage yet. Integration of additional data from other fitness apps.\n* Conduct the CRISP-DM cycle again.\n  * Do new features improve results? \n  * Try different machine learning algorithms, do they improve results?        \n* Only when I\u2019m satisfied with the model will I move to the deployment stage.","0a2e643c":"The stats show a negative impact across all fitness metrics. ","c2900153":"The R squared for the MLR model is 38%. Next, I tried an out of the box Random Forest (RF) model. ","84b73596":"\nLooking at the impacts of lockdown on calories burned. \nThe bar plot below shows that of the **380** days of data the country was in lockdown for **49%** of them.","1f82b126":"## **3. Data Understanding**\n### **3.1 Overview**\n\nThe downloaded data from Fitbit gave me 380 days of data.\nEach feature was a separate file which I merged before uploading to Kaggle. `Calories`, `Distance` & `Steps` were originally at a minute frequency but changed too daily. \n\n* `Calories` amount of calories burned that day\n* `Sedentary minutes`, basically the numbers of minutes you are sitting or lying down that day.\n* `Moderately active minutes` & `Very active minutes` on the Fitbit dashboard these are added together and referred to as Active minutes. To earn active minute's you have to go through 10 minutes or more of continuous moderate-to-intense activity.\n* `Lightly active minutes` the minutes between sedentary and moderately\/very active. \n* `Distance` the total distance travelled in cm.\n* `Steps` number of steps taken that day.","a2518267":"A slight improvement over the MLR model. Next, I tuned the hyperparameters for the RF model. ","cd0c5785":"The times were I burnt fewer calories nearly all occurred during the lockdown, 5 out of 7 days. While days I burnt the most calories generally happened on days, not in lockdown.   ","908426a9":"There are 41 days where total minutes equal 1,440 minutes. However 16 of these are days were I registered zero active minutes, distance travel and steps taken. So I replaced these days with NaNs. For the days were the total number of minutes total 1,440 minutes, I found the average proportion for each minute feature. Form the this we see that sedentary accounts for on average 91% of the 1,440 minutes each day. ","274baa22":"There are zero values present for `Distance`,`Steps` and the minute features seen below.","877bdfc7":"## **5. Modelling**\n\n\nSplit the data set, 80% train data and 20% test data.\nI used a multiple linear regression (MLR) model as a base model to compare the other two models two. ","71af8537":"The Fitbit data begins 9th January 2020. ","548373f4":"## **1. Introduction**\n\n\nThis project aims to develop a fitness dashboard with my KPIs using the data I collected on several fitness apps. I will follow the CRISP-DM approach for this project. \n\nI decided to post this notebook as a guide for anyone looking to do a similar project and for anyone with suggestions on how to improve it.","c8d207e7":"Next, I checked to see if the minute features totalled to 1,440 minutes. I added each minute feature together and subtracted from 1440. This new feature was called `Missing`. ","ba4bea26":"Rechecking the correlations again, things look better.  ","c8d39fda":"Next I will try a gradient boosting model","85f35caf":"### **3.2 Cleaning**\n\nChecking for missing values below. There are missing values for some of the minute features.","7ecaac3a":"Next to replaced the NaNs, I created features for the day of the week and lockdown status. ","ad9b7281":"Using the proportions found I assigned the unaccounted minutes from `Missing` to each of the minute features.  ","2f00cb83":"Our tuned model performs worse than the out of the box one. Below we can see the feature importance. It looks like the day of the week feature isn't that important and may want to drop it. ","b98fb44f":"The data ends 22nd January 2021"}}