{"cell_type":{"2dfae3a3":"code","46016884":"code","86e885d4":"code","7bbe43f3":"code","188a7a92":"code","6f691a38":"code","e378eb7a":"code","331958b2":"code","85779c49":"code","3eacb662":"code","57b56753":"code","89812286":"code","0715aa97":"code","0690cdfb":"code","8422089b":"code","d964787b":"code","92924425":"code","b4761465":"code","dde3f0e2":"code","d6bc35c5":"code","6b75b1c3":"code","79329738":"code","d9c6e410":"markdown","8caa9f8c":"markdown","aa724f84":"markdown"},"source":{"2dfae3a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside4of the current session","46016884":"from sklearn import preprocessing\nimport matplotlib.pyplot as plt \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\n\n%matplotlib inline","86e885d4":"df = pd.read_csv('..\/input\/summeranalytics2020\/train.csv')\ndf_test = pd.read_csv(\"..\/input\/summeranalytics2020\/test.csv\")\ndf.head()","7bbe43f3":"print(df['Attrition'].value_counts())\ndf['Behaviour'].value_counts()","188a7a92":"df = df.drop_duplicates(subset = [\"Age\", \"Attrition\", \"BusinessTravel\", \"Department\", \"DistanceFromHome\", \"Education\", \"EducationField\", \"EmployeeNumber\", \"EnvironmentSatisfaction\", \"Gender\", \"JobInvolvement\", \"JobRole\", \"JobSatisfaction\", \"MaritalStatus\", \"MonthlyIncome\", \"NumCompaniesWorked\", \"OverTime\", \"PercentSalaryHike\", \"PerformanceRating\", \"StockOptionLevel\", \"TotalWorkingYears\", \"TrainingTimesLastYear\", \"YearsAtCompany\", \"YearsInCurrentRole\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\", \"CommunicationSkill\"])","6f691a38":"# THE NEW SHAPE OF THE COLUMN\ndf.shape","e378eb7a":"# CHANGING CATEGORICAL DATA TO NUMERICAL DATA\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelencoder = LabelEncoder()\n\ndf['BusinessTravel'] = labelencoder.fit_transform(df['BusinessTravel'])\ndf['Department'] = labelencoder.fit_transform(df['Department'])\ndf['EducationField'] = labelencoder.fit_transform(df['EducationField'])\ndf['Gender'] = labelencoder.fit_transform(df['Gender'])\ndf['JobRole'] = labelencoder.fit_transform(df['JobRole'])\ndf['MaritalStatus'] = labelencoder.fit_transform(df['MaritalStatus'])\ndf['OverTime'] = labelencoder.fit_transform(df['OverTime'])\n\ndf_test['BusinessTravel'] = labelencoder.fit_transform(df_test['BusinessTravel'])\ndf_test['Department'] = labelencoder.fit_transform(df_test['Department'])\ndf_test['EducationField'] = labelencoder.fit_transform(df_test['EducationField'])\ndf_test['Gender'] = labelencoder.fit_transform(df_test['Gender'])\ndf_test['JobRole'] = labelencoder.fit_transform(df_test['JobRole'])\ndf_test['MaritalStatus'] = labelencoder.fit_transform(df_test['MaritalStatus'])\ndf_test['OverTime'] = labelencoder.fit_transform(df_test['OverTime'])","331958b2":"# DROPPING \"Behaviour\" COLUMN, CAUSE IT IS OF NO USE\ndf_test = df_test.drop([\"Behaviour\"], axis = 1)\ndf = df.drop([\"Behaviour\"], axis = 1)","85779c49":"# SEPARATING FEAURE AND TARGET COLUMN\nX =  df.loc[:, df.columns != 'Attrition']\ny = df.loc[:, df.columns == 'Attrition']","3eacb662":"# NOW LET'S SCALE THE DATA\nfrom sklearn.preprocessing import scale\nX = scale(X)\ndf_test_final = scale(df_test)","57b56753":"# WITH HEAT MAP WE CAN FIND RELATION SHIPS BETWEEN EACH COLUMNS\nplt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot=True, fmt='.0%')","89812286":"# LET'S HISTOGRAM FOR NUMERICAL DATA OF DATAFRAME\ndf.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)# ; avoid having the matplotlib verbose informations","0715aa97":"# SPLITTING THE DATA\nx_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state = 1)","0690cdfb":"# CHECKING THE SHAPE OF DATA\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_val.shape)","8422089b":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score","d964787b":"# PREDICTING WITH LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(solver='lbfgs', max_iter=1000, C=0.5, penalty='l2', random_state=1)\nlogreg_train = LogisticRegression(solver='lbfgs', max_iter=1000, C=0.5, penalty='l2', random_state=1)\nlogreg_full = LogisticRegression(solver='lbfgs', max_iter=1000, C=0.5, penalty='l2', random_state=1)\n\nlogreg_full = logreg_full.fit(X, y)\nlogreg_train = logreg_train.fit(x_train, y_train)\n\ny_pred_train = logreg_full.predict(x_train)\ny_pred_val = logreg_train.predict_proba(x_val) \ny_pred_val = y_pred_val[:,1]\n\ny_pred = logreg_full.predict_proba(df_test_final)\ndf_logreg_final = y_pred[:,1]\n\nprint(\"------logreg-------\")\nprint(\"accuracy\", accuracy_score(y_train, y_pred_train))\nprint(\"roc_auc\", roc_auc_score(y_train, y_pred_train))\nprint(\"roc_auc\", roc_auc_score(y_val, y_pred_val))","92924425":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(loss = \"deviance\", learning_rate = 1, n_estimators = 5500, criterion = \"friedman_mse\")\ngbc_train = GradientBoostingClassifier(loss = \"deviance\", learning_rate = 1, n_estimators = 5500, criterion = \"friedman_mse\")\ngbc_full = GradientBoostingClassifier(loss = \"deviance\", learning_rate = 1, n_estimators = 5500, criterion = \"friedman_mse\")\n\ngbc_train = gbc_train.fit(x_train, y_train) \ngbc_full = gbc_full.fit(X, y)\n\ny_pred_train = gbc_full.predict(x_train)\ny_pred_val = gbc_train.predict_proba(x_val) \ny_pred_val = y_pred_val[:, 1]\n\ny_pred = gbc_full.predict_proba(df_test_final)\ndf_gbc_final = y_pred[:,1]\n\nprint(\"------gbc-------\")\nprint(\"accuracy\", accuracy_score(y_train, y_pred_train))\nprint(\"roc_auc\", roc_auc_score(y_train, y_pred_train))\nprint(\"roc_auc\", roc_auc_score(y_val, y_pred_val))","b4761465":"from sklearn.svm import SVC\n\nsvc = SVC(C = 1, kernel = 'rbf',random_state = 42, probability = True)\nsvc_train = SVC(C = 1, kernel = 'rbf',random_state = 42, probability = True)\nsvc_full = SVC(C = 1, kernel = 'rbf',random_state = 42, probability = True)\n\nsvc_full = svc_full.fit(X, y)\nsvc_train = svc_train.fit(x_train, y_train)\n\ny_pred_train = svc_full.predict(x_train) \n\ny_pred_val = svc_train.predict_proba(x_val) \ny_pred_val = y_pred_val[:,1]\n\ny_pred = svc_full.predict_proba(df_test_final)\ndf_svc_final = y_pred[:,1]\n\nprint(\"accuracy of traing: \",accuracy_score(y_train, y_pred_train))\nprint(\"roc_auc of training: \", roc_auc_score(y_train, y_pred_train))\nprint(\"roc_auc of validation: \", roc_auc_score(y_val, y_pred_val))","dde3f0e2":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(hidden_layer_sizes=(4,2,2), activation='relu', solver='adam', max_iter=1000, alpha = 0.0007, random_state = 56, learning_rate = \"invscaling\")\nmlp_test = MLPClassifier(hidden_layer_sizes=(4,2,2), activation='relu', solver='adam', max_iter=1000, alpha = 0.0007, random_state = 56, learning_rate = \"invscaling\")\nmlp_full = MLPClassifier(hidden_layer_sizes=(4,2,2), activation='relu', solver='adam', max_iter=1000, alpha = 0.0007, random_state = 56, learning_rate = \"invscaling\")\n\nmlp_full = mlp_full.fit(X, y)\nmlp_test = mlp_test.fit(x_train, y_train)\n\ny_pred_train = mlp_full.predict(x_train) \n\ny_pred_val = mlp_test.predict_proba(x_val) \ny_pred_val = y_pred_val[:,1]\n\ny_pred = mlp_full.predict_proba(df_test_final)\ndf_svc_final = y_pred[:,1]\n\nprint(\"accuracy of traing: \",accuracy_score(y_train, y_pred_train))\nprint(\"roc_auc of training: \", roc_auc_score(y_train, y_pred_train))\nprint(\"roc_auc of validation: \", roc_auc_score(y_val, y_pred_val))","d6bc35c5":"from sklearn.ensemble import VotingClassifier \n\nestimator = [] \nestimator.append(('LR', logreg)) \nestimator.append(('SV', svc))\nestimator.append(('MLP', mlp))\nestimator.append(('NB', gbc))\n\nvot_test = VotingClassifier(estimators = estimator, voting ='soft') \nvot_test = vot_test.fit(x_train, y_train) \ny_pred_test = vot_test.predict_proba(x_val) \n\nvot_full = VotingClassifier(estimators = estimator, voting ='soft') \nvot_full = vot_full.fit(X, y) \ny_pred = vot_test.predict(x_train) \n\nprint(\"accuracy of traing: \",accuracy_score(y_train, y_pred))\nprint(\"roc_auc of training: \", roc_auc_score(y_train, y_pred))\nprint(\"roc_auc of validation: \", roc_auc_score(y_val, y_pred_test[:,1]))","6b75b1c3":"y_vc_pred = vot_full.predict_proba(df_test_final)\ny_vc_pred_final = y_vc_pred[:, 1]\nprint(len(y_vc_pred))","79329738":"my_submission = pd.DataFrame({'Id': df_test.Id, 'Attrition': y_vc_pred_final})\nmy_submission.to_csv('submission.csv', index=False)","d9c6e410":"*AS WE CAN SEE VOTING CLASSIFIER WORKS BEST HERE*","8caa9f8c":"****IMPLEMENTING EDA****","aa724f84":"**[](http:\/\/)**MODEL FITTING: Here we will check 4 models individually and together, see which one is better and use that model."}}