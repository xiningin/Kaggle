{"cell_type":{"3a32cbe7":"code","77677a74":"code","62733e08":"code","411f047a":"code","70528aeb":"code","15b26b9a":"code","b1cac65b":"code","1a88e4fe":"code","f63a3b4e":"code","df9710cd":"code","713dc189":"code","609931b3":"code","51d33dc0":"code","f90cfa7e":"code","6960a714":"code","4d53a0ba":"code","bb101ab1":"code","a3395375":"code","ce5b6aa5":"code","0add3b5b":"code","e98fde6e":"code","1c570307":"code","20bbb982":"code","f5dd13bb":"code","136bbcb0":"code","1f493c48":"code","ec772b5d":"code","73d9aa5e":"code","26b20c11":"code","2698248b":"code","d20028fb":"code","695a8e81":"code","51d49bde":"code","b856ef6e":"code","fe8aec24":"code","faf24314":"code","71a01e64":"code","b15092a5":"code","06f0431f":"code","b951dc59":"code","3e9fccca":"code","dbef32b2":"code","67373aa2":"code","b50cc2cd":"code","3ffd0e85":"code","b522f071":"code","4c09d853":"code","1bb0e277":"code","1c6c8c68":"code","a4598a2a":"code","1d6eec25":"code","c0940f29":"code","6271cd82":"code","8a872a59":"markdown","c082a13b":"markdown","2aed87f8":"markdown","55bd9471":"markdown","e268095d":"markdown","8a806ab8":"markdown","693aa254":"markdown","ee667e04":"markdown","d43c8f13":"markdown","6d55f508":"markdown","aa9444ef":"markdown","69623b5d":"markdown","5f1c2aab":"markdown","a986865a":"markdown","dceeb4b6":"markdown","bca81b61":"markdown","c115b4d5":"markdown","d118899f":"markdown","cdfa8c83":"markdown","a3c805d5":"markdown","efde0c51":"markdown","7ae744d5":"markdown","3fb8ba5a":"markdown","75444bac":"markdown","7504b50a":"markdown","6999e203":"markdown","6b7db591":"markdown","2060eb55":"markdown","6b007574":"markdown","b0ee5017":"markdown","90b553af":"markdown","6712b163":"markdown","26a72e8a":"markdown","8a0ea1db":"markdown","b25a44da":"markdown","556ec8c3":"markdown","490cdeef":"markdown","3fb8be82":"markdown","e4e672ab":"markdown","4eb85f47":"markdown"},"source":{"3a32cbe7":"import os\nimport warnings\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sea\nimport numpy as np\nimport math as mt\nimport scipy\n\nfrom sklearn.preprocessing import StandardScaler\nwarnings.filterwarnings(\"ignore\")","77677a74":"url = '..\/input\/cardiovascular-disease\/cardiovascular.txt'\ndata = pd.read_csv(url,sep=';',decimal=',')\n\n# let's separate index from other columns\ndata.index = data.iloc[:,0]\ndf = data.iloc[:,1:]\n\ndf = df.drop(['chd','famhist'],axis=1)\n","62733e08":"data.head()\n","411f047a":"data.shape","70528aeb":"df.head()","15b26b9a":"df.head()","b1cac65b":"df.dtypes","1a88e4fe":"df = df.astype('float')","f63a3b4e":"df.dtypes","df9710cd":"df.describe()","713dc189":"famhist_height = data.famhist.value_counts()\/data.shape[0]\n\nchd_height =data.chd.value_counts()\/data.shape[0]\n\nprint(famhist_height)\nprint(chd_height)\n\nfig = plt.figure(figsize=(17,4))\nax1 = fig.add_subplot(1, 2, 1)\nplt.bar(x=['Absent','Present'],height=famhist_height,color='yellow')\nax2 = fig.add_subplot(1, 2, 2)\nplt.bar(x=['No Crises crise','Crise'],height=chd_height,color='b')\nplt.show()","609931b3":"plt.figure(figsize=(16,5))\ndf.boxplot()\nplt.title(\"Distribution of the values \u200b\u200bof all potential predictors\")\nplt.show()","51d33dc0":"df=StandardScaler().fit_transform(df)\n\ndf=pd.DataFrame(df,columns=['sbp', 'tobacco', 'ldl', 'adiposity','obesity','alcohol', 'age','typea'])\n\ndf.index=data.index","f90cfa7e":"df.head()","6960a714":"df1=pd.concat([df,data.famhist,data.chd],axis=1)\n\n# and take a look \ndf1.head()","4d53a0ba":"plt.figure(figsize=(16,5))\nsea.boxplot(data=df1.iloc[:,:-2])\nplt.title(\"Distribution of the values \u200b\u200bof all potential standardized predictors\")\nplt.grid()\nplt.show()","bb101ab1":"R=round(df.corr(),ndigits=3)\nsea.heatmap(round(R,ndigits=2))\nplt.show()","a3395375":"n=df.shape[0]\np=df.shape[1]\n\nkhi2=-(n-1-(2*p+5)\/6)*mt.log(np.linalg.det(R)) # chi test\n\nddl=p*(p-1)\/2\n\np_valeur=scipy.stats.chi2.pdf(khi2,ddl)","ce5b6aa5":"print(p_valeur < 0.01)","0add3b5b":"inv_R=np.linalg.inv(R)\n\nA=np.zeros(shape=(inv_R.shape[0],inv_R.shape[1]))\nfor i in range(inv_R.shape[0]):\n    for j in range(i+1,inv_R.shape[1]):\n        A[i,j]= -inv_R[i,j]\/np.sqrt(inv_R[i,i]*inv_R[j,j])\n        A[j,i]=A[i,j]\n        \nR=R.values\n\nkmo_numerateur= np.sum(R**2)-np.sum(np.diag(R**2))\nkmo_denominateur=kmo_numerateur + (np.sum(A**2)-np.sum(np.diag(A**2)))\nkmo=kmo_numerateur\/kmo_denominateur\n\nprint(\"kmo :\",round(kmo,ndigits=2))","e98fde6e":"R=round(df.corr(method='spearman'),ndigits=3)\nsea.heatmap(round(R,ndigits=2))\nplt.show()","1c570307":"R=round(df.corr(method='kendall'),ndigits=3)\nsea.heatmap(round(R,ndigits=2))\nplt.show()","20bbb982":"#rank the group of records that have the same value\ndf=df.rank()","f5dd13bb":"from sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\n","136bbcb0":"# let's split our data into I\/O\nX=df.iloc[:,:-1] ; y=df1['chd']\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=23) ","1f493c48":"print( X_train.shape)","ec772b5d":"print(X_test.shape)","73d9aa5e":"pca=PCA(n_components=5) # split in 5 components\n\nprincipalComponents = pca.fit_transform(X_train)\n\nfactors_Df = pd.DataFrame(data = principalComponents, columns =['PC1','PC2','PC3','PC4','PC5'])\n\nfactors_Df.index=X_train.index","26b20c11":"# take a look\nfactors_Df.head()","2698248b":"# let's summary \nprint(pca.explained_variance_ratio_)\nprint(pca.explained_variance_ratio_.cumsum())\nprint(pca.explained_variance_ratio_.sum())","d20028fb":"fig = plt.figure(figsize=(14,6))\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2)\n\nax1.plot(np.arange(1,6),pca.explained_variance_ratio_,color='b',marker='o')\nax2.plot(np.arange(1,6),np.cumsum(pca.explained_variance_ratio_),color='gold',marker='o')\n\nax1.set_xlabel('Factors')\nax1.set_title('Explained variance')\nax2.set_xlabel('Factors')\nax2.set_title('Explained variance cumsum')\n\nplt.show()","695a8e81":"var_fig=plt.figure(figsize=(9,9)) ; pcs = pca.components_\n\nfor i, (x, y) in enumerate(zip(pcs[0, :], pcs[1, :])):\n    # show the segment of the origin on point (x, y)\n    plt.plot([0, x], [0, y], color='green')\n    # show the composition\n    plt.text(x, y,df[['sbp','tobacco','ldl','adiposity','obesity','alcohol','age','typea']].columns[i], fontsize='14')\n\nplt.plot([-0.8, 0.8], [0, 0], color='grey', ls='--') # horizontal line y=0    \nplt.plot([0, 0], [-0.8, 0.8], color='grey', ls='--') # vertical line x=0\nplt.title('Var Projections', fontsize=19)\ncercle = plt.Circle((0.515,0.50),0.377,color='k',fill=False)\nvar_fig.add_artist(cercle)\nplt.grid()","51d49bde":"names=pd.DataFrame(data=data.chd,columns=['chd'])\n\nfinal_factors_Df = pd.concat([factors_Df, data['chd']], axis = 1)\nid_factors=pd.concat([names,final_factors_Df],axis=1)\nid_factors=id_factors.sort_values(by=['PC1'])\nfinal_factors_Df=final_factors_Df.sort_values(by=['PC1'])","b856ef6e":"fig = plt.figure(figsize = (16,5))\nax = fig.add_subplot(1,1,1)\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('Best Plan-1 (Axis 1 & 2)', fontsize = 20)\ntargets = [0,1]\ncolors = ['yellow','red']\nfor target, color in zip(targets,colors):\n    indicesToKeep = final_factors_Df['chd'] == target\n    ax.scatter(final_factors_Df.loc[indicesToKeep, 'PC1'],final_factors_Df.loc[indicesToKeep, 'PC2'], c = color, s = 50)\n    \nax.legend(targets)\nax.grid()\nplt.show()","fe8aec24":"fig = plt.figure(figsize = (16,5))\nax = fig.add_subplot(1,1,1)\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 3', fontsize = 15)\nax.set_title('Best Plan-2 (Axis 1 & 3)', fontsize = 20)\ntargets = [0,1]\ncolors = ['yellow','red']\nfor target, color in zip(targets,colors):\n    indicesToKeep = final_factors_Df['chd'] == target\n    ax.scatter(final_factors_Df.loc[indicesToKeep, 'PC1'], final_factors_Df.loc[indicesToKeep, 'PC3'], c = color, s = 50)\n    \nax.legend(targets)\nax.grid()","faf24314":"X_supp=X_test ; y_supp=y_test\ncoordSupp=pca.transform(X_supp)","71a01e64":"coordSupp.shape","b15092a5":"fig = plt.figure(figsize = (16,5))\nax = fig.add_subplot(1,1,1)\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('Test sample on best Plan-2 (Axis 1 & 2)', fontsize = 20)\ndf2=pd.DataFrame(data=coordSupp,index=X_supp.index,columns=final_factors_Df.columns[0:5])\ndf0=df1.loc[X_supp.index]['chd']\ngdf=pd.concat([df2,df0],axis=1)\ntargets = [0,1]\ncolors = ['green','red']\nfor target, color in zip(targets,colors):\n    indicesToKeep = gdf['chd'] == target\n    ax.scatter(gdf.loc[indicesToKeep, 'PC1'], gdf.loc[indicesToKeep, 'PC2'],c= color, s = 50,marker='s')\n    ax.legend(targets)\n\nax.grid()\nplt.show()","06f0431f":"fig = plt.figure(figsize = (16,5))\nax = fig.add_subplot(1,1,1)\nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 3', fontsize = 15)\nax.set_title('Test sample on second best Plan-2 (Axis 1 & 3)', fontsize = 20)\ndf2=pd.DataFrame(data=coordSupp,index=X_supp.index,columns=final_factors_Df.columns[0:5])\ndf0=df1.loc[X_supp.index]['chd']\ngdf=pd.concat([df2,df0],axis=1)\ntargets = [0,1]\ncolors = ['green','red']\nfor target, color in zip(targets,colors):\n    indicesToKeep = gdf['chd'] == target\n    ax.scatter(gdf.loc[indicesToKeep, 'PC1'], gdf.loc[indicesToKeep, 'PC3'] , c= color, s = 50,marker='s')\n\nax.legend(targets)\nax.grid()\nplt.show()","b951dc59":"X_train=pd.concat([data.loc[factors_Df.index]['famhist'],factors_Df],axis=1)\nX_train.head()","3e9fccca":"test_DF=pd.DataFrame(coordSupp,columns=['PC1','PC2','PC3','PC4','PC5'],index=X_test.index)\nX_test=pd.concat([data.loc[X_test.index]['famhist'],test_DF],axis=1)\n\nX_test.head()","dbef32b2":"## import Libraries\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc,classification_report\nfrom sklearn.pipeline import Pipeline","67373aa2":"pred=[]\nX_train=factors_Df.iloc[:,1:4]\nparam_grid={'bootstrap':[True], 'max_depth':[110,130,150,170],'min_samples_leaf':[7,9,11,13],'min_samples_split':[6,8,12],'n_estimators':[10,15,20,25]}\ngs=GridSearchCV(estimator=RandomForestClassifier(),param_grid=param_grid,cv=5)\n\ngs.fit(X_train,y_train)\nbest_rf=gs.best_estimator_\ny_true,y_pred=y_supp,best_rf.predict(X_test.iloc[:,1:4])\n\ndf=pd.DataFrame(classification_report(y_true,y_pred,output_dict=True))\nsea.heatmap(df.iloc[:-1, :].T, annot=True)","b50cc2cd":"data=data.drop(['typea'],axis=1)\ndata.head()","3ffd0e85":"X_train=data.loc[X_train.index]; X_test=data.loc[X_test.index]\nX_train['famhist'].replace({\"Absent\":0,\"Present\":1},inplace=True)\nX_test['famhist'].replace({\"Absent\":0,\"Present\":1},inplace=True)\n\ny=df1['chd']; y_train=y.loc[y_train.index]; y_test=y[y_test.index]\n \n\nparam_grid={'bootstrap':[True], 'max_depth':[110,130,150,170],'min_samples_leaf':[7,9,11,13],'min_samples_split':[8,12,14],'n_estimators':[10,15,20,25]}\ngs=GridSearchCV(estimator=RandomForestClassifier(),param_grid=param_grid,cv=5)\ngs.fit(X_train,y_train)\nbest_rf_=gs.best_estimator_\ny_true,y_pred=y_supp,best_rf_.predict(X_test)\npred.append(y_pred)","b522f071":"df=pd.DataFrame(classification_report(y_true,y_pred,output_dict=True))\nsea.heatmap(df.iloc[:-1, :].T, annot=True)","4c09d853":"X_train=factors_Df.iloc[:,1:4]\nSVCpipe = Pipeline([('SVC',LinearSVC())])\nparam_grid = {'SVC__C':np.arange(0.01,100,10)}\nlinearSVC = GridSearchCV(SVCpipe,param_grid,cv=5,return_train_score=True)\nlinearSVC.fit(X_train,y_train)\nbest_svc=linearSVC.best_estimator_\ny_true,y_pred=y_supp,best_svc.predict(X_test.iloc[:,1:4])","1bb0e277":"df=pd.DataFrame(classification_report(y_true,y_pred,output_dict=True))\nsea.heatmap(df.iloc[:-1, :].T, annot=True)","1c6c8c68":"X_train=data.loc[X_train.index]; X_test=data.loc[X_test.index]\ny=df1['chd']; y_train=y.loc[y_train.index]; y_test=y[y_test.index]\nX_train['famhist'].replace({\"Absent\":0,\"Present\":1},inplace=True)\nX_test['famhist'].replace({\"Absent\":0,\"Present\":1},inplace=True)","a4598a2a":"param_grid = {'SVC__C':np.arange(0.01,100,10)}\nlinearSVC = GridSearchCV(SVCpipe,param_grid,cv=5,return_train_score=True)\nlinearSVC.fit(X_train,y_train)\nbest_svc=linearSVC.best_estimator_\ny_true,y_pred=y_supp,best_svc.predict(X_test)\npred.append(y_pred)","1d6eec25":"df=pd.DataFrame(classification_report(y_true,y_pred,output_dict=True))\nsea.heatmap(df.iloc[:-1, :].T, annot=True)","c0940f29":"s=pred[0]+pred[1]\npred_finale=[]\nfor i in range(len(s)):\n    if (s[i] >= 1):\n        pred_finale.append(1)\n    else:\n        pred_finale.append(0)\npred_finale=pd.DataFrame(pred_finale)","6271cd82":"df=pd.DataFrame(classification_report(y_true,y_pred,output_dict=True))\nsea.heatmap(df.iloc[:-1, :].T, annot=True)","8a872a59":"## Bartlett's sphericity test","c082a13b":"## Kaiser\u2019s Measure of Sampling Adequacy","2aed87f8":"## I hope you find this kernel useful and enjoyable.\n## Your comments and feedback are most welcome.\n\n## Upvote if you liked it and found useful.","55bd9471":"### Correlation :","e268095d":"In this part, a majority vote is taken. A performance check of this\nclassification system of the two best models is subsequently carried out","8a806ab8":"## Dimensionality Reduction","693aa254":"####  The binary variable \u2018famhist\u2019 is added to the training set","ee667e04":"## 2. Support Vector Machine\n### 2.1 Principal Component","d43c8f13":"In this part, we'll train 2 classifiers SVM and  RF .\nFirst on main components, then on the initial data and we compare their performance\nto assess the contribution (if any) of the reduction in size in terms of decreasing the\nnoise in the construction of predictive models.","6d55f508":"## Loading Dataset\nwe will use the cardiovascular dataset hosted in kaggle ","aa9444ef":"## Projection of the test sample","69623b5d":"\nThe observation of the coefficients of the correlation matrix above suggests that:\n\n- Age is correlated with tobacco consumption and level of adiposity;\n- Obesity is strongly correlated with adiposity;\n- Idl is correlated with adiposity.\n- In short, older and obese subjects tend to have more fat accumulated under\n  the skin.","5f1c2aab":"looks better with standardization :) !!","a986865a":"The shared inertia explained by the first 5 components is around 90%","dceeb4b6":"## Visualization of projections variables","bca81b61":"check the p-value","c115b4d5":"We need to convert object types to float","d118899f":"## Data Description","cdfa8c83":"## Projections of patients from the learning base","a3c805d5":"The Purpose of this document is to show empirically the potential interest of the\ndimensionality Reduction in Machine Learning with Ensemble Methods algorithms.","efde0c51":"#### The binary variable \u2018famhist\u2019 is added to the test set","7ae744d5":"## Import Libraries","3fb8ba5a":"## Visualization of the explained variance","75444bac":"### Correlation matrix (Kendall)","7504b50a":"## Predictive analysis","6999e203":"Conclusion: With dimensionality reduction, we don't necessarily get better\nresults only with the dataset. It should be noted that a rotation of the factorial axis would probably\nimproved results","6b7db591":"## 2. Support Vector Machine\n### 2.2 Initial Data","2060eb55":"## Overview","6b007574":"## 1. Random Forest\n### 1.2 Initial Data","b0ee5017":"Since the kmo index is between 0.6 and 0.7,\na compression relevant index can be obtained.\nSince some of the values \u200b\u200bseemed to remain extreme, we are trying a less sensitive approach to them","90b553af":"Let's Add the family history and the target to our dataframe","6712b163":"## 1. Random Forest\n### 1.1 Principal component","26a72e8a":"### Principal Component Analysis (PCA)","8a0ea1db":"We observe a large Scale difference between variables. \nWe need to standardize them to avoid  those with large scales wrongly have too much weight in the calculations.\nRegarding abberant values, their effect should be reduced by methods that are not very sensitive to them.","b25a44da":"A non-parametric PCA based on the Spearman correlation matrix is \u200b\u200bperformed in\ndue to the presence of many atypical values","556ec8c3":"Conclusion of the sphericity test : \n\nHypothesis : Othogonality of the variables\n\nSince the p-value is less than the chosen threshold, we reject the null hypothesis of orthogonality of\nvariables.\n\nso : PCA is therefore relevant within the meaning of this test.\n","490cdeef":"Let's take a look for our data","3fb8be82":"## Data Description : \n\n1. sbp: systolic blood pressure\n2. tobacco: cumulative tobacco (kg)\n3. ldl: low densiity lipoprotein cholesterol\n4. adiposity\n5. famhist: family history of heart disease (Present, Absent)\n6. typea: type-A behavior\n7. obesity\n8. alcohol: current alcohol consumption\n9. age: age at onset\n10. chd: coronary heart disease\n\nyou can take more information about the dataset [here](https:\/\/www.kaggle.com\/yassinehamdaoui1\/cardiovascular-disease)","e4e672ab":"\n### Correlation matrix (Spearman)","4eb85f47":"## Interpretation"}}