{"cell_type":{"27a5869f":"code","017e9713":"code","0f01c335":"code","1b635d2b":"code","774260a2":"code","65bc2c76":"code","e9b993e6":"code","43a6914b":"code","544147b9":"code","6caf927e":"code","eac9cd04":"code","8f8da8c8":"code","3153005d":"code","3d051eef":"code","05393183":"code","84423958":"code","a46eaf59":"code","51c971fc":"code","99cfb535":"code","a1be7352":"code","7baebaa8":"code","5728b4b3":"code","b74b39c1":"code","fa946f4a":"code","1b72ec7c":"code","109e6a20":"code","8a14e2fd":"code","d64f4f6a":"code","bd740ed4":"code","d886ffaf":"code","c87cebf5":"code","843dc5ce":"code","a78c064d":"code","f75e3a58":"code","7964320b":"code","cfdbe809":"code","3886a6a8":"code","4bd66b6e":"code","bdbaf8d3":"code","e3374d35":"code","2634170c":"code","64584d16":"code","0135a421":"markdown","6a87c16a":"markdown","ce324453":"markdown","1d6f18c1":"markdown","0147f1f3":"markdown","ac75ca05":"markdown","241863b4":"markdown","a87b6d1e":"markdown","321c91e6":"markdown","8a5036a1":"markdown","c5e954f4":"markdown","049ef19e":"markdown","f4499d97":"markdown","87243cbb":"markdown","48e1605d":"markdown","f9d7d811":"markdown","67720f99":"markdown","21050879":"markdown","4f2562ac":"markdown","fc32a5cd":"markdown","2ea12a55":"markdown","68f99556":"markdown","8ca7ed2c":"markdown","a3dad313":"markdown","b3ebfa13":"markdown","504cd489":"markdown","81b4e833":"markdown","98cf38fe":"markdown","0547b796":"markdown","b44cfad0":"markdown","308b7c18":"markdown","5cb9cbad":"markdown","542bc603":"markdown","690ce63d":"markdown","148779ae":"markdown","2738bfbe":"markdown","83781475":"markdown","d8b7ed40":"markdown","db92c841":"markdown","56415837":"markdown","35c3f751":"markdown","4f6af06b":"markdown","f2bbb3e1":"markdown","c582f940":"markdown"},"source":{"27a5869f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","017e9713":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","0f01c335":"dataset = pd.read_csv ('..\/input\/breast-cancer-wisconsin-data\/data.csv')","1b635d2b":"dataset.shape","774260a2":"dataset.columns","65bc2c76":"dataset.info ()","e9b993e6":"dataset.head ()","43a6914b":"dataset.drop ('id', axis = 1, inplace = True)\ndataset.drop ('Unnamed: 32', axis = 1, inplace = True)","544147b9":"dataset.shape ","6caf927e":"dataset.head ()","eac9cd04":"dataset.isna ()","8f8da8c8":"dict = {}\nfor i in list(dataset.columns):\n    dict[i] = dataset[i].value_counts().shape[0]\n\npd.DataFrame(dict,index=[\"unique count\"]).transpose()","3153005d":"from sklearn.preprocessing import LabelEncoder\nlabelencoder_Y = LabelEncoder()\ndataset.diagnosis = labelencoder_Y.fit_transform(dataset.diagnosis)\ndataset.head (10)","3d051eef":"df = pd.DataFrame (dataset, columns = ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',\t'perimeter_se',\t'area_se',\t'smoothness_se',\t'compactness_se',\t'concavity_se',\t'concave points_se',\t'symmetry_se',\t'fractal_dimension_se',\t'radius_worst',\t'texture_worst',\t'perimeter_worst',\t'area_worst',\t'smoothness_worst',\t'compactness_worst',\t'concavity_worst',\t'concave points_worst',\t'symmetry_worst',\t'fractal_dimension_worst'])\ndf.corr ()","05393183":"corr_Matrix = df.corr ()\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap (corr_Matrix, linewidths = 0.5, annot = True, fmt= '.1f',ax=ax)\nplt.show ()","84423958":"label = []\nfor i in range (30):\n  if corr_Matrix.diagnosis[i+1]<0.5 or i>=10 :\n    label.append (dataset.columns.values[i+1])\ndataset.drop (labels = label, axis = 1, inplace = True)\ndataset.head ()","a46eaf59":"sns.pairplot(dataset, hue = \"diagnosis\")\nplt.show()","51c971fc":"sns.countplot (x = 'diagnosis',data = dataset)\nplt.show ()","99cfb535":"# X = Matrix of Features\n# Y = Response Feature\n\nX = dataset.iloc [:, 1:].values\nY = dataset.iloc [:, 0].values\nX.shape","a1be7352":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size = 0.25, random_state = 1)","7baebaa8":"X_train.shape","5728b4b3":"Y_train.shape","b74b39c1":"X_test.shape","fa946f4a":"Y_test.shape","1b72ec7c":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler ()\nX_train = sc.fit_transform (X_train)\nX_test = sc.transform (X_test)","109e6a20":"print (X_train [:5, :])","8a14e2fd":"print (X_test [:5, :])","d64f4f6a":"from sklearn.metrics import confusion_matrix, accuracy_score","bd740ed4":"from sklearn.linear_model import LogisticRegression \nclassifier_log = LogisticRegression ()\nclassifier_log.fit (X_train, Y_train)\nY_pred_log = classifier_log.predict (X_test)\ncm_log = confusion_matrix (Y_test, Y_pred_log)\nacc_log = accuracy_score (Y_test, Y_pred_log)","d886ffaf":"from sklearn.neighbors import KNeighborsClassifier\nclassifier_knn = KNeighborsClassifier ()\nclassifier_knn.fit (X_train, Y_train)\nY_pred_knn = classifier_knn.predict (X_test)\ncm_knn = confusion_matrix (Y_test, Y_pred_knn)\nacc_knn = accuracy_score (Y_test, Y_pred_knn)","c87cebf5":"from sklearn.naive_bayes import GaussianNB\nclassifier_nb = GaussianNB ()\nclassifier_nb.fit (X_train, Y_train)\nY_pred_nb = classifier_nb.predict (X_test)\ncm_nb = confusion_matrix (Y_test, Y_pred_nb)\nacc_nb = accuracy_score (Y_test, Y_pred_nb)","843dc5ce":"from sklearn.svm import SVC\nclassifier_svm = SVC (kernel = 'rbf', random_state = 0)\nclassifier_svm.fit (X_train, Y_train)\nY_pred_svm = classifier_svm.predict (X_test)\ncm_svm = confusion_matrix (Y_test, Y_pred_svm)\nacc_svm = accuracy_score (Y_test, Y_pred_svm)","a78c064d":"from sklearn.tree import DecisionTreeClassifier\nclassifier_dtc = DecisionTreeClassifier (criterion = 'entropy', random_state = 0)\nclassifier_dtc.fit (X_train, Y_train)\nY_pred_dtc = classifier_dtc.predict (X_test)\ncm_dtc = confusion_matrix (Y_test, Y_pred_dtc)\nacc_dtc = accuracy_score (Y_test, Y_pred_dtc)","f75e3a58":"from sklearn.ensemble import RandomForestClassifier\nclassifier_rfc = RandomForestClassifier (n_estimators = 100, criterion = 'entropy', random_state = 1)\nclassifier_rfc.fit (X_train, Y_train)\nY_pred_rfc = classifier_rfc.predict (X_test)\ncm_rfc = confusion_matrix (Y_test, Y_pred_rfc)\nacc_rfc = accuracy_score (Y_test, Y_pred_rfc)","7964320b":"prediction_columns = [\"NAME OF MODEL\", \"ACCURACY SCORE\"]\ndf_pred = {\"NAME OF MODEL\" : [\"LOGISTIC REGRESSION\", \"K-NN\", \"NAIVE BAYES\", \"SVM\", \"DECISION TREE\", \"RANDOM FOREST\"],\n           \"ACCURACY SCORE \" : [acc_log, acc_knn, acc_nb, acc_svm, acc_dtc, acc_rfc]}\ndf_predictions = pd.DataFrame (df_pred)\ndf_predictions","cfdbe809":"from sklearn.model_selection import GridSearchCV","3886a6a8":"parameters = [{'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}]\ngrid_search = GridSearchCV(estimator = classifier_log,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_log = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_accuracy_log)\nprint(best_parameters)","4bd66b6e":"parameters = [{'n_neighbors': [3,5,7,10,13,15], 'weights': ['uniform', 'distance'],\n                'p': [1,2]}]\ngrid_search = GridSearchCV(estimator = classifier_knn,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_knn = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_accuracy_knn)\nprint(best_parameters)","bdbaf8d3":"parameters = [{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'rbf'],\n                'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = classifier_svm,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_svm = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_accuracy_svm)\nprint(best_parameters)","e3374d35":"parameters = [{'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150], \n                'max_leaf_nodes': [2,4,6,10,15,30,40,50,100], 'min_samples_split': [2, 3, 4]}]\ngrid_search = GridSearchCV(estimator = classifier_dtc,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_dtc = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_accuracy_dtc)\nprint(best_parameters)","2634170c":"parameters = [{'n_estimators': [100,200,300],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [10,25,50,'none'],\n               'min_samples_leaf': [1, 2], \n               'min_samples_split': [2, 5]}]\ngrid_search = GridSearchCV(estimator = classifier_rfc,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)\nbest_accuracy_rfc = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(best_accuracy_rfc)\nprint(best_parameters)","64584d16":"prediction_columns = [\"NAME OF MODEL\", \"ACCURACY SCORE\", \"BEST ACCURACY (AFTER HYPER-PARAMETER TUNING)\"]\ndf_pred = {\"NAME OF MODEL\" : [\"LOGISTIC REGRESSION\", \"K-NN\", \"NAIVE BAYES\", \"SVM\", \"DECISION TREE\", \"RANDOM FOREST\"],\n           \"ACCURACY SCORE \" : [acc_log, acc_knn, acc_nb, acc_svm, acc_dtc, acc_rfc],\n           \"BEST ACCURACY (AFTER HYPER-PARAMETER TUNING)\" : [best_accuracy_log, best_accuracy_knn, \"-\", best_accuracy_svm, best_accuracy_dtc, best_accuracy_rfc]}\ndf_predictions = pd.DataFrame (df_pred)\ndf_predictions","0135a421":"Since all the entries of the **isna ()** function are **false**, we can conclude that there is no missing data in the dataset.","6a87c16a":"### SPLITTING THE MATRIX OF FEATURES AND RESPONSE FEATURE INTO TRAINING AND TEST SETS\n\nAs the names suggest, the model algorithms are trained using the **TRAINING SET** and then the model algorithms apply their learnings from training onto the **TEST SET** to get the predicted values which are then compared to the actual values to get the accuracy.\n","ce324453":"### LOGISTIC REGRESSION MODEL","1d6f18c1":"# IMPORTING THE LIBRARIES AND DATASET","0147f1f3":"### DECISION TREE MODEL","ac75ca05":"From the table above it is fairly evident that the **SUPPORT VECTOR MACHINE** has the highest accuracy score of **0.923077 (92.30%)** for our dataset.","241863b4":"In the above plots, **0** corresponds to **BENIGN** and **1** corresponds to **MALIGNANT**.\n\nThe visuals show a trend that has been followed throughout each plot. At lower values of the features, the diagnosis is predominantly **BENIGN** and at higher values, **MALIGNANT** has been the chief diagnosis. \n","a87b6d1e":"### DATASET","321c91e6":"# CONCLUSION","8a5036a1":"### CORRELATION MATRIX AND HEATMAP\n\nThe **Correlation Matrix** as the name suggests is a matrix which shows us how each feature variable of the dataset is co-related to each other.\n\nWe use the **Heatmap** as a visually pleasing way to show the relationships between features.","c5e954f4":"# EXPLORATORY ANALYSIS OF DATASET\n\nIn this section, we will be performing some basic operations on the dataset in order to analyse the data as a s]whole. For example, we will checking out the size of the dataset, what are the different features, what are the input types of the features, etc. to name a few.","049ef19e":"### FORMING FINAL WORKING DATASET\n\nNow that we have identified the key features that will play the major role while making predictions, we are going to drop the rest of the features from the dataset.","f4499d97":"### DECISION TREE MODEL","87243cbb":"### RANDOM FOREST MODEL","48e1605d":"### PAIRPLOT VISUALS\n\nNext is a visual representation of how the remaining features apart from the response feature **DIAGNOSIS** are related to each other.","f9d7d811":"### RANDOM FOREST MODEL","67720f99":"The dataset now contains only those features which will be playing an important role in the classification of the type of the tumor and thus we will be using only these to train (and test) our model.","21050879":"# DATA PREPROCESSING\n\nThis section involves transforming the raw data that we have into a more understandable format for the algorithm to process. This is done so that the data which we will be feeding into the algorithm is not garbage and we don't get false predictions in return. This includes techniques like scaling of features, encoding of data, splitting the data into training and test sets, etc.","4f2562ac":"### ENCODING THE CATEGORICAL VARIABLE\n\nTo ensure that the entire dataset is of a continuous numerical form, we will be encoding the categorial variable **DIAGNOSIS** and converting into a numerical form, preferably into 0s and 1s.\n\nFor this, we will be making use of the **LabelEncoder** class from the **Preprocessing** module of the **Sklearn** library","fc32a5cd":"From the above table, it is clearly visible that the **DIAGNOSIS** feature is taking 0s and 1s as values.\n\n0 --> **Benign**\n\n1 --> **Malignant**","2ea12a55":"### FEATURE SCALING\n\nIn any dataset, there could be features that dominate over others while evaluating the accuracy. We don't want that. We want all features to have a more or less equal say in deciding the accuracy. Also if a feature in the dataset is big in scale compared to others then in algorithms where Euclidean Distance is measured this big scaled feature becomes dominating and needs to be normalized.\n\nFor this we'll be using one of the most used feature scaling method there is, **STANDARD SCALER**. This method assumes your data to be normally distributed within each feature and scales them in such a way that the distribution becomes centred around **0** with a standard deviation of **1**.","68f99556":"### SPLITTING DATASET INTO DEPENDENT AND INDEPENDENT VARIABLES\n\nNow finally we will be splitting the updated dataset we have into two parts. The first is a collection of the independent variables and is called the **MATRIX OF FEATURES**. The other is a collection of the dependent variables and is known as **RESPONSE FEATURE**.","8ca7ed2c":"### LOGISTIC REGRESSION MODEL","a3dad313":"### COUNTPLOT VISUAL\n\nThe count plot will give us a more clearer picture regarding the actual number of data points for each diagnosis.\n\nIn the plot below, **0** corresponds to **BENIGN** and **1** corresponds to **MALIGNANT**.","b3ebfa13":"# HYPER-PARAMETER TUNING\n\nWe wil now try to tune our model algorithms and see whether is it possible for us to achieve any increase in the accuracy  scores by making any changes in the parameter values. The technique that we will be using provides us with the optimum parameter values using which we can get the maximum accuracy possible. Also, there is the possibilty that a new model is found to have the highest accuracy after the paramter tuning is done.","504cd489":"### K-NN MODEL","81b4e833":"From the count plot, it can be easily inferred that there are more **BENIGN** diagnosed data points than **MALIGNANT** diagnosed data points.","98cf38fe":"### REMOVING UNNEEDED FEATURES\n\nThe columns **id** and **Unnamed: 32** don't play any role in the prediction and hence we can drop them from the dataset.","0547b796":"### CHECKING THE NUMBER OF UNIQUE VALUES\n\nNext we will be checking how many unique values does each feature have, in order to get a much better understanding of the dataset we are working on.","b44cfad0":"### NAIVE BAYES MODEL","308b7c18":"# MODEL IMPLEMENTATIONS\n\nIn this section we will be building a model using the different classification algorithms that we have like K-NN, Logistic Regression, Naive Bayes, etc. and calculating the model accuracy for each algorithm to see which would be best suited for our dataset.\n\nAlso, we will be optimizing our model using methods which could result in a new classification algorithm having the best accuracy.","5cb9cbad":"# BASIC VISUALISATION OF DATASET\n\nAfter doing a theoretical analysis in the previous section, we will be moving on to visual analysis of the dataset. This will include a number of pair plots and a heat map between the different features and how they affect each other and how they will affect our algorithm. We will also be getting a general idea about which features will play a more active role while determining the accuracy of the model.","542bc603":"### LIBRARIES","690ce63d":"### NAIVE BAYES MODEL\n\nThe naive bayes algorithm doesn't have any hyper-parameter to tune, so we have nothing to perform grid search over.","148779ae":"### SVM MODEL","2738bfbe":"### CHECKING FOR MISSING DATA\n\nNext we need to check for any missing data that might be present in the dataset. For this, we will be using the **isna ()** function of the **Pandas** library","83781475":"From the above heatmap we can infer a few things :-\n\n1. The **\"_se\"** features are very weakly co-related (ranging from 0.0 to 0.5) with the response variable **DIAGNOSIS** and we won't be considering them for our final working dataset.\n\n2. The **\"_mean\"** and **\"_worst\"** features apart from being very strongly co-related to the response variable **DIAGNOSIS** are also very strongly co-related to their corresponding selves. For example, **\"radius_mean\"** has a correlation of 1.0 with **\"radius_worst\"**. This implies that we don't need to consider both **\"_mean\"** and **\"_worst\"** features together and we can make use of either one of the sets. For this notebook, we will be making use of the **\"_mean\"** features.\n\nFrom the **\"_mean\"** features we will be selecting only those which have a correlation of 0.5 and above with the response variable **DIAGNOSIS**.","d8b7ed40":"From the result of the above function we can see that we have only 1 categorical data feature and the rest are continuous data features. ","db92c841":"# FINAL ACCURACIES AFTER HYPER-PARAMETER TUNING","56415837":"### K-NN MODEL","35c3f751":"To conclude this notebook, it is fairly evident that it is the **SUPPORT VECTOR MACHINE** model that has come out triumphant with the highest accuracies both before and after the hyper-parameter tuning. It ended up with an accuracy of **0.923077 (92.37%)** before hyper-parameter tuning and **0.936434 (93.64%)** after and is hence, the best suited model out of the rest for the given dataset.","4f6af06b":"### SVM MODEL","f2bbb3e1":"### ACCURACY COMPARISON","c582f940":"# ABOUT DATASET\n\nAny tumour (abnormal growth of cells) in the human body can be broadly classified into two types - **Benign** (non-cancerous cell growth) and **Malignant** (cancerous cell growth). This dataset is a collection of all those patients whose bodies were examined to have a tumour and have been classified to be either \"Benign\" or \"Malignant\" on the basis of a collection of features specific to the cell growth like radius of cells, surface area of growth, etc.\n\n# ABOUT NOTEBOOK\n\nIn this notebook I have tried to create an algorithm to achieve the best possible accuracy while predicting the nature of the tumour present. For this, I will be comparing the accuracy of the different classification algorithms and then performing hyper-parameter tuning to achieve the best set of parameters giving the best accuracy for the models."}}