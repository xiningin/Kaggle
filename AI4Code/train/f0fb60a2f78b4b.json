{"cell_type":{"3d4bba55":"code","e555a9bc":"code","1bcb5bea":"code","9ad465b0":"code","2de0f491":"code","762f2989":"code","e694b849":"code","d306f29f":"code","2837a880":"code","c2aa640a":"code","9392c2dd":"code","53f732a5":"code","fbc4d394":"code","4fbc010b":"code","8f676bce":"code","2c654e0c":"code","0852222b":"code","7f3ecfb1":"code","219bafa6":"code","7681222b":"code","57529936":"code","d1139108":"code","e74652fc":"code","569f3717":"code","c3036358":"code","e83c2902":"code","5c921024":"code","4e07d2d4":"code","75d756cf":"code","6fced4d2":"code","8f24892a":"code","f1142f99":"code","3bf3e674":"code","fd689c7c":"code","c8bf1fe9":"code","b5adbd57":"code","e342fd02":"code","fd9f0430":"code","6a1a1024":"code","3b84c161":"code","06cce8df":"code","c9b232f5":"code","f5966302":"code","082a8427":"code","cbf5355e":"markdown","66ccfeea":"markdown","b8990aeb":"markdown","9a78ceed":"markdown","06320b6a":"markdown","26510a4a":"markdown","0e81e43a":"markdown","9ab04d76":"markdown","7fcec104":"markdown","2bec71a5":"markdown","c63f6ae2":"markdown","359437a3":"markdown","28a71775":"markdown","fc8d4162":"markdown","75a000ca":"markdown","81e69962":"markdown","d45b51e1":"markdown","747f99d0":"markdown","30ec2310":"markdown","3d5fa16e":"markdown"},"source":{"3d4bba55":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport seaborn as sns","e555a9bc":"train= pd.read_csv('..\/input\/titanic\/train.csv')\ntest= pd.read_csv('..\/input\/titanic\/test.csv')","1bcb5bea":"train.info()","9ad465b0":"#now first I shall be dropping the columns which I described above as the ones which are not needed for the analysis\ntrain.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","2de0f491":"# finding the columns which have null values.\nnull_vals = train.columns[train.isnull().any()]\nprint('Columns with null values: \\n', null_vals)","762f2989":"train.duplicated().any()","e694b849":"train.head()","d306f29f":"train['Age'].median()\n","2837a880":"train['Embarked'].mode()[0]","c2aa640a":"train['Age'].fillna(train['Age'].median(),inplace=True)","9392c2dd":"train['Embarked'].fillna(train['Embarked'].mode()[0],inplace= True)","53f732a5":"# finding the columns which have null values.\nnull_vals = train.columns[train.isnull().any()]\nprint('Columns with null values: \\n', null_vals)","fbc4d394":"test.info()","4fbc010b":"#now first I shall be dropping the columns which I described above as the ones which are not needed for the analysis\ntest.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","8f676bce":"test.head()","2c654e0c":"# finding the columns which have null values in test data.\nnull_vals = test.columns[test.isnull().any()]\nprint('Columns with null values: \\n', null_vals)","0852222b":"test['Age'].median()","7f3ecfb1":"test['Fare'].mode()[0]","219bafa6":"test['Age'].fillna(test['Age'].median(),inplace=True)","7681222b":"test['Fare'].fillna(test['Fare'].mode()[0] , inplace=True)","57529936":"null_vals = test.columns[test.isnull().any()]\nprint('Columns with null values: \\n', null_vals)","d1139108":"test.duplicated().any()","e74652fc":"train.plot(kind='box', figsize=(10,8))","569f3717":"column_name= ['Age', 'SibSp', 'Parch', 'Fare']\ntrain[column_name]= train[column_name].clip(lower= train[column_name].quantile(0.15), upper= train[column_name].quantile(0.85), axis=1)","c3036358":"train.drop(columns=['Parch'], axis=1, inplace=True)","e83c2902":"train.plot(kind='box', figsize= (10,8)) ","5c921024":"test.plot(kind='box', figsize= (10,8)) ","4e07d2d4":"column_name= ['Age', 'SibSp', 'Parch', 'Fare']\ntest[column_name]=test[column_name].clip(lower= test[column_name].quantile(0.15), upper= test[column_name].quantile(0.85), axis=1)","75d756cf":"\ntest.drop(columns=['Parch'], axis=1, inplace=True)","6fced4d2":"test.plot(kind='box', figsize= (10,8))","8f24892a":"sns.barplot(x=\"Sex\", y=\"Survived\", data=train)","f1142f99":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=train)","3bf3e674":"sns.barplot(x=\"SibSp\",y=\"Survived\", data=train)","fd689c7c":"train= pd.get_dummies(train, columns=['Pclass', 'Sex', 'Embarked' ], drop_first= True)\n\ntest= pd.get_dummies(test, columns=['Pclass', 'Sex', 'Embarked' ], drop_first= True)","c8bf1fe9":"train.head()","b5adbd57":"test.head()","e342fd02":"X_train = train.drop('Survived',axis=1)\nY_train = train['Survived']\nX_test  = test","fd9f0430":"#Apply Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")","6a1a1024":"#Apply Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nY_pred = gaussian.predict(X_test)\n\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(round(acc_gaussian,2,), \"%\")","3b84c161":"#Apply RandomForest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"Score: \", round(random_forest.oob_score_, 4)*100, \"%\")","06cce8df":"test_df = pd.read_csv('..\/input\/titanic\/test.csv')","c9b232f5":"output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': Y_pred})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","f5966302":"output.head()","082a8427":"output.tail()","cbf5355e":"Dropping Parch column because almost 60-70% values are 0\n","66ccfeea":"Let us do some Data Visualization","b8990aeb":"Dropping the columns which are not required for this analysis.","9a78ceed":"Build ML Model","06320b6a":"Import Libraries and Modules","26510a4a":"Let Do Feature Encoding","0e81e43a":"Replace null vales wih mean and mode ","9ab04d76":"Looking at the columns which have null values and duplicate values","7fcec104":"Import Train and test datasets","2bec71a5":"Let us find outliers in train and test data","c63f6ae2":"There are no outliers in train dataset now. lets do it same for test data","359437a3":"As it clearly visible Females have the highest chances for survival","28a71775":"As it states, people from class 1 have higher chances of survival","fc8d4162":"Looking at the columns which have null values and duplicate values","75a000ca":"Now, there are no outliers in the test dataset.","81e69962":"Now there is no null and duplicate values present on the train data set","d45b51e1":"checking Duplicated values ","747f99d0":"Looking at the duplicate values","30ec2310":"There are no duplicate values in the dataframe","3d5fa16e":"Replace null values with the help of mean and mode method"}}