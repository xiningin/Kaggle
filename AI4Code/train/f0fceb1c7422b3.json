{"cell_type":{"312bf299":"code","e99ab096":"code","394ae40f":"code","2d310764":"code","dc174d2c":"code","0457242a":"code","e6a6ff48":"code","73cdd334":"code","a39b8194":"code","6516ad93":"code","5ee9208b":"code","c2313955":"code","f8033d44":"code","0f7b34f1":"code","4aa6cf50":"code","084cd8f4":"code","9b9a10b2":"code","56d97419":"code","3f40e6f1":"code","5208f3a5":"code","a2eada51":"code","9801cdc9":"code","5f51f50f":"code","575e3aad":"code","c07ac298":"code","14baf40e":"code","d640b9f4":"code","752a94dc":"code","2815d562":"code","e9b2192b":"code","219ca44b":"code","e62106f4":"code","cbd81e16":"markdown","3e265e1c":"markdown","dd03f197":"markdown","6ae79df4":"markdown","b4311ca0":"markdown","384a6e4f":"markdown","80a13c40":"markdown","130112d5":"markdown","f0400a7c":"markdown","f4fb2728":"markdown","6dd51bc1":"markdown","4d12201e":"markdown","7ed8ef08":"markdown","d30d1048":"markdown","7c50b3e4":"markdown","c4c8c49b":"markdown","a3b4ed34":"markdown","1b58eb82":"markdown","a3f0ecf8":"markdown","672dbafd":"markdown","365f6a0a":"markdown","071d9810":"markdown","3e6c4811":"markdown","a6341559":"markdown","68930943":"markdown","545a8e09":"markdown","0637fb6d":"markdown","0e5c0c99":"markdown","85071507":"markdown","9d798877":"markdown","91bbf014":"markdown","b59d8a56":"markdown","0c38a9bc":"markdown"},"source":{"312bf299":"#let's start with importing the libraries we need to:\n# 1- read and manipulate data:\nimport pandas as pd\nimport numpy as np\n# 2- visualize data and graphs\nimport matplotlib.pyplot as plt\n","e99ab096":"#Now lets read the data \nraw_data = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\n#Now lets explore our data: \nraw_data.head(10)","394ae40f":"#lets check for missing data:\nmissing_data_print={}\nmissing_data_print['column_name']=[]\nmissing_data_print['Missing Entries']=[]\nfor elt in raw_data.columns:\n    missing_data_print['column_name'].append(elt)\n    missing_data_print['Missing Entries'].append(raw_data[elt].isna().sum())\nmissing_data_print=pd.DataFrame(missing_data_print)\nmissing_data_print","2d310764":"hist = raw_data.hist(bins=100,color='red',figsize=(16, 16))","dc174d2c":"#so we have 21 columns our target column is the price \n#because we are trying to creat a model that predicts the sell price of a house\ny=raw_data['price']\nX=raw_data.drop(['price','id'],axis=1)\n\n#we can easily notice that all data are either integers or floats except for the selling date\n#therefore I propose to convert this column into an integer column\n#actually the selling date (column date) are of type string with the format yyyymmddT000000 \n#so I propose to keep only the forme yyyymmdd and then convert this string into integer\n#with this form we will be able to represent the date with integer while keeping the order relation between the dates\n# first january 2015 will be coded 20150101 while first april 2016 will be coded 20160401 and thus we keep \n# the relation ship of first april 2016 is more recently (>) than first of january 2015\nX['date']=X['date'].apply(lambda x : int(x[0:8]))\n\n#we have a column named yr_built which indicates the year of built of the house \n#this column is not so pertinent since the houses are selled on different dates\n#I propose to replace the year of built with the age of the house at the selling date\n#I will name this column (age)\nX['age']=X['date']\/\/10000 - X['yr_built']\n\n#we have a column named yr_renovated that indicates the year of renovation of the house\n#this column is equal to zero in the cases the renovation is never done\n#I propose to change this column by a column that I name (yrs_since_renovation) \n#this column will contains the years since last renovation before selling the house\nX['yrs_since_renovation'] = X.apply(lambda x : min(x['date']\/\/10000-x['yr_renovated'],x['age']),axis=1)\n\n","0457242a":"df=X[['yr_built','age','yr_renovated','yrs_since_renovation']]\nhist = df.hist(bins=100,color='red',figsize=(16, 16))","e6a6ff48":"plt.figure(figsize=(16, 16))\ni=1\nfor elt in X.columns :\n    plt.subplot(7,3,i)\n    plt.scatter(X[elt],y)\n    plt.xlim((X[elt].min(),X[elt].max()))\n    plt.xlabel(elt)\n    plt.ylabel('price u.m.')\n    i+=1\n\nplt.tight_layout(0.05)\nplt.show()","73cdd334":"f = plt.figure(figsize=(19, 15))\ndf=pd.concat([X, y], axis=1)\nplt.matshow(df.corr(), fignum=f.number)\nplt.xticks(range(df.shape[1]), df.columns, fontsize=14, rotation=90)\nplt.yticks(range(df.shape[1]), df.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title('Correlation Matrix', fontsize=16,y=-0.08)\nplt.show()","a39b8194":"from sklearn.model_selection import train_test_split\nX_mod=X.drop(['yr_built','yr_renovated'],axis=1)\nX_mtrain, X_mtest, y_train, y_test = train_test_split(X_mod.values,y.values,test_size=0.2,random_state=0)\nX_nor=X.drop(['yrs_since_renovation','age'],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X_nor.values,y.values,test_size=0.2,random_state=0)\n####","6516ad93":"from sklearn.metrics import mean_squared_error,r2_score\nfrom math import sqrt","5ee9208b":"Results = {}\nResults['Method']=[]\nResults['R2-score']=[]\nResults['RMSE']=[]","c2313955":"Predictions = pd.DataFrame()\nPredictions['Ground Truth'] = y_test","f8033d44":"from sklearn.linear_model import LinearRegression\n#with new features\nSLR = LinearRegression()\nSLR.fit(X_mtrain,y_train)\ny_pred = SLR.predict(X_mtest)\nResults['Method'].append(\"SLR with new features\")\nResults['R2-score'].append(r2_score(y_test,y_pred))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred)))\n#with old features\nSLR_nor = LinearRegression()\nSLR_nor.fit(X_train,y_train)\ny_pred2 = SLR_nor.predict(X_test)\nResults['Method'].append(\"SLR with old features\")\nResults['R2-score'].append(r2_score(y_test,y_pred2))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred2)))\n#saving the predictions\nPredictions['SLR_new_F']=y_pred\nPredictions['SLR_old_F']=y_pred2","0f7b34f1":"#we are going to eliminate variables that won't make a difference \n#we add the constant variable x_0\nX_Ttrain = np.append(np.ones((len(X_mtrain),1)).astype(int),X_mtrain,1)\nX_Ttest = np.append(np.ones((len(X_mtest),1)).astype(int),X_mtest,1)","4aa6cf50":"import statsmodels.api as sm\n#our X_optimal is initialized to X_Ttrain\nX_opt=X_Ttrain[:,:]","084cd8f4":"#Step 1 :Fit the ALL IN model\nmodel_MLR=sm.OLS(endog=y_train,exog=X_opt).fit()\nmodel_MLR.summary()","9b9a10b2":"Column_to_delete=6\ncolumns_to_keep=[]\nfor elt in range(X_opt.shape[1]):\n    if elt != Column_to_delete :\n        columns_to_keep.append(elt)\nX_opt = X_opt[:,columns_to_keep]\nX_Ttest = X_Ttest[:,columns_to_keep]\nX_opt.shape","56d97419":"model_MLR=sm.OLS(endog=y_train,exog=X_opt).fit()\nmodel_MLR.summary()","3f40e6f1":"from sklearn.linear_model import LinearRegression\n#with new features\nSLR = LinearRegression()\n#Do not forget to take out the constant otherwise you'll get lower R-squared\nX_opt=X_opt[:,1:]\nSLR.fit(X_opt,y_train)\nX_Ttest=X_Ttest[:,1:]\ny_pred = SLR.predict(X_Ttest)\nResults['Method'].append(\"Multi-LR with new features\")\nResults['R2-score'].append(r2_score(y_test,y_pred))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred)))","5208f3a5":"#we are going to eliminate variables that won't make a difference \n#we add the constant variable x_0\nX_Ttrain = np.append(np.ones((len(X_train),1)).astype(int),X_train,1)\nX_Ttest = np.append(np.ones((len(X_test),1)).astype(int),X_test,1)","a2eada51":"import statsmodels.api as sm\n#our X_optimal is initialized to X_Ttrain\nX_opt=X_Ttrain[:,:]","9801cdc9":"#Step 1 :Fit the ALL IN model\nmodel_MLR=sm.OLS(endog=y_train,exog=X_opt).fit()\nmodel_MLR.summary()","5f51f50f":"from sklearn.linear_model import LinearRegression\n#with new features\nSLR = LinearRegression()\nX_opt=X_opt[:,1:]\nSLR.fit(X_opt,y_train)\nX_Ttest=X_Ttest[:,1:]\ny_pred = SLR.predict(X_Ttest)\nResults['Method'].append(\"Multi-LR with old features\")\nResults['R2-score'].append(r2_score(y_test,y_pred))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred)))","575e3aad":"from sklearn.preprocessing import MinMaxScaler\nX_mod_Scaler = MinMaxScaler()\nX_mtrain_s=X_mod_Scaler.fit_transform(X_mtrain)\nX_mtest_s = X_mod_Scaler.transform(X_mtest)\n\nX_Scaler = MinMaxScaler()\nX_train_s=X_Scaler.fit_transform(X_train)\nX_test_s = X_Scaler.transform(X_test)\n\ny_Scaler = MinMaxScaler()\ny_train_s=y_Scaler.fit_transform(y_train.reshape(-1,1))\ny_train_s=y_train_s.reshape(len(y_train),)\ny_test_s=y_Scaler.transform(y_test.reshape(-1,1))\ny_test_s=y_test_s.reshape(len(y_test),)","c07ac298":"from sklearn.svm import SVR\n#with new features\nsvr_RBF = SVR(kernel='rbf',gamma='auto')\nsvr_RBF.fit(X_mtrain_s,y_train_s.reshape(17290,))\ny_pred_s = svr_RBF.predict(X_mtest_s)\ny_pred = y_Scaler.inverse_transform(y_pred_s.reshape(-1,1))\nResults['Method'].append(\"Guassian SVR with new features\")\nResults['R2-score'].append(r2_score(y_test,y_pred))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred)))\n\n#with old features\nsvr_RBF2 = SVR(kernel='rbf',gamma='auto')\nsvr_RBF2.fit(X_train_s,y_train_s.reshape(17290,))\ny_pred_s2 = svr_RBF2.predict(X_test_s)\ny_pred2 = y_Scaler.inverse_transform(y_pred_s2.reshape(-1,1))\nResults['Method'].append(\"Guassian SVR with old features\")\nResults['R2-score'].append(r2_score(y_test,y_pred2))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred2)))\n\n#Saving Predictions\nPredictions['Gaussian_SVR_new_F']=y_pred.reshape(len(y_test),)\nPredictions['Gaussian_SVR_old_F']=y_pred2.reshape(len(y_test),)","14baf40e":"#using decision tree\nfrom sklearn.tree import DecisionTreeRegressor\n#With new features\nDTR = DecisionTreeRegressor(random_state=0)\nDTR.fit(X_mtrain,y_train)\ny_pred = DTR.predict(X_mtest)\nResults['Method'].append(\"DTR with new features\")\nResults['R2-score'].append(r2_score(y_test,y_pred))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred)))\n\n#with old features\nDTR_2 = DecisionTreeRegressor(random_state=0)\nDTR_2.fit(X_train,y_train)\ny_pred2 = DTR.predict(X_test)\nResults['Method'].append(\"DTR with old features\")\nResults['R2-score'].append(r2_score(y_test,y_pred2))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred2)))\n\nDecisionTreesRegressionsPredictions=pd.DataFrame()\nPredictions['DTR_new_F']=y_pred\nPredictions['DTR_old_F']=y_pred2","d640b9f4":"#random forest\nfrom sklearn.ensemble import RandomForestRegressor\n#with new features\nRFR_mod = RandomForestRegressor(n_estimators=100,random_state=0)\nRFR_mod.fit(X_mtrain,y_train)\ny_pred = RFR_mod.predict(X_mtest)\nResults['Method'].append(\"100Tree_RFR with new features\")\nResults['R2-score'].append(r2_score(y_test,y_pred))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred)))\n\n#with old features\nRFR_nor = RandomForestRegressor(n_estimators=100,random_state=0)\nRFR_nor.fit(X_train,y_train)\ny_pred2 = RFR_nor.predict(X_test)\nResults['Method'].append(\"100Tree_RFR with old features\")\nResults['R2-score'].append(r2_score(y_test,y_pred2))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred2)))\n\n#saving predictions\nPredictions['100T_RFR_new_F']=y_pred\nPredictions['100T_RFR_old_F']=y_pred2","752a94dc":"#with new features\nRFR_mod_2 = RandomForestRegressor(n_estimators=200,random_state=0)\nRFR_mod_2.fit(X_mtrain,y_train)\ny_pred = RFR_mod_2.predict(X_mtest)\nResults['Method'].append(\"200Tree_RFR with new features\")\nResults['R2-score'].append(r2_score(y_test,y_pred))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred)))\n#with old features\nRFR_nor_2 = RandomForestRegressor(n_estimators=200,random_state=0)\nRFR_nor_2.fit(X_train,y_train)\ny_pred2 = RFR_nor_2.predict(X_test)\nResults['Method'].append(\"200Tree_RFR with old features\")\nResults['R2-score'].append(r2_score(y_test,y_pred2))\nResults['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred2)))\n\n#saving predictions\nPredictions['200T_RFR_new_F']=y_pred\nPredictions['200T_RFR_old_F']=y_pred2","2815d562":"from sklearn.preprocessing import MinMaxScaler\nX_mod_Scaler = MinMaxScaler()\nX_mtrain_s=X_mod_Scaler.fit_transform(X_mtrain)\nX_mtest_s = X_mod_Scaler.transform(X_mtest)\n\nX_Scaler = MinMaxScaler()\nX_train_s=X_Scaler.fit_transform(X_train)\nX_test_s = X_Scaler.transform(X_test)\n\ny_Scaler = MinMaxScaler()\ny_train_s=y_Scaler.fit_transform(y_train.reshape(-1,1))\ny_train_s=y_train_s.reshape(len(y_train),)\ny_test_s=y_Scaler.transform(y_test.reshape(-1,1))\ny_test_s=y_test_s.reshape(len(y_test),)","e9b2192b":"from sklearn.neighbors import KNeighborsRegressor\n#for different k values example [1,10,50]\nk_values =[1,10,20,50]\nfor k in k_values:\n    #New Features\n    k_nn=KNeighborsRegressor(n_neighbors = k)\n    k_nn.fit(X_mtrain, y_train)  \n    y_pred=k_nn.predict(X_mtest)\n    Results['Method'].append(\"K=({})_NN with new features\".format(k))\n    Results['R2-score'].append(r2_score(y_test,y_pred))\n    Results['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred)))\n    k_nn_2=KNeighborsRegressor(n_neighbors = k)\n    k_nn_2.fit(X_train, y_train)  \n    y_pred2=k_nn_2.predict(X_test)\n    Results['Method'].append(\"K=({})_NN with old features\".format(k))\n    Results['R2-score'].append(r2_score(y_test,y_pred2))\n    Results['RMSE'].append(sqrt(mean_squared_error(y_test,y_pred2)))\n    ","219ca44b":"Results_DF = pd.DataFrame(Results)\nResults_DF","e62106f4":"Predictions.head(10)","cbd81e16":"## For Old Features","3e265e1c":"# Decision Tree Regression","dd03f197":"## Data Min-Max Normalization","6ae79df4":"### Step-by-Step Backward Elimination","b4311ca0":"## The regression","384a6e4f":"## Preparing the training and the test sets","80a13c40":"## Predictions","130112d5":"### Data Adaptation","f0400a7c":"## Data Min-Max Normalization","f4fb2728":"The obtained plots do not give too much information. We can not be sure about our observation, but we can assume the following:\n* The number of bedrooms seems to have an influence on the price. As shown in the plot, the price tends to be higher when the number of bedrooms is near 5. \n* The number of bathrooms seems to have the same influence as that of the bedrooms. The price tends to be higher when the number of bathrooms is between 3 and 5. \n* The interior living space (sqft_living) seems to directly influence the price. The larger the living space the higher the price tends to be. The same can be said for the space above the ground, the basement space, the interior space of the 15 nearest neighbors and for the level of construction and design (sqft_above, sqft_basement, sqft_living15, and grade).\n* Sqft_lot and sqft_lot15 represent respectively the space of the land and the space of the land of the nearest 15 neighbors. These two elements indicates two tendencies for the price. It seems like the price is higher when they are low and then it gradually regain hight when they get bigger. Well this can mean that we are dealing with two types of housing (apartments\/big building and big houses\/mansions).\n","6dd51bc1":"# Reading Data","4d12201e":"## K-NN Regressions","7ed8ef08":"## Algorithms performances","d30d1048":"## Data Preprocessing","7c50b3e4":"### Step-by-Step Backward Elimination","c4c8c49b":"## Defining the Results Table","a3b4ed34":"# Comparison of the obtained results","1b58eb82":"## For New Features","a3f0ecf8":"# Multiple Linear Regression","672dbafd":"# Support Vector Regression","365f6a0a":"## Features","071d9810":"## Random Forest with 200 Tree","3e6c4811":"## Defining the Metrics","a6341559":"# Conclusion","68930943":"## Future Work","545a8e09":"## Ensemble Learning","0637fb6d":"## Defining the Predictions Table","0e5c0c99":"# Random Forest Regression","85071507":"# Simple Linear Regression","9d798877":"You can notice that the histogram of age is almost an anti symetric replicate of the yr_built histogram. The histogram that describes the number of years since renovation is different from the yr_renovated which represents almost two classes, the first has a value 0 for houses that were never renovated and a second class around the 20th-21st century. As for yrs_since_renovation is quite identical to the histogram of age (which is expected from the formula used to compute this feature). I think using the two new features will not cause a big difference. \nWe will see this later. In the following sections, all used methods are used for both old (yr_built and yr_renovated) and new (age, yrs_since_renovation) features.","91bbf014":"# K-Nearest Neighbors","b59d8a56":"## Random Forest with 100 Tree","0c38a9bc":"### Data Adaptation\n"}}