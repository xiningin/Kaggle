{"cell_type":{"82487726":"code","4d60504c":"code","2d1fedca":"code","43387358":"code","cac4d400":"code","44bdfdc4":"code","d26d07a7":"code","5582c78f":"code","b9acaac3":"code","085cdec9":"code","f74533cf":"code","a30c24b2":"code","d641b2ef":"code","ad418797":"code","822b164c":"code","ff1aec57":"code","6e9459b6":"code","92340082":"code","abdbe91c":"code","c4c1e647":"code","a279a3a0":"code","656f4799":"code","adf14ad2":"code","aad3985c":"code","509b9e62":"code","e26e5fed":"code","def34676":"code","f6cf63d9":"code","d0f861a9":"code","f456549d":"code","4cf39534":"code","a2704070":"code","aadc44ac":"code","1ee3d246":"code","e54b43f4":"code","6ac1c06c":"code","60d9404a":"code","2542a1db":"code","36a24608":"code","4310bb0d":"code","c2b71ab1":"code","91b241c8":"code","19b65c9f":"code","5681b221":"code","5061c3c0":"code","8ad10de2":"code","4f40545e":"code","96c8d9f2":"code","bfcf4e34":"code","f93c7758":"code","0e52f7bc":"code","21309d67":"code","b8042f9d":"code","0508b62b":"code","705803f3":"code","204be8c6":"code","74172f5c":"code","7bb2b4ad":"code","943889e8":"code","72771516":"code","613291a2":"code","ff556cd5":"code","7ac4ae19":"code","82a71c28":"code","935ebd46":"code","572e3993":"code","f7773085":"code","df86334b":"code","4d76dfe0":"code","f6f756c1":"code","51fff10f":"code","08b3d006":"code","18d08bbb":"code","1f856eaa":"markdown","e7391787":"markdown","0b40cc74":"markdown","9cf877e4":"markdown","d6bb53d9":"markdown","bf57e3ad":"markdown","c6df7152":"markdown","bd66486e":"markdown","f512d2fa":"markdown","2e17ab58":"markdown","0fe70449":"markdown","0cb3a3d6":"markdown","064d2271":"markdown","57c6720d":"markdown","53ce4b1f":"markdown","ce5650a3":"markdown","a29210f5":"markdown","f0faf50b":"markdown","a3b55ad1":"markdown","c4417a72":"markdown","aa4a534b":"markdown","ed8d5487":"markdown","f1e1d50b":"markdown"},"source":{"82487726":"import numpy as np\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\n%matplotlib inline\n\n\nimport scipy.stats as stats\n","4d60504c":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain.head()","2d1fedca":"train.Survived.value_counts()","43387358":"# Checking Missing Values\nmissingno.matrix(train)","cac4d400":"train.isna().sum()","44bdfdc4":"train.info()","d26d07a7":"# our target col distribution\ntrain.Survived.value_counts()","5582c78f":"#drop cabin is model fails\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['PassengerId','Survived','Name','Ticket','Cabin'], axis=1), train['Survived'],\n                                                   test_size=0.2, random_state=0)","b9acaac3":"X_train.head()","085cdec9":"X_train['Embarked'].value_counts()","f74533cf":"# let's see category columns \ncat = [col for col in X_train.columns if X_train[col].dtypes == 'O']\ncat","a30c24b2":"# Let's get Categorical Dummies before flling missing values\n#trainset\ncat_variables = X_train[['Sex', 'Embarked']]\ncat_dummies = pd.get_dummies(cat_variables)\ncat_dummies.head()","d641b2ef":"#testset\ncat_variables_test = X_test[['Sex', 'Embarked']]\ncat_dummies_test = pd.get_dummies(cat_variables_test)\ncat_dummies_test.head()","ad418797":"#we will drop the original \u201cSex\u201d and \u201cEmbarked\u201d columns from the data frame and add the dummy variables.\n#trainset\nX_train = X_train.drop(['Sex', 'Embarked'], axis = 1)\nX_train = pd.concat([X_train, cat_dummies], axis=1)\nX_train.head()","822b164c":"X_test = X_test.drop(['Sex', 'Embarked'], axis = 1)\nX_test = pd.concat([X_test, cat_dummies_test], axis=1)\nX_test.head()","ff1aec57":"X_train.describe()","6e9459b6":"#trainset\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5)\nX_train = pd.DataFrame(imputer.fit_transform(X_train),columns = X_train.columns)\nX_train.isna().sum()","92340082":"#test data\nimputer = KNNImputer(n_neighbors=5)\nX_test = pd.DataFrame(imputer.fit_transform(X_test),columns = X_test.columns)\nX_test.isna().sum()","abdbe91c":"X_train.columns","c4c1e647":"X_train.shape","a279a3a0":"X_test.shape","656f4799":"# Checking Accuracy Before Outliers\n\n#Loading the libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n#train the model\nmodel_rf=RandomForestClassifier(n_estimators=100)\nmodel_rf.fit(X_train,y_train)\npred1=model_rf.predict(X_test)\n#check the accuracy of the model\naccuracy_score(y_test,pred1)","adf14ad2":"#print(classification_report(y_test,pred1))","aad3985c":"X_train.head()","509b9e62":"features = ['Age', 'Fare']\nfeatures","e26e5fed":"sns.set_style('dark')\nfor col in features:\n    plt.figure(figsize=(15,4))\n    plt.subplot(131)\n    sns.distplot(X_train[col], label=\"skew: \" + str(np.round(X_train[col].skew(),2)))\n    plt.legend()\n    plt.subplot(132)\n    sns.boxplot(X_train[col])\n    plt.subplot(133)\n    stats.probplot(X_train[col], plot=plt)\n    plt.tight_layout()\n    plt.show()","def34676":"df_cap = X_train.copy()","f6cf63d9":"def iqr_capping(df, cols, factor):\n    \n    for col in cols:\n        \n        q1 = df[col].quantile(0.25)\n        q3 = df[col].quantile(0.75)\n        \n        iqr = q3 - q1\n        \n        upper_whisker = q3 + (factor*iqr)\n        lower_whisker = q1 - (factor*iqr)\n        \n        df[col] = np.where(df[col]>upper_whisker, upper_whisker,\n                 np.where(df[col]<lower_whisker, lower_whisker, df[col]))","d0f861a9":"iqr_capping(df_cap, features, 1.5)","f456549d":"for col in features:\n    plt.figure(figsize=(16,4))\n    \n    plt.subplot(141)\n    sns.distplot(X_train[col], label=\"skew: \" + str(np.round(X_train[col].skew(),2)))\n    plt.title('Before')\n    plt.legend()\n    \n    plt.subplot(142)\n    sns.distplot(df_cap[col], label=\"skew: \" + str(np.round(df_cap[col].skew(),2)))\n    plt.title('After')\n    plt.legend()\n    \n    plt.subplot(143)\n    sns.boxplot(X_train[col])\n    plt.title('Before')\n    \n    plt.subplot(144)\n    sns.boxplot(df_cap[col])\n    plt.title('After')\n    plt.tight_layout()\n    plt.show()","4cf39534":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n#train the model\nmodel_rf=RandomForestClassifier(n_estimators=100)\nmodel_rf.fit(df_cap,y_train)\npred=model_rf.predict(X_test)\n#check the accuracy of the model\naccuracy_score(y_test,pred)\n","a2704070":"from sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)\npredictions = logmodel.predict(X_test)\nprint(classification_report(y_test,predictions))","aadc44ac":"#print(classification_report(y_test,pred))","1ee3d246":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","e54b43f4":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(df_cap, y_train)\ny_pred1 = gaussian.predict(X_test)\nacc_gaussian = round(accuracy_score(y_pred1, y_test) * 100, 2)\nprint(acc_gaussian)","6ac1c06c":"#print('RF train roc-auc: {}'.format(roc_auc_score(y_train, y_pred1)))","60d9404a":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\nconfusion_matrix(y_test, y_pred1, labels=(1,0))","2542a1db":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(df_cap, y_train)\ny_pred2 = logreg.predict(X_test)\nacc_logreg = round(accuracy_score(y_pred2, y_test) * 100, 2)\nprint(acc_logreg)","36a24608":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\nconfusion_matrix(y_test, y_pred2, labels=(1,0))","4310bb0d":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(df_cap, y_train)\ny_pred3 = svc.predict(X_train)\nacc_svc = round(accuracy_score(y_pred3, y_train) * 100, 2)\nprint(acc_svc)","c2b71ab1":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(df_cap, y_train)\ny_pred = linear_svc.predict(X_train)\nacc_linear_svc = round(accuracy_score(y_pred, y_train) * 100, 2)\nprint(acc_linear_svc)","91b241c8":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(df_cap, y_train)\ny_pred = perceptron.predict(X_train)\nacc_perceptron = round(accuracy_score(y_pred, y_train) * 100, 2)\nprint(acc_perceptron)","19b65c9f":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(df_cap, y_train)\ny_pred = decisiontree.predict(X_test)\nacc_decisiontree = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_decisiontree)","5681b221":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\nconfusion_matrix(y_test, y_pred, labels=(1,0))","5061c3c0":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(df_cap, y_train)\ny_pred = randomforest.predict(X_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_randomforest)","8ad10de2":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\nconfusion_matrix(y_test, y_pred, labels=(1,0))","4f40545e":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(df_cap, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_knn)","96c8d9f2":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\nconfusion_matrix(y_test, y_pred, labels=(1,0))","bfcf4e34":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(df_cap, y_train)\ny_pred = sgd.predict(X_test)\nacc_sgd = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_sgd)","f93c7758":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\nconfusion_matrix(y_test, y_pred, labels=(1,0))","0e52f7bc":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(df_cap, y_train)\ny_pred = gbk.predict(X_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","21309d67":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\nconfusion_matrix(y_test, y_pred, labels=(1,0))","b8042f9d":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","0508b62b":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","705803f3":"## Apply RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train, y_train)\nytrain_pred = rf_model.predict_proba(X_train)\nprint('RF train roc-auc: {}'.format(roc_auc_score(y_train, ytrain_pred[:,1])))\nytest_pred = rf_model.predict_proba(X_test)\nprint('RF test roc-auc: {}'.format(roc_auc_score(y_test, ytest_pred[:,1])))","204be8c6":"from sklearn.linear_model import LogisticRegression\nlog_classifier=LogisticRegression()\nlog_classifier.fit(X_train, y_train)\nytrain_pred = log_classifier.predict_proba(X_train)\nprint('Logistic train roc-auc: {}'.format(roc_auc_score(y_train, ytrain_pred[:,1])))\nytest_pred = log_classifier.predict_proba(X_test)\nprint('Logistic test roc-auc: {}'.format(roc_auc_score(y_test, ytest_pred[:,1])))","74172f5c":"from sklearn.ensemble import AdaBoostClassifier\nada_classifier=AdaBoostClassifier()\nada_classifier.fit(X_train, y_train)\nytrain_pred = ada_classifier.predict_proba(X_train)\nprint('Adaboost train roc-auc: {}'.format(roc_auc_score(y_train, ytrain_pred[:,1])))\nytest_pred = ada_classifier.predict_proba(X_test)\nprint('Adaboost test roc-auc: {}'.format(roc_auc_score(y_test, ytest_pred[:,1])))","7bb2b4ad":"from sklearn.neighbors import KNeighborsClassifier\nknn_classifier=KNeighborsClassifier()\nknn_classifier.fit(X_train, y_train)\nytrain_pred = knn_classifier.predict_proba(X_train)\nprint('Adaboost train roc-auc: {}'.format(roc_auc_score(y_train, ytrain_pred[:,1])))\nytest_pred = knn_classifier.predict_proba(X_test)\nprint('Adaboost test roc-auc: {}'.format(roc_auc_score(y_test, ytest_pred[:,1])))","943889e8":"pred=[]\nfor model in [rf_model,log_classifier,ada_classifier,knn_classifier]:\n    pred.append(pd.Series(model.predict_proba(X_test)[:,1]))\nfinal_prediction=pd.concat(pred,axis=1).mean(axis=1)\nprint('Ensemble test roc-auc: {}'.format(roc_auc_score(y_test,final_prediction)))","72771516":"pd.concat(pred,axis=1)","613291a2":"final_prediction","ff556cd5":"#### Calculate the ROc Curve\n\n\nfpr, tpr, thresholds = roc_curve(y_test, final_prediction)\nthresholds","7ac4ae19":"from sklearn.metrics import accuracy_score\naccuracy_ls = []\nfor thres in thresholds:\n    y_pred = np.where(final_prediction>thres,1,0)\n    accuracy_ls.append(accuracy_score(y_test, y_pred, normalize=True))\n    \naccuracy_ls = pd.concat([pd.Series(thresholds), pd.Series(accuracy_ls)],\n                        axis=1)\naccuracy_ls.columns = ['thresholds', 'accuracy']\naccuracy_ls.sort_values(by='accuracy', ascending=False, inplace=True)\naccuracy_ls.head()","82a71c28":"def plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","935ebd46":"plot_roc_curve(fpr,tpr)","572e3993":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nids = test['PassengerId']\nids.head()","f7773085":"cat_variables2 = test[['Sex', 'Embarked']]\ncat_dummies2 = pd.get_dummies(cat_variables2)\ncat_dummies2.head()","df86334b":"test = test.drop(['Sex', 'Embarked'], axis = 1)\ntest = pd.concat([test, cat_dummies2], axis=1)\ntest.head()","4d76dfe0":"df_cap.head()","f6f756c1":"test = test.drop(['PassengerId','Name', 'Cabin','Ticket'], axis = 1)\ntest.head()","51fff10f":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5)\ntest = pd.DataFrame(imputer.fit_transform(test),columns = test.columns)\ntest.isna().sum()","08b3d006":"#set ids as PassengerId and predict survival \n#ids = test['PassengerId']\npredictions = sgd.predict(test)\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.PassengerId.astype(int)\noutput.to_csv('submission.csv', index=False)","18d08bbb":"output.Survived.value_counts()","1f856eaa":"# ROC and AUC Curves","e7391787":"## Handling Outliers","0b40cc74":"# 1) Import Necessary Libraries","9cf877e4":"# Score After Outliers","d6bb53d9":"Note: Label Encoder will be used for y col(Target col), but our target col is already in 0 & 1 (no need of label encoder)","bf57e3ad":"# Let's compare the accuracies of each model!","c6df7152":"## Adaboost Classifier","bd66486e":"## Capping using IQR method","f512d2fa":"# After Removing Outliers","2e17ab58":"Now we will focus on selecting the best threshold for maximum accuracy","0fe70449":"## KNN Imputer","0cb3a3d6":"### Random Forests","064d2271":"# 3) Feature Engineering \n","57c6720d":"## We see there are missing values in 'Age', 'Cabin', 'Embarked'\n\nLet's see total missing values","53ce4b1f":"## Logistic Regression","ce5650a3":"## Let's split data","a29210f5":"We will be handling outliers using IQR Method","f0faf50b":"## KNN Imptuer is a distance-based imputation method and it requires us to normalize our data. Otherwise, the different scales of our data will lead the KNN Imputer to generate biased replacements for the missing values. But The Min Max Values have little difference, We don't need to feature Scale.","a3b55ad1":"# 2) Read our training data","c4417a72":"## KNNClassifier","aa4a534b":"# Testing Different Models\nI will be testing the following models with my training data (got the list from here):\n\n* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\nFor each model, we set the model, fit it with 80% of our training data, predict for 20% of the training data and check the accuracy.","ed8d5487":"# Creating Submission File","f1e1d50b":"Necessary Information (according to this table)\nSurvival: Survival (0 = No; 1 = Yes)\n\nPclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n\nName : Name\n\nSex : Sex\n\nAge : Age\n\nSibsp : Number of Siblings\/Spouses Aboard\n\nParch : Number of Parents\/Children Aboard\n\nTicket : Ticket Number\n\nFare : Passenger Fare\n\nCabin : Cabin Number\n\nEmbarked : Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)"}}