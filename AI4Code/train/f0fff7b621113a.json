{"cell_type":{"7415fa75":"code","1424610f":"code","6988699c":"code","65a1097b":"code","4e6bd1c6":"code","bcf75c77":"code","4bcf6aa7":"code","2ec801cd":"code","66e330dc":"code","77988b62":"code","c0309548":"code","ace7cd7a":"code","fbdb4350":"code","13190842":"code","5963cb6d":"code","60c7d78b":"code","61d8c20e":"code","6f3a1c22":"code","813b4cae":"code","2c8f2b13":"code","c04d6cd4":"code","aaf79eef":"code","17358457":"code","48d36a3a":"code","62e1bf05":"code","af4a29f4":"code","a170283a":"code","dcb63fea":"code","573475b8":"code","1110ed45":"code","6040f892":"code","91f4735d":"code","005e8de4":"code","4577556b":"code","7945b725":"code","b79dd6b2":"code","85e0bc91":"markdown","6293c77f":"markdown","c9fd500b":"markdown","15cc84d6":"markdown","7b8a64e0":"markdown","2d5edf23":"markdown","51ee530c":"markdown","5c655981":"markdown","c9b2870d":"markdown","ddf8dd87":"markdown","86468f58":"markdown","75ba5184":"markdown","1ff48a14":"markdown","40dc3ad3":"markdown","9373135c":"markdown","1ccd780d":"markdown","23f7b76e":"markdown","8b5eaad5":"markdown","04d4a122":"markdown","f196ca4e":"markdown","06861d79":"markdown","0b429f25":"markdown","b36ffeff":"markdown","2775f3a1":"markdown","8c3c6221":"markdown","9a6e6498":"markdown","923d2eec":"markdown","494d05cb":"markdown","c0cf21c0":"markdown","c84f5c6d":"markdown","2a4ab737":"markdown","d8dfd8a5":"markdown","ab6c0675":"markdown","d44c74f3":"markdown","65af7d8a":"markdown","cc83655f":"markdown","acd37463":"markdown","6740f072":"markdown","d2b6e6fa":"markdown","0c275771":"markdown","1c689f3e":"markdown","72ed8f7a":"markdown"},"source":{"7415fa75":"#import plotly \n#import plotly.plotly as py\n#import plotly.graph_objs as go\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n#Scikit learn librairies\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import scale\n\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline","1424610f":"df=pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\ndf_bureau=pd.read_csv('..\/input\/home-credit-default-risk\/bureau.csv')\ndftmp= pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')","6988699c":"df.shape","65a1097b":"df.head(10)","4e6bd1c6":"df.dtypes.value_counts()","bcf75c77":"fig11=plt.figure()\nax11=plt.axes()\nthe_target = dftmp['TARGET']\nthe_target.replace(to_replace=[1,0], value= ['YES','NO'], inplace = True)\nplt.title('Target repartition')\nax11 = ax11.set(xlabel='Default proportion', ylabel='Number of people')\nthe_target.value_counts().plot.bar()\nplt.show()","4bcf6aa7":"print(df.shape)\ndf=df.merge(right=df_bureau,how='inner', on='SK_ID_CURR')\nprint(df.shape)","2ec801cd":"df.TARGET.isnull().sum()","66e330dc":"df.dtypes.value_counts()","77988b62":"fig11=plt.figure()\nax11=plt.axes()\nthe_target = dftmp['TARGET']\nthe_target.replace(to_replace=[1,0], value= ['YES','NO'], inplace = True)\nplt.title('Target repartition')\nax11 = ax11.set(xlabel='Default proportion', ylabel='Number of people')\nthe_target.value_counts().plot.pie(startangle=90, autopct='%1.1f%%')\nplt.show()","c0309548":"#A function to print every graph with the ID as \ndef print_all_values():\n    df1=df.drop('SK_ID_CURR',axis=1)\n    cols=df1.columns\n    for col in cols:\n        if (df[col].dtypes !='object'):\n\n            fig1=plt.figure()\n            ax1=plt.axes()\n            plt.scatter(df[[col]],df.SK_ID_CURR,alpha=1,s=0.5)\n            plt.title(col)\n            ax1 = ax1.set(xlabel=col, ylabel='ID')\n            plt.show()\n            \n            \nprint_all_values()","ace7cd7a":"#Plotting the income of the people making default\ndf1=df[df.AMT_INCOME_TOTAL <1600000.0]\ndf1=df1[df.TARGET ==1 ]\ndf2=df[df.TARGET ==1 ]\n\n\nfig2=plt.figure()\nax2=plt.axes()\nplt.scatter(df2.SK_ID_CURR ,df2.AMT_INCOME_TOTAL ,alpha=1)\nplt.title('Repartition des salaires sans limite de maximum')\nax2 = ax2.set(xlabel='ID', ylabel='Salaire')\n\nfig1=plt.figure()\nax1=plt.axes()\nplt.scatter(df1.SK_ID_CURR ,df1.AMT_INCOME_TOTAL ,alpha=1)\nplt.title('Repartition des salaires avec une limite de maximum de 1600000$')\nax1 = ax1.set(xlabel='ID', ylabel='Salaire')\nplt.show()","fbdb4350":"print(df.shape)\ndf=df[df.AMT_INCOME_TOTAL <1750000.0]\ndf=df[df.CNT_FAM_MEMBERS <12]\ndf=df[df.OBS_30_CNT_SOCIAL_CIRCLE <50]\ndf=df[df.DEF_30_CNT_SOCIAL_CIRCLE <20]\ndf=df[df.OBS_60_CNT_SOCIAL_CIRCLE <55]\ndf=df[df.DEF_60_CNT_SOCIAL_CIRCLE <15]\ndf=df[df.AMT_REQ_CREDIT_BUREAU_HOUR <4]\ndf=df[df.AMT_REQ_CREDIT_BUREAU_QRT <55]\ndf=df[df.CNT_CREDIT_PROLONG <6.5]\nprint(df.shape)","13190842":"df['OWN_CAR_AGE']=df['OWN_CAR_AGE'].fillna(0)\n\n\ncols=['APARTMENTS_AVG','BASEMENTAREA_AVG','COMMONAREA_AVG','ELEVATORS_AVG','ENTRANCES_AVG','FLOORSMAX_AVG','FLOORSMIN_AVG','LANDAREA_AVG','LIVINGAPARTMENTS_AVG','LIVINGAREA_AVG','NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_AVG','APARTMENTS_MODE','BASEMENTAREA_MODE','COMMONAREA_MODE','ELEVATORS_MODE','ENTRANCES_MODE','FLOORSMAX_MODE','FLOORSMIN_MODE','LANDAREA_MODE','LIVINGAPARTMENTS_MODE','LIVINGAREA_MODE','NONLIVINGAPARTMENTS_MODE','NONLIVINGAREA_MODE','APARTMENTS_MEDI','BASEMENTAREA_MEDI','COMMONAREA_MEDI','ELEVATORS_MEDI','ENTRANCES_MEDI','FLOORSMAX_MEDI','FLOORSMIN_MEDI','LANDAREA_MEDI','LIVINGAPARTMENTS_MEDI','LIVINGAREA_MEDI','NONLIVINGAPARTMENTS_MEDI','NONLIVINGAREA_MEDI']        \nfor i in df.index:\n    if (df.loc[i,'FLAG_OWN_REALTY'] =='N'):\n        for col in cols:\n            df.set_value(i,col,0)\n            \n\ndf['NAME_TYPE_SUITE']=df['NAME_TYPE_SUITE'].fillna('Unknown')\ndf['OCCUPATION_TYPE']=df['OCCUPATION_TYPE'].fillna('Unknown')","5963cb6d":"fig1=plt.figure()\nax1=plt.axes()\nplt.scatter(df.OWN_CAR_AGE,df.FLAG_OWN_CAR,color='cyan')\nplt.title('Graphique de \"Own_Car\" et \"Own_Car_Age\"')\nax1 = ax1.set(xlabel='Own_Car_Age', ylabel='Own_Car')\nplt.show()","60c7d78b":"df['DAYS_BIRTH'] = df['DAYS_BIRTH']\/(-365)\ndf=df.rename(columns={'DAYS_BIRTH':'AGE'})\ndf.AGE.describe()","61d8c20e":"#Dataset of missing values order by percentage\ndef nan_count_df(df_to_print):\n    \n    nan_count = df_to_print.isnull().sum()\n\n    nan_percentage = (nan_count \/ len(df))*100\n\n    nan_df=pd.concat([nan_percentage], axis=1)\n    nan_df=nan_df.rename(columns={0:'Percentage'})\n    nan_df=nan_df[nan_df.Percentage != 0]\n    nan_df = nan_df.sort_values(by='Percentage',ascending=False)\n    return nan_df\n\nnan_df=nan_count_df(df)\nnan_df","6f3a1c22":"print(df.shape)\ndef delete_columns(df_transformed,df_missing_values,max_value):\n        cols=df_missing_values[df_missing_values['Percentage']>=max_value].T.columns\n        for col in cols:\n            df_transformed=df_transformed.drop(col, axis=1)\n        \n        return df_transformed\ndf=delete_columns(df,nan_df,62)\nprint(df.shape)","813b4cae":"df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","2c8f2b13":"#useless columns\ncolumns_to_drop = ['SK_ID_CURR','WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START','NAME_TYPE_SUITE','FLAG_MOBIL','FLAG_CONT_MOBILE']\ndf=df.drop(columns=columns_to_drop)","c04d6cd4":"#Encodage pour 2 cat\u00e9gories\ndef two_cat_encoding(df_to_transf):\n    le = LabelEncoder()\n\n    for cols in df_to_transf:\n        if df_to_transf[cols].dtype == 'object':\n            if len(list(df_to_transf[cols].unique())) == 2:\n                le.fit(df_to_transf[cols])\n                df_to_transf[cols] = le.transform(df_to_transf[cols])\n    return df_to_transf\ndf=two_cat_encoding(df)","aaf79eef":"df = pd.get_dummies(df)","17358457":"print('Les nouvelles dimensions du dataframes sont :\\n', df.shape)","48d36a3a":"df_columns=df.columns\ndf=df.dropna()","62e1bf05":"X =df.drop('TARGET',axis=1)\ny = df['TARGET']  \n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)","af4a29f4":"from sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression(fit_intercept=True,intercept_scaling=1,max_iter=200,tol=0.0001,random_state=None)\nlogisticRegr.fit(X_train, y_train)","a170283a":"#ERROR\nerror = (1 - logisticRegr.score(X_test, y_test))*100\nprint('Score  = ',logisticRegr.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","dcb63fea":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda=LinearDiscriminantAnalysis(n_components=None)\nlda.fit(X_train, y_train)","573475b8":"#ERROR\nerror = (1 - lda.score(X_test, y_test))*100\nprint('Score  = ',lda.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","1110ed45":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=300, oob_score=True, random_state=0)\nrf.fit(X_train,y_train)","6040f892":"error = (1 - rf.score(X_test, y_test))*100\nprint('Score  = ',rf.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","91f4735d":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train,y_train)","005e8de4":"error = (1 - clf.score(X_test, y_test))*100\nprint('Score  = ',clf.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","4577556b":"from sklearn.metrics import classification_report, confusion_matrix\npredictions = logisticRegr.predict(X_test)\n\nprint(classification_report(y_test,predictions))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions))","7945b725":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(estimator = logisticRegr , \n                         X=X_train, \n                         y=y_train, \n                         cv=3)\nprint('Cross-validation scores de r\u00e9ussite: %s' %(scores))\nprint('CV pr\u00e9cision: %.3f +\/- %.3f' %(np.mean(scores), np.std(scores)))","b79dd6b2":"print('Taux de r\u00e9ussite par mod\u00e8le:\\n\\nR\u00e9gression Logistique:',logisticRegr.score(X_test, y_test)*100,'%','\\n\\nLDA:',lda.score(X_test, y_test)*100,'%','\\n\\nRandom Forest Classifier:',rf.score(X_test, y_test)*100,'%','\\n\\nDecision Tree Classifier:',clf.score(X_test, y_test)*100,'%')","85e0bc91":"La r\u00e9partition de la target n'est pas \u00e9quilibr\u00e9e. Il y a environ 8% des individus qui ont fait d\u00e9faut. Cette distribution sera \u00e0 prendre en compte lors de l'analyse de nos r\u00e9sultats.","6293c77f":"Ce jeu de donn\u00e9e est constitu\u00e9 de plusieurs bases de donn\u00e9es. Nous allons donc joindre notre base de donn\u00e9e \u00e0 la base de donn\u00e9e 'bureau.csv'. L'ID que nous utilisons pour joindre les bases est 'SK_ID_CURR' car c'est la seule colonne commune aux deux tables.","c9fd500b":"#### Score et erreur","15cc84d6":"#### Score et erreur","7b8a64e0":"Maintenant, nous allons encod\u00e9 les colonnes des types 'object' de deux mani\u00e8res diff\u00e9rentes:\n\n1. Pour les colonnes de 2 cat\u00e9gories, on les encode avec 1 et 0.\n2. Pour celles qui ont plus de 2 cat\u00e9gories, on les encode avec la methode des OneHotEncoding pour r\u00e9duire le biais induit","2d5edf23":"-------------------------------------------------------------------------\n        Matthieu64\n--------------------------------------------------------------------------","51ee530c":"#### Spliting the data between training and testing","5c655981":"On utilise la methode get_dummies() pour faire le oneHotEncoding. Elle s\u00e9parera donc les variables en utilisant des flags.","c9b2870d":"# Home Credit Default Risk LDA","ddf8dd87":"Il y a des colonnes ou plus de 70% des valeurs sont manquantes. Nous ne pouvons pas remplacer ces valeurs par des moyennes ou des m\u00e9diannes car elles ne seraient pas repr\u00e9sentatives des donn\u00e9es. Nous ne pouvons pas non plus supprimer les lignes avec des valeurs manquantes car nous supprimerions 70% des valeurs de notre base de donn\u00e9e. La seule solution ici est de supprimer les colonnes avec trop de valeurs manquantes. Pour cela nous avons cr\u00e9e une fonction qui, \u00e0 partir du dataframes pr\u00e9c\u00e9dent, supprime les colonnes avec un pourcentage de valeur manquantes sup\u00e9rieur \u00e0 celui donn\u00e9 en param\u00e8tre.","86468f58":"On affiche maintenant un graphique de la r\u00e9partition de la target (le defaut de paiement).","75ba5184":"Regardons le nombre de colonne de chaque type.","1ff48a14":"##### 2. Les colonnes \u00e0 plus de 2 cat\u00e9gories","40dc3ad3":"On affiche les 10 premiers elements de la base de donn\u00e9e.","9373135c":"On fini par faire une cross-validation","1ccd780d":"#### Score et erreur","23f7b76e":"On affiche \u00e9galement le nombre de colonnes par types.","8b5eaad5":"On v\u00e9rifie que la nouvelle distribution de la target est consistante.","04d4a122":"Apr\u00e8s entrainement de nos algorithmes, nous avons obtenu les r\u00e9sultats suivants sur le jeu de test:","f196ca4e":"Ensuite, on remplace les valeurs manquantes lorsque cela est possible.","06861d79":"## Tree Decision Classifier","0b429f25":"#### Score et erreur","b36ffeff":"Nous avons donc \u00e0 pr\u00e9sent 132 colonnes au lieu de 138.","2775f3a1":"On remarque qu'il y a des valeurs aberantes, par exemple pour les salaires. Voici-ci dessous un graphique avec et sans la  valeur aberante. Ces valeurs r\u00e9duisent grandement la precision des pr\u00e9diction des algorithmes de machine learning, principalement la regression logistique.","8c3c6221":"On en profite aussi pour remplacer la variable 'DAYS_BIRTH' par l'age de la personne pour plus de lisibilit\u00e9.","9a6e6498":"##  Charging the datasets and librairies","923d2eec":"On peut donc supprimer cette valeur. On fait de meme pour les autres cat\u00e9gories o\u00f9 cela est le cas.\n\nNous ne pouvons malheuresement pas utiliser la fonction que l'on avait cod\u00e9 pour la pr\u00e9c\u00e9dente base de donn\u00e9e car nos ordinateurs ne sont pas assez puissants pour effectuer les calculs dans un temps raisonnable. Nous nous contenterons donc d'une suppression manuelle.","494d05cb":"##### 1. Les colonnes \u00e0 deux cat\u00e9gories","c0cf21c0":"On affiche les dimensions de la base 'application_train.csv'","c84f5c6d":"---------------------------------------------------------------------------------------------\n#                  Algorithmes de Machine Learning\n----------------------------------------------------------------------------------------------","2a4ab737":"Afin d'avoir une meilleure visibilit\u00e9 de la data nous souhaitons cr\u00e9es des graphiques de nos donn\u00e9es.\n\nNous avons \u00e9crit une fonction afin de d'afficher la valeur de chaque individu pour chaque cat\u00e9gorie.","d8dfd8a5":"Apres la fusion des bases, nous avons maintenant 138 colonnes et 1 465 325 lignes. Verifions qu'il n'y a pas d'invidus avec une target manquante:","ab6c0675":"On supprime les colonnes qui ne se sont pas av\u00e9r\u00e9es utiles \u00e0 la prediction.","d44c74f3":"Nous avons cr\u00e9e une fonction qui cr\u00e9er un dataframe avec le nom de la cat\u00e9gorie et le pourcentage de valeurs manquantes. ","65af7d8a":"On commence par supprimer les valeurs nulles que nous n'avons pas r\u00e9ussi \u00e0 remplacer.","cc83655f":"# Exploration des donn\u00e9es","acd37463":"## Random Forest Classifier","6740f072":"# R\u00e9sultats","d2b6e6fa":"Regardons le nombre de cat\u00e9gories pour chaque colonne de type 'objet'.","0c275771":"# Encodage des cat\u00e9gories","1c689f3e":"# Cross Validation","72ed8f7a":"Par exemple, pour la variable 'OWN_CAR_AGE', lorsque la personne n'a pas de voiture (le drapeau 'OWN_CAR' est \u00e0 z\u00e9ro), l'age de sa voiture est remplie avec 'Nan'. On remarque que la solution optimale et de remplacer les valeurs d'age manquantes par 0 (cf. graph ci-dessous)."}}