{"cell_type":{"386b1b71":"code","928e49ae":"code","d495d043":"code","4a485b94":"code","b9bec8c3":"code","165497c8":"code","b682d1d1":"code","6e6257d4":"code","b35782a0":"code","dc0de9ca":"code","e71a5950":"code","ed6abd9e":"code","bb21c72b":"code","42827b68":"code","ea92ab0b":"code","c0b2a37a":"code","f035f8fa":"code","efc751e5":"code","f3ad6488":"code","e8c38388":"code","3ce3c97d":"code","48dc084e":"code","8423771a":"code","adbedcd5":"code","575c773b":"code","b14ef21d":"code","d3ecb38c":"code","8a67aab1":"code","d527c7ea":"code","b8115f06":"markdown","c867d898":"markdown","daaf9379":"markdown","97edb3fc":"markdown","9d5d9e2d":"markdown","4bf2a870":"markdown","47197e84":"markdown","8742e947":"markdown","40d05e82":"markdown","87ff061a":"markdown","c630df2c":"markdown","d42da093":"markdown","a7d1a1b0":"markdown","1e1faae4":"markdown","7be907cf":"markdown","97186d01":"markdown","bf99cb84":"markdown","ed49c442":"markdown","52c04abc":"markdown","645a5071":"markdown","0be5a37b":"markdown","98dd8bd6":"markdown","282c53e1":"markdown","7f6b76ae":"markdown","8791dac1":"markdown","67905c0c":"markdown","39305188":"markdown","428f56b3":"markdown","2257c51d":"markdown","4d9bb677":"markdown","6a7e95fd":"markdown"},"source":{"386b1b71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","928e49ae":"import numpy as np\nimport pandas as pd \nimport cv2\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil\nfrom glob import glob\n%matplotlib inline\n\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, BatchNormalization\nfrom keras.models import Model\nfrom keras.optimizers import Adam \nfrom keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, EarlyStopping\n\nfrom keras.preprocessing.image import ImageDataGenerator\n","d495d043":"def plotImages(artist,directory):\n    print(artist)\n    multipleImages = glob(directory)\n    plt.rcParams['figure.figsize'] = (15, 15)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    i_ = 0\n    for l in multipleImages[:25]:\n        im = cv2.imread(l)\n        im = cv2.resize(im, (128, 128)) \n        plt.subplot(5, 5, i_+1) #.set_title(l)\n        plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n        i_ += 1","4a485b94":"plotImages(\"Vincent van Gogh\",\"..\/input\/best-artworks-of-all-time\/images\/images\/Vincent_van_Gogh\/**\")      ","b9bec8c3":"plotImages(\"Leonardo da Vinci\",\"..\/input\/best-artworks-of-all-time\/images\/images\/Leonardo_da_Vinci\/**\")      ","165497c8":"plotImages(\"Andy Warhol\",\"..\/input\/best-artworks-of-all-time\/images\/images\/Andy_Warhol\/**\")      ","b682d1d1":"artists = pd.read_csv('..\/input\/best-artworks-of-all-time\/artists.csv')","6e6257d4":"artists","b35782a0":"artists.shape","dc0de9ca":"plt.figure(figsize=(5,5))\nnationalityPlot = sns.countplot(y='nationality',data=artists)\nnationalityPlot","e71a5950":"figsize=(15, 5)\nticksize = 14\ntitlesize = ticksize + 8\nlabelsize = ticksize + 5\n\nparams = {'figure.figsize' : figsize,\n          'axes.labelsize' : labelsize,\n          'axes.titlesize' : titlesize,\n          'xtick.labelsize': ticksize,\n          'ytick.labelsize': ticksize}\n\nplt.rcParams.update(params)\n\ncol1 = \"name\"\ncol2 = \"paintings\"\n\nsns.barplot(x=col1, y=col2, data=artists)\nplt.title(\"Painting Count by Artist\")\nplt.xlabel(\"Artist\")\nplt.ylabel(\"Painting Count\")\nplt.xticks(rotation=90)\nplt.plot()","ed6abd9e":"# Sort artists by number of paintings\nartists = artists.sort_values(by=['paintings'], ascending=False)\n\n# Create a dataframe with artists having more than 200 paintings\nartists_top = artists[artists['paintings'] >= 200].reset_index()\nartists_top = artists_top[['name', 'paintings']]\n\nartists_top['class_weight'] = artists_top.paintings.sum() \/ (artists_top.shape[0] * artists_top.paintings)\nartists_top","bb21c72b":"# Set class weights - assign higher weights to underrepresented classes\nclass_weights = artists_top['class_weight'].to_dict()\nclass_weights","42827b68":"# There is some problem recognizing 'Albrecht_D\u00fcrer' (don't know why, worth exploring)\n# So I'll update this string as directory name to df's\nupdated_name = \"Albrecht_Du\u0308rer\".replace(\"_\", \" \")\nartists_top.iloc[4, 0] = updated_name","ea92ab0b":"# Explore images of top artists\nimages_dir = '..\/input\/best-artworks-of-all-time\/images\/images\/'\nartists_dirs = os.listdir(images_dir)\nartists_top_name = artists_top['name'].str.replace(' ', '_').values\n\n# See if all directories exist\nfor name in artists_top_name:\n    if os.path.exists(os.path.join(images_dir, name)):\n        print(\"Found -->\", os.path.join(images_dir, name))\n    else:\n        print(\"Did not find -->\", os.path.join(images_dir, name))","c0b2a37a":"# Print few random paintings\nimport random\nn = 5\nfig, axes = plt.subplots(1, n, figsize=(20,10))\n\nfor i in range(n):\n    random_artist = random.choice(artists_top_name)\n    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n    random_image_file = os.path.join(images_dir, random_artist, random_image)\n    image = plt.imread(random_image_file)\n    axes[i].imshow(image)\n    axes[i].set_title(\"Artist: \" + random_artist.replace('_', ' '))\n    axes[i].axis('off')\n\nplt.show()","f035f8fa":"batch_size = 64\ntrain_input_shape = (224, 224, 3)\nn_classes = artists_top.shape[0]\n\ntrain_datagen = ImageDataGenerator(validation_split=0.2,\n                                   rescale=1.\/255.,\n                                   #rotation_range=45,\n                                   #width_shift_range=0.5,\n                                   #height_shift_range=0.5,\n                                   shear_range=5,\n                                   #zoom_range=0.7,\n                                   horizontal_flip=True,\n                                   vertical_flip=True,\n                                  )\n\ntrain_generator = train_datagen.flow_from_directory(directory=images_dir,\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"training\",\n                                                    shuffle=True,\n                                                    classes=artists_top_name.tolist()\n                                                   )\n\nvalid_generator = train_datagen.flow_from_directory(directory=images_dir,\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"validation\",\n                                                    shuffle=True,\n                                                    classes=artists_top_name.tolist()\n                                                   )\n\nSTEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n\/\/valid_generator.batch_size\nprint(\"Total number of batches =\", STEP_SIZE_TRAIN, \"and\", STEP_SIZE_VALID)","efc751e5":"# Load pre-trained model\nfrom keras.applications import *\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)\n\nfor layer in base_model.layers:\n    layer.trainable = True","f3ad6488":"base_model.summary()","e8c38388":"classifier = Flatten()(base_model.output)\n\n#Initialize the CNN\nclassifier = Dense(512, activation='relu')(classifier)\nclassifier = BatchNormalization()(classifier)\n\nclassifier= Dense(16, activation='relu')(classifier)\nclassifier = BatchNormalization()(classifier)\n\noutput = Dense(n_classes, activation = 'softmax')(classifier)","3ce3c97d":"model = Model(inputs=base_model.input, outputs=output)","48dc084e":"#Compile the CNN\nmodel.compile(optimizer = Adam(lr =0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])","8423771a":"n_epoch = 10\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, \n                           mode='auto', restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              verbose=1, mode='auto')","adbedcd5":"history = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr],\n                              use_multiprocessing=True,\n                              workers=16,nlp\n                              class_weight=class_weights\n                             )","575c773b":"# Freeze core ResNet layers and train again \nfor layer in model.layers:\n    layer.trainable = False\n\nfor layer in model.layers[:50]:\n    layer.trainable = True\n\noptimizer = Adam(lr=0.0001)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])\n\nn_epoch = 50\nhistory2 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr, early_stop],\n                              use_multiprocessing=True,\n                              workers=16,\n                              class_weight=class_weights\n                             )","b14ef21d":"plt.figure(figsize=(10, 10))\n\nplt.subplot(2, 2, 1)\nplt.plot(history.history['loss'], label='Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.title('Training - Loss Function')\n\nplt.subplot(2, 2, 2)\nplt.plot(history.history['accuracy'], label='Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend()\nplt.title('Train - Accuracy')","d3ecb38c":"# Prediction accuracy on train data\nscore = model.evaluate_generator(train_generator, verbose=1)\nprint(\"Prediction accuracy on train data =\", score[1])","8a67aab1":"score = model.evaluate_generator(valid_generator, verbose=1)\nprint(\"Prediction accuracy on test data =\", score[1])","d527c7ea":"# Prediction\nfrom keras.preprocessing import *\n\nn = 4\nfig, axes = plt.subplots(1, n, figsize=(25,10))\n\nfor i in range(n):\n    random_artist = random.choice(artists_top_name)\n    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n    random_image_file = os.path.join(images_dir, random_artist, random_image)\n\n    # Original image\n\n    test_image = image.load_img(random_image_file, target_size=(train_input_shape[0:2]))\n\n    # Predict artist\n    test_image = image.img_to_array(test_image)\n    test_image \/= 255.\n    test_image = np.expand_dims(test_image, axis=0)\n\n    prediction = model.predict(test_image)\n    prediction_probability = np.amax(prediction)\n    prediction_idx = np.argmax(prediction)\n\n    labels = train_generator.class_indices\n    labels = dict((v,k) for k,v in labels.items())\n\n    title = \"Actual artist = {}\\nPredicted artist = {}\\nPrediction probability = {:.2f} %\" \\\n                .format(random_artist.replace('_', ' '), labels[prediction_idx].replace('_', ' '),\n                        prediction_probability*100)\n\n    # Print image\n    axes[i].imshow(plt.imread(random_image_file))\n    axes[i].set_title(title)\n    axes[i].axis('off')\n\nplt.show()","b8115f06":"### Create dataframe with artists having min of 200 paintings.","c867d898":"Lets display some random paintings for clear visualization whats happening right.","daaf9379":"### Display some paintaings by **Leonardo da Vinci**","97edb3fc":"**ImageDataGenerator()** the ImageDataGenerator accepts the original data, randomly transforms it, and returns only the new, transformed data.","9d5d9e2d":"Here I have first sorted data according to feature painting so that all my data in sorted manner and according to their painting name. I have taken paintings >200 for our analysis just taken sample data.\n","4bf2a870":"### Plot artists and count of paintings","47197e84":"### Objective:\n\nDevelop an algorithm which will identify the artist when provided with a painting, with state of the art precision.","8742e947":"### Build Model\n","40d05e82":"So here in this part we are going to build model which train our data as previously I have mentioned that I will use state of the art technique like **ResNet50** model. I can use CNN(Convolutional Neural Network) but when I read the research paper that **ResNet50** network does a tremendous job on image data so let\u2019s begin this section.","87ff061a":"### Display some paintaings by **Andy Warhol**","c630df2c":"### Train Model\n","d42da093":"Freezing a layer prevents its weights from being modified. This technique is often used in transfer learning, where the base model(trained on some other dataset)is frozen indicating that this layer should not be trained","a7d1a1b0":"**ResNet50** model is also called identity layer why because the sole purpose of identity layer is skip-connection that means skip one layer in ResNet model which helps reducing vanishing gradient problem ","1e1faae4":"So, from above we could see that given 4 random images our model predicted right artist name with given image on average probability of around 87% and above.","7be907cf":"### Load the CSV data","97186d01":"### Compile the CNN","bf99cb84":"### Fine tuning the model","ed49c442":"### Display some paintaings by **Vincent van Gogh**","52c04abc":"### Print few random paintings along with artists name","645a5071":"Data Augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. This technique like padding , cropping , shifting , flipping etc.","0be5a37b":"### Test Model\n\nHere taking randomly 5 images and giving this random images to model to predict the artist by giving 5 random images that belong to particular artist or not.","98dd8bd6":"The ResNet model is adapted to the 1000 categories of ImageNet. Our task, however, is to classify some artists.\n\nWhat can we do? With keras, it's easy to import only the convolutional part of VGG16, by setting the include_top parameter to False :","282c53e1":"Dataset: https:\/\/www.kaggle.com\/ikarus777\/best-artworks-of-all-time","7f6b76ae":"ImageNet is an image database organized according to the WordNet hierarchy this is freely available for researcher and data scientist for research purposes.","8791dac1":"### print classification report","67905c0c":"### Create your Convolutional Neural Network","39305188":"I have also added one more feature in this called \u2018class_weight\u2019 which put weight on paintings why we want weight actually simple putting weight on something it tells us the importance of that particular thing right.","428f56b3":"### Data Augmentation\n","2257c51d":"Create a key value pairs of class index and weights","4d9bb677":"Let us do some data exploration\n### Plot paintings by nationality of the painter. ","6a7e95fd":"Here is the result after training the model for first time accuracy touched by my model is 0.98 or (98%) and loss reduce from 1.07 to 0.21 means model doing good now we freeze layers and re-train again."}}