{"cell_type":{"87ed0591":"code","292024ed":"code","87441f7a":"code","3056ed14":"code","90283dc2":"code","6dea035b":"code","498be121":"code","ee568b8f":"code","f80bab48":"code","ce8f6a4e":"code","d915cdb3":"code","6ec8c35c":"code","3e851647":"code","c1975c53":"code","6bd7250b":"code","afe0c48e":"code","3e779b42":"code","c7eb3c1e":"code","9bbde66a":"code","aa7f1965":"code","eb60c216":"code","dc7937df":"markdown","64ffcd5a":"markdown","d7b39d84":"markdown","63957379":"markdown","2f816c3c":"markdown","877a399f":"markdown","15a4ed47":"markdown","886c04c5":"markdown","4bda5980":"markdown","1ac9f58f":"markdown","b1f36aa0":"markdown","fca9c106":"markdown"},"source":{"87ed0591":"%pip -q install timm","292024ed":"# Import packages, setup our logger and check if we have CUDA\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport torchvision as tv\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport timm\nfrom timm.utils import *\n\nimport os\nimport shutil\nimport time\nfrom collections import OrderedDict\n\nsetup_default_logging()\n\nprint('PyTorch version:', torch.__version__)\nif torch.cuda.is_available():\n    print('CUDA available')\n    device='cuda'\nelse:\n    print('WARNING: CUDA is not available')\n    device='cpu'\n\nBATCH_SIZE = 96","87441f7a":"# a basic validation routine and runner that configures each model and loader\n\ndef validate(model, loader, criterion=None, device='cuda'):\n    # metrics\n    batch_time = timm.utils.AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    \n    # for collecting per sample prediction\/loss details\n    losses_val = []\n    top5_idx = []\n    top5_val = []\n    \n    end = time.time()\n    with torch.no_grad():\n        for i, (input, target) in enumerate(loader):\n            target = target.to(device)\n            input = input.to(device)\n            output = model(input)\n            \n            if criterion is not None:\n                loss = criterion(output, target)\n                if not loss.size():\n                    losses.update(loss.item(), input.size(0))\n                else:\n                    # only bother collecting top5 we're also collecting per-example loss\n                    output = output.softmax(1)\n                    top5v, top5i = output.topk(5, 1, True, True)\n                    top5_val.append(top5v.cpu().numpy())\n                    top5_idx.append(top5i.cpu().numpy())\n                    losses_val.append(loss.cpu().numpy())\n                    losses.update(loss.mean().item(), input.size(0))\n                \n            prec1, prec5 = timm.utils.accuracy(output, target, topk=(1, 5))\n            top1.update(prec1.item(), input.size(0))\n            top5.update(prec5.item(), input.size(0))\n\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % 20 == 0:\n                print('Test: [{0}\/{1}]\\t'\n                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f}, {rate_avg:.3f}\/s) \\t'\n                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                    i, len(loader), batch_time=batch_time,\n                    rate_avg=input.size(0) \/ batch_time.avg,\n                    top1=top1, top5=top5))\n\n    results = OrderedDict(\n        top1=top1.avg, top1_err=100 - top1.avg,\n        top5=top5.avg, top5_err=100 - top5.avg,\n    )\n    if criterion is not None:\n        results['loss'] = losses.avg\n    if len(top5_idx):\n        results['top5_val'] = np.concatenate(top5_val, axis=0)\n        results['top5_idx'] = np.concatenate(top5_idx, axis=0)\n    if len(losses_val):\n        results['losses_val'] = np.concatenate(losses_val, axis=0)\n    print(' * Prec@1 {:.3f} ({:.3f}) Prec@5 {:.3f} ({:.3f})'.format(\n       results['top1'], results['top1_err'], results['top5'], results['top5_err']))\n    return results\n\n\ndef runner(model_args, dataset, device='cuda', collect_loss=False):\n    model_name = model_args['model']\n    model = timm.create_model(model_name, pretrained=True)\n    model = model.to(device)\n    model.eval()\n\n    data_config = timm.data.resolve_data_config(model_args, model=model, verbose=True)\n\n    loader = timm.data.create_loader(\n        dataset,\n        input_size=data_config['input_size'],\n        batch_size=BATCH_SIZE,\n        use_prefetcher=True,\n        interpolation='bicubic',\n        mean=data_config['mean'],\n        std=data_config['std'],\n        crop_pct=1.0, #data_config['crop_pct'],\n        num_workers=2)\n\n    criterion = None\n    if collect_loss:\n        criterion = torch.nn.CrossEntropyLoss(reduction='none').to(device)\n    results = validate(model, loader, criterion, device)\n    \n    # cleanup checkpoint cache to avoid running out of disk space\n    shutil.rmtree(os.path.join(os.environ['HOME'], '.cache', 'torch', 'checkpoints'), True)\n    \n    # add some non-metric values for charting \/ comparisons\n    results['model'] = model_name\n    results['img_size'] = data_config['input_size'][-1]\n\n    # create key to identify model in charts\n    key = [model_name, str(data_config['input_size'][-1])]\n    key = '-'.join(key)\n    return key, results","3056ed14":"# load the dataset\n#dataset = tv.datasets.ImageFolder(root=\"..\/input\/imagenet-sketch\/sketch\")\ndataset = timm.data.Dataset(\"..\/input\/imagenet-sketch\/sketch\")\nassert len(dataset) == 50889","90283dc2":"def show_img(ax, img):\n    npimg = img.numpy()\n    ax.imshow(np.transpose(npimg, (1,2,0)), interpolation='bicubic')\n\nfig = plt.figure(figsize=(8, 16), dpi=100)\nax = fig.add_subplot('111')\nnum_images = 4*8\nimages = []\ndataset.transform = transforms.Compose([\n    transforms.Resize(320, Image.BICUBIC),\n    transforms.CenterCrop(320),\n    transforms.ToTensor()])\nfor i in np.random.permutation(np.arange(len(dataset)))[:num_images]:\n    images.append(dataset[i][0])\n   \ngrid_img = make_grid(images, nrow=4, padding=10, normalize=True, scale_each=True)\nshow_img(ax, grid_img)    \n","6dea035b":"models = [\n    dict(model='mobilenetv3_100'),\n    dict(model='dpn68b'),\n    dict(model='gluon_resnet50_v1d'),\n    dict(model='efficientnet_b2'),\n    dict(model='gluon_seresnext50_32x4d'),\n    dict(model='dpn92'),\n    dict(model='gluon_seresnext101_32x4d'),\n    dict(model='inception_resnet_v2'),\n    dict(model='pnasnet5large'),\n    dict(model='tf_efficientnet_b5'),\n    dict(model='ig_resnext101_32x8d'),\n    dict(model='ig_resnext101_32x16d'),\n    dict(model='ig_resnext101_32x32d'),\n    dict(model='ig_resnext101_32x48d'),\n]\n\n# Run all the models through validation\nresults = OrderedDict()\nfor ma in models:\n    mk, mr = runner(ma, dataset, device)\n    results[mk] = mr\n\nresults_df = pd.DataFrame.from_dict(results, orient='index')\nresults_df.to_csv('.\/cached-results.csv')","498be121":"# Setup the common charting elements\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\n\nnames_all = list(results.keys())\ntop1_all = np.array([results[m]['top1'] for m in names_all])\ntop1_sort_ix = np.argsort(top1_all)\ntop1_sorted = top1_all[top1_sort_ix]\ntop1_names_sorted = np.array(names_all)[top1_sort_ix]\n\ntop5_all = np.array([results[m]['top5'] for m in names_all])\ntop5_sort_ix = np.argsort(top5_all)\ntop5_sorted = top5_all[top5_sort_ix]\ntop5_names_sorted = np.array(names_all)[top5_sort_ix]","ee568b8f":"fig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.barh(top1_names_sorted, top1_sorted, color='lightcoral')\n\nax1.set_title('Top-1 by Model')\nax1.set_xlabel('Top-1 Accuracy (%)')\nax1.set_yticklabels(top1_names_sorted)\nax1.autoscale(True, axis='both')\n\nacc_min = top1_sorted[0]\nacc_max = top1_sorted[-1]\nplt.xlim([math.ceil(acc_min - .3*(acc_max - acc_min)), math.ceil(acc_max)])\n\nplt.vlines(plt.xticks()[0], *plt.ylim(), color='0.5', alpha=0.2, linestyle='--')\nplt.show()\n\nprint('Results by top-1 accuracy:')\nresults_by_top1 = list(sorted(results.keys(), key=lambda x: results[x]['top1'], reverse=True))\nfor m in results_by_top1:\n  print('  Model: {:30} Top-1 {:4.2f}, Top-5 {:4.2f}'.format(m, results[m]['top1'], results[m]['top5']))","f80bab48":"# download ImageNet-1k resuls run on my model collection for top-1\/top-5 comparisons\n!wget -q https:\/\/raw.githubusercontent.com\/rwightman\/pytorch-image-models\/master\/results\/results-all.csv\n    \noriginal_df = pd.read_csv('.\/results-all.csv', index_col=0)\n\noriginal_results = original_df.to_dict(orient='index')","ce8f6a4e":"# some helpers for the dumbbell plots\nimport matplotlib.lines as mlines\n\ndef label_line_horiz(ax, line, label, color='0.5', fs=14, halign='center'):\n    xdata, ydata = line.get_data()\n    x1, x2 = xdata\n    xx = 0.5 * (x1 + x2)\n    text = ax.annotate(\n        label, xy=(xx, ydata[0]), xytext=(0, 1), textcoords='offset points',\n        size=fs, color=color, zorder=3,\n        bbox=dict(boxstyle=\"round\", fc=\"w\", color='0.5'),\n        horizontalalignment='center',\n        verticalalignment='center')\n    return text\n\ndef draw_line_horiz(ax, p1, p2, label, color='black'):\n    l = mlines.Line2D(*zip(p1, p2), color=color, zorder=0)\n    ax.add_line(l)\n    label_line(ax, l, label)\n    return l\n\ndef label_line_vert(ax, line, label, color='0.5', fs=14, halign='center'):\n    xdata, ydata = line.get_data()\n    y1, y2 = ydata\n    yy = 0.5 * (y1 + y2)\n    text = ax.annotate(\n        label, xy=(xdata[0], yy), xytext=(0, 0), textcoords='offset points',\n        size=fs, color=color, zorder=3,\n        bbox=dict(boxstyle=\"round\", fc=\"w\", color='0.5'),\n        horizontalalignment='center',\n        verticalalignment='center')\n    return text\n\ndef draw_line_vert(ax, p1, p2, label, color='black'):\n    l = mlines.Line2D(*zip(p1, p2), color=color, zorder=0)\n    ax.add_line(l)\n    label_line_vert(ax, l, label)\n    return l\n","d915cdb3":"fig = plt.figure()\nax1 = fig.add_subplot(111)\n\n# draw the ImageNet-Sketch dots, we're sorted on this\nax1.scatter(x=top1_names_sorted, y=top1_sorted, s=64, c='lightcoral',marker=\"o\", label='ImageNet-Sketch')\n\n# draw the original ImageNet-1k validation dots\norig_top1 = [original_results[results[n]['model']]['top1'] for n in top1_names_sorted]\nax1.scatter(x=top1_names_sorted, y=orig_top1, s=64, c='steelblue', marker=\"o\", label='ImageNet-1K')\n\nfor n, vo, vn in zip(top1_names_sorted, orig_top1, top1_sorted):\n    draw_line_vert(ax1, (n, vo), (n, vn),\n                   str(round(vo - vn, 2)), 'skyblue')\n\nax1.set_title('Top-1 Difference')\nax1.set_ylabel('Top-1 Accuracy (%)')\nax1.set_xlabel('Model')\nyl, yh = ax1.get_ylim()\nyl = 5 * ((yl + 1) \/\/ 5 + 1) \nyh = 5 * (yh \/\/ 5 + 1)\nfor y in plt.yticks()[0][1:-1]:\n    ax1.axhline(y, 0.02, 0.98, c='0.5', alpha=0.2, linestyle='-.')\nax1.set_xticklabels(top1_names_sorted, rotation='-30', ha='left')\nax1.legend(loc='upper left')\nplt.show()","6ec8c35c":"fig = plt.figure()\nax1 = fig.add_subplot(111)\n\n# draw the ImageNet-Sketch top-5 dots, we're sorted on this\nax1.scatter(x=top5_names_sorted, y=top5_sorted, s=64, c='lightcoral',marker=\"o\", label='ImageNet-Sketch')\n\n# draw the original ImageNet-1k validation dots\norig_top5 = [original_results[results[n]['model']]['top5'] for n in top5_names_sorted]\nax1.scatter(x=top5_names_sorted, y=orig_top5, s=64, c='steelblue', marker=\"o\", label='ImageNet-1K')\n\nfor n, vo, vn in zip(top5_names_sorted, orig_top5, top5_sorted):\n    draw_line_vert(ax1, (n, vo), (n, vn),\n                   str(round(vo - vn, 2)), 'skyblue')\n\nax1.set_title('Top-5 Difference')\nax1.set_ylabel('Top-5 Accuracy (%)')\nax1.set_xlabel('Model')\nyl, yh = ax1.get_ylim()\nyl = 5 * ((yl + 1) \/\/ 5 + 1) \nyh = 5 * (yh \/\/ 5 + 1)\nfor y in plt.yticks()[0][2:-2]:\n    ax1.axhline(y, 0.02, 0.98, c='0.5', alpha=0.2, linestyle='-.')\nax1.set_xticklabels(top5_names_sorted, rotation='-30', ha='left')\nax1.legend(loc='upper left')\nplt.show()","3e851647":"print('Results by absolute accuracy gap between ImageNet-Sketch and original ImageNet top-1:')\n\ngaps = {x: (results[x]['top1'] - original_results[results[x]['model']]['top1']) for x in results.keys()}\nsorted_keys = list(sorted(results.keys(), key=lambda x: gaps[x], reverse=True))\nfor m in sorted_keys:\n  print('  Model: {:30} {:4.2f}%'.format(m, gaps[m]))\nprint()\n\nprint('Results by relative accuracy gap between ImageNet-Sketch and original ImageNet top-1:')\ngaps = {x: 100 * (results[x]['top1'] - original_results[results[x]['model']]['top1']) \/ original_results[results[x]['model']]['top1'] for x in results.keys()}\nsorted_keys = list(sorted(results.keys(), key=lambda x: gaps[x], reverse=True))\nfor m in sorted_keys:\n  print('  Model: {:30} {:4.2f}%'.format(m, gaps[m]))\nprint()","c1975c53":"print('Results by relative accuracy gap between ImageNet-Sketch and original ImageNet top-5:')\ngaps = {x: (results[x]['top5'] - original_results[results[x]['model']]['top5']) for x in results.keys()}\nsorted_keys = list(sorted(results.keys(), key=lambda x: gaps[x], reverse=True))\nfor m in sorted_keys:\n  print('  Model: {:30} {:4.2f}%'.format(m, gaps[m]))\nprint()\n\nprint('Results by relative accuracy gap between ImageNet-Sketch and original ImageNet top-5:')\ngaps = {x: 100 * (results[x]['top5'] - original_results[results[x]['model']]['top5']) \/ original_results[results[x]['model']]['top5'] for x in results.keys()}\nsorted_keys = list(sorted(results.keys(), key=lambda x: gaps[x], reverse=True))\nfor m in sorted_keys:\n  print('  Model: {:30} {:4.2f}%'.format(m, gaps[m]))","6bd7250b":"# create mappings of label id to text and synset\n!wget -q https:\/\/raw.githubusercontent.com\/HoldenCaulfieldRye\/caffe\/master\/data\/ilsvrc12\/synset_words.txt\nwith open('.\/synset_words.txt', 'r') as f:\n    split_lines = [l.strip().split(' ') for l in f.readlines()]\n    id_to_synset = dict(enumerate([l[0] for l in split_lines]))\n    id_to_text = dict(enumerate([' '.join(l[1:]) for l in split_lines]))","afe0c48e":"# re-run validation on just one model, this time collecting per-example losses and predictions\nBATCH_SIZE=128\nmk, mr = runner(dict(model='ig_resnext101_32x16d'), dataset, device, collect_loss=True)    ","3e779b42":"# a function to display images in a grid and ground truth vs predictions for specified indices\ndef show_summary(indices, dataset, nrows):\n    col_scale = len(indices) \/\/ nrows\n    top5_idx = mr['top5_idx'][indices]\n    top5_val = mr['top5_val'][indices]\n\n    images = []\n    labels = []\n    filenames = []\n\n    dataset.transform = transforms.Compose([\n        transforms.Resize(320, Image.BICUBIC),\n        transforms.CenterCrop(320),\n        transforms.ToTensor()])\n\n    for i in indices:\n        img, label = dataset[i]\n        images.append(img)\n        labels.append(label)\n    filenames = dataset.filenames(list(indices), basename=True)\n\n    fig = plt.figure(figsize=(8, 8 * col_scale), dpi=100)\n    ax = fig.add_subplot('111')\n    grid_best = make_grid(images, nrow=nrows, padding=10, normalize=True, scale_each=True)\n    show_img(ax, grid_best)\n    plt.show()\n\n    summary = OrderedDict()\n    for i, l in enumerate(labels):\n        image_name = id_to_synset[i] + '\/' + filenames[i]\n        summary[image_name] = {}\n        summary[image_name]['gt'] = id_to_text[l]\n        summary[image_name]['predictions'] = [(100. * pv, id_to_text[pi]) for pi, pv in zip(top5_idx[i], top5_val[i])]\n    return summary","c7eb3c1e":"nrows = 2\nnum_images = 10\nbest_idx = np.argsort(mr['losses_val'])[:num_images]\nbest_summary = show_summary(best_idx, dataset, nrows)","9bbde66a":"print('Best prediction ground truth vs predictions')\nfor k, v in best_summary.items():\n    print('{} ground truth = {}'.format(k, v['gt']))\n    print('Predicted:')\n    for p, l in v['predictions']:\n        if p > 2e-3:\n            print('  {:.3f} {}'.format(p, l))\n    print()","aa7f1965":"nrows = 2\nnum_images = 20\nworst_idx = np.argsort(mr['losses_val'])[-num_images:][::-1]\nworst_summary = show_summary(worst_idx, dataset, nrows)","eb60c216":"print('Worst prediction ground truth vs predictions')\nfor k, v in worst_summary.items():\n    print('{} ground truth = {}'.format(k, v['gt']))\n    print('Predicted:')\n    for p, l in v['predictions']:\n        if p > 2e-3:\n            print('  {:.3f} {}'.format(p, l))\n    print()","dc7937df":"# The Worst Predictions\nLooking at these samples, I actually feel the model could be doing even better. There are clearly issues with the dataset in terms of mislabled examples and confusing images that have multiple valid answers.\n\nFrom working with adversarial attacks and defense in the past, 'Jigsaw' is one of those classes that's easy to confuse NN. You can basically apply any image on top of a jigsaw puzzle, and this is indeed the case in examples for this class. You can make a similar case for the shower curtains, a tighter crop of those illustrations would have helped.\n","64ffcd5a":"# The Best Predictions\nWhat does the model predict best? Nothing too suprising here (aside from upside down washing machine perhaps?), all of these samples have distinct shapes for their class or good levels of detail.","d7b39d84":"# Top-1 Results by Model\n\nThe figure and text output below display the Top-1 accuracy of the models on the ImageNetSketch dataset. Right away we see that the Instragram pretrained ResNeXt models are in class of their own and suprisingly doing not too shabbily","63957379":"We'll be using models and helpers from my PyTorch Image Models (TIMM) at https:\/\/github.com\/rwightman\/pytorch-image-models","2f816c3c":"# Text Summary of Top-1 and Top-5 Differences\n\nIf you prefer text, an absolute and relative % diff ranking of all models by their top-1 and top-5","877a399f":"# Can pretrained ImageNet models generalize to sketches?\n\nI've been exploring generalization capabilities of models recently after [poking around with the ImageNetV2](https:\/\/colab.research.google.com\/github\/rwightman\/pytorch-image-models\/blob\/master\/notebooks\/GeneralizationToImageNetV2.ipynb) dataset and [comparing models for runtime performance](https:\/\/colab.research.google.com\/github\/rwightman\/pytorch-image-models\/blob\/master\/notebooks\/EffResNetComparison.ipynb). Several models, especially Facebook's 'weakly supervised' Instagram tag pretrained ResNeXt models (https:\/\/pytorch.org\/hub\/facebookresearch_WSL-Images_resnext\/) stood out as having a comparatively lower drop in accuracy when applied to different datasets without retraining them. \n\nI wanted to explore this further on other datasets that had OOTB compatibility with ImageNet pretrained models. The ImageNet-Sketch dataset here fits the bill -- the same classes as ImageNet-1k but a very different distribution in terms of the images themselves. Let's see how the models perform without training them on this dataset.","15a4ed47":"# The Images\nLooking at a random sampling from the dataset we can get a taste of what the images look like. First thing that stood out for me as that they are lacking in color, not completely, but almost completely void of color. In terms of quality and detail, there is a wide range; highly detailed sketches with lots of texture to cartoonish or child like drawings. This is definitely very different from ImageNet samples. I am not expecting reasonable performance.","886c04c5":"# Best and Worst Predictions\nWe're going to re-run inference on one of our better models -- a ResNext101-32x16 pretrained on Instagram tags. We'll collect per-example losses and top-5 predictions and then display the results.","4bda5980":"# Top-5 Differences\nThe same as above for the Top-5 validation scores. Much the same.","1ac9f58f":"# Results\nWe're going to walk through the results as follows:\n1. Top-1 results by model\n2. Compare differences for Top-1 accuracy between each model on ImageNet-1K and ImageNet-Sketch\n3. Same as above, but for Top-5 differences","b1f36aa0":"# The Models\nThe pretrained models that will be run cover a wide range of performance on ImageNet-1k -- from mobile optimized MobileNet-V3, through the workhorse range in the the ResNet50, SE-ResNext50, to some larger, higher performing models like PNASNet5-Large and EfficientNet-B5, and finally ending up with the full set of released Instagram pretrained ResNeXt101 models.","fca9c106":"# Top-1 Differences\n> Looking a bit closer, we plot the differences between the ImageNet-Sketch Top-1 validation scores and the original ImageNet-1k validation scores. Again, the IG ResNeXts stand apart. Also visible are other model to model variations in how well they do on Sketch vs original validation. The InceptionResnetV2 is one of the less impacted non IG models, while the EfficientNet-B2 and MobileNet-V3 are hit the hardest."}}