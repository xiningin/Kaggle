{"cell_type":{"87663786":"code","3442647a":"code","3af6b2ac":"code","b9436839":"code","ec38af0f":"code","25170449":"code","fd14374e":"code","59f41441":"code","df71d119":"code","5f27ae21":"code","de10de26":"code","e6c7e28e":"code","c870e13a":"code","49ed6d73":"code","8cb3d91d":"code","2f9461f9":"code","b99b5702":"code","4b0d19de":"code","f7317013":"code","8d1048ba":"code","e973f439":"code","89e01aef":"code","0e6acf31":"code","58e41c72":"code","3f09d342":"code","a4e6e8cc":"code","37fa0d43":"code","6ae18bb3":"code","9b28d049":"code","cbad33ee":"code","27053734":"code","1174d52b":"code","2e247f38":"code","35bc6f48":"code","88cd8cff":"code","5fea0d9c":"code","68570614":"code","a101911e":"code","695c26be":"code","8244d799":"code","2505d9d5":"code","f5b28d41":"code","59cf2f09":"code","867a4a1c":"code","62f703bb":"code","88f9569f":"code","172a8882":"code","2d03a78c":"code","a05395fb":"code","3fecd2e9":"code","aa45c33f":"code","4605cde9":"markdown","b224d42f":"markdown","79f099b9":"markdown","8d4f20c6":"markdown","99e82b23":"markdown","5c5a282a":"markdown","6ee8c2bf":"markdown","dd3114aa":"markdown","6374159d":"markdown","b99ac9d3":"markdown","73682f01":"markdown","f9965832":"markdown"},"source":{"87663786":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3442647a":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","3af6b2ac":"print(train_df.shape)\nprint(test_df.shape)","b9436839":"train_df.head()","ec38af0f":"test_df.head()","25170449":"train_df['label'].value_counts()","fd14374e":"pos_tweet = train_df['label'].value_counts()[0]\nneg_tweet = train_df['label'].value_counts()[1]\n\ntotal = len(train_df)\n\nprint(\"percentage of positive tweets : \", (pos_tweet\/total)*100 )\nprint(\"percentage of negative tweets : \", (neg_tweet\/total)*100)","59f41441":"train_df[train_df['label'] == 1].head()","df71d119":"train_df['tweet'][0]","5f27ae21":"class Decontract():\n    \n    pattern = [(r'won\\'t', ' will not'),\n                 (r'\\'s', ' is'),\n                 (r'don\\'t', ' do not'),\n                 (r'can\\'t', ' can not'),\n                 (r'n\\'t','not'),\n                 (r'\\'re','are'),\n                 (r'\\'d','would'),\n                 (r'\\'ll','will'),\n                 (r'\\'t','not'),\n                 (r'\\'ve','have'),\n                 (r'\\'m','am')\n                ]\n    \n    def __init__(self) :\n        pass\n        \n    def replace(self,text):\n        for (raw,replace) in self.pattern:\n            regex = re.compile(raw)\n            text = regex.sub(replace,text)\n        return text","de10de26":"import re\nfrom bs4 import BeautifulSoup \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndecontract = Decontract() \nstopWords = set(stopwords.words('english'))\n\ndef preprocessing(sentence) :\n    \n    #removing tweeter handles\n    sentence = re.sub(r'@[\\w]*','', sentence)\n    \n    #removing html tags\n    sentence = BeautifulSoup(sentence, 'html.parser').get_text()\n    \n    #Decontract words\n    sentence = decontract.replace(sentence)\n    \n    #remove non-alphabetic characters\n    sentence = re.sub('[^A-Za-z]+',\" \", sentence)\n    \n    #remove all words with length less than 3 and lower case the words\n    sentence = ' '.join([word.lower() for word in sentence.split() if len(word)>3])\n\n    #removing stop words\n    sentence = ' '.join([word for word in sentence.split() if word not in stopWords])\n    \n    #stemming the word using porterStemmer\n    sentence = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(sentence)])\n    \n    return sentence\n    \n    \n    ","e6c7e28e":"train_df['preprocessed_tweet'] = train_df['tweet'].apply(preprocessing)","c870e13a":"train_df.head()","49ed6d73":"train_df.drop(['id','tweet'],axis = 1, inplace = True)\ntrain_df.head()","8cb3d91d":"y = train_df['label']\ntrain_df.drop(['label'],axis = 1, inplace = True)\ntrain_df.head()","2f9461f9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.33, stratify=y)\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, stratify=y_train)","b99b5702":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nvectorizer.fit(X_train['preprocessed_tweet'].values)\n\nX_train_tfidf = vectorizer.transform(X_train['preprocessed_tweet'].values)\nX_cv_tfidf = vectorizer.transform(X_cv['preprocessed_tweet'].values)\nX_test_tfidf = vectorizer.transform(X_test['preprocessed_tweet'].values)\n\nprint(\"After vectorizations\")\nprint(X_train_tfidf.shape, y_train.shape)\nprint(X_cv_tfidf.shape, y_cv.shape)\nprint(X_test_tfidf.shape, y_test.shape)\nfeature_names = vectorizer.get_feature_names()","4b0d19de":"#sentiments strength of tweet of X_train\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer \nfrom tqdm import tqdm\n\nsid = SentimentIntensityAnalyzer() \n\nneg_sentiment = []\nneu_sentiment = []\npos_sentiment = []\ncom_sentiment = []\n\nfor sentence in tqdm(X_train['preprocessed_tweet']):\n    vector = sid.polarity_scores(sentence)\n    neg_sentiment.append(vector['neg'])\n    neu_sentiment.append(vector['neu'])\n    pos_sentiment.append(vector['pos'])\n    com_sentiment.append(vector['compound'])\n    \nX_train_neg_sentiment = np.array(neg_sentiment).reshape(-1,1)\nX_train_neu_sentiment = np.array(neu_sentiment).reshape(-1,1)\nX_train_pos_sentiment = np.array(pos_sentiment).reshape(-1,1)\nX_train_com_sentiment = np.array(com_sentiment).reshape(-1,1)","f7317013":"#sentiments strength of tweet of X_cv\n\nsid = SentimentIntensityAnalyzer() \n\nneg_sentiment = []\nneu_sentiment = []\npos_sentiment = []\ncom_sentiment = []\n\nfor sentence in tqdm(X_cv['preprocessed_tweet']):\n    vector = sid.polarity_scores(sentence)\n    neg_sentiment.append(vector['neg'])\n    neu_sentiment.append(vector['neu'])\n    pos_sentiment.append(vector['pos'])\n    com_sentiment.append(vector['compound'])\n    \nX_cv_neg_sentiment = np.array(neg_sentiment).reshape(-1,1)\nX_cv_neu_sentiment = np.array(neu_sentiment).reshape(-1,1)\nX_cv_pos_sentiment = np.array(pos_sentiment).reshape(-1,1)\nX_cv_com_sentiment = np.array(com_sentiment).reshape(-1,1)","8d1048ba":"#sentiments strength of tweet of X_test\n\nsid = SentimentIntensityAnalyzer() \n\nneg_sentiment = []\nneu_sentiment = []\npos_sentiment = []\ncom_sentiment = []\n\nfor sentence in tqdm(X_test['preprocessed_tweet']):\n    vector = sid.polarity_scores(sentence)\n    neg_sentiment.append(vector['neg'])\n    neu_sentiment.append(vector['neu'])\n    pos_sentiment.append(vector['pos'])\n    com_sentiment.append(vector['compound'])\n    \nX_test_neg_sentiment = np.array(neg_sentiment).reshape(-1,1)\nX_test_neu_sentiment = np.array(neu_sentiment).reshape(-1,1)\nX_test_pos_sentiment = np.array(pos_sentiment).reshape(-1,1)\nX_test_com_sentiment = np.array(com_sentiment).reshape(-1,1)","e973f439":"from scipy.sparse import hstack\n\nX_train = hstack((X_train_neg_sentiment,X_train_neu_sentiment,X_train_pos_sentiment,X_train_com_sentiment,X_train_tfidf )).tocsr()\nX_cv = hstack((X_cv_neg_sentiment,X_cv_neu_sentiment,X_cv_pos_sentiment,X_cv_com_sentiment,X_cv_tfidf )).tocsr()\nX_test = hstack((X_test_neg_sentiment,X_test_neu_sentiment,X_test_pos_sentiment,X_test_com_sentiment,X_test_tfidf )).tocsr()\n\nprint(X_train.shape, y_train.shape)\nprint(X_cv.shape, y_cv.shape)\nprint(X_test.shape, y_test.shape)","89e01aef":"import matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import roc_auc_score\n\ntrain_auc = []\ncv_auc = []\nalpha = [0.00001,0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n\nfor a in alpha:\n    clf = SGDClassifier(loss = 'log', penalty = 'l2' , alpha = a,class_weight = \"balanced\")\n    clf.fit(X_train,y_train)\n    y_train_pred = clf.predict_proba(X_train)[:,1]\n    y_cv_pred = clf.predict_proba(X_cv)[:,1]\n    \n    train_auc.append(roc_auc_score(y_train,y_train_pred))\n    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\n    \nplt.plot(np.log(alpha), train_auc, label = \"Train AUC\")\nplt.plot(np.log(alpha), cv_auc, label = \"CV AUC\")\n\nplt.scatter(np.log(alpha), train_auc, label = \"Train AUC points\")\nplt.scatter(np.log(alpha), cv_auc, label = \"CV AUC points\")\n\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"Error Loss\")\nplt.grid()\nplt.show()","0e6acf31":"lr_tfidf_best_alpha = 0.1","58e41c72":"from sklearn.metrics import roc_curve,auc\n\nlr = SGDClassifier(loss = 'log', alpha = lr_tfidf_best_alpha, penalty = 'l2',class_weight = \"balanced\")\nlr.fit(X_train, y_train)\n\ny_train_pred = lr.predict_proba(X_train)[:,1]\ny_test_pred = lr.predict_proba(X_test)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_cv, y_cv_pred)\n\ntrain_auc_tfidf_lr = auc(train_fpr,train_tpr)\ntest_auc_tfidf_lr = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr,train_tpr,label = \"Train Auc = \"+str(train_auc_tfidf_lr))\nplt.plot(test_fpr,test_tpr,label = \"Test Auc = \"+str(test_auc_tfidf_lr))\nplt.legend()\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","3f09d342":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train Confusion matrix\")\ntrain_cm = confusion_matrix(y_train,lr.predict(X_train))\nax = sns.heatmap(train_cm, annot=True, fmt=\"d\", linewidths=.5, cbar=False)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.title(\"Confusion matrix\")\nax","a4e6e8cc":"print(\"Test Confusion matrix\")\ntrain_cm = confusion_matrix(y_test,lr.predict(X_test))\nax = sns.heatmap(train_cm, annot=True, fmt=\"d\", linewidths=.5, cbar=False)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.title(\"Confusion matrix\")\nax","37fa0d43":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\ntrain_auc = []\ncv_auc = []\n\nalpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\npenalty = ['l1','l2']\n\nparameters = {'alpha' : alpha, 'penalty' : penalty}\n\nsvm = SGDClassifier(loss = 'hinge', class_weight = 'balanced')\nclf = GridSearchCV(svm, parameters, scoring = 'roc_auc', cv=8)\nclf.fit(X_train, y_train) \n\nprint(clf.best_estimator_.alpha)\nprint(clf.best_estimator_.penalty)","6ae18bb3":"print(clf.score(X_test,y_test))","9b28d049":"lr_svm_tfidf_best_alpha = 0.01\nlr_svm_tfidf_best_penalty = 'l2'","cbad33ee":"from sklearn.calibration import CalibratedClassifierCV\n\nlr_svm = SGDClassifier(loss = 'hinge', class_weight = 'balanced', alpha = lr_svm_tfidf_best_alpha, penalty = lr_svm_tfidf_best_penalty )\n\n#configuring calibrated model to obtain output probabilities, because SGDClassifier with hinge los s don't give output probabilities \nlr_svm_cc = CalibratedClassifierCV(lr_svm, cv = 8)\nlr_svm_cc.fit(X_train,y_train)\n\ny_train_pred = lr_svm_cc.predict_proba(X_train)[:,1]\ny_test_pred = lr_svm_cc.predict_proba(X_test)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train,y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_tfidf_lr_svm = auc(train_fpr, train_tpr)\ntest_auc_tfidf_lr_svm = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr, train_tpr, label = \"Train Auc = \"+str(train_auc_tfidf_lr_svm))\nplt.plot(test_fpr, test_tpr, label = \"Test Auc = \"+str(test_auc_tfidf_lr_svm))\n\nplt.legend()\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","27053734":"from sklearn.ensemble import RandomForestClassifier\n\ntrain_auc = []\ncv_auc = []\n\nmax_depth = [1, 5, 10, 50, 100, 500, 1000]\nn_estimators  = [5, 10, 100, 500, 1000]\n\nfor estimator in n_estimators:\n    train_lst = []\n    cv_lst = []\n    for depth in max_depth:\n        clf = RandomForestClassifier(class_weight = \"balanced_subsample\", max_depth = depth, n_estimators = estimator, n_jobs = -1)\n        clf.fit(X_train, y_train)\n        \n        y_train_pred = clf.predict_proba(X_train)[:,1]\n        y_cv_pred = clf.predict_proba(X_cv)[:,1]\n        \n        train_lst.append(roc_auc_score(y_train,y_train_pred))\n        cv_lst.append(roc_auc_score(y_cv,y_cv_pred))\n        \n    train_auc.append(train_lst)\n    cv_auc.append(cv_lst)","1174d52b":"# creating dataframe of train auc for heatmap\ndf = pd.DataFrame(train_auc)","2e247f38":"#heat map of train auc\nax = sns.heatmap(df, annot=True, linewidths=.5, cbar=False,yticklabels=n_estimators, xticklabels=max_depth)\nax\nplt.xlabel(\"max_depth\");\nplt.ylabel(\"n_estimators\")","35bc6f48":"#creating data frame of cv_auc for heat map\ndf = pd.DataFrame(cv_auc)","88cd8cff":"#heat map of cv auc\nax = sns.heatmap(df, annot = True, linewidths=.5, cbar=False,yticklabels=n_estimators, xticklabels=max_depth)\nax\nplt.xlabel(\"max_depth\");\nplt.ylabel(\"n_estimators\")","5fea0d9c":"rf_best_max_depth = 10\nrf_best_n_estimators = 500","68570614":"rf = RandomForestClassifier(class_weight = \"balanced_subsample\", max_depth = rf_best_max_depth, n_estimators = rf_best_n_estimators, n_jobs = -1)\nrf.fit(X_train,y_train)\ny_train_pred = rf.predict_proba(X_train)[:,1]\ny_test_pred = rf.predict_proba(X_test)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_tfidf_rf = auc(train_fpr, train_tpr)\ntest_auc_tfidf_rf = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr, train_tpr, label = \"Train AUC \" + str(train_auc_tfidf_rf))\nplt.plot(test_fpr, test_tpr, label = \"Test AUC \" +str(test_auc_tfidf_rf))\nplt.legend()\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC\")\nplt.show()","a101911e":"from xgboost.sklearn import XGBClassifier\n\ntrain_auc = []\ncv_auc = []\n\nmax_depth = [3,5,7,9]\nn_estimators = [5,10,100,500,1000]\n\nfor estimator in n_estimators :\n    train_lst = []\n    cv_lst = []\n    for depth in max_depth : \n        xgb_model = XGBClassifier(max_depth = depth, n_estimators = estimator, n_jobs = -1)\n        xgb_model.fit(X_train, y_train)\n        \n        y_train_pred = xgb_model.predict_proba(X_train)[:,1]\n        y_cv_pred = xgb_model.predict_proba(X_cv)[:,1]\n        \n        train_lst.append(roc_auc_score(y_train, y_train_pred))\n        cv_lst.append(roc_auc_score(y_cv, y_cv_pred))\n        \n    train_auc.append(train_lst)\n    cv_auc.append(cv_lst)\n    \n","695c26be":"df = pd.DataFrame(train_auc)\n\nax = sns.heatmap(df, annot=True, linewidths=.5, cbar=False,yticklabels=n_estimators, xticklabels=max_depth)\nax\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"n_estimators\")","8244d799":"df = pd.DataFrame(cv_auc)\nax = sns.heatmap(df, annot=True, linewidths=.5, cbar=False,yticklabels=n_estimators, xticklabels=max_depth)\nax\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"n_estimators\")","2505d9d5":"rf_best_max_depth = 3\nrf_best_n_estimators = 500","f5b28d41":"xgb_model = XGBClassifier(max_depth = rf_best_max_depth, n_estimators = rf_best_n_estimators, n_jobs = -1)\nxgb_model.fit(X_train, y_train)\n\ny_train_pred = xgb_model.predict_proba(X_train)[:,1]\ny_test_pred = xgb_model.predict_proba(X_test)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train,y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\ntrain_auc_tfidf_xgb = auc(train_fpr, train_tpr)\ntest_auc_tfidf_xgb = auc(test_fpr, test_tpr)\n\nplt.plot(train_fpr, train_tpr, label = \"Train AUC = \" + str(train_auc_tfidf_xgb))\nplt.plot(test_fpr, test_tpr, label = \"Test AUC = \" + str(test_auc_tfidf_xgb))\nplt.legend()\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC\")\nplt.grid()\nplt.show()","59cf2f09":"from prettytable import PrettyTable\n\nx = PrettyTable()\nx.field_names = [\"Model\", \"AUC\"]\nx.add_row([\"Logistic Regression\",test_auc_tfidf_lr])\nx.add_row([\"Linear SVM\",test_auc_tfidf_lr_svm])\nx.add_row([\"Random Forest\",test_auc_tfidf_rf])\nx.add_row([\"XGBoost\",test_auc_tfidf_xgb])\nprint(x)","867a4a1c":"test_df['preprocessed_tweet'] = test_df['tweet'].apply(preprocessing)","62f703bb":"test_df.head()","88f9569f":"test_df.drop(['id','tweet'],axis = 1, inplace = True)\ntest_df.head()","172a8882":"#applying tfidf vectorizer on preprocessed_tweet\ntest_df_tfidf = vectorizer.transform(test_df['preprocessed_tweet'].values)","2d03a78c":"#sentiments strength of tweet of test dataframe\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer \nfrom tqdm import tqdm\n\nsid = SentimentIntensityAnalyzer() \n\nneg_sentiment = []\nneu_sentiment = []\npos_sentiment = []\ncom_sentiment = []\n\nfor sentence in tqdm(test_df['preprocessed_tweet']):\n    vector = sid.polarity_scores(sentence)\n    neg_sentiment.append(vector['neg'])\n    neu_sentiment.append(vector['neu'])\n    pos_sentiment.append(vector['pos'])\n    com_sentiment.append(vector['compound'])\n    \ntest_df_neg_sentiment = np.array(neg_sentiment).reshape(-1,1)\ntest_df_neu_sentiment = np.array(neu_sentiment).reshape(-1,1)\ntest_df_pos_sentiment = np.array(pos_sentiment).reshape(-1,1)\ntest_df_com_sentiment = np.array(com_sentiment).reshape(-1,1)","a05395fb":"#creating data matrix \ntest_data_matrix = hstack((test_df_neg_sentiment,test_df_neu_sentiment,test_df_pos_sentiment,test_df_com_sentiment,test_df_tfidf )).tocsr()","3fecd2e9":"#Predicting using XGBoost model\ny_pred = xgb_model.predict(test_data_matrix)","aa45c33f":"y_pred","4605cde9":"4) XGBOOST","b224d42f":"1) Applying Logistic Regression","79f099b9":"**Preprocessing steps :**\n1. As twitter handles(@user) are not giving any useful information therefore we remove it.\n2. Remove html tags.\n3. Decontract text like won't => would not \n4. Removing punctuations, non-alphabetic character.\n5. remove small words like any, all, his, her as this words are not giving any useful information.\n6. lowercase the all the words.\n7. remove stop words.\n8. do stemming ","8d4f20c6":"<h3>Summary of each model with their respective AUC Score<\/h3>","99e82b23":"As shown above large number of tweets are not racist\/sexist.","5c5a282a":"**Vectorizing text data**","6ee8c2bf":"3) Random Forest","dd3114aa":"**Splitting data into train,cv and test**","6374159d":"**Observation :** From above table it seems that XGBoost performs better than other models therefore we use XGBoost with other vectorizer or for prediction of test dataset.","b99ac9d3":"**Modelling**","73682f01":"<h3>Prediction on Test dataset using XGBoost<\/h3>","f9965832":"2) linear SVM"}}