{"cell_type":{"617b3549":"code","06f7c5e5":"code","b8322d12":"code","a9209709":"code","231096c7":"code","85cc22a2":"code","89ce57cd":"code","8ebf826e":"code","0770ad5e":"code","c9e20c5a":"code","8f6ef9cd":"code","7a3d2336":"code","19d36834":"code","25387716":"code","87058891":"code","78043d30":"code","4294a839":"code","5d12b462":"code","1deb52ea":"code","bd76eb0c":"code","059c8401":"code","75f97193":"code","002455f5":"code","a4eea71e":"code","64c64164":"code","6eae0b88":"code","5e475966":"code","e2958b00":"code","c17e9df2":"code","899aca7b":"code","43edf8ac":"code","dba711f7":"code","a8beed2b":"code","76acee47":"code","63dafbf5":"markdown","7576cdd5":"markdown","e75507c3":"markdown","2e576553":"markdown","b390577e":"markdown","82063c51":"markdown","ab377928":"markdown","0afbcf44":"markdown","1ccfa813":"markdown","9de87323":"markdown","213ce86d":"markdown","845ce0ba":"markdown","cbf4431c":"markdown","fa0fb0c9":"markdown","37e6b877":"markdown","087fb67f":"markdown","01ae938f":"markdown","6bc87d3e":"markdown","492b889b":"markdown","70425de8":"markdown","5fe98c16":"markdown","eb08f5f6":"markdown","d8972a11":"markdown"},"source":{"617b3549":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.datasets import make_imbalance\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()","06f7c5e5":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain =  pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ntrain.head()","b8322d12":"train.info()","a9209709":"train.describe()","231096c7":"train.isnull().sum()","85cc22a2":"df_len = len(train)\ndf_all = pd.concat([train, test], sort=False, ignore_index=True)\ndf_all","89ce57cd":"df_all.boxplot(column=['SibSp', 'Parch'])","8ebf826e":"df_all[df_all['Parch']> 6].style.background_gradient(cmap='viridis', subset=['Parch'])","0770ad5e":"display(df_all[df_all['SibSp']> 6].style.background_gradient(cmap='viridis', subset=['SibSp']))","c9e20c5a":"df_all.boxplot(column=['Fare'])","8f6ef9cd":"with pd.option_context('display.precision', 2):\n    display(df_all[df_all['Fare']> 300].style.background_gradient(cmap='viridis', subset=['Fare']))","7a3d2336":"df_all.boxplot(column=['Age'])","19d36834":"df_all[df_all['Age']> 70].style.background_gradient(cmap='viridis', subset=['Age'])","25387716":"sns.countplot(x='Survived', data=train)","87058891":"sns.countplot(x='Survived', hue='Sex', data=train)","78043d30":"sns.countplot(x='Survived', hue='Pclass', data=train)","4294a839":"df_all['Embarked'] = df_all['Embarked'].fillna(df_all['Embarked'].mode()[0]) # The mode is the value that appears most\ndf_all['Cabin'] = df_all['Cabin'].fillna('U') # U for Unknown, later we'll plot this\n\ndf_all['Ticket'] = df_all['Ticket'].fillna('Unknown') #Ticket is also unknown\n\n# # Filling the missing value in Fare with the median Fare of 3rd class alone passenger\ndf_all['Fare'] = df_all['Fare'].fillna(df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]) \n\n#Age is filled with the median of ages by Sex and by Pclass\ndf_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","5d12b462":"df_all.isnull().sum()","1deb52ea":"df_all['Cabin'] = df_all['Cabin'].apply(lambda x: x[0])\n\nidx = df_all[df_all['Cabin'] == 'T'].index\ndf_all.loc[idx, 'Cabin'] = 'A'\n\ndf_all['Cabin'] = df_all['Cabin'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Cabin'] = df_all['Cabin'].replace(['D', 'E'], 'DE')\ndf_all['Cabin'] = df_all['Cabin'].replace(['F', 'G'], 'FG')","bd76eb0c":"train = df_all[:df_len]\nsns.catplot(x='Survived', hue='Cabin', col='Pclass',kind=\"count\", data=train)","059c8401":"names = df_all['Name'].str.split(',')\nnames = names.apply(lambda x: x[1]).str.split(\" \")\nnames = names.apply(lambda x: x[1])\ndf_all['Title'] = names\n\ndf_all[\"Title\"] = df_all[\"Title\"].replace(to_replace=[\"Major.\" ,\"Col.\",\"Capt.\",\"the\",\"Don.\",\n                                                      \"Jonkheer.\",\"Lady.\",\"Sir.\", \"Dona.\",\n                                                     \"Rev.\", \"Dr.\"], value=\"Noble\") \ndf_all[\"Title\"] = df_all[\"Title\"].replace(to_replace=[\"Mlle.\",\"Ms.\"],value=\"Miss.\") \ndf_all[\"Title\"] = df_all[\"Title\"].replace(to_replace=[\"Mme.\"], value=\"Mrs.\") \n\ndf_all[\"Title\"].value_counts()","75f97193":"surnames = df_all['Name'].str.split(',')\nsurnames = surnames.map(lambda x: x[0])\ndf_all['Surname'] = surnames","002455f5":"df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')\n\ndf_all['Family'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family'] = df_all['Family'].map(family_map)\n\ndf_all['FareBin'] = pd.qcut(df_all['Fare'], 4)\n\ndf_all['AgeBin'] = pd.cut(df_all['Age'].astype(int), 5)","a4eea71e":"df_all = df_all.drop(['SibSp','Parch','Fare','Age','Name','Ticket','PassengerId'], axis=1)\n\n# df_all[\"Sex\"], _ = pd.factorize(df_all[\"Sex\"], sort=True)\ndf_all[\"Embarked\"], _ = pd.factorize(df_all[\"Embarked\"], sort=True)\ndf_all[\"Title\"], _ = pd.factorize(df_all[\"Title\"], sort=True)\ndf_all[\"Cabin\"], _ = pd.factorize(df_all[\"Cabin\"], sort=True)\ndf_all[\"AgeBin\"], _ = pd.factorize(df_all[\"AgeBin\"], sort=True)\ndf_all[\"FareBin\"], _ = pd.factorize(df_all[\"FareBin\"], sort=True)\ndf_all[\"Family\"], _ = pd.factorize(df_all[\"Family\"], sort=True)\ndf_all[\"Surname\"], _ = pd.factorize(df_all[\"Surname\"], sort=True)\n\ndf_all = pd.get_dummies(df_all) #Sex\ndf_all.head()","64c64164":"X = df_all[:df_len]\nX_test = df_all[df_len:]","6eae0b88":"corr = X.corr()\nfig, ax = plt.subplots(figsize=(7,7))\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True,\n    ax=ax\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","5e475966":"Y = X['Survived']\nX = X.drop('Survived', axis=1)\nX_test = X_test.drop('Survived', axis=1)","e2958b00":"from collections import Counter\n\ndef ratio_func(y, multiplier, minority_class):\n    target_stats = Counter(y)\n    return {minority_class: int(multiplier * target_stats[minority_class])}\n\nmultipliers = [0.6, 0.4, 0.2]\nfor i, multiplier in enumerate(multipliers, start=1):\n\n    X_, y_ = make_imbalance(X, Y, sampling_strategy=ratio_func,\n                            **{\"multiplier\": multiplier,\n                               \"minority_class\": 1})\n    \n    X = X.append(X_)\n    Y = Y.append(y_)\n    \nlen(Y)","c17e9df2":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nss.fit(X)\nX = ss.transform(X)\nX_test = ss.transform(X_test)\n\nY = Y.values","899aca7b":"X_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.3, random_state=42)\n\nprint('X_train: {}, x_test {}, y_train {}, y_test {}'.format(X_train.shape, x_test.shape, y_train.shape, y_test.shape))","43edf8ac":"rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=10, max_features='sqrt',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=2, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=4,\n                       oob_score=True, verbose=0,\n                       )\n\nmodel = BaggingClassifier(base_estimator=rf, n_estimators=10, random_state=0)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(x_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: {}\".format(acc))","dba711f7":"rf.fit(X, Y)\nindex = [\"Pclass\", \"Cabin\", \"Embarked\", \"Title\", \"Family\", \"Ticket_Frequency\", \"FareBin\", \"AgeBin\", \"Sex_female\", \"Sex_male\", \"Surname\"]\nfi = pd.DataFrame(rf.feature_importances_, index=index)\ndisplay(fi.style.bar(color='#d65f5f'))","a8beed2b":"prediction = model.predict(X_test)\n\nprediction = prediction.reshape(prediction.shape[0])\nprediction = np.round(prediction,0)\nprediction = prediction.astype(int)","76acee47":"sample[\"Survived\"] = prediction\nsample.to_csv(\"submission.csv\", index=False)\nsample.head()","63dafbf5":"Scale the data between -1 and 1.","7576cdd5":"# 2. EDA (Exploratory Data Analysis)\n\n* First, we'll look at the missing data\n* Then we'll look at the proportion of who survived and who didn't survived \n* After the **feature engineering** we can plot some more charts","e75507c3":"Merge the cabins in three classes. ","2e576553":"Separate train and test datasets","b390577e":"# 6. Predict and save","82063c51":"Extract the title from the name, and the surnames","ab377928":"# 1. Import libraries and data\n\n* Import Numpy and pandas for data manipulation\n* Import **sklearn** ensemble algorithms: *Bagging classifier* and *Random Forest classifier*\n* Import **sklearn** accuracy metric and StandardScaler (scales between 0 and 1)\n* Import chart utilities (matplotlib and seaborn)\n\n","0afbcf44":"# 4. *Re-sampling* of imbalanced data\n\nWe'll create new data using the *make_imbalance* method. This method is going to balance the data too (Survived=0 == Survived=1).","1ccfa813":"# 5. Training the *Random Forest* Algorithm with the *Bagging Classifier*","9de87323":"Training. The RandomForestClassified parameters was chosen using the GridSearchCV.","213ce86d":"Looking at the **Parch** (Parent and children) and **Sibsp** (Sibling and spouse), we can see all the values  greater than 6 that look like outliers are actually from all the *Sage* family.","845ce0ba":"### Now, we'll look at some Survived or not data\n\nThere's at least 150 more deaths than survived in the training data.","cbf4431c":"### Create new features\n\n* Ticket_frequency for the count of each ticket\n* Family that's the union of SibSp + Parch + 1 (the person itself)\n* The Family is mapped by size (alone, small, medium and large)\n* The FareBin maps the Fare into 4 intervals\n* The AgeBin maps the Age into 5 intervals","fa0fb0c9":"# 3. Feature engineering\n\nFirst, we'll impute some missing data","37e6b877":"### 1.1. Concatenate the train and test data","087fb67f":"Split into train and validation sets.","01ae938f":"As expected, more women survived. \"Women and children first!\"","6bc87d3e":"The \\$512.33 fare actually is from the same ticket, so probably isn't an outlier.","492b889b":"Drop the columns that will not be used.\n\nFactorize the categorical data (**One hot encoding** for the Sex feature).","70425de8":"Create Y and drop the *Survived* column.","5fe98c16":"As expected\u00b2, more deaths on the third class","eb08f5f6":"See the most important features","d8972a11":"See the correlation between the variables"}}