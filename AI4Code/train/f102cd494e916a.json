{"cell_type":{"b4c2d845":"code","c0c671e5":"code","c0d9eb68":"code","d4a4c171":"code","909a489d":"code","239a959b":"code","00d8f5c2":"code","175e478d":"code","2a168872":"code","5a23b319":"code","1d406958":"code","a91e655f":"code","4c9573ab":"code","ad7970dd":"code","c098ebc4":"code","0b8e2e71":"code","df930369":"code","6fb3b41d":"code","cba56de5":"code","0c84330e":"code","aa88b695":"code","4665eab1":"code","60e7d7ce":"code","81a0b9d3":"code","defdc524":"code","347bea1d":"code","81bfae4c":"code","f9afde8c":"code","e64c6507":"code","55be5d1a":"code","cb234a54":"code","2dca5c26":"code","847079f5":"code","c4cd100d":"code","3cff4501":"code","100ac501":"code","9442c0cb":"code","82df1597":"code","31accb04":"code","b7ec3f03":"code","54075ab6":"code","8c869fb6":"code","c0a9afcb":"code","ad94aa4d":"code","92834520":"code","05e0388f":"code","51cd12d2":"code","d6875d17":"code","d0906e5a":"code","cf27d11a":"code","48b07fb0":"code","1415e4f6":"code","4bd27396":"code","3251283e":"code","5adf5ee5":"code","cbf907d7":"code","cc1d4af5":"code","0b99b7af":"code","e72fc5c4":"code","dee635e5":"code","4b265221":"code","de6381c9":"code","4b18db25":"code","2e36d80d":"code","75ebe23c":"code","07a739d6":"code","0b803581":"code","84d1ce1e":"code","6986989a":"code","ba596730":"code","236f5809":"code","9f35e8d8":"code","fc6d1a13":"code","9bad4025":"code","0da2a532":"code","2463db17":"code","6ecb7a0a":"code","a76a62c7":"code","7daf29db":"code","41379def":"markdown","bff324b3":"markdown","dace84a3":"markdown","0521926f":"markdown","00c1e4d6":"markdown","31b582cd":"markdown","a96382b4":"markdown","24f201b7":"markdown","2f791129":"markdown","580472db":"markdown","17b0dc94":"markdown","4d9180a6":"markdown","18b75d42":"markdown","356e7944":"markdown","12d855a8":"markdown","6b239a78":"markdown","97b14042":"markdown","173cc7a6":"markdown","f3b18f9f":"markdown","531d1215":"markdown","464e7e8a":"markdown","eb8f1e0b":"markdown","252e86e7":"markdown"},"source":{"b4c2d845":"pip install nlpaug","c0c671e5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nimport textblob\nfrom textblob import TextBlob, Word\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c0d9eb68":"traindata=pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')\ntestdata=pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv')","d4a4c171":"traindata.head()","909a489d":"testdata.head()","239a959b":"traindata['label'].value_counts()","00d8f5c2":"%matplotlib inline\nsns.countplot(traindata['label'])\nplt.title('Class-Distribution')","175e478d":"import nlpaug.augmenter.sentence as nas\n","2a168872":"WANDB_API_KEY='sonu'","5a23b319":"aug = nas.ContextualWordEmbsForSentenceAug(model_path='xlnet-base-cased')","1d406958":"text='am not interested in linguistics that does not address race racism is about power raciolinguistics brings'\naugmented_texts = aug.augment(text, n=3)\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Texts:\")\nprint(augmented_texts)\n","a91e655f":"augmented_texts","4c9573ab":"ls=[]\ndef data_augument(df):\n    augmented_texts = aug.augment(df, n=10)\n    for i in augmented_texts:\n        ls.append(i)\n    return(augmented_texts)\n","ad7970dd":"from tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()","c098ebc4":"traindata[traindata['label']==1]['tweet'].progress_apply(data_augument)","0b8e2e71":"array=np.array(ls)","df930369":"np.save('array',array)\n","6fb3b41d":"augmented_texts=np.load('..\/input\/array-file\/array.npy')","cba56de5":"aug_data=pd.DataFrame(augmented_texts,columns=['tweet'])","0c84330e":"aug_data['label']=1","aa88b695":"aug_data.head()","4665eab1":"traindata=pd.concat([traindata.drop(columns=['id']), aug_data], join=\"outer\").sample(frac=1).reset_index(drop=True)","60e7d7ce":"traindata.head()","81a0b9d3":"\nplt.pie(traindata['label'].value_counts(), autopct='%1.1f%%', shadow=True,labels=['Negative Class','Positive Class'])\nplt.title('Class Distribution');\nplt.show()","defdc524":"traindata['preclean_no_words']=  [len(t) for t in traindata.tweet]\nsns.boxplot(traindata.preclean_no_words)","347bea1d":"#traindata['preclean_no_words']=  [len(t) for t in traindata.tweet]\nsns.boxplot(traindata[traindata['label']==0].preclean_no_words)","81bfae4c":"sns.boxplot(traindata[traindata['label']==1].preclean_no_words)","f9afde8c":"traindata['no_of_characters']=traindata['tweet'].str.len()\ntraindata.head()","e64c6507":"%matplotlib inline\nplt.figure(figsize=(8,8))\nsns.distplot(traindata['no_of_characters'])","55be5d1a":"traindata['no_of_words']=traindata['tweet'].apply(lambda x: len(str(x).split(\" \")))\ntraindata.head()","cb234a54":"plt.figure(figsize=(8,8))\nsns.distplot(traindata['no_of_words'])","2dca5c26":"print(traindata['no_of_characters'].max(),\"Max'm of all characters\")\nprint(traindata['no_of_words'].max(),\"Max'm of all words\")\n","847079f5":"traindata['no_of_hash']=traindata['tweet'].apply(lambda x:len([x for x in x.split() if x.startswith('#')]))\ntraindata.head()","c4cd100d":"testdata['no_of_hash']=testdata['tweet'].apply(lambda x:len([x for x in x.split() if x.startswith('#')]))\ntestdata.head()","3cff4501":"plt.figure(figsize=(8,8))\nsns.distplot(traindata['no_of_hash'])","100ac501":"traindata['no_of_digits']=traindata['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ntraindata.head()","9442c0cb":"testdata['no_of_digits']=testdata['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ntestdata.head()","82df1597":"plt.figure(figsize=(8,8))\nsns.countplot(traindata['no_of_digits'])","31accb04":"train_process=traindata.copy()","b7ec3f03":"train_process['tweet_lowercase'] = train_process['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntrain_process[['tweet', 'tweet_lowercase']].tail()","54075ab6":"testdata['tweet_lowercase'] = testdata['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntestdata[['tweet', 'tweet_lowercase']].tail()","8c869fb6":"stop_words = stopwords.words('english')+['0624',\n '07800',\n '07950',\n '08a',\n '100',\n '1000',\n '1000x',\n '100k',\n '101',\n '106',\n '10alltypespos',\n '10am',\n '10days',\n '10k',\n '10th',\n '1117',\n '11400',\n '11th',\n '1200',\n '123',\n '12313',\n '1299',\n '12mill',\n '13479',\n '13th',\n '13thdocumentary',\n '140',\n '14000',\n '14200',\n '142017',\n '148',\n '1499',\n '14th',\n '1500',\n '15000',\n '150516',\n '15thcentury',\n '160',\n '1600',\n '1625',\n '17th',\n '180',\n '18th',\n '1900',\n '190k',\n '1930s',\n '1960',\n '1968',\n '1970',\n '1980',\n '1996',\n '1999',\n '19th',\n '1gabba',\n '1pun',\n '1st',\n '1stammendment',\n '2',\n '200',\n '2000',\n '2001',\n '2002',\n '2003',\n '2004',\n '2006',\n '2008',\n '2009',\n '200k',\n '2010',\n '2011',\n '2012',\n '2013',\n '2014',\n '2015',\n '2016',\n '201617',\n '2016a',\n '2016election',\n '2016highlights',\n '2016ia',\n '2016in4a',\n '2016in4worda',\n '2016in4words',\n '2016in4wordsa',\n '2016in4worlds',\n '2016ina',\n '2016release',\n '2017',\n '2017fail',\n '2017in3words',\n '2017in3wordsa',\n '2017npr',\n '2018',\n '201a',\n '20days',\n '20th',\n '2100',\n '21st',\n '230pmet',\n '23rd',\n '247',\n '24h',\n '24hrs',\n '24th',\n '25th',\n '26th',\n '280',\n '299',\n '2a',\n '2day',\n '2days',\n '2i',\n '2ia1',\n '2k16',\n '2nd',\n '2nites',\n '2nn\u00f03',\n '2pac',\n '2pm',\n '2raise',\n '2stand',\n '2the',\n '2\u00f0',\n '2\u00f0n',\n '2\u00f0\u00f0',\n '2\u00f0\u00f01',\n '2\u00f0\u00f0\u00f01',\n '2\u00f0\u00f0\u00f0\u00f01',\n '30th',\n '342',\n '350',\n '35th',\n '360',\n '38billion',\n '399',\n '3rd',\n '4',\n '400',\n '400000',\n '40404',\n '41',\n '4a',\n '4a1',\n '4aa1',\n '4ai',\n '4ai\u00f0',\n '4ejapan',\n '4i',\n '4i1',\n '4maps',\n '4n\u00f03',\n '4o3',\n '4o4o4',\n '4pm',\n '4sa',\n '4th',\n '4wd',\n '4\u00e6a',\n '4\u00f0',\n '4\u00f01',\n '4\u00f03',\n '4\u00f0a\u00f0',\n '4\u00f0o3',\n '4\u00f0\u00f0',\n '4\u00f0\u00f01',\n '4\u00f0\u00f03',\n '4\u00f0\u00f0\u00f0',\n '4\u00f0\u00f0\u00f01',\n '4\u00f0\u00f0\u00f03',\n '4\u00f0\u00f0\u00f0\u00f0',\n '4\u00f0\u00f0\u00f0\u03bc\u00f01',\n '4\u03bc',\n '4\u03bco3',\n '50',\n '500',\n '50islamicinfo',\n '50th',\n '564943',\n '5hrs',\n '5sos',\n '5th',\n '5wordtrumplethinskin',\n '60',\n '600',\n '60minutes',\n '630',\n '6417153640',\n '642',\n '6pm',\n '6th',\n '6yearolds',\n '70',\n '700',\n '700000',\n '703',\n '799',\n '800',\n '80snostalgia',\n '80yrold',\n '8990',\n '8pm',\n '8th',\n '900',\n '90th',\n '911',\n '940pm',\n '946',\n '952',\n '999',\n '99c',\n '99c99p',\n '99c99pa',\n '99ca',\n '99cents',\n '99p',\n '9am',\n '9pm',\n '9th',\n '__luicalibre__s',\n '_animaladvocate',\n '_\u00f8u\u00f8u',\n 'a1',\n 'a15',\n 'a17',\n 'a1aaaa',\n 'a1i',\n 'aa1',\n 'aaa',\n 'aaaa',\n 'aaaaa',\n 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n 'aaahis',\n 'aaall',\n 'aaaplay',\n 'aampe',\n 'aand',\n 'aande',\n 'aanne',\n 'aantiislamista',\n 'aap',\n 'aape',\n 'aaron',\n 'abandoned',\n 'abba',\n 'abc',\n 'abd',\n 'abe',\n 'abeed']\n","c0a9afcb":"stop_words","ad94aa4d":"train_process['tweet_stopwords'] = train_process['tweet_lowercase'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\ntrain_process[['tweet_stopwords','tweet']].head()\n","92834520":"testdata['tweet_stopwords'] = testdata['tweet_lowercase'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\ntestdata[['tweet_stopwords','tweet']].head()","05e0388f":"train_process['tweet_punctuation'] = train_process['tweet_stopwords'].str.replace('[^\\w\\s]', '')\ntrain_process[['tweet', 'tweet_punctuation']].head()\n","51cd12d2":"testdata['tweet_punctuation'] = testdata['tweet_stopwords'].str.replace('[^\\w\\s]', '')\ntestdata[['tweet', 'tweet_punctuation']].head()","d6875d17":"train_process['tweet_single_letter']=train_process['tweet_punctuation'].apply(lambda words: ' '.join( [w for w in words.split() if len(w)>2] ))\ntrain_process[['tweet_single_letter','tweet']].head()","d0906e5a":"testdata['tweet_single_letter']=testdata['tweet_punctuation'].apply(lambda words: ' '.join( [w for w in words.split() if len(w)>2] ))\ntestdata[['tweet_single_letter','tweet']].head()","cf27d11a":"print('top 25 used words')\nprint('-----------------')\nprint(pd.Series(''.join(train_process['tweet_single_letter']).split()).value_counts()[0:25])\n","48b07fb0":"print('least 25 used words')\nprint('-------------------')\nprint(pd.Series(''.join(train_process['tweet_single_letter']).split()).value_counts()[-25:])","1415e4f6":"##train_process['tweet_correct']=train_process['tweet_single_letter'].progress_apply(lambda x: str(TextBlob(x).correct()))\n","4bd27396":"train_process['tweet_lemma']=train_process['tweet_single_letter'].progress_apply(lambda words: ' '.join( [WordNetLemmatizer().lemmatize(w) for w in words.split()]))\ntrain_process[['tweet_lemma','tweet']].head()","3251283e":"testdata['tweet_lemma']=testdata['tweet_single_letter'].progress_apply(lambda words: ' '.join( [WordNetLemmatizer().lemmatize(w) for w in words.split()]))\ntestdata[['tweet_lemma','tweet']].head()","5adf5ee5":"X_train,X_test,y_train,y_test=train_test_split(train_process['tweet_lemma'],train_process['label'],test_size=0.33,random_state=42)","cbf907d7":"feature = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1,1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n","cc1d4af5":"feature.fit(X_train) \nx_train=feature.transform(X_train)","0b99b7af":"testdata=feature.transform(testdata['tweet_lemma'])","e72fc5c4":"x_test=feature.transform(X_test)","dee635e5":"x_train.toarray()","4b265221":"#feature.get_feature_names()","de6381c9":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report\nimport time","4b18db25":"param_grid = {'C': np.arange(20,30,2),\n              'max_iter': np.arange(100,1200,100),\n              'penalty': ['l1','l2']}","2e36d80d":"kf = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n\ni=1\n\n\nfor train_index,test_index in kf.split(x_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = x_train[train_index],x_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    #print(train_index)\n    model = RandomizedSearchCV(estimator=LogisticRegression(class_weight='balanced'),param_distributions=param_grid,verbose=1)\n    \n\n    model.fit(xtr, ytr)\n    #print (model.best_params_)\n    pred=model.predict(xvl)\n    print('roc_auc_score',roc_auc_score(yvl,pred))\n    i+=1","75ebe23c":"print ('best parameters',model.best_params_)","07a739d6":"roc_auc_logistic = roc_auc_score(yvl,pred).mean()\nf1_logistic = f1_score(yvl,pred).mean()\nprint('Mean - ROC AUC', roc_auc_logistic)\nprint('F1 Score - ', f1_logistic)\nprint('Confusion Matrix \\n',confusion_matrix(yvl,pred))\n","0b803581":"import warnings\nwarnings.filterwarnings('ignore')","84d1ce1e":"#DecisionTree with tuned hyperparameters\nfrom sklearn.tree import DecisionTreeClassifier\nstart_time = time.time()\nparam_grid = {'criterion': ['gini','entropy'],\n             'min_samples_split':[50,70,100,150],\n             'max_features': ['sqrt','log2']}\n\n\ni=1\nkf = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\nfor train_index,test_index in kf.split(x_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = x_train[train_index],x_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    \n    model = RandomizedSearchCV(estimator=DecisionTreeClassifier(class_weight={0:1,1:5}),param_distributions=param_grid,verbose=1)\n    \n\n    model.fit(xtr, ytr)\n    #print (model.best_params_)\n    pred=model.predict(xvl)\n    print('roc_auc_score',roc_auc_score(yvl,pred))\n    i+=1\n\nprint(\"Execution time: \" + str((time.time() - start_time)) + ' ms')\nprint ('best parameters',model.best_params_)\n","6986989a":"#Model Accuracy\nroc_auc_dt = roc_auc_score(yvl,pred).mean()\nf1_dt = f1_score(yvl,pred).mean()\nprint('Mean - ROC AUC', roc_auc_dt)\nprint('F1 Score - ', f1_dt)\nprint('Confusion Matrix \\n',confusion_matrix(yvl,pred))","ba596730":"from sklearn.ensemble import RandomForestClassifier\nstart_time = time.time()\nparam_grid = {'criterion': ['entropy'],\n             'min_samples_split':np.arange(10,100,20),\n             'max_features': ['sqrt'],\n             'n_estimators':[10,20,30]}\n\ni=1\nkf = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\nfor train_index,test_index in kf.split(x_train,y_train):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = x_train[train_index],x_train[test_index]\n    ytr,yvl = y_train.iloc[train_index],y_train.iloc[test_index]\n    \n    model = RandomizedSearchCV(estimator=RandomForestClassifier(),param_distributions=param_grid,verbose=1)\n    \n\n    model.fit(xtr, ytr)\n    #print (model.best_params_)\n    pred=model.predict(xvl)\n    print('roc_auc_score',roc_auc_score(yvl,pred))\n    i+=1\n\nprint(\"Execution time: \" + str((time.time() - start_time)) + ' ms')\nprint ('best parameters',model.best_params_)","236f5809":"#Model Accuracy\nroc_auc_rf = roc_auc_score(yvl,pred).mean()\nf1_rf = f1_score(yvl,pred).mean()\nprint('Mean - ROC AUC', roc_auc_rf)\nprint('F1 Score - ', f1_rf)\nprint('Confusion Matrix \\n',confusion_matrix(yvl,pred))","9f35e8d8":"results = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest'],\n    'Mean - ROC AUC Score (Fold=10)': [roc_auc_logistic, roc_auc_dt, roc_auc_rf],\n    'Mean - F1 Score': [f1_logistic,f1_dt,f1_rf]})","fc6d1a13":"results.head()","9bad4025":"logistic=LogisticRegression(penalty='l2',max_iter=100,C=28)","0da2a532":"testdata","2463db17":"logistic.fit(x_train,y_train)\npred=logistic.predict(testdata.toarray())","6ecb7a0a":"sub=pd.DataFrame(testdata['id'])","a76a62c7":"sub['label']=pred","7daf29db":"sub.to_csv('sub.csv')","41379def":"#### Train-Test Split","bff324b3":"#### Remove the punctuations","dace84a3":"## Data Augmentation","0521926f":"#### read the dataset using read_csv() of pandas library","00c1e4d6":"#### What is Data Augumentation?\n\n> Data augumentation is a technique to overcome the imbalance in the target label of the dataset.Most of us must have augumented image data either by rotating the image,zooming,adding noises,etc, By doing these we basically increase the data.For images, there is a class in keras (ImageDatagenerator()) which helps to produce new images. For the text data, I have used nlpaug library.for better understandin you could read the blog\n>>https:\/\/towardsdatascience.com\/data-augmentation-library-for-text-9661736b13ff\n","31b582cd":"#### Remove the stopwords","a96382b4":"![source](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*Uf_qQ0zF8G8y9zUhndA08w.png)\n> The above shows the venn diagram of NLP","24f201b7":"# Sentiment Analysis Using ML-Agorithms with Data Augumentation\n## What is NLP?\n> Natural Language Processing is also abbrevated as NLP.Whatever we speakor write is understandable to a human,which becomes very difficult for a computer to decode from the text\/speech that we write\/speak. To make it a computer understanable,we basically process the texts into a number, as we know computer can understand only the number,such that we can apply this in the machine learning Algorithms.  \n![NLP](https:\/\/venturebeat.com\/wp-content\/uploads\/2018\/09\/natural-language-processing-e1572968977211.jpg?fit=578%2C289&strip=all)","2f791129":"#### Top 25 and Least 25 words shown","580472db":"#### TF-IDF\n>> TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.","17b0dc94":"## Decision Tree Classifier","4d9180a6":"You can give your own api key","18b75d42":"Hope you liked it!!\nDO upvote it!!","356e7944":"#### Lower Case","12d855a8":"#### Import Library","6b239a78":"## Logistic Regression","97b14042":"#### Remove the single letter present","173cc7a6":"#### Text Pre-Processing","f3b18f9f":"## Random Forest Classifier","531d1215":"#### the evaluation metric is F-1 score","464e7e8a":"it is unbalanced data as we could see there is only 7% which accounts positive class while rest are negative class","eb8f1e0b":"#### Lemmatize the sentences","252e86e7":"After augmnetation the dataset is now balanced"}}