{"cell_type":{"055bbc32":"code","9d4b2175":"code","a854a6d2":"code","a7a7cd67":"code","52a66631":"code","b045e07c":"code","074c0cdf":"code","80d6e5cd":"code","28e33ad5":"code","d7961d9a":"code","378a6028":"code","fe8734e8":"code","8a60a77c":"code","130a5fac":"code","dc41c6f5":"code","e1847117":"code","d3956c0d":"code","fd66eb8a":"code","4977d1c0":"code","5fb75cc0":"code","c5db42b1":"code","7add14aa":"code","3de39d8b":"code","c80b31f3":"code","abe6e824":"code","6663facb":"code","d99bcbbb":"code","11ddce35":"code","57cd69da":"code","a19c3597":"code","88b04af7":"code","f9a0ee4c":"code","b180f1f1":"code","1fbdcb22":"code","a69c51c9":"code","505b1392":"code","74a79c30":"code","bb71d8a2":"code","33dd89b0":"code","39d88127":"code","42170e12":"code","36da891e":"markdown","f53ab2c5":"markdown","08ced987":"markdown","3ae68b14":"markdown","f49cc79f":"markdown","2d925991":"markdown","aa782fd6":"markdown","f6afa53e":"markdown","33446888":"markdown","7bbcc4ea":"markdown","ff0479fe":"markdown","7aae29c2":"markdown","8b6a6e38":"markdown","a6ed7a55":"markdown","e8bbc8ca":"markdown","a5592064":"markdown","cef3c5f8":"markdown","1043fb16":"markdown","4661484a":"markdown","59a1f70c":"markdown","4c2718cd":"markdown","ec565053":"markdown"},"source":{"055bbc32":"### General libraries ###\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport graphviz \nfrom graphviz import Source\nfrom IPython.display import SVG\n\n##################################\n\n### ML Models ###\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree.export import export_text\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n##################################\n\n### Metrics ###\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, mean_squared_error, mean_absolute_error, classification_report","9d4b2175":"# Load the data.\nfile = '..\/input\/car.csv'\ndata = pd.read_csv(file)\n\n# Information\ndata.info()","a854a6d2":"# Shape of the data set.\nprint(\"The data set has {} rows and {} columns.\".format(data.shape[0],data.shape[1]))","a7a7cd67":"# Check for missing values.\ndata.isna().any()","52a66631":"# Check for duplicate rows.\ndata.duplicated().any()","b045e07c":"# Checking the values from each column.\nfor col in data.columns:\n    print(\"Column:\", col)\n    print(data[col].value_counts(),'\\n')","074c0cdf":"# Plotting the values of each column.\nfor i in data.columns:\n    labels = data[i].unique()\n    values = data[i].value_counts()\n    fig = go.Figure(data=[go.Pie(labels=labels, values=values)])\n    fig.update_layout(title=go.layout.Title(text='Value distribution for column: \"{}\"'.format(i),x=.5))\n    fig.show()","80d6e5cd":"# Create category types.\nbuying_type = CategoricalDtype(['low','med','high','vhigh'], ordered=True)\nmaint_type = CategoricalDtype(['low','med','high','vhigh'], ordered=True)\ndoors_type = CategoricalDtype(['2','3','4','5more'], ordered=True)\npersons_type = CategoricalDtype(['2','4','more'], ordered=True)\nlug_boot_type = CategoricalDtype(['small','med','big'], ordered=True)\nsafety_type = CategoricalDtype(['low','med','high'], ordered=True)\nclass_type = CategoricalDtype(['unacc','acc','good','vgood'], ordered=True)\n\n# Convert all categorical values to category type.\ndata.buying = data.buying.astype(buying_type)\ndata.maint = data.maint.astype(maint_type)\ndata.doors = data.doors.astype(doors_type)\ndata.persons = data.persons.astype(persons_type)\ndata.lug_boot = data.lug_boot.astype(lug_boot_type)\ndata.safety = data.safety.astype(safety_type)\ndata.class_val = data.class_val.astype(class_type)","28e33ad5":"# Convert categories into integers for each column.\ndata.buying=data.buying.replace({'low':0, 'med':1, 'high':2, 'vhigh':3})\ndata.maint=data.maint.replace({'low':0, 'med':1, 'high':2, 'vhigh':3})\ndata.doors=data.doors.replace({'2':0, '3':1, '4':2, '5more':3})\ndata.persons=data.persons.replace({'2':0, '4':1, 'more':2})\ndata.lug_boot=data.lug_boot.replace({'small':0, 'med':1, 'big':2})\ndata.safety=data.safety.replace({'low':0, 'med':1, 'high':2})\ndata.class_val=data.class_val.replace({'unacc':0, 'acc':1, 'good':2, 'vgood':3})","d7961d9a":"# The data set after the conversion.\ndata.head()","378a6028":"plt.figure(figsize=(10,6))\nsns.set(font_scale=1.2)\nsns.heatmap(data.corr(),annot=True, cmap='rainbow',linewidth=0.5)\nplt.title('Correlation matrix');","fe8734e8":"# Choose attribute columns and class column.\nX=data[data.columns[:-1]]\ny=data['class_val']","8a60a77c":"# Split to train and test sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","130a5fac":"# Initialize a Logistic Regression classifier.\nlogreg=LogisticRegression(solver='saga', multi_class='auto', random_state=42, n_jobs=-1)\n\n# Train the classifier.\nlogreg.fit(X_train,y_train)","dc41c6f5":"# Make predictions.\nlog_pred=logreg.predict(X_test)\n\n# CV score\nlogreg_cv = cross_val_score(logreg,X_train,y_train,cv=10)","e1847117":"# The mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.3f\" % mean_squared_error(y_test, log_pred))\n\n# Explained average absolute error (average error).\nprint(\"Mean absolute error (MAE): %.3f\" % mean_absolute_error(y_test, log_pred))\n\n# Explained variance score: 1 is perfect prediction.\nprint('Accuracy: %.3f' % logreg.score(X_test, y_test))\n\n# CV Accuracy\nprint('CV Accuracy: %.3f' % logreg_cv.mean())","d3956c0d":"# Plot confusion matrix for Logistic regression.\nlogreg_matrix = confusion_matrix(y_test,log_pred)\nplt.figure(figsize=(8,8))\nsns.set(font_scale=1.4)\nsns.heatmap(logreg_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Logistic Regression');","fd66eb8a":"# Hyperparameters to be checked.\nparameters = {'C':[0.0001,0.001, 0.01, 1, 0.1, 10, 100, 1000],\n              'penalty':['none','l2'],\n              'solver':['lbfgs','sag','saga','newton-cg']\n             }\n\n# Logistic Regression classifier.\ndefault_logreg=LogisticRegression(multi_class='auto', random_state=42, n_jobs=-1)\n\n# GridSearchCV estimator.\ngs_logreg = GridSearchCV(default_logreg, parameters, cv=10, verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_logreg.fit(X_train,y_train)","4977d1c0":"# Make predictions with the best parameters.\ngs_log_pred=gs_logreg.predict(X_test)","5fb75cc0":"# Best parameters.\nprint(\"Best Logistic Regression Parameters: {}\".format(gs_logreg.best_params_))\n\n# The mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.3f\" % mean_squared_error(y_test, gs_log_pred))\n\n# Explained average absolute error (average error).\nprint(\"Mean absolute error (MAE): %.3f\" % mean_absolute_error(y_test, gs_log_pred))\n\n# Cross validation accuracy for the best parameters.\nprint('CV Accuracy: %0.3f' % gs_logreg.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (gs_logreg.score(X_test,y_test)))","c5db42b1":"# Plot confusion matrix for GridSearchCV Logistic regression.\ngs_logreg_matrix = confusion_matrix(y_test,log_pred)\nplt.figure(figsize=(8,8))\nsns.set(font_scale=1.4)\nsns.heatmap(gs_logreg_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix \\nfor GridSearchCV Logistic Regression');","7add14aa":"# Initialize a decision tree estimator.\ntr = tree.DecisionTreeClassifier(max_depth=5, criterion='entropy', random_state=42)\n\n# Train the estimator.\ntr.fit(X_train, y_train)","3de39d8b":"# Plot the tree.\ndot_data = tree.export_graphviz(tr, out_file=None, feature_names=X.columns,class_names=['unacc', 'acc', 'good', 'vgood'], filled=True, rounded=True, special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","c80b31f3":"# Print the tree in a simplified version.\nr = export_text(tr, feature_names=X.columns.tolist())\nprint(r)","abe6e824":"# Make predictions.\ntr_pred=tr.predict(X_test)\n\n# CV score\ntr_cv = cross_val_score(tr,X_train,y_train,cv=10)","6663facb":"# The mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.3f\" % mean_squared_error(y_test, tr_pred))\n\n# Explained average absolute error (average error).\nprint(\"Mean absolute error (MAE): %.3f\" % mean_absolute_error(y_test, tr_pred))\n\n# Explained variance score: 1 is perfect prediction.\nprint('Accuracy: %.3f' % tr.score(X_test, y_test))\n\n# CV Accuracy\nprint('CV Accuracy: %.3f' % tr_cv.mean())","d99bcbbb":"# Print confusion matrix for Decision tree.\ntr_matrix = confusion_matrix(y_test,tr_pred)\nplt.figure(figsize=(8,8))\nsns.set(font_scale=1.4)\nsns.heatmap(tr_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Decision tree');","11ddce35":"# Hyperparameters to be checked.\nparameters = {'criterion':['gini','entropy'],\n              'max_depth':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n             }\n\n# Default Decision tree estimator.\ndefault_tr = tree.DecisionTreeClassifier(random_state=42)\n\n# GridSearchCV estimator.\ngs_tree = GridSearchCV(default_tr, parameters, cv=10, n_jobs=-1,verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_tree.fit(X_train,y_train)","57cd69da":"# Make predictions with the best parameters.\ngs_tree_pred=gs_tree.predict(X_test)","a19c3597":"# Best parameters.\nprint(\"Best Decision tree Parameters: {}\".format(gs_tree.best_params_))\n\n# The mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.3f\" % mean_squared_error(y_test, gs_tree_pred))\n\n# Explained average absolute error (average error).\nprint(\"Mean absolute error (MAE): %.3f\" % mean_absolute_error(y_test, gs_tree_pred))\n\n# Cross validation accuracy for the best parameters.\nprint('CV accuracy: %0.3f' % gs_tree.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (gs_tree.score(X_test,y_test)))","88b04af7":"# Print confusion matrix for GridSearchCV Decision tree.\ngs_tr_matrix = confusion_matrix(y_test,gs_tree_pred)\nplt.figure(figsize=(8,8))\nsns.set(font_scale=1.4)\nsns.heatmap(gs_tr_matrix,annot=True, cbar=False, cmap='twilight', linewidth=0.5, fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for GridSearchCV Decision tree');","f9a0ee4c":"# Initialize a Multi-layer Perceptron classifier.\nmlp = MLPClassifier(hidden_layer_sizes=(5),max_iter=1000, random_state=42, shuffle=True, verbose=False)\n\n# Train the classifier.\nmlp.fit(X_train, y_train)","b180f1f1":"# Make predictions.\nmlp_pred = mlp.predict(X_test)\n\n# CV score\nmlp_cv = cross_val_score(mlp,X_train,y_train,cv=10)","1fbdcb22":"# The mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.3f\" % mean_squared_error(y_test, mlp_pred))\n\n# Explained average absolute error (average error).\nprint(\"Mean absolute error (MAE): %.3f\" % mean_absolute_error(y_test, mlp_pred))\n\n# Explained variance score: 1 is perfect prediction.\nprint('Accuracy: %.3f' % mlp.score(X_test, y_test))\n\n# CV Accuracy\nprint('CV Accuracy: %.3f' % mlp_cv.mean())","a69c51c9":"# Plot confusion matrix for MLP.\nmlp_matrix = confusion_matrix(y_test,mlp_pred)\nplt.figure(figsize=(8,8))\nsns.set(font_scale=1.4)\nsns.heatmap(mlp_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for MLP');","505b1392":"# Hyperparameters to be checked.\nparameters = {'activation':['logistic','tanh','relu'],\n              'solver': ['lbfgs','adam','sgd'],\n              'alpha':10.0 ** -np.arange(1,3),\n              'hidden_layer_sizes':[(5),(100),(3),(4),(3,1),(5,3)]}\n\n# MLP estimator.\ndefault_mlp = MLPClassifier(random_state=42)\n\n# GridSearchCV estimator.\ngs_mlp = GridSearchCV(default_mlp, parameters, cv=10, n_jobs=-1,verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_mlp.fit(X_train,y_train)","74a79c30":"# Make predictions with the best parameters.\ngs_mlp_pred=gs_mlp.predict(X_test)","bb71d8a2":"# Best parameters.\nprint(\"Best MLP Parameters: {}\".format(gs_mlp.best_params_))\n\n# The mean squared error (relative error).\nprint(\"Mean squared error (MSE): %.3f\" % mean_squared_error(y_test, gs_mlp_pred))\n\n# Explained average absolute error (average error).\nprint(\"Average absolute error (MAE): %.3f\" % mean_absolute_error(y_test, gs_mlp_pred))\n\n# Cross validation accuracy for the best parameters.\nprint('CV accuracy: %0.3f' % gs_mlp.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (gs_mlp.score(X_test,y_test)))","33dd89b0":"# Print confusion matrix for GridSearchCV MLP.\ngs_mlp_matrix = confusion_matrix(y_test,gs_mlp_pred)\nplt.figure(figsize=(8,8))\nsns.set(font_scale=1.4)\nsns.heatmap(gs_mlp_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for GridSearchCV MLP');","39d88127":"# Ploting metrics\nerrors=['Accuracy','CV-accuracy','MSE', 'MAE']\n\nfig = go.Figure(data=[\n    go.Bar(name='Logistic Regression', x=errors, y=[logreg.score(X_test, y_test),logreg_cv.mean(),mean_squared_error(y_test, log_pred), mean_absolute_error(y_test, log_pred)]),\n    go.Bar(name='Decision tree', x=errors, y=[tr.score(X_test, y_test),tr_cv.mean(),mean_squared_error(y_test, tr_pred), mean_absolute_error(y_test, tr_pred)]),\n    go.Bar(name='MLP', x=errors, y=[mlp.score(X_test, y_test),mlp_cv.mean(),mean_squared_error(y_test, mlp_pred), mean_absolute_error(y_test, mlp_pred)]),\n    go.Bar(name='GridSearchCV+Logistic Regression', x=errors, y=[gs_logreg.score(X_test, y_test),gs_logreg.best_score_,mean_squared_error(y_test, gs_log_pred), mean_absolute_error(y_test, gs_log_pred)]),\n    go.Bar(name='GridSearchCV+Decision tree', x=errors, y=[gs_tree.score(X_test, y_test),gs_tree.best_score_,mean_squared_error(y_test, gs_tree_pred), mean_absolute_error(y_test, gs_tree_pred)]),\n    go.Bar(name='GridSearchCV+MLP', x=errors, y=[gs_mlp.score(X_test, y_test),gs_mlp.best_score_,mean_squared_error(y_test, gs_mlp_pred), mean_absolute_error(y_test, gs_mlp_pred)])\n])\n\nfig.update_layout(\n    title='Metrics for each model',\n    xaxis_tickfont_size=14,    \n    barmode='group',\n    bargap=0.15, # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1 # gap between bars of the same location coordinate.\n)\nfig.show()","42170e12":"d={\n'': ['Logistic Regression','GridSearchCV + Logistic Regression','Decision Tree','GridSearchCV + Decision Tree','Neural Network (MLP)','GridSearchCV + Neural Network (MLP)'],\n'Accuracy': [logreg.score(X_test, y_test), gs_logreg.score(X_test,y_test),tr.score(X_test, y_test),gs_tree.score(X_test,y_test),mlp.score(X_test, y_test),gs_mlp.score(X_test, y_test)],\n'CV Accuracy': [logreg_cv.mean(), gs_logreg.best_score_, tr_cv.mean(),gs_tree.best_score_,mlp_cv.mean(),gs_mlp.best_score_],\n'MSE': [mean_squared_error(y_test, log_pred),mean_squared_error(y_test, gs_log_pred),mean_squared_error(y_test, tr_pred), mean_squared_error(y_test, gs_tree_pred),mean_squared_error(y_test, mlp_pred),mean_squared_error(y_test, gs_mlp_pred)],\n'MAE': [mean_absolute_error(y_test, log_pred),mean_absolute_error(y_test, gs_log_pred),mean_absolute_error(y_test, tr_pred), mean_absolute_error(y_test, gs_tree_pred),mean_absolute_error(y_test, mlp_pred),mean_absolute_error(y_test, gs_mlp_pred)]\n}\n\nresults=pd.DataFrame(data=d).round(3).set_index('')\nresults","36da891e":"## Confusion Matrix for GridSearchCV Decision tree","f53ab2c5":"\n## Introduction\n\nThis notebook was created for analysis and prediction making of the Car evaluation data set from UCI Machine Learning Library. The data set can be accessed separately from the UCI Machine Learning Repository page, [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Car+Evaluation).\n\n## Data Set Information\n\nCar Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:\n\n- `class_val`: car category\n- `buying`: buying price\n- `maint`: price of the maintenance\n- `doors`: number of doors\n- `persons`: capacity in terms of persons to carry\n- `lug_boot`: the size of luggage boot\n- `safety`: estimated safety of the car\n\nThe Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety.\n\n## Models\n\nWe will create 3 models in order to make predictions and compare them with the original paper. These models are:\n\n- Logistic Regression\n- Decision tree\n- Neural Network\n\nAfter the initial predictions, each model will be \"optimized\" by GridSearchCV estimator, which will search for the best set of hyperparameters for every model.\n\n## Metrics\n\nMetrics such as accuracy, cross-validation accuracy, mean squared error (MSE) and mean average error (MAE) will be used for all the models.","08ced987":"## Confusion Matrix for Logistic Regression","3ae68b14":"## Import libraries\/packages","f49cc79f":"## Neural network (MLP)","2d925991":"## Results","aa782fd6":"## Metrics for GridSearchCV MLP","f6afa53e":"## Part 3: Modeling\n\nIn this section we build and try 3 models:\n - Logistic Regression\n - Decision tree\n - Neural network\n\nEach model will be trained and make a prediction for the test set. Accuracy, f1 score, confusion matrix and ROC will be calculated for each model. Then we will use the `GridSearchCV` module to tune our models and search for the best hyperparameters in order to increase the accuracy of each model.","33446888":"Since all the columns are categorical, we change the data types to \"category\". This will come in handy in case we want to sort any column of the data set.","7bbcc4ea":"## Metrics for GridSearchCV Decision tree","ff0479fe":"## Confusion Matrix for Decision tree","7aae29c2":"## Grid search for Decision tree","8b6a6e38":"## Part 1: Load and clean the data","a6ed7a55":"## Metrics for Neural Network (MLP)","e8bbc8ca":"## Metrics for GridSearchCV Logistic Regression","a5592064":"## Metrics for Decision tree","cef3c5f8":"## Logistic Regression","1043fb16":"## Part 2: Preprocessing\n\nIn this part we prepare our data for our models. This means that we choose the columns that will be our independed variables and which column the class that we want to predict. Once we are done with that, we split our data into train and test sets and perfom a standardization upon them.","4661484a":"## Grid search for Neural Network","59a1f70c":"## Metrics for Logistic Regression","4c2718cd":"## Grid search for Logistic Regression","ec565053":"## Decision tree"}}