{"cell_type":{"9a899936":"code","caba63cd":"code","e3881c22":"code","56b3eb8e":"code","405409f7":"code","dad4ed70":"code","e2baeedf":"code","9f063704":"code","fbc30e1c":"markdown","a098938c":"markdown","dc645b72":"markdown","f1b2d0d1":"markdown","31744142":"markdown","b90357dc":"markdown","392e1088":"markdown"},"source":{"9a899936":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nos.chdir('..')\n# Any results you write to the current directory are saved as output.","caba63cd":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import grad\nfrom torch.autograd import Variable\nimport torchvision\nimport torchvision.transforms as transforms\nimport shutil\nimport statistics as st\nimport matplotlib.image as mpimg\nimport pylab\n\n%pylab inline\npylab.rcParams['figure.figsize'] = (10, 10)","e3881c22":"class generator(nn.Module):\n    def __init__(self, dim_in, dim=64):\n        super(generator,self).__init__()\n        def genblock(dim_in, dim_out):\n            block = nn.Sequential( nn.ConvTranspose2d(in_channels = dim_in, \n                                                      out_channels = dim_out,\n                                                      kernel_size = 5, \n                                                      stride=2, \n                                                      padding=2,\n                                                      output_padding = 1,\n                                                      bias = False),\n                                    nn.BatchNorm2d(dim_out),\n                                    nn.ReLU()\n                                    )\n            return block\n        def genimg(dim_in):\n            block = nn.Sequential( nn.ConvTranspose2d(in_channels = dim_in, \n                                                      out_channels = 3,\n                                                      kernel_size = 5, \n                                                      stride=2, \n                                                      padding=2,\n                                                      output_padding = 1,\n                                                      ),\n                                    nn.Tanh()\n                                    )\n            return block\n        \n        self.prepare = nn.Sequential(nn.Linear(dim_in, dim*8*4*4, bias=False),\n                                     nn.BatchNorm1d(dim*8*4*4),\n                                     nn.ReLU())\n        \n        self.generate = nn.Sequential(genblock(dim*8, dim*4),\n                                      genblock(dim*4, dim*2),\n                                      genblock(dim*2, dim),\n                                      genimg(dim))\n    def forward(self, x):\n        x = self.prepare(x)\n        x = x.view(x.size(0), -1,4,4)\n        x = self.generate(x)\n        return x\n#%%\nclass critic(nn.Module):\n    def __init__(self, dim_in, dim=64):\n        super(critic, self).__init__()\n        \n        def critic_block(dim_in , dim_out):\n            block = nn.Sequential(nn.Conv2d(in_channels = dim_in, \n                                            out_channels = dim_out,\n                                            kernel_size = 5, \n                                            stride=2, \n                                            padding=2),\n                                    nn.InstanceNorm2d(dim_out, affine= True),\n                                    nn.LeakyReLU(0.2))\n            return block\n        self.analyze = nn.Sequential(nn.Conv2d(in_channels = dim_in, \n                                               out_channels = dim,\n                                               kernel_size = 5, \n                                               stride=2, \n                                               padding=2),\n                                     nn.LeakyReLU(0.2),\n                                     critic_block(dim,dim*2),\n                                     critic_block(dim*2,dim*4),\n                                     critic_block(dim*4, dim*8),\n                                     nn.Conv2d(in_channels=dim*8, \n                                               out_channels=1,\n                                               kernel_size=4))\n    def forward(self,x):\n        x = self.analyze(x)\n        x =x.view(-1)\n        return x","56b3eb8e":"def gradient_penalty(x,y,f):\n    shape =[x.size(0)] + [1] * (x.dim() -1)\n    alpha = torch.rand(shape).cuda()\n    z = x+ alpha *(y-x)\n    z = Variable(z,requires_grad=True)\n    z=z.cuda()\n    o=f(z)\n    g = grad(o,z, grad_outputs=torch.ones(o.size()).cuda(), create_graph=True)[0].view(z.size(0), -1)\n    gp = ((g.norm(p=2,dim=1))**2).mean()\n    return gp\n#%%\ndef save_checkpoint(state, save_path, is_best=False, max_keep=None):\n    # save checkpoint\n    torch.save(state, save_path)\n\n    # deal with max_keep\n    save_dir = os.path.dirname(save_path)\n    list_path = os.path.join(save_dir, 'latest_checkpoint')\n\n    save_path = os.path.basename(save_path)\n    if os.path.exists(list_path):\n        with open(list_path) as f:\n            ckpt_list = f.readlines()\n            ckpt_list = [save_path + '\\n'] + ckpt_list\n    else:\n        ckpt_list = [save_path + '\\n']\n\n    if max_keep is not None:\n        for ckpt in ckpt_list[max_keep:]:\n            ckpt = os.path.join(save_dir, ckpt[:-1])\n            if os.path.exists(ckpt):\n                os.remove(ckpt)\n        ckpt_list[max_keep:] = []\n\n    with open(list_path, 'w') as f:\n        f.writelines(ckpt_list)\n\n    # copy best\n    if is_best:\n        shutil.copyfile(save_path, os.path.join(save_dir, 'best_model.ckpt'))\n#%%\ndef load_checkpoint(ckpt_dir_or_file, map_location=None, load_best=False):\n    if os.path.isdir(ckpt_dir_or_file):\n        if load_best:\n            ckpt_path = os.path.join(ckpt_dir_or_file, 'best_model.ckpt')\n        else:\n            with open(os.path.join(ckpt_dir_or_file, 'latest_checkpoint')) as f:\n                ckpt_path = os.path.join(ckpt_dir_or_file, f.readline()[:-1])\n    else:\n        ckpt_path = ckpt_dir_or_file\n    ckpt = torch.load(ckpt_path, map_location=map_location)\n    print(' [*] Loading checkpoint from %s succeed!' % ckpt_path)\n    return ckpt","405409f7":"epochs = 100 #Ideally go for 500+ epochs . Less epochs have been taken just for the purpose of this notebook\nbatch_size = 180\nn_critic=5\nlr=0.0002\nz_dim = 100\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])])\ndata = torchvision.datasets.ImageFolder('.\/input', transform = transform)\ndataloader = torch.utils.data.DataLoader(data,\n                                         batch_size=batch_size,\n                                         shuffle=True,\n                                         num_workers=3)\nC = critic(3)\nG = generator(z_dim)\nC = C.cuda()\nG = G.cuda()\nprint(C)\nprint(G)\nprint(\"Generator : \")\nprint(G)\nprint(\"Critic\")\nprint(C)\nstart_epoch=0\nG_opt = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5,0.999))\nC_opt = torch.optim.Adam(C.parameters(), lr=lr, betas=(0.5,0.999))\n#%%\ncheckpoint = '.\/checkpoints\/wgan_gp'\nsave_dir = '.\/sample_images\/wgan_gp'\nif not isinstance(checkpoint, (list, tuple)):\n    paths = [checkpoint]\n    for path in paths:\n        if not os.path.isdir(path):\n            os.makedirs(path)\nif not isinstance(save_dir, (list, tuple)):\n    paths = [save_dir]\n    for path in paths:\n        if not os.path.isdir(path):\n            os.makedirs(path)\ntry:\n    ckpt = load_checkpoint(checkpoint)\n    start_epoch = ckpt['epoch']\n    C.load_state_dict(ckpt['D'])\n    G.load_state_dict(ckpt['G'])\n    C_opt.load_state_dict(ckpt['d_optimizer'])\n    G_opt.load_state_dict(ckpt['g_optimizer'])\nexcept:\n    print(' [*] No checkpoint!')\n    start_epoch = 0\n#%%\nz_sample = Variable(torch.randn(100, z_dim)).cuda()\n","dad4ed70":"import time\nfor epoch in range(start_epoch, epochs):\n    start_time = time.time()\n    C_loss= []\n    G_loss=[]\n    G.train()\n    for i, (images, _) in enumerate(dataloader):\n        step = epoch * len(dataloader) + i + 1\n\n        images = Variable(images)\n        batch = images.size(0)\n        images = images.cuda()\n        z = Variable(torch.randn(batch, z_dim))\n        z = z.cuda()\n        \n        generated = G(z)\n        real_criticized = C(images)\n        fake_criticized = C(generated)\n        \n        em_distance = real_criticized.mean() - fake_criticized.mean()\n        grad_penalty = gradient_penalty(images.data, generated.data, C)\n        \n        CriticLoss = -em_distance + grad_penalty*10\n        C_loss.append(CriticLoss.item())\n        C.zero_grad()\n        CriticLoss.backward()\n        C_opt.step()\n        \n        if step % n_critic == 0:\n            z = Variable(torch.randn(batch, z_dim))\n            z = z.cuda()\n            generated = G(z)\n            fake_criticized = C(generated)\n            GenLoss = -fake_criticized.mean()\n            G_loss.append(GenLoss.item())\n            C.zero_grad()\n            G.zero_grad()\n            GenLoss.backward()\n            G_opt.step()\n            print(\"Epoch {} : {}\/{} :: {} mins\".format(epoch+1, i+1, len(dataloader), (time.time()-start_time)\/60), end='\\r')         \n    print(\"Epoch {} completed\".format(epoch+1))\n    G.eval()\n    fake_gen_images = (G(z_sample).data +1)\/2.0\n    torchvision.utils.save_image(fake_gen_images, save_dir+'\/Epoch '+str(epoch+1)+\".jpg\",nrow=10)\n    save_checkpoint({'epoch': epoch + 1,\n                           'D': C.state_dict(),\n                           'G': G.state_dict(),\n                           'd_optimizer': C_opt.state_dict(),\n                           'g_optimizer': G_opt.state_dict()},\n                          '%s\/Epoch_(%d).ckpt' % (checkpoint, epoch + 1),\n                          max_keep=2)","e2baeedf":"a= os.listdir('sample_images\/wgan_gp\/')\na.sort()\nprint(a[-1])\nimg=mpimg.imread('sample_images\/wgan_gp\/'+a[-1])\nplt.imshow(img)\nplt.axis('off')\nplt.show()","9f063704":"!wget https:\/\/raw.githubusercontent.com\/spandan2\/Wgan-GP_cats\/master\/sample_images\/wgan_gp\/Epoch%20500.jpg\nimg=mpimg.imread('Epoch 500.jpg')\nplt.imshow(img)\nplt.axis('off')\nplt.show()","fbc30e1c":"Let's import our dependencies","a098938c":"# Output after training for 500 epochs","dc645b72":"**As mentioned in the paper, WGANs require a Lipschitz condition which on a high level can be understood as clipping the weights to a certain limit.\nWGAN-GP enforces this Lipschitz condition not by clipping weights but by using a gradient penalty that is learnable unlike a hardcoded clipping.**\n\n\nOther utilities have been defined for checkpointing and for saving images","f1b2d0d1":"> While defining the architecture of discriminators and Generators, we found that instead of rewriting blocks we can use functions to define blocks with much less repitition of code","31744142":"# **This is an implementation of Wasserstein GANs with gradient penalty.**\n## **Link to the paper is : https:\/\/arxiv.org\/pdf\/1704.00028.pdf**\n\nWasserstein GANs suggest a change in the distance function calculated while training any Generative Adverserial Network.\nEarth mover's distance is a weaker distance and in their paper,. \nThe other distances like Total Variance, KL Divergence and JS Divergence failed in the case of low dimensional manifolds where the distributions may have very little common projection space.\nThe mathematical details of the advantages of this distance can be read here : https:\/\/arxiv.org\/pdf\/1701.07875.pdf\n\n\nWe can even use Resnet-101 as a generator in WGAN-GP but let's keep it simple and train it on a DC GAN architecture \nGo on to my repository https:\/\/github.com\/spandan2\/Wgan-GP_cats for an implementation with tensorboard support","b90357dc":"# **Outputs as trained in this notebook**","392e1088":"**Let us now train the model . GANs converge noise data to a distribution. Here, the noise data is 100 dimensional. \nNote that the critic here has 3 channels as input as it must take an RGB image and segregate it from the real images.**"}}