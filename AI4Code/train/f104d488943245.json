{"cell_type":{"bf5ddf86":"code","5f089ab5":"code","4dcf9bdf":"code","8790e94e":"code","1f38fb7e":"code","b1d705f2":"code","50719ae4":"code","f9c4c211":"code","a1ec7471":"code","8fbfe7e0":"code","7a0cdbcb":"code","a03f2b56":"code","f7a5c76b":"code","58950b3b":"code","93043ca5":"code","bf81c397":"code","15a039a8":"code","ee8b3942":"code","9666797a":"code","13e24dab":"code","299ebe34":"code","e1a3dc78":"code","9e8e40a5":"code","be48b2a7":"code","db6ca7ed":"code","046ca5c5":"code","862620b7":"code","1b2cf970":"code","336ea4e7":"code","d1c046e9":"code","1b288c16":"code","2a021dd7":"code","627ca673":"code","2eb3659f":"code","536a9a08":"code","21e1b074":"code","3193315f":"code","dd84d51b":"code","d52c6ef0":"code","946f1e3c":"code","0ec08ecf":"code","fec70467":"code","343c5e80":"code","801c7ad3":"code","2da64b32":"code","25aee0f8":"code","919b5427":"code","fa34c536":"code","8c489926":"code","7080e774":"code","46c8ca1f":"code","c3a85e36":"code","89568430":"code","cec641c6":"code","2425dfc0":"code","cafddd92":"code","7d873473":"code","ceb1e5e9":"code","320751a1":"code","e3ae6746":"code","74b443b2":"code","12838ded":"code","c8b53813":"code","17765f06":"code","9497b9e6":"code","7d04c0c4":"code","d8a65d9c":"code","0956b87a":"code","d2e78b26":"code","6393fefd":"code","c907d91f":"code","f30f96fc":"code","20817f19":"code","f68304e3":"code","7d80c2d1":"code","39781e9c":"code","314504de":"code","93f35bd3":"code","190ee1c8":"code","22abdacb":"code","a8b43837":"code","2687095d":"code","8401e17c":"code","c02c4a10":"code","f5bf0575":"code","a4c63930":"code","e3c34f76":"code","8b642abb":"code","0ce93e83":"code","5fc5f004":"code","91761151":"code","38d2c72e":"code","7c9f84fd":"code","55bd4f59":"code","2fe66441":"code","39443e44":"code","772d320b":"code","a91b2f41":"code","69a281ab":"code","6323a074":"code","8d04686d":"code","2653221d":"code","3f2f728c":"code","e9f540e9":"code","2e6dc728":"code","4ba618ec":"code","9c02fdb2":"code","17bbc9ab":"code","86b2d51a":"code","39dee550":"code","fe136794":"code","79d0394f":"code","f562c350":"code","e4f7f5c1":"code","434f12ee":"code","750846ac":"markdown","489a4f91":"markdown","e7afa259":"markdown","213b62ef":"markdown","b0f8afd0":"markdown","6bb74fec":"markdown","44dcaef9":"markdown","225b22ca":"markdown","f741d1fb":"markdown","05892805":"markdown","5d0ef0fa":"markdown","ea162abf":"markdown","3dfcb499":"markdown","12a714e0":"markdown","c16e3c6a":"markdown","82c22d7b":"markdown","c8a42594":"markdown","a12f80a8":"markdown","1dfb5287":"markdown","04ad8cc8":"markdown","0fd2e10b":"markdown","5e2ca3f2":"markdown","b53099df":"markdown","21d33d52":"markdown","39582733":"markdown","b1faaaa7":"markdown","0dd8ca30":"markdown","c24fa08a":"markdown","89ecdc2c":"markdown","39ad55ee":"markdown","4b39e5dd":"markdown","d2871747":"markdown","13e9a223":"markdown","a948aa06":"markdown","afca5f36":"markdown","fa80d09c":"markdown","7cf1c2f2":"markdown","74ed54d2":"markdown","17a2c3cb":"markdown","fc4e53b3":"markdown","799b922a":"markdown","bb2a0980":"markdown","71727bb6":"markdown","83c6cbeb":"markdown","79e155fd":"markdown","83eea3f2":"markdown","626b2e9c":"markdown","9eaff88a":"markdown","062bf939":"markdown","341b5d7e":"markdown","568123ce":"markdown","eed1ee06":"markdown","ae2ded67":"markdown","336c9201":"markdown"},"source":{"bf5ddf86":"#data analysis libraries \nimport pandas as pd\nimport numpy as np\n\n#visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Setting pandas to display a N number of columns and rows \npd.set_option('display.max_row',33)\npd.set_option('display.max_column',111)","5f089ab5":"# Import the data\nhousing = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')","4dcf9bdf":"# Peek at the Data\nhousing.head()","8790e94e":"# shape of data\nhousing.shape","1f38fb7e":"# get a quick description of the data, in particular the number of non-null, and each attribute\u2019s type \nhousing.info()","b1d705f2":"# see a summary of the numerical attributes\nhousing.describe()","50719ae4":"# Class Distribution \nhousing['ocean_proximity'].value_counts()","f9c4c211":"# Find NaN values \nhousing.isna().sum()","a1ec7471":"#Visualization of the variables' distribution\n\ncolumns = ['longitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms','households', 'median_income','longitude',\n        'latitude','median_house_value']\n\n\ndef distplot(nrows, ncols, columns):\n\n    rows=nrows\n    cols=ncols\n\n    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(18, 12))\n\n    columns = columns\n    index=0\n\n    for i in range(rows):\n        for j in range(cols):\n            sns.distplot(housing[columns[index]], ax=ax[i][j], bins=40)\n            index+=1\n\n        \ndistplot(3, 3, columns)","8fbfe7e0":"# zoom in on the target variable\nplt.figure()\nplt.hist(housing['median_house_value'], bins=150)\nplt.show()","7a0cdbcb":"# The shape of noisy data \nhousing[housing['median_house_value'] >= 500000 ].shape","a03f2b56":"# The shape of data without noisy values\nhousing[housing['median_house_value'] <= 500000].shape","f7a5c76b":"# Remove noisy values\nhousing = housing[housing['median_house_value'] <= 500000]\nhousing.shape","58950b3b":"# check if our target is clean\nplt.figure()\nplt.hist(housing['median_house_value'], bins=100)\nplt.show()","93043ca5":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"] \/ housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"] \/ housing[\"total_rooms\"]\nhousing[\"population_per_household\"] = housing[\"population\"] \/ housing[\"households\"]","bf81c397":"housing.head(3)","15a039a8":"# Visualization of the distribution of these new variables\n\nnew_columns = ['rooms_per_household', 'bedrooms_per_room','population_per_household']\n\nfor col in new_columns :\n    plt.figure()\n    sns.distplot(housing[col])\n    plt.show()\n","ee8b3942":"#Visualization of the correlation between all the numerical variables\nplt.figure(figsize=(15,7))\ncorr_matrix = housing.corr()\nsns.heatmap(corr_matrix, annot=True, cmap=\"YlGnBu\")","9666797a":"# The correlation between the targer and the other variables\ncorr_matrix['median_house_value'].sort_values(ascending=False)","13e24dab":"housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, label='population',\n            figsize=(10,7),c='median_house_value',s=housing['population']\/100, cmap=plt.get_cmap('cubehelix') ,colorbar=True)\n              \nplt.legend()","299ebe34":"#housing = housing[['median_income','total_rooms','bedrooms_per_room','households','total_bedrooms','housing_median_age','population_per_household','ocean_proximity','longitude','latitude','median_house_value']]\n#housing.head()","e1a3dc78":"housing.dropna(axis=0, inplace=True)\nprint(housing.isna().sum())","9e8e40a5":"from sklearn.preprocessing import OneHotEncoder\n\nhousing_cat = housing[['ocean_proximity']]\n\ntransformer = OneHotEncoder(sparse=False)\nhousing_ohe = transformer.fit_transform(housing_cat)\nhousing_ohe","be48b2a7":"OneHotEncoder = pd.DataFrame(data = housing_ohe, columns=['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND'])\n\nOneHotEncoder ","db6ca7ed":"# Make sure that there is no missing values\nOneHotEncoder.isna().sum()","046ca5c5":"# reset index \nhousing = housing.set_index(np.arange(0,19475))\nhousing","862620b7":"# Delete the variable 'ocean_proximity'\nhousing.drop(columns=['ocean_proximity'], axis=1, inplace=True)\nhousing","1b2cf970":"# Check their shapes before concatenation\nprint(OneHotEncoder.shape)\nprint(housing.shape)","336ea4e7":"# concatenate housing and OneHotEncoder\nhousing_prep = pd.concat([housing, OneHotEncoder], axis=1)\nhousing_prep\n","d1c046e9":"# Again, make sure after concatenation that there are no missing values left\nplt.figure(figsize=(12,4))\nsns.heatmap(housing_prep.isna(), cmap='YlGnBu')","1b288c16":"housing = housing_prep.copy()","2a021dd7":"#Import necessary libraries \n\nfrom sklearn.model_selection import cross_val_score, learning_curve, RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectKBest, f_classif, chi2\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n","627ca673":"def train_test_split_(data ,target_var) :\n\n    X = data.drop([target_var], axis=1)\n    y = data[target_var]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0 )\n    return (X_train, X_test, y_train, y_test)\n\n\n\ndef model_val_scores(mod, X_train, y_train, cv=5):\n        \n    score_val = []\n    standard_deviation = []\n        \n    scores = cross_val_score(mod, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n    \n    rmse_scores = np.sqrt(-scores)\n    scores_mean = rmse_scores.mean()\n    scores_std = rmse_scores.std()\n        \n    score_val.append(scores_mean)\n    standard_deviation.append(scores_std)\n    \n    return pd.DataFrame(data=[score_val, standard_deviation], index = ['scores_val', 'scores_std']) \n\n\ndef learning_curves(mod, X_train, y_train , cv=5):\n    \n    N , train_score, val_score = learning_curve(mod, X_train, y_train,  cv=5 , train_sizes=np.linspace(0.2 ,1.0, 5))\n\n    plt.plot(N, train_score.mean(axis=1), label='Train')\n    plt.plot(N, val_score.mean(axis=1), label='Validation')\n    plt.xlabel('train size')\n    plt.legend()\n    \n\ndef RandomizeSearchCV_(model, param_grid, X_train, y_train ) :\n    \n    randomSCV = RandomizedSearchCV(model, param_grid, n_iter=30, cv=5, scoring='neg_mean_squared_error', random_state=42)\n\n    randomSCV.fit(X_train, y_train)\n    model_best_params = randomSCV.best_estimator_\n    \n    print('best score :', randomSCV.best_score_ )\n    print('best params :', randomSCV.best_params_ )\n    \n    return model_best_params\n\ndef GridSearchCV_(mod, param_grid, X_train, y_train):\n    grid = GridSearchCV(estimator=mod, param_grid=param_grid, cv= 5, scoring='neg_mean_squared_error')\n    \n    grid.fit(X_train, y_train)\n    model_best_params = grid.best_estimator_\n    \n    print('best score :', grid.best_score_ )\n    print('best params :', grid.best_params_ )\n    \n    return model_best_params\n\n\ndef performance_metrics(y_test, y_pred):\n    \n    r2_scores  = []\n    mae_value  = []\n    mse_value  = []\n    rmse_value = []\n   \n    scores = r2_score(y_test, y_pred)\n    mae = mean_absolute_error(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    \n    r2_scores.append(scores)\n    mae_value.append(mae)\n    mse_value.append(mse)\n    rmse_value.append(rmse)\n    \n    metrics_dataframe=pd.DataFrame(data= [r2_scores, mae_value, mse_value, rmse_value],\n                                index=['r2_score','MAE','MSE','RMSE'])\n    return metrics_dataframe.T\n","2eb3659f":"X_train, X_test, y_train, y_test = train_test_split_(housing, 'median_house_value')\n\nprint('X_train :', X_train.shape)\nprint('X_test :', X_test.shape)\nprint('y_train :', y_train.shape)\nprint('y_test :', y_test.shape)","536a9a08":"from sklearn.linear_model import LinearRegression","21e1b074":"# n_jobs=-1 tells Scikit-Learn to use all available cores\nLR_model = make_pipeline(LinearRegression(n_jobs=-1))","3193315f":"model_val_scores(LR_model, X_train , y_train)","dd84d51b":"learning_curves(LR_model, X_train, y_train)","d52c6ef0":"housing_must_corr_vars = housing_prep[['total_rooms','median_income','bedrooms_per_room','rooms_per_household','median_house_value','<1H OCEAN','INLAND','NEAR OCEAN','NEAR BAY','ISLAND']]\n\n#housing_must_corr_vars.head(2)","946f1e3c":"X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split_(housing_must_corr_vars,'median_house_value' )","0ec08ecf":"#Training and testing the model\nLR_model.fit(X_train_corr, y_train_corr)\n\ny_pred = LR_model.predict(X_test_corr)","fec70467":"# Model's performance\nperformance_metrics(y_test_corr, y_pred)","343c5e80":"#Train and test the model\nLR_model.fit(X_train, y_train)\n\ny_pred = LR_model.predict(X_test)","801c7ad3":"# Model's performance\nperformance_metrics(y_test, y_pred)","2da64b32":"from sklearn.linear_model import Ridge","25aee0f8":"Ridge_model = make_pipeline(StandardScaler(), Ridge(alpha=1, solver='cholesky'))","919b5427":"model_val_scores(Ridge_model, X_train, y_train)","fa34c536":"learning_curves(Ridge_model, X_train, y_train)","8c489926":"Ridge_model.get_params().keys()","7080e774":"param_grid = {'ridge__alpha':np.arange(0.001, 5, 0.1 ),\n              'ridge__tol':np.arange(0.00001, 0.1, 0.5),\n              'ridge__solver':['auto','cholesky']}\n\nRidge_model_best_params = GridSearchCV_(Ridge_model, param_grid , X_train, y_train)","46c8ca1f":"Ridge_model_best_params.fit(X_train, y_train)\n\ny_pred = Ridge_model_best_params.predict(X_test)","c3a85e36":"performance_metrics(y_test, y_pred)","89568430":"from sklearn.linear_model import Lasso","cec641c6":"Lasso_model = make_pipeline(StandardScaler(),  Lasso(alpha=0.01, max_iter=10000))","2425dfc0":"model_val_scores(Lasso_model, X_train, y_train)","cafddd92":"learning_curves(Lasso_model, X_train, y_train)","7d873473":"Lasso_model.get_params().keys()","ceb1e5e9":"param_grid = {'lasso__alpha':np.arange(0.0001, 0.1, 0.01)}\n\nLasso_best_params = GridSearchCV_(Lasso_model, param_grid, X_train, y_train)","320751a1":"Lasso_best_params.fit(X_train, y_train)\n\ny_pred = Lasso_best_params.predict(X_test)","e3ae6746":"performance_metrics(y_test, y_pred)","74b443b2":"from sklearn.linear_model import ElasticNet","12838ded":"ELN_model  = make_pipeline(StandardScaler(), ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=100000))","c8b53813":"model_val_scores(ELN_model, X_train, y_train)","17765f06":"learning_curves(ELN_model, X_train, y_train)","9497b9e6":"ELN_model.get_params().keys()","7d04c0c4":"param_grid = {'elasticnet__alpha':np.arange(0.001, 0.1, 0.1 ),\n              'elasticnet__l1_ratio':np.arange(0, 1, 0.1)}\n\nELN_model_best_params = RandomizeSearchCV_(ELN_model, param_grid, X_train_corr, y_train_corr )","d8a65d9c":"learning_curves(ELN_model_best_params, X_train, y_train)","0956b87a":"ELN_model_best_params.fit(X_train, y_train)\n\ny_pred = ELN_model_best_params.predict(X_test)","d2e78b26":"performance_metrics(y_test_corr, y_pred)","6393fefd":"from sklearn.svm import SVR","c907d91f":"SVR_model = make_pipeline( StandardScaler(), SVR(epsilon=1.5, kernel='linear', C=100) )","f30f96fc":"model_val_scores(SVR_model, X_train, y_train)","20817f19":"learning_curves(SVR_model, X_train, y_train)","f68304e3":"SVR_model.get_params().keys()","7d80c2d1":"param_grid = {'svr__C':np.arange(10, 50, 5),\n              'svr__epsilon':np.arange(1, 6, 1),\n              'svr__kernel':['rbf','linear']}\n\nSVR_best_params = RandomizeSearchCV_(SVR_model, param_grid , X_train, y_train)","39781e9c":"SVR_best_params.fit(X_train, y_train)","314504de":"y_pred = SVR_best_params.predict(X_test)","93f35bd3":"performance_metrics(y_test, y_pred)","190ee1c8":"from sklearn.neighbors import KNeighborsRegressor","22abdacb":"KNR_model = make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=15,n_jobs=-1 ))","a8b43837":"model_val_scores(KNR_model, X_train, y_train)","2687095d":"learning_curves(KNR_model, X_train, y_train)","8401e17c":"KNR_model.get_params().keys()","c02c4a10":"param_grid = {'kneighborsregressor__n_neighbors':np.arange(5, 100, 1),\n             'kneighborsregressor__p':[1,2],\n             'kneighborsregressor__weights':['uniform','distance'],\n             'kneighborsregressor__leaf_size':np.arange(20,50,5)}\n\n\nKNR_model_best_params = RandomizeSearchCV_(KNR_model, param_grid , X_train, y_train)","f5bf0575":"learning_curves(KNR_model_best_params, X_train, y_train)","a4c63930":"KNR_model_best_params.fit(X_train, y_train)\n\ny_pred = KNR_model_best_params.predict(X_test)","e3c34f76":"performance_metrics(y_test, y_pred)","8b642abb":"from sklearn.tree import DecisionTreeRegressor","0ce93e83":"DTR_model = make_pipeline(DecisionTreeRegressor())","5fc5f004":"model_val_scores(DTR_model ,X_train, y_train)","91761151":"learning_curves(DTR_model, X_train, y_train)","38d2c72e":"DTR_model.get_params().keys()","7c9f84fd":"#Let's gain a comprehensive understanding of these hyperparameters using tree visualizations.\nparam_grid = {'decisiontreeregressor__max_depth':np.arange(5, 30, 1), \n              'decisiontreeregressor__max_features':np.arange(7, 12, 1)               \n             }\n\nDTR_model_best_params = GridSearchCV_(DTR_model, param_grid , X_train, y_train)","55bd4f59":"learning_curves(DTR_model_best_params, X_train, y_train)","2fe66441":"DTR_model_best_params.fit(X_train, y_train)\n\ny_pred = DTR_model_best_params.predict(X_test)","39443e44":"performance_metrics(y_test, y_pred)","772d320b":"from sklearn.ensemble import RandomForestRegressor","a91b2f41":"RFR_model  = make_pipeline(RandomForestRegressor(n_estimators=12, n_jobs=-1))","69a281ab":"model_val_scores(RFR_model, X_train, y_train)","6323a074":"learning_curves(RFR_model, X_train, y_train)","8d04686d":"RFR_model.get_params().keys()","2653221d":"param_grid = {'randomforestregressor__max_depth':np.arange(10, 40, 10),\n             'randomforestregressor__n_estimators':np.arange(200, 300, 20)}\n\nRFR_model_best_params = GridSearchCV_(RFR_model, param_grid , X_train, y_train)","3f2f728c":"learning_curves(RFR_model_best_params, X_train, y_train)","e9f540e9":"RFR_model_best_params.fit(X_train, y_train)\n\ny_pred = RFR_model_best_params.predict(X_test)","2e6dc728":"performance_metrics(y_test, y_pred)","4ba618ec":"from sklearn.ensemble import AdaBoostRegressor","9c02fdb2":"ada_model = make_pipeline(AdaBoostRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=200, learning_rate=0.5))","17bbc9ab":"model_val_scores(ada_model, X_train, y_train)","86b2d51a":"learning_curves(ada_model, X_train, y_train)","39dee550":"ada_model.get_params().keys()","fe136794":"param_grid = {'adaboostregressor__base_estimator__max_depth':np.arange(5,20,5),\n              'adaboostregressor__n_estimators':np.arange(100, 300, 50),\n              'adaboostregressor__learning_rate' : np.arange(0.001, 0.9, 0.9)\n               }\n\nada_best_params = RandomizeSearchCV_(ada_model, param_grid, X_train, y_train )","79d0394f":"learning_curves(ada_best_params, X_train, y_train)","f562c350":"ada_best_params.fit(X_train, y_train)\n\ny_pred = ada_best_params.predict(X_test)","e4f7f5c1":"performance_metrics(y_test, y_pred)","434f12ee":"final_RMSE = pd.DataFrame( data  = [[44623.94, 45386.56, 51730.20, 56308.76, 60356.87, 60372.75, 60381.28, 60381.32, 63395.09],\n                                    [0.791,0.783,0.719,0.667,0.6179,0.6177,0.61766,0.61763,0.578]],\n                          columns  = ['Random Forest Regressor','AdaBoost Regressor','KNeighbors Regressor',\n                                   'Decision Tree Regressor','ElasticNet','Ridge', 'Lasso', 'Linear Regression',   \n                                   'Support Vector Regressor'],\n                         index =['RMSE','R2'])\n\n\nfinal_RMSE = final_RMSE.T\n\ncm = sns.light_palette('green', as_cmap=True)\n\nfinal_RMSE = final_RMSE.style.background_gradient(cmap=cm)\nfinal_RMSE","750846ac":"> As can be seen from the table below, Random Forest Regressor resulted to be the best model for this dataset because of:\n\n- highest R^2 score\n- lowest root mean squared error\n","489a4f91":"#### Create functions ","e7afa259":"### Random Forest ","213b62ef":"These histograms reveal a few things :\n\n- The median house value was capped\n- Machine Learning algorithms may learn that prices will never go beyond that limit \"$500,000\" \n- Some variables are tail heavy : they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. \n    \n","b0f8afd0":"- <a src='https:\/\/www.amazon.fr\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1491962291'> Hands-On Machine Learning with Scikit-Learn and TensorFlow <\/a>\n- <a src='https:\/\/machinelearnia.com\/machine-learning\/'>MachineLearnia<\/a>\n- <a src='https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing'>Sklearn Preprocessing<\/a>\n- <a src='https:\/\/machinelearningmastery.com\/linear-regression-for-machine-learning\/'>Machine Learning Mastery: Linear Regression for Machine Learning<\/a>\n- <a src='https:\/\/medium.com\/analytics-vidhya\/writing-math-equations-in-jupyter-notebook-a-naive-introduction-a5ce87b9a214'>Writting math equations <\/a>\n- <a src='https:\/\/towardsdatascience.com\/k-nearest-neighbors-94395f445221'> towardsdatascience KNN <\/a>\n- <a src='https:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2'> towardsdatascience Random Forest <\/a>","6bb74fec":"### Import Necessary Libraries","44dcaef9":"- Thank you for reading my Notebook. Please feel free to improve me by suggesting anything \n- Please upvote if you found this kernel useful!","225b22ca":"Decision Trees limitations :\n- Decision Trees is that they are very sensitive to small variations in the training data. In other words, if the training data is changed the resulting decision tree can be quite different, and in turn, the predictions can be quite different\n- Also Decision trees are computationally expensive to train, carry a big risk of overfitting, and tend to find local optima because they can\u2019t go back after they have made a split.\n- Random Forests can limit this instability by averaging predictions over many trees ","f741d1fb":"## Ensemble Methods","05892805":"#### Correlation Matrix","5d0ef0fa":"#### Remove Missing Values","ea162abf":"### ElasticNet","3dfcb499":"### AdaBoost","12a714e0":"## Preprocessing","c16e3c6a":"#### Handle categorial variables","82c22d7b":"> ### Performance ranking :","c8a42594":"- Scikit-Learn uses the Classification And Regression Tree (CART) algorithm to train Decision Trees .\n- the idea is really quite simple: the algorithm first splits the training set into two subsets using a single feature k and a threshold $t_{k}$, then it splits the subsets using the same logic, then the sub-subsets, and so on, in a way that minimizes the MSE.\n- To avoid overfitting the training data, we need to restrict the Decision Tree\u2019s freedom (max_depth, min_samples_split, min_samples_leaf  ...) during training. As we know , this is called regularization \n- One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don\u2019t require feature scaling or centering also dummy encoding\n-  CART cost function for regression equation :\n\n$J(k,t_{K}) = \\frac{m_{left}}{m}MSE_{left}+\\frac{m_{right}}{m}MSE_{right}$   \n\n\nwhere : $\\binom{MSE_{node} = \\sum _{i \\in node} ( \\hat{y}_{node} - y^{(i)}) ^{2}}{\\hat{y}_{node} = \\frac{1}{m_{node}} \\sum _{i \\in node} y^{(i)}}$","a12f80a8":"- For a linear model, regularization is typically achieved by constraining the weights of the model. \n- We will look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights.","1dfb5287":"This geographical scatterplot of the data tells us that the housing prices are very much related to the location (close to the ocean) and to the population density","04ad8cc8":"- **learning curves**: \n    - Learning curves are plots that show changes in learning performance over time in terms of experience.\n    - Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.\n    - Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain.\n- **Grid search** \n     - performs a combination of hyperparameter tuning in order to determine the optimal combination values for a given model.\n     - The grid search approach is fine when you are exploring relatively few combinations\n\n- **Randomized Search** \n     - Can be used in much the same way as the GridSearchCV , but instead of trying out all possible combinations , it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.\n\n\n\n- **Regression Evaluation Metrics**\n\n  - <i>Mean Absolute Error (MAE)<\/i>:\n  \n      - is the mean of the absolute value of the errors\n      - sometimes it is called the Manhattan norm (l1 norm).\n      - Mathematical Formula :   $MAE(X,h)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}|h(x^{i})-y^{i}|$ \n  \n  - <i> Mean Squared Error (MSE)<\/i>: \n      - is the mean of the squared errors\n      - Mathematical Formula:   $MSE(X,h)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}(h(x^{i})-y^{i})^{2}$\n  \n  - <i>Root Mean Squared Error (RMSE)<\/i>:\n       - is the square root of the mean of the squared errors \n       - corresponds to the Euclidian norm (l2 norm)\n       - RMSE is more sensitive to outliers than the MAE\n       - RMSE is interpretable in the \"y\" units\n       - Mathematical Formula: :  $RMSE(X,h)=\\sqrt{ \\frac{1}{m}\\sum\\limits_{i=1}^{m}(h(x^{i})-y^{i})^{2}}$ \n  \n  - <i>Coefficient of determination (R2)<\/i>:\n       - It is used to check how well-observed results are reproduced by the model, depending on the ratio of total deviation of results described by the model.\n       - the value of R2 range from 0 to 1 \n           - A model with an R2 equal 1 perfetly predicts the target variable, whereas a model with R2 of 0 always fails to predict the target variable\n       - Mathematical Formula : $R^{2}= 1 - \\frac{SS_{res}}{SS_{tot}}$   Where,\n       \n            - $SS_{res}$ is the sum of squares of the residual errors between the actual y and the predicated y.\n            - it's representing the ML algorithm error score \n            - $SS_{tot}$ is the total sum of the errors (the sum of the squared deviation of the actual y from the                centre)\n            - it is representing the Mean Model","0fd2e10b":"- Another regularized version of Linear Regression : just like Ridge Regression, it adds a regularization term to the cost function, but it uses the l1 norm of the weight vector instead of half the square of the l2 norm\n- Lasso Regression cost function equation : $J(\\theta) =MSE(\\theta) + \\alpha \\sum \\limits _{i=1} ^{n} |\\theta_{i}| $\n- Lasso Regression automatically performs feature selection \n- Specifying penalty=\"l1\" in linear regression models like SGD indicates that we want to add a regularization term to the cost function","5e2ca3f2":"-  \"Sorry if i made any mistakes. English is not my native language\"","b53099df":"- linear Regression model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term)\n- Linear Regression models use two different ways to compute the model parameters that best fit the model to the training set :\n    - Using a direct \u201c**closed-form**\u201d equation that directly computes the model parameters that minimize the cost function over the training set , like the Normal Equation\n    - Using an **iterative optimization approach** that gradually tweaks the model parameters in order to minimize the cost function over the training set, like  Gradient Descent (GD)\n        - Some few variants of Gradient Descent : Batch GD, Mini-batch GD, and Stochastic GD\n\n- Linear Assumption\n    - **Linear Assumption** :Linear regression assumes that the relationship between our input and output is linear, if it is not the case we may need to transform data to make the relationship linear (e.g. log transform for an exponential )\n    - **Remove Noise** : Linear regression assumes that your input and output variables are not noisy\n    - **Remove Collinearity** :Linear regression will over-fit your data when you have highly correlated input variables\n    - **Gaussian Distributions** : Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\n    - **Rescale Inputs** : Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization.\n","21d33d52":"KNN how it works :\n  - Calculate the distance between the data sample and every other sample with the help of a method such as Euclidean.\n  - Sort these values of distances in ascending order.\n  - Choose the top K values from the sorted distances.\n  - return the mean of the nearest K neighbors.\n\n- KNN is a non-parametric algorithm because it does not assume anything about the training data. This makes it useful for problems having non-linear data.\n- KNN can be very sensitive to the scale of data as it relies on computing the distances. For features with a higher scale, the calculated distances can be very high and might produce poor results. It is thus advised to scale the data before running the KNN.\n- KNN can be computationally expensive both in terms of time and storage, if the data is very large because KNN has to store the training data to work. This is generally not the case with other supervised learning models.","39582733":"- SVM Regression tries to fit as many instances as possible on the street while limiting margin violations (instances off the street)\n- The width of the street is controlled by a hyperparameter \u03b5 (epsilon)\n- SVMs are sensitive to the feature scales\n- The hyperparameter C acts like a regularization hyperparameter : if our model is overfitting, we should reduce it, and if it is underfitting, we should increase it \n\n\n","b1faaaa7":"### Regularized Linear Models","0dd8ca30":"- Elastic Net is a middle ground between Ridge Regression and Lasso Regression\n- The regularization term is a simple mix of both Ridge and Lasso\u2019s regularization terms, and you can control the mix ratio r\n    - When r=0 ,Elastic Net is equivalent to Ridge Regression\n    - When r = 1,Elastic Net is equivalent to Lasso Regression \n- Elastic Net cost function Equation : $J(\\theta) =MSE(\\theta) + r\\alpha \\sum \\limits _{i=1} ^{n} |\\theta_{i}| + \\frac{1-r}{2} \\alpha \\sum \\limits _{i=1} ^{n} \\theta ^{2}_{i} $","c24fa08a":"## Decision Trees","89ecdc2c":"## Support Vector Machine ","39ad55ee":"####  Lasso Regression","4b39e5dd":"### Feature Engineering\n1 - Add new variables :","d2871747":"#### Handle noisy data","13e9a223":"- Linear Regression model uses the Normal Equation which directly computes the model parameters that best fit the model to the training set (the model parameters that minimize the cost function over the training set)\n- Linear Regression model equation :\n  $\\hat{Y} = \\theta_{0} + \\sum \\limits _{i=1} ^{n} X_{i}\\theta_{i} $\n  \n   - \u0177 is the predicted value\n   - n is the number of features.\n   - $X_{i}$ is the $i^{th}$ feature value.\n   - $\\theta$ is the model parameter (including the bias term $\\theta_{\u03b8}$  and the feature weights $\\theta_{1}$ , $\\theta_{2}$ ,..., $\\theta_{3}$ ).\n\n\nNormal Equation : \n\n- The Normal Equation : $\\hat{\\theta} = (X^{T}.X)^{-1}.X^{T}.y $\n   - $\\hat{\\theta}$ is the value of \u03b8 that minimizes the cost function\n   - y is the vector of target values containing $y_{1}$ to $y_{m}$\n   - $X^{T}.X$ is the dot product of $X^{T}$ and $X$\n   - $X^{T}$ is the transpose of $X$ \n- The Normal Equation gets very slow when the number of features grows large \n- The Normal Equation handles large training sets efficiently, provided they can fit in memory\n- Feature scaling is not necessary\n- Predictions are very fast\n\n","a948aa06":"- The total_bedrooms variable have missing values \n","afca5f36":"#### Splitting the data ","fa80d09c":"- An Ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions (increase performance) than any individual model, a model comprised of many models is called an Ensemble model.\n- Ensemble methods can decrease variance using bagging approach, bias using a boosting approach, or improve predictions using stacking approach","7cf1c2f2":"## Modelling","74ed54d2":"###  Data Analysis","17a2c3cb":"- Random Forest is an ensemble of Decision Trees , generally trained via the bagging method (or sometimes pasting)\n     - **Bagging method** (Bootstrap Aggregation) : refers to random sampling with replacement .It is can be used to reduce the variance for those algorithm that have high variance, typically decision trees\n     - **Pasting method**: it refers to random sampling without replacement with the same features as the bagging method\n- RandomForestRegressor  has all the hyperparameters of a DecisionTreeRegressor (to control how trees are grown), plus all the hyperparameters of a BaggingRegressor to control the ensemble itself.\n- The trees in random forests are run in parallel. There is no interaction between these trees while building the trees.\n- It can handle thousands of input variables without variable deletion.\n- It operates by constructing a multitude of decision trees at training time and outputting the mean prediction of the individual trees.","fc4e53b3":"# Welcome to my First Kernel !\n","799b922a":"- Let's try to feed some correlated features to the learning algorithm and see if we could achieve better performance than before.\n\n> Based on the correlation matrix and the histograms seen from above :\n   - The most correlated variables with the target are median income, total rooms , bedrooms per room and rooms per household\n   - There is no collinearity between these variables (the correlation between these variables against each other is not too strong the coefficient of correlation doesn't exceed |0.7|)\n   - the correlated features follow a Normal Distribution\n\n","bb2a0980":"#### Ridge Regression","71727bb6":"## K-Nearest Neighbors ","83c6cbeb":"- AdaBoost is a **boosting** algorithm .\n  - Boosting algorithm : refers to a group of algorithms that utilize weighted averages to make weak learners into stronger learners. Each model that runs, dictates what features the next model will focus on.\n- How it works : the new predictor corrects  its predecessor by paying a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases.\n- If AdaBoost ensemble is overfitting the training set, we can try reducing the number of estimators or more strongly regularizing the base estimator.","79e155fd":"<i> we will assume that we are looking at a Maximizing metric, which means that bigger scores on the y-axis indicate more or better learning.<\/i>\n \n- The training loss remains flat over training size\n- The validation loss decreases to a point and begins increasing again then flattening in the end with the training score in the same value.\n","83eea3f2":" - In this case, the linear regression model is unable to generalize new instances, resulting in an increase in generalization error. Therefore a high RMSE on the test data\n - Let's train the linear regression model on the X_train and y_train dataset and test it on the X_test dataset","626b2e9c":"#### Visualization of Geographic Data  ","9eaff88a":"#### References","062bf939":"### LinearRegression","341b5d7e":"# Linear Models","568123ce":"- Ridge Regression is a regularized version of Linear Regression\n- The regularization term equal to : $ \\alpha \\sum \\limits _{i=1} ^{n} \\theta ^{2}_{i}$ is added to the cost function. This forces the algorithm to not only fit the data but also keep the model weights as small as possible\n- Ridge Regression cost function equation : $J(\\theta) =MSE(\\theta) + \\frac{1}{2} \\alpha \\sum \\limits _{i=1} ^{n} \\theta ^{2}_{i}$\n- Specifying penalty=\"l2\" in linear regression models like SGD indicates that we want to add a regularization term to the cost function","eed1ee06":"##### Some explorations : ","ae2ded67":"- Unlike the linear regression model trained only with the correlated features, we got a low Root Mean Squared Error in this case \n![](http:\/\/)  ","336c9201":"### table of contents :\n\n- **Introduction**:\n  - About California housing dataset : \n    - Dependent variable : median_house_value\n    - Independent variables : \n        - Numerical variables : housing_median_age, total_rooms, total_bedrooms, population,         households, median_income ,longitude and latitude\n        - Categorial variables : ocean_proximity\n- **Data exploration ** :\n  - data's shape and type\n  - distribution of variables\n  - Find missing values\n  - Correlation between independent and dependent variables\n- **Feature Engineering** :\n  - Add new variables\n  - Handle missing values\n  - Handle noisy data \n- **Preprocessing** :\n  - Encode the Data\n  - Split data into training and validation set\n- **Modelling** :\n  - The Algorithms that i used in this notebook, plus a brief description of each one\n      - Linear Models : LinearRegression ,Laso, Ridge, ElasticNet\n      - Support Vector Machine Regressor\n      - K-Nearest Neighbors Regressor\n      - Decision Tree Regressor\n      - Ensemble methods : RandomForestRegressor and AdaBoost\n  - Fine Tune Algorithms\n  - The Metrics that i used to quantify the models' performance, plus a brief description of each one \n      - MAE\n      - MSE\n      - RMSE\n      - R2 \n  \n  \n"}}