{"cell_type":{"2e844225":"code","695d98cf":"code","fdef2bbd":"code","a576b2db":"code","3c193034":"code","143fbca6":"code","063c30a0":"code","80dd44bf":"code","8dc63747":"code","53ee544b":"code","f7101d3d":"code","65605113":"code","2828ac86":"code","97781f11":"code","3b926d7d":"code","bebf73fc":"code","ad2e8f7a":"code","e5c2d2a3":"code","b7c99c43":"code","c1ef85ac":"code","835a1d31":"code","ed3c67d6":"code","c9c5a61b":"code","fc14afee":"code","60f0988a":"code","c6131583":"code","bed9cbce":"code","ae91a92c":"code","a4020bc6":"code","9723900b":"code","068ff0a1":"code","910cf86f":"code","5c297bca":"code","15ae714a":"code","df55422b":"code","e73960bf":"code","12865ee5":"code","f9c460b1":"code","7468f4df":"code","807370d8":"code","9c250b2a":"code","89ff2fa1":"code","81a26449":"code","43aeae72":"code","2455954e":"code","634fef26":"code","44342585":"code","1a9f2384":"code","5a5ea8fb":"code","02d1c7a8":"code","e903186b":"code","ec59728a":"code","4e4da7c0":"code","2e7c3edb":"code","3343122c":"code","24edad6d":"code","f0bd1eb4":"code","5601936b":"code","e03b6a05":"code","9326454b":"code","062d30ad":"code","cfdb5c26":"code","3b217ede":"code","67606de6":"code","489f8426":"code","e9fc89fd":"markdown","e3a5c585":"markdown","8c6ff5a5":"markdown","4ad01b1c":"markdown","ba11fc4c":"markdown","84ec1d15":"markdown","cb8ee73f":"markdown","07bdb078":"markdown","44f1f13b":"markdown","3e08f196":"markdown","5c0337ff":"markdown","20fcf27d":"markdown","352a11a3":"markdown","fb012e28":"markdown","4f5c56a4":"markdown","86996f61":"markdown","1d502113":"markdown","8b2f21c1":"markdown","8fa59241":"markdown","29208b91":"markdown","2bfcfece":"markdown","4d45fabd":"markdown","877ed8e2":"markdown","3489ec35":"markdown","31aa9d10":"markdown","49e7e13e":"markdown","1877feb1":"markdown","6e55b016":"markdown","53b3d987":"markdown","514c6e98":"markdown","d772a585":"markdown","97275380":"markdown","b7a8be76":"markdown","09ec05a6":"markdown","36946169":"markdown","c32b761f":"markdown","e777e195":"markdown","9873fa99":"markdown","3b384a7b":"markdown","0f1187d5":"markdown","457eaeca":"markdown","64c16f43":"markdown","cae04b5c":"markdown","6d9653c8":"markdown","fdd23b29":"markdown","a853ac4e":"markdown"},"source":{"2e844225":"import numpy as np   # for numerical computation with n dimensional arrays \nimport pandas as pd # for data manipluation \nimport matplotlib.pyplot as plt # for data visualization \nimport seaborn as sns # for conditional plotting ","695d98cf":"data = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")  # pandas method to read csv \ndata.head()","fdef2bbd":"data.info()  # for complete understanding about data ","a576b2db":"data.describe()  # checking descripion ","3c193034":"data.describe(include= \"object\")  # checking description including object ","143fbca6":"data.isnull().sum()  #checking missing values ","063c30a0":"# Checking Categorical Features in dataset and counting it \n\ncat_features = [columns_name for columns_name in data.columns if data[columns_name].dtype == \"O\"]\nprint(\"Number of Categorical Features we have in data is : {}  \".format(len(cat_features)))\nprint(\"Here is Listed features {} \".format(cat_features))\n","80dd44bf":"# Checking Numerical Features in dataset and counting it \n\nnum_features = [columns_name for columns_name in data.columns if data[columns_name].dtype != \"O\"]\nprint(\"Number of Numerical Features we have in data is : {}  \".format(len(num_features)))\nprint(\"Here is the Listed Numerical features {} \".format(num_features))\n","8dc63747":"for i  in cat_features:\n  count = len(data[i].unique())\n  print(\"The Cardinality of each column {} : {} \".format(i,count))\n\n# hence the unqiue value in  date column is very large, Thus\n# Cardinality of Data is high which poses several problem to the model in terms of efficiency \n# because we need to do numerical encoding or create dummies ","53ee544b":"data[\"Date\"] = pd.to_datetime(data[\"Date\"])   #Converting into Date\ndata[\"Year\"] = data[\"Date\"].dt.year           #abstracting year in different column \ndata[\"Month\"] = data[\"Date\"].dt.month         #abstracting month in diffrent column  \ndata[\"Day\"] = data[\"Date\"].dt.day             #abstracting day in diffrent column  ","f7101d3d":"data.head()  # we can see the columns in the end ","65605113":"# Dropping Date Column \n\ndata.drop(\"Date\", axis = 1, inplace = True)\ndata.head()","2828ac86":"categorical_features = [column_name for column_name in data.columns if data[column_name].dtype ==\"O\"]","97781f11":"categorical_features    # now we have the features othe than date ","3b926d7d":"missing_cat = data[categorical_features].isnull().sum() # Total missing values \nprint(missing_cat)","bebf73fc":"plt.figure(figsize= (8,4))\nplt.bar(missing_cat.index,missing_cat.values ) # by visualization we can see that winDir3pm has large number of missing values ","ad2e8f7a":"cat_features_with_null = [features for features in categorical_features if data[features].isnull().sum()]  # list will return with categorical features\nfor each_feature in cat_features_with_null:\n  data[each_feature] = data[each_feature].fillna(data[each_feature].mode()[0])     #each cat_feature is filled by most repeated value \n\n","e5c2d2a3":"data[categorical_features].isnull().sum()  # here we can see ther is no missing values now ","b7c99c43":"num_col = [num_col for num_col in data.columns if data[num_col].dtype !=\"O\"] \n#numerical features extraction from the data ","c1ef85ac":"missing_num = data[num_col].isnull().sum()    # counting missing values in each  num_feature \nmissing_num","835a1d31":"plt.figure(figsize=(14,6))\nplt.bar(missing_num.index,missing_num.values)    # ploting missing values \nplt.xticks(rotation=70)","ed3c67d6":"num_col_with_null_values = [col for col in num_col if data[col].isnull().sum()]\nmissing_values = data[num_col_with_null_values].isnull().sum()\nmissing_values","c9c5a61b":"# we can not directly impute missing values by using mean becuase there could be a chance of outliers in the data \n# lets check by box plot \nplt.figure(figsize=(12,10))\ndata.boxplot(num_col_with_null_values,rot = 90)","fc14afee":"# we can see that there are many outliers in specially Rainfall and other featrures \n# lets handle this ","60f0988a":"# Remove Outliers from Numerical Features \n\nfeatures_with_outliers = num_col_with_null_values\nfor feature in features_with_outliers:\n  q1 = data[feature].quantile(0.25)\n  q3 = data[feature].quantile(0.75)\n  IQR = q3-q1\n  lower_limit = q1 - (IQR*1.5)\n  upper_limit = q3 + (IQR*1.5)\n  data.loc[data[feature]<lower_limit, feature] = lower_limit\n  data.loc[data[feature]>upper_limit, feature] = upper_limit\n\n# after running this the outliers will be removed from our numerical features of the data ","c6131583":"plt.figure(figsize=(12,5))\ndata.boxplot(num_col_with_null_values,rot = 90)    # we can see there is no outliers now ","bed9cbce":"for features in num_col_with_null_values:\n  data[features] = data[features].fillna(data[features].mean())   # all missing values imputed by mean ","ae91a92c":"data[num_col_with_null_values].isnull().sum()  # we can see there is no null values now ","a4020bc6":"# Exploring Numerical Variables\n\ndf_num = data.select_dtypes(include = ['float64'])\ndf_num.head()\ndf_num.columns","9723900b":"df_num.shape\n\ndata['RainTomorrow'].unique()\n\n#View the Frequency Distribution\ndata['RainTomorrow'].value_counts()\n\nf, ax = plt.subplots(figsize=(6, 8))\nax = sns.countplot(y=\"RainTomorrow\", data=data)\nplt.show()\n\n# Histogram for Numerical Variables\n\ndf_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);\n\n# Exploring Categorical Variables\n\ndf_cat = data.select_dtypes(include = ['O'])\ndf_cat.head()\n\ndf_cat.shape","068ff0a1":"# Exploring Targeted Variable \n\nsns.countplot(data[\"RainTomorrow\"])\n\n# we can see there is more numbers of \"no\" values and it is imbalanced data \n# Although we can balance it but there is no need because it is actuall demographics ","910cf86f":"sns.lineplot(data= data,x=\"Sunshine\",y=\"Rainfall\",color = \"green\")\n\n# we can see sunshine is inversely proportional to the Rainfall ","5c297bca":"sns.lineplot(data= data,x=\"Sunshine\",y=\"Evaporation\",color = \"blue\")\n\n# we can see sunshine is directly proportional to the Rainfall ","15ae714a":"data.isnull().sum() # as there is no null values now \n# and for Machine learning we know machine understand only numerical data so we need to convert \n# categorical features into Dummy variables or need to do encoding ","df55422b":"cat_fea = [i for i in data.columns if data[i].dtype == \"O\"]\ncat_fea   # Finding final categorical features that is need to be turned into Numericals Encoding means numerical values ","e73960bf":"# here we are defining a function \ndef encode_cat(feature):   # function will take feature \n  mapping_dic = {}         # empty dic \n  unique_val = list(data[feature].unique())    # extract particular feature from data and convert into list \n  for idx in range(len(unique_val)):      #loop will run till the len of list \n    mapping_dic[unique_val[idx]] = idx    # assign the index to the index of the list \n  return mapping_dic       # return the feature \n","12865ee5":"for i in cat_fea:   # we have the list of categorical features \n  data[i].replace(encode_cat(i), inplace = True)  \n  print(data.head())","f9c460b1":"data[\"RainTomorrow\"].unique()","7468f4df":"plt.figure(figsize =(24,12))\nsns.heatmap(data.corr(), annot = True)","807370d8":"y = data[\"RainTomorrow\"]  # dependent or target \nx = data.drop([\"RainTomorrow\"],axis = 1)  # indepedent or input ","9c250b2a":"# Exploring Targeted Variable \n\nsns.countplot(data[\"RainTomorrow\"])\n\n# as we see  can see here the data is unbalanced so we are going to use over sampling method for better forcasting ","89ff2fa1":"from imblearn.over_sampling import SMOTE\nsmote=SMOTE()\nx,y=smote.fit_resample(x,y)\n\nprint(x.shape,y.shape)","81a26449":"# we will use ExtraTreesRegressor for getting the weight of evidence \nfrom sklearn.ensemble import ExtraTreesRegressor\nfeat_sel_model = ExtraTreesRegressor()\nfeat_sel_model.fit(x,y)\nfeatures_imp = feat_sel_model.feature_importances_\nfeatures_imp","43aeae72":"features_imp_val = pd.Series(features_imp, index = x.columns)\nplt.figure(figsize=(10,8))\nfeatures_imp_val.nlargest(10).plot(kind = \"barh\")","2455954e":"from sklearn.model_selection import train_test_split   # testing and splitting \nx_train, x_test, y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0 )","634fef26":"print(\"Length of Training Data: {}\".format(len(x_train)))\nprint(\"Length of Testing Data: {}\".format(len(x_test)))","44342585":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\n","1a9f2384":"x_test = scaler.transform(x_test)","5a5ea8fb":"from sklearn.linear_model import LogisticRegression\nclassifier_logreg = LogisticRegression(solver='liblinear', random_state=0)\nclassifier_logreg.fit(x_train, y_train)","02d1c7a8":"y_pred = classifier_logreg.predict(x_test)\ny_pred","e903186b":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy Score: {}\".format(accuracy_score(y_test,y_pred)))","ec59728a":"print(\"Train Data Score: {}\".format(classifier_logreg.score(x_train, y_train)))\nprint(\"Test Data Score: {}\".format(classifier_logreg.score(x_test, y_test)))","4e4da7c0":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(classifier_logreg, x_test,y_test)\n","2e7c3edb":"y_pred_logreg_proba = classifier_logreg.predict_proba(x_test)\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_logreg_proba[:,1])\nplt.figure(figsize=(6,4))\nplt.plot(fpr,tpr,'-g',linewidth=1)\nplt.plot([0,1], [0,1], 'k--' )\nplt.title('ROC curve for Logistic Regression Model')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel('True Positive Rate')\nplt.show()","3343122c":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(classifier_logreg, x_train, y_train, cv = 5, scoring='accuracy')\nprint('Cross-validation scores:{}'.format(scores))\nprint('Average cross-validation score: {}'.format(scores.mean()))","24edad6d":"def evaluate(model):\n    model.fit(x_train,y_train)\n    accuracy=model.score(x_test,y_test)\n    \n    print('model name ',model)\n    print('accuracy ',accuracy)","f0bd1eb4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nlr=LogisticRegression()\nsvm=SVC()\ndt=DecisionTreeClassifier(max_depth=6)\nrf=RandomForestClassifier(max_samples=0.9)\nknn=KNeighborsClassifier(n_neighbors=5)\n\nmodels=[lr,dt,rf,knn]\n\nfor model in models:\n    evaluate(model)","5601936b":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(max_samples=0.9)\nclassifier.fit(x_train,y_train)","e03b6a05":"y_pred = classifier.predict(x_test)\ny_pred","9326454b":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy Score: {}\".format(accuracy_score(y_test,y_pred)))","062d30ad":"print(\"Train Data Score: {}\".format(classifier.score(x_train, y_train)))\nprint(\"Test Data Score: {}\".format(classifier.score(x_test, y_test)))","cfdb5c26":"print(\"Train Data Score: {}\".format(classifier.score(x_train, y_train)))\nprint(\"Test Data Score: {}\".format(classifier.score(x_test, y_test)))","3b217ede":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(classifier, x_test,y_test)\n","67606de6":"y_pred_logreg_proba = classifier_logreg.predict_proba(x_test)\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_logreg_proba[:,1])\nplt.figure(figsize=(6,4))\nplt.plot(fpr,tpr,'-g',linewidth=1)\nplt.plot([0,1], [0,1], 'k--' )\nplt.title('ROC curve for Logistic Regression Model')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel('True Positive Rate')\nplt.show()","489f8426":"import pickle\npickle.dump(rf, open('iri.pkl', 'wb'))\nloaded_model = pickle.load(open(\"iri.pkl\", 'rb'))\nresult = loaded_model.score(x_test, y_test)\nprint(result)","e9fc89fd":"## Cross Validation ","e3a5c585":"## univariate analysis ","8c6ff5a5":"## Bivariate Analysis ","4ad01b1c":"## Random Forest Classifier ","ba11fc4c":"# Model Building ","84ec1d15":"## Model Score ","cb8ee73f":"# Finalizing Algorithm \n\n* we will use Random Forest Classifier as it gives 90% Accuracy for our forecasting of tomorrow rain ","07bdb078":"## for Continous Features ","44f1f13b":"## Confusion Martix ","3e08f196":"# Importing Libraries ","5c0337ff":"* This dataset contains about 10 years of daily weather observations from many locations across Australia.\n\n* A binary classification Problem \n\n* Forecast Tomorrow will be rain or not ","20fcf27d":"## ROC Curve ","352a11a3":"# Checking Cardanility ","fb012e28":"## Model initilization ","4f5c56a4":"# Using Different Algos ","86996f61":"## Model Accuarcy ","1d502113":"### Information we get from the data \n* 145460 rows \n* 23 columns \n* 7 columns we have object \n* 16 columns we have folat \n","8b2f21c1":"# Handling Missing values ","8fa59241":"# Feature Scaling ","29208b91":"## Model Score \n\n* Checking underfitting and overfitting ","2bfcfece":"# spliiting data into tarining and testing ","4d45fabd":"# EDA","877ed8e2":"## Model Fitting ","3489ec35":"# over Sampling ","31aa9d10":"# Separation of Categorical (discrete) or Numerical (Contnious) ","49e7e13e":"# Corrlation","1877feb1":"## Model Accuracy ","6e55b016":"* This dataset contains about 10 years of daily weather observations from many locations across Australia.\n\n* A binary classification Problem \n\n* Forecast Tomorrow will be rain or not \nThis dataset contains about 10 years of daily weather observations from many locations across Australia. Before any processing, we must first obtain information about the data we are going to use.\n\n**Task to perform**\n\n predict the target variable Rain_Tomorrow\n  the column which are the input are as follows:\n\n\n*   DATE: the date if each day is provided to check the data is consistent, we can break date in day, month and year.\n*   LOCATION: Albury, New castle, Richmode and other areas of Australia is covered\n*   MIN TEMP: Min temperatura of each location is provided which helps to check the humidity of the environment\n*   MAX TEMP: Max temperature is given to check the probablity of rain\n*   RANFALL: Rainfall record is also given, there are som areas where 0.0 value is given so there were no rain happen about 10 years in that region.\n*   EVAPORATION: Mostly the value of evaporation is NAN in different areas.\n*   SUNSHNE: Sunshine value is mostly null in areas which the cloudy environment.\n*   WIND GUST DIR: The direction od wind blowing is provided there.\n*   WIND GUST SPEED: The speed of wind is also provided there.\n*   WindDir9am : The direction of wind at specific time is there so the comparison b\/w different times will help in analysis of Rain_Tomorrow.\n*   WindDir3pm : Speed direction at 3pm\n*   WindSpeed9am :  Within Australia, wind speeds are generally presented in kilometres per hour. Thats why different time stamp wind speed is given there.\n*   WindSpeed3pm : As above, due to change of wind speed per hour, this will help to predict the target variable.\n*   Humidity9am : The humidity ranges b\/w 6 hours in Australia.\n*   Humidity3pm : As above, 6 hours change is provided for analysis\n*   Pressure9am : Pressure rate per 6 hours is given\n*   Pressure3pm\t: As it ranges or varies from 6 hours, the data will help to find accurate results.\n*   Cloud9am : The weather changes about 6 hours \n*   Cloud3pm : 6 hours gap in cloudy env data will helpful in prediction.\n*   Temp9am\t: Temperature varien from north to south\n*   Temp3pm\t: Temperature also varies fom 6 hours time stamp\n*   RainToday\t: Rain Today is helpful to predict either its going to rai tomorrow.\n*   RainTomorrow : Rain Tomorrow is our target variable to predict the rain in Australia by using above given features.","53b3d987":"# Feature Estimation \n\n\n* we can also use weight of evidence \n* but here we will use  ExtraTreesRegressor ","514c6e98":"# spliiting data ","d772a585":"# Data Dictionary \n\n\n","97275380":"# Using Model for deployment or in production phase \n\n* We can use it now for production phase \n* it could be deployed on heruko or streamlit \n* we can develope web app on it \n* now we are dumbing our solution ","b7a8be76":"## Model Testing ","09ec05a6":"## Handling Cardinatlity issue ","36946169":"## Handling Missing Values for Categorical ","c32b761f":"## ROC Curve ","e777e195":"* number of unique vaues in cardinality could be \n* if there are more unique values then it is hard to do label encoding \n* also effect on the efficiency of the model ","9873fa99":"# WE have done \n* we have converted all categorical data into numerical values \n* first we separtated it and then we converted into numerical ","3b384a7b":"## Model Fitting ","0f1187d5":"# Loading Data ","457eaeca":"# Data Preprocessing ","64c16f43":"# Numerical Encoding ","cae04b5c":"## for categorical Variables ","6d9653c8":"# Problem Statement ","fdd23b29":"## Model Testing ","a853ac4e":"## Confusion Matrix "}}