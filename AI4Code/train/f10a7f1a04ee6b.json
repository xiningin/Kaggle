{"cell_type":{"05bfc28a":"code","550a2795":"code","c1104b0d":"code","876b65f0":"code","ea2685fc":"code","0b49595e":"code","e8e19a0a":"code","5951fea4":"code","209ab571":"code","75eb5042":"code","e8402405":"code","8748e5ad":"code","2e042746":"code","750d86ce":"markdown","20c7af87":"markdown","6a2399e0":"markdown","fc6eafb3":"markdown","fd5f8fff":"markdown"},"source":{"05bfc28a":"import numpy as np \nimport pandas as pd \n\nimport os\n","550a2795":"!pip install ktrain","c1104b0d":"import tensorflow as tf\nimport ktrain\nfrom ktrain import text\nfrom sklearn.model_selection import train_test_split","876b65f0":"#loading the train dataset\nticket_data = pd.read_excel('..\/input\/amsticketdata\/TicketData.xlsx', dtype = str)","ea2685fc":"train, test = train_test_split(ticket_data, test_size=0.2)\ntrain.head()","0b49595e":"(X_train, y_train), (X_test, y_test), preproc = text.texts_from_df(train_df=train,\n                                                                   text_column = 'Short description',\n                                                                   label_columns = 'Label',\n                                                                   val_df = test,\n                                                                   maxlen = 320,\n                                                                   preprocess_mode = 'bert')","e8e19a0a":"model = text.text_classifier(name = 'bert',\n                             train_data = (X_train, y_train),\n                             preproc = preproc)","5951fea4":"learner = ktrain.get_learner(model=model, train_data=(X_train, y_train),\n                   val_data = (X_test, y_test),\n                   batch_size = 14)","209ab571":"# # find out best learning rate?\nlearner.lr_find(max_epochs=2)\nlearner.lr_plot()","75eb5042":"learner.lr_plot(n_skip_beginning=625, n_skip_end=825)","e8402405":"#Essentially fit is a very basic training loop, whereas fit one cycle uses the one cycle policy callback\n\nlearner.fit_onecycle(lr = 2.8e-4, epochs = 1)\npredictor = ktrain.get_predictor(learner.model, preproc)\n","8748e5ad":"#sample dataset to test on\n\ndata = ['Job control ',\n        'Job Production control  abended with a rc 8',\n        'P&D Messaging\/ user unable to sign in']\npredictor.predict(data)","2e042746":"predictor.save('\/kaggle\/working\/bert')\nfrom IPython.display import FileLink\nFileLink(r'bert\/tf_model.h5')\n# FileLink(r'bert\/tf_model.preproc')\n","750d86ce":"After running above code, which takes 2-3 hours to run, I have found lr = 2.8e-4 as the best lr with minimum loss. If you want you can run again and try to zoom in to view the recommended lr in the graph","20c7af87":"Import all the data","6a2399e0":"# Install `ktrain`","fc6eafb3":"As per Google recommendation of BERT 14 is good bacth size for 320 maxlen\n\nPlease refer to this article: https:\/\/towardsdatascience.com\/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358\n\nAlso check google gitlab article about the batch size\nhttps:\/\/github.com\/google-research\/bert\/blob\/master\/README.md#out-of-memory-issues\n","fd5f8fff":"# Save the BERT model"}}