{"cell_type":{"8b2cdbc2":"code","1f013e06":"code","cd91bb3d":"code","087c7ae8":"code","127ad836":"code","640b2f56":"code","904e674b":"code","d132fc84":"code","bd6a755a":"code","6b636b55":"code","25a2d928":"code","a75ebdee":"code","1d7348c1":"code","1f85ffcb":"code","8a5dd3da":"code","a75b6b3a":"code","854bdb85":"code","f31d7337":"code","0b019eab":"code","594f3e01":"code","81d3efc2":"code","8d1d76b7":"code","84642ace":"code","35835f56":"code","952859b7":"code","f1b9336c":"code","dd9d44b0":"code","652b1630":"code","7019c0cd":"code","54f66201":"code","c267fadf":"code","abccc072":"code","70631d2c":"code","76975ec6":"code","01b63de8":"code","38e076db":"code","29f54e0f":"code","546d8405":"markdown","84e71222":"markdown","3bd22cf5":"markdown","d889abad":"markdown","22c74421":"markdown","cc8ebd46":"markdown","16af90c0":"markdown","6d4ce58b":"markdown","c36db75f":"markdown","1802a766":"markdown","ef8d1ed2":"markdown","a162858d":"markdown","59e260df":"markdown","63d9dba7":"markdown"},"source":{"8b2cdbc2":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","1f013e06":"from fastai.vision import *","cd91bb3d":"path = Path('\/kaggle\/input\/')\npath.ls()","087c7ae8":"df = pd.read_csv(path\/'train_v2.csv')\ndf.head()","127ad836":"tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.)","640b2f56":"np.random.seed(42)\nsrc = (ImageItemList.from_csv(path, 'train_v2.csv', folder='train-jpg', suffix='.jpg')\n       .random_split_by_pct(0.2)\n       .label_from_df(sep=' '))","904e674b":"data = (src.transform(tfms, size=128)\n        .databunch(num_workers=0).normalize(imagenet_stats))","d132fc84":"data.show_batch(rows=3, figsize=(12,9))","bd6a755a":"arch = models.resnet50","6b636b55":"acc_02 = partial(accuracy_thresh, thresh=0.2)\nf_score = partial(fbeta, thresh=0.2)\nlearn = create_cnn(data, arch, metrics=[acc_02, f_score], model_dir='\/tmp\/models')","25a2d928":"learn.lr_find()","a75ebdee":"learn.recorder.plot()","1d7348c1":"lr = 0.01","1f85ffcb":"learn.fit_one_cycle(5, slice(lr))","8a5dd3da":"learn.save('stage-1-rn50')","a75b6b3a":"learn.unfreeze()","854bdb85":"learn.lr_find()\nlearn.recorder.plot()","f31d7337":"learn.fit_one_cycle(5, slice(1e-5, lr\/5))","0b019eab":"learn.save('stage-2-rn50')","594f3e01":"data = (src.transform(tfms, size=256)\n        .databunch(num_workers=0).normalize(imagenet_stats))\n\nlearn.data = data\ndata.train_ds[0][0].shape","81d3efc2":"learn.freeze()","8d1d76b7":"learn.lr_find()\nlearn.recorder.plot()","84642ace":"lr=1e-2\/2","35835f56":"learn.fit_one_cycle(5, slice(lr))","952859b7":"learn.save('stage-1-256-rn50')","f1b9336c":"learn.unfreeze()","dd9d44b0":"learn.fit_one_cycle(5, slice(1e-5, lr\/5))","652b1630":"learn.recorder.plot_losses()","7019c0cd":"learn.save('stage-2-256-rn50')","54f66201":"learn.export()","c267fadf":"test = ImageItemList.from_folder(path\/'test-jpg').add(ImageItemList.from_folder(path\/'test-jpg-additional'))\nlen(test)","abccc072":"learn = load_learner(path, test=test)\npreds, _ = learn.get_preds(ds_type=DatasetType.Test)","70631d2c":"thresh = 0.2\nlabelled_preds = [' '.join([learn.data.classes[i] for i,p in enumerate(pred) if p > thresh]) for pred in preds]","76975ec6":"labelled_preds[:5]","01b63de8":"fnames = [f.name[:-4] for f in learn.data.test_ds.items]","38e076db":"df = pd.DataFrame({'image_name':fnames, 'tags':labelled_preds}, columns=['image_name', 'tags'])","29f54e0f":"df.to_csv(path\/'submission.csv', index=False)","546d8405":"To put this in a `DataBunch` while using the [data block API](https:\/\/docs.fast.ai\/data_block.html), we then need to using `ImageItemList` (and not `ImageDataBunch`). This will make sure the model created has the proper loss function to deal with the multiple classes.","84e71222":"## Multiclassification","3bd22cf5":"You won't really know how you're going until you submit to Kaggle, since the leaderboard isn't using the same subset as we have for training. But as a guide, 50th place (out of 938 teams) on the private leaderboard was a score of `0.930`.","d889abad":"...And fine-tune the whole model:","22c74421":"## Multi-label prediction with Planet Amazon dataset","cc8ebd46":"Contrary to the pets dataset studied in last lesson, here each picture can have multiple labels. If we take a look at the csv file containing the labels (in 'train_v2.csv' here) we see that each 'image_name' is associated to several tags separated by spaces.","16af90c0":"We use the LR Finder to pick a good learning rate.","6d4ce58b":"(This section will be covered in part 2 - please don't ask about it just yet! :) )","c36db75f":"Then we can fit the head of our network.","1802a766":"## fin","ef8d1ed2":"`show_batch` still works, and show us the different labels separated by `;`.","a162858d":"We use parentheses around the data block pipeline below, so that we can use a multiline statement without needing to add '\\\\'.","59e260df":"Private Leaderboard score: 0.9296 (around 80th)","63d9dba7":"To create a `Learner` we use the same function as in lesson 1. Our base architecture is resnet34 again, but the metrics are a little bit differeent: we use `accuracy_thresh` instead of `accuracy`. In lesson 1, we determined the predicition for a given class by picking the final activation that was the biggest, but here, each activation can be 0. or 1. `accuracy_thresh` selects the ones that are above a certain threshold (0.5 by default) and compares them to the ground truth.\n\nAs for Fbeta, it's the metric that was used by Kaggle on this competition. See [here](https:\/\/en.wikipedia.org\/wiki\/F1_score) for more details."}}