{"cell_type":{"cd8f73bc":"code","34604854":"code","bfe27fb8":"code","90559a4d":"code","6d4dc86e":"code","042fff44":"code","939651ce":"code","fa75594d":"code","21b49111":"code","fb92c738":"code","1eb1095f":"code","0fd0dd0e":"code","5ef0e417":"code","3a7b4d45":"code","7da835f1":"code","5e682d9c":"code","f24feb4c":"code","f9709cc3":"code","89b847bc":"code","7aef6ecd":"code","bf70a614":"code","42f94551":"code","a6bf5524":"code","a191b08d":"code","a43fd8b0":"code","d77df335":"code","a5d603d1":"code","82ebef8f":"code","e17f8ffb":"code","8e152ada":"code","3a514ec4":"code","473e7299":"code","dc1a6771":"code","0924b26b":"code","6b48efaa":"code","68b4bb95":"code","00736805":"code","e4b24a13":"code","22285320":"code","44faf8b5":"code","71f32fb2":"code","4ffef783":"code","0281d749":"code","1f1530ae":"code","e7957862":"code","2b780ea1":"code","8aacdbbc":"code","0defaa1c":"code","48b38712":"code","510f96de":"code","669ca774":"code","81edbf33":"code","15e3d2b4":"code","2eec7802":"code","6a45600d":"code","93f4d856":"code","5d75ea57":"code","486a2efc":"code","362c26ad":"code","b87708db":"code","82de3fe3":"code","a7d4e58a":"code","3691c0b1":"code","e3ee9df5":"code","c0419ef8":"code","96f86ad6":"code","313d4584":"code","1c72abae":"code","7c476bbd":"code","4a09def8":"code","49a726f6":"code","41179a6e":"code","05bdc4a4":"code","fbeb4322":"code","c2dd1c09":"code","97b49a15":"code","ff875350":"code","14bd4c1e":"code","d722acf4":"code","b68196d6":"code","8ec33f22":"code","df9bbefb":"code","9d239cb1":"code","aa821698":"code","f57e70c2":"code","e9adc3cf":"code","c353f358":"code","8acd1dc8":"code","132ad7bb":"code","0c51a688":"code","8a98c60d":"code","ae3ed7af":"code","00ce2547":"code","3af88797":"code","135acfe1":"code","967d7da0":"code","c8fb2cc0":"code","71d5f917":"markdown","c3961c1a":"markdown","aa40d314":"markdown","a89f6c7c":"markdown","9b6d10e4":"markdown","c3e00a36":"markdown","60b5be56":"markdown","0b085c3c":"markdown","217c1572":"markdown","16cc153c":"markdown","bc20cdd6":"markdown","023e7f86":"markdown","5301a355":"markdown","99065dbb":"markdown","310e30d9":"markdown","fbbcc601":"markdown","e2c25b15":"markdown","7e697d9f":"markdown","cfa10dd4":"markdown","d67d483c":"markdown","bdcdabdd":"markdown","61921a6c":"markdown","450c4e4b":"markdown","edeaff0a":"markdown","9a8ec6f0":"markdown","4acd0ec5":"markdown","17e78971":"markdown","c62942c3":"markdown","637fc6c6":"markdown","856292f4":"markdown","d03edc96":"markdown","6719e1fe":"markdown","86bee058":"markdown","5cb7b2bc":"markdown"},"source":{"cd8f73bc":"# Importing all required packages\nimport numpy as np\nimport pandas as pd\n\n# for plotting purpose\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# For splitting the data into train and test\nfrom sklearn.model_selection import train_test_split\n\n# For reskaling purpose\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","34604854":"# Reading Car price file\ncarprice = pd.read_csv('..\/input\/CarPrice_Assignment.csv')\n\n# Lets look into car price data \nprint(carprice.shape)\ncarprice.info()\ncarprice.head(2)\n\n# Total 205 rows & 26 columns present in dataset. \n# Fortunately there are No null values.","bfe27fb8":"# Lets check basic statistics of dataset\n\ncarprice.describe()","90559a4d":"# Lets prepare data for further analysis\n# Need to pick only name of Car from CarName column of the dataset\n# Lets look first unique values of CarName\n\ncarprice.CarName.unique()\n\n# We could notice that '-' present with few car names.\n# lets get rid of '-' and change it to space so that we could pick first word from CarName column and use space as dilimiter.\n# There are few car names which are written incorrectly & VW in short form of volkswagen.\n\n","6d4dc86e":"# Get rid of '-' from CarName column. We will same column name for further analysis.\n\ncarprice[\"CarName\"] = carprice[\"CarName\"].str.replace('-', ' ')\n\n# correct few incorrect names of Cars\ncarprice[\"CarName\"] = carprice[\"CarName\"].str.replace('maxda', 'mazda')\ncarprice[\"CarName\"] = carprice[\"CarName\"].str.replace('Nissan', 'nissan')\ncarprice[\"CarName\"] = carprice[\"CarName\"].str.replace('porcshce', 'porsche')\ncarprice[\"CarName\"] = carprice[\"CarName\"].str.replace('toyouta', 'toyota')\ncarprice[\"CarName\"] = carprice[\"CarName\"].str.replace('vw', 'volkswagen')\ncarprice[\"CarName\"] = carprice[\"CarName\"].str.replace('vokswagen', 'volkswagen')\n\n# Lets pick only first word from carName column\ncarprice[\"CarCompanyName\"] = carprice[\"CarName\"].str.split().str[0]\n\n# Lets look for unique columns\ncarprice.CarCompanyName.unique()\n# Looks good now.","042fff44":"# Find Duplicates rows if any\n\ncarprice.loc[carprice.duplicated()]\n\n# There are no duplicate rows as no rows were printed in output.","939651ce":"# plot a pair plot\n\n#sns.pairplot(carprice)\n#plt.show()\n","fa75594d":"# Carname vs price\n\nplt.figure(figsize=(20, 8))\nsns.boxplot(x = 'CarCompanyName', y = 'price', data = carprice)\nplt.xticks(rotation = 90)\nplt.show()\n\n# Jaguar,Buick,Porsche have highest average price in all cars.","21b49111":"# Lets look some more variables aganist Price\n\nplt.figure(figsize=(20, 12))\nplt.subplot(2,2,1)\nsns.boxplot(x = 'symboling', y = 'price', data = carprice)\nplt.subplot(2,2,2)\nsns.boxplot(x = 'fueltype', y = 'price', data = carprice)\nplt.subplot(2,2,3)\nsns.boxplot(x = 'enginetype', y = 'price', data = carprice)\nplt.xticks(rotation = 90)\nplt.subplot(2,2,4)\nsns.boxplot(x = 'carbody', y = 'price', data = carprice)\nplt.xticks(rotation = 90)\nplt.show()","fb92c738":"# Lets look for rest of the categorical variables\n\nplt.figure(figsize=(28, 13))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'enginelocation', y = 'price', data = carprice)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'fuelsystem', y = 'price', data = carprice)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'cylindernumber', y = 'price', data = carprice)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'aspiration', y = 'price', data = carprice)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'doornumber', y = 'price', data = carprice)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'drivewheel', y = 'price', data = carprice)\nplt.show()","1eb1095f":"# Lets drop few columns which have no significance in our analysis. column Car_ID\ncarprice.drop(['car_ID'], inplace = True, axis =1)\n\n# Lets drop CarName as well. We ahve already picked car company name from it & now this column has no use.\ncarprice.drop(['CarName'], inplace = True, axis =1)\n","0fd0dd0e":"# Applying the map function to the fueltype\ncarprice['fueltype'] = carprice['fueltype'].map({'gas': 1, 'diesel': 0})\n# Applying the map function to the aspiration\ncarprice['aspiration'] = carprice['aspiration'].map({'std': 1, 'turbo': 0})\n# Applying the map function to the doornumber\ncarprice['doornumber'] = carprice['doornumber'].map({'two': 1, 'four': 0})\n# Applying the map function to the enginelocation\ncarprice['enginelocation'] = carprice['enginelocation'].map({\"front\": 1, \"rear\": 0})","5ef0e417":"carprice.head()","3a7b4d45":"# Get the dummy variables for the feature 'carbody' and store it in a new variable - 'body_dummy'\nbody_dummy = pd.get_dummies(carprice['carbody'])\nbody_dummy.head(5) ","7da835f1":"# Let's drop the first column from status df using 'drop_first = True'\nbody_dummy = pd.get_dummies(carprice['carbody'],drop_first=True)\n\n# Add the results to the original carprice dataframe\ncarprice = pd.concat([carprice, body_dummy], axis = 1)\ncarprice.head()","5e682d9c":"# Drop 'carbody' as we have created the dummies for it\ncarprice.drop(['carbody'], axis = 1, inplace = True)\ncarprice.head()","f24feb4c":"# Get the dummy variables for the feature 'carbody' and store it in a new variable - 'wheel_dummy'\nwheel_dummy = pd.get_dummies(carprice['drivewheel'])\nwheel_dummy.head(5) ","f9709cc3":"# Let's drop the first column from status df using 'drop_first = True'\nwheel_dummy = pd.get_dummies(carprice['drivewheel'], drop_first = True)\n# Add the results to the original carprice dataframe\ncarprice = pd.concat([carprice, wheel_dummy], axis = 1)\ncarprice.head()","89b847bc":"# Drop 'carbody' as we have created the dummies for it\ncarprice.drop(['drivewheel'], axis = 1, inplace = True)\ncarprice.head()","7aef6ecd":"# Get the dummy variables for the feature 'enginetype' and store it in a new variable - 'enginetype_dummy'\nenginetype_dummy = pd.get_dummies(carprice['enginetype'])\nenginetype_dummy.head()","bf70a614":"# Let's drop the first column from enginetype_dummy df using 'drop_first = True'\nenginetype_dummy = pd.get_dummies(carprice['enginetype'],drop_first = True)\n# Add the results to the original carprice dataframe\ncarprice = pd.concat([carprice, enginetype_dummy], axis = 1)\ncarprice.head()","42f94551":"# Drop 'enginetype' as we have created the dummies for it\ncarprice.drop(['enginetype'], axis = 1, inplace = True)","a6bf5524":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncarprice['cylindernumber'] = le.fit_transform(carprice.cylindernumber.values)","a191b08d":"# Get the dummy variables for the feature 'enginetype' and store it in a new variable - 'enginetype_dummy'\n#cylindernumber_dummy = pd.get_dummies(carprice['cylindernumber'])\n#cylindernumber_dummy.head()","a43fd8b0":"# Let's drop the first column from status df using 'drop_first = True'\n#cylindernumber_dummy = pd.get_dummies(carprice['cylindernumber'], drop_first = True)\n# Add the results to the original carprice dataframe\n#carprice = pd.concat([carprice, cylindernumber_dummy], axis = 1)\ncarprice['cylindernumber'].head(5)","d77df335":"# Drop 'cylindernumber' as we have created the dummies for it\n# carprice.drop(['cylindernumber'], axis = 1, inplace = True)\n","a5d603d1":"# Get the dummy variables for the feature 'fuelsystem' and store it in a new variable - 'fuelsystem_dummy'\nfuelsystem_dummy = pd.get_dummies(carprice['fuelsystem'])\nfuelsystem_dummy.head()","82ebef8f":"# Let's drop the first column from fuelsystem_dummy df using 'drop_first = True'\nfuelsystem_dummy = pd.get_dummies(carprice['fuelsystem'], drop_first = True)\n# Add the results to the original carprice dataframe\ncarprice = pd.concat([carprice, fuelsystem_dummy], axis = 1)\ncarprice.head()","e17f8ffb":"# Drop 'fuelsystem' as we have created the dummies for it\ncarprice.drop(['fuelsystem'], axis = 1, inplace = True)\n","8e152ada":"# Get the dummy variables for the feature 'CarName' and store it in a new variable - 'CarName_dummy'\nCarName_dummy = pd.get_dummies(carprice['CarCompanyName'])\nCarName_dummy.head()","3a514ec4":"# Let's drop the first column from CarName_dummy df using 'drop_first = True'\nCarName_dummy = pd.get_dummies(carprice['CarCompanyName'], drop_first = True)\n# Add the results to the original carprice dataframe\ncarprice = pd.concat([carprice, CarName_dummy], axis = 1)\ncarprice.head()","473e7299":"# Drop 'fuelsystem' as we have created the dummies for it\ncarprice.drop(['CarCompanyName'], axis = 1, inplace = True)","dc1a6771":"print(carprice.shape)\ncarprice.head()","0924b26b":"# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\ndf_train, df_test = train_test_split(carprice, train_size = 0.7, test_size = 0.3, random_state = 100)","6b48efaa":"scaler = MinMaxScaler()\n# Apply scaler() to all the columns except the binary and 'dummy' variables\nnum_vars = ['symboling', 'carlength', 'carheight', 'enginesize', 'boreratio','stroke', 'compressionratio', 'horsepower',\n'peakrpm', 'citympg', 'price','carwidth','curbweight','highwaympg','wheelbase','cylindernumber']\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\ndf_train.head()","68b4bb95":"df_train.describe()","00736805":"y_train = df_train.pop('price')\nX_train = df_train","e4b24a13":"# Running RFE with the output number of the variable equal to 20\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)                          # running RFE\nrfe = rfe.fit(X_train, y_train)","22285320":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","44faf8b5":"col = X_train.columns[rfe.support_]\ncol","71f32fb2":"X_train.columns[~rfe.support_]","4ffef783":"# Creating X_test dataframe with RFE selected variables\nX_train_1 = X_train[col]","0281d749":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_1 = sm.add_constant(X_train_1)","1f1530ae":"lm = sm.OLS(y_train,X_train_1).fit()   # Running the linear model","e7957862":"#Let's see the summary of our linear model\nprint(lm.summary())","2b780ea1":"# Lets drop Buick which has high p value .142\nX_train_2 = X_train_1.drop([\"peakrpm\"], axis = 1)","8aacdbbc":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_2 = sm.add_constant(X_train_2)\nlm = sm.OLS(y_train,X_train_2).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","0defaa1c":"# Calculate the VIFs for the new model after dropping carname buick\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","48b38712":"#Correlation using heatmap\nplt.figure(figsize = (20, 10))\nsns.heatmap(X_train_2.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","510f96de":"X_train_3 = X_train_2.drop([\"enginelocation\"], axis = 1)","669ca774":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_3 = sm.add_constant(X_train_3)","81edbf33":"lm = sm.OLS(y_train,X_train_3).fit()   # Running the linear model","15e3d2b4":"#Let's see the summary of our linear model\nprint(lm.summary())","2eec7802":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_3\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6a45600d":"# boreration is highly correlated with enginesize & carlength. Lets drop it\nX_train_4 = X_train_3.drop([\"carheight\"], axis = 1)","93f4d856":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_4 = sm.add_constant(X_train_4)\nlm = sm.OLS(y_train,X_train_4).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","5d75ea57":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_4\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","486a2efc":"X_train_5 = X_train_4.drop([\"mitsubishi\"], axis = 1)","362c26ad":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_5 = sm.add_constant(X_train_5)\nlm = sm.OLS(y_train,X_train_5).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","b87708db":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_5\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","82de3fe3":"X_train_6 = X_train_5.drop([\"renault\"], axis = 1)","a7d4e58a":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_6 = sm.add_constant(X_train_6)\nlm = sm.OLS(y_train,X_train_6).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","3691c0b1":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_6\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e3ee9df5":"X_train_7 = X_train_6.drop([\"cylindernumber\"], axis = 1)","c0419ef8":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_7 = sm.add_constant(X_train_7)\nlm = sm.OLS(y_train,X_train_7).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","96f86ad6":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_7\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","313d4584":"#Correlation using heatmap\nplt.figure(figsize = (11, 4))\nsns.heatmap(X_train_7.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","1c72abae":"X_train_8 = X_train_7.drop([\"curbweight\"], axis = 1)","7c476bbd":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_8 = sm.add_constant(X_train_8)\nlm = sm.OLS(y_train,X_train_8).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","4a09def8":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_8\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","49a726f6":"X_train_9 = X_train_8.drop([\"l\"], axis = 1)","41179a6e":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_9 = sm.add_constant(X_train_9)\nlm = sm.OLS(y_train,X_train_9).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","05bdc4a4":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_9\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","fbeb4322":"X_train_10 = X_train_9.drop([\"peugeot\"], axis = 1)","c2dd1c09":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_10 = sm.add_constant(X_train_10)\nlm = sm.OLS(y_train,X_train_10).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","97b49a15":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_10\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","ff875350":"X_train_11 = X_train_10.drop([\"ohcf\"], axis = 1)","14bd4c1e":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_11 = sm.add_constant(X_train_11)\nlm = sm.OLS(y_train,X_train_11).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","d722acf4":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_11\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b68196d6":"X_train_12 = X_train_11.drop([\"subaru\"], axis = 1)","8ec33f22":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_12 = sm.add_constant(X_train_12)\nlm = sm.OLS(y_train,X_train_12).fit()   # Running the linear model\n#Let's see the summary of our linear model\nprint(lm.summary())","df9bbefb":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_12\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9d239cb1":"y_train_price = lm.predict(X_train_12)","aa821698":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 5)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","f57e70c2":"# Apply scaler() to all the columns except the binary and 'dummy' variables\nnum_vars = ['symboling', 'carlength', 'carheight', 'enginesize', 'boreratio','stroke', 'compressionratio', 'horsepower',\n'peakrpm', 'citympg', 'price','carwidth','curbweight','highwaympg','wheelbase','cylindernumber']\ndf_test[num_vars] = scaler.transform(df_test[num_vars])","e9adc3cf":"df_test.describe()","c353f358":"y_test = df_test.pop('price')\nX_test = df_test","8acd1dc8":"X_train_new = X_train_12.drop([\"const\"], axis = 1)","132ad7bb":"\nvif = pd.DataFrame()\nX = X_train_12\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0c51a688":"# Creating X_test_new dataframe by dropping variables from X_test & picking only columns which were in train model.\nX_test_new = X_test[X_train_new.columns]\nX_test_new.info()","8a98c60d":"# Adding constant variable to test dataframe\nX_test_new = sm.add_constant(X_test_new)\n# Making predictions\ny_pred = lm.predict(X_test_new)","ae3ed7af":"from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_test, y_pred)\nprint('Mean_Squared_Error :' ,mse)","00ce2547":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","3af88797":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","135acfe1":"# Actual vs Predicted\nc = [i for i in range(1,63,1)]\nfig = plt.figure()\nplt.plot(c,y_test, color=\"blue\", linewidth=3.5, linestyle=\"-\")     #Plotting Actual\nplt.plot(c,y_pred, color=\"red\",  linewidth=3.5, linestyle=\"-\")  #Plotting predicted\nfig.suptitle('Actual and Predicted', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('Car Price', fontsize=16)  ","967d7da0":"# Error terms\nfig = plt.figure()\nc = [i for i in range(1,63,1)]\n# plt.plot(c,y_test-y_pred_m9, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nplt.scatter(c,y_test-y_pred)\n\nfig.suptitle('Error Terms', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                      # X-label\nplt.ylabel('ytest-ypred', fontsize=16)  ","c8fb2cc0":"# Plotting the error terms to understand the distribution.\nfig = plt.figure()\nsns.distplot((y_test-y_pred),bins=5)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test-y_pred', fontsize=18)                  # X-label\nplt.ylabel('Index', fontsize=16)  ","71d5f917":"#### Carname Peugeot has high p value and lets drop it.","c3961c1a":"#### cylindernumber is positively correlated with enginesize. lets drop it.","aa40d314":"- Now, we don't need five columns. \n- You can drop the convertible column, as the type of carbody can be identified with just the last four columns where \u2014\n\n- 0000 will correspond to convertible\n- 0100 will correspond to hatchback\n- 0010 will correspond to sedan\n- 0010 will correspond to wagon\n","a89f6c7c":"#### boreratio seems to be correlated with carlength and enginesize. Lets drop it.","9b6d10e4":"### Residual Analysis of the train data\n##### So, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","c3e00a36":"### Dividing into X and Y sets for the model building","60b5be56":"#### Observations from Heat map:\n- Looking into above heatmap and could see enginetype rotor and cylindernumber two are very highly correlated. \n- enginetype l and carname peugeot are very highly correlated. lets drop enginetype l first.","0b085c3c":"#### Dividing into X_test and y_test","217c1572":"#### Rebuilding the model without enginetype \"l\"","16cc153c":"#### Observations:\n- Diesel cars have more average price than Gas fuel type\n- hardtop and convertible have higher average price than others.\n- Avearge prices of symboling -1 & 0 are high.\n- Its assigned insurance risk rating, A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n- ohc Engine type seems to be most favored type.","bc20cdd6":"### Building model using statsmodel, for the detailed statistics","023e7f86":"## Multiple Linear Regression ","5301a355":"### Splitting the Data into Training and Testing Sets\n- As we know, the first basic step for regression is performing a train-test split.","99065dbb":"#### Applying the scaling on the test sets","310e30d9":"\n#### Let's make a pairplot of all the numeric variables.","fbbcc601":"#### Lets drop cylindernumber four due to high vif. Not droping enginesize for now as it reduces r square value significantly.","e2c25b15":"## Data Preparation\n- Creating Dummy variables and Transforming Categorical variables\n- Could see that dataset has many columns with only two types of values\n- But in order to fit a regression line, we would need numerical values and not string. \n- Hence, we need to convert them to 1s and 0s\n","7e697d9f":"#### stroke has high p value greater than 0.05. lets drop it.","cfa10dd4":"### Problem Statement","d67d483c":"#### Visualising Categorical Variables\n- As you might have noticed, there are a few categorical variables as well. \n- Let's make a boxplot for some of these variables.\n","bdcdabdd":"### Building our model\n- We will be using the LinearRegression function from SciKit Learn for its compatibility with RFE (which is a utility from sklearn)\n\n### RFE\n- Recursive feature elimination","61921a6c":"### Data Cleaning","450c4e4b":"#### Lets drop cylindernumber two due to high vif \"inf\"","edeaff0a":"#### Now we will do similar process on other variables.","9a8ec6f0":"#### Now we have 205 rows and 61 columns.\n- Lets proceed further.","4acd0ec5":"A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. \n\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. \n\nThe company wants to know:\n\n1. Which variables are significant in predicting the price of a car\n2. How well those variables describe the price of a car","17e78971":"#### Observations: \n- doornumber doesn't show much impact. No sugnificant difference between the categories in it.\n- turbo have higher price range than the std.\n- enginelocation at rear seems to have price but data is less.\n- mpfi and idi having the highest price range. \n- Most high price cars seems to prefer rwd drivewheel\n- it seems high priced cars have more cylinders. ","c62942c3":"## Step 2: Visualising the Data\n\nLet's now spend some time doing what is arguably the most important step - **understanding the data**.\n- If there is some obvious multicollinearity going on, this is the first place to catch it\n- Here's where we'll also identify if some predictors directly have a strong association with the outcome variable\n\nWe'll visualise our data using `matplotlib` and `seaborn`.","637fc6c6":"### Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.","856292f4":"### Making Predictions Using the Final Model\n\n- Now that we have fitted the model and checked the normality of error terms,\n- it's time to go ahead and make predictions using the final, i.e. 10th model.","d03edc96":"#### cylindernumber three has p-value of .058. greater than 0.05 so lets drop it.","6719e1fe":"- Now, we don't need 3 columns. \n- You can drop the 4wd column, as the type of drivewheel can be identified with just the last 2 columns where \u2014\n\n- 00 will correspond to 4wd\n- 01 will correspond to rwd\n- 10 will correspond to fwd","86bee058":"\n### Rescaling the Features \n\nAs we saw in the demonstration for Simple Linear Regression, scaling doesn't impact our model. It is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. As you know, there are two common ways of rescaling:\n\n1. Min-Max scaling \n2. Standardisation (mean-0, sigma-1) \n\nWe will use MinMax scaling.","5cb7b2bc":"### Observations from Pair Plot:\n\n- There are categorical variables but numeric so those also have presence in pair plot.\n- price has positive correaltion with -\n- carlength, carwidth, curbweight, enginesize, boreratio, horsepower,wheelbase\n- carheight shows not much correlation with price\n- citympg, highwaympg - seem to have a significant negative correlation with price.\n\n- We can conclude that linear regression can be applied on this data set."}}