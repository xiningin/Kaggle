{"cell_type":{"079dc8a9":"code","c2867895":"code","2d8eb9bb":"code","4e8d40ab":"code","79fb2d41":"code","f42a73e4":"code","c3133357":"code","6d0c71b4":"code","e3f2cf89":"code","931fa689":"code","16960a75":"code","f7b446a5":"code","77537590":"code","6ecbf78f":"code","ec4d522f":"code","50e9a957":"code","7e7bbecf":"code","fcec6c2b":"code","eff5c948":"code","15c8ac88":"code","4f2bcc72":"code","aab7443c":"code","ec84f8ae":"markdown","7d50f097":"markdown","8b5e638f":"markdown","c1afd772":"markdown","b8061d79":"markdown","639dced8":"markdown","7d63f0ca":"markdown","b326b7d6":"markdown","87f0f920":"markdown","4090e964":"markdown","c2c98a6f":"markdown","3879caca":"markdown","a617bb3a":"markdown","2430dfdd":"markdown","60e81032":"markdown","337694c1":"markdown","4be52cd5":"markdown","0dd57669":"markdown","e3604f51":"markdown","71f12a26":"markdown","7a30e172":"markdown","a56475df":"markdown","967e9cfd":"markdown","ee8d21bf":"markdown","a9d6dae0":"markdown","19afbde9":"markdown","f8649834":"markdown","c4aa853d":"markdown","ca514d52":"markdown","6ccfde60":"markdown","da79902f":"markdown","5c754e01":"markdown","ea6a8e2f":"markdown","68cb7160":"markdown","f8738107":"markdown","27bcf7f5":"markdown"},"source":{"079dc8a9":"import logging\nimport time\nimport warnings\n\nimport catboost as cb\nimport joblib\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.compose import (\n    ColumnTransformer,\n    make_column_selector,\n    make_column_transformer,\n)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import log_loss, mean_squared_error\nfrom sklearn.model_selection import (\n    KFold,\n    StratifiedKFold,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\", level=logging.INFO\n)\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nwarnings.filterwarnings(\"ignore\")","c2867895":"tps_df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntps_df.head()","2d8eb9bb":"tps_df.shape","4e8d40ab":"# Find the number of missing values across rows\ntps_df.isnull().sum(axis=1)","79fb2d41":"def num_missing_row(X: pd.DataFrame, y=None):\n    # Calculate some metrics across rows\n    num_missing = X.isnull().sum(axis=1)\n    num_missing_std = X.isnull().std(axis=1)\n\n    # Add the above series as a new feature to the df\n    X[\"#missing\"] = num_missing\n    X[\"num_missing_std\"] = num_missing_std\n\n    return X","f42a73e4":"from sklearn.preprocessing import FunctionTransformer\n\nnum_missing_estimator = FunctionTransformer(num_missing_row)","c3133357":"# Check number of columns before\nprint(f\"Number of features before preprocessing: {len(tps_df.columns)}\")\n\n# Apply the custom estimator\ntps_df = num_missing_estimator.transform(tps_df)\nprint(f\"Number of features after preprocessing: {len(tps_df.columns)}\")","6d0c71b4":"def custom_function(X, y=None):\n    ...\n\n\ndef inverse_of_custom(X, y=None):\n    ...\n\n\nestimator = FunctionTransformer(func=custom_function, inverse_func=inverse_of_custom)","e3f2cf89":"from sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):\n    pass","931fa689":"from sklearn.preprocessing import PowerTransformer\n\n\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self._estimator = PowerTransformer()","16960a75":"class CustomLogTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self._estimator = PowerTransformer()\n\n    def fit(self, X, y=None):\n        X_copy = np.copy(X) + 1\n        self._estimator.fit(X_copy)\n\n        return self","f7b446a5":"custom_log = CustomLogTransformer()\ncustom_log.fit(tps_df)","77537590":"class CustomLogTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self._estimator = PowerTransformer()\n\n    def fit(self, X, y=None):\n        X_copy = np.copy(X) + 1\n        self._estimator.fit(X_copy)\n\n        return self\n\n    def transform(self, X):\n        X_copy = np.copy(X) + 1\n\n        return self._estimator.transform(X_copy)","6ecbf78f":"custom_log = CustomLogTransformer()\ncustom_log.fit(tps_df)\n\ntransformed_tps = custom_log.transform(tps_df)","ec4d522f":"transformed_tps[:5, :5]","50e9a957":"class CustomLogTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self._estimator = PowerTransformer()\n\n    def fit(self, X, y=None):\n        X_copy = np.copy(X) + 1\n        self._estimator.fit(X_copy)\n\n        return self\n\n    def transform(self, X):\n        X_copy = np.copy(X) + 1\n\n        return self._estimator.transform(X_copy)\n\n    def inverse_transform(self, X):\n        X_reversed = self._estimator.inverse_transform(np.copy(X))\n\n        return X_reversed - 1","7e7bbecf":"custom_log = CustomLogTransformer()\n\ntps_transformed = custom_log.fit_transform(tps_df)\ntps_inversed = custom_log.inverse_transform(tps_transformed)","fcec6c2b":"tps_df.values[:5, 5]","eff5c948":"tps_inversed[:5, 5]","15c8ac88":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\n\nxgb_pipe = make_pipeline(\n    FunctionTransformer(num_missing_row),\n    SimpleImputer(strategy=\"constant\", fill_value=-99999),\n    CustomLogTransformer(),\n    xgb.XGBClassifier(\n        n_estimators=1000, tree_method=\"gpu_hist\", objective=\"binary:logistic\"\n    ),\n)\n\nX, y = tps_df.drop(\"claim\", axis=1), tps_df[[\"claim\"]].values.flatten()\nsplit = train_test_split(X, y, test_size=0.33, random_state=1121218)\nX_train, X_test, y_train, y_test = split","4f2bcc72":"xgb_pipe.fit(X_train, y_train)\npreds = xgb_pipe.predict_proba(X_test)\n\nroc_auc_score(y_test, preds[:, 1])","aab7443c":"class CustomTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self):\n        pass\n\n    def transform(self):\n        pass\n\n    def inverse_transform(self):\n        pass","ec84f8ae":"We also could have used `np.exp` instead of `inverse_transform`. Now, let's make a final check:","7d50f097":"Working as expected. Now, as I said earlier, we need a method for reverting the transform:","8b5e638f":"This way, you get `fit_transform` for free. If you don't need any of `__init__`, `fit`, `transform` or `inverse_transform` methods, omit them and the parent Sklearn classes take care of everything. The logic of these methods are entirely up to your coding skills and needs.","c1afd772":"# How to Write Powerful Code Others Envy With Custom Sklearn Transformers\n## Do everything in Sklearn and everyone will be happy\n![](https:\/\/cdn-images-1.medium.com\/max\/1440\/1*y3zaj2WueHb-j6hPwugEGA.jpeg)","b8061d79":"ince we have a simple function, no need to call `fit` as it just returns the estimator untouched. The only requirement of `FunctionTransformer` is that the passed function should accept the data as its first argument. Optionally, you can pass the target array as well if you need it inside the function:","639dced8":"# Integrating simple functions with `FunctionTransformer`","7d63f0ca":"> But wait! We didn't write `fit_transform` - where did that come from? It is simple - when you inherit from `BaseEstimator` and `TransformerMixin`, you get a `fit_transform` method for free. \n\nAfter the inverse transform, you can compare it with the original data:","b326b7d6":"# You might also be interested...\n\nhttps:\/\/towardsdatascience.com\/how-to-work-with-million-row-datasets-like-a-pro-76fb5c381cdd\n\nhttps:\/\/towardsdatascience.com\/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997\n\nhttps:\/\/towardsdatascience.com\/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\n\nhttps:\/\/towardsdatascience.com\/tired-of-clich%C3%A9-datasets-here-are-18-awesome-alternatives-from-all-domains-196913161ec9\n\nhttps:\/\/towardsdatascience.com\/love-3blue1brown-animations-learn-how-to-create-your-own-in-python-in-10-minutes-8e0430cf3a6d","87f0f920":"One of the most common scaling options for skewed data is a logarithmic transform. But here is a caveat: if a feature has even a single 0, the transformation with `np.log` or Sklearn's `PowerTransformer` return an error.\n\nSo, as a workaround, people add 1 to all samples and then apply the transformation. If the transformation is performed on the target array, you will also need an inverse transform. For that, after making predictions, you need to use the exponential function and subtract 1. Here is what it looks like in code:","4090e964":"Single `fit`, single `predict` - how awesome would that be?\n\nYou get the data, fit your pipeline just one time, and it takes care of everything\u200a-\u200apreprocessing, feature engineering, modeling, everything. All you have to do is call predict and have the output.\n\nWhat kind of pipeline is *that* powerful? Yes, Sklearn has many transformers, but it doesn't have one for every imaginable preprocessing scenario. So, is such a pipeline a *pipe* dream?\n\nAbsolutely not. Today, we will learn how to create custom Sklearn transformers that enable you to integrate virtually any function or data transformation into Sklearn's Pipeline classes.","c2c98a6f":"# Wrapping up...","3879caca":"```python\n# FunctionTransformer signature\ndef custom_function(X, y=None):\n    ...\n\nestimator = FunctionTransformer(custom_function)  # no errors\n\ncustom_pipeline = make_pipeline(StandardScaler(), estimator, xgb.XGBRegressor())\ncustom_pipeline.fit(X, y)\n```","a617bb3a":"We first create a class that inherits from `BaseEstimator` and `TransformerMixin` classes of `sklearn.base`. Inheriting from these classes allows Sklearn pipelines to recognize our classes as custom estimators. \n\nThen, we will write the `__init__` method, where we just initialize an instance of `PowerTransformer`:","2430dfdd":"Writing good code is a skill developed over time. You will realize that a big part of it comes from using the existing tools and libraries at the right time and place, without having to reinvent the wheel.\n\nOne of such tools is Sklearn pipelines and custom transformers are just extensions of them. Use them well and you will produce quality code with little effort.","60e81032":"Let's create a function that takes a DataFrame as input and implements the above operation:","337694c1":"Now, adding this function into a pipeline is just as easy as passing it to the `FunctionTransformer`:","4be52cd5":"```python\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nimport xgboost as xgb\n\nxgb_pipe = make_pipeline(\n                SimpleImputer(strategy='mean'),\n                StandardScaler(),\n                xgb.XGBRegressor()\n            )\n\n_ = xgb_pipe.fit(X, y)\n```","0dd57669":"# What are Sklearn pipelines?","e3604f51":"# Integrating more complex preprocessing steps with custom transformers","71f12a26":"# Introduction","7a30e172":"Check out the [documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.FunctionTransformer.html) for details on other arguments.","a56475df":"```python\ny_transformed = np.log(y + 1)\n\n_ = model.fit(X, y_transformed)\npreds = np.exp(model.predict(X, y_transformed) - 1)\n```","967e9cfd":"This works, but we have the same old problem\u200a-\u200awe can't include this into a pipeline out of the box. Sure, we could use our newfound friend `FunctionTransformer`, but it is not well-suited for more complex preprocessing steps such as this.\n\nInstead, we will write a custom transformer class and create the `fit`, `transform` functions manually. In the end, we will again have a Sklearn-compatible estimator that we can pass into a pipeline. Let's start:","ee8d21bf":"Let's make another check:","a9d6dae0":"Next, we have the `transform`, in which we just use the `transform` method of PowerTransformer after adding 1 to the passed data:","19afbde9":"> This section assumes some knowledge of Python object-oriented programming (OOP). Specifically, the basics of creating classes and inheritance. If you are not already down with those, check out my [Python OOP series](https:\/\/ibexorigin.medium.com\/list\/objectoriented-programming-essentials-for-data-scientists-cf2ff3dc9fc9?source=user_lists---------1-------cf2ff3dc9fc9---------------------), written for data scientists.","f8649834":"The `fit` method should return the transformer itself, which is done by returning `self`. Let's test what we have done so far:","c4aa853d":"Next, we write the `fit` where we add 1 to all features in the data and fit the PowerTransformer:","ca514d52":"Passing a custom function to `FunctionTransformer` creates an estimator with `fit`, `transform` and `fit_transform` methods:","6ccfde60":"In this month's (September) TPS Competition on Kaggle, one of the ideas that boosted model performance significantly was adding the number of missing values in a row as a new feature. This is a custom operation, not implemented in Sklearn, so let's create a function to achieve that after importing the data:","da79902f":"Now, we have a custom transformer ready to be included in a pipeline. Let's put everything together:","5c754e01":"I have talked at length about the nitty-gritty of Sklearn pipelines and their benefits in an [older post](https:\/\/towardsdatascience.com\/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d). The most notable advantages are their ability to collapse all preprocessing and modeling steps into a single estimator, preventing data leakage by never calling `fit` on validation sets and an added bonus that makes the code concise, reproducible, and modular.\n\nBut this whole idea of atomic, neat pipelines breaks when we need to perform operations that are not built into Sklearn as estimators. For example, what if you need to extract regex patterns to clean text data? What do you do if you want to create a new feature combining existing ones based on domain knowledge?\n\nTo preserve all the benefits that come with pipelines, you need a way to integrate your custom preprocessing and feature engineering logic into Sklearn. That's where custom transformers come into play.","ea6a8e2f":"# Setup","68cb7160":"`FunctionTransformer` also accepts an inverse of the passed function if you ever need to revert the changes:","f8738107":"Below is a simple pipeline that imputes the missing values in numeric data, scales them, and fits an XGBRegressor to `X`, `y`:","27bcf7f5":"Even though log transform actually hurt the score, we got our custom pipeline working!\n\nIn short, the signature of your custom transformer class should be like this:"}}