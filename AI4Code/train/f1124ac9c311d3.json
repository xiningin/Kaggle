{"cell_type":{"1cb7c7cc":"code","946f49bc":"code","c0859951":"code","f8a7cc1e":"code","e1328861":"code","77498e7f":"code","7dc5e8db":"code","2ad099fc":"code","e54bc8f9":"code","88705ef5":"code","4bcb2595":"code","a3524638":"code","d268c500":"code","d0e781d3":"code","1020ea1d":"code","e2239283":"code","e9453c3e":"code","49cfabc6":"code","c89ad3d3":"code","d2c5508e":"code","d61fcc50":"code","cc992f42":"code","0e2b69e7":"code","e76310e8":"code","fd0a59b2":"code","79bcbf5b":"markdown","b7447d58":"markdown"},"source":{"1cb7c7cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","946f49bc":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","c0859951":"import pathlib\ndata_dir = '..\/input\/flowers-recognition\/flowers\/flowers'\ndata_dir = pathlib.Path(data_dir)","f8a7cc1e":"roses = len(list(data_dir.glob('*\/*.jpg')))\nprint(roses)\n# PIL.Image.open(roses[65])","e1328861":"img_height = 180\nimg_width = 180\nbatch_size = 32","77498e7f":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","7dc5e8db":"valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split = 0.2,\n    subset = 'validation',\n    seed = 123,\n    image_size = (img_height,img_width),\n    batch_size = batch_size)","2ad099fc":"class_names = train_ds.class_names\nprint(class_names)","e54bc8f9":"plt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","88705ef5":"for image_batch, labels_batch in train_ds:\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break","4bcb2595":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)","a3524638":"normalization_layer = layers.experimental.preprocessing.Rescaling(1.\/255)","d268c500":"normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\nimage_batch, labels_batch = next(iter(normalized_ds))\nfirst_image = image_batch[0]\n# Notice the pixels values are now in `[0,1]`.\nprint(np.min(first_image), np.max(first_image)) ","d0e781d3":"num_classes = 5\n\nmodel = Sequential([\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(img_height, img_width, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","1020ea1d":"model.summary()","e2239283":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","e9453c3e":"epochs=10\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","49cfabc6":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(10, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend()\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\nplt.show()","c89ad3d3":"data_augmentation = keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(img_height, \n                                                              img_width,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)","d2c5508e":"plt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n  for i in range(9):\n    augmented_images = data_augmentation(images)\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n    plt.axis(\"off\")","d61fcc50":"model = Sequential([\n  data_augmentation,\n  layers.experimental.preprocessing.Rescaling(1.\/255),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","cc992f42":"model.summary()","0e2b69e7":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","e76310e8":"epochs = 15\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","fd0a59b2":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(10, 7))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend()\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\nplt.show()","79bcbf5b":"## Overfiting","b7447d58":"# data augementation and Dropout"}}