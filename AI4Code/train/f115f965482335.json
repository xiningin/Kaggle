{"cell_type":{"0f763bec":"code","d7ef7d66":"code","52dabb97":"code","b272364e":"code","645677c9":"code","a111b52d":"code","e27f0bc4":"code","3c1e1c6e":"code","362c7545":"code","e92c4073":"code","fd47f110":"code","83139c2f":"code","288f88d3":"code","d45ddbbe":"markdown","43ede793":"markdown","27bc784f":"markdown","6d1063aa":"markdown","6159c656":"markdown","8d4f7e4c":"markdown","72924eb7":"markdown","44dfb835":"markdown","f6ac692a":"markdown"},"source":{"0f763bec":"import glob, os, yaml, csv\nimport numpy as np\nfrom cv2 import imread , resize\nfrom keras.layers import Conv2D, UpSampling2D, MaxPooling2D, Input, Flatten, Reshape, Dense,Conv2DTranspose, Add\nfrom keras import Model, callbacks\nimport matplotlib.pyplot as plt\n%matplotlib inline","d7ef7d66":"#We read the images and store them in numpy arrays\nX=np.array([imread(each) for each in glob.glob(os.path.join(os.getcwd() , '..\/input\/train\/*.png'))[:4]])\ny=np.array([imread(each) for each in glob.glob(os.path.join(os.getcwd() , '..\/input\/train_cleaned\/*.png'))[:4]])","52dabb97":"plt.figure(figsize=(16, 8))\nfor i in range(4):\n    plt.subplot(241+i)\n    fig=plt.imshow(X[i])\n    fig.axes.get_xaxis().set_visible(False)\n    fig.axes.get_yaxis().set_visible(False)\n    plt.subplot(245+i)\n    fig=plt.imshow(y[i])\n    fig.axes.get_xaxis().set_visible(False)\n    fig.axes.get_yaxis().set_visible(False)\nplt.show()","b272364e":"shapes=np.unique([imread(each).shape for each in glob.glob(os.path.join(os.getcwd() , '..\/input\/train\/*.png'))],axis=0)\nprint(shapes)","645677c9":"class Autoencoder:\n    def __init__(self,\n                dimensions_factor=1,\n                layers=4,\n                k=3,\n                filter_size=None,\n                pooling_factor=None,\n                only_decoder=False,\n                only_encoder=False,\n                loss='mean_squared_error',\n                channels=3,\n                start_filters=32,\n                skip_connection=True):\n        \n        #Initialization of non-specified variables\n        self.channels=channels\n        self.layers=layers\n        self.k=k\n        self.filter_size=[(k,k)]*(layers*2) if (filter_size is None) else filter_size\n        self.pooling_factor=[2]*(layers*2) if (pooling_factor is None) else pooling_factor\n        self.dimensions_factor=dimensions_factor\n        self.model_type='conv_AE'\n        self.start_filters=start_filters\n        self.encoder_layers=[]\n        self.skip_connection=skip_connection\n        \n        #Build encoder\n        self.input_img = Input(shape=(None, None, self.channels))\n        x=self.input_img\n        self.encoder_layers.append(self.input_img)\n        for i in range(self.layers):\n            filters=int(self.pooling_factor[i]*int(x.shape[-1])*self.dimensions_factor) if i != 0 else self.start_filters\n            x = Conv2D(filters, self.filter_size[i], activation='relu', padding='same',name='encoding_conv_'+str(i))(x)\n            self.encoder_layers.append(x)\n            x = MaxPooling2D((self.pooling_factor[i], self.pooling_factor[i]), padding='valid',name='encoding_pool_'+str(i))(x)\n\n        x = Conv2D(int(x.shape[-1]), (1,1), activation='relu', padding='same',name='code')(x)\n        self.coded=x\n        for i in range(layers):\n            filters=int(self.encoder_layers[-(i+1)].shape[-1])#int(int(x.shape[-1])\/(self.pooling_factor[i])*self.dimensions_factor)\n            x = Conv2DTranspose(filters, self.filter_size[i], activation='relu', strides=self.pooling_factor[i], padding='same',name='decoding_conv_'+str(i))(x)\n            if self.skip_connection:\n                x = Add()([x,self.encoder_layers[-(i+1)]])\n        x = Conv2D(self.channels, (3), activation='sigmoid', padding='same',name='decoded')(x)\n        self.decoded=x\n        autoencoder = Model(self.input_img, self.decoded)\n        autoencoder.compile(optimizer='adam', loss=loss, metrics=['mse'])\n\n        self.model=autoencoder\n\n    def get_encoder(self):\n        if self.model is None:\n            raise('Model is not created')\n        else:\n            encoder=Model(self.input_img, self.coded)\n\n            return encoder\n\n    def get_decoder(self):\n        if self.model is None:\n            raise('Model is not created')\n        else:\n            decoder=Model(self.coded, self.decoded)\n\n            return decoder","a111b52d":"class Image_generator():\n    def __init__(self,path,val_percentage=0.115,batch_size=4, reduction_factor=16):\n        self.base_path=path\n        self.batch_size=batch_size\n        self.reduction_factor=reduction_factor\n        \n        self.y=np.array([imread(each).astype('float32') \/ 255 for each in sorted(glob.glob(os.path.join(path, '..\/input\/train_cleaned\/*.png')))])\n        self.X=np.array([imread(each).astype('float32') \/ 255 for each in sorted(glob.glob(os.path.join(path , '..\/input\/train\/*.png')))])\n\n        self.idx_train,self.idx_val = self.split_batches(val_percentage)\n        \n        self.train_steps = len(self.idx_train)\n        self.val_steps = len(self.idx_val)\n    \n    def split_batches(self,val_percentage):\n        idx=[]\n        self.shapes=np.unique([each.shape for each in self.X],axis=0)\n        for shape in self.shapes:\n            shape_idx=np.argwhere([all(a.shape==shape) for a in self.X]).flatten()\n            np.random.shuffle(shape_idx)\n            idx.extend(np.array_split(shape_idx, len(shape_idx)\/self.batch_size))\n        idx=np.array([x for x in idx if x.size == self.batch_size])\n        np.random.shuffle(idx)\n        samples=len(idx)\n        val_samples=int(samples*val_percentage)\n        \n        return idx[:-val_samples],idx[-val_samples:]\n\n    def check_size(self,batch_x,batch_y,axis):\n        for each_axis in axis:\n            while (batch_x.shape[each_axis]%self.reduction_factor)!=0:\n                batch_x=np.insert(batch_x,batch_x.shape[each_axis],1,axis=each_axis)\n                batch_y=np.insert(batch_y,batch_y.shape[each_axis],1,axis=each_axis)\n        return batch_x,batch_y\n\n    def get_train_batch(self):\n        while True:\n            batch_x = np.stack(self.X[self.idx_train[0]])\n            batch_y = np.stack(self.y[self.idx_train[0]])\n            batch_x, batch_y = self.check_size(batch_x, batch_y,[-2,-3])\n            self.idx_train = np.roll(self.idx_train,1,axis=0)\n            yield batch_x, batch_y\n\n    def get_val_batch(self):\n        while True:\n            batch_x = np.stack(self.X[self.idx_val[0]])\n            batch_y = np.stack(self.y[self.idx_val[0]])\n            batch_x, batch_y = self.check_size(batch_x, batch_y,[-2,-3])\n            self.idx_val = np.roll(self.idx_val,1,axis=0)\n            yield batch_x, batch_y","e27f0bc4":"np.random.seed(42)\ndata_generator=Image_generator(os.getcwd())\nautoencoder=Autoencoder(loss='binary_crossentropy',skip_connection=True)","3c1e1c6e":"log=autoencoder.model.fit_generator(generator = data_generator.get_train_batch(),\n                                  steps_per_epoch=data_generator.train_steps,\n                                  epochs=50,\n                                  shuffle=False,\n                                  validation_data = data_generator.get_val_batch(),\n                                  validation_steps = data_generator.val_steps,\n                                  use_multiprocessing = False)\n","362c7545":"plt.figure()\n[plt.plot(v,label=str(k)) for k,v in log.history.items()]\nplt.legend()\nplt.show()","e92c4073":"def to_csv(npdata,ids):\n    with open('submission.csv', 'w') as csvfile:\n        csvwriter = csv.writer(csvfile, delimiter=',',\n                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n        csvwriter.writerow(('id','value'))\n        for i,each in enumerate(npdata):\n            rows,cols,_=each.shape\n            for row in range(rows):\n                for col in range(cols):\n                    id_pixel=str(ids[i])+'_'+str(row+1)+'_'+str(col+1)\n                    value_pixel=str(np.mean(each[row,col,:]))\n                    csvwriter.writerow([id_pixel,value_pixel])\n\nX_test=np.array([imread(each) for each in sorted(glob.glob(os.path.join(os.getcwd() , '..\/input\/test\/*.png')))])\nids=[each.split('\/')[-1][:-4] for each in sorted(glob.glob(os.path.join(os.getcwd() , '..\/input\/test\/*.png')))]\npredictions=[]\nfor x in X_test:\n    original_shape=x.shape\n    while (x.shape[0]%16)!=0:\n        x=np.insert(x,x.shape[0],1,axis=0)\n    while (x.shape[1]%16)!=0:\n        x=np.insert(x,x.shape[1],1,axis=1)\n    x=x.reshape((1,)+x.shape)\n    \n    prediction=autoencoder.model.predict(x)\n    prediction=prediction.reshape(prediction.shape[1:])\n    prediction=prediction[:original_shape[0],:original_shape[1],:]\n    \n    predictions.append(prediction)\n    print('%d of %d predictions calculated'%(len(predictions),len(X_test)), end='\\r') \nprint('\\nSaving...')\nto_csv(np.array(predictions),ids)\nprint('Saved')\n\nindexes=[4,5,6,7]\nfig=plt.figure(figsize=(10, 15))\nfor i,idx in enumerate(indexes):\n    fig.add_subplot(len(indexes), 2, (i)*2+1)\n    plt.imshow(X_test[i])\n    fig.add_subplot(len(indexes), 2, (i)*2+2)\n    plt.imshow(predictions[i])\nplt.show()","fd47f110":"encoder= autoencoder.get_encoder()\n\nexample=X_test[10].reshape((1,)+X_test[10].shape)\nfilters=encoder.predict(example)\n\nplt.figure(1)\nplt.imshow(example.reshape(example.shape[1:4]))\n","83139c2f":"%%capture\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nfig=plt.figure(2)\nims=[]\nfor i in range(filters.shape[-1]):\n    im = plt.imshow(filters[:,:,:,i].reshape(filters.shape[1:3]), animated=True)\n    ims.append([im])\nani = animation.ArtistAnimation(fig, ims, interval=500, blit=True,\n                                repeat_delay=0)","288f88d3":"HTML(ani.to_jshtml())","d45ddbbe":"# Denoising Convolutional Autoencoders for Text","43ede793":"If we want to avoid to reshape the images (and therefore, introduce unwanted errors), we should make our model fully convolutional, without specifying input size.\nLet's start with a basic constructor for this DAE! ","27bc784f":"To go from noisy images (top) to clean ones (bottom), I will use a Denoising Autoencoder, a type of neural network that converts an image to a representation of the data contained in it, and then reconverts it to an image. This process is used so the network learns to extract the meaningfull data and to ignore the noise.\nSince both the input and ouput are images we will use convolutions as the basic layer of the autoencoder for this application.\nHowever, if we check, we will discover that not all images have the same shape:","6d1063aa":"Finally, we can train the autoencoder. With this configuration, you should get pretty good results and a mse of .005 aprox., but feel free to play with it.","6159c656":"Since Keras cannot deal with images of different sizes within the same batch, let's make a generator that returns batches of consistent size.\nAnother restriction of this generator is that all heights and widths should be divisible by 4, due to the two maxpooling layers. So, if a dimension is not divisible, the generator will padd with 1. both X and y ","8d4f7e4c":"Finally, we will make the predictions. Since we don't care much about computing performance here we will make it image by image, padding when neccesary.","72924eb7":"## For Curious:","44dfb835":"Lets take a step into the model and check what is it **actually** doing. To to that, we plot a random image, and the filters in the middle of the autoencoder (see the animation).","f6ac692a":"In this Notebook we will run an example Convolutional Autoencoder to denoise text images.\nFirst of all, let's start with some imports."}}