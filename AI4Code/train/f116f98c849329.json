{"cell_type":{"b44dee67":"code","2425badf":"code","7d90bf57":"code","e6d45b67":"code","a2f04632":"code","7cb480ca":"code","42b22971":"code","9657fb04":"markdown","c6feae76":"markdown","6f0a705a":"markdown","9b2aafb4":"markdown"},"source":{"b44dee67":"import torch\nimport os\n\n\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n    \n    def add_word(self, word):\n        if not word in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n    \n    def __len__(self):\n        return len(self.word2idx)\n\n\nclass Corpus(object):\n    def __init__(self):\n        self.dictionary = Dictionary()\n\n    def get_data(self, path, batch_size=20):\n        # Add words to the dictionary\n        with open(path, 'r') as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + ['<eos>']\n                tokens += len(words)\n                for word in words: \n                    self.dictionary.add_word(word)  \n        \n        # Tokenize the file content\n        ids = torch.LongTensor(tokens)\n        token = 0\n        with open(path, 'r') as f:\n            for line in f:\n                words = line.split() + ['<eos>']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n        num_batches = ids.size(0) \/\/ batch_size\n        ids = ids[:num_batches*batch_size]\n        return ids.view(batch_size, -1)","2425badf":"# Some part of the code was referenced from below.\n# https:\/\/github.com\/yunjey\/pytorch-tutorial\/blob\/master\/tutorials\/02-intermediate\/language_model\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.nn.utils import clip_grad_norm_\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters\nembed_size = 128\nhidden_size = 1024\nnum_layers = 1\nnum_epochs = 5\nnum_samples = 200     # number of words to be sampled\nbatch_size = 20\nseq_length = 30\nlearning_rate = 0.002\n\ndevice","7d90bf57":"# Load \"Penn Treebank\" dataset\ncorpus = Corpus()\nids = corpus.get_data('..\/input\/ptb.train.txt', batch_size)\nvocab_size = len(corpus.dictionary)\nnum_batches = ids.size(1) \/\/ seq_length\n\ntest_corpus = Corpus()\ntest_ids = test_corpus.get_data('..\/input\/ptb.test.txt', batch_size)\ntest_vocab_size =  len(test_corpus.dictionary)\ntest_num_batches = test_ids.size(1) \/\/ seq_length\n\nprint(num_batches)\nprint(test_num_batches)","e6d45b67":"# RNN based language model\nclass RNNLM(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n        super(RNNLM, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        \n    def forward(self, x, h):\n        # Embed word ids to vectors\n        x = self.embed(x)\n        \n        # Forward propagate LSTM\n        out, (h, c) = self.lstm(x, h)\n        \n        # Reshape output to (batch_size*sequence_length, hidden_size)\n        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n        \n        # Decode hidden states of all time steps\n        out = self.linear(out)\n        return out, (h, c)\n\nmodel = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","a2f04632":"# Truncated backpropagation\ndef detach(states):\n    return [state.detach() for state in states]\n\n\n# Train the model\nfor epoch in range(num_epochs):\n    # Set initial hidden and cell states\n    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n    \n    for i in range(0, ids.size(1) - seq_length, seq_length):\n        # Get mini-batch inputs and targets\n        inputs = ids[:, i:i+seq_length].to(device)\n        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n        \n        # Forward pass\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n        states = detach(states)\n        outputs, states = model(inputs, states)\n        loss = criterion(outputs, targets.reshape(-1))\n        \n        # Backward and optimize\n        model.zero_grad()\n        loss.backward()\n        clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        step = (i+1) \/\/ seq_length\n        if step % 500 == 0:\n            print ('Epoch [{}\/{}], Step[{}\/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))","7cb480ca":"import math\n\n# Test the model\nstates = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n              torch.zeros(num_layers, batch_size, hidden_size).to(device))\ntest_loss = 0.\nwith torch.no_grad():\n    for i in range(0, test_ids.size(1) - seq_length, seq_length):\n        # Get mini-batch inputs and targets\n        inputs = test_ids[:, i:i+seq_length].to(device)\n        targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n        \n        # Forward pass\n        states = detach(states)\n        outputs, states = model(inputs, states)\n        test_loss += criterion(outputs, targets.reshape(-1)).item()\n\ntest_loss = test_loss \/ test_num_batches\nprint('-' * 89)\nprint('test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('-' * 89)\n\n\n# Generate texts using trained model\nwith torch.no_grad():\n    with open('sample.txt', 'w') as f:\n        # Set intial hidden ane cell states\n        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n                 torch.zeros(num_layers, 1, hidden_size).to(device))\n\n        # Select one word id randomly\n        prob = torch.ones(vocab_size)\n        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n\n        for i in range(num_samples):\n            # Forward propagate RNN \n            output, state = model(input, state)\n\n            # Sample a word id\n            prob = output.exp()\n            word_id = torch.multinomial(prob, num_samples=1).item()\n\n            # Fill input with sampled word id for the next time step\n            input.fill_(word_id)\n\n            # File write\n            word = corpus.dictionary.idx2word[word_id]\n            word = '\\n' if word == '<eos>' else word + ' '\n            f.write(word)\n\n            if (i+1) % 100 == 0:\n                print('Sampled [{}\/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))","42b22971":"# Save the model checkpoints\n# torch.save(model.state_dict(), 'model.ckpt')","9657fb04":"# 2. Define the model","c6feae76":"# 1. Load data","6f0a705a":"# 3. Train the model","9b2aafb4":"# 4. Test the model"}}