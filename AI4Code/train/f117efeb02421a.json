{"cell_type":{"6e2bc38f":"code","d38b6ec1":"code","fc935884":"code","b0a0ed74":"code","d61c614c":"code","3543931b":"code","e3e51743":"code","abe674d6":"code","fe4e33a4":"code","dda487ed":"code","4df258b7":"code","9cc0a6ee":"code","4030bc4d":"code","292fcdb3":"code","35cd2966":"code","9fcce9b8":"code","9d4d896b":"code","187a77fb":"code","ec93baa0":"code","445d5463":"code","8ab6e95c":"code","3822ed79":"code","681f7f44":"code","b245042a":"code","6b9a65de":"code","f458e11f":"code","524420fc":"code","9fe0cdcb":"code","b0798f59":"code","c266e55d":"code","5b50409a":"code","9888213b":"code","02e501a1":"code","b9abeeec":"code","2a1d7305":"code","a099d8a6":"code","4a715eb3":"code","0480a47e":"code","977d9da0":"code","e545aeb6":"code","26730b06":"code","fb52ded3":"code","5ca57d6f":"code","d258f95b":"code","70d59a31":"code","6a6af28e":"code","881f49e8":"code","4c473eb7":"code","8db347eb":"code","e6cf7ec9":"code","3505109c":"code","b89ac9a0":"code","16ceacbe":"code","42c25b68":"code","6ae9c28c":"code","e452d6e2":"markdown","db3890e7":"markdown","9f715808":"markdown","6a187b2a":"markdown","b0e0e21e":"markdown","98158c33":"markdown","dbb1a4e6":"markdown","9a6ee936":"markdown","a5bfebd1":"markdown","1ebc0d8c":"markdown","dbb1636e":"markdown","5169f44b":"markdown","94de03a3":"markdown","1aca9a3c":"markdown","3c758fce":"markdown","0739ab3b":"markdown","1f783a7f":"markdown","708f1c68":"markdown","e60f97aa":"markdown","7268d14b":"markdown","7ce8867b":"markdown","0f9dcc2c":"markdown","d8fac16f":"markdown","a784561a":"markdown"},"source":{"6e2bc38f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d38b6ec1":"import matplotlib.pyplot as plt \nimport seaborn as sns","fc935884":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","b0a0ed74":"train","d61c614c":"test","3543931b":"train.dtypes.unique()","e3e51743":"#correlation matrix\ncorrmat = train.corr().abs()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","abe674d6":"train_na = (train.isnull().sum() \/ len(train)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' : train_na})\nmissing_data.head(20)","fe4e33a4":"test_na = (test.isnull().sum() \/ len(test)) * 100\ntest_na = test_na.drop(test_na[test_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' : test_na})\nmissing_data.head(20)","dda487ed":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['PassengerId']\ntest_ID = test['PassengerId']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"PassengerId\", axis = 1, inplace = True)\ntest.drop(\"PassengerId\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","4df258b7":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.Survived.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['Survived'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","9cc0a6ee":"all_data.drop(['Name', 'Ticket'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","4030bc4d":"def get_deck(cabin):\n    if cabin is np.nan or cabin.startswith('T'):\n        return 'X'\n    else:\n        return cabin[0]\n    \nall_data.Cabin = all_data.Cabin.apply(get_deck)\nprint(\"all_data size is : {}\".format(all_data.shape))","292fcdb3":"all_data.Embarked.fillna(\"S\",inplace=True)","35cd2966":"def familySize_to_cat(size):\n    if size == 1: return \"alone\"\n    if size >= 2 and size <= 4: return 'small'\n    if size >= 5 and size <= 7: return 'medium'\n    if size > 7 : return \"large\"\n\nall_data['FamilySize'] = all_data.SibSp + all_data.Parch + 1\nall_data['FamilySizeCategory'] = all_data.FamilySize.apply(familySize_to_cat)","9fcce9b8":"all_data['RealFare'] = np.nan\nidxs = all_data['Fare'].notnull() | all_data['Fare'] != 0\nall_data['RealFare'][idxs] = all_data.Fare[idxs] \/ all_data.FamilySize[idxs]\nall_data.drop(['Fare'], axis=1, inplace=True)","9d4d896b":"all_data.columns","187a77fb":"from sklearn.preprocessing import LabelEncoder\ncols = ('Pclass', 'Sex', 'Cabin', 'Embarked', 'FamilySizeCategory')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))\nall_data","ec93baa0":"from scipy import stats \nfrom scipy.stats import norm, skew\n\n#histogram and normal probability plot\nsns.distplot(all_data['RealFare'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data['RealFare'], plot=plt)","445d5463":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","8ab6e95c":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","3822ed79":"# histogram and normal probability plot\nsns.distplot(all_data['RealFare'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data['RealFare'], plot=plt)","681f7f44":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","b245042a":"all_data_na = (all_data.isnull().sum() \/ len(test)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' : all_data_na})\nmissing_data.head(20)","6b9a65de":"from sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score","f458e11f":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model, x_train, y_train):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)\n    rmse= np.sqrt(-cross_val_score(model, x_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","524420fc":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nrandom_forest = RandomForestRegressor(n_estimators=250, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10)\ngboost = GradientBoostingRegressor(n_estimators=250, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nadaboost = AdaBoostRegressor(n_estimators=150)\n\nmodels = {\"Lasso\" : lasso, \n          \"ElasticNet\" : ENet,\n         \"KernelRidge\" : KRR,\n         \"RandomForest\" : random_forest,\n         \"AdaBoost\" : adaboost,\n         \"GradientBoosting\" : gboost}","9fe0cdcb":"all_data_not_na = all_data.dropna(axis=0)\ntarget = all_data_not_na['RealFare']\nall_data_not_na.drop(['RealFare'], axis=1, inplace=True)\nprint(all_data_not_na.shape)","b0798f59":"for model_name in models.keys():\n    score = rmsle_cv(models[model_name], all_data_not_na, target)\n    print(\"\\n{} score: {:.4f} ({:.4f})\\n\".format(model_name, score.mean(), score.std()))","c266e55d":"selected = [\"KernelRidge\", \"RandomForest\", \"GradientBoosting\"]\nav_pred = np.zeros(all_data_not_na.shape[0])\nfor model_name in selected:\n    models[model_name].fit(all_data_not_na.values, target)\n    model_train_pred = models[model_name].predict(all_data_not_na.values)\n    av_pred += model_train_pred\n    print(model_name, \"train score:\", np.square(mean_squared_error(target, model_train_pred)))\n    \nav_pred \/= len(selected)\nprint(\"Averaging train score:\", np.square(mean_squared_error(target, av_pred)))","5b50409a":"na_idx = all_data['RealFare'].isnull()\nav_pred = np.zeros(all_data[na_idx].shape[0])\nfor model_name in selected:\n    av_pred += models[model_name].predict(all_data[na_idx].drop('RealFare', axis = 1))\nav_pred \/= len(selected)\nall_data['RealFare'][na_idx] = av_pred","9888213b":"print((all_data['RealFare'].isnull().sum() \/ len(test)) * 100)","02e501a1":"# histogram and normal probability plot\nsns.distplot(all_data['RealFare'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data['RealFare'], plot=plt)","b9abeeec":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nrandom_forest = RandomForestRegressor(n_estimators=150, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10)\ngboost = GradientBoostingRegressor(n_estimators=150,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10)\n\n\nmodels = {\"Lasso\" : lasso, \n          \"ElasticNet\" : ENet,\n         \"KernelRidge\" : KRR,\n         \"RandomForest\" : random_forest,\n         \"GradientBoosting\" : gboost}","2a1d7305":"all_data_not_na = all_data.dropna(axis=0)\ntarget = all_data_not_na['Age']\nall_data_not_na.drop(['Age'], axis=1, inplace=True)\nprint(all_data_not_na.shape)","a099d8a6":"for model_name in models.keys():\n    score = rmsle_cv(models[model_name], all_data_not_na, target)\n    print(\"\\n{} score: {:.4f} ({:.4f})\\n\".format(model_name, score.mean(), score.std()))","4a715eb3":"selected = [\"KernelRidge\", \"RandomForest\", \"GradientBoosting\"]\nav_pred = np.zeros(all_data_not_na.shape[0])\nfor model_name in selected:\n    models[model_name].fit(all_data_not_na.values, target)\n    model_train_pred = models[model_name].predict(all_data_not_na.values)\n    av_pred += model_train_pred\n    print(model_name, \"train score:\", np.square(mean_squared_error(target, model_train_pred)))\n    \nav_pred \/= len(selected)\nprint(\"Averaging train score:\", np.square(mean_squared_error(target, av_pred)))","0480a47e":"na_idx = all_data['Age'].isnull()\nav_pred = np.zeros(all_data[na_idx].shape[0])\nfor model_name in selected:\n    av_pred += models[model_name].predict(all_data[na_idx].drop('Age', axis = 1))\nav_pred \/= len(selected)\nall_data['Age'][na_idx] = av_pred","977d9da0":"print((all_data['Age'].isnull().sum() \/ len(test)) * 100)","e545aeb6":"# histogram and normal probability plot\nsns.distplot(all_data['Age'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data['Age'], plot=plt)","26730b06":"all_data_na = (all_data.isnull().sum() \/ len(test)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' : all_data_na})\nmissing_data.head(20)","fb52ded3":"def cut_age(age):\n    if age <= 15:\n        return 'child'\n    if age >= 60:\n        return 'senior'\n    return 'adult'\n\nall_data['Age_Cat'] = all_data.Age.apply(cut_age)","5ca57d6f":"lbl = LabelEncoder() \nlbl.fit(list(all_data['Age_Cat'].values)) \nall_data['Age_Cat'] = lbl.transform(list(all_data['Age_Cat'].values))","d258f95b":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","70d59a31":"from sklearn.svm import SVC, NuSVC\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import log_loss, f1_score\nimport xgboost\nfrom xgboost import XGBClassifier\nxgboost.set_config(verbosity=0)","6a6af28e":"#Validation function\nn_folds = 5\n\ndef log_loss_cv(model, x_train, y_train):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)\n    log_loss = -cross_val_score(model, x_train.values, y_train, scoring=\"neg_log_loss\", cv = kf)\n    f1 = cross_val_score(model, x_train.values, y_train, scoring=\"f1\", cv = kf)\n    return log_loss, f1","881f49e8":"models = {\n    \"Support Vector 1\" : SVC(gamma= 0.001, C=100,  kernel='rbf', probability=True),\n    \"Support Vector 2\" : SVC(gamma=2, C=1, probability=True),\n    'Logistic regression' : LogisticRegression(solver='liblinear', C=1000, penalty='l2', random_state=13),\n    \"RandomForest\" : RandomForestClassifier(max_depth=4, n_estimators= 75, max_features='auto'),\n    \"GradientBoosting\" : GradientBoostingClassifier(n_estimators=250),\n    \"KNN\" : KNeighborsClassifier(n_neighbors=12,metric='manhattan',weights='uniform'),\n    \"XGB\" : XGBClassifier(n_estimators= 250,\n                          use_label_encoder=False, eval_metric='logloss')\n}","4c473eb7":"for model_name in models.keys():\n    loss, f1 = log_loss_cv(models[model_name], train, y_train)\n    print(\"\\n{} loss: {:.4f} ({:.4f}), f1: {:.4f} ({:.4f})\\n\".format(model_name, loss.mean(), loss.std(), f1.mean(), f1.std()))","8db347eb":"for model_name in models.keys():\n    models[model_name].fit(train.values, y_train)\n    model_train_pred = models[model_name].predict(train.values)\n    print(model_name, \"train score:\", log_loss(y_train, model_train_pred), \"f1:\", f1_score(y_train, model_train_pred))","e6cf7ec9":"selected = [\"RandomForest\", \n            \"GradientBoosting\",\n            \"XGB\"]\nvoting = VotingClassifier([(name, models[name]) for name in selected], voting='soft')","3505109c":"log_loss_cv(voting, train, y_train)\nloss, f1 = log_loss_cv(voting, train, y_train)\nprint(\"\\nVouting loss: {:.4f} ({:.4f}), f1: {:.4f} ({:.4f})\\n\".format(loss.mean(), loss.std(), \n                                                                      f1.mean(), f1.std()))\nvoting.fit(train.values, y_train)\nmodel_train_pred = voting.predict(train.values)\nprint(\"Vouting train score:\", log_loss(y_train, model_train_pred), \n      \"f1:\", f1_score(y_train, model_train_pred))","b89ac9a0":"test_pred = voting.predict(test.values)","16ceacbe":"test_pred.shape","42c25b68":"sample_submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsample_submission","6ae9c28c":"sub = pd.DataFrame()\nsub['PassengerId'] = test_ID\nsub['Survived'] = test_pred\nsub.to_csv('submission.csv',index=False)","e452d6e2":"# Submission","db3890e7":"Firstly let's restore Fare. I will train some models, then take the best and calculate average.","9f715808":"#### Embarked, missing values\n\n- S is majority, fill with this","6a187b2a":"Concatenate data for processing purposes","b0e0e21e":"# Data exploration","98158c33":"Check that no more data is missing in 'RealFare' column.","dbb1a4e6":"I defined custom cross-validation function, because I want to use KFold schedule in sklearn.model_selection.cross_val_score.","9a6ee936":"## Missing data","a5bfebd1":"#### Let's restore 'Age' features too","1ebc0d8c":"# Data processing","dbb1636e":"### Cabin -> Deck\n\n- some cabin data can be aquired from othe family members, but only in two cases, it is not worth it\n- so X if person does not have cabin","5169f44b":"### Fare\n\n- fare needs to be divided with family size\n- some of data is 0, so I will count it as missing too","94de03a3":"## Labeling categorical data","1aca9a3c":"### Name, Ticket","3c758fce":"## Skewness\n\nSome of the data is skewed, so let's apply boxcox transform.","0739ab3b":"Let's add one more feature","1f783a7f":"Split the data","708f1c68":"Drop ids","e60f97aa":"## Feature selection & engineering\nPartially copied from https:\/\/www.kaggle.com\/lovroselic\/titanic-ls-take-2#Feature-selection-&-engineering","7268d14b":"#### Family size\n\n- I will bin it to categorical","7ce8867b":"Check missing data","0f9dcc2c":"# Titanic\n\nThis is my lab work for machine learning course in my university.","d8fac16f":"# Modeling\n\nLet's finally predict target variable. I'll train some classifiers, then combine the best using VotingClassifier.","a784561a":"There are missing data in 'Age' and 'RealFare' features. I will try to restore it by training the model on existing data and predicting missing."}}