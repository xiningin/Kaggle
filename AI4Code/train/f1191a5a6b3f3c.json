{"cell_type":{"266a43c7":"code","ca72def4":"code","1adc282b":"code","3d2d3193":"code","1ea2ed5f":"code","92425e05":"code","a9a3a656":"code","3b8c133b":"code","9bebce54":"code","92d48baf":"code","43db1ac4":"code","83a62cac":"code","cc89a089":"code","ff621dcc":"code","5f98af99":"code","1cd51ce3":"code","04c9bc76":"code","e8c11b2d":"code","695166e8":"code","3e2f5093":"code","e24f931d":"code","6cb71503":"code","7f9f9dda":"code","67c24448":"code","5d3096ac":"code","822bd45a":"code","3da41365":"code","db219997":"code","7fd0cfc6":"code","c12adb7b":"code","657f255a":"code","e38d1235":"code","62f2a6b0":"code","6cda994f":"code","86911ca4":"code","5013b057":"code","324ef479":"code","a4f99292":"code","271f4d3a":"code","94eaddda":"code","dd788094":"code","02707fde":"code","f94c68ff":"code","aeab25e6":"code","9eef5299":"markdown","fa159889":"markdown","2ba5d12f":"markdown","6a201b5b":"markdown"},"source":{"266a43c7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import display\npd.set_option('display.max_rows', None)\n","ca72def4":"df=pd.read_csv('..\/input\/finding-donor-for-charity\/census.csv')","1adc282b":"df.head()","3d2d3193":"df.info()","1ea2ed5f":"df.isnull().sum()","92425e05":"df['income'].value_counts()","a9a3a656":"import seaborn as sns","3b8c133b":"sns.set(style='whitegrid',color_codes=True)","9bebce54":"sns.factorplot(\"sex\",col='education_level',data=df,hue='income',kind='count',col_wrap=4)","92d48baf":"df['income']=df['income'].str.strip()","43db1ac4":"n_records=df.shape[0]\n","83a62cac":"n_greater_than_50K=df[df['income']=='>50K'].shape[0]\nn_atmost_50K=df[df['income']=='<=50K'].shape[0]","cc89a089":"percentage_greater_than_50K=(n_greater_than_50K\/n_records)*100","ff621dcc":"print('Income Greater than 50K:{}'.format(n_greater_than_50K))\nprint('Income AtMost 50K:{}'.format(n_atmost_50K))","5f98af99":"percentage_greater_than_50K","1cd51ce3":"df['capital-gain'].value_counts()","04c9bc76":"df['capital-loss'].value_counts()","e8c11b2d":"df['hours-per-week'].value_counts()","695166e8":"skewed_features=['capital-loss','capital-gain']\nfeatures_raw = df.drop('income', axis = 1)\nfeature_log_transformed=pd.DataFrame(data=features_raw)","3e2f5093":"feature_log_transformed[skewed_features] = features_raw[skewed_features].apply(lambda x: np.log(x + 1))","e24f931d":"from sklearn.preprocessing import MinMaxScaler","6cb71503":"scaler=MinMaxScaler()\nnumerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']","7f9f9dda":"feature_log_minmax_transform = pd.DataFrame(data = feature_log_transformed)\nfeature_log_minmax_transform[numerical] = scaler.fit_transform(feature_log_transformed[numerical])","67c24448":"df.head()","5d3096ac":"display(feature_log_minmax_transform.head(n = 5))","822bd45a":"final_feature=pd.get_dummies(feature_log_minmax_transform)","3da41365":"income_raw=df['income']\nincome = income_raw.apply(lambda x: 0 if x == '<=50K' else 1)","db219997":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(final_feature, \n                                                    income, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","7fd0cfc6":"from sklearn.metrics import fbeta_score, accuracy_score","c12adb7b":"from time import time\ndef evaluate(model,sample_size,X_train,y_train,X_test,y_test):\n    results={}\n    start=time()\n    model=model.fit(X_train[:sample_size],y_train[:sample_size])\n    end=time()\n    results['train_time']=end-start\n    # Prediction on Testing DataSets\n    start=time()\n    predictions_test=model.predict(X_test)\n    predictions_train=model.predict(X_train[:300])\n    end=time()\n    \n    results['pred_time']=end-start\n    \n    results['acc_train'] = accuracy_score(y_train[:300], predictions_train)\n    results['acc_test'] = accuracy_score(y_test, predictions_test)\n    \n    results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta = 0.5)\n    results['f_test'] = fbeta_score(y_test, predictions_test, beta = 0.5)\n    print(\"{} trained on {} samples.\".format(model.__class__.__name__, sample_size))\n    return results","657f255a":"\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n\nclf_SVC=SVC(random_state=40)\nclf_ADA=AdaBoostClassifier(random_state=15)\nclf_RAN=RandomForestClassifier(random_state=3)\n\nsamples_100 = len(y_train)\nsamples_10 = int(0.1 * len(y_train))\nsamples_1 = int(0.01 * len(y_train))\n\nresults = {}\nfor clf in [clf_SVC, clf_ADA, clf_RAN]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        results[clf_name][i] = \\\n        evaluate(clf, samples, X_train, y_train, X_test, y_test)\n","e38d1235":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer","62f2a6b0":"clf=AdaBoostClassifier(random_state=14)\nparameters = {'n_estimators': [50, 100, 200],\n             'learning_rate': [1.0, 2.0]}","6cda994f":"scorer=make_scorer(fbeta_score,beta=0.5)\ngrid_obj = GridSearchCV(clf, parameters, scoring=scorer)","86911ca4":"grid_fit = grid_obj.fit(X_train, y_train)","5013b057":"best_clf = grid_fit.best_estimator_","324ef479":"predictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)","a4f99292":"print(\"Unoptimized model\\n------\")\nprint(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))","271f4d3a":"from sklearn.ensemble import AdaBoostClassifier\nmodel=AdaBoostClassifier().fit(X_train,y_train)\nfeature_importances=model.feature_importances_","94eaddda":"from sklearn.base import clone","dd788094":"X_train_reduced=X_train[X_train.columns.values[(np.argsort(feature_importances)[::-1])[:5]]]\nX_test_reduced=X_test[X_test.columns.values[(np.argsort(feature_importances)[::-1])[:5]]]","02707fde":"clone_clf=(clone(best_clf)).fit(X_train_reduced,y_train)","f94c68ff":"reduced_pred=clone_clf.predict(X_test_reduced)","aeab25e6":"print(\"Final Model trained on full data\\n------\")\nprint(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\nprint(\"\\nFinal Model trained on reduced data\\n------\")\nprint(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, reduced_pred)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, reduced_pred, beta = 0.5)))","9eef5299":"## Generally Higher income people can donate so >50k==1 ,<=50k==0","fa159889":"## Skewed Value","2ba5d12f":"## Feature Selection \nWe will lets find out will it increases model performance or decreases it?","6a201b5b":"## If Training Time is an Issue then we can use reduce feature or otherwise we are good to go with Full Data"}}