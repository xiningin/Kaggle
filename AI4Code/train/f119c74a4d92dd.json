{"cell_type":{"6471eb2b":"code","33e6773b":"code","24e80161":"code","0459ece0":"code","c8c47945":"code","88f08529":"code","ced9b310":"code","e28320ea":"code","f8c811b6":"code","bac8ece9":"code","d63cc0af":"code","52c53c00":"code","c6d18c78":"code","41f2ecbf":"code","33c4cbf4":"code","aa113e34":"code","47c5e7fc":"code","12dc8f70":"code","be8c1919":"code","244a9e56":"code","a5cfdf6e":"markdown"},"source":{"6471eb2b":"import sys\nsys.path.insert(0, '..\/input\/siim-acr-pneumothorax-segmentation')\n\nimport fastai\nfrom fastai.vision import *\nfrom mask_functions import *","33e6773b":"fastai.__version__","24e80161":"SZ = 256\npath = Path(f'..\/input\/pneumotorax{SZ}\/data{SZ}\/data{SZ}')","0459ece0":"# copy pretrained weights for resnet34 to the folder fastai will search by default\nPath('\/tmp\/.cache\/torch\/checkpoints\/').mkdir(exist_ok=True, parents=True)\n!cp '..\/input\/resnet34\/resnet34.pth' '\/tmp\/.cache\/torch\/checkpoints\/resnet34-333f7ec4.pth'","c8c47945":"# Setting div=True in open_mask\nclass SegmentationLabelList(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\n    \nclass SegmentationItemList(SegmentationItemList):\n    _label_cls = SegmentationLabelList\n\n# Setting transformations on masks to False on test set\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\nfastai.data_block.ItemLists.transform = transform","88f08529":"# Create databunch\ndata = (SegmentationItemList.from_folder(path=path\/'train')\n        .split_by_rand_pct(0.2)\n        .label_from_func(lambda x : str(x).replace('train', 'masks'), classes=[0, 1])\n        .add_test((path\/'test').ls(), label=None)\n        .transform(get_transforms(), size=SZ, tfm_y=True)\n        .databunch(path=Path('.'), bs=32)\n        .normalize(imagenet_stats))","ced9b310":"# Display some images with masks\ndata.show_batch()","e28320ea":"# Create U-Net with a pretrained resnet34 as encoder\nlearn = unet_learner(data, models.resnet34, metrics=[dice])","f8c811b6":"# Fit one cycle of 6 epochs with max lr of 1e-3\nlearn.fit_one_cycle(6)","bac8ece9":"# Unfreeze the encoder (resnet34)\nlearn.unfreeze()","d63cc0af":"# Fit one cycle of 12 epochs\nlr = 1e-3\nlearn.fit_one_cycle(12, slice(lr\/30, lr))","52c53c00":"# Predictions for the validation set\npreds, ys = learn.get_preds()\npreds = preds[:,1,...]\nys = ys.squeeze()","c6d18c78":"def dice_overall(preds, targs):\n    n = preds.shape[0]\n    preds = preds.view(n, -1)\n    targs = targs.view(n, -1)\n    intersect = (preds * targs).sum(-1).float()\n    union = (preds+targs).sum(-1).float()\n    u0 = union==0\n    intersect[u0] = 1\n    union[u0] = 2\n    return (2. * intersect \/ union)","41f2ecbf":"# Find optimal threshold\ndices = []\nthrs = np.arange(0.01, 1, 0.01)\nfor i in progress_bar(thrs):\n    preds_m = (preds>i).long()\n    dices.append(dice_overall(preds_m, ys).mean())\ndices = np.array(dices)","33c4cbf4":"best_dice = dices.max()\nbest_thr = thrs[dices.argmax()]\n\nplt.figure(figsize=(8,4))\nplt.plot(thrs, dices)\nplt.vlines(x=best_thr, ymin=dices.min(), ymax=dices.max())\nplt.text(best_thr+0.03, best_dice-0.01, f'DICE = {best_dice:.3f}', fontsize=14);\nplt.show()","aa113e34":"# Plot some samples\nrows = 10\nplot_idx = ys.sum((1,2)).sort(descending=True).indices[:rows]\nfor idx in plot_idx:\n    fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(12, 4))\n    ax0.imshow(data.valid_ds[idx][0].data.numpy().transpose(1,2,0))\n    ax1.imshow(ys[idx], vmin=0, vmax=1)\n    ax2.imshow(preds[idx], vmin=0, vmax=1)\n    ax1.set_title('Targets')\n    ax2.set_title('Predictions')","47c5e7fc":"# Predictions for test set\npreds, _ = learn.get_preds(ds_type=DatasetType.Test)\npreds = (preds[:,1,...]>best_thr).long().numpy()\nprint(preds.sum())","12dc8f70":"# Generate rle encodings (images are first converted to the original size)\nrles = []\nfor p in progress_bar(preds):\n    im = PIL.Image.fromarray((p.T*255).astype(np.uint8)).resize((1024,1024))\n    im = np.asarray(im)\n    rles.append(mask2rle(im, 1024, 1024))","be8c1919":"ids = [o.stem for o in data.test_ds.items]\nsub_df = pd.DataFrame({'ImageId': ids, 'EncodedPixels': rles})\nsub_df.loc[sub_df.EncodedPixels=='', 'EncodedPixels'] = '-1'\nsub_df.head()","244a9e56":"sub_df.to_csv('submission.csv', index=False)","a5cfdf6e":"# Kernel guideline\n\n### Version 2 - start here\n* Image size 128x128\n* Unet with pretrained resnet34 encoder\n* Best threshold selection\n* Output visualization\n* Total run time of about 34 minutes\n\n### Version 3 - 5-fold ensemble\n* Example of 5-fold ensemble using sklearn KFold function, based on version 2\n* Changed learning rates\n* Total run time of about 132 minutes\n\n### Version 4 - 256x256 \n* Based on version 2 but with images of size 256x256\n* Batch size reduced to 32\n\n### Where to go next?\n* Look at as many examples as you can and try to understand why the model fails when it does;\n* There are several ways to convert a 1024x1024 to a lower resolution (e.g., bilinear, nearest), some may be more appropriate to this competition than others;\n* Progressive rescaling (start with small images like 64x64, train, save the weights, increase to 128x128, train with previous weights, and so on);"}}