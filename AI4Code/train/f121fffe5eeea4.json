{"cell_type":{"37fa5494":"code","9f7ec4a9":"code","da1e1b7c":"code","94dded4a":"code","e4a59780":"code","96596726":"code","f5203689":"code","0a68652f":"code","0108743c":"code","d3734c65":"code","7d083b01":"code","4fd63a8b":"code","8fcbf075":"code","718f62c2":"code","b7c63da7":"code","36be9362":"code","5a3609d1":"code","4ac705f2":"code","bce22e4b":"code","268d5290":"code","c2225124":"code","83aec5a2":"code","1c5e3d04":"code","ad010c7e":"code","fb5f0ec0":"code","d926d16e":"code","c7c96581":"code","6f93661e":"code","73167f35":"code","8a8688dc":"code","3a94aec7":"code","45cb51d7":"code","3a07c455":"code","42cb0bc1":"code","2df1687a":"code","087c94c3":"code","8c110173":"code","4d42872f":"code","c4eb196a":"code","b2bc8e66":"code","8d26e6a4":"code","0bf247dc":"code","c8ef49b2":"code","586572b9":"code","34a2f528":"code","1534e8a6":"code","acbcb155":"code","f0b23f38":"code","34475592":"code","0b63bbfb":"code","79f7191f":"code","1690eafe":"code","efab1214":"code","727cf5e4":"code","4fdf2ec0":"code","a2c89d65":"code","f6827e38":"code","c538d933":"code","38c51dff":"code","9ef575e2":"code","882822f9":"code","c71e0923":"code","a4cf5b03":"code","bda67602":"code","8e4f2996":"code","68cf20ec":"code","9b69d54b":"code","79559dbc":"code","35fe0b8b":"code","0739d336":"code","eae0e41c":"code","4a567cd6":"code","ba88b50c":"code","483416c2":"code","f7ece598":"code","aaa03ef7":"code","46984c3e":"code","a3e546a0":"code","84ff5acf":"code","1df8d91f":"code","92997776":"code","afe1fdf1":"code","40d52c7f":"code","426317b0":"code","16d5ecba":"code","9b30aaa8":"code","618ba6a4":"code","7e2b7b6e":"code","1ce3e605":"code","eee1426d":"code","6e1c22a9":"code","e6eb9392":"code","486e38e5":"code","bd063bf1":"code","a1a890f5":"code","7bb6d48c":"code","5d7a7da7":"code","a0db3993":"code","31a91a19":"code","6b18aa7f":"code","916b2433":"code","22b0f7b5":"code","3e8f9863":"code","3f7f6c51":"code","28431d76":"code","cb3245ea":"code","2cc03fc8":"code","5a828e04":"code","e6290a94":"code","cdbe4b0c":"code","9564204e":"code","1283bde1":"code","8a141981":"code","6a9b033c":"code","3c8bc2cb":"code","2e43a570":"code","d4a66617":"code","1d665982":"markdown","60c0853d":"markdown","72743762":"markdown","564d4ec6":"markdown","ff7522da":"markdown","e0bef434":"markdown","eb15ee52":"markdown","5ee7193d":"markdown","05204acf":"markdown","82a02a9f":"markdown","edb797e1":"markdown","875de33d":"markdown","48ae9775":"markdown","2ac4d5fb":"markdown","8f2d5112":"markdown","83d0f750":"markdown","79217807":"markdown","d8fab263":"markdown","fb677e38":"markdown","5e83a338":"markdown","9de6dfc4":"markdown","6b95b369":"markdown","ede71bca":"markdown","ca79ca24":"markdown","20f02fd1":"markdown","27dfddc6":"markdown"},"source":{"37fa5494":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os \nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom IPython.display import Markdown, displa","9f7ec4a9":"train_df=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","da1e1b7c":"train_df.columns","94dded4a":"train_df.head()","e4a59780":"test_df.columns","96596726":"test_df.head()","f5203689":"test_df.isnull().sum()","0a68652f":"train_df['keyword'].value_counts()","0108743c":"sns.barplot(y=train_df['keyword'].value_counts()[:25].index,x=train_df['keyword'].value_counts()[:25],orient='horizontal',palette='rocket')","d3734c65":"sns.barplot(y=train_df['location'].value_counts()[:25].index,x=train_df['location'].value_counts()[:25],orient='horizontal',palette='viridis')","7d083b01":"train_df['location'].value_counts()","4fd63a8b":"train_df.isnull().sum()","8fcbf075":"#1. Function to replace NAN values with mode value\ndef impute_nan_most_frequent_category(DataFrame,ColName):\n    # .mode()[0] - gives first category name\n     most_frequent_category=DataFrame[ColName].mode()[0]\n    \n    # replace nan values with most occured category\n     DataFrame[ColName + \"_Imputed\"] = DataFrame[ColName]\n     DataFrame[ColName + \"_Imputed\"].fillna(most_frequent_category,inplace=True)\n#2. Call function to impute most occured category\nfor Columns in ['keyword','location']:\n    impute_nan_most_frequent_category(train_df,Columns)\n    \n# Display imputed result\ntrain_df[['keyword','keyword_Imputed','location','location_Imputed']].head(10)\n#3. Drop actual columns\ntrain_df = train_df.drop(['keyword','location'], axis = 1)","718f62c2":"train_df.isnull().sum()","b7c63da7":"# removing the null values from text dataset as well\nfor Columns in ['keyword','location']:\n    impute_nan_most_frequent_category(test_df,Columns)\n    \n# Display imputed result\ntest_df[['keyword','keyword_Imputed','location','location_Imputed']].head(10)\n#3. Drop actual columns\ntest_df=test_df.drop(['keyword','location'], axis = 1)","36be9362":"test_df.isnull().sum()","5a3609d1":"train_df.info()","4ac705f2":"train_df['target'].value_counts()","bce22e4b":"train_df.groupby('target').describe()","268d5290":"# Let's get the length of the tweets\ntrain_df['length']=train_df['text'].apply(len)\ntrain_df","c2225124":"test_df['length']=train_df['text'].apply(len)\ntest_df","83aec5a2":"train_df['text'].values[:60]","1c5e3d04":"train_df['length'].plot(bins=100, kind='hist',color='magenta')","ad010c7e":"test_df['length'].plot(bins=100,kind='hist',color='blue')","fb5f0ec0":"#separating the tweets according to the target value given\nnodis=train_df[train_df['target']==0]\nnodis","d926d16e":"nodis_tweets=nodis['text']\nnodis_tweets.values[:30]","c7c96581":"nodis.shape","6f93661e":"dis=train_df[train_df['target']==1]\ndis","73167f35":"dis_tweets=dis['text']\ndis_tweets.values[:30]","8a8688dc":"#let's calculate the percentage of disaster and no disater\nprint('no disater percentage',(len(nodis)\/len(train_df))*100,'%')","3a94aec7":"#let's calculate the percentage of disaster \nprint('disater percentage',(len(dis)\/len(train_df))*100,'%')","45cb51d7":"sns.countplot(train_df['target'],label='no disater tweets vs disaste tweets')","3a07c455":"plt.style.use('ggplot')\nf,axes=plt.subplots(1,2,figsize=(10,5))\nf.suptitle('Characters in tweets')\n\ntweet_len=train_df[train_df['target']==1]['text'].str.len()\nax1=sns.histplot(tweet_len,ax=axes[0],color='red')\n# ax1.set(xlabel='common xlabel', ylabel='common ylabel')\nax1.set_title('disaster tweets')\n\ntweet_len=train_df[train_df['target']==0]['text'].str.len()\nax2=sns.histplot(tweet_len,ax=axes[1])\n# ax2.set(xlabel='common xlabel', ylabel='common ylabel')\nax2.set_title('Not disaster tweets')","42cb0bc1":"# let us first visualize the count of punctuations and stopwords","2df1687a":"text=''\nfor i in train_df['text']:\n    text+=i","087c94c3":"from collections import defaultdict,Counter\ncount_punctuation=defaultdict(int)\nimport string\n\nfor x,y in Counter(text).items():\n\n    if x in string.punctuation:\n        count_punctuation[x]=y\nprint(count_punctuation)","8c110173":"# sns.set(rc={'figure.figsize':(10,5)})\n# ax=sns.barplot(x='punctuation',y='count',\n#             data=pd.DataFrame(count_punctuation.items()\n#                               ,columns=['punctuation','count']).sort_values(by=['count'], ascending=False))\n# ax.set_title('Count of Punctuations')","4d42872f":"import string\nstring.punctuation\n","c4eb196a":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords.words('english')","b2bc8e66":"def message_cleaning(message):\n    Test_punc_removed = [char for char in message if char not in string.punctuation]\n    Test_punc_removed_join = ''.join(Test_punc_removed)\n    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]\n    Test_punc_removed_join_clean_join=' '.join(Test_punc_removed_join_clean) \n    return Test_punc_removed_join_clean_join","8d26e6a4":"train_df['text']=train_df['text'].apply(message_cleaning)","0bf247dc":"train_df['text']=train_df['text'].str.lower()","c8ef49b2":"print(train_df['text'])","586572b9":"test_df['text']=test_df['text'].apply(message_cleaning)","34a2f528":"test_df['text']=test_df['text'].str.lower()","1534e8a6":"train_df['text'].str.contains('http?').sum()","acbcb155":"train_df['text'].str.contains('http').sum()","f0b23f38":"review=train_df['text'][31]","34475592":"review","0b63bbfb":"pd.set_option('display.max_rows',50)\n# to show all the 2000 rows otherwise it will display with the gap\ntrain_df['text'].str.contains('http',regex=True)[:50]","79f7191f":"import re\ndef remove_urls(review):\n    url_pattern=re.compile(r'href|http.\\w+')\n    return url_pattern.sub(r'', review)\n# substitute with space","1690eafe":"def removeNumbers(text):\n    clean_text = re.sub(r'\\d+', '', text)\n    return clean_text\n","efab1214":"def removeEmojis(text):\n    allchars = [c for c in text]\n    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI[\"en\"]]\n    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n    return clean_text","727cf5e4":"train_df['text']=train_df['text'].apply(remove_urls)","4fdf2ec0":"test_df['text']=test_df['text'].apply(remove_urls)","a2c89d65":"train_df['text']=train_df['text'].apply(removeNumbers)","f6827e38":"!pip install emoji\nimport emoji\ntrain_df['text']=train_df['text'].apply(removeEmojis)","c538d933":"train_df['text'][31]\n# lets again observe that there are any urls in the text or not","38c51dff":"train_df['text'].str.contains('http').sum()","9ef575e2":"#separating the tweets according to the target value given\nnodis=train_df[train_df['target']==0]\nnodis","882822f9":"nodis_tweets=nodis['text']\nnodis_tweets.values[:30]","c71e0923":"nodis.shape","a4cf5b03":"dis=train_df[train_df['target']==1]\ndis","bda67602":"dis_tweets=dis['text']\ndis_tweets.values[:30]","8e4f2996":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(dis_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(nodis_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","68cf20ec":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","9b69d54b":"\ndef convert_abrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","79559dbc":"train_df['text']=train_df['text'].apply(convert_abrev)\ntest_df['text']=test_df['text'].apply(convert_abrev)","35fe0b8b":"train_df['text']","0739d336":"train_df['text'][20]","eae0e41c":"from nltk.stem.snowball import SnowballStemmer\nstemmer=SnowballStemmer(language='english')\ntokens = train_df['text'][20].split()\nclean_text = ' '\n\nfor token in tokens:\n    print(token + ' --> ' + stemmer.stem(token))","4a567cd6":"def stemWord(text):\n    stemmer = SnowballStemmer(language='english')\n    tokens = text.split()\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text+\" \"+stemmer.stem(token)      \n    \n    return clean_text\n\nprint(\"Text before stemWord function: \" + train_df['text'][32])\nprint(\"Text after stemWord function: \" + stemWord(train_df['text'][32]))","ba88b50c":"train_df['text']=train_df['text'].apply(stemWord)\ntest_df['text']=test_df['text'].apply(stemWord)","483416c2":"for txt in train_df['text'][:20]:\n  print(txt);","f7ece598":"!pip install spacy\nimport spacy.cli\n# nlp = spacy.load('en_core_web_lg')","aaa03ef7":"nlp = spacy.cli.download('en_core_web_lg')\n","46984c3e":"nlp = spacy.load('en_core_web_lg')","a3e546a0":"doc = nlp(\"she will be performing all of her assigned responsibilities \")\n#for token in doc:\n   # print(token.lemma_)\nfor noun in doc.noun_chunks:\n    print(noun.text)","84ff5acf":"for word in doc:\n  print(word.text,word.lemma_)\n","1df8d91f":"def lemmatizeWord(text):\n  tokens=nlp(text)\n  clean_text=' '\n  for token in tokens:\n    clean_text=clean_text+\" \"+token.lemma_\n  return clean_text\nprint(\"Text before lemmatizeWord function: \" + train_df['text'][32])\nprint(\"Text after lemmatizeWord function: \" + lemmatizeWord(train_df['text'][32]))\n\ndoc = \"Atharva will be performing all of the assigned responsibilities\"\nlemmatizeWord(doc) ","92997776":"train_df['text']=train_df['text'].apply(lemmatizeWord)\ntest_df['text']=test_df['text'].apply(lemmatizeWord)","afe1fdf1":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer()\nbag_train=cv.fit_transform(train_df['text'])\nbag_test=cv.fit_transform(test_df['text'])","40d52c7f":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntfidf_train=tfidf.fit_transform(train_df['text'])\ntfidf_test=tfidf.fit_transform(test_df['text'])","426317b0":"with nlp.disable_pipes():\n  train_vectors = np.array([nlp(text).vector for text in train_df.text])\n  test_vectors = np.array([nlp(text).vector for text in train_df.text])","16d5ecba":"from sklearn.svm import LinearSVC\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score","9b30aaa8":"svc_wordEmbed=LinearSVC(random_state=1,dual=False,max_iter=10000)\nsvc_wordEmbed.fit(train_vectors,train_df['target'])","618ba6a4":"scores = model_selection.cross_val_score(svc_wordEmbed, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","7e2b7b6e":"from sklearn.metrics import classification_report, confusion_matrix\n","1ce3e605":"y_predicted_train1=svc_wordEmbed.predict(train_vectors)\ny_predicted_train1","eee1426d":"from sklearn import metrics\nr2_Score= metrics.r2_score(train_df['target'],y_predicted_train1)\nprint(\"r2 score is:{}\".format(r2_Score))","6e1c22a9":"cm=confusion_matrix(train_df['target'],y_predicted_train1)\nsns.heatmap(cm,annot=True)","e6eb9392":"print(classification_report(train_df['target'],y_predicted_train1))","486e38e5":"# seeing on the vectors through countvectorizer\n","bd063bf1":"svc_cv=LinearSVC(random_state=1,dual=False,max_iter=10000)\nsvc_cv.fit(bag_train,train_df['target'])","a1a890f5":"scores = model_selection.cross_val_score(svc_cv,bag_train, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","7bb6d48c":"y_predicted_train2=svc_cv.predict(bag_train)\ny_predicted_train2","5d7a7da7":"from sklearn import metrics\nr2_Score= metrics.r2_score(train_df['target'],y_predicted_train2)\nprint(\"r2 score is:{}\".format(r2_Score))","a0db3993":"cm=confusion_matrix(train_df['target'],y_predicted_train2)\nsns.heatmap(cm,annot=True)","31a91a19":"svc_tfidf=LinearSVC(random_state=1,dual=False,max_iter=10000)\nsvc_tfidf.fit(tfidf_train,train_df['target'])","6b18aa7f":"y_predicted_train=svc_tfidf.predict(tfidf_train)\ny_predicted_train","916b2433":"cm=confusion_matrix(train_df['target'],y_predicted_train)\nsns.heatmap(cm,annot=True)","22b0f7b5":"# first lets see for xgb word embed\nfrom xgboost import XGBClassifier","3e8f9863":"xgb_wordEmbed = XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)","3f7f6c51":"xgb_wordEmbed.fit(train_vectors,train_df['target'])","28431d76":"scores = model_selection.cross_val_score(xgb_wordEmbed, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","cb3245ea":"y_predicted_train3=xgb_wordEmbed.predict(train_vectors)\ny_predicted_train3","2cc03fc8":"from sklearn import metrics\nr2_Score= metrics.r2_score(train_df['target'],y_predicted_train3)\nprint(\"r2 score is:{}\".format(r2_Score))","5a828e04":"cm=confusion_matrix(train_df['target'],y_predicted_train3)\nsns.heatmap(cm,annot=True)","e6290a94":"clf_xgb_TFIDF = XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                    subsample=0.8, nthread=10, learning_rate=0.1)\nclf_xgb_TFIDF.fit(tfidf_train,train_df['target'])\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, tfidf_train, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","cdbe4b0c":"# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nclf_NB.fit(bag_train,train_df[\"target\"])\nscores = model_selection.cross_val_score(clf_NB, bag_train,train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","9564204e":"y_train_prediction4=clf_NB.predict(bag_train)\ny_train_prediction4","1283bde1":"from sklearn import metrics\nr2_Score= metrics.r2_score(train_df['target'],y_train_prediction4)\nprint(\"r2 score is:{}\".format(r2_Score))","8a141981":"# lets check for tfidf\nclf_NB_TFIDF = MultinomialNB()\nclf_NB_TFIDF.fit(tfidf_train,train_df[\"target\"])\nscores = model_selection.cross_val_score(clf_NB_TFIDF,tfidf_train,train_df[\"target\"], cv=3, scoring=\"f1\")\nscores","6a9b033c":"import pandas as pd\nsample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","3c8bc2cb":"sample_submission.shape","2e43a570":"y_predicted_train3=y_predicted_train3[:3263]","d4a66617":"sample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsample_submission[\"target\"] = y_predicted_train3.astype('int64')\nsample_submission.to_csv(\"submission.csv\", index=False)","1d665982":"![](https:\/\/miro.medium.com\/max\/1476\/0*z9jqZsQ7JSTZGSZz.jpg)","60c0853d":"lets remove emojis and numbers as well","72743762":"Transforming into vector form ","564d4ec6":"**LET'S PERFORM TEXT NORMALIZATION**","ff7522da":"**lets define a common function that could be applied on full text**","e0bef434":"Generating a wordcloud for disaster tweets and non-disaster tweets","eb15ee52":"**LETS APPLY LEMMATIZATION TO OUR TEXT COLUMN NOW**","5ee7193d":"**LETS DO STEMMING ON OUR DATA**","05204acf":"Word Vectors\/Word Embeddings\n\nA word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.","82a02a9f":"**Bag of words model**\n\nA bag-of-words (B.o.w) is a representation of text that describes the occurrence of words within a document. It involves two things:\n\nA vocabulary of known words. A measure of the presence of known words. It is called a \u201cbag\u201d of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.","edb797e1":"**we can observe that most of the headlines containes url in the so we will deal with them in upcoming blocks**","875de33d":"We can observe that large amount of percentage is there for tweets which indicates no information about the disaster","48ae9775":"# **NATURAL LANGUAGE PROCESSING WITH DISASTER TWEETS**","2ac4d5fb":"Lemmatization is a systematic process of removing the inflectional form of a token and transform it into a lemma. It makes use of word structure, vocabulary, part of speech tags, and grammar relations.\n\nThe output of lemmatization is a root word called a lemma. for example \u201cam\u201d, \u201care\u201d, \u201cis\u201d will be converted to \u201cbe\u201d. Similarly, running runs, \u2018ran\u2019 will be replaced by \u2018run\u2019.\n","8f2d5112":"#now lets perform predictions by testing out diffferent models","83d0f750":"Text normalization is the process of transforming text into a single canonical form that it might not have had before.For example, the word \u201cgooood\u201d and \u201cgud\u201d can be transformed to \u201cgood\u201d, its canonical form.ANTINATIONALIST can be transformed to nationalist.","79217807":"No lets apply both of these concepts to our text column","d8fab263":"Lets remove punctuation and stopwords from the text given","fb677e38":"we can observe that in disaster tweets words like suicide,bomber,fire ,hiroshima appears more frequently\nAnd one can easily identify looking the word cloud of non diasater tweetes,what are the frequent words that happen to arrive","5e83a338":"#now lets give a try on XGBOOST","9de6dfc4":"Stemming is a elementary rule-based process for removing inflationary forms from a given token.The output of the error is the stem of a word. for example laughing, laughed, laughs, laugh all will become laugh after the stemming process.\nAnother example is studies,will be converted to studi,basically it tries to remove the suffix from the word","6b95b369":"#preparing the submission file","ede71bca":"Lets convert all the abbreviations to its full form. Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-bert-keras?scriptVersionId=31186559","ca79ca24":"WE can observe from the above results that XGBOOST on wordembedding give bbetter results as compared to other models and the r2 score is also nearby 0.94","20f02fd1":"#now lets try naive bayes","27dfddc6":"**TFIDF Features** \n\nAnother common representation is TF-IDF (Term Frequency - Inverse Document Frequency). TF-IDF is similar to bag of words except that each term count is scaled by the term's frequency in the corpus.\nUsing TF-IDF can potentially improve your models.\n\nTerm Frequency: is a scoring of the frequency of the word in the current document.\n\nTF = (Number of times term t appears in a document)\/(Number of terms in the document)\n\nInverse Document Frequency: is a scoring of how rare the word is across documents.\n\nIDF = 1+log(N\/n), where, N is the number of documents and n is the number of documents a term t has appeared in\n"}}