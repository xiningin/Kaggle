{"cell_type":{"39505b81":"code","f34630b4":"code","8fdfd9b5":"code","06f869df":"code","deb2c8c5":"code","b80ac480":"code","6100da51":"code","8f411ad6":"code","1583a497":"code","ca6ca4f7":"code","2cabac60":"code","9266d818":"code","f27549ff":"code","822fa84b":"code","d95a1423":"code","25a28286":"code","657a6f1e":"code","4a0e8dc8":"code","960e6ddf":"code","74889784":"markdown","a03a70b4":"markdown","84b58d9b":"markdown","dd65fcc7":"markdown","597f7b4d":"markdown","da12084c":"markdown","c5c38014":"markdown","a611fdf7":"markdown","2aa79098":"markdown","178eac6c":"markdown","59c2d60e":"markdown","b56d54a3":"markdown","3f8ddc8e":"markdown","e6b4aacc":"markdown"},"source":{"39505b81":"import pandas as pd\ndata_df=pd.read_csv('..\/input\/chinese-mnist\/chinese_mnist.csv')\ndata_df.groupby([\"value\",\"character\"]).size()","f34630b4":"total = data_df.isnull().sum().sort_values(ascending = False)\npercent = (data_df.isnull().sum()\/data_df.isnull().count()*100).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","8fdfd9b5":"import os\nimage_files = list(os.listdir(\"..\/input\/chinese-mnist\/data\/data\"))\nprint(\"Number of image files: {}\".format(len(image_files)))","06f869df":"import skimage.io\nimport numpy as np\n\ndef image_files(x):\n    file_name = f\"input_{x[0]}_{x[1]}_{x[2]}.jpg\"\n    return file_name\ndata_df[\"file\"] = data_df.apply(image_files, axis=1)\n\ndef image_sizes(file_name):\n    image = skimage.io.imread(\"..\/input\/chinese-mnist\/data\/data\/\" + file_name)\n    return list(image.shape)\n\nimage_size = np.stack(data_df['file'].apply(image_sizes))\nimage_size_df = pd.DataFrame(image_size,columns=['w','h'])\ndata_df = pd.concat([data_df,image_size_df],axis=1, sort=False)\n\ndata_df.head()","deb2c8c5":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(data_df, test_size=0.2, random_state=42, stratify=data_df[\"code\"].values)\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"code\"].values)\n\nprint(\"Train set: {}\".format(train_df.shape[0]))\nprint(\"Test set: {}\".format(test_df.shape[0]))\nprint(\"Validation set: {}\".format(val_df.shape[0]))","b80ac480":"import skimage.transform\n\ndef read_image(file_name):\n    image = skimage.io.imread(\"..\/input\/chinese-mnist\/data\/data\/\" + file_name)\n    image = skimage.transform.resize(image, (64, 64, 1), mode='reflect')\n    return image\n\ndef categories_encoder(dataset, var='character'):\n    X = np.stack(dataset['file'].apply(read_image))\n    y = pd.get_dummies(dataset[var], drop_first=False)\n    return X, y\n\nX_train, y_train = categories_encoder(train_df)\nX_val, y_val = categories_encoder(val_df)\nX_test, y_test = categories_encoder(test_df)","6100da51":"from keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization,LeakyReLU\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom keras.utils import to_categorical\n\nmodel=Sequential()\nmodel.add(Conv2D(32, kernel_size=3, input_shape=(64, 64,1), activation='relu', padding='same'))\nmodel.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(y_train.columns.size, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","8f411ad6":"early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1)\n\ntrain_model  = model.fit(X_train, y_train,\n                  batch_size=32,\n                  epochs=10,\n                  verbose=1,\n                  validation_data=(X_val, y_val))","1583a497":"model.evaluate(X_test, y_test)","ca6ca4f7":"from sklearn import metrics\nprint(metrics.classification_report(np.argmax(y_test.values,axis=1), np.argmax(model.predict(X_test),axis=1), target_names=y_test.columns))","2cabac60":"from tensorflow.keras.layers import GlobalAveragePooling2D, Activation\n\nmodel=Sequential()\nmodel.add(Conv2D(32, kernel_size=3, input_shape=(64, 64,1), activation='relu', padding='same'))\n# model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n# model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(y_train.columns.size, kernel_size=3, activation='relu', padding='same'))\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Activation('softmax'))\n# model.add(Dense(y_train.columns.size, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","9266d818":"early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1)\ntrain_model  = model.fit(X_train, y_train,\n                  batch_size=32,\n                  epochs=50,\n                  verbose=1,\n                  validation_data=(X_val, y_val),\n                  callbacks=[early_stopping])","f27549ff":"model.evaluate(X_test, y_test)","822fa84b":"\nprint(metrics.classification_report(np.argmax(y_test.values,axis=1), np.argmax(model.predict(X_test),axis=1), target_names=y_test.columns))","d95a1423":"model=Sequential()\nmodel.add(Conv2D(32, kernel_size=3, input_shape=(64, 64,1), activation='relu', padding='same'))\n# model.add(Conv2D(32, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n# model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(y_train.columns.size, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","25a28286":"early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1)\n\ntrain_model  = model.fit(X_train, y_train,\n                  batch_size=32,\n                  epochs=50,\n                  verbose=1,\n                  validation_data=(X_val, y_val),\n                  callbacks=[early_stopping])","657a6f1e":"model.evaluate(X_test, y_test)","4a0e8dc8":"\nprint(metrics.classification_report(np.argmax(y_test.values,axis=1), np.argmax(model.predict(X_test),axis=1), target_names=y_test.columns))","960e6ddf":"from keras import models\n\nlayer_outputs = [layer.output for layer in model.layers][0:6]\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs) \nactivations = activation_model.predict(X_test[0:1])\n\nlayer_names = []\nfor layer in model.layers:\n    layer_names.append(layer.name)\n    \nimages_per_row = 16\n\nfor layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n    n_features = layer_activation.shape[-1] # Number of features in the feature map\n    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n    n_cols = n_features \/\/ images_per_row # Tiles the activation channels in this matrix\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size, # Displays the grid\n                         row * size : (row + 1) * size] = channel_image\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","74889784":"<div class=\"jumbotron\">\n  <h1 class=\"display-4\">Thank you for making till the end!<\/h1>\n  <p class=\"lead\">Please upvote if you find this notebook helps you. Any comments are welcomed. I'd love to get feedbacks!<\/p>\n  \n<\/div>","a03a70b4":"**Encode the categories**","84b58d9b":"<h1 id=\"#1\">Good Practices: The insights of the data<\/h1>","dd65fcc7":"<ul class=\"list-group\">\n  <li class=\"list-group-item active\">Notebook Content<\/li>\n    \n  <li class=\"list-group-item d-flex justify-content-between align-items-center\" ><span class=\"badge badge-primary badge-pill\">1<\/span> <a href=\"#1\">\n    Insights of Data\n    <\/a>\n  <\/li>\n  <li class=\"list-group-item d-flex justify-content-between align-items-center\"> <span class=\"badge badge-primary badge-pill\">2<\/span> <a href=\"#2\">\n    VGG-like model\n     <\/a>\n  <\/li>\n  <li class=\"list-group-item d-flex justify-content-between align-items-center\"><span class=\"badge badge-primary badge-pill\">3<\/span> <a href=\"#3\">\n    No-Dense Model\n    <\/a>\n  <\/li>\n    <li class=\"list-group-item d-flex justify-content-between align-items-center\"><span class=\"badge badge-primary badge-pill\">4<\/span> <a href=\"#4\">\n    Simpler Model\n    <\/a>\n  <\/li>\n    <li class=\"list-group-item d-flex justify-content-between align-items-center\"><span class=\"badge badge-primary badge-pill\">5<\/span> <a href=\"#5\">\n    Visualizing the intermidate layer (Simpler Model)\n    <\/a>\n  <\/li>\n<\/ul>\n\n<div>\n<a href=\"https:\/\/khvmaths.medium.com\/chinese-digit-mnist-1b46f51e8f75\"><p style=\"text-align:center;\">Medium Article:<img src=\"data:image\/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiPjxwYXRoIGQ9Ik0xMiAwYy02LjYyNyAwLTEyIDUuMzczLTEyIDEyczUuMzczIDEyIDEyIDEyIDEyLTUuMzczIDEyLTEyLTUuMzczLTEyLTEyLTEyem0wIDJjNS41MTQgMCAxMCA0LjQ4NiAxMCAxMHMtNC40ODYgMTAtMTAgMTAtMTAtNC40ODYtMTAtMTAgNC40ODYtMTAgMTAtMTB6bS0yLjQyNiAxNC43NDFoLTMuNTc0di0uMjAybDEuMjYxLTEuNTI5Yy4xMzQtLjEzOS4xOTUtLjMzNS4xNjItLjUyNnYtNS4zMDRjLjAxNS0uMTQ3LS4wNDEtLjI5My0uMTUxLS4zOTJsLTEuMTIxLTEuMzV2LS4yMDFoMy40NzlsMi42ODkgNS44OTcgMi4zNjQtNS44OTdoMy4zMTd2LjIwMWwtLjk1OC45MTljLS4wODMuMDYzLS4xMjQuMTY2LS4xMDYuMjY5djYuNzQ4Yy0uMDE4LjEwMy4wMjMuMjA2LjEwNi4yNjlsLjkzNi45MTl2LjIwMWgtNC43MDZ2LS4yMDFsLjk2OS0uOTQxYy4wOTUtLjA5NS4wOTUtLjEyMy4wOTUtLjI2OXYtNS40NTVsLTIuNjk1IDYuODQ0aC0uMzY0bC0zLjEzNy02Ljg0NHY0LjU4N2MtLjAyNi4xOTMuMDM4LjM4Ny4xNzQuNTI2bDEuMjYgMS41Mjl2LjIwMnoiLz48L3N2Zz4=\"><\/p><\/a><\/div>","597f7b4d":"<h1 id=\"#2\">VGG-like Model<\/h1>","da12084c":"**Check for missing data**","c5c38014":"<h1 id=\"#2\">No-Dense Model<\/h1>","a611fdf7":"**Well, no missing data, but how about missing images?**","2aa79098":"**Split the dataset**","178eac6c":"**Add the image path and sizes to the dataframe!**","59c2d60e":"<h1 id=\"#4\">Simpler Model<\/h1>","b56d54a3":"<h1 id=\"#5\">Visualizing the Model<\/h1>","3f8ddc8e":"<div style=\"background: #e6e6d8 url('https:\/\/dl.dropboxusercontent.com\/s\/t0gu051d08sei65\/bg-retro-noise.png');\n  color: #121212; min-height:300px;\">\n    \n<section style=\"position: absolute;\n  width: 100%;\n  min-width: 800px;\n  text-align: center;\n  top: 50%;\n  margin-top: -55px;\">\n    \n  <h3 style=\"transform: matrix(1, -0.15, 0, 1, 0, 0);\n  -ms-transform: matrix(1, -0.10, 0, 1, 0, 0);\n  -webkit-transform: matrix(1, -0.15, 0, 1, 0, 0);\n  text-transform: uppercase;\n  font-weight: 400;\n  font-size: 70px;\n  text-shadow: 4px 5px #e6e6d8, 6px 7px #c6a39a;\">Chinese MNIST<\/h3>\n    <p style=\"text-align:center;\">@koayhongvin<\/p>\n<\/section>\n    <\/div>","e6b4aacc":"**Check for distribution of data**"}}