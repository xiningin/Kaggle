{"cell_type":{"4f660689":"code","5313c482":"code","4e90da56":"code","2237b11f":"code","1264f7a8":"code","e39bbb0d":"code","f060a347":"code","8bdd3c4a":"code","c08cffa9":"code","301d0af0":"code","82ebdf42":"code","5acf4c16":"code","52d644d0":"code","e50fdc53":"code","5d8ced8a":"code","ffe3e214":"code","687640a3":"code","b3e623e1":"code","1b3e4df6":"code","82c32ce2":"code","a5e4389c":"code","e9013958":"code","f5a5c0f1":"code","77c63cdf":"code","67393597":"code","9d7928a4":"code","18d32c42":"code","e2052427":"code","d90f6412":"code","46629c19":"code","46afbd2d":"code","92a40c42":"code","aa16fae8":"code","8bf16fb8":"code","29e76b9f":"code","72c4629e":"code","a97c90f8":"code","c2583447":"code","f6cf0b3e":"code","1842795a":"code","89366b4f":"code","e79849cd":"code","5df3ae94":"code","bd3548d2":"code","314db5b9":"code","0e7db5c8":"code","f65f016e":"code","24da96e7":"code","cda10370":"code","ed3182e9":"code","2f0695f0":"code","4c153dad":"code","0fe07e31":"code","c1c8a14c":"markdown","b23b157b":"markdown","8fd1b9e7":"markdown","3f8e0856":"markdown","d219b89e":"markdown","17222485":"markdown","e6eb7907":"markdown","2b436b48":"markdown","1d00d48e":"markdown","53fcf8dd":"markdown","bbf6a853":"markdown","d926d472":"markdown","568f873f":"markdown","bbb03a30":"markdown","ebea66b1":"markdown","644fe990":"markdown","99a01379":"markdown","c71e6fbb":"markdown","ba7b0a17":"markdown","c35c93c6":"markdown","d9e5130c":"markdown","9eb9475c":"markdown","50c55538":"markdown","bd806203":"markdown","fff2ce4f":"markdown","73008ac2":"markdown"},"source":{"4f660689":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5313c482":"import itertools\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport operator\nimport random\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n#warnings.filterwarnings(action='once')\n%matplotlib inline","4e90da56":"df = pd.read_csv('\/kaggle\/input\/ManUtd_All_Season_Data.csv')   \ndf.head(100)","2237b11f":"df.shape","1264f7a8":"df.isnull().sum()","e39bbb0d":"df = df.drop(['Division'], axis = 1)\ndf.head()","f060a347":"df.dropna(subset = [\"P\"], inplace=True)\ndf.isnull().sum()","8bdd3c4a":"sns.relplot(x=\"Season\", y=\"Pos\", ci=None, kind=\"line\",dashes = False, markers=True, data=df, height = 5, aspect = 3)","c08cffa9":"X = df[['Season', 'P', 'W', 'D', 'L', 'F', 'A', 'Pts']]\ny = df[['Pos']]\n# x1 and y1 will be used for fitting and training data that is not pre-processed\nX1 = df[['Season', 'P', 'W', 'D', 'L', 'F', 'A', 'Pts']]\ny1 = df[['Pos']]","301d0af0":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score\nlin_df = LinearRegression()  \nlin_df.fit(X_train, y_train)\nlr_pred = lin_df.predict(X_test)   \nlr_pred[0:5]","82ebdf42":"linrgr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))\nlinrgr_r2 = r2_score(y_test, lr_pred)\nprint(\"RMSE Score for Test set: \",linrgr_rmse)\nprint(\"R2 Score: \",linrgr_r2)","5acf4c16":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nfrom sklearn import linear_model\nreg = linear_model.Ridge()\nreg.fit(X_train, y_train)\nrdg_pred = reg.predict(X_test)\nprint(\"The coefficients after ridge regression is :\", reg.coef_)\nprint(\"The intercept after ridge regression is :\", reg.intercept_)\nrdg_pred[0:5]","52d644d0":"rdgrgr_rmse = np.sqrt(mean_squared_error(y_test, rdg_pred))\nrdgrgr_r2 = r2_score(y_test, rdg_pred)\nprint(\"RMSE Score for Test set: \",rdgrgr_rmse)\nprint(\"R2 Score: \",rdgrgr_r2)","e50fdc53":"# Finding the best value of alpha using cross validation\nfrom sklearn.linear_model import LassoCV\nregr = LassoCV()\nregr.fit(X, y)\nprint(regr.alpha_)","5d8ced8a":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nfrom sklearn import linear_model\nclf = linear_model.Lasso(alpha=regr.alpha_)\nclf.fit(X_train, y_train)\nlas_pred = clf.predict(X_test)\nprint(\"The coefficients after lasso regression is :\", clf.coef_)\nprint(\"The intercept after lasso regression is :\", clf.intercept_)\nlas_pred[0:5]","ffe3e214":"lasrgr_rmse = np.sqrt(mean_squared_error(y_test, las_pred))\nlasrgr_r2 = r2_score(y_test, las_pred)\nprint(\"RMSE Score for Test set: \",lasrgr_rmse)\nprint(\"R2 Score: \",lasrgr_r2)","687640a3":"# Finding the best value of alpha using cross validation\nfrom sklearn.linear_model import ElasticNetCV\nregr = ElasticNetCV()\nregr.fit(X, y)\nprint(regr.alpha_)","b3e623e1":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n# Actual regression on test set\nfrom sklearn.linear_model import ElasticNet\nreg = ElasticNet(alpha=regr.alpha_)\nreg.fit(X_train, y_train)\nen_pred = reg.predict(X_test)\nprint(\"The coefficients after elasticnet regression is :\", reg.coef_)\nprint(\"The intercept after elasticnet regression is :\", reg.intercept_)\nen_pred[0:5]","1b3e4df6":"enrgr_rmse = np.sqrt(mean_squared_error(y_test, en_pred))\nenrgr_r2 = r2_score(y_test, en_pred)\nprint(\"RMSE Score for Test set: \",enrgr_rmse)\nprint(\"R2 Score: \",enrgr_r2)","82c32ce2":"# Finding the best value of alpha using cross validation\nfrom sklearn.linear_model import LassoLarsCV\nregr = LassoLarsCV()\nregr.fit(X, y)\nprint(regr.alpha_)","a5e4389c":"from sklearn import linear_model\nreg = linear_model.LassoLars(alpha=regr.alpha_)\nreg.fit(X_train, y_train)\nlslr_pred = reg.predict(X_test)\nprint(\"The coefficients after elasticnet regression is :\", reg.coef_)\nprint(\"The intercept after elasticnet regression is :\", reg.intercept_)\nlslr_pred[0:5]","e9013958":"lslrrgr_rmse = np.sqrt(mean_squared_error(y_test, lslr_pred))\nlslrrgr_r2 = r2_score(y_test, lslr_pred)\nprint(\"RMSE Score for Test set: \",lslrrgr_rmse)\nprint(\"R2 Score: \",lslrrgr_r2)","f5a5c0f1":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn import linear_model\n\nplrgr_rmse = np.zeros(9)\nplrgr_r2 = np.zeros(9)\n\nfor i in range(1,10):\n    poly_df = PolynomialFeatures(degree = i)\n    transform_poly = poly_df.fit_transform(X_train)\n    clf = linear_model.LinearRegression()\n    clf.fit(transform_poly,y_train)\n    polynomial_predict = clf.predict(poly_df.fit_transform(X_test))\n    plrgr_rmse[i-1] = np.sqrt(mean_squared_error(y_test,polynomial_predict))\n    plrgr_r2[i-1] = r2_score(y_test,polynomial_predict)\n    print(\"\\nThe predicted values with degree = \",i,\" is \\n\",polynomial_predict[0:5])\n    print(\"\\nRMSE Score of Test set for degree \", i,\" is: \",plrgr_rmse[i-1])\n    print(\"R2 RMSE Score of Test set for degree \", i,\" is: \",plrgr_r2[i-1]) \n\nprint(\"\\nThe best RMSE score of Test Set is \", plrgr_rmse.min(), \" with degree = \",plrgr_rmse.argmin()+1)\nprint(\"The max R2 score of Test Set is \", plrgr_r2.max(), \" with degree = \",plrgr_r2.argmax()+1)","77c63cdf":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nk = ['mse', 'friedman_mse', 'mae']\ndt_rmse = np.zeros(3)\ndt_r2 = np.zeros(3)\nn = 0\n\nfrom sklearn.tree import DecisionTreeRegressor\nfor i in k:\n    dt_reg = DecisionTreeRegressor(criterion = i)          # create  DecisionTreeReg with sklearn\n    dt_reg.fit(X_train,y_train)\n    dt_predict = dt_reg.predict(X_test)\n    dt_rmse[n] = np.sqrt(mean_squared_error(y_test,dt_predict))\n    dt_r2[n] = r2_score(y_test,dt_predict)\n    print(\"\\nThe predicted values for Test Set using criterion = \",i,\" is: \",dt_predict[0:5])\n    print(\"\\nThe RMSE score for Test Set using criterion = \",i,\" is: \",dt_rmse[n])\n    print(\"The R2 score for Test Set using criterion = \",i,\" is: \",dt_r2[n])\n    n += 1   \nprint(\"\\nThe best RMSE score for Test Set is \", dt_rmse.min())\nprint(\"The max R2 score of Test Set is \", dt_r2.max())   ","67393597":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nk = ['mse', 'mae']\nn = 0\nrf_rmse = np.zeros(2)\nrf_r2 = np.zeros(2)\n\nfrom sklearn.ensemble import RandomForestRegressor\nfor i in k:\n    rf_reg = RandomForestRegressor(criterion = i)\n    rf_reg.fit(X_train,y_train)\n    rf_pred = rf_reg.predict(X_test)\n    rf_rmse[n] = np.sqrt(mean_squared_error(y_test,rf_pred))\n    rf_r2[n] = r2_score(y_test,dt_predict)\n    print(\"\\nThe predicted values for Test Set using criterion = \",i,\" is: \",rf_pred[0:5])\n    print(\"\\nThe RMSE score for Test Set using criterion = \",i,\" is: \",rf_rmse[n])\n    print(\"The R2 score for Test Set using criterion = \",i,\" is: \",rf_r2[n])\n    n += 1   \n    \nprint(\"\\nThe best RMSE score for Test Set is \", rf_rmse.min())\nprint(\"The max R2 score of Test Set is \", rf_r2.max())       ","9d7928a4":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nk = ['linear', 'square', 'exponential']\nn = 0\nadb_rmse = np.zeros(3)\nadb_r2 = np.zeros(3)\n\nfrom sklearn.ensemble import AdaBoostRegressor\nfor i in k:\n    ada_regr = AdaBoostRegressor(loss = i)\n    ada_regr.fit(X_train,y_train)\n    ada_pred = ada_regr.predict(X_test)\n    adb_rmse[n] = np.sqrt(mean_squared_error(y_test,ada_pred))\n    adb_r2[n] = r2_score(y_test,ada_pred)\n    print(\"\\nThe predicted values for Test Set using loss = \",i,\" is: \",ada_pred[0:5])\n    print(\"\\nThe RMSE score for Test Set using loss = \",i,\" is: \",adb_rmse[n])\n    print(\"The R2 score for Test Set using loss = \",i,\" is: \",adb_r2[n])\n    n += 1   \nprint(\"\\nThe best RMSE score for Test Set is \", adb_rmse.min())\nprint(\"The max R2 score of Test Set is \", adb_r2.max())           ","18d32c42":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nk = ['ls', 'lad', 'huber', 'quantile']\nn = 0\ngdb_rmse = np.zeros(4)\ngdb_r2 = np.zeros(4)\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfor i in k:\n    reg = GradientBoostingRegressor(loss = i)\n    reg.fit(X_train, y_train)\n    grdbst_pred = reg.predict(X_test)\n    gdb_rmse[n] = np.sqrt(mean_squared_error(y_test,grdbst_pred))\n    gdb_r2[n] = r2_score(y_test,grdbst_pred)\n    print(\"\\nThe predicted values for Test Set using loss = \",i,\" is: \",grdbst_pred[0:5])\n    print(\"\\nThe RMSE score for Test Set using loss = \",i,\" is: \",gdb_rmse[n])\n    print(\"The R2 score for Test Set using loss = \",i,\" is: \",gdb_r2[n])\n    n += 1   \nprint(\"\\nThe best RMSE score for Test Set is \", gdb_rmse.min())\nprint(\"The max R2 score of Test Set is \", gdb_r2.max())    ","e2052427":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nn = 0\nknn_rmse = np.zeros(9)\nknn_r2 = np.zeros(9)\n\nfor i in range(1,10):\n    neigh = KNeighborsRegressor(n_neighbors=i)\n    neigh.fit(X_train, y_train)\n    knn_pred = neigh.predict(X_test)\n    knn_rmse[n] = np.sqrt(mean_squared_error(y_test,knn_pred))\n    knn_r2[n] = r2_score(y_test,knn_pred)\n    print(\"\\nThe predicted values for Test Set with neighbor k = \",i,\" is: \\n\",knn_pred[0:5])\n    print(\"\\nThe RMSE score for Test Set with neighbor k = \",i,\" is: \",knn_rmse[n])\n    print(\"The R2 score for Test Set with neighbor k = \",i,\" is: \",knn_r2[n])\n    n += 1   \nprint(\"\\nThe best RMSE score for Test Set is \", knn_rmse.min(), \" with neighbor k = \", knn_rmse.argmin()+1)\nprint(\"The max R2 score of Test Set is \", knn_r2.max(), \" with neighbor k = \", knn_r2.argmax()+1)   ","d90f6412":"X= preprocessing.StandardScaler().fit(X).transform(X)\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nfrom sklearn.linear_model import SGDRegressor\nreg = SGDRegressor()\nreg.fit(X_train, y_train)\nsgd_pred = reg.predict(X_test)\nsgd_pred[0:5]","46629c19":"sgd_rmse = np.sqrt(mean_squared_error(y_test,sgd_pred))\nsgd_r2 = r2_score(y_test,sgd_pred)\nprint(\"RMSE Score for Test set: \",sgd_rmse)\nprint(\"R2 Score for Test set: \",sgd_r2)","46afbd2d":"#X= preprocessing.StandardScaler().fit(X).transform(X)\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nfrom sklearn.svm import SVR\nk = ['linear', 'poly', 'rbf', 'sigmoid']\nn = 0\nsvr_rmse = np.zeros(4)\nsvr_r2 = np.zeros(4)\n\nfor i in k:\n    reg = SVR(kernel = i)\n    reg.fit(X_train, y_train)\n    svr_pred = reg.predict(X_test)\n    svr_rmse[n] = np.sqrt(mean_squared_error(y_test,svr_pred))\n    svr_r2[n] = r2_score(y_test,svr_pred)\n    print(\"\\nThe predicted values for Test Set with kernel = \",i,\" is: \",svr_pred[0:5])\n    print(\"\\nThe RMSE score for Test Set with kernel = \",i,\" is: \",svr_rmse[n])\n    print(\"The R2 score for Test Set with kernel = \",i,\" is: \",svr_r2[n])\n    n += 1   \nprint(\"\\nThe best RMSE score for Test Set is \", svr_rmse.min())\nprint(\"The max R2 score of Test Set is \", svr_r2.max())         ","92a40c42":"#X= preprocessing.StandardScaler().fit(X).transform(X)\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nfrom sklearn.svm import NuSVR\nk = ['linear', 'poly', 'rbf', 'sigmoid']\nn = 0\nnusvr_rmse = np.zeros(4)\nnusvr_r2 = np.zeros(4)\n\nfor i in k:\n    reg = NuSVR(kernel = i)\n    reg.fit(X_train, y_train)\n    nusvr_pred = reg.predict(X_test)\n    nusvr_rmse[n] = np.sqrt(mean_squared_error(y_test,nusvr_pred))\n    nusvr_r2[n] = r2_score(y_test,nusvr_pred)\n    print(\"\\nThe predicted values for Test Set with kernel = \",i,\" is: \",nusvr_pred[0:5])\n    print(\"\\nThe RMSE score for Test Set with kernel = \",i,\" is: \",nusvr_rmse[n])\n    print(\"The R2 score for Test Set with kernel = \",i,\" is: \",nusvr_r2[n])\n    n += 1   \nprint(\"\\nThe best RMSE score for Test Set is \", nusvr_rmse.min())\nprint(\"The max R2 score of Test Set is \", nusvr_r2.max())       ","aa16fae8":"#X= preprocessing.StandardScaler().fit(X).transform(X)\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)\n\nfrom sklearn.svm import LinearSVR\n\nreg = LinearSVR()\nreg.fit(X_train, y_train)\nlinsvr_pred = reg.predict(X_test)\nlinsvr_rmse = np.sqrt(mean_squared_error(y_test,linsvr_pred))\nlinsvr_r2 = r2_score(y_test,linsvr_pred)\nprint(\"\\nThe predicted values for Test Set is: \",linsvr_pred[0:5])\nprint(\"\\nThe RMSE score for Test Set is: \",linsvr_rmse)\nprint(\"The R2 score for Test Set is: \",linsvr_r2)  ","8bf16fb8":"# rmse = root mean squared score.......r2 = R2-Score\n# 1.Linear Models\n# 1.1. Linear Regression\nlinrgr_rmse\nlinrgr_r2\n\n# 1.2. Ridge Regression\nrdgrgr_rmse\nrdgrgr_r2\n\n# 1.3. Lasso\nlasrgr_rmse\nlasrgr_r2\n\n# 1.4. ElasticNet\nenrgr_rmse\nenrgr_r2\n\n# 1.5. LarsLasso\nlslrrgr_rmse\nlslrrgr_r2\n\n# 2.Polynomial Regression\nplrgr_rmse = plrgr_rmse.min()\nplrgr_r2 = plrgr_r2.max()\n\n# 3.Decision Tree\ndt_rmse = dt_rmse.min()\ndt_r2 = dt_r2.max()\n\n# 4.Ensemble Methods\n# 4.1. Random Forest\nrf_rmse = rf_rmse.min()\nrf_r2 = rf_r2.max()\n\n# 4.2. AdaBoost\nadb_rmse = adb_rmse.min()\nadb_r2 = adb_r2.max()\n\n# 4.3. GradientBoost\ngdb_rmse = gdb_rmse.min()\ngdb_r2 = gdb_r2.max()\n\n# 5.KNearestNeighbor\nknn_rmse = knn_rmse.min()\nknn_r2 = knn_r2.max()\n\n# 6.Stochastic Gradient Descent\nsgd_rmse\nsgd_r2\n\n# 7.Support Vector Machines\n# 7.1. SVR\nsvr_rmse = svr_rmse.min()\nsvr_r2 = svr_r2.max()\n\n# 7.2. NuSVR\nnusvr_rmse = nusvr_rmse.min()\nnusvr_r2 = nusvr_r2.max()\n# 7.3. LinearSVR\nlinsvr_rmse = linsvr_rmse\nlinsvr_r2 = linsvr_r2\n\n#max of all\nmin_rmse = [linrgr_rmse,rdgrgr_rmse,lasrgr_rmse,enrgr_rmse,lslrrgr_rmse,plrgr_rmse,dt_rmse,rf_rmse,adb_rmse,gdb_rmse,knn_rmse,sgd_rmse,svr_rmse,nusvr_rmse,linsvr_rmse]\nmax_r2 = [linrgr_r2,rdgrgr_r2,lasrgr_r2,enrgr_r2,lslrrgr_r2,plrgr_r2,dt_r2,rf_r2,adb_r2,gdb_r2,knn_r2,sgd_r2,svr_r2,nusvr_r2,linsvr_r2]","29e76b9f":"data = {'Algorithm':['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'ElasticNet Regression', 'LarsLasso Regression', 'Polynomial Regression','Decision Tree Regression','Random Forest Regression','AdaBoost Regression','Gradient Boosting Regression','KNearest Neighbor Regression','Stochastic Gradient Regression','Support Vector Regression','Nu Support Vector Regression','Linear Support Vector Regression'], \n        'R2-Sore':max_r2, 'Root Mean Squared Error':min_rmse}\ns = pd.DataFrame(data, index = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\ns","72c4629e":"print(\"The best R-squared score is shown by Gradient Boosting Regression \")\nprint(\"\\nR2 Score = \",gdb_r2)","a97c90f8":"fd = pd.read_csv('\/kaggle\/input\/ManUtd_All_Season_Data.csv')  \nfd.head(100)","c2583447":"fd = fd.drop(['Division'], axis = 1)\nfd.dropna(subset = [\"P\"], inplace=True)","f6cf0b3e":"fd.isnull().sum()","1842795a":"# For the dataset pertaining to next 30 years I have used the standard deviation and mean of W to get a random integer\nprint(\"mean of W = \",round(fd.W.mean()))\nprint(\"std of W = \",round(fd.W.std()))\nmin1 = round(fd.W.mean())-round(fd.W.std())\nmax1 = round(fd.W.mean())+round(fd.W.std())\nprint(\"The random value which we require is between \",min1,\" and \",max1)","89366b4f":"w = np.zeros(30)\nfor i in range(0,30):\n    w[i] = random.randint(13,25)","e79849cd":"print(\"mean of D = \",round(fd.D.mean()))\nprint(\"std of D = \",round(fd.D.std()))\nmin2 = round(fd.D.mean())-round(fd.D.std())\nmax2 = round(fd.D.mean())+round(fd.D.std())\nprint(\"The random value which we require is between \",min2,\" and \",max2)","5df3ae94":"d = np.zeros(30)\nfor i in range(0,30):\n    d[i] = random.randint(6,12)","bd3548d2":"# No. of matches lost = Total matches played - (Matches won + matches drawn)\nl = np.zeros(30)\nfor i in range(0,30):\n    l[i] = (38 - (w[i]+d[i]))","314db5b9":"print(\"mean of F = \",round(fd.F.mean()))\nprint(\"std of F = \",round(fd.F.std()))\nmin4 = round(fd.F.mean())-round(fd.F.std())\nmax4 = round(fd.F.mean())+round(fd.F.std())\nprint(\"The random value which we require is between \",min4,\" and \",max4)","0e7db5c8":"f = np.zeros(30)\nfor i in range(0,30):\n    f[i] = random.randint(52,84)","f65f016e":"print(\"mean of A = \",round(df.A.mean()))\nprint(\"std of A = \",round(df.A.std()))\nmin5 = round(fd.A.mean())-round(fd.A.std())\nmax5 = round(fd.A.mean())+round(fd.A.std())\nprint(\"The random value which we require is between \",min5,\" and \",max5)","24da96e7":"a = np.zeros(30)\nfor i in range(0,30):\n    a[i] = random.randint(31,67)","cda10370":"# Total points earned = 3*Matches Won + Matches drawn \np = np.zeros(30)\nfor i in range(0,30):\n    p[i] = (3*w[i]+d[i])","ed3182e9":"n = 0\ntest_set = np.zeros((30,8))\n#Total number of matches played in a season\ns = 38\nfor i in range(2019,2049):\n    test_set[n] = [i, s, w[n], d[n], l[n], f[n], a[n], p[n]]\n    n += 1\ntest_set[0:5]    ","2f0695f0":"from sklearn.ensemble import GradientBoostingRegressor\n\nreg = GradientBoostingRegressor(loss = 'lad')\n#fitting the data that is not pre processed\nreg.fit(X1, y1)\ngrdbst_pred = reg.predict(test_set)\n","4c153dad":"print(\"\\nThe predicted values for Test Set is: \",grdbst_pred[0:30])","0fe07e31":"plpred = np.round(grdbst_pred)\nprint(\"\\nThe predicted rounded values for Test Set is: \",plpred[0:30])","c1c8a14c":"### LinearSVR","b23b157b":"# Linear Models","8fd1b9e7":"The dataset have been taken from https:\/\/en.wikipedia.org\/wiki\/List_of_Manchester_United_F.C._seasons. It has 10 columns in total. Season, Division, P = Total Matches played, W = No. of matches won, D = No. of matches drawn, L = No. of matches lost, F = No. of goals scored for Manchester United, A = No. of goals conceded by Manchester United, Pts = Total points earned, Pos = The final position earned by Manchester United.","3f8e0856":"### Ridge Regression","d219b89e":"# About the Dataset","17222485":"### ElasticNet","e6eb7907":"### ADABoost","2b436b48":"# KNearestNeighbor","1d00d48e":"According to the predicted data, Manchester United will not be winning the league anytime soon and not even in the next 30 years. ","53fcf8dd":"### SVR","bbf6a853":"### Lasso","d926d472":"## Final Report","568f873f":"# Support Vector Machines","bbb03a30":"# Report on accuracy of different algorithms using RMSE value and R2 score","ebea66b1":"### Random Forest Model","644fe990":"# 4.Decision Tree Regression","99a01379":"### Gradient Boosting Regression","c71e6fbb":"I have tried to predict the season in the next 30 years when Manchester United is going to lift the Premier League trophy. I have used several algorithms for training and testing and have chosen the best algorithm for predicting. The dataset comprises of each of the relevant data related to Manchester United since English Football started to be played in divisions. ","ba7b0a17":"# Ensemble Methods","c35c93c6":"# Polynomial Regression","d9e5130c":"# Stochastic Gradient Descent","9eb9475c":"### LassoLars","50c55538":"### 1.Linear Regression","bd806203":"### NuSVR","fff2ce4f":"### Gradient Boosting","73008ac2":"# Creating a test set for the next 30 years and predicting Manchester United's League Standing using the best algorithm."}}