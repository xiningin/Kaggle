{"cell_type":{"f2a449cb":"code","87fb94d9":"code","ae0b5ac9":"code","517842bc":"code","6f0075ef":"code","f8f40402":"code","0b4b70c9":"markdown","d925bb77":"markdown","1f5346dd":"markdown","671b892a":"markdown","0511bad2":"markdown"},"source":{"f2a449cb":"%config InlineBackend.figure_format = 'svg'\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics","87fb94d9":"def read_data():\n    '''read csv data to memory\n    '''\n    train = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv')\n    print('train has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n    test = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/test.csv')\n    print('test has {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n    return train, test\n\n\ndef run_lgb():\n    '''train a lightgbm model \n    '''\n    y = train['target'].values\n    x = train.drop(['id', 'target'], axis = 1)\n    x_test = test.drop('id', axis = 1)\n\n\n    x_train, x_val, y_train, y_val = train_test_split(x, y, \n                                                      test_size = 0.2, \n                                                      random_state = 42, \n                                                      stratify = y)\n    categorical_features = [c for c, col in enumerate(train.columns) if 'cat' in col]\n    train_data = lgb.Dataset(x_train, label = y_train, categorical_feature = categorical_features)\n    test_data = lgb.Dataset(x_val, label = y_val, categorical_feature = categorical_features)\n    \n    # to record eval results for plotting\n    evals_result = {} \n\n    parameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n     }\n    start = time.time()\n    model = lgb.train(parameters,\n                       train_data,\n                       valid_sets = [train_data, test_data],\n                       num_boost_round = 500,\n                       evals_result = evals_result,\n                       early_stopping_rounds = 100, \n                       verbose_eval = 25)\n    print('model training costs %.2f seconds'%(time.time() - start))\n    \n    plt.rcParams['figure.facecolor'] = 'white'\n    ax = lgb.plot_metric(evals_result, metric = 'auc')\n    plt.title('LightGBM Learning Curve')\n    plt.show()\n    \n    y_pred_val = model.predict(x_val.values, num_iteration = model.best_iteration)\n    val_roc = metrics.roc_auc_score(y_val, y_pred_val)\n    print('Out of folds auc roc score is {:.4f}'.format(val_roc))\n\n    y = model.predict(x_test.values, num_iteration = model.best_iteration)  \n    output = pd.DataFrame({'id': test['id'], 'target': y})\n    output.to_csv('submission_lgb.csv', index = False)\n    return output\n\n\ntrain, test = read_data()\noutput_lgb = run_lgb()\noutput_lgb","ae0b5ac9":"def plot_catboost(y_label):\n    '''plot catboost learning curve\n    '''\n    learn_error = pd.read_csv('.\/catboost_info\/learn_error.tsv', sep='\\t')\n    test_error = pd.read_csv('.\/catboost_info\/test_error.tsv', sep='\\t')\n    metric = pd.concat([learn_error, test_error.iloc[:,1]], axis=1)\n    metric.columns = ['iterations','train','valid']\n    plt.rcParams['figure.facecolor'] = 'white'\n    metric.plot(x='iterations',y=['train','valid'])\n    plt.ylabel(y_label)\n    plt.grid()\n    plt.show()\n\ndef run_cat():\n    '''train a catboost\n    '''\n    x_test = test.drop('id', axis = 1)\n    train0 = train.fillna('nan')\n    x_test0 = x_test.fillna('nan')\n    \n    categorical_features = [col for col in train.columns if 'cat' in col]\n    categorical_features.remove('ps_car_11_cat')\n    #feature = train0[categorical_features]\n    label_encoder = LabelEncoder()\n\n    for col in categorical_features:\n        train0[col]= label_encoder.fit_transform(train0[col]) \n        x_test0[col]= label_encoder.fit_transform(x_test0[col]) \n        \n    y = train0['target'].values\n    x = train0.drop(['id', 'target'], axis = 1)\n    \n    \n    x_train, x_val, y_train, y_val = train_test_split(x, y, \n                                                      test_size = 0.2, \n                                                      random_state = 42, \n                                                      stratify = y)\n    \n    model = CatBoostClassifier(iterations = 500,\n                              learning_rate = 0.02,\n                              depth = 10,\n                              eval_metric = 'AUC',\n                              random_seed = 42,\n                              bagging_temperature = 0.2,\n                              od_type = 'Iter',\n                              metric_period = 50,\n                              od_wait = 20)\n    \n    start = time.time()\n    model.fit(x_train, y_train,\n                 eval_set = (x_val, y_val),\n                 use_best_model = True,\n                 cat_features = categorical_features,\n                 verbose = 50)\n    \n    plot_catboost('AUC')\n    \n    print('model training costs %.2f seconds'%(time.time() - start))\n    y_pred_val = model.predict_proba(x_val)\n    val_roc = metrics.roc_auc_score(y_val, y_pred_val[:,1])\n    print('Out of folds auc roc score is {:.4f}'.format(val_roc))\n    \n    y = model.predict_proba(x_test0)  \n    output = pd.DataFrame({'id': test['id'], 'target': y[:,1]})\n    output.to_csv('submission_cat.csv', index = False)\n    return output\n\noutput_cat = run_cat()\noutput_cat","517842bc":"\ndef encoder_data(train, onehot = True):\n    '''convert categorical feature to one-hot encoding\n    '''\n    if onehot:\n        categorical_features = [col for col in train.columns if 'cat' in col]\n        categorical_features.remove('ps_car_11_cat')\n        feature = train[categorical_features]\n        label_encoder = LabelEncoder()\n\n        for col in categorical_features:\n            train[col]= label_encoder.fit_transform(train[col]) \n    #print('categorical features:',categorical_features)\n    \n        onehotencoder = OneHotEncoder()\n        for col in categorical_features:\n            X = onehotencoder.fit_transform(train[col].values.reshape(-1,1)).toarray()\n            col_name = [col + str(int(i)) for i in range(X.shape[1])]\n            dfOneHot = pd.DataFrame(X, columns = col_name) \n            train.drop(col, axis=1 ,inplace =True) \n            train = pd.concat([train, dfOneHot], axis=1)\n    else:\n        pass\n    print('Our data has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n    return train","6f0075ef":"def run_xgb():\n    '''train a xgboost model\n    '''\n    y = train['target'].values\n    x = train.drop(['id', 'target'], axis = 1)\n    x_test = test.drop('id', axis = 1)\n    x = np.array(x)\n    y = np.array(y)\n    x_train, x_val, y_train, y_val = train_test_split(x, y, \n                                                      test_size = 0.2, \n                                                      random_state = 42, \n                                                      stratify = y)\n    parameters = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    #'is_unbalance': 'true',\n    'learning_rate': 0.05,\n    #'verbose': 0,\n    #'max_depth': 5,\n    #'gamma':10,\n    #'min_child_weight': 10,\n    #'reg_alpha': 8,\n    #'reg_lambda': 1.3\n    }   \n    train_data = xgb.DMatrix(x_train, label = y_train)\n    test_data = xgb.DMatrix(x_val, label = y_val)\n    eval_sets = [(train_data, 'train'), (test_data, 'eval')]\n    evals_result = {} \n    start = time.time()\n    model = xgb.train(parameters,\n                       train_data,\n                       evals = eval_sets,\n                       num_boost_round = 500,\n                       evals_result = evals_result,\n                       early_stopping_rounds = 100, \n                       verbose_eval = 25)\n    print('model training costs %.2f seconds'%(time.time() - start))\n    y_pred_val = model.predict(xgb.DMatrix(x_val))\n    val_roc = metrics.roc_auc_score(y_val, y_pred_val)\n    print('Out of folds auc roc score is {:.4f}'.format(val_roc))\n    \n    plt.rcParams['figure.facecolor'] = 'white'\n    ax = lgb.plot_metric(evals_result, metric = 'auc')\n    plt.title('Xgboost Learning Curve')\n    plt.show()\n    \n    y = model.predict(xgb.DMatrix(x_test.values)) \n    output = pd.DataFrame({'id': test['id'], 'target': y})\n    output.to_csv('submission_xgb.csv', index = False)\n    return output","f8f40402":"train1 = encoder_data(train,onehot = False)\ntest1 = encoder_data(test,onehot = False)\noutput_xgb = run_xgb()\noutput_xgb","0b4b70c9":"# Xgboost","d925bb77":"[Xgboost Parameters](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#learning-task-parameters)","1f5346dd":"# CatBoost\n\nhttps:\/\/catboost.ai\/","671b892a":"# LightGBM\n\n[LightGBM Parameters](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html)","0511bad2":"<!---\nloss_values = evals_result['train']['auc']\nval_loss_values = evals_result['eval']['auc']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'b', color = 'navy',label = 'accuracy')\nplt.plot(epochs, val_loss_values, 'b', color = 'maroon', label = 'val_accuracy')\nplt.title('Xgboost Learning Curve')\nplt.xlabel('Iterations')\nplt.ylabel('auc')\nplt.legend()\nplt.show()\n--->"}}