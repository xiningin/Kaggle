{"cell_type":{"92797ada":"code","2df7a950":"code","228494af":"code","b8697c74":"code","be735c7a":"code","cd9aed43":"code","877ea7aa":"code","63f277d7":"code","ad65ae04":"code","7caaea0e":"code","2da612ac":"code","e802a9d5":"code","0c9ac18e":"code","f6438780":"code","8453ded7":"markdown","3497155e":"markdown","5a996d66":"markdown","36b56208":"markdown","89006cea":"markdown","a86cd6d7":"markdown","efc84b7c":"markdown","a26326d8":"markdown","aa499639":"markdown","01852322":"markdown","191b6703":"markdown","6c2d7391":"markdown","9b8a757c":"markdown","a6f6c407":"markdown","5a61dfd3":"markdown","f682b9a3":"markdown","4dac5bf5":"markdown","9abf45e3":"markdown","4f1e2802":"markdown","47af57f8":"markdown"},"source":{"92797ada":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2df7a950":"import matplotlib.pyplot as plt\nimport seaborn as sns","228494af":"data = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndata.head()","b8697c74":"data.info()","be735c7a":"data.describe()","cd9aed43":"plt.subplots(figsize =(15,9))\nsns.heatmap(data.corr(),cmap=\"Greens\",annot = True)\nplt.show()","877ea7aa":"sns.countplot(x = \"target\", data = data)\ndata.loc[:,\"target\"].value_counts()","63f277d7":"##### from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nx_data,y = data.loc[:,data.columns != 'target'], data.loc[:,'target']\nx_train, x_test, y_train, y_test = train_test_split(x_data,y,test_size = 0.3, random_state =42)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train,y_train)\n\nprint(\" Accuracy result for K = 3: \",knn.score(x_test,y_test))\n","ad65ae04":"# Model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of K\nfor i, K in enumerate(neig):\n    # K from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors = K)\n    # Fit with KNN\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize = [13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","7caaea0e":"from sklearn.linear_model import LogisticRegression\n\n# normalization\nx = (x_data - np.min(x_data)\/(np.max(x_data) - np.min(x_data)))\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_data,y = data.loc[:,data.columns != 'target'], data.loc[:,'target']\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 42)\n\nlr = LogisticRegression(random_state = 42)\nlr.fit(x_train,y_train)\n\nprint(\"Accuracy result of Logistic Regression is {} \".format(lr.score(x_test,y_test)))","2da612ac":"from sklearn.svm import SVC\n\nsvm = SVC(random_state = 1)\n\nsvm.fit(x_train,y_train)\n\nprint(\"Accuracy result of SVM is : \",svm.score(x_test,y_test))","e802a9d5":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nprint(\"Accuracy result of Naive Bayes is \",nb.score(x_test,y_test))","0c9ac18e":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"Accuracy result of Decision Tree Classifier is \",dt.score(x_test,y_test))\n\ny_pred = dt.predict(x_test)\ny_true = y_test\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_pred,y_true)\n\n# we can visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","f6438780":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\n\nprint(\"Accuracy result of Random Forest Classification is \",rf.score(x_test,y_test))\n\ny_pred = rf.predict(x_test)\ny_true = y_test\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidths = 0.5,linecolor = \"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","8453ded7":"# Random Forest Classification\n* Random Forest is one of the popular machine learning models because it gives good results even without hyper parameter estimation and is applicable to both regression and classification problems.\n","3497155e":"* Let's read our data and look into it with data.head() function.\n* data.head() function allows us to see the first 5 lines.\n","5a996d66":"# Logistic Regression\n  * Logistic Regression is a regression method for classifying operations. It is used for classification of categorical or numerical data. It works only if the dependent variable, that is, the result can only take 2 different values.","36b56208":"#   K-NEAREST NEIGHBORS (KNN)\n    * KNN: Look at the K closest labeled data points.\n    * Classification method.\n    * First we need to train our data. Train = fit\n    * fit(): fits the data, train the data.\n    * Let's show it.\n","89006cea":"As you can see:\n\n    * length:303(range index)\n    * 14 columns have values. As you can see they are non-null.\n    * 13 columns have values which are integer.\n    * 1 column has value which is float.","a86cd6d7":"# Supervised Learning\n* a series of inputs and corresponding outputs are given to the system\n* the system produces a function that provides that situation with techniques such as regression.","efc84b7c":"* According to result, the best K value is 10.\n* Now let's find other accuracy results with another classification methods.","a26326d8":"* We found accuracy result for K value.\n* Well,how can we find the best K value? The answer is in model complexity.","aa499639":" * The \"describe()\" function allows us to see the statistical summary of the columns.So let's look it.","01852322":"* Now we will do correlation of columns.\n* Then we will explain:\"What is Correlation?\".**","191b6703":"# Naive Bayes Classification\n* The way the algorithm works calculates the probability of each state for an element and classifies it based on the highest probability value.","6c2d7391":"Now it's time to explain somethings:\n    \n    * Dark tones represent positive correlation.\n    * Light tones represent negative correlation.\n    \n    \n    * So let's compare somethings:\n        * Slope has positive correlation with thalach.\n        * Slope has negative correlation with oldpeak.\n \n","9b8a757c":"* We can learn how many men and how many women.","a6f6c407":"  **Comparison of Classification models**\n\n* KNN Model: 0.6923\n* Logistic Regression Model: 0.8131\n* Supporting Vector Machine (SVM) Model: 0.7032\n* Naive Bayes Classification Model: 0.8351\n* Decision Tree Classification Model: 0.7472\n* Random Forest Classification Model: 0.8131\n\n        * As you can see the most suitable model for our dataset is Naive Bayes Classification Model.\n        * I look forward to your appreciation and criticism.\n        * I apologize for any misspellings, if any.\n        \n","5a61dfd3":"* Is there any NaN value and length of this data? So lets look at info.","f682b9a3":"# EXPLORATORY DATA ANALYSIS (EDA)\n* It is a method for analyzing data sets to summarize its basic features, usually by visual methods.\n","4dac5bf5":"* Firstly we need to import libraries which are exactly necessary for us.\n* These are matplotlib.pylot and seaborn libraries.\n* matplotlib.pyplot for graphic drawings.\n* seaborn for visualization.","9abf45e3":"# Decision Tree Classification\n* Working logic creates more than one decision tree. When it will produce a result, the average value in these decision trees is taken and the result is produced.\n* Also I'll use Confusion Matrix. What is Confusion Matrix?\n    * Where the output is two or more, machine learning classification problems exist to measure performance. It is a table with 4 different combinations with predicted and real values.","4f1e2802":"I explained the Supervised Learning and EDA in a easy words. Now it's time to code.","47af57f8":"# Support Vector Machine (SVM)\n* It is named as the right decision line that divides the data set into two. Although it is possible to draw infinite decision lines, the important thing is to determine the optimal, that is, the most appropriate decision line."}}