{"cell_type":{"9ffdf7d9":"code","e025caa5":"code","b5855ad4":"code","43ba66ff":"code","419c20ab":"code","a97adb81":"code","0a8fded3":"code","d60d6b9c":"code","9b70037a":"code","a58da17d":"code","81e03685":"code","11ed66f4":"code","5c5a9f11":"code","f9bc331f":"code","04015722":"code","cc998de4":"code","35c4d35b":"code","fbcc134b":"code","812c498a":"code","67b9916a":"code","f2db43fd":"code","4d91a138":"code","22939755":"code","08d596bc":"code","026e3d2e":"code","ac10215c":"code","e6e712e2":"code","3e0db874":"code","77078d1d":"code","dbccbe55":"code","6293caed":"code","bf03f718":"code","60eed366":"code","6419d5c6":"code","8ad48021":"code","5f0ee899":"code","65477f66":"code","81453ed1":"code","99e72675":"markdown","5241dea8":"markdown","15177540":"markdown","975ce751":"markdown","6714c8bf":"markdown","9e2dff03":"markdown","934ce701":"markdown","19bfea1f":"markdown","b8d42059":"markdown","61f1f016":"markdown","4a15990d":"markdown","2de4d501":"markdown"},"source":{"9ffdf7d9":"# importing library\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#reading the csv file into a dataframe\nted = pd.read_csv('..\/input\/ted-dataset\/ted.csv')\n\n# checking the 1st 5 rows of the data \nted.head()","e025caa5":"# checking the shape of the data (rows, columns)\nted.shape","b5855ad4":"# cheacking data types\nted.dtypes","43ba66ff":"# checking missing values\nted.isna().sum()","419c20ab":"ted.sort_values('comments').tail()","a97adb81":"# which talk have more comments per views\nted['comments_per_views'] = ted.comments \/ ted.views\nted.sort_values('comments_per_views').tail()","0a8fded3":"# which talk have les views per comment\nted['views_per_comments'] = ted.views \/ ted.comments\nted.sort_values('views_per_comments').head()","d60d6b9c":"ted.comments.plot(kind='hist')","9b70037a":"# filtering talks with more than or equal to 1000 comments shows that only 32 talks out of 2500\nted[ted.comments >= 1000].shape","a58da17d":"# make sense to exclude talks with 1000 comments or more from the plot to get more readiable and informative plot\nted[ted.comments < 1000].comments.plot(kind='hist')","81e03685":"# the code line below does the same thing as the cell code above but with using query method\nted.query('comments < 1000').comments.plot(kind='hist')","11ed66f4":"# another way is using loc which is more flixable + increasing the number of bis in the histogram to see more details (default bins is 10)\nted.loc[ted.comments < 1000, 'comments'].plot(kind='hist', bins=20)","5c5a9f11":"# take 10 random samples of the event column\nted.event.sample(10)","f9bc331f":"# look at the film_date colum, from ted website we know that the value in film_date is unix timestamp\nted.film_date.head()","04015722":"#change the value in film_date to be readiable with pd.to_datetime() and put it in new colum\nted['film_datetime'] = pd.to_datetime(ted.film_date, unit='s')\n\n#check with sample method (taking random samples) to see if it worked\nted[['event','film_datetime']].sample(5)","cc998de4":"# check the dtype for the column film_datetime\nted.dtypes","35c4d35b":"# select the year from the column film_datetime (its cool to check dt options as well) then count the values and after that plot it\nted.film_datetime.dt.year.value_counts().plot()","fbcc134b":"# the problem with the plot above is the order of the data. the plot will graph \n#it as it shows without ordering them and to fix that we \n#need to order the output before poltting it. notice that the index in the output for ted.film_datetime.dt.year.value_counts() line is the year \n\nted.film_datetime.dt.year.value_counts().sort_index().plot()\n","812c498a":"ted.event.value_counts().head()","67b9916a":"# count the talks in each event and show the mean of views \nted.groupby('event').views.agg(['count', 'mean']).sort_values('mean').tail()","f2db43fd":"# the results from above shows TEDxPuget Sound thas the largest mean values but also shows it has 1 talk\n# only. that does not reflect that this talk is what should be the best talk\n# so we add to agg() method the sum of the views and we sorted by sum \nted.groupby('event').views.agg(['count', 'mean','sum']).sort_values('sum').tail()","4d91a138":"# Lets have a look at rating colum\nted.ratings.head()","22939755":"# lets have a look at the 1st line. you can use the code line below or you can use ted.ratings[0]\nted.loc[0,'ratings']","08d596bc":"# what is the data type of ratings column ? is it list of dictionaries ?\ntype(ted.loc[0,'ratings'])","026e3d2e":"# the above shows the dtype is a str and not a list of dictioneries which we can call it a\n# stringfied list of dictioeries. and here we should unpack this data. the egneric way to do this is by \n# using abstract syntax tree module and use the function called literal_eval\n\nimport ast\nast.literal_eval(ted.ratings[0])\n\n","ac10215c":"# now we need to apply this to the entire series\n\nted ['ratings_list'] = ted.ratings.apply(ast.literal_eval)\nted.ratings_list[0]","e6e712e2":"# what we are trying to acomplish is to sum the count. we are going to build a function for that\n# and then we use apply method\n\ndef get_num_ratings(list_of_dicts):\n    num = 0\n    for d in list_of_dicts:\n        num = num + d['count']\n    return num\n\n# check if the function is working correctly\nget_num_ratings(ted.ratings_list[0])","3e0db874":"# use apply method\nted['num_ratings'] = ted.ratings_list.apply(get_num_ratings)","77078d1d":"# another way\npd.DataFrame(ted.ratings_list[0])['count'].sum()","dbccbe55":"ted.ratings_list.head()","6293caed":"# does all talks ratings have 'Funny' rating ?\nted.ratings.str.contains('Funny').value_counts()\n\n#Yes since we have total of 2550 rows and the output of the value_counts() is 2550","bf03f718":"# we will write a function to return the count if funny ratings\n\ndef get_funny_ratings(list_of_dicts):\n    for d in list_of_dicts:\n        if d['name'] == 'Funny':\n            return d['count']\n\n# apply the function\nted['funny_ratings'] = ted.ratings_list.apply(get_funny_ratings)\nted.funny_ratings.head()","60eed366":"ted['funny_ratings_rate'] = ted.funny_ratings \/ ted.num_ratings\nted.funny_ratings_rate.head()","6419d5c6":"ted.sort_values('funny_ratings_rate').speaker_occupation.head(20)","8ad48021":"ted.groupby('speaker_occupation').funny_ratings_rate.mean().sort_values().tail()","5f0ee899":"ted.speaker_occupation.describe()","65477f66":"occupation_counts = ted.speaker_occupation.value_counts()\ntop_occupations = occupation_counts[occupation_counts >= 5].index\nted_top_occupations = ted[ted.speaker_occupation.isin(top_occupations)]\nted_top_occupations.shape","81453ed1":"ted_top_occupations.groupby('speaker_occupation').funny_ratings_rate.mean().sort_values()","99e72675":"### Lessons we learned from the exercise:\n* Read the documentatio (specailly for the dataset\n* Use the datetime data type for dates and time\n* Check your work as you go\n* Consider excluding data if it might not be relevant (are all the talks in the data set are Ted talks? there is a talk on 1972 where Ted was established on 1984)\n\n# 5. what were the \"best\" events in TED history to attend?\nthis question is not asking about best talk but event like TED2006, TED20011 ... etc. and to answer this question we need to figure out how to define \"best\" and use that definition to calculate which event is \"best.\n> Take 8 minutes to think about it\n","5241dea8":"### Lessons we learned from the exercise:\n* Check your assumptions about your data\n* Check wether oyur results are reasonable\n* take advantage of the fact that pandas operations often output a DataFrame or Series data type\n* Watch out for small sample sizes\n* Consider the impact of missing data","15177540":"### Step 2. calculate the percentage of the funny ratings","975ce751":"This notebook is the work through of [Data Science Best Practices with pandas PyCon 2019](https:\/\/www.youtube.com\/watch?v=dPwLlJkSHLo) by Kevin Markham, the founder of [Data school](https:\/\/www.youtube.com\/c\/dataschool\/about).\nthe dataset used in this notebook is TED Talks dataset\n# 1. Introduction to Dataset","6714c8bf":"### Lessons we learned from the exercise:\n* Pay attntion to data types in pandas\n* Use apply method any time it is necessory\n\n# 7. Count the total numbers of the ratings recieved by each talk\ncreate new column num_ratings to hold the count\n\n> Take 10 minutes to think about it\n\n### Bonus:\n* for each talk, calculate the % of ratings that were negative\n* for each talk calcute the avarage number of ratings it recieved per day since it was puplished","9e2dff03":"### Lessons we learned from the exercise:\n* consider the limitation and basis of your data when analyzing it\n* make your results understandable\n\n# 3. visualize the distripution of comments\n> Take 3 minutes to thik about it","934ce701":"# 2. Whitch talks provoke the most online discussion?\n> Take 3 minutes to think about it","19bfea1f":"### Lessons we learned from the exercise:\n* Choose your plot type based on the question you are answering and the data type(s) you are working with\n* Use pandas one- liner to iterate through plots quickly\n* Try modifying the plot defaults \n* Create plots involves decision-making\n\n# 4. plot customization\nplot the number of talks that took place each year\n> Take 12 minutes to think about it","b8d42059":"### Lessons we learned from the exercise:\n* Write your code in small chuncks and check your code as you go\n* Lambda is best for simple functions\n\n# 8. Which occupations deliver the funniest TED talks on avarage ?\n> there is many way to do this, think about it for 15 minutes\n\n### Bonus:\n* for each talk, calculate the most frequent ratings\n* for each talk, clean the occupation data so that there is one occupation per talk\n\n### Step 1: count the number of funny ratings","61f1f016":"### Lessons we learned from the exercise:\n* Think creatively for how you can use the data ou have to answer your questions\n* Watch out for small sample sizes\n\n# 6. Unpack the Rating data","4a15990d":"### Step 3. Analyze the funny rate by occupation","2de4d501":"### Step 4. Focus on occupation that are well-represented in the data"}}