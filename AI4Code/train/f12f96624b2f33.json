{"cell_type":{"93fbe50e":"code","ad38bf63":"code","d1cdfda0":"code","676aa3a4":"code","1b0f507a":"code","e8845bac":"code","8f210e83":"code","527b6ae3":"code","1b356390":"code","886b5697":"code","017274fb":"code","27cbdfbe":"code","1bfb84e7":"code","64839bf7":"code","c9cbd326":"code","e34fafaf":"code","90d156d2":"code","c7afc44f":"code","5e9fdb00":"code","05f51c2b":"code","a71375ee":"code","688301c2":"code","400066c8":"code","15ac116e":"code","92df26ba":"code","6480848a":"code","606a74e8":"code","dd0cbdc9":"code","f5dabdd1":"code","81c84b0a":"code","77fb88a2":"code","338de2b9":"code","d40153eb":"code","5ee0a191":"code","8d1096bf":"code","018f45a6":"code","b0f8be95":"code","08d41c4d":"code","7ae392ed":"code","9935fcfa":"code","0e4ec1bb":"code","04c7b99b":"code","2d37cf2a":"code","5906cefc":"code","c30ce930":"code","e14740ff":"code","16228f7a":"code","7acad7e2":"code","a63a02a9":"code","884b1681":"code","e3e9f121":"code","eef90884":"code","431adfba":"code","6250d45f":"code","8a7b297f":"code","5a3f8d6d":"code","6b19348e":"code","11e9e5ef":"code","f18b5e4e":"code","3cc429c9":"code","e7a5b7cb":"code","373cf81a":"code","ca2d9440":"code","fdc4a023":"code","5304316e":"code","dd7a97e3":"code","76afa187":"code","05d8553c":"code","b9f6bc53":"code","c2b6dafa":"code","929d62fe":"code","096461fe":"code","5f4a6b14":"code","7db36759":"code","17e5c246":"code","a12b1568":"code","16f693b9":"code","4d5d5a87":"code","4c651f38":"code","30d6193c":"code","5f8ffbe4":"code","48122bd4":"code","90998f59":"code","96139a85":"code","a86c3dfd":"code","57ba6aff":"code","c99552b8":"code","f54610fb":"code","a72f46df":"code","92f48a0f":"code","f6eec3cb":"code","3cd0a56e":"code","655c65e0":"code","949269b1":"code","95e58557":"code","ff3cb90f":"code","2c1f6be4":"code","c775f5ce":"code","6a04f176":"code","cfef0e2c":"code","b562286b":"code","021a7885":"code","20ef6743":"code","da9fef9a":"code","fb28320e":"code","99f26b6b":"code","ccc3e491":"code","a735f18c":"code","f9fc928d":"code","d50f62d6":"code","4ce06e5f":"code","5eb1acc8":"code","674d52b4":"code","017e5533":"code","e22fc425":"code","8d35a4ff":"code","f2365714":"code","712a8dfa":"code","60655728":"code","22625ecc":"code","722c992d":"code","e3fa8cea":"code","e47efdf7":"code","fcdbf40e":"code","50437fcb":"code","4dd419b1":"code","bbe6b9eb":"code","1bea5eb4":"code","e3ed2515":"code","b6ddf6f4":"code","22a393ca":"code","15bcd585":"code","060e9bed":"code","93e159a9":"code","4c21f658":"code","72acc6db":"code","7e092df3":"code","377a2fbf":"code","7e013bbe":"code","c8c4dc2e":"code","a0a16c9d":"code","8a373a9d":"code","428108c3":"code","7ff19a46":"code","e037a1ec":"code","5819432d":"code","b8bca415":"code","fb2f7574":"code","b9ac933f":"code","445fde8c":"code","69aa813a":"code","1bd65946":"code","5b3dfadb":"code","43483a9a":"code","dd51395b":"markdown","d0080519":"markdown","6919eaf9":"markdown","33cd47cb":"markdown","942b7591":"markdown","429fdaf6":"markdown","9478663b":"markdown","f8ff0715":"markdown","f93fc10b":"markdown","4bb5810e":"markdown","306e1005":"markdown","d67964d8":"markdown","0621afbb":"markdown","a9772ef8":"markdown","f55fad40":"markdown","34aa8ccb":"markdown","d703b817":"markdown","1b5b2bf5":"markdown","cc06a7bf":"markdown","4c56f315":"markdown","8ba25d0f":"markdown","2c2ba2d8":"markdown","235a75b9":"markdown","bfa899e0":"markdown","33cd44a7":"markdown","554f6518":"markdown","bcfa62b1":"markdown","81976c78":"markdown","fedc637b":"markdown","210cc6a0":"markdown","32137697":"markdown","e5ea23da":"markdown"},"source":{"93fbe50e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ad38bf63":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import mode\n\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.metrics import auc\n\nimport os\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_colwidth',500)\npd.set_option('display.max_columns',5000)\nfrom IPython.display import Image\nimport os\n!ls ..\/input\/\nencoder = LabelEncoder()","d1cdfda0":"train = pd.read_csv('..\/input\/train.csv')\ncampaign = pd.read_csv('..\/input\/campaign_data.csv')\nitems = pd.read_csv('..\/input\/item_data.csv')\ncoupons = pd.read_csv('..\/input\/coupon_item_mapping.csv')\ncust_demo = pd.read_csv('..\/input\/customer_demographics.csv')\ncust_tran = pd.read_csv('..\/input\/customer_transaction_data.csv')\ntest = pd.read_csv('..\/input\/test.csv')","676aa3a4":"train.shape, campaign.shape, items.shape, coupons.shape, cust_demo.shape, cust_tran.shape, test.shape","1b0f507a":"print('Train Dataframe')\nprint(train.isnull().sum())\nprint('======================')\nprint('Campaign Dataframe')\nprint(campaign.isnull().sum())\nprint('======================')\nprint('Items Dataframe')\nprint(items.isnull().sum())\nprint('======================')\nprint('Coupons Dataframe')\nprint(coupons.isnull().sum())\nprint('======================')\nprint('Customer Demographics Dataframe')\nprint(cust_demo.isnull().sum())\nprint('======================')\nprint('Customer Transaction Dataframe')\nprint(cust_tran.isnull().sum())\nprint('======================')\n\nprint(test.isnull().sum())","e8845bac":"train.head()","8f210e83":"train.redemption_status.value_counts(normalize=True)*100","527b6ae3":"value=train['redemption_status'].value_counts().plot(kind='bar')\nplt.ylabel('redemption_status')","1b356390":"cust_demo.head()","886b5697":"cust_demo.info()","017274fb":"cust_demo.marital_status.value_counts()","27cbdfbe":"cust_demo.family_size.value_counts()","1bfb84e7":"    cust_demo.no_of_children.value_counts()","64839bf7":"#The below lines of code is to get rid of the + and keeping 5+ as 5 and 3+ as 3 and converting the columns to int data type.\n#type of family size = int64 ... Cant apply astype as we have 5+ as family size\n#no of children = int64 ... we need to ignore the NaN values while converting to float\ncust_demo['family_size'] = cust_demo.family_size.apply(lambda x: int(re.sub('\\+','',x)))\ncust_demo['no_of_children'] = cust_demo.no_of_children.apply(lambda x: int(re.sub('\\+','',x)) if pd.notna(x) else x)","c9cbd326":"#Filling NaN values for marital_status\n\n#customers with family size =1 will be single\ncust_demo.loc[pd.isnull(cust_demo.marital_status) & (cust_demo.family_size == 1),'marital_status'] = 'Single'\n\n#customers whos family size - no of childrens == 1, will also be single \n#This is applicable where there is only 1 parent --- We treat 1 parent as Single\ncust_demo.loc[(cust_demo.family_size - cust_demo.no_of_children == 1) & pd.isnull(cust_demo.marital_status),'marital_status'] = 'Single'\n\n#from the orignal data we have 186 of 196 customers with diff of 2 in their family size and number of childrens as\n#Married (see the below cell) and hence where ever the difference is 2 and marital status is NaN and No of Children is \n#NaN we impute the Mariatl Status with Married\ncust_demo.loc[(pd.isnull(cust_demo.marital_status)) & ((cust_demo.family_size - cust_demo.no_of_children) == 2)  \n              & (pd.notnull(cust_demo.no_of_children)),'marital_status'] = 'Married'\n\n#original data shows customers with fam size == 2, and NaN in no of childrens are majorly Married (see below cell skipping 1 cell)\ncust_demo.loc[pd.isnull(cust_demo.marital_status) & (pd.isnull(cust_demo.no_of_children)) \n              & (cust_demo.family_size ==2),'marital_status'] = 'Married'","e34fafaf":"a = cust_demo.marital_status.groupby((cust_demo.family_size - cust_demo.no_of_children) == 2).value_counts()\nprint(a[True])","90d156d2":"#b = cust_demo.marital_status.groupby((cust_demo.family_size) == 2 & pd.isnull(cust_demo.no_of_children)).value_counts()","c7afc44f":"cust_demo.marital_status.isnull().sum()","5e9fdb00":"#FillingNaN values for no of children\n\n#Married people with family_size ==2 will have 0 childrens\ncust_demo.loc[pd.isnull(cust_demo.no_of_children) & (cust_demo.marital_status == 'Married') & (cust_demo.family_size == 2),'no_of_children'] = 0\n\n#customers with family size 1 will have zero childrens\ncust_demo.loc[pd.isnull(cust_demo.no_of_children) & (cust_demo.family_size == 1), 'no_of_children'] = 0\n\n#singles with family size == 2, will probably have 1 child\ncust_demo.loc[pd.isnull(cust_demo.no_of_children) & (cust_demo.family_size == 2),'no_of_children'] = 1\n\ncust_demo['no_of_children']=cust_demo['no_of_children'].astype(np.int64)","05f51c2b":"cust_demo.no_of_children.isnull().sum()","a71375ee":"cust_demo.info()","688301c2":"#Label Encoding Marital Status --- 0 is Single and 1 is Married\ncust_demo[\"marital_status\"] = encoder.fit_transform(cust_demo[\"marital_status\"])","400066c8":"# Label Encoding age_range ... 18-25 is 0, 26-35 is 1, 36-45 is 2, 46-55 is 3, 56-70 is 4 and 70+ is 5\ncust_demo[\"age_range\"] = encoder.fit_transform(cust_demo[\"age_range\"])","15ac116e":"cust_demo.head()","92df26ba":"campaign.head()","6480848a":"campaign.info()","606a74e8":"campaign.campaign_type.value_counts()","dd0cbdc9":"#Label Encoding Campaign type\ncampaign[\"campaign_type\"] = encoder.fit_transform(campaign.campaign_type)","f5dabdd1":"#Converting the date columns to date time\ncampaign['start_date'] = pd.to_datetime(campaign['start_date'], format = '%d\/%m\/%y')\ncampaign['end_date'] = pd.to_datetime(campaign['end_date'], format = '%d\/%m\/%y')","81c84b0a":"#Creating a new column campaign_duration\ncampaign[\"campaign_duration\"] = campaign[\"end_date\"] - campaign[\"start_date\"]\ncampaign[\"campaign_duration\"] = campaign[\"campaign_duration\"].apply(lambda x: x.days) ","77fb88a2":"campaign.head()","338de2b9":"cust_tran.head()","d40153eb":"cust_tran.info()","5ee0a191":"#Converting the date column into date time\n#Reset the index of the DataFrame, and use the default one instead.\n#If the DataFrame has a MultiIndex, this method can remove one or more levels.\ncust_tran['date'] = pd.to_datetime(cust_tran['date'])\ncust_tran = cust_tran.sort_values('date').reset_index(drop=True)","8d1096bf":"cust_tran.head()","018f45a6":"#Creating 3 new columns from the date column\ncust_tran['day'] = cust_tran[\"date\"].apply(lambda x: x.day)\ncust_tran['dow'] = cust_tran[\"date\"].apply(lambda x: x.weekday())\ncust_tran['month'] = cust_tran[\"date\"].apply(lambda x: x.month)","b0f8be95":"cust_tran.head()","08d41c4d":"#Given selling_price and other_discount are for the entire transaction. Hence getting the Actual value of the transaction.\ncust_tran.selling_price = cust_tran.selling_price\/cust_tran.quantity\ncust_tran.other_discount = cust_tran.other_discount\/cust_tran.quantity\ncust_tran.selling_price = cust_tran.selling_price - cust_tran.other_discount","7ae392ed":"#Inserting a new column to know if the coupon was used or not\ncust_tran['coupon_used'] = cust_tran.coupon_discount.apply(lambda x: 1 if x !=0 else 0)","9935fcfa":"cust_tran.head()","0e4ec1bb":"items.head()","04c7b99b":"items.brand_type.value_counts()","2d37cf2a":"items.category.value_counts()","5906cefc":"#Label Encoding the brand_type and category columns\nitems.brand_type = encoder.fit_transform(items[\"brand_type\"])\nitems.category = encoder.fit_transform(items[\"category\"])","c30ce930":"items.head()","e14740ff":"coupons.head()","16228f7a":"Image(\"..\/input\/Schema.png\")","7acad7e2":"coupons_items = pd.merge(coupons, items, on=\"item_id\", how=\"left\")","a63a02a9":"coupons_items.head()","884b1681":"cust_tran.head()","e3e9f121":"# Aggregate transactions by item_id by mean for a particular customer\ntransactions1 = pd.pivot_table(cust_tran, index = \"item_id\", \n               values=['customer_id','quantity','selling_price', 'other_discount','coupon_discount','coupon_used'],\n               aggfunc={'customer_id':lambda x: len(set(x)),\n                        'quantity':np.mean,\n                        'selling_price':np.mean,\n                        'other_discount':np.mean,\n                        'coupon_discount':np.mean,\n                        'coupon_used': np.sum\n                        } )\ntransactions1.reset_index(inplace=True)\ntransactions1.rename(columns={'customer_id': 'no_of_customers'}, inplace=True)","eef90884":"transactions1.head()","431adfba":"# Aggregate transactions by item_id by sum for a particular customer\ntransactions2 = pd.pivot_table(cust_tran, index = \"item_id\", \n               values=['customer_id','quantity','selling_price', 'other_discount','coupon_discount'],\n               aggfunc={'customer_id':len,\n                        'quantity':np.sum,\n                        'selling_price':np.sum,\n                        'other_discount':np.sum,\n                        'coupon_discount':np.sum,\n                        } )\ntransactions2.reset_index(inplace=True)\ntransactions2.rename(columns={'customer_id': 't_counts', 'quantity':'qu_sum',\n                             'selling_price':'price_sum', 'other_discount':'od_sum',\n                             'coupon_discount':'cd_sum'}, inplace=True)","6250d45f":"transactions2.head()","8a7b297f":"transactions1 = pd.merge(transactions1, transactions2, on='item_id',how='left' )","5a3f8d6d":"transactions1['total_discount_mean'] = transactions1['coupon_discount'] + transactions1['other_discount']\ntransactions1['total_discount_sum'] = transactions1['od_sum'] + transactions1['cd_sum']\ntransactions1.head()","6b19348e":"item_coupon_trans = pd.merge(coupons_items, transactions1, on='item_id', how='left')","11e9e5ef":"item_coupon_trans.head()","f18b5e4e":"item_coupon_trans.columns","3cc429c9":"coupon = pd.pivot_table(item_coupon_trans, index =\"coupon_id\",\n                         values=[ 'item_id', 'brand', 'brand_type', 'category',\n       'coupon_discount', 'coupon_used', 'no_of_customers', 'other_discount',\n       'quantity', 'selling_price', 'cd_sum', 't_counts', 'od_sum', 'qu_sum',\n       'price_sum', 'total_discount_mean', 'total_discount_sum'],\n              aggfunc={'item_id':lambda x: len(set(x)),\n                       'brand':lambda x: mode(x)[0][0],\n                       'brand_type':lambda x: mode(x)[0][0],\n                       'category':lambda x: mode(x)[0][0],\n                       'coupon_discount':np.mean,\n                       'no_of_customers':np.mean,\n                       'other_discount':np.mean,\n                       'quantity':np.mean,\n                       'selling_price':np.mean,\n                      'coupon_used': np.sum,\n                       'cd_sum': np.sum,\n                       't_counts': np.sum,\n                       'od_sum': np.sum,\n                       'qu_sum': np.sum,\n                       'price_sum': np.sum,\n                       'total_discount_mean': np.mean,\n                       'total_discount_sum': np.sum\n                      })\ncoupon.reset_index(inplace=True)","e7a5b7cb":"coupon.rename(columns={'item_id':'item_counts'}, inplace=True)","373cf81a":"coupon.head()","ca2d9440":"# Aggregate transactions by customer_id\ntransactions3 = pd.pivot_table(cust_tran, index = \"customer_id\", \n               values=['item_id','quantity','selling_price', 'other_discount','coupon_discount','coupon_used','day','dow','month'],\n               aggfunc={'item_id':lambda x: len(set(x)),\n                        'quantity':np.mean,\n                        'selling_price':np.mean,\n                        'other_discount':np.mean,\n                        'coupon_discount':np.mean,\n                        'coupon_used': np.sum,\n                        'day':lambda x: mode(x)[0][0],\n                        'dow':lambda x: mode(x)[0][0],\n                        'month':lambda x: mode(x)[0][0]}\n              )\ntransactions3.reset_index(inplace=True)\ntransactions3.rename(columns={'item_id': 'no_of_items'}, inplace=True)\ntransactions3.head()","fdc4a023":"# Aggregate transactions by customer_id by sum\ntransactions4 = pd.pivot_table(cust_tran, index = \"customer_id\", \n               values=['item_id','quantity','selling_price', 'other_discount','coupon_discount'],\n               aggfunc={'item_id':len,\n                        'quantity':np.sum,\n                        'selling_price':np.sum,\n                        'other_discount':np.sum,\n                        'coupon_discount':np.sum}\n              )\ntransactions4.reset_index(inplace=True)\ntransactions4.rename(columns={'item_id': 'customer_id_count','quantity':'qa_sum','selling_price':'pprice_sum',\n                             'other_discount':'odd_sum','coupon_discount':'cdd_sum'  }, inplace=True)\ntransactions4.head()","5304316e":"transactions = pd.merge(transactions3, transactions4, on='customer_id', how='left')\ntransactions.head()","dd7a97e3":"def merge_all(df): \n    df=  pd.merge(df, coupon, on=\"coupon_id\", how=\"left\")\n    df = pd.merge(df, campaign, on=\"campaign_id\", how=\"left\")\n    df = pd.merge(df, cust_demo, on=\"customer_id\", how=\"left\")\n    df = pd.merge(df, transactions, on='customer_id', how='left')\n    return df","76afa187":"train = merge_all(train)\ntest = merge_all(test)","05d8553c":"train.shape, test.shape","b9f6bc53":"## To save the final file after merging the data\n##train.to_csv('FinalData.csv')","c2b6dafa":"train.isnull().sum()","929d62fe":"test.isnull().sum()","096461fe":"def deal_na(df):\n    for col in cust_demo.columns.tolist()[1:]:\n        df[col].fillna(mode(df[col]).mode[0], inplace=True)\n    return df\n\ntrain = deal_na(train)\ntest = deal_na(test)","5f4a6b14":"train.isnull().sum()","7db36759":"test.isnull().sum()","17e5c246":"test_id = test['id']\ntarget = train['redemption_status']\ntrain.drop(['id','campaign_id','start_date','end_date', 'redemption_status'], axis=1, inplace=True)\ntest.drop(['id','campaign_id','start_date','end_date'], axis=1, inplace=True)","a12b1568":"train.head()","16f693b9":"train.columns","4d5d5a87":"train.shape","4c651f38":"target","30d6193c":"x = pd.DataFrame(train)\ny = target\ncol_names = ['cd_sum','coupon_discount_x', 'coupon_used_x', 'item_counts', 'no_of_customers',\n       'od_sum', 'other_discount_x', 'price_sum', 'qu_sum', 'quantity_x',\n       'selling_price_x', 't_counts', 'total_discount_mean',\n       'total_discount_sum', 'campaign_type', 'campaign_duration',\n        'family_size', 'no_of_children',\n       'income_bracket', 'coupon_discount_y', 'coupon_used_y',\n       'no_of_items', 'other_discount_y', 'quantity_y',\n       'selling_price_y', 'cdd_sum', 'customer_id_count', 'odd_sum', 'qa_sum',\n       'pprice_sum']\nfeatures = x[col_names]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\nx[col_names] = features\nx = np.array(x)","5f8ffbe4":"x","48122bd4":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=2439)","90998f59":"x_train.shape,y_train.shape,x_test.shape,y_test.shape","96139a85":"LR = LogisticRegression()\nLR.fit(x_train,y_train)\ny_pred_LR = LR.predict(x_test)\nprint(classification_report(y_test,y_pred_LR))","a86c3dfd":"roc_auc_score(y_test,y_pred_LR)\nModel = ['Logistic Regression']\nROC_AUC_Accuracy = [roc_auc_score(y_test,y_pred_LR)]","57ba6aff":"from sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \nresults=confusion_matrix(y_test,y_pred_LR)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_LR) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_LR) )","c99552b8":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_LR ):\n    cm = metrics.confusion_matrix( y_test,y_pred_LR )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","f54610fb":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_LR )\n\nprint(\"confusion matrix = \\n\",mat_pruned)","a72f46df":"def create_conf_mat(y_test,y_pred_LR):\n    if (len(y_test.shape) != len(y_pred_LR.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_LR.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_LR)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","92f48a0f":"    conf_mat = create_conf_mat(y_test,y_pred_LR)\n    sns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Actual Values')\n    plt.title('Actual vs. Predicted Confusion Matrix')\n    plt.show()","f6eec3cb":"#Get predicted probabilites\ntarget_probailities_log = LR.predict_proba(x_test)[:,1]","3cd0a56e":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","655c65e0":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","949269b1":"params = {\n    \n    'n_neighbors': range(1,5),\n    'weights': ['uniform','distance'],\n    'algorithm': ['ball_tree','kd_tree','auto'],\n    'p': [1,2,3]\n}\n\nknn = KNeighborsClassifier()\n\nrs = RandomizedSearchCV(estimator=knn,n_jobs=-1,cv=3,param_distributions=params,scoring='recall')\nrs.fit(x,y)","95e58557":"knn = KNeighborsClassifier(**rs.best_params_)\nknn.fit(x_train,y_train)\ny_pred_knn = knn.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_knn))\nModel.append('k-Nearest-Neighbours')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_knn))","ff3cb90f":"print(classification_report(y_test,y_pred_knn))","2c1f6be4":"Model,ROC_AUC_Accuracy","c775f5ce":"results=confusion_matrix(y_test,y_pred_knn)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_knn) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_knn) )\n","6a04f176":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_knn ):\n    cm = metrics.confusion_matrix( y_test,y_pred_knn )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n","cfef0e2c":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_knn )\n\nprint(\"confusion matrix = \\n\",mat_pruned)\n","b562286b":"def create_conf_mat(y_test,y_pred_knn):\n    if (len(y_test.shape) != len(y_pred_knn.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_knn.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_knn)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb\n","021a7885":"conf_mat = create_conf_mat(y_test,y_pred_knn)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","20ef6743":"#Get predicted probabilites\ntarget_probailities_log = knn.predict_proba(x_test)[:,1]","da9fef9a":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","fb28320e":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()\n","99f26b6b":"nb = GaussianNB()\nnb.fit(x_train,y_train)\ny_pred_nb = nb.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_nb))\nModel.append('Naive Bayes')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_nb))","ccc3e491":"print(classification_report(y_test,y_pred_nb))","a735f18c":"Model,ROC_AUC_Accuracy","f9fc928d":"results=confusion_matrix(y_test,y_pred_nb)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_nb) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_nb) )","d50f62d6":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_nb ):\n    cm = metrics.confusion_matrix( y_test,y_pred_nb )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n","4ce06e5f":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_nb )\n\nprint(\"confusion matrix = \\n\",mat_pruned)\n","5eb1acc8":"def create_conf_mat(y_test,y_pred_nb):\n    if (len(y_test.shape) != len(y_pred_nb.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_nb.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_nb)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb\n","674d52b4":"conf_mat = create_conf_mat(y_test,y_pred_nb)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()\n","017e5533":"#Get predicted probabilites\ntarget_probailities_log = nb.predict_proba(x_test)[:,1]","e22fc425":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","8d35a4ff":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()\n","f2365714":"Model,ROC_AUC_Accuracy","712a8dfa":"params = {\n    \n    'criterion':['gini','entropy'],\n    'splitter':['best','random'],\n    'max_depth':range(1,10),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\n\ndt = DecisionTreeClassifier()\n\nrs = RandomizedSearchCV(estimator=dt,n_jobs=-1,cv=3,param_distributions=params,scoring='recall')\nrs.fit(x,y)","60655728":"dt = DecisionTreeClassifier(**rs.best_params_)\ndt.fit(x_train,y_train)\ny_pred_dt = dt.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_dt))\nModel.append('Decision Tree')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_dt))","22625ecc":"print(classification_report(y_test,y_pred_dt))","722c992d":"Model,ROC_AUC_Accuracy","e3fa8cea":"results=confusion_matrix(y_test,y_pred_dt)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_dt) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_dt) )","e47efdf7":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_dt):\n    cm = metrics.confusion_matrix( y_test,y_pred_dt )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n","fcdbf40e":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_dt )\n\nprint(\"confusion matrix = \\n\",mat_pruned)\n","50437fcb":"def create_conf_mat(y_test,y_pred_dt):\n    if (len(y_test.shape) != len(y_pred_dt.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_dt.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_dt)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","4dd419b1":"conf_mat = create_conf_mat(y_test,y_pred_dt)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","bbe6b9eb":"#Get predicted probabilites\ntarget_probailities_log = dt.predict_proba(x_test)[:,1]","1bea5eb4":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","e3ed2515":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","b6ddf6f4":"Model,ROC_AUC_Accuracy","22a393ca":"params = {\n    \n    'n_estimators':range(10,100,10),\n    'criterion':['gini','entropy'],\n    'max_depth':range(2,10,1),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\n\nrf = RandomForestClassifier()\n\nrs = RandomizedSearchCV(estimator=rf,param_distributions=params,cv=5,scoring='recall',n_jobs=-1)\nrs.fit(x,y)","15bcd585":"rf = RandomForestClassifier(**rs.best_params_)\nrf.fit(x_train,y_train)\ny_pred_rf = rf.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_rf))\nModel.append('Random Forest')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_rf))","060e9bed":"print(classification_report(y_test,y_pred_rf))","93e159a9":"results=confusion_matrix(y_test,y_pred_rf)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_rf) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_rf) )\n","4c21f658":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_rf ):\n    cm = metrics.confusion_matrix( y_test,y_pred_rf )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n","72acc6db":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_rf )\nprint(\"confusion matrix = \\n\",mat_pruned)\n","7e092df3":"def create_conf_mat(y_test,y_pred_rf):\n    if (len(y_test.shape) != len(y_pred_rf.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_rf.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_rf)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb\n","377a2fbf":"conf_mat = create_conf_mat(y_test,y_pred_rf)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()\n","7e013bbe":"#Get predicted probabilites\ntarget_probailities_log = rf.predict_proba(x_test)[:,1]","c8c4dc2e":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)\n","a0a16c9d":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","8a373a9d":"Model,ROC_AUC_Accuracy","428108c3":"LR_Bag = BaggingClassifier(base_estimator=LR,n_estimators=100,n_jobs=-1,random_state=1)\nknn_Bag = BaggingClassifier(base_estimator=knn,n_estimators=100,n_jobs=-1,random_state=1)\nnb_Bag = BaggingClassifier(base_estimator=nb,n_estimators=100,n_jobs=-1,random_state=1)\ndt_Bag = BaggingClassifier(base_estimator=dt,n_estimators=100,n_jobs=-1,random_state=1)","7ff19a46":"kf = KFold(n_splits=3,shuffle=True,random_state=1)\nmodel = LR_Bag\nname = 'Bagged-LR'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+\/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","e037a1ec":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = nb_Bag\nname = 'Bagged-NB'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+\/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","5819432d":"kf = KFold(n_splits=3,shuffle=True,random_state=1)\nmodel = dt_Bag\nname = 'Bagged-DT'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+\/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","b8bca415":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMModel,LGBMClassifier","fb2f7574":"LR_Boost = AdaBoostClassifier(base_estimator=LR,n_estimators=100,learning_rate=0.01,random_state=1)\nknn_Boost = AdaBoostClassifier(base_estimator=knn,n_estimators=100,learning_rate=0.01,random_state=1)\nnb_Boost = AdaBoostClassifier(base_estimator=nb,n_estimators=100,learning_rate=0.01,random_state=1)\ndt_Boost = AdaBoostClassifier(base_estimator=dt,n_estimators=100,learning_rate=0.01,random_state=1)\nrf_Boost = AdaBoostClassifier(base_estimator=rf,n_estimators=100,learning_rate=0.01,random_state=1)\ngb_Boost = GradientBoostingClassifier(n_estimators=100,learning_rate=0.01)\nlgbm = LGBMClassifier(objective='binary',n_estimators=100,reg_alpha=2,reg_lambda=5,random_state=1,learning_rate=0.01,is_unbalance=True)","b9ac933f":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = LR_Boost\nname = 'Boosted-LR'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+\/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","445fde8c":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = nb_Boost\nname = 'Boosted-NB'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+\/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","69aa813a":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = dt_Boost\nname = 'Boosted-DT'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+\/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","1bd65946":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = lgbm\nname = 'LGBM'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+\/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","5b3dfadb":"Model,ROC_AUC_Accuracy","43483a9a":"final_result = pd.DataFrame({'Model':Model,'Accuracy':ROC_AUC_Accuracy})\nfinal_result","dd51395b":"## Now Moving Towards Boosting","d0080519":"## Dealing with Items Dataframe","6919eaf9":"## Dealing with Customer Demographic Dataframe","33cd47cb":"### Step 6:\nAggregating cust_trans on 'coupon_id'","942b7591":"## Merging Train and Test Data with Other Data","429fdaf6":"1. Merge coupon item data and items data on item_id\n2. Aggregate transactions by item_id\n3. Merge 1 and 2 on item_id\n4. Aggregate 3 on coupon_id\n5. Merge 4 and train on coupon_id\n6. Aggregate transactions on customer_id\n7. Merge 5 with campaign data on campaign_id\n8. Merge 7 with customer demographic data on customer_id\n9. Merge 6 with 8 on customer_id respectively","9478663b":"## Caution --- Algorithm - 'brute' was crashing the system, hence removed it.","f8ff0715":"### Step 5,7,8,9\n\nMerge 4 and train on coupon_id\n\nMerge 5 with campaign data on campaign_id\n\nMerge 7 with customer demographic data on customer_id\n\nMerge 6 with 8 on customer_id respectively","f93fc10b":"# Data is Ready here, But we need to check for Null Values which could have been created while merging.","4bb5810e":"### Step 3:\nMerge coupon_items and transaction1 on 'item_id'","306e1005":"## Let us see the schema first before proceeding","d67964d8":"### Step 4:\nNow Aggregating item_coupon_trans on 'coupon_id'","0621afbb":"### Step 2:\nAggregate the customer transaction by 'item_id'\n\nBefore getting into the below code ... Understand the working of pd.pivot_table here\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.pivot_table.html\n\nFollowing Blog is an excellent demonstration of the working of Pivot tables in Python\nhttps:\/\/pbpython.com\/pandas-pivot-table-explained.html","a9772ef8":"## Dealing with Campaign Dataframe","f55fad40":"# Cleaning, Preprocessing and Feature Engineering","34aa8ccb":"**Observation**\n\nThere are missing values in:\n\n1. age_range\n2. marital_status\n3. rented\n4. family_size\n5. no_of_children\n6. income_bracke","d703b817":"### Step 1: \nMerging coupons and items data on 'item_id'","1b5b2bf5":"### As all of these columns are present in cust_demo dataframe, filling the NaN's with the mode.","cc06a7bf":"# Now using the Bagging Classifiers","4c56f315":"## Dealing with Customer Transaction Dataframe","8ba25d0f":"# Note ... If Possible do the EDA here ... Univariate and Bivariate Analysis","2c2ba2d8":"# Naive Bayes","235a75b9":"# Random Forest","bfa899e0":"Inference\n\nThe dataset is highly imbalanced. As of now proceeding with the same imbalanced data.","33cd44a7":"![](http:\/\/)![](http:\/\/)# Train Dataframe","554f6518":"# Logistic Regression","bcfa62b1":"# Model Building - To be Covered Are:\n1. Logistic Regression\n2. kNN\n3. Naive Bayes\n4. Decision Tree\n5. Random Forest\n6. Logistic Regression -- Bagged\n7. kNN -- Bagged\n8. Naive Bayes - Bagged\n9. Decision Tree -- Bagged\n10. Logistic Regression -- Boosted\n11. Naive Bayes -- Boosted\n12. Decision Tree -- Boosted\n13. Random Forest --Boosted\n14. Gradient Boosting Classifier\n15. Light GBM (LGBM)","81976c78":"## Now Dropping off the Unwanted Columns and making the data ready for Model Building","fedc637b":"# Now we can Start with Merging the Data Frames As all the Dataframes have data in the proper format","210cc6a0":"# Decision Tree","32137697":"# kNN","e5ea23da":"# Dealing with Coupoun Dataframe"}}