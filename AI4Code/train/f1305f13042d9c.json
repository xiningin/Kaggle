{"cell_type":{"0ccd3ed4":"code","0a268a96":"code","946f9940":"code","bc380968":"code","aa0332e8":"code","a8dce605":"code","2e7c21e8":"code","55c85e38":"code","f2fbb855":"code","4a7ed04b":"code","3fab81a2":"code","9236aa3b":"code","d5df16f0":"code","e6223ddd":"code","e0a22d37":"code","d0ce260c":"code","e3ca3818":"markdown","f881ef39":"markdown","07be7484":"markdown","8c9c8311":"markdown","3429ba9a":"markdown","add13601":"markdown"},"source":{"0ccd3ed4":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint(f'Tensorflow version: {tf.__version__}')","0a268a96":"glass_data = pd.read_csv('..\/input\/glass\/glass.csv', parse_dates=True, encoding = \"cp1252\")\nglass_data.head()","946f9940":"glass_data.groupby('Type').count().reset_index()","bc380968":"glass_data['Type'].replace(to_replace={1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6}, inplace=True)","aa0332e8":"fig = plt.figure(figsize = (10,6))\nsns.countplot(data=glass_data, x='Type')","a8dce605":"corr = glass_data.corr(method = \"pearson\")\n# corr = glass_data.corr(method = \"spearman\")\n# corr = glass_data.corr(method = \"kendall\")\n\nf, ax = plt.subplots(figsize=(10, 10))\n\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax, annot=True)","2e7c21e8":"X = glass_data[['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe']]\n\ny = glass_data['Type']","55c85e38":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","f2fbb855":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","4a7ed04b":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(units=150, input_shape=(X_train.shape[1],), activation='relu'),\n  tf.keras.layers.Dense(units=300, activation='relu'),\n  tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(units=750, activation='relu'),\n  tf.keras.layers.Dropout(0.7),\n  tf.keras.layers.Dense(units=1350, activation='relu'),\n  tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(units=250, activation='relu'),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dense(units=350, activation='relu'),\n  tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(units=1500, activation='relu'),\n  tf.keras.layers.Dropout(0.7),\n  tf.keras.layers.Dense(units=1300, activation='relu'),\n  tf.keras.layers.Dropout(0.7),\n  tf.keras.layers.Dense(units=750, activation='relu'),\n  tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(units=250, activation='relu'),\n  tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(units=7, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","3fab81a2":"cl = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=90)","9236aa3b":"fig, ax = plt.subplots(figsize=(15,5))\n\nplt.plot(cl.history['accuracy'], label='accuracy')\nplt.plot(cl.history['val_accuracy'], label='val_accuracy', linestyle='--')\nplt.plot(cl.history['loss'], label='loss')\nplt.plot(cl.history['val_loss'], label='val_loss', linestyle='--')\nplt.legend()","d5df16f0":"ModelLoss, ModelAccuracy = model.evaluate(X_test, y_test)\n\nprint(f'Test Loss is {ModelLoss}')\nprint(f'Test Accuracy is {ModelAccuracy}')","e6223ddd":"y_pred = model.predict(X_test)\ny_test_list=list(y_test)\ntotal=len(y_test_list)\ncorrect=0\n\n# for i in range(len(y_test_list)):\n#   print(f'{i+1} - {y_pred[4][i]:.3f} - {y_test_list[4]}')\n#   if np.argmax(y_pred[i])+1==y_test_list[i]:\n#     print(f'{i+1} - {np.argmax(y_pred[i])} - {y_test_list[i]}')\n\nfor i in range(total):\n  # print(f'{np.argmax(y_pred[i])} - {np.amax(y_pred[i])} - {y_test_list[i]}')\n  if(np.argmax(y_pred[i])==y_test_list[i]):\n    correct+=1\n    \nprint(f'{correct}\/{total}')\nprint(correct\/total)","e0a22d37":"p_test = model.predict(X_test).argmax(axis=1)\ncm = tf.math.confusion_matrix(y_test, p_test)\n\nf, ax = plt.subplots(figsize=(7, 5))\nsns.heatmap(cm, annot=True, cmap='Blues', square=True, linewidths=0.01, linecolor='grey')\nplt.title('Confustion matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","d0ce260c":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nmodels=[(\"Logistic Regression\",LogisticRegression()),\n        (\"Linear Discriminant Analysis\",LinearDiscriminantAnalysis()),\n        (\"Decision Tree\",DecisionTreeClassifier()),\n        (\"Random Forest\",RandomForestClassifier()),\n        (\"Extra Trees\",ExtraTreesClassifier()),\n        (\"Gradient Boostin\",GradientBoostingClassifier()),\n        (\"KNeighbors\",KNeighborsClassifier()),\n        (\"SVM\",SVC()),\n        (\"Gaussian Naive Bayes\",GaussianNB()),\n        (\"Ada Boost\",AdaBoostClassifier())]\n\n    \nfor name, model in models:\n    results = cross_val_score(model, X_train, y_train.values.ravel(), cv=10, scoring='accuracy')\n    print(f\"\\x1b[94m{name}\\x1b[0m: \\x1b[95m{results.mean():.4f}\\x1b[0m \u00b1 {results.std():.4f}\")","e3ca3818":"## Train the model","f881ef39":"## Get the Data","07be7484":"### Split up the data to training set and test set","8c9c8311":"### Test for correlation","3429ba9a":"### Normalization of the data","add13601":"### Confustion matrix"}}