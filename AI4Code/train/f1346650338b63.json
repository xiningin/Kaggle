{"cell_type":{"a8b980c6":"code","1b84803a":"code","c36037a0":"code","fcaf5933":"code","ef14b9b3":"code","4a78e6db":"code","45fe3087":"code","bd4b34bb":"code","345eeb0f":"code","289a755e":"code","9d84cec1":"code","a9caba0c":"code","62c53171":"code","0421d57a":"code","7924a288":"code","49335540":"code","aac98272":"code","9b5617ea":"code","d520ddc5":"code","ed183aed":"code","c91a08b7":"code","fd4cc292":"code","f90dce34":"code","6ffe1068":"code","f7e46834":"code","2169298e":"code","4880a2a1":"code","5bab9df9":"code","cde90f78":"code","2fd38634":"code","f2fce112":"code","2c7fef92":"code","977dcaa4":"code","5fe29082":"code","0406b380":"code","704e76fe":"code","86f0587e":"code","c5f5f6f8":"code","7b729380":"code","d664c973":"code","c6405238":"code","b2eca36b":"code","8b2588f3":"code","18e05111":"code","80c65ed6":"code","2298652a":"code","739f6b90":"code","69605c04":"code","5273c91b":"code","ed31e29a":"code","3e7c775b":"code","de92cbcb":"code","52107199":"markdown","25b2407a":"markdown","e8b4f3bb":"markdown","db05b23e":"markdown","275a56d4":"markdown","8f80fdbd":"markdown","e54b5c9b":"markdown","ff39c551":"markdown"},"source":{"a8b980c6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport  os # data \n\n# Image processing\nfrom PIL import Image, ImageFile\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\n# Plotting\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg","1b84803a":"path, dirs, files = next(os.walk(\"..\/input\/boats\"))\ndirs","c36037a0":"boat_types = ['cruise ship', 'gondola', 'kayak', 'sailboat']","fcaf5933":"import glob\n\ni = 0\n\nX_data = []\nY_data = []\n\nfor boat in boat_types:\n    files = glob.glob (\"..\/input\/boats\/\" + str(boat) + \"\/*.jpg\")\n    \n    for myFile in files:\n      img = Image.open(myFile)\n      #img.thumbnail((width, height), Image.ANTIALIAS) # resizes image in-place keeps ratio\n      img = img.resize((128,128), Image.ANTIALIAS) # resizes image without ratio\n      img = np.array(img)\n\n      if img.shape == (128, 128, 3):\n        # Add the numpy image to matrix with all data\n        X_data.append (img)\n        Y_data.append (i)\n        \n    i += 1","ef14b9b3":"X = np.array(X_data)\nY = np.array(Y_data)\n# Print shapes to see if they are correct\nprint(X.shape)\nprint(Y.shape)","4a78e6db":"from keras.utils.np_utils import to_categorical\nX = X.astype('float32') \/ 255.0\ny_cat = to_categorical(Y_data, len(boat_types))","45fe3087":"boats = []\nnumber_of_boats = []\n\npath, dirs, files = next(os.walk(\"..\/input\/boats\"))  \n\nfor dir in dirs:\n  path2, dirs2, files2 = next(os.walk(\"..\/input\/boats\/\" + dir))  \n  boats.append(dir)\n  number_of_boats.append(len(files2))\n\n \ndf = pd.DataFrame({'Boat Types':boats, 'N':number_of_boats})\ndf = df.sort_values(['N'], ascending=False)\n\ndf_actual = df.set_index('Boat Types')\ndf_actual = df_actual.loc[boat_types]\ndf_actual = df_actual.sort_values(['N'], ascending=False)","bd4b34bb":"fig, axes = plt.subplots(2,2, figsize=(14,14))  # 1 row, 2 columns\ndf.plot('Boat Types', ax=axes[0,0], kind='bar', legend=False, color=[plt.cm.Paired(np.arange(len(df)))], width=0.95)\ndf_actual.plot(kind='bar', ax=axes[0,1], legend=False, color=[plt.cm.Paired(np.arange(len(df)))], width=0.95)\ndf.plot('Boat Types', 'N', kind='pie', labels=df['Boat Types'], ax=axes[1,0])\ndf_actual.plot('N', kind='pie', ax=axes[1,1], subplots=True)\nplt.tight_layout()","345eeb0f":"plt.close('all')\nplt.figure(figsize=(12, 12))\n\nfor i in range(16):\n  # Plot the images in a 4x4 grid\n  plt.subplot(4, 4, i+1)\n\n  # Plot image [i]\n  plt.imshow(X[i])\n  \n  # Turn off axis lines\n  cur_axes = plt.gca()\n  cur_axes.axes.get_xaxis().set_visible(False)\n  cur_axes.axes.get_yaxis().set_visible(False)","289a755e":"from keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.callbacks import EarlyStopping","9d84cec1":"import keras.backend as K\ndef load_CNN(output_size):\n  K.clear_session()\n  model = Sequential()\n  model.add(Conv2D(128, (5, 5),\n               input_shape=(128, 128, 3),\n               activation='relu'))\n  model.add(MaxPool2D(pool_size=(2, 2)))\n  #model.add(BatchNormalization())\n\n  model.add(Conv2D(64, (3, 3), activation='relu'))\n  model.add(MaxPool2D(pool_size=(2, 2)))\n  #model.add(BatchNormalization())\n\n  model.add(Conv2D(32, (3, 3), activation='relu'))\n  model.add(MaxPool2D(pool_size=(2, 2)))\n  #model.add(BatchNormalization())\n\n  model.add(Flatten())\n  model.add(Dense(512, activation='relu'))\n  model.add(Dense(output_size, activation='softmax'))\n  return model","a9caba0c":"early_stop_loss = EarlyStopping(monitor='loss', patience=3, verbose=1)\nearly_stop_val_acc = EarlyStopping(monitor='val_acc', patience=3, verbose=1)\nmodel_callbacks=[early_stop_loss, early_stop_val_acc]","62c53171":"from sklearn.model_selection import train_test_split\n\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Flatten\nfrom keras.layers import Activation, Dense\nfrom keras.utils.np_utils import to_categorical\n\nfrom keras.optimizers import SGD, Adam, Adagrad, RMSprop","0421d57a":"X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2)","7924a288":"print(\"The model has \" + str(len(X_train)) + \" inputs\")","49335540":"model = load_CNN(4)","aac98272":"model.summary()","9b5617ea":"model.compile(loss='categorical_crossentropy',\n              optimizer=Adam(lr=0.0005),\n              metrics=['accuracy'])\n\nweights = model.get_weights()","d520ddc5":"batch_sizes = [4, 8, 16, 32, 64, 128]\n\nhistories_acc = []\nhistories_val = []\nfor batch_size in batch_sizes:\n  model.set_weights(weights)\n  h = model.fit(X_train, y_train,\n                batch_size=batch_size,\n                epochs=25,\n                verbose=0,\n                callbacks=[early_stop_loss],\n                shuffle=True,\n                validation_data=(X_test, y_test))\n\n  histories_acc.append(h.history['acc'])\n  histories_val.append(h.history['val_acc'])\nhistories_acc = np.array(histories_acc)\nhistories_val = np.array(histories_val)","ed183aed":"learning_rates = [0.01, 0.005, 0.001, 0.0005, 0.0001]\nlrsHistories_acc = []\nlrsHistories_val = []\n\nfor lr in learning_rates:\n\n    model=load_CNN(4)\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=Adam(lr=lr),\n                  metrics=['accuracy'])\n    h = model.fit(X_train, y_train, \n                  batch_size=16, \n                  epochs=25, \n                  verbose=0, \n                  callbacks=[early_stop_loss],\n                  shuffle=True,\n                  validation_data=(X_test, y_test))\n\n    lrsHistories_acc.append(h.history['acc'])\n    lrsHistories_val.append(h.history['val_acc'])\nlrsHistories_acc = np.array(lrsHistories_acc)\nlrsHistories_val = np.array(lrsHistories_val)","c91a08b7":"optimizers = ['SGD(lr=0.0001)',\n              'SGD(lr=0.0001, momentum=0.3)',\n              'SGD(lr=0.0001, momentum=0.3, nesterov=True)',  \n              'Adam(lr=0.0001)',\n              'Adagrad(lr=0.0001)',\n              'RMSprop(lr=0.0001)']\n\noptimizeList_acc = []\noptimizeList_val = []\n\nfor opt_name in optimizers:\n\n    model=load_CNN(4)\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=eval(opt_name),\n                  metrics=['accuracy'])\n    h = model.fit(X_train, y_train, \n                  batch_size=16, \n                  epochs=25, \n                  verbose=0, \n                  callbacks=[early_stop_loss],\n                  shuffle=True,\n                  validation_data=(X_test, y_test))\n\n    optimizeList_acc.append(h.history['acc'])\n    optimizeList_val.append(h.history['val_acc'])\noptimizeList_acc = np.array(optimizeList_acc)\noptimizeList_val = np.array(optimizeList_val)","fd4cc292":"import random\nimage_number = random.randint(0,len(X_train))\nprint(image_number)","f90dce34":"from keras.models import Model\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(X_train[image_number].reshape(1,128,128,3))\n \ndef display_activation(activations, col_size, row_size, act_index): \n    activation = activations[act_index]\n    activation_index=0\n    fig, ax = plt.subplots(row_size, col_size, figsize=(12,12))\n    cur_axes = plt.gca()\n    cur_axes.axes.get_xaxis().set_visible(False)\n    cur_axes.axes.get_yaxis().set_visible(False)    \n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n            activation_index += 1","6ffe1068":"plt.figure(figsize=(8, 8))\nplt.imshow(X_train[image_number])","f7e46834":"display_activation(activations, 4, 4, 4)","2169298e":"acc_lr = lrsHistories_acc\nval_lr = lrsHistories_val\n\nacc_bs = histories_acc\nval_bs = histories_val\n\nacc_opt = optimizeList_acc\nval_opt = optimizeList_val","4880a2a1":"plt.subplots(2,1,figsize=(12,12))\nplt.subplot(211)\nfor b in acc_bs:\n  plt.plot(b)\n  plt.title('Accuracy for different batch sizes')\n  #plt.ylim(0, 1.01)\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['4', '8', '16', '32', '64', '128'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim(bottom, 1.01)   # set the ylim to bottom, top\n\nplt.subplot(212)\nfor z in val_bs:\n  plt.plot(z)\n  plt.title('Validation accuracy for different batch sizes')\n  #plt.ylim(0, 1.01)\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['8', '16', '32', '64', '128', '256'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim((bottom - bottom*0.03), (top + top*0.03))   # set the ylim to bottom, top","5bab9df9":"plt.subplots(2,1,figsize=(12,12))\nplt.subplot(211)\nfor x in acc_lr:\n  plt.plot(x)\n  plt.title('Accuracy for different learning rates')\n  #plt.ylim(0, 1.01)\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['0.01', '0.005', '0.001', '0.0005', '0.0001'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim(bottom, 1.01)   # set the ylim to bottom, top\n\nplt.subplot(212)\nfor y in val_lr:\n  plt.plot(y)\n  plt.title('Validation accuracy for different learning rates')\n  #plt.ylim(0, 1.01)\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['0.01', '0.005', '0.001', '0.0005', '0.0001'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim((bottom - bottom*0.03), (top + top*0.03))   # set the ylim to bottom, top","cde90f78":"plt.subplots(2,1,figsize=(12,12))\nplt.subplot(211)\nfor x in acc_opt:\n  plt.plot(x)\n  plt.title('Accuracy for different optimizers')\n  #plt.ylim(0, 1.01)\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['SGD(lr=0.001)',\n              'SGD(lr=0.001, momentum=0.3)',\n              'SGD(lr=0.001, momentum=0.3, nesterov=True)',  \n              'Adam(lr=0.001)',\n              'Adagrad(lr=0.001)',\n              'RMSprop(lr=0.001)'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim(bottom, 1.01)   # set the ylim to bottom, top\n\nplt.subplot(212)\nfor y in val_opt:\n  plt.plot(y)\n  plt.title('Validation accuracy for different optimizers')\n  #plt.ylim(0, 1.01)\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['SGD(lr=0.001)',\n              'SGD(lr=0.001, momentum=0.3)',\n              'SGD(lr=0.001, momentum=0.3, nesterov=True)',  \n              'Adam(lr=0.001)',\n              'Adagrad(lr=0.001)',\n              'RMSprop(lr=0.001)'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim((bottom - bottom*0.03), (top + top*0.03))   # set the ylim to bottom, top","2fd38634":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns","f2fce112":"model=load_CNN(4)\n    \nmodel.compile(loss='binary_crossentropy',\n              optimizer=Adagrad(0.0005),\n              metrics=['accuracy'])","2c7fef92":"h = model.fit(X_train, y_train, \n              batch_size=8, \n              epochs=25, \n              verbose=0, \n              callbacks=[early_stop_loss],\n              shuffle=True,\n              validation_data=(X_test, y_test))","977dcaa4":"y_pred = model.predict(X_test) \ny_pred_classes = np.argmax(y_pred,axis = 1) \ny_true = np.argmax(y_test,axis = 1)","5fe29082":"plt.plot(h.history['acc'], label='accuracy')\nplt.plot(h.history['val_acc'], label='validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['acc',\n            'val_acc'], loc='best')","0406b380":"con_matrix = confusion_matrix(y_true, y_pred_classes, labels=[0,1,2,3]) \n\nplt.figure(figsize=(10,10))\nplt.title('Prediction of boat types')\nsns.heatmap(con_matrix, annot=True, fmt=\"d\", linewidths=.5)","704e76fe":"# Create a data generation variable with characteristics\ndatagen = ImageDataGenerator(\n        rotation_range=30,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        shear_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        fill_mode='nearest')","86f0587e":"train_generator = datagen.flow(X_train, y_train, batch_size=256)\nvalidation_generator = datagen.flow(X_test, y_test, batch_size=256)","c5f5f6f8":"plt.close('all')\nfor X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=16):\n    plt.figure(figsize=(12, 12))\n    for i in range(16):\n        plt.subplot(4, 4, i+1)\n        plt.imshow(X_batch[i])\n        \n        # Turn off axis lines\n        cur_axes = plt.gca()\n        cur_axes.axes.get_xaxis().set_visible(False)\n        cur_axes.axes.get_yaxis().set_visible(False)\n    break","7b729380":"model = load_CNN(4)\n    \nmodel.compile(loss='binary_crossentropy',\n              optimizer=Adam(lr=0.0001),\n              metrics=['accuracy'])\n\nweights_aug = model.get_weights()","d664c973":"epochs = 25","c6405238":"batch_sizes = [8, 16, 32, 64, 128, 256]\n\ndata_augmentation_bs_acc = []\ndata_augmentation_bs_val = []\nfor batch_size in batch_sizes:\n  \n  train_generator = datagen.flow(X_train, y_train, batch_size=batch_size)\n  validation_generator = datagen.flow(X_test, y_test, batch_size=batch_size)\n  \n  model.set_weights(weights_aug)\n  h = model.fit_generator(train_generator,\n                    steps_per_epoch=len(X_train) \/ 32,\n                    epochs=epochs,\n                    validation_data=validation_generator,\n                    validation_steps=10,\n                    callbacks=[early_stop_loss],\n                    verbose=0)\n\n  data_augmentation_bs_acc.append(h.history['acc'])\n  data_augmentation_bs_val.append(h.history['val_acc'])\ndata_augmentation_bs_acc = np.array(data_augmentation_bs_acc)\ndata_augmentation_bs_val = np.array(data_augmentation_bs_val)","b2eca36b":"data_augmentation_lr_acc = []\ndata_augmentation_lr_val = []\n\ntrain_generator = datagen.flow(X_train, y_train, batch_size=16)\nvalidation_generator = datagen.flow(X_test, y_test, batch_size=16)\n\nfor lr in learning_rates:\n\n    model=load_CNN(4)\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=Adam(lr=lr),\n                  metrics=['accuracy'])\n    h = model.fit_generator(train_generator,\n                    steps_per_epoch=len(X_train) \/ 32,\n                    epochs=epochs,\n                    validation_data=validation_generator,\n                    callbacks=[early_stop_loss],\n                    verbose=0,\n                    validation_steps=10)\n\n    data_augmentation_lr_acc.append(h.history['acc'])\n    data_augmentation_lr_val.append(h.history['val_acc'])\ndata_augmentation_lr_acc = np.array(data_augmentation_lr_acc)\ndata_augmentation_lr_val = np.array(data_augmentation_lr_val)","8b2588f3":"data_augmentation_opt_acc = []\ndata_augmentation_opt_val = []\n\nfor opt_name in optimizers:\n\n    model=load_CNN(4)\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=eval(opt_name),\n                  metrics=['accuracy'])\n    h = model.fit_generator(train_generator,\n                    steps_per_epoch=len(X_train) \/ 32,\n                    epochs=epochs,\n                    validation_data=validation_generator,\n                    callbacks=[early_stop_loss],\n                    verbose=0,\n                    validation_steps=10)\n\n    data_augmentation_opt_acc.append(h.history['acc'])\n    data_augmentation_opt_val.append(h.history['val_acc'])\ndata_augmentation_opt_acc = np.array(data_augmentation_opt_acc)\ndata_augmentation_opt_val = np.array(data_augmentation_opt_val)","18e05111":"plt.subplots(2,1,figsize=(12,12))\nplt.subplot(211)\nfor b in data_augmentation_bs_acc:\n  plt.plot(b)\n  plt.title('Accuracy for different batch sizes')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['8', '16', '32', '64', '128', '256'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim(bottom, 1.01)   # set the ylim to bottom, top\n\nplt.subplot(212)\nfor z in data_augmentation_bs_val:\n  plt.plot(z)\n  plt.title('Validation accuracy for different batch sizes')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['8', '16', '32', '64', '128', '256'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim((bottom - bottom*0.03), (top + top*0.03))   # set the ylim to bottom, top","80c65ed6":"plt.subplots(2,1,figsize=(12,12))\nplt.subplot(211)\nfor x in data_augmentation_lr_acc:\n  plt.plot(x)\n  plt.title('Accuracy for different learning rates')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['0.01', '0.005', '0.001', '0.0005', '0.0001'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim(bottom, 1.01)   # set the ylim to bottom, top\n\nplt.subplot(212)\nfor y in data_augmentation_lr_val:\n  plt.plot(y)\n  plt.title('Validation accuracy for different learning rates')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['0.01', '0.005', '0.001', '0.0005', '0.0001'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim((bottom - bottom*0.03), (top + top*0.03))   # set the ylim to bottom, top","2298652a":"plt.subplots(2,1,figsize=(12,12))\nplt.subplot(211)\nfor x in data_augmentation_opt_acc:\n  plt.plot(x)\n  plt.title('Data augmentation accuracy for different optimizers')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['SGD(lr=0.001)',\n              'SGD(lr=0.001, momentum=0.3)',\n              'SGD(lr=0.001, momentum=0.3, nesterov=True)',  \n              'Adam(lr=0.001)',\n              'Adagrad(lr=0.001)',\n              'RMSprop(lr=0.001)'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim(bottom-0.01, 1.01)   # set the ylim to bottom, top\n\nplt.subplot(212)\nfor y in data_augmentation_opt_val:\n  plt.plot(y)\n  plt.title('Data augmentation validation accuracy for different optimizers')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(['SGD(lr=0.001)',\n              'SGD(lr=0.001, momentum=0.3)',\n              'SGD(lr=0.001, momentum=0.3, nesterov=True)',  \n              'Adam(lr=0.001)',\n              'Adagrad(lr=0.001)',\n              'RMSprop(lr=0.001)'], loc='best')\n  bottom, top = plt.ylim()  # return the current ylim\n  plt.ylim((bottom - bottom*0.03), (top + top*0.03))   # set the ylim to bottom, top","739f6b90":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns","69605c04":"model=load_CNN(4)\n    \nmodel.compile(loss='binary_crossentropy',\n              optimizer=Adam(0.0002),\n              metrics=['accuracy'])","5273c91b":"h = model.fit_generator(train_generator,\n                steps_per_epoch=len(X_train) \/ 32,\n                epochs=150,\n                validation_data=validation_generator,\n                #callbacks=[early_stop_loss],\n                verbose=0,\n                validation_steps=10)","ed31e29a":"y_pred = model.predict(X_test) \ny_pred_classes = np.argmax(y_pred,axis = 1) \ny_true = np.argmax(y_test,axis = 1)","3e7c775b":"plt.plot(h.history['acc'], label='accuracy')\nplt.plot(h.history['val_acc'], label='validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['acc',\n            'val_acc'], loc='best')","de92cbcb":"\ncon_matrix = confusion_matrix(y_true, y_pred_classes, labels=[0,1,2,3]) \n\nplt.figure(figsize=(10,10))\nplt.title('Prediction of boat types')\nsns.heatmap(con_matrix, annot=True, fmt=\"d\", linewidths=.5)","52107199":"**Optimal network with the specifications from above (still without augmentation)**\n\n    - Batch size of 8\n    - Learning rate of 0.0005\n    - Optimizer Adagrad","25b2407a":"**Output visualization**\n\n    - Show a number of features from the model as images","e8b4f3bb":"**Optimal network with augmentation with specifications from the graphs above**","db05b23e":"**Converting the lists to numpy arrays and make the output categorical**","275a56d4":"**Data augmentation**\n\n    - Using augmented images to see the differences in accuracy","8f80fdbd":"**Convolutional Neural Network testing with and without data augmentation to review the differences**\n\n**Note: This is not meant to be perfect but more of a learning process we want to share**\n\nThis kernel is made by:\n    - Giel Oomen (giel.oomen@gmail.com)\n    - Gijs Beneken (gijsbeneken@gmail.com)\n    \n from the Netherlands\n    \n The goal is to test different specifications for one architecture (which has been designed with trial and error). The differences are in batch sizes, learning rates and optimizers.","e54b5c9b":"**Accuracy visualization**\n    - Plot the accuracy and validation accuracy per batch size\n    - Plot the accuracy and validation accuracy per learning rate\n    - Plot the accuracy and validation accuracy per optimmizer\n    - Show the confusion matrix","ff39c551":"**CNN Without Data Augmentation**\n\n   - Split the in- and output datasets into train and test sets with a 80\/20% ratio\n   - Testing with different batch sizes\n   - Testing with different learning rates\n   - Testing with different optimizers"}}