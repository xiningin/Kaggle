{"cell_type":{"c2ce105a":"code","b9618934":"code","befa8fed":"code","0d46af8c":"code","8ca6615d":"code","543163dc":"code","786358b9":"code","61374df4":"code","c65ad678":"code","2d3ec5f0":"code","b93ce1f2":"code","ad04a8ec":"code","77ca95ab":"code","73a72f56":"code","b1a9e96c":"code","dea57a2c":"code","a03ce106":"code","b4331874":"code","b087aaea":"code","487f568c":"markdown","fd08847c":"markdown","76701daa":"markdown","8798ed6c":"markdown","d454e20c":"markdown","f00a7ea4":"markdown","3fca657f":"markdown","7790afff":"markdown","658f884a":"markdown","067fb6d9":"markdown","058e5fee":"markdown","20d302eb":"markdown","3206edf1":"markdown","28f4b275":"markdown","0edc41ba":"markdown","feccde32":"markdown","761ac4fa":"markdown","2a7726fc":"markdown","8319e58e":"markdown","69897bda":"markdown","2ad4f0d1":"markdown"},"source":{"c2ce105a":"#%% Imports\n\n# Basic Imports \nimport numpy as np\nimport pandas as pd\n\n# Plotting \nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# Metrics \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report,accuracy_score, recall_score, roc_auc_score, precision_score\nfrom sklearn.metrics import roc_curve, auc\n\n# ML Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier \n\n# Model Tuning \nfrom bayes_opt import BayesianOptimization\n\n# Feature Importance \nimport shap\n\n# Ignore Warnings \nimport warnings\nwarnings.filterwarnings('ignore')","b9618934":"#%% Read data.csv\ndata_csv = pd.read_csv('..\/input\/company-bankruptcy-prediction\/data.csv')\n\n# Rename Column names \n# X0 = target \ncolumn_names = ['Y']\ncolumn_names = column_names + ['X' + str(num) for num in range(1,len(data_csv.columns))]\ncolumn_names_df = pd.DataFrame({\"Var_name\": column_names,\"Description\": data_csv.columns})\n\ndata_csv.columns = column_names \ndata_csv.info(verbose = True,show_counts = True)","befa8fed":"column_names_df.style","0d46af8c":"for int_column in data_csv.select_dtypes(include=\"int64\"):\n    print(data_csv[int_column].value_counts())\n    print(\"\\n\")\n\n# drop column C94 for being a useless feature, only value is 1 \ndata_csv = data_csv.drop(\"X94\",axis = \"columns\")\n\n# this dataset is imbalance 6599 ones and 220 zeroes","8ca6615d":"# Create test and train set 80-20\n\n#%% Sepearte features and target from data_csv\nX = data_csv.drop(\"Y\",axis = \"columns\")\ny = data_csv[\"Y\"]\n\n#%%  train-test stratified split using 80-20 \ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.2, random_state = 0, shuffle= True,stratify = y)\n\n# By using a stratified split, the raito of 1 and 0s are consistent btwn the two splits \nprint(train_y.value_counts())\nprint(\"\\n\")\nprint(valid_y.value_counts())","543163dc":"# Great Function found on Kaggle for plotting a Confusion Matrix\n# https:\/\/www.kaggle.com\/grfiv4\/plot-a-confusion-matrix\ndef plot_confusion_matrix_kaggle(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()\n    \n# binarize an array based of a threshold \ndef binarizeArray(array,threshold = 0.5):\n    return [0 if num < threshold else 1 for num in array]","786358b9":"# Create initial models\nLogReg = LogisticRegression(random_state=0).fit(train_X, train_y)\nXGBClass = xgb.XGBClassifier(eval_metric  = \"logloss\", max_depth=5, learning_rate=0.01, n_estimators=100, gamma=0, \n                        min_child_weight=1, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.005,seed = 0).fit(train_X,train_y)\nRFClass = RandomForestClassifier(n_estimators = 50, max_depth = 50,n_jobs = -1, random_state = 0).fit(train_X,train_y)\nLGBMClass = LGBMClassifier(random_state=0).fit(train_X, train_y)","61374df4":"# initial models performance on Test Data \n    \npred_y = LogReg.predict(valid_X)\nprint(\"                    Logistic Regression\")\nprint(classification_report(valid_y,pred_y,digits=3))\n\npred_y = XGBClass.predict(valid_X)\nprint(\"                    XGBoost Classifier\")\nprint(classification_report(valid_y,pred_y,digits=3))\n\npred_y = RFClass.predict(valid_X)\nprint(\"                    Random Forest Classifier\")\nprint(classification_report(valid_y,pred_y,digits=3))\n\npred_y = LGBMClass.predict(valid_X)\nprint(\"                    LightGBM Classifier\")\nprint(classification_report(valid_y,pred_y,digits=3))","c65ad678":"##% parameter tuning for lightgbm \n# store the catagorical features names as a list      \ncat_features = data_csv.select_dtypes(['object']).columns.to_list()\n# print(cat_features)\n\n# Create the LightGBM data containers\n# Make sure that cat_features are used\ntrain_lgbdata=lgb.Dataset(train_X,label=train_y, categorical_feature = cat_features,free_raw_data=False)\ntest_lgbdata=lgb.Dataset(valid_X,label=valid_y, categorical_feature = cat_features,free_raw_data=False)","2d3ec5f0":"# https:\/\/github.com\/fmfn\/BayesianOptimization\ndef search_best_param(X,y,cat_features):\n    \n    trainXY = lgb.Dataset(data=X, label=y,categorical_feature = cat_features,free_raw_data=False)\n    # define the lightGBM cross validation\n    def lightGBM_CV(max_depth, num_leaves, n_estimators, learning_rate, subsample, colsample_bytree, \n                lambda_l1, lambda_l2, min_child_weight):\n    \n        params = {'boosting_type': 'gbdt', 'objective': 'binary', 'metric':'auc', 'verbose': -1,\n                  'early_stopping_round':100}\n        \n        params['max_depth'] = int(round(max_depth))\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params[\"n_estimators\"] = int(round(n_estimators))\n        params['learning_rate'] = learning_rate\n        params['subsample'] = subsample\n        params['colsample_bytree'] = colsample_bytree\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_child_weight'] = min_child_weight\n    \n        score = lgb.cv(params, trainXY, nfold=5, seed=1, stratified=True, verbose_eval =False, metrics=['auc'])\n        return np.mean(score['auc-mean']) # maximize auc-mean\n\n    # use bayesian optimization to search for the best hyper-parameter combination\n    lightGBM_Bo = BayesianOptimization(lightGBM_CV, \n                                       {\n                                          'max_depth': (5, 50),\n                                          'num_leaves': (20, 100),\n                                          'n_estimators': (50, 500),\n                                          'learning_rate': (0.01, 0.3),\n                                          'subsample': (0.7, 0.8),\n                                          'colsample_bytree' :(0.5, 0.99),\n                                          'lambda_l1': (0, 5),\n                                          'lambda_l2': (0, 3),\n                                          'min_child_weight': (2, 50) \n                                      },\n                                       random_state = 1,\n                                       verbose = 3\n                                      )\n    np.random.seed(1)\n    \n    lightGBM_Bo.maximize(init_points= 5, n_iter=5) # 5 + 5, 10 iterations \n    # n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    # init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    # more iterations more time spent searching \n    \n    params_set = lightGBM_Bo.max['params']\n    \n    # get the params of the maximum target     \n    max_target = -np.inf\n    for i in lightGBM_Bo.res: # loop thru all the residuals \n        if i['target'] > max_target:\n            params_set = i['params']\n            max_target = i['target']\n    \n    params_set.update({'verbose': -1})\n    params_set.update({'metric': 'auc'})\n    params_set.update({'boosting_type': 'gbdt'})\n    params_set.update({'objective': 'binary'})\n    \n    params_set['max_depth'] = int(round(params_set['max_depth']))\n    params_set['num_leaves'] = int(round(params_set['num_leaves']))\n    params_set['n_estimators'] = int(round(params_set['n_estimators']))\n    params_set['seed'] = 1 #set seed\n    \n    return params_set","b93ce1f2":"# Search for best param on Training Set\nbest_params = search_best_param(train_X,train_y,cat_features)","ad04a8ec":"# Print best_params\nfor key, value in best_params.items():\n    print(key, ' : ', value)","77ca95ab":"# Train lgbm_best using the best params found from Bayesian Optimization\nlgbm_best = lgb.train(best_params,\n                 train_lgbdata,\n                 num_boost_round = 100,\n                 valid_sets = test_lgbdata,\n                 early_stopping_rounds = 100,\n                 verbose_eval = 25\n                 )","73a72f56":"##% Feature Importance \n# https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\nlgb.plot_importance(lgbm_best,figsize=(25,20),max_num_features = 10)","b1a9e96c":"##% Feature Importance using shap package \n# import shap\nshap_values = shap.TreeExplainer(lgbm_best).shap_values(valid_X)\nshap.summary_plot(shap_values, valid_X)","dea57a2c":"#%% ROC Curve for training\/validation data\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n# from sklearn.metrics import roc_curve, auc\n\ny_probas = lgbm_best.predict(valid_X) \n\nfpr, tpr, _ = roc_curve(valid_y, y_probas)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for training data')\nplt.legend(loc=\"lower right\")\nplt.show()","a03ce106":"#%% Plot ROC curve and find best threshold to binarize the predictions from LGBM\n# https:\/\/machinelearningmastery.com\/threshold-moving-for-imbalanced-classification\/\npred_y = lgbm_best.predict(valid_X)\n# calculate roc curves\nfpr, tpr, thresholds = roc_curve(valid_y, pred_y)\n# calculate the g-mean for each threshold\ngmeans = np.sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = np.argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n# plot the roc curve for the model\npyplot.figure(num=0, figsize=[6.4, 4.8])\npyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\npyplot.plot(fpr, tpr, marker='.', label='LightGBM')\npyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\n# show the plot\npyplot.show()","b4331874":"# Great Function found on Kaggle for plotting a Confusion Matrix\n# https:\/\/www.kaggle.com\/grfiv4\/plot-a-confusion-matrix\ndef plot_confusion_matrix_kaggle(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()\n\n# binarize an array based of a threshold \ndef binarizeArray(array,threshold = 0.5):\n    return [0 if num < threshold else 1 for num in array]","b087aaea":"#%% Plot Confusion Matrix with best threshold\npref_y_bin = binarizeArray(pred_y,thresholds[ix])\ncm = confusion_matrix(valid_y,pref_y_bin)\nplot_confusion_matrix_kaggle(cm =cm, \n                      normalize    = False,\n                      target_names = ['Not Bankrupt(0)', 'Bankrupt(1)'],\n                      title        = \"Confusion Matrix\")\nprint(classification_report(valid_y,pref_y_bin))\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(valid_y, pref_y_bin)*100.0))\nprint(\"Recall: %.2f%%\" % ((recall_score(valid_y,pref_y_bin))*100.0))","487f568c":"<a id=\"Table-Of-Contents\"><\/a>\n# Table Of Contents\n* [Table Of Contents](#Table-Of-Contents)\n* [Task Details](#Task-Details)\n* [Importing Libraries](#Importing-Libraries)\n* [Notes](#Notes)\n    - [Context](#Context)\n    - [Target](#Target)\n    - [Attribute-Information](#Attribute-Information)\n* [Read in Data](#Read-in-Data)\n    - [Data.csv](#Data.csv)\n* [Preprocessing Data](#Preprocessing-Data)\n    - [Train-Test Stratified Split](#Train-Test-Stratified-Split)\n* [Initial Models](#Initial-Models)\n* [LightGBM Classifier](#LightGBM-Classifier)\n    - [Tuning LightGBM](#Tuning-LightGBM)\n    - [Model Metrics](#Model-Metrics)\n    - [Bayesian Optimization](#Bayesian-Optimization)\n    - [Feature Importance](#Feature-Importance)\n* [LightGBM Model Peformance](#LightGBM-Model-Peformance)\n    - [ROC Curve](#ROC-Curve)\n    - [Confusion Matrix](#Confusion-Matrix)\n* [Conclusion](#Conclusion)","fd08847c":"<a id=\"#Train-Test-Stratified-Split\"><\/a>\n## Train-Test Stratified Split","76701daa":"<a id=\"Task-Details\"><\/a>\n# Task Details\nOur top priority in this business problem is to identify companies in bankruptcy.\n\n## Evaluation\nEvaluation using F1-Score.\nThe F1-Score is defines as the harmonic mean between precision and recall:\n<img src=\"https:\/\/datascience103579984.files.wordpress.com\/2019\/04\/capture4-17.png\"\/>","8798ed6c":"<a id=\"Read-in-Data\"><\/a>\n# Read in Data","d454e20c":"<a id=\"Tuning-LightGBM\"><\/a>\n## Tuning LightGBM","f00a7ea4":"<a id=\"Initial Models\"><\/a>\n# Initial Models\nI applied different machine learning algorthims to test which model perform better on this dataset. I've listed below various machine learning techniques applied in this section.\n\n1. Logistic Regression\n2. XGBoost Classifier\n3. Random Forest Classifier\n5. LightGBM Classifier","3fca657f":"label encoding does not need to be used here as the columns are already fine. The categorical variables can stay as int64 type as they are only binary variables. ","7790afff":"<a id=\"LightGBM-Classifier\"><\/a>\n# LightGBM Classifier","658f884a":"<a id=\"Conclusion\"><\/a>\n# Conclusion\n\n**Conclusion**\n* Good dataset to work with, enough observations to create a model\n* Difficult to achieve a good model metric as dataset is imbalanced\n\n**Closing Remarks**  \n* Please comment and like the notebook if it of use to you! Have a wonderful year! \n\n**More Notebooks** \n\n**Regression Notebooks**   \n[https:\/\/www.kaggle.com\/josephchan524\/housepricesregressor-using-lightgbm](https:\/\/www.kaggle.com\/josephchan524\/housepricesregressor-using-lightgbm)\n\n[https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundregressor-using-lightgbm-feb2021](https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundregressor-using-lightgbm-feb2021)\n\n[https:\/\/www.kaggle.com\/josephchan524\/studentperformanceregressor-rmse-12-26-r2-0-26](https:\/\/www.kaggle.com\/josephchan524\/studentperformanceregressor-rmse-12-26-r2-0-26)\n\n\n**Classification Notebooks**  \n[https:\/\/www.kaggle.com\/josephchan524\/hranalytics-lightgbm-classifier-auc-80](https:\/\/www.kaggle.com\/josephchan524\/hranalytics-lightgbm-classifier-auc-80)\n\n[https:\/\/www.kaggle.com\/josephchan524\/bankchurnersclassifier-recall-97-accuracy-95](https:\/\/www.kaggle.com\/josephchan524\/bankchurnersclassifier-recall-97-accuracy-95)\n\n[https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundclassifier-using-lightgbm-mar2021](https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundclassifier-using-lightgbm-mar2021)  \n\n3-16-2020\nJoseph","067fb6d9":"# Model Metrics\nI've created several functions to help evalaute a model's performance such as confusion matrix, accuarcy, recall, and F1-score.    ","058e5fee":"<a id=\"Importing-Libraries\"><\/a>\n# Importing Libraries","20d302eb":"Focus on F1-score for 1. It can be seen that it is difficult to determine if a company files bankruptcy. \n\nLightGBM Classifier perform the best so I proceeded to implement hyperparameter optimization onto LightGBM.  ","3206edf1":"<a id=\"data.csv\"><\/a>\n## Data.csv","28f4b275":"<a id=\"Notes\"><\/a>\n# Notes\n\n## Context\nThe data were collected from the Taiwan Economic Journal for the years 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange.\n\n## Target\nY - Bankrupt?: Class label: 0 - Not Bankrupt , 1 - Bankrupt\n\n## Attribute Information\nX1 - ROA(C) before interest and depreciation before interest: Return On Total Assets(C)  \nX2 - ROA(A) before interest and % after tax: Return On Total Assets(A)  \nX3 - ROA(B) before interest and depreciation after tax: Return On Total Assets(B)  \nX4 - Operating Gross Margin: Gross Profit\/Net Sales  \nX5 - Realized Sales Gross Margin: Realized Gross Profit\/Net Sales  \nX6 - Operating Profit Rate: Operating Income\/Net Sales  \nX7 - Pre-tax net Interest Rate: Pre-Tax Income\/Net Sales  \nX8 - After-tax net Interest Rate: Net Income\/Net Sales  \nX9 - Non-industry income and expenditure\/revenue: Net Non-operating Income Ratio  \nX10 - Continuous interest rate (after tax): Net Income-Exclude Disposal Gain or Loss\/Net Sales  \nX11 - Operating Expense Rate: Operating Expenses\/Net Sales  \nX12 - Research and development expense rate: (Research and Development Expenses)\/Net Sales  \nX13 - Cash flow rate: Cash Flow from Operating\/Current Liabilities  \nX14 - Interest-bearing debt interest rate: Interest-bearing Debt\/Equity  \nX15 - Tax rate (A): Effective Tax Rate  \nX16 - Net Value Per Share (B): Book Value Per Share(B)  \nX17 - Net Value Per Share (A): Book Value Per Share(A)  \nX18 - Net Value Per Share (C): Book Value Per Share(C)  \nX19 - Persistent EPS in the Last Four Seasons: EPS-Net Income  \nX20 - Cash Flow Per Share  \nX21 - Revenue Per Share (Yuan \u00a5): Sales Per Share  \nX22 - Operating Profit Per Share (Yuan \u00a5): Operating Income Per Share  \nX23 - Per Share Net profit before tax (Yuan \u00a5): Pretax Income Per Share  \nX24 - Realized Sales Gross Profit Growth Rate  \nX25 - Operating Profit Growth Rate: Operating Income Growth  \nX26 - After-tax Net Profit Growth Rate: Net Income Growth  \nX27 - Regular Net Profit Growth Rate: Continuing Operating Income after Tax Growth  \nX28 - Continuous Net Profit Growth Rate: Net Income-Excluding Disposal Gain or Loss Growth  \nX29 - Total Asset Growth Rate: Total Asset Growth  \nX30 - Net Value Growth Rate: Total Equity Growth  \nX31 - Total Asset Return Growth Rate Ratio: Return on Total Asset Growth  \nX32 - Cash Reinvestment %: Cash Reinvestment Ratio  \nX33 - Current Ratio  \nX34 - Quick Ratio: Acid Test  \nX35 - Interest Expense Ratio: Interest Expenses\/Total Revenue  \nX36 - Total debt\/Total net worth: Total Liability\/Equity Ratio  \nX37 - Debt ratio %: Liability\/Total Assets  \nX38 - Net worth\/Assets: Equity\/Total Assets  \nX39 - Long-term fund suitability ratio (A): (Long-term Liability+Equity)\/Fixed Assets  \nX40 - Borrowing dependency: Cost of Interest-bearing Debt  \nX41 - Contingent liabilities\/Net worth: Contingent Liability\/Equity  \nX42 - Operating profit\/Paid-in capital: Operating Income\/Capital  \nX43 - Net profit before tax\/Paid-in capital: Pretax Income\/Capital  \nX44 - Inventory and accounts receivable\/Net value: (Inventory+Accounts Receivables)\/Equity  \nX45 - Total Asset Turnover  \nX46 - Accounts Receivable Turnover  \nX47 - Average Collection Days: Days Receivable Outstanding  \nX48 - Inventory Turnover Rate (times)  \nX49 - Fixed Assets Turnover Frequency  \nX50 - Net Worth Turnover Rate (times): Equity Turnover  \nX51 - Revenue per person: Sales Per Employee  \nX52 - Operating profit per person: Operation Income Per Employee  \nX53 - Allocation rate per person: Fixed Assets Per Employee  \nX54 - Working Capital to Total Assets  \nX55 - Quick Assets\/Total Assets  \nX56 - Current Assets\/Total Assets  \nX57 - Cash\/Total Assets  \nX58 - Quick Assets\/Current Liability  \nX59 - Cash\/Current Liability  \nX60 - Current Liability to Assets  \nX61 - Operating Funds to Liability  \nX62 - Inventory\/Working Capital  \nX63 - Inventory\/Current Liability  \nX64 - Current Liabilities\/Liability  \nX65 - Working Capital\/Equity  \nX66 - Current Liabilities\/Equity  \nX67 - Long-term Liability to Current Assets  \nX68 - Retained Earnings to Total Assets  \nX69 - Total income\/Total expense  \nX70 - Total expense\/Assets  \nX71 - Current Asset Turnover Rate: Current Assets to Sales  \nX72 - Quick Asset Turnover Rate: Quick Assets to Sales  \nX73 - Working capitcal Turnover Rate: Working Capital to Sales  \nX74 - Cash Turnover Rate: Cash to Sales  \nX75 - Cash Flow to Sales  \nX76 - Fixed Assets to Assets  \nX77 - Current Liability to Liability  \nX78 - Current Liability to Equity  \nX79 - Equity to Long-term Liability  \nX80 - Cash Flow to Total Assets  \nX81 - Cash Flow to Liability  \nX82 - CFO to Assets  \nX83 - Cash Flow to Equity  \nX84 - Current Liability to Current Assets  \nX85 - Liability-Assets Flag: 1 if Total Liability exceeds Total Assets, 0 otherwise  \nX86 - Net Income to Total Assets  \nX87 - Total assets to GNP price  \nX88 - No-credit Interval  \nX89 - Gross Profit to Sales  \nX90 - Net Income to Stockholder's Equity  \nX91 - Liability to Equity  \nX92 - Degree of Financial Leverage (DFL)  \nX93 - Interest Coverage Ratio (Interest expense to EBIT)  \nX94 - Net Income Flag: 1 if Net Income is Negative for the last two years, 0 otherwise  \nX95 - Equity to Liability  \n\n## Source\nDeron Liang and Chih-Fong Tsai, deronliang '@' gmail.com; cftsai '@' mgt.ncu.edu.tw, National Central University, Taiwan\nThe data was obtained from UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Taiwanese+Bankruptcy+Prediction\n\n## Relevant Papers\nLiang, D., Lu, C.-C., Tsai, C.-F., and Shih, G.-A. (2016) Financial Ratios and Corporate Governance Indicators in Bankruptcy Prediction: A Comprehensive Study. European Journal of Operational Research, vol. 252, no. 2, pp. 561-572.\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S0377221716000412","0edc41ba":"<a id=\"Preprocessing-Data\"><\/a>\n# Preprocessing Data","feccde32":"<a id=\"LightGBM-Model-Peformance \"><\/a>\n# LightGBM Model Peformance ","761ac4fa":"<a id=\"Confusion-Matrix\"><\/a>\n## Confusion Matrix","2a7726fc":"<a id=\"Bayesian-Optimization\"><\/a>\n## Bayesian Optimization","8319e58e":"<a id=\"Feature-Importance \"><\/a>\n## Feature Importance ","69897bda":"# <center>BankruptcyClassifier <\/center>\n<img src=\"https:\/\/www.popoptiq.com\/wp-content\/uploads\/2018\/06\/business-man-watching-company-go-bankrupt-pop.jpg\"\/>","2ad4f0d1":"<a id=\"ROC-Curve\"><\/a>\n## ROC Curve"}}