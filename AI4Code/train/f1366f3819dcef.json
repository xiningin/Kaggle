{"cell_type":{"12f18b23":"code","8de4db25":"code","787597ea":"code","cf1c33b7":"code","ef6217e2":"code","0ceb3994":"code","77e1a090":"code","3ce50b90":"code","b40f57c3":"code","5093eb46":"code","803975dd":"code","412442be":"code","fb165357":"code","b4c8fdae":"code","b1c8b491":"code","145a84d4":"code","120764f3":"code","815f9334":"code","196b0cf6":"code","ce02e911":"markdown","63ae3079":"markdown","82016e83":"markdown","d9b2f4df":"markdown","638bbe3a":"markdown","11f7b62b":"markdown","be99130c":"markdown","8a99338c":"markdown","e00abaf8":"markdown","fb9c91e0":"markdown","66854fd6":"markdown","aeb695f9":"markdown","26c019f4":"markdown"},"source":{"12f18b23":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","8de4db25":"# Reading Train File\ntrain_df = pd.read_csv(\"..\/input\/dont-overfit-ii\/train.csv\")\n#train_df.head()\ntrain_df = train_df.drop(\"id\", axis=1)\n# Create a data with all columns except target\nX_train_not_scaled = train_df.drop(\"target\", axis=1)\n\n# Create a target. labels dataset\ny_train = (train_df[[\"target\"]])\n\n# Reading Test File\ntest_df = pd.read_csv(\"..\/input\/dont-overfit-ii\/test.csv\")\n#test_df.head() \nX_test_not_scaled = test_df.drop(columns=['id'])","787597ea":"train_df.shape\nX_test_not_scaled.shape","cf1c33b7":"train_df.isnull().values.any()","ef6217e2":"X_test_not_scaled.isnull().values.any()","0ceb3994":"#train_df.isnull().sum().sort_values(ascending = False).head(2)","77e1a090":"#X_test_not_scaled.isnull().sum().sort_values(ascending = False).head(2)","3ce50b90":"train_df.applymap(np.isreal).values.all()","b40f57c3":"X_test_not_scaled.applymap(np.isreal).values.all()","5093eb46":"from sklearn.preprocessing import StandardScaler\n\n#Scaling Numerical columns\nstd = StandardScaler()\nX_train = std.fit_transform(X_train_not_scaled)\nX_train = pd.DataFrame(X_train)\n#X_train = X_train_not_scaled.merge(X_train,left_index=True,right_index=True,how = \"left\")\nX_train.shape","803975dd":"#Scaling Numerical columns\nstd = StandardScaler()\nX_test = std.fit_transform(X_test_not_scaled)\nX_test = pd.DataFrame(X_test)\n#X_test = X_test_not_scaled.merge(X_test,left_index=True,right_index=True,how = \"left\")\nX_test.shape","412442be":"y_train.value_counts()","fb165357":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Find best hyperparameters (roc_auc)\nrandom_state = 42\nlog_clf = LogisticRegression(solver='liblinear',random_state = random_state)\nparam_grid = {'class_weight' : ['balanced', None], \n                'penalty' : ['l2','l1'],  \n                'C' : [0.001, 0.01, 0.1, 1, 10, 100]}\n\ngrid = GridSearchCV(estimator = log_clf, param_grid = param_grid , scoring = 'roc_auc', verbose = 1, n_jobs = -1, cv = 20)\n\ngrid.fit(X_train,y_train)\n\nprint(\"Best Score:\" + str(grid.best_score_))\nprint(\"Best Parameters: \" + str(grid.best_params_))\n\nbest_parameters = grid.best_params_","b4c8fdae":"log_clf = LogisticRegression(solver='liblinear',random_state = random_state,**best_parameters)\nlog_clf.fit(X_train,y_train)","b1c8b491":"from sklearn import metrics\ny_pred_proba = log_clf.predict_proba(X_train)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_train,  y_pred_proba)\nauc = metrics.roc_auc_score(y_train, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","145a84d4":"y_pred_0_1 = log_clf.predict(X_test)\ny_pred_0_1","120764f3":"y_test_predicted_proba = log_clf.predict_proba(X_test)[::,1]\ny_test_predicted_proba","815f9334":"test_df['target'] = y_pred_0_1","196b0cf6":"test_df[['id', 'target']].to_csv('\/kaggle\/working\/submission.csv', index=False)","ce02e911":"# AUCROC \"just for showing\"","63ae3079":"# Check for Non Numaric values","82016e83":"# Importing libraries","d9b2f4df":"# Reading the Files","638bbe3a":"# Preparing Submission files","11f7b62b":"here i have used grid search to find the best scores for the logistic regression model:\n* i have used the **\"liblinear\"** solver as its very suitable at small data set as we have at the training data \n* for the grid parameters i choosed:\n  * {\"**class_wight**\"} : as the target here at the data is **not equally distributed** between 0,1 o records are bigger than 1 records 2.7 times\n  * {\"**penalty**\"} : to ckhoose the best **norm-method** for the solver in optimization of the cost function.\n  * {\"**C**\"} : to control the REGULARIZATION to avoid the OVERFITTING of the model as **C equals the reciprocal of the LAMBDA hyper parameter**. ","be99130c":"i don't know why are the compitition scores not improving in significant way !! i have tried many things in grid search of the model and data preparation and this notebook is the best scores of my work which didn't vary significantly!!, this may be due any change in the data sets from KAGGLE side and i wish to know the reason !!!","8a99338c":"# GridSearch CV and Modeling","e00abaf8":"# Missing Values","fb9c91e0":"# Test Prediction","66854fd6":"test data","aeb695f9":"# Standard Scaling","26c019f4":"train data"}}