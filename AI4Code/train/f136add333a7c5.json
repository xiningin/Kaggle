{"cell_type":{"fe37ae18":"code","c814e568":"code","2db45de2":"code","15aec066":"code","3725a35d":"code","1f3d56e7":"code","c76b8da8":"code","0068eb33":"code","7e166bc4":"code","13e062ed":"code","a3aeff48":"code","fc4b86ca":"code","2252b0f1":"code","f4537b0a":"code","81289f33":"code","f85d0a17":"code","79233957":"code","2e728447":"markdown","116b45b8":"markdown","874289df":"markdown","87bfc15d":"markdown","14a34e5c":"markdown","b53fb016":"markdown","e47ce0f0":"markdown","3adb0586":"markdown","e9eed76b":"markdown","ee6044c4":"markdown","f6153df5":"markdown","0597e539":"markdown","8ed4da90":"markdown","c0e70301":"markdown","094935c2":"markdown"},"source":{"fe37ae18":"import pandas as pd\nimport numpy as np\nfrom itertools import product, combinations\nimport gc\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.ensemble import (RandomForestClassifier\n                              , RandomForestRegressor\n                              , AdaBoostClassifier\n                              , ExtraTreesClassifier\n                             )\nfrom lightgbm import LGBMClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV","c814e568":"rand_state = 719\n\nfrom time import time\ndef timer(t): # t = beginning of timing\n    timing = time() - t\n    if timing < 60:\n        return str(round(timing,1)) + ' second(s)'\n    elif timing < 3600:\n        return str(round(timing \/ 60,1)) + ' minute(s)'\n    else:\n        return str(round(timing \/ 3600,1)) + ' hour(s)'","2db45de2":"data_path = '\/kaggle\/input\/learn-together\/'\ndef reload(x):\n    return pd.read_csv(data_path + x, index_col = 'Id')\n\ntrain = reload('train.csv')\nn_train = len(train)\ntest = reload('test.csv')\nn_test = len(test)\n\nindex_test = test.index.copy()\ny_train = train.Cover_Type.copy()\n\nall_data = train.iloc[:,train.columns != 'Cover_Type'].append(test)\nall_data['train'] = [1]*n_train + [0]*n_test\n\ndel train\ndel test","15aec066":"questionable_0 = ['Hillshade_9am', 'Hillshade_3pm']\n\ncorr_cols = {'Hillshade_9am': ['Hillshade_3pm', 'Aspect', 'Slope', 'Soil_Type10', 'Wilderness_Area1'\n                               ,'Wilderness_Area4', 'Vertical_Distance_To_Hydrology']\n            , 'Hillshade_3pm': ['Hillshade_9am', 'Hillshade_Noon', 'Slope', 'Aspect']\n            }","3725a35d":"rfr = RandomForestRegressor(n_estimators = 100, random_state = rand_state, verbose = 1, n_jobs = -1)\n\n# for col in questionable_0: \n#     print('='*20)\n#     scores = cross_val_score(rfr,\n#                              all_data_non0[corr_cols[col]], \n#                              all_data_non0[col],\n#                              n_jobs = -1)\n#     print(col + ': {0:.4} (+\/- {1:.4}) ## [{2}]'.format(scores.mean(), scores.std()*2, ', '.join(map(str, np.round(scores,4)))))\n\n# ====================\n# Hillshade_9am: 1.0 (+\/- 0.00056) ## [0.9995, 0.9993, 0.9988]\n# ====================\n# Hillshade_3pm: 1.0 (+\/- 0.0029) ## [0.9981, 0.9971, 0.9947]\n\n## NEAR PERFECT SCORES FOR ALL => no need further feature engineering for questionable_0 predictions","1f3d56e7":"for col in questionable_0:\n    print('='*20)\n    print(col)\n    all_data_0 = all_data[all_data[col] == 0].copy()\n    all_data_non0 = all_data[all_data[col] != 0].copy()\n    rfr.fit(all_data_non0[corr_cols[col]], all_data_non0[col])\n    pred = rfr.predict(all_data_0[corr_cols[col]])\n    pred_col = 'predicted_{}'.format(col)\n    \n    all_data[pred_col] = all_data[col].copy()\n    all_data.loc[all_data_0.index, pred_col] = pred\n\nfor col in questionable_0:\n    all_data['predicted_{}'.format(col)] = all_data['predicted_{}'.format(col)].apply(int)","c76b8da8":"def aspect_slope(df):\n    df['AspectSin'] = np.sin(np.radians(df.Aspect))\n    df['AspectCos'] = np.cos(np.radians(df.Aspect))\n    df['AspectSin_Slope'] = df.AspectSin * df.Slope\n    df['AspectCos_Slope'] = df.AspectCos * df.Slope\n    df['AspectSin_Slope_Abs'] = np.abs(df.AspectSin_Slope)\n    df['AspectCos_Slope_Abs'] = np.abs(df.AspectCos_Slope)\n    df['Hillshade_Mean'] = df[['Hillshade_9am',\n                              'Hillshade_Noon',\n                              'Hillshade_3pm']].apply(np.mean, axis = 1)\n    return df","0068eb33":"def distances(df):\n    horizontal = ['Horizontal_Distance_To_Fire_Points', \n                  'Horizontal_Distance_To_Roadways',\n                  'Horizontal_Distance_To_Hydrology']\n    \n    df['Euclidean_to_Hydrology'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)\n    df['EuclidHydro_Slope'] = df.Euclidean_to_Hydrology * df.Slope\n    df['Elevation_VDH_sum'] = df.Elevation + df.Vertical_Distance_To_Hydrology\n    df['Elevation_VDH_diff'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['Elevation_2'] = df.Elevation**2\n    df['Elevation_3'] = df.Elevation**3\n    df['Elevation_log1p'] = np.log1p(df.Elevation) # credit: https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\/notebook\n    \n    for col1, col2 in combinations(zip(horizontal, ['HDFP', 'HDR', 'HDH']), 2):\n        df['{0}_{1}_diff'.format(col1[1], col2[1])] = df[col1[0]] - df[col2[0]]\n        df['{0}_{1}_sum'.format(col1[1], col2[1])] = df[col1[0]] + df[col2[0]]\n    \n    df['Horizontal_sum'] = df[horizontal].sum(axis = 1)\n    return df","7e166bc4":"def OHE_to_cat(df, colname, data_range): # data_range = [min_index, max_index+1]\n    df[colname] = sum([i * df[colname + '{}'.format(i)] for i in range(data_range[0], data_range[1])])\n    return df","13e062ed":"soils = [\n    [7, 15, 8, 14, 16, 17,\n     19, 20, 21, 23], #unknow and complex \n    [3, 4, 5, 10, 11, 13],   # rubbly\n    [6, 12],    # stony\n    [2, 9, 18, 26],      # very stony\n    [1, 24, 25, 27, 28, 29, 30,\n     31, 32, 33, 34, 36, 37, 38, \n     39, 40, 22, 35], # extremely stony and bouldery\n]\nsoil_dict = {}\nfor index, soil_group in enumerate(soils):\n    for soil in soil_group:\n        soil_dict[soil] = index\n\ndef rocky(df):\n    df['Rocky'] = sum(i * df['Soil_Type' + str(i)] for i in range(1,41))\n    df['Rocky'] = df['Rocky'].map(soil_dict)\n    return df","a3aeff48":"t = time()\nall_data = aspect_slope(all_data)\nall_data = distances(all_data)\nall_data = OHE_to_cat(all_data, 'Wilderness_Area', [1,5])\nall_data = OHE_to_cat(all_data, 'Soil_Type', [1,41])\nall_data = rocky(all_data)\nall_data.drop(['Soil_Type7', 'Soil_Type15', 'train'] + questionable_0, axis = 1, inplace = True)\n\n# Important columns: https:\/\/www.kaggle.com\/hoangnguyen719\/beginner-eda-and-feature-engineering\nimportant_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology'\n                  , 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways'\n                  , 'Hillshade_Noon', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1'\n                  , 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type3', 'Soil_Type4', 'Soil_Type10'\n                  , 'predicted_Hillshade_9am', 'predicted_Hillshade_3pm', 'AspectSin', 'AspectCos'\n                  , 'AspectSin_Slope', 'AspectCos_Slope', 'AspectSin_Slope_Abs', 'AspectCos_Slope_Abs'\n                  , 'Hillshade_Mean', 'Euclidean_to_Hydrology', 'EuclidHydro_Slope'\n                  , 'Elevation_VDH_sum', 'Elevation_VDH_diff', 'Elevation_2', 'Elevation_3'\n                  , 'Elevation_log1p', 'HDFP_HDR_diff', 'HDFP_HDR_sum', 'HDFP_HDH_diff'\n                  , 'HDFP_HDH_sum', 'HDR_HDH_diff', 'HDR_HDH_sum', 'Horizontal_sum'\n                  , 'Wilderness_Area', 'Soil_Type', 'Rocky'\n                 ]\n\nall_data = all_data[important_cols]\nprint('Total data transforming time: {}'.format(timer(t)))","fc4b86ca":"X_train = all_data.iloc[:n_train,:].copy()\nX_test = all_data.iloc[n_train:, :].copy()\ndel all_data\n\ndef mem_reduce(df):\n    # credit: https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover\n    t = time()\n    start_mem = df.memory_usage().sum() \/ 1024.0**2\n    for col in df.columns:\n        if df[col].dtype=='float64': \n            df[col] = df[col].astype('float32')\n        if df[col].dtype=='int64': \n            if df[col].max()<1: df[col] = df[col].astype(bool)\n            elif df[col].max()<128: df[col] = df[col].astype('int8')\n            elif df[col].max()<32768: df[col] = df[col].astype('int16')\n            else: df[col] = df[col].astype('int32')\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Reduce from {0:.3f} MB to {1:.3f} MB (decrease by {2:.2f}%)'.format(start_mem, end_mem, \n                                                                (start_mem - end_mem)\/start_mem*100))\n    print('Total memory reduction time: {}'.format(timer(t)))\n    return df\n\nX_train = mem_reduce(X_train)\nprint('='*10)\nX_test=mem_reduce(X_test)\ngc.collect()","2252b0f1":"# RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 719\n                             , max_depth = 464\n                             , max_features = 0.3\n                             , min_samples_split = 2\n                             , min_samples_leaf = 1\n                             , bootstrap = False\n                             , verbose = 0\n                             , random_state = rand_state\n                            )\n# ExtraTreesClassifier\netc = ExtraTreesClassifier(n_estimators = 177\n                          , max_depth = 794\n                          , max_features = 0.9\n                          , min_samples_leaf = 1\n                          , min_samples_split = 2\n                          , bootstrap = False\n                          )\n\n# AdaBoostClassifier\nadac = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 19)\n                         , n_estimators = 794\n                         , learning_rate = 1\n                         )\n\n# HistGradientBoostingClassifier\nlgbc = LGBMClassifier(num_leaves = 50\n                      , max_depth = 15\n                      , learning_rate = 0.1\n                      , n_estimators = 1000\n                      , reg_lambda = 0.1\n                      , objective = 'multiclass'\n                      , num_class = 7\n                     )\n\n# KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 1\n                           , n_jobs =-1)","f4537b0a":"meta_clf = RandomForestClassifier(n_estimators = 700\n                                  , max_depth = 300\n                                  , min_samples_split = 2\n                                  , min_samples_leaf = 1\n                                  , max_features = 1\n                                  , bootstrap = False\n                                 )\nparams = {'use_probas': [True, False]\n         }\nscc = StackingCVClassifier(classifiers = [rfc, etc, adac, lgbc, knn]\n                           , meta_classifier = meta_clf\n                           , cv = 3\n                           , random_state = rand_state\n                          )\n\ngrid = GridSearchCV(estimator = scc\n                  , param_grid = params\n                  , n_jobs = -1\n                  , cv = 3\n                  , scoring = 'accuracy'\n                  , verbose = 1\n                  )\nt = time()\ngrid.fit(X_train, y_train)\nprint('Total training time: {}'.format(timer(t)))\n\npredict = grid.predict(X_test)\nprint('Best hyper-parameters found:')\nprint(grid.best_params_)\nprint('\\nFitting time:')\nprint(grid.refit_time_)\nprint('\\Best score:')\nprint(grid.best_score_)","81289f33":"results = pd.DataFrame(grid.cv_results_)\nresults.sort_values(by=['rank_test_score'], inplace=True)\nresults[['params', 'mean_fit_time','mean_test_score', 'rank_test_score']].head(10)","f85d0a17":"print('Best 10 hyper-params combinations: ')\nprint(results[['params', 'mean_fit_time','mean_test_score', 'rank_test_score']].head(10))","79233957":"output = pd.DataFrame({'Id': index_test\n                       ,'Cover_Type': predict\n                      })\noutput.to_csv('Submission.csv', index=False)","2e728447":"## III. Summary and Output","116b45b8":"## 2. Other Features\n### 2.1. Aspect, Slope & Shadow","874289df":"### 2.4. Rockiness","87bfc15d":"This kernel is part 6 of my work in this competition with other work listed below. Any comments or suggestions you may have are greatly appreciated!\n1. [EAD and Feature Engineering](https:\/\/www.kaggle.com\/hoangnguyen719\/1-eda-and-feature-engineering\/notebook) (preliminary data exploration and features reduction)\n2. [ExtraTreesClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/extratree-tuning) (hyper-parameter tuning for ExtraTree model)\n3. [AdaboostClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/adaboost-tuning) (hyper-parameter tuning for AdaBoostClassifier model)\n4. [LGBMClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/lightgbm-tuning) (hyper-parameter tuning for LightGBM Classifier model)\n5. [KNearestClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/knn-tuning) (hyper-parameter tuning for KNearestNeighbor Classifier model)\n6. [StackingCVClassifier (use_probas tuning)](https:\/\/www.kaggle.com\/hoangnguyen719\/stacking-use-probas-tuning) (stacking multiple classifier using StackingCV)\n7. [Mis-Classified Inspection](https:\/\/www.kaggle.com\/hoangnguyen719\/mis-classified-inspection) (examining mis-classified instances)","14a34e5c":"### 2.5. Combining everything","b53fb016":"*Note*: Many of my EDA parts have been greatly inspired by previous kernels in the competition and I have been trying to give credits to the owners as much as I can. However, because (1) many kernels appear to have the same ideas (even codes), which makes it hard to trace back where the ideas originated from, and (2) I carelessly forgot to note down all the sources (this is totally my bad), sometimes the credit may not be given where it's due. I apologize beforehand, and please let me know in the comment section if you have any question or suggestions. Thank you!","e47ce0f0":"### 2.2. Distances & Elevation","3adb0586":"# I. Load Data","e9eed76b":"### 3.2. Set up models\nThe hyperparameters are tuned and chosen from the following notebooks:\n- [ExtraTreesClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/extratree-tuning)\n- [AdaBoostClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/adaboost-tuning)\n- [LGBMClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/lightgbm-tuning)\n- [KNearestNeighbors tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/knn-tuning)\n- [RandomForestClassifier tuning](https:\/\/www.kaggle.com\/joshofg\/pure-random-forest-hyperparameter-tuning) (by @joshofg ).","ee6044c4":"### 3.1 Reduce data memory\nCredit to @arateris in this __[notebook](https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover)__.","f6153df5":"### 3.4. Output for submission","0597e539":"### 3.3. Hyper-params tuning for `StackingCVClassifier` 's `use_probas`\nMeta-classifier was chosen based on a separate notebook from `RandomForestClassifier`, `ExtraTreeClassifier` and `AdaboostClassifier`, among which `RandomForestClassifier` has the best performance. Due to limited kernel memory, the meta-classifier's hyper-parameters were chosen arbitrarily based on the `RandomForestClassifier` tuning result mentioned above.","8ed4da90":"Tuning's best 10 outputs","c0e70301":"# II. Feature Engineering\n## 1. Impute \"Fake\" 0s\n@arateris discovered in [this notebook](https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover) that some 0 values in _Hillshade\\_3pm_ were quite odd and actually could have been errors in data entry. He used `RandomForestRegressor` to impute these values which he believed should have been different than 0. \n<br><br>\nIn my [EAD notebook](https:\/\/www.kaggle.com\/hoangnguyen719\/eda-and-feature-engineering) I expanded the search to all other features that contain 0s and found _Hillshade\\_9am_ to suspicious. The imputation section below, which also uses `RandomForestRegressor` therefore is applied on both _Hillshade\\_9am_ and _Hillshade\\_3pm_.","094935c2":"### 2.3. Categorical"}}