{"cell_type":{"94df70fb":"code","2c66930e":"code","8ad263fe":"code","9124b4e2":"code","2e9e543a":"code","d6e70a68":"code","0ab865c1":"code","0cfd4040":"code","49c74b56":"code","b4e2e9d9":"code","7f9663ad":"code","ccee3d67":"code","58297736":"code","7a514868":"code","58b4c3a8":"code","9e89368c":"code","73c7e311":"code","4b898ce7":"code","951d529d":"code","d9369b37":"code","41bc954b":"markdown","0e6a8578":"markdown","5e18652b":"markdown","e9fee8da":"markdown","8a16f98f":"markdown","a073317c":"markdown","a8ebadbc":"markdown","47dccf6c":"markdown","a5b10152":"markdown","b97bdca2":"markdown","48e1efe4":"markdown","82b09ff4":"markdown","5097814f":"markdown","6f501b9d":"markdown","30f11138":"markdown","22a9337f":"markdown","3c168c71":"markdown","431d61c0":"markdown","61f2138f":"markdown","e1402a3b":"markdown","dee222c3":"markdown","5430cb41":"markdown","336def85":"markdown","773cd401":"markdown","609a32f5":"markdown","87a587b2":"markdown","1ffd62b0":"markdown","869e1161":"markdown","170965fe":"markdown","18b40a84":"markdown","20b0fac4":"markdown"},"source":{"94df70fb":"from IPython.display import Image\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt","2c66930e":"fashion_mnist= keras.datasets.fashion_mnist\n(xtrain_full,ytrain_full),(xtest,ytest)=fashion_mnist.load_data()","8ad263fe":"xtrain_full.shape\n","9124b4e2":"xtrain_full.dtype","2e9e543a":"xvalid,xtrain=xtrain_full[:5000]\/255.0,xtrain_full[5000:]\/255.0\nyvalid, ytrain=ytrain_full[:5000], ytrain_full[5000:]","d6e70a68":"class_names = [\"T-shirt\/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\"Sandal\", \"Shirt\",\n               \"Sneaker\", \"Bag\", \"Ankle boot\"]","0ab865c1":"print(ytrain[0])\nprint(class_names[ytrain[0]])","0cfd4040":"plt.imshow(xtrain[0])","49c74b56":"model = keras.models.Sequential([\nkeras.layers.Flatten(input_shape=[28, 28]),\nkeras.layers.Dense(400, activation=\"relu\"),\nkeras.layers.Dense(250, activation=\"relu\"),\nkeras.layers.Dense(100, activation=\"relu\"),\nkeras.layers.Dense(10, activation=\"softmax\")\n])","b4e2e9d9":"model.summary()","7f9663ad":"weights,bias = model.layers[1].get_weights()\nprint(weights.shape)\nprint(bias.shape)","ccee3d67":"weights","58297736":"bias","7a514868":"model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"sgd\",metrics=[\"accuracy\"])","58b4c3a8":"Early_stopping=keras.callbacks.EarlyStopping(patience=5)","9e89368c":"history = model.fit(xtrain,ytrain,epochs=40 , validation_data=(xvalid,yvalid),callbacks=[Early_stopping])","73c7e311":"pd.DataFrame(history.history).plot(figsize=(15,8))\nplt.grid()\nplt.show()","4b898ce7":"model.evaluate(xtest,ytest)","951d529d":"xnew=xtest[:3]\nyprob=model.predict_classes(xnew)\nl=[class_names[i] for i in yprob ]\nprint(l)","d9369b37":"\nplt.figure(figsize=(10,10))\nfor i in range(3):\n    plt.subplot(1,3,i+1)\n    xnew=xtest[i]\n    plt.imshow(xnew)\n    plt.xlabel(l[i])","41bc954b":"### Train the model:","0e6a8578":"## Libraries:","5e18652b":"### Since we are going to train the NN using Grandient Descent, we must scale the input features. Simpilicty, we just scale the pixel down to the 0-1 by divideing the photos by 255.","e9fee8da":"#### Create a lisr of items we have them in label column:","8a16f98f":"#### Before starting with training the data, let's use a method called EarlyStopping from callbacks keras.\n#### It will interrupt training when it measures no progress on the validation set for a number of epochs. So, in this case, The number of epochs can be set to a large value since training will stop automatically when there is no more progress.","a073317c":"#### the Dense layer initialized the connection weights randomly,and the biases were just initialized to zeros, which is fine.\n#### We can by \"kernel_initializer\" (kernel is another name for the matrix of connection weights) or \"bias_initializer\" create a new layer with other values.","a8ebadbc":"### Load the data:","47dccf6c":"### The neural network is trained. At each epoch during training, Keras displays the number of instances processed so far the mean training time per sample, the loss and accuracy both on the training set and the validation set. You can see that the training loss went down, which is a good sign, and the validation accuracy reached 87% after 5 epochs . With using Early_Stopping method from Callbacks keras, we can see that we only needed 32 Epochs to reatch the optimal val_acc with loss_val from a method's point of view . Val_Acc is not so far from the training accuracy, so there does not seem to be much overfitting going on at the end. With Val_accuracy=89% and training_accuracy=93%.","a5b10152":"#### we can use for loss parameter: loss=keras.losses.sparse_categorical_crossentropy. Also for optimizer: optimizer=keras.optimizer.SGD()","b97bdca2":"![image.png](attachment:image.png)","48e1efe4":"#### I will load fashine_MNIST dataset directly from Keras:","82b09ff4":"### **Note1:** because we have sparse labels (i.e):for each instance there is just a target class index, from 0 to 9 inthis case so we use \"sparse_categorical_crossentropy\". If instead we had one target probability per class for each instance [0,1] then we use: \"categorical_crossentropy\".","5097814f":"### We can accessed all the weights in all layers:","6f501b9d":"### Build the NN:","30f11138":" *   60,000 training examples.\n *   10,000 testing examples.\n *   10 classes.\n\n","22a9337f":"### Compile the model:","3c168c71":"### The fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set","431d61c0":"### You can see that both the training and validation accuracy steadily increase during training, while the training and validation loss decrease. Good! Moreover, the validation curves are \"quite\" close to the training curves, which means that there is not too much \"overfitting\". \n### At the begining of training , you can see that the val_accuracy is much better than training_Accuracy: that's normal especially, when the validation set is fairly small. However, at the end the triaining performance beats the validation performance.\n\n### If the accuracy was not satisfied, we can retune the model\u2019s hyperparameters. for example the number of layers, the number of neurons per layer, the types of activation functions we use for each hidden layer, the number of training epochs, the batch size\n\n","61f2138f":"### i will continue with this accuracy, so i will evaluate the model on the test set to estimate the generalization error before you deploy the model to production.","e1402a3b":"### *  Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\n### *  The training and test data sets have 785 columns.\n","dee222c3":"### it's known in general, that it is common to get slightly lower performance on the test set than on the validation set because the hyperparameters are tuned on the validation set, not the test set.\n","5430cb41":"### **Note2:** SGD means we will train the model using simple Stochastic Gradient Descent.","336def85":"### Note that Dense layers often have a lot of parameters. For example, the first hiddenlayer has 784 \u00d7 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting.","773cd401":"### Prediction on new instance:","609a32f5":"### Some Notes:\n#### *  each pixel takes values between (1 - 255) according to its darkness.\n#### *  Each row is a separate image.\n#### *  there is a Label column and the remaining columns are pixel numbers (784 total).","87a587b2":"## Data Description:","1ffd62b0":" 1.  T-shirt\/top\n 2.  Trouser\n 3.  Pullover\n 4.  Dress\n 5.  Coat\n 6.  Sandal\n 7.  Shirt\n 8.  Sneaker\n 9.  Bag\n 10.  Ankle boot","869e1161":"### Also, we should create a validation set:","170965fe":"# <center>  Simple Classification MLP using the Sequential API with Fashion_MINST dataset. <\/center>","18b40a84":"### Let's check the previous result by see the image:","20b0fac4":"###  Size and type of data:"}}