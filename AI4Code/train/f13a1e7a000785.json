{"cell_type":{"a8b8b9b3":"code","6f049f4c":"code","37e1eaa0":"code","3caf23b8":"code","5c96e71d":"code","837a9ea1":"code","f4dc41fe":"code","f045acf2":"code","9f365f9f":"code","9de3f51c":"code","6b2e0635":"code","876972eb":"code","ab3e87e0":"code","258bcc7b":"code","73269303":"code","c6e0be8e":"code","35ee4b2c":"code","b6b4522d":"code","6900a04e":"code","2df6b9d6":"code","8a1179e2":"code","bdcbc53c":"code","c89737f8":"code","9feb9448":"code","6c9cc906":"code","639d76ab":"code","487f9af7":"code","f1703c8d":"code","3b680e4b":"code","8c404224":"code","2058eff1":"code","12d7234c":"code","440d5d63":"code","d4fffe2c":"code","61cd4d18":"code","73aafe6e":"code","7977ba3f":"code","a19ea513":"code","fc338c5e":"code","ad7fa9c7":"code","60691617":"code","0737a48e":"code","03b3f928":"code","b056366f":"code","7d2c14d6":"code","28f3c433":"code","57fbdc07":"code","67078c9e":"code","bb5e5f3a":"code","174dc60a":"code","d57fa783":"code","a5100a0e":"code","e58e95a2":"code","f284b4a3":"code","ddc2926e":"code","519ea4c7":"code","58f013f4":"code","5c96c600":"code","d04aec60":"code","e74f484d":"code","7776dd56":"code","cc35cc35":"code","54c18418":"code","f1d0f7d1":"code","33183d24":"code","ee1b31fb":"code","db23c851":"code","0d993de6":"code","8203d172":"code","57e56769":"code","e1edd2ba":"code","ec49f6a0":"code","7170592f":"code","cc7f453a":"code","b40cd8d1":"code","dbac5606":"code","32aa7108":"code","2ccbc33a":"code","c3a46e36":"code","68281a8c":"code","e5784bd3":"code","f5ef7712":"code","705ae18c":"code","9b1cc4c8":"code","8d18d8fb":"code","acca9031":"code","fc7fea4c":"code","37a16e98":"code","4a821784":"code","52236cb8":"code","4ea3a97e":"code","8e706f37":"code","666e7ca3":"code","71c2df56":"code","a022fe56":"code","5bb2d893":"code","4ac1e4f9":"code","d1a3d85e":"code","861d07c4":"code","d9cff198":"code","de7d0f01":"code","d9538b1e":"code","b9bedd1c":"code","3e140711":"code","a2afd705":"code","93b8e7cf":"code","ced72d12":"code","51df6475":"code","f19449d2":"code","3f4c3b99":"code","f7954754":"code","625c06bc":"code","25a89617":"code","0ae5ebd4":"code","200556be":"code","34767b2a":"code","f2872af4":"code","81ee7298":"code","317d12ed":"code","53aa1ff8":"code","776a7ecc":"code","f63e7942":"code","02aff010":"code","804ee39e":"code","36b2004d":"code","561ad0cb":"code","f39f2137":"code","06cfd1f5":"code","b6d70769":"code","4f340704":"code","d88b1446":"code","baefb954":"code","ec40fe33":"code","e2ad2792":"code","6f072cfa":"code","623d71e6":"code","ca1ea34d":"code","d991603b":"code","5b4d1f0e":"code","e5456033":"code","17949b56":"code","87a6be39":"code","8bc6bed6":"code","ef0699c0":"code","046048cd":"code","61081e4e":"code","2d66ada6":"code","006db29d":"code","f81434ca":"code","38ee9f2c":"code","538216e0":"code","7636e37a":"code","08d8325e":"code","a15c2b66":"code","f35a9bde":"code","57fd0429":"code","7ccc22da":"code","afe0600c":"code","c1694fe4":"code","4bc77ee4":"code","c838f54e":"code","41bba5b6":"code","608013fa":"markdown","4a0eeca4":"markdown","6be1b329":"markdown","8c59e7eb":"markdown","4546bcc3":"markdown","ea87a1f5":"markdown","58966210":"markdown","8936e8fc":"markdown","0492d12a":"markdown","6fbed9a1":"markdown","95e99f0f":"markdown","83e58778":"markdown","8af7a2d7":"markdown","de1f262c":"markdown","e7b6da90":"markdown","36349801":"markdown","e85f1357":"markdown","8aff0118":"markdown","5d56a840":"markdown","febed900":"markdown","dd6df6e1":"markdown","4483d30a":"markdown","2e96508a":"markdown","5951cf3f":"markdown","e4ce6789":"markdown","98b6d9f6":"markdown","c2f9c852":"markdown","065086ff":"markdown","39711308":"markdown","36502f2a":"markdown","ceab7a28":"markdown","1f0f48ff":"markdown","41a3ab15":"markdown","bcaf2875":"markdown","17529c47":"markdown","bf24c1e2":"markdown","9372d090":"markdown","1f13c2cd":"markdown","87a24110":"markdown","92e6027e":"markdown","fe89be05":"markdown","ed2a321b":"markdown","fcb2a83b":"markdown","ce20daf7":"markdown","4db26626":"markdown","c51b9947":"markdown","4d650cc4":"markdown","90b80919":"markdown","358c227f":"markdown"},"source":{"a8b8b9b3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nloan = pd.read_csv(\"..\/input\/credit_assessment_data.csv\", sep=\",\")\nloan.info()","6f049f4c":"# let's look at the first few rows of the df\nloan.head()","37e1eaa0":"# Looking at all the column names\nloan.columns","3caf23b8":"# summarising number of missing values in each column\nloan.isnull().sum()","5c96e71d":"# percentage of missing values in each column\nround(loan.isnull().sum()\/len(loan.index), 2)*100","837a9ea1":"# removing the columns having more than 90% missing values\nmissing_columns = loan.columns[100*(loan.isnull().sum()\/len(loan.index)) > 90]\nprint(missing_columns)","f4dc41fe":"loan = loan.drop(missing_columns, axis=1)\nprint(loan.shape)\n\n","f045acf2":"# summarise number of missing values again\n100*(loan.isnull().sum()\/len(loan.index))","9f365f9f":"# There are now 2 columns having approx 32 and 64% missing values - \n# description and months since last delinquent\n\n# let's have a look at a few entries in the columns\nloan.loc[:, ['desc', 'mths_since_last_delinq']].head()","9de3f51c":"# dropping the two columns\nloan = loan.drop(['desc', 'mths_since_last_delinq'], axis=1)","6b2e0635":"# summarise number of missing values again\n100*(loan.isnull().sum()\/len(loan.index))","876972eb":"# missing values in rows\nloan.isnull().sum(axis=1)","ab3e87e0":"# checking whether some rows have more than 5 missing values\nlen(loan[loan.isnull().sum(axis=1) > 5].index)","258bcc7b":"loan.info()","73269303":"# The column int_rate is character type, let's convert it to float\nloan['int_rate'] = loan['int_rate'].apply(lambda x: pd.to_numeric(x.split(\"%\")[0]))","c6e0be8e":"# checking the data types\nloan.info()","35ee4b2c":"# also, lets extract the numeric part from the variable employment length\n\n# first, let's drop the missing values from the column (otherwise the regex code below throws error)\nloan = loan[~loan['emp_length'].isnull()]\n\n# using regular expression to extract numeric values from the string\nimport re\nloan['emp_length'] = loan['emp_length'].apply(lambda x: re.findall('\\d+', str(x))[0])\n\n# convert to numeric\nloan['emp_length'] = loan['emp_length'].apply(lambda x: pd.to_numeric(x))","b6b4522d":"# looking at type of the columns again\nloan.info()","6900a04e":"behaviour_var =  [\n  \"delinq_2yrs\",\n  \"earliest_cr_line\",\n  \"inq_last_6mths\",\n  \"open_acc\",\n  \"pub_rec\",\n  \"revol_bal\",\n  \"revol_util\",\n  \"total_acc\",\n  \"out_prncp\",\n  \"out_prncp_inv\",\n  \"total_pymnt\",\n  \"total_pymnt_inv\",\n  \"total_rec_prncp\",\n  \"total_rec_int\",\n  \"total_rec_late_fee\",\n  \"recoveries\",\n  \"collection_recovery_fee\",\n  \"last_pymnt_d\",\n  \"last_pymnt_amnt\",\n  \"last_credit_pull_d\",\n  \"application_type\"]\nbehaviour_var","2df6b9d6":"# let's now remove the behaviour variables from analysis\ndf = loan.drop(behaviour_var, axis=1)\ndf.info()","8a1179e2":"# also, we will not be able to use the variables zip code, address, state etc.\n# the variable 'title' is derived from the variable 'purpose'\n# thus let get rid of all these variables as well\n\ndf = df.drop(['title', 'url', 'zip_code', 'addr_state'], axis=1)","bdcbc53c":"df['loan_status'] = df['loan_status'].astype('category')\ndf['loan_status'].value_counts()","c89737f8":"# filtering only fully paid or charged-off\ndf = df[df['loan_status'] != 'Current']\ndf['loan_status'] = df['loan_status'].apply(lambda x: 0 if x=='Fully Paid' else 1)\n\n# converting loan_status to integer type\ndf['loan_status'] = df['loan_status'].apply(lambda x: pd.to_numeric(x))\n\n# summarising the values\ndf['loan_status'].value_counts()","9feb9448":"# default rate\nround(np.mean(df['loan_status']), 2)","6c9cc906":"# plotting default rates across grade of the loan\nsns.barplot(x='grade', y='loan_status', data=df)\nplt.show()","639d76ab":"# lets define a function to plot loan_status across categorical variables\ndef plot_cat(cat_var):\n    sns.barplot(x=cat_var, y='loan_status', data=df)\n    plt.show()\n    ","487f9af7":"# compare default rates across grade of loan\nplot_cat('grade')","f1703c8d":"# term: 60 months loans default more than 36 months loans\nplot_cat('term')","3b680e4b":"# sub-grade: as expected - A1 is better than A2 better than A3 and so on \nplt.figure(figsize=(16, 6))\nplot_cat('sub_grade')","8c404224":"# home ownership: not a great discriminator\nplot_cat('home_ownership')","2058eff1":"# verification_status: surprisingly, verified loans default more than not verifiedb\nplot_cat('verification_status')","12d7234c":"# purpose: small business loans defualt the most, then renewable energy and education\nplt.figure(figsize=(16, 6))\nplot_cat('purpose')","440d5d63":"# let's also observe the distribution of loans across years\n# first lets convert the year column into datetime and then extract year and month from it\ndf['issue_d'].head()","d4fffe2c":"from datetime import datetime\ndf['issue_d'] = df['issue_d'].apply(lambda x: datetime.strptime(x, '%b-%y'))\n","61cd4d18":"# extracting month and year from issue_date\ndf['month'] = df['issue_d'].apply(lambda x: x.month)\ndf['year'] = df['issue_d'].apply(lambda x: x.year)\n\n\n","73aafe6e":"# let's first observe the number of loans granted across years\ndf.groupby('year').year.count()","7977ba3f":"# number of loans across months\ndf.groupby('month').month.count()","a19ea513":"# lets compare the default rates across years\n# the default rate had suddenly increased in 2011, inspite of reducing from 2008 till 2010\nplot_cat('year')","fc338c5e":"# comparing default rates across months: not much variation across months\nplt.figure(figsize=(16, 6))\nplot_cat('month')","ad7fa9c7":"# loan amount: the median loan amount is around 10,000\nsns.distplot(df['loan_amnt'])\nplt.show()","60691617":"# binning loan amount\ndef loan_amount(n):\n    if n < 5000:\n        return 'low'\n    elif n >=5000 and n < 15000:\n        return 'medium'\n    elif n >= 15000 and n < 25000:\n        return 'high'\n    else:\n        return 'very high'\n        \ndf['loan_amnt'] = df['loan_amnt'].apply(lambda x: loan_amount(x))\n","0737a48e":"df['loan_amnt'].value_counts()","03b3f928":"# let's compare the default rates across loan amount type\n# higher the loan amount, higher the default rate\nplot_cat('loan_amnt')","b056366f":"# let's also convert funded amount invested to bins\ndf['funded_amnt_inv'] = df['funded_amnt_inv'].apply(lambda x: loan_amount(x))","7d2c14d6":"# funded amount invested\nplot_cat('funded_amnt_inv')","28f3c433":"# lets also convert interest rate to low, medium, high\n# binning loan amount\ndef int_rate(n):\n    if n <= 10:\n        return 'low'\n    elif n > 10 and n <=15:\n        return 'medium'\n    else:\n        return 'high'\n    \n    \ndf['int_rate'] = df['int_rate'].apply(lambda x: int_rate(x))","57fbdc07":"# comparing default rates across rates of interest\n# high interest rates default more, as expected\nplot_cat('int_rate')","67078c9e":"# debt to income ratio\ndef dti(n):\n    if n <= 10:\n        return 'low'\n    elif n > 10 and n <=20:\n        return 'medium'\n    else:\n        return 'high'\n    \n\ndf['dti'] = df['dti'].apply(lambda x: dti(x))","bb5e5f3a":"# comparing default rates across debt to income ratio\n# high dti translates into higher default rates, as expected\nplot_cat('dti')","174dc60a":"# funded amount\ndef funded_amount(n):\n    if n <= 5000:\n        return 'low'\n    elif n > 5000 and n <=15000:\n        return 'medium'\n    else:\n        return 'high'\n    \ndf['funded_amnt'] = df['funded_amnt'].apply(lambda x: funded_amount(x))","d57fa783":"plot_cat('funded_amnt')\n","a5100a0e":"# installment\ndef installment(n):\n    if n <= 200:\n        return 'low'\n    elif n > 200 and n <=400:\n        return 'medium'\n    elif n > 400 and n <=600:\n        return 'high'\n    else:\n        return 'very high'\n    \ndf['installment'] = df['installment'].apply(lambda x: installment(x))","e58e95a2":"# comparing default rates across installment\n# the higher the installment amount, the higher the default rate\nplot_cat('installment')","f284b4a3":"# annual income\ndef annual_income(n):\n    if n <= 50000:\n        return 'low'\n    elif n > 50000 and n <=100000:\n        return 'medium'\n    elif n > 100000 and n <=150000:\n        return 'high'\n    else:\n        return 'very high'\n\ndf['annual_inc'] = df['annual_inc'].apply(lambda x: annual_income(x))","ddc2926e":"# annual income and default rate\n# lower the annual income, higher the default rate\nplot_cat('annual_inc')","519ea4c7":"# employment length\n# first, let's drop the missing value observations in emp length\ndf = df[~df['emp_length'].isnull()]\n\n# binning the variable\ndef emp_length(n):\n    if n <= 1:\n        return 'fresher'\n    elif n > 1 and n <=3:\n        return 'junior'\n    elif n > 3 and n <=7:\n        return 'senior'\n    else:\n        return 'expert'\n\ndf['emp_length'] = df['emp_length'].apply(lambda x: emp_length(x))","58f013f4":"# emp_length and default rate\n# not much of a predictor of default\nplot_cat('emp_length')","5c96c600":"# purpose: small business loans defualt the most, then renewable energy and education\nplt.figure(figsize=(16, 6))\nplot_cat('purpose')","d04aec60":"# lets first look at the number of loans for each type (purpose) of the loan\n# most loans are debt consolidation (to repay otehr debts), then credit card, major purchase etc.\nplt.figure(figsize=(16, 6))\nsns.countplot(x='purpose', data=df)\nplt.show()","e74f484d":"# filtering the df for the 4 types of loans mentioned above\nmain_purposes = [\"credit_card\",\"debt_consolidation\",\"home_improvement\",\"major_purchase\"]\ndf = df[df['purpose'].isin(main_purposes)]\ndf['purpose'].value_counts()","7776dd56":"# plotting number of loans by purpose \nsns.countplot(x=df['purpose'])\nplt.show()","cc35cc35":"# let's now compare the default rates across two types of categorical variables\n# purpose of loan (constant) and another categorical variable (which changes)\n\nplt.figure(figsize=[10, 6])\nsns.barplot(x='term', y=\"loan_status\", hue='purpose', data=df)\nplt.show()\n","54c18418":"# lets write a function which takes a categorical variable and plots the default rate\n# segmented by purpose \n\ndef plot_segmented(cat_var):\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=cat_var, y='loan_status', hue='purpose', data=df)\n    plt.show()\n\n    \nplot_segmented('term')","f1d0f7d1":"# grade of loan\nplot_segmented('grade')","33183d24":"# home ownership\nplot_segmented('home_ownership')","ee1b31fb":"# year\nplot_segmented('year')","db23c851":"# emp_length\nplot_segmented('emp_length')","0d993de6":"# loan_amnt: same trend across loan purposes\nplot_segmented('loan_amnt')","8203d172":"# interest rate\nplot_segmented('int_rate')","57e56769":"# installment\nplot_segmented('installment')","e1edd2ba":"# debt to income ratio\nplot_segmented('dti')","ec49f6a0":"# annual income\nplot_segmented('annual_inc')","7170592f":"# variation of default rate across annual_inc\ndf.groupby('annual_inc').loan_status.mean().sort_values(ascending=False)","cc7f453a":"# one can write a function which takes in a categorical variable and computed the average \n# default rate across the categories\n# It can also compute the 'difference between the highest and the lowest default rate' across the \n# categories, which is a decent metric indicating the effect of the varaible on default rate\n\ndef diff_rate(cat_var):\n    default_rates = df.groupby(cat_var).loan_status.mean().sort_values(ascending=False)\n    return (round(default_rates, 2), round(default_rates[0] - default_rates[-1], 2))\n\ndefault_rates, diff = diff_rate('annual_inc')\nprint(default_rates) \nprint(diff)\n","b40cd8d1":"# filtering all the object type variables\ndf_categorical = df.loc[:, df.dtypes == object]\ndf_categorical['loan_status'] = df['loan_status']\n\n# Now, for each variable, we can compute the incremental diff in default rates\nprint([i for i in df.columns])","dbac5606":"# storing the diff of default rates for each column in a dict\nd = {key: diff_rate(key)[1]*100 for key in df_categorical.columns if key != 'loan_status'}\nprint(d)","32aa7108":"df.shape","2ccbc33a":"df['loan_status'].value_counts()","c3a46e36":"df = df.dropna()","68281a8c":"df.isnull().sum()","e5784bd3":"df.head()","f5ef7712":"df.info()","705ae18c":"# drop id,member_id,emp_title\ndf.drop(['id','member_id','emp_title'],axis=1,inplace=True)","9b1cc4c8":"# import required libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom imblearn.metrics import sensitivity_specificity_support\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC","8d18d8fb":"le = preprocessing.LabelEncoder()","acca9031":"df = df.apply(le.fit_transform)","fc7fea4c":"df.head()","37a16e98":"X = df.drop('loan_status',axis=1)\ny = df.loan_status\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25, random_state = 4, stratify = y)","4a821784":"# print shapes of train and test sets\nX_train.shape\ny_train.shape\nX_test.shape\ny_test.shape","52236cb8":"import statsmodels.api as sm","4ea3a97e":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","8e706f37":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","666e7ca3":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)","71c2df56":"rfe.support_","a022fe56":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","5bb2d893":"col = X_train.columns[rfe.support_]","4ac1e4f9":"X_train.columns[~rfe.support_]","d1a3d85e":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","861d07c4":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","d9cff198":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","de7d0f01":"y_train_pred_final = pd.DataFrame({'loan_status':y_train.values, 'loan_status_prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train.index\ny_train_pred_final.head()","d9538b1e":"y_train_pred_final['predicted'] = y_train_pred_final.loan_status_prob.map(lambda x: 1 if x > 0.3 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","b9bedd1c":"from sklearn import metrics","3e140711":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.loan_status, y_train_pred_final.predicted )\nprint(confusion)","a2afd705":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.loan_status, y_train_pred_final.predicted))","93b8e7cf":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","ced72d12":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","51df6475":"col = col.drop('issue_d',1)\ncol","f19449d2":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","3f4c3b99":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","f7954754":"y_train_pred[:10]","625c06bc":"y_train_pred_final['loan_status_prob'] = y_train_pred","25a89617":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.loan_status_prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","0ae5ebd4":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.loan_status, y_train_pred_final.predicted))","200556be":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","34767b2a":"# Let's drop TotalCharges since it has a high VIF\ncol = col.drop('sub_grade')\ncol","f2872af4":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","81ee7298":"y_train_pred[:10]","317d12ed":"y_train_pred_final['loan_status_prob'] = y_train_pred","53aa1ff8":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.loan_status_prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","776a7ecc":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.loan_status, y_train_pred_final.predicted))","f63e7942":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","02aff010":"# Let's drop TotalCharges since it has a high VIF\ncol = col.drop('year')\ncol","804ee39e":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","36b2004d":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","561ad0cb":"y_train_pred[:10]","f39f2137":"y_train_pred_final['loan_status_prob'] = y_train_pred","06cfd1f5":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.loan_status_prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","b6d70769":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.loan_status,y_train_pred_final.predicted))","4f340704":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","d88b1446":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","baefb954":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","ec40fe33":"# Let us calculate specificity\nTN \/ float(TN+FP)","e2ad2792":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","6f072cfa":"# positive predictive value \nprint (TP \/ float(TP+FP))","623d71e6":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","ca1ea34d":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","d991603b":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.loan_status, y_train_pred_final.loan_status_prob, drop_intermediate = False )","5b4d1f0e":"draw_roc(y_train_pred_final.loan_status, y_train_pred_final.loan_status_prob)","e5456033":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.loan_status_prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","17949b56":"# apply pca to train data\npca = Pipeline([('scaler', StandardScaler()), ('pca', PCA())])","87a6be39":"pca.fit(X_train)\nchurn_pca = pca.fit_transform(X_train)","8bc6bed6":"# extract pca model from pipeline\npca = pca.named_steps['pca']\n\n# look at explainded variance of PCA components\nprint(pd.Series(np.round(pca.explained_variance_ratio_.cumsum(), 4)*100))","ef0699c0":"# plot feature variance\nfeatures = range(pca.n_components_)\ncumulative_variance = np.round(np.cumsum(pca.explained_variance_ratio_)*100, decimals=4)\nplt.figure(figsize=(175\/20,100\/20)) # 100 elements on y-axis; 175 elements on x-axis; 20 is normalising factor\nplt.plot(cumulative_variance)","046048cd":"# create pipeline\nPCA_VARS = 10\nsteps = [('scaler', StandardScaler()),\n         (\"pca\", PCA(n_components=PCA_VARS)),\n         (\"logistic\", LogisticRegression(class_weight='balanced'))\n        ]\npipeline = Pipeline(steps)","61081e4e":"# fit model\npipeline.fit(X_train, y_train)\n\n# check score on train data\npipeline.score(X_train, y_train)","2d66ada6":"# predict churn on test data\ny_pred = pipeline.predict(X_test)\n\n# create onfusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n# check sensitivity and specificity\nsensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n\n# check area under curve\ny_pred_prob = pipeline.predict_proba(X_test)[:, 1]\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))","006db29d":"# class imbalance\ny_train.value_counts()\/y_train.shape","f81434ca":"# PCA\npca = PCA()\n\n# logistic regression - the class weight is used to handle class imbalance - it adjusts the cost function\nlogistic = LogisticRegression(class_weight={0:0.1, 1: 0.9})\n\n# create pipeline\nsteps = [(\"scaler\", StandardScaler()), \n         (\"pca\", pca),\n         (\"logistic\", logistic)\n        ]\n\n# compile pipeline\npca_logistic = Pipeline(steps)\n\n# hyperparameter space\nparams = {'pca__n_components': [10, 15], 'logistic__C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n\n# create 5 folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# create gridsearch object\nmodel = GridSearchCV(estimator=pca_logistic, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)","38ee9f2c":"# fit model\nmodel.fit(X_train, y_train)","538216e0":"# cross validation results\npd.DataFrame(model.cv_results_)","7636e37a":"# print best hyperparameters\nprint(\"Best AUC: \", model.best_score_)\nprint(\"Best hyperparameters: \", model.best_params_)","08d8325e":"# predict churn on test data\ny_pred = model.predict(X_test)\n\n# create onfusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n# check sensitivity and specificity\nsensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n\n# check area under curve\ny_pred_prob = model.predict_proba(X_test)[:, 1]\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))","a15c2b66":"# random forest - the class weight is used to handle class imbalance - it adjusts the cost function\nforest = RandomForestClassifier(class_weight={0:0.1, 1: 0.9}, n_jobs = -1)\n\n# hyperparameter space\nparams = {\"criterion\": ['gini', 'entropy'], \"max_features\": ['auto', 0.4]}\n\n# create 5 folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# create gridsearch object\nmodel = GridSearchCV(estimator=forest, cv=folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)","f35a9bde":"# fit model\nmodel.fit(X_train, y_train)","57fd0429":"# print best hyperparameters\nprint(\"Best AUC: \", model.best_score_)\nprint(\"Best hyperparameters: \", model.best_params_)","7ccc22da":"# predict churn on test data\ny_pred = model.predict(X_test)\n\n# create onfusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n# check sensitivity and specificity\nsensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n\n# check area under curve\ny_pred_prob = model.predict_proba(X_test)[:, 1]\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))","afe0600c":"# run a random forest model on train data\nmax_features = int(round(np.sqrt(X_train.shape[1])))    # number of variables to consider to split each node\nprint(max_features)\n\nrf_model = RandomForestClassifier(n_estimators=100, max_features=max_features, class_weight={0:0.1, 1: 0.9}, oob_score=True, random_state=4, verbose=1)","c1694fe4":"# fit model\nrf_model.fit(X_train, y_train)","4bc77ee4":"# OOB score\nrf_model.oob_score_","c838f54e":"# predict churn on test data\ny_pred = rf_model.predict(X_test)\n\n# create onfusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n# check sensitivity and specificity\nsensitivity, specificity, _ = sensitivity_specificity_support(y_test, y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n\n# check area under curve\ny_pred_prob = rf_model.predict_proba(X_test)[:, 1]\nprint(\"ROC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))","41bba5b6":"# predictors\nfeatures = df.drop('loan_status', axis=1).columns\n\n# feature_importance\nimportance = rf_model.feature_importances_\n\n# create dataframe\nfeature_importance = pd.DataFrame({'variables': features, 'importance_percentage': importance*100})\nfeature_importance = feature_importance[['variables', 'importance_percentage']]\n\n# sort features\nfeature_importance = feature_importance.sort_values('importance_percentage', ascending=False).reset_index(drop=True)\nprint(\"Sum of importance=\", feature_importance.importance_percentage.sum())\nfeature_importance","608013fa":"### Hyperparameter tuning - PCA and Logistic Regression","4a0eeca4":"Clearly, as the grade of loan goes from A to G, the default rate increases. This is expected because the grade is decided by Lending Club based on the riskiness of the loan. ","6be1b329":"### Plotting the ROC Curve","8c59e7eb":"The data looks clean by and large. Let's also check whether all columns are in the correct format.","4546bcc3":"A good way to quantify th effect of a categorical variable on default rate is to see 'how much does the default rate vary across the categories'. \n\nLet's see an example using annual_inc as the categorical variable.","ea87a1f5":"There are a few variables with high VIF. It's best to drop these variables as they aren't helping much with prediction and unnecessarily making the model complex. The variable 'PhoneService' has the highest VIF. So let's start by dropping that.","58966210":"Thus, there is a 6% increase in default rate as you go from high to low annual income. We can compute this difference for all the variables and roughly identify the ones that affect default rate the most.","8936e8fc":"In general, debt consolidation loans have the highest default rates. Lets compare across other categories as well.","0492d12a":"You can see that the number of loans has increased steadily across years. ","6fbed9a1":"#### Creating new column 'predicted' with 1 if loan_status_prob > 0.5 else 0","95e99f0f":"### Feature Selection using RFE","83e58778":"# Data Cleaning\n\nSome columns have a large number of missing values, let's first fix the missing values and then check for other types of data quality problems.","8af7a2d7":"Next, let's start with univariate analysis and then move to bivariate analysis.\n\n## Univariate Analysis\n\nFirst, let's look at the overall default rate.\n","de1f262c":"You can see that fully paid comprises most of the loans. The ones marked 'current' are neither fully paid not defaulted, so let's get rid of the current loans. Also, let's tag the other two values as 0 or 1. ","e7b6da90":"Some of the important columns in the dataset are loan_amount, term, interest rate, grade, sub grade, annual income, purpose of the loan etc.\n\nThe **target variable**, which we want to compare across the independent variables, is loan status. The strategy is to figure out compare the average default rates across various independent variables and identify the  ones that affect default rate the most.\n\n","36349801":"### Checking VIF again","e85f1357":"There are some more columns with missing values, but let's ignore them for now (since we are not doing any modeling, we don't need to impute all missing values anyway). \n\nBut let's check whether some rows have a large number of missing values.","8aff0118":"Next, let's have a look at the target variable - loan_status. We need to relabel the values to a binary form - 0 or 1, 1 indicating that the person has defaulted and 0 otherwise.\n\n","5d56a840":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","febed900":"### Choosing Best Features","dd6df6e1":"~ 12 components explain 90% variance\n\n~ 15 components explain 95% variance","4483d30a":"### PCA","2e96508a":"Typically, variables such as acc_now_delinquent, chargeoff within 12 months etc. (which are related to the applicant's past loans) are available from the credit bureau. ","5951cf3f":"Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","e4ce6789":"### Random Forest","98b6d9f6":"## Data Understanding","c2f9c852":"### PCA and Logistic Regression","065086ff":"### Assessing the model with StatsModel","39711308":"In the upcoming analyses, we will segment the loan applications across the purpose of the loan, since that is a variable affecting many other variables - the type of applicant, interest rate, income, and finally the default rate. ","36502f2a":"The column description contains the comments the applicant had written while applying for the loan. Although one can use some text analysis techniques to derive new features from this column (such as sentiment, number of positive\/negative words etc.), we will not use this column in this analysis. \n\nThere is an important reason we shouldn't use this column in analysis - since at the time of loan application, we will not have this data (it gets generated months after the loan has been approved), it cannot be used as a predictor of default at the time of loan approval. \n\nThus let's drop the two columns.","ceab7a28":"\n\nThe analysis is divided into four main parts:\n1. Data understanding \n2. Data cleaning (cleaning missing values, removing redundant columns etc.)\n3. Data Analysis \n4. Recommendations\n","1f0f48ff":"You can see that many columns have 100% missing values, some have 65%, 33% etc. First, let's get rid of the columns having 100% missing values.","41a3ab15":"## Segmented Univariate Analysis\n\nWe have now compared the default rates across various variables, and some of the important predictors are purpose of the loan, interest rate, annual income, grade etc.\n\nIn the credit industry, one of the most important factors affecting default is the purpose of the loan - home loans perform differently than credit cards, credit cards are very different from debt consolidation loans etc. \n\nThis comes from business understanding, though let's again have a look at the default rates across the purpose of the loan.\n","bcaf2875":"## Model Building","17529c47":"### Let's now check the VIFs again","bf24c1e2":"Let's first visualise the average default rates across categorical variables.\n","9372d090":"So ,Overall accuracy increases","1f13c2cd":"#### Checking VIFs","87a24110":"Let's analyse the top 4 types of loans based on purpose: consolidation, credit card, home improvement and major purchase.","92e6027e":"Let's now analyse how the default rate varies across continuous variables.","fe89be05":"### Metrics beyond simply accuracy","ed2a321b":"The overall default rate is about 14%.  ","fcb2a83b":"### Making Prediction","ce20daf7":"The easiest way to analyse how default rates vary across continous variables is to bin the variables into discrete categories.\n\nLet's bin the loan amount variable into small, medium, high, very high.","4db26626":"Most loans are granted in December, and in general in the latter half of the year.","c51b9947":"## Data Analysis\n\nLet's now move to data analysis. To start with, let's understand the objective of the analysis clearly and identify the variables that we want to consider for analysis. \n\nThe objective is to identify predictors of default so that at the time of loan application, we can use those variables for approval\/rejection of the loan. Now, there are broadly three types of variables - 1. those which are related to the applicant (demographic variables such as age, occupation, employment details etc.), 2. loan characteristics (amount of loan, interest rate, purpose of loan etc.) and 3. Customer behaviour variables (those which are generated after the loan is approved such as delinquent 2 years, revolving balance, next payment date etc.).\n\nNow, the customer behaviour variables are not available at the time of loan application, and thus they cannot be used as predictors for credit approval. \n\nThus, going forward, we will use only the other two types of variables.\n\n","4d650cc4":"### Evaluate on test data","90b80919":"### Feature Importance","358c227f":"### Finding Optimal CutOff point"}}