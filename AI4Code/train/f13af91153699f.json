{"cell_type":{"70ad1238":"code","6740d160":"code","259c9872":"code","1f2b5564":"code","2e3fe8e7":"code","fd733ad7":"code","85297bfd":"code","bf2c7730":"code","d239a837":"code","5543326e":"code","cfcc448c":"code","71c9379a":"code","e8c153e0":"code","3cfc19c6":"code","8afe5884":"code","43438d6a":"code","cbd10cd3":"code","9c3ba279":"code","6b4bed51":"code","2b0181da":"code","908aeb5a":"code","52c8b20b":"code","162fd19b":"code","c1274c28":"code","e6a4d673":"code","6821d954":"code","770fed51":"code","0f94c653":"code","eb502461":"code","e0b3c187":"code","ba9000fb":"code","0a92c976":"code","ecfc0424":"code","9ff3281d":"code","31e54a09":"code","7b3d2eb3":"code","b4b03ed6":"code","1511903a":"code","96f7279d":"code","8eff0c3d":"code","7d770e05":"code","83f1955a":"code","183e865e":"code","8b006501":"code","bbacef3e":"code","7a79cc8f":"code","d9263c6b":"code","1ae448d1":"code","28ed3365":"markdown","b55e3af1":"markdown","3133276c":"markdown","e1c299b6":"markdown","79ed8e96":"markdown","0d86e210":"markdown","0f375a01":"markdown","1c5c9a09":"markdown","efc26d26":"markdown","ff7b9d7b":"markdown","c4df96ae":"markdown","918942af":"markdown","4ba33c53":"markdown","d7d94f97":"markdown","edbcb8f6":"markdown","03efae37":"markdown","73263743":"markdown","39cf40cf":"markdown","3b115096":"markdown","8cf52984":"markdown","0b9fe0d7":"markdown","a71fa870":"markdown"},"source":{"70ad1238":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6740d160":"import cv2, os\nimport numpy as np\nimport seaborn as sns\nimport urllib.request as lib\nimport requests\nimport matplotlib.pyplot as plt","259c9872":"cv2.__version__","1f2b5564":"#image = lib.urlopen('http:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2015\/01\/google_logo.png')\n!wget https:\/\/ae01.alicdn.com\/kf\/HTB1TVk9SFXXXXabXXXXq6xXFXXX3\/Fashion-Wristwatch-New-Wrist-Watch-Men-Watches-Top-Brand-Luxury-Famous-Quartz-Watch-for-Men-Male.jpg","2e3fe8e7":"os.listdir()","fd733ad7":"img = cv2.imread(\"Fashion-Wristwatch-New-Wrist-Watch-Men-Watches-Top-Brand-Luxury-Famous-Quartz-Watch-for-Men-Male.jpg\",1)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.figure(figsize = (12,8))\nplt.title(\"Image of a wrist-watch\", size = 20)\nplt.imshow(img)\nplt.xticks([]), plt.yticks([])","85297bfd":"sns.distplot(img.flatten(),kde = False)","bf2c7730":"img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\nret,thresh1 = cv2.threshold(img_gray, 130, 255, cv2.THRESH_BINARY)\n\nplt.figure(figsize = (12,8))\nplt.imshow(thresh1)","d239a837":"img.shape","5543326e":"sobelx = cv2.Sobel(img,int(cv2.CV_64F),1,0,ksize=3) #ksize=3 means we'll be using the 3x3 Sobel filter\nsobely = cv2.Sobel(img,int(cv2.CV_64F),0,1,ksize=3)\n\n#To plot the vertical and horizontal edge detectors side by side\nplt.figure(figsize=(14,16))\nplt.subplot(2,2,1)\nplt.imshow(sobelx, cmap = 'gray')\nplt.title('Sobel X (vertical edges)')\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(2,2,2)\nplt.imshow(sobely, cmap = 'gray')\nplt.xticks([])\nplt.yticks([])\nplt.title('Sobel Y (horizontal edges)')","cfcc448c":"height, width = img.shape[0:2]","71c9379a":"startRow = int(height*.12)\nstartCol = int(width*.12)\nendRow = int(height*.90)\nendCol = int(width*.90)","e8c153e0":"cropped_image = img[startRow:endRow, startCol:endCol]","3cfc19c6":"plt.figure(figsize = (12,18))\n\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.title(\"Original Image\", size = 20)\nplt.xticks([]), plt.yticks([])\n\nplt.subplot(1,2,2)\nplt.imshow(cropped_image)\nplt.title(\"Cropped Image\", size = 20)\nplt.xticks([]), plt.yticks([])","8afe5884":"newImg = cv2.resize(img, (500, 750))\nplt.figure(figsize = (12,8))\nplt.imshow(newImg)","43438d6a":"plt.figure(figsize = (14,16))\n\nplt.subplot(1,3,1)\nplt.title(\"Original Image\", size = 20)\nplt.imshow(img)\nplt.xticks([]),plt.yticks([])\n\nplt.subplot(1,3,2)\ncontrast_img = cv2.addWeighted(img, 3, np.zeros(img.shape, img.dtype), 0, 0)\nplt.title(\"Contrasted Image\", size = 20)\nplt.imshow(contrast_img)\nplt.xticks([]),plt.yticks([])\n\nplt.subplot(1,3,3)\ncontrast_img = cv2.addWeighted(img, 1, np.zeros(img.shape, img.dtype), 0, 127)\nplt.title(\"Image with high brightness & 0 contrast\", size = 15)\nplt.imshow(contrast_img)\nplt.xticks([]),plt.yticks([])","cbd10cd3":"cont_gray = cv2.cvtColor(contrast_img, cv2.COLOR_RGB2GRAY)\nequ = cv2.equalizeHist(cont_gray)\nimproved = cv2.cvtColor(equ, cv2.COLOR_GRAY2RGB)\nclahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(5,5))\ncl1 = clahe.apply(cont_gray)\n\nplt.figure(figsize = (14,20))\n\nplt.subplot(1,3,1)\nplt.imshow(cont_gray)\nplt.title(\"Original Gray Image\",size = 16)\nplt.xticks([]), plt.yticks([])\n\nplt.subplot(1,3,2)\nplt.imshow(equ)\nplt.title(\"After equalizing\", size = 16)\nplt.xticks([]), plt.yticks([])\n\nplt.subplot(1,3,3)\nplt.imshow(cl1)\nplt.title(\"After equalizing using CLAHE\", size = 16)\nplt.xticks([]), plt.yticks([])","9c3ba279":"plt.figure(figsize = (16,5))\n\nplt.subplot(1,3,1)\nsns.distplot(contrast_img.flatten(), kde = False)\nplt.title(\"Histogram of contrasted Image\", size = 16)\n\nplt.subplot(1,3,2)\nsns.distplot(equ.flatten(), kde = False, color = \"lime\")\nplt.title(\"Histogram after equalizing\", size = 16)\n\nplt.subplot(1,3,3)\nsns.distplot(cl1.flatten(), kde = False, color = \"crimson\")\nplt.title(\"Equalizing using CLAHE\", size = 16)","6b4bed51":"blur_img_1 = cv2.GaussianBlur(img, (7,7), cv2.BORDER_REFLECT_101)\nblur_img_2 = cv2.GaussianBlur(img, (11,11), cv2.BORDER_REFLECT_101)\nplt.figure(figsize = (14,18))\n\nplt.subplot(1,2,1)\nplt.title(\"Gausian Blurred image\", size = 20)\nplt.imshow(blur_img_1)\nplt.xticks([]), plt.yticks([])\n\nplt.subplot(1,2,2)\nplt.title(\"Gausian Blurred with high kernel size\", size = 20)\nplt.imshow(blur_img_2)\nplt.xticks([]), plt.yticks([])","2b0181da":"edge = cv2.Laplacian(img, -1, ksize = 5, scale = 1, delta = 0, borderType = cv2.BORDER_DEFAULT)\nplt.figure(figsize = (12,8))\nplt.title(\"Laplacian High Pass Filter\", size = 20)\nplt.imshow(edge)\nplt.xticks([]), plt.yticks([])","908aeb5a":"sns.distplot(edge.flatten(), kde = False)","52c8b20b":"med_blur_image = cv2.medianBlur(img,5)\nplt.figure(figsize = (12,8))\nplt.title(\"Blurring using Median technique\", size = 20)\nplt.imshow(med_blur_image)","162fd19b":"plt.figure(figsize = (18, 24))\n\nplt.subplot(1,3,1)\nedge_img = cv2.Canny(img,100,200)\nplt.imshow(edge_img)\nplt.title(\"Edge detection\", size = 20)\nplt.xticks([]), plt.yticks([])\n\nplt.subplot(1,3,2)\nplt.imshow(img)\nplt.title(\"Original Image\", size = 20)\nplt.xticks([]), plt.yticks([])\n\nplt.subplot(1,3,3)\nedge_img_2 = cv2.Canny(img,50,100)\nplt.imshow(edge_img_2)\nplt.title(\"Gradient thresh changed\", size = 14)\nplt.xticks([]), plt.yticks([])","c1274c28":"!wget https:\/\/thumbs.dreamstime.com\/z\/adult-man-wearing-jeans-plaid-shirt-red-cap-isolated-white-background-36005651.jpg","e6a4d673":"os.listdir()","6821d954":"img2 = cv2.imread(\"adult-man-wearing-jeans-plaid-shirt-red-cap-isolated-white-background-36005651.jpg\",-1)\nimg_rgb = cv2.cvtColor(img2,cv2.COLOR_BGR2RGB)","770fed51":"plt.figure(figsize = (16,22))\n\nplt.subplot(1,3,1)\nplt.title(\"Image in RGB format\", size = 20)\nplt.imshow(img_rgb)\nplt.xticks([]),plt.yticks([])\n\nplt.subplot(1,3,2)\nplt.title(\"Image in BGR format\", size = 20)\nplt.imshow(img2)\nplt.xticks([]),plt.yticks([])\n\nplt.subplot(1,3,3)\nimg_rgb2hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\nplt.title(\"Image in HSV format\", size = 20)\nplt.imshow(img_rgb2hsv)\nplt.xticks([]), plt.yticks([])","0f94c653":"lower_red = np.array([0, 100, 100])#,np.uint8\nupper_red = np.array([5, 255, 255])#,np.uint8\n\nhsv_img = cv2.cvtColor(img_rgb,cv2.COLOR_RGB2HSV)\n\nimage_thresh = cv2.inRange(hsv_img, lower_red, upper_red)\nres = cv2.bitwise_and(hsv_img, hsv_img, mask= image_thresh)\n#cv2.imwrite('output2.jpg', frame_threshed)\nplt.figure(figsize = (12,12))\nplt.title(\"Color Detection\", size = 20)\nplt.imshow(res)\nplt.xticks([]), plt.yticks([])","eb502461":"!wget https:\/\/i.ytimg.com\/vi\/B7xrGE8GHD4\/maxresdefault.jpg","e0b3c187":"os.listdir()","ba9000fb":"img3 = cv2.imread(\"maxresdefault.jpg\", -1)\nimg3_rgb = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\nplt.figure(figsize = (12,12))\nplt.imshow(img3_rgb)\nplt.xticks([]), plt.yticks([])","0a92c976":"img3_rgb.shape","ecfc0424":"pixel_values = img3_rgb.reshape((-1, 3))\npixel_values.shape","9ff3281d":"df = pd.DataFrame(pixel_values)\ndf.head()","31e54a09":"pixel_values = np.float32(pixel_values)","7b3d2eb3":"criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 150, 0.2)","b4b03ed6":"k = 6\n_, labels, (centers) = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)","1511903a":"# convert back to 8 bit values\ncenters = np.uint8(centers)\n\n# flatten the labels array\nlabels = labels.flatten()","96f7279d":"segmented_image = centers[labels.flatten()]","8eff0c3d":"# reshape back to the original image dimension\nsegmented_image = segmented_image.reshape(img3_rgb.shape)\nplt.figure(figsize = (12,12))\nplt.imshow(segmented_image)\nplt.xticks([]), plt.yticks([])","7d770e05":"#f, axarr = plt.subplots(1,2, figsize = (22,12))\n#axarr[0].imshow(img3_rgb)\n#axarr[1].imshow(segmented_image)\n\nplt.figure(figsize = (22,12))\nplt.subplot(1, 2, 1)\nplt.imshow(img3_rgb)\nplt.title(\"Original Image\", size = 16)\nplt.xticks([]), plt.yticks([])\n\nplt.subplot(1, 2, 2)\nplt.imshow(segmented_image)\nplt.title(\"Segmented Image\", size = 16)\nplt.xticks([]), plt.yticks([])","83f1955a":"!wget https:\/\/cdn.abcotvs.com\/dip\/images\/4041625_082318-kgo-sky7-hazy-skies-vid.jpg","183e865e":"os.listdir()","8b006501":"img_pollu = cv2.imread(\"4041625_082318-kgo-sky7-hazy-skies-vid.jpg\", -1)\npollu_rgb = cv2.cvtColor(img_pollu, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize = (12,8))\nplt.imshow(pollu_rgb)\nplt.title(\"Original Image\", size = 20)\nplt.xticks([]), plt.yticks([])","bbacef3e":"sns.distplot(pollu_rgb.flatten(), kde = False)","7a79cc8f":"pollu_gray = cv2.cvtColor(pollu_rgb,cv2.COLOR_RGB2GRAY)\nequ = cv2.equalizeHist(pollu_gray)\n\nplt.figure(figsize = (18,26))\nplt.subplot(1,3,1)\nplt.imshow(pollu_gray)\nplt.title(\"Gray Scale Original Image\", size = 18)\nplt.xticks([]), plt.yticks([])\n\nplt.subplot(1,3,2)\nplt.imshow(equ)\nplt.title(\"Gray Scale Image after equalizing\", size = 18)\nplt.xticks([]), plt.yticks([])\n\nplt.subplot(1,3,3)\nclahe_1 = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(5,5))\ncl2 = clahe_1.apply(pollu_gray)\nplt.imshow(cl2)\nplt.title(\"Gray Scale Image using CLAHE\", size = 18)\nplt.xticks([]), plt.yticks([])","d9263c6b":"plt.figure(figsize = (18,20))\n\nplt.subplot(1,2,1)\nplt.imshow(pollu_rgb)\nplt.title(\"Original Image\", size = 20)\nplt.xticks([]), plt.yticks([])\n\nplt.subplot(1,2,2)\npollu_con_rgb = cv2.cvtColor(cl2, cv2.COLOR_GRAY2RGB)\nplt.imshow(pollu_con_rgb)\nplt.title(\"Image after CLAHE & after converting to RGB\", size = 16)\nplt.xticks([]),plt.yticks([])","1ae448d1":"plt.figure(figsize = (16,5))\n\nplt.subplot(1,3,1)\nsns.distplot(pollu_gray.flatten(), kde = False, color = \"red\")\nplt.title(\"Pixels Distribution for gray_scale\", size = 14)\n\nplt.subplot(1,3,2)\nsns.distplot(equ.flatten(), kde = False, color = \"green\")\nplt.title(\"Pixels Distribution after histogram equalization\", size = 12)\n\nplt.subplot(1,3,3)\nsns.distplot(cl2.flatten(), kde = False, color = \"orange\")\nplt.title(\"Pixels Distribution after CLAHE\", size = 12)","28ed3365":"# Resize Image","b55e3af1":"# Color Detection","3133276c":"Let's also try to see how this image will look like in the HSV color format.","e1c299b6":"# Image Segmentation","79ed8e96":"So using CLAHE we can see that all the \"**smogy**\" effect is gone.","0d86e210":"Let's blur image using Gausian Blur, then we will do it using median blur.","0f375a01":"# High Pass Filter\n\n\nHigh pass filter basically allows only high frequency regions of an image, and high frequency regions are mainly sharp edges, noises, etc, so high noise images must be smoothened using low pass filters and then further processing needs to done. Also here we are using Laplacian high pass filter.","1c5c9a09":"**Now let's try to improve the quality of the above contrasted image**. Actually the concept with contrast is that, for high or low contrasted image, the pixel intensity distribution will be highly skewed. But for good quality image, the pixel intensity distribution is well distributed. So the \"**histogram equalization**\" techniques tries to distribute the pixel intensity equally, and thus improving the quality of the high or low contrasting image.","efc26d26":"# Crop Image","ff7b9d7b":"So as can be seen above, the 3 columns are the pixel values for each r, g and b.","c4df96ae":"We will try to segment this image into 6 colors using k-means algorithm.","918942af":"Applying high pass filter using \"Sobel\"","4ba33c53":"# Edge Detection","d7d94f97":"**Very important concept that I forgot to mention above is that, OpenCV supports color format in BGR, but standard format is RGB. I will show you image for both the formats.**","edbcb8f6":"# Detecting Red Colour Hat","03efae37":"# Thresholding based on pixel values","73263743":"Edge detection is found based on the gradient of image intensity. The place where there is a sudden change in intensity gradient, there is an edge. So if we provide minimum and maximum values intensity gradient, based on that edge is found out. Of course there are other calculations that goes on in this algo.","39cf40cf":"Now let's try to crop the image a little.","3b115096":"# Contrast Image","8cf52984":"Now let's generate a high contrast image. Contrasted image is generated using a formula, which is **(new_img = a * original_img + b)**. Here a is alpha, which is responsible for the contrast effect, and b is beta which represents brightness, which ranges between -127 to 127. So if value of a is greater than 1, then the image will be of high contrast, if in between 0 and 1, then it will be of low contrast, and if it is 1, then image will have no effect.\n\nAlso cv2.addWeighted has paramters **cv2.addWeighted(source_img1, alpha1, source_img2, alpha2, beta)**. If we want to do contrasting for only one image, then for the other image we need to use np.zero to create an array of the same original image dimension, and put all the pixel values to zero. Here I will be using 5 as alpha value.","0b9fe0d7":"# Blurring Image","a71fa870":"Let's create 6 clusters."}}