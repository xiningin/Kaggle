{"cell_type":{"ceb7a247":"code","d975f1d6":"code","af3799bc":"code","a9efeb7c":"code","f67a9bcd":"code","5bf6e8d6":"code","6db4e2df":"code","88713650":"code","c91d265a":"code","22e4dd6b":"code","2d9aa1a3":"code","f1f62f5f":"code","1d8f97ca":"code","f00c7310":"code","b7aaf08b":"code","c9029d8d":"code","a51fd2af":"code","dd538d62":"code","93ff173f":"code","3b1ad3fc":"code","125b75ca":"code","346311bd":"code","2f129bc2":"code","fe1a91db":"code","8510e2d2":"code","440da118":"code","b4fb313b":"code","17b78461":"code","70ae23d3":"code","b1f0f864":"code","376d9407":"code","503cf6ce":"code","9368d9fa":"code","01db0743":"code","5958d9ba":"code","fe4dec19":"code","771ef0e9":"code","a8176e53":"code","1d06f069":"code","da71a420":"markdown","697f3716":"markdown","7be882c5":"markdown","db2199ec":"markdown","36c844b6":"markdown","18d50eed":"markdown","b2a37feb":"markdown","de0646cb":"markdown","fede33c0":"markdown","a3901434":"markdown","9a71d039":"markdown","674c191b":"markdown","5301d2c1":"markdown","74ca0dcb":"markdown","655edd73":"markdown","527e32e5":"markdown","3f91ac5d":"markdown","f1f13710":"markdown","0322472a":"markdown","0b44f7be":"markdown","2d398784":"markdown","4a515789":"markdown","d72e9a4e":"markdown","be6baee2":"markdown","11d47453":"markdown","eb1e1b70":"markdown","dea904ac":"markdown","f8f64dbb":"markdown","080673d9":"markdown","68d04f9e":"markdown"},"source":{"ceb7a247":"import pandas as pd\nfrom pandas.api.types import CategoricalDtype\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom catboost import CatBoostRegressor\nfrom sklearn.feature_selection import mutual_info_regression\nfrom category_encoders import MEstimateEncoder\nimport optuna\nimport numpy as np","d975f1d6":"pd.set_option('display.max_rows', 80)\nMETRIC = 'MAE'\nEARLY_STOP = 50\nTRIALS = 50\n# Default catboost parameters\ncat_params = {#'task_type': \"GPU\",\n              'eval_metric': METRIC,\n              'random_state': 0,\n              'n_estimators': 3000,\n              'early_stopping_rounds': EARLY_STOP,\n              'verbose': False}","af3799bc":"df_train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\ndf_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\nprint(df_train.shape)\nprint(df_test.shape)","a9efeb7c":"# Remove outliers according to http:\/\/jse.amstat.org\/v19n3\/decock.pdf\ndf_train = df_train[df_train['GrLivArea'] < 4000]","f67a9bcd":"df = pd.concat([df_train, df_test])","5bf6e8d6":"for col in df.select_dtypes(['object']):\n    print(f'{col}:\\n{df[col].unique()}\\n')","6db4e2df":"df['Exterior2nd'] = df['Exterior2nd'].replace({'Wd Shng': 'WdShing', 'CmentBd': 'CemntBd', 'Brk Cmn': 'BrkComm'})","88713650":"df['MSZoning'] = df['MSZoning'].replace({'C (all)': 'C'})\ndf['Neighborhood'] = df['Neighborhood'].replace({'NAmes': 'Names'})\ndf['BldgType'] = df['BldgType'].replace({'2fmCon': '2FmCon', 'Duplex': 'Duplx', 'Twnhs': 'TwnhsI'})","c91d265a":"df.select_dtypes('number').aggregate(['min', 'max']).T","22e4dd6b":"df['GarageYrBlt'] = df['GarageYrBlt'].where(df['GarageYrBlt'] <= 2010, df['YearBuilt'])","2d9aa1a3":"def clean(df):\n    df['Exterior2nd'] = df['Exterior2nd'].replace({'Wd Shng': 'WdShing', 'CmentBd': 'CemntBd', 'Brk Cmn': 'BrkComm'})\n    df['MSZoning'] = df['MSZoning'].replace({'C (all)': 'C'})\n    df['Neighborhood'] = df['Neighborhood'].replace({'NAmes': 'Names'})\n    df['BldgType'] = df['BldgType'].replace({'2fmCon': '2FmCon', 'Duplex': 'Duplx', 'Twnhs': 'TwnhsI'})\n    df['GarageYrBlt'] = df['GarageYrBlt'].where(df['GarageYrBlt'] <= 2010, df['YearBuilt'])\n    return df","f1f62f5f":"# The nominal (unordered) categorical features\nnominal_categories = {\n    \"MSSubClass\": CategoricalDtype(categories=['None', 20, 30, 40, 45, 50, 60, 70, 75, 80, 85, 90, 120, 150, 160, 180, 190]),\n    \"MSZoning\": CategoricalDtype(categories=['None', 'A', 'C', 'FV', 'I', 'RH', 'RL', 'RP', 'RM']),\n    \"Street\": CategoricalDtype(categories=['None', 'Grvl', 'Pave']),\n    \"Alley\": CategoricalDtype(categories=['None', 'Grvl', 'Pave']),\n    \"LandContour\": CategoricalDtype(categories=['None', 'Lvl', 'Bnk', 'HLS', 'Low']), # ordinal?\n    \"LotConfig\": CategoricalDtype(categories=['None', 'Inside', 'Corner', 'CulDSac', 'FR2', 'FR3']), # ordinal?\n    \"Neighborhood\": CategoricalDtype(categories=['None', 'Blmngtn', 'Blueste', 'BrDale', 'BrkSide', 'ClearCr', 'CollgCr', 'Crawfor', 'Edwards', 'Gilbert', 'IDOTRR', 'MeadowV', 'Mitchel', 'Names', 'NoRidge', 'NPkVill', 'NridgHt', 'NWAmes', 'OldTown', 'SWISU', 'Sawyer', 'SawyerW', 'Somerst', 'StoneBr', 'Timber', 'Veenker']),\n    \"Condition1\": CategoricalDtype(categories=['None', 'Artery', 'Feedr', 'Norm', 'RRNn', 'RRAn', 'PosN', 'PosA', 'RRNe', 'RRAe']),\n    \"Condition2\": CategoricalDtype(categories=['None', 'Artery', 'Feedr', 'Norm', 'RRNn', 'RRAn', 'PosN', 'PosA', 'RRNe', 'RRAe']),\n    \"BldgType\": CategoricalDtype(categories=['None', '1Fam', '2FmCon', 'Duplx', 'TwnhsE', 'TwnhsI']),\n    \"HouseStyle\": CategoricalDtype(categories=['None', '1Story', '1.5Fin', '1.5Unf', '2Story', '2.5Fin', '2.5Unf', 'SFoyer', 'SLvl']),\n    \"RoofStyle\": CategoricalDtype(categories=['None', 'Flat', 'Gable', 'Gambrel', 'Hip', 'Mansard', 'Shed']),\n    \"RoofMatl\": CategoricalDtype(categories=['None', 'ClyTile', 'CompShg', 'Membran', 'Metal', 'Roll', 'Tar&Grv', 'WdShake', 'WdShngl']),\n    \"Exterior1st\": CategoricalDtype(categories=['None', 'AsbShng', 'AsphShn', 'BrkComm', 'BrkFace', 'CBlock', 'CemntBd', 'HdBoard', 'ImStucc', 'MetalSd', 'Other', 'Plywood', 'PreCast', 'Stone', 'Stucco', 'VinylSd', 'Wd Sdng', 'WdShing']),\n    \"Exterior2nd\": CategoricalDtype(categories=['None', 'AsbShng', 'AsphShn', 'BrkComm', 'BrkFace', 'CBlock', 'CemntBd', 'HdBoard', 'ImStucc', 'MetalSd', 'Other', 'Plywood', 'PreCast', 'Stone', 'Stucco', 'VinylSd', 'Wd Sdng', 'WdShing']),\n    \"MasVnrType\": CategoricalDtype(categories=['None', 'BrkCmn', 'BrkFace', 'CBlock', 'Stone']),\n    \"Foundation\": CategoricalDtype(categories=['None', 'BrkTil', 'CBlock', 'PConc', 'Slab', 'Stone', 'Wood']),\n    \"Heating\": CategoricalDtype(categories=['None', 'Floor', 'GasA', 'GasW', 'Grav', 'OthW', 'Wall']),\n    \"GarageType\": CategoricalDtype(categories=['None', '2Types', 'Attchd', 'Basment', 'BuiltIn', 'CarPort', 'Detchd']),\n    \"MiscFeature\": CategoricalDtype(categories=['None', 'Elev', 'Gar2', 'Othr', 'Shed', 'TenC']),\n    \"SaleType\": CategoricalDtype(categories=['None', 'WD', 'CWD', 'VWD', 'New', 'COD', 'Con', 'ConLw', 'ConLI', 'ConLD', 'Oth']),\n    \"SaleCondition\": CategoricalDtype(categories=['None', 'Normal', 'Abnorml', 'AdjLand', 'Alloca', 'Family', 'Partial'])\n}\n\n# The ordinal (ordered) categorical features \nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(1,11))\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"IR3\", \"IR2\", \"IR1\", \"Reg\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"FuseP\", \"FuseF\", \"Mix\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add None level for missing values\nordered_levels = {key: ['None'] + value for key, value in ordered_levels.items()}","1d8f97ca":"def encode(df):\n    # Nominal categories\n    for col, cats in nominal_categories.items():\n        df[col] = df[col].astype(cats)\n    # Ordinal categories\n    for col, levels in ordered_levels.items():\n        df[col] = df[col].astype(CategoricalDtype(levels, ordered=True))\n    return df\n\ndf = encode(df)","f00c7310":"nan_by_col = df.isnull().sum().drop(labels='SalePrice')\ncols_with_nan = nan_by_col[nan_by_col > 0]\nprint(f'There are {len(cols_with_nan)} columns with missing values:\\n{cols_with_nan.sort_values(ascending=False)}')\nprint(f'Total number of missing entries: {nan_by_col.sum()}')","b7aaf08b":"for col in cols_with_nan.index:\n    df[col + '_was_missing'] = df[col].isnull().astype(int)","c9029d8d":"cols_with_typical = ['ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual']\ndf[cols_with_typical] = df[cols_with_typical].fillna('TA')\ndf['Functional'] = df['Functional'].fillna('Typ')\ndf['Electrical'] = df['Electrical'].fillna('FuseA')","a51fd2af":"print(df['Utilities'].value_counts())\ndf['Utilities'] = df['Utilities'].fillna('AllPub')","dd538d62":"categorical_columns = df.select_dtypes('category').columns\ndf[categorical_columns] = df[categorical_columns].fillna('None')","93ff173f":"# We have to remove the target column (SalePrice) from the list of numerical columns before imputing\nnumerical_columns = df.select_dtypes('number').columns.drop('SalePrice')\ndf[numerical_columns] = df[numerical_columns].fillna(0)","3b1ad3fc":"def impute(df):\n    nan_by_col = df.isnull().sum().drop(labels='SalePrice')\n    cols_with_nan = nan_by_col[nan_by_col > 0]\n    \n    for col in cols_with_nan.index:\n        df[col + '_was_missing'] = df[col].isnull().astype(int)\n\n    cols_with_typical = ['ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual']\n    df[cols_with_typical] = df[cols_with_typical].fillna('TA')\n    df['Functional'] = df['Functional'].fillna('Typ')\n    df['Electrical'] = df['Electrical'].fillna('FuseA')\n\n    df['Utilities'] = df['Utilities'].fillna('AllPub')\n\n    categorical_columns = df.select_dtypes('category').columns\n    df[categorical_columns] = df[categorical_columns].fillna('None')\n\n    numerical_columns = df.select_dtypes('number').columns.drop('SalePrice')\n    df[numerical_columns] = df[numerical_columns].fillna(0)\n    return df","125b75ca":"def load_data():\n    # Read data\n    df_train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\n    df_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\n    # Remove outliers\n    df_train = df_train[df_train['GrLivArea'] < 4000]\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","346311bd":"df_train, df_test = load_data()","2f129bc2":"def score_dataset(X, y, model=CatBoostRegressor(**cat_params), metric='MAE'):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    if (metric == 'RMSLE'):\n        y = np.log(y)\n        score = cross_val_score(\n            model, X, y, cv=5, scoring=\"neg_mean_squared_error\",\n        )\n    elif (metric == 'MAE'):\n        score = cross_val_score(\n            model, X, y, cv=5, scoring=\"neg_mean_absolute_error\",\n        )\n    score = -1 * score.mean()\n    if (metric == 'RMSLE'):\n        score = np.sqrt(score)\n    return score","fe1a91db":"X_train = df_train.copy()\ny_train = X_train.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X_train, y_train, metric=METRIC)\nprint(f\"Baseline score: {baseline_score:.5f} {METRIC}\")","8510e2d2":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","440da118":"X_train = df_train.copy()\ny_train = X_train.pop(\"SalePrice\")\n\nmi_scores = make_mi_scores(X_train, y_train)\nmi_scores.round(3)","b4fb313b":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0.0]","17b78461":"X_train = df_train.copy()\ny_train = X_train.pop(\"SalePrice\")","70ae23d3":"def label_encode(df):\n    X = df.copy()\n    for col in X.select_dtypes([\"category\"]):\n        X[col] = X[col].cat.codes\n    return X","b1f0f864":"def mathematical_transforms(df):\n    X = pd.DataFrame()\n    X[\"LivLotRatio\"] = df['GrLivArea'] \/ df['LotArea']\n    X[\"Spaciousness\"] = (df['1stFlrSF'] + df['2ndFlrSF']) \/ df['TotRmsAbvGrd']\n    return X","376d9407":"def interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X = X.mul(df.GrLivArea, axis=0)\n    return X","503cf6ce":"def counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"3SsnPorch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    X['TotalRooms'] = df['TotRmsAbvGrd'] + df['FullBath'] + df['HalfBath']\n    return X","9368d9fa":"def group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X","01db0743":"def pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n    return X","5958d9ba":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","fe4dec19":"def create_features(df_train, df_test=None):\n    X_train = df_train.copy()\n    y_train = X_train.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X_train, y_train)\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X_train = pd.concat([X_train, X_test])\n\n    #X_train = drop_uninformative(X_train, mi_scores)\n    X_train = X_train.join(mathematical_transforms(X_train))\n    #X_train = X_train.join(interactions(X_train))\n    X_train = X_train.join(counts(X_train))\n    #X_train = X_train.join(group_transforms(X_train))\n    X_train = X_train.join(pca_inspired(X_train))\n    X_train = label_encode(X_train)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X_train.loc[df_test.index, :]\n        X_train.drop(df_test.index, inplace=True)\n\n    # Target encoding\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X_train = X_train.join(encoder.fit_transform(X_train, y_train, cols=[\"Neighborhood\"]))\n\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n        return X_train, X_test\n    else:\n        return X_train\n\n#df_train, df_test = load_data()\n#X_train = create_features(df_train)\n#y_train = df_train[\"SalePrice\"]\n#score_dataset(X_train, y_train, metric=METRIC)","771ef0e9":"X_train = create_features(df_train)\ny_train = df_train[\"SalePrice\"]","a8176e53":"def objective(trial):\n    best_cat_params = {\n        'n_estimators': trial.suggest_int(\"n_estimators\", 1000, 10000),\n        'max_depth': trial.suggest_int(\"max_depth\", 2, 10),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        'min_child_samples': trial.suggest_int(\"min_child_samples\", 1, 10),\n        'colsample_bylevel': trial.suggest_float(\"colsample_bylevel\", 0.2, 1.0),\n        'subsample': trial.suggest_float(\"subsample\", 0.2, 1.0),\n        #'gamma': trial.suggest_discrete_uniform('gamma', 0.0, 20.0, 0.1),\n        'reg_lambda': trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True)\n    }\n    catb = CatBoostRegressor(**{**cat_params, **best_cat_params})\n    return score_dataset(X_train, y_train, catb, metric=METRIC)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=TRIALS)\nbest_cat_params = study.best_params","1d06f069":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train[\"SalePrice\"]\n\ncatb = CatBoostRegressor(**{**cat_params, **best_cat_params})\ncatb.fit(X_train, y_train)\npredictions = catb.predict(X_test)\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)","da71a420":"Finally, we compile all of these steps in a function for later reuse.","697f3716":"Let's set some defaults.","7be882c5":"For the rest of the categorical columns, we'll just impute with *\\\"None\\\"*.","db2199ec":"# Feature Engineering","36c844b6":"## Create Final Feature Set","18d50eed":"# Establish a Baseline Score","b2a37feb":"## Imputation\nLet's first look at how many missing values we have and the columns where they're located.","de0646cb":"The data has 80 columns (plus the target column we're trying to predict - *SalePrice*).","fede33c0":"# Train Model and Create Submission","a3901434":"# Read and Preprocess Data","9a71d039":"All that's left are numerical columns. We will impute 0 for these.","674c191b":"# Feature Selection","5301d2c1":"All of these steps are summarized in this function.","74ca0dcb":"We create indicator columns for columns with missing values.","655edd73":"3 of the numerical columns in the data (*MSSubClass*, *OverallQual*, and *OverallCond*) are actually categorical.\n\nWe define the nominal and ordinal features' categories and levels according to the data description.","527e32e5":"We will use optuna to optimize our CATBoost regressor's hyperparameters.","3f91ac5d":"This function lets us drop any features with an MI score of 0.","f1f13710":"# Import Libraries","0322472a":"To find out which features are more useful for predicting the target variable, we calculate the MI (Mutual Information) score for each one of them.","0b44f7be":"## Inspect Values of Categorical Columns","2d398784":"There are values that are not consistent with the provided *data_description.txt* file. Let's fix these too.","4a515789":"We'll concatenate training and test data to preprocess them together.","d72e9a4e":"We notice that the *GarageYrBlt* column which describes the year the garage was built has impossible values (as high as 2207).\n\nLet's replace those values with the year the house was built.","be6baee2":"And now we can convert the categorical columns to their respective categorical types.","11d47453":"# Hyperparameter Tuning","eb1e1b70":"All the values of the *Utilities* column are equal to *AllPub* except one. So we'll impute with that.","dea904ac":"## Inspect Values of Numerical Columns\nLet's look at the range of numerical values to see if there are any problems.","f8f64dbb":"## Encoding","080673d9":"We will impute with the TA\/Typical\/Average\/Normal level whenever the column has that level but lacks a *NA\/None* level.","68d04f9e":"The *Exterior2nd* column contains some typos. Let's fix them."}}