{"cell_type":{"823905f6":"code","9b8d4862":"code","3eb856c7":"code","1e31c346":"code","910ecc4e":"code","6cfa50d2":"code","f21ed255":"code","87fd6484":"code","097c8133":"code","00953a89":"code","af7a13f7":"code","b25e4224":"code","971b9b10":"code","b5ced165":"code","9d177d6f":"code","da44846a":"code","a8dc4825":"code","b2ce105d":"code","66959c04":"code","0322f641":"code","54d9cda3":"markdown","03b93ebd":"markdown","d8c9fe8d":"markdown","5ee8b030":"markdown","5ebbbdfd":"markdown","42c3c171":"markdown","ddda3241":"markdown","f3f26cb4":"markdown"},"source":{"823905f6":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9b8d4862":"import pandas as pd\nitem_categories = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\ntrain  = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nsample_submission = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\ntrain.head()","3eb856c7":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","1e31c346":"train = train[train.item_price < 100000]\ntrain = train[train.item_cnt_day < 750]","910ecc4e":"num_month = train['date_block_num'].max()\nmonth_list=[i for i in range(num_month+1)]\nshop = []\nfor i in range(num_month+1):\n    shop.append(5)\nitem = []\nfor i in range(num_month+1):\n    item.append(5037)\nmonths_full = pd.DataFrame({'shop_id':shop, 'item_id':item,'date_block_num':month_list})","6cfa50d2":"train_cleaned = train.drop(labels = ['date', 'item_price'], axis = 1)\n# g\u00fcnl\u00fck \u00f6\u011fe say\u0131s\u0131n\u0131 ayl\u0131k \u00f6\u011fe say\u0131s\u0131na de\u011fi\u015ftirme\ntrain_cleaned = train_cleaned.groupby([\"item_id\",\"shop_id\",\"date_block_num\"]).sum().reset_index()\ntrain_cleaned = train_cleaned.rename(index=str, columns = {\"item_cnt_day\":\"item_cnt_month\"})\ntrain_cleaned = train_cleaned[[\"item_id\",\"shop_id\",\"date_block_num\",\"item_cnt_month\"]]\ntrain_cleaned.tail()","f21ed255":"train_cleaned.describe()","87fd6484":"clean= pd.merge(train_cleaned, train, how='right', on=['shop_id','item_id','date_block_num'])\nclean = clean.sort_values(by=['date_block_num'])\nclean.fillna(0.00,inplace=True)\nclean.tail()","097c8133":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nX_full = clean.copy()\nX_test_full = clean.copy()\n\n# Na indexlerini silme\nX_full.dropna(axis=0, subset=['item_cnt_month'], inplace=True)\ny = X_full.item_cnt_month\nX_full.drop(['item_cnt_month'], axis=1, inplace=True)\n\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.66,\n                                                                random_state=42)\n\n\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# N\u00fcmerik s\u00fctunlar\u0131 se\u00e7me\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","00953a89":"X_train.describe()","af7a13f7":"from sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n#LightGBM Regressor\nimport lightgbm\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import mean_absolute_error\n\n# Eksik veriyi SimpleImputer ile doldurma\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Kategorik de\u011fi\u015fkenleri d\u00fczenleme\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Say\u0131sal ve kategorik veriler i\u00e7in veri \u00f6n i\u015fleme\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","b25e4224":"# modeli olu\u015fturma\nmodel = LGBMRegressor()\nparams = {\"feature_fraction\":[0.8,0.5,0.1],\n            \"max_depth\":[2,4,5,10,15],\n          \"n_estimators\":[50,100,200]}","971b9b10":"from sklearn.model_selection import train_test_split , GridSearchCV\ncv_model = GridSearchCV(model , params, cv=10, verbose=2 , n_jobs=-1).fit(X_train, y_train)","b5ced165":"cv_model.best_params_","9d177d6f":"lgbm_tuned = LGBMRegressor(feature_fraction=0.8, max_depth=15, n_estimators=200).fit(X_train, y_train)\n\n# Do\u011frulama verilerinin \u00f6n i\u015flenmesi, tahminleri alma\n\npreds = lgbm_tuned.predict(X_valid)\nprint(f'Model test accuracy: {lgbm_tuned.score(X_valid, y_valid)*100:.3f}%')","da44846a":"feature_imp = pd.Series(lgbm_tuned.feature_importances_,\n                        index=X_valid.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y = feature_imp.index)\nplt.xlabel(\"Features Importance Scores\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Importances\")\nplt.show()   ","a8dc4825":"# '\u0130tem_cnt_month' \u00f6\u011fesinde \u00f6nceden 'item_cnt_month' kadar sat\u0131r olmad\u0131\u011f\u0131 i\u00e7in nan-de\u011ferleri ekleme\nsample_submission['item_cnt_month'] = pd.Series(preds)\nsample_submission.apply(lambda col: col.drop_duplicates().reset_index(drop=True))","b2ce105d":"#eksik nan-de\u011ferlerinin ortalama-de\u011ferlerle doldurulmas\u0131\nsample_submission['item_cnt_month'].fillna(sample_submission['item_cnt_month'].median(), inplace = True)","66959c04":"# csv dosyas\u0131 olu\u015fturma\nsample_submission.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","0322f641":"if len(sample_submission) == len(test):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(sample_submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","54d9cda3":"## Outliers","03b93ebd":"# **Veriyi \u00c7ekme**","d8c9fe8d":"# Tahmin verisini export etme\n","5ee8b030":"# Model Tuning ve Tahmin","5ebbbdfd":"# Makine \u00d6\u011frenmesi modelini kurma\n","42c3c171":"De\u011fi\u015fkenlerin \u00f6nem d\u00fczeyleri birbirine \u00e7ok yak\u0131n g\u00f6z\u00fck\u00fcyor. O y\u00fczden de\u011fi\u015fken optimizasyonu yapm\u0131yoruz.","ddda3241":"max_depth ve n_estimators de\u011ferleri verdi\u011fimiz aral\u0131\u011f\u0131n en y\u00fcksek de\u011ferlerinde \u00e7\u0131kt\u0131. Daha y\u00fcksek de\u011ferler verebilirim ancak bundan ka\u00e7\u0131n\u0131yorum. Sebebi ise overfitting durumuna d\u00fc\u015fmek istememem.","f3f26cb4":"# Train datas\u0131n\u0131 d\u00fczenleme"}}