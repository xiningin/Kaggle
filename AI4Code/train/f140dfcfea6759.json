{"cell_type":{"e6653c1b":"code","4eb00399":"code","c5ffb683":"code","8b1dbaf8":"code","c09936be":"code","54dc746f":"code","438cfec8":"markdown","fb566a9f":"markdown","6720c3e5":"markdown","46d5ce53":"markdown","1f992375":"markdown","20d3f1ff":"markdown","fc57b2de":"markdown","a733009d":"markdown","e7737850":"markdown","7f6fe135":"markdown","2f269827":"markdown","3d80b7af":"markdown","271596e0":"markdown","1f919735":"markdown","987a67e2":"markdown","40d53011":"markdown","65a2a24c":"markdown","8e0e2840":"markdown","e8266308":"markdown","db3c3658":"markdown","18574c00":"markdown","134e4873":"markdown","c08ffd30":"markdown","ae544744":"markdown"},"source":{"e6653c1b":"import pickle as pk\nimport pandas as pd\nclf1_trained, clf2_trained, clf3_trained, clf4_trained = pk.load(open('..\/input\/model-training\/Model_Training_Models.p', \"rb\"))\nResults1, Results2, Results3, Results4 = pk.load(open('..\/input\/model-training\/Model_Training_Results.p', \"rb\"))\nclf5_trained, clf6_trained, best_clf = pk.load(open('..\/input\/machine-learning-with-more-data\/Data_up-models.p',\"rb\"))\nResults5, Results6, Results_Final = pk.load(open('..\/input\/machine-learning-with-more-data\/Data_up-results.p',\"rb\"))\n\n\nList_Results = [eval('Results' + str(i)) for i in range(1,7)] + [Results_Final]\nTest_Accuracies = [Result['Test']['Accuracy'] for Result in List_Results]\nTest_F1 = [Result['Test']['F1'] for Result in List_Results]\nTest_AUC = [Result['Test']['AUC'] for Result in List_Results]\nTest_mean_per = [Result['Test']['Percision'].mean() for Result in List_Results]\nTest_mean_rec = [Result['Test']['Recall'].mean() for Result in List_Results]\nTest_TN_rate = [Result['Test']['Confusion Matrix'][0,0] for Result in List_Results]\nTest_TP_rate = [Result['Test']['Confusion Matrix'][1,1] for Result in List_Results]\nTest_FP_rate = [Result['Test']['Confusion Matrix'][0,1] for Result in List_Results]\nTest_FN_rate = [Result['Test']['Confusion Matrix'][1,0] for Result in List_Results]\nSummary = pd.DataFrame([Test_Accuracies, Test_F1, Test_AUC, Test_mean_per, Test_mean_rec, Test_TN_rate,\n                        Test_TP_rate, Test_FP_rate, Test_FN_rate], \n                      index = ['Accuracy', 'F1','AUC','Average Precision','Average Recall', 'TN','TP','FP','FN'], \n                      columns = ['GNB', 'Gradient Boosting', 'GNB (down sampling)', 'Gradient Boosting (down sampling)',\n                              'GNB (down sampling plus augmentation)', 'Gradient Boosting (down sampling plus augmentation)',\n                                'Gradient Boosting Optimized (down sampling plus augmentation)']).T\nSummary","4eb00399":"custom_bow_features1 = embedding_fit1.transform(custom_words).toarray()\n\npredicted_sentiment1 = clf1_trained.predict(custom_bow_features1)[0]","c5ffb683":"clf6_trained","8b1dbaf8":"from utils_data import review_to_words\nimport pickle as pk\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nvocabulary_size = 10000\nvectorizer = CountVectorizer(max_features = vocabulary_size, \n                             preprocessor = lambda x: x, \n                             tokenizer = lambda x:x)\n\nvocab1, vocab2, vocab3, vocab4 = pk.load(open('..\/input\/model-training\/Model_Training_Vectorizers.p',\"rb\"))\nvocab5, vocab6 = pk.load(open('..\/input\/machine-learning-with-more-data\/Data_up_Vectorizers.p',\"rb\"))\n\n\ndata1,data2, data3, data4 = pk.load(open('..\/input\/model-training\/Model_Training_Data.p',\"rb\"))\ndata5,data6 = pk.load(open('..\/input\/machine-learning-with-more-data\/Data_up_Data.p',\"rb\"))\n\nembedding_fit1= vectorizer.fit(data1['X_train'])\nembedding_fit2 = vectorizer.fit(data2['X_train'])\nembedding_fit3 = vectorizer.fit(data3['X_train'])\nembedding_fit4 = vectorizer.fit(data4['X_train'])\nembedding_fit5 = vectorizer.fit(data5['X_train'])\nembedding_fit6 = vectorizer.fit(data6['X_train'])\n\n## Test with custom review\ncustom_review1 = 'There are a lot fascinating features in this kindle'\ncustom_review2 = 'what a poor Iphone this was. No added features despite its sky-rocket price'\ncustom_review3 = 'Honestly, I was hesitant before making this purchase. Now, I am so excited about this tablet.'\ncustom_review4 = 'A lot of expectations about this Galaxy Note 10 have been made, yet they are vanished now.'\ncustom_review5 = 'Very Bad kindle'\ntrue_sentiment1 = 'Pos'\ntrue_sentiment2 = 'Neg'\ntrue_sentiment3 = 'Pos'\ntrue_sentiment4 = 'Neg'\ntrue_sentiment5 = 'Neg'\n\nCustom_Reviews = [custom_review1, custom_review2, custom_review3, custom_review4, custom_review5]\nCustom_Labels = [true_sentiment1, true_sentiment2, true_sentiment3, true_sentiment4,true_sentiment5]\nfor custom_review, true_sentiment in zip(Custom_Reviews, Custom_Labels):\n    print('***********************************************************************')\n    custom_words = review_to_words(custom_review)\n    \n    custom_bow_features1 = embedding_fit1.transform(custom_words).toarray()\n    custom_bow_features2 = embedding_fit2.transform(custom_words).toarray()\n    custom_bow_features3 = embedding_fit3.transform(custom_words).toarray()\n    custom_bow_features4 = embedding_fit4.transform(custom_words).toarray()\n    custom_bow_features5 = embedding_fit5.transform(custom_words).toarray()\n    custom_bow_features6 = embedding_fit6.transform(custom_words).toarray()\n\n    \n    predicted_sentiment1 = clf1_trained.predict(custom_bow_features1)[0]\n    predicted_sentiment2 = clf2_trained.predict(custom_bow_features2)[0]\n    predicted_sentiment3 = clf3_trained.predict(custom_bow_features3)[0]\n    predicted_sentiment4 = clf4_trained.predict(custom_bow_features4)[0]\n    predicted_sentiment5 = clf5_trained.predict(custom_bow_features5)[0]\n    predicted_sentiment6 = clf6_trained.predict(custom_bow_features6)[0]\n    predicted_sentiment7 = best_clf.predict(custom_bow_features6)[0]\n    print(\"--- Custom review ---\")\n    print(custom_review)\n    print(\"\\n--- Preprocessed words ---\")\n    print(custom_words)\n    print('True sentiment: {}'.format(true_sentiment))\n    print('predicted sentiment for GaussianNaiveBayes without undersampling or augmentaiton :{}'.format(predicted_sentiment1))\n    print('predicted sentiment for GradientBoosting without undersampling or augmentaiton :{}'.format(predicted_sentiment2))\n    print('predicted sentiment for GaussianNaiveBayes with undersampling only:{}'.format(predicted_sentiment3))\n    print('predicted sentiment for GradientBoosting with undersampling only:{}'.format(predicted_sentiment4))\n    print('predicted sentiment for GaussianNaiveBayes with undersampling and augmentaiton :{}'.format(predicted_sentiment5))\n    print('predicted sentiment for GradientBoosting with undersampling and augmentaiton :{}'.format(predicted_sentiment6))\n    print('predicted sentiment for Optimized GradientBoosting with undersampling and augmentaiton :{}'.format(predicted_sentiment7))","c09936be":"custom_bow_features1.sum()","54dc746f":"custom_bow_features4.sum()","438cfec8":"Performing _**down sampling**_ yields a more balanced data set. However, it comes with the cost of reducing the size of the data set significantly. Thus, performing _**data upsampling**_ to increase the minority class before rebalancing the data again , using down sampling, could help in providing more balanced data set, yet with more training examples. The benefits of such approach is reported in [Data Upsampling](https:\/\/www.kaggle.com\/omarayman67\/machine-learning-with-more-data). Finally, the model is optimized to provide the best possible results for this problem.\n\nIt is interesting to compare the confusion matrices resulted from this stage \n\n1- **Naive Bayes Classifier**: Although its simplicity, the model yielded impressive score with respect to the initial performance. \n\n<figure class=\"image\">\n    <table><tr>\n    <td> <img src=\"https:\/\/i.ibb.co\/ft6mFnb\/NB-Imbalanced.png\" alt=\"NB Imbalance\" style=\"width: 800px;\"\/> <\/td>\n    <td> <img src=\"https:\/\/i.ibb.co\/868HZtp\/NB-down-up.png\" alt=\"NB down sample up sample\" style=\"width: 800px;\"\/> <\/td>    \n    <\/tr><\/table>\n    <figcaption><center>Left: Initial imbalanced dataset performance, Right: after down as well as up sampling  <\/center><\/figcaption>\n<\/figure>\n\n\n2- **Gradient Boosting Classifier**: the optmized model shows huge superiority over the initial model revealing the value of this lengthy procedure. \n\n<figure class=\"image\">\n    <table><tr>\n    <td> <img src=\"https:\/\/i.ibb.co\/nmGkCTV\/GB-imbalanced.png\" alt=\"xgboost Imbalance\" style=\"width: 800px;\"\/> <\/td>\n    <td> <img src=\"https:\/\/i.ibb.co\/Mn1yN67\/GB-down-up-optimized.png\" alt=\"xgboost down sample up sample optimized\" style=\"width: 800px;\"\/> <\/td>    \n    <\/tr><\/table>\n    <figcaption><center>Left: Initial imbalanced dataset performance, Right: optimized after down as well as up sampling  <\/center><\/figcaption>\n<\/figure>","fb566a9f":"- Data is highly imbalanced more toward positive attitudes than negative ones. Thus, it is important to handle this imbalance before expecting any good results from the model\n\n<figure class=\"image\">\n    <table><tr>\n    <td> <img src=\"https:\/\/i.ibb.co\/Hgj8ffY\/imbalance2.png\" alt=\"imbalance_best_mapping\" style=\"width: 800px;\"\/> <\/td>   \n    <\/tr><\/table>\n    <figcaption><center>Data Imbalance<\/center> <\/figcaption>\n<\/figure>\n\n- Removing additional custom stop words (i.e., derived from the data itself) make the word cloud more clearer giving the model more expressive words per category especially for minority class (i.e., negative). Example for the label 1 is illustrated below.\n\n<figure class=\"image\">\n    <table><tr>\n    <td> <img src=\"https:\/\/i.ibb.co\/vC57jKZ\/word-cloud-1-before-stop.png\" alt=\"cloud_1_before_stop\" style=\"width: 500px;\"\/> <\/td>\n    <td> <img src=\"https:\/\/i.ibb.co\/ZVys5SV\/word-cloud-1-after-stop.png\" alt=\"cloud_1_after_stop\" style=\"width: 500px;\"\/> <\/td>\n    <\/tr><\/table>\n    <figcaption><center>Left: Before removing stop words, Right: After removing stop words<\/center> <\/figcaption>\n<\/figure>  \n\n- Combining down sampling technique with additional stop words on bi-class data helps seperate the data signficantly. Thus, the model acting on this data set is expected to be better than the whole dataset with the cost of reducing total number of training examples.\n\n<figure class=\"image\">\n    <table><tr>\n    <td> <img src=\"https:\/\/i.ibb.co\/fqtF1J9\/tsne-imb2.png\" alt=\"tsne_2_labels_stop\" style=\"width: 800px;\"\/> <\/td>\n    <td> <img src=\"https:\/\/i.ibb.co\/wMXNMPb\/tsne-down-stop.png\" alt=\"tsne_2_labels_down_sampled_stop\" style=\"width: 800px;\"\/> <\/td>    \n    <\/tr><\/table>\n    <figcaption><center>Left: 2 labels without down sampling or stop words, Right: 2 labels after down sampling and stop words <\/center><\/figcaption>\n<\/figure>","6720c3e5":"---\n<a id='Introduction'><\/a>\n# <center>Introduction <\/center>\n---","46d5ce53":"A crucial step in the machine learning process is to get familiarized by the data under study in order to extract helpful insights before digging into model training. \n\nThis part is reported in [Sentiment-Analysis-PreProcessing](https:\/\/www.kaggle.com\/omarayman67\/pre-processing) and the most remarkable insights are highlighted below","1f992375":"---\n# <center>Table of Contents<\/center>\n---\n<ul>\n<li><a href=\"#Introduction\">Introduction<\/a><\/li>\n    \n<li><a href=\"#Business\">Business Opportunity<\/a><\/li>\n    \n<li><a href=\"#Formulation\">Problem Formulation<\/a><\/li>\n  \n<li><a href=\"#Data\">Data Collection And Integration<\/a><\/li>\n\n<li><a href=\"#Pre_processing\">Data Preprocessing And Visualizations <\/a><\/li>\n\n<li><a href=\"#Model_Training\">Model Training And Evaluation <\/a><\/li>\n\n<li><a href=\"#Data_Upsampling\">Data Upsampling <\/a><\/li>  \n    \n<li><a href=\"#Conclusions\">Conclusions<\/a><\/li>\n<\/ul>","20d3f1ff":"<a id = 'Business'><\/a>\n\n---\n# <center> Business Opportunity <\/center>\n---","fc57b2de":"Dealing with imbalanced data sets is one of the most cumbersome challenges in any machine learning solution. _However_, working on increasing the minority class samples whenever possible can yield more effective classifiers. ","a733009d":"`Machine Learning (ML)` emerges as an elegent approach for many use cases commonly faced in both industry and academia. _**<u>From business prespective<\/u>**_, the `machine learning` always starts by identyfing either _a potential opportunity_ or a _business problem_. A bird's eye view of the processes involved in any `machine learning` solution is depicted in the figure below <br>\n\n\n<figure class=\"image\">\n    <table><tr>\n    <td> <img src=\"https:\/\/i.ibb.co\/5FyfFkr\/ML-pipeline.jpg\" alt=\"ML_Pipeline\" style=\"width: 1000px;\"\/> <\/td>   \n    <\/tr><\/table>\n<\/figure>\n","e7737850":"<a id = 'Formulation'><\/a>\n\n---\n# <center> Problem Formulation <\/center>\n---","7f6fe135":"<a id='Model_Training'><\/a>\n# Model Training And Evaluation","2f269827":"<a id ='Results'><\/a>\n# Results Summary","3d80b7af":"<a id='Data_Upsampling'><\/a>\n# Data Upsampling","271596e0":"<a id= 'Conclusions'><\/a>\n# Conclusions And Future Work ","1f919735":"<a id='Data'><\/a>\n\n---\n# <center> Data Collection and Integration <\/center>\n---","987a67e2":"This project exemplfies the whole `Machine Learning (ML)` pipeline on a prominent `Natural Language Processing (NLP)` use case, the _**<u>Sentiment Analysis<\/u>**_ one.","40d53011":"With the rise of online social media platforms like _Twitter_, _Facebook_ and _Reddit_, and the proliferation of customer reviews on sites like _Amazon_ and _Yelp_, access to massive text-based data sets is granted , more than ever before!,.  They can be analyzed in order to determine how large portions of the population feel about certain products, events, etc. This is a huge business opportunity to make more data-driven decisions _whenever_ the **scale**, **variety** and **speed** is required. ","65a2a24c":"[`Datafiniti`](https:\/\/datafiniti.co\/products\/product-data\/) is company that provides instant access to the web data by  compiling data from thousands of websites to create standardized databases of business, product, and property information. This project leverages [different samples](https:\/\/www.kaggle.com\/datafiniti\/consumer-reviews-of-amazon-products?select=Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv) of _**<u>consumer reviews<\/u>**_ for `Amazon` products like the Kindle, Fire TV Stick, and more from Datafiniti's Product Database updated between **September 2017 and October 2018 as well as February 2019 and April 2019**. Each product listing includes the name Amazon in the Brand and Manufacturer field. All fields within this dataset have been flattened, with some omitted, to streamline the data analysis.\n\n\n","8e0e2840":"## Future work\n---\n- Explore the effects of varying prediction threshold on the results. \n- Use Amazon SageMaker to perform model optimization on extreme gradient boosting classifier using more effective methods (e.g., bayseian and random optimization)\n- Explore Deep learning solutions to help the model identifies contextual information. ","e8266308":"<a id='Adversial'><\/a>\n# Adversarial Testing","db3c3658":"Based on [data preprocessing](https:\/\/www.kaggle.com\/omarayman67\/pre-processing) results, it is clear that formulating the business opportunity into a **_binary classification problem_** could serve as a simple, yet effective first step toward more complex models. [Model Training](https:\/\/www.kaggle.com\/omarayman67\/model-training) reports training 4 different models on a sample data set. Furthermore, it explores the severe effect of the implanced data set on the binary classifiers. Finally, it mitigate the effects of label impalancing by performing down sampling to the majority class to help train the model more effectively. \n\nThe confusion matrix for the 4 models is reported below for both training and test sets. \n\n1- **Naive Bayes Classifier** \n\n<figure class=\"image\">\n    <table><tr>\n    <td> <img src=\"https:\/\/i.ibb.co\/ft6mFnb\/NB-Imbalanced.png\" alt=\"Naive Bayes Imbalance\" style=\"width: 800px;\"\/> <\/td>\n    <td> <img src=\"https:\/\/i.ibb.co\/VQvqWk1\/NB-down.png\" alt=\"Naive Bayes down sample\" style=\"width: 800px;\"\/> <\/td>    \n    <\/tr><\/table>\n    <figcaption><center>Left: before down sampling, after down sampling <\/center><\/figcaption>\n<\/figure>\n\n2- **Gradient Boosting Classifier** \n\n<figure class=\"image\">\n    <table><tr>\n    <td> <img src=\"https:\/\/i.ibb.co\/nmGkCTV\/GB-imbalanced.png\" alt=\"xgboost Imbalance\" style=\"width: 800px;\"\/> <\/td>\n    <td> <img src=\"https:\/\/i.ibb.co\/YZv24wx\/GB-down.png\" alt=\"xgboost down sample\" style=\"width: 800px;\"\/> <\/td>    \n    <\/tr><\/table>\n    <figcaption><center>Left: before down sampling, after down sampling <\/center><\/figcaption>\n<\/figure>","18574c00":"There are different ways to formulate a machine learning problem out of the aforementioned business opportunity. _For example_, it is possible to scale the customer satisfiction level from 1 to 5 or 10 and _hence_ it would be more appropriate to deal with it either as a `multi-class classification` problem or even as a `regression problem`. _Nevertheless_, there are more simpler versions of the problem which classify the text either has a _positive_ or _negative_ attitude and hence it is a `binary classification problem.`   \n\nThus, given the complexity of the given problem, the collective work of multiple workloads covers the whole machine learning pipline is presented in this report","134e4873":"# <center> Sentiment Analysis With Consumer Reviews of Amazon Products <\/center>\n\n<figure class=\"image\">\n    <table><tr>\n    <td> <img src=\"https:\/\/i.ibb.co\/YWbjqNv\/sentimentlogo2.jpg\" alt=\"sentimentlogo2\" style=\"width: 1000px;\"\/> <\/td>   \n    <\/tr><\/table>\n<\/figure>\n\n---\n\n# <u>Made by<\/u> :  <center> Omar Khater <\/center>\n---","c08ffd30":"`Natural Language Processing (NLP)` is one of the hottest trends that exploits the huge advancement in `machine learning` solutions but dedicated for dealing with textual data. A high overview on the pipeline used for any NLP solution is illustrated as well.\n\n\n<figure class=\"image\">\n    <table><tr>\n    <td> <img src=\"https:\/\/i.ibb.co\/K66sZXM\/NLP-pipeline.jpg\" alt=\"NLP Pipeline\" style=\"width: 1000px;\"\/> <\/td>   \n    <\/tr><\/table>\n<\/figure>","ae544744":"<a id='Pre_processing'><\/a>\n\n---\n# <center> Data Preprocessing And Visualization <\/center>\n---"}}