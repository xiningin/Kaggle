{"cell_type":{"1838595d":"code","261d0755":"code","4f9ede2e":"code","82cb2579":"code","bdd22d48":"code","4b4a214a":"code","036c5139":"code","65433a58":"code","c31a859a":"code","72f79f5a":"code","894115c4":"code","88947aee":"code","c97af982":"code","7986d65a":"code","7be02f41":"code","b846ec8f":"code","37dc534f":"code","66fedf37":"code","a3f13ce1":"code","4a27a4cd":"code","51c45637":"code","0719a964":"code","92c6f804":"code","a1d048e3":"code","097da56f":"code","b78a152f":"code","40d72ef1":"code","dc168de0":"code","9cb743fd":"code","4045419d":"code","cd65a13a":"code","80518b60":"code","93f483cb":"code","a1b533b7":"code","449e360a":"code","ff9f60c3":"code","2ca6e3fa":"code","705ce2e6":"code","7ccbc9aa":"code","631ebb26":"code","17ec3b52":"code","8c524ebb":"code","624e3ecf":"code","5adff900":"code","393f1027":"markdown","470d4ca9":"markdown","0e3de1f1":"markdown","8fd3e026":"markdown","62d20cbe":"markdown","d24a05b9":"markdown","d75c9f3e":"markdown","9acc1c66":"markdown","8e4d19e6":"markdown","f48a6d93":"markdown","581664d1":"markdown","2b05fd31":"markdown","c73ada66":"markdown","5552c613":"markdown","5937fd26":"markdown","e6da305a":"markdown","ea96b1cb":"markdown","39c26e7b":"markdown","4a7a2328":"markdown","d1358fba":"markdown"},"source":{"1838595d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","261d0755":"import h5py as h5fy","4f9ede2e":"h5f=h5fy.File('..\/input\/street-view-house-nos-h5-file\/SVHN_single_grey1.h5','r')","82cb2579":"h5f.keys","bdd22d48":"X_train = h5f['X_train'][:]\ny_train = h5f['y_train'][:]\nX_test = h5f['X_test'][:]\ny_test = h5f['y_test'][:]\nprint('X_train' ,X_train.shape)\nprint('y_train' ,y_train.shape)\nprint('X_test' ,X_test.shape)\nprint('y_test' ,y_test.shape)","4b4a214a":"import matplotlib.pyplot as plt\nplt.imshow(X_train[9][:][:])\nprint('Label: ', y_train[9])","036c5139":"from tensorflow.keras.utils import to_categorical","65433a58":"X_train.shape","c31a859a":"#Let's flatten in out and convert the 3D into 2D tumpy array\nX_train = X_train.reshape((X_train.shape[0], -1))\nX_test = X_test.reshape((X_test.shape[0], -1))","72f79f5a":"# converting y data into categorical (one-hot encoding)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","894115c4":"print(\"X_train\",X_train.shape,\" X_test\", X_test.shape, 'y_train',y_train.shape, 'y_test',y_test.shape)","88947aee":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import BatchNormalization, Dropout","c97af982":"def create_vanila_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, )))                  \n    model.add(Activation('relu'))    \n    model.add(Dense(50))                   \n    model.add(Activation('relu'))    \n    model.add(Dense(50))                   \n    model.add(Activation('relu'))    \n    model.add(Dense(50))                    \n    model.add(Activation('relu'))    \n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n    \n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","7986d65a":"    model = create_vanila_model()\n    history = model.fit(X_train, y_train, batch_size=200, epochs = 200,verbose = 0)","7be02f41":"results = model.evaluate(X_test, y_test)","b846ec8f":"def create_batchnorm_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, )))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n    \n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","37dc534f":"    model = create_batchnorm_model()\n    history = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","66fedf37":"results = model.evaluate(X_test, y_test)","a3f13ce1":"def mlp_kernel_init_Batch_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    \n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","4a27a4cd":"model = create_batchnorm_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","51c45637":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","0719a964":"def bn_ki_dropout_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd , loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","92c6f804":"model = bn_ki_dropout_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","a1d048e3":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","097da56f":"def adam_optimizer_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    \n    adam = optimizers.Adam(lr = 0.001)\n    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","b78a152f":"model = adam_optimizer_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","40d72ef1":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","dc168de0":"def adam_with_dropout_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    \n    adam = optimizers.Adam(lr = 0.001)\n    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","9cb743fd":"model = adam_with_dropout_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","4045419d":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","cd65a13a":"def sgd_momentum_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    sdgm = optimizers.SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer = sdgm, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","80518b60":"model = sgd_momentum_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","93f483cb":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","a1b533b7":"def sgd_momentum_ki_u_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='uniform'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='uniform'))\n    model.add(Activation('softmax'))\n    sdgm = optimizers.SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer = sdgm, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","449e360a":"model = sgd_momentum_ki_hu_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","ff9f60c3":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","2ca6e3fa":"def adam_ki_hn_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","705ce2e6":"model = adam_ki_hn_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","7ccbc9aa":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","631ebb26":"def adam_ki_hn_model_plus_one_Hiddnen():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu')) \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","17ec3b52":"model = adam_ki_hn_model_plus_one_Hiddnen()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 0)","8c524ebb":"#Let's evaluate with test data:\nresults = model.evaluate(X_test, y_test)","624e3ecf":"def adam2_with_dropout_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (1024, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dropout(0.5))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    \n    adam = optimizers.Adam(lr = 0.01 , beta_1=0.9 , decay =0)\n    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","5adff900":"model = adam2_with_dropout_model()\nhistory = model.fit(X_train, y_train, batch_size=100, epochs = 100, verbose = 1)","393f1027":"**Observation : Adam optimizer with dropout strategy looks a bit better as our test accuracy is ~76%.**","470d4ca9":"**Data Pre-processing**","0e3de1f1":"Let's Test by changing our network with a new hidden layer and observe if we get better result!","8fd3e026":"**Observation** : With the above hyper parameter tuinig our best model would have accuracy 80%.","62d20cbe":"Observation : We have the accuracy of 80%. Not tried with  Image Augmentation. This might have helped. Please do let me know if you have any comments on that!","d24a05b9":"**Read the data from the h5py file and understand the train\/test**","d75c9f3e":"**Seems just adding a kernel_initializer has no improvement. Let's try adding a dropout to train our model little better.**","9acc1c66":"**Observation : As we can see here, our model accuracy slightly decreased after addition of new hidden layer. This can be attributed to the model is short of started memorizing the trained dataset and will not generalize well. So let's don't change the structure of our NN**","8e4d19e6":"Observation : This shows that **we need all the features and can't afford to ignore any features**. Let's go back and don't use dropout. Instead may be we can try a different optimizer altogether.","f48a6d93":"Let's use a kernel initializer(The fancy term use just for initilizing weights :))","581664d1":"**Just 56% of accuracy is not acceptable. Let's use BatchNormalization.**","2b05fd31":"****Observation about dataset:****\n\n1) We have a tensor with 32*32*n Dimension.(n is number of rows in the tensor while each row as 32*32 matrix)\n\n2) We have training values interms of pixel seems. Each 32*32 defines an image of number.\n\n![](http:\/\/)3) We have to predict the number (0 to 9) which is our target variable. Our target is multi-level classification so it is evident that while, input shape is 32*32*n(each row of tensor being 32*32 pixel) and our o\/p or target has only one column i.e. from 0 to 9.","c73ada66":"### **Observation : With SGD momentum optimizer model looks a bit better as our test accuracy is ~80%.**","5552c613":"Surely 72% accuracy is not acceptable. But the significant of Batchnormalization is immense here. Let's try tuning our hyper parameters a bit further but keep the very nature of our NN intact.","5937fd26":"### Observation : With adam optimizer, model looks a bit better as our test accuracy is > 80%","e6da305a":"Observation : With SGD momentum optimizer and uniform kernel initializer. Seems not better than SGD momentum + he uniform KI.","ea96b1cb":"**let's plot one input row of our tensor**","39c26e7b":"Let's try with a Vanila model:\n1) Sequential model \n2) relu as Activation Function for input and Hidden Layer and softmax as Activation Function for our output layer\n3) With SGD optimizer and loss function categorical_crossentropy\n","4a7a2328":"So with **Adam as optimizer we got an over-fit model**. Let's go back to dropout strategy with Adam. would be worth to observe!","d1358fba":"Let's Build a NN "}}