{"cell_type":{"19985f52":"code","7738a4bb":"code","c5f6a157":"code","1dd2ade0":"code","336b456e":"code","70efc7dd":"code","6985ea98":"code","c524ffdb":"code","fb71777d":"code","458c1a02":"code","2fbee18e":"code","9ba9c5ab":"code","ecb1e78b":"markdown","12613413":"markdown","55a2ad51":"markdown","bdf74f4a":"markdown","08a9c3af":"markdown","f304d31e":"markdown","e1ed369d":"markdown","9d55c5d6":"markdown","5cca80c4":"markdown","584c51bc":"markdown","dfd07819":"markdown","8706e9b7":"markdown"},"source":{"19985f52":"import pandas as pd\nimport numpy as np\nimport string\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport collections\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","7738a4bb":"df=pd.read_csv(\"..\/input\/dataset.csv\", sep=\",\", na_values='?')\nDupes = ['Sponsors', 'Plotsize', 'Account2']\nfor col in Dupes:\n    df[col] = df[col].apply(lambda x: x.lower())\ndf['Plotsize'] = df['Plotsize'].apply(lambda x: \"me\" if (x==\"m.e.\") else x)\ndf['Motive'] = df['Motive'].apply(lambda x: \"q10\" if x==\"p10\" else x)\ndf['Plotsize'] = df['Plotsize'].apply(lambda x: \"1sm\" if x==\"sm\" else \"2me\" if x==\"me\" else \"3la\" if x==\"la\" else \"4xl\" if x==\"xl\" else x)","c5f6a157":"df_comp = df.copy()\ndf_comp_mod = df_comp.drop(['id'],1)\ntarget = df_comp['Class']","1dd2ade0":"df_full = pd.DataFrame()\nfor col in df_comp_mod.columns:\n    if(col in ['Account1','History','Motive','InstallmentRate','Tenancy Period','Age','Monthly Period']):\n        df_full[col] = df_comp_mod[col].fillna(df_comp_mod[col].mode()[0])\n    elif(col in ['InstallmentCredit', 'Yearly Period','Credit1']):\n        df_full[col] = df_comp_mod[col].fillna(df_comp_mod[col].mean())\n    else:\n        df_full[col] = df_comp_mod[col]","336b456e":"le = LabelEncoder()\ndf_full_oh = df_full.copy()\ndf_copy = df_full.copy()\nfor col in df_full.columns:\n    if(df_full[col].dtype == np.object):\n        le.fit(df_full[col])\n        df_full[col] = le.transform(df_full[col])","70efc7dd":"f, ax = plt.subplots(figsize=(20, 16))\ncorr = df_full.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax, annot = True);","6985ea98":"data = df_full.loc[:, df_full.columns != 'Class'].loc[:174]\ntarg = df_full['Class'].loc[:174]\nmodel = ExtraTreesClassifier(n_estimators=10, n_jobs=-1, random_state=0)\nmodel.fit(data,targ)\npreds = model.predict(data)\ndt = dict(zip(model.feature_importances_,np.array(data.columns)))\nod = collections.OrderedDict(sorted(dt.items()))\nitems = []\nfor key, value in sorted(dt.items()):\n    items.append(value)","c524ffdb":"def one_hot(df,sca = 'std'):\n    df_full_oh = df.copy()\n    oh_cols = []\n    for col in df_full_oh.columns:\n        if(df_full_oh[col].dtype == np.object and df_full_oh[col].nunique()>50):\n            le.fit(df_full_oh[col])\n            df_full_oh[col] = le.transform(df_full_oh[col])\n        elif(df_full_oh[col].dtype == np.object):\n            oh_cols.append(col)\n    df_full_oh = pd.get_dummies(df_full_oh, columns=oh_cols)\n    if(sca=='mms'):\n        sc = MinMaxScaler()\n    else:\n        sc = StandardScaler()\n    df_full_oh = pd.DataFrame(sc.fit_transform(df_full_oh),columns = df_full_oh.columns)\n    return df_full_oh","fb71777d":"def reduce_to_three(pred,tar):\n    val = pd.DataFrame(pred[:175],columns=['Class'])\n    tar = pd.DataFrame(tar.loc[:174],columns=['Class'])\n    pred = pd.DataFrame(pred,columns=['Class']) \n    for cls in pred['Class'].unique():\n        tst = tar[val['Class']==cls]\n        if(len(tst['Class'].value_counts())==0):\n            nw_cls = tar['Class'].value_counts().idxmax()\n        else:\n            nw_cls = tst['Class'].value_counts().idxmax()\n        pred['Class'].apply(lambda x: nw_cls if x==cls else x)\n    return pred,tar","458c1a02":"def best_acc(pred,tar):\n    pred,tar = reduce_to_three(pred,tar)  \n    best_ac = 0  \n    val = pd.DataFrame(pred[:175],columns=['Class'])\n    \n    combi = [[0,1,2],[0,2,1],[1,0,2],[1,2,0],[2,0,1],[2,1,0]]\n    out = pred\n    bi=0\n    \n    for i,comb in enumerate(combi):\n        pr_temp = val['Class'].apply(lambda x: comb[0] if x==0 else comb[1] if x==1 else comb[2])\n        acc_temp = accuracy_score(pr_temp,tar)\n        if(acc_temp>best_ac):\n            best_ac = acc_temp\n            out = pred['Class'].apply(lambda x: comb[0] if x==0 else comb[1] if x==1 else comb[2])\n            \n    return best_ac*100,out.as_matrix()","2fbee18e":"cols_rem = items[:12]\n\ntarg = df_full['Class']\ndf_main = df_copy.copy()\ndf_main = df_main.loc[:, df_main.columns != 'Class']\ndf_main.drop(cols_rem,axis=1,inplace=True)\ndata = one_hot(df_main,'std')\n\npca1 = PCA(n_components=2)\npca1.fit(data)\nT1 = pca1.transform(data)\n\nkmpred = KMeans(n_clusters = 5, random_state = 42).fit_predict(data)\nkmacc,kmout = best_acc(kmpred,targ)\n\nplt.title(\"Predicted KMeans Acc = \" + str(kmacc))\nplt.scatter(T1[:, 0], T1[:, 1], c=kmout)\n\nplt.show()\n\nprint(\"Model Accuracy: \",kmacc)\n\nif 'Class' in df.columns:\n    df.drop(['Class'],axis=1,inplace=True)\ndf['Class'] = kmout\n# df[['id','Class']].loc[175:].to_csv(\"Submissions\/submission.csv\",index=False)\nsub_df = df[['id','Class']].loc[175:]","9ba9c5ab":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(sub_df)","ecb1e78b":"## Function to reduce multiple clusters to 3","12613413":"## Submission for Kaggle Kernel","55a2ad51":"## Function to match predicted values and to print the best accuracy","bdf74f4a":"## Running an ExtraTrees Classifier to see feature importances","08a9c3af":"## IMPORTING NECESSARY LIBRARIES","f304d31e":"## Label Encoding","e1ed369d":"## MODEL after dropping the least 12 features and using 5 clusters initially","9d55c5d6":"## Filling all NANs","5cca80c4":"## Creating a copy of the Dataset","584c51bc":"## Importing Data and Pre-processing to modify duplicates","dfd07819":"## Heatmap to check correlations after Label Encoding all Categorical Variables","8706e9b7":"## Function to One-Hot Encode the Categorical COlumns"}}