{"cell_type":{"21e8d087":"code","d4b10986":"code","929680fa":"code","2a1f5a91":"code","44b2a50d":"code","9b8c15b6":"code","ba55e8e9":"code","300c340f":"code","021910d2":"code","5b1cb1a7":"code","e2beeb3c":"code","00c37945":"code","de65155e":"code","256b0431":"code","a5e1fe07":"code","11aaddd3":"code","bda658dd":"code","6aa9cb02":"code","6ce8e1f1":"code","c10ff3aa":"code","96725a44":"code","6ad7174a":"code","1372595a":"markdown","9aaa4ba2":"markdown","e4d27968":"markdown","d68b9691":"markdown","f879633e":"markdown","0bb586e6":"markdown","6a1067ca":"markdown","88becc1b":"markdown","5fb10234":"markdown","4c8c71b3":"markdown","33e0aa7f":"markdown","d2224f4c":"markdown","6d47e0f1":"markdown","14a0c64a":"markdown","525cdb11":"markdown","d0def814":"markdown","1fdfd007":"markdown"},"source":{"21e8d087":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.set()\n\nfrom sklearn.mixture import GaussianMixture\nfrom matplotlib.colors import LogNorm\nfrom matplotlib.ticker import MaxNLocator\n\nimport os\nprint(os.listdir(\"..\/input\"))","d4b10986":"data = pd.read_csv(\"..\/input\/Iris.csv\", index_col=0)\ndata.head()","929680fa":"features = [\"PetalLengthCm\", \"PetalWidthCm\"] ","2a1f5a91":"color_dict = {\"Iris-setosa\": \"deeppink\",\n\"Iris-versicolor\": \"Orange\",\n\"Iris-virginica\": \"Yellow\"}\n\nnames_dict = {\"Iris-setosa\": \"Setosa\",\n\"Iris-versicolor\": \"Versicolor\",\n\"Iris-virginica\": \"Virginica\"}\n\nfor species in color_dict.keys():\n    plt.scatter(data[data.Species==species][features[0]],\n    data[data.Species==species][features[1]],\n    c=color_dict[species],\n    label=names_dict[species])\n\n\nplt.xlabel(features[0])\nplt.ylabel(features[1])\nplt.title(\"Scatterplot\")\nplt.legend() ","44b2a50d":"def get_outliers(log_prob, epsilon):\n    outliners = np.where(log_prob <= epsilon, 1, 0)\n    return outliners ","9b8c15b6":"def make_density_plot(data, features, model, outliers):\n    \n    # display predicted scores by the model as a contour plot\n    x = np.linspace(data[features[0]].min(), data[features[0]].max())\n    y = np.linspace(data[features[1]].min(), data[features[1]].max())\n    X, Y = np.meshgrid(x, y)\n\n    XX = np.array([X.ravel(), Y.ravel()]).T\n    Z = model.score_samples(XX)\n    Z = Z.reshape(X.shape)\n\n    levels = MaxNLocator(nbins=100).tick_values(Z.min(), Z.max()) \n    cmap = plt.get_cmap('BuGn')\n\n    plt.figure(figsize=(10,10))\n    plt.contourf(X, Y, Z.reshape(X.shape), cmap=cmap, levels=levels) \n    plt.scatter(model.means_[:,0], model.means_[:,1], color=\"Red\")\n    g1 = plt.scatter(data[data.Outlier==0][features[0]].values,\n    data[data.Outlier==0][features[1]].values, label=\"Normal\",s=4.0,c=\"lightblue\")\n    g2 = plt.scatter(data[data.Outlier==1][features[0]].values,\n    data[data.Outlier==1][features[1]].values, label=\"Abnormal\",s=4.5,c=\"hotpink\")\n    plt.legend(handles=[g1,g2]) \n    \n    plt.xlabel(features[0])\n    plt.ylabel(features[1])\n    return plt","ba55e8e9":"X_train = data[features].values\n\nmodel = GaussianMixture(n_components=3, covariance_type=\"full\")\nmodel.fit(X_train)\nlog_prob = model.score_samples(X_train)\noutliers = get_outliers(log_prob, 0.15)\ndata[\"Outlier\"] = outliers ","300c340f":"plt.figure(figsize=(20,5))\n\nsns.distplot(log_prob, kde=False, bins=50, color=\"Red\")\ng1 = plt.axvline(np.quantile(log_prob, 0.25), color=\"Green\", label=\"Q_25\")\ng2 = plt.axvline(np.quantile(log_prob, 0.5), color=\"Blue\", label=\"Q_50 - Median\")\ng3 = plt.axvline(np.quantile(log_prob, 0.75), color=\"Green\", label=\"Q_75\")\ng4 = plt.axvline(np.quantile(log_prob, 0.05), color=\"Purple\", label=\"Q_5\")\nhandles = [g1, g2, g3, g4]\nplt.xlabel(\"log-probabilities of the data spots\")\nplt.ylabel(\"frequency\")\nplt.legend(handles) ","021910d2":"epsilon = np.quantile(log_prob, 0.05)\nprint(\"epsilon: %f\" % epsilon)","5b1cb1a7":"outliers = get_outliers(log_prob, epsilon)\ndata[\"Outlier\"] = outliers","e2beeb3c":"make_density_plot(data, features, model, outliers)","00c37945":"data.head() \ndata.Outlier.value_counts() ","de65155e":"species_probas = np.round(model.predict_proba(X_train),2)\nbest_species_idx = np.argmax(species_probas,axis=1)\ndata[\"Predicted\"] = best_species_idx","256b0431":"def map_to_species(data, features):\n    grouped = data.groupby(\"Species\")[features].mean()\n    feature_means = grouped.values\n    \n    names_map = {}\n    for m in data.Predicted.unique():\n        predicted_mean = data[data.Predicted == m][features].mean().values\n        distances = np.zeros(feature_means.shape[0])\n    \n        for f in range(feature_means.shape[0]):\n            distances[f] = np.linalg.norm(predicted_mean - feature_means[f])\n        name = grouped.index.values[np.argmin(distances)]\n        names_map[m] = name  \n    \n    return names_map","a5e1fe07":"names_map = map_to_species(data, features)","11aaddd3":"data[\"Predicted_Species\"] = data[\"Predicted\"].apply(lambda l: names_map[l])","bda658dd":"ig, ax = plt.subplots(1,2,figsize=(20,5))\n\nfor species in color_dict.keys():\n    ax[0].scatter(data[data.Species==species][features[0]],\n                data[data.Species==species][features[1]],\n                c=color_dict[species],\n                label=names_dict[species])\n    ax[1].scatter(data[data.Predicted_Species==species][features[0]],\n                data[data.Predicted_Species==species][features[1]],\n                c=color_dict[species],\n                label=names_dict[species])\n\nax[0].set_xlabel(features[0])\nax[0].set_ylabel(features[1])\nax[0].set_title(\"True species\")\nax[0].legend()\n\nax[1].set_xlabel(features[0])\nax[1].set_ylabel(features[1])\nax[1].set_title(\"Predicted species\")\nax[1].legend()","6aa9cb02":"color = np.zeros(best_species_idx.shape[0])\nfor n in range(len(color)):\n    color[n] = species_probas[n,best_species_idx[n]]","6ce8e1f1":"data[\"Correctness\"] = np.where(data.Predicted_Species == data.Species, 1, 0)","c10ff3aa":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nax[0].scatter(data[data.Predicted == 0][features[0]],\n            data[data.Predicted == 0][features[1]],\n            c=color[data.Predicted == 0],\n            cmap=\"Reds\", \n            vmin=0, \n            vmax=1)\nax[0].scatter(data[data.Predicted == 1][features[0]],\n            data[data.Predicted == 1][features[1]],\n            c=color[data.Predicted == 1],\n            cmap=\"Greens\", \n            vmin=0, \n            vmax=1)\nax[0].scatter(data[data.Predicted == 2][features[0]],\n            data[data.Predicted == 2][features[1]],\n            c=color[data.Predicted == 2],\n            cmap=\"Blues\", \n            vmin=0, \n            vmax=1)\nax[0].set_xlabel(features[0])\nax[0].set_ylabel(features[1])\nax[0].set_title(\"Uncertainty of species-predicition\")\nax[1].scatter(data[features[0]],\n              data[features[1]],\n              c=data.Correctness.values,\n              cmap=\"RdYlGn\", vmin=0,vmax=1 )\nax[1].set_xlabel(features[0])\nax[1].set_ylabel(features[1])\nax[1].set_title(\"Correctness of species-predictions\")","96725a44":"all_features = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\n\nY_train = data[all_features].values\n\n\nmodel_all = GaussianMixture(n_components=3, covariance_type=\"full\")\nmodel_all.fit(Y_train)\nnew_log_prob = model_all.score_samples(Y_train)\noutliers = get_outliers(new_log_prob, 0.15)\ndata[\"Outlier\"] = outliers\n\nspecies_probas_all = np.round(model_all.predict_proba(Y_train),2)\nbest_species_all_idx = np.argmax(species_probas_all,axis=1)\ndata[\"Predicted\"] = best_species_all_idx\n\ndef map_to_species(data, features):\n    grouped = data.groupby(\"Species\")[all_features].mean()\n    all_feature_means = grouped.values\n    \n    names_map = {}\n    for m in data.Predicted.unique():\n        all_predicted_mean = data[data.Predicted == m][all_features].mean().values\n        all_distances = np.zeros(all_feature_means.shape[0])\n    \n        for f in range(all_feature_means.shape[0]):\n            all_distances[f] = np.linalg.norm(all_predicted_mean - all_feature_means[f])\n        name = grouped.index.values[np.argmin(all_distances)]\n        names_map[m] = name  \n    \n    return names_map\n\nnames_map = map_to_species(data, all_features)\n\ndata[\"Predicted_Species\"] = data[\"Predicted\"].apply(lambda l: names_map[l])\n\ncolor = np.zeros(best_species_all_idx.shape[0])\nfor n in range(len(color)):\n    color[n] = species_probas_all[n,best_species_all_idx[n]]\n\ndata[\"Correctness\"] = np.where(data.Predicted_Species == data.Species, 1, 0)\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nax[0].scatter(data[data.Predicted == 0][features[0]],\n            data[data.Predicted == 0][features[1]],\n            c=color[data.Predicted == 0],\n            cmap=\"Reds\", \n            vmin=0, \n            vmax=1)\nax[0].scatter(data[data.Predicted == 1][features[0]],\n            data[data.Predicted == 1][features[1]],\n            c=color[data.Predicted == 1],\n            cmap=\"Greens\", \n            vmin=0, \n            vmax=1)\nax[0].scatter(data[data.Predicted == 2][features[0]],\n            data[data.Predicted == 2][features[1]],\n            c=color[data.Predicted == 2],\n            cmap=\"Blues\", \n            vmin=0, \n            vmax=1)\nax[0].set_xlabel(features[0])\nax[0].set_ylabel(features[1])\nax[0].set_title(\"Uncertainty of species-predicition\")\nax[1].scatter(data[features[0]],\n              data[features[1]],\n              c=data.Correctness.values,\n              cmap=\"RdYlGn\", vmin=0,vmax=1 )\nax[1].set_xlabel(features[0])\nax[1].set_ylabel(features[1])\nax[1].set_title(\"Correctness of species-predictions\")","6ad7174a":"ig, ax = plt.subplots(1,2,figsize=(20,5))\n\nfor species in color_dict.keys():\n    ax[0].scatter(data[data.Species==species][features[0]],\n                data[data.Species==species][features[1]],\n                c=color_dict[species],\n                label=names_dict[species])\n    ax[1].scatter(data[data.Predicted_Species==species][features[0]],\n                data[data.Predicted_Species==species][features[1]],\n                c=color_dict[species],\n                label=names_dict[species])\n\nax[0].set_xlabel(features[0])\nax[0].set_ylabel(features[1])\nax[0].set_title(\"True species\")\nax[0].legend()\n\nax[1].set_xlabel(features[0])\nax[1].set_ylabel(features[1])\nax[1].set_title(\"Predicted species\")\nax[1].legend()","1372595a":"# Visualize the abnormal data","9aaa4ba2":"The plots above show us the comparison between the original, true species-assignment on the right and the predictes species-assignment on the left. On first sight we can see that our model predicted most of the species right, there are only some differences between the Versicolor- and Virginica-species.\n\nFor a better visualization we will look at the uncertainty our model works with in the plot below:","e4d27968":"In the left plot we can see that our model is now way more certain about it predictions, because the amount of darker spots increased. So we would think that its predictions are more correct now. But by comparing this uncertainty-plot to the correctness-plot on the right we can see that there is still the same amount of wrong predictions. Even some spots that our model is really safe in predicting right were predicted false. In the PetalLengthCm\/PetalWidthCm-Plot we can see that there is only one false predicted data spot where our model really was uncertain about.\nSo it seems like our model got a bit confused!","d68b9691":"# Introduction\n\nIn this Kernel we are working with the Iris-Species dataset, which includes three iris species with a sample size of 50, combined with four measuring sizes: Sepal-Length, Sepal-Width, Petal-Length and Petal-Width.\nOur aim is to analyse the given dataset so we can detect abnormal data.\nThe Iris-Species data is perfect for anomaly detection because of it's clear and complete structrue, but also because every species has the same amount of given data. \n\nFor our analysis we want to use the Gaussian Mixture Model. This model is really handy for our aim to detect abnormal data  and to make predictions of the species per plant. It combines several multivariate normal distributions like so:\n\n$$ p(x) = \\sum_{k=1}^{K} \\pi_{k} \\cdot N(x|\\mu_{k}, \\Sigma_{k}) $$ \n\nIn our case we choose three normal distributions (one for each species):\n\n$$ p(x) = \\pi_{1} \\cdot N(x|\\mu_{1}, \\Sigma_{1}) + \\pi_{2} \\cdot N(x|\\mu_{2}, \\Sigma_{2}) + \\pi_{3} \\cdot N(x|\\mu_{3}, \\Sigma_{3}) $$ \n\np(x) is our probability density that tells us if a data spot is in a region of high or low density. This way we can decide if a data spot $x_{n}$ is near to other spots or not as its quantity $p(x_{n})$ stands for the probability of a data spot belonging to our known dataset or being abnormal data. \n\nOn the other hand the Gaussian Micture Model can also predict to which species a data spot belongs. As we assume that each species has its own normal distribution we can obtain the probabilities that a data spot belongs to one specific species by calculating   $\\pi_{species_{i}} \\cdot N(x_{spot}|\\mu_{species_{i}}, \\Sigma_{species_{i}})$ . Per spot the probabilites over all species sum up to 1.\n\n# Getting started\n\nFirst of all we have to import all librarys we need and the dataset we want to work with:\n","f879633e":"# The data\n\nNow we want to have a look at the given dataset:","0bb586e6":"As we expected there are three different species with the same amount of data points. \nThe plots of all feature-combinations have a something in common: The data-clusters of the Versicolor- and Virginca-species are overlapping a little bit and are therefore near each other, while the Setosa-species is always separated from the other two species.\n\n*Note for SepalLengthCm and SepalWidthCm:* In this plot the overlapping of Versicolor and Virginica is more distinct than in the other plots. This could lead to some problems in the further analysis: In the Sepal-Length range of about 5.5 to 7.0 and the Sepal-Width range of about 2.3 to 3.3 it's hard to tell if a iris is a Versicolor or a Verginica - so probably our model won't be able to decide between these two species either. This is a much bigger range than in the plots of the other feature-combinations and thus a much higher range of vageueness.\n\n","6a1067ca":"In the sections above we tried to predict the species of our data spots by looking at only two features. Now we want to see what happens if we take all features into account. We would suspect that the predictions get better, because then our model has more information about  each data spot and could predict the species with more certainty.\nWe will then visualize the outcome just like above for a better comparison. Let's see what happens!","88becc1b":"We can also look at the number of abnormal data our model predicted:","5fb10234":"The left plot above shows the uncertainty of our model, which occurs by predicting the assignment of the species: The darker the color the more certain the model is by making it's prediction.\nAs we can see the prediction of the Setosa-species (green) is really reliable in all data spots, while the predictions of Versicolor (red) and Virginica (blue) gets more uncertain the closer you are getting to the overlapsing part of both data spots. Therefore it is most likely that we can find some false predictions in this area, while the predictions of the other data spots should be mainly right.\n\nIn the right plot we can see that this is really the case. It shows for which data spots the species was assigned right (all green spots) and for which the prediciton was wrong (red spots). As we can see the red spots (almost) only appear in the area between the Versicolor and the Virginica clusters. ","4c8c71b3":"As we can see the data consists of five columns:\n* Id\n* SepalLengthCm\n* SepaWidthCm\n* PetalLengthCm\n* PetalWidthCm\n* Species\n\nFor further analyzes we now want to look at two of the features only so we can get a clearer outcome. I chose \"PetalLengthCm\" and\"PetalWidthCm\", but by changing the strings in the \"features\"-list below you can pick the features you want to have a look at. \nNow let's look at our data in dependency of these two features:","33e0aa7f":"# Choosing the outlier threshold","d2224f4c":"# Fitting with all features","6d47e0f1":"# Implement the Anomaly Detection","14a0c64a":"# Predicting the species\n\nAfter detecting the abnormal data with our Gaussian Mixture Model we now want to see if our model can also predict the species of our data spots by finding clusters in our data:","525cdb11":"Before we can detect the abnormal data spots we have to decide what data we label as abnormal. This is done through an epsilon in our get_outliers-function which we implemented above. We now have to choose this epsilon.\nTherefore we now plot the frequency of the log-probabilities of our data spots and mark the median, the 25%- and 50%-percentile with a line to see where we can find most of our data.\nThen we are also plotting a line which divides our data in 5% and 95%, so that everything below this line are our outliers. This line is the violet line in the plot below.\nThe log-probability where this violet line appers is going to be our epsilon.","d0def814":"In the plot below we can see the following things:\nThe big red points are the means of the three species-distributions. \nThe pink spots describe the abnormal data. These are all spots outside of the distributions (so all spots in the light green area). For them the probability of belonging to one of the species is, recording to our model, very low; but as we already know we could totally assign them to one species. Therefore it is really helpfull to know which data spots are abnormal.\n\n*Note for SepalLenghtCm and SepalWidthCm \/ for PetalLengthCm and SepalWidthCm: *  The means of Versicolor and Verginica are close together so the distributions of these two are overlapping. Because of that it seems like we only have two distributions in our plot instead od three.\n\n*Note for PetalLengthCm and PetalWidthCm:* The data spots of the Setosa-species are much more crowded compared to the more widely spread data spots of Versicolor and Virginica. Because of these distinct density differences the probability of a data spot being in the range of the Setosa-species is much higher than being in one of the other two ranges. Therefore our plot below seems to be \nunbalanced.\n\n*Note for SepalLengthCm an PetalWidthCm:* In this plot we can see a combination of the two situations above: Again the density differences are so distinct that the probability of a data spot being in the Setosa-range is much higher than belonging to one of the other species. We can also see that the means of Versicolor and Virginica are close together so their normal distributions are overlapping, though not as strong as with the SepalLength-SepalWidth or the PetalLength-SepalWidth combination.","1fdfd007":"So as we can see our epsilon should be about -2.83."}}