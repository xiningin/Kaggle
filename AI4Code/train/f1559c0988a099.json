{"cell_type":{"5bce4a9b":"code","43f5f62f":"code","f87bcfd3":"code","c765c935":"code","22cdc78f":"code","801b413e":"code","3c498c4b":"code","10dfeb51":"code","e4b8afe9":"code","ecc5fa9c":"code","18da8382":"code","86caf7d3":"code","c46e79d3":"code","3bae7919":"code","c45544b1":"code","4d42cf13":"code","90a1a2b6":"markdown","365359a8":"markdown"},"source":{"5bce4a9b":"import numpy as np\nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom sklearn import metrics\n\nplt.rcParams['figure.figsize'] = [12, 7]\nsns.set(rc={'figure.figsize':(7,8)})","43f5f62f":"data = pd.read_csv(\"..\/input\/coffee-quality-database-from-cqi\/arabica_data_cleaned.csv\")\ndata.head()","f87bcfd3":"data.info()","c765c935":"msno.matrix(data)","22cdc78f":"df = data[[\"Country.of.Origin\", \"Harvest.Year\", \"Variety\", \"Processing.Method\", \"Category.One.Defects\", \"Category.Two.Defects\", \"Quakers\", \"altitude_mean_meters\", \"Total.Cup.Points\"]]\ndf = df.dropna()\ndf = df.reset_index()\n\ndf = df.drop(\"index\", axis = 1)\ndf.head()","801b413e":"cleaned_df = df[[\"Country.of.Origin\", \"Harvest.Year\", \"Variety\", \"Processing.Method\", \"Category.One.Defects\", \"Category.Two.Defects\", \"Quakers\", \"altitude_mean_meters\", \"Total.Cup.Points\"]]\n\ncleaned_df.head()\ncleaned_df.plot(kind='box', subplots=True, layout=(2,3), \n        sharex=False, sharey=False)\nplt.show()","3c498c4b":"#All the data preparation goes here\n\n#Melakukan data cleaning untuk features harvest year\ncleaned_df.loc[cleaned_df[\"Harvest.Year\"] == \"2017 \/ 2018\", \"Harvest.Year\"] = \"2018\"\ncleaned_df.loc[cleaned_df[\"Harvest.Year\"] == \"2016 \/ 2017\", \"Harvest.Year\"] = \"2017\"\ncleaned_df.loc[cleaned_df[\"Harvest.Year\"] == \"2015\/2016\", \"Harvest.Year\"] = \"2016\"\ncleaned_df.loc[cleaned_df[\"Harvest.Year\"] == \"2014\/2015\", \"Harvest.Year\"] = \"2015\"\ncleaned_df.loc[cleaned_df[\"Harvest.Year\"] == \"2013\/2014\", \"Harvest.Year\"] = \"2014\"\ncleaned_df.loc[cleaned_df[\"Harvest.Year\"] == \"2011\/2012\", \"Harvest.Year\"] = \"2012\"\n\n#Mengkelompokan data negara asal menjadi others untuk negara yang memiliki data kopi dibawah 10\na = cleaned_df['Country.of.Origin'].value_counts() <= 5\nb = cleaned_df['Country.of.Origin'].value_counts()\nfor i in range(len(a.index)):\n    if(a[i]):\n        cleaned_df.loc[cleaned_df[\"Country.of.Origin\"] == a.index[i], \"Country.of.Origin\"] = \"Others\"\n        \n#Mengkelompokan data negara asal menjadi others untuk negara yang memiliki data kopi dibawah 10\na = cleaned_df['Variety'].value_counts() <= 1\nb = cleaned_df['Variety'].value_counts()\nfor i in range(len(a.index)):\n    if(a[i]):\n        cleaned_df.loc[cleaned_df[\"Variety\"] == a.index[i], \"Variety\"] = \"Others\"\n\n#Menghapus data altitude outliers yaang tidak masuk akal\ncleaned_df.drop(cleaned_df.loc[cleaned_df['altitude_mean_meters'] > 2000].index, inplace = True) \ncleaned_df.drop(cleaned_df.loc[cleaned_df['altitude_mean_meters'] < 182].index, inplace = True) \n\n#Melakukan perbaikan tipe data pada variabel dalam dataset \ncleaned_df.loc[:,\"Category.One.Defects\"] = cleaned_df[\"Category.One.Defects\"].astype(int)\ncleaned_df.loc[:,\"Harvest.Year\"] = cleaned_df[\"Harvest.Year\"].astype(int)\ncleaned_df.loc[:,\"Total.Cup.Points\"] = cleaned_df[\"Total.Cup.Points\"].astype(float)\ncleaned_df[\"Quakers\"] = cleaned_df[\"Quakers\"].astype(int)\n\n\n#Membuat features grading biji kopi berdasarkan features defects dan quakers\ncut_labels = [\"Specialty\", \"Premium\", \"Exchange\", \"Below Standard\"] # 1 = Specialty Grade, 2 = Premium Coffee Grade, 3 = Exchange Coffee Grade\ncut_bins = [-1, 3, 15, 23, 100]\ncleaned_df['Green.Beans.Grade'] = cleaned_df[\"Category.One.Defects\"].values + cleaned_df[\"Category.Two.Defects\"].values + cleaned_df[\"Quakers\"].values\ncleaned_df['Green.Beans.Grade'] = pd.cut(cleaned_df['Green.Beans.Grade'], bins=cut_bins, labels=cut_labels)\n\n#Membuat features Total Cupping Point menjadi categorical, menjadi features Cupping.Grade\ncut_labels = [\"UGQ\", \"Premium\", \"Specialty\"] # 1 = Specialty Quality, 2 = Premium Quality, 3 = Usually Good Quality\ncut_bins = [50, 80, 84, 90]\ncleaned_df['Cupping.Grade'] = pd.cut(cleaned_df['Total.Cup.Points'], bins=cut_bins, labels=cut_labels)\n\ncleaned_df.info()","10dfeb51":"#Mengambil features yang akan digunakan\nmodel_df = cleaned_df[[\"Country.of.Origin\", \"Harvest.Year\", \"Variety\", \"Processing.Method\", \"Green.Beans.Grade\", 'Cupping.Grade', \"Category.One.Defects\",\t\"Category.Two.Defects\",\t\"Quakers\"]]\nax = sns.countplot(x=\"Cupping.Grade\", data=model_df)\nax.tick_params(labelsize=15)","e4b8afe9":"model_df = cleaned_df[[\"Country.of.Origin\", \"Harvest.Year\", \"Variety\", \"Processing.Method\", \"Green.Beans.Grade\", 'Cupping.Grade', \"Category.One.Defects\",\t\"Category.Two.Defects\",\t\"Quakers\"]]\ndf1 = model_df.loc[model_df[\"Cupping.Grade\"] == \"UGQ\"]\ndf2 = model_df.loc[model_df[\"Cupping.Grade\"] == \"Specialty\"]\nframes = [model_df, df1, df1, df2, df2]\nmodel_df = pd.concat(frames)\n\nax = sns.countplot(x=\"Cupping.Grade\", data=model_df)\nax.tick_params(labelsize=15)","ecc5fa9c":"model_df.head()","18da8382":"# Label Encoding\nfrom sklearn import preprocessing\nencoder = preprocessing.LabelEncoder()\nencode_df = model_df.copy()\ncolumn_name = [\"Country.of.Origin\", \"Harvest.Year\", \"Variety\", \"Processing.Method\", \"Green.Beans.Grade\", 'Cupping.Grade']\n\nlabel = list()\nfor i in range(0,6):\n    encoder.fit(encode_df[column_name[i]])\n    encode_df.loc[:,column_name[i]] = (encoder.transform(encode_df[column_name[i]]))\n    label.append(encoder.inverse_transform(encode_df[column_name[i]]))\n\n    unique, counts = np.unique(label[i], return_counts=True)\n    print(np.asarray((unique, counts)).T)\n    unique, counts = np.unique(encode_df.loc[:,column_name[i]], return_counts=True)\n    print(np.asarray((unique, counts)).T)","86caf7d3":"# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX = encode_df.drop(\"Cupping.Grade\", axis = 1)\nY = encode_df[\"Cupping.Grade\"]\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n\nprint('Training Features Shape:', x_train.shape)\nprint('Training Labels Shape:', y_train.shape)\nprint('Testing Features Shape:', x_test.shape)\nprint('Testing Labels Shape:', y_test.shape)\n","c46e79d3":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestClassifier\n# Instantiate model with 1000 decision trees\n\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 9)\nrf = RandomForestClassifier(n_estimators = 30, random_state = 42)\n# Train the model on training data\nrf.fit(x_train, y_train)\ny_pred=rf.predict(x_test)\n\nprint(\"Accuracy: %0.5f\" % (metrics.accuracy_score(y_test, y_pred)))","3bae7919":"from sklearn.tree import export_graphviz\nfrom sklearn import tree\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\n\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'D:\/Anaconda\/Library\/bin\/graphviz'\n\na = [\"Premium\", \"Specialty\", \"UGQ\"]\nlabels = X.columns\ngraph = Source(tree.export_graphviz(rf[10] ,feature_names = labels, class_names = a, max_depth = 2, filled = True))\ndisplay(SVG(graph.pipe(format='svg')))","c45544b1":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n\ncm = (confusion_matrix(y_test, y_pred))\na = [\"Premium\", \"Specialty\", \"UGQ\"]\n\nsns.heatmap(cm, xticklabels = a, yticklabels = a,annot=True, fmt='g')\nprint(classification_report(y_test, y_pred))","4d42cf13":"import scikitplot as skplt\n\ny_probas = rf.predict_proba(x_test)\nskplt.metrics.plot_roc_curve(y_test, y_probas)\nplt.show()","90a1a2b6":"> For this Classification Problem, i'm using features based on geographic, post-harvest information, category defects and total cupping point. I want to know how this variables can affect the flavor quality(Total.Cup.Points - Grading by Coffee Q.Grader) ","365359a8":"> In this dataset, The data are unbalanced, To solve this im gonna do Random Over Sampling"}}