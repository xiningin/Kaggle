{"cell_type":{"0f1214af":"code","8b2564eb":"code","5a13aa12":"code","c3b9661e":"code","3e0c5976":"code","34ab6568":"code","9f2d8ec7":"code","31d9d071":"code","1fdc315f":"code","ee94a980":"code","10121565":"code","4858d553":"code","2271f356":"code","71106991":"code","33bc6ec8":"code","21e25c55":"code","1700b802":"code","944bcd4b":"code","558658fe":"code","cb4dc14b":"code","097467c7":"code","e5f74cb4":"code","f701bb9e":"code","69726ca2":"code","2f17e074":"code","cd16f21a":"code","fa13d15a":"code","d609d87c":"code","d70be5c4":"code","2107bb75":"code","e6082beb":"code","a49e271a":"code","0d9ca29e":"code","24f9e959":"code","d48975e3":"code","adaac6ca":"code","ee81d002":"code","ef902e37":"code","71dcefe8":"code","4dce3020":"code","66c8bb66":"code","8963770c":"code","277b3371":"code","646ecfa3":"code","fa742af9":"code","d45f9228":"code","14ec1ec8":"code","f9495262":"code","17427821":"code","33c6dd2e":"code","ae8abc0d":"code","52170031":"code","4cad7ad7":"code","5190f10b":"code","efc14498":"code","0717d1de":"code","5f6e13d8":"code","5f737271":"code","6131805c":"code","28995f3e":"code","e8025c29":"code","9ca50505":"code","104936d0":"code","48376e2a":"code","d9fa90ba":"code","cc70ae2e":"markdown"},"source":{"0f1214af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8b2564eb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","5a13aa12":"import warnings\nwarnings.filterwarnings('ignore')","c3b9661e":"df_raw = pd.read_csv(\"\/kaggle\/input\/paysim1\/PS_20174392719_1491204439457_log.csv\") #, nrows=int(1e6))","3e0c5976":"df = df_raw.copy(deep=True)\n#df.info()","34ab6568":"#Dataset's shape\ndf.shape","9f2d8ec7":"df = df.rename(columns={'oldbalanceOrg':'oldBalanceOrig', 'newbalanceOrig':'newBalanceOrig', \\\n                        'oldbalanceDest':'oldBalanceDest', 'newbalanceDest':'newBalanceDest'})","31d9d071":"list(df.columns.values)","1fdc315f":"df.head(3)","ee94a980":"df.tail(3)","10121565":"df.info()","4858d553":"df.describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.99]).round(2).T","2271f356":"#Check null values\ndf.isnull().values.any()  #df.isnull().sum()","71106991":"# 'isFlaggedFraud' column analysis\ndf_temp_Frag_verification = df[df['isFraud']==1]\nprint(\"Total Fraud:\",df_temp_Frag_verification.shape[0])\nprint(\"\\n\\n'isFraud'x'isFlaggedFraud' Analysis: \\n\\n\",(df_temp_Frag_verification.groupby(['isFraud','isFlaggedFraud'])['step'].size()))\nprint(\"\\n'isFlaggedFraud' accuracity:\", (df[df['isFlaggedFraud']==1].shape[0]\/df_temp_Frag_verification.shape[0]))","33bc6ec8":"del df_temp_Frag_verification","21e25c55":"#Conclusion: The 'isFlaggedFraud' accuracity is almost irrelevant, we will drop it from our dataset soon.","1700b802":"corrmat = df.corr()\nsns.set(font_scale=1.15)\nf, ax = plt.subplots(figsize=(8, 6))\nhm = sns.heatmap(corrmat, \n                 cbar=True, \n                 annot=True, \n                 square=True, \n                 fmt='.2f', \n                 annot_kws={'size': 10}, \n                 yticklabels=corrmat.columns, \n                 xticklabels=corrmat.columns)","944bcd4b":"sns.heatmap(df[['amount','isFraud']].corr(),annot = True)","558658fe":"df['step_day'] = df['step'].map(lambda x: x\/\/24)","cb4dc14b":"df['hour'] = df['step_day'].map(lambda x: x%24)","097467c7":"df['step_week'] = df['step_day'].map(lambda x: x\/\/7)","e5f74cb4":"df['step_day'].value_counts().sort_index(ascending=True).plot('bar')\nplt.xlabel(\"Days of the month\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of transactions by days of the month\")","f701bb9e":"df['hour'].value_counts().sort_index(ascending=True).plot('bar')\nplt.xlabel(\"Hours of the day\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of transactions by hours of the day\")","69726ca2":"df['step_week'].value_counts().sort_index(ascending=True).plot('bar')\nplt.xlabel(\"Weeks of the month\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of transactions by weeks of the month\")","2f17e074":"df[(df.isFraud == 0)]['hour'].plot.hist(bins=24,color='blue',label='Valid')\nplt.xlabel(\"Hours of the day\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of Valid transactions by hours of the day\")\nplt.legend()\nplt.show()\n","cd16f21a":"df[(df.isFraud == 1)]['hour'].plot.hist(bins=24,color='orange',label='Fraud')\nplt.xlabel(\"Hours of the day\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of Fraud transactions by hours of the day\")\nplt.legend()\nplt.show()","fa13d15a":"df[(df.isFraud == 0)]['step_day'].plot.hist(bins=7,color='blue',label='Valid')\nplt.xlabel(\"Days of the month\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of Valid transactions by days of the month\")\nplt.legend()\nplt.show()","d609d87c":"fig, ax = plt.subplots() #1,2, figsize=(7,9)\ndf[(df.isFraud == 1)]['step_day'].plot.hist(bins=7,color='orange',label='Fraud')\nplt.xlabel(\"Days of the month\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of Fraud transactions by days of the month\")\nplt.legend()\nplt.show()","d70be5c4":"df[(df.isFraud == 0)]['step_week'].plot.hist(bins=4,color='blue',label='Valid')\nplt.xlabel(\"Weeks of the month\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of Valid transactions by weeks of the month\")\nplt.legend()\nplt.show()","2107bb75":"df[(df.isFraud == 1)]['step_week'].plot.hist(bins=4,color='orange',label='Fraud')\nplt.xlabel(\"Weeks of the month\")\nplt.ylabel(\"# of transactions\")\nplt.title(\"# of Fraud transactions by weeks of the month\")\nplt.legend()\nplt.show()","e6082beb":"df['ID_NO'] = df.nameOrig.map(lambda x: x[:1])\ndf['ID_NO'].value_counts()","a49e271a":"df['ID_ND'] = df.nameDest.map(lambda x: x[:1])\ndf['ID_ND'].value_counts(1)","0d9ca29e":"df['ID']=df['ID_NO']+df['ID_ND']\ndf['ID'].value_counts(1).round(2)","24f9e959":"df.groupby(['type','isFraud'])['type'].count()   #,'amount_O','amount_D'","d48975e3":"df['wrong_orig_bal'] = np.where((df[\"oldBalanceOrig\"] - df[\"amount\"] - df[\"newBalanceOrig\"]>0.01)|(df[\"oldBalanceOrig\"] - df[\"amount\"] - df[\"newBalanceOrig\"]<-0.01),1,0)\ndf['wrong_dest_bal'] = np.where((df[\"newBalanceDest\"] + df[\"amount\"] - df[\"newBalanceDest\"]>0.01)|(df[\"newBalanceDest\"] + df[\"amount\"] - df[\"newBalanceDest\"]>0.01),1,0)","adaac6ca":"df.groupby(['wrong_orig_bal','isFraud']).size()#\/len(df)).round(4)*100","ee81d002":"(df.groupby(['wrong_orig_bal','isFraud']).size()\/len(df)).round(4)*100","ef902e37":"df.groupby(['wrong_dest_bal','isFraud']).size()#\/len(df)).round(4)*100","71dcefe8":"(df.groupby(['wrong_dest_bal','isFraud']).size()\/len(df)).round(4)*100","4dce3020":"df.head(5).T","66c8bb66":"df['nameOrig'].value_counts().sort_values(ascending=False).head(10)","8963770c":"round(len(df['nameOrig'].value_counts())\/len(df),3)*100","277b3371":"df['nameDest'].value_counts().sort_values(ascending=False).head(10)\n","646ecfa3":"round(len(df['nameDest'].value_counts())\/len(df),3)*100","fa742af9":"De_Para_type={'type':['PAYMENT','TRANSFER','CASH_OUT','DEBIT','CASH_IN'],'type_num':[0,1,2,3,4]}\nDe_Para_ID={'ID':['CC','CM','MC','MM'],'ID_num':[0,1,10,11]}\ndf_Dp_type = pd.DataFrame(De_Para_type, columns = ['type', 'type_num'])\ndf_Dp_ID = pd.DataFrame(De_Para_ID, columns = ['ID', 'ID_num'])\nprint(df_Dp_ID)\nprint()\nprint(df_Dp_type)","d45f9228":"df_temp = pd.merge(df, df_Dp_type, left_on='type', right_on='type')\ndf = pd.merge(df_temp, df_Dp_ID, left_on='ID', right_on='ID')\ndf.head()","14ec1ec8":"df.type.unique()","f9495262":"#Get dummies for the type feature\ndf_get_type = pd.get_dummies(df['type'], drop_first=True)","17427821":"df = pd.concat([df,df_get_type],axis=1)","33c6dd2e":"#del df_Dp_ID, df_Dp_type, df_get_type","ae8abc0d":"df.head(3).T","52170031":"#Dropping features as previously mentioned\ndf_clean = df.drop(columns=['type','nameOrig','oldBalanceOrig','nameDest','oldBalanceDest','isFlaggedFraud'], axis=1) #inplace=True","4cad7ad7":"df_clean = df_clean.drop(columns=['ID_NO','ID_ND','ID'], axis=1)\n#df_clean = df_clean.drop(columns=['type'], axis=1)","5190f10b":"df_clean.head(3)","efc14498":"df.shape","0717d1de":"df_clean.shape","5f6e13d8":"df_clean.groupby(['isFraud']).size()","5f737271":"df_clean_Fraud = df_clean[df['isFraud']==1]\ndf_clean_Valid = df_clean[df['isFraud']==0]\nprint(\"Before Sample:\\n\",df_clean_Fraud.shape[0], df_clean_Valid.shape[0])\n#df_clean_Valid.head()\n#df_clean_Fraud.shape[0]\ndf_clean_Valid = df_clean_Valid.sample(df_clean_Fraud.shape[0])\nprint(\"After Sample:\\n\",df_clean_Fraud.shape[0],df_clean_Valid.shape[0])","6131805c":"df_new = pd.concat([df_clean_Valid, df_clean_Fraud])\ndf_new.shape[0]","28995f3e":"df_new.head()","e8025c29":"df_final=df_new","9ca50505":"#Now separate the independent varaibles as X and dependent variable i.e. isFraud as y_target\ny_target = df_final['isFraud']\nX=df_final.drop('isFraud', axis=1)\nX.shape,y_target.shape[0]\n","104936d0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y_target, test_size = 0.3, random_state = 26, stratify=y_target)","48376e2a":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    GaussianNB(),\n    LogisticRegression(),\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier()]\n\n\nfor clf in classifiers:\n    \n    clf.fit(X_train, y_train)\n    \n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    \n    y_pred = clf.predict(X_train)\n    \n    print(\"Accuracy:     \", round(metrics.accuracy_score(y_train, y_pred),4)*100)\n    print(\"Precision:    \", round(metrics.precision_score(y_train, y_pred),4)*100)\n    print(\"Recall:       \", round(metrics.recall_score(y_train, y_pred),4)*100)","d9fa90ba":"\nfor clf in classifiers:\n    \n    clf.fit(X_test, y_test)\n    \n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    \n    y_pred = clf.predict(X_test)\n    \n    print(\"Accuracy:     \", round(metrics.accuracy_score(y_test, y_pred),4)*100)\n    print(\"Precision:    \", round(metrics.precision_score(y_test, y_pred),4)*100)\n    print(\"Recall:       \", round(metrics.recall_score(y_test, y_pred),4)*100)","cc70ae2e":"If you see in the correlation values in heatmap, we observe that these features are highly correlated:\n            newBalanceDest and oldBalanceDest\n            newBalanceOrig and oldBalanceOrig\nHence we can remove one of the two features for each one. Let's remove oldBalanceDest and oldBalanceOrig too in the future."}}