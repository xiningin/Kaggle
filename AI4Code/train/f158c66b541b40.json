{"cell_type":{"b7bdaf2d":"code","7b29548c":"code","bf92be67":"code","2e5f80f0":"code","a73a330d":"code","c26c3ee8":"code","bab53434":"code","8fd69424":"code","a90f89a3":"code","2261b3a3":"code","c805ffc0":"code","eb8cfb72":"code","e9c92841":"markdown","61455e86":"markdown","d4b098c8":"markdown","b094109b":"markdown","744d5180":"markdown","eb1de8a4":"markdown","1473a50b":"markdown","e4ec7a60":"markdown"},"source":{"b7bdaf2d":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L","7b29548c":"class BatchNormalization(L.Layer):\n    def __init__(self, eps=1e-5, momentum=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.momentum = momentum\n    \n    def build(self, input_shape):\n        self.exp_mean = self.add_weight(shape=input_shape[-1], initializer='zeros', trainable=False)\n        self.exp_var = self.add_weight(shape=input_shape[-1], initializer='ones', trainable=False)\n        \n        self.scale = self.add_weight(shape=input_shape[-1], initializer='zeros', trainable=True)\n        self.shift = self.add_weight(shape=input_shape[-1], initializer='ones', trainable=True)\n    \n    def call(self, x):\n        x_shape = x.shape\n        batch_size = x_shape[0]\n        channels = x.shape[-1] \n        \n        x = tf.reshape(x, (batch_size, -1, channels))\n        \n        mean = tf.reduce_mean(x, [0, 1])\n        mean_x2 = tf.reduce_mean((x ** 2), [0, 1])\n        var = mean_x2 - mean ** 2\n        \n        self.exp_mean = (1 - self.momentum) * self.exp_mean + self.momentum * mean\n        self.exp_var = (1 - self.momentum) * self.exp_var + self.momentum * var\n        \n        mean = self.exp_mean\n        var = self.exp_var\n        \n        x_norm = (x - tf.reshape(mean, (1, 1, -1))) \/ tf.reshape(tf.sqrt(var + self.eps), (1, 1, -1))\n        x_norm = tf.reshape(self.scale, (1, 1, -1)) * x_norm +tf.reshape(self.shift, (1, 1, -1))\n        return tf.reshape(x_norm, x_shape)        ","bf92be67":"x = tf.random.normal((32, 24, 24, 3))\nassert BatchNormalization(3)(x).shape == x.shape","2e5f80f0":"class LayerNormalization(L.Layer):\n    def __init__(self, eps=1e-5,  **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps    \n        \n    def build(self, input_shape):\n        self.gain = self.add_weight(shape=input_shape, initializer='zeros', trainable=True)  \n        self.bias = self.add_weight(shape=input_shape, initializer='ones', trainable=True)  \n    \n    def call(self, x):\n        normalized_shape = x.shape[1:]\n        \n        dims = [-(i + 1) for i in range(len(normalized_shape))]\n          \n        mean = tf.reduce_mean(x, dims, keepdims=True)\n        mean_x2 = tf.reduce_mean((x**2), dims, keepdims=True)\n        var = mean_x2 - mean ** 2\n        \n        x_norm = (x - mean) \/ tf.sqrt(var + self.eps)\n        x_norm = self.gain * x_norm + self.bias\n\n        return x_norm","a73a330d":"x = tf.random.normal((32, 24, 24, 3))\nassert LayerNormalization()(x).shape == x.shape","c26c3ee8":"class InstanceNormalization(L.Layer):\n    def __init__(self, eps=1e-5,  **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        \n    def build(self, input_shape):\n        self.scale = self.add_weight(shape=input_shape[-1], initializer='zeros', trainable=True)\n        self.shift = self.add_weight(shape=input_shape[-1], initializer='ones', trainable=True)\n    \n    def call(self, x):\n        x_shape = x.shape\n        batch_size = x_shape[0]\n        channels = x.shape[-1] \n        \n        x = tf.reshape(x, (batch_size, -1, channels))\n        \n        mean = tf.reduce_mean(x, [1], keepdims=True)\n        mean_x2 = tf.reduce_mean((x ** 2), [1], keepdims=True)\n        var = mean_x2 - mean ** 2\n        \n        x_norm = (x - mean) \/ tf.sqrt(var + self.eps)\n        x_norm = tf.reshape(self.scale, (1, 1, -1)) * x_norm +tf.reshape(self.shift, (1, 1, -1))\n        \n        return tf.reshape(x_norm, x_shape) ","bab53434":"x = tf.random.normal((32, 24, 24, 3))\nassert InstanceNormalization(3)(x).shape == x.shape","8fd69424":"class GroupNormalization(L.Layer):\n    def __init__(self, groups, channels, eps=1e-5,  **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.groups = groups\n        self.channels = channels\n        \n        self.scale = self.add_weight(shape=channels, initializer='zeros', trainable=True)  \n        self.shift = self.add_weight(shape=channels, initializer='ones', trainable=True)   \n        \n    def call(self, x):\n        x_shape = x.shape\n        batch_size = x_shape[0]\n        \n        x = tf.reshape(x, (batch_size, -1, self.groups))\n        \n        mean = tf.reduce_mean(x, [1], keepdims=True)\n        mean_x2 = tf.reduce_mean((x ** 2), [1], keepdims=True)\n        var = mean_x2 - mean ** 2\n        \n        x_norm = (x - mean) \/ tf.sqrt(var + self.eps)\n        x_norm = tf.reshape(x_norm, (batch_size, -1, self.channels))\n        x_norm = tf.reshape(self.scale, (1, 1, -1)) * x_norm +tf.reshape(self.shift, (1, 1, -1))\n        \n        return tf.reshape(x_norm, x_shape) ","a90f89a3":"x = tf.random.normal((32, 24, 24, 8))\nassert GroupNormalization(2, 8)(x).shape == x.shape","2261b3a3":"def weight_standardization(weight):\n    c_out, c_in, *kernel_shape = weight.shape\n    weight = tf.reshape(weight, (c_out, -1))\n    \n    mean = tf.reduce_mean(weight, [1], keepdims=True)\n    mean_x2 = tf.reduce_mean((weight ** 2), [1], keepdims=True)\n    var = mean_x2 - mean ** 2\n    \n    weight = (weight - mean) \/ (tf.sqrt(var + eps))\n    \n    return tf.reshape(weight, (c_out, c_in, *kernel_shape)) ","c805ffc0":"class ChannelNormalization(L.Layer):\n    def __init__(self, groups, channels, eps=1e-5,  **kwargs):\n        super().__init__(**kwargs)\n        self.eps = eps\n        self.groups = groups\n        self.channels = channels\n        \n        self.scale = self.add_weight(shape=groups, initializer='zeros', trainable=True)  \n        self.shift = self.add_weight(shape=groups, initializer='ones', trainable=True)          \n        \n    def call(self, x):\n        x_shape = x.shape\n        batch_size = x_shape[0]\n        \n        x = tf.reshape(x, (batch_size, -1, self.groups))\n        \n        mean = tf.reduce_mean(x, [1], keepdims=True)\n        mean_x2 = tf.reduce_mean((x ** 2), [1], keepdims=True)\n        var = mean_x2 - mean ** 2\n        \n        x_norm = (x - mean) \/ tf.sqrt(var + self.eps)\n        x_norm = tf.reshape(self.scale, (1, 1, -1)) * x_norm +tf.reshape(self.shift, (1, 1, -1))\n        \n        return tf.reshape(x_norm, x_shape) ","eb8cfb72":"x = tf.random.normal((32, 24, 24, 8))\nassert ChannelNormalization(2, 8)(x).shape == x.shape","e9c92841":"# Group Normalization\nBatch Normalization works well for large enough batch sizes but not well for small batch sizes, because it normalizes over the batch. Training large models with large batch sizes is not possible due to the memory capacity of the devices.\n\nGroup Normalization introduced in the paper [Group Normalization](https:\/\/arxiv.org\/pdf\/1803.08494.pdf), normalizes a set of features together as a group. This is based on the observation that classical features such as SIFT and HOG are group-wise features. The paper proposes dividing feature channels into groups and then separately normalizing all channels within each group.\n\nAll normalization layers can be defined by the following computation.\n<center><h3>$ \\hat{x}_{i} = \\frac{1}{\\sigma_{i}}(x_{i} - \\mu_{i}) $<\/h3><\/center>\nwhere $ \\mu_{i} $ and $ \\sigma_{i} $ are mean and standard deviation\n<center><h3>$ \\mu_{i}  = \\frac{1}{m}\\Sigma_{k \\in S_{i}} x_{k} $<\/h3><\/center>\n<center><h3>$ \\sigma_{i}  = \\sqrt{\\frac{1}{m}\\Sigma_{k \\in S_{i}} (x_{k} - \\mu_{i})^{2} + \\epsilon} $<\/h3><\/center>\n$ S_{i} $ is the set of indexes across which the mean and standard deviation are calculated for index i. m is the size of the set $ S_{i} $ which is the same for all i.\n\nThe definition of $ S_{i} $ is different for Batch normalization, Layer normalization, and Instance normalization.\n\n\n**Batch Normalization**\n<center><h3> $ S_{i} = \\{ k|k_{c} = i_{c} \\} $ <\/h3><\/center>\nThe values that share the same feature channel are normalized together.\n<br><br>\n\n**Layer Normalization**\n<center><h3> $ S_{i} = \\{ k|k_{n} = i_{n} \\} $ <\/h3><\/center>\nThe values from the same sample in the batch are normalized together.\n<br><br>\n\n**Instance Normalization**\n<center><h3> $ S_{i} = \\{ k|k_{n} = i_{n}, k_{c} = i_{c} \\} $ <\/h3><\/center>\nThe values from the same sample and same feature channel are normalized together.\n\n<br><br>\n\n**Group Normalization**\n<center><h3> $ S_{i} = \\{ k|k_{n} = i_{n}, floor(\\frac{k_{c}}{C\/G}) = floor(\\frac{i_{c}}{C\/G}) \\} $ <\/h3><\/center>\nwhere $ G $ is the number of groups and $ C $ is the number of channels.\n\nGroup normalization normalizes values of the same sample and the same group of channels together.","61455e86":"# Batch Normalization\n\nBatch Normalization was first discussed in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https:\/\/arxiv.org\/pdf\/1502.03167.pdf)\n\nThey define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. This adversely affects training speed because the later layers have to adapt to the shifted distribution.\n\nThey proposed that by whitening the inputs to each layer,we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift.\n\nWhitening is linearly transforming inputs to have zero mean, unit variance, and be uncorrelated.\n\n**The paper introduces Batch Normalization as follows:**\n\n1. Normalize each feature independently to have zero mean and unit variance:\n<center><h3>$$ \\hat{x}^{(k)} = \\frac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}} $$<\/h3><\/center>\nwhere $ x = (x^{(1)}...x^{(d)}) $ is the d-dimensional input.\n2. The estimates of mean and variance are from the mini-batch for normalization; instead of calculating the mean and variance across the whole dataset.\n3. Normalizing each feature to zero mean and unit variance could affect what the layer can represent. To overcome this each feature is scaled and shifted by two trained parameters.\n<center><h3>$$ y^{(k)} = \\gamma^{(k)}\\hat{x}^{(k)} + \\beta^{(k)} $$<\/h3><\/center>\nwhere $ y^{(k)} $ is the output of the batch normalization layer.\n4. An exponential moving average of mean and variance is calculated during the training phase and is then used during inference.","d4b098c8":"# Batch-Channel Normalization\n\nThis first performs a batch normalizationThis first performs a batch normalization. Then a channel normalization is performed.\n\nChannel Normalization is similar to Group Normalization but affine transform is done group wise.","b094109b":"# Layer Normalization\nLayer normalization, introduced in the paper [Layer Normalization](https:\/\/arxiv.org\/pdf\/1607.06450.pdf) is a simpler normalization method that is generally used for NLP tasks but works on a wider range of settings.\n\n<center><h3>$ LN(x) = \\gamma . \\frac{X - E_{H, W, C}[X]}{\\sqrt{Var_{H, W, C}[x] + \\epsilon}} + \\beta $<\/h3><\/center>","744d5180":"# Normalization in Neural Networks\n\nIn this notebook I'll discuss the different types of Normalizations that are commonly used in Neural Networks, along with their applications and implementations","eb1de8a4":"# Instance Normalization\nInstance normalization was introduced in the paper [Instance Normalization: The Missing Ingredient for Fast Stylization](https:\/\/arxiv.org\/pdf\/1607.08022.pdf) to improve style transfer.\n\n<center><h3>$ IN(x) = \\gamma . \\frac{X - E_{H, W}[X]}{\\sqrt{Var_{H, W}[x] + \\epsilon}} + \\beta $<\/h3><\/center>","1473a50b":"# References\n1. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https:\/\/arxiv.org\/pdf\/1502.03167.pdf)\n2. [Layer Normalization](https:\/\/arxiv.org\/pdf\/1607.06450.pdf)\n3. [Instance Normalization: The Missing Ingredient for Fast Stylization](https:\/\/arxiv.org\/pdf\/1607.08022.pdf)\n4. [Group Normalization](https:\/\/arxiv.org\/pdf\/1803.08494.pdf)\n5. [Micro-Batch Training with Batch-Channel Normalization and Weight Standardization](https:\/\/arxiv.org\/pdf\/1903.10520.pdf)","e4ec7a60":"# Weight Standardization\nBatch Normalization doesn't work well when the batch size is too small, which happens when training large networks because of device memory limitations. The paper [Micro-Batch Training with Batch-Channel Normalization and Weight Standardization](https:\/\/arxiv.org\/pdf\/1903.10520.pdf) introduces Weight Standardization with Batch-Channel Normalization as a better alternative.\n\n<center><h3> $ \\hat{W_{i, j}} = \\frac{W_{i, j} - \\mu w_{i, .}}{ \\sigma w_{i, .} } $ <\/h3><\/center>\nwhere\n<center><h3> $ W \\in  R^{O \\times I} $ <\/h3><\/center>\n<center><h3> $ \\mu w_{i, .} = \\frac{1}{I} \\Sigma_{j=1}^{I} W_{i, j} $ <\/h3><\/center>\n<center><h3> $  \\sigma w_{i, .} = \\sqrt{\\frac{1}{I} \\Sigma_{j=1}^{I} W_{i, j}^{2} - \\mu w_{i, .}^{2} + \\epsilon} $ <\/h3><\/center>\nfor a 2D-convolution layer $ O $ is the number of output channels and $ I $ is the number of input channels times the kernel size $ (I = C_{in} \\times K_{H} \\times K_{W}) $"}}