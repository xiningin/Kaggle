{"cell_type":{"1ba31e8d":"code","ed8c54ba":"code","5986c786":"code","d68b91b7":"code","4571f301":"code","6a710d41":"code","137287da":"code","9f139f07":"code","f77df508":"code","1912d5af":"code","7097059d":"code","825c3eee":"code","b71767f5":"code","f2027e37":"code","14b1861e":"code","23bff62d":"code","a0b40e19":"code","feec0278":"code","7d236e93":"code","f14b1249":"code","70e36734":"code","bc0ef472":"code","75b9970b":"code","acc9ec27":"code","2b32cd71":"code","a83d4952":"code","052135f3":"code","c95d6041":"code","f9ec8a86":"code","66317e5d":"code","839aaef3":"code","ff8f9b77":"code","e81c87ad":"code","cd8eb2b6":"code","8f05e86b":"code","0e1b07ef":"code","bc9b71d9":"code","a4fef6de":"code","765d96e3":"code","02540d56":"code","38a66c64":"code","2539029f":"code","0640ec32":"code","cebd31c2":"code","09faad51":"code","51096a17":"code","0bd0af0d":"code","7ad52ff5":"code","8a51d05f":"code","1bd5baa6":"code","0731d722":"code","87c9f1f3":"code","50207e8f":"code","353d94ec":"code","aaa82e4d":"code","d7f32886":"code","b21889af":"code","485f7163":"code","5c4206de":"code","41a49762":"code","e90e9b07":"code","8577a1da":"code","f5abb774":"code","b706f340":"code","c6e1c94c":"code","2e90f7fe":"code","5e60a290":"code","3c96705c":"code","7028d9ca":"code","976e9e70":"code","8f1ba8d7":"code","7b8eafbb":"code","ec1aff72":"code","a46e9331":"code","b2e496eb":"code","60675781":"code","bb9fad3b":"code","37fa060b":"code","986920d5":"code","34425ef4":"markdown","0bc527b2":"markdown","e7b99d5b":"markdown","bccdd6c3":"markdown"},"source":{"1ba31e8d":"import os\nimport json\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pydicom\nfrom keras import layers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\nfrom keras.initializers import Constant\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom tensorflow.python.ops import array_ops\nfrom tqdm import tqdm\nfrom keras import backend as K\nimport tensorflow as tf\nimport keras\nfrom keras.applications import Xception\nfrom keras.models import Model, load_model\nfrom math import ceil, floor\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.metrics import log_loss\nfrom keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D","ed8c54ba":"os.listdir('\/kaggle\/input\/rsna-intracranial-hemorrhage-detection\/rsna-intracranial-hemorrhage-detection')","5986c786":"BASE_PATH = '\/kaggle\/input\/rsna-intracranial-hemorrhage-detection\/rsna-intracranial-hemorrhage-detection\/'\nTRAIN_DIR = 'stage_2_train\/'\nTEST_DIR = 'stage_2_test\/'\ntrain_df = pd.read_csv(BASE_PATH + 'stage_2_train.csv')","d68b91b7":"sub_df = pd.read_csv(BASE_PATH + 'stage_2_sample_submission.csv')\n\ntrain_df['filename'] = train_df['ID'].apply(lambda st: \"ID_\" + st.split('_')[1] + \".png\")\ntrain_df['type'] = train_df['ID'].apply(lambda st: st.split('_')[2])\nsub_df['filename'] = sub_df['ID'].apply(lambda st: \"ID_\" + st.split('_')[1] + \".png\")\nsub_df['type'] = sub_df['ID'].apply(lambda st: st.split('_')[2])\n\nprint(train_df.shape)\ntrain_df.head()","4571f301":"test_df = pd.DataFrame(sub_df.filename.unique(), columns=['filename'])\nprint(test_df.shape)\ntest_df.head()","6a710d41":"png_test_df = pd.DataFrame(sub_df.filename.unique(), columns=['filename'])\nprint(png_test_df.shape)\npng_test_df.head()\n","137287da":"dcm_df = test_df\ndcm_df['filename'] = dcm_df['filename'].apply(lambda x: x.replace('.png', '.dcm'))\ndcm_df","9f139f07":"png_test_df","f77df508":"subtypes = train_df.groupby('type').sum()\nsubtypes","1912d5af":"sns.barplot(y=subtypes.index, x=subtypes.Label, palette=\"deep\")","7097059d":"np.random.seed(2019)\nsample_files = np.random.choice(os.listdir(BASE_PATH + TRAIN_DIR), 400000) # take the rest for testing\nsample_df = train_df[train_df.filename.apply(lambda x: x.replace('.png', '.dcm')).isin(sample_files)]","825c3eee":"pivot_df = sample_df[['Label', 'filename', 'type']].drop_duplicates().pivot(\n    index='filename', columns='type', values='Label').reset_index()\nprint(pivot_df.shape)\npivot_df","b71767f5":"#one_df = pivot_df.drop(pivot_df.loc[pivot_df['subdural']==0].index)\n#one_df","f2027e37":"#zero_df = pivot_df.drop(pivot_df.loc[pivot_df['any']==1].index)\n#zero_df","14b1861e":"#zero_df = zero_df.sample(47166)\n#zero_df","23bff62d":"#sample_df = pd.concat([zero_df, one_df])\n#sample_df","a0b40e19":"#zero_df = pivot_df.drop(pivot_df.loc[pivot_df['any']==1].index)\n#zero_df","feec0278":"#zero_df = zero_df.sample(47166)","7d236e93":"#sample_df = pd.concat([zero_df, one_df])\n#sample_df","f14b1249":"#from sklearn.utils import shuffle\n#sample_df = shuffle(sample_df)","70e36734":"validation_df = pivot_df.sample(int(len(pivot_df) * 0.15))  \nvalidation_df ","bc0ef472":"y_true = []\nfor i in range(len(validation_df)): \n    y_true.append(validation_df.iloc[i,1])\n        \n","75b9970b":"len(y_true)","acc9ec27":"full_true = []\nfor i in range(len(validation_df)): \n    for j in range(1,7): \n        full_true.append(validation_df.iloc[i,j])\n        \n","2b32cd71":"#len(full_true)","a83d4952":"training_df = pivot_df[~(pivot_df.filename.isin(validation_df.filename))]\ntraining_df\n","052135f3":"print(training_df.head())\nprint(validation_df.head())\n","c95d6041":"def get_pixels_hu(scan): \n    image = np.stack([scan.pixel_array])\n    image = image.astype(np.int16) \n    \n    image[image == -2000] = 0\n    \n    intercept = scan.RescaleIntercept\n    slope = scan.RescaleSlope\n    \n    if slope != 1: \n        image = slope * image.astype(np.float64)\n        image = image.astype(np.int16)\n    \n    image += np.int16(intercept) \n    \n    return np.array(image, dtype=np.int16)","f9ec8a86":"def apply_window(image, center, width):\n    image = image.copy()\n    min_value = center - width \/\/ 2\n    max_value = center + width \/\/ 2\n    image[image < min_value] = min_value\n    image[image > max_value] = max_value\n    return image\n\n\ndef apply_window_policy(image):\n\n    image1 = apply_window(image, 40, 80) # brain\n    image2 = apply_window(image, 80, 200) # subdural\n    image3 = apply_window(image, 40, 380) # bone\n    image1 = (image1 - 0) \/ 80\n    image2 = (image2 - (-20)) \/ 200\n    image3 = (image3 - (-150)) \/ 380\n    image = np.array([\n        image1 - image1.mean(),\n        image2 - image2.mean(),\n        image3 - image3.mean(),\n    ]).transpose(1,2,0)\n\n    return image\n#maybe try a new function ","66317e5d":"def save_and_resize(filenames, load_dir):    \n    save_dir = '\/kaggle\/tmp\/'\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    for filename in tqdm(filenames):\n        try:\n            path = load_dir + filename\n            new_path = save_dir + filename.replace('.dcm', '.png')\n            dcm = pydicom.dcmread(path)\n            image = get_pixels_hu(dcm)\n            image = apply_window_policy(image[0])\n            image -= image.min((0,1))\n            image = (255*image).astype(np.uint8)\n            image = cv2.resize(image, (299, 299)) #smaller\n            res = cv2.imwrite(new_path, image)\n            \n        except ValueError:\n            continue # it returns a black image, super weird ","839aaef3":"save_and_resize(filenames=sample_files, load_dir=BASE_PATH + TRAIN_DIR)\nsave_and_resize(filenames=dcm_df.filename, load_dir=BASE_PATH + TEST_DIR)","ff8f9b77":"def create_model():    \n    base_model = Xception(weights = 'imagenet', include_top = False, input_shape = (299,299,3))\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.15)(x)\n    y_pred = Dense(6, activation = 'sigmoid')(x)\n\n    return Model(inputs = base_model.input, outputs = y_pred)","e81c87ad":"LR = 0.00005\nmodel = create_model()","cd8eb2b6":"#from keras.utils.vis_utils import plot_model\n#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","8f05e86b":"model.compile(optimizer = Adam(learning_rate = LR), \n              loss = 'binary_crossentropy', # <- requires balance\/ Binary for unbalanced\n              metrics = [tf.keras.metrics.SensitivityAtSpecificity(0.5)]) #run both ","0e1b07ef":"from keras_preprocessing.image import ImageDataGenerator","bc9b71d9":"test_df","a4fef6de":"BATCH_SIZE = 16 # had to revert back to 16 to have a comparaison point with the large model I ran locally \n\ndef create_datagen():\n    return ImageDataGenerator()\n\ndef create_test_gen():\n    return ImageDataGenerator().flow_from_dataframe(\n        png_test_df,\n        directory=  '\/kaggle\/tmp\/',\n        x_col='filename',\n        class_mode=None,\n        target_size=(299, 299),\n        batch_size=BATCH_SIZE,\n        shuffle=False\n    )\n\ndef create_train_gen(datagen):\n    return datagen.flow_from_dataframe(\n        training_df, \n        directory='\/kaggle\/tmp\/',\n        \n        x_col='filename', \n        y_col=['any', 'epidural', 'intraparenchymal', \n               'intraventricular', 'subarachnoid', 'subdural'],\n        class_mode='raw',\n        target_size=(299, 299),\n        batch_size=BATCH_SIZE,\n        \n       \n    )\ndef create_val_gen(datagen): \n    return datagen.flow_from_dataframe(\n        validation_df, \n        directory='\/kaggle\/tmp\/',\n        \n        x_col='filename', \n        y_col=['any', 'epidural', 'intraparenchymal', \n               'intraventricular', 'subarachnoid', 'subdural'],\n        class_mode='raw',\n        target_size=(299, 299),\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        \n    )\n\n# Using original generator\ndata_generator = create_datagen()\ntrain_gen = create_train_gen(data_generator)\nval_gen = create_val_gen(data_generator)\ntest_gen = create_test_gen()","765d96e3":"model.summary()","02540d56":"checkpoint = ModelCheckpoint(\n    'effnetb4.h5', \n    monitor='val_loss', \n    verbose=0, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)\nEarly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=1, \n                                              mode='auto', baseline=None, restore_best_weights=False)\n#train_length = len(train_df)\ntotal_steps = sample_files.shape[0] \/\/ BATCH_SIZE\ntotal_steps = total_steps \/\/ 4\nhistory = model.fit_generator(\n    train_gen,\n    steps_per_epoch = total_steps,\n    validation_data=val_gen,\n    validation_steps=total_steps * 0.15,\n    callbacks=[checkpoint, Early_stop],\n    epochs=10\n)","38a66c64":"acc = history.history['sensitivity_at_specificity']\nval_acc = history.history['val_sensitivity_at_specificity']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'b', label='Training Sens')\nplt.plot(epochs, val_acc, 'g', label='Validation Sens')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\n\nplt.title('Training and validation accuracy')\nplt.legend()\nfig = plt.figure()\nfig.savefig('acc.png')\n\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'g', label='Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and validation loss')\n\nplt.legend()\nplt.show()","2539029f":"test_preds = model.predict_generator(test_gen, verbose = 1)","0640ec32":"test_preds","cebd31c2":"test_df = test_df.join(pd.DataFrame(test_preds, columns=[\n    'any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural'\n]))\n","09faad51":"test_df = test_df.melt(id_vars=['filename'])\n\n# Combine the filename column with the variable column\ntest_df['ID'] = test_df.filename.apply(lambda x: x.replace('.png', '')) + '_' + test_df.variable\ntest_df['Label'] = test_df['value']","51096a17":"test_df[['ID', 'Label']].to_csv('submission.csv', index=False)","0bd0af0d":"val_preds = model.predict_generator(val_gen, verbose = 1)\n","7ad52ff5":"val_preds","8a51d05f":"#y_preds = []\n#for i in range(len(val_preds)):\n#    y_preds.append(0)\n#    for value in val_preds[i]: \n#        if value > 0.5: \n#            y_preds[i] = 1\n#            break\n            \n        \n#len(y_preds)\n","1bd5baa6":"#from sklearn.metrics import roc_curve\n#fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_true, y_preds)","0731d722":"#from sklearn.metrics import auc\n#auc_keras = auc(fpr_keras, tpr_keras)","87c9f1f3":"#plt.figure(1)\n#plt.plot([0, 1], [0, 1], 'k--')\n#plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n\n#plt.xlabel('False positive rate')\n#plt.ylabel('True positive rate')\n#plt.title('ROC curve')\n#plt.legend(loc='best')\n#plt.show()","50207e8f":"#from sklearn.metrics import confusion_matrix\n#print('2*2 Confusion Matrix')\n#print(confusion_matrix(y_true, y_preds))\n#cm = confusion_matrix(y_true, y_preds)","353d94ec":"import itertools   \ndef plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","aaa82e4d":"cm_labels = ['no hemorrhage', 'has hemorrhage']","d7f32886":"#plot_confusion_matrix(cm=cm, classes=cm_labels, title='Confusion Matrix')","b21889af":"#predictions_list = []\n#for pred in val_preds: \n  #  predictions_list.append(pred)\n\nlen(predictions_list)","485f7163":"validation_frame = validation_df.drop(['filename'], axis=1)\nvalidation_frame ","5c4206de":"len(validation_frame) ","41a49762":"if len(predictions_list) == len(validation_frame): \n    validation_frame.iloc[:,:] = predictions_list\nelse: \n    print(\"fix this issue\")\n        ","e90e9b07":"validation_frame.insert(0, \"filename\", validation_df.filename)\nvalidation_frame.insert(7, \"true_any\" ,validation_df.iloc[:,1])\nvalidation_frame.insert(8, \"true_epidural\", validation_df.epidural)\nvalidation_frame.insert(9, \"true_intraparenchymal\", validation_df.intraparenchymal)\nvalidation_frame.insert(10, \"true_intraventricular\", validation_df.intraventricular)\nvalidation_frame.insert(11, \"true_subarachnoid\", validation_df.subarachnoid)\nvalidation_frame.insert(12, \"true_subdural\", validation_df.subdural)","8577a1da":"validation_frame","f5abb774":"for i in range(100): \n    if validation_frame.iloc[i,1] > 0.8: \n        print(\"ID is : \" + str(validation_frame.iloc[i,0]))\n        for j in range(1,7): \n            print(\"predicition = \" +  str(validation_frame.iloc[i,j]) )\n        for k in range(7,13): \n            print(\"true predicition = \" +  str(validation_frame.iloc[i,k]))\n# activation map","b706f340":"any_preds = validation_frame['any']\nmax_index = any_preds.idxmax()\nmax_index","c6e1c94c":"def img_to_heatmap(): \n    highest_predicted_img = validation_frame.loc[max_index,'filename']\n    if validation_frame.loc[max_index, 'true_any'] == 1:\n        return highest_predicted_img","2e90f7fe":"highest_predicted_img =  img_to_heatmap()\nhighest_predicted_img","5e60a290":"test_df","3c96705c":"test_df","7028d9ca":"predictions_list_test = []\nfor pred in test_preds: \n    predictions_list_test.append(pred)\n\n","976e9e70":"test_frame =  test_sample_df.drop(['filename'], axis=1)\ntest_frame","8f1ba8d7":"test_frame.iloc[:,:] = predictions_list_test\ntest_frame","7b8eafbb":"test_sample_df = test_sample_df.stack().reset_index()\ntest_sample_df","ec1aff72":"test_frame.insert(0, \"filename\", test_df.filename)\ntest_frame","a46e9331":"from PIL import Image\nfor i in range(20): \n  \n    for j in range(1,7): \n        if test_frame.iloc[i,j] > 0.8: \n            path = '\/kaggle\/tmp\/' + str(test_frame.iloc[i,0])\n            img = Image.open(path)\n            plt.imshow(img)\n            print(str(test_frame.iloc[i,0]) + \" has a probability: \"  + str(test_frame.iloc[i,j]) + \" for a '\" + str(test_frame.columns[j]) + \"' type of hemorrhage\")\n            plt.show()","b2e496eb":"#heatmap \n#The code used to show the heatmake was taken from: https:\/\/keras.io\/examples\/vision\/grad_cam\/\n#Only slightly modified to fit this workflow and return the image with the highest predicition from the validtion set \nfrom IPython.display import Image, display\n\npreprocess_input = keras.applications.xception.preprocess_input\ndecode_predictions = keras.applications.xception.decode_predictions\n\nlast_conv_layer_name = \"block14_sepconv2_act\"\n\nimg_path = '\/kaggle\/tmp\/' + str(highest_predicted_img)\ndisplay(Image(img_path))","60675781":"def get_img_array(img_path, size):\n    # `img` is a PIL image of size 299x299\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (299, 299, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()","bb9fad3b":"img_size = (299, 299)\n\nimg_array = preprocess_input(get_img_array(img_path, size=img_size))\n\nmodel.layers[-1].activation = None\n\nheatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n\n# Display heatmap\nplt.matshow(heatmap)\nplt.show()\n","37fa060b":"import matplotlib.cm as cm\ndef save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = keras.preprocessing.image.load_img(img_path)\n    img = keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n    display(Image(cam_path))\n","986920d5":"save_and_display_gradcam(img_path, heatmap)","34425ef4":"**Testing\/prediction**\n* Fill the sample documents provided in the dataset + test on validation data\n* Try to make a scrpit that takes an image, and return the probability for each label \n* Makes a labeled validation set? \n* Other ways to validate ","0bc527b2":"Idea here: make pred on validation, then for each image load the image, the prediciton, and the labels in validation_df ","e7b99d5b":"*** Things to look at for optmization: **\n* Different file sizes\n* Different Batch sizes and more\/less epoch \n* using different learning rate or metrics (AUC) \n* Sensitivy and specificty \n* Only take 'any\" or most frequent -> make tables \n* Get balanced data \n* Batch normalization \n* adaptive loss function\n* Data augmentation? GANs?","bccdd6c3":"**From this we can note a few things: **\n* There are 107933 images with \"any\" hemmorhages. This is quite low compared to the 720000 images we have in the dataset \n* Thus, we could create a generator with all the images containing hemorrages and the same amount of images not subject to an hemmorhage. \n* Additionaly, all types of hemmorhages are realtively equaly represented, except the 'epidural' type that has only 3145 cases in the whole dataset. We could try to run in with this type discarded.  "}}