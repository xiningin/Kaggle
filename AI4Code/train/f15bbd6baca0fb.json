{"cell_type":{"2972f906":"code","faa8c29d":"code","9954ce66":"code","ced2d469":"code","058437a7":"code","9c007fdf":"code","d969d3fb":"code","0b2885a5":"code","0c22eb09":"code","f78ee333":"code","ef27365f":"code","4ec54ca0":"code","a7fdacbb":"code","ba6350f0":"code","a2b68692":"code","5203119b":"code","6734e38e":"code","96be6f99":"code","c632a34d":"code","adbf6c6b":"code","6c1cdc4e":"code","6f101573":"code","5d14aeb7":"code","b3e127fc":"code","56ac398d":"code","10845403":"code","e664a9ce":"code","554a71c9":"code","83bb0444":"code","ef48b8a2":"code","637c5755":"code","3b158fae":"code","06baf8a8":"code","78ada430":"code","af2041e8":"code","6a3ebd5b":"code","eaba3bd9":"code","a90aa4cd":"code","184144c3":"code","1b17a6d6":"code","ce989e66":"code","ed11e546":"code","7920a806":"code","aea28a18":"code","51e72f47":"code","27640c59":"code","5b59e42e":"code","71c8decb":"code","b355298a":"code","16b024b5":"code","73323cfa":"code","f09a7cda":"code","911e6bfc":"code","af38b5ce":"code","1c56409f":"code","04f5bf2c":"code","032dd9f7":"code","2ef31631":"code","27595b05":"code","be6ea731":"code","4c96aeea":"code","cf42aee9":"code","85c30cc1":"code","98248473":"code","2e1cc8ca":"code","a31beaed":"code","bee4749a":"code","101eb0df":"code","4c70cc1b":"code","46c8f80e":"code","101d9689":"code","e227f1c1":"code","952d9a6b":"code","9f33793b":"code","530ecd12":"code","7feeb23e":"code","7ea2b51a":"code","c6c9d909":"code","518c7155":"code","2d66bcf7":"code","b4b49691":"code","af271136":"markdown","19352be7":"markdown","5f939541":"markdown","00342745":"markdown","d6d3f0d9":"markdown","8ddbd618":"markdown","49da291f":"markdown","0a3ad2f8":"markdown","8972f277":"markdown","133db0b8":"markdown","caa3c10b":"markdown","2b455e51":"markdown","66e25322":"markdown","261314a5":"markdown","217fcf2a":"markdown","25c0d465":"markdown","332fb4d7":"markdown","dc2fdd01":"markdown","7eb0aa08":"markdown","27d24e55":"markdown","97b9f996":"markdown","2974d7f3":"markdown","f0b371da":"markdown","6cfb5ca3":"markdown","d8429f29":"markdown","64309033":"markdown","49d83e78":"markdown","3ae88741":"markdown","78e2ec88":"markdown","16621739":"markdown","c37bc80e":"markdown","e66e209f":"markdown","ef27af95":"markdown","44a12f8f":"markdown","add3d059":"markdown","8a074dec":"markdown","ae48fd9d":"markdown","613d9ab7":"markdown","3dc92237":"markdown","c6d3c8ea":"markdown","216d2fb8":"markdown","dfa33f3e":"markdown","648695da":"markdown","526e1533":"markdown","c84f919c":"markdown","289dda17":"markdown","a0129555":"markdown","3ce27fc2":"markdown","1f14e2c1":"markdown","54495a81":"markdown","932731a7":"markdown","829ba68f":"markdown","8679e6d1":"markdown","44e2ef1a":"markdown","d61ae8a7":"markdown","65aa7f43":"markdown","5d4b0460":"markdown","8e06950f":"markdown","b60006ba":"markdown","80f93158":"markdown","a4029f25":"markdown","276c520c":"markdown","1f56139a":"markdown","0ee83800":"markdown","b88e8478":"markdown","3484643c":"markdown","9090605a":"markdown","48af7b60":"markdown","af4287be":"markdown","010c5a81":"markdown","32201dd2":"markdown","8b877b95":"markdown","8393ebd5":"markdown","761ae0a6":"markdown","ead0ec8a":"markdown","d2600437":"markdown","482848bf":"markdown","b9569b08":"markdown","04d9d873":"markdown","575dbc2a":"markdown","4ab6ec73":"markdown","956fb1d6":"markdown","0393478a":"markdown","7d744a65":"markdown","b1f27db7":"markdown","19cc1d82":"markdown","da939db2":"markdown","761708ab":"markdown","9a18f60b":"markdown","717e5aa5":"markdown","665b4889":"markdown","0e9a216e":"markdown","91beb4c1":"markdown","d405c15f":"markdown"},"source":{"2972f906":"from IPython.display import Image\nImage(\"..\/input\/svmimages\/images\/svm35.png\") ","faa8c29d":"Image(\"..\/input\/svmimages\/images\/svm3.png\")","9954ce66":"Image(\"..\/input\/svmimages\/images\/svm5.png\")","ced2d469":"Image(\"..\/input\/svmimages\/images\/svm4.png\")","058437a7":"Image(\"..\/input\/svmimages\/images\/svm7.png\")","9c007fdf":"Image(\"..\/input\/svmimages\/images\/svm8.png\")","d969d3fb":"Image(\"..\/input\/svmimages\/images\/svm9.png\")","0b2885a5":"Image(\"..\/input\/svmimages\/images\/svm10.png\")","0c22eb09":"#### for the point X4:\n\n","f78ee333":"Image(\"..\/input\/svmimages\/images\/svm11.png\")","ef27365f":"Image(\"..\/input\/svmimages\/images\/svm12.png\")","4ec54ca0":"Image(\"..\/input\/svmimages\/images\/svm13.png\")","a7fdacbb":"Image(\"..\/input\/svmimages\/images\/svm14.png\")","ba6350f0":"Image(\"..\/input\/svmimages\/images\/SVM15.PNG\")","a2b68692":"Image(\"..\/input\/svmimages\/images\/svm16.png\")","5203119b":"Image(\"..\/input\/svmimages\/images\/svm17.png\")\n\n","6734e38e":"Image(\"..\/input\/svmimages\/images\/svm18.png\")","96be6f99":"Image(\"..\/input\/svmimages\/images\/svm20.png\")","c632a34d":"Image(\"..\/input\/svmimages\/images\/svm21.png\")","adbf6c6b":"Image(\"..\/input\/svmimages\/images\/svm22.png\")","6c1cdc4e":"Image(\"..\/input\/svmimages\/images\/svm23.png\")","6f101573":"Image(\"..\/input\/svmimages\/images\/svm24.png\")","5d14aeb7":"Image(\"..\/input\/svmimages\/images\/svm25.png\")","b3e127fc":"Image(\"..\/input\/svmimages\/images\/svm26.png\")","56ac398d":"Image(\"..\/input\/svmimages\/images\/svm27.png\")","10845403":"Image(\"..\/input\/svmimages\/images\/svm28.png\")","e664a9ce":"Image(\"..\/input\/svmimages\/images\/svm36.png\")","554a71c9":"Image(\"..\/input\/svmimages\/images\/svm30.png\")","83bb0444":"Image(\"..\/input\/svmimages\/images\/svm31.png\")","ef48b8a2":"### Image 1\nImage(\"..\/input\/svmimages\/images\/svm32.png\")","637c5755":"#Image 2\nImage(\"..\/input\/svmimages\/images\/svm33.png\")","3b158fae":"Image(\"..\/input\/svmimages\/images\/svm34.png\")","06baf8a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns # for statistical data visualization\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","78ada430":"data = '..\/input\/pulsar-stars\/pulsar_stars.csv'\n\ndf = pd.read_csv(data)","af2041e8":"df.shape","6a3ebd5b":"# let's preview the dataset\n\ndf.head()","eaba3bd9":"# Now, I will view the column names to check for leading and trailing spaces.\n\n# view the column names of the dataframe\n\ncol_names = df.columns\n\ncol_names","a90aa4cd":"# remove leading spaces from column names\n\ndf.columns = df.columns.str.strip()","184144c3":"# view column names again\n\ndf.columns\n","1b17a6d6":"# rename column names because column name is very long\n\ndf.columns = ['IP Mean', 'IP Sd', 'IP Kurtosis', 'IP Skewness', \n              'DM-SNR Mean', 'DM-SNR Sd', 'DM-SNR Kurtosis', 'DM-SNR Skewness', 'target_class']","ce989e66":"# view the renamed column names\n\ndf.columns","ed11e546":"# check distribution of target_class column\n\ndf['target_class'].value_counts()","7920a806":"# view the percentage distribution of target_class column\n\ndf['target_class'].value_counts()\/np.float(len(df))","aea28a18":"# view summary of dataset\n\ndf.info()","51e72f47":"# check for missing values in variables\n\ndf.isnull().sum()","27640c59":"# Outliers in numerical variables\n\n# view summary statistics in numerical variables\n\nround(df.describe(),2)","5b59e42e":"\nplt.figure(figsize=(24,20))\n\n\nplt.subplot(4, 2, 1)\nfig = df.boxplot(column='IP Mean')\nfig.set_title('')\nfig.set_ylabel('IP Mean')\n\n\nplt.subplot(4, 2, 2)\nfig = df.boxplot(column='IP Sd')\nfig.set_title('')\nfig.set_ylabel('IP Sd')\n\n\nplt.subplot(4, 2, 3)\nfig = df.boxplot(column='IP Kurtosis')\nfig.set_title('')\nfig.set_ylabel('IP Kurtosis')\n\n\nplt.subplot(4, 2, 4)\nfig = df.boxplot(column='IP Skewness')\nfig.set_title('')\nfig.set_ylabel('IP Skewness')\n\n\nplt.subplot(4, 2, 5)\nfig = df.boxplot(column='DM-SNR Mean')\nfig.set_title('')\nfig.set_ylabel('DM-SNR Mean')\n\n\nplt.subplot(4, 2, 6)\nfig = df.boxplot(column='DM-SNR Sd')\nfig.set_title('')\nfig.set_ylabel('DM-SNR Sd')\n\n\nplt.subplot(4, 2, 7)\nfig = df.boxplot(column='DM-SNR Kurtosis')\nfig.set_title('')\nfig.set_ylabel('DM-SNR Kurtosis')\n\n\nplt.subplot(4, 2, 8)\nfig = df.boxplot(column='DM-SNR Skewness')\nfig.set_title('')\nfig.set_ylabel('DM-SNR Skewness')","71c8decb":"# plot histogram to check distribution\n\n\nplt.figure(figsize=(24,20))\n\n\nplt.subplot(4, 2, 1)\nfig = df['IP Mean'].hist(bins=20)\nfig.set_xlabel('IP Mean')\nfig.set_ylabel('Number of pulsar stars')\n\n\nplt.subplot(4, 2, 2)\nfig = df['IP Sd'].hist(bins=20)\nfig.set_xlabel('IP Sd')\nfig.set_ylabel('Number of pulsar stars')\n\n\nplt.subplot(4, 2, 3)\nfig = df['IP Kurtosis'].hist(bins=20)\nfig.set_xlabel('IP Kurtosis')\nfig.set_ylabel('Number of pulsar stars')\n\n\n\nplt.subplot(4, 2, 4)\nfig = df['IP Skewness'].hist(bins=20)\nfig.set_xlabel('IP Skewness')\nfig.set_ylabel('Number of pulsar stars')\n\n\nplt.subplot(4, 2, 5)\nfig = df['DM-SNR Mean'].hist(bins=20)\nfig.set_xlabel('DM-SNR Mean')\nfig.set_ylabel('Number of pulsar stars')\n\n\n\nplt.subplot(4, 2, 6)\nfig = df['DM-SNR Sd'].hist(bins=20)\nfig.set_xlabel('DM-SNR Sd')\nfig.set_ylabel('Number of pulsar stars')\n\n\nplt.subplot(4, 2, 7)\nfig = df['DM-SNR Kurtosis'].hist(bins=20)\nfig.set_xlabel('DM-SNR Kurtosis')\nfig.set_ylabel('Number of pulsar stars')\n\n\nplt.subplot(4, 2, 8)\nfig = df['DM-SNR Skewness'].hist(bins=20)\nfig.set_xlabel('DM-SNR Skewness')\nfig.set_ylabel('Number of pulsar stars')","b355298a":"# Declare feature vector and target variable\n\nX = df.drop(['target_class'], axis=1)\n\ny = df['target_class']","16b024b5":"# split X and y into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","73323cfa":"# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","f09a7cda":"# Feature Scaling \n\n\ncols = X_train.columns","911e6bfc":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)","af38b5ce":"X_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns=[cols])","1c56409f":"X_train.describe()","04f5bf2c":"# import SVC classifier\nfrom sklearn.svm import SVC\n\n\n# import metrics to compute accuracy\nfrom sklearn.metrics import accuracy_score\n\n\n# instantiate classifier with default hyperparameters\nsvc=SVC() \n\n\n# fit classifier to training set\nsvc.fit(X_train,y_train)\n\n# make predictions on test set\ny_pred=svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","032dd9f7":"# instantiate classifier with rbf kernel and C=100\nsvc=SVC(C=100.0) \n\n\n# fit classifier to training set\nsvc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","2ef31631":"# instantiate classifier with rbf kernel and C=1000\nsvc=SVC(C=1000.0) \n\n\n# fit classifier to training set\nsvc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=svc.predict(X_test)\n\n# compute and print accuracy score\nprint('Model accuracy score with rbf kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","27595b05":"# instantiate classifier with linear kernel and C=1.0\nlinear_svc=SVC(kernel='linear', C=1.0) \n\n\n# fit classifier to training set\nlinear_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred_test=linear_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with linear kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))","be6ea731":"# Run SVM with linear kernel and C=100.0\n\n# instantiate classifier with linear kernel and C=100.0\nlinear_svc100=SVC(kernel='linear', C=100.0) \n\n\n# fit classifier to training set\nlinear_svc100.fit(X_train, y_train)\n\n\n# make predictions on test set\ny_pred=linear_svc100.predict(X_test)\n\n# compute and print accuracy score\nprint('Model accuracy score with linear kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","4c96aeea":"# Run SVM with linear kernel and C=1000.0\n\n# instantiate classifier with linear kernel and C=1000.0\nlinear_svc1000=SVC(kernel='linear', C=1000.0) \n\n\n# fit classifier to training set\nlinear_svc1000.fit(X_train, y_train)\n\n\n# make predictions on test set\ny_pred=linear_svc1000.predict(X_test)\n\n# compute and print accuracy score\nprint('Model accuracy score with linear kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","cf42aee9":"y_pred_train = linear_svc.predict(X_train)\n\ny_pred_train","85c30cc1":"print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))","98248473":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(linear_svc.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(linear_svc.score(X_test, y_test)))","2e1cc8ca":"# check class distribution in test set\n\ny_test.value_counts()","a31beaed":"# check null accuracy score\n\nnull_accuracy = (3306\/(3306+274))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))","bee4749a":"# instantiate classifier with polynomial kernel and C=1.0\npoly_svc=SVC(kernel='poly', C=1.0) \n\n\n# fit classifier to training set\npoly_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=poly_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","101eb0df":"# instantiate classifier with polynomial kernel and C=100.0\npoly_svc100=SVC(kernel='poly', C=100.0) \n\n\n# fit classifier to training set\npoly_svc100.fit(X_train, y_train)\n\n\n# make predictions on test set\ny_pred=poly_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","4c70cc1b":"# instantiate classifier with sigmoid kernel and C=1.0\nsigmoid_svc=SVC(kernel='sigmoid', C=1.0) \n\n\n# fit classifier to training set\nsigmoid_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=sigmoid_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with sigmoid kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","46c8f80e":"# instantiate classifier with sigmoid kernel and C=100.0\nsigmoid_svc100=SVC(kernel='sigmoid', C=100.0) \n\n\n# fit classifier to training set\nsigmoid_svc100.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=sigmoid_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with sigmoid kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","101d9689":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_test)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","e227f1c1":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","952d9a6b":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_test))","9f33793b":"# Classification accuracy\nTP = cm[0,0]\nTN = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]","530ecd12":"# print classification accuracy\n\nclassification_accuracy = (TP + TN) \/ float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))","7feeb23e":" # print classification error\n\nclassification_error = (FP + FN) \/ float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))","7ea2b51a":" # print precision score\n\nprecision = TP \/ float(TP + FP)\n\n\nprint('Precision : {0:0.4f}'.format(precision))","c6c9d909":"# Recall \n\nrecall = TP \/ float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))","518c7155":"# plot ROC Curve\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_test)\n\nplt.figure(figsize=(6,4))\n\nplt.plot(fpr, tpr, linewidth=2)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\n\nplt.title('ROC curve for Predicting a Pulsar Star classifier')\n\nplt.xlabel('False Positive Rate (1 - Specificity)')\n\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()","2d66bcf7":"# compute ROC AUC\n\nfrom sklearn.metrics import roc_auc_score\n\nROC_AUC = roc_auc_score(y_test, y_pred_test)\n\nprint('ROC AUC : {:.4f}'.format(ROC_AUC))","b4b49691":"# calculate cross-validated ROC AUC \n\nfrom sklearn.model_selection import cross_val_score\n\nCross_validated_ROC_AUC = cross_val_score(linear_svc, X_train, y_train, cv=10, scoring='roc_auc').mean()\n\nprint('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))","af271136":"## TUNING PARAMETERS OF SVM","19352be7":"## References \n\n1. https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine\n\n2. https:\/\/www.datacamp.com\/community\/tutorials\/svm-classification-scikit-learn-python\n\n3. http:\/\/dataaspirant.com\/2017\/01\/13\/support-vector-machine-algorithm\/\n\n4. https:\/\/www.ritchieng.com\/machine-learning-evaluate-classification-model\/\n\n5. https:\/\/en.wikipedia.org\/wiki\/Kernel_method\n\n6. https:\/\/en.wikipedia.org\/wiki\/Polynomial_kernel\n\n7. https:\/\/en.wikipedia.org\/wiki\/Radial_basis_function_kernel\n\n8. https:\/\/data-flair.training\/blogs\/svm-kernel-functions\/","5f939541":"#### for the point X1 :\n\n","00342745":"#### Compare the train-set and test-set accuracy\nNow, I will compare the train-set and test-set accuracy to check for overfitting.","d6d3f0d9":"The training-set accuracy score is 0.9783 while the test-set accuracy to be 0.9830. These two values are quite comparable. So, there is no question of overfitting.","8ddbd618":"#### How should you choose the value of C?\nThere is no rule of thumb to choose a C value, it totally depends on your testing data. The only option I see is trying bunch of different values and choose the value which gives you lowest misclassification rate on testing data. I would suggest you to use gridsearchCV, in which you can directly give a list of different values parameter and it will tell you which value is best.","49da291f":"The confusion matrix shows 3289 + 230 = 3519 correct predictions and 17 + 44 = 61 incorrect predictions.","0a3ad2f8":"#### Explanation: \nWhen Xi = 7 the point is classified incorrectly because for point 7 the wT + b will be smaller than one and this violates the constraints. So we found the misclassification because of constraint violation. Similarly, we can also say for points Xi = 8.","8972f277":"## Split data into separate training and test set ","133db0b8":"###  Soft-Margin SVM","caa3c10b":"## Run SVM with default hyperparameters \n\nDefault hyperparameter means C=1.0, kernel=rbf and gamma=auto among other parameters.","2b455e51":"\n\nPolynomial kernel is very popular in Natural Language Processing. The most common degree is d = 2 (quadratic), since larger degrees tend to overfit on NLP problems. It can be visualized with the following diagram.","66e25322":"## Dual form of SVM:\nNow, let\u2019s consider the case when our data set is not at all linearly separable.\n\n","261314a5":"On closer inspection, we can suspect that all the continuous variables may contain outliers.\n\nI will draw boxplots to visualise outliers in the above variables.","217fcf2a":"The images below (same as image 1 and image 2) are example of two different regularization parameter. Top one has some misclassification due to lower regularization value. Higher value leads to results like bottom one.\n\n","25c0d465":"1. SVM can be used for linearly separable as well as non-linearly separable data. Linearly separable data is the hard margin whereas non-linearly separable data poses a soft margin.\n\n\n2. Feature Mapping used to be quite a load on the computational complexity of the overall training performance of the model. However, with the help of Kernel Trick, SVM can carry out the feature mapping using the simple dot product.\n\n\n3. It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n\n\n4. SVM Classifiers offer good accuracy and perform faster prediction compared to Na\u00efve Bayes algorithm.\n\n\n5. SVMs provide compliance to the semi-supervised learning models. It can be used in areas where the data is labeled as well as unlabeled. It only requires a condition to the minimization problem which is known as the Transductive SVM.","332fb4d7":"We can see that our model accuracy score is 0.9830 but null accuracy score is 0.9235. So, we can conclude that our SVM classifier is doing a very good job in predicting the class labels.","dc2fdd01":"## PROS AND CONS ASSOCIATED WITH SVM","7eb0aa08":"#### for point X7:\n\n","27d24e55":"### Maximal-Margin Classifier\nThe Maximal-Margin Classifier is a hypothetical classifier that best explains how SVM works in practice.\n\nThe numeric input variables (x) in your data (the columns) form an n-dimensional space. For example, if you had two input variables, this would form a two-dimensional space.\n\nA hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions you can visualize this as a line and let\u2019s assume that all of our input points can be completely separated by this line. For example:\n\nB0 + (B1 * X1) + (B2 * X2) = 0\n\n\n\nWhere the coefficients (B1 and B2) that determine the slope of the line and the intercept (B0) are found by the learning algorithm, and X1 and X2 are the two input variables.\n\nYou can make classifications using this line. By plugging in input values into the line equation, you can calculate whether a new point is above or below the line.\n\n1. Above the line, the equation returns a value greater than 0 and the point belongs to the first class (class 0).\n2. Below the line, the equation returns a value less than 0 and the point belongs to the second class (class 1).\n3. A value close to the line returns a value close to zero and the point may be difficult to classify.\n4. If the magnitude of the value is large, the model may have more confidence in the prediction.\n\nThe distance between the line and the closest data points is referred to as the margin. The best or optimal line that can separate the two classes is the line that as the largest margin. This is called the Maximal-Margin hyperplane.\n\nThe margin is calculated as the perpendicular distance from the line to only the closest points. Only these points are relevant in defining the line and in the construction of the classifier. These points are called the support vectors. They support or define the hyperplane.\n\nThe hyperplane is learned from training data using an optimization procedure that maximizes the margin.","97b9f996":"## Run SVM with sigmoid kernel ","2974d7f3":"We can see that the occurences of most frequent class 0 is 3306. So, we can calculate null accuracy by dividing 3306 by total number of occurences.","f0b371da":"## Declare feature vector and target variable","6cfb5ca3":"1. SVM doesn\u2019t give the best performance for handling text structures as compared to other algorithms that are used in handling text data. This leads to loss of sequential information and thereby, leading to worse performance.\n\n\n2. It doesn\u2019t perform well, when we have large data set because the required training time is higher\n\n\n3. It also doesn\u2019t perform very well, when the data set has more noise i.e. target classes are overlapping\n\n\n4. SVM doesn\u2019t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. \n\n\n5. The choice of the kernel is perhaps the biggest limitation of the support vector machine. Considering so many kernels present, it becomes difficult to choose the right one for the data.","d8429f29":"## What is Kernel trick?","64309033":"The above boxplots confirm that there are lot of outliers in these variables.\n\n","49d83e78":"Let\u2019s say there are \u201cm\u201d dimensions:\n\nthus the equation of the hyperplane in the \u2018M\u2019 dimension: \n\nwhere\n\nWi = vectors(W0,W1,W2,W3\u2026\u2026Wm)\n\nb = biased term (W0)\n\nX = variables.","3ae88741":"#### Handle outliers with SVMs\n\nThere are 2 variants of SVMs. They are hard-margin variant of SVM and soft-margin variant of SVM.\n\nThe hard-margin variant of SVM does not deal with outliers. In this case, we want to find the hyperplane with maximum margin such that every training point is correctly classified with margin at least 1. This technique does not handle outliers well.\n\nAnother version of SVM is called soft-margin variant of SVM. In this case, we can have a few points incorrectly classified or classified with a margin less than 1. But for every such point, we have to pay a penalty in the form of C parameter, which controls the outliers. Low C implies we are allowing more outliers and high C implies less outliers.\n\nThe message is that since the dataset contains outliers, so the value of C should be high while training the model.\n\n#### Check the distribution of variables\n\nNow, I will plot the histograms to check distributions to find out if they are normal or skewed.","78e2ec88":"#### Explanation:\nwhen the point X4 we can say that point lies on the hyperplane in the negative region and the equation determines that the product of our actual output and the hyperplane equation is equal to 1 which means the point is correctly classified in the negative domain.","16621739":"We can see that there are no missing values in the dataset.","c37bc80e":"#### Explanation: \nwhen the point X3 we can say that point lies away from the hyperplane and the equation determines that the product of our actual output and the hyperplane equation is greater 1 which means the point is correctly classified in the positive domain.","e66e209f":"###  Regularization Parameter( C )\n\n(A regularization parameter that controls the trade off between the achieving a low training error and a low testing error that is the ability to generalize your classifier to unseen data.)\n\nThe Regularization parameter (often termed as Penalty parameter C) tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly.Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points.","ef27af95":"## Results and conclusion\n\n1. There are outliers in our dataset. So, as I increase the value of C to limit fewer outliers, the accuracy increased. This is true with different kinds of kernels.\n\n2. We get maximum accuracy with rbf and linear kernel with C=100.0 and the accuracy is 0.9832. So, we can conclude that our model is doing a very good job in terms of predicting the class labels. But, this is not true. Here, we have an imbalanced dataset. Accuracy is an inadequate measure for quantifying predictive performance in the imbalanced dataset problem. So, we must explore confusion matrix that provide better guidance in selecting models.\n\n3. ROC AUC of our model is very close to 1. So, we can conclude that our classifier does a good job in classifying the pulsar star.","44a12f8f":"## Data Preparation for SVM\n\nHow to best prepare your training data when learning an SVM model.\n\n1. Numerical Inputs: SVM assumes that your inputs are numeric. If you have categorical inputs you may need to covert them to binary dummy variables (one variable for each category).\n\n\n2. Binary Classification: Basic SVM as described in this post is intended for binary (two-class) classification problems. Although, extensions have been developed for regression and multi-class classification.","add3d059":"We can see that there are 17898 instances and 9 variables in the data set.","8a074dec":"## ROC - AUC  Curve","ae48fd9d":"Thus from the above examples, we can conclude that for any point Xi:\n\nif  Yi(WT*Xi +b) \u2265 1:\n\nthen Xi is correctly classified\n\nelse:\n\nXi is incorrectly classified.\n\nSo we can see that if the points linearly separable then only our hyperplane is able to distinguish between them and if any outlier is introduced then it is not able to separate them. So these type of SVM is called as hard margin SVM (since we have very strict constraints to correctly classify each and every datapoint).","613d9ab7":"#### Compare model accuracy with null accuracy\n\nSo, the model accuracy is 0.9832. But, we cannot say that our model is very good based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.\n\nSo, we should first check the class distribution in the test set.","3dc92237":"We can see that we obtain a higher accuracy with C=100.0 as higher C means less outliers.\n\nNow, I will further increase the value of C=1000.0 and check accuracy.","c6d3c8ea":"Support Vector Regression (SVR) uses the same principle as SVM, but for regression problems. Let\u2019s spend a few minutes understanding the idea behind SVR\n\n\n#### The Idea Behind Support Vector Regression\n\nThe problem of regression is to find a function that approximates mapping from an input domain to real numbers on the basis of a training sample. So let\u2019s now dive deep and understand how SVR works actually.","216d2fb8":"### Cons:","dfa33f3e":"### Linear kernel :\n\nIn linear kernel, the kernel function takes the form of a linear function as follows-\n\nlinear kernel : K(xi , xj ) = xiT xj\n\nLinear kernel is used when the data is linearly separable. It means that data can be separated using a single line. It is one of the most common kernels to be used. It is mostly used when there are large number of features in a dataset. Linear kernel is often used for text classification purposes.\n\nTraining with a linear kernel is usually faster, because we only need to optimize the C regularization parameter. When training with other kernels, we also need to optimize the \u03b3 parameter. So, performing a grid search will usually take more time.","648695da":"##  Run SVM with polynomial kernel","526e1533":"### Sigmoid kernel\n\nSigmoid kernel has its origin in neural networks. We can use it as the proxy for neural networks. Sigmoid kernel is given by the following equation \u2013\n\nsigmoid kernel : k (x, y) = tanh(\u03b1xTy + c)\n\nSigmoid kernel can be visualized with the following diagram-\n\n#### Sigmoid kernel\n\n","c84f919c":"## Mathematical formulation of SVM \n\nIf the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the \"margin\", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations:\n","289dda17":"### Radial Basis Function Kernel\n\nGaussian RBF(Radial Basis Function) is another popular Kernel method used in SVM models for more. RBF kernel is a function whose value depends on the distance from the origin or from some point. Gaussian Kernel is of the following format:-\n\n","a0129555":"# Support Vector Machines (SVM) : Beginner To Advanced","3ce27fc2":"#### Run SVM with rbf kernel and C=100.0\n\nWe have seen that there are outliers in our dataset. So, we should increase the value of C as higher C means fewer outliers. So, I will run SVM with kernel=rbf and C=100.0.","1f14e2c1":"The following diagram demonstrates the SVM classification with rbf kernel\n\n#### SVM Classification with rbf kernel\n","54495a81":"ROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.\n\nROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in classifying the pulsar star.","932731a7":"## Introduction to Support Vector Machines:","829ba68f":"#### In the context of SVMs, there are 4 popular kernels:-\n\n1. Linear kernel,\n2. Polynomial kernel,\n3. Radial Basis Function (RBF) kernel (also called Gaussian kernel) \n4. Sigmoid kernel. \n\n#### These are described below -","8679e6d1":"## Import dataset\n\n","44e2ef1a":"In practice, SVM algorithm is implemented using a kernel. It uses a technique called the kernel trick. In simple words, a kernel is just a function that maps the data to a higher dimension where data is separable. A kernel transforms a low-dimensional input data space into a higher dimensional space. So, it converts non-linear separable problems to linear separable problems by adding more dimensions to it. Thus, the kernel trick helps us to build a more accurate classifier. Hence, it is useful in non-linear separation problems.\n\nWe can define a kernel function as follows-","d61ae8a7":"#### for the point X3:\n   \n","65aa7f43":"## Confusion matrix \n\n\nA confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.","5d4b0460":"#### Note:\n\nIf \u03b1i>0 then Xi is a Support vector and when \u03b1i=0 then Xi is not a support vector.\n\n#### Observation:\n\n1. To solve the actual problem we do not require the actual data point instead only the dot product between every pair of a vector may suffice.\n\n2. To calculate the \u201cb\u201d biased constant we only require dot product.\n\n3. The major advantage of dual form of SVM over Lagrange formulation is that it only depends on the \u03b1.","8e06950f":"And finally last but very importrant characteristic of SVM classifier. SVM to core tries to achieve a good margin. A margin is a separation of line to the closest class points.\n\nA good margin is one where this separation is larger for both the classes. Images below gives to visual example of good and bad margin. A good margin allows the points to be in their respective classes without crossing to other class.","b60006ba":"We can see that percentage of observations of the class label 0 and 1 is 90.84% and 9.16%. So, this is a class imbalanced problem. I will deal with that in later section.","80f93158":"## SVM Objective\n\nIn SVMs, our main objective is to select a hyperplane with the maximum possible margin between support vectors.\nSVM searches for the maximum margin hyperplane in the following 2 step process \u2013\n\n1. Generate hyperplanes which segregates the classes in the best possible way. There are many hyperplanes that might classify the data. We should look for the best hyperplane that represents the largest separation, or margin, between the two classes.\n\n\n2. So, we choose the hyperplane so that distance from it to the support vectors on each side is maximized. If such a hyperplane exists, it is known as the  **maximum margin hyperplane**   and the linear classifier it defines is known as a **maximum margin classifier.**\n\nThe following diagram illustrates the concept of maximum margin and maximum margin hyperplane in a clear manner.","a4029f25":"the equations of each hyperplane can be considered as:\n\n","276c520c":"ROC Curve\n\nAnother tool to measure the classification model performance visually is ROC Curve. ROC Curve stands for Receiver Operating Characteristic Curve. An ROC Curve is a plot which shows the performance of a classification model at various classification threshold levels.","1f56139a":"if \u03bei= 0,\n\nthe points can be considered as correctly classified.\n\nelse:\n\n\u03bei> 0 , Incorrectly classified points.\n\nso if \u03bei> 0 it means that Xi(variables)lies in incorrect dimension, thus we can think of \u03bei as an error term associated with Xi(variable). The average error can be given as\n\n","0ee83800":"basically, we can separate each data point by projecting it into the higher dimension by adding relevant features to it. But with SVM there is a powerful way to achieve this task of projecting the data into a higher dimension. The above-discussed formulation was the **Primal form of SVM** . The alternative method is dual form of SVM which uses Lagrange\u2019s multiplier to solve the constraints optimization problem.\n\n","b88e8478":"#### Note:\n\nOur main aim here is to decide a decision boundary at \u2018a\u2019 distance from the original hyperplane such that data points closest to the hyperplane or the support vectors are within that boundary line.\n\nHence, we are going to take only those points that are within the decision boundary and have the least error rate, or are within the Margin of Tolerance. This gives us a better fitting model.","3484643c":"### Hard-Margin SVM\n\nAssume 3 hyperplanes namely (\u03c0, \u03c0+, \u03c0\u2212) such that \u2018\u03c0+\u2019 is parallel to \u2018\u03c0\u2019 passing through the support vectors on the positive side and \u2018\u03c0\u2212\u2019 is parallel to \u2018\u03c0\u2019 passing through the support vectors on the negative side.\n\n","9090605a":"## Run SVM with linear kernel \n\n\nRun SVM with linear kernel and C=1.0","48af7b60":"Assuming that the equation of the hyperplane is as follows:\n\nY = wx+b (equation of hyperplane)\n\nThen the equations of decision boundary become:\n\nwx+b= +a\n\nwx+b= -a\n\nThus, any hyperplane that satisfies our SVR should satisfy:\n\n-a < Y- wx+b < +a ","af4287be":"## Support Vector Regression (SVR)","010c5a81":"### Pros","32201dd2":"This formulation is called the Soft margin technique.\n\n#### Note:\nTo find the vector w and the scalar b such that the hyperplane represented by w and b maximizes the margin distance and minimizes the loss term subjected to the condition that all points are correctly classified.","8b877b95":"## Feature Scaling","8393ebd5":"#### Let\u2019s look into the constraints which are not classified:\n\n","761ae0a6":"### Margin","ead0ec8a":"#### Explanation: \nwhen the point X1 we can say that point lies on the hyperplane and the equation determines that the product of our actual output and the hyperplane equation is 1 which means the point is correctly classified in the positive domain.","d2600437":"Consider these two red lines as the decision boundary and the blue line as the hyperplane. Our **objective,** when we are moving on with SVR, is to basically consider the points that are within the decision boundary line. Our best fit line is the hyperplane that has a maximum number of points.\n\nThe first thing that we\u2019ll understand is what is the decision boundary (the danger red line above!). Consider these lines as being at any distance, say \u2018a\u2019, from the hyperplane. So, these are the lines that we draw at distance \u2018+a\u2019 and \u2018-a\u2019 from the hyperplane. This \u2018a\u2019 in the text is basically referred to as epsilon.","482848bf":"We can see that there are 9 variables in the dataset. 8 are continuous variables and 1 is discrete variable. The discrete variable is target_class variable. It is also the target variable.","b9569b08":"###  Table of Contents\n* [Introduction to Support Vector Machines](#Introduction-to-Support-Vector-Machines)\n* [SVM TERMINOLOGIES](#SVM-TERMINOLOGIES)\n* [SVM Objective](#SVM-Objective)\n* [Mathematical formulation of SVM](#Mathematical-formulation-of-SVM)\n* [Dual form of SVM](#Dual-form-of-SVM)\n* [What is Kernel trick?](#What-is-Kernel-trick?)\n* [Support Vector Regression (SVR)](#Support-Vector-Regression-(SVR))\n* [TUNING PARAMETERS OF SVM](#TUNING-PARAMETERS-OF-SVM)\n* [Data Preparation for SVM](#Data-Preparation-for-SVM)\n* [PROS AND CONS ASSOCIATED WITH SVM](#PROS-AND-CONS-ASSOCIATED-WITH-SVM)\n* [Exploratory data analysis ](#Exploratory-data-analysis )\n* [Declare feature vector and target variable](#Declare-feature-vector-and-target-variable)\n* [Split data into separate training and test set](#Split-data-into-separate-training-and-test-set)\n* [Feature Scaling](#Feature-Scaling)\n* [Run SVM with default hyperparameters](#Run-SVM-with-default-hyperparameters)\n* [Run SVM with linear kernel](#Run-SVM-with-linear-kernel)\n* [Run SVM with polynomial kernel ](#Run-SVM-with-polynomial-kernel )\n* [Run SVM with sigmoid kernel ](#Run-SVM-with-sigmoid-kernel )\n* [Confusion matrix ](#Confusion-matrix )\n* [Classification Report](#Classification-Report)\n* [ROC - AUC Curve](#ROC-AUC-Curve)\n* [Results and conclusion](#Results-and-conclusion)\n* [References](References)\n","04d9d873":"### Gamma\n\nThe gamma parameter defines how far the influence of a single training example reaches, with low values meaning \u2018far\u2019 and high values meaning \u2018close\u2019. In other words, with low gamma, points far away from plausible seperation line are considered in calculation for the seperation line. Where as high gamma means the points close to plausible line are considered in calculation.","575dbc2a":"             **I hope you will find this kernel useful and your UPVOTES would be highly appreciated**","4ab6ec73":"#### Note: \n\nWe get maximum accuracy with rbf and linear kernel with C=100.0. and the accuracy is 0.9832. Based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n\nBut, this is not true. Here, we have an imbalanced dataset. The problem is that accuracy is an inadequate measure for quantifying predictive performance in the imbalanced dataset problem.\n\nSo, we must explore alternative metrices that provide better guidance in selecting models. In particular, we would like to know the underlying distribution of values and the type of errors our classifer is making.\n\nOne such metric to analyze the model performance in imbalanced classes problem is Confusion matrix.","956fb1d6":"#### for the point X6 :\n  \n","0393478a":"### Polynomial Kernel\nPolynomial kernel represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables. The polynomial kernel looks not only at the given features of input samples to determine their similarity, but also combinations of the input samples.\n\nFor degree-d polynomials, the polynomial kernel is defined as follows \u2013\n\n#### Polynomial kernel : \n","7d744a65":"####  Kernel function\n","b1f27db7":" thus our objective, mathematically can be described as\n","19cc1d82":"## Classification Report\nClassification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model. I have described these terms in later.\n\nWe can print a classification report as follows:-\n\n","da939db2":"Support Vector Machines (SVMs in short) are machine learning algorithms that are used for classification and regression purposes. SVMs are one of the powerful machine learning algorithms for classification, regression and outlier detection purposes. An SVM classifier builds a model that assigns new data points to one of the given categories. Thus, it can be viewed as a non-probabilistic binary linear classifier.\n\nThe original SVM algorithm was developed by Vladimir N Vapnik and Alexey Ya. Chervonenkis in 1963. At that time, the algorithm was in early stages. The only possibility is to draw hyperplanes for linear classifier. In 1992, Bernhard E. Boser, Isabelle M Guyon and Vladimir N Vapnik suggested a way to create non-linear classifiers by applying the kernel trick to maximum-margin hyperplanes. The current standard was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.\n\nSVMs can be used for linear classification purposes. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using the kernel trick. It enable us to implicitly map the inputs into high dimensional feature spaces.","761708ab":"### I hope you find this kernel useful and enjoyable,Your comments and feedback are most welcome.\n\n","9a18f60b":"#### Explanation: \nwhen the point X6 we can say that point lies away from the hyperplane in the negative region and the equation determines that the product of our actual output and the hyperplane equation is greater 1 which means the point is correctly classified in the negative domain.","717e5aa5":"In this case, we can see that the accuracy had decreased with C=1000.0","665b4889":"## SVM TERMINOLOGIES\n\n###  Hyperplane\nA hyperplane is a decision boundary which separates between given set of data points having different class labels. The SVM classifier separates data points using a hyperplane with the maximum amount of margin. This hyperplane is known as the maximum margin hyperplane and the linear classifier it defines is known as the maximum margin classifier.\n\n### Support Vectors\nSupport vectors are the sample data points, which are closest to the hyperplane. These data points will define the separating line or hyperplane better by calculating margins.\n\n### Margin\nA margin is a separation gap between the two lines on the closest data points. It is calculated as the perpendicular distance from the line to support vectors or closest data points. In SVMs, we try to maximize this separation gap so that we get maximum margin.\n\nThe following diagram illustrates these concepts visually.\n","0e9a216e":"We basically consider that the data is linearly separable and this might not be the case in real life scenario. We need an update so that our function may skip few outliers and be able to classify almost linearly separable points. For this reason, we introduce a new Slack variable ( \u03be ) which is called Xi.\nif we introduce \u03be it into our previous equation we can rewrite it as\n\n","91beb4c1":"We can see that we can obtain higher accuracy with C=100.0 and C=1000.0 as compared to C=1.0.\n\nHere, y_test are the true class labels and y_pred are the predicted class labels in the test-set.","d405c15f":"##  Exploratory data analysis "}}