{"cell_type":{"f7fe6c8f":"code","cfa638eb":"code","f0d43e87":"code","60eab6d3":"code","4a87cbc0":"code","c356afa1":"code","c3968712":"code","3e6bd4ae":"code","095f5a1d":"code","8dadc517":"code","37ef2f2f":"code","4fab32c1":"code","ff71c48e":"code","f0ed2a4c":"code","aa83e177":"code","341c0af3":"code","ac6b0450":"code","b8042462":"code","29d0621a":"code","4d1a5c47":"code","197f3608":"code","810c2e1a":"code","e0619185":"code","ed94a4b0":"code","b3b24e12":"code","2d6d8f3c":"code","06246235":"code","205d0676":"code","fcb0a282":"code","91a50c34":"code","9e06767c":"code","d4da8b03":"code","951edb55":"code","a7d412d6":"code","3651bb00":"code","f3b98914":"code","a7457bf3":"code","36b98042":"code","c32fa1b5":"markdown","5a8cb344":"markdown","49bcfcf7":"markdown","5bff87cf":"markdown","48ec2d99":"markdown","1b2f06e3":"markdown","d9cd2d0d":"markdown","e8b30c3c":"markdown","bf743d0f":"markdown","a9a38560":"markdown","c58ef5fd":"markdown","9db627d6":"markdown","f9f82335":"markdown","9b70ef48":"markdown","0fbb9a18":"markdown","cc3ac5cb":"markdown","766d75db":"markdown","646b554c":"markdown","7a570f82":"markdown","177437e1":"markdown","ff827c3d":"markdown","2be1415a":"markdown","1d4d8da0":"markdown","5cf89424":"markdown","ded450e1":"markdown","c9a28896":"markdown","2ca029c1":"markdown","ace42ba6":"markdown"},"source":{"f7fe6c8f":"import copy\nimport math\n\nimport lightgbm as lgb\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport shap\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import LabelEncoder","cfa638eb":"train_df = pd.read_csv('..\/input\/train.csv', index_col='Id')\ntrain_df","f0d43e87":"X_train = train_df.drop(columns='SalePrice')\ny_train = train_df['SalePrice']","60eab6d3":"X_test = pd.read_csv('..\/input\/test.csv', index_col='Id')\nX_test","4a87cbc0":"X_all = pd.concat([X_train, X_test])","c356afa1":"numer_features = set(X_train.select_dtypes(exclude=['category', 'object']).columns)\nsorted(numer_features)","c3968712":"X_all[numer_features].hist(\n    bins=50, log=True, figsize=(15, 40), layout=(math.ceil(len(numer_features) \/ 3), 3));","3e6bd4ae":"nan_counts = X_all[numer_features].isna().sum()\nnan_counts[nan_counts > 0].sort_index()","095f5a1d":"for feature in [\n    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF',\n    'GarageArea', 'GarageCars', 'MasVnrArea', 'TotalBsmtSF'\n]:\n    for X in [X_train, X_test]:\n        X.loc[X[feature].isna(), feature] = 0.","8dadc517":"X_all.loc[~X_all['GarageYrBlt'].isna(), 'GarageArea'].hist();","37ef2f2f":"for X in [X_train, X_test]:\n    selection = (X['GarageYrBlt'].isna() | (X['GarageYrBlt'] > 2015))\n    X.loc[selection, 'GarageYrBlt'] = X.loc[selection, 'YearBuilt']","4fab32c1":"y_train.groupby(X_train['LotFrontage'].isna()).mean()","ff71c48e":"selection = ~X_all['LotFrontage'].isna()\n\nfig = plt.figure(figsize=(12.8, 4.8))\naxes = fig.add_subplot(1, 2, 1)\naxes.hist(\n    [X_all.loc[selection, 'LotArea'], X_all.loc[~selection, 'LotArea']],\n    label=['Present', 'Missing'], bins=30, density=True)\naxes.set_yscale('log')\naxes.set_xlabel('LotArea')\naxes.legend()\n\naxes = fig.add_subplot(1, 2, 2)\naxes.scatter(X_all.loc[selection, 'LotArea'].apply(np.sqrt), X_all.loc[selection, 'LotFrontage'], s=1)\naxes.set_xlabel('sqrt(LotArea)')\naxes.set_ylabel('LotFrontage');\n","f0ed2a4c":"sel = ~X_train['LotFrontage'].isna()\nfX = X_train.loc[sel, 'LotArea'].apply(np.sqrt).to_numpy().reshape(-1, 1)\nfy = X_train.loc[sel, 'LotFrontage']\n\nfrontage_model = KNeighborsRegressor(50)\nfrontage_model.fit(fX, fy)\n\nfig = plt.figure()\naxes = fig.add_subplot(111)\naxes.scatter(fX, fy, s=1)\nx = np.linspace(0., 300.)\naxes.plot(x, frontage_model.predict(x.reshape(-1, 1)), c='C1')\naxes.set_xlim(x[0], x[-1])\naxes.set_xlabel('sqrt(LotArea)')\naxes.set_ylabel('LotFrontage')\nfig;","aa83e177":"for X in [X_train, X_test]:\n    sel = X['LotFrontage'].isna()\n    X.loc[sel, 'LotFrontage'] = frontage_model.predict(\n        X.loc[sel, 'LotArea'].apply(np.sqrt).to_numpy().reshape(-1, 1))","341c0af3":"for X in [X_train, X_test, X_all]:\n    X['MSSubClass'] = X['MSSubClass'].astype('category')\nnumer_features.remove('MSSubClass')","ac6b0450":"cat_features = set(X_train.select_dtypes(include=['category', 'object']).columns)\ncat_features","b8042462":"for feature in sorted(cat_features):\n    print(feature, set(X_all[feature].unique()))","29d0621a":"y_train.groupby(X_train['BsmtCond']).mean().sort_values()","4d1a5c47":"y_train.groupby(X_train['HeatingQC']).mean().sort_values()","197f3608":"converted_cat_features = set()\ndef convert_features(features, mapping):\n    if isinstance(features, str):\n        features = [features]\n    for feature in features:\n        for X in [X_train, X_test]:\n            X[feature] = X[feature].map(mapping)\n    converted_cat_features.update(features)\n    \nquality_features = set(X_train.columns[X_train.isin(['TA', 'Ex']).any()])\nquality_transform = {'Po': -2, 'Fa': -1, 'TA': 0, 'Gd': 1, 'Ex': 2, np.nan: 0}\nconvert_features(quality_features, quality_transform)","810c2e1a":"convert_features('Alley', {'Grvl': 1, 'Pave': 2, np.nan: 0})  # Assume NaN means no alley\nconvert_features('CentralAir', {'N': 0, 'Y': 1})\nconvert_features('GarageFinish', {'Unf': 0, 'RFn': 1, 'Fin': 2, np.nan: 0})  # 'RFn' for 'rough finish'?\nconvert_features('PavedDrive', {'N': 0, 'P': 1, 'Y': 2})  # 'P' for 'partial'?\nconvert_features('Street', {'Grvl': 0, 'Pave': 1})\nconvert_features('LandSlope', {'Gtl': 0, 'Mod': 1, 'Sev': 2})  # Gentle, moderate, severe\nconvert_features('BsmtExposure', {'No': 0, 'Mn': 1, 'Av': 2, 'Gd': 3, np.nan: 0})","e0619185":"drop_features = set()\nfor feature in sorted(cat_features):\n    counts = X_train[feature].value_counts(dropna=False).sort_values(ascending=False)\n    if counts.iloc[0] \/ len(X_train) > 0.9:\n        drop_features.add(feature)\nprint(drop_features)\n\nfor X in [X_train, X_test]:\n    X.drop(columns=drop_features, inplace=True)","ed94a4b0":"for X in [X_train, X_test, X_all]:\n    for feature in [\n        'BsmtFinType1', 'BsmtFinType2', 'Exterior1st', 'Exterior2nd', 'Fence', 'GarageType',\n        'MSZoning', 'MasVnrType', 'SaleType'\n    ]:\n        X[feature].fillna('None', inplace=True)","b3b24e12":"for feature in sorted(cat_features - converted_cat_features - drop_features):\n    print(feature, sorted(list(X_all[feature].unique())))","2d6d8f3c":"unseen_categories = {}\nfor feature in sorted(cat_features - converted_cat_features - drop_features):\n    cats_train = set(X_train[feature].unique())\n    cats_test = set(X_test[feature].unique())\n    cats_unseen = cats_test - cats_train\n    if cats_unseen:\n        unseen_categories[feature] = cats_unseen\n        print(feature, sorted(cats_unseen), np.sum(X_test[feature].isin(cats_unseen)))","06246235":"for feature, values in unseen_categories.items():\n    for value in values:\n        replacement = X_train[feature].value_counts().idxmax()\n        X_test[feature].replace(value, replacement, inplace=True)\n        X_all[feature].replace(value, replacement, inplace=True)","205d0676":"passthrough_cat_features = list(cat_features - converted_cat_features - drop_features)\nfor feature in passthrough_cat_features:\n    encoder = LabelEncoder()\n    X_train[feature] = encoder.fit_transform(X_train[feature])\n    X_test[feature] = encoder.transform(X_test[feature])\n    X_all[feature] = encoder.transform(X_all[feature])","fcb0a282":"random_forest = RandomForestRegressor(n_estimators=1000, random_state=8266)\nscores = cross_val_score(random_forest, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nprint('{:g} +- {:g}'.format(scores.mean(), scores.std()))","91a50c34":"lgb_train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=None)\nbase_params = {\n    'objective': 'mae',\n    'seed': 5902,\n    'num_threads': 1\n}\nhistory = lgb.cv(\n    base_params, lgb_train_data, num_boost_round=1000, early_stopping_rounds=10,\n    categorical_feature=passthrough_cat_features,\n    nfold=5, stratified=False, verbose_eval=10\n)\nprint('CV error: {:g} +- {:g} at iteration {}'.format(\n    history['l1-mean'][-1], history['l1-stdv'][-1], len(history['l1-mean'])))","9e06767c":"def scan_1d(fixed_parameters, floating_parameter, values, nfold=5, logx=False):\n    params = copy.copy(base_params)\n    params.update(fixed_parameters)\n    cv_errors, cv_error_stds = [], []\n    for value in values:\n        params[floating_parameter] = value\n        history = lgb.cv(\n            params, lgb_train_data, num_boost_round=1000, early_stopping_rounds=10,\n            categorical_feature=passthrough_cat_features,\n            nfold=nfold, stratified=False, verbose_eval=False\n        )\n        if len(history['l1-mean']) == 1000:\n            print('Warning: CV error has not reached minimum.')\n        cv_errors.append(history['l1-mean'][-1])\n        cv_error_stds.append(history['l1-stdv'][-1])\n    best_index = np.argmin(cv_errors)\n    \n    fig = plt.figure()\n    axes = fig.add_subplot(111)\n    axes.errorbar(values, cv_errors, yerr=cv_error_stds, marker='o', ms=3, lw=0, elinewidth=0.8)\n    axes.plot(values[best_index], cv_errors[best_index], marker='o', ms=3, c='C3', zorder=3)\n    if logx:\n        axes.set_xscale('log')\n    axes.set_xlabel(floating_parameter)\n    axes.set_ylabel('CV error')\n    \n    return cv_errors","d4da8b03":"scan_1d({'learning_rate': 0.05}, 'num_leaves', [5, 10, 15, 20, 30, 50, 100]);","951edb55":"best_params = {\n    'learning_rate': 0.05,\n    'num_leaves': 15,\n    'max_depth': -1,\n    'bagging_freq': 1,\n    'bagging_fraction': 0.7,\n    'feature_fraction': 0.4,\n    'min_data_in_leaf': 10,\n    'lambda_l1': 0.,\n    'lambda_l2': 0.,\n    'min_gain_to_split': 0.,\n    'min_data_per_group': 40,\n    'max_cat_threshold': 20,\n    'cat_l2': 10.,\n    'cat_smooth': 8.\n}\nbest_params.update(base_params)","a7d412d6":"history = lgb.cv(\n    best_params, lgb_train_data, num_boost_round=1000, early_stopping_rounds=10,\n    categorical_feature=passthrough_cat_features,\n    nfold=10, stratified=False, verbose_eval=10\n)\nprint('CV error: {:g} +- {:g} at iteration {}'.format(\n    history['l1-mean'][-1], history['l1-stdv'][-1], len(history['l1-mean'])))","3651bb00":"history = {}\nmodel = lgb.train(\n    best_params, lgb_train_data, num_boost_round=383,\n    categorical_feature=passthrough_cat_features,\n    valid_sets=[lgb_train_data], verbose_eval=10, evals_result=history\n)\n\nfig = plt.figure()\naxes = fig.add_subplot(111)\nlgb.plot_metric(history, ax=axes);","f3b98914":"shap.initjs()\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_train)","a7457bf3":"shap.summary_plot(shap_values, X_train)","36b98042":"train_predictions = model.predict(X_train)\nprint('Train error:', mean_absolute_error(train_predictions, y_train))\n\ntest_predictions = model.predict(X_test)\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': test_predictions})\noutput.to_csv('submission.csv', index=False)","c32fa1b5":"Remaining categorical features will be handled by LightGBM directly. Only need to represent them with numbers.","5a8cb344":"There are only a few examples with these new categories, and it would be a pity to drop whole features because of this. Instead, replace the new values with with the most likely categories in the training set.","49bcfcf7":"I don't expect `LotFrontage` to bring a lot of information on top of the area, property type, neighbourhood, and so on. Not to spend much time on it, I'm going to impute it with a simple kNN regressor. It captures the linear scaling in the bulk of the distribution but at the same time does not extrapolate it to extreme values, which is probably a sane model. For a more accurate treatment see [this notebook](https:\/\/www.kaggle.com\/ogakulov\/lotfrontage-fill-in-missing-values-house-prices).","5bff87cf":"Finally, check importance of individual features using the `shap` package.","48ec2d99":"## Numeric features","1b2f06e3":"Check if there are missing values:","d9cd2d0d":"Train LightGBM with default parameters:","e8b30c3c":"Check if there are categories that are only seen in the test set:","bf743d0f":"The last column with missing values is `LotFrontage`. It's not clear what is the reason for having missing values there. Could they indicate that the house is situated at a substantial distance from the road? Then this would probably affect the average price, but it doesn't seem to:","a9a38560":"Drop features for which the most frequent value is found in more than 90% of instances.","c58ef5fd":"Missing built year for garage does not necessarily imply that the property doesn't have a garage:","9db627d6":"There is a number of ordinal features, which can be converted into numbers in a straightforward manner. Examples are the various quality measures, such as `ExterQual` and `KitchenQual`. Although it's hard to guess what does \u2018TA\u2019 mean. Let's see how the quality label correlate with the price for a few features:","f9f82335":"## Data\n\nMeanings of all features are provided [here](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/data).","9b70ef48":"Assume that the garage was built in the same year as the house then. Also change the year for a time-travelling garage from the XXIII century, which can be seen in the distributions above.","0fbb9a18":"Fill all NaNs in the remaning categorical columns:","cc3ac5cb":"## Regression\n\nIn contrast to what is claimed in the [description](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/overview\/evaluation) of the competition, the scoring is based on the mean absolute error (mentioned [here](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/discussion\/105838) and confirmed by comparing the score computed locally against the one reported in the leaderboard).","766d75db":"Lastly, column `MSSubClass` actually holds categorical data. Convert it accordingly.","646b554c":"Start with a random forest as a baseline:","7a570f82":"Proceed with other straightforward cases:","177437e1":"Most of these columns seem to refer to properties of a non-existent garage or basement. Set all these areas and numbers of bathrooms to zeros (note the distributions above already have peaks at zeros):","ff827c3d":"After a few iterations, arrive at the set of hyperparameters given below. Train the final model with them. The number of iterations to perform is determined with a cross-validation run.","2be1415a":"## Categorical features","1d4d8da0":"Optimize the hyperparameters by performing repeated 1D scans along different axes, as in the example below.","5cf89424":"## Submission","ded450e1":"Try to predict the missing values from other features. The single most relevant one is probably `LotArea`. The first plot below compares the distributions of the area for properties with known and missing lengths of the frontage. Although in the latter case the distribution has a heavier tail, the difference is not drastic. The second plot illustrates the dependence between `LotFrontage` and square root of `LotArea`. There is a somewhat linear trend with a large variance.","c9a28896":"Summary of the remaining categorical features:","2ca029c1":"## Introduction\n\nThis notebook is a development of the final exercise from the \u2018[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)\u2019 micro-course. Main items:\n* Missing values are imputed.\n* Ordinal categorical features are identified and converted to numeric ranks.\n* LightGBM is used to predict the price. It handles nominal categorical features directly.\n* Hyperparameters of LightGBM are tuned.\n* Importances of individual features are estimated with the `shap` package.\n","ace42ba6":"It seems \u2018TA\u2019 means average (\u2018typical\u2019?) quality. Let's convert all quality features to numbers. Map NaNs to the neutral value."}}