{"cell_type":{"04f41dd9":"code","6a345da1":"code","df32ba00":"code","2f0338e4":"markdown","cffc7ee7":"markdown","66d55232":"markdown"},"source":{"04f41dd9":"import pandas as pd\nimport numpy as np\nfrom joblib import Parallel, delayed\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import f1_score\nimport scipy\n\n\ndef simple_pipeline():\n    print(\"Load data\")\n    train, test = load_data()\n    \n    data = pd.concat([train, test], axis=0, ignore_index=True)\n    print(\"Vectorization\")\n    X = vectorization(data.drop('target', axis=1))\n    if type(X) == scipy.sparse.coo_matrix:\n        X = X.tocsr()\n        \n    test_mask = data.is_test.values\n    \n    X_train = X[~test_mask]\n    y_train = data['target'][~test_mask]\n    \n    X_test = X[test_mask]\n    if scipy.sparse.issparse(X):\n        X_train.sort_indices()\n        X_test.sort_indices()\n\n    model = build_model(X_train, y_train)\n    \n    print(\"Prediction with model\")\n    p = model.predict(X_test)\n    \n    print(\"Generate submission\")\n    make_submission(data[test_mask], p)\n\n\ndef load_data():\n    train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n    train['is_test'] = False\n    \n    test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n    test['target'] = -1\n    test['is_test'] = True\n    \n    return train, test\n\n\ndef calculate_validation_metric(model, X, y, metric):\n    folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=0)\n    score = cross_val_score(model, X, y, scoring=metric, cv=folds, n_jobs=4)\n    \n    return np.mean(score), model\n\n\ndef select_model(X, y):\n    models = [\n        LinearSVC(C=30),\n        LinearSVC(C=10),\n        LinearSVC(C=3),\n        LinearSVC(C=1),\n        LinearSVC(C=0.3),\n        LinearSVC(C=0.1),\n        LinearSVC(C=0.03),\n        RidgeClassifier(alpha=30),\n        RidgeClassifier(alpha=10),\n        RidgeClassifier(alpha=3),\n        RidgeClassifier(alpha=1),\n        RidgeClassifier(alpha=0.3),\n        RidgeClassifier(alpha=0.1),\n        RidgeClassifier(alpha=0.03),\n        LogisticRegression(C=30),\n        LogisticRegression(C=10),\n        LogisticRegression(C=3),\n        LogisticRegression(C=1),\n        LogisticRegression(C=0.3),\n        LogisticRegression(C=0.1),\n        LogisticRegression(C=0.03),\n    ]\n    \n    results = [calculate_validation_metric(\n        model, X, y, 'f1_macro',\n    ) for model in models]\n\n    best_result, best_model = max(results, key = lambda x: x[0]) \n    print(\"Best model validation result: {:.4f}\".format(best_result))\n    print(\"Best model: {}\".format(best_model))\n    \n    return best_model\n\n\ndef build_model(X, y):\n    print(\"Selecting best model\")\n    best_model = select_model(X, y)\n    \n    print(\"Refit model to full dataset\")\n    best_model.fit(X, y)\n    \n    return best_model\n\n    \ndef make_submission(data, p):\n    submission = data[['id']].copy()\n    submission['target'] = p\n    submission.to_csv('submission.csv', index=False)","6a345da1":"from gensim.models.word2vec import Word2Vec \nimport re\nimport nltk\nfrom nltk import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.sparse import hstack\n\ndef text_to_sent(t):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    text = t.fillna(\"\").str.lower()\n    sentences = text.str.split().apply(lambda x: [wordnet_lemmatizer.lemmatize(w) for w in x])\n    return [[y for y in x if re.match('[\u0430-\u044f\u0451a-z0-9]', y)]\n            for x in sentences]\n\ndef vectorization(data):\n    \"\"\"\n    data is concatenated train and test datasets with target excluded\n    Result value \"vectors\" expected to have some number of rows as data\n    \"\"\"\n    \n    word_vec = TfidfVectorizer(\n        ngram_range=(1, 1),\n        max_df=0.99,\n        min_df=2,\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n        norm='l2'\n    )\n    \n    char_vec = TfidfVectorizer(\n    analyzer='char_wb',\n    ngram_range=(3, 8), \n    max_df=0.99, \n    min_df=0.001,\n    use_idf=True,\n    smooth_idf=True,\n    sublinear_tf=False,\n    norm='l2'\n    )\n    \n    text = data['text'].fillna('')\n    char_vectors = char_vec.fit_transform(text)\n    \n    word_vectors = word_vec.fit_transform(text)\n#     vectors =  hstack((char_vectors, word_vectors))\n\n    sentences = text_to_sent(data['text'])\n\n    num_features = 300  # \u0438\u0442\u043e\u0433\u043e\u0432\u0430\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\n    min_word_count = 2  # \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u043e\u0441\u0442\u044c \u0441\u043b\u043e\u0432\u0430, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u043e \u043f\u043e\u043f\u0430\u043b\u043e \u0432 \u043c\u043e\u0434\u0435\u043b\u044c\n    num_workers = 8     # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044f\u0434\u0435\u0440 \u0432\u0430\u0448\u0435\u0433\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u0430, \u0447\u0442\u043e\u0431 \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0432 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0442\u043e\u043a\u043e\u0432\n    context = 10        # \u0440\u0430\u0437\u043c\u0435\u0440 \u043e\u043a\u043d\u0430 \n    downsampling = 1e-3 # \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u044f\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\n\n    model = Word2Vec(sentences, workers=num_workers, size=num_features,\n                     min_count=min_word_count, window=context, sample=downsampling)\n    \n    index2word_set = set(model.wv.index2word)\n    \n    def text_to_vec(words):\n        text_vec = np.zeros((300,), dtype=\"float32\")\n        n_words = 0\n        for word in words:\n            if word in index2word_set:\n                n_words = n_words + 1\n                text_vec = np.add(text_vec, model.wv[word])\n        if n_words != 0:\n            text_vec \/= n_words\n        return text_vec\n    \n    texts_vecs = np.zeros((len(data['text']), 300), dtype=\"float32\")\n\n    for i, s in enumerate(sentences):\n        texts_vecs[i] = text_to_vec(s)\n        \n    vectors =  hstack((char_vectors, texts_vecs, word_vectors))\n    \n    return vectors","df32ba00":"%%time \n\nsimple_pipeline()","2f0338e4":"# Fixed pipeline\nIn order to participate, the part below need to be unchanged","cffc7ee7":"# Your part\n## In *vectorization* method you can change everything and use any dependencies","66d55232":"# **Vectorizers** - #1 micro challenge\n# Rules\nI have an idea of an alternative challenge format for a while. I want to test it.\nIn short, it's a short challenge with specific measurable goals to be achieved.\n\nIn this challenge, you are given a fixed pipeline and only can change the vectorization process. The vectorization method interface is fixed, the rest is up to you.\n\nIn order to compete, you need to **make your Kaggle notebook public**.\n\n# Challenge [data](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data)\nData is the same as for the official competition, you can read description here https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data\n\n# Goals\n- \ud83e\udd49 Bronze. F1-score >= **0.80** at **public** leaderboard \n- \ud83e\udd48 Silver. F1-score >= **0.81** at **public** leaderboard\n- \ud83e\udd47 Gold. F1-score >= **0.81** at **public** leaderboard + runtime is below **1 minute**\n\n# [Submit](https:\/\/forms.gle\/H8MPo4xpu4NDVsX49)\nYou can submit your **public** Kaggle notebook via this [link](https:\/\/forms.gle\/H8MPo4xpu4NDVsX49) \n# [Leaderboard](http:\/\/bit.ly\/36pSp3S) \nThe final leaderboard is sorted by a medal type and then by submission time. The earlier you achieved the goal is better. You can see current leaderboard by this [link](http:\/\/bit.ly\/36pSp3S)"}}