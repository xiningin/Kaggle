{"cell_type":{"9d1095db":"code","d9d5d0d6":"markdown"},"source":{"9d1095db":"from fastai.vision.all import *\n\nclass MixUp(MixHandler):\n    \"Implementation of https:\/\/arxiv.org\/abs\/1710.09412\"\n    def __init__(self, alpha=.4): super().__init__(alpha)\n    def before_batch(self):\n        lam = self.distrib.sample((self.y.size(0),)).squeeze().to(self.x[0].device)\n        lam = torch.stack([lam, 1-lam], 1)\n        self.lam = lam.max(1)[0]\n        shuffle = torch.randperm(self.y.size(0)).to(self.x[0].device)\n        xb1, xb2, self.yb1 = list(L(self.xb[0]).itemgot(shuffle)), list(L(self.xb[1]).itemgot(shuffle)), list(L(self.yb).itemgot(shuffle))\n\n        nx_dims = len(self.x[0].size())\n#         print(nx_dims) # 4\n\n        a = L(xb1, self.xb[0])\n        b = L(xb2, self.xb[1])\n        \n        a = a.map_zip(torch.lerp, weight=unsqueeze(self.lam, n=nx_dims-1))\n        b = b.map_zip(torch.lerp, weight=unsqueeze(self.lam, n=nx_dims-3))   \n\n        self.learn.xb = (a[0], b[0])\n\n        \n        if not self.stack_y:\n            ny_dims = len(self.y.size())\n            self.learn.yb = list(L(self.yb1, self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))\n","d9d5d0d6":"<div class=\"alert alert-warning\">\nThis work consists of two parts:     \n    <ul>\n        <li> It is a modified version of the original fastai API mixup code for the introduction of meta data in training. <\/li>\n         <li> If you find it helpful, please give a upvote! It will inspire me to share more instereting work! <\/li>\n        <li> Can be applied in the notebook: https:\/\/www.kaggle.com\/calvchen\/swin-transformer-224-meta with cbs=[MixUp(0.2)].<\/a><\/li>\n    <\/ul>\n    \n<\/div>\n\n\n    \n\n\n\n\n"}}