{"cell_type":{"794a65b5":"code","1a4c50b7":"code","a7cabf50":"code","7d01eb68":"code","85de92d0":"code","c2263fc5":"code","9b32c3a8":"code","bde7a40a":"code","39933386":"code","bab90326":"code","2aa78d38":"code","c9fc7035":"code","e64af5b9":"markdown","08d1d810":"markdown","80d17cb3":"markdown","75485790":"markdown","e797ea86":"markdown","f163d641":"markdown","29266816":"markdown"},"source":{"794a65b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom IPython.display import display, Image\nfrom matplotlib.pyplot import imshow\nfrom keras.layers import Conv2D, UpSampling2D, InputLayer\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import img_to_array, load_img\nfrom skimage.color import lab2rgb, rgb2lab\n\n\nimport os\nprint(os.listdir(\"..\/input\/urban-and-rural-photos\/rural_and_urban_photos\/train\/\"))","1a4c50b7":"INPUT_IMAGE_SRC = '..\/input\/urban-and-rural-photos\/rural_and_urban_photos\/train\/urban\/urban_11.jpeg'\ndisplay(Image(INPUT_IMAGE_SRC, width=225))","a7cabf50":"#Converting RGB to LAB\nimage = img_to_array(load_img(INPUT_IMAGE_SRC, target_size=(200,200))) \/ 255\nlab_image = rgb2lab(image)\nlab_image.shape","7d01eb68":"lab_image_norm = (lab_image + [0,128,128]) \/ [100,255,255]","85de92d0":"\n#Input ---> Black and White Layer\nX = lab_image_norm[:,:,0]\n\n#output ---> ab channels\nY = lab_image_norm[:,:,1:]","c2263fc5":"X = X.reshape(1, X.shape[0], X.shape[1], 1)\nY = Y.reshape(1, Y.shape[0], Y.shape[1], 2)","9b32c3a8":"model = Sequential()\nmodel.add(InputLayer(input_shape=(None, None, 1)))\nmodel.add(Conv2D(8, (3, 3), activation='relu', padding='same', strides=2))\nmodel.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(16, (3, 3), activation='relu', padding='same', strides=2))\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(2, (3,3), activation='tanh', padding='same'))\n\n#Finish model\nmodel.compile(optimizer = 'rmsprop', loss = \"mse\")\nmodel.fit(x=X, y=Y, batch_size = 1, epochs = 2000, verbose = 1)","bde7a40a":"model.evaluate(X,Y, batch_size = 1)","39933386":"output = model.predict(X)\ncur = np.zeros((200, 200, 3))\ncur[:,:,0] = X[0][:,:,0]\ncur[:,:,1:] = output[0]\n\n\ncur = (cur * [100, 255, 255]) - [0, 128, 128]\nrgb_image = lab2rgb(cur)\nimshow(rgb_image)\n","bab90326":"INPUT_IMAGE_SRC_PRE = '..\/input\/urban-and-rural-photos\/rural_and_urban_photos\/train\/urban\/urban_1.jpeg'\ndisplay(Image(INPUT_IMAGE_SRC_PRE, width=225))","2aa78d38":"imagepre = img_to_array(load_img(INPUT_IMAGE_SRC_PRE, target_size=(200,200))) \/ 255\nlab_image_pre = rgb2lab(imagepre)\nlab_image_norm_pre = (lab_image_pre + [0,128,128]) \/ [100,255,255]\nX = lab_image_norm[:,:,0]\nX = X.reshape(1, X.shape[0], X.shape[1], 1)","c9fc7035":"output = model.predict(X)\ncur = np.zeros((200, 200, 3))\ncur[:,:,0] = X[0][:,:,0]\ncur[:,:,1:] = output[0]\n\n\ncur = (cur * [100, 255, 255]) - [0, 128, 128]\nrgb_image = lab2rgb(cur)\nimshow(rgb_image)\n","e64af5b9":"\n\nOne more normalization step is needed, before we can extract the data. The color values in the different Lab layers are in various ranges, as follows:\n\n    L: (0, 100)\n    a & b: (-128, 128)","08d1d810":"![Model Architecture](https:\/\/raw.githubusercontent.com\/lukemelas\/lukemelas.github.io-src\/master\/content\/assets\/images\/colorization\/model.jpg)","80d17cb3":"MODEL ARCHITECTURE(But we are using are own model not resnet)","75485790":"\n\nHaving turned the image into an Lab represantion, let's define the inputs and outputs. The input in our case will be the black and white layer (lab_image[:,:,0])\n","e797ea86":"The Conv2D layer we will use later expects the inputs and training outputs to be of the following format:\n# (samples, rows, cols, channels), so we need to do some reshaping","f163d641":"The Lab color space describes mathematically all perceivable colors in the three dimensions L for lightness and a and b for the color components green\u2013red and blue\u2013yellow.and the reason why we want to use it instead of RGB","29266816":"Array Slicing---->a[start:stop:step]\n"}}