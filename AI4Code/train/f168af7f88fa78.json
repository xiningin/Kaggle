{"cell_type":{"a6efb40d":"code","09b8cbed":"code","1a8055fe":"code","b6bdcc5f":"code","69ef8689":"code","64618bfb":"code","4d4a70bc":"code","f7614853":"code","86c026c2":"code","2ded7473":"code","5c08350b":"code","793ebd14":"code","4bbfcb96":"code","321d5a3d":"code","88da381e":"markdown"},"source":{"a6efb40d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","09b8cbed":"pd.set_option('mode.chained_assignment', None)\ntest = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\ntrain['Province_State'].fillna('', inplace=True)\ntest['Province_State'].fillna('', inplace=True)\ntrain['Date'] =  pd.to_datetime(train['Date'])\ntest['Date'] =  pd.to_datetime(test['Date'])\ntrain = train.sort_values(['Country_Region','Province_State','Date'])\ntest = test.sort_values(['Country_Region','Province_State','Date'])\n\n# Fix error in train data\ntrain[['ConfirmedCases', 'Fatalities']] = train.groupby(['Country_Region', 'Province_State'])[['ConfirmedCases', 'Fatalities']].transform('cummax') ","1a8055fe":"from tqdm import tqdm\nimport warnings\n\ndef RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))\n\nfeature_day = [1,20,50,100,200,500,1000,5000,10000,15000,20000,50000,100000,200000]\ndef CreateInput(data):\n    feature = []\n    for day in feature_day:\n        #Get information in train data\n        data.loc[:,'Number day from ' + str(day) + ' case'] = 0\n        if (train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['ConfirmedCases'] < day)]['Date'].count() > 0):\n            fromday = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['ConfirmedCases'] < day)]['Date'].max()        \n        else:\n            fromday = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].min()       \n        for i in range(0, len(data)):\n            if (data['Date'].iloc[i] > fromday):\n                day_denta = data['Date'].iloc[i] - fromday\n                data['Number day from ' + str(day) + ' case'].iloc[i] = day_denta.days \n        feature = feature + ['Number day from ' + str(day) + ' case']\n    \n    return data[feature]\n","b6bdcc5f":"!pip install pmdarima","69ef8689":"import pmdarima as pm\n\npred_data_all = pd.DataFrame()\nwith tqdm(total=len(train['Country_Region'].unique())) as pbar:\n    for country in train['Country_Region'].unique():\n    #for country in ['Vietnam']:\n        for province in train[(train['Country_Region'] == country)]['Province_State'].unique():\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\")\n                df_train = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]\n                df_test = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n                X_train = CreateInput(df_train)\n                y_train_confirmed = df_train['ConfirmedCases'].ravel()\n                y_train_fatalities = df_train['Fatalities'].ravel()\n                X_pred = CreateInput(df_test)\n\n                # Define feature to use by X_pred\n                feature_use = X_pred.columns[0]\n                for i in range(X_pred.shape[1] - 1,0,-1):\n                    if (X_pred.iloc[0,i] > 10):\n                        feature_use = X_pred.columns[i]\n                        break\n                idx = X_train[X_train[feature_use] == 0].shape[0]          \n                adjusted_X_train = X_train[idx:][feature_use].values.reshape(-1, 1)\n                adjusted_y_train_confirmed = y_train_confirmed[idx:]\n                adjusted_y_train_fatalities = y_train_fatalities[idx:] #.values.reshape(-1, 1)\n\n                pred_data = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n                max_train_date = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].max()\n                min_test_date = pred_data['Date'].min()            \n\n                model = pm.auto_arima(adjusted_y_train_confirmed, suppress_warnings=True, seasonal=False, error_action=\"ignore\")            \n                y_hat_confirmed = model.predict(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                y_train_confirmed = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['ConfirmedCases'].values\n                y_hat_confirmed = np.concatenate((y_train_confirmed,y_hat_confirmed), axis = 0)                        \n\n                model = pm.auto_arima(adjusted_y_train_fatalities, suppress_warnings=True, seasonal=False, error_action=\"ignore\")            \n                y_hat_fatalities = model.predict(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                y_train_fatalities = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['Fatalities'].values\n                y_hat_fatalities = np.concatenate((y_train_fatalities,y_hat_fatalities), axis = 0)            \n\n                pred_data['ConfirmedCases_hat'] = y_hat_confirmed\n                pred_data['Fatalities_hat'] = y_hat_fatalities\n                pred_data_all = pred_data_all.append(pred_data)\n        pbar.update(1)\n    \ndf_val = pd.merge(pred_data_all,train[['Date','Country_Region','Province_State','ConfirmedCases','Fatalities']],on=['Date','Country_Region','Province_State'], how='left')\ndf_val.loc[df_val['Fatalities_hat'] < 0,'Fatalities_hat'] = 0\ndf_val.loc[df_val['ConfirmedCases_hat'] < 0,'ConfirmedCases_hat'] = 0\n\ndf_val_1 = df_val.copy()","64618bfb":"from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import NumeralTickFormatter\nfrom bokeh.palettes import Spectral11\noutput_notebook()\ndef plotCountry(df_country, name):\n    p = figure(title=name + \" Confirmed Cases Forecast\", x_axis_label='Date', x_axis_type='datetime', y_axis_label='Confirmed Cases')\n    p.line(df_country['Date'], df_country['ConfirmedCases_hat'], legend_label=\"Confirmed Cases\", line_width=2)\n    p.legend.location = \"top_left\"\n    p.yaxis.formatter=NumeralTickFormatter(format=\"\u20180.0a\")    \n    show(p)\n\n    p = figure(title=name + \" Fatalities Forecast\", x_axis_label='Date', x_axis_type='datetime', y_axis_label='Fatalities Cases')\n    p.line(df_country['Date'], df_country['Fatalities_hat'], legend_label=\"Fatalities \", line_width=2)\n    p.legend.location = \"top_left\"\n    p.yaxis.formatter=NumeralTickFormatter(format=\"\u20180.0a\")    \n    show(p)\n\ndef plotTop(df_val):\n    df_now = train.groupby(['Date','Country_Region']).sum().sort_values(['Country_Region','Date']).reset_index()\n    df_now['New Cases'] = df_now['ConfirmedCases'].diff()\n    df_now['New Fatalities'] = df_now['Fatalities'].diff()\n    df_now = df_now.groupby('Country_Region').apply(lambda group: group.iloc[-1:]).reset_index(drop = True)\n\n    p = figure(title=\" Top 5 Confirmed Cases Forecast\", x_axis_label='Date', x_axis_type='datetime', y_axis_label='Confirmed Cases')\n    mypalette=Spectral11[0:5]\n    i = 0\n    for country in df_now.sort_values('ConfirmedCases', ascending=False).head(5)['Country_Region'].values:\n        df_country = df_val[df_val['Country_Region'] == country].groupby(['Date','Country_Region']).sum().reset_index()\n        idx = df_country[((df_country['ConfirmedCases'].isnull() == False) & (df_country['ConfirmedCases'] > 0))].shape[0]\n        p.line(df_country['Date'], df_country['ConfirmedCases_hat'], legend_label= country + \" Confirmed Cases\", line_color=mypalette[i], line_width=2)\n        p.legend.location = \"top_left\"\n        p.yaxis.formatter=NumeralTickFormatter(format=\"\u20180.0a\")    \n        i = i+1\n\n    show(p)        ","4d4a70bc":"country = \"Vietnam\"\ndf_country = df_val[df_val['Country_Region'] == country].groupby(['Date','Country_Region']).sum().reset_index()\nplotCountry(df_country,country)","f7614853":"df_country = df_val.groupby(['Date']).sum().reset_index()\nplotCountry(df_country,'World')","86c026c2":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\npred_data_all = pd.DataFrame()\nwith tqdm(total=len(train['Country_Region'].unique())) as pbar:\n    for country in train['Country_Region'].unique():\n    #for country in ['Spain']:\n        for province in train[(train['Country_Region'] == country)]['Province_State'].unique():\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\")\n                df_train = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]\n                df_test = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n                X_train = CreateInput(df_train)\n                y_train_confirmed = df_train['ConfirmedCases'].ravel()\n                y_train_fatalities = df_train['Fatalities'].ravel()\n                X_pred = CreateInput(df_test)\n\n                # Define feature to use by X_pred\n                feature_use = X_pred.columns[0]\n                for i in range(X_pred.shape[1] - 1,0,-1):\n                    if (X_pred.iloc[0,i] > 10):\n                        feature_use = X_pred.columns[i]\n                        break\n                idx = X_train[X_train[feature_use] == 0].shape[0]          \n                adjusted_X_train = X_train[idx:][feature_use].values.reshape(-1, 1)\n\n                adjusted_y_train_confirmed = y_train_confirmed[idx:]\n                adjusted_y_train_fatalities = y_train_fatalities[idx:] #.values.reshape(-1, 1)\n                \n                # Log to forecast Not log because of decrease trending\n                #adjusted_y_train_confirmed = np.log1p(adjusted_y_train_confirmed + 1)\n                #adjusted_y_train_fatalities = np.log1p(adjusted_y_train_fatalities + 1)\n                \n\n                pred_data = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n                max_train_date = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].max()\n                min_test_date = pred_data['Date'].min()            \n\n                #model = pm.auto_arima(adjusted_y_train_confirmed, suppress_warnings=True, seasonal=False, error_action=\"ignore\")            \n                #y_hat_confirmed = model.predict(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                model = SARIMAX(adjusted_y_train_confirmed, order=(1,1,0),\n                                #seasonal_order=(1,1,0,12),\n                                measurement_error=True).fit(disp=False)\n                y_hat_confirmed = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                # inverse log\n                #y_hat_confirmed = np.expm1(y_hat_confirmed)\n                \n                y_train_confirmed = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['ConfirmedCases'].values\n                y_hat_confirmed = np.concatenate((y_train_confirmed,y_hat_confirmed), axis = 0)                        \n\n                #model = pm.auto_arima(adjusted_y_train_fatalities, suppress_warnings=True, seasonal=False, error_action=\"ignore\")\n                #y_hat_fatalities = model.predict(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                # inverse log\n                #y_hat_fatalities = np.expm1(y_hat_fatalities)\n                \n                model = SARIMAX(adjusted_y_train_fatalities, order=(1,1,0),\n                                #seasonal_order=(1,1,0,12),\n                                measurement_error=True).fit(disp=False)\n                y_hat_fatalities = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                                \n                y_train_fatalities = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['Fatalities'].values\n                y_hat_fatalities = np.concatenate((y_train_fatalities,y_hat_fatalities), axis = 0)            \n\n                pred_data['ConfirmedCases_hat'] = y_hat_confirmed\n                pred_data['Fatalities_hat'] = y_hat_fatalities\n                pred_data_all = pred_data_all.append(pred_data)\n        pbar.update(1)\n    \ndf_val = pd.merge(pred_data_all,train[['Date','Country_Region','Province_State','ConfirmedCases','Fatalities']],on=['Date','Country_Region','Province_State'], how='left')\ndf_val.loc[df_val['Fatalities_hat'] < 0,'Fatalities_hat'] = 0\ndf_val.loc[df_val['ConfirmedCases_hat'] < 0,'ConfirmedCases_hat'] = 0\n\ndf_val_2 = df_val.copy()","2ded7473":"country = \"Vietnam\"\ndf_country = df_val[df_val['Country_Region'] == country].groupby(['Date','Country_Region']).sum().reset_index()\nplotCountry(df_country,country)","5c08350b":"df_country = df_val.groupby(['Date']).sum().reset_index()\nplotCountry(df_country,'World')","793ebd14":"plotTop(df_val_1)","4bbfcb96":"plotTop(df_val_2)","321d5a3d":"df_val = df_val_2\nsubmission = df_val[['ForecastId','ConfirmedCases_hat','Fatalities_hat']]\nsubmission.columns = ['ForecastId','ConfirmedCases','Fatalities']\nsubmission = submission.round({'ConfirmedCases': 0, 'Fatalities': 0})\nsubmission.to_csv('submission.csv', index=False)\nsubmission","88da381e":"### SARIMAX Model\n\n\n\n\nNotebook from week 3 competition is [here](https:\/\/www.kaggle.com\/akashsuper2000\/arima-model)<br\/>\n<br\/>\nCredit to @binhlc for the code from week-3 competition.\nCheck out the original notebook from binhlc [here](https:\/\/www.kaggle.com\/binhlc\/sars-cov-2-arima-model-week-3\/)"}}