{"cell_type":{"1bc19447":"code","891cf09f":"code","ebc16a15":"code","999a5c7b":"code","2a352103":"code","5e585c4f":"code","c2ab2ec2":"code","6fef2a9b":"code","88f218bc":"code","556c4a0f":"code","9dd0c1a3":"code","52066b25":"code","eabcc8a0":"markdown","d6c499f4":"markdown","37c14616":"markdown","09b6751b":"markdown","85dd2156":"markdown","0fa6f2d1":"markdown","e11ab6bf":"markdown","f1ecf140":"markdown","feec1a5d":"markdown","6f57b994":"markdown","fb4885f1":"markdown","896b7b0a":"markdown","a9be772a":"markdown"},"source":{"1bc19447":"# datatable installation with internet\n!pip install datatable==0.11.0 > \/dev\/null\n\nimport numpy as np \nimport pandas as pd \nimport datatable as dt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","891cf09f":"%%time\ntrain = pd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\")","ebc16a15":"train.info()","999a5c7b":"# writing dataset as pickle\ntrain.to_pickle(\"jane_street_train.pkl.gzip\")\n\n# writing dataset as feather\ntrain.to_feather(\"jane_street_train.feather\")\n\n# writing dataset as parquet\ntrain.to_parquet(\"jane_street_train.parquet\")\n\n# writing dataset as jay\ndt.Frame(train).to_jay(\"jane_street_train.jay\")\n\n# writing dataset as hdf5\ntrain.to_hdf(\"jane_street_train.h5\", \"jane_street_train\")","2a352103":"%%time\ntrain_pickle = pd.read_pickle(\".\/jane_street_train.pkl.gzip\")","5e585c4f":"train_pickle.info()","c2ab2ec2":"%%time\ntrain_feather = pd.read_feather(\".\/jane_street_train.feather\")","6fef2a9b":"train_feather.info()","88f218bc":"%%time\ntrain_parquet = pd.read_parquet(\".\/jane_street_train.parquet\")","556c4a0f":"train_parquet.info()","9dd0c1a3":"%%time\ntrain_jay = dt.fread(\".\/jane_street_train.jay\")","52066b25":"train_jay.shape","eabcc8a0":"### 2.2 Feather","d6c499f4":"hdf5 641\u00b5s and CSV 1min 35s.","37c14616":"# Train.csv transforming!\n\nAs we can see, the fille train.csv of the competition has 138 features and 2390490 instances.  \nJust to read it with pandas.read_csv() it takes about 1min 43s and read it everytime we start the notebook would take too much time in the long run.  \n\n**As [Vopani](https:\/\/www.kaggle.com\/rohanrao) did in the RIIID competition and helped me a lot, I'm convertin this data to formats that are faster to read and therefore better to read!**\n\n#### You may find the train data in all this formats in this dataset: https:\/\/www.kaggle.com\/pedrocouto39\/jane-street-market-train-data-best-formats","09b6751b":"# 3. Conclusion\nThough the original train.csv didn't take an extreme long time be read, we can improve it a lot by using other formats as above and that is why I will share this data in dataset to make it availible to anyone in the competition. \nLet's save time focus on where it is more import: data exploration and building models!\n\nJust in order to compare the reading time:\n- CSV: 1min 42s\n- Pickle: 4.45s\n- Feather: 4.35s\n- Parquet: 8.31s\n- Jay: 8.12ms or 0.0812s (blazing fast!)","85dd2156":"### 2.4 Jay","0fa6f2d1":"Pickle 4.45s and CSV 1min 43s.","e11ab6bf":"### 2.3 Parquet","f1ecf140":"Happy Competition!!","feec1a5d":"Feather 4.35s and CSV 1min 43s.","6f57b994":"### 2.1 Pickle","fb4885f1":"# 1. Converting to multiple formatas\nThe formats that will be created are:\n1. *Pickle* - great for object serialization and though it has a slower performance when comparing with other formats, it may work for our porpuse.\n2. *Feather* - is a fast, lightweight, and easy-to-use binary file format for storing data frames.\n3. *Parquet* - compared to a traditional approach where data is stored in row-oriented approach, parquet is more efficient in terms of storage and performance.\n4. *Jay* - also a binary format, that means it is fast, lightweight, and easy-to-use binary file format for storing data frames.","896b7b0a":"# 2. Reading and timing\nNow let's read each file and time them to see how long it will take for each one.","a9be772a":"hdf5 8.75s and CSV 1min 35s."}}