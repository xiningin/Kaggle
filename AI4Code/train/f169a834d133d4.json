{"cell_type":{"f6cc950a":"code","7dd982e1":"code","0e463b19":"code","a5258978":"code","ffbf6c33":"code","f8a088d9":"code","d9904167":"code","7cf44826":"code","5b037bbf":"code","d0a9e678":"code","fb60b2d2":"code","5349ec6d":"code","6b4f11f6":"code","220a3eef":"code","382957c1":"code","0cc44535":"code","e5044c78":"code","394311fd":"code","4698d2c2":"code","b5e6293b":"code","a6c7e5d1":"code","16be4bdb":"code","042a105c":"code","ccdf75dd":"code","428cabf8":"code","f6f8680d":"code","31a1f889":"code","c6aad3bd":"code","cd76e64e":"code","7e0fa3e2":"code","7e230e38":"code","3e334181":"code","4256997f":"code","d2878219":"code","aa7c2080":"code","cb93c1c3":"code","14609a1f":"code","ff833501":"code","3cc47227":"code","0046625a":"code","2fdd5d7a":"code","14824a1a":"code","250cff27":"code","a81094c2":"code","72c51848":"code","567e4c71":"code","35a709e3":"code","223368ec":"code","98d51105":"code","d906a554":"code","5a027add":"code","680bb8b2":"code","73c57afb":"code","bf836714":"code","8bbbd4dc":"code","318c58f8":"code","ecd4e7be":"code","1000137f":"code","f3ce4124":"code","3de31f75":"code","65a19f18":"code","1cba79a3":"code","3accff18":"code","60d54dab":"code","d2d724e6":"code","0c1ad4f4":"code","a7cddeed":"code","32e086e5":"code","043a7584":"code","7f0207cc":"code","434eea0e":"code","b15a899c":"code","d820bd29":"code","7de5326a":"code","715b7951":"markdown","e79ce568":"markdown","a43c6186":"markdown","f596d480":"markdown","ac24534b":"markdown","4c375b28":"markdown","5cd0f0a3":"markdown","374f77e0":"markdown","b7661628":"markdown","0071b28c":"markdown","5d141898":"markdown","b87d4a3f":"markdown","ab719212":"markdown","bc7b7f6a":"markdown","923b98ac":"markdown","13f4397b":"markdown","4867790d":"markdown","235d09ff":"markdown","2ca2043e":"markdown","85a7667c":"markdown","2bb81177":"markdown","bbd87c8c":"markdown","3db10164":"markdown","d5432d53":"markdown","27ed0e9f":"markdown","894f1f5c":"markdown","765980d1":"markdown","d40e2449":"markdown","a4c5bbc6":"markdown","484d3f2a":"markdown","0000363d":"markdown","20d3e973":"markdown","e42da15b":"markdown","b9efe15c":"markdown","cd84455d":"markdown","bf877f7f":"markdown","46108210":"markdown","9aed3821":"markdown","44960213":"markdown","c9473b95":"markdown","7cfc9b63":"markdown","485f384b":"markdown","7d221621":"markdown","30f614cf":"markdown","78d22177":"markdown","4d6f1182":"markdown","f24e061f":"markdown","99f32555":"markdown","972030c0":"markdown","420f59ae":"markdown","a87af794":"markdown","2e5feb4e":"markdown","e444f5bf":"markdown","e7b1e783":"markdown","fdb86e1a":"markdown"},"source":{"f6cc950a":"# to handle datasets\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nfrom scipy import stats\n# for text \/ string processing\nimport re\n \n# for plotting\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \n# to divide train and test set\nfrom sklearn.model_selection import train_test_split\n \n# feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n \n# for tree binarisation\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n \n \n# to build the models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n \n# to evaluate the models\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics\n \npd.pandas.set_option('display.max_columns', None)\n \nimport warnings\nwarnings.filterwarnings('ignore')","7dd982e1":"train=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission=pd.read_csv('..\/input\/titanic\/gender_submission.csv')","0e463b19":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","a5258978":"resumetable(train)","ffbf6c33":"# find categorical variables\ncategorical = [var for var in train.columns if train[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))\n\n# find numerical variables\nnumerical = [var for var in train.columns if train[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))","f8a088d9":"# view of categorical variables\ntrain[categorical].head()","d9904167":"# view of numerical variables\ntrain[numerical].head()","7cf44826":"# let's visualise the values of the discrete variables\nfor var in ['Pclass',  'SibSp', 'Parch']:\n    print(var, ' values: ', train[var].unique())","5b037bbf":"# let's visualise the percentage of missing values\ntrain.isnull().mean()","d0a9e678":"numerical = [var for var in numerical if var not in['Survived', 'PassengerId']]\nnumerical","fb60b2d2":"# let's make boxplots to visualise outliers in the continuous variables \n# Age and Fare\n \nplt.figure(figsize=(15,6))\nplt.subplot(1, 2, 1)\nfig = train.boxplot(column='Age')\nfig.set_title('')\nfig.set_ylabel('Age')\n \nplt.subplot(1, 2, 2)\nfig = train.boxplot(column='Fare')\nfig.set_title('')\nfig.set_ylabel('Fare')","5349ec6d":"# first we plot the distributions to find out if they are Gaussian or skewed.\n# Depending on the distribution, we will use the normal assumption or the interquantile\n# range to find outliers\n \nplt.figure(figsize=(15,6))\nplt.subplot(1, 2, 1)\nfig = train.Age.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Age')\n \nplt.subplot(1, 2, 2)\nfig = train.Fare.hist(bins=20)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Fare')","6b4f11f6":"# find outliers\n \n# Age\nUpper_boundary = train.Age.mean() + 3* train.Age.std()\nLower_boundary = train.Age.mean() - 3* train.Age.std()\nprint('Age outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_boundary, upperboundary=Upper_boundary))\n \n# Fare\nIQR = train.Fare.quantile(0.75) - train.Fare.quantile(0.25)\nLower_fence = train.Fare.quantile(0.25) - (IQR * 3)\nUpper_fence = train.Fare.quantile(0.75) + (IQR * 3)\nprint('Fare outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))","220a3eef":"#  outlies in discrete variables\nfor var in ['Pclass',  'SibSp', 'Parch']:\n    print(train[var].value_counts() \/ np.float(len(train)))\n    print()","382957c1":"for var in categorical:\n    print(var, ' contains ', len(train[var].unique()), ' labels')","0cc44535":"# Cabin\ntrain['Cabin_numerical'] = train.Cabin.str.extract('(\\d+)') # extracts number from string\ntrain['Cabin_numerical'] = train['Cabin_numerical'].astype('float') # parses the above variable to float type\n \ntrain['Cabin_categorical'] = train['Cabin'].str[0] # captures first letter of string (the letter of the cabin)\n \n# same for submission data\ntest['Cabin_numerical'] = test.Cabin.str.extract('(\\d+)')\ntest['Cabin_numerical'] = test['Cabin_numerical'].astype('float')\n \ntest['Cabin_categorical'] = test['Cabin'].str[0]\n \ntrain[['Cabin', 'Cabin_numerical', 'Cabin_categorical']].head()","e5044c78":"\n# drop the original variable\ntrain.drop(labels='Cabin', inplace=True, axis=1)\ntest.drop(labels='Cabin', inplace=True, axis=1)","394311fd":"#  Ticket\n# extract the last bit of ticket as number\ntrain['Ticket_numerical'] = train.Ticket.apply(lambda s: s.split()[-1])\ntrain['Ticket_numerical'] = np.where(train.Ticket_numerical.str.isdigit(), train.Ticket_numerical, np.nan)\ntrain['Ticket_numerical'] = train['Ticket_numerical'].astype('float')\n \n# extract the first part of ticket as category\ntrain['Ticket_categorical'] = train.Ticket.apply(lambda s: s.split()[0])\ntrain['Ticket_categorical'] = np.where(train.Ticket_categorical.str.isdigit(), np.nan, train.Ticket_categorical)\n \n# submission\ntest['Ticket_numerical'] = test.Ticket.apply(lambda s: s.split()[-1])\ntest['Ticket_numerical'] = np.where(test.Ticket_numerical.str.isdigit(), test.Ticket_numerical, np.nan)\ntest['Ticket_numerical'] = test['Ticket_numerical'].astype('float')\n \n# extract the first part of ticket as category\ntest['Ticket_categorical'] = test.Ticket.apply(lambda s: s.split()[0])\ntest['Ticket_categorical'] = np.where(test.Ticket_categorical.str.isdigit(), np.nan, test.Ticket_categorical)\n \ntrain[['Ticket', 'Ticket_numerical', 'Ticket_categorical']].head()","4698d2c2":"# let's explore the ticket categorical part a bit further\ntrain.Ticket_categorical.unique()","b5e6293b":"# it contains several labels, some of them seem very similar apart from the punctuation\n# I will try to reduce this number of labels a bit further\n \n# remove non letter characters from string\ntext = train.Ticket_categorical.apply(lambda x: re.sub(\"[^a-zA-Z]\", '', str(x)))\n \n# to visualise the output and compare with input\npd.concat([text, train.Ticket_categorical], axis=1)","a6c7e5d1":"# set to upper case: we reduce the number of labels quite a bit\ntext = text.str.upper()\ntext.unique()","16be4bdb":"# process the variable in submission as well\ntrain['Ticket_categorical'] = text\n \ntest['Ticket_categorical'] = test.Ticket_categorical.apply(lambda x: re.sub(\"[^a-zA-Z]\", '', str(x)))\ntest['Ticket_categorical'] = test['Ticket_categorical'].str.upper()","042a105c":"# drop the original variable\ntrain.drop(labels='Ticket', inplace=True, axis=1)\ntest.drop(labels='Ticket', inplace=True, axis=1)","ccdf75dd":"def get_title(passenger):\n    # extracts the title from the name variable\n    line = passenger\n    if re.search('Mrs', line):\n        return 'Mrs'\n    elif re.search('Mr', line):\n        return 'Mr'\n    elif re.search('Miss', line):\n        return 'Miss'\n    elif re.search('Master', line):\n        return 'Master'\n    else:\n        return 'Other'\n    \ntrain['Title'] = train['Name'].apply(get_title)\ntest['Title'] = test['Name'].apply(get_title)\n \ntest[['Name', 'Title']].head()","428cabf8":"# drop the original variable\ntrain.drop(labels='Name', inplace=True, axis=1)\ntest.drop(labels='Name', inplace=True, axis=1)","f6f8680d":"# create a variable indicating family size (including the passenger)\n# sums siblings and parents\n \ntrain['Family_size'] = train['SibSp']+train['Parch']+1\ntest['Family_size'] = test['SibSp']+test['Parch']+1\n \nprint(train.Family_size.value_counts()\/ np.float(len(train)))\n \n(train.Family_size.value_counts() \/ np.float(len(train))).plot.bar()","31a1f889":"# variable indicating if passenger was a mother\ntrain['is_mother'] = np.where((train.Sex =='female')&(train.Parch>=1)&(train.Age>18),1,0)\ntest['is_mother'] = np.where((test.Sex =='female')&(test.Parch>=1)&(test.Age>18),1,0)\n \ntrain[['Sex', 'Parch', 'Age', 'is_mother']].head()","c6aad3bd":"train.loc[train.is_mother==1, ['Sex', 'Parch', 'Age', 'is_mother']].head()","cd76e64e":"print('there were {} mothers in the Titanic'.format(train.is_mother.sum()))","7e0fa3e2":"train[['Cabin_numerical', 'Ticket_numerical', 'is_mother', 'Family_size']].isnull().mean()","7e230e38":"# first we plot the distributions to find out if they are Gaussian or skewed.\n# Depending on the distribution, we will use the normal assumption or the interquantile\n# range to find outliers\n \nplt.figure(figsize=(15,6))\nplt.subplot(1, 2, 1)\nfig = train.Cabin_numerical.hist(bins=50)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Cabin number')\n \nplt.subplot(1, 2, 2)\nfig = train.Ticket_numerical.hist(bins=50)\nfig.set_ylabel('Number of passengers')\nfig.set_xlabel('Ticket number')","3e334181":"# let's visualise outliers with the boxplot and whiskers\nplt.figure(figsize=(15,6))\nplt.subplot(1, 2, 1)\nfig = train.boxplot(column='Cabin_numerical')\nfig.set_title('')\nfig.set_ylabel('Cabin number')\n \nplt.subplot(1, 2, 2)\nfig = train.boxplot(column='Ticket_numerical')\nfig.set_title('')\nfig.set_ylabel('Ticket number')","4256997f":"# Ticket numerical\nIQR = train.Ticket_numerical.quantile(0.75) - train.Ticket_numerical.quantile(0.25)\nLower_fence = train.Ticket_numerical.quantile(0.25) - (IQR * 3)\nUpper_fence = train.Ticket_numerical.quantile(0.75) + (IQR * 3)\nprint('Ticket number outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=Lower_fence, upperboundary=Upper_fence))\npassengers = len(train[train.Ticket_numerical>Upper_fence]) \/ np.float(len(train))\nprint('Number of passengers with ticket values higher than {upperboundary}: {passengers}'.format(upperboundary=Upper_fence, \\\n                                                                                                 passengers=passengers))","d2878219":"train[['Cabin_categorical', 'Ticket_categorical', 'Title']].isnull().mean()","aa7c2080":"for var in ['Cabin_categorical', 'Ticket_categorical', 'Title']:\n    print(var, ' contains ', len(train[var].unique()), ' labels')","cb93c1c3":"# rare \/ infrequent labels (less than 1% of passengers)\nfor var in ['Cabin_categorical', 'Ticket_categorical', 'Title']:\n    print(train[var].value_counts() \/ np.float(len(train)))\n    print()","14609a1f":"# Let's separate into train and test set\n \nX_train, X_test, y_train, y_test = train_test_split(train, train.Survived, test_size=0.2,\n                                                    random_state=0)\nX_train.shape, X_test.shape","ff833501":"# let's group again the variables into categorical or numerical\n# now considering the newly created variables\n \ndef find_categorical_and_numerical_variables(dataframe):\n    cat_vars = [col for col in train.columns if train[col].dtypes == 'O']\n    num_vars  = [col for col in train.columns if train[col].dtypes != 'O']\n    return cat_vars, num_vars\n                 \ncategorical, numerical = find_categorical_and_numerical_variables(train) ","3cc47227":"categorical","0046625a":"numerical = [var for var in numerical if var not in ['Survived','PassengerId']]\nnumerical","2fdd5d7a":"# print variables with missing data\nfor col in numerical:\n    if X_train[col].isnull().mean()>0:\n        print(col, X_train[col].isnull().mean())","14824a1a":"def impute_na(X_train, df, variable):\n    # make temporary df copy\n    temp = df.copy()\n    \n    # extract random from train set to fill the na\n    random_sample = X_train[variable].dropna().sample(temp[variable].isnull().sum(), random_state=0)\n    \n    # pandas needs to have the same index in order to merge datasets\n    random_sample.index = temp[temp[variable].isnull()].index\n    temp.loc[temp[variable].isnull(), variable] = random_sample\n    return temp[variable]","250cff27":"# Age and ticket\n# add variable indicating missingness\nfor df in [X_train, X_test, test]:\n    for var in ['Age', 'Ticket_numerical']:\n        df[var+'_NA'] = np.where(df[var].isnull(), 1, 0)\n    \n# replace by random sampling\nfor df in [X_train, X_test, test]:\n    for var in ['Age', 'Ticket_numerical']:\n        df[var] = impute_na(X_train, df, var)\n    \n \n# Cabin numerical\nextreme = X_train.Cabin_numerical.mean() + X_train.Cabin_numerical.std()*3\nfor df in [X_train, X_test, test]:\n    df.Cabin_numerical.fillna(extreme, inplace=True)","a81094c2":"# print variables with missing data\nfor col in categorical:\n    if X_train[col].isnull().mean()>0:\n        print(col, X_train[col].isnull().mean())","72c51848":"# add label indicating 'Missing' to Cabin categorical\n# or replace by most frequent label in Embarked\n \nfor df in [X_train, X_test, test]:\n    df['Embarked'].fillna(X_train['Embarked'].mode()[0], inplace=True)\n    df['Cabin_categorical'].fillna('Missing', inplace=True)","567e4c71":"# check absence of null values\nX_train.isnull().sum()","35a709e3":"X_test.isnull().sum()","223368ec":"# Fare in the test dataset contains one null value, I will replace it by the median \ntest.Fare.fillna(X_train.Fare.median(), inplace=True)","98d51105":"def top_code(df, variable, top):\n    return np.where(df[variable]>top, top, df[variable])\n \nfor df in [X_train, X_test, test]:\n    df['Age'] = top_code(df, 'Age', 73)\n    df['SibSp'] = top_code(df, 'SibSp', 4)\n    df['Parch'] = top_code(df, 'Parch', 2)\n    df['Family_size'] = top_code(df, 'Family_size', 7)","d906a554":"# let's check that it worked\nfor var in ['Age',  'SibSp', 'Parch', 'Family_size']:\n    print(var, ' max value: ', X_train[var].max())","5a027add":"# let's check that it worked\nfor var in ['Age',  'SibSp', 'Parch', 'Family_size']:\n    print(var, ' max value: ', test[var].max())","680bb8b2":"X_train.head()","73c57afb":"# test.Fare.isnull().sum()\ntest.Ticket_numerical.isnull().sum()","bf836714":"# find unfrequent labels in categorical variables\nfor var in categorical:\n    print(var, X_train[var].value_counts()\/np.float(len(X_train)))\n    print()","8bbbd4dc":"def rare_imputation(variable, which='rare'):    \n    # find frequent labels\n    temp = X_train.groupby([variable])[variable].count()\/np.float(len(X_train))\n    frequent_cat = [x for x in temp.loc[temp>0.01].index.values]\n    \n    # create new variables, with Rare labels imputed\n    if which=='frequent':\n        # find the most frequent category\n        mode_label = X_train.groupby(variable)[variable].count().sort_values().tail(1).index.values[0]\n        X_train[variable] = np.where(X_train[variable].isin(frequent_cat), X_train[variable], mode_label)\n        X_test[variable] = np.where(X_test[variable].isin(frequent_cat), X_test[variable], mode_label)\n        test[variable] = np.where(test[variable].isin(frequent_cat), test[variable], mode_label)\n    \n    else:\n        X_train[variable] = np.where(X_train[variable].isin(frequent_cat), X_train[variable], 'Rare')\n        X_test[variable] = np.where(X_test[variable].isin(frequent_cat), X_test[variable], 'Rare')\n        test[variable] = np.where(test[variable].isin(frequent_cat), test[variable], 'Rare')","318c58f8":"rare_imputation('Cabin_categorical', 'frequent')\nrare_imputation('Ticket_categorical', 'rare')","ecd4e7be":"# let's check that it worked\nfor var in categorical:\n    print(var, X_train[var].value_counts()\/np.float(len(X_train)))\n    print()","1000137f":"# let's check that it worked\nfor var in categorical:\n    print(var, test[var].value_counts()\/np.float(len(test)))\n    print()","f3ce4124":"for df in [X_train, X_test, test]:\n    df['Sex']  = pd.get_dummies(df.Sex, drop_first=True)","3de31f75":"print(X_train.Sex.unique())\nprint(X_test.Sex.unique())\nprint(test.Sex.unique())","65a19f18":"def encode_categorical_variables(var, target):\n        # make label to risk dictionary\n        ordered_labels = X_train.groupby([var])[target].mean().to_dict()\n        \n        # encode variables\n        X_train[var] = X_train[var].map(ordered_labels)\n        X_test[var] = X_test[var].map(ordered_labels)\n        test[var] = test[var].map(ordered_labels)\n \n# enccode labels in categorical vars\nfor var in categorical:\n    encode_categorical_variables(var, 'Survived')","1cba79a3":"variables_that_need_scaling = ['Pclass', 'Age', 'Sibsp', 'Parch', 'Cabin_numerical', 'Family_size']","3accff18":"training_vars = [var for var in X_train.columns if var not in ['PassengerId', 'Survived']]\ntraining_vars","60d54dab":"# fit scaler\nscaler = MinMaxScaler() # create an instance\nscaler.fit(X_train[training_vars])","d2d724e6":"xgb_model = xgb.XGBClassifier()\n \neval_set = [(X_test[training_vars], y_test)]\nxgb_model.fit(X_train[training_vars], y_train, eval_metric=\"auc\", eval_set=eval_set, verbose=False)\n \npred = xgb_model.predict_proba(X_train[training_vars])\nprint('xgb train roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\npred = xgb_model.predict_proba(X_test[training_vars])\nprint('xgb test roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","0c1ad4f4":"rf_model = RandomForestClassifier()\nrf_model.fit(X_train[training_vars], y_train)\n \npred = rf_model.predict_proba(X_train[training_vars])\nprint('RF train roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\npred = rf_model.predict_proba(X_test[training_vars])\nprint('RF test roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","a7cddeed":"ada_model = AdaBoostClassifier()\nada_model.fit(X_train[training_vars], y_train)\n \npred = ada_model.predict_proba(X_train[training_vars])\nprint('Adaboost train roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\npred = ada_model.predict_proba(X_test[training_vars])\nprint('Adaboost test roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","32e086e5":"logit_model = LogisticRegression()\nlogit_model.fit(scaler.transform(X_train[training_vars]), y_train)\n \npred = logit_model.predict_proba(scaler.transform(X_train[training_vars]))\nprint('Logit train roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\npred = ada_model.predict_proba(scaler.transform(X_test[training_vars]))\nprint('Logit test roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","043a7584":"pred_ls = []\nfor model in [xgb_model, rf_model, ada_model, logit_model]:\n    pred_ls.append(pd.Series(model.predict_proba(X_test[training_vars])[:,1]))\n \nfinal_pred = pd.concat(pred_ls, axis=1).mean(axis=1)\nprint('Ensemble test roc-auc: {}'.format(roc_auc_score(y_test,final_pred)))","7f0207cc":"tpr, tpr, thresholds = metrics.roc_curve(y_test, final_pred)\nthresholds","434eea0e":"accuracy_ls = []\nfor thres in thresholds:\n    y_pred = np.where(final_pred>thres,1,0)\n    accuracy_ls.append(metrics.accuracy_score(y_test, y_pred, normalize=True))\n    \naccuracy_ls = pd.concat([pd.Series(thresholds), pd.Series(accuracy_ls)],\n                        axis=1)\naccuracy_ls.columns = ['thresholds', 'accuracy']\naccuracy_ls.sort_values(by='accuracy', ascending=False, inplace=True)\naccuracy_ls.head()","b15a899c":"importance = pd.Series(rf_model.feature_importances_)\nimportance.index = training_vars\nimportance.sort_values(inplace=True, ascending=False)\nimportance.plot.bar(figsize=(12,6))","d820bd29":"importance = pd.Series(xgb_model.feature_importances_)\nimportance.index = training_vars\nimportance.sort_values(inplace=True, ascending=False)\nimportance.plot.bar(figsize=(12,6))","7de5326a":"importance = pd.Series(np.abs(logit_model.coef_.ravel()))\nimportance.index = training_vars\nimportance.sort_values(inplace=True, ascending=False)\nimportance.plot.bar(figsize=(12,6))","715b7951":"Title and Cabin are not highly cardinal, Ticket on the other hand has quite a few labels. Let's explore the percentage of passengers within each label to identify rare labels.","e79ce568":"<html><font size=5 color='red'>If you liked this kernel, please drop an UPVOTE. It motivates me to produce more quality content :)<\/font><\/html>","a43c6186":"As I was analysing the outliers at the beginning of the notebook, I was taking a note on the preprocessing that I thought would be more convenient for each one of them. The notes are summarised here:\n\nAge: top-coding (73)\n\nFare: equal frequency binning\n\nSibsp: top-coding (4)\n\nParch: top-coding (2)\n\nFamily Size: top-coding (7)\n\nTicket_number: equal frequency binning","f596d480":"**Cabin and Ticket contain both numbers and letters. We could extract the numerical part and then the non-numerical part and generate 2 variables out of them, to see if that adds value to our predictive models.**","ac24534b":"**Util Function**","4c375b28":"**Number of labels: cardinality**","5cd0f0a3":"The new variable Family size is discrete, because it is the sum of 2 discrete variables. It takes a finite number of values, and large families were rare on the Titanic. In fact, families larger than 7 people were rare, so ** I will cap family size at 7.**","374f77e0":"## Outliers","b7661628":"**Summary of data**","0071b28c":"**Separate train and test set**","5d141898":"### Types of problems within variables II","b87d4a3f":" will use equal width discretisation for this variable.","ab719212":"The scaler is now ready, we can use it in a machine learning algorithm when required. See below.","bc7b7f6a":"We should remove from the dataset Ages > 73\n\nThere are a few methods to handle outliers, one is top-coding, the other one is discretisation of variables.** I will use top-coding for Age,**","923b98ac":"**Feature importance**","13f4397b":"**Outlies in discrete variables**","4867790d":"**xgboost**","235d09ff":"# Machine Learning algorithm","2ca2043e":"**Logistic regression**","85a7667c":"Cabin_numerical, as expected contains the same amount of missing data than the original variable Cabin.\n\nTicket, also contains a small percentage of missing values. The other newly created variables do not contain missing data, as expected.","2bb81177":"**Three of the variables contain missing data,**\n\nAge (~20%),\n\nCabin (~77%)\n\n Embarked (< 1%)","bbd87c8c":"Cabin contains the rare labels G and T: replace by most frequent category\n\nTicket contains a lot of infrequent labels: replace by rare\n\nTitle does not contain rare labels\n\nBecause the number of passengers in the rare cabins is so small, grouping them into a new category called rare, will be in itself rare, and may be prone to overfitting. This, in cabin, I will replace rare labels by the most frequent category.\n\nIn ticket_categorical, on the other hand, the number of infrequent labels is high, therefore grouping them into a new label makes sense.","3db10164":"**Missing values**","d5432d53":"3 Discrete variables: Pclass, SibSp and Parch\n\n2 continuous variables: Fare and Age\n\n1 Id variable: PassengerId (it is a label for each of the passengers)\n\n1 binary: Survived (target variable).","27ed0e9f":"The variables Name, Ticket and Cabin are highly cardinal, i.e., they contain a lot of labels. In addition, those variables are not usable as such, and they require some manual preprocessing. I will do that before proceeding with the data exploration","894f1f5c":"Sex: one hot encoding\n\nRemaining variables: replace by risk probability","765980d1":"**Adaboost**","d40e2449":"# Pre-processing of mixed type of variables ","a4c5bbc6":"**New categorical variables: Missing values**","484d3f2a":"Both Age and Fare contain outliers. Let's find which valuers are the outliers","0000363d":"As expected, Cabin contains the same amount of missing data as the original Cabin variable.\n\nThe other 2 variables do not show missing data.","20d3e973":"**Pclass does not contain outliers,** as all its numbers are present in at least 20% of the passengers.\n\nSibSp This variable indicates the number of of siblings \/ spouses aboard the Titanic. Values bigger than 4, are rare. So I will cap this variable at 4 **(top coding).**\n\nParch This variable indicates the number of parents \/ children aboard the Titanic. We can see that values > 2 are rare (present in less than 1% of passengers). Thus I will cap this variable at 2 (top-coding).","e42da15b":"Age and ticket contains < 50% NA: create additional variable with NA + random sample imputation\nCabin_numerical contains > 50% NA: impute NA by value far in the distribution","b9efe15c":"Select threshold for maximum accuracy","cd84455d":"### Engineering Missing Data in categorical variables","bf877f7f":"**New categorical variables: rare labels**","46108210":"Let's look for missing data, outliers, cardinality and rare labels in the newly created variables.","9aed3821":"Embarked NA imputed by most frequent category, because NA is low\n\nCabin_categorical imputed by 'Missing', because NA is high","44960213":"Cabin_numerical does not contain outliers. Ticket_numerical seems to contain a few outliers. Let's find out more about it.","c9473b95":"**New categorical variables: cardinality**","7cfc9b63":"Age is quite Gaussian and Fare is skewed, so I will use the Gaussian assumption for Age, and the interquantile range for Fare","485f384b":"Let's calculate the percentage of passengers for each of the values that can take the discrete variables in the titanic dataset. I will call outliers, those values that are present in less than 1% of the passengers. This is exactly the same as finding rare labels in categorical variables. Discrete variables, in essence can be pre-processed \/ engineered as if they were categorical. Keep this in mind.\n","7d221621":"Engineering rare labels in categorical variables","30f614cf":"## Engineering missing values in numerical variables","78d22177":"**New numerical variables: Missing values**","4d6f1182":"**New numerical variables: Outliers**","f24e061f":"**Random_forest**","99f32555":"**The variable Name contains 891 different values, one for each of the passengers. We wouldn't be able to use this variable as is. However, we can extract some data from it, for example the title. See below.**","972030c0":"The variables Cabin and Ticket contain both numbers and letters. Let's create 2 variables for each extracting the numerical and categorical part.","420f59ae":"**There are a mixture of categorical and numerical variables. Numerical are those of type int and float. Categorical those of type object.**","a87af794":"**Encode categorical variables **","2e5feb4e":"**Outliers in Numerical variables**","e444f5bf":"## tailored preprocessing for the Titanic dataset","e7b1e783":"## Types of problems within the variables","fdb86e1a":"**Types of variables, summary:**\n\n5 categorical variables: from them 2 could be treated as mixed type of variables (numbers and strings)\n\n7 numerical variables: 3 discrete, 2 continuous, 1 Id, and 1 binary target"}}