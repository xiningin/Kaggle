{"cell_type":{"a29a508f":"code","de73d3a5":"code","b45a2c95":"code","6fdee15b":"code","1dbf8818":"code","8fdc2dbb":"code","48a89e8b":"code","c2fc2ec3":"code","e2e7fc31":"code","6ca293f1":"code","8e84fb8b":"code","9e4b3a41":"code","681701af":"code","0b5602ab":"code","76e191fb":"code","458a23c1":"code","a9b2185e":"code","93ba9a81":"code","4610cbcd":"code","ca4e75da":"code","2c551e9d":"code","0d64a3ca":"code","b25d85e2":"code","d0eaa2f2":"code","d3d0b6f3":"code","36e8f72e":"code","63f08d8b":"code","9cf46cbf":"code","9a6184f9":"markdown"},"source":{"a29a508f":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport random as rn\n\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom math import sqrt\nfrom catboost import Pool, CatBoostClassifier,CatBoostRegressor\nfrom sklearn.metrics import roc_auc_score\nimport tqdm\nfrom tqdm import tqdm_notebook,tqdm\nfrom sklearn.utils import shuffle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\npath=\"..\/input\/\"\nos.listdir(\"..\/input\/\")","de73d3a5":"train_data=pd.read_csv(path+\"train.csv\")\ntest_data=pd.read_csv(path+\"test.csv\")","b45a2c95":"features = [x for x in train_data.columns if x not in ['ID_code', \"target\"]]\nlabel = \"target\"","6fdee15b":" df_test = test_data.values\n\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in tqdm_notebook(range(df_test.shape[1])):\n    if feature in [0]:\n        print('ok')\n        continue\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]","1dbf8818":"idx_ = list(real_samples_indexes)","8fdc2dbb":"data=pd.concat([train_data,test_data.loc[idx_]])","48a89e8b":"\nfor col in tqdm_notebook(features):\n   \n    count=data[col].value_counts()\n    rank=len(data[col].unique())\n    train_data[\"rank_\"+col]=train_data[col].map(count)\n    test_data[\"rank_\"+col]=test_data[col].map(count)\n     ","c2fc2ec3":"import seaborn as sns\n%matplotlib inline\nsns.distplot(train_data['rank_var_188'],color=\"red\")\nsns.distplot(test_data['rank_var_188'],color=\"blue\")","e2e7fc31":"for col in tqdm_notebook(features):\n   \n    unique_value = train_data['rank_'+col].unique().tolist()+test_data['rank_'+col].unique().tolist()\n    uq_v = np.min(sorted(list(set(unique_value))))\n    train_data['cut_'+col] = train_data[col]\n    test_data['cut_'+col] = test_data[col]\n    median1=data[col].mean()\n#     median2=data[col].mean()\n\n    train_data['cut_'+col][(train_data['rank_'+col]==uq_v)]=median1\n    test_data['cut_'+col][(test_data['rank_'+col]==uq_v)]=median1","6ca293f1":"train=train_data#.join(train_clu)\ntest=test_data#.join(test_clu)\n\n","8e84fb8b":"\ndel train_data,test_data\ngc.collect()","9e4b3a41":"features = [x for x in train.columns if x not in ['ID_code', \"target\"]+['o-c_var_'+str(i) for i in range(200)]]\nlabel = \"target\"","681701af":"gc.collect()","0b5602ab":"features = [\"cut_var_\"+str(i) for i in range(200)]+[\"rank_var_\"+str(i) for i in range(200)]","76e191fb":"\"feature count:\",len(features)","458a23c1":"import lightgbm as lgb\ndef run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n    folds=5\n    \n    kf = StratifiedKFold(n_splits=folds, random_state=99999, shuffle=True)\n    fold_splits = kf.split(train, target)\n    \n    cv_scores = []\n    pred_full_test = 0\n    \n    pred_train = np.zeros((train.shape[0], folds))\n    feature_importance_df = pd.DataFrame()\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print( label + ' | FOLD ' + str(i) + '\/'+str(folds))\n        if isinstance(train, pd.DataFrame):\n            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        else:\n            dev_X, val_X = train[dev_index], train[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        params2 = params.copy()\n        pred_val_y, pred_test_y, importances = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n        gc.collect()\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n       \n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n          \n           \n            print(label + ' cv score {}: AUC {} '.format(i, cv_score))\n            print(\"##\"*40)\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df['feature'] =train.columns.values\n        fold_importance_df['importance'] =importances\n        fold_importance_df['fold'] = i\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n        i += 1\n#     print('{} cv RMSE scores : {}'.format(label, cv_scores))\n    print('{} cv mean AUC score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std AUC score : {}'.format(label, np.std(cv_scores)))\n   \n\n    \n    pred_full_test = pred_full_test \/ float(folds)\n    results = {'label': label,\n               'train': pred_train, 'test': pred_full_test,\n                'cv': cv_scores, \n               'importance': feature_importance_df,\n               }\n    return results\n\nparams = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.4,\n        'boost_from_average': 'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.04,\n        'learning_rate': 0.0083,\n#         'max_depth': 3,\n        'metric': 'auc',\n        'min_data_in_leaf': 80,\n        'min_sum_hessian_in_leaf': 10.0,\n        'num_leaves': 13,\n        'num_threads': -1,\n        'tree_learner': 'serial',\n        'objective': 'binary',\n        # 'device_type':'gpu',\n        # 'is_unbalance':True,\n        'verbosity': -1,\n     'verbose_eval': 4000,\n         \n          'num_rounds': 1000000,\n     'early_stop': 4000,\n    }\n\ndef runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n#     print('Prep LGB')\n    d_train = lgb.Dataset(train_X, label=train_y)\n    d_valid = lgb.Dataset(test_X, label=test_y)\n    watchlist = [d_train, d_valid]\n#     print('Train LGB')\n    num_rounds = params.pop('num_rounds')\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    print('Predict 1\/2')\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n   \n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    gc.collect()\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance()\ndef runCAT(train_X, train_y, test_X, test_y, test_X2, params):\n#     print('Prep LGB')\n    d_train = Pool(train_X, label=train_y)\n    d_valid = Pool(test_X, label=test_y)\n    watchlist = (d_train, d_valid)\n#     print('Train LGB')\n    num_rounds = params.pop('num_rounds')\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    model = CatBoostClassifier(iterations=num_rounds, \n        learning_rate = 0.003,\n        od_type='Iter',\n         od_wait=early_stop,\n        loss_function=\"Logloss\",\n        eval_metric='AUC',\n#         depth=3,\n        bagging_temperature=0.7,                   \n        random_seed = 2019,\n#         task_type='GPU'\n                          )\n    model.fit(d_train,eval_set=d_valid,\n            use_best_model=True,\n            verbose=verbose_eval\n                         )\n    \n    print('Predict 1\/2')\n    pred_test_y = model.predict_proba(test_X)[:,1]\n    \n    pred_test_y2 = model.predict_proba(test_X2)[:,1]\n    gc.collect()\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), 0\n \nresults = run_cv_model(train[features], test[features], train[label], runLGB, params, roc_auc_score, 'LGB')\n","a9b2185e":"imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\nimp=imports.sort_values('importance', ascending=False)\nimp.index=range(len(imp))\nimp.iloc[:610]","93ba9a81":"train_predictions = [r[0] for r in results['train']]\nroc_auc_score(train[label],train_predictions)\n","4610cbcd":"train_predictions = [r[0] for r in results['train']]\ntest_predictions = [r[0] for r in results['test']]\nsub=test[['ID_code']]\nsub['target']=test_predictions\nif not os.path.exists(\"submit\"):\n    os.mkdir(\"submit\")\nsub.to_csv(\"submit\/lgb_submmision.csv\",index=False)","ca4e75da":"train_oof=train[['ID_code']]\ntrain_oof['target']=train_predictions \noof=pd.concat([train_oof,sub])\nif not os.path.exists(\"oof\"):\n    os.mkdir(\"oof\")\noof.to_csv(\".\/oof\/lgb_oof.csv\",index=False)","2c551e9d":"# https:\/\/stackoverflow.com\/questions\/50554272\/randomly-shuffle-items-in-each-row-of-numpy-array\ndef disarrange(a, axis=-1):\n    \"\"\"\n    Shuffle `a` in-place along the given axis.\n\n    Apply numpy.random.shuffle to the given axis of `a`.\n    Each one-dimensional slice is shuffled independently.\n    \"\"\"\n    b = a.swapaxes(axis, -1)\n    # Shuffle `b` in-place along the last axis.  `b` is a view of `a`,\n    # so `a` is shuffled in place, too.\n    shp = b.shape[:-1]\n    for ndx in np.ndindex(shp):\n        np.random.shuffle(b[ndx])\n    return\n\ndef augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        disarrange(x1,axis=0)\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        disarrange(x1,axis=0)\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y\n","0d64a3ca":"# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=99999)\n# oof = train[['ID_code']]\n# oof['target'] = 0\n# predictions = test[['ID_code']]\n# val_aucs = []\n# feature_importance_df = pd.DataFrame()","b25d85e2":"# gc.collect()","d0eaa2f2":"# import lightgbm as lgb\n# # lgb_params  = {\n# #         'bagging_freq': 5,\n# #         'bagging_fraction': 0.4,\n# #         'boost_from_average': 'false',\n# #         'boost': 'gbdt',\n# #         'feature_fraction': 0.05,\n# #         'learning_rate': 0.01,\n# #         'max_depth': -1,\n# #         'metric': 'auc',\n# #         'min_data_in_leaf': 80,\n# #         'min_sum_hessian_in_leaf': 10.0,\n# #         'num_leaves': 13,\n# #         'num_threads': -1,\n# #         'tree_learner': 'serial',\n# #         'objective': 'binary',\n# #         # 'device_type':'gpu',\n# #         # 'is_unbalance':True,\n# #         'verbosity': -1,\n\n# #           \"random_state\":1017,\n# #     }\n# lgb_params  = {\n#         'bagging_freq': 5,\n#         'bagging_fraction': 0.335,\n#         'boost_from_average': 'false',\n#         'boost': 'gbdt',\n#         'feature_fraction': 0.043,\n#         'learning_rate': 0.0083,\n#         'max_depth': -1,\n#         'metric': 'auc',\n#         'min_data_in_leaf': 80,\n#         'min_sum_hessian_in_leaf': 10.0,\n#         'num_leaves': 13,\n#         'num_threads': -1,\n#         'tree_learner': 'serial',\n#         'objective': 'binary',\n#         # 'device_type':'gpu',\n#         # 'is_unbalance':True,\n#         'verbosity': -1,\n\n        \n#     }","d3d0b6f3":"# for fold, (trn_idx, val_idx) in enumerate(skf.split(train, train['target'])):\n#     X_train, y_train = train.iloc[trn_idx][features], train.iloc[trn_idx]['target']\n#     X_valid, y_valid = train.iloc[val_idx][features], train.iloc[val_idx]['target']\n    \n#     N = 5\n#     p_valid,yp = 0,0\n#     for i in range(N):\n#         print(\"##\"*40)\n#         print(\"FOLD {} N {} \".format(fold+1,i+1))\n#         X_t, y_t = augment(X_train.values, y_train.values)\n#         X_t = pd.DataFrame(X_t)\n#         X_t.columns = features\n    \n#         trn_data = lgb.Dataset(X_t, label=y_t)\n#         val_data = lgb.Dataset(X_valid, label=y_valid)\n#         evals_result = {}\n#         lgb_clf = lgb.train(lgb_params,\n#                         trn_data,\n#                         10000000,\n#                         valid_sets = [trn_data, val_data],\n#                         early_stopping_rounds=4000,\n#                         verbose_eval = 4000,\n#                         evals_result=evals_result\n#                        )\n#         p_valid += lgb_clf.predict(X_valid,num_iteration=lgb_clf.best_iteration)\n#         yp += lgb_clf.predict(test[features],num_iteration=lgb_clf.best_iteration)\n#         gc.collect()\n        \n#     gc.collect()\n#     oof['target'][val_idx] = p_valid\/N\n#     val_score = roc_auc_score(y_valid, p_valid)\n#     print(\"fold {}|\".format(fold+1),\"auc score : \",val_score)\n#     val_aucs.append(val_score)\n    \n#     predictions['fold{}'.format(fold+1)] = yp\/N","36e8f72e":"# mean_auc = np.mean(val_aucs)\n# std_auc = np.std(val_aucs)\n# all_auc = roc_auc_score(train['target'], oof['target'])\n# print(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))","63f08d8b":"# if not os.path.exists(\"augment\"):\n#     os.mkdir(\"augment\")\n# predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\n# predictions.to_csv('.\/augment\/lgb_all_predictions.csv', index=None)\n# sub_df = pd.DataFrame({\"ID_code\":test[\"ID_code\"].values})\n# sub_df[\"target\"] = predictions['target']\n# sub_df.to_csv(\".\/augment\/lgb_submission.csv\", index=False)\n# oof_all=pd.concat([oof,sub_df])\n# oof_all.to_csv('.\/augment\/augment_lgb_oof.csv', index=False)\n\n","9cf46cbf":"# with open(\"log.txt\",\"w\")as f:\n#     f.write(\"auc oof:{} mean:{}\".format(roc_auc_score(train['target'], oof['target']),np.mean(val_aucs)))","9a6184f9":"### augment"}}