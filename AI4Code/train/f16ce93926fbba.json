{"cell_type":{"ccfccdd3":"code","4727eb53":"code","0fcb1031":"code","ffc7b5e0":"code","c66cbca6":"code","b8c9d255":"code","3740ad61":"code","e49e702e":"code","e87519d9":"code","ae6a8077":"code","12ee61c1":"code","243fcf2a":"code","544b3b22":"code","51d572ab":"code","84636dfe":"code","17227d7c":"code","4d6bb9a6":"code","1855efd1":"code","e3f10da5":"code","a812cfaa":"code","90f5c103":"code","00bf4f3c":"code","51eda20d":"code","1cdace04":"code","4d632d94":"code","1e243f90":"code","4a273ac3":"code","fb1586de":"code","7416cc72":"code","75f6f050":"code","49abee15":"code","cbe83372":"code","40a9c2b8":"code","8092b2b9":"code","e1130c24":"code","1d6eb879":"code","cb3cc4df":"code","0625b810":"code","50344e85":"code","ac80b6e7":"code","666f1c36":"code","e0bb9505":"code","ad0bc389":"code","476f86a3":"code","99ab3f55":"code","f685f7ec":"code","3057446f":"code","fb39115f":"code","850f1887":"code","2f8053f3":"code","ea2a61b7":"code","86d4af65":"code","a54908d4":"code","58d63f7b":"code","a034e2f9":"code","696bf321":"code","54e401e4":"code","8431649f":"code","d36e9a38":"code","e222cb89":"code","5282a7b6":"code","a0dd7741":"code","7337c0c5":"code","e5e591d3":"markdown","22598d35":"markdown"},"source":{"ccfccdd3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4727eb53":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0fcb1031":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","ffc7b5e0":"print(\"training set dimensions: \",train.shape)\nprint(\"test set dimensions: \",test.shape)","c66cbca6":"train.head()","b8c9d255":"train.dtypes","3740ad61":"train.info()","e49e702e":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap=\"Blues_r\")","e87519d9":"categorical_cols= train.select_dtypes(include=['object'])\nprint(f'The dataset contains {len(categorical_cols.columns.tolist())} categorical columns')\nfor cols in categorical_cols.columns:\n    print(cols,':', len(categorical_cols[cols].unique()),'labels')","ae6a8077":"import plotly.graph_objects as go\n\nnight_colors = ['#D3DBDD',  'navy',  '#57A7F3']\nlabels = [x for x in train.Embarked.value_counts().index]\nvalues = train.Embarked.value_counts()\n\n# Use `hole` to create a donut-like pie chart\nfig=go.Figure(data=[go.Pie(labels=[\"Southampton\",\"Cherbourg\",\"Queenstown\"],values=values,hole=.3,pull=[0,0,0.06,0])])\n\nfig.update_layout(\n    title_text=\"Port of embarkation\")\nfig.update_traces(marker=dict(colors=night_colors))\nfig.show()","12ee61c1":"labels = [x for x in train.Sex.value_counts().index]\nvalues = train.Sex.value_counts()\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3,pull=[0.03, 0])])\n\nfig.update_layout(\n    title_text=\"Gender \")\nfig.update_traces(marker=dict(colors=night_colors))\nfig.show()\n","243fcf2a":"import plotly.figure_factory as ff\nfrom plotly.offline import iplot\nage=train['Age'].dropna()\nfig = ff.create_distplot([age],['Age'],bin_size=1,)\nfig.update_traces(marker=dict(color='#57A7F3'))\nfig.update_layout(\n    title=\"Age Distribution\",\n)\niplot(fig, filename='Basic Distplot')","544b3b22":"fig = ff.create_distplot([train['Fare']],['Fare'],bin_size=10)\nfig.update_traces(marker=dict(color='#57A7F3'))\nfig.update_layout(\n    title=\"Fare Distribution\",\n)\niplot(fig, filename='Basic Distplot')","51d572ab":"labels = [x for x in train.Pclass.value_counts().index]\nvalues = train.Pclass.value_counts()\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=['class 3','class 1','class 2'], values=values, hole=.3,pull=[0,0,0.04])])\n\nfig.update_layout(\n    title_text=\"Ticket class \")\nfig.update_traces(marker=dict(colors=night_colors))\nfig.show()","84636dfe":"import plotly.offline as py\ndef PieChart(column, title, limit):\n    male = train[train['Sex'] == 'male']\n    female = train[train['Sex'] == 'female']\n    count_male = male[column].value_counts()[:limit].reset_index()\n    count_female = female[column].value_counts()[:limit].reset_index()\n    color = ['#D3DBDD','navy','#57A7F3', 'lightgrey','orange', 'gold','lightgreen', \n                            '#D0F9B1','tomato', 'tan']\n    \n    trace1 = go.Pie(labels=count_male['index'], \n                    values=count_male[column], \n                    name= \"male\", \n                    hole= .4, \n                    domain= {'x': [0, .48]},\n                   marker=dict(colors=color))\n\n    trace2 = go.Pie(labels=count_female['index'], \n                    values=count_female[column], \n                    name=\"female\", \n                    hole= .4,  \n                    domain= {'x': [.52, 1]})\n    layout = dict(title= title, font=dict(size=13), legend=dict(orientation=\"h\"),\n                  annotations = [\n                      dict(\n                          x=.21, y=.5,\n                          text='Male', \n                          showarrow=False,\n                          font=dict(size=16)\n                      ),\n                      dict(\n                          x=.80, y=.5,\n                          text='Female', \n                          showarrow=False,\n                          font=dict(size=16)\n                      )\n        ])\n\n    fig = dict(data=[trace1, trace2], layout=layout)\n    py.iplot(fig)","17227d7c":"PieChart('Embarked', \"Embarkation and gender\", 3)","4d6bb9a6":"train['Survived']=train['Survived'].astype('category')\ntrain['Pclass']=train['Pclass'].astype('category')\ntrain['SibSp']=train['SibSp'].astype('category')","1855efd1":"import plotly.express as px\n\ndf = train.groupby(by=[\"Survived\", \"Sex\"]).size().reset_index(name=\"counts\")\npx.bar(data_frame=df, x=\"Sex\", y=\"counts\", color=\"Survived\", barmode=\"group\",color_discrete_sequence =['#8FCBD8','#D3DBDD'],title=\"Survived vs Gender\")","e3f10da5":"df = train.groupby(by=[\"Survived\", \"Pclass\"]).size().reset_index(name=\"counts\")\npx.bar(data_frame=df, x=\"Pclass\", y=\"counts\", color=\"Survived\", barmode=\"group\",color_discrete_sequence =['#8FCBD8','#D3DBDD']\n      ,title=\"Survived vs Ticket class\")","a812cfaa":"df = train.groupby(by=[\"Survived\", \"Embarked\"]).size().reset_index(name=\"counts\")\npx.bar(data_frame=df, x=\"Embarked\", y=\"counts\", color=\"Survived\", barmode=\"group\",\n       color_discrete_sequence =['#D3DBDD','#8FCBD8'],title=\"Survived vs Embarkation\")","90f5c103":"df = train.groupby(by=[\"Pclass\", \"Embarked\"]).size().reset_index(name=\"counts\")\npx.bar(data_frame=df, x=\"Embarked\", y=\"counts\", color=\"Pclass\", barmode=\"group\",color_discrete_sequence =['navy','#57A7F3','#D3DBDD'],\n      title=\"Ticket class vs Embarkation\")","00bf4f3c":"px.scatter(data_frame = train\n           ,x = 'Fare'\n           ,y = 'Age'\n           ,color = 'Survived',\n           size='Parch',\n           hover_data=['Sex', 'Age'],\n           color_discrete_sequence =['navy','#57A7F3','#D3DBDD'],\n           title=\"Age vs Fare\"\n           )","51eda20d":"df = train.groupby(by=[\"Survived\", \"SibSp\"]).size().reset_index(name=\"counts\")\npx.bar(data_frame=df, x=\"SibSp\", y=\"counts\", color=\"Survived\", barmode=\"group\",color_discrete_sequence =['#D3DBDD','#8FCBD8'],\n      title=\"Survived vs SibSp\")","1cdace04":"df = train.groupby(by=[\"Survived\", \"Parch\"]).size().reset_index(name=\"counts\")\npx.bar(data_frame=df, x=\"Parch\", y=\"counts\", color=\"Survived\", barmode=\"group\",color_discrete_sequence =['#D3DBDD','#8FCBD8'],\n      title=\"Survived vs Parch\")","4d632d94":"train=train.drop(['PassengerId','Name','Ticket'],1)\ntest=test.drop(['PassengerId','Name','Ticket'],1)","1e243f90":"train['Survived']=train['Survived'].astype('int')\ntrain['Pclass']=train['Pclass'].astype('int')\ntrain['SibSp']=train['SibSp'].astype('int')","4a273ac3":"train.info()","fb1586de":"features=[features for features in train.columns if train[features].isnull().sum()>1]\nfor feature in features:\n    print(feature, np.round(train[feature].isnull().mean(), 4),  ' % missing values.\\n')","7416cc72":"train=train.drop(['Cabin'],1)\ntest=test.drop(['Cabin'],1)","75f6f050":"categorical_cols_train= train.select_dtypes(include=['object'])\n\nprint(f'The dataset contains {len(categorical_cols_train.columns.tolist())} categorical columns')","49abee15":"categorical_cols_train.describe()","cbe83372":"categorical_cols_missing = categorical_cols_train.columns[categorical_cols_train.isnull().any()]\ncategorical_cols_missing","40a9c2b8":"\nfrom sklearn.impute import SimpleImputer\ncategoricalImputer = SimpleImputer(missing_values = np.NaN,strategy = 'most_frequent')\nfor feature in categorical_cols_missing:\n     categorical_cols_train[feature] = categoricalImputer.fit_transform(categorical_cols_train[feature].values.reshape(-1,1))\n     train[feature] = categoricalImputer.fit_transform(train[feature].values.reshape(-1,1))","8092b2b9":"train['Age'].describe()","e1130c24":"train['Fare'].describe()","1d6eb879":"np.random.seed(1)\ntrain['Age'].fillna(np.random.randint(20,38), inplace = True)\ntest['Age'].fillna(np.random.randint(20,38), inplace = True)\ntest['Fare'].fillna(np.random.randint(0,31), inplace = True)","cb3cc4df":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap=\"Blues_r\")","0625b810":"train.info()","50344e85":"train.head()","ac80b6e7":"train.isnull().any()","666f1c36":"train=pd.get_dummies(train, columns=[\"Embarked\"])\ntest=pd.get_dummies(test, columns=[\"Embarked\"])","e0bb9505":"data = train[[i for i in train.columns if i not in ('Age','SibSp','Parch','Pclass','Fare',)]]","ad0bc389":"train.shape","476f86a3":"from scipy.stats import chi2_contingency\nimport numpy as np\n\ndef cramers_V(var1,var2):\n  crosstab =np.array(pd.crosstab(var1,var2, rownames=None, colnames=None)) # Cross table building\n  stat = chi2_contingency(crosstab)[0] # Keeping of the test statistic of the Chi2 test\n  obs = np.sum(crosstab) # Number of observations\n  mini = min(crosstab.shape)-1 # Take the minimum value between the columns and the rows of the cross table\n  return (stat\/(obs*mini))","99ab3f55":"rows= []\n\nfor var1 in data:\n  col = []\n  for var2 in data:\n    cramers =cramers_V(data[var1], data[var2]) # Cramer's V test\n    col.append(round(cramers,2)) # Keeping of the rounded value of the Cramer's V  \n  rows.append(col)\n  \ncramers_results = np.array(rows)\ndf = pd.DataFrame(cramers_results, columns = data.columns, index =data.columns)\ndf","f685f7ec":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(7,6))\n\nmask = np.zeros_like(df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nwith sns.axes_style(\"white\"):\n  ax = sns.heatmap(df, mask=mask,vmin=0., vmax=1, square=True, cmap=\"Blues_r\")\n  ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 9)\n  ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 9)\n  \nplt.show()","3057446f":"data1 = train[[i for i in train.columns if i in ('Age','SibSp','Parch','Pclass','Fare',)]]\nplt.figure(figsize=(8,6))\nsns.heatmap(data1.corr(),annot=True,cmap=\"Blues_r\")\nplt.show()","fb39115f":"train=train.drop(['Embarked_C'],1)\ntest=test.drop(['Embarked_C'],1)","850f1887":"train.head()","2f8053f3":"X= train.drop(['Survived'],1)\ny=train['Survived']","ea2a61b7":"\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1234)","86d4af65":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#libraries for model evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report","a54908d4":"from sklearn.linear_model import RidgeClassifier\nrc =RidgeClassifier()\nmodel0=rc.fit(x_train, y_train)\nprint(\"train accuracy:\",model0.score(x_train, y_train),\"\\n\",\"test accuracy:\",model0.score(x_test,y_test))\nrcpred = rc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for logistic regression\")\nprint(classification_report(rcpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for logistic regression\")\ndisplr = plot_confusion_matrix(rc, x_test, y_test,cmap=plt.cm.Blues , values_format='d')","58d63f7b":"#linear discriminant analysis\nlda = LinearDiscriminantAnalysis()\nmodel2=lda.fit(x_train, y_train)\nprint(\"train accuracy:\",model2.score(x_train, y_train),\"\\n\",\"test accuracy:\",model2.score(x_test,y_test))\n\nldapred = lda.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for linear discriminant analysis\")\nprint(classification_report(ldapred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for linear discriminant analysis\")\ndisplr = plot_confusion_matrix(lda, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","a034e2f9":"#decision tree classifier\ndt=DecisionTreeClassifier()\nmodel3=dt.fit(x_train, y_train)\nprint(\"train accuracy:\",model3.score(x_train, y_train),\"\\n\",\"test accuracy:\",model3.score(x_test,y_test))\n\ndtpred = dt.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for decision tree classifier\")\nprint(classification_report(dtpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for decision tree classifier\")\ndisplr = plot_confusion_matrix(dt, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","696bf321":"#random forest classifier\nrf=RandomForestClassifier(random_state=1234)\nmodel4=rf.fit(x_train, y_train)\nprint(\"train accuracy:\",model4.score(x_train, y_train),\"\\n\",\"test accuracy:\",model4.score(x_test,y_test))\n\nrfpred = rf.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for random forest classifier\")\nprint(classification_report(rfpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for random forest classifier\")\ndisplr = plot_confusion_matrix(rf, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","54e401e4":"# gradient boost classifier \ngbm=GradientBoostingClassifier()\nmodel5=gbm.fit(x_train, y_train)\nprint(\"train accuracy:\",model5.score(x_train, y_train),\"\\n\",\"test accuracy:\",model5.score(x_test,y_test))\n\ngbmpred = gbm.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for gradient boosting classifier\")\nprint(classification_report(gbmpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for gradient boosting classifier\")\ndisplr = plot_confusion_matrix(gbm, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","8431649f":"# adaboost classifier \nada=AdaBoostClassifier()\nmodel6=ada.fit(x_train, y_train)\nprint(\"train accuracy:\",model6.score(x_train, y_train),\"\\n\",\"test accuracy:\",model6.score(x_test,y_test))\n\nadapred = ada.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(adapred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\ndisplr = plot_confusion_matrix(ada, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","d36e9a38":"# extreme gradient boost classifier\nxgb = XGBClassifier(random_state=1234)\nmodel7=xgb.fit(x_train.values, y_train)\nprint(\"train accuracy:\",model7.score(x_train, y_train),\"\\n\",\"test accuracy:\",model7.score(x_test,y_test))\n\nxgbpred = xgb.predict(x_test.values)\nprint(\"\\n\")\nprint(\"classification report for extreme gradient boosting classifier\")\nprint(classification_report(xgbpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extreme gradient boosting classifier\")\ndisplr = plot_confusion_matrix(xgb, x_test.values, y_test ,cmap=plt.cm.Blues , values_format='d')","e222cb89":"# extra tree classifier\nextree = ExtraTreesClassifier()\nmodel8=extree.fit(x_train, y_train)\nprint(\"train accuracy:\",model8.score(x_train, y_train),\"\\n\",\"test accuracy:\",model8.score(x_test,y_test))\n\nextpred = extree.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extra tree classifier\")\nprint(classification_report(extpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extra tree classifier\")\ndisplr = plot_confusion_matrix(extree, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","5282a7b6":"# voting classifer\nfrom sklearn.ensemble import VotingClassifier\nclf1 = RandomForestClassifier(random_state=1234)\nclf2 = LogisticRegression(max_iter=2000,penalty='l2')\n\nvc = VotingClassifier(estimators=[('xgb', clf1),('lr', clf2)], voting='soft')\nmodel9=vc.fit(x_train, y_train)\nprint(\"train accuracy:\",model9.score(x_train, y_train),\"\\n\",\"test accuracy:\",model9.score(x_test,y_test))\n\nvcpred = vc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for voting classifier\")\nprint(classification_report(vcpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for voting classifier\")\ndisplr = plot_confusion_matrix(vc, x_test, y_test ,cmap=plt.cm.Blues, values_format='d')","a0dd7741":"# stacking classifier \nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nestimators = [('rf', RandomForestClassifier(random_state=1234)),('ext', make_pipeline(LogisticRegression(max_iter=2000,penalty='l2')))]\nsc= StackingClassifier( estimators=estimators)\n\nmodel10=sc.fit(x_train, y_train)\nprint(\"train accuracy:\",model10.score(x_train, y_train),\"\\n\",\"test accuracy:\",model10.score(x_test,y_test))\n\nscpred = sc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for voting classifier\")\nprint(classification_report(scpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for voting classifier\")\ndisplr = plot_confusion_matrix(sc, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","7337c0c5":"from catboost import CatBoostClassifier\n\ncc = CatBoostClassifier(silent=True )\nmodel11=cc.fit(x_train, y_train)\nprint(\"train accuracy:\",model11.score(x_train, y_train),\"\\n\",\"test accuracy:\",model11.score(x_test,y_test))\n\nccpred = cc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extra tree classifier\")\nprint(classification_report(ccpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extra tree classifier\")\ndisplr = plot_confusion_matrix(cc, x_test, y_test ,cmap=plt.cm.Blues , values_format='d')","e5e591d3":"data.info()","22598d35":"Introduction: This assignment will show some facts about the Titanic, the survivors, the classes, etc.\n\nObjetives: Predict the survival of passengers on Titanic using machine learning and deep learning."}}