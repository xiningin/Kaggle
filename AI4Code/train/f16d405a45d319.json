{"cell_type":{"b627512c":"code","b33b4c3e":"code","3e150d6b":"code","f6e0b019":"code","6dcddd69":"code","54c4df7c":"code","054db802":"code","87ee3324":"code","90446267":"code","da691c86":"code","74b02f84":"code","06cafb70":"code","7323173c":"code","fabe3c52":"code","ac252ed7":"code","b4f91af2":"code","da7c2efc":"code","02973ef4":"code","a65ef4ae":"code","0e4b07a5":"code","e96d7868":"code","dde0ee41":"code","1489e6c7":"code","b541f6d9":"code","9a0eb296":"code","1045abdb":"code","cfbcf5c5":"code","2a3c3c84":"code","d70688c3":"code","ee427d61":"code","e9e7a889":"code","5bce0448":"code","a99e816c":"code","d146268f":"markdown","8425377f":"markdown","a54d9eea":"markdown","0c51dbd4":"markdown","9a1479e7":"markdown","2e9c4031":"markdown","808ed16d":"markdown","0698febe":"markdown","42419f60":"markdown","f2f91cd6":"markdown","de308343":"markdown","d5990e0a":"markdown","6008c66b":"markdown","93630413":"markdown","0500a683":"markdown","312219e8":"markdown","7022ce8b":"markdown","1f036762":"markdown","d25e31a6":"markdown","68b066c1":"markdown","04456854":"markdown","b30603bc":"markdown","3b58eb3e":"markdown"},"source":{"b627512c":"import os\nimport datetime\nimport math\nimport random\nimport warnings\nimport re\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.dates as mdates\nfrom datetime import datetime\nimport networkx as nx\n\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport nltk \n\nimport spacy\nnlp=spacy.load('en')\nnlp.remove_pipe('parser')\nnlp.remove_pipe('ner')\n# nlp.remove_pipe('tagger')\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport gensim\nimport pyLDAvis\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook()\n\n#import plotly.offline as py\n#py.init_notebook_mode(connected=True)","b33b4c3e":"input_dir = '..\/input' \nfor f in  os.listdir(input_dir):\n    print(f)","3e150d6b":"df_professionals = pd.read_csv(\"..\/input\/professionals.csv\",)\ndf_groups = pd.read_csv(\"..\/input\/groups.csv\")\ndf_comments = pd.read_csv(\"..\/input\/comments.csv\")\ndf_school_memberships = pd.read_csv(\"..\/input\/school_memberships.csv\")\ndf_tags = pd.read_csv(\"..\/input\/tags.csv\")\ndf_emails = pd.read_csv(\"..\/input\/emails.csv\")\ndf_group_memberships = pd.read_csv(\"..\/input\/group_memberships.csv\")\ndf_answers = pd.read_csv(\"..\/input\/answers.csv\")\ndf_answers_scores=pd.read_csv(\"..\/input\/answer_scores.csv\")\ndf_students = pd.read_csv(\"..\/input\/students.csv\")#,index_col = \"students_id\", parse_dates = ['students_date_joined'])\ndf_matches = pd.read_csv(\"..\/input\/matches.csv\")\ndf_questions = pd.read_csv(\"..\/input\/questions.csv\")\ndf_questions_score = pd.read_csv(\"..\/input\/question_scores.csv\")\n\ndf_tag_users = pd.read_csv(\"..\/input\/tag_users.csv\")\ndf_tag_questions = pd.read_csv(\"..\/input\/tag_questions.csv\")","f6e0b019":"warnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth',-1)\n\nSEED=2019\nrandom.seed(SEED)\nnp.random.seed(SEED)","6dcddd69":"# Spacy Tokenfilter for part-of-speech tagging\ntoken_pos = ['NOUN', 'VERB', 'PROPN', 'ADJ', 'INTJ', 'X']\n\n# The data export was from 1. February 2019. For Production use datetime.now()\nactual_date = datetime(2019, 2 ,1)","54c4df7c":"def nlp_preporocessing(data):\n    \"\"\"Use NLP to transform the text corpus to cleaned sentences and word tokens\n\n        param data: List with sentences, which should be processed.\n        return processed_tokens: List with the cleaned and tokenized sentences\n    \"\"\"\n    def token_filter(token):\n        \"\"\" Keep tokens who are alphabatic, in the POS(Part-Of-Speach) list and not in stop list\"\"\"\n        return not token.is_stop and token.is_alpha and token.pos_ in token_pos\n    \n    data = [re.compile(r'<[^>]+>').sub('', x) for x in data] #Remove HTML-tags    \n    processed_tokens=[]\n    data_pipe=nlp.pipe(data)\n    for doc in data_pipe:\n        filtered_tokens=[token.lemma_.lower() for token in doc if token_filter(token)]\n        processed_tokens.append(filtered_tokens)\n    return processed_tokens\n    ","054db802":"# Transform datetime datatypes\ndf_questions.questions_date_added=pd.to_datetime(df_questions.questions_date_added, infer_datetime_format=True)\ndf_answers.answers_date_added=pd.to_datetime(df_answers.answers_date_added, infer_datetime_format=True)\ndf_professionals.professionals_date_joined=pd.to_datetime(df_professionals.professionals_date_joined, infer_datetime_format=True)\ndf_students.students_date_joined=pd.to_datetime(df_students.students_date_joined, infer_datetime_format=True)\ndf_emails.emails_date_sent=pd.to_datetime(df_emails.emails_date_sent, infer_datetime_format=True)\ndf_comments.comments_date_added=pd.to_datetime(df_comments.comments_date_added, infer_datetime_format=True)","87ee3324":"df_emails.dtypes","90446267":"# Merge Question Title and Body for creating full text\ndf_questions['questions_full_text'] = df_questions['questions_title'] +'\\r\\n\\r\\n'+ df_questions['questions_body']\n\n# Count of answers\ndf_temp = df_answers.groupby(df_answers.answers_question_id).size()\ndf_questions['questions_answers_count'] = pd.merge(df_questions, pd.DataFrame(df_temp.rename('count')), left_on='questions_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n\n# First answer for questions\ndf_temp = df_answers[['answers_question_id', 'answers_date_added']].groupby('answers_question_id').min()\n\ndf_questions['questions_first_answers'] = pd.merge(df_questions, pd.DataFrame(df_temp), left_on='questions_id', right_index=True, how='left')['answers_date_added']\n\n# Last answer fo any questions\ndf_temp = df_answers[['answers_question_id', 'answers_date_added']].groupby('answers_question_id').max()\ndf_questions['questions_last_answers'] = pd.merge(df_questions, pd.DataFrame(df_temp), left_on='questions_id', right_index=True, how='left')['answers_date_added']\n\n# Hearts score of questions\ndf_temp=pd.merge(df_questions, df_questions_score,left_on='questions_id', right_on='id',how='left')\ndf_questions['questions_hearts']=df_temp['score'].fillna(0).astype(int)\n\n# # Questions Tags list\ndf_temp=pd.merge(df_questions, df_tag_questions, left_on='questions_id', right_on='tag_questions_question_id', how='inner')\ndf_temp=pd.merge(df_temp, df_tags,left_on='tag_questions_tag_id', right_on='tags_tag_id', how='inner')\ndf_temp=(df_temp.groupby('questions_id')['tags_tag_name'].apply(list).rename('questions_tags')).to_frame()\n\ndf_questions['questions_tags']=pd.merge(df_questions,df_temp, left_on='questions_id', right_index=True, how='left')['questions_tags']\n\n# Get NLP Token\ndf_questions['nlp_tokens'] = nlp_preporocessing(df_questions['questions_full_text'])\n","da691c86":"### Answers\n# Days required to answer the question\ndf_temp = pd.merge(df_questions, df_answers, left_on='questions_id', right_on='answers_question_id')\ndf_answers['time_delta_answer'] = (df_temp.answers_date_added - df_temp.questions_date_added) \n\n# Ranking for answers time\ndf_answers['answer_time_rank']=df_answers.groupby('answers_question_id')['time_delta_answer'].rank(method='min').astype(int)\n\n# Hearts Score\ndf_temp = pd.merge(df_answers, df_answers_scores, left_on='answers_id', right_on='id', how='left')\ndf_answers['answers_hearts'] = df_temp['score'].fillna(0).astype(int)","74b02f84":"# Time since Professional Join the Careervillage\ndf_professionals['professionals_time_delta_joined']=actual_date-df_professionals.professionals_date_joined\n\n# Number of answers given by professional\ndf_temp=df_answers.groupby('answers_author_id').size()\ndf_professionals['professionals_answers_count']=pd.merge(df_professionals, pd.DataFrame(df_temp.rename('count')),left_on='professionals_id',right_index=True, how='left')['count'].fillna(0).astype(int)\n\n#Number of comments given by professional\ndf_temp=df_comments.groupby('comments_author_id').size()\ndf_professionals['professionals_comments_count']=pd.merge(df_professionals, pd.DataFrame(df_temp.rename('count')),left_on='professionals_id',right_index=True, how='left')['count'].fillna(0).astype(int)\n\n# Last activity (Answer)\ndf_temp=pd.DataFrame(df_answers.groupby('answers_author_id')['answers_date_added'].max().rename('last_answer'))\ndf_professionals['date_last_answer']=pd.merge(df_professionals, df_temp, left_on='professionals_id', right_index=True, how='left')['last_answer'] \n\n# First Activity (Answer)\ndf_temp=pd.DataFrame(df_answers.groupby('answers_author_id')['answers_date_added'].min().rename('first_answer'))\ndf_professionals['date_first_answer']=pd.merge(df_professionals, df_temp, left_on='professionals_id', right_index=True, how='left')['first_answer']\n\n# Last Comment Activity\ndf_temp=pd.DataFrame(df_comments.groupby('comments_author_id')['comments_date_added'].max().rename('last_comment'))\ndf_professionals['date_last_comment']=pd.merge(df_professionals, df_temp, left_on='professionals_id', right_index=True, how='left')['last_comment']\n\n# First Comment Activity\ndf_temp=pd.DataFrame(df_comments.groupby('comments_author_id')['comments_date_added'].min().rename('first_comment'))\ndf_professionals['date_first_comment']=pd.merge(df_professionals, df_temp, left_on='professionals_id', right_index=True, how='left')['first_comment']\n\n#Professional Last Activity date Comment or Answer\ndf_professionals['date_last_activity']=df_professionals[['date_last_answer','date_last_comment']].max(axis=1)\n\n#Professional first Activity date Comment or Answer\ndf_professionals['date_first_activity']=df_professionals[['date_first_answer','date_first_comment']].min(axis=1)\n\n# Total Hearts score\ndf_temp=pd.DataFrame(df_answers.groupby('answers_author_id')['answers_hearts'].sum().rename('answers_hearts'))\ndf_professionals['Professional_answers_hearts']=pd.merge(df_professionals, df_temp, left_on='professionals_id',right_index=True, how='left')['answers_hearts'].fillna(0).astype(int)\n\n# Professionals Tags to List\ndf_temp=pd.merge(df_tag_users, df_tags, left_on='tag_users_tag_id', right_on='tags_tag_id' , how='inner')\ndf_temp =df_temp[['tag_users_user_id','tags_tag_name']]\ndf_temp=df_temp.groupby('tag_users_user_id')['tags_tag_name'].apply(list).rename('professionals_tags')\ndf_professionals['professional_tags']=pd.merge(df_professionals, df_temp.to_frame(), left_on='professionals_id', right_index=True,how='left')['professionals_tags']","06cafb70":"df_professionals.head(2)","7323173c":"# Time delta since student Join the network or career \ndf_students['students_time_delta_joined']=actual_date-df_students.students_date_joined\n\n# Number of questions asked by the student\ndf_temp=pd.DataFrame(df_questions.groupby('questions_author_id').size().rename('count'))\ndf_students['students_questions_count']=(pd.merge(df_students,df_temp, left_on='students_id', right_index=True, how='left')['count'])\ndf_students['students_questions_count']=df_students['students_questions_count'].fillna(0).astype(int)\n\n# Number of Comment given by  the student\ndf_temp=pd.DataFrame(df_comments.groupby('comments_author_id').size().rename('count'))\ndf_students['students_comments_count']=(pd.merge(df_students,df_temp, left_on='students_id', right_index=True, how='left')['count'])\ndf_students['students_comments_count']=df_students['students_comments_count'].fillna(0).astype(int)\n\n# Last Question asked by the student\ndf_temp=pd.DataFrame(df_questions.groupby('questions_author_id')['questions_date_added'].max().rename('Last_asked_question'))\ndf_students['date_last_question']=pd.merge(df_students,df_temp, left_on='students_id', right_index=True, how='left')['Last_asked_question']\n\n# First Question asked by the student\ndf_temp=pd.DataFrame(df_questions.groupby('questions_author_id')['questions_date_added'].min().rename('first_asked_question'))\ndf_students['date_first_question']=pd.merge(df_students,df_temp, left_on='students_id', right_index=True, how='left')['first_asked_question']\n\n# Last Comment given by the student\ndf_temp=pd.DataFrame(df_comments.groupby('comments_author_id')['comments_date_added'].max().rename('Last_given_comments'))\ndf_students['date_last_comment']=pd.merge(df_students,df_temp, left_on='students_id', right_index=True, how='left')['Last_given_comments']\n\n# First Comment given by the student\ndf_temp=pd.DataFrame(df_comments.groupby('comments_author_id')['comments_date_added'].min().rename('first_given_comments'))\ndf_students['date_first_comment']=pd.merge(df_students,df_temp, left_on='students_id', right_index=True, how='left')['first_given_comments']\n\n\n#Student Last Activity date Comment or questions\ndf_students['date_last_activity']=df_students[['date_last_question','date_last_comment']].max(axis=1)\n\n#Student First Activity date Comment or questions\ndf_students['date_first_activity']=df_students[['date_first_question','date_first_comment']].min(axis=1)\n\n# Total Hearts score\ndf_temp=pd.DataFrame(df_questions_score.groupby('id').sum())\ndf_students['students_questions_hearts']=pd.merge(df_students,df_temp, left_on='students_id', right_index=True, how='left')['score'].fillna(0).astype(int)\n\n# Students Tags to List\ndf_temp=pd.merge(df_tag_users, df_tags, left_on='tag_users_tag_id', right_on='tags_tag_id' , how='inner')\ndf_temp =df_temp[['tag_users_user_id','tags_tag_name']]\ndf_temp= df_temp.groupby('tag_users_user_id')['tags_tag_name'].apply(list).rename('student_tags').to_frame()\ndf_students['students_tags']=pd.merge(df_students, df_temp, left_on='students_id', right_index=True,how='inner')['student_tags']","fabe3c52":"df_emails_response=pd.merge(df_emails, df_matches, left_on='emails_id', right_on='matches_email_id', how='inner')\ndf_emails_response=pd.merge(df_emails_response, df_questions,left_on='matches_question_id', right_on='questions_id', how='inner')\ndf_emails_response=pd.merge(df_emails_response, df_answers, left_on=['emails_recipient_id','matches_question_id'], right_on=['answers_author_id','answers_question_id'],how='inner')\ndf_emails_response = df_emails_response.drop(['matches_email_id', 'matches_question_id', 'answers_id', 'answers_author_id', 'answers_body', 'answers_question_id'], axis=1)\ndf_emails_response = df_emails_response.drop(['questions_author_id', 'questions_title', 'questions_body', 'questions_full_text'], axis=1)\n\n# Time taken in answer publishing \ndf_emails_response['time_delta_email_answer'] = (df_emails_response['answers_date_added'] - df_emails_response['emails_date_sent'])\n\n# Time span in between question raised and mail sent professional \ndf_emails_response['time_delta_question_email'] = (df_emails_response['emails_date_sent'] - df_emails_response['questions_date_added'])","ac252ed7":"plt_professionals=df_professionals.groupby(df_professionals.professionals_date_joined.dt.year).size()\/len(df_professionals.index)\nplt_students = df_students.groupby(df_students.students_date_joined.dt.year).size()\/len(df_students.index)\nplt_questions=df_questions.groupby(df_questions.questions_date_added.dt.year).size()\/len(df_questions.index)\nplt_answers=df_answers.groupby(df_answers.answers_date_added.dt.year).size()\/len(df_answers.index)\nplt_comments=df_comments.groupby(df_comments.comments_date_added.dt.year).size()\/len(plt_comments.index)\nplt_emails=df_emails.groupby(df_emails.emails_date_sent.dt.year).size()\/len(df_emails.index)","b4f91af2":"plt_data=pd.DataFrame({\n        'Professionals':plt_professionals,\n        'Students':plt_students,\n        'Questions': plt_questions, \n        'Answers': plt_answers,\n        'Emails': plt_emails,\n        'Comments': plt_comments\n    })\nplt_data.plot(kind='bar', figsize=(15,5))\nplt.xlabel('Year')\nplt.ylabel('Proportion')\nplt.title('Distribution over time')\nplt.show()","da7c2efc":"plt_temp=df_professionals[['professionals_location','professionals_industry','professionals_headline']].fillna('Missing')\nplt_temp=plt_temp.applymap(lambda x:x if x=='Missing' else 'Available')\nplt_professional_industry=plt_temp.groupby('professionals_industry').size()\/len(df_temp.index)\nplt_professional_location=plt_temp.groupby('professionals_location').size()\/len(df_temp.index)\nplt_professional_headline=plt_temp.groupby('professionals_headline').size()\/len(df_temp.index)\n\nplt_professionals_tags=df_tag_users['tag_users_user_id'].unique() # Unique User List\nplt_professionals_tags=df_professionals.professionals_id.apply(lambda x : 'Available' if x in plt_professionals_tags else 'Missing').rename('professionals_tags')\nplt_professionals_tags=plt_professionals_tags.groupby(plt_professionals_tags).size()\/len(plt_professionals_tags.index)\n\n\nplt_professionals_group=df_group_memberships['group_memberships_user_id'].unique() # Unique User List\nplt_professionals_group=df_professionals.professionals_id.apply(lambda x : 'Available' if x in plt_professionals_group else 'Missing').rename('professionals_group')\nplt_professionals_group=plt_professionals_group.groupby(plt_professionals_group).size()\/len(plt_professionals_group.index)\n\n\nplt_professionals_school=df_school_memberships['school_memberships_user_id'].unique() # Unique User List\nplt_professionals_school=df_professionals.professionals_id.apply(lambda x : 'Available' if x in plt_professionals_school else 'Missing').rename('professionals_school')\nplt_professionals_school=plt_professionals_school.groupby(plt_professionals_school).size()\/len(plt_professionals_school.index)\n\nplt_temp=df_professionals[['professionals_answers_count', 'professionals_comments_count']]\nplt_temp=plt_temp.applymap(lambda x : 'Available' if x>0 else 'Missing')\n\nplt_professionals_answers=plt_temp.groupby('professionals_answers_count').size()\/len(plt_temp.index)\nplt_professionals_comments=plt_temp.groupby('professionals_comments_count').size()\/len(plt_temp.index)\n\nplt_data = pd.DataFrame({'Location': plt_professionals_location,\n                        'Industry': plt_professionals_industry,\n                        'Headline': plt_professionals_headline,\n                        'Tags': plt_professionals_tags,\n                        'Groups': plt_professionals_group,\n                        'Schools': plt_professionals_school,\n                        'Answers': plt_professionals_answers,\n                        'Comments': plt_professionals_comments,})\n\nplt_data.T.plot(kind='bar', stacked=True, figsize=(15, 5))\nplt.ylabel('Proportion')\nplt.title('Missing values for professionals')\nplt.yticks(np.arange(0, 1.05, 0.1))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\nplt_data.T","02973ef4":"%time\ndf_temp=df_students[['students_location']].fillna('Missing')\ndf_temp=df_temp.applymap(lambda x:x if x =='Missing' else 'Available')\nplt_students_location=df_temp.groupby('students_location').size()\/len(df_temp.index)\nplt_students_tags=df_tag_users.tag_users_user_id.unique()\nplt_students_tags=df_students['students_id'].apply(lambda x: 'Available' if x in plt_students_tags else 'Missing').rename('students_tags')\nplt_students_tags=plt_students_tags.groupby(plt_students_tags).size()\/len(plt_students_tags.index)\n\nplt_students_group=df_group_memberships.group_memberships_user_id.unique()\nplt_students_group=df_students.students_id.apply(lambda  x: 'Available' if x in plt_students_group else 'Missing' ).rename('students_groups')\nplt_students_group=plt_students_group.groupby(plt_students_group).size()\/len(plt_students_group.index)\n\nplt_students_school = df_school_memberships.school_memberships_user_id.unique()\nplt_students_school = df_students['students_id'].apply(lambda x: 'Available' if x in plt_students_school else 'Missing').rename('students_schools')\nplt_students_school = plt_students_school.groupby(plt_students_school).size()\/len(plt_students_school.index)\n\ndf_temp=df_students[['students_questions_count','students_comments_count']]\ndf_temp=df_temp.applymap(lambda x:  'Available' if x >0 else 'Missing')\nplt_students_questions = df_temp.groupby('students_questions_count').size()\/len(df_temp.index)\nplt_students_comments = df_temp.groupby('students_comments_count').size()\/len(df_temp.index)\n\nplt_students_data = pd.DataFrame({'Location': plt_students_location,\n                        'Tags': plt_students_tags,\n                        'Groups': plt_students_group,\n                        'Schools': plt_students_school,\n                        'Answers': plt_students_questions,\n                        'Comments': plt_students_comments,})","a65ef4ae":"plt_students_data.T.plot(kind='bar', stacked=True, figsize=(15, 5))\nplt.ylabel('Proportion')\nplt.title('Missing values for students')\nplt.yticks(np.arange(0, 1.05, 0.1))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\nplt_students_data.T","0e4b07a5":"df_students_tags=df_tag_users[df_tag_users.tag_users_user_id.isin(df_students.students_id)]\ndf_students_tags= pd.merge(df_students_tags, df_tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\ndf_students_tags['user_type'] = 'student'\ndf_professionals_tags=df_tag_users[df_tag_users.tag_users_user_id.isin(df_professionals.professionals_id)]\ndf_professionals_tags= pd.merge(df_professionals_tags, df_tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\ndf_professionals_tags['user_type'] = 'professional'\n\ndf_questions_tags=df_tag_questions\ndf_questions_tags = pd.merge(df_questions_tags, df_tags, left_on='tag_questions_tag_id', right_on='tags_tag_id')\ndf_questions_tags['user_type'] = 'question'\nplt_data =pd.concat([df_students_tags, df_professionals_tags, df_questions_tags])","e96d7868":"df_students_tags.shape, df_professionals_tags.shape, df_questions_tags.shape, ","dde0ee41":"# plt_data =plt_data [['tags_tag_name','user_type']].pivot_table(index='tags_tag_name', columns='user_type',aggfunc=len, fill_value=0)\n# plt_data ['professional'] = df_tags_data['professional'] \/ df_professionals.shape[0]\n# plt_data ['student'] = plt_data ['student'] \/ df_students.shape[0]\n# plt_data ['question'] = plt_data ['question'] \/ df_questions.shape[0]\n# plt_data['sum'] = (plt_data['professional'] + plt_data['student'] + plt_data['question'])\n# # plt_data = plt_data.sort_values(by='sum', ascending=False).drop(['sum'], axis=1).head(100)\n# plt_data=plt_data.sort_values(by='sum', ascending=False).drop(['sum'], axis=1).head(100)","1489e6c7":"# plt_data\n# Bubble sort\nfig, ax = plt.subplots(facecolor='w',figsize=(15, 15))\nax.set_xlabel('Professionals')\nax.set_ylabel('Questions')\nax.set_title('Tags Matching')\nax.set_xlim([0, max(plt_data.professional)+0.001])\nax.set_ylim([0, max(plt_data.question)+0.005])\nimport matplotlib.ticker as mtick\nax.xaxis.set_major_formatter(mtick.FuncFormatter(\"{:.2%}\".format))\nax.yaxis.set_major_formatter(mtick.FuncFormatter(\"{:.2%}\".format))\nax.grid(True)\ni =0\nfor key, row in plt_data.iterrows():\n    ax.scatter(row.professional, row.question, s=10+row.student*10**5, alpha=0.5)\n    if i >25:\n        ax.annotate('{}: {:.2%}'.format(key, row['student']), xy=(row['professional'], row['question']))\n    i += 1\nplt.show()","b541f6d9":"# WordCloud\nplt.figure(figsize=(20, 20))\nwordcloud_values=['student', 'professional', 'question']\naxisNum = 1\nfor wordcloud_value in wordcloud_values:\n    wordcloud=WordCloud(margin=0, max_words=20,random_state=41).generate_from_frequencies(plt_data[wordcloud_value])\n    ax=plt.subplot(1,3,axisNum)\n    plt.imshow(wordcloud)\n    plt.title(wordcloud_value)\n    plt.axis(\"off\")\n    axisNum+=1\nplt.show()","9a0eb296":"df_plt_professionals=df_professionals\ndf_plt_professionals=df_plt_professionals[(df_plt_professionals.professionals_date_joined >= '01-01-2016') & (df_plt_professionals.professionals_date_joined <= '30-06-2018')]\ndf_plt_professionals=(df_plt_professionals.date_first_activity -df_plt_professionals.professionals_date_joined).dt.days.fillna(9999).astype(int)\ndf_plt_professionals=df_plt_professionals.groupby(df_plt_professionals).size()\/len(df_plt_professionals)\ndf_plt_professionals=df_plt_professionals.rename(lambda x:0  if x <0.0 else x)\ndf_plt_professionals=df_plt_professionals.rename(lambda  x:0  if x <=7.0 or  x==9999 else '>7')\ndf_plt_professionals=df_plt_professionals.rename({9999:'NaN'})\nplt_professionals =df_plt_professionals.groupby(level=0).sumx","1045abdb":"plt_students =df_students\nplt_students = plt_students[(plt_students['students_date_joined'] >= '01-01-2016') & (plt_students['students_date_joined'] <= '30-06-2018')]\nplt_students = (plt_students['date_first_activity'] - plt_students['students_date_joined']).dt.days.fillna(9999).astype(int)\nplt_students = plt_students.groupby(plt_students).size()\/len(plt_students.index)\nplt_students = plt_students.rename(lambda x: 0 if x < 0.0 else x)\nplt_students = plt_students.rename(lambda x: x if x <= 7.0 or x == 9999 else '> 7')\nplt_students = plt_students.rename({9999:'NaN'})\nplt_students = plt_students.groupby(level=0).sum()\n\nplt_data = pd.DataFrame({'Professionals': plt_professionals,\n                        'Students': plt_students})\n\nplt_data.plot(kind='bar', figsize=(15, 5))\nplt.xlabel('Days')\nplt.ylabel('Proportion')\nplt.title('Days for first activity after registration')\nplt.show()","cfbcf5c5":"df_plt_professionals=((actual_date-df_professionals.))\n","2a3c3c84":"# Gensim Dictionary\nextremes_no_below=10\nextremes_no_above=0.6\nextremes_keep_n=8000","d70688c3":"# Linear Discriminant Analysis\nnum_topics=18\npasses=20\nchuncksize=1000\nalpha=1\/50","ee427d61":"def get_model_results(ldamodel, corpus, dictonary):\n    \"\"\" Create doc-topic probabilities  table and visualization for the LDA modle \"\"\"\n    vis=pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sorted=False)\n    transformed=ldamodel.get_model_results(corpus)\n    df=pd.DataFrame.from_records([{v:k} for v , k in rows for row in transformed])\n    return vis, df","e9e7a889":"def get_model_wordCloud(ldamodel):\n    \"\"\" Create  a Word Cloud  for each topic of the LDA model\"\"\"\n    plot_cols=3\n    plots_rows=math.ceil(num_topics\/3)\n    axisNum=0\n    plt.figureg(figsize=(5*plot_cols, 3*plot_rows))\n    for topicID in range(ldamodel.state.get_lambda().shape[0]):\n        #Gether most relevent term for the given topic\n        topics_terms=ldamodel.state.get_lambda()\n        tempDict={}\n        for i in range(1, len(topics_terms[0])):\n            tempDict[ldamodel.id2word[i]]= topics_terms[topicID,i]\n            \n        #draw the wordcloud\n        wordcloud=WordCloud(margin=0,max_words=0).generate_from_frequencies(tmpDict)\n        axisNum  +=1\n        ax=plt.subplot(plot_rows, plot_cols, axisNum)\n        \n        plt.imshow(wordcloud, interpolation='bilinear')\n        title=topicID\n        plt.title(title)\n        plt.axis(\"off\")\n        plt.margins(x=0,y=0)\n    plt.show()\n        ","5bce0448":"def  topic_query(data, query ):\n    \"\"\" Get Documents matching the query with  the doc-topic probabilities\"\"\"\n    result=data\n    result[\"sort\"]=0\n    for topic in query:\n        result=result[result[topic] >=query[topic]]\n        result['sort']+=result[topic]\n    result = result.sort_values(['sort'], ascending=False)\n    result = result.drop('sort', axis=1)\n    result = result.head(5)\n    return result","a99e816c":"%time\nlda_tokens=df_questions.nlp_tokens\n# gensim dictionary\n\nlda_dict=gensim.corpora.Dictionary(lda_tokens)\nprint(len(lda_dict))\nlda_dict.filter_extremes(no_above=extremes_no_above, no_below=extremes_no_below, keep_n=extremes_keep_n)\nprint(len(lda_dict))\nlda_corpus=[lda_dict.doc2bow(doc) for doc in lda_tokens]\nlda_tfidf=gensim.models.TfidfModel(lda_corpus)\nlda_corpus=lda_tfidf[lda_corpus]\n\nlda_model=gensim.models.LdaModel(lda_corpus,num_topics=num_topics, id2word=lda_dict, passes=passes, update_every=0, chunksize=chuncksize, alpha=alpha,random_state=41)\n\n# Create visualization and Doc-Topic Probabilities\n\n","d146268f":"### Recommendation Engine\nThe Recommendation engine should help to answer the following questions.\n*     **When a student asked a questions. are there already any similar questions that might help him?**       \n       - When a student ask a a question, there may already be similar questions. This could help him with his own questions.\n*     **Which professionals are most expected to answer the new question?**      \n      - New Question should be answered quikly. For this reason, it should be forwared to  professional, who can answer the question best.\n*     **Which Question should be forwarded  to a professional to answer?**\n       - The previous point was from the prespective of a new questions. Here the professional is the focus.Every professional should  be desplayed questions.which could be possible for him to answer (e.g. home screen or daily\/weekly e-mail).\n*     **Which hastags might intrested a user because of his previous activties?**\n       - The last point is to recommend hashtags to a user that might be of interest to him.\n\n","8425377f":"### Get Similar Questions\n","a54d9eea":"### Missing Values (Students)\nIt's a little different with the students. Only the location is specified by most students, while  the rest is rather not used.","0c51dbd4":">## Students ","9a1479e7":"### ***Time Series ***\n\nHere we can see in which year the most user activity was. There was a large increase in 2016. 40% of all questions and comments were in this year. Most of the professionals (40%) have joined in the year 2018. But this hasn't increase the number of answers and comments.\nIn some future analyses, we will limit the data to 2016 and onwards. This is to prevent possible noise from the Career Village start time.","2e9c4031":"### Design ML Model","808ed16d":"### Last Activity After Registration\n\nDepending on the last comment, question or answer of a user, We have extract the last activity date. On the previously plot we have seen, that many users haven't done  any activity yet. For the **'Last Activity'** plot, we take a look only on users with alreday have one activity(dropna).\n\nOn the cumulative histogram, we see that in the last 12 months only 39% of professionls and 24% oif students have written a comment. questions or anwer.\n\n50% professional haven't done any activity for 17 monthns.","0698febe":"# Topic Model (Linear Discriminant Analysis(LDA))\nIn this section, I will implement a LDA model to get topoic probabilities fo questions. We can use this to see how topics are distributed across  questions  and which word characterize them\n\nNew Question can be allocated to topic  and farwarded to the professional who are familiar with these topics.\n    1. Use NLP on the questions corpus\n        * use part-of-speech tagging to filter word\n        * Filter Exterm values of corpus.\n        * Calculate the TF-IDF\n    2. Train a LDA model\n    3. Give the topics names\n    4. Get the topic probability of a query text.","42419f60":"## Feature Extraction","f2f91cd6":"##### Parameters\nThe *token_pos * is used in spacy to extract only words belonging to the *part-of-speech (POS)* tagging. With this we want to reduce the text to only relevant words. A description of all available POS tags can be read at https:\/\/spacy.io\/api\/annotation#pos-tagging\n\nThe actual_date should simulate the current date, because we only have data until 2019-01-31 end of the day.","de308343":"## ***Import libraries***","d5990e0a":"> ##  Emails Responses","6008c66b":"## ***Load dataset***","93630413":"### Features","0500a683":"## EDA ","312219e8":"#### For working with time delta, we will first of all transform the datetime columns (date_added and date_joined). After this we will create the following new features. \n\n* **Questions Dataframe**\n    * **Questions_full_text** : Merge the questions title with with the body for later use for NLP\n    * **Questions_answers_count** : Number of answer a questions has.\n    * **Questions_first_answers** : Timestamp of first answer on questions.\n    * **Questions_last_answers** : Timestamp of last answer on questions.\n    * **questions_hearts:** Hearts score for the question.\n    * **questions_tags: :** List with tags for the questioN\n    * **nlp_tokens** : Extract relevent tokens from the questions full text.\n\n* **Aaswer Dataframe**\n    * **time_delta_answer** : Time delta from questions to answer.\n    * **answers_time_rank:** Ranking for time to answer the question.\n    * **answers_hearts:** Hearts score for the answer.\n    \n* **Professional Dataframe**\n    * **Professional_time_delta_joined** : Time since Professional creating the Account\n    * **Professional_answers_count** : Number of written answers.\n    * **Professional_comments_count** : Number of written comments.\n    * **date_first_answer** : Date first answer.\n    * **date_last_answer** : Date last answer. \n    * **date_first_comment**: Date first comment.\n    * **date_last_comment**: Date last comment.\n    * **date_last_activity**: Date last activity (answer or comment).\n    * **date_first_activity**: Date first activity (answer or comment).\n    * **Professional_answers_hearts** : Hearts scores for all answers.\n    * **Professional_tags** : List with tags for the professionals.\n   \n* **Questions Dataframe**\n    * **students_time_delta_joined**: Time since creating the account.\n    * **students_questions_count**: Number of written questions.\n    * **students_comments_count**: Number of written comments.\n    * **date_last_questions**: Date last question.\n    * **date_first_questions**: Date first question.\n    * **date_last_comment**: Date last comment.\n    * **date_first_comment**: Date first comment.\n    * **date_last_activity**: Date last activity (question or comment).\n    * **date_first_activity**: Date first activity (question or comment).\n    * **students_questions_hearts:** Hearts score for all questions.\n    * **students_tags:** List with tags for the students. \n   \n   \n* **new DataFrame emails_response**:\n   - Has the response activity from professionals to emails and additional informations about the questions behind.\n    * **time_delta_email_answer**: Time needed the question was answered after the email was send.\n    * **time_delta_question_email**: Time needed the email was send after the questions was written.\n    ","7022ce8b":"### First Activity After Registration\nHere we will see how long it took, That a professional makes his first answer or a student his first question after registration to portal. Most of them write the first answer or question in the first day or haven't written yet and used their account for other activites. ","1f036762":"# Career Recommendation\n--- \nWhat is Recommendation ?\n\nRecommendation systems are  ML system, which help in getting relevent information or suggestion\n\n- There are two Type\n\n**1. Content-Based**|\n\n**2. Collaborative Filtering**\n\n","d25e31a6":"> #### Tags Matching\n- The size of bubbles depends on how many students  have subscribe the tags.  The X-axis is how many professionals have subscribe the tags and the y-axis in how many questions the tag is used.\n- The top tag for professionals is telecommunicaitons on the right site with about 11% but tag doesn't appear in many questions  or students subscription.\n- The top tag for questions is college with 15.6% and career with 6.5%. The other top tags are career specific(medicine, Engineering, business,...)\n- The Top tag for students is college  but with only 1.5% of the sutudents subscribe the tag. ","68b066c1":"### Missing values (Professionals)\n\nThe *location*, *industry* and *Headline* were specified by the most professionals. Even hastags are used by most. The specification of school is made however only by the fewest.The industry could be therefore be a good feature of recommendation. Especially for new authors who have not written any answer or comments. If a professional is in the medical field, he should not necessary to get questions about a career as a lawyers.","04456854":"### Global Parameters \n","b30603bc":"> ###  Professionals","3b58eb3e":">## Questions"}}