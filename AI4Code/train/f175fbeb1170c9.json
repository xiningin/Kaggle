{"cell_type":{"6c40df14":"code","16bb62cc":"code","a6638926":"code","a4f2e78a":"code","c3064d50":"code","4523e627":"code","7ea75797":"code","9d7aa07a":"code","feaa691b":"code","e2d178ee":"code","a554cb35":"code","cb6f73b7":"code","e54cbb33":"code","af18395b":"code","92448f35":"code","38857efa":"code","a8c57a1b":"code","678589a5":"code","303d7ea5":"code","6ac14290":"code","a4f2ddd9":"code","4267decb":"code","a264e974":"code","7079c63f":"code","6e6557d9":"code","ace21120":"code","ea6b8a72":"code","01e71cc8":"code","d7a52e67":"code","3fc68912":"code","76dd4d29":"code","dcd1304d":"code","110c5511":"markdown","14858224":"markdown","0d92a536":"markdown","e9c364b0":"markdown","f09eefd4":"markdown","dfd94448":"markdown","40d91f13":"markdown","d216ed5e":"markdown","07dd753d":"markdown","5473517c":"markdown","fa5bfc89":"markdown","38dcf7df":"markdown","e490c7e7":"markdown","62498d09":"markdown","71ac9d8e":"markdown","7eba9aef":"markdown","ed43e898":"markdown"},"source":{"6c40df14":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","16bb62cc":"import numpy as np \nimport pandas as pd \npd.set_option('display.max_columns', 500)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc\nimport time\nimport sys\nimport datetime\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing as pp\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn import metrics\n# Plotly library\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)\n\n\nfrom tqdm import tqdm","a6638926":"dtypes = {\n        'MachineIdentifier':                                    'category',\n        'ProductName':                                          'category',\n        'EngineVersion':                                        'category',\n        'AppVersion':                                           'category',\n        'AvSigVersion':                                         'category',\n        'IsBeta':                                               'int8',\n        'RtpStateBitfield':                                     'float16',\n        'IsSxsPassiveMode':                                     'int8',\n        'DefaultBrowsersIdentifier':                            'float16',\n        'AVProductStatesIdentifier':                            'float32',\n        'AVProductsInstalled':                                  'float16',\n        'AVProductsEnabled':                                    'float16',\n        'HasTpm':                                               'int8',\n        'CountryIdentifier':                                    'int16',\n        'CityIdentifier':                                       'float32',\n        'OrganizationIdentifier':                               'float16',\n        'GeoNameIdentifier':                                    'float16',\n        'LocaleEnglishNameIdentifier':                          'int8',\n        'Platform':                                             'category',\n        'Processor':                                            'category',\n        'OsVer':                                                'category',\n        'OsBuild':                                              'int16',\n        'OsSuite':                                              'int16',\n        'OsPlatformSubRelease':                                 'category',\n        'OsBuildLab':                                           'category',\n        'SkuEdition':                                           'category',\n        'IsProtected':                                          'float16',\n        'AutoSampleOptIn':                                      'int8',\n        'PuaMode':                                              'category',\n        'SMode':                                                'float16',\n        'IeVerIdentifier':                                      'float16',\n        'SmartScreen':                                          'category',\n        'Firewall':                                             'float16',\n        'UacLuaenable':                                         'float32',\n        'Census_MDC2FormFactor':                                'category',\n        'Census_DeviceFamily':                                  'category',\n        'Census_OEMNameIdentifier':                             'float16',\n        'Census_OEMModelIdentifier':                            'float32',\n        'Census_ProcessorCoreCount':                            'float16',\n        'Census_ProcessorManufacturerIdentifier':               'float16',\n        'Census_ProcessorModelIdentifier':                      'float16',\n        'Census_ProcessorClass':                                'category',\n        'Census_PrimaryDiskTotalCapacity':                      'float32',\n        'Census_PrimaryDiskTypeName':                           'category',\n        'Census_SystemVolumeTotalCapacity':                     'float32',\n        'Census_HasOpticalDiskDrive':                           'int8',\n        'Census_TotalPhysicalRAM':                              'float32',\n        'Census_ChassisTypeName':                               'category',\n        'Census_InternalPrimaryDiagonalDisplaySizeInInches':    'float16',\n        'Census_InternalPrimaryDisplayResolutionHorizontal':    'float16',\n        'Census_InternalPrimaryDisplayResolutionVertical':      'float16',\n        'Census_PowerPlatformRoleName':                         'category',\n        'Census_InternalBatteryType':                           'category',\n        'Census_InternalBatteryNumberOfCharges':                'float32',\n        'Census_OSVersion':                                     'category',\n        'Census_OSArchitecture':                                'category',\n        'Census_OSBranch':                                      'category',\n        'Census_OSBuildNumber':                                 'int16',\n        'Census_OSBuildRevision':                               'int32',\n        'Census_OSEdition':                                     'category',\n        'Census_OSSkuName':                                     'category',\n        'Census_OSInstallTypeName':                             'category',\n        'Census_OSInstallLanguageIdentifier':                   'float16',\n        'Census_OSUILocaleIdentifier':                          'int16',\n        'Census_OSWUAutoUpdateOptionsName':                     'category',\n        'Census_IsPortableOperatingSystem':                     'int8',\n        'Census_GenuineStateName':                              'category',\n        'Census_ActivationChannel':                             'category',\n        'Census_IsFlightingInternal':                           'float16',\n        'Census_IsFlightsDisabled':                             'float16',\n        'Census_FlightRing':                                    'category',\n        'Census_ThresholdOptIn':                                'float16',\n        'Census_FirmwareManufacturerIdentifier':                'float16',\n        'Census_FirmwareVersionIdentifier':                     'float32',\n        'Census_IsSecureBootEnabled':                           'int8',\n        'Census_IsWIMBootEnabled':                              'float16',\n        'Census_IsVirtualDevice':                               'float16',\n        'Census_IsTouchEnabled':                                'int8',\n        'Census_IsPenCapable':                                  'int8',\n        'Census_IsAlwaysOnAlwaysConnectedCapable':              'float16',\n        'Wdft_IsGamer':                                         'float16',\n        'Wdft_RegionIdentifier':                                'float16',\n        'HasDetections':                                        'int8'\n        }","a4f2e78a":"%%time\nnrows = 1000000\n#_______________________________________________________________________________\n# retained_columns = numerical_columns + categorical_columns\ntrain = pd.read_csv('..\/input\/train.csv',\n                    nrows = nrows,\n#                     usecols = retained_columns,\n                    dtype = dtypes)","c3064d50":"train.head()","4523e627":"target = train['HasDetections']\ntrain.drop('HasDetections', inplace=True, axis=1)","7ea75797":"num_datatypes = ['int8', 'int16', 'int32', 'float16', 'float32']\nnumerical_columns = [c for c,v in dtypes.items() if v in num_datatypes]","9d7aa07a":"true_numerical_columns = [\n    'Census_ProcessorCoreCount',\n    'Census_PrimaryDiskTotalCapacity',\n    'Census_SystemVolumeTotalCapacity',\n    'Census_TotalPhysicalRAM',\n    'Census_InternalPrimaryDiagonalDisplaySizeInInches',\n    'Census_InternalPrimaryDisplayResolutionHorizontal',\n    'Census_InternalPrimaryDisplayResolutionVertical',\n    'Census_InternalBatteryNumberOfCharges'\n]","feaa691b":"false_numerical_columns = [col for col in numerical_columns if col not in true_numerical_columns]","e2d178ee":"train_true_num = train[true_numerical_columns]\ntrain_true_num.head(2)","a554cb35":"train_true_num['MachineIdentifier'] = train['MachineIdentifier']","cb6f73b7":"lgb_params = {'num_leaves': 60,\n         'min_data_in_leaf': 60, \n         'objective':'binary',\n         'max_depth': -1,\n         'learning_rate': 0.1,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.8,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'auc',\n         \"lambda_l1\": 0.1,\n         \"random_state\": 42,\n         \"verbosity\": -1}","e54cbb33":"train_true_num.head()","af18395b":"train_true_num.shape","92448f35":"gc.collect()","38857efa":"folds = KFold(n_splits=3, shuffle=True, random_state=42)\noof = np.zeros(len(train_true_num))\n# categorical_columns = [c for c in categorical_columns if c not in ['MachineIdentifier']]\nfeatures = [c for c in train_true_num.columns if c not in ['MachineIdentifier']]\n# predictions = np.zeros(len(test))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\nstart_time= time.time()\nscore = [0 for _ in range(folds.n_splits)]\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_true_num.values, target.values)):\n    print(\"Fold No.{}\".format(fold_+1))\n    trn_data = lgb.Dataset(train_true_num.iloc[trn_idx][features],\n                           label=target.iloc[trn_idx],\n#                            categorical_feature = categorical_columns\n                          )\n    val_data = lgb.Dataset(train_true_num.iloc[val_idx][features],\n                           label=target.iloc[val_idx],\n#                            categorical_feature = categorical_columns\n                          )\n\n    num_round = 10000\n    clf = lgb.train(lgb_params,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    \n    oof[val_idx] = clf.predict(train_true_num.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    print(\"Time elapsed: {:<5.2}s\".format((time.time() - start_time) \/ 3600))\n    score[fold_] = metrics.roc_auc_score(target.iloc[val_idx], oof[val_idx])\n\nfeat_num_true_score = metrics.roc_auc_score(target, oof)\nprint(\"CV score: {:<8.5f}\".format(feat_num_true_score))\nprint(\"Total time elapsed: {:<5.2}s\".format(time.time() - start))","a8c57a1b":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(8,8))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds) -- True Numerical Features')\nplt.tight_layout()","678589a5":"def PolynomialFeatures_labeled(input_df,power):\n    '''Basically this is a cover for the sklearn preprocessing function. \n    The problem with that function is if you give it a labeled dataframe, it ouputs an unlabeled dataframe with potentially\n    a whole bunch of unlabeled columns. \n\n    Inputs:\n    input_df = Your labeled pandas dataframe (list of x's not raised to any power) \n    power = what order polynomial you want variables up to. (use the same power as you want entered into pp.PolynomialFeatures(power) directly)\n\n    Ouput:\n    Output: This function relies on the powers_ matrix which is one of the preprocessing function's outputs to create logical labels and \n    outputs a labeled pandas dataframe   \n    '''\n    poly = pp.PolynomialFeatures(power)\n    output_nparray = poly.fit_transform(input_df)\n    powers_nparray = poly.powers_\n\n    input_feature_names = list(input_df.columns)\n    target_feature_names = [\"Constant Term\"]\n    for feature_distillation in powers_nparray[1:]:\n        intermediary_label = \"\"\n        final_label = \"\"\n        for i in range(len(input_feature_names)):\n            if feature_distillation[i] == 0:\n                continue\n            else:\n                variable = input_feature_names[i]\n                power = feature_distillation[i]\n                intermediary_label = \"%s^%d\" % (variable,power)\n                if final_label == \"\":         #If the final label isn't yet specified\n                    final_label = intermediary_label\n                else:\n                    final_label = final_label + \" x \" + intermediary_label\n        target_feature_names.append(final_label)\n    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n    return output_df","303d7ea5":"train_true_num_copy = train_true_num.drop('MachineIdentifier', axis=1)","6ac14290":"train_true_num_pow2 = PolynomialFeatures_labeled(train_true_num_copy.fillna(0), 2)\ntrain_true_num_pow2.head()","a4f2ddd9":"train_true_num_pow2['MachineIdentifier'] = train_true_num['MachineIdentifier']\ntrain_true_num_pow2.shape","4267decb":"train_true_num_pow2 = reduce_mem_usage(train_true_num_pow2)","a264e974":"gc.collect()","7079c63f":"folds = KFold(n_splits=3, shuffle=True, random_state=42)\noof = np.zeros(len(train_true_num_pow2))\n# categorical_columns = [c for c in categorical_columns if c not in ['MachineIdentifier']]\nfeatures = [c for c in train_true_num_pow2.columns if c not in ['MachineIdentifier']]\n# predictions = np.zeros(len(test))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\nstart_time= time.time()\nscore = [0 for _ in range(folds.n_splits)]\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_true_num_pow2.values, target.values)):\n    print(\"Fold No.{}\".format(fold_+1))\n    trn_data = lgb.Dataset(train_true_num_pow2.iloc[trn_idx][features],\n                           label=target.iloc[trn_idx],\n#                            categorical_feature = categorical_columns\n                          )\n    val_data = lgb.Dataset(train_true_num_pow2.iloc[val_idx][features],\n                           label=target.iloc[val_idx],\n#                            categorical_feature = categorical_columns\n                          )\n\n    num_round = 10000\n    clf = lgb.train(lgb_params,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    \n    oof[val_idx] = clf.predict(train_true_num_pow2.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    print(\"Time elapsed: {:<5.2}s\".format((time.time() - start_time) \/ 3600))\n    score[fold_] = metrics.roc_auc_score(target.iloc[val_idx], oof[val_idx])\n\nfeat_num_pow2_score = metrics.roc_auc_score(target, oof)\nprint(\"CV score: {:<8.5f}\".format(feat_num_pow2_score))\nprint(\"Total time elapsed: {:<5.2}s\".format(time.time() - start))","6e6557d9":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(12,12))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds) -- True Numerical Features (including Polynomials)')\nplt.tight_layout()","ace21120":"del false_numerical_columns[-1]","ea6b8a72":"train_false_num = train[false_numerical_columns]\ntrain_false_num['MachineIdentifier'] = train['MachineIdentifier']\ntrain_false_num.head(2)","01e71cc8":"train_false_num.shape","d7a52e67":"train_false_num = reduce_mem_usage(train_false_num)","3fc68912":"gc.collect()","76dd4d29":"folds = KFold(n_splits=3, shuffle=True, random_state=42)\noof = np.zeros(len(train_false_num))\n# categorical_columns = [c for c in categorical_columns if c not in ['MachineIdentifier']]\nfeatures = [c for c in train_false_num.columns if c not in ['MachineIdentifier']]\n# predictions = np.zeros(len(test))\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\nstart_time= time.time()\nscore = [0 for _ in range(folds.n_splits)]\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_false_num.values, target.values)):\n    print(\"Fold No.{}\".format(fold_+1))\n    trn_data = lgb.Dataset(train_false_num.iloc[trn_idx][features],\n                           label=target.iloc[trn_idx],\n#                            categorical_feature = categorical_columns\n                          )\n    val_data = lgb.Dataset(train_false_num.iloc[val_idx][features],\n                           label=target.iloc[val_idx],\n#                            categorical_feature = categorical_columns\n                          )\n\n    num_round = 10000\n    clf = lgb.train(lgb_params,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    \n    oof[val_idx] = clf.predict(train_false_num.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    print(\"Time elapsed: {:<5.2}s\".format((time.time() - start_time) \/ 3600))\n    score[fold_] = metrics.roc_auc_score(target.iloc[val_idx], oof[val_idx])\n\nfeat_num_false_score = metrics.roc_auc_score(target, oof)\nprint(\"CV score: {:<8.5f}\".format(feat_num_false_score))\nprint(\"Total time elapsed: {:<5.2}s\".format(time.time() - start))","dcd1304d":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(12,12))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds) -- False Numerical Features')\nplt.tight_layout()","110c5511":"### So a considerably higher CV for using just the 'false' numeric features","14858224":"### The code below is from https:\/\/stackoverflow.com\/questions\/36728287\/sklearn-preprocessing-polynomialfeatures-how-to-keep-column-names-headers-of","0d92a536":"### Let's try an AUC calculation just based on these features","e9c364b0":"### FabienDaniel's kernel (https:\/\/www.kaggle.com\/fabiendaniel\/detecting-malwares-with-lgbm\/comments#440611) has been my reference throughout this competition. \n#### His categorization of the features is very clean and he lists three principal kinds of features::\n* Numerical features (which includes categories represented as numbers and true numerical features)\n*  Binary features (these could also be considered Categorical, but let's make this distinction and see what we get for now)\n* Categorical features\n\n## Here we analyze the* numerical features ('true' and 'false')* while the categorical features (binary and otherwise) will be analyzed in another kernel\n\n### In this kernel I plan to study these kinds of features in some detail, and analyze which ones are more important to the AUC, so that we have a better idea of what gets more importance as we go into the deeper end of the competition\n\n### *Note:*: We will generally not put much effort into getting high accuracies with this kernel. The goal is to understand the relative importances of the various kinds of features to the target. Once we have that knowledge in place, then we can hopefully get a better idea of what kind of feature engineering will actually be of use. ","f09eefd4":"### Let's look at how accurate our calculations would be just for the true_numerical features","dfd94448":"### Now let's plot the feature importances","40d91f13":"### Let's look at the CV score and the feature importances of these","d216ed5e":"### In summary, we analyze the numerical features in the Microsoft Malware Detection training set. We find that just utilizing the 'true' numerical features gives us a marginally better CV than utilizing some polynomial features of the same (in addition). However, we do see certain combinations of features contributing large feature importances, which we would do well to keep in mind for competition entries. \n### Just using the 'false' numerical features gives us a much higher AUC score, and we might want to test out polynomial features for features like 'AvProductStatesIdentifier', 'AvProductsInstalled' and 'Census_ProcessorModelIdentifier'. \n\n### Most of the 'false' numeric features can be probably represented as some variety of categorical features so this is also something to probably look into. \n\n","07dd753d":"### Let's add some polynomial features to this data frame of true numerical features","5473517c":"# 1. Numerical Features\n## i. \"True\" Numerical Features\n### First let's look at the numerical columns","fa5bfc89":"## ii. \"False\" Numerical Features\n### Let's look at the other 'false' numerical features","38dcf7df":"### The CV accuracy is marginally lower than before, but in the Feature Importances we do see a couple of things that might turn out to be important. Census_TotalPhysicalRAM gets pushed down (as compared to when we just used the true numerical features without polynomials), and the product of Census_TotalPhysicalRam and Census_InternalPrimaryDiagonalDisplaySizeInInches becomes very important. \n\n### Must check how much of this is due to the relatively arbitrary handling of nans","e490c7e7":"### Of these, only a handful of columns (as Fabien points out) are really numerical","62498d09":"### Let's read in a fraction of the training set","71ac9d8e":"#### Using Theo Viel's method (https:\/\/www.kaggle.com\/theoviel\/load-the-totality-of-the-data)","7eba9aef":"### Let's check the AUC and feature_importances for these","ed43e898":"### As a basic step, let's set all the missing values to zero (will this skew the distribution?)"}}