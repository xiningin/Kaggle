{"cell_type":{"58f88c59":"code","d1573aaf":"code","99d1dbb8":"code","863422f7":"code","1ed4b782":"code","967a7130":"code","e9732dc0":"code","b998068f":"code","0b29c846":"code","289882c8":"code","e9b5515d":"code","a20d54a8":"code","c269147c":"code","c20e193c":"code","f4694d09":"code","cc6b77e6":"code","79e5f074":"code","1d0329c6":"markdown","4fd70dd6":"markdown","4a7d6100":"markdown","c7b08592":"markdown","6a1b14ee":"markdown","d00a8357":"markdown","2d0f8008":"markdown","9a7cc615":"markdown","8cb69479":"markdown","5b91db4e":"markdown","9a935534":"markdown","52d7e250":"markdown"},"source":{"58f88c59":"import tensorflow as tf\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2 as cv\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, Input\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU\nfrom tqdm.notebook import tqdm\nplt.rc('figure',figsize=(16,8))\nsns.set_context('paper',font_scale=1.5)\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\nprint(tf.__version__)","d1573aaf":"#GLOBAL VARS\nFOLDER_PATH     = '\/kaggle\/input\/6000-children-and-teen-book-covers\/BookCovers\/'\nN_IMAGES        = 6232\nLATENT_DIM      = 512\nN_BATCHES       = 128\nN_EPOCHS        = 350\nBATCH_PER_EPOCH = N_IMAGES \/\/ N_BATCHES\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","99d1dbb8":"images = np.array([cv.resize(plt.imread(FOLDER_PATH+i),(128,128),interpolation = cv.INTER_AREA)\/ 255.0 for i in os.listdir(FOLDER_PATH)])\n\n","863422f7":"_, axs = plt.subplots(4, 4, figsize=(12, 12))\naxs = axs.flatten()\nfor img, ax in zip(images, axs):\n    ax.imshow(img)\nplt.tight_layout()\nplt.show()","1ed4b782":"\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope() as tpu:\n    #Classifier\n    Discriminator = Sequential(name='Discriminator')\n\n    Discriminator.add(Input(shape=(128,128,3))) \n\n    Discriminator.add(Conv2D(128, (4,4), strides=(2, 2), padding='same'))\n    Discriminator.add(LeakyReLU(alpha=0.2))\n\n    Discriminator.add(Conv2D(256, (4,4), strides=(2, 2), padding='same'))\n    Discriminator.add(LeakyReLU(alpha=0.2))\n\n\n    Discriminator.add(Conv2D(512, (4,4), strides=(2, 2), padding='same'))\n    Discriminator.add(LeakyReLU(alpha=0.2))\n\n    Discriminator.add(Flatten())\n    Discriminator.add(Dense(1, activation='sigmoid'))\n\n    opt = Adam(lr=0.0002, beta_1=0.5)\n    Discriminator.compile(loss='binary_crossentropy', optimizer=opt)\n","967a7130":"tf.keras.utils.plot_model(Discriminator,    \n                          show_shapes=True,\n                          show_dtype=True,\n)","e9732dc0":"\n# Generator\nwith tpu_strategy.scope() as tpu:\n\n    Generator = Sequential(name='Generator')\n\n    Generator.add(Dense(256*16*16, input_dim=LATENT_DIM)) \n    Generator.add(LeakyReLU(alpha=0.2))\n    Generator.add(Reshape((16, 16, 256)))\n\n    Generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    Generator.add(LeakyReLU(alpha=0.2))\n\n    Generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    Generator.add(LeakyReLU(alpha=0.2))\n\n    Generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    Generator.add(LeakyReLU(alpha=0.2))\n\n\n    Generator.add(Conv2D(3, (8,8), activation='sigmoid', padding='same'))","b998068f":"tf.keras.utils.plot_model(Generator,    \n                          show_shapes=True,\n                          show_dtype=True,\n)","0b29c846":"examples_input = np.random.randn(16,LATENT_DIM)\nexamples = Generator.predict(examples_input)","289882c8":"\n#Gan Model\nDiscriminator.trainable = False\nwith tpu_strategy.scope() as tpu:\n\n    GAN_model = Sequential(name='GAN_model')\n    GAN_model.add(Generator) \n    GAN_model.add(Discriminator)\n\n    opt = Adam(lr=0.0002, beta_1=0.5)\n    GAN_model.compile(loss='binary_crossentropy', optimizer=opt)\n\n","e9b5515d":"tf.keras.utils.plot_model(GAN_model,    \n                          show_shapes=True,\n                          show_dtype=True,\n)","a20d54a8":"with tpu_strategy.scope() as tpu:\n\n    Generator_1       = tf.keras.models.load_model('\/kaggle\/input\/bookcover-generator-dc-gan\/Generator.h5')\n    Discriminator_1   = tf.keras.models.load_model('\/kaggle\/input\/bookcover-generator-dc-gan\/Discriminator.h5')\n    GAN_model_1       = tf.keras.models.load_model('\/kaggle\/input\/bookcover-generator-dc-gan\/GAN_model.h5')\n    ","c269147c":"for i in range(len(Discriminator.layers)):\n    (Discriminator.layers[i]).set_weights(Discriminator_1.layers[i].get_weights())\nfor i in range(len(Generator.layers)):\n    (Generator.layers[i]).set_weights(Generator_1.layers[i].get_weights())\nfor i in range(len(GAN_model_1.layers)):\n    (GAN_model.layers[i]).set_weights(GAN_model_1.layers[i].get_weights())\n      ","c20e193c":"for i in range(N_EPOCHS):\n    for j in (range(BATCH_PER_EPOCH)):\n        \n        ix = np.random.randint(0, N_IMAGES, N_BATCHES)    \n        \n        X_real = images[ix] \n        y_real = np.ones((N_BATCHES, 1))-0.1 \n        \n        noise_input = np.random.randn(N_BATCHES,LATENT_DIM)\n        \n        X_fake = Generator.predict(noise_input)\n        y_fake = np.zeros((N_BATCHES, 1)) \n        \n        D_loss_real = Discriminator.train_on_batch(X_real, y_real)\n        D_loss_fake = Discriminator.train_on_batch(X_fake, y_fake)\n        \n        X_gan = np.random.randn(N_BATCHES,LATENT_DIM)\n        y_gan = np.ones((N_BATCHES, 1)) \n        \n        G_loss = GAN_model.train_on_batch(X_gan, y_gan)\n    if i % 30 == 0:\n        examples = Generator.predict(examples_input)\n        plt.figure(figsize=(16,1))\n        for i in range(16):\n            plt.subplot(1, 16, 1 + i)\n            plt.axis('off')\n            plt.imshow(examples[i])\n        plt.show()","f4694d09":"n = 64\nnoise_input = np.random.randn(n,LATENT_DIM)\nexamples = Generator.predict(noise_input)\nplt.figure(figsize=(12,14))\nfor i in range(n):\n\t\tplt.subplot(8, 8, 1 + i)\n\t\tplt.axis('off')\n\t\tplt.imshow(examples[i])\n\t\tplt.title(i)\n","cc6b77e6":"Z_1 = noise_input[np.random.randint(0,64,1)[0]]\nZ_2 = noise_input[np.random.randint(0,64,1)[0]]\n\ninterpolated = np.array([alpha*Z_1 + (1-alpha)*Z_2 for alpha in np.arange(0,1,0.1)])\nexamples = Generator.predict(interpolated)\nplt.figure(figsize=(20,11))\nfor i in range(10):\n    plt.title(f'Transition {i+1}')\n    plt.subplot(2, 5, 1 + i)\n    plt.axis('off')\n    plt.imshow(examples[i])\nplt.show()\n","79e5f074":"Generator.save('.\/Generator.h5')\nDiscriminator.save('.\/Discriminator.h5')\nGAN_model.save('.\/GAN_model.h5')\n","1d0329c6":"<img src=\"https:\/\/i.ibb.co\/qsLCQjS\/egan.jpg\" style=\"width:1100px;height:400;\">\n","4fd70dd6":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Assembled GAN<\/h1>\n\n","4a7d6100":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Generator<\/h1>\n\n","c7b08592":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Image Loading\/Preprocessing and Presets<\/h1>\n\n","6a1b14ee":"**Note**: During the training process, every 50 epochs, an example output will be plotted to evaluate the model's progression. ","d00a8357":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Model Training<\/h1>\n\n","2d0f8008":"<img src=\"https:\/\/i.ibb.co\/fnhMX9k\/descriminator.png\" style=\"width:1100px;height:400;\">\n","9a7cc615":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Discriminator<\/h1>\n\n","8cb69479":"**We Will Use Weights Trained in The Perivous Versions Of This Notebook To Get Better Results and See How The Training Progressed After 2 Run of N Epochs**","5b91db4e":"<img src=\"https:\/\/i.ibb.co\/chKLqLh\/generator.png\" style=\"width:1100px;height:400;\">\n","9a935534":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Model Evaluation<\/h1>\n\n","52d7e250":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h1>\n\n"}}