{"cell_type":{"cd38dd61":"code","a068ba66":"code","b5d90f76":"code","4ad1c37e":"code","8ac5bb37":"code","de9779b0":"code","6fc4760d":"code","c2ef7325":"code","c9fb417b":"code","71c20000":"code","4cd97bab":"code","e0d8ae80":"code","01385d01":"code","f4509e6d":"code","93b140d7":"code","6c6a1a91":"code","93f03327":"code","e5fe5d73":"code","8ad60252":"code","253d51fe":"code","404bfd7a":"code","51a118f2":"code","2fd07e88":"code","72cdbb4c":"code","dd1c2eeb":"code","78db2058":"code","cfae90ab":"code","6bdf94b1":"code","5c5bcb18":"code","4caa0ee9":"code","13921e28":"code","776c557d":"code","e5721dbc":"code","d13c6d60":"code","6080c194":"code","41450386":"code","5d2e47c4":"code","ffbf9645":"code","5ace92e8":"code","8de15707":"code","a038e49e":"code","b5244953":"code","3135622d":"code","0df944f0":"code","8ae66138":"code","7443cb27":"code","55e7ea8d":"code","5942f93e":"code","44b34efd":"code","34fb9266":"code","380c6c7e":"code","304120de":"code","0fcb28ea":"code","7308f86b":"code","281aa22d":"code","c763c0a6":"code","dfb31b63":"code","2494510d":"code","19844e35":"code","582ed35c":"code","24f538f2":"code","d34ee17e":"code","15187440":"code","c2d89c55":"code","902ab26d":"code","4329c1ad":"code","0d05b10f":"code","9c65225d":"code","dd0d8c79":"code","3d09970c":"code","acd1a3ed":"code","c9799551":"code","a1382c81":"code","207f999f":"code","c53a902a":"code","51ae636c":"code","a207ce4c":"code","bb92e97e":"code","fcd65706":"code","c062bad8":"code","222a43d4":"code","702e0891":"code","59559c0f":"code","f2c88c38":"code","a82936c1":"code","7c4f8c9b":"code","4ed4cd16":"code","cad3a34f":"code","a00c0d54":"code","3c6d6939":"code","9dfb963e":"code","ad7e4255":"code","827a5781":"code","70729ba3":"code","c1dc0360":"code","8036ba9b":"code","ee85e6a3":"code","2bd8bd08":"code","b7bc63ac":"code","2d15aa50":"code","b1b2b9f0":"code","9bc62dc5":"code","27d688d7":"markdown","61fc6b4e":"markdown","bed89c72":"markdown","f9aa35d0":"markdown","7601134f":"markdown","aa20d8d5":"markdown","8ad9902b":"markdown","d63b4077":"markdown","4d107091":"markdown","296cf6b1":"markdown","d0acd15d":"markdown","a0fc0723":"markdown","28cf1424":"markdown","0699dd17":"markdown","891ea0a9":"markdown","808201ed":"markdown","95a418d8":"markdown","162622b8":"markdown","e87cdd79":"markdown","bb76c34f":"markdown","85364828":"markdown","f598af03":"markdown","c5456cdf":"markdown","0e868b02":"markdown","d3dcce94":"markdown","36f3f2de":"markdown","7d72aed7":"markdown","6609daf3":"markdown","fa4c442d":"markdown","a3a53ebb":"markdown","b94d22b5":"markdown","9def385a":"markdown","d5ea8aaf":"markdown","338e75a4":"markdown","e7c65670":"markdown","54aa11c6":"markdown","55990e97":"markdown","de21d9d5":"markdown","ce2bceb6":"markdown","e05d069c":"markdown","a6d5b528":"markdown","99df49ef":"markdown","9003a66b":"markdown"},"source":{"cd38dd61":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a068ba66":"#Importing Libraries\nimport sklearn as sk\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport pylab as py\nfrom haversine import haversine\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set(style = 'white')","b5d90f76":"data = pd.read_csv(\"..\/input\/newyorktaxi\/nyc_taxi_trip_duration.csv\")","4ad1c37e":"data.head()","8ac5bb37":"data.columns","de9779b0":"data.shape","6fc4760d":"#Lets check for unique values of each variables\ndata.nunique()","c2ef7325":"data.dtypes","c9fb417b":"#we will convert the data type of \"store_and_fwd_flag\" and \"vendor_id\" to category data type.\ndata['store_and_fwd_flag'] = data['store_and_fwd_flag'].astype('category')\ndata['vendor_id'] = data['vendor_id'].astype('category')","71c20000":"#we know id variable is unique for every customers with uniform distribution. Therefore it is not of much use so we will drop it.\ndata = data.drop(columns=[\"id\"])","4cd97bab":"#Converting the data type of pickup and dropoff datetime as datetime index\ndata['pickup_datetime']=pd.DatetimeIndex(data['pickup_datetime'])\ndata['dropoff_datetime']= pd.DatetimeIndex(data['dropoff_datetime'])","e0d8ae80":"#Creating an instance(data) of Datetimeindex class using \"pickup_datetime and dropoff_datetime\"\np_date=pd.DatetimeIndex(data['pickup_datetime'])\nd_date= pd.DatetimeIndex(data['dropoff_datetime'])","01385d01":"data['day_of_week']=data['pickup_datetime'].dt.weekday","f4509e6d":"data['hour_of_day'] = data['pickup_datetime'].dt.hour","93b140d7":"#CHECKING NEW EXTRACTED COLUMNS USING DATETIME\ndata[['day_of_week','hour_of_day']].head()","6c6a1a91":"data['dropday_of_week'] = data['dropoff_datetime'].dt.weekday","93f03327":"data['drop_hour_of_day'] = data['dropoff_datetime'].dt.hour","e5fe5d73":"#CHECKING NEW EXTRACTED COLUMNS USING DATETIME\ndata[['dropday_of_week','drop_hour_of_day']].head()\n","8ad60252":"# \"calc_distance is a function to calculate distance between pickup and dropoff coordinates using \"Haversine formula\".\ndef calc_distance(data):\n    pickup = (data['pickup_latitude'], data['pickup_longitude'])\n    drop = (data['dropoff_latitude'], data['dropoff_longitude'])\n    return haversine(pickup, drop)\n","253d51fe":"data['distance'] = data.apply(lambda x: calc_distance(x), axis = 1)","404bfd7a":"data['distance'].head()","51a118f2":"# Calculate speed in km\/h for further insights\ndata['speed'] = (data.distance\/(data.trip_duration\/3600))","2fd07e88":"data.dtypes.reset_index()","72cdbb4c":"#UNIVARIATE ANALYSIS-\n#Let us start doing univariate analysis with Taget variable - 'trip_duration'\nsns.histplot(data=data, x=\"trip_duration\", bins = 20)","dd1c2eeb":"#Lets plot boxplot\nsns.boxplot(data[\"trip_duration\"])\n","78db2058":"data['trip_duration'].sort_values(ascending=False)","cfae90ab":"#We can see that there is an entry which is significantly different from others.\n#As there is a single row only, let us drop this row.\n\ndata.drop(data[data['trip_duration'] == 1939736].index, inplace = True)","6bdf94b1":"data.shape","5c5bcb18":"cab_details  = ['passenger_count','distance','speed']\npickup_details = ['hour_of_day','day_of_week']\ndropoff_details = ['dropday_of_week','drop_hour_of_day']\n","4caa0ee9":"# custom function for easy and efficient analysis of numerical univariate\n\ndef UVA_numeric(data, var_group):\n  '''\u00a0\n  Univariate_Analysis_numeric\n  takes a group of variables (INTEGER and FLOAT) and plot\/print all the descriptives and properties along with KDE.\n\n  Runs a loop: calculate all the descriptives of i(th) variable and plot\/print it\n  '''\n\n  size = len(var_group)\n  plt.figure(figsize = (7*size,3), dpi = 100)\n  \n  #looping for each variable\n  for j,i in enumerate(var_group):\n    \n    # calculating descriptives of variable\n    mini = data[i].min()\n    maxi = data[i].max()\n    ran = data[i].max()-data[i].min()\n    mean = data[i].mean()\n    median = data[i].median()\n    st_dev = data[i].std()\n    skew = data[i].skew()\n    kurt = data[i].kurtosis()\n\n    # calculating points of standard deviation\n    points = mean-st_dev, mean+st_dev\n\n    #Plotting the variable with every information\n    plt.subplot(1,size,j+1)\n    sns.kdeplot(data[i], shade=True)\n    sns.lineplot(points, [0,0], color = 'black', label = \"std_dev\")\n    sns.scatterplot([mini,maxi], [0,0], color = 'orange', label = \"min\/max\")\n    sns.scatterplot([mean], [0], color = 'red', label = \"mean\")\n    sns.scatterplot([median], [0], color = 'blue', label = \"median\")\n    plt.xlabel('{}'.format(i), fontsize = 20)\n    plt.ylabel('density')\n    plt.title('std_dev = {}; kurtosis = {};\\nskew = {}; range = {}\\nmean = {}; median = {}'.format((round(points[0],2),round(points[1],2)),\n                                                                                                   round(kurt,2),\n\n                                                                                                   round(skew,2),\n                                                                                                   (round(mini,2),round(maxi,2),round(ran,2)),\n                                                                                                   round(mean,2),\n                                                                                                   round(median,2)))\n","13921e28":"UVA_numeric(data,cab_details)","776c557d":"#using count function for passenger_count\ndata.passenger_count.value_counts()","e5721dbc":"#Let us remove the rows which have 0 or 7 or 9 passenger count.\n\ndata=data[data['passenger_count']!=0]\ndata=data[data['passenger_count']<=6]","d13c6d60":"data.shape","6080c194":"data.passenger_count.value_counts()","41450386":"#We will use sort function to sort distance variable in descending order\ndata['distance'].sort_values(ascending=False)\n","5d2e47c4":"#Lets use count function to find the number of trips with 0 km distance\ndata['distance'].value_counts()","ffbf9645":"# Lets drop all the distance below 200 meters and above 100 km\ndata=data[data['distance']>=0.2]\ndata=data[data['distance']<=100]","5ace92e8":"data.shape","8de15707":"UVA_numeric(data,pickup_details)","a038e49e":"UVA_numeric(data,dropoff_details)","b5244953":"#Lets look at the geospatial or location features to check consistency. They should not vary much as we are only considering trips within New York city.\n\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(2,2,figsize=(10, 10), sharex=False, sharey = False)\nsns.despine(left=True)\nsns.distplot(data['pickup_latitude'].values, label = 'pickup_latitude',color=\"b\",bins = 100, ax=axes[0,0])\nsns.distplot(data['pickup_longitude'].values, label = 'pickup_longitude',color=\"r\",bins =100, ax=axes[1,0])\nsns.distplot(data['dropoff_latitude'].values, label = 'dropoff_latitude',color=\"b\",bins =100, ax=axes[0,1])\nsns.distplot(data['dropoff_longitude'].values, label = 'dropoff_longitude',color=\"r\",bins =100, ax=axes[1,1])\nplt.setp(axes, yticks=[])\nplt.tight_layout()\nplt.show()","3135622d":"data = data.loc[(data.pickup_latitude > 40.6) & (data.pickup_latitude < 40.9)]\ndata = data.loc[(data.dropoff_latitude>40.6) & (data.dropoff_latitude < 40.9)]\ndata = data.loc[(data.dropoff_longitude > -74.05) & (data.dropoff_longitude < -73.7)]\ndata = data.loc[(data.pickup_longitude > -74.05) & (data.pickup_longitude < -73.7)]\ndata_data_new = data.copy()\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(2,2,figsize=(10, 10), sharex=False, sharey = False)#\nsns.despine(left=True)\nsns.distplot(data_data_new['pickup_latitude'].values, label = 'pickup_latitude',color=\"b\",bins = 100, ax=axes[0,0])\nsns.distplot(data_data_new['pickup_longitude'].values, label = 'pickup_longitude',color=\"r\",bins =100, ax=axes[0,1])\nsns.distplot(data_data_new['dropoff_latitude'].values, label = 'dropoff_latitude',color=\"b\",bins =100, ax=axes[1, 0])\nsns.distplot(data_data_new['dropoff_longitude'].values, label = 'dropoff_longitude',color=\"r\",bins =100, ax=axes[1, 1])\nplt.setp(axes, yticks=[])\nplt.tight_layout()\n\nplt.show()","0df944f0":"def UVA_category(data, var_group):\n\n  '''\n  Univariate_Analysis_categorical\n  takes a group of variables (category) and plot\/print all the value_counts and barplot.\n  '''\n  # setting figure_size\n  size = len(var_group)\n  plt.figure(figsize = (7*size,5), dpi = 100)\n\n  # for every variable\n  for j,i in enumerate(var_group):\n    norm_count = data[i].value_counts(normalize = True)\n    n_uni = data[i].nunique()\n\n  #Plotting the variable with every information\n    plt.subplot(1,size,j+1)\n    sns.barplot(norm_count, norm_count.index , order = norm_count.index)\n    plt.xlabel('fraction\/percent', fontsize = 20)\n    plt.ylabel('{}'.format(i), fontsize = 20)\n    plt.title('n_uniques = {} \\n value counts \\n {};'.format(n_uni,norm_count))\n    ","8ae66138":"UVA_category(data, ['store_and_fwd_flag'])  ","7443cb27":"UVA_category(data, ['vendor_id'])   ","55e7ea8d":"\ndata.isnull().sum()","5942f93e":"def UVA_outlier(data, var_group, include_outlier = True):\n  '''\n  Univariate_Analysis_outlier:\n  takes a group of variables (INTEGER and FLOAT) and plot\/print boplot and descriptives\\n\n  Runs a loop: calculate all the descriptives of i(th) variable and plot\/print it \\n\\n\n\n  data : dataframe from which to plot from\\n\n  var_group : {list} type Group of Continuous variables\\n\n  include_outlier : {bool} whether to include outliers or not, default = True\\n\n  '''\n\n  size = len(var_group)\n  plt.figure(figsize = (7*size,4), dpi = 100)\n  \n  #looping for each variable\n  for j,i in enumerate(var_group):\n    \n    # calculating descriptives of variable\n    quant25 = data[i].quantile(0.25)\n    quant75 = data[i].quantile(0.75)\n    IQR = quant75 - quant25\n    med = data[i].median()\n    whis_low = med-(1.5*IQR)\n    whis_high = med+(1.5*IQR)\n\n    # Calculating Number of Outliers\n    outlier_high = len(data[i][data[i]>whis_high])\n    outlier_low = len(data[i][data[i]<whis_low])\n\n    if include_outlier == True:\n      print(include_outlier)\n      #Plotting the variable with every information\n      plt.subplot(1,size,j+1)\n      sns.boxplot(data[i], orient=\"v\")\n      plt.ylabel('{}'.format(i))\n      plt.title('With Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low\/high) = {} \\n'.format(\n                                                                                                   round(IQR,2),\n                                                                                                   round(med,2),\n                                                                                                   (round(quant25,2),round(quant75,2)),\n                                                                                                   (outlier_low,outlier_high)\n                                                                                                   ))\n      \n    else:\n      # replacing outliers with max\/min whisker\n      df1 = data[var_group][:]\n      df1[i][df1[i]>whis_high] = whis_high+1\n      df1[i][df1[i]<whis_low] = whis_low-1\n      \n      # plotting without outliers\n      plt.subplot(1,size,j+1)\n      sns.boxplot(df1[i], orient=\"v\")\n      plt.ylabel('{}'.format(i))\n      plt.title('Without Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low\/high) = {} \\n'.format(\n                                                                                                   round(IQR,2),\n                                                                                                   round(med,2)\n                                                                                                   (round(quant25,2),round(quant75,2)),\n                                                                                                   (outlier_low,outlier_high)\n                                                                                                   ))  \n\n","44b34efd":"UVA_outlier(data,cab_details)","34fb9266":"UVA_outlier(data,pickup_details)","380c6c7e":"UVA_outlier(data,dropoff_details)","304120de":"numerical = data[['trip_duration','passenger_count','distance','speed','hour_of_day','day_of_week','dropday_of_week','drop_hour_of_day']]\n\n","0fcb28ea":"numerical.dtypes","7308f86b":"correlation = numerical.corr()\ncorrelation\n","281aa22d":"# plotting heatmap usill all methods for all numerical variables\nplt.figure(figsize=(36,6), dpi=140)\nfor j,i in enumerate(['pearson','kendall','spearman']):\n  plt.subplot(1,3,j+1)\n  correlation = numerical.dropna().corr(method=i)\n  sns.heatmap(correlation, linewidth = 2)\n  plt.title(i, fontsize=18)","c763c0a6":"def TwoSampZ(X1, X2, sigma1, sigma2, N1, N2):\n  '''\n  takes mean, standard deviation, and number of observations and returns p-value calculated for 2-sampled Z-Test\n  '''\n  from numpy import sqrt, abs, round\n  from scipy.stats import norm\n  ovr_sigma = sqrt(sigma1**2\/N1 + sigma2**2\/N2)\n  z = (X1 - X2)\/ovr_sigma\n  pval = 2*(1 - norm.cdf(abs(z)))\n  return pval\ndef TwoSampT(X1, X2, sd1, sd2, n1, n2):\n  '''\n  takes mean, standard deviation, and number of observations and returns p-value calculated for 2-sample T-Test\n  '''\n  from numpy import sqrt, abs, round\n  from scipy.stats import t as t_dist\n  ovr_sd = sqrt(sd1**2\/n1 + sd2**2\/n2)\n  t = (X1 - X2)\/ovr_sd\n  df = n1+n2-2\n  pval = 2*(1 - t_dist.cdf(abs(t),df))\n  return pval\n\ndef Bivariate_cont_cat(data, cont, cat, category):\n  #creating 2 samples\n  x1 = data[cont][data[cat]==category][:]\n  x2 = data[cont][~(data[cat]==category)][:]\n  \n  #calculating descriptives\n  n1, n2 = x1.shape[0], x2.shape[0]\n  m1, m2 = x1.mean(), x2.mean()\n  std1, std2 = x1.std(), x2.mean()\n  \n  #calculating p-values\n  t_p_val = TwoSampT(m1, m2, std1, std2, n1, n2)\n  z_p_val = TwoSampZ(m1, m2, std1, std2, n1, n2)\n\n  #table\n  table = pd.pivot_table(data=data, values=cont, columns=cat, aggfunc = np.mean)\n\n  #plotting\n  plt.figure(figsize = (15,6), dpi=140)\n  \n  #barplot\n  plt.subplot(1,2,1)\n  sns.barplot([str(category),'not {}'.format(category)], [m1, m2])\n  plt.ylabel('mean {}'.format(cont))\n  plt.xlabel(cat)\n  plt.title('t-test p-value = {} \\n z-test p-value = {}\\n {}'.format(t_p_val,\n                                                                z_p_val,\n                                                                table))\n\n  # boxplot\n  plt.subplot(1,2,2)\n  sns.boxplot(x=cat, y=cont, data=data)\n  plt.title('categorical boxplot')\n  \n  \n","dfb31b63":"Bivariate_cont_cat(data, 'trip_duration','vendor_id', 1)","2494510d":"Bivariate_cont_cat(data, 'trip_duration','store_and_fwd_flag', 'Y')","19844e35":"Bivariate_cont_cat(data, 'distance','store_and_fwd_flag', 'Y')","582ed35c":"Bivariate_cont_cat(data, 'speed','store_and_fwd_flag', 'Y')","24f538f2":"def BVA_categorical_plot(data, tar, cat):\n  '''\n  take data and two categorical variables,\n  calculates the chi2 significance between the two variables \n  and prints the result with countplot & CrossTab\n  '''\n  #isolating the variables\n  data = data[[cat,tar]][:]\n\n  #forming a crosstab\n  table = pd.crosstab(data[tar],data[cat],)\n  f_obs = np.array([table.iloc[0][:].values,\n                    table.iloc[1][:].values])\n\n  #performing chi2 test\n  from scipy.stats import chi2_contingency\n  chi, p, dof, expected = chi2_contingency(f_obs)\n  \n  #checking whether results are significant\n  if p<0.05:\n    sig = True\n  else:\n    sig = False\n\n  #plotting grouped plot\n  sns.countplot(x=cat, hue=tar, data=data)\n  plt.title(\"p-value = {}\\n difference significant? = {}\\n\".format(round(p,8),sig))\n\n  #plotting percent stacked bar plot\n  #sns.catplot(ax, kind='stacked')\n  ax1 = data.groupby(cat)[tar].value_counts(normalize=True).unstack()\n  ax1.plot(kind='bar', stacked='True',title=str(ax1))\n  int_level = data[cat].value_counts()\n  \n","d34ee17e":"BVA_categorical_plot(data, 'vendor_id', 'store_and_fwd_flag')","15187440":"#Missing values\n\npd.isnull(data).sum()","c2d89c55":"##Distance\nQ1 = data['distance'].quantile(0.25)\nQ3 = data['distance'].quantile(0.75)\n\nIQR = data['distance'].quantile(0.75) - data['distance'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","902ab26d":"#Replacing the outliers value by above whisker value\ndata['distance'] = np.where(data['distance'] >7.9, whisker_2,data['distance'])","4329c1ad":"##Speed\n\nQ1 = data['speed'].quantile(0.25)\nQ3 = data['speed'].quantile(0.75)\n\nIQR = data['speed'].quantile(0.75) - data['speed'].quantile(0.25)\nIQR\n\nwhisker_1 = Q1 - (1.5*IQR)\nwhisker_2 = Q3 + (1.5*IQR)\n\nwhisker_1, whisker_2","0d05b10f":"# Replacing the outliers with above whisker value\ndata['speed'] = np.where(data['speed'] >31, whisker_2,data['speed'])","9c65225d":"# Convert vendor_id and store_and_fwd_flag to one hot encoded features\ndata = pd.concat([data,pd.get_dummies(data['vendor_id'],prefix = str('vendor_id'),prefix_sep='_')],axis = 1)\ndata = pd.concat([data,pd.get_dummies(data['store_and_fwd_flag'],prefix = str('store_and_fwd_flag'),prefix_sep='_')],axis = 1)","dd0d8c79":"df = data[['passenger_count','trip_duration','day_of_week','hour_of_day','distance','speed','vendor_id_1','vendor_id_2','store_and_fwd_flag_N','store_and_fwd_flag_Y']].copy()","3d09970c":"df.describe()","acd1a3ed":"numericals = ['passenger_count','trip_duration','day_of_week','hour_of_day','distance','speed']","c9799551":"from sklearn.preprocessing import StandardScaler\n","a1382c81":"for i in numericals:\n    df[i] = np.log(df[i]+10)\nstd = StandardScaler()\nscaled = std.fit_transform(df[numericals])\nscaled = pd.DataFrame(scaled, columns = numericals)","207f999f":"df.columns","c53a902a":"df.head()","51ae636c":"x = df.drop(['trip_duration'], axis=1)\ny = df['trip_duration']\nx.shape, y.shape","a207ce4c":"# Importing the train test split function\nfrom sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y,test_y = train_test_split(x,y, random_state = 56)","bb92e97e":"test_x.shape","fcd65706":"#importing Linear Regression and metric mean square error\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.metrics import mean_absolute_error as mae","c062bad8":"lr = LR()\n\n# Fitting the model\nlr.fit(train_x, train_y)","222a43d4":"train_predict = lr.predict(train_x)\nk = mae(train_predict, train_y)\nprint('Training Mean Absolute Error', k )","702e0891":"test_predict = lr.predict(test_x)\nk = mae(test_predict, test_y)\nprint('Test Mean Absolute Error    ', k )","59559c0f":"lr.coef_","f2c88c38":"residuals.shape","a82936c1":"plt.figure(figsize=(10, 6), dpi=120, facecolor='w', edgecolor='b')\nf = range(0,179525)\nk = [0 for i in range(0,179525)]\nplt.scatter( f, residuals.residuals[:], label = 'residuals')\nplt.plot( f, k , color = 'red', label = 'regression line' )\nplt.xlabel('fitted points ')\nplt.ylabel('residuals')\nplt.title('Residual plot')\nplt.ylim(-5, 5)\nplt.legend()","7c4f8c9b":"plt.figure(figsize=(10, 6), dpi=120, facecolor='w', edgecolor='b')\nplt.hist(residuals.residuals, bins = 150)\nplt.xlabel('Error')\nplt.ylabel('Frequency')\nplt.title('Distribution of Error Terms')\nplt.show()","4ed4cd16":"# importing the QQ-plot from the from the statsmodels\nfrom statsmodels.graphics.gofplots import qqplot","cad3a34f":"\n\n## Plotting the QQ plot\nfig, ax = plt.subplots(figsize=(5,5) , dpi = 120)\nqqplot(residuals.residuals, line = 's' , ax = ax)\nplt.ylabel('Residual Quantiles')\nplt.xlabel('Ideal Scaled Quantiles')\nplt.title('Checking distribution of Residual Errors')\nplt.show()","a00c0d54":"# Importing Variance_inflation_Factor funtion from the Statsmodels\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n# Calculating VIF for every column (only works for the not Catagorical)\nVIF = pd.Series([variance_inflation_factor(df.values, i) for i in range(df.shape[1])], index =df.columns)\nVIF","3c6d6939":"df1 = df[['passenger_count','trip_duration','hour_of_day','day_of_week','vendor_id_1','vendor_id_2','store_and_fwd_flag_N','store_and_fwd_flag_Y']].copy()","9dfb963e":"df1","ad7e4255":"x1 = df1.drop(['trip_duration'], axis=1)\ny1 = df1['trip_duration']\nx1.shape, y1.shape","827a5781":"train_x1,test_x1,train_y1,test_y1 = train_test_split(x1,y1, random_state = 57)","70729ba3":"# Creating instance of Linear Regresssion\nlr1 = LR()\n\n# Fitting the model\nlr1.fit(train_x1, train_y1)","c1dc0360":"train_predict = lr1.predict(train_x1)\nk1 = mae(train_predict, train_y1)\nprint('Training Mean Absolute Error', k1 )","8036ba9b":"test_predict = lr1.predict(test_x1)\nk = mae(test_predict, test_y1)\nprint('Test Mean Absolute Error    ', k )","ee85e6a3":"lr1.coef_","2bd8bd08":"#Checking Assumptions of linear regression\n# Arranging and calculating the Residuals\nresiduals1 = pd.DataFrame({\n    'fitted values' : test_y1,\n    'predicted values' : test_predict,\n})\n\nresiduals1['residuals1'] = residuals1['fitted values'] - residuals1['predicted values']\nresiduals1.head()","b7bc63ac":"plt.figure(figsize=(10, 6), dpi=120, facecolor='w', edgecolor='b')\nf = range(0,179525)\nk = [0 for i in range(0,179525)]\nplt.scatter( f, residuals1.residuals1[:], label = 'residuals')\nplt.plot( f, k , color = 'red', label = 'regression line' )\nplt.xlabel('fitted points ')\nplt.ylabel('residuals')\nplt.title('Residual plot')\nplt.ylim(-5, 5)\nplt.legend()","2d15aa50":"plt.figure(figsize=(10, 6), dpi=120, facecolor='w', edgecolor='b')\nplt.hist(residuals1.residuals1, bins = 100)\nplt.xlabel('Error')\nplt.ylabel('Frequency')\nplt.title('Distribution of Error Terms')\nplt.show()","b1b2b9f0":"fig, ax = plt.subplots(figsize=(5,5) , dpi = 120)\nqqplot(residuals1.residuals1, line = 's' , ax = ax)\nplt.ylabel('Residual Quantiles')\nplt.xlabel('Ideal Scaled Quantiles')\nplt.title('Checking distribution of Residual Errors')\nplt.show()","9bc62dc5":"VIF = pd.Series([variance_inflation_factor(df1.values, i) for i in range(df1.shape[1])], index =df1.columns)\nVIF","27d688d7":"# Checking Assumptions of linear regression","61fc6b4e":"From this list, we clearly see that there happens to be one Independent Variable over the value of 5.5, which means that there is correlation that exhibit the Multicollinearity in the dataset.From above we can see that distance is causing multicollinearity problem. Lets seep what happens when we drop it","bed89c72":"#The histogram shows the data is highly skewed","f9aa35d0":"#Bivariate ANALYSIS : CATEGORICAL-CATEGORICAL VARIABLES ","7601134f":"#here we can see for weekday we have comparatively less number of rides on weekend which might be the result of holidays at maximum institutions. Also we can see that the peek happens on the \"Thursday\".\n\n#For hours of day we can see that the no trips starts decreasing after 12 midnigh and reaches its minimun around 5 A.M. and there after it starts increasing as people starts going to their workplace. The maximum rush happens towards the evening time.","aa20d8d5":"\n# Arranging and calculating the Residuals\nresiduals = pd.DataFrame({\n    'fitted values' : test_y,\n    'predicted values' : test_predict,\n})\n\nresiduals['residuals'] = residuals['fitted values'] - residuals['predicted values']\nresiduals.head()","8ad9902b":"# The Residual plot clearly Looks Homoscedastic, i.e. the the variance of the error across the dataset is nearly constant","d63b4077":"# we can see that their are outliers in all the varibales of cab details.","4d107091":"# Missing Values","296cf6b1":"# We can see that there is significant difference between trip duration of vendor 1 and vendor2. From above graph it can be inferred that the mean duration of trip is higher for vendor 2.","d0acd15d":"# UNIVARIATE ANALYSIS -CATEGORICAL VARIABLES\n","a0fc0723":"We see there are 2893 trips with 0 km distance.\n\nThe reasons for 0 km distance can be:\n\nThe dropoff location couldn\u2019t be tracked.\nThe driver deliberately took this ride to complete a target ride number.\nThe passengers canceled the trip.\n","28cf1424":"# Preprocessing of Data","0699dd17":"# BIVARIATE ANALYSIS - CONTINUOUS CONTINUOUS VARIABLES","891ea0a9":"# According to the Histogram, the distribution of error is nearly normal, But there are some outliers on the lower end and higher end of the errors.","808201ed":"# Findings - (Here, red represents pickup and dropoff Longitudes & blue represents pickup & dropoff lattitudes)\n\n1. From the plot above it is clear that pick and drop latitude are centered around 40 to 41, and longitude are situated around -74 to -73. \n2. Some extreme co-ordinates has squeezed the plot such that we see a spike here\n3. A good idea is to remove these outliers and look at the distribution more closely","95a418d8":"# Creating instance of Linear Regresssion\n","162622b8":"#There is no difference between the speed of trip with and without network issue.","e87cdd79":"# Here we can see that \"vendor 2\" has larger share in total trips.","bb76c34f":"# There are 33 trips with 0 passenger count and there is only 1 trip each for 7 and 9 passengers.","85364828":"# here we have to predict total trip duration. Therefore the droppoff variables will have no relevance in predicting the total trip duration. So, we will make a new data frame excluding dropp off variables. ALso we have used pickup latitude, pickup longitude, dropoff latitude and dropoff longitude in calculating distance. So we will also drop it.","f598af03":"# BIVARIATE ANALYSIS - CONTINUOUS CATEGORICAL VARIABLES","c5456cdf":"# Now errors are normally distributed","0e868b02":"# INTRODUCING DUMMIES TO CATEGORICAL VARIABLES","d3dcce94":"#We see that id has 729322 unique values which are equal to the number of rows in our dataset.\n#There are 2 unique vendor ids.\n#There are 9 unique passenger counts.\n#There are 2 unique values for store_and_fwd_flag,which are Y and N.","36f3f2de":"We can clearly see that in very less number of trips there was issue of network problems","7d72aed7":"# Here we can see that the trip which has network issue takes longer duration than the trip with no network issues.","6609daf3":"# QQ Plots also confirm normality of residuals. ","fa4c442d":"# The QQ-plot clearly verifies our findings from the the histogram of the residuals, the data is mostly normal in nature, but there sre some outliers on the higher end of the Residues.","a3a53ebb":"# Residuals here also are homoscedastic in nature","b94d22b5":"# Here we can see that our data set do not have any missing values.","9def385a":"# Cab_details\n#Passenger_count:\nIT tells us that most of the ride compramised of single individual. Also it tell us there are some trips with even 0 passenger count and very few  trip with people comprising 4 or more people.\nFurther we will use count function to determine the no of trips with different individuals.\n#Distance:\n\n#Average_distance of trip is around 3.5km  and we can see that it is highly skewed. But one thing to point out that there are many trips with 0 km distance.So, we will have to use count function and sort function to determine both the no of trips with 0 km and any inappropriate data respectively.\n\n#Speed:\nIn speed plot we can see that like distance it has a larger density at 0 speed and highly positive skewnees tells other side of the story. As we have calculated the speed using distance variable. Therefore outliers in distance might have lead to outliers in the speed also.\n","d5ea8aaf":"# 1. Exploratory Data Analysis, Feature Engineering, Preprocessing and Machine Learning Techniques\nLet's check the data files! According to the data description we should find the following columns:\n\n - **id** - a unique identifier for each trip\n - **vendor_id** - a code indicating the provider associated with the trip record\n - **pickup_datetime** - date and time when the meter was engaged\n - **dropoff_datetime** - date and time when the meter was disengaged\n - **passenger_count** - the number of passengers in the vehicle (driver entered value)\n - **pickup_longitude** - the longitude where the meter was engaged\n - **pickup_latitude** - the latitude where the meter was engaged\n - **dropoff_longitude** - the longitude where the meter was disengaged\n - **dropoff_latitude** - the latitude where the meter was disengaged\n - **store_and_fwd_flag** - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server (Y=store and forward; N=not a store and forward trip)\n - **trip_duration** - (target) duration of the trip in seconds\n\n","338e75a4":"#The box plot shows that clearly we have an outlier","e7c65670":"# we can infer from above that \"vendor 1\" mainly has problem with network related issues. This is also statistically significant.","54aa11c6":"# OUTLIER TREATMENT","55990e97":"# UNIVARIATE ANALYSIS - NUMERICAL\n","de21d9d5":"#we can see that the dropday_of_week and drop_hour_of_day follows the same pattern as the pickup week and pickup hours","ce2bceb6":"# our data set do not contain any missing values. ","e05d069c":"\n# There seems to be high correlation between pickup and dropoff variables.","a6d5b528":"# here we can see that the trip which has network issue travells more distance.","99df49ef":"# List of Hypothesis and investigation to perform under this combination.  \n#1Are trips with lost signals result in more time duration to complete the trip?\n#2Are trips with lost signasl result in more distance travelled?\n#3Is there is difference in time taken to complete the trip by different vendors?\n#4Is there significant difference in average speed of trip?","9003a66b":"# We can see that large no of trips are with 0 distance and few trips with more than 100 km"}}