{"cell_type":{"814d14c6":"code","0def4099":"code","845431d6":"code","71f40816":"code","71adc5c0":"code","34f1ba6e":"code","602082ae":"code","300aa67b":"code","126a3410":"code","2b8b0d5e":"code","bc25e89f":"code","36210a7f":"code","951d5346":"code","1ec715cf":"code","cad804c9":"code","37785804":"code","b65abd72":"code","a5c3f743":"code","81123497":"code","c1fee248":"code","fa8671b5":"code","8b5e442d":"code","111e38af":"code","465270c2":"code","d53a4c64":"code","eeb1ddf9":"code","024893a7":"code","51a2729b":"code","a72768c9":"code","3706f198":"code","5cac8f70":"code","d2bf6e94":"code","e90fa2b8":"code","99a6b1fc":"code","0193df97":"code","da60b02b":"code","5d049877":"code","aa003f94":"code","2dea88fd":"code","321d598d":"code","26a03acf":"code","46303ab3":"code","48468bf2":"code","bc1dc0ce":"code","8f805ce5":"code","a395d3ab":"code","d33491d5":"code","25235b0b":"code","6ecc47ca":"markdown","4abea6cc":"markdown","5875cdd9":"markdown","9be673d1":"markdown","57305807":"markdown","0728f4e0":"markdown","f7c9af4b":"markdown","8b080087":"markdown","fca2a22e":"markdown","b8d7bc38":"markdown","f0c57c72":"markdown","07ab501f":"markdown","74fa2885":"markdown","89886b2f":"markdown","18ab8fab":"markdown","9c1244a9":"markdown","3fdbd20b":"markdown","26ea9dbf":"markdown","dbc00bc4":"markdown","88aea5c8":"markdown","9fa3beef":"markdown","ae20b277":"markdown","f3fb6404":"markdown","4ef38be6":"markdown","0bcbe6a0":"markdown"},"source":{"814d14c6":"#Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plots\nimport sklearn as sk # models\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom string import punctuation # punctuation array\nfrom xgboost import XGBRegressor # XGBoost Regression\nfrom xgboost import plot_importance\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n%matplotlib inline \nimport gc, warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\ndef print_files():\n    import os\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\ndef downcast_dtypes(df, inplace=False):\n    '''\n    input  df: object\n    output df: object\n    \n    reject size of col type\n    '''\n    if not inplace:\n        data = df.copy()\n    else:\n        data = df\n    float_cols = [c for c in data if data[c].dtype in [\"float32\", \"float64\"]]\n    int_cols = [c for c in data if data[c].dtype in [\"int64\", \"int32\"]]\n    data[float_cols] = data[float_cols].astype(np.float16)\n    data[int_cols] = data[int_cols].astype(np.int16)\n    return data","0def4099":"print_files()\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nsales_train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv', index_col=\"ID\")\nsample_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')","845431d6":"# Reduse data size\ndowncast_dtypes(sales_train, inplace=True)\nprint(sales_train.info())\n# No missing values in sales_train\nprint(\"Num of missing values in sales_train: %d\" %sales_train.isnull().sum().sum())\n# 6 duplicated rows in sales_train\nprint(\"Num of duplicated rows in sales_train: %d\" %sales_train.duplicated().sum())","71f40816":"fig = plt.figure(figsize=(25,5))\n(ax1,ax2) = fig.subplots(1,2)\nax1.boxplot(x=sales_train.item_price, vert=False)\nax1.set_xlabel(\"item_price\")\nax2.boxplot(x=sales_train.item_cnt_day, vert=False)\nax1.set_xlabel(\"item_cnt_day\")\nprint(\"item_price outliers item_id\", *sales_train[sales_train.item_price>45000].index.values)\nprint(\"item_cnt_day outliers item_id\", *sales_train[sales_train.item_cnt_day>999].index.values)","71adc5c0":"print(\"Price less then zero index\", *sales_train[sales_train.item_price<0].index.values)\nmed = sales_train[(sales_train.shop_id==32)&(sales_train.item_id==11365)\\\n            &(sales_train.item_price>0)&(sales_train.date_block_num==4)].median()\nsales_train.iloc[484683] = med","34f1ba6e":"train = sales_train[sales_train.item_price<100000][sales_train.item_cnt_day<1001]","602082ae":"print(shops.iloc[np.r_[10,11,23,24,39,40,0,57,1,58]])","300aa67b":"# Map duplicated shop_id through a dictionary\nd = {0:57, 1:58, 10:11, 23:24, 39:40}\nshops[\"shop_id\"] = shops[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\nsales_train[\"shop_id\"] = sales_train[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\ntest[\"shop_id\"] = test[\"shop_id\"].apply(lambda x: d[x] if x in d.keys() else x)\n# Remove all punctuation in shop_name\nshops['shop_name_c'] = shops['shop_name'].apply(lambda string: \"\".join([pt for pt in string if pt not in punctuation])) \nshops['shop_name_c'] = shops['shop_name_c'].str.lower()\n\n#Exctract new features\n\n#shop_type\nshops['shop_type'] = shops['shop_name_c'].apply(lambda x: '\u043c\u0442\u0440\u0446' if '\u043c\u0442\u0440\u0446' in x else '\u0442\u0440\u0446' if '\u0442\u0440\u0446' in x else '\u0442\u0440\u043a' if '\u0442\u0440\u043a' in x else '\u0442\u0446' if '\u0442\u0446' in x else '\u0442\u043a' if '\u0442\u043a' in x else '\u0442\u0446')\n\n#shop_city\nshops['shop_city'] = shops['shop_name_c'].str.partition(' ')[0]\n","126a3410":"#OneHotEncoder for shop_type\nOHE = OneHotEncoder(handle_unknown='ignore', dtype=np.int8)\nOHE.fit(np.array(shops['shop_type'].unique()).reshape(len(shops['shop_type'].unique()), 1))\nOneHot_transform = OHE.transform(np.array(shops['shop_type']).reshape(-1, 1))\n#prepare to megre\nOneHot_transform = pd.DataFrame(data=OneHot_transform.toarray(), columns=[\"lab_mtrc\",\"lab_tk\",\"lab_trk\",\"lab_trc\",\"lab_tc\"])\n\nshops_merged = pd.merge(shops, OneHot_transform, how='left',left_on=\"shop_id\", right_on=OneHot_transform.index)","2b8b0d5e":"#LabelEncoding shop_city\nshops_merged['shop_city_code'] = LabelEncoder().fit_transform(shops['shop_city'])","bc25e89f":"shop_prepared = shops_merged.drop([\"shop_name\",\"shop_name_c\", \"shop_type\", \"shop_city\"], axis=1)\ngc.collect()","36210a7f":"#(84, 2)\nitem_categories.shape\n#No duplicates\nitem_categories[\"item_category_name\"].duplicated().sum()\nitem_categories.rename(columns={'item_category_id': 'category_item_id'}, inplace=True)\nitems.rename(columns={'item_category_id': 'category_item_id'}, inplace=True)","951d5346":"pd.options.display.max_rows = item_categories.shape[0]","1ec715cf":"#Clean type and encode category freature\nitem_categories[\"category\"] = item_categories[\"item_category_name\"]\\\n                            .str.split(\"-\")\\\n                            .map(lambda x: x[0].strip())\nitem_categories[\"category_code\"] = LabelEncoder().fit_transform(item_categories['category'])\n#Extract subcategory\nitem_categories[\"subcat\"] = item_categories[\"item_category_name\"]\\\n                            .str.split(\"-\")\\\n                            .map(lambda x: x[1].strip() if len(x) > 1 else 'no_SUB') # 0 -> 1\nitem_categories[\"subcat_code\"] = LabelEncoder().fit_transform(item_categories['subcat'])\nitem_categories_prepared = item_categories.drop([\"item_category_name\",\"category\", \"subcat\"], axis=1)","cad804c9":"items","37785804":"itemsa = items.copy()\nimport re\nitemsa.item_name = itemsa.item_name.map(lambda x: re.search(r\"(\\(.*?\\))|(\\[(.*?)\\])\", x))\nitemsa.item_name = itemsa.item_name.map(lambda x: (x.group() if type(x)!=type(None) else \"no_feature\"))\nitemsa.item_name = itemsa.item_name.map(lambda x: x.strip(\"().[]+\")) \nitems[\"item_name_add_code\"] = LabelEncoder().fit_transform(itemsa.item_name)\nitems_prepared = items.drop([\"item_name\"],axis=1)","b65abd72":"test.item_id.nunique() * test.shop_id.nunique()","a5c3f743":"from itertools import product\nproduct_train = []\ncolms = ['date_block_num', 'shop_id', 'item_id']\nfor m in range(34):\n    sales = sales_train[sales_train.date_block_num==m]\n    product_train.append(np.array(list(product([m], sales.shop_id.unique(), sales.item_id.unique()))))\nproduct_train = pd.DataFrame(np.vstack(product_train), columns=colms)\nproduct_train.sort_values(colms, inplace=True)","81123497":"group = sales_train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nproduct_train = pd.merge(product_train, group, on=colms, how='left')","c1fee248":"product_train.item_cnt_month = product_train.item_cnt_month.fillna(0).clip(0,20)","fa8671b5":"test[\"date_block_num\"] = 34","8b5e442d":"product_train = pd.concat([product_train, test], ignore_index=True, sort=False, keys=colms)\nproduct_train.fillna(0, inplace=True)","111e38af":"product_train = pd.merge(product_train, shop_prepared, on=['shop_id'], how='left')\nproduct_train = pd.merge(product_train, items_prepared, on=['item_id'], how='left')\nproduct_train = pd.merge(product_train, item_categories_prepared, on=['category_item_id'], how='left')","465270c2":"product_train = product_train.drop_duplicates().reset_index(drop=True)","d53a4c64":"def lag_feature(df, lags, col):\n    '''\n    General function for compute lags.\n    input: df: pd.DataFrame, lags: list of lags, col: name of lagging column(string)\n    output: df with LAGged feature(s)\n    '''\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","eeb1ddf9":"# Data downcast\nfor x in ['date_block_num','shop_id', 'lab_mtrc', 'lab_tk', 'lab_trk', 'lab_trc', 'lab_tc', 'shop_city_code', 'category_item_id', 'category_code', 'subcat_code']:\n    product_train[x] = product_train[x].astype(np.int8)\n    \nproduct_train['item_name_add_code'] = product_train['item_name_add_code'].astype(np.int16)\nproduct_train['item_cnt_month'] = product_train['item_cnt_month'].astype(np.float16)\nproduct_train['item_id'] = product_train['item_id'].astype(np.int16)","024893a7":"product_train = lag_feature(product_train, [1,2,3,7,12], 'item_cnt_month')","51a2729b":"product_train.columns.to_frame()","a72768c9":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize = (16,6), dpi = 80)\nproduct_train.item_cnt_month = product_train.item_cnt_month.astype(np.int16)\ncols = ['subcat_code']\nplot_acf(product_train.groupby(['date_block_num', *cols]).agg({'item_cnt_month': ['mean']}), ax = ax1, lags = 13)\nplot_pacf(product_train.groupby(['date_block_num', *cols]).agg({'item_cnt_month': ['mean']}), ax = ax2, lags = 13);\n","3706f198":"def mean_encode(df, grop_b:list, lags:list):\n    grouped = df.groupby(['date_block_num', *grop_b]).agg({'item_cnt_month': ['mean']})\n    col_name = [str_part.split('_')[0] for str_part in grop_b]\n    col_name = str('date_'+'_'.join(col_name)+'_avg_item_cnt')\n    grouped.columns = [col_name]\n    grouped.reset_index(inplace=True)\n\n    df = pd.merge(df, grouped, on=['date_block_num',*grop_b], how='left')\n    df[col_name] = df[col_name].astype(np.float16)\n    df = lag_feature(df, lags, col_name)\n    df.drop([col_name], axis=1, inplace=True)\n    return df","5cac8f70":"%%time\nproduct_train = mean_encode(product_train, ['item_id'], [1,2,3])\nproduct_train = mean_encode(product_train, ['shop_id','category_item_id'], [1,6])\nproduct_train = mean_encode(product_train, ['shop_id','shop_city_code'], [1,6])\nproduct_train = mean_encode(product_train, ['shop_city_code'], [1])\nproduct_train = mean_encode(product_train, ['item_id', 'category_item_id'], [1])\nproduct_train = mean_encode(product_train, ['category_item_id'], [1,2,3])\nproduct_train = mean_encode(product_train, ['subcat_code'], [1])\n","d2bf6e94":"group = sales_train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nproduct_train = pd.merge(product_train, group, on=['item_id'], how='left')\nproduct_train['item_avg_item_price'] = product_train['item_avg_item_price'].astype(np.float16)\n\ngroup = sales_train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nproduct_train = pd.merge(product_train, group, on=['date_block_num','item_id'], how='left')\nproduct_train['date_item_avg_item_price'] = product_train['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nproduct_train = lag_feature(product_train, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    product_train['delta_price_lag_'+str(i)] = \\\n        (product_train['date_item_avg_item_price_lag_'+str(i)] - product_train['item_avg_item_price']) \/ product_train['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nproduct_train['delta_price_lag'] = product_train.apply(select_trend, axis=1)\nproduct_train['delta_price_lag'] = product_train['delta_price_lag'].astype(np.float16)\nproduct_train['delta_price_lag'].fillna(0, inplace=True)\n\n# https:\/\/stackoverflow.com\/questions\/31828240\/first-non-null-value-per-row-from-a-list-of-pandas-columns\/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nproduct_train.drop(fetures_to_drop, axis=1, inplace=True)\n","e90fa2b8":"sales_train['revenue'] = sales_train['item_price'] *  sales_train['item_cnt_day']","99a6b1fc":"# Month revenue trend in shop\ngroup = sales_train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nproduct_train = pd.merge(product_train, group, on=['date_block_num','shop_id'], how='left')\nproduct_train['date_shop_revenue'] = product_train['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nproduct_train = pd.merge(product_train, group, on=['shop_id'], how='left')\nproduct_train['shop_avg_revenue'] = product_train['shop_avg_revenue'].astype(np.float32)\n\nproduct_train['delta_revenue'] = (product_train['date_shop_revenue'] - product_train['shop_avg_revenue']) \/ product_train['shop_avg_revenue']\nproduct_train['delta_revenue'] = product_train['delta_revenue'].astype(np.float16)\n\nproduct_train = lag_feature(product_train, [1], 'delta_revenue')\n\nproduct_train.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","0193df97":"%%time\ncache = {}\nproduct_train['item_shop_last_sale'] = -1\nproduct_train['item_shop_last_sale'] = product_train['item_shop_last_sale'].astype(np.int8)\nfor idx, row in product_train.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        product_train.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num       ","da60b02b":"product_train['item_shop_first_sale'] = product_train['date_block_num'] - product_train.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nproduct_train['item_first_sale'] = product_train['date_block_num'] - product_train.groupby('item_id')['date_block_num'].transform('min')","5d049877":"# ADD month\nproduct_train['month'] = product_train['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n# ADD day in month respectively\nproduct_train['days'] = product_train['month'].map(days).astype(np.int8)","aa003f94":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nproduct_train = product_train[product_train.date_block_num > 11]\nproduct_train = fill_na(product_train)","2dea88fd":"product_train.info()","321d598d":"import pickle\nproduct_train.to_pickle('data.pkl')\ndel cache\ndel OneHot_transform\ndel shops_merged\ndel shop_prepared\ndel item_categories_prepared\ndel group\ndel items_prepared\n# del test\ndel items\ndel shops\ndel item_categories\ndel sales_train\ngc.collect();","26a03acf":"dataset = pd.read_pickle('\/kaggle\/input\/datapkl\/data.pkl')","46303ab3":"dataset = dataset[['date_block_num',\n       'shop_id',\n       'item_id',\n       'item_cnt_month',\n#        'lab_mtrc',\n#        'lab_tk',\n#        'lab_trk',\n#        'lab_trc',\n#        'lab_tc',\n       'shop_city_code',\n       'category_item_id',\n       'item_name_add_code',\n       'category_code',\n       'subcat_code',\n       'item_cnt_month_lag_1',\n       'item_cnt_month_lag_2',\n       'item_cnt_month_lag_3',\n#        'item_cnt_month_lag_7',\n#        'item_cnt_month_lag_12',\n       'date_item_avg_item_cnt_lag_1',\n       'date_item_avg_item_cnt_lag_2',\n       'date_item_avg_item_cnt_lag_3',\n       'date_shop_category_avg_item_cnt_lag_1',\n#        'date_shop_category_avg_item_cnt_lag_6',\n       'date_shop_shop_avg_item_cnt_lag_1',\n#        'date_shop_shop_avg_item_cnt_lag_6',\n       'date_shop_avg_item_cnt_lag_1',\n       'date_item_category_avg_item_cnt_lag_1',\n       'date_category_avg_item_cnt_lag_1',\n       'date_category_avg_item_cnt_lag_2',\n       'date_category_avg_item_cnt_lag_3',\n       'date_subcat_avg_item_cnt_lag_1',\n       'delta_price_lag',\n       'delta_revenue_lag_1',\n       'item_shop_last_sale',\n       'item_shop_first_sale',\n       'item_first_sale',\n       'month',\n       'days'\n        ]]","48468bf2":"X_train = dataset[dataset.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = dataset[dataset.date_block_num < 33]['item_cnt_month']\nX_valid = dataset[dataset.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = dataset[dataset.date_block_num == 33]['item_cnt_month']\nX_test = dataset[dataset.date_block_num == 34].drop(['item_cnt_month'], axis=1)","bc1dc0ce":"del dataset\ngc.collect();","8f805ce5":"xbg_model = XGBRegressor(\n    tree_method = 'gpu_hist',\n    max_depth=10,\n    n_estimators=800,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=29)\n\nxbg_model.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","a395d3ab":"from catboost import CatBoostRegressor\ncatboost_model = CatBoostRegressor(\n    iterations=1000,\n    max_ctr_complexity=8,\n    random_seed=29,\n    od_type='Iter',\n    od_wait=25,\n    verbose=50,\n    task_type='GPU',\n    loss_function='RMSE',\n    learning_rate = 0.3\n)\ncatboost_model.fit(\n    X_train, Y_train,\n    eval_set=(X_valid, Y_valid)\n)","d33491d5":"##XGBoost\n# from sklearn.model_selection import GridSearchCV\n# xgb = XGBRegressor(\n#     tree_method = 'gpu_hist'\n#     )\n# params = {'max_depth':[4,8,10],\n#     'n_estimators':[800,1000],\n#     'min_child_weight':[300], \n#     'colsample_bytree':[0.7,0.8], \n#     'subsample':[0.8], \n#     'eta':[0.1,0.2,0.3],    \n#     'seed':[27]}\n# model = GridSearchCV(xgb, param_grid=params, n_jobs=1)\n\n\n# model.fit(\n#     X_train, \n#     Y_train,  \n#     eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n#     verbose=10, \n#     eval_metric='rmse',\n#     early_stopping_rounds = 5)\n\n##CatBoost\n\n# from catboost import CatBoostRegressor\n# catboost_model = CatBoostRegressor()\n# params = {\n#     'iterations':[1000,800],\n#     'max_ctr_complexity':[8,5,6],\n#     'random_seed':[29],\n#     'od_type':['Iter'],\n#     'od_wait':[25,15,20],\n#     'verbose':[50],\n#     'task_type':['GPU'],\n#     'loss_function':['RMSE'],\n# }\n# catboost_model.fit(\n#     X_train, Y_train,\n#     eval_set=(X_valid, Y_valid)\n# )\n","25235b0b":"Y_pred_xgb = xbg_model.predict(X_valid).clip(0, 20)\nY_pred_cat = catboost_model.predict(X_valid).clip(0, 20)\n\nY_test_xgb = xbg_model.predict(X_test).clip(0, 20)\nY_test_cat = catboost_model.predict(X_test).clip(0, 20)\n\nX_lev2_train = pd.DataFrame(np.array([Y_pred_xgb,Y_pred_cat]).T, columns=['XBG','CAT'])\nX_lev2_test = pd.DataFrame(np.array([Y_test_xgb,Y_test_cat]).T, columns=['XBG','CAT'])\n\nmodel_level2 =  LinearRegression()\nmodel_level2.fit(X_lev2_train, Y_valid)\nY_answer = model_level2.predict(X_lev2_test).clip(0, 20)\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_answer\n})\nsubmission.to_csv('submission.csv', index=False)\n","6ecc47ca":"Months since the last sale for each shop\/item pair and for item only.","4abea6cc":"Train 13-33   \nValidation 34  \nTest 35  ","5875cdd9":"## Hyperparameter tuning","9be673d1":"\n## Trend item price features\n> I was looking for the closest not nullable price, to compare it with the avg price. If an item costs less than in the past - its is a positive thend, otherwise - negative. The more difference between closest price and avg price - the more trend value.","57305807":"Append test set as 34th month","0728f4e0":"New duplicated rows appeared, drop them!","f7c9af4b":"## Joining to train\n<hr>\nSo, what do we have on this moment?\nWe've prepared data set with standart features. Now we going to concatinate all data in one set  \n  \nBut first look at the test set. It has shape (214200,1). There is product of some shops and some items within 34 month.","8b080087":"### Sales_train outliers","fca2a22e":"## LAG FEATURES  \n### item_cnt_month lag","b8d7bc38":"## Items processing\n<hr>\nWe encode `additional` feature in brackets () - (UNV).","f0c57c72":"## Fill na in lag and del first 12 month\nIn order to haven't data leakage","07ab501f":"Join other data to set","74fa2885":"Months since the first sale for each shop\/item pair and for item only.","89886b2f":"## Sales_train processing\n<hr>","18ab8fab":"The structure of the `shops`  \nShop City | Shop type | Shop name\n\nHere we clean duplicates and extract feature `shop_type` , `shop_city`, `shop_name_c`  \nEncode all data with LabelEncoder()","9c1244a9":"There are items with large prices and sales.   \nI've researched it in details and decided to remove items with price > 100000 and sales > 1001  \nWe have item_name = '\u0414\u043e\u0441\u0442\u0430\u0432\u043a\u0430 (EMS)' - delivery in every case.","3fdbd20b":"## Dataset prepared","26ea9dbf":" Test set has 363 unique `item_id`, they were met in train set newer. We can calculate monthly sales for train set and extend it with *zero* sales for each unique pair. Following this idea the train set will be similar to test set.","dbc00bc4":"## Stacking","88aea5c8":"## Item_categories processing\n<hr>","9fa3beef":"## Mean encoded features\n\nCalculate mean encoded features and find most interesting lags with acf and pacf plots","ae20b277":"## Shops processing\n<hr>","f3fb6404":"### Date features","4ef38be6":"Now need to aggregate train set by shop\/item pairs to calculate target sum, then clip(0,20) target value. This way train target will be similar to the test predictions.","0bcbe6a0":"\u0421arefully examining `shops`, you can see that there are duplicates  \n"}}