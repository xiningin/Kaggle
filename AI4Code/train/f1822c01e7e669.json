{"cell_type":{"564b8d1f":"code","0d1e3d00":"code","f102b7e4":"code","c446b8f8":"code","ee5f06e0":"code","ca779300":"code","38089ce7":"code","9176c035":"code","d659564d":"code","8b8ab27f":"code","50c80052":"code","516738ab":"code","dc6791a8":"code","70d97a16":"code","43bcd52c":"code","f0f550cc":"code","291e0ea8":"code","21731330":"code","73a9cc06":"code","2b1c14ba":"code","ff3a9d0c":"code","513df0de":"code","e0061662":"code","f792605d":"code","48f98a75":"markdown","f95f94c5":"markdown","e7d445f4":"markdown","97db2c14":"markdown","61b6918c":"markdown","d9d9d66e":"markdown","206fc6e1":"markdown","cbe5f2d5":"markdown","ad9e3975":"markdown","7ff62957":"markdown","f234e5a0":"markdown","2e031f65":"markdown","ef87bebb":"markdown"},"source":{"564b8d1f":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt","0d1e3d00":"training_set_base_path = \"..\/input\/cat-and-dog\/training_set\/training_set\/\"\nvalidation_set_base_path = \"..\/input\/cat-and-dog\/test_set\/test_set\/\"\n\ntraining_cat_path = training_set_base_path + \"cats\"\ntraining_dog_path = training_set_base_path + \"dogs\"\nvalidation_cat_path = validation_set_base_path + \"cats\"\nvalidation_dog_path = validation_set_base_path + \"dogs\"","f102b7e4":"print(\"Number of training images: \", len(os.listdir(training_cat_path)) + len(os.listdir(training_dog_path)))\nprint(\"Number of validation images: \", len(os.listdir(validation_cat_path)) + len(os.listdir(validation_dog_path)))","c446b8f8":"training_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1\/255)","ee5f06e0":"training_generator = training_gen.flow_from_directory(directory=training_set_base_path,\n                                                      target_size=(400, 400),\n                                                      class_mode=\"categorical\",\n                                                      batch_size=32)","ca779300":"training_gen_aug = keras.preprocessing.image.ImageDataGenerator(rescale=1\/255,\n                                                                rotation_range=90,\n                                                                width_shift_range=0.2,\n                                                                height_shift_range=0.2,\n                                                                shear_range=0.2,\n                                                                zoom_range=0.2,\n                                                                fill_mode=\"nearest\",\n                                                                horizontal_flip=True)","38089ce7":"training_generator_aug = training_gen_aug.flow_from_directory(directory=training_set_base_path,\n                                                      target_size=(400, 400),\n                                                      class_mode=\"categorical\",\n                                                      batch_size=32)","9176c035":"validation_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1\/255)","d659564d":"validation_generator = validation_gen.flow_from_directory(directory=validation_set_base_path,\n                                                          target_size=(400, 400),\n                                                          class_mode=\"categorical\",\n                                                          batch_size=32)","8b8ab27f":"inputs = keras.layers.Input(shape=(400, 400, 3))\nx = keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(inputs)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(units=128, activation=tf.nn.relu)(x)\nx = keras.layers.Dense(units=64, activation=tf.nn.relu)(x)\nx = keras.layers.Dense(units=32, activation=tf.nn.relu)(x)\nx = keras.layers.Dense(units=16, activation=tf.nn.relu)(x)\noutputs = keras.layers.Dense(units=2, activation=tf.nn.softmax)(x)","50c80052":"model = keras.models.Model(inputs=inputs, outputs=outputs)","516738ab":"model.summary()","dc6791a8":"model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"acc\"])","70d97a16":"pass","43bcd52c":"model.save_weights(\"initial_model.h5\")","f0f550cc":"history = model.fit(x=training_generator,\n          epochs=200,\n          steps_per_epoch=64,\n          validation_data=validation_generator,\n          validation_steps=32)","291e0ea8":"with open(\"history.json\", \"w\") as file:\n    json.dump(history.history, file)","21731330":"model.load_weights(\"initial_model.h5\")\nhistory_aug = model.fit(x=training_generator_aug,\n          epochs=200,\n          steps_per_epoch=64,\n          validation_data=validation_generator,\n          validation_steps=32)","73a9cc06":"with open(\"history_aug.json\", \"w\") as file:\n    json.dump(history_aug.history, file)","2b1c14ba":"with open(\"history.json\", 'r') as file:\n    data = file.read()\n\nhistory = json.loads(data)","ff3a9d0c":"with open(\"history_aug.json\", 'r') as file:\n    data = file.read()\n\nhistory_aug = json.loads(data)","513df0de":"training_acc = history[\"acc\"]\ntraining_val_acc = history[\"val_acc\"]\naug_training_acc = history_aug[\"acc\"]\naug_training_val_acc = history_aug[\"val_acc\"]\nepochs = list(range(len(training_acc)))","e0061662":"plt.plot(epochs, training_acc, 'bo', label=\"Training Acc\")\nplt.plot(epochs, training_val_acc, 'b', label=\"Validation Acc\")\nplt.title(\"Training Acc vs Validation Acc without Augmentation\")\nplt.legend()\nplt.show()","f792605d":"plt.plot(epochs, aug_training_acc, 'bo', label=\"Training Acc\")\nplt.plot(epochs, aug_training_val_acc, 'b', label=\"Validation Acc\")\nplt.title(\"Training Acc vs Validation Acc with Augmentation\")\nplt.legend()\nplt.show()","48f98a75":"### Compilation","f95f94c5":"### Architecture","e7d445f4":"## Import Package","97db2c14":"## Define Model","61b6918c":"## Save Model","d9d9d66e":"## Create ImageDataGenerator for Training with Augmentation","206fc6e1":"## Load Dataset","cbe5f2d5":"## Train Model with Non-Augmented Data","ad9e3975":"## Compare Two Training Process","7ff62957":"## Train Model with Augmented Data","f234e5a0":"## Create ImageDataGenerator for Testing","2e031f65":"## Create ImageDataGenerator for Training without Augmentation","ef87bebb":"### Callback"}}