{"cell_type":{"6f4b1e12":"code","5a3d25bb":"code","b34c25e4":"code","18142210":"code","c5ac4f5a":"code","d705b58c":"code","ac514835":"code","7d712918":"code","8f3feb35":"code","92f6939b":"code","60e2a5fa":"code","96584c09":"code","5ab09a2b":"code","d0a905ef":"code","c28671c2":"code","29f89646":"code","8338fc21":"code","a33d8fe8":"code","301e87e3":"code","d4678ac4":"code","a032ec16":"markdown","88857a93":"markdown","639edb5e":"markdown","40156101":"markdown","92bd6ca1":"markdown","6b55d701":"markdown","f821fa32":"markdown","991358bb":"markdown","da4f86cb":"markdown","7c0854cc":"markdown","62bd8d58":"markdown","ba87c46a":"markdown","d44b4acb":"markdown","2c64dd69":"markdown","c316cb96":"markdown","cda5e5c2":"markdown","9913842b":"markdown","4ce836c5":"markdown","2587e1c9":"markdown","4e2d0dd0":"markdown"},"source":{"6f4b1e12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5a3d25bb":"#import data\n\ndata = pd.read_csv(\"..\/input\/column_2C_weka.csv\")\nprint(data.info())","b34c25e4":"#split data to x, y \nx = data.drop([\"class\"], axis = 1)\ny = data[\"class\"].values\n#normalized data\nx = (x - np.min(x)) \/ (np.max(x) - np.min(x)).values\n\nx.head()","18142210":"#%% Show the ratio of normal\/abnormal\nimport seaborn as sns\n\nrate = pd.Series(y).value_counts()\nplt.figure(figsize=[5,5])\nplt.pie(rate.values, explode = [0, 0], labels = rate.index,  autopct = \"%1.1f%%\")\nplt.show()","c5ac4f5a":"plt.figure(figsize=[15,5])\n\n# Create dataframe and reshape\ncolumns = list(x.columns) #column names\n\ndf = x.copy()\ndf[\"class\"] = y #df = x_data + y_data\ndf = pd.melt(df, value_vars=columns, id_vars='class') #id = class olsun,  di\u011fer columnlar\u0131 variable olarak da\u011f\u0131t\n\n\n#Plot\nplt.figure(figsize=(16,6))\npal = sns.cubehelix_palette(2, rot=.5, dark=.3)\nsns.swarmplot(x=\"variable\",y=\"value\", hue=\"class\", palette=pal, data=df)\nplt.show()","d705b58c":"#change y values abnormal\/normal to 0\/1\ny = np.array( [1 if each == \"Abnormal\" else 0 for each in y] )","ac514835":"#  SPLIT DATA TO train and test\nfrom sklearn.model_selection import train_test_split\n\n#x = checkup, y = classes\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","7d712918":"#%% PARAMETER INITIALIZE \ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1), 0.01)\n    b = 0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1\/(1 + np.exp(-z))\n    return y_head","8f3feb35":"def forward_backward_propagation(w, b, x_train, y_train):\n    #foward propagation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = - y_train * np.log(y_head) - (1-y_train) * np.log(1-y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1] # B\u00f6lme sebebi \u00e7\u0131kan sonucu normalize etmek\n\n    #backward propagation   \n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T))) \/ x_train.shape[1]\n    derivative_bias = np.sum(y_head - y_train) \/ x_train.shape[1]\n    \n    gradients = {\"derivative_weight\" : derivative_weight, \"derivative_bias\" : derivative_bias}\n    \n    return cost, gradients","92f6939b":"def update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = [] #T\u00fcm costlar\u0131 depolamak i\u00e7in, analiz i\u00e7in\n    cost_list2 = [] #Her 10 ad\u0131mda bir cost de\u011ferlerini depolar\n    index = [] # Cost2'nin ka\u00e7\u0131nc\u0131 i de\u011ferlerine denk geldi\u011fini g\u00f6sterir\n    \n    #updating parameters\n    for i in range(number_of_iteration):\n        #make forward and backward propagation and find cost and gradients\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost) \n        #Update et\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 100 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i : %f\" %(i, cost))\n    \n    parameters = {\"weight\" : w, \"bias\" : b} #Elimdeki son weight ve bias de\u011ferleri\n    \n    #Parametrelerin g\u00fcncelleme \u00e7izimleri\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation='vertical')\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list ","60e2a5fa":"#%% PREDICT, TEST \u0130\u00c7\u0130N VER\u0130LEN DATA'NIN SONU\u00c7LARINI TAHM\u0130N ET\n\ndef predict(w, b, x_test):\n    # test i\u00e7in verilen data x_test\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    Y_prediction = np.zeros((1, x_test.shape[1])) #Tahmin sonu\u00e7lar\u0131 i\u00e7in bir array olu\u015ftur. \u00d6r : 1,150...\n    \n    for i in range(z.shape[1]): #her s\u00fct\u00fcn i\u00e7in gezilecek\n        if z[0,i] <= 0.5:\n            Y_prediction[0, i] = 0\n        else:\n            Y_prediction[0, i] = 1\n    \n    return Y_prediction","96584c09":"#%% NE KADAR DO\u011eRU TAHM\u0130N ED\u0130LD\u0130\n    \ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    #initialize\n    dimension = x_train.shape[0] #that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    \n    #W ve b de\u011ferlerini g\u00fcncelle. Train ve Test datalar\u0131n\u0131 tahmin et\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iterations)\n    \n    #y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    \n    #Ne kadar yanl\u0131\u015f var\n    print(\"My Test Accuracy : {} %\" .format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return y_prediction_test","5ab09a2b":"#%% CLASSIFICATION WITH MY LOGISTIC REGRESYON\n\n#BENIM REGRESSION TAHMINLERIM\nmy_predict =logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 5, num_iterations = 1000).reshape(-1,1)\n\n\n#CONFUSION MATRIX, TAHMINLER NE KADAR DOGRU\nfrom sklearn.metrics import confusion_matrix\nmy_cm = confusion_matrix(y_test, my_predict)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#MY LR CONFUSION MATRIX PLOT \nplt.figure(figsize=(5,5))\nsns.heatmap(my_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"MY CONFUSION MATRIX PLOT\")\nplt.show()","d0a905ef":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T, y_train.T)\n\n#SKLEARN REGRESSION PREDICTS\ny_sk_predict =  lr.predict(x_test.T)\n\n#ACCURACY\nlr_score = lr.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To (Sklearn)Logistic Reg: {}\".format(lr_score))\n\n#CONFUSION MATRIX\nsk_cm = confusion_matrix(y_test, y_sk_predict)\n\n#SKLEARN lR CONFUS\u0130ON MATR\u0130X PLOT\nplt.figure(figsize=(5,5))\nsns.heatmap(sk_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"SK CONFUS\u0130ON MATR\u0130X PLOT\")\nplt.show()","c28671c2":"#%% CLASSIFICATION WITH KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_score = []\n#k degelerine gore score'lar\u0131 bul\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train.T, y_train.T)\n    knn_score.append( knn.score(x_test.T, y_test.T) )\n\ndf = pd.DataFrame(knn_score)\n#K DEGERLERINE GORE DOGRULUK ORANLARINI CIZ\nplt.figure(figsize=(7,5))\nplt.plot(df.index+1, df.values, color=\"blue\")\nplt.title(\"K Degerlerine G\u00f6re Accuracy\")\nplt.xlabel(\"K value\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n#K = 15 EN IYI DEGER (K= 14 ICIN TAHMINLER YAP)\nknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(x_train.T, y_train.T)\ny_knn_predict = knn.predict(x_test.T)\n\n#ACCURACY YAZ\nknn_score = knn.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To KNN(K=15): {}\".format(knn_score))\n\n##CONFUSION MATRIX, TAHMINLER NE KADAR DOGRU\nknn_cm = confusion_matrix(y_test, y_knn_predict)\n\n#KNN CONFUS\u0130ON MATR\u0130X PLOT\nplt.figure(figsize=(6,5))\nsns.heatmap(knn_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"K=15 CONFUS\u0130ON MATR\u0130X PLOT\")\nplt.show()","29f89646":"#%% CLASSIFICATION WITH SVM  (SUPPORT VECTOR MACHINE)\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state = 42)\nsvm.fit(x_train.T, y_train.T)\n\n#ACCURACY YAZ\nsvm_score = svm.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To SVM : {}\".format(svm_score))\n\n#PREDICT WITH SVM\nsvm_predict = svm.predict(x_test.T)\n\n#CONFUSION MATRIX\nsvm_cm = confusion_matrix(y_test, svm_predict)\n\n#SVM CONFUS\u0130ON MATRIX PLOT\nplt.figure(figsize=(5,5))\nsns.heatmap(svm_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"SK CONFUS\u0130ON MATR\u0130X PLOT\")\nplt.show()","8338fc21":"# CLASSIFICATION WITH NAIVE BAYES\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train.T, y_train.T)\n\n#ACCURACY\nnb_score = nb.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To Naive Bayes : {}\".format(nb_score))\n\n#PREDICT WITH NAIVE BAYES\nnb_predict = nb.predict(x_test.T)\n\n#CONFUSION MATRIX\nnb_cm = confusion_matrix(y_test, nb_predict)\n\n#NAIVE BAYES CONFUS\u0130ON MATRIX PLOT\nplt.figure(figsize=(5,5))\nsns.heatmap(nb_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"NAIVE BAYES CONFUS\u0130ON MATR\u0130X PLOT\")\nplt.show()","a33d8fe8":"#%% CLASSIFICATION WITH DESCION TREE\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train.T, y_train.T)\n\n#ACCURACY\ndt_score = dt.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To Decision Tree : {}\".format(dt_score))\n\n#PREDICT WITH decision tree\ndt_predict = dt.predict(x_test.T)\n\n#CONFUSION MATRIX\ndt_cm = confusion_matrix(y_test, nb_predict)\n\n#DESCION TREE CONFUS\u0130ON MATRIX PLOT\nplt.figure(figsize=(5,5))\nsns.heatmap(dt_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"DECISION TREE CONFUS\u0130ON MATR\u0130X\")\nplt.show()","301e87e3":"#%% CLASSIFICATION WITH RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=300, random_state=1)\nrf.fit(x_train.T, y_train.T)\n\n#ACCURACY YAZ\nrf_score = rf.score(x_test.T, y_test.T) * 100\nprint(\"Test Accuracy According To Random Forest Algorithm : {}\".format(rf_score))\n\n#PREDICT WITH RANDOM FOREST\nrf_predict = rf.predict(x_test.T)\n\n#CONFUSION MATRIX\nrf_cm = confusion_matrix(y_test, rf_predict)\n\n#RANDOM FOREST CONFUS\u0130ON MATRIX PLOT\nplt.figure(figsize=(5,5))\nsns.heatmap(rf_cm, annot = True, linewidth = 0.5, linecolor=\"red\", fmt = \".0f\")\nplt.xlabel(\"Predict Values\")\nplt.ylabel(\"True Values\")\nplt.title(\" RANDOM FOREST ALGORITHM CONFUS\u0130ON MATR\u0130X\")\nplt.show()","d4678ac4":"trace = go.Bar(\n    x=['Logistic Regression', 'KNN', 'SVM', 'Naive Bayes', 'Decision Tree', 'Random Forest'],\n    y=[lr_score, knn_score, svm_score, nb_score, dt_score, rf_score],\n    marker=dict(color=['#008BF8', '#0FFF95', '#EE6C4D', '#A30000', '#2081C3', '#FF7700']),\n)\n\nlayout = go.Layout(\n    title='Accuracy Comparison The All Algorithms',\n)\n\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","a032ec16":"<a id=\"1\"><\/a>\n**DATA ANALYSIS**","88857a93":"<a id=\"17\"><\/a>\n**ACCURACY COMPARISON THE ALL ALGORITHMS**","639edb5e":"<a id=\"13\"><\/a>\n**3. SVM (Support Vector Machine) Algorithm**","40156101":"<a id=\"7\"><\/a>\n**5.  Predict**","92bd6ca1":"<a id=\"12\"><\/a>\n**2. KNN Algorithm**","6b55d701":"<a id=\"16\"><\/a>\n**6. Random Forest Algotihm**","f821fa32":"<a id=\"14\"><\/a>\n**4. Bayes Algorithm**","991358bb":"<a id=\"8\"><\/a>\n**6. Score**","da4f86cb":"<a id=\"5\"><\/a>\n**3. Foward and Backward Propagation**","7c0854cc":"<a id=\"4\"><\/a>\n**2.  Parameter Initialize**","62bd8d58":"<a id=\"15\"><\/a>\n**5. Desicon Tree Algorithm**","ba87c46a":"<a id=\"11\"><\/a>\n**1. Logistic Regression Algorithm**","d44b4acb":"<a id=\"10\"><\/a>\n**CLASSIFICATION ALGORITHMS WITH SKLEARN**","2c64dd69":"**[DATA ANALYSIS](#1)**\n\n\n**[CLASSIFICATION  WITH MY LOGISTIC REGRESSION MODEL](#2)**\n1. [Data preparation](#3)\n2. [Parameter Initialize](#4)\n3. [Foward and Backward Propagation](#5)\n4. [Learning Algorithm(Updating Parameters)](#6)\n5. [Predict](#7)\n6. [Score](#8)\n7. [Classification](#9)\n\n**[CLASSIFICATION ALGORITHMS WITH SKLEARN](#10)**\n1. [Logistic Regression Algorithm](#11)\n2. [KNN Algorithm](#12)\n3. [SVM (Support Vector Machine) Algorithm](#13)\n4. [Naive Bayes Algorithm](#14)\n5. [Desicon Tree Algorithm](#15)\n6. [Random Forest Algotihm](#16)\n\n**[ACCURACY COMPARISON THE ALL ALGORITHMS](#17)**\n\n     \n\n    ","c316cb96":"<a id=\"9\"><\/a>\n**7. Classification**","cda5e5c2":"<a id=\"2\"><\/a>\n**CLASSIFICATION  WITH MY LOGISTIC REGRESSION MODEL**","9913842b":"**Show the values spreading according to their class**","4ce836c5":"<a id=\"3\"><\/a>\n**1. Data preparation **","2587e1c9":"<a id=\"6\"><\/a>\n**4. Learning Algorithm(Updating Parameters) **","4e2d0dd0":"**Show the ratio of normal\/abnormal as a pie chart **"}}