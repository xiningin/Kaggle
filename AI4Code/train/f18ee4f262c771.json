{"cell_type":{"55d7f67d":"code","cadc73f6":"code","3eafc489":"code","2b059bf8":"code","99cc8286":"code","2849079b":"code","f22729a7":"code","d474afbc":"code","9f6207e5":"code","630df953":"code","57353fe7":"code","7cbe44ce":"code","b0f0a16b":"code","856e928f":"code","04943957":"code","b49e5790":"code","2ca11a20":"code","01d006bd":"code","c3cac838":"code","af5470d0":"code","691bab41":"code","2765a710":"code","ae27172e":"code","27475de3":"code","586d89a5":"code","12ea565c":"code","839d5d79":"code","89577dd2":"code","7cc551d7":"code","f973edd8":"code","10ab5591":"code","2a7736eb":"code","64db342f":"code","46d79bae":"code","463cdfdf":"code","cbc708f5":"code","a7483894":"code","225477ba":"code","2a799a58":"code","c3478c9d":"code","869cd716":"code","2287f7b3":"code","2b33b5cc":"code","8e2edef6":"code","fca82b17":"code","95f1b903":"code","d09a3564":"code","36d0e5e6":"code","71794bf8":"markdown","f0a33845":"markdown","4b6170a9":"markdown","cd1f3cd1":"markdown","56c3f7ee":"markdown","1da7ec5f":"markdown","18d3c9c4":"markdown","7874def6":"markdown","0f52b705":"markdown","f04cc93a":"markdown","71c1f1f6":"markdown","4ee9c599":"markdown","d7eed64d":"markdown","785d6277":"markdown","52b3bcd0":"markdown","d5dbac32":"markdown","a70e8cc5":"markdown","0d71aa67":"markdown","ced24c2b":"markdown","63856648":"markdown","762ec262":"markdown","189a311a":"markdown","f22ecb91":"markdown","d80a141b":"markdown"},"source":{"55d7f67d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cadc73f6":"import pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(color_codes=True)\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom pyearth import Earth\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nimport warnings\nwarnings.simplefilter('ignore')","3eafc489":"# Load Data\ndf = pd.read_csv(\"\/kaggle\/input\/vehicle-dataset-from-cardekho\/car data.csv\")\nprint(\"Shape of Dataset:\", df.shape)\ndf.head()","2b059bf8":"# Data Information\ndf.info()","99cc8286":"# Descriptive statistics\ndf.describe()","2849079b":"# Show the distribution of car price \nsns.distplot(df['Selling_Price'],color=\"blue\")","f22729a7":"# Find IQR\nQ1 = df['Selling_Price'].quantile(0.25)\nQ3 = df['Selling_Price'].quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","d474afbc":"# Remove outliers with a criteria: 1.5 x IOR\ndf = df[~((df['Selling_Price'] < (Q1 - 1.5 * IQR)) |(df['Selling_Price'] > (Q3 + 1.5 * IQR)))]\ndf.shape","9f6207e5":"# Show the distribution of price: outliers removed\nsns.distplot(df['Selling_Price'], color=\"blue\")","630df953":"# Show the list of car models\nprint(df['Car_Name'].unique().tolist())","57353fe7":"# Show the frequency of each car model\nfor index, value in df['Car_Name'].value_counts().iteritems():\n    print(index, ': ', value)","7cbe44ce":"# Show the value counts of transmission\nfor index, value in df['Fuel_Type'].value_counts().iteritems():\n    print(index, ': ', value)","b0f0a16b":"# Show the value counts of transmission\nfor index, value in df['Seller_Type'].value_counts().iteritems():\n    print(index, ': ', value)","856e928f":"# Show the value counts of transmission\nfor index, value in df['Transmission'].value_counts().iteritems():\n    print(index, ': ', value)","04943957":"# Show the value counts of transmission\nfor index, value in df['Owner'].value_counts().iteritems():\n    print(index, ': ', value)","b49e5790":"# Representing categorical data using swarm plots\nfig = plt.figure(figsize=(10,7))\nplt.subplot(2,2,1)\nsns.swarmplot(x = 'Fuel_Type', y = 'Selling_Price', data = df, palette=\"winter\")\nplt.subplot(2,2,2)\nsns.swarmplot(x = 'Seller_Type', y = 'Selling_Price', data = df, palette=\"winter\")\nplt.subplot(2,2,3)\nsns.swarmplot(x = 'Transmission', y = 'Selling_Price', data = df, palette=\"winter\")\nplt.subplot(2,2,4)\nsns.swarmplot(x = 'Owner', y = 'Selling_Price', data = df, palette=\"winter\")\nplt.tight_layout()\nplt.show()","2ca11a20":"# Create a list of continuous variables\ncont = [\"Selling_Price\", \"Present_Price\", \"Year\", \"Kms_Driven\"]\n\n# Create a dataframe of continuous variables\ndf_cont = df[cont]","01d006bd":"# Visualize correlation between continuous variables\n\n# Compute the correlation matrix\ncorr = df_cont.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(4, 3))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=\"winter\", vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)","c3cac838":"# Visualize regression between each feature and output variable\n\n# The number of variables\nk = 4\nfig = plt.figure(figsize=(16,6))\n# Correlations between each variable\ncorrmat = df_cont.corr()\n# Take k elements in descending order of coefficient \ncols = corrmat.nlargest(k, \"Selling_Price\")[\"Selling_Price\"].index\n# Calculate correlation\nfor i in np.arange(1,k):\n    regline = df_cont[cols[i]]\n    ax = fig.add_subplot(1,3,i)\n    sns.regplot(x=regline, y=df['Selling_Price'], scatter_kws={\"color\": \"royalblue\", \"s\": 3},\n                line_kws={\"color\": \"turquoise\"})\nplt.tight_layout()\nplt.show()","af5470d0":"# Split X and y\nX = df.drop(['Car_Name', 'Selling_Price'], axis=1)\ny = df['Selling_Price']","691bab41":"# Create dummies for categorical variables\n\n# subset all categorical variables\ncars_categorical = X.select_dtypes(include=['object'])\n# convert into dummies\ncars_dummies = pd.get_dummies(cars_categorical, drop_first=True)\n# drop categorical variables \nX = X.drop(list(cars_categorical.columns), axis=1)\n# concat dummy variables with X\nX = pd.concat([X, cars_dummies], axis=1)","2765a710":"# Scale the features\n\n# Store column names since the column names will be lost after scaling\ncols = X.columns\n\n# Scale the features and convert it back to a dataframe\nX = pd.DataFrame(scale(X))\n\n# Write in the column names again\nX.columns = cols\nX.columns","ae27172e":"# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","27475de3":"# Instantiate the model\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Make prediction\ny_pred = lm.predict(X_test)\n\n# Performance metrics\nlr_r2= r2_score(y_test, y_pred)\nlr_mae = mean_absolute_error(y_test, y_pred)\n\n# Show the metrics\nprint(\"Linear Regression R2: \", lr_r2)\nprint(\"Linear Regression MAE: \", lr_mae)","586d89a5":"# Evaluate the model based on the assumption of linear regression:\n\n# Assumption 1. The error terms are normally distributed with mean approximately 0.\n\nfig = plt.figure()\nsns.distplot((y_test-y_pred),bins=50, color=\"blue\")\nfig.suptitle('Error Terms', fontsize=14)                  \nplt.xlabel('y_test-y_pred', fontsize=12)                  \nplt.ylabel('Index', fontsize=12)                          \nplt.show()","12ea565c":"# Assumption 2: Homoscedasticity, i.e. the variance of the error term (y_true-y_pred) is constant.\n\nc = [i for i in range(len(y_pred))]\nfig = plt.figure()\nplt.plot(c,y_test-y_pred, color=\"blue\", linewidth=2.5, linestyle=\"-\", alpha=0.4)\nfig.suptitle('Error Terms', fontsize=14)               \nplt.xlabel('Index', fontsize=12)                      \nplt.ylabel('ytest-ypred', fontsize=12)                \nplt.show()","839d5d79":"# Assumption 3: There is little correlation between the predictors. i.e., Multicollinearity:\n\npredictors = ['Year', 'Present_Price', 'Kms_Driven', 'Owner', 'Fuel_Type_Diesel','Fuel_Type_Petrol', \n              'Seller_Type_Individual', 'Transmission_Manual']\n\n# Compute the correlation matrix\ncors = X.loc[:, list(predictors)].corr()\n\n# Generate a mask for the upper triangle\nmask_2 = np.triu(np.ones_like(cors, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(9, 6))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(cors, mask=mask_2, cmap=\"winter\", vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)\nplt.show()","89577dd2":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","7cc551d7":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f973edd8":"X = X.drop('Fuel_Type_Diesel', axis=1)","10ab5591":"# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","2a7736eb":"# Instantiate the model\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Make prediction\ny_pred = lm.predict(X_test)\n\n# Performance metrics\nlr_r2= r2_score(y_test, y_pred)\nlr_mae = mean_absolute_error(y_test, y_pred)\n\n# Show the metrics\nprint(\"Linear Regression R2: \", lr_r2)\nprint(\"Linear Regression MAE: \", lr_mae)","64db342f":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","46d79bae":"# Initiate the model\nmars_model = Earth()\n\n# By default, we do not need to set any of the algorithm hyperparameters.\n# The algorithm automatically discovers the number and type of basis functions to use.\n\n# Fit the model\nmars_model.fit(X_train, y_train)\n\n# Making predictions\nmars_y_pred = mars_model.predict(X_test)\n\n# Performance Metrics\nmars_r2 = r2_score(y_test, mars_y_pred)\nmars_mae = mean_absolute_error(y_test, mars_y_pred)\n\n# Show the model performance\nprint(\"MARS R2: \", mars_r2)\nprint(\"MARS MAE: \", mars_mae)","463cdfdf":"# Initiate the model\ndt_model = DecisionTreeRegressor()\n\n# Grid search\ndt_gs = GridSearchCV(dt_model,\n                     param_grid = {'max_depth': range(1, 11),\n                                   'min_samples_split': range(1, 10, 1)},\n                     cv=5,\n                     n_jobs=1,\n                     scoring='neg_mean_squared_error')\n\ndt_gs.fit(X_train, y_train)\n\nprint(dt_gs.best_params_)\nprint(-dt_gs.best_score_)","cbc708f5":"# Initiate the best model\ndt_model_best = DecisionTreeRegressor(max_depth=8, min_samples_split=2)\n\n# Fit the best model\ndt_model_best.fit(X_train, y_train)","a7483894":"# Make predictions\ndt_y_pred = dt_model_best.predict(X_test)\n\n# Performance metrics\ndt_r2 = r2_score(y_test, dt_y_pred)\ndt_mae = mean_absolute_error(y_test, dt_y_pred)\n\n# Show the model performance\nprint(\"DT R2: \", dt_r2)\nprint(\"DT MAE: \", dt_mae)","225477ba":"# Initiate the model\nxgb_model = xgb.XGBRegressor()\n\n# Grid search\nxgb_gs = GridSearchCV(xgb_model,\n                      param_grid = {'max_depth': range(8, 15),\n                                   'min_samples_split': range(2, 11, 3)},\n                      cv=5,\n                      n_jobs=1,\n                      scoring='neg_mean_squared_error')\n                      \nxgb_gs.fit(X_train, y_train)\n\nprint(xgb_gs.best_params_)\nprint(-xgb_gs.best_score_)","2a799a58":"# Initiate the best model\nxgb_model_best = xgb.XGBRegressor(max_depth=9, min_samples_split=2)\n\n# Fit the best model\nxgb_bst = xgb_model_best.fit(X_train, y_train)","c3478c9d":"# Make predictions\nxgb_y_pred = xgb_bst.predict(X_test)\n\n# Performance metrics\nxgb_r2 = r2_score(y_test, xgb_y_pred)\nxgb_mae = mean_absolute_error(y_test, xgb_y_pred)\n\n# Show the model performance\nprint(\"XGB R2: \", xgb_r2)\nprint(\"XGB MAE: \", xgb_mae)","869cd716":"# Define a DNN\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model","2287f7b3":"# Initiate DNN\ndnn = KerasRegressor(build_fn=create_model, epochs=10000, batch_size=20, verbose=1)\n\n# Fit DNN\ndnn_history = dnn.fit(X_train, y_train)","2b33b5cc":"# Visualize the DNN learning\nloss_train = dnn_history.history['loss']\nepochs = range(1,10001)\nplt.figure(figsize=(8,6))\nplt.plot(epochs, loss_train, 'royalblue', label='Training loss', linewidth=3)\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","8e2edef6":"# Make predictions\ndnn_y_pred = dnn.predict(X_test)\n\n# Performance metrics\ndnn_r2 = r2_score(y_test, dnn_y_pred)\ndnn_mae = mean_absolute_error(y_test, dnn_y_pred)\n\n# Show the model performance\nprint(\"DNN R2: \", dnn_r2)\nprint(\"DNN MAE: \", dnn_mae)","fca82b17":"results_table = pd.DataFrame([[np.mean(lr_r2), np.mean(lr_mae)],\n                             [np.mean(mars_r2), np.mean(mars_mae)],\n                             [np.mean(dt_r2), np.mean(dt_mae)],\n                             [np.mean(xgb_r2), np.mean(xgb_mae)],\n                             [np.mean(dnn_r2), np.mean(dnn_mae)]],\n                            columns=['R2', 'MAE'],\n                            index=[\"Linear Regression\",\"MARS\",\"Decision Tree\",\"XGBoost\",\"DNN\"])\npd.options.display.precision = 3\nresults_table","95f1b903":"pred_table = pd.DataFrame({\"Linear Regression: Predicted Price\": y_pred,\n                           \"MARS: Predicted Price\": mars_y_pred,\n                           \"Decision Tree: Predicted Price\": dt_y_pred,\n                           \"XGBoost: Predicted Price\": xgb_y_pred,\n                           \"DNN: Predicted Price\": dnn_y_pred,\n                          \"Actual Price\": y_test})","d09a3564":"# Visualize the predicted price and actual price\nfig = plt.figure(figsize=(10,10))\nplt.subplot(3,2,1)\nsns.regplot(x = 'Linear Regression: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,2)\nsns.regplot(x = 'MARS: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,3)\nsns.regplot(x = 'Decision Tree: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,4)\nsns.regplot(x = 'XGBoost: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,5)\nsns.regplot(x = 'DNN: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.tight_layout()\nplt.show()","36d0e5e6":"# Import libaries\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Show permutation importance\nperm = PermutationImportance(dnn, random_state=1).fit(X,y)\neli5.show_weights(perm, feature_names = X.columns.tolist())","71794bf8":"The second assumption seems to be met.","f0a33845":"## 3.2. Multivariate Adaptive Regression Splines (MARS)","4b6170a9":"## 2.3. Exploration of Continuous Variables","cd1f3cd1":"## 2.2. Exploration of Categorical Variables","56c3f7ee":"# 5. Feature Importance","1da7ec5f":"Some features are highly correlated. So let's check the multicolliearity by VIF.","18d3c9c4":"## 2.4. Data Preparation for Modeling","7874def6":"# 1. Import Libraries and Load Dataset","0f52b705":"## 3.5. Deep Neural Network","f04cc93a":"The distribution is right-skewed. Let's remove outliers using the IQR as the criteria.","71c1f1f6":"Since DNN achieved the highest performance, show the feature importance of DNN.","4ee9c599":"Since great many car models are contained, let's ignore car models in prediction.","d7eed64d":"## 3.3. Decision Tree Regression","785d6277":"## 2.1. Remove Outliers in Target Variable","52b3bcd0":"## 3.1. Linear Regression","d5dbac32":"- Predict used car price by various regression models\n\n- Regression Models:\n  - Linear Regression\n  - Multivariate Adaptive Regression Splines\n  - Decision Tree Regressor\n  - XGBoost Regressor\n  - Deep Neural Network\n\n- Performace Metrics:\n  - R-Squared\n  - Mean Absolute Error","a70e8cc5":"## 3.4. XGB Regression","0d71aa67":"There are no missing values in the dataset.","ced24c2b":"The first assumption seems to be met.","63856648":"\"fuel_Petrol\" shows the highest VIF, so let's delete it.","762ec262":"# 3. Regression","189a311a":"# 2. Data Exploration and Preprocessing","f22ecb91":"# Used Car Price Prediction","d80a141b":"# 4. Summary of Results"}}