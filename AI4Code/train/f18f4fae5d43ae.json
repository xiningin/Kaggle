{"cell_type":{"6d7bb243":"code","e5884114":"code","cfb64c9c":"code","47bb6ffe":"code","432ca9b5":"code","917c5d2b":"code","77e680e9":"code","2591a864":"markdown","e12cd6db":"markdown","bd6a69fa":"markdown","0bc5fff6":"markdown","c24d4de2":"markdown","92753004":"markdown","588ed2b3":"markdown","28aa1e91":"markdown","ca5c34de":"markdown","b7515498":"markdown","969c3162":"markdown"},"source":{"6d7bb243":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualisation purposes\nfrom sklearn.tree import DecisionTreeClassifier ,plot_tree # Our model and a handy tool for visualising trees\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5884114":"train_file_path = '..\/input\/titanic\/train.csv'\n\n# Create a new Pandas DataFrame with our training data\ntitanic_train_data = pd.read_csv(train_file_path)\n\n#titanic_test_data.columns\ntitanic_train_data.describe(include='all')\n#titanic_train_data.head()","cfb64c9c":"# Let's reduce our data to only the features we need and the target.\n# The features we chose have similar 'count' values when we describe() them\n# We need to keep the target as part of our DataFrame for now.\nselected_columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']\n\n# Create our new training set containing only the features we want\nprepared_data = titanic_train_data[selected_columns]\n\n# Drop rows (axis=0) that contain missing values\nprepared_data = prepared_data.dropna(axis=0)\n\n# Check that you still have a good 'count' value. The value should be the same for all columns.\n# If your count is very low then you may need to remove features with the lowest count.\nprepared_data.describe(include='all')","47bb6ffe":"# Separate out the prediction target\ny = prepared_data.Survived\n\n# Drop the target column (axis=1) from the original dataframe and use the rest as our feature data\nX = prepared_data.drop('Survived', axis=1)\n\n# Take a look at the data again\nX.head()\n#y.head()","432ca9b5":"# One hot encode the features. This will only act on columns containing non-numerical values.\none_hot_X = pd.get_dummies(X)\n\none_hot_X.head()","917c5d2b":"# Create a decision tree classifier with a maximum depth of 3 for easy display later on\n# Try changing the max_depth to see what happens\nsurvivor_predictor = DecisionTreeClassifier(max_depth=3)\n\n# Train the model on the one hot encoded data\nsurvivor_predictor.fit(one_hot_X, y)\n\n# Let's plot the tree to see what it looks like!\nplt.figure(figsize = (20,10))\nplot_tree(survivor_predictor,\n          feature_names=one_hot_X.columns,\n          class_names=['perished', 'survived'],\n          filled=True)\nplt.show()\n\n# Note for class_names we've used strings to represent each of the values.\n# However, the real values are 0 for perished and 1 for survived.\n# Class names for plot_tree must be strings so to get the right replacement values\n# we had to do the following:\n# First get a list of classes the tree will classify things as with the following command\n### print(survivor_predictor.classes_) ###\n# This gives us [0,1] \n# Now we can create a new list with the replacement class strings in the same order.","77e680e9":"print(\"Making predictions for the first 5 passengers in the training set.\")\n\n# Get the first five predictions as a list\npred = survivor_predictor.predict(one_hot_X)\n\nprint(\"The predictions are:\")\n\n# Merge actual target values and predictions back in with original features to see how we went.\nX['Survived'] = y\nX['Predicted'] = pred\n\nX.head()","2591a864":"# Gather and explore the data\nNow that we've set things up the next step is to add data to our proejct. You can do this by clicking the __Add data__ button in the top right and searching Kaggle for a data set that interests you.\n\nIt would be best to limit your search to data that:\n- Is in .csv format\n- Contains mostly numerical values\n- May contain categorical values but with limited options such as male\/female.\n- Doesn't have many missing values\n\nOnce you've found the right data set, follow the prompts to add it to your notebook. It should appear under the _input_ folder icon as you see it in the top right of this notebook. For this example, we've chosen data about passengers on the titanic.\n\nNow that we have a file containing data, let's get it into a Pandas DataFrame and take a peek.","e12cd6db":"# Setting up\nThe below code contains necessary steps for setting up our machine learning environment. Key features are described in the comments.","bd6a69fa":"\n\n# Evaluate model performance and tune hyperparameters\nNow that we have a sweet looking model, let's see how good it is at predicting passenger survival on our training set. ","0bc5fff6":"## Pretty Cool!\nTake a good look at the decision tree. \n- Does the hierarchy of nodes make sense? \n- Are the values used to make decisions what you expected?\n- Is it telling a plausible story about what kind of passengers are more likely to survive?\n- Is there anything that surprises you?\n\nNote that there are other ways to view a decision tree and there may be other parameters you could include when plotting the tree to display the nodes differently, but this is fine for now.","c24d4de2":"Now we have two columns for Sex, one for female and one for male. Consider that if we had even more categories then we'd get even more columns, which could cause performance problems if there are a large number of categories in our data.\n\n# Choose and Train a Model\nNow that we have data our model can digest, let's use it to train a model and make some predictions. We're going to use a __Decision Tree Classifier__ which is different from the Decision Tree Regressor used in the [Intro to Machine Learning course](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning) in that it makes categorical predictions instead of continuous numerical predictions. \n\nIn this case, the category we want to predict is whether or not a passenger survived, with the output being a 1 if they survived and a 0 if they did not. Decision Tree Classifiers are also able to work with non-numerical prediction targets as well. For example, you might have a 'y' that contains the names (as strings) of different species of flowers. It's only features that need to be encoded.\n\nFor an example of a Decision Tree Classifier working with a non-numerical 'y' and a more in-depth look at how they work, take a look at this Kaggle notebook (https:\/\/www.kaggle.com\/chrised209\/decision-tree-modeling-of-the-iris-dataset)\n\nOk, let's train our model and see what it looks like.","92753004":"## Wow! Perfect!... or is it?\n\nRemember when doing the [Intro to Machine Learning course](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning) we discovered the problem with evaluating our model on the training data. It looks like our predictions are good at the moment, but will they be as good for data our model hasn't seen yet?\n\nIn practice, it's always a good idea to split the data from the original training set into a training and validation\/test set, otherwise it's difficult to know how good our predictions really are if we're testing our model on the same data it was trained with!\n\nFor now, we're just worried about how to make predictions and displaying them. When you do your own project, you'll want to __test your model on separate test data using different hyperparameter values__ (such as tree depth or features) and compare the __Mean Absolute Error__ of your predictions, just as you did in the [Intro to Machine Learning course](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning), before making any conclusions. You might even want to create __multiple models__ and compare the predictions between them as well!","588ed2b3":"# Conclusion\nNow that you have some predictions it's important to talk about them. Some questions you might want to answer are:\n- How accurate is your final model?\n- How does it compare to other models you've tested with this data set?\n- Do you think the features you selected are appropriate? Why?\n- What could be done to improve your model further?\n\n... and that's it!","28aa1e91":"# Introduction\nEvery data science investigation needs an introduction. In this example project we're going to explore how to use a Decision Tree Classifier to predict whether or not passengers on the titanic would have survived the disaster. \n\nIn your project, you'll choose a different set of data (or the same, but make your analysis different to this one!) and make your own predictions.\n\nNote that this project assumes that you've already completed the [Intro to Machine Learning course](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning) on Kaggle and are familiar with Decision Tree Classifiers and Regressors. If you need to brush up on these, watch the following [YouTube playlist on different types of decision trees.](https:\/\/www.youtube.com\/playlist?list=PLH8ela8ws3gvTRV0iZezkod_0egMCNwGq)","ca5c34de":"# Prepare the data\nIn this example, we want to predict whether or not a passenger __survived__ the Titanic disaster. Therefore the 'Survived' column is our prediction target.\n\nBefore we can separate our prediction target 'y' from the rest of the data, we need to do some preparation so that there aren't any rows with missing values as our machine learning model will not be able to handle them.\n\n## Select features and target then drop missing values\nChoosing our features first will help reduce the total number of rows we need to drop (remove).\n\nWe want to choose a selection of features that are:\n- Relevant to our predictions\n- Don't have many missing values\n\nNote that we'll also be including the target for now.\n","b7515498":"## Split data into training and testing data.\nSplitting the training set into two subsets is important because you need to have data that your model hasn't seen yet with actual values to compare to your predictions to be able to tell how well it is performing. In this example project we're skipping this step, but when you do your project you'll need to consider how you want to split your data. The [Intro to Machine Learning course](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning) goes through how to do this. \n\n## Separate Features From Target\nNow that we have a set of data (as a Pandas DataFrame) without any missing values, let's separate the features we will use for training from the target.\n\n\n","969c3162":"## One Hot Encode Categorical Data \nOne of the difficulties of working with machine learning models is that most of them can only work with numerical features. One problem with our current data is that 'Sex' is categorical, with values of either _male_ or _female_ which are not numbers!\n\nTo use 'Sex' as a feature, we must find a way to encode each category as a number. One Hot Encoding is the most widely used approach for doing this. One Hot Encoding creates new (binary) columns, indicating the presence of each possible category value in the original data. In other words, it separates each of the options for a category into a separate column, where a 1 means that the row fits the category in question and a zero indicates it doesn't.\n\n__For example:__\n\n<img src=\"https:\/\/i.imgur.com\/mtimFxh.png\" alt=\"One Hot Encoding\" style=\"width:600px\">\n\nWatch this video (https:\/\/www.youtube.com\/watch?v=v_4KWmkwmsU) for more information about how and why this works.\n\nThe Pandas __get_dummies__ function is the easiest way to One Hot Encode categorical data. Here's how it's done."}}