{"cell_type":{"c3035a9f":"code","b8706f4e":"code","56ad4c09":"code","98541fe3":"code","dc92de90":"code","19db192c":"code","7a75d167":"code","29ab8ca2":"code","4cbf7579":"code","3acc141b":"code","d89a520b":"code","1ab90c5b":"code","a2cb8105":"code","018c36f2":"code","7618262d":"code","eda17c4f":"code","40688404":"code","91a4ef34":"code","765600ac":"code","ebe2ceab":"code","3973a74b":"code","166761bd":"code","f937f031":"code","364dbcca":"code","83302b76":"code","3d9ee347":"code","14ca4d2d":"code","b67e06c2":"code","d3af726b":"code","e46e206f":"markdown","abe8be84":"markdown","82feff46":"markdown","dd7d5dea":"markdown","367477ec":"markdown","38fa8123":"markdown","46f067b9":"markdown","6a3c31c9":"markdown","d862bf1a":"markdown","d5201e94":"markdown","f84b88c3":"markdown","5721ab9c":"markdown","46b0d5db":"markdown"},"source":{"c3035a9f":"#Generic Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re,string,unicodedata\nfrom string import punctuation\nfrom math import pi\nfrom PIL import Image\n\n#Plotting Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom bokeh.plotting import figure\nfrom bokeh.io import output_notebook, show, output_file\nfrom bokeh.models import ColumnDataSource, HoverTool, Panel\nfrom bokeh.models.widgets import Tabs\nfrom bokeh.palettes import Category20c\nfrom bokeh.transform import cumsum\nfrom bokeh.resources import INLINE\n\n#NLTK Libraries\nimport nltk\nfrom nltk.corpus import stopwords\n\n#Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Garbage Collection\nimport gc\n\n#downloading wordnet\/punkt dictionary\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')\n\n#WordCloud Generator\nfrom wordcloud import WordCloud,STOPWORDS\n\n#Gensim Library for Text Processing\nimport gensim.parsing.preprocessing as gsp\nfrom gensim import utils\n\nfrom textblob import TextBlob, Word\n\n#Keyword Extraction Libraries\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\n#Tabulate\nfrom tabulate import tabulate","b8706f4e":"data_url = '..\/input\/hatred-on-twitter-during-metoo-movement\/MeTooHate.csv'\ndata = pd.read_csv(data_url, header='infer')","56ad4c09":"data = data.drop(['status_id', 'created_at', 'favorite_count', 'retweet_count',\n       'location', 'followers_count', 'friends_count', 'statuses_count',\n       'category'], axis=1)","98541fe3":"#Inspect\ndata.head()","dc92de90":"#Shape\nprint(\"Total Tweets: \",data.shape[0])","19db192c":"#Check for null values\nprint(\"Number of records with null text columns: \",data['text'].isna().sum())","7a75d167":"#Dropping records with null value\ndata = data.dropna()","29ab8ca2":"#Taking Backup of prep'd data\ndata_bkp = data.copy()","4cbf7579":"#Number of Characters\ndata['chars'] = data['text'].str.len()","3acc141b":"# Function to plot histogram\ndef plot_sumry(dataframe,column, title=''):\n    plt.figure(figsize=(10,5))\n    sns.set_palette('pastel')\n    sns.set_color_codes()\n    ax = sns.distplot(dataframe[column], color='midnightblue', bins=25)\n    ax.set_title(title, fontsize=15)\n    \n    x_min = dataframe[column].min()\n    x_max = dataframe[column].max()\n    x_mean = dataframe[column].mean()\n    \n    print(f'Stat Summary of Tweet {column.capitalize()}:\\n'\n          f'Minimum Character Count   : {x_min}\\n'\n          f'Maximum Character Count   : {x_max}\\n'\n          f'Average Character Count   : {round(x_mean)}')\n\n    \nplot_sumry(data, 'chars', 'Characters Distribution')\n    \n","d89a520b":"# Creating a Sampler function\n\ndef sampler(dataframe,column):\n    \n    temp_df = dataframe[(dataframe[column]>=33) & (dataframe[column]<=150)]\n    \n    # sampling 15%\n    df_sample = pd.DataFrame(temp_df['text'].sample(frac=0.10, replace=True, random_state=1))\n\n    return df_sample.reset_index(drop=True)\n","1ab90c5b":"#Creating a new sampled dataset\ndata_sample = sampler(data,'chars')\nprint(\"Number of Tweets in Sampled Dataset: \", data_sample.shape[0])","a2cb8105":"#Inspect new sampled dataset\ndata_sample.head()","018c36f2":"# Create list of pre-processing func (gensim)\nprocesses = [\n               gsp.strip_tags, \n               gsp.strip_punctuation,\n               gsp.strip_multiple_whitespaces,\n               gsp.strip_numeric,\n               gsp.remove_stopwords, \n               gsp.strip_short, \n               gsp.stem_text\n            ]\n\n# Create func to pre-process text\ndef proc_txt(txt):\n    text = txt.lower()\n    text = utils.to_unicode(text)\n    for p in processes:\n        text = p(text)\n    return text\n","7618262d":"#Creating a new column with processed text\ndata_sample['text_proc'] = data_sample['text'].apply(lambda x: proc_txt(x))","eda17c4f":"#Taking a backup\ndata_proc_bkp = data_sample.copy()","40688404":"# Creating a function to analyse the tweet sentiments\n\ndef sentiment_analyzer(text):\n    #vad_sent = SentimentIntensityAnalyzer()\n    #sentiment_dict = vad_sent.polarity_scores(text) \n    TB_sentiment_polarity = TextBlob(text).sentiment.polarity\n    \n    # decide sentiment as positive, negative and neutral \n    if TB_sentiment_polarity >= 0.00 : \n        return \"Has No Hatred\" \n  \n    elif TB_sentiment_polarity <= 0.00 : \n        return \"Has Hatred\" \n  \n    else : \n        return \"Is Neutral\"\n    \n","91a4ef34":"#Analysing the sentiment\ndata_sample['sentiments'] = data_sample['text_proc'].apply(lambda x: sentiment_analyzer(x))","765600ac":"data_sample.head()","ebe2ceab":"#Taking Backup\ndata_sentiments = data_sample.copy()","3973a74b":"sentiment_count = data_sample.groupby('sentiments').size()\n\n# Data to plot\nlabels = 'Has Hatred', 'Has No Hatred'\nsizes = [sentiment_count[0], sentiment_count[1]]\ncolors = ['dimgrey', 'lightgray']\nexplode = (0.1, 0)  # explode 1st slice\nfig = plt.figure(figsize=[8, 6])\n\n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.title(\"Sentiment Distribution (sample data)\", fontsize=16)\nplt.show()\n","166761bd":"#Creating a stopword lists\nstopword_list = set(stopwords.words('english'))\n\n#get the text from text_proc columns\ndocs = data_sample['text_proc'].tolist()\n\n#Create vocab of words & ignore words that appear in 80% of documents\ncv = CountVectorizer(max_df=0.80,\n                     stop_words=stopword_list,\n                     max_features=40000\n                    )\n\n#Fitting the CountVector to the list created above\nword_count_vector = cv.fit_transform(docs)","f937f031":"#Compute IDF Value\ntf_transform = TfidfTransformer(smooth_idf=True, use_idf=True)\ntf_transform.fit(word_count_vector)","364dbcca":"# -- Extract Keywords Custom Function --\n\n#Sort in Descending Order\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1],x[0]), reverse=True)\n    \n#Extract Keywords\ndef extraction(feature_names, sorted_items, n):\n    sorted_items = sorted_items[:n]\n    \n    score_vals = []\n    feature_vals = []\n    \n    for idx, score in sorted_items:\n        \n        score_vals.append(round(score,3))\n        feature_vals.append(feature_names[idx])\n        \n    return feature_vals","83302b76":"#Extracting Keywords\n\nfeature_names = cv.get_feature_names()\n\n#Creating seperate dataframe for different sentiments\ndf_Hate = data_sample[(data_sample['sentiments'] == 'Has Hatred')]\ndf_noHate = data_sample[(data_sample['sentiments'] == 'Has No Hatred')]\n\n#Seperate docs for Hatred & No Hatred\ndocs_Hate = df_Hate['text'].tolist()\ndocs_noHate = df_noHate['text'].tolist()\n\nfor ht, nht in zip(docs_Hate,docs_noHate):\n    tf_idf_vector_Hatred = tf_transform.transform(cv.transform([ ht ]))\n    tf_idf_vector_noHatred = tf_transform.transform(cv.transform([ nht ]))\n    \nsorted_items_Hatred = sort_coo(tf_idf_vector_Hatred.tocoo())\nsorted_items_noHatred = sort_coo(tf_idf_vector_noHatred.tocoo())\n\nHatred_keywords = extraction(feature_names,sorted_items_Hatred,100)\nnoHatred_keywords = extraction(feature_names,sorted_items_noHatred,100)","3d9ee347":"tab_data = [[Hatred_keywords]]\nprint(tabulate(tab_data, headers=['Keywords in Tweets with Hatred']))\nprint(\" \")\ntab_data_nht = [[noHatred_keywords]]\nprint(tabulate(tab_data_nht, headers=['Keywords in Tweets with no Hatred']))\n","14ca4d2d":"# Function to plot word cloud\ndef plot_wordcloud(text, mask=None, max_words=400, max_font_size=120, figure_size=(12.0,12.0), \n                   title = None, title_size=20, image_color=False):\n\n    wordcloud = WordCloud(background_color='white',\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    mask = mask)\n    wordcloud.generate(text)\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'green', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nd = '..\/input\/masks\/masks-wordclouds\/'","b67e06c2":"txt = str(df_Hate.text)\ncomments_mask = np.array(Image.open(d + 'comment.png'))\nplot_wordcloud(txt, comments_mask, max_words=1000, max_font_size=100, \n               title = 'Common Words in Tweets with Hatred', title_size=30)","d3af726b":"txt = str(df_noHate.text)\ncomments_mask = np.array(Image.open(d + 'comment.png'))\nplot_wordcloud(txt, comments_mask, max_words=1000, max_font_size=100, \n               title = 'Common Words in Tweets with no Hatred', title_size=30)","e46e206f":"# Sentiment Analysis","abe8be84":"For the analysis I'm only interested in the text, hence dropping all the other columns","82feff46":"# Libraries","dd7d5dea":"# Data","367477ec":"# Word Clouds ...","38fa8123":"# Visualising Sentiments..","46f067b9":"### I hope this notebook has taught you something the way it has done to me. Please upvote if it has enlightened you even a slight bit :-)","6a3c31c9":"# Keyword Extraction","d862bf1a":"# Basic Feature Extraction","d5201e94":"# Sampling\n\nFrom the distribution plot above, \n\n* It can be understood that there are a large number of tweets with 1 & 150 characters. \n\nBased on the information above & the information [here](https:\/\/techcrunch.com\/2018\/10\/30\/twitters-doubling-of-character-count-from-140-to-280-had-little-impact-on-length-of-tweets\/#:~:text=Meanwhile%2C%20most%20tweets%20continue%20to,a%20tweet%20is%2033%20characters.) which states that the average tweet length is 33 characters, we are going to create a subset of the original dataset.\n\nAnd from this subset we are going to randomly sample **10%** of the tweets.\n\nThe reason for sampling is very simple:\n\n* Even the above created subset is huge (~400K+ tweets) \n* Less time consuming for sentiment analysis\n* %10 is a good sampling size to give us a rough idea \n* And keep this notebook easy & light :-)","f84b88c3":"# Data Prep","5721ab9c":"# Tweet Analysis during MeToo Movement\n\nIn this notebook, we will try to analyse the tweets that were made during MeToo Movement. The analysis will span from the very basic feature extraction to sentiment analysis (& everything in between). This analysis is done on the [Hatred on Twitter During MeToo Movement](https:\/\/www.kaggle.com\/rahulgoel1106\/hatred-on-twitter-during-metoo-movement) dataset.\n\n![](https:\/\/www.shadesofgrace.org\/wp-content\/uploads\/Lets-Get-Started.jpg)\n","46b0d5db":"# Text Processing"}}