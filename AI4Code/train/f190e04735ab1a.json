{"cell_type":{"96932ff5":"code","aec89557":"code","19c06377":"code","75c6a1cd":"code","ba36ac1e":"code","ed41e671":"code","2cb8d1a3":"code","514a91fa":"code","14ef7aeb":"code","54d105cf":"code","78b96fab":"code","2f3feae2":"code","086485b7":"code","cf866c7e":"code","14eab358":"code","ca6cdc95":"code","12bce33c":"code","3f5d2c83":"code","929c8a04":"code","61c6b29d":"code","3c8c65f7":"code","48c4666b":"code","3fe10d1e":"code","0c622e4b":"code","f59f9992":"code","83ff9707":"code","b0cfc4ee":"code","f4e3604d":"code","13cac875":"code","84b9674b":"code","8059d381":"code","db1981c8":"code","1b62a807":"code","fef9da30":"code","59fdd37a":"code","faf24211":"code","91256046":"code","0abe91e3":"code","abb6417c":"code","695e8da6":"code","aa461024":"code","9deb4bba":"code","4a74873f":"code","05d48aa1":"code","aab31716":"code","33b1501d":"code","01640972":"code","fc419553":"code","e07c4d99":"code","b0b97cb2":"code","a01fb1db":"code","93d2751c":"code","f8d397b7":"markdown","b2c710e4":"markdown","2ab70abd":"markdown","1990b572":"markdown","ced3b05f":"markdown","bbe9bc2c":"markdown","0a5f742e":"markdown","04676fb5":"markdown","15679741":"markdown","2099c175":"markdown","e01a3bdb":"markdown","440c2f85":"markdown","823f4627":"markdown","e852ca2d":"markdown","81ec75ad":"markdown","ed9ae2c9":"markdown","0bb1be46":"markdown","0b23517e":"markdown","e3166752":"markdown","477330b1":"markdown","79095889":"markdown","1ad6e268":"markdown","7ba45f5f":"markdown","05c076ea":"markdown","57bb4e4f":"markdown","bb331d4d":"markdown","0311d66d":"markdown","844336ec":"markdown","b296e480":"markdown","2163b75d":"markdown","0615c76a":"markdown","3c8d5ae5":"markdown","aa1a6a52":"markdown","da2a866f":"markdown","536bcefc":"markdown","a260a954":"markdown","068ba18f":"markdown","e401c2cb":"markdown","44297095":"markdown","7b71b8af":"markdown","68b7e8bf":"markdown","df6f69d7":"markdown","fd11e02c":"markdown","c6e45ebf":"markdown","890d666c":"markdown","93b47000":"markdown","3f69b19a":"markdown","97563d03":"markdown","40bf7010":"markdown"},"source":{"96932ff5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns # visualization \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n# import xgboost as xgb\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","aec89557":"plt.style.use('seaborn-deep')\n%matplotlib inline","19c06377":"Train_df = pd.read_csv('..\/input\/train.csv')\nTest_df = pd.read_csv('..\/input\/test.csv')","75c6a1cd":"Train_df.head(10)","ba36ac1e":"Test_df.head()","ed41e671":"Train_df.set_index('PassengerId',inplace=True)\nTest_df.set_index('PassengerId',inplace=True)","2cb8d1a3":"Train_df.head()","514a91fa":"Train_df.info()\nTest_df.info()\nTrain_df.describe()","14ef7aeb":"median_age_train=Train_df[\"Age\"].median()\nTrain_df[\"Age\"].fillna(median_age_train, inplace=True)\n# fill NaN Age values with zero , preferred value for XGB \n# Train_df[\"Age\"].fillna(0, inplace=True)\nTrain_df_clean = Train_df.dropna(subset=[\"Embarked\"]).drop(\"Cabin\",axis=1).drop(\"Ticket\",axis=1)\n#median_age_test=Test_df[\"Age\"].median()\nTest_df[\"Age\"].fillna(median_age_train, inplace=True)\n#Test_df[\"Age\"].fillna(0, inplace=True)\nTest_df_clean = Test_df.dropna(subset=[\"Embarked\"]).drop(\"Cabin\",axis=1).drop(\"Ticket\",axis=1)","54d105cf":"def extract_title(name):\n    if \"Mrs\" in name :\n        title = 0\n    elif \"Mr\" in name :\n        title = 1\n    elif \"Miss\" in name :\n        title = 2\n    elif \"Master\" in name :\n        title = 3\n    elif \"Dr\" in name :\n        title = 4\n    else :\n        title = 5\n    return title \n\nTrain_df_clean[\"Title\"] = Train_df_clean[\"Name\"].apply(extract_title)\nTest_df_clean[\"Title\"] = Test_df_clean[\"Name\"].apply(extract_title)\nTrain_df_clean = Train_df_clean.drop(\"Name\",axis=1)\nTest_df_clean = Test_df_clean.drop(\"Name\",axis=1)\nTrain_df_clean[\"Familysize\"] = Train_df_clean[\"SibSp\"]+Train_df_clean[\"Parch\"]+1\nTest_df_clean[\"Familysize\"] = Test_df_clean[\"SibSp\"]+Test_df_clean[\"Parch\"]+1","78b96fab":"le = LabelEncoder()\nTrain_df_clean[\"Sex\"] = le.fit_transform(Train_df_clean[\"Sex\"])\nprint(le.classes_)\nTest_df_clean[\"Sex\"] = le.fit_transform(Test_df_clean[\"Sex\"])","2f3feae2":"le2 = LabelEncoder()\nTrain_df_clean[\"Embarked\"] = le2.fit_transform(Train_df_clean[\"Embarked\"])\nprint(le2.classes_)\nTest_df_clean[\"Embarked\"] = le2.fit_transform(Test_df_clean[\"Embarked\"])","086485b7":"median_fare=Test_df_clean[\"Fare\"].median()\nTest_df_clean[\"Fare\"].fillna(median_fare, inplace=True)\nTest_df_clean.info()","cf866c7e":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score\n#predicting missing values in age using Random Forest\n# using function from Poonam LigadeTitanic Survival Prediction End to End ML Pipeline\n\ndef fill_missing_age(df):\n    \n    #Feature set\n    age_df = df[['Age','Embarked','Fare', 'Parch', 'SibSp',\n                  'Title','Pclass','Familysize'\n                 ]]\n    # Split datasets into train and test\n    train  = age_df.loc[ (df.Age.notnull()) ]# known Age values\n    test = age_df.loc[ (df.Age.isnull()) ]# null Ages\n    \n    # All age values are stored in a target array\n    y = train.values[:, 0]\n    \n    # All the other values are stored in the feature array\n    X = train.values[:, 1::]\n    \n    # Create and fit a model\n    rtr = RandomForestRegressor(n_estimators= 2000,n_jobs=-1)\n    rtr.fit(X, y)\n    \n    # Use the fitted model to predict the missing values\n    predictedAges = rtr.predict(test.values[:, 1::])\n\n    \n    # Assign those predictions to the full data set\n    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n    \n    return df\n\n#Train_df_clean=fill_missing_age(Train_df_clean)\n#Test_df_clean=fill_missing_age(Test_df_clean)\n\nTrain_df_clean.info()\n\n","14eab358":"Test_df_clean.info()","ca6cdc95":"#Test_df_clean.Age.iloc[[10]] = 20\n","12bce33c":"Train_df_clean.head(20)\nTrain_df_clean.info()\nTest_df_clean.info()","3f5d2c83":"Train_df_clean.head(20)","929c8a04":"Train_df_clean.info()\nTrain_df_clean.head()","61c6b29d":"corr_matrix = Train_df_clean.corr()\ncorr_matrix[\"Survived\"].sort_values(ascending=False)","3c8c65f7":"sns.countplot(x=\"Sex\", hue=\"Survived\",data=Train_df_clean);","48c4666b":"sns.countplot(x=\"Title\", hue=\"Survived\",data=Train_df_clean);","3fe10d1e":"sns.countplot(x=\"Pclass\", hue=\"Survived\",data=Train_df_clean);","0c622e4b":"sns.countplot(x=\"Familysize\", hue=\"Survived\",data=Train_df_clean);\n","f59f9992":"sns.set(rc={'figure.figsize':(35.7,16.27)})\nsns.countplot(x=\"Age\",data=Train_df_clean);","83ff9707":"sns.set(rc={'figure.figsize':(35.7,16.27)})\nsns.countplot(x=\"Fare\",data=Train_df_clean);","b0cfc4ee":"def agecat(Age):\n    Ageclass = 1\n    if Age <= 5 :\n        Ageclass = 1\n    elif ( Age > 5 and Age <= 8 ):\n        Ageclass = 2\n    elif ( Age > 8 and Age <= 13 ):\n        Ageclass = 3\n    elif ( Age > 13 and Age <= 20 ):\n        Ageclass = 4\n    elif ( Age > 20 and Age <= 40 ):\n        Ageclass = 5\n    elif ( Age > 40 and Age <= 60 ):\n        Ageclass = 6\n    else : \n        Ageclass = 7\n    return Ageclass\n\ndef child(Age):\n    ischild = 1\n    if Age <= 5:\n       ischild = 1\n    else:\n       ischild = 0\n    return ischild\n\ndef farecat(Fare):\n    Fareclass = 1\n    if Fare <= 5 :\n        Fareclass = 1\n    elif ( Fare > 5 and Fare < 15 ):\n        Fareclass = 2\n    elif ( Fare > 15 and Fare < 30 ):\n        Fareclass = 3\n    elif ( Fare > 30 and Fare < 50 ):\n        Fareclass = 4\n    else : \n        Fareclass = 5\n    return Fareclass\n\n#Train_df_clean[\"Ageclass\"] = Train_df_clean[\"Age\"].apply(agecat)\n#Test_df_clean[\"Ageclass\"] = Test_df_clean[\"Age\"].apply(agecat)\n#Train_df_clean[\"Fareclass\"] = Train_df_clean[\"Fare\"].apply(farecat)\n#Test_df_clean[\"Fareclass\"] = Test_df_clean[\"Fare\"].apply(farecat)\nTrain_df_clean[\"ischild\"] = Train_df_clean[\"Age\"].apply(child)\nTest_df_clean[\"ischild\"] = Test_df_clean[\"Age\"].apply(child)\n","f4e3604d":"Train_df_clean.head(10)","13cac875":"#sns.countplot(x=\"Ageclass\", hue=\"Survived\",data=Train_df_clean);","84b9674b":"#sns.countplot(x=\"Fareclass\", hue=\"Survived\",data=Train_df_clean);","8059d381":"corr_matrix = Train_df_clean.corr()\ncorr_matrix[\"Survived\"].sort_values(ascending=False)","db1981c8":"#scaler = MinMaxScaler()\n\n#Train_df_clean[[\"Age\",\"Fare\"]] = scaler.fit_transform(Train_df_clean[[\"Age\",\"Fare\"]])\n#Test_df_clean[[\"Age\",\"Fare\"]] = scaler.fit_transform(Test_df_clean[[\"Age\",\"Fare\"]])\n#Train_df_clean.head(20)\n\n#Train_df_clean = Train_df_clean.drop(\"Age\",axis=1).drop(\"Fare\",axis=1)\n#Test_df_clean = Test_df_clean.drop(\"Age\",axis=1).drop(\"Fare\",axis=1)","1b62a807":"y = Train_df_clean[\"Survived\"]\nX = Train_df_clean.drop(\"Survived\",axis=1)\nX_test = Test_df_clean","fef9da30":"from sklearn.model_selection import train_test_split\nX_tr,X_te,y_tr,y_te = train_test_split(X,y,random_state = 42,test_size=0.01)","59fdd37a":"from sklearn import tree\n# Initialize our decision tree object\n#titanic_tree = tree.DecisionTreeClassifier()\n\n# Train our decision tree (tree induction + pruning)\n#titanic_tree.fit(X_tr,y_tr)","faf24211":"#y_tr_pred = titanic_tree.predict(X_tr)","91256046":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\n\n#print(confusion_matrix(y_tr, y_tr_pred))\n#print(accuracy_score(y_tr, y_tr_pred))\n#print(recall_score(y_tr, y_tr_pred))\n#print(precision_score(y_tr, y_tr_pred))","0abe91e3":"#y_te_pred = titanic_tree.predict(X_te)","abb6417c":"#print(confusion_matrix(y_te, y_te_pred))\n#print(accuracy_score(y_te, y_te_pred))\n#print(recall_score(y_te, y_te_pred))\n#print(precision_score(y_te, y_te_pred))","695e8da6":"#param_grid = { \n #'n_estimators': [150,200],\n #'max_features': ['auto', 'sqrt', 'log2'],\n #'max_depth' : [5,8],\n #'criterion' :['gini', 'entropy'],\n #'min_samples_split': [3,5],\n #'min_samples_leaf': [3,5,20]\n#}","aa461024":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n#Initialize our Random Forest object\n#titanic_forest = RandomForestClassifier(random_state=42)\n\n#CV_titanic_forest = GridSearchCV(estimator=titanic_forest, param_grid=param_grid, cv= 8)\n#CV_titanic_forest.fit(X_tr, y_tr)\n\n#CV_titanic_forest.best_params_","9deb4bba":"\n# Initialize our Random Forest object\ntitanic_forest_final = RandomForestClassifier(random_state=42,max_features=8, n_estimators= 2500, max_depth=8, criterion='gini',min_samples_split=5,min_samples_leaf=1)\n\n# Train our Random Forest model\ntitanic_forest_final.fit(X_tr,y_tr)","4a74873f":"y_tr_pred = titanic_forest_final.predict(X_tr)\n\nprint(confusion_matrix(y_tr, y_tr_pred))\nprint(accuracy_score(y_tr, y_tr_pred))\nprint(recall_score(y_tr, y_tr_pred))\nprint(precision_score(y_tr, y_tr_pred))","05d48aa1":"#y_te_pred = titanic_forest_final.predict(X_te)\n\n#print(confusion_matrix(y_te, y_te_pred))\n#print(accuracy_score(y_te, y_te_pred))\n#print(recall_score(y_te, y_te_pred))\n#print(precision_score(y_te, y_te_pred))\n","aab31716":"from sklearn.linear_model import LogisticRegression\n#titanic_lr = LogisticRegression()\n\n#titanic_lr.fit(X_tr,y_tr)\n","33b1501d":"#X_tr_matrix = X_tr.as_matrix()\n#X_te_matrix = X_te.as_matrix()\n#X_test_matrix = X_test.as_matrix()","01640972":"#titanic_gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.03).fit(X_tr_matrix, y_tr)\n\n#y_tr_pred = titanic_gbm.predict(X_tr_matrix)\n\n#print(confusion_matrix(y_tr, y_tr_pred))\n#print(accuracy_score(y_tr, y_tr_pred))\n#print(recall_score(y_tr, y_tr_pred))\n#print(precision_score(y_tr, y_tr_pred))\n\n","fc419553":"#y_te_pred = titanic_gbm.predict(X_te_matrix)\n\n#print(confusion_matrix(y_te, y_te_pred))\n#print(accuracy_score(y_te, y_te_pred))\n#print(recall_score(y_te, y_te_pred))\n#print(precision_score(y_te, y_te_pred))\n","e07c4d99":"#y_pred_test = titanic_tree.predict(X_test)\ny_pred_test = titanic_forest_final.predict(X_test)\n#y_pred_test = titanic_gbm.predict(X_test_matrix)\n#y_pred_test = titanic_lr.predict(X_test)\ny_pred_test_df = pd.DataFrame(y_pred_test)\ny_pred_test_df.head()","b0b97cb2":"X_test.info()\nX_test=X_test.reset_index()\n#X_test.head()\n\ndf_submission = pd.concat([X_test[\"PassengerId\"],y_pred_test_df],axis=1)\ndf_submission.rename(columns={0:'Survived'}, inplace=True)\ndf_submission.head(10)","a01fb1db":"list(df_submission)","93d2751c":"filename = 'csv_submission.csv'\ndf_submission.to_csv(filename,index=False,header=True)\nprint('Saved file: ' + filename)","f8d397b7":"## Continue Train & Test data first glance ","b2c710e4":"## to replace Null values for AGE we train a model based on other attributes and we predict null values ","2ab70abd":"## Add actual Age values for null values in Test Dataset using data from Encyclopedia Titanica ","1990b572":"## Too many NaN values for Cabin : we drop this attribute\n## We also drop  Ticket number attributes ","ced3b05f":"## Let's see a correlation matrix","bbe9bc2c":"## Let's see the distribution of survivors using Pclass ","0a5f742e":"## Time to encode Sex attribute as numerical ","04676fb5":"## Make predictions on Test dataset using last model trained ","15679741":"## Prediction sur le fichier TEST X_te","2099c175":"# SELECTING, TRAINING, EVALUATING MODELS ","e01a3bdb":"## Let's see the distribution of passengers using Fare","440c2f85":"## Still one NaN value in Test file to correct","823f4627":"# FINAL SUBMISSION","e852ca2d":"## Predict & Score XGB on X_te test file","81ec75ad":"## Woww , better chance to survive as a woman ..","ed9ae2c9":"## Fit & predict XGB on X_tr training file","0bb1be46":"## And Test Data","0b23517e":"# EDA - DATA EXPLORATION ","e3166752":"## Better to be a baby to survive...","477330b1":"## Lets us try a Logistic Regression model","79095889":"## Add a category \"Title\" based on name : Mrs, Mr,Miss, Other then drop Name & add Familysize feature","1ad6e268":"## Use our Random Forest model on X_te test file to check its performance ","7ba45f5f":"## Not surprising the Upper class members had better chance to embark on a lifeboat ","05c076ea":"## Actually, on new data , the accuracy score is lower at 0.73 & only 72% of Survivors predicted correctly (Recall ) .\n## Several ways to improve this : \n## K-fold cross validation ( evaluating the Decision Tree K times using training data subsets ),\n## and \/ or using another algorithm : Random Forest training multiple decision trees . Let's start with a Random Forest model","57bb4e4f":"## OK... Accuracy is better than with pure Decision Tree ( 0.79 vs 0.76 ) , but recall still at at 72% ... will need rework later...","bb331d4d":"## review dataframe after cleaning & encoding","0311d66d":"# DATA PRE-PROCESSING","844336ec":"## Let's see the distribution of passengers using Age","b296e480":"![](https:\/\/goo.gl\/images\/r3NxMN)","2163b75d":"## Let's see the distribution of survivors between male & female ","0615c76a":"## So, we have in Train file 177 null values for Age ( 20% of the items ) , 687 for Cabin & 2 for Embarked \n## We could replace Age null values using data about passengers from https:\/\/www.encyclopedia-titanica.org\/ but it is a bit cumbersome, since no xls export is available  : as a short term measure, we replace instances  NaN Age values by the median Age value in the Train & Test files. \n","3c8d5ae5":"## Now we fit a model : Decision Tree , on X_tr part of X train dataframe","aa1a6a52":"## Let's see the distribution of survivors using Familysize","da2a866f":"## Such high accuracy ... Seems there is overfitting as usual with Decision Tree... Let's check the score on the X_te file ","536bcefc":"## Prediction sur le fichier X_tr","a260a954":"## Indexing Train_df et Test_df with PassengerId column ","068ba18f":"## So now we split labels & features ","e401c2cb":"   ##  First examination of Train data","44297095":"## Time to encode embarked attribute as numerical ","7b71b8af":"## Now we apply feature scaling for Age & Fare","68b7e8bf":"## Add a category \"Title\" based on name : Mrs, Mr,Miss, Other then drop Name & add Familysize feature","df6f69d7":" ## We add  features for Age, Childhood, Fare : Ageclass , Fareclass\n## Age 0-5 :  1  , 6-10 : 2 , 10-20 : 3 , 20-40 : 4, 40-60 : 5, >60 : 6 \n","fd11e02c":"## We split the Train dataframe into Train & Test ","c6e45ebf":"## A look at distribution of Survivors according to Age & Fare Class & IsChild","890d666c":"## Build fichier de predictions pour le fichier de TEST","93b47000":"# This is a basic first version of TITANIC competition to make a first Submission on Kaggle","3f69b19a":"## Let's try an Extreme Gradient Boosting model ( XGB ) now ","97563d03":"## Calculer la matrice de confusion pour le fichier de Training","40bf7010":"## Import input files in pandas dataframes "}}