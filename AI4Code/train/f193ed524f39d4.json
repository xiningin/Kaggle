{"cell_type":{"7781b088":"code","cda8ca2d":"code","40618d10":"code","31c63b46":"code","2e896ef7":"code","5647a6fc":"code","5ab18019":"code","3c928af6":"code","df3d3b23":"code","ba47148c":"code","883b9e11":"code","1ff3c13b":"code","3e5df334":"code","2874510a":"code","c9d3fe72":"code","d0f0d133":"markdown","e4cf1219":"markdown","037ecd07":"markdown","5599f46b":"markdown","41357653":"markdown","aa2c0e73":"markdown","a5ca42f2":"markdown","0ebd124c":"markdown"},"source":{"7781b088":"import numpy as np \nimport pandas as pd\nimport os\nimport string\nfrom unidecode import unidecode\nimport dill\nfrom IPython.display import Image as IPythonImage, display\nfrom PIL import Image\nimport pytesseract\nfrom Levenshtein import jaro_winkler\nimport matplotlib.pyplot as plt","cda8ca2d":"# Look at the labels\nvalidation_df = pd.read_csv(\"..\/input\/handwriting-recognition\/written_name_validation_v2.csv\")\nvalidation_df.head()","40618d10":"# Display a few randomly chosen images along with their labels\nPATH = '..\/input\/handwriting-recognition\/validation_v2\/validation'\n\n# Print randomly selected images and their labels\nNUMBER_OF_IMAGES = 4\nrandom_index = np.random.choice(validation_df.shape[0], NUMBER_OF_IMAGES, False)\nfor row in validation_df.loc[random_index, :].itertuples(index=False):\n    filename = os.path.join(PATH, row.FILENAME)\n    print(row.IDENTITY)\n    display(IPythonImage(filename=filename))","31c63b46":"# Iterate over the data-frame, running Tesseract OCR and recording the output of Tesseract\n# Takes a fair bit of time (2 hours)\noutput = {}\nfor row in validation_df.itertuples():\n    file_path = os.path.join(PATH, row.FILENAME)\n    image = Image.open(file_path)\n    text = pytesseract.image_to_string(image)\n    output[row.FILENAME] = text\n    if ((row.Index + 1) % 5000) == 0:\n        print(f\"Processed {row.Index + 1} rows\")","2e896ef7":"# Convert into df\nresult_df = pd.DataFrame()\nfor i, (k, v) in enumerate(output.items()):\n    if (i + 1) % 5000 == 0:\n        print(f\"Converted output from {i + 1} images\")\n    \n    text = [t for t in v.split('\\n') if t not in ['', ' ', '\\n', '\\x0c']]\n    \n    temp_df = pd.DataFrame({\n        'FILENAME': [k] * len(text), \n        'TEXT': text\n    })\n    result_df = pd.concat([result_df, temp_df], ignore_index=True)\n\n# Look at the top 10 text tokens (these will probably be generic terms like \"nom\")\nresult_df['TEXT'].value_counts().nlargest(10)","5647a6fc":"# Filter and get possible variations of \"Nom\", \"Prenom\" etc\nresult_df\\\n    .loc[result_df['TEXT'].str.upper().str.contains('NOM|PRENOM|DATE DE NAISSANCE CLASSE') , \"TEXT\"]\\\n    .head(30)","5ab18019":"# Helper function to remove tokens that are common words and stand-alone punctuation\ndef filter_tokens(token_list, tokens_to_filter, remove_punctuation = True):\n    # Punctuation and empty text\n    punctuation_list = list(string.punctuation) + ['']\n    result = []\n    for t in token_list:\n        # Remove stand-alone punctuation\/empty string\n        # Unicode has some fancy punctuation marks (slanting quotes, for example)\n        # remove them using unidecode\n        if remove_punctuation and unidecode(t) in punctuation_list:\n            continue\n        \n        # Remove if any token in tokens_to_filter list is a substring of current token\n        found = 0\n        for t_filter in tokens_to_filter:\n            if t_filter.upper() in t.upper():\n                # token is matched in tokens_to_filter list\n                found = 1\n                break\n        \n        if found == 1:\n            continue\n        else:\n            result.append(t)\n    \n    return result\n\n\n# Define text to remove\nSINGLE_TOKENS_REMOVE = ['NOM', 'PRENOM']\nMULTI_TOKENS_REMOVE = ['DATE DE NAISSANCE CLASSE']\n\n# Remove tokens\nresult_df['CLEAN_TEXT'] = result_df['TEXT']\\\n    .str\\\n    .split(' ')\\\n    .apply(lambda c: ' '.join(filter_tokens(c, SINGLE_TOKENS_REMOVE)))\n\nfor multi_token in MULTI_TOKENS_REMOVE:\n    result_df['CLEAN_TEXT'] = np.where(result_df['CLEAN_TEXT'].str.contains(multi_token), \n                                       '', \n                                       result_df['CLEAN_TEXT'])\n\nresult_df.head(40)","3c928af6":"# Combine remaining multiple words into 1 (per filename)\nclean_result = result_df\\\n    .groupby('FILENAME')['CLEAN_TEXT']\\\n    .apply(''.join)\\\n    .reset_index()\n\nclean_result.head(20)","df3d3b23":"# Create 1 dataframe with both actual and OCR labels\nocr_vs_actual = validation_df.merge(clean_result, how='left', on='FILENAME')\n\n# Remove labels which do not exist\nocr_vs_actual = ocr_vs_actual.loc[ocr_vs_actual['IDENTITY'].notnull(), :]\n\n# Remove spaces in OCR output\nocr_vs_actual['CLEAN_TEXT'] = ocr_vs_actual['CLEAN_TEXT'].str.replace('\\\\s', '', regex=True)\nocr_vs_actual.head(10)","ba47148c":"# Create jaro-winkler similarity score\nvectorized_jaro_winkler = np.vectorize(jaro_winkler)\n\nocr_vs_actual['SIMILARITY_SCORE'] = vectorized_jaro_winkler(ocr_vs_actual['IDENTITY'].str.upper(), \n                                                            np.where(ocr_vs_actual['CLEAN_TEXT'].isnull(), \n                                                                     '', \n                                                                     ocr_vs_actual['CLEAN_TEXT'].str.upper()))\nocr_vs_actual.head(10)","883b9e11":"# Plot histogram of similarity scores to see how well we did\nplt.style.use('seaborn-white')\nplt.figure(figsize=(8,3), dpi=120)\nplt.hist(ocr_vs_actual['SIMILARITY_SCORE'], bins=50, alpha=0.5, color='steelblue', edgecolor='none')\nplt.title('Histogram of Jaro-Winkler similarity score between label and OCR-results')\nplt.show()","1ff3c13b":"# Create bins. Non-uniform width.\nsecond_lowest_score = ocr_vs_actual.loc[(ocr_vs_actual['SIMILARITY_SCORE'] != 0), 'SIMILARITY_SCORE'].min()\n\nocr_vs_actual['BINS'] = pd.cut(ocr_vs_actual['SIMILARITY_SCORE'], \n                               bins=[0] + np.linspace(second_lowest_score, 0.95, 9).tolist() + [0.96, 0.97, 0.98, 0.99, 1.01], \n                               labels = ['no-match', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', 'best-match'],\n                               right=False)\nocr_vs_actual['BINS'].value_counts().sort_index()","3e5df334":"# Highest similarity score images\nNUMBER_OF_IMAGES = 5\nrandom_filename = np.random.choice(ocr_vs_actual.loc[ocr_vs_actual['BINS'] == 'best-match', 'FILENAME'].tolist(), NUMBER_OF_IMAGES, False)\nfor row in ocr_vs_actual.loc[ocr_vs_actual['FILENAME'].isin(random_filename), :].itertuples(index=False):\n    filename = os.path.join(PATH, row.FILENAME)\n    print(f\"\"\"Filename: {row.FILENAME}\\nActual: {row.IDENTITY}\\nOCR: {row.CLEAN_TEXT}\"\"\")\n    display(IPythonImage(filename=filename))","2874510a":"# Mid-range similarity score images\nNUMBER_OF_IMAGES = 5\nrandom_filename = np.random.choice(ocr_vs_actual.loc[ocr_vs_actual['BINS'] == '8', 'FILENAME'].tolist(), NUMBER_OF_IMAGES, False)\nfor row in ocr_vs_actual.loc[ocr_vs_actual['FILENAME'].isin(random_filename), :].itertuples(index=False):\n    filename = os.path.join(PATH, row.FILENAME)\n    print(f\"\"\"Filename: {row.FILENAME}\\nActual: {row.IDENTITY}\\nOCR: {row.CLEAN_TEXT}\"\"\")\n    display(IPythonImage(filename=filename))","c9d3fe72":"# No matches\nNUMBER_OF_IMAGES = 5\nrandom_filename = np.random.choice(ocr_vs_actual.loc[ocr_vs_actual['BINS'] == 'no-match', 'FILENAME'].tolist(), NUMBER_OF_IMAGES, False)\nfor row in ocr_vs_actual.loc[ocr_vs_actual['FILENAME'].isin(random_filename), :].itertuples(index=False):\n    filename = os.path.join(PATH, row.FILENAME)\n    print(f\"\"\"Filename: {row.FILENAME}\\nActual: {row.IDENTITY}\\nOCR: {row.CLEAN_TEXT}\"\"\")\n    display(IPythonImage(filename=filename))","d0f0d133":"Looking at the first 10 data points, it looks like results with high similarity score are pretty good.\n\nLet's plot the distribution of the similarity score to quickly see how we did on the whole population.","e4cf1219":"The results seem OK for a baseline approach, using out-of-the-box software. 13K high quality results out of 42K (30% success rate).\n\nLet's create bins based on the similarity score and sample a few images to see how we did. The goal is to see how hard the image actually was, for the OCR model to make mistakes.","037ecd07":"## Summary of the Tesseract approach\n\nTesseract is, possibly, the most mature open-source OCR software with a focus on working well out-of-the-box and using classical CV, modern DL techniques, as well as language models to correct common parsing errors. \n\nHowever, it looks like OCR is still not a fully solved problem especially for hand-written text. \n\nAnalysis of the OCR results show that even easy-to-read hand-written text (easy for me to read, at least) still suffer from false parsing.\n\nI will explore this problem further in later iterations of the notebook.","5599f46b":"We see, from the most frequent text tokens, that there's a secondary problem of removing the ancilliary text (form field data). \n\nIdeally, we would adopt an approach based on how \"centered\" a text segment is in the image:\n1. Create bounding boxes for all words\n2. Keep words that are in bounding boxes near the center of the image\n3. Also filter out any common words (like \"Nom\", \"Prenom\" etc)\n\nThe Tesseract software provides support for returning the bounding boxes of text read from images. \n\nMy current implementation doesn't take advantage of that yet, however, so I will implement step 3 only (for now).","41357653":"### Baseline Performance using Text-Similarity Metrics\n\nFor computing the performance, I will be using the [Jaro-Winkler algorithm](https:\/\/en.wikipedia.org\/wiki\/Jaro%E2%80%93Winkler_distance) to detect similarity between the captured text and the actual text.\n\nThis is necessary because OCR is a hard task, and we don't want to mark a near-miss (maybe because there's a mismatch in 1 character) as a failure. This also allows us to get a quantitative understanding of how good our OCR system is.","aa2c0e73":"# Optical Character Recognition - Handwriting Recognition\n\nThis notebook looks at computer vision techniques for OCR, specifically for handwriting recognition.\n\nSkimming through the data-set provided, we can see that there is a large training sample (> 300K) and the images are gray-scale, with black text on white background. Many of the images have the person writing their name in block letters, which is good since identifying cursive letters is a harder challenge for OCR software.\n\nThe dataset seems to have images from a hand-filled form, and we can see that there is also text from the form fields as well (For example, \"nom\" (which is \"Last Name\")). This will be a challenge, since the OCR method will pick up these text as well and we will need to filter them out.\n\nLet's look at 4 random images and their corresponding labels below.","a5ca42f2":"## Baseline model using Tesseract-OCR\n\nOCR has been actively researched since long before the resurgence of deep learning. Accordingly, there are solutions that use no deep learning at all, and use classical computer vision approaches.\n\nThe [Tesseract Project](https:\/\/github.com\/tesseract-ocr\/tesseract) is an open source OCR project that started in 1985 in HP labs. In 2006, HP handed over the project to Google.\n\nThe current version of Tesseract, Tesseract 4.0, uses a blended pipeline of computer vision and deep learning, utilizing an LSTM model to detect text lines in the image. \n\nHere is how the pipeline looks like (taken from a [2016 talk by Google engineers](https:\/\/github.com\/tesseract-ocr\/docs\/blob\/master\/das_tutorial2016\/2ArchitectureAndDataStructures.pdf) on the Tesseract internals):\n\n![Tesseract_pipeline.png](attachment:Tesseract_pipeline.png)\n\nIn fact, I was surprised to discover that **the Tesseract project implemented an LSTM model before TensorFlow**! Details are present [here](https:\/\/github.com\/tesseract-ocr\/docs\/blob\/master\/das_tutorial2016\/6ModernizationEfforts.pdf) (navigate to the section called \"What about TensorFlow\")\n\nFor the baseline model, I will be using PyTesseract, which is a Python binding to the Tesseract API. I won't actually be training a model, since Tesseract comes fully loaded and is supposed to work out-of-the-box. They do provide an API to train an OCR model, which I will explore in a later iteration.","0ebd124c":"It looks like handling these common words is going to be a bit of a pain. \n\nHere's 1 strategy:\n\n1. Tokenize based on spaces\n2. For each token, if these common words form a substring, then delete the word\n3. If a token is a special character (like \":\" or \"=\") delete it (since it's probably not someone's name)\n4. Recombine remaining tokens separated by space\n\nExamples:\n\n1. \"NOM: DEBRRD\" -> \"DEBRRD\" (Delete 'NOM:' and keep the rest of the text)\n2. \"NOM: = FAURE\" -> \"FAURE\" (Delete 'NOM:' as well as '=')"}}