{"cell_type":{"6edd2e58":"code","3942ab16":"code","916bad7a":"code","da2d4b97":"code","6e244977":"code","0c1248ac":"code","eb1a48cd":"code","881ad828":"code","4dfd7515":"code","2a589076":"code","006a0552":"code","036c8b7f":"code","3fd9e1c3":"code","b5ac0be1":"code","cbac9d68":"code","355dddb0":"code","a8f7300b":"code","0f96df84":"code","a0d0beea":"code","a0f2ddd5":"code","fc616661":"code","54de8dbb":"code","bdac9135":"code","4b73541b":"code","5c81b291":"code","ed6005b7":"code","2e768147":"code","bb4dfc5c":"code","95271e29":"code","cba36c4f":"code","3ffb794a":"code","428edf53":"code","45d33bb9":"code","d7aa0c40":"code","9e922bcf":"code","3b350deb":"code","6f7fba9c":"code","8a3b0879":"code","5c692055":"code","ea299ce6":"code","0a40620c":"code","341b5a40":"code","f52de045":"code","2f257e13":"markdown","0e740a91":"markdown","f7a6f1f7":"markdown","a2533a10":"markdown","73bdd8db":"markdown","d5620e41":"markdown","db0cf954":"markdown","ad19ad56":"markdown","b9e7c0a7":"markdown","996af25a":"markdown","31886fcd":"markdown","4144e158":"markdown","e363ebbc":"markdown","7ba992dc":"markdown","33bc78cb":"markdown","752b1fbb":"markdown","60497f94":"markdown","7dc7c87e":"markdown","811c905b":"markdown","8cd2d706":"markdown","7dd9cf25":"markdown","d04f5247":"markdown","4eaa024d":"markdown","a71fbe5c":"markdown","e05ee5cc":"markdown","3e99b2ef":"markdown","540a18e9":"markdown","9e319281":"markdown","909887ca":"markdown","a1b54792":"markdown","7ce48709":"markdown","6b34fb24":"markdown","0b95bea3":"markdown","ee12bbf7":"markdown","b7addd1b":"markdown","2b8e6072":"markdown","5194cbe8":"markdown","3b4fdda8":"markdown","b9e9886a":"markdown","d3383b0b":"markdown","9ef60405":"markdown"},"source":{"6edd2e58":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport scipy.stats as stats\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import RFECV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","3942ab16":"dataset = pd.read_csv('..\/input\/winequality-red.csv')\ndataset.info()","916bad7a":"dataset.head()","da2d4b97":"dataset.quality.value_counts()","6e244977":"plt.figure(figsize=(10, 5))\nsns.countplot(x='quality', data=dataset)\nplt.xlabel('Quality', fontsize=14)\nplt.ylabel('Count', fontsize=14)","0c1248ac":"dataset.describe()","eb1a48cd":"dataset.hist(bins=50, figsize=(15, 15))","881ad828":"dataset.quality[dataset.quality < 7] = 0\ndataset.quality[dataset.quality >= 7] = 1","4dfd7515":"dataset.quality.value_counts()","2a589076":"plt.figure(figsize=(10, 5))\nsns.countplot(x='quality', data=dataset)\nplt.xlabel('Quality', fontsize=14)\nplt.ylabel('Count', fontsize=14)","006a0552":"redwine = dataset.copy()\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(x='quality', y='alcohol', data=redwine)\nplt.title('Boxplot for alcohol', fontsize=16)\nplt.xlabel('Quality', fontsize=14)\nplt.ylabel('Alcohol', fontsize=14)","036c8b7f":"plt.figure(figsize=(16, 5))\n\nplt.subplot(121)\nsns.distplot(redwine[redwine.quality==0]['alcohol'], color='r')\nplt.title('Distribution of alcohol for \"not good\" wine', fontsize=16)\nplt.xlabel('Alcohol', fontsize=14)\nplt.subplot(122)\nsns.distplot(redwine[redwine.quality==1]['alcohol'], color='c')\nplt.title('Distribution of alcohol for good wine', fontsize=16)\nplt.xlabel('Alcohol', fontsize=14)","3fd9e1c3":"plt.figure(figsize=(8, 5))\nsns.boxplot(x='quality', y='volatile acidity', data=redwine)\nplt.title('Boxplot for volatile acidity', fontsize=16)\nplt.xlabel('Quality', fontsize=14)\nplt.ylabel('Volatile Acidity', fontsize=14)","b5ac0be1":"plt.figure(figsize=(16, 5))\n\nplt.subplot(121)\nsns.distplot(redwine[redwine.quality==0]['volatile acidity'], color='r')\nplt.title('Distribution of volatile acidity for \"not good\" wine', fontsize=16)\nplt.xlabel('Volatile Acidity', fontsize=14)\nplt.subplot(122)\nsns.distplot(redwine[redwine.quality==1]['volatile acidity'], color='c')\nplt.title('Distribution of volatile acidity for good wine', fontsize=16)\nplt.xlabel('Volatile Acidity', fontsize=14)","cbac9d68":"data = redwine.loc[:, ['fixed acidity', 'volatile acidity', 'citric acid']]\nax = sns.PairGrid(data)\nax.map_lower(sns.kdeplot)\nax.map_upper(sns.scatterplot)\nax.map_diag(sns.kdeplot)","355dddb0":"ax = sns.jointplot(x=redwine.loc[:, 'free sulfur dioxide'], y=redwine.loc[:, 'total sulfur dioxide'], kind='reg')\nax.annotate(stats.pearsonr)","a8f7300b":"plt.figure(figsize=(12,5))\n\nplt.subplot(121)\nsns.boxplot(x='quality', y='free sulfur dioxide', data=redwine)\nplt.title('Boxplot for free sulphur dioxide', fontsize=16)\nplt.xlabel('Quality', fontsize=14)\nplt.ylabel('Free sulfur Dioxide', fontsize=14)\nplt.subplot(122)\nsns.boxplot(x='quality', y='total sulfur dioxide', data=redwine)\nplt.title('Boxplot for total sulphur dioxide', fontsize=16)\nplt.xlabel('Quality', fontsize=14)\nplt.ylabel('Total sulfur Dioxide', fontsize=14)","0f96df84":"plt.figure(figsize=(8,5))\nsns.boxplot(x='quality', y='residual sugar', data=redwine)\nplt.title('Boxplot for Residual sugar', fontsize=16)\nplt.xlabel('Quality', fontsize=14)\nplt.ylabel('Residual Sugar', fontsize=14)","a0d0beea":"plt.figure(figsize=(8,5))\nsns.boxplot(x='quality', y='chlorides', data=redwine)\nplt.title('Boxplot for Chlorides', fontsize=16)\nplt.xlabel('Quality', fontsize=14)\nplt.ylabel('Chlorides', fontsize=14)","a0f2ddd5":"plt.figure(figsize=(16, 5))\n\nplt.subplot(121)\nsns.distplot(redwine[redwine.quality==0]['pH'], color='r')\nplt.title('Distribution of pH for \"not good\" wine', fontsize=16)\nplt.xlabel('pH', fontsize=14)\nplt.subplot(122)\nsns.distplot(redwine[redwine.quality==1]['pH'], color='c')\nplt.title('Distribution of pH for good wine', fontsize=16)\nplt.xlabel('pH', fontsize=14)","fc616661":"for col, x in zip(('k','c','g'),('fixed acidity', 'volatile acidity', 'citric acid')):\n    ax = sns.jointplot(x=x, y='pH', data=redwine, kind='reg', color=col)\n    ax.annotate(stats.pearsonr)\n    plt.xlabel(x, fontsize=14)\n    plt.ylabel('pH', fontsize=14)","54de8dbb":"plt.figure(figsize=(16, 5))\n\nplt.subplot(121)\nsns.distplot(redwine[redwine.quality==0]['density'], color='r')\nplt.title('Distribution of density for \"not good\" wine', fontsize=16)\nplt.xlabel('Density', fontsize=14)\nplt.subplot(122)\nsns.distplot(redwine[redwine.quality==1]['density'], color='c')\nplt.title('Distribution of density for good wine', fontsize=16)\nplt.xlabel('Density', fontsize=14)","bdac9135":"for col, x in zip(('k','c'),('alcohol', 'residual sugar')):\n    ax = sns.jointplot(x=x, y='density', data=redwine, kind='reg', color=col)\n    ax.annotate(stats.pearsonr)\n    plt.xlabel(x, fontsize=14)\n    plt.ylabel('density', fontsize=14)","4b73541b":"# Separating independent and dependent variable\nredwine_X = redwine.drop('quality', axis=1)\nredwine_y = redwine['quality']\ncol = redwine_X.columns\n\n# Standardization\nsd = StandardScaler()\nredwine_X_scaled = sd.fit_transform(redwine_X)\nredwine_X_scaled = pd.DataFrame(redwine_X_scaled, columns=col)\n\n# Swarmplot for first 5 features\ndata = pd.concat([redwine_y, redwine_X_scaled.iloc[:, :5]], axis=1)\ndata = pd.melt(data, id_vars='quality', var_name='feature', value_name='value')\n\nplt.figure(figsize=(12,6))\nsns.swarmplot(x='feature', y='value', hue='quality', data=data)\nplt.title('Swarmplot for first 5 features', fontsize=16)\nplt.xlabel('Feature', fontsize=14)\nplt.ylabel('Value', fontsize=14)","5c81b291":"# Swarmplot for last 6 features\ndata = pd.concat([redwine_y, redwine_X_scaled.iloc[:, 5:]], axis=1)\ndata = pd.melt(data, id_vars='quality', var_name='feature', value_name='value')\n\nplt.figure(figsize=(12,6))\nsns.swarmplot(x='feature', y='value', hue='quality', data=data)\nplt.title('Swarmplot for last 6 features', fontsize=16)\nplt.xlabel('Feature', fontsize=14)\nplt.ylabel('Value', fontsize=14)","ed6005b7":"plt.figure(figsize=(10, 5))\nsns.heatmap(redwine.corr(), annot=True, fmt='.2f')","2e768147":"dataset_additional = dataset.copy()\n\ndataset_additional['total_acidity'] = dataset_additional['fixed acidity'] + dataset_additional['volatile acidity']\ndataset_additional['total_acidity_citric'] = dataset_additional['total_acidity'] + dataset_additional['citric acid']\ndataset_additional['bound_sulphur_dioxide'] = dataset_additional['total sulfur dioxide'] - dataset_additional['free sulfur dioxide']\ndataset_additional['org_minus_fail_SG'] = (dataset_additional['alcohol'] * 7.36)\/1000","bb4dfc5c":"plt.figure(figsize=(12, 6))\nsns.heatmap(dataset_additional.corr(), annot=True, fmt='.2f')","95271e29":"X = dataset.drop('quality', axis=1)\ny = dataset['quality']\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(X, y):\n    strat_train_set = dataset.loc[train_index]\n    strat_test_set = dataset.loc[test_index]","cba36c4f":"strat_train_set['quality'].value_counts()\/len(strat_train_set), strat_test_set['quality'].value_counts()\/len(strat_test_set)","3ffb794a":"# Check it with the actual dataset proportions\ndataset['quality'].value_counts()\/len(dataset)","428edf53":"X_train = strat_train_set.drop('quality', axis=1)\ny_train = strat_train_set['quality']\nX_test = strat_test_set.drop('quality', axis=1)\ny_test = strat_test_set['quality']","45d33bb9":"sd = StandardScaler()\n\nX_train_scaled = sd.fit_transform(X_train)","d7aa0c40":"log_reg = LogisticRegression(random_state=42)\nsvm_clf = SVC(random_state=42)\ntree_clf = DecisionTreeClassifier(random_state=42)\nforest_clf = RandomForestClassifier(random_state=42)\n\nX_test_scaled = sd.transform(X_test)\n\nfor clf in (log_reg, svm_clf, tree_clf, forest_clf):\n    predicted = cross_val_predict(clf, X_train_scaled, y_train, cv=10)\n    print(clf.__class__.__name__, ': ', accuracy_score(y_train, predicted))","9e922bcf":"param_grid = [\n    {'n_estimators': [10, 25, 50, 100, 150], 'max_features': [2,3,6,9]},\n    {'bootstrap': [False], 'n_estimators': [10, 25, 50, 100, 150], 'max_features': [2,3,6,9]}\n]\n\nforest_clf = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(forest_clf, param_grid=param_grid, cv=5)\ngrid_search.fit(X_train_scaled, y_train)","3b350deb":"grid_search.best_params_","6f7fba9c":"attributes = X.columns\nsorted(zip(grid_search.best_estimator_.feature_importances_, attributes), reverse=True)","8a3b0879":"# Feature Selection\nforest_clf_2 = RandomForestClassifier()\nrfecv = RFECV(forest_clf_2, step=1, cv=5, scoring='accuracy')\nrfecv.fit(X_train_scaled, y_train)","5c692055":"print('Optimal no. of features: ', rfecv.n_features_)\nprint('Best Features: ', attributes[rfecv.support_])","ea299ce6":"# No. of features vs CV scores\nplt.xlabel('number of features', fontsize=14)\nplt.ylabel('CV Score', fontsize=14)\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)","0a40620c":"attributes_selected = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'total sulfur dioxide', 'density', 'sulphates', 'alcohol']\n\nX_train_selected = strat_train_set[attributes_selected]\nX_test_selected = strat_test_set[attributes_selected]\n\nX_train_sel_scaled = sd.fit_transform(X_train_selected)\nX_test_sel_scaled = sd.transform(X_test_selected)\n\ngrid_search.best_estimator_.fit(X_train_sel_scaled, y_train)","341b5a40":"y_pred = grid_search.best_estimator_.predict(X_test_sel_scaled)\naccuracy_score(y_test, y_pred)","f52de045":"confusion_matrix(y_test, y_pred)","2f257e13":"Now let's *standardize* our dataset as all features are on different scales.","0e740a91":"AND WE GOT AN ACCURACY OF 94.06%. Not that of a high accuracy but pretty satisfying! Also looking at the confusion matrix we can see that out of 320 instances in our test set, we made 301 correct predictions and 19 wrong predictions.\n\nThank you for looking this kernel. This is my very first kernel and i am still learning. Let me know your thoughts, be open to find any mistakes, or ways\/alternatives I could have done better. And lastly, if you do like it then don't forget to have a glass of wine! Cheers!!","f7a6f1f7":"## Exploratory Data Analysis","a2533a10":"So yes they have a good correlation of 0.67. How are they related to wine quality tho?","73bdd8db":"Lets start training our models. We will try several algorithms and see which model gives us the highest accuracy. We will evaluate using cross-validation. The following code performs *K-fold Cross-Validation* where k=10. So basically training set will be splitted into 10 subsets, then the model will train on 9 of the subsets and evaluate on the other subset. It repeats this process 10 times.","d5620e41":"## Training model using Cross-Validation","db0cf954":"Hmmmmm! Seems like pH does not have to do anything with the quality of our wines. Also I am thinking whether it does have any relation with our *acid* features.","ad19ad56":"**density** does have a slightly good negative correlation with alcohol. Increase in the alcohol content decreases the density of water! Also as this is a binary classification problem we can create swarmplots for our different features and observe if we can see a clear distinction between good and non-good wines. As the features are on different scales I'll transform them to the same scales using standardization and then create a swarmplot","b9e7c0a7":"Non-good wines do have few outliers with high alcohol content. But in general we can see that good wines have higher alcohol content.","996af25a":"I don't think so. Also, from the description it says that it is dependent on the percentage of alcohol and sugar content. So from our features I am going to check the relation of **density** with **alcohol**(duh) and **residual sugar**.","31886fcd":"Uhhhhh no! It doesn't seem to play an important role with the quality of wine. Let's check the same for **chlorides**.","4144e158":"- We can see that the median of alchol content for 'not good' and good wines are quite well separated. Maybe it would be a good feature for our classification. Let's check the distribution.","e363ebbc":"[](http:\/\/)![](http:\/\/)So it turns out 9 features play an important role in classification.","7ba992dc":"Okay so we can see that the data is unbalanced with wine quality of 5 or 6 the most frequent in our dataset. I want this to be a binary classification project so I'll just set an arbitray cut off later.\n\nLet's check the distribution first!","33bc78cb":"Most of the distributions are skewed to the right. Also density and pH show a normal distribution.\n\nAlso I would like to set an arbitrary cutoff for our target variable. As I want this to be a binary classification project, I will classify any wine with a quality 7 or greater as good(1) and the remainder as not good(0).","752b1fbb":"pH does have a good negative correlation with **citric acid** and **fixed acidity**. Let's do the same for **density**(the density of water is close to that of water depending on the percent alcohol and sugar content). \n\nThe histogram of **density** plotted above shows a normal distribution with a density ranging from 0.99 to 1.0025 which is very well in the range of water. Let's check it out if whether a wine with high\/low density has an impact on its quality.","60497f94":"Now let's do the same for **free sulfur dioxide** and **total sulfur dioxide**.","7dc7c87e":"## Get the data","811c905b":"1. * We can observe the peak at 9. Therefore we choose these 9 features and finally test it on the test set.","8cd2d706":"Similary, as you may have guessed **sulphates** and **alcohol** are well separated making it easy for classification.\n\nNow let's look for correlations!","7dd9cf25":"## Evaluate on the Test set","d04f5247":"Now lets start our classification modeling process. First with the help of stratified shuffle split lets split into train and test sets to ensure our balance of good and non-good wines is maintained as our data is highly unbalanced.","4eaa024d":"We can see that non-good wines have a slightly higher content of **free sulfur dioxide** and **total sulfur dioxide** in comparison with good wines. However, good wines do have few outliers.","a71fbe5c":"So we have 11 features and 1 target variable. The dataset is small with just 1599 instances and we do not have any null values (Yay!). Also what I notice is all the features are numerical and they have different scales.\n\nNow as we know that our target variable **quality** is categorical, let's see how many types of categories we have.","e05ee5cc":"Although we cannot see a clear distinction, we can still say that **volatile acidity**, **citric acid** would be good for classification. While **fixed acidity**, **residual sugar** and **chlorides** are too mixed up to classify between good and non-good wines. Now let's do the same for last 6 features.","3e99b2ef":"I am no expert in wines but after reading the description of **volatile acidity**(the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste) I am guessing it would have a negative relationship with quality? Let's check it out!","540a18e9":"Now that the additional features are created, lets check the heatmap again.","9e319281":"So it turns out the best parameters for our random forest classifier is to have *max_features* set to 3 and *n_estimators* set to 50. We can also look at the feature importances to see which feature plays an important role in classifying the quality of wine. ","909887ca":"## Prepare the data","a1b54792":"Hmmm, there is a fairly good positive correlation between **alcohol** and **quality**. **sulphates** and **citric acid** have a weak positive correlation with **quality**. While **volatile acidity** shows a negative correlation with **quality**.\n\nWhat more important to see is whether our features are correlated with each other? For example, **fixed acidity** has a good positive correlation with **density** and negative correlation with **pH**. But it isn't that fairly high. Similarly, we can observe other good correlations between features but none of them show a fairly high correlation, so we decide to keep all of the features for now to predict the quality of wine. \n\n## Creating new features\n\nCreating new features from existing features is also an important step to check if it improves the accuracy or even correlation with our dependent variable. After little research, I came up with the following new features:\n\n- **total_acidity** - Sum of fixed and volatile acidity. [**fixed acidity + volatile acidity**]\n- **total_acidity_citric** - I learnt that **citric acid** is a type of titratable or **total acid**, so i thought maybe i should add that too in **total acidity**?. Or else let's just create a new feature for that! [**fixed acidity + volatile acidity + citric acid**]\n- **bound_sulphur_dioxide** - **total sulphur dioxide** is actually a sum of bound(fixed) SO2 and **free sulphur dioxide**. [**total sulphur dioxide - free sulphur dioxide**]\n- **org_minus_final_SG** - $$ \\% alcohol(by volume) = \\frac{Original Specific Gravity - Final Specific Gravity}{7.36} \\times 1000 $$\n\n*Fun fact: Coke has about the same level of sugar, at 108 g\/L, as some of the sweetest dessert wines!*","7ce48709":"Now let's visualize the data! I think alcohol content would play a big role in the quality of good and not good wines.","6b34fb24":"Now what about **residual sugar**?(the amount of sugar remaining after fermentation stops). Does the remaining sugar have any impact on the quality of wines?","0b95bea3":"So the winner is RandomForest. It gave us an accuracy score of 89.91%. We still have to test it on our test set. \n\n## Fine-Tuning the model\n\nBut first let's fine-tune it a little using *GridSearchCV*! Here I am evaluating 3 x 4 = 12 combinations of *n_estimators* and *max_features* in the first dictionary. Similary, I am trying in the other dictionary but with *bootstrap* set to False.(pasting)","ee12bbf7":"<img src=\"https:\/\/unasalahat.files.wordpress.com\/2013\/06\/m2a1_2.jpg\">\n\nCheers! We will be exploring the red wine quality dataset. Every one loves a glass of wine once in a while, but what goes behind the scenes? We are provided with physiochemical features(the input) and sensory(the output) variables. Let's find what physiochemical features make the wine best! Wine quality ofcourse depends on other factors such as grape types etc. but due to the limitation of the dataset we will just focus on the physiochemical features. So let's begin!","b7addd1b":"YES! we can clearly see that non-good wines have higher volatile acidity!\n\nNow the term \"acid\" appears in three of our variables - **fixed acidity**, **volatile acidity**, **citric acid**. Are they correlated?","2b8e6072":"Nope! Just like **Residual Sugar** we cannot observe any relation with quality.\n\n**pH** : describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale. By checking the histogram of pH we can observe our wines are between 3-4 on the pH scale. But how it is ditributed between good and non-good wines?","5194cbe8":"The data is still unbalanced so when we split into training and test set we must ensure that we maintain this proportion.","3b4fdda8":"Yup! our guess was right. We can see that **fixed acidity** and **citric acid** have a fairly positive correlation, while **volatile acidity** and **citric acid** have a negative correlation. So during feature selection we might be able to eliminate one of the features(or might not!)","b9e9886a":"This additional features doesn't seem to correlate well with our dependent variable **quality**, however, something cool to observe is how high **total_acidity** and **total_acidity_citric** is positively correlated with **fixed acidity**. It's good because too much **volatile acidity** can lead to off flavors and aromas. **org_minus_fail_SG** will obviously have same correlations as that of **alcohol**. Because all the four additional features are highly correlated with other features, we will not use them in our classification model.","d3383b0b":"## Feature Selection","9ef60405":"As we can see not all features are playing an important role in classification. Therefore we need to identify how many and which features to select. Here we will be using the recursive feature elimination along with cross-validation to see the accuracy as well."}}