{"cell_type":{"4c4a4baf":"code","42d5c8ab":"code","01f03b17":"code","af43e7a7":"code","eb65bd1a":"code","d0281c97":"code","315de637":"code","dfa0a056":"code","e84089ff":"code","6d60391c":"code","97a72e86":"code","61b44045":"code","55b02c2f":"code","3b14f3c4":"code","3c30b4eb":"code","5c8f17be":"code","62cfbcbd":"code","b262c904":"code","90bf73a1":"code","14a31366":"code","4df1d89e":"code","54fed0bb":"code","e1c4eccc":"code","156596d1":"code","b4fdcaaf":"code","a5d79748":"code","4b67ffa5":"code","87ae3a28":"code","40807ccd":"code","3a095897":"code","482cd5dc":"code","e1723cf2":"code","80b278c3":"code","bfb24699":"code","db563e18":"code","33507174":"code","edac666f":"code","7ee8699c":"code","19d62fc0":"code","3ccf7648":"code","f6fcf2bd":"code","eb07b560":"code","bdddeca3":"code","cc792b7d":"code","182c267c":"code","5c0f6f81":"code","ed3d9fde":"code","e272f898":"code","e7092560":"code","1dd0ccfd":"code","4ce00f26":"code","1a7d277d":"code","a9a32be1":"code","cef0e6d9":"code","e9e8e22f":"code","e5b6bd42":"code","981f8e5c":"code","71f5ad7e":"code","b2935c71":"code","eccf6b19":"markdown"},"source":{"4c4a4baf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","42d5c8ab":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","01f03b17":"#%%capture\n#!pip install fastai==0.7.0\n#from fastai.imports import *\n#from fastai.structured import *","af43e7a7":"#from fastai.imports import *\n#from fastai.structured import *\n\nfrom pandas_summary import DataFrameSummary\nfrom IPython.display import display\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.utils.fixes import signature\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","eb65bd1a":"# reading the data\n\ntrain = pd.read_csv('..\/input\/train (1).csv', parse_dates=[\"Date.of.Birth\", \"DisbursalDate\"])\ntest = pd.read_csv('..\/input\/test_bqCt9Pv.csv', parse_dates=[\"Date.of.Birth\", \"DisbursalDate\"])\nsub = pd.read_csv('..\/input\/submission (1).csv')\n\n# getting the shapes of the datasets\nprint(\"Shape of Train :\", train.shape)\nprint(\"Shape of Test :\", test.shape)\nprint(\"Shape of submission :\", sub.shape)","d0281c97":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","315de637":"display_all(train[5:100])","dfa0a056":"unique_id_train = train['UniqueID']\nunique_id_test = test['UniqueID']\ndata = pd.concat([train, test], axis = 0, ignore_index=True)","e84089ff":"data['Disbursal_month'] = data['DisbursalDate'].dt.month","6d60391c":"display_all(data.isnull().sum().sort_index()\/len(data))\ndata.isnull().sum()","97a72e86":"#display_all(data.isnull().sum().sort_index()\/len(data))\n#data.isnull().sum()\n\"\"\"data['asset_cost_flag'] = 0 #eligible\ndata['asset_cost_flag'][data['asset_cost']>200000] = 1\ndata['asset_cost_flag'][data['asset_cost']<40000] = 1\ndata['asset_cost_flag'].value_counts()\"\"\"","61b44045":"data['disbursed_amount_flag'] = 0 # eligible\ndata['disbursed_amount_flag'][data['disbursed_amount']<8000]=1 # not eligible\ndata['disbursed_amount_flag'][data['disbursed_amount']>200000] = 1","55b02c2f":"data['Employment.Type'].fillna(1, inplace = True)# not eligible\ndata['Employment.Type'] = data['Employment.Type'].replace('Self employed', 0)\ndata['Employment.Type'] = data['Employment.Type'].replace('Salaried', 0)\n#data['Employment.Type'] = data['Employment.Type'].replace('Unemployed', 1) \ndata['Employment.Type'].value_counts()","3b14f3c4":"#Age of the customer at the time of loan disbursal\n# extracting the year of birth of the customers\ndata['Year_of_birth'] = data['Date.of.Birth'].dt.year\ndata['Age']=2018-data['Year_of_birth']\n#converting ALL negative ages to zero\ndata['Age'][data['Age'] < 0] = 0","3c30b4eb":"#calculating remaining time to retirement \ndata['time.to.retire']= 65 - data['Age']\ndata['time.to.retire'][data['time.to.retire'] < 0] = 0\ndata['time.to.retire'][data['time.to.retire'] > 47] = 0\ndata['within.age.limit'] = 1 #eligible for loan\ndata['within.age.limit'][data['Age'] < 18] = 0\ndata['within.age.limit'][data['Age'] > 65] = 0\ndata['within.age.limit'].value_counts()","5c8f17be":"# encodings for bureau score(perform cns score distribution)\n\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('No Bureau History Available', 0)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Sufficient History Not Available', 0)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Not Enough Info available on the customer', 0)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: No Activity seen on the customer (Inactive)',0)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: No Updates available in last 36 months', 0)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Only a Guarantor', 0)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: More than 50 active Accounts found',0)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('M-Very High Risk', 5)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('L-Very High Risk', 5)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('K-High Risk', 4)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('J-High Risk', 4)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('I-Medium Risk', 3)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('H-Medium Risk', 3)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('G-Low Risk', 2)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('F-Low Risk', 2)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('E-Low Risk', 2)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('D-Very Low Risk', 1)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('C-Very Low Risk', 1)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('B-Very Low Risk', 1)\ndata['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('A-Very Low Risk', 1)\n\n# checing the values in bureau score\ndata['PERFORM_CNS.SCORE.DESCRIPTION'].value_counts()","62cfbcbd":"data['manufacturer_id'] = data['manufacturer_id'].replace(86, 11)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(45, 10)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(51, 9)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(48, 8)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(49, 7)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(120, 6)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(67, 5)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(145, 4)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(153, 3)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(152, 2)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(156, 1)\ndata['manufacturer_id'] = data['manufacturer_id'].replace(155, 0)\ndata['manufacturer_id'].value_counts()","b262c904":"data['Postal.zone'] = round(data['Current_pincode_ID']\/1000)\ndata['Postal.zone'] = data['Postal.zone'].astype(int)\ndata['major_city'] = 1 # not a major city\ndata['major_city'][data['Postal.zone']==1] = 0\ndata['major_city'][data['Postal.zone']==4] = 0\ndata['major_city'][data['Postal.zone']==6] = 0\ndata['major_city'][data['Postal.zone']==7] = 0\ndata['major_city'].value_counts()","90bf73a1":"#Converting the AVERAGE.ACCT.AGE to months\ndata['AVERAGE.ACCT.AGE_year'] = data['AVERAGE.ACCT.AGE'].apply(lambda x: x.split('yrs')[0])\ndata['AVERAGE.ACCT.AGE_month'] = data['AVERAGE.ACCT.AGE'].apply(lambda x: x.split(' ')[1])\ndata['AVERAGE.ACCT.AGE_month'] = data['AVERAGE.ACCT.AGE_month'].apply(lambda x: x.split('mon')[0])\ndata['AVERAGE.ACCT.AGE_month']= data['AVERAGE.ACCT.AGE_month'].astype(int)\ndata['AVERAGE.ACCT.AGE_year'] = data['AVERAGE.ACCT.AGE_year'].astype(int)\ndata['AVERAGE.ACCT.AGE_period']= round((data['AVERAGE.ACCT.AGE_year']*12 + data['AVERAGE.ACCT.AGE_month'])\/12,1)\n\n#Converting the CREDIT.HISTORY.LENGTH to years\ndata['CREDIT.HISTORY.LENGTH_year'] = data['CREDIT.HISTORY.LENGTH'].apply(lambda x: x.split('yrs')[0])\ndata['CREDIT.HISTORY.LENGTH_month'] = data['CREDIT.HISTORY.LENGTH'].apply(lambda x: x.split(' ')[1])\ndata['CREDIT.HISTORY.LENGTH_month'] = data['CREDIT.HISTORY.LENGTH_month'].apply(lambda x: x.split('mon')[0])\ndata['CREDIT.HISTORY.LENGTH_month']= data['CREDIT.HISTORY.LENGTH_month'].astype(int)\ndata['CREDIT.HISTORY.LENGTH_year'] = data['CREDIT.HISTORY.LENGTH_year'].astype(int)\ndata['CREDIT.HISTORY.LENGTH_period']= round((data['CREDIT.HISTORY.LENGTH_year']*12 + data['CREDIT.HISTORY.LENGTH_month'])\/12,1)\ndata['CREDIT.HISTORY.LENGTH_period'].value_counts()\ndata['AVERAGE.ACCT.AGE_period']","14a31366":"data['loan_tenure_flag'] = 0\ndata['loan_tenure_flag'][data['AVERAGE.ACCT.AGE_period']<0.25]=1 #not valid\ndata['loan_tenure_flag'][data['AVERAGE.ACCT.AGE_period']>4]=1 # not valid\ndata['loan_tenure_flag'].value_counts()","4df1d89e":"#data.drop(['VoterID_flag', 'PAN_flag', 'Driving_flag'], axis = 1, inplace = True)","54fed0bb":"data['TOTAL.NO.OF.ACCTS'] = data['PRI.NO.OF.ACCTS'] + data['SEC.NO.OF.ACCTS']\ndata['TOTAL.ACTIVE.ACCTS'] = data['PRI.ACTIVE.ACCTS'] + data['SEC.ACTIVE.ACCTS']\ndata['TOTAL.OVERDUE.ACCTS'] = data['PRI.OVERDUE.ACCTS'] + data['SEC.OVERDUE.ACCTS']\ndata['TOTAL.CURRENT.BALANCE'] = data['PRI.CURRENT.BALANCE'] + data['SEC.CURRENT.BALANCE']\ndata['TOTAL.SANCTIONED.AMOUNT'] = data['PRI.SANCTIONED.AMOUNT'] +data['SEC.SANCTIONED.AMOUNT']\ndata['TOTAL.DISBURSED.AMOUNT'] = data['PRI.DISBURSED.AMOUNT'] + data['SEC.DISBURSED.AMOUNT']\ndata['TOTAL.INSTAL.AMT'] = data['PRIMARY.INSTAL.AMT'] + data['SEC.INSTAL.AMT']\ndata.drop(['PRI.NO.OF.ACCTS','PRI.ACTIVE.ACCTS',  'PRI.OVERDUE.ACCTS','PRI.CURRENT.BALANCE','PRI.SANCTIONED.AMOUNT',\n           'PRI.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT', 'SEC.NO.OF.ACCTS', 'SEC.OVERDUE.ACCTS', 'SEC.CURRENT.BALANCE',\n           'SEC.SANCTIONED.AMOUNT', 'SEC.DISBURSED.AMOUNT','SEC.ACTIVE.ACCTS', 'SEC.INSTAL.AMT'], axis = 1, inplace = True)\n","e1c4eccc":"data['TOTAL.CURRENT.BALANCE'][data['TOTAL.CURRENT.BALANCE']<0]=0\ndata['TOTAL.SANCTIONED.AMOUNT'][data['TOTAL.SANCTIONED.AMOUNT']<0]=0","156596d1":"print(data.shape)\nprint(data.columns)","b4fdcaaf":"data.drop(['Date.of.Birth','Employee_code_ID','UniqueID','supplier_id', 'Year_of_birth', 'Current_pincode_ID', 'MobileNo_Avl_Flag', 'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH','AVERAGE.ACCT.AGE_year', 'AVERAGE.ACCT.AGE_month','CREDIT.HISTORY.LENGTH_year',\n           'CREDIT.HISTORY.LENGTH_month', 'DisbursalDate' ],axis = 1, inplace = True) ","a5d79748":"#data.drop(['Date.of.Birth','disbursed_amount','asset_cost', 'branch_id','Employee_code_ID','State_ID', 'UniqueID', 'Year_of_birth', 'Age', 'Current_pincode_ID', 'supplier_id', 'MobileNo_Avl_Flag', 'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH','AVERAGE.ACCT.AGE_year', 'AVERAGE.ACCT.AGE_month','CREDIT.HISTORY.LENGTH_year','CREDIT.HISTORY.LENGTH_month', 'DisbursalDate','PERFORM_CNS.SCORE' ]\n                 #, axis = 1, inplace = True)\nprint(data.shape)\nprint(data.columns)","4b67ffa5":"data['branch_id'] = data['branch_id'].astype('category')#82 unique\ndata['State_ID'] = data['State_ID'].astype('category')#22 unique\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata['branch_id'] = le.fit_transform(data['branch_id'])\ndata['State_ID'] = le.fit_transform(data['State_ID'])","87ae3a28":"#display_all(data)","40807ccd":"data['TOTAL.ACTIVE.ACCTS'] = np.log1p(data['TOTAL.ACTIVE.ACCTS'])\ndata['TOTAL.CURRENT.BALANCE'] = np.log1p(data['TOTAL.CURRENT.BALANCE'])\ndata['TOTAL.DISBURSED.AMOUNT']= np.log1p(data['TOTAL.DISBURSED.AMOUNT'])\ndata['TOTAL.INSTAL.AMT']= np.log1p(data['TOTAL.INSTAL.AMT'])\ndata['TOTAL.NO.OF.ACCTS']= np.log1p(data['TOTAL.NO.OF.ACCTS'])\ndata['TOTAL.OVERDUE.ACCTS']= np.log1p(data['TOTAL.OVERDUE.ACCTS'])\ndata['TOTAL.SANCTIONED.AMOUNT']= np.log1p(data['TOTAL.SANCTIONED.AMOUNT'])","3a095897":"\n\"\"\"ltv =data['ltv'].median()\ncred  = data['CREDIT.HISTORY.LENGTH_period'].median()\naccage = data['AVERAGE.ACCT.AGE_period'].median()\nnewacc = data['NEW.ACCTS.IN.LAST.SIX.MONTHS'].median()\nasset=data['asset_cost'].median()\ndisbamt = data['disbursed_amount'].median()\nprint(ltv, cred, accage, newacc, asset, disbamt)\"\"\"","482cd5dc":"\"\"\"data['CREDIT.HISTORY.LENGTH_period']= np.log1p(data['CREDIT.HISTORY.LENGTH_period'])\ndata['AVERAGE.ACCT.AGE_period']= np.log1p(data['AVERAGE.ACCT.AGE_period'])\ndata['NEW.ACCTS.IN.LAST.SIX.MONTHS']= np.log1p(data['NEW.ACCTS.IN.LAST.SIX.MONTHS'])\ndata['asset_cost'] = np.log1p(data['asset_cost'])\ndata['disbursed_amount']=np.log1p(data['disbursed_amount'])\ndata['ltv']=np.log1p(data['ltv'])\"\"\"","e1723cf2":"#data['TOTAL.CURRENT.BALANCE'].fillna(0, inplace = True)\n#data['TOTAL.SANCTIONED.AMOUNT'].fillna(0, inplace = True)\ndata.isnull().sum()","80b278c3":"#split into original test and train\ndata['loan_default'].fillna(2, inplace = True)\ntest_df = data[data['loan_default'] == 2]\ntrain_df = data[data['loan_default'] != 2]\ntest_df.drop(['loan_default'], axis = 1, inplace = True)\nprint(\"train shape\" + str(train_df.shape))\nprint(\"test shape\"+str(test_df.shape))","bfb24699":"#train_df['loan_default'][train_df['within.age.limit']==0] = 1#not eligible for loan\n#train_df['loan_default'][train_df['Employment.Type']==0] = 1\n# 1 = loan defaulty and 0 non loan defaultys\ny_train = train_df['loan_default']\ntrain_df.drop(['loan_default'], axis = 1, inplace = True)\ny_train.value_counts()","db563e18":"!pip install -U imbalanced-learn","33507174":"from imblearn.over_sampling import SMOTE","edac666f":"from imblearn.over_sampling import SMOTE\n\nx_resample, y_resample = SMOTE().fit_sample(train_df, y_train.values.ravel()) \n\n# checking the shape of x_resample and y_resample\nprint(\"Shape of x:\", x_resample.shape)\nprint(\"Shape of y:\", y_resample.shape)\n#print(x_resample.columns)","7ee8699c":"# train and valid sets from train\n\"\"\"from sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(train_df, y_train, test_size = 0.2, random_state = 0)\n\n# checking the shapes\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_valid.shape)\nprint(y_valid.shape)\"\"\"","19d62fc0":"from sklearn import linear_model #for logistic regression\nfrom sklearn.neural_network import MLPClassifier #for neural network\nfrom sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, cross_val_score, cross_val_predict, validation_curve \n#GridSearchCV is used to optimize parameters of the models used\n#the other modules and functions \nfrom sklearn.ensemble import VotingClassifier #for creating ensembles of classifiers\n","3ccf7648":"# standardization\n\n\"\"\"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_valid = sc.transform(x_valid)\ntest_df = sc.transform(test_df)\"\"\"","f6fcf2bd":"# RANDOM FOREST CLASSIFIER\n\"\"\"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nmodel_rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=0.5, n_jobs=-1)\nkfold = StratifiedKFold(n_splits=5, random_state=7)\ncv_results = cross_val_score(model_rf, train_df, y_train, cv=kfold)\nprint (cv_results.mean()*100, \"%\")\nmodel_rf.fit(x_train, y_train)\n\ny_pred = model_rf.predict(x_valid)\n\nprint(\"Training Accuracy: \", model_rf.score(x_train, y_train))\nprint('Testing Accuarcy: ', model_rf.score(x_valid, y_valid))\"\"\"","eb07b560":"\"\"\"from sklearn.model_selection import GridSearchCV\nm = RandomForestClassifier()\nparameters = [{'n_estimators': [100], \n               'min_samples_leaf': [3,5,7, 8, 9], \n               'max_features': ['sqrt', 'log2', 0.5]}\n             ]\ngrid_search = GridSearchCV(estimator = m,\n                           param_grid = parameters,\n                           scoring = 'roc_auc',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(x_train, y_)\nbest_accuracy = grid_search.best_score_#given by mean of 10 folds\nbest_parameters = grid_search.best_params_\nprint (\"best accuracy is {}\".format(best_accuracy))\nprint (\"best parametrs are {}\".format(best_parameters))\"\"\"","bdddeca3":"\"\"\"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nmodel_ada = AdaBoostClassifier(n_estimators=100, learning_rate=0.001)\nmodel_ada.fit(x_train, y_train)\n\ny_pred = model_ada.predict(x_valid)\n\nprint(\"Training Accuracy: \", model_ada.score(x_train, y_train))\nprint('Testing Accuarcy: ', model_ada.score(x_valid, y_valid))\"\"\"","cc792b7d":"x = pd.DataFrame(x_resample, columns=train_df.columns)\ny_resample","182c267c":"x.shape","5c0f6f81":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n#from sklearn.model_selection import cross_validation, metrics   #Additional scklearn functions\nfrom sklearn.model_selection import GridSearchCV   #Perforing grid search\n\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4","ed3d9fde":"from xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nmodel_xgb = XGBClassifier(n_estimators=120, learning_rate=1, n_jobs=-1,random_state=42)\n#kfold = KFold(n_splits=5,random_state=7)\nkfold = StratifiedKFold(n_splits=5, random_state=7)\ncv_results = cross_val_score(model_xgb, x, y_resample, cv=kfold, scoring='roc_auc')\nprint (cv_results.mean()*100, \"%\")\n\"\"\"model_xgb.fit(train_df, y_train)\n\ny_pred = model_xgb.predict(x_valid)\n\nprint(\"Training Accuracy: \", model_xgb.score(x_train, y_train))\nprint('Testing Accuarcy: ', model_xgb.score(x_valid, y_valid))\"\"\"\n# age score = 79.93900708025433 %\n#age, Employmenttype, kfold score =80.%\n#age, Employementtype, stratified fold = 80.62\n#smote and droping min no. features(35) 88.65\n# after log transformation of 7 features(Total group) 88.7837","e272f898":"model_xgb.fit( x, y_resample)","e7092560":"import pandas as pd\nfeature_importances = pd.DataFrame(model_xgb.feature_importances_,\n                                   index = x.columns,\n                                    columns=['importance']).sort_values('importance', \n                                                                        ascending=False)\nfeature_importances.head","1dd0ccfd":"\"\"\"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nmodel_lgb = LGBMClassifier()\nkfold = KFold(n_splits=5,random_state=7)\ncv_results = cross_val_score(model_lgb, x_norm, y_train, cv=kfold)\nprint (cv_results.mean()*100, \"%\")\n#model_lgb.fit(train_df, y_train)\n\n#y_pred = model_lgb.predict(x_valid)\n\n#print(\"Training Accuracy: \", model_lgb.score(x_train, y_train))\n#print('Testing Accuarcy: ', model_lgb.score(x_valid, y_valid))","4ce00f26":"\"\"\"x_mean = train_df.mean()\nx_std = train_df.std()\nx_norm = (train_df - x_mean)\/x_std\nprint (x_norm.shape)\"\"\"","1a7d277d":"\"\"\"test_mean = test_df.mean()\ntest_std = test_df.std()\ntest_norm = (test_df - test_mean)\/test_std\nprint (test_norm.shape)\"\"\"","a9a32be1":"\"\"\"logreg = linear_model.LogisticRegression()\nkfold = KFold(n_splits=5,random_state=7)\ncv_results = cross_val_score(logreg, x_norm, y_train, cv=kfold)\nprint (cv_results.mean()*100, \"%\")\"\"\"","cef0e6d9":"\"\"\"logreg = linear_model.LogisticRegression()\nparam_grid = {\"C\":[0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}\ngrid = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=kfold)\ngrid.fit(x_norm,y)\nprint (grid.best_estimator_)\nprint (grid.best_score_*100, \"%\")\"\"\"","e9e8e22f":"\"\"\"clf = MLPClassifier(solver='sgd',learning_rate='adaptive', random_state=1, activation='relu', hidden_layer_sizes=(15,),\n                n_iter_no_change = 20, early_stopping=True)\nkfold = KFold(n_splits=5,random_state=7)\ncv_results = cross_val_score(clf, x_norm, y_train, cv=kfold)\nprint (cv_results.mean()*100, \"%\")\"\"\"","e5b6bd42":"#clf.fit(x_norm, y_train)","981f8e5c":"#y_pred_rf = model_rf.predict(x_test)\n#y_pred_ada = model_ada.predict(x_test)\ny_pred_xgb = model_xgb.predict(test_df)\n#y_pred_lgb = model_lgb.predict(test_df)\n#y_pred_mpl = clf.predict(test_norm)","71f5ad7e":"sub['loan_default']= y_pred_xgb\nsub.to_csv(\"submission_xgb6.csv\",index=False)\nsub['loan_default'].value_counts()","b2935c71":"\"\"\"clf1 = linear_model.LogisticRegression()\nclf2 = MLPClassifier(solver='lbfgs', alpha=1.0,hidden_layer_sizes=(15,), random_state=1, activation='logistic')\neclf = VotingClassifier(estimators=[('lr', clf1), ('nn', clf2)], voting='soft', weights=[1,2])\ncv_results = cross_val_score(eclf, x_norm, y_train, cv=kfold)\nprint (cv_results.mean()*100, \"%\")\"\"\"","eccf6b19":"skewness measure"}}