{"cell_type":{"1f03d081":"code","14ed3278":"code","e0334961":"code","084ecbc6":"code","eb6f7723":"code","330ec840":"code","408da387":"code","d9e6ce83":"code","068e103b":"code","7108a835":"code","c87a8aba":"code","4db64960":"code","f94cb26e":"code","4b24ae26":"markdown"},"source":{"1f03d081":"!pip install tensorflow_decision_forests\nimport tensorflow_decision_forests as tfdf\nimport os\nimport math","14ed3278":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\n\n\nimport gc\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import initializers\n\nfrom keras.models import Model\n","e0334961":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\nsubmission = submission.set_index('id')\n","084ecbc6":"# targets for the NN with Onehot encoding and target with labelencoding for Gradient\ntargets = pd.get_dummies(train['target'])\ndic = {'Class_1':0,'Class_2':1,'Class_3':2,'Class_4':3,'Class_5':4,'Class_6':5,'Class_7':6,'Class_8':7,'Class_9':8}\ntarget = train[\"target\"].map(dic)\ntrain[\"target\"] = target","eb6f7723":"def custom_metric(y_true, y_pred):\n    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n    loss = K.mean(cce(y_true, y_pred))\n    return loss\n\ncce = tf.keras.losses.CategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_custom_metric', min_delta=1e-05, patience=6, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_custom_metric', factor=0.7, patience=2, verbose=0,\n    mode='min')","330ec840":"y_valids = []\n\noof_NN_a = np.zeros((train.shape[0],9))\npred_NN_a = np.zeros((test.shape[0],9))\n\noof_NN_g = np.zeros((train.shape[0],9))\npred_NN_g = np.zeros((test.shape[0],9))\n\nNN_g_train_preds = []\nNN_g_test_preds = []\n\nNN_a_train_preds = []\nNN_a_test_preds = []\n\nN_FOLDS = 3\nSEED = 41\nEPOCH = 60\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state= SEED)\n\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(train,target)):\n    print(f\"\\n--------TRAINING FOLD {fold} ---------\\n\")\n\n    X_train = train.iloc[:,1:-1].iloc[tr_idx]\n    y_train = targets.iloc[tr_idx]\n    y_train1 = target.iloc[tr_idx]\n    X_test = train.iloc[:,1:-1].iloc[ts_idx]\n    y_test = targets.iloc[ts_idx]\n    y_test1 = target.iloc[ts_idx]\n\n    K.clear_session()\n\n    conv_inputs = layers.Input(shape = (75))\n    embed = layers.Embedding (input_dim = 354, \n                  output_dim = 7,\n                  embeddings_regularizer='l2')(conv_inputs)\n    embed = layers.Conv1D(12,1,activation = 'relu')(embed)        \n    embed = layers.Flatten()(embed)\n    hidden = layers.Dropout(0.3)(embed)\n\n    hidden = tfa.layers.WeightNormalization(\n        layers.Dense(\n        units=32,\n        activation ='selu',\n        kernel_initializer = \"lecun_normal\"))(hidden)\n\n    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))\n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n        units = 32,\n        activation='relu',\n        kernel_initializer = \"lecun_normal\"))(output) \n    output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))\n    output1 = tfa.layers.WeightNormalization(\n    layers.Dense(\n        units = 32, \n        activation = 'relu',\n        kernel_initializer = \"lecun_normal\"))(output)\n\n    conv_outputs = layers.Dense(\n        units = 9, \n        activation ='softmax',\n        kernel_initializer =\"lecun_normal\")(output1)\n\n    model_conv = Model(conv_inputs,conv_outputs)\n\n    #================= NN CONV MODEL training ========\n    \n    print(\"\\nTraining of NN model starting\\n\")\n    model_conv.compile(loss='categorical_crossentropy', \n    optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n    metrics=custom_metric)\n    model_conv.fit(X_train, y_train,\n    batch_size = 256, epochs = EPOCH,\n    validation_data=(X_test, y_test),\n    callbacks=[es, plateau],\n    verbose = 0)\n\n    #============== Convolution Model prediction =======\n\n    pred_a = model_conv.predict(X_test)\n    oof_NN_a[ts_idx] += pred_a \n    score_NN_a = log_loss(y_test, pred_a)\n    pred_NN_a += model_conv.predict(test.iloc[:,1:]) \/ N_FOLDS\n    \n    #=============== Gradient input preparation  ========\n\n    # We take the output1 layer of the NN to feed the Gradient Boosting\n    nn_model_without_head = tf.keras.models.Model(inputs=model_conv.inputs,\n                            outputs=output1)\n    param = {\n    'preprocessing': nn_model_without_head,\n    'use_hessian_gain':True,\n    'selective_gradient_boosting_ratio':0.2,\n    'categorical_algorithm':'RANDOM', \n    'num_trees': 300,\n    'selective_gradient_boosting_ratio':0.2,\n    'subsample': 0.7870499728626467,\n    'shrinkage': 0.018653897565237845,\n    'max_depth' : 3,\n    'min_examples' : 11,\n    'l1_regularization': 3.5480988121992953,\n    'l2_categorical_regularization': 0.11074398839677566\n    }\n\n    model_Gradient_with_NN = tfdf.keras.GradientBoostedTreesModel(**param)\n\n    #==================== Gradient training  =============\n\n    print(\"\\nTraining of GBT with NN model starting\\n\")\n    metrics = [tf.keras.metrics.CategoricalCrossentropy()]\n    model_Gradient_with_NN.compile(metrics=metrics)\n    model_Gradient_with_NN.fit(np.array(X_train),np.array(y_train1))\n\n    #================= GBT Model prediction ==============\n\n    pred_g = model_Gradient_with_NN.predict(X_test)\n    oof_NN_g[ts_idx] += pred_g\n    score_GBT_NN = log_loss(y_test1, pred_g)\n    print(f\"\\nFOLD {fold} Score of NN Model alone: {score_NN_a}\")\n    print(f\"\\nFOLD {fold} Score of GBT after NN : {score_GBT_NN}\\n\")\n    pred_NN_g += model_Gradient_with_NN.predict(test.iloc[:,1:]) \/ N_FOLDS\n\n    # =========PREPROCESSING FOR FUTURE OPTIMIZATION======\n\n    y_valid = target.iloc[ts_idx]\n    y_valids.append(y_valid)\n\n    NN_a_train_preds.append(pred_a)\n    NN_a_test_preds.append(model_conv.predict(test.iloc[:,1:]))\n\n    NN_g_train_preds.append(pred_g)\n    NN_g_test_preds.append(model_Gradient_with_NN.predict(test.iloc[:,1:]))\n\nprint (\"\\n***************************************************\")\nscore_a = log_loss(targets, oof_NN_a)\nprint(f\"\\n FINAL SCORE NN MODEL ALONE : {score_a}\")\nscore_NN_g = log_loss(target, oof_NN_g)\nprint(f\"\\n FINAL SCORE GBT MODEL AFTER NN: {score_NN_g}\")\nprint (\"\\n***************************************************\")","408da387":"from scipy.optimize import minimize\nscores = []\nweights = []\nfor y, NN_a_pred,NN_g_pred in zip( y_valids,  \n                                   NN_a_train_preds,\n                                   NN_g_train_preds                \n                                    ):\n    preds = []\n    preds.append(NN_a_pred)\n    preds.append(NN_g_pred)\n    \n    def log_weight_loss(weights):\n        weighted_pred = ((weights[0]*preds[0]) + (weights[1]*preds[1]))\n        return log_loss(y, weighted_pred)\n    starting_values = [10]*len(preds) \n    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n    #bounds = [(0,1)]*len(preds) \n    res = minimize(log_weight_loss, \n                   starting_values, \n                   method='Nelder-Mead', \n                   #bounds=bounds, \n                   constraints=cons)\n    \n    weights.append(res['x'])\n    scores.append(res['fun'])","d9e6ce83":"results = pd.DataFrame(weights, columns =['Model NN','Model GBT'])\nresults['better model'] = results.idxmax(axis=1, skipna=True)\nresults ['max_value'] = results.max(axis=1)\nresults['scores'] = scores\ndisplay(results)","068e103b":"results.groupby(['better model'])['max_value'].count()","7108a835":"folds = N_FOLDS\nfinal_weights = sum(weights)\/(folds)\nweighted_preds = np.array((final_weights[0] * sum(np.array(NN_a_test_preds)\/(folds)))\n                           +(final_weights[1] * sum(np.array(NN_g_test_preds)\/(folds))))","c87a8aba":"submission[['Class_1', 'Class_2', 'Class_3', 'Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = weighted_preds\nsubmission.to_csv('weighted5.csv')","4db64960":"submission[['Class_1', 'Class_2', 'Class_3', 'Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = (pred_NN_g + pred_NN_a)\/2\nsubmission.to_csv('blend3.csv')","f94cb26e":"submission[['Class_1', 'Class_2', 'Class_3', 'Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = pred_NN_g\nsubmission.to_csv('gradient1.csv')","4b24ae26":"![image.png](attachment:95d8fcaf-28fc-40c7-8f5d-71b70ce124fa.png)\n\n<h3>Neural Network and Gradient boosting compete and we take the best with optimization or blending"}}