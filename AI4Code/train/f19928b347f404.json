{"cell_type":{"dfd993e1":"code","2034b8b3":"code","b52d5c1e":"code","89c1196e":"code","e8e88987":"code","482ec423":"code","459afd4c":"code","1c39fcdb":"code","6c6d3aaa":"code","ff6b87c3":"markdown","8d6626c1":"markdown","de72b288":"markdown","2618c162":"markdown","c9a51702":"markdown","86013099":"markdown","84057d47":"markdown","0d5cec2e":"markdown","b717b76d":"markdown","bf8ad551":"markdown","d334180f":"markdown","b814771b":"markdown","020d4a45":"markdown"},"source":{"dfd993e1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Set random seed to make results reproducable\nnp.random.seed(42)\nplt.style.use('seaborn')","2034b8b3":"df = pd.read_csv('..\/input\/clinvar_conflicting.csv',dtype={0: object, 38: str, 40: object})\ndf.fillna(0,inplace=True)\ndf.head()","b52d5c1e":"# Features histograms\ndf.drop('CLASS',axis=1).hist(figsize=(12,7))\nplt.suptitle(\"Features histograms\", fontsize=16)\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.show()\nsns.countplot(x='CLASS',data=df)\nplt.title(\"Target label histogram\")\nplt.show()","89c1196e":"import random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, LogisticRegressionCV, RidgeClassifier, RidgeClassifierCV, PassiveAggressiveClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\n#from sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Set random seed to make results reproducable\nnp.random.seed(42)\n","e8e88987":"# Balance\ng = df.groupby('CLASS')\ndf_balanced = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n# Extract smaller sample to avoid memory error later, when training starts\ndf_balanced = df_balanced.sample(1000)\n\n# Illustrate balancing results on plots\nf, ax = plt.subplots(1,2)\n# Before balancing plot\ndf.CLASS.value_counts().plot(kind='bar', ax=ax[0])\nax[0].set_title(\"Before\")\nax[0].set_xlabel(\"CLASS value\")\nax[0].set_ylabel(\"Count\")\n# After balanced plot\ndf_balanced.CLASS.value_counts().plot(kind='bar',ax=ax[1])\nax[1].set_title(\"After\")\nax[1].set_xlabel(\"CLASS value\")\nax[1].set_ylabel(\"Count\")\n\nplt.suptitle(\"Balancing data by CLASS column value\")\nplt.tight_layout()\nplt.subplots_adjust(top=0.8)\nplt.show()","482ec423":"# Features - all columns except 'CLASS'\n# Target label = 'CLASS' column\nX=df_balanced.drop('CLASS',axis=1)\n# One hot encoding\nX=pd.get_dummies(X, drop_first=True)\ny=df_balanced['CLASS']\ny=pd.get_dummies(y, drop_first=True)\n\n# Train\/test split\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\n\n# Normalize using StandardScaler\nscaler=StandardScaler()\ntrain_X=scaler.fit_transform(train_X)\ntest_X=scaler.transform(test_X)\n\n# Histogram of target labels distribution\ntest_y.hist()\nplt.title(\"Target feature distribution: CLASS values\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Count\")\nplt.show()","459afd4c":"# Models to try\nmodels = [LogisticRegression(), \n          LogisticRegressionCV(), \n          PassiveAggressiveClassifier(),\n          RidgeClassifier(),\n          RidgeClassifierCV(),\n          KNeighborsClassifier(),\n          #RadiusNeighborsClassifier(),\n          NearestCentroid(),\n          DecisionTreeClassifier(), \n          AdaBoostClassifier(), \n          BaggingClassifier(),\n          ExtraTreesClassifier(),\n          GradientBoostingClassifier(),\n          RandomForestClassifier(), \n          SGDClassifier(),\n          GaussianNB(),\n          GaussianProcessClassifier(),\n          LinearDiscriminantAnalysis(),\n          QuadraticDiscriminantAnalysis(),\n          MLPClassifier(),\n          SVC()\n         ]\n# Gather metrics here\naccuracy_by_model={}\n\n# Train then evaluate each model\nfor model in models:\n    model.fit(train_X, train_y)\n    pred_y = model.predict(test_X)\n    score = accuracy_score(test_y, pred_y)\n    # Fill metrics dictionary\n    model_name = model.__class__.__name__\n    accuracy_by_model[model_name]=score  \n","1c39fcdb":"# Draw accuracy by model chart\nacc_df = pd.DataFrame(list(accuracy_by_model.items()), columns=['Model', 'Accuracy']).sort_values('Accuracy', ascending=False).reset_index(drop=True)\nacc_df.index=acc_df.index+1\nsns.barplot(data=acc_df,y='Model',x='Accuracy')\nplt.xlim(0,1)\nplt.title('Accuracy of models with default settings')\nplt.xticks(rotation=45)\nplt.show()\n\n# Print table\nacc_df","6c6d3aaa":"best_model = acc_df[acc_df.Accuracy==acc_df.Accuracy.max()]\nbest_model","ff6b87c3":"<h1>6. Calculating results<\/h1>\n<h2>6.1 Compare gathered metrics<\/h2>\nAs we can see below, accuracy is far from 1.0 )) maybe more input data preparation could improve the metrics. But our models did their best with the data we prepared for them. Have a look at the results.","8d6626c1":"<h1>4. Simple EDA<\/h1>","de72b288":"<h1>1. What is the purpose of this kernel?<\/h1>","2618c162":"<h1>3. Read the data<\/h1>","c9a51702":"<h3>5.2.2 Train\/test split, encode, scale input data<\/h3>\nSome models requre one hot encoding, some are sensible to data normalization. Do encode and normalize data to make each model happy. Then train\/test split with default ratio.\n","86013099":"<h2>5.2 Prepare the data<\/h2>\n\n<h3>5.2.1  Balancing by target feature<\/h3>\n\nTarget CLASS column has skewed distribution. Let's balance it first, then extract a sample from balanced data. Using full dataset produces memory error here.","84057d47":"<h2>5.1 Import ML libraries<\/h2>","0d5cec2e":"<h2>5.3 Run models<\/h2>\nCreate models, train and evaluate them. Store  accuracy by model in dictionary. Do not adjust models, default settings used.","b717b76d":"<h1>5. Classification with different models<\/h1>\nBalance, scale and split the data to train and test. Then create the list of models, train each and get it's metrics - **accuracy** in our case.","bf8ad551":"Lightweight EDA  just to get basic understanding what the data is. We can see that target column named CLASS is skewed, so will need to rebalance the data before classification.","d334180f":"<h1>2. Import common libraries<\/h1>","b814771b":"<h2>6.2. Choose the hero of the Kernel<\/h2>\n\nLadies and gentlemen, we have a winner! The winnerrrr iisssss...","020d4a45":"> The goal is to **compare 20 different models out-of-the-box**, without special tuning. Draw performance comparison chart at the end and choose the winner model ))"}}