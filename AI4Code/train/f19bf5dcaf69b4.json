{"cell_type":{"c866ffa7":"code","990e51d5":"code","8fd82a9b":"code","5607ed70":"code","7c0f9e07":"code","66f99466":"code","cbcbb2b8":"code","8a860432":"code","bdaf797f":"code","2e943bf9":"code","5512586f":"code","2487b505":"code","6cfc4b3f":"code","cd2db9b4":"code","5a500650":"code","fa929d8f":"code","37495914":"code","bfe0f869":"code","1633a34c":"code","060923da":"code","602e7adf":"code","7a3cfb00":"code","c4695d62":"code","abaf9096":"code","d2754589":"code","605daa24":"code","4ccebe1d":"code","431439e8":"code","66926ac6":"code","339cd139":"code","ff0ca48b":"code","d7c509bc":"code","4c18b6c7":"code","249c55b0":"code","32837d24":"code","78f8539a":"code","cdfea062":"code","623f9606":"code","10bbf12b":"code","dbaba1da":"code","202ccec2":"code","1ec4a3aa":"code","9cebf38a":"code","3bfc7df5":"code","fad3009c":"code","4eeeb5e4":"code","68e1ec77":"code","da248791":"code","43364ebc":"code","0851f5ac":"code","38ff4140":"code","4acec2cd":"code","58060a71":"code","ad24aa9a":"code","21e8d508":"code","87bf5b7e":"code","c76983f9":"code","f2dedfe4":"code","855fcf07":"code","588017f8":"code","de6ca766":"code","a0806458":"code","589c1cb8":"code","57f7ae94":"code","6d16539c":"code","a4bd57e4":"code","791fd3e4":"code","7b663d01":"code","d394e06a":"code","3911d203":"code","5ae63873":"code","c309e3c0":"code","ebc0f779":"code","4d4d8a43":"code","433abd4a":"code","16b4c432":"code","a340d38c":"code","99e1e62d":"code","7060f63f":"code","77519ac8":"code","e878ba67":"code","e4b5359a":"code","7f491dfc":"code","c16c2834":"code","b789820c":"code","7d1071ac":"code","b6692620":"code","36226339":"markdown","657b7ec2":"markdown","6b9bf35a":"markdown","f4154a17":"markdown","a46e46b4":"markdown","968a86d5":"markdown","c016365a":"markdown","627dc94a":"markdown","eb9dfaa3":"markdown","8b82b013":"markdown","665ea641":"markdown","e4e4f405":"markdown"},"source":{"c866ffa7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","990e51d5":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import f1_score\nimport sklearn.base as skb\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport sklearn.linear_model as sklm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport random\nseed = 5\nnp.random.seed(seed)\n\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.monospace'] = 'Ubunto Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12,8)","8fd82a9b":"df = pd.read_csv('..\/input\/hackerearth-how-not-to-lose-a-customer-in-10-days\/train.csv')\ndf","5607ed70":"df_test = pd.read_csv('..\/input\/hackerearth-how-not-to-lose-a-customer-in-10-days\/test.csv')\ndf_test","7c0f9e07":"# Setting the target feature\ntargetFeature = 'churn_risk_score'","66f99466":"df.nunique()","cbcbb2b8":"# Spliting the dataset into numerical and categorical features\n\ndef datasetShape(df):\n    rows,cols = df.shape\n    print(\"The dataframe has \"+ str(rows)+\" No. of rows and \"+str(cols)+\" of columns\")\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","8a860432":"print(df.shape)\ndf.drop_duplicates(inplace=True)\nprint(df.shape)\nnum_feature,cat_feature = divideFeatures(df)\nnum_feature","bdaf797f":"cat_feature","2e943bf9":"df.drop(['customer_id','Name','security_no','referral_id'],axis=1,inplace=True)\ndf_test.drop(['customer_id','Name','security_no','referral_id'],axis=1,inplace=True)\n","5512586f":"num_feature,cat_feature = divideFeatures(df)\nnum_feature","2487b505":"cat_feature.nunique()","6cfc4b3f":"cat_feature['joining_date']","cd2db9b4":"cat_feature['last_visit_time']","5a500650":"cat_feature.info()","fa929d8f":"cat_feature['avg_frequency_login_days'].replace(['Error'],np.nan,inplace=True)","37495914":"arr = cat_feature['avg_frequency_login_days'].unique()","bfe0f869":"cat_feature['avg_frequency_login_days'].astype('float64').plot(kind='box')","1633a34c":"fig = plt.figure(figsize=(16,16))\nfor i in range(len(num_feature.columns)):\n    fig.add_subplot(3,4,i+1)\n    sns.boxplot(y=num_feature.iloc[:,i])\nplt.tight_layout()\nplt.show()","060923da":"df['avg_frequency_login_days'].replace(['Error'],np.nan,inplace=True)\ndf_test['avg_frequency_login_days'].replace(['Error'],np.nan,inplace=True)","602e7adf":"df['avg_frequency_login_days'] = df['avg_frequency_login_days'].astype('float64')\ndf_test['avg_frequency_login_days'] = df_test['avg_frequency_login_days'].astype('float64')","7a3cfb00":"num_feature,cat_feature = divideFeatures(df)\nnum_feature","c4695d62":"fig = plt.figure(figsize=(16,16))\nfor i in range(len(num_feature.columns)):\n    fig.add_subplot(3,3,i+1)\n    sns.boxplot(y=num_feature.iloc[:,i])\nplt.tight_layout()\nplt.show()","abaf9096":"sns.pairplot(df)\nplt.show()","d2754589":"corr = df.corr()\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","605daa24":"skewed_features = num_feature.apply(lambda x:x.skew()).sort_values(ascending=False)\nskewed_features","4ccebe1d":"df.isnull().sum()","431439e8":"df.region_category.value_counts()","66926ac6":"sns.set_theme(style=\"white\", context=\"talk\")\nf,(ax1,ax2,ax3) = plt.subplots(3,1,figsize=(15,15))\nsns.barplot(x = 'gender',y =targetFeature,palette=\"rocket\",ax=ax1,data=df)\nsns.barplot(x = 'region_category',y =targetFeature,palette=\"vlag\",ax=ax2,data=df)\nsns.barplot(x = 'membership_category',y =targetFeature,palette=\"deep\",ax=ax3,data=df)","339cd139":"fig,ax = plt.subplots(1,1,figsize=(20,7))\nsns.stripplot(y ='region_category', x =targetFeature, data = df, \n              jitter = True, hue ='gender', dodge = True,ax=ax)","ff0ca48b":"df.info()","d7c509bc":"df['avg_time_spent'].plot(kind='box')","4c18b6c7":"# Dropping all rows whose 'avg_time_spent' is negative \nlow = df['avg_time_spent'] < 0\nlow = np.where(low)\ndf.drop(low[0],inplace=True)","249c55b0":"df.isnull().sum()","32837d24":"# Reseting the default index\ndf.index = range(0,df.shape[0])","78f8539a":"# Dropping all rows whose 'points_in_wallet' is negative \nl1 = np.where(df['points_in_wallet'] < 0)\ndf.drop(l1[0],inplace=True)\ndf.index = range(0,df.shape[0])","cdfea062":"# Dropping all rows whose 'avg_frequency_login_days' is negative \nl2 = np.where(df['avg_frequency_login_days'] < 0)\ndf.drop(l2[0],inplace=True)\ndf.index = range(0,df.shape[0])","623f9606":"# Dropping all rows whose 'avg_transaction_value' is negative \nl3 = np.where(df['avg_transaction_value'] < 0)\ndf.drop(l3[0],inplace=True)\ndf.index = range(0,df.shape[0])","10bbf12b":"# Dropping all rows whose 'days_since_last_login' is negative \nl4 = np.where(df['days_since_last_login'] < 0)\ndf.drop(l4[0],inplace=True)\ndf.index = range(0,df.shape[0])","dbaba1da":"df.dropna(how='any',inplace=True)","202ccec2":"df_test.info()","1ec4a3aa":"df['joining_date'] = pd.to_datetime(df['joining_date'])\ndf['last_visit_time'] = pd.to_datetime(df['last_visit_time'])\n\ndf_test['joining_date'] = pd.to_datetime(df_test['joining_date'])\ndf_test['last_visit_time'] = pd.to_datetime(df_test['last_visit_time'])","9cebf38a":"df['jday'] = df['joining_date'].dt.day\ndf_test['jday'] = df_test['joining_date'].dt.day","3bfc7df5":"df['jday'] = df['joining_date'].dt.day\ndf['jmonth'] = df['joining_date'].dt.month\ndf['jweek'] = df['joining_date'].dt.dayofweek\n\ndf_test['jday'] = df_test['joining_date'].dt.day\ndf_test['jmonth'] = df_test['joining_date'].dt.month\ndf_test['jweek'] = df_test['joining_date'].dt.dayofweek","fad3009c":"df['diff'] =  ((df['last_visit_time'] - df['joining_date']).apply(lambda x:str(x).split()[0])).astype('float64')\ndf_test['diff'] =  ((df_test['last_visit_time'] - df_test['joining_date']).apply(lambda x:str(x).split()[0])).astype('float64')","4eeeb5e4":"df.index = range(0,df.shape[0])\n\n# Removing all the rows having 'churn_risk_score' == -1 \nl5 = np.where(df['churn_risk_score'] == -1)\ndf.drop(l5[0],inplace=True)\ndf.index = range(0,df.shape[0])","68e1ec77":"df","da248791":"# Clearly target labels are imbalanced. So we'll use Over sampling techinques\ndf['churn_risk_score'].value_counts()\n","43364ebc":"df_test.info()","0851f5ac":"df.info()","38ff4140":"# Selecting categorical features into a new dataframe\nsm = ['gender','region_category','membership_category','joined_through_referral',\n      'preferred_offer_types','medium_of_operation','internet_option','used_special_discount',\n       'offer_application_preference','past_complaint','complaint_status','feedback']\ndf_sm = df[sm]\ndf_test_sm = df_test[sm]","4acec2cd":"# Creating dummies for categorical feature columns\ndf_sm = pd.get_dummies(df_sm)\ndf_test_sm = pd.get_dummies(df_test_sm)","58060a71":"# Selecting Numerical features into a new dataframe\n\nss = ['days_since_last_login','avg_time_spent','avg_transaction_value','avg_frequency_login_days',\n'points_in_wallet','used_special_discount','churn_risk_score','jday','jmonth','jweek','diff']\ndf_ss = df[ss]\nss_t = ['days_since_last_login','avg_time_spent','avg_transaction_value','avg_frequency_login_days',\n'points_in_wallet','used_special_discount','jday','jmonth','jweek','diff']\ndf_test_ss = df_test[ss_t]","ad24aa9a":"# Concatenating the dummy columns and numerical feature columns into a new dataframe\ndf_new = pd.concat([df_ss,df_sm],axis=1)\ndf_test_new = pd.concat([df_test_ss,df_test_sm],axis=1)","21e8d508":"df_test_new.info()","87bf5b7e":"df_new.drop(['used_special_discount'],axis=1,inplace=True)\ndf_test_new.drop(['used_special_discount'],axis=1,inplace=True)","c76983f9":"df_test_new.isnull().sum()","f2dedfe4":"df_test_new['avg_frequency_login_days'].fillna(value=df_test_new['avg_frequency_login_days'].mean(),inplace=True)\ndf_test_new['points_in_wallet'].fillna(value=df_test_new['points_in_wallet'].mean(),inplace=True)","855fcf07":"df_test_new.info()","588017f8":"df_new.dropna(axis=0,how='any',inplace=True)","de6ca766":"df_new.info()","a0806458":"X = df_new.drop(['churn_risk_score'],axis=1).values\nY = df_new['churn_risk_score'].values","589c1cb8":"# Applying SMOTE Over Sampling Strategy\nfrom imblearn.over_sampling import SMOTE\noversampler = SMOTE()\nX_ov,Y_ov = oversampler.fit_resample(X,Y)\nprint('The dataset before oversampling: ',X.shape,Y.shape)\nprint('The dataset after oversampling: ',X_ov.shape,Y_ov.shape)","57f7ae94":"# Now we are having equally balanced target labels\nprint(sum(Y == 1),sum(Y_ov == 1))\nprint(sum(Y == 2),sum(Y_ov == 2))\nprint(sum(Y == 3),sum(Y_ov == 3))\nprint(sum(Y == 4),sum(Y_ov == 4))\nprint(sum(Y == 5),sum(Y_ov == 5))\n","6d16539c":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nX_ov = scaler.fit_transform(X_ov)\nX_test = df_test_new.values\nX_test = scaler.transform(X_test)","a4bd57e4":"x_train,x_test,y_train,y_test = train_test_split(X_ov,Y_ov,train_size=0.7)\nprint(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)","791fd3e4":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,f1_score\nrf = RandomForestClassifier(n_estimators=1000,max_depth=25)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","7b663d01":"fig = plt.figure(figsize=(6, 6))\nax= plt.subplot()\ncm = confusion_matrix(y_true=y_test,y_pred=y_pred)\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g')\nax.set_xlabel('Predicted label')\nax.set_ylabel('Actual label')\nplt.show()","d394e06a":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(100,100,100))\nmlp.fit(x_train,y_train)\ny_pred = mlp.predict(x_test)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","3911d203":"fig = plt.figure(figsize=(6, 6))\nax= plt.subplot()\ncm = confusion_matrix(y_true=y_test,y_pred=y_pred)\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g')\nax.set_xlabel('Predicted label')\nax.set_ylabel('Actual label')\nplt.show()","5ae63873":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(criterion='entropy',max_depth=15, min_samples_split=10, min_samples_leaf=12)\ntree.fit(x_train,y_train)\ny_pred = tree.predict(x_test)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","c309e3c0":"fig = plt.figure(figsize=(6, 6))\nax= plt.subplot()\ncm = confusion_matrix(y_true=y_test,y_pred=y_pred)\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g')\nax.set_xlabel('Predicted label')\nax.set_ylabel('Actual label')\nplt.show()","ebc0f779":"from sklearn.ensemble import GradientBoostingClassifier\ngbrt = GradientBoostingClassifier(n_estimators=100,max_depth=10,min_samples_leaf = 4, min_samples_split= 5)\ngbrt.fit(x_train,y_train)\ny_pred = gbrt.predict(x_test)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","4d4d8a43":"fig = plt.figure(figsize=(6, 6))\nax= plt.subplot()\ncm = confusion_matrix(y_true=y_test,y_pred=y_pred)\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g')\nax.set_xlabel('Predicted label')\nax.set_ylabel('Actual label')\nplt.show()","433abd4a":"from xgboost import XGBClassifier\nxgb = XGBClassifier(n_jobs=-1,n_estimators=1000,max_depth=10)\nxgb.fit(x_train,y_train)\ny_pred = xgb.predict(x_test)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","16b4c432":"fig = plt.figure(figsize=(6, 6))\nax= plt.subplot()\ncm = confusion_matrix(y_true=y_test,y_pred=y_pred)\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g')\nax.set_xlabel('Predicted label')\nax.set_ylabel('Actual label')\nplt.show()","a340d38c":"class MixModel(skb.BaseEstimator, skb.RegressorMixin, skb.TransformerMixin):\n\n    def __init__(self, algs):\n        self.algs = algs\n\n    def fit(self, X, y):\n        self.algs_ = [skb.clone(x) for x in self.algs]\n        \n       \n        for alg in self.algs_:\n            alg.fit(X, y)\n\n        return self\n    \n\n    def predict(self, X):\n        predictions = np.column_stack([\n            stacked_model.predict(X) for stacked_model in self.algs_\n        ])\n        return (np.mean(predictions, axis=1)).astype('int64')","99e1e62d":"mixed_model = MixModel(algs = [xgb,rf,gbrt])\nmixed_model.fit(x_train, y_train)\n","7060f63f":"y_pred = mixed_model.predict(x_test)\nprint(classification_report(y_true=y_test,y_pred=y_pred))\nprint(f1_score(y_true=y_test,y_pred=y_pred,average='macro'))","77519ac8":"fig = plt.figure(figsize=(6, 6))\nax= plt.subplot()\ncm = confusion_matrix(y_true=y_test,y_pred=y_pred)\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g')\nax.set_xlabel('Predicted label')\nax.set_ylabel('Actual label')\nplt.show()","e878ba67":"print(X_test.shape,X.shape)","e4b5359a":"df_sub = pd.read_csv('..\/input\/hackerearth-how-not-to-lose-a-customer-in-10-days\/sample_submission.csv')\ndf_sub","7f491dfc":"client_score = pd.DataFrame(xgb.predict(X_test),columns=['churn_risk_score'])\nclient_score","c16c2834":"df_test = pd.read_csv('..\/input\/hackerearth-how-not-to-lose-a-customer-in-10-days\/test.csv')\ndf_pr = pd.DataFrame(df_test['customer_id'],columns=['customer_id'])\ndf_pr","b789820c":"df_pr = pd.concat([df_pr,client_score],axis=1)","7d1071ac":"df_pr","b6692620":"df_pr.to_csv('.\/Churn_risk_final_submission.csv')","36226339":"# Task 5- Test Evaluation and Submission","657b7ec2":"# Mixed model of XGB ,Random Forest & Gradient Boosting Classifier","6b9bf35a":"# Task 3- Data Preparation (with Feature Engineering) ","f4154a17":"## Random Forest Classifier","a46e46b4":"# Task 1- Importing libraries and data","968a86d5":"## MLPClassifier with three hidden layers of 100 nodes each","c016365a":"# Task 4- Data Modelling","627dc94a":"## Decision Tree Classifier","eb9dfaa3":"# Last Notes\nTry more different type of models and hypertunning with current models to find better results.XGB Classifier is giving best results for me.\nAlso some better feature engineering may bring excellent results.\n\nIf you like my work, show your appreciation with an upvote and share this notebook.\n","8b82b013":"## Gradient Boosting Classifier","665ea641":"## XGBClassifier","e4e4f405":"# Task 2- Exploratory Data Analysis (EDA)"}}