{"cell_type":{"57b597d8":"code","9f7943ef":"code","72797429":"code","fe444909":"code","60ace268":"code","f90b32b1":"code","75d80b13":"code","7b4eca0e":"code","a314024a":"code","26fd7924":"code","45ee802f":"code","cac5cf3d":"code","811b03d9":"markdown","77065872":"markdown","d2c6940e":"markdown","edaec4b5":"markdown","0f77056b":"markdown"},"source":{"57b597d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9f7943ef":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.utils as ku\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport string","72797429":"data_train = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/train.zip\")\ndata_val = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/test.zip\")\n\nprint('Training data shape:',data_train.shape)\nprint('Validation data shape:',data_val.shape)\ndata_train.head()","fe444909":"StopWords = set(stopwords.words('english'))\n\ndef text_preprocess(text):\n    trans = str.maketrans('','',string.punctuation)\n    text = text.translate(trans)\n    text = ' '.join([word.lower() for word in text.split() if word.lower() not in StopWords])\n    return text\n\ndata_train['text'] = data_train['text'].apply(text_preprocess)\ndata_val['text'] = data_val['text'].apply(text_preprocess)\ndata_train.head()","60ace268":"label_encoder = LabelEncoder()\nX_train = data_train['text']\nX_train = X_train.tolist()\nX_test = data_val['text']\nX_test = X_test.tolist()\ny_train = data_train['author']\ny_train = label_encoder.fit_transform(y_train)\ny_train_cat = ku.to_categorical(y_train, num_classes=3)\nval_id = data_val['id']\n\nlemmatizer = WordNetLemmatizer()\nX_train_lemm = []\nfor text in X_train:\n    lem_text = ''\n    for word in text.split():\n        lem_word = lemmatizer.lemmatize(word, pos='v')\n        lem_word = lemmatizer.lemmatize(lem_word)\n        lem_text = lem_text + ' ' + lem_word\n    X_train_lemm.append(lem_text)\n\nX_test_lemm = []\nfor text in X_test:\n    lem_text = ''\n    for word in text.split():\n        lem_word = lemmatizer.lemmatize(word, pos='v')\n        lem_word = lemmatizer.lemmatize(lem_word)\n        lem_text = lem_text + ' ' + lem_word\n    X_test_lemm.append(lem_text)\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train_lemm)\nvocab_size = len(tokenizer.word_index)\nmax_len = 150\ntrain_seq = tokenizer.texts_to_sequences(X_train_lemm)\ntrain_pad = pad_sequences(train_seq, maxlen=max_len)\ntest_seq = tokenizer.texts_to_sequences(X_test_lemm)\ntest_pad = pad_sequences(test_seq, maxlen=max_len)\n\nlabel2idx = {\n    'EAP': 0,\n    'HPL': 1,\n    'MWS': 2\n}","f90b32b1":"tfidf = TfidfVectorizer(ngram_range=(1,3), min_df=5, max_df=0.5)\nX_train_tfidf = tfidf.fit_transform(X_train_lemm)\nX_test_tfidf = tfidf.transform(X_test_lemm)","75d80b13":"clf = LogisticRegression(max_iter=1000).fit(X_train_tfidf, y_train)\ny_pred = clf.predict(X_test_tfidf)\nprint(y_pred)\noutput_prob = clf.predict_proba(X_test_tfidf)\noutput_prob[:,0]","7b4eca0e":"model = keras.Sequential([\n    keras.layers.Embedding(vocab_size+1, 300, input_length=max_len),\n    keras.layers.SpatialDropout1D(0.5),\n    keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n    keras.layers.Bidirectional(keras.layers.LSTM(32, dropout=0.3, recurrent_dropout=0.3)),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dense(3, activation='softmax')\n])\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nhistory = model.fit(train_pad, y_train_cat, epochs=20, batch_size=512)","a314024a":"model.summary()","26fd7924":"y_pred_nn = model.predict_classes(test_pad)\nprint(y_pred_nn)\n","45ee802f":"#cosine similarity between outputs from both methods.\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity([y_pred], [y_pred_nn])","cac5cf3d":"#Submission file.\ndf = pd.DataFrame()\ndf['id'] = val_id\ndf['EAP'] = output_prob[:,0]\ndf['HPL'] = output_prob[:,1]\ndf['MWS'] = output_prob[:,2]\n\ndf.to_csv('Submission.csv', index=False)","811b03d9":"# III. Tokenization and Lemmatization","77065872":"# II. Text Preprocessing","d2c6940e":"# IV. Training using TFIDF Vectorizer","edaec4b5":"# I. Importing Libraries and Data","0f77056b":"# V. Training using Bi-LSTM NN"}}