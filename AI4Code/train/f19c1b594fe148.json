{"cell_type":{"61a6be88":"code","bfa98f18":"code","b3d73523":"code","52ba9008":"code","81cde25e":"code","a9615c88":"code","0cb697e6":"code","dd71ae50":"code","2d1f830f":"code","517d4b68":"code","f474e7ae":"code","c0fd0aa7":"code","276bad33":"code","531277cc":"code","b9c38ee2":"code","1b46183b":"code","6feb6d62":"code","f83a2e75":"code","2871010e":"code","291c1322":"code","28ac4c06":"code","b9155cae":"code","ee3135c4":"code","e15eca68":"code","27b08e13":"code","33d864ed":"code","22bb3101":"code","f4a960ab":"code","84ae7803":"code","5f5e18a6":"code","3687888a":"code","4859c934":"code","36bd638e":"code","2d0adc8c":"code","cd0a2124":"code","edce384d":"code","005d4a68":"code","47b6ce4b":"code","54eb5cd3":"code","9a9ead83":"code","3960e91f":"code","381704b0":"code","4b2df0ce":"code","6f7e0586":"code","30395e82":"code","f7783f8c":"code","fbe32feb":"code","05b181f7":"code","4884fe26":"code","bae2d817":"code","b72c93a3":"code","5e2b743c":"code","c06b8c93":"code","3a07d300":"code","90198581":"code","2d515795":"code","ad55e2eb":"code","38d3004b":"code","122cecbb":"code","5385f7a5":"code","81718fde":"code","af8fb91a":"code","da78c176":"code","e5bb8781":"code","932f99a9":"code","d7eb567a":"code","d7396e5c":"code","917e677b":"code","e2ba1e0e":"code","66edd6cf":"code","297668be":"code","b6564005":"code","e0fdefeb":"code","bb0050dd":"code","fd81d757":"code","2d6e56b0":"code","0755277a":"code","d3ea8394":"code","b5267e41":"code","82de71b8":"code","8476694d":"code","f7bee278":"code","3ec7fe2f":"code","c1fcd91b":"code","aff80a53":"code","208a8e0f":"code","dd0859e8":"code","01b20ad2":"code","6d366c80":"code","5c935f06":"code","a5f8e9fc":"code","40ceef1e":"code","632cf11b":"code","7316376e":"code","f33a1c13":"code","74d6e1c8":"code","dc29b5ac":"code","a3332a94":"code","422a4d97":"code","14b4065a":"code","5b7554ba":"code","883cabcc":"code","62a813f1":"code","12395ae3":"code","d1a516b0":"code","5da76974":"code","21836146":"code","d730ed06":"code","a5333d78":"code","d1a1f3fe":"code","592d1495":"code","6924fd82":"code","3669ca54":"code","2fda6ee0":"code","19847636":"code","14edae30":"code","cb1996e5":"code","63475be3":"code","28b9dc26":"code","adb73357":"code","61cdc5ed":"code","eaadd049":"code","f6b4a8f6":"code","45ba5404":"code","5033b6dd":"code","62f97607":"code","77849fd8":"code","3d26a5cc":"code","0c2ec91d":"code","b314e786":"code","af4362a4":"markdown","a971e384":"markdown","327ea010":"markdown","10317a78":"markdown","c8c6d6bd":"markdown","7fecb113":"markdown","775bfc3e":"markdown","bd969bba":"markdown","7aa7a830":"markdown","90f150dc":"markdown","81028367":"markdown","9af6924a":"markdown","c114c803":"markdown","4e4697d1":"markdown","5ef6fcd7":"markdown","21afccd2":"markdown","a7989d1a":"markdown","c76123f0":"markdown","31972633":"markdown"},"source":{"61a6be88":"# Basic Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport os\nimport missingno\nfrom collections import Counter\nimport wordcloud\nimport emoji\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport string\nimport nltk\nimport re","bfa98f18":"# print(os.getcwd())","b3d73523":"# os.chdir(\"G:\/\/Excel\")","52ba9008":"df = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding = 'latin-1')","81cde25e":"df.head()","a9615c88":"df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1, inplace = True)","0cb697e6":"df.head()","dd71ae50":"df.columns = [\"class\", \"message\"]","2d1f830f":"df.head()","517d4b68":"df[\"len\"] = [len(text) for text in df.message]","f474e7ae":"df.groupby(\"class\").describe()","c0fd0aa7":"df[\"class\"].value_counts()\/df[\"class\"].shape[0]*100","276bad33":"df.head()","531277cc":"stopword_list = nltk.corpus.stopwords.words(\"english\")","b9c38ee2":"tokener = nltk.word_tokenize","1b46183b":"common_words = {\"word\": [], \"count\": []}\n\nfor word in df[\"message\"]:\n    tok = tokener(word)\n    m_counter = Counter(tok)\n    common_words[\"word\"].append(m_counter.most_common(1)[0][0])\n    common_words[\"count\"].append(m_counter.most_common(1)[0][1])","6feb6d62":"common_words_df = pd.concat([df, pd.DataFrame(common_words)], axis = 1)","f83a2e75":"words_cloud = wordcloud.WordCloud().generate_from_text(' '.join(common_words_df[\"word\"]))","2871010e":"plt.figure(figsize=(10,6))\nplt.imshow(words_cloud)","291c1322":"df = df.drop(\"len\", axis = 1)","28ac4c06":"df.head()","b9155cae":"df.message = df.message.apply(str.lower)","ee3135c4":"def remove_whitespace(text):\n    \n    clean_words = re.sub(string.whitespace, \" \", text)\n    \n    return clean_words\n\ndf.message = df.message.apply(remove_whitespace)","e15eca68":"def remove_punch(text):\n    \n    rm_punch = str(text).translate(str.maketrans(\" \", \" \", string.punctuation))\n    \n    return rm_punch\n\ndf.message = df.message.apply(remove_punch)","27b08e13":"df.message[4]","33d864ed":"def remove_stopwords(text):\n    \n    tok_word = tokener(text)\n    \n    clean_words = []\n    \n    for word in tok_word:\n        if word in stopword_list:\n            continue\n        else:\n            clean_words.append(word)\n    words = ' '.join(clean_words)\n    return words","22bb3101":"df.message = df.message.apply(remove_stopwords)","f4a960ab":"ham_class = df[df[\"class\"] == \"ham\"]\nspam_class = df[df[\"class\"] == \"spam\"]","84ae7803":"ham_text = ' '.join(ham_class.message)\nspam_text = ' '.join(spam_class.message)","5f5e18a6":"ham_wordcloud = wordcloud.WordCloud().generate_from_text(ham_text)\nspam_wordcloud = wordcloud.WordCloud().generate_from_text(spam_text)","3687888a":"plt.figure(figsize=(16,12))\nplt.subplot(121)\nplt.title(\"Word Cloud of Ham Class After Pre-Processing\")\nplt.imshow(ham_wordcloud)\nplt.subplot(122)\nplt.title(\"Word Cloud of Spam Class After Pre-Processing\")\nplt.imshow(spam_wordcloud)","4859c934":"def remove_digits(text):\n    \n    clean_text = str(text).translate(str.maketrans(\"\", \"\", string.digits))\n    \n    return clean_text","36bd638e":"df.message = df.message.apply(remove_digits)","2d0adc8c":"df.message[2]","cd0a2124":"remove_whitespace(df.message[2])","edce384d":"def remove_extraspace(text):\n    \n    clean_text = re.sub(r\"\\s+\", \" \", text)\n    \n    return clean_text","005d4a68":"df.message = df.message.apply(remove_extraspace)","47b6ce4b":"lm = nltk.WordNetLemmatizer()","54eb5cd3":"def lemma(text):\n    \n    tok_word = tokener(text)\n    \n    lemma_word = []\n    \n    for tok in tok_word:\n        lemma_tok = lm.lemmatize(tok)\n        lemma_word.append(lemma_tok)\n        \n    return ' '.join(lemma_word)","9a9ead83":"df[\"lemma_message\"] = df.message.apply(lemma)","3960e91f":"df.head(5)","381704b0":"from sklearn.model_selection import train_test_split","4b2df0ce":"final_df = df.iloc[:, [0, 2]]","6f7e0586":"X = df.iloc[:, 1]\ny = df.iloc[:, 0]","30395e82":"from sklearn.preprocessing import LabelBinarizer","f7783f8c":"from sklearn.feature_extraction.text import TfidfVectorizer","fbe32feb":"lb = LabelBinarizer()","05b181f7":"scaled_y = lb.fit_transform(y)","4884fe26":"tf_idf = TfidfVectorizer(ngram_range=(1,3))","bae2d817":"scaled_x = tf_idf.fit_transform(X)","b72c93a3":"import sklearn\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC","5e2b743c":"from sklearn.model_selection import StratifiedKFold","c06b8c93":"kfold= StratifiedKFold(n_splits= 10)","3a07d300":"sklearn.metrics.accuracy_score","90198581":"def evaluate_model(y_train, y_test, y_train_predict, y_test_predict):\n    \n    print(\"*******Accuracy*******\\n\")\n    \n    train_accuracy = sklearn.metrics.accuracy_score(y_train, y_train_predict)\n    test_accuracy = sklearn.metrics.accuracy_score(y_test, y_test_predict)\n    \n    print(\"Train Accuracy: %s\" %(train_accuracy))\n    print(\"Test Accuracy: %s\" %(test_accuracy))\n    \n    train_error = 1-train_accuracy\n    test_error = 1-test_accuracy\n    \n    print(\"\\n\")\n    print(\"Train Error: %s\" %(train_error))\n    print(\"Test Error: %s\" %(test_error))\n    \n    print(\"\\n******* F1-Score **********\")\n    \n    train_f1_score = sklearn.metrics.f1_score(y_train, y_train_predict)\n    test_f1_score = sklearn.metrics.f1_score(y_test, y_test_predict)\n    \n    print(\"\\n\")\n    print(\"Train F1-Score: %s\" %(train_f1_score))\n    print(\"Test F1-Score: %s\" %(test_f1_score))\n    \n    print(\"\\n********* Log Loss ***********\")\n    train_log_loss = sklearn.metrics.log_loss(y_train, y_train_predict)\n    test_log_loss = sklearn.metrics.log_loss(y_test, y_test_predict)\n    \n    print(\"\\n\")\n    print(\"Train Log Loss: %s\" %(train_log_loss))\n    print(\"Test Log Loss: %s\" %(test_log_loss))\n    \n    print(\"\\n********* Classification Report *********\")\n    train_cf_report = pd.DataFrame(sklearn.metrics.classification_report(y_train, y_train_predict, output_dict = True))\n    test_cf_report = pd.DataFrame(sklearn.metrics.classification_report(y_test, y_test_predict, output_dict = True))\n    \n    print(\"\\n\")\n    print(\"Train Classification Report:\")\n    print(train_cf_report)\n    print(\"Test Classification Report:\")\n    print(test_cf_report)\n    \n    print(\"\\n********* F-Beta Score ********\")\n    train_fbeta_score = sklearn.metrics.fbeta_score(y_train, y_train_predict, beta = 0.5)\n    test_fbeta_score = sklearn.metrics.fbeta_score(y_test, y_test_predict, beta = 0.5)\n    \n    print(\"\\n\")\n    print(\"Train FBeta Score: %s\" %(train_fbeta_score))\n    print(\"Test FBeta Score: %s\" %(test_fbeta_score))\n    \n    print(\"\\n******** Confustion Matrix *********\")\n    train_conf_mtx = sklearn.metrics.confusion_matrix(y_train, y_train_predict)\n    test_conf_mtx = sklearn.metrics.confusion_matrix(y_test, y_test_predict)\n    \n    print(\"\\n\")\n    print(\"Train Confusion Matrix:\")\n    print(train_conf_mtx)\n    print(\"Test Confusion Matrix:\")\n    print(test_conf_mtx)","2d515795":"lr_train_accuracy = []\nlr_test_accuracy = []\nlr_train_f1_score = []\nlr_test_f1_score = []\n\nfor train_idx, test_idx in kfold.split(scaled_x, scaled_y):\n    x_train, x_test, y_train, y_test = scaled_x[train_idx], scaled_x[test_idx], scaled_y[train_idx], scaled_y[test_idx]\n    lr = LogisticRegression(random_state = 42)\n    lr.fit(x_train, y_train)\n    train_predict = lr.predict(x_train)\n    test_predict = lr.predict(x_test)\n    \n    train_accu = sklearn.metrics.accuracy_score(y_train, train_predict)\n    test_accu = sklearn.metrics.accuracy_score(y_test, test_predict)\n    \n    train_f1 = sklearn.metrics.f1_score(y_train, train_predict)\n    test_f1 = sklearn.metrics.f1_score(y_test, test_predict)\n    \n    lr_train_accuracy.append(train_accu)\n    lr_test_accuracy.append(test_accu)\n    lr_train_f1_score.append(train_f1)\n    lr_test_f1_score.append(test_f1)","ad55e2eb":"plt.figure(figsize=(16,4))\nplt.subplot(121)\nplt.title(\"Train Accuracy v\/s Test Accuracy: %s\" %(np.mean(lr_test_accuracy)))\nplt.plot(lr_train_accuracy, marker = \"o\")\nplt.plot(lr_test_accuracy, marker = \"o\", linestyle = \"--\")\nplt.legend([\"Train Accuracy\", \"Test Accuracy\"])\nplt.subplot(122)\nplt.title(\"Train F1 Score v\/s Test F1 Score: %s\" %(np.mean(lr_test_f1_score)))\nplt.plot(lr_train_f1_score, marker = \"o\", color = \"r\")\nplt.plot(lr_test_f1_score,  marker = \"o\", linestyle = \"--\", color = \"orange\")\nplt.legend([\"Train F1-Score\", \"Test F1-Score\"])","38d3004b":"knn_train_accuracy = []\nknn_test_accuracy = []\nknn_train_f1_score = []\nknn_test_f1_score = []\n\nfor train_idx, test_idx in kfold.split(scaled_x, scaled_y):\n    x_train, x_test, y_train, y_test = scaled_x[train_idx], scaled_x[test_idx], scaled_y[train_idx], scaled_y[test_idx]\n    knn = KNeighborsClassifier()\n    knn.fit(x_train, y_train)\n    train_predict = knn.predict(x_train)\n    test_predict = knn.predict(x_test)\n    \n    train_accu = sklearn.metrics.accuracy_score(y_train, train_predict)\n    test_accu = sklearn.metrics.accuracy_score(y_test, test_predict)\n    \n    train_f1 = sklearn.metrics.f1_score(y_train, train_predict)\n    test_f1 = sklearn.metrics.f1_score(y_test, test_predict)\n    \n    knn_train_accuracy.append(train_accu)\n    knn_test_accuracy.append(test_accu)\n    knn_train_f1_score.append(train_f1)\n    knn_test_f1_score.append(test_f1)","122cecbb":"plt.figure(figsize=(16,4))\nplt.subplot(121)\nplt.title(\"Train Accuracy v\/s Test Accuracy: %s\" %(np.mean(knn_test_accuracy)))\nplt.plot(knn_train_accuracy, marker = \"o\")\nplt.plot(knn_test_accuracy, marker = \"o\", linestyle = \"--\")\nplt.legend([\"Train Accuracy\", \"Test Accuracy\"])\nplt.subplot(122)\nplt.title(\"Train F1 Score v\/s Test F1 Score: %s\" %(np.mean(knn_test_f1_score)))\nplt.plot(knn_train_f1_score, marker = \"o\", color = \"r\")\nplt.plot(knn_test_f1_score,  marker = \"o\", linestyle = \"--\", color = \"orange\")\nplt.legend([\"Train F1-Score\", \"Test F1-Score\"])","5385f7a5":"dt_train_accuracy = []\ndt_test_accuracy = []\ndt_train_f1_score = []\ndt_test_f1_score = []\n\nfor train_idx, test_idx in kfold.split(scaled_x, scaled_y):\n    x_train, x_test, y_train, y_test = scaled_x[train_idx], scaled_x[test_idx], scaled_y[train_idx], scaled_y[test_idx]\n    dt = DecisionTreeClassifier(random_state=42)\n    dt.fit(x_train, y_train)\n    train_predict = dt.predict(x_train)\n    test_predict = dt.predict(x_test)\n    \n    train_accu = sklearn.metrics.accuracy_score(y_train, train_predict)\n    test_accu = sklearn.metrics.accuracy_score(y_test, test_predict)\n    \n    train_f1 = sklearn.metrics.f1_score(y_train, train_predict)\n    test_f1 = sklearn.metrics.f1_score(y_test, test_predict)\n    \n    dt_train_accuracy.append(train_accu)\n    dt_test_accuracy.append(test_accu)\n    dt_train_f1_score.append(train_f1)\n    dt_test_f1_score.append(test_f1)","81718fde":"plt.figure(figsize=(16,4))\nplt.subplot(121)\nplt.title(\"Train Accuracy v\/s Test Accuracy: %s\" %(np.mean(dt_test_accuracy)))\nplt.plot(dt_train_accuracy, marker = \"o\")\nplt.plot(dt_test_accuracy, marker = \"o\", linestyle = \"--\")\nplt.legend([\"Train Accuracy\", \"Test Accuracy\"])\nplt.subplot(122)\nplt.title(\"Train F1 Score v\/s Test F1 Score: %s\" %(np.mean(dt_test_f1_score)))\nplt.plot(dt_train_f1_score, marker = \"o\", color = \"r\")\nplt.plot(dt_test_f1_score,  marker = \"o\", linestyle = \"--\", color = \"orange\")\nplt.legend([\"Train F1-Score\", \"Test F1-Score\"])","af8fb91a":"lgbm_train_accuracy = []\nlgbm_test_accuracy = []\nlgbm_train_f1_score = []\nlgbm_test_f1_score = []\n\nfor train_idx, test_idx in kfold.split(scaled_x, scaled_y):\n    x_train, x_test, y_train, y_test = scaled_x[train_idx], scaled_x[test_idx], scaled_y[train_idx], scaled_y[test_idx]\n    lgbm = LGBMClassifier(random_state=42)\n    lgbm.fit(x_train, y_train)\n    train_predict = lgbm.predict(x_train)\n    test_predict = lgbm.predict(x_test)\n    \n    train_accu = sklearn.metrics.accuracy_score(y_train, train_predict)\n    test_accu = sklearn.metrics.accuracy_score(y_test, test_predict)\n    \n    train_f1 = sklearn.metrics.f1_score(y_train, train_predict)\n    test_f1 = sklearn.metrics.f1_score(y_test, test_predict)\n    \n    lgbm_train_accuracy.append(train_accu)\n    lgbm_test_accuracy.append(test_accu)\n    lgbm_train_f1_score.append(train_f1)\n    lgbm_test_f1_score.append(test_f1)","da78c176":"plt.figure(figsize=(16,4))\nplt.subplot(121)\nplt.title(\"Train Accuracy v\/s Test Accuracy: %s\" %(np.mean(lgbm_test_accuracy)))\nplt.plot(lgbm_train_accuracy, marker = \"o\")\nplt.plot(lgbm_test_accuracy, marker = \"o\", linestyle = \"--\")\nplt.legend([\"Train Accuracy\", \"Test Accuracy\"])\nplt.subplot(122)\nplt.title(\"Train F1 Score v\/s Test F1 Score: %s\" %(np.mean(lgbm_test_f1_score)))\nplt.plot(lgbm_train_f1_score, marker = \"o\", color = \"r\")\nplt.plot(lgbm_test_f1_score,  marker = \"o\", linestyle = \"--\", color = \"orange\")\nplt.legend([\"Train F1-Score\", \"Test F1-Score\"])","e5bb8781":"xgb_train_accuracy = []\nxgb_test_accuracy = []\nxgb_train_f1_score = []\nxgb_test_f1_score = []\n\nfor train_idx, test_idx in kfold.split(scaled_x, scaled_y):\n    x_train, x_test, y_train, y_test = scaled_x[train_idx], scaled_x[test_idx], scaled_y[train_idx], scaled_y[test_idx]\n    xgb = XGBClassifier(random_state=42)\n    xgb.fit(x_train, y_train)\n    train_predict = xgb.predict(x_train)\n    test_predict = xgb.predict(x_test)\n    \n    train_accu = sklearn.metrics.accuracy_score(y_train, train_predict)\n    test_accu = sklearn.metrics.accuracy_score(y_test, test_predict)\n    \n    train_f1 = sklearn.metrics.f1_score(y_train, train_predict)\n    test_f1 = sklearn.metrics.f1_score(y_test, test_predict)\n    \n    xgb_train_accuracy.append(train_accu)\n    xgb_test_accuracy.append(test_accu)\n    xgb_train_f1_score.append(train_f1)\n    xgb_test_f1_score.append(test_f1)","932f99a9":"plt.figure(figsize=(16,4))\nplt.subplot(121)\nplt.title(\"Train Accuracy v\/s Test Accuracy: %s\" %(np.mean(xgb_test_accuracy)))\nplt.plot(xgb_train_accuracy, marker = \"o\")\nplt.plot(xgb_test_accuracy, marker = \"o\", linestyle = \"--\")\nplt.legend([\"Train Accuracy\", \"Test Accuracy\"])\nplt.subplot(122)\nplt.title(\"Train F1 Score v\/s Test F1 Score: %s\" %(np.mean(xgb_test_f1_score)))\nplt.plot(xgb_train_f1_score, marker = \"o\", color = \"r\")\nplt.plot(xgb_test_f1_score,  marker = \"o\", linestyle = \"--\", color = \"orange\")\nplt.legend([\"Train F1-Score\", \"Test F1-Score\"])","d7eb567a":"multinom_train_accuracy = []\nmultinom_test_accuracy = []\nmultinom_train_f1_score = []\nmultinom_test_f1_score = []\n\nfor train_idx, test_idx in kfold.split(scaled_x, scaled_y):\n    x_train, x_test, y_train, y_test = scaled_x[train_idx], scaled_x[test_idx], scaled_y[train_idx], scaled_y[test_idx]\n    multinom = MultinomialNB()\n    multinom.fit(x_train, y_train)\n    train_predict = multinom.predict(x_train)\n    test_predict = multinom.predict(x_test)\n    \n    train_accu = sklearn.metrics.accuracy_score(y_train, train_predict)\n    test_accu = sklearn.metrics.accuracy_score(y_test, test_predict)\n    \n    train_f1 = sklearn.metrics.f1_score(y_train, train_predict)\n    test_f1 = sklearn.metrics.f1_score(y_test, test_predict)\n    \n    multinom_train_accuracy.append(train_accu)\n    multinom_test_accuracy.append(test_accu)\n    multinom_train_f1_score.append(train_f1)\n    multinom_test_f1_score.append(test_f1)","d7396e5c":"plt.figure(figsize=(16,4))\nplt.subplot(121)\nplt.title(\"Train Accuracy v\/s Test Accuracy: %s\" %(np.mean(multinom_test_accuracy)))\nplt.plot(multinom_train_accuracy, marker = \"o\")\nplt.plot(multinom_test_accuracy, marker = \"o\", linestyle = \"--\")\nplt.legend([\"Train Accuracy\", \"Test Accuracy\"])\nplt.subplot(122)\nplt.title(\"Train F1 Score v\/s Test F1 Score: %s\" %(np.mean(multinom_test_f1_score)))\nplt.plot(multinom_train_f1_score, marker = \"o\", color = \"r\")\nplt.plot(multinom_test_f1_score,  marker = \"o\", linestyle = \"--\", color = \"orange\")\nplt.legend([\"Train F1-Score\", \"Test F1-Score\"])","917e677b":"svc_classif_train_accuracy = []\nsvc_classif_test_accuracy = []\nsvc_classif_train_f1_score = []\nsvc_classif_test_f1_score = []\n\nfor train_idx, test_idx in kfold.split(scaled_x, scaled_y):\n    x_train, x_test, y_train, y_test = scaled_x[train_idx], scaled_x[test_idx], scaled_y[train_idx], scaled_y[test_idx]\n    svc_classif = SVC(random_state=42)\n    svc_classif.fit(x_train, y_train)\n    train_predict = svc_classif.predict(x_train)\n    test_predict = svc_classif.predict(x_test)\n    \n    train_accu = sklearn.metrics.accuracy_score(y_train, train_predict)\n    test_accu = sklearn.metrics.accuracy_score(y_test, test_predict)\n    \n    train_f1 = sklearn.metrics.f1_score(y_train, train_predict)\n    test_f1 = sklearn.metrics.f1_score(y_test, test_predict)\n    \n    svc_classif_train_accuracy.append(train_accu)\n    svc_classif_test_accuracy.append(test_accu)\n    svc_classif_train_f1_score.append(train_f1)\n    svc_classif_test_f1_score.append(test_f1)","e2ba1e0e":"plt.figure(figsize=(16,4))\nplt.subplot(121)\nplt.title(\"Train Accuracy v\/s Test Accuracy: %s\" %(np.mean(svc_classif_test_accuracy)))\nplt.plot(svc_classif_train_accuracy, marker = \"o\")\nplt.plot(svc_classif_test_accuracy, marker = \"o\", linestyle = \"--\")\nplt.legend([\"Train Accuracy\", \"Test Accuracy\"])\nplt.subplot(122)\nplt.title(\"Train F1 Score v\/s Test F1 Score: %s\" %(np.mean(svc_classif_test_f1_score)))\nplt.plot(svc_classif_train_f1_score, marker = \"o\", color = \"r\")\nplt.plot(svc_classif_test_f1_score,  marker = \"o\", linestyle = \"--\", color = \"orange\")\nplt.legend([\"Train F1-Score\", \"Test F1-Score\"])","66edd6cf":"sgd_train_accuracy = []\nsgd_test_accuracy = []\nsgd_train_f1_score = []\nsgd_test_f1_score = []\n\nfor train_idx, test_idx in kfold.split(scaled_x, scaled_y):\n    x_train, x_test, y_train, y_test = scaled_x[train_idx], scaled_x[test_idx], scaled_y[train_idx], scaled_y[test_idx]\n    sgd = SGDClassifier(random_state=42)\n    sgd.fit(x_train, y_train)\n    train_predict = sgd.predict(x_train)\n    test_predict = sgd.predict(x_test)\n    \n    train_accu = sklearn.metrics.accuracy_score(y_train, train_predict)\n    test_accu = sklearn.metrics.accuracy_score(y_test, test_predict)\n    \n    train_f1 = sklearn.metrics.f1_score(y_train, train_predict)\n    test_f1 = sklearn.metrics.f1_score(y_test, test_predict)\n    \n    sgd_train_accuracy.append(train_accu)\n    sgd_test_accuracy.append(test_accu)\n    sgd_train_f1_score.append(train_f1)\n    sgd_test_f1_score.append(test_f1)","297668be":"plt.figure(figsize=(16,4))\nplt.subplot(121)\nplt.title(\"Train Accuracy v\/s Test Accuracy: %s\" %(np.mean(sgd_test_accuracy)))\nplt.plot(sgd_train_accuracy, marker = \"o\")\nplt.plot(sgd_test_accuracy, marker = \"o\", linestyle = \"--\")\nplt.legend([\"Train Accuracy\", \"Test Accuracy\"])\nplt.subplot(122)\nplt.title(\"Train F1 Score v\/s Test F1 Score: %s\" %(np.mean(sgd_test_f1_score)))\nplt.plot(sgd_train_f1_score, marker = \"o\", color = \"r\")\nplt.plot(sgd_test_f1_score,  marker = \"o\", linestyle = \"--\", color = \"orange\")\nplt.legend([\"Train F1-Score\", \"Test F1-Score\"])","b6564005":"pa_classif_train_accuracy = []\npa_classif_test_accuracy = []\npa_classif_train_f1_score = []\npa_classif_test_f1_score = []\n\nfor train_idx, test_idx in kfold.split(scaled_x, scaled_y):\n    x_train, x_test, y_train, y_test = scaled_x[train_idx], scaled_x[test_idx], scaled_y[train_idx], scaled_y[test_idx]\n    pa_classif = PassiveAggressiveClassifier(random_state=42)\n    pa_classif.fit(x_train, y_train)\n    train_predict = pa_classif.predict(x_train)\n    test_predict = pa_classif.predict(x_test)\n    \n    train_accu = sklearn.metrics.accuracy_score(y_train, train_predict)\n    test_accu = sklearn.metrics.accuracy_score(y_test, test_predict)\n    \n    train_f1 = sklearn.metrics.f1_score(y_train, train_predict)\n    test_f1 = sklearn.metrics.f1_score(y_test, test_predict)\n    \n    pa_classif_train_accuracy.append(train_accu)\n    pa_classif_test_accuracy.append(test_accu)\n    pa_classif_train_f1_score.append(train_f1)\n    pa_classif_test_f1_score.append(test_f1)","e0fdefeb":"plt.figure(figsize=(16,4))\nplt.subplot(121)\nplt.title(\"Train Accuracy v\/s Test Accuracy: %s\" %(np.mean(pa_classif_test_accuracy)))\nplt.plot(pa_classif_train_accuracy, marker = \"o\")\nplt.plot(pa_classif_test_accuracy, marker = \"o\", linestyle = \"--\")\nplt.legend([\"Train Accuracy\", \"Test Accuracy\"])\nplt.subplot(122)\nplt.title(\"Train F1 Score v\/s Test F1 Score: %s\" %(np.mean(pa_classif_test_f1_score)))\nplt.plot(pa_classif_train_f1_score, marker = \"o\", color = \"r\")\nplt.plot(pa_classif_test_f1_score,  marker = \"o\", linestyle = \"--\", color = \"orange\")\nplt.legend([\"Train F1-Score\", \"Test F1-Score\"])","bb0050dd":"ridge_classif_train_accuracy = []\nridge_classif_test_accuracy = []\nridge_classif_train_f1_score = []\nridge_classif_test_f1_score = []\n\nfor train_idx, test_idx in kfold.split(scaled_x, scaled_y):\n    x_train, x_test, y_train, y_test = scaled_x[train_idx], scaled_x[test_idx], scaled_y[train_idx], scaled_y[test_idx]\n    ridge_classif = RidgeClassifier(random_state=42)\n    ridge_classif.fit(x_train, y_train)\n    train_predict = ridge_classif.predict(x_train)\n    test_predict = ridge_classif.predict(x_test)\n    \n    train_accu = sklearn.metrics.accuracy_score(y_train, train_predict)\n    test_accu = sklearn.metrics.accuracy_score(y_test, test_predict)\n    \n    train_f1 = sklearn.metrics.f1_score(y_train, train_predict)\n    test_f1 = sklearn.metrics.f1_score(y_test, test_predict)\n    \n    ridge_classif_train_accuracy.append(train_accu)\n    ridge_classif_test_accuracy.append(test_accu)\n    ridge_classif_train_f1_score.append(train_f1)\n    ridge_classif_test_f1_score.append(test_f1)","fd81d757":"plt.figure(figsize=(16,4))\nplt.subplot(121)\nplt.title(\"Train Accuracy v\/s Test Accuracy: %s\" %(np.mean(ridge_classif_test_accuracy)))\nplt.plot(ridge_classif_train_accuracy, marker = \"o\")\nplt.plot(ridge_classif_test_accuracy, marker = \"o\", linestyle = \"--\")\nplt.legend([\"Train Accuracy\", \"Test Accuracy\"])\nplt.subplot(122)\nplt.title(\"Train F1 Score v\/s Test F1 Score: %s\" %(np.mean(ridge_classif_test_f1_score)))\nplt.plot(ridge_classif_train_f1_score, marker = \"o\", color = \"r\")\nplt.plot(ridge_classif_test_f1_score,  marker = \"o\", linestyle = \"--\", color = \"orange\")\nplt.legend([\"Train F1-Score\", \"Test F1-Score\"])","2d6e56b0":"train_accuracy_score = pd.DataFrame([lr_train_accuracy, knn_train_accuracy, dt_train_accuracy, lgbm_train_accuracy, xgb_train_accuracy, multinom_train_accuracy, svc_classif_train_accuracy, sgd_train_accuracy, pa_classif_train_accuracy, ridge_classif_train_accuracy], index = [\"Logistic\", \"KNN\", \"DecisionTree\", \"LightGBM\", \"XGB\", \"Multinom Bayes\", \"SVM\", \"SGD\", \"Passive Classifier\", \"Ridge\"])\ntest_accuracy_score = pd.DataFrame([lr_test_accuracy, knn_test_accuracy, dt_test_accuracy, lgbm_test_accuracy, xgb_test_accuracy, multinom_test_accuracy, svc_classif_test_accuracy, sgd_test_accuracy, pa_classif_test_accuracy, ridge_classif_train_accuracy], index = [\"Logistic\", \"KNN\", \"DecisionTree\", \"LightGBM\", \"XGB\", \"Multinom Bayes\", \"SVM\", \"SGD\", \"Passive Classifier\", \"Ridge\"])\ntrain_f1_score = pd.DataFrame([lr_train_f1_score, knn_train_f1_score, dt_train_f1_score, lgbm_train_f1_score, xgb_train_f1_score, multinom_train_f1_score, svc_classif_train_f1_score, sgd_train_f1_score, pa_classif_train_f1_score, ridge_classif_train_accuracy], index = [\"Logistic\", \"KNN\", \"DecisionTree\", \"LightGBM\", \"XGB\", \"Multinom Bayes\", \"SVM\", \"SGD\", \"Passive Classifier\", \"Ridge\"])\ntest_f1_score = pd.DataFrame([lr_test_f1_score, knn_test_f1_score, dt_test_f1_score, lgbm_test_f1_score, xgb_test_f1_score, multinom_test_f1_score, svc_classif_test_f1_score, sgd_test_f1_score, pa_classif_test_f1_score, ridge_classif_train_f1_score], index = [\"Logistic\", \"KNN\", \"DecisionTree\", \"LightGBM\", \"XGB\", \"Multinom Bayes\", \"SVM\", \"SGD\", \"Passive Classifier\", \"Ridge\"])","0755277a":"from IPython.display import display, HTML","d3ea8394":"CSS = \"\"\"\n.output {\n    flex-direction: col;\n}\n\"\"\"\n\nHTML('<style>{}<\/style>'.format(CSS))","b5267e41":"display(train_accuracy_score.style.background_gradient(cmap = \"Blues\"), display_id = 'train_acc')\ndisplay(test_accuracy_score.style.background_gradient(cmap = \"Blues\"), display_id = 'test_acc')\ndisplay(train_f1_score.style.background_gradient(cmap = \"Blues\"), display_id = 'train_f1_score')\ndisplay(test_f1_score.style.background_gradient(cmap = \"Blues\"), display_id = 'test_f1_score')","82de71b8":"model_avg_scores = {'model': [], \"train_accuracy\": [], \"test_accuracy\": [], \"train_f1\": [], \"test_f1\": []}\n\nfor idx1, row1 in train_accuracy_score.iterrows():\n    model_avg_scores[\"model\"].append(idx1)\n    model_avg_scores['train_accuracy'].append(np.mean(row1))\n    \nfor idx2, row2 in test_accuracy_score.iterrows():\n    model_avg_scores['test_accuracy'].append(np.mean(row2))\n    \nfor idx3, row3 in train_f1_score.iterrows():\n    model_avg_scores['train_f1'].append(np.mean(row3))\n    \nfor idx4, row4 in test_f1_score.iterrows():\n    model_avg_scores['test_f1'].append(np.mean(row4))","8476694d":"model_score_df = pd.DataFrame(model_avg_scores)","f7bee278":"plt.figure(figsize=(18,14))\nplt.subplot(221)\nplt.title(\"Model's Train Accuracy Score\")\nsns.barplot(x = model_score_df[\"train_accuracy\"], y = model_score_df.model, color = \"skyblue\")\nplt.xticks(rotation = \"90\")\nfor idx, val in enumerate(model_score_df.train_accuracy):\n    plt.text(val, idx, round(float(val),3))\nplt.subplot(222)\nplt.title(\"Model's Test Accuracy Score\")\nsns.barplot(x = model_score_df[\"test_accuracy\"], y = model_score_df.model, color = \"g\")\nplt.xticks(rotation = \"90\")\nfor idx, val in enumerate(model_score_df.test_accuracy):\n    plt.text(val, idx, round(float(val),3))\nplt.subplot(223)\nplt.title(\"Model's Train F1 Score\")\nsns.barplot(x = model_score_df[\"train_f1\"], y = model_score_df.model, color = \"skyblue\")\nplt.xticks(rotation = \"90\")\nfor idx, val in enumerate(model_score_df.train_f1):\n    plt.text(val, idx, round(float(val),3))\nplt.subplot(224)\nplt.title(\"Model's Test F1 Score\")\nsns.barplot(x = model_score_df[\"test_f1\"], y = model_score_df.model, color = \"g\")\nplt.xticks(rotation = \"90\")\nfor idx, val in enumerate(model_score_df.test_f1):\n    plt.text(val, idx, round(float(val),3))\nplt.tight_layout()\nplt.show()","3ec7fe2f":"model_score_df.style.background_gradient(cmap = \"Blues_r\")","c1fcd91b":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score","aff80a53":"# pa_weights = np.linspace(0.0,0.99,200)\n# [{0:x, 1:1.0-x} for x in pa_weights]","208a8e0f":"pa_param_grid = {\n    \"C\": [0.0001, 0.001, 0.01, 1.0, 10, 100],\n    \"fit_intercept\": [True, False],\n    \"max_iter\": [100, 500, 1000],\n    \"tol\": [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 1.0, 0.0005],\n    \"shuffle\": [True, False],\n    \"class_weight\": [\"balanced\"]\n}","dd0859e8":"pa_classif = PassiveAggressiveClassifier(random_state=42, verbose = 0)","01b20ad2":"random_search = RandomizedSearchCV(estimator = pa_classif, param_distributions = pa_param_grid, scoring = \"f1\", cv = 10, random_state = 42)","6d366c80":"x_train, x_test, y_train, y_test = train_test_split(scaled_x, scaled_y, test_size = 0.30, random_state = 42)","5c935f06":"random_search.fit(x_train, y_train)","a5f8e9fc":"random_search.best_score_","40ceef1e":"random_search.best_estimator_","632cf11b":"plt.figure(figsize=(10,5))\nplt.title(\"Passive Classifier Mean Test Score Plot\")\nplt.plot(random_search.cv_results_[\"mean_test_score\"], marker = \"o\")\nplt.xlabel(\"Cross Validation\")\nplt.ylabel(\"Mean Test Score\")","7316376e":"random_search.cv_results_[\"mean_test_score\"]","f33a1c13":"y_train_predict = random_search.predict(x_train)","74d6e1c8":"y_test_predict = random_search.predict(x_test)","dc29b5ac":"evaluate_model(y_train, y_test, y_train_predict, y_test_predict)","a3332a94":"ridge_params = {\n    'alpha': [1.0,0.1,0.001,0.0001,0.5,0.005,0.0005,10,25,50,75],\n    'fit_intercept': [True, False],\n    'normalize': [True, False],\n    'tol': [0.001,0.01,0.1,0.005,0.05,0.5,0.2,0.02,0.002],\n    'class_weight': ['balanced'],\n    'max_iter': [100,500,1000, False]\n}","422a4d97":"ridge_classifier_hp = RidgeClassifier(random_state = 0)","14b4065a":"ridge_random_search = RandomizedSearchCV(estimator=ridge_classifier_hp, param_distributions=ridge_params, cv = 10, scoring = 'f1', random_state = 42, verbose = 0)","5b7554ba":"ridge_random_search.fit(x_train, y_train)","883cabcc":"ridge_random_search.best_estimator_","62a813f1":"ridge_random_search.best_score_","12395ae3":"ridge_random_search.cv_results_[\"mean_test_score\"]","d1a516b0":"plt.figure(figsize=(10,5))\nplt.title(\"Ridge Classifier Mean Test Score Plot\")\nplt.plot(ridge_random_search.cv_results_[\"mean_test_score\"], marker = \"o\")\nplt.xlabel(\"Cross Validation\")\nplt.ylabel(\"Mean Test Score\")","5da76974":"ridge_ytrain_predict = ridge_random_search.predict(x_train)\nridge_ytest_predict = ridge_random_search.predict(x_test)","21836146":"evaluate_model(y_train, y_test, ridge_ytrain_predict, ridge_ytest_predict)","d730ed06":"# RidgeClassifier(alpha=75, class_weight='balanced', max_iter=500, normalize=True,\n#                 random_state=0, tol=0.01)","a5333d78":"grid_params = {\n    \n    'alpha': [25,50,60,70,75,80,85],\n    'max_iter': [400,420,450,480,500,550,600],\n    'tol': [0.01,0.02,0.03,0.04,0.05]\n}","d1a1f3fe":"ridge = RidgeClassifier(random_state=0, class_weight='balanced', normalize=True)","592d1495":"grid_search = GridSearchCV(estimator=ridge, param_grid = grid_params, scoring = 'f1', cv = 10)","6924fd82":"best_model = grid_search.fit(x_train, y_train)","3669ca54":"best_model.best_estimator_","2fda6ee0":"best_model.best_params_","19847636":"best_model.best_score_","14edae30":"plt.figure(figsize=(10,6))\nplt.title(\"Ridge Mean Test Score\")\nplt.plot(best_model.cv_results_[\"mean_test_score\"])\nplt.xlabel(\"Cross Validation\")\nplt.ylabel(\"Mean Test Score\")","cb1996e5":"train_predict = best_model.predict(x_train)\ntest_predict = best_model.predict(x_test)","63475be3":"evaluate_model(y_train, y_test, train_predict, test_predict)","28b9dc26":"f_ridge =  RidgeClassifier(alpha=75, class_weight='balanced', max_iter=500, normalize=True, random_state=0, tol=0.01)","adb73357":"f_ridge.fit(x_train, y_train)","61cdc5ed":"train_final_prediction = f_ridge.predict(x_train)","eaadd049":"test_final_prediction = f_ridge.predict(x_test)","f6b4a8f6":"train_predict_df = pd.DataFrame(train_final_prediction)\ntest_predict_df = pd.DataFrame(test_final_prediction)","45ba5404":"final_prediction = train_predict_df.append(test_predict_df).reset_index().drop(\"index\", axis = 1)","5033b6dd":"final_df = df.iloc[:, :2]","62f97607":"final_df[\"class_predict\"] = final_prediction","77849fd8":"class_mapping = {0: \"ham\", 1:\"spam\"}","3d26a5cc":"final_df.class_predict = final_df.class_predict.map(class_mapping)","0c2ec91d":"plt.figure(figsize=(12,6))\nplt.subplot(121)\nplt.title(\"Actual Class\")\nsns.countplot(final_df[\"class\"])\nplt.subplot(122)\nplt.title(\"Predicted Class\")\nsns.countplot(final_df[\"class_predict\"])\nplt.tight_layout()","b314e786":"final_df.to_csv(\"spam_submission.csv\", index = False)","af4362a4":"## Passive Aggressive Classifier","a971e384":"## Ridge Classifier","327ea010":"As you can see above, the KNN algorithm is the worst model when it comes to handling the imbalanced dataset. If we take a look at the accuracy of the KNN is fairly decent but if we take a look at the F1 score, we can see the model is not performing good so we can say that in the presence of class imbalance, the accuracy would not be an ideal metric to look at but I have added to see the metric comparison in case of imabalance dataset.\n\nIf I have to choose any 2-3 algorithm for our hyperparameter optimization, I will prefer Ridge Classifier & Passive Classifier for our next step.","10317a78":"## Light GBM Classifier","c8c6d6bd":"### I hope you liked this kernel..!!!!","7fecb113":"## Logistic Regression","775bfc3e":"## Hyperparameter Optimization \n\nNow in this section we're going to perform hyperparameter optimization to see the performance of our both the models if we can improve it further by doing manual iteration into our model's parameters.","bd969bba":"### Grid Search Cross Validation - Ridge Classifier","7aa7a830":"## XGBoost Classifier","90f150dc":"## Multinomial Naive Bayes","81028367":"So based on the hyperparameter tuning, I can say that ridge classifier is performing slightly better when it comes to prediction and reducing the False Positive so I will be picking the Ridge Classifier to create my final model. Now, we are going to perform the Grid Search CV on the parameter that we've got from the randomized search cross validation.","9af6924a":"## Decision Trees","c114c803":"### Passive Aggressive Classifier Hyperparameter Tuning with Balanced Weight","4e4697d1":"It seems that the message are bit longer in the spam class as comapred to ham also spam message are less as compared to ham which clearly indicated the class imbalance in our data which we need to fix but we will going to cover this in our later pre-processing steps.","5ef6fcd7":"## K-Nearest Neighbor","21afccd2":"### Ridge Classifier Hyperparameter Tuning","a7989d1a":"## SVM Classifier - Radial Bias Function","c76123f0":"## SGD Classifier","31972633":"## Final Model"}}