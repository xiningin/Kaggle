{"cell_type":{"4c434748":"code","7a9fa3ff":"code","87aaf564":"code","b3d7225f":"code","38bb93b4":"code","5d14ad9e":"code","1892ba89":"code","7950985f":"code","5cfb9d7b":"code","6c94d02c":"code","6f4d4a17":"code","e8d0c433":"code","036fdd64":"code","1fdb1d62":"code","5f49effa":"code","f05971a7":"code","c90a33d8":"code","fdb79945":"code","85cd259e":"code","bcc714f9":"code","bf42c634":"code","03f556f9":"code","f728d7ac":"markdown","9139bb16":"markdown","b56447eb":"markdown","b7e89031":"markdown","c5670400":"markdown","c27fd947":"markdown","c1ecc5d2":"markdown"},"source":{"4c434748":"import sys\nsys.path.append('\/kaggle\/input\/efficientnet-keras-dataset\/efficientnet_kaggle')\nfrom efficientnet.tfkeras import *","7a9fa3ff":"from matplotlib import pyplot as plt\nimport math, os, cv2, gc, re\nimport numpy as np, pandas as pd\nfrom time import time\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K","87aaf564":"DEVICE = 'GPU'\n\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","b3d7225f":"SEED = 34 \n             \nIMAGE_SIZE = [256, 256]               \n\nBATCH_SIZE = 16 * REPLICAS \n\nFOLDS = 5\n\nVERBOSE = 1","38bb93b4":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = ((tf.cast(image, tf.float32) \/ 255.0) - 0.449) \/ 0.226     \n    #image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_labeled_tfrecord(example, return_image_name):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"target\": tf.io.FixedLenFeature([], tf.int64), \n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['target'], tf.int32)\n    img_name = example['image_name']\n    \n    if return_image_name:\n        return image, label, img_name\n    else:\n        return image, label\n\ndef read_unlabeled_tfrecord(example, return_image_name):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"id\": tf.io.FixedLenFeature([], tf.string), \n    }\n    \n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum if return_image_name else 0\n\ndef get_dataset(files, shuffle=False, repeat=False, labeled=True, return_image_names=True,\n                batch_size=BATCH_SIZE, dim=IMAGE_SIZE[0]):\n   \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n\n    if repeat:\n        ds = ds.repeat()\n    \n    if shuffle: \n        ds = ds.shuffle(2048)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n        \n    if labeled: \n        ds = ds.map(lambda example: read_labeled_tfrecord(example, return_image_names), \n                    num_parallel_calls=AUTO)  \n    else:\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), \n                    num_parallel_calls=AUTO)  \n\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(AUTO)\n    \n    return ds\n\ndef load_image(jpeg_path, image_id):  \n    img = ((cv2.imread(os.path.join(jpeg_path, image_id))\/255.0) - .449) \/ .226\n    #img = cv2.imread(os.path.join(jpeg_path, image_id))\/255.0\n    img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]))[:, :, ::-1]\n    return img\n\ndef generator(filepath, paths, batch_size=BATCH_SIZE):\n    i=0\n    while i <= len(paths):\n        batch = []\n        for cpt in range(batch_size):\n            if i + cpt >= len(paths):\n                i += batch_size\n                break\n            batch.append(load_image(filepath, paths[i+cpt]))\n            \n        i += batch_size\n        yield np.stack(batch)\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r'-([0-9]*)\\.').search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","5d14ad9e":"TRAINING_FILENAMES =  tf.io.gfile.glob(f'..\/input\/cassava-leaf-disease-tfrecords-{IMAGE_SIZE[0]}x{IMAGE_SIZE[0]}' + '\/*.tfrec')\nTRAINING_FILENAMES_ORG = tf.io.gfile.glob('..\/input\/cassava-leaf-disease-classification\/train_tfrecords' + '\/*.tfrec')\nTEST_JPEG_PATH = \"..\/input\/cassava-leaf-disease-classification\/test_images\"\n\nsubmission = pd.read_csv('..\/input\/cassava-leaf-disease-classification\/sample_submission.csv')","1892ba89":"FINE_TUNE = True\nFT_EPOCHS = 3","7950985f":"import cuml\nprint('RAPIDS version',cuml.__version__)","5cfb9d7b":"def efficientnet(b, image_size, head=False, LR=5e-4):\n    efns = [EfficientNetB0, EfficientNetB1, EfficientNetB2,\n            EfficientNetB3, EfficientNetB4, EfficientNetB5,\n            EfficientNetB6]\n    with strategy.scope():\n        efficient = efns[b](\n            input_shape=(image_size, image_size, 3),\n            weights='noisy-student', #imagenet\n            include_top=False\n        )\n        efficient.trainable=True\n        \n        if head:\n            model = tf.keras.Sequential([\n                efficient,\n                tf.keras.layers.GlobalAveragePooling2D(name='pooling'), \n                tf.keras.layers.Dropout(.2), \n                tf.keras.layers.Dense(5, activation='softmax')\n            ])\n            \n        else:\n            model = tf.keras.Sequential([\n                efficient,\n                tf.keras.layers.GlobalAveragePooling2D()]) \n                \n    if head: model.compile(optimizer=tf.keras.optimizers.Adam(LR), \n                           loss='sparse_categorical_crossentropy',\n                           metrics=['sparse_categorical_accuracy'])\n        \n    return model","6c94d02c":"FOLDS = 5\nSEED = 34\nN_NEIGH = 10","6f4d4a17":"skf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\npreds_all = []\npreds_model = []\noof_pred = []\noof_labels = []\n\nfor f, (train_index, val_index) in enumerate(skf.split(TRAINING_FILENAMES)):\n    \n    print('#'*30); print('#### FOLD',f+1); print('#'*30); print('')\n    print('Getting datasets...'); print('')\n    \n    train_ds = get_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[train_index]['TRAINING_FILENAMES']),\n                                            labeled=True, return_image_names=False, repeat=False, shuffle=False)\n    val_ds = get_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[val_index]['TRAINING_FILENAMES']),\n                                            labeled=True, return_image_names=False, repeat=False, shuffle=False)\n    test_ds = generator(TEST_JPEG_PATH, submission.image_id.values)\n    train_labs = [target.numpy() for img, target in iter(train_ds.unbatch())]\n    val_labs = [target.numpy() for img, target in iter(val_ds.unbatch())]\n    \n    effnet_ = efficientnet(b=3, image_size=IMAGE_SIZE[0], head=FINE_TUNE)\n    \n    if FINE_TUNE:\n        train_ds_ = get_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[train_index]['TRAINING_FILENAMES']),\n                                            labeled=True, return_image_names=False, repeat=True, shuffle=True)\n        \n        print('Fine tuning EfficientNet...'); print('')\n        ct_train = count_data_items(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[train_index]['TRAINING_FILENAMES']))\n        effnet_.fit(train_ds_, \n                    validation_data=val_ds,\n                    verbose=1, \n                    steps_per_epoch=ct_train\/\/BATCH_SIZE,\n                    epochs=FT_EPOCHS)\n        print('')\n        effnet = tf.keras.Model(inputs = effnet_.input, \n               outputs = effnet_.get_layer('pooling').output)\n\n    else: effnet = effnet_\n\n    print('Getting embeddings...'); print('')\n    embed = effnet.predict(train_ds, verbose=1)\n    embed_val = effnet.predict(val_ds, verbose=1)\n    embed_test = effnet.predict(test_ds, verbose=1)\n    np.save(f'embed_b4_{f}_{IMAGE_SIZE[0]}',embed.astype('float32'))\n    np.save(f'embed_val_b4_{f}_{IMAGE_SIZE[0]}',embed_val.astype('float32'))\n    \n    print(''); print('Training and inferring...'); print('')\n    model = cuml.neighbors.KNeighborsClassifier(n_neighbors=N_NEIGH)\n    model.fit(embed, np.array(train_labs))\n    print('Training and inference complete.'); print('')\n    \n    preds = model.predict_proba(embed_test)\n    preds_model.append(preds) \n    \n    acc = accuracy_score(model.predict(embed_val), np.array(val_labs))\n    print(f'Fold {f + 1} accuracy: {acc}'); print('')\n    \n    oof_labels.append([target.numpy() for img, target in iter(val_ds.unbatch())])\n    x_oof = val_ds.map(lambda image, image_name: image)\n    oof_pred.append(model.predict(embed_val))\n    \npreds_model = np.stack(preds_model).mean(0)\npreds_all.append(preds_model)\npreds_all = np.stack(preds_all)","e8d0c433":"y_true = np.concatenate(oof_labels)\ny_preds = np.concatenate(oof_pred)\n\nprint(classification_report(y_true, y_preds))\nprint(f\"OOF accuracy score: {accuracy_score(y_true, y_preds)}\")","036fdd64":"train_dummy = get_dataset(TRAINING_FILENAMES, labeled=True,\n                         return_image_names=True, repeat=False, \n                         shuffle=False)\nnames = np.array([img_name.numpy().decode(\"utf-8\") for img, label, img_name in iter(train_dummy.unbatch())])\nlabels = np.array([label.numpy() for img, label, img_name in iter(train_dummy.unbatch())])\ntrain_full = get_dataset(TRAINING_FILENAMES, labeled=True,\n                       return_image_names=False, repeat=False, \n                       shuffle=False)\nembed_full = effnet.predict(train_full, verbose=1)","1fdb1d62":"train = pd.DataFrame()\ntrain['image_id'] = names\ntrain['label'] = labels","5f49effa":"N_CLUSTERS = 5\nmodel = cuml.KMeans(n_clusters=N_CLUSTERS)\nmodel.fit(embed_full)\ntrain['cluster'] = model.labels_\ntrain.head()","f05971a7":"JPEG_TRAIN = '..\/input\/cassava-leaf-disease-classification\/train_images\/'\n\nfor k in range(N_CLUSTERS):\n    print('#'*25);\n    print(f'#### Cluster {k} of similar train images')\n    print('#'*25)\n    df = train.loc[train.cluster==k]\n    plt.figure(figsize=(20,10))\n    for j in range(8):\n        plt.subplot(2,4,j+1)\n        img = cv2.imread(JPEG_TRAIN+names[df.index[j]])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.axis('off')\n        plt.title(f\"{names[df.index[j]]}, Target = {df.loc[df.index[j],'label']}\")\n        plt.imshow(img)  \n    plt.show()","c90a33d8":"PERPLEXITY = 5","fdb79945":"model = cuml.TSNE(perplexity=PERPLEXITY)\nembed2D = model.fit_transform(embed_full)\ntrain['x'] = embed2D[:,0]\ntrain['y'] = embed2D[:,1]","85cd259e":"plt.figure(figsize=(10,10))\ndf1 = train.loc[train.label==0]\ndf2 = train.loc[train.label==1]\ndf3 = train.loc[train.label==2]\ndf4 = train.loc[train.label==3]\ndf5 = train.loc[train.label==4]\n\nplt.scatter(df1.x,df1.y,c='blue',s=10,label='0')\nplt.scatter(df2.x,df2.y,c='cornflowerblue',s=10,label='1')\nplt.scatter(df3.x,df3.y,c='purple',s=10,label='2')\nplt.scatter(df4.x,df4.y,c='steelblue',s=10,label='3')\nplt.scatter(df5.x,df5.y,c='mediumpurple',s=10,label='4')\nplt.legend();","bcc714f9":"print(preds_all.shape)\npreds_all","bf42c634":"submission[\"label\"] = preds_all.mean(0).argmax(1)\nsubmission.to_csv(\"submission.csv\", index=False)","03f556f9":"submission","f728d7ac":"# V. RAPIDS cuML T-SNE\n\n**We can of course also project these points in `1792` dimensional space to a 2 dimensional space to see how are samples are clustered. Forgive the ensuing mess of colors...**","9139bb16":"# VI. Submission","b56447eb":"# III. RAPIDS cuML kNN\n\n**Now we can extract image embeddings with an EfficientNet and use RAPIDS cuML kNN for quick GPU model training. The kNN training doesn't take long at all, it is the feature extraction that takes a while. You can save the embeddings and upload them later to make the below loop faster.**","b7e89031":"# II. Dataset Functions\n\n**Note that we are using `noisy-student` pretrained weights, so we need to normalize our inputs by subtracting ImageNet mean `.449`and dividing by standard deviation `.226`. We must do this because the EfficientNet is not being trained anymore: if it was, the models weights would correct themselves. You can experiment with training on Cassava data before extracting embeddings: you will get different features and perhaps they will do better than just ImageNet pretraining.**","c5670400":"# Cassava RAPIDS kNN\n\n**The RAPIDS suite from NVIDIA is a collection of libraries that allows you to perform end-to-end data science pipelines entirely on GPUs. Leading Kaggle solutions and higher performing models\/solutions rely on iterations; the more you can iterate and experiment, the more optimal solution you will find, so ensuring a fast pipeline is key. We can easily do this with RAPIDS libraries like `cuDF` and `cuML`. We will focus on the `cuML` library in this notebook, using `cuML KNN` for inference, and `cuML KMeans \/ T-SNE` to visualize how our data is clustered.**\n\n**We will extract image embeddings with EfficientNets and then use these CNN embeddings to train a RAPIDS cuML kNN to find similar images to see how kNN performs compared to more advanced models, like deep CNNs. This kernel is entirely motivated by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte)'s notebook [here](https:\/\/www.kaggle.com\/cdeotte\/rapids-cuml-knn-find-duplicates) from the Melanoma competition, in which he uses this technique to find duplicate images. Before giving this notebook an upvote (if you intend to), please give his one first.**\n\n**Note that I am using [Dimitre](https:\/\/www.kaggle.com\/dimitreoliveira)'s TFRecords that can be found [here](https:\/\/www.kaggle.com\/dimitreoliveira\/cassava-leaf-disease-tfrecords-512x512). He also has 128x128, 256x256, and 384x384 sized images that I added for experimental purposes. Please give his datasets an upvote (and his work in general, it is excellent).**","c27fd947":"# I. Configuration","c1ecc5d2":"# IV. RAPIDS cuML KMeans\n\n**The embeddings generated by the EfficientNetB4 are dimension `1792`, so each image is represented as a point in `1792` dimensional space. We can cluster these points together with a RAPIDS KMeans model and then view what the images in each cluster look like. For now, we will force `N_CLUSTERS == 5`.**"}}