{"cell_type":{"d9700e64":"code","b9045c9e":"code","34430b20":"code","da106a0b":"code","f19eb4d0":"code","843a935d":"code","3853b5ab":"code","a450832b":"code","64314d8b":"code","ab0dc338":"code","a506c1d6":"code","1436cc42":"code","59056b66":"code","b1a95c11":"code","6049565c":"code","ebc867b7":"markdown","6d03368f":"markdown","5f7c9274":"markdown","36326875":"markdown","5ca50778":"markdown","25a5cd4f":"markdown","1bf3eea1":"markdown","5751a882":"markdown","256bec9c":"markdown","59bf1d5b":"markdown","8e456ba9":"markdown","69c51981":"markdown","37e31fcf":"markdown","5a2aa799":"markdown","e28fa6b0":"markdown","530243c6":"markdown","87b68075":"markdown","7219a32b":"markdown","b12894e6":"markdown","2fd79feb":"markdown","9c3e39cb":"markdown","b0a72cf8":"markdown"},"source":{"d9700e64":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils import model_zoo\nfrom torchvision.ops import nms\nfrom torchvision import transforms\n\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport skimage\n\nimport re\nimport collections\nfrom functools import partial\nimport math\nimport pandas as pd\nimport numpy as np\nfrom timeit import default_timer as timer\nimport copy\nimport albumentations as albu\nfrom albumentations.pytorch.transforms import ToTensor\nfrom six.moves import map, zip","b9045c9e":"########################################################################\n############### HELPERS FUNCTIONS FOR MODEL ARCHITECTURE ###############\n########################################################################\n\n\n# Parameters for the entire model (stem, all blocks, and head)\nGlobalParams = collections.namedtuple('GlobalParams', [\n    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n    'num_classes', 'width_coefficient', 'depth_coefficient',\n    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n\n# Parameters for an individual model block\nBlockArgs = collections.namedtuple('BlockArgs', [\n    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n    'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n\n# Change namedtuple defaults\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\nclass SwishImplementation(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = torch.sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\nclass MemoryEfficientSwish(nn.Module):\n    def forward(self, x):\n        return SwishImplementation.apply(x)\n\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\ndef round_filters(filters, global_params):\n    \"\"\" Calculate and round number of filters based on depth multiplier. \"\"\"\n    multiplier = global_params.width_coefficient\n    if not multiplier:\n        return filters\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(filters + divisor \/ 2) \/\/ divisor * divisor)\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef drop_connect(inputs, p, training):\n    \"\"\" Drop connect. \"\"\"\n    if not training: return inputs\n    batch_size = inputs.shape[0]\n    keep_prob = 1 - p\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n    binary_tensor = torch.floor(random_tensor)\n    output = inputs \/ keep_prob * binary_tensor\n    return output\n\n\ndef get_same_padding_conv2d(image_size=None):\n    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n        Static padding is necessary for ONNX exporting of models. \"\"\"\n    if image_size is None:\n        return Conv2dDynamicSamePadding\n    else:\n        return partial(Conv2dStaticSamePadding, image_size=image_size)\n\n\nclass Conv2dDynamicSamePadding(nn.Conv2d):\n    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n    def forward(self, x):\n        ih, iw = x.size()[-2:]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih \/ sh), math.ceil(iw \/ sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            x = F.pad(x, [pad_w \/\/ 2, pad_w - pad_w \/\/ 2, pad_h \/\/ 2, pad_h - pad_h \/\/ 2])\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass Conv2dStaticSamePadding(nn.Conv2d):\n    \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n        # Calculate padding based on image size and save it\n        assert image_size is not None\n        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih \/ sh), math.ceil(iw \/ sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            self.static_padding = nn.ZeroPad2d((pad_w \/\/ 2, pad_w - pad_w \/\/ 2, pad_h \/\/ 2, pad_h - pad_h \/\/ 2))\n        else:\n            self.static_padding = Identity()\n\n    def forward(self, x):\n        x = self.static_padding(x)\n        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n        return x\n\n\nclass Identity(nn.Module):\n    def __init__(self, ):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n\n\n########################################################################\n############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n########################################################################\n\n\ndef efficientnet_params(model_name):\n    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n    params_dict = {\n        # Coefficients:   width,depth,res,dropout\n        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n    }\n    return params_dict[model_name]\n\n\nclass BlockDecoder(object):\n    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n\n    @staticmethod\n    def _decode_block_string(block_string):\n        \"\"\" Gets a block through a string notation of arguments. \"\"\"\n        assert isinstance(block_string, str)\n\n        ops = block_string.split('_')\n        options = {}\n        for op in ops:\n            splits = re.split(r'(\\d.*)', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        # Check stride\n        assert (('s' in options and len(options['s']) == 1) or\n                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n\n        return BlockArgs(\n            kernel_size=int(options['k']),\n            num_repeat=int(options['r']),\n            input_filters=int(options['i']),\n            output_filters=int(options['o']),\n            expand_ratio=int(options['e']),\n            id_skip=('noskip' not in block_string),\n            se_ratio=float(options['se']) if 'se' in options else None,\n            stride=[int(options['s'][0])])\n\n    @staticmethod\n    def _encode_block_string(block):\n        \"\"\"Encodes a block to a string.\"\"\"\n        args = [\n            'r%d' % block.num_repeat,\n            'k%d' % block.kernel_size,\n            's%d%d' % (block.strides[0], block.strides[1]),\n            'e%s' % block.expand_ratio,\n            'i%d' % block.input_filters,\n            'o%d' % block.output_filters\n        ]\n        if 0 < block.se_ratio <= 1:\n            args.append('se%s' % block.se_ratio)\n        if block.id_skip is False:\n            args.append('noskip')\n        return '_'.join(args)\n\n    @staticmethod\n    def decode(string_list):\n        \"\"\"\n        Decodes a list of string notations to specify blocks inside the network.\n        :param string_list: a list of strings, each string is a notation of block\n        :return: a list of BlockArgs namedtuples of block args\n        \"\"\"\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n        return blocks_args\n\n    @staticmethod\n    def encode(blocks_args):\n        \"\"\"\n        Encodes a list of BlockArgs to a list of strings.\n        :param blocks_args: a list of BlockArgs namedtuples of block args\n        :return: a list of strings, each string is a notation of block\n        \"\"\"\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(BlockDecoder._encode_block_string(block))\n        return block_strings\n\n\ndef efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n    \"\"\" Creates a efficientnet model. \"\"\"\n\n    blocks_args = [\n        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n        'r3_k5_s22_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n        'r1_k3_s22_e6_i192_o320_se0.25',\n    ]\n    blocks_args = BlockDecoder.decode(blocks_args)\n\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=drop_connect_rate,\n        # data_format='channels_last',  # removed, this is always true in PyTorch\n        num_classes=num_classes,\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        depth_divisor=8,\n        min_depth=None,\n        image_size=image_size,\n    )\n\n    return blocks_args, global_params\n\n\ndef get_model_params(model_name, override_params):\n    \"\"\" Get the block args and global params for a given model \"\"\"\n    if model_name.startswith('efficientnet'):\n        w, d, s, p = efficientnet_params(model_name)\n        # note: all models have drop connect rate = 0.2\n        blocks_args, global_params = efficientnet(\n            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n    else:\n        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n    if override_params:\n        # ValueError will be raised here if override_params has fields not included in global_params.\n        global_params = global_params._replace(**override_params)\n    return blocks_args, global_params\n\n\nurl_map = {\n    'efficientnet-b0': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet\/efficientnet-b0-355c32eb.pth',\n    'efficientnet-b1': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet\/efficientnet-b1-f1951068.pth',\n    'efficientnet-b2': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet\/efficientnet-b2-8bb594d6.pth',\n    'efficientnet-b3': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet\/efficientnet-b3-5fb5a3c3.pth',\n    'efficientnet-b4': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet\/efficientnet-b4-6ed6700e.pth',\n    'efficientnet-b5': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet\/efficientnet-b5-b6417697.pth',\n    'efficientnet-b6': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet\/efficientnet-b6-c76e70fd.pth',\n    'efficientnet-b7': 'http:\/\/storage.googleapis.com\/public-models\/efficientnet\/efficientnet-b7-dcc49843.pth',\n}\n\n\ndef load_pretrained_weights(model, model_name, load_fc=True):\n    \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n    state_dict = model_zoo.load_url(url_map[model_name])\n    if load_fc:\n        model.load_state_dict(state_dict)\n    else:\n        state_dict.pop('_fc.weight')\n        state_dict.pop('_fc.bias')\n        res = model.load_state_dict(state_dict, strict=False)\n        assert set(res.missing_keys) == set(['_fc.weight', '_fc.bias']), 'issue loading pretrained weights'\n    print('Loaded pretrained weights for {}'.format(model_name))","34430b20":"class MBConvBlock(nn.Module):\n    \"\"\"\n    Mobile Inverted Residual Bottleneck Block\n    Args:\n        block_args (namedtuple): BlockArgs, see above\n        global_params (namedtuple): GlobalParam, see above\n    Attributes:\n        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n    \"\"\"\n\n    def __init__(self, block_args, global_params):\n        super().__init__()\n        self._block_args = block_args\n        self._bn_mom = 1 - global_params.batch_norm_momentum\n        self._bn_eps = global_params.batch_norm_epsilon\n        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n        self.id_skip = block_args.id_skip  # skip connection and drop connect\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Expansion phase\n        inp = self._block_args.input_filters  # number of input channels\n        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n        if self._block_args.expand_ratio != 1:\n            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n        # Depthwise convolution phase\n        k = self._block_args.kernel_size\n        s = self._block_args.stride\n        self._depthwise_conv = Conv2d(\n            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n            kernel_size=k, stride=s, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Squeeze and Excitation layer, if desired\n        if self.has_se:\n            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n\n        # Output phase\n        final_oup = self._block_args.output_filters\n        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n        self._swish = MemoryEfficientSwish()\n\n    def forward(self, inputs, drop_connect_rate=None):\n        \"\"\"\n        :param inputs: input tensor\n        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n        :return: output of block\n        \"\"\"\n\n        # Expansion and Depthwise Convolution\n        x = inputs\n        if self._block_args.expand_ratio != 1:\n            x = self._swish(self._bn0(self._expand_conv(inputs)))\n        \n        x = self._swish(self._bn1(self._depthwise_conv(x)))\n\n        # Squeeze and Excitation\n        if self.has_se:\n            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            x_squeezed = self._se_expand(self._swish(self._se_reduce(x_squeezed)))\n            x = torch.sigmoid(x_squeezed) * x\n\n        x = self._bn2(self._project_conv(x))\n\n        # Skip connection and drop connect\n        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n            if drop_connect_rate:\n                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n            x = x + inputs  # skip connection\n        return x\n\n    def set_swish(self, memory_efficient=True):\n        \"\"\"Sets swish function as memory efficient (for training) or standard (for export)\"\"\"\n        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n\n\nclass EfficientNet(nn.Module):\n    \"\"\"\n    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n    Args:\n        blocks_args (list): A list of BlockArgs to construct blocks\n        global_params (namedtuple): A set of GlobalParams shared between blocks\n    Example:\n        model = EfficientNet.from_pretrained('efficientnet-b0')\n    \"\"\"\n\n    def __init__(self, blocks_args=None, global_params=None):\n        super().__init__()\n        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n        assert len(blocks_args) > 0, 'block args must be greater than 0'\n        self._global_params = global_params\n        self._blocks_args = blocks_args\n\n        # Get static or dynamic convolution depending on image size\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        # Batch norm parameters\n        bn_mom = 1 - self._global_params.batch_norm_momentum\n        bn_eps = self._global_params.batch_norm_epsilon\n\n        # Stem\n        in_channels = 3  # rgb\n        out_channels = round_filters(32, self._global_params)  # number of output channels\n        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n        \n        # Build blocks\n        self._blocks = nn.ModuleList([])\n        for i in range(len(self._blocks_args)):\n            # Update block input and output filters based on depth multiplier.\n            self._blocks_args[i] = self._blocks_args[i]._replace(\n                input_filters=round_filters(self._blocks_args[i].input_filters, self._global_params),\n                output_filters=round_filters(self._blocks_args[i].output_filters, self._global_params),\n                num_repeat=round_repeats(self._blocks_args[i].num_repeat, self._global_params)\n            )\n\n            # The first block needs to take care of stride and filter size increase.\n            self._blocks.append(MBConvBlock(self._blocks_args[i], self._global_params))\n            if self._blocks_args[i].num_repeat > 1:\n                self._blocks_args[i] = self._blocks_args[i]._replace(input_filters=self._blocks_args[i].output_filters, stride=1)\n            for _ in range(self._blocks_args[i].num_repeat - 1):\n                self._blocks.append(MBConvBlock(self._blocks_args[i], self._global_params))\n\n        # Head'efficientdet-d0': 'efficientnet-b0',\n        in_channels = self._blocks_args[len(self._blocks_args)-1].output_filters  # output of final block\n        out_channels = round_filters(1280, self._global_params)\n        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # Final linear layer\n        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n        self._dropout = nn.Dropout(self._global_params.dropout_rate)\n        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n        self._swish = MemoryEfficientSwish()\n\n    def set_swish(self, memory_efficient=True):\n        \"\"\"Sets swish function as memory efficient (for training) or standard (for export)\"\"\"\n        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n        for block in self._blocks:\n            block.set_swish(memory_efficient)\n\n\n    def extract_features(self, inputs):\n        \"\"\" Returns output of the final convolution layer \"\"\"\n        # Stem\n        x = self._swish(self._bn0(self._conv_stem(inputs)))\n\n        P = []\n        index = 0 \n        num_repeat = 0\n         # Blocks\n        for idx, block in enumerate(self._blocks):\n            drop_connect_rate = self._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) \/ len(self._blocks)\n            x = block(x, drop_connect_rate=drop_connect_rate)\n            num_repeat = num_repeat + 1\n            if(num_repeat == self._blocks_args[index].num_repeat):\n                num_repeat = 0\n                index = index + 1\n                P.append(x)\n        return P\n\n    def forward(self, inputs):\n        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n        # Convolution layers\n        P = self.extract_features(inputs)\n        return P\n    \n    @classmethod\n    def from_name(cls, model_name, override_params=None):\n        cls._check_model_name_is_valid(model_name)\n        blocks_args, global_params = get_model_params(model_name, override_params)\n        return cls(blocks_args, global_params)\n\n    @classmethod\n    def from_pretrained(cls, model_name, num_classes=1000, in_channels = 3):\n        model = cls.from_name(model_name, override_params={'num_classes': num_classes})\n        load_pretrained_weights(model, model_name, load_fc=(num_classes == 1000))\n        if in_channels != 3:\n            Conv2d = get_same_padding_conv2d(image_size = model._global_params.image_size)\n            out_channels = round_filters(32, model._global_params)\n            model._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        return model\n    \n    @classmethod\n    def from_pretrained(cls, model_name, num_classes=1000):\n        model = cls.from_name(model_name, override_params={'num_classes': num_classes})\n        load_pretrained_weights(model, model_name, load_fc=(num_classes == 1000))\n\n        return model\n\n    @classmethod\n    def get_image_size(cls, model_name):\n        cls._check_model_name_is_valid(model_name)\n        _, _, res, _ = efficientnet_params(model_name)\n        return res\n\n    @classmethod\n    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n        \"\"\" Validates model name. None that pretrained weights are only available for\n        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. \"\"\"\n        num_models = 4 if also_need_pretrained_weights else 8\n        valid_models = ['efficientnet-b'+str(i) for i in range(num_models)]\n        if model_name not in valid_models:\n            raise ValueError('model_name should be one of: ' + ', '.join(valid_models))\n    \n    def get_list_features(self):\n        list_feature = []\n        for idx in range(len(self._blocks_args)):\n            list_feature.append(self._blocks_args[idx].output_filters)\n        \n        return list_feature","da106a0b":"def conv_ws_2d(input,\n               weight,\n               bias=None,\n               stride=1,\n               padding=0,\n               dilation=1,\n               groups=1,\n               eps=1e-5):\n    c_in = weight.size(0)\n    weight_flat = weight.view(c_in, -1)\n    mean = weight_flat.mean(dim=1, keepdim=True).view(c_in, 1, 1, 1)\n    std = weight_flat.std(dim=1, keepdim=True).view(c_in, 1, 1, 1)\n    weight = (weight - mean) \/ (std + eps)\n    return F.conv2d(input, weight, bias, stride, padding, dilation, groups)\n\nclass ConvWS2d(nn.Conv2d):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=True,\n                 eps=1e-5):\n        super(ConvWS2d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias)\n        self.eps = eps\n\n    def forward(self, x):\n        return conv_ws_2d(x, self.weight, self.bias, self.stride, self.padding,\n                          self.dilation, self.groups, self.eps)\nconv_cfg = {\n    'Conv': nn.Conv2d,\n    'ConvWS': ConvWS2d,\n    # TODO: octave conv\n}\n\ndef build_conv_layer(cfg, *args, **kwargs):\n    \"\"\" Build convolution layer\n    Args:\n        cfg (None or dict): cfg should contain:\n            type (str): identify conv layer type.\n            layer args: args needed to instantiate a conv layer.\n    Returns:\n        layer (nn.Module): created conv layer\n    \"\"\"\n    if cfg is None:\n        cfg_ = dict(type='Conv')\n    else:\n        assert isinstance(cfg, dict) and 'type' in cfg\n        cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop('type')\n    if layer_type not in conv_cfg:\n        raise KeyError('Unrecognized norm type {}'.format(layer_type))\n    else:\n        conv_layer = conv_cfg[layer_type]\n\n    layer = conv_layer(*args, **kwargs, **cfg_)\n\n    return layer\n\nnorm_cfg = {\n    # format: layer_type: (abbreviation, module)\n    'BN': ('bn', nn.BatchNorm2d),\n    'SyncBN': ('bn', nn.SyncBatchNorm),\n    'GN': ('gn', nn.GroupNorm),\n    # and potentially 'SN'\n}\n\ndef build_norm_layer(cfg, num_features, postfix=''):\n    \"\"\" Build normalization layer\n    Args:\n        cfg (dict): cfg should contain:\n            type (str): identify norm layer type.\n            layer args: args needed to instantiate a norm layer.\n            requires_grad (bool): [optional] whether stop gradient updates\n        num_features (int): number of channels from input.\n        postfix (int, str): appended into norm abbreviation to\n            create named layer.\n    Returns:\n        name (str): abbreviation + postfix\n        layer (nn.Module): created norm layer\n    \"\"\"\n    assert isinstance(cfg, dict) and 'type' in cfg\n    cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop('type')\n    if layer_type not in norm_cfg:\n        raise KeyError('Unrecognized norm type {}'.format(layer_type))\n    else:\n        abbr, norm_layer = norm_cfg[layer_type]\n        if norm_layer is None:\n            raise NotImplementedError\n\n    assert isinstance(postfix, (int, str))\n    name = abbr + str(postfix)\n\n    requires_grad = cfg_.pop('requires_grad', True)\n    cfg_.setdefault('eps', 1e-5)\n    if layer_type != 'GN':\n        layer = norm_layer(num_features, **cfg_)\n        if layer_type == 'SyncBN':\n            layer._specify_ddp_gpu_num(1)\n    else:\n        assert 'num_groups' in cfg_\n        layer = norm_layer(num_channels=num_features, **cfg_)\n\n    for param in layer.parameters():\n        param.requires_grad = requires_grad\n\n    return name, layer\n\nclass ConvModule(nn.Module):\n    \"\"\"A conv block that contains conv\/norm\/activation layers.\n    Args:\n        in_channels (int): Same as nn.Conv2d.\n        out_channels (int): Same as nn.Conv2d.\n        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n        stride (int or tuple[int]): Same as nn.Conv2d.\n        padding (int or tuple[int]): Same as nn.Conv2d.\n        dilation (int or tuple[int]): Same as nn.Conv2d.\n        groups (int): Same as nn.Conv2d.\n        bias (bool or str): If specified as `auto`, it will be decided by the\n            norm_cfg. Bias will be set as True if norm_cfg is None, otherwise\n            False.\n        conv_cfg (dict): Config dict for convolution layer.\n        norm_cfg (dict): Config dict for normalization layer.\n        activation (str or None): Activation type, \"ReLU\" by default.\n        inplace (bool): Whether to use inplace mode for activation.\n        order (tuple[str]): The order of conv\/norm\/activation layers. It is a\n            sequence of \"conv\", \"norm\" and \"act\". Examples are\n            (\"conv\", \"norm\", \"act\") and (\"act\", \"conv\", \"norm\").\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias='auto',\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 activation='relu',\n                 inplace=True,\n                 order=('conv', 'norm', 'act')):\n        super(ConvModule, self).__init__()\n        assert conv_cfg is None or isinstance(conv_cfg, dict)\n        assert norm_cfg is None or isinstance(norm_cfg, dict)\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.activation = activation\n        self.inplace = inplace\n        self.order = order\n        assert isinstance(self.order, tuple) and len(self.order) == 3\n        assert set(order) == set(['conv', 'norm', 'act'])\n\n        self.with_norm = norm_cfg is not None\n        self.with_activatation = activation is not None\n        # if the conv layer is before a norm layer, bias is unnecessary.\n        if bias == 'auto':\n            bias = False if self.with_norm else True\n        self.with_bias = bias\n\n        if self.with_norm and self.with_bias:\n            warnings.warn('ConvModule has norm and bias at the same time')\n\n        # build convolution layer\n        self.conv = build_conv_layer(\n            conv_cfg,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias)\n        # export the attributes of self.conv to a higher level for convenience\n        self.in_channels = self.conv.in_channels\n        self.out_channels = self.conv.out_channels\n        self.kernel_size = self.conv.kernel_size\n        self.stride = self.conv.stride\n        self.padding = self.conv.padding\n        self.dilation = self.conv.dilation\n        self.transposed = self.conv.transposed\n        self.output_padding = self.conv.output_padding\n        self.groups = self.conv.groups\n\n        # build normalization layers\n        if self.with_norm:\n            # norm layer is after conv layer\n            if order.index('norm') > order.index('conv'):\n                norm_channels = out_channels\n            else:\n                norm_channels = in_channels\n            self.norm_name, norm = build_norm_layer(norm_cfg, norm_channels)\n            self.add_module(self.norm_name, norm)\n\n        # build activation layer\n        if self.with_activatation:\n            # TODO: introduce `act_cfg` and supports more activation layers\n            if self.activation not in ['relu']:\n                raise ValueError('{} is currently not supported.'.format(\n                    self.activation))\n            if self.activation == 'relu':\n                self.activate = nn.ReLU(inplace=inplace)\n    @property\n    def norm(self):\n        return getattr(self, self.norm_name)\n    def forward(self, x, activate=True, norm=True):\n        for layer in self.order:\n            if layer == 'conv':\n                x = self.conv(x)\n            elif layer == 'norm' and norm and self.with_norm:\n                x = self.norm(x)\n            elif layer == 'act' and activate and self.with_activatation:\n                x = self.activate(x)\n        return x\n\nimport numpy as np\nimport torch.nn as nn\n\n\ndef xavier_init(module, gain=1, bias=0, distribution='normal'):\n    assert distribution in ['uniform', 'normal']\n    if distribution == 'uniform':\n        nn.init.xavier_uniform_(module.weight, gain=gain)\n    else:\n        nn.init.xavier_normal_(module.weight, gain=gain)\n    if hasattr(module, 'bias'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef normal_init(module, mean=0, std=1, bias=0):\n    nn.init.normal_(module.weight, mean, std)\n    if hasattr(module, 'bias'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef uniform_init(module, a=0, b=1, bias=0):\n    nn.init.uniform_(module.weight, a, b)\n    if hasattr(module, 'bias'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef kaiming_init(module,\n                 mode='fan_out',\n                 nonlinearity='relu',\n                 bias=0,\n                 distribution='normal'):\n    assert distribution in ['uniform', 'normal']\n    if distribution == 'uniform':\n        nn.init.kaiming_uniform_(\n            module.weight, mode=mode, nonlinearity=nonlinearity)\n    else:\n        nn.init.kaiming_normal_(\n            module.weight, mode=mode, nonlinearity=nonlinearity)\n    if hasattr(module, 'bias'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef bias_init_with_prob(prior_prob):\n    \"\"\" initialize conv\/fc bias value according to giving probablity\"\"\"\n    bias_init = float(-np.log((1 - prior_prob) \/ prior_prob))\n    return bias_init","f19eb4d0":"class BIFPN(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=0,\n                 end_level=-1,\n                 stack=1,\n                 add_extra_convs=False,\n                 extra_convs_on_inputs=True,\n                 relu_before_extra_convs=False,\n                 no_norm_on_lateral=False,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 activation=None):\n        super(BIFPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.activation = activation\n        self.relu_before_extra_convs = relu_before_extra_convs\n        self.no_norm_on_lateral = no_norm_on_lateral\n        self.stack = stack\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n        self.extra_convs_on_inputs = extra_convs_on_inputs\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        self.stack_bifpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n                activation=self.activation,\n                inplace=False)\n            self.lateral_convs.append(l_conv)\n\n        for ii in range(stack):\n            self.stack_bifpn_convs.append(BiFPNModule(channels=out_channels,\n                                                      levels=self.backbone_end_level-self.start_level,\n                                                      conv_cfg=conv_cfg,\n                                                      norm_cfg=norm_cfg,\n                                                      activation=activation))\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                if i == 0 and self.extra_convs_on_inputs:\n                    in_channels = self.in_channels[self.backbone_end_level - 1]\n                else:\n                    in_channels = out_channels\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    activation=self.activation,\n                    inplace=False)\n                self.fpn_convs.append(extra_fpn_conv)\n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n    def forward(self, inputs):\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n        \n        # part 1: build top-down and down-top path with stack\n        used_backbone_levels = len(laterals)\n        for bifpn_module in self.stack_bifpn_convs:\n            laterals = bifpn_module(laterals)\n        outs = laterals\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.extra_convs_on_inputs:\n                    orig = inputs[self.backbone_end_level - 1]\n                    outs.append(self.fpn_convs[0](orig))\n                else:\n                    outs.append(self.fpn_convs[0](outs[-1]))\n                for i in range(1, self.num_outs - used_backbone_levels):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)\n\n\nclass BiFPNModule(nn.Module):\n    def __init__(self,\n                 channels,\n                 levels,\n                 init=0.5,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 activation=None,\n                 eps = 0.0001):\n        super(BiFPNModule, self).__init__()\n        self.activation = activation\n        self.eps = eps \n        self.levels = levels\n        self.bifpn_convs = nn.ModuleList()\n        # weighted\n        self.w1 = nn.Parameter(torch.Tensor(2, levels).fill_(init))\n        self.relu1 = nn.ReLU()\n        self.w2 = nn.Parameter(torch.Tensor(3, levels - 2).fill_(init))\n        self.relu2 = nn.ReLU()\n        for jj in range(2):\n            for i in range(self.levels-1):  # 1,2,3\n                fpn_conv = nn.Sequential(\n                    ConvModule(\n                        channels,\n                        channels,\n                        3,\n                        padding=1,\n                        conv_cfg=conv_cfg,\n                        norm_cfg=norm_cfg,\n                        activation=self.activation,\n                        inplace=False)\n                        )\n                self.bifpn_convs.append(fpn_conv)\n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n    def forward(self, inputs):\n        assert len(inputs) == self.levels\n        # build top-down and down-top path with stack\n        levels = self.levels\n        # w relu\n        w1 = self.relu1(self.w1)\n        w1 \/= torch.sum(w1, dim=0) + self.eps  # normalize\n        w2 = self.relu2(self.w2)\n        w2 \/= torch.sum(w2, dim=0) + self.eps # normalize \n        # build top-down\n        idx_bifpn = 0\n        pathtd = inputs\n        inputs_clone = []\n        for in_tensor in inputs:\n            inputs_clone.append(in_tensor.clone())\n        \n        for i in range(levels - 1, 0, -1):\n            pathtd[i - 1] = (w1[0, i-1]*pathtd[i - 1] + w1[1, i-1]*F.interpolate(pathtd[i], scale_factor=2, mode='nearest'))\/(w1[0, i-1] + w1[1, i-1] + self.eps)\n            pathtd[i - 1] = self.bifpn_convs[idx_bifpn](pathtd[i - 1])\n            idx_bifpn = idx_bifpn + 1\n        # build down-top\n        for i in range(0, levels - 2, 1):\n            pathtd[i + 1] = (w2[0, i] * pathtd[i + 1] + w2[1, i] * F.max_pool2d(pathtd[i], kernel_size=2) + w2[2, i] * inputs_clone[i + 1])\/(w2[0, i] + w2[1, i] + w2[2, i] + self.eps)\n            pathtd[i + 1] = self.bifpn_convs[idx_bifpn](pathtd[i + 1])\n            idx_bifpn = idx_bifpn + 1\n\n        pathtd[levels - 1] = (w1[0, levels-1] * pathtd[levels - 1] + w1[1, levels-1] * F.max_pool2d(pathtd[levels - 2], kernel_size=2))\/(w1[0, levels-1] + w1[1, levels-1] + self.eps)\n        pathtd[levels - 1] = self.bifpn_convs[idx_bifpn](pathtd[levels - 1])\n        return pathtd","843a935d":"def multi_apply(func, *args, **kwargs):\n    pfunc = partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))\n\nclass RetinaHead(nn.Module):\n    \"\"\"\n    An anchor-based head used in [1]_.\n    The head contains two subnetworks. The first classifies anchor boxes and\n    the second regresses deltas for the anchors.\n    References:\n        .. [1]  https:\/\/arxiv.org\/pdf\/1708.02002.pdf\n    Example:\n        >>> import torch\n        >>> self = RetinaHead(11, 7)\n        >>> x = torch.rand(1, 7, 32, 32)\n        >>> cls_score, bbox_pred = self.forward_single(x)\n        >>> # Each anchor predicts a score for each class except background\n        >>> cls_per_anchor = cls_score.shape[1] \/ self.num_anchors\n        >>> box_per_anchor = bbox_pred.shape[1] \/ self.num_anchors\n        >>> assert cls_per_anchor == (self.num_classes - 1)\n        >>> assert box_per_anchor == 4\n    \"\"\"\n\n    def __init__(self,\n                num_classes,\n                 in_channels,\n                 feat_channels=256,\n                 anchor_scales=[8, 16, 32],\n                 anchor_ratios=[0.5, 1.0, 2.0],\n                 anchor_strides=[4, 8, 16, 32, 64],\n                 stacked_convs=4,\n                 octave_base_scale=4,\n                 scales_per_octave=3,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 **kwargs):\n        super(RetinaHead, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.feat_channels = feat_channels\n        self.anchor_scales = anchor_scales\n        self.anchor_ratios = anchor_ratios\n        self.anchor_strides = anchor_strides\n        self.stacked_convs = stacked_convs\n        self.octave_base_scale = octave_base_scale\n        self.scales_per_octave = scales_per_octave\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        octave_scales = np.array(\n            [2**(i \/ scales_per_octave) for i in range(scales_per_octave)])\n        anchor_scales = octave_scales * octave_base_scale\n        self.cls_out_channels = num_classes\n        self.num_anchors = len(self.anchor_ratios) * len(self.anchor_scales)\n        self._init_layers()\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n            self.reg_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n        self.retina_cls = nn.Conv2d(\n            self.feat_channels,\n            self.num_anchors * self.cls_out_channels,\n            3,\n            padding=1)\n        self.retina_reg = nn.Conv2d(\n            self.feat_channels, self.num_anchors * 4, 3, padding=1)\n        self.output_act = nn.Sigmoid()\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.retina_cls, std=0.01, bias=bias_cls)\n        normal_init(self.retina_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_feat = x\n        reg_feat = x\n        for cls_conv in self.cls_convs:\n            cls_feat = cls_conv(cls_feat)\n        for reg_conv in self.reg_convs:\n            reg_feat = reg_conv(reg_feat)\n        \n        cls_score = self.retina_cls(cls_feat)\n        cls_score = self.output_act(cls_score)\n        # out is B x C x W x H, with C = n_classes + n_anchors\n        cls_score = cls_score.permute(0, 2, 3, 1)\n        batch_size, width, height, channels = cls_score.shape\n        cls_score = cls_score.view(batch_size, width, height, self.num_anchors, self.num_classes)\n        cls_score = cls_score.contiguous().view(x.size(0), -1, self.num_classes)\n\n\n        bbox_pred = self.retina_reg(reg_feat)\n        bbox_pred = bbox_pred.permute(0, 2, 3, 1)\n        bbox_pred = bbox_pred.contiguous().view(bbox_pred.size(0), -1, 4)\n        return cls_score, bbox_pred\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)","3853b5ab":"class BBoxTransform(nn.Module):\n    \n    def __init__(self, mean=None, std=None):\n        super(BBoxTransform, self).__init__()\n        if mean is None:\n            self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32))\n        else:\n            self.mean = mean\n        if std is None:\n            self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32))\n        else:\n            self.std = std\n\n    def forward(self, boxes, deltas):\n\n        widths  = boxes[:, :, 2] - boxes[:, :, 0]\n        heights = boxes[:, :, 3] - boxes[:, :, 1]\n        ctr_x   = boxes[:, :, 0] + 0.5 * widths\n        ctr_y   = boxes[:, :, 1] + 0.5 * heights\n\n        dx = deltas[:, :, 0] * self.std[0] + self.mean[0]\n        dy = deltas[:, :, 1] * self.std[1] + self.mean[1]\n        dw = deltas[:, :, 2] * self.std[2] + self.mean[2]\n        dh = deltas[:, :, 3] * self.std[3] + self.mean[3]\n\n        pred_ctr_x = ctr_x + dx * widths\n        pred_ctr_y = ctr_y + dy * heights\n        pred_w     = torch.exp(dw) * widths\n        pred_h     = torch.exp(dh) * heights\n\n        pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n        pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n        pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w\n        pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h\n\n        pred_boxes = torch.stack([pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)\n\n        return pred_boxes\n\nclass ClipBoxes(nn.Module):\n\n    def __init__(self, width=None, height=None):\n        super(ClipBoxes, self).__init__()\n\n    def forward(self, boxes, img):\n\n        batch_size, num_channels, height, width = img.shape\n\n        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n\n        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)\n        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)\n      \n        return boxes\n\nclass RegressionModel(nn.Module):\n    def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n        super(RegressionModel, self).__init__()\n        \n        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n        self.act1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act2 = nn.ReLU()\n        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act3 = nn.ReLU()\n        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act4 = nn.ReLU()\n        self.output = nn.Conv2d(feature_size, num_anchors*4, kernel_size=3, padding=1)\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.act1(out)\n        out = self.conv2(out)\n        out = self.act2(out)\n        out = self.conv3(out)\n        out = self.act3(out)\n        out = self.conv4(out)\n        out = self.act4(out)\n        out = self.output(out)\n        # out is B x C x W x H, with C = 4*num_anchors\n        out = out.permute(0, 2, 3, 1)\n        return out.contiguous().view(out.shape[0], -1, 4)\n\nclass ClassificationModel(nn.Module):\n    def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256):\n        super(ClassificationModel, self).__init__()\n        self.num_classes = num_classes\n        self.num_anchors = num_anchors\n        \n        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n        self.act1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act2 = nn.ReLU()\n        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act3 = nn.ReLU()\n        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act4 = nn.ReLU()\n        self.output = nn.Conv2d(feature_size, num_anchors*num_classes, kernel_size=3, padding=1)\n        self.output_act = nn.Sigmoid()\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.act1(out)\n        out = self.conv2(out)\n        out = self.act2(out)\n        out = self.conv3(out)\n        out = self.act3(out)\n        out = self.conv4(out)\n        out = self.act4(out)\n        out = self.output(out)\n        out = self.output_act(out)\n        # out is B x C x W x H, with C = n_classes + n_anchors\n        out1 = out.permute(0, 2, 3, 1)\n        batch_size, width, height, channels = out1.shape\n        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)\n        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n\nclass Anchors(nn.Module):\n    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n        super(Anchors, self).__init__()\n\n        if pyramid_levels is None:\n            self.pyramid_levels = [3, 4, 5, 6, 7]\n        if strides is None:\n            self.strides = [2 ** x for x in self.pyramid_levels]\n        if sizes is None:\n            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n        if ratios is None:\n            self.ratios = np.array([0.5, 1, 2])\n        if scales is None:\n            self.scales = np.array([2 ** 0, 2 ** (1.0 \/ 3.0), 2 ** (2.0 \/ 3.0)])\n\n    def forward(self, image):\n        \n        image_shape = image.shape[2:]\n        image_shape = np.array(image_shape)\n        image_shapes = [(image_shape + 2 ** x - 1) \/\/ (2 ** x) for x in self.pyramid_levels]\n\n        # compute anchors over all pyramid levels\n        all_anchors = np.zeros((0, 4)).astype(np.float32)\n\n        for idx, p in enumerate(self.pyramid_levels):\n            anchors         = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n            all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n\n        all_anchors = np.expand_dims(all_anchors, axis=0)\n\n        return torch.from_numpy(all_anchors.astype(np.float32)).to(image.device)\n\ndef generate_anchors(base_size=16, ratios=None, scales=None):\n    \"\"\"\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales w.r.t. a reference window.\n    \"\"\"\n\n    if ratios is None:\n        ratios = np.array([0.5, 1, 2])\n\n    if scales is None:\n        scales = np.array([2 ** 0, 2 ** (1.0 \/ 3.0), 2 ** (2.0 \/ 3.0)])\n\n    num_anchors = len(ratios) * len(scales)\n\n    # initialize output anchors\n    anchors = np.zeros((num_anchors, 4))\n\n    # scale base_size\n    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n\n    # compute areas of anchors\n    areas = anchors[:, 2] * anchors[:, 3]\n\n    # correct for ratios\n    anchors[:, 2] = np.sqrt(areas \/ np.repeat(ratios, len(scales)))\n    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n\n    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n\n    return anchors\n\ndef compute_shape(image_shape, pyramid_levels):\n    \"\"\"Compute shapes based on pyramid levels.\n    :param image_shape:\n    :param pyramid_levels:\n    :return:\n    \"\"\"\n    image_shape = np.array(image_shape[:2])\n    image_shapes = [(image_shape + 2 ** x - 1) \/\/ (2 ** x) for x in pyramid_levels]\n    return image_shapes\n\ndef anchors_for_shape(\n    image_shape,\n    pyramid_levels=None,\n    ratios=None,\n    scales=None,\n    strides=None,\n    sizes=None,\n    shapes_callback=None,\n):\n\n    image_shapes = compute_shape(image_shape, pyramid_levels)\n\n    # compute anchors over all pyramid levels\n    all_anchors = np.zeros((0, 4))\n    for idx, p in enumerate(pyramid_levels):\n        anchors         = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales)\n        shifted_anchors = shift(image_shapes[idx], strides[idx], anchors)\n        all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n\n    return all_anchors\n\n\ndef shift(shape, stride, anchors):\n    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n\n    shifts = np.vstack((\n        shift_x.ravel(), shift_y.ravel(),\n        shift_x.ravel(), shift_y.ravel()\n    )).transpose()\n\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = anchors.shape[0]\n    K = shifts.shape[0]\n    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n    all_anchors = all_anchors.reshape((K * A, 4))\n\n    return all_anchors","a450832b":"MODEL_MAP = {\n    'efficientdet-d0': 'efficientnet-b0',\n    'efficientdet-d1': 'efficientnet-b1',\n    'efficientdet-d2': 'efficientnet-b2',\n    'efficientdet-d3': 'efficientnet-b3',\n    'efficientdet-d4': 'efficientnet-b4',\n    'efficientdet-d5': 'efficientnet-b5',\n    'efficientdet-d6': 'efficientnet-b6',\n    'efficientdet-d7': 'efficientnet-b6',\n}\nclass EfficientDet(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 network = 'efficientdet-d0',\n                 D_bifpn=3,\n                 W_bifpn=88,\n                 D_class=3,\n                 is_training=True,\n                 threshold=0.5,\n                 iou_threshold=0.5):\n        super(EfficientDet, self).__init__()\n        self.backbone = EfficientNet.from_pretrained(MODEL_MAP[network])\n        self.is_training = is_training\n        self.neck = BIFPN(in_channels=self.backbone.get_list_features()[-5:],\n                                out_channels=W_bifpn,\n                                stack=D_bifpn,\n                                num_outs=5)\n        self.bbox_head = RetinaHead(num_classes = num_classes,\n                                    in_channels = W_bifpn)\n\n\n        self.anchors = Anchors()\n        self.regressBoxes = BBoxTransform()\n        self.clipBoxes = ClipBoxes()\n        self.threshold = threshold\n        self.iou_threshold = iou_threshold\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. \/ n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        self.freeze_bn()\n\n    def forward(self, inputs):\n        x = self.extract_feat(inputs)\n        outs = self.bbox_head(x)\n        classification = torch.cat([out for out in outs[0]], dim=1)\n        regression = torch.cat([out for out in outs[1]], dim=1)\n        anchors = self.anchors(inputs)\n        if self.is_training:\n            return classification, regression, anchors\n        else:\n            transformed_anchors = self.regressBoxes(anchors, regression)\n            transformed_anchors = self.clipBoxes(transformed_anchors, inputs)\n            scores = torch.max(classification, dim=2, keepdim=True)[0]\n            scores_over_thresh = (scores > self.threshold)[0, :, 0]\n\n            if scores_over_thresh.sum() == 0:\n                print('No boxes to NMS')\n                # no boxes to NMS, just return\n                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n            classification = classification[:, scores_over_thresh, :]\n            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n            scores = scores[:, scores_over_thresh, :]\n            anchors_nms_idx = nms(transformed_anchors[0, :, :], scores[0, :, 0], iou_threshold = self.iou_threshold)\n            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(dim=1)\n            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n    def freeze_bn(self):\n        '''Freeze BatchNorm layers.'''\n        for layer in self.modules():\n            if isinstance(layer, nn.BatchNorm2d):\n                layer.eval()\n    def extract_feat(self, img):\n        \"\"\"\n            Directly extract features from the backbone+neck\n        \"\"\"\n        x = self.backbone(img)\n        x = self.neck(x[-5:])\n        return x","64314d8b":"def get_augumentation(phase, width=512, height=512, min_area=0., min_visibility=0.):\n    list_transforms = []\n    if phase == 'train':\n        list_transforms.extend([\n            albu.augmentations.transforms.LongestMaxSize(\n                max_size=width, always_apply=True),\n            albu.PadIfNeeded(min_height=height, min_width=width,\n                             always_apply=True, border_mode=0, value=[0, 0, 0]),\n            albu.augmentations.transforms.RandomResizedCrop(\n                height=height,\n                width=width, p=0.3),\n            albu.augmentations.transforms.Flip(),\n            albu.augmentations.transforms.Transpose(),\n            albu.OneOf([\n                albu.RandomBrightnessContrast(brightness_limit=0.5,\n                                              contrast_limit=0.4),\n                albu.RandomGamma(gamma_limit=(50, 150)),\n                albu.NoOp()\n            ]),\n            albu.OneOf([\n                albu.RGBShift(r_shift_limit=20, b_shift_limit=15,\n                              g_shift_limit=15),\n                albu.HueSaturationValue(hue_shift_limit=5,\n                                        sat_shift_limit=5),\n                albu.NoOp()\n            ]),\n            albu.CLAHE(p=0.8),\n            albu.HorizontalFlip(p=0.5),\n            albu.VerticalFlip(p=0.5),\n        ])\n    if(phase == 'test'):\n        list_transforms.extend([\n            albu.Resize(height=height, width=width)\n        ])\n    list_transforms.extend([\n        albu.Normalize(mean=(0.485, 0.456, 0.406),\n                       std=(0.229, 0.224, 0.225), p=1),\n        ToTensor()\n    ])\n    if(phase == 'test'):\n        return albu.Compose(list_transforms)\n    return albu.Compose(list_transforms, bbox_params=albu.BboxParams(format='pascal_voc', min_area=min_area,\n                                                                     min_visibility=min_visibility, label_fields=['category_id']))\n\ndef vis_bbox(img, bbox, label=None, score=None,\n             instance_colors=None, alpha=1., linewidth=2., ax=None):\n    \"\"\"Visualize bounding boxes inside the image.\n    Args:\n        img (~numpy.ndarray): An array of shape :math:`(3, height, width)`.\n            This is in RGB format and the range of its value is\n            :math:`[0, 255]`. If this is :obj:`None`, no image is displayed.\n        bbox (~numpy.ndarray): An array of shape :math:`(R, 4)`, where\n            :math:`R` is the number of bounding boxes in the image.\n            Each element is organized\n            by :math:`(y_{min}, x_{min}, y_{max}, x_{max})` in the second axis.\n        label (~numpy.ndarray): An integer array of shape :math:`(R,)`.\n            The values correspond to id for label names stored in\n            :obj:`label_names`. This is optional.\n        score (~numpy.ndarray): A float array of shape :math:`(R,)`.\n             Each value indicates how confident the prediction is.\n             This is optional.\n        label_names (iterable of strings): Name of labels ordered according\n            to label ids. If this is :obj:`None`, labels will be skipped.\n        instance_colors (iterable of tuples): List of colors.\n            Each color is RGB format and the range of its values is\n            :math:`[0, 255]`. The :obj:`i`-th element is the color used\n            to visualize the :obj:`i`-th instance.\n            If :obj:`instance_colors` is :obj:`None`, the red is used for\n            all boxes.\n        alpha (float): The value which determines transparency of the\n            bounding boxes. The range of this value is :math:`[0, 1]`.\n        linewidth (float): The thickness of the edges of the bounding boxes.\n        ax (matplotlib.axes.Axis): The visualization is displayed on this\n            axis. If this is :obj:`None` (default), a new axis is created.\n    Returns:\n        ~matploblib.axes.Axes:\n        Returns the Axes object with the plot for further tweaking.\n    from: https:\/\/github.com\/chainer\/chainercv\n    \"\"\"\n             \n    if label is not None and not len(bbox) == len(label):\n        raise ValueError('The length of label must be same as that of bbox')\n    if score is not None and not len(bbox) == len(score):\n        raise ValueError('The length of score must be same as that of bbox')\n\n    # Returns newly instantiated matplotlib.axes.Axes object if ax is None\n    if ax is None:\n        fig = plt.figure()\n        # ax = fig.add_subplot(1, 1, 1)\n        h, w, _ = img.shape\n        w_ = w \/ 60.0\n        h_ = w_ * (h \/ w)\n        fig.set_size_inches((w_, h_))\n        ax = plt.axes([0, 0, 1, 1])\n    ax.imshow(img.astype(np.uint8))\n    ax.axis('off')\n    # If there is no bounding box to display, visualize the image and exit.\n    if len(bbox) == 0:\n        return fig, ax\n\n    if instance_colors is None:\n        # Red\n        instance_colors = np.zeros((len(bbox), 3), dtype=np.float32)\n        instance_colors[:, 0] = 51\n        instance_colors[:, 1] = 51\n        instance_colors[:, 2] = 224\n    instance_colors = np.array(instance_colors)\n\n    for i, bb in enumerate(bbox):\n        xy = (bb[0], bb[1])\n        height = bb[3] - bb[1]\n        width = bb[2] - bb[0]\n        color = instance_colors[i % len(instance_colors)] \/ 255\n        ax.add_patch(plt.Rectangle(\n            xy, width, height, fill=False,\n            edgecolor=color, linewidth=linewidth, alpha=alpha))\n\n        caption = []\n        caption.append(label[i])\n        if(len(score)>0):\n            sc = score[i]\n            caption.append('{}'.format(sc))\n\n        if len(caption) > 0:\n            face_color = np.array([225, 51, 123])\/255\n            ax.text(bb[0], bb[1],\n                    ': '.join(caption),\n                    fontsize=12,\n                    color='black',\n                    style='italic',\n                    bbox={'facecolor': face_color, 'edgecolor': face_color, 'alpha': 1, 'pad': 0})\n    return fig, ax","ab0dc338":"VOC_CLASSES = (  # always index 0\n    'aeroplane', 'bicycle', 'bird', 'boat',\n    'bottle', 'bus', 'car', 'cat', 'chair',\n    'cow', 'diningtable', 'dog', 'horse',\n    'motorbike', 'person', 'pottedplant',\n    'sheep', 'sofa', 'train', 'tvmonitor')","a506c1d6":"EFFICIENTDET = {\n    'efficientdet-d0': {'input_size': 512,\n                        'backbone': 'B0',\n                        'W_bifpn': 64,\n                        'D_bifpn': 2,\n                        'D_class': 3},\n    'efficientdet-d1': {'input_size': 640,\n                        'backbone': 'B1',\n                        'W_bifpn': 88,\n                        'D_bifpn': 3,\n                        'D_class': 3},\n    'efficientdet-d2': {'input_size': 768,\n                        'backbone': 'B2',\n                        'W_bifpn': 112,\n                        'D_bifpn': 4,\n                        'D_class': 3},\n    'efficientdet-d3': {'input_size': 896,\n                        'backbone': 'B3',\n                        'W_bifpn': 160,\n                        'D_bifpn': 5,\n                        'D_class': 4},\n    'efficientdet-d4': {'input_size': 1024,\n                        'backbone': 'B4',\n                        'W_bifpn': 224,\n                        'D_bifpn': 6,\n                        'D_class': 4},\n    'efficientdet-d5': {'input_size': 1280,\n                        'backbone': 'B5',\n                        'W_bifpn': 288,\n                        'D_bifpn': 7,\n                        'D_class': 4},\n    'efficientdet-d6': {'input_size': 1408,\n                        'backbone': 'B6',\n                        'W_bifpn': 384,\n                        'D_bifpn': 8,\n                        'D_class': 5},\n    'efficientdet-d7': {'input_size': 1636,\n                        'backbone': 'B6',\n                        'W_bifpn': 384,\n                        'D_bifpn': 8,\n                        'D_class': 5},\n}","1436cc42":"class Detect(object):\n    \"\"\"\n        dir_name: Folder or image_file\n    \"\"\"\n\n    def __init__(self, weights, num_class=21, network='efficientdet-d1', size_image=(640, 640)):\n        super(Detect,  self).__init__()\n        self.weights = weights\n        self.size_image = size_image\n        self.device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() else 'cpu')\n        self.transform = get_augumentation(phase='test', width=self.size_image[1], height=self.size_image[0])\n        if(self.weights is not None):\n            print('Load pretrained Model')\n            checkpoint = torch.load(\n                self.weights, map_location=lambda storage, loc: storage)\n            num_class = checkpoint['num_class']\n            network = checkpoint['network']\n\n        self.model = EfficientDet(num_classes=num_class,\n                     network=network,\n                     W_bifpn=EFFICIENTDET[network]['W_bifpn'],\n                     D_bifpn=EFFICIENTDET[network]['D_bifpn'],\n                     D_class=EFFICIENTDET[network]['D_class'],\n                     is_training=False\n                     )\n\n        if(self.weights is not None):\n            state_dict = checkpoint['state_dict']\n            self.model.load_state_dict(state_dict)\n        self.model = self.model.cuda()\n        self.model.eval()\n\n    def process(self, file_name=None, img=None, show=False, score=True):\n        if file_name is not None:\n            img = cv2.imread(file_name)\n#         origin_img = copy.deepcopy(img)\n        origin_img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        augmentation = self.transform(image=img)\n        img = augmentation['image']\n        img = img.to(self.device)\n        img = img.unsqueeze(0)\n\n        with torch.no_grad():\n            scores, classification, transformed_anchors = self.model(img)\n            bboxes = list()\n            labels = list()\n            bbox_scores = list()\n            colors = list()\n            for j in range(scores.shape[0]):\n                bbox = transformed_anchors[[j], :][0].data.cpu().numpy()\n                x1 = int(bbox[0]*origin_img.shape[1]\/self.size_image[1])\n                y1 = int(bbox[1]*origin_img.shape[0]\/self.size_image[0])\n                x2 = int(bbox[2]*origin_img.shape[1]\/self.size_image[1])\n                y2 = int(bbox[3]*origin_img.shape[0]\/self.size_image[0])\n                bboxes.append([x1, y1, x2, y2])\n                label_name = VOC_CLASSES[int(classification[[j]])]\n                labels.append(label_name)\n\n                if score:\n                    score = np.around(scores[[j]].cpu().numpy(), decimals=2) * 100\n                    bbox_scores.append(int(score))\n                    \n            if show:\n                fig, ax = vis_bbox(img=origin_img, bbox=bboxes,\n                                   label=labels, score=bbox_scores)\n                fig.savefig('.\/demo.png')\n                plt.show()\n            else:\n                return origin_img","59056b66":"detect = Detect(weights='..\/input\/efficientdetd0\/checkpoint_VOC_efficientdet-d1_37.pth')","b1a95c11":"test = pd.read_csv('..\/input\/pku-autonomous-driving\/sample_submission.csv')","6049565c":"n = np.random.randint(len(test))\nfile_path = '..\/input\/pku-autonomous-driving\/test_images\/' + test[\"ImageId\"][n] + '.jpg'\ndetect.process(file_name=file_path, show=True)","ebc867b7":"EfficientNet","6d03368f":"vis_bbox, get_augmentation","5f7c9274":"## 2. Multi-Scale Feature Representations :\n**we first formulate the multi-scale feature fusion problem, and then introduce the two main ideas for our proposed BiFPN: efficient bidirectional cross-scale connections and weighted feature fusion.**\n\n- structure :\n![2](https:\/\/pic1.zhimg.com\/80\/v2-c5b7e0dcb2f455e2e4037e10922bbb50_hd.jpg)\n\n- weighted feature fusion\n![a](https:\/\/pic1.zhimg.com\/80\/v2-328a45d131dac4c7aefe0eddffbf0bcc_hd.jpg)","36326875":"module","5ca50778":"EfficientDet","25a5cd4f":"EFFICIENTDET","1bf3eea1":"### Detect","5751a882":"BIFPN","256bec9c":"### EfficientDet","59bf1d5b":"## ['EfficientDet: Scalable and Efficient Object Detection'](https:\/\/link.zhihu.com\/?target=https%3A\/\/arxiv.org\/abs\/1911.09070v1)\n- Google's paper in Nov 20th\n\nno official open-source now, just find some repos.\n\nhttps:\/\/github.com\/toandaominh1997\/EfficientDet.Pytorch\n\nIf you think it's useful, please give me an upvote.","8e456ba9":"VOC_CLASSES","69c51981":"utils","37e31fcf":"RetinaHead","5a2aa799":"# EfficientNet backbone\n## 1. What's EfficientNet?\n[**EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks ICML 2019 **](https:\/\/arxiv.org\/abs\/1905.11946)\n\nTensorflow(official): https:\/\/github.com\/tensorflow\/tpu\/tree\/master\/models\/official\/efficientnet\n\nPytorch: https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch\n\n![efficientnet](https:\/\/img-blog.csdnimg.cn\/20190607190809941.png?x-oss-process=image\/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90cmVudC5ibG9nLmNzZG4ubmV0,size_16,color_FFFFFF,t_70)","e28fa6b0":"# How to build BiFPN ?\n## 1. What's FPN?\n[**Feature Pyramid Networks for Object Detection**](https:\/\/arxiv.org\/abs\/1612.03144)\n![fpn](http:\/\/file.elecfans.com\/web1\/M00\/59\/BB\/pIYBAFtqRd-AZlifAAAziaQTkYk948.png)\n","530243c6":"## wonderful result :\n![1](http:\/\/pics5.baidu.com\/feed\/c8177f3e6709c93d6cc3fbc78f4a2ad9d100544d.jpeg?token=989fe2e4ec45157cb9f0fa85fa9944e2&s=21B6ED3251AFF0EE1ED908C10000F0B3)","87b68075":"## 2. BiFPN\n![image.png](attachment:image.png)","7219a32b":"# Demo","b12894e6":"## 3. Model Scaling :\n**Based on our BiFPN, we have developed a new family of detection models named EfficientDet. In this section, we will discuss the network architecture and a new compound scaling method for EfficientDet.**\n![3](https:\/\/pic3.zhimg.com\/80\/v2-e89c42b5c07697837b2883b0174916da_hd.jpg)","2fd79feb":"conv_module","9c3e39cb":"# Related Work\n## 1. One-Stage Detectors :\n**In this paper, we mainly follow the one-stage detector design, and we show it is possible to achieve both better efficiency and higher accuracy with optimized network architectures.**","b0a72cf8":"## 4. Result :\n- 1\n![4](https:\/\/pic4.zhimg.com\/80\/v2-039b1197596fff9af09b55bb24b53e33_hd.jpg)\n\n- 2\n![5](http:\/\/pics2.baidu.com\/feed\/b7fd5266d016092498ca5cd7da70e7ffe4cd348f.png?token=5bf52f9beca3ddc6c420785935ee6994&s=2AAC7A22C4F1C988185DB1CB0000C0B1)\n\n- 3\n![6](http:\/\/pics4.baidu.com\/feed\/1c950a7b02087bf403c35afce2a4852911dfcf70.jpeg?token=36b5e37c58493aa8ee12e82cb817dce1&s=8D0EED1211CC48EA0CC9A1DA000080B2)"}}