{"cell_type":{"c8e7d0d4":"code","ccfc0921":"code","586ba678":"code","736c8650":"code","e5c299ef":"code","7697b162":"code","4db989ec":"code","11db67a3":"code","aa8d55db":"code","bea68f03":"code","a7c6a6a7":"code","5519e34e":"code","7098e76c":"code","3c8ac1df":"code","d4eab18a":"code","6a9f4e4a":"code","b7118111":"code","8825e015":"code","7de86bbf":"code","a0d5b11f":"code","211e792f":"code","1ea8d765":"code","1427da43":"code","82fb4b7b":"code","522bf3e3":"code","489217e4":"code","61f15ef4":"code","9cd761f0":"code","3a4be564":"code","e4e2ca64":"code","169b3565":"code","f5657b55":"code","9476a8b9":"code","74c0bdf1":"code","a205a3c4":"code","e2beb1f7":"code","50ee2010":"code","f1d57e6c":"code","d24853b1":"code","548e2fb3":"code","993ccc42":"code","9daa5515":"markdown","eb1f27b8":"markdown","667c77aa":"markdown","e23f2d6f":"markdown","a4bfa9c4":"markdown","c2636f0d":"markdown"},"source":{"c8e7d0d4":"!pip install tensorflow==2.2.0","ccfc0921":"!pip install  numba  \n!pip install cudatoolkit","586ba678":"import io\nfrom numba import jit, cuda\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow import keras\n\nimport tqdm","736c8650":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","e5c299ef":"LAMBDA=10\nBATCH_SIZE =  4\nEPOCHS_NUM = 30\nimage_size = (256, 256)\nbuffer_size= 256","7697b162":"DATA_PATH = KaggleDatasets().get_gcs_path()","4db989ec":"import re\nMONET_FILENAMES = tf.io.gfile.glob(str(DATA_PATH + '\/monet_tfrec\/*.tfrec'))\nprint('Monet TFRecord Files:', len(MONET_FILENAMES))\n\nPHOTO_FILENAMES = tf.io.gfile.glob(str(DATA_PATH + '\/photo_tfrec\/*.tfrec'))\nprint('Photo TFRecord Files:', len(PHOTO_FILENAMES))\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nn_monet_samples = count_data_items(MONET_FILENAMES)\nn_photo_samples = count_data_items(PHOTO_FILENAMES)\n\n\n\nprint(f'Monet TFRecord files: {len(MONET_FILENAMES)}')\nprint(f'Monet image files: {n_monet_samples}')\nprint(f'Photo TFRecord files: {len(PHOTO_FILENAMES)}')\nprint(f'Photo image files: {n_photo_samples}')\nprint(f\"Batch_size: {BATCH_SIZE}\")\nprint(f\"Epochs number: {EPOCHS_NUM}\")","11db67a3":"\ndef decode_image(image):\n    #image = tf.image.resize(image, [286, 286,3],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) \/ 127.5) - 1\n    image = tf.reshape(image, [*image_size, 3])\n    image = tf.image.random_flip_left_right(image)\n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\n","aa8d55db":"def data_augmentation(image):\n    photo_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    photo_spatial = tf.random.uniform([], 0, 1.0, dtype= tf.float32)\n    cropping = tf.random.uniform([], 0, 1.0, dtype= tf.float32)\n    \n    \n    if cropping >0.5:\n        image = tf.image.resize(image, [286, 286])\n        image = tf.image.random_crop(image, size =[256, 256,3])\n        \n        if cropping >0.9:\n            image = tf.image.resize(image, [286, 286])\n            image = tf.image.random_crop(image, size=[256, 256, 3])\n            \n            \n    if photo_rotate > 0.9:\n        image = tf.image.rot90(image, k=3)\n        \n    elif photo_rotate > 0.7 and photo_rotate <=0.9:\n        image = tf.image.rot90(image, k=2)\n        \n    elif photo_rotate > 0.5 and photo_rotate <=0.7:\n        image = tf.image.rot90(image, k=1)\n        \n        \n    if photo_spatial >0.6:\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        \n        \n        if photo_spatial >0.9:\n            image = tf.image.transpose(image)\n            \n    return image","bea68f03":"def load_dataset(filename, aug):\n    dataset = tf.data.TFRecordDataset(filename)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls= AUTOTUNE)\n    if aug:\n        dataset = dataset.map(aug, num_parallel_calls=  AUTOTUNE)\n    return dataset","a7c6a6a7":"def get_dataset(filename, aug = True):\n    dataset = load_dataset(filename, aug)\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","5519e34e":"monet_image = get_dataset(MONET_FILENAMES,  aug = data_augmentation)\nphotos = get_dataset(PHOTO_FILENAMES,  aug = data_augmentation)","7098e76c":"example_monet = iter(monet_image)\nexample_photo = iter(photos)\n\n_ = plt.figure(figsize=  (15, 30))\n\nfor i in range(10):\n    plt.subplot(10, 2, i*2+1)\n    plt.imshow(tf.cast(next(example_monet)[0] * 127.5 + 127.5, tf.uint8))\n    plt.title(\"Monet image\")\n\n    plt.subplot(10,2,i*2 +2)\n    plt.imshow(tf.cast(next(example_photo)[0] * 127.5 + 127.5, tf.uint8))\n    plt.title(\"Photo image\")\n\n","3c8ac1df":"OUTPUT_CHANNELS = 3\n# Weights initializer for the layers.\nkernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n# Gamma initializer for instance normalization.\ngamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\ninput_img_size = (256, 256, 3)","d4eab18a":"class ReflectionPadding2D(layers.Layer):\n    def __init__(self, padding=(1,1), **kwargs):\n        self.padding = tuple(padding)\n        super(ReflectionPadding2D, self).__init__(**kwargs)\n        \n    def call(self, input_tensor, mask= None):\n        padding_width, padding_height= self.padding\n        \n        padding_tensor= [[0,0],[padding_height, padding_height], \n                         [padding_width, padding_width], [0,0],]\n        \n        return tf.pad(input_tensor, padding_tensor, mode= 'REFLECT')\n    \n    \n    \n    \n    ","6a9f4e4a":"def downsample(x, filters, activation,kernel_initializer= kernel_init, kernel_size= (3,3),\n              strides= (2,2), padding= 'same', gamma_initializer = gamma_init, use_bias= False, apply_instancenorm=True):\n    x= keras.Sequential()\n    x.add(layers.Conv2D(filters, kernel_size, strides= strides, kernel_initializer= kernel_initializer, \n                        padding= padding, use_bias= use_bias))\n    if apply_instancenorm:\n        x.add(tfa.layers.InstanceNormalization(gamma_initializer= gamma_initializer))\n        \n    x.add(tf.keras.layers.BatchNormalization())\n    \n    x.add(layers.LeakyReLU())\n    \n    return x","b7118111":"def upsample(x, filters, activation, kernel_initializer= kernel_init, kernel_size =(3,3),\n            strides= (2,2), padding=\"same\", gamma_initializer= gamma_init, use_bias= False, apply_dropout=True):\n    x= keras.Sequential()\n    x.add(layers.Conv2DTranspose(filters, kernel_size, strides= strides, padding= padding,\n                                    kernel_initializer= kernel_initializer, use_bias= use_bias))\n    \n    x.add(tfa.layers.InstanceNormalization(gamma_initializer = gamma_init))\n    x.add(tf.keras.layers.BatchNormalization())\n    if apply_dropout:\n        x.add(layers.Dropout(0.5))\n        \n    \n        \n    x.add(layers.ReLU())\n    \n    return x\n    \n    ","8825e015":"def Generator(kernel_initializer= kernel_init, num_residual_blocks=9):\n    inputs = layers.Input(shape=[256,256,3])\n\n    # bs = batch size\n    down_stack = [\n        downsample(64, 4, apply_instancenorm=False, activation= layers.LeakyReLU(0.2)), # (bs, 128, 128, 64)\n        downsample(128, 4, activation= layers.LeakyReLU(0.2)), # (bs, 64, 64, 128)\n        downsample(256, 4, activation= layers.LeakyReLU(0.2)), # (bs, 32, 32, 256)\n        downsample(512, 4, activation= layers.LeakyReLU(0.2)), # (bs, 16, 16, 512)\n        downsample(512, 4, activation= layers.LeakyReLU(0.2)), # (bs, 8, 8, 512)\n        downsample(512, 4, activation= layers.LeakyReLU(0.2)), # (bs, 4, 4, 512)\n        downsample(512, 4, activation= layers.LeakyReLU(0.2)), # (bs, 2, 2, 512)\n        downsample(1024, 4, activation= layers.LeakyReLU(0.2))\n    \n        \n    ]\n\n    up_stack = [\n        upsample(1024, 4, apply_dropout=True, activation= layers.Activation(\"relu\")),\n        upsample(512, 4, apply_dropout=True, activation= layers.Activation(\"relu\")), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True, activation= layers.Activation(\"relu\")), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True, activation= layers.Activation(\"relu\")), # (bs, 8, 8, 1024)\n        upsample(512, 4, activation= layers.Activation(\"relu\")), # (bs, 16, 16, 1024)\n        upsample(256, 4, activation= layers.Activation(\"relu\")), # (bs, 32, 32, 512)\n        upsample(128, 4, activation= layers.Activation(\"relu\")), # (bs, 64, 64, 256)\n        upsample(64, 4, activation= layers.Activation(\"relu\")),\n        \n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=kernel_initializer,\n                                  activation='tanh') # (bs, 256, 256, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n    \n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n        \n    x = ReflectionPadding2D(padding=(3, 3))(x)\n    x = layers.Conv2D(128, (7, 7), padding=\"valid\")(x)\n    x = last(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=x)","7de86bbf":"with strategy.scope():\n    monet_generator= Generator()\n    photo_generator= Generator()","a0d5b11f":"from keras.utils.vis_utils import plot_model\nplot_model(monet_generator, to_file='Generator.png', show_shapes=True, show_layer_names=True)","211e792f":"def discriminator(kernel_initializer= kernel_init, gamma_initializer= gamma_init):\n    \n    input_image = layers.Input(shape=[256, 256, 3], name='input_image')\n\n    x = input_image\n\n    down1 = downsample(64, 4, apply_instancenorm = True , activation = layers.LeakyReLU(0.2))(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4, activation = layers.LeakyReLU(0.2))(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4, activation = layers.LeakyReLU(0.2))(down2)\n    #down4 = downsample(512, 4, activation = layers.LeakyReLU(0.2))(down3)\n    #down5 = downsample(120, 4, activation = layers.LeakyReLU(0.2))(down4)# (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, (4,4), strides=(1,1),\n                         kernel_initializer=kernel_initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(conv)\n    batchnorm1 = tf.keras.layers.BatchNormalization()(norm1)\n\n    leaky_relu = layers.LeakyReLU()(batchnorm1)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides=(1,1),padding=\"same\",\n                         kernel_initializer=kernel_initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=input_image, outputs=last) ","1ea8d765":"with strategy.scope():\n    monet_discriminator= discriminator()\n    photo_discriminator = discriminator()","1427da43":"plot_model(photo_discriminator, to_file='Discriminator.png', show_shapes=True, show_layer_names=True)","82fb4b7b":"random_img = monet_generator(next(example_monet) *127.5 + 127.5)\nrandom_img = random_img[0]\nplt.imshow(random_img)\nplt.axis('off')\n\n","522bf3e3":"random_img = photo_generator(next(example_photo) *127.5 + 127.5)\nrandom_img = random_img[0]\nplt.imshow(random_img)\nplt.axis('off')","489217e4":"monet_example = monet_discriminator(next(example_monet))\nphoto_example = photo_discriminator(next(example_photo))\nmonet_example = monet_example[0]\nphoto_example = photo_example[0]\n\n_ = plt.figure(figsize= (10,10))\nplt.subplot(1,2,1)\nplt.imshow(tf.squeeze(monet_example, 2))\nplt.title(\"Monet image\")\n\nplt.subplot(1,2,2)\nplt.imshow(tf.squeeze(photo_example, 2))\nplt.title(\"Photo image\")","61f15ef4":"\n\n\n\n\nclass CycleGAN(keras.Model):\n    def __init__(self, monet_generator, photo_generator, monet_discriminator, photo_discriminator,\n                lambda_cycle=10.0, lambda_identity = 0.5):\n        \n        \n        super(CycleGAN, self).__init__()\n        self.monet_g = monet_generator\n        self.photo_g = photo_generator\n        \n        self.monet_d = monet_discriminator\n        self.photo_d = photo_discriminator\n        \n        self.lambda_cycle = lambda_cycle\n        self.lambda_identity= lambda_identity\n        \n        \n        \n        \n        \n        \n    def compile(self, monet_g_optimizer, photo_g_optimizer, monet_d_optimizer,\n               photo_d_optimizer, gen_loss_fn, dis_loss_fn, cycle_loss_fn, identity_loss_fn):\n        \n        \n        super(CycleGAN, self).compile()\n        \n        \n        self.monet_g_optimizer = monet_g_optimizer\n        self.photo_g_optimizer = photo_g_optimizer\n        \n        self.monet_d_optimizer = monet_d_optimizer\n        self.photo_d_optimizer = photo_d_optimizer\n        \n        self.gen_loss_fn = gen_loss_fn\n        self.dis_loss_fn = dis_loss_fn\n        \n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n      \n    @tf.autograph.experimental.do_not_convert\n    def train_step(self, batch_data):\n        real_monet, real_photo= batch_data\n        \n        with tf.GradientTape(persistent= True) as tape:\n            fake_photo = self.monet_g(real_monet, training= True)\n            fake_monet = self.photo_g(real_photo, training = True)\n            \n            \n            cycled_monet = self.photo_g(fake_photo, training =True)\n            cycled_photo = self.monet_g(fake_monet, training= True)\n            \n            \n            same_monet= self.photo_g(real_monet, training= True)\n            same_photo = self.monet_g(real_photo, training= True)\n            \n            \n            dis_real_monet = self.monet_d(real_monet, training = True)\n            dis_fake_monet = self.monet_d(fake_monet, training= True)\n            \n            dis_real_photo = self.photo_d(real_photo, training = True)\n            dis_fake_photo = self.photo_d(fake_photo, training = True)\n            \n            gen_monet_loss= self.gen_loss_fn(dis_fake_photo)\n            gen_photo_loss= self.gen_loss_fn(dis_fake_monet)\n            \n            cycle_loss_monet = self.cycle_loss_fn(real_photo, cycled_photo, LAMBDA)* self.lambda_cycle\n            cycle_loss_photo = self.cycle_loss_fn(real_monet, cycled_monet,LAMBDA)* self.lambda_cycle\n            \n            total_cycle_loss= cycle_loss_monet + cycle_loss_photo\n            \n            id_loss_monet = (self.identity_loss_fn(real_monet, same_monet,LAMBDA) * self.lambda_cycle * self.lambda_identity)\n            id_loss_photo = (self.identity_loss_fn(real_photo, same_photo, LAMBDA) * self.lambda_cycle * self.lambda_identity)\n            \n            #total loss of generator\n            total_loss_monet = gen_monet_loss+ cycle_loss_monet + id_loss_monet \n            total_loss_photo = gen_photo_loss + cycle_loss_photo + id_loss_photo\n            \n            \n            #discrminator_loss\n            dis_monet_loss = self.dis_loss_fn(dis_real_monet, dis_fake_monet)\n            dis_photo_loss = self.dis_loss_fn(dis_real_photo, dis_fake_photo)\n        \n        #Generator gradient\n        grad_monet_g = tape.gradient(total_loss_monet, self.monet_g.trainable_variables)\n        grad_photo_g = tape.gradient(total_loss_photo, self.photo_g.trainable_variables)\n        \n        \n        #Discriminator_gradient\n        grad_monet_d = tape.gradient(dis_monet_loss, self.monet_d.trainable_variables)\n        grad_photo_d = tape.gradient(dis_photo_loss, self.photo_d.trainable_variables)\n        \n        \n        self.monet_g_optimizer.apply_gradients(zip(grad_monet_g, self.monet_g.trainable_variables))\n        self.photo_g_optimizer.apply_gradients(zip(grad_photo_g, self.photo_g.trainable_variables))\n        \n        \n        \n        \n        self.monet_d_optimizer.apply_gradients(zip(grad_monet_d, self.monet_d.trainable_variables))\n        self.photo_d_optimizer.apply_gradients(zip(grad_photo_d, self.photo_d.trainable_variables))\n        \n        \n        \n        \n        \n        return {\n            \"Monet_generator_loss\": total_loss_monet,\n            \"Photo_generator_loss\": total_loss_photo,\n            \"Monet_discriminator_loss\": dis_monet_loss,\n            \"Photo_discriminator_loss\": dis_photo_loss\n        }\n    \n    \n   \n            \n            \n        \n        \n        ","9cd761f0":"LAMBDA=10","3a4be564":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits = True, \n                                                       reduction= tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits= True,\n                                                           reduction= tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n        \n        total_disc_loss = real_loss + generated_loss\n        \n        return total_disc_loss * 0.5","e4e2ca64":"\n\nwith strategy.scope():\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, \n                                                  reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n\n","169b3565":"with strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n        return LAMBDA * loss1","f5657b55":"\n\nwith strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss\n\n","9476a8b9":"with strategy.scope():\n    monet_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    monet_d_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_d_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","74c0bdf1":"\n\nwith strategy.scope():\n    cycle_gan_model = CycleGAN(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator)\n\n    cycle_gan_model.compile(\n        monet_g_optimizer = monet_g_optimizer,\n        photo_g_optimizer = photo_g_optimizer,\n        monet_d_optimizer = monet_d_optimizer,\n        photo_d_optimizer = photo_d_optimizer,\n        gen_loss_fn = generator_loss,\n        dis_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )\n\n","a205a3c4":"class GANMonitor(keras.callbacks.Callback):\n    \"\"\"A callback to generate and save images after each epoch\"\"\"\n\n    def __init__(self, num_img=4):\n        self.num_img = num_img\n\n    def on_epoch_end(self, epoch, logs=None):\n        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n        for i, img in enumerate(monet_image.take(self.num_img)):\n            prediction = self.model.monet_generator(img)[0].numpy()\n            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n            ax[i, 0].imshow(img)\n            ax[i, 1].imshow(prediction)\n            ax[i, 0].set_title(\"Input image\")\n            ax[i, 1].set_title(\"Translated image\")\n            ax[i, 0].axis(\"off\")\n            ax[i, 1].axis(\"off\")\n\n            prediction = keras.preprocessing.image.array_to_img(prediction)\n            prediction.save(\n                \"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1)\n            )\n        plt.show()\n        plt.close()\n","e2beb1f7":"plotter = GANMonitor()\ncheckpoint_filepath = \".\/model_checkpoints\/cyclegan_checkpoints.{epoch:03d}\"\nmodel_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath\n)","50ee2010":"cycle_gan_model.fit(\n    tf.data.Dataset.zip((monet_image, photos)),\n    epochs=25,verbose=1\n)","f1d57e6c":"_, ax = plt.subplots(5, 2, figsize=(12, 12))\nfor i, img in enumerate(photos.take(5)):\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(img)\n    ax[i, 1].imshow(prediction)\n    ax[i, 0].set_title(\"Input Photo\")\n    ax[i, 1].set_title(\"Monet-esque\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()","d24853b1":"import PIL\n! mkdir ..\/images","548e2fb3":"\ni = 1\nfor img in photos:\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"..\/images\/\" + str(i) + \".jpg\")\n    i += 1\n","993ccc42":"import shutil\nshutil.make_archive(\"\/kaggle\/working\/images\", 'zip', \"\/kaggle\/images\")","9daa5515":"def residual_block(x, activation, kernel_initializer = kernel_init,\n                  kernel_size =(3,3), strides=(1,1), padding=\"valid\",\n                  gamma_initializer= gamma_init, use_bias= False):\n    \n    dim = x.shape[-1]\n    input_tensor = x\n    \n    x= ReflectionPadding2D()(input_tensor)\n    x= layers.Conv2D(dim, kernel_size, strides=strides,\n                    kernel_initializer = kernel_initializer, padding= padding, use_bias=use_bias)(x)\n    \n    x= tfa.layers.InstanceNormalization(gamma_initializer = gamma_initializer)(x)\n    x= activation(x)\n    x = ReflectionPadding2D()(x)\n    x = layers.Conv2D(dim, kernel_size= kernel_size, strides= strides, kernel_initializer= kernel_initializer,\n                     padding= padding, use_bias = use_bias)(x)\n    \n    x= tfa.layers.InstanceNormalization(gamma_initializer = gamma_initializer)(x)\n    x= layers.add([input_tensor, x])\n    \n    return x\n","eb1f27b8":"to_monet = monet_generator(example_photo)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Original Photo\")\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Monet-esque Photo\")\nplt.imshow(to_monet[0] * 0.5 + 0.5)\nplt.show()","667c77aa":"### **I have taken help from the notebook provided by [Keras](https:\/\/keras.io\/examples\/generative\/cyclegan\/) and [Tensorflow](https:\/\/www.tensorflow.org\/tutorials\/generative\/cyclegan) . Many many thaks to [Amy Jang](http:\/\/www.kaggle.com\/amyjang) for her notebook [Monet CycleGAN Tutorial](https:\/\/www.kaggle.com\/amyjang\/monet-cyclegan-tutorial). As I am first time learner of GAN it was very helpful.**","e23f2d6f":"# ***CYCLEGAN***\n\n### CycleGAN uses a cycle consistency loss to enable training without the need for paired data. In other words, it can translate from one domain to another without a one-to-one mapping between the source and target domain.  This opens up the possibility to do a lot of interesting tasks like photo-enhancement, image colorization, style transfer, etc. All you need is the source and the target dataset (which is simply a directory of images).","a4bfa9c4":"def Generator(filters= 64, num_downsampling_blocks=4, num_residual_blocks=9,  num_upsample_blocks=2,\n             gamma_initializer= gamma_init, name =None):\n    image_input= layers.Input(shape=[256,256,3], name=name)\n    \n    \n    x= ReflectionPadding2D(padding=(3,3))(image_input)\n    x = layers.Conv2D(filters, (7,7), kernel_initializer= kernel_init, use_bias= False)(x)\n    \n    x = tfa.layers.InstanceNormalization(gamma_initializer = gamma_initializer)(x)\n    \n    x =  layers.Activation(\"relu\")(x)\n    \n    for _ in range(num_downsampling_blocks):\n        filters *=2\n        x=downsample(x, filters= filters, activation= layers.Activation(\"relu\"))\n        \n        \n    for _ in range(num_residual_blocks):\n        \n        \n        x=residual_block(x, activation= layers.Activation(\"relu\"))\n        \n        \n    for _ in range(num_upsample_blocks):\n        filters \/\/=2\n        \n        \n        x=upsample(x, filters, activation= layers.Activation(\"relu\"))\n        \n    x = ReflectionPadding2D(padding=(3, 3))(x)\n    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n    x = layers.Activation(\"tanh\")(x)\n    \n    model= keras.models.Model(inputs= image_input, outputs=x, name=name)\n    \n    return model\n    \n    ","c2636f0d":"Cycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the  original sentence.\n\nIn cycle consistency loss, \n\n* Image $X$ is passed via generator $G$ that yields generated image $\\hat{Y}$.\n* Generated image $\\hat{Y}$ is passed via generator $F$ that yields cycled image $\\hat{X}$.\n* Mean absolute error is calculated between $X$ and $\\hat{X}$.\n\n$$forward\\ cycle\\ consistency\\ loss: X -> G(X) -> F(G(X)) \\sim \\hat{X}$$\n\n$$backward\\ cycle\\ consistency\\ loss: Y -> F(Y) -> G(F(Y)) \\sim \\hat{Y}$$\n\n\n![Cycle loss](images\/cycle_loss.png)"}}