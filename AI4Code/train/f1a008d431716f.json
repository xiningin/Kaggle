{"cell_type":{"eda615b4":"code","f4a047f8":"code","2c344611":"code","85aa748a":"code","9c1b55a5":"code","3d743acb":"code","69e14335":"code","7d77a7cd":"code","2f9974ad":"code","3df1ac41":"code","b52da68e":"code","1e10bec3":"code","527e8506":"code","4289cc34":"code","24821b28":"code","9b400ef9":"code","f434001e":"code","6f619f38":"markdown","ff4959b1":"markdown","b29cbf07":"markdown","bb98d8b4":"markdown","68036c5d":"markdown","cb16d582":"markdown","b900e214":"markdown","3dff19b5":"markdown"},"source":{"eda615b4":"import numpy as np \nimport pandas as pd\nimport os\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\n","f4a047f8":"df = pd.read_csv(\"\/kaggle\/input\/indonesian-names\/indonesian-names.csv\")\ndf.head()","2c344611":"df.columns","85aa748a":"df.isnull().sum()","9c1b55a5":"df.info()","3d743acb":"print(df.gender.unique())\ndf = df.replace(' m', 'm')\ndf = df[df.gender != \"LK\"]\ndf = df[df.gender != \"P\"]\nprint(df.gender.unique())","69e14335":"df.gender.value_counts().plot(kind=\"bar\")\ndf.gender.value_counts()","7d77a7cd":"df.gender.replace({'f':0,'m':1},inplace=True)\ndf.head()","2f9974ad":"y = df.gender\nx = df.name","3df1ac41":"cv = CountVectorizer()\nX = cv.fit_transform(x)\ncv.get_feature_names()[:5]","b52da68e":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","1e10bec3":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(x_train,y_train)\nclf.score(x_test,y_test)","527e8506":"print(\"Validation Accuracy\",clf.score(x_test,y_test)*100,\"%\")","4289cc34":"print(\"Training Accuracy\",clf.score(x_train,y_train)*100,\"%\")","24821b28":"plot_confusion_matrix(clf, x_test, y_test)","9b400ef9":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    MultinomialNB()]\n\nfor name, clf in zip(names, classifiers):\n    print(name,   clf)\n    clf.fit(x_train, y_train)\n    score = clf.score(x_test, y_test)\n    print(\"Validation Accuracy\",score*100,\"%\")\n","f434001e":"import xgboost as xgb\ndt = xgb.DMatrix(x_train,label=y_train)\ndv = xgb.DMatrix(x_test,label=y_test)\nparams = {\n    \"eta\": 0.2,\n    \"max_depth\": 4,\n    \"objective\": \"binary:logistic\",\n    \"silent\": 1,\n    \"base_score\": np.mean(y_train),\n    'n_estimators': 1000,\n    \"eval_metric\": \"logloss\"\n}\nmodel = xgb.train(params, dt, 2000, [(dt, \"train\"),(dv, \"valid\")], verbose_eval=200)\ny_pred = model.predict(dv)\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, (y_pred>0.5))\nprint(cm)\n# Calculate the accuracy on test set\npredict_accuracy_on_test_set = (cm[0,0] + cm[1,1])\/(cm[0,0] + cm[1,1]+cm[1,0] + cm[0,1])\nax = sns.heatmap(cm, linewidth=0.5)\nplt.show()\nprint(\"xgboost Acc : \", predict_accuracy_on_test_set)","6f619f38":"### Plot by gender","ff4959b1":"## Naive Bayes to train","b29cbf07":"## Checking Other Algortihms for Accuracy!","bb98d8b4":"## Feature Extraction ","68036c5d":"## Rearranging Data","cb16d582":"### Remove unwanted columns","b900e214":"### Convert string to integer","3dff19b5":"# Gender Prediction\n* Rearrange Data\n* Feature Extraction\n* Naive Bayes\n* Other Algorithms ( xgboost, SVM, Random forest,..)"}}