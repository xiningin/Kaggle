{"cell_type":{"3efca65a":"code","110fcb3a":"code","569f6b06":"code","c0d46fed":"code","868ae31f":"code","f13022a0":"code","77925509":"code","80968991":"code","caa7f27a":"code","624e07ba":"code","82421f86":"code","e27a3d08":"code","f46a4286":"code","d5036027":"code","b75eb40a":"code","d5cf9a63":"code","8603e9c5":"code","2c13bc7e":"code","3a0185b1":"code","be2d2318":"code","eada0ce2":"code","c03cb642":"code","a26521ab":"code","4503d390":"code","dac70d5f":"code","9fa9586d":"code","8ef1cc87":"code","f7f54ae9":"code","dcfad353":"code","7e5d7b42":"code","016f8e64":"code","458d6c88":"code","2e0accfc":"code","e135c6e9":"code","d326d180":"code","9cda747d":"code","c009d340":"code","efc4dc6e":"code","aaddff23":"code","1326173c":"code","c40bb4c5":"code","5af7778a":"code","65cad98c":"code","06be5eba":"code","dfaae849":"code","53c4ed72":"code","020c11b9":"code","8242ccf5":"code","ed7edda9":"code","1a32a6b2":"code","2bb17e58":"code","0e31d4c6":"markdown","bc91d656":"markdown","6bf1528f":"markdown","63254a3f":"markdown","0a81d97e":"markdown","d6b909a6":"markdown","fb4e2712":"markdown","7c3a502a":"markdown","2ec0418c":"markdown","9f335329":"markdown","59e69acd":"markdown","f2e86bf9":"markdown","009d713f":"markdown","1950ac65":"markdown","452f5e0a":"markdown","2b2e4a7e":"markdown","2aec27af":"markdown","814f9bbb":"markdown","ea9d28c6":"markdown","1408b9ba":"markdown","33aaf05d":"markdown","acf3ec5e":"markdown"},"source":{"3efca65a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","110fcb3a":"df = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')","569f6b06":"df.head().T","c0d46fed":"df.describe()","868ae31f":"df.info()","f13022a0":"df.dtypes","77925509":"#  Let's check if any duplication of rows\n\ndf.duplicated().any()","80968991":"#  Dropping unnecessary columns for now Loan_ID\n\ndf = df.drop(\"Loan_ID\", axis=1)\ndf.head().T","caa7f27a":"# Splitting data into categorical and numerical.\n\ncat_data = []\nnum_data = []\n\nfor index, type in enumerate(df.dtypes):\n    if type == \"object\":\n        cat_data.append(df.iloc[:, index])\n    else:\n        num_data.append(df.iloc[:,index])","624e07ba":"cat_data = pd.DataFrame(cat_data).transpose()\ncat_data.head()","82421f86":"num_data = pd.DataFrame(num_data).transpose()\nnum_data.head()","e27a3d08":"cat_data.isna().sum()","f46a4286":"# Filling missing values with mode.\n\ncat_data = cat_data.apply(lambda x: x.fillna(x.value_counts().index[0]))\ncat_data.isna().sum()","d5036027":"#  Filling missing values in the numerical data.\n#  Firstly, let's check the stats.\nnum_data.describe()","b75eb40a":"num_data.isna().sum()","d5cf9a63":"#  For Loan amount since there is major difference between median and mean. So, let's fill the missing values with median.\n\nnum_data.LoanAmount = num_data.LoanAmount.fillna(num_data[\"LoanAmount\"].median())\nnum_data.LoanAmount.isna().sum()","8603e9c5":"# Filling remaining missing values with previously occuring value in resspective columns.\n\nnum_data.Loan_Amount_Term = num_data.Loan_Amount_Term.fillna(method=\"bfill\")\nnum_data.Credit_History = num_data.Credit_History.fillna(method=\"bfill\")","2c13bc7e":"# Rechecking, if any missing values remaining.\nnum_data.isna().sum()","3a0185b1":"sns.distplot(num_data.ApplicantIncome);","be2d2318":"sns.distplot(num_data.CoapplicantIncome);","eada0ce2":"sns.distplot(num_data.LoanAmount);","c03cb642":"#  Since above plots are not gaussian. Let's normalise them and get rid of outliers.\n\nnum_data.ApplicantIncome = np.log(num_data.ApplicantIncome)\nnum_data.CoapplicantIncome = np.log(num_data.CoapplicantIncome + 1)  # Since some values are zero to avoid log 0 =infinity.\nnum_data.LoanAmount = np.log(num_data.LoanAmount)\nnum_data.Loan_Amount_Term = np.log(num_data.Loan_Amount_Term)","a26521ab":"#   Again checking the plots.\n\nsns.distplot(num_data.ApplicantIncome);","4503d390":"sns.distplot(num_data.CoapplicantIncome);","dac70d5f":"sns.distplot(num_data.LoanAmount);","9fa9586d":"#  Married v\/s Loan status\nsns.countplot(x=\"Married\", hue=\"Loan_Status\", data=cat_data);\n\n#  There is higher chance of loan approval if you are married.","8ef1cc87":"#  Gender v\/s Loan status\nsns.countplot(x=\"Gender\", hue=\"Loan_Status\", data=cat_data);\n\n#  Most of the males have got there loans approved.","f7f54ae9":"#  Dependents v\/s Loan status\nsns.countplot(x=\"Dependents\", hue=\"Loan_Status\", data=cat_data);\n\n#  Having zero dependency increases the probability of loan approval.","dcfad353":"#  Education v\/s Loan status\nsns.countplot(x=\"Education\", hue=\"Loan_Status\", data=cat_data);\n\n#  Graduation preffered over non graduates.\n","7e5d7b42":"#  Concatenating the updated categorical and numerical data\n\ndata = pd.concat([num_data, cat_data], axis=1)\ndata.head().T","016f8e64":"from sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\n\ncols = ['Gender', 'Married', 'Education', \n        'Self_Employed', 'Property_Area', \n        'Loan_Status', 'Dependents']\nfor col in cols:\n    data[col] = LE.fit_transform(data[col])\n\ndata.head().T","458d6c88":"corr = data.corr()\nplt.figure(figsize=(15, 12))\nsns.heatmap(corr, annot=True);","2e0accfc":"#  Splitting into x and y\n\nx = data.drop([\"Loan_Amount_Term\", \"Self_Employed\", \"Loan_Status\"], axis=1)\ny = data[\"Loan_Status\"]","e135c6e9":"#  Splliting into train and test\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)","d326d180":"# \u00a0Put models in a dictionary\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import plot_confusion_matrix\n\n\n\nmodels = {'RandomForestClassifier': RandomForestClassifier(),\n          'ExtraTreesClassifier': ExtraTreesClassifier(),\n          'DecisionTreeClassifier': DecisionTreeClassifier(),\n          'LogisticRegression': LogisticRegression(),\n          'KNeighborsClassifier': KNeighborsClassifier(),\n          'SVC': SVC()\n         }\n\ndef Fit_Score(model, x_train, y_train, x_test, y_test, x, y):\n    \n    \n    np.random.seed(45)\n    model_scores = {}\n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        model_scores[name] = {\"Accuracy\": model.score(x_test, y_test),\n                              \"cv_acc\": np.mean(cross_val_score(model, x, y, cv=5, scoring=\"accuracy\"))}\n    return model_scores","9cda747d":"Scores = Fit_Score(model=models, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, x=x, y=y)\npd.DataFrame(Scores.values(), Scores.keys())","c009d340":"grid = {\"n_estimators\": np.arange(10, 1000, 50),\n       \"max_depth\": [3, 10],\n       \"min_samples_split\": np.arange(2, 20, 2),\n       \"min_samples_leaf\": np.arange(1, 20, 2)}\n#  Tunning\n\nnp.random.seed(45)\nrs_clf = RandomizedSearchCV(RandomForestClassifier(n_jobs=1), \n                            param_distributions=grid, \n                            cv=5, n_iter=15, verbose=True, refit=True)\n\nrs_clf.fit(x_train, y_train)\n","efc4dc6e":"rs_clf.best_params_","aaddff23":"rs_clf.score(x_test, y_test)","1326173c":"grid2 = {\"n_estimators\": [960],\n       \"max_depth\": [3, 10],\n       \"min_samples_split\": [12, 14],\n       \"min_samples_leaf\": [13, 12]}\n#  Tunning\nnp.random.seed(45)\n\ngs_clf = GridSearchCV(RandomForestClassifier(n_jobs=1), \n                            param_grid=grid2, \n                            cv=5, verbose=True, refit=True)\n\ngs_clf.fit(x_train, y_train)\n","c40bb4c5":"gs_clf.best_params_","5af7778a":"gs_clf.score(x_test, y_test)","65cad98c":"#  Let's try with some random params if the score increases\n\ngrid3 = {\"C\": np.logspace(-4, 4, 20),\n        \"solver\": [\"liblinear\"]}\n\n        \n#  Tunning\nnp.random.seed(45)\nrs_lr = RandomizedSearchCV(LogisticRegression(n_jobs=1), \n                            param_distributions=grid3, \n                            cv=5, n_iter=15, verbose=True, refit=True)\n\nrs_lr.fit(x_train, y_train)","06be5eba":"rs_lr.best_params_","dfaae849":"rs_lr.score(x_test, y_test)","53c4ed72":"def Analytics(model, x_train, y_train, x_test, y_test, x, y):\n    \n\n    models = {'RandomForestClassifier': RandomForestClassifier(max_depth=3, min_samples_leaf=13, \n                                                               min_samples_split=12, n_estimators=960),\n              'ExtraTreesClassifier': ExtraTreesClassifier(),\n              'DecisionTreeClassifier': DecisionTreeClassifier(),\n              'LogisticRegression': LogisticRegression(solver='liblinear', C=545.5594781168514),\n              'KNeighborsClassifier': KNeighborsClassifier(),\n              'SVC': SVC()\n              }\n    model_scores = {}\n\n    np.random.seed(45)\n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        y_preds = model.predict(x_test)\n        model_scores[name] ={\"cv_acc\": np.mean(cross_val_score(model, x, y, cv=5, scoring=\"accuracy\")),\n                             \"cv_prec\": np.mean(cross_val_score(model, x, y, cv=5, scoring=\"precision\")),\n                             \"cv_recall\": np.mean(cross_val_score(model, x, y, cv=5, scoring=\"recall\")),\n                             \"cv_f1\": np.mean(cross_val_score(model, x, y, cv=5, scoring=\"f1\"))\n                             }\n        \n             \n    return model_scores\n    ","020c11b9":"final_scores = Analytics(models, x_train, y_train, x_test, y_test, x, y)\nscores = pd.DataFrame(final_scores.values(), final_scores.keys())\nscores","8242ccf5":"LR = LogisticRegression(solver='liblinear', C=545.5594781168514)\n\nLR.fit(x_train, y_train)\nROC_Curve = plot_roc_curve(LR, x_test, y_test);\nConf_Matrix = plot_confusion_matrix(LR, x_test, y_test);\nROC_Curve, Conf_Matrix","ed7edda9":"ET = ExtraTreesClassifier()\nnp.random.seed(45)\nET.fit(x_train, y_train)\nROC_Curve = plot_roc_curve(ET, x_test, y_test);\nConf_Matrix = plot_confusion_matrix(ET, x_test, y_test);","1a32a6b2":"LR.coef_","2bb17e58":"feature_dict = dict(zip(x.columns, list(LR.coef_[0])))\nfeatures = pd.DataFrame(feature_dict, index=[0])\nfeatures.plot.bar(figsize=(10, 6));","0e31d4c6":"##### Data summary","bc91d656":"* 2. Logistic Regression","6bf1528f":"#### Since our sample is imbalanced, Precision and recall also plays an important role in addition to accuracy.\n#### Best models:\n* Logistic Regression\/Random Forest Classifier\/SVC (good accuracy)\n* Extra Trees Classifier (better precesion)\n\nlet's further explore\n\nLogistic Regression\n","63254a3f":"##### Importing initial libraries","0a81d97e":"##### Getting deep into categorical data.","d6b909a6":"b. Grid Search CV","fb4e2712":"##### Improving the models.\n* Further tunning the hyper parameters.","7c3a502a":"##### Converting categorical data to numbers.","2ec0418c":"#### Further different metrics study. \nLet's take losistic regression as the perfect model.","9f335329":"Extra Trees Classifier","59e69acd":"* Categorical data visualisation","f2e86bf9":"###### Visualising Correlation","009d713f":"* 1. Random Forest Classifier\n\na. Random Search CV","1950ac65":"### From above tunning it seems the best fit would be with accuracy of 81 %.","452f5e0a":"### Conclusion\n\n##### Models with better scores are:\n\n* RandomForestClassifier\/LogisticRegression: \n\n* cv_acc: 0.803\n* cv_prec: 0.792\n* cv_recall: 0.97\n* cv_f1: 0.870\n\nWe can also further standardise the data and even tune other models to get the better scores. \n\n##### Please do suggest some more better models. ","2b2e4a7e":"##### Interpretation:\n    * The dependent variable Loan_status has less correlation with Loan amount term and Self employed.\n    Shall drop them during modelling.\n    ","2aec27af":"A classification problem, to predict whether a loan should be approved or not.\n\nWorkflow:\n    \n    1. Importing libraries\n    2. Loading data\n    3. Summarising data\n    4. Filling missing values if any, for both categorical and numerical\n       * Categorical: Mode\n       * Numerical: Median\/Mean\/Bfill\n    5. Exploratory data analysis\n       * Data visualisation\n       * Normalisation of data if any outliers found\n    6. Conversion of object data type to numbers\n    7. Correlation between different variables. (Drop any independent variables if not coorelated to the dependent variable.)\n    8. Evaluating different models based on different metrics (Cross validated accuracy, precision, f1 score,\n                                                              recall,AUC curve, confusion matrix)\n       * Random Forest Classifier\n       * Extra Tree Classifier\n       * SVC\n       * Logistic Regression\n       * Kneighbours Classifier\n       * Decision Tree Classifier\n    9. Tuning the hyper parameters of the best models.\n    10.Feature importance","814f9bbb":"##### Statistical Overview","ea9d28c6":"###### Modelling\n","1408b9ba":"#### Feature Importance","33aaf05d":"* Distribution plots for numerical data","acf3ec5e":"##### Exploratory Data Analysis"}}