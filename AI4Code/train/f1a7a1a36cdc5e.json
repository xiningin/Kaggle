{"cell_type":{"6a623e81":"code","7316a5db":"code","19314919":"code","263f88e7":"code","805f2f3e":"code","b5d473d3":"code","997d8e3f":"code","cdfe618d":"code","2847d28c":"code","5a88f0a1":"code","1a436372":"code","99b46040":"markdown"},"source":{"6a623e81":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random\nfrom typing import Union\nfrom math import ceil\nfrom os import mkdir","7316a5db":"EOS = chr(10) # End of sentence\n\ndef build_vocabulary() -> list:\n    # builds a vocabulary using ASCII characters\n    vocabulary = [chr(i) for i in range(10, 128)]\n    return vocabulary\n\ndef word2index(vocabulary: list, word: str) -> int:\n    # returns the index of 'word' in the vocabulary\n    return vocabulary.index(word)\n\ndef words2onehot(vocabulary: list, words: list) -> np.ndarray:\n    # transforms the list of words given as argument into\n    # a one-hot matrix representation using the index in the vocabulary\n    n_words = len(words)\n    n_voc = len(vocabulary)\n    indices = np.array([word2index(vocabulary, word) for word in words])\n    a = np.zeros((n_words, n_voc))\n    a[np.arange(n_words), indices] = 1\n    return a\n\ndef sample_word(vocabulary: list, prob: np.ndarray) -> str:\n    # sample a word from the vocabulary according to 'prob'\n    # probability distribution (the softmax output of our model)\n    return np.random.choice(vocabulary, p=prob)","19314919":"class Model:\n    def __init__(self, vocabulary: list = [], a_size: int = 0):\n        self.vocab = vocabulary\n        self.vocab_size = len(vocabulary)\n        self.a_size = a_size\n        self.combined_size = self.vocab_size + self.a_size\n        \n        # weights and bias used to compute the new a\n        # (a = vector that is passes to the next time step)\n        self.wa = tf.Variable(tf.random.normal(\n            stddev=1.0\/(self.combined_size+self.a_size),\n            shape=(self.combined_size, self.a_size),\n            dtype=tf.double))\n        self.ba = tf.Variable(tf.random.normal(\n            stddev=1.0\/(1+self.a_size),\n            shape=(1, self.a_size),\n            dtype=tf.double))\n        \n        # weights and bias used to compute y (the softmax predictions)\n        self.wy = tf.Variable(tf.random.normal(\n            stddev=1.0\/(self.a_size+self.vocab_size),\n            shape=(self.a_size, self.vocab_size),\n            dtype=tf.double))\n        self.by = tf.Variable(tf.random.normal(\n            stddev=1.0\/(1+self.vocab_size),\n            shape=(1, self.vocab_size),\n            dtype=tf.double))\n        \n        self.weights = [self.wa, self.ba, self.wy, self.by]\n        self.optimizer = tf.keras.optimizers.Adam()\n    \n    def __call__(self,\n                 a: Union[np.ndarray, tf.Tensor],\n                 x: Union[np.ndarray, tf.Tensor],\n                 y: Union[np.ndarray, tf.Tensor, None] = None) -> tuple:\n        \n        a_new = tf.math.tanh(tf.linalg.matmul(tf.concat([a, x], axis=1), self.wa)+self.ba)\n        y_logits = tf.linalg.matmul(a_new, self.wy)+self.by\n        if y is None:\n            # during prediction return softmax probabilities\n            return (a_new, tf.nn.softmax(y_logits))\n        else:\n            # during training return loss\n            return (a_new, tf.math.reduce_mean(\n                        tf.nn.softmax_cross_entropy_with_logits(y, y_logits)))\n    \n    def fit(self,\n            sentences: list,\n            batch_size: int = 128,\n            epochs: int = 10) -> None:\n        \n        n_sent = len(sentences)\n        num_batches = ceil(n_sent \/ batch_size)\n        \n        for epoch in range(epochs):\n            \n            random.shuffle(sentences)\n            start = 0\n            batch_idx = 0\n            \n            while start < n_sent:\n                \n                print('Training model: %05.2f%%' %\n                      (100*(epoch*num_batches+batch_idx+1)\/(epochs*num_batches),),\n                      end='\\r')\n                \n                batch_idx += 1\n                end = min(start+batch_size, n_sent)\n                batch_sent = sentences[start:end]\n                start = end\n                batch_sent.sort(reverse=True, key=lambda s: len(s))\n                \n                init_num_words = len(batch_sent)\n                a = np.zeros((init_num_words, self.a_size))\n                x = np.zeros((init_num_words, self.vocab_size))\n                \n                time_steps = len(batch_sent[0])+1\n                \n                with tf.GradientTape() as tape:\n                \n                    losses = []\n                    for t in range(time_steps):\n                        words = []\n                        for i in range(init_num_words):\n                            if t > len(batch_sent[i]):\n                                break\n                            if t == len(batch_sent[i]):\n                                words.append(EOS)\n                                break\n                            words.append(batch_sent[i][t])\n\n                        y = words2onehot(self.vocab, words)\n                        n = y.shape[0]\n                        a, loss = self(a[0:n], x[0:n], y)\n                        losses.append(loss)\n                        x = y\n                    \n                    loss_value = tf.math.reduce_mean(losses)\n                \n                grads = tape.gradient(loss_value, self.weights)\n                self.optimizer.apply_gradients(zip(grads, self.weights))\n\n    def sample(self) -> str:\n        # sample a new sentence from the learned model\n        sentence = ''\n        a = np.zeros((1, self.a_size))\n        x = np.zeros((1, self.vocab_size))\n        while True:\n            a, y_hat = self(a, x)\n            word = sample_word(self.vocab, tf.reshape(y_hat, (-1,)))\n            if word == EOS:\n                break\n            sentence += word\n            x = words2onehot(self.vocab, [word])\n        return sentence\n    \n    def predict_next(self, sentence: str) -> str:\n        # predict the next part of the sentence given as parameter\n        a = np.zeros((1, self.a_size))\n        for word in sentence.strip():\n            x = words2onehot(self.vocab, [word])\n            a, y_hat = self(a, x)\n        s = ''\n        while True:\n            word = sample_word(self.vocab, tf.reshape(y_hat, (-1,)))\n            if word == EOS:\n                break\n            s += word\n            x = words2onehot(self.vocab, [word])\n            a, y_hat = self(a, x)\n        return s\n    \n    def save(self, name: str) -> None:\n        mkdir(f'.\/{name}')\n        with open(f'.\/{name}\/vocabulary.txt', 'w') as f:\n            f.write(','.join(self.vocab))\n        with open(f'.\/{name}\/a_size.txt', 'w') as f:\n            f.write(str(self.a_size))\n        np.save(f'.\/{name}\/wa.npy', self.wa.numpy())\n        np.save(f'.\/{name}\/ba.npy', self.ba.numpy())\n        np.save(f'.\/{name}\/wy.npy', self.wy.numpy())\n        np.save(f'.\/{name}\/by.npy', self.by.numpy())\n    \n    def load(self, name: str) -> None:\n        with open(f'.\/{name}\/vocabulary.txt', 'r') as f:\n            self.vocab = f.read().split(',')\n        with open(f'.\/{name}\/a_size.txt', 'r') as f:\n            self.a_size = int(f.read())\n            \n        self.vocab_size = len(self.vocab)\n        self.combined_size = self.vocab_size + self.a_size\n        \n        self.wa = tf.Variable(np.load(f'.\/{name}\/wa.npy'))\n        self.ba = tf.Variable(np.load(f'.\/{name}\/ba.npy'))\n        self.wy = tf.Variable(np.load(f'.\/{name}\/wy.npy'))\n        self.by = tf.Variable(np.load(f'.\/{name}\/by.npy'))\n        self.weights = [self.wa, self.ba, self.wy, self.by]","263f88e7":"df = pd.read_csv('..\/input\/million-headlines\/abcnews-date-text.csv')\ndf","805f2f3e":"vocabulary = build_vocabulary()","b5d473d3":"sentences = df['headline_text'].values.tolist()","997d8e3f":"model = Model(vocabulary, 1024)","cdfe618d":"model.fit(sentences, batch_size=4096, epochs=50)","2847d28c":"model.save('news_headlines_model')\n# model.load('news_headlines_model')","5a88f0a1":"for i in range(20):\n    print(model.sample())","1a436372":"s = 'scientists just discovered'\ns += model.predict_next(s)\ns","99b46040":"### A simple RNN language model implemented from scratch with TensorFlow\n#### It is similar to my previous notebook that used words vocabulary,\n#### but this time I tried a vocabulary of ASCII characters instead"}}