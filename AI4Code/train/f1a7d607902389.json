{"cell_type":{"c60bf4bc":"code","f0e26f5e":"code","8c3811f9":"code","21c77b8d":"code","791a0af8":"code","cb840cd9":"code","17caf791":"code","a0c6652a":"code","cb3a3347":"code","e53e966b":"code","47036b8a":"code","19e53062":"code","98967997":"code","13cc8948":"code","48dfe96a":"code","5ccaa4eb":"code","ba63e80e":"code","5a2b5215":"code","2e1690f2":"code","7304f005":"code","60b8ebf7":"code","197151c8":"code","bc65fecf":"code","ad3d03b6":"code","79797a98":"code","8d722831":"code","373b27ec":"code","2d892716":"code","07667ccc":"code","a2c8239f":"code","9d00720b":"code","f51501b7":"code","9a00abb1":"code","613bdd33":"code","58fd5e69":"code","639a01c3":"code","b2f64d16":"code","4ac1ade0":"code","c4b42e76":"code","5052266e":"code","1519ad00":"code","733e07ec":"code","473ff2ed":"code","4f9e59d0":"code","bb256798":"code","0263b1cf":"code","31ed2f1d":"code","e2e2290d":"code","96a4b29b":"code","9dafc025":"code","71197890":"code","a73be355":"code","5a92f888":"code","a42ea589":"code","5916b404":"code","e757ae4f":"code","d66961fe":"code","bf3ec266":"code","6c80a59c":"code","fe845947":"code","eaa400b5":"code","e75ae3af":"code","485e7312":"markdown","fc3fbff0":"markdown","96b74d10":"markdown","59d222ad":"markdown","999e75b9":"markdown","003606d4":"markdown","3ddd6d79":"markdown","7c6f5154":"markdown","b748ee3b":"markdown","640b2ef0":"markdown","7cfb9ebf":"markdown","cac7bccd":"markdown","6526f259":"markdown","e6cde016":"markdown","352b05cc":"markdown","7a5f1efa":"markdown","3f53bff3":"markdown","53bd463d":"markdown","7f751fc1":"markdown","003bcf48":"markdown","2ef1efee":"markdown","7c300558":"markdown","def73860":"markdown","3c1e5acf":"markdown","8045f7b6":"markdown","99761b93":"markdown","d028dfa0":"markdown","00d304ef":"markdown","a6528911":"markdown","7b6a2cf3":"markdown","6093fb1a":"markdown","42622557":"markdown","0df1613b":"markdown","c7df2daf":"markdown","b57f222d":"markdown","608caff1":"markdown","901ccaea":"markdown","f28a709d":"markdown","8233565d":"markdown","e934f398":"markdown","03fecf06":"markdown","f5ee0f12":"markdown","f9718514":"markdown","f6531f30":"markdown","6b14e722":"markdown","acedf560":"markdown","242c7fd2":"markdown","83702769":"markdown","0b7e8b77":"markdown"},"source":{"c60bf4bc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom datetime import datetime\nsns.set_style('darkgrid')\n\nRED = '#D10000'\nBLUE = '#0082FF'","f0e26f5e":"data = pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')\ndata.head()","8c3811f9":"# get the column list\ndata.info()","21c77b8d":"# first blood\nall(data['blueFirstBlood'] == data['redFirstBlood'].apply(lambda x: 0 if x == 1 else 1))","791a0af8":"# blue kills is red deaths\nall(data['blueKills'] == data['redDeaths'])","cb840cd9":"# blue deaths is red kills\nall(data['blueDeaths'] == data['redKills'])","17caf791":"# blue experience difference is the negative of red experience difference\nall(data['blueExperienceDiff'] == data['redExperienceDiff'].apply(lambda x: -1*x))","a0c6652a":"# blue gold difference is the negative of red gold difference\nall(data['blueGoldDiff'] == data['redGoldDiff'].apply(lambda x: -1*x))","cb3a3347":"data.drop(['redFirstBlood','redDeaths','redKills','redExperienceDiff','redGoldDiff', 'gameId'],\n          axis = 1, inplace = True)","e53e966b":"blue_team = [column for column in data.columns if 'blue' in column]\nred_team = [column for column in data.columns if 'red' in column]","47036b8a":"data[blue_team].hist(color = BLUE,\n                     figsize = (10,10))\nplt.tight_layout()\nplt.show()","19e53062":"data[red_team].hist(color = RED,\n                    figsize = (10,10))\n\nplt.tight_layout()\nplt.show()","98967997":"# collect the columns that are \"normally distributed\".\nnormal_columns = ['blueWardsPlaced','blueWardsDestroyed','blueKills','blueDeaths',\n                  'blueAssists','blueTotalGold','blueAvgLevel','blueTotalExperience',\n                  'blueTotalMinionsKilled', 'blueTotalJungleMinionsKilled',\n                  'blueGoldDiff','blueExperienceDiff','blueCSPerMin','blueGoldPerMin',\n                  'redWardsPlaced','redAssists','redTotalGold','redAvgLevel','redTotalExperience',\n                  'redTotalMinionsKilled','redTotalJungleMinionsKilled',\n                  'redCSPerMin','redGoldPerMin']\n\n# we need to group the columns with similar range so that the boxplots look interpretable\nblue_normal = [column for column in normal_columns if 'blue' in column]\ndata[blue_normal].describe().T","13cc8948":"# group the blue columns by relative size\nsmall_blue = ['blueWardsPlaced','blueWardsDestroyed','blueKills',\n              'blueDeaths','blueAssists','blueAvgLevel','blueTotalMinionsKilled',\n              'blueTotalJungleMinionsKilled','blueCSPerMin']\nlarge_blue = [column for column in blue_normal if column not in small_blue]","48dfe96a":"def plot_boxplot(data, subset, color, title):\n    plt.figure(figsize = (11,9))\n    g = sns.boxplot(data = data[subset], color = color)\n    g.set_title(title)\n    plt.xticks(rotation = 30)\n    plt.show()","5ccaa4eb":"# now we visualize\nplot_boxplot(data, small_blue, BLUE, 'Outlier Analysis for Blue Team with Small Values')","ba63e80e":"plot_boxplot(data, large_blue, BLUE, 'Outlier Analysis for Blue Team with Large Values')","5a2b5215":"def remove_outliers(data):\n    \"\"\"\n    Remove the outlier if it is 3 standard deviations away from the mean\n    \"\"\"\n    for column in data.columns:\n        column_mean = data[column].mean()\n        column_std = data[column].std()\n        threshold = column_std*3 # three standard deviations\n        lower, upper = column_mean - threshold, column_mean + threshold\n        data = data[(data[column] >= lower) & (data[column] <= upper)]\n    return data","2e1690f2":"# remove the outliers for the blue team\ndata_no_outliers = remove_outliers(data)","7304f005":"# now we visualize after removing outliers\nplot_boxplot(data_no_outliers, small_blue, BLUE, 'Boxplot After Removing Statistical Outliers for Blue Team with Small Values')","60b8ebf7":"plot_boxplot(data_no_outliers, large_blue, BLUE, 'Boxplot After Removing Statistical Outliers for Blue Team with Large Values')","197151c8":"red_normal = [column for column in normal_columns if 'red' in column]\ndata[red_normal].describe().T","bc65fecf":"small_red = ['redWardsPlaced','redAssists','redAvgLevel',\n             'redTotalMinionsKilled','redTotalJungleMinionsKilled', \n             'redCSPerMin']\nlarge_red = [column for column in red_normal if column not in small_red]","ad3d03b6":"# visualize small red\nplot_boxplot(data, small_red, RED, 'Outlier Analysis for Red Team with Small Values')","79797a98":"plot_boxplot(data, large_red, RED, 'Outlier Analysis for Red Team with Large Values')","8d722831":"plot_boxplot(data_no_outliers, small_red, RED, 'Boxplot After Removing Statistical Outliers for Red Team with Small Values')","373b27ec":"plot_boxplot(data_no_outliers, large_red, RED, 'Boxplot After Removing Statistical Outliers for Red Team with Large Values')","2d892716":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndata_tr = pd.DataFrame(scaler.fit_transform(data_no_outliers.drop('blueWins', axis = 1)), \n                       columns = data_no_outliers.drop('blueWins', axis = 1).columns)\n\n# add the bluewins column back in\ndata_tr['blueWins'] = data_no_outliers['blueWins']","07667ccc":"# verify that the columns are scaled\ndata_tr.describe().T","a2c8239f":"data_tr.dropna(inplace = True, axis = 0)\ndata_tr.describe().T","9d00720b":"def FormatIt(delta):\n    \"\"\"\n    Input is a datetime object. We will format it based on how many\n    seconds it contains\n    \"\"\"\n    message = 'Time elapsed is '\n    # format the seconds\n    if (delta.seconds == 0) & (delta.microseconds < 1000000):\n        print(message + '{} microseconds'.format(delta.microseconds))\n  \n    elif delta.seconds < 60:\n        print(message + '{} seconds'.format(delta.seconds))\n    \n    elif delta.seconds < 3600:\n        minutes = int(delta.seconds \/ 60)\n        seconds = delta.seconds % 60\n        print(message + '{} minutes {} seconds'.format(minutes, seconds))\n      \n    else:\n        hours = int(delta.seconds \/ 3600)\n        minutes = int(delta.seconds \/ 60)\n        seconds = delta.seconds % 60\n        print(message + '{} hours {} minutes {} seconds'.format(hours, minutes, seconds))\n\nclass TimeIt():\n    \"\"\"\n    So we can find out how long the algorithms take to run\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Get the current time\n        \"\"\"\n        self.start = datetime.now()\n    \n    def stop(self):\n        self.end = datetime.now()\n        self.delta = self.end - self.start\n        FormatIt(self.delta)\n        \nclass Clusters():\n    \"\"\"\n    So that we can plot the clusters of each algorithm easily\n    \"\"\"\n    def __init__(self, cluster_name, cluster_list, data):\n        self.cluster_name = cluster_name\n        self.cluster_list = cluster_list\n        self.cluster_labels = [model.labels_ for model in self.cluster_list]\n        self.data = data.copy()\n                \n    def plot(self):\n        fig, axs = plt.subplots(2,2, figsize = (11,9))\n        \n        # add the first cluster label to the df\n        self.data.insert(0, 'Clusters', self.cluster_labels[0])\n        \n        # first scatter plot\n        g = sns.scatterplot(data = self.data,\n                            x = 'blueTotalGold',\n                            y = 'redTotalGold',\n                            hue = 'Clusters',\n                            alpha = 0.5,\n                            edgecolor = 'k',\n                            palette = 'viridis',\n                            ax = axs[0][0])\n        \n        # replace the cluster labels for the next model\n        self.data['Clusters'] = self.cluster_labels[1]\n        \n        # second scatter plot\n        g = sns.scatterplot(data = self.data,\n                            x = 'blueTotalGold',\n                            y = 'redTotalGold',\n                            hue = 'Clusters',\n                            alpha = 0.5,\n                            edgecolor = 'k',\n                            palette = 'viridis',\n                            ax = axs[0][1])\n        \n        # replace the cluster labels for the next model\n        self.data['Clusters'] = self.cluster_labels[2]\n        \n        # third scatter plot\n        g = sns.scatterplot(data = self.data,\n                            x = 'blueTotalGold',\n                            y = 'redTotalGold',\n                            hue = 'Clusters',\n                            alpha = 0.5,\n                            edgecolor = 'k',\n                            palette = 'viridis',\n                            ax = axs[1][0])\n        \n        # replace the cluster labels for the next model\n        self.data['Clusters'] = self.cluster_labels[3]\n        \n        # fourth scatter plot\n        g = sns.scatterplot(data = self.data,\n                            x = 'blueTotalGold',\n                            y = 'redTotalGold',\n                            hue = 'Clusters',\n                            alpha = 0.5,\n                            edgecolor = 'k',\n                            palette = 'viridis',\n                            ax = axs[1][1])\n        \n        title = 'Scatterplots for {}'.format(self.cluster_name)\n        fig.suptitle(title, fontsize = 16)\n        fig.tight_layout()\n        fig.subplots_adjust(top=0.88)\n        plt.show()","f51501b7":"# make a copy so we don't change the original df\ndata_copy = data_tr.copy()\ndata_not_scaled = pd.DataFrame(scaler.inverse_transform(data_tr.drop('blueWins', axis = 1)),\n                               columns = data_tr.drop('blueWins', axis = 1).columns)\ndata_not_scaled['blueWins'] = data_tr['blueWins']","9a00abb1":"from sklearn.cluster import KMeans\n\ntimer = TimeIt()\nk_means_list = [KMeans(n_clusters = k, random_state = 42).fit(data_copy.values) for k in range(2,6)]\ntimer.stop()","613bdd33":"# now we plot it\nkmeans_cluster = Clusters('K-Means', k_means_list, data_not_scaled)\nkmeans_cluster.plot()","58fd5e69":"from sklearn.cluster import AffinityPropagation\n\ntimer = TimeIt()\nAP_list = [AffinityPropagation(damping = 0.5 + i*0.05, random_state = 42).fit(data_copy.values) for i in range(4)]\ntimer.stop()","639a01c3":"# now we plot\nAP_clusters = Clusters('Affinity Propagation', AP_list, data_not_scaled)\nAP_clusters.plot()","b2f64d16":"from sklearn.cluster import MeanShift\n\ntimer = TimeIt()\nMS_list = [MeanShift(bandwidth = 1.25*i).fit(data_copy.values) for i in range(1,5)]\ntimer.stop()","4ac1ade0":"MS_clusters = Clusters('Mean Shift', MS_list, data_not_scaled)\nMS_clusters.plot()","c4b42e76":"from sklearn.cluster import SpectralClustering\n\ntimer = TimeIt()\nSC_list = [SpectralClustering(random_state = 42, n_clusters = i).fit(data_copy.values) for i in range(2,6)]\ntimer.stop()","5052266e":"SC_clusters = Clusters('Spectral Clustering', SC_list, data_not_scaled)\nSC_clusters.plot()","1519ad00":"from sklearn.cluster import AgglomerativeClustering","733e07ec":"timer = TimeIt()\nWard_list = [AgglomerativeClustering(n_clusters = i, linkage = 'ward').fit(data_copy.values) for i in range(2,6)]\ntimer.stop()","473ff2ed":"Ward_clusters = Clusters('Ward Clustering', Ward_list, data_not_scaled)\nWard_clusters.plot()","4f9e59d0":"timer = TimeIt()\nComplete_list = [AgglomerativeClustering(n_clusters = i, linkage = 'complete').fit(data_copy.values) for i in range(2,6)]\ntimer.stop()","bb256798":"Complete_clusters = Clusters('Complete Clustering', Complete_list, data_not_scaled)\nComplete_clusters.plot()","0263b1cf":"timer = TimeIt()\nAverage_list = [AgglomerativeClustering(n_clusters = i, linkage = 'average').fit(data_copy.values) for i in range(2,6)]\ntimer.stop()","31ed2f1d":"Average_clusters = Clusters('Average Clustering', Average_list, data_not_scaled)\nAverage_clusters.plot()","e2e2290d":"timer = TimeIt()\nSingle_list = [AgglomerativeClustering(n_clusters = i, linkage = 'single').fit(data_copy.values) for i in range(2,6)]\ntimer.stop()","96a4b29b":"Single_clusters = Clusters('Single Clustering', Single_list, data_not_scaled)\nSingle_clusters.plot()","9dafc025":"from sklearn.cluster import DBSCAN\n\ntimer = TimeIt()\nDB_list = [DBSCAN(eps = 2 + 0.25*i).fit(data_copy.values) for i in range(1,5)]\ntimer.stop()","71197890":"DB_clusters = Clusters('DBSCAN Clustering', DB_list, data_not_scaled)\nDB_clusters.plot()","a73be355":"from sklearn.cluster import OPTICS\n\ntimer = TimeIt()\nopt_list = [OPTICS(min_samples = i).fit(data_copy.values) for i in range(2,6)]\ntimer.stop()","5a92f888":"opt_clusters = Clusters('OPTICS Clustering', opt_list, data_not_scaled)\nopt_clusters.plot()","a42ea589":"from sklearn.cluster import Birch\n\ntimer = TimeIt()\nbirch_list = [Birch(n_clusters = i).fit(data_copy.values) for i in range(2,6)]\ntimer.stop()","5916b404":"birch_clusters = Clusters('Birch Clustering', birch_list, data_not_scaled)\nbirch_clusters.plot()","e757ae4f":"data_not_scaled['blueVisionScore'] = data_not_scaled['blueWardsPlaced'] + data_not_scaled['redWardsDestroyed']\ndata_not_scaled['redVisionScore'] = data_not_scaled['redWardsPlaced'] + data_not_scaled['blueWardsDestroyed']\n# make sure tha columns were added\ndata_not_scaled.columns","d66961fe":"\"\"\"\nI am hiding this cell and the following cell because they don't look clean.\nIf anyone has an idea of how to produce these columns in a better way,\nplease let me know!\n\"\"\"\n# first we define a column that contains points for how advantaged the team is\ntimer = TimeIt()\ndata_not_scaled['blueAdvantagePoints'] = [0]*data_not_scaled.shape[0]\nfor ind in range(data_not_scaled.shape[0]):\n    data_not_scaled.loc[ind ,'blueAdvantagePoints'] = int(data_not_scaled['blueKills'].iloc[ind] > data_not_scaled['blueDeaths'].iloc[ind]) + int(data_not_scaled['blueAssists'].iloc[ind] > data_not_scaled['redAssists'].iloc[ind]) + int(data_not_scaled['blueEliteMonsters'].iloc[ind] > data_not_scaled['redEliteMonsters'].iloc[ind]) + int(data_not_scaled['blueDragons'].iloc[ind] > data_not_scaled['redDragons'].iloc[ind]) + int(data_not_scaled['blueHeralds'].iloc[ind] > data_not_scaled['redHeralds'].iloc[ind]) + int(data_not_scaled['redTowersDestroyed'].iloc[ind] > data_not_scaled['blueTowersDestroyed'].iloc[ind]) + int(data_not_scaled['blueAvgLevel'].iloc[ind] > data_not_scaled['redAvgLevel'].iloc[ind]) + int(data_not_scaled['blueTotalMinionsKilled'].iloc[ind] > data_not_scaled['redTotalMinionsKilled'].iloc[ind]) + int(data_not_scaled['blueTotalJungleMinionsKilled'].iloc[ind] > data_not_scaled['redTotalJungleMinionsKilled'].iloc[ind]) + int(data_not_scaled['blueGoldDiff'].iloc[ind] > 0) + int(data_not_scaled['blueExperienceDiff'].iloc[ind] > 0) + int(data_not_scaled['blueCSPerMin'].iloc[ind] > data_not_scaled['redCSPerMin'].iloc[ind]) + int(data_not_scaled['blueGoldPerMin'].iloc[ind] > data_not_scaled['redGoldPerMin'].iloc[ind]) + int(data_not_scaled['blueVisionScore'].iloc[ind] > data_not_scaled['redVisionScore'].iloc[ind])\ntimer.stop()","bf3ec266":"# now the red team\ntimer = TimeIt()\ndata_not_scaled['redAdvantagePoints'] = [0]*data_not_scaled.shape[0]\nfor ind in range(data_not_scaled.shape[0]):\n    data_not_scaled.loc[ind ,'redAdvantagePoints'] = int(data_not_scaled['blueKills'].iloc[ind] < data_not_scaled['blueDeaths'].iloc[ind]) + int(data_not_scaled['blueAssists'].iloc[ind] < data_not_scaled['redAssists'].iloc[ind]) + int(data_not_scaled['blueEliteMonsters'].iloc[ind] < data_not_scaled['redEliteMonsters'].iloc[ind]) + int(data_not_scaled['blueDragons'].iloc[ind] < data_not_scaled['redDragons'].iloc[ind]) + int(data_not_scaled['blueHeralds'].iloc[ind] < data_not_scaled['redHeralds'].iloc[ind]) + int(data_not_scaled['redTowersDestroyed'].iloc[ind] < data_not_scaled['blueTowersDestroyed'].iloc[ind]) + int(data_not_scaled['blueAvgLevel'].iloc[ind] < data_not_scaled['redAvgLevel'].iloc[ind]) + int(data_not_scaled['blueTotalMinionsKilled'].iloc[ind] < data_not_scaled['redTotalMinionsKilled'].iloc[ind]) + int(data_not_scaled['blueTotalJungleMinionsKilled'].iloc[ind] < data_not_scaled['redTotalJungleMinionsKilled'].iloc[ind]) + int(data_not_scaled['blueGoldDiff'].iloc[ind] < 0) + int(data_not_scaled['blueExperienceDiff'].iloc[ind] < 0) + int(data_not_scaled['blueCSPerMin'].iloc[ind] < data_not_scaled['redCSPerMin'].iloc[ind]) + int(data_not_scaled['blueGoldPerMin'].iloc[ind] < data_not_scaled['redGoldPerMin'].iloc[ind]) + int(data_not_scaled['blueVisionScore'].iloc[ind] < data_not_scaled['redVisionScore'].iloc[ind])\ntimer.stop()","6c80a59c":"# let's look at the columns we just created\ndata_not_scaled.loc[:5, ('blueAdvantagePoints', 'redAdvantagePoints')]","fe845947":"# finally, we make a column defining whether blue team has the advantage or not.\ntimer = TimeIt()\ndata_not_scaled['blueAdvantage'] = [np.nan]*data_not_scaled.shape[0]\nfor ind in range(data_not_scaled.shape[0]):\n    if data_not_scaled.loc[ind, 'blueAdvantagePoints'] > data_not_scaled.loc[ind, 'redAdvantagePoints']:\n        data_not_scaled.loc[ind, 'blueAdvantage'] = 1\n    elif data_not_scaled.loc[ind, 'blueAdvantagePoints'] < data_not_scaled.loc[ind, 'redAdvantagePoints']:\n        data_not_scaled.loc[ind, 'blueAdvantage'] = 0\n    else:\n        data_not_scaled.loc[ind, 'blueAdvantage'] = np.nan\ntimer.stop()","eaa400b5":"data_not_scaled.loc[:5, ('blueAdvantagePoints', 'redAdvantagePoints', 'blueAdvantage')]","e75ae3af":"# I suspect it is around 20-25%\nblue_loss_with_advantage = data_not_scaled[(data_not_scaled['blueWins'] == 0) & (data_not_scaled['blueAdvantage'] == 1)].shape[0]\/data_not_scaled.shape[0]\nred_loss_with_advantage = data_not_scaled[(data_not_scaled['blueWins'] == 1) & (data_not_scaled['blueAdvantage'] == 0)].shape[0]\/data_not_scaled.shape[0]\n\nprint('Percent of games where blue lost with the advantage: {:.2f}%\\n'.format(blue_loss_with_advantage*100))\nprint('Percent of games where red lost with the advantage: {:.2f}%'.format(red_loss_with_advantage*100))","485e7312":"<a id = 'birch'><\/a>\n## Birch\n\nAccording to the sklearn documentation: \"The Birch builds a tree called the Clustering Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Clustering Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Clustering Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.\"","fc3fbff0":"<a id = 'single'><\/a>\n### Single Clustering","96b74d10":"[back to top](#top)","59d222ad":"<a id = 'intro'><\/a>\n# Introduction\n\nLeague of Legends is an online-multiplayer video game consisting of two, five player teams. The objective of the game is to destroy the enemy team's nexus. To gain an advantage, teams collect gold through kills and objectives. The game is highly competitive and requires teamwork.\n\nIn this analysis, we will look at data consisting of the first 10 minutes of Diamond ranked games. We will use Sklearn to cluster the data using all the available clustering algorithms. The purpose of this project is to familiarize myself with the Sklean clustering API and to show others how simple it is to use it.","999e75b9":"In both of the boxplots above, we can see many outliers. As mentioned before, we will remove data points that are 3 standard deviations away from the mean.","003606d4":"[back to top](#top)","3ddd6d79":"[back to top](#top)","7c6f5154":"Now we gather the blue and red team columns to get a rough idea of their distributions.","b748ee3b":"[back to top](#top)","640b2ef0":"To create the advantage column, we should compare (blue kills and red kills), \n(blue assists and red assists), (blue elite monsters and red elite monsters), \n(blue dragons and red dragons), (blue heralds and red heralds),\n(blue towers destroyed and red towers destroyed),\n(blue avg level and red avg level),\n(blue total minions killed and red total minions killed), (blue jungle minions killed and red jungle minions killed),\n(gold difference), (experience difference), (blue cs per min and red cs per min), (blue gold per min and red gold per min),\nand (vision score)","7cfb9ebf":"[back to top](#top)","cac7bccd":"<a id = 'remove_outliers'><\/a>\n## Remove Outliers\n\nNext, we will visualize the features using boxplots and remove any statistical outliers. For this analysis, we will keep any data points that are within 3 standard deviations from the mean.\n\nWe start with the blue team.","6526f259":"We can see that this dataset consists of 40 columns. Some of the columns are named differently, but contain the same information. For example, the \"blueKills\" column should be the same as the \"redDeaths\" column. Afterall, if a blue team gets a kill, then a red team member must have been killed.\n\n<a id = 'repeated_columns'><\/a>\n## Remove Repeated Columns\n\nNext we can see a list of potential repeated columns:\n* blueFirstBlood is the opposite of redFirstBlood. If blue team gets first blood, the column is assigned a 1 and 0 for the red team. The opposite is true for red team.\n* blueKills is redDeaths\n* blueDeaths is redKills\n* blueExperienceDiff is the negative of redExperienceDiff\n* blueGoldDiff is the negative redGoldDiff\n\nBelow we verify that these relationships are true.","e6cde016":"<a id = 'complete'><\/a>\n### Complete Clustering","352b05cc":"[back to top](#top)","7a5f1efa":"<a id = 'advantage'><\/a>\n## Team Advantage Analysis\n\nBefore we end this notebook, I wanted to spend time to create a column for which team has the advantage. I will define advantage as the team that has the higher combined stats after the first 10 minutes into the game. For example: we will say that blue team has the advantage in a game if they have more kills, less deaths, more gold, more experience, etc.\n\nWe begin by defining a column for the team's vision score. In the game, vision is controlled by placing vision wards that allow the team to see an area on the map that was previously hidden. The enemy team can destroy wards and deny visibility to some areas on the map. Having better vision of the map gives the team an advantage, so I think it is necessary to define this column.","3f53bff3":"<a id = 'top'><\/a>\n# Table of Contents\n\n* [Introduction](#intro)\n\n* [Data Cleaning](#cleaning)\n    * [Remove Repeated Columns](#repeated_columns)\n    * [Remove Outliers](#remove_outliers)\n    * [Rescaling Features](#rescaling)\n    * [Remove Null Values](#null)\n   \n* [Clustering](#clusters)\n    * [K-Means](#kmeans)\n    * [Affinity Propagation](#affinity)\n    * [Mean Shift](#mean_shift)\n    * [Spectral Clustering](#spectral)\n    * [Agglomerative Clustering](#agglomerative)\n        * [Ward](#ward)\n        * [Complete](#complete)\n        * [Average](#average)\n        * [Single](#single)\n    * [DSCAN](#dbscan)\n    * [OPTICS](#optics)\n    * [Birch](#birch)\n\n* [Clustering Discussion](#discussion)\n* [Team Advantage Column](#advantage)","53bd463d":"<a id = 'average'><\/a>\n### Average Clustering","7f751fc1":"<a id = 'mean_shift'><\/a>\n## Mean Shift\n\nAccording to the sklearn documentation, \"MeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region.\"\n\nThe hyperparameter for mean-shift is the bandwidth. We will run 4 different Mean Shift algorithms with different values for the bandwidth.","003bcf48":"[back to top](#top)","2ef1efee":"From the histogram above, we can see that the columns that are approximately normal are blueKills, blueDeaths, blueAssists, blueTotalGold, blueAvgLevel, blueTotalExperience, blueTotalMinionsKilled, blueTotalJungleMinionsKilled, blueGoldDiff, blueExperienceDiff, blueCSPerMin and blueGoldPerMin.","7c300558":"<a id = 'spectral'><\/a>\n## Spectral Clustering\n\nAccording to the sklearn documentation: \"Spectral Clustering performs a low-dimensional embedding of the affinity matrix between samples, followed by clustering, e.g., by KMeans, of the components of the eigenvectors in the low dimensional space\".\n\nThe hypyerparameter for spectral clustering is number of clusters. We will run 4 different Spectral Clustering algorithms with 2,3,4 and 5 for the number of clusters.","def73860":"<a id = 'optics'><\/a>\n## OPTICS\n\nAccording to the sklearn documentation: \"The OPTICS algorithm shares many similarities with the DBSCAN algorithm, and can be considered a generalization of DBSCAN that relaxes the eps requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, which assigns each sample both a reachability_ distance, and a spot within the cluster ordering_ attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of inf set for max_eps, then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given eps value using the cluster_optics_dbscan method. Setting max_eps to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points.\"","3c1e5acf":"<a id = 'dbscan'><\/a>\n## DBSCAN\nAccording to the sklearn documentation: \"The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, min_samples and eps, which define formally what we mean when we say dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster.\"","8045f7b6":"From the histogram above, we can see similar normal distributions in the red team columns: redAssists, redTotalGold, redAvgLevel, redTotalExperience, redTotalMinionsKilled, redTotalJungleMinionsKilled, redCSPerMin, and redGoldPerMin.","99761b93":"Let's visualize the same features after the outliers have been removed.","d028dfa0":"<a id = 'discussion'><\/a>\n## Clustering Discussion\n\nOf all of the clustering algorithms that we saw, the one that clustered the games best was the K-Means Clustering algorithm. This algorithm converged to a solution for all four cases in 6 seconds, which was much faster than the other clustering algorithms.\n\nIf we take a closer look at the documentation, we can see that SKlean mentions that the algorithm works best when we suspect that there are a small amount of clusters.","00d304ef":"Let's do the same thing to the red team.","a6528911":"<a id = 'kmeans'><\/a>\n## K-Means Clustering\n\nAccording to the Sklearn documentation, \"The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.\"\n\nWe will run 4 different K-Means Clustering algorithms using 2,3,4, and 5 clusters.","7b6a2cf3":"Now we remove the outliers.","6093fb1a":"<a id = 'ward'><\/a>\n### Ward Clustering","42622557":"[back to top](#top)","0df1613b":"<a id = 'affinity'><\/a>\n## Affinity Propagation\n\nFrom wikipedia, \"affinity propagation is a clustering algorithm based on the concept of 'message passing' between data points. Unlike clustering algorithms such as k-means or k-medoids, affinity propagation does not require the number of clusters to be determined or estimated before running the algorithm. Similar to k-mediods, affinity propagation finds 'exemplars', members of the input set that are representative of clusters.\"\n\nhttps:\/\/en.wikipedia.org\/wiki\/Affinity_propagation\n\n\nWe will run 4 different Affinity Propagation algorithms with different values for the damping hyperparameter.","c7df2daf":"[back to top](#top)","b57f222d":"[back to top](#top)","608caff1":"[back to top](#top)","901ccaea":"<a id = 'agglomerative'><\/a>\n## Agglomerative Clustering\n\nAccording to the sklearn documentation: \"Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample.\"\n\nWe will be experimenting with number of clusters and the linkage parameters. The different linkage parameters are: ward, complete, average, and single.","f28a709d":"The decrease the amount of features, we will drop the repeated columns that contain red team information. We are keeping the blue team columns just because of the \"blueWins\" feature. We will also drop the \"gameId\", since it does not contribute to the characteristics of the games.","8233565d":"[back to top](#top)","e934f398":"<a id = 'null'><\/a>\n## Remove Null-Values\n\nWe can see above that the blueWins column has 6330 non-null values compared to the other features which have 7888 non-null values. Some of the values were dropped. We will remove the columns that contain null values.","03fecf06":"<a id = 'cleaning'><\/a>\n# Data Cleaning\n\nWe begin our analysis with reading the csv file and inspecting its columns.","f5ee0f12":"[back to top](#top)","f9718514":"Now that we have the advantage column defined, we can see what percent of games blue team lost when they had the advantage.","f6531f30":"[back to top](#top)","6b14e722":"[back to top](#top)","acedf560":"[back to top](#top)","242c7fd2":"<a id = 'rescaling'><\/a>\n## Rescaling Features\n\nWe will use Sklearn's StandardScaler to rescale the data.","83702769":"<a id = 'clusters'><\/a>\n# Clustering\n\nSklearn's API allows us to build many unsupervised learning models easily. For this analysis, we will use all of the unsupervised learning algorithms that sklearn supports. This includes: K-Means, Affinity Propagation, Mean Shift, Spectral Clustering, Agglomerative Clustering, DBSCAN, OPTICS, BIRCH, and Gaussian Mixtures. A summary of how each clustering algorithm works is found in the documentation [here](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html).\n\n\nI would like to see how long it takes to run these clustering algorithms. Below we define a function that will time the algortihms for us.\n\nFor each clustering algorithm, we will plot the Blue Team's total gold earned versus the Red Team's.","0b7e8b77":"[back to top](#top)"}}