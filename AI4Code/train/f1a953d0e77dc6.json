{"cell_type":{"d2feb025":"code","7b472ea7":"code","486d87fe":"code","e8c62026":"code","2725c794":"code","752155da":"code","78ee7aeb":"code","4f9d3212":"code","83930339":"code","d9f5cf71":"code","865545aa":"code","efd8d075":"code","87e9cf15":"code","3153e22b":"code","3ceb15b0":"code","4e7fa6e9":"code","6a93118c":"code","534d3a3b":"code","1fd0ed40":"code","ebedd840":"code","daf15097":"code","95a22f3d":"code","61058cee":"code","b50bb81b":"code","dffc9600":"code","e7768f7b":"code","89ac7bb3":"code","e4e16415":"code","23b21d00":"code","403ec5c0":"code","219bb6e4":"code","aee47156":"code","09a7ff3c":"code","856d4a3e":"code","02d866e2":"code","5fc4c03d":"code","28dbb4aa":"code","f73a6f82":"code","709c2092":"code","9b85ebd7":"code","3d84f8c7":"code","26e2705d":"code","5a1c8596":"code","842452b6":"code","856b8f2f":"code","e0da2310":"code","bfdd94e2":"code","64ee33e2":"code","06d4ac4b":"code","2def0e23":"code","1187fd20":"code","c6c129da":"code","204c59bd":"code","cc4b6e4e":"code","4385a06c":"code","8c0e1235":"code","9aba1f65":"code","0e9f1142":"code","ccac6d22":"code","47dcf02a":"code","e095da40":"code","febc3b6d":"code","2ca71ed4":"code","2a874294":"code","8a606e3f":"code","78aa00ad":"code","e82fa58a":"code","5415900e":"code","e93a954b":"code","02d85f9a":"code","7175471c":"code","e14ec7ac":"code","9ca79b3a":"code","740c8aed":"code","94fd3fa7":"code","93fe190c":"code","8208db60":"code","4d7426ec":"code","26768841":"code","abc8259b":"code","42ffc676":"code","d7c50a11":"code","70a7b0d6":"code","9414a21b":"code","c851ba5b":"code","1336b220":"code","fd5fb156":"code","94e1f3e6":"code","b463619f":"code","774464c8":"code","f7707033":"code","7bb26cea":"code","57ae5750":"code","56020594":"code","2cecc012":"code","741caf1f":"code","09bc5ba4":"code","cc5b983e":"code","a41bd434":"code","c09bf4c1":"markdown","f395ed51":"markdown","9333830b":"markdown","3cecc4b1":"markdown","0960a4b7":"markdown","045f4ab1":"markdown","af86c614":"markdown","c975dbf0":"markdown","c15d4220":"markdown","846e9d57":"markdown","52388cf7":"markdown","43c98198":"markdown","7ef9c2ec":"markdown","94c22ab0":"markdown","3651c0a9":"markdown","5af7e260":"markdown","b497dd7d":"markdown","8d7d2132":"markdown","a71261bb":"markdown","6e11dab2":"markdown","57470764":"markdown","0598097d":"markdown","fbad45ba":"markdown","64d0bda8":"markdown","911480c6":"markdown","dfab82b6":"markdown","a2775f2a":"markdown","a30715dc":"markdown"},"source":{"d2feb025":"import warnings\nimport os \n\nfrom collections import defaultdict\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\nimport networkx as nx\nimport scipy.cluster.hierarchy as sch\nfrom scipy.cluster.hierarchy import fcluster\n\nimport random\n\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import functional as F\n\n# install datatable\n!pip install datatable\nimport datatable as dt\n\nfrom numba import njit\n\nimport gc\n\nwarnings.simplefilter(action=\"ignore\")\n\nproject_home = \"\/kaggle\/input\/jane-street-market-prediction\"","7b472ea7":"def seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n    torch.backends.cudnn.benchmark = False\n#     torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.deterministic = False\n    \nseed_torch()","486d87fe":"train_file = os.path.join(project_home,'train.csv')\nfeatures_file = os.path.join(project_home,'features.csv')\nexample_test_file = os.path.join(project_home,'example_test.csv')\nexample_sample_submission_file = os.path.join(project_home,'example_sample_submission.csv')\n\ntrain_data_datatable = dt.fread(train_file)\n\ndf_train = train_data_datatable.to_pandas()\ndf_features = pd.read_csv(features_file)\ndf_example_test = pd.read_csv(example_test_file)\ndf_example_sample_submission = pd.read_csv(example_sample_submission_file)","e8c62026":"features = [ col for col in df_train.columns if \"feature\" in col ]\nresps = [ col for col in df_train.columns if \"resp\" in col ]\ntarget_resp = [resp_ for resp_ in resps if \"_\" not in resp_]\ntarget = [\"weight\"] + target_resp + features ","2725c794":"df_train.info()","752155da":"\"\"\"\nReduce Memory Usage by 75%\nhttps:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day\n\"\"\"\n\n## Reduce Memory\n\ndef reduce_memory_usage(df):\n    \n    start_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            \n            else:\n#                 reducing float16 for calculating numpy.nanmean\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    end_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) \/ start_memory} % \")\n    return df\n\ndf_train = reduce_memory_usage(df_train)\ndf_train.info()","78ee7aeb":"\"\"\"\nThe codes from 'NaN values depending on Time of Day'\nhttps:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day\n\"\"\"\n\n# def chunks(l, n):\n#     \"\"\" Yield n successive chunks from l.\n#     \"\"\"\n#     newn = int(len(l) \/ n)\n#     for i in range(0, n-1):\n#         yield l[i*newn:i*newn+newn]\n#     yield l[n*newn-newn:]","4f9d3212":"\"\"\"\nThe codes from 'NaN values depending on Time of Day'\nhttps:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day\n\"\"\"\n\n# #count\n# nan_values_train = (df_train\n#  .apply(lambda x: x.isna().sum(axis = 0)\/len(df_train))\n#  .to_frame()\n#  .rename(columns = {0: 'percentage_nan_values'})\n# .sort_values('percentage_nan_values', ascending = False)\n# )\n\n# display((df_train\n#  .apply(lambda x: x.isna().sum(axis = 0))\n#  .to_frame()\n#  .rename(columns = {0: 'count_nan_values'})\n# .sort_values('count_nan_values', ascending = False)\n# .transpose()), nan_values_train.transpose(),\n#        print(\"Number of features with at least one NaN value: {}\/{}\".format(len(nan_values_train.query('percentage_nan_values>0')),\n#                                                                            len(df_train.columns))))","83930339":"\"\"\"\nThe codes from 'NaN values depending on Time of Day'\nhttps:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day\n\"\"\"\n\n# fig, ax = plt.subplots(figsize = (20, 12))\n\n# sns.set_palette(\"RdBu\", 10)\n# #RdBu, YlGn\n# ax = sns.barplot(x='percentage_nan_values', \n#             y='feature', \n#             palette = 'GnBu_r',\n#             data=nan_values_train.reset_index().rename(columns = {'index': 'feature'}).head(40))\n\n# for p in ax.patches:\n#     width = p.get_width() \n#     if width < 0.01:# get bar length\n#         ax.text(width,       # set the text at 1 unit right of the bar\n#             p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n#             '{:1.4f}'.format(width), # set variable to display, 2 decimals\n#             ha = 'left',   # horizontal alignment\n#             va = 'center')  # vertical alignment\n#     else:\n#         if width < 0.04:\n#             color_text = 'black'\n#         else:\n#             color_text = 'white'\n#         ax.text(width \/2, \n#                 # set the text at 1 unit right of the bar\n#             p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n#             '{:1.4f}'.format(width), # set variable to display, 2 decimals\n#             ha = 'left',   # horizontal alignment\n#             va = 'center',\n#             color = color_text,\n#             fontsize = 10)  # vertical alignment\n\n# ax.set_title('Top 40 Features for percentage of NaN Values')","d9f5cf71":"\"\"\"\nThe codes from 'NaN values depending on Time of Day'\nhttps:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day\n\"\"\"\n\n# df_plt_train = df_train.copy()\n# df_plt_train['daily_ts_id'] = (df_plt_train.groupby('date').cumcount())\n\n# top_nan_features = nan_values_train.head(40).index.tolist() #take the first 40 with most nans\n# features_chunks = chunks(top_nan_features, 20)\n# mini_df = pd.concat([(df_plt_train[top_nan_features].isna().astype(int)),df_plt_train[['ts_id']]], 1).iloc[:30000, :]\n# new_day = (df_plt_train.iloc[:30000, :].query(\"daily_ts_id == 0\").ts_id.tolist())\n\n# df_plt_train = (df_plt_train.set_index('date').join(df_plt_train.groupby('date').size().reset_index().rename(columns = {0: 'daily_number_of_trades'}).set_index('date'))\n#         .reset_index())\n\n# df_plt_train['pseudo_time_of_day'] = (df_plt_train['daily_ts_id']\/df_plt_train['daily_number_of_trades'])\n# gc.collect()\n\n# nan_df = pd.concat([(df_plt_train[top_nan_features].isna().astype(int)), df_plt_train[['pseudo_time_of_day']]], 1)","865545aa":"\"\"\"\nThe codes from 'NaN values depending on Time of Day'\nhttps:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day\n\"\"\"\n\n# features_chunks = chunks(top_nan_features, 10)\n# chunk_len = 4\n\n# for k_chunk in features_chunks:\n#     fig, axes = plt.subplots(2, 2, figsize = (40, 30))\n#     ax = axes.ravel()\n    \n#     for i in range(len(k_chunk)):\n\n#         feature_name = k_chunk[i]\n#         feature = nan_df.loc[nan_df[feature_name] == 1]['pseudo_time_of_day']\n        \n#         sns.distplot(feature, hist=True, kde=True, color = 'red', hist_kws={'edgecolor':'black'},\n#                      kde_kws={'linewidth': 2, 'color': 'blue'}, ax = ax[i%10])\n#         ax[i%10].grid(True)\n#         ax[i%10].set(xlabel = 'pseudo_time_of_day')\n#         ax[i%10].set_title(feature_name)","efd8d075":"# df_day_0 = df_train.loc[df_train.date==0]\n\n# features_list = [features[i*30:(i+1)*30]for i in range(5)]\n# resp_features = [[\"resp\"]+ features for features in features_list]\n\n# df_resp_features_0 = df_day_0.loc[:, resp_features[0]]\n\n# fig, axes = plt.subplots(30,4,figsize=[28,120])\n\n# for idx, col in enumerate(tqdm_notebook(resp_features[0][1:])):\n#     targets = [\"resp\"] + [col]\n\n#     df_target = df_resp_features_0.loc[:,targets]\n#     # 1. distplot\n#     sns.distplot(df_target[col], ax=axes[idx,0])\n#     # 2. boxplot\n#     sns.boxplot(df_target[col], ax=axes[idx,1])\n#     # 3. scatterplot with resp\n#     sns.regplot(data=df_target, x=col, y=\"resp\", ax=axes[idx,2])\n#     # 4. cumulative data\n#     df_target[col].cumsum().plot(ax=axes[idx,3])\n\n# plt.suptitle(\"Overall Distribution per features_0-29 [dist, box, scatter, cumsum]\",y=0.89, size=20)","87e9cf15":"# df_resp_features_1 = df_day_0.loc[:, resp_features[1]]\n\n# fig, axes = plt.subplots(30,4,figsize=[28,120])\n\n# for idx, col in enumerate(tqdm_notebook(resp_features[1][1:])):\n#     targets = [\"resp\"] + [col]\n\n#     df_target = df_resp_features_1.loc[:,targets]\n#     # 1. distplot\n#     sns.distplot(df_target[col], ax=axes[idx,0])\n#     # 2. boxplot\n#     sns.boxplot(df_target[col], ax=axes[idx,1])\n#     # 3. scatterplot with resp\n#     sns.regplot(data=df_target, x=col, y=\"resp\", ax=axes[idx,2])\n#     # 4. cumulative data\n#     df_target[col].cumsum().plot(ax=axes[idx,3])\n\n# plt.suptitle(\"Overall Distribution per features_30-59 [dist, box, scatter, cumsum]\",y=0.89, size=20)","3153e22b":"# df_resp_features_2 = df_day_0.loc[:, resp_features[2]]\n\n# fig, axes = plt.subplots(30,4,figsize=[28,120])\n\n# for idx, col in enumerate(tqdm_notebook(resp_features[2][1:])):\n#     targets = [\"resp\"] + [col]\n\n#     df_target = df_resp_features_2.loc[:,targets]\n#     # 1. distplot\n#     sns.distplot(df_target[col], ax=axes[idx,0])\n#     # 2. boxplot\n#     sns.boxplot(df_target[col], ax=axes[idx,1])\n#     # 3. scatterplot with resp\n#     sns.regplot(data=df_target, x=col, y=\"resp\", ax=axes[idx,2])\n#     # 4. cumulative data\n#     df_target[col].cumsum().plot(ax=axes[idx,3])\n\n# plt.suptitle(\"Overall Distribution per features_60-89 [dist, box, scatter, cumsum]\",y=0.89, size=20)","3ceb15b0":"# df_resp_features_3 = df_day_0.loc[:, resp_features[3]]\n\n# fig, axes = plt.subplots(30,4,figsize=[28,120])\n\n# for idx, col in enumerate(tqdm_notebook(resp_features[3][1:])):\n#     targets = [\"resp\"] + [col]\n\n#     df_target = df_resp_features_3.loc[:,targets]\n#     # 1. distplot\n#     sns.distplot(df_target[col], ax=axes[idx,0])\n#     # 2. boxplot\n#     sns.boxplot(df_target[col], ax=axes[idx,1])\n#     # 3. scatterplot with resp\n#     sns.regplot(data=df_target, x=col, y=\"resp\", ax=axes[idx,2])\n#     # 4. cumulative data\n#     df_target[col].cumsum().plot(ax=axes[idx,3])\n\n# plt.suptitle(\"Overall Distribution per features_90-119 [dist, box, scatter, cumsum]\",y=0.89, size=20)","4e7fa6e9":"# df_resp_features_4 = df_day_0.loc[:, resp_features[4]]\n\n# fig, axes = plt.subplots(10,4,figsize=[28,40])\n\n# for idx, col in enumerate(tqdm_notebook(resp_features[4][1:])):\n#     targets = [\"resp\"] + [col]\n\n#     df_target = df_resp_features_4.loc[:,targets]\n#     # 1. distplot\n#     sns.distplot(df_target[col], ax=axes[idx,0])\n#     # 2. boxplot\n#     sns.boxplot(df_target[col], ax=axes[idx,1])\n#     # 3. scatterplot with resp\n#     sns.regplot(data=df_target, x=col, y=\"resp\", ax=axes[idx,2])\n#     # 4. cumulative data\n#     df_target[col].cumsum().plot(ax=axes[idx,3])\n\n# plt.suptitle(\"Overall Distribution per features_120-129 [dist, box, scatter, cumsum]\",y=0.89, size=20)","6a93118c":"\"\"\"\nThe codes from 'Optimise Speed of Filling-NaN Function'\nhttps:\/\/www.kaggle.com\/gogo827jz\/optimise-speed-of-filling-nan-function\n\"\"\"\n\ndef for_loop(method, matrix, values):\n    for i in range(matrix.shape[0]):\n        matrix[i] = method(matrix[i], values)\n    return matrix\n\ndef for_loop_ffill(method, matrix):\n    tmp = np.zeros(matrix.shape[1],dtype=np.float32)\n    for i in range(matrix.shape[0]):\n        matrix[i] = method(matrix[i], tmp)\n        tmp = matrix[i]\n    return matrix\n\n@njit\ndef fillna_npwhere_njit(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","534d3a3b":"# for estimating how many datas will be deleted by techniques.\ndate_ts = [\"date\",\"ts_id\"]\ndf_date_ts = df_train.loc[:,date_ts]","1fd0ed40":"# converting numpy for efficient calcualtion.\n# ft 1~129\nnp_train = df_train.loc[:,features[1:]].values\nnp_train.shape\n\n# ft 0\nnp_train_ft0 = df_train.loc[:,features[0]].values","ebedd840":"# nead pre-calculate 1.2GB per action\nf_mean = np.nanmean(np_train,axis=0)","daf15097":"print('fillna_npwhere_njit (mean-filling):')\nnp_mf_train = for_loop(fillna_npwhere_njit, np_train, f_mean)","95a22f3d":"# fig = plt.figure(figsize=(20,80))\n# fig.suptitle('Features Box plot with 0.1% 99.9% whiskers',fontsize=22, y=.89)\n# grid =  gridspec.GridSpec(33,4,figure=fig,hspace=.5,wspace=.05)\n# counter = 0\n# for i in range(33):\n#     for j in range(4):\n#         if counter > 128:\n#             break\n            \n#         subf = fig.add_subplot(grid[i, j]);\n#         sns.boxplot(x= np_mf_train[:, counter],saturation=.5,color= 'blue', ax= subf,width=.5,whis=(.1,99.9));\n#         subf.axvline(np_mf_train[:, counter].mean(),color= 'darkorange', label='Mean', linestyle=':',linewidth=3)\n#         subf.axvline(np.percentile(np_mf_train[:, counter],97),color= 'chocolate', label='97%', linestyle=':',linewidth=2)\n#         subf.axvline(np.percentile(np_mf_train[:, counter],99.5),color= 'darkblue', label='99.5%', linestyle=':',linewidth=2)\n#         subf.axvline(np.percentile(np_mf_train[:, counter],.5),color= 'red', label='0.5%', linestyle=':',linewidth=2)\n#         subf.legend().set_visible(False)\n#         subf.set_xlabel('')\n#         subf.set_title('{}'.format(features[counter+1]),fontsize=16)\n#         counter += 1\n#         gc.collect()\n\n# handles, labels = subf.get_legend_handles_labels()\n# fig.legend(handles, labels,ncol=4, bbox_to_anchor=(0.90, 0.893),fontsize=10,\n#            title= 'Scale',title_fontsize=14,bbox_transform =fig.transFigure);\n# plt.show();","61058cee":"sigma = 6\n\n@njit\ndef drop_sigma(array):\n    mean = np.mean(array)\n    std = np.std(array)\n    low, high = mean - sigma * std, mean + sigma * std\n    index = (array > low) & (array < high)\n    \n    return index\n\nnp_index = None\n\nfor i in tqdm_notebook(range(np_mf_train.shape[1])):\n    target = np_mf_train[:,i]\n    index = drop_sigma(target).reshape(-1,1)\n    if np_index is None:\n        np_index = index\n    else:\n        np_index = np.concatenate([np_index, index], axis=1)\n        \n# https:\/\/stackoverflow.com\/questions\/16468717\/iterating-over-numpy-matrix-rows-to-apply-a-function-each\nnp_index = np.apply_along_axis(lambda x: False if False in x else True, axis=1, arr=np_index)","b50bb81b":"np_date_ts = df_date_ts.values\nnp_date_ts = np.concatenate([np_date_ts,np_index.reshape(-1,1)],axis=1)","dffc9600":"def get_numpy_count_sum(array):\n    dict_deletion = dict()\n    \n    max_date = array[:,0].max()+1\n    for i in tqdm_notebook(range(max_date)):\n        target = array[array[:,0]==i]\n        count = target.shape[0]\n        non_zero = target[:,2].sum()\n        dict_deletion[i] = (count,non_zero, count-non_zero)\n\n    return dict_deletion\n\ndict_deletion = get_numpy_count_sum(np_date_ts)","e7768f7b":"dates = dict_deletion.keys()\ntotal = list(map(lambda x: x[0],dict_deletion.values()))\nremainder = list(map(lambda x: x[1],dict_deletion.values()))\ndelete = list(map(lambda x: x[2],dict_deletion.values()))","89ac7bb3":"fig, ax = plt.subplots(figsize=(15, 5))\ntotal = pd.Series(total, name=\"total\")\nremainder = pd.Series(remainder, name=\"remainder\")\ndelete = pd.Series(delete, name=\"delete\")\nax.set_xlabel (\"Date\", fontsize=18)\nax.set_title (\"Deletion Target via outlier handling\", fontsize=18)\ntotal.plot(lw=3)\nremainder.plot(lw=3)\ndelete.plot(lw=3)\nplt.legend(loc=\"upper left\");\n# del total\ndel remainder\n# del delete\ngc.collect();","e4e16415":"print(f\"{delete.sum()\/total.sum()*100:.4f}%({delete.sum()} among {total.sum()}) of data is deleted by {sigma}-sigma confience interval\")\n\ndel total\ndel delete","23b21d00":"np_train = np_mf_train[np_index]\nnp_train_ft0 = np_train_ft0[np_index]\n\nprint(np_train_ft0.shape, np_train.shape)","403ec5c0":"# fig = plt.figure(figsize=(20,80))\n# fig.suptitle('[After deleting outlier] Features Box plot with 0.1% 99.9% whiskers',fontsize=22, y=.89)\n# grid =  gridspec.GridSpec(33,4,figure=fig,hspace=.5,wspace=.05)\n# counter = 0\n# for i in range(33):\n#     for j in range(4):\n#         if counter > 128:\n#             break\n            \n#         subf = fig.add_subplot(grid[i, j]);\n#         sns.boxplot(x= np_train[:, counter],saturation=.5,color= 'blue', ax= subf,width=.5,whis=(.1,99.9));\n#         subf.axvline(np_train[:, counter].mean(),color= 'darkorange', label='Mean', linestyle=':',linewidth=3)\n#         subf.axvline(np.percentile(np_train[:, counter],97),color= 'chocolate', label='97%', linestyle=':',linewidth=2)\n#         subf.axvline(np.percentile(np_train[:, counter],99.5),color= 'darkblue', label='99.5%', linestyle=':',linewidth=2)\n#         subf.axvline(np.percentile(np_train[:, counter],.5),color= 'red', label='0.5%', linestyle=':',linewidth=2)\n#         subf.legend().set_visible(False)\n#         subf.set_xlabel('')\n#         subf.set_title('{}'.format(features[counter+1]),fontsize=16)\n#         counter += 1\n#         gc.collect()\n\n# handles, labels = subf.get_legend_handles_labels()\n# fig.legend(handles, labels,ncol=4, bbox_to_anchor=(0.90, 0.893),fontsize=10,\n#            title= 'Scale',title_fontsize=14,bbox_transform =fig.transFigure);\n# plt.show();","219bb6e4":"# 1.2GB\n\nweight_resp = [\"weight\", \"resp\"]\ndf_weight_resp = df_train.loc[:, weight_resp]\ndf_weight_resp.loc[:,\"wresp\"] = df_weight_resp[\"weight\"].values * df_weight_resp[\"resp\"].values\n\nnp_ww_wresp = df_weight_resp.values\nnp_ww_wresp = np_ww_wresp[np_index,:]\n\ndf_weight_resp = df_weight_resp.drop(\"weight\",axis=1)\nnp_wresp = df_weight_resp.values\nnp_wresp = np_wresp[np_index,:]\n\nnp_wresp.shape","aee47156":"n_days = 31\n\nn_days_row = [dict_deletion[key][1] for key in list(dict_deletion.keys())[:n_days]]\nn_days_row = sum(n_days_row)\nn_days_row\n\n# features_{0,...,129}\nnp_days_ft0 = np_train_ft0[:n_days_row]\nnp_days_ft = np_train[:n_days_row,:]\nnp_days_wresp = np_wresp[:n_days_row,:]","09a7ff3c":"np_days = np.concatenate([np_days_ft0.reshape(-1,1), np_days_ft, np_days_wresp],axis=1)\nnp_days.shape","856d4a3e":"for i in range(np_days.shape[1]):\n    np_days[:,i]= ((np_days[:,i] - np.mean(np_days[:,i])) \/ np.std(np_days[:,i]))","02d866e2":"# print till 5 roews and 10 columns for looking at result\nnp_days[:5,:10]","5fc4c03d":"r = np.linspace(-1,1,500)\nd = np.sqrt(0.5*(1-r))\n\nplt.figure(figsize=(12,6))\nplt.title('Correlation-based distance')\nplt.xlabel('Correlation coefficient')\nplt.ylabel('Distance')\nplt.plot(r,d)\nplt.show()","28dbb4aa":"ab_features = [\"ft_\"+feature.split(\"_\")[1] for feature in features]\nnp_days_ft = ab_features + [\"resp\", \"wresp\"]\n\ndf_days = pd.DataFrame(np_days, columns = np_days_ft)\n\ncorr_mat = df_days.corr()\n\ndist = np.sqrt(0.5*(1-corr_mat))\ndist.shape","f73a6f82":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(23,9))\n\nsns.heatmap(corr_mat,ax= ax1, cmap='coolwarm');\nsns.heatmap(dist,    ax= ax2, cmap='coolwarm');\nax1.title.set_text('Correlation matrix')\nax2.title.set_text('Distance matrix')\nplt.show()","709c2092":"# Complete graph from distance matrix\nG = nx.from_numpy_matrix(dist.to_numpy())\n\nlabels = dist.columns.values\nlabels = [s.replace('ft_','') for s in labels]\nG = nx.relabel_nodes(G, dict(zip(range(len(labels)), labels)))","9b85ebd7":"# Minimum spanning tree\nT=nx.minimum_spanning_tree(G)\n\nfig = plt.figure(figsize=(20,20))\nnx.draw_networkx(T,\n                 with_labels=True, \n                 font_size=9, \n                 cmap=plt.cm.coolwarm,\n                 pos=nx.kamada_kawai_layout(T),vmin=0, vmax=1)\nplt.show()","3d84f8c7":"import scipy.cluster.hierarchy as sch\nfrom scipy.cluster.hierarchy import fcluster\n\n# Linkage matrix\nlink=sch.linkage(dist,'average')","26e2705d":"# Plot dendrogram\n\nfig = plt.figure(figsize=(20, 8))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Feature')\nplt.ylabel('Distance')\nplt.hlines(1.65,0,1320)\ndn = sch.dendrogram(link,leaf_rotation=90.,leaf_font_size=11.)\nplt.show()","5a1c8596":"# fcluster forms flat clusters from the hierarchical clustering defined by the given linkage matrix.\n\nmax_d = 1.65\nlar_clusters = fcluster(link,t=max_d, criterion='distance')","842452b6":"lar_clusters.shape","856b8f2f":"df_lar_clust = pd.DataFrame({'Cluster':lar_clusters, 'Features':np_days_ft})\ndf_lar_clust.groupby('Cluster').count()","e0da2310":"# Save the cluster-feature in a dictionary \nclust_lar_feat = {}\nfor k in np.unique(lar_clusters):\n    clust_lar_feat[k] = df_lar_clust[df_lar_clust.Cluster == k].Features.values","bfdd94e2":"clust_lar_feat","64ee33e2":"def getQuasiDiag(link):\n    # Sort clustered items by distance\n    link=link.astype(int)\n    sortIx=pd.Series([link[-1,0],link[-1,1]])\n    numItems=link[-1,3] # number of original items\n    while sortIx.max()>=numItems:\n        sortIx.index=range(0,sortIx.shape[0]*2,2) # make space\n        df0=sortIx[sortIx>=numItems] # find clusters\n        i=df0.index;j=df0.values-numItems\n        sortIx[i]=link[j,0] # item 1\n        df0=pd.Series(link[j,1],index=i+1)\n        sortIx=sortIx.append(df0) # item 2\n        sortIx=sortIx.sort_index() # re-sort\n        sortIx.index=range(sortIx.shape[0]) # re-index\n    return sortIx.tolist()","06d4ac4b":"sortIx=getQuasiDiag(link)\nsortIx=corr_mat.index[sortIx].tolist() # recover labels\ncorr_diag=corr_mat.loc[sortIx,sortIx] # reorder\n\nsortIx=getQuasiDiag(link)\nsortIx=dist.index[sortIx].tolist() # recover labels\ndist_diag=dist.loc[sortIx,sortIx] # reorder","2def0e23":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(23,9))\n\nsns.heatmap(corr_diag,ax= ax1, cmap='coolwarm');\nsns.heatmap(dist_diag,ax= ax2, cmap='coolwarm');\nax1.title.set_text('Quasi-diagonal Correlation matrix')\nax2.title.set_text('Quasi-diagonal Distance matrix')\nplt.show()","1187fd20":"\"\"\"\nThe codes from '[github] (pyRMT) Randomized Matrix Theory Python Code'\nhttps:\/\/github.com\/GGiecold\/pyRMT\n\"\"\"\n\n\nfrom __future__ import division, print_function\nfrom builtins import reversed\nfrom builtins import map, zip\nfrom collections import MutableSequence, Sequence\nimport copy\nfrom math import ceil\nfrom numbers import Complex, Integral, Real\nimport sys\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.covariance import EmpiricalCovariance\nfrom sklearn.preprocessing import StandardScaler\n\nclass pyRMT:\n    \n    def optimalShrinkage(self, X, return_covariance=False, method='rie'):\n        \"\"\"This function computes a cleaned, optimal shrinkage, \n           rotationally-invariant estimator (RIE) of the true correlation \n           matrix C underlying the noisy, in-sample estimate \n           E = 1\/T X * transpose(X)\n           associated to a design matrix X of shape (T, N) (T measurements \n           and N features).\n           One approach to getting a cleaned estimator that predates the\n           optimal shrinkage, RIE estimator consists in inverting the \n           Marcenko-Pastur equation so as to replace the eigenvalues\n           from the spectrum of E by an estimation of the true ones.\n           This approach is known to be numerically-unstable, in addition\n           to failing to account for the overlap between the sample eigenvectors\n           and the true eigenvectors. How to compute such overlaps was first\n           explained by Ledoit and Peche (cf. reference below). Their procedure\n           was extended by Bun, Bouchaud and Potters, who also correct\n           for a systematic downward bias in small eigenvalues.\n\n           It is this debiased, optimal shrinkage, rotationally-invariant\n           estimator that the function at hand implements.\n\n           In addition to above method, this funtion also provides access to:  \n           - The finite N regularization of the optimal RIE for small eigenvalues\n             as provided in section 8.1 of [3] a.k.a the inverse wishart (IW) regularization.\n           - The direct kernel method of O. Ledoit and M. Wolf in their 2017 paper [4]. \n             This is a direct port of their Matlab code.\n\n\n           Parameters\n           ----------\n           X: design matrix, of shape (T, N), where T denotes the number\n               of samples (think measurements in a time series), while N\n               stands for the number of features (think of stock tickers).\n\n           return_covariance: type bool (default: False)\n               If set to True, compute the standard deviations of each individual\n               feature across observations, clean the underlying matrix\n               of pairwise correlations, then re-apply the standard\n               deviations and return a cleaned variance-covariance matrix.\n\n           method: type string, optional (default=\"rie\")\n               - If \"rie\" : optimal shrinkage in the manner of Bun & al.\n                with no regularisation  \n               - If \"iw\" : optimal shrinkage in the manner of Bun & al.\n                with the so called Inverse Wishart regularization\n               - If 'kernel': Direct kernel method of Ledoit  Wolf.\n           Returns\n           -------\n           E_RIE: type numpy.ndarray, shape (N, N)\n               Cleaned estimator of the true correlation matrix C. A sample\n               estimator of C is the empirical covariance matrix E \n               estimated from X. E is corrupted by in-sample noise.\n               E_RIE is the optimal shrinkage, rotationally-invariant estimator \n               (RIE) of C computed following the procedure of Joel Bun \n               and colleagues (cf. references below).\n\n               If return_covariance=True, E_clipped corresponds to a cleaned\n               variance-covariance matrix.\n           References\n           ----------\n           1 \"Eigenvectors of some large sample covariance matrix ensembles\",\n             O. Ledoit and S. Peche\n             Probability Theory and Related Fields, Vol. 151 (1), pp 233-264\n           2 \"Rotational invariant estimator for general noisy matrices\",\n             J. Bun, R. Allez, J.-P. Bouchaud and M. Potters\n             arXiv: 1502.06736 [cond-mat.stat-mech]\n           3 \"Cleaning large Correlation Matrices: tools from Random Matrix Theory\",\n             J. Bun, J.-P. Bouchaud and M. Potters\n             arXiv: 1610.08104 [cond-mat.stat-mech]\n           4 \"Direct Nonlinear Shrinkage Estimation of Large-Dimensional Covariance Matrices (September 2017)\", \n             O. Ledoit and M. Wolf https:\/\/ssrn.com\/abstract=3047302 or http:\/\/dx.doi.org\/10.2139\/ssrn.3047302\n        \"\"\"\n\n        try:\n            assert isinstance(return_covariance, bool)\n        except AssertionError:\n            raise\n            sys.exit(1)\n\n        T, N, transpose_flag = self.checkDesignMatrix(X)\n        if transpose_flag:\n            X = X.T\n\n        if not return_covariance:\n            X = StandardScaler(with_mean=False,\n                               with_std=True).fit_transform(X)\n\n        ec = EmpiricalCovariance(store_precision=False,\n                                 assume_centered=True)\n        ec.fit(X)\n        E = ec.covariance_\n\n        if return_covariance:\n            inverse_std = 1.\/np.sqrt(np.diag(E))\n            E *= inverse_std\n            E *= inverse_std.reshape(-1, 1)\n\n        eigvals, eigvecs = np.linalg.eigh(E)\n        eigvecs = eigvecs.T\n\n        q = N \/ float(T)\n        lambda_N = eigvals[0]  # The smallest empirical eigenvalue,\n                               # given that the function used to compute\n                               # the spectrum of a Hermitian or symmetric\n                               # matrix - namely np.linalg.eigh - returns\n                               # the eigenvalues in ascending order.\n        lambda_hats = None\n\n        if method is not 'kernel':\n            use_inverse_wishart = (method == 'iw')\n            xis = map(lambda x: self.xiHelper(x, q, E), eigvals)\n            Gammas = map(lambda x: self.gammaHelper(x, q, N, lambda_N, inverse_wishart=use_inverse_wishart), eigvals)\n            xi_hats = map(lambda a, b: a * b if b > 1 else a, xis, Gammas)\n            lambda_hats = xi_hats\n        else:\n             lambda_hats = directKernel(q, T, N, eigvals)\n\n        E_RIE = np.zeros((N, N), dtype=float)\n        for lambda_hat, eigvec in zip(lambda_hats, eigvecs):\n            eigvec = eigvec.reshape(-1, 1)\n            E_RIE += lambda_hat * eigvec.dot(eigvec.T)\n\n        tmp = 1.\/np.sqrt(np.diag(E_RIE))\n        E_RIE *= tmp\n        E_RIE *= tmp.reshape(-1, 1)\n\n        if return_covariance:\n            std = 1.\/inverse_std\n            E_RIE *= std\n            E_RIE *= std.reshape(-1, 1)\n\n        return E_RIE\n\n    def checkDesignMatrix(self, X):\n        \"\"\"\n           Parameters\n           ----------\n           X: a matrix of shape (T, N), where T denotes the number\n               of samples and N labels the number of features.\n               If T < N, a warning is issued to the user, and the transpose\n               of X is considered instead.\n           Returns:\n           T: type int\n           N: type int\n           transpose_flag: type bool\n               Specify if the design matrix X should be transposed\n               in view of having less rows than columns.       \n        \"\"\"\n\n        try:\n            assert isinstance(X, (np.ndarray, pd.DataFrame, pd.Series,\n                                  MutableSequence, Sequence))\n        except AssertionError:\n            raise\n            sys.exit(1)\n\n        X = np.asarray(X, dtype=float)\n        X = np.atleast_2d(X)\n\n        if X.shape[0] < X.shape[1]:\n            warnings.warn(\"The Marcenko-Pastur distribution pertains to \"\n                          \"the empirical covariance matrix of a random matrix X \"\n                          \"of shape (T, N). It is assumed that the number of \"\n                          \"samples T is assumed higher than the number of \"\n                          \"features N. The transpose of the matrix X submitted \"\n                          \"at input will be considered in the cleaning schemes \"\n                          \"for the corresponding correlation matrix.\", UserWarning)\n\n            T, N = reversed(X.shape)\n            transpose_flag = True\n        else:\n            T, N = X.shape\n            transpose_flag = False\n\n        return T, N, transpose_flag\n\n\n    def xiHelper(self, x, q, E):\n        \"\"\"Helper function to the rotationally-invariant, optimal shrinkage\n           estimator of the true correlation matrix (implemented via function\n           optimalShrinkage of the present module). \n           Parameters\n           ----------\n           x: type derived from numbers.Real\n               Would typically be expected to be an eigenvalue from the\n               spectrum of correlation matrix E. The present function\n               can however handle an arbitrary floating-point number.\n           q: type derived from numbers.Real\n               The number parametrizing a Marcenko-Pastur spectrum.\n           E: type numpy.ndarray\n               Symmetric correlation matrix associated with the \n               Marcenko-Pastur parameter q specified above.\n           Returns\n           -------\n           xi: type float\n               Cleaned eigenvalue of the true correlation matrix C underlying\n               the empirical correlation E (the latter being corrupted \n               with in-sample noise). This cleaned version is computed\n               assuming no prior knowledge on the structure of the true\n               eigenvectors (thereby leaving the eigenvectors of E unscathed). \n           References\n           ----------\n           * \"Rotational invariant estimator for general noisy matrices\",\n             J. Bun, R. Allez, J.-P. Bouchaud and M. Potters\n             arXiv: 1502.06736 [cond-mat.stat-mech]\n           * \"Cleaning large Correlation Matrices: tools from Random Matrix Theory\",\n             J. Bun, J.-P. Bouchaud and M. Potters\n             arXiv: 1610.08104 [cond-mat.stat-mech]\n        \"\"\"\n\n        try:\n            assert isinstance(x, Real)\n            assert isinstance(q, Real)\n            assert isinstance(E, np.ndarray) and E.shape[0] == E.shape[1]\n            assert np.allclose(E.transpose(1, 0), E)\n        except AssertionError:\n            raise\n            sys.exit(1)\n\n        N = E.shape[0]\n\n        z = x - 1j \/ np.sqrt(N)\n        s = self.stieltjes(z, E)\n        xi = x \/ abs(1 - q + q * z * s)**2\n\n        return xi\n\n    def stieltjes(self, z, E):\n        \"\"\"\n           Parameters\n           ----------\n           z: complex number\n           E: square matrix\n           Returns\n           -------\n           A complex number, the resolvent of square matrix E, \n           also known as its Stieltjes transform.\n           Reference\n           ---------\n           \"Financial Applications of Random Matrix Theory: a short review\",\n           J.-P. Bouchaud and M. Potters\n           arXiv: 0910.1205 [q-fin.ST]\n        \"\"\"\n\n        try:\n            assert isinstance(z, Complex)\n\n            assert isinstance(E, (np.ndarray, pd.DataFrame,\n                                  MutableSequence, Sequence))\n            E = np.asarray(E, dtype=float)\n            E = np.atleast_2d(E)\n            assert E.shape[0] == E.shape[1]\n        except AssertionError:\n            raise\n            sys.exit(1)\n\n        N = E.shape[0]\n\n        ret = z * np.eye(N, dtype=float) - E\n        ret = np.trace(ret) \/ N\n\n        return ret\n\n    def gammaHelper(self, x, q, N, lambda_N, inverse_wishart=False):\n        \"\"\"Helper function to optimalShrinkage function defined below.\n           The eigenvalue to the cleaned estimator of a true correlation\n           matrix are computed via the function xiHelper defined above in\n           the module at hand. \n\n           It is known however that when N is not very large\n           a systematic downward bias affects the xiHelper estimator for small\n           eigenvalues of the noisy empirical correlation matrix. This bias\n           can be heuristically corrected by computing\n           xi_hat = xi_RIE * max(1, Gamma),\n           with Gamma evaluated by the function gammaHelper herewith.\n           Parameters\n           ----------\n           x: type float or any other type derived from numbers.Real\n               Typically an eigenvalue from the spectrum of a sample\n               estimate of the correlation matrix associated to some\n               design matrix X. However, the present function supports\n               any arbitrary floating-point number x at input.\n           q: type derived from numbers.Real\n               Parametrizes a Marcenko-Pastur spectrum.\n           N: type derived from numbers.Integral\n               Dimension of a correlation matrix whose debiased, \n               rotationally-invariant estimator is to be assessed via\n               the function RIE (see below), of which the present function\n               is a helper.\n           lambda_N: type derived from numbers.Real\n               Smallest eigenvalue from the spectrum of an empirical\n               estimate to a correlation matrix.\n\n           inverse_wishart: type bool default: False\n                Wether to use inverse wishart regularization\n           Returns\n           ------\n           Gamma: type float\n               Upward correction factor for computing a debiased \n               rotationally-invariant estimator of a true underlying \n               correlation matrix. \n           Reference\n           ---------\n           \"Cleaning large Correlation Matrices: tools from Random Matrix Theory\",\n            J. Bun, J.-P. Bouchaud and M. Potters\n            arXiv: 1610.08104 [cond-mat.stat-mech]\n        \"\"\"\n\n        try:\n            assert isinstance(x, Real)\n            assert isinstance(q, Real)\n            assert isinstance(N, Integral)\n            assert isinstance(lambda_N, Real)\n        except AssertionError:\n            raise\n            sys.exit(1)\n\n        z = x - 1j \/ np.sqrt(N)\n\n        lambda_plus = (1 + np.sqrt(q))**2\n        lambda_plus \/= (1 - np.sqrt(q))**2\n        lambda_plus *= lambda_N\n        sigma_2 = lambda_N \/ (1 - np.sqrt(q))**2\n\n        # gmp defined below stands for the Stieltjes transform of the\n        # rescaled Marcenko-Pastur density, evaluated at z\n        gmp = z + sigma_2 * (q - 1) - np.sqrt((z - lambda_N) * (z - lambda_plus))\n        gmp \/= 2 * q * sigma_2 * z\n\n        Gamma = abs(1 - q + q * z * gmp)**2\n        Gamma *= sigma_2\n\n        if inverse_wishart:\n            kappa = 2 * lambda_N \/ ((1 - q - lambda_N) ** 2 - 4 * q * lambda_N)\n            alpha_s = 1 \/ (1 + 2 * q * kappa)\n            denom = x \/ (1 + alpha_s * (x - 1.))\n            Gamma \/= denom\n        else: \n            Gamma \/= x\n\n        return Gamma\n","c6c129da":"rmt = pyRMT()\ncorr_mat = rmt.optimalShrinkage(df_days)","204c59bd":"corr_mat = pd.DataFrame(corr_mat, columns=np_days_ft)\ndist = np.sqrt(0.5*(1-corr_mat))","cc4b6e4e":"dist = dist.fillna(0)","4385a06c":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(23,9))\n\nsns.heatmap(corr_mat,ax= ax1, cmap='coolwarm');\nsns.heatmap(dist,    ax= ax2, cmap='coolwarm');\nax1.title.set_text('Correlation matrix')\nax2.title.set_text('Distance matrix')\nplt.show()","8c0e1235":"import networkx as nx\nimport scipy.cluster.hierarchy as sch\nfrom scipy.cluster.hierarchy import fcluster\n\n# Complete graph from distance matrix\nG = nx.from_numpy_matrix(dist.to_numpy())\n\nlabels = dist.columns.values\nlabels = [s.replace('ft_','') for s in labels]\nG = nx.relabel_nodes(G, dict(zip(range(len(labels)), labels)))","9aba1f65":"# Minimum spanning tree\nT=nx.minimum_spanning_tree(G)\n\nfig = plt.figure(figsize=(20,20))\nnx.draw_networkx(T,\n                 with_labels=True, \n                 font_size=9, \n                 cmap=plt.cm.coolwarm,\n                 pos=nx.kamada_kawai_layout(T),vmin=0, vmax=1)\nplt.show()","0e9f1142":"# Linkage matrix\nlink=sch.linkage(dist,'average')\n# link2=sch.linkage(dist,'complete')","ccac6d22":"# Plot dendrogram\n\nfig = plt.figure(figsize=(20, 8))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Feature')\nplt.ylabel('Distance')\nplt.hlines(1.37,0,1320)\ndn = sch.dendrogram(link,leaf_rotation=90.,leaf_font_size=11.)\nplt.show()","47dcf02a":"# fcluster forms flat clusters from the hierarchical clustering defined by the given linkage matrix.\n\nmax_d = 1.37\nden_clusters = fcluster(link,t=max_d, criterion='distance')","e095da40":"df_den_clust = pd.DataFrame({'Cluster':den_clusters, 'Features':np_days_ft})\ndf_den_clust.groupby('Cluster').count()","febc3b6d":"# Save the cluster-feature in a dictionary \nclust_den_feat = {}\nfor k in np.unique(den_clusters):\n    clust_den_feat[k] = df_den_clust[df_den_clust.Cluster == k].Features.values","2ca71ed4":"for k in np.unique(den_clusters):\n    print('Cluster_{}'.format(k,2),'->', df_den_clust[df_den_clust.Cluster == k].Features.values)","2a874294":"# origin\nnp_train = np.concatenate([np_train_ft0.reshape(-1,1), np_train],axis=1)\nnp_train.shape","8a606e3f":"clust_den_feat","78aa00ad":"# delete wresp and resp for lar\/den cluster \nclust_lar_feat = {key: np.array([int(value.split(\"_\")[1]) for value in values[np.where(~((values == \"resp\") | (values == \"wresp\")))]]) for key, values in clust_lar_feat.items()}\nclust_den_feat = {key: np.array([int(value.split(\"_\")[1]) for value in values[np.where(~((values == \"resp\") | (values == \"wresp\")))]]) for key, values in clust_den_feat.items()}","e82fa58a":"def cluster_feature(dict_clust, array):\n    result = None\n    for key, values in dict_clust.items():\n        mid_result = np.ones(array.shape[0]).reshape(-1,1)\n        for col in values:\n            mid_result *= array[:,col].reshape(-1,1)\n        \n        if result is None:\n            result = mid_result\n        else:\n            result = np.concatenate([result,mid_result],axis=1)\n        \n            \n    return result","5415900e":"np_lar_train = cluster_feature(clust_lar_feat, np_train)\nnp_den_train = cluster_feature(clust_den_feat, np_train)","e93a954b":"# date numpy array after outlier deletion (for rolling feature)\ndate = np_date_ts[np.where(np_date_ts[:,2] == 1)][:,0]","02d85f9a":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nlar_scaler = MinMaxScaler()\nden_scaler = MinMaxScaler()","7175471c":"np_train = scaler.fit_transform(np_train)\nnp_lar_train = lar_scaler.fit_transform(np_lar_train)\nnp_den_train = den_scaler.fit_transform(np_den_train)","e14ec7ac":"np_date_drop = np_date_ts[np_index]\nnp_date_drop.shape","9ca79b3a":"# \ud558\ub098\uc758 datapoint(row)\uc5d0\uc11c 0\uc778 \uac12\uc744 \uac00\uc9c0\ub294 index\nnp_lar_index = np.apply_along_axis(lambda x: x==0, axis=1, arr=np_lar_train).sum(axis=1) == True\nnp_den_index = np.apply_along_axis(lambda x: x==0, axis=1, arr=np_den_train).sum(axis=1) == True","740c8aed":"np.where(np_lar_index == True)","94fd3fa7":"np.where(np_den_index == True)","93fe190c":"from collections import deque\n\ndef moving_avg_per_date(array, date_array, num_date=500, seq_len=5):\n    dict_result = dict()\n    window = deque(maxlen=seq_len+1)\n\n    for date in range(500):\n        if date < seq_len:\n            result = np.zeros(array.shape[1])\n            dict_result[date] = result\n\n            stacking = np.mean(array[np.where(date_array[:,0] == date)],axis=0) \n            window.append(stacking)\n\n        else:\n            window.popleft()\n            stacking = np.mean(array[np.where(date_array[:,0] == date)],axis=0) \n            window.append(stacking)\n\n            dict_result[date] = np.mean(window,axis=0)\n            \n    return dict_result\n\ndef moving_std_per_date(array, date_array, num_date=500, seq_len=5):\n    dict_result = dict()\n    window = deque(maxlen=seq_len+1)\n\n    for date in range(500):\n        if date < seq_len:\n            result = np.zeros(array.shape[1])\n            dict_result[date] = result\n\n            stacking = np.std(array[np.where(date_array[:,0] == date)],axis=0) \n            window.append(stacking)\n\n        else:\n            window.popleft()\n            stacking = np.std(array[np.where(date_array[:,0] == date)],axis=0) \n            window.append(stacking)\n\n            dict_result[date] = np.mean(window,axis=0)\n            \n    return dict_result","8208db60":"seq_len = 5\ndict_mov_avg = moving_avg_per_date(np_train, np_date_drop, seq_len = seq_len)\ndict_mov_std = moving_std_per_date(np_train, np_date_drop, seq_len = seq_len)","4d7426ec":"# lar \/ den corrleation rolling feature\n\n# seq_len = 5\n# dict_lar_mov_avg = moving_avg_per_date(np_lar_train, np_date_drop, seq_len = seq_len)\n# dict_lar_mov_std = moving_std_per_date(np_lar_train, np_date_drop, seq_len = seq_len)\n\n# dict_lar_mov_avg = moving_avg_per_date(np_lar_train, np_date_drop, seq_len = seq_len)\n# dict_lar_mov_std = moving_std_per_date(np_lar_train, np_date_drop, seq_len = seq_len)","26768841":"# df_date = pd.DataFrame(np_date_drop[:,0],columns=[\"date\"])\n\n# df_train = pd.DataFrame(np_train, columns = features)\n# df_train = pd.concat([df_date,df_train],axis=1)\n\n# df_mov_avg = pd.DataFrame.from_dict(dict_date_mov_avg).transpose()\n# df_mov_avg.columns = [feature+f\"_{seq_len}_avg\" for feature in features]\n# df_mov_std = pd.DataFrame.from_dict(dict_date_mov_std).transpose()\n# df_mov_std.columns = [feature+f\"_{seq_len}_std\" for feature in features]\n\n# df_mov = pd.concat([df_mov_avg, df_mov_std], axis=1)\n\n# columns = df_mov.columns.to_list()\n# df_mov = df_mov.reset_index()\n# df_mov.columns = [\"date\"] + columns\n\n# dt_train = dt.Frame(df_train)\n# dt_mov = dt.Frame(df_mov)\n\n# dt_mov.key = \"date\"\n\n# dt_train = dt_train[:,:,dt.join(dt_mov)]\n\n# dt_ww_wresp = dt.Frame(np_ww_wresp,names=[\"weight\",\"resp\",\"wresp\"])\n# dt_train = dt.cbind([dt_train,dt_ww_wresp])","abc8259b":"# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# # features + weight\n# X = dt_train[:,1:-2]\n# y = dt_train[:,\"resp\"].to_pandas()[\"resp\"].apply(lambda x: 1 if x > 0 else 0)\n\n# scaled_weight = scaler.fit_transform(X[:,\"weight\"].to_pandas())\n# X[:,\"weight\"] = scaled_weight\n\n# class TimeSeries_Dataset(Dataset):\n#     def __init__(self,X,y,seq_len=28):\n#         super(TimeSeries_Dataset,self).__init__()\n#         self.X = X\n#         self.y = y\n#         self.seq_len = seq_len\n        \n#     def __len__(self):\n#         return self.X.shape[0] - (self.seq_len) \n        \n        \n#     def __getitem__(self,index):\n#         X = torch.tensor(self.X[index:index+self.seq_len,:].to_numpy(), dtype=torch.float)\n#         y = torch.tensor(self.y[index+self.seq_len], dtype=torch.float)\n#         return X,y","42ffc676":"# date for dictionary\n# np_dates = np_date_drop[:,0]\n# np_action = np.array(list(map(lambda x: 1 if x > 0 else 0,np_ww_wresp[:,1])))","d7c50a11":"# X = np_lar_train\n# y = np.array(list(map(lambda x: 1 if x > 0 else 0 ,np_ww_wresp[:,1])))","70a7b0d6":"# class TimeSeries_Dataset(Dataset):\n#     def __init__(self,dates,X,dict_avg,dict_std,y,seq_len=28):\n#         super(TimeSeries_Dataset,self).__init__()\n#         self.dates = dates\n#         self.X = X\n#         self.y = y\n#         self.dict_avg = dict_avg\n#         self.dict_std = dict_std\n#         self.seq_len = seq_len\n        \n#     def __len__(self):\n#         return self.X.shape[0] - (self.seq_len) \n        \n        \n#     def __getitem__(self,index):\n#         date = self.dates[index]\n#         X = self.X[index:index+self.seq_len,:]\n#         # Main reason of low performance\n#         np_avg = np.array(list(map(lambda x: self.dict_avg[x],self.dates[:self.seq_len])))\n#         np_std = np.array(list(map(lambda x: self.dict_std[x],self.dates[:self.seq_len])))\n        \n#         X = torch.tensor(np.concatenate([X, np_avg, np_std], axis=1),dtype=torch.float) \n#         y = torch.tensor(self.y[index+self.seq_len], dtype=torch.float)\n#         return X,y","9414a21b":"# train_dataset = TimeSeries_Dataset(np_dates, X, dict_mov_avg, dict_mov_std, y,seq_len=31)","c851ba5b":"# np_mov_avg = np.array(list(map(lambda x: dict_lar_mov_avg[x],np_dates)))\n# np_mov_std = np.array(list(map(lambda x: dict_lar_mov_std[x],np_dates)))","1336b220":"# class TimeSeries_Dataset(Dataset):\n#     def __init__(self,X,X_avg,X_std,y,seq_len=28):\n#         super(TimeSeries_Dataset,self).__init__()\n#         self.X = X\n#         self.y = y\n#         self.X_avg = X_avg\n#         self.X_std = X_std\n#         self.seq_len = seq_len\n        \n#     def __len__(self):\n#         return self.X.shape[0] - (self.seq_len) \n        \n        \n#     def __getitem__(self,index):\n#         X = self.X[index:index+self.seq_len,:]\n#         np_avg = self.X_avg[index:index+self.seq_len,:]\n#         np_std = self.X_std[index:index+self.seq_len,:]\n        \n#         X = torch.tensor(np.concatenate([X, np_avg, np_std], axis=1),dtype=torch.float) \n#         y = torch.tensor(self.y[index+self.seq_len], dtype=torch.float)\n#         return X,y","fd5fb156":"# train_dataset = TimeSeries_Dataset(X, np_mov_avg, np_mov_std, y,seq_len=31)","94e1f3e6":"X = np_train\ny = np.array(list(map(lambda x: 1 if x > 0 else 0 ,np_ww_wresp[:,1])))","b463619f":"class TimeSeries_Dataset(Dataset):\n    def __init__(self,X,y,seq_len=28):\n        super(TimeSeries_Dataset,self).__init__()\n        self.X = X\n        self.y = y\n        self.seq_len = seq_len\n        \n    def __len__(self):\n        return self.X.shape[0] - (self.seq_len) \n        \n        \n    def __getitem__(self,index):\n        X = torch.tensor(self.X[index:index+self.seq_len,:], dtype=torch.float)\n        y = torch.tensor(self.y[index+self.seq_len], dtype=torch.float)\n        return X,y","774464c8":"train_dataset = TimeSeries_Dataset(X,y,seq_len=31)","f7707033":"class LSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTM,self).__init__()\n        self.hidden_dim = hidden_dim\n        self.layer_dim = layer_dim\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n        # self.fc = nn.Linear(hidden_dim,output_dim)\n        \n        # take sigmoid for Classification\n        self.fc = nn.Sequential(\n                      nn.Linear(hidden_dim,output_dim),\n                      nn.Sigmoid()\n                    )\n        \n    def forward(self, x):\n        h0,c0 = self.init_state(x)\n        output, (h_n, c_n) = self.lstm(x,(h0,c0))\n        out = self.fc(output[:,-1,:])\n        return out\n        \n    def init_state(self, x):\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n        return h0, c0","7bb26cea":"batch_size = 4096\nlearning_rate = 0.01\nepochs = 10\n\nseq_len = 31\ninput_dim = train_dataset[0][0].shape[1]\nhidden_dim = 256\nlayer_dim = 2\noutput_dim = 1\n\nmodel = LSTM(input_dim, hidden_dim, layer_dim, output_dim)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(),lr=learning_rate)","57ae5750":"train_size = int(len(train_dataset) * 0.8)\nvalid_size = len(train_dataset) - train_size","56020594":"train_dataset, valid_dataset= torch.utils.data.random_split(train_dataset, [train_size, valid_size], generator=torch.Generator().manual_seed(1029))","2cecc012":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) \nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)","741caf1f":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"","09bc5ba4":"train_score = defaultdict(list)\nvalid_score = defaultdict(list)\n\nmodel = model.to(device)\nbest_auc = 0\n\nIS_CONTIN = False\n\nfile_name = f\"LSTM_{input_dim}_{hidden_dim}_{layer_dim}.pth\"\nmodel_file_path = os.path.join(project_home, file_name)\n\nif ~IS_CONTIN:\n    if os.path.isfile(model_file_path):\n        os.remove(model_file_path)\nelse:\n    model.load_state_dict(torch.load(model_file_path))\n\nfor epoch in tqdm_notebook(range(epochs)):\n\n    train_acc = 0\n    train_auc = 0\n\n    valid_acc = 0\n    valid_auc = 0\n\n    for idx, (inputs, label) in enumerate(train_dataloader):\n        model.train()\n        optimizer.zero_grad()\n\n        inputs = inputs.to(device)\n        label = label.to(device).unsqueeze(1)\n        \n        outputs = model(inputs)\n        loss = criterion(outputs,label)\n        loss.backward()\n        optimizer.step()\n\n        train_preds = np.array(list(map(lambda x: 1 if x > 0.5 else 0,outputs.cpu().detach().numpy())))\n\n        train_batch_acc = (np.concatenate(label.cpu().detach().numpy()) == train_preds).sum() \/ inputs.size(0)\n        train_batch_auc = roc_auc_score(label.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n\n        train_acc += train_batch_acc \/ len(train_dataloader)\n        train_auc += train_batch_auc \/ len(train_dataloader)\n\n    train_score[\"acc\"].append(train_acc)\n    train_score[\"auc\"].append(train_auc)\n\n    with torch.no_grad():\n        model.eval()\n        for idx, (inputs, label) in enumerate(valid_dataloader):\n\n            inputs = inputs.to(device)\n            label = label.to(device).unsqueeze(1)\n\n            outputs = model(inputs)\n\n            valid_preds = np.array(list(map(lambda x: 1 if x > 0.5 else 0,outputs.cpu().detach().numpy())))\n\n            valid_batch_acc = (np.concatenate(label.cpu().detach().numpy()) == valid_preds).sum() \/ inputs.size(0)\n            valid_batch_auc = roc_auc_score(label.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n\n            valid_acc += valid_batch_acc \/ len(valid_dataloader)\n            valid_auc += valid_batch_auc \/ len(valid_dataloader)\n\n    print(f\"EPOCH:{epoch+1}|{epochs}; ACC(train\/valid):{train_acc:.4f}\/{valid_acc:.4f}; ROC_AUC(train\/valid):{train_auc:.4f}\/{valid_auc:.4f}\")\n    \n    valid_score[\"acc\"].append(valid_acc)\n    valid_score[\"auc\"].append(valid_auc)\n\n    if valid_auc > best_auc:\n        print(f\"best model changed {best_auc:.4f} -> {valid_auc:.4f}\")\n        best_model_state = deepcopy(model.state_dict())\n#         torch.save(best_model_state, model_file_path)\n        best_auc = valid_auc\n\nmodel.load_state_dict(best_model_state)","cc5b983e":"# import janestreet\n\n# model.eval()\n# X_test = None\n# env = janestreet.make_env()\n# env_iter = env.iter_test()\n# for (idx,(test_df, pred_df)) in enumerate(tqdm_notebook(env_iter)):\n#     if test_df['weight'].item() > 0:\n#         weight = test_df.loc[:,\"weight\"].reset_index(drop=True)\n#         date = test_df.loc[:,\"date\"].reset_index(drop=True)\n        \n#         test_np = test_df.loc[:,test_df.columns[1:-1]].values\n#         isnan_np = np.isnan(test_np[:,1:])\n#         test_np[:,1:][isnan_np] = f_mean[np.newaxis,:][isnan_np]\n#         X_target = pd.DataFrame(test_np,columns = features).reset_index(drop=True)\n        \n#         test_df = pd.concat([weight, X_target, date],axis=1)\n        \n#         if X_test is None:\n#             X_test = pd.concat([X_target for _ in range(seq_len)],axis=0)\n            \n#         X_test = pd.concat([X_test.iloc[1:], X_target] ,axis=0)\n#         preds = model(torch.tensor(X_test.values[np.newaxis,:], dtype=torch.float).to(device))\n#         preds = preds.cpu().detach().numpy()\n#         action = 1 if preds > 0.5 else 0\n#         pred_df.action = action\n#     else:\n#         pred_df.action = 0\n#     env.predict(pred_df)","a41bd434":"import janestreet\n\nmodel.eval()\nX_test = None\nenv = janestreet.make_env()\nenv_iter = env.iter_test()\nfor (idx,(test_df, pred_df)) in enumerate(tqdm_notebook(env_iter)):\n    if test_df['weight'].item() > 0:\n        test_df = pd.DataFrame(test_df, columns=features)\n        \n        test_df_T = test_df.transpose()\n        index_np = test_df.isnull().values.reshape(-1,1)\n        f_mean_cols = test_df_T.loc[index_np].index\n        f_mean_col_nums = list(map(lambda x: int(x.split(\"_\")[1]), f_mean_cols))\n        \n        for col, col_num in zip(f_mean_cols,f_mean_col_nums):\n            test_df.loc[:,col] = f_mean[col_num-1]\n        \n        if X_test is None:\n            X_test = pd.concat([test_df for _ in range(seq_len)],axis=0)\n            \n        X_test = pd.concat([X_test.iloc[1:], test_df] ,axis=0)\n        preds = model(torch.tensor(X_test.values[np.newaxis,:], dtype=torch.float).to(device))\n        preds = preds.cpu().detach().numpy()\n        action = 1 if preds > 0.5 else 0\n        pred_df.action = action        \n        \n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","c09bf4c1":"[](http:\/\/)","f395ed51":"Using datatable is too slow during training process, Although dt(datatable) was efficient for merging data without any memory leakage issue.","9333830b":"### Prediction\n\n#### Results of FEs\n\nI get results in empirical way. it seems imgs below.\n\n**So, I decided apply Origin data without FE method**\n\nParameter\n    * batch_size = 4096\n    * learning_rate = 0.01\n    * epochs = 10\n    * seq_len = 31\n    * hidden_dim = 256\n    * layer_dim = 2\n    * output_dim = 1\n\n* Origin\n\n![Result_origin.png](attachment:Result_origin.png)\n\n* Origin_with_Rolling\n\n![Result_origin_roll.png](attachment:Result_origin_roll.png)\n\n* Lar\n\n![Result_lar_heirarchi.png](attachment:Result_lar_heirarchi.png)\n\n* Lar_rolling\n\n![Result_lar_heirarchi_roll.png](attachment:Result_lar_heirarchi_roll.png)\n\n* Den\n\n![Result_den_heirarchi.png](attachment:Result_den_heirarchi.png)\n\n* Den_rolling\n\n![Result_den_heirarchi_roll.png](attachment:Result_den_heirarchi_roll.png)","3cecc4b1":"#### 2. Handling Outliers\n\nBecause we fill null values with mean-based way, we could make a line as n percentiles like below plots.","0960a4b7":"#### 3. Clustering\n\n**Reference**\n\n\\- <a href=\"https:\/\/www.kaggle.com\/apolitano20\/jane-street-features-hierarchical-clustering\">Features Hierarchical Clustering<\/a>\n\nhttps:\/\/www.kaggle.com\/apolitano20\/jane-street-features-hierarchical-clustering\n\n**Additional Context with Reference**\n\n\\- Denoising the correlation matrix using 'Random Matrix Theory'\n\nAndrea Politano who is Author of reference above said like this in comment of notebook.\n\nQ. Great work, I think that engineering features using information-theoretic reasoning instead of correlation-based reasoning is better given the noisiness of the data.\n\nA. Agree. **Another possible approach is using random matrix theory to denoise the correlation matrix** (https:\/\/arxiv.org\/pdf\/1610.08104.pdf). It would be interesting indeed to see **what happens to the correlation-based distance when calculated with a denoised correlation matrix.**\n\n**So, I'll gonna denoise the correlation matrix using 'Random Matrix Theory' where it is in github below**\n\nhttps:\/\/github.com\/GGiecold\/pyRMT\n\n========================================================================================================================\n\n**\"All contexts below are from Reference\"**\n\n\\- <a href=\"https:\/\/www.kaggle.com\/apolitano20\/jane-street-features-hierarchical-clustering\">Features Hierarchical Clustering<\/a>\n\nhttps:\/\/www.kaggle.com\/apolitano20\/jane-street-features-hierarchical-clustering\n\n**partial vs hierarchical**\n\n* partitial: produce a flat partition, where an element belongs to one and only one cluster\n\n* hierarchical: create a multi-layered partition, with clusters of elements at the bottom level, clusters of clusters at the next level, and so on, until a single all-inclusive cluster at the top of that hierarchy.\n\n    \\- aglomerative(to inclusive side), divisive(to individual side): difference btw direction to make cluster)\n    \n**Types of cluster models**\n \n* Connectivity models: (e.g.: hierarchical clustering) are based on a notion of distance connectivity.\n\n* Centroid models: (e.g.: k-means algorithm) represents each cluster by a single mean vector.\n\n* Distribution models: (e.g.: EM algorithms) clusters are modeled using statistical distributions.\n\n* Density models: (e.g.: DBSCAN, OPTICS) defines clusters as connected dense regions in the data space.\n\n* Graph-based models: (e.g.: HCS clustering algorithm) work by representing the similarity in data through a similarity graph and then finding all the highly connected subgraphs.\n\nAmong hierarchical methods, there are two representative choice, 'correlation distance based' and 'Information-based'\n\n","045f4ab1":"Origin Dataset spends about 2mins for training 1epoch","af86c614":"Enhancing Performance for this Dataset","c975dbf0":"# Jane_Pytorch-LSTM-Implementation \ud83d\udd25\n\n## References\n\n### EDAs\n1. <a href=\"https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance\">Jane Street: EDA of day 0 and feature importance<\/a>\n\n2. <a href=\"https:\/\/www.kaggle.com\/muhammadmelsherbini\/jane-street-extensive-eda-pca-starter\">\nJane_street_Extensive_EDA & PCA starter \ud83d\udcca\u26a1<\/a>\n\n3. <a href=\"https:\/\/www.kaggle.com\/hamzashabbirbhatti\/eda-a-quant-s-prespective\">\nEDA \/ A Quant's Prespective<\/a>\n\n4. <a href=\"https:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day\">NaN values depending on Time of Day<\/a>\n\n5. <a href=\"https:\/\/www.kaggle.com\/sbunzini\/reduce-memory-usage-by-75\">Reduce Memory Usage by 75%<\/a>\n\n### Feature Engineering\n\n1. <a href=\"https:\/\/www.kaggle.com\/gogo827jz\/optimise-speed-of-filling-nan-function\">Optimise Speed of Filling-NaN Function<\/a>\n\n2. <a href=\"https:\/\/www.kaggle.com\/apolitano20\/jane-street-features-hierarchical-clustering\">Features Hierarchical Clustering<\/a>\n\n3. <a href=\"https:\/\/github.com\/GGiecold\/pyRMT\"> [github] (pyRMT) Randomized Matrix Theory Python Code<\/a>\n\n4. <a href=\"https:\/\/www.kaggle.com\/lucasmorin\/running-algos-fe-for-fast-inference\">\ud83c\udfc3 Running Algos \ud83c\udfc3 FE for fast inference<\/a>\n\n### Implementing Pytorch-LSTM \ud83d\udd25\n\n1. <a href=\"https:\/\/www.kaggle.com\/omershect\/learning-pytorch-lstm-deep-learning-with-m5-data\">Learning Pytorch LSTM Deep Learning with M5 Data<\/a>\n\n2. <a href=\"https:\/\/www.kaggle.com\/backtracking\/lstm-baseline-pytorch\">LSTM-Baseline-Pytorch<\/a>\n\n## Abstract\n\nThis kernel is approach using LSTM via Pytorch. For now when a month left till end of competition, **Most people have focused 'Bottle-Neck AE + MLP with Tuning' Solution for taking 1st place.** Although the situation is like above, **I make a note for LSTM due to my curiosity.**\n\nI have seen some LSTM implements at M5-accurcy competition like my references. And, for adapting it in JaneStreet i read LSTM refs like above. **I really appreciate about their effort what they want to share their experts.**\n\n**In conclusion, first, I have thought this competition is not good for using LSTM.** Because we just get one trade opportunity when we make a prediciton(It's called 'Bottle Neck'). So, There is a problem when we fill null data unlike training data. That's why there is a few rnn and lstm implementations. Even though there is this kind of problem, by stacking last data gradually within sequence length size.\n\nBefore doing best solution of this compeition, **I'll do adapt some apporaches for increasing LSTM perfomance as long as I can.** \n\n**I'll leave comment when I finish to adapt some approaches what I want. :D**\n\n## Comment at 2021-02-01\n\nI commited about Performance Check per Dataset(Origin\/Large Hierarchi\/Denoise Hierarchi | with rolling data).\nUnfortunately, Addional FE datasets are pretty bad perfomance during validation.\nSo I choose Origin Dataset with mean-filling and 6-sigma Confidence Interval deletion.\n\nI'll share submission result of it later.\n\nAnd I'll explore Best Solution soon within 1~2days.\n\nI really glad to you guys about reacting my kernel.\n\n**And plz give a upvote to the References what I saw and applied.**\n\nThanks to you all!!\n\n\n## Timeline\n\n* **[Day1]** 2021-01-19 \n\n    1. LSTM-pytorch baseline test running for submission (epoch:1; input_dim:130; hidden_dim:256; layer_dim:2)\n    2. Filling NaN Values \/ Handling Outlier\n    3. Feature Hierarchical Clustering (Correlation-based distance, Variance of Information, Denoise Large Correlation by Random Matrix Theory)\n    \n* **[Day2]** 2021-01-20\n\n    1. Filling NaN Values \/ Handling Outlier\n    \n* **[Day3]** 2021-01-20\n\n    1. Handling Outlier with **overcoming memory leakage issue**\n    2. Application of Hierarchical Clustering Corrleation Distance Method (Non-Denoise, Denoise)\n    \n* **[Day4]** 2021-01-26\n\n    1. Application of Hierarchical Clustering Corrleation Distance Method (Non-Denoise, Denoise)\n    2. Create Squential Dataset\n    \n* **[Day5]** 2021-01-27\n\n    1. Hierarchical Clustered FE dataset (Large Corr and Denoise Large Corr)  \n  \n* **[Day6]** 2021-01-28\n\n    1. Creating Additional Features (rolling mean\/std per time length)\n    2. Create Squence using preprocessed data for LSTM model\n    \n* **[Day7]** 2021-01-29\n\n    1. Merging processed data\n    2. Create pytorch dataset\n\n* **[Day8]** 2021-01-31\n\n    1. Handling Training Speed issue (due to data broadcasting per date of rolling mean and std)\n    \n* **[Day8]** 2021-02-01\n\n    1. Performance Check per Dataset(Origin\/Large Hierarchi\/Denoise Hierarchi | with rolling data)\n    2. Commenting EDA codes for actual prediction","c15d4220":"As I said via refereces, there are null values on two times per day when beginning and mid-time of days.\n\nIt said there is **'pre-order trading'** in some stock market and **'lunch-break'**. It seems reasonable due to as shareholder of stock market in my country.\n\nThen, How we gonna fill NaN value?\n\nThere is the way to delete NaN value when we make a prediction. But I don't want to.\n\nSo, Let we check some plots for our features.\n* Distribution per feature\n* Boxplot per feature\n* Scatter plot with resp\n* Cumsum per featrue","846e9d57":"## Implementation\n\n### Process for Time-Series Data\n\nImage From:\n\n<a href=\"https:\/\/www.kaggle.com\/omershect\/learning-pytorch-lstm-deep-learning-with-m5-data\">\nLearning Pytorch LSTM Deep Learning with M5 Data<\/a>\n\n\\- https:\/\/www.kaggle.com\/omershect\/learning-pytorch-lstm-deep-learning-with-m5-data\n\n![M5_Process-of-Time-Series.png](attachment:M5_Process-of-Time-Series.png)","52388cf7":"If I set sigma to 6, **6% of data was deleted.**","43c98198":"Before Creating Additional Features, Scaling Dataset first.","7ef9c2ec":"##### **Denoising Correlation-based distance by Randmomized Matrix Theory**\n\n**Reference**\n\n<a href=\"https:\/\/github.com\/GGiecold\/pyRMT\"> [github] (pyRMT) Randomized Matrix Theory Python Code<\/a>","94c22ab0":"#### 4. Creating Additional Features (for time-series data)\n\nNow, It's time to create Additional Feature for Time- series by using data cleaned by Null value Imputation, Removing Outliers, and Clustering.","3651c0a9":"#### 1. Filling NaN Values\n\n\nBefore making various features, I'd like to fill null value on out dataset because I couldn't use later method what it will be adapted.\n\nFirst we need to take a look at null distribution on our dataset for finding some patterns.\n\nThe pattern is that null values are occurred at the beginning of trade and mid-time of trade day and it's showed per intra-day pattern like references.\n\n* <a href=\"https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance\">Jane Street: EDA of day 0 and feature importance<\/a>\n* <a href=\"https:\/\/www.kaggle.com\/tomwarrens\/nan-values-depending-on-time-of-day\">NaN values depending on Time of Day<\/a>\n\n\n","5af7e260":"Using datatable is too slow during training process, Although dt(datatable) was efficient for merging data without any memory leakage issue.","b497dd7d":"### Reduce Memory Usage\n\n* <a href=\"https:\/\/www.kaggle.com\/omershect\/learning-pytorch-lstm-deep-learning-with-m5-data\">Learning Pytorch LSTM Deep Learning with M5 Data<\/a>\n* <a href=\"https:\/\/www.kaggle.com\/sbunzini\/reduce-memory-usage-by-75\">Reduce Memory Usage by 75%<\/a>\n\nWe need to reduce memory usage because our dataset is too huge.","8d7d2132":"There are **several LINKAGE ways** define how to cluster features in Hierarchical Clustering\n\n* Average\n* Complete\n* Single\n\nI Just tried **Average** one for my kernel","a71261bb":"I think that most features have **normal distribution**.\n\nThe mean method would be nice for fillnas.\n\nWhen it comes to fill NaN values, howerver, there are **two representative ways** in notebooks of competition.\n* **mean**\n* **fill-forward**\n\nThen, we need to use **efficient way for filling NaN values** because our dataset is so huge what we are talking about above.\n\nReference below is nice way to implement it what we want **using numba package**.\n\n<a href=\"https:\/\/www.kaggle.com\/gogo827jz\/optimise-speed-of-filling-nan-function\">Optimise Speed of Filling-NaN Function<\/a>\n\n**I fill NaN values as mean-based method** because most features have **normal distribution**","6e11dab2":"### Feature Engineering\n\n1. Filling NaN Values\n\n2. Handling Outlier\n\n3. Clustering using features\n\n4. Creating Additional Features(for time-series data)","57470764":"##### **Correlation-based distance(w\/o denoising)**\n\n**\"All contexts below are from Reference\"**\n\n\\- <a href=\"https:\/\/www.kaggle.com\/apolitano20\/jane-street-features-hierarchical-clustering\">Features Hierarchical Clustering<\/a>\n\nhttps:\/\/www.kaggle.com\/apolitano20\/jane-street-features-hierarchical-clustering\n\nLet us analyze the correlation-based metric. Consider the  N\u00d7F  feature matrix, where in our case  F=131(w\/ resp or wresp)  and  N  is the number of observations. We would like to combine these  F  column vectors into a hierarchical structure of clusters. The steps are as follows:\n\ncompute the  F\u00d7F  correlation matrix  \u03c1={\u03c1i,j}i,j=1,...,N \nderive a distance matrix  D=di,j=12(1\u2212\u03c1i,j)\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u221a \ncompute the linkage matrix  L  from the distance matrix  D \nrearrange the rows and columns of the correlation matrix so that the largest values lie along the diagonal (Quasi-Diagonalization).\nTo visualize the distance between features, we will use two very helpful tools: Dendrograms and Minimum Spanning Trees.\n\nIt can be shown [2] that  di,j  is a distance in the mathematical sense. If two features are perfectly anticorrelated (\u03c1=-1), the distance between them is d=1, while if they are perfectly correlated (\u03c1=1), the distance is d=0.\n\nThe distance as a function of the Pearson correlation coefficient is plotted below.","0598097d":"Spliting Dataset for Empirical Comparison \n* Origin\n* Large Corr Hierarchical Cluster\n* Denoised Corr Hierarchical Cluster ","fbad45ba":"### Import packages and dataset","64d0bda8":"Additional Dataset spends about 11mins for training 1epoch\n\nThe main reason of low performance is concatenating process in Pytorch Dataset. \n\nThe reason was **broadcasting of daily dictionary to Dataset**","911480c6":"There are many outliers although we set whisker of boxplot between 0.01 and 0.99.\n\nIf we delete data with this sense, about 8 % of data is deleted (In EDA).\n\nWith same manner of it, I executed to remove those using standard deviation with sigma like handling normal distribution","dfab82b6":"#### Create Dataset by Time-wise seqeunce for LSTM\n\nImage from:\n\n<a href=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\">Understanding LSTM Networks<\/a>\n\n\\- https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n\n![M5_single-lstm.png](attachment:M5_single-lstm.png)\n\nWe need to make 'Sequence' for training LSTM Model above. In above image, the Xt0 .. Xt27 means a fragment per sequence. So we gather Xt0...Xt27 to XT0 as simliar single row common dataset. The process to make dataset seems like below image per time sequence(len). By gathering by XT0...XT(N-28-1[due to number begins from 0]), we could train LSTM model like ordinary point-wise ML model.\n\nImage from:\n\n<a href=\"https:\/\/www.kaggle.com\/omershect\/learning-pytorch-lstm-deep-learning-with-m5-data\">\nLearning Pytorch LSTM Deep Learning with M5 Data<\/a>\n\n\\- https:\/\/www.kaggle.com\/omershect\/learning-pytorch-lstm-deep-learning-with-m5-data\n\n![M5_create-squence.png](attachment:M5_create-squence.png)","a2775f2a":"### Setting Random seed for prediction\n\nReference from: \n\nhttps:\/\/github.com\/pytorch\/pytorch\/issues\/7068","a30715dc":"Among the way creating additional feature, I just tried rolling average and std per specific time sequence.\n**(I'll add various one later like reference's way)**\n\n* Rolling Average - moving_avg_per_date (like pandas.Series.rolling.mean())\n* Rolling Std - moving_std_per_date (like pandas.Series.rolling.std())\n\nImages from:\n\n<a href=\"https:\/\/www.kaggle.com\/omershect\/learning-pytorch-lstm-deep-learning-with-m5-data\">\nLearning Pytorch LSTM Deep Learning with M5 Data<\/a>\n\n\\- https:\/\/www.kaggle.com\/omershect\/learning-pytorch-lstm-deep-learning-with-m5-data\n\n\n![M5_feature-lag.png](attachment:M5_feature-lag.png)\n\n![M5_feature-rolling-mean.png](attachment:M5_feature-rolling-mean.png)"}}