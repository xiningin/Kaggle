{"cell_type":{"0616389e":"code","4fa580e6":"code","d3485345":"code","d9b39913":"code","175be5c9":"code","3e63bc85":"code","bd748f50":"code","0951ad2a":"code","576c7f6b":"code","54f0467c":"code","408faee4":"code","ed06c68d":"code","87d01e90":"code","cfa0e729":"code","cbd5ad91":"code","93ad6ff1":"code","3d362848":"code","c1604b18":"markdown","24aa8ecf":"markdown","b7888150":"markdown","08fe417e":"markdown","af963742":"markdown","4a605cd2":"markdown","f0bf4558":"markdown","f3adde8b":"markdown","8629f166":"markdown"},"source":{"0616389e":"import pandas as pd\n\ndata = pd.read_table('..\/input\/TrainingDataSet_Maize.csv', index_col=0)\n","4fa580e6":"data.describe()","d3485345":"y = data.yield_anomaly\ncolumns_of_interest = ['year_harvest', 'NUMD', 'IRR', 'ETP_1', 'ETP_2',\n       'ETP_3', 'ETP_4', 'ETP_5', 'ETP_6', 'ETP_7', 'ETP_8', 'ETP_9', 'PR_1',\n       'PR_2', 'PR_3', 'PR_4', 'PR_5', 'PR_6', 'PR_7', 'PR_8', 'PR_9', 'RV_1',\n       'RV_2', 'RV_3', 'RV_4', 'RV_5', 'RV_6', 'RV_7', 'RV_8', 'RV_9',\n       'SeqPR_1', 'SeqPR_2', 'SeqPR_3', 'SeqPR_4', 'SeqPR_5', 'SeqPR_6',\n       'SeqPR_7', 'SeqPR_8', 'SeqPR_9', 'Tn_1', 'Tn_2', 'Tn_3', 'Tn_4', 'Tn_5',\n       'Tn_6', 'Tn_7', 'Tn_8', 'Tn_9', 'Tx_1', 'Tx_2', 'Tx_3', 'Tx_4', 'Tx_5',\n       'Tx_6', 'Tx_7', 'Tx_8', 'Tx_9']\nX = data[columns_of_interest]\nX.describe()","d9b39913":"y.describe()","175be5c9":"from sklearn.tree import DecisionTreeRegressor\n\n# Define model\ndtr_model = DecisionTreeRegressor()\n\n# Fit model\ndtr_model.fit(X, y)\n","3e63bc85":"print(\"Making predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"The predictions are\")\nprint(dtr_model.predict(X.head()))","bd748f50":"from sklearn.metrics import mean_absolute_error\n\npredicted_yield_anoms = dtr_model.predict(X)\n# In-sample error\nmean_absolute_error(y, predicted_yield_anoms)","0951ad2a":"from sklearn.model_selection import train_test_split\n\n# split data into training, test and validation data, for both predictors and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state = 0, test_size=500)\ntrain_X, val_X, train_y, val_y = train_test_split(train_X, train_y, random_state = 0, test_size=500)\n\n\nprint(len(X), len(y))\nprint(len(train_X), len(train_y))\nprint(len(test_X), len(test_y))\nprint(len(val_X), len(val_y))\n# Define model\ndtr_model = DecisionTreeRegressor()\n# Fit model\ndtr_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nval_predictions = dtr_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))","576c7f6b":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return mae","54f0467c":"# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [2, 2, 3, 5, 50, 100, 500]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: {}  \\t\\t Mean Absolute Error:  {}\".format(max_leaf_nodes, my_mae))","408faee4":"# Let's search some more around 50\nfor max_leaf_nodes in [5, 10, 20, 50, 100, 200]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: {}  \\t\\t Mean Absolute Error:  {}\".format(max_leaf_nodes, my_mae))","ed06c68d":"# Let's search some more around 50\nfor max_leaf_nodes in [20, 30, 40, 45, 50, 55, 60, 70]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: {}  \\t\\t Mean Absolute Error:  {}\".format(max_leaf_nodes, my_mae))","87d01e90":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor()\nforest_model.fit(train_X, train_y)\nyield_anom_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, yield_anom_preds))","cfa0e729":"def get_mae_rf(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return mae","cbd5ad91":"# Read the test data\ntest = pd.read_table('..\/input\/TestDataSet_Maize_blind.csv')\n# Treat the test data in the same way as training data. In this case, pull same columns.\ntest_X = test[columns_of_interest]\n# Use the model to make predictions\npredicted_anoms = forest_model.predict(test_X)\n# We will look at the predicted prices to ensure we have something sensible.\nprint(predicted_anoms)","93ad6ff1":"test.describe()","3d362848":"#my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n\n#my_submission.to_csv('submission.csv', index=False)","c1604b18":"Fork of Kaggle\/Learn\/ML Advanced Regression\n\nThis is a quick and dirty baseline that is missing many basic pieces. It doesn't even use categorical variables (year, department number) correctly.","24aa8ecf":"#### Now we can tune max_leaf_nodes","b7888150":"#### So we'll pick max_leaf_nodes = 20\nThis value is tuned for this model on this dataset.\n\n## Random Forest\nThis is a much better model for this regression problem","08fe417e":"## Experimenting With Different Models\n\nWe can test different models and tune hyperparameters. An utility function will help.","af963742":"### Model validation using MAE","4a605cd2":"### Submit to the competition","f0bf4558":"### Let's setup a validation dataset and compute MAE on it","f3adde8b":"### Select some columns of interest","8629f166":"### Build a simple Decision Tree model"}}