{"cell_type":{"9a36c3f1":"code","cb786a81":"code","eb585c17":"code","c96b1d34":"code","3531e7fa":"code","9f8e0bd7":"code","8589091f":"code","54f2e9c6":"code","f89dc381":"code","bad273bb":"code","71889d08":"code","aa52fa86":"code","7684e151":"code","c76d79a3":"code","91c76505":"code","698f07e0":"code","ab622c3f":"code","f2815eda":"code","82d59a0c":"code","f6a87609":"code","41656f17":"code","a3f4594b":"code","ac48dac1":"code","26290051":"code","93c772cb":"code","09dd55d8":"code","b50e3fd3":"code","5cf46bc9":"code","c3389c13":"code","65273dcc":"code","258b92b7":"code","189ecfc5":"code","cf23b28b":"code","45763c59":"code","a6fadd6d":"code","7b635773":"code","8aab00aa":"code","4ad3d07d":"code","2145af71":"code","4b5ee56f":"code","72f45062":"code","afd6ea3c":"code","503e0c9d":"code","4fec7b2a":"code","dea2c8b3":"code","0c77c14b":"code","6f371d2f":"code","f5ecc169":"code","0a919e26":"code","ba3f6c5f":"code","27ad2405":"code","18ef4b4f":"code","3866cc19":"code","60769115":"code","ffefa46a":"code","336382ac":"code","32f15448":"code","13bee6d7":"code","16790f8d":"code","54138288":"code","d3904a25":"code","b88f76ad":"code","a5e971a1":"code","5ae86e57":"code","cf488e8b":"code","4fcd4fc0":"code","11e868c6":"code","23a629d2":"code","5ddebdf3":"code","9cefc0dc":"code","61bcd9d5":"code","661da172":"code","fe1b06e8":"code","8bc503b9":"code","d7f1fb2b":"code","2a453b0c":"code","3948e636":"code","c1b5282a":"markdown","b02f5d58":"markdown","315dd897":"markdown","d0d4f53a":"markdown","4e7b638f":"markdown","caac0d7b":"markdown","ab4a6f8a":"markdown","05af480c":"markdown","21519181":"markdown","9fe1dd5a":"markdown","5eacf659":"markdown","3bae94fd":"markdown","26f2cf98":"markdown","cfadc768":"markdown","d7f9eb18":"markdown","1b05a442":"markdown","a422effd":"markdown","8e46c728":"markdown","ce28a4f5":"markdown","79700f13":"markdown","621c8255":"markdown","598bc90b":"markdown","a843006e":"markdown","2abe3b7d":"markdown","a1c26b4e":"markdown","2b894e92":"markdown","e0b9427e":"markdown","2baadbb3":"markdown","d1f2d70a":"markdown","05d4dcfa":"markdown","530441c5":"markdown","139d65bc":"markdown","1c1f85d3":"markdown","6e156788":"markdown","30e39a7f":"markdown","b060c3a0":"markdown","a3e28bac":"markdown","4173b416":"markdown","abe4ba3d":"markdown","45e0ebc3":"markdown","cac742b6":"markdown","901f92f4":"markdown","bfa769d3":"markdown"},"source":{"9a36c3f1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import kurtosis, skew\n%matplotlib inline\nfrom pylab import rcParams\nimport statsmodels.api as sm\nfrom scipy.stats import linregress\nimport pylab\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\n\nplt.rcParams[\"figure.figsize\"] = 8,4\nimport warnings\nwarnings.filterwarnings(\"ignore\")","cb786a81":"df = pd.read_csv(\"..\/input\/StudentsPerformance.csv\")\n#df = pd.read_csv(\"StudentsPerformance.csv\")","eb585c17":"df.head(5)","c96b1d34":"# Check for possible null values in the dataset as missing values potentially falsify the statistical models\ndf.isnull().sum()\n# the dataset does not contain missing values","3531e7fa":"df.describe()\n# the describe method gives a quick overview of basic metrics of the dataset","9f8e0bd7":"df[\"gender\"] = df[\"gender\"].astype(\"category\")\ndf[\"race\/ethnicity\"] = df[\"race\/ethnicity\"].astype(\"category\")\ndf[\"parental level of education\"] = df[\"parental level of education\"].astype(\"category\") \ndf[\"lunch\"] = df[\"lunch\"].astype(\"category\")\ndf[\"test preparation course\"] = df[\"test preparation course\"].astype(\"category\") ","8589091f":"# to get a more generalised idea about performance we add a total score field to the dataset which sums up all exam scores\ndf[\"total\"] = df[\"math score\"] +df[\"reading score\"]+df[\"writing score\"]","54f2e9c6":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\naxes[0].hist(df[\"math score\"], bins=20,  color = \"skyblue\")\naxes[0].set_xlabel(\"Math Score\")\naxes[1].hist(df[\"writing score\"], bins=20,  color = \"orange\")\naxes[1].set_xlabel(\"Writing Score\")\naxes[2].hist(df[\"reading score\"], bins=20,  color = \"blue\")\naxes[2].set_xlabel(\"Reading Score\")\n\nfig.tight_layout()\n","f89dc381":"# investigate Skewness and Kurtosis\nstats.describe(df[\"math score\"])","bad273bb":"# To check for normality we need to get the standardized skweness and kurtosis scores\nmath_score_skew = skew(df[\"math score\"])\nprint(\"Math score skweness: \" , math_score_skew)\n#standard error of skewness =SQRT(6\/N)\nstandard_error_of_skewness = (6\/ len(df[\"math score\"])  ) ** 0.5\nprint(\"Standard error of skweness\", standard_error_of_skewness)\nprint(\"Standardized skewness: \", math_score_skew \/ standard_error_of_skewness)","71889d08":"# a rough formula for the standard error for kurtosis is =SQRT(24\/N)\nstandard_error_of_kurtosis = (24\/  len(df[\"math score\"]) )**0.5\nprint(\"Standard error of skewness\" , standard_error_of_kurtosis)\nprint(\"Standardized kurtosis\", kurtosis(df[\"math score\"]) \/ standard_error_of_kurtosis )","aa52fa86":"# plot histogram\nplt.hist(df[\"math score\"], bins= np.arange(0,100,3)   ,density=True ,color=['gray']  ,edgecolor='black', linewidth=1.0,)\nplt.ylabel(\"Density\")\nplt.xlabel(\"Grade\")\n\n# plot normalverteilung\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\nmu, std = norm.fit(df[\"math score\"])\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\nplt.grid(True)\nplt.show()","7684e151":"sm.qqplot(df[\"math score\"], line ='s',color=\"grey\" )\npylab.show()","c76d79a3":"sorted_standardized = sorted(preprocessing.scale(df[\"math score\"]))\nsorted_standardized_not_in1_96 = [i for i in sorted_standardized if (i > 1.96 or i < -1.96)]\nprint(\"Fall outside: \",len(sorted_standardized_not_in1_96)  )\nprint(\"Thats a total of \", round((len(sorted_standardized_not_in1_96)\/ len(sorted_standardized)) *100,2) ,\"%\"  )","91c76505":"sorted_standardized = sorted(preprocessing.scale(df[\"math score\"]))\nsorted_standardized_not_in3_29 = [i for i in sorted_standardized if (i > 3.29 or i < -3.29)]\nprint(\"Fall outside: \",len(sorted_standardized_not_in3_29)  )\nprint(\"Thats a total of \", round((len(sorted_standardized_not_in3_29)\/ len(sorted_standardized)) *100,2) ,\"%\"  )","698f07e0":"# investigate Skewness and Kurtosis\nstats.describe(df[\"reading score\"])","ab622c3f":"# To check for normality we need to get the standardized skweness and kurtosis scores\nreading_score_skew = skew(df[\"reading score\"])\nprint(\"reading score skweness: \" , reading_score_skew)\n#standard error of skewness =SQRT(6\/N)\nstandard_error_of_skewness = (6\/ len(df[\"reading score\"])  ) ** 0.5\nprint(\"Standard error of skweness\", standard_error_of_skewness)\nprint(\"Standardized skewness: \", reading_score_skew \/ standard_error_of_skewness)","f2815eda":"# plot histogram\nplt.hist(df[\"reading score\"], bins= np.arange(0,100,3)   ,density=True ,color=['gray']  ,edgecolor='black', linewidth=1.0,)\nplt.ylabel(\"Density\")\nplt.xlabel(\"Grade\")\n\n# plot normal distribution\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\nmu, std = norm.fit(df[\"reading score\"])\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'k', linewidth=2)\nplt.grid(True)\nplt.show()","82d59a0c":"sm.qqplot(df[\"reading score\"], line ='s',color=\"grey\" )\npylab.show()","f6a87609":"sorted_standardized = sorted(preprocessing.scale(df[\"reading score\"]))\nsorted_standardized_in1_96 = [i for i in sorted_standardized if (i > 1.96 or i < -1.96)] # get elements outside boundary\nprint(\"Fall outside: \",len(sorted_standardized_in1_96)  )\nprint(\"Thats a total of \", round((len(sorted_standardized_in1_96)\/ len(sorted_standardized)) *100,2) ,\"%\"  )","41656f17":"result = linregress(df[\"math score\"], df[\"reading score\"])#\nprint(result)","a3f4594b":"# an alternative to get Pearson's correlation coefficient directly.\nstats.pearsonr(df[\"math score\"], df[\"reading score\"])","ac48dac1":"dfc= df.drop(columns= \"total\")\ndfc.corr(method='pearson')\nf,ax = plt.subplots(figsize=(7, 7))\nsns.heatmap(dfc.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax)\nplt.show()","26290051":"sns.countplot(x=df[\"gender\"], palette=\"Set3\")\n# both genders are representated equally in the data sample","93c772cb":"df.groupby(\"gender\")[\"math score\"].describe()","09dd55d8":"male = df[df[\"gender\"] == \"male\" ]\nmale.reset_index(inplace= True)\nfemale = df[df[\"gender\"] == \"female\" ]\nfemale.reset_index(inplace= True)","b50e3fd3":"stats.levene(male[\"math score\"] , female[\"math score\"])","5cf46bc9":"t , p = stats.ttest_ind(male[\"math score\"] , female[\"math score\"] , equal_var = True)\nprint(stats.ttest_ind(male[\"math score\"] , female[\"math score\"] , equal_var = True))","c3389c13":"n1 = len(male)\nn2 = len(female)\neta_squared = (t**2)\/ (t**2+ n1+ n2 - 2)\nprint(eta_squared)","65273dcc":"palette ={\"male\":\"C3\",\"female\":\"C4\"}\nvis1= sns.lmplot(data= df, x=\"math score\", y=\"reading score\", fit_reg=False,palette = palette, hue=\"gender\", height=8, aspect=1)\nvis1.ax.plot((0,100),(0,100) ,c=\"gray\", ls=\"--\")\nplt.show()","258b92b7":"f, axes = plt.subplots(1,3,figsize=(15,5), sharex=True, sharey=True)\nw = sns.violinplot(data=df, x=\"gender\", y=\"math score\",palette = palette,ax=axes[0])\nw = sns.violinplot(data=df, x=\"gender\", y=\"reading score\",ax=axes[1]  , palette = palette )\nw = sns.violinplot(data=df, x=\"gender\", y=\"writing score\",ax=axes[2], palette = palette)","189ecfc5":"set(df[\"parental level of education\"])\n# these are the distinct values in the parental level of education column","cf23b28b":"plt.figure(figsize=(15,4))\nax = sns.countplot(x=df[\"parental level of education\"], palette=\"Set2\")\n#ax.set_xticklabels(ax.get_xticklabels(), fontsize=7)","45763c59":"len(df[df[\"parental level of education\"]==\"master's degree\"]) # students where both parents obtain a master's degree","a6fadd6d":"pl = df.groupby(\"parental level of education\")\nmeanscore_byEducation =pl.mean()\nmedianscore_byEducation=pl.median()","7b635773":"cm = sns.light_palette(\"green\", as_cmap=True)\ns= meanscore_byEducation.style.background_gradient(cmap=cm)\ns","8aab00aa":"meanscore_byEducation.plot.bar(y=\"total\",figsize=(15,5) )\nplt.title(\"Total Score Mean by parental level of Education\")\nplt.show()","4ad3d07d":"sns.set(style=\"whitegrid\")\nax = sns.boxplot(x=\"parental level of education\", y=\"total\", data=df, showfliers = False)\nplt.gcf().set_size_inches(11.7, 8.27)\nax = sns.swarmplot(x=\"parental level of education\", y=\"total\", data=df, color=\".25\")\nplt.ylim((50,300))\nplt.gcf().set_size_inches(11.7, 8.27)\nplt.show()","2145af71":"df[\"test preparation course\"].unique()","4b5ee56f":"ax = sns.countplot(x=df[\"test preparation course\"], palette=\"Set2\")","72f45062":"palette ={\"completed\":\"C5\",\"none\":\"C4\"}\nsns.pairplot(df.loc[:, df.columns != 'total'], hue=\"test preparation course\", diag_kind=\"kde\" , palette=palette,plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'},size=3)","afd6ea3c":"prep =df.groupby(\"test preparation course\")\nprepare_mean = prep.mean()\nprepare_mean","503e0c9d":"def failed(mathScore):\n    if(mathScore<50): \n        return 1 # failed\n    else: \n        return 0 # passed","4fec7b2a":"#copy data model\nprepared_Data = df\n#insert column (independent variable)\ny = list(map(failed, prepared_Data[\"math score\"]))\nprepared_Data[\"y\"] = prepared_Data[\"math score\"].apply(lambda x: failed(x) ) # variable to predict\n#delete math score from model as math failed is directly dependent on it\nprepared_Data = prepared_Data.drop(columns= [\"math score\",\"total\", \"lunch\"] )\nprepared_Data.head()","dea2c8b3":"total = len(prepared_Data)\nfailed = len(prepared_Data[prepared_Data[\"y\"] ==1 ])\npassed = len(prepared_Data[prepared_Data[\"y\"] ==0 ])\nprint(\"Percentage of Students that failed maths:\" + str(((failed\/total)*100)) +\"%\")\nprint(\"Percentage of Students that passed the maths exam:\" + str(((passed\/total)*100))  +\"%\")","0c77c14b":"ax = sns.countplot( x=np.asarray(y))\nrcParams['figure.figsize'] = 15, 5","6f371d2f":"grouped = prepared_Data.groupby(\"y\").mean()\ngrouped","f5ecc169":"#get categorical columns so we can generate dummy variables\ndef is_categorical(array_like):\n    return array_like.dtype.name == 'category'\n\ncatFilter = [  is_categorical(prepared_Data.iloc[:,i])  for i in range(0, len(prepared_Data.columns) )] \ncategoricalCols = prepared_Data.columns[catFilter].tolist()\nprint(categoricalCols)    ","0a919e26":"#Get dummy variables for al categorical columns\ncat_vars= categoricalCols\nfor var in cat_vars:\n    cat_list = \"var\"+\"_\" +var\n    cat_list = pd.get_dummies(prepared_Data[var],drop_first=True, prefix=var)\n    df1= prepared_Data.join(cat_list)\n    prepared_Data= df1\n    ","ba3f6c5f":"#Remove original categorical columns\ncat_vars= categoricalCols\ndata_vars=prepared_Data.columns.values.tolist()\nto_keep=[i for i in data_vars if i not in cat_vars]\nfinalDf = prepared_Data[to_keep]","27ad2405":"X = finalDf.loc[: , finalDf.columns != \"y\"]\ny = finalDf.loc[: , finalDf.columns == \"y\"]\nos = SMOTE(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\ncolumns = X_train.columns\nos_data_X,os_data_y=os.fit_sample(X_train, y_train) #create oversampling on traning data only\nos_data_X = pd.DataFrame(data=os_data_X,columns=columns )\nos_data_y= pd.DataFrame(data=os_data_y,columns=['y'])\n\n# we can Check the numbers of our data\nprint(\"length of oversampled data is \",len(os_data_X))\nprint(\"Number of passed\",len(os_data_y[os_data_y['y']==0]))\nprint(\"Number of failed\",len(os_data_y[os_data_y['y']==1]))\nprint(\"Proportion of passed data in oversampled data is \",len(os_data_y[os_data_y['y']==0])\/len(os_data_X))\nprint(\"Proportion of failed data in oversampled data is \",len(os_data_y[os_data_y['y']==1])\/len(os_data_X))","18ef4b4f":"print(y_train.shape)","3866cc19":"scaler = StandardScaler() #creates a scaler object\nscaler = scaler.fit(os_data_X) #fits the object to our training data set\nos_data_X = scaler.transform(os_data_X) #transforms the dataset into a scaled one\n# the scaler object is now used to transform the test set as well\nX_test = scaler.transform(X_test)","60769115":"# RFE: this algorithm is used to pick relevant features and leave out features with with low significance to improve model performance.\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfinalDf_vars=finalDf.columns.values.tolist()\ny=['y']\nX=[i for i in finalDf_vars if i not in y]\n\nlogreg = LogisticRegression()\nrfe = RFE(logreg, 20)\nrfe = rfe.fit(os_data_X, os_data_y.values.ravel())\nprint(rfe.support_)\nprint(rfe.ranking_)","ffefa46a":"X=os_data_X\ny=os_data_y['y']","336382ac":"## Implementing the Model\nimport statsmodels.api as sm\nlogit_model=sm.Logit(y,X)\nresult=logit_model.fit()\nprint(result.summary2())","32f15448":"from sklearn import metrics\n\nlogreg = LogisticRegression()\n#logreg.fit(X_train, y_train)\nlogreg.fit(os_data_X, os_data_y)","13bee6d7":"y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","16790f8d":"# Creating the Confusion Matrix to visualize the model performance\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","54138288":"class_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","d3904a25":"# Applying k-Fold Cross Validation to test model performance accross different validation subsets\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = logreg, X = os_data_X, y = os_data_y, cv = 10, scoring=\"accuracy\")\nprint(accuracies.mean() )\nprint(accuracies.std() )\n","b88f76ad":"#GRID SEARCH to find better hyperparameters\nfrom sklearn.model_selection import GridSearchCV\nparametersLog = [{'C':[0.01, 0.1,1,10,100,1000], 'penalty':[\"l1\",\"l2\"], 'fit_intercept':[True, False]  }]\ngrid_searchLog =GridSearchCV(estimator=logreg, \n                          param_grid = parametersLog,\n                          scoring = \"accuracy\", cv=10, n_jobs=-1)\ngrid_searchLog = grid_searchLog.fit(os_data_X,os_data_y)\nbest_accuracyLog = grid_searchLog.best_score_\nbest_parametersLog = grid_searchLog.best_params_","a5e971a1":"print(best_accuracyLog)\nprint(best_parametersLog)","5ae86e57":"# get precision and recall values to validate model performance\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","cf488e8b":"# display ROC Curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","4fcd4fc0":"os_data_X = pd.DataFrame(data=os_data_X,columns=columns )\nos_data_y= pd.DataFrame(data=os_data_y,columns=['y']) # convert both sets into dataframes","11e868c6":"#Analyse feature importance\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\n#X, y = X_train, y_train\nX, y = os_data_X, os_data_y\nprint(X.shape)\n\nclf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(X, y)\nprint(clf.feature_importances_ )\n\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)","23a629d2":"names = list(X.columns)\nimportance = clf.feature_importances_ ","5ddebdf3":"rcParams['figure.figsize'] = 15, 5\nplt.bar(np.arange(len(names)), importance   )\nplt.xticks(np.arange(len(names))  , names, rotation='vertical')\nplt.tick_params(axis='both', which='major', labelsize=15)\nplt.show()","9cefc0dc":"#Build a neural network\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nX_train = os_data_X\ny_train = os_data_y\n\n#Initialising ANN\nclassifier = Sequential()\n#Adding input Layer and first hidden layer\nclassifier.add(Dense(input_dim=13, output_dim = 6, init= 'uniform', activation='relu'))#uniform to init weights\n#add second hidden layer\nclassifier.add(Dense( output_dim = 6, init= 'uniform', activation='relu'))\n#add  output layer\nclassifier.add(Dense( output_dim = 1, init= 'uniform', activation='sigmoid'))\n#compile ANN\nclassifier.compile(optimizer = 'adam', loss= 'binary_crossentropy', metrics = ['accuracy'])\n#fitting ANN to trainning set\nclassifier.fit(X_train, y_train, batch_size=10, nb_epoch=100)\n\n\n\n","61bcd9d5":"#predict test set results\ny_pred = classifier.predict(X_test)\ny_pred= [1 if i>0.5 else 0  for i in y_pred]\n\n#\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","661da172":"accuracy_score = classifier.evaluate(X_test, y_test, verbose=0)[1]\naccuracy_score","fe1b06e8":"# Fitting SVM to the Training set\nfrom sklearn.svm import SVC\nclassifierSVM = SVC(kernel = 'linear' , C=10)  \nclassifierSVM.fit(X_train,y_train)","8bc503b9":"y_pred2 = classifierSVM.predict(X_test)\n\ncm2 = confusion_matrix(y_test, y_pred2)\nprint(cm2)","d7f1fb2b":"accuraciesSVM = cross_val_score(estimator = classifierSVM, X = X_train, y = y_train, cv = 10)\nprint(accuraciesSVM.mean() )\nprint(accuraciesSVM.std() )","2a453b0c":"#GRID SEARCH to find best model and hyperparameters\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'C':[0.1,1,10,100,1000], 'kernel':['linear']  },\n               {'C':[0.1,10,100,1000], 'kernel':['rbf'], 'gamma':[0.5,0.1,0.01,0.001,0.0001]}]\ngrid_search =GridSearchCV(estimator=classifierSVM, \n                          param_grid = parameters,\n                          scoring = \"accuracy\", cv=10, n_jobs=-1)\ngrid_search = grid_search.fit(X_train,y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_","3948e636":"print(best_accuracy)\nprint(best_parameters)","c1b5282a":"# SVM Model","b02f5d58":"Setting the penalty prameter to \"l1\" could improve the model performance currently being set to l2.","315dd897":"The above boxplot visualisez the significant better performance of students whose parent have a master's degree in comparison with students whose parent went to High School only.","d0d4f53a":"We showed that we can treat both variables Math Score and Reading Score as normally distributed so we can investigate the correlation between the two variables using a parametric test such as the Pearson test.","4e7b638f":"# Math Score Analysis","caac0d7b":"# General Score Analysis","ab4a6f8a":"Completing the preparation course seems to have a positive effect on the score for all three exams.","05af480c":"We can already see that students who failed the math exam perform considerably worse in other exams as well.","21519181":"The Eta-Squared statistic indicated a small - moderate effect size (0.02)","9fe1dd5a":"# Neural network","5eacf659":"To get insights about students passing the math exam and to predict if students will fail the math exam we add an additional column. The grades are split as follows:\nabove 80 = A Grade <br>\n70 to 80 = B Grade<br>\n60 to 70 = C Grade<br>\n50 to 60 = D Grade<br>\nbelow 50 = Fail<br>\n\nNote: These classificications are not based on real scenarious and are just defined for test purposes.","3bae94fd":"Histogram and QQ-Plot both indicate a normal distribution. <br>\nWe can now investigate the impact of outliers. If at 0.05 level 95% of our standardized data is within the limits of +-1.96 we can treat it as normal. If sample size < 80 we can use +-2.58 as a boundary if it is larger we can use +-3.29 as a boundary.","26f2cf98":"Reading and writing score are the best indicators to predict a students chance of failing the math exam.","cfadc768":"At first we want to conduct a univariate analysis of the the Math Score variable. This includes central tendency and normality checks. It is essential to know if a variable is normally distributed to see if parametric or non-parametric statistical tests need to be applied. <br>\nAs seen above Median and Mean of Math score are similar which would indicate a normally distributed dataset. However we want to further investigate if this is the case.","d7f9eb18":"The above scatter plot visualizes the high correlation between math and reading score. It can also be seen that the male data points tend to be located below the dashed line while females data point are rather located above.\nIn the example of a single person a point above the dashed line means that their reading score is higher than their math score.","1b05a442":"# Oversampling\nBecause the data set is imbalanced we apply one of the multiple options we have in order to deal with such imbalanced datasets - oversampling. It is important that this oversampling is only done on training data.","a422effd":"### Eta Squared Score \nTo check our results we also calculate the effect using the eta_squared statistic. <br>\n$\\eta^{2} = \\frac{t^2}{t^2 + (N1 + N2 + -2)}$ <br>\n\n- 0.01 = small effect\n- 0.06 = moderate effect\n- 0.14 = large effect","8e46c728":"The following used Machine Learning Algorithms Logistic Regression, SVM and Neural Networks perform usually better when features are on a similar scale and close to normal distribution.\nAs out dataset is normally distributed we will be using sklearn's standard scaler library.\n","ce28a4f5":"We are confident that more than 95% of our data falls withing the boundary of 1.96 and can therefore assume normality.","79700f13":"Standardized scores for skewness and kurtosis between -2 and +2 prove normal univariate distribution. <br> \nIn this case the standardized skewness score exceeds these limits so we need to further investigate to see if we can apply parametric tests. We will do this after doing the above test for kurtosis.","621c8255":"# Test Preparation Course Analysis","598bc90b":"H0 Hypothesis in the Levene's test states that all variances are equal. In the above test we get a p-value >0.05 meaning an insignificant result. We can therefore accept H0 stating that Homogeneity of Variances is given.","a843006e":"## Math Score Difference\nIn the next section we want to analyse if there is a statistically significant performance difference in math exams between male and female students. We will use an Independent T-Test as two unrelated groups (males\/females) are compared. In order to conduct a t-test we first have to analyze Homogeneity of Variances using a Levene test.. We already proved normality of the data above.","2abe3b7d":"As 0.2% < 5% we can treat our data as normal as we are confident that more than 95% of our standardized data falls within our ranges. <br>\n","a1c26b4e":"The standardized Kurtosis score for the variable Math score is within the boundaries of -2 and 2. However, we still need to investigate further as the standardized skew fell outside the -2 2 boundaries. At first we create a histogram to see how much Skew there is.","2b894e92":"# Feature Scaling","e0b9427e":"We can see that the dataset is imbalanced which has to be considered when preparing the dataset and training the models.\nThere are a couple of methods to deal with imbalanced datasets such as oversampling the minority class, undersampling the majority class or changing the Machine Learning Algorithm to a model that performs well to imbalanced datasets such as Decision Trees.","2baadbb3":"A significant smaller part (59 students) of students has parents where both mother and father graduated with a master degree. This has to be considered at a later stage when averages across all groups are calculated.","d1f2d70a":"The above Heatmap shows that there is a high correlation between writing and reading score. the correlation coefficient is smaller for math score and writing score but it is still considered a very strong relationship..","05d4dcfa":"# Gender Analysis","530441c5":"As the P-Value for x1  is above 0.05 we could remove that features from our model.","139d65bc":"# Train Model","1c1f85d3":"## Comparison of exam results by gender","6e156788":"The t-test result is statistically significant as the p-value < 0.05. This means that there is a statistically significant difference in math exam performance between genders.","30e39a7f":"Standardized score for skewness falls outside the boundaries of -2 and 2. therefore we need to investigate further to see if data is distributed normally.","b060c3a0":"# Parental Level of Education Analysis","a3e28bac":"To analyze other possible correlations without further drilling into univariate analysis of the writing score variable we can generate the below heatmap from the seaborn library for a more visual approach.","4173b416":"A correlation coefficient of 0.81 and a p-value < 0.01 indicates a very strong positive relationship.","abe4ba3d":"The above table shows that there is a significant difference of student performance in exams for different levels of parent education. Students whose parents graduated with a Master's Degree perform significantly better that students whose parents went to High School only.","45e0ebc3":"# Reading Score analysis","cac742b6":"#  Logistic Regression","901f92f4":"The above Heatmap shows that our model predicted 242 + 29 cases correctly while 8 + 21 students were incorrectly classified.","bfa769d3":"As we can see above the SVM could be slightly improved by adjusting the penalty parameter C to 100. "}}