{"cell_type":{"a74ac878":"code","abd0714c":"code","621a5fc3":"code","a169ba7a":"code","f81660bd":"code","9e0fec78":"code","98336ee6":"code","b2cd86c4":"code","e5374d0b":"code","3a1dcd81":"code","2cc45780":"code","990036df":"code","74beeda3":"code","0a30c38c":"code","c787886d":"code","5fe76240":"code","c9acfa39":"code","a1d58113":"code","2767a287":"code","ae173074":"code","6fe3a509":"code","6072d920":"code","c0d86d47":"code","744d0901":"code","ef78f82c":"code","2e06654f":"code","1087daa0":"code","176d1f44":"code","8dcedeec":"code","09ec4c3e":"code","752ad14f":"code","5f994756":"code","d609b258":"code","fcec1067":"code","917704cb":"code","18b2796d":"code","943b14b9":"code","6b9c1466":"markdown","37c9a13c":"markdown","0e7c60be":"markdown","3b0d3a2a":"markdown","ac37f7d3":"markdown","2af3b436":"markdown","eb8edc3d":"markdown","e198ce8f":"markdown","7ae14ceb":"markdown","94c95225":"markdown","2c620bc3":"markdown","4c2f9c84":"markdown","d78c4d0d":"markdown","5b42c8ee":"markdown","063f0cbb":"markdown","e7cc74ed":"markdown","587bbd7c":"markdown","60983c4f":"markdown","ab4486d7":"markdown"},"source":{"a74ac878":"#REQUIRED LIBRARIES\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\n# from sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.cluster import KMeans\n# import pickle\n# from sklearn.externals import joblib\n\n# import pandas_profiling as pp\n\nwarnings.filterwarnings('ignore')\ngc.enable()\n%matplotlib inline","abd0714c":"#CHECKING ALL AVAILABLE FILES\npath='\/kaggle\/input\/tabular-playground-series-sep-2021\/'\ndata_files=list(os.listdir(path))\ndf_files=pd.DataFrame(data_files,columns=['file_name'])\ndf_files['size_in_mb']=df_files.file_name.apply(lambda x: round(os.path.getsize(path+x)\/(1024*1024),2))\ndf_files['type']=df_files.file_name.apply(lambda x:'file' if os.path.isfile(path+x) else 'directory')\ndf_files['file_count']=df_files[['file_name','type']].apply(lambda x: 0 if x['type']=='file' else len(os.listdir(path+x['file_name'])),axis=1)\n\nprint('Following files are available under path:',path)\ndisplay(df_files)","621a5fc3":"#ALL CUSTOM FUNCTIONS\n\n#FUNCTION FOR PROVIDING FEATURE SUMMARY\ndef feature_summary(df_fa):\n    print('DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    col_list=['null','unique_count','data_type','max\/min','mean','median','mode','std','skewness','sample_values']\n    df=pd.DataFrame(index=df_fa.columns,columns=col_list)\n    df['null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])\/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df['unique_count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df['data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.at[col,'max\/min']=str(round(df_fa[col].max(),2))+'\/'+str(round(df_fa[col].min(),2))\n            df.at[col,'mean']=round(df_fa[col].mean(),4)\n            df.at[col,'median']=round(df_fa[col].median(),4)\n            df.at[col,'mode']=round(df_fa[col].mode()[0],4)\n            df.at[col,'std']=round(df_fa[col].std(),4)\n            df.at[col,'skewness']=round(df_fa[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.at[col,'max\/min']=str(df_fa[col].max())+'\/'+str(df_fa[col].min())\n        df.at[col,'sample_values']=list(df_fa[col].unique())\n    display(df_fa.head())      \n    return(df.fillna('-'))\n\n#PREDICTION FUNCTIONS\n\ndef claim_predictor(X,y,test,model,model_name):  \n\n    df_preds=pd.DataFrame()\n    df_preds_x=pd.DataFrame()\n    k=1\n    splits=5\n    avg_score=0\n\n    #CREATING STRATIFIED FOLDS\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=200)\n    print('\\nStarting KFold iterations...')\n    for train_index,test_index in skf.split(X,y):\n        df_X=X[train_index,:]\n        df_y=y[train_index]\n        val_X=X[test_index,:]\n        val_y=y[test_index]\n       \n\n    #FITTING MODEL\n        model.fit(df_X,df_y)\n\n    #PREDICTING ON VALIDATION DATA\n        col_name=model_name+'xpreds_'+str(k)\n        preds_x=pd.Series(model.predict_proba(val_X)[:,1])\n        df_preds_x[col_name]=pd.Series(model.predict_proba(X)[:,1])\n\n    #CALCULATING ACCURACY\n        acc=roc_auc_score(val_y,preds_x)\n        print('Iteration:',k,'  roc_auc_score:',acc)\n        if k==1:\n            score=acc\n            best_model=model\n            preds=pd.Series(model.predict_proba(test)[:,1])\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=preds\n        else:\n            preds1=pd.Series(model.predict_proba(test)[:,1])\n            preds=preds+preds1\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=preds1\n            if score<acc:\n                score=acc\n                best_model=model\n        avg_score=avg_score+acc        \n        k=k+1\n    print('\\n Best score:',score,' Avg Score:',avg_score\/splits)\n    #TAKING AVERAGE OF PREDICTIONS\n    preds=preds\/splits\n    \n    print('Saving test and train predictions per iteration...')\n    df_preds.to_csv(model_name+'.csv',index=False)\n    df_preds_x.to_csv(model_name+'_.csv',index=False)\n    x_preds=df_preds_x.mean(axis=1)\n    return preds,best_model,x_preds \n","a169ba7a":"%%time\n#READING DATASET\n\ndf_train=pd.read_csv(path+'train.csv')\ndf_test=pd.read_csv(path+'test.csv')\ndf_submission=pd.read_csv(path+'sample_solution.csv')","f81660bd":"#Understanding Target (claim) feature distribution\npie_labels=['Claim-'+str(df_train['claim'][df_train.claim==1].count()),'No Claim-'+\n            str(df_train['claim'][df_train.claim==0].count())]\npie_share=[df_train['claim'][df_train.claim==1].count()\/df_train['claim'].count(),\n           df_train['claim'][df_train.claim==0].count()\/df_train['claim'].count()]\nfigureObject, axesObject = plt.subplots(figsize=(6,6))\npie_colors=('orange','grey')\npie_explode=(.01,.01)\naxesObject.pie(pie_share,labels=pie_labels,explode=pie_explode,autopct='%.2f%%',colors=pie_colors,startangle=30,shadow=True)\naxesObject.axis('equal')\nplt.title('Percentage of Claim - No Claim Observations',color='blue',fontsize=12)\nplt.show()","9e0fec78":"#Correlation check\ncorr = df_train.iloc[:,1:].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Plotting correlation heatmap\nplt.subplots(figsize=(22,20))\nsns.heatmap(corr,mask=mask,xticklabels=corr.columns,yticklabels=corr.columns)\nplt.show()","98336ee6":"%%time\nX=df_train.drop(['id','claim'],axis=1)\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nX_= imputer.fit_transform(X)\npca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(X_)\nprincipalDf = pd.DataFrame(data = principalComponents,columns = ['principal_component_1','principal_component_2','principal_component_3'])\nprincipalDf['claim']=df_train['claim']\n\nfig = plt.figure(figsize=(15,15))\nax = fig.add_subplot(111, projection = '3d')\n\nax.set_xlabel(\"principal_component_1\")\nax.set_ylabel(\"principal_component_2\")\nax.set_zlabel(\"principal_component_3\")\n\nsc=ax.scatter(xs=principalDf['principal_component_1'], ys=principalDf['principal_component_2'],\n              zs=principalDf['principal_component_3'],c=principalDf['claim'],cmap='OrRd')\nplt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\nplt.show()","b2cd86c4":"gc.collect()","e5374d0b":"del X,X_\ngc.collect()","3a1dcd81":"pd.set_option('display.max_rows', len(df_train.columns))\nfeature_summary(df_train)","2cc45780":"feature_summary(df_submission)","990036df":"gc.collect()","74beeda3":"%%time\nfeatures=list(df_test.columns[1:])\n\ndf_train['n_missing'] = df_train[features].isna().sum(axis=1)\ndf_test['n_missing'] = df_test[features].isna().sum(axis=1)\n\ndf_train['std'] = df_train[features].std(axis=1)\ndf_test['std'] = df_test[features].std(axis=1)\n\ndf_train['mean'] = df_train[features].mean(axis=1)\ndf_test['mean'] = df_test[features].mean(axis=1)\n\ndf_train['max'] = df_train[features].max(axis=1)\ndf_test['max'] = df_test[features].max(axis=1)\n\ndf_train['min'] = df_train[features].min(axis=1)\ndf_test['min'] = df_test[features].min(axis=1)\n\ndf_train['kurt'] = df_train[features].kurtosis(axis=1)\ndf_test['kurt'] = df_test[features].kurtosis(axis=1)\n\nfeatures += ['n_missing', 'std','mean','max','min','kurt']","0a30c38c":"%%time\ndf_train[features] = df_train[features].fillna(df_train[features].mean())\ndf_test[features] = df_test[features].fillna(df_test[features].mean())","c787886d":"%%time\nscaler = StandardScaler()\ndf_train[features] = scaler.fit_transform(df_train[features])\ndf_test[features] = scaler.transform(df_test[features])","5fe76240":"X=df_train.drop(['id','claim'],axis=1).to_numpy()\ny=df_train['claim'].values\ntest=df_test.drop(['id'],axis=1).to_numpy()","c9acfa39":"del df_train,df_test,scaler\ngc.collect()","a1d58113":"X.shape,y.shape,test.shape","2767a287":"%%time\nmodel=LogisticRegression()\nprint('Logistic Regression parameters:\\n',model.get_params())\n\nlogistic_predictions,best_logistic_model,LRpreds=claim_predictor(X,y,test,model,'LR')","ae173074":"logistic_predictions","6fe3a509":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_logistic_model.coef_[0]\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (20,25))\nsns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt);","6072d920":"gc.collect()","c0d86d47":"%%time\ncatb_params = {\n    'eval_metric' : 'AUC',\n    'iterations': 15585, \n    'objective': 'CrossEntropy',\n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n   'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\nmodel=CatBoostClassifier(**catb_params)\nprint('CatBoost paramters:\\n',model.get_params())\n\ncatb_predictions,best_catb_model,CBpreds=claim_predictor(X,y,test,model,'CB')","744d0901":"catb_predictions","ef78f82c":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_catb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (10,25))\nsns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt);","2e06654f":"gc.collect()","1087daa0":"%%time\nlgbm_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n   'device_type': 'gpu', \n    'n_estimators': 10000, \n    'learning_rate': 0.12230165751633416, \n    'num_leaves': 1400, \n    'max_depth': 8, \n    'min_child_samples': 3100, \n    'reg_alpha': 10, \n    'reg_lambda': 65, \n    'min_split_gain': 5.157818977461183, \n    'subsample': 0.5, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.2\n}\n\nmodel=lgb.LGBMClassifier(**lgbm_params)\nprint('LGBM parameters:\\n',model.get_params())\n\nlgb_predictions,best_lgb_model,LGBpreds=claim_predictor(X,y,test,model,'LGB')","176d1f44":"lgb_predictions","8dcedeec":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_lgb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (10,25))\nsns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt);","09ec4c3e":"gc.collect()","752ad14f":"%%time\nxgb_params = {\n    'eval_metric': 'auc', \n    'objective': 'binary:logistic', \n   'tree_method': 'gpu_hist', \n   'gpu_id': 0, \n   'predictor': 'gpu_predictor', \n    'n_estimators': 10000, \n    'learning_rate': 0.01063045229441343, \n    'gamma': 0.24652519525750877, \n    'max_depth': 4, \n    'min_child_weight': 366, \n    'subsample': 0.6423040816299684, \n    'colsample_bytree': 0.7751264493218339, \n    'colsample_bylevel': 0.8675692743597421, \n    'lambda': 0, \n    'alpha': 10\n}\nmodel=xgb.XGBClassifier(**xgb_params)\nprint('XGB parameters:\\n',model.get_params())\n\nxgb_predictions,best_xgb_model,XGBpreds=claim_predictor(X,y,test,model,'XGB')","5f994756":"xgb_predictions","d609b258":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_xgb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (10,25))\nsns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt);","fcec1067":"gc.collect()","917704cb":"%%time\n# blending_ratios=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nblending_ratios=[0.0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\nroc_final=0\nLR_ratio=0\nLGB_ratio=0\nCB_ratio=0\nXGG_ratio=0\nfor i in blending_ratios:\n    for j in blending_ratios:\n        for k in blending_ratios:\n            for l in blending_ratios:\n                if((i+j+k+l==1) and (i>0 and j>0 and k>0 and l>0)):\n                    roc_new=roc_auc_score(y,(LRpreds*i+LGBpreds*j+CBpreds*k+XGBpreds*l))\n                    print(\"LRratio: \",i,\" LGBratio: \",j,\" CBratio:\",k,\" XGBratio: \",l,\" ROCscore: \",roc_new)\n                    if roc_new>roc_final:\n                        roc_final=roc_new\n                        LR_ratio=i\n                        LGB_ratio=j\n                        CB_ratio=k\n                        XGB_ratio=l\nprint(\"Final Ratios, LR ratio: \",LR_ratio,\" LGB ratio: \",LGB_ratio,\" CB ratio:\",CB_ratio,\" XGB ratio: \",XGB_ratio,\" ROC score: \",roc_final)","18b2796d":"df_submission['claim']=lgb_predictions*LGB_ratio+catb_predictions*CB_ratio+logistic_predictions*LR_ratio+xgb_predictions*XGB_ratio\n#CREATING SUMBISSION FILE\ndf_submission.to_csv('submission.csv',index=False)","943b14b9":"df_submission","6b9c1466":"<h2>Exploratory Data Analysis<\/h2>","37c9a13c":"<h2>Available Files<\/h2>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","0e7c60be":"<h2 id=\"FeatureEng\">Feature Engineering<\/h2>\nCreating features using given features.\n\n<h4>Observations<\/h4>\nMissing value count per observation proved to be a very strong engineered feature for both linear and gradiant boost models\n\n<br><a href=\"#Approach\">back to main menu<\/a>","3b0d3a2a":"<h2 id=\"LGBM\">LGBMClassifier<\/h2>\n<ul>\n    <li>We are using GPU for training this model<\/li>\n<\/ul>\n    \n\n<h4>Hyperparameters picked up from below very informative notebook<\/h4>\nhttps:\/\/www.kaggle.com\/mlanhenke\/tps-09-simple-basic-stacking-lgbm-catb-xgb\n\n<br><a href=\"#Approach\">back to main menu<\/a>","ac37f7d3":"<h2 id=\"CatBoost\">CatBoostClassifier<\/h2>\n\n<ul>\n    <li>We are using GPU for training this model<\/li>\n<\/ul>\n\n<h4>Hyperparameters picked up from below very informative notebook<\/h4>\nhttps:\/\/www.kaggle.com\/mlanhenke\/tps-09-simple-basic-stacking-lgbm-catb-xgb\n\n<br><a href=\"#Approach\">back to main menu<\/a>","2af3b436":"<h2 id=\"TrainVisual\">Visualizating Training dataset<\/h2>\nWe are making use of PCA, dimentionality reduction technique to Visualize Training dataset.<br>\nVisualization is also helpful in understanding any grouping or patterns within dataset.\n<h4>Observation<\/h4>\nNo pattern or grouping observed in training dataset\n\n<br><a href=\"#Approach\">back to main menu<\/a>","eb8edc3d":"<h2>Required Libraries<\/h2>","e198ce8f":"<h2 id=\"FeatureSummary\">Understanding Training dataset features<\/h2>\nUserstanding Training dataset features using basic statistical measures\n\n<h4>Observations<\/h4>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","7ae14ceb":"<h2 id=\"Target\">Understanding Target feature distribution<\/h2>\nLets visualize Target feature.\n\n<h4>Observation<\/h4>\nAs observations have almost equal count of claim and no claim observations, this is a Balanced dataset. \n\n<br><a href=\"#Approach\">back to main menu<\/a>","94c95225":"<h2 id=\"LogisticRegression\">LogisticRegression<\/h2>\nStarting with Basic Linear Model\n\n<h4>Observations<\/h4>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","2c620bc3":"<h2 id=\"Corr\">Correlation Check<\/h2>\nLets check if there are any correlated features. If two features are highly correlated we can remove one of the feature.\nThis will help in dimentionality reduction.\n\n<h4>Observation<\/h4>\nNo correlation is observed among Training dataset features.\n\n<br><a href=\"#Approach\">back to main menu<\/a>","4c2f9c84":"<h2 id=\"Approach\">Approach to the problem<\/h2>\nIdea is to develop a generalized approach for solving any binary classification problem\n<ol>\n    <li>Performing exploratory data analysis.<\/li>\n    <ol>\n        <li><a href=\"#Target\">Understanding Target feature distribution<\/a><\/li>\n        <li><a href=\"#Corr\">Correlation check<\/a><\/li>\n        <li><a href=\"#TrainVisual\">Visualizing Training dataset<\/a><\/li>\n        <li><a href=\"#FeatureSummary\">Understanding Training dataset features<\/a><\/li>\n    <\/ol>\n    <li><a href=\"#FeatureEng\">Feature Engineering.<\/a><\/li> \n     <li>Data Preparation.<\/li>\n    <ol>\n        <li><a href=\"#MissingValues\">Handling missing values<\/a><\/li>\n    <\/ol>\n    <li>Training Linear and Gradient Boost Base models.<\/li>\n    <ol>\n        <li><a href=\"#LogisticRegression\">Logistic Regression<\/a><\/li>\n        <li><a href=\"#CatBoost\">CatBoost Classification<\/a><\/li>\n        <li><a href=\"#LGBM\">LGBM Classification<\/a><\/li>\n        <li><a href=\"#XGB\">XGBoost Classification<\/a><\/li>\n    <\/ol>\n    <li>Basic Blending.<\/li>\n    <ol>\n        <li><a href=\"#Ratios\">Calculating best blending Ratios (using training preditions to calculate blending ratios)<\/a><\/li>\n        <li><a href=\"#FinalPred\">Calculating blended prediction<\/a><\/li>\n    <\/ol>\n<\/ol>","d78c4d0d":"<h2>All Custom Functions for this Notebook<\/h2>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","5b42c8ee":"<h2 id=\"XGB\">XGBClassifier<\/h2>\n\n<ul>\n    <li>We are using GPU for training this model<\/li>\n<\/ul>\n\n<h4>Hyperparameters picked up from below very informative notebook<\/h4>\nhttps:\/\/www.kaggle.com\/mlanhenke\/tps-09-simple-basic-stacking-lgbm-catb-xgb\n\n<br><a href=\"#Approach\">back to main menu<\/a>","063f0cbb":"<h2 id=\"Ratios\">Calculating best blending Ratios (using training preditions to calculate blending ratios)<\/h2>\nWe are trying to calculate best blending ratio on trained dataset and then applying the same to predicted test values\n\n<h4>Observation<\/h4>\n<ul>\n    <li>This approach lead to selecting ratio 1.0 for best model and 0.0 for others.<\/li>\n    <li>If we observe results by above three models, we can conclude different models are working better than other on different part of dataset.Now how can we bring this to our blending strategy?<\/li>\n    <li>As changed blending strategy, we will try to find best blending ratio with each ratio greater than zero<\/li>\n<\/ul>\n\n\n<br><a href=\"#Approach\">back to main menu<\/a>","e7cc74ed":"<b>Problem Statement:<\/b> Calculating claim probability on an insurance policy.\n\n<b>Problem type:<\/b> A binary classification problem\n\n<b>Evaluation matrix:<\/b> Submissions are evaluated on area under the <b>ROC(receiver operating characteristic)<\/b> curve between the predicted probability and the observed target.","587bbd7c":"<h2>Blending<\/h2>","60983c4f":"<h2 id=\"BlendPred\">Calculating blended prediction<\/h2>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","ab4486d7":"<h2 id=\"MissingValues\">Handling missing values<\/h2>\nThere are number of missing values in this dataset. We are replacing missing values for a feature with its mean.\n\n<h4>Observations<\/h4>\n\n\n<br><a href=\"#Approach\">back to main menu<\/a>"}}