{"cell_type":{"053da5f9":"code","8332328e":"code","e1870b5b":"code","194bd6b9":"code","ab9c5e3f":"code","ad812d93":"code","2c3a13af":"code","c54ed868":"code","93fd7b40":"code","8c993563":"code","da411ff2":"code","0fcfcb8e":"code","23cd3b48":"code","88feef94":"code","17f60245":"code","d7690cb4":"code","80c7db11":"code","77ad6494":"code","0af534f5":"code","a63483ed":"code","2faf9fd3":"code","6a3761f9":"code","6b4c6e16":"code","c56870bc":"code","bb17e548":"code","f71fcb62":"code","b5c85bd6":"code","838559d9":"code","7e8a805f":"code","f19af445":"code","5391b28c":"code","3b1bf7b2":"code","8575078d":"code","99556f7f":"code","18cc2780":"code","2db7b830":"code","e5a27b01":"code","7740eb44":"code","941256a0":"code","5fb4a025":"code","5e0cd75a":"code","37c05209":"code","d0e1a591":"code","f9eb573f":"code","035148cc":"code","988a8b99":"code","39ea0adc":"code","ac7f4c70":"markdown","665d2a1c":"markdown","31fa5324":"markdown","5a7cedac":"markdown","941dd8cb":"markdown","feab9ac6":"markdown","9a8fa7a4":"markdown","e7d0d80f":"markdown","9c00bf30":"markdown","0db9e1aa":"markdown","267cb33b":"markdown","b5a00bd6":"markdown","b344d685":"markdown","c273d3d1":"markdown","2c7a5760":"markdown","dfa1577d":"markdown","d18fe1c3":"markdown","b3925f0a":"markdown","4a088d6a":"markdown","156481b2":"markdown","d0565516":"markdown","fbe3c8fa":"markdown","157d3a7d":"markdown","b1459996":"markdown","030ea89d":"markdown","9587d505":"markdown","168e7adb":"markdown","73d21393":"markdown","f4dbfde3":"markdown"},"source":{"053da5f9":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport re\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans","8332328e":"def load_from_csv(wkgdir):\n    \n    bio = pd.read_csv(wkgdir + '.\/biorxiv_clean.csv')\n    comm = pd.read_csv(wkgdir + '.\/clean_comm_use.csv')\n    noncomm = pd.read_csv(wkgdir + '.\/clean_noncomm_use.csv')\n    custom = pd.read_csv(wkgdir + '.\/clean_pmc.csv')\n    \n    corpus = pd.concat([bio,comm,noncomm,custom], ignore_index=True)\n    \n    return corpus","e1870b5b":"corpus = load_from_csv('..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/')","194bd6b9":"corpus.info()","ab9c5e3f":"# Replace missing data with NA\ncorpus = corpus.fillna('not available')","ad812d93":"def clean(col):\n    col = col.replace('\\n', '')\n    col = col.replace('\\r', '')    \n    col = col.replace('\\t', '')\n    col = re.sub(\"\\[[0-9]+(,[0-9]+)*\\]\", \"\", col)\n    col = re.sub(\"\\([0-9]+(,[0-9]+)*\\)\", \"\", col)\n    col = re.sub(\"\\{[0-9]+(,[0-9]+)*\\}\", \"\", col)\n\n    return col\n    \ncorpus['abstract'] = corpus['abstract'].apply(clean)\ncorpus['text'] = corpus['text'].apply(clean)","2c3a13af":"corpus.rename(columns={'text':'body_text'}, inplace=True)\ncorpus['text'] = corpus.title + ' ' + corpus.abstract + ' ' + corpus.body_text ","c54ed868":"# Drop astract and body text columns\ncorpus = corpus.drop(['body_text', 'abstract'], axis=1)","93fd7b40":"# Target term lists\ns1 = ['risk', 'risks', 'risk factor', 'risk factors', 'determinant', 'determinants','susceptibility']\ns2 = ['smoking', 'smoker', 'smokers', 'smoke', 'smokes', 'cigarette', 'cigarettes', 'tobacco', 'pre-existing respiratory', 'pre-existing pulmonary', 'asthma', 'copd']\ns3 = ['coinfection', 'coinfections', 'coexisting respiratory', 'coexisting infection', 'coexisting infections', 'coexisting viral', 'comorbidity', 'comorbidities']\ns4 = ['transmissible', 'transmission', 'transmissibility', 'communicable', 'communicability', 'infectivity', 'virulent', 'virulence', 'pathogenicity','contagious','contagion','contagiosity']\ns5 = ['neonates', 'neonate', 'newborn', 'newborns', 'pregnant', 'pregnancy', 'pregnancies', 'gestation']\ns6 = ['socio-economic', 'socioeconomic', 'economic', 'social behavior', 'behavioral', 'social factors', 'poverty','homelessness','food insecurity','disparity','disparities']\ns7 = ['reproductive number', 'incubation period', 'serial interval', 'environmental factors']\ns8 = ['severity', 'severe', 'fatal', 'fatality', 'morbidity', 'mortality']\ns9 = ['high-risk patients', 'hospitalized patient', 'high-risk patient', 'symptomatic patients', 'symptomatic hospitalized']\ns10 = ['population', 'populations', 'group', 'groups', 'demographics', 'demography', 'demographic', 'ethnic', 'ethnicities', 'ethnicity', 'racial', 'race', 'cultural', 'culture', 'gender']\ns11 = ['public', 'health', 'public health']\ns12 = ['measures', 'policy', 'proactive', 'assessment', 'partnership', 'priorities', 'practices', 'awareness']\ns13 = ['control', 'management', 'mitigation']","8c993563":"# CountVectorizer with select vocabulary\n\ndef vocab_vectorizer(feature, vocab):\n\n    '''\n    Generates: document term matrix using a select vocabulary\n    Args: \n        feature - dataframe column containing text to vectorize\n        vocab - the specific list of terms from which to generate the matrix\n    Returns: document term matrix as a dataframe \n    '''\n\n    # generate a document term matrix\n    CV1 = CountVectorizer(input=\"content\", ngram_range=(1,2), vocabulary = vocab)\n    DTM1 = CV1.fit_transform(corpus[feature])\n\n    # add col names\n    ColNames=CV1.get_feature_names()\n\n    DF1 = pd.DataFrame(DTM1.toarray(), columns=ColNames)\n\n    # add row names\n    Dict1 = {}\n    for i in range(0, len(corpus.paper_id)):\n        Dict1[i] = corpus.paper_id[i]\n    DF1 = DF1.rename(Dict1, axis='index')\n\n    return DF1","da411ff2":"term_sets = [s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12,s13] \ndtm_sets = []\n\nfor s in tqdm(term_sets):\n    dtm_sets.append(vocab_vectorizer(feature='text', vocab=s))","0fcfcb8e":"def word_count (field):\n    \n    tokens = field.split()\n    field_length = len(tokens)\n    \n    return field_length","23cd3b48":"doc_lengths = pd.DataFrame( {'word_count' : corpus['text'].apply(word_count)} )","88feef94":"def length_normalize(dtm, doc_lengths):\n    \n    # Replace missing data with NA\n    #dtm = dtm.fillna(0)\n    \n    for i in range(dtm.shape[1]):\n    # Select column by index position using iloc[]\n        columnSeriesObj = dtm.iloc[: , i]\n        columnSeriesObj2 = columnSeriesObj \/ doc_lengths.word_count.values\n        dtm.iloc[: , i] = columnSeriesObj2\n\n    return dtm\n\n\ndtm_sets_norm = []\n\nfor s in dtm_sets:\n    dtm_sets_norm.append(length_normalize(s,doc_lengths))","17f60245":"dtm_sets_norm = []\n\nfor s in dtm_sets:\n    dtm_sets_norm.append(length_normalize(s,doc_lengths))","d7690cb4":"def max_set_features(s):\n    \n    s['max'] = s.max(axis=1)\n    \n    max_array = np.array(s['max']).reshape(-1,1)\n    \n    min_max_scaler = preprocessing.MinMaxScaler()\n    \n    s['min_max'] = min_max_scaler.fit_transform(max_array)\n    \n    return s\n","80c7db11":"dtm_sets_max = []\n\nfor s in dtm_sets_norm:\n    dtm_sets_max.append(max_set_features(s))","77ad6494":"dtm_dict = { 'paper_id' : corpus['paper_id']\n            , 'dtm1' : dtm_sets_max[0]['min_max'].values\n            ,'dtm2' : dtm_sets_max[1]['min_max'].values\n            ,'dtm3' : dtm_sets_max[2]['min_max'].values\n            ,'dtm4' : dtm_sets_max[3]['min_max'].values\n            ,'dtm5' : dtm_sets_max[4]['min_max'].values\n            ,'dtm6' : dtm_sets_max[5]['min_max'].values\n            ,'dtm7' : dtm_sets_max[6]['min_max'].values\n            ,'dtm8' : dtm_sets_max[7]['min_max'].values\n            ,'dtm9' : dtm_sets_max[8]['min_max'].values\n            ,'dtm10' : dtm_sets_max[9]['min_max'].values\n            ,'dtm11' : dtm_sets_max[10]['min_max'].values\n            ,'dtm12' : dtm_sets_max[11]['min_max'].values\n            ,'dtm13' : dtm_sets_max[12]['min_max'].values\n           }\n\ndtm_columns=['paper_id', 'dtm1', 'dtm2','dtm3','dtm4', 'dtm5', 'dtm6','dtm7', 'dtm8','dtm9','dtm10', 'dtm11', 'dtm12','dtm13']\n\ndtm_summary = pd.DataFrame(dtm_dict,columns=dtm_columns)\n\ndel dtm_dict, dtm_columns","0af534f5":"task1a = pd.DataFrame({ 'paper_id' : corpus['paper_id'], 'score' : dtm_summary['dtm1'] * dtm_summary['dtm2']}\n                      , columns=['paper_id','score'])\n\ntask1b = pd.DataFrame({ 'paper_id' : corpus['paper_id'], 'score' : dtm_summary['dtm1'] * dtm_summary['dtm3'] * dtm_summary['dtm4']}\n                      , columns=['paper_id','score'])\n\ntask1c = pd.DataFrame({ 'paper_id' : corpus['paper_id'], 'score' : dtm_summary['dtm1'] * dtm_summary['dtm5']}\n                      , columns=['paper_id','score'])\n\ntask1d = pd.DataFrame({ 'paper_id' : corpus['paper_id'], 'score' : dtm_summary['dtm1'] * dtm_summary['dtm6']}\n                      , columns=['paper_id','score'])\n\ntask2 = pd.DataFrame({ 'paper_id' : corpus['paper_id'], 'score' : dtm_summary['dtm4'] * dtm_summary['dtm7']}\n                      , columns=['paper_id','score'])\n\ntask3 = pd.DataFrame({ 'paper_id' : corpus['paper_id'], 'score' : dtm_summary['dtm8'] * dtm_summary['dtm9']}\n                      , columns=['paper_id','score'])\n\ntask4 = pd.DataFrame({ 'paper_id' : corpus['paper_id'], 'score' : dtm_summary['dtm1'] * dtm_summary['dtm10']}\n                      , columns=['paper_id','score'])\n\ntask5 = pd.DataFrame({ 'paper_id' : corpus['paper_id'], 'score' : dtm_summary['dtm11'] * dtm_summary['dtm12'] * dtm_summary['dtm13']}\n                      , columns=['paper_id','score'])","a63483ed":"task1a = task1a.sort_values(by='score',ascending=False)\ntask1b = task1b.sort_values(by='score',ascending=False)\ntask1c = task1c.sort_values(by='score',ascending=False)\ntask1d = task1d.sort_values(by='score',ascending=False)\ntask2 = task2.sort_values(by='score',ascending=False)\ntask3 = task3.sort_values(by='score',ascending=False)\ntask4 = task4.sort_values(by='score',ascending=False)\ntask5 = task5.sort_values(by='score',ascending=False)","2faf9fd3":"task1a.head()","6a3761f9":"task1a_top100 = task1a.iloc[:100,:]\ntask1b_top100 = task1b.iloc[:100,:]\ntask1c_top100 = task1c.iloc[:100,:]\ntask1d_top100 = task1d.iloc[:100,:]\ntask2_top100 = task2.iloc[:100,:]\ntask3_top100 = task3.iloc[:100,:]\ntask4_top100 = task4.iloc[:100,:]\ntask5_top100 = task5.iloc[:100,:]","6b4c6e16":"task1a_top100 = task1a_top100.merge(corpus, how='left', on='paper_id', sort=False )\ntask1b_top100 = task1b_top100.merge(corpus, how='left', on='paper_id', sort=False )\ntask1c_top100 = task1c_top100.merge(corpus, how='left', on='paper_id', sort=False )\ntask1d_top100 = task1d_top100.merge(corpus, how='left', on='paper_id', sort=False )\ntask2_top100 = task2_top100.merge(corpus, how='left', on='paper_id', sort=False )\ntask3_top100 = task3_top100.merge(corpus, how='left', on='paper_id', sort=False )\ntask4_top100 = task4_top100.merge(corpus, how='left', on='paper_id', sort=False )\ntask5_top100 = task5_top100.merge(corpus, how='left', on='paper_id', sort=False )","c56870bc":"task1a_top100.head()","bb17e548":"#eliminate any remaining numbers from text\ntask1a_top100['text'] = task1a_top100['text'].replace('\\d+', 'NUM', regex=True)\ntask1b_top100['text'] = task1b_top100['text'].replace('\\d+', 'NUM', regex=True)\ntask1c_top100['text'] = task1c_top100['text'].replace('\\d+', 'NUM', regex=True)\ntask1d_top100['text'] = task1d_top100['text'].replace('\\d+', 'NUM', regex=True)\ntask2_top100['text'] = task2_top100['text'].replace('\\d+', 'NUM', regex=True)\ntask3_top100['text'] = task3_top100['text'].replace('\\d+', 'NUM', regex=True)\ntask4_top100['text'] = task4_top100['text'].replace('\\d+', 'NUM', regex=True)\ntask5_top100['text'] = task5_top100['text'].replace('\\d+', 'NUM', regex=True)","f71fcb62":"# TfidfVectorizer with select vocabulary\n\ndef tfidf_vectorizer(df):\n\n    '''\n    Generates: tfidf document term matrix\n    Args: dataframe\n    Returns: document term matrix as a dataframe \n    '''\n\n    \n    # generate a document term matrix\n    CV1 = TfidfVectorizer(input=\"content\", ngram_range=(1,2), stop_words = 'english',  max_df=0.3, max_features=100)\n    DTM1 = CV1.fit_transform(df['text'])\n\n    # add col names\n    ColNames=CV1.get_feature_names()\n\n    DF1 = pd.DataFrame(DTM1.toarray(), columns=ColNames)\n\n    # add row names\n    Dict1 = {}\n    for i in range(0, len(df.paper_id)):\n        Dict1[i] = df.paper_id[i]\n    DF1 = DF1.rename(Dict1, axis='index')\n\n    return DF1\n    ","b5c85bd6":"task1a_top100_tfidf = tfidf_vectorizer(task1a_top100)\ntask1b_top100_tfidf = tfidf_vectorizer(task1b_top100)\ntask1c_top100_tfidf = tfidf_vectorizer(task1c_top100)\ntask1d_top100_tfidf = tfidf_vectorizer(task1d_top100)\ntask2_top100_tfidf = tfidf_vectorizer(task1d_top100)\ntask3_top100_tfidf = tfidf_vectorizer(task1d_top100)\ntask4_top100_tfidf = tfidf_vectorizer(task1d_top100)\ntask5_top100_tfidf = tfidf_vectorizer(task1d_top100)","838559d9":"task1a_top100_tfidf.head()","7e8a805f":"task1b_top100_tfidf.head()","f19af445":"def tfidf_pca(df):\n    \n    pca = PCA(n_components=2).fit(df)\n    pca_2d = pca.transform(df)\n\n    for i in range(0, pca_2d.shape[0]):\n\n        c1 = plt.scatter(pca_2d[i,0],pca_2d[i,1],c='r',\n            marker='+')\n        \n    return c1","5391b28c":"task1a_tfidf_pca = tfidf_pca(task1a_top100_tfidf)","3b1bf7b2":"task1b_tfidf_pca = tfidf_pca(task1b_top100_tfidf)","8575078d":"task1c_tfidf_pca = tfidf_pca(task1c_top100_tfidf)","99556f7f":"task1d_tfidf_pca = tfidf_pca(task1d_top100_tfidf)","18cc2780":"def tfidf_pca_kmeans(df,num_clusters):\n    \n    pca = PCA(n_components=2).fit(df)\n    pca_2d = pca.transform(df)\n\n    task_tfidf_kmeans = KMeans(n_clusters=num_clusters, random_state=111)\n    task_tfidf_kmeans.fit(df)\n\n    c1 = plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=task_tfidf_kmeans.labels_)\n\n    return c1, task_tfidf_kmeans","2db7b830":"task1a_tfidf_plot, task1a_tfidf_pca_kmeans = tfidf_pca_kmeans(task1a_top100_tfidf, num_clusters=3)","e5a27b01":"task1b_tfidf_plot, task1b_tfidf_pca_kmeans = tfidf_pca_kmeans(task1b_top100_tfidf, num_clusters=3)","7740eb44":"task1c_tfidf_plot, task1c_tfidf_pca_kmeans = tfidf_pca_kmeans(task1c_top100_tfidf, num_clusters=3)","941256a0":"task1d_tfidf_plot, task1d_tfidf_pca_kmeans = tfidf_pca_kmeans(task1d_top100_tfidf, num_clusters=3)","5fb4a025":"def tfidf_kmeans(df,num_clusters):\n\n    task_tfidf_kmeans = KMeans(n_clusters=num_clusters, random_state=111)\n    task_tfidf_kmeans.fit(df)\n    \n    task_tfidf_kmeans_labels = task_tfidf_kmeans.labels_\n    task_tfidf_kmeans_df = pd.DataFrame([df.index,task_tfidf_kmeans_labels]).T\n    \n    task_tfidf_kmeans_df.columns = ['paper_id', 'cluster']\n\n    return task_tfidf_kmeans_df","5e0cd75a":"task1a_tfidf_pca_kmeans_df = tfidf_kmeans(task1a_top100_tfidf,3)\ntask1b_tfidf_pca_kmeans_df = tfidf_kmeans(task1b_top100_tfidf,3)\ntask1c_tfidf_pca_kmeans_df = tfidf_kmeans(task1c_top100_tfidf,3)\ntask1d_tfidf_pca_kmeans_df = tfidf_kmeans(task1d_top100_tfidf,3)\ntask2_tfidf_pca_kmeans_df = tfidf_kmeans(task2_top100_tfidf,3)\ntask3_tfidf_pca_kmeans_df = tfidf_kmeans(task3_top100_tfidf,3)\ntask4_tfidf_pca_kmeans_df = tfidf_kmeans(task4_top100_tfidf,3)\ntask5_tfidf_pca_kmeans_df = tfidf_kmeans(task5_top100_tfidf,3)","37c05209":"task1a_tfidf_pca_kmeans_df.info()","d0e1a591":"task1a_tfidf_cluster1 = task1a_tfidf_pca_kmeans_df.loc[task1a_tfidf_pca_kmeans_df['cluster']==0]","f9eb573f":"task1a_tfidf_cluster1[:10]","035148cc":"def prepare_output(task_df, task_tfidf_df, task_pca_kmeans_df):\n    \n    task_tfidf_df.reset_index(inplace=True)\n    task_tfidf_df.rename(columns={'index':'paper_id'}, inplace=True)\n    \n    task_df = task_df.merge(task_pca_kmeans_df, how='left', on='paper_id', sort=False )\n    task_df = task_df.merge(task_tfidf_df, how='left', on='paper_id', sort=False )\n    \n    return task_df","988a8b99":"task1a_top100 = prepare_output(task1a_top100, task1a_top100_tfidf, task1a_tfidf_pca_kmeans_df)\n#task1b_top100 = prepare_output(task1b_top100, task1b_top100_tfidf, task1b_tfidf_pca_kmeans_df)\ntask1c_top100 = prepare_output(task1c_top100, task1c_top100_tfidf, task1c_tfidf_pca_kmeans_df)\ntask1d_top100 = prepare_output(task1d_top100, task1d_top100_tfidf, task1d_tfidf_pca_kmeans_df)\ntask2_top100 = prepare_output(task2_top100, task2_top100_tfidf, task2_tfidf_pca_kmeans_df)\ntask3_top100 = prepare_output(task3_top100, task3_top100_tfidf, task3_tfidf_pca_kmeans_df)\ntask4_top100 = prepare_output(task4_top100, task4_top100_tfidf, task4_tfidf_pca_kmeans_df)\ntask5_top100 = prepare_output(task5_top100, task5_top100_tfidf, task5_tfidf_pca_kmeans_df)\n\ntask1a_top100.to_csv('task1a_top100.csv', sep=',')\n#task1b_top100.to_csv('task1b_top100.csv', sep=',')\ntask1c_top100.to_csv('task1c_top100.csv', sep=',')\ntask1d_top100.to_csv('task1d_top100.csv', sep=',')\ntask2_top100.to_csv('task2_top100.csv', sep=',')\ntask3_top100.to_csv('task3_top100.csv', sep=',')\ntask4_top100.to_csv('task4_top100.csv', sep=',')\ntask5_top100.to_csv('task5_top100.csv', sep=',')","39ea0adc":"# task1b is run separately from the others because index column has different name\ntask1b_top100_tfidf.reset_index(inplace=True)\ntask1b_top100_tfidf.rename(columns={'level_0':'paper_id'}, inplace=True)\ntask1b_top100 = task1b_top100.merge(task1b_tfidf_pca_kmeans_df, how='left', on='paper_id', sort=False )\ntask1b_top100 = task1b_top100.merge(task1b_top100_tfidf, how='left', on='paper_id', sort=False )\ntask1b_top100.to_csv('task1b_top100.csv', sep=',')","ac7f4c70":"### k-means clustering may be able to help identify subsets of related documents within the 100 most task-relevant documents for each task.","665d2a1c":"### Assemble the results into one data frame","31fa5324":"# Exploratory analysis of COVID-19 risk factors","5a7cedac":"### Generate raw word counts for each document that can be used to normalize term frequencies by document length","941dd8cb":"# 3. Combining document-Term set scores to produce task relevance scores","feab9ac6":"# 1. Load and clean the data","9a8fa7a4":"# Introduction\n\nExploration of the [Covid-19 dataset](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge) for the task: [What do we know about COVID-19 risk factors?](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=558).  \n\nSocial determinants of health will be explored for knowledge that might be derived from the literature dataset. Based on keywords taken from the task details, subsets of terms were used to extract document-term matrices relevant to each task.  Highly interesting results could be manually reviewed by an expert.\n\nFor example, for Task 1a, a list of terms related to \u2018risk factors\u2019 and another list of terms related to \u2018smoking\u2019 or \u2018pre-existing pulmonary disease\u2019 could be combined to create a 'query'.  To broaden each term set, additional synonyms were added each of the term sets by manual curation.\n\nThe relevant terms from the task details are shown below enclosed in brackets with superscripts that identify the number of the corresponding term set.\n\n<b>Task Details<\/b>\n1. Data on potential {risk factors}<sup>1<\/sup>\n    <br>a. {Smoking, pre-existing pulmonary disease}<sup>2<\/sup>\n    <br>b. {Co-infections}<sup>3<\/sup> (determine whether {co-existing respiratory\/viral infections}<sup>3<\/sup> make the virus more {transmissible or virulent}<sup>4<\/sup>) and other {co-morbidities}<sup>3<\/sup>\n    <br>c. {Neonates and pregnant}<sup>5<\/sup> women\n    <br>d. {Socio-economic}<sup>6<\/sup> and {behavioral factors}<sup>6<\/sup> to understand the {economic}<sup>6<\/sup> impact of the virus and whether there were differences.\n2. {Transmission dynamics}<sup>7<\/sup> of the virus, including the basic {reproductive number, incubation period, serial interval, modes of transmission and environmental factors}<sup>7<\/sup>\n3. {Severity}<sup>8<\/sup> of disease, including risk of {fatality}<sup>8<\/sup> among {symptomatic hospitalized patients}<sup>9<\/sup>, and {high-risk patient}<sup>9<\/sup> groups\n4. {Susceptibility}<sup>1<\/sup> of {populations}<sup>10<\/sup>\n5. {Public health}<sup>11<\/sup> {mitigation}<sup>13<\/sup> {measures}<sup>12<\/sup> that could be effective for {control}<sup>13<\/sup>\n","e7d0d80f":"### For each task, calculate the score by multiplying the normalized max scores for each set together to generate a composite score representing how strongly each document reflects each task.","9c00bf30":"# 6. Output information for the top 100 ranked documents for each task","0db9e1aa":"### Join together all task-related document information","267cb33b":"### Loop through the term sets to get a document-term matrix for each.","b5a00bd6":"### Combine the title, abstract, and body text into a new column","b344d685":"### Sort the dataframes, take the top 100 results from each task, and get the text for those documents","c273d3d1":"# 5.  Visualize top 100 documents for each task as kmeans clusters","2c7a5760":"### Clean up the columns in the dataframe","dfa1577d":"### Calculate the maximum feature value for every document for each term set, then min-max normalize the values of the maximum value feature.","d18fe1c3":"# 4. Generate TFIDF-weighted document-term matrices for each task","b3925f0a":"### Look at some of the documents in a cluster","4a088d6a":"# 2. Creating a document term matrix for each term set","156481b2":"### Begin by looking at a principle components analysis of the top 11 documents for some tasks","d0565516":"## Term set composition\n\nUsing this method, the term sets that were constructed are shown below. The root terms taken from the task details are highlighted.\n\t\t\t\n![image.png](attachment:image.png)\n","fbe3c8fa":"### Create document term matrices for lists of terms that are relevant to each of the tasks","157d3a7d":"## Scoring documents for task relevance\n\n<b>Computing term set relevance for each document:<\/b>\n1. For each document, compute the document-length normalized term frequency for each term\n2. For each document, compute the document's term set relevance (the maximimum score out of all the terms in the term set for a given document)\n3. Min-Max normalize the document's term set relevance\n\n<b>Combining the term sets to produce task relevance:<\/b>\n<br>\n<br>\nTo construct a \u2018query\u2019 from the task lists, the scored document-term set relevance tables were combined according to the the following matrix, where squares shown in green represent the term lists that were combined for each task. To produce an overall score for each document for each task, the document's term set relevances were multiplied together for all the term sets making up each task.\n\n\n\n![image.png](attachment:image.png)","b1459996":"## Discussion of the pros\/cons of this method\n\n<b>Pros:<\/b>\n<br>\n* This method attempts to capture all parts of a task by assigning equal importance to different parts of the task.  For example, when determining Public health mitigation measures that could be effective for control (task 5), equal weights are assigned to three different requirements of the task by multiplying the three representative term sets together.  For example, to capture the linkage between Public health <--> measures <--> mitigation\/control. It requires all three components of the relationship to be strongly represented by thier respective term sets. \n* It utilizes a straightforward bag of words approach (via the term document matrix) and does not require complex logic to identify relevant documetnts. \n<br>\n\n<br>\n<b>Cons:<\/b>\n* While the method for determining document relatedness to a task is conceptually basic, it may miss subtleties in the thesis of the documents that would be better captured by an NLP methods, such as the order of cause and effect between relevant terms.  In the preceding example, the linkage between Public health <--> measures <--> for mitigation\/control does not imply a cause and effect relationship such as Public health --> measures --> for mitigation\/control.\n","030ea89d":"### Here, for the top 100 most highly ranked documents for each task, generate feature vectors containing the top 100 TF-IDF-weighted features","9587d505":"### Get the k-means cluster identifications for the top 100 ranked documents for all of the tasks","168e7adb":"## Acknowledgements\n\nThe author wishes to thank user [xhlulu](https:\/\/www.kaggle.com\/xhlulu) for permission to reuse the dataset located at the kernal [here](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv).","73d21393":"### Normalize the vectors by dividing by the document length","f4dbfde3":"### Based on the above PCA analysis, it looks like three k-means clusters should be about right"}}