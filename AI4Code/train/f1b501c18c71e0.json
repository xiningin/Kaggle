{"cell_type":{"3486e67a":"code","eed5e9ff":"code","8ce4a818":"code","07b6c19d":"code","0a935385":"code","8e9041af":"code","89e1bca4":"code","d6ba76c2":"code","d3a40a71":"code","bbb4e8dd":"code","f502d3ef":"code","96bb85bd":"code","4ff36682":"code","0588a229":"code","cf9b7352":"code","f2c8096b":"code","c5518f65":"code","47c632d1":"code","02df47cd":"code","77f2db80":"code","69ded64c":"code","8e9f9c2b":"code","73d55cba":"code","3abd2f60":"code","5cc06171":"code","1022cdb9":"code","9dde7e09":"code","f18e67ee":"code","5a9bc1e1":"code","d4c3b07f":"code","9244eb35":"code","5f09db25":"code","6fed652a":"code","37c11575":"code","7534bdc7":"code","2481eb5c":"code","0610ab8d":"code","dff06169":"code","26d1c0a0":"code","284dd702":"code","3eae579e":"code","c7c89010":"code","84434b93":"code","5abfe670":"code","9e655c7b":"code","58363495":"code","1747bb5b":"code","9e0aa797":"code","6f6f38b4":"code","c2bb1d2d":"code","61586a6c":"code","40f090d3":"code","e494d363":"code","e2cc0b17":"code","b9a7679f":"code","9003b2c6":"code","b97f31e7":"code","1124cf3c":"code","75d49102":"code","850519e0":"code","5cda7943":"code","9e05b9cb":"code","189e6cd8":"code","e3ca6ec2":"code","afedd076":"code","9251533c":"code","1cc2559b":"code","ab3dc0af":"code","f0904458":"code","827aeecb":"code","67987b4d":"code","f8266747":"code","3e2dfc88":"code","4d2d893e":"code","7fd33161":"code","0d302c8b":"code","d0227328":"code","49cb489b":"code","69b82167":"code","41bc0e51":"code","977ed576":"code","7530cb15":"code","e4015f8b":"code","fe951591":"code","d57dcdf8":"code","07f27482":"code","1393b42e":"code","e3ab46e3":"code","017d5b30":"code","cba2fc2f":"code","072dd75e":"code","ef6b83f2":"code","8ab3a985":"code","ef0c6c9e":"code","cc0663a5":"code","74c21ddc":"code","72aeeae5":"code","46a7d218":"code","4f6e0c0d":"code","878db7e1":"code","15f0035c":"code","e0638923":"code","412a3e27":"code","0cecbffd":"code","648ae132":"code","e8e7c537":"code","1d319c9b":"code","204c1c16":"code","bb2cf5dd":"code","ba4b8d94":"code","6c003e84":"code","f3aa6e6e":"code","88648f61":"code","455e1774":"code","fdbccb96":"code","8731f65c":"code","182e484e":"code","e93f139d":"code","8c6255b8":"code","b810dda7":"code","f4346bb0":"code","2fb0b03c":"markdown","0dbfa8d3":"markdown","02554beb":"markdown","bc1da4ac":"markdown","a6556301":"markdown","245af594":"markdown","e1517e76":"markdown","1631a161":"markdown","5737b70e":"markdown","24486460":"markdown","29405b46":"markdown","53a01e03":"markdown","939f62fd":"markdown","c387e4e2":"markdown","13642785":"markdown","3d3e7c75":"markdown","c333f7c0":"markdown","d26350e0":"markdown","f75e2400":"markdown","1f801f4d":"markdown","33d79f12":"markdown","05ba12b6":"markdown","57e9114e":"markdown","219a605e":"markdown","0656a384":"markdown","fb8dc8f5":"markdown","1042d08e":"markdown","b055a65f":"markdown","fa8f7436":"markdown","22f6ceea":"markdown","af7f0958":"markdown","ff5d3390":"markdown","a3275e0f":"markdown","9f7b19ff":"markdown","0437123d":"markdown","9f7c5b3e":"markdown","4a2deb72":"markdown","22f9c7ba":"markdown","5ddf7f6c":"markdown","0f837dce":"markdown","057ab16b":"markdown","1b483e8c":"markdown","ce303780":"markdown","2cfeba05":"markdown","fe68a54a":"markdown","2790a69c":"markdown","0c0930df":"markdown","6d2ad24d":"markdown","f8924c5c":"markdown","460d7f60":"markdown","c1dc1c60":"markdown","b1b959db":"markdown","8828dba4":"markdown","3dfb5e01":"markdown","6964e483":"markdown","45897cf5":"markdown","a476bb9d":"markdown","33d229cf":"markdown","8779b67b":"markdown","6516ec77":"markdown","178a945f":"markdown","65ffd4b8":"markdown","6d7aa861":"markdown","c5e0c9b2":"markdown","361d3960":"markdown","097bc84e":"markdown","67f400aa":"markdown","365e94e9":"markdown","0395aabf":"markdown","38812648":"markdown","fa020a13":"markdown","dfd37d0a":"markdown","7019dee1":"markdown","dc40cf74":"markdown","3f26c8bf":"markdown","f5d96219":"markdown","fcbe557f":"markdown"},"source":{"3486e67a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eed5e9ff":"# Read Data\nimport numpy as np                     # Linear Algebra (calculate the mean and standard deviation)\nimport pandas as pd                    # manipulate data, data processing, load csv file I\/O (e.g. pd.read_csv)\n\n# Visualization\nimport seaborn as sns                  # Visualization using seaborn\nimport matplotlib.pyplot as plt        # Visualization using matplotlib\n%matplotlib inline\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\n# style\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"\nplt.style.use(\"fivethirtyeight\")       # Set Graphs Background style using matplotlib\nsns.set_style(\"darkgrid\")              # Set Graphs Background style using seaborn\n\n# ML model building; Pre Processing & Evaluation\nfrom sklearn.model_selection import train_test_split                     # split  data into training and testing sets\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge          # Linear Regression, Lasso and Ridge\nfrom sklearn.linear_model import LogisticRegression                      # Logistic Regression\nfrom sklearn.tree import DecisionTreeRegressor                           # Decision tree Regression\nfrom sklearn.ensemble import RandomForestClassifier                      # this will make a Random Forest Classifier\nfrom sklearn import svm                                                  # this will make a SVM classificaiton\nfrom sklearn.svm import SVC                                              # import SVC from SVM\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report      # this creates a confusion matrix\nfrom sklearn.metrics import roc_curve,auc                                # ROC\nfrom sklearn.preprocessing import StandardScaler                         # Standard Scalar\nfrom sklearn.model_selection import GridSearchCV                         # this will do cross validation\n\nimport warnings                        # Ignore Warnings\nwarnings.filterwarnings(\"ignore\")","8ce4a818":"# Import first 5 rows\ncover = pd.read_csv(\"\/kaggle\/input\/forest-cover-type-prediction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/forest-cover-type-prediction\/test.csv\")","07b6c19d":"display(cover.head())\ndisplay(test.head())","0a935385":"# checking dimension (num of rows and columns) of dataset\nprint(\"Training data shape (Rows, Columns):\",cover.shape)\nprint(\"Training data shape (Rows, Columns):\",test.shape)","8e9041af":"cover['Cover_Type'].value_counts()","89e1bca4":"# check dataframe structure like columns and its counts, datatypes & Null Values\ncover.info()","d6ba76c2":"cover.dtypes.value_counts()","d3a40a71":"# Gives number of data points in each variable\ncover.count()","bbb4e8dd":"# Listing Number of missing values by feature column wise\ncover.isnull().sum()","f502d3ef":"# any() check null values by columns\ncover.isnull().any()","96bb85bd":"plt.figure(figsize=(17,10))\nsns.heatmap(cover.isnull(), yticklabels=False, cbar=False, cmap='viridis')","4ff36682":"for column in cover.columns:\n    print(column,cover[column].nunique())","0588a229":"numerical_features = cover.select_dtypes(exclude='object')\nnumerical_features","cf9b7352":"discrete_feature=[feature for feature in numerical_features if len(cover[feature].unique())<25]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","f2c8096b":"continuous_features=[feature for feature in numerical_features if feature not in discrete_feature+['Cover_Type']]\nprint(\"Continuous feature Count {}\".format(len(continuous_features)))","c5518f65":"continuous_features","47c632d1":"fig, ax = plt.subplots(3,4, figsize=(14,9))\nsns.distplot(cover.Elevation, bins = 20, ax=ax[0,0]) \nsns.distplot(cover.Aspect, bins = 20, ax=ax[0,1]) \nsns.distplot(cover.Slope, bins = 20, ax=ax[0,2]) \nsns.distplot(cover.Horizontal_Distance_To_Hydrology, bins = 20, ax=ax[0,3])\nsns.distplot(cover.Vertical_Distance_To_Hydrology, bins = 20, ax=ax[1,0]) \nsns.distplot(cover.Horizontal_Distance_To_Roadways, bins = 20, ax=ax[1,1]) \nsns.distplot(cover.Hillshade_9am, bins = 20, ax=ax[1,2]) \nsns.distplot(cover.Hillshade_Noon, bins = 20, ax=ax[1,3])\nsns.distplot(cover.Hillshade_3pm, bins = 20, ax=ax[2,0])\nsns.distplot(cover.Horizontal_Distance_To_Fire_Points, bins = 20, ax=ax[2,1])\nplt.show()","02df47cd":"plt.figure(figsize=(20,60), facecolor='white')\nplotnumber =1\nfor feature in continuous_features:\n    data=cover.copy()\n    ax = plt.subplot(12,3,plotnumber)\n    plt.scatter(cover[feature], cover['Cover_Type'])\n    plt.xlabel(feature)\n    plt.ylabel('Cover_Type')\n    plt.title(feature)\n    plotnumber+=1\nplt.show()","77f2db80":"# boxplot on numerical features to find outliers\nplt.figure(figsize=(18,15), facecolor='white')\nplotnumber =1\nfor numerical_feature in numerical_features:\n    ax = plt.subplot(19,3,plotnumber)\n    sns.boxplot(cover[numerical_feature])\n    plt.xlabel(numerical_feature)\n    plotnumber+=1\nplt.show()","69ded64c":"plt.figure(figsize = (14,12))\nplt.title('Correlation of Numeric Features with Sale Price', y=1, size=16)\nsns.heatmap(cover.corr(), square = True, vmax=0.8)","8e9f9c2b":"corr = cover.drop('Cover_Type', axis=1).corr()\nplt.figure(figsize=(17, 14))\n\nsns.heatmap(corr[(corr >= 0.5) | (corr <= -0.4)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","73d55cba":"corrmat = cover.iloc[:,:10].corr()\nf, ax = plt.subplots(figsize = (12,8))\nsns.heatmap(corrmat, cmap='viridis', vmax=0.8, annot=True, square=True);","3abd2f60":"# descriptive statistics (numerical columns)\npd.set_option('display.max_columns', None)\ncover.describe()","5cc06171":"# Histogram for \"Elevation\"\nplt.figure(figsize=(5,4))\nsns.distplot(cover.Elevation,rug=True)","1022cdb9":"sns.distplot(cover.Aspect)\nplt.grid()","9dde7e09":"# Histogram for \"Elevation\"\nsns.boxplot(cover['Elevation'])","f18e67ee":"sns.boxplot(cover.Vertical_Distance_To_Hydrology)\nplt.title('Vertical_Distance_To_Hydrology')","5a9bc1e1":"# Histogram for \"All Features\"\ncover.hist(figsize=(16, 20), bins=50, xlabelsize=7, ylabelsize=7);","d4c3b07f":"sns.violinplot(x=cover['Cover_Type'],y=cover['Elevation'])\nplt.grid()","9244eb35":"sns.violinplot(x=cover['Cover_Type'],y=cover['Aspect'])\nplt.grid()","5f09db25":"# vertical distance to the hydrology column\nsns.violinplot(x=cover.Cover_Type, y=cover.Vertical_Distance_To_Hydrology)","6fed652a":"# Line Plot between \"Aspect\" and \"Cover_Type\"\nplt.figure(figsize=(7,6))\nsns.lineplot(x=cover['Aspect'], y=cover['Cover_Type'])\n\nplt.xlabel('Aspect', fontsize=15, fontweight='bold')\nplt.ylabel('Cover_Type', fontsize=15, fontweight='bold')\n\nplt.title('Aspect Vs Cover_Type', fontsize=18, fontweight='bold')\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.show()","37c11575":"# Line Plot between \"Slope\" and \"Cover_Type\"\nplt.figure(figsize=(7,6))\nsns.lineplot(x=cover['Slope'], y=cover['Cover_Type'])\n\nplt.xlabel('Slope', fontsize=15, fontweight='bold')\nplt.ylabel('Cover_Type', fontsize=15, fontweight='bold')\n\nplt.title('Slope Vs Cover_Type', fontsize=18, fontweight='bold')\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.show()","7534bdc7":"fig = px.scatter(cover, x='Elevation', y= 'Horizontal_Distance_To_Roadways', color='Cover_Type', width=800, height=400)\nfig.show()","2481eb5c":"# Scatter Plot between \"GrLivArea\" and \"SalePrice\"\nplt.figure(figsize=(7,6))\nsns.scatterplot(cover.Aspect, cover.Hillshade_3pm)\n\nplt.xlabel('Aspect', fontsize=15, fontweight='bold')\nplt.ylabel('Hillshade_3pm', fontsize=15, fontweight='bold')\n\nplt.title('Aspect Vs Hillshade_3pm', fontsize=18, fontweight='bold')\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.show()","0610ab8d":"# Scatter Plot between \"Horizontal_Distance_To_Hydrology\" and \"Vertical_Distance_To_Hydrology\" variable\nplt.figure(figsize=(7,6))\nsns.scatterplot(cover['Horizontal_Distance_To_Hydrology'], cover['Vertical_Distance_To_Hydrology'])\n\nplt.xlabel('Horizontal_Distance_To_Hydrology', fontsize=15, fontweight='bold')\nplt.ylabel('Vertical_Distance_To_Hydrology', fontsize=15, fontweight='bold')\n\nplt.title('Horizontal_Distance_To_Hydrology Vs Vertical_Distance_To_Hydrology', fontsize=18, fontweight='bold')\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.show()","dff06169":"# Scatter Plot between \"Hillshade_Noon\" and \"Hillshade_3pm\"\nfig = px.scatter(cover,x='Hillshade_Noon',y= 'Hillshade_3pm',color='Cover_Type',width=800,height=400)\nfig.show()","26d1c0a0":"# Scatter Plot between \"Aspect\" and \"Hillshade_9am\"\nfig = px.scatter(cover,x='Aspect',y= 'Hillshade_9am',color='Cover_Type',width=800,height=400)\nfig.show()","284dd702":"# Scatter Plot between \"Hillshade_9am\" and \"Hillshade_3pm\"\nfig = px.scatter(cover,x='Hillshade_9am',y= 'Hillshade_3pm',color='Cover_Type',width=800,height=400)\nfig.show()","3eae579e":"# Scatter Plot between \"Slope\" and \"Hillshade_Noon\"\nfig = px.scatter(cover,x='Slope',y= 'Hillshade_Noon',color='Cover_Type',width=800,height=400)\nfig.show()","c7c89010":"# Count Plot for \"Cover_Type\"\nplt.figure(figsize = (15, 9))\nsns.countplot(x = 'Cover_Type', data = cover)\nxt = plt.xticks(rotation=45)","84434b93":"# A violin plot is a hybrid of a box plot and a kernel density plot, which shows peaks in the data.\ncols = cover.columns\nsize = len(cols) - 1 # We don't need the target attribute\n# x-axis has target attributes to distinguish between classes\nx = cols[size]\ny = cols[0:size]\n\nfor i in range(0, size):\n    sns.violinplot(data=cover, x=x, y=y[i])\n    plt.show()","5abfe670":"sns.set()\ncolumns = cover.iloc[:,:10]\nsns.pairplot(columns, kind ='scatter', diag_kind='kde')","9e655c7b":"# Checking the value count for different soil_types\nfor i in range(10, cover.shape[1]-1):\n    j = cover.columns[i]\n    print (cover[j].value_counts())","58363495":"cover.iloc[:,:10].skew()","1747bb5b":"# Checking the skewness of \"LotArea\" attributes\nsns.distplot(cover['Horizontal_Distance_To_Hydrology'])\nSkew_Horizontal_Distance_To_Hydrology = cover['Horizontal_Distance_To_Hydrology'].skew()\nplt.title(\"Skew:\"+str(Skew_Horizontal_Distance_To_Hydrology))","9e0aa797":"# calculating the square for the column df['LotArea'] column\nsns.distplot(np.sqrt(cover['Horizontal_Distance_To_Hydrology']))\nSkew_Horizontal_Distance_To_Hydrology_sqrt = np.sqrt(cover['Horizontal_Distance_To_Hydrology']+1).skew()\nplt.title(\"Skew:\"+str(Skew_Horizontal_Distance_To_Hydrology_sqrt))","6f6f38b4":"# Checking the skewness of \"Vertical_Distance_To_Hydrology\" attributes\nsns.distplot(cover['Vertical_Distance_To_Hydrology'])\nSkew_Vertical_Distance_To_Hydrology = cover['Vertical_Distance_To_Hydrology'].skew()\nplt.title(\"Skew:\"+str(Skew_Vertical_Distance_To_Hydrology))","c2bb1d2d":"# Checking the skewness of \"Horizontal_Distance_To_Roadways\" attributes\nsns.distplot(cover['Horizontal_Distance_To_Roadways'])\nSkew_Horizontal_Distance_To_Roadways = cover['Horizontal_Distance_To_Roadways'].skew()\nplt.title(\"Skew:\"+str(Skew_Horizontal_Distance_To_Roadways))","61586a6c":"# calculating the square for the column df['Horizontal_Distance_To_Roadways'] column\nsns.distplot(np.sqrt(cover['Horizontal_Distance_To_Roadways']))\nSkew_Horizontal_Distance_To_Roadways_sqrt = np.sqrt(cover['Horizontal_Distance_To_Roadways']).skew()\nplt.title(\"Skew:\"+str(Skew_Horizontal_Distance_To_Roadways_sqrt))","40f090d3":"# Checking the skewness of \"Hillshade_9am\" attributes\nsns.distplot(cover['Hillshade_9am'])\nSkew_Hillshade_9am = cover['Hillshade_9am'].skew()\nplt.title(\"Skew:\"+str(Skew_Hillshade_9am))","e494d363":"# calculating the square for the column df['Hillshade_9am'] column\nsns.distplot(np.power(cover['Hillshade_9am'],5))\nSkew_Hillshade_9am_power = np.power(cover['Hillshade_9am'],5).skew()\nplt.title(\"Skew:\"+str(Skew_Hillshade_9am_power))","e2cc0b17":"# Checking the skewness of \"Hillshade_Noon\" attributes\nsns.distplot(cover['Hillshade_Noon'])\nSkew_Hillshade_Noon = cover['Hillshade_Noon'].skew()\nplt.title(\"Skew:\"+str(Skew_Hillshade_Noon))","b9a7679f":"# calculating the square for the column df['Hillshade_9am'] column\nsns.distplot(np.power(cover['Hillshade_Noon'],5))\nSkew_Hillshade_Noon_power = np.power(cover['Hillshade_Noon'],5).skew()\nplt.title(\"Skew:\"+str(Skew_Hillshade_Noon_power))","9003b2c6":"# Checking the skewness of \"Horizontal_Distance_To_Fire_Points\" attributes\nsns.distplot(cover['Horizontal_Distance_To_Fire_Points'])\nSkew_Horizontal_Distance_To_Fire_Points = cover['Horizontal_Distance_To_Fire_Points'].skew()\nplt.title(\"Skew:\"+str(Skew_Horizontal_Distance_To_Fire_Points))","b97f31e7":"# calculating the square for the column df['Horizontal_Distance_To_Fire_Points'] column\nsns.distplot(np.cbrt(cover['Horizontal_Distance_To_Fire_Points']))\nSkew_Horizontal_Distance_To_Fire_Points_cube = np.cbrt(cover['Horizontal_Distance_To_Fire_Points']).skew()\nplt.title(\"Skew:\"+str(Skew_Horizontal_Distance_To_Fire_Points_cube))","1124cf3c":"# Checking the skewness of \"Slope\" attributes\nsns.distplot(cover['Slope'])\nSkew_Slope = cover['Slope'].skew()\nplt.title(\"Skew:\"+str(Skew_Slope))","75d49102":"# calculating the square for the column df['Slope'] column\nsns.distplot(np.sqrt(cover['Slope']))\nSkew_Slope_sqrt = np.sqrt(cover['Slope']).skew()\nplt.title(\"Skew:\"+str(Skew_Slope_sqrt))","850519e0":"cover['dist_hydr'] = np.sqrt(cover['Vertical_Distance_To_Hydrology']**2 + cover['Horizontal_Distance_To_Hydrology']**2)\ntest['dist_hydr'] = np.sqrt(cover['Vertical_Distance_To_Hydrology']**2 + cover['Horizontal_Distance_To_Hydrology']**2)","5cda7943":"sns.distplot(cover['dist_hydr'], color='green')","9e05b9cb":"cover.head()","189e6cd8":"test.head()","e3ca6ec2":"# standardizing the columns except \"soil type and wilderness_area\" since they are binary  \n\ncover_new = cover.iloc[:,:11]\ncover_new['dist_hydr'] = cover['dist_hydr']\ncover_new.info()","afedd076":"sc = StandardScaler()\nsc.fit(cover_new)\ncover_new = sc.transform(cover_new)","9251533c":"cover_new[:10,1]","1cc2559b":"cover.iloc[:,1:11] = cover_new[:,0:10]","ab3dc0af":"cover['dist_hydr'] = cover_new[:,10]","f0904458":"# Correlation of \"independant features\" with \"target\" feature\n# Drop least correlated features; since we have hign dimmensional data \ncover_corr = cover.corr()\ncover_corr['Cover_Type'].abs().sort_values(ascending=False)","827aeecb":"# Independant variable\nX = cover.drop(columns='Cover_Type',axis=1)\n# Dependant variable\ny = cover['Cover_Type']","67987b4d":"# split  data into training and testing sets of 70:30 ratio\n# 20% of test size selected\n# random_state is random seed\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=4)","f8266747":"# shape of X & Y test \/ train\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","3e2dfc88":"clf_accuracy=[]","4d2d893e":"LogReg = LogisticRegression(max_iter=1000)\nLogReg.fit(X_train, y_train)","7fd33161":"y_pred_LogReg = LogReg.predict(X_test)\nclf_accuracy.append(accuracy_score(y_test, y_pred_LogReg))\nprint(accuracy_score(y_test, y_pred_LogReg))","0d302c8b":"print(\"Train Score {:.2f} & Test Score {:.2f}\".format(LogReg.score(X_train, y_train), LogReg.score(X_test, y_test)))","d0227328":"print(\"Model\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Logistic Regression \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_LogReg)),\n            mean_squared_error(y_test, y_pred_LogReg),\n            mean_absolute_error(y_test, y_pred_LogReg),\n            r2_score(y_test, y_pred_LogReg)))","49cb489b":"DTR = DecisionTreeRegressor()\nDTR.fit(X_train, y_train)","69b82167":"y_pred_DTR = DTR.predict(X_test)","41bc0e51":"clf_accuracy.append(accuracy_score(y_test, y_pred_DTR))\nprint(accuracy_score(y_test, y_pred_DTR))","977ed576":"print(\"Train Score {:.2f} & Test Score {:.2f}\".format(DTR.score(X_train, y_train), DTR.score(X_test, y_test)))","7530cb15":"print(\"Model\\t\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Decision Tree Regressor \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_DTR)),\n            mean_squared_error(y_test, y_pred_DTR),\n            mean_absolute_error(y_test, y_pred_DTR),\n            r2_score(y_test, y_pred_DTR)))\n\nplt.scatter(y_test, y_pred_DTR)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.title(\"Decision Tree Regressor\")\n\nplt.show()","e4015f8b":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)","fe951591":"pred_rf = rf.predict(X_test)","d57dcdf8":"clf_accuracy.append(accuracy_score(y_test, pred_rf ))\nprint(accuracy_score(y_test, pred_rf ))","07f27482":"print(\"Train Score {:.2f} & Test Score {:.2f}\".format(rf.score(X_train, y_train), rf.score(X_test, y_test)))","1393b42e":"print(\"Model\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Random Forest \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, pred_rf )),\n            mean_squared_error(y_test, pred_rf ),\n            mean_absolute_error(y_test, pred_rf ),\n            r2_score(y_test, pred_rf )))","e3ab46e3":"KNN = KNeighborsClassifier()\n\nl=[i for i in range(1,11)]\naccuracy=[]\n\nfor i in l:\n    KNN = KNeighborsClassifier(n_neighbors=i, weights='distance')\n    KNN.fit(X_train, y_train)\n    pred_knn = KNN.predict(X_test)\n    accuracy.append(accuracy_score(y_test, pred_knn))\n\nplt.plot(l,accuracy)\nplt.title('knn_accuracy plot')\nplt.xlabel('neighbors')\nplt.ylabel('accuracy')\nplt.grid()\n\nprint(max(accuracy))\n\nclf_accuracy.append(max(accuracy))","017d5b30":"print(\"Model\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Random Forest \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, pred_rf )),\n            mean_squared_error(y_test, pred_rf ),\n            mean_absolute_error(y_test, pred_rf ),\n            r2_score(y_test, pred_rf )))","cba2fc2f":"import xgboost\nreg_xgb = xgboost.XGBClassifier(max_depth=7)\nreg_xgb.fit(X_train,y_train)","072dd75e":"# predicting X_test\ny_pred_xgb = reg_xgb.predict(X_test)","ef6b83f2":"print(\"Train Score {:.2f} & Test Score {:.2f}\".format(reg_xgb.score(X_train,y_train),reg_xgb.score(X_test,y_test)))","8ab3a985":"clf_accuracy.append(accuracy_score(y_test, y_pred_xgb))\nprint(accuracy_score(y_test, y_pred_xgb))","ef0c6c9e":"print(\"Model\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"XGBClassifier \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, pred_rf )),\n            mean_squared_error(y_test, pred_rf ),\n            mean_absolute_error(y_test, pred_rf ),\n            r2_score(y_test, pred_rf )))","cc0663a5":"nb = GaussianNB()\nnb.fit(X_train,y_train)","74c21ddc":"pred_nb = nb.predict(X_test)","72aeeae5":"clf_accuracy.append(accuracy_score(y_test, pred_nb))\nprint(accuracy_score(y_test, pred_nb))","46a7d218":"print(\"Train Score {:.2f} & Test Score {:.2f}\".format(nb.score(X_train,y_train),nb.score(X_test,y_test)))","4f6e0c0d":"print(\"Model\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Naive Bayes Classifier \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, pred_nb )),\n            mean_squared_error(y_test, pred_nb ),\n            mean_absolute_error(y_test, pred_nb ),\n            r2_score(y_test, pred_nb )))","878db7e1":"# classification Report\nprint(classification_report(y_test, pred_rf))","15f0035c":"# Confusion Matrix\ncf_matrix = confusion_matrix(y_test, pred_rf)\nprint('Confusion Matrix \\n',cf_matrix)","e0638923":"plt.figure(figsize=(7,6))\nsns.heatmap(cf_matrix, cmap='coolwarm', annot=True, linewidth=1, fmt=\"d\")\nplt.show()","412a3e27":"classifier_list=['Logistic Regression','Decision Tree','Random Forest','KNN','xgboost','nbayes']\nclf_accuracy1 = [0.6488095238095238,0.781415343915344,0.8621031746031746,0.6458333333333334,0.8753306878306878,0.6504629629629629]","0cecbffd":"plt.figure(figsize=(7,6))\nsns.barplot(x=clf_accuracy1, y=classifier_list)\nplt.grid()\nplt.xlabel('accuracy')\nplt.ylabel('classifier')\nplt.title('classifier vs accuracy plot')","648ae132":"models = [LogReg, DTR, rf, KNN, reg_xgb, nb]\nnames = ['Logistic Regression','Decision Tree','Random Forest','KNN','xgboost','nbayes']\nrmses = []\n\nfor model in models:\n    rmses.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n\nx = np.arange(len(names)) \nwidth = 0.3\n\nfig, ax = plt.subplots(figsize=(10,7))\nrects = ax.bar(x, rmses, width)\nax.set_ylabel('RMSE')\nax.set_xlabel('Models')\n\nax.set_title('RMSE with Different Algorithms')\n\nax.set_xticks(x)\nax.set_xticklabels(names, rotation=45)\n\nfig.tight_layout()","e8e7c537":"y_pred_test = reg_xgb.predict(test)","1d319c9b":"submission = pd.DataFrame({'Id': test['Id'], 'Cover_Type': y_pred_test})\nsubmission.to_csv('Forest Covetype.csv', index=False)","204c1c16":"from scipy import stats\nfrom scipy.stats import f_oneway\nfrom scipy.stats import ttest_ind","bb2cf5dd":"stats.ttest_1samp(cover['Elevation'],0)","ba4b8d94":"street_table = pd.crosstab(cover['Elevation'], cover['Cover_Type'])\nprint(street_table)","6c003e84":"street_table.values ","f3aa6e6e":"# Observed Values\nObserved_Values = street_table.values \nprint(\"Observed Values :-\\n\",Observed_Values)","88648f61":"val = stats.chi2_contingency(street_table)\nval","455e1774":"Expected_Values = val[3]","fdbccb96":"no_of_rows = len(street_table.iloc[0:2,0])\nno_of_columns = len(street_table.iloc[0,0:2])\nddof = (no_of_rows-1)*(no_of_columns-1)\nprint(\"Degree of Freedom:-\",ddof)\nalpha = 0.05","8731f65c":"from scipy.stats import chi2\nchi_square = sum([(o-e)**2.\/e for o,e in zip(Observed_Values, Expected_Values)])\nchi_square_statistic = chi_square[0]+chi_square[1]","182e484e":"print(\"chi-square statistic:-\",chi_square_statistic)","e93f139d":"critical_value = chi2.ppf(q=1-alpha,df=ddof)\nprint('critical_value:',critical_value)","8c6255b8":"# p-value\np_value = 1-chi2.cdf(x=chi_square_statistic, df=ddof)\nprint('p-value:', p_value)\nprint('Significance level: ',alpha)\nprint('Degree of Freedom: ',ddof)\nprint('p-value:', p_value)","b810dda7":"if chi_square_statistic>=critical_value:\n    print(\"Reject H0,There is a relationship between 2 categorical variables\")\nelse:\n    print(\"Retain H0,There is no relationship between 2 categorical variables\")\n    \nif p_value<=alpha:\n    print(\"Reject H0,There is a relationship between 2 categorical variables\")\nelse:\n    print(\"Retain H0,There is no relationship between 2 categorical variables\")","f4346bb0":"import statsmodels.api as sms\nmodel = sms.OLS(y,X).fit()\nmodel.summary()","2fb0b03c":"### 5. Violin Plot","0dbfa8d3":"### 4. Count Plot","02554beb":"- There is no missing values in dataset.","bc1da4ac":"### 5. XGBoost","a6556301":"- all features have outliers","245af594":"#### Horizontal_Distance_To_Hydrology Vs Vertical_Distance_To_Hydrology","e1517e76":"#### Hillshade_9am vs Hillshade_3pm","1631a161":"### 2. Decision Tree","5737b70e":"<h2 style=\"color:blue\" align=\"left\"> 7. Model building and Evaluation <\/h2>","24486460":"### classification Report","29405b46":"- skewness is close to zero means normally distributed.","53a01e03":"#### 1. Find Unwanted Columns\n\n- There is no unwanted column present in given dataset to remove.\n\n     EX: ID","939f62fd":"### 3. Random Forest","c387e4e2":"### 1. Logistic Regression","13642785":"### Chi-Square Test-\n- The test is applied when you have two categorical variables from a single population. It is used to determine whether there is a significant association between the two variables.","3d3e7c75":"<h2 style=\"color:blue\" align=\"left\"> 2. Load data <\/h2>","c333f7c0":"### Checking for Numerical and Categorical features","d26350e0":"### 1. Line Plot","f75e2400":"#### 3. Find Features with one value","1f801f4d":"### 3. Multivariate Analysis\n\n- 1. Pair Plot","33d79f12":"#### Histogram","05ba12b6":"### c. Checking Skewness for feature \"Horizontal_Distance_To_Roadways\"","57e9114e":"####  Aspect Vs Hillshade_9am","219a605e":"#### Correlation Heat Map","0656a384":"#### Aspect Vs Cover_Type","fb8dc8f5":"- Our dataset features consists of only integers","1042d08e":"#### 11. Relation between Continous numerical Features and Labels","b055a65f":"### f. Checking Skewness for feature \"Horizontal_Distance_To_Fire_Points\"","fa8f7436":"### b. Checking Skewness for feature \"Vertical_Distance_To_Hydrology\"","22f6ceea":"#### Logistic Regression\n- Accuracy on Test Data Set with Logistic Regression : 64%\n\n#### Decision Tree\n- Accuracy on Test Data Set with DecisionTree Regression : 78%\n\n#### Random Forest\n- Accuracy on Test Data Set with Random Forest Classifier : 86%\n\n#### KNN\n- Accuracy on Test Data Set with K Nearest Neighbours : 64%\n\n#### XGBoost Model\n- Accuracy on Test Data Set with XGBoost Classifier : 87%\n\n#### Naive Bayes Classifier\n- Accuracy on Test Data Set with Naive Bayes Classifier : 65%\n\n\n- So far **GBoost Model** proved to be the best performing model with **87% accuracy**.","af7f0958":"#### Slope Vs Cover_Type","ff5d3390":"#### 14. Descriptive statistics","a3275e0f":"#### 12. Find Outliers in numerical features","9f7b19ff":"### e. Checking Skewness for feature \"Hillshade_Noon\"","0437123d":"- Skewness tells us about the symmetry in a distribution.\n\n* If the **skewness** is **between -0.5 to +0.5** then we can say data is **fairly symmetrical**.\n  \n* If the **skewness** is **between -1 to -0.5 or 0.5 to 1** then data is **moderately skewed**.\n  \n* If the **skewness** is **less than -1 and greater than +1** then our data is **heavily skewed**.","9f7c5b3e":"#### 8. Find Discrete Numerical Features","4a2deb72":"#### 2. Find Missing Values\n\n- Checking missing values by below methods:\n\n     1. df.isnull().sum()\n        - It returns null values for each column\n          \n     2. isnull().any()\n        - It returns True if column have NULL Values\n        - It returns False if column don't have NULL Values\n          \n     3. Heatmap()\n        - Missing value representation using heatmap.\n          \n     4. Percentage of Missing values","22f9c7ba":"- The **skewness** is **between -0.5 to +0.5** then we can say data is **fairly symmetrical**.","5ddf7f6c":"<h2 style=\"color:blue\" align=\"left\"> 1. Import necessary Libraries <\/h2>","0f837dce":"#### 9. Find Continous Numerical Features","057ab16b":"### Drop Features have more missing values","1b483e8c":"- We can see a positive correlation between Elevation and Distance to Roadways.","ce303780":"### Confusion Matrix Heatmap","2cfeba05":"- Aspect plot contains couple of normal distribution for several classes\n- Hillshade 9am and 12pm displays left skew (long tail towards left)\n- Wilderness_Area3 gives no class distinction.\n- Soil_Type, 1,5,8,9,12,14,18-22, 25-30 and 35-40 offer class distinction as values are not present for many classes","fe68a54a":"<h2 style=\"color:blue\" align=\"left\"> 3. EDA (Exploratory Data Analysis) <\/h2>\n\n- EDA is a way of **Visualizing, Summarizing and interpreting** the information that is **hidden in rows and column** format.\n\n### Steps involved in EDA:\n1. Find Unwanted Columns\n- Find Missing Values\n- Find Features with one value\n- Explore the Categorical Features\n- Find Categorical Feature Distribution\n- Relationship between Categorical Features and Label\n- Explore the Numerical Features\n- Find Discrete Numerical Features\n- Relation between Discrete numerical Features and Labels\n- Find Continous Numerical Features\n- Distribution of Continous Numerical Features\n- Relation between Continous numerical Features and Labels\n- Find Outliers in numerical features\n- Explore the Correlation between numerical features","2790a69c":"- \"Aspect\" & \"Hillshade_3pm\" are in between -0.5 and +0.5. Fairly skewed.\n- In our above data,\n    1. Slope\n    2. Horizontal_Distance_To_Hydrology\n    3. Vertical_Distance_To_Hydrology\n\n- Are highly positively, right skewed.\n\n    1. Elevation\n    2. Hillshade_9am\n    3. Hillshade_Noon\n    \n- Are highly negitively, left skewed.","0c0930df":"- The **skewness** is **between -0.5 to +0.5** then we can say data is **fairly symmetrical**.","6d2ad24d":"- Since there is no Categorical Features, we can skip steps from 4 to 6.","f8924c5c":"<h3 style=\"color:green\" align=\"left\"> Hypothesis Testing <\/h3>","460d7f60":"#### Selected HeatMap","c1dc1c60":"<h2 style=\"color:blue\" align=\"left\"> 7. Check & Reduce Skewness <\/h2>","b1b959db":"#### 13. Explore the Correlation between numerical features","8828dba4":"### 3. Bivariate Analysis\n\n- **Bivariate Analysis** : data involves **two different variables**.\n\n     1. Bar Charts\n     2. Scatter Plots\n     3. FacetGrid\n     \n\n-  There are **three** types of bivariate analysis\n\n     1. Numerical & Numerical\n     2. Categorical & Categorical\n     3. Numerical & Categorical","3dfb5e01":"#### Pair Plot between 'SalePrice' and correlated variables","6964e483":"### Submission","45897cf5":"#### Correlation requires continuous data. Hence, ignore Wilderness_Area and Soil_Type as they are binary values","a476bb9d":"- The **skewness** is **between -0.5 to +0.5** then we can say data is **fairly symmetrical**.","33d229cf":"- The **skewness** is **between -0.5 to +0.5** then we can say data is **fairly symmetrical**.","8779b67b":"- it seems all continuous features are not normally distributed\n\n- **Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Horizontal_Distance_To_Fire_Points** are **right skewed**\n\n- **Elevation, Hillshade_9am, Hillshade_3pm** is **left skewed**.","6516ec77":"### 6. Naive Bayes Classifier","178a945f":"### Scaling\n- Standardizing the data i.e. to rescale the features to have a **mean of zero** and **standard deviation of 1.**","65ffd4b8":"### Score Summary :","6d7aa861":"#### Slope Vs Hillshade_Noon","c5e0c9b2":"#### Elevation Vs HD Roadways","361d3960":"<h2 style=\"color:green\" align=\"left\"> 5. Data Visualization <\/h2>\n\n- Used below **visualisation libraries**\n\n     1. Matplotlib\n     2. Seaborn (statistical data visualization)\n     \n### 1. Categorical\n\n- Categorical data :\n\n     1. Numerical Summaries\n     2. Histograms\n     3. Pie Charts\n\n\n### 2. Univariate Analysis\n\n- Univariate Analysis : data consists of **only one variable (only x value)**.\n\n     1. Line Plots \/ Bar Charts\n     2. Histograms\n     3. Box Plots \n     4. Count Plots\n     5. Descriptive Statistics techniques\n     6. Violin Plot","097bc84e":"### 4. KNN (K Nearest Neighbors)","67f400aa":"### g. Checking Skewness for feature \"Slope\"","365e94e9":"- The **skewness** is **between -0.5 to +0.5** then we can say data is **fairly symmetrical**.","0395aabf":"### d. Checking Skewness for feature \"Hillshade_9am\"","38812648":"### 2. Scatter Plot","fa020a13":"#### Aspect  Vs Hillshade_3pm","dfd37d0a":"### a. Checking Skewness for feature \"Horizontal_Distance_To_Hydrology\"","7019dee1":"#### Hillshade_Noon Vs Hillshade_3pm","dc40cf74":"### Confusion Matrix","3f26c8bf":"#### 10. Distribution of Continous Numerical Features","f5d96219":"- Count is 581012 for each column, so no data point is missing.\n- Wilderness_Area and Soil_Type are one hot encoded. Hence, they could be converted back for some analysis.\n- Scales are not the same for all. Hence, rescaling and standardisation may be necessary for some algos.","fcbe557f":"#### 7. Explore the Numerical Features"}}