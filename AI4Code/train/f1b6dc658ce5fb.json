{"cell_type":{"40c03f66":"code","afa73681":"code","20a0944b":"code","47983c47":"code","f38a7c2d":"code","e0e55309":"code","8b49b148":"code","58c6add0":"code","cb07a04d":"code","4ef7466e":"code","356040fc":"code","27e1f2a7":"code","86c689f9":"code","e1662d27":"code","76d7a2fe":"code","fbc5d2b5":"code","b365f35a":"code","e8732764":"code","c84be8b2":"code","498b56cb":"code","ab9bf57a":"code","2445f190":"code","d18ce88e":"code","a27a7041":"code","a04151ff":"code","e980d288":"code","a199ae62":"code","7d176674":"code","dcebcdfc":"code","43be28e9":"code","8b027fec":"code","af7a4cff":"code","95f030db":"code","1af36641":"code","c66bf806":"code","9f0ede6d":"code","e434b163":"code","1f4fea21":"code","b98498ff":"code","e658955b":"code","c57f51ee":"code","046118c1":"code","f22c4570":"code","0ff52824":"code","20aaf07b":"code","d773e17c":"code","483c658b":"code","bf4e8c39":"code","238f0268":"code","8f3d47cc":"code","45265c37":"code","f5de2d48":"code","4d066570":"code","782f064f":"code","eeee45a7":"code","9c5a6dc1":"code","06f5ceb9":"code","d18b5b8f":"markdown","b4ea02bf":"markdown","57b2b2af":"markdown","70c12b34":"markdown","08c3bf80":"markdown","438a4d3a":"markdown","75c3e10b":"markdown","3d2bd26e":"markdown","b140335e":"markdown","aebd85a0":"markdown","229dc0fa":"markdown","5831aa44":"markdown","c4537418":"markdown","eb2d9bc6":"markdown","789b141b":"markdown","60af82dd":"markdown","1e36373a":"markdown","abf2b7ff":"markdown","57ecd703":"markdown","ea548d58":"markdown","03bd7b50":"markdown","192bc4e6":"markdown"},"source":{"40c03f66":"import sys\nsys.path.append('..\/input\/iterativestratification')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nimport copy\nimport gc\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA , TruncatedSVD\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nos.listdir('..\/input\/lish-moa')\n\npd.set_option('max_columns', 2000)","afa73681":"n_comp_GENES = 450\nn_comp_CELLS = 2\nVarianceThreshold_for_FS = 0.8\nDropout_Model = 0.25\nprint('n_comp_GENES', n_comp_GENES, 'n_comp_CELLS', n_comp_CELLS, 'total', n_comp_GENES + n_comp_CELLS)","20a0944b":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","47983c47":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","f38a7c2d":"# RankGauss - transform to Gauss\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=1000,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","e0e55309":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","8b49b148":"len(GENES)","58c6add0":"# GENES\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (TruncatedSVD(n_components=n_comp_GENES, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_GENES)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","cb07a04d":"len(CELLS)","4ef7466e":"# CELLS\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (TruncatedSVD(n_components=n_comp_CELLS, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_CELLS)])\n\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","356040fc":"train_features.shape","27e1f2a7":"train_features.head(5)","86c689f9":"var_thresh = VarianceThreshold(VarianceThreshold_for_FS)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","e1662d27":"train_features.head(5)","76d7a2fe":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","fbc5d2b5":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","b365f35a":"train.head(5)","e8732764":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","c84be8b2":"tail = train[target_cols].sum() ","498b56cb":"tail_cols = tail[tail<20]","ab9bf57a":"tail_cols = tail_cols.index","2445f190":"import numpy as np\nimport pandas as pd\nimport random\n\nfrom sklearn.neighbors import NearestNeighbors\n\n\ndef get_tail_label(df):\n    \"\"\"\n    fixed the columns of interest here  not the best solution but working on it.\n    \n    Give tail label colums of the given target dataframe\n    \n    args\n    df: pandas.DataFrame, target label df whose tail label has to identified\n    \n    return\n    tail_label: list, a list containing column name of all the tail label\n    \"\"\"\n    columns = df.columns\n    n = len(columns)\n    irpl = np.zeros(n)\n    for column in range(n):\n        irpl[column] = df[columns[column]].value_counts()[1]\n    irpl = max(irpl)\/irpl\n    mir = np.average(irpl)\n    tail_label = []\n    for i in range(n):\n        if irpl[i] > mir:\n            tail_label.append(columns[i])\n    return tail_label # maybe hardcode this or improve it as it is not producing the required labels\n\ndef get_index(df):\n  \"\"\"\n  give the index of all tail_label rows\n  args\n  df: pandas.DataFrame, target label df from which index for tail label has to identified\n    \n  return\n  index: list, a list containing index number of all the tail label\n  \"\"\"\n  tail_labels = get_tail_label(df)\n  index = set()\n  for tail_label in tail_labels:\n    sub_index = set(df[df[tail_label]==1].index)\n    index = index.union(sub_index)\n  return list(index)\n\ndef get_minority_instace(X, y):\n    \"\"\"\n    Give minority dataframe containing all the tail labels\n    \n    args\n    X: pandas.DataFrame, the feature vector dataframe\n    y: pandas.DataFrame, the target vector dataframe\n    \n    return\n    X_sub: pandas.DataFrame, the feature vector minority dataframe\n    y_sub: pandas.DataFrame, the target vector minority dataframe\n    \"\"\"\n    index = get_index(y)\n    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n    return X_sub, y_sub\n\ndef nearest_neighbour(X):\n    \"\"\"\n    Give index of 5 nearest neighbor of all the instance\n    \n    args\n    X: np.array, array whose nearest neighbor has to find\n    \n    return\n    indices: list of list, index of 5 NN of each element in X\n    \"\"\"\n    nbs=NearestNeighbors(n_neighbors=120 ,metric='euclidean',algorithm='kd_tree').fit(X) # should be 206 here\n    euclidean,indices= nbs.kneighbors(X)\n    return indices\n\ndef MLSMOTE(X,y, n_sample):\n    \"\"\"\n    Give the augmented data using MLSMOTE algorithm\n    \n    args\n    X: pandas.DataFrame, input vector DataFrame\n    y: pandas.DataFrame, feature vector dataframe\n    n_sample: int, number of newly generated sample\n    \n    return\n    new_X: pandas.DataFrame, augmented feature vector data\n    target: pandas.DataFrame, augmented target vector data\n    \"\"\"\n    indices2 = nearest_neighbour(X)\n    n = len(indices2)\n    new_X = np.zeros((n_sample, X.shape[1]))\n    target = np.zeros((n_sample, y.shape[1]))\n    for i in range(n_sample):\n        reference = random.randint(0,n-1)\n        neighbour = random.choice(indices2[reference,1:])\n        all_point = indices2[reference]\n        nn_df = y[y.index.isin(all_point)]\n        ser = nn_df.sum(axis = 0, skipna = True)\n        target[i] = np.array([1 if val>2 else 0 for val in ser])\n        ratio = random.random()\n        gap = X.loc[reference,:] - X.loc[neighbour,:]\n        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n    new_X = pd.DataFrame(new_X, columns=X.columns)\n    target = pd.DataFrame(target, columns=y.columns)\n    new_X = pd.concat([X, new_X], axis=0)\n    target = pd.concat([y, target], axis=0)\n    return new_X, target\n\n","d18ce88e":"train_s = train.drop(['sig_id'] , axis = 1)\n\ntrain_s = pd.get_dummies(train_s, columns=['cp_time','cp_dose'])\n\n\nX= train_s.drop(target_cols , axis = 1) \ny = train_s[target_cols]#Creating a Dataframe\nX_sub, y_sub = get_minority_instace(X, y)   #Getting minority instance of that datframe\nX_res,y_res =MLSMOTE(X_sub, y_sub, 500)     #Applying MLSMOTE to augment the dataframe\n","a27a7041":"X_res.reset_index(drop=True , inplace= True)\ny_res.reset_index(drop=True , inplace= True)","a04151ff":"train = train.drop(['sig_id'] , axis =1)\ntrain = pd.get_dummies(train, columns=['cp_time','cp_dose'])","e980d288":"train_res = pd.concat([X_res, y_res] , axis=1 )","a199ae62":"train_res","7d176674":"# original train data set\ntarget_f =  train[target_cols]\ntop_targets = pd.Series(target_f.sum()).sort_values(ascending=False)[:5]\nbottom_targets = pd.Series(target_f.sum()).sort_values()[:5]\nfig, axs = plt.subplots(figsize=(9,9) , nrows=2)\nsns.barplot(top_targets.values , top_targets.index , ax = axs[0] ).set(title = \"Top five targets\")\nsns.barplot(bottom_targets.values , bottom_targets.index, ax = axs[1] ).set(title = \"bottom five targets\")\nplt.show()","dcebcdfc":"# only tail_cols\ntarget_f =  train_res[get_tail_label(y)]\ntop_targets = pd.Series(target_f.sum()).sort_values(ascending=False)[:5]\nbottom_targets = pd.Series(target_f.sum()).sort_values()[:5]\nfig, axs = plt.subplots(figsize=(9,9) , nrows=2)\nsns.barplot(top_targets.values , top_targets.index , ax = axs[0] ).set(title = \"Top five targets\")\nsns.barplot(bottom_targets.values , bottom_targets.index, ax = axs[1] ).set(title = \"bottom five targets\")\nplt.show()","43be28e9":"#definately an improvent for class imbalace \n# still need to fix this for bottom 2 instances though any ideas here would be great","8b027fec":"train_res_f = train.append(train_res)","af7a4cff":"#final_df\ntarget_f =  train_res_f[target_cols]\ntop_targets = pd.Series(target_f.sum()).sort_values(ascending=False)[:5]\nbottom_targets = pd.Series(target_f.sum()).sort_values()[:5]\nfig, axs = plt.subplots(figsize=(9,9) , nrows=2)\nsns.barplot(top_targets.values , top_targets.index , ax = axs[0] ).set(title = \"Top five targets\")\nsns.barplot(bottom_targets.values , bottom_targets.index, ax = axs[1] ).set(title = \"bottom five targets\")\nplt.show()","95f030db":"train_res = train.append(train_res)\ntrain_res = train_res.reset_index(drop=True)","1af36641":"train_res","c66bf806":"folds = train_res.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train_res.drop(target_cols , axis=1), y=train_res[target_cols])):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","9f0ede6d":"train = train_res.copy()\nprint(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","e434b163":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","1f4fea21":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","b98498ff":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","e658955b":"import tqdm\ndef process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    \n    ###### maybe use the extrcated features\n#     for stats in tqdm.tqdm(['sum', 'mean', 'std', 'kurt', 'skew']):\n#         data['g_'+stats] = getattr(data[GENES], stats)(axis=1)\n#         data['c_'+stats] = getattr(data[CELLS], stats)(axis=1)\n#         data['gc_'+stats] = getattr(data[GENES+CELLS], stats)(axis=1)\n    return data","c57f51ee":"feature_cols = [c for c in folds.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","046118c1":"folds.head()","f22c4570":"test_ = process_data(test)","0ff52824":"test_[feature_cols].values","20aaf07b":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1500","d773e17c":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(Dropout_Model)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(Dropout_Model)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","483c658b":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = folds\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n","bf4e8c39":"# feature_cols","238f0268":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","8f3d47cc":"# Averaging on multiple SEEDS\n\nSEED = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n","45265c37":"train_targets_scored","f5de2d48":"len(target_cols)\n","4d066570":"# valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n# y_true = train_targets_scored[target_cols].values\n# y_pred = valid_results[target_cols].values\n\n# score = 0\n# for i in range(len(target_cols)):\n#     score_ = log_loss(y_true[:, i], y_pred[:, i])\n#     score += score_ \/ target.shape[1]\n    \n# print(\"CV log_loss: \", score)    ","782f064f":"# valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n# y_true = train_targets_scored[target_cols].values\n# y_pred = valid_results[target_cols].values\n\n# score = 0\n# for i in range(len(target_cols)):\n#     score_ = log_loss(y_true[:, i], y_pred[:, i])\n#     score += score_ \/ target.shape[1]\n    \n# print(\"CV log_loss: \", score)    ","eeee45a7":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","9c5a6dc1":"sub.shape","06f5ceb9":"sub","d18b5b8f":"# Acknowledgements\n\n* [MoA: Pytorch-RankGauss-PCA-NN upgrade & 3D visual](https:\/\/www.kaggle.com\/vbmokin\/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual) Thanks for the great notebook. \n* The only value addition here is MLSMOTE the rest is picked up from the above \ud83d\udc46 notebook with a few adjustments.\n* Do leave an upvote!!!!!!!\n* MLSMOTE does seem to have potential but needs some more work IMO. Any leads here would be helpful feel free to leave a comment.\n","b4ea02bf":"### 4.6 Dataset Classes\n\n","57b2b2af":"### It is recommended:\n* **n_comp_GENES** smaller, \n* **n_comp_CELLS** more,\n* **VarianceThreshold_for_FS** more, so that **train_features** is less.","70c12b34":"# SAMPLING MOA USING MLSMOTE.","08c3bf80":"### 4.5 CV folds\n","438a4d3a":"# Kindly leave an upvote Thanks \ud83d\ude01 \ud83d\ude4f","75c3e10b":"### 2.2 Previous commits <a class=\"anchor\" id=\"2.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","3d2bd26e":"### 4.4 FS by Variance Encoding<a class=\"anchor\" id=\"4.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","b140335e":"There is definately a huge class imbalance in this problem. But I didnt see any notebooks which handles this or any sampling techniques so the aim of this notebook is to use MLSMOTE: a synthetic data genration process for multilabel classification problems.","aebd85a0":"### 4.7 Smoothing\n\n","229dc0fa":"best LB before MLSMOTE - 0.01841\nbest LB afer MLSMOTE - 0.01844\n\n","5831aa44":"## 4. FE & Data Preprocessing <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","c4537418":"### 4.3 PCA features<a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","eb2d9bc6":"## 3. Download data<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","789b141b":"## 5. Modeling\n","60af82dd":"[Go to Top](#0)","1e36373a":"### 4.1 RankGauss\n\n\n","abf2b7ff":"### 4.2 Seed<a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","57ecd703":"<b> Exploring MLSMOTE as a potential sampling technique to scale up the samples of tailend classes. There are many labels with few ~10 labels the aim is to scale them up to handle the class imbalance. Another way to solve it could be to playaround with wieghts. ","ea548d58":"## 6. Prediction & Submission \n\n","03bd7b50":"# MLSMOTE","192bc4e6":"### 4.8 Preprocessing\n\n"}}