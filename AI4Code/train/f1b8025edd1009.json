{"cell_type":{"60f0836a":"code","d80d2953":"code","f4a539bd":"code","7cd57de2":"code","5492d821":"code","b6743d0a":"code","6add07bf":"code","f4fec80a":"code","a29bb634":"code","7c1a4b39":"code","f63a3977":"code","b05d30fd":"code","2130e6b7":"code","765131e0":"code","1086e564":"code","a1b6ab7e":"code","b00bb4c1":"code","272e29ab":"code","1773388d":"code","dccc1bc0":"code","ed1238b6":"code","dac70048":"code","79e27e7b":"code","b880fd27":"code","f2e5a91a":"code","8c259fa9":"code","c79b0c68":"code","3a6ce868":"code","151b7146":"code","6476a3d8":"code","6a9b4762":"code","6a873523":"code","1d88548a":"code","7e3319c3":"code","219c21e9":"markdown","3f52aacd":"markdown","fd8e29dc":"markdown","11852c0c":"markdown","651042c0":"markdown","57eb9aa2":"markdown","60270727":"markdown","23f2383e":"markdown","95fd28be":"markdown","1398948c":"markdown","8f825497":"markdown","7f954f08":"markdown","b8b8a8be":"markdown","cf191a23":"markdown","9d97ba40":"markdown","9f5c30a8":"markdown","52fe3220":"markdown","734575cc":"markdown","3d38384c":"markdown","9937629b":"markdown","ffdfd663":"markdown","effb04d0":"markdown","410969cb":"markdown","c8cd960c":"markdown","a65d5498":"markdown","41f363ec":"markdown","108fa4d9":"markdown","0e6e72e1":"markdown","ee5fefb1":"markdown","f3c945ad":"markdown","bc5af099":"markdown","c3f8201e":"markdown","36475622":"markdown","09b021c3":"markdown","58a4365f":"markdown","926f5306":"markdown","72e7a3af":"markdown","5a2ccbc3":"markdown","4a23be53":"markdown","8d91af9e":"markdown","e9956593":"markdown","9d0b6338":"markdown","c1ea80b5":"markdown","06c5272c":"markdown","50fa6985":"markdown","8da95b4a":"markdown","b2a5716b":"markdown","5f665454":"markdown","ca66dd24":"markdown","29770abd":"markdown","0af97e5f":"markdown","cae8171e":"markdown"},"source":{"60f0836a":"!pip install Lifetimes\n!pip install scikit-learn-extra","d80d2953":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport datetime\nfrom datetime import timedelta\nfrom datetime import date\nimport squarify\nfrom lifetimes.plotting import *\nfrom lifetimes.utils import *\nfrom lifetimes import BetaGeoFitter\nfrom lifetimes import GammaGammaFitter\nfrom scipy.stats import gamma, beta\nfrom sklearn_extra.cluster import KMedoids\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata_folder = \"\/kaggle\/input\/arketing-campaign\/\"","f4a539bd":"dataset=pd.read_csv(data_folder+'marketing_campaign.csv',header=0,sep=';') \ndataset.head(10)","7cd57de2":"dataset['Spending']=dataset['MntWines']+dataset['MntFruits']+dataset['MntMeatProducts']+dataset['MntFishProducts']+dataset['MntSweetProducts']+dataset['MntGoldProds']\ndataset['Transactions']=dataset['NumWebPurchases']+dataset['NumCatalogPurchases']+dataset['NumStorePurchases']\ndataset=dataset[['ID','Spending','Transactions','Recency','Dt_Customer']]\ndataset = dataset[dataset['Transactions'] > 1] #We keep customers with repeated purchases, implying number of transactions must be at least 2\ndataset = dataset[dataset['Spending'] > 0]","5492d821":"print(\"Summary of the last 2 years spending\")\nprint(\"Number of transactions: \", dataset['Transactions'].sum())\nprint(\"Total sales: \",dataset['Spending'].sum())\nprint(\"Number of customers:\", dataset['ID'].nunique())","b6743d0a":"recency_df = dataset[['ID','Recency']]\nrecency_df","6add07bf":"frequency_df = dataset[['ID','Transactions']]\ntemp_df = recency_df.merge(frequency_df,on='ID')\nfrequency_df","f4fec80a":"monetary_df = dataset[['ID','Spending']]\nmonetary_df","a29bb634":"tx_user  = temp_df.merge(monetary_df,on='ID')\ntx_user.columns = ['ID','Recency','Frequency','Monetary']\ntx_user","7c1a4b39":"#Select number of clusters for each attributes\n#Step 1 : Clusters for Recency\nsse={}\ntx_recency = tx_user[['Recency']]\nfor k in range(1, 10):\n    kmedoids = KMedoids(n_clusters=k, random_state=0, max_iter=1000,init='k-medoids++',metric='euclidean').fit(tx_recency)\n    tx_recency[\"clusters\"] = kmedoids.labels_\n    sse[k] = kmedoids.inertia_\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.show()","f63a3977":"kmedoids = KMedoids(n_clusters=5, random_state=0, max_iter=1000,init='k-medoids++',metric='euclidean').fit(tx_recency)\ntx_user['RecencyCluster'] = kmedoids.predict(tx_recency)\n\n#function for ordering cluster numbers\ndef order_cluster(cluster_field_name, target_field_name,df,ascending):\n    new_cluster_field_name = 'new_' + cluster_field_name\n    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n    df_new['index'] = df_new.index\n    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n    df_final = df_final.drop([cluster_field_name],axis=1)\n    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n    return df_final\n\ntx_user = order_cluster('RecencyCluster', 'Recency',tx_user,False)\n#see details of each cluster\ntx_user.groupby('RecencyCluster')['Recency'].describe()","b05d30fd":"tx_frequency = tx_user[['Frequency']]\n\nkmedoids = KMedoids(n_clusters=5, random_state=0, max_iter=1000,init='k-medoids++',metric='euclidean').fit(tx_frequency)\ntx_user['FrequencyCluster'] = kmedoids.predict(tx_frequency)\n\n#order the frequency cluster\ntx_user = order_cluster('FrequencyCluster', 'Frequency',tx_user,True)\n\n#see details of each cluster\ntx_user.groupby('FrequencyCluster')['Frequency'].describe()","2130e6b7":"tx_monetary = tx_user[['Monetary']]\n\nkmedoids = KMedoids(n_clusters=5, random_state=0, max_iter=1000,init='k-medoids++',metric='euclidean').fit(tx_monetary)\ntx_user['MonetaryCluster'] = kmedoids.predict(tx_monetary)\n\n#order the cluster numbers\ntx_user = order_cluster('MonetaryCluster', 'Monetary',tx_user,True)\n\n#show details of the dataframe\ntx_user.groupby('MonetaryCluster')['Monetary'].describe()","765131e0":"segt_map = {\n    r'30': 'Promising',\n    r'23': 'Loyal customers',\n    r'24': 'Loyal customers',\n    r'33': 'Loyal customers',\n    r'34': 'Loyal customers',\n    r'43': 'Loyal customers',\n    r'32': 'Potential loyalist',\n    r'31': 'Potential loyalist',\n    r'42': 'Potential loyalist',\n    r'41': 'Potential loyalist',\n    r'21': 'Need attention',\n    r'22': 'Need attention',\n    r'12': 'Need attention',\n    r'11': 'Need attention',\n    r'40': 'New customers',\n    r'20': 'About to sleep',\n    r'14': 'Cant loose them',\n    r'04': 'Cant loose them',\n    r'10': 'Lost',\n    r'00': 'Lost',\n    r'01': 'Lost',\n    r'02': 'At risk',\n    r'03': 'At risk',\n    r'13': 'At risk',\n    r'44': 'Champions',\n}\n\ntx_user['Segment'] = tx_user['RecencyCluster'].map(str) + tx_user['FrequencyCluster'].map(str)\ntx_user['Segment'] = tx_user['Segment'].replace(segt_map, regex=True)\ntx_user.head()","1086e564":"# count the number of customers in each segment\nsegments_counts = tx_user['Segment'].value_counts().sort_values(ascending=True)\n\nfig, ax = plt.subplots()\n\nbars = ax.barh(range(len(segments_counts)),\n              segments_counts,\n              color='silver')\nax.set_frame_on(False)\nax.tick_params(left=False,\n               bottom=False,\n               labelbottom=False)\nax.set_yticks(range(len(segments_counts)))\nax.set_yticklabels(segments_counts.index)\n\nfor i, bar in enumerate(bars):\n        value = bar.get_width()\n        if segments_counts.index[i] in ['Champions', 'Loyal customers']:\n            bar.set_color('firebrick')\n        ax.text(value,\n                bar.get_y() + bar.get_height()\/2,\n                '{:,} ({:}%)'.format(int(value),\n                                   int(value*100\/segments_counts.sum())),\n                va='center',\n                ha='left'\n               )\n\nplt.show()","a1b6ab7e":"# Calculate average values for each RFM segment, and return a size of each segment \ntx_user_viz = tx_user.groupby('Segment').agg({\n    'Recency': 'mean',\n    'Frequency': 'mean',\n    'Monetary': ['mean', 'count'],\n}).round(1)\n# Print the aggregated dataset\ntx_user_viz","b00bb4c1":"tx_user_viz.columns = ['Recencymean','Frequencymean', 'Monetarymean','Count']\nfig = plt.gcf()\nax = fig.add_subplot()\nfig.set_size_inches(16, 9)\nsquarify.plot(sizes=tx_user_viz['Count'], \n              label=['Lost',\n                     'About to Sleep',\n                     'Cant loose them',\n                     'Promising',\n                     'New Customers',\n                     'Need Attention',\n                     'Potential Loyalists',\n                     'At risk',\n                     'Loyal Customers',\n                     'Champions',\n                     ], alpha=.6 )\nplt.title(\"RFM Segments\",fontsize=22,fontweight=\"bold\")\nax.set_xlabel('Recency',fontsize=12)\nax.set_ylabel('Frequency',fontsize=12)\nplt.axis('on')\nplt.show()","272e29ab":"tx_user  = tx_user.merge(dataset[['ID','Dt_Customer']],on='ID')\ntx_user.head(10)","1773388d":"last_date = date(2014,10, 4)\ntx_user['Age']=pd.to_datetime(tx_user['Dt_Customer'], dayfirst=True,format = '%Y-%m-%d')\ntx_user['Age'] = pd.to_numeric(tx_user['Age'].dt.date.apply(lambda x: (last_date - x)).dt.days, downcast='integer')\n\ntx_user['Recency']=(tx_user['Age']-tx_user['Recency'])\n\ntx_user['Monetary_value']=tx_user['Monetary']\/tx_user['Frequency']\ntx_user['Frequency']=tx_user['Frequency']-1","dccc1bc0":"tx_user=tx_user[['ID','Frequency','Recency','Age','Monetary_value','Segment']]\ntx_user","ed1238b6":"bgf = BetaGeoFitter(penalizer_coef=0.000000005)\nbgf.fit(tx_user['Frequency'], tx_user['Recency'], tx_user['Age'])","dac70048":"# plot the estimated gamma distribution of \u03bb (customers' propensities to purchase)\nplot_transaction_rate_heterogeneity(bgf);","79e27e7b":"bgf.summary","b880fd27":"# visualize our frequency\/recency matrix\nfig = plt.figure(figsize=(12,8))\nplot_frequency_recency_matrix(bgf, T = 30);","f2e5a91a":"fig = plt.figure(figsize=(12,8))\nplot_probability_alive_matrix(bgf);","8c259fa9":"t = 30 # to calculate the number of expected repeat purchases over the next 30 days\ntx_user['Predicted_purchases'] = bgf.conditional_expected_number_of_purchases_up_to_time(t, tx_user['Frequency'], tx_user['Recency'], tx_user['Age'])\ntx_user.sort_values(by='Predicted_purchases').tail(5)","c79b0c68":"tx_user['p_alive'] = bgf.conditional_probability_alive(tx_user['Frequency'], tx_user['Recency'], tx_user['Age'])\ntx_user.sort_values(by='Predicted_purchases').tail(5)","3a6ce868":"sns.distplot(tx_user['p_alive']);","151b7146":"# We fit the Gamma-Gamma model to our data\nggf = GammaGammaFitter(penalizer_coef=0.00005)\nggf.fit(frequency = tx_user['Frequency'], monetary_value = tx_user['Monetary_value'])","6476a3d8":"tx_user['predicted_Sales'] = ggf.conditional_expected_average_profit(tx_user['Frequency'], tx_user['Monetary_value'])\ntx_user.head()","6a9b4762":"print(f\"Expected Average sales: {tx_user['predicted_Sales'].mean()}\")\nprint(f\"Actual Average sales: {tx_user['Monetary_value'].mean()}\")","6a873523":"tx_user['LTV'] = ggf.customer_lifetime_value(bgf,tx_user['Frequency'], tx_user['Recency'], tx_user['Age'], tx_user['Monetary_value'],\n    time = 12,freq='D',discount_rate = 0.01)\ntx_user.head()","1d88548a":"pd.options.display.float_format = \"{:.2f}\".format\nbest_projected_cust_LTV = tx_user.sort_values('LTV').tail(10)\nbest_projected_cust_LTV","7e3319c3":"pd.options.display.float_format = \"{:.0f}\".format\n# Calculate average values for each RFM segment\ntx_user_clv = tx_user.groupby('Segment').agg({'LTV': 'mean',}).sort_values('LTV',ascending=False)\ntx_user_clv","219c21e9":"We can now calculate the average Long Term Value of each RFM segment we define earlier for the next 12 months","3f52aacd":"The dataset already have all the variables needed to create the RFM metrics. We just need to prepare the data.\n\nWe wrill create two variables :\n\n>- Variable __*Spending*__ as the sum of the amount spent on the 6 product categories.\n>- Variable __*Transactions*__ as the total number of purchases made by the customer.\n\n\nWe will remove the unused variables for this analysis and keep only the customers who made more than 1 repeat purchase in order to calculate the Customer Lifetime Value.","fd8e29dc":"### A. Recency calculation <a class=\"anchor\" id=\"section_3_1\"><\/a>","11852c0c":"### Table of Contents\n\n* [1. Theoretical concepts](#section_1)\n    * [RFM segmentation](#section_1_1)\n    * [Customer Lifetime Value](#section_1_2) \n    ___\n* [2. Data Preprocessing](#section_2)\n    * [Feature engineering](#section_2_1)\n    * [Statistical summary](#section_2_2)\n    \n    ___\n* [3. RFM Segmentation](#section_3)\n    * [Recency calculation](#section_3_1)\n    * [Frequency calculation](#section_3_2)\n    * [Monetary calculation](#section_3_3)\n    * [Segment creation](#section_3_4)\n    \n    ___\n* [4. CLV modeling](#section_4)\n    * [Deriving RFM Metrics](#section_4_1)\n    * [Retention Model fitting](#section_4_2)\n    * [Value Model fitting](#section_4_3)\n    * [CLV estimates](#section_4_4)\n        \n    ___\n* [5. Conclusion](#section_5)\n    \n    ___","651042c0":"### D. CLV estimates <a class=\"anchor\" id=\"section_4_4\"><\/a>","57eb9aa2":"### A. RFM Segmentation <a class=\"anchor\" id=\"section_1_1\"><\/a>","60270727":"#### Metrics analysis per segment ","23f2383e":"We can plot our top 10 customers based on LTV.  \nWe can see that these 10 customers all belong to the Champions\/Loyal customers\/Potential loyalist segments","95fd28be":"The second interesing heatmap is the probability of a customer of still being alive.\n>- If a customer has a high number of transactions (frequency) and the time between their first and last transaction is hight (recency), his\/her probability of still being alive is high. (bottom right)  \n>- If a customer has a small number of transactions but the recency is low, then his\/her probability of still being alive is also high (top left)","1398948c":"### B. Statistical Summary <a class=\"anchor\" id=\"section_2_2\"><\/a>","8f825497":"#### Segment visualization","7f954f08":"### B. Retention Model fitting <a class=\"anchor\" id=\"section_4_2\"><\/a>","b8b8a8be":"- **CLV definition**","cf191a23":"# 2. Data Preprocessing <a class=\"anchor\" id=\"section_2\"><\/a>","9d97ba40":"# 3. RFM Segmentation <a class=\"anchor\" id=\"section_3\"><\/a>","9f5c30a8":"We covered in the <a href=\"https:\/\/www.kaggle.com\/raphael2711\/data-prep-visual-eda-and-statistical-hypothesis\">previous notebooks<\/a> the data discovery steps (data types, data shape, data completeness, etc..)  \nWe will therefore directly start with the feature engineering step and the analysis of the statistical metrics relevant for this usecase.","52fe3220":"#### Monetary clusters creation","734575cc":">- We can easily understand from the above heatmap that if a customer has made 30 transactions and their latest purchase as when they were 700 days old, then they are considered as the **best customers** and are more likely to buy in the following 30 days. (bottom right)  \n>- We can also notice the interesting area in light blue around (5 ; 500) which represents customers who buy infrequently but we have seen them recently. We are not sure if they are dead or if they might purchase again soon. (probability around 0.5) ","3d38384c":"Customer Lifetime Value can be viewed as the economic value derived from the firm's relationship with its customers. \nCLV is defined as a measure of the present value of future cash flows attributed to the customer relationship.In other words, CLV measure the net profit a customer will bring to the firm over the future periods. Hence past customer transactions may be used as a predictive driver of the economic value of a firm's customer relationship.\n\nThe CLV formula can be written as :\n\n$$CLV = \\sum_{n=1}^{N} \\frac {Value_{n}*Retention^{n}}{ (1+ DiscountRate)^{n}}$$\n","9937629b":"### C. Monetary calculation <a class=\"anchor\" id=\"section_3_3\"><\/a>","ffdfd663":"# 5. Conclusion <a class=\"anchor\" id=\"section_5\"><\/a>","effb04d0":"#### DataFrame aggregation","410969cb":"While it exists several version of BTYD models, I will here use the BG\/NBD model.  \nBG\/NBD was introduced in 2004 by Peter Fader and stands for Beta Geometric\/Negative Binomial Distribution.   \nThe model distinguish customer behaviour in two parts:\n- The buying process which models the probability a customer makes a purchase\n- The dying process (or dropout) which models the probability a customer quit and never purchase again\n\nBG\/NBD model is based on 5 assumptions :\n>1. While active, the number of transactions made by a customer follows a **Poisson distribution** with transaction rate $\\lambda$\n>2. Heterogenity in transaction rate $\\lambda$ follows a **Gamma distribution** (each customer has its own probability of buying)\n>3. After any transaction, a customer becomes inactive with probability $p$. The point at which a customer \"drops out\" (or \"die\") is distributed across the transactions according to a **Geometric distribution**\n>4. Heterogeneity in $p$ (dropout probability) follows a **Beta distribution** \n>5. The transaction rate $\\lambda$ and the dropout probability $p$ vary independently across customers\n\nOnce these probability distributions have been fitted, we obtain for each customer :\n- $P(X(t)=x| \\lambda ,p) $ : the probability of observing $x$ transactions in a time period of lenght $t$\n- $E(X(t)| \\lambda ,p) $ : the expected number of transactions in a time period of lenght $t$\n- $P(\\tau>t) $ : the probability of a customer becoming inactive at period $\\tau$","c8cd960c":"In this analysis we will divide our customers in 5 clusters for each RFM metrics leading to 5x5x5 clusters","a65d5498":"#### The Frequency\/Recency Heatmap helps us better understanding how the model estimates the  probability of a customer still being alive and their expected number of future purchases","41f363ec":"#### Estimates the average transaction value for each customer","108fa4d9":"# 4. CLV modeling <a class=\"anchor\" id=\"section_4\"><\/a>","0e6e72e1":"We can quickly check if the predicted sales and the actual sales are not ","ee5fefb1":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#005097; border:0' role=\"tab\" aria-controls=\"home\"><center>RFM Segmentation and CLV modeling <\/center><\/h1>","f3c945ad":"#### Estimates the expected number of repeat purchases for each customer","bc5af099":"In this final step, we calculate the Long Term Value for each customer over the next 12 months. As explained in the theorical part, we will assume a monthly discount rate of 1%","c3f8201e":"#### Frequency clusters creation","36475622":"In order to keep a manageable number of segments, the segments are created using only the recency and frequency scores.  \nThe monetary score is often viewed as an aggregation metric for summarizing transactions.","09b021c3":"In this step, we will predict the number of repeat purchase each customer will make in the next 30 days","58a4365f":"### D. Segment creation <a class=\"anchor\" id=\"section_3_4\"><\/a>","926f5306":"#### Metrics calculation","72e7a3af":"The summary above shows the estimated distribution parameter values from the dataset.   \nThe model can now use this parameters to predict the future number of transactions for each customer and their churn rate.","5a2ccbc3":"#### Recency clusters creation","4a23be53":"We can see that segments with the highest LTV values are the **Champions**, followed just after by the **Loyal customers**.  \nWe can use our new RFM segmentation along the LTV to develop a classification model and determine wich customers are most likely to be receptive to our next promotionnal marketing campaign","8d91af9e":"### C. Value Model fitting <a class=\"anchor\" id=\"section_4_3\"><\/a>","e9956593":"### A. Deriving RFM Metrics <a class=\"anchor\" id=\"section_4_1\"><\/a>","9d0b6338":"RFM segmentation is a scoring technique used to better quantify customer behavior. During marketing campaigns, not all customers should be contacted with the same effort. Direct marketing segmentation enables to group customers in different segments and anayze their profitability accordingly.\n\nRFM metrics are closely related to the Customer Lifetime Value as frequency and monetary value affect directly **CLV** and recency affects **retention**\n>- **Recency** : Time since last order  \n>- **Frequency** : Total number of transactions\n>- **Monetary** : Total transactions value\n\nThese metrics are very important to understand customer behavior :\n- The more **recent** the purchase, the more **responsive** the customer is to promotions\n- The more **frequently** customers buy, the more **engaged** they are","c1ea80b5":"### B. Frequency calculation <a class=\"anchor\" id=\"section_3_2\"><\/a>","06c5272c":"BTYD model is built on 4 metrics which are closely related to the ones used for RFM segmentation :\n- **Frequency** : The number of repeated purchases the customer made after his first date of first purchase\n- **Age** (Time) : The period the customer has been enrolled in the company, expressed in days, weeks or even months. \n   $\\textit{Age = Last date in dataset - first customer purchase date }$\n- **Recency** : The age of the customer when he made its last purchase  \n    $\\textit{Recency = Last customer purchase date - first customer purchase date }$\n- **Monetary value** : The average  amount spent by a customer","50fa6985":"### A. Feature Engineering <a class=\"anchor\" id=\"section_2_1\"><\/a>","8da95b4a":"The results we got are fine. We can now calculate the Customer Lifetime Values","b2a5716b":"### B. Customer Lifetime Value <a class=\"anchor\" id=\"section_1_2\"><\/a>","5f665454":"# 1. Theoretical concepts <a class=\"anchor\" id=\"section_1\"><\/a>","ca66dd24":"- **Buy Till You Die model (BTYD model)**","29770abd":"#### Estimates the probability of a customer still being alive","0af97e5f":"To calculate CLV, we will retrieve the \"DT_Customer\" which will help us calculate the Recency and Age variables in our BTYD model.  \n`Note that Recency that we will use in our BTYD model is different that the one we used in our RFM segmentation `","cae8171e":"#### Elbow method"}}