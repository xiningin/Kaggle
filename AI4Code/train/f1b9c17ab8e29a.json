{"cell_type":{"58fefb01":"code","6c580060":"code","8302c01e":"code","98d6ac0a":"code","af35ff42":"code","91c7d91d":"code","be46f38f":"code","9cdaaa32":"code","68577477":"code","ddd6b8f5":"code","2eca8338":"code","8c9a2b21":"code","2b6c7e4c":"code","9bc1d68a":"code","f7b66640":"code","f311113f":"code","c03b4585":"code","95faaed6":"code","4718afb9":"code","85e3a7ff":"code","5e3532cf":"code","ca3b100e":"code","e6fb2b35":"code","74c15062":"code","4b6eef3c":"code","d206cd2d":"code","f4eed419":"code","2954990c":"code","14817f79":"code","75f2184b":"code","44b4efbb":"code","799d9bf6":"code","fcd073b0":"code","c554ddca":"code","1785899c":"code","870ec68f":"code","cafd56df":"code","13d420ce":"code","997058d1":"code","8eb3f7fe":"code","3f756ce9":"code","3238ac52":"code","489f2ce5":"code","d7b0fbb9":"code","99cbbd7c":"code","a678a0b7":"code","79ce529a":"code","4b46dc7d":"code","7a16d840":"code","b89c57fb":"code","a74f94f7":"markdown","6153353e":"markdown","54cc2395":"markdown","f3e2534f":"markdown","b8debf85":"markdown"},"source":{"58fefb01":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6c580060":"import pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv', nrows=20000)\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv', nrows=200000)\n\nprint(train.shape)\nprint(test.shape)","8302c01e":"train.head(3)","98d6ac0a":"train.columns","af35ff42":"# Check How many variable we need to predict\ntrain['target'].unique()","91c7d91d":"# Lets Drop What not requeired .. In this case Both have id which need to be removed\n\ny_train = train['target']\ntest_id = test['id'] # Future Requirement\ntrain.drop(['id', 'target'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)\n\n","be46f38f":"\ntrain.head(2)","9cdaaa32":"test.head(2)","68577477":"train.columns","ddd6b8f5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, y_train, test_size=0.33, stratify=y_train)","2eca8338":"X_train.columns","8c9a2b21":"X_test.columns","2b6c7e4c":"print(X_train.shape)\nprint(X_test.shape)","9bc1d68a":"# from sklearn.preprocessing import LabelEncoder\n\n# #Auto encodes any dataframe column of type category or object.\n# def dummyEncode(df):\n#         columnsToEncode = list(df.select_dtypes(include=['category','object']))\n#         s1 = ['a', 'b', np.nan]\n#         le = LabelEncoder()\n#         for feature in df.columns:\n#             try:\n#                 df[feature] = le.fit_transform(df[feature])\n#             except:\n#                 print('Error encoding '+feature)\n#         return df","f7b66640":"from sklearn.preprocessing import LabelEncoder\n\n#Auto encodes any dataframe column of type category or object.\ndef dummyEncode(df):\n    s1 = [ 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\n    \n    for feature in s1:\n        dummy = pd.get_dummies(df['{}'.format(feature)])\n        df = pd.concat([df, dummy], axis=1)\n        \n    return df\n    \n    ","f311113f":"# dummy = pd.get_dummies(X_train['nom_1'])\n# # X_train = X_train.drop(['nom_1'], axis=1, inplace=True)\n# X_train = pd.concat([X_train, dummy], axis=1)","c03b4585":"X_train.head(1)","95faaed6":"X_train = dummyEncode(X_train)\nX_test = dummyEncode(X_test)","4718afb9":"X_train.head(3)","85e3a7ff":"\n# X_train = pd.get_dummies(X_train, columns=X_train.columns, drop_first=True, sparse=True)\n# X_test = pd.get_dummies(X_test, columns=X_test.columns, drop_first=True, sparse=True)","5e3532cf":"# X_train = X_train.sparse.to_coo().tocsr()\n# X_test = X_test.sparse.to_coo().tocsr()","ca3b100e":"X_train.columns","e6fb2b35":"TF_Map = { 'T' : 1, 'F' : 0}\nYN_Map = { 'Y' : 1, 'N' : 0}\n\n\nX_train['bin_3_'] =  X_train['bin_3'].map(TF_Map)\nX_train['bin_4_'] = X_train['bin_4'].map(YN_Map)\n\nX_test['bin_3_'] = X_test['bin_3'].map(TF_Map)\nX_test['bin_4_'] = X_test['bin_4'].map(YN_Map)\n","74c15062":"X_train.head(3)","4b6eef3c":"def dropExtrafeatures(df):\n    df.drop([ 'bin_4', 'bin_3', 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5'], axis=1, inplace=True)\n    return df\n    ","d206cd2d":"X_train = dropExtrafeatures(X_train)\nX_test = dropExtrafeatures(X_test)","f4eed419":"X_train.head(3)","2954990c":"print(X_train.shape)\nprint(X_test.shape)","14817f79":"# X_train = X_train.iloc[:X_train.shape[0], :]\n# X_test = X_test.iloc[:X_train.shape[0], :]\n\n\nX_train.columns","75f2184b":"# Get missing columns in the training test\nmissing_cols = set( X_train.columns ) - set( X_test.columns )\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    X_test[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\nX_test = X_test[X_train.columns]","44b4efbb":"X_train.head(3)","799d9bf6":"# if (str(X_train.dtypes) == 'object'):\n#     print(X_train.columns)\nX_train.bin_4_.unique()","fcd073b0":"# X_train.bin_4[:1]","c554ddca":"print(X_train.shape)\nprint(X_test.shape)","1785899c":"# # It Gives us Memory error SO I am going to use PCA overit\n\n# from sklearn.preprocessing import StandardScaler \n# sc = StandardScaler() \n  \n# X_train = sc.fit_transform(X_train) \n# X_test = sc.transform(X_test) ","870ec68f":"# from sklearn.decomposition import PCA \n  \n# pca = PCA(n_components = 2) \n  \n# X_train = pca.fit_transform(X_train) \n# X_test = pca.transform(X_test) \n  \n# explained_variance = pca.explained_variance_ratio_ ","cafd56df":"\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport matplotlib.pyplot as plt","13d420ce":"# X_train = np.array(X_train).tocsr()\n# X_test = np.array(X_test).tocsr()","997058d1":"# neigh = LogisticRegression()\n# parameters = {'C':[0.01,0.1,1,10,100]}\n# # clf_search = GridSearchCV(neigh, parameters, cv=3, scoring='roc_auc')\n# clf_search =  RandomizedSearchCV(neigh, param_distributions=parameters, n_iter=10, cv=5, iid=False)\n# clf_search.fit(X_train, y_train)\n\n# print(clf_search.best_estimator_)","8eb3f7fe":"def plotROCCurveGraph(X_train_roc, y_train_roc, X_test_roc, y_test_roc, best_alpha):\n    # for i in tqdm(parameters):\n    neigh = LogisticRegression( C=best_alpha, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False)\n    neigh.fit(X_train_roc, y_train_roc)\n\n    y_train_pred = neigh.predict_proba(X_train_roc)[:,1]\n    y_test_pred = neigh.predict_proba(X_test_roc)[:,1]\n\n    train_fpr, train_tpr, tr_thresholds = roc_curve(y_train_roc, y_train_pred)\n    test_fpr, test_tpr, te_thresholds = roc_curve(y_test_roc, y_test_pred)\n\n    m_Auc = str(auc(train_fpr, train_tpr))\n\n\n    plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n    plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n    plt.legend()\n    plt.xlabel(\"FPR\")\n    plt.ylabel(\"TPR\")\n    plt.title(\"ROC Curve\")\n    plt.grid()\n    plt.show()\n    \n    \n    \n    return neigh, m_Auc,","3f756ce9":"clf, m1_Auc, = plotROCCurveGraph(X_train, y_train, X_test, y_test, 0.1)","3238ac52":"test = dummyEncode(test)","489f2ce5":"TF_Map = { 'T' : 1, 'F' : 0}\nYN_Map = { 'Y' : 1, 'N' : 0}\n\n\ntest['bin_3_'] =  test['bin_3'].map(TF_Map)\ntest['bin_4_'] = test['bin_4'].map(YN_Map)","d7b0fbb9":"test = dropExtrafeatures(test)","99cbbd7c":"missing_cols = set( X_train.columns ) - set( test.columns )\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    test[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\ntest = test[X_train.columns]","a678a0b7":"test.shape","79ce529a":"# test_1st = test[:10000]\n# test_2nd = test[10000:20000]\n# test_3rd = test[20000:30000]\n# test_4th = test[30000:40000]\n# test_5th = test[40000:50000]\n\ndef index_marks(nrows, chunk_size):\n    return range(1 * chunk_size, (nrows \/\/ chunk_size + 1) * chunk_size, chunk_size)\n\ndef split(dfm, chunk_size):\n    indices = index_marks(dfm.shape[0], chunk_size)\n    return np.split(dfm, indices)\n\nchunks = split(test, 10000)\n\nresult_toSubmit = []\n\nfor c in chunks:\n    if (c.shape[0] != 0):\n        result_toSubmit.extend(clf.predict(c))\n        print(\"Shape: {}; {}\".format(c.shape, c.index))\n\n","4b46dc7d":"result_toSubmit[:]","7a16d840":"len(result_toSubmit)","b89c57fb":"submission = pd.DataFrame({'id': test_id, 'target': result_toSubmit})\nsubmission.to_csv('v2_submission.csv', index=False)","a74f94f7":"> Parameter Tuning","6153353e":"> Logistic Regression","54cc2395":"> Looks like files are having diff count of feature created.","f3e2534f":"* Its A Binary Classificatoin","b8debf85":"> Split"}}