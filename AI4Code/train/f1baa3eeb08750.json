{"cell_type":{"acc49513":"code","1ef5030a":"code","9dfe819c":"code","c5caf9dc":"code","a8dfacad":"code","a4465196":"code","5cb4ffc9":"code","58be55ee":"code","81231988":"code","8df3a00f":"code","392ceafa":"code","5cb081cf":"code","04b00c7e":"code","1ee8017d":"code","da549d95":"code","59843173":"code","e99eea91":"code","888672ba":"code","852e592d":"code","dd2ffdec":"code","bed9119a":"code","2313c656":"code","4a06d836":"code","576e3196":"code","4c19d38d":"code","eb3dd2de":"code","85ed886b":"code","190e2072":"code","5c468928":"code","a9ed77cb":"code","0d36617e":"code","a9c21438":"code","9ed1b9ef":"code","e45cb304":"code","02890628":"code","58ddde57":"code","ab66039c":"code","2a8bbb72":"code","a1bb704e":"code","b0b997c1":"code","334aa346":"code","eae37265":"code","c8b56331":"code","f2d585a1":"code","6127d83b":"code","bd014172":"code","b92ca31d":"code","e027a128":"code","466772bb":"code","bd103df5":"code","dfee688c":"code","53b864e8":"markdown","fb6d6882":"markdown","bda87cfc":"markdown","5c312734":"markdown","9cda281e":"markdown","8632b478":"markdown","399a3b88":"markdown","f64262b7":"markdown","f95cf62f":"markdown","da4f9dda":"markdown","cadee4ab":"markdown","78d341e7":"markdown","0ab0d8cb":"markdown","2c1c3c9a":"markdown","cea4d38b":"markdown","a8756d96":"markdown","84460bdc":"markdown","80797ac1":"markdown","ff00622b":"markdown","59cba85e":"markdown","4838efda":"markdown","734543e3":"markdown","ac419dea":"markdown","44e2ad2e":"markdown","84f3bb12":"markdown","09702e65":"markdown","e929a0c1":"markdown","31912bbd":"markdown","7d49e389":"markdown","3c5c5dee":"markdown","3242a5ab":"markdown","f4990406":"markdown","b656a2c3":"markdown","8753352f":"markdown","1abc07d8":"markdown","6a03bd55":"markdown","2ea2054c":"markdown","e4b8a26e":"markdown","c90eb6a5":"markdown","1edaa7b8":"markdown","292d00ef":"markdown","f39138c8":"markdown","236da477":"markdown","ffc0661a":"markdown","257a45df":"markdown","7d3c2943":"markdown","06889311":"markdown","e70b9c67":"markdown","bbebe93c":"markdown","bf5f1265":"markdown","4734da4d":"markdown","c4ca8e4d":"markdown","e621065d":"markdown"},"source":{"acc49513":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.colors import ListedColormap\n%matplotlib inline \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1ef5030a":"data = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","9dfe819c":"data.head()","c5caf9dc":"data.info()","a8dfacad":"data.drop(\"id\", axis=1, inplace=True)","a4465196":"data.drop('Unnamed: 32', axis=1, inplace=True)","5cb4ffc9":"data.diagnosis.unique()","58be55ee":"data.diagnosis = data['diagnosis'].map({'M':1, 'B':0})","81231988":"data.describe()","8df3a00f":"sns.countplot(data[\"diagnosis\"])","392ceafa":"f, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(data.corr(), annot=True, fmt='.1f',\n            ax=ax, cmap='coolwarm', vmin=-1, vmax=1)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title('Correlation Map', size=14);","5cb081cf":"flt = np.abs(data.corr()['diagnosis']) > .75","04b00c7e":"corr_feat = data.corr().columns[flt].tolist()","1ee8017d":"f, ax = plt.subplots(figsize = (8,8))\nsns.heatmap(data[corr_feat].corr(), annot=True, fmt='.2f',\n           ax=ax, cmap='coolwarm',vmin=-1,vmax=1)\nplt.xticks(rotation=60)\nplt.yticks(rotation=0)\nplt.title('Correlation Between Features (Th>0.75)');","da549d95":"def melt(dataset, param):\n    data_melted = pd.melt(dataset, id_vars=param,\n                     var_name=\"features\",\n                     value_name=\"value\")\n    return data_melted\n\ndef boxplot(dataset, param):\n    plt.figure(figsize= (14,8))\n    sns.boxplot(x=\"features\", y=\"value\", hue=param, data=dataset)\n    plt.xticks(rotation = 90)\n    return plt.show()\n\ndef pairplot(dataset, param):\n    sns.pairplot(dataset, diag_kind='kde', markers='+', hue=param);","59843173":"boxplot(melt(data,\"diagnosis\"),\"diagnosis\")","e99eea91":"pairplot(data[corr_feat],\"diagnosis\")","888672ba":"feat = list(data.columns[1:11])\ndataM = data[data.diagnosis ==1]\ndataB = data[data.diagnosis ==0]","852e592d":"plt.rcParams.update({'font.size': 8})\nf, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\naxes = axes.ravel()\nfor i,ax in enumerate(axes):\n    ax.figure\n    binwidth= (max(data[feat[i]]) - min(data[feat[i]]))\/50\n    ax.hist([dataM[feat[i]],dataB[feat[i]]], \n            bins=np.arange(min(data[feat[i]]), max(data[feat[i]]) + binwidth, binwidth), \n            alpha=0.5,stacked=True, density=True, label=['M','B'],color=['r','g'])\n    ax.legend(loc='upper right')\n    ax.set_title(feat[i])\nplt.tight_layout()\nplt.show()","dd2ffdec":"X = data.drop(['diagnosis'], axis=1)\ny = data.diagnosis","bed9119a":"column = X.columns.tolist()","2313c656":"LOF = LocalOutlierFactor()\ny_pred = LOF.fit_predict(X)\nX_score = LOF.negative_outlier_factor_","4a06d836":"outlier_score = pd.DataFrame()\noutlier_score['score'] = X_score\noutlier_score.head()","576e3196":"radius = (X_score.max() - X_score) \/ (X_score.max() - X_score.min())\noutlier_score[\"radius\"] = radius\nfilt = outlier_score[\"score\"] < -2.5\noutlier_index = outlier_score[filt].index.tolist()\nplt.figure(figsize = (12,8))\nplt.scatter(X.iloc[outlier_index,0], X.iloc[outlier_index,1], \n            color = \"blue\", s = 50, label = \"Outliers\" )\nplt.scatter(X.iloc[:,0], X.iloc[:,1], color =\"k\", s=3, label = \"Data Points\" )\nplt.scatter(X.iloc[:,0],X.iloc[:,1], s=1000*radius, edgecolor = \"r\", \n            facecolors = \"none\", label=\"Outlier Scores\")\nplt.legend()\nplt.show()","4c19d38d":"X = X.drop(outlier_index)\ny = y.drop(outlier_index).values","eb3dd2de":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","85ed886b":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train_df = pd.DataFrame(X_train, columns= column)","190e2072":"X_train_df.head()","5c468928":"X_train_df.describe()","a9ed77cb":"df = X_train_df","0d36617e":"df['diagnosis'] = y_train","a9c21438":"boxplot(melt(df,'diagnosis'),'diagnosis')","9ed1b9ef":"pairplot(df[corr_feat],'diagnosis')","e45cb304":"def classification_and_fit_model(model, data, predictors, outcome):\n    model.fit(data[predictors], data[outcome])\n    predictions = model.predict(data[predictors])\n    accuracy =metrics.accuracy_score(predictions, data[outcome])\n    print('Accuracy : %s' % '{0:.3%}'.format(accuracy))\n    kf = KFold(n_splits=5)\n    error = []\n    for train, test in kf.split(data):\n        train_predictors = data[predictors].iloc[train,:]\n        train_target = data[outcome].iloc[train]\n        model.fit(train_predictors, train_target)\n        error.append(model.score(data[predictors].iloc[test,:],\n                                data[outcome].iloc[test]))\n        print('Cross-Validation Score : %s' % '{0:.3%}'.format(np.mean(error)))\n    model.fit(data[predictors],data[outcome])\n        ","02890628":"predictor = ['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean', 'concave points_mean']","58ddde57":"outcome = 'diagnosis'","ab66039c":"model = LogisticRegression()","2a8bbb72":"classification_and_fit_model(model, X_train_df, predictor, outcome)","a1bb704e":"predictor1 = ['radius_mean']","b0b997c1":"classification_and_fit_model(model, X_train_df, predictor1, outcome)","334aa346":"model = DecisionTreeClassifier()","eae37265":"classification_and_fit_model(model, X_train_df, predictor, outcome)","c8b56331":"classification_and_fit_model(model, X_train_df, predictor1, outcome)","f2d585a1":"features_mean = list(X_train_df.columns[1:11])","6127d83b":"model = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)","bd014172":"classification_and_fit_model(model, X_train_df, features_mean, outcome)","b92ca31d":"feature_importance = pd.Series(model.feature_importances_, index=features_mean).sort_values(ascending=False)\nfeature_importance","e027a128":"model = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)","466772bb":"classification_and_fit_model(model, X_train_df, predictor, outcome)","bd103df5":"model = RandomForestClassifier(n_estimators=100)","dfee688c":"classification_and_fit_model(model, X_train_df, predictor1, outcome)","53b864e8":"### Conclusion <a class=\"anchor\" id=\"chapter15\"><\/a>","fb6d6882":"The accuracy of the predictions are good but not great. The cross-validation scores are reasonable. Can we do better with another model?","bda87cfc":"The best model to be used for diagnosing breast cancer as found in this analysis is the Random Forest model with the top 5 predictors, 'concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean'. It gives a prediction accuracy of ~95% and a cross-validation score ~ 95% for the test data set.","5c312734":"We see outliers. Let's remove them now","9cda281e":"### Classification and Build a Model <a class=\"anchor\" id=\"chapter11\"><\/a>","8632b478":"Let's look at the statistical information of our dataset","399a3b88":"### Decision Tree <a class=\"anchor\" id=\"chapter13\"><\/a>","f64262b7":"### Load Libraries <a class=\"anchor\" id=\"chapter1\"><\/a>","f95cf62f":"### Diagnosis vs Features <a class=\"anchor\" id=\"chapter6\"><\/a>","da4f9dda":"### Outlier Detection <a class=\"anchor\" id=\"chapter7\"><\/a>","cadee4ab":"The prediction accuracy is good. What happens if we use just one predictor? Use the mean_radius:","78d341e7":"### Random Forest <a class=\"anchor\" id=\"chapter14\"><\/a>","0ab0d8cb":"Columns are initials of Malignant and Bening results. These values are not the kind our algorithms will understand. For this, we will define Malignant as 1 and Bening as 0.","2c1c3c9a":"Using top 5 features","cea4d38b":"### Logistic Regression <a class=\"anchor\" id=\"chapter12\"><\/a>","a8756d96":"### Standardization <a class=\"anchor\" id=\"chapter10\"><\/a>","84460bdc":"* mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other. In any of the histograms there are no noticeable large outliers that warrants further cleanup.","80797ac1":"* Unlike the above, the mean values of cell *radius*, *perimeter*, *area*, *compactness*, *concavity* and *concave points* can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.","ff00622b":"### Load the data <a class=\"anchor\" id=\"chapter2\"><\/a>","59cba85e":"In order for our model to give more accurate results, we need to standardize our data.","4838efda":"Let's detect outliers.","734543e3":"### Table of Contents\n\n* [Load Libraries](#chapter1)\n* [Load the data](#chapter2)\n* [Cleaning and Preparing the data](#chapter3)\n* [Exploratory Data Analysis](#chapter4)\n* [Correlation Matrix](#chapter5)\n* [Diagnosis vs Features](#chapter6)\n* [Outlier Detection](#chapter7)\n* [Drop Outliers](#chapter8)\n* [Creating Test and Train Dataset](#chapter9)\n* [Standardization](#chapter10)\n* [Classification and Build a Model](#chapter11)\n* [Logistic Regression](#chapter12)\n* [Decision Tree](#chapter13)\n* [Random Forest](#chapter14)\n* [Conclusion](#chapter15)","ac419dea":"Let's plot the distribution of the values in the features by those who are sick and those who are not.","44e2ad2e":"Using the top 5 features only changes the prediction accuracy a bit but I think we get a better result if we use all the predictors.","84f3bb12":"looks better.","09702e65":"It looks like there are outliers. If we remove them and standardize our values, we get more accurate results.","e929a0c1":"Here we are over-fitting the model probably due to the large number of predictors. Let use a single predictor, the obvious one is the radius of the cell.","31912bbd":"This gives a similar prediction accuracy and a cross-validation score.","7d49e389":"#### Thanks for viewing my notebook :)\nI will be very happy if you vote.","3c5c5dee":"On average, 350 of our samples have benign tumors and 200 have malignant tumors.","3242a5ab":"### Cleaning and Preparing the data <a class=\"anchor\" id=\"chapter3\"><\/a>","f4990406":"Using a single predictor gives a 97% prediction accuracy for this model but the cross-validation score is not that great.","b656a2c3":"Let's look at the values of the column we are going to predict","8753352f":"As a result, if we divide our data into patients and non-patients, our prediction model will give more accurate results.","1abc07d8":"### Creating Test and Train Dataset <a class=\"anchor\" id=\"chapter9\"><\/a>","6a03bd55":"An advantage with Random Forest is that it returns a feature importance matrix which can be used to select features. So lets select the top 5 features and use them as predictors","2ea2054c":"Now let's look at the distribution of our values","e4b8a26e":"Since this data set is not ordered, I am going to do a simple 70:30 split to create a training data set and a test data set.","c90eb6a5":"Using all the features improves the prediction accuracy and the cross-validation score is great.","1edaa7b8":"This gives a better prediction accuracy too but the cross-validation is not great","292d00ef":"When we plot the highly correlated features by classifying them according to the target value, we see that the data with benign tumors and data with malignant tumors are grouped in separate values. In other words, the fact that one of the features we are comparing has a high value causes the other to have a high value. The analysis we will do in this way will not give exactly correct results. Because when the value of one of the features with high correlation increases, the other increases as well.","f39138c8":"* Based on the observations in the histogram plots, we can reasonably hypothesize that the cancer diagnosis depends on the mean cell radius, mean perimeter, mean area, mean compactness, mean concavity and mean concave points. We can then perform a logistic regression analysis using those features as follows:","236da477":"What happens if we use a single predictor as before? Just check.","ffc0661a":"### Exploratory Data Analysis <a class=\"anchor\" id=\"chapter4\"><\/a>","257a45df":"Here we see the 5 features that are most correlated with each other. One of them is our target value. This is good","7d3c2943":"### Correlation Matrix <a class=\"anchor\" id=\"chapter5\"><\/a>","06889311":"The accuracy of the prediction is much much better here. But does it depend on the predictor?","e70b9c67":"Let's plot the traits with a correlation greater than 0.75","bbebe93c":"I will see if I can improve this more by tweaking the model further and trying out other models in a later version of this analysis.","bf5f1265":"Since we don't need the *id* column and the empty *Unnamed: 32* column in the dataset, we remove them.","4734da4d":"Here we are going to build a classification model and evaluate its performance using the training set.","c4ca8e4d":"### Drop Outliers <a class=\"anchor\" id=\"chapter8\"><\/a>","e621065d":"Let's look at the density of our target value"}}