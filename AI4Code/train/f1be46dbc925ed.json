{"cell_type":{"0e68a89d":"code","4612c0bb":"code","2c35eef8":"code","9ad1425b":"code","babd9c0d":"code","a4358812":"code","bbafe090":"code","839d833a":"code","4164c2f9":"code","53aced12":"code","5501a59f":"code","ffaf1ea5":"code","9569b063":"code","3f0b4a1f":"code","67312bff":"code","4084b635":"code","7e2e93b0":"code","510d7a72":"code","79eba4ff":"code","8b183796":"code","4fe0e7e7":"code","9f8fa001":"code","ab636253":"code","e006572d":"code","fe038ecc":"code","94d1716c":"code","8182e755":"code","1beb400f":"code","cc74b8e8":"code","f5ed0df7":"code","36d6d6c9":"code","f9d71175":"code","a42161f8":"markdown","89193956":"markdown","bde63e91":"markdown","f4c1990b":"markdown","c6f617fb":"markdown","a5aff72c":"markdown","7d57d422":"markdown"},"source":{"0e68a89d":"!cat ..\/input\/movielens-100k-dataset\/ml-100k\/u.info","4612c0bb":"!pip install uszipcode","2c35eef8":"import pandas as pd\nimport numpy as np","9ad1425b":"users_df = pd.read_csv(\n    '..\/input\/movielens-100k-dataset\/ml-100k\/u.user', sep='|', \n    names=['user_id', 'age', 'sex', 'occupation', 'zip_code']\n)\nusers_df[\"user_id\"] = users_df[\"user_id\"].astype(int).apply(lambda x: x-1)\nusers_df.set_index('user_id', inplace=True)\nusers_df","babd9c0d":"genres = [\n    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\",\n]\nmovies_df = pd.read_csv(\n    '..\/input\/movielens-100k-dataset\/ml-100k\/u.item', sep='|',\n    names=['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url', *genres]\n)\n\nmovies_df[\"movie_id\"] = movies_df[\"movie_id\"].astype(int).apply(lambda x: int(x-1))\nmovies_df[\"release_date\"] = pd.to_datetime(movies_df['release_date'], format='%d-%b-%Y')\n# .apply(lambda x: str(x).split('-')[-1])\n# movies_df[movies_df[\"release_date\"] == 'nan'] = '1995'\n# movies_df[\"release_date\"] = movies_df[\"release_date\"].apply(int)\nmovies_df.drop(columns=['video_release_date', 'imdb_url'], inplace=True)\nmovies_df.set_index('movie_id', inplace=True)\nmovies_df","a4358812":"ratings_df = pd.read_csv(\n    '..\/input\/movielens-100k-dataset\/ml-100k\/u.data', sep='\\t',\n    names=['user_id', 'movie_id', 'rating', 'unix_timestamp']\n)\nratings_df[\"user_id\"] = ratings_df[\"user_id\"].astype(int).apply(lambda x: x-1)\nratings_df[\"movie_id\"] = ratings_df[\"movie_id\"].astype(int).apply(lambda x: x-1)\nratings_df['rating'] = ratings_df['rating'].apply(float)\nratings_df['rating_date'] = pd.to_datetime(ratings_df['unix_timestamp'], unit='s')\nratings_df['rating_binary'] = (ratings_df['rating'] >= ratings_df['rating'].mean()).astype(int)\nratings_df.drop(columns=['unix_timestamp'], inplace=True)\nratings_df","bbafe090":"from sklearn.cluster import KMeans\nfrom sklearn import preprocessing\nfrom uszipcode import SearchEngine\nimport itertools as it\nimport math","839d833a":"def series_to_np(series):\n    return np.array(list(series.values))","4164c2f9":"# Convert genres to list of values\nmovies_df['genres'] = movies_df.apply(lambda x: [i for i, g in enumerate(genres) if x[g]], axis=1)\n# movies_df.drop(columns=genres, inplace=True)\nmovies_df","53aced12":"# Normalize Discrete\noccupations = list(users_df['occupation'].unique())\nusers_df['feat_occupation'] = users_df['occupation'].apply(lambda x: occupations.index(x))\nuser_occupation_oh = series_to_np(users_df['feat_occupation'].apply(lambda x: np.eye(len(occupations))[x]))\nprint(user_occupation_oh.shape)\n\nsexes = list(users_df['sex'].unique())\nusers_df['feat_sex'] = users_df['sex'].apply(lambda x: sexes.index(x))\nuser_sex_oh = series_to_np(users_df['feat_sex'].apply(lambda x: np.eye(len(sexes))[x]))\nprint(user_sex_oh.shape)","5501a59f":"# Cluster zip codes by latlon to get area feat\nsearch = SearchEngine()\nlatlon = users_df['zip_code'].apply(lambda x: search.by_zipcode(x).to_dict())\\\n    .apply(lambda x: [x['lat'] or 37.41, x['lng'] or -122.09])\nkmeans = KMeans(n_clusters=5, random_state=0).fit(np.array(list(latlon.values)))\nusers_df['feat_geo_area'] = kmeans.labels_\nuser_geo_oh = series_to_np(users_df['feat_geo_area'].apply(lambda x: np.eye(5)[x]))\nprint(user_geo_oh.shape)","ffaf1ea5":"# Normalize ages\nusers_df['feat_age'] = preprocessing.StandardScaler().fit_transform(users_df['age'].values.reshape(-1, 1)).squeeze()\nuser_age = series_to_np(users_df['feat_age']).reshape((-1, 1))\nprint(user_age.shape)","9569b063":"# Extract Example Age\nratings_df_tmp = ratings_df.join(movies_df, on='movie_id')\nratings_df['example_age'] = (ratings_df_tmp['rating_date'] - ratings_df_tmp['release_date']).apply(lambda x: float(x.days))\nratings_df['example_age'] = ratings_df['example_age'].fillna(ratings_df['example_age'].mean())\nratings_df['example_age'] = preprocessing.StandardScaler().fit_transform(ratings_df['example_age'].values.reshape(-1, 1)).squeeze()\nratings_df","3f0b4a1f":"# Create User Feature Vector\nusers_df['feat_vec'] = list(np.hstack([user_occupation_oh, user_sex_oh, user_geo_oh, user_age]))","67312bff":"from sklearn.model_selection import train_test_split","4084b635":"user_ratings_grouped = (ratings_df\n    .join(movies_df[['genres']], on='movie_id', how='inner')\n    .sort_values(by='rating_date')\n    .groupby(['user_id'])\n    .agg({'movie_id': list, 'rating_date': list, 'rating_binary': list, 'genres': list, 'example_age': list})\n)\nuser_ratings_grouped","7e2e93b0":"# Split user history using windows\nWINDOW_SIZE = 16\n\ndef extract_windows(user_id, item):\n    for j in range(len(item['movie_id']) - (WINDOW_SIZE + 1)):\n        # Use only positives for labels. Is it excessive though\n#         if item.rating_binary[j + WINDOW_SIZE] != 1: continue\n            \n        movie_ids = item['movie_id'][j:j+WINDOW_SIZE]\n        ratings = item['rating_binary'][j:j+WINDOW_SIZE]\n        genres = list(set(it.chain(*item['genres'][j:j+WINDOW_SIZE])))\n        positives = list(np.array(movie_ids)[np.array(ratings, dtype=bool)])\n        negatives = list(np.array(movie_ids)[~np.array(ratings, dtype=bool)])\n        next_id = item['movie_id'][j + WINDOW_SIZE]\n        next_label = item['rating_binary'][j+WINDOW_SIZE]\n        example_age = item['example_age'][j + WINDOW_SIZE]\n        \n        \n        yield (\n            user_id, movie_ids, ratings, genres,\n            positives, negatives, next_id, next_label, example_age\n        )\n    \nwindows_df = pd.DataFrame(\n    it.chain(*it.starmap(extract_windows, user_ratings_grouped.iterrows())), \n    columns=['user_id', 'movie_ids', 'ratings', 'genres', 'positives', 'negatives', 'next', 'next_label', 'example_age']\n)\nwindows_df","510d7a72":"examples_unbalanced_df = (windows_df[['user_id', 'positives', 'negatives', 'genres', 'next', 'next_label', 'example_age']]\n       .join(users_df['feat_vec'], on='user_id')\n       .rename(columns={'genres': 'search', 'next': 'label', 'feat_vec': 'features'})\n)\nexamples_unbalanced_df","79eba4ff":"def balance(df, key, replace=False):\n    print(df.groupby(key).size().describe())\n    n_examples = math.ceil(df.groupby(key).size().mean())\n    print(f'Using {n_examples} examples per {key}')\n    balanced_df = df.groupby(key)\\\n        .apply(lambda x: x.sample(min(n_examples, len(x) if not replace else n_examples), replace=replace)) \\\n        .reset_index(drop=True)\n    print(balanced_df.groupby(key).size().describe())\n    return balanced_df\n\n# Balance training data by limiting amount of examples per user\nexamples_df = balance(examples_unbalanced_df, 'user_id', False)\n\n# Balance training data by limiting amount of examples per label\nexamples_df = balance(examples_df, 'label', True)\n\n# examples_df = examples_unbalanced_df","8b183796":"import tensorflow as tf\nimport tensorflow.keras as k\nimport os\nfrom time import strftime","4fe0e7e7":"class MaskedEmbeddingsAggregatorLayer(tf.keras.layers.Layer):\n    def __init__(self, agg_mode='sum', **kwargs):\n        super(MaskedEmbeddingsAggregatorLayer, self).__init__(**kwargs)\n\n        if agg_mode not in ['sum', 'mean']:\n            raise NotImplementedError('mode {} not implemented!'.format(agg_mode))\n        self.agg_mode = agg_mode\n    \n    @tf.function\n    def call(self, inputs, mask=None):\n#         masked_embeddings = tf.ragged.boolean_mask(inputs, mask)\n        masked_embeddings = inputs\n        if self.agg_mode == 'sum':\n            aggregated =  tf.reduce_sum(masked_embeddings, axis=1)\n        elif self.agg_mode == 'mean':\n            aggregated = tf.reduce_mean(masked_embeddings, axis=1)\n        \n        return aggregated\n    \n    def get_config(self):\n        # this is used when loading a saved model that uses a custom layer\n        return {'agg_mode': self.agg_mode}\n\nclass L2NormLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(L2NormLayer, self).__init__(**kwargs)\n    \n    @tf.function\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            inputs = tf.ragged.boolean_mask(inputs, mask).to_tensor()\n        return tf.math.l2_normalize(inputs, axis=-1)\n\n    def compute_mask(self, inputs, mask):\n        return mask","9f8fa001":"def CandidateGeneration(n_items, n_search_items, n_features, embedding_dim, user_dim):\n    # Define layers\n    search_ids_input = k.Input(shape=(None, ), name='search_ids')\n    positive_ids_input = k.Input(shape=(None, ), name='positive_ids')\n    negative_ids_input = k.Input(shape=(None, ), name='negative_ids')\n    features_input = k.Input(shape=(n_features), name='features_vec')\n\n    search_embedding_layer = k.layers.Embedding(input_dim=n_search_items, output_dim=EMBEDDING_DIMS, \n                                                mask_zero=True, trainable=True, name='search_embeddings')\n    labels_embedding_layer = k.layers.Embedding(input_dim=n_items, output_dim=EMBEDDING_DIMS,\n                                                mask_zero=True, trainable=True, name='labels_embeddings')\n    avg_embeddings = MaskedEmbeddingsAggregatorLayer(agg_mode='mean', name='aggregate_embeddings')\n\n    dense_1 = k.layers.Dense(units=DENSE_UNITS, name='dense_1')\n    dense_2 = k.layers.Dense(units=DENSE_UNITS, name='dense_2')\n    dense_3 = k.layers.Dense(units=user_dim, name='dense_3')\n\n    l2_norm_1 = L2NormLayer(name='l2_norm_1')\n    dense_output = k.layers.Dense(n_items, activation=tf.nn.softmax, name='dense_output')\n    \n    # Feature extraction\n    search_embeddings = search_embedding_layer(search_ids_input)\n    l2_norm_search = l2_norm_1(search_embeddings)\n    avg_search = avg_embeddings(l2_norm_search)\n    \n    labels_positive_embeddings = labels_embedding_layer(positive_ids_input)\n    l2_norm_positive = l2_norm_1(labels_positive_embeddings)\n    avg_positive = avg_embeddings(l2_norm_positive)\n    \n    labels_negative_embeddings = labels_embedding_layer(negative_ids_input)\n    l2_norm_negative = l2_norm_1(labels_negative_embeddings)\n    avg_negative = avg_embeddings(l2_norm_negative)\n    \n    concat_inputs = tf.keras.layers.Concatenate(axis=1)([\n        avg_search, avg_positive, avg_negative, features_input\n    ])\n    \n    # Dense Layers\n    dense_1_features = dense_1(concat_inputs)\n    dense_1_relu = tf.keras.layers.ReLU(name='dense_1_relu')(dense_1_features)\n    dense_1_batch_norm = tf.keras.layers.BatchNormalization(name='dense_1_batch_norm')(dense_1_relu)\n\n    dense_2_features = dense_2(dense_1_batch_norm)\n    dense_2_relu = tf.keras.layers.ReLU(name='dense_2_relu')(dense_2_features)\n    dense_2_batch_norm = tf.keras.layers.BatchNormalization(name='dense_2_batch_norm')(dense_2_relu)\n\n    dense_3_features = dense_3(dense_2_batch_norm)\n    dense_3_relu = tf.keras.layers.ReLU(name='dense_3_relu')(dense_3_features)\n    dense_3_batch_norm = tf.keras.layers.BatchNormalization(name='user_vec')(dense_3_relu)\n    outputs = dense_output(dense_3_batch_norm)\n    \n    model = tf.keras.models.Model(\n        inputs=[search_ids_input, positive_ids_input, negative_ids_input, features_input],\n        outputs=[outputs]\n    )\n    \n    return model","ab636253":"train_df, val_df = train_test_split(examples_df)","e006572d":"EMBEDDING_DIMS = 16\nDENSE_UNITS = 64\nUSER_DIM=32\nLEARNING_RATE = 0.003\n\n# Compile Model\nmodel = CandidateGeneration(\n    n_items=movies_df.index.astype(int).max()+2, \n    n_search_items=len(genres) + 1, \n    n_features=len(examples_df.iloc[0]['features']), \n    embedding_dim=EMBEDDING_DIMS,\n    user_dim=USER_DIM\n)\noptimiser = k.optimizers.Adam(learning_rate=LEARNING_RATE)\nmodel.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy')\n\nmodified_model = k.Model(inputs=model.input, outputs=[model.get_layer('user_vec').output, *model.outputs])","fe038ecc":"tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True,dpi=42)","94d1716c":"model.fit(\n    x=[\n        k.preprocessing.sequence.pad_sequences(train_df['search']),\n        k.preprocessing.sequence.pad_sequences(train_df['positives']),\n        k.preprocessing.sequence.pad_sequences(train_df['negatives']),\n        k.preprocessing.sequence.pad_sequences(train_df['features'])\n    ], \n    y=train_df['label'].values,\n    validation_data=([\n        k.preprocessing.sequence.pad_sequences(val_df['search']),\n        k.preprocessing.sequence.pad_sequences(val_df['positives']),\n        k.preprocessing.sequence.pad_sequences(val_df['negatives']),\n        k.preprocessing.sequence.pad_sequences(val_df['features'])\n    ], val_df['label'].values),\n    callbacks=[\n        k.callbacks.TensorBoard(os.path.join(\"logs\", strftime(\"%Y-%m-%d %H:%M:%S\")), histogram_freq=1),\n        k.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n    ], \n    steps_per_epoch=1, \n    epochs=400, \n    verbose=1,\n)\n","8182e755":"from sklearn.neighbors import KDTree\nimport io","1beb400f":"movie_embeddings = model.get_layer('labels_embeddings').get_weights()[0][1:]\n\n# Export Embeddings\ntitles = list(movies_df['title'])\n\nwith io.open('vecs.tsv', 'w', encoding='utf-8') as out_v:\n    with io.open('meta.tsv', 'w', encoding='utf-8') as out_m:\n        for i, title in enumerate(movies_df['title']):\n          out_m.write(title + \"\\n\")\n          out_v.write('\\t'.join([str(x) for x in movie_embeddings[i]]) + \"\\n\")\n\nmovie_embeddings.shape","cc74b8e8":"sample_df = examples_df.sample(1000)\nuser_vecs, movie_vecs = modified_model.predict(x=[\n        k.preprocessing.sequence.pad_sequences(sample_df['search']),\n        k.preprocessing.sequence.pad_sequences(sample_df['positives']),\n        k.preprocessing.sequence.pad_sequences(sample_df['negatives']),\n        k.preprocessing.sequence.pad_sequences(sample_df['features'])\n])\nuser_vecs.shape","f5ed0df7":"top_k = 5\nmovie_vecs = np.array([movie_vecs[i, :].argsort()[-top_k:] for i in range(len(movie_vecs))]) - 1\nmovie_vecs.shape","36d6d6c9":"kdt = KDTree(user_vecs, metric='euclidean')","f9d71175":"sample_user_df = examples_df.sample(1)\nex_user_vecs, _ = modified_model.predict(x=[\n        k.preprocessing.sequence.pad_sequences(sample_user_df['search']),\n        k.preprocessing.sequence.pad_sequences(sample_user_df['positives']),\n        k.preprocessing.sequence.pad_sequences(sample_user_df['negatives']),\n        k.preprocessing.sequence.pad_sequences(sample_user_df['features'])\n])\nex_dist, ex_movie_vec_ids = kdt.query(ex_user_vecs, 6)\ncandidates = set(filter(lambda x: x > 0, movie_vecs[ex_movie_vec_ids.squeeze()].flatten()))\ncandidates_df = movies_df.iloc[list(candidates)]\ncandidates_df","a42161f8":"## Load the datasets","89193956":"## Prepare Watch History Data\nWe use next watch prediction instead of held out watch prediction","bde63e91":"## Ranking (TODO)","f4c1990b":"## Build a nearest neighbor model \/ index","c6f617fb":"## Build the Candidate Generation Model","a5aff72c":"## Prepare Feature Data","7d57d422":"# Deep Neural Networks for YouTube Recommendations\nAn example implementation of the in paper described architecture.\n\nUses rating history to predict the next watched movie. Only the canidate generation part.\n\nThe embeddings used are trained within the model, which for scalability reasons I expect yt team does not.\n\nThe paper is a little bit vague around the details (of classification and knn). For example how example age ties into the next video classification probability. Therefore it is excluded from this notebook. "}}