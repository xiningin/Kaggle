{"cell_type":{"723ddbb1":"code","4cd64b21":"code","30a806a2":"code","e6c9591f":"code","a2b4cb7f":"code","d4e1981d":"code","00789658":"code","77e64ff6":"code","584981c9":"code","5a2325a6":"code","146c3054":"code","951f2f81":"code","63a4b84c":"code","7662d443":"code","e89a3c07":"code","650fef98":"code","52d554e3":"code","deb5746b":"code","8a3f9627":"code","bcc5874f":"code","7beb6f2f":"code","1e99e9b7":"code","5caa0ae4":"code","fb96223a":"code","c281531b":"code","1074bbf8":"code","fefe8c1a":"code","6acc5cb4":"markdown","9d62e289":"markdown","4e1a73bb":"markdown","1e088694":"markdown","2f008b12":"markdown","a7dc0f98":"markdown","aaf5b443":"markdown","a4c0c03a":"markdown","5379912d":"markdown","9327b4ed":"markdown","4e3931e7":"markdown","f0dcaf71":"markdown","07d9c0df":"markdown","cbe6e318":"markdown","591b046b":"markdown","10586a9f":"markdown","ee553fd1":"markdown","97f45081":"markdown","07916ae3":"markdown","c60a2729":"markdown","bd983394":"markdown","b31a5de9":"markdown","673daf2d":"markdown","1ca53faa":"markdown","3b299e50":"markdown","1db9e8c3":"markdown","af601c68":"markdown","fbc6009e":"markdown","c0a5b7b3":"markdown","b900cba4":"markdown"},"source":{"723ddbb1":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4cd64b21":"df = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')\ndf.head()","30a806a2":"df.info()","e6c9591f":"#Estat\u00edsticas Descritivas\ndf.describe()","a2b4cb7f":"# Verificando a distribui\u00e7\u00e3o da vari\u00e1vel BAD(Target)\ndf['BAD'].plot.hist(bins=5)","d4e1981d":"#frenqu\u00eancia dos dados da vari\u00e1vel BAD\ndf['BAD'].value_counts()","00789658":"#Verificando a quantidade de valores missing nas vari\u00e1veis\ndf.isnull().sum()","77e64ff6":"# Removendo os valores missing value da base\n#df2 =df.copy()\ndf.dropna(axis=0,how='any',inplace= True)\ndf.info(), df.isna().any() \n","584981c9":"#valores da vari\u00e1vel target\ndf['BAD'].value_counts().plot(kind='bar',title='Frequ\u00eancia da Vari\u00e1vel BAD')\ndf['BAD'].value_counts()","5a2325a6":"# Correla\u00e7\u00e3o das vari\u00e1veis num\u00e9ricas\nplt.figure(figsize= (15, 8))\nsns.heatmap(df.corr(), square=True, annot=True, linewidth=0.5)","146c3054":"#Distribui\u00e7\u00e3o da vari\u00e1vel REASON por BAD\ndf.groupby(['BAD'])['REASON'].value_counts()","951f2f81":"#Distribui\u00e7\u00e3o da vari\u00e1vel JOB por BAD\ndf.groupby(['BAD'])['JOB'].value_counts()","63a4b84c":"# Gerando Dummies para modelos que utilizam apenas variaveis num\u00e9ricas\n\ndf = pd.get_dummies(df, columns=['REASON', 'JOB'])\ndf.head().T","7662d443":"#Normalizando os dados\n\n#from sklearn.preprocessing import StandardScaler\n#sc = StandardScaler()\n#df = pd.DataFrame(sc.fit_transform(df), columns=df.columns)","e89a3c07":"# importando a biblioteca\nfrom sklearn.model_selection import train_test_split\n\n#Separando em treino e teste\ntreino, teste = train_test_split(df, test_size=0.20, random_state=42)\n\n# N\u00e3o vou usar o dataset de valida\u00e7\u00e3o\n\ntreino.shape, teste.shape  ","650fef98":"# Lista das colunas que ser\u00e3o usadas\nusadas = [c for c in treino.columns if c not in ['BAD','REASON','JOB']]","52d554e3":"#importando m\u00e9trica\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import f1_score\n","deb5746b":"# importanto o modelo\nfrom sklearn.ensemble import RandomForestClassifier\n\n#instanciando o modelo\nrf = RandomForestClassifier(n_estimators=200,random_state=42)","8a3f9627":"# treinando o modelo\nrf.fit(treino[usadas], treino['BAD'])\n\n# gerando predicoes do modelo com os dados de teste\npred_teste = rf.predict(teste[usadas])\n\n#Medindo a acuracia nos dados de teste\naccuracy_score(teste['BAD'],pred_teste), balanced_accuracy_score(teste['BAD'],pred_teste), f1_score(teste['BAD'],pred_teste)","bcc5874f":"# Avaliando a importancia de cada coluna (cada vari\u00e1vel de entrada)\npd.Series(rf.feature_importances_, index=usadas).sort_values().plot.barh()","7beb6f2f":"# importando a bilbioteca para plotar o gr\u00e1fico de Matriz de Confus\u00e3o\nimport scikitplot as skplt\n\n# Matriz de Confus\u00e3o - Dados de Valida\u00e7\u00e3o\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_teste)","1e99e9b7":"# Setando parametros\nrf2 = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=-1, n_estimators=500,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,  class_weight='balanced')\n# treinando o modelo RF2\nrf2.fit(treino[usadas], treino['BAD'])","5caa0ae4":"#relizando a predicao do RF2 com base teste\npred_teste2 = rf2.predict(teste[usadas])\n\n#m\u00e9trica para RF2 validacao\naccuracy_score(teste['BAD'],pred_teste2), balanced_accuracy_score(teste['BAD'],pred_teste2), f1_score(teste['BAD'],pred_teste2)","fb96223a":"# Matriz de Confus\u00e3o - Dados de Valida\u00e7\u00e3o\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_teste2)","c281531b":"# Importar o modelo\nfrom xgboost import XGBClassifier\n\n# Instanciar o modelo\nxgb = XGBClassifier(n_estimators=900, n_jobs=-1, random_state=42, learning_rate=0.05)\n\n# treinando o modelo\nxgb.fit(treino[usadas],treino['BAD']) \n\n","1074bbf8":"# Fazendo predi\u00e7\u00f5es\npred_xgb_teste = xgb.predict(teste[usadas])\n\n# Metr\u00edcas XGB teste\naccuracy_score(teste['BAD'],pred_xgb_teste), balanced_accuracy_score(teste['BAD'],pred_xgb_teste), f1_score(teste['BAD'],pred_xgb_teste)","fefe8c1a":"# Matriz de Confus\u00e3o - Dados de Valida\u00e7\u00e3o\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_xgb_teste)","6acc5cb4":"# Conclus\u00e3o\n\nAo comparar os modelos pela a acur\u00e1cia e o F1 score, o modelo que teve o melhor desenpenho foi o\nXGBoost, ele obteve 0,96 de acuracia e 0,70 de F1 score, o modelo RandonForest Classifier sem ajustes  0.958 e o f1 score foi de 0,66, j\u00e1 o modelo RandomForest Classifier com ajustes  obteve a acuracia de 0.956 e o F1 Score 0,68.\n\nTodos os modelos se sairam bem, mas quando olhamos as matriz de confus\u00e3o, percebemos que os modelos n\u00e3o s\u00e3o bons para prever se o cliente est\u00e3o inadimplente , mas \u00e9 otimo para prever se em dia, os modelos tem muitas falso positivos nesse caso, \n\nPortanto teria que fazer um novo tratamento nos dados e ou uma imputa\u00e7\u00e3o  missing values ou at\u00e9 normalizar os dados, para que seja rodado um novo modelo. ","9d62e289":"No gr\u00e1fico acima observa-se que as vari\u00e1veis que foram geradas dummers, n\u00e3o s\u00e3o importantes para o modelo","4e1a73bb":"\nA Matriz de Confus\u00e3o mostra que a base de dados \u00e9 bastante desbalanceada, Assim, a an\u00e1lise das m\u00e9tricas de especifidade e esfor\u00e7o pode real\u00e7ar os falsos positivos","1e088694":"## Gera\u00e7\u00e3o Amostras de Treino e Teste","2f008b12":"# <center>Dicion\u00e1rio de Dados<\/center>\n\n\n**BAD:** 1 = cliente inadimplente no empr\u00e9stimo 0 = empr\u00e9stimo reembolsado (vari\u00e1vel target)\n\n**LOAN**: Valor do empr\u00e9stimo\n\n**MORTDUE**: Valor devido da hipoteca existente\n\n**VALUE**: valor da propriedade atual\n\n**REASON**: DebtCon = consolida\u00e7\u00e3o da d\u00edvida; HomeImp = melhoramento da casa\n\n**JOB**: Seis categorias profissionais\n\n**YOJ**: Anos no emprego atual\n\n**DEROG**: N\u00famero de principais relat\u00f3rios depreciativos\n\n**DELINQ**: n\u00famero de linhas de cr\u00e9dito inadimplentes\n\n**CLAGE**: Idade da linha comercial mais antiga em meses\n\n**NINQ**: N\u00famero de linhas de cr\u00e9dito recentes\n\n**CLNO**: N\u00famero de linhas de cr\u00e9dito\n\n**DEBTINC**: R\u00e1cio d\u00edvida \/ rendimento","a7dc0f98":"Ao analisar a quantidade de missing na base, temos duas op\u00e7\u00f5es: imputa\u00e7\u00e3o de dados ou a remo\u00e7\u00e3o dos valores com missing, nesse caso eu vou remover os valores missing.","aaf5b443":"# <center>Tratamento dos Dados<\/center>","a4c0c03a":"Observa-se que o grupo que trabalho Other e ProfExe possuem um n\u00famero maior de maus pagadores","5379912d":"O Dataset possui 5.960 linhas e 13 colunas,com exce\u00e7\u00e3o das vari\u00e1veis BAD e LOAN todas as outras vari\u00e1veis possui valores missing e duas vari\u00e1veis object JOB e REASON","9327b4ed":"## Modelo XGBoost","4e3931e7":"* # Verificando os tipos e os valores nulos","f0dcaf71":"Para melhor ajuste ao modelo, foram dummizadas as tabelas com type Object","07d9c0df":"* ### Carregando os dados","cbe6e318":"### An\u00e1lise Descritiva Explorat\u00f3ria\n\n","591b046b":"Ao remover os missing da base a vari\u00e1vel target, continuou muito desbalanceada ","10586a9f":"### Utilizando o RandonForest Classifier com ajuste nos par\u00e2metros\n","ee553fd1":"Estat\u00edsticas de todas vari\u00e1veis quantitativas","97f45081":"Ap\u00f3s o ajustes no modelo, deve um pequena melhora, mas nada muito significativos.","07916ae3":"Ao verificar a an\u00e1lise de correla\u00e7\u00e3o, observa-se que as vari\u00e1veis VALUE X MORTDUE tem a maior correla\u00e7\u00e3o.","c60a2729":"O XGBoost apresentou uma melhora um pouco maior no modelo, contudo n\u00e3o foi significante.","bd983394":"Como podemos observar no gr\u00e1fico acima e na frequ\u00eancia, a distribui\u00e7\u00e3o da vari\u00e1vel Target(BAD) n\u00e3o est\u00e1 equilibrada, provavelmente as pessoas que pagaram v\u00e3o enviesar o modelo.","b31a5de9":"Nota: Neste trabalho foi realizada a modelagem utilzando uma amostra para valida\u00e7\u00e3o inclusive, contudo, devida a baixa quantidade de registros, foi utilizado apenas treino e teste ","673daf2d":"# Importando as bibliotecas","1ca53faa":"Foram ajustados os par\u00e2metros de aumentando o n\u00famero de estimadores para 500, quando o default \u00e9 100, e informando que o n\u00famero de folhas aceitavel para as ramifica\u00e7\u00f5es das \u00e1rvores de decis\u00e3o como 2.","3b299e50":"A avalia\u00e7\u00e3o dos histogramas mostra inicialmente que :\n- A vari\u00e1vel BAD (Target) possui poucos valores 1 para treinamento do modelo\n- A maior parte dos valores totais de financiamento (LOAN) possuem uma distribui\u00e7\u00e3o pr\u00f3xima da normalidade, e os valores a receber(MORTDUE), na m\u00e9dia, s\u00e3o maiores que os totais emprestados. Observa-se o terror dos juros banc\u00e1rios\n- Os valores das propriedades possuem distribui\u00e7\u00e3o pr\u00f3xima dos valores dos financiamentos\n- O DEROG, algo equivalente \u00e0 um aviso de negativa\u00e7\u00e3o do servi\u00e7o de prote\u00e7\u00e3o ao consumidor, \u00e9 baixo, contudo possui uma correla\u00e7\u00e3o pr\u00f3xima de moderada(p=0,25) com os maus pagadores.\n- O DELINQ, linhas de cr\u00e9dito com inadimpl\u00eancia, tamb\u00e9m possui correla\u00e7\u00e3o pr\u00f3xima \u00e0 moderada(p=0,27) com maus pagadores\n- O n\u00famero de linhas de cr\u00e9dito possui correla\u00e7\u00e3o, mas a intensidade \u00e9 menor(p=0,13), com maus pagadores\n- Por fim, a base possui um indicador (D\u00e9bitos\/Renda) que possui uma correla\u00e7\u00e3o pr\u00f3xima a moderada (p=0,23), sendo um bom indicador.","1db9e8c3":"### Featuring Engineering","af601c68":"<h1><center>Centro Universit\u00e1rio IESB<\/center><\/h1>\nCurso: Ci\u00eancias de Dados - P\u00f5s-Gradua\u00e7\u00e3o<br>\nAluno: ROBSON BATISTA DA SILVA<br>\nDisciplina: Data Mining e Machine Learning II\n\n   ","fbc6009e":"# <center>**Explora\u00e7\u00e3o dos Dados**<\/center>","c0a5b7b3":"## Modelo RandomForest Classifier","b900cba4":"A vari\u00e1vel raz\u00e3o do d\u00e9bito apresentou na categoria Debtcon a maior quatidade de n\u00e3o pagar, mas por um outro lado essa categoria \u00e9 aque mais quinta seu debidos."}}