{"cell_type":{"e51f4143":"code","cca30f5a":"code","5d6f17ac":"code","36762302":"code","f4de3a28":"code","c34aad90":"code","a20453ed":"code","c8940415":"code","c44ae9df":"code","a2bef5f9":"code","bf73fa7e":"code","4d1023c9":"code","a9fba6b4":"code","64921a1c":"code","e20b0603":"code","559dbcfe":"code","d66345b4":"code","5349df1f":"code","ceb63e10":"code","75aa5eee":"code","608a6bd8":"code","a32e632e":"code","57717844":"code","9449f6ff":"code","e5da37cc":"code","c4bf091d":"code","5b8cc75a":"code","1a15b164":"markdown","23f7a4bc":"markdown","12b6886f":"markdown","1da3c896":"markdown","48b64cfd":"markdown","3c270158":"markdown","ddda486f":"markdown","08d023e9":"markdown","10636256":"markdown","761bb5ac":"markdown","8a9aa79a":"markdown","3dc3c43a":"markdown","634ab24e":"markdown","64a2e6ed":"markdown","cad0ef2b":"markdown","bf5875ba":"markdown","82fdf520":"markdown","ce21e7b5":"markdown","932bc71a":"markdown","88a1d856":"markdown","0370f95b":"markdown","2f1763fe":"markdown","2addbdc6":"markdown"},"source":{"e51f4143":"import cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport os\nimport math\n%matplotlib inline\nimport time\n\n#pytorch utility imports\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision.utils import make_grid\n\n#neural net imports\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nstart = torch.cuda.Event(enable_timing=True) #time measure during cuda training\nend = torch.cuda.Event(enable_timing=True)","cca30f5a":"test_df = pd.read_csv('..\/input\/mnist-in-csv\/mnist_test.csv')\ntrain_df = pd.read_csv('..\/input\/mnist-in-csv\/mnist_train.csv')\ntrain_df.head()","5d6f17ac":"train_labels = train_df['label'].values # converting to numpy also\n\ntest_labels=test_df['label'].values\ntrain_images = (train_df.iloc[:,1:].values).astype('float32')\ntest_images = (test_df.iloc[:,1:].values).astype('float32')","36762302":"print(\"train images shape\",train_images.shape)\nprint(\"train labels shape\",train_labels.shape)\nprint(\"test images shape\",test_images.shape)\nprint(\"test labels shape\",test_labels.shape)","f4de3a28":"train_images = train_images.reshape(train_images.shape[0], 28, 28)\ntest_images = test_images.reshape(test_images.shape[0], 28, 28)\nprint(train_images.shape)\nprint(test_images.shape)","c34aad90":"#train samples\nfor i in range(6, 9):\n    plt.subplot(330 + (i+1))\n    plt.imshow(train_images[i].squeeze(), cmap=plt.get_cmap('gray'))\n    plt.title(train_labels[i])\n","a20453ed":"train_images_tensor = torch.tensor(train_images)\/255.0 #default torch.FloatTensor\ntrain_labels_tensor = torch.tensor(train_labels)\ntrain_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n\ntest_images_tensor = torch.tensor(test_images)\/255.0\ntest_labels_tensor = torch.tensor(test_labels)\ntest_tensor = TensorDataset(test_images_tensor, test_labels_tensor)","c8940415":"train_loader = DataLoader(train_tensor, batch_size=16, num_workers=2, shuffle=True)\ntest_loader = DataLoader(test_images_tensor, batch_size=16, num_workers=2, shuffle=False)","c44ae9df":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(784, 548)\n        self.bc1 = nn.BatchNorm1d(548)\n        \n        self.fc2 = nn.Linear(548, 252)\n        self.bc2 = nn.BatchNorm1d(252)\n        \n        self.fc3 = nn.Linear(252, 10)\n        \n        \n    def forward(self, x):\n        x = x.view((-1, 784))\n        h = self.fc1(x)\n        h = self.bc1(h)\n        h = F.relu(h)\n        h = F.dropout(h, p=0.5, training=self.training)\n        \n        h = self.fc2(h)\n        h = self.bc2(h)\n        h = F.relu(h)\n        h = F.dropout(h, p=0.2, training=self.training)\n        \n        h = self.fc3(h)\n        out = F.log_softmax(h)\n        return out\n\nmodel = Model()","a2bef5f9":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nif (device.type=='cuda'):\n    model.cuda() # convert model to cuda model\n\n    \noptimizer = optim.Adam(model.parameters(), lr=0.001) #adam optimizer from optim module","bf73fa7e":"if (device.type=='cuda'):\n    start.record() #timer start\n\nmodel.train()\n\n\nlosses = []\nfor epoch in range(20):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Get Samples\n        if (device.type=='cuda'):\n            data, target = Variable(data.cuda()), Variable(target.cuda())\n        else:\n            data, target = Variable(data), Variable(target) # making group of 16\n            \n        \n        # Init\n        optimizer.zero_grad() #making gradient zero for new mini-batch. \n\n        # Predict\n        y_pred = model(data) \n         \n        \n        # Calculate loss\n        loss = F.cross_entropy(y_pred, target)\n        losses.append(loss.data)\n        \n        # Backpropagation\n        loss.backward()  #It computes gradient of loss w.r.t all the parameters and store them in (parameter.grad) attribute.\n        optimizer.step() #optimizer.step() updates all the parameters based on (parameter.grad)\n        \n        \n        # Display\n        #if batch_idx % 100 == 1:\n        print('\\r Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx \/ len(train_loader), loss.data), end='')\n            \n    print()\n    \nif (device.type=='cuda'):\n    end.record()","4d1023c9":"if (device.type=='cuda'):\n    evaluate_x=test_images_tensor.cuda()\n    evaluate_y=test_labels_tensor.cuda()\nelse:\n    evaluate_x=test_images_tensor\n    evaluate_y=test_labels_tensor\n    \n\noutput = model(evaluate_x)\n\npred = output.data.max(1)[1]\nd = pred.eq(evaluate_y.data).cpu()\na=(d.sum().data.cpu().numpy())\nb=d.size()\nb=torch.tensor(b)\nb=(b.sum().data.cpu().numpy())\naccuracy = a\/b\n\nprint('Accuracy:', accuracy)","a9fba6b4":"if (device.type=='cuda'):\n    torch.cuda.synchronize()\n    print(start.elapsed_time(end)\/1000,\"sec\")","64921a1c":"test_df = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv')\ntrain_df = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv')","e20b0603":"print((train_df['label'].unique()).shape )# There are 24 possible labels, 9=J and 25=Z require motion so they are absent.\nprint(np.sort(train_df['label'].unique()))","559dbcfe":"train_labels = train_df['label'].values\ntest_labels=test_df['label'].values\ntrain_images = (train_df.iloc[:,1:].values).astype('float32')\ntest_images = (test_df.iloc[:,1:].values).astype('float32')","d66345b4":"print(\"train images shape\",train_images.shape)\nprint(\"train labels shape\",train_labels.shape)\nprint(\"test images shape\",test_images.shape)\nprint(\"test labels shape\",test_labels.shape)","5349df1f":"train_images = train_images.reshape(train_images.shape[0],1, 28, 28)\ntest_images = test_images.reshape(test_images.shape[0],1, 28, 28)","ceb63e10":"print(train_images.shape)\nprint(test_images.shape)","75aa5eee":"train_images_tensor = torch.tensor(train_images)\/255.0 #default torch.FloatTensor\ntrain_labels_tensor = torch.tensor(train_labels)\ntrain_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n\ntest_images_tensor = torch.tensor(test_images)\/255.0\ntest_labels_tensor = torch.tensor(test_labels)\ntest_tensor = TensorDataset(test_images_tensor, test_labels_tensor)","608a6bd8":"train_loader = DataLoader(train_tensor, batch_size=16, num_workers=2, shuffle=True)\ntest_loader = DataLoader(test_images_tensor, batch_size=16, num_workers=2, shuffle=False)","a32e632e":"import torch.nn.functional as F\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\nfrom torch.optim import Adam, SGD\n\nclass Net(nn.Module):                                           # class Net inherits from predefined Module class in torch.nn\n    def __init__(self):                                         # calling constructor of  parent class\n        super().__init__()                                     \n        \n        \n        self.conv1 = nn.Conv2d(1,32,3)              # 2d convolution layer : (input : 1 image , output : 32 channels , kernel size : 3*3)\n        self.conv2 = nn.Conv2d(32,64,3)\n        self.conv3 = nn.Conv2d(64,128,3)\n        \n        self.linear_in = None                      # used to calculate input of first linear layer by passing fake data through 2d layers\n        x = torch.rand(28,28).view(-1,1,28,28)     # using convs function\n        self.convs(x)\n    \n        self.fc1 = nn.Linear(self.linear_in,512)\n        self.fc2 = nn.Linear(512,26)\n        \n    def convs(self,x):\n        x = F.max_pool2d(F.relu(self.conv1(x)) , (2,2) )      # relu used for activation function \n        x = F.max_pool2d(F.relu(self.conv2(x)) , (2,2) )      # max_pool2d for max pooling results of each kernel with window size 2*2\n        x = F.max_pool2d(F.relu(self.conv3(x)) , (2,2) )\n        \n        if self.linear_in == None:\n            self.linear_in = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]  # input of first linear layer is multiplication of dimensions of ouput \n        return x                                                        # tensor of the 2d layers\n    \n    def forward(self,x):                                    # forward pass function uses the convs function to pass through 2d layers\n        x = self.convs(x)\n        x = x.view(-1,self.linear_in)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = F.log_softmax(x ,dim = -1)                     # log_softmax for finding output neuron with highest value\n        return x\n    \nnet = Net()","57717844":"print(net)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nif (device.type=='cuda'):\n    model.cuda() # CUDA\n\nnet.to(device)\n\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)","9449f6ff":"if (device.type=='cuda'):\n    start.record() \n    \nloss_log = []\nfor epoch in range(20): # loop over dataset multiple times\n    running_loss = 0.0\n    for i, (data,target) in enumerate(train_loader):\n\n        \n        if (device.type=='cuda'):\n            inputs,labels= Variable(data.cuda()), Variable(target.cuda())\n        else:\n            inputs,labels= Variable(data), Variable(target)\n       \n        \n        \n        # zero parameter gradients\n        optimizer.zero_grad()\n        \n        # forward + backward + optimize\n        outputs = net(inputs)\n\n        loss =  F.cross_entropy(outputs, labels)\n        #print(loss)\n        \n   \n        \n        loss.backward()\n        optimizer.step()\n        \n      \n        #if i % 100 == 1:\n        print('\\r Train Epoch: {} [{}\/{} ({:.0f}%)] \\tLoss: {:.6f}'.format( epoch, i * len(data), len(train_loader.dataset),\n                                                                           100. * i \/ len(train_loader), loss.data), end='')\n        \n    print(\"\")\n                \nprint('Finished Training')\nif (device.type=='cuda'):\n    end.record()","e5da37cc":"if (device.type=='cuda'):\n    evaluate_x=test_images_tensor.cuda()\n    evaluate_y=test_labels_tensor.cuda()\nelse:\n    evaluate_x=test_images_tensor\n    evaluate_y=test_labels_tensor\n    \n\noutput = net(evaluate_x)\n\npred = output.data.max(1)[1]\nd = pred.eq(evaluate_y.data).cpu()\na=(d.sum().data.cpu().numpy())\nb=d.size()\nb=torch.tensor(b)\nb=(b.sum().data.cpu().numpy())\naccuracy = a\/b\n\nprint('Accuracy:', accuracy*100)","c4bf091d":"if (device.type=='cuda'):\n    torch.cuda.synchronize()\n    print(start.elapsed_time(end)\/1000,\"sec\")","5b8cc75a":"from sklearn.metrics import f1_score\nprint(\"f1 score =\",f1_score(test_labels, pred.cpu().numpy(), average='macro'))","1a15b164":"### Visualize some training images","23f7a4bc":"### Separating labels and features (pixel)","12b6886f":"## Conclusion\nLets get connected on [Linkedin](https:\/\/www.linkedin.com\/in\/manzoor-bin-mahmood\/)\n\nVisit my [website](https:\/\/manzoormahmood.github.io\/) \n\n### Please **upvote** my work if you like it.","1da3c896":"### Loading dataset","48b64cfd":"### Changing to Tensor","3c270158":"### DataLoader for train and test \n* Batch size 16","ddda486f":"### Reshape features\n*Note: For images reshape will be in 4D*","08d023e9":"### Model\nNet(\n\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  \n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  \n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n  \n  (fc1): Linear(in_features=128, out_features=512, bias=True)\n  \n  (fc2): Linear(in_features=512, out_features=26, bias=True)\n  \n)","10636256":"### Convert numpy data into tensor for PyTorch\n* torch.tensor default uses float\n* feature are divided by 255 to change them into suitable range","761bb5ac":"# What is PyTorch?\n* Its a machine learning library used for NLP, computer vision,etc.\n* Provides Tensor computing with GPU support.\n\n# Modules in Pytorch\n*  **Autograd module:**\nPyTorch uses a method called automatic differentiation. A recorder records what operations have performed, and then it replays it backward to compute the gradients. This method is especially powerful when building neural networks to save time on one epoch by calculating differentiation of the parameters at the forward pass.\n\n* **Optim module:**\ntorch.optim is a module that implements various optimization algorithms used for building neural networks. \n\n* **nn module:**\nPyTorch autograd makes it easy to define computational graphs and take gradients, but raw autograd can be a bit too low-level for defining complex neural networks. This is where the nn module can help.\n\n# Topics covered in this notebook\n* Handwritten Digits Classification (Numerical Data)-**Digit MNIST**\n* Objects Image Classification (Image Data, CNN)-**Sign Language MNIST**\n","8a9aa79a":"### DataLoader","3dc3c43a":"### Model important terms\n\n* nn.Linear : Applies a linear transformation (in_features, out_features)\n* nn.BatchNorm1d : By normalizing the inputs we are able to bring all the inputs features to the same scale\n* nn.functional.relu : Applies the rectified linear unit function element-wise\n* nn.functioncal.dropout : Dropout and offers a very computationally cheap and remarkably effective regularization method to reduce overfitting and improve generalization error in deep neural networks of all kinds.\n\n### **Model used**\n\nModel(\n\n  (fc1): Linear(in_features=784, out_features=548, bias=True)\n  \n  (bc1): BatchNorm1d(548, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  \n  (fc2): Linear(in_features=548, out_features=252, bias=True)\n  \n  (bc2): BatchNorm1d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  \n  (fc3): Linear(in_features=252, out_features=10, bias=True)\n  \n)\n\n\n","634ab24e":"### Importing lib","64a2e6ed":"# Other use case to be covered in upcoming notebook\n\n* R-CNN\n* Sentiment Text Classification (Text Data, RNN)\n* Image Style Transfer (Transfer Learning)","cad0ef2b":"### Reshape features","bf5875ba":"### Importing dataset ","82fdf520":"### Predicting the output and accuracy","ce21e7b5":"### Separating labels and features","932bc71a":"# Objects Image Classification (Image Data, CNN)-Sign Language MNIST","88a1d856":"### Calculating the F1 score","0370f95b":"# Handwritten Digits Classification (Numerical Data)-Digit MNIST","2f1763fe":" ### Time used for training if cuda is used","2addbdc6":"### Checking for cuda availability"}}