{"cell_type":{"08d7ff1c":"code","5761b379":"code","27dfdd55":"code","f644193c":"markdown","3f2a19b9":"markdown"},"source":{"08d7ff1c":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# loading dummy submission file\nsub_file = pd.read_csv(r'..\/input\/tspfeb2021\/submission_blend_1.csv')\n\n# loading data including 15 best scores\ndf_sub = pd.read_csv(r'..\/input\/tspfeb2021\/best_blend_1.csv')\ndf_sub = df_sub.iloc[:, :15]\n\n# a rough correlation based visualization of 15 best scores\nplt.figure(figsize=(10,10))\nsns.heatmap(df_sub.corr(), cmap='Spectral')\nplt.ylabel('file index numbers')\nplt.xlabel('file index numbers')\nplt.show()","5761b379":"# basic analysis and visualization of subgroups in different color.\nplt.figure(figsize=(12, 5))\ndf_mean_corr = pd.DataFrame({'mean_corr': df_sub.corr().mean()})\ndf_mean_corr = df_mean_corr.sort_values('mean_corr', ascending=False)\ndf_mean_corr = df_mean_corr.reset_index()\n\nplt.plot(df_mean_corr.index[:2], df_mean_corr['mean_corr'].values[:2], 'o', ms=10)\nplt.plot(df_mean_corr.index[2:6], df_mean_corr['mean_corr'].values[2:6], 'o', ms=10)\nplt.plot(df_mean_corr.index[6:10], df_mean_corr['mean_corr'].values[6:10], 'o', ms=10)\nplt.plot(df_mean_corr.index[10:11], df_mean_corr['mean_corr'].values[10:11], 'o', ms=10)\nplt.plot(df_mean_corr.index[11:12], df_mean_corr['mean_corr'].values[11:12], 'o', ms=10)\nplt.plot(df_mean_corr.index[12:13], df_mean_corr['mean_corr'].values[12:13], 'o', ms=10)\nplt.plot(df_mean_corr.index[13:14], df_mean_corr['mean_corr'].values[13:14], 'o', ms=10)\nplt.plot(df_mean_corr.index[14:], df_mean_corr['mean_corr'].values[14:], 'o', ms=10)\nplt.xticks([*range(len(df_mean_corr))], df_mean_corr['index'].tolist())\nplt.title('determination of sub_groups')\nplt.ylabel('a correlation related index')\nplt.xlabel('file index numbers')\nplt.show()","27dfdd55":"# a linear combination to achieve much better scores\ndf_sub['weighted_avg'] = abs(1 * (\n    -70 * ( -5 * df_sub['4'] + 50 * df_sub['9'] ) \/ 45 +\n    150 * ( 50 * df_sub['0'] + 50 * df_sub['3'] + 20 * df_sub['5'] + 1 * df_sub['10'] ) \/ 121 +\n     -5 * ( 10 * df_sub['1'] + 20 * df_sub['6'] + 10 * df_sub['7'] + 5 * df_sub['11']) \/ 45 +\n     -5 * ( 50 * df_sub['2'] +  7 * df_sub['8'] + 5 * df_sub['12'] + 5 * df_sub['13'] +1 * df_sub['14']) \/ 68\n) \/ 70 )\n\n# # create the final submission file\nsubmission = pd.DataFrame({'ID': sub_file.ID, 'target': df_sub['weighted_avg'].tolist()})\nsubmission.to_csv(r'submission.csv', index=False)","f644193c":"# Blend Boosting study on Tabular Playground Series - Feb 2021:\nHere I share with you a systematic blend boosting study on Tabular Playground Series - Feb 2021. (https:\/\/www.kaggle.com\/c\/tabular-playground-series-feb-2021). I just collect some submission files on the kaggle.\n\nBasically, I start to analysis of correlations, then decide to sort them according to their sum of correlation values in between. This lets me divide ~15 scores into 4 subgroups. Then I make internal linear calibration in each subgroup by considering their scores on the Kaggle. If you spend much more time, you can always achieve much betters scores.","3f2a19b9":"### It gets a 0.84174 as public score, and looks the best score on Kaggle so far ;-)"}}