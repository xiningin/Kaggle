{"cell_type":{"7bf9b5f8":"code","217b410a":"code","f969acfd":"code","d21479c7":"code","e959bdcb":"code","b9615abb":"code","93ab24b7":"code","8d280b9b":"code","801a7ca8":"code","d8fb73af":"code","a0efe571":"code","55a74fe5":"code","06a1be4f":"code","e51b58bf":"code","13a45a38":"code","c63fe202":"code","3f9b7d29":"code","6a31f891":"code","b685179c":"code","9a6cb3f4":"code","25c44a4e":"code","a459bc1b":"code","a52243a5":"code","1f4d9781":"code","484719f0":"code","93a07b00":"code","b84fd0b6":"code","061f970b":"code","bb006219":"code","20c9f546":"code","29fc075d":"code","79f52cbe":"code","80ac2917":"code","b1686258":"code","97e1aa1a":"code","6acbb3f7":"code","a4596a4e":"code","05ff0eb2":"code","415e0cf4":"code","5fe774f3":"markdown","bb1a592e":"markdown","65d37cdd":"markdown","b7a99dfb":"markdown","8a2a1f4f":"markdown","4dae8ec0":"markdown","025fba97":"markdown","f1e95fa8":"markdown","9acd4ad4":"markdown","670309b1":"markdown","e761b5ee":"markdown","31f8e634":"markdown","5b4d43b1":"markdown","da20ddbd":"markdown","b4c743f8":"markdown","5fc0957d":"markdown","89f07778":"markdown","b0a87d91":"markdown"},"source":{"7bf9b5f8":"import numpy as np \nimport pandas as pd \n\nimport os\nimport re\n\nimport matplotlib as plt\nimport seaborn as sns\nimport itertools\nimport nltk \nimport string\nfrom wordcloud import WordCloud\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob,Word\nfrom collections import Counter\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\nimport plotly.figure_factory as ff \nfrom plotly.offline import iplot\nfrom plotly import tools\ncolors = px.colors.qualitative.Prism\npio.templates.default = \"plotly_white\"\n\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom wordcloud import WordCloud\nfrom scipy.stats import probplot\nfrom plotly.offline import iplot\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\nimport nltk\nfrom nltk import tokenize\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\n\ndef cprint(string:str, end=\"\\n\"):\n    \"\"\"\n    A little utility function for printing and stuff\n    \"\"\"\n    _pprint(f\"[black]{string}[\/black]\", end=end)\n    \nfrom transformers import *\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport random\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom tqdm import tqdm\n    \n    \n# see our files:\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n","217b410a":"train_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\nsample_submission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","f969acfd":"train_df.head()","d21479c7":"train_df.info()","e959bdcb":"test_df.head()","b9615abb":"test_df.info()","93ab24b7":"sample_submission.head()","8d280b9b":"def plot_target(target):\n    \n    print(f'{target}\\n{\"-\" * len(target)}')\n        \n    print(f'Mean: {train_df[target].mean():.4}  -  Median: {train_df[target].median():.4}  -  Std: {train_df[target].std():.4}')\n    print(f'Min: {train_df[target].min():.4}  -  25%: {train_df[target].quantile(0.25):.4}  -  50%: {train_df[target].quantile(0.5):.4}  -  75%: {train_df[target].quantile(0.75):.4}  -  Max: {train_df[target].max():.4}')\n    print(f'Skew: {train_df[target].skew():.4}  -  Kurtosis: {train_df[target].kurtosis():.4}')\n    missing_values_count = train_df[train_df[target].isnull()].shape[0]\n    training_samples_count = train_df.shape[0]\n    print(f'Missing Values: {missing_values_count}\/{training_samples_count} ({missing_values_count * 100 \/ training_samples_count:.4}%)')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(24, 8), dpi=100)\n    sns.kdeplot(train_df[target], label=target, fill=True, ax=axes[0])\n    axes[0].axvline(train_df[target].mean(), label=f'{target} Mean', color='r', linewidth=2, linestyle='--')\n    axes[0].axvline(train_df[target].median(), label=f'{target} Median', color='b', linewidth=2, linestyle='--')\n    probplot(train_df[target], plot=axes[1])\n    axes[0].legend(prop={'size': 15})\n    \n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=12)\n        axes[i].tick_params(axis='y', labelsize=12)\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('')\n    axes[0].set_title(f'{target} Distribution in Training Set', fontsize=15, pad=12)\n    axes[1].set_title(f'{target} Probability Plot', fontsize=15, pad=12)\n\n    plt.show()\n\nplot_target('target')","801a7ca8":"fig, ax = plt.subplots(figsize=(16, 6))\nno_baseline = train_df['standard_error'] > 0\nsns.scatterplot(x=train_df.loc[no_baseline, 'target'], y=train_df.loc[no_baseline, 'standard_error'], ax=ax)\nax.set_title(f'standard_error vs target', size=18, pad=15)\nax.set_xlabel('target', size=15, labelpad=12)\nax.set_ylabel('standard_error', size=15, labelpad=12)\nax.tick_params(axis='x', labelsize=12, pad=10)\nax.tick_params(axis='y', labelsize=12, pad=10)\nplt.show()","d8fb73af":"def get_wordnet_pos(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\nlemmatizer = WordNetLemmatizer()\ndef clean_text(text):\n    text = re.sub('[^A-Za-z0-9]+', ' ', text.lower())\n    words = nltk.word_tokenize(text)\n    tagged = nltk.pos_tag(words)\n    words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n    words = [word for word in words if word not in stopwords.words('english')]\n    return words\n\ndef get_ngrams(words, n):\n    return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n\ncorpus = []\nfor text, target in train_df[['excerpt', 'target']].itertuples(index=False):\n    sentences = []\n    for sentence in tokenize.sent_tokenize(text):\n        words = clean_text(sentence)\n        unigrams = get_ngrams(words, n=1)\n        bigrams = get_ngrams(words, n=2)\n        trigrams = get_ngrams(words, n=3)\n        sentences.append(words)\n    corpus.append({\n        'target' : target,\n        'text' : text,\n        'sentences' : sentences,\n        'unigrams' : unigrams,\n        'bigrams' : bigrams,\n        'trigrams' : trigrams,\n    })\n\ncorpus = sorted(corpus, key=lambda x: x['target'])","a0efe571":"top_lowest = corpus[:500]\nlowest_target_sentence_lengths = [ \\\n    np.mean([len(sentence) for sentence in datapoint['sentences']]) \\\n    for datapoint in top_lowest \\\n]\n\ntop_highest = corpus[-500:]\nhighest_target_sentence_lengths = [ \\\n    np.mean([len(sentence) for sentence in datapoint['sentences']]) \\\n    for datapoint in top_highest \\\n]\n\ntop_lowest_mean = np.mean(lowest_target_sentence_lengths)\ntop_lowest_std = np.std(lowest_target_sentence_lengths)\ntop_highest_mean = np.mean(highest_target_sentence_lengths)\ntop_highest_std = np.std(highest_target_sentence_lengths)\n\nfig, ax = plt.subplots(figsize=(8, 10), facecolor='#f6f5f5')\nax.errorbar(\n    x=[0, 1],\n    y=[top_lowest_mean, top_highest_mean],\n    yerr=[top_lowest_std, top_highest_std],\n    fmt='o'\n)\n\nax.set_title('Average sentence length and Readability')\nax.set_ylabel('Sentence length')\nax.set_xticks([0, 1])\nax.set_xticklabels(['Top lowest readability', 'Top highest readability'])\n\nplt.show()","55a74fe5":"def text_cleaning(excerpt):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    excerpt = ''.join([k for k in text if k not in string.punctuation])\n    excerpt = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n#     text = re.sub(\"\/'+\/g\", ' ', text)\n    \n    return text","06a1be4f":"%%time\ntqdm.pandas()\ntrain_df['excerpt'] = train_df['excerpt'].progress_apply(text_cleaning)","e51b58bf":"train_df['temp_list'] = train_df['excerpt'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train_df['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","13a45a38":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(train_df['excerpt'])","c63fe202":"import numpy as np \nimport pandas as pd \n\nimport os\nfrom transformers import *\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport random\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom tqdm import tqdm","3f9b7d29":"def set_seed(seed = 0):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nseed = 82\nrandom_state = set_seed(seed)","6a31f891":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","b685179c":"train_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')","9a6cb3f4":"train_df['excerpt_len'] = train_df.excerpt.apply(lambda x: len(x.split()))","25c44a4e":"print(train_df.excerpt_len.max())","a459bc1b":"train_df['fold'] = -1\ngkf = KFold(n_splits=5)\nfor fold, (train, val) in enumerate(gkf.split(train_df.excerpt, train_df.target)):\n    train_df.loc[val,'fold']=fold\n\nfold = 0\nvalidation_df = train_df[train_df.fold==0].reset_index(drop=True)\ntrain_df = train_df[train_df.fold!=0].reset_index(drop=True)","a52243a5":"print(train_df.target.mean(), validation_df.target.mean())","1f4d9781":"class Data(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):       \n        excerpt = self.data.excerpt[idx]\n        target = self.data.target[idx]\n        return excerpt, target","484719f0":"train_data = Data(data = train_df) \ntrain_loader = DataLoader(dataset = train_data, shuffle=True, batch_size = 8)\n\nval_data = Data(data = validation_df) \nval_loader = DataLoader(dataset = val_data, shuffle=False, batch_size = 64)","93a07b00":"class ReadabilityModel(PreTrainedModel): \n    def __init__(self, conf):\n        super(ReadabilityModel, self).__init__(conf) \n        self.roberta = RobertaModel.from_pretrained(model_name, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l1 = nn.Linear(768 * 1, 1)\n        torch.nn.init.normal_(self.l1.weight, std=0.02)\n    \n    def forward(self, ids, mask):\n        out = self.roberta(\n            input_ids=ids,\n            attention_mask=mask\n        )\n        out = out['hidden_states']\n        out = out[-1]\n        out = self.drop_out(out)\n        out = torch.mean(out, 1, True)\n        \n        preds = self.l1(out)\n\n        preds = preds.squeeze(-1).squeeze(-1)\n\n        return preds","b84fd0b6":"model_name = 'roberta-base'\ntokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n\nmodel_config = RobertaConfig.from_pretrained(model_name)\nmodel_config.output_hidden_states = True\n\nmodel = ReadabilityModel(model_config)\nmodel = model.to(device)\n\noptimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\nscheduler = get_constant_schedule_with_warmup(optimizer, 100)\n\nloss_fct = nn.MSELoss()\n\nepochs = 3","061f970b":"for epoch in range(epochs):\n    model.train()\n    for i, (excerpts, targets) in enumerate(tqdm(train_loader)):\n        optimizer.zero_grad()\n        batch = tokenizer(list(excerpts), truncation=True, padding=True, return_tensors='pt', add_special_tokens=True)\n        input_ids = batch['input_ids']\n        input_ids = input_ids.to(device, dtype=torch.long)\n        attention_mask = batch['attention_mask']\n        attention_mask = attention_mask.to(device, dtype=torch.long)\n            \n        targets=torch.tensor(targets).to(device, dtype=torch.float)\n \n        preds = model(input_ids, attention_mask)       \n        \n        loss = torch.sqrt(loss_fct(preds, targets))\n        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        loss = loss.item()\n        \n        if i==0:\n            loss_train = loss\n        else:\n            loss_train = loss_train + loss  \n    loss_train = loss_train\/(i+1)\n    \n    model.eval()\n    with torch.no_grad():\n        for i, (excerpts, targets) in enumerate(tqdm(val_loader)):\n            optimizer.zero_grad()\n            batch = tokenizer(list(excerpts), truncation=True, padding=True, return_tensors='pt', add_special_tokens=True)\n            input_ids = batch['input_ids']\n            input_ids = input_ids.to(device, dtype=torch.long)\n            attention_mask = batch['attention_mask']\n            attention_mask = attention_mask.to(device, dtype=torch.long)\n                \n            targets=torch.tensor(targets).to(device, dtype=torch.float)\n     \n            preds = model(input_ids, attention_mask)       \n            \n            loss = torch.sqrt(loss_fct(preds, targets))\n            loss = loss.item()\n            \n            preds = preds.cpu().detach().numpy()\n            targets = targets.cpu().detach().numpy()\n            if i==0:\n                loss_val = loss\n                preds_val = preds\n                targets_val = targets\n            else:\n                loss_val = loss_val + loss  \n                preds_val = np.concatenate((preds_val,preds), axis=None)\n                targets_val = np.concatenate((targets_val,targets), axis=None)\n                \n        loss_val = loss_val \/ (i+1)\n        rms_val = mean_squared_error(targets_val, preds_val, squared=False)\n        print('Epoch: {} - Loss: {:.6f} - Loss val: {:.6f} - RMSE: {:.3f}'.format(\n            epoch + 1, loss_train, loss_val, rms_val))","bb006219":"torch.save(model.state_dict(), 'roberta_baseline.bin')","20c9f546":"test_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","29fc075d":"test_df","79f52cbe":"class Data(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):       \n        excerpt = self.data.excerpt[idx]\n        return excerpt","80ac2917":"test_data = Data(data = test_df) \ntest_loader = DataLoader(dataset = test_data, shuffle=False, batch_size = 64)","b1686258":"class ReadabilityModel(PreTrainedModel): \n    def __init__(self, conf):\n        super(ReadabilityModel, self).__init__(conf) \n        self.roberta = RobertaModel(config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l1 = nn.Linear(768 * 1, 1)\n        torch.nn.init.normal_(self.l1.weight, std=0.02)\n    \n    def forward(self, ids, mask):\n        out = self.roberta(\n            input_ids=ids,\n            attention_mask=mask\n        )\n        out = out['hidden_states']\n        out = out[-1]\n        out = self.drop_out(out)\n        out = torch.mean(out, 1, True)\n        \n        preds = self.l1(out)\n\n        preds = preds.squeeze(-1).squeeze(-1)\n\n        return preds","97e1aa1a":"tokenizer = RobertaTokenizerFast.from_pretrained('..\/input\/robertabase', model_max_length=514) \n\nmodel_config = RobertaConfig()\nmodel_config.output_hidden_states = True\nmodel_config.max_position_embeddings=514\nmodel_config.vocab_size = 50265\nmodel_config.type_vocab_size = 1\n\nmodel = ReadabilityModel(model_config)\nif torch.cuda.is_available():\n    model.load_state_dict(torch.load(\"..\/input\/commonlit-readability-roberta-simple-baseline\/roberta_baseline.bin\"))\nelse: \n    model.load_state_dict(torch.load(\"..\/input\/commonlit-readability-roberta-simple-baseline\/roberta_baseline.bin\", map_location=torch.device('cpu')))\nmodel = model.to(device)","6acbb3f7":"model.eval()\nwith torch.no_grad():\n    for i, excerpts in enumerate(tqdm(test_loader)):\n        batch = tokenizer(list(excerpts), truncation=True, padding=True, return_tensors='pt', add_special_tokens=False)\n        input_ids = batch['input_ids']\n        input_ids = input_ids.to(device, dtype=torch.long)\n        attention_mask = batch['attention_mask']\n        attention_mask = attention_mask.to(device, dtype=torch.long)\n            \n        preds = model(input_ids, attention_mask)       \n        preds = preds.cpu().detach().numpy()\n\n        if i==0:\n            preds_test = preds\n        else:\n            preds_test = np.concatenate((preds_test,preds), axis=None)","a4596a4e":"submission = pd.DataFrame({'id': test_df.id, 'target': preds_test})","05ff0eb2":"submission.to_csv('\/kaggle\/working\/submission.csv', index=False)","415e0cf4":"submission","5fe774f3":"**Thank you so much the notebooks:**\n\n*  [CommonLit Readibility Prize Extensive EDA + Model](http:\/\/)\n*  [CommonLit readability: RoBerta inference](http:\/\/)\n*  [CommonLit Readability Prize - EDA](http:\/\/)","bb1a592e":"> **Target and sentence length**","65d37cdd":"b. Inferencing the  model","b7a99dfb":"**standard_error vs target**","8a2a1f4f":"**Cleaning the data**","4dae8ec0":"Let's see the data insight first!","025fba97":"# **2. Reading data**","f1e95fa8":"**Target probability**","9acd4ad4":"# **3. Data analysis**","670309b1":"> **So, as a result**: Texts with shorter sentence lengths are often easier to read.","e761b5ee":"**So, as a result,** around -1 we have more target and less error.","31f8e634":"**Hi every one**\n\n**Q.** How much is readibility of text(excerpts)**?**\n\n**A.** Data analysis and RoBerta model can be helpful!\n\n","5b4d43b1":"Great, there aren't any null! ","da20ddbd":"# 4. Modelling","b4c743f8":"a. Training the model","5fc0957d":"**RoBerta model**","89f07778":"# **1. Importing modules and libraries**","b0a87d91":"And now, WordCloud to visuale the most common words in the all excerpts of train data!\nAltough befor that it is needed to clean train data!"}}