{"cell_type":{"28eebfad":"code","65a3d4c3":"code","b974bff0":"code","e0615c59":"code","36c77207":"code","de5131e3":"code","ff108f87":"code","59a38df0":"code","805b1797":"code","cb2e809d":"code","e32ed45b":"code","cc296c25":"code","24237e33":"code","f94b4126":"code","2cd5af0e":"code","add27c18":"code","dc119edc":"code","1c23e4cd":"code","9e5917dd":"code","50271bc1":"code","4bbd1bde":"code","509fd199":"markdown","6b927372":"markdown","8b16e07f":"markdown","d9921b9c":"markdown","ea2718d1":"markdown","b1f11c43":"markdown","e3f02c09":"markdown","e6a1eb62":"markdown","9d47a14d":"markdown","53f29dee":"markdown","6b379c94":"markdown"},"source":{"28eebfad":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n\nimport torch \nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom tqdm.notebook import tqdm\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","65a3d4c3":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')\ndf_sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","b974bff0":"X = df_train.drop(['id', 'loss'], axis = 1)\ny = df_train['loss']","e0615c59":"Test = df_test.drop(['id'], axis = 1).values\nTest.shape","36c77207":"scaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)\ny = y.values\nTest = scaler.transform(Test)\nX.shape, y.shape, Test.shape","de5131e3":"X = np.vstack((X, X[:880]))\ny = np.concatenate((y, y[:880]), axis=0)\nX.shape, y.shape","ff108f87":"class CustomTabulaDataset(Dataset):\n    def __init__(self, labels, features, transform=None, target_transform=None):\n        self.labels = torch.from_numpy(labels.astype(np.float32))\n        self.features = torch.from_numpy(features.astype(np.float32))\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        feature = self.features[idx]\n        label = self.labels[idx]\n        return feature, label","59a38df0":"batch_size = 64\n#batch_size = 100\n\nfeatures_train, features_test, targets_train, targets_test = train_test_split(X, \n                                                                              y, \n                                                                              test_size=0.2,\n                                                                              shuffle = True)\n\ntrain_data = CustomTabulaDataset(targets_train, features_train)\ntest_data = CustomTabulaDataset(targets_test, features_test)\n\n# Create data loaders.\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\nfor featur, target in test_dataloader:\n    print(\"Shape of X [N, C, H, W]: \", featur.shape)\n    print(\"Shape of y: \", target.shape, target.dtype)\n    break","805b1797":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using {} device\".format(device))","cb2e809d":"class TorchNNModel(torch.nn.Module):\n    def __init__(self, dim_inp, L640, L384, L256, L128, L64, L32, L16, dim_out):\n        super(TorchNNModel, self).__init__()\n        self.features = nn.Sequential(\n            nn.Linear(dim_inp, L640),\n#             nn.Dropout(p=0.5),\n            nn.ELU(),\n            nn.Linear(L640, L640),\n            nn.ELU(),\n            nn.Linear(L640, L384),\n            nn.ELU(),\n            nn.Linear(L384, L384),\n            nn.ELU(),\n#             nn.Dropout(p=0.5),\n            nn.Linear(L384, L256),\n#             nn.Dropout(p=0.5),\n            nn.ELU(),\n            nn.Linear(L256, L256),\n            nn.ELU(),\n            nn.Linear(L256, L128),\n#             nn.Dropout(p=0.5),\n            nn.ELU(),\n            nn.Linear(L128, L128),\n            nn.ELU(),\n            nn.Linear(L128, L64),\n            nn.ELU(),\n            nn.Linear(L64, L64),\n            nn.ELU(),\n            nn.Linear(L64, L32),\n            nn.ELU(),\n            nn.Linear(L32, L32),\n            nn.ELU(),\n            nn.Linear(L32, L16),\n            nn.ELU(),\n            nn.Linear(L16, L16),\n            nn.ELU(),\n            nn.Linear(L16, L16),\n            nn.ELU(),\n            nn.Linear(L16, L32),\n            nn.ELU(),\n            nn.Linear(L32, L32),\n            nn.ELU(),\n            nn.Linear(L32, L64),\n            nn.ELU(),\n            nn.Linear(L64, L64),\n            nn.ELU(),\n            nn.Linear(L64, L128),\n            nn.ELU(),\n            nn.Linear(L128, L128),\n            nn.ELU(),\n            nn.Linear(L128, L256),\n            nn.ELU(),\n            nn.Linear(L256, L256),\n            nn.ELU(),\n            nn.Linear(L256, L384),\n            nn.ELU(),\n            nn.Linear(L384, L384),\n            nn.ELU(),\n            nn.Linear(L384, L640),\n            nn.ELU(),\n            nn.Linear(L640, L640),\n            nn.ELU(),\n            nn.Linear(L640, dim_out)           \n        )\n    \n    def forward(self, x):\n        x1 = self.features(x)\n        return x1\n\ndim_inp, L640, L384, L256, L128, L64, L32, L16, dim_out = 100, 640, 384, 256, 128, 64, 32, 16, 1\n\nmodel = TorchNNModel(dim_inp, L640, L384, L256, L128, L64, L32, L16, dim_out).to(device)\nprint(model)","e32ed45b":"# EarlyStopping\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print            \n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","cc296c25":"def train_model(model, batch_size, patience, n_epochs):\n    \n    # to track the training loss as the model trains\n    train_losses = []\n    # to track the validation loss as the model trains\n    valid_losses = []\n    # to track the average training loss per epoch as the model trains\n    avg_train_losses = []\n    # to track the average validation loss per epoch as the model trains\n    avg_valid_losses = [] \n    \n    # initialize the early_stopping object\n    early_stopping = EarlyStopping(patience=patience, delta = 5, verbose=True)\n    \n    for epoch in tqdm(range(1, n_epochs + 1)):\n\n        ###################\n        # train the model #\n        ###################\n        model.train() # prep model for training\n        for batch, (data, target) in enumerate(train_dataloader, 1):\n            # clear the gradients of all optimized variables\n            data = data.to(device=device)\n            target = target.to(device=device)\n            \n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # record training loss\n            train_losses.append(loss.item())\n\n        ######################    \n        # validate the model #\n        ######################\n        model.eval() # prep model for evaluation\n        for data, target in test_dataloader:\n            data = data.to(device=device)\n            target = target.to(device=device)\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the loss\n            loss = criterion(output, target)\n            # record validation loss\n            valid_losses.append(loss.item())\n\n        # print training\/validation statistics \n        # calculate average loss over an epoch\n        train_loss = np.average(train_losses)\n        valid_loss = np.average(valid_losses)\n        avg_train_losses.append(train_loss)\n        avg_valid_losses.append(valid_loss)\n        \n        epoch_len = len(str(n_epochs))\n        \n        print_msg = (f'[{epoch:>{epoch_len}}\/{n_epochs:>{epoch_len}}] ' +\n                     f'train_loss: {train_loss:.5f} ' +\n                     f'valid_loss: {valid_loss:.5f}')\n        \n        print(print_msg)\n        \n        # clear lists to track next epoch\n        train_losses = []\n        valid_losses = []\n        \n        # early_stopping needs the validation loss to check if it has decresed, \n        # and if it has, it will make a checkpoint of the current model\n        early_stopping(valid_loss, model)\n        \n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n        \n    # load the last checkpoint with the best model\n    model.load_state_dict(torch.load('checkpoint.pt'))\n\n    return  model, avg_train_losses, avg_valid_losses","24237e33":"criterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.Adam(model.parameters(), lr= 1e-7)","f94b4126":"batch_size = 100\nn_epochs = 500\n\n# early stopping patience; how long to wait after last time validation loss improved.\npatience = 5\n\nmodel, train_loss, valid_loss = train_model(model, batch_size, patience, n_epochs)","2cd5af0e":"# visualize the loss as the network trained\nfig = plt.figure(figsize=(10,8))\nplt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\nplt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n\n# find position of lowest validation loss\nminposs = valid_loss.index(min(valid_loss))+1 \nplt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.ylim() # consistent scale\nplt.xlim(0, len(train_loss)+1) # consistent scale\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\nfig.savefig('loss_plot.png', bbox_inches='tight')","add27c18":"with torch.no_grad():\n    pred= model(torch.tensor(Test, dtype=torch.float).to(device=device))\n    print(f'Predicted: \"{pred.shape}\"')","dc119edc":"pred = pred.cpu()","1c23e4cd":"pred = pred.squeeze(1).numpy()\npred = np.absolute(pred)\nresult = pd.DataFrame(pred)","9e5917dd":"result = pd.merge(df_test['id'], result, left_index = True, right_index = True)","50271bc1":"result.columns = df_sub.columns","4bbd1bde":"result.to_csv(\"Torch_result.csv\", index=False)","509fd199":"#### Apply EarlyStopping","6b927372":"#### In my notebook I use code of this guy\n[https:\/\/github.com\/Bjarten\/early-stopping-pytorch](http:\/\/)\n##### (Unfortunetly I can not install this package correctly)","8b16e07f":"#### Prediction transferring from GPU to CPU memory","d9921b9c":"![dcc6Gsm.jpg](attachment:9178075c-59c8-4e8b-94a0-0d0a883aa264.jpg)","ea2718d1":"#### If you find this notebook usefull, please upvote)\n#### If you know how we can improve this net please comment)\n#### Thank you in advance!","b1f11c43":"#### For a (number rows) % 64 == 0 in the table, extend the table by 880 rows\n##### (This is usefull for GPU calculation)","e3f02c09":"![d2071e79c202971ee2aedb0f7239549e.jpg](attachment:73375bd3-a80e-431d-8868-4626125c770d.jpg)","e6a1eb62":"#### You can use pin_memory.\n##### (It`s slightly improve learning speed)","9d47a14d":"### Custom Autoencoder(PyTorch)","53f29dee":"#### Make prediction on Test dataset","6b379c94":"#### Scaling \n##### (Transform \"Train\" and \"Test\" with \"Train\"`s Fit) "}}