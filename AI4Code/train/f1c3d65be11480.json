{"cell_type":{"21ae8c23":"code","10c91e38":"code","0112a869":"code","8ce68a7c":"code","721841a6":"code","633d70d3":"code","cb1e6598":"code","2102aaa4":"code","7f3436d5":"code","41caabe6":"code","650a02a8":"code","0fb2c17e":"code","f2523d64":"code","425f09bf":"code","39556ab4":"code","e0a6b787":"code","8a199d20":"code","b092c80c":"code","1bfa0768":"code","920091f4":"code","357ca849":"code","6a2d1ca6":"code","246b25fd":"code","8d98dafb":"code","99003836":"code","0a08746a":"code","8bd35443":"code","9a75d7a0":"code","ab628ed5":"markdown","26116864":"markdown","1bfe10df":"markdown","8d097d40":"markdown","c97eea28":"markdown","5853bebb":"markdown","ba5f38c0":"markdown","c309ce8d":"markdown","f40304ed":"markdown","b0bfe074":"markdown","4b8e13b0":"markdown","4d251db0":"markdown","e4c8d4ab":"markdown","beb39dbd":"markdown","aac14fcc":"markdown","2ad0b22b":"markdown","78a07f0e":"markdown","393a91b8":"markdown","3578babb":"markdown","4be6d91f":"markdown","8ba7747f":"markdown","7d6deedb":"markdown","9d0e806d":"markdown","aa4304c1":"markdown","50b1433c":"markdown","ae99a206":"markdown","7278d548":"markdown","be577ed7":"markdown","b632f6e2":"markdown","29851cd8":"markdown","ae621090":"markdown","e2be479d":"markdown","57f9ce14":"markdown","e0ff09af":"markdown"},"source":{"21ae8c23":"#@title Import relevant modules\n\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers\n\n# The following lines adjust the granularity of reporting. \npd.options.display.max_rows = 30\npd.options.display.float_format = \"{:.8f}\".format","10c91e38":"column_names = [\n  'country',\n  'alchoholic_beverages',\n  'animal_products',\n  'animal_fats',\n  'aquatic_products',\n  'cereals_excluding_beer',\n  'eggs',\n  'fish_and_seafood',\n  'fruits',\n  'meat',\n  'miscellaneous',\n  'milk_excluding_butter',\n  'offals',\n  'oilcrops',\n  'pulses',\n  'spices',\n  'starchy_roots',\n  'stimulants',\n  'sugar_crops',\n  'sugar_and_sweeteners',\n  'treenuts',\n  'vegetal_products',\n  'vegetal_oils',\n  'vegetables',\n  'obesity',\n  'undernourished',\n  'confirmed',\n  'deaths',\n  'recovered',\n  'active',\n  'population',\n  'unit',\n]\n\ndiet_data = pd.read_csv(\n  filepath_or_buffer='https:\/\/raw.githubusercontent.com\/GrozescuRares\/diet_vs_corona\/master\/diet_vs_corona.csv',\n  skiprows=1,\n  names=column_names,\n)\ndiet_data = diet_data.reindex(np.random.permutation(diet_data.index))\n\ndiet_data.head()","0112a869":"#@title Get statistics on the dataset.\n\ndiet_data.describe()","8ce68a7c":"#@title Get correlation matrix\n\ndiet_data.corr()","721841a6":"# Define features and labels.\nfeature_names = ['animal_products', 'cereals_excluding_beer', 'obesity', 'vegetal_products']\nlabel_name = 'deaths'","633d70d3":"# Inspect features data\ndiet_data[feature_names].head()","cb1e6598":"for feature_name in feature_names:\n  diet_data.hist(column=feature_name)","2102aaa4":"# Get a data frame which only contains the features and the label\ntraining_columns = feature_names + [label_name]\ntraining_df = diet_data[training_columns]\ntraining_df = training_df.astype(np.float32)\n\n# Drop records with nan values\ntraining_df = training_df.dropna()\n\nprint('Dropped records with missing values.')","7f3436d5":"def zscore(mean, std, val):\n  epsilon = 0.000001\n  \n  return (val - mean) \/ (epsilon + std)\n\nz_score_scaled_feature_names = ['animal_products', 'obesity', 'vegetal_products']\nlog_scaled_feature_names = ['cereals_excluding_beer']\n\ntraining_df_copy = training_df.copy()\nz_score_scaled_features = training_df_copy[z_score_scaled_feature_names].copy()\n\n# Apply z-score on 'Animal Products', 'Obesity' and 'Vegetal Products'\nfor feature_name in z_score_scaled_feature_names:\n  mean = z_score_scaled_features[feature_name].mean()\n  std = z_score_scaled_features[feature_name].std()\n  z_score_scaled_features[feature_name] = zscore(mean, std, z_score_scaled_features[feature_name])\n  z_score_scaled_features.hist(column=feature_name)\n\nlog_scaled_features = training_df_copy[log_scaled_feature_names].copy()\nfor feature_name in log_scaled_feature_names:\n  # Apply log scaling for 'Cereals - Excluding Beer'\n  log_scaled_features[feature_name] = np.log(log_scaled_features[feature_name])\n  log_scaled_features.hist(column=feature_name)","41caabe6":"training_df[label_name] = training_df[label_name].astype(np.float32) * 100.0\ntraining_df[label_name] = training_df[label_name].round(4)\ntraining_df[label_name] = training_df[label_name].map(lambda val: np.log(val + 1))\n\ntraining_df.describe()","650a02a8":"training_df.hist(column=label_name)","0fb2c17e":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(training_df[feature_names], training_df[label_name], test_size=0.10)\n\nprint('We have {} training records and {} records for evaluating the model.'.format(len(X_train), len(X_test)))","f2523d64":"# Create the features normalized using z-score.\nz_score_scaled_features = [\n  tf.feature_column.numeric_column(\n      feature_name,\n      normalizer_fn=lambda val: zscore(X_train.mean()[feature_name], X_train.std()[feature_name], val),\n  )\n  for feature_name in z_score_scaled_feature_names\n]\n\n# Create the features normalized using log scaling\nlog_scaled_features = [\n  tf.feature_column.numeric_column(\n      feature_name,\n      normalizer_fn=lambda val: tf.math.log(val),\n  )\n  for feature_name in log_scaled_feature_names\n]\n\n# Create the input layer\ninput_layer = layers.DenseFeatures(z_score_scaled_features + log_scaled_features)\n\nprint('Created input layer.')","425f09bf":"def create_model(my_learning_rate, input_layer):\n  \"\"\"Create and compile a simple linear regression model.\"\"\"\n\n  model = tf.keras.models.Sequential()\n\n  # Add the layer containing the feature columns to the model.\n  model.add(input_layer)\n\n  # Add one linear layer to the model to yield a simple linear regressor.\n  model.add(tf.keras.layers.Dense(units=1, input_shape=(1, )))\n\n  # Construct the layers into a model that TensorFlow can execute.\n  model.compile(\n    optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n    loss='mean_squared_error',\n    metrics=[tf.keras.metrics.RootMeanSquaredError()],\n  )\n\n  return model\n\nprint('Defined create_model function.')","39556ab4":"def train_model(model, x, y, epochs, batch_size):\n  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n\n  features = {name:np.array(value) for name, value in x.items()}\n  label = y.to_numpy()\n\n  history = model.fit(\n    x=features,\n    y=label,\n    batch_size=batch_size,\n    epochs=epochs,\n    shuffle=True,\n  )\n\n  # The list of epochs is stored separately from the rest of history.\n  epochs = history.epoch\n  \n  # Isolate the mean absolute error for each epoch.\n  hist = pd.DataFrame(history.history)\n  rmse = hist['root_mean_squared_error']\n\n  return epochs, rmse\n\nprint('Defined train_model function.')   ","e0a6b787":"def plot_the_loss_curve(epochs, rmse):\n  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Root Mean Squared Error')\n\n  plt.plot(epochs, rmse, label=\"Loss\")\n  plt.legend()\n  plt.ylim([rmse.min()*0.94, rmse.max()* 1.05])\n  plt.show()\n\nprint('Defined plot function.')","8a199d20":"# The following variables are the hyperparameters.\nlearning_rate = 0.003\nepochs = 64\nbatch_size = 12\n\n# Create and compile the model.\nmodel = create_model(learning_rate, input_layer)\n\n# Train the model on the training set.\nepochs, rmse = train_model(model, X_train, Y_train, epochs, batch_size)\n\nplot_the_loss_curve(epochs, rmse)","b092c80c":"print(\"\\n: Evaluate the new model against the test set:\")\n\ntest_features = {name:np.array(value) for name, value in X_test.items()}\n\nresults = model.evaluate(x=test_features, y=Y_test.to_numpy(), batch_size=batch_size)","1bfa0768":"new_data = {\n  'animal_products': [17.7],\n  'cereals_excluding_beer': [7.9],\n  'obesity': [10.5],\n  'vegetal_products': [26.2],\n}\n\nnew_data = {name:np.array(value) for name, value in new_data.items()}\n\nresults = model.predict(new_data)\n\nprint('The predicted deaths percentage is {}.'.format(results[0][0]))","920091f4":"#@title Import relevant modules\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# The following lines adjust the granularity of reporting. \npd.options.display.max_rows = 30\npd.options.display.float_format = \"{:.8f}\".format","357ca849":"column_names = [\n  'country',\n  'alchoholic_beverages',\n  'animal_products',\n  'animal_fats',\n  'aquatic_products',\n  'cereals_excluding_beer',\n  'eggs',\n  'fish_and_seafood',\n  'fruits',\n  'meat',\n  'miscellaneous',\n  'milk_excluding_butter',\n  'offals',\n  'oilcrops',\n  'pulses',\n  'spices',\n  'starchy_roots',\n  'stimulants',\n  'sugar_crops',\n  'sugar_and_sweeteners',\n  'treenuts',\n  'vegetal_products',\n  'vegetal_oils',\n  'vegetables',\n  'obesity',\n  'undernourished',\n  'confirmed',\n  'deaths',\n  'recovered',\n  'active',\n  'population',\n  'unit',\n]\nused_column_names = [\n  'animal_products',\n  'cereals_excluding_beer',\n  'vegetal_products',\n  'obesity',\n  'deaths',\n]\n\ndiet_data_simple = pd.read_csv(\n  filepath_or_buffer='https:\/\/raw.githubusercontent.com\/GrozescuRares\/diet_vs_corona\/master\/diet_vs_corona.csv',\n  skiprows=1,\n  names=column_names,\n  usecols=used_column_names,\n)\n\ndiet_data_simple = diet_data_simple.dropna()\ndiet_data_simple.head()","6a2d1ca6":"#@title Get statistics on the dataset.\n\ndiet_data_simple.describe()","246b25fd":"#@title Sort data by deaths\ndiet_data_sorted = diet_data_simple.sort_values(by=['deaths'])\n\ndiet_data_sorted","8d98dafb":"#@title Separate data in groups\ndiet_data_sorted = diet_data_sorted[diet_data_sorted.deaths != 0.0]\n\nhighest_deaths_rate_data = diet_data_sorted.tail(10)\nlowest_deaths_rate_data = diet_data_sorted.head(10)\n\nprint('Data was separated in two groups by deaths rate.')","99003836":"#@title Check records with the highest death rate\n\nhighest_deaths_rate_data","0a08746a":"#@title Check records with the lowest death rate\n\nlowest_deaths_rate_data","8bd35443":"#@title Compute average for both groups\n\nhighest_deaths_rate_mean = {column_name:highest_deaths_rate_data[column_name].mean() for column_name in used_column_names[:-1]}\nprint('Average values for records with highest death rate: \\n{}'.format(highest_deaths_rate_mean))\n\nlowest_deaths_rate_mean = {column_name:lowest_deaths_rate_data[column_name].mean() for column_name in used_column_names[:-1]}\nprint('Average values for records with lowest death rate: \\n{}'.format(lowest_deaths_rate_mean))","9a75d7a0":"#@title Visualize charts\n\nlabels = used_column_names[:-1]\n\nx = np.array([0, 2, 4, 6])  # the label locations\nwidth = 0.7  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(20, 12))\nrects1 = ax.bar(x - width\/2, highest_deaths_rate_mean.values(), width, label='High deaths rate group')\nrects2 = ax.bar(x + width\/2, lowest_deaths_rate_mean.values(), label='Low deaths rate group')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Percentage')\nax.set_title('Percentage of fat income')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend(loc='upper right')\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 1),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\nautolabel(rects1)\nautolabel(rects2)\n\nplt.show()","ab628ed5":"## Objective\n\nThe objective of this data analyses is to confirm that a population with a healthy diet and lifestyle has a low rate of deaths related to the coronavirus pandemic.","26116864":"## The dataset\n\nIn order to prove our theory, we'll use [Covid-19 healthy dataset](https:\/\/www.kaggle.com\/mariaren\/covid19-healthy-diet-dataset). In the ML Approach, we observed that *Animal Products*, *Cereals - Excluding Beer*, *Vegetal Products* and *Obesity* are the most correlated to the deaths percentage, so we'll be using those values.","1bfe10df":"## Model\n\nThe sections bellow are dedicted to the processes of creating, training and evaluating the model.","8d097d40":"### Train the model\n\nIn this section we'll create the model and train it on the labeled examples.","c97eea28":"### Evaluate the model","5853bebb":"# Diet Vs Coronavirus - Data Approach","ba5f38c0":"After evaluating the model, despite the fact that the predictions are not that accurate due to the very low amount of examples used for training, we can still observe that increasing the percentage of features that have a negative corellation (if their values increase, the outcome decreases) while decreasing the percentage of features that have a positive corellation, we get a lower value for the percentage of deaths. For example a distribution of:\n\n\n```\n{\n  'animal_products': 18.7,\n  'cereals_excluding_beer': 7.9,\n  'obesity': 20.5,\n  'vegetal_products': 15.2,\n}\n```\nwill always result in a greater death percentage outcome than:\n\n\n```\n{\n  'animal_products': 14.7,\n  'cereals_excluding_beer': 7.9,\n  'obesity': 20.5,\n  'vegetal_products': 19.2,\n}\n```\n\nSo, we can conlude that by changing just a little bit the proportions of fat income types, we can make a difference by the end of the day.\n\n\n","c309ce8d":"By visualizing the histograms we can conclude the following:\n\n\n*   *Animal Products*, *Obesity* and *Vegetal Products* have a roughly normal distribution. We'll probably just scale their values using z-score formula.\n*   *Cereals - Excluding Beer* on the other hand, present a right skewed distribution. Maybe a log scalling we'll help us getting a normal distribution for those two features.\n\n","f40304ed":"#### Visualize features data distribution\n\nVisualizing the distribution of the data we'll help us in decideing if we need to normalize the data. We'll plot the histogram of each feature using pandas.  ","b0bfe074":"### Data processing\n\nIn this section we'll look at how we can normalize our data in order to obtain a normal distribution and we'll decide how should we handle the records with missing values.","4b8e13b0":"### Define functions that create and train a model; define plot function\n\nWe'll define a function for creating and compiling a simple linear regression model.","4d251db0":"# Diet Vs Coronavirus - ML Approach","e4c8d4ab":"## Objective\n\nThe objective of this model is to predict the percentage of deaths (per country) related to the coronavirus pandemic, taking into account statistical data about the food habbits of the population (food types: animal, eggs, fish, beer, etc.). From the predictions of the model, we can conclude which types of food have a bigger impact in the final outcome.","beb39dbd":"## Load the data\n\nWe'll load the data in memory using pandas, selecting only the five columns that we are intereseted in.","aac14fcc":"### Data acquisition\n\nSince our dataset is not that large, we'll use pandas to load the data in memory from a .csv file.","2ad0b22b":"#### Dropping records\n\nSince we have missing values for the label and due to the context of the problem, we'll drop that records.","78a07f0e":"We'll define the function that we are going to use in order to plot the results of the training process.","393a91b8":"### Creating the input layer\n\nIn this section we'll create the input layer that will be used by our model. When defining the columns will take into consideration the normalization methods that we discussed in the *Data normalization* phase.","3578babb":"## Import relevant modules\n\nThe following hidden code cell imports the necessary packages that we'll use in order to explore, process the data and to create, run and evaluate the model.","4be6d91f":"After analyzing the statistics we identified some anomalies:\n\n\n*   There are missing values for the columns: Obesity, Confirmed, Deaths, Recovered and Active.\n*   For several columns the value of max seems very high compared to the other quantiles, which suggest that for those column we have some outlier values. For example, for the Fruits - Excluding Wine column we have a maximum value of 9.7. Given the quantile values and the mean, std values, we would expect the max value to be aproximately 3.0. This issue also occours for the the Oilcrops, Pulses, Spices, Starchy Roots and Population columns.\n\nAll this considered, we need to carefully choose our features and decide how to handle the examples which have missing values for some columns.\n\n","8ba7747f":"### Data exploration\n\nIn this section we'll explore the dataset, since a large part of most machine learning projects is getting to know your data.","7d6deedb":"After observing and analyzing those statistics, we consider that a good approach would be to sort the records by the deaths percentage and then selecting ten rows; five of them representing records with highest percentage of deaths, and another five with the lowest. Last but not least, we'll do an average of the values for each group of records and than we'll compare them using piecharts.","9d0e806d":"#### Find feature(s) whose raw values correlate with the label\n\nWe want to find out which features has more predictive power in the case of our problem. In order to get that information we'll use the [**correlation matrix**](https:\/\/medium.com\/towards-artificial-intelligence\/training-a-machine-learning-model-on-a-dataset-with-highly-correlated-features-debddf5b2e34).","aa4304c1":"It seems that after applying z-score and log scaling we got a much more normal distribution for all of our features. So, we are definitely going to stick with this approach on model creation. ","50b1433c":"The function bellow represents the training process of the model on a given dataset.","ae99a206":"### Splitting the dataset\n\nWe split the dataset into training and testing data, separating the features from the label.","7278d548":"As you can observe from the bar chart, it is clear that a population which has a healthy diet consisting of vegetal products and cereals has a lower death rate in comparison with a population which has a higher obesity rate and consumes more animal products.\nIn conclusion, based on this data we can confirm that a population with a healthy diet and lifestyle has a low rate of deaths related to the coronavirus pandemic. ","be577ed7":"## Data\n\nThe following sections will be dedicated to the processes of data acquisition, exploration and processing.","b632f6e2":"## Analyze the data\n\nThis section is dedicated to the process of analyzing the data and confirming our theory. We'll start by taking a look on the statistics related to the dataset.","29851cd8":"## The dataset\n\nIn order to train and evaluate the model, we'll use [Covid-19 healthy dataset](https:\/\/www.kaggle.com\/mariaren\/covid19-healthy-diet-dataset). Unfortunately, the number of labeled examples is pretty low, so we'll create a classic ML model. ","ae621090":"After analyzing the correlation matrix we can conclude that 'Animal Products', 'Cereals - Excluding Beer', 'Vegetal Products' and 'Obesity' correlate more with 'Deaths'. So, we'll use those values as features (numeric features).","e2be479d":"#### Data noise and label normalization\n\nThe last thing that we need to do before creating the model is removing the noise of the label values and bring the label to a similar range as the features. For reducing the complexity of the computations, we'll keep just the first 4 digits after the floating point.\n\nFor avoiding logging 0 values which cause -inf results, we add +1 at logging","57f9ce14":"#### Data normalization\n\nIn the last section we plotted the histogram for all the features and we saw that the values of *Animal fats* and *Cereals - Excluding Beer* are not uniformly distributed. In this section we'll explore z-score and log scalling.\n**Note**: We'll apply the scalling on a copy of diet_data, just for visualizing the difference. The actual scalling will be done within the model creation.","e0ff09af":"## Import relevant modules\n\nThe following hidden code cell imports the necessary packages that we'll use in order to explore, process and visualize the data"}}