{"cell_type":{"5b8c2b78":"code","571256b2":"code","53af5e95":"code","abab71e1":"code","f51f56f7":"code","2329ab9f":"code","a99ab2d2":"code","f8d3f2fc":"code","e9182d75":"code","546f6b93":"code","af03d0dc":"code","b02ed4e2":"code","c2a23226":"code","74d421cc":"code","caa0314f":"code","1a1d6303":"code","fc2f7e4c":"code","bd0c1586":"code","b1054b2d":"code","22f9e89d":"code","31c92d8f":"code","87728b1c":"code","36a9ead3":"code","f241ffcc":"code","c8bb0c67":"code","76af27b3":"code","8e1c7937":"code","f00eaf55":"code","9d7fb5f4":"code","5240f908":"code","2b50380a":"code","4913fedf":"code","96b326d7":"code","2ee48d3d":"code","0e7186e0":"code","5ef8f21e":"code","c765d997":"markdown","1bf0bd18":"markdown","ad786b82":"markdown","738715e8":"markdown","27aee15e":"markdown","123bc00f":"markdown","925970fe":"markdown","200c1b1f":"markdown","e0a9f836":"markdown"},"source":{"5b8c2b78":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","571256b2":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub_sample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","53af5e95":"df_train.head()","abab71e1":"print(f\"train data's shape: {df_train.shape}\")\nprint(f\"test data's shape: {df_test.shape}\")","f51f56f7":"df_train.isnull().sum()","2329ab9f":"df_train.groupby(df_train.target).count().text","a99ab2d2":"train = df_train.iloc[:,3:]\ntest = df_test.iloc[:,3:] \n\ntrain.head()","f8d3f2fc":"import re\nimport emoji\nimport string\nfrom nltk.stem import WordNetLemmatizer\n\ndef text_processing(txt):\n    # remove @person tags\n    txt = re.sub(r'@[A-Za-z0-9_]+','',txt)\n    # remove Tweeter's symbols\n    txt = re.sub(r'#','',txt)\n    txt = re.sub(r'RT : ','',txt)\n    # remove new lines\n    txt = re.sub(r'\\n',' ',txt)\n    # remove emojis\n    txt = re.sub(emoji.get_emoji_regexp(), r\"\", txt)\n    # remove links\n    txt = re.sub(r\"https?:\\\/\\\/[A-Za-z0-9\\.\\\/]+\",\"\",txt)\n    txt = re.sub(r\"https?:\/\/\\S+|www\\.\\S+\",\"\",txt)\n    # remove symbols\n    txt = re.sub(r\"<.*?>\",\" \",txt)\n    # remove puncuation\n    txt = txt.translate(str.maketrans('','',string.punctuation))\n    # capitalization\n    txt = txt.lower()\n    # lemmitzation\n    lemmatizer = WordNetLemmatizer()\n    txt = lemmatizer.lemmatize(txt)\n\n    return txt.strip()","e9182d75":"from nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n\ndef remove_stopwords(txt):\n    txt = [word.lower() for word in txt.split() if word.lower() not in stop]\n    return \" \".join(txt)","546f6b93":"for _ in range(3):\n    n = np.random.randint(7613)\n    print(\"\\nText before processing:\")\n    print(train['text'][n])\n    print(\"\\nAfter:\")\n    print(remove_stopwords(text_processing(train['text'][n])))\n    print(\"----------------------------------\")","af03d0dc":"train.text = train.text.apply(text_processing)\ntrain.text = train.text.apply(remove_stopwords)\n\ntest.text = test.text.apply(text_processing)\ntest.text = test.text.apply(remove_stopwords)","b02ed4e2":"corpus=train['text'].values","c2a23226":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features=10000)\nvectorizer.fit(corpus)\nmat=vectorizer.transform(corpus).toarray()\nfeatures = vectorizer.get_feature_names()\nmat.shape","74d421cc":"# top n features\nn=20\nindices = np.argsort(vectorizer.idf_)[::-1]\ntop_features = [features[i] for i in indices[:n]]\nprint(top_features)","caa0314f":"from wordcloud import WordCloud\nx = vectorizer.vocabulary_\nCloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(x)\nplt.imshow(Cloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","1a1d6303":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val=train_test_split(mat,train['target'],test_size=0.2)\nX_train","fc2f7e4c":"# Training the classifier & predicting on test data\nfrom sklearn.naive_bayes import MultinomialNB\nMultinomialNB_classifier = MultinomialNB()\nMultinomialNB_classifier.fit(X_train, y_train)\n\ny_pred = MultinomialNB_classifier.predict(X_val)\n\n# Classification metrics\nfrom sklearn.metrics import accuracy_score, classification_report\nclassification_report = classification_report(y_val, y_pred)\n\nprint('\\n Accuracy: ', accuracy_score(y_val, y_pred))\nprint('\\nClassification Report')\nprint('======================================================')\nprint('\\n', classification_report)","bd0c1586":"from sklearn.linear_model import LogisticRegression\nLogisticRegression_classifier = LogisticRegression()\nLogisticRegression_classifier.fit(X_train, y_train)\n\ny_pred = LogisticRegression_classifier.predict(X_val)\n\n# Classification metrics\nfrom sklearn.metrics import accuracy_score, classification_report\nclassification_report = classification_report(y_val, y_pred)\n\nprint('\\n Accuracy: ', accuracy_score(y_val, y_pred))\nprint('\\nClassification Report')\nprint('======================================================')\nprint('\\n', classification_report)","b1054b2d":"import xgboost as xgb\nXGB_classifier = xgb.XGBClassifier()\nXGB_classifier.fit(X_train, y_train)\n\ny_pred = XGB_classifier.predict(X_val)\n\n# Classification metrics\nfrom sklearn.metrics import accuracy_score, classification_report\nclassification_report = classification_report(y_val, y_pred)\n\nprint('\\n Accuracy: ', accuracy_score(y_val, y_pred))\nprint('\\nClassification Report')\nprint('======================================================')\nprint('\\n', classification_report)","22f9e89d":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train['text'],train['target'])","31c92d8f":"import tensorflow as tf\nimport tensorflow.keras as k\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","87728b1c":"vocab = 80000\noov = '<OOV>'\nembedding = 32\npadding = 'post'\ntruncate = 'post'","36a9ead3":"tokenizer = Tokenizer(num_words = vocab, oov_token = oov)\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index","f241ffcc":"maxlength=max(len(txt) for txt in X_train)\nmaxlength=20","c8bb0c67":"X_train_tok = tokenizer.texts_to_sequences(X_train)\nX_train_tok_pad = pad_sequences(X_train_tok, maxlen=maxlength, padding=padding, truncating=truncate)\n\nX_val_tok = tokenizer.texts_to_sequences(X_val)\nX_val_tok_pad = pad_sequences(X_val_tok, maxlen=maxlength, padding=padding, truncating=truncate)\n\ny_train_values = y_train.values\ny_val_values = y_val.values\n\nX_test_tok = tokenizer.texts_to_sequences(test['text'])\nX_test_tok_pad = pad_sequences(X_test_tok ,maxlen=maxlength, padding=padding, truncating=truncate)","76af27b3":"from keras import layers \n\nmodel = tf.keras.Sequential([\n    layers.Embedding(vocab, embedding, input_length=maxlength),\n    layers.Dense(10, activation='relu'),\n    layers.Bidirectional(layers.LSTM(128,return_sequences = True)),\n    layers.LSTM(64),\n    layers.Dropout(0.3),\n    layers.Flatten(),\n    layers.Dense(units = 10,activation = 'relu'),\n    layers.Dropout(0.3),\n    layers.Dense(units = 1, activation = 'sigmoid')\n])\nmodel.summary()","8e1c7937":"model.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n    metrics=['accuracy']\n)","f00eaf55":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)","9d7fb5f4":"history = model.fit(\n    X_train_tok_pad,\n    y_train_values,\n    epochs = 30,\n    validation_data = (X_val_tok_pad, y_val_values),\n    callbacks=[early_stop]\n)","5240f908":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[0:, ['loss', 'val_loss']].plot()\nplt.show()","2b50380a":"history_df.loc[0:, ['accuracy','val_accuracy']].plot()\nplt.show()","4913fedf":"corpus_test=test['text'].values\nX_test=vectorizer.transform(corpus_test).toarray()\nprediction = LogisticRegression_classifier.predict(X_test)\nX_test","96b326d7":"#prediction = model.predict(X_test_tok_pad)\n\nprediction","2ee48d3d":"y_pred = (prediction > 0.5).astype(int)\ny_pred","0e7186e0":"sub = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsub['target'] = y_pred\nsub.head()","5ef8f21e":"sub.to_csv('submission3.csv', index=False)","c765d997":"## Exploratory analysis","1bf0bd18":"## Loading libraries and data","ad786b82":"## Data preparation","738715e8":"We will only keep text data","27aee15e":"We will:\n* remove tags\n* remove hyperlinks\n* remove emojis\n* remove symbols\n* remove capital letters\n* remove ponctuation\n* lemitize the text","123bc00f":"### TF-iDF embedding","925970fe":"* 0 corresponds to a fake tweet\n* 1 correponds to a tweet about a real disaster","200c1b1f":"## Predictions","e0a9f836":"## LSTM approach"}}