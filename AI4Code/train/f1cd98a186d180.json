{"cell_type":{"0a10b01d":"code","fd0cd595":"code","8e224a6b":"code","f2e0a4c5":"code","ecf87613":"code","891039cc":"code","46f09232":"code","c6f69eff":"code","9641eba4":"code","a06b9cb4":"code","e54f9de7":"code","d03ccf18":"code","bb108dde":"code","a1ca1380":"code","00bb81e3":"code","f04e5acb":"code","8af3935d":"code","6c09caf4":"code","b573740a":"code","81684d4b":"code","ddb6607f":"code","cea00386":"code","ca92e7dc":"code","2868a02c":"code","7c95baab":"code","71241b58":"code","e7f0efdb":"code","2abcf792":"code","c1437e72":"code","a1237f08":"code","dbe9281f":"code","a118eff8":"code","1fb0ee02":"code","4e04a4fe":"code","8300750f":"code","4a43c7ce":"code","1fd70db4":"markdown","35f719c9":"markdown","831d6147":"markdown","fb75bcae":"markdown","101e00f6":"markdown","1c0bf18f":"markdown","63662bfc":"markdown","36065630":"markdown","440faa63":"markdown","60013898":"markdown","9d28c5df":"markdown","d0f22dbd":"markdown","2843b88d":"markdown","4355b344":"markdown","db107a29":"markdown","78666f0d":"markdown","430942d3":"markdown","cd8695e0":"markdown","57269969":"markdown","d3d52b9f":"markdown","6910247f":"markdown","4ff1759f":"markdown","0d0c3ac6":"markdown","ebb77f88":"markdown","2fd9f780":"markdown","abc1f8e9":"markdown","d6fb6515":"markdown","44bc0afe":"markdown","e808644d":"markdown","2ac56c64":"markdown","5b706728":"markdown","bfe28c93":"markdown","95098c5a":"markdown","e845ea47":"markdown","67cf8929":"markdown","1a70cec4":"markdown","9e13916d":"markdown","d4187454":"markdown","22332fc6":"markdown","71219887":"markdown","4de9a142":"markdown"},"source":{"0a10b01d":"import numpy  as np # linear algebra\nimport pandas as pd # data processing\nimport os\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndef plotPercentageRate(p_data, p_feature, p_colorsIdx, p_title):\n    v_plot_l1 = p_data.copy()\n    v_plot_l1['Level_01'] = v_plot_l1[p_feature].apply(lambda x: x.split('|')[-2].replace('_', '<br>'))\n    v_plot_l1 = ( v_plot_l1.reset_index().groupby(['Level_01'])[['index']].count()\n                   .rename(columns = {'index': 'Count'}).reset_index() )\n    v_plot_l1['Count'] = v_plot_l1['Count'] \/ v_plot_l1['Count'].sum() * 100\n    v_lambda = ( lambda x:   '<i style=\"color:blue\" >' + x['Level_01']\n                           + '<\/i><br>' + '<b><i style=\"color:blue\" >(' \n                           + str(np.round(x['Count'], 2)) + '%)<\/i><\/b>' )\n    v_plot_l1['Level_01_D'] = v_plot_l1[['Level_01', 'Count']].apply(v_lambda, axis = 1)\n    v_plot_l1 = v_plot_l1.set_index('Level_01')[['Level_01_D']].to_dict(orient = 'index')\n\n    v_plot = ( p_data.reset_index().groupby([p_feature])[['index']].count()\n                   .rename(columns = {'index': 'Count'}).reset_index() )\n    v_plot['Count'] = v_plot['Count'] \/ v_plot['Count'].sum() * 100\n    \n    v_plot['Level_01'] = v_plot[p_feature].apply(lambda x: x.split('|')[-2].replace('_', '<br>'))\n    if len(v_plot_l1) > 1:\n        v_plot['Level_01'] = v_plot['Level_01'].apply(lambda x: v_plot_l1[x]['Level_01_D'])\n    \n    v_plot['Level_02'] = v_plot[p_feature].apply(lambda x: x.split('|')[-1].replace('_', '<br>'))\n    v_lambda = ( lambda x:    x['Level_02'] \n                           + '<br>' + '<b>(' \n                           + str(np.round(x['Count'], 2)) + '%)<\/b>'\n                           + '<br>' + ' ' )\n    v_plot['Level_02'] = v_plot[['Level_02', 'Count']].apply(v_lambda, axis = 1)\n    \n    v_colors = ['crimson' if idx in p_colorsIdx else 'lightslategray' for idx in range(v_plot.shape[0])] \n    fig = go.Figure(data=[go.Bar( x = [ v_plot['Level_01'].tolist(), v_plot['Level_02'].tolist() ],\n                                  y = v_plot['Count'],\n                                  marker_color = v_colors )])\n    fig.update_layout( title_text = f'{p_title} - Percentage Rate', height = 350,\n                       margin = go.layout.Margin( l = 0, # left margin\n                                                  r = 0, # right margin \n                                                  b = 0, # bottom margin\n                                                  t = 40, # top margin\n                     ) )\n    fig.show()\n    \n    return","fd0cd595":"v_dataRaw = pd.read_csv( '\/kaggle\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv', \n                         sep = ',', header = [0, 1], low_memory = False )\nv_colsMap = { col[0]: col[1] for col in v_dataRaw.columns }\nv_dataRaw.columns = ['Time in Sec'] + list(v_colsMap.keys())[1:]\nprint('Dataset size: ', v_dataRaw.shape)\ndisplay(v_dataRaw.head(3))","8e224a6b":"# Check all the questions that were used in the survey\nv_questions = sorted(set( [f\"{'_'.join(key.split('_')[:2])} ==> {value[:200].split('?')[0]}\" \n                              for key, value in v_colsMap.items()] ))\nv_questions = pd.DataFrame({value.split(' ==> ')[0]: value.split(' ==> ')[1] for value in v_questions}, index = [0]).T\nv_questions.reset_index(inplace = True)\nv_questions.columns = ['Question', 'Description']\nv_questions['No'] = v_questions['Question'].apply(lambda x: x.split('_')[0][1:]).apply(lambda x: 0 if not x.isnumeric() else int(x))\nv_questions = v_questions[ ~v_questions['Question'].str.contains('OTHER') ].sort_values('No').drop('No', axis = 1)\nfig = go.Figure(data = [go.Table( header = dict( values = list(v_questions.columns),\n                                                 align  = 'left',\n                                                 fill_color = 'paleturquoise'),\n                                  cells  = dict( values = [ v_questions['Question'], v_questions['Description'] ],\n                                                 fill_color = 'lavender',\n                                                 align = 'left' )) ])\nfig.update_layout( height = 350, margin = go.layout.Margin( l = 0, r = 0, b = 0, t = 40 ) )\nfig.show()\n\n#--------------------------------------------------------------------------------------------\nX_data     = v_dataRaw[list(v_dataRaw.columns)[:7]].copy()\nv_usedCols = X_data.columns.tolist()\nX_data.columns = [ 'Filling_Duration', 'Age', 'Gender', 'Residence_Country', 'Highest_Degree', 'Role_Title', \n                   'Dev_Experience' ]\nX_data['Age'] = X_data['Age'].replace({ '60-69': '60+',\n                                        '70+':   '60+' })\n\n#--------------------------------------------------------------------------------------------\nv_lambda = lambda x: '__'.join(sorted([value for value in x if value != '']))\nfor item in [ ('Q25',      'Salary'),\n              ('Q15',      'ML_years'),\n              ('Q13',      'TPU_Usage'),\n              ('Q24_Part', 'Role_Activities'),\n              ('Q20',      'Company_Industry'),\n              ('Q21',      'Company_Size'),\n              ('Q22',      'Company_DS_Team_Size'),\n              ('Q23',      'Company_ML_Used'),\n              ('Q26',      'Company_ML_Budget'),\n              ('Q8',       'Recommend_Programming_Language'),\n              ('Q41',      'Most_Used_Analyse_Tool'), \n              ('Q9_Part',  'IDE'),\n              ('Q7_Part',  'Dev_Programming_Language'),\n              ('Q10_Part', 'Hosted_Notebooks'),\n              ('Q12_Part', 'Specialized_Hardware'),\n              ('Q14_Part', 'Data_Visualization_Library'),                    \n              ('Q18_Part', 'Computer_Vision_Methods'),\n              ('Q19_Part', 'NLP_Algorithm'),\n              ('Q17_Part', 'ML_Algorithm'),\n              ('Q16_Part', 'ML_Framework'),\n              ('Q31_A_',   'ML_Managed'),\n              ('Q31_B_',   'ML_Managed_2years'),\n              ('Q36_A_',   'ML_Auto_01_Tools'),\n              ('Q36_B_',   'ML_Auto_01_Tools_2years'),\n              ('Q37_A_',   'ML_Auto_02_Tools'),\n              ('Q37_B_',   'ML_Auto_02_Tools_2years'),\n              ('Q38_A_',   'ML_Experiments'),\n              ('Q38_B_',   'ML_Experiments_2years'),\n              ('Q39_Part', 'ML_Public_Share'),\n              ('Q40_Part', 'DS_Learning_Platform'),\n              ('Q42_Part', 'DS_Media_Source'),      \n              ('Q28',      'Best_Cloud_Platform'),\n              ('Q27_A_',   'Cloud_Computing_Platform'),\n              ('Q27_B_',   'Cloud_Computing_Platform_2years'),\n              ('Q11',      'Cloud_Computing_Platform_Most_Used'),\n              ('Q29_A_',   'Cloud_Computing_Products'),\n              ('Q29_B_',   'Cloud_Computing_Products_2years'),\n              ('Q30_A_',   'Data_Storage_Products'),\n              ('Q30_B_',   'Data_Storage_Products_2years'),\n              ('Q32_A_',   'BigData_Product'),\n              ('Q32_B_',   'BigData_Product_2years'),\n              ('Q33',      'BigData_Product_Most_Used'),\n              ('Q34_A_',   'BI_Tool'),\n              ('Q34_B_',   'BI_Tool_2years'),\n              ('Q35',      'BI_Tool_Most_Used') ]:\n    if not '_' in item[0]:\n        X_data[item[1]] = v_dataRaw[item[0]]\n        v_usedCols.append(item[0])\n    else:\n        v_tmpCols = [col for col in v_dataRaw.columns if item[0] in col\n                                                         or (    '_Part' in item[0]\n                                                             and col == f'{item[0].split(\"_\")[0]}_OTHER') ]\n        X_data[item[1]] = v_dataRaw[v_tmpCols].fillna('').apply(v_lambda, axis = 1)\n        v_usedCols += v_tmpCols\n\nv_data_Ck = v_dataRaw[[col for col in v_dataRaw.columns if col not in v_usedCols]]\nif v_data_Ck.shape[1] > 0:\n    display(v_data_Ck)\n    display({key: value for key, value in v_colsMap.items() if key in v_data_Ck.columns})\n    \nX_baseData = X_data.reset_index()\nX_baseData['index'] = X_baseData['index'].apply(lambda x: f'ID_{x}')\nX_baseData.set_index('index', inplace = True)\ndisplay(X_baseData.head(3))","f2e0a4c5":"import plotly.graph_objects as go\n\nv_map = { 'Data Scientist':                  'Data Scientist',\n          'Data Engineer':                   'Data\/ML Engineer',\n          'Machine Learning Engineer':       'Data\/ML Engineer',\n          'Research Scientist':              'Research',\n          'Statistician':                    'Research',\n          #-----------------------------------------------------------\n          'Data Analyst':                    'Analyst',\n          'Business Analyst':                'Analyst',\n          #-----------------------------------------------------------\n          'Software Engineer':               'Software Engineer',\n          'Program\/Project Manager':         'IT__Other',\n          'Product Manager':                 'IT__Other',\n          'DBA\/Database Engineer':           'IT__Other',\n          'Developer Relations\/Advocacy':    'IT__Other', }\nX_baseData['Role_Title_F'] = X_baseData['Role_Title'].replace(v_map)\n\nv_cntRole_1 = X_baseData[ X_baseData['Role_Title'] == 'Student' ].shape[0] \/ X_baseData.shape[0] * 100\nv_cntRole_2 = X_baseData[ X_baseData['Role_Title'] != 'Student' ].shape[0] \/ X_baseData.shape[0] * 100\nv_data = { 'nodes':   [ 'Role<br>Titles', \n                        f'Students<br>({np.round(v_cntRole_1, 2)}%)', f'Professionals<br>({np.round(v_cntRole_2, 2)}%)' ],\n           'parents': ['', 'Role<br>Titles', 'Role<br>Titles'],\n           'values':  [ 100.1, v_cntRole_1, v_cntRole_2 * 1.001] }\nv_cntRole_2 = v_cntRole_2 * X_baseData.shape[0] \/ 100\nfor value in X_baseData.reset_index().groupby('Role_Title_F')[['index']].count().reset_index().to_dict(orient = 'records'):\n    if value['Role_Title_F'] == 'Student': continue\n    v_perc = np.round(value['index'] \/ X_baseData.shape[0] * 100, 2)\n    if v_perc > 5:\n        v_data['nodes'].append(f\"{value['Role_Title_F'].replace(' ', '<br>')}<br>({v_perc}%)\")\n    else:\n        v_data['nodes'].append(value['Role_Title_F'].replace(' ', '<br>'))\n    v_data['parents'].append(v_data['nodes'][2])\n    v_data['values'].append(v_perc)\n\nfig = go.Figure(go.Sunburst( labels  = v_data['nodes'],\n                             parents = v_data['parents'],\n                             values  = [np.round(value, 4) for value in v_data['values']],\n                             branchvalues = 'total',\n                             insidetextorientation = 'radial',\n                             marker  = dict( colorscale = 'solar' ), ))\nfig.update_layout(margin = dict(t = 0, l = 0, r = 0, b = 0), uniformtext=dict(minsize=10), height = 450)\nfig.show()\n\nX_professionals = X_baseData[ X_baseData['Role_Title'] != 'Student' ].copy()\nX_professionals['Role_Title__Filtered'] = X_professionals['Role_Title_F']\nX_professionals.drop(['Role_Title', 'Role_Title_F'], axis = 1, inplace = True)","ecf87613":"v_MLProfile = X_professionals.copy()\nv_colName   = 'Highest_Degree'\nv_colNameF  = f'{v_colName}__Filtered'\nv_MLProfile[v_colName] = v_MLProfile[v_colName].fillna('0|Unknown|***NAV')\n\n#-----------------------------------------------------------------------------------------------\nfor item in [ ('I prefer not to answer',               '1|No University|No_Answer'),\n              ('No formal education past high school', '1|No University|No_University'),\n              ('without earning a bachelor',           '1|No University|No_University'),\n              ('Bachelor\u2019s degree',                    '2|University|Bachelor_Degree'),\n              ('Master\u2019s degree',                      '3|University|Master_Degree'),\n              ('Doctoral degree',                      '4|University|Doctoral_Degree'),\n              ('Professional doctorate',               '5|University|Professional_Doctorate') ]:\n    v_idx = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: item[0] in x) ].index\n    v_MLProfile.loc[v_idx, v_colName] = item[1]\n\nplotPercentageRate( v_MLProfile, v_colName, p_colorsIdx = [0, 1], p_title = 'Highest Degree - Initial')\nv_MLProfile = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: x.split('|')[1] != 'No University' ) ]\nv_MLProfile[v_colName] = v_MLProfile[v_colName].apply(lambda x: x.replace('|University', ''))\nv_MLProfile[v_colName] = v_MLProfile[v_colName].replace({ '4|Doctoral_Degree':        '4|Doctoral_Degree+',\n                                                          '5|Professional_Doctorate': '4|Doctoral_Degree+' })\nv_MLProfile[v_colNameF] = v_MLProfile[v_colName].apply(lambda x: x.split('|')[0] + '| |' + x.split('|')[1])\nv_MLProfile.drop(v_colName, axis = 1, inplace = True)\nplotPercentageRate( v_MLProfile, v_colNameF, p_colorsIdx = [], p_title = 'Highest Degree - Filtered')\nX_professionals = v_MLProfile.copy()","891039cc":"v_MLProfile = X_professionals.copy()\nv_colName   = 'Company_ML_Used'\nv_colNameF  = f'{v_colName}__Filtered'\nv_MLProfile[v_colName] = v_MLProfile[v_colName].fillna('0|Unknown|***NAV')\n\n#-----------------------------------------------------------------------------------------------\nfor item in [ ('I do not know',                              '0|Unknown|I do not know'),\n              ('No (we do not use ML methods)',              '1|No_Machine_Learning|No_Machine_Learning'),\n              ('We are exploring ML methods',                '2|Machine_Learning|Machine_Learning_Exploring'),\n              ('We use ML methods for generating insights',  '3|Machine_Learning|Machine_Learning_Generate_Insights'),\n              ('models in production for less than 2 years', '4|Machine_Learning|Machine_Learning_Less_than_2 years'),\n              ('models in production for more than 2 years', '5|Machine_Learning|Machine_Learning_More_than_2 years') ]:\n    v_idx = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: item[0] in x) ].index\n    v_MLProfile.loc[v_idx, v_colName] = item[1]\n    \nplotPercentageRate( v_MLProfile, v_colName, p_colorsIdx = [0, 1], p_title = 'Machine Learning Implementation - Initial')\nv_MLProfile[v_colNameF] = v_MLProfile[v_colName]\nv_MLProfile.drop(v_colName, axis = 1, inplace = True)\nv_MLProfile = v_MLProfile[ v_MLProfile[v_colNameF].apply(lambda x: x.split('|')[1] != 'Unknown' ) ]\nplotPercentageRate( v_MLProfile, v_colNameF, p_colorsIdx = [], p_title = 'Machine Learning Implementation - Filtered')\n\nX_professionals = v_MLProfile.copy()","46f09232":"v_MLProfile = X_professionals.copy()\nv_colName   = 'ML_years'\nv_colNameF  = f'{v_colName}__Filtered'\nv_MLProfile[v_colName] = v_MLProfile[v_colName].fillna('0|Unknown|***NAV')\n\n#-----------------------------------------------------------------------------------------------\nfor item in [ ('I do not use machine learning',            '0|No_Machine_Learning|No_ML'),\n              ('Under 1 year',                             '1|Level_Very_Junior|Less than 1 year'),\n              ('1-2 years',                                '2|Level_Junior|1 - 2 Years'),\n              ('2-3 years',                                '3|Level_Junior+|2 - 3 Years'),\n              ('3-4 years',                                '4|Level_Medium|3 - 4 Years'),\n              ('4-5 years',                                '4|Level_Medium|4 - 5 Years'),\n              ('5-10 years',                               '5|Level_Senior|5 - 10 Years'),\n              ('10-20 years',                              '5|Level_Senior|10 - 20 Years'),\n              ('20 or more',                               '5|Level_Senior|20+ Years'), ]:\n    v_idx = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: item[0] in x) ].index\n    v_MLProfile.loc[v_idx, v_colName] = item[1]\n\nplotPercentageRate( v_MLProfile, v_colName, p_colorsIdx = [0, 1], p_title = 'Machine Learning Years - Initial')\nv_MLProfile = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: 'Level' in x ) ]\nv_MLProfile[v_colNameF] = v_MLProfile[v_colName].apply(lambda x: '|'.join(x.split('|')[:2]))\nv_MLProfile.drop(v_colName, axis = 1, inplace = True)\nv_MLProfile = v_MLProfile[ v_MLProfile[v_colNameF].str.contains('Level') ]\nv_MLProfile[v_colNameF] = v_MLProfile[v_colNameF].apply(lambda x: x.split('|')[0]\n                                                                  + (      '|Junior|' if 'Junior' in x.split('|')[1] \n                                                                      else '|Not_Junior|' )  \n                                                                  + x.split('|')[1])\nplotPercentageRate( v_MLProfile, v_colNameF, p_colorsIdx = [], p_title = 'Machine Learning Years - Filtered')\nX_professionals = v_MLProfile.copy()","c6f69eff":"v_MLProfile = X_professionals.copy()\nv_colName   = 'Gender'\nv_colNameF  = f'{v_colName}__Filtered'\nv_MLProfile[v_colName] = v_MLProfile[v_colName].fillna('0|Unknown|***NAV')\n\n#-----------------------------------------------------------------------------------------------\nfor item in [ ('Prefer not to say',                        '0|Not_Declared|Prefer not to say'),\n              ('Prefer to self-describe',                  '0|Not_Declared|Prefer to self-describe'),\n              ('Nonbinary',                                '0|Not_Declared|Nonbinary'),\n              ('Man',                                      '1|Declared|Male'),\n              ('Woman',                                    '2|Declared|Female'), ]:\n    v_idx = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: item[0] in x) ].index\n    v_MLProfile.loc[v_idx, v_colName] = item[1]\n\nplotPercentageRate( v_MLProfile, v_colName, p_colorsIdx = [0, 1, 2], p_title = 'Gender - Initial')\nv_MLProfile = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: x.split('|')[1] == 'Declared' ) ]\nv_MLProfile[v_colNameF] = v_MLProfile[v_colName].apply(lambda x: x.split('|')[0] + '| |' + x.split('|')[2])\nv_MLProfile.drop(v_colName, axis = 1, inplace = True)\nplotPercentageRate( v_MLProfile, v_colNameF, p_colorsIdx = [], p_title = 'Gender - Filtered')\nX_professionals = v_MLProfile.copy()","9641eba4":"v_MLProfile = X_professionals.copy()\nv_colName   = 'Age'\nv_colNameF  = f'{v_colName}__Filtered'\nv_MLProfile[v_colName] = v_MLProfile[v_colName].fillna('0|Unknown|***NAV')\n\n#-----------------------------------------------------------------------------------------------\nfor item in [ ('18-21',                '1|Less_Than_30|18-24'),\n              ('22-24',                '1|Less_Than_30|18-24'),\n              ('25-29',                '1|Less_Than_30|25-29'),\n              ('30-34',                '2|Less_Than_40|30-34'),\n              ('35-39',                '2|Less_Than_40|35-39'), \n              ('40-44',                '3|Less_Than_50|40-44'), \n              ('45-49',                '3|Less_Than_50|45-49'), \n              ('50-54',                '4|Less_Than_60|50-59'), \n              ('55-59',                '4|Less_Than_60|50-59'), \n              ('60+',                  '5|60+|60+'), \n            ]:\n    v_idx = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: item[0] in x) ].index\n    v_MLProfile.loc[v_idx, v_colName] = item[1]\n\nplotPercentageRate( v_MLProfile, v_colName, p_colorsIdx = [7], p_title = 'Age - Initial')\nv_MLProfile = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: x.split('|')[1] != '60+' ) ]\nv_map = ['18-24', '25-29', '30-34']\nv_MLProfile[v_colNameF] = v_MLProfile[v_colName].apply(lambda x: x.split('|')[0] \n                                                                + ( '|Less Than 35 Years Old|' if x.split('|')[2] in v_map\n                                                                    else '|More Than 35 Years Old|') \n                                                                + x.split('|')[2])\nv_MLProfile.drop(v_colName, axis = 1, inplace = True)\nplotPercentageRate( v_MLProfile, v_colNameF, p_colorsIdx = [], p_title = 'Age - Filtered')\nX_professionals = v_MLProfile.copy()","a06b9cb4":"v_MLProfile = X_professionals.copy()\nv_colName   = 'Salary'\nv_colNameF  = f'{v_colName}__Filtered'\nv_lambda = (lambda x: x.replace(',', '').replace('$', '').replace('>', '-'))\nv_MLProfile[v_colName] = v_MLProfile[v_colName].fillna('***NAV').apply(v_lambda)\nv_lambda = (lambda x:      '0|Unknown|***NAV' if x == '***NAV' \n                      else '9|High|200k+'        if x == '-1000000'\n                      else '1|Low|0 - 1K'        if int(x.split('-')[1]) < 1000\n                      else '2|Low|1k - 2K'       if int(x.split('-')[1]) < 2000\n                      else '3|Low|2K - 5K'       if int(x.split('-')[1]) < 5000\n                      else '4|Low|5K - 10K'      if int(x.split('-')[1]) < 10000\n                      else '5|Medium|10K - 20K'  if int(x.split('-')[1]) < 20000\n                      else '6|Medium|20K - 50K'  if int(x.split('-')[1]) < 50000\n                      else '7|High|50K - 100K'   if int(x.split('-')[1]) < 100000\n                      else '8|High|100K - 200K'  if int(x.split('-')[1]) < 200000\n                      else '9|High|200k+' )\nv_MLProfile[v_colName] = v_MLProfile[v_colName].apply(v_lambda)\n\nplotPercentageRate( v_MLProfile, v_colName, p_colorsIdx = [0], p_title = 'Salary - Initial')\nv_MLProfile = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: x.split('|')[1] != 'Unknown' ) ]\nv_MLProfile[v_colNameF] = v_MLProfile[v_colName]\nv_MLProfile.drop(v_colName, axis = 1, inplace = True)\nplotPercentageRate( v_MLProfile, v_colNameF, p_colorsIdx = [], p_title = 'Salary - Filtered')\nX_professionals = v_MLProfile.copy()","e54f9de7":"v_MLProfile = X_professionals.copy()\nv_colName   = 'Company_DS_Team_Size'\nv_colNameF  = f'{v_colName}__Filtered'\nv_MLProfile[v_colName] = v_MLProfile[v_colName].fillna('0|Unknown|***NAV')\n\n#-----------------------------------------------------------------------------------------------\nfor item in [ ('0',                    '1|NO_DSTeam|0'),\n              ('1-2',                  '2|Very_Small_DSTeam|1 - 2'),\n              ('3-4',                  '3|Small_DSTeam|3 - 4'),\n              ('5-9',                  '4|Medium_DSTeam|5 - 9'),\n              ('10-14',                '4|Medium_DSTeam|10 - 14'),\n              ('15-19',                '5|Large_DSTeam|15 - 19'),\n              ('20+',                  '5|Large_DSTeam|20+'),\n            ]:\n    v_idx = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: item[0] == x) ].index\n    v_MLProfile.loc[v_idx, v_colName] = item[1]\n\nplotPercentageRate( v_MLProfile, v_colName, p_colorsIdx = [7], p_title = 'Data Science Team Size - Initial')\nv_MLProfile[v_colNameF] = v_MLProfile[v_colName].apply(lambda x: x.split('|')[0] + '| |' + x.split('|')[1])\nv_MLProfile.drop(v_colName, axis = 1, inplace = True)\nplotPercentageRate( v_MLProfile, v_colNameF, p_colorsIdx = [], p_title = 'Data Science Team Size - Grouped')\nX_professionals = v_MLProfile.copy()","d03ccf18":"v_MLProfile = X_professionals.copy()\nv_colName   = 'Company_Size'\nv_colNameF  = f'{v_colName}__Filtered'\nv_MLProfile[v_colName] = v_MLProfile[v_colName].fillna('0|Unknown|***NAV')\n\n#-----------------------------------------------------------------------------------------------\nfor item in [ ('0-49',                             '1|Small|0 - 49 employees'),\n              ('50-249',                           '2|Medium|50 - 249 employees'),\n              ('250-999',                          '2|Medium|250 - 999 employees'),\n              ('1000-9,999',                       '3|Large|1000 - 9999 employees'),\n              ('10,000 or more',                   '3|Large|10000+ employees') ]:\n    v_idx = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: item[0] in x) ].index\n    v_MLProfile.loc[v_idx, v_colName] = item[1]\n\nplotPercentageRate( v_MLProfile, v_colName, p_colorsIdx = [], p_title = 'Company Size - Initial')\nv_MLProfile[v_colNameF] = v_MLProfile[v_colName].apply(lambda x: '|'.join(x.split('|')[:2]))\nv_MLProfile[v_colNameF] = v_MLProfile[v_colNameF].apply(lambda x: x.split('|')[0] + '| |' + x.split('|')[1])\nv_MLProfile.drop(v_colName, axis = 1, inplace = True)\nplotPercentageRate( v_MLProfile, v_colNameF, p_colorsIdx = [], p_title = 'Company Size - Grouped')\nX_professionals = v_MLProfile.copy()","bb108dde":"v_MLProfile = X_professionals.copy()\nv_colName   = 'Dev_Experience'\nv_colNameF  = f'{v_colName}__Filtered'\nv_MLProfile[v_colName] = v_MLProfile[v_colName].fillna('0|Unknown|***NAV')\n\n#-----------------------------------------------------------------------------------------------\nfor item in [ ('I have never written code',                '0|No_Coding|No_Coding'),\n              ('< 1 years',                                '1|Level_Very_Junior|Less than 1 year'),\n              ('1-3 years',                                '2|Level_Junior|1 - 3 years'),\n              ('3-5 years',                                '3|Level_Medium|3 - 5 years'),\n              ('5-10 years',                               '3|Level_Medium|5 - 10 years'),\n              ('10-20 years',                              '4|Level_Senior|10 - 20 years'),\n              ('20+',                                      '4|Level_Senior|20+ years'), ]:\n    v_idx = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: item[0] in x) ].index\n    v_MLProfile.loc[v_idx, v_colName] = item[1]\n\nplotPercentageRate( v_MLProfile, v_colName, p_colorsIdx = [], p_title = 'Development Experience - Initial')\nv_MLProfile = v_MLProfile[ v_MLProfile[v_colName].apply(lambda x: 'Level' in x ) ]\nv_MLProfile[v_colNameF] = v_MLProfile[v_colName].apply(lambda x: '|'.join(x.split('|')[:2]))\nv_MLProfile[v_colNameF] = v_MLProfile[v_colNameF].apply(lambda x: x.split('|')[0] + '| |' + x.split('|')[1])\nv_MLProfile.drop(v_colName, axis = 1, inplace = True)\nplotPercentageRate( v_MLProfile, v_colNameF, p_colorsIdx = [], p_title = 'Development Experience - Grouped')\nX_professionals = v_MLProfile.copy()","a1ca1380":"v_MLProfile = X_professionals.copy()\nv_colName   = 'Residence_Country'\nv_colNameF  = f'{v_colName}__Filtered'\nv_MLProfile[v_colName] = v_MLProfile[v_colName].fillna('0|Unknown|***NAV')\n\n#-----------------------------------------------------------------------------------------------\nv_map = { 'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom',\n          'United States of America':                             'United States',\n          'Hong Kong (S.A.R.)':                                   'China',\n          'South Korea':                                          'S. Korea',\n          'South Africa':                                         'S. Africa',\n          'Taiwan':                                               'Vietnam',\n          'Viet Nam':                                             'Vietnam',\n          'Iran, Islamic Republic of...':                         'Iran',\n          'I do not wish to disclose my location':                'Other' }\nv_countriesMap = pd.read_csv( '\/kaggle\/input\/countries\/Countries.csv', sep = ',', low_memory = False )\nv_continentsMap = { 'eu':    '1|Europe|Others',\n                    'na':    '2|North America|Others',\n                    'sa':    '3|South America|Others',\n                    'af':    '4|Africa|Others',\n                    'asia':  '5|Asia|Others',\n                    'ocean': '6|Oceania|Others' }\nv_countriesMap['continent'] = v_countriesMap['continent'].apply(lambda x: x if not x in v_continentsMap.keys() \n                                                                          else v_continentsMap[x] )\nv_countriesMap = v_countriesMap.set_index('name')[['continent']].to_dict(orient = 'index')\nv_countriesMap = { key: value['continent'] for key, value in v_countriesMap.items() }\nv_countriesMap['India'] = '5|Asia|India'\nv_countriesMap['United States'] = '2|North America|USA'\nv_countriesMap['Other'] = '9|Other|Other'\n\n#-----------------------------------------------------------------------------------------------\nv_MLProfile[v_colName] = v_MLProfile[v_colName].replace(v_map).replace(v_countriesMap)\nplotPercentageRate( v_MLProfile, v_colName, p_colorsIdx = [], p_title = 'Development Experience - Grouped')\nv_MLProfile[v_colNameF] = v_MLProfile[v_colName]\nv_MLProfile.drop(v_colName, axis = 1, inplace = True)\nX_professionals = v_MLProfile.copy()","00bb81e3":"from sklearn.model_selection import train_test_split\n\nX_data = X_professionals[[col for col in X_professionals.columns if '__Filtered' in col]].copy()\nX_data.columns = [col.replace('__Filtered', '') for col in X_data.columns]\nfor col in X_data.columns:\n    if col in ['Residence_Country']: continue\n    v_values = X_data[col].value_counts().index.tolist()\n    if sum([1 for value in v_values if '|' in value]) > 0:        \n        if len(set([value.split('|')[1] for value in v_values])) > 1:\n            X_data[f'{col}__Categ'] = X_data[col].apply(lambda x: x.split('|')[1])\n        X_data[col] = X_data[col].apply(lambda x: x.split('|')[0] + '__' + x.split('|')[2])\ny_data = pd.DataFrame()\nv_labelCols = [col for col in X_data.columns if 'Salary' in col]\nfor label in sorted(set([col for col in v_labelCols])):\n    y_data[label] = X_data[[col for col in v_labelCols if label in col]].sum(axis = 1) \n\nX_data  = X_data.drop(v_labelCols, axis = 1)\nX_dataD = pd.get_dummies(X_data)\ndisplay(X_data.head(3).T.head())\ndisplay(y_data.head(3).T.head())\nX_train, X_valid, y_train, y_valid = train_test_split( X_dataD, y_data,\n                                                       test_size    = 0.3, \n                                                       random_state = 2013,\n                                                       stratify     = y_data )\nprint('Total number of records: ', X_data.shape, y_data.shape)\nprint('Train records: ', X_train.shape, y_train.shape)","f04e5acb":"import tensorflow as tf\nfrom numpy.random import seed\nfrom datetime import datetime\n\nseed(2013)\ntf.random.set_seed(2013)","8af3935d":"class AutoEncoderBase(tf.keras.models.Model):\n    \n    __threshold__  = None\n    \n    def __init__(self, p_inputDims, p_outputDims, p_encodedDims):\n        super().__init__()    \n        # deconstruct \/ encoder\n        self.__encoder__ = tf.keras.models.Sequential([ \n                                  tf.keras.layers.Dense(p_inputDims, activation = 'selu'), \n                                  tf.keras.layers.Dense(30, activation = 'selu'),\n                                  tf.keras.layers.Dropout(0.1),\n                                  tf.keras.layers.Dense(20, activation = 'selu'),\n                                  tf.keras.layers.Dropout(0.1),\n                                  tf.keras.layers.Dense(p_encodedDims, activation = 'selu') ])\n        # reconstruction \/ decoder\n        self.__decoder__ = tf.keras.models.Sequential([                   \n                                  tf.keras.layers.Dense(20, activation = 'selu'),\n                                  tf.keras.layers.Dropout(0.1),\n                                  tf.keras.layers.Dense(30, activation = 'selu'),\n                                  tf.keras.layers.Dropout(0.1),\n                                  tf.keras.layers.Dense(p_outputDims, activation = 'relu') ])            \n        return\n        \n    def call(self, inputs):\n        v_encoded = self.__encoder__(inputs)\n        v_decoded = self.__decoder__(v_encoded)\n        return v_decoded\n    \n    def getEncoder(self):\n        return self.__encoder__\n\nclass AutoEncoder():\n    \n    __model__   = None\n    __encoder__ = None\n    \n    def __init__(self, p_model):\n        self.__model__   = p_model\n        self.__encoder__ = self.__model__.get_layer(self.__model__.layers[0].name)\n        self.__encoder__.summary()\n        return\n    \n    def set_threshold(self, X_data, y_data):\n        y_pred = self.__model__.predict(X_data.values)\n        v_loss = tf.keras.losses.mean_squared_logarithmic_error(y_data, y_pred).numpy()\n        self.__threshold__ = np.mean(v_loss) + 1.8 * np.std(v_loss)\n        return \n    \n    def getPredictions(self, X_data, y_data):\n        y_pred = self.__model__.predict(X_data.values)\n        v_loss = pd.Series(tf.keras.losses.mean_squared_logarithmic_error(y_data, y_pred).numpy(), index = X_data.index)\n        v_loss = ( v_loss <= self.__threshold__ ).astype(int)\n        y_pred = pd.DataFrame(self.__encoder__.predict(X_data.values), index = X_data.index)\n        y_pred.columns = [f'Dimension__{col + 1}' for col in y_pred.columns]\n        y_pred['Common'] = v_loss\n        display(v_loss.value_counts())\n        return y_pred\n\ntry:\n    v_path = '\/kaggle\/input\/professionals-common-profiles-autoencoder\/autoencoder_Model.dmp'\n    v_autoencoder = tf.keras.models.load_model(v_path)\nexcept:\n    v_path = 'autoencoder_Model.dmp'\n    v_BATCH_SIZE = 16\n\n    # define our early stopping\n    early_stop = tf.keras.callbacks.EarlyStopping( monitor   = 'val_loss',\n                                                   min_delta = 0.0001,\n                                                   patience  = 10,\n                                                   verbose   = 1, \n                                                   mode      = 'min',\n                                                   restore_best_weights = True ) \n\n    v_autoencoder = AutoEncoderBase( p_inputDims   = X_train.shape[1], \n                                     p_outputDims  = X_train.shape[1], \n                                     p_encodedDims = 9 )\n    v_autoencoder.compile(loss = 'mse', metrics = ['msle'], optimizer = 'adam')\n\n    # Train the model\n    history = v_autoencoder.fit( X_train, X_train,\n                                 shuffle    = True,\n                                 epochs     = 25,\n                                 batch_size = v_BATCH_SIZE,\n                                 verbose    = 2,\n                                 callbacks  = [early_stop],\n                                 validation_data = (X_valid, X_valid) )\n\n    v_autoencoder.save(v_path)\n    v_autoencoder = tf.keras.models.load_model(v_path)\n\nv_autoencoder = AutoEncoder(v_autoencoder)\n\n# Compute the threshold based on the training data\nv_autoencoder.set_threshold(X_train, X_train)\n\n# Create predictions\nv_predEnc = pd.concat([ v_autoencoder.getPredictions(X_dataD, X_dataD), X_data ], axis = 1) \n\n# Display the distribution of the profiles\nv_plot = ( v_predEnc['Common'].fillna(2).value_counts() \/ v_predEnc.shape[0] * 100 ).reset_index()\nv_plot.columns = ['Common Profile', 'Percentage']\nv_plot['Common Profile'] = v_plot['Common Profile'].replace({0: 'No', 1: 'Yes'})\n\nfig = px.bar(v_plot, y = 'Common Profile', x = 'Percentage', color = 'Common Profile',\n             labels={'pop':'population of Canada'}, height=400)\nfig.show()\n\n# Select only the common profiles\nv_predEnc = v_predEnc[ v_predEnc['Common'] == 1 ].drop('Common', axis = 1)","6c09caf4":"from yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster     import KMeans\n\nX_kmean = v_predEnc[[col for col in v_predEnc.columns if 'Dimension__' in col]]\nv_model = KMeans()\n# k is range of number of clusters.\nvisualizer = KElbowVisualizer(v_model, k = (2, 30), timings = False, random_state = 2013)\nvisualizer.fit(X_kmean)              # Fit data to visualizer\nvisualizer.show()                    # Finalize and render figure\n\nv_kmean = KMeans(n_clusters = 9, random_state = 2013)\nv_kmean.fit(X_kmean)\nX_clusters = v_predEnc[[col for col in v_predEnc.columns if not 'Dimension__' in col]].copy()\nX_clusters['Cluster'] = v_kmean.predict(X_kmean)","b573740a":"import lightgbm as lgb\nimport sklearn.metrics as metrics\n\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n    target_names: given classification classes such as [0, 1, 2] the class names, for example: ['high', 'medium', 'low']\n    title:        the text to display at the top of the matrix\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n        \n    cm = pd.DataFrame(cm.tolist())\n    for col in cm.columns:\n        cm[col] = cm[col].apply(lambda x: 0 if x < 0.1 else np.round(x, 2))\n    cm = cm.values\n    \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, cm[i, j] if cm[i, j] > 0 else '',\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    ax = plt.gca()\n    ax.grid(False)\n    plt.show()\n    return\n\n# Create the ytraining and validation dataset\nX_lgb = pd.get_dummies(X_clusters.drop(['Cluster'], axis = 1).fillna('***NAV'))\ny_lgb = X_clusters['Cluster']\n\nX_train, X_valid, y_train, y_valid = train_test_split( X_lgb, y_lgb,\n                                                       test_size    = 0.3, \n                                                       random_state = 2013,\n                                                       stratify     = y_lgb )\nprint('Total number of records: ', X_data.shape, y_data.shape)\nprint('Train records: ', X_train.shape, y_train.shape)\n\n# Set the parameters for the LGB Model\nv_params = {}\nv_params['objective']        = 'multiclass' # the target to predict is the number of clusters\nv_params['is_unbalance']     = False\nv_params['n_jobs']           = -1\nv_params['random_state']     = 2013\nv_params['metric']           = ['multi_logloss']\nv_params['num_class']        = y_lgb.nunique() + 1\nv_params['learning_rate']    = 0.05\nv_params['num_leaves']       = 30\nv_params['max_depth']        = 9\nv_params['feature_fraction'] = 0.75\nv_params['bagging_fraction'] = 0.75\nv_params['verbosity'] = -1\n\n# Train the LGB model\nv_lgbtrain = lgb.Dataset(X_train, y_train)\nv_lgbvalid = lgb.Dataset(X_valid, y_valid)\nv_model    = lgb.train( v_params, v_lgbtrain, 2000, valid_sets = [v_lgbtrain, v_lgbvalid], early_stopping_rounds = 100, \n                        verbose_eval = 20 )\n\n# Reduce the number of features used and retrain the model if we have more than 75 features in the base dataset\nif X_train.shape[1] > 75:\n    v_features = pd.DataFrame(sorted(zip(X_train.columns, v_model.feature_importance('gain'))), columns=['Feature', 'Value'])\n    v_features = v_features.sort_values('Value', ascending = False).head(75)['Feature'].tolist()\n    X_lgb = X_lgb[v_features]\n    v_lgbtrain = lgb.Dataset(X_train[v_features], y_train)\n    v_lgbvalid = lgb.Dataset(X_valid[v_features], y_valid)\n    v_model    = lgb.train( v_params, v_lgbtrain, 2000, valid_sets = [v_lgbtrain, v_lgbvalid], early_stopping_rounds = 100, \n                            verbose_eval = 20 )\n\n# Generate the predictions\nv_pred = pd.DataFrame(v_model.predict(X_lgb, num_iteration = v_model.best_iteration), index = X_lgb.index)\nv_pred['__Pred'] = v_pred.apply(lambda x: np.argmax(x), axis = 1)\nv_pred['__Real'] = y_lgb\nv_pred[['__Real', '__Pred']].value_counts()\n\n# Show the confusion matrix\nv_confMatrix = metrics.confusion_matrix(v_pred['__Real'], v_pred['__Pred'])\nplot_confusion_matrix(v_confMatrix, [f'Cluster_{label + 1}' for label in sorted(v_pred['__Real'].unique())] )","81684d4b":"import shap\n\n# Calculate the SHAP values and plot the summary\nv_explainer   = shap.TreeExplainer(v_model)\nv_shap_values = v_explainer.shap_values(X_lgb)\nshap.summary_plot(v_shap_values, X_lgb, max_display = 20)","ddb6607f":"!pip install pyvis","cea00386":"from pyvis.network import Network\nimport json\n\nv_options = {\n  \"nodes\": {\n    \"borderWidth\": 3,\n    \"borderWidthSelected\": 5,\n    \"fixed\": {\n      \"x\": True\n    }\n  },\n  \"edges\": {\n    \"arrows\": {\n      \"to\": {\n        \"enabled\": True\n      }\n    },\n    \"color\": {\n      \"inherit\": True\n    },\n    \"smooth\": False\n  },\n  \"layout\": {\n    \"hierarchical\": {\n      \"enabled\": True,\n      \"treeSpacing\": 225,\n      \"sortMethod\": \"directed\"\n    }\n  },\n  \"physics\": {\n    \"hierarchicalRepulsion\": {\n      \"centralGravity\": 0\n    },\n    \"minVelocity\": 0.75,\n    \"solver\": \"hierarchicalRepulsion\"\n  }\n}\n\nv_profilesPath = { 'Profile 1': ['Senior ML', 'Older\\nThan 35 years|S'],\n                   'Profile 3': ['Medium ML', 'Younger\\nThan 35 years|M'],\n                   'Profile 9': ['Medium ML', 'Older\\nThan 35 years|M'],\n                   'Profile 5': ['Junior ML', 'Older\\nThan 35 years|J', 'Large\\nCompany|J'],\n                   'Profile 4': ['Junior ML', 'Older\\nThan 35 years|J', 'Small\/Medium\\nCompany|J'],                   \n                   'Profile 8': ['Junior ML', 'Younger\\nThan 35 years|J', 'Very Junior\\nExp.Dev|J',],\n                   'Profile 2': ['Junior ML', 'Younger\\nThan 35 years|J', 'Junior\\nExp.Dev|J', 'Bachelor\\nDegree|J'],\n                   'Profile 7': ['Junior ML', 'Younger\\nThan 35 years|J', 'Junior\\nExp.Dev|J', 'Master\\nDegree|J'],\n                   'Profile 6': ['Junior ML', 'Younger\\nThan 35 years|J', 'Medium\\nExp.Dev|J',], }\nv_group = { key: idx + 2 for idx, key in enumerate(list(v_profilesPath.keys())) }\n\nv_network_S = Network(notebook = True, height = '500px', width = '600px')\nv_network_J = Network(notebook = True, height = '780px', width = '1000px')\n\nv_network_S.add_node(2, label = 'Senior ML', size = 25, color = 'red')\nv_network_S.add_node(3, label = 'Medium ML', size = 25, color = 'red')\nv_network_J.add_node(4, label = 'Junior ML', size = 25, color = 'red')\n\nv_nodes = { 'Experience ML': 1, 'Senior ML': 2, 'Medium ML': 3, 'Junior ML': 4 }\nfor key, paths in v_profilesPath.items():\n    v_network = (      v_network_S if paths[0] == 'Senior ML'\n                  else v_network_S if paths[0] == 'Medium ML'\n                  else v_network_J )\n    v_lastNode = paths[0]\n    for node in paths:\n        if not node in v_nodes.keys(): \n            v_nodes[node] = len(v_nodes) + 1\n            v_network.add_node(v_nodes[node], label = (node if not '|' in node else node.split('|')[0]), size = 6, group = v_group[key])\n            v_network.add_edge(v_nodes[v_lastNode], v_nodes[node], weight = 3.5)\n        v_lastNode = node\n    \n    v_nodes[key] = len(v_nodes) + 1\n    v_network.add_node(v_nodes[key], label = key, color = 'blue', size = 15)\n    v_network.add_edge(v_nodes[v_lastNode], v_nodes[key])","ca92e7dc":"v_network_S.set_options(json.dumps(v_options))\nv_network_S.show('networkS.html')","2868a02c":"v_network_J.set_options(json.dumps(v_options))\nv_network_J.show('networkJ.html')","7c95baab":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nv_profileName = {}\nv_avgFeat  = X_lgb.describe().T[['mean']]\nv_clusters = []\nfor cluster in sorted(y_lgb.unique()):\n    v_data = X_lgb.copy()\n    v_data['Cluster'] = y_lgb\n    v_dataC = v_data[ v_data['Cluster'] == cluster ] \n    v_data = v_dataC.drop('Cluster', axis = 1).describe().T[['mean']]\n    v_dataC = np.round(v_dataC.shape[0] \/ X_lgb.shape[0] * 100, 2)\n    v_profileName[cluster] = v_profilesPath[f'Profile {cluster + 1}'][0] + f'\\nProfile {cluster + 1}\\n({v_dataC}%)'\n    v_clusters.append(v_data[   (v_data['mean'] > 0.7)\n                              | (v_data['mean'] < 0.3) ].rename(columns = {'mean': v_profileName[cluster]}))\n\ndef maskImportant(p_data):\n    p_data = p_data.T\n    \n    v_mask = p_data.copy().fillna(1)\n    for col in v_mask.columns:\n        v_mask[col] = v_mask[col].apply(lambda x: 0 if x < 0.1 else x)\n    v_mask['Sum'] = v_mask.sum(axis = 1)\n    p_data = p_data[ v_mask['Sum'] > 0 ]\n    \n    v_mask = p_data.copy()\n    for col in v_mask.columns:\n        v_mask[col] = v_mask[col].apply(lambda x: 0 if np.isnan(x) else 1)\n    v_mask['__Rank'] = v_mask.sum(axis = 1)\n    \n    for col in p_data.columns:\n        p_data[col] = p_data[col].apply(lambda x: 0 if x < 0.05 else x)\n    p_data['__Sum']  = p_data.fillna(0).sum(axis = 1)\n    v_idx = p_data[ p_data['__Sum'] == 0 ].index\n    p_data['__Rank'] = v_mask['__Rank']        \n    p_data.loc[v_idx, '__Rank'] = p_data['__Rank'].max() + 1\n    \n    v_mask = v_mask.sort_values('__Rank', ascending = False)\n    v_mask = v_mask[ v_mask['__Rank'] > 0 ].index.values\n    p_data = p_data.loc[v_mask, :]\n\n    return p_data.sort_values(['__Rank', '__Sum'], ascending = [False, False]).drop(['__Rank', '__Sum'], axis = 1)\n\ndef plotProfile(p_data, p_top = 100, p_avgFeat = v_avgFeat):\n    v_data = p_data.reset_index().merge(p_avgFeat.reset_index(), how = 'inner', on = 'index').set_index('index')\n    for col in p_data.columns:\n        v_data[col] = np.round(v_data[col] \/ v_data['mean'], 3)\n        v_data[col] = v_data[col].replace({0: np.NaN})\n    v_data = v_data[['mean'] + p_data.columns.tolist()]\n        \n    v_figSize = ( (4 * p_data.shape[1]) if p_data.shape[1] > 1 else 6, 0.4 * p_data.shape[0] )        \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = v_figSize)\n    sns.heatmap(np.round(p_data.head(p_top), 2), annot = True, linewidths = .5, cmap = \"Blues\", ax = ax1)\n    sns.heatmap(np.round(v_data.head(p_top), 2), annot = True, linewidths = .5, cmap = \"Reds\", ax = ax2)\n    ax1.xaxis.tick_top()\n    ax1.title.set_text('Profiles')\n    ax2.xaxis.tick_top()\n    ax2.yaxis.set_visible(False)\n    ax2.title.set_text('Difference with AVG')\n    plt.subplots_adjust(left = 0.1,bottom = 0.1, right = 0.9, top = 0.9, wspace = 0.4, hspace = 0.4)\n    plt.show()\n    return\n    \nv_clusters = maskImportant(pd.concat(v_clusters, axis = 1).T)\nv_clusters_F = v_clusters.T\n\nfor item in [ 'ML_years__Categ_Not_Junior',\n              'Age__Categ_More Than 35 Years Old',\n              'Dev_Experience_2__Level_Junior' ]:\n    plotProfile(maskImportant(v_clusters_F[ np.round(v_clusters_F[item], 0) == 1 ]))\n    v_clusters_F = maskImportant(v_clusters_F[ np.round(v_clusters_F[item], 0) != 1 ]).T\n\nplotProfile(v_clusters_F.T)","71241b58":"X_cluster = pd.concat([y_lgb, X_lgb], axis = 1)\nX_cluster['Cluster'] = X_cluster['Cluster'].apply(lambda x: v_profileName[x])\nv_data = []\nfor col in X_lgb.columns:\n    v_tmp = X_cluster[ X_cluster[col] != 0 ]\n    v_tmp = v_tmp.groupby('Cluster')[[col]].sum() \/ v_tmp[col].sum() * 100\n    v_data.append(v_tmp)\nv_data = pd.concat(v_data, axis = 1).T.fillna(0)\nv_cols = v_data.columns\nv_data['__Min']  = v_data[v_cols].min(axis = 1)\nv_data['__Max']  = v_data[v_cols].max(axis = 1)\nv_data['__Diff'] = v_data['__Max'] - v_data['__Min']\nv_data = ( v_data[ v_data['__Max'] > 20 ]\n              .sort_values('__Diff', ascending = False)\n              .drop(['__Min', '__Max'], axis = 1) \n              .rename(columns = {'__Diff': 'Difference\\nBetween\\nMin and Max'}) )\nv_data['__Total']  = v_data[v_cols].sum(axis = 1)\nfig, ax = plt.subplots(figsize = ( 1.3 * v_data.shape[1], 0.5 * v_data.shape[0] ))\nsns.heatmap( np.round(v_data, 2), annot = True, linewidths = .5, cmap = \"viridis_r\", fmt = '.0f', vmax = 40, ax = ax )\nax.xaxis.tick_top()\nplt.show()\n\nprint('\\n')\nv_data.drop(v_data.columns.tolist()[-2:], axis = 1, inplace = True)\nfor col in v_data.columns:\n    v_perc = float(col.split('\\n')[2].split('%')[0][1:])\n    v_data[col] = v_data[col] \/ v_perc\n    v_data[col] = v_data[col].apply(lambda x: np.NaN if x < 0.02 else x)\n\nv_colors  = sns.color_palette(\"Reds_r\")[:2] + sns.color_palette(\"Greys\")[:1] * 4 + sns.color_palette(\"Blues\")[-2:]\nfig, ax = plt.subplots(figsize = ( 1.3 * v_data.shape[1], 0.5 * v_data.shape[0] ))\nsns.heatmap( np.round(v_data, 2), annot = True, linewidths = .5, cmap = v_colors, fmt = '.2f', vmin = 0.5, vmax = 2, ax = ax )\nax.xaxis.tick_top()\nplt.title('Rate Difference with expected value')\nplt.show()","e7f0efdb":"def layoutPerProfile( p_data, p_cluster, p_cols = [('N\/A', 'N\/A')], p_total = False, p_profileName = v_profileName, p_title = None, p_debug = False,\n                      p_groupValues = False, p_minGroup = 12 ):    \n    v_dataA = []\n    v_valuesU_All = []\n    v_valuesAvg_All = []\n    v_return = {}\n    v_mergedDS = p_cluster.reset_index().merge(p_data.reset_index(), how = 'inner', on = 'index')\n    if p_debug: print(v_mergedDS.shape)\n    v_none = '*** None'\n    for item in p_cols:\n        # Select the merged data for the column of interest\n        v_col  = item[0]\n        v_data = v_mergedDS[['index', 'Cluster', v_col]].copy()\n        v_data[v_col] = v_data[v_col].fillna(v_none).replace({'None': v_none, '': v_none})\n        v_data[v_col] = v_data[v_col].apply(lambda x: v_none if x.strip() in ['No \/ None', ''] else x)\n        v_data[f'{v_col}__L'] = v_data[v_col].apply(lambda x: x.split('__')).apply(lambda x: [v_none] if len(x) == 0 else x)\n        \n        # Extract all the possible values for the given feature\n        v_valuesU = []\n        for value in pd.Series(v_data[v_col].unique()).apply(lambda x: x.split('__')).tolist():\n            v_valuesU += value\n        v_values = list(sorted(set(v_valuesU)))\n                \n        #----------------------------------------------------------------------------------------------------\n        # Create the columns based on the extracted features\n        v_valuesU = v_data[['index', 'Cluster']].copy()\n        for value in v_values:\n            v_valuesU[value.strip()] = v_data[f'{v_col}__L'].apply(lambda x: value in x).astype(int)            \n        v_valuesU = v_valuesU.set_index(['index', 'Cluster']).stack().reset_index()\n        v_groupedCols    = {}\n        v_groupedColsAll = []\n        if p_groupValues:\n            v_valuesU2 = v_valuesU.pivot_table( index   = ['index', 'Cluster'],\n                                                columns = 'level_2',\n                                                values  = 0 )\n            v_cols   = [col for col in v_valuesU2.columns if col == v_none]\n            v_values = v_valuesU2.drop(v_cols, axis = 1).sum().to_dict()\n            v_values = sorted(set([value.split(' ')[0] for value in v_values.keys() if len(value.split(' ')[0]) > 4\n                                                                                   and value.split(' ')[0] not in v_values.keys()]))\n            for value in v_values:\n                v_cols = [col for col in v_valuesU2.columns if col.split(' ')[0] == value]\n                if len(v_cols) > 1:\n                    v_col = f'Grouped__{value}'\n                    v_valuesU2[v_col] = v_valuesU2[v_cols].max(axis = 1)\n                    v_groupedCols[v_col] = v_cols\n                    \n            if item[0] == 'BigData_Product':\n                v_col = f'Grouped___NotCloud'\n                v_groupedCols[v_col] = ['Microsoft SQL Server', 'MongoDB', 'MySQL', 'Oracle Database', 'PostgreSQL', 'SQLite', 'IBM Db2',]\n                v_valuesU2[v_col] = v_valuesU2[v_groupedCols[v_col]].max(axis = 1)\n                \n                v_col2 = f'Grouped___Cloud'\n                v_groupedCols[v_col2] = [value for value in v_valuesU['level_2'].unique() if value not in v_groupedCols[v_col] + [v_none, 'Other',]]\n                v_valuesU2[v_col2] = v_valuesU2[v_groupedCols[v_col2]].max(axis = 1)                \n                    \n            v_values = v_valuesU2[list(v_groupedCols.keys())].sum() \/ v_valuesU2.shape[0] * 100\n            v_values = v_values[ v_values < 10 ]\n            v_groupedCols = {key: value for key, value in v_groupedCols.items() if key not in v_values.index.tolist()}\n            v_valuesU2 = v_valuesU2[list(v_groupedCols.keys())].stack().reset_index()\n            v_valuesU  = pd.concat([v_valuesU, v_valuesU2])               \n            for value in v_groupedCols.values():\n                v_groupedColsAll += value\n        v_valuesU['level_2'] = v_valuesU['level_2'].apply(lambda x: f'{item[1]} - {x}')\n        if p_debug: print('Total number of cases - Step 01:', v_valuesU[[0]].sum().sum(), v_valuesU2[[0]].sum().sum())   \n                    \n        if item[0] == 'Role_Title':\n            v_valuesU['level_2'] = v_valuesU['level_2'].apply(lambda x: 'Engineer' if 'Engineer' in x else x)\n            \n        #----------------------------------------------------------------------------------------------------\n        # Group values into Other(s) if below a certain threshold\n        v_valuesAvg = v_valuesU.groupby('level_2')[[0]].sum().sort_values(0, ascending = False).reset_index().reset_index()\n        v_valuesAvg['Avg'] = v_valuesAvg[0] \/ v_data.shape[0] * 100\n        v_valuesAvg = v_valuesAvg[ ~(   ( v_valuesAvg['index'] < 10 )\n                                      & ( v_valuesAvg['Avg'] > p_minGroup ) ) ]['level_2'].tolist()\n        v_valuesAvg = {value: f'_ {item[1]} - Other(s)' for value in v_valuesAvg if value not in list(v_groupedCols.keys())}   \n        v_groupedColsAll = [value for value in v_valuesAvg.keys() if value in v_groupedColsAll]\n        v_valuesU = v_valuesU[ ~v_valuesU.isin(v_groupedColsAll) ]\n        v_valuesU['level_2'] = v_valuesU['level_2'].replace(v_valuesAvg)        \n        v_return[v_col] = [value.replace(f'{item[1]} - ', '') for value in v_valuesU['level_2'].unique()]        \n\n        #----------------------------------------------------------------------------------------------------\n        # Aggregate the data\n        v_valuesU = v_valuesU.groupby(['index', 'Cluster', 'level_2'])[[0]].max().reset_index()        \n        v_valuesU = v_valuesU.groupby(['Cluster', 'level_2']).agg({0: ['sum', 'count']}).reset_index()\n        v_valuesU.columns = ['Cluster', v_col, 'Count', 'Cluster_No'] \n        v_clusterNo = v_valuesU[['Cluster', 'Cluster_No']].drop_duplicates()\n        v_clusterNo['Cluster_No'] = np.round(v_clusterNo['Cluster_No'] \/ v_mergedDS.shape[0] * 100, 2)                 \n        v_valuesU = v_valuesU.pivot_table( index   = ['Cluster', 'Cluster_No'],\n                                           columns = v_col,\n                                           values  = 'Count',\n                                           aggfunc = np.sum )  \n        v_valuesU.columns = v_valuesU.columns.tolist()\n        v_valuesAvg = v_valuesU.sum().reset_index()\n        v_valuesAvg['Avg'] = v_valuesAvg[0] \/ v_data.shape[0] * 100\n        v_valuesAvg = v_valuesAvg.set_index('index').sort_values('Avg', ascending = False)\n            \n        #----------------------------------------------------------------------------------------------------         \n        if p_total: \n            v_valuesU['__Total'] = v_valuesU.sum(axis = 1)        \n            v_valuesAvg.loc['__Total', 'Avg'] = 100\n        \n        #----------------------------------------------------------------------------------------------------         \n        v_valuesAvg_All.append(v_valuesAvg)\n        if p_debug: print('Total number of cases - Step 02:', v_valuesU.sum().sum())\n        \n        #----------------------------------------------------------------------------------------------------         \n        v_valuesU = v_valuesU.reset_index().set_index('Cluster')\n        for col in v_valuesU.drop('Cluster_No', axis = 1).columns:\n            v_valuesU[col] = v_valuesU[col] \/ v_valuesU['Cluster_No'] * 100\n        \n        #----------------------------------------------------------------------------------------------------\n        v_valuesU = v_valuesU.drop('Cluster_No', axis = 1).T\n        v_valuesU_All.append(v_valuesU)\n    \n    #----------------------------------------------------------------------------------------------------\n    v_valuesAvg_All = pd.concat(v_valuesAvg_All)\n    if p_debug:\n        print('Total number of cases - Step 03:', p_data.shape, p_cluster.shape, v_mergedDS.shape)\n        display(v_clusterNo) \n    v_clusterNo = v_clusterNo.set_index('Cluster').to_dict(orient = 'index')\n    v_profileName = { key: value.split('\\n') for key, value in p_profileName.items() }\n    v_profileName = { key: [ ( 'Ju.' if value[0].strip() == 'Junior ML' else 'Me.' if value[0].strip() == 'Medium ML' else 'Se.' ), \n                                value[1], f\"({v_clusterNo[key]['Cluster_No']}%)\" ] for key, value in v_profileName.items() }\n    v_profileName = { key: '\\n'.join(value) for key, value in v_profileName.items() }\n    v_valuesU_All = pd.concat(v_valuesU_All)\n    v_valuesU_All.columns = [v_profileName[col] for col in v_valuesU_All.columns]\n    v_valuesU_All = v_valuesU_All[sorted(list(v_valuesU_All.columns))]\n    \n    #----------------------------------------------------------------------------------------------------\n    v_sort = v_valuesU_All.index.tolist()\n    v_data_01 = v_valuesU_All.copy()   \n    v_data_02 = pd.concat([v_valuesAvg_All, v_valuesU_All], axis = 1)\n    for col in v_valuesU_All.columns:\n        v_data_02[col] = np.round(v_data_02[col] \/ v_data_02['Avg'] - 1, 2)\n    v_data_02 = v_data_02[v_valuesU_All.columns]\n\n    v_figSize = (20, 0.6 * v_data_01.shape[0])\n    v_colors  = sns.color_palette(\"Reds_r\")[:2] + sns.color_palette(\"Greys\")[:1] * 4 + sns.color_palette(\"Blues\")[-2:]\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = v_figSize, gridspec_kw={'width_ratios': [9, 1, 9]})\n    sns.heatmap(np.round(v_data_01.loc[v_sort, :], 2), annot = True, fmt = '.0f', linewidths = .5, cmap = \"viridis_r\", vmax = 75, ax = ax1)\n    sns.heatmap(np.round(v_valuesAvg_All.loc[v_sort, ['Avg']], 2), annot = True, fmt = '.0f', linewidths = .5, cmap = \"viridis_r\", vmax = 75, ax = ax2, cbar=False)\n    sns.heatmap(np.round(v_data_02.loc[v_sort, :], 2), annot = True, linewidths = .5, cmap = v_colors, vmin = -0.5, vmax = 0.5, ax = ax3)\n    ax1.xaxis.tick_top()\n    ax1.title.set_text('Profiles')\n    ax2.xaxis.tick_top()\n    ax2.yaxis.set_visible(False)\n    ax2.xaxis.set_visible(False)\n    ax2.title.set_text('Average')\n    ax3.yaxis.set_visible(False)\n    ax3.xaxis.tick_top()\n    ax3.title.set_text('Difference with Median')\n    plt.subplots_adjust(left = 0.1,bottom = 0.1, right = 0.9, top = 0.9 if p_title is None else 0.8, wspace = 0.1, hspace = 0.1)    \n    plt.suptitle(p_title)  \n    plt.show()\n    \n    return [ v_return, v_valuesAvg_All, v_data_01 ]\n\nv_profileChar = {}\nv_profileChar['Role_Title'] = layoutPerProfile(X_baseData, y_lgb, [('Role_Title', 'Role Title')], p_total = True, p_minGroup = 10 )\nv_profileChar['Residence_Country'] = layoutPerProfile( X_professionals, y_lgb, [('Residence_Country__Filtered', 'Residence Country')], \n                                                       p_total = True, p_minGroup = 7 )\n\nX_professionalsF = X_baseData[['Role_Title']].reset_index().merge(X_professionals.reset_index(), how = 'inner', on = 'index').set_index('index')\nX_professionalsF['Role_Title__Grouped'] = X_professionalsF['Role_Title'].apply(lambda x: x if x in v_profileChar['Role_Title'][0]['Role_Title']\n                                                                                     else 'Engineer' if 'Engineer' in x\n                                                                                     else '_Other(s)' )\nv_profileChar['Role_Title'][2] = v_profileChar['Role_Title'][2].drop('__Total')\nv_profileChar['Residence_Country'][2] = v_profileChar['Residence_Country'][2].drop('__Total')","2abcf792":"def layoutPerProfileRole(X_professionalsF, y_lgb, p_cols, p_minGroup, p_debug = False, p_groupValues = True):\n    v_data_01, v_data_02 = [], []\n    for role in sorted(X_professionalsF['Role_Title__Grouped'].unique()):\n        print('\\n***********************************************************************************')\n        print(role)\n        _, v_tmp_01, v_tmp_02 = layoutPerProfile( X_professionalsF[ X_professionalsF['Role_Title__Grouped'] == role ], y_lgb, p_cols, \n                                                  p_debug = p_debug, p_groupValues = p_groupValues, p_minGroup = p_minGroup )\n        v_tmp_01['Role'] = role\n        v_tmp_02['Role'] = role\n        v_data_01.append(v_tmp_01)\n        v_data_02.append(v_tmp_02)\n        \n    v_data_01 = pd.concat(v_data_01)\n    v_data_02 = pd.concat(v_data_02)\n    v_dataG = v_data_01.reset_index()\n    v_dataG = v_dataG[ v_dataG['Avg'] > 30 ]\n    v_dataG = v_dataG.pivot_table( index   = 'index',\n                                   columns = 'Role',\n                                   values  = 'Avg')\n    v_dataG.index = v_dataG.index.tolist()\n    v_figSize = (10, 0.6 * v_dataG.shape[0])\n    fig, ax = plt.subplots(figsize = v_figSize)\n    sns.heatmap(np.round(v_dataG, 2), annot = True, fmt = '.0f', linewidths = .5, cmap = \"viridis_r\", vmin = 50, vmax = 75, ax = ax)    \n    ax.xaxis.tick_top()\n    ax.title.set_text('Average Usage by Role - minimum average = 30%')\n    plt.show()\n    \n    return [None, v_data_01, v_data_02]\n    \nv_profileChar['BigData_BI'] = layoutPerProfileRole(X_professionalsF, y_lgb, [ ('BigData_Product', 'BigData_Product'),\n                                                                              ('BigData_Product_Most_Used', 'BigData_Product_Most_Used'),\n                                                                              ('BI_Tool', 'BI_Tool') ], p_minGroup = 30 )","c1437e72":"v_cols = [ ('Cloud_Computing_Platform',           'Cloud Computing Platform'),\n           ('Cloud_Computing_Platform_Most_Used', 'Cloud Computing Platform Most Used'),\n           ('Cloud_Computing_Products',           'Cloud Computing Products'),\n           ('Data_Storage_Products',              'Data Storage Products') ]\nv_profileChar['Cloud'] = layoutPerProfileRole(X_professionalsF, y_lgb, v_cols, p_minGroup = 25 )","a1237f08":"v_profileChar['ML_Auto'] = layoutPerProfileRole(X_professionalsF, y_lgb, [ ('ML_Managed', 'ML_Managed'),\n                                                                           ('ML_Auto_01_Tools', 'ML_Auto_01_Tools'),\n                                                                           ('ML_Auto_02_Tools', 'ML_Auto_02_Tools') ], p_minGroup = 30 )","dbe9281f":"v_profileChar['Dev_IDE'] = layoutPerProfileRole(X_professionalsF, y_lgb, [ ('Dev_Programming_Language', 'Programming Language'),\n                                                                           ('IDE', 'IDE') ], p_minGroup = 33 )","a118eff8":"_ = layoutPerProfileRole(X_professionalsF, y_lgb, [ ('NLP_Algorithm', 'NLP_Algorithm'),\n                                                ('Computer_Vision_Methods', 'Computer_Vision_Methods'),\n                                                ('Most_Used_Analyse_Tool', 'Most_Used_Analyse_Tool') ], p_minGroup = 25 )","1fb0ee02":"_ = layoutPerProfileRole(X_professionalsF, y_lgb, [ ('Hosted_Notebooks', 'Hosted_Notebooks') ], p_minGroup = 10 )","4e04a4fe":"_ = layoutPerProfileRole(X_professionalsF, y_lgb, [ ('ML_Algorithm', 'ML_Algorithm'),\n                                                    ('ML_Framework', 'ML_Framework') ], p_minGroup = 25 )","8300750f":"from plotly.subplots import make_subplots\n\nv_salaries = []\nfor roleTitle in sorted(X_professionalsF['Role_Title__Grouped'].unique()):\n    v_data = y_lgb.reset_index().merge(X_professionalsF[ X_professionalsF['Role_Title__Grouped'] == roleTitle ].reset_index(), how = 'inner', on = 'index').set_index('index')\n    v_map = {key: ' - '.join(value.split('\\n')[:2]) for key, value in v_profileName.items()}\n    v_data['Cluster'] = v_data['Cluster'].replace(v_map)\n    v_map = { '1|Europe|Others':          '2. Europe',\n              '2|North America|USA':      '1. USA',\n              '2|North America|Others':   '6. Others',\n              '3|South America|Others':   '6. Others',\n              '4|Africa|Others':          '5. Africa',\n              '5|Asia|India':             '3. India',\n              '5|Asia|Others':            '4. Asia (except India)',\n              '6|Oceania|Others':         '6. Others',\n              '9|Other|Other':            '6. Others' }\n    v_data['Residence_Country__Filtered'] = v_data['Residence_Country__Filtered'].replace(v_map)\n    v_data = v_data[ ~v_data['Residence_Country__Filtered'].isin(['6. Others']) ]\n    v_data = v_data.groupby(['Salary__Filtered', 'Cluster', 'Residence_Country__Filtered', 'Role_Title__Grouped'])[['Filling_Duration']].count()\n    v_data.columns = ['Count']\n    v_data.reset_index(inplace = True)\n    v_grp = ( v_data.groupby(['Cluster', 'Residence_Country__Filtered'])[['Count']].sum()\n                    .rename(columns = { 'Count': 'Count_Grp' }).reset_index() )\n    v_data = v_data.merge(v_grp, how = 'inner', on = ['Cluster', 'Residence_Country__Filtered'])\n    v_data['Percentage'] = np.round(v_data['Count'] \/ v_data['Count_Grp'], 4)\n\n    fig = make_subplots()\n    for valueF in v_data['Residence_Country__Filtered'].unique():\n        v_plot = v_data[ v_data['Residence_Country__Filtered'] == valueF ].copy()\n        v_plot['x'] = v_plot[['Residence_Country__Filtered', 'Cluster', ]].apply(lambda x: ' - '.join(x), axis = 1)\n        v_plot['y'] = v_plot['Salary__Filtered'].apply(lambda x: x.replace('|', ' - '))    \n        fig.add_trace( go.Scatter( x = v_plot['x'].values, y = v_plot['y'].values, \n                                   text = v_plot['Residence_Country__Filtered'], name = valueF, mode = 'markers',\n                                   marker = dict( size = v_plot[\"Percentage\"],\n                                                  sizeref = 2. * max(v_plot[\"Percentage\"])  \/ (9.**2), ),\n                                   hovertemplate = \"<b>%{text}<\/b><br><br>\"\n                                                     + \"Profile and Residence: %{x}<br>\" \n                                                     + \"Salary: %{y}<br>\" \n                                                     + \"Percentage: %{marker.size:%}\" \n                                                     + \"<extra><\/extra>\", ) )\n    fig.update_layout( xaxis  = dict(type = 'category',categoryorder = 'category ascending'),\n                       yaxis  = dict(type = 'category',categoryorder = 'category ascending'), \n                       height = 650, width = 1200 )\n    fig.update_layout( title_text = f\"Role Title - {roleTitle}\")\n    fig.show()    \n    v_salaries.append(v_data)\nv_salaries = pd.concat(v_salaries)","4a43c7ce":"for residence in sorted(v_salaries['Residence_Country__Filtered'].unique()):\n    v_data = v_salaries[ v_salaries['Residence_Country__Filtered'] == residence ]\n    \n    fig = make_subplots()\n    for valueF in v_data['Role_Title__Grouped'].unique():\n        v_plot = v_data[   ( v_data['Role_Title__Grouped'] == valueF )\n                         & ( v_data['Percentage'] > 0.09 ) ].copy()\n        v_plot['Salary__Filtered'] = v_plot['Salary__Filtered'].apply(lambda x: x.replace('|', ' - '))   \n        v_plot['x'] = v_plot[['Salary__Filtered', 'Role_Title__Grouped', ]].apply(lambda x: ' - '.join(x), axis = 1) \n        fig.add_trace( go.Scatter( x = v_plot['x'].values, y = v_plot['Cluster'].values, \n                                   name = valueF, mode = 'markers',\n                                   text = v_plot['Salary__Filtered'],\n                                   marker = dict( size = v_plot[\"Percentage\"],\n                                                  sizeref = 2. * max(v_plot[\"Percentage\"])  \/ (9.**2), ),\n                                   hovertemplate = \"<b>%{text}<\/b><br><br>\"\n                                                     + \"Salary and Role: %{x}<br>\" \n                                                     + \"Profile: %{y}<br>\" \n                                                     + \"Percentage: %{marker.size:%}\" \n                                                     + \"<extra><\/extra>\", ) )\n    fig.update_layout( xaxis  = dict(type = 'category',categoryorder = 'category ascending'),\n                       yaxis  = dict(type = 'category',categoryorder = 'category ascending'), \n                       height = 650, width = 1200 )\n    fig.update_layout( title_text = f\"Residence Country \/ Continent - {residence}\")\n    fig.show()","1fd70db4":"## 4.3 Feature Engineering for feature **<font color = 'blue'>Development Experience<\/font>**\n\nWe check the different **Development Experiences** that can be found in the dataset. We can see that we can use the following distribution:\n   - **13%** of respondents are **Very Junior** with less than 1 year of Development Experience\n   - **26%** of respondents are **Junior** with 1 to 3 years of Development Experience\n   - **37%** of respondents are **Medium** with 3 to 10 years of Development Experience\n   - **24%** of respondents are **Seniors** with more than 10 years of Development Experience","35f719c9":"# 1. Competition main information\n\nThis dataset provides insights from Kaggle\u2019s annual user survey focused on working data scientists.\n\nKaggle has conducted an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. The survey was live from 09\/01\/2021 to 10\/04\/2021, and after cleaning the data we finished with 25,973 responses!\n\nThe data scientists and ML engineers submitted responses on their backgrounds and day to day experience - everything from educational details to salaries to preferred technologies and techniques.\n\nThe challenge objective: tell a data story about a subset of the data science community represented in this survey, through a combination of both narrative text and data exploration.\n\n**As a <font color = 'blue'>Data Scientist<\/font> I want to understand who are the <font color = 'blue'>Kaggle Professionals<\/font> working with data** (as Data Scientists, Data\/ML Engineers, etc) and the **<font color = 'blue'>Common Profiles<\/font>** for these professionals.","831d6147":"### 5.5.2 Main characteristics distribution\n\nFindings:\n   - **<font color = 'blue'>70%<\/font>** of professionals that have a **Senior Level for Machine Learning** will be found into **Profile 1**\n   - **<font color = 'blue'>68%<\/font>** of professionals that have a **Very Junior Development Experience** will be found into **Profile 8** \n   - **<font color = 'blue'>80%<\/font>** of professionals that have a **Junior Development Experience** will be found into **Profile 2** and **Profile 7** \n   - **<font color = 'blue'>97%<\/font>** of professionals that have a **Medium Level for Machine Learning** will be found into **Profile 3** and **Profile 9** \n   - **<font color = 'blue'>90%<\/font>** of professionals that have a **Senior Development Experience** will be found into **Profile 4**, **Profile 5**, **Profile 9** and **Profile 1**\n   - Profiles **Profile 4** and **Profile 5** corresponds to **Junior Level for Machine Learning**, which means that the professionals into these 2 profiles have changed their carrer path from IT Development to ML Development\n   - **<font color = 'blue'>50%<\/font>** of professionals that have the role title **IT Other** will be found into **Profile 4** and **Profile 5** \n   - the professionals that have **Medium \/ Senior Level for Machine Learning** also have **Medium \/ Senior Development Experience**\n   - **<font color = 'blue'>88%<\/font>** of professionals from Africa are **Junior Level for Machine Learning**\n   - **<font color = 'blue'>80%<\/font>** of professionals from India are **Junior Level for Machine Learning**\n   - **<font color = 'blue'>70%<\/font>** of professionals from South America are **Junior Level for Machine Learning**\n   - **<font color = 'blue'>60%<\/font>** of professionals that have a **Doctoral Degree+** also have a **Medium \/ Senior Level for Machine Learning**\n   - **<font color = 'blue'>only 22%<\/font>** of companies having a **Very Small Data Science Team** will have professionals with a **Medium \/ Senior Level for Machine Learning**\n   - **<font color = 'blue'>only 12%<\/font>** of professionals that have a **Bachelor Degree** also have a **Medium \/ Senior Level for Machine Learning**\n   - **<font color = 'blue'>only 17%<\/font>** of professionals that have the role title **Software Engineer** also have a **Medium \/ Senior Level for Machine Learning**","fb75bcae":"\n## 3.7 Filter respondents based on the **<font color = 'blue'>Salary<\/font>**\n\nWe check the different **Salaries** that can be found in the dataset. We can see that we have the following distribution:\n   - **2.6%** of respondents **did not declare their salary** \n   - **40%** of respondents have a **Low Salary** \n   - **26%** of respondents have a **Medium Salary** \n   - **31%** of respondents have a **High Salary** \n\nFor the rest or the analysis we will only select the respondents that have **<font color = 'blue'>Declared their Salary<\/font>**.","101e00f6":"We will create **9** profiles for the Professionals.\n\nIn order to understand how performants are our profiles and also which are the common characteristics, we will create a LightGBM multi-classification model.","1c0bf18f":"## 2.1 Import Raw Data\n\nWhen importing the raw data we see that there are **25973** records and **369** features in the raw dataset.","63662bfc":"## 5.1. Create a training and validation dataset\n\nWe will split the dataset into train and validation.\n\nThe training dataset will used during the learning process by the model.\n\nThe validation dataset is a sample of data held back from training the model that is used to give an estimate of the model skill and performance.","36065630":"## 2.2 Merge features for multiple selection questions\n\nResponses to **multiple choice questions** (only a single choice can be selected) were recorded in individual columns. \n\nResponses to **multiple selection questions** (multiple choices can be selected) were split into multiple columns (with one column per answer choice). We will join all the columns linked to one question into one feature for easier processing and feature engineering.","440faa63":"\n## 3.5 Filter respondents based on the **<font color = 'blue'>Gender<\/font>**\n\nWe check the different **Genders** that can be found in the dataset. We can see that we have the following distribution:\n   - **1.72%** of respondents provided a different value than **Man** or **Women** \n   - **98.2%** of respondents have Declared their Gender\n\nFor the rest or the analysis we will only select the respondents that have a **<font color = 'blue'>Declared Gender<\/font>**.\n\nIn the filtered dataset we can see that we have **85%** of the respondents that are **Male** and **15%** of the respondents that are **Female**.","60013898":"---------","9d28c5df":"Findings:\n   - respondents having the role title **Data Analyst** will have **<font color = 'blue'>lower<\/font>** salaries then **Data Scientists** or **Engineers**\n   \n   <br>\n   \n   - in ***USA*** respondents having the role title **Data Analyst** and **Junior Level for Machine Learning** will have the **<font color = 'blue'>highest<\/font>** salaries for **Profile 5** (working for **Large Size Companies**)\n   - in ***USA*** respondents having the role title **Data Scientist** and **Very Junior Level for Machine Learning** will have **<font color = 'blue'>lowest<\/font>** salaries when compared to all the other **Data Scientist** profiles\n   - in ***USA*** respondents having the role title **Engineer** will have the **<font color = 'blue'>highest<\/font>** salary\n   \n   <br>\n   \n   - in ***Europe*** respondents having **Medium \/ Senior Level for Machine Learning** will have the **<font color = 'blue'>highest<\/font>** salaries\n   \n   <br>\n   \n   - in ***India*** respondents having the role title **Data Analyst** will have the **<font color = 'blue'>lowest<\/font>** salaries\n   - in ***India*** respondents having the role title **Data Analyst** and **Junior Level for Machine Learning** will have the **<font color = 'blue'>highest<\/font>** salaries for **Profile 5** (working for **Large Size Companies**)\n   - in ***India*** respondents having **Senior Level for Machine Learning** will have the **<font color = 'blue'>highest<\/font>** salaries\n   \n   <br>\n   \n   - in ***Asia*** respondents having the role title **Engineer** and **Other** and **Junior Level for Machine Learning** will have the **<font color = 'blue'>highest<\/font>** salaries for **Profile 5** (working for **Large Size Companies**)\n   \n   <br>\n   \n   - in ***Africa*** respondents having **Senior Level for Machine Learning** will have the **<font color = 'blue'>highest<\/font>** salaries","d0f22dbd":"### 5.5.5 **ML Managed** and **ML Auto**\n\nFindings:\n\n   - more than **<font color = 'blue'>75%<\/font>** of the roles are not using **ML Managed** and **ML Auto**, except for the **Data Scientists**\n   - **Medium \/ Senior Level for Machine Learning** roles are more likely to use **ML Managed** and **ML Auto**\n   \n   - **<font color = 'blue'>35%<\/font>** of the **Data Scientists** are using **ML Managed**\n   - **<font color = 'blue'>33%<\/font>** of the **Data Scientists** are using **ML Auto**","2843b88d":"## 4.4 Feature Engineering for feature **<font color = 'blue'>Residence Country<\/font>**\n\nWe check the different **Residence Country** that can be found in the dataset. We can see that we can use the following distribution:\n   - **42%** of respondents are from **Asia**, from which **22%** of respondents are from **India**\n   - **21%** of respondents are from **Europe**\n   - **14%** of respondents are from **North America**\n   - **7.5%** of respondents are from **Africa**\n   - **7%** of respondents are from **South America**\n   - **3.5%** of respondents are from **Oceania**","4355b344":"\n## 3.4 Filter respondents based on the **<font color = 'blue'>Experience with Machine Learning<\/font>**\n\nWe check the different **Machine Learning Years** that can be found in the dataset. We can see that we have the following distribution:\n   - **14%** of respondents **didn't reply** or **don't have Machine Learning Experience** \n   - **86%** of respondents have **Machine Learning Experience**\n\nFor the rest or the analysis we will only select the respondents that have **<font color = 'blue'>Machine Learning Experience<\/font>**.\n\nWe will also group the data based on the type of experience: **Very Junior** (less than 1 Year of Experience), **Junior** (with 1 to 2 Years of Experience), **Junior+** (with 2 to 3 Years of Experience), **Medium** (with 3 to 5 Years of Experience) and **Senior** (with more than 5 Years of Experience).\n\nIn the filtered dataset we can see that **71%** of the respondents have a **Junior** Experience with Machine Learning.","db107a29":"We can see from the **exceptional performance** of the model that the different profiles that we have identified have specific characteristics which **<font color = 'blue'>Makes them Unique<\/font>**.","78666f0d":"### 5.5.6 **Programming Language** and **IDE**\n\nFindings:\n\n   - more than **<font color = 'blue'>80%<\/font>** of the respondents are using **<font color = 'blue'>Jupyter Environments<\/font>** and **<font color = 'blue'>Python<\/font>**\n   - **<font color = 'blue'>SQL<\/font>** is used by **Data Scientists**, **Data Analysts** and **Engineers** \n   - **<font color = 'blue'>35%<\/font>** of **Data Scientists** and **Data Analysts** are using **<font color = 'blue'>R<\/font>**. The percentage increases to over **<font color = 'blue'>50%<\/font>** for **Data Scientists** and **Data Analysts** which have a **Medium \/ Senior Level for Machine Learning** \n   - **Data Scientists** and **Data Analysts** which have a **Medium \/ Senior Level for Machine Learning** are more likely to use **<font color = 'blue'>Other Programming Languages<\/font>**\n   \n   <br>\n   \n   - **<font color = 'blue'>67%<\/font>** of **Data Analysts** are using **SQL**\n   <br>\n   \n   - **<font color = 'blue'>67%<\/font>** of **Data Scientists** are using **SQL**\n   <br>\n   \n   - **<font color = 'blue'>76%<\/font>** of **Engineers** are using **Other Programing Languages** (C, C++, Java, JavaScript, etc)\n   - **<font color = 'blue'>64%<\/font>** of **Engineers** are using **Visual Studio**\n   - **<font color = 'blue'>38%<\/font>** of **Engineers** are using **PyCharm**\n   - **<font color = 'blue'>53%<\/font>** of **Engineers** are using **Visual Studio Code**","430942d3":"## 5.5. Analyze Identified profiles","cd8695e0":"  * 2 - Professionals that have **<font color = '#ba5618'>Junior Experience with Machine Learning<\/font>** (**6 Profiles**)\n      - 2.1. Professionals that are **<font color = '#ba5618'>Older then 35 years<\/font>** (**2 Profiles**)\n        - One profile for Professionals:\n          - **<font color = '#ba5618'>Not Working for Large Companies<\/font>** \n          - 21% of them are working for companies without a Data Science Team (**1.8 times** more than in the average profile)\n          - 13% of them have the role title IT Other (**1.8 times** more than in the average profile)\n          - 18% of them have the role title Other (**1.8 times** more than in the average profile)\n          - Usually not having the role title Data Scientist (50% less than in the average profile)\n        - One profile for Professionals:\n          - **<font color = '#ba5618'>Working for Large Companies<\/font>** (**2.4 times** more than in the average profile)\n          - 20% of them have the role title IT Other (**2.6 times** more than in the average profile)\n          - 16% of them have the role title Other (**1.6 times** more than in the average profile)\n          - Usually not having the role title Research (45% less than in the average profile)\n          - Usually not having the role title Data Scientist (50% less than in the average profile)\n          - Usually not working is Small Data Science Teams (40% less than in the average profile)\n      - 2.2. Professionals that are **<font color = '#ba5618'>Younger then 35 years<\/font>** (**4 Profiles**)\n        - One profile for Professionals:\n          - **<font color = '#ba5618'>Junior Level in Development<\/font>** (**3.6 times** more than in the average profile)\n          - **<font color = '#ba5618'>Bachelor Degree<\/font>** (**3 times** more than in the average profile)\n          - 23% of them have the role title Analyst (**1.3 times** more than in the average profile)\n          - 14% of them are living in Africa (**1.9 times** more than in the average profile)\n          - Usually not living in Europe (60% less than in the average profile)\n        - One profile for Professionals:\n          - **<font color = '#ba5618'>Junior Level in Development<\/font>** (**3.6 times** more than in the average profile)\n          - **<font color = '#ba5618'>Master Degree<\/font>** (**1.9 times** more than in the average profile)\n          - 24% of them have the role title Analyst (**1.4 times** more than in the average profile)\n          - 19% of them are Female (**1.4 times** more than in the average profile)\n        - One profile for Professionals:\n          - **<font color = '#ba5618'>Medium Level in Development<\/font>** (**2.7 times** more than in the average profile)\n          - 23% of them have the role title Software Engineer (**1.8 times** more than in the average profile)\n          - 18% of them have the role title ML Engineer (**1.2 times** more than in the average profile)\n        - One profile for Professionals:\n          - **<font color = '#ba5618'>Very Junior Level in Development<\/font>** (**7.2 times** more than in the average profile)\n          - Very Junior Level with Machine Learning (**3 times** more than in the average profile)\n          - 24% of them are working for companies without a Data Science Team (**2 times** more than in the average profile)\n          - 21% of them are Female (**1.5 times** more than in the average profile)\n          - 14% of them are living in Africa (**1.8 times** more than in the average profile)","57269969":"### 5.5.10 **Salary**\nFindings:\n\n   - regardless of the Role Title, repondents residing in **<font color = 'blue'>USA<\/font>** and **<font color = 'blue'>Europe<\/font>** have the **highest** salaries\n   - salaries for respondents from **<font color = 'blue'>Africa<\/font>** are the **lowest**\n   - regardless of the Role Title, **Medium \/ Senior Level for Machine Learning** have the **highest** salaries\n   \n   <br>\n   \n   - respondants having a role title **Data Analyst** and **Medium \/ Senior Level for Machine Learning**:\n       - in **USA** will likely have a salary between **<font color = 'blue'>50k and 200k<\/font>**\n       - in **Europe** will likely have a salary between **<font color = 'blue'>20k and 100k<\/font>**\n       - in **India** will likely have a salary between **<font color = 'blue'>1k and 10k<\/font>**       \n       - in **India** **<font color = 'blue'>50%<\/font>** of the respondents from **Profile 5**  will have a salary between **<font color = 'blue'>20k and 50k<\/font>**       \n       - in **Asia** will likely have a salary between **<font color = 'blue'>50k and 100k<\/font>**\n       - in **Africa** will likely have a salary between **<font color = 'blue'>5k and 20k<\/font>**\n   \n   <br>\n   \n   - respondants having a role title **Data Scientist** and **Medium \/ Senior Level for Machine Learning**:\n       - in **USA** will likely have a salary between **<font color = 'blue'>100k and 200+ k<\/font>**\n       - in **Europe** will likely have a salary between **<font color = 'blue'>50k and 200k<\/font>**\n       - in **India** will likely have a salary between **<font color = 'blue'>20k and 100k<\/font>**\n       - in **Asia** will likely have a salary between **<font color = 'blue'>50k and 200k<\/font>**\n       - in **Africa** will likely have a salary between **<font color = 'blue'>50k and 100k<\/font>**\n   \n   <br>\n   \n   - respondants having a role title **Engineer** and **Medium \/ Senior Level for Machine Learning**:   \n       - in **USA** will likely have a salary between **<font color = 'blue'>100k and 200+ k<\/font>**\n       - in **Europe** will likely have a salary between **<font color = 'blue'>50k and 100k<\/font>**\n       - in **India** will likely have a salary between **<font color = 'blue'>50k and 200k<\/font>**\n       - in **Asia** will likely have a salary between **<font color = 'blue'>50k and 200k<\/font>**\n       - in **Africa** will likely have a salary between **<font color = 'blue'>1k and 5k<\/font>**\n       - in **Africa** **<font color = 'blue'>50%<\/font>** of the respondents having **Senior Level for Machine Learning**  will have a salary between **<font color = 'blue'>50k and 100k<\/font>**\n       \n   <br>\n   \n   - respondants having a role title **Other** and **Medium \/ Senior Level for Machine Learning**:      \n      - in **USA** will likely have a salary between **<font color = 'blue'>100k and 200+ k<\/font>**\n      - in **Europe** will likely have a salary between **<font color = 'blue'>20 and 200k<\/font>**\n      - in **India** will likely have a salary between **<font color = 'blue'>20k and 50k<\/font>**\n      - in **Asia** will likely have a salary between **<font color = 'blue'>20k and 200k<\/font>**\n      - in **Africa** will likely have a salary between **<font color = 'blue'>10k and 50k<\/font>**\n   ","d3d52b9f":"### 5.5.9 **ML Algorithm** and **ML Framework**\nFindings:\n\n   - **Data Analysts** are the the only role through the respondents that have less then **<font color = 'blue'>30%<\/font>** of respondents using **<font color = 'blue'>Deep Learning<\/font>**\n   - more then **<font color = 'blue'>70%<\/font>** of respondents are implementing **Linear and Logistic Regression** models and **Sklearn**\n   - **<font color = 'blue'>76%<\/font>** of **Data Scientists** are implementing **Decision Trees or Random Forest** models\n   - **<font color = 'blue'>Tensorflow<\/font>** is the most used **ML Framework** for **Deep Learning**\n   - **Medium \/ Senior Level for Machine Learning** are more likely to use more complex **ML Algorithm** and **ML Framework**","6910247f":"### 5.5.7 **NLP Algorithm** and **Computer Vision**\n\nFindings:\n\n   - more than **<font color = 'blue'>83%<\/font>** of **Data Analysts** are not using **NLP Algorithm** and **Computer Vision**\n   - **<font color = 'blue'>32%<\/font>** of **Data Analysts** are using **Basic Statistical Software**\n   <br>\n   \n   - **<font color = 'blue'>30%<\/font>** of **Data Scientists** and **Engineers** are using **NLP Algorithms**\n   - **<font color = 'blue'>34%<\/font>** of **Data Scientists** and **Engineers** are using **Computer Vision**\n   - **<font color = 'blue'>45%<\/font>** of **Engineers** are using **Computer Vision**\n   <br>\n   - **Data Scientists** which have a **Very Junior Level for Machine Learning** or the **Junior Level for Machine Learning** respondents that work in **Large Companies** are less likely to use **NLP Algorithm** and **Computer Vision**. For the other profiles we see quite a similar usage for **<font color = 'blue'>Deep Learning<\/font>**\n   <br>\n   \n   - **Engineers** which have a **Medium \/ Senior Level for Machine Learning** are more likely to use **<font color = 'blue'>Deep Learning<\/font>**","4ff1759f":"# 3. Filter the Dataset\n\nThe goal of this analysis is to find the different profiles of respondents that have replied to the **2021 Kaggle Survey** and:\n   * are **<font color = 'blue'>Professionals<\/font>** - based on the feature **Role Title**\n   * have an **<font color = 'blue'>University Degree<\/font>**\n   * **<font color = 'blue'>Know if the Company has or not Machine Learning Implemented<\/font>**\n   * have **<font color = 'blue'>Experience with Machine Learning<\/font>**\n   * have a **<font color = 'blue'>Declared Gender<\/font>**\n   * have **<font color = 'blue'>less than 60 years old<\/font>**\n   * have **<font color = 'blue'>Declared Their Salary<\/font>**","0d0c3ac6":"## 4.2 Feature Engineering for feature **<font color = 'blue'>Company Size<\/font>**\n\nWe check the different **Company Size** that can be found in the dataset. We can see that we can use the following distribution:\n   - **29.5%** of respondents work in a **Small Company** with 0 to 49 employees\n   - **30.5%** of respondents work in a **Medium Company** with 50 to 249 employees\n   - **40%** of respondents work in a **Large Company** with 250+ employees","ebb77f88":"#  2. Import, Clean and Transform Data\n\nIn order to be able to analyse the data we will start by cleaning and transforming the features in the dataset.\n\nWe will be executing the following steps:\n   >       1. import needed packages\n   >       2. create the utility functions and classes that will be used during the analysis\n   >       3. import the raw data\n   >       4. merge features for multiple selection questions","2fd9f780":"\n## 3.3 Filter respondents based on the **<font color = 'blue'>Company Machine Learning Implementation<\/font>**\n\nWe check the different **Company Machine Learning Implementations** that can be found in the dataset. We can see that we have the following distribution:\n   - **29.6%** of respondents **didn't reply** or **don't know** the type of Machine Learning implemented in their company\n   - **16.6%** of respondents are working for a company where **No Machine Learning is implemented** in their company\n   - **53.6%** of respondents are working for a company where **Machine Learning is implemented** in their company\n\nFor the rest or the analysis we will only select the respondents for which the **Company Machine Learning Implementation** **<font color = 'blue'>is Known<\/font>**.","abc1f8e9":"### 5.5.3 **BigData Product** and **BI Tools**\n\nFindings:\n\n   - **<font color = 'blue'>52%<\/font>** of **Data Analysts** are using **BI Tools**\n   - **Data Analysts** which have a **Senior Level for Machine Learning** are less likely to use **BI Tools** (decrease to **35%**)\n   - **56%** of **Data Analysts** are using **BigData Products**\n   - **7%** of **Data Analysts** are using **BigData Cloud Related Solutions**\n   <br>\n   \n   - **<font color = 'blue'>63%<\/font>** of **Data Scientists** are using **BigData Products**\n   - **<font color = 'blue'>33%<\/font>** of **Data Scientists** are using **BigData Cloud Related Solutions**\n   - **Data Scientists** which have a **Medium \/ Senior Level for Machine Learning** are more likely to use **BigData Cloud Related solutions**\n   <br>\n   \n   - **56%** of **Engineers** are using **BigData Products**\n   - **7%** of **Engineers** are using **BigData Cloud Related Solutions**\n   - **<font color = 'blue'>34%<\/font>** of **Engineers** are using **BI Tools**\n   <br>\n   \n   - **40%** of other role titles will use **BI Tools**\n   - **<font color = 'blue'>55%<\/font>** of other role titles which have a **Medium \/ Senior Level for Machine Learning** are more likely to use **BigData Products**, but only **5%** will use **BigData Cloud Related Solutions**","d6fb6515":"\n## 3.6 Filter respondents based on the **<font color = 'blue'>Age<\/font>**\n\nWe check the different **Ages** that can be found in the dataset. We can see that we have the following distribution:\n   - **39.6%** of respondents have **Less Than 30 Years** \n   - **31%** of respondents have **Between 30 and 40 Years** \n   - **17.6%** of respondents have **Between 40 and 50 Years** \n   - **8%** of respondents have **Between 50 and 60 Years** \n   - **3.5%** of respondents have **Mre than 60 Years**\n\nFor the rest or the analysis we will only select the respondents that have **<font color = 'blue'>Less than 60 years old<\/font>**.\n\nIn the filtered dataset we have **60%** of the Professional respondants that are **Younger than 35 years old**.","44bc0afe":"### 5.5.1 Main Profiles Characteristics\n\nFor the respondants of the Kaggle Survey that:\n   - have an **<font color = 'blue'>University Degree<\/font>**\n   - **<font color = 'blue'>know if the Company has or not Machine Learning Implemented<\/font>**\n   - have **<font color = 'blue'>Experience with Machine Learning<\/font>**\n   - have a **<font color = 'blue'>Declared Gender<\/font>**\n   - have **<font color = 'blue'>less than 60 years old<\/font>**\n   - have **<font color = 'blue'>Declared Their Salary<\/font>**\n\nwe have identified **<font color = 'blue'>9 Profiles<\/font>** with the following characteristics:\n\n   * 1 - Professionals that have **<font color = '#ba5618'>Higher Experience with Machine Learning<\/font>** (**3 Profiles**)\n        - One profile for Professionals:\n          - **<font color = '#ba5618'>Senior Level with Machine Learning<\/font>** (**7.5 times** more than in the average profile)\n          - **<font color = '#ba5618'>Older than 35 years<\/font>** (**2.2 times** more than in the average profile)\n          - Senior Level in Development (**4 times** more than in the average profile)\n          - Usually not working for Small Companies (50% less than in the average profile)\n          - Usually not living in Asia (30% less than in the average profile)\n          - Usually not living in India (70% less than in the average profile)\n          - Usually not doing Machine Learning Exploring (50% less than in the average profile)\n        - One profile for Professionals:\n          - **<font color = '#ba5618'>Medium Level with Machine Learning<\/font>** (**5 times** more than in the average profile)\n          - **<font color = '#ba5618'>Younger than 35 years<\/font>** (**1.7 times** more than in the average profile)\n          - Medium Level in Development (**2.3 times** more than in the average profile)\n          - 43% of them are living in Europe or USA (1.3 times more than in the average profile)\n          - 21% of them have a Doctoral Degree or higher (1.2 times more than in the average profile)\n          - 22% of them have the role title ML Engineer (1.5 times more than in the average profile)\n        - One profile for Professionals:\n          - **<font color = '#ba5618'>Medium Level with Machine Learning<\/font>** (**5.2 times** more than in the average profile)\n          - **<font color = '#ba5618'>Older than 35 years<\/font>** (**2.5 times** more than in the average profile)\n          - Medium Level in Development (**2.3 times** more than in the average profile)\n          - Usually not working for Companies that don't have Machine Learning Implemented (40% less than in the average profile)\n          - 50% of them are living in Europe or USA (1.4 times more than in the average profile)\n          - Usually not living in India (40% less than in the average profile)","e808644d":"## 3.1 Filter Dataset based on the **<font color = 'blue'>Role Title<\/font>**\n\nWe will check the different **Role Titles** that can be found in the dataset.\n\nWe can see that **26%** of the respondents in the dataset are **Students** and **74%** of the respondents could be considered as **Professionals**. \n\nBased on the distribution that we find in the Role Titles, we decide to **transform the data** in new categories in order to group the values for which we have very few respondents:\n\n| Role Title | Percentage Rate | New Feature Value |\n| --- | --- | --- |\n| Data Scientist | 13.92% | Data Scientist |\n| Machine Learning Engineer | 5.77% | Data \/ ML Engineer |\n| Data Engineer | 2.57% | Data \/ ML Engineer |\n| Research Scientist | 5.92% | Research |\n| Statistician | 1.21% | Research |\n| Data Analyst | 8.86% | Analyst |\n| Business Analyst | 3.73% | Analyst |\n| Software Engineer |  9.43% | Software Engineer |\n| Program\/Project Manager | 3.27% | IT__Other |\n| Product Manager | 1.23% | IT__Other |\n| DBA\/Database Engineer | 0.66% | IT__Other |\n| Developer Relations\/Advocacy | 0.38% | IT__Other |\n| Other | 9.21% | Other |\n| Currently not employed | 7.65% | Currently not employed |\n\nFor the rest or the analysis we will only select the **<font color = 'blue'>Professionals<\/font>**.","2ac56c64":"### 5.5.8 **Hosted Notebooks**\nFindings:\n\n   - repondents which are **Older than 35 years** and have **Medium \/ Senior Level for Machine Learning** are less likely to post on **Hosted Notebooks**","5b706728":"### 5.5.4 **Cloud Computing Platform**\n\nFindings:\n\n   - **<font color = 'blue'>50%<\/font>** of **Data Analysts** are using a **Computing Platform**\n   - **<font color = 'blue'>70%<\/font>** of **Data Analysts** are using a **Laptop**\n   - **Data Analysts** which have a **Medium \/ Senior Level for Machine Learning** are more likely to use a **Computing Platform**\n   - **<font color = 'blue'>72%<\/font>** of **Data Analysts** are **not** using a **Cloud Computing Product**\n   - **<font color = 'blue'>69%<\/font>** of **Data Analysts** are **not** using **Data Storage Products**   \n   <br>\n\n   - **<font color = 'blue'>64%<\/font>** of **Data Scientists** are using a **Computing Platform**\n   - **<font color = 'blue'>39%<\/font>** of **Data Scientists** are using **AWS**\n   - **<font color = 'blue'>28%<\/font>** of **Data Scientists** are using **GCP**\n   - **<font color = 'blue'>56%<\/font>** of **Data Scientists** are using a **Laptop**\n   - **Data Scientists** which have a **Medium \/ Senior Level for Machine Learning** are more likely to use **Data Storage Products** and **Cloud Computing** \n   - **Data Scientists** which have a **Very Junior Level for Machine Learning** are less likely to use **Data Storage Products** and **Cloud Computing** (**78%** of them use a **Laptop**, **58% don't** use a **Computing Platform**, **75% don't** use **Cloud Computing Products** and **75%** don't use **Data Storage Products**)\n   - **<font color = 'blue'>48%<\/font>** of **Data Scientists** are using **Data Storage Products**\n   - **<font color = 'blue'>29%<\/font>** of **Data Scientists** are using **AWS - S3**\n   <br>\n\n   - **<font color = 'blue'>56%<\/font>** of **Engineers** are using a **Computing Platform**\n   - **<font color = 'blue'>33%<\/font>** of **Engineers** are using **AWS**\n   - **<font color = 'blue'>27%<\/font>** of **Engineers** are using **GCP**\n   - **<font color = 'blue'>55%<\/font>** of **Engineers** are using a **Laptop**\n   - **Engineers** which have a **Medium \/ Senior Level for Machine Learning** are more likely to use **Data Storage Products** and **Cloud Computing** \n   - **Engineers** which have a **Very Junior Level for Machine Learning** are less likely to use **Data Storage Products** and **Cloud Computing** (**75%** of them use a **Laptop**, **51% don't** use a **Computing Platform**, **66% don't** use **Cloud Computing Products** and **68%** don't use **Data Storage Products**)\n   - **<font color = 'blue'>42%<\/font>** of **Engineers** are using **Data Storage Products**\n   <br>\n   \n   - **61%** of the other roles use a **Laptop**, **55% don't** use a **Computing Platform**, **73% don't** use **Cloud Computing Products** and **71%** don't use **Data Storage Products**\n   - **Medium \/ Senior Level for Machine Learning** profiles are more likely to use **Data Storage Products** and **Cloud Computing**","bfe28c93":"# 4. Feature Engineering\n\nWe will apply feature engineering techniques for the following features:\n   - [x] **Company Data Science Team Size**\n   - [x] **Company Size**\n   - [x] **Development Experience**\n   - [x] **Residence Country**","95098c5a":"\n## 3.2 Filter respondents based on the **<font color = 'blue'>Highest Degrees<\/font>**\n\nWe check the different values that have been provided for the feature **Highest Degrees**. We can see that we have the following distribution:\n   - **9%** of respondents **didn't reply** or have **No University Degree** \n   - **91%** of respondents have an **University Degree**\n   \nAs we only have **1.77%** of the respondents that have a **Professional Doctorate** we will consider that they have a **Doctoral Degree +**.\n\nFor the rest or the analysis we will only select the respondents that have an **<font color = 'blue'>University Degree<\/font>**.","e845ea47":"We can see that **5%** of the dataset has been marked as **Special Profiles** and that **95%** of the dataset has been marked as **Common Profiles**.","67cf8929":"## 5.4. Most important features\n\nWe will check the most important features for the identified profiles by using SHAP Values. \n\nIn 2017 two computer scientists from the University of Washington published a technique for generating fast and practical explanations of a particular kind of ML called tree-based models (specifically, a variant called XGBoost). The algorithm\u2019s authors named their work SHAP, for Shapley additive explanations.\n\nWe can see that the most important features for the different profiles are:\n   - **Age Category for Less Than 35 Years Old**\n   - **Machine Learning Experience Level Junior**\n   - **Development Experience Level Medium**\n   - **Development Experience Level Junior**\n   - **Highest Degree Master**","1a70cec4":"# 6. Conclusions\n\nFrom the data science community represented in the Kaggle Survey we have choosen a subset of **Professionals**, for which we have created 9 profiles.\n\nThe main characteristic for this profiles is the **ML Level** and we have seen that most of the technological, programming, algorithm and methods are becoming more complex as the **<font color = 'blue'>Seniority Level Increases<\/font>** and the salary is following the same trend.\n\nA very interesting profile is **Profile 5**, which resembles very well to **Profile 4** when looking at the main characteristics. The differentiator is the size of the company; the respondants in **Profile 5** are working for **Large Size Companies**. We can see that this profile is **less likely** to be exposed to **more complex algorithms and technologies** (expecially for Role Titles **Data Scientist** and **Engineer**), but they usually have **higher** salaries.\n\n\n#### **I hope that you have enjoyed this analysis and don't hesitate to provide your feedback in the comments section !**","9e13916d":"# 5. Common Profiles\n\nWe will apply an anomaly detection algorithm in order to identify:\n   - [x] **Common Profiles**\n   - [ ] **Special Profiles**\n   \nAnomaly detection, also called outlier detection, is the identification of unexpected observations or items that differ significantly from the norm. Anomaly detection rests upon two basic assumptions:\n   * Anomalies in data occur only very rarely\n   * The features of data anomalies are significantly different from those of normal instances\n   \nBased on the encoded data returned by the outlier detection algorithm, we will filter the common profiles and do a segmentation through clustering.\n\nWe will also apply a multi-classification model in order to check that the profiles all well splitted.\n\nIn the end we will analyse all these profiles in order to understand their common and\/or specific characteristics.","d4187454":"## 5.2 Identify the **Comon Profiles**\n\nWe create the AutoEncoder model which will be trained in order to identify the **<font color = 'blue'>Common Profiles<\/font>**.\n\nThis **Unsupervised Method** of anomaly detection detects anomalies in the unlabeled dataset based solely on the intrinsic properties of that data. The working assumption is that, as in most cases, the large majority of the instances in the data set will be normal. The anomaly detection algorithm will then detect instances that appear to fit with the rest of the dataset least congruently.","22332fc6":"### 5.5.3 **Role Title** and **Residence Country**\n\nFindings:\n   - Role Titles **Data Scientist** and **Engineer (Software \/ Data \/ ML)** are the most common ones with an average value of **28%**\n   - Role Title **Data Analyst** is rather composed by professionals that have a **Junior Level for Machine Learning**\n   - **Profile 2** and **Profile 8** have around **40%** of the respondants based in India (70% above average value)\n   - **Profile 4** and **Profile 5** have only **25%** of their respondents having a role title **Data Scientist** or **Data Engineer**\n   - **Medium \/ Senior Level for Machine Learning** profiles have around **40%** of the respondants having the Role Title **Data Scientist** compared to **Profile 4** and **Profile 5** where it drops to **14%** (50% below average value)\n   - **Medium \/ Senior Level for Machine Learning** profiles have around **50%** of the respondants based in Europe (50% above average value) and USA (more than 70% above average value)   ","71219887":"## 4.1 Feature Engineering for feature **<font color = 'blue'>Company Data Science Team Size<\/font>**\n\nWe check the different **Company Data Science Team Size** that can be found in the dataset. We can see that we can use the following distribution:\n   - **12.2%** of respondents work in a company that has **No Data Science Team** \n   - **23.5%** of respondents work in a company that has a **Very Small Data Science Team** \n   - **17.6%** of respondents work in a company that has a **Small Data Science Team** \n   - **20.3%** of respondents work in a company that has a **Medium Data Science Team** \n   - **26.2%** of respondents work in a company that has a **Large Data Science Team** ","4de9a142":"## 5.3. Create the profiles through clustering\n\nPeople seek approximation based on similarity, especially when it comes to business statistics. With data, the process is much more precise, what we call clustering. The term clustering is used to refer to the act of grouping information.\n\nSeparating into groups, categorizing, and segmenting is a way of gathering information or data based on common characteristics.\n\nWe decide to create these profiles in order to better understand the common attributes of the Professionals, but also the differences between them."}}