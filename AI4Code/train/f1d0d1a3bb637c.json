{"cell_type":{"1eab02f8":"code","32f7d20b":"code","6bc8bb07":"code","ae6592a5":"code","1c48181f":"code","36260df9":"code","89072162":"code","12314d4b":"code","071cc76e":"code","8a67eaa6":"code","800d5ee4":"code","11e2c0cf":"code","47a92ce2":"code","09c94fcf":"code","cbb77ef6":"code","d2a12d89":"code","dc98a591":"code","7f80827a":"code","13762af3":"code","ba18ec6f":"code","f2416d07":"code","8e05a313":"code","b29519b6":"code","3c2d056f":"code","2b931d62":"code","4bac94d6":"code","baaa8fbf":"code","1c48545d":"code","f6f3e00e":"code","e14e0bc6":"code","19bad42f":"code","7cf075ea":"code","feb4044a":"code","5da4d944":"code","3df54c23":"code","d2d6c702":"code","72d71163":"code","14269da8":"code","016b409f":"code","a51949f4":"code","da5a2539":"code","bdeec2f1":"code","ba97cd21":"code","844f6eac":"code","99de569d":"code","e409281a":"code","183122d9":"code","9143e029":"code","2826302a":"code","fc82728c":"code","b75a9812":"code","20d14604":"code","615b7790":"code","b0ae7c60":"code","2ebf90d9":"code","b713278d":"code","e04c0483":"code","71e11c7b":"code","bf2c015e":"code","b5ada0ca":"code","9ad4c49b":"code","8c9fb035":"code","66bf4346":"code","88f05617":"code","c78d0888":"code","843c92cc":"code","c3333a37":"code","fd090a87":"code","16ec3cc2":"code","b7f95714":"code","2ef54897":"code","0525b7bf":"code","8d9b6632":"code","85d2c925":"code","ab37e91e":"code","11007d60":"code","b6a76b78":"code","373e942a":"code","572ab9e7":"code","722a4a0c":"code","cf44c158":"code","525ea70e":"code","e5025378":"code","ca81dba4":"code","dfe0ec3b":"code","24b370e9":"code","4904c331":"code","ccba45b5":"code","870d9e48":"code","9517235e":"markdown","87c14c6f":"markdown","8b077120":"markdown","d82f6735":"markdown","b9b8b393":"markdown","0fbe3aae":"markdown","1b7d7743":"markdown"},"source":{"1eab02f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32f7d20b":"import os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"end_to_end_project\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","6bc8bb07":"import os\nimport tarfile\nfrom six.moves import urllib\n\nDOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml\/master\/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets\/housing\/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    os.makedirs(housing_path, exist_ok=True)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\nfetch_housing_data()","ae6592a5":"import pandas as pd\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","1c48181f":"housing = load_housing_data()\nprint(housing.head())\nhousing.info()","36260df9":"print(housing[\"ocean_proximity\"].value_counts())\nhousing.describe()","89072162":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nsave_fig(\"attribute_histogram_plots\")\nplt.show()","12314d4b":"import numpy as np\n\n# For illustration only. Sklearn has train_test_split()\ndef split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\ntrain_set, test_set = split_train_test(housing, 0.2)\nprint(len(train_set), \"train +\", len(test_set), \"test\")","071cc76e":"from zlib import crc32\n\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]","8a67eaa6":"import hashlib\n\ndef test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio","800d5ee4":"def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio","11e2c0cf":"housing_with_id = housing.reset_index()   # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")","47a92ce2":"housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")","09c94fcf":"test_set.head()","cbb77ef6":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\nprint(test_set.head())\nprint(housing[\"median_income\"].hist())","d2a12d89":"housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\nprint(housing[\"income_cat\"].value_counts())\nhousing[\"income_cat\"].hist()","dc98a591":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\nprint(strat_test_set[\"income_cat\"].value_counts() \/ len(strat_test_set))\nprint(housing[\"income_cat\"].value_counts() \/ len(housing))","7f80827a":"def income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() \/ len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] \/ compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] \/ compare_props[\"Overall\"] - 100","13762af3":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","ba18ec6f":"housing = strat_train_set.copy()\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\nsave_fig(\"bad_visualization_plot\")\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\nsave_fig(\"better_visualization_plot\")","f2416d07":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n    sharex=False)\nplt.legend()\nsave_fig(\"housing_prices_scatterplot\")","8e05a313":"import matplotlib.image as mpimg\ncalifornia_img=mpimg.imread(PROJECT_ROOT_DIR + '\/images\/end_to_end_project\/california.png')\nax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                       s=housing['population']\/100, label=\"Population\",\n                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n                       colorbar=False, alpha=0.4,\n                      )\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n           cmap=plt.get_cmap(\"jet\"))\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\n\nprices = housing[\"median_house_value\"]\ntick_values = np.linspace(prices.min(), prices.max(), 11)\ncbar = plt.colorbar()\ncbar.ax.set_yticklabels([\"$%dk\"%(round(v\/1000)) for v in tick_values], fontsize=14)\ncbar.set_label('Median House Value', fontsize=16)\n\nplt.legend(fontsize=16)\nsave_fig(\"california_housing_prices_plot\")\nplt.show()","b29519b6":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","3c2d056f":"# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\nsave_fig(\"scatter_matrix_plot\")","2b931d62":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1)\nplt.axis([0, 16, 0, 550000])\nsave_fig(\"income_vs_house_value_scatterplot\")","4bac94d6":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]","baaa8fbf":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","1c48545d":"housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n             alpha=0.2)\nplt.axis([0, 5, 0, 520000])\nplt.show()","f6f3e00e":"housing.describe()","e14e0bc6":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\nsample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\nprint(sample_incomplete_rows)\n# sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1\nsample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2\nmedian = housing[\"total_bedrooms\"].median()\nsample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3\nprint(sample_incomplete_rows)","19bad42f":"try:\n    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\nexcept ImportError:\n    from sklearn.preprocessing import Imputer as SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")","7cf075ea":"housing_num = housing.drop('ocean_proximity', axis=1)\n# alternatively: housing_num = housing.select_dtypes(include=[np.number])\nimputer.fit(housing_num)\nprint(imputer.statistics_)\nhousing_num.median().values","feb4044a":"X = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing.index)\nhousing_tr.loc[sample_incomplete_rows.index.values]","5da4d944":"\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing_num.index)\nhousing_tr.head()","3df54c23":"housing_cat = housing[['ocean_proximity']]\nhousing_cat.head(10)","d2d6c702":"try:\n    from sklearn.preprocessing import OrdinalEncoder\nexcept ImportError:\n    from future_encoders import OrdinalEncoder # Scikit-Learn < 0.20","72d71163":"ordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nprint(housing_cat_encoded[:10])\nprint(ordinal_encoder.categories_)","14269da8":"try:\n    from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20\n    from sklearn.preprocessing import OneHotEncoder\nexcept ImportError:\n    from future_encoders import OneHotEncoder # Scikit-Learn < 0.20\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","016b409f":"print(housing_cat_1hot.toarray())\ncat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nprint(housing_cat_1hot)\nprint(cat_encoder.categories_)\nprint(housing.columns)","a51949f4":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# get the right column indices: safer than hard-coding indices 3, 4, 5, 6\nrooms_ix, bedrooms_ix, population_ix, household_ix = [\n    list(housing.columns).index(col)\n    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix]\n        population_per_household = X[:, population_ix] \/ X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","da5a2539":"from sklearn.preprocessing import FunctionTransformer\n\ndef add_extra_features(X, add_bedrooms_per_room=True):\n    rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix]\n    population_per_household = X[:, population_ix] \/ X[:, household_ix]\n    if add_bedrooms_per_room:\n        bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n        return np.c_[X, rooms_per_household, population_per_household,\n                     bedrooms_per_room]\n    else:\n        return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = FunctionTransformer(add_extra_features, validate=False,\n                                 kw_args={\"add_bedrooms_per_room\": False})\nhousing_extra_attribs = attr_adder.fit_transform(housing.values)","bdeec2f1":"housing_extra_attribs = pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=housing.index)\nhousing_extra_attribs.head()","ba97cd21":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","844f6eac":"housing_num_tr","99de569d":"try:\n    from sklearn.compose import ColumnTransformer\nexcept ImportError:\n    from future_encoders import ColumnTransformer # Scikit-Learn < 0.20","e409281a":"num_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","183122d9":"print(housing_prepared)\nprint(housing_prepared.shape)","9143e029":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# Create a class to select numerical or categorical columns \nclass OldDataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","2826302a":"num_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nold_num_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(num_attribs)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n        ('std_scaler', StandardScaler()),\n    ])\n\nold_cat_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(cat_attribs)),\n        ('cat_encoder', OneHotEncoder(sparse=False)),\n    ])","fc82728c":"from sklearn.pipeline import FeatureUnion\n\nold_full_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", old_num_pipeline),\n        (\"cat_pipeline\", old_cat_pipeline),\n    ])","b75a9812":"old_housing_prepared = old_full_pipeline.fit_transform(housing)\nprint(old_housing_prepared)","20d14604":"np.allclose(housing_prepared, old_housing_prepared)","615b7790":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","b0ae7c60":"# let's try the full preprocessing pipeline on a few training instances\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","2ebf90d9":"print(\"Labels:\", list(some_labels))","b713278d":"from sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","e04c0483":"from sklearn.metrics import mean_absolute_error\n\nlin_mae = mean_absolute_error(housing_labels, housing_predictions)\nlin_mae","71e11c7b":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)","bf2c015e":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","b5ada0ca":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","9ad4c49b":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","8c9fb035":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","66bf4346":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","88f05617":"housing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","c78d0888":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","843c92cc":"scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()","c3333a37":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = svm_reg.predict(housing_prepared)\nsvm_mse = mean_squared_error(housing_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","fd090a87":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","16ec3cc2":"grid_search.best_params_","b7f95714":"grid_search.best_estimator_","2ef54897":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","0525b7bf":"pd.DataFrame(grid_search.cv_results_)","8d9b6632":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","85d2c925":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","ab37e91e":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","11007d60":"extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","b6a76b78":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","373e942a":"from scipy import stats","572ab9e7":"confidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nmean = squared_errors.mean()\nm = len(squared_errors)\n\nnp.sqrt(stats.t.interval(confidence, m - 1,\n                         loc=np.mean(squared_errors),\n                         scale=stats.sem(squared_errors)))","722a4a0c":"tscore = stats.t.ppf((1 + confidence) \/ 2, df=m - 1)\ntmargin = tscore * squared_errors.std(ddof=1) \/ np.sqrt(m)\nnp.sqrt(mean - tmargin), np.sqrt(mean + tmargin)","cf44c158":"zscore = stats.norm.ppf((1 + confidence) \/ 2)\nzmargin = zscore * squared_errors.std(ddof=1) \/ np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)","525ea70e":"full_pipeline_with_predictor = Pipeline([\n        (\"preparation\", full_pipeline),\n        (\"linear\", LinearRegression())\n    ])\n\nfull_pipeline_with_predictor.fit(housing, housing_labels)\nfull_pipeline_with_predictor.predict(some_data)","e5025378":"my_model = full_pipeline_with_predictor\nfrom sklearn.externals import joblib\njoblib.dump(my_model, \"my_model.pkl\") # DIFF\n#...\nmy_model_loaded = joblib.load(\"my_model.pkl\") # DIFF","ca81dba4":"from scipy.stats import geom, expon\ngeom_distrib=geom(0.5).rvs(10000, random_state=42)\nexpon_distrib=expon(scale=1).rvs(10000, random_state=42)\nplt.hist(geom_distrib, bins=50)\nplt.show()\nplt.hist(expon_distrib, bins=50)\nplt.show()","dfe0ec3b":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n    ]\n\nsvm_reg = SVR()\ngrid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=4)\ngrid_search.fit(housing_prepared, housing_labels)","24b370e9":"negative_mse = grid_search.best_score_\nrmse = np.sqrt(-negative_mse)\nprint(rmse)\ngrid_search.best_params_","4904c331":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon, reciprocal\n\n# see https:\/\/docs.scipy.org\/doc\/scipy\/reference\/stats.html\n# for `expon()` and `reciprocal()` documentation and more probability distribution functions.\n\n# Note: gamma is ignored when kernel is \"linear\"\nparam_distribs = {\n        'kernel': ['linear', 'rbf'],\n        'C': reciprocal(20, 200000),\n        'gamma': expon(scale=1.0),\n    }\n\nsvm_reg = SVR()\nrnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n                                verbose=2, n_jobs=4, random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)\nnegative_mse = rnd_search.best_score_\nrmse = np.sqrt(-negative_mse)\nprint(rmse)\nprint(rnd_search.best_params_)","ccba45b5":"expon_distrib = expon(scale=1.)\nsamples = expon_distrib.rvs(10000, random_state=42)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.title(\"Exponential distribution (scale=1.0)\")\nplt.hist(samples, bins=50)\nplt.subplot(122)\nplt.title(\"Log of this distribution\")\nplt.hist(np.log(samples), bins=50)\nplt.show()","870d9e48":"from sklearn.base import BaseEstimator, TransformerMixin\n\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]","9517235e":"This Code is from Hands-on Machine learning with scikit-learn book.","87c14c6f":"## A full pipeline with both preparation and prediction\n","8b077120":"# Fine-tune your model","d82f6735":"# Discover and visualize the data to gain insights","b9b8b393":"# Select and train a model ","0fbe3aae":"## Model persistence using joblib","1b7d7743":"# Prepare the data for Machine Learning algorithms\n"}}