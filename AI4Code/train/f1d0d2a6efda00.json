{"cell_type":{"cec64f1a":"code","6cd43f2f":"code","ea51afca":"code","2079da4f":"code","4bd3d4b7":"code","ccacdf22":"code","84f58e63":"code","1b50fd8d":"code","e0ee8835":"code","055eba8e":"code","3bf3c496":"code","63fbc5c5":"code","f0c73636":"code","11ce39dd":"code","6bc313f3":"code","a77a8183":"code","0e3b0706":"code","635dcd1d":"code","a78ad829":"code","b6c6daca":"code","3d0fd385":"code","266f7a8e":"code","17e89b24":"code","7755797a":"code","81b1369a":"code","6c328e44":"code","64dc6542":"code","225ea55c":"code","915a1d09":"code","31dd5ee1":"code","49b421d7":"code","51d6a746":"code","04583564":"code","1cc6ef04":"markdown","ff1b06fa":"markdown","3d1598be":"markdown","16e72228":"markdown","43847a2c":"markdown","b8f5f623":"markdown","88849e27":"markdown","18726ff3":"markdown","f7d6653d":"markdown","78b13897":"markdown","483459ec":"markdown","0afb6a8a":"markdown","28b08075":"markdown","5ded2d87":"markdown","76621558":"markdown","b0267fa4":"markdown","c8310756":"markdown","529d6dcf":"markdown","7eb18d91":"markdown","31b7f8a4":"markdown","2274dbae":"markdown"},"source":{"cec64f1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nfrom math import ceil # import ceil function from math module\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# model building\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6cd43f2f":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.describe(include='all')","ea51afca":"train.info()","2079da4f":"train.sample(5)","4bd3d4b7":"# get total and percent of missing values for each feature\ntotal_missing = train.isnull().sum().sort_values(ascending=False)\npercent_missing = (train.isnull().sum() * 100\/train.isnull().count()).sort_values(ascending=False)\npd.concat([total_missing, percent_missing], axis=1, keys=['total', 'percent'])","ccacdf22":"print(train.groupby('Sex')['Survived'].mean())\n# train.plot(x='Sex', y='Survived')\n# train.groupby('Sex')['Survived'].mean().plot(x='Sex')\n\nsns.barplot(x='Sex', y='Survived', data=train).set_title('Percent Survived by Gender')","84f58e63":"# percent survived by pclass\nsns.barplot(x='Pclass', y='Survived', data=train)\n\n#print percentage of people by Pclass that survived\nprint(\"Pclass 1:\", round(train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100, 2), \"% survived\")\nprint(\"Pclass 2:\", round(train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100, 2), '% survived')\nprint(\"Pclass 3:\", round(train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100, 2), '% survived')","1b50fd8d":"# passengers survived by Pclass and sex\nsns.barplot(x='Pclass', y='Survived', data=train, hue='Sex')","e0ee8835":"sns.barplot(x='Pclass', y='Survived', data=train, hue='Embarked')","055eba8e":"# import math\ntrain_nonNullAge = train[train['Age'].notnull()]\ntrain_nonNullAge['Age'] = train_nonNullAge['Age'].apply(lambda x: ceil(x))\n\nplt.title('Survival rate across ages ( 0 to 1 )')\ntrain_nonNullAge.groupby('Age')['Survived'].mean().plot(kind='line')","3bf3c496":"# Combine Sibsp, Parch into Family feature and drop the 2 features\ntrain['FamilySize'] = train['SibSp'] + train['Parch']\ntrain.drop(['SibSp', 'Parch'], axis=1, inplace=True)","63fbc5c5":"# drop Cabin, ticket and name of passenger\ntrain.drop(['Cabin','Name', 'Ticket'], axis=1, inplace=True)","f0c73636":"# Remove rows with null embarked values\ntrain.drop(train[train['Embarked'].isnull()].index, inplace=True)","11ce39dd":"# Fill age values in null fields using randint(mean-std, mean+std, count(null values))\ntrain['Age'].fillna(value=np.random.randint(train['Age'].mean() - train['Age'].std(), train['Age'].mean() + train['Age'].std()), inplace=True)\ntrain.isnull().any().any()","6bc313f3":"# show max 11 columns\npd.set_option('display.max_columns', 11)\n\n# convert categoricals sex and embarked to numericals for modelling purpose\n\n# lets map sex male to 0, female to 1\nsex_map = {'female': 0, 'male': 1}\n\n# Emabrked mapping\nembarked_map = {'S': 0, 'C': 1, 'Q': 2}\n\ntrain['Sex'] = train['Sex'].map(sex_map)\ntrain['Embarked'] = train['Embarked'].map(embarked_map)\n\ntrain.head()","a77a8183":"# from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.2, random_state = 0)","0e3b0706":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","635dcd1d":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","a78ad829":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","b6c6daca":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","3d0fd385":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","266f7a8e":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","17e89b24":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","7755797a":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","81b1369a":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","6c328e44":"# Comparing accuracies of each models\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","64dc6542":"# Clean test data and Submit Predictions\ntest.describe(include='all')","225ea55c":"# missing values for test data\ntotal_missing_test = test.isnull().sum().sort_values(ascending=False)\npercent_missing_test = (test.isnull().sum() * 100\/test.isnull().count()).sort_values(ascending=False)\npd.concat([total_missing_test, percent_missing_test], axis=1, keys=['total', 'percent'])","915a1d09":"# save passenger IDs to use in submission file\ntest_ids = test['PassengerId']\n\n# drop cabin, Name, Ticket, combine Sibsp, Parch into FamiliSize and drop those 2 features\ntest['FamilySize'] = test['SibSp'] + test['Parch']\ntest.drop(['PassengerId', 'Cabin', 'SibSp', 'Parch', 'Name', 'Ticket'], axis=1, inplace=True)\n\ntest.head()","31dd5ee1":"# fill with mean values for fare\ntest['Age'].fillna(value=np.random.randint(test['Age'].mean() - test['Age'].std(), test['Age'].mean() + test['Age'].std()), inplace=True)\ntest.isnull().sum()","49b421d7":"# null record of Fare in test\ntest_fare_null = test[test['Fare'].isnull()].index\ntest['Fare'] = test['Fare'].fillna(test['Fare'].mean())\n\n# False implying that no feature of any record is null.\ntest.isnull().any().any()","51d6a746":"# converting test categoricals sex and embarked to numericals just like we did for train\n\n# sex_map = {'female': 0, 'male': 1}\n\n# Emabrked mapping\n# embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n\ntest['Sex'] = test['Sex'].map(sex_map)\ntest['Embarked'] = test['Embarked'].map(embarked_map)\n\n# all data is converted to format the way we did for train data.\n# test data is ready to be fed into random forrest model since it gave maximum accuracy.\ntest.head()","04583564":"# randomforest\n\n#set the output as a dataframe and convert to csv file named submission.csv\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_ids,\n        \"Survived\": randomforest.predict(test)\n    })\n\nsubmission.to_csv('submission.csv', index=False)","1cc6ef04":"* less than 50% passengers were survived.\n* more than 50% passengers travelled in 3rd class ( pclass = 3 )\n* less than 50% passengers have atleast 1 sibling or spouse.\n* less than 25% passengers have atleast 1 parent or child.\n* mean age of passengers is 29\n* more male passengers than female (m - 577)","ff1b06fa":"<h2>2. Read and explore Data<\/h2>","3d1598be":"**Testing Different Models**<br>\nI will be testing the following models with my training data:\n\n* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\n\nFor each model, we set the model, fit it with 80% of our training data, predict for other 20% of the training data and check the accuracy.","16e72228":"People less than 20 years had high Survival rate","43847a2c":"Remove Cabin feature, fill Age with values the way we filled for train data and mean for fare","b8f5f623":"<p>We can find missing data in couple of columns.<\/p>\n<p>Age, Cabin, Embarked have null or missing entries<\/p>","88849e27":"<h2>8. Create Submission File<\/h2>","18726ff3":"<h2>3. Missing Values<\/h2>","f7d6653d":"Random Forest algorithm gives the maximum accuracy. So let's use it to test our model","78b13897":"<h2>7. Clean Test Data to input to model<\/h2>","483459ec":"<h2>Table of Contents<\/h2>\n1. [ Import libraries ](#1.-Import-necessary-libraries)\n2. [ Read and explore data ](#2.-Read-and-explore-Data)\n3. [ Missing Values ](#3.-Missing-Values)\n4. [ Data Visualization ](#4.-Data-Visualization)\n5. [ Manipulation ](#5.-Data-Manipulation)\n6. [ Modelling ](#6.-Modelling)\n7. [ Clean test data to input to model ](#7.-Clean-Test-Data-to-input-to-model)\n8. [ Submission ](#8- Create-Submission-File)\n9. [ Conclusion ](#9.-Conclusion)","0afb6a8a":"    Numerical : Age, Fare -> (continous)  |  Pclass, Sibsp, Parch, pclass (discrete)\n    Categorical : Sex, Survived, Embarked\n    Alphanumerics : Ticket, Cabin","28b08075":"*<h4> Missing Value Observations <\/h4>*\n* Cabin has 77% missing values and it's alphanumeric. let us delete this feature.\n* Age has 19.8% missing and it's important feature in predicting the survival. We have to fill this feature.\n* Embarked has just 2 values missing. we can remove those 2 rows","5ded2d87":"<h2>6. Modelling<\/h2>\n<h3>Choosing the best model<\/h3>\n* Splitting the Training Data\n* We will use part of our training data (20% in this case) to test the accuracy of our different models.","76621558":"All missing values are treated. <br>\nNow that whole data is numerical and no data is missing, we can build a model using it.","b0267fa4":"<h2>1. Import necessary libraries<\/h2>","c8310756":"<h2>5. Data Manipulation<\/h2>","529d6dcf":"<h2>9. Conclusion<\/h2>\n* Random Forest algorithm gave the best accuracy for training dataset and the same is used to predict the survival of test dataset. The accuracy of the model is 73%.\n* Accuracy of the model can be improved taking names into consideration.\n* Age and Sex are major features effecting the survival of passengers. ( female of Pclass 1 has the highest survival rate while Male of Pclass3 has the lowest survival rate according the data we observed in data visualization section )\n* Around 20% of Age values are missing and it's treatment greatly affects the model prediction. The Missing values filled using aggregate mean values can be improved using the parameters like name, title of passenger present in name and their sex.","7eb18d91":"`train.isnull().any().any()` False indicates that none of the columns in DataFrame contains a NULL entry. <br>\nSo DataFrame is missing value free.","31b7f8a4":"Definitions and quick thoughts for reference:\n\n* **PassengerId**. Unique identification of the passenger. *( primary key, not needed for model building )*\n* **Survived**. Survival (0 = No, 1 = Yes). Binary variable - our target variable y.\n* <u>**Pclass**. Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd). Ready to go.<\/u>\n* **Name**. Name of the passenger. **Format** -> lastname, (Mr\/Ms\/Miss etc) first name (other name for female). Mr\/Ms\/Miss etc can give additional info like their occupation <i> ( will remove for now ) <\/i>\n* **Sex**. Sex of passenger. Categorical variable that should be encoded.\n* <u>**Age**. Age of passengers in years. Ready to go.<\/u>\n* <u>**SibSp**. # of siblings \/ spouses aboard the Titanic.<\/u> <i> ( combine with Parch to create a new feature family size )<\/i>\n* <u>**Parch**. # of parents \/ children aboard the Titanic.  <\/u>\n* **Ticket**. Ticket number. some complicated structure ( will remove for now )\n* <u>**Fare**. Passenger fare.  Ready to go.<\/u>\n* **Cabin**. Cabin number. It needs to be parsed.  ( will remove for now )\n* **Embarked**. Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton). Categorical feature that should be encoded.","2274dbae":"<h2> 4. Data Visualization <\/h2>"}}