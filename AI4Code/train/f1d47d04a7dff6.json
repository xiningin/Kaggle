{"cell_type":{"cdef85fc":"code","9c463102":"code","ae9c405e":"code","47e6a3e0":"code","de88af18":"code","23eb9129":"code","31329bde":"code","22ac3fdb":"code","c331880b":"code","3de7ff90":"code","b3a8afc5":"code","2c53e0b6":"code","eef4406e":"markdown","30a2809e":"markdown","7d33694c":"markdown","da58846a":"markdown","592705ed":"markdown","1d974b4a":"markdown","e1d53032":"markdown","dd30f30a":"markdown","3e3a3820":"markdown","26b6cca9":"markdown","564ca0c0":"markdown"},"source":{"cdef85fc":"import numpy as np \nimport pandas as pd \n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (\nActivation, BatchNormalization, Concatenate, Dense,\nEmbedding, Flatten, Input, Multiply, Reshape)\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfrom PIL import Image\nfrom keras.preprocessing.image import img_to_array, load_img","9c463102":"img = load_img(\"..\/input\/male-and-female-faces-dataset\/Male and Female face dataset\/Female Faces\/0 (1).jpeg\", target_size = (1024, 1024, 3))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()\n\nx = img_to_array(img)\nprint(x.shape)","ae9c405e":"img_shape = (1024, 1024, 3)\nz_dim = 100\nnum_classes = 2","47e6a3e0":"def build_generator(z_dim):\n    \n    model = Sequential()\n   \n    model.add(Dense(8*8*1024 , input_dim = z_dim))\n    model.add(Reshape((8, 8, 1024))) # Reshapes input into a 8*8*1024 tensor.\n    \n    model.add(Conv2DTranspose(512, kernel_size = 3, strides= 2 , padding = \"same\")) # (16* 16* 512)\n    \n    model.add(BatchNormalization())\n    \n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2DTranspose(256, kernel_size = 3, strides= 2 , padding = \"same\")) # (32* 32* 256)\n    \n    model.add(BatchNormalization())\n    \n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2DTranspose(256, kernel_size = 3, strides= 2 , padding = \"same\")) # (64* 64* 256)\n    \n    model.add(BatchNormalization())\n    \n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2DTranspose(128, kernel_size = 3, strides= 2 , padding = \"same\")) # (128* 128* 128)\n    \n    model.add(BatchNormalization())\n    \n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2DTranspose(64, kernel_size = 3, strides= 2 , padding = \"same\")) # (256* 256* 64)\n    \n    model.add(BatchNormalization())\n    \n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2DTranspose(32, kernel_size = 3, strides= 2 , padding = \"same\")) # (512* 512* 32)\n    \n    model.add(BatchNormalization())\n    \n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2DTranspose(3, kernel_size = 3, strides= 2 , padding = \"same\")) # (1024* 1024* 3)\n    \n    model.add(BatchNormalization())\n    \n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Activation('tanh'))\n    \n    return model\n    ","de88af18":"def build_cgan_generator(z_dim):\n    \n    z = Input(shape = (z_dim, ))\n    \n    label = Input(shape = (1, ) , dtype= \"int32\")\n    \n    embedding_label = Embedding(num_classes, z_dim, input_length = 1)(label)\n    \n    embedding_label = Flatten()(embedding_label)\n    \n    joined_representation = Multiply()([z, embedding_label])\n    \n    generator = build_generator(z_dim)\n    \n    conditioned_img = generator(joined_representation)\n    \n    return Model([z, label], conditioned_img)   \n","23eb9129":"def build_discriminator(img_shape):\n    model = Sequential() # input shape (1024* 1024* 3*2)\n    \n    model.add(Conv2D(32, kernel_size = 3, strides = 2, padding = \"same\", \n                     input_shape = (img_shape[0], img_shape[1], img_shape[2] * 2))) \n    # Number of images multiplied by 2 # (512* 512* 32)\n    \n    model.add(BatchNormalization())\n\n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2D(64, kernel_size = 3, strides = 2, padding = \"same\", input_shape = (img_shape))) # (256* 256* 64)\n    \n    model.add(BatchNormalization())\n\n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2D(64, kernel_size = 3, strides = 2, padding = \"same\", input_shape = (img_shape))) # (128* 128* 64)\n    \n    model.add(BatchNormalization())\n\n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2D(128, kernel_size = 3, strides = 2, padding = \"same\", input_shape = (img_shape))) # (64* 64* 128)\n    \n    model.add(BatchNormalization())\n\n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2D(256, kernel_size = 3, strides = 2, padding = \"same\", input_shape = (img_shape))) # (32* 32* 256)\n    \n    model.add(BatchNormalization())\n\n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2D(512, kernel_size = 3, strides = 2, padding = \"same\", input_shape = (img_shape))) # (16* 16* 512)\n    \n    model.add(BatchNormalization())\n\n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Conv2D(1024, kernel_size = 3, strides = 2, padding = \"same\", input_shape = (img_shape))) # (8* 8* 1024)\n    \n    model.add(BatchNormalization())\n\n    model.add(LeakyReLU(alpha = 0.01))\n    \n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n    \n    return model","31329bde":"def build_cgan_discriminator(img_shape):\n    img = Input(shape = img_shape)\n    \n    label = Input(shape = (1, ))\n    \n    embedding_label = Embedding(num_classes , np.prod(img_shape) , input_length = 1)(label)\n    \n    embedding_label = Flatten()(embedding_label)\n    \n    embedding_label = Reshape(img_shape)(embedding_label)\n    \n    concatenated = Concatenate(axis=-1)([img, embedding_label])\n\n    discriminator = build_discriminator(img_shape)\n    \n    classification = discriminator(concatenated)\n\n    return Model([img, label], classification)","22ac3fdb":"def build_cgan(generator, discriminator):\n    \n    z = Input(shape=(z_dim, ))\n    \n    label = Input(shape=(1, ))\n    \n    img = generator([z, label])\n    \n    classification = discriminator([img, label])\n    \n    return Model([z, label], classification)","c331880b":"discriminator = build_cgan_discriminator(img_shape)\n\ndiscriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n\ngenerator = build_cgan_generator(z_dim)\n\ndiscriminator.trainable = False\n\ncgan = build_cgan(generator, discriminator)\n\ncgan.compile(loss='binary_crossentropy', optimizer=Adam())","3de7ff90":"foldernames = os.listdir('..\/input\/male-and-female-faces-dataset\/Male and Female face dataset')\ncategories = []\nfiles = []\n\nfor k, folder in enumerate(foldernames):\n    filenames = os.listdir(\"..\/input\/male-and-female-faces-dataset\/Male and Female face dataset\/\" + folder);\n    for file in filenames:\n        im = Image.open(\"..\/input\/male-and-female-faces-dataset\/Male and Female face dataset\/\" + folder + \"\/\" + file);\n        im = np.asarray(im) \/ 127.5 - 1\n        files.append(im)\n        categories.append(k)\n\n        \ndf = pd.DataFrame({\n    'filename': files,\n    'category': categories\n})\n","b3a8afc5":"accuracies = []\nlosses = []\n\ndef train(iterations, batch_size, sample_interval):\n\n    for i in range(1000):\n        array[i] = array[i] \/ 127.5 - 1.\n\n    real = np.ones((batch_size, 1))\n\n    fake = np.zeros((batch_size, 1))\n\n    for iteration in range(iterations):\n        idx = np.random.randint(0, df.shape[0], batch_size)\n        \n        imgs, labels = df.filename[idx], df.category[idx]\n        \n        z = np.random.normal(0, 1, (batch_size, z_dim))\n\n        gen_imgs = generator.predict([z, labels])\n\n        d_loss_real = discriminator.train_on_batch([imgs, labels], real)\n\n        d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)\n\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        z = np.random.normal(0, 1, (batch_size, z_dim))\n\n        labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n\n        g_loss = cgan.train_on_batch([z, labels], real)\n\n        if (iteration + 1) % sample_interval == 0:\n\n            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %\n\n            (iteration + 1, d_loss[0], 100 * d_loss[1], g_loss))\n\n            losses.append((d_loss[0], g_loss))\n\n            accuracies.append(100 * d_loss[1])\n        ","2c53e0b6":"iterations = 12000\nbatch_size = 32\nsample_interval = 500\ntrain(iterations, batch_size, sample_interval)","eef4406e":"![2021-11-25 (3).png](attachment:37fc4562-8676-43d5-b339-1efe52ab8eea.png)\n\n\nIn the generator, we reshape the label to the size of the noise (using embedding). In discriminator, on the other hand, label shape is adjusted to the size of the image shape and concanated with the image coming from the generator.","30a2809e":"<a id=\"5\"><\/a><br>\n## Gan Model","7d33694c":"<a id=\"4\"><\/a><br>\n## Discriminator Model\n![discriminator.png](attachment:cdb0b9a6-348c-4e04-abec-fcd848982b2c.png)\n\nNotice that the discriminator takes two images as input.","da58846a":"<a id=\"1\"><\/a><br>\n## Import Libraries","592705ed":"<a id=\"2\"><\/a><br>\n## Overwiev the Data","1d974b4a":"We implement the CGAN Generator. By now, you should be familiar with much of this network from chapters 4 and 7. The modifications made for the CGAN center around input handling, where we use embedding and element-wise multiplication to combine the random noise vector z and the label y into a joint representation. Let\u2019s walk through what the code does: \n\n1-) Take label y (an image woman or man face) and turn it into a dense vector of size z_dim (the length of the random noise vector) by using the Keras Embedding layer. \n\n2-) Combine the label embedding with the noise vector z into a joint representation by using the Keras Multiply layer. As its name suggests, this layer multiplies the corresponding entries of the two equal-length vectors and outputs a single vector of the resulting products. \n\n3-) Feed the resulting vector as input into the rest of the CGAN Generator network to synthesize an image.","e1d53032":"<a id=\"3\"><\/a><br>\n## Generator Model\n![Generator.png](attachment:4d949d83-6b07-44df-834c-76b3abce5978.png)\n\nLet\u2019s call the conditioning label y. The Generator uses the random noise vector z and the label y to synthesize a fake example G(z, y) = x*|y (read as \u201cx* given that, or conditioned on, y\u201d). The goal of this fake example is to look (in the eyes of the Discriminator) as close as possible to a real example for the given label. \n\n\nThe reason why we use leaky relu is that the derivative on the negative side does not reset and lose the information as relu does. Leaky relu has a slight slope in the negative and does not lose learning.\n\nNormalizing the images to be between \u20131 and 1 is still typically a good idea according to almost every machine learning resource. We generally normalize because of the easier tractability of computations, as is the case with the rest of machine learning. Given this restriction on the inputs, it is a good idea to restrict your Generator\u2019s final output with, for example, a tanh activation function.  We prefer tanh here because the tanh derivative is higher than the sigmoid function between [-1,1]","dd30f30a":"<a id=\"7\"><\/a><br>\n## Output\n\nGANs need a lot of memory to train, but unfortunately I do not have the device to provide this memory. I tried many pieces of code that will use less memory over the internet, but none of them worked. Then I tried to run it on different platforms from kaggle, but my computer crashed. We wrote the code but we can't see the result. Actually, the important thing was that you had an idea about the conditional GANs. The rest is up to a powerful computer. I hope you weren't too mad at me.","3e3a3820":"# Conditional GANs with Keras\n\nWhile the feature sought in ordinary GANs is only to produce fake images that cannot be distinguished from the real one, in CGANs it is also required to produce images that match the label. For example, the CGAN Discriminator should learn to reject the pair regardless of whether the example (handwritten numeral 3) is real or fake, because it does not match the label 4. The CGAN Discriminator should also learn to reject all image-label pairs in which the image is fake, even if the label matches the image. Accordingly, in order to fool the Discriminator, it is not enough for the CGAN Generator to produce realistic-looking data. The examples it generates also need to match their labels. After the Generator is fully trained, this then allows us to specify what example we want the CGAN to synthesize by passing it the desired label.\n\n\n\n\nContent :\n* [Import libraries](#1)\n* [Overwiev the Data](#2)\n* [Generator Model](#3)\n* [Discriminator Model](#4)\n* [Gan Model](#5)\n* [Data Normalization and Convert into CSV Format](#6)\n* [Output](#7)\n* [In Summary](#8)\n\n","26b6cca9":"<a id=\"6\"><\/a><br>\n## Data Normalization and Convert into CSV Format","564ca0c0":"<a id=\"8\"><\/a><br>\n## In Summary\n\nDon't forget to upvote if you like it. If you want to know more about GANs, definitely get the GANs in Action book by Jakub Langr and Vladimir Bok, which I have benefited greatly from. Thanks for your time on the notebook."}}