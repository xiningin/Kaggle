{"cell_type":{"33ea6918":"code","cb6ac782":"code","22d20284":"code","a083fa6d":"code","db807c7b":"code","29a4a8a8":"code","b5d01839":"code","4bb01841":"code","b8bdff6a":"code","43e403b8":"code","7cc10f66":"code","d0823a96":"code","05ae6b6d":"code","901eb9df":"code","edf19f97":"code","4ab2ec88":"code","7fc0f41e":"code","bc1d8889":"code","c2c5323f":"code","fbba9214":"code","b5a03cce":"code","d21e88ef":"code","7c3b6072":"code","7034e27e":"code","1f260737":"code","101bac43":"code","d430b17c":"code","c2d03ab5":"code","aff52237":"code","1e6fc7cf":"code","ac37dd84":"code","d5e3b08e":"code","7323eb9a":"code","a7fc56bb":"code","97058ef0":"code","bcd3d148":"code","605bd597":"code","dc645230":"code","7e132060":"code","c3f5a0df":"code","377e1a21":"code","dc1e7139":"code","a903f41e":"code","4688c360":"code","f997c52e":"code","3cdb7e3b":"code","bb3358e6":"code","9dd828e7":"code","a7ca40de":"code","d335f33c":"code","0a7ed720":"code","c03d9e26":"code","ff765f38":"code","4a6e32f6":"code","8773c895":"code","43b01703":"code","f75969b0":"code","93470b6f":"code","04db3490":"code","9b714419":"code","d59561a0":"code","0d8cba5e":"code","14386b6b":"code","bbe03aa0":"code","9b5e7ee7":"code","592569f7":"code","481462b8":"code","1b98af6d":"code","0a8a6753":"code","858fb96c":"code","9b418c27":"code","56a7a4ac":"code","bfa4642a":"code","d4528e6b":"code","0f10d4c6":"code","89b58570":"code","84f24987":"code","d0fc0c2f":"code","b11d8690":"code","60dbc77f":"code","30903087":"code","cf30dc54":"code","22c9b7ba":"code","f2dbe5fc":"code","fba4dc42":"code","28135df8":"code","84643928":"code","be726a1f":"code","03c5fecd":"code","3b6f586e":"code","7b93ab99":"code","b85528f6":"code","423e4ee0":"code","58d3b560":"code","7b581bb5":"code","371b7e45":"code","f9ea823d":"code","4fca1eb7":"code","ba9fc244":"code","3791da40":"code","fa9f21c6":"code","8908c01f":"code","b8a2bd68":"code","efeca1f3":"code","59f3431e":"code","9981e3a0":"code","5dc2fc2a":"markdown","8d525582":"markdown","d6a6b8a1":"markdown","0c4d4c9d":"markdown","7a321132":"markdown","196bd28d":"markdown","526470ce":"markdown","961c7e0b":"markdown","2afdbdad":"markdown","63700c53":"markdown","d7d3b798":"markdown","ba98b9bd":"markdown","f71ce7e2":"markdown","59d9c2ec":"markdown","99d209c8":"markdown","f0ab460e":"markdown","35012ad9":"markdown","ba298674":"markdown","a45daceb":"markdown","d84fd894":"markdown","e73332fb":"markdown","3b7e792e":"markdown","a4cf4057":"markdown","c4c99c2e":"markdown","f48bc9e2":"markdown","24db4ada":"markdown","9d351fa6":"markdown","1620cdaf":"markdown","2b4adf7d":"markdown","89252dc5":"markdown","54a9266b":"markdown","b6d0ba8f":"markdown","27c725bf":"markdown","cf13b566":"markdown"},"source":{"33ea6918":"# Basic Libraries\nimport numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cb6ac782":"# Visualization\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\nimport seaborn as sns\n\n# Data preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# StratifiedKFold\nfrom sklearn.model_selection import StratifiedKFold\n\n# Grid search\nfrom sklearn.model_selection import GridSearchCV\n\n# Learning curve\nfrom sklearn.model_selection import learning_curve\n\n# Validation curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import cross_val_score\n\n# Confusion matrix and scores\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# ROC curve\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy import interp","22d20284":"class k_fold_cross_val:\n    def __init__(self, X_train, y_train, estimator, cv):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.estimator = estimator\n        self.cv = cv\n        \n    def cross_val_kfold(self):\n        kfold = StratifiedKFold(n_splits=self.cv, random_state=10)\n        self.kfold = kfold\n        \n        scores = []\n        for train_idx, test_idx in self.kfold.split(self.X_train, self.y_train):\n            self.estimator.fit(self.X_train[train_idx], self.y_train.values[train_idx])\n            score = self.estimator.score(self.X_train[test_idx], self.y_train.values[test_idx])\n            scores.append(score)\n            print(\"Class: %s, Acc: %.3f\" % (np.bincount(self.y_train.values[train_idx]), score))\n            self.scores = scores\n            \n    def score(self):\n        scores = cross_val_score(estimator=self.estimator, X=self.X_train, y=self.y_train, cv=self.cv, n_jobs=1)\n        print(\"CV accuracy scores: %s\" % self.scores)\n        print(\"CV accuracy: %.3f +\/- %.3f\" % (np.mean(self.scores), np.std(self.scores)))\n        \n    def draw_roc_curve(self, X_test, y_test):\n        self.X_test = X_test\n        self.y_test = y_test\n        \n        mean_tpr=0\n        mean_fpr=np.linspace(0,1,100)\n        plt.figure(figsize=(10,6))\n        for train_idx, test_idx in self.kfold.split(self.X_train, self.y_train):\n            proba = self.estimator.fit(self.X_train[train_idx], self.y_train.values[train_idx]).predict_proba(self.X_train[test_idx])\n            fpr, tpr, thresholds = roc_curve(y_true=self.y_train.values[test_idx], y_score=proba[:,1], pos_label=1)\n            mean_tpr += interp(mean_fpr, fpr, tpr)\n            mean_tpr[0] = 0\n            roc_auc = auc(fpr, tpr)\n            plt.plot(fpr, tpr, lw=1, label=\"ROC fold (area=%.2f)\" %(roc_auc))\n        \n        # Line\n        plt.plot([0,1], [0,1], linestyle='--', color=(0.6,0.6,0.6), label=\"random guessing\")\n        # plot mean of fpr, tpr roc_auc\n        mean_tpr \/= self.cv\n        mean_tpr[-1] = 1.0\n        mean_auc = auc(mean_fpr, mean_tpr)\n        plt.plot(mean_fpr, mean_tpr, 'k--', label=\"mean ROC (area = %.2f)\" % mean_auc, color=\"blue\")\n        # Line\n        plt.plot([0,0,1], [0,1,1], lw=2, linestyle=':', color=\"black\", label='perfect performance')\n        plt.xlabel(\"false positive rate\")\n        plt.ylabel(\"true positive rate\")\n        plt.title(\"Receiver Operator Characteristic\")\n        plt.legend()","a083fa6d":"def draw_learning_curve(estimator, X_train, y_train):\n    # learning curve\n    train_sizes, train_scores, test_scores = learning_curve(estimator=estimator, X=X_train, y=y_train, train_sizes=np.linspace(0.1,1,10), cv=10, n_jobs=1)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # plot\n    plt.figure(figsize=(10,6))\n    # train data\n    plt.plot(train_sizes, train_mean, color=\"blue\", marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes, train_mean+train_std, train_mean-train_std, color=\"blue\", alpha=0.15)\n    # val data\n    plt.plot(train_sizes, test_mean, color=\"green\", marker='s', linestyle='--', markersize=5, label='validation accuracy')\n    plt.fill_between(train_sizes, test_mean+test_std, test_mean-test_std, color=\"green\", alpha=0.15)\n\n    plt.grid()\n    plt.xlabel(\"Number of trainig samples\")\n    plt.ylabel(\"Accuracy\")\n    plt.ylim([0.8,1.0])\n    plt.title(\"Learning curve\")\n    plt.legend()","db807c7b":"def draw_validation_curve(estimator, X_train, y_train, param_name, param_range, xscale):\n    # validation curve\n    train_scores, test_scores = validation_curve(estimator=estimator, X=X_train, y=y_train, param_name=param_name, param_range=param_range, cv=10)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    \n    # plot\n    plt.figure(figsize=(10,6))\n    # train data\n    plt.plot(param_range, train_mean, color=\"blue\", marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(param_range, train_mean+train_std, train_mean-train_std, color=\"blue\", alpha=0.15)\n    # val data\n    plt.plot(param_range, test_mean, color=\"green\", marker='s', linestyle='--', markersize=5, label='validation accuracy')\n    plt.fill_between(param_range, test_mean+test_std, test_mean-test_std, color=\"green\", alpha=0.15)\n\n    plt.grid()\n    plt.xlabel(\"{}\".format(param_name))\n    if xscale==\"log\":\n        plt.xscale(\"log\")\n    else:\n        pass\n    plt.ylabel(\"Accuracy\")\n    plt.ylim([0.8,1.0])\n    plt.title(\"Validation curve\")\n    plt.legend()","29a4a8a8":"def confmat_roccurve(X_test, y_test, y_pred, estimator):\n    # create confusion matrix\n    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n    # visualiazation confusion matrix\n    fig, ax = plt.subplots(1,2,figsize=(18,6))\n    \n    ax[0].matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax[0].text(x=j, y=i, s=confmat[i,j], va=\"center\", ha=\"center\")\n            \n    ax[0].set_xlabel(\"predicted label\")\n    ax[0].set_ylabel(\"true label\")\n    ax[0].set_title(\"confusion matrix\")\n    # Score\n    print(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n    print(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n    print(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n    print(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\n    \n    # visualization roc curve\n    y_score = estimator.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n    ax[1].plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr), color=\"blue\")\n    ax[1].plot([0,1], [0,1], linestyle='--', color=(0.6,0.6,0.6), label='random')\n    ax[1].plot([0,0,1], [0,1,1], linestyle=':', color=\"black\", label='perfect performance')\n    ax[1].set_xlabel(\"false positive rate\")\n    ax[1].set_ylabel(\"true positive rate\")\n    ax[1].set_title(\"Receiver Operator Characteristic\")\n    ax[1].legend()","b5d01839":"## Data loading\ndf = pd.read_csv(\"\/kaggle\/input\/housesalesprediction\/kc_house_data.csv\", header=0)","4bb01841":"# data frame\ndf.head()","b8bdff6a":"# Null values\ndf.isnull().sum().sum()","43e403b8":"# Data info\ndf.info()","7cc10f66":"plt.figure(figsize=(10,6))\nsns.distplot(df[\"price\"])\nplt.vlines([df[\"price\"].quantile(0.75)], 0, 0.000002, \"red\", linestyles='-')\nplt.vlines([df[\"price\"].quantile(0.98)], 0, 0.000002, \"blue\", linestyles='-')\nplt.xlabel(\"price\")\nplt.ylabel(\"frequency\")","d0823a96":"quat_98 = df[\"price\"].quantile(0.98)\nquat_75 = df[\"price\"].quantile(0.75)\n\ndf = df[df[\"price\"]<=quat_98]","05ae6b6d":"# define function\ndef price_flg(x):\n    if x[\"price\"] > quat_75:\n        res = 1\n    else:\n        res = 0\n    return res\n# apply function\ndf[\"price_flg\"] = df.apply(price_flg, axis=1)","901eb9df":"# Checking\ndf[\"price_flg\"].value_counts()","edf19f97":"# Target value\ny = df[\"price_flg\"]","4ab2ec88":"ex_columns = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view',\n              'condition', 'grade','sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15']","7fc0f41e":"# yr_built\nlatest_year = df[\"yr_built\"].max()\ndf[\"yr_built\"] = latest_year - df[\"yr_built\"]","bc1d8889":"# define function\ndef renov(x):\n    if x[\"yr_renovated\"] == 0:\n        res = x[\"yr_built\"]\n    else:\n        res = latest_year - x[\"yr_renovated\"]\n    return res\n\n# apply function\ndf[\"yr_renovated\"] = df.apply(renov, axis=1)","c2c5323f":"X = df[ex_columns]","fbba9214":"# Sample 200\nsns.pairplot(X.sample(200))","b5a03cce":"## Correlation\nmatrix = X.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(matrix, vmax=1, vmin=-1, cmap=\"bwr\", square=True)","d21e88ef":"# data split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)","7c3b6072":"# Scaling\nsc = StandardScaler()\nsc.fit(X_train)\n\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.fit_transform(X_test)","7034e27e":"# Library\nfrom sklearn.linear_model import LogisticRegression\n\n# Instance\nlr = LogisticRegression()","1f260737":"# prameters\nparam_range = [0.001, 0.01, 0.1, 1.0, 10, 100]\npenalty = ['l1', 'l2']\nparam_grid = [{\"C\":param_range, \"penalty\":penalty}]\n\n# Optimization by Grid search\ngs = GridSearchCV(estimator=lr, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n\ngs = gs.fit(X_train_std, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)","101bac43":"# Test data validation by best estimator\ngs_l = gs.best_estimator_\ny_pred = gs_l.predict(X_test_std)\nprint('Test accuracy: %.3f' % gs_l.score(X_test_std, y_test))","d430b17c":"# cross validation\ncv = k_fold_cross_val(X_train_std, y_train, gs_l, 5)\ncv.cross_val_kfold()","c2d03ab5":"# cross val score\ncv.score()","aff52237":"# cv roc curve\ncv.draw_roc_curve(X_test_std, y_test)","1e6fc7cf":"# learning curve\ndraw_learning_curve(gs_l, X_train_std, y_train)","ac37dd84":"# validation curve\n# param C\ndraw_validation_curve(lr, X_train_std, y_train, \"C\", param_range, \"log\")","d5e3b08e":"# Confusion matrix and ROC curve\nconfmat_roccurve(X_test_std, y_test, y_pred, gs_l)","7323eb9a":"# coefficient\ncoef = pd.DataFrame({\"Variable\":X.columns, \"Coef\":gs_l.coef_[0]}).sort_values(by=\"Coef\")\nintercept = pd.DataFrame([[\"intercept\", gs_l.intercept_[0]]], columns=coef.columns)\n\ncoef = coef.append(intercept)\n\n# Visualization\nplt.figure(figsize=(10,6))\nplt.bar(coef[\"Variable\"], coef[\"Coef\"])\nplt.xlabel(\"Variables\")\nplt.xticks(rotation=90)\nplt.ylabel(\"Coefficient\")","a7fc56bb":"# Library\nimport scipy as sp\nfrom scipy import stats\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm","97058ef0":"data = pd.concat([pd.DataFrame(X_train_std, columns=X.columns), pd.DataFrame({\"price_flg\":y_train.values})], axis=1)\ndata.head()","bcd3d148":"# predict logistic regression model\nlr_stats = smf.glm(formula=\"price_flg ~ bedrooms+bathrooms+sqft_living+sqft_lot+floors+waterfront+view+condition+grade+sqft_above+sqft_basement+yr_built+yr_renovated+sqft_living15+sqft_lot15\", data=data, family=sm.families.Binomial()).fit()\nlr_stats.summary()","605bd597":"# plot by max coef variable vs prediction\nplt.figure(figsize=(10,6))\nsns.lmplot(x=\"grade\", y=\"price_flg\", data=data, logistic=True, scatter_kws={\"color\":\"blue\"}, line_kws={\"color\":\"black\"}, x_jitter=0.1, y_jitter=0.02)","dc645230":"# Library\n# Omitted because calculation is heavy\n# from sklearn.svm import SVC\n\n# Instance\n# svm = SVC(random_state=10, kernel=\"linear\", probability=True)","7e132060":"# prameters\n# Omitted because calculation is heavy\n# param_range = [0.1, 1.0, 10, 100]\n# param_grid = [{\"C\":param_range, \"gamma\":param_range}]\n\n# Optimization by Grid search\n# gs = GridSearchCV(estimator=svm, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n\n# gs = gs.fit(X_train_std, y_train)\n\n# print(gs.best_score_)\n# print(gs.best_params_)","c3f5a0df":"# Test data validation by best estimator\n# Omitted because calculation is heavy\n# gs_sl = gs.best_estimator_\n# y_pred = gs_sl.predict(X_test_std)\n# print('Test accuracy: %.3f' % gs_sl.score(X_test_std, y_test))","377e1a21":"# cross validation\n# Omitted because calculation is heavy\n# cv = k_fold_cross_val(X_train_std, y_train, gs_sl, 5)\n# cv.cross_val_kfold()","dc1e7139":"# cross val score\n# Omitted because calculation is heavy\n# cv.score()","a903f41e":"# cv roc curve\n# Omitted because calculation is heavy\n# cv.draw_roc_curve(X_test_std, y_test)","4688c360":"# learning curve\n# Omitted because calculation is heavy\n# draw_learning_curve(gs_sl, X_train_std, y_train)","f997c52e":"# validation curve\n# Omitted because calculation is heavy\n# draw_validation_curve(svm, X_train_std, y_train, \"C\", param_range, \"log\")","3cdb7e3b":"# Confusion matrix and ROC curve\n# confmat_roccurve(X_test_std, y_test, y_pred, gs_sl)","bb3358e6":"# Library\n# Omitted because calculation is heavy\n# from sklearn.svm import SVC\n\n# Instance\n# svm = SVC(random_state=10, kernel='rbf', probability=True)","9dd828e7":"# prameters\n# Omitted because calculation is heavy\n# param_range = [0.1, 1.0, 10, 100]\n# param_grid = [{\"C\":param_range, \"gamma\":param_range}]\n\n# Optimization by Grid search\n# gs = GridSearchCV(estimator=svm, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n\n# gs = gs.fit(X_train_std, y_train)\n\n# print(gs.best_score_)\n# print(gs.best_params_)","a7ca40de":"# Test data validation by best estimator\n# Omitted because calculation is heavy\n# gs_sr = gs.best_estimator_\n# y_pred = gs_sr.predict(X_test_std)\n# print('Test accuracy: %.3f' % gs_sr.score(X_test_std, y_test))","d335f33c":"# cross validation\n# Omitted because calculation is heavy\n# cv = k_fold_cross_val(X_train_std, y_train, gs_sr, 5)\n# cv.cross_val_kfold()","0a7ed720":"# cross val score\n# Omitted because calculation is heavy\n# cv.score()","c03d9e26":"# cv roc curve\n# Omitted because calculation is heavy\n# cv.draw_roc_curve(X_test_std, y_test)","ff765f38":"# learning curve\n# Omitted because calculation is heavy\n# draw_learning_curve(gs_s, X_train_std, y_train)","4a6e32f6":"# validation curve\n# Omitted because calculation is heavy\n# draw_validation_curve(svm, X_train_std, y_train, \"C\", param_range, \"log\")","8773c895":"# Confusion matrix and ROC curve\n# confmat_roccurve(X_test_std, y_test, y_pred, gs_sr)","43b01703":"# Library\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Instance\nknn = KNeighborsClassifier(metric='minkowski')","f75969b0":"# prameters\nparam_range = [10, 15, 20, 25]\nparam_grid = [{\"n_neighbors\":param_range, \"p\":[1,2]}]\n\n# Optimization by Grid search\ngs = GridSearchCV(estimator=knn, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n\ngs = gs.fit(X_train_std, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)","93470b6f":"# Test data validation by best estimator\ngs_kn = gs.best_estimator_\ny_pred = gs_kn.predict(X_test_std)\nprint('Test accuracy: %.3f' % gs_kn.score(X_test_std, y_test))","04db3490":"# cross validation\ncv = k_fold_cross_val(X_train_std, y_train, gs_kn, 5)\ncv.cross_val_kfold()","9b714419":"# cross val score\ncv.score()","d59561a0":"# cv roc curve\ncv.draw_roc_curve(X_test_std, y_test)","0d8cba5e":"# learning curve\ndraw_learning_curve(gs_kn, X_train_std, y_train)","14386b6b":"# validation curve\ndraw_validation_curve(knn, X_train_std, y_train, \"n_neighbors\", param_range, \"log\")","bbe03aa0":"# Confusion matrix and ROC curve\nconfmat_roccurve(X_test_std, y_test, y_pred, gs_kn)","9b5e7ee7":"# Library\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Instance\ntree = DecisionTreeClassifier(random_state=10)","592569f7":"# prameters\nparam_range = [1, 3, 5, 10]\nleaf = [17, 18, 19, 20, 21, 22, 23]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\n# Optimization by Grid search\ngs = GridSearchCV(estimator=tree, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n\ngs = gs.fit(X_train.values, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)","481462b8":"# Test data validation by best estimator\ngs_tr = gs.best_estimator_\ny_pred = gs_tr.predict(X_test.values)\nprint('Test accuracy: %.3f' % gs_tr.score(X_test, y_test))","1b98af6d":"# cross validation\ncv = k_fold_cross_val(X_train.values, y_train, gs_tr, 5)\ncv.cross_val_kfold()","0a8a6753":"# cross val score\ncv.score()","858fb96c":"# cv roc curve\ncv.draw_roc_curve(X_test.values, y_test)","9b418c27":"# learning curve\ndraw_learning_curve(gs_tr, X_train, y_train)","56a7a4ac":"# validation curve\ndraw_validation_curve(tree, X_train, y_train, \"max_depth\", param_range, \"\")","bfa4642a":"# Confusion matrix and ROC curve\nconfmat_roccurve(X_test, y_test, y_pred, gs_tr)","d4528e6b":"# Library\n!pip install dtreeviz\nfrom sklearn import tree\nfrom dtreeviz.trees import *\nimport graphviz","0f10d4c6":"# Fitting\ntree_c = tree.DecisionTreeClassifier(max_depth=5, max_leaf_nodes=20)\ntree_c.fit(X_train, y_train)","89b58570":"# Visualization\nviz = dtreeviz(tree_c, X_train, y_train, target_name=\"price_flg\", feature_names=list(X_train.columns), class_names=list(y_train))\nviz","84f24987":"# Library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instance\nforest = RandomForestClassifier(n_estimators=10, random_state=10)","d0fc0c2f":"# prameters\nparam_range = [5, 10, 15, 20]\nleaf = [15, 20, 25, 30]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\n# Optimization by Grid search\ngs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n\ngs = gs.fit(X_train, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)","b11d8690":"# Test data validation by best estimator\ngs_rf = gs.best_estimator_\ny_pred = gs_rf.predict(X_test)\nprint('Test accuracy: %.3f' % gs_rf.score(X_test, y_test))","60dbc77f":"# cross validation\ncv = k_fold_cross_val(X_train.values, y_train, gs_rf, 5)\ncv.cross_val_kfold()","30903087":"# cross val score\ncv.score()","cf30dc54":"# cv roc curve\ncv.draw_roc_curve(X_test.values, y_test)","22c9b7ba":"# learning curve\ndraw_learning_curve(gs_rf, X_train, y_train)","f2dbe5fc":"# validation curve\ndraw_validation_curve(forest, X_train, y_train, \"max_depth\", param_range, \"\")","fba4dc42":"# Confusion matrix and ROC curve\nconfmat_roccurve(X_test, y_test, y_pred, gs_rf)","28135df8":"forest = RandomForestClassifier(criterion='gini', max_depth=10, max_leaf_nodes=19)\nforest.fit(X_train, y_train)\n\nimportance = forest.feature_importances_\n\nindices = np.argsort(importance)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" %(f+1, 30, X_train.columns[indices[f]], importance[indices[f]]))","84643928":"# Library\nimport xgboost as xgb\n\n# Instance\nxgb = xgb.XGBClassifier(random_state=10)","be726a1f":"# prameters\nmax_depth = [10, 15, 20, 25]\nmin_samples_leaf = [1,3,5]\nmin_samples_split = [1,2,4]\n\nparam_grid = [{\"max_depth\":max_depth,\n               \"min_samples_leaf\":min_samples_leaf, \"min_samples_split\":min_samples_split}]\n\n# Optimization by Grid search\ngs = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n\ngs = gs.fit(X_train, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)","03c5fecd":"# Test data validation by best estimator\ngs_xg = gs.best_estimator_\ny_pred = gs_xg.predict(X_test)\nprint('Test accuracy: %.3f' % gs_xg.score(X_test, y_test))","3b6f586e":"# cross validation\ncv = k_fold_cross_val(X_train.values, y_train, gs_xg, 5)\ncv.cross_val_kfold()","7b93ab99":"# cross val score\ncv.score()","b85528f6":"# cv roc curve\ncv.draw_roc_curve(X_test.values, y_test)","423e4ee0":"# learning curve\n# draw_learning_curve(gs_xg, X_train.values, y_train)","58d3b560":"# validation curve\n# draw_validation_curve(xgb, X_train.values, y_train, \"max_depth\", param_range, \"\")","7b581bb5":"# Confusion matrix and ROC curve\nconfmat_roccurve(X_test.values, y_test, y_pred, gs_xg)","371b7e45":"# Library\nimport lightgbm as lgb\n\n# Instance\nlgb = lgb.LGBMClassifier()","f9ea823d":"# prameters\nmax_depth = [5, 10, 15]\nmin_samples_leaf = [1,3,5,7]\nmin_samples_split = [4,6, 8, 10]\n\nparam_grid = [{\"max_depth\":max_depth,\n               \"min_samples_leaf\":min_samples_leaf, \"min_samples_split\":min_samples_split}]\n\n# Optimization by Grid search\ngs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n\ngs = gs.fit(X_train, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)","4fca1eb7":"# Test data validation by best estimator\ngs_lg = gs.best_estimator_\ny_pred = gs_lg.predict(X_test)\nprint('Test accuracy: %.3f' % gs_lg.score(X_test, y_test))","ba9fc244":"# cross validation\ncv = k_fold_cross_val(X_train.values, y_train, gs_lg, 5)\ncv.cross_val_kfold()","3791da40":"# cross val score\ncv.score()","fa9f21c6":"# cv roc curve\ncv.draw_roc_curve(X_test.values, y_test)","8908c01f":"# learning curve\ndraw_learning_curve(gs_lg, X_train, y_train)","b8a2bd68":"# validation curve\ndraw_validation_curve(lgb, X_train, y_train, \"max_depth\", param_range, \"\")","efeca1f3":"# Confusion matrix and ROC curve\nconfmat_roccurve(X_test.values, y_test, y_pred, gs_lg)","59f3431e":"# ROC AUC scores, calculated from y_pred\nlr_score = roc_auc_score(y_true=y_test, y_score=gs_l.predict(X_test_std))\nkn_score = roc_auc_score(y_true=y_test, y_score=gs_kn.predict(X_test_std))\ntr_score = roc_auc_score(y_true=y_test, y_score=gs_tr.predict(X_test))\nrf_score = roc_auc_score(y_true=y_test, y_score=gs_rf.predict(X_test))\nxg_score = roc_auc_score(y_true=y_test, y_score=gs_xg.predict(X_test.values))\nlg_score = roc_auc_score(y_true=y_test, y_score=gs_lg.predict(X_test))","9981e3a0":"# scores\nname = [\"logistic regression\", \"k Neighbors\", \"Decision tree\", \"Random forest\", \"XGB\", \"LGBM\"]\nscore = [lr_score, kn_score, tr_score, rf_score, xg_score, lg_score]\n\nlast = pd.DataFrame({\"name\":name, \"score\":score})\n\nplt.figure(figsize=(10,6))\nplt.bar(last[\"name\"], last[\"score\"])\nplt.xlabel(\"Classification method\")\nplt.ylabel(\"ROC AUC score\")\nplt.xticks(rotation=90)","5dc2fc2a":"### StratifiedKFold & ROC curve","8d525582":"# Classification method\n- Logistic Regression\n- SVM\n- kernel SVM\n- k-nearest neighbor algorithm\n- Decision tree\n- Random forest\n- XGBoost\n- LGBM\n- Perceptron","d6a6b8a1":"## Data preprocessing","0c4d4c9d":"# Decision tree\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html?highlight=decision%20tree#sklearn.tree.DecisionTreeClassifier\n\n* For tree methods, X_train data change to X_train from X_train_std, but they neet to change array with (.values).\nScikitlearn libraries can be taked this preprocessing by program, but other may be down, so, they may be before insert function.","7a321132":"### Learning curve","196bd28d":"# Logistic regression\nhttps:\/\/scikit-learn.org\/stable\/modules\/linear_model.html","526470ce":"Red line is quantile 75% position, Also, the tail of the distribution is quite long. For the purpose of summarizing the analysis method, this time we treat it as an outlier and exclude data with prices above the top 2% (blue line) in advance.","961c7e0b":"To compare the prediction results, separate into training data and test data. For regression analysis, data scaling is required, so that processing is performed.","2afdbdad":"## Define function","63700c53":"## Libraries","d7d3b798":"### Confusion matrix and roc curve","ba98b9bd":"The zip code, latitude and longitude are not used this time.","f71ce7e2":"## Feature importance","59d9c2ec":"### Validation curve","99d209c8":"# SVM(Support vector machine)\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html","f0ab460e":"# Random forest\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#sklearn.ensemble.RandomForestClassifier","35012ad9":"## Logistic regression by stats model (It can take validation of p values)","ba298674":"### Confirmation of explanatory variables","a45daceb":"The year of construction is changed to the number of years of construction. The largest year is set as the latest and used as the starting point.\nIf yr_renov is 0, it is the building age, otherwise it is the number of years from the latest year.","d84fd894":"### Create target flag","e73332fb":"### Data distribution","3b7e792e":"# XGB\nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/model.html","a4cf4057":"### Classification Medhod\n- Logistic Regression\n- SVM\n- kernel SVM\n- k-nearest neighbor algorithm\n- Decision tree\n- Random forest\n- XGBoost\n- LGBM","c4c99c2e":"## ----XGB cannot draw the learning curve and the validation curve well.----","f48bc9e2":"# k-nearest neighbor algorithm\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighbor#sklearn.neighbors.KNeighborsClassifier","24db4ada":"## Data loadin and check","9d351fa6":"## Data preprocessing","1620cdaf":"# LGBM\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/","2b4adf7d":"Create a summary of classification machine learning methods using House price prediction as the subject.\n\nFor classification, the price was set at the top 25% and the other variables were used as the objective variables to create the prediction model.","89252dc5":"# Notebook, Classification method","54a9266b":"# kernel SVM","b6d0ba8f":"## Visualization tree\nWith dtreeviz","27c725bf":"## Summery\ncalcurated by y_pred(0 or 1 flag)","cf13b566":"Many collinearities are confirmed. This time, it will be carried out as it is, and the regularization effect will be confirmed in the linear prediction."}}