{"cell_type":{"0f11327d":"code","0d0e2926":"code","67c086f9":"code","fee3be78":"code","1e43fb5f":"code","86ba6356":"code","832c23dd":"code","1af7b132":"code","66ce3e11":"code","ab27383c":"code","6b0e785d":"code","30eea64e":"code","d58dd827":"code","fdbcaea4":"code","226e6c81":"code","8196ffa1":"code","ce675063":"code","8637cde7":"code","3bed0082":"code","c2c38352":"code","e66c6234":"code","1f3c9c1d":"code","7817bfd9":"code","dbdd0fbd":"code","a87cc536":"code","706ef1bd":"code","9e504da2":"code","cba45e0c":"code","6f6fc641":"code","41a514b1":"code","29099009":"markdown","ebe5d29f":"markdown","da806985":"markdown","cf85b039":"markdown","ebf20740":"markdown","a9b3eac4":"markdown","25350de1":"markdown","e5ff059b":"markdown","e5e5e71a":"markdown","22e01f75":"markdown"},"source":{"0f11327d":"\nimport numpy as np \nimport pandas as pd \nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","0d0e2926":"df = pd.read_excel(\"..\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx\")\n\n\nprint(df.shape)\n\ndf.head(5)","67c086f9":"df.dtypes","fee3be78":"df.select_dtypes(include=['object']).head()","1e43fb5f":"print(df.select_dtypes(include=['object']).isnull().sum())","86ba6356":"df.select_dtypes(exclude=['object']).isnull().sum()","832c23dd":"df.dropna().shape","1af7b132":"df_cat = df.select_dtypes(include=['object'])\ndf_numeric = df.select_dtypes(exclude=['object'])\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nidf = pd.DataFrame(imp.fit_transform(df_numeric))\nidf.columns = df_numeric.columns\nidf.index = df_numeric.index\n\n\nidf.isnull().sum()\n","66ce3e11":"def perc_to_int(percentile):\n    #print(percentile, ''.join(filter(str.isdigit, percentile)))\n    return(int(''.join(filter(str.isdigit, percentile))))\n   \ndef wdw_to_int(window):\n    if window == \"ABOVE_12\":\n        window = \"ABOVE-13\"\n    #print(window, ''.join(filter(str.isdigit, window.split(\"-\")[1])))\n    return(int(''.join(filter(str.isdigit, window.split(\"-\")[1]))))","ab27383c":"df_cat['AGE_PERCENTILE'] = df_cat.AGE_PERCENTIL.apply(lambda x: perc_to_int(x))\ndf_cat['WINDOW'] = df_cat.WINDOW.apply(lambda x: wdw_to_int(x))\n","6b0e785d":"df_cat['AGE_PERCENTILE']=df_cat['AGE_PERCENTILE'].astype(\"float64\")\ndf_cat['WINDOW']=df_cat['WINDOW'].astype(\"float64\")\nprint(df_cat.head())","30eea64e":"df_cat=df_cat.drop(columns=[\"AGE_PERCENTIL\"])\ndf_cat.head()","d58dd827":"def min_max_convert_wdw(x):\n    return (((x-2) *2) \/ 11) -1\ndef min_max_convert_age(x):\n    return (((x-10)*2) \/80) -1\ndf_cat['AGE_PERCENTILE'] = df_cat.AGE_PERCENTILE.apply(lambda x: min_max_convert_age(x))\ndf_cat['WINDOW'] = df_cat.WINDOW.apply(lambda x:min_max_convert_wdw(x))\n    ","fdbcaea4":"idf.drop([\"PATIENT_VISIT_IDENTIFIER\"],1)\nidf = pd.concat([idf,df_cat ], axis=1)\nidf.head()","226e6c81":"\ncor = idf.corr(method=\"pearson\")\ncor_target = abs(cor[\"ICU\"])\nrelevant_features = cor_target[cor_target>0.3]\nrelevant_features.index","8196ffa1":"data = idf[relevant_features.index]","ce675063":"data.shape","8637cde7":"\n\ndata.ICU.value_counts()","3bed0082":"plt.figure(figsize=(15,7))\npercentile = age = sns.countplot(sorted(idf.AGE_PERCENTILE), hue='ICU', data=idf)\nplt.xticks(rotation=40)\nplt.xlabel(\"Age Percentile\")\nplt.ylabel(\"Patient Count\")\nplt.title(\"COVID-19 ICU Admissions by Age Percentile\")\nplt.legend(title = \"ICU Admission\",labels=['Not Admitted', 'Admitted'], loc = 0)","c2c38352":"\n\n\ndata.ICU = data.ICU.astype(int)\n\ndata.head()","e66c6234":"y = data.ICU\nX = data.drop(\"ICU\", 1)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42, shuffle = True)","1f3c9c1d":"X_train.shape","7817bfd9":"y_train","dbdd0fbd":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nimport numpy\nimport numpy as np","a87cc536":"model = Sequential()\n\nmodel.add(Dense(512, input_dim=16,activation='relu'))\n\nmodel.add(Dense(250,activation=\"relu\"))\n\nmodel.add(Dense(124,activation=\"relu\"))\n\nmodel.add(Dense(54,activation=\"relu\"))\n\nmodel.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nmodel.summary()\n\n\n\n","706ef1bd":"history = model.fit(X_train, y_train, epochs=250, batch_size=16, validation_split=0.1)","9e504da2":"f = plt.figure(1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\n#plt.show()\n# summarize history for loss\ng = plt.figure(2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\n#plt.show()\n\n\n# evaluate the model\nscores = model.evaluate(X_test, y_test)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\ny_pred = model.predict(X_test)","cba45e0c":"def mapin(x):\n    if x>0.4:\n        return 1\n    else:\n        return 0\ndef array_for(x):\n    return np.array([mapin(xi) for xi in x])","6f6fc641":"y_pred=array_for(y_pred)","41a514b1":"from sklearn.metrics import auc\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, fmt = 'g', cmap = 'Blues')\n\nprint(classification_report(y_test, y_pred))\nprint(\"AUC = \",roc_auc_score(y_test, y_pred))\n\ny_pred_keras = model.predict(X_test).ravel()\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)\nauc_keras = auc(fpr_keras, tpr_keras)\n\nplt.figure(3)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')","29099009":"So there are only two type of string object and there is no null value in those category . But that is not the case for numeric entry as we can see below. ","ebe5d29f":"Now, onto our logistic regression. We'll need to encode our categorical variable, WINDOW. We'll use get_dummies for this purpose. Also, we'll change AG_ABOVE65 and ICU back to int (they became floats during impution), just to make things look nicer.","da806985":"****OldRange = (OldMax - OldMin)  \nNewRange = (NewMax - NewMin)  \nNewValue = (((OldValue - OldMin) * NewRange) \/ OldRange) + NewMin****","cf85b039":"Now for the logisitic regression. It's actually pretty easy. We'll use the logistic regression model to then predict y values in y_hat, which we'll compare to y_test (the actual values)","ebf20740":"In contrast, there doesn't seem to be a clear relation between age percentile and ICU admission.","a9b3eac4":"These are all the features with a correlation coefficient over 0.2, indicating that each of these variables is at least weakly correlated to ICU Admission. Some of these features are clearly correlated with one another (included are the mean, median, max, and min for a single metric), so it may be a good idea to drop even more of these values. For now, though, let's just see what result we get with all of them. The dataframe we'll be analyzing going forward will consist of these features, plus the categorical WINDOW feature. I observed a positive correlation between WINDOW and ICU in some visual data exploration I conducted, which can be seen immediately below.","25350de1":"# Building a Predictive Model for COVID-19 ICU Admission\n\nWe will be trying to implement a multi-layer perceptron in order to predict the requirement of ICU for A patient. ","e5ff059b":"Now, we create two dataframes, one being the dependent variable, ICU, and the other being all the independent variables. We'll then create a train-test split in order to evaluate the model later.","e5e5e71a":"Next, we'll identify the most important numerical features. We'll use the corr method to identify all the features with at least a weak correlation to the dependent variable (ICU Admission)","22e01f75":"Here we can see that if we drop all the row having atleast one null value, the sample size reduces drastically. So we won't drop the row, we will go for imputation. "}}