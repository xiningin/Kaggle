{"cell_type":{"a59e4373":"code","63921762":"code","c5cf7361":"code","4ef6eb67":"code","93d009e2":"code","a4469c10":"code","4c2ca9b2":"code","c361528a":"code","c95bfd3c":"code","d04a4ae6":"code","0050cd05":"code","cf745d35":"code","23c68cb6":"code","aedaa610":"code","cbe42312":"code","959b0104":"code","5060de6d":"code","6c3c9fc7":"code","d941f4e0":"code","c5c19933":"code","6127ea3f":"code","6d7bd1a0":"code","de142937":"code","6e5b90e8":"markdown","19c6353d":"markdown","56752456":"markdown","a41d30d1":"markdown","c221d6c8":"markdown","67ae3635":"markdown","6e685d33":"markdown","7e909c58":"markdown","5f4543d1":"markdown","1defe3fa":"markdown","00707db4":"markdown","3e683722":"markdown","37830017":"markdown"},"source":{"a59e4373":"import pandas as pd\nimport operator\nimport re\nfrom bs4 import BeautifulSoup\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.externals import joblib\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n\n\n#Load files and datasets\nanswer=pd.read_csv('..\/input\/answers.csv')\nprofessional=pd.read_csv('..\/input\/professionals.csv')\ntags=pd.read_csv('..\/input\/tags.csv')\nquestion_score=pd.read_csv('..\/input\/question_scores.csv')\nanswer_score=pd.read_csv('..\/input\/answer_scores.csv')\nquesti=pd.read_csv('..\/input\/questions.csv')\ntag_question=pd.read_csv('..\/input\/tag_questions.csv')\ntag_user=pd.read_csv('..\/input\/tag_users.csv')","63921762":"questi[questi['questions_body']=='<p>I am a sophomore in Boston and I am not sure what I want to do yet. I really think I want to be a pediatric nurse but I am not sure if I will want to go further into the health and medicine field if I really like it. <\/p>']","c5cf7361":"questi[questi['questions_body'].str.contains('http:\/\/www.typeoflawyer.com\/different-types-of-law-careers\/')]","4ef6eb67":"questi.isnull().sum() ","93d009e2":"#Rename the name of column in 'tag_users.csv' table to be merged with 'tags.csv'\ntag_user=tag_user.rename(index=str, columns={'tag_users_tag_id':'tags_tag_id'})\ntag_professional=tag_user.merge(tags, how='left', on='tags_tag_id')\n\n#Merging the tag_professional for question with 'professionals.csv'\ntag_professional=tag_professional.rename(index=str, columns={'tag_users_user_id':'professionals_id'})\nfinal_professional=professional.merge(tag_professional, how='left', on='professionals_id')\n\n#Rename the name of column in 'Professionals.csv' table to be merged with 'answers.csv'\nfinal_professional=final_professional.rename(index=str, columns={'professionals_id':'answers_author_id'})\nanswer_prof_merged=answer.merge(final_professional, how='left', on='answers_author_id')\n\n#Rename the name of column in 'answer_score.csv' table to be merged with 'answer_prof_merged'\nanswer_score1=answer_score.rename(index=str, columns={'id':'answers_id'})\nanswer_prof_merged=answer_prof_merged.merge(answer_score1, how='left', on='answers_id')\n\n#Rename the name of column in 'question_scores.csv' table to be merged with 'questions.csv'\nquestion_score1=question_score.rename(index=str, columns={'id':'questions_id'})\nquesti=questi.merge(question_score1, how='left', on='questions_id')\n\n#Rename the name of column in 'tag_questions.csv' table to be merged with 'tags.csv'\ntag_question=tag_question.rename(index=str, columns={'tag_questions_question_id':'questions_id'})\ntag1=tags.rename(index=str, columns={'tags_tag_id':'tag_questions_tag_id'})\n\nfinal_tag=tag_question.merge(tag1, how='left', on='tag_questions_tag_id')\n\n\n#Merging the tags for question with 'questions.csv'\nquesti1=questi.merge(final_tag, how='left', on='questions_id')","a4469c10":"import nltk\n\ntemporary1=[]\ntemporary2=[]\nend=[]\nquestion_new_list=[]\n\n#Cleaned Texts from HTML tags, URL, Hashtag, and then translating all of them to english as the default \ndef cleaned_list(file,name):\n    text_=list(str(x) for x in file[name])\n    question_new_list=[]\n    for i in text_:\n        cleaned_str=BeautifulSoup(i)\n        cleaned_text=cleaned_str.get_text()\n        result= re.sub(r\"http\\S+\", \"\", cleaned_text)\n        result0=re.sub(\"(\\\\d|\\\\W)+\",\" \",result)\n        result1=result0.replace('#','')\n        question_new_list.append(result1)\n    return question_new_list\n                   \n#Some of questions are posted as title rather than body, so we merged it both of them as 'merged text'\ndef merged_body_and_title(file):\n    bodies= cleaned_list(file,'questions_body')\n    titles= cleaned_list(file,'questions_title')\n    for i in range(0, len(titles)):\n        nltk_tokens_body = nltk.word_tokenize(bodies[i])\n        nltk_tokens_title = nltk.word_tokenize(titles[i])\n        for f in nltk_tokens_title :\n            temporary1.append(f)\n            t=' '.join(temporary1)\n        for v in nltk_tokens_body :\n            temporary2.append(v)\n            t1=' '.join(temporary2)\n            t3=t+'.'+t1\n        end.append(t3)\n        temporary1.clear()\n        temporary2.clear()\n    return end\n\nquesti1['merged_question']=merged_body_and_title(questi1)\nanswer_prof_merged['merged_question']=cleaned_list(answer_prof_merged,'answers_body')","4c2ca9b2":"non_nan_list=list(str(x) for x in questi1['merged_question'].drop_duplicates())\nanswer_list=list(str(x) for x in answer_prof_merged['answers_body'])\n\ntfidf=TfidfVectorizer(stop_words='english')\nX_idf = tfidf.fit_transform(non_nan_list)","c361528a":"# Sum_of_squared_distances = []\n# K = range(80,200)\n# for k in K:\n#     print(k)\n#     model = KMeans(n_clusters=k)\n#     km=model.fit(X_idf)\n#     Sum_of_squared_distances.append(km.inertia_)","c95bfd3c":"# plt.figure(figsize=(80,80))\n# plt.plot(K, Sum_of_squared_distances, 'bx-')\n# plt.xlabel('k')\n# plt.ylabel('Sum_of_squared_distances')\n# plt.title('Elbow Method For Optimal k')\n# plt.show()","d04a4ae6":"\ntrue_k =138\nmodel = KMeans(n_clusters=true_k, init='k-means++',max_iter=300, n_init=10)\nmodel.fit(X_idf)\n    \nprint(\"Top terms per cluster:\")\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = tfidf.get_feature_names()\n\n# print(order_centroids)\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for t in order_centroids[i, :10]:\n        print(' %s' % terms[t]),\nprint","0050cd05":"import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n \ndef similarity(X_list, y_list):\n    X = tfidf.transform(X_list)\n    y=tfidf.transform(y_list)\n    \n    d=cosine_similarity(X, y)\n    return d\n ","cf745d35":"def labeling(name_list):\n    clustering=[]\n    for i in name_list:\n        Y = tfidf.transform([i])\n        prediction = model.predict(Y)\n        for e in prediction:\n            clustering.append(e)\n    return clustering\n","23c68cb6":"full_list_question=list(str(x) for x in questi1['merged_question'])\nquesti1['label']=labeling(full_list_question)","aedaa610":"answer2=answer_prof_merged.rename(index=str, columns={'answers_question_id':'questions_id'})\n\nanswer_and_question=answer2.merge(questi1,how='left', on='questions_id')\n\nanswer_and_question.head()","cbe42312":"answer_and_question.isnull().sum()","959b0104":"from datetime import datetime\ninterval=[]\nfor i in range (0, len(answer_and_question)):\n    answer_date=datetime.strptime(str(answer_and_question['answers_date_added'][i]),'%Y-%m-%d %H:%M:%S UTC+0000')\n    sent_date=datetime.strptime(str(answer_and_question['questions_date_added'][i]),'%Y-%m-%d %H:%M:%S UTC+0000')\n    diff= (answer_date-sent_date).days\n    interval.append(diff)\nanswer_and_question['interval']=interval","5060de6d":"answer_recommend=answer_and_question.groupby('answers_author_id').agg({'interval':'mean'})\nanswer_final_end=answer_and_question.merge(answer_recommend, how='left', on='answers_author_id')","6c3c9fc7":"answer_recommend.head()","d941f4e0":"def not_answerred_prof(label):\n    label=answer_and_question.loc[answer_and_question['label'].isin(label)]\n    list_tags=list(str(x) for x in label['tags_tag_name_x'].drop_duplicates())\n    for i in list_tags:\n        if i is not np.nan:\n            final=final_professional.loc[final_professional['tags_tag_name'].isin(list_tags)]\n            final1=final[['answers_author_id','tags_tag_name']].drop_duplicates(subset='answers_author_id')\n    return final1","c5c19933":"list_tags=['politics','aviation']\nfinal=final_professional[final_professional['tags_tag_name'].isin(list_tags)]\nfinal.head()","6127ea3f":"\n\ndef find_the_professional(text_list):\n    list_similarity=[]\n    label_list=labeling(text_list)\n    for i in range(0,len(text_list)):\n        df=answer_final_end.loc[answer_final_end['label'].isin(label_list)]\n        sim_score=similarity(text_list,list(str(x) for x in df['merged_question_y']))\n        for b in sim_score:\n            for c in b:\n                list_similarity.append(c)\n        df['similarity_score']=list_similarity\n        df1=df.sort_values(by=['similarity_score'], ascending=False)\n        professional_result=df1[['answers_author_id','interval_y', 'similarity_score', 'score_x']].drop_duplicates(subset='answers_author_id')\n        professional_result=professional_result.rename(columns={'interval_y':'interval mean', 'score_x':'heart'})\n        additional_prof=not_answerred_prof(label_list)\n        print('Label number:',label_list)\n        print('\\033[1m','Top 10 List of Professionals For This Question:',text_list[i],'\\033[1m')\n        return professional_result[0:10], additional_prof[0:10]\n\n\n","6d7bd1a0":"def final_recommendation(text_list):\n    a,b=find_the_professional(text_list)\n    display(a)\n    print('\\033[1m','--------------------------------------------------------------------------------------------------------------','\\033[1m')\n    print('\\033[1m','Top 10 Additional Professionals:','\\033[1m')\n    display(b)","de142937":"final_recommendation(['how to get scholarship ?'])","6e5b90e8":"# 7. Presenting The Reccomended Professional\nUsing its label to filter specific professional and sorted it based on 'similarity_score' to present its final result. We present The Top 10 recommended professionals to answer.","19c6353d":"# 4. KMeans Process and TFIDF\n\nTo get the optimal cluster, we need to remove all duplicate question text first in non_nan_list as the input. Then we can process it.","56752456":"# 1. Introduction & EDA\n\nIn this module to find the recommended professionals to answer some questions, we will use 'questions.csv','answers.csv', 'professionals.csv', 'answers_score.csv', 'questions_score.csv' and 'tags.csv' files.\n\nDuring the Exploratory Data Analysis (EDA) of these files, in 'questions.csv' file, we found some question having HTML tags within its text body. Hashtag also attached as well. We also can find some of the questions are placed in questions_title rather than in questions_body column and vice versa. Some of the questions also having URL links within its text.\n\n\n","a41d30d1":"Now, we can merge answer dataframe with questi1 dataframe that has been labeled in \"answer_and_question\". ","c221d6c8":"The result shows that the 'elbow' is created between 100 to 150 number of clusters. So, we will take  138 as the optimal k for our KMeans's input.","67ae3635":"We also check the similarity between the question text as the input with its question in \"questi1\" to support the accuracy of KMeans clustering using \"similarity\" function.","6e685d33":"# 3. Data Processing\n\nFor clustering, we will use KMeans to get the main top words per cluster. This clustering will be labeled on each question in 'questi1' table. But before that, we need to find the optimal number of the cluster as the input for our KMeans. That's why we will do \"Elbow Method\" to find optimal k.\n\nFor the experiment, let's take range from 80 to 200 number of clusters.","7e909c58":"After the KMeans clustering, now we can labeling all question in 'questi1' dataframe.","5f4543d1":"# 5. Finding How Fast The Response Comes From Professional\n\nIn order to get this data, we need to get interval time between the date of question first posted and the date of that questions answered.","1defe3fa":"# 6. Additional: How to get reccomended professional who hasn't yet answered any question?\nSometimes, even the professional in 'professionals.csv' hasn't been answered any question, we need to send them a relevant question to open more chance for that question to be answered. In order to do this, we will get any tags name that related with 'label' column. Based on those tags, we can search for other professional in 'professional.csv'.","00707db4":"After we got the interval days of response, we can find mean of response time for each professional using aggregate() and groupby() function. The smaller mean indicates faster that professional in answering a question.","3e683722":"# 2. Pre-Processing Data\n\nBefore clustering and labeling process, we need to merge several tables into 'questions.csv'\n","37830017":"After that, we need to clean up every question text from HTML tags, hashtag using \"cleaned_list\" function. We also need to remove all URL links. The main purpose of this process is to provide good quality text data before continuing to tokenization and TFIDF vectorizer.\n\n\nIn \"merged_body_and_title\" function, it will merge between question text body and question text title together in \"merged_question\" column, to avoid any question body that is misplaced in title column and vice versa."}}