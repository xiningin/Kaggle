{"cell_type":{"7f49b0e9":"code","8daf02d6":"code","e9ef0001":"code","5f9cc000":"code","11edf9f2":"code","506a3d04":"code","d4bd8c7e":"markdown","1270123c":"markdown","878d001d":"markdown","cb92b56b":"markdown","68527ced":"markdown","1d99a61d":"markdown"},"source":{"7f49b0e9":"from matplotlib.pyplot import figure,bar,xlabel,ylabel,legend,rc, plot,savefig, title, scatter, show\nfrom numpy             import argsort, exp, zeros, int32, sum, sqrt, log, argmin, mean, std\nfrom numpy.random      import rand\nfrom os                import walk\nfrom os.path           import join\nfrom pandas            import read_csv\nfrom random            import random, randrange, gauss,sample\nfrom time              import time\n","8daf02d6":"N_TRIALS       = 25\nN_CONTESTS     = 20 # Minnimum number of contests for each contestant\nN_MAX          = 150 # Limit number of contestants sampled - set to None for no limit\nMAX_ITERATIONS = 1000 # Used to limit iteations while computing BT\nFREQUENCY      = 5  # For work in progress plots\nPLOT_FILE      = 'bt-iterations'\nEPSILON        = 1e-6 # Controls iterations\n\n","e9ef0001":"train_data    = None\ndf_colours    = None\nxkcd_colours  = None\nfor dirname, _, filenames in walk('\/kaggle\/input'):\n    for filename in filenames:\n        path_name = join(dirname, filename)\n        if filename.startswith('train'):\n            train_data = read_csv(path_name)\n        if filename.startswith('colors'):\n            df_colours = read_csv(path_name)\n            xkcd_colours = df_colours.XKCD_COLORS.dropna()\n            \n\n                       ","5f9cc000":"def count_wins(Lambdas):\n    Wins = zeros((N_MAX,N_MAX))\n    for i in range(N_MAX):\n        for j in range(N_CONTESTS):\n            k = (i+randrange(1,N_MAX)) % N_MAX\n            if random() < Lambdas[i]\/(Lambdas[i] + Lambdas[k]):\n                Wins[i,k] += 1\n            else:\n                Wins[k,i] += 1\n    return Wins","11edf9f2":"def update(p,w_symmetric,W,N):\n    p1 = zeros(N)\n    for i in range(N):\n        Divisor = 0\n        for j in range(N):\n            if i!=j and p[i]+p[j]>0:\n                Divisor += w_symmetric[i,j]\/(p[i]+p[j])\n        p1[i] = W[i]\/Divisor\n    return p1\/sum(p1)\n\n\ndef normalize(p):\n    return p \/ sum(p)","506a3d04":"Targets        = train_data.target.to_numpy()\nSEs            = train_data.standard_error.to_numpy()\nif N_MAX == None:\n    N_MAX = len(Targets) \nelif N_MAX<len(Targets):\n    Indices = sample(list(range(len(Targets))),N_MAX)\n    Targets = [Targets[i] for i in Indices]\n    SEs     = [SEs[i] for i in Indices]\nelse:\n    N_MAX = len(Targets)\n    \nBetas          = sorted(Targets)\n\n# Find an index such that Beta[index] is as close to zero as possible.\n# We will chosse that as \"the\" zero of Beta\nindex_min_beta = argmin([abs(b) for b in Betas])\n\nLambdas        = exp(Betas)\nN,_            = train_data.shape\n\nfig            = figure(figsize=(10,10))\n\nplot(range(N_MAX),[b - Betas[index_min_beta] for b in Betas],label     = r'$\\beta$')\nplot(range(N_MAX), SEs, label='Standard Error')\nstart  = time()\nScores = zeros((N_MAX,N_TRIALS))\nfor trial in range(N_TRIALS):\n    w              = count_wins( Lambdas)\n    w_symmetric    = w + w.transpose() # Symmetrize w\n    W              = sum(w,axis=1)  # Number won by i\n    Ps             = normalize(rand(N_MAX))\n\n    for k in range(MAX_ITERATIONS):\n        p1 = update(Ps,w_symmetric,W,N_MAX)\n        if (abs(Ps-p1)<EPSILON*p1).all():\n            Ps = p1\n            break\n        Ps  = p1\n        \n    Beta_Calculated = log(Ps)\n    # Work out offset to make this curve match original Betas at Beta==0\n    offset = Beta_Calculated[index_min_beta]-Betas[index_min_beta]\n    # Shift curve to make it match original Betas at Beta==0\n    Beta_Shifted = [l - offset for l in Beta_Calculated]\n    Scores[:,trial] = Beta_Shifted\n    if trial%FREQUENCY==0:\n        plot(range(N_MAX),Beta_Shifted,  linestyle = ':',color=xkcd_colours[trial%len(xkcd_colours)])\n    if trial%10==0:\n        print (f'Trial {trial}')\n\nmu    = mean(Scores,axis=1)\nsigma = std(Scores,axis=1)\nplot (mu,    label = r'mean $\\forall$ trials')\nplot (sigma, label = r'$\\sigma$')\nlegend()\nxlabel('index')\nylabel('p')\nelapsed = int(time() - start)\ntitle(f'{N_MAX} Contestants, {N_CONTESTS} contests. Time = {elapsed} seconds, eps={EPSILON}, k={k}')\n\n\nfig.savefig(f'{PLOT_FILE}')\n\nshow()","d4bd8c7e":"# Main calculation\n\n1. Plot the $\\beta_i$, which reprsenet the ground truth.\n1. Product a number of graphs, each estimating the $\\beta_i$ from a set of contests.\n1. Since $\\beta_i$ is modulo an additive constant, shift each plot so its zero matches the zero of $\\beta_i$.","1270123c":"# Load Data\n\n## Data Dictionary\n\n|Train|Public Test|Hidden Test|Description|\n|--------------|--------------|----------|----------------------------------------------------|\n|id|id|id|Unique ID for excerpt|\n|url_legal|url_legal|- |URL of source (Omitted from some records in the test set--see [note](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/238670#1306025))|\n|license|license |-|License of source material (Omitted from some records in the test set--see [note](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/238670#1306025))|\n|excerpt|excerpt|excerpt|Text for predicting readability|\n|target|-|-|Readability|\n|standard_error|-|-|Measure of spread of scores among multiple raters for each excerpt|","878d001d":"# Bradley-Terry\n## Purpose\nI created this notebook to gain a better understanding of the [Bradley-Terry Model](https:\/\/en.wikipedia.org\/wiki\/Bradley%E2%80%93Terry_model), which has been [used to allocate scores](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240423) for the [Common Lit Readability Prize](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize).\nIn particular I want to know:\n1. Can we reconstruct the target scores from the results of \"contests\" between documents?\n1. Is the reconstruction consistent? How much does it depend on the way the contestants have been sampled to build contests?\n1. Can I explain the standard error?\n\n\n## Bradley-Terry\n\n[1] and [4] give two forms of the equation.\n$$\\begin{align}P(\\text{i beats j}) =& \\frac{e^{\\beta_i}}{e^{\\beta_i}+e^{\\beta_j}}\\\\\nlogit(P(\\text{i beats j})) =& \\lambda_i - \\lambda_j \\text{, where }\\\\\n\\lambda_i =& e^{\\beta_i} \\text{ and }\\\\\nlogit(p) =& \\log\\big(\\frac{p}{1-p}\\big)\\end{align}$$\n\n|Question|Outcome|\n|---------------------------------------|-----------------------------------------------------------|\n|Since some of the target scores in the context are negative, I conjecture that they correspond to the $\\beta_i$||\n|Target scores in contest are approximately Gaussian.What does this say about probabilities of contests?||\n\n## Methods\n\nThis code contained in this notebook sets up contests between pairs of texts in the training dataset. The opponents for each text are selected at random, and the outcome of each context is chosen at random, with the probability of extract $i$ being deemed easier to read than extract $j$ given by $\\frac{e^{\\beta_i}}{e^{\\beta_i}+e^{\\beta_j}}$. \n\n## Results\n\nThe plot shows the spread of computed $\\beta_i$ versus the supplied values. The solid lines represent actual data or statistics calculated by this notebook; dotted lines represent selected runs of the analysis. ![](.\/bt-iterations.png)\n\nIt shows wide variability, but the mean of the computed values does match those supplied. Moreover the standard deviation, ($\\sigma$) matches the standard deviation.\n\nI have limited the number of samples from the data because the calculation is a bit slow. I plan to test overnight with the full dataset.\n\n## Conclusions\n\n1. Reconstruction is possible, but it mat be too slow to be practicable.\n\n## Further Work\n\n1. Quantify variability to see whether it explains the standard error.\n1. Does thae variability constain the accuracy that can be obtained in the contest.\n\n## References\n||Title|Author|\n|---|-----------------------------------------------|-----------------------------|\n|[1]|[Bradley-Terry Model](https:\/\/en.wikipedia.org\/wiki\/Bradley%E2%80%93Terry_model)|Wikipedia Editors|\n|[2]|[Log5, the Logit Link and Bradley-Terry](https:\/\/www.kaggle.com\/jaredcross\/log5-logistic-regression-and-bradley-te\/comments)|Jared Cross|\n|[3]|[Target scores](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240423)|Scott Crossley|\n|[4]|[Bradley-Terry Models in R](https:\/\/www.jstatsoft.org\/article\/view\/v012i01\/v12i01.pdf)|David Firth|\n|[5]|[Common Lit Readability Prize](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize)|Scott Crossley|\n|[6]|[Efficient Bayesian Inference for Generalized Bradley-Terry Models](https:\/\/arxiv.org\/abs\/1011.1761)|Francois Caron and Arnaud Doucet|\n","cb92b56b":"# Hyper parameters","68527ced":"## Update probabilities\n\nThe algorithm in [1] involves iterating the following equations until convergence has been achieved:\n\n$$\\begin{align} p^\\prime_i =& \\frac{W_i}{\\sum_{i \\ne j} \\frac{W_{ij}+W_{ji}}{p_i+p_j}} \\text{ and}\\\\\np_i =& \\frac{p^\\prime_i}{\\sum p_j}  \\end{align} $$ \n\nThe $p_i$ correspond to $\\lambda_i$.","1d99a61d":"# Estimate Parameters\n\nThis is the algorithm from [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Bradley%E2%80%93Terry_model#Estimating_the_parameters). We will plot progress to convergence, and\ncompare result with initial values.\n\n## First we need to compute the number of wins for i competing with k\n\n1. We stated above that the probability if _i_ beating _k_ is given by the equation $P(\\text{i beats k}) = \\frac{e^{\\beta_i}}{e^{\\beta_i}+e^{\\beta_k}}$.\n2. For each _i_ we draw a number of contestants, and we randomly assign a winner to each contest using the appropriate probabilty.\n"}}