{"cell_type":{"0cd8dfbe":"code","e5f1723a":"code","7e65794d":"code","e5f99603":"code","4e5872ef":"code","acc1991a":"code","3a5c5b33":"code","d68c53d5":"code","6cd3504a":"code","9bf87867":"code","5a7e6ebf":"code","05231a5e":"code","519be695":"code","069327ee":"code","47444bb0":"code","30d72e6b":"code","0417a38d":"code","5d2f1852":"code","88fac790":"code","fcdb1ef8":"code","5e1ba60d":"code","990da6e0":"code","009d835a":"code","f5ce587a":"code","64ef95a8":"code","6d1f05d3":"code","6cb3dc6e":"code","e82864de":"code","62a7bb6e":"code","d3b8dcbb":"code","726980e9":"code","e1f5a0a3":"code","3c96da2e":"code","b9ad6f74":"code","05c75a45":"code","3b46d579":"code","14102f9a":"code","3885e3ee":"code","60f6d9dc":"code","73180723":"code","0a05c958":"code","8bae2918":"code","bc05912d":"markdown","7cd5b73b":"markdown","958e3e99":"markdown","2a953fa3":"markdown","31c5e538":"markdown","62082062":"markdown","9e9e9403":"markdown","567f86e4":"markdown","7ba210f8":"markdown","30b0a709":"markdown","54b38b9d":"markdown","f567aeac":"markdown","a877153b":"markdown","5aa73720":"markdown","6db7c7aa":"markdown"},"source":{"0cd8dfbe":"# import useful libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","e5f1723a":"# import train dataset\ndf_train = pd.read_csv(\"..\/input\/malignant-comments\/Malignant_train.csv\")\ndf_train.head()","7e65794d":"# import test dataset\ndf_test = pd.read_csv(\"..\/input\/malignant-comments\/Malignant_test.csv\")\ndf_test.head()","e5f99603":"# check shape of the train and test dataset\nprint(df_train.shape)\nprint(df_test.shape)","4e5872ef":"# check information of train data\nprint(df_train.info())\n\n# check information of test data\nprint(df_test.info())","acc1991a":"# check null values of train data using heatmap\nsns.heatmap(df_train.isnull())","3a5c5b33":"# check discriptive statistics of the train dataset\ndf_train.describe(include='all')","d68c53d5":"# check correlation of numerical features using heatmap\nsns.heatmap(df_train.corr(), annot=True)","6cd3504a":"# check count plot of all target features. \ncolumn = ['malignant','highly_malignant','loathe','rude','abuse','threat']\nfor i in column:\n    print(i)\n    print('\\n')\n    print(df_train[i].value_counts())\n    sns.countplot(df_train[i])\n    plt.show()","9bf87867":"# create a label feature, which is combination of all target columns.\nall_labels = ['malignant','highly_malignant','rude','threat','abuse','loathe']\ndf_train['Label'] = df_train[all_labels].sum(axis=1)","5a7e6ebf":"df_train.head(8)","05231a5e":"# plot label column count\nplt.figure(figsize=(9,5))\nsns.countplot(df_train['Label'])\nplt.title(\"Label Count\",fontsize=20)\nplt.show()","519be695":"# Here, we convert label column in form of 0 and 1 (scaling).\n# 0 = good comments and 1 = bad comments\ndf_train['Label'] = df_train['Label']>0\ndf_train['Label'] = df_train['Label'].astype(int)","069327ee":"df_train.head(8)","47444bb0":"# Here, we plot our label column\nsns.countplot(df_train['Label'])\nplt.show()\n\ndf_train['Label'].value_counts()","30d72e6b":"# Now, we plot wordcloud of malignant comments and see which type word is most used in malignant comments.\nfrom wordcloud import WordCloud\nhams = df_train['comment_text'][df_train['malignant']==1]\nspam_cloud = WordCloud(width=750,height=500,background_color='black',max_words=45).generate(' '.join(hams))\nplt.figure(figsize=(10,8),facecolor='k')\nplt.imshow(spam_cloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","0417a38d":"# Now, we plot wordcloud of abuse comments and see which type word is most used in abuse comments.\nhams = df_train['comment_text'][df_train['abuse']==1]\nspam_cloud = WordCloud(width=750,height=500,background_color='black',max_words=45).generate(' '.join(hams))\nplt.figure(figsize=(10,8),facecolor='k')\nplt.imshow(spam_cloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","5d2f1852":"# import useful libraries\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE()","88fac790":"# calculating comments length\ndf_train[\"comment_length\"] = df_train[\"comment_text\"].str.len()","fcdb1ef8":"# Convert all comments text into lower case\ndf_train['Cleaned_comment_text'] = df_train['comment_text'].str.lower()\ndf_test['cleaned_comment_text'] = df_test['comment_text'].str.lower()","5e1ba60d":"# remove punctuation from cleaned comment text column\ndf_train['Cleaned_comment_text'] = df_train['Cleaned_comment_text'].str.replace('[^\\w\\s]','')\ndf_test['cleaned_comment_text'] = df_test['cleaned_comment_text'].str.replace('[^\\w\\s]','')","990da6e0":"# removing stopwords from cleaned comment text column\ndf_train['Cleaned_comment_text'] = df_train['Cleaned_comment_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf_test['cleaned_comment_text'] = df_test['cleaned_comment_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf_train.head()","009d835a":"# Now, let's remove digits from the cleaned comment text column\ndf_train['Cleaned_comment_text'] = df_train['Cleaned_comment_text'].str.replace('\\d+', '')\ndf_test['cleaned_comment_text'] = df_test['cleaned_comment_text'].str.replace('\\d+', '')","f5ce587a":"# Here, we use Lemmatizing. Lemmatization is the process of converting a word to its base form.\ndf_train['Cleaned_comment_text'] = df_train['Cleaned_comment_text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(x) for x in x.split()))\ndf_test['cleaned_comment_text'] = df_test['cleaned_comment_text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(x) for x in x.split()))\ndf_train.head()","64ef95a8":"# calculating cleaned comments length\ndf_train[\"Cleaned_comment_length\"] = df_train[\"Cleaned_comment_text\"].str.len()\ndf_train.head()","6d1f05d3":"# total reduced length\nprint(\"Original Length: \",df_train['comment_length'].sum())\nprint(\"Cleaned Length: \",df_train['Cleaned_comment_length'].sum())","6cb3dc6e":"# convert text into vectors using TF-IDF\ntf_vec = TfidfVectorizer(max_features=8000, stop_words='english')\nfeature = tf_vec.fit_transform(df_train['Cleaned_comment_text'])\n\n# split the target column.\n# here, our target column is label and it is a classification problem.\nx = feature\ny = df_train['Label']","e82864de":"# convert test data's text into vectors using TF-IDF\ntf_vec = TfidfVectorizer(max_features=8000, stop_words='english')\nfeature_test = tf_vec.fit_transform(df_test['cleaned_comment_text'])","62a7bb6e":"# Hear, we use SMOTE(resampling) method to cop up with imbalanced classification of target variable.\n# SMOTE(Synthetic Minority Oversampling Technique) algorithm generates synthetic samples of minority class.\nx_smote,y_smote = smote.fit_resample(x,y)\n\n# check shape of the train dataset before oversampling\nprint(x.shape)\nprint(y.shape)\n\n# check shape of the train dataset after oversampling \nprint(x_smote.shape)\nprint(y_smote.shape)","d3b8dcbb":"# check the counts of original target column\ny.value_counts()","726980e9":"# check the count of target column after oversampling \ny_smote.value_counts()","e1f5a0a3":"# split train and test data\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_smote,y_smote,test_size=0.20,random_state=0)","3c96da2e":"# our problem is classification type of problem.\n# import useful libraries for machine learning algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nmodel = [LogisticRegression(solver='liblinear'),DecisionTreeClassifier(),MultinomialNB()]\n\nfor m in model:\n    m.fit(x_train,y_train)\n    train = m.score(x_train,y_train)\n    predm = m.predict(x_test)\n    print(\"Accuracy of\",m,\"is:\")\n    print(\"Accuracy of training model is:\",train)\n    print(\"Accuracy Score:\",accuracy_score(y_test,predm))\n    print(\"Confusion matrix:\",\"\\n\",confusion_matrix(y_test,predm))\n    print(\"Classification report:\",\"\\n\",classification_report(y_test,predm))\n    print(\"************************************************************\")\n    print(\"\\n\")","b9ad6f74":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train,y_train)\ntrain = rf.score(x_train,y_train)\npred_rf=rf.predict(x_test)\nprint(\"Accuracy of training model is:\",train)\nprint(\"Accuracy Score:\",accuracy_score(y_test,pred_rf))\nprint(\"Confusion matrix:\",\"\\n\",confusion_matrix(y_test,pred_rf))\nprint(\"Classification report:\",\"\\n\",classification_report(y_test,pred_rf))","05c75a45":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\nparameters = {'learning_rate':[0.01,0.1]}\nclf = GridSearchCV(gbc,parameters)\nclf.fit(x_train,y_train)\nprint(clf.best_params_)","3b46d579":"gbc = GradientBoostingClassifier(learning_rate=0.1)\ngbc.fit(x_train,y_train)\ntrain = gbc.score(x_train,y_train)\npredgbc = gbc.predict(x_test)\nprint(\"Accuracy of training model is:\",train)\nprint(\"Accuracy Score:\",accuracy_score(y_test,predgbc)*100)\nprint(\"Confusion matrix:\",\"\\n\",confusion_matrix(y_test,predgbc))\nprint(\"Classification report:\",\"\\n\",classification_report(y_test,predgbc))","14102f9a":"from xgboost import XGBClassifier\nxg = XGBClassifier()\nxg.fit(x_train,y_train)\ntrain = xg.score(x_train,y_train)\npredxg = xg.predict(x_test)\nprint(\"Accuracy of training model is:\",train)\nprint(\"Accuracy Score:\",accuracy_score(y_test,predxg))\nprint(\"Confusion matrix:\",\"\\n\",confusion_matrix(y_test,predxg))\nprint(\"Classification report:\",\"\\n\",classification_report(y_test,predxg))","3885e3ee":"from catboost import CatBoostClassifier\ncb = CatBoostClassifier()\ncb.fit(x_train,y_train)\ntrain = cb.score(x_train,y_train)\npredcb = cb.predict(x_test)\nprint(\"Accuracy of training model is:\",train)\nprint(\"Accuracy Score:\",accuracy_score(y_test,predcb))\nprint(\"Confusion matrix:\",\"\\n\",confusion_matrix(y_test,predcb))\nprint(\"Classification report:\",\"\\n\",classification_report(y_test,predcb))  ","60f6d9dc":"# check auc_roc curve and auc score of best model\nfrom sklearn.metrics import roc_curve,auc\nfpr,tpr,thresholds = roc_curve(pred_rf,y_test)\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nplt.plot(fpr,tpr,color=\"orange\", lw=3, label=(\"ROC curve (area = %0.2f)\" % roc_auc))\nplt.plot([0,1],[0,1],color = \"navy\",lw=3,linestyle=\"--\")\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"RandomForest Classifier\")\nplt.legend(loc = \"lower right\")\nplt.show()","73180723":"#save best result\ndf1 = pd.DataFrame(pred_rf)\ndf1.to_csv(\"rf_malignant.csv\")\n#save best model\nimport joblib\njoblib.dump(rf,\"rf_malignant.obj\")","0a05c958":"# check our test dataset with best model\ntest_dataset = rf.predict(feature_test)\nprint(test_dataset)","8bae2918":"#save test dataset result\ndf2 = pd.DataFrame(test_dataset)\ndf2.to_csv(\"rf_malignant_test.csv\")","bc05912d":"## Find Best Model","7cd5b73b":"In our train dataset we have 159571 rows and 8 features, while in test dataset 153164 rows and 2 features are present.","958e3e99":"From the above count plot we can see that, even after merge all target columns, it is still imbalanced. So, to solve this issue we use oversampling method in further process. ","2a953fa3":"1. We use some algorithms and we find randomforest classifier as best model. It gives 99% training model accuracy and 97% testing accuracy. Randomforest classifier also gives good precision and recall score along with f1 score.\n2. Here, we don't use hyperparameter tuning because it takes too much time as well as some algorithm also gives memory error.","31c5e538":"## Bagging and Boosting methods","62082062":"We don't have any null value in our train and test dataset. ","9e9e9403":"Above all count plot is our target feature and we can see that there is imbalanced classification and this issue affect our final result. So, we will use oversampling method in further process.","567f86e4":"1. From the above table, we can see that there no duplicate data present in comment_text column.\n2. All numerical columns have only two values i.e. 0 and 1.","7ba210f8":"## Data Preparation","30b0a709":"From the above heatmap, we can clearly see that there is no null value found in our dataset.","54b38b9d":"## Data Analysis","f567aeac":"From the above plot we can see that, we get best area under the curve for randomforest classifier, which is 98%.","a877153b":"## Data Reading and Understanding","5aa73720":"# Malignant Comments Project","6db7c7aa":"From the above plot we can say that, most of the comments are good and very less numbers of comments is bad.(0=good comments and others are bad comments.)"}}