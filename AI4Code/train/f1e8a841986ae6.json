{"cell_type":{"b5088907":"code","01aa6d99":"code","57c00253":"code","7043af0d":"code","eb9eaf40":"code","251a3414":"code","f6506058":"code","2a07cd7f":"code","aefa2af1":"code","4234b64d":"code","509a4110":"code","9ef55319":"code","c5229ba0":"code","0490a9e2":"code","db79ce1f":"code","757d3fc4":"code","96d15b9f":"code","9d9eae95":"code","dbcc9aa4":"code","d957fd27":"code","cbb4bc25":"code","c20f38dc":"code","c4dfc8d7":"code","8cfcb9c5":"code","6f81d028":"code","ce587d2a":"code","36d1ff42":"code","944facbf":"code","92185ec8":"code","3a7ba55c":"code","ff081551":"code","15b38163":"code","54c236d9":"code","8a47a36e":"code","adf982f3":"code","7e7b6069":"code","0d075678":"code","3222bf0b":"markdown","1ea1b552":"markdown","0fa80ebf":"markdown","8a1d92a0":"markdown","dd58344a":"markdown","53e2f9ad":"markdown","dedd5739":"markdown","94305a50":"markdown","cae98f5f":"markdown","a53501a5":"markdown"},"source":{"b5088907":"#!pip install albumentations\nimport albumentations","01aa6d99":"import torch\nfrom albumentations import ( Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, IAAAdditiveGaussianNoise, Transpose, ToGray )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport matplotlib.pyplot as plt\n\nseed = 42\n\nimport pandas as pd\nimport os\nimport cv2\nfrom torch.utils.data import Dataset,DataLoader\nfrom tqdm import tqdm","57c00253":"class Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\timage = image*(1\/255)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","7043af0d":"train_path = '..\/input\/ranzcr-clip-catheter-line-classification\/train'\ntrain_files = os.listdir(train_path)\n\ntrain_df = pd.read_csv('..\/input\/ranzcr-clip-catheter-line-classification\/train.csv')\n\ntrain = train_df.reset_index(drop=True) # reset index on both dataframes\n\nprint(train.shape)\n\nnum_channel = 3\n\nimg_size = 255","eb9eaf40":"mean_list = [[ 0.4914168 , 0.4914168 , 0.4914168] , [0.485,0.456,0.406] , [0.9,0.9,0.9] , 0.496 ]\nstd_list = [[0.407278, 0.407278 , 0.407278] , [0.229,0.224,0.225] , [0.9,0.9,0.9] , 0.407278]\n\nmean = mean_list[0]\nstd=  std_list[0]\n\nnorm_255 = False #False\n\n\n#Not needed. Instead pass value of max_pixel value.\n\nif norm_255 == True:\n\tfor x in range(0,3):\n\t\tmean[x] = mean[x]*255\n\t\tstd[x] = std[x]*255\n\n\nprint(mean , std)\n\n","251a3414":"recale_image_by = 1\nmax_pixel_value = 255 \n\n\nclass Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\timage = image*(1\/recale_image_by)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels\n    \n    \ndef Show_Xrays(augmentation):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 1 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(8, 8)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(1,1, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            print(data[i])\n\nfor i in range(0,4):\n    mean = mean_list[i]\n    std = std_list[i]\n    print(mean , std)\n    train_augs = albumentations.Compose([ albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                         albumentations.Normalize(mean= mean ,std= std , max_pixel_value=max_pixel_value),])\n\n    Show_Xrays(train_augs)","f6506058":"recale_image_by = 255\nmax_pixel_value = 1 \n\n\nclass Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\timage = image*(1\/recale_image_by)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels\n    \n    \ndef Show_Xrays(augmentation):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 1 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(8, 8)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(1,1, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            print(data[i])\n\nfor i in range(0,4):\n    mean = mean_list[i]\n    std = std_list[i]\n    print(mean , std)\n    train_augs = albumentations.Compose([ albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                         albumentations.Normalize(mean= mean ,std= std , max_pixel_value=max_pixel_value),])\n\n    Show_Xrays(train_augs)","2a07cd7f":"class Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\t#image = image*(1\/recale_image_by)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","aefa2af1":"\ndef Show_Xrays(augmentation):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 9 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(25, 25)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(3,3, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            ","4234b64d":"train_augs = albumentations.Compose([ albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                     #albumentations.Normalize(mean= mean ,std= std ,),\n                                    ])\n\nShow_Xrays(train_augs)","509a4110":"train_augs = albumentations.Compose([ albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                     albumentations.Normalize(mean= mean ,std= std ,p = 1),\n                                    ])\n\nShow_Xrays(train_augs)","9ef55319":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7),                            \n                                    ])\nShow_Xrays(train_augs)","c5229ba0":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7), \n                                        albumentations.CLAHE(clip_limit=(1,10), p= 1),\n                                        #albumentations.Normalize(mean= mean ,std= std ,)\n                                    ])\nShow_Xrays(train_augs)","0490a9e2":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7), \n                                        albumentations.CLAHE(clip_limit=(1,10), p= 1),\n                                        albumentations.Normalize(mean= mean ,std= std ,)\n                                    ])\nShow_Xrays(train_augs)","db79ce1f":"train_augs = albumentations.Compose([   albumentations.Normalize(mean= mean ,std= std ,) , \n                                     albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7), \n                                        #albumentations.CLAHE(clip_limit=(1,10), p= 1)\n                                        \n                                    ])\nShow_Xrays(train_augs)","757d3fc4":"ShiftScaleRotate(always_apply=False, p=1, shift_limit_x=(-0.0625, 0.0625),\n                 shift_limit_y=(-0.0625, 0.0625), scale_limit=(-0.09999999999999998, 0.10000000000000009),\n                 rotate_limit=(-45, 45), interpolation=1, border_mode=4, value=None, mask_value=None)","96d15b9f":"train_augs = albumentations.Compose([albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                #albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.3), lightness=(0.5, 0.7), p=1),\n                #albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=5, p= 0.5),\n\t\t\t\t#albumentations.Normalize(mean= mean ,  std= std ,) \n               ])\n\nShow_Xrays(train_augs)\n\n","9d9eae95":"train_augs = albumentations.Compose([albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\t#albumentations.ShiftScaleRotate(shift_limit_x=(-0.0125, 0.0125),shift_limit_y=(-0.0125, 0.0125) ,rotate_limit=(-15, 15) , p=1),\n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                ########albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\t#####albumentations.ElasticTransform(alpha=3),\n                #albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                #albumentations.MotionBlur(p=1),\n                #albumentations.MedianBlur(p=1),\n                #albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                ###albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                #albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                #albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=5, p= 0.5),\n\t\t\t\t#albumentations.Normalize(mean= mean ,  std= std ,) \n               ])\n\nShow_Xrays(train_augs)\n\n","dbcc9aa4":"mean = [0.485,0.456,0.406]\nstd = [0.229,0.224,0.225]\n\ntransform = albumentations.Compose([  albumentations.RandomCrop(width=256, height=256),\n    albumentations.Normalize(mean= mean ,  std= std ,) \n])\n\n# Read an image with OpenCV and convert it to the RGB colorspace\nimage = cv2.imread(\"..\/input\/ranzcr-clip-catheter-line-classification\/test\/1.2.826.0.1.3680043.8.498.10003659706701445041816900371598078663.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Augment an image\ntransformed = transform(image=image)\ntransformed_image = transformed[\"image\"]\n\n\nfig = plt.figure()\nfig.set_size_inches(15, 15)\n\nax = plt.subplot(1,1, 1)\nplt.tight_layout()\nax.axis('off')\nplt.imshow(image)","d957fd27":"reference_images = ","cbb4bc25":"augment_list = [albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\talbumentations.ShiftScaleRotate(p=1),\n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\talbumentations.ElasticTransform(alpha=3),\n                albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                albumentations.MotionBlur(p=1),\n                albumentations.MedianBlur(p=1),\n                albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=10, p= 0.5),\n\t\t\t\talbumentations.Normalize(mean= mean ,  std= std ,) \n               ]","c20f38dc":"augment_list = [  albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\t  ########albumentations.HorizontalFlip(p=1),\n\t\t\t\t  albumentations.ShiftScaleRotate(p=1),\n\t\t\t\t   albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\t   albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\t   albumentations.CLAHE(clip_limit=(1,4), p=1),\n                   albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\t   #albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n\t\t\t\t   albumentations.ElasticTransform(alpha=3),\n                 albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                 #albumentations.GaussianBlur(p=1),\n                 albumentations.MotionBlur(p=1),\n                albumentations.MedianBlur(p=1),\n                albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                #albumentations.imgaug.transforms.IAASuperpixels(p_replace=0.1, n_segments=100, p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                ######albumentations.augmentations.domain_adaptation.HistogramMatching (reference_images , blend_ratio=(0.5, 1.0), p=0.5),\n                #albumentations.augmentations.transforms.ToSepia(p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                #albumentations.imgaug.transforms.IAAAdditiveGaussianNoise(loc=0, scale=(2.5500000000000003, 12.75), per_channel=False, p=1),\n                \n                albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                #albumentations.augmentations.transforms.Solarize(threshold=128, p=1),\n                #albumentations.augmentations.transforms.RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, alpha_coef=0.08, p=1),\n                #albumentations.augmentations.transforms.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), blur_value=7, brightness_coefficient=0.7, rain_type=None, p=1),\n\t\t\t\t#albumentations.Cutout(max_h_size=int(img_size * 0.1), max_w_size=int(img_size * 0.1), num_holes=5, p=1),\n\t\t\t\talbumentations.Normalize(mean= mean ,  std= std ,) \n               ]","c4dfc8d7":"def Show_save_Xrays(augmentation , name):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 1 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(25, 25)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(1 ,1, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            fig.savefig( str(name) + '.png', dpi=200)","8cfcb9c5":"Save_to_folder = False\n\n\nif Save_to_folder == True:\n    for i in range(len(augment_list)):\n        train_augs = albumentations.Compose([ albumentations.Resize(img_size, img_size) , augment_list[i]])\n        Show_save_Xrays(train_augs , augment_list[i])","6f81d028":"!zip -r -q 'Albumentation_1*1.zip'  .\/","ce587d2a":"!rm .\/*.zip","36d1ff42":"!rm .\/*.png","944facbf":"def get_mean_std(loader):\n    # var[X] = E[X**2] - E[X]**2\n    channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n\n    for data, _ in tqdm(loader):\n        #channels_sum += torch.mean(data, dim=[0, 2, 3])\n        channels_sum += torch.Tensor.float(data ).mean(2, dim=[0, 2, 3])\n        \n        channels_sqrd_sum += torch.Tensor.float(data ** 2, dim=[0, 2, 3])\n        #channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n        num_batches += 1\n\n    mean = channels_sum \/ num_batches\n    std = (channels_sqrd_sum \/ num_batches - mean ** 2) ** 0.5\n\n    return mean, std","92185ec8":"train_augs = albumentations.Compose([ albumentations.Resize(img_size, img_size),    ToTensorV2()     ])\n\ntrainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , train_augs )\ntrainloader = DataLoader(trainset, batch_size = 9 , num_workers = 3 , shuffle = False)","3a7ba55c":"mean = [125.3113, 125.3113, 125.3113]\nstd = [103.8559, 103.8559, 103.8559]","ff081551":"for i in range(0,3):\n    a = (((std[i] - mean[i] )**2)\/417)**0.5\n    print(a)","15b38163":"for i in range(0,3):\n    a = (std[i]*(1\/255))\n    print(a)","54c236d9":"std = [0.407278, 0.407278 , 0.407278]","8a47a36e":"for i in range(0,3):\n    a = (mean[i]*(1\/255))\n    print(a)","adf982f3":"mean = [ 0.4914168 , 0.4914168 , 0.4914168]\nstd = [0.407278, 0.407278 , 0.407278]","7e7b6069":"R_channel = 0\nG_channel = 0\nB_channel = 0\n\npathDir = '..\/input\/ranzcr-clip-catheter-line-classification\/test\/*.jpg'\n\nfrom glob import glob\nimg_list = glob(pathDir)\nprint(len(img_list))","0d075678":"R_total , G_total ,B_total = 0, 0 , 0\n\ntotal_pixel = 0\nfor idx in range(len(img_list)):\n    filename = img_list[idx]\n    img = plt.imread(filename)\n\n    total_pixel = total_pixel + img.shape[0] * img.shape[1]\n\n    R_total += np.sum((img[:, :, 0] - R_mean) ** 2)\n    G_total = G_total + np.sum((img[:, :, 1] - G_mean) ** 2)\n    B_total = B_total + np.sum((img[:, :, 2] - B_mean) ** 2)\n\n    \n    \nR_std = sqrt(R_total \/ total_count)\nG_std = sqrt(G_total \/ total_count)\nB_std = sqrt(B_total \/ total_count)\n\n\ntraindata = datasets.ImageFolder(data_dir + '\/train', transforms.ToTensor())\nimage_means = torch.stack([t.mean(1).mean(1) for t, c in traindata])\nimage_means.mean(0)\n","3222bf0b":"\n\nclass Ranzcr_jpg_train_dataset(Dataset):    \n\t\n    def __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\tif self.num_channels == 1:\n\t\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ), 0)\n        \n\t\telse:\n\t\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = image #*(1\/255)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","1ea1b552":"# Reducing image pixels by a factor of 255 first\n\nNot needed\n\nmean and std are already scaled in albumentations , so we dont need to multiply mean\/std by 255. [code_here](https:\/\/github.com\/albumentations-team\/albumentations\/blob\/ff83de8a51184bca97be3a9fc6905c56b44c494a\/albumentations\/augmentations\/functional.py#L129)","0fa80ebf":"# Finding your our mean and std for image dataset","8a1d92a0":"Image = Image\/255","dd58344a":"# Trying differnt combinations of image augmentation\n\n(I hope to find one that magically works!)","53e2f9ad":"# Baseline","dedd5739":"# Saving all varints of augmentation to folder","94305a50":"https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.RandomRain","cae98f5f":"# Normalizing data first","a53501a5":"# Try different Mean and standard deviations"}}