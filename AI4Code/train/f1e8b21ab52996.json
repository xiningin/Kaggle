{"cell_type":{"425eb55c":"code","e121c3d0":"code","2301a455":"code","1241daa4":"code","53b38391":"code","58dbc2d3":"code","f2d5452f":"code","d5d9941e":"code","7e71ec4b":"code","3285a4ce":"code","eab54b11":"code","3c5ecbf5":"code","98d55d59":"code","afffb481":"code","2348a7c1":"code","1cf0549d":"code","34d3a787":"code","a55f4996":"code","5da0f3d6":"code","79e3cfd3":"code","681b3cba":"code","95749b0f":"code","6b7d3355":"code","2577af22":"code","6d3d5ce0":"code","6a4c85fd":"code","c24ef1ea":"code","22b6b58e":"code","8eadc39a":"code","0423428f":"code","1933365c":"code","180636a1":"code","3a972cd5":"code","6f7a9006":"code","a3e8f9ba":"markdown","a93daf3d":"markdown","6c479f21":"markdown","08d2ea18":"markdown","4be1b514":"markdown"},"source":{"425eb55c":"#  This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e121c3d0":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","2301a455":"train_df.head()","1241daa4":"test_df.head()","53b38391":"print(\"No. of training examples: \" + str(train_df.shape[0]))\nprint(\"No. of testing examples: \" + str(test_df.shape[0]))","58dbc2d3":"# check for missing values\ntrain_df.isna().sum()","f2d5452f":"# calculating percent missing values for Age, Cabin, Embarked\nprint(\"Percentage missing age values: \" + str((train_df['Age'].isna().sum()\/train_df.shape[0])*100))\nprint(\"Percentage missing cabin values: \" + str((train_df['Cabin'].isna().sum()\/train_df.shape[0])*100))\nprint(\"Percentage missing embarked values: \" + str((train_df['Embarked'].isna().sum()\/train_df.shape[0])*100))","d5d9941e":"# Distribution of ages in data\nage_distribution = dict(train_df[\"Age\"].value_counts())","7e71ec4b":"lists = sorted(age_distribution.items()) # sorted by key\nx, y = zip(*lists)\nplt.plot(x, y)\nplt.show()","3285a4ce":"train_data = train_df.copy()\ntrain_data[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True) # Since age distribution is a bit skewed towards the left side, it's better we use median of ages to fill the missing age values\ntrain_data[\"Embarked\"].fillna(train_df['Embarked'].value_counts().idxmax(), inplace=True) # Filling with most frequent occuring value\ntrain_data.drop('Cabin', axis=1, inplace=True) # Dropping Cabin column since 77% cabin values are missing","eab54b11":"train_data.isna().sum()","3c5ecbf5":"train_data.head()","98d55d59":"# creating one hot encodings for Pclass, embarked, sex\ntraining=pd.get_dummies(train_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntraining.drop('Sex_female', axis=1, inplace=True)\ntraining.drop('PassengerId', axis=1, inplace=True)\ntraining.drop('Name', axis=1, inplace=True)\ntraining.drop('Ticket', axis=1, inplace=True)\n\nfinal_train = training\nfinal_train.head()","afffb481":"#Applying same changes to test set\ntest_df.isna().sum()","2348a7c1":"test_data = test_df.copy()\ntest_data[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\ntest_data[\"Fare\"].fillna(train_df[\"Fare\"].median(skipna=True), inplace=True)\ntest_data.drop('Cabin', axis=1, inplace=True)\n\ntesting = pd.get_dummies(test_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntesting.drop('Sex_female', axis=1, inplace=True)\ntesting.drop('PassengerId', axis=1, inplace=True)\ntesting.drop('Name', axis=1, inplace=True)\ntesting.drop('Ticket', axis=1, inplace=True)\n\nfinal_test = testing\nfinal_test.head()","1cf0549d":"sc = StandardScaler()\nfinal_train[[\"Age\", \"Fare\"]] = sc.fit_transform(final_train[[\"Age\", \"Fare\"]])\nfinal_test[[\"Age\", \"Fare\"]] = sc.fit_transform(final_test[[\"Age\", \"Fare\"]])\n\nfinal_train.head()","34d3a787":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\ncols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Pclass_1\", \"Pclass_2\", \"Pclass_3\", \"Embarked_C\", \"Embarked_Q\", \"Embarked_S\", \"Sex_male\"]\nX = final_train[cols]\ny = final_train['Survived']\nmodel = RandomForestClassifier()\n\n# selecting top 8 features\nrfe = RFE(model, n_features_to_select = 8)\nrfe = rfe.fit(X, y)\nprint('Top 8 most important features: ' + str(list(X.columns[rfe.support_])))","a55f4996":"selected_features = ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass_1', 'Pclass_3', 'Embarked_S', 'Sex_male']","5da0f3d6":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree (bagging)\nbootstrap = [True, False]\n\n# Creating random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","79e3cfd3":"# First create the base model to tune\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model \n# ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass_1', 'Pclass_3', 'Embarked_S', 'Sex_male']\nrf_random.fit(final_train[selected_features], final_train['Survived'])","681b3cba":"rf_random.best_params_","95749b0f":"X = final_train[selected_features]\ny = final_train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=2)","6b7d3355":"# fitting both base random forest model and RandomizedSearchCV random forest model\nbase_model = RandomForestClassifier()\nbase_model.fit(X_train, y_train)","2577af22":"best_random = RandomForestClassifier(n_estimators = 2000,\n min_samples_split= 5,\n min_samples_leaf = 1,\n max_features = 'sqrt',\n max_depth = 100,\n bootstrap = True)\nbest_random.fit(X_train, y_train)\n","6d3d5ce0":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    accuracy = accuracy_score(test_labels, predictions) * 100\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))    \n    return accuracy","6a4c85fd":"base_accuracy = evaluate(base_model, X_test, y_test)","c24ef1ea":"\nrandom_accuracy = evaluate(best_random, X_test, y_test)","22b6b58e":"from sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)","8eadc39a":"grid_search.fit(final_train[selected_features], final_train['Survived'])","0423428f":"grid_search.best_params_","1933365c":"best_grid = RandomForestClassifier(n_estimators = 100,\n min_samples_split= 10,\n min_samples_leaf = 3,\n max_features = 3,\n max_depth = 100,\n bootstrap = True)\nbest_grid.fit(X_train, y_train)\n","180636a1":"grid_accuracy = evaluate(best_grid, X_test, y_test)","3a972cd5":"final_test['Survived'] = base_model.predict(final_test[selected_features])\nfinal_test['PassengerId'] = test_df['PassengerId']\n\nresults = final_test[['PassengerId','Survived']]","6f7a9006":"results.to_csv(\"submission.csv\", index=False)","a3e8f9ba":"This will try out 1 * 4 * 2 * 3 * 3 * 4 = 288 combinations of settings. We can fit the model, display the best hyperparameters, and evaluate performance","a93daf3d":"On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2 * 12 * 2 * 3 * 3 * 10 = 4320 settings! However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.","6c479f21":"We saw that Random Search did not give us any performance benefits in terms of parameter values, so let's try grid search to find optimal parameter values","08d2ea18":"The random forest classifier has too many parameters, so it is quite difficult to get an optimal value of these parameters manually. For this, let us try Random Hyperparameter grid and check for optimal values.\n\nThe code upto this point is the same as the code we used in logistic regression model. It can be found at [https:\/\/www.kaggle.com\/vardaanbajaj\/kernel6a7859f833]","4be1b514":"From all the above 3 models, we can see that the accuracy was over 80% for all of them which is much better than logistic regression accuracy. However, it is surprising that the base Random Forest model had the highest accuracy than others. This can be due to the fact that the training dataset was not large enough or the values we supplied for grid search to choose from might have included the range of optimal values. We can definitely toy around with it to improve our accuracy though. Now, let's just submit the results."}}