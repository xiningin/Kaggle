{"cell_type":{"e706d865":"code","f0e7ede1":"code","22da31c9":"code","7c3886c0":"code","c8de3dfd":"code","2141bf8e":"code","a1873e95":"code","f16e725a":"code","d5cb4276":"code","c5b64efe":"code","9a637c6f":"code","1b84c7a1":"code","440198a6":"code","bf18a9f3":"code","fc26799d":"code","0d6d61c3":"code","ea6fb04e":"code","6577b7f6":"code","304f2f1f":"code","667a4295":"code","af4baf98":"code","6b8a198e":"code","14126204":"code","324bc802":"code","73d46514":"code","4fb8a6fc":"code","a663aa5e":"code","718996bd":"code","475373bc":"code","5b0a7e66":"code","e8c00df9":"code","fde4e78c":"code","9ba067b9":"code","73e0da42":"code","c202d9d9":"code","105f6231":"code","11c411d3":"code","f750af00":"code","7293a474":"code","1f7f6676":"code","37f8d313":"code","549249e0":"code","b1e2f041":"code","5646961a":"code","f3912aa5":"code","91901540":"code","be4155a6":"code","0e3fc109":"code","57f5cf17":"code","d639d9e5":"code","0438348e":"code","e3a0c25d":"code","a7bb7792":"code","e8f3347d":"code","555a14cc":"code","f443324b":"code","057e30d0":"code","bd68190d":"markdown","11d7ebb9":"markdown","6def325d":"markdown","195f6d13":"markdown","58d39713":"markdown","2a23a205":"markdown","1d7e109f":"markdown","97e5c397":"markdown","779de480":"markdown","13317e42":"markdown","bb821937":"markdown","59ecdbd9":"markdown","6f5ee06c":"markdown","92cceeba":"markdown","f581a5db":"markdown","776bbb37":"markdown","85b7f20e":"markdown","391c88a4":"markdown","24a63651":"markdown","72335f37":"markdown","1845f12f":"markdown","337f4d03":"markdown","f7e78213":"markdown","59dce4e6":"markdown","5ac8d3ee":"markdown","c6adb86c":"markdown","2cc8cd08":"markdown","3dd13c1c":"markdown","f382c4be":"markdown","4ee3c1a4":"markdown","9c342a4d":"markdown","3f45db72":"markdown","bf75bd76":"markdown","cd31e2c6":"markdown","1b062543":"markdown","54ed0235":"markdown","459b4625":"markdown","a182f3e2":"markdown","8c70d14d":"markdown","8219a673":"markdown","db207bfd":"markdown","17e4bc62":"markdown","2a09413b":"markdown","ad15d020":"markdown","360ef5c2":"markdown","bd0d31f7":"markdown","5cda1d71":"markdown","900f144f":"markdown","fb0fd461":"markdown","6ca40e06":"markdown","01066836":"markdown","db9d811f":"markdown","aad3d98e":"markdown"},"source":{"e706d865":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# plotly\n# import plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n# word cloud library\nfrom wordcloud import WordCloud\n# Using plotly + cufflinks in offline mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nTrain=pd.read_csv(\"..\/input\/healthcare-provider-fraud-detection-analysis\/Train-1542865627584.csv\")       \nTrain.head(2)\n\n\n# Any results you write to the current directory are saved as output.","f0e7ede1":"Test = pd.read_csv('..\/input\/healthcare-provider-fraud-detection-analysis\/Test-1542969243754.csv');\nTest.head(2)","22da31c9":"Train_Outpatient=pd.read_csv(\"..\/input\/healthcare-provider-fraud-detection-analysis\/Train_Outpatientdata-1542865627584.csv\")       \nTrain_Outpatient.head(2)","7c3886c0":"Test_Outpatient = pd.read_csv('..\/input\/healthcare-provider-fraud-detection-analysis\/Test_Outpatientdata-1542969243754.csv');\nTest_Outpatient.head(2)","c8de3dfd":"Train_Inpatient = pd.read_csv('..\/input\/healthcare-provider-fraud-detection-analysis\/Train_Inpatientdata-1542865627584.csv');\nTrain_Inpatient.head(2)","2141bf8e":"Test_Inpatient=pd.read_csv(\"..\/input\/healthcare-provider-fraud-detection-analysis\/Test_Inpatientdata-1542969243754.csv\")       \nTest_Inpatient.head(2)\n","a1873e95":"Train_Beneficiary=pd.read_csv(\"..\/input\/healthcare-provider-fraud-detection-analysis\/Train_Beneficiarydata-1542865627584.csv\")       \nTrain_Beneficiary.head(2)\n","f16e725a":"\nTest_Beneficiary = pd.read_csv('..\/input\/healthcare-provider-fraud-detection-analysis\/Test_Beneficiarydata-1542969243754.csv');\nTest_Beneficiary.head(2)","d5cb4276":"df_procedures1 =  pd.DataFrame(columns = ['Procedures'])\ndf_procedures1['Procedures'] = pd.concat([Train_Inpatient[\"ClmProcedureCode_1\"], Train_Inpatient[\"ClmProcedureCode_2\"], Train_Inpatient[\"ClmProcedureCode_3\"], Train_Inpatient[\"ClmProcedureCode_4\"], Train_Inpatient[\"ClmProcedureCode_5\"], Train_Inpatient[\"ClmProcedureCode_6\"]], axis=0, sort=True).dropna()\ndf_procedures1['Procedures'].head(10)\ngrouped_procedure_df = df_procedures1['Procedures'].value_counts()\n\ndf_diagnosis = pd.DataFrame(columns = ['Diagnosis'])\ndf_diagnosis['Diagnosis'] = pd.concat([Train_Inpatient[\"ClmDiagnosisCode_1\"], Train_Inpatient[\"ClmDiagnosisCode_2\"], Train_Inpatient[\"ClmDiagnosisCode_3\"], Train_Inpatient[\"ClmDiagnosisCode_4\"], Train_Inpatient[\"ClmDiagnosisCode_5\"], Train_Inpatient[\"ClmDiagnosisCode_6\"], Train_Inpatient[\"ClmDiagnosisCode_7\"],  Train_Inpatient[\"ClmDiagnosisCode_8\"], Train_Inpatient[\"ClmDiagnosisCode_9\"], Train_Inpatient[\"ClmDiagnosisCode_10\"]], axis=0, sort=True).dropna()\ndf_diagnosis['Diagnosis'].head(10)\ngrouped_diagnosis_df = df_diagnosis['Diagnosis'].value_counts()\n\ngrouped_procedure_df1 = grouped_procedure_df.to_frame()\ngrouped_procedure_df1.columns = ['count']\ngrouped_procedure_df1['Procedure'] = grouped_procedure_df1.index\ngrouped_procedure_df1['Percentage'] = (grouped_procedure_df1['count']\/sum(grouped_procedure_df1['count']))*100\n\ngrouped_diagnosis_df = grouped_diagnosis_df.to_frame()\ngrouped_diagnosis_df.columns = ['count']\ngrouped_diagnosis_df['Diagnosis'] = grouped_diagnosis_df.index\ngrouped_diagnosis_df['Percentage'] = (grouped_diagnosis_df['count']\/sum(grouped_diagnosis_df['count']))*100\n\n# taking only top 20 \n\nplot_procedure_df1 = grouped_procedure_df1.head(20)\nplot_diagnosis_df1 = grouped_diagnosis_df.head(20)\n\n# Plotting the most commonly used diagnosis and procedures \nfrom matplotlib import pyplot as plt\n\nplot_procedure_df1['Procedure'] = 'P' + plot_procedure_df1['Procedure'].astype(str)\nplot_procedure_df1.sort_values(by=['Percentage'])\nplot_procedure_df1.iplot(x ='Procedure', y='Percentage', kind='bar',xTitle='Procedure', color ='green',\n                  yTitle='Percentage', title='Procedure Distribution', categoryorder='total descending')\n\nplot_diagnosis_df1['Diagnosis'] = 'D' + plot_diagnosis_df1['Diagnosis'].astype(str)\nplot_diagnosis_df1.sort_values(by=['Percentage'])\nplot_diagnosis_df1.iplot(x ='Diagnosis', y='Percentage', kind='bar',xTitle='Diagnosis', color ='green',\n                  yTitle='Percentage', title='Diagnosis Distribution', categoryorder='total descending')","c5b64efe":"df_procedures2 =  pd.DataFrame(columns = ['Procedures'])\ndf_procedures2['Procedures'] = pd.concat([Train_Outpatient[\"ClmProcedureCode_1\"], Train_Outpatient[\"ClmProcedureCode_2\"], Train_Outpatient[\"ClmProcedureCode_3\"], Train_Outpatient[\"ClmProcedureCode_4\"], Train_Outpatient[\"ClmProcedureCode_5\"], Train_Outpatient[\"ClmProcedureCode_6\"]], axis=0, sort=True).dropna()\ndf_procedures2['Procedures'].head(10)\ngrouped_procedure_df2 = df_procedures2['Procedures'].value_counts()\n\ndf_diagnosis2 = pd.DataFrame(columns = ['Diagnosis'])\ndf_diagnosis2['Diagnosis'] = pd.concat([Train_Outpatient[\"ClmDiagnosisCode_1\"], Train_Outpatient[\"ClmDiagnosisCode_2\"], Train_Outpatient[\"ClmDiagnosisCode_3\"], Train_Outpatient[\"ClmDiagnosisCode_4\"], Train_Outpatient[\"ClmDiagnosisCode_5\"], Train_Outpatient[\"ClmDiagnosisCode_6\"], Train_Outpatient[\"ClmDiagnosisCode_7\"],  Train_Outpatient[\"ClmDiagnosisCode_8\"], Train_Outpatient[\"ClmDiagnosisCode_9\"], Train_Outpatient[\"ClmDiagnosisCode_10\"]], axis=0, sort=True).dropna()\ndf_diagnosis2['Diagnosis'].head(10)\ngrouped_diagnosis_df2 = df_diagnosis2['Diagnosis'].value_counts()\n\ngrouped_procedure_df_op = grouped_procedure_df2.to_frame()\ngrouped_procedure_df_op.columns = ['count']\ngrouped_procedure_df_op['Procedure'] = grouped_procedure_df_op.index\ngrouped_procedure_df_op['Percentage'] = (grouped_procedure_df_op['count']\/sum(grouped_procedure_df_op['count']))*100\n\ngrouped_diagnosis_df_op = grouped_diagnosis_df2.to_frame()\ngrouped_diagnosis_df_op.columns = ['count']\ngrouped_diagnosis_df_op['Diagnosis'] = grouped_diagnosis_df_op.index\ngrouped_diagnosis_df_op['Percentage'] = (grouped_diagnosis_df_op['count']\/sum(grouped_diagnosis_df_op['count']))*100\n\n# taking only top 20 \n\nplot_procedure_df2 = grouped_procedure_df_op.head(20)\nplot_diagnosis_df2 = grouped_diagnosis_df_op.head(20)\n\n# Plotting the most commonly used diagnosis and procedures \nfrom matplotlib import pyplot as plt\n\n\nplot_procedure_df2['Procedure'] = 'P' + plot_procedure_df2['Procedure'].astype(str)\nplot_procedure_df2.sort_values(by=['Percentage'])\nplot_procedure_df2.iplot(x ='Procedure', y='Percentage', kind='bar',xTitle='Procedure', color ='yellow',\n                  yTitle='Percentage', title='Procedure Distribution', categoryorder='total descending')\n\nplot_diagnosis_df2['Diagnosis'] = 'D' + plot_diagnosis_df2['Diagnosis'].astype(str)\nplot_diagnosis_df2.sort_values(by=['Percentage'])\nplot_diagnosis_df2.iplot(x ='Diagnosis', y='Percentage', kind='bar',xTitle='Diagnosis', color ='yellow',\n                  yTitle='Percentage', title='Diagnosis Distribution', categoryorder='total descending')","9a637c6f":"Train.head()\nT_fraud = Train['PotentialFraud'].value_counts()\ngrouped_train_df = T_fraud.to_frame()\n\ngrouped_train_df.columns = ['count']\ngrouped_train_df['Fraud'] = grouped_train_df.index\ngrouped_train_df['Percentage'] = (grouped_train_df['count']\/sum(grouped_train_df['count']))*100\ngrouped_train_df['Percentage'].iplot( kind='bar',color = \"blue\", title = 'Distribution')","1b84c7a1":"len(Train_Inpatient)","440198a6":"Train_f =  pd.DataFrame(columns = ['PotentialFraud', 'Provider'])\nTrain_f = Train.loc[(Train['PotentialFraud'] == 'Yes')]\nfraud_provider_ip_df = pd.merge(Train_Inpatient, Train_f, how='inner', on='Provider')\nlen(fraud_provider_ip_df)","bf18a9f3":"(len(fraud_provider_ip_df)\/len(Train_Inpatient)) * 100","fc26799d":"len(Train_Outpatient)","0d6d61c3":"fraud_provider_op_df = pd.merge(Train_Outpatient, Train_f, how='inner', on='Provider')\nlen(fraud_provider_op_df)","ea6fb04e":"(len(fraud_provider_op_df)\/len(Train_Outpatient))*100","6577b7f6":"df_procedures2 =  pd.DataFrame(columns = ['Procedures'])\ndf_procedures2['Procedures'] = pd.concat([fraud_provider_ip_df[\"ClmProcedureCode_1\"], fraud_provider_ip_df[\"ClmProcedureCode_2\"], fraud_provider_ip_df[\"ClmProcedureCode_3\"], fraud_provider_ip_df[\"ClmProcedureCode_4\"], fraud_provider_ip_df[\"ClmProcedureCode_5\"], fraud_provider_ip_df[\"ClmProcedureCode_6\"]], axis=0, sort=True).dropna()\ndf_procedures2['Procedures'].head(10)\ngrouped_F_procedure_df = df_procedures2['Procedures'].value_counts()\n\ndf_diagnosis2 = pd.DataFrame(columns = ['Diagnosis'])\ndf_diagnosis2['Diagnosis'] = pd.concat([fraud_provider_ip_df[\"ClmDiagnosisCode_1\"], fraud_provider_ip_df[\"ClmDiagnosisCode_2\"], fraud_provider_ip_df[\"ClmDiagnosisCode_3\"], fraud_provider_ip_df[\"ClmDiagnosisCode_4\"], fraud_provider_ip_df[\"ClmDiagnosisCode_5\"], fraud_provider_ip_df[\"ClmDiagnosisCode_6\"], fraud_provider_ip_df[\"ClmDiagnosisCode_7\"],  fraud_provider_ip_df[\"ClmDiagnosisCode_8\"], fraud_provider_ip_df[\"ClmDiagnosisCode_9\"], fraud_provider_ip_df[\"ClmDiagnosisCode_10\"]], axis=0, sort=True).dropna()\ndf_diagnosis2['Diagnosis'].head(10)\ngrouped_F_diagnosis_df = df_diagnosis2['Diagnosis'].value_counts()\n\ngrouped_F_procedure_df2 = grouped_F_procedure_df.to_frame()\ngrouped_F_procedure_df2.columns = ['count']\ngrouped_F_procedure_df2['Procedure'] = grouped_F_procedure_df2.index\ngrouped_F_procedure_df2['Percentage'] = (grouped_F_procedure_df2['count']\/sum(grouped_F_procedure_df2['count']))*100\n\ngrouped_F_diagnosis_df2 = grouped_F_diagnosis_df.to_frame()\ngrouped_F_diagnosis_df2.columns = ['count']\ngrouped_F_diagnosis_df2['Diagnosis'] = grouped_F_diagnosis_df2.index\ngrouped_F_diagnosis_df2['Percentage'] = (grouped_F_diagnosis_df2['count']\/sum(grouped_F_diagnosis_df2['count']))*100\n\nplot_F_procedure_df1 = grouped_F_procedure_df2.head(20)\n\nplot_F_diagnosis_df1 = grouped_F_diagnosis_df2.head(20)\n\nplot_F_procedure_df1.plot(x ='Procedure', y='Percentage', kind = 'bar', color ='g')\nplot_F_diagnosis_df1.plot(x ='Diagnosis', y='Percentage', kind = 'bar', color ='g')\n\n","304f2f1f":"df_procedures_op2 =  pd.DataFrame(columns = ['Procedures'])\ndf_procedures_op2['Procedures'] = pd.concat([fraud_provider_op_df[\"ClmProcedureCode_1\"], fraud_provider_op_df[\"ClmProcedureCode_2\"], fraud_provider_op_df[\"ClmProcedureCode_3\"], fraud_provider_op_df[\"ClmProcedureCode_4\"], fraud_provider_op_df[\"ClmProcedureCode_5\"], fraud_provider_op_df[\"ClmProcedureCode_6\"]], axis=0, sort=True).dropna()\ndf_procedures_op2['Procedures'].head(10)\ngrouped_F_procedure_op_df = df_procedures_op2['Procedures'].value_counts()\n\ndf_diagnosis_op2 = pd.DataFrame(columns = ['Diagnosis'])\ndf_diagnosis_op2['Diagnosis'] = pd.concat([fraud_provider_op_df[\"ClmDiagnosisCode_1\"], fraud_provider_op_df[\"ClmDiagnosisCode_2\"], fraud_provider_op_df[\"ClmDiagnosisCode_3\"], fraud_provider_op_df[\"ClmDiagnosisCode_4\"], fraud_provider_op_df[\"ClmDiagnosisCode_5\"], fraud_provider_op_df[\"ClmDiagnosisCode_6\"], fraud_provider_op_df[\"ClmDiagnosisCode_7\"],  fraud_provider_op_df[\"ClmDiagnosisCode_8\"], fraud_provider_op_df[\"ClmDiagnosisCode_9\"], fraud_provider_op_df[\"ClmDiagnosisCode_10\"]], axis=0, sort=True).dropna()\ndf_diagnosis_op2['Diagnosis'].head(10)\ngrouped_F_diagnosis_op_df = df_diagnosis2['Diagnosis'].value_counts()\n\ngrouped_F_procedure_opdf2 = grouped_F_procedure_op_df.to_frame()\ngrouped_F_procedure_opdf2.columns = ['count']\ngrouped_F_procedure_opdf2['Procedure'] = grouped_F_procedure_opdf2.index\ngrouped_F_procedure_opdf2['Percentage'] = (grouped_F_procedure_opdf2['count']\/sum(grouped_F_procedure_opdf2['count']))*100\n\ngrouped_F_diagnosis_opdf2 = grouped_F_diagnosis_op_df.to_frame()\ngrouped_F_diagnosis_opdf2.columns = ['count']\ngrouped_F_diagnosis_opdf2['Diagnosis'] = grouped_F_diagnosis_opdf2.index\ngrouped_F_diagnosis_opdf2['Percentage'] = (grouped_F_diagnosis_opdf2['count']\/sum(grouped_F_diagnosis_opdf2['count']))*100\n\nplot_F_procedure_opdf1 = grouped_F_procedure_opdf2.head(20)\n\nplot_F_diagnosis_opdf1 = grouped_F_diagnosis_opdf2.head(20)\n\nplot_F_procedure_opdf1.plot(x ='Procedure', y='Percentage', kind = 'bar', color ='c')\nplot_F_diagnosis_opdf1.plot(x ='Diagnosis', y='Percentage', kind = 'bar', color ='c')","667a4295":"Train_Beneficiary.head(2)","af4baf98":"fraud_beneficiary_ip_op_df = pd.merge(Train_Beneficiary, fraud_provider_ip_df, how='inner', on='BeneID')\nfraud_beneficiary_ip_op_df = pd.merge(Train_Beneficiary, fraud_provider_op_df, how='inner', on='BeneID')\nTrain_F_Beneficiary_grouped = fraud_beneficiary_ip_op_df['State'].value_counts()\nTrain_F_Beneficiary_grouped1 = Train_F_Beneficiary_grouped.to_frame()\nTrain_F_Beneficiary_grouped1['Count'] =  Train_F_Beneficiary_grouped1['State']\nTrain_F_Beneficiary_grouped1['STATE'] = Train_F_Beneficiary_grouped1.index\nTrain_F_Beneficiary_grouped1 = Train_F_Beneficiary_grouped1.drop(['State'], axis = 1)\nTrain_F_Beneficiary_grouped1 = Train_F_Beneficiary_grouped1.head(20)\nTrain_F_Beneficiary_grouped1.plot(x ='STATE', y='Count', kind = 'bar')","6b8a198e":"import seaborn as sns\n\nfraud_beneficiary_ip_op_df['DOB'] =  pd.to_datetime(fraud_beneficiary_ip_op_df['DOB'], format='%Y-%m-%d')  \nnow = pd.to_datetime('2009-12-01' , format = '%Y-%m-%d') # Assuming this is 2009 data as the last recorded death is for 2009\nfraud_beneficiary_ip_op_df['DOB'] = fraud_beneficiary_ip_op_df['DOB'].where(fraud_beneficiary_ip_op_df['DOB'] < now, fraud_beneficiary_ip_op_df['DOB'] -  np.timedelta64(100, 'Y'))   # 2\nfraud_beneficiary_ip_op_df['age'] = (now - fraud_beneficiary_ip_op_df['DOB']).astype('<m8[Y]')    # 3\nax = fraud_beneficiary_ip_op_df['age'].plot.hist(bins=20, alpha=0.5, figsize=(8, 6), edgecolor='k')","14126204":"Train_Beneficiary['DOB'] =  pd.to_datetime(Train_Beneficiary['DOB'], format='%Y-%m-%d')  \nnow = pd.to_datetime('2009-12-01' , format = '%Y-%m-%d') # Assuming this is 2009 data as the last recorded death is for 2009\nTrain_Beneficiary['DOB'] = Train_Beneficiary['DOB'].where(Train_Beneficiary['DOB'] < now, Train_Beneficiary['DOB'] -  np.timedelta64(100, 'Y'))   # 2\nTrain_Beneficiary['age'] = (now - Train_Beneficiary['DOB']).astype('<m8[Y]')    # 3\nax = Train_Beneficiary['age'].plot.hist(bins=20, alpha=0.5, figsize=(8, 6), edgecolor='k')","324bc802":"ax = Train_Inpatient['InscClaimAmtReimbursed'].plot.hist(bins=20, alpha=0.5, figsize=(8, 6), facecolor='g', edgecolor='k')","73d46514":"Train_Inpatient_1 = pd.merge(Train_Inpatient, Train, how='inner', on='Provider')\ng = sns.FacetGrid(Train_Inpatient_1, col='PotentialFraud', height=8)\ng.map(plt.hist, 'InscClaimAmtReimbursed', bins=20, color = 'g')","4fb8a6fc":"Train_Inpatient_1 = Train_Inpatient_1.loc[(Train_Inpatient_1['PotentialFraud'] == 'Yes')]\nTotal = Train_Inpatient_1['InscClaimAmtReimbursed'].sum()\nprint(Total)","a663aa5e":"ax = Train_Outpatient['InscClaimAmtReimbursed'].plot.hist(bins=100,range=[0, 5000], alpha=0.5, figsize=(8, 6), facecolor='c', edgecolor='k')","718996bd":"Train_Outpatient_1 = pd.merge(Train_Outpatient, Train, how='inner', on='Provider')\ng = sns.FacetGrid(Train_Outpatient_1, col='PotentialFraud', height=8)\ng.map(plt.hist, 'InscClaimAmtReimbursed', bins=20, range=[0, 5000], color ='c')","475373bc":"Train_Beneficiary.head()","5b0a7e66":"Train_Beneficiary.isna().sum()","e8c00df9":"Test_Beneficiary.isna().sum()","fde4e78c":"Train_Beneficiary['DOB'] = pd.to_datetime(Train_Beneficiary['DOB'] , format = '%Y-%m-%d')\nTrain_Beneficiary['DOD'] = pd.to_datetime(Train_Beneficiary['DOD'],format = '%Y-%m-%d',errors='ignore')\nTrain_Beneficiary['Age'] = round(((Train_Beneficiary['DOD'] - Train_Beneficiary['DOB']).dt.days)\/365)\n\nTest_Beneficiary['DOB'] = pd.to_datetime(Test_Beneficiary['DOB'] , format = '%Y-%m-%d')\nTest_Beneficiary['DOD'] = pd.to_datetime(Test_Beneficiary['DOD'],format = '%Y-%m-%d',errors='ignore')\nTest_Beneficiary['Age'] = round(((Test_Beneficiary['DOD'] - Test_Beneficiary['DOB']).dt.days)\/365)\n\n## As we see that last DOD value is 2009-12-01 ,which means Beneficiary Details data is of year 2009.\n## so we will calculate age of other benficiaries for year 2009.\n\nTrain_Beneficiary.Age.fillna(round(((pd.to_datetime('2009-12-01' , format = '%Y-%m-%d') - Test_Beneficiary['DOB']).dt.days)\/365),\n                                 inplace=True)\n\n\nTest_Beneficiary.Age.fillna(round(((pd.to_datetime('2009-12-01' , format = '%Y-%m-%d') - Test_Beneficiary['DOB']).dt.days)\/365),\n                                 inplace=True)\n\n\nTest_Beneficiary.head(2)","9ba067b9":"# Joining inpatinet and outpatient data with a column defining if it is inpatient or outpatient ","73e0da42":"## Creating the master DF for test\nTrain_Inpatient['EncounterType'] = 0\nTrain_Outpatient['EncounterType'] = 1\nframes = [Train_Inpatient, Train_Outpatient]\nTrainInAndOut = pd.concat(frames)\nTrainInAndOutBenf = pd.merge(TrainInAndOut, Train_Beneficiary, how='inner', on='BeneID')\nMaster_df = pd.merge(TrainInAndOutBenf, Train, how='inner', on='Provider')\n\n##Creating the master DF for Test \nTest_Inpatient['EncounterType'] = 0\nTest_Outpatient['EncounterType'] = 1\nframes = [Test_Inpatient, Test_Outpatient]\nTestInAndOut = pd.concat(frames)\nTestInAndOutBenf = pd.merge(TestInAndOut, Test_Beneficiary, how='inner', on='BeneID')\nMasterTest_df = pd.merge(TestInAndOutBenf, Test, how='inner', on='Provider')","c202d9d9":"Master_df['DOB'] = pd.to_datetime(Master_df['DOB'] , format = '%Y-%m-%d')\nMaster_df['DOD'] = pd.to_datetime(Master_df['DOD'],format = '%Y-%m-%d',errors='ignore')\nMaster_df['Age'] = round(((Master_df['DOD'] - Master_df['DOB']).dt.days)\/365)\n\nMasterTest_df['DOB'] = pd.to_datetime(MasterTest_df['DOB'] , format = '%Y-%m-%d')\nMasterTest_df['DOD'] = pd.to_datetime(MasterTest_df['DOD'],format = '%Y-%m-%d',errors='ignore')\nMasterTest_df['Age'] = round(((MasterTest_df['DOD'] - MasterTest_df['DOB']).dt.days)\/365)\n\n## As we see that last DOD value is 2009-12-01 ,which means Beneficiary Details data is of year 2009.\n## so we will calculate age of other benficiaries for year 2009.\n\nMaster_df.Age.fillna(round(((pd.to_datetime('2009-12-01' , format = '%Y-%m-%d') - Master_df['DOB']).dt.days)\/365),\n                                 inplace=True)\n\n\nMasterTest_df.Age.fillna(round(((pd.to_datetime('2009-12-01' , format = '%Y-%m-%d') - MasterTest_df['DOB']).dt.days)\/365),\n                                 inplace=True)\n\n\nMasterTest_df.head(2)\nMaster_df = Master_df.drop(['age'], axis = 1) \n\n","105f6231":"## removing the column DOD and DOB also creating a new column IsDead as we already have the age we do not need date of death and date of birth \n\nMaster_df.loc[Master_df['DOD'].isnull(), 'IsDead'] = '0'\nMaster_df.loc[(Master_df['DOD'].notnull()), 'IsDead'] = '1'\nMaster_df = Master_df.drop(['DOD'], axis = 1)\nMaster_df = Master_df.drop(['DOB'], axis = 1)\n\n## Same activity on the test data \nMasterTest_df.loc[MasterTest_df['DOD'].isnull(), 'IsDead'] = '0'\nMasterTest_df.loc[(MasterTest_df['DOD'].notnull()), 'IsDead'] = '1'\nMasterTest_df = MasterTest_df.drop(['DOD'], axis = 1)\nMasterTest_df = MasterTest_df.drop(['DOB'], axis = 1)\nMasterTest_df.head()\n","11c411d3":"Master_df['AdmissionDt'] = pd.to_datetime(Master_df['AdmissionDt'] , format = '%Y-%m-%d')\nMaster_df['DischargeDt'] = pd.to_datetime(Master_df['DischargeDt'],format = '%Y-%m-%d')\nMaster_df['DaysAdmitted'] = ((Master_df['DischargeDt'] - Master_df['AdmissionDt']).dt.days)+1\nMaster_df.loc[Master_df['EncounterType'] == 1, 'DaysAdmitted'] = '0'\nMaster_df[['EncounterType','DaysAdmitted','DischargeDt','AdmissionDt']].head()\nMaster_df = Master_df.drop(['DischargeDt'], axis = 1)\nMaster_df = Master_df.drop(['AdmissionDt'], axis = 1)\n\n## Performing the same operations on test data \nMasterTest_df['AdmissionDt'] = pd.to_datetime(MasterTest_df['AdmissionDt'] , format = '%Y-%m-%d')\nMasterTest_df['DischargeDt'] = pd.to_datetime(MasterTest_df['DischargeDt'],format = '%Y-%m-%d')\nMasterTest_df['DaysAdmitted'] = ((MasterTest_df['DischargeDt'] - MasterTest_df['AdmissionDt']).dt.days)+1\nMasterTest_df.loc[MasterTest_df['EncounterType'] == 1, 'DaysAdmitted'] = '0'\nMasterTest_df[['EncounterType','DaysAdmitted','DischargeDt','AdmissionDt', 'DeductibleAmtPaid']].head()\nMasterTest_df = MasterTest_df.drop(['DischargeDt'], axis = 1)\nMasterTest_df = MasterTest_df.drop(['AdmissionDt'], axis = 1)\n","f750af00":"MasterTest_df.loc[MasterTest_df['DeductibleAmtPaid'].isnull(), 'DeductibleAmtPaid'] = '0'\nMaster_df.loc[Master_df['DeductibleAmtPaid'].isnull(), 'DeductibleAmtPaid'] = '0'\n","7293a474":"#Master_df.isna().sum()\nMaster_df.isna().sum()","1f7f6676":"Master_df.shape","37f8d313":"admit_diagnosis = Master_df['ClmAdmitDiagnosisCode'].value_counts()\nadmit_diagnosis_df = admit_diagnosis.to_frame()\nadmit_diagnosis_df ['Percentage_ClmAdmitDiagnosis'] = (admit_diagnosis_df['ClmAdmitDiagnosisCode']\/admit_diagnosis_df['ClmAdmitDiagnosisCode'].sum())*100\nadmit_diagnosis_df ['Percentage_ClmAdmitDiagnosis'] = admit_diagnosis_df ['Percentage_ClmAdmitDiagnosis'].cumsum()\nadmit_diagnosis_df.loc[admit_diagnosis_df['Percentage_ClmAdmitDiagnosis'] > 80, 'Percentage_ClmAdmitDiagnosis'] = 0\nadmit_diagnosis_df.drop(['ClmAdmitDiagnosisCode'], axis = 1) \nadmit_diagnosis_df['ClmAdmitDiagnosisCode'] = admit_diagnosis_df.index\nMaster_df = pd.merge(Master_df, admit_diagnosis_df, how='inner', on='ClmAdmitDiagnosisCode')\nMaster_df.loc[Master_df['Percentage_ClmAdmitDiagnosis'] == 0, 'ClmAdmitDiagnosisCode'] = 0\nMaster_df.tail(5)\n","549249e0":"diagnosis1 = Master_df['ClmDiagnosisCode_1'].value_counts()\ndiagnosis1_df = diagnosis1.to_frame()\ndiagnosis1_df ['Percentage_ClmDiagnosisCode_1'] = (diagnosis1_df['ClmDiagnosisCode_1']\/diagnosis1_df['ClmDiagnosisCode_1'].sum())*100\ndiagnosis1_df ['Percentage_ClmDiagnosisCode_1'] = diagnosis1_df['Percentage_ClmDiagnosisCode_1'].cumsum()\ndiagnosis1_df.loc[diagnosis1_df['Percentage_ClmDiagnosisCode_1'] > 60, 'Percentage_ClmDiagnosisCode_1'] = 0\n# for checking the number of items we will end up with \n# subset_df = diagnosis1_df[diagnosis1_df['Percentage_ClmDiagnosisCode_1'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosis1_df.drop(['ClmDiagnosisCode_1'], axis = 1) \ndiagnosis1_df['ClmDiagnosisCode_1'] = diagnosis1_df.index\nMaster_df = pd.merge(Master_df, diagnosis1_df, how='inner', on='ClmDiagnosisCode_1')\nMaster_df.loc[Master_df['Percentage_ClmDiagnosisCode_1'] == 0, 'ClmDiagnosisCode_1'] = 0\nMaster_df.tail(5)","b1e2f041":"diagnosis10 = Master_df['ClmDiagnosisCode_10'].value_counts()\ndiagnosis10_df = diagnosis10.to_frame()\ndiagnosis10_df ['Percentage_ClmDiagnosisCode_10'] = (diagnosis10_df['ClmDiagnosisCode_10']\/diagnosis10_df['ClmDiagnosisCode_10'].sum())*100\ndiagnosis10_df ['Percentage_ClmDiagnosisCode_10'] = diagnosis10_df['Percentage_ClmDiagnosisCode_10'].cumsum()\ndiagnosis10_df.loc[diagnosis10_df['Percentage_ClmDiagnosisCode_10'] > 60, 'Percentage_ClmDiagnosisCode_10'] = 0\n# for checking the number of items we will end up with \n# subset_df = diagnosis10_df[diagnosis10_df['Percentage_ClmDiagnosisCode_10'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosis10_df.drop(['ClmDiagnosisCode_10'], axis = 1) \ndiagnosis10_df['ClmDiagnosisCode_10'] = diagnosis10_df.index\nMaster_df = pd.merge(Master_df, diagnosis10_df, how='inner', on='ClmDiagnosisCode_10')\nMaster_df.loc[Master_df['Percentage_ClmDiagnosisCode_10'] == 0, 'ClmDiagnosisCode_10'] = 0\nMaster_df.tail(5)","5646961a":"diagnosis2 = Master_df['ClmDiagnosisCode_2'].value_counts()\ndiagnosis2_df = diagnosis2.to_frame()\ndiagnosis2_df ['Percentage_ClmDiagnosisCode_2'] = (diagnosis2_df['ClmDiagnosisCode_2']\/diagnosis2_df['ClmDiagnosisCode_2'].sum())*100\ndiagnosis2_df ['Percentage_ClmDiagnosisCode_2'] = diagnosis2_df['Percentage_ClmDiagnosisCode_2'].cumsum()\ndiagnosis2_df.loc[diagnosis2_df['Percentage_ClmDiagnosisCode_2'] > 80, 'Percentage_ClmDiagnosisCode_2'] = 0\n#for checking the number of items we will end up with \n# subset_df = diagnosis2_df[diagnosis2_df['Percentage_ClmDiagnosisCode_2'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosis2_df.drop(['ClmDiagnosisCode_2'], axis = 1) \ndiagnosis2_df['ClmDiagnosisCode_2'] = diagnosis2_df.index\nMaster_df = pd.merge(Master_df, diagnosis2_df, how='inner', on='ClmDiagnosisCode_2')\nMaster_df.loc[Master_df['Percentage_ClmDiagnosisCode_2'] == 0, 'ClmDiagnosisCode_2'] = 0\nMaster_df.tail(5)","f3912aa5":"diagnosis3 = Master_df['ClmDiagnosisCode_3'].value_counts()\ndiagnosis3_df = diagnosis3.to_frame()\ndiagnosis3_df ['Percentage_ClmDiagnosisCode_3'] = (diagnosis3_df['ClmDiagnosisCode_3']\/diagnosis3_df['ClmDiagnosisCode_3'].sum())*100\ndiagnosis3_df ['Percentage_ClmDiagnosisCode_3'] = diagnosis3_df['Percentage_ClmDiagnosisCode_3'].cumsum()\ndiagnosis3_df.loc[diagnosis3_df['Percentage_ClmDiagnosisCode_3'] > 80, 'Percentage_ClmDiagnosisCode_3'] = 0\n#for checking the number of items we will end up with \n# subset_df = diagnosis3_df[diagnosis3_df['Percentage_ClmDiagnosisCode_3'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosis3_df.drop(['ClmDiagnosisCode_3'], axis = 1) \ndiagnosis3_df['ClmDiagnosisCode_3'] = diagnosis3_df.index\nMaster_df = pd.merge(Master_df, diagnosis3_df, how='inner', on='ClmDiagnosisCode_3')\nMaster_df.loc[Master_df['Percentage_ClmDiagnosisCode_3'] == 0, 'ClmDiagnosisCode_3'] = 0\nMaster_df.tail(5)","91901540":"diagnosis4 = Master_df['ClmDiagnosisCode_4'].value_counts()\ndiagnosis4_df = diagnosis4.to_frame()\ndiagnosis4_df ['Percentage_ClmDiagnosisCode_4'] = (diagnosis4_df['ClmDiagnosisCode_4']\/diagnosis4_df['ClmDiagnosisCode_4'].sum())*100\ndiagnosis4_df ['Percentage_ClmDiagnosisCode_4'] = diagnosis4_df['Percentage_ClmDiagnosisCode_4'].cumsum()\ndiagnosis4_df.loc[diagnosis4_df['Percentage_ClmDiagnosisCode_4'] > 80, 'Percentage_ClmDiagnosisCode_4'] = 0\n#for checking the number of items we will end up with \n# subset_df = diagnosis4_df[diagnosis4_df['Percentage_ClmDiagnosisCode_4'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosis4_df.drop(['ClmDiagnosisCode_4'], axis = 1) \ndiagnosis4_df['ClmDiagnosisCode_4'] = diagnosis4_df.index\nMaster_df = pd.merge(Master_df, diagnosis4_df, how='inner', on='ClmDiagnosisCode_4')\nMaster_df.loc[Master_df['Percentage_ClmDiagnosisCode_4'] == 0, 'ClmDiagnosisCode_4'] = 0\nMaster_df.tail(5)","be4155a6":"diagnosis5 = Master_df['ClmDiagnosisCode_5'].value_counts()\ndiagnosis5_df = diagnosis5.to_frame()\ndiagnosis5_df ['Percentage_ClmDiagnosisCode_5'] = (diagnosis5_df['ClmDiagnosisCode_5']\/diagnosis5_df['ClmDiagnosisCode_5'].sum())*100\ndiagnosis5_df ['Percentage_ClmDiagnosisCode_5'] = diagnosis5_df['Percentage_ClmDiagnosisCode_5'].cumsum()\ndiagnosis5_df.loc[diagnosis5_df['Percentage_ClmDiagnosisCode_5'] > 80, 'Percentage_ClmDiagnosisCode_5'] = 0\n#for checking the number of items we will end up with \n# subset_df = diagnosis5_df[diagnosis5_df['Percentage_ClmDiagnosisCode_5'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosis5_df.drop(['ClmDiagnosisCode_5'], axis = 1) \ndiagnosis5_df['ClmDiagnosisCode_5'] = diagnosis5_df.index\nMaster_df = pd.merge(Master_df, diagnosis5_df, how='inner', on='ClmDiagnosisCode_5')\nMaster_df.loc[Master_df['Percentage_ClmDiagnosisCode_5'] == 0, 'ClmDiagnosisCode_5'] = 0\nMaster_df.tail(5)","0e3fc109":"diagnosis6 = Master_df['ClmDiagnosisCode_6'].value_counts()\ndiagnosis6_df = diagnosis6.to_frame()\ndiagnosis6_df ['Percentage_ClmDiagnosisCode_6'] = (diagnosis6_df['ClmDiagnosisCode_6']\/diagnosis6_df['ClmDiagnosisCode_6'].sum())*100\ndiagnosis6_df ['Percentage_ClmDiagnosisCode_6'] = diagnosis6_df['Percentage_ClmDiagnosisCode_6'].cumsum()\ndiagnosis6_df.loc[diagnosis6_df['Percentage_ClmDiagnosisCode_6'] > 70, 'Percentage_ClmDiagnosisCode_6'] = 0\n#for checking the number of items we will end up with \n# subset_df = diagnosis6_df[diagnosis6_df['Percentage_ClmDiagnosisCode_6'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosis6_df.drop(['ClmDiagnosisCode_6'], axis = 1) \ndiagnosis6_df['ClmDiagnosisCode_6'] = diagnosis6_df.index\nMaster_df = pd.merge(Master_df, diagnosis6_df, how='inner', on='ClmDiagnosisCode_6')\nMaster_df.loc[Master_df['Percentage_ClmDiagnosisCode_6'] == 0, 'ClmDiagnosisCode_6'] = 0\nMaster_df.tail(5)","57f5cf17":"diagnosis7 = Master_df['ClmDiagnosisCode_7'].value_counts()\ndiagnosis7_df = diagnosis7.to_frame()\ndiagnosis7_df ['Percentage_ClmDiagnosisCode_7'] = (diagnosis7_df['ClmDiagnosisCode_7']\/diagnosis7_df['ClmDiagnosisCode_7'].sum())*100\ndiagnosis7_df ['Percentage_ClmDiagnosisCode_7'] = diagnosis7_df['Percentage_ClmDiagnosisCode_7'].cumsum()\ndiagnosis7_df.loc[diagnosis7_df['Percentage_ClmDiagnosisCode_7'] > 70, 'Percentage_ClmDiagnosisCode_7'] = 0\n#for checking the number of items we will end up with \n# subset_df = diagnosis7_df[diagnosis7_df['Percentage_ClmDiagnosisCode_7'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosis7_df.drop(['ClmDiagnosisCode_7'], axis = 1) \ndiagnosis7_df['ClmDiagnosisCode_7'] = diagnosis7_df.index\nMaster_df = pd.merge(Master_df, diagnosis7_df, how='inner', on='ClmDiagnosisCode_7')\nMaster_df.loc[Master_df['Percentage_ClmDiagnosisCode_7'] == 0, 'ClmDiagnosisCode_7'] = 0\nMaster_df.tail(5)","d639d9e5":"diagnosis8 = Master_df['ClmDiagnosisCode_8'].value_counts()\ndiagnosis8_df = diagnosis8.to_frame()\ndiagnosis8_df ['Percentage_ClmDiagnosisCode_8'] = (diagnosis8_df['ClmDiagnosisCode_8']\/diagnosis8_df['ClmDiagnosisCode_8'].sum())*100\ndiagnosis8_df ['Percentage_ClmDiagnosisCode_8'] = diagnosis8_df['Percentage_ClmDiagnosisCode_8'].cumsum()\ndiagnosis8_df.loc[diagnosis8_df['Percentage_ClmDiagnosisCode_8'] > 70, 'Percentage_ClmDiagnosisCode_8'] = 0\n#for checking the number of items we will end up with \n# subset_df = diagnosis8_df[diagnosis8_df['Percentage_ClmDiagnosisCode_8'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosis8_df.drop(['ClmDiagnosisCode_8'], axis = 1) \ndiagnosis8_df['ClmDiagnosisCode_8'] = diagnosis8_df.index\nMaster_df = pd.merge(Master_df, diagnosis8_df, how='inner', on='ClmDiagnosisCode_8')\nMaster_df.loc[Master_df['Percentage_ClmDiagnosisCode_8'] == 0, 'ClmDiagnosisCode_8'] = 0\nMaster_df.tail(5)","0438348e":"diagnosis9 = Master_df['ClmDiagnosisCode_9'].value_counts()\ndiagnosis9_df = diagnosis9.to_frame()\ndiagnosis9_df ['Percentage_ClmDiagnosisCode_9'] = (diagnosis9_df['ClmDiagnosisCode_9']\/diagnosis9_df['ClmDiagnosisCode_9'].sum())*100\ndiagnosis9_df ['Percentage_ClmDiagnosisCode_9'] = diagnosis9_df['Percentage_ClmDiagnosisCode_9'].cumsum()\ndiagnosis9_df.loc[diagnosis9_df['Percentage_ClmDiagnosisCode_9'] > 70, 'Percentage_ClmDiagnosisCode_9'] = 0\n#for checking the number of items we will end up with \n# subset_df = diagnosis9_df[diagnosis9_df['Percentage_ClmDiagnosisCode_9'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosis9_df.drop(['ClmDiagnosisCode_9'], axis = 1) \ndiagnosis9_df['ClmDiagnosisCode_9'] = diagnosis9_df.index\nMaster_df = pd.merge(Master_df, diagnosis9_df, how='inner', on='ClmDiagnosisCode_9')\nMaster_df.loc[Master_df['Percentage_ClmDiagnosisCode_9'] == 0, 'ClmDiagnosisCode_9'] = 0\nMaster_df.tail(5)","e3a0c25d":"diagnosisGr = Master_df['DiagnosisGroupCode'].value_counts()\ndiagnosisGr_df = diagnosisGr.to_frame()\ndiagnosisGr_df ['Percentage_DiagnosisGroupCode'] = (diagnosisGr_df['DiagnosisGroupCode']\/diagnosisGr_df['DiagnosisGroupCode'].sum())*100\ndiagnosisGr_df ['Percentage_DiagnosisGroupCode'] = diagnosisGr_df['Percentage_DiagnosisGroupCode'].cumsum()\ndiagnosisGr_df.loc[diagnosisGr_df['Percentage_DiagnosisGroupCode'] > 70, 'Percentage_DiagnosisGroupCode'] = 0\n#for checking the number of items we will end up with \n# subset_df = diagnosisGr_df[diagnosisGr_df['Percentage_DiagnosisGroupCode'] != 0]\n# subset_df.count()\n#diagnosis1_df.head()\ndiagnosisGr_df.drop(['DiagnosisGroupCode'], axis = 1) \ndiagnosisGr_df['DiagnosisGroupCode'] = diagnosisGr_df.index\nMaster_df = pd.merge(Master_df, diagnosisGr_df, how='inner', on='DiagnosisGroupCode')\nMaster_df.loc[Master_df['Percentage_DiagnosisGroupCode'] == 0, 'DiagnosisGroupCode'] = 0\nMaster_df.tail(5)","a7bb7792":"Master_df = pd.get_dummies(Master_df,columns=['ClmAdmitDiagnosisCode', 'ClmDiagnosisCode_1','ClmDiagnosisCode_10', 'ClmDiagnosisCode_2','ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5', 'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7' , 'ClmDiagnosisCode_8', 'ClmDiagnosisCode_9', 'DiagnosisGroupCode'], prefix='DiagnosisCodeA')\nMaster_df.tail()","e8f3347d":"for_PCA_diag = Master_df.loc[:,Master_df.columns.str.startswith(\"DiagnosisCode\")]\nfor_PCA_diag.head()","555a14cc":"# #Master_df[Master_df.columns[pd.Series(Master_df.columns).str.startswith('ClmDiagnosisCode')]]\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = .90) # all the PCA features explaining more than 1% (I verified it by trying and checking the explained_variance_ratio_)\nreduced = pca.fit_transform(for_PCA_diag)\n","f443324b":"var = pca.explained_variance_ratio_.cumsum()\nplt.plot(var)","057e30d0":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n# #Master_df.columns[pd.Series(Master_df.columns).str.startswith('AdmitDiagnosis')].tolist()","bd68190d":":) Work in progress ... ","11d7ebb9":"**ClmDiagnosisCode_6**","6def325d":"Finding the top 80% of the diagnosis codes and making all others 0 from the following fields and then perform one hot encoding over the same \n* ClmAdmitDiagnosisCode              \n* ClmDiagnosisCode_1                  \n* ClmDiagnosisCode_10                \n* ClmDiagnosisCode_2                 \n* ClmDiagnosisCode_3                 \n* ClmDiagnosisCode_4                 \n* ClmDiagnosisCode_5                 \n* ClmDiagnosisCode_6                 \n* ClmDiagnosisCode_7                 \n* ClmDiagnosisCode_8                 \n* ClmDiagnosisCode_9\n* DiagnosisGroupCode                 \n","195f6d13":"For procedures, diagnosis codes and Physician let us try one hot encoding and then follwed by PCA \n\n\n**DIAGNOSIS** \n\nPlease note: \n1. Number of diagnosis codes and procedure codes are huge and doing a one hot encoding on the same we will explode the data set - so we are taking the top 80%(cumilatively) used items for one hot encoding and then performing a PCA on them all together.\n(example - performing one hot encoding for all the diagnosis related fields first and then performing PCA on all of them together)\n\nWe can do this because many times the diagnosis codes might be present on any of the columns and it really does not matter at which position the diagnosis code was used at as that might depend on factors such as if the patient is visiting a specialist or a general physician, or if a base diagnosis code is applied or a subbordinate one which in the later case cannot be the diagnosis code 1. \n\nAll we care for here is if we can find a pattern in the usability of the codes all together and the same logic can be applied for procedure codes as well.","58d39713":"# OUTPATIENT","2a23a205":"**ClmDiagnosisCode_4**","1d7e109f":"One hot encoding for Claim Admit diagnosis ","97e5c397":"# Exploratory Data Analysis \n\n* Looking for the most common procedure codes which are applied for the fradulent and non fradulent services to see any specific pattern ","779de480":"# Section 2","13317e42":"**ClmDiagnosisCode_9**","bb821937":"Calculating the number of days the patient was admitted to the dospital and removing admission and discharge date, For outpatients as they do not get admitted will put number of days admitted = 0 ","59ecdbd9":"241288510 - around 240 Million dollars worth of claim might have some fradulent activity. Even if we assume that it has just 10% fradulent activity the amount will be quite huge","6f5ee06c":"We see a minor difference between the most used diagnosis and procedure codes between inpatient and outpatients \n\nWe see that for inpatinet the most common procedure used is 9904, 3722, 4516 among others\n\nWe see that for inpatinet the most common Diagnosis used is 4019, 25000, 2724 among others","92cceeba":"# 4.Which states\/localities have the highest number of potential frauds \nData contains both inpatient and outpatient encounters","f581a5db":"Here too we see a similar pattern","776bbb37":"Only the date of death is empty - makes sense for the people who are alive","85b7f20e":"Please note the colors for the following:\nInpatient -> green\nOutpatient -> cayan\nCombined -> blue ","391c88a4":"# **Let's see how many providers are fraudulent  **","24a63651":"Inpatient data as a whole not just the fradulent activities","72335f37":"# INPATIENT","1845f12f":"Let us create the data joining beneficiaries, Inpatient and out patient data and then we will see if any data conversion is required or not","337f4d03":"**ClmDiagnosisCode_10**","f7e78213":"# INPATIENT\nPotential fradulent encounters analysis of procedures and diagnosis codes used","59dce4e6":"# 2. What are the most common procedures and diagnosis codes performed by the potential fradulent providers","5ac8d3ee":"A short description of the problem at hand ","c6adb86c":"# Inference \n\nSo we see there are **189394** outpatient cases that the potential fradulent providers have interacted with at one point or the other during their services at the hospital. This is around **37%** of the cases which we have in our inpatient data. \n\n***This means from our outpatient dataset for training we can have fradulent activities on around 38% of encounters***","2cc8cd08":"**ClmDiagnosisCode_2**","3dd13c1c":"For the problem at hand and the data which we have we will go forward with our analysis in the structured manner as discussed below \n    1. Section 1 \n           a. Rough analysis and understanding of the data and identify how useful the data is for analysis \n           b. Try answering the below questions :\n                i.   What are the most common Procedures and Diagnosis codes\n                ii.  What are the most common procedures and diagnosis codes performed by the potential fradulent providers\n                iii. How many providers submit potential fraud claims\n                iv.  Which states\/localities have the highest number of frauds\n                v.   What is the average cost of potential fraud claims and also what is the cost as a % of whole a. Checking the outliers for such claims\n                vi.  Is there any average age on which most of the time fradulent procedures or claims are applied on\n\n\n    2. Section 2\n            a. Descriptive analysis of the data \n            b. Missing values\n            c. Outliers \n            d. Prepare the data\n            \n            \n    3. Section 3 \n            a. Apply models \n\n\n\n    ","f382c4be":"# Inpatient Data ","4ee3c1a4":"This seems logical as most of the patients are of an age >65 ","9c342a4d":"**ClmDiagnosisCode_3**","3f45db72":"It makes sense most of the outpatient claims are of a smaller amount ","bf75bd76":"# What is the average cost of potential fraud claims and also what is the cost as a % of whole a. Checking the outliers for such claims","cd31e2c6":"Here too we see a similar pattern with a little variation in the type of procedures used for fradulent activities ","1b062543":"**ClmDiagnosisCode_1**","54ed0235":"# OUTPATIENT","459b4625":"Adding the columns - Age","a182f3e2":"# Now let us check which were the most used procedure codes and diagnosis codes used by the potential fradulent providers ","8c70d14d":"We see that it is a significantly large amount which might be fradulent.","8219a673":"**ClmDiagnosisCode_5**","db207bfd":"**ClmDiagnosisCode_8**","17e4bc62":"# OUTPATIENT \nPotential fradulent encounters analysis of procedures and diagnosis codes used","2a09413b":"# **Section 1**","ad15d020":"# INPATIENT","360ef5c2":"A little variation from the inpatient data if we knew the actual procedure codes we could identify why this was the case. ","bd0d31f7":"**DiagnosisGroupCode**","5cda1d71":"Checking for missing values in the data set ","900f144f":"# Average Age for the data set and as a comparison for the probable fradulent activites applied on what age range","fb0fd461":"**ClmDiagnosisCode_7**","6ca40e06":"We see that for inpatinet the most common procedure used is 4019, 9904, 2724 among others\n\nWe see that for inpatinet the most common **Diagnosis** used is 4019, 2724,25000 among others","01066836":"# OUTPATIENT","db9d811f":"# Inference \n\nSo we see there are **23402** admitted(inpatients) cases that the potential fradulent providers have interacted with at one point or the other during their services at the hospital. This is around **58%** of the cases which we have in our inpatient data. \n\n***This means from our inpatient dataset for training we can have fradulent activities on more than half of them - 58% are potential fradulent encounters***","aad3d98e":"**ClmAdmitDiagnosisCode**"}}