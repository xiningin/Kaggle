{"cell_type":{"38631bc0":"code","ebbba693":"code","24ac5eab":"code","b063c507":"code","14f6b8f6":"code","78addb48":"code","1c10f38a":"code","356e87c4":"code","db9e5bdb":"code","261ee1b9":"code","04d8c754":"code","304de48b":"code","3a145204":"code","57cc0b09":"code","c639bb1d":"code","a4946ce1":"code","cbd21e69":"code","f68d61d8":"code","c29a1eaa":"code","9cfb5bd4":"code","7748fe1b":"markdown","3fdd80a3":"markdown","6d4a2efe":"markdown","f892009a":"markdown"},"source":{"38631bc0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# pd.set_option(\"display.max_rows\", None)\npd.set_option('display.max_rows', None)  # or 1000 to get all the rows for our dataframe\")","ebbba693":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\")","24ac5eab":"train_df.head(10)","b063c507":"test_df.head(10)","14f6b8f6":"Summary = pd.DataFrame(train_df.dtypes, columns=['Dtype'])\nSummary[\"max\"] = train_df.max()\nSummary[\"min\"] = train_df.min()\nSummary[\"Null\"] = train_df.isnull().sum() # to get null values\nSummary[\"First\"] = train_df.iloc[0] # to get first value\nSummary[\"Second\"] = train_df.iloc[1] # to get second value\nSummary","78addb48":"y = train_df[\"loss\"]\ntrain_df = train_df.drop([\"id\"], axis=1)\ntrain_df = train_df.drop([\"loss\"], axis=1)","1c10f38a":"print(len(train_df)) # 250,000","356e87c4":"test_df = test_df.drop([\"id\"], axis=1)","db9e5bdb":"train_df[\"max_value\"] = train_df.max(axis = 1)\ntrain_df[\"min_value\"] = train_df.min(axis = 1 )\ntrain_df[\"mean\"] = train_df.mean(axis = 1)\ntrain_df[\"median\"] = train_df.median(axis = 1)\ntrain_df[\"std\"] = train_df.std(axis = 1)\n# train_df[\"unique\"] = train_df.nunique(axis = 1)\nprint(train_df[\"median\"].tail(10))","261ee1b9":"\ntest_df[\"max_value\"] = test_df.max(axis = 1)\ntest_df[\"min_value\"] = test_df.min(axis = 1 )\ntest_df[\"mean\"] = test_df.mean(axis = 1)\ntest_df[\"median\"] = test_df.median(axis = 1)\ntest_df[\"std\"] = test_df.std(axis = 1)\nprint(test_df[\"max_value\"].tail(10))","04d8c754":"y.value_counts() # we can see that there are many values to y and they are not equally assigned","304de48b":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom xgboost import XGBRegressor, XGBClassifier\n# import lightgbm\n\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, stratify = y, random_state = 123, test_size = 0.1)","3a145204":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx = scaler.fit_transform(train_df)\ntest_X = scaler.transform(test_df)","57cc0b09":"# def objective( trial, data = x, target = y ):\n    \n#     X_train, X_test, y_train, y_test = train_test_split( data, target, test_size = 0.25, random_state = 123 )\n#     params = {\n#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 2000, 100),\n#         \"subsample\": trial.suggest_discrete_uniform(\"subsample\", 0.6, 1, 0.1),\n#         \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\", 0.6, 1, 0.1),\n#         \"eta\": trial.suggest_loguniform(\"eta\", 1e-3, 0.1),\n#         \"reg_alpha\": trial.suggest_int(\"reg_alpha\", 1, 50),\n#         \"reg_lambda\": trial.suggest_int(\"reg_lambda\", 5, 100),\n#         \"max_depth\": trial.suggest_int(\"max_depth\", 5, 20),\n#         \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 5, 20),\n#     }\n#     model = xgb.XGBRegressor(**params, tree_method='gpu_hist', random_state=42)\n#     model.fit(X_train, y_train, eval_set = [ (X_test, y_test) ],verbose = False, eval_metric='rmse')\n\n#     y_preds = model.predict(X_test)\n#     loss = np.sqrt(mean_squared_error(y_test, y_preds))\n    \n#     return loss","c639bb1d":"import optuna\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import *\n\nOPTUNA_OPTIMIZATION = True\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","a4946ce1":"# if OPTUNA_OPTIMIZATION:\n#     display(optuna.visualization.plot_optimization_history(study))\n#     display(optuna.visualization.plot_slice(study))\n#     display(optuna.visualization.plot_parallel_coordinate(study))","cbd21e69":"study.best_trial.params","f68d61d8":"x.shape","c29a1eaa":"# from sklearn.model_selection import StratifiedKFold\n\n# xgb_params = study.best_trial.params\n# xgb_params['tree_method'] = 'gpu_hist'\n# xgb_params['random_state'] = 123\n# test_preds=None\n\n# print(\"\\033[91mTraining........\")\n\n# kf = StratifiedKFold(n_splits = 10 , shuffle = True , random_state = 42)\n# for fold, (tr_index , val_index) in enumerate(kf.split(x , y)):\n    \n#     print(\"\u2059\" * 10)\n#     print(f\"Fold {fold + 1}\")\n    \n#     x_train,x_val = x[tr_index] , x[val_index]\n#     y_train,y_val = y[tr_index] , y[val_index]\n        \n#     eval_set = [(x_val, y_val)]\n    \n#     model =xgb.XGBRegressor(**xgb_params)\n#     model.fit(x_train, y_train, eval_set = eval_set, verbose = False)\n    \n#     train_preds = model.predict(x_train)    \n#     val_preds = model.predict(x_val)\n    \n#     print(np.sqrt(mean_squared_error(y_val, val_preds)))\n    \n#     if test_preds is None:\n#         test_preds = model.predict(test_X)\n#     else:\n#         test_preds += model.predict(test_X)\n\n# print(\"-\" * 50)\n# print(\"\\033[97mTraining Done\")\n\n# test_preds \/= 10","9cfb5bd4":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\nsubmission['loss']=test_preds\nsubmission.to_csv(\"subcat.csv\",index=False)","7748fe1b":"### Reading the Data","3fdd80a3":"### Adding extra features","6d4a2efe":"Values need to be scaled but that is for later.","f892009a":"There are some 100 columns in here as we can see that"}}