{"cell_type":{"72547332":"code","92166853":"code","24163c69":"code","e5467543":"code","4e31cd7e":"code","49957b01":"code","a28d3143":"code","95c7f2f5":"code","d696810b":"code","64f1b4f2":"code","688b9cda":"code","37a2eed9":"code","501a0ecb":"code","5b99f3e7":"code","aada4f23":"code","abca3a22":"code","97278f74":"code","70635c61":"code","e3c93ff3":"code","9efb6a4b":"code","8b936259":"code","040f0e1d":"code","f86ead37":"code","9ebf2cf2":"code","a09828df":"code","d77ef0bf":"code","d7aae83f":"code","9ae02f59":"code","266a68e6":"code","13edb241":"code","3f8a1555":"code","7d022bec":"code","3f03d41a":"code","58a5efd6":"code","8f7e08ad":"code","9275e798":"code","94bcf46f":"code","ad6fe5b7":"code","450a878a":"code","adecfe22":"code","203e6ef6":"code","3a93dfd3":"code","7e10b097":"code","0bd100a6":"code","96e0d017":"code","eb8e46d6":"code","331ff376":"code","968b3b26":"code","423ca5da":"code","2aa2517f":"code","6bf7e5b3":"code","22bdfc49":"code","7f0a5bd6":"code","6b39ad67":"code","a795fc38":"code","c3631603":"code","3d1faae4":"code","2b88156f":"code","e8ad57ed":"code","f3c0f59f":"code","06f5cda7":"code","b63a3433":"code","a1bf6848":"code","d856a8d7":"code","ca4d2ba5":"code","6b8b45b9":"code","95cf1825":"code","6a3dab60":"code","4aefdbce":"code","e62e491c":"code","63da67d7":"markdown","393f0738":"markdown","41d900a6":"markdown","33084d12":"markdown","5ed9887a":"markdown","a78bd032":"markdown","df175ab6":"markdown","d6007a87":"markdown","95463e3e":"markdown","c44a7f1a":"markdown","b516c3bd":"markdown","f44f0efe":"markdown","2ed0a0ed":"markdown","f094f4e7":"markdown","d65f45a7":"markdown","10e88c8b":"markdown","2d70a254":"markdown","f00b21ae":"markdown","263919e8":"markdown","8db824a0":"markdown","bb7a311f":"markdown","8b4345a6":"markdown","89f7970b":"markdown","cbd6c8a6":"markdown","26258ebd":"markdown","6bec197a":"markdown","caba55c7":"markdown","ea9504b3":"markdown","56772942":"markdown","18f016dd":"markdown","6be2f306":"markdown"},"source":{"72547332":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm #Color Map, make color map\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA \nfrom sklearn.metrics import silhouette_samples, silhouette_score #Know the ideal number of clusters\nfrom sklearn.ensemble import RandomForestClassifier \nfrom collections import Counter","92166853":"data = pd.read_csv('..\/input\/fifa19\/data.csv')","24163c69":"data.shape","e5467543":"data.head()","4e31cd7e":"#create indexes: will start at zero until the number of columns\n\nfor i, col in enumerate(data.columns):\n    print(i, col)","49957b01":"cols = [21, 26, 27]\ncols += range(54, 83)","a28d3143":"data = data.iloc[:, cols]","95c7f2f5":"data.head()","d696810b":"#missing values:\ndata.isna().sum(axis = 0)","64f1b4f2":"len(data) - len(data.dropna())","688b9cda":"(60 \/ len(data)) * 100","37a2eed9":"data = data.dropna()","501a0ecb":"data.isna().sum(axis = 0)","5b99f3e7":"def hist_boxplot(feature):\n  fig, ax = plt.subplots(1, 2)\n  ax[0].hist(feature)\n  ax[1].boxplot(feature)","aada4f23":"data_stats = data.describe()\ndata_stats","abca3a22":"hist_boxplot(data_stats.loc['min'])","97278f74":"hist_boxplot(data_stats.loc['mean'])","70635c61":"hist_boxplot(data_stats.loc['max'])","e3c93ff3":"data['Height'].head()","9efb6a4b":"data['Height'] = data['Height'].str.split('\\'')\ndata['Height'] = [30.48 * int(elem[0]) + 2.54 * int(elem[1]) for elem in data['Height']]\nhist_boxplot(data['Height'])","8b936259":"data['Weight'].head()","040f0e1d":"data['Weight'] = data['Weight'].str.split('l')\ndata['Weight'] = [int(elem[0]) * 0.453 for elem in data['Weight']]\nhist_boxplot(data['Weight'])","f86ead37":"position = np.array(data['Position'])\nnp.unique(position, return_counts = True)","9ebf2cf2":"data = data.drop(['Position'], axis = 1)\ndata.head()","a09828df":"scaler = MinMaxScaler()\ntrain = scaler.fit_transform(data)","d77ef0bf":"train","d7aae83f":"wcss = []\nK = range(1, 12) #we have 11 players in a game\nfor k in K:\n    KM = KMeans(n_clusters = k)\n    KM = KM.fit(train)\n    wcss.append(KM.inertia_)","9ae02f59":"plt.plot(K, wcss, 'bx-')\nplt.xlabel('k')\nplt.ylabel('WCSS')\nplt.title('Elbow Method');","266a68e6":"pca = PCA(n_components = 2)\ndata_pca = pca.fit_transform(train)\npca.explained_variance_ratio_\nexp_var = [round(i, 1) for i in pca.explained_variance_ratio_ * 100]","13edb241":"range_n_clusters = range(2, 12)\nfor n_clusters in range_n_clusters:\n  fig, (ax1, ax2) = plt.subplots(1, 2)\n  fig.set_size_inches(18, 7)\n\n  ax1.set_xlim([-0.1, 1])\n  ax1.set_ylim([0, len(train) + (n_clusters + 1) * 10])\n\n  clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n  cluster_labels = clusterer.fit_predict(train)\n  #print(cluster_labels)\n  #print(np.unique(cluster_labels))\n\n  silhouette_avg = silhouette_score(train, cluster_labels)\n  print(\"For n_clusters = \", n_clusters, \" Average score: \", silhouette_avg)\n\n  sample_silhouette_values = silhouette_samples(train, cluster_labels)\n  #print(sample_silhouette_values)\n  #print(len(sample_silhouette_values))\n\n  y_lower = 10\n  for i in range(n_clusters):\n    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n    ith_cluster_silhouette_values.sort()\n    #print(ith_cluster_silhouette_values.shape)\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n\n    y_upper = y_lower + size_cluster_i\n    #print(y_upper)\n    \n    ax1.fill_betweenx(np.arange(y_lower, y_upper), ith_cluster_silhouette_values)\n\n    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n    y_lower = y_upper + 10\n\n  ax1.set_title(\"The silhouette plot for the various clusters\")\n  ax1.set_xlabel(\"The silhouette coefficient values\")\n  ax1.set_ylabel(\"Cluster label\")   \n\n  ax1.axvline(x = silhouette_avg, color = \"red\", linestyle = \"--\")\n\n  ax1.set_yticks([])\n  ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n  colors = cm.nipy_spectral(cluster_labels.astype(float) \/ n_clusters)\n  ax2.scatter(data_pca[:, 0], data_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')\n\n  centers = clusterer.cluster_centers_\n  centers = pca.transform(centers)\n  ax2.scatter(centers[:, 0], centers[:,1], marker='o', c='white', alpha=1, s=200, edgecolor='k')\n\n  for i, c in enumerate(centers):\n    ax2.scatter(c[0], c[1], marker='$%d$' % i, s=50, edgecolor='k')\n\n  ax2.set_title(\"The visualization of the clustered data\")\n  ax2.set_xlabel('PC1 (' + str(exp_var[0]) + '% variance explained')\n  ax2.set_ylabel('PC2 (' + str(exp_var[1]) + '% variance explained')\n\n  plt.suptitle((\"Silhouette analysis for Kmeans clustering on sample data with n_clusters = %d\" % n_clusters),\n               fontsize=14, fontweight='bold')","3f8a1555":"km = KMeans(n_clusters = 4, n_init = 100, random_state = 0)\nkm.fit(train)","7d022bec":"print(km.cluster_centers_) #it shows the position of the centers","3f03d41a":"position[2]","58a5efd6":"group = km.labels_ #will return in these records to which group it belongs (km.labels_)\ncomp = []\nfor i in range(0, len(position)):\n    ele = tuple((position[i], group[i]))\n    comp.append(ele)","8f7e08ad":"comp[0:4]","9275e798":"#how many records per group\ncount = Counter(comp)\ncount","94bcf46f":"comp = pd.DataFrame({'Position': [i[0] for i in list(count.keys())],\n                     'Group': [i[1] for i in list(count.keys())],\n                     'Numbers': list(count.values())})","ad6fe5b7":"comp.head()","450a878a":"comp.shape","adecfe22":"#ordering groups according to their position:\ncomp = comp.sort_values(['Position', 'Group'])\ncomp.head()","203e6ef6":"#analyzing in percentage\ncomp_per = pd.DataFrame()\npos = comp['Position'].unique()\npos","3a93dfd3":"for p in pos:\n    comp_p = comp[comp['Position'] == p] \n    sum_N = sum(comp_p['Numbers'])\n    comp_p['Numbers'] = comp_p['Numbers'] \/ sum_N\n    comp_per = comp_per.append(comp_p)\ncomp_per = comp_per.sort_values(['Group', 'Numbers', 'Position'])","7e10b097":"comp_per.head()","0bd100a6":"comp_per.tail()","96e0d017":"#let's create a bar graph with the percentage of frequencies for each group:\n\ncomp_barplot = pd.DataFrame({'Position': sum([[ele] * 4 for ele in np.unique(position)], []),\n                             'Group': sum([['0', '1', '2', '3'] * len(np.unique(position))], []),\n                             'Numbers': [0] * 4 * len(np.unique(position))})","eb8e46d6":"comp_barplot.head(10)","331ff376":"#we will now find the value of 'Numbers':\n\nfor row in range(0, len(comp_barplot)):\n    pos = comp_barplot.iloc[row, 0]\n    gro = int(comp_barplot.iloc[row, 1])\n    reg = comp_per.loc[(comp_per['Position'] == pos) & (comp_per['Group'] == gro), :]\nif len(reg) > 0:\n    comp_barplot.iloc[row, 2] = reg['Numbers'].values","968b3b26":"comp_barplot.head()","423ca5da":"comp_barplot.tail()","2aa2517f":"counter1 = Counter(comp_per[comp_per['Numbers'] >= 0.5]['Group'])\ncounter1","6bf7e5b3":"#for security and warranty, we will leave this ordering code\ncounter1 = dict(sorted(counter1.items(), key = lambda x: x[0]))\ncounter1","22bdfc49":"x = [str(ele) for ele in list(counter1.keys())]\nx","7f0a5bd6":"p1 = plt.bar(x, counter1.values())\ncounter2 = Counter(comp_per[comp_per['Numbers'] < 0.5]['Group'])\ncounter2 = dict(sorted(counter2.items(), key = lambda x: x[0]))\nx = [str(ele) for ele in list(counter2.keys())]\n\np2 = plt.bar(x, counter2.values(), bottom=list(counter1.values()))\nplt.title('Number of positions designated to each group')\nplt.xlabel('Group')\nplt.ylabel('Number of positions')\nplt.legend((p1[0], p2[0]), ('Proportion >= 0.5', 'Proportion < 0.5'))","6b39ad67":"#most frequent positions for each group:\nfor i in range(4):\n    g = comp_per[(comp_per['Group'] == i) & (comp_per['Numbers'] >= 0.5)][['Position', 'Numbers']]\n    g = g.sort_values(by = 'Numbers')\n    plt.barh(g['Position'], g['Numbers'])\n    plt.axvline(0.5, color = 'r', linestyle = '--')\n    plt.title('Positions best associated with Group ' + str(i))\n    plt.show()","a795fc38":"rf = RandomForestClassifier()\nrf.fit(train, group)","c3631603":"importances = rf.feature_importances_\nimportances","3d1faae4":"features = data.columns\nimp = pd.DataFrame({'Features': features, 'Importance': importances})\nimp.head()","2b88156f":"#the 5 most important features\n\nimp = imp.sort_values(by = 'Importance', ascending = False)\nimp.head()","e8ad57ed":"#the 5 least important features\n\nimp.tail()","f3c0f59f":"#accumulated sum\n\nimp['Sum Importance'] = imp['Importance'].cumsum()\nimp = imp.sort_values(by = 'Importance')\nimp.head()","06f5cda7":"imp.tail()","b63a3433":"plt.figure(figsize=(8,8))\nplt.barh(imp['Features'], imp['Importance'])\nl1 = plt.axhline(len(imp) - (len(imp['Features'][imp['Sum Importance'] < 0.50]) + 1.5), linestyle='-.', color = 'r')\nl2 = plt.axhline(len(imp) - (len(imp['Features'][imp['Sum Importance'] < 0.90]) + 1.5), linestyle='--', color = 'r')\nl3 = plt.axhline(len(imp) - (len(imp['Features'][imp['Sum Importance'] < 0.99]) + 1.5), linestyle='-', color = 'r')\nplt.legend(title = 'Cut-offs of acumulated importance', handles=(l1, l2, l3), labels = ('50%', '90%', '99%'))\nplt.title('Feature importance in group assignment')","a1bf6848":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nX_train, X_test, Y_train, Y_test = train_test_split(train, group, test_size = 0.2)","d856a8d7":"import seaborn as sns\n\nsns.countplot(Y_train)","ca4d2ba5":"rf = RandomForestClassifier()\nrf.fit(X_train, Y_train)\n\npred = rf.predict(X_test)\naccuracy_score(pred, Y_test)","6b8b45b9":"cm = confusion_matrix(pred, Y_test)\ncm","95cf1825":"from yellowbrick.classifier import ConfusionMatrix\nconfusion_matrix = ConfusionMatrix(rf)\nconfusion_matrix.fit(X_train, Y_train)\nconfusion_matrix.score(X_test, Y_test)\nconfusion_matrix.show();","6a3dab60":"new = X_test[0]\nnew","4aefdbce":"new = new.reshape(1, -1)\n#will return which group it belongs to\nrf.predict(new) ","e62e491c":"#using probability of belonging to each group\nrf.predict_proba(new)","63da67d7":"A new one has entered and now a simulation of which group and e will be allocated:","393f0738":"As we can see, most of the minimum values are more distributed up to the value 16, 17 for example, with a large gap between the 15 and the 20. On the side graph, it is clear that the values are more concentrated in the value 5,0.","41d900a6":"The 21 is the Position of the player. We will creat a new variable to get the Position, Height, Weight and the technical characteristics. We will get the 21, 26, 27 and it'll concatenate with the 54 until 82 because it is the technical parts:","33084d12":"We will now make a comparison and see if the position of the player with the group in which he was harassed.","5ed9887a":"# **Using Random Forest for classification:**","a78bd032":"Create a \"for\" to view each of the attributes:","df175ab6":"In this accumulated sum it shows that Interceptions and StandingTackle represent 21% of the importance of all characteristics. That is, these characteristics added together represent 44% of the importance.","d6007a87":"# **K - Means**","95463e3e":"We will now use a scale so that the values are between 0 and 1, we will do this because we will use KMeans to perform the grouping, as we can see the height has higher values than the Heading Accuracy, which affects the model. With that we will leave them in closer values for better results.","c44a7f1a":"# **Studying the weight of the players**","b516c3bd":"# **Preparing the database:**","f44f0efe":"# **We will use the technical characteristics of the player to know which group he fits into**","2ed0a0ed":"How we can see, there is 60 missing values in Position. We can see if the missing values of the others attributes have a relationship with the missing values for the Position:","f094f4e7":"I will convert the height to centimeters:","d65f45a7":"# **Conclusions:**\nThese analyzes give us the conclusion that we can separate the players into 4 groups. And it also allows us to be aware of the most important characteristics for the realization of the grouping and in this way to be able to improve efficiency. You can also have the result if that player is better in that position, considering his characteristics or if by them, it is better to be allocated to other groups, where he can show better performance. **Enables the best allocation of the player.**","10e88c8b":"Now let's creat a new data fram with the columns we will usw to make the clusters of the FIFA's players.","2d70a254":"Let's convert to kilogram","f00b21ae":"Following the rule of the elbow method, we will choose number 4.","263919e8":"#  **Studying player height**","8db824a0":"The histogram shows an interval between values 92 to approximately 93. As well as there is an interval between values 94 and 95 approximately. On the side we see a greater concentration at 95.","bb7a311f":"Choosing the number of clusters with the silhouette method:\n\n* Takes into account cohesion and separation\n* Cohesion: measure of distance from one point to all other points in the same group\n* Separation: measure the distance of a point with the points of the other clusters\n* The coefficient is in the range of [-1, 1]\n* The value -1 indicates that the cluster is bad (records in \"wrong\" groups)\n* The value 0 indicates \"indifference\" (above the decision line, one cluster next to the other)\n* The value 1 indicates that the clusters are far apart (the closer to 1, the better)","8b4345a6":"# **Random Forest**","89f7970b":"Now we can delete the Position column from the database","cbd6c8a6":"# **Analyzing the statistics:**","26258ebd":"Here we see that the values are more concentrated in the range between 50 and 55.","6bec197a":"Now we will use the PCA:","caba55c7":"We can see the 60 records that have no Position, also do not have the other records, so we will delete these 60 records with missing data","ea9504b3":"All passed the red line (which is the silhouette), the greater the number of clusters, the closer the cluetrs are, with more mixtures occurring and the groups passing less and less than the red line. The more you cross the red line, the more distributed the group is. For visualizations, the ideal number of clauster would be with the number 4 or 5. As we saw earlier, using the elbow method, the best was n_clusters = 4.","56772942":"**Import the libraries we will use:**","18f016dd":"# **Predicting the player**","6be2f306":"# Using the WCSS:"}}