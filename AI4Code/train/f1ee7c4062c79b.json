{"cell_type":{"17f5efe2":"code","95b94833":"code","85207b88":"code","afbb2a06":"code","02845d3c":"code","e4df5d36":"code","67baa831":"code","cf0e1af2":"code","651ebfc9":"code","472526b9":"code","c3ae40ee":"code","131ec5cf":"code","4b134818":"code","bddb36ae":"code","944a2107":"code","92baf34f":"code","338be02a":"code","b5117cd6":"code","5c356daa":"code","736435b5":"code","889df349":"code","c84c5ad2":"code","2bbb74e9":"code","466fbdba":"code","61a7a213":"code","33e54aaf":"markdown","d2995ce2":"markdown","9ec10312":"markdown","26378596":"markdown","884b30a5":"markdown","270137ac":"markdown","a0d67acf":"markdown","55d463f5":"markdown","b5d9d508":"markdown","032fdab0":"markdown","77e0947b":"markdown","f9218df4":"markdown","6d363a4c":"markdown","aff8eed3":"markdown","2f91e4d3":"markdown","6fef5060":"markdown"},"source":{"17f5efe2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","95b94833":"!pip install captum","85207b88":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as TF\n\nfrom torch.utils.data import random_split\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom torchvision.utils import make_grid\nfrom torchvision import models as models \nfrom torchvision import transforms\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\nfrom torchvision import models\n\nfrom captum.attr import IntegratedGradients\nfrom captum.attr import Saliency\nfrom captum.attr import DeepLift\nfrom captum.attr import NoiseTunnel\nfrom captum.attr import visualization as viz\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","afbb2a06":"# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","02845d3c":"input_dir = '..\/input\/flowers-recognition\/flowers\/'","e4df5d36":"#check models which are supported\ndir(models)","67baa831":"# load and transform data using ImageFolder\n\n# VGG-16 Takes 224x224 images as input, so we resize all of them\ndata_transform = transforms.Compose([transforms.RandomResizedCrop(224), \n                                      transforms.ToTensor()])\n\ndatabase = datasets.ImageFolder(input_dir, transform=data_transform)\ntest_size = 500\ntraining_size = len(database) - test_size\ntrain_ds, test_ds = random_split(database, [training_size, test_size])\n\n# print out some data stats\nprint('Num training images: ', len(train_ds))\nprint('Num test images: ', len(test_ds))\n","cf0e1af2":"classes = database.classes\nprint(classes)","651ebfc9":"# define dataloader parameters\nbatch_size = 10\nnum_workers=0\n\n# prepare data loaders\ntrain_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, \n                                           num_workers=num_workers, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, \n                                           num_workers=num_workers, shuffle=True)","472526b9":"# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy() # convert images to numpy for display\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 8))\nfor idx in np.arange(10):\n    ax = fig.add_subplot(2, 5, idx+1, xticks=[], yticks=[])\n    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n    ax.set_title(classes[labels[idx]],fontsize=30)","c3ae40ee":"# Load the pretrained model from pytorch\nm_v2 = models.mobilenet_v2(pretrained=True)\n\n# print out the model structure\n#print(m_v2)","131ec5cf":"print(m_v2.classifier[1].in_features) \nprint(m_v2.classifier[1].out_features) ","4b134818":"# Freeze training for all \"features\" layers\nfor param in m_v2.features.parameters():\n    param.requires_grad = False","bddb36ae":"import torch.nn as nn\n\nn_inputs = m_v2.classifier[1].in_features\n\nlast_layer = nn.Linear(n_inputs, len(classes))\n\nm_v2.classifier[1] = last_layer\n\n# if GPU is available, move the model to GPU\nif train_on_gpu:\n    m_v2.cuda()\n\n# check that the last layer\nprint(m_v2.classifier[1].out_features)\nprint(m_v2)","944a2107":"import torch.optim as optim\n\n# define loss function (categorical cross-entropy)\ncriterion = nn.CrossEntropyLoss()\n\n# define optimizer\noptimizer = optim.Adam(m_v2.classifier.parameters(), lr=0.01)","92baf34f":"# number of epochs to train the model\nn_epochs = 2\n\nfor epoch in range(1, n_epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    # model by default is set to train\n    for batch_i, (data, target) in enumerate(train_loader):\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = m_v2(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss \n        train_loss += loss.item()\n        \n        if batch_i % 20 == 19:    # print training loss every specified number of mini-batches\n            print('Epoch %d, Batch %d loss: %.16f' %\n                  (epoch, batch_i + 1, train_loss \/ 20))\n            train_loss = 0.0","338be02a":"# track test loss \n# over 5 flower classes\ntest_loss = 0.0\nclass_correct = list(0. for i in range(5))\nclass_total = list(0. for i in range(5))\n\nm_v2.eval() # eval mode\n\n# iterate over test data\nfor data, target in test_loader:\n    # move tensors to GPU if CUDA is available\n    if train_on_gpu:\n        data, target = data.cuda(), target.cuda()\n    # forward pass: compute predicted outputs by passing inputs to the model\n    output = m_v2(data)\n    # calculate the batch loss\n    loss = criterion(output, target)\n    # update  test loss \n    test_loss += loss.item()*data.size(0)\n    # convert output probabilities to predicted class\n    _, pred = torch.max(output, 1)    \n    # compare predictions to true label\n    correct_tensor = pred.eq(target.data.view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    # calculate test accuracy for each object class\n    for i in range(batch_size):\n        label = target.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n\n# calculate avg test loss\ntest_loss = test_loss\/len(test_loader.dataset)\nprint('Test Loss: {:.6f}\\n'.format(test_loss))\n\nfor i in range(5):\n    if class_total[i] > 0:\n        print('Test Accuracy of %5s: %2d%% (%2d\/%2d)' % (\n            classes[i], 100 * class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %5s: N\/A (no training examples)' % (classes[i]))\n\nprint('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","b5117cd6":"# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages_np = images.numpy() # convert images to numpy for display\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 8))\nfor idx in np.arange(10):\n    ax = fig.add_subplot(2, 5, idx+1, xticks=[], yticks=[])\n    plt.imshow(np.transpose(images_np[idx], (1, 2, 0)))\n    ax.set_title(classes[labels[idx]],fontsize=30)","5c356daa":"def attribute_image_features(algorithm, input, **kwargs):\n    net.zero_grad()\n    tensor_attributions = algorithm.attribute(input,\n                                              target=labels[ind],\n                                              **kwargs\n                                             )\n    \n    return tensor_attributions","736435b5":"ind = 0\nnet = m_v2\n\ninput = images[ind].unsqueeze(0)\n\nsaliency = Saliency(m_v2)\ngrads = saliency.attribute(input, target=labels[ind].item())\ngrads = np.transpose(grads.squeeze().cpu().detach().numpy(), (1, 2, 0))","889df349":"ig = IntegratedGradients(net)\nattr_ig, delta = attribute_image_features(ig, input, baselines=input * 0, return_convergence_delta=True)\nattr_ig = np.transpose(attr_ig.squeeze().cpu().detach().numpy(), (1, 2, 0))\nprint('Approximation delta: ', abs(delta))","c84c5ad2":"print('Original Image')\nprint('Predicted:', classes[labels[ind]])\noriginal_image = np.transpose((images[ind].cpu().detach().numpy() \/ 2) + 0.5, (1, 2, 0))\n\n_ = viz.visualize_image_attr(None, original_image, \n                      method=\"original_image\", title=\"Original Image\")\n\noriginal_image = np.transpose((images[ind].cpu().detach().numpy() \/ 2) + 0.5, (1, 2, 0))\n_ = viz.visualize_image_attr(grads, original_image, method=\"blended_heat_map\", sign=\"absolute_value\",\n                          show_colorbar=True, title=\"Overlayed Gradient Magnitudes\")\n\n_ = viz.visualize_image_attr(attr_ig, original_image, method=\"blended_heat_map\",sign=\"all\",\n                          show_colorbar=True, title=\"Overlayed Integrated Gradients\")","2bbb74e9":"ind = 5\nnet = m_v2\n\ninput = images[ind].unsqueeze(0)\n\nsaliency = Saliency(m_v2)\ngrads = saliency.attribute(input, target=labels[ind].item())\ngrads = np.transpose(grads.squeeze().cpu().detach().numpy(), (1, 2, 0))","466fbdba":"ig = IntegratedGradients(net)\nattr_ig, delta = attribute_image_features(ig, input, baselines=input * 0, return_convergence_delta=True)\nattr_ig = np.transpose(attr_ig.squeeze().cpu().detach().numpy(), (1, 2, 0))\nprint('Approximation delta: ', abs(delta))","61a7a213":"print('Original Image')\nprint('Predicted:', classes[labels[ind]])\noriginal_image = np.transpose((images[ind].cpu().detach().numpy() \/ 2) + 0.5, (1, 2, 0))\n\n_ = viz.visualize_image_attr(None, original_image, \n                      method=\"original_image\", title=\"Original Image\")\n\noriginal_image = np.transpose((images[ind].cpu().detach().numpy() \/ 2) + 0.5, (1, 2, 0))\n_ = viz.visualize_image_attr(grads, original_image, method=\"blended_heat_map\", sign=\"absolute_value\",\n                          show_colorbar=True, title=\"Overlayed Gradient Magnitudes\")\n\n_ = viz.visualize_image_attr(attr_ig, original_image, method=\"blended_heat_map\",sign=\"all\",\n                          show_colorbar=True, title=\"Overlayed Integrated Gradients\")\n","33e54aaf":"<span style=\"color:Blue\"> **Observation:**\n\n* Our model decided that the pixels representing the characteristics of a sunflower are important. It seems reasonable.","d2995ce2":"# Visualizing some sample data","9ec10312":"# Add Final Classifier Layer\nIf you have set the pre-trained feature extractor, add a fully-connected layer as the last classifier.","26378596":"# Loading Data using DataLoaders","884b30a5":"![Transfer Learning](https:\/\/miro.medium.com\/max\/1400\/1*lUyfhgm9mzEmtXWfxbp17w.gif)\n\nPicture Credit: https:\/\/miro.medium.com","270137ac":"**Why transfer learning?**\n\n* Training a convolution network from scratch is time-consuming and resource-intensive.\n* The more complex the model, the harder it is to train.\n* It takes a lot of effort to actually learn from scratch.\n* Finally, everything except the fully connected layer is used as a fixed feature extractor.\n* Define a new final classifier layer and apply it to training.","a0d67acf":"------------------------------------------------\n# Testing","55d463f5":"<hr style=\"border: solid 3px blue;\">\n\n# Interpreting Models\n\n![](https:\/\/d33wubrfki0l68.cloudfront.net\/c183cb531dbcc4e397e7645b28a5cbb2d47889ac\/0e818\/img\/content\/brainneural.gif)\n\nPicture Credit: https:\/\/d33wubrfki0l68.cloudfront.net\n\nWe want to understand on what basis our CNN model judged the flower type. Therefore, we want to visually understand the judgment basis of our model through the following two methods.\n\n**Saliency detection**\n\nSaliency detection refers to separating an object of interest from a background that is not of interest, and the result is a binarized image as shown below. These detection methods help you spend less time and energy in determining the most relevant parts of an image. In other words, simplifying the representation of an image to make it more meaningful and easier to analyze.\n\n**IntegratedGradients**\n\nA gradient for each pixel of the generated images will appear, and integrating these gradients will show the effect on the overall pixel output. More details about integrated gradients can be found in the original paper: https:\/\/arxiv.org\/abs\/1703.01365","b5d9d508":"# Defining Loss Function and Optimizer\nWe use cross-entropy loss and stochastic gradient descent. Choose the learning rate appropriately.\n","032fdab0":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction","77e0947b":"-----------------------------------------\n# Importing library","f9218df4":"# Training","6d363a4c":"<span style=\"color:Blue\"> **Observation:**\n\n* Our model decided that the pixels representing the Tulip characteristics were important. It seems reasonable.","aff8eed3":"--------------------------------------------------------------\n# Defining the Model\n\n**Create the model in the following order:**\n\n1. Load the pre-trained model.\n2. All weight information is \"freeze\" for use as a fixed feature extractor.\n3. The last fully connected layer is deleted.\n4. Change the last Classifier to be suitable for classifying flower images.\n\nFreezing the weight means that the weight information in the pre-trained model is not updated during training.","2f91e4d3":"# Transforming the Data\n\nIn order to perform transfer learning using a pre-trained model, images must be appropriately preprocessed. 224 x 224 images is used as input. Therefore, the input flower image must also be resized to the corresponding size.","6fef5060":"------------------------------------------------------\n# Visualizing Sample Test Results"}}