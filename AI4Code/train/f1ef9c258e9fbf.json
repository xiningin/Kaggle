{"cell_type":{"00040dc5":"code","e65eff8d":"code","bbf826fc":"code","2469b41d":"code","77339504":"code","56553667":"code","8bc80866":"code","733ae9b8":"code","d42e3d1d":"code","0250f7c4":"code","788b425e":"code","d71fc198":"code","89f337e6":"code","991ac55a":"code","5147b0dd":"code","ff210f3c":"code","5e88cdf1":"code","3723859f":"code","e6c258df":"code","5369b60b":"code","781e1a4a":"code","496aef5c":"code","882e5c72":"code","56cec1b4":"code","b85177e6":"code","242f2dd1":"code","1c9488f0":"code","d660f438":"code","2d010a46":"code","600630bc":"code","a3602b22":"code","2345c1b4":"code","2f7b7cc7":"code","36856521":"code","23f52762":"code","a91c1007":"code","04421279":"code","02f41116":"code","c48044ef":"code","465bda80":"code","4d2c6031":"code","8389c9be":"code","1a28667e":"code","42c6cd11":"code","49b016a3":"code","852de3c4":"code","929aea87":"code","f8a0b434":"code","50cd5cd8":"code","ce901f84":"code","03430ade":"code","8172a97a":"code","d3bcb012":"code","3a95d8ff":"code","429e61db":"code","54381ba4":"code","621225b1":"code","b52d3f94":"code","128a3932":"code","08d0f7a9":"code","c587ac11":"code","619cb4ae":"code","a7ffd2d9":"code","e104df8e":"code","51ce9c73":"code","9c53379a":"code","6b5fb0de":"code","078e1302":"code","8b86d425":"code","ccf5ef14":"code","043421be":"code","afa3d784":"code","c45ef2db":"code","3bfca6ef":"code","0d0a6af4":"code","854759c6":"code","bc165323":"code","f428b970":"code","8f55b938":"code","38c291d2":"code","50da2b7d":"code","5a97b615":"code","4bfdef06":"code","82d5378d":"code","33fd2b21":"code","7d8d7506":"code","b9177f97":"code","1966ec1e":"code","4bd51105":"code","e5944c33":"code","5fb982ee":"code","8b0e4fd0":"code","90e3ee33":"code","ee6b35b1":"code","9f20ff95":"code","428d8b42":"code","ee184ad7":"markdown","d518d2fd":"markdown","9a8fc3fa":"markdown","d756087d":"markdown","3b356a3d":"markdown","652e3b4b":"markdown","fe78a10a":"markdown","e634a8f7":"markdown","4e5618c8":"markdown","21a7224f":"markdown","5b50c763":"markdown","2cd1a632":"markdown","e8e79dac":"markdown","539867a1":"markdown","b7ada03c":"markdown","0163d80e":"markdown","d25ce2da":"markdown","d6dfca71":"markdown","ca12d037":"markdown","51a6b05f":"markdown","6e96b412":"markdown","31428548":"markdown","84e5f56f":"markdown","b38308c6":"markdown","1fb1c40b":"markdown","cec8cd11":"markdown","e47f4ec5":"markdown","b0b8c92c":"markdown","35a366e6":"markdown","934d55d7":"markdown"},"source":{"00040dc5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom sklearn import (datasets, model_selection)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\n\nimport matplotlib.pyplot as plt\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","e65eff8d":"iris= datasets.load_iris()\niris_df = pd.DataFrame(iris.data, columns = iris.feature_names)\niris_df['tgt'] = iris.target\niris_df.head()","bbf826fc":"part1 , part2 = model_selection.train_test_split(iris_df)\nlen(iris_df), len(part1), len(part2)","2469b41d":"from sklearn import neighbors\nmy_model = neighbors.KNeighborsClassifier()\nmy_model.fit(part1.drop(columns = 'tgt'), part1.tgt)\nmy_model.predict(part2.drop(columns = 'tgt'))","77339504":"from sklearn import metrics\n\npreds = my_model.predict(part2.drop(columns='tgt'))\nmetrics.accuracy_score(preds, part2.tgt)","56553667":"cm = metrics.confusion_matrix(part2.tgt, preds)\ncm","8bc80866":"import seaborn as sns\n\n%matplotlib inline\nsns.heatmap(cm)","733ae9b8":"#load the csv\ndata  = pd.read_csv('\/kaggle\/input\/satandgpa-lr\/SATandGPA_LinearRegression.csv')\ndata.head()","d42e3d1d":"x = data['SAT'] # input\ny = data['GPA'] # output","0250f7c4":"x_matrix = x.values.reshape(-1,1)\nprint(x.shape)\ny.shape","788b425e":"reg = LinearRegression()","d71fc198":"reg.fit(x_matrix, y)","89f337e6":"#r-square\nreg.score(x_matrix,y)","991ac55a":"#cofficient\nreg.coef_","5147b0dd":"#intercept\nreg.intercept_","ff210f3c":"new_data = pd.DataFrame(data=[1740, 1760], columns=['SAT'])\nnew_data","5e88cdf1":"reg.predict(new_data)","3723859f":"new_data['Predicted GPA'] = reg.predict(new_data)\nnew_data","e6c258df":"#plot it in scatter\n\nplt.scatter(x,y)\nyhat = reg.coef_*x_matrix + reg.intercept_\nfig = plt.plot(x, yhat, lw = 4 , c= 'orange' , label = 'reg line')\nplt.xlabel('SAT Score', fontsize = 20)\nplt.ylabel('Student GPA', fontsize = 20)\n# plt.xlim(0)\n# plt.ylim(0)\nplt.show()","5369b60b":"data = pd.read_csv('\/kaggle\/input\/multiplelineregressionsampledata\/1.02. Multiple linear regression.csv')\ndata","781e1a4a":"data.describe()","496aef5c":"#independent and dependent\n\nx = data[['SAT', 'Rand 1,2,3']]\ny = data['GPA']","882e5c72":"reg = LinearRegression()\nreg.fit(x,y)","56cec1b4":"reg.coef_","b85177e6":"reg.score(x, y)","242f2dd1":"reg.intercept_","1c9488f0":"#calc r-squar\nreg.score(x, y)","d660f438":"# R-square Formula\n\nx.shape","2d010a46":"r2 = reg.score(x,y)\nn = x.shape[0]\np = x.shape[1]\n\nadjusted_r2 = 1-(1 -r2) * (n-1)\/(n-p-1)\nadjusted_r2","600630bc":"f_regression(x,y) # it will produce two array , one is stastices and anothere is p value. P value is demed as more important","a3602b22":"p_value = f_regression(x,y)[1]\np_value.round(3)","2345c1b4":"reg_summary  = pd.DataFrame(data= x.columns.values , columns=['Features'])\nreg_summary","2f7b7cc7":"reg_summary ['cofficient'] = reg.coef_\nreg_summary ['p-value'] = p_value.round(3)","36856521":"reg_summary","23f52762":"#using standerScaller\nscaler = StandardScaler()\nscaler.fit(x)","a91c1007":"x_scale = scaler.transform(x)\nx_scale","04421279":"#regression with scale\n\nreg = LinearRegression()\nreg.fit(x_scale,y)","02f41116":"reg.coef_","c48044ef":"reg.intercept_","465bda80":"reg_summary = pd.DataFrame([['Intercept'] , ['SAT'] , ['Rand 1,2,3']] , columns=['Features'])\nreg_summary['Weights'] = reg.intercept_, reg.coef_[0], reg.coef_[1]\nreg_summary # output : intercept is call bias, it's nothing but bias","4d2c6031":"new_data = pd.DataFrame(data = [[1700,2],[1800,1]], columns = ['SAT', 'Rand 1,2,3'])\nnew_data","8389c9be":"reg.predict(new_data)","1a28667e":"#Train Test split\na = np.arange(1,101)\na","42c6cd11":"b = np.arange(501, 601)\nb","49b016a3":"a_train, a_test, b_train, b_test = train_test_split(a,b, test_size = 0.2, random_state = 42)\na_train.shape, a_test.shape","852de3c4":"a_train","929aea87":"a_test","f8a0b434":"raw_data = pd.read_csv('\/kaggle\/input\/input-file\/1.04. Real-life example.csv')\nraw_data.head()","50cd5cd8":"raw_data.describe(include='all')","ce901f84":"#data cleaning \ndata = raw_data.drop(['Model'], axis=1) # droping from the column\ndata.describe(include='all')","03430ade":"data.isnull().sum()","8172a97a":"# droping from the row\ndata_no_mv= data.dropna(axis=0)\ndata_no_mv.describe()","d3bcb012":"# we are trying to dig the outliers because from the above discriptors price have bigger jump between data like min and max p[rice]\nsns.distplot(data_no_mv['Price']) \n#solution ==> remove top 1 percent of observations","3a95d8ff":"q = data_no_mv['Price'].quantile(0.99)\ndata_1  = data_no_mv[data_no_mv['Price'] < q]\ndata_1.describe()","429e61db":"sns.distplot(data_no_mv['Price']) ","54381ba4":"sns.distplot(data_no_mv['Mileage']) ","621225b1":"# milage has the same problem as price, so we are doing same things\nq = data_1['Mileage'].quantile(0.99)\ndata_2  = data_1[data_1['Mileage'] < q]","b52d3f94":"sns.distplot(data_2['Mileage']) ","128a3932":"# EngineV has the same problem as price, so we are doing same things\n\ndata_3  = data_2[data_2['EngineV'] < 6.5]","08d0f7a9":"sns.distplot(data_3['EngineV']) ","c587ac11":"# for year, we are only keeping value greater then 1 percentile\n\nq = data_3['Year'].quantile(0.01)\ndata_4  = data_3[data_3['Year'] > q]","619cb4ae":"sns.distplot(data_4['Year']) ","a7ffd2d9":"data_cleaned = data_4.reset_index(drop=True)","e104df8e":"data_cleaned.describe(include='all')","51ce9c73":"# checking the linerity\n\nf, (ax1, ax2,ax3) = plt.subplots(1,3,sharey = True, figsize = (15,3))\nax1.scatter(data_cleaned['Year'], data_cleaned['Price'])\nax1.set_title('price and year')\n\nax2.scatter(data_cleaned['EngineV'], data_cleaned['Price'])\nax2.set_title('price and Engine')\n\nax3.scatter(data_cleaned['Mileage'], data_cleaned['Price'])\nax3.set_title('price and Mileage')","9c53379a":"# since plot are not quite linear , we are going to use log transformations\nlog_price = np.log(data_cleaned['Price'])\ndata_cleaned['log_price'] = log_price","6b5fb0de":"# checking the linerity\n\nf, (ax1, ax2,ax3) = plt.subplots(1,3,sharey = True, figsize = (15,3))\nax1.scatter(data_cleaned['Year'], data_cleaned['log_price'])\nax1.set_title('price and year')\n\nax2.scatter(data_cleaned['EngineV'], data_cleaned['log_price'])\nax2.set_title('price and Engine')\n\nax3.scatter(data_cleaned['Mileage'], data_cleaned['log_price'])\nax3.set_title('price and Mileage')\n\nplt.show()","078e1302":"# data_cleaned = data_cleaned.drop(['Price'], axis=1)\ndata_cleaned.head(10)","8b86d425":"data_cleaned.columns.values","ccf5ef14":"variables = data_cleaned[['Mileage', 'Year', 'EngineV']]\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(variables.values,i) for i in range(variables.shape[1])]\nvif['features'] = variables.columns","043421be":"vif \n# vif : vif between 1 < VIF 5 : perfectly okay\n# vif : vif = 1 : no Multicollinearity\n# vif : vif :  VIF > 5 : unacceptable","afa3d784":"#now year has way more effect on model, so we drop it, otherwise it will be overfitting\ndata_no_Multicollinearity = data_cleaned.drop(['Year'], axis=1)","c45ef2db":"data_with_dummies = pd.get_dummies(data_no_Multicollinearity, drop_first=True)\ndata_with_dummies.head()","3bfca6ef":"data_with_dummies.columns.values","0d0a6af4":"cols = ['log_price', 'Mileage', 'EngineV','Brand_BMW',\n       'Brand_Mercedes-Benz', 'Brand_Mitsubishi', 'Brand_Renault',\n       'Brand_Toyota', 'Brand_Volkswagen', 'Body_hatch', 'Body_other',\n       'Body_sedan', 'Body_vagon', 'Body_van', 'Engine Type_Gas',\n       'Engine Type_Other', 'Engine Type_Petrol', 'Registration_yes']","854759c6":"data_preprocessed = data_with_dummies[cols]\ndata_preprocessed.head()","bc165323":"targets= data_preprocessed['log_price']\ninputs = data_preprocessed.drop(['log_price'], axis=1)","f428b970":"scaler = StandardScaler()\nscaler.fit(inputs)\ninput_scaled = scaler.transform(inputs)","8f55b938":"x_train, x_test , y_train, y_test  = train_test_split(input_scaled, targets, test_size  = 0.2, random_state = 42)","38c291d2":"reg = LinearRegression()\nreg.fit(x_train, y_train)","50da2b7d":"y_hat  = reg.predict(x_train)","5a97b615":"plt.scatter(y_train, y_hat)\n\nplt.xlabel('target (y_train)', size =20)\nplt.ylabel('Predictions (y_hat)', size =20)\n\nplt.xlim(6, 13)\nplt.ylim(6, 13)\n\nplt.show()","4bfdef06":"sns.distplot(y_train - y_hat)\nplt.title('Residual PDF', size =20)\n\nplt.show()","82d5378d":"reg.score(x_train, y_train)","33fd2b21":"reg_summary  = pd.DataFrame(inputs.columns.values, columns=['Features'])\nreg_summary['weights'] = reg.coef_\nreg_summary","7d8d7506":"data_cleaned['Brand'].unique()","b9177f97":"y_hat_test = reg.predict(x_test)","1966ec1e":"plt.scatter(y_test, y_hat_test)\n\nplt.xlabel('target (y_test)', size =20)\nplt.ylabel('Predictions (y_hat_test)', size =20)\n\nplt.xlim(6, 13)\nplt.ylim(6, 13)\n\nplt.show()","4bd51105":"df_pf = pd.DataFrame(np.exp(y_hat_test), columns=['Predictions'])\ndf_pf.head()","e5944c33":"df_pf['Target'] = np.exp(y_test)\ndf_pf.head()","5fb982ee":"y_test.head(20)","8b0e4fd0":"y_test = y_test.reset_index(drop=True)\ny_test.head(20)","90e3ee33":"df_pf['Target'] = np.exp(y_test)\ndf_pf","ee6b35b1":"df_pf['Residual'] = df_pf['Target'] - df_pf['Predictions']\ndf_pf['Differences%'] = np.absolute(df_pf['Residual']\/df_pf['Target'] *100)\ndf_pf","9f20ff95":"df_pf.describe()","428d8b42":"pd.options.display.max_rows = 999\npd.set_option('display.float_format', lambda x: '%.2f' % x)\ndf_pf.sort_values(by=['Differences%'])","ee184ad7":"Adding Dummy variables for categorical data, all at once\n\nif we have N categories fir a feature, we have to create N-1 dummies","d518d2fd":"# testing","9a8fc3fa":"**Intro To scikit learn**\n\n# Loadning practice datasets and split\n","d756087d":"# Multiple Liner Regression\n\n\n","3b356a3d":"# Adjusted r-squared","652e3b4b":"# Feature Scaling and Standardization","fe78a10a":"the one that has a lower price seems to have higher differences , it is aslove preoved that out model is overfitted","e634a8f7":"# Find the weight and bias","4e5618c8":"# making prediction","21a7224f":"on the above figure , it has long tail on left side , so we can say this data is right skeweed which means overfitting. On the other hand , there is small tail on right so there is no underfitting","5b50c763":"Split the data into train test ","2cd1a632":"# Evaluating where we ware right or wrong with confusion metrics ","e8e79dac":"# Checking the OLS Assumptions","539867a1":"# Exploring probability distribution","b7ada03c":"# scale the data","0163d80e":"# Make a graphs of what where we ware wrong","d25ce2da":"It shows that SAT was a usefull variable for the model and Rand 1,2,3 is not. Because the p_value is higher the .50","d6dfca71":"# predict the price ","ca12d037":"# Evaluating the reasults\n\n**Using Seaborn, calculating matrix**","51a6b05f":"# creating summary table","6e96b412":"# check for Multicollinearity","31428548":"# How to detect the variable that is not needed for the model. This is very important beccause it uses more resources and doesn't contribute to anything in our model\n\n# Feature Selection --> No designated model but there is a method call \"f_regression\": \n\n**f_regression creates simple liner regressios of each feature and the dependent variable**","84e5f56f":"# Model Selectinon and train","b38308c6":"# create the regression","1fb1c40b":"# Dealing with Missing Values","cec8cd11":"# Example One","e47f4ec5":"# Create Regression\n","b0b8c92c":"# set the input and target ","35a366e6":"# Build Module, train them and predict them\n\n**build and predict with sklearn**","934d55d7":"# overfitting and underfitting\n\nOverfitting is harder to detech because accuracy of model is high. Underfitting is easy to discover.\nSolution, well one of: Split the data into train and test. Range can be (80:test, 20:train) or (90,10)"}}