{"cell_type":{"e2987a1f":"code","ab0fb371":"code","a7a544a7":"code","d51c61a5":"code","7d5f7be6":"code","753d357b":"code","d2558675":"code","0c5f0835":"code","4a2a01b2":"code","760cc287":"code","4d8edb79":"code","aa04b6f8":"code","e63f0b5d":"code","b795bc91":"code","c7cc8433":"code","2fe7d54b":"code","f1936d05":"code","cb9b9f38":"code","4e31bcfd":"code","b1e60748":"code","1d98f870":"markdown","8ea67b03":"markdown","e254c2cc":"markdown","e9fa4be5":"markdown","5e5e643d":"markdown","eeffc1e7":"markdown","ee1f7779":"markdown","e07e05a0":"markdown","b58e527c":"markdown","162d2b82":"markdown","7947a22d":"markdown","85fe1d89":"markdown"},"source":{"e2987a1f":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n                          RobertaConfig, RobertaModel, RobertaTokenizer)\n\nfrom tqdm import tqdm\nimport pandas as pd\nimport random\nimport numpy as np\nimport requests\nimport re","ab0fb371":"def set_seed(seed,n_gpu=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(seed)\n        \nset_seed(0)","a7a544a7":"class Config:\n    max_length=64\n    lr=1e-6\n    batch_size=64\n    train_epochs = 3\n    save_epoch = 3 # Save a model every 3 epochs\n    max_grad_norm = 1\n    train_ratio=0.6\n    valid_ratio=0.3\n    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    \nconfig = Config()","d51c61a5":"def text_preprocess(raw_text,tokenizer,config):\n    MAX_LENGTH = config.max_length\n    \n    url_pattern = re.compile(r'http:\/\/.*? |https:\/\/.*? ')\n    at_pattern = re.compile(r'@.*? ')\n    tag_pattern = re.compile(r'#.*? ')\n    face_pattern = re.compile(r'\\x89.{2}')\n    bracket_pattern = re.compile('\\[.*?\\]')\n    repeat_pattern = re.compile(r'((.)\\2+)')\n    sep_pattern = re.compile(r'\\[SEP\\]')\n\n    result = []\n    input_ids = []\n    attention_mask =[]\n    for text in tqdm(raw_text):\n        text += ' '\n        url_iter = url_pattern.finditer(text)\n        for url_matcher in url_iter:\n            url = url_matcher.group().strip()\n            try:\n    #             title = get_url_title(url)\n                  title = None\n            except:\n                title = None\n            title = '[SEP] '+title+' [SEP]' if title else ''\n            text = text.replace(url,title)                           # replace url with [SEP] title [SEP]\n\n        text = text.replace('\\n',' [SEP] ')\n        text = at_pattern.sub('',text)                               # remove @user\n        text = face_pattern.sub('',text)                             # remove \\x89 (i dont know what it is)\n        tag_iter = tag_pattern.finditer(text)                        # replace #tag with [SEP] tag [SEP]\n        for tag_matcher in tag_iter:\n            tag = tag_matcher.group().strip()\n            text = text.replace(tag,'[SEP] '+tag[1:]+' [SEP]')\n\n        text = text.replace('|',' [SEP] ')\n\n        brackets = bracket_pattern.findall(text)\n        for b in brackets:\n            if b != '[SEP]':\n                text = text.replace(b,'[SEP] '+b[1:-1]+' [SEP]')\n\n        sep_iter = sep_pattern.finditer(text)\n        new_text = ''\n        prev_idx = 0\n    #     print(text)\n        for sep_matcher in sep_iter:\n            parts = text[prev_idx:sep_matcher.start()]\n    #         print(parts)\n    #         print('='*10)\n            parts = parts.strip()\n            new_text += parts+' [SEP] ' if parts else ''\n            prev_idx = sep_matcher.end()\n        text = new_text + text[prev_idx:]\n\n        rep_iter = repeat_pattern.finditer(text)\n        new_text = ''\n        prev_idx = 0\n        for rep_matcher in rep_iter:\n            new_text += text[prev_idx:rep_matcher.start()+1]\n            prev_idx = rep_matcher.end()\n        text = new_text + text[prev_idx:]\n\n        text = text.replace('  ',' ')\n        text = text.strip().strip('-')\n        text = text.strip('-').strip()\n        while text[:5]=='[SEP]':\n            text = text[5:].strip()\n        while text[-5:]=='[SEP]':\n            text = text[:-5].strip()\n            \n        text = text.replace('[SEP]',tokenizer.sep_token)\n        text = text.lower()\n        result.append(text)\n        iid,am = tokenizer(text).values()\n        if len(iid)<=MAX_LENGTH:\n            pad_len = MAX_LENGTH-len(iid)\n            iid += [1]*pad_len\n            am += [0]*pad_len\n        else:\n            iid = iid[:MAX_LENGTH]\n            am = am[:MAX_LENGTH]\n        input_ids.append(iid)\n        attention_mask.append(am)\n    return result,input_ids,attention_mask\n\ndef preprocess(df,tokenizer,config):\n    raw_text = df['text'].to_list()\n    sample_id = df['id'].to_list()\n    try:\n        target = df['target'].to_list()\n    except:\n        target = [0]*len(sample_id)\n        \n    result,input_ids,attention_mask = text_preprocess(raw_text,tokenizer,config)\n    return {'id':sample_id,\n            'text':result,\n            'input_ids':input_ids,\n            'attention_mask':attention_mask,\n            'label':target}","7d5f7be6":"raw = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nraw.head()","753d357b":"tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")","d2558675":"raw = preprocess(raw,tokenizer,config)","0c5f0835":"import random\nimport pickle\nfrom tqdm import trange\n\ndef kfold(data,k=5,shuffle=True,file_prefix='\/kaggle\/working\/kfold',file_suffix='tuple2'):\n    keys = list(data.keys())\n    total_idx = list(range(len(data[keys[0]])))\n    total = len(total_idx)\n    valid_num = int(total\/k)\n    if shuffle:\n        random.shuffle(total_idx)\n    for i in trange(k):\n        train_data = {}\n        valid_data = {}\n        for key in data.keys():\n            train_data[key] = []\n            valid_data[key] = []\n\n        valid_idx = total_idx[valid_num*(k-1):valid_num*k]\n        for idx in valid_idx:\n            for key in valid_data.keys():\n                valid_data[key].append(data[key][idx])\n                \n        train_idx = total_idx[:valid_num*(k-1)]+total_idx[valid_num*k:]\n        for idx in train_idx:\n            for key in train_data.keys():\n                train_data[key].append(data[key][idx])\n    \n        with open('{pf}_{num}.{sf}'.format(pf=file_prefix,num=i,sf=file_suffix),'wb') as f:\n            pickle.dump((train_data,valid_data),f)\n\ndef dataset_split(data,train_ratio=0.6,valid_ratio=0.3,shuffle=True):\n    assert train_ratio+valid_ratio<=1 , \"The train_ratio + valid_ratio should not be greater than 1\"\n    keys = list(data.keys())\n    total_idx = list(range(len(data[keys[0]])))\n    total = len(total_idx)\n    train_num = int(total*train_ratio)\n    valid_num = int(total*valid_ratio)\n    if shuffle:\n        random.shuffle(total_idx)\n    \n    train_data = {}\n    valid_data = {}\n    test_data = {}\n    for k in data.keys():\n        train_data[k] = []\n        valid_data[k] = []\n        test_data[k] = []\n        \n    for idx in total_idx[:train_num]:\n        for k in train_data.keys():\n            train_data[k].append(data[k][idx])\n        \n    for idx in total_idx[train_num:train_num+valid_num]:\n        for k in valid_data.keys():\n            valid_data[k].append(data[k][idx])\n        \n    for idx in total_idx[train_num+valid_num:]:\n        for k in test_data.keys():\n            test_data[k].append(data[k][idx])\n        \n    return train_data,valid_data,test_data","4a2a01b2":"kfold(raw,k=5)","760cc287":"class MyDataset(Dataset):\n    def __init__(self, train_data):\n        self.id = train_data['id']\n        self.text = train_data['text']\n        self.input_ids = train_data['input_ids']\n        self.attention_mask = train_data['attention_mask']\n        self.label = train_data['label']\n        assert len(self.text)==len(self.label)\n        self.idx = list(range(len(self.label)))\n#         random.shuffle(self.idx)\n\n    def __getitem__(self, index):\n#         idx = self.idx[index]\n        idx = index\n        return {\n                'id':torch.tensor(self.id[idx],dtype=torch.long),\n                'text':self.text[idx],\n                'input_ids':torch.tensor(self.input_ids[idx],dtype=torch.long),\n                'attention_mask':torch.tensor(self.attention_mask[idx],dtype=torch.long),\n                'label':torch.tensor(self.label[idx],dtype=torch.long)\n                }\n    \n    def __len__(self):\n        return len(self.label)","4d8edb79":"class ClassificationHead(nn.Module):\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\n    def __init__(self, input_size, output_size, d_model_0, d_model_1, dropout_prob):\n        super().__init__()\n        self.norm= nn.BatchNorm1d(input_size)\n#         self.norm = nn.LayerNorm(input_size)\n        self.dense = nn.Linear(input_size, d_model_0)\n        self.norm_1= nn.BatchNorm1d(d_model_0)\n#         self.norm_1= nn.LayerNorm(d_model_0)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.dense_1 = nn.Linear(d_model_0, d_model_1)  \n        self.norm_2= nn.BatchNorm1d(d_model_1)\n#         self.norm_2= nn.LayerNorm(d_model_1)\n        self.out_proj = nn.Linear(d_model_1, output_size)\n\n    def forward(self, features, **kwargs):\n        x = self.norm(features)\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.relu(self.norm_1(x))\n        x = self.dropout(x)\n        x = self.dense_1(x)\n        x = torch.relu(self.norm_2(x))\n        x = self.dropout(x)        \n        x = self.out_proj(x)\n        return x","aa04b6f8":"class Model0(nn.Module):\n    def __init__(self,encoder,config):\n        super().__init__()\n        self.encoder = encoder\n#         self.dense = nn.Linear(768,2)\n        self.classifier=ClassificationHead(768,2,256,64,0.2)\n        self.classifier.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https:\/\/github.com\/pytorch\/pytorch\/pull\/5617\n            module.weight.data.normal_(mean=0.0, std=0.02)\n    \n    def forward(self,input_ids,attention_mask,label=None):\n        hidden_states = self.encoder(input_ids=input_ids,attention_mask=attention_mask)[1]\n        logits = self.classifier(hidden_states)\n#         logits = self.dense(hidden_states)\n        loss_fct = CrossEntropyLoss()\n        if label is None:\n            loss = 0\n        else:\n            loss = loss_fct(logits, label)\n        prob=torch.softmax(logits,-1)\n        \n        return prob,loss","e63f0b5d":"class Model1(nn.Module):\n    def __init__(self,encoder,config):\n        super().__init__()\n        self.encoder = encoder\n#         self.dense = nn.Linear(768,2)\n        self.conv1d = nn.Conv1d(768*2,1024,config.max_length)\n        self.classifier=ClassificationHead(1024,2,512,256,0.2)\n        self.classifier.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https:\/\/github.com\/pytorch\/pytorch\/pull\/5617\n            module.weight.data.normal_(mean=0.0, std=0.02)\n    \n    def forward(self,input_ids,attention_mask,label=None):\n        hidden_states = self.encoder(input_ids=input_ids,attention_mask=attention_mask)[-1][-2:]\n        hidden_states = torch.cat(hidden_states,-1)\n        hidden_states = hidden_states.permute(0,2,1)\n        hidden_states = self.conv1d(hidden_states)\n        hidden_states = torch.squeeze(hidden_states)\n        logits = self.classifier(hidden_states)\n#         logits = self.dense(hidden_states)\n        loss_fct = CrossEntropyLoss()\n        if label is None:\n            loss = 0\n        else:\n            loss = loss_fct(logits, label)\n        prob=torch.softmax(logits,-1)\n        \n        return prob,loss","b795bc91":"class Model2(nn.Module):\n    def __init__(self,encoder,config):\n        super().__init__()\n        self.encoder = encoder\n#         self.dense = nn.Linear(768,2)\n        self.conv1d = nn.Conv1d(768,1024,config.max_length)\n        self.classifier=ClassificationHead(1024,2,512,256,0.2)\n        self.classifier.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https:\/\/github.com\/pytorch\/pytorch\/pull\/5617\n            module.weight.data.normal_(mean=0.0, std=0.02)\n    \n    def forward(self,input_ids,attention_mask,label=None):\n        hidden_states = self.encoder(input_ids=input_ids,attention_mask=attention_mask)[-1][-2:]\n        hidden_states = hidden_states[0] + hidden_states[1]\n        hidden_states = hidden_states.permute(0,2,1)\n        hidden_states = self.conv1d(hidden_states)\n        hidden_states = torch.squeeze(hidden_states)\n        logits = self.classifier(hidden_states)\n#         logits = self.dense(hidden_states)\n        loss_fct = CrossEntropyLoss()\n        if label is None:\n            loss = 0\n        else:\n            loss = loss_fct(logits, label)\n        prob=torch.softmax(logits,-1)\n        \n        return prob,loss","c7cc8433":"import sklearn.metrics\n\ndef eval(model,dl,device,batch_num=-1):\n    torch.cuda.empty_cache()\n    model.to(device)\n    model.eval()\n    preds = []\n    labels = []\n    batch_count = 0\n    for data in tqdm(dl,desc='[Evaluating]',position=0):\n#         text = data['text']\n        input_ids = data['input_ids'].to(device)\n        attention_mask = data['attention_mask'].to(device)\n        label = data['label'].to(device)\n        with torch.no_grad():\n            prob,_ = model(input_ids,attention_mask,label)\n        preds.append(torch.argmax(prob,-1))\n        labels.append(label)\n        batch_count += 1\n        if batch_count > batch_num and batch_num>0:\n            break\n    label = torch.cat(labels,-1).cpu()\n    pred = torch.cat(preds,-1).cpu()\n    return sklearn.metrics.f1_score(label, pred)","2fe7d54b":"def infer(model,test_data,tokenizer,config,save=False,save_path='submission.csv'):\n    model.eval()\n    test_dl = DataLoader(MyDataset(test_data),shuffle=False,batch_size=config.batch_size)\n    preds = []\n    idx = []\n    dl_iter = tqdm(test_dl,desc='[Infering]',position=0)\n    device = config.device\n    for data in dl_iter:\n        text = data['text']\n        input_ids = data['input_ids'].to(device)\n        attention_mask = data['attention_mask'].to(device)\n    #     label = data['label'].to(device)\n        model.eval()\n        prob,loss = model(input_ids,attention_mask)\n        pred = torch.argmax(prob,-1).detach().cpu().numpy()\n        preds.append(pred)\n        idx.append(data['id'].detach().cpu().numpy())\n        dl_iter.set_postfix(id=data['id'][0].item())\n\n    preds = np.concatenate(preds)\n    idx = np.concatenate(idx)\n    if save:\n        pd.DataFrame({'id':idx,'preds':preds}).to_csv(save_path)\n    return idx,preds","f1936d05":"def train(model,config,save_name='model',fold_num=0):\n    with open('kfold_{fold_num}.tuple2'.format(fold_num=fold_num),'rb') as f:\n        train_data,valid_data = pickle.load(f)\n\n    train_dl = DataLoader(MyDataset(train_data),shuffle=True,batch_size=config.batch_size)\n    valid_dl = DataLoader(MyDataset(valid_data),shuffle=True,batch_size=config.batch_size)\n\n    optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8,weight_decay=0.08)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(len(train_dl)*config.train_epochs*0.2),num_training_steps=len(train_dl)*config.train_epochs) \n    step_count = 0\n    total_loss = 0\n\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model.to(device)\n    model.train()\n\n    total_f1_score = 0\n    for epoch in range(config.train_epochs):\n        dl_iter = tqdm(train_dl,desc='[Training]',position=0)\n        for data in dl_iter:\n            text = data['text']\n            input_ids = data['input_ids'].to(device)\n            attention_mask = data['attention_mask'].to(device)\n            label = data['label'].to(device)\n            model.train()\n            prob,loss = model(input_ids,attention_mask,label)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm) # \u68af\u5ea6\u622a\u65ad\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step()\n\n            step_count += 1\n            total_loss += loss.item()\n            avg_loss = total_loss\/step_count\n            pred = torch.argmax(prob,-1).detach().cpu().numpy()\n            label = label.detach().cpu().numpy()\n            total_f1_score += sklearn.metrics.f1_score(label, pred)\n            f1_score = total_f1_score\/step_count\n            dl_iter.set_postfix(epoch=epoch,loss=loss.item(),avg_loss=avg_loss,f1_score=f1_score)\n\n        model.eval()\n        f1_score = eval(model,valid_dl,device,-1)\n        tqdm.write('[epcoh_{epoch}] f1_score: {f1_score}'.format(epoch=epoch,f1_score=f1_score))\n        if (epoch+1)%config.save_epoch == 0:\n            torch.save(model,'{model_name}_fold{fold_num}_epoch{epoch}.pt'.format(model_name=save_name,fold_num=fold_num,epoch=epoch))\n        model.train()","cb9b9f38":"RobertaModel.from_pretrained('roberta-base') # download pretrained model","4e31bcfd":"import os\n\ntry:\n    os.mkdir('\/kaggle\/working\/models\/') # make a directory to save model\nexcept:\n    pass\nconfig.train_epochs = 3\nconfig.save_epoch = 3\nfor fn in range(5):\n    tqdm.write('='*10+'FOLD_{fn}'.format(fn=fn)+'='*10)\n    encoder = RobertaModel.from_pretrained('roberta-base')\n    encoder.config.output_hidden_states = True\n    model = Model2(encoder,config)\n    train(model,config,save_name='\/kaggle\/working\/models\/model2',fold_num=fn)\n    del model\n    del encoder","b1e60748":"test_raw = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_data = preprocess(test_raw,tokenizer,config)\nvote = []\nmodel_num = 0\nfor home, dirs, files in os.walk('\/kaggle\/working\/models\/'):\n    for filename in files:\n        if filename.endswith('.pt'):\n            model_num += 1\n            model = torch.load(os.path.join(home,filename))\n            idx,preds = infer(model,test_data,tokenizer,config,save_path='{mn}.csv'.format(mn=filename[:-3]))\n            vote.append(preds)\n            del model\n            \nvote = np.array(vote).T\nresult = np.sum(vote,axis=1)[:,None]\nthreshold = int(model_num\/2)\nprint('Model Num: {mn}\\nThreshold: {ts}'.format(mn=model_num,ts=threshold))\nresult = np.apply_along_axis(lambda x:1 if x>threshold else 0,1,result)\npd.DataFrame({'id':idx,'target':result}).to_csv('\/kaggle\/working\/submission.csv',index=0)","1d98f870":"`kfold()` can divide the training set into 5 equal parts, and each one is used as the verification set in turn. So we will get 5 sets (each set contains training set and test set), which are saved as files by pickle.\n\n`dataset_split()` can divide training set, valid set and test set according to `train_ratio` and `valid_ratio`.","8ea67b03":"Set seed to ensure the repeatability of the experiment","e254c2cc":"## Ensamble","e9fa4be5":"We first define a Dataset in order to train the model","5e5e643d":"`ClassificationHead` is a simple MLP(Multilayer Perceptron) model for classification","eeffc1e7":"## Modeling","ee1f7779":"We usually use a custom `Config` class to store commonly used training parameters","e07e05a0":"## Preprocess","b58e527c":"Train the model and save","162d2b82":"We have defined 3 slightly different models. \n\nAccording to the experiment, the F1score of Model2 and Model1 are close, and the F1score of Model0 is the worst.\n\nYou can choose any model you like to train, or train all models for ensamble. We also have an ensamble algorithm at the end of the notebook.","7947a22d":"## Training, testing, inferring","85fe1d89":"`text_preprocess()` can remove special characters in the `text` and use `tokenizer` to encode.\n`preprocess()` can use `text_preprocess()` to process every row in `df`(pandas.Dataframe)"}}