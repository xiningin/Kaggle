{"cell_type":{"0ab782c3":"code","c67cb087":"code","2a8ea626":"code","a9f703b2":"code","b9694624":"code","c2ab01ed":"code","9f1ab26a":"code","85831f90":"code","01a3778a":"code","edc4907c":"code","522b66a5":"code","ccf20ffd":"code","80b998a2":"code","33aeb13c":"code","f8ec29f1":"code","d4c1cf54":"code","b940e125":"code","4924b857":"code","ba8987c7":"code","1d1fbf4e":"code","8754a381":"code","536b9174":"code","5e44ce2c":"code","c7af4cea":"code","803714c6":"markdown","f62dc4f3":"markdown","f7be4346":"markdown","c82e144e":"markdown","74adeb19":"markdown","4521f26e":"markdown"},"source":{"0ab782c3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n","c67cb087":"\nimport re\nimport numpy as np\n### necessary functions from the keras library\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, LSTM\nfrom keras.optimizers import RMSprop\nfrom keras.utils.data_utils import get_file\nfrom keras.models import load_model\nimport keras\nimport random\nimport pickle","2a8ea626":"\n# Read text from himu.txt file \nbangla_text = open('..\/input\/himu.txt').read()\nprint('\u0986\u09ae\u09be\u09a6\u09c7\u09b0 \u0985\u09b0\u09bf\u099c\u09bf\u09a8\u09be\u09b2 \u099f\u09c7\u0995\u09cd\u09b8\u099f \u098f ' + str(len(bangla_text)) + ' \u09ac\u09b0\u09cd\u09a3 \u0986\u099b\u09c7')","a9f703b2":"# Printing 2000 words\nbangla_text[:2000]","b9694624":"def remove_tabs(text):\n    \n    text = text.replace('\\n',' ') \n    text = text.replace('\\t',' ')\n    text = text.replace('\\r',' ')\n    text = text.replace('\\u200d',' ')\n    \n    text = re.sub(' +',' ',text)\n    \n    return text\n\nbangla_text = remove_tabs(bangla_text)","c2ab01ed":"# After remove new line and tabs\nbangla_text[:2000]","9f1ab26a":"def remove_symbols(txt):\n    \n    chars = ['$', '!', '@', '?', '%', '\/', '*', '-', '&', '(', ')', '\"', \"'\", ',', ';', ':',\n             '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0','\u2018','\u0964','\u2019','\u2014', \n              '\u09e7', '\u09e8', '\u09e9', '\\u200c', '\u2013', '\u201c', '\u201d', '\u2026',\n              'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',\n             't', 'u', 'w', 'y', 'z', '\u0964', ',', '?','I', 'K', 'R', 'T', 'Y'] \n    for c in chars:\n        if c in txt:\n            txt = txt.replace(c, \" \")\n\n    # \u09af\u09a6\u09bf \u0985\u09a4\u09bf\u09b0\u09bf\u0995\u09cd\u09a4 \u09b8\u09cd\u09aa\u09c7\u09b8 \u09a5\u09be\u0995\u09c7 \u09a4\u09be\u09b9\u09b2\u09c7 \u098f\u0995\u099f\u09be \u09b8\u09cd\u09aa\u09c7\u09b8 \u098f \u09b0\u09bf\u09aa\u09cd\u09b2\u09c7\u09b8 \u0995\u09b0\u09ac\u09c7\u0964 \n    txt = txt.replace('  ',' ')\n    \n    return txt\n    \nbangla_text = remove_symbols(bangla_text)","85831f90":"# After cleaning\nbangla_text[:2000]","01a3778a":"# \u099f\u09cb\u099f\u09be\u09b2 \u0987\u0989\u09a8\u09bf\u0995 \u0995\u09cd\u09af\u09be\u09b0\u09c7\u0995\u09cd\u099f\u09be\u09b0 \u0997\u09a3\u09a8\u09be \nchars = sorted(list(set(bangla_text)))\n\nprint (\"\u098f\u0987 corpus \u098f \u0986\u099b\u09c7 \u09b8\u09b0\u09cd\u09ac\u09ae\u09cb\u099f \" +  str(len(bangla_text)) + \" \u099f\u09bf \u0995\u09cd\u09af\u09be\u09b0\u09c7\u0995\u09cd\u099f\u09be\u09b0 \")\nprint (\"\u098f\u0987 corpus \u098f \u0986\u099b\u09c7 \u09b8\u09b0\u09cd\u09ac\u09ae\u09cb\u099f \" +  str(len(chars)) + \" \u099f\u09bf \u0987\u0989\u09a8\u09bf\u0995 \u0995\u09cd\u09af\u09be\u09b0\u09c7\u0995\u09cd\u099f\u09be\u09b0\")","edc4907c":"def window_transform_text(text,window_size,step_size):\n    # containers for input\/output pairs\n    inputs = [text[i:i+window_size] for i in range(0, len(text)-window_size, step_size)]\n    outputs = [text[i] for i in range(window_size, len(text), step_size)]    \n    return inputs,outputs","522b66a5":"# run your text window-ing function \nwindow_size = 100\nstep_size = 5\ninputs, outputs = window_transform_text(bangla_text,window_size,step_size)","ccf20ffd":"# print out a few of the input\/output pairs to verify that we've made the right kind of stuff to learn from\nprint('input = ' + inputs[1])\nprint('output = ' + outputs[1])\nprint('--------------')\nprint('input = ' + inputs[302])\nprint('output = ' + outputs[302])","80b998a2":"# print out the number of unique characters in the dataset\nchars = sorted(list(set(bangla_text)))\nprint (\"this corpus has \" +  str(len(chars)) + \" unique characters\")\nprint ('and these characters are ')\nprint (chars)","33aeb13c":"# this dictionary is a function mapping each unique character to a unique integer\nchars_to_indices = dict((c, i) for i, c in enumerate(chars))  # map each unique character to unique integer\n\n# this dictionary is a function mapping each unique integer back to a unique character\nindices_to_chars = dict((i, c) for i, c in enumerate(chars))  # map each unique integer back to unique character","f8ec29f1":"# transform character-based input\/output into equivalent numerical versions\ndef encode_io_pairs(text,window_size,step_size):\n    # number of unique chars\n    chars = sorted(list(set(text)))\n    num_chars = len(chars)\n    \n    # cut up text into character input\/output pairs\n    inputs, outputs = window_transform_text(text,window_size,step_size)\n    \n    # create empty vessels for one-hot encoded input\/output\n    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n    \n    # loop over inputs\/outputs and tranform and store in X\/y\n    for i, sentence in enumerate(inputs):\n        for t, char in enumerate(sentence):\n            X[i, t, chars_to_indices[char]] = 1\n        y[i, chars_to_indices[outputs[i]]] = 1\n        \n    return X,y","d4c1cf54":"window_size = 100\nstep_size = 5\nX,y = encode_io_pairs(bangla_text,window_size,step_size)","b940e125":"def create_rrn_model():\n\n    model = Sequential()\n    model.add(LSTM(200, input_shape=(window_size, len(chars))))\n    model.add(Dense(len(chars)))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model","4924b857":"model = create_rrn_model()\nmodel.summary()","ba8987c7":"# a small subset of our input\/output pairs\nXtrain = X[:,:,:]\nytrain = y[:,:]","1d1fbf4e":"def training(model, Xtrain, ytrain, batch_size=500, epochs=50):\n    \n    model.fit(Xtrain, ytrain, batch_size=500, epochs=epochs,verbose = 1)\n    \n    # \u09ae\u09a1\u09c7\u09b2 \u09b8\u09c7\u09ad\n    model.save('model.h5')","8754a381":"training(model, Xtrain, ytrain,epochs=5)","536b9174":"with open('history', 'wb') as file_pi:\n        pickle.dump(model.history, file_pi)","5e44ce2c":"# function that uses trained model to predict a desired number of future characters\ndef predict_next_chars(model,input_chars,num_to_predict):     \n    # create output\n    predicted_chars = ''\n    for i in range(num_to_predict):\n        # convert this round's predicted characters to numerical input    \n        x_test = np.zeros((1, window_size, 61))\n        #print(x_test.shape)\n        #x_test.reshape(1, 100, 28)\n        for t, char in enumerate(input_chars):\n            \n            x_test[0, t, chars_to_indices[char]] = 1.\n\n        # make this round's prediction\n        test_predict = model.predict(x_test,verbose = 0)[0]\n\n        # translate numerical prediction back to characters\n        r = np.argmax(test_predict)                           # predict class of each test input\n        d = indices_to_chars[r] \n\n        # update predicted_chars and input\n        predicted_chars+=d\n        #print(r)\n        input_chars+=d\n        input_chars = input_chars[1:]\n    return predicted_chars","c7af4cea":"# get an appropriately sized chunk of characters from the text\nstart_inds = [200, 500, 800, 1200]\n\n# load in weights\n#model.load_weights('model_weights\/best_RNN_small_textdata_weights.hdf5')\nnew_model = load_model('model.h5')\nfor s in start_inds:\n    start_index = s\n    input_chars = bangla_text[start_index: start_index + window_size]\n\n    # use the prediction function\n    predict_input = predict_next_chars(new_model,input_chars,num_to_predict = 100)\n\n    # print out input characters\n    print('------------------')\n    input_line = 'input chars = ' + '\\n' +  input_chars + ' \"' + '\\n'\n    print(input_line)\n\n    # print out predicted characters\n    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n    print(line)","803714c6":"# Bengali language model created\n\n\nFull code: Github [click](https:\/\/github.com\/m-d-hasan\/Himu_Bangla_Text_Generator)\n\n\n Reference :\n *  1. Character base language model: click [here](https:\/\/machinelearningmastery.com\/develop-character-based-neural-language-model-keras\/)\n*  2. Bangla epub: click [here](https:\/\/www.amarboi.com\/search\/label\/epub)","f62dc4f3":"**Input\/output pair**","f7be4346":"But still various symbols exists, we have to remove these symbols or special characters","c82e144e":"# This is Bengali Character-to-character language model. This dataset created from Himu book which is a popular fictional character created by the Bangladeshi writer Humayun Ahmed.","74adeb19":"** Model creaton **\n\n\u0986\u09ae\u09be\u09a6\u09c7\u09b0 RNN \u09ae\u09a1\u09c7\u09b2 \u098f \u098f\u0995\u099f\u09bf \u09b8\u09bf\u0999\u09cd\u0997\u09c7\u09b2 LSTM \u09b9\u09bf\u09a1\u09c7\u09a8 \u09b2\u09c7\u09af\u09bc\u09be\u09b0 \u09a5\u09be\u0995\u09ac\u09c7\u0964\n\n    layer 1 \u09b9\u099a\u09cd\u099b\u09c7 \u09e8\u09e6\u09e6 \u09b9\u09bf\u09a1\u09c7\u09a8 \u0987\u0989\u09a8\u09bf\u099f \u098f\u09b0 \u098f\u0995\u099f\u09bf LSTM \u09ae\u09a1\u09bf\u0989\u09b2 --> \u09a8\u09cb\u099f input_shape = (window_size,len(chars)) \u09af\u09c7\u0996\u09be\u09a8\u09c7, len(chars) = number of unique characters\n\n    layer 2 \u09b9\u099a\u09cd\u099b\u09c7 \u098f\u0995\u099f\u09bf linear module, fully connected, with len(chars) hidden units --> \u09af\u09c7\u0996\u09be\u09a8\u09c7, len(chars) = number of unique characters\n\n    layer 3 \u09b9\u099a\u09cd\u099b\u09c7 \u098f\u0995\u099f\u09bf softmax activation\n\n    \u098f\u0987 \u09ae\u09a1\u09bf\u0989\u09b2\u09c7 categorical_crossentropy loss \u09ac\u09cd\u09af\u09ac\u09b9\u09be\u09b0 \u0995\u09b0\u09c7\u099b\u09bf\u0964","4521f26e":"**Cleaning dataset**"}}