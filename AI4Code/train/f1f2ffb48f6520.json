{"cell_type":{"7a9c4598":"code","ab06bff7":"code","a993ee10":"code","4c85fc2c":"code","14f1af66":"code","65f4127a":"code","c1e6f818":"code","a10c89db":"code","be58b747":"code","b6c5c650":"code","fa5c5bf1":"code","dc882ef3":"code","ff50b57a":"code","66320d02":"code","8d1cf44f":"code","1cd95079":"code","977f6241":"code","6304764e":"code","0c5473a6":"code","01370e8c":"code","1b7204f6":"code","e03b9397":"code","534573fb":"code","e86aeb71":"code","1536433a":"code","81406a5a":"code","bc8d519a":"code","779d7237":"code","4fe33963":"code","b565e3e0":"code","89d28f45":"code","9921fdba":"code","b568dd9e":"code","7d707c8e":"code","fc2fae60":"code","0a381f2f":"code","75676df8":"code","1321e319":"code","c6956da4":"code","e3631e14":"code","9323aab4":"code","1dfa2466":"code","34f255ce":"markdown","c2ed379f":"markdown","43883ac9":"markdown","b6f23622":"markdown","b4ecf5f3":"markdown","b7c341d7":"markdown","ea819fca":"markdown","b859cb90":"markdown","5ab2b923":"markdown","9fdc65ae":"markdown","db9bd13b":"markdown","310aed1d":"markdown","6b11b5e5":"markdown","4908c310":"markdown","30f28f32":"markdown","be655c3d":"markdown","e010f84d":"markdown","ad1a169a":"markdown","e9175f33":"markdown","e6ec0e5f":"markdown","c8417928":"markdown","5cc1e346":"markdown","9b57a08c":"markdown","a1be8cd9":"markdown","20f2fcbc":"markdown","95dad749":"markdown","25bdeb4d":"markdown"},"source":{"7a9c4598":"import math\nimport numpy as np\nimport pandas as pd\n\n#chart\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Preprocessing\nimport scipy.stats as ss\nimport category_encoders as ce\n\nfrom sklearn.base import clone\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import  RandomForestRegressor\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.datasets import make_classification\n\nimport warnings\nwarnings.filterwarnings( 'ignore' )\n\nsns.set(style='darkgrid')","ab06bff7":"df = pd.read_csv('..\/input\/employees-evaluation-for-promotion\/employee_promotion.csv')","a993ee10":"df.head(3)","4c85fc2c":"print(f'number of rows {df.shape[0]}')\nprint(f'number of cols {df.shape[1]}')","14f1af66":"df.describe()","65f4127a":"df.info()","c1e6f818":"df1 = df.copy()","a10c89db":"(df1.isna().sum() \/ len(df1)) * 100","be58b747":"# I choose fill with the median to fill numerical variables\ndf1.age.fillna(df1['age'].median(), inplace=True )\n\ndf1.previous_year_rating.fillna(df1['previous_year_rating'].median(),  inplace=True )\n\ndf1.avg_training_score.fillna(df1['avg_training_score'].median(), inplace=True )\n\n# # I will use the value more often to fill categorical variables\n\n#df1['education'].value_counts()\ndf1.education.fillna(\"Bachelor's\", inplace=True )\n\n#drop employee_id because is a unique value to each one employee\n\ndf1.drop('employee_id', axis=1, inplace=True)\n\n#Transform genter to binary value\ndf1['gender'] = df1['gender'].map({'f': 0, 'm': 1}).astype(int)","b6c5c650":"df_num = df1.select_dtypes(include=['int64', 'float64'])\ndf_cat = df1.select_dtypes(exclude=['int64', 'float64'])","fa5c5bf1":"plt.figure(figsize=(15,6))\nsns.countplot(x='is_promoted', data=df1);","dc882ef3":"# Central Tendency - mean, meadina\nct1 = pd.DataFrame( df_num.apply( np.mean ) ).T\nct2 = pd.DataFrame( df_num.apply( np.median ) ).T\n# dispersion - std, min, max, range, skew, kurtosis\nd1 = pd.DataFrame( df_num.apply( np.std ) ).T\nd2 = pd.DataFrame( df_num.apply( min ) ).T\nd3 = pd.DataFrame( df_num.apply( max ) ).T\nd4 = pd.DataFrame( df_num.apply( lambda x: x.max() - x.min() ) ).T\nd5 = pd.DataFrame( df_num.apply( lambda x: x.skew() ) ).T\nd6 = pd.DataFrame( df_num.apply( lambda x: x.kurtosis() ) ).T\n\nm = pd.concat( [d2, d3, d4, ct1, ct2, d1, d5, d6] ).T.reset_index()\nm.columns = ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std','skew', 'kurtosis']\nm","ff50b57a":"fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(15, 10))\n\nfor i, feature in enumerate(df_num):\n    row = int(i\/4)\n    col = i%4\n    sns.distplot(df_num.iloc[:, i], ax=axs[row][col])\n\nplt.suptitle('Distirbution of features')\nplt.tight_layout\n","66320d02":"plt.figure(figsize=(35,8));\nplt.subplot( 1, 4, 1 );\nsns.countplot(x='department', data=df_cat);\nplt.xticks(rotation=45);\n\nplt.subplot( 1, 4, 2 );\nsns.countplot( y='region',  data=df_cat);\nplt.xticks(rotation=45);\n\nplt.subplot( 1, 4, 3 );\nsns.countplot( x='education', data=df_cat);\nplt.xticks(rotation=45);\n\nplt.subplot( 1, 4, 4 );\nsns.countplot( x='recruitment_channel', data=df_cat);\nplt.xticks(rotation=45);","8d1cf44f":"df1.sample()","1cd95079":"plt.figure(figsize=(15,6))\nsns.histplot(x='length_of_service', hue='is_promoted', data=df1, palette='Set2', bins=30, kde=True);","977f6241":"plt.figure(figsize=(15,6))\nsns.countplot(x='education', hue='is_promoted', data=df1)","6304764e":"plt.figure(figsize=(15,6))\nsns.histplot(x='age', hue='is_promoted', data=df1, bins=20, kde=True)","0c5473a6":"plt.figure(figsize=(15,6))\nsns.kdeplot(x='avg_training_score', hue='is_promoted', data=df1, shade=True)\nplt.xticks(rotation=45);","01370e8c":"plt.figure(figsize=(15,6))\ncorrelation = df_num.corr( method='pearson' )\nsns.heatmap( correlation, annot=True );","1b7204f6":"def cramers_v(confusion_matrix):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher,\n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum()\n    phi2 = chi2 \/ n\n    r, k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))\n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr \/ min((kcorr-1), (rcorr-1)))","e03b9397":"a = df_cat\n\na1 = cramers_v(pd.crosstab(a['department'], a[\"department\"]).to_numpy())\na2 = cramers_v(pd.crosstab(a['department'], a[\"region\"]).to_numpy())\na3 = cramers_v(pd.crosstab(a['department'], a[\"education\"]).to_numpy())\na4 = cramers_v(pd.crosstab(a['department'], a[\"recruitment_channel\"]).to_numpy())\n\na5 = cramers_v(pd.crosstab(a['region'], a[\"department\"]).to_numpy())\na6 = cramers_v(pd.crosstab(a['region'], a[\"region\"]).to_numpy())\na7 = cramers_v(pd.crosstab(a['region'], a[\"education\"]).to_numpy())\na8 = cramers_v(pd.crosstab(a['region'], a[\"recruitment_channel\"]).to_numpy())\n\na9 = cramers_v(pd.crosstab(a['education'], a[\"department\"]).to_numpy())\na10 = cramers_v(pd.crosstab(a['education'], a[\"region\"]).to_numpy())\na11 = cramers_v(pd.crosstab(a['education'], a[\"education\"]).to_numpy())\na12 = cramers_v(pd.crosstab(a['education'], a[\"recruitment_channel\"]).to_numpy())\n\n\na13 = cramers_v(pd.crosstab(a['recruitment_channel'], a[\"department\"]).to_numpy())\na14 = cramers_v(pd.crosstab(a['recruitment_channel'], a[\"region\"]).to_numpy())\na15 = cramers_v(pd.crosstab(a['recruitment_channel'], a[\"education\"]).to_numpy())\na16 = cramers_v(pd.crosstab(a['recruitment_channel'], a[\"recruitment_channel\"]).to_numpy())\n\n# Final dataset\nd = pd.DataFrame( {'department': [a1, a2, a3, a4],\n'region': [a5, a6, a7, a8],\n'education': [a9, a10, a11, a12],\n'recruitment_channel': [a13, a14, a15, a16] })\nd = d.set_index( d.columns )","534573fb":"plt.figure(figsize=(15,6))\nsns.heatmap( d, annot=True);","e86aeb71":"dfcat = df_cat.copy()\ndfnum = df_num.drop('is_promoted', axis=1)","1536433a":"# create an object of the OneHotEncoder\nOHE = ce.OneHotEncoder(cols=['department',\n                             'region',\n                             'education',\n                             'recruitment_channel'],use_cat_names=True)\n# encode the categorical variables\ndfcat = OHE.fit_transform(dfcat)","81406a5a":"df_all = pd.concat([dfnum, dfcat], axis= 1)","bc8d519a":"X = df_all.copy()\ny = df1['is_promoted']","779d7237":"#put the values in the same scale.\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X, columns = df_all.columns)","4fe33963":"X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(X, y, test_size = 0.25)","b565e3e0":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train_feat, y_train_feat)","89d28f45":"def imp_df(column_names, importances):\n  df = pd.DataFrame({'feature': column_names,\n                     'feature_importance': importances}).sort_values('feature_importance', ascending = False).reset_index(drop = True)\n  return df","9921fdba":"\ndef drop_col_feat_imp(model, X_train_feat, y_train_feat, random_state = 42):\n    \n    # clone the model to have the exact same specification as the one initially trained\n    model_clone = clone(model)\n    # set random_state for comparability\n    model_clone.random_state = random_state\n    # training and scoring the benchmark model\n    model_clone.fit(X_train_feat, y_train_feat)\n    benchmark_score = model_clone.score(X_train_feat, y_train_feat)\n    # list for storing feature importances\n    importances = []\n    \n    # iterating over all columns and storing feature importance (difference between benchmark and new model)\n    for col in X_train_feat.columns:\n        model_clone = clone(model)\n        model_clone.random_state = random_state\n        model_clone.fit(X_train_feat.drop(col, axis = 1), y_train_feat)\n        drop_col_score = model_clone.score(X_train_feat.drop(col, axis = 1), y_train_feat)\n        importances.append(benchmark_score - drop_col_score)\n    \n    importances_df = imp_df(X_train_feat.columns, importances)\n    return importances_df\n","b568dd9e":"feat = drop_col_feat_imp(rf, X_train_feat, y_train_feat)","7d707c8e":"feat_15 = feat.head(10)","fc2fae60":"plt.figure(figsize=(15,6))\nsns.barplot(data=feat_15, y='feature', x='feature_importance');","0a381f2f":"# I will try with this features\nselect_cols = ['avg_training_score', 'age', 'length_of_service','previous_year_rating', 'gender','no_of_trainings','awards_won']\nX = X[select_cols]","75676df8":"\nX, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=2,\n                           n_clusters_per_class=1,\n                           weights=[0.05, 0.94],\n                           class_sep=0.8, random_state=0)\n\ncolors = ['#ef8a62' if v == 0 else '#67a9cf' if v == 1 else '#f7f7f7' for v in y]\nkwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\nplt.figure(figsize=(10,6))\nplt.scatter(X[:, 0], X[:, 1], c=colors, **kwarg_params)\nsns.despine()\nplt.suptitle(\"Benchmark Data Resampled with SMOTE\")\npass","1321e319":"from imblearn.over_sampling import SMOTE\n\nX_resampled, y_resampled = SMOTE().fit_resample(X, y)\nkwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\ncolors = ['#ef8a62' if v == 0 else '#67a9cf' if v == 1 else '#f7f7f7' for v in y_resampled]\nplt.figure(figsize=(10,6))\nsns.scatterplot(X_resampled[:, 0], X_resampled[:, 1], c=colors, **kwarg_params)\nsns.despine()\nplt.suptitle(\"Benchmark Data Resampled with SMOTE\");","c6956da4":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.25)","e3631e14":"from sklearn.metrics import accuracy_score\nimport xgboost as xgb\n\nxgb_cl = xgb.XGBClassifier()\nxgb_cl.fit(X_train, y_train)\npreds = xgb_cl.predict(X_test)","9323aab4":"print('XGBoost : \\n', classification_report(y_test, preds))\nprint('XGBoost Accuracy: ', accuracy_score(y_test, preds))","1dfa2466":"print('MAE:', mean_absolute_error(y_test, preds))\nprint('MSE:', mean_squared_error(y_test, preds))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, preds)))","34f255ce":"Feature importance refers to a class of techniques for assigning scores to input features to a predictive model that indicates the relative importance of each feature when making a prediction.\n\n","c2ed379f":"# **SMOTE**","43883ac9":"**NUMERICAL**","b6f23622":"# **0.0. IMPORTS**","b4ecf5f3":"# **4.0. EXPLORATORY DATA ANALYSIS**","b7c341d7":"## **4.1. UNIVARIATED ANALYSIS**","ea819fca":"### **5.0.1. One hot encoder**","b859cb90":"### **5.0.2. Scaler**","5ab2b923":"**CATEGORICAL**","9fdc65ae":"### **4.1.2 Numerical Variable**","db9bd13b":"### **4.1.5 Multivariate Analysis**","310aed1d":"Cramer's V statistic allows to understand correlation between two categorical features in one data set. \n\n**It is calculated as:**\n\n**Cramer\u2019s V = \u221a(X2\/n) \/ min(c-1, r-1)**\n\nwhere:\n\n**X2:** The Chi-square statistic\n\n**n:** Total sample size\n\n**r:** Number of rows\n\n**c:** Number of columns\n","6b11b5e5":"# **6.0. FEATURE SELECTION**","4908c310":"**the class is unbalanced**","30f28f32":"### **3.1.2 Split Numerical and Categorical**","be655c3d":"### **4.0.0. Target Variable**","e010f84d":"# **7.0. MACHINE LEARNING**","ad1a169a":"# **1.0. LOADING DATA**","e9175f33":"# **5.0. DATA PREPARATION**","e6ec0e5f":"### **4.1.3 Categorical Variable**","c8417928":"### **3.0.1. Fill NA**","5cc1e346":"# **3.0. DATA PREPROCESSING**","9b57a08c":"**As we can see, the columns age, length_of_service and avg_training_score doesnt have a normal curve. So, after i will use log to do a normalization.**","a1be8cd9":"#**2.0. DATA DESCRIPTION**","20f2fcbc":"\u2022 **employeeid:** Unique ID for the employee \u2022 department: Department of employee \n\n\u2022 **region:** Region of employment (unordered)\n\n \u2022 **education:** Education Level \n\n\u2022 **gender:** Gender of Employee \u2022 recruitmentchannel: Channel of recruitment for employee\n\n\u2022 **no_ of_ trainings:** no of other trainings completed in the previous year on soft skills, technical skills, etc.\n\n\u2022 **age:** Age of Employee\n\n\u2022 **previous_ year_ rating:** Employee Rating for the previous year\n\n\u2022 **length_ of_ service:** Length of service in years\n\n\u2022 **awards_ won:** if awards won during the previous year then 1 else 0\n\n\u2022 **avg_ training_ score:** Average score in current training evaluations\n\n\u2022 **is_promoted:** (Target) Recommended for promotion","95dad749":"### **6.0.1 Random Forest Regressor**","25bdeb4d":"### **4.1.4 Bivariate Analysis**"}}