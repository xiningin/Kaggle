{"cell_type":{"f28dd7be":"code","14f8aee5":"code","90b2abe3":"code","dca99ddf":"code","898e5ee4":"code","e3ad51f2":"code","9a2d265e":"code","d2f3e075":"code","8953ce00":"code","b9022bdf":"code","478b883a":"code","9d1344f6":"code","a433b5c3":"code","c67a196d":"code","0b25fa22":"code","48eb8fd3":"code","6950a713":"code","86694dd5":"markdown","10199c7e":"markdown","458961bd":"markdown","1210fab1":"markdown","47a84fc8":"markdown","38495e4f":"markdown","d0f1d321":"markdown","698841bf":"markdown","1ea3de54":"markdown","0ac5e506":"markdown","7888b408":"markdown","62ff7c5e":"markdown","518e67fe":"markdown","c01b5e89":"markdown","33913d14":"markdown","299e95d1":"markdown","ba6f68e8":"markdown"},"source":{"f28dd7be":"from typing import Tuple\n\nimport pandas as pd\nfrom pandas import DataFrame\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization\nfrom keras import metrics","14f8aee5":"seed = 7\nnp.random.seed(seed)","90b2abe3":"df = pd.read_csv(\"..\/input\/data.csv\")","dca99ddf":"df.head()","898e5ee4":"df.shape # shows the shape of data stored in matrix","e3ad51f2":"df.info()","9a2d265e":"print(df[\"diagnosis\"].unique())","d2f3e075":"B, M = df[\"diagnosis\"].value_counts()\nax = sns.countplot(x=\"diagnosis\", data=df)\n\nprint(\"Benign count: {}\".format(B))\nprint(\"Malignat count: {}\".format(M))\n\nplt.show()","8953ce00":"df = df.drop([\"id\", \"Unnamed: 32\"], 1)","b9022bdf":"df.isnull().values.any()","478b883a":"X = StandardScaler().fit_transform(df.iloc[:,1:31])\ny = LabelEncoder().fit_transform(df.diagnosis.values)","9d1344f6":"def create_logistic_model(input_features_dimension) -> Sequential:\n    model = Sequential()\n    model.add(Dense(1, input_dim=input_features_dimension, activation='sigmoid')) \n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[metrics.mae, metrics.mse, 'acc'])\n    return model\n\ndef create_model1(input_features_dimension) -> Sequential: \n    model = Sequential()\n        \n    model.add(Dense(64, input_dim=input_features_dimension))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(32))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(16))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[metrics.mae, metrics.mse, 'acc'])\n    \n    return model\n\ndef create_model2(input_features_dimension) -> Sequential: \n    model = Sequential()\n    \n    model = Sequential()\n    model.add(Dense(64, activation=\"relu\", input_dim=input_features_dimension))\n    model.add(Dropout(0.5))\n    model.add(Dense(32, activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.mae, metrics.mse, 'acc'])\n    #model.summary()\n    \n    return model\n\ndef create_model3(input_features_dimension) -> Sequential: \n    model = Sequential()\n        \n    model.add(Dense(32, input_dim=input_features_dimension))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(16))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(16))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[metrics.mae, metrics.mse, 'acc'])\n    \n    return model\n\ndef create_model4(input_features_dimension) -> Sequential: \n    model = Sequential()\n        \n    model.add(Dense(32, input_dim=input_features_dimension))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(32))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(32))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[metrics.mae, metrics.mse, 'acc'])\n    \n    return model\n\ndef create_model5(input_features_dimension) -> Sequential: \n    model = Sequential()\n        \n    model.add(Dense(64, input_dim=input_features_dimension))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(32))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(32))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[metrics.mae, metrics.mse, 'acc'])\n    \n    return model\n\ndef create_model6(input_features_dimension) -> Sequential: \n    model = Sequential()\n        \n    model.add(Dense(64, input_dim=input_features_dimension))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(64))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(32))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[metrics.mae, metrics.mse, 'acc'])\n    \n    return model","a433b5c3":"def crossvalidation(X, y, model_creator, folds_number = 10, epochs = 100, batch_size = 64) -> str:\n    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n    cvscores = []\n    \n    for train, test in kfold.split(X, y):\n        model = model_creator(input_features_dimension=X[train].shape[1])\n        model.fit(X[train], y[train], epochs=epochs, batch_size=batch_size, verbose=0)\n        scores = model.evaluate(X[test], y[test], verbose=0)\n        cvscores.append(scores[3] * 100)    \n        \n    return [np.mean(cvscores), np.std(cvscores)]","c67a196d":"def perform_tests(\n        models_creators = {\"logistic model\": create_logistic_model, \n                           \"model1\": create_model1, \n                           \"model2\": create_model2, \n                           \"model3\": create_model3, \n                           \"model4\": create_model4,\n                           \"model5\": create_model5,\n                           \"model6\": create_model6},\n        epochs_to_test = [200, 150, 100, 50, 25], \n        batch_sizes_to_test = [256, 128, 64, 32, 16, 8],\n        X = X,\n        y = y):\n    results = [] \n\n    for model_creator in models_creators:\n        for epochs in epochs_to_test:\n            for batch_size in batch_sizes_to_test:\n                validation_result = (\n                    crossvalidation(\n                        X, \n                        y, \n                        models_creators[model_creator], \n                        epochs=epochs, \n                        batch_size=batch_size)\n                )\n                \n                final_results = [model_creator]\n                final_results.extend(validation_result)\n                final_results.extend([epochs, batch_size])\n                \n                results.append(final_results)\n                \n                print(\"Crossvalidation of {} for {} batch size and {} epochs completed.\".format(model_creator, batch_size, epochs))\n\n    for result in results: \n        print(result)\n        \n    return results\n  \n# Run this to perform tests\n# perform_tests()","0b25fa22":"models_results = pd.DataFrame(\n    columns=[\"model_name\", \"accuracy\", \"deviation\", \"epochs\", \"batch_size\"],\n    data = [\n        (\"logistic model\", 95.08, 1.73, 200, 256),\n        (\"logistic model\", 97.00, 1.77, 200, 128),\n        (\"logistic model\", 97.89, 1.31, 200, 64),\n        (\"logistic model\", 98.07, 1.46, 200, 32),\n        (\"logistic model\", 97.71, 1.12, 200, 16),\n        (\"logistic model\", 97.54, 1.15, 200, 8),\n        (\"logistic model\", 94.56, 2.77, 150, 256),\n        (\"logistic model\", 95.61, 1.79, 150, 128),\n        (\"logistic model\", 97.70, 1.60, 150, 64),\n        (\"logistic model\", 97.89, 1.32, 150, 32),\n        (\"logistic model\", 98.07, 1.23, 150, 16),\n        (\"logistic model\", 97.72, 1.36, 150, 8),\n        (\"logistic model\", 90.15, 5.92, 100, 256),\n        (\"logistic model\", 95.07, 2.83, 100, 128),\n        (\"logistic model\", 97.53, 1.79, 100, 64),\n        (\"logistic model\", 97.19, 1.79, 100, 32),\n        (\"logistic model\", 98.24, 1.11, 100, 16),\n        (\"logistic model\", 98.42, 0.95, 100, 8),\n        (\"logistic model\", 82.64, 6.59, 50, 256),\n        (\"logistic model\", 86.62, 7.13, 50, 128),\n        (\"logistic model\", 95.78, 2.28, 50, 64),\n        (\"logistic model\", 96.13, 1.31, 50, 32),\n        (\"logistic model\", 97.18, 1.61, 50, 16),\n        (\"logistic model\", 98.06, 1.65, 50, 8),\n        (\"logistic model\", 59.25, 21.56, 25, 256),\n        (\"logistic model\", 83.40, 14.65, 25, 128),\n        (\"logistic model\", 88.06, 6.26, 25, 64),\n        (\"logistic model\", 94.02, 3.10, 25, 32),\n        (\"logistic model\", 96.13, 1.73, 25, 16),\n        (\"logistic model\", 97.71, 1.58, 25, 8),\n        (\"model1\", 98.07, 1.46, 200, 256),\n        (\"model1\", 97.89, 1.53, 200, 128),\n        (\"model1\", 97.71, 1.58, 200, 64),\n        (\"model1\", 97.71, 1.37, 200, 32),\n        (\"model1\", 97.71, 1.77, 200, 16),\n        (\"model1\", 97.18, 1.97, 200, 8),\n        (\"model1\", 97.71, 1.13, 150, 256),\n        (\"model1\", 98.06, 0.94, 150, 128),\n        (\"model1\", 97.71, 1.13, 150, 64),\n        (\"model1\", 97.53, 1.41, 150, 32),\n        (\"model1\", 97.71, 1.58, 150, 16),\n        (\"model1\", 96.83, 1.89, 150, 8),\n        (\"model1\", 97.89, 1.06, 100, 256),\n        (\"model1\", 97.53, 1.41, 100, 128),\n        (\"model1\", 97.89, 1.06, 100, 64),\n        (\"model1\", 97.88, 1.32, 100, 32),\n        (\"model1\", 97.88, 1.53, 100, 16),\n        (\"model1\", 97.71, 1.58, 100, 8),\n        (\"model1\", 95.95, 2.39, 50, 256),\n        (\"model1\", 97.01, 1.12, 50, 128),\n        (\"model1\", 98.24, 1.37, 50, 64),\n        (\"model1\", 97.71, 1.13, 50, 32),\n        (\"model1\", 97.53, 1.17, 50, 16),\n        (\"model1\", 97.71, 1.58, 50, 8),\n        (\"model1\", 95.08, 1.53, 25, 256),\n        (\"model1\", 96.31, 1.83, 25, 128),\n        (\"model1\", 96.66, 2.16, 25, 64),\n        (\"model1\", 97.88, 1.53, 25, 32),\n        (\"model1\", 97.72, 1.37, 25, 16),\n        (\"model1\", 97.53, 1.41, 25, 8),\n        (\"model2\", 97.71, 1.58, 200, 256),\n        (\"model2\", 97.71, 1.58, 200, 128),\n        (\"model2\", 97.71, 1.77, 200, 64),\n        (\"model2\", 97.71, 1.58, 200, 32),\n        (\"model2\", 97.19, 1.80, 200, 16),\n        (\"model2\", 97.54, 1.60, 200, 8),\n        (\"model2\", 97.54, 1.16, 150, 256),\n        (\"model2\", 97.71, 1.58, 150, 128),\n        (\"model2\", 97.89, 1.31, 150, 64),\n        (\"model2\", 97.36, 1.96, 150, 32),\n        (\"model2\", 97.71, 1.77, 150, 16),\n        (\"model2\", 97.54, 2.25, 150, 8),\n        (\"model2\", 97.71, 1.13, 100, 256),\n        (\"model2\", 97.71, 1.12, 100, 128),\n        (\"model2\", 97.89, 1.52, 100, 64),\n        (\"model2\", 97.19, 1.79, 100, 32),\n        (\"model2\", 97.54, 1.61, 100, 16),\n        (\"model2\", 97.71, 1.38, 100, 8),\n        (\"model2\", 96.83, 2.23, 50, 256),\n        (\"model2\", 97.01, 1.59, 50, 128),\n        (\"model2\", 97.71, 1.38, 50, 64),\n        (\"model2\", 97.90, 1.87, 50, 32),\n        (\"model2\", 97.89, 1.54, 50, 16),\n        (\"model2\", 97.72, 1.58, 50, 8),\n        (\"model2\", 94.71, 2.26, 25, 256),\n        (\"model2\", 96.83, 1.56, 25, 128),\n        (\"model2\", 97.37, 1.41, 25, 64),\n        (\"model2\", 97.72, 1.36, 25, 32),\n        (\"model2\", 97.71, 1.58, 25, 16),\n        (\"model2\", 97.19, 1.61, 25, 8),\n        (\"model3\", 97.89, 1.06, 200, 256),\n        (\"model3\", 97.71, 1.37, 200, 128),\n        (\"model3\", 98.24, 1.36, 200, 64),\n        (\"model3\", 97.71, 1.58, 200, 32),\n        (\"model3\", 98.06, 1.24, 200, 16),\n        (\"model3\", 97.54, 1.39, 200, 8),\n        (\"model3\", 97.18, 1.61, 150, 256),\n        (\"model3\", 97.71, 1.13, 150, 128),\n        (\"model3\", 97.89, 1.32, 150, 64),\n        (\"model3\", 97.88, 1.32, 150, 32),\n        (\"model3\", 97.53, 1.41, 150, 16),\n        (\"model3\", 97.36, 1.42, 150, 8),\n        (\"model3\", 97.71, 1.37, 100, 256),\n        (\"model3\", 98.24, 1.11, 100, 128),\n        (\"model3\", 97.89, 1.31, 100, 64),\n        (\"model3\", 97.89, 1.32, 100, 32),\n        (\"model3\", 97.71, 1.58, 100, 16),\n        (\"model3\", 97.88, 1.32, 100, 8),\n        (\"model3\", 94.91, 2.01, 50, 256),\n        (\"model3\", 96.84, 1.90, 50, 128),\n        (\"model3\", 97.01, 2.22, 50, 64),\n        (\"model3\", 98.06, 1.24, 50, 32),\n        (\"model3\", 97.71, 1.76, 50, 16),\n        (\"model3\", 97.89, 1.32, 50, 8),\n        (\"model3\", 92.10, 3.85, 25, 256),\n        (\"model3\", 95.09, 3.00, 25, 128),\n        (\"model3\", 95.07, 1.36, 25, 64),\n        (\"model3\", 96.31, 1.68, 25, 32),\n        (\"model3\", 97.18, 1.43, 25, 16),\n        (\"model3\", 96.31, 2.28, 25, 8),\n        (\"model4\", 97.36, 1.60, 200, 256),\n        (\"model4\", 97.72, 1.93, 200, 128),\n        (\"model4\", 97.71, 1.58, 200, 64),\n        (\"model4\", 97.88, 1.32, 200, 32),\n        (\"model4\", 97.71, 1.58, 200, 16),\n        (\"model4\", 97.18, 1.40, 200, 8),\n        (\"model4\", 97.01, 1.56, 150, 256),\n        (\"model4\", 97.36, 1.80, 150, 128),\n        (\"model4\", 97.37, 1.41, 150, 64),\n        (\"model4\", 97.36, 1.62, 150, 32),\n        (\"model4\", 98.23, 1.12, 150, 16),\n        (\"model4\", 97.88, 1.32, 150, 8),\n        (\"model4\", 97.54, 1.61, 100, 256),\n        (\"model4\", 98.06, 0.95, 100, 128),\n        (\"model4\", 97.89, 1.05, 100, 64),\n        (\"model4\", 97.89, 1.53, 100, 32),\n        (\"model4\", 98.06, 1.24, 100, 16),\n        (\"model4\", 97.88, 1.53, 100, 8),\n        (\"model4\", 96.31, 1.85, 50, 256),\n        (\"model4\", 97.19, 0.86, 50, 128),\n        (\"model4\", 97.54, 1.16, 50, 64),\n        (\"model4\", 97.53, 1.79, 50, 32),\n        (\"model4\", 97.88, 1.32, 50, 16),\n        (\"model4\", 97.54, 1.16, 50, 8),\n        (\"model4\", 95.78, 1.80, 25, 256),\n        (\"model4\", 96.48, 2.48, 25, 128),\n        (\"model4\", 97.19, 1.15, 25, 64),\n        (\"model4\", 97.36, 1.65, 25, 32),\n        (\"model4\", 97.53, 1.18, 25, 16),\n        (\"model4\", 96.66, 1.46, 25, 8),\n        (\"model5\", 97.71, 1.12, 200, 256),\n        (\"model5\", 98.07, 1.46, 200, 128),\n        (\"model5\", 97.54, 1.79, 200, 64),\n        (\"model5\", 98.06, 1.46, 200, 32),\n        (\"model5\", 97.53, 1.60, 200, 16),\n        (\"model5\", 96.84, 1.31, 200, 8),\n        (\"model5\", 97.53, 1.18, 150, 256),\n        (\"model5\", 97.89, 1.32, 150, 128),\n        (\"model5\", 98.07, 1.65, 150, 64),\n        (\"model5\", 98.24, 1.36, 150, 32),\n        (\"model5\", 97.88, 1.32, 150, 16),\n        (\"model5\", 97.19, 1.40, 150, 8),\n        (\"model5\", 98.06, 0.94, 100, 256),\n        (\"model5\", 97.53, 1.17, 100, 128),\n        (\"model5\", 97.89, 1.05, 100, 64),\n        (\"model5\", 97.89, 1.05, 100, 32),\n        (\"model5\", 98.41, 1.24, 100, 16),\n        (\"model5\", 97.36, 1.80, 100, 8),\n        (\"model5\", 96.83, 2.23, 50, 256),\n        (\"model5\", 97.19, 1.41, 50, 128),\n        (\"model5\", 97.88, 1.32, 50, 64),\n        (\"model5\", 98.06, 1.24, 50, 32),\n        (\"model5\", 97.89, 1.06, 50, 16),\n        (\"model5\", 97.53, 1.41, 50, 8),\n        (\"model5\", 94.73, 3.60, 25, 256),\n        (\"model5\", 95.96, 1.92, 25, 128),\n        (\"model5\", 97.36, 1.18, 25, 64),\n        (\"model5\", 97.36, 1.18, 25, 32),\n        (\"model5\", 97.19, 1.16, 25, 16),\n        (\"model5\", 97.18, 1.95, 25, 8),\n        (\"model6\", 97.72, 1.76, 200, 256),\n        (\"model6\", 98.07, 1.99, 200, 128),\n        (\"model6\", 97.89, 1.72, 200, 64),\n        (\"model6\", 97.54, 1.41, 200, 32),\n        (\"model6\", 97.36, 1.80, 200, 16),\n        (\"model6\", 97.36, 1.42, 200, 8),\n        (\"model6\", 98.59, 1.05, 150, 256),\n        (\"model6\", 98.06, 0.94, 150, 128),\n        (\"model6\", 97.71, 1.58, 150, 64),\n        (\"model6\", 97.89, 1.72, 150, 32),\n        (\"model6\", 97.71, 1.77, 150, 16),\n        (\"model6\", 97.71, 2.09, 150, 8),\n        (\"model6\", 98.07, 1.22, 100, 256),\n        (\"model6\", 98.24, 1.11, 100, 128),\n        (\"model6\", 97.54, 1.40, 100, 64),\n        (\"model6\", 97.71, 1.13, 100, 32),\n        (\"model6\", 98.41, 1.24, 100, 16),\n        (\"model6\", 97.53, 1.41, 100, 8),\n        (\"model6\", 97.54, 0.87, 50, 256),\n        (\"model6\", 97.71, 1.58, 50, 128),\n        (\"model6\", 97.54, 1.16, 50, 64),\n        (\"model6\", 97.01, 1.57, 50, 32),\n        (\"model6\", 97.89, 1.72, 50, 16),\n        (\"model6\", 98.06, 1.66, 50, 8),\n        (\"model6\", 95.42, 1.98, 25, 256),\n        (\"model6\", 96.83, 1.56, 25, 128),\n        (\"model6\", 96.84, 1.71, 25, 64),\n        (\"model6\", 97.72, 1.58, 25, 32),\n        (\"model6\", 97.89, 1.31, 25, 16),\n        (\"model6\", 97.36, 1.17, 25, 8)\n    ]\n)","48eb8fd3":"def draw_heatmaps(\n        models_results_df: DataFrame,\n        model_name_column = \"model_name\", \n        batch_size_column = \"batch_size\", \n        epochs_column = \"epochs\",\n        accuracy_column = \"accuracy\"):\n    for model in models_results_df[model_name_column].unique():\n        plt.figure(figsize=(6,2))\n\n        current_data = (\n            models_results_df[models_results_df[model_name_column] == model]\n                .groupby([batch_size_column, epochs_column])[accuracy_column]\n                .sum()\n                .unstack()\n        )\n\n        sns.heatmap(data=current_data, annot=True, fmt='.2f', cmap=\"coolwarm\")\n\n        plt.suptitle(model)\n        plt.show()\n    \ndraw_heatmaps(models_results)","6950a713":"models_results.loc[models_results.accuracy.idxmax()]","86694dd5":"After removing unused columns there is no empty values in dataset so this step could be ommited.\n\n#### 3\/4. Standarization and feature extraction\nStandarization and feature extraction steps will be performed as a single step. Data will be standarized using `StandardScaler` from `scikit-learn`. Labels encoding will be performed using `LabelEncoder` also from `scikit-learn`. Labels will be extracted from column `diagnosis`, all remaining columns will be treated as features.","10199c7e":"## Dataset analysys\n\nIn this part we will focus on basic data transformation. It should contain dive into data structure. The result should be finding and removing unnecessary data parts from dataset and understanding of data. This part should be introduction to feature engineering. \n\n### Firstly let's check, structure of the data:","458961bd":"### And setup seed for Numpy:","1210fab1":"On those analysys we were able to reach 98.59% of accuracy on 10-fold crossvalidation.","47a84fc8":"# Breast cancer classification problem\n\nThis notebook will contain analysys and example attempt to solve the problem of classification breast cancer, basing on dataset placed here:\nhttps:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\n\n### Let's import all necessary libraries:","38495e4f":"### Next thing is reading dataset:","d0f1d321":"Almost all values are float numeric format, only two are different. The `id` column is probably unique identifier of observation, `diagnosis` is most likely our label column. This column should contain values represented one of few states which we will be using as label for classification purpose.\n\nLet's check out what values it contain:","698841bf":"Let's draw some heatmaps using those data:","1ea3de54":"### Cross-validation\n\nResults are validated using Stratified 10-fold [cross-validation](https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics). Below are presented utils methods used for performing tests.","0ac5e506":"### Results\n\nBest accuracy that was reached:","7888b408":"#### 2. Filling empty values\nLet's check out if there are any empty values data:","62ff7c5e":"This shows us that we have 569 rows\/observations which contains 33 columns\/features each. This is quite enough amount of observations and should be sufficient for create classification model. \n\n### Let's search for label column:","518e67fe":"Thanks to above methods we were able to prepare a set of results presented below:","c01b5e89":"Observations are set in rows and features are stored in columns, lets check how many observations we have:","33913d14":"`X` and `y` are standard names for features and labels. When we have prepared dataset as features and labels we can start classification.\n\n## Classification\n\nThis section will be devoted to classification model building. The main used tool will be Keras with TensorFlow backend. Keras was chosen because of it's easy usage and very descriptive API. Below are presented models that are tested in cross-validation process.","299e95d1":"This column contain only two values. `M` stands for `malignant` and `B` for `benign`. This values determines the type of tumor. There are two values so classifier which we are going to build will be binary. \n\nLet's check how many observations we have for each of this values:","ba6f68e8":"This analysys shows us that dataset is unbalanced. We have more `Benign` cases that `Malignat`. This is very common problem and in this case we should't worry about that because disproportion is relatively small.\n\n## Feature engineering\n\nThis part will contain feature engineering for our data. Basic feature engineering contain 4 steps:\n\n1. Filtering the data \n2. Filling empty values if there is a need\n3. Standarization of all values \n4. Features extraction\n\nAll this steps will be performed on our dataset.\n\n#### 1. Filtering the data\nData filtering step will contain removing unused columns. Columns that should be removed are `id` and `Unnamed: 32`. First is unique identifier of observation and second contains only empty values."}}