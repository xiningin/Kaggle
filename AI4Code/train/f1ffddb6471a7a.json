{"cell_type":{"7875719c":"code","24730e15":"code","930b430f":"code","c8f0b86a":"code","a574253c":"code","f9b6a597":"code","40f1d1a2":"code","801b61aa":"code","85543181":"code","944f5593":"code","7053edb4":"code","94f8689c":"code","502482f8":"code","098a5e32":"code","83e4934d":"markdown","86846df2":"markdown","5c7875a4":"markdown","3179393a":"markdown","f6ca03f3":"markdown","cc337d20":"markdown"},"source":{"7875719c":"# Import the necessary packages\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","24730e15":"# Import and read dataset\n\ninput_ = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\ndf = pd.read_csv(input_)\n\ndf.head(10)","930b430f":"import pandas_profiling as pdp\nreport = pdp.ProfileReport(df, title='Pandas Profiling Report')","c8f0b86a":"report.to_widgets() ","a574253c":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","f9b6a597":"x = df.drop(columns='DEATH_EVENT')\ny = df['DEATH_EVENT']\n\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","40f1d1a2":"df.describe()","801b61aa":"df=df[df['ejection_fraction']<70]","85543181":"## data preprocessing\n\n#inp_data = df.drop(df[['DEATH_EVENT']], axis=1)\ninp_data = df.iloc[:,[0,4,7,11]]\nout_data = df[['DEATH_EVENT']]\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=0)\n\n## Applying Transformer\nsc=StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","944f5593":"## X_train, X_test, y_train, y_test Shape\n\nprint(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","7053edb4":"## I coded this method for convenience and to avoid writing the same code over and over again\n\ndef result(clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    \n    print('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\n    print('Decision Tree Classifier f1-score      : {:.4f}'.format(f1_score( y_test , y_pred)))\n    print('Decision Tree Classifier precision     : {:.4f}'.format(precision_score(y_test, y_pred)))\n    print('Decision Tree Classifier recall        : {:.4f}'.format(recall_score(y_test, y_pred)))\n    print(\"Decision Tree Classifier roc auc score : {:.4f}\".format(roc_auc_score(y_test,y_pred)))\n    print(\"\\n\",classification_report(y_pred, y_test))\n    \n    plt.figure(figsize=(6,6))\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    sns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")\n    plt.title(\"DecisionTreeClassifier Confusion Matrix (Rate)\")\n    plt.show()\n    \n    cm = confusion_matrix(y_test,y_pred)\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\",\n                xticklabels=[\"FALSE\",\"TRUE\"],\n                yticklabels=[\"FALSE\",\"TRUE\"],\n                cbar=False)\n    plt.title(\"DecisionTreeClassifier Confusion Matrix (Number)\")\n    plt.show()\n    \ndef sample_result(class_weight=None,criterion='gini',max_depth=None,max_features=None,max_leaf_nodes=None,min_samples_split=2):    \n    scores = [] \n    for i in range(0,10000): # 10.000 samples\n        X_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2)\n        clf = DecisionTreeClassifier(class_weight= class_weight,\n                                     criterion=criterion,\n                                     max_depth=max_depth,\n                                     max_features=max_features,\n                                     max_leaf_nodes=max_leaf_nodes,\n                                     min_samples_split=min_samples_split) \n        sc=StandardScaler()\n        X_train = sc.fit_transform(X_train)\n        X_test = sc.fit_transform(X_test)\n        clf.fit(X_train, y_train)\n        scores.append(accuracy_score(clf.predict(X_test), y_test)) \n    \n    plt.hist(scores)\n    plt.show()\n    print(\"Best Score: {}\\nMean Score: {}\".format(np.max(scores), np.mean(scores)))","94f8689c":"clf = DecisionTreeClassifier(random_state=0)\nresult(clf)\nsample_result()","502482f8":"param_grid = {\n    \"max_depth\": np.arange(1,10),\n    \"min_samples_split\": [0.001, 0.01, 0.1, 0.2, 0.02, 0.002],\n    \"criterion\": [\"gini\", \"entropy\", None],\n    \"max_leaf_nodes\": np.arange(1,10),\n    \"class_weight\": [\"balanced\", None]\n}\n\nclf = DecisionTreeClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=2, cv=10)\ngrid.fit(X_train, y_train)\ngrid.best_params_","098a5e32":"clf = DecisionTreeClassifier(\n    class_weight='balanced',\n    criterion='gini',\n    max_depth=1,\n    max_leaf_nodes=2,\n    min_samples_split=0.001,\n    random_state=0\n)\n\nresult(clf)\n# class_weight=None,criterion='gini',max_depth=None,max_features=None,max_leaf_nodes=None,min_samples_split=2\nsample_result('balanced',\"gini\",1 ,None , 2,  0.001)","83e4934d":"## Reporting\nI evaluated the results I found with Confusion Matrix, the results are as follows:\n**Correctly predicted -> %98.34 (361 of 406 predict are correct)**\n- True Negative -> %71.67 (43 people) -> Those who were predicted not to die and who did not die\n- True Positive -> %26.67 (16 people) -> Those who were predicted to die and who did die\n\n**Wrong predicted-> %10.98 (45 of 406 predict are wrong)**\n- False Positive -> %00.00 (0 people) -> Those who were predicted to die but who did not die\n- False Negative -> %01.67 (1 people) -> Those who were predicted to not die but who did die","86846df2":"---","5c7875a4":"### Advanced Method","3179393a":"## Coding Time","f6ca03f3":"![cart](https:\/\/i.ibb.co\/qmJPYVf\/decision-RTree.png)\n\n- **ML Part 1** - Logistic Regression\n- **ML Part 2** - K-Nearest Neighbors (KNN)\n- **ML Part 3** - Support Vector Machine (SVM)\n- **ML Part 4** - Artificial Neural Network (NN)\n- **ML Part 5 - Classification and Regression Tree (CART)**\n- **ML Part 6** - Random Forests\n- **ML Part 7** - Gradient Boosting Machines (GBM)\n- **ML Part 8** - XGBoost\n- **ML Part 9** - LightGBM\n- **ML Part 10** - CatBoost\n\n---\nDecision tree is a tree-based algorithm used to solve regression and classification problems. A homogeneous possibility to derive output is framed by an inverted tree branched from the distributed root node to highly heterogeneous leaf nodes. Regression trees use classification trees for the dependent variable with continuous values and for the dependent variable with discrete values.\n\n## Basic Theory\nThe decision tree is derived from arguments where each node has a condition on a property. Nodes decide which node goes to the next node depending on the condition. Once the leaf node is reached, an output is predicted. The correct set of conditions makes the tree fruitful. entropy \/ information gain is used as criteria for selecting conditions in nodes. A recursive, greedy-based algorithm is used to derive the tree structure.\n\n![](https:\/\/miro.medium.com\/max\/690\/1*xzF10JmR3K0rnZ8jtIHI_g.png)\nIn the above diagram, we can see a tree with internal nodes (conditions) and leaf nodes (reject \/ accept offer) with labels.\n\n\n## Algorithm for Choosing Conditions\nFor CART (classification and regression trees), we use the gini index as the classification criteria. It is a measure for calculating how well data points are mixed.\n![](https:\/\/miro.medium.com\/max\/415\/0*asbVp_8lwEsbfpOv.png)\nThe maximum gini indexed feature is chosen as the next condition at each stage of building the decision tree. The gini score will be maximum when the set is unevenly mixed.\n\n\n## Advantages\n- There is no need for preprocessing on the data.\n- There are no assumptions about the distribution of data.\n- Manages collinearity efficiently.\n- Decision trees can provide a clear explanation for the forecast.\n\n## Disadvantages\n- If we continue to build the tree to achieve high purity, we may have overfitted the model. Decision tree pruning can be used to solve this problem.\n- Prone to outliers.\n- The tree can get very complex when training complex data sets.\n- It loses valuable information while processing continuous variables.\n\n## Hyperparameters\nThe decision tree contains many hyperparameters and I will list a few of them.\n- criterion\n    - What cost function to select the next tree node. Mostly used are gini \/ entropy.\n- maximum depth\n    - The maximum allowed depth of the decision tree.\n- minimum sample division\n    - It is the minimum node required to split an internal node.\n- minimum sample sheet\n    - Minimum sample required in leaf node.\n    \n\n## Comparison with Other Models\n### Decision Trees vs Random Forest\n- Random Forest is a collection of decision trees and the average \/ majority vote of the forest is chosen as the predicted output.\n- The Random Forest model will be less inclined to adapt to the Decision tree and will provide a more general solution.\n- Random Forest is more robust and accurate than decision trees.\n\n### Decision Trees vs KNN\n- Both are nonparametric methods.\n- While the decision tree supports automatic feature interaction, it cannot be KNN.\n- The decision tree is faster due to the expensive real time implementation of KNN.\n\n### Decision Trees vs Naive Bayes\n- The decision tree is a distinctive model, while Naive bayes is a generative model.\n- Decision trees are more flexible and easy.\n- Decision tree pruning can neglect some core values in the training data, resulting in accuracy for one shot.\n\n### Decision Trees vs Neural Networks (NN)\n- Both find nonlinear solutions and have interactions between independent variables.\n- Decision trees are better when there is a large set of categorical values in education data.\n- When the scenario asks for an explanation about the decision, the decision trees are better than NN.\n- When there is sufficient training data, NN performs better than the decision tree.\n\n### Decision Trees vs SVM\n- While SVM uses kernel numbers to solve nonlinear problems, decision trees derive hyperrectangles in the input space to solve the problem.\n- Decision trees are better for categorical data and handle collinearity better than SVM.","cc337d20":"### Simple Metod\nI applied Decision Tree directly without changing anything and the result is as follows"}}