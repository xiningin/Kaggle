{"cell_type":{"7968b411":"code","6454751b":"code","8fa42ca5":"code","71be348b":"code","0e4312d2":"code","1a0f031b":"code","392c54d9":"code","4e45d890":"code","1852de65":"code","5c4ce307":"code","739cf869":"code","43841bfc":"code","d96fc68d":"code","5dca3aa8":"code","658db9c2":"code","15151e29":"code","030c82a9":"code","244875ea":"code","bc608fb4":"code","ecde28e0":"markdown","99d2543d":"markdown","70790c65":"markdown","05060148":"markdown","1e7fa6ba":"markdown","7e29ed71":"markdown"},"source":{"7968b411":"import numpy as np \nimport pandas as pd\nfrom sklearn.preprocessing import  LabelEncoder\nfrom tqdm.auto import tqdm\nimport random\nimport os\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\nimport dill\nimport tensorflow.keras.backend as K\nfrom tqdm.auto import tqdm\nfrom tensorflow.keras import mixed_precision\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, AutoConfig,TFAutoModel\nimport json","6454751b":"# NEW on TPU in TensorFlow 24: shorter cross-compatible TPU\/GPU\/multi-GPU\/cluster-GPU detection code\n\ntry: # detect TPUs\n    tpu  = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    tf.config.experimental_connect_to_cluster(tpu )\n    tf.tpu.experimental.initialize_tpu_system(tpu )\n    strategy = tf.distribute.TPUStrategy(tpu )\n    print('Using TPU')\nexcept ValueError: # detect GPUs\n    tpu = None\n    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","8fa42ca5":"seed=999\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\nos.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\ntf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nprint('Mixed precision enabled')","71be348b":"TRAIN = False ","0e4312d2":"features = pd.read_csv(\"..\/input\/nbme-score-clinical-patient-notes\/features.csv\")\npatient_notes = pd.read_csv(\"..\/input\/nbme-score-clinical-patient-notes\/patient_notes.csv\")\ntest = pd.read_csv(\"..\/input\/nbme-score-clinical-patient-notes\/test.csv\")\ntrain= pd.read_csv(\"..\/input\/nbme-score-clinical-patient-notes\/train.csv\")\nsample_submission= pd.read_csv(\"..\/input\/nbme-score-clinical-patient-notes\/sample_submission.csv\")","1a0f031b":"test = test.merge(patient_notes,on=['case_num','pn_num']).merge(features,on=['case_num','feature_num'])\ntrain = train.merge(patient_notes,on=['case_num','pn_num']).merge(features,on=['case_num','feature_num'])","392c54d9":"train.head(5)","4e45d890":"MODEL_NAME = 'bert-base-uncased'\nDATA_PATH = \"..\/input\/nbmebertv1\"\nDATA_EXISTS = os.path.exists(DATA_PATH)\nSEQUENCE_LENGTH = 512","1852de65":"if DATA_EXISTS:\n    tokenizer = AutoTokenizer.from_pretrained(DATA_PATH+\"\/my_tokenizer\/\",normalization=True)\n    config = AutoConfig.from_pretrained(DATA_PATH+\"\/my_tokenizer\/config.json\")\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,normalization=True)\n    config = AutoConfig.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained('my_tokenizer')\n    config.save_pretrained('my_tokenizer')","5c4ce307":"EMPTY =  'EMPTY'\nCLASSES = [EMPTY,]+features.feature_num.unique().tolist()\n\nif DATA_EXISTS:\n    label_encoder = dill.load(open(DATA_PATH+\"\/label_encoder.dill\",'rb'))\nelse:\n    # label_encoder\n    label_encoder = LabelEncoder()\n    # Encode labels\n    label_encoder.fit(CLASSES)\n    dill.dump(label_encoder,open('label_encoder.dill','wb'))\ntrain['TARGET']= label_encoder.transform(train['feature_num'])\ntest['TARGET']= label_encoder.transform(test['feature_num'])\nN_CLASSES = len(label_encoder.classes_)\nEMPTY_IDX = label_encoder.transform([EMPTY,]) [0]","739cf869":"def decode_location(locations):\n    for x in [\"[\",\"]\",\"'\"]:\n        locations = locations.replace(x,'')\n    locations = locations.replace(',',';')\n    locations = locations.split(\";\")\n    res = []\n    for location in locations:\n        if location:\n            x,y = location.split()\n            res.append((int(x),int(y)))\n    return sorted(res,key=lambda x:x[0])\n    ","43841bfc":"if DATA_EXISTS:\n    sequences = np.load(open(DATA_PATH+\"\/sequences.npy\",'rb'))\n    masks = np.load(open(DATA_PATH+\"\/masks.npy\",'rb'))\n    labels = np.load(open(DATA_PATH+\"\/labels.npy\",'rb'))\nelse:\n    sequences, labels, masks = [], [], []\n    for g1 in tqdm(train.groupby('pn_num')):\n        gdf = g1[1]\n        pn_history  = gdf.iloc[0].pn_history\n\n        tokens = tokenizer.encode_plus(pn_history, max_length=SEQUENCE_LENGTH, padding='max_length',truncation=True, return_offsets_mapping=True)\n        sequence = tokens['input_ids']\n        attention_mask = tokens['attention_mask']\n        label = np.array([EMPTY_IDX for _ in range(SEQUENCE_LENGTH)])\n\n        # BUILD THE TARGET ARRAY\n        offsets = tokens['offset_mapping']\n        label_empty = True\n        for index, row in gdf.iterrows():\n            TARGET = row.TARGET\n            for i, (w_start, w_end) in enumerate(offsets):\n                for start,end in decode_location(row.location):\n                    if w_start < w_end and (w_start >= start) and (end >= w_end):\n                        label[i] = TARGET\n                        label_empty = False\n                    if w_start >= w_end:\n                        break\n        if not label_empty:\n            sequences.append(sequence)\n            masks.append(attention_mask)\n            labels.append(label)\n\n    sequences = np.array(sequences).astype(np.int32)\n    masks = np.array(masks).astype(np.uint8)\n    labels = np.array(tf.keras.utils.to_categorical(labels,N_CLASSES)).astype(np.uint8)\n\n    np.save(open(\"sequences.npy\",'wb'), sequences)\n    np.save(open(\"masks.npy\",'wb'), masks)\n    np.save(open(\"labels.npy\",'wb'), labels)","d96fc68d":"def build_model():\n    \n    tokens = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), name = 'tokens', dtype=tf.int32)\n    attention = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), name = 'attention', dtype=tf.int32)\n    \n    if DATA_EXISTS:\n        config = AutoConfig.from_pretrained(DATA_PATH+\"\/my_tokenizer\/config.json\")\n        backbone = TFAutoModel.from_config(config)\n    else:\n        config = AutoConfig.from_pretrained(MODEL_NAME)\n        backbone = TFAutoModel.from_pretrained(MODEL_NAME,config=config)\n    \n    out = backbone(tokens, attention_mask=attention)[0]\n    out = tf.keras.layers.Dropout(0.2)(out)\n    out = tf.keras.layers.Dense(N_CLASSES, activation='softmax')(out)\n    \n    model = tf.keras.Model([tokens,attention],out)\n    \n    return model","5dca3aa8":"if TRAIN:\n    with strategy.scope():\n        model = build_model()\n\n        callback = tf.keras.callbacks.EarlyStopping(monitor='loss',mode='min', patience=3)\n\n        # Compile the model\n        model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n                      loss=tf.keras.losses.categorical_crossentropy,metrics=['acc',])\n\n        history = model.fit((sequences,masks),labels,\n                            batch_size=12,\n                            epochs=10,\n                            callbacks=[callback,])\n\n        model.save_weights(f'model.h5')","658db9c2":"if not TRAIN:\n    model = build_model()\n    model.load_weights(DATA_PATH+\"\/model.h5\")","15151e29":"test_sequences, test_masks, test_offsets = [], [],[]\nrow_ids = []\ntargets = []\n\nfor g1 in tqdm(test.groupby('pn_num')):\n    gdf = g1[1]\n    pn_history  = gdf.iloc[0].pn_history\n    targets.append([])\n    row_ids.append([])\n    \n    test_tokens = tokenizer.encode_plus(pn_history, max_length=SEQUENCE_LENGTH, padding='max_length',truncation=True, return_offsets_mapping=True)\n    test_sequence = test_tokens['input_ids']\n    test_attention_mask = test_tokens['attention_mask'] \n\n    # BUILD THE TARGET ARRAY\n    offset = test_tokens['offset_mapping']\n    \n    for index, row in gdf.iterrows():\n        targets[-1].append(row.TARGET)\n        row_ids[-1].append(row.id)\n         \n    test_sequences.append(test_sequence)\n    test_masks.append(test_attention_mask)\n    test_offsets.append(offset)\n\ntest_sequences = np.array(test_sequences).astype(np.int32)\ntest_masks = np.array(test_masks).astype(np.uint8)\ntargets_to_row_ids = [dict(zip(a,b)) for a,b in zip(targets,row_ids)]","030c82a9":"preds = model.predict((test_sequences,test_masks),batch_size=16)\npreds = np.argmax(preds,axis=-1)","244875ea":"def decode_position(pos):\n    return \";\".join([\" \".join(np.array(p).astype(str)) for p in pos])\n\n\ndef translate(preds,targets_to_row_ids,offsets):\n    all_ids = []\n    all_pos = []\n\n    for k in range(len(preds)):\n        offset = offsets[k]\n        pred = preds[k]\n        targets_to_ids = targets_to_row_ids[k]\n        \n        prediction = {targets_to_ids[t]:[] for t in targets_to_ids}\n        i = 0\n        while i<SEQUENCE_LENGTH:\n            label = pred[i]\n            \n            if label == EMPTY_IDX:\n                i += 1\n                continue\n            if label in targets_to_ids:\n                key = targets_to_ids[label]\n                start = offset[i][0]\n                while i<SEQUENCE_LENGTH:\n                    if pred[i] != label:\n                        break\n                    else:\n                        end = max(offset[i])\n                    i += 1\n                if  end == 0:\n                    break\n                prediction[key].append((start,end))\n            else:\n                i+=1\n        for key in prediction:\n            all_ids.append(key)\n            all_pos.append(decode_position(prediction[key]))\n    df = pd.DataFrame({\n        \"id\":all_ids,\n        \"location\": all_pos\n    })\n    return df","bc608fb4":"sub = translate(preds,targets_to_row_ids,test_offsets)\nsub.to_csv('submission.csv',index=False)\nsub.head(50)","ecde28e0":"# Encode the label","99d2543d":" # Load dataframes","70790c65":"# Tokenizer","05060148":"# Submit","1e7fa6ba":"# Model training","7e29ed71":"# Define Model"}}