{"cell_type":{"f9255138":"code","b4dc1fc3":"code","3a76afb5":"code","89bd6eea":"code","f85053ca":"code","0c3557eb":"code","8842a790":"code","dba67a1e":"code","38ea6eaf":"code","f706cad6":"code","97f8b4bf":"code","70a92f45":"code","4597641c":"code","f73872f2":"code","a4683188":"code","99257f33":"code","2706433c":"code","bd5856d0":"code","417b403b":"code","ae38774a":"code","e51b7184":"code","5627132a":"code","f18b8b7c":"code","132c97f1":"code","678aba14":"code","8a0fb545":"code","da55d745":"code","787b7bcf":"code","354b4e8b":"code","cddd591e":"code","56b61176":"code","d56d5c29":"code","ea304de3":"code","f3ea970a":"code","e5913bd2":"code","6cbe67d1":"code","8275f3a1":"code","a72f9093":"markdown","332b4269":"markdown","b2fa422b":"markdown","b9758448":"markdown","d900d716":"markdown","358a05e5":"markdown","90bd2efd":"markdown","afedefaa":"markdown","5011222f":"markdown","711eb355":"markdown","f28be71d":"markdown","b595fff3":"markdown","d1450881":"markdown","e266fee8":"markdown","03f58f06":"markdown","32901bd5":"markdown","bb1cf2f9":"markdown","85486e1a":"markdown","0c63b532":"markdown","5ef50d40":"markdown","b25b7086":"markdown","851ce4cf":"markdown","3d8cc2cf":"markdown","bff47601":"markdown","e7aabe1a":"markdown"},"source":{"f9255138":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.axes\n\nfrom sklearn.svm import SVC,LinearSVC, SVR\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.externals import joblib\n\nfrom sklearn.metrics import roc_auc_score, precision_recall_fscore_support, classification_report \nfrom sklearn.metrics import precision_recall_curve, confusion_matrix, roc_curve, auc\n\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n#plt.style.use('dark_background')\ncurrent_palette = sns.color_palette('colorblind')\nsns.palplot(current_palette)","b4dc1fc3":"data_set = pd.read_csv('..\/input\/loan_data_set.csv')","3a76afb5":"data_set.info()","89bd6eea":"data_set.LoanAmount = data_set.LoanAmount*1000","f85053ca":"data_set.Loan_Status.value_counts(normalize = True).reset_index()","0c3557eb":"sns.barplot(data = data_set.Loan_Status.value_counts(normalize = True).reset_index(),\n            x = 'index',\n            y = 'Loan_Status',\n            palette=current_palette)","8842a790":"def prop_check(data):\n    f, axes = plt.subplots(6,2,figsize= (12,20))\n    plt.suptitle('Train data, count vs proportion of each object feature vs Loan_Status', size =16, y = 0.9)\n    col = data.columns[1:data.shape[1]-1]\n    r = 0\n    for i in col:\n        if (data.dtypes == 'object')[i]:        \n            data_prop = (data['Loan_Status']\n                          .groupby(data[i])\n                          .value_counts(normalize = True)\n                          .rename('prop')\n                          .reset_index())\n            sns.countplot(data = data, \n                          x ='Loan_Status', \n                          hue = i, \n                          ax = axes[r,0], \n                          hue_order=data_prop[i].unique(), \n                          palette=current_palette)\n            sns.barplot(data = data_prop, \n                        x = 'Loan_Status', \n                        y = 'prop',\n                        hue = i,\n                        ax = axes[r,1],\n                        palette=current_palette)\n            r = r+1\nprop_check(data_set)","dba67a1e":"def make_index(df):\n    df.set_index('Loan_ID', inplace=True)\n    return df\ndata_set = make_index(data_set)","38ea6eaf":"data_set.dropna(inplace=True)","f706cad6":"sns.pairplot(data_set, hue = 'Loan_Status', palette=current_palette)","97f8b4bf":"def categorize(df):\n    df.Gender.replace({'Male': 1, 'Female': 0}, inplace = True)\n    df.Married.replace({'Yes': 1, 'No': 0}, inplace = True)\n    df.Education.replace({'Graduate': 1, 'Not Graduate': 0}, inplace = True)\n    df.Self_Employed.replace({'Yes': 1, 'No': 0}, inplace = True)\n    df = df.join(pd.get_dummies(df.Dependents, prefix='Dependents'))\n    df.drop(columns= ['Dependents', 'Dependents_3+'], inplace=True)\n    df = df.join(pd.get_dummies(df.Property_Area, prefix='Property_Area'))\n    df.drop(columns= ['Property_Area', 'Property_Area_Rural'], inplace=True)\n    return df","70a92f45":"data_set = categorize(data_set)","4597641c":"data_set.Loan_Status.replace({'Y': 1, 'N':0}, inplace=True)","f73872f2":"def add_feat(df):\n    ln_monthly_return = np.log(df.LoanAmount\/df.Loan_Amount_Term)\n    df['ln_monthly_return'] = (ln_monthly_return - np.mean(ln_monthly_return))\/(np.std(ln_monthly_return)\/np.sqrt(len(ln_monthly_return)))\n    \n    ln_total_monthly_income = np.log(df.ApplicantIncome + df.CoapplicantIncome)\n    df['ln_total_income'] = (ln_total_monthly_income - np.mean(ln_total_monthly_income))\/(np.std(ln_total_monthly_income)\/np.sqrt(len(ln_total_monthly_income)))\n    \n    ln_LoanAmount = np.log(1000*df.LoanAmount)\n    df['ln_LoanAmount'] = (ln_LoanAmount - np.mean(ln_LoanAmount))\/(np.std(ln_LoanAmount)\/np.sqrt(len(ln_LoanAmount)))\n    \n    \n    return df\n","a4683188":"data_set = add_feat(data_set)","99257f33":"\ndef norm_plt(df):\n    f, axes = plt.subplots(3,2,figsize= (12,15),squeeze=False)\n\n    ######total income########\n    sns.distplot(df.ln_total_income\n                 ,ax=axes[0,0]).set_title('ln(total_income) norm distribution')\n    #axes[0,0].set_xlim(-100,100)\n    axes[0,0].text(0.03, 0.85,\n                   'skew: {0:0.2}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.ln_total_income),\n                                          kurtosis(df.ln_total_income)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[0,0].transAxes,\n                   bbox={'facecolor': 'white'})\n    sns.distplot((df.ApplicantIncome+df.CoapplicantIncome),\n                 ax=axes[0,1]).set_title('total_income distribution')\n    axes[0,1].text(0.7, 0.85,\n                   'skew: {0:0.2f}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.ApplicantIncome+df.CoapplicantIncome),\n                           kurtosis(df.ApplicantIncome+df.CoapplicantIncome)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[0,1].transAxes,\n                   bbox={'facecolor': 'white'})\n\n    #######monthly return###########\n    sns.distplot(df.ln_monthly_return,\n                 ax=axes[1,0]).set_title('ln(monthly_return) norm distribution')\n    #axes[1,0].set_xlim(-100,100)\n    axes[1,0].text(0.03, 0.85,\n                   'skew: {0:0.2}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.ln_monthly_return),\n                           kurtosis(df.ln_monthly_return)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[1,0].transAxes,\n                   bbox={'facecolor': 'white'})\n\n    sns.distplot((1000*df.LoanAmount\/df.Loan_Amount_Term),\n                 ax=axes[1,1]).set_title('monthly_return distribution')\n    axes[1,1].text(0.7, 0.85,\n                   'skew: {0:0.2f}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.LoanAmount\/df.Loan_Amount_Term),\n                           kurtosis(df.LoanAmount\/df.Loan_Amount_Term)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[1,1].transAxes,\n                   bbox={'facecolor': 'white'})\n\n    ######norm ln_LoanAmount########\n    sns.distplot(df.ln_LoanAmount\n                 ,ax=axes[2,0]).set_title('ln(LoanAmount) norm distribution')\n    #axes[2,0].set_xlim(-100,100)\n    axes[2,0].text(0.03, 0.85,\n                   'skew: {0:0.2}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.ln_LoanAmount),\n                                          kurtosis(df.ln_LoanAmount)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[2,0].transAxes,\n                   bbox={'facecolor': 'white'})\n    sns.distplot((df.LoanAmount),\n                 ax=axes[2,1]).set_title('LoanAmount distribution')\n    axes[2,1].text(0.7, 0.85,\n                   'skew: {0:0.2f}\\nkurtosis: {1:0.2f}'\n                   .format(skew(df.LoanAmount),\n                           kurtosis(df.LoanAmount)),\n                   horizontalalignment='left',\n                   verticalalignment='bottom',\n                   transform=axes[2,1].transAxes,\n                   bbox={'facecolor': 'white'})\n    \n    \n    ####### adding grid to the graph#########\n    for i in range(3):\n        for j in range(2):\n            axes[i,j].grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')","2706433c":"norm_plt(data_set)","bd5856d0":"dropit=['LoanAmount', \n        'Loan_Amount_Term', \n        'ApplicantIncome',\n        'CoapplicantIncome',\n        'Married',\n        'Dependents_0',\n        'Dependents_1',\n        'Dependents_2']\ndata_set.drop(columns=dropit, \n           inplace=True)","417b403b":"data_set['Loan_Status'].value_counts(normalize=True)\nsns.barplot(data = data_set.Loan_Status.value_counts(normalize = True).reset_index(),\n            x = 'index',\n            y = 'Loan_Status',\n            palette=current_palette)\nplt.grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')","ae38774a":"\ndef cv_check(X,y, CV):\n    models = [\n        RandomForestClassifier(criterion='gini',\n                               n_estimators=50,\n                               max_depth=11,\n                               max_features=6,\n                               random_state=42,\n                               class_weight='balanced_subsample',\n                               n_jobs=4),\n        SVC(C=1, kernel='rbf', gamma='auto',random_state=42,class_weight='balanced'),\n        LogisticRegression(solver='lbfgs',\n                           multi_class='ovr',\n                           max_iter=500,\n                           C=1,\n                           random_state=42,\n                           class_weight='balanced'),\n        GaussianNB(),\n        #LinearSVC(C=1, \n        #         max_iter=500,\n        #          random_state=0),\n        DummyClassifier(strategy='most_frequent',random_state=42)\n    ]\n\n    entries = []\n    \n    for model in models:\n        model_name = model.__class__.__name__\n        print (\"Currently fitting: {}\".format(model_name))\n        accuracies = cross_val_score(model,\n                                     X,\n                                     y, \n                                     scoring='roc_auc', cv=CV, n_jobs=4)\n        for fold_idx, accuracy in enumerate(accuracies):\n            entries.append((model_name, fold_idx, accuracy))\n        cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'roc_auc'])\n        \n    return cv_df","e51b7184":"def cv_bp(cv_df, title, axes):\n    axes.grid(b=True, \n              which='both', \n              axis='both', \n              color='grey', \n              linestyle = '--', \n              linewidth = '0.3')    \n    sns.boxplot(x='model_name', \n                y='roc_auc', \n                data=cv_df, \n                width = 0.5, \n                ax=axes,\n                palette=current_palette).set_title(title)\n    sns.stripplot(x='model_name', \n                  y='roc_auc',\n                  data=cv_df, \n                  size=5, jitter=True, \n                  edgecolor=\"grey\", \n                  linewidth=1, \n                  ax=axes)\n    plt.ylim(0.2,1)\n    plt.savefig('{}.png'.format(title), format='png')\n    #plt.show()","5627132a":"f, axes = plt.subplots(1,1,figsize= (20,8),squeeze=False, sharey=True)\ncv_bp(cv_check(data_set.drop(['Loan_Status'],axis=1),\n               data_set.Loan_Status,10), '{} without NAs'.format('train'),axes[0,0])","f18b8b7c":"def model_score(train, model, grid_values, scorers_list):\n    X_train = train.drop(columns=['Loan_Status'])\n    y_train = train['Loan_Status']\n    \n    clf_dict = {}\n    \n    for i, scorer in enumerate(scorers_list):\n        clf_eval = GridSearchCV(model, param_grid=grid_values, scoring=scorer, cv=5, iid=False)\n        clf_eval.fit(X_train,y_train)\n        print('Grid best parameters for {0}: {1} scoring: {2}'\n              .format(scorer, clf_eval.best_params_, round(clf_eval.best_score_,3)))\n        clf_dict[scorer] = clf_eval\n    return clf_dict","132c97f1":"grid_values = {'max_features': [4, 5, 6, 7],\n              'max_depth': [3, 7, 11, 13]}\nscorers_list = ['accuracy','roc_auc','precision','recall', 'f1']\n\nrf_cv = model_score(data_set,\n            RandomForestClassifier(random_state=42, \n                                   n_jobs=4, \n                                   class_weight='balanced_subsample', \n                                   n_estimators=50), \n            grid_values, \n            scorers_list)\n\ntemp_df1 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df1[i]=rf_cv[i].cv_results_['mean_test_score'][rf_cv[i].cv_results_['param_max_features']==4]\ntemp_df1['max_depth'] = rf_cv['roc_auc'].cv_results_['param_max_depth'][rf_cv['roc_auc'].cv_results_['param_max_features']==4]\ntemp_df1.set_index('max_depth', inplace=True)\nprint('4:\\n')\ntemp_df1\n\ntemp_df2 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df2[i]=rf_cv[i].cv_results_['mean_test_score'][rf_cv[i].cv_results_['param_max_features']==6]\ntemp_df2['max_depth'] = rf_cv['roc_auc'].cv_results_['param_max_depth'][rf_cv['roc_auc'].cv_results_['param_max_features']==6]\ntemp_df2.set_index('max_depth', inplace=True)\nprint('6:\\n')\ntemp_df2","678aba14":"grid_values = {'C': [0.01, 0.1, 1, 10, 100],\n              'penalty': ['l1', 'l2']}\nscorers_list = ['accuracy','roc_auc','precision','recall', 'f1']\n\n\nlr_cv = model_score(data_set,\n                    LogisticRegression(solver='liblinear',random_state=42, max_iter=500,\n                                      class_weight='balanced'),\n                    grid_values,\n                    scorers_list)\n\n\ntemp_df1 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df1[i]=lr_cv[i].cv_results_['mean_test_score'][lr_cv[i].cv_results_['param_penalty']=='l1']\ntemp_df1['C'] = lr_cv['roc_auc'].cv_results_['param_C'][lr_cv['roc_auc'].cv_results_['param_penalty']=='l1']\ntemp_df1.set_index('C', inplace=True)\nprint('l1:\\n')\ntemp_df1\n\ntemp_df2 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df2[i]=lr_cv[i].cv_results_['mean_test_score'][lr_cv[i].cv_results_['param_penalty']=='l2']\ntemp_df2['C'] = lr_cv['roc_auc'].cv_results_['param_C'][lr_cv['roc_auc'].cv_results_['param_penalty']=='l2']\ntemp_df2.set_index('C', inplace=True)\nprint('l2:\\n')\ntemp_df2","8a0fb545":"grid_values = {'C': [1, 10],\n              'gamma': [0.5, 0.7, 0.9, 0.95]}\nscorers_list = ['accuracy','roc_auc','precision','recall', 'f1']\n\n\nsvc_cv = model_score(data_set,\n                    SVC(random_state=42, class_weight='balanced',kernel='rbf'),\n                    grid_values,\n                    scorers_list)\n\n\ntemp_df1 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df1[i]=svc_cv[i].cv_results_['mean_test_score'][svc_cv[i].cv_results_['param_C']==1]\ntemp_df1['gamma'] = svc_cv['roc_auc'].cv_results_['param_gamma'][svc_cv['roc_auc'].cv_results_['param_C']==1]\ntemp_df1.set_index('gamma', inplace=True)\nprint('C=1:\\n')\ntemp_df1\n\ntemp_df2 = pd.DataFrame()\nfor i in scorers_list:\n      temp_df2[i]=svc_cv[i].cv_results_['mean_test_score'][svc_cv[i].cv_results_['param_C']==10]\ntemp_df2['gamma'] = svc_cv['roc_auc'].cv_results_['param_gamma'][svc_cv['roc_auc'].cv_results_['param_C']==10]\ntemp_df2.set_index('gamma', inplace=True)\nprint('C=10:\\n')\ntemp_df2","da55d745":"def mod_eval(df,predictions, predprob, y_test, title):\n    # prints confusion matrix heatmap    \n    cm = confusion_matrix(df.Loan_Status[y_test.index], predictions)\n    sns.heatmap(cm, annot=True, fmt='.3g', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes']).set_title(title)\n    plt.xlabel('Real')\n    plt.ylabel('Predict')\n    \n    print(classification_report(df.Loan_Status[y_test.index], predictions))\n    \n    f, axes = plt.subplots(1,2,figsize= (20,6),squeeze=False)\n\n    fpr, tpr, _ = roc_curve(df.Loan_Status[y_test.index], predprob[:,1])\n    roc_auc = auc(fpr,tpr)\n    axes[0,0].plot(fpr, tpr, lw=3)\n    axes[0,0].set_title('{} ROC curve (area = {:0.2f})'.format(title, roc_auc))\n    axes[0,0].set(xlabel='False Positive Rate',ylabel='True Positive Rate')\n    axes[0,0].grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')\n\n    precision, recall, thresholds = precision_recall_curve(y_test, predprob[:,1])\n    best_index = np.argmin(np.abs(precision-recall)) # set the best index to be the minimum delta between precision and recall\n    axes[0,1].plot(precision,recall)\n    axes[0,1].set_title('{} Precision-Recall Curve'.format(title))\n    axes[0,1].set(xlabel='Precision', ylabel='Recall', xlim=(0.4,1.05))\n    axes[0,1].plot(precision[best_index],recall[best_index],'o',color='r')\n    axes[0,1].grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')","787b7bcf":"def model_training(classifier,df):\n    clf = classifier\n    t=df.drop(columns=['Loan_Status'])\n    X_train, X_test, y_train, y_tests = train_test_split(t,\n                                                         df['Loan_Status'],\n                                                         test_size=ts,\n                                                         stratify=df['Loan_Status'])\n    clf.fit(X_train, y_train)\n    return clf","354b4e8b":"#RandomForest\nmax_depth=11\nmax_features=6","cddd591e":"#LogisticRegression\nlr_C=0.1\npenalty='l1'","56b61176":"#SVC\nsvc_C=1\ngamma=0.9","d56d5c29":"#Test Size\nts = 0.333","ea304de3":"rf = model_training(RandomForestClassifier(random_state=42, \n                                           n_jobs=4, \n                                           n_estimators=50, \n                                           max_depth=max_depth,\n                                           max_features=max_features),data_set)\n\nt=data_set.drop(columns=['Loan_Status'])\nX_train, X_test, y_train, y_test = train_test_split(t,\n                                                     data_set['Loan_Status'],\n                                                     test_size=ts,\n                                                     stratify=data_set['Loan_Status'])\n\nmod_eval(data_set, rf.predict(X_test), rf.predict_proba(X_test), y_test, 'RandomForest')\nfi_df = pd.DataFrame({'fi': rf.feature_importances_},index=t.columns).sort_values(by='fi', ascending=False)\nfi_df\nplt.show()\nplt.figure(figsize=(12,5))\nplt.xticks(rotation='vertical')\nsns.barplot(x=fi_df.index, y=fi_df['fi'], palette=current_palette)\nplt.grid(b=True, which='both', axis='both', color='grey', linestyle = '--', linewidth = '0.3')\nplt.show()","f3ea970a":"lr = model_training(LogisticRegression(C=lr_C, \n                                       penalty=penalty,\n                                       solver='liblinear',\n                                       max_iter=1000),data_set)\n\nt=data_set.drop(columns=['Loan_Status'])\nX_train, X_test, y_train, y_test = train_test_split(t,\n                                                    data_set['Loan_Status'],\n                                                    test_size=ts,\n                                                    random_state = 42, stratify=data_set['Loan_Status'])\n\nt = 0.71\npredprob = lr.predict_proba(X_test)\n\npred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]\n\n#pred_y = lr.predict(X_test)\nmod_eval(data_set, pred_y, lr.predict_proba(X_test), y_test, 'LogisticRegressin') \nplt.show()","e5913bd2":"gnb = model_training(GaussianNB(),data_set)\n\nt=data_set.drop(columns=['Loan_Status'])\nX_train, X_test, y_train, y_test = train_test_split(t,\n                                                    data_set['Loan_Status'],\n                                                    test_size=ts,\n                                                    random_state = 42, stratify=data_set['Loan_Status'])\n\n\nt = 0.75\npredprob = gnb.predict_proba(X_test)\n\npred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]\n#pred_y = gnb.predict(X_test)\nmod_eval(data_set,pred_y, gnb.predict_proba(X_test), y_test, 'GaussianNB')\nplt.show()","6cbe67d1":"svc = model_training(SVC(kernel='linear',\n                         C=1, \n                         gamma='auto',\n                         class_weight='balanced',\n                         probability=True),data_set)\n\n\nt=data_set.drop(columns=['Loan_Status'])\nX_train, X_test, y_train, y_test = train_test_split(t,\n                                                    data_set['Loan_Status'],\n                                                    test_size=ts,\n                                                    random_state = 42, stratify=data_set['Loan_Status'])\n\nt=0.75\nprint('t={}'.format(t))\npredprob = svc.predict_proba(X_test)\n\npred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]\n#pred_y = svc.predict(X_test)\n\nmod_eval(data_set,pred_y, svc.predict_proba(X_test), y_test, 'SVC')\nplt.show()","8275f3a1":"dummy = model_training(DummyClassifier(strategy='stratified'),data_set)\n\nt=data_set.drop(columns=['Loan_Status'])\nX_train, X_test, y_train, y_test = train_test_split(t,\n                                                    data_set['Loan_Status'],\n                                                    test_size=ts,\n                                                    random_state = 42, stratify=data_set['Loan_Status'])\n\nt = 0.5\npredprob = dummy.predict_proba(X_test)\n\npred_y = [np.ceil(x) if x>=t else np.floor(x) for x in predprob[:,1]]\n#pred_y = dummy.predict(X_test)\n\nmod_eval(data_set, pred_y, dummy.predict_proba(X_test), y_test, 'Dummy')\nplt.show()","a72f9093":"### RandomForest","332b4269":"### LogisticRegression","b2fa422b":"> ### Adding new features:\n- naive estimation of monthly loan return (LoanAmount\/Loan_Amount_Term) normalized and with ln() let us get the distribution as closer as can be to normal distribution\n- total income (ApplicantIncome + CoaplicantIncome) normlized and with ln() let us get the distribution as closer as can be to normal distribution","b9758448":"- using grid search cross validation we scanned to find the optimal values for each model variables\n- the values chosen after several runs to get the optimum variables within the optimal range for the roc_auc as closer we can get to 1","d900d716":"- lets check the new features distributions","358a05e5":"## SVC","90bd2efd":"- we can see the data is not balanced, 69% approved for load while 31% where not\n- lets check the proportion in each feature between Y and N","afedefaa":"## RandomForest","5011222f":"- In our case: binary classification, the \"cross_val_score\" function uses StratifiedKFold cross validation to reduce the bias effect in imbalanced data.\n- it is a good approach for the cases the target column distribution is biased","711eb355":"### import data","f28be71d":"## LogisticRegression","b595fff3":"## Categorizing numerically object columns by column name\n- each column translated to binary value\n- multi variables columns slpitted with dummy columns\n- drop the duplications in the data","d1450881":"### Classifiers optimization","e266fee8":"- drop all NAs\n- previous attempts to fill the gaps showed that most frequent dummy classifier bring the best results, although increases the bias within the data","03f58f06":"### SVC","32901bd5":"## Summary\n- The RandomForest classifier brings the best results\n- Although, it might seems a bit overfitted according the CrossValidation session before the last fitting\n- Dummy classifier is essensial for sanity check along the analysis","bb1cf2f9":"### The following are the main functions to run each model evaluation in the chosen best values","85486e1a":"## Dummy","0c63b532":"### Chosen values per model","5ef50d40":"## Model selection","b25b7086":"- the skew and the kurtosis are much closer to 0, as expected from normalized normal distribution\n- now lets drop the neglectable features those are dependeds of the new features created above\n- i found in previous runs the Married and Dependents are neglectable too","851ce4cf":"- the following is a sanity check for the distribution between Y and N in the target column\n- the distribution between the values didn't change much (less than 0.5%)","3d8cc2cf":"## NaiveBayes","bff47601":"- we can see here first sign to some correlation with both the incomes features and the loan amount\n- lets work them out to be more models friendly (normaly distributed and with 0 mean)\n","e7aabe1a":"- as the graphs indicate, they are mostly distributed evenly between the features\n- we can also learn there is no ovious correlation jumping out at this stage\n- lets make the Loan_ID column to be an index for further analysis (we could also delete it)"}}