{"cell_type":{"f75e0d57":"code","ea96275d":"code","d47df926":"code","917b9edd":"code","586aa47c":"code","34d63837":"code","18f65ac7":"code","b18775c9":"code","559489b1":"code","2a763706":"code","b19ea8bd":"markdown","b1c79bc8":"markdown","336f60b9":"markdown","beed8cdd":"markdown","c88ad384":"markdown","0b0525f5":"markdown","835b2985":"markdown","886453fc":"markdown","7a3a3519":"markdown","64b38def":"markdown","54cf84b2":"markdown"},"source":{"f75e0d57":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ea96275d":"train=pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\n\nX_train = np.array(train.iloc[:,1:])\ny_train = np.array(train.iloc[:,0])\n\nX_test = np.array(test)\n\nX_train=X_train.reshape(42000,28,28,1)\n\nX_test=X_test.reshape(28000,28,28,1)\n","d47df926":"import keras\nfrom keras.layers import Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dense,Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import OneHotEncoder\nimport cv2\n\n\n\n#Normalization\n\nX_train=X_train\/255\nX_test=X_test\/255\n\n#encoding \nencoder=OneHotEncoder()\ny_train=y_train.reshape(-1,1)\ny_train=encoder.fit_transform(y_train)\n","917b9edd":"\nmodel=Sequential()\nmodel.add(Conv2D(6,5,5,input_shape=(28,28,1),activation=\"tanh\"))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Conv2D(16,5,5,activation=\"tanh\"))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\n\n\n\n\nmodel.add(Flatten())\nmodel.add(Dense(120,activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(84,activation=\"relu\"))\nmodel.add(Dense(10,activation=\"softmax\"))\n","586aa47c":"\nmodel.compile(optimizer=Adam(),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()\n","34d63837":"from keras.callbacks import ModelCheckpoint,EarlyStopping\n\ncheckpoint=ModelCheckpoint(\"mnist.h5\",monitor=\"loss\",mode=\"min\",save_best_only=True,verbose=1)\nearlystop=EarlyStopping(monitor=\"loss\",min_delta=0,patience=10,verbose=1,restore_best_weights=True)\n\ncallbacks=[earlystop,checkpoint]\n","18f65ac7":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nwith tf.device(\"\/gpu:0\"):\n    history=model.fit(X_train,y_train,epochs=200,callbacks=callbacks)\n\n#give 98.6% in epoch 10 you can try more epochs \n","b18775c9":"plt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.plot([x for x in range(0,len(history.history[\"loss\"]))],history.history[\"loss\"],color=\"blue\")\nplt.show","559489b1":"plt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\n\nplt.plot([x for x in range(0,len(history.history[\"accuracy\"]))],history.history[\"accuracy\"],color=\"green\")\nplt.show","2a763706":"X_test=X_test.reshape(28000,28,28,1)\npred=model.predict(X_test)\n\nX_test=X_test.reshape(28000,28,28)\n\nimport random\nfig = plt.figure()\n\n\n\ncounter=1\nfor i in range(0,10):\n    counter+=1\n    rand=random.randint(0,len(X_test))\n    if(i%2==0):\n        counter=1\n        fig = plt.figure()\n    fig.add_subplot(1,2,counter).set_title(np.argmax(pred[rand]))\n    plt.imshow(X_test[rand])\n","b19ea8bd":"> **Visualize training **","b1c79bc8":"**<h1>Build LeNet CNN<\/h1>**","336f60b9":"<h1> Encode Labels <\/h1>\n    (e.g 1 to 1000000000 )","beed8cdd":"<h1> Callbacks <\/h1>\n> Choose the best weights and stop","c88ad384":"<h1>Data Preprocessing <\/h1>\nreshape training And Test data to fit Conv2D shape","0b0525f5":"**<h1>Test some images <\/h1>**","835b2985":"<h1>LeNet-5 Architecture<\/h1>","886453fc":"<h1> Train our model <\/h1>\n>  history used to draw learning curve\n","7a3a3519":"![](https:\/\/engmrk.com\/wp-content\/uploads\/2018\/09\/LeNet_Original_Image.jpg)","64b38def":"![](https:\/\/engmrk.com\/wp-content\/uploads\/2018\/09\/LeNEt_Summary_Table.jpg)","54cf84b2":"> **Plot Learning curve**"}}