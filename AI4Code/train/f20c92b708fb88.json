{"cell_type":{"3beea233":"code","2d25727b":"code","746b96f8":"code","b869ba0a":"code","262ef861":"code","0fb56994":"code","78f67a6a":"code","694978ab":"code","16865f58":"code","bac94408":"code","a819f8d7":"code","e4ec965d":"code","41ea9929":"code","e12531d2":"code","07db4413":"code","e2576b10":"code","346c4600":"code","93baf259":"code","8871d2dc":"code","7108bf70":"code","c89bca33":"markdown","609e3283":"markdown","7630726e":"markdown","0bfacd4b":"markdown"},"source":{"3beea233":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d25727b":"x = np.load(\"\/kaggle\/input\/sign-language-digits-dataset\/X.npy\")\ny = np.load(\"\/kaggle\/input\/sign-language-digits-dataset\/Y.npy\")","746b96f8":"# Some samples\nimg_size = 64\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    plt.axis('off')\n    plt.imshow(x[i*100].reshape(img_size, img_size))","b869ba0a":"print(f\"Shape of x : {x.shape}\\nShape of y : {y.shape}\")","262ef861":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=11)","0fb56994":"# Reshape\nx_train = x_train.reshape(-1,64,64,1)\nx_test = x_test.reshape(-1,64,64,1)","78f67a6a":"print(f\"Shape of x_train : {x_train.shape}\\nShape of x_test : {x_test.shape}\\nShape of y_train : {y_train.shape}\\nShape of y_test : {y_test.shape}\\n\")","694978ab":"from sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Activation\nfrom keras.optimizers import Adam","16865f58":"input_shape = x_train.shape[1:]\ninput_shape","bac94408":"# CNN\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 8, kernel_size = (5,5), padding=\"Same\", activation=\"relu\", input_shape=input_shape))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3), padding=\"Same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(filters = 24, kernel_size = (3,3), padding=\"Same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation = 'relu'))\nmodel.add(Dense(10, activation = 'softmax'))","a819f8d7":"model.summary()","e4ec965d":"model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","41ea9929":"history = model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test))","e12531d2":"final_loss, final_accuracy = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Final validation loss: {0:.4f}, final validation accuracy: {1:.4f}\".format(final_loss, final_accuracy))","07db4413":"final_loss, final_accuracy = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Final train loss: {0:.4f}, final train accuracy: {1:.4f}\".format(final_loss, final_accuracy))","e2576b10":"# Accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy vs Epoch')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss vs Epoch')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","346c4600":"# Prediction from validation \nY_pred = model.predict(x_test)\nY_pred_class = np.argmax(Y_pred, axis=1) # prediction classes to one hot vectors\nY_true = np.argmax(y_test, axis=1)\n\nconfusion_matrix = confusion_matrix(Y_true, Y_pred_class)\nf, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(confusion_matrix, annot=True, linewidths=0.01, cmap=\"Greens\", linecolor=\"gray\", fmt=\".1f\", ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","93baf259":"for i in range(len(confusion_matrix)):\n    print(\"Class : \",str(i))\n    print(\"Number of Wrong Predictions : \", str(sum(confusion_matrix[i])-confusion_matrix[i][i]), \"out of \"+str(sum(confusion_matrix[i])))\n    print(\"Percentage of True Predictions : {:.2f}%\".format(confusion_matrix[i][i] \/ (sum(confusion_matrix[i])\/100) ))\n    print(\"***********************************************************\")","8871d2dc":"values = {'pred': model.predict_classes(x_test), 'true': np.argmax(y_test,axis=1)} \ndf = pd.DataFrame(data=values)\n\n# Looking at wrong predicted values\narray1 = np.array(df[(df.pred != df.true) & (df.true==1)].index)\nprint(array1)\n\n# Total mistakes\ndf2 = df[(df.pred != df.true)]\ndf2","7108bf70":"plt.figure(figsize = (12,12))\n\nfor i in range(len(df2)):\n    plt.subplot(6, 3, i+1)\n    img = x_test[df2.index[i]]\n    img = img.reshape((64,64))\n    plt.imshow(img, cmap='gray')\n    plt.title(\"True Class: \" + str(df2[\"true\"].iloc[i])+\" -  Pred Class: \" + str(df2[\"pred\"].iloc[i]))\n    plt.axis('off')\n    \nplt.show()","c89bca33":"# Read Dataset","609e3283":"## Train-Test Split","7630726e":"## CNN Modeling","0bfacd4b":"## Evaluate"}}