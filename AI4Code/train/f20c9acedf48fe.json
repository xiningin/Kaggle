{"cell_type":{"59b703da":"code","8ae41de3":"code","ceb412c2":"code","f76f8d81":"code","7df8f7cd":"code","c7910639":"code","d1f29dd9":"code","113e5395":"code","06d5e9c5":"code","f04080c9":"code","705274e0":"code","0f46af6d":"code","c408a95f":"code","2903108d":"code","60929641":"code","11df5936":"code","e8eb23d1":"code","d0388705":"code","2838527f":"code","f9bcbf2e":"code","b40daacb":"code","86434912":"code","4d21f745":"code","6ab1be3f":"code","938e99a0":"code","e76bb714":"code","af7a0a1a":"code","de377338":"code","3d9baf9e":"code","6db42ba8":"code","a3c0405f":"code","f08a26f5":"code","aeb8125c":"code","e2664bcb":"code","656aeeea":"code","5125a95f":"code","08cca115":"code","8915b7a7":"code","b48479d6":"code","8d038589":"code","8a58999a":"code","38828362":"code","ae2eb6e7":"code","e3f05e39":"code","4cc2914c":"code","cd041229":"code","67cec914":"markdown","5d20b213":"markdown","d8a092e9":"markdown","4505d378":"markdown","c8ecb4f7":"markdown","6a947212":"markdown","9b52b974":"markdown","be9ec49a":"markdown","087a189c":"markdown","859fd16b":"markdown","68b26335":"markdown","cf9b2ae7":"markdown","16338ec9":"markdown","668e4803":"markdown","2cc952fc":"markdown","eccc5087":"markdown"},"source":{"59b703da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n%matplotlib inline","8ae41de3":"# To print multiple output in a cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","ceb412c2":"train=pd.read_csv('..\/input\/hackerearth-ml-challenge-pet-adoption\/train.csv')\n\ntest=pd.read_csv('..\/input\/hackerearth-ml-challenge-pet-adoption\/test.csv')","f76f8d81":"train.head(20)\ntrain.shape\n\nprint('-----------'*5)\ntest.head()\ntest.shape","7df8f7cd":"# test[test['length(m)']==0]\n\ntrain[train['length(m)']==0]\n\n# train[train['height(cm)']==0]","c7910639":"train['breed_category'].value_counts()\n\ntrain['pet_category'].value_counts()","d1f29dd9":"class data_fabrication:\n#     def __init__(self,train_data,test_data):\n#         self.train_data=train_data\n#         self.test_data=test_data\n        \n    def merge(self,train_data,test_data):\n\n        \"\"\"\n        create a new columns to segregate the train and test data\n\n        \"\"\"\n        train_data['train_or_test']='train'\n        test_data['train_or_test']='test'\n        df=pd.concat([train_data,test_data])\n        return df\n\n        \"\"\"\n        Now let's create a method to deal with pet_id \n        By simply removing the ```alphabatics and _``` character\n        \"\"\"\n    \n    def Id(self,train_data,test_data):\n        df=self.merge(train,test)\n        df['U_pet_id']=df['pet_id'].str.replace('ANSL_','').astype('int')\n        \n        # No improvement with these variable \n        \n#         df['week_pet_id'] = df['U_pet_id']%7\n#         df['month_pet_id'] = df['U_pet_id']%30\n#         df['year_pet_id'] = df['U_pet_id']%365\n#         df['num_weeks_pet_id'] = df['U_pet_id']\/\/7\n#         df['num_year_pet_id'] = df['U_pet_id']\/\/365\n#         df['num_quarter_pet_id'] = df['U_pet_id']\/\/90\n#         df['quarter_pet_id'] = df['U_pet_id']%90  \n        \n        df.drop(['pet_id'],axis=1,inplace=True)\n        return df\n    \n    #function to impute the missing values \n    def Impute(self,train_data,test_data):\n        df=self.Id(train_data,test_data)\n        df['condition']=df['condition'].fillna(3.0).astype('int')\n        return df\n        ","113e5395":"# class for time series feature creation.\n\nclass Time_Series_Transformation():    \n    \n    def __init__(self,train,test):\n        self.object1=data_fabrication()\n        \n    \n    def create_week_date_featues_arrival(self): \n        \n        df=self.object1.Impute(train,test)\n        \n        df['Year_arrival'] = pd.to_datetime(df['listing_date']).dt.year\n        df['Month_arrival'] = pd.to_datetime(df['listing_date']).dt.month\n        df['Day_arrival'] = pd.to_datetime(df['listing_date']).dt.day\n        df['Dayofweek_arrival'] = pd.to_datetime(df['listing_date']).dt.dayofweek\n        df['DayOfyear_arrival'] = pd.to_datetime(df['listing_date']).dt.dayofyear\n        df['Week_arrival'] = pd.to_datetime(df['listing_date']).dt.week\n        df['Quarter_arrival'] = pd.to_datetime(df['listing_date']).dt.quarter \n        df['Is_month_start_arrival'] = pd.to_datetime(df['listing_date']).dt.is_month_start\n        df['Is_month_end_arrival'] = pd.to_datetime(df['listing_date']).dt.is_month_end\n        df['Is_quarter_start_arrival'] = pd.to_datetime(df['listing_date']).dt.is_quarter_start\n        df['Is_quarter_end_arrival'] = pd.to_datetime(df['listing_date']).dt.is_quarter_end\n        df['Is_year_start_arrival'] = pd.to_datetime(df['listing_date']).dt.is_year_start\n        df['Is_year_end_arrival'] = pd.to_datetime(df['listing_date']).dt.is_year_end\n        df['Semester_arrival'] = np.where(df['listing_date'].isin([1,2]),1,2)\n        df['Is_weekend_arrival'] = np.where(df['listing_date'].isin([5,6]),1,0)\n        df['Is_weekday_arrival'] = np.where(df['listing_date'].isin([0,1,2,3,4]),1,0)\n        df['Days_in_month_arrival'] = pd.to_datetime(df['listing_date']).dt.days_in_month\n        df['Hour_arrival'] = pd.to_datetime(df['listing_date']).dt.hour\n        df.drop(['listing_date'],axis=1,inplace=True)\n        return df\n    \n    def create_week_date_featues_issue(self):\n        \n        df=self.create_week_date_featues_arrival()\n        \n        df['Year_issue'] = pd.to_datetime(df['issue_date']).dt.year\n        df['Month_issue'] = pd.to_datetime(df['issue_date']).dt.month\n        df['Day_issue'] = pd.to_datetime(df['issue_date']).dt.day\n        df['Dayofweek_issue'] = pd.to_datetime(df['issue_date']).dt.dayofweek\n        df['DayOfyear_issue'] = pd.to_datetime(df['issue_date']).dt.dayofyear\n        df['Week_issue'] = pd.to_datetime(df['issue_date']).dt.week\n        df['Quarter_issue'] = pd.to_datetime(df['issue_date']).dt.quarter \n        df['Is_month_start_issue'] = pd.to_datetime(df['issue_date']).dt.is_month_start\n        df['Is_month_end_issue'] = pd.to_datetime(df['issue_date']).dt.is_month_end\n        df['Is_quarter_start_issue'] = pd.to_datetime(df['issue_date']).dt.is_quarter_start\n        df['Is_quarter_end_issue'] = pd.to_datetime(df['issue_date']).dt.is_quarter_end\n        df['Is_year_start_issue'] = pd.to_datetime(df['issue_date']).dt.is_year_start\n        df['Is_year_end_issue'] = pd.to_datetime(df['issue_date']).dt.is_year_end\n        df['Semester_issue'] = np.where(df['issue_date'].isin([1,2]),1,2)\n        df['Is_weekend_issue'] = np.where(df['issue_date'].isin([5,6]),1,0)\n        df['Is_weekday_issue'] = np.where(df['issue_date'].isin([0,1,2,3,4]),1,0)\n        df['Days_in_month_issue'] = pd.to_datetime(df['issue_date']).dt.days_in_month\n        df.drop(['issue_date'],axis=1,inplace=True)\n      \n        return df\n    \n    \n\n    # With below function we gonna calculate\n    # The different bettween arriaval date and issued date\n\n    def time_based_feature(self):\n        df=self.create_week_date_featues_issue()\n        df['year_took']=df['Year_arrival']-df['Year_issue']\n        df['months_took']=df['Month_arrival']-df['Month_issue']\n#         df['months_took']=df['year_took']*12\n        df['days_took']=df['Day_arrival']-df['Day_issue']\n    \n#         df['days_took']=df['year_took']*365\n\n#         df['quarter_took']=df['Quarter_arrival']-df['Quarter_issue']\n    \n#         df['semester_took']=df['Semester_arrival']-df['Semester_issue']\n        return df\n    \n    def dummies_creation(self,columns):\n        df=self.time_based_feature()\n        for i in columns:\n            df = pd.get_dummies(df, columns=[i])            \n        return df\n                ","06d5e9c5":"# start with the first class\nnew=Time_Series_Transformation(train,test)\n\ncol=['Is_month_start_issue','Is_month_end_issue','Is_quarter_start_issue','Is_quarter_end_issue','Is_year_start_issue','Is_year_end_issue',\n    'Is_month_start_arrival','Is_month_end_arrival','Is_quarter_start_arrival','Is_quarter_end_arrival','Is_year_start_arrival','Is_year_end_arrival',\n    'condition','color_type','Year_arrival']\n\ndf=new.dummies_creation(col)","f04080c9":"# df.head()\n# df.shape","705274e0":"# let;s try without dropping the U_pet_id(outputs a drop in the score)\n\n# df.drop(['U_pet_id'],axis=1,inplace=True)","0f46af6d":"# seperate the train and test data\n\n\ntrain_d=df.loc[df.train_or_test.isin(['train'])]\ntest_d=df.loc[df.train_or_test.isin(['test'])]\n\ntrain_d.drop(columns={'train_or_test'},axis=1,inplace=True)\ntest_d.drop(columns={'train_or_test'},axis=1,inplace=True)\n","c408a95f":"test_d.drop(columns={'breed_category','pet_category'},axis=1,inplace=True)","2903108d":"import lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import classification_report\n\n\n\nclass model_building:\n        \n#     def __init__(self,parameters,label_col1,label_col2):\n#         self.parameters=parameters\n#         self.label_col1='pet_category'\n#         self.label_col2='breed_category'\n                \n    def create_model(self,df,column,drop_column):\n        print('This time we are training model for {}'.format(column))\n        df=df.drop([drop_column],axis=1)\n        \n        label_col=column\n        df_train, df_eval = train_test_split(df, test_size=0.30,random_state=42, \n                                             shuffle=True,stratify=df[label_col])\n        \n        feature_cols = df.columns.tolist()\n\n        feature_cols.remove(column)\n        \n        params={'learning_rate': 0.03,'max_depth': 9,'n_estimators': 5000,\n                'objective': 'multiclass','boosting_type': 'gbdt','subsample': 0.7,\n                'random_state': 42,'colsample_bytree': 0.7,'min_data_in_leaf': 31,\n                'reg_alpha': 1.7,'reg_lambda': 1.11}        \n        \n        clf = lgb.LGBMClassifier(**params)\n        \n        clf.fit(df_train[feature_cols], df_train[label_col], early_stopping_rounds=100, \n                eval_set=[(df_train[feature_cols], df_train[label_col]), (df_eval[feature_cols], df_eval[label_col])], \n                eval_metric='multi_logloss', verbose=True)\n        \n        \n        \n        eval_score = accuracy_score(df_eval[label_col], clf.predict(df_eval[feature_cols]))\n        \n        print('\\n')\n        print('------------'*5)\n        print('Eval ACC: {}'.format(eval_score))\n        \n        df_train = pd.concat((df_train, df_eval))\n        \n        clf = lgb.LGBMClassifier(**params)\n\n        clf.fit(df_train[feature_cols], df_train[label_col], eval_metric='multi_error', verbose=False)\n\n        # eval_score_auc = roc_auc_score(df_train[label_col], clf.predict(df_train[feature_cols]))\n        eval_score_acc = accuracy_score(df_train[label_col], clf.predict(df_train[feature_cols]))\n\n        print('ACC: {}'.format(eval_score_acc))   \n        \n        return clf,feature_cols\n\n               \n    def create_prediction(self,df,column,drop_column):\n        \n        clf,feature_col=self.create_model(df,column,drop_column)\n        self.test=test_d\n        prediction = clf.predict(self.test[feature_col])\n        return prediction\n","60929641":"# 'breed_category'\n# 'pet_category'\n\nmodel=model_building()\n# model=model.create_model(train_d,'pet_category','breed_category')\n\nprediction=model.create_prediction(train_d,'pet_category','breed_category')\n\n","11df5936":"model2=model_building()\nprediction2=model2.create_prediction(train_d,'breed_category','pet_category')","e8eb23d1":"# prediction2\n# train_d.shape","d0388705":"# y1=train_d['pet_category']\n# y2=train_d['breed_category']\n\n# # x=train_d.drop(['breed_category','pet_category'],axis=1)\n\n# train_d1=train_d.drop(['breed_category'],axis=1)\n\n# train_d2=train_d.drop(['pet_category'],axis=1)","2838527f":"# # Model training importations\n\n# import lightgbm as lgb\n\n# from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n\n# from sklearn.metrics import mean_squared_log_error, mean_squared_error\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import roc_auc_score\n# from sklearn.metrics import accuracy_score\n# from sklearn.metrics import log_loss\n# from sklearn.metrics import classification_report\n# import lightgbm as lgb\n","f9bcbf2e":"# label_col1='pet_category'\n# label_col2='breed_category'","b40daacb":"# df_train1, df_eval1 = train_test_split(train_d1, test_size=0.30, random_state=42, shuffle=True, stratify=train_d1[label_col1])","86434912":"# # for firt model\n# feature_cols1 = train_d1.columns.tolist()\n\n# feature_cols1.remove('pet_category')\n\n# #for second model\n\n# feature_cols2 = train_d2.columns.tolist()\n\n# feature_cols2.remove('breed_category')\n","4d21f745":"# params = {}\n# params['learning_rate'] = 0.25\n# params['max_depth'] = 10\n# params['n_estimators'] = 10000\n# params['objective'] = 'multiclass'\n# params['boosting_type'] = 'gbdt'\n# params['subsample'] = 0.7\n# params['random_state'] = 42\n# params['colsample_bytree']=0.7\n# params['min_data_in_leaf'] = 25\n# params['reg_alpha'] = 1.7\n# params['reg_lambda'] = 1.11","6ab1be3f":"# params","938e99a0":"# clf = lgb.LGBMClassifier(**params)","e76bb714":"# clf.fit(df_train1[feature_cols1], df_train1[label_col1], early_stopping_rounds=1000, eval_set=[(df_train1[feature_cols1], df_train1[label_col1]), (df_eval1[feature_cols1], df_eval1[label_col1])], eval_metric='multi_error', verbose=True)\n","af7a0a1a":"# eval_score = accuracy_score(df_eval1[label_col1], clf.predict(df_eval1[feature_cols1]))\n\n# print('Eval ACC: {}'.format(eval_score))","de377338":"# df_train1 = pd.concat((df_train1, df_eval1))","3d9baf9e":"# clf = lgb.LGBMClassifier(**params)\n\n# clf.fit(df_train1[feature_cols1], df_train1[label_col1], eval_metric='multi_error', verbose=False)\n\n# # eval_score_auc = roc_auc_score(df_train[label_col], clf.predict(df_train[feature_cols]))\n# eval_score_acc = accuracy_score(df_train1[label_col1], clf.predict(df_train1[feature_cols1]))\n\n# print('ACC: {}'.format(eval_score_acc))","6db42ba8":"# y_pred1 = clf.predict(test_d[feature_cols1])","a3c0405f":"# train_d2=train_d.drop(['pet_category'],axis=1)\n\n# let's add the predicted feature and the observe our final result\n\n# train_d2['pet_category']=train['pet_category']","f08a26f5":"# label_col2='breed_category'","aeb8125c":"# df_train2, df_eval2 = train_test_split(train_d2, test_size=0.30, random_state=42, shuffle=True, stratify=train_d2[label_col2])","e2664bcb":"# feature_cols2 = train_d2.columns.tolist()\n\n# feature_cols2.remove('breed_category')","656aeeea":"# clf2 = lgb.LGBMClassifier(**params)","5125a95f":"# clf2.fit(df_train2[feature_cols2], df_train2[label_col2], early_stopping_rounds=1000, eval_set=[(df_train2[feature_cols2], df_train2[label_col2]), (df_eval2[feature_cols2], df_eval2[label_col2])], eval_metric='multi_error', verbose=True)\n","08cca115":"# eval_score2 = accuracy_score(df_eval2[label_col2], clf2.predict(df_eval2[feature_cols2]))\n\n# print('Eval ACC: {}'.format(eval_score2))","8915b7a7":"# df_train2 = pd.concat((df_train2, df_eval2))","b48479d6":"# clf2 = lgb.LGBMClassifier(**params)\n\n# clf2.fit(df_train2[feature_cols2], df_train2[label_col2], eval_metric='multi_error', verbose=False)\n\n# # eval_score_auc = roc_auc_score(df_train[label_col], clf.predict(df_train[feature_cols]))\n# eval_score_acc = accuracy_score(df_train2[label_col2], clf2.predict(df_train2[feature_cols2]))\n\n# print('ACC: {}'.format(eval_score_acc))","8d038589":"# add first predicted values inthe test data \n# test_d['pet_category']=y_pred1","8a58999a":"# y_pred2 = clf2.predict(test_d[feature_cols2])","38828362":"# train.head()","ae2eb6e7":"# submission=pd.DataFrame({'pet_id':test['pet_id'],'breed_category':y_pred2,'pet_category':y_pred1})\n\nsubmission=pd.DataFrame({'pet_id':test['pet_id'],'breed_category':prediction2,'pet_category':prediction})","e3f05e39":"submission.head()","4cc2914c":"# submission.to_csv('submission28.csv',index=False)","cd041229":"submission.shape","67cec914":"* ## Define a class for Initial fabrication of data","5d20b213":"> A leading pet adoption agency is planning to create a virtual tour experience for their customers showcasing all animals that are available in their shelter. To enable this tour experience, you are required to build a Machine Learning model that determine the type of breed of the animal based on its physical attributes and other factors.","d8a092e9":"# Data","4505d378":"- If you learn something through this kernel then hit upvote and if want to discuss something then comment.","c8ecb4f7":"## Class for predictor and evaluator..","6a947212":"<img src='https:\/\/media-fastly.hackerearth.com\/media\/hackathon\/hackerearth-machine-learning-challenge-pet-adoption\/images\/b96edbc6d2-PetAdoption_FBImage.jpg' height=450 width=650\/>","9b52b974":"### Training a model","be9ec49a":"### Training model for 1st prediction (i.e pet_category)","087a189c":"# <center> <u>Adopt a Buddy<\/u><\/center>","859fd16b":"# Problem Statement","68b26335":"### Prediction for second feature","cf9b2ae7":"## <center>Column Description<\/center>\n\n| <font color='blue'><h2>Column Name<\/h2><\/font>                       | <font color='blue'><h2>Description<\/h2><\/font>                                                                                        |\n| ------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------:|\n| Pet Id                       | Pet UniqueID                                                                                                   | \n| issue_Date                          | \tDate on which the pet was issued to the shelter                                                                  | \n| lasting_date                            | Date when pet arrived at shelter                                                                                    | \n| condition                              | Condition of the pet                                                                                 | \n| color_type                               | Color of the pet                                                                                   |\n| lenght(m)                               | length of the pet(in meter)                                                                                   |\n|height(cm)                             |  Height of the pet (in centimete)                   |\n| X1,X2                             | Anonymous Columns                                          |\n| <h3>breed_category<\/h3>                             | <u><h3>Breed category of the pet(target variable)<\/h3><\/u>\n                                         |\n|<h3>pet_category<\/h3>                              |   <u><h3>Category of the pet(target variable)<\/h3><\/u>        |\n","16338ec9":"### Training model for second prediction i.e y2)","668e4803":"## Find useful --Do Upvote--\n\nThe aim of this kernel is to provide a solution for data science projects by using an Object Oriented Programming(OOP) approach. Object oriented programming is largely based on personal experience and is open to development. For this reason, code design can be improved according to the comments to be made for the kernel.\n\n- Comments and criticisms will provide a better code design.","2cc952fc":"### Prediction for first feature","eccc5087":"## Let's Begin\n\nHi, My name is ```vinay vikram```, \n\nSince after this COVID-19 pendemic i started participating in HackerEarth's Hackathon and trust me i am loving them. \n\nFrom last few hackathon i got in to this idea of making my notebook public since from beginning without runing the competitive sprit.\n\n        - Here i do share my approach theoritically without opensourcing my code.\n        - So let's come together and work together with mutual knowledge sharing.\n        - If you have better ideas share yours and if you have any suggestion regarding my work share that too.\n        \n \n## Kindly do Upvote if you learn something "}}