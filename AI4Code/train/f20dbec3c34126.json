{"cell_type":{"9b3841d0":"code","16c4d80f":"code","483cd7fb":"code","5cf88797":"code","cd991115":"code","2d07a95e":"code","0be6fbdf":"code","0c0e5286":"code","f8afbefb":"code","5903b9a9":"code","ea35c76a":"code","2fcc3455":"code","549024b6":"code","b49f755d":"code","36d042f8":"code","6540cdfe":"markdown","892a7d79":"markdown"},"source":{"9b3841d0":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport matplotlib.cm as cm\nimport fnmatch\nimport os\nimport numpy as np\nimport librosa\nimport matplotlib.pyplot as plt\nimport librosa.display\nfrom sklearn.manifold import TSNE\nimport json\n# Importing library \nimport csv \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob","16c4d80f":"cd ..\/input\/toronto-emotional-speech-set-tess\/'TESS Toronto emotional speech set data'\/","483cd7fb":"files = []\nfor root, dirnames, filenames in os.walk('.'):\n    for filename in fnmatch.filter(filenames, '*.wav'):\n        files.append(os.path.join(root, filename))\n\nprint(\"found %d .wav files\"%(len(files)))","5cf88797":"def get_features(y, sr):\n    y = y[0:sr]  # analyze just first second\n    S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n    log_S = librosa.amplitude_to_db(S, ref=np.max)\n    mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)\n    delta_mfcc = librosa.feature.delta(mfcc, mode='nearest')\n    delta2_mfcc = librosa.feature.delta(mfcc, order=2, mode='nearest')\n    feature_vector = np.concatenate((np.mean(mfcc,1), np.mean(delta_mfcc,1), np.mean(delta2_mfcc,1)))\n    feature_vector = (feature_vector-np.mean(feature_vector)) \/ np.std(feature_vector)\n    return feature_vector","cd991115":"feature_vectors = []\nsound_paths = []\nfor i,f in enumerate(files):\n    if i % 100 == 0:\n        print(\"get %d of %d = %s\"%(i+1, len(files), f))\n    y, sr = librosa.load(f)\n    feat = get_features(y, sr)\n    feature_vectors.append(feat)\n    sound_paths.append(f)\n        \nprint(\"calculated %d feature vectors\"%len(feature_vectors))","2d07a95e":"feature_vectors[1].shape","0be6fbdf":"model = TSNE(n_components=2, learning_rate=150, perplexity=30, verbose=2, angle=0.1).fit_transform(feature_vectors)","0c0e5286":"symbol=[]\nsymbol=[1]*1400\nx=[2]*1400\nsymbol.extend(x)","f8afbefb":"file=[1,2,3,4,5,6,7]\ncolor=[]\nfor i in file:\n    x=[i]*200\n    color.extend(x)\ncolor.extend(color)","5903b9a9":"len(color)","ea35c76a":"x_axis=model[:,0]\ny_axis=model[:,1]\nimport plotly.express as px\nfig = px.scatter(x=x_axis, y=y_axis,color=color,symbol=symbol,opacity=0.7)\nfig.show()","2fcc3455":"x_axis[0].shape","549024b6":"model = TSNE(n_components=3, learning_rate=150, perplexity=30, verbose=2, angle=0.1).fit_transform(feature_vectors)","b49f755d":"files","36d042f8":"x_axis=model[:,0]\ny_axis=model[:,1]\nz_axis=model[:,2]\nimport plotly.express as px\nfig = px.scatter_3d(x=x_axis, y=y_axis, z=z_axis,color=color,symbol=symbol,opacity=0.7)\n\n#,color=emotion,symbol=age\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()\n# plt.figure(figsize = (10,10))\n# plt.scatter(x_axis, y_axis)\n# plt.show()","6540cdfe":"**# t-SNE on Audio Datasets","892a7d79":"T-distributed Stochastic Neighbor Embedding (t-SNE) is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton.[1] It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability. Now We will create a t-SNE plot of a group of audio clips."}}