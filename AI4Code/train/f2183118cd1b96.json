{"cell_type":{"49f14d9d":"code","8f96d639":"code","e4acfe87":"code","a6a2101e":"code","fbdb8955":"code","a9498658":"code","a1271391":"code","dcf278a5":"code","89365dd3":"code","350f2178":"code","75684c6d":"code","334b2cf3":"code","82fd394c":"code","5852daa5":"code","0e0aa17b":"code","49e79024":"code","f4910574":"code","453f2751":"code","c1a0161e":"code","bb3e3697":"code","607e342b":"code","04e26b70":"code","c0566a6f":"code","97a69d1b":"code","3e842b0d":"code","04af422b":"code","c0e212c1":"code","12597cfe":"code","9e9b5f53":"code","871d754d":"code","7c0d7c51":"markdown","003117e9":"markdown","bbce3f24":"markdown"},"source":{"49f14d9d":"import pandas as pd \nimport numpy as np","8f96d639":"import os\nprint(os.listdir(\"..\/input\"))","e4acfe87":"path=\"..\/input\/\"\nos.chdir(path)\ndata = pd.read_csv(\"Personal Loan Data.csv\", low_memory=False)","a6a2101e":"data.head()","fbdb8955":"def drop_columns(df,columns):\n    df1 = df.drop(columns, axis=1)\n    return df1\n\n    \n    \ndef encode_text_dummy(df, name):\n    dummies = pd.get_dummies(df[name])\n    for x in dummies.columns:\n        dummy_name = f\"{name}-{x}\"\n        df[dummy_name] = dummies[x]\n    df.drop(name, axis=1, inplace=True)\n    \ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)\n        \n                 \n\ndef check_uniques(df,col):\n    print(df.dtypes)\n    print('Dim Of Data')\n    print(df.shape)\n    col=np.array(col)\n    for y in col:\n        col1 = pd.DataFrame(df[y])\n        col1 = col1.drop_duplicates()\n        print(y)\n        print(np.array(col1[y]))\n        print(\"_______________\")\n        print(col1.shape)\n        print(\"============================================================\")\n        \ndef na_value(df,value,col):\n    df[col]=df[col].fillna(value) \n    \ndef na_value_zero(df):\n    df=df.fillna(0)\n    \ndef get_success_failure(df,col,outcome):\n    print('Dim Of Data')\n    print(df.shape)\n    col1 = pd.DataFrame(df[col])\n    col1 = col1.drop_duplicates()\n    for y in col1[col]:\n        df2 = df.loc[df[col] == y]\n        df3 = df2.loc[df2[outcome] == 1]\n        df4 = df2.loc[df2[outcome] == 0]\n        r2,c2 = df2.shape\n        r3,c3 = df3.shape\n        r1,c1 = df4.shape\n        print(col+'      '+str(y)+'      No Of data points='+str(r2)+'   no_of_success='+str(r3)+'  no_of_failure='+str(r1))\n        \ndef success_failure(df,outcome):\n    print('Dim Of Data')\n    r,c= df.shape\n    df3 = df.loc[df[outcome] == 1]\n    df4 = df.loc[df[outcome] == 0]\n    r1,c1 = df3.shape\n    r2,c2 = df4.shape\n    print('No Of success='+str(r1)+'No Of failure='+str(r2))\n\ndef to_float(df,col):\n    for y in col:\n        df[y] = df[y].astype('float')\n        \ndef to_category(df,col):\n    for y in col:\n        df[y] = df[y].astype('category')\n ","a9498658":"col_float = ['Age', 'Experience', 'Income', 'Family', 'CCAvg',\n       'Mortgage']\ncol_cat = ['Education', 'Personal Loan', 'Securities Account', 'CD Account',\n       'Online', 'CreditCard']\nto_float(data,col_float)\n\nto_category(data,col_cat)","a1271391":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, auc, roc_curve, classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import train_test_split\n#import parfit.parfit as pf\n\nimport math\nfrom IPython.display import display\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\n\nimport seaborn as sns\n\nfrom scikitplot.metrics import plot_roc_curve\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.utils import shuffle\nimport sklearn\nprint (sklearn.__version__)","dcf278a5":"def test_train(data, t, target):  \n   # Shuffle the dataset to mix up the rows\n    data = shuffle(data, random_state=10)\n    train_data, test_data = train_test_split(data, test_size=t, random_state=10)\n     \n     # Convert into Arrays and Encode\n    train_x = train_data.drop(target, axis=1)\n    train_y = train_data[target]\n    test_x = test_data.drop(target, axis=1)\n    test_y = test_data[target]\n    print(\"Train_x Shape :: \", train_x.shape)\n    print(\"Train_y Shape :: \", train_y.shape)\n    print(\"Test_x Shape :: \", test_x.shape)\n    print(\"Test_y Shape :: \", test_y.shape)\n    return train_x, train_y, test_x, test_y, train_data, test_data\n\ndef random_forest_classifier(features, target):\n    \"\"\"\n    To train the random forest classifier with features and target data\n    :param features:\n    :param target:\n    :return: trained random forest classifier\n    \"\"\"\n    clf = RandomForestClassifier()\n    clf.fit(features, target)\n    return clf\n\ndef print_score(m, roc_auc=False, clf_report=False, cnf_mat=False):\n    print(\"Training score: {}\".format(m.score(train_x, train_y)))\n    print(\"Test score: {}\".format(m.score(test_x, test_y)))\n    \n    pred = m.predict(test_x)\n    \n    if hasattr(m, 'oob_score_'):\n        print(\"OOB score: {}\".format(m.oob_score_))\n\n    if clf_report:\n        print(classification_report(test_y, pred))\n\n    if roc_auc:\n        fpr, tpr, thresholds = roc_curve(test_y, m.predict_proba(test_x)[:, 1])\n        print(\"ROC AUC score: {}\".format(auc(fpr, tpr)))\n        plot_roc_curve(test_y, m.predict_proba(test_x))\n        plt.show()\n        \n    if cnf_mat:\n        cm = pd.DataFrame(confusion_matrix(test_y, pred))\n        sns.heatmap(cm, annot=True)\n        plt.show()\n        print(pd.crosstab(test_y, pred,\n                          rownames=['True'], colnames=['Predicted'], margins=True))\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)\n\ndef plot_fi(fi): \n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","89365dd3":"train_x, train_y, test_x, test_y, train_data, test_data = test_train(data, .2, 'Personal Loan')","350f2178":"# Create random forest classifier instance\ntrained_model = random_forest_classifier(train_x, train_y)\nprint(\"Trained model :: \", trained_model)\npredictions = trained_model.predict(test_x)\n \nfor i in range(0,10):\n    print(\"Actual outcome :: {} and Predicted outcome :: {}\".format(list(test_y)[i], predictions[i]))","75684c6d":"print(\"Train Accuracy :: \", accuracy_score(train_y, trained_model.predict(train_x)))\nprint(\"Test Accuracy  :: \", accuracy_score(test_y, predictions))\nprint(\" Confusion matrix \", confusion_matrix(test_y, predictions))","334b2cf3":"m=trained_model\nprint_score(m,roc_auc=True)","82fd394c":"import seaborn as sns\n\n# Predictions\npred = m.predict(test_x)\n# Confidence of predictions\npred_proba = m.predict_proba(test_x)\n\n# Confusion matrix\ncm = pd.DataFrame(confusion_matrix(test_y, predictions))\n\nsns.heatmap(cm, annot=True)\nplt.show()\n\nprint(pd.crosstab(test_y, pred, rownames=['True'], colnames=['Predicted'], margins=True))","5852daa5":"m = RandomForestClassifier(n_jobs=-1, class_weight='balanced', random_state=1)\n%time m.fit(train_x, train_y);","0e0aa17b":"# Accuracy scores on training and validation data\nprint_score(m)\n\nfi = rf_feat_importance(m, train_x)\nprint(fi)\nplot_fi(fi);","49e79024":"from sklearn import ensemble\n","f4910574":"def g_b_classifier(features, target, params):\n    \"\"\"\n    To train the gb classifier with features and target data\n    :param features:\n    :param target:\n    :return: trained gb classifier\n    \"\"\"\n    clf = ensemble.GradientBoostingClassifier(**params)\n    clf.fit(features, target)\n    return clf","453f2751":"#define parameters\nparams = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,\n          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\n\n# Create random forest classifier instance\ntrained_model = g_b_classifier(train_x, train_y, params)\nprint(\"Trained model :: \", trained_model)\npredictions = trained_model.predict(test_x)\n \nfor i in range(0, 5):\n    print(\"Actual outcome :: {} and Predicted outcome :: {}\".format(list(test_y)[i], predictions[i]))","c1a0161e":"print(\"Train Accuracy :: \", accuracy_score(train_y, trained_model.predict(train_x)))\nprint(\"Test Accuracy  :: \", accuracy_score(test_y, predictions))\nprint(\" Confusion matrix \", confusion_matrix(test_y, predictions))","bb3e3697":"def gb_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)\n\nfrom sklearn.metrics import accuracy_score, auc, roc_curve, classification_report, confusion_matrix, roc_auc_score\nfrom scikitplot.metrics import plot_roc_curve\nm=trained_model\nprint_score(m,roc_auc=True)","607e342b":"import seaborn as sns\n\n# Predictions\npred = m.predict(test_x)\n# Confidence of predictions\npred_proba = m.predict_proba(test_x)\n\n# Confusion matrix\ncm = pd.DataFrame(confusion_matrix(test_y, predictions))\n\nsns.heatmap(cm, annot=True)\nplt.show()\n\nprint(pd.crosstab(test_y, pred, rownames=['True'], colnames=['Predicted'], margins=True))","04e26b70":"%time m.fit(train_x, train_y);\n# Accuracy scores on training and validation data\nprint_score(m)\nfi = rf_feat_importance(m, train_x)\nprint(fi)\nplot_fi(fi);","c0566a6f":"# Import Packages\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelEncoder","97a69d1b":"# Define the Encoder Function\ndef one_hot_encode(labels):\n    n_labels = len(labels)\n    n_unique_labels = len(np.unique(labels))\n    one_hot_encode = np.zeros((n_labels, n_unique_labels))\n    one_hot_encode[np.arange(n_labels), labels] = 1\n    return(one_hot_encode)\n\n\n# Define the Model\ndef multilayer_perceptron(x, weights, biases):\n    \n    # Hidden Layer with RELU Activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n    layer_1 = tf.nn.sigmoid(layer_1)\n    \n    # Hidden Layer with Sigmoid Activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n    layer_2 = tf.nn.sigmoid(layer_2)\n    \n    # Hidden Layer with Sigmoid Activation\n    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n    layer_3 = tf.nn.sigmoid(layer_3)\n    \n    # Hidden Layer with Sigmoid Activation\n    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n    layer_4 = tf.nn.relu(layer_4)\n    \n    # Output Layer with Linear Activation\n    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n    return(out_layer)","3e842b0d":"# Convert into Arrays and Encode\ntrain_x = train_x.values\n\ntest_x = test_x.values\n\n\n# Encode the Dependent Variable\ntrain_y = one_hot_encode(train_y)\ntest_y = one_hot_encode(test_y)\n\n# Inspect the Shape of the Training and Testing\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)","04af422b":"# Define the important parameters and variables to work with the tensors\nlearning_rate = 0.1\ntraining_epochs = 1000\ncost_history = np.empty(shape=[1], dtype=float)\nn_dim = train_x.shape[1]\nprint(\"n_dim\", n_dim)\nn_class = 2\n#|model_path = outpath + '\/ann_model'\n\n# Define the number of hidden layers and number of neurons for each layer\nn_hidden_1 = 60\nn_hidden_2 = 60\nn_hidden_3 = 60\nn_hidden_4 = 60\n\ngraph = tf.get_default_graph()\n\nx = tf.placeholder(tf.float32, [None, n_dim])\nW = tf.Variable(tf.zeros([n_dim, n_class]))\nb = tf.Variable(tf.zeros([n_class]))\ny_ = tf.placeholder(tf.float32, [None, n_class])\n\n# Define the Weights and the Biased for Each Layer\nweights = {\n    'h1': tf.Variable(tf.truncated_normal([n_dim, n_hidden_1])),\n    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),\n    'h4': tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4])),\n    'out': tf.Variable(tf.truncated_normal([n_hidden_4, n_class])),\n}\n\nbiases = {\n    'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),\n    'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),\n    'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),\n    'b4': tf.Variable(tf.truncated_normal([n_hidden_4])),\n    'out': tf.Variable(tf.truncated_normal([n_class])),\n}","c0e212c1":"# Initialize All the Variables\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver(save_relative_paths=True)\n\n# Call the Model\ny = multilayer_perceptron(x, weights, biases)\n\n# Define the cost function and optimizer\ncost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\ntraining_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n\n# Open a Session\nsess = tf.Session()\nsess.run(init)\n\n# Compute the Cost and the Accuracy for each Epoch\nmse_history = []\naccuracy_history = []\n\nfor epoch in range(training_epochs):\n    sess.run(training_step, feed_dict={x: train_x, y_: train_y})\n    cost = sess.run(cost_function, feed_dict={x: train_x, y_: train_y})\n    cost_history = np.append(cost_history, cost)\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    \n    pred_y = sess.run(y, feed_dict={x: test_x})\n    mse = tf.reduce_mean(tf.square(pred_y - test_y))\n    mse_ = sess.run(mse)\n    mse_history.append(mse_)\n    accuracy = (sess.run(accuracy, feed_dict={x: train_x, y_: train_y}))\n    accuracy_history.append(accuracy)\n    \n    print('Epoch:', epoch, '- Cost:', cost, '- MSE:', mse_, '- Train Accuracy:', accuracy)","12597cfe":"# Plot MSE and Accuracy Graph\nplt.plot(mse_history, 'r')\nplt.title('MSE History over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('MSE')\nplt.show()\n\nplt.plot(cost_history)\nplt.title('Cost History over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Cost')\nplt.show()\n\nplt.plot(accuracy_history)\nplt.title('Accuracy History over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.show()\n\n# Print the Final Accuracy\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\"Test Accuracy:\", (sess.run(accuracy, feed_dict={x:test_x, y_: test_y})))\n\n# Print the Final Mean SQuare Error\npred_y = sess.run(y, feed_dict={x: test_x})\nmse = tf.reduce_mean(tf.square(pred_y - test_y))\nprint(\"MSE: %.4f\" % sess.run(mse))","9e9b5f53":"#train_x = biased_modeling_data.drop('IsCancelled', axis=1).values\n#train_y = biased_modeling_data['IsCancelled']\ndef read_dataset(df):\n    X=df.drop('Personal Loan', axis=1).values\n    y1=df['Personal Loan']\n    #encode the dependent variables\n    encoder = LabelEncoder()\n    encoder.fit(y1)\n    y = encoder.transform(y1)\n    Y = one_hot_encode(y)\n    print (X.shape)\n    return(X,Y,y1)\n\n#read the dataset\nX,Y,y1 = read_dataset(test_data)","871d754d":"#initialize all the variables\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\nsess = tf.Session()\nsess.run(init)\n#saver.restore(sess)\nprediction = tf.argmax(y,1)\ncorrect_prediction=tf.equal(prediction,tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\nactual = []\npredicted = []\n   \nfor i in range(1,1000):\n    prediction_run=sess.run(prediction,feed_dict={x:X[i].reshape(1,11)})\n    print(\"Original Class:\",y1[i],\"Pedicted_class:\",prediction_run[0])\n    actual.append(y1[i])\n    predicted.append(prediction_run[0])\n    \n# Confusion matrix\ncm = pd.DataFrame(confusion_matrix(actual, predicted))\n\nsns.heatmap(cm, annot=True)\nplt.show()\n\n#print(pd.crosstab(actual, predicted, rownames=['True'], colnames=['Predicted'], margins=True))","7c0d7c51":"# Tensor Flow -NN\n","003117e9":"# Random forest classifier\n","bbce3f24":"# Gradient boosting classifier\n"}}