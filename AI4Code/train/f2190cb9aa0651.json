{"cell_type":{"2ef3141d":"code","d579d54a":"code","cff42207":"code","e9c2d35b":"code","93012cef":"code","807d7031":"code","a1ad967f":"code","3c91f3bf":"code","a47dae05":"code","b0a308d5":"code","549000d5":"code","46de90b1":"code","5ac5fef5":"code","141ff51c":"code","e24fcdd8":"code","3ca80daf":"code","85a3255b":"code","06ec235c":"code","d04d562a":"code","cd55c89c":"code","32508ada":"code","92fc54ff":"code","55996ae9":"code","1b699463":"markdown","97bc13cb":"markdown","c90e182c":"markdown","d464de38":"markdown","ce3b71b9":"markdown","2cb95221":"markdown","02d53796":"markdown","abef61f0":"markdown","c631264f":"markdown","b535248b":"markdown","25bd1925":"markdown","06525354":"markdown","0f1d1c1c":"markdown","9e4257da":"markdown"},"source":{"2ef3141d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_log_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d579d54a":"train = pd.read_csv('..\/input\/bike-sharing-demand\/train.csv')\ntest = pd.read_csv('..\/input\/bike-sharing-demand\/test.csv')\ntrain.head()","cff42207":"train['year'] = [t.year for t in pd.DatetimeIndex(train.datetime)]\ntrain['month'] = [t.month for t in pd.DatetimeIndex(train.datetime)]\ntrain['day'] = [t.day for t in pd.DatetimeIndex(train.datetime)]\ntrain['hour'] = [t.hour for t in pd.DatetimeIndex(train.datetime)]\n\ntest['year'] = [t.year for t in pd.DatetimeIndex(test.datetime)]\ntest['month'] = [t.month for t in pd.DatetimeIndex(test.datetime)]\ntest['day'] = [t.day for t in pd.DatetimeIndex(test.datetime)]\ntest['hour'] = [t.hour for t in pd.DatetimeIndex(test.datetime)]","e9c2d35b":"train.head()","93012cef":"train.drop('datetime',axis=1,inplace=True)\ntest.drop('datetime',axis=1,inplace=True)","807d7031":"train.head()","a1ad967f":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nfig, ax = plt.subplots(2,2)\nsns.barplot(train['season'],train['count'],ax=ax[0,0]);\nsns.barplot(train['holiday'],train['count'],ax=ax[0,1]);\nsns.barplot(train['workingday'],train['count'],ax=ax[1,0]);\nsns.barplot(train['weather'],train['count'],ax=ax[1,1]);","3c91f3bf":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nfig, ax = plt.subplots(2,2)\nsns.distplot(train['temp'],ax=ax[0,0]);\nsns.distplot(train['atemp'],ax=ax[0,1]);\nsns.distplot(train['humidity'],ax=ax[1,0]);\nsns.distplot(train['windspeed'],ax=ax[1,1]);","a47dae05":"sns.set(rc={'figure.figsize':(15,10)})\nsns.heatmap(train.corr(),annot=True,linewidths=0.5);","b0a308d5":"train.drop(['casual','registered'],axis=1,inplace=True)","549000d5":"sns.set(rc={'figure.figsize':(20,5)})\nsns.barplot(x=train['month'],y=data['count']);","46de90b1":"season = pd.get_dummies(train['season'],prefix='season')\ntrain = pd.concat([train,season],axis=1)","5ac5fef5":"train.drop('season',axis=1,inplace=True)\ntrain.head()","141ff51c":"weather = pd.get_dummies(train['weather'],prefix='weather')\n\ntrain = pd.concat([train,weather],axis=1)\n\ntrain.drop('weather',axis=1,inplace=True)\ntrain.head()","e24fcdd8":"train.columns.to_series().groupby(data.dtypes).groups","3ca80daf":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","85a3255b":"X = train.drop('count',axis=1)\ny = train['count']\nX = sc.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","06ec235c":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nmodel_rf = rf.fit(X_train,y_train)\ny_pred_rf = model_rf.predict(X_test)\nnp.sqrt(mean_squared_log_error(y_test,y_pred_rf))","d04d562a":"n_estimators = [int(x) for x in range(200,2000,100)]\nmax_feature = ['auto','sqrt']\nmin_sample_split = [2,5,10]\nmin_sample_leaf = [1,2,4]\nmax_depth = [int(x) for x in range(10,110,11)]\nmax_depth.append(None)","cd55c89c":"random_grid = {'n_estimators': n_estimators,\n              'max_depth': max_depth,\n              'max_features': max_feature,\n              'min_samples_leaf': min_sample_leaf,\n              'min_samples_split': min_sample_split}","32508ada":"random_grid","92fc54ff":"rf_tune = RandomForestRegressor()\nfrom sklearn.model_selection import RandomizedSearchCV\nrf_random = RandomizedSearchCV(estimator=rf_tune,param_distributions=random_grid,n_iter=100,cv=5,verbose= 2,n_jobs=-1)","55996ae9":"final_rf = RandomForestRegressor(max_depth=87,max_features='auto',min_samples_leaf=1,min_samples_split=2,n_estimators=1300)\nfinal_model_rf = final_rf.fit(X_train,y_train)\ny_final_pred = final_model_rf.predict(X_test)\nnp.sqrt(mean_squared_log_error(y_test,y_final_pred))","1b699463":"**Data Visualisation**","97bc13cb":"**Splitting data into Train and Test split**","c90e182c":"**Model Building**","d464de38":"by running **rf_random(X_train,y_train)** we will get the optimal parameter as below\n\n* max_depth=87\n* max_features='auto'\n* min_samples_leaf=1\n* min_samples_split=2\n* n_estimators=1300\n\nnow we will make final Random forest model with hyperparameters","ce3b71b9":"This are distribution plot of humidity, windspeed, temp and atemp","2cb95221":"The above plots show us intuitively how count parameter differs with workingday, weather, season, holiday","02d53796":"In above output we can see we converted the datetime column in Machine Learning firendly format now we will drop the datetime column","abef61f0":"**Data Transformation**","c631264f":"The above plot explains the demand of the bicycle according to month.","b535248b":"**Correlation Plot**","25bd1925":"Now we will convert the current datetime column into Machine Learning friendly format i.e Year, Month, Day, Hour.","06525354":"We cans see that registered\/casual is highly correlated with the count which means most of the bike were registered.","0f1d1c1c":"Now we will find the optimal solution using RandomizedSearchCV, the estimator will be Random Forest and parameters will be all the parameters in random_grid","9e4257da":"The above Random Forest model is using default parameter of the Random forest model to reduse RMSLE more we will do hyperparameter tuning the parameters considered are listed below"}}