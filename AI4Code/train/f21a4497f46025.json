{"cell_type":{"5b2490c6":"code","0f6e469c":"code","3e16fc56":"code","045385f8":"code","2fb95ad3":"code","74eaf1b0":"code","df090b77":"code","7a2ae724":"code","b52a5f1c":"code","f833958b":"code","29382419":"code","fe671fe2":"code","d0ac6235":"code","7e50acee":"code","812d4f6f":"code","f2c50465":"code","fec0b7e5":"code","d1b31fe6":"code","24415738":"code","2cfae00a":"code","1f402d40":"code","003764eb":"code","76766526":"markdown","e72f0c80":"markdown","8c04514d":"markdown","7e9972fb":"markdown","1839a4ea":"markdown","a3c53633":"markdown","cbba6c24":"markdown","c675e20e":"markdown","40cdfef3":"markdown","ebf4f9b6":"markdown","70c227df":"markdown","641be890":"markdown"},"source":{"5b2490c6":"!pip install datasets transformers[sentencepiece]","0f6e469c":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\n\nimport transformers\nfrom transformers import AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizer, RobertaTokenizer, DistilBertConfig, BertConfig, TFDistilBertModel","3e16fc56":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","045385f8":"train_df.head()","2fb95ad3":"len(train_df)","74eaf1b0":"labels = train_df['target'].unique()\nlen_labels = len(labels)\n\nlabels, len_labels","df090b77":"# split train dataset into train, validation and test sets\ntrain_text, val_text, train_labels, val_labels = train_test_split(train_df['text'], train_df['target'], \n                                                                    random_state=101, \n                                                                    test_size=0.2, \n                                                                    stratify=train_df['target'])\n\ntest_text = test_df['text']\n\nlen(train_text), len(val_text), len(test_text)","7a2ae724":"# Downloading My nlp healper function script\n!wget https:\/\/raw.githubusercontent.com\/vishalrk1\/pytorch\/main\/nlp_helper.py","b52a5f1c":"from nlp_helper import remove_html, remove_punctuation, lowercase_text, word_lemmatizer\nimport nltk\nnltk.download('wordnet')\n\n# Remove punctuation\ndef remove_punctuation(text):\n    punc = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n    no_punct = ''.join([c for c in text if c not in punc])\n    no_punct = no_punct.lower()\n    return no_punct\n\ndef preprocess_text(text):\n    text = remove_html(text)\n    text = remove_punctuation(text)\n      # text = remove_stopwords(text)\n      # text = lowercase_text(text)\n    text = word_lemmatizer(text)\n    text = ''.join(text)\n    return text\n\n\ntrain_text = train_text.apply(lambda x: preprocess_text(x))\nval_text = val_text.apply(lambda x: preprocess_text(x))\ntest_text = test_text.apply(lambda x: preprocess_text(x))","f833958b":"# get length of all the messages in the train set\nseq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","29382419":"import transformers\nfrom transformers import DistilBertTokenizer, RobertaTokenizer\ndistil_bert = 'distilbert-base-uncased'\n\n# Defining DistilBERT tokonizer\ntokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True, max_length=30, pad_to_max_length=True)\n\n# downloading model config for model \nconfig = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\nconfig.output_hidden_states = False\n\ntransformer_model = TFDistilBertModel.from_pretrained(distil_bert, config=config)","fe671fe2":"# tokenizing all sentences and creating iinputs for model\nfrom tqdm.notebook import tqdm \n\ndef tokenize(sentences, tokenizer):\n    input_ids, input_masks, input_segments = [],[],[]\n    for sentence in tqdm(sentences):\n        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=30, pad_to_max_length=True, return_attention_mask=True, return_token_type_ids=True)\n\n        input_ids.append(inputs['input_ids'])\n        input_masks.append(inputs['attention_mask'])\n        input_segments.append(inputs['token_type_ids'])        \n        \n    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')","d0ac6235":"print('Input Tokens')\ninput_train_ids, input_train_masks, input_train_segments = tokenize(train_text.to_list(), tokenizer)\n\nprint('Validation Tokens')\ninput_val_ids, input_val_masks, input_val_segments = tokenize(val_text.to_list(), tokenizer)\n\nprint('test Tokens')\ninput_test_ids, input_test_masks, input_test_segments = tokenize(test_text.to_list(), tokenizer)","7e50acee":"train_input = tf.data.Dataset.from_tensor_slices((input_train_ids, input_train_masks))\ntrain_output = tf.data.Dataset.from_tensor_slices(train_labels.to_numpy())\ntrain_dataset = tf.data.Dataset.zip((train_input, train_output))\ntrain_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\ntrain_dataset, len(train_dataset)","812d4f6f":"val_input = tf.data.Dataset.from_tensor_slices((input_val_ids, input_val_masks))\nval_output = tf.data.Dataset.from_tensor_slices(val_labels.to_numpy())\nval_dataset = tf.data.Dataset.zip((val_input, val_output))\nval_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\nval_dataset, len(val_dataset)","f2c50465":"test_input = tf.data.Dataset.from_tensor_slices((input_test_ids, input_test_masks))\ntest_dataset = tf.data.Dataset.zip((test_input))\ntest_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\ntest_dataset, len(test_dataset)","fec0b7e5":"input_ids_in = tf.keras.layers.Input(shape=(30,), name='input_token', dtype='int32')\ninput_masks_in = tf.keras.layers.Input(shape=(30,), name='masked_token', dtype='int32') \n\nembedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\nX = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embedding_layer)\nX = tf.keras.layers.Dropout(0.4)(X)\nX = tf.keras.layers.LSTM(128, return_sequences=False)(X)\nX = tf.keras.layers.Dense(64, activation='relu')(X)\nX = tf.keras.layers.Dropout(0.2)(X)\nX = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n\nmodel = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\nfor layer in model.layers[:3]:\n    layer.trainable = False","d1b31fe6":" model.summary()","24415738":"model.compile(\n    loss = 'binary_crossentropy',\n    optimizer = tf.keras.optimizers.Adam(),\n    metrics = ['accuracy']\n)\n\nhistory_1 = model.fit(\n    train_dataset,\n    epochs = 25,\n    steps_per_epoch = len(train_dataset),\n    validation_data = val_dataset,\n    validation_steps = int(0.5 * len(val_dataset)),\n    callbacks = [\n                 tf.keras.callbacks.EarlyStopping(patience=4, restore_best_weights=True),\n                 tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3),\n    ]\n)","2cfae00a":"model.evaluate(val_dataset)","1f402d40":"pred = model.predict([input_test_ids, input_test_masks])\npred = np.squeeze(pred).round()","003764eb":"df = pd.DataFrame()\ndf['id'] = test_df['id']\ndf['target'] = pred.astype(int)\n\ndf.to_csv('submission.csv', index=False)\ndf","76766526":"Here are some resources that i used\n* **[Hugging Face Course](https:\/\/huggingface.co\/course\/chapter0\/1?fw=pt)**\n* **[Fine-Tunning Pretrained Models](https:\/\/towardsdatascience.com\/russian-troll-tweets-classification-using-bert-abec09e43558)**\n* **[classify tweet with bert](https:\/\/www.tensorflow.org\/text\/tutorials\/classify_text_with_bert)**","e72f0c80":"# Importing Tensorflow and Transformers","8c04514d":"# Importing Datasets","7e9972fb":"## Spliting train data into traun and validation datasets","1839a4ea":"# Bert Model & tokenizer","a3c53633":"# Calculating Sequence length","cbba6c24":"# saving test df predictions to csv file","c675e20e":"# optimizer, Loss function and training Model","40cdfef3":"## Preprocessing text Data \n\n* Removing HTML content from tweets\n* removing punctuation\n* Removing stop words\n* Lowercasing all tweets\n\n**You can download my small nlp healper functions script from https:\/\/github.com\/vishalrk1\/pytorch\/blob\/main\/nlp_helper.py**","ebf4f9b6":"# Defining Model","70c227df":"# Creating TF Datasets for model Training","641be890":"# Creating Tokenized data"}}