{"cell_type":{"b2e9da70":"code","c2a438b1":"code","0478f527":"code","b815f3b1":"code","d9398381":"code","ccb00192":"code","1fdd87fb":"code","3912b105":"code","aef6b968":"code","b1716cbe":"markdown","a8925d25":"markdown","b0803877":"markdown","6c33a1f8":"markdown","8b04a0cf":"markdown","9cdc5ad0":"markdown","a316f57c":"markdown","b8db8c09":"markdown","f5ab5e29":"markdown","e8b5ab0b":"markdown","6e647f19":"markdown"},"source":{"b2e9da70":"import pandas as pd\nimport numpy as np\n\n\n\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nYTrain=train_data[\"SalePrice\"]\ntrain_data","c2a438b1":"NANColumns=[]\ni=-1\nfor a in test_data.isnull().sum():\n    i+=1\n    if a!=0:\n        print(test_data.columns[i],a)\n        NANColumns.append(test_data.columns[i])","0478f527":"train_data[\"LotFrontage\"]=train_data[\"LotFrontage\"].replace(np.nan, np.mean(train_data[\"LotFrontage\"]))\ntrain_data[\"GarageYrBlt\"]=train_data[\"GarageYrBlt\"].replace(np.nan, np.mean(train_data[\"GarageYrBlt\"]))\ntrain_data=train_data.drop(columns=[\"Alley\",\"PoolQC\",\"MasVnrType\",\"Fence\",\"MiscFeature\",\"Id\"])\n\ntrain_data[\"MasVnrArea\"]=train_data[\"MasVnrArea\"].replace(np.nan, np.mean(train_data[\"MasVnrArea\"]))\ntrain_data[\"BsmtQual\"]=train_data[\"BsmtQual\"].replace(np.nan,\"X\")\ntrain_data[\"BsmtCond\"]=train_data[\"BsmtCond\"].replace(np.nan,\"X\")\ntrain_data[\"BsmtExposure\"]=train_data[\"BsmtExposure\"].replace(np.nan,\"X\")\ntrain_data[\"BsmtFinType1\"]=train_data[\"BsmtFinType1\"].replace(np.nan,\"X\")\ntrain_data[\"BsmtFinType2\"]=train_data[\"BsmtFinType2\"].replace(np.nan,\"X\")\ntrain_data[\"Electrical\"]=train_data[\"Electrical\"].replace(np.nan,\"X\")\ntrain_data[\"FireplaceQu\"]=train_data[\"FireplaceQu\"].replace(np.nan,\"X\")\ntrain_data[\"GarageType\"]=train_data[\"GarageType\"].replace(np.nan,\"X\")\ntrain_data[\"GarageFinish\"]=train_data[\"GarageFinish\"].replace(np.nan,\"X\")\ntrain_data[\"GarageQual\"]=train_data[\"GarageQual\"].replace(np.nan,\"X\")\ntrain_data[\"GarageCond\"]=train_data[\"GarageCond\"].replace(np.nan,\"X\")\n\n\n\n\ntest_data[\"LotFrontage\"]=test_data[\"LotFrontage\"].replace(np.nan, np.mean(test_data[\"LotFrontage\"]))\ntest_data[\"GarageYrBlt\"]=test_data[\"GarageYrBlt\"].replace(np.nan, np.mean(test_data[\"GarageYrBlt\"]))\ntest_data[\"BsmtFinSF1\"]=test_data[\"BsmtFinSF1\"].replace(np.nan, np.mean(test_data[\"BsmtFinSF1\"]))\ntest_data[\"BsmtFinSF2\"]=test_data[\"BsmtFinSF2\"].replace(np.nan, np.mean(test_data[\"BsmtFinSF2\"]))\ntest_data[\"BsmtUnfSF\"]=test_data[\"BsmtUnfSF\"].replace(np.nan, np.mean(test_data[\"BsmtUnfSF\"]))\ntest_data[\"TotalBsmtSF\"]=test_data[\"TotalBsmtSF\"].replace(np.nan, np.mean(test_data[\"TotalBsmtSF\"]))\ntest_data[\"BsmtHalfBath\"]=test_data[\"BsmtHalfBath\"].replace(np.nan, np.mean(test_data[\"BsmtHalfBath\"]))\ntest_data[\"BsmtFullBath\"]=test_data[\"BsmtFullBath\"].replace(np.nan, np.mean(test_data[\"BsmtFullBath\"]))\ntest_data[\"GarageArea\"]=test_data[\"GarageArea\"].replace(np.nan, np.mean(test_data[\"GarageArea\"]))\ntest_data[\"GarageCars\"]=test_data[\"GarageCars\"].replace(np.nan, np.mean(test_data[\"GarageCars\"]))\n\ntest_data=test_data.drop(columns=[\"Alley\",\"PoolQC\",\"MasVnrType\",\"Fence\",\"MiscFeature\",\"Id\"])\n\ntest_data[\"MasVnrArea\"]=test_data[\"MasVnrArea\"].replace(np.nan, np.mean(test_data[\"MasVnrArea\"]))\ntest_data[\"BsmtQual\"]=test_data[\"BsmtQual\"].replace(np.nan,\"X\")\ntest_data[\"BsmtCond\"]=test_data[\"BsmtCond\"].replace(np.nan,\"X\")\ntest_data[\"BsmtExposure\"]=test_data[\"BsmtExposure\"].replace(np.nan,\"X\")\ntest_data[\"BsmtFinType1\"]=test_data[\"BsmtFinType1\"].replace(np.nan,\"X\")\ntest_data[\"BsmtFinType2\"]=test_data[\"BsmtFinType2\"].replace(np.nan,\"X\")\ntest_data[\"Electrical\"]=test_data[\"Electrical\"].replace(np.nan,\"X\")\ntest_data[\"FireplaceQu\"]=test_data[\"FireplaceQu\"].replace(np.nan,\"X\")\ntest_data[\"GarageType\"]=test_data[\"GarageType\"].replace(np.nan,\"X\")\ntest_data[\"GarageFinish\"]=test_data[\"GarageFinish\"].replace(np.nan,\"X\")\ntest_data[\"GarageQual\"]=test_data[\"GarageQual\"].replace(np.nan,\"X\")\ntest_data[\"GarageCond\"]=test_data[\"GarageCond\"].replace(np.nan,\"X\")\n    \n\n\ntest_data[\"SaleType\"]=test_data[\"SaleType\"].replace(np.nan,\"o\")\ntest_data[\"Functional\"]=test_data[\"Functional\"].replace(np.nan,\"Typ\")\ntest_data[\"KitchenQual\"]=test_data[\"KitchenQual\"].replace(np.nan,\"Gd\")\ntest_data[\"MSZoning\"]=test_data[\"MSZoning\"].replace(np.nan,\"X\")\ntest_data[\"Utilities\"]=test_data[\"Utilities\"].replace(np.nan,\"X\")\ntest_data[\"Exterior1st\"]=test_data[\"Exterior1st\"].replace(np.nan,\"X\")\ntest_data[\"Exterior2nd\"]=test_data[\"Exterior2nd\"].replace(np.nan,\"X\")\ntest_data[\"GarageCond\"]=test_data[\"GarageCond\"].replace(np.nan,\"X\")","b815f3b1":"#This is for if we want to use all data after creating our system.\n#Normally it's not correct to use all data while training,but,as it turns out for this spesific dataset it increases accuracy.\n\nY_Btrain=YTrain\nBtrain=train_data\n\n\n\nY_train=YTrain[0:1200]\nY_cross=YTrain[1200:1460]\n\n\ntrain_data=train_data.drop(\"SalePrice\",axis=1)\nBtrain=Btrain.drop(\"SalePrice\",axis=1)\n\n\n\nCross_data=train_data[1200:1460]\ntrain_data=train_data[0:1200]","d9398381":"CATEGORICAL_COLUMNS =[]\nNUMERIC_COLUMNS =[]\n\n\ni=-1\nfor a in train_data.dtypes:\n    i+=1\n    if  a==float or a==int:\n        NUMERIC_COLUMNS.append(train_data.columns[i])\n    elif a==object:\n        CATEGORICAL_COLUMNS.append(train_data.columns[i])","ccb00192":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nBtrain[CATEGORICAL_COLUMNS]=Btrain[CATEGORICAL_COLUMNS].apply(lambda col: le.fit_transform(col))\ntrain_data[CATEGORICAL_COLUMNS]=train_data[CATEGORICAL_COLUMNS].apply(lambda col: le.fit_transform(col))\nCross_data[CATEGORICAL_COLUMNS]=Cross_data[CATEGORICAL_COLUMNS].apply(lambda col: le.fit_transform(col))\ntest_data[CATEGORICAL_COLUMNS]=test_data[CATEGORICAL_COLUMNS].apply(lambda col: le.fit_transform(col))","1fdd87fb":"from sklearn.neural_network import MLPRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n\nmlp = MLPRegressor(random_state=1,hidden_layer_sizes=(400,1),max_iter=400)\nmlp.fit(Btrain,Y_Btrain)\n\n\n\n\nGReg = GradientBoostingRegressor(random_state=0)\nGReg.fit(Btrain,Y_Btrain)\n\n\n\n\n\nCAT= CatBoostRegressor(verbose=0, n_estimators=300)\nCAT.fit(Btrain,Y_Btrain)\n\n\n\nLGMB = LGBMRegressor()\nLGMB.fit(Btrain,Y_Btrain)\n\n\n\n\nXGBRegressor = XGBRegressor(objective='reg:squarederror')\nXGBRegressor.fit(Btrain,Y_Btrain)\n\n\n\nprint(\"Result of  GradientBoostingRegressor is {}\".format(GReg.score(Cross_data,Y_cross)))\nprint(\"Result of  CATRegressor is {}\".format(CAT.score(Cross_data,Y_cross)))\nprint(\"Result of  LGBMRegressor is {}\".format(LGMB.score(Cross_data,Y_cross)))\nprint(\"Result of MLPRegressor  is {}\".format(mlp.score(Cross_data,Y_cross)))\nprint(\"Result of  XGBRegressor is {}\".format(XGBRegressor.score(Cross_data,Y_cross)))","3912b105":"Results=[]\nfor a in CAT.predict(test_data):\n    Results.append(int(a))\noutput=pd.DataFrame({'Id':range(1461,2920), 'SalePrice': Results})\n\noutput.to_csv('CAT.csv',index=False) ","aef6b968":"from keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nmodel=tf.keras.Sequential([tf.keras.layers.Dense(units=1,input_shape=[1,74],    kernel_initializer='ones',\n    kernel_regularizer=tf.keras.regularizers.L1(0.1),\n    )])\nmodel.add(tf.keras.layers.Dense(50))\nmodel.add(tf.keras.layers.Dense(25))\nmodel.add(tf.keras.layers.Dense(10))\n\n#Adam\nbatch_size = 8\nopt = tf.keras.optimizers.Adam(learning_rate=0.01)\n\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer=opt,loss=\"MeanAbsoluteError\")\nK=model.fit(train_data,Y_train,batch_size=batch_size,epochs=10)\n\n\nmodel.evaluate(Cross_data,Y_cross)","b1716cbe":"# Encoding\n**In order to distinguish numeric and categorical columns, the following code will create 2 lists**","a8925d25":"# Dividing our data into Train set and Cross-Validation se","b0803877":"# Cleaning the data\n**First, we need to find nan\/null variables by the following code(To detect all null values,we should use this both for train and test set)**","6c33a1f8":"**Now we can encode our categorical data.In the apply method, we have given a function which uses LabelEncoder to convert the categorical values into the numeric ones.**","8b04a0cf":"# Extra-for TensorFlow lowers\n* To make practice with TensorFlow neural networks, you can create a code like the following one:\n* Examination of code:\n* 1-You can create different Neural Network structures simply by adding new layers to your initial one. Your initial layer must start with 74 nodes since there are 74 variables in our cleaned data as input.\n* 2-You can add different types of regularizers, to avoid overfitting, but be careful if it is to regularized it can suffer from underfitting.\n* 3-You can rearrange batch_size. When you increase batch size simply you are dividing your data into more pieces to create your hypothesis faster.However, it can create a worse hypothesis as your number of batches increases. If you wanna know why this happens you can check this website(https:\/\/machinelearningmastery.com\/gentle-introduction-mini-batch-gradient-descent-configure-batch-size\/)\n* 4-You can rearrange the number of epochs. Normally the number of epochs(number of we train our model) should be much much higher than 10 (to get better accuracy).\n* 5-You can rearrange the learning rate. If your learning rate is too big you can overshoot optimum minima, if it is too small it can be too slow to converge to your optimum minima.","9cdc5ad0":"# Creating & Training & Testing different models\n\n*Following models are ridiculously easy to create. First, we will define our model like A=func() and then we will train with A.fit(x,y).--Normally first I have trained these with train set rather than all data(And as a convention this is correct). However for this problem when we submit our results, the result which is trained with all data(Btrain) is slightly better.--*","a316f57c":"# Submitting the Output","b8db8c09":"**Then we should change the null variables(If our null variables belong a categorical column than we can replace it with an arbitrary letter-like \"X\"-,if our null variables belong a numeric column we can replace it with the average value of its column, plausibly.)**","f5ab5e29":"*As we see we have very high accuracy(You can also try this with train_set rather than all data.).I have submitted all these results, what I found out is all of these results are having error less than 0.14, however CATregressor has the best one with 0.129*","e8b5ab0b":"*I hope this notebook helped you :)*","6e647f19":"# Uploading Data"}}