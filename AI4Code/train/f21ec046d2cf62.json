{"cell_type":{"0205709a":"code","88df806a":"code","b33c8a56":"code","41d41c02":"code","57d28ff1":"code","5a041ebd":"code","b616a615":"code","75cbe6a4":"code","095ad2c1":"code","b1e6d758":"code","0220c0b8":"code","413d5dd3":"code","2dd858cc":"code","ea0f77a8":"code","2691e237":"code","bde73360":"code","974ddb30":"code","dd8321c5":"code","e92ee566":"code","ff920580":"code","228a8439":"code","57fabedc":"code","a5a29c2f":"code","c43ec700":"code","23485005":"code","bdb289ac":"code","7a38c259":"code","0a25d5f1":"code","652daf32":"code","05249ce1":"code","a77472e5":"code","b8c16eef":"code","e862e487":"code","22063874":"code","a62cd5ca":"code","50240546":"code","f0a39161":"code","7542262a":"code","5165f2ab":"code","a457b7d2":"code","d3f23c18":"code","1a3b8b4f":"code","688db1b4":"code","6e05a8e6":"code","2ef29fed":"code","34538dfc":"code","82438e24":"code","6aee8ed2":"code","e9218e41":"code","3f063315":"code","59ee177d":"code","4bd22c9c":"code","a500fb8b":"code","044814cc":"code","81cfcc6f":"code","16a14f46":"code","8e6bcb24":"code","ee52fe93":"code","ed4b7c9a":"code","5cdbd7ce":"code","351d3b59":"code","6a4df86a":"code","b03e6775":"code","49107181":"code","6777ce12":"code","c57a1001":"code","c1fff2b3":"code","8314803f":"code","7c772ae8":"code","9c684872":"code","eae79a11":"code","67040d34":"markdown","498c3d8a":"markdown","e5f6e680":"markdown","a7043c28":"markdown","adc724cf":"markdown","6a346d14":"markdown","6baad80f":"markdown","ec7705e5":"markdown","b361b372":"markdown","4766fa14":"markdown","3ea5642b":"markdown","1b3a2e73":"markdown","da0c77dd":"markdown","fee493ec":"markdown","5278c202":"markdown","133a681c":"markdown","04910fc5":"markdown","4cd52038":"markdown","52049829":"markdown","6d2d2586":"markdown","41507efc":"markdown","a9c1554b":"markdown","b9d88d02":"markdown","b8202951":"markdown","d7986e4f":"markdown","04992c6b":"markdown","105732ea":"markdown","15283232":"markdown","2e10821d":"markdown","21df22a8":"markdown","b5fa7e7a":"markdown","e4a196ce":"markdown","c58cee56":"markdown","78a7de41":"markdown","45ba78cd":"markdown","e07248e7":"markdown"},"source":{"0205709a":"! apt remove -y openjdk-11-jre-headless","88df806a":"!apt install -y openjdk-8-jdk openjdk-8-jre","b33c8a56":"!pip install pyspark","41d41c02":"import os\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom pylab import *\nfrom pyspark.sql.functions import udf, concat, col, lit\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\nfrom pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()\nsqlContext = SQLContext(sc)","57d28ff1":"df = spark.read.format(\"csv\").option(\"header\", \"true\").load('..\/input\/water-quality-data\/waterquality.csv')\ngdf = gpd.read_file('..\/input\/india-states\/Igismap\/Indian_States.shp')","5a041ebd":"df.show(5)","b616a615":"df.dtypes","75cbe6a4":"from pyspark.sql.types import FloatType","095ad2c1":"df = df.withColumn(\"TEMP\",df[\"TEMP\"].cast(FloatType()))\ndf = df.withColumn(\"pH\",df[\"pH\"].cast(FloatType()))\ndf = df.withColumn(\"DO\",df[\"DO\"].cast(FloatType()))\ndf = df.withColumn(\"CONDUCTIVITY\",df[\"CONDUCTIVITY\"].cast(FloatType()))\ndf = df.withColumn(\"BOD\",df[\"BOD\"].cast(FloatType()))\ndf = df.withColumn(\"NITRATE_N_NITRITE_N\",df[\"NITRATE_N_NITRITE_N\"].cast(FloatType()))\ndf = df.withColumn(\"FECAL_COLIFORM\",df[\"FECAL_COLIFORM\"].cast(FloatType()))\ndf.dtypes","b1e6d758":"df=df.drop('TOTAL_COLIFORM')","0220c0b8":"df.createOrReplaceTempView(\"df_sql\")","413d5dd3":"df_clean = spark.sql('''Select * from df_sql where TEMP is not null and DO is not null \n                        and pH is not null and BOD is not null and CONDUCTIVITY is not null\n                        and NITRATE_N_NITRITE_N is not null and FECAL_COLIFORM is not null''')","2dd858cc":"df_clean.createOrReplaceTempView(\"df_sql\")","ea0f77a8":"do = spark.sql(\"Select DO from df_sql\")\ndo = do.rdd.map(lambda row : row.DO).collect()\nph = spark.sql(\"Select pH from df_sql\")\nph = ph.rdd.map(lambda row : row.pH).collect()\nbod = spark.sql(\"Select BOD from df_sql\")\nbod = bod.rdd.map(lambda row : row.BOD).collect()\nnn = spark.sql(\"Select NITRATE_N_NITRITE_N from df_sql\")\nnn = nn.rdd.map(lambda row : row.NITRATE_N_NITRITE_N).collect()","2691e237":"fig,ax = plt.subplots(num=None,figsize=(14,6), dpi=80, facecolor='w', edgecolor='k')\nsize=len(do)\nax.plot(range(0,size), do, color='blue', animated=True, linewidth=1, label='Dissolved Oxygen')\nax.plot(range(0,size), ph, color='red', animated=True, linewidth=1, label='pH')\nfig,ax2 = plt.subplots(num=None,figsize=(14,6), dpi=80, facecolor='w', edgecolor='k')\nax2.plot(range(0,size), bod, color='orange', animated=True, linewidth=1, label='BOD')\nax2.plot(range(0,size), nn, color='green', animated=True, linewidth=1, label='NN')\nlegend=ax.legend()\nlegend=ax2.legend()","bde73360":"con = spark.sql(\"Select CONDUCTIVITY from df_sql\")\ncon = con.rdd.map(lambda row : row.CONDUCTIVITY).collect()\nfec = spark.sql(\"Select FECAL_COLIFORM from df_sql\")\nfec = fec.rdd.map(lambda row : row.FECAL_COLIFORM).collect()","974ddb30":"fig,ax = plt.subplots(num=None,figsize=(14,6), dpi=80, facecolor='w', edgecolor='k')\nax.plot(range(0,size), con, color='blue', animated=True, linewidth=1)\nfig,ax2 = plt.subplots(num=None,figsize=(14,6), dpi=80, facecolor='w', edgecolor='k')\nax2.plot(range(0,size), fec, color='red', animated=True, linewidth=1)","dd8321c5":"df=df_clean.toPandas()\ndf.dtypes","e92ee566":"start=0\nend=448\nstation=df.iloc [start:end ,0]\nlocation=df.iloc [start:end ,1]\nstate=df.iloc [start:end ,2]\ndo= df.iloc [start:end ,4].astype(np.float64)\nvalue=0\nph = df.iloc[ start:end,5]  \nco = df.iloc [start:end ,6].astype(np.float64)\nbod = df.iloc [start:end ,7].astype(np.float64)\nna= df.iloc [start:end ,8].astype(np.float64)\nfc=df.iloc [2:end ,9].astype(np.float64)\n\n","ff920580":"df=pd.concat([station,location,state,do,ph,co,bod,na,fc],axis=1)\ndf. columns = ['station','location','state','do','ph','co','bod','na','fc']","228a8439":"df['npH']=df.ph.apply(lambda x: (100 if (8.5>=x>=7)  \n                                 else(80 if  (8.6>=x>=8.5) or (6.9>=x>=6.8) \n                                      else(60 if (8.8>=x>=8.6) or (6.8>=x>=6.7) \n                                          else(40 if (9>=x>=8.8) or (6.7>=x>=6.5)\n                                              else 0)))))","57fabedc":"df['ndo']=df.do.apply(lambda x:(100 if (x>=6)  \n                                 else(80 if  (6>=x>=5.1) \n                                      else(60 if (5>=x>=4.1)\n                                          else(40 if (4>=x>=3) \n                                              else 0)))))","a5a29c2f":"df['nco']=df.fc.apply(lambda x:(100 if (5>=x>=0)  \n                                 else(80 if  (50>=x>=5) \n                                      else(60 if (500>=x>=50)\n                                          else(40 if (10000>=x>=500) \n                                              else 0)))))","c43ec700":"df['nbdo']=df.bod.apply(lambda x:(100 if (3>=x>=0)  \n                                 else(80 if  (6>=x>=3) \n                                      else(60 if (80>=x>=6)\n                                          else(40 if (125>=x>=80) \n                                              else 0)))))","23485005":"df['nec']=df.co.apply(lambda x:(100 if (75>=x>=0)  \n                                 else(80 if  (150>=x>=75) \n                                      else(60 if (225>=x>=150)\n                                          else(40 if (300>=x>=225) \n                                              else 0)))))","bdb289ac":"df['nna']=df.na.apply(lambda x:(100 if (20>=x>=0)  \n                                 else(80 if  (50>=x>=20) \n                                      else(60 if (100>=x>=50)\n                                          else(40 if (200>=x>=100) \n                                              else 0)))))\n\ndf.head()\ndf.dtypes","7a38c259":"df['wph']=df.npH * 0.165\ndf['wdo']=df.ndo * 0.281\ndf['wbdo']=df.nbdo * 0.234\ndf['wec']=df.nec* 0.009\ndf['wna']=df.nna * 0.028\ndf['wco']=df.nco * 0.281\ndf['wqi']=df.wph+df.wdo+df.wbdo+df.wec+df.wna+df.wco \ndf","0a25d5f1":"df['quality']=df.wqi.apply(lambda x:('Excellent' if (25>=x>=0)  \n                                 else('Good' if  (50>=x>=26) \n                                      else('Poor' if (75>=x>=51)\n                                          else('Very Poor' if (100>=x>=76) \n                                              else 'Unsuitable')))))","652daf32":"#renaming state names\ngdf['st_nm'].replace({\"Andaman & Nicobar Island\": \"Andaman and Nicobar Islands\",\n                      \"Arunanchal Pradesh\": \"Arunachal Pradesh\",\n                      'Dadara & Nagar Havelli':'Dadra and Nagar Haveli and Daman and Diu',\n                      'Jammu & Kashmir':'Jammu and Kashmir',\n                      'NCT of Delhi':'Delhi'}, inplace=True)\ndf['state'].replace({\"TAMILNADU\": \"TAMIL NADU\"}, inplace=True)\n\n#Capitalizing only the first letter of each word\ndf['state'] = df['state'].str.title()","05249ce1":"gdf = gdf.rename(columns={\"st_nm\": \"state\"})\nmerged = pd.merge(gdf, df , how='outer', on='state')\nmerged['coords'] = merged['geometry'].apply(lambda x: x.representative_point().coords[:])\nmerged['coords'] = [coords[0] for coords in merged['coords']]\nmerged = merged.drop_duplicates(subset =\"state\") \n\nsns.set_context(\"talk\")\nsns.set_style(\"dark\")\ncmap = 'Blues'\nfigsize = (20, 15)\nax = merged.plot(column= 'wqi', cmap=cmap, \n                          figsize=figsize, scheme='User_Defined',\n                          classification_kwds=dict(bins=[0,25,50,75,100]),\n                          edgecolor='black', legend = True)\nfor idx, row in merged.iterrows():\n    ax.text(row.coords[0], row.coords[1], s=row['wqi'], horizontalalignment='center', bbox={'facecolor': 'yellow', 'alpha':0.8, 'pad': 1, 'edgecolor':'blue'})\n\nax.get_legend().set_title('Water Quality Index')\nax.set_title(\"Water Quality Index in each state \", size = 25)\n\nax.set_axis_off()\nplt.axis('equal')\nplt.show()","a77472e5":"spark_df = sqlContext.createDataFrame(df)","b8c16eef":"spark_df.show()","e862e487":"spark_df.createOrReplaceTempView(\"df_sql\")","22063874":"State = spark.sql(\"Select state from df_sql\")\nState = State.rdd.map(lambda row : row.state).collect()","a62cd5ca":"Wqi = spark.sql(\"Select wqi from df_sql\")\nWqi = Wqi.rdd.map(lambda row : row.wqi).collect()","50240546":"plt.barh(State,Wqi)\n\nplt.xlabel(\"WQI\")\nplt.ylabel(\"STATES\")\n\n\nplt.show()","f0a39161":"from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nvectorAssembler = VectorAssembler(inputCols=[\"npH\",\"ndo\",\"nbdo\",\"nec\",\"nna\",\"nco\"], outputCol=\"features\")\nnormalizer = Normalizer(inputCol=\"features\",outputCol=\"features_norm\")","7542262a":"from pyspark.ml.regression import LinearRegression\nlr = LinearRegression(featuresCol=\"features_norm\",labelCol=\"wqi\",maxIter=10,regParam=0.3,elasticNetParam=0.2)","5165f2ab":"from pyspark.ml import Pipeline","a457b7d2":"pipeline = Pipeline(stages=[vectorAssembler,normalizer,lr])","d3f23c18":"train_data,test_data=spark_df.randomSplit([0.8,0.2])","1a3b8b4f":"model = pipeline.fit(train_data)","688db1b4":"predictions = model.transform(train_data)","6e05a8e6":"predictions.select(\"wqi\",\"prediction\").show()","2ef29fed":"model.stages[2].summary.r2","34538dfc":"df = spark_df.toPandas()","82438e24":"data = df.iloc[:,9:15].values\npred = df.iloc[:,21:22].values","6aee8ed2":"from sklearn.model_selection import train_test_split \ndata_train,data_test,pred_train,pred_test = train_test_split(data,pred,test_size=0.20,random_state=1)\npred_train.shape","e9218e41":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","3f063315":"model2 = Sequential()\nmodel2.add(Dense(350,input_dim=6, activation='relu'))\nmodel2.add(Dense(350,activation='relu'))\nmodel2.add(Dense(350,activation='relu'))\nmodel2.add(Dense(1,activation='linear'))","59ee177d":"keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False )\nmodel2.compile(loss='mean_squared_error',optimizer='Adam', metrics=['mse'])","4bd22c9c":"model2.summary()","a500fb8b":"perform = model2.fit(data_train,pred_train,epochs=50,batch_size=32)","044814cc":"prediction = model2.predict(data_train)","81cfcc6f":"plt.plot(perform.history['loss'])","16a14f46":"plt.plot(pred_train,'bo',prediction,'g+')","8e6bcb24":"spark_df = sqlContext.createDataFrame(df)","ee52fe93":"from pyspark.ml.feature import StringIndexer","ed4b7c9a":"indexer = StringIndexer(inputCol=\"quality\",outputCol=\"label\")\nvectorAssembler2 = VectorAssembler(inputCols=[\"npH\",\"ndo\",\"nbdo\",\"nec\",\"nna\",\"nco\",\"wqi\"], outputCol=\"features2\")\nnormalizer2 = Normalizer(inputCol=\"features2\",outputCol=\"features_norm2\")","5cdbd7ce":"from pyspark.ml.classification import LogisticRegression","351d3b59":"lor = LogisticRegression(featuresCol=\"features_norm2\",labelCol=\"label\",maxIter=10)","6a4df86a":"pipeline2 = Pipeline(stages=[indexer,vectorAssembler2,normalizer2,lor])","b03e6775":"train_data,test_data=spark_df.randomSplit([0.8,0.2])","49107181":"model3 = pipeline2.fit(train_data)","6777ce12":"predictions2 = model3.transform(train_data)","c57a1001":"predictions2.select(\"label\",\"prediction\").show()","c1fff2b3":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\neval = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('label').setPredictionCol('prediction')\neval.evaluate(predictions2)","8314803f":"names = [\"Very Poor\",\"Poor\",\"Good\",\"Unsuitable\",\"Excellent\"]","7c772ae8":"predictions2.createOrReplaceTempView(\"predictions2_sql\")","9c684872":"pred = spark.sql(\"Select prediction from predictions2_sql\")\npred = pred.rdd.map(lambda row : int(row.prediction)).collect()\nqua = spark.sql(\"Select quality from predictions2_sql\")\nqua = qua.rdd.map(lambda row : row.quality).collect()","eae79a11":"for x in range(100):\n    print(\"Predicted:\", names[pred[x]], \"Actual:\", qua[x])","67040d34":"#### Let us convert our data to pandas frame. We are doing this because to train a model we need what we have to predict which is not in data. So we have to calculate water quality index which requires many steps but can be easily done using pandas and in less number of steps. Also we will able to visualize our data in tabular form more effectively.","498c3d8a":"### The Water Quality Index is calculated by aggregating the quality rating with the weight linearly, \n#### WQI = \u2211 (qn x Wn)\n#### where qn =Quality rating for the nth Water quality parameter, Wn= unit weight for the nth parameters.       \n#### Although for calculation qn we have standard formula but it was not possible in this case, so we applied a standard method for calculating quality rating for each parameter.","e5f6e680":"<a id=4><\/a>\n# **Data Cleaning**","a7043c28":"<a id=6><\/a>\n# **Feature Engineering**","adc724cf":"#### Let's visualize the water quality index in each state of India.","6a346d14":"# **Table of Contents**\n* [Setting up the environment](#1)\n* [Importing Libraries](#2)\n* [Uploading the data](#3)\n* [Data Cleaning](#4)\n* [EDA](#5)\n* [Feature Engineering](#6)\n* [Model Creation](#7)","6baad80f":"#### Then import LogisticRegression from pyspark.ml.classification and applied it to our normalized data. Afterthat, import Pipeline from pyspark.ml and include all those steps in the pipeline that have been done.","ec7705e5":"#### Now we check the performance of our model.","b361b372":"#### Then import LinearRegression from pyspark.ml.regression and applied it to our normalized data. Afterthat, import Pipeline from pyspark.ml and include all those steps in the pipeline that have been done.","4766fa14":"<a id=1><\/a>\n# **Setting up the environment**","3ea5642b":"#### As our quality column contains values in string format so first we indexed them using StringIndexer. Then data is converted which are required to predict water quality into vector form by using VectorAssembler. Then we normalize our data by using Normalizer.","1b3a2e73":"# **Water Quality Prediction**\nWe all know water is one of the most essential resource for our living. But as the development is increasing, we are exploiting water by wasting it and treating it with harmful materials which makes water impure and unfit for use. This is the reason it is very important to know the quality of water. This kernel is based on water quality prediction. In this kernel, water quality index (WQI) and quality status of water is predicted through some parameters that affects water quality. \nIn this notebook I have performed Data Cleaning steps and did Exploratory Data Analysis. Then I have did some calculations as the data does not contain the column which can be used for prediction.\nThen I have created 3 models for prediction. The first model is Non-Deep Learning based Linear Regression model. The second model is Deep Learning Based Linear Regression and the last one is Logistic Regression model. I have only used sparkml to create all the models.","da0c77dd":"#### Let us again convert the whole data in spark frame for further processes.","fee493ec":"#### As we observed obove that all the columns have string data types, but for the calculation of water quality index we need to convert them in float data type. So we will convert the required columns in the float data type.","5278c202":"<a id=5><\/a>\n# **EDA**\n### Let's visualize our data.","133a681c":"#### Now let us check our predictions.","04910fc5":"## Logistic Regression Model\n#### Here we are creating a logistic regression model because we don't have to predict a continuous value. ","4cd52038":"#### Now we check performance of our model","52049829":"#### Now we apply the formula of wqi by first multiplying all the quality rating with its weight and then summed all the values.","6d2d2586":"## Water Quality Prediction\n#### After predicting water quality index, now we classify water on the basis of its WQI and predict its quality.","41507efc":"#### Now we want to remove all the rows which contain any null value in it. So for applying a SQL query we first have to register it has a virtual temporary table and then we will issue SQL query. We are doing this because it is important to perform data cleansing steps as it will make our model to work better.","a9c1554b":"## Non Deep Learning Based Linear Regresion Model\n\n#### In this, first data is converted which are required to predict WQI into vector form by using VectorAssembler. Then we normalize our data by using Normalizer.","b9d88d02":"<a id=2><\/a>\n# **Importing libraries**","b8202951":"<a id=7><\/a>\n# **Model Creation**\n#### Now we apply machine learning and deep learning algorithms to predict the data.","d7986e4f":"#### Then we classify the water on the basis of their water quality index.","04992c6b":"#### As the quality column is in string format so we convert our predicted data which are in numbers to their real string values and compared with the actual data.","105732ea":"#### Before starting we first have to change the java version because if we will use version 11 then we will get some errors and we will not be able to use pyspark properly. So we will delete java version 11 and install java version 8.","15283232":"<a id=3><\/a>\n# **Uploading the data**\n#### Then, we upload the data in the spark frame.","2e10821d":"#### Then we initialize model and add layers to it. Afterwards, the model is compiled with optimizer Adam and loss function mean squared error and then training is done.","21df22a8":"#### Now we will first install pyspark.","b5fa7e7a":"### Initialization","e4a196ce":"## Deep Learning Based Linear Regression Model\n#### In this first we collect our data in an array form and to reduce number of steps we converted our data in pandas frame. ","c58cee56":"#### Before training, our data is randomly split in two parts so as to avoid overfitting and then training is done.","78a7de41":"#### Now we check performance of our model.","45ba78cd":"#### Now as column TOTAL_COLIFORM is not required so we will drop this column. ","e07248e7":"**Please if you want to share any suggestion or any doubts regarding any step in the notebook comment below and I will definitely try to solve your doubt.<br> Also, if you want to know more about Spark ML or if you don't know much about Spark ML you can view my another notebook: - https:\/\/www.kaggle.com\/utcarshagrawal\/titanic-spark-ml-magic-eda-feature-engineering\/notebook.<br> This notebook will work as a perfect tutorial for beginners.**\n\n## <font color='red'> Please do an upvote if you find this kernel useful or if you liked the kernel! <\/font>"}}