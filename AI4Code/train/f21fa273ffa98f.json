{"cell_type":{"d6fb614a":"code","ebed529a":"code","e5e4bd6a":"code","6a3fe803":"code","cfbd2600":"code","08b1458e":"code","aeefc9a4":"code","06c84672":"code","3adfece9":"code","22918082":"code","77bbe804":"code","e1b64588":"code","e1aa6cf1":"code","33b20bda":"code","13e2f9a7":"markdown","3aecaad8":"markdown"},"source":{"d6fb614a":"import torch\nimport torch.nn as nn\nimport random\nimport string\nimport sys\nimport unidecode\nfrom torch.utils.tensorboard import SummaryWriter","ebed529a":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nall_characters = string.printable\nn_characters = len(all_characters)\n\nprint(n_characters, all_characters)","e5e4bd6a":"import pandas\n\nfile1 = pandas.read_csv('..\/input\/indian-male-baby-names\/Indian-Male-Names.csv')\n\nfile1.head()","6a3fe803":"all_names = [str(x).split(\" \")[0] for x in file1[\"name\"].values]\n# we only need first names\nall_names[:5]","cfbd2600":"file2 = open(\"names_parsed.txt\", 'w+')\nfile2.write(\"\\n\".join(all_names))\n\nimport unidecode\nfile = unidecode.unidecode(open(\"names_parsed.txt\", \"r+\").read())\n\nfile[:5]","08b1458e":"class myRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(myRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x, hidden, cell, test=False):\n        out = self.embed(x)\n        if (test):\n            print(\"output: \", out)\n            print(\"output unsqueeze 1: \", out.unsqueeze(1))\n            print(\"hidden : \", hidden)\n            print(\"cell : \", cell)\n        out, ( hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[0], -1))\n        \n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device=device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device=device)\n        return hidden, cell","aeefc9a4":"# input_size = 10\n# output_size = 2\n# hidden_size = 2\n# num_layers = 2\n# batch_size = 2","06c84672":"# rnn = myRNN(input_size, output_size, hidden_size, num_layers)","3adfece9":"# hidden,cell = rnn.init_hidden(batch_size)\n# hidden.shape, cell.shape","22918082":"# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","77bbe804":"# x = torch.tensor([1,2]).to(device=device)\n\n# x is ultimately a tensor of index","e1b64588":"# rnn.forward(x, hidden, cell)\n\n# it is working but dont know how","e1aa6cf1":"len(all_characters)","33b20bda":"class myGenerator:\n    def __init__(self):\n        self.chunk_len = 250\n        self.hidden_size = 256\n        self.num_layers = 2\n        self.lr = 0.003\n        self.num_epochs = 200\n        self.print_every = 40\n        self.batch_size = 1\n    \n    def char_tensor(self, str_input):\n        #converts string input to tensor \n        c_tensor = torch.zeros(len(str_input)).long()\n        \n        for c in range(len(str_input)):\n            try:\n                c_tensor[c] = all_characters.index(str_input[c])\n            except Exception as e:\n                print(e)\n                print(f\" error by {str_input}\")\n        \n        return c_tensor\n\n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        \n        text_str = file[start_idx:end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n        \n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n        \n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = \"A\", predict_len = 100, temprature=0.85):\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        \n        predicted = initial_str\n        \n        for p in range(len(initial_str) - 1):\n            _, (hidden,cell) = self.rnn(initial_input[p].view(1).to(device=device), hidden, cell)\n        \n        last_char = initial_input[-1]\n        \n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(last_char.view(1).to(device=device), hidden, cell)\n            output_dist = output.data.view(-1).div(temprature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n            \n        return predicted\n            \n    def train(self):\n        #input_size, output_size, hidden_size, num_layers\n        self.rnn = myRNN(n_characters, self.hidden_size, self.num_layers, n_characters).to(device=device)\n        optimizer = torch.optim.Adam(self.rnn.parameters(),lr = self.lr)\n        \n        loss_criterion = nn.CrossEntropyLoss()\n        writer = SummaryWriter(f'runs\/names0')\n        \n        print(\"start training\")\n        \n        for epoch in range(1,self.num_epochs+1):\n            inp, target = self.get_random_batch()\n            \n            inp = inp.to(device=device)\n            target = target.to(device=device)\n            \n            optimizer.zero_grad()\n            \n            hidden, cell = self.rnn.init_hidden(batch_size= self.batch_size)\n            loss = 0\n            \n            for c in range(self.chunk_len):\n                output, (hidden, cell) = self.rnn(inp[:,c], hidden, cell)\n                loss += loss_criterion(output, target[:,c])\n#             out , (hidden1, cell1) = self.rnn(inp)\n#             loss = loss_criterion(out, target)\n            \n            loss.backward()\n            \n            optimizer.step()\n            loss = loss.item() \/ self.chunk_len\n            \n            if epoch % self.print_every == 0:\n                print(f\"loss for epoch {epoch+1} : {loss}\")\n                print(self.generate())\n            \n            writer.add_scalar('Training loss', loss, global_step=epoch)\n            \n            \n        \n        \ngenames = myGenerator()\ngenames.train()        ","13e2f9a7":"### Testing","3aecaad8":"https:\/\/www.youtube.com\/watch?v=WujVlF_6h5A&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=33\n\n### testing over"}}