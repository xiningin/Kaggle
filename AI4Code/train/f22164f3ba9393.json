{"cell_type":{"a8c0a3c4":"code","c663726d":"code","e6813f60":"code","b0a29487":"code","7be65fe5":"code","8b2ef553":"code","29187eb6":"code","86e5ecfa":"code","9b956495":"code","dc11466d":"code","b9073e37":"code","cfc4b204":"code","8fc39652":"code","caec3073":"code","a2508d8d":"markdown","fc806614":"markdown","5caf47ad":"markdown","72f14a23":"markdown","b291a96b":"markdown"},"source":{"a8c0a3c4":"!pip install pytorch-tabnet","c663726d":"import numpy as np \nimport pandas as pd \nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport torch\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\n\nimport plotly.express as px\nfrom matplotlib import pyplot as plt","e6813f60":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\ndf_sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')\n\n# DEFINE CATEGORICAL AND CONTINUOUS VARIABLES\n\nCAT_COLS = [c for c in df_train.columns if c.startswith(\"cat\")]\nNUM_COLS = [c for c in df_train.columns if c.startswith(\"cont\")]\n\nLOW_FREQ_THRESH = 50\n\nencoders = {}\n# Categorical features need to be LabelEncoded\nfor cat_col in CAT_COLS:\n    label_enc = LabelEncoder()\n    \n    # Group low frequencies into one value\n    value_counts = df_train[cat_col].value_counts()\n    is_low_frequency = value_counts < LOW_FREQ_THRESH    \n    low_freq_values = value_counts.index[is_low_frequency]    \n    if len(low_freq_values)>0:\n        df_train.loc[df_train[cat_col].isin(low_freq_values), cat_col] = \"low_frequency\"\n        # update test set as well\n        df_test.loc[df_test[cat_col].isin(low_freq_values), cat_col] = \"low_frequency\"\n        \n    df_train[cat_col] = label_enc.fit_transform(df_train[cat_col])\n    encoders[cat_col] = label_enc\n    \n# Encode test set\nfor cat_col in CAT_COLS:\n    label_enc = encoders[cat_col]\n    le_dict = dict(zip(label_enc.classes_, label_enc.transform(label_enc.classes_)))\n    # Replace unknown values by the most common value\n    # Changing this to another value might make more sense\n    if le_dict.get(\"low_frequency\") is not None:\n        default_val = le_dict[\"low_frequency\"]\n    else:\n        default_val = df_train[cat_col].mode().values[0]\n    df_test[cat_col] = df_test[cat_col].apply(lambda x: le_dict.get(x, default_val ))\n    \n# Clip numerical features in test set to match training set\nfor num_col in NUM_COLS:\n    df_test[num_col] = np.clip(df_test[num_col], df_train[num_col].min(), df_train[num_col].max())\n    \n    # Taken from https:\/\/www.kaggle.com\/siavrez\/kerasembeddings\n    df_train[f'q_{num_col}'], bins_ = pd.qcut(df_train[num_col], 25, retbins=True, labels=[i for i in range(25)])\n    df_test[f'q_{num_col}'] = pd.cut(df_test[num_col], bins=bins_, labels=False, include_lowest=True)\n    CAT_COLS.append(f'q_{num_col}')\n    \nFEATURES = CAT_COLS + NUM_COLS","b0a29487":"cat_dims = df_train[CAT_COLS].nunique().to_list()\ncat_idxs = [FEATURES.index(cat_col) for cat_col in CAT_COLS]\ncat_emb_dims = np.ceil(np.log(cat_dims)).astype(np.int).tolist()\ncat_emb_dims = np.ceil(np.clip((np.array(cat_dims)) \/ 2, a_min=1, a_max=50)).astype(np.int).tolist()","7be65fe5":"X = df_train[FEATURES].values\ny = df_train[\"target\"].values\n\nX_test = df_test[FEATURES].values","8b2ef553":"from pytorch_tabnet.pretraining import TabNetPretrainer\n\nN_D = 16\nN_A = 16\nN_INDEP = 2\nN_SHARED = 2\nN_STEPS = 1 #2\nMASK_TYPE = \"sparsemax\"\nGAMMA = 1.5\nBS = 512\nMAX_EPOCH =  20 # 20\nPRETRAIN = True\n\n\nif PRETRAIN:\n    pretrain_params = dict(n_d=N_D, n_a=N_A, n_steps=N_STEPS,  #0.2,\n                           n_independent=N_INDEP, n_shared=N_SHARED,\n                           cat_idxs=cat_idxs,\n                           cat_dims=cat_dims,\n                           cat_emb_dim=cat_emb_dims,\n                           gamma=GAMMA,\n                           lambda_sparse=0., optimizer_fn=torch.optim.Adam,\n                           optimizer_params=dict(lr=2e-2),\n                           mask_type=MASK_TYPE,\n                           scheduler_params=dict(mode=\"min\",\n                                                 patience=3,\n                                                 min_lr=1e-5,\n                                                 factor=0.5,),\n                           scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,                         \n                           verbose=1,\n                          )\n\n    pretrainer = TabNetPretrainer(**pretrain_params)\n\n    pretrainer.fit(X_train=X_test, \n                   eval_set=[X],\n                   max_epochs=MAX_EPOCH,\n                   patience=25, batch_size=BS, virtual_batch_size=BS, #128,\n                   num_workers=0, drop_last=True,\n                   pretraining_ratio=0.5 # The bigger your pretraining_ratio the harder it is to reconstruct\n                  )","29187eb6":"BS = 2048\nMAX_EPOCH =  20\nskf = StratifiedKFold(n_splits=5, random_state=2021, shuffle=True)\n\n\ndf_train['oof_preds'] = np.nan\n\nfold_nb = 1\nfor train_index, valid_index in skf.split(X, y):\n    X_train, X_valid = X[train_index], X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n\n    tabnet_params = dict(n_d=N_D, \n                         n_a=N_A,\n                         n_steps=N_STEPS, gamma=GAMMA,\n                         n_independent=N_INDEP, n_shared=N_SHARED,\n                         lambda_sparse=1e-5,\n                         seed=0,\n                         clip_value=2,\n                         cat_idxs=cat_idxs,\n                         cat_dims=cat_dims,\n                         cat_emb_dim=cat_emb_dims,\n                         mask_type=MASK_TYPE,\n                         device_name='auto',\n                         optimizer_fn=torch.optim.Adam,\n                         optimizer_params=dict(lr=5e-2, weight_decay=1e-5),\n                         scheduler_params=dict(max_lr=5e-2,\n                                               steps_per_epoch=int(X_train.shape[0] \/ BS),\n                                               epochs=MAX_EPOCH,\n                                               #final_div_factor=100,\n                                               is_batch_level=True),\n                        scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n#                               scheduler_params=dict(mode='max',\n#                                                     factor=0.5,\n#                                                     patience=5,\n#                                                     is_batch_level=False,),\n#                               scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                         verbose=1)\n    # Defining TabNet model\n    model = TabNetClassifier(**tabnet_params)\n\n    model.fit(X_train=X_train, y_train=y_train,\n              from_unsupervised=pretrainer if PRETRAIN else None,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_name=[\"train\", \"valid\"],\n              eval_metric=[\"auc\"],\n              batch_size=BS,\n              virtual_batch_size=256,\n              max_epochs=MAX_EPOCH,\n              drop_last=True,\n              pin_memory=True,\n              patience=10,\n             )\n\n\n    val_preds = model.predict_proba(X_valid)[:, -1]\n    print('auc:', roc_auc_score(y_true=y_valid, y_score=val_preds))\n    \n    df_train.loc[valid_index, 'oof_preds'] = val_preds\n    \n    test_preds = model.predict_proba(X_test)[:, -1]\n    df_sub[f\"fold_{fold_nb}\"] = test_preds\n    fold_nb+=1","86e5ecfa":"global_auc = roc_auc_score(y_true=df_train.target, y_score=df_train.oof_preds)\nprint(f\"Global AUC score : {global_auc:.4f}\")","9b956495":"# Global Feature importance\nfeat_importances = model.feature_importances_\nindices = np.argsort(feat_importances)\n\nfig = px.bar(y=feat_importances[indices], x=[FEATURES[idx] for idx in indices],\n             title=\"Global feature importance\")\nfig.update_xaxes(type='category', title='Feature Name')\nfig.update_yaxes(title='Importance')","dc11466d":"LIMIT_EXPLAIN = 60000\n\nexplain_mat, masks = model.explain(X_valid[:LIMIT_EXPLAIN, :])\n# Normalize the importance by sample\nnormalized_explain_mat = np.divide(explain_mat, explain_mat.sum(axis=1).reshape(-1, 1))\n\n# Add prediction to better understand correlation between features and predictions\nexplain_and_preds = np.hstack([normalized_explain_mat, val_preds[:LIMIT_EXPLAIN].reshape(-1, 1)])","b9073e37":"# sort rows in prediction order\nsorted_index = np.argsort(explain_and_preds[:,-1])\n\npx.imshow(explain_and_preds[sorted_index],\n          labels=dict(x=\"Features\", y=\"Samples\", color=\"Importance\"),\n          x=FEATURES+[\"prediction\"],\n          title=\"Sample wise feature importance (reality is more complex than global feature importance)\")","cfc4b204":"non_null_idx = (explain_and_preds == 0).sum(axis=0) < 0.9*LIMIT_EXPLAIN\ncorrelation_importance = np.corrcoef(explain_and_preds[:, non_null_idx].T)\n\npx.imshow(correlation_importance,\n          labels=dict(x=\"Features\", y=\"Features\", color=\"Correlation\"),\n          x=np.array(FEATURES+[\"prediction\"])[non_null_idx], y=np.array(FEATURES+[\"prediction\"])[non_null_idx],\n          title=\"Correlation between attention mechanism for each feature and predictions\")","8fc39652":"final_sub = df_sub.copy()\nfold_cols = [f for f in final_sub.columns if f.startswith(\"fold\")]\nfinal_sub['target'] = final_sub[fold_cols].mean(axis=1)\nfinal_sub.drop(columns=fold_cols, inplace=True)","caec3073":"final_sub.to_csv('submission.csv', index=False)","a2508d8d":"# Create submission file","fc806614":"## Pip install pytorch-tabnet","5caf47ad":"# Few plots to understand one model (last fold)","72f14a23":"# How to read this\n\nIt looks like looking at only a few features is enough to get a pretty decent score...\n\nThis defintely should be investigated!","b291a96b":"### Let's try to use information contained in test set to pretrain models\n\nAs the pretraining seems quite smooth, it might help to try pretraining on both X and X_test to learn from more data."}}