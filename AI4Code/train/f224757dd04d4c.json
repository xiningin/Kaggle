{"cell_type":{"89e8f015":"code","8f04cae9":"code","ee36465c":"code","bce2dcc6":"code","d99582ca":"code","ff213af7":"code","09c4d4ea":"code","2d7457a1":"code","c18eb8d3":"code","76ea2f69":"code","22ee3003":"code","cd220d40":"code","8155bd83":"code","218dbd0e":"code","26692e9a":"code","0ea6de70":"code","59f6f50b":"code","02f0ab54":"code","5faeda57":"code","b544c3b6":"code","103cc6a3":"code","b3ff0f04":"code","f8590e4f":"code","2f293fb9":"code","368d6f59":"markdown","fe509cab":"markdown","ba0b7b47":"markdown","62f26740":"markdown","61c5077b":"markdown","1a5dbf13":"markdown","97de0b05":"markdown","ab6d91be":"markdown","ff9f5dc0":"markdown","4ce1ba92":"markdown","68fd5405":"markdown","82366b3c":"markdown","c59a380a":"markdown","a81d6080":"markdown","4efeb49e":"markdown","4665d948":"markdown","ba6c6d8a":"markdown","1529a1a0":"markdown","a2606487":"markdown","fbb81841":"markdown","95fc85c5":"markdown","4a9dbcdf":"markdown","0e9f69d0":"markdown","e0b3ef86":"markdown","ec7ce1f5":"markdown","723d41df":"markdown"},"source":{"89e8f015":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n# Ignore warning messages\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","8f04cae9":"# Modelling Algorithms\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import PassiveAggressiveClassifier\n\n# Modelling Helpers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\n\n# Computations\nimport itertools\n\n# Visualization\nimport matplotlib.pyplot as plt","ee36465c":"train = pd.read_csv(\"..\/input\/fake-news\/train.csv\")\ntest  = pd.read_csv (\"..\/input\/fake-news\/test.csv\")\nsubmit  = pd.read_csv (\"..\/input\/fake-news\/submit.csv\")","bce2dcc6":"train.head()","d99582ca":"print(f\"Train Shape : {train.shape}\")\nprint(f\"Test Shape : {test.shape}\")\nprint(f\"Submit Shape : {submit.shape}\")","ff213af7":"train.info()","09c4d4ea":"train.isnull().sum()","2d7457a1":"train.dtypes.value_counts()","c18eb8d3":"test=test.fillna(' ')\ntrain=train.fillna(' ')","76ea2f69":"# Create a column with all the data available\ntest['total']=test['title']+' '+test['author']+' '+test['text']\ntrain['total']=train['title']+' '+train['author']+' '+train['text']","22ee3003":"# Have a glance at our training set\ntrain.info()\ntrain.head()","cd220d40":"# Dividing the training set by using train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train['total'], train.label, test_size=0.20, random_state=0)","8155bd83":"# Initialize the `count_vectorizer` \ncount_vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english') \n# Fit and transform the training data.\ncount_train = count_vectorizer.fit_transform(X_train)\n# Transform the test set \ncount_test = count_vectorizer.transform(X_test)","218dbd0e":"#Initialize the `tfidf_vectorizer` \ntfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n#Fit and transform the training data \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n#Transform the test set \ntfidf_test = tfidf_vectorizer.transform(X_test)","26692e9a":"nb_classifier = MultinomialNB(alpha = 0.1)\nnb_classifier.fit(count_train, y_train)\npred_nb_count = nb_classifier.predict(count_test)\nacc_nb_count = metrics.accuracy_score(y_test, pred_nb_count)\nprint(acc_nb_count)","0ea6de70":"# tune the hyperparameter alpha for the naive bayes classifier\nfor alpha in np.arange(0,1,.05):\n    nb_classifier_tune = MultinomialNB(alpha=alpha)\n    nb_classifier_tune.fit(count_train, y_train)\n    pred_tune = nb_classifier_tune.predict(count_test)\n    score = metrics.accuracy_score(y_test, pred_tune)\n    print(\"Alpha: {:.2f} Score: {:.5f}\".format(alpha, score))","59f6f50b":"# Let's re-run our fine-tuned model and plot the confusion matrix\nnb_classifier = MultinomialNB(alpha = 0.15)\nnb_classifier.fit(count_train, y_train)\npred_nb_count = nb_classifier.predict(count_test)\ncm = metrics.confusion_matrix(y_test, pred_nb_count, labels=[0,1])\n\n# Creating a function that outputs a confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    \nplot_confusion_matrix(cm, classes=['TRUE','FAKE'], title ='Confusion matrix for a MultinomialNB with Count Vectorizer')","02f0ab54":"nb_classifier = MultinomialNB(alpha = 0.1)\nnb_classifier.fit(tfidf_train, y_train)\npred_nb_tfidf = nb_classifier.predict(tfidf_test)\nacc_nb_tfidf = metrics.accuracy_score(y_test, pred_nb_tfidf)\nprint(acc_nb_tfidf)","5faeda57":"# tune the hyperparameter alpha for the naive bayes classifier\nfor alpha in np.arange(0,0.1,.01):\n    nb_classifier_tune = MultinomialNB(alpha=alpha)\n    nb_classifier_tune.fit(tfidf_train, y_train)\n    pred_tune = nb_classifier_tune.predict(tfidf_test)\n    score = metrics.accuracy_score(y_test, pred_tune)\n    print(\"Alpha: {:.2f}  Score: {:.5f}\".format(alpha, score))","b544c3b6":"# Let's run the optimized model with best value of hyperparameter and check the confusion matrix\nnb_classifier = MultinomialNB(alpha = 0.01)\nnb_classifier.fit(tfidf_train, y_train)\npred_nb_tfidf = nb_classifier.predict(tfidf_test)\ncm2 = metrics.confusion_matrix(y_test, pred_nb_tfidf, labels=[0,1])\nplot_confusion_matrix(cm2, classes=['TRUE','FAKE'], title ='Confusion matrix for a MultinomialNB with Tf-IDF')","103cc6a3":"from sklearn.linear_model import PassiveAggressiveClassifier\nlinear_classifier = PassiveAggressiveClassifier(max_iter=10)\nlinear_classifier.fit(count_train, y_train)\npred_linear_count = linear_classifier.predict(count_test)\nacc_linear_count = metrics.accuracy_score(y_test, pred_linear_count)\nprint(acc_linear_count)\ncm6 = metrics.confusion_matrix(y_test, pred_linear_count, labels=[0,1])\nplot_confusion_matrix(cm6, classes=['TRUE','FAKE'], title ='Confusion matrix for a PA Classifier with Count Vectorizer')","b3ff0f04":"linear_classifier = PassiveAggressiveClassifier(max_iter=10)\nlinear_classifier.fit(tfidf_train, y_train)\npred_linear_tfidf = linear_classifier.predict(tfidf_test)\nacc_linear_tfidf = metrics.accuracy_score(y_test, pred_linear_tfidf)\nprint(acc_linear_tfidf)\ncm5 = metrics.confusion_matrix(y_test, pred_linear_tfidf, labels=[0,1])\nplot_confusion_matrix(cm5, classes=['TRUE','FAKE'], title ='Confusion matrix for a PA Classifier with Tf-IDF')","f8590e4f":"logreg = LogisticRegression(C=1e5)\nlogreg.fit(tfidf_train, y_train)\npred_logreg_tfidf = logreg.predict(tfidf_test)\npred_logreg_tfidf_proba = logreg.predict_proba(tfidf_test)[:,1]\nacc_logreg_tfidf = metrics.accuracy_score(y_test,pred_logreg_tfidf)\nprint(acc_logreg_tfidf)\ncm4 = metrics.confusion_matrix(y_test, pred_logreg_tfidf, labels=[0,1])\nplot_confusion_matrix(cm4, classes=['TRUE','FAKE'], title ='Confusion matrix for a Logistic Regression with Tf-IDF')","2f293fb9":"logreg = LogisticRegression(C=1e5)\nlogreg.fit(count_train, y_train)\npred_logreg_count = logreg.predict(count_test)\nacc_logreg_count = metrics.accuracy_score(y_test,pred_logreg_count)\nprint(acc_logreg_count)\ncm3 = metrics.confusion_matrix(y_test, pred_logreg_count, labels=[0,1])\nplot_confusion_matrix(cm3, classes=['TRUE','FAKE'], title ='Confusion matrix for a Logistic Regression with Count Vectorizer')","368d6f59":"<p style = \"font-size : 20px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong>If model is trained on a concatenation of the title, the author and the main text, the model would be more generalized because adding more words to the input might increase the reliablity of the model.<\/strong><\/p>","fe509cab":"<p style = \"font-size : 35px; color :#000080 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #808080; border-radius: 5px 5px;\"><strong>Data Preprocessing<\/strong><\/p>","ba0b7b47":"<p style = \"font-size : 30px; color : #4e8d7c ; font-family : 'Comic Sans MS';  \"><strong>  3. Passive Agressive Classifier With Count Vectorizer <\/strong><\/p> \n\n","62f26740":"<p style = \"font-size : 30px; color : #4e8d7c ; font-family : 'Comic Sans MS';  \"><strong> 1. Multinomial Naive Bayes with Count Vectorizer (BagofWords) <\/strong><\/p> \n\n\n![bagword_omdrsi.png](http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1507674699\/bagword_omdrsi.png)\n","61c5077b":"<p style = \"font-size : 20px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong>This model has a very high accuracy score, and only 58 records were misclassified. So far, Logistic Regression played it best ! Let's see the same with CountVectorizer ie. Bag of Words concept.<\/strong><\/p>\n\n","1a5dbf13":"\n<p style = \"font-size : 35px; color :#000080 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #808080; border-radius: 5px 5px;\"><strong>Dataset<\/strong><\/p>\n\n<p style = \"font-size : 25px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong><\/strong>1. train.csv: A full training dataset with the following attributes:<\/p>\n<p style = \"font-size : 25px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong><\/strong>2. id: unique id for a news article.<\/p>\n<p style = \"font-size : 25px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong><\/strong>3. title: the title of a news article.<\/p>\n<p style = \"font-size : 25px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong><\/strong>4. author: author of the news article.<\/p>\n<p style = \"font-size : 25px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong><\/strong>5. text: the text of the article; could be incomplete.<\/p>\n<p style = \"font-size : 25px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong><\/strong>6. label: a label that marks the article as potentially unreliable. Where 1: unreliable and 0: reliable.<\/p>\n\n\n","97de0b05":"<p style = \"font-size : 50px; color :  #000000 ; font-family : 'Comic Sans MS'; text-align : center; background-color :#FFFF00 ; border-radius: 5px 5px;\"><strong>Fake News \ud83d\udcf0 Classification - NLP<\/strong><\/p> \n\n","ab6d91be":"<p style = \"font-size : 25px; color : #8cff32 ; font-family : 'Comic Sans MS';  \"><strong>It's THE BEST MODEL YAY ! This is our favorite model so far. Indeed, even though the accuracy score is a bit lower, we have less fake news labeled as true news ie. only 44. Therefore, I choose this model because it seems to maximize the accuracy while minimizing the false negative rate!<\/strong><\/p>\n\n","ff9f5dc0":"<p style = \"font-size : 20px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong>Although we observe more false negative, the overall accuracy is much better, hence so far this is our best model. Let's try with Logistic Regression now!<\/strong><\/p>\n\n","4ce1ba92":"<p style = \"font-size : 20px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong> This confusion matrix above confirms that this new model is slightly better (and its accuracy score is 94.4 %). However, too many fake news are still labeled as true news. Let's try with another model called PassiveAgressive Classifier which is special for text classification purposes.<\/strong><\/p>\n","68fd5405":"<img style=\"float: center;  border:5px solid #DC143C; width:75%\" src = https:\/\/miro.medium.com\/max\/1320\/1*VIEVy548fut8IJotJlQKSQ.jpeg>","82366b3c":"<p style = \"font-size : 30px; color : #4e8d7c ; font-family : 'Comic Sans MS';  \"><strong> 2. Multinomial Naive Bayes with TF-IDF Vectorizer <\/strong><\/p> \n ","c59a380a":"<p style = \"font-size : 35px; color :#000080 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #808080; border-radius: 5px 5px;\"><strong> Using Suitable ML models with Count Vectorizer and TF-IDF Vectorizer<\/strong><\/p>\n","a81d6080":"<p style = \"font-size : 30px; color : #4e8d7c ; font-family : 'Comic Sans MS';  \"><strong>1. Count Vectorizer <\/strong><\/p> \n\n<p style = \"font-size : 20px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong>CountVectorizer is used to convert a collection of text documents to a vector of term\/token counts. It also enables the \u200bpre-processing of text data prior to generating the vector representation. This functionality makes it a highly flexible feature representation module for text.<\/strong><\/p>\n\n![vectorchart.PNG](https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--qveZ_g7d--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/raw.githubusercontent.com\/cassieview\/intro-nlp-wine-reviews\/master\/imgs\/vectorchart.PNG)","4efeb49e":"<p style = \"font-size : 35px; color :#000080 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #808080; border-radius: 5px 5px;\"><strong>Vectorizing our Data<\/strong><\/p>\n\n<p style = \"font-size : 20px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong>Word Embeddings or Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities\/semantics. The process of converting words into numbers are called Vectorization.<\/strong><\/p>\n\n![0*qTIL0VGKFsQy6TtA](https:\/\/miro.medium.com\/max\/1536\/0*qTIL0VGKFsQy6TtA)","4665d948":"<p style = \"font-size : 35px; color :#000080 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #808080; border-radius: 5px 5px;\"><strong>Loading Dataset<\/strong><\/p>","ba6c6d8a":"<p style = \"font-size : 30px; color : #4e8d7c ; font-family : 'Comic Sans MS';  \"><strong>   6. Logistic Regression with CountVectorizer <\/strong><\/p> \n","1529a1a0":"<p style = \"font-size : 30px; color : #4e8d7c ; font-family : 'Comic Sans MS';  \"><strong>  4. Passive Agressive Classifier With TF-IDF Vectorizer <\/strong><\/p> \n\n","a2606487":"<p style = \"font-size : 35px; color :#000080 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #808080; border-radius: 5px 5px;\"><strong>Importing Libraries<\/strong><\/p>","fbb81841":"<p style = \"font-size : 20px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong>We get much better results than with the MultinomialNB model, both in terms of accuracy and in terms of false negative. Only 60 fake news were labeled as true news this time. Let's try with the Tf-IDF method. <\/strong><\/p>\n\n","95fc85c5":"<p style = \"font-size : 30px; color : #4e8d7c ; font-family : 'Comic Sans MS';  \"><strong>Handle missing values :-<\/strong><\/p>","4a9dbcdf":"<p style = \"font-size : 30px; color : #4e8d7c ; font-family : 'Comic Sans MS';  \"><strong> 2. Tf-IDF Vectorizer <\/strong><\/p> \n\n<p style = \"font-size : 20px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong>TF-IDF is an abbreviation for Term Frequency Inverse Document Frequency. This is very common algorithm to transform text into a meaningful representation of numbers which is used to fit machine algorithm for prediction.<\/strong><\/p>\n\n\n\n![1*qQgnyPLDIkUmeZKN2_ZWbQ.png](https:\/\/miro.medium.com\/max\/3604\/1*qQgnyPLDIkUmeZKN2_ZWbQ.png)","0e9f69d0":"<p style = \"font-size : 30px; color : #4e8d7c ; font-family : 'Comic Sans MS';  \"><strong>  5. Logistic Regression with TF-IDF Vectorizer <\/strong><\/p> \n\n","e0b3ef86":"<p style = \"font-size : 35px; color :#000080 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #808080; border-radius: 5px 5px;\"><strong>Introduction<\/strong><\/p>\n\n\n<p style = \"font-size : 25px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong><\/strong>In the following analysis, we will talk about how one can create an NLP to detect whether the news is real or fake. Nowadays, fake news has become a common trend. Even trusted media houses are known to spread fake news and are losing their credibility. So, how can we trust any news to be real or fake?<\/p>\n","ec7ce1f5":"<p style = \"font-size : 20px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong> We see that although our model has a general accuracy of 94.3 %, which is good, but it does not really score well in view of number of false negative. 223 fake news are classified as true news with this model, which is not pleasing to see. So we will try to use the Tf-IDF vectorizer on this same model to see if it performs better.<\/strong><\/p>\n","723d41df":"<p style = \"font-size : 20px; color : #800000 ; font-family : 'Comic Sans MS';  \"><strong>The best score is obtained for alpha = 0.15, and is equal to 0.94279.<\/strong><\/p>\n"}}