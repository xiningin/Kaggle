{"cell_type":{"2ae8b8ea":"code","32e60f2d":"code","a8645dd6":"code","7386daba":"code","6c035190":"code","ad648288":"code","5e9ccb32":"code","1f8c6dd3":"code","41e52274":"code","82aabbd6":"code","da7457f7":"code","9b90393f":"code","a9c72a72":"code","3b09b716":"code","e0678f61":"code","5e5f9073":"code","327bbf05":"code","b432719d":"code","6675f8d2":"code","e436fd89":"code","8f856a34":"code","11143904":"code","dd374eb8":"code","5dbc55e5":"code","0ceaadb9":"code","b11c0bc9":"code","0e7e7245":"code","1584f934":"code","9095b677":"code","dd552cf1":"code","14030962":"code","8423db5c":"code","1a8d93f0":"code","9d8fcd37":"code","a7dd0142":"markdown","97795ebd":"markdown","e18ddc4a":"markdown","b162f72e":"markdown","b5421905":"markdown","072ff204":"markdown","7dd4d9f0":"markdown","d2fa7d26":"markdown","ce0612cc":"markdown","f03a4713":"markdown","3dd1085b":"markdown","5b885cc0":"markdown","fba6c5bd":"markdown","f19be058":"markdown","890914f7":"markdown","6842bc0a":"markdown","de6d1578":"markdown","81a27f46":"markdown","1c376d23":"markdown","65067402":"markdown","b36afd6f":"markdown","18cc0f1e":"markdown","8bc00b06":"markdown","254ca5d7":"markdown","8b4ae05e":"markdown","f8121958":"markdown","410a9c64":"markdown","8c963f1f":"markdown","a62c9fda":"markdown","fec217e3":"markdown"},"source":{"2ae8b8ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","32e60f2d":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","a8645dd6":"import os\n#os.listdir('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')","7386daba":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head(3)","6c035190":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","ad648288":"x=tweet.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","5e9ccb32":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","1f8c6dd3":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","41e52274":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","82aabbd6":"def create_corpus(target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","da7457f7":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","9b90393f":"x,y=zip(*top)\nplt.bar(x,y)","a9c72a72":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx,y=zip(*top)\nplt.bar(x,y)","3b09b716":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","e0678f61":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","5e5f9073":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","327bbf05":"sns.barplot(x=y,y=x)","b432719d":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","6675f8d2":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","e436fd89":"df=pd.concat([tweet,test])\ndf.shape","8f856a34":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","11143904":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","dd374eb8":"df['text']=df['text'].apply(lambda x : remove_URL(x))","5dbc55e5":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","0ceaadb9":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","b11c0bc9":"df['text']=df['text'].apply(lambda x : remove_html(x))","0e7e7245":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","1584f934":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","9095b677":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","dd552cf1":"df['text']=df['text'].apply(lambda x : remove_punct(x))","14030962":"!pip install pyspellchecker","8423db5c":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","1a8d93f0":"#df['text']=df['text'].apply(lambda x : correct_spellings(x)#)","9d8fcd37":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))\n\n# create a link to download the dataframe\ncreate_download_link(df)\n\n# \u2193 \u2193 \u2193  Yay, download link! \u2193 \u2193 \u2193","a7dd0142":"As we know,twitter tweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.So let's start.","97795ebd":"we will do a bigram (n=2) analysis over the tweets.Let's check the most common bigrams in tweets.","e18ddc4a":"We will need lot of cleaning here..","b162f72e":"Now,we will move on to class 0.","b5421905":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.","072ff204":"Removing urls\u00b6","7dd4d9f0":"Spelling Correction","d2fa7d26":"First,we will do very basic analysis,that is character level,word level and sentence level analysis.","ce0612cc":"First we will analyze tweets with class 0.","f03a4713":"Romoving Emojis\u00b6","3dd1085b":"Lot of cleaning needed !","5b885cc0":"Common stopwords in tweets\u00b6","fba6c5bd":"Removing punctuations\u00b6","f19be058":"GloVe for Vectorization\u00b6","890914f7":"Common words ?\u00b6","6842bc0a":"Number of characters in tweets\u00b6","de6d1578":"Analyzing punctuations.\nFirst let's check tweets indicating real disaster.","81a27f46":"Even if I'm not good at spelling I can correct it with python :) I will use pyspellcheker to do that.","1c376d23":"Exploratory Data Analysis of tweets\u00b6","65067402":"Number of words in a tweet\u00b6","b36afd6f":"In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.","18cc0f1e":"Removing HTML tags\u00b6","8bc00b06":"Now,we will analyze tweets with class 1.","254ca5d7":"ohh,as expected ! There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)","8b4ae05e":"Importing required Libraries.","f8121958":"Ngram analysis\u00b6","410a9c64":"Data Cleaning\u00b6","8c963f1f":"Average word length in a tweet\u00b6","a62c9fda":"Loading the data and getting basic idea\u00b6","fec217e3":"Class distribution\nBefore we begin with anything else,let's check the class distribution.There are only two classes 0 and 1."}}