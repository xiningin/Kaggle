{"cell_type":{"8621977a":"code","77c29f06":"code","99f0f03b":"code","1f14c9b0":"code","fcc5525d":"code","8d5cd637":"code","8633a31f":"code","cfd4b163":"code","71c87ded":"code","629ee644":"code","06f2d649":"code","26a590b6":"code","99137dcc":"code","42f306fb":"code","4e644b03":"code","700161ac":"code","ec404288":"code","e4f209e2":"code","22ef05a3":"code","83dad9ef":"code","93dc0227":"code","4ee5a13e":"code","2dc30c09":"code","cb25e953":"code","77d1fdc3":"code","7a428774":"code","60ecd47a":"code","9060ff19":"code","9f16fea0":"code","d7b22f51":"code","430439b1":"code","b8aad679":"code","8b838728":"code","5b063c8f":"markdown","bfffc1d0":"markdown","41ec33f0":"markdown","887323b9":"markdown","589b7ee9":"markdown","7cb0b030":"markdown","987f5689":"markdown","dfe50d4c":"markdown","43b5a2bc":"markdown","b68251f6":"markdown","4a935171":"markdown","989745d2":"markdown","46a53804":"markdown","df0892b1":"markdown","0d5c1045":"markdown","3ed245fe":"markdown","52cfc46a":"markdown","5e75cbaa":"markdown","23a01825":"markdown","b11c8361":"markdown","d72329b0":"markdown","c195f3a2":"markdown","09663e78":"markdown","744e86aa":"markdown","aeac13f4":"markdown"},"source":{"8621977a":"import numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","77c29f06":"pd.set_option('display.max_columns', None) #display all the columns\ndf=pd.read_csv(\"\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\ndf.head()","99f0f03b":"df.info()","1f14c9b0":"df.describe()","fcc5525d":"df.isnull().sum()","8d5cd637":"#fill the null values in the dataset \ndf[\"salary\"]=df[\"salary\"].fillna(0)","8633a31f":"sns.countplot(x = \"gender\", hue = \"gender\" ,data = df)","cfd4b163":"sns.pairplot(df)","71c87ded":"categorical = [\"gender\",\"ssc_b\",\"hsc_b\",\"hsc_s\",\"degree_t\",\"workex\",\"specialisation\",\"status\"]\nplt.figure(figsize = (25, 20))\nplotnumber = 1\n\nfor col in categorical:\n    if plotnumber <= 9: \n        ax = plt.subplot(3, 3, plotnumber)\n        sns.countplot(df[col])\n        plt.xlabel(col, fontsize = 15)\n        \n    plotnumber += 1\nplt.tight_layout()\nplt.show()","629ee644":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor i in categorical:\n    df[i]=le.fit_transform(df[i]) ","06f2d649":"plt.figure(figsize = (20, 15))\nplotnumber = 1\n\nfor col in df.columns:\n    if plotnumber <= 8:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.boxplot(df[col])\n        plt.xlabel(col, fontsize = 15)\n    \n    plotnumber += 1\nplt.tight_layout()\nplt.show()","26a590b6":"import plotly.graph_objects as go\ncorr = df.corr()\ngraph = go.Figure()\ngraph.add_trace(go.Heatmap(z=corr.values, x=corr.index.values, y=corr.columns.values))\ngraph.show()","99137dcc":"import copy\ndf=df.drop(\"sl_no\",axis=1)\ndf_reg = copy.deepcopy(df)         \ndf_class = copy.deepcopy(df)","42f306fb":"df_class=df_class.drop(\"salary\",axis=1)","4e644b03":"df_reg","700161ac":"df_class","ec404288":"!pip install smogn","e4f209e2":"import smogn\ndf_reg_smogn = smogn.smoter(\n    data = df_reg,       #dataset \n    y = 'salary'         #label for the prediction ,i.e in our case salary\n)","22ef05a3":"sns.kdeplot(df_reg['salary'], label = \"Original\")           #inblue\nsns.kdeplot(df_reg_smogn['salary'], label = \"Modified\")     #in orange","83dad9ef":"X = df_reg_smogn.iloc[:,:-1].values\ny = df_reg_smogn.iloc[:,-1].values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n","93dc0227":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test) ","4ee5a13e":"!pip install lazypredict","2dc30c09":"from lazypredict.Supervised import LazyRegressor\nreg = LazyRegressor(ignore_warnings=True, custom_metric=None)     \nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)\nmodels","cb25e953":"from xgboost import XGBRegressor\nxgbr = XGBRegressor(verbosity=0) \nxgbr.fit(X_train,y_train)\n#bellow is the Adjusted R-Squared for the model\n1 - (1-xgbr.score(X_test, y_test))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1)","77d1fdc3":"X = df_class.iloc[:,:-1].values\ny = df_class.iloc[:,-1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\nprint(X_train.shape,X_test.shape)","7a428774":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","60ecd47a":"from lazypredict.Supervised import LazyClassifier\nclf = LazyClassifier(predictions=True)\nmodels, predictions = clf.fit(X_train, X_test, y_train,y_test)\nmodels","9060ff19":"X = df_class.iloc[:,:-1].values\ny = df_class.iloc[:,-1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\nprint(X_train.shape,X_test.shape)\n","9f16fea0":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=0)\nX_train, y_train = sm.fit_resample(X_train, y_train.ravel())\nprint(X_train.shape,X_test.shape)","d7b22f51":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","430439b1":"from lazypredict.Supervised import LazyClassifier\nclf = LazyClassifier(predictions=True)\nmodels, predictions = clf.fit(X_train, X_test, y_train,y_test)\nmodels","b8aad679":"from sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import classification_report\nbc = BernoulliNB()\nbc.fit(X_train,y_train)\nprint(classification_report(y_test,bc.predict(X_test)))","8b838728":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import balanced_accuracy_score\nli = np.arange(0.0, 10.0, 0.1)    \nli_ = [i for i in range(10)]\n\n\nparams = {'alpha': li ,\n         'binarize' : li_,\n         'fit_prior' : [True,False]\n         }\n\n\nbernoulli_nb_grid = GridSearchCV(BernoulliNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5 ,scoring = 'balanced_accuracy')\nbernoulli_nb_grid.fit(X_train,y_train)\n\n\n\nprint('Test Accuracy : %.3f'%bernoulli_nb_grid.best_estimator_.score(X_test, y_test))\n\n\nprint('Best Accuracy Through Grid Search : %.3f'%bernoulli_nb_grid.best_score_)\nprint('Best Parameters : ',bernoulli_nb_grid.best_params_)","5b063c8f":"We can see a small improvement in the balanced accuracy ","bfffc1d0":"As we can see that the XGBRegressor and the RandomForest model are the ones with the highest accuracy , this is only to be ecpected since both the models aren't affected by outliers ","41ec33f0":"Now that we have a clear picture about the meaning of each column, we can now unerstand what should be our end goal \n\nFor the above given data we can have a total of 3 tasks \n1. Given the data , predict wether a candidate will get placed \n2. Predict what kind of placement package will be offered to the candidate - > can be used for negotiation \n3. Predict wether the candidate will get placed if yes then go predict his salary \n","887323b9":"Calling the lazypredict again","589b7ee9":"1. Now that we have dealt with the imbalance issue we can move onto splitting the data and scaling it\n2. Since our dataset dosen't have many rows i have avoided dropping the rows with outliers in it\n3. the above is also the reason i have reduced the test_size","7cb0b030":"> ***Visulaizing the given data***","987f5689":"As we cam see that there are a couple of outliers in the dataset ,we can use the following ways to deal with them \n1. Choose a model which is not Affected by outliers \n2. Remove or transform the data -> we can't remove the data since we only have 215 rows","dfe50d4c":"I will try o keep updating the notebook as time goes on , Thanks for your time ","43b5a2bc":"Label Encoding the categorical columns ","b68251f6":"But first we will be dropping the unnessary columns present in the dataset and create two seperate dataset , one for regression and other for classification","4a935171":"Now to move onto classification","989745d2":"1. I will not be optimising any hyperparameters simply because Adjusted R-Squared value of .94 is pretty good\n2. However the data available is extremely limited so , the actual performance of the model is questionable","46a53804":"In notebooks it is not necessary to use the head functions to view the data\n1. The reason i split the dataframes is so that we can deal with the imbalence problem easily","df0892b1":"1. Lazypredict trains the data on all the default sklearn models and returns the metrics report as shown bellow ,it is useful when you have no idea on what model to use.\n2. However it only trains the base models , we can improve the accuracy of the models listed with hyper parameter optimization\n","0d5c1045":"However we have another issue with the given dataset , that is imbalenced data distribution\n1. To deal with this we will be using the smote and smogn packages \n2. SMOTE is used for classification\n3. SMOGN is used for regression ","3ed245fe":"Now that we have dealt with the imbalance problem for the regression dataset and aslo split it into training and testing, we will now move on to select a model ","52cfc46a":"Now we will check for any outliers in the dataset ","5e75cbaa":"Now lets try doing that again but this time using smote","23a01825":"We can see that salary has null values , this is because the candidate didn't get placed , so naturally their salary value will be empty \/ 0\n","b11c8361":"Now to improve the performance of the model with hyperparameter optimisation","d72329b0":"**Now to understand what each column means**\n1. sl_no -> Serial number for every row \n2. gender - > gender of the person , M for male , F for female\n3. ssc_p ->  10th Grade percentage \n4. ssc_b ->  10th Grade Board of Education - Central or Others \n5. hsc_p -> 12th Grade percentage\n6. hsc_b -> 12th Grade Board of Education- Central or Others\n7. hsc_s -> Specialization in Higher Secondary Education \n8. degree_p -> Degree Percentage\n9. degree_t -> Degree type \n10. workex -> Work Experience\n11. etest_p -> Employability test percentage\n12. specialisation -> Post Graduation(MBA)- Specialization\n13. mba_p -> MBA percentage\n14. status -> Status of placement\n15. salary -> Salary offered if placed ","c195f3a2":"Now to train the model present and optimize it","09663e78":"> Performing EDA to better understand the data","744e86aa":"There are a lot of models to select from , but remember that our data has outliers present in it \n1. This means we can't select models that are sensitive to outliers -> this means treebased models or XGB  model and a few others are viable\n2. Some models don,t require scaling so, it's better to avoid it whenever it's not necessary\n3. When confused to select a model simply call the lazypredict\n4. Lazypredict trains models which require scaling , so we will scale the data before calling it ","aeac13f4":"Scaling the data"}}