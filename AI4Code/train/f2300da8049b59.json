{"cell_type":{"bad85cd9":"code","b9c1fcd0":"code","f85871b7":"code","6bccc9d7":"code","54965916":"code","1f447759":"code","ce2b3b3b":"markdown","c47e2c15":"markdown","81ae40c9":"markdown"},"source":{"bad85cd9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9c1fcd0":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","f85871b7":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","6bccc9d7":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n","54965916":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","1f447759":"from sklearn.ensemble import RandomForestClassifier\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","ce2b3b3b":"first machine learning model\nWe'll build what's known as a random forest model. This model is constructed of several \"trees\" (there are three trees in the picture below, but we'll construct 100!) that will individually consider each passenger's data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in four different columns (\"Pclass\", \"Sex\", \"SibSp\", and \"Parch\") of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file my_submission.csv.\n\n","c47e2c15":"Our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.\nThe sample submission file in gender_submission.csv assumes that all female passengers survived (and all male passengers died).\n\nIs this a reasonable first guess? We'll check if this pattern holds true in the data (in train.csv).","81ae40c9":"The code above calculates the percentage of male passengers (in train.csv) who survived.\n\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender_submission.csv is not a bad first guess, and it makes sense that it performed reasonably well!\n\nBut at the end of the day, this gender-based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we'll use machine learning to automate this for us."}}