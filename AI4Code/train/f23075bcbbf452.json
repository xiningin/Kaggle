{"cell_type":{"d1ff60e5":"code","62cc743e":"code","4bb0a282":"code","52fd59c2":"code","823f10ff":"code","1a5c5b1f":"code","34351273":"code","27c9a4a1":"code","b1ad6ab6":"code","9e3e7eb4":"code","99947593":"code","af2a1b43":"code","4e0670a8":"code","053ef456":"code","43b20a7c":"code","4c94a7b9":"code","f16a6436":"code","cf4b6687":"code","eb8fabe0":"code","33a3c2b5":"code","67bd99a0":"code","f30b9798":"code","5668ef96":"code","9e932177":"code","dec01c00":"code","58140ca2":"code","248f92e8":"code","3a53b6d7":"markdown","37aecf8b":"markdown","37091a41":"markdown","0b9503c0":"markdown","f8e77590":"markdown","948f5d20":"markdown","30eabe60":"markdown"},"source":{"d1ff60e5":"import plotly.offline as pyo\npyo.init_notebook_mode()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2 \nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom sklearn import preprocessing\nimport random\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n!pip install visualkeras","62cc743e":"def Create_Directory_DataFrame():\n    df =pd.DataFrame(columns=['Class','Location'])\n    basedir = '..\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/'\n    for Dir in os.listdir(basedir):\n        for Class in os.listdir(basedir+Dir):\n            for location in os.listdir(basedir+Dir+'\/'+Class+'\/'):\n                df = df.append({'Class':Class,'Location':basedir+Dir+'\/'+Class+'\/'+location},ignore_index=True)\n    df = df.sample(frac = 1) \n    return df\ndf = Create_Directory_DataFrame()\nprint(df.shape)\ndf.head()","4bb0a282":"count = 1\nf = plt.figure(figsize=(50,13))\nfor Class in df['Class'].unique():\n    seg = df[df['Class']==Class]\n    address =  seg.sample().iloc[0]['Location']\n    img = cv2.imread(address,0)\n    ax = f.add_subplot(2, 5,count)\n    ax = plt.imshow(img)\n    ax = plt.title(Class,fontsize= 30)\n    count = count + 1\nplt.suptitle(\"Alzymer Classification\", size = 32)\nplt.show()","52fd59c2":"w , h= 32,32\nfinal_class = 4","823f10ff":"from tqdm import tqdm\nfrom sklearn.preprocessing import OneHotEncoder\ntrain_image = []\nfor location in tqdm(df.iloc[:]['Location']):\n    img = cv2.imread(location,0)\n    img = cv2.resize(img, (w,h), interpolation = cv2.INTER_AREA)\n    img = img.reshape(w,h,1)\n    train_image.append(img)\nX = np.array(train_image)\ny = np.array(df.iloc[:]['Class'])\ny = y.reshape(y.shape[0],1)\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(y)\nprint(enc.categories_)\ny = enc.transform(y).toarray()\nprint('Data   :   '+str(X.shape))\nprint('Output :   '+str(y.shape))","1a5c5b1f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\nprint('Train data    :'+str(X_train.shape))\nprint('Test data     :'+str(X_test.shape))\nprint('Train Output  :'+str(y_train.shape))\nprint('Test Output   :'+str(y_test.shape))","34351273":"def conv_block(filters):\n    block = tf.keras.Sequential([\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPool2D()\n    ]\n    )\n    return block\ndef dense_block(units, dropout_rate):\n    block = tf.keras.Sequential([\n        tf.keras.layers.Dense(units, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(dropout_rate)\n    ])\n    return block\ndef build_model(act , final_class , w , h ):\n    model = tf.keras.Sequential([\n        tf.keras.Input(shape=(w , h , 1)),\n        \n        tf.keras.layers.Conv2D(16, 3, activation=act, padding='same'),\n        tf.keras.layers.Conv2D(16, 3, activation=act, padding='same'),\n        tf.keras.layers.MaxPool2D(),\n        \n        conv_block(32),\n        conv_block(64),\n        \n        conv_block(128),\n        tf.keras.layers.Dropout(0.2),\n        \n        conv_block(256),\n        tf.keras.layers.Dropout(0.2),\n        \n        tf.keras.layers.Flatten(),\n        dense_block(512, 0.7),\n        dense_block(128, 0.5),\n        dense_block(64, 0.3),\n        \n        tf.keras.layers.Dense(final_class, activation='sigmoid')\n    ])\n    return model\ndef wrap(Training_Output_Results , Opt , Act ,  history):\n    epoch  = len(history.history['loss'])\n    epochs = list(np.arange(1,epoch + 1,1))\n    Optimizer = np.repeat(Opt,epoch).tolist()\n    Activation = np.repeat(Act,epoch).tolist()\n    cumiliated_res = {}\n    cumiliated_res['Epochs']=epochs\n    cumiliated_res['Optimizer']=Optimizer\n    cumiliated_res['Activation_Function']=Activation\n    cumiliated_res['Train_Loss']=history.history['loss']\n    cumiliated_res['Train_Accuracy']=history.history['accuracy']\n    cumiliated_res['Train_Recall']=history.history['recall']\n    cumiliated_res['Val_Loss']=history.history['val_loss']\n    cumiliated_res['Val_Accuracy']=history.history['val_accuracy']\n    cumiliated_res['Val_Recall']=history.history['val_recall']\n    convertDictionary = pd.DataFrame(cumiliated_res)\n    Training_Output_Results = Training_Output_Results.append(convertDictionary)\n    return Training_Output_Results","27c9a4a1":"Optimisers = ['Adam','Rmsprop','Adagrad']\nTraining_Output_Results =pd.DataFrame(columns=['Epochs','Optimizer','Activation_Function','Train_Loss','Train_Accuracy','Train_Recall',                                             'Val_Loss','Val_Accuracy','Val_Precision','Val_Recall'])\ndef Optimise_verify(Training_Output_Results):\n    for opt in Optimisers:\n        model = build_model('relu', final_class , w , h)\n        METRICS = [\n                'accuracy',\n                tf.keras.metrics.Recall(name='recall'),\n            \n        ]  \n        model.compile(\n                optimizer=opt,\n                loss='categorical_crossentropy',\n                metrics=METRICS\n            )\n        history = model.fit(X_train, y_train, epochs=25, validation_split=0.3, batch_size=15,verbose=1,shuffle=True)\n        Training_Output_Results = wrap(Training_Output_Results , opt,'relu',history)\n        print('---------------------Round for '+opt+' Completed-----------------------------------------')\n    return Training_Output_Results\n    \n    \nTraining_Output_Results = Optimise_verify(Training_Output_Results)","b1ad6ab6":"Training_Output_Results=Training_Output_Results.sample(frac = 1) \nprint(Training_Output_Results.shape)\nTraining_Output_Results.to_csv('Optimizer_64*64_data.csv', index = False) \nTraining_Output_Results.tail()","9e3e7eb4":"opt = pd.read_csv('.\/Optimizer_64*64_data.csv')","99947593":"import plotly.express as px\nimport plotly.io as pio\nimport statsmodels.api as sm\nscatterplot = px.scatter(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Train_Accuracy\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"black\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Accuracy',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\npyo.iplot(scatterplot, filename = 'Opt_train_acc')","af2a1b43":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Train_Loss\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Train Loss',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_train_loss')","4e0670a8":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Accuracy\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Accuracy',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_acc')","053ef456":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Recall\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Recall',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_recall')\n","43b20a7c":"scatterplot = px.area(\n    data_frame=opt,\n    x=\"Epochs\",\n    y=\"Val_Loss\",\n    color=\"Optimizer\",                                              # set opacity of markers\n    color_discrete_sequence=[\"red\",\"orange\",\"blue\",\"green\"],   # set marker colors. When color colum isn't numeric data\n   \n    #facet_col='Optimizer',       # assign marks to subplots in the horizontal direction\n    #facet_col_wrap=2,           # maximum number of subplot columns. Do not set facet_row\n    \n    #log_x=True,                 # x-axis is log-scaled\n    #log_y=True,                 # y-axis is log-scaled\n    \n    title='Validation Loss',           # figure title\n    #width=500,                  # figure width in pixels\n    #height=500,                # igure height in pixels\n    template='presentation',     # 'ggplot2', 'seaborn', 'simple_white', 'plotly',\n                                # 'plotly_white', 'plotly_dark', 'presentation',\n                                # 'xgridoff', 'ygridoff', 'gridon', 'none'\n)\n\n# print(scatterplot)\n\npyo.iplot(scatterplot, filename = 'Opt_val_loss')","4c94a7b9":"import plotly.graph_objects as go\ntab_opt = opt[opt['Epochs']==20]\nfinal_col = np.delete(tab_opt.columns[0:], [0,2])\nfig = go.Figure(data=[go.Table(\n    header=dict(values=list(final_col),\n                fill_color='paleturquoise',\n                align='left'),\n    cells=dict(values=[tab_opt.Optimizer , tab_opt.Train_Loss,tab_opt.Train_Accuracy,tab_opt.Train_Recall,tab_opt.Val_Loss,tab_opt.Val_Accuracy,tab_opt.Val_Recall],\n               fill_color='lavender',\n               align='left'))\n])\n\nfig.show()","f16a6436":"import plotly.graph_objects as go\nty =opt[opt['Epochs']==25].iloc[:,3:]\nnm = ty.columns\nty = ty.values.tolist()\ndata = []\n\nfor j in range(len(nm)):\n        lt = []\n        for i in range(len(Optimisers)):\n            lt.append(ty[i][j])\n            \n        data.append(go.Bar(name = nm[j],x=Optimisers, y=lt))\nfig = go.Figure(data=data)\n# Change the bar mode\nfig.update_layout(barmode='group')\nfig.show()","cf4b6687":"def wrap(Training_Output_Results , lr ,  history):\n    epoch  = len(history.history['loss'])\n    epochs = list(np.arange(1,epoch + 1,1))\n    Optimizer = np.repeat(lr,epoch).tolist()\n    cumiliated_res = {}\n    cumiliated_res['Epochs']=epochs\n    cumiliated_res['Learning Rate']=Optimizer\n    cumiliated_res['Train_Loss']=history.history['loss']\n    cumiliated_res['Train_Accuracy']=history.history['accuracy']\n    cumiliated_res['Val_Loss']=history.history['val_loss']\n    cumiliated_res['Val_Accuracy']=history.history['val_accuracy']\n    convertDictionary = pd.DataFrame(cumiliated_res)\n    Training_Output_Results = Training_Output_Results.append(convertDictionary)\n    return Training_Output_Results","eb8fabe0":"Training_Output_Results =pd.DataFrame(columns=['Epochs','Learning Rate','Train_Loss','Train_Accuracy','Train_Precision','Val_Loss','Val_Accuracy','Val_Precision'])\ndef LR_verify(Training_Output_Results):\n        model = build_model('relu', final_class , w , h)\n        METRICS = [\n                'accuracy'\n        ]  \n        model.compile(\n                optimizer='Adam',\n                loss='categorical_crossentropy',\n                metrics=METRICS\n            )\n        history = model.fit(X_train, y_train, epochs=100, validation_split=0.3, batch_size=20,verbose=1,shuffle=True)\n        Training_Output_Results = wrap(Training_Output_Results , 0.00002548,history)\n        return Training_Output_Results,model,history\n    \n    \nTraining_Output_Results ,model,history= LR_verify(Training_Output_Results)","33a3c2b5":"model.save('Alz.h5')","67bd99a0":"Training_Output_Results.tail()","f30b9798":"def Plot(history , name , model):\n    model.save(name+'.h5')\n    epochs = range(1,len(history.history['loss']) + 1)\n    epochs = list(epochs)\n    fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Train Loss\", \"Train Accuracy\" , \"Train Precision\",\"Train Recall\", \"Validation Loss\", \"Validation Accuracy\",\n                                                      \"Validation Precision\",\"Validation Recall\"))\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['loss']), row=1, col=1)\n    fig.add_trace(go.Scatter(x=epochs, y=history.history['accuracy']), row=1, col=2)\n    fig.update_layout(showlegend=False,height=1000, width=1200, title_text=name)\n    pyo.iplot(fig, filename = 'Act_train_rec')","5668ef96":"import visualkeras\nvisualkeras.layered_view(model)","9e932177":"from keras.utils import plot_model\nplot_model(model, to_file='model.png',show_shapes=True)","dec01c00":"Plot(history , 'final_model',model)","58140ca2":"from keras.models import Model\nimport matplotlib.pyplot as pyplot\nfrom matplotlib.pyplot import figure\nfrom numpy import expand_dims\ndef image_transform_gray(image):\n    plt.figure(figsize=(25,8))\n    plt.imshow(image.reshape(w,h))\n    plt.title(enc.inverse_transform(y[0].reshape(1,final_class))[0][0],size = 20)\n    plt.show()\n    img = expand_dims(image, axis=0)\n    model1 = Model(inputs=model.inputs, outputs=model.layers[0].output)\n    feature_maps = model1.predict(img)\n    figure(num=None, figsize=(25, 30), dpi=80, facecolor='w', edgecolor='k')\n    square = 4\n    ix = 1\n    for _ in range(square):\n        for _ in range(square):\n            # specify subplot and turn of axis\n            ax = pyplot.subplot(square, square, ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            # plot filter channel in grayscale\n            pyplot.imshow(feature_maps[0, :, :, ix-1],cmap='gray')\n            ix += 1\n    # show the figure\n    pyplot.show()\nimage_transform_gray(X[89])","248f92e8":"import plotly.graph_objects as go\nfrom sklearn.metrics import classification_report\ndef binary_classify(y_pred):\n    for inp in y_pred:\n        maximum = 0\n        index = 0\n        for i in range(final_class):\n            if(maximum != max(maximum,inp[i])):\n                maximum = max(maximum,inp[i])\n                index = i\n            inp[i] = 0\n        inp[index]=1\n    return y_pred\ndef create_result(y):\n    y_final = []\n    for i in range(y.shape[0]):\n        y_final.append(enc.inverse_transform(y[i].reshape(1,final_class))[0][0])\n    return y_final \ndef remove_none(y , y_pred):\n    index = []\n    for i in range(len(y)-1,0,-1):\n        if y_pred[i] == None :\n            del y[i]\n            del y_pred[i]\n        \n    return y , y_pred\ndef label_encode(y , y_pred):\n    le = preprocessing.LabelEncoder()\n    le.fit(y_pred)\n    print('Classes   :    '+str(le.classes_))\n    y = le.transform(y)\n    y_pred = le.transform(y_pred)\n    return y , y_pred\ndef Test_Results_compiled(model,history, name =''):\n    print('Results '+name)\n    y_pred = model.evaluate(X_test , y_test,verbose =1)\n    index = len(history.history['loss']) - 1\n    fig = go.Figure(data=[\n        go.Bar(name = 'Accuracy',x=['Training','Validation','Real World Data'], y=[history.history['accuracy'][index] ,history.history['val_accuracy'][index],y_pred[1] ]),\n        go.Bar(name = 'Loss',x=['Training','Validation','Real World Data'], y=[history.history['loss'][index] ,history.history['val_loss'][index],y_pred[0] ]),\n\n    ])\n    fig.update_layout(barmode='group')\n    fig.update_yaxes(type = \"log\")\n    pyo.iplot(fig, filename = 'Act_train_rec')\n\nTest_Results_compiled(model,history,'CNN Based Model')","3a53b6d7":"# Final Model\n","37aecf8b":"# Segmentation in Traing and Test Data Sets\nTraining : 80%\n\nTest : 20%\n\nValidation : 30%","37091a41":"# Preprocessing","0b9503c0":"# Choice of Optimizer to minimize the loss","f8e77590":"# Create A Stacked Numpy Array\n\nI Brief Idea for Image Loading\n\nRead Images\nResize them\nAdd to a list\nConvert the list to np array\n\n\nII Output array\n\nRead the classes\nreshape to 1 D array\nOne hot encode the same","948f5d20":"# Model\nTo make our model more modular and easier to understand, let's define some blocks. As we're building a convolution neural network, we'll create a convolution block and a dense layer block. The following method will define the function to build our model for us. The Dropout layers are important as they \"drop out,\" hence the name, certain nodes to reduce the likelikhood of the model overfitting. We want to end the model with a Dense layer of one node For our metrics, we want to include precision and recall as they will provide use with a more informed picture of how good our model is. Accuracy tells us what fractions are the labels are correct. \n\nPrecision is the number of true positives (TP) over the sum of TP and false positives (FP). It shows what fraction of labeled positives are actually correct.\n\nRecall is the number of TP over the sum of TP and false negatves (FN). It shows what fraction of actual positives are correct.","30eabe60":"# Ploting"}}