{"cell_type":{"08ea6c5d":"code","b7e99f32":"code","0fbc93de":"code","4453375c":"code","d4d0a602":"code","9fdabddb":"code","0f265da0":"code","88f9c4e6":"code","87560a2b":"code","c9e594a7":"code","7ca6815d":"code","483f547b":"code","d7190879":"code","8be0fe26":"code","4eacee16":"code","dedee053":"markdown"},"source":{"08ea6c5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport random\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import mean_squared_error, roc_auc_score \nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, train_test_split\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 30)\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=UserWarning)","b7e99f32":"%%time\ntrain = pd.read_feather('..\/input\/oct21tp-feathercreator\/Oct21TP_train.feather')\n\ntest = pd.read_feather('..\/input\/oct21tp-feathercreator\/Oct21TP_test.feather')\n\nTARGET = \"target\"","0fbc93de":"train = train.set_index(\"id\")\ntest = test.set_index(\"id\")\ntarget = train[TARGET]","4453375c":"'''\nDescription: Generate new feature by several statistic methods\nArgs:\n    dataset: The chosen dataset\n    numerical_features: The numerical features in a list\n    categorical_features: The categorical features in a list\nReturn: None\n'''\ntrain_head = train.head(2)\ntest_head = test.head(2)\n\n# quite a lot\nbool_features = list((train_head.drop(columns = TARGET).T)[train_head.dtypes == \"bool\"].index)\nnumeric_features = list(train_head.drop(columns = TARGET).drop(columns = bool_features).T.index)\n\ndef feature_generator(dataset, numerical_features, categorical_features):\n    # Numerical feature\n    dataset['n_min'] = dataset[numerical_features].min(axis=1)\n    print(\"min done\")\n    dataset['n_max'] = dataset[numerical_features].max(axis=1)\n    print(\"max done\")\n    dataset['n_std'] = dataset[numerical_features].std(axis=1)\n    print(\"std done\")\n    dataset['n_mean'] = dataset[numerical_features].mean(axis=1)\n    print(\"mean done\")\n    # Categorical feature\n    dataset['c_sum'] = dataset[categorical_features].sum(axis=1)\n    print(\"cat sum done\")\n#     dataset['c_mode'] = dataset[categorical_features].mode(axis=1)\n#     print(\"mode done\")\n    \n    # Generate new feature by several statistic methods\nnew_features = ['n_min', 'n_max', 'n_std', 'n_mean', 'c_sum', 'c_mode']\nfeature_generator(train, numeric_features, bool_features)\nfeature_generator(test, numeric_features, bool_features)","d4d0a602":"# Holdout set of 50% for Ensamble Meta Train\nRANDOM_SEED = 42\n\ntrain, holdout = train_test_split(\n    train,\n    test_size = 0.25,\n    shuffle = True,\n    stratify = train[TARGET],\n    random_state = RANDOM_SEED,\n)\n\ntarget = train[TARGET].astype('int')\noof_target = holdout[TARGET].astype('int')\n\ntrain = train.drop(columns=TARGET)\nholdout = holdout.drop(columns=TARGET)\n","9fdabddb":"oof_df = pd.DataFrame()\noof_df[\"id\"] = holdout.index\n\ntest_df = pd.DataFrame()\ntest_df[\"id\"] = test.index\n\nxgb_params = {\n    \"random_state\": 0,\n    \"n_estimators\": 8000,\n    \"learning_rate\":0.008,\n    \"eval_metric\": \"auc\",\n    \"objective\":\"binary:logistic\",\n    \"use_label_encoder\": False,\n    \"booster\": \"gbtree\",\n    # GPU\n    \"gpu_id\": 0,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\"\n}\n\nxgbc = XGBClassifier(**xgb_params)\nxgbc.fit(train, target, verbose=False)\nprint(\"fitted\")","0f265da0":"# xgbc.evals_result() #Return the evaluation results of eval_sets\noof_df[\"xgb\"] = xgbc.predict_proba(holdout)[:,1]\nauc_xgbc = roc_auc_score(oof_target, oof_df[\"xgb\"])\ntest_df[\"xgb\"] = xgbc.predict_proba(test)[:,1]\nprint(f'AUC: {auc_xgbc}')","88f9c4e6":"catb_params = {\n    \"random_seed\": 0,\n    \"iterations\": 8000,\n    \"learning_rate\":0.008,\n    \"eval_metric\" : \"AUC\",\n    \"verbose\": 0,\n    # GPU\n    \"task_type\" : \"GPU\",\n    \"devices\" : \"0\",\n}\n\ncatbc = CatBoostClassifier(**catb_params)\ncatbc.fit(train, target, verbose=False)\nprint(\"fitted\")","87560a2b":"oof_df[\"ctb\"] = catbc.predict_proba(holdout)[:,1]\nauc_catbc = roc_auc_score(oof_target, oof_df[\"ctb\"])\ntest_df[\"ctb\"] = catbc.predict_proba(test)[:,1]\nprint(f'AUC: {auc_catbc}')","c9e594a7":"\nlgbc_params = {\n    \"n_estimators\":8000, \n    \"learning_rate\":0.008, \n    \"objective\":'binary',                      \n    \"metric\":'auc',                       \n    \"reg_alpha\":10,\n    \"reg_lambda\":0.1,                     \n    \"num_leaves\":31,\n    \"max_depth\":-1,\n    \"subsample\":0.6,\n    \"subsample_freq\":1, \n    \"colsample_bytree\":0.4,\n    \"min_child_weight\":256,\n    \"min_child_samples\":20, \n    \"random_state\":0,\n    # GPU\n    \"device\": \"gpu\"\n}\n\nlgbc = LGBMClassifier(**lgbc_params)\n\nlgbc.fit(train, target, eval_metric='auc', verbose=-1)\nprint(\"fitted\")\n\noof_df[\"lgb\"] = lgbc.predict_proba(holdout)[:,1]\nauc_lgbc = roc_auc_score(oof_target, oof_df[\"lgb\"])\n\ntest_df[\"lgb\"] = catbc.predict_proba(test)[:,1]\nprint(f'AUC: {auc_lgbc}')","7ca6815d":"import optimizeauc as oa","483f547b":"opt = oa.OptimizeAUC()","d7190879":"opt.fit(oof_df.drop(columns=\"id\"), oof_target)","8be0fe26":"predictions = opt.predict(test_df.drop(columns=\"id\"))\n","4eacee16":"submissions = pd.DataFrame()\nsubmissions[\"id\"] = test.index\nsubmissions[\"target\"] = list(predictions)\n\nsubmissions.to_csv('submission.csv', index=False, header=submissions.columns)\nsubmissions.head()","dedee053":"# Libraries and Data import"}}