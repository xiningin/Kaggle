{"cell_type":{"671232cd":"code","078de9b8":"code","a5bf0f3d":"code","af90e3e9":"code","a3fb57fa":"code","e1151130":"code","b398da31":"code","feae980d":"code","999d26ca":"code","7c358247":"code","d0c66d57":"code","be1161c5":"code","d29cec4f":"code","0e8735b7":"code","8ed9f10e":"code","ad6015f7":"code","9f5007eb":"code","cb9bc652":"code","a9be7c63":"code","01297a92":"code","87a24971":"code","4afc99cd":"code","15962c03":"code","38d0828c":"code","0e9d88e1":"code","5ac3c5ce":"code","56c795cb":"code","05377b12":"code","ccda489c":"code","ba302ad5":"code","ca77b326":"code","371b1f0b":"code","999f0cee":"code","c1d3c4b6":"code","f26d2001":"code","e34d3640":"code","1ecebb65":"code","bbee60d6":"code","e03b57c7":"code","df217132":"code","d4038c81":"code","07662392":"code","fc8e6a9e":"code","884a0f89":"code","703c2958":"code","a65e7581":"code","f74508cc":"code","0fc0ec02":"code","01358923":"code","62f5bdb3":"markdown","2db24bfb":"markdown","08a98b48":"markdown","684b345b":"markdown","d0abd4e9":"markdown","f8c4d64d":"markdown","30808c89":"markdown","384b1dcb":"markdown","a132c916":"markdown","dd666ae9":"markdown","2214c555":"markdown","154aa3d1":"markdown","5e3ec77a":"markdown","af7e6835":"markdown","b987ff5f":"markdown","8d70bfe9":"markdown","f1f08cf7":"markdown","8b6ed388":"markdown","fecb447c":"markdown","6b5ab7f9":"markdown","b79e977a":"markdown","b6b5fd1d":"markdown","feaa6a67":"markdown","f3431f60":"markdown","b4007547":"markdown","cab6da03":"markdown","c65162f3":"markdown"},"source":{"671232cd":"#making the imports\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')\n","078de9b8":"#reading the csv file provided\ndata = pd.read_csv('..\/input\/creditcard.csv')","a5bf0f3d":"#checking the head\ndata.head()","af90e3e9":"#having a look at the column data types \ndata.info()","a3fb57fa":"#checking for nulls.\n\ndata.isnull().sum()","e1151130":"#checking the target variable distribution\nplt.figure(figsize = (8,6))\nsns.set_style('dark')\nsns.countplot(data['Class'])\nplt.show()","b398da31":"plt.figure(figsize = (8,6))\nsns.countplot(data['Class'])\nplt.yscale('log')\nplt.show()","feae980d":"#dataset histogram\ndata.hist(figsize = (20,20))\nplt.show()","999d26ca":"#checking the distribution of transaction amount and time\n\nplt.figure(figsize = (16,6))\nplt.subplot(1,2,1)\nsns.distplot(data['Amount'], color='r', kde= False)\nplt.title('Distribution of Transaction Amount', fontsize=16)\nplt.yscale('log')\n\nplt.subplot(1,2,2)\nsns.distplot(data['Time'], color='b', kde= False)\nplt.title('Distribution of Transaction Time', fontsize=16)\nplt.yscale('log')\n\nplt.show()\n","7c358247":"#checking the percentage of fraud transactions\n\nfraud = data[data['Class'] == 1]\nvalid = data[data['Class'] == 0]\n\nfraud_ratio = len(fraud)\/ float(len(valid) + len(fraud))\n\nprint('The number of Fraudulent cases is: {} \\n'.format(len(fraud)))\nprint('The number of valid transactions is: {} \\n'.format(len(valid)))\nprint('The ratio of fraudulent transactions is: {}'.format(fraud_ratio))","d0c66d57":"#heat map of data to see correlation\n\nplt.figure(figsize = (14,10))\nsns.heatmap(data.corr(), cmap= 'coolwarm')\nplt.show()","be1161c5":"#lets divide the data into X and y\n\ncolumns = data.columns.tolist()\n\ncols = [c for c in columns if c not in ['Class']]\ntarget = 'Class'\n\nX = data[cols]\ny = data[target]\n\nprint(X.shape)\nprint(y.shape)","d29cec4f":"#scale the data\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nscaled_X = scaler.fit_transform(X)","0e8735b7":"#convert target variable to numpy array\ny = y.as_matrix()","8ed9f10e":"#print the shapes of X and y\nprint(scaled_X.shape)\nprint('\\n')\nprint(y.shape)","ad6015f7":"#make the imports\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report","9f5007eb":"#doing the train test split (20% test data)\nX_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.20, random_state=101)","cb9bc652":"#fit the data to default Ramdom Forest classifier.\nrfc = RandomForestClassifier()\nrfc.fit(X_train,y_train)","a9be7c63":"#get the predictions\npred_Random_Forest = rfc.predict(X_test)","01297a92":"#function for plotting confusion matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.coolwarm):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize = 20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0, fontsize = 20)\n    plt.yticks(tick_marks, classes, fontsize = 20)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        1#print('Confusion matrix, without normalization')\n        #print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\",\n                 fontsize=25)\n\n    plt.tight_layout()\n    plt.ylabel('True label', fontsize = 20)\n    plt.xlabel('Predicted label', fontsize = 20)","87a24971":"#confusion matrix\ncnf_matrix = confusion_matrix(y_test,pred_Random_Forest)\nnp.set_printoptions(precision=2)","4afc99cd":"#print the precision, recall , accuracy and confusion matrix\n\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score\n\nprint('Precision Score: {}\\n'.format(precision_score(y_test,pred_Random_Forest)))\nprint('Recall Score: {}\\n'.format(recall_score(y_test,pred_Random_Forest)))\nprint('Accuracy Score: {}\\n'.format(accuracy_score(y_test,pred_Random_Forest)))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure(figsize = (8,6))\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='CONF MATRIX')\nplt.show()","15962c03":"# import and instantiate the grid search cv\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\nfold = KFold(n_splits= 3, shuffle= True, random_state= 42)\n\nparam_grid = {\n    'max_depth': [8,10,12],\n    'min_samples_leaf': range(100, 200, 300),\n    'min_samples_split': range(200, 300, 400),\n    'n_estimators': [50,100,200]\n}\n# Create a base model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = fold, n_jobs = -1,verbose = 1)","38d0828c":"#fit the grid (will take some time)\ngrid_search.fit(X_train, y_train)","0e9d88e1":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","5ac3c5ce":"#lets predict with these parameters\n\nrandom_final = RandomForestClassifier(bootstrap= True, \n                                      max_depth=12,\n                                      min_samples_leaf=100, \n                                      min_samples_split=200,\n                                      n_estimators=50)\n\nrandom_final.fit(X_train,y_train)","56c795cb":"#make predictions with this model\npred_3 = random_final.predict(X_test)","05377b12":"#confusion matrix\nconf_matt = confusion_matrix(y_test,pred_3)","ccda489c":"#print the precision, recall , accuracy and confusion matrix\n\nprint('Precision Score: {}\\n'.format(precision_score(y_test,pred_3)))\nprint('Recall Score: {}\\n'.format(recall_score(y_test,pred_3)))\nprint('Accuracy Score: {}\\n'.format(accuracy_score(y_test,pred_3)))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure(figsize = (8,6))\nplot_confusion_matrix(conf_matt\n                      , classes=class_names\n                      , title='CONF MATRIX')\nplt.show()","ba302ad5":"#lets use logistic regression model\n\nfrom sklearn.linear_model import LogisticRegression\n\n#after multiple runs below parameters seem to give the best restult. \n\nlog_reg = LogisticRegression(C = 0.01, penalty= 'l2')\n\nlog_reg.fit(X_train,y_train)\n\npred_log_reg = log_reg.predict(X_test)\n\nconf_mat = confusion_matrix(y_test,pred_log_reg)\n\n\n#print the precision, recall , accuracy and confusion matrix\n\nprint('Precision Score: {}\\n'.format(precision_score(y_test,pred_log_reg)))\nprint('Recall Score: {}\\n'.format(recall_score(y_test,pred_log_reg)))\nprint('Accuracy Score: {}\\n'.format(accuracy_score(y_test,pred_log_reg)))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure(figsize = (8,6))\nplot_confusion_matrix(conf_mat\n                      , classes=class_names\n                      , title='CONF MATRIX')\nplt.show()","ca77b326":"#making the imports\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance","371b1f0b":"#try xgboost with default parameters\n\nxgb_model = XGBClassifier()\n\nxgb_model.fit(X_train,y_train)\n\nxgb_pred = xgb_model.predict(X_test)\n\nconf_mat = confusion_matrix(y_test,xgb_pred)\n\n\n#print the precision, recall , accuracy and confusion matrix\n\nprint('Precision Score: {}\\n'.format(precision_score(y_test,xgb_pred)))\nprint('Recall Score: {}\\n'.format(recall_score(y_test,xgb_pred)))\nprint('Accuracy Score: {}\\n'.format(accuracy_score(y_test,xgb_pred)))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure(figsize = (8,6))\nplot_confusion_matrix(conf_mat\n                      , classes=class_names\n                      , title='CONF MATRIX')\nplt.show()\n","999f0cee":"# hyperparameter tuning with XGBoost (will take some time to run)\n\n# creating a KFold object \nfolds = KFold(n_splits= 3, shuffle= True, random_state= 101)\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.1,0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]}          \n\n\n# specify model\nxgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid, \n                        scoring= 'roc_auc', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True, \n                       n_jobs= -1)      \n","c1d3c4b6":"model_cv.fit(X_train,y_train)","f26d2001":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',model_cv.best_score_,'using',model_cv.best_params_)","e34d3640":"#lets use model with these parameters\nxgb_model_2 = XGBClassifier(max_depth=2, n_estimators=200, learning_rate= 0.1, subsample= 0.6)","1ecebb65":"#lets make predictions using this model\nxgb_model_2.fit(X_train,y_train)\n\nxgb_pred = xgb_model_2.predict(X_test)\n\nconf_mat = confusion_matrix(y_test,xgb_pred)\n\n\n#print the precision, recall , accuracy and confusion matrix\n\nprint('Precision Score: {}\\n'.format(precision_score(y_test,xgb_pred)))\nprint('Recall Score: {}\\n'.format(recall_score(y_test,xgb_pred)))\nprint('Accuracy Score: {}\\n'.format(accuracy_score(y_test,xgb_pred)))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure(figsize = (8,6))\nplot_confusion_matrix(conf_mat\n                      , classes=class_names\n                      , title='CONF MATRIX')\nplt.show()","bbee60d6":"#making the imports\n\nimport tensorflow as tf\nimport itertools\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils.np_utils import to_categorical","e03b57c7":"# checking the shapes of test and train\nprint(np.shape(X_train))\nprint(np.shape(y_train))\nprint(np.shape(X_test))\nprint(np.shape(y_test))","df217132":"#define the model\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=30, activation='relu'))\nmodel.add(Dropout(0.9))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.9))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.9))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.9))\nmodel.add(Dense(2, activation='softmax'))  # With 2 outputs\n","d4038c81":"#compile using adam optimizer\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","07662392":"#convert y_train and y_test to categorical values with 2 classes\ny_train = to_categorical(y_train, num_classes = 2)\ny_test = to_categorical(y_test, num_classes = 2)","fc8e6a9e":"#lets do first 10 epochs with a batch size of 2048\nepoch = 10\nbatch_size = 2048\nmodel.fit(X_train, y_train, epochs=epoch, batch_size=batch_size)","884a0f89":"#evaluate the model\nscore, acc = model.evaluate(X_test, y_test)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","703c2958":"#training for more epochs to get better results (lets say 50 epochs)\nhistory = model.fit(X_train, y_train, batch_size = 2048, epochs = 50, \n         validation_data = (X_test, y_test), verbose = 2)","a65e7581":"# Check the history keys\nhistory.history.keys()","f74508cc":"#convert to a data frame\nhist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\n\n#make the epoch start from 1\ndef add_one(x):\n    return x+1\n\nhist['epoch'] = hist['epoch'].apply(add_one)\n\nhist","0fc0ec02":"#plotting the results to see difference between train and validation accuracy\/loss\n\nplt.figure(figsize = (14,6))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.plot(hist['epoch'],hist['val_acc'], label = 'Val Accuracy')\nplt.plot(hist['epoch'],hist['acc'], label = 'Train Accuracy')\nplt.xticks(range(1,51))\nplt.legend(loc = 'lower right')\nplt.title('Accuracy')\nplt.show()\n\nplt.figure(figsize = (14,6))\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(hist['epoch'],hist['val_loss'], label = 'Val Loss')\nplt.plot(hist['epoch'],hist['loss'], label = 'Train Loss')\nplt.xticks(range(1,51))\nplt.legend()\nplt.title('Loss')\nplt.show()","01358923":"#lets predict using this trained model and get the confusion matrix\n\nnn_pred = model.predict(X_test)\n\npred_classes = np.argmax(nn_pred,axis = 1)\n\ny_true = np.argmax(y_test,axis = 1) \n\nconf_mat = confusion_matrix(y_true,pred_classes)\n\n\n#print the precision, recall , accuracy and confusion matrix\n\nprint('Precision Score: {}\\n'.format(precision_score(y_true,pred_classes)))\nprint('Recall Score: {}\\n'.format(recall_score(y_true,pred_classes)))\nprint('Accuracy Score: {}\\n'.format(accuracy_score(y_true,pred_classes)))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure(figsize = (8,6))\nplot_confusion_matrix(conf_mat\n                      , classes=class_names\n                      , title='CONF MATRIX')\nplt.show()","62f5bdb3":"There are some outliers  in Amount column. ","2db24bfb":"In **summary** we can say that:\n\n* We did not observe a clear distinction between normal and fraudulent transactions during data analysis.\n\n* All the features of this dataset were PCA transformed other than the time and amount, which don't indicated a proper correlation with the target variable.\n\n* If we had the orignal data feature engineering could have given better results. \n\n* Mostly models with default parameters (Random Forest\/ Xgboost) resulted in better performance than doing the GridSearch for hyper parameter tuning.\n\n* So I would suggest to stick with either Xgboost or Random Forest for this problem.","08a98b48":"## Model building and predictions\n## Random Forest\n\nWe will try different models. Let's begin with Random Forest.","684b345b":"Now recall is at 61%.","d0abd4e9":"## XgBoost","f8c4d64d":"As we can see that all columns are named as V... This is a PCA transformed dataset and we don't have any info about the customers or other features of these transactions. ","30808c89":"## Exploratory Data Analysis (EDA)\nLets do some data visualization.","384b1dcb":"Precision and accuracy is good but recall needs improvement. ","a132c916":"## Neural Nets","dd666ae9":"**Looks like grid search did not help much** **:-----((((**\n","2214c555":"Model accuracy is quite high. :-)","154aa3d1":"## Data Pre-Processing","5e3ec77a":"## Scaling\/ Normalization","af7e6835":"As we can see that mostly columns are type 'float'. ","b987ff5f":"We can see that there is no strong co-relatin between amount\/class or time\/class.\nSome of the columns before V19 have negative co-relation with the class column.","8d70bfe9":"## Logistic Regression\n\nLets try logistic regression and see if we can get a better result. ","f1f08cf7":"## Background: \nIt is important for credit Card companies to detect fraud cases. In this data set we are given PCA transformed dataset [due to confidentiality requirements] on the basis of which we have to judge if the transaction is legit or a fraudulent one. The dataset is high imablanced as most of the transactions are legit. \n\nWe will do some exploratory data analysis and also try different machine learning models to check which one gives better results on this highly skewed data. ","8b6ed388":"## Grid Search CV\nLets use grid search to tune the hyper parameters.","fecb447c":"The accuracy is good but very bad recall and precision. Looks like neural networks is not a good approach for this problem (highly skewed). Although further tuning and playing with layers can give better results. ","6b5ab7f9":"**So we have improved our recall using Xgboost. :-)** \n\nNow we have recall at 83%.","b79e977a":"Although other than 'time' and 'amount' columns are already scaled but let's apply the scaling to all to bring them on to a similar range.","b6b5fd1d":"The validation set loss starts to show some improvement after epoch 40.","feaa6a67":"Highly imbalanced data as fraudluent transactions are very less. Lets try to see it on log scale.","f3431f60":"We can see that most of the columns are concentrated around zero value.","b4007547":"So no need for missing values imputation as all columns are null free.","cab6da03":"**Seems like grid search did not make much of a difference from previous value.**\n","c65162f3":"So only 0.17% of the total transactions are fraudulent. Highly skewed data set."}}