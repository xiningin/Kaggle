{"cell_type":{"68f4ac2a":"code","1e348fa2":"code","4861c877":"code","b54b62b4":"code","7d03acde":"code","742f4097":"code","f775459d":"code","f834ec82":"code","29d216db":"code","8617672a":"code","a44d94fa":"code","004f9c14":"code","ac6c4faf":"code","c903d812":"code","c7cd00a6":"code","a9cada83":"code","124d4903":"code","03c239bd":"code","65837055":"code","7a9377cb":"markdown","7ab4d74b":"markdown","0f32033f":"markdown","22a8e9f9":"markdown","59f27018":"markdown","f5d63e48":"markdown","109a2fa4":"markdown","87953f2a":"markdown"},"source":{"68f4ac2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e348fa2":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4861c877":"data= pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","b54b62b4":"data.head()","7d03acde":"Not_Fraudulent,Fraudulent= data[\"Class\"].value_counts()","742f4097":"print((Not_Fraudulent\/len(data[\"Class\"]))*100),\nprint((Fraudulent\/len(data[\"Class\"]))*100)\nsns.countplot(\"Class\",data=data)","f775459d":"from sklearn.preprocessing import RobustScaler\n\nrob_scaler = RobustScaler()\n\ndata['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n\ndata.drop(['Time','Amount'], axis=1, inplace=True)\ndata.head()","f834ec82":"data = data.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = data.loc[data['Class'] == 1]\nnon_fraud_df = data.loc[data['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","29d216db":"f,ax1 = plt.subplots(1, 1, figsize=(24,20))\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","8617672a":"from sklearn.model_selection import train_test_split\n\nX_has= new_df.drop(\"Class\",axis=1)\ny_has= new_df[\"Class\"]\n\n# train and test data\nX_train, X_test, y_train, y_test = train_test_split(X_has, y_has)","a44d94fa":"model= LogisticRegression()\nmodel.fit(X_has,y_has)\ny_pred= model.predict(X_test)\n\nprint(classification_report(y_test,y_pred))","004f9c14":"X_bf=data.drop(\"Class\",axis=1)\ny_bf=data[\"Class\"]\nX_train_bf,X_test_bf,Y_train_bf,Y_test_bf= train_test_split(X_bf,y_bf)","ac6c4faf":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_resample(X_train, y_train)\n\n# check the balanced data\nprint(pd.Series(y_rus).value_counts())\n","c903d812":"undersampledlatermodel= LogisticRegression()\nundersampledlatermodel.fit(X_rus,y_rus)\ny_pred_undersampled_later= undersampledlatermodel.predict(X_rus)\n\n\nprint(classification_report(y_rus,y_pred_undersampled_later))","c7cd00a6":"from sklearn.metrics import confusion_matrix\n\nfig, ax = plt.subplots()\nsns.heatmap(confusion_matrix(y_rus, y_pred_undersampled_later, normalize='true'), annot=True, ax=ax)\n\nax.set_title(\"Confusion Matrix\")\nax.set_ylabel(\"Real Value\")\nax.set_xlabel(\"Predicted\")\n\nplt.show()","a9cada83":"from sklearn.tree import export_graphviz,DecisionTreeClassifier\nmodel_tree= DecisionTreeClassifier()\nmodel_tree.fit(X_rus,y_rus)\ny_pred_undersampledrus= model_tree.predict(X_rus)\n\nprint(classification_report(y_rus,y_pred_undersampledrus))","124d4903":"from IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus","03c239bd":"!pip install pydotplus","65837055":"dot = export_graphviz(model_tree, filled=True, rounded=True,\n                      feature_names=X.columns, class_names=['0', '1'])\n\ngraph = pydotplus.graph_from_dot_data(dot)  \nImage(graph.create_png())","7a9377cb":"*Class Imbalance is dealt in this case by downsampling the NON_FRAUD Cases to the count of the Fraud Cases.*","7ab4d74b":"# DEALING WITH CLASS IMBALANCE","0f32033f":"# Scaling Time And Amount","22a8e9f9":"**CASE_1:**","59f27018":"**CASE:2**","f5d63e48":"****There is a huge Class imbalance problem\nand chances are that we might overfit the problem and not detect anomalies at all.Also, since all the data is scaled, we shall scale time and amount as  well.****","109a2fa4":"**CASE-1:\nUnder_Sampled and then used Train_Test_Split**\n\n\n****CASE_2:\nFirst did the Train_Test Split and then undersampling****","87953f2a":"****WE CAN SEE THAT CASE-2 HAS A BETTER REPORT THAN CASE-1****"}}