{"cell_type":{"3fb67ab4":"code","2483c086":"code","239c2cfd":"code","47604722":"code","6d989e44":"code","82565c43":"code","517dba57":"code","fdb3c2f9":"code","7fc81c39":"code","04292406":"code","f84287b2":"code","89ddd061":"code","56227377":"code","c9b6767c":"code","788b397d":"code","73e5c853":"code","251ccc9f":"code","866a1141":"code","b2a94ba6":"code","212230e2":"code","5055f4ef":"code","db098bd5":"code","8d65ee9e":"code","34c8fbc3":"code","58f64a6b":"markdown","8f9742eb":"markdown","82dff70b":"markdown","5762c682":"markdown","29da4e49":"markdown","34d6190f":"markdown","5df9b546":"markdown","f680e285":"markdown","89fd8621":"markdown","2d4a38bd":"markdown","09588f5d":"markdown","e0a01965":"markdown"},"source":{"3fb67ab4":"\"\"\"\nCreated on Tue May  4 00:11:06 2021\n\n@author: YUSUF BARAN TANRIVERD\u0130\n\n\n\"\"\"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# accuracy_score average_precision_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import confusion_matrix, recall_score, roc_auc_score, balanced_accuracy_score, precision_score\nfrom sklearn.metrics import roc_curve\n\ndata = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n","2483c086":"# Turn categorical values into label encoding\n# ref: https:\/\/pbpython.com\/categorical-encoding.html\n\ndef label_encoding(data, feature_name):\n    # encodes with an alphabetical order  of original titles\n    data[feature_name] = data[feature_name].astype('category')\n    data[feature_name] = data[feature_name].cat.codes\n    return\n\n\nlabel_encoding(data, 'gender')\nlabel_encoding(data, 'ever_married')\nlabel_encoding(data, 'work_type')\nlabel_encoding(data, 'Residence_type')\nlabel_encoding(data, 'smoking_status')","239c2cfd":"data","47604722":"median = data['bmi'].median()\ndata['bmi'].fillna(median, inplace=True)","6d989e44":"sns.pairplot(data.iloc[:, 5:], hue='stroke') \nplt.show()","82565c43":"# create datasets    \nxy = data.iloc[:, 1:].to_numpy()\nnp.random.shuffle(xy)\n\n\nx = xy[:, :-1]\ny = xy[:, -1]","517dba57":"def convert_one_hot_vectors(y):\n    target_list = []\n    one_hot = np.eye(2)\n    for target in y:\n        if target == 0:\n            target_list.append(one_hot[0])\n        if target == 1:\n            target_list.append(one_hot[1])\n    y = np.asarray(target_list)\n    \n    #x, y = shuffle_data(x, y)\n    return y\n\ndef get_accuracy(y_preds, y_labels):\n    matches = 0\n    for boolean in y_preds == y_labels:\n        if boolean == True: matches = matches +1\n    return matches\/ len(y_labels)* 100\n\n#ref:https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\ndef confusion_matrix_scorer(y_preds, y_labels):\n    cm = confusion_matrix(y_labels, y_preds)\n    return {'tn': cm[0, 0], 'fp': cm[0, 1],\n            'fn': cm[1, 0], 'tp': cm[1, 1]}\n\n#ref: https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\ndef plot_roc_auc_curve(y_preds, y_labels, model_name):\n    ns_auc = roc_auc_score(y_labels, np.zeros(y_labels.shape))\n    lr_auc = roc_auc_score(y_labels, y_preds)\n    # summarize scores\n    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n    print(f'{model_name}: ROC AUC=%.3f' % (lr_auc))\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = roc_curve(y, np.zeros(y.shape))\n    lr_fpr, lr_tpr, _ = roc_curve(y, preds)\n    # plot the roc curve for the model\n    ns_curve, = plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n    plt.plot(lr_fpr, lr_tpr, marker='.', label=f'{model_name}')\n    # axis labels\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()\n    ns_curve.remove()\n\ndef print_metrics(y_preds, y_labels, model_name):\n    print(\"\\n MODEL: \", model_name )\n    \n    print(\"Accuracy:\", get_accuracy(y_preds, y_labels))\n    print(\"Null rate of test:\", get_accuracy(y_preds, np.zeros(y.shape)))\n    print(\"\\n CONFUSION MATRIX\")\n    print(confusion_matrix_scorer(y_preds, y_labels))\n\n    # recall is tp\/ (tp+fn)\n    print(\"Recall: \", recall_score(y_preds, y_labels))\n    \n    # precision is tp\/(tp+fp)\n    print(\"Precision:\", precision_score(y_preds, y_labels))\n    \n    # (recall+ specificity)\/2\n    print(\"Balanced Accuracy:\", balanced_accuracy_score(y_labels, y_preds)* 100)\n\n\ndef fit_LinearRegression():\n    regressor = LinearRegression()\n    tmp_y = convert_one_hot_vectors(y)\n    regressor.fit(x,tmp_y)\n        \n    probs = regressor.predict(x)\n    return  probs, regressor","fdb3c2f9":"probs, _  = fit_LinearRegression()\npreds = np.argmax(probs, axis= 1)\nprint_metrics(preds, y, 'Linear Regression (imbalanced data)')\n# balanced accuracy in this case would be 0.5\n\n\nprint(\"PREVELANCE -> pr(stroke|sample) = \",len(data[data['stroke'] == 1])\/len(data))\n# see the scatter plot also.\n\n\n","7fc81c39":"from imblearn.over_sampling import SMOTENC\n\n#CALL RAW DATA\nx = xy[:, :-1]\ny = xy[:, -1]\n\n#SMOTE the data with SMOTENC algorithm\ncat_col_index = [0, 4, 5, 6, 9]\noversample = SMOTENC(categorical_features=cat_col_index, k_neighbors=2)\nx, y = oversample.fit_resample(x, y)\n\ndf = pd.DataFrame(x, columns= ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status'])\n\ndf['stroke'] = y\n# print(df)\n#sns.pairplot(df.iloc[:, 5:], hue='stroke') \n#plt.show()\n#sns.pairplot(df.iloc[:, 5:], hue='stroke') \n#plt.show()\nprint(\" \\n AFTER SMOTENC...\")\n\nprobs, linearModelv2 = fit_LinearRegression()\npreds = np.argmax(probs, axis= 1)\n\nprint_metrics(preds, y, 'Linear Regression after SMOTE')\n\nprint(\" NEW PREVELANCE ->pr(stroke|sample) = \", len(df[df['stroke'] == 1])\/len(df))","04292406":"sns.pairplot(df.iloc[:, 5:], hue='stroke') \nplt.show()","f84287b2":"# import libs and define some useful functions\nfrom keras.layers import *\nimport tensorflow as tf\nimport keras\n\ndef get_evaluation_accuracy(history):\n    return 100 * history['accuracy']\n\n\ndef plot_metrics(model_name, history, metric_name):\n    plt.figure()\n    e = range(1, len(history.history[metric_name]) + 1)\n    plt.plot(e, history.history[metric_name], 'bo', label=metric_name)\n    plt.plot(e, history.history[f'val_{metric_name}'], 'b', label=f'val_{metric_name}')\n    plt.xlabel('Epoch')\n    plt.ylabel(f'{metric_name}')\n    plt.legend(loc='lower right')\n    plt.title(f'Comparing training and validation loss of {model_name}')\n    plt.savefig(f'.\/{model_name}.png')\n\ndef metrics_table_for_evaluation(model_name, history):\n    tp = history['true_pos']\n    fp = history['false_pos']\n    tn = history['true_neg']\n    fn = history['false_neg']\n    \n    cm = pd.DataFrame(index=('predicted YES', 'predicted NO'), columns=('actual YES', 'actual NO'))\n    cm.iloc[0,0] = np.mean(tp)\n    cm.iloc[0,1] = np.mean(fn)\n    cm.iloc[1,0] = np.mean(fp)\n    cm.iloc[1,1] = np.mean(tn)\n    \n    mcs = pd.DataFrame(index=[model_name], columns=('recall (TPR)', 'specificty', 'sensitivity', 'precision', 'false positive rate', 'accuracy'))\n    \n    mcs.iloc[0,0] = np.mean(tp)\/ (np.mean(tp)+ np.mean(fp))\n    mcs.iloc[0,1] = np.mean(tn)\/ (np.mean(tn)+ np.mean(fp)) \n    mcs.iloc[0,2] = np.mean(tp)\/ (np.mean(tp)+ np.mean(fn)) \n    mcs.iloc[0,3] = np.mean(tp)\/ (np.mean(tp)+ np.mean(fp)) \n    mcs.iloc[0,4] = np.mean(fp)\/ (np.mean(tn)+ np.mean(fp)) \n    mcs.iloc[0,5] = np.mean(history['accuracy']) \n    \n    print(cm)\n    print(mcs)","89ddd061":"#HYPERPARAMATERS\n\nLEARNING_RATE = 0.001\nDROPOUT_RATE = 0.5\nEPOCHS= 100\nBATCH_SIZE = 10\nLOSS_FN = 'binary_crossentropy'","56227377":"# create train_dataset\n# test split %90\n\nboundary = int(len(x) *0.1)\nx_train = x[boundary:]\ny_train = y[boundary:]\n    \n# create test_dataset\nx_test = x[:boundary]\ny_test = y[:boundary]","c9b6767c":"def dropout_dnn(number_of_classes=2, lr=LEARNING_RATE, dr=DROPOUT_RATE):\n    model = keras.Sequential([\n        Dense(512, activation='relu'),\n        Dense(256, activation='relu'),\n        Dropout(dr),\n        Dense(64, activation='relu'),\n        Dropout(dr),\n        Dense(16, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(loss=LOSS_FN,\n                  metrics=['accuracy', \n                           tf.keras.metrics.TrueNegatives(name='true_neg'),\n                           tf.keras.metrics.FalsePositives(name='false_pos'),\n                           tf.keras.metrics.FalseNegatives(name='false_neg'),\n                           tf.keras.metrics.TruePositives(name='true_pos'),\n                           ],\n                  optimizer=tf.keras.optimizers.Adam(lr))\n    return model\n\n","788b397d":"es_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)","73e5c853":"densenet = dropout_dnn()\ndropout_history = densenet.fit(x_train, y_train, validation_split=0.2, \n                               batch_size=BATCH_SIZE, verbose=1, epochs=EPOCHS\n                               )\n\n#save model\ntf.keras.models.save_model(\n    densenet,\n    '.\/models\/densenet.h5'\n)\n\n\npd.DataFrame.from_dict(dropout_history.history).to_csv('.\/dropout_histor.csv', index=False)\n\n","251ccc9f":"plot_metrics(history=dropout_history, metric_name='loss', model_name='densenet')\nplot_metrics(history=dropout_history, metric_name='accuracy', model_name='densenet')","866a1141":"eval_history = densenet.evaluate(x_test, y_test, return_dict=True)\n\nmetrics_table_for_evaluation('MLP test', eval_history)","b2a94ba6":"metrics_table_for_evaluation('MLP train', dropout_history.history)\n\n\n","212230e2":"print(\"NULL RATE\")\ndensenet.evaluate(x_test, np.zeros(y_test.shape))\n\nprint(\"FULL RATE\")\nindices = [i for i in range(len(y)) if y[i]==1]\ndensenet.evaluate(x_test, np.ones(y_test.shape))","5055f4ef":"from collections import Counter\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom xgboost import XGBClassifier","db098bd5":"# define model\nmodel = XGBClassifier()\n# define evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n","8d65ee9e":"model.fit(x, y)","34c8fbc3":"evals_result = model.score(x_test, y_test)\n\nevals_result","58f64a6b":"To stop training when no change happens:","8f9742eb":"EXP3 DEEP LEARNING METHODS\n\nREFERENCES:\n https:\/\/towardsdatascience.com\/handling-overfitting-in-deep-learning-models-c760ee047c6e\n\n https:\/\/towardsdatascience.com\/dont-overfit-how-to-prevent-overfitting-in-your-deep-learning-models-63274e552323\n\n https:\/\/towardsdatascience.com\/multi-layer-perceptron-using-tensorflow-9f3e218a4809\n","82dff70b":"EXP2 SMOTE - REGRESSION","5762c682":"SPECIAL CASES\n1) NaN values\nA very common way to replace missing values is using a median.\nReplace using median \n\nref: https:\/\/towardsdatascience.com\/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b\n\n2) UNKNOWN values of smoking status\n What to do? \n They shouldn't play as a factor category- \n i.e they should be neutral on the subject of smoking is whether harmful or not!\n For now, let's pass them as another category.\n ","29da4e49":"EXP1 LINEAR REGRESSION","34d6190f":"**Tranining**","5df9b546":"**Some Useful Functions**","f680e285":"EXP4 XGBOOST","89fd8621":"DEFINE MODEL with a dropout layer.","2d4a38bd":"It always give 'negative'. Imbalanced target problem?   Yes, solutions might be:\n    \nSMOTE\n\nMODEL CHANGING\n\nDIFFERENT METRIC EVALUATION \n\n(ref: https:\/\/towardsdatascience.com\/regression-for-imbalanced-data-with-application-edf93517247c)","09588f5d":"**Create Datasets**","e0a01965":"It is seen that data is imbalanced heavily. Let's experiment with that before handling."}}