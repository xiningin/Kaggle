{"cell_type":{"98f5b959":"code","9b699b6c":"code","8626d879":"code","d3908466":"code","0f0b3228":"code","b3ae0896":"code","9e7ef8a7":"code","74cea3e3":"code","3280e2a6":"code","83af79dc":"code","4340a55a":"code","c85824bd":"code","fa6e4576":"code","c6b22720":"code","4998f3bd":"code","d71f3862":"code","c94838ce":"code","d5537828":"code","a880e4f5":"code","c1a8b3bd":"code","dfa5895d":"code","e2ad2cf7":"code","c4a6a336":"code","badc3a6d":"code","ba3da2b7":"code","d02a5dd1":"code","a5b2f14e":"code","6281ab0e":"code","683c8024":"code","4d5f8913":"code","2300e3bc":"code","fb99270b":"code","4d97e73c":"code","cf46aafb":"code","f2f44682":"markdown","efb22a76":"markdown","5cbc801d":"markdown","e61a703a":"markdown","319384b6":"markdown","956174fa":"markdown","e13ada5b":"markdown","fccb250a":"markdown","2a5466dc":"markdown","28840934":"markdown","f649cb8d":"markdown","25b82c06":"markdown","571cf62b":"markdown","7218e14f":"markdown","62bfa0cb":"markdown","0a3a3f93":"markdown"},"source":{"98f5b959":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9b699b6c":"data = pd.read_csv('\/kaggle\/input\/onion-or-not\/OnionOrNot.csv')","8626d879":"data.shape","d3908466":"data.head(10)","0f0b3228":"display(data['text'][0])\ndisplay(data['text'][2])\ndisplay(data['text'][5])\ndisplay(data['text'][9])","b3ae0896":"display(data['text'][1])\ndisplay(data['text'][6])\ndisplay(data['text'][7])","9e7ef8a7":"import matplotlib.pyplot as plt\nlabels = [1,0]\nplt.pie(data['label'].value_counts() )\nplt.legend(labels)\nplt.title('Onions or not')","74cea3e3":"import re","3280e2a6":"data_process = data.copy()","83af79dc":"data_process.head()","4340a55a":"data_process[\"text\"] = data_process[\"text\"].str.replace('[^a-zA-Z]', ' ', regex=True)","c85824bd":"display(data['text'][2])\ndisplay(data_process['text'][2])","fa6e4576":"data_process[\"text\"] = data_process[\"text\"].str.lower()","c6b22720":"display(data['text'][2])\ndisplay(data_process['text'][2])","4998f3bd":"from keras.preprocessing.text import Tokenizer","d71f3862":"num_words = 20000\nmax_len = 150\nemb_size = 128\nX = data_process[\"text\"]","c94838ce":"X","d5537828":"token = Tokenizer(num_words = num_words)\ntoken.fit_on_texts(list(X))","a880e4f5":"X = token.texts_to_sequences(X)","c1a8b3bd":"display(data['text'][2])\ndisplay(data_process['text'][2])\ndisplay(X[2])","dfa5895d":"plt.plot(X[2], label = 'sample text spectra')\nplt.title(str(data['text'][2]))","e2ad2cf7":"from keras.preprocessing import sequence\n\nX = sequence.pad_sequences(X, maxlen = 150)\ny = pd.get_dummies(data_process['label'])","c4a6a336":"y = y.values","badc3a6d":"from sklearn.model_selection import train_test_split","ba3da2b7":"x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.1, random_state = 42)","d02a5dd1":"import nltk\nfrom keras.models import Sequential\nfrom keras.layers import Input,Dense, LSTM, Dropout, Flatten, Embedding, Bidirectional, GlobalMaxPool1D","a5b2f14e":"def model():\n    \n    inp = Input(shape = (max_len, ))\n    layer = Embedding(num_words, emb_size)(inp)\n    layer = Bidirectional(LSTM(50, return_sequences = True, recurrent_dropout = 0.1))(layer)\n    \n    layer = GlobalMaxPool1D()(layer)\n    layer = Dropout(0.2)(layer)\n    \n    \n    layer = Dense(50, activation = 'relu')(layer)\n    layer = Dropout(0.2)(layer)\n    layer = Dense(50, activation = 'relu')(layer)\n    layer = Dropout(0.2)(layer)\n    layer = Dense(50, activation = 'relu')(layer)\n    layer = Dropout(0.2)(layer)\n    \n    \n    layer = Dense(2, activation = 'softmax')(layer)\n    model = Model(inputs = inp, outputs = layer)\n    \n    \n    model.compile(loss = 'binary_crossentropy', optimizer = 'nadam', metrics=['accuracy'])\n    return model","6281ab0e":"from keras.models import Model\nfrom keras.utils import plot_model\nimport matplotlib.image as mpimg\n\nmodel = model()\nmodel.summary()\n\nplot_model(model, to_file='onion.png',show_shapes=True, show_layer_names=True)\nplt.figure(figsize = (30,20))\nimg = mpimg.imread('\/kaggle\/working\/onion.png')\nimgplot = plt.imshow(img)","683c8024":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfile_path = 'save.hd5'\ncheckpoint = ModelCheckpoint(file_path, monitor = 'val_loss', save_best_only=True)\nearly_stop = EarlyStopping(monitor = 'loss', patience = 1)","4d5f8913":"history = model.fit(x_train, y_train, batch_size = 32, epochs = 3, validation_split = 0.1, callbacks = [checkpoint,early_stop])","2300e3bc":"val_loss = history.history['val_loss']\nloss = history.history['loss']","fb99270b":"print('validation loss: ', val_loss[-1])\nprint('training loss: ', loss[-1])","4d97e73c":"score = model.evaluate(x_test, y_test)\nprint(model.metrics_names)\nprint(score)\nprint('test loss: ', score[0])\nprint('test accuracy: ', score[1])","cf46aafb":"history.history","f2f44682":"I got the feature extraction notes on https:\/\/www.kaggle.com\/nihalbey\/spam-detection-and-deep-nlp . I was stuck before wondering how to be done with those dataframe","efb22a76":"Limit the time, and save the model. \n\nHow to save and load if interrupted: https:\/\/www.kaggle.com\/danmoller\/make-best-use-of-a-kernel-s-limited-uptime-keras","5cbc801d":"### And this is r\/NotTheOnion","e61a703a":"### Separating train and test data","319384b6":"### Fitting model","956174fa":"### Lets see this onion","e13ada5b":"## Let's have a look","fccb250a":"### 3. Tokenization","2a5466dc":"### 1. Remove Puctuation","28840934":"## NLP\n\nFinally this part came, lets start!\n\nThe traditional LSTM was time consuming, therefore the bidirectional used and combined with Dense layer for time saving, thanks to this kernel https:\/\/www.kaggle.com\/lsjsj92\/toxic-nlp-with-keras-lstm#648911 . ","f649cb8d":"# WHY?\n\nFirst of all, I want to introduce what's so called 'onion'. Of course, a kind of cooking ingredient (although one of my friend treat it like a fruit). A story could be so utopiad, then crowds don't believe it, in spite of the fact that it happened. Later, I will show you about the story. \n\nThe onion articles labeled as 1, and the r\/NotTheOnion articles are labeled 0.","25b82c06":"## Preparing the library and Preprocessing\n\nThe ONN was based on NLP with DNN approach.\n\n1. First, we will remove punctuation.\n2. Second, decapitalize letters\n3. Tokenization","571cf62b":"### See the differences? they were awful, right? ","7218e14f":"#### Prepare yourself, it will be long, so its better to save the model","62bfa0cb":"### 2. Turn to lowercase","0a3a3f93":"## Lets see the prediction result!"}}