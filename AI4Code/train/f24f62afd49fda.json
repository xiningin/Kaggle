{"cell_type":{"9d936b7a":"code","4c1ced32":"code","d0a8fe90":"code","6a6b2a57":"code","495a8a2c":"code","49e1aee2":"code","d20146f7":"code","7aad0e89":"code","d1f6cc05":"code","92b012e9":"code","4247f62a":"code","3b43edf1":"code","32cf9424":"code","f9cfc661":"code","ccd40ea7":"code","e62f085f":"code","4be9557d":"code","a6774e98":"code","8f43901d":"code","862ec793":"code","e601c871":"code","dde04e8b":"code","c9e1cab8":"code","a2cf2b6f":"code","aec619e4":"code","df80174e":"code","5df7519a":"code","f171153f":"code","7be12bab":"code","776ddacf":"code","ff27b322":"code","2aabb138":"code","b00b51ff":"code","73968fd0":"code","2aafec78":"code","2681c247":"code","263420c5":"code","0b36fa69":"code","61e7c99d":"code","74fb76f9":"code","2792ec36":"code","47e0e47c":"code","57140ce3":"code","72a9562f":"code","5ba55773":"code","ed0374a3":"code","7c7325b7":"code","4eecbe6b":"code","d8232592":"code","49728d2a":"code","ea008411":"code","f86019c5":"code","fd628554":"code","06bab056":"code","619a2c0b":"code","52ae975c":"code","1095081d":"code","425902f1":"code","ab19e753":"code","a07b85ae":"code","11c48c17":"code","7ed851bc":"code","f0a1928d":"code","17df3d12":"code","6aa1e8ac":"code","fe00e7c1":"code","7a2c9dfb":"code","a3ca5b0f":"code","514a6da8":"code","af901c66":"code","ae2eb0d4":"code","d11f638b":"code","f1abefdc":"code","a5a1fec1":"code","a8f56f71":"code","e97d61be":"code","325076f2":"markdown","24f8e4c4":"markdown","4a7206c9":"markdown","1706ae70":"markdown","9367dcff":"markdown","c5d558ee":"markdown","a2718ce5":"markdown","f89bb1fe":"markdown","d555c0fd":"markdown","a5904203":"markdown","98e6c185":"markdown","16641ce6":"markdown","5b205f50":"markdown","832180d9":"markdown","d564637b":"markdown","9c8836f0":"markdown","fc8577a6":"markdown","02b49e88":"markdown","3aeb5855":"markdown","159921f5":"markdown","0a326458":"markdown","d6cf9b16":"markdown","14c31fa9":"markdown","85f6e804":"markdown","bdda48de":"markdown","f1fcb931":"markdown"},"source":{"9d936b7a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c1ced32":"# Importing the necessary library \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Displaying the all the columns of the dataframe\npd.set_option('display.max_columns',None)","d0a8fe90":"# Reading the dataset\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","6a6b2a57":"# print the shape of the dataset rows and columns\nprint(df_train.shape)","495a8a2c":"# Print out the top 5 records\ndf_train.head(5)","49e1aee2":"# Here we will check the percentage of nan values present in each features\n# 1-step make list of features which has missing values\n\nfeature_with_nan = [feature for feature in df_train.columns if df_train[feature].isnull().sum()>1]\nfor feature in feature_with_nan:\n    print(feature, np.round(df_train[feature].isnull().mean(),4),'% missing values')","d20146f7":"# ploting the relationship between sales price and missing values using the bar plot\n\nfor feature in feature_with_nan:\n    # deep copy the dataset\n    df = df_train.copy()\n    \n    # Let's make a variable that indicates 1 if the observation was missing values\n    # 0 for not missing values\n    df[feature] = np.where(df[feature].isnull(),1,0)\n    \n    #Let's calculate the mean salesprice where the information is missing values\n    plt.figure(figsize=(10,5))\n    df.groupby(feature)['SalePrice'].mean().plot.bar()\n    plt.ylabel('Sales price')\n    plt.legend(['1 for missing values'])\n    plt.show()","7aad0e89":"print('Id of the house {}'.format(len(df_train.Id)))","d1f6cc05":"# List of numerical variables\nnumerical_features = [feature for feature in df_train.columns if df[feature].dtypes !='O']\n\nprint('Number of numerical variables {}'.format(len(numerical_features)))\n\n#Display the numerical features\ndf_train[numerical_features].head()","92b012e9":"# List of variables that contain year information \nyear_feature =[feature for feature in numerical_features if 'Yr' in feature or 'Year' in feature]\nprint(year_feature)","4247f62a":"# Let's explore the content of these year variables\nfor feature in year_feature:\n    print(feature, df_train[feature].unique())","3b43edf1":"# Lets analyze the temporal datetime variable\n# We will check wheather there is a relation between year the house is sold\n\ndf_train.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('Median House Price')\nplt.title('House Price VS YearSold')\nplt.show()","32cf9424":"# Here we will compare the difference between all years feature with the yearSold\n\nfor feature in year_feature:\n    if feature != 'YrSold':\n        df = df_train.copy()\n        \n        # we will capture the difference between year variable and year of house\n        df[feature]=df['YrSold'] - df[feature]\n        \n        plt.scatter(df[feature],df['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()","f9cfc661":"# Numerical variables are usally of 2'type\n# continous variables and discrete variables\n\ndiscrete_feature = [feature for feature in numerical_features if len(df[feature].unique()) <25 and feature not in year_feature+['Id']]\nprint('Discrete vaiables count: {}'.format(len(discrete_feature)))\ndiscrete_feature","ccd40ea7":"df_train[discrete_feature].head()","e62f085f":"# Lets find the relationship between the and sales price\nfor feature in discrete_feature:\n    df = df_train.copy()\n    \n    df.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","4be9557d":"continous_feature = [feature for feature in numerical_features if feature not in discrete_feature+year_feature+['Id']]\nprint('Number of continous feature {}'.format(len(continous_feature)))","a6774e98":"# Lets analyse the continous value by createing histograms\nfor feature in continous_feature:\n    df = df_train.copy()\n    \n    df[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n    plt.title(feature)\n    plt.show()","8f43901d":"# we will be using logrithmic transformation\nfor feature in continous_feature:\n    df = df_train.copy()\n    \n    if 0 in df[feature].unique():\n        pass\n    else:\n        df[feature]=np.log(df[feature])\n        df['SalePrice']= np.log(df['SalePrice'])\n        \n        plt.scatter(df[feature],df['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.title(feature)\n        plt.show()","862ec793":"for feature in continous_feature:\n    df = df_train.copy()\n    \n    if 0 in df_train[feature].unique():\n        pass\n    else:\n        df[feature] = np.log(df[feature])\n        df.boxplot(column = feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","e601c871":"# List  of categorical variables\ncategorical_features = [feature for feature in df_train.columns if df_train[feature].dtypes =='O']\ncategorical_features","dde04e8b":"df_train[categorical_features].head()","c9e1cab8":"for feature in categorical_features:\n    print('The feature is {} and number of categories are {}'.format(feature, len(df[feature].unique())))","a2cf2b6f":"# Find the relationship between categorical variables and the dependent feature\nfor feature in categorical_features:\n    df = df_train.copy()\n    df.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalesPrice')\n    plt.title(feature)\n    plt.title(feature)\n    plt.show()","aec619e4":"# replace missing value with a new label\nfeature_nan = [feature for feature in df_train.columns if df_train[feature].isnull().sum()>1 and df_train[feature].dtypes == 'O']\n\ndef replace_cat_feature(df,feature_nan):\n    data = df.copy()\n    \n    data[feature_nan]=data[feature_nan].fillna('Missing')\n    return data\ndf_train = replace_cat_feature(df_train, feature_nan)\ndf_train[feature_nan].isnull().sum()","df80174e":"df_train.head()","5df7519a":"# Now lets check for numerical variables the contains missing value\nnumerical_with_nan = [feature for feature in df_train.columns if df_train[feature].isnull().sum()>1 and df_train[feature].dtypes != 'O']\n\n# We will print the numerical nan variables and percentage of missing value\nfor feature in numerical_with_nan:\n    print('{}: {}% missing values'.format(feature, np.around(df_train[feature].isnull().mean(),4)))","f171153f":"#repacing the numerical missing\n# note when lots of outlier present in the dataset to replace with mode and median\n\nfor feature in numerical_with_nan:\n    # we will replace by using median since there are outlier\n    median_value = df_train[feature].median()\n    \n    # create a new feature to capture nan values\n    df_train[feature+'nan']=np.where(df_train[feature].isnull(),1,0)\n    df_train[feature].fillna(median_value,inplace=True)\n    \ndf_train[numerical_with_nan].isnull().sum()","7be12bab":"df_train.head()","776ddacf":"# temporal variables (date time variables)\nfor feature in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    df_train[feature]=df_train['YrSold']-df_train[feature]","ff27b322":"df_train.head()","2aabb138":"df_train[['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']]","b00b51ff":"df_train.head()","73968fd0":"num_features = ['LotFrontage', 'LotArea','1stFlrSF','GrLivArea','SalePrice']\n\n# convert skewed features into log normal\nfor feature in num_features:\n    df_train[feature]=np.log(df[feature])","2aafec78":"# handling the Nan value of LotFrontage features\ndf_train['LotFrontage'].fillna(df['LotFrontage'].median(),inplace=True)","2681c247":"df_train.head()","263420c5":"categorical_features = [feature for feature in df_train.columns if df_train[feature].dtypes =='O']\ncategorical_features","0b36fa69":"for feature in categorical_features:\n    temp = df_train.groupby(feature)['SalePrice'].count()\/len(df_train)\n    temp_df = temp[temp>0.01].index\n    df_train[feature]=np.where(df_train[feature].isin(temp_df),df_train[feature],'Rare_var')","61e7c99d":"df_train.head()","74fb76f9":"for feature in categorical_features:\n    labels_ordered = df_train.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered = {k:i for i,k in enumerate(labels_ordered,0)}\n    df_train[feature]=df_train[feature].map(labels_ordered)","2792ec36":"feature_scale = [feature for feature in df_train.columns if feature not in ['id','SalePrice']]\n\nscaler = MinMaxScaler()\nscaler.fit(df_train[feature_scale])","47e0e47c":"# transform the train and test and add on the id and salesprice\n\nx_train  = pd.concat([df_train[['Id','SalePrice']].reset_index(drop = True),\n                      pd.DataFrame(scaler.transform(df_train[feature_scale]), columns = feature_scale)],axis=1)\n","57140ce3":"# capture the dependent feature and independent feature\ny = x_train[['SalePrice']]\ny_train = y.copy()\nx_train = x_train.drop(['Id', 'SalePrice'],axis=1)","72a9562f":"## Apply the feature selection\n# first, I specify the Lasso Regression model, and I\n# select a suitable alpha (equivalent of penalty)\n# The bigger the alpha the less features that will be selected\n\n# Then I use the selectFromModel oject from sklearn, which\n# will select the features which coefficients are non-zero\n\nfeature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0))\nfeature_sel_model.fit(x_train, y_train)","5ba55773":"# Let's print the number of total and selected features\n\n#this is how we can make a list of the selected features\nselected_feature_Lasso = x_train.columns[(feature_sel_model.get_support())]\n\n# Let's print some stats\nprint('Total features: {}'.format(x_train.shape[1]))\nprint('Selected features: {}'.format(len(selected_feature_Lasso)))\nprint('features with coefficients shrank to zero: {}'.format(np.sum(feature_sel_model.estimator_.coef_ == 0)))","ed0374a3":"# feature selectin function\ndef feature_selection(model,x,y,name):\n    feature_sel_model = SelectFromModel(model)\n    feature_sel_model.fit(x,y)\n    \n    #this is how we can make a list of the selected features\n    selected_feature = x.columns[(feature_sel_model.get_support())]\n    \n    #Let's print some stats\n    print('Name of the model {}'.format(name))\n    print('Total feature: {}'.format(x.shape[1]))\n    print('Selected feature: {}'.format(len(selected_feature)))\n    return selected_feature","7c7325b7":"# feature selction for LinearModel\nselected_feature_Linear = feature_selection(LinearRegression(),x_train,y_train,'Linear_Regression')","4eecbe6b":"# feature selection for RandomForestRegression\nselected_feature_RandomForest = feature_selection(RandomForestRegressor(),x_train,y_train,'RandomForestRegressor')","d8232592":"# feature selection for XBGRegressor\nselected_feature_XGBRegressor = feature_selection(XGBRegressor(),x_train,y_train,'XGBRegressor')","49728d2a":"# capute the selected feature from the training data and split the data into train and test\nx_df = x_train.copy()\nx_train = x_train[selected_feature_Linear]\nx_train,x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=0)","ea008411":"# fit the linear regression model\nmodel_linear = LinearRegression()\nmodel_linear.fit(x_train, y_train)","f86019c5":"# checking the score of linear model\ny_pred = model_linear.predict(x_test)\nprint('Score of the model {}'.format(np.round(model_linear.score(x_test, y_test),2)))","fd628554":"# root mean squared error of test se\nprint('Root mean squared of test set: {}'.format(np.sqrt(mean_squared_error(y_test,y_pred))))","06bab056":"# capute the selected feature from the training data and split the data into train and test\nx = x_df[selected_feature_XGBRegressor]\nx_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state=0)","619a2c0b":"# fit the XGBRegressor\nxgb = XGBRegressor()\nxgb.fit(x_train, y_train)","52ae975c":"# root mean squared error of test se\nprint('Root mean squared of test set: {}'.format(np.round(np.sqrt(mean_squared_error(y_test,y_pred)),2)))","1095081d":"# capute the selected feature from the training data and split the data into train and test\nx = x_df[selected_feature_RandomForest]\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=0)","425902f1":"# import and fit the model\nrandom_reg = RandomForestRegressor()\nrandom_reg.fit(x_train, y_train)","ab19e753":"# root mean squared error on the test set\ny_pred = random_reg.predict(x_test)\nprint('root mean squared error: {}'.format(np.round(np.sqrt(mean_squared_error(y_test,y_pred)),2)))","a07b85ae":"# capture the selected features from the training data and split the data into train and test\nx = x_df[selected_feature_Lasso]\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=0)","11c48c17":"# import and fit the model\nmodel_lasso = Lasso(alpha=0.005, random_state=0)\nmodel_lasso.fit(x_train,y_train)","7ed851bc":"# root mean squared error on the test set\ny_pred = model_lasso.predict(x_test)\nprint('root mean squared error: {}'.format(np.round(np.sqrt(mean_squared_error(y_test,y_pred)),2)))","f0a1928d":"# split the data into test and train\nx = x_df[selected_feature_RandomForest]\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=0)","17df3d12":"# set the hyperparameter grid\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [1, 2, 4],\n    'min_samples_split': [2, 5, 10],\n    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000,2500,3000]\n}","6aa1e8ac":"# fit the RandomizedSerchCV\nrfr = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = rfr, param_distributions = param_grid, n_iter = 100, cv=5, verbose=2, random_state=42, n_jobs=-1)\nrf_random.fit(x_train, y_train)","fe00e7c1":"print('The best parameter selection by the randomizedSearchCV: {}'.format(rf_random.best_params_))","7a2c9dfb":"# Importing the model and set the hyperparameter\nmodel_rf = RandomForestRegressor(n_estimators =1800, min_samples_split=5, min_samples_leaf=1, max_features='sqrt',max_depth=None, bootstrap=True)","a3ca5b0f":"# fiting the model\nmodel_rf.fit(x_train, y_train)","514a6da8":"# predict the result\ny_pred = model_rf.predict(x_test)\nprint('RMSE value: {}'.format(np.round(np.sqrt(mean_squared_error(y_test, y_pred)))))","af901c66":"# \ny_pred = model_rf.predict(x_test)\nprint('Mean Absolute Error: {}'.format(np.round(mean_absolute_error(y_test, y_pred),3)))\nprint('Root Mean Squared error: {}'.format(np.round(np.sqrt(mean_squared_error(y_test, y_pred)),3)))\nprint('R2 Score: {}'.format(np.round(r2_score(y_test, y_pred))))","ae2eb0d4":"x = df_test[selected_feature_RandomForest]","d11f638b":"# handling the nan value\nfeature_with_nan = [feature for feature in x.columns if x[feature].isnull().sum()>=1]\nfor feature in feature_with_nan:\n    x[feature].fillna(x[feature].median(),inplace=True)","f1abefdc":"# encoding the categorical feature\nencoder = LabelEncoder()\nx['Neighborhood'] = encoder.fit_transform(x['Neighborhood'])","a5a1fec1":"# normalize the data\nscaler = MinMaxScaler()\nx = scaler.fit_transform(x)","a8f56f71":"y_pred = model_rf.predict(x)","e97d61be":"my_submission = pd.DataFrame({'Id': df_test.Id, 'SalePrice': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","325076f2":"Here with the relatinship between the missing values and dependent variable is clearly visible. So we need the replace nan values with something meaningfull values. Which we will do in the feature Engineering\n\nFrom the above dataset some of the feature like id is not required","24f8e4c4":"### **outliers**","4a7206c9":"##### **Temporal variables(Eg. Datetime variables)**\nFrom the dataset we have 4 year variable. we have extract information from the datetime variables like no of years or not of days. One example in this specific scenario can be difference in years between the year the house was built and the year the house was sold we will perform this analysis in the feature engineering.","1706ae70":"## **submission**","9367dcff":"Since they are many missing values, we need to find the releationship between missing values and saleprice","c5d558ee":"### **1. Linear Regression**","a2718ce5":"#### 2. **Distribution of Numerical Variables**","f89bb1fe":"#### 4. **Categorical Variables**","d555c0fd":"### **1. In data analysis find out the following:**\n1. Missing values\n2. All the numerical variables\n3. Distribution of the numerical variables\n4. Categorical variables\n5. Cardinality of catergorical variables\n6. Outliers\n7. Relationship between independent and dependent feature(SalePrice)","a5904203":"## **Building the final Model**","98e6c185":"## **Model Evaluation**","16641ce6":"## **Feature Scalling**","5b205f50":"#### 2. **Numerical Variables**","832180d9":"### **3. Random Forest Regressor**","d564637b":"# **Welcome kagglers**\n\n## **Problem Statement:**\nWe have to develope tha Machine Learning model to predicate approperiate the House price based on the given features set. \n\n## **All the Lifecycles of the Data Science Project:**\n* Data Analysis\n* Feature Engineering\n* Feature Selection\n* Model Building\n* Hyper Parameter Tunning\n* Model Evaluation\n* Model Deployment\n* Model Monitoring\n\nIn this notebook we have to done the Data Analysis, Feature Engineering, Feature Selection, Model Building, Hyper parameter tunning, Model Evaluation","9c8836f0":"We compare the rmse values of the four model. We have to select the RandomForestRegressor","fc8577a6":"### **4. Lasso**","02b49e88":"## **Model Building**","3aeb5855":"We will remove categorical variables that are present less than 1% of the observation","159921f5":"### **Numerical Variables**\nSince the numerical variables are skewed we will perform log normal distribuation\n","0a326458":"#### **Continous Variable**","d6cf9b16":"## **Hyperparameter Tunning**","14c31fa9":"#### 1. **Missing Values**","85f6e804":"# **Thankyou**","bdda48de":"### **2. XGBRegressor**","f1fcb931":"### **Handling Rare Categorical Feature**"}}