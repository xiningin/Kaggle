{"cell_type":{"31ec3f86":"code","acbc43cc":"code","202eee20":"code","798df9ca":"code","289fefa4":"code","45f189a9":"code","4dd02f5d":"code","66e7db79":"code","5f18228a":"code","61f0df47":"code","a0daf7f4":"code","1017d65b":"code","7d9211f7":"code","339d976d":"markdown","c220a39d":"markdown","52040e95":"markdown","b749ef2f":"markdown","c87e78ef":"markdown","e326f6d6":"markdown","12ab2183":"markdown","d551f889":"markdown","85c89738":"markdown","4fb65b67":"markdown","9af4d6cb":"markdown","c5e6cc54":"markdown"},"source":{"31ec3f86":"INPUT_CSV = \"cycle-cell.csv\" #\"mucus-manu.csv\" #\"test1_mucus_LFIT_no_selection.csv\" #\"test2_mucus_LFIT_selection.csv\"# \"test1_mucus_LFIT_no_selection.csv\" #\"mucusV1.csv\" #'example_data.csv'\nSHORTEN_TARGET_LABEL = True # Only keep \"f115\" from \"f115 = formula\"\nALGORITHM = \"gula\" #\"pride\"","acbc43cc":"!pip install pylfit","202eee20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport pylfit\nimport itertools\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","798df9ca":"# Constants\nDATA_PATH = \"\/kaggle\/input\/totembionet-data\/\"\nDEBUG = False\nCSV_FILE = DATA_PATH+INPUT_CSV\nDATA_LOADING = \"panda\" # \"homemade\"\nHEADER_OFFSET = 1","289fefa4":"if INPUT_CSV in [\"mucusV1.csv\", \"mucusV2.csv\", \"test1_mucus_LFIT_no_selection.csv\", \"test2_mucus_LFIT_selection.csv\"]:\n    DATA_LOADING = \"type1\"\n    NB_FEATURES = None\nif INPUT_CSV == \"mucus-manu.csv\":\n    DATA_LOADING = \"type2\"\n    NB_FEATURES = 12\n    SHORTEN_TARGET_LABEL = True\n#if INPUT_CSV == \"cycle-cell.csv\":\n#    DATA_LOADING = \"type3\"\n#    NB_FEATURES = 54\n#    SHORTEN_TARGET_LABEL = False\nif INPUT_CSV == \"cycle-cell.csv\":\n    DATA_LOADING = \"type4\"\n    NB_FEATURES = 54\n    SHORTEN_TARGET_LABEL = False\n    HEADER_OFFSET = 1","45f189a9":"def extract_from_csv(file_path, data_loading=\"type1\", nb_features=None):\n    file_name = file_path[file_path.rindex(\"\/\")+1:]\n    features = []\n    targets = []\n    data = []\n    \n    f = open(file_path,\"r\")\n    line_id = 0\n    for line in f:\n        line_id += 1\n        line = line.strip()\n        #print(line)\n        \n        if data_loading == \"type1\":\n            # Ignore first line\n            if line_id == 1:\n                continue\n\n            line = line.split(\",\")\n            line = [i.strip(\" \") for i in line if i != \"\"]\n\n            # Extract features names\n            if line_id == 2:\n                features = line\n                continue\n\n            # Extract features domains\n            if line_id == 3:\n                for i, token in enumerate(line):\n                    if i < len(features):\n                        min_val = int(token[:token.index(\".\")])\n                        max_val = int(token[token.rindex(\".\")+1:])\n                        features[i] = (features[i], [str(val) for val in list(range(min_val,max_val+1))])\n                    else:\n                        target_label = token\n                        if SHORTEN_TARGET_LABEL:\n                            target_label = target_label[:target_label.index(\"=\")-1]\n                        targets.append((target_label, [\"0\",\"1\"]))\n\n                continue\n\n            # Extract data line\n            if len(line) != len(features)+len(targets):\n                print(\"Error line\",line_id,\" expected\",len(features)+len(targets),\"columns, got \",len(line))\n                continue\n\n            feature_domains = []\n            target_state = []\n            for i, value in enumerate(line):\n                if i < len(features):\n                    if \".\" in value:\n                        min_val = int(value[:value.index(\".\")])\n                        max_val = int(value[value.rindex(\".\")+1:])\n                        feature_domains.append([str(val) for val in list(range(min_val, max_val+1))])\n                    else:\n                        feature_domains.append([value])\n                else:\n                    target_state.append(value)\n\n            # Generate all combinations of domains\n            feature_states = [list(i) for i in list(itertools.product(*feature_domains))]\n            for i in feature_states:\n                data.append((i,target_state))\n                \n        elif data_loading == \"type2\":\n\n            line = line.split(\",\")\n            line = [i.strip(\" \") for i in line if i != \"\"]\n\n            # Extract features names\n            if line_id == 1:\n                features = line[:NB_FEATURES]\n                print(features)\n                for i, token in enumerate(line):\n                    if i >= len(features):\n                        target_label = token\n                        if SHORTEN_TARGET_LABEL:\n                            target_label = target_label[:target_label.index(\"=\")-1]\n                        targets.append((target_label, [\"0\",\"1\"]))\n                continue\n\n            # Extract features domains\n            if line_id == 2:\n                for i, token in enumerate(line):\n                    if i < len(features):\n                        min_val = int(token[:token.index(\".\")])\n                        max_val = int(token[token.rindex(\".\")+1:])\n                        features[i] = (features[i], [str(val) for val in list(range(min_val,max_val+1))])\n                continue\n\n            # Extract data line\n            if len(line) != len(features)+len(targets):\n                print(\"Error line\",line_id,\" expected\",len(features)+len(targets),\"columns, got \",len(line))\n                continue\n\n            feature_domains = []\n            target_state = []\n            for i, value in enumerate(line):\n                if i < len(features):\n                    if \".\" in value:\n                        min_val = int(value[:value.index(\".\")])\n                        max_val = int(value[value.rindex(\".\")+1:])\n                        feature_domains.append([str(val) for val in list(range(min_val, max_val+1))])\n                    else:\n                        feature_domains.append([value])\n                else:\n                    target_state.append(value)\n\n            # Generate all combinations of domains\n            feature_states = [list(i) for i in list(itertools.product(*feature_domains))]\n            for i in feature_states:\n                data.append((i,target_state))\n                \n        elif data_loading == \"type3\":\n\n                line = line.split(\",\")\n                line = [i.strip(\" \") for i in line]\n\n                # Extract features names\n                if line_id == 1:\n                    features = line[1:NB_FEATURES]\n                    for i, token in enumerate(line[1:]):\n                        if i >= len(features):\n                            target_label = token\n                            if SHORTEN_TARGET_LABEL:\n                                target_label = target_label[:target_label.index(\"=\")-1]\n                            targets.append((target_label, [\"0\",\"1\"]))\n                    continue\n\n                # Extract features domains\n                if line_id == 2:\n                    for i, token in enumerate(line[1:]):\n                        if i < len(features):\n                            min_val = int(token[:token.index(\".\")])\n                            max_val = int(token[token.rindex(\".\")+1:])\n                            features[i] = (features[i], [str(val) for val in list(range(min_val,max_val+1))])\n                    continue\n\n                # Extract data line\n                if len(line[1:]) != len(features)+len(targets):\n                    print(\"Error line\",line_id,\" expected\",len(features)+len(targets),\"columns, got \",len(line[1:]))\n                    continue\n\n                feature_domains = []\n                target_state = []\n                for i, value in enumerate(line[1:]):\n                    if i < len(features):\n                        if \".\" in value:\n                            min_val = int(value[:value.index(\".\")])\n                            max_val = int(value[value.rindex(\".\")+1:])\n                            feature_domains.append([str(val) for val in list(range(min_val, max_val+1))])\n                        else:\n                            feature_domains.append([value])\n                    else:\n                        target_state.append(value)\n\n                # Generate all combinations of domains\n                feature_states = [list(i) for i in list(itertools.product(*feature_domains))]\n                for i in feature_states:\n                    data.append((i,target_state))\n        \n    return features, targets, data\n\n","4dd02f5d":"if DATA_LOADING in [\"type1\", \"type2\", \"type3\"]:\n    features, targets, data = extract_from_csv(CSV_FILE, DATA_LOADING, NB_FEATURES)\n    feature_names = [i for i,j in features]\n    target_names = [i for i,j in targets]\n    print(\"features:\", features)\n    print(\"targets:\", targets)\n    #print(\"data:\", data)\nelif DATA_LOADING in [\"type4\"]:\n    df = pd.read_csv(CSV_FILE)\n    header = list(df.columns)\n    feature_names = header[:NB_FEATURES]\n    target_names = header[NB_FEATURES:]\n    print(\"Features: \", feature_names)\n    print(\"Targets: \", target_names)\nelse:\n    df = pd.read_csv(CSV_FILE)\n    header = list(df.columns)[HEADER_OFFSET:]\n    first_target = next(x for x in header if x[0] == \"f\")\n    nb_features = header.index(first_target)\n    feature_names = header[:nb_features]\n    target_names = header[nb_features:]\n    features = [(x,[\"0\",\"1\"]) for x in feature_names]\n    targets = [(x,[\"0\",\"1\"]) for x in target_names]\n    print(\"Features: \", feature_names)\n    print(\"Targets: \", target_names)\n    print(\"Features domain: \", features)\n    print(\"Targets domain: \", targets)\n    df","66e7db79":"if DATA_LOADING in [\"type1\", \"type2\"]:\n    dataset = pylfit.preprocessing.transitions_dataset_from_array(data=data, feature_domains=features, target_domains=targets)\nelse:\n    dataset = pylfit.preprocessing.transitions_dataset_from_csv(path=CSV_FILE, feature_names=feature_names, target_names=target_names)\n    features = dataset.features\n    targets = dataset.targets\n#dataset.features = features\n#dataset.targets = targets\n#dataset.summary()","5f18228a":"model = pylfit.models.DMVLP(features=dataset.features, targets=dataset.targets)\nmodel.compile(algorithm=ALGORITHM) # model.compile(algorithm=\"gula\")\nmodel.fit(dataset, verbose=1)\nmodel.summary()","61f0df47":"if INPUT_CSV == \"test1_mucus_LFIT_no_selection.csv\":\n    requests = [\n    ([(\"calcium\", \"0\"), (\"Operon-e\", \"0\")],[(\"AG(!operon=2))\",\"1\")]), # test1_mucus_LFIT_no_selection\n    ([(\"calcium\", \"0\"), (\"Operon-e\", \"2\")],[(\"(AG(!operon=0))\",\"1\")]), # test1_mucus_LFIT_no_selection\n    ([(\"calcium\", \"1\")],[(\"(AF(AG(operon=2)))\",\"1\")]), # test1_mucus_LFIT_no_selection\n    ([],[(\"AG(!operon=2))\",\"1\")]), # test1_mucus_LFIT_no_selection\n    ([],[(\"(AG(!operon=0))\",\"1\")]), # test1_mucus_LFIT_no_selection\n    ([],[(\"(AF(AG(operon=2)))\",\"1\")]), # test1_mucus_LFIT_no_selection\n    ]\n\nif INPUT_CSV == \"test2_mucus_LFIT_selection.csv\":\n    requests = [\n    ([],[(\"(AG(!operon=2))\",\"1\")]), # test2_mucus_LFIT_selection\n    ([],[(\"(AG(!operon=0))\",\"1\")]), # test2_mucus_LFIT_selection\n    ([],[(\"(AF(AG(operon=2)))\",\"1\")]), # test2_mucus_LFIT_selection\n    ]\n    \nif INPUT_CSV == \"mucus-manu.csv\":\n    requests = [\n    ([(\"Calcium\", \"0\")],[(\"f113\",\"1\")]),\n    ([(\"Calcium\", \"0\")],[(\"f124\",\"1\")]),\n    ([(\"Calcium\", \"1\")],[(\"f112\",\"1\")]),\n    ([(\"Calcium\", \"0\"), (\"Operon-env\",\"0\")],[(\"f112\",\"1\")]),\n    ([],[(\"f123\",\"1\")]),\n    ([(\"Calcium\", \"0\")],[(\"f125\",\"1\")]),\n    ([(\"Calcium\", \"0\")],[(\"f126\",\"1\")]),\n    ]\n    \nif INPUT_CSV == \"cycle-cell.csv\":\n    requests = [\n    ([],[(\"Ftest\",\"1\")]),\n    ]\n    \n#request = [(\"f123\",\"1\"),(\"f224\",\"1\")] # mucusV1.csv\n#request = [(\"f123\",\"1\"),(\"f111\",\"1\")] # mucusV2.csv\n#request = [(i,\"1\") for i,j in targets] # All target to 1","a0daf7f4":"from pylfit.objects.rule import Rule\nfrom pylfit.algorithms.algorithm import Algorithm\n\n# Encode request into var\/val ids\nencoded_requests = [([(var_id,features[var_id][1].index(val)) for var,val in request[0] for var_id in [feature_names.index(var)]], \n                   [(var_id,targets[var_id][1].index(val)) for var,val in request[1] for var_id in [target_names.index(var)]]) for request in requests]\n\nprint(\"Input file: \", INPUT_CSV)\nprint()\noutputs = []\nfor request_id, encoded_request in enumerate(encoded_requests):\n    print(\"Request\", requests[request_id])\n    # Extract rules with requested target\n    targets_compatible_rules = [(var_id,val_id, [r for r in model.rules if r.head_variable == var_id and r.head_value == val_id]) for var_id,val_id in encoded_request[1]]\n    # Remove rules with other value of requested features\n    #compatible_rules = [r for r in compatible_rules if r.body]\n    compatible_rules = []\n    for var_id,val_id,rules in targets_compatible_rules:\n        valid_rules = []\n        for r in rules:\n            valid = True\n            #print(r)\n            for (var,val) in encoded_request[0]:\n                if r.has_condition(var) and r.get_condition(var) != val:\n                    valid = False\n                    break\n            if valid:\n                valid_rules.append(r)\n        compatible_rules.append((var_id,val_id,valid_rules))\n\n\n    #print(\"Targets rules\",compatible_rules)\n\n    # Search for compatible rules set\n    compatible_rules = [z for x,y,z in compatible_rules]\n    nb_combinations = np.prod([len(l) for l in compatible_rules])\n    done = 0\n\n    valid_combinations = []\n    for combination in itertools.product(*compatible_rules):\n        done += 1\n        #print(done,\"\/\",nb_combinations)\n\n        condition_variables = set()\n        conditions = set()\n        valid_combo = True\n        for r in combination:\n            for var,val in r.body:\n                if var not in condition_variables:\n                    condition_variables.add(var)\n                    conditions.add((var,val))\n                elif (var,val) not in conditions:\n                    valid_combo = False\n                    break\n            if not valid_combo:\n                break\n\n        if valid_combo:\n            #print(\"valid combo found: \", combination)\n            valid_combinations.append(combination)\n            #break\n\n    # Extract combination of conditions\n    output_encoded = [sorted([cond for r in combi for cond in r.body]) for combi in valid_combinations]\n    output_decoded = [[(dataset.features[i][0],dataset.features[i][1][j]) for i,j in combi] for combi in output_encoded]\n    \n    output_weighted = []\n    for combi_id, combi in enumerate(output_encoded):\n        r = Rule(-1,-1,len(dataset.features),[cond for cond in combi])\n        weight = 0\n        for s1,s2 in dataset.data:\n            s = Algorithm.encode_state(s1, dataset.features)\n            if r.matches(s):\n                weight += 1\n        output_weighted.append((weight,output_decoded[combi_id]))\n\n    output_weighted.sort(reverse=True)\n    output_weighted\n    \n    outputs.append(output_weighted)\n    display(output_weighted)\n    print()","1017d65b":"print(\"> Input file: \", INPUT_CSV)\nprint()\nfor request_id, output_decoded in enumerate(outputs):\n    print(\">> Minimal parameters combinations for request:\", requests[request_id])\n    print()\n    for i in output_decoded:\n        break\n        msg = \">>> \"\n        for x,y in i:\n            msg += (x+\"=\"+y) + \" and \"\n        print(msg[:-5])\n    print()","7d9211f7":"# TODO:\n# - output weight of patern OK\n# - When no combi valid, output why\n# - fix feature in request","339d976d":"## 1.1 User variables","c220a39d":"## Output","52040e95":"## 2.1 Extract features\/targets columns","b749ef2f":"# 3.1 Create dataset","c87e78ef":"## 1.2 Installs","e326f6d6":"## 1.4 Constants","12ab2183":"## 1.3 Imports","d551f889":"# 3. PyLFIT","85c89738":"# 3.2 Model learning","4fb65b67":"# 3.4 Request","9af4d6cb":"# 2. Data Preprocessing","c5e6cc54":"# 1. Settings"}}