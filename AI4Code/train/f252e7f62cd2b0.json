{"cell_type":{"f65e4c14":"code","1ca5204f":"code","1cf30659":"code","212d1fe3":"code","f820b3fb":"code","138973df":"code","b4801bf3":"code","2c9a79d7":"code","8c684cee":"code","b3c20b5e":"code","aa9be8c6":"code","2c050cb0":"code","699b51bf":"code","5653a3a2":"code","b5ab4b28":"code","211781b7":"markdown","8e6a44c7":"markdown","7bf0bf4b":"markdown","d1c8a0c7":"markdown","50e8f0d2":"markdown","d99d1c95":"markdown"},"source":{"f65e4c14":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,RepeatVector\nfrom keras.layers import Flatten\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","1ca5204f":"data=pd.read_csv('..\/input\/sales_train.csv')\n","1cf30659":"data.info()","212d1fe3":"data.head()","f820b3fb":"data['date'] = pd.to_datetime(data['date'])","138973df":"data.info()","b4801bf3":"data.set_index(['date'],inplace=True)\ndata = data['item_cnt_day'].resample('D').sum()\ndf=pd.DataFrame(data)","2c9a79d7":"plt.figure(figsize=(16,8))\ndf['item_cnt_day'].plot()\nplt.xlabel('Date')\nplt.ylabel('Number of Products Sold')\nplt.show()","8c684cee":"df_1=df.values\ndf_1=df_1.astype('float32')\n\nscaler = MinMaxScaler(feature_range=(-1,1))\nts = scaler.fit_transform(df_1)","b3c20b5e":"df.info()","aa9be8c6":"timestep = 30\n\nX= []\nY=[]\n\nraw_data=ts\n\nfor i in range(len(raw_data)- (timestep)):\n    X.append(raw_data[i:i+timestep])\n    Y.append(raw_data[i+timestep])\n\n\nX=np.asanyarray(X)\nY=np.asanyarray(Y)\n\n\nk = 850\nXtrain = X[:k,:,:]  \nYtrain = Y[:k]    ","2c050cb0":"model = Sequential()\nmodel.add(Conv1D(filters=128, kernel_size=2, activation='relu', input_shape=(30, 1)))\nmodel.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\nmodel.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(Xtrain, Ytrain, epochs=200, verbose=0)","699b51bf":"Xtest = X[k:,:,:]  \nYtest= Y[k:]  ","5653a3a2":"preds = model.predict(Xtest)\npreds = scaler.inverse_transform(preds)\n\n\nYtest=np.asanyarray(Ytest)  \nYtest=Ytest.reshape(-1,1) \nYtest = scaler.inverse_transform(Ytest)\n\n\nYtrain=np.asanyarray(Ytrain)  \nYtrain=Ytrain.reshape(-1,1) \nYtrain = scaler.inverse_transform(Ytrain)\n\nmean_squared_error(Ytest,preds)","b5ab4b28":"from matplotlib import pyplot\npyplot.figure(figsize=(20,10))\npyplot.plot(Ytest)\npyplot.plot(preds, 'r')\npyplot.show()","211781b7":"### Let's import the libraries","8e6a44c7":"# Time Series Predictions\n\nWe're going to try to predict the daily time series. We are going to use CNN models.\n\n# CNN Model\n\nA one-dimensional CNN is a CNN model that has a convolutional hidden layer that operates over a 1D sequence. This is followed by perhaps a second convolutional layer in some cases, such as very long input sequences, and then a pooling layer whose job it is to distill the output of the convolutional layer to the most salient elements.\n\nThe convolutional and pooling layers are followed by a dense fully connected layer that interprets the features extracted by the convolutional part of the model. A flatten layer is used between the convolutional layers and the dense layer to reduce the feature maps to a single one-dimensional vector.\n\nWe can define a 1D CNN Model for univariate time series forecasting as follows.\n\n\n\n","7bf0bf4b":"We are going to convert hourly data to daily data","d1c8a0c7":"### Thank you \n\n","50e8f0d2":"As we can see, the date is object and we have to convert date column to datetime. So we are going to use to_datetime function for the convert.","d99d1c95":"If you like it please vote "}}