{"cell_type":{"6c45266d":"code","ff9b282b":"code","28eb82c7":"code","5792cfd7":"code","2be40380":"code","0d53b622":"code","92c82da0":"code","e45df049":"code","7c42e6dc":"code","b0c89f3c":"code","980fa284":"code","d9e2c484":"code","53a211e2":"code","27114494":"code","5b510796":"code","3bfa37ae":"code","9f54cb6c":"code","104f65c0":"code","04416958":"code","8b0d99b0":"code","3ba9e269":"code","0c377883":"code","0a73c5c2":"code","6710eee4":"code","caf0acfc":"code","e0b27af2":"code","d2972d0b":"code","3bed831e":"code","b500edbe":"code","7775e777":"code","893bfadd":"code","4ffaba18":"code","5295146c":"code","737649e9":"code","0a0ff3e3":"code","6ed18a2e":"code","5b1e210c":"code","8bad05cb":"code","9dd5bd7a":"code","5106a922":"code","07a7a3cb":"code","5f5de64f":"code","a72e8419":"code","be967212":"code","4894de08":"code","58ab87ec":"code","119d6f4c":"code","f7a85f45":"code","08416161":"code","37b593f6":"code","e9454a14":"code","4757c79b":"code","d113231b":"code","b0d8d51f":"code","ebab0ab9":"code","e7fa6d94":"code","48f06413":"code","273cdce7":"code","9f387dd9":"code","15ec7777":"code","15642814":"code","e82b1230":"code","563aaaf7":"code","b8f5ac04":"code","723f82d5":"code","bf984abc":"markdown","f29d7da3":"markdown","7e615192":"markdown","0149aa32":"markdown","c29dd95c":"markdown","f54927a9":"markdown","7c1f1764":"markdown","58872055":"markdown","53ac4b99":"markdown","0a2ae91c":"markdown","6dd41c78":"markdown","1989fd87":"markdown","deddac90":"markdown","8303d8ff":"markdown","b4b2e546":"markdown","ba4a252b":"markdown","12d2d3a9":"markdown","dcfd5d4e":"markdown","16cba62e":"markdown","70c60fc7":"markdown","93342345":"markdown","efc4d63b":"markdown","05050602":"markdown","65627ff3":"markdown","f4ecda31":"markdown","1f599f06":"markdown","395f7804":"markdown","630f1dc8":"markdown","92794df5":"markdown","2c1cc620":"markdown","1589143b":"markdown","d0f4a749":"markdown","2e7daf1b":"markdown","b2732566":"markdown","a0233224":"markdown","0a58b8be":"markdown","09b8e5dc":"markdown","5f6b7f6e":"markdown","4e56f2dd":"markdown"},"source":{"6c45266d":"# linear algebra\nimport numpy as np \n# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd \n# data visualization\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly.subplots import make_subplots\ninit_notebook_mode(connected=True)","ff9b282b":"Auto = pd.read_csv('\/kaggle\/input\/automobile-dataset\/Automobile_data.csv')","28eb82c7":"Auto.head()","5792cfd7":"Auto.info()","2be40380":"Auto.describe()","0d53b622":"# Let's convert the special characters to NaN\nmissing_values = ['?','--','-','??','.']\n\n# new dataframe after replacing special characters from the set\nAuto = Auto.replace(missing_values, np.nan)\nAuto.head()","92c82da0":"# Let us look at the total missing values in the data set\n# Looking for any missing values in the dataframe column\nmiss_val = Auto.columns[Auto.isnull().any()]\n\n# printing out the columns and the total number of missing values of all the column\nfor column in miss_val:\n    print(column, Auto[column].isnull().sum())","e45df049":"# defining two empty lists for columns and its values\nnan_columns = []\nnan_values = []\n\nfor column in miss_val:\n    nan_columns.append(column)\n    nan_values.append(Auto[column].isnull().sum())\n\n# plotting the graph\nfig, ax = plt.subplots(figsize=(15,8))\nplt.bar(nan_columns, nan_values, color = 'purple', width = 0.5)\nax.set_xlabel(\"Column Names\")\nax.set_ylabel(\"Count of missing values\")\nax.set_title(\"Variables with missing values\");","7c42e6dc":"# Lets take the median to fill the missing values for the following features.\n# In this process, we use the limit direction as both.\n\n# Normalized losses\nmedian_value= Auto['normalized-losses'].median()\nAuto['normalized-losses']=Auto['normalized-losses'].fillna(median_value)\n\n# Bore\nmedian_value= Auto['bore'].median()\nAuto['bore']=Auto['bore'].fillna(median_value)\n\n# Stroke\nmedian_value= Auto['stroke'].median()\nAuto['stroke']=Auto['stroke'].fillna(median_value)\n\n# Horsepower\nmedian_value= Auto['horsepower'].median()\nAuto['horsepower']=Auto['horsepower'].fillna(median_value)\n\n# Peak-RPM\nmedian_value= Auto['peak-rpm'].median()\nAuto['peak-rpm']=Auto['peak-rpm'].fillna(median_value)\n\n# Price\nmedian_value= Auto['price'].median()\nAuto['price']=Auto['price'].fillna(median_value)","b0c89f3c":"# Looking what body_style and make our missing values have\nAuto[['make','body-style']][Auto['num-of-doors'].isnull()==True]","980fa284":"# Looking for number of doors for a sedan model of Mazda\nAuto['num-of-doors'][(Auto['body-style']=='sedan') & (Auto['make']=='mazda')]","d9e2c484":"# Similarly, looking for number of doors for a sedan model of Dodge\nAuto['num-of-doors'][(Auto['body-style']=='sedan') & (Auto['make']=='dodge')]","53a211e2":"Auto['num-of-doors'] = Auto['num-of-doors'].fillna('four')\n\n# dictionary mapping for num of doors\na=Auto['num-of-doors'].map({'two':2,'four':4})\nAuto['num-of-doors']=a","27114494":"# converting data type to int\nAuto['num-of-doors'] = Auto['num-of-doors'].astype(str).astype(int)   ","5b510796":"Auto.info()","3bfa37ae":"# Heatmap\n\nplt.subplots(figsize=(20,8))\nsns.heatmap(Auto.isnull(),yticklabels=False,cbar=False,cmap='Greens_r')","9f54cb6c":"Auto.isna().sum()","104f65c0":"from scipy.stats import norm\nfrom scipy import stats\n\nsns.distplot(Auto['price'], fit=norm);\nfig = plt.figure()","04416958":"#skewness and kurtosis\nprint(\"Skewness: %f\" % Auto['price'].skew())\nprint(\"Kurtosis: %f\" % Auto['price'].kurt())","8b0d99b0":"sns.pairplot(Auto[['city-mpg', 'engine-size', 'wheel-base']], palette='Set1')","3ba9e269":"sns.pairplot(Auto[['highway-mpg', 'engine-size', 'wheel-base']], palette='Set1')","0c377883":"fig = px.histogram(Auto, x=\"make\", title='Count of cars based on OEM')\nfig.show()","0a73c5c2":"# Comparing Symboling and make\n\nfig, ax = plt.subplots(figsize=(30,10)) \nsns.violinplot(x=\"make\", y=\"symboling\", data=Auto, palette='deep')\n\nplt.title(\"Risk factor based on make\")","6710eee4":"label = Auto[\"body-style\"].unique()\nsizes = Auto[\"body-style\"].value_counts().values\n\n# Now we could define the Pie chart\n# pull is given as a fraction of the pie radius. This serves the same purpose as explode \nfig_pie1 = go.Figure(data=[go.Pie(labels=label, values=sizes, pull=[0.1, 0, 0, 0])])\n# Defining the layout\nfig_pie1.update_layout(title=\"Body-style Propotion\",    \n        font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#7f7f7f\"\n    ))\nfig_pie1.show()","caf0acfc":"# Plotting multiple violinplots, including a box and scatter diagram\nfig_vio1 = px.violin(x = Auto['body-style'], y = Auto[\"price\"], box=True, points=\"all\")\n# Defining the layout\nfig_vio1.update_layout(\n    title=\"Body-style and Price\",\n    xaxis_title=\"Body-style\",\n    yaxis_title=\"Price\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18\n    ))\nfig_vio1.show()","e0b27af2":"# since the datatype of horsepower and peak-rpm is object, let's convert it first into integer\nAuto['horsepower'] = Auto['horsepower'].astype(int)\nAuto['peak-rpm'] = Auto['peak-rpm'].astype(int)\n\n# let's create the new column for torque\nAuto['torque'] = ((Auto['horsepower'] * 5252) \/ Auto['peak-rpm'])\nAuto.head()","d2972d0b":"# Extracting the first 10 largest Horsepower values\nHP = Auto['horsepower'].nlargest(10)\nHP_Index = [129,49,126,127,128,105,73,74,15,16]\n# Extracting the corresponding 10 values from the column 'make'\nmake_hp = Auto['make'].iloc[HP_Index]\n\n#creating a new dataframe with this values.\ndata = {'HP': HP, 'Make':make_hp} \ndf = pd.DataFrame(data) \ndf","3bed831e":"plt.subplots(figsize=(15,8))\nax = sns.barplot(x=\"Make\", y=\"HP\", data=df, palette='deep')\nax.set_xlabel(\"Make\")\nax.set_ylabel(\"HP\")\nax.set_title(\"Fastest accelerating car\");","b500edbe":"sns.pairplot(Auto[['horsepower','torque','peak-rpm']], palette='Set1', kind=\"reg\")","7775e777":"plt.figure(figsize=(30,15))\nsns.heatmap(Auto.corr(),annot=True,cmap='viridis',linecolor='white',linewidths=1);","893bfadd":"# Calculating the combined mpg and creating a new dataframe\nComb_mpg = ((Auto['highway-mpg'] * 0.4) + (Auto['city-mpg'] * 0.55))\ndata1 = {'comb-mpg':Comb_mpg, 'make':Auto['make'], 'fuel-type':Auto['fuel-type']}\n\ndf1 = pd.DataFrame(data1) # for the easiness of visualising","4ffaba18":"# We use a violin plot mwith hue as the fuel type to know which car is fuel efficient\n\n#fuel_effi = df1.nlargest(5, ['comb-mpg'])  ----------- to know the first most fuel efficient cars\n#fuel_effi\n\nplt.subplots(figsize=(30,10))\nvx = sns.violinplot(x=\"make\", y=\"comb-mpg\", data=df1,hue='fuel-type',split=True,palette='Set1')\nvx.set_xlabel(\"Make\")\nvx.set_ylabel(\"Comb-mpg\")\nvx.set_title(\"Fuel efficient cars\");","5295146c":"fig = px.violin(Auto, y=\"price\", x=\"drive-wheels\", box=True, points=\"all\", \n                color='fuel-type', hover_data=Auto.columns)\nfig.show()","737649e9":"Auto.head()","0a0ff3e3":"Auto.info()","6ed18a2e":"#Let's split fuel type, drive wheels and engine location \n\nfuel_type = pd.get_dummies(Auto['fuel-type'], drop_first=True) \n# this would remove the diesel column after spliting the fuel-type column, which is automatically predictable\n\ndrive_wheels = pd.get_dummies(Auto['drive-wheels'], drop_first=True) \n# this would remove the 4wd column after spliting the drive-wheels column, which is automatically predictable\n\nengine_location = pd.get_dummies(Auto['engine-location'], drop_first=True) \n# this would remove the front column after spliting the engine-location column, which is automatically predictable\n\naspiration = pd.get_dummies(Auto['aspiration'], drop_first=True) \n# this would remove the std column after spliting the aspiration column, which is automatically predictable","5b1e210c":"Auto.drop(['fuel-type','drive-wheels','engine-location', 'aspiration'],axis=1,inplace=True)","8bad05cb":"Auto_df = pd.concat([Auto,fuel_type,drive_wheels,engine_location, aspiration], axis=1)","9dd5bd7a":"Auto_df.head()","5106a922":"# converting the following data type to float\nAuto_df['normalized-losses'] = Auto_df['normalized-losses'].astype(float)\nAuto_df['bore'] = Auto_df['bore'].astype(float)\nAuto_df['stroke'] = Auto_df['stroke'].astype(float)\nAuto_df['price'] = Auto_df['price'].astype(float)","07a7a3cb":"Auto_df.info()","5f5de64f":"Auto_df.select_dtypes(include='object')","a72e8419":"Auto_df.drop(['make','body-style','engine-type','num-of-cylinders', 'fuel-system'],axis=1,inplace=True)","be967212":"Auto_clean = pd.concat([Auto_df],axis=1)","4894de08":"Auto_clean.info()","58ab87ec":"from sklearn.model_selection import train_test_split","119d6f4c":"X = Auto_clean.drop('price', axis=1) # This would consider all the columns except price\ny = Auto_clean['price']","f7a85f45":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)","08416161":"from sklearn.linear_model import LinearRegression","37b593f6":"lm = LinearRegression()","e9454a14":"lm.fit(X_train,y_train)","4757c79b":"# printing the y intercept\nprint(\"y intercept is :\", lm.intercept_)","d113231b":"# printing the coefficients or the slope value\nprint(\"coefficients are :\", lm.coef_)","b0d8d51f":"# creating the dataframe with the coefficient\ncoeff_df = pd.DataFrame(lm.coef_,X.columns,columns=['Coefficient'])\ncoeff_df","ebab0ab9":"predictions = lm.predict(X_test)","e7fa6d94":"# lets look at the predictions\npredictions","48f06413":"# now let us look at the y_test\ny_test","273cdce7":"plt.subplots(figsize=(10,8))\nax = sns.scatterplot(x=y_test, y=predictions,\n                    sizes=(20, 200), legend=\"full\", palette=\"Set2\")","9f387dd9":"# Residuals are the difference between the actual and the predicted values\nsns.jointplot(x=y_test,y=predictions,kind='hex')","15ec7777":"# ploting the prediction error\npred_error = y_test - predictions\n\nsns.distplot((pred_error))","15642814":"# Linear regression using Statsmodel\nimport statsmodels.api as sm\n\n# Unlike sklearn that adds an intercept to our data for the best fit, statsmodel doesn't. We need to add it ourselves\n# Remember, we want to predict the price based off our features.\n# X represents our predictor variables, and y our predicted variable.\n# We need now to add manually the intercepts\n\nX_endog = sm.add_constant(X_train)","e82b1230":"# fitting the model\nls = sm.OLS(y_train, X_endog).fit() # OLS = Ordinary Least Squares\n# summary of the model\nls.summary() ","563aaaf7":"# Printing the P values \nprint(ls.pvalues)","b8f5ac04":"from sklearn import metrics","723f82d5":"# Mean Absolute Error (MAE)\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\n\n# Mean Squared Error (MSE)\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\n\n# Root Mean Squared Error (RMSE)\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n\n# R-squared (R2)\nprint('R^2 Score:', metrics.r2_score(y_test, predictions))","bf984abc":"**Let's look at some of the details of the dataset**","f29d7da3":"From this plot, it is quite clear that volvo outperform other OEMs, regarding the safety of the cars produced.\n\nNow we could take a look at the body style for the cars.","7e615192":"From the above barplot there are totally 7 features with missing values. Taking a close look at the values that we have here, it is evident all the missing feature variables can't be extracted from other available features but number of doors. We could get the value of **num-of-doors**, from features like **make** and **body-style**. For the temporal series of data, since we have outliers which lie an abnormal distance from other values in a random sample from population, it is recommendable to use median value to fill these missing data. ","0149aa32":"The visualization says the rear wheel drive is costly in make, regardless whether it is petrol or diesel. This is because, getting power to the rear wheels need putting up the long driveshaft, fixing differential in the back and then connecting wheels to differentials just like in Front wheel drive. This would endure extra cost even during maintenance as well.","c29dd95c":"From the Correlation matrix it is evident that there are variables that are strongly correlated like city mpg and highway mpg. Likewise, there are variables that are negatively correlated too, like torque and mpg. For a better insight on looking at the fuel efficient car maker, let's get the average mpg and do some visulaisation. \n\n**The Combined MPG value is the most prominent for the purpose of quick and easy comparison across vehicles. Combined fuel economy is a weighted average of City and Highway MPG values that is calculated by weighting the City value by 55% and the Highway value by 45%.**\n\nWe could from the available mpg values, calculate the combined fuel economy. ","f54927a9":"# Predictions from our Model\n\nLet's grab predictions from the test set and see how well it did","7c1f1764":"# Introduction\n\nIn this Kernel I would like to explain few methods for analysing, cleaning, visualizing a dataset and Introduction to linear regression. For Linear regression I have used a sklearn and statsmodel comparison in evaluating the model, to check the commensuration. Furthermore as an important part of the linear regression, I have also included the claculation of Regression Evaluation Metrics.\n\nHere is the dataset from 1985 Ward's Automotive Yearbook. This data set consists of three types of entities: \n1. the specification of an auto in terms of various characteristics\n2. its assigned insurance risk rating \n3. its normalized losses in use as compared to other cars \n\nMoreover there is a risk factor assigned for the cars, based on its price. ","58872055":"**Training and Predicting data**","53ac4b99":"# Regression Evaluation Metrics\n\nHere are the common evaluation metrics for regression problems:\n\n* **Mean Absolute Error (MAE)** is the mean of the absolute value of the errors\n \n* **Mean Squared Error (MSE)** is the mean of the squared errors\n\n* **Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors:\n\n* **R-squared (R2)** indicates the percentage of the variance in the dependent variable that the independent variables explain collectively\n \nComparing these metrics:\n\n1. MAE is the easiest to understand, because it's the average error.\n2. MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n3. RMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n4. R-Squared could be interpreted so -\n    *  0 : doesn't fit the data\n    *  1 : better fit for the data\n    *  -1 : due to overfitting of data\n   \nAll of these are loss functions, because we want to minimize them   ","0a2ae91c":"**Interpreting the coefficients**\n\nLet's look at how the following features affect the price of the car. \n\n* Holding all other features fixed, a 1 unit increase in **engine-size** is associated with an *increase of $76.828156* in the price\n\n* Holding all other features fixed, a 1 unit increase in **city-mpg** is associated with an *increase of $29.058026* in the price","6dd41c78":"# Best racer!!!\n\nOut of curiosity, I am interested in looking for the fastest-accelerating car. \n\n**So what Matters More for Acceleration: Horsepower or Torque??**\n\nIt is a complicated question to find out what actually matters here. Inorder to answer this, we have to know that we\u2019re interested in is the force at the wheels. It is to be noted that gearing is the translator between the engine and the wheels. Ultimately Gearing magnifies torque, which is why it\u2019s so important in racing. So the transmission only sees what\u2019s coming off the engine, while the wheels see the resulting force combination of the engine plus the transmission. That\u2019s what horsepower represents! This is because horsepower encompasses not only the engine\u2019s torque but the total torque that gets delivered to the wheels.\n\n**Let's add a new column representing Torque**\n\nHere in the dataset we have no column representing the torque. However we could calculate the Torque from the Horsepower and the RPM, using the following equation\n\n***Torque = (Horsepower x 5252) \u00f7 RPM***","1989fd87":"Now lets move on to the next feature (num-of-doors), which is predictable from the available features (make and body-style). In this case let us look at the make and body style corresponding to the missing variable, num-of-doors. We already know there are 2 values missing in the column for num-of-doors feature.","deddac90":"Let's get our environment ready with the libraries we'll need and then import the data!","8303d8ff":"From the visualization we found out that convertible is the most produced type. However the price for hardtop is more than the convertible, making it the most expensive body type in the class","b4b2e546":"**The p value is extremely small and so it rejects the null hypothesis, supporting the fact that the data represents the effect**","ba4a252b":"**Few features which are considered important are sorted and the data type is converted to float**","12d2d3a9":"# Check out the Data","dcfd5d4e":"**Using Statsmodel OLS for Regression**","16cba62e":"# Exploratory Data Analysis\n**Let's explore the data!**\n\nFirst of all look at the distribution of price","70c60fc7":"**Model Evaluation**\n\nLet's evaluate the model by checking out it's coefficients and how we can interpret them. ","93342345":"**Residuals**\n\nLet's quickly explore the residuals to make sure everything was okay with our data.\n","efc4d63b":"From the above plot, we could see that Japanese based Honda is the most fuel efficient car, followed by the amrican brand Chevrolet. The second and third runner up is again another Japanese brand, Nissan and Toyota respectively. \nIt is also noticeable that the Petrol engine cars are much efficient than its counterpart (Diesel) by a margin close to 4 mpg.\n\nNow move on to the drive wheels of the cars and let's look at the expensive category in this.","05050602":"Porsche has the highest Horsepower and so it is the fastest accelerated car, followed by Jaguar. \n\n# Correlation between different features\n\nUsing a heatmap, the correlation is calculated","65627ff3":"This tells that the most common price range observed would be certainly between 5000 and 15000. It must be noted that the number of cars in the expensive price range is too less. The model seems to be more accurate in predicting the prices of the cars. ","f4ecda31":"Let's look at the relationship between engine size, wheel base and mileage of the car.\nHere is a brief idea about the features we plot here:\n\n1. Engine size: The size of an engine is measured in cubic centimetres (cc) and refers to the total volume of air and fuel that\u2019s pushed through the engine by its cylinders. For example, a 1,000cc engine has the capacity to displace one litre - or 1,000 cubic centimetres - of this air-fuel mixture.\n\n2. Wheelbase: It is the horizontal distance between the centers of the front and rear wheels.\n![image.png](attachment:image.png)","1f599f06":"Now let us look at the car make, which OEM produces the more number of cars","395f7804":"# Data Cleaning\n\nData cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting.\n\n**The first step would be to convert categorical features to dummy variables, else the machine learning algorithm won't be able to directly take in those features as inputs.**","630f1dc8":"From the two results, it is clear that for a sedan body style, there are 4 doors. So we now replace the Nan value to 4.","92794df5":"In order to create a linear regression model, we now to need to remove all irrelevant data. For this purpose, all the columns with 'object' datatype are removed.","2c1cc620":"# Missing Data\nBefore analysing and visulaizing the data, let us look for the missing values in the data set. From the dataset it is noticed that there are few missing values, which are represented using special characters. The first aim is to replace all the special characters to NaN and then to choose the best method in removing the missing values.\n","1589143b":"**The given data is deviating from normal distribution, with positive skewness and with a positive kurtosis value, we have a leptokurtic distribution**.","d0f4a749":"# Building Linear Regression Model\n\nThe dataset available is splitted into training and test set. We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable, in this case the price column.\n\n\n\n**Train-Test Split using Sklearn**","2e7daf1b":"We have found that there are two missing values for the num-of-doors column. The body style and make is also clear. BInorder to decide the missing value, let us look at the num of doors corresponding to the sedan model of both dodge and mazda. ","b2732566":"# Contents\n\nThe complete steps in estimating the price of the cars (our goal) is achieved through the following steps. \n\n**1. Check out Data** - looking around the Dataset and its information.\n\n**2. Missing Data** - Handling the missing data using different techniques.\n\n**3. Exploratory Data Analysis (EDA)** - exploring the different features and its behaviour.\n\n**4. Correlation between different features** - using Heatmaps in studying correlation.\n\n**5. Data cleaning** - for model development; irrelevant, incomplete, incorrect or inaccurate variables are identified incomplete and these data are replaced, modifying, or deleted. \n\n**6. Building Regression Model** - the dataset is splitted for training and testing.\n\n**7. Predictions from Model** - This is to look at how well the model behaved in predicting the outcome.\n\n**8. Calculating the P-Value** - This is to ensure whether the data really represent the observed effect.\n\n**9. Regression Evaluation Metrics** - These are the common evaluation metrics for regression problems.","a0233224":"From the figure, we can implicitly say the value of coefficients and intercept we found earlier in sklearn commensurate with the output from statsmodels.","0a58b8be":"In both the cases, it is important to note that the mileage of car is inversely propotional to the engine size and wheel base.\nEventhough a larger engine is more powerful, they generally use up more fuel than smaller ones. Similarly when the wheel base is big, then the mileage drops.","09b8e5dc":"From the above codes, we have filled up all those missing values, with meaningful values for analysis. But let's check it before proceeding to the next step.\n","5f6b7f6e":"Toyota is the leading producer of cars, followed by nissan and then mazda\n\n**Risk-factor study**\n\nCars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process \"symboling\". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.****","4e56f2dd":"# Calculating the P-Value\nThe P Value basically helps to answer the question: \u2018Does the data really represent the observed effect?\u2019. The P Value is the probability of seeing the effect(E) when the null hypothesis is true. A sufficiently low value is required to reject the null hypothesis. So, when the p-value is low enough, we reject the null hypothesis and conclude the observed effect holds. This level of \u2018low enough\u2019 cutoff is called the alpha level, and you need to decide it before conducting a statistical test.\n\nAlpha level is the cutoff probability for p-value to establish statistical significance for a given hypothesis test. For an observed effect to be considered as statistically significant, the p-value of the test should be lower than the pre-decided alpha value. Typically for most statistical tests (but not always), alpha is set as 0.05. But when the occurrence of the event is rare, you may want to set a very low alpha. The rarer it is, the lower the alpha. Whereas for a more likely event, it can go up to 0.1."}}