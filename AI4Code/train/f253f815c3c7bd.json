{"cell_type":{"5dec62fb":"code","2419adb5":"code","6bbc6c9f":"code","67aec5a4":"code","6da1bf98":"code","616bac2a":"code","2b7c1730":"code","2cbe136a":"code","a442295d":"code","9f58bd05":"code","7bb32e60":"code","f46fbbc2":"code","9b5f950d":"code","e1dc2e01":"code","fa1c4673":"code","3c23752d":"code","8fc0a7fb":"code","d535a788":"code","02ee03f9":"code","489bb23d":"code","ec3e223e":"code","869d8f9d":"code","fd584c78":"code","ecdc46df":"code","3b80ad4d":"code","def61ba4":"code","466e414a":"code","e28c4ac5":"code","3800dc6b":"code","ceb94c66":"markdown","be9da705":"markdown","38af056d":"markdown","92bd43ac":"markdown","2f31e8f1":"markdown","4c70fe18":"markdown"},"source":{"5dec62fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder #Encode Categorical Features\nimport lightgbm as lgb #Gradient Boosting Machine\nimport matplotlib.pyplot as plt #Visualization\nimport seaborn as sns #Visualization\nfrom sklearn.model_selection import KFold #N-Fold Validation\nfrom sklearn.metrics import mean_squared_error #Evaluation Metric\nimport optuna #hyperparams Tuning","2419adb5":"trainSet = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')","6bbc6c9f":"trainSet.head()","67aec5a4":"#plot the Target Distribution\nsns.displot(data=trainSet, x=\"target\", kde=True)","6da1bf98":"len(trainSet[trainSet.target < 5])\/len(trainSet)","616bac2a":"len(trainSet[trainSet.target > 10])\/len(trainSet)","2b7c1730":"#From the distribution graph, I would like to get rid of rows which has target < 5 and > 10 to minimize outlier.\ntrainSet = trainSet[(trainSet.target > 5) & (trainSet.target < 10)]","2cbe136a":"#encode categorical feats\ncat_feat = [f\"cat{val}\" for val in range(0,10)]\n\nlabelEnc = [LabelEncoder() for _ in range(len(cat_feat))]\n\nfor i in range(len(cat_feat)):\n    trainSet[cat_feat[i]] = labelEnc[i].fit_transform(trainSet[cat_feat[i]])","a442295d":"#Lets see the Correlation of each features and target\n\ncorr = trainSet.drop(['id'], axis=1).corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corr, mask=mask, cmap='BrBG', vmin=-1, vmax=1, annot=True)","9f58bd05":"cont_var = [f\"cont{val}\" for val in range(14)]\nfor i in cont_var:\n    trainSet[i] = np.log(trainSet[i])","7bb32e60":"#Seperate features and its target\ny = trainSet.target\nX = trainSet.drop(['target', 'id'], axis=1)","f46fbbc2":"def objective(trial):\n    # Define the search spaces, for your guidance, visit the optuna official sample codes https:\/\/optuna.org\/#code_examples\n    params = {\n        'num_iterations' : trial.suggest_int('num_iterations', 100, 1000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 256),\n        'num_leaves': trial.suggest_int('num_leaves', 15, 256),\n        'lambda_l1': trial.suggest_float('lambda_l1', 0, 25.0),\n        'lambda_l2': trial.suggest_float('lambda_l2', 0, 25.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 0, 25.0),\n        'random_state': 47,\n        'boosting_type': 'gbdt', \n        'verbose': -1\n    }\n\n    # Use 5 folds cross-validation\n    N_FOLDS = 5\n    rmse_score = 0\n    lgbm_models = []\n\n    kf = KFold(n_splits = N_FOLDS)\n    \n    for folds, (train_idx,val_idx) in enumerate(kf.split(X, y)):\n        print(f\"folds: {folds}\")\n        trainSet = lgb.Dataset(X.iloc[train_idx], y.iloc[train_idx])\n        valSet = lgb.Dataset(X.iloc[val_idx], y.iloc[val_idx])\n\n        model = lgb.train(params, trainSet)\n        lgbm_models.append(model)\n        y_pred = model.predict(X.iloc[val_idx])\n\n        rmse_score += mean_squared_error(y.iloc[val_idx], y_pred, squared=False)\/N_FOLDS\n\n        print(mean_squared_error(y.iloc[val_idx], y_pred, squared=False))\n        \n    return rmse_score","9b5f950d":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Start the hyperparams tunning and suppress any warnings\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective)","e1dc2e01":"best_params = study.best_params\nprint(study.best_params)","fa1c4673":"study.best_value","3c23752d":"#For time sake, I will not rerun the hyperparam tunning, here is the best Hyperparams I got from optuna tunner\n\nbest_params = {'num_iterations': 748,\n             'learning_rate': 0.021972728143721563,\n             'min_data_in_leaf': 251,\n             'num_leaves': 201,\n             'lambda_l1': 10.618325636467706,\n             'lambda_l2': 5.65105835287371,\n             'bagging_freq': 0,\n             'feature_fraction': 0.20664741485758317,\n             'random_state': 47,\n             'boosting_type': 'gbdt', \n             'verbose': -1,\n             'metric': 'rmse'\n              }","8fc0a7fb":"N_FOLDS = 5\nrmse_score = 0\nlgbm_models = []\neval_results = [{} for _ in range (N_FOLDS)]\n\nkf = KFold(n_splits = N_FOLDS)","d535a788":"#Train our LGBM using the best parameter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfor folds, (train_idx,val_idx) in enumerate(kf.split(X, y)):\n    print(f\"folds: {folds}\")\n    trainSet = lgb.Dataset(X.iloc[train_idx], y.iloc[train_idx])\n    valSet = lgb.Dataset(X.iloc[val_idx], y.iloc[val_idx])\n    \n    model = lgb.train(best_params, trainSet, valid_sets=[trainSet, valSet], evals_result=eval_results[folds])\n    lgbm_models.append(model)\n    y_pred = model.predict(X.iloc[val_idx])\n    \n    rmse_score += mean_squared_error(y.iloc[val_idx], y_pred, squared=False)\/N_FOLDS\n    \n    print(mean_squared_error(y.iloc[val_idx], y_pred, squared=False))","02ee03f9":"print(rmse_score)","489bb23d":"#plot the rmse score for each iteration in 5th fold model\nlgb.plot_metric(eval_results[4])","ec3e223e":"lgb.plot_importance(lgbm_models[4])","869d8f9d":"testSet = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\n\nfor i in range(len(cat_feat)):\n    testSet[cat_feat[i]] = labelEnc[i].transform(testSet[cat_feat[i]])","fd584c78":"cont_var = [f\"cont{val}\" for val in range(14)]\nfor i in cont_var:\n    testSet[i] = np.log(testSet[i])","ecdc46df":"id = testSet.id\ntestSet.drop('id', axis=1, inplace=True)","3b80ad4d":"y_pred = np.zeros(len(testSet))","def61ba4":"for model in lgbm_models:\n    y_pred += model.predict(testSet)","466e414a":"y_pred = pd.DataFrame(y_pred\/N_FOLDS)","e28c4ac5":"submFile = pd.concat([id, y_pred],axis=1)\nsubmFile.columns = ['id', 'target']","3800dc6b":"submFile.to_csv('submFile.csv', index=False)","ceb94c66":"# Predict the Test Set","be9da705":"From the correlation matrix, I could say that there is no single feature that is highly correlated to the target. So for this notebook, I will use all those features.","38af056d":"# Data Preprocessing","92bd43ac":"# End Hyperparam Tuning","2f31e8f1":"# Optuna Hyperparams Tuning on Light GBM Model","4c70fe18":"# Create Submission File as in sample_submission.csv"}}