{"cell_type":{"6800ab43":"code","bdcf7de0":"code","9f7171d1":"code","aca1ee2b":"code","45840eac":"code","760c03db":"code","594d3d2c":"code","90a52d0a":"code","0b0ec9b4":"code","be26d597":"code","bec0ffe4":"code","1aa687ea":"code","9a3f4fc2":"code","f065910b":"code","cda36f04":"code","2f0a79ec":"code","54883581":"code","1cb583ef":"code","07579e2a":"code","6ed9b7a6":"code","7656cefe":"code","6437ede5":"code","438f8b3f":"code","1599001d":"code","45379e5b":"code","ccbd0cef":"markdown","7e5f6910":"markdown","d43e179a":"markdown","022695d3":"markdown","472dbd02":"markdown","3e2d44f8":"markdown","9afe8a41":"markdown","50b2656a":"markdown","793ad026":"markdown","b6a38593":"markdown","c3e66286":"markdown","5394b0af":"markdown"},"source":{"6800ab43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bdcf7de0":"df = pd.read_csv(os.path.join(dirname, filename), low_memory=False)\ndf.head()","9f7171d1":"df.columns","aca1ee2b":"# Rename columns to remove spaces and the kW unit \ndf.columns = [col[:-5].replace(' ','_') if 'kW' in col else col for col in df.columns]\n\n# Drop rows with nan values \ndf = df.dropna()\n\n# The columns \"use\" and \"house_overall\" are the same, so let's remove the 'house_overall' column\ndf.drop(['House_overall'], axis=1, inplace=True)\n\n# The columns \"gen\" and \"solar\" are the same, so let's remove the 'solar' column\ndf.drop(['Solar'], axis=1, inplace=True)\n\n# drop rows with cloudCover column values that are not numeric (bug in sensors) and convert column to numeric\ndf = df[df['cloudCover']!='cloudCover']\ndf[\"cloudCover\"] = pd.to_numeric(df[\"cloudCover\"])\n\n# Create columns that regroup kitchens and furnaces \ndf['kitchen'] = df['Kitchen_12'] + df['Kitchen_14'] + df['Kitchen_38']\ndf['Furnace'] = df['Furnace_1'] + df['Furnace_2']\n\n# Convert \"time\" column (which is a unix timestamp) to a Y-m-d H-M-S \nimport time \nstart_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(int(df['time'].iloc[0])))\ntime_index = pd.date_range(start_time, periods=len(df), freq='min')  \ntime_index = pd.DatetimeIndex(time_index)\ndf = df.set_index(time_index)\ndf = df.drop(['time'], axis=1)","45840eac":"df.shape","760c03db":"df.columns","594d3d2c":"# lower frist letter of a string  \nfunc = lambda s: s[:1].lower() + s[1:] if s else ''","90a52d0a":"cols = list(df.dtypes.keys())\ncateg_cols = [col for col in cols if df[col].dtype=='O']\nnum_cols = [col for col in cols if col not in categ_cols]\nprint('categ_cols : ', categ_cols)\nprint('num_cols : ', num_cols)","0b0ec9b4":"# Let's remove rows with values that appear less than a certain percentage %\n\ndef remove_less_percent(col, percent):\n    keys_to_conserve = [key for key,value in df[col].value_counts(normalize=True).items() if value>=percent]\n    return df[df[col].isin(keys_to_conserve)]\n\nprint(len(df))\ndf = remove_less_percent('summary', 0.05)\nprint(len(df))\ndf = remove_less_percent('icon', 0.05)\nprint(len(df))","be26d597":"# plot bars of unique values of categorical columns\n\ndef plot_bars(col):\n    \n    import matplotlib.pyplot as plt \n    from matplotlib.pyplot import figure\n\n    figure(figsize=(14, 8), dpi=80)\n    plt.xticks(rotation = 90)\n    \n    D = df[col].value_counts(normalize=True).to_dict()\n\n    plt.bar(*zip(*D.items()))\n    plt.show()","bec0ffe4":"plot_bars('icon')","1aa687ea":"plot_bars('summary')","9a3f4fc2":"df['use'].resample(rule='D').mean().plot(figsize=(25,5))","f065910b":"df['temperature'].resample(rule='D').mean().plot(figsize=(25,5))","cda36f04":"df['cloudCover'].resample(rule='D').mean().plot(figsize=(25,5))","2f0a79ec":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\ngeneral_energy_cols = ['gen', 'use']\ngeneral_energy_per_month = df[general_energy_cols].resample('M').sum() # for energy we use sum to calculate overall consumption in period \n\nplt.figure(figsize=(20,10))\n\nsns.lineplot(data=general_energy_per_month, dashes=False)","54883581":"rooms_energy_cols = ['Home_office', 'Wine_cellar','Garage_door',\n                       'kitchen', 'Barn', 'Well','Living_room']\n\nrooms_energy_per_month = df[rooms_energy_cols].resample('M').mean()   \n\nplt.figure(figsize=(20,8))\n\nsns.lineplot(data=rooms_energy_per_month, dashes=False)","1cb583ef":"equipements_cols = ['Microwave', 'Dishwasher', 'Furnace', 'Fridge'] \n\nequipements_energy_per_month = df[equipements_cols].resample('M').mean()   \n\nplt.figure(figsize=(25,8))\n\nsns.lineplot(data= equipements_energy_per_month, dashes=False)","07579e2a":"weather_columns = ['temperature','humidity', 'visibility', 'apparentTemperature', \n                   'windSpeed', 'dewPoint']\n\nweather_per_month = df[weather_columns].resample('M').mean()   \n\nplt.figure(figsize=(25,8))\n\nsns.lineplot(data=weather_per_month, dashes=False)","6ed9b7a6":"fig,ax = plt.subplots(figsize=(20, 18)) \ncorr = df[weather_columns].corr()\nsns.heatmap(corr, annot=True, vmin=-1.0, vmax=1.0, center=0)\nax.set_title('Correlation of Weather Information', size=20)\nplt.show()","7656cefe":"# Let's install & import changefinder python library \n!pip install changefinder\nimport changefinder","6437ede5":"from scipy import stats\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\n\n\ndef chng_detection(col, _r=0.01, _order=1, _smooth=10):\n    cf = changefinder.ChangeFinder(r=_r, order=_order, smooth=_smooth)\n    ch_df = pd.DataFrame()\n    ch_df[col] = df[col].resample('D').mean()\n    \n    # calculate the change score\n    ch_df['change_score'] = [cf.update(i) for i in ch_df[col]]\n    ch_score_q1 = stats.scoreatpercentile(ch_df['change_score'], 25) \n    ch_score_q3 = stats.scoreatpercentile(ch_df['change_score'], 75) \n    thr_upper = ch_score_q3 + (ch_score_q3 - ch_score_q1) * 3\n    \n    anom_score = hv.Curve(ch_df['change_score'])\n    anom_score_th = hv.HLine(thr_upper).opts(color='red', line_dash=\"dotdash\")\n    \n    anom_points = [[ch_df.index[i],ch_df[col][i]] for i, score in enumerate(ch_df[\"change_score\"]) if score > thr_upper]\n    org = hv.Curve(ch_df[col],label=col).opts(yformatter='%.1fkw')\n    detected = hv.Points(anom_points, label=f\"{col} detected\").opts(color='red', legend_position='bottom', size=5)\n\n    return ((anom_score * anom_score_th).opts(title=f\"{col} Change Score & Threshold\") + \\\n            (org * detected).opts(title=f\"{col} Detected Points\")).opts(opts.Curve(width=800, height=300, show_grid=True, tools=['hover'])).cols(1)","438f8b3f":"chng_detection('use', _r=0.001, _order=1, _smooth=3)","1599001d":"df.columns","45379e5b":"chng_detection('Furnace', _r=0.001, _order=1, _smooth=3)","ccbd0cef":"----","7e5f6910":"# Data Exploration","d43e179a":"### Data reading","022695d3":"---","472dbd02":"* The energy consumption of kitchen, garage and the well remained almost the same throughout the year\n* There's seasonality of energy consumption in other parts of the house :\n    * A clear spike in september in the energy consumed by the wine cellar and the home office\n    * A clear downtrend in the summer for the barn energy consumption","3e2d44f8":"### Data Analysis","9afe8a41":"* Discounting parameter  \ud835\udc5f(0<\ud835\udc5f<1)  : The smaller this value, the greater the influence of the past data points and the greater the variation in the change score\n* Order parameter for AR  \ud835\udc5c\ud835\udc5f\ud835\udc51\ud835\udc52\ud835\udc5f  : How far past data points are included in the model\n* Smoothing window  \ud835\udc60\ud835\udc5a\ud835\udc5c\ud835\udc5c\ud835\udc61\u210e  : The greater this parameter is, the easier it is to capture the essential changes rather than the outliers, but if it is too large, it will be difficult to capture the changes themselves","50b2656a":"The usage of the furnace decreases in the summer","793ad026":"### Data Preprocessing","b6a38593":"* Case 1 Change Detection : Detecting excessive energy consumption in advance and preventing increase in usage fees.\n* Case 2 Predict Future Consumption : Predicting future energy consumption and generation by utilizing weather information and optimizing energy supply.\n\n    [**Inspired by kohei-mu**](https:\/\/www.kaggle.com\/koheimuramatsu\/change-detection-forecasting-in-smart-home#6.-Modeling)","c3e66286":"#### Case 1 : Change detection\n\nThe change point is the point at which the trends in time series data change over time.\nOutliers indicate a momentary abnormal condition (rapid decrease or increase), while change points mean that the abnormal condition does not return to its original state and continue.\n\nLet's use ChangeFinder algorithm \n\nChangeFinder is an algorithm used to detect change points.\nChangeFinder uses the log-likelihood based on the SDAR(Sequencially Discounting AR) algorithm to calculate the change score.\nSDAR algorithm introduces a discounting parameter into the AR algorithm to reduce the influence of past data, so that even non-stationary time series data can be learned robustly.\n\nChangeFinder has two steps of model training:\n* Training STEP1\nTrain a time series model at each data point using the SDAR algorithm\nBased on the trained time series model, calculate the likelihood that the data points at the next time point will appear\nCalculate the logarithmic loss and use it as an outlier score\n\n    \ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52(\ud835\udc65\ud835\udc61)=\u2212\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc43\ud835\udc61\u22121(\ud835\udc65\ud835\udc61|\ud835\udc651,\ud835\udc652,\u2026,\ud835\udc65\ud835\udc61\u22121)\n \nSmoothing Step\nSmooth the outlier score within the smoothing window( \ud835\udc4a ).\nBy smoothing, the score due to outliers is attenuated, and it is possible to determine whether the abnormal condition has continued for a long time.\n\n    \ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52_\ud835\udc60\ud835\udc5a\ud835\udc5c\ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc51(\ud835\udc65\ud835\udc61)=1\ud835\udc4a\u2211\ud835\udc61=\ud835\udc61\u2212\ud835\udc4a+1\ud835\udc61\ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52(\ud835\udc65\ud835\udc56)\n \n* Training STEP2\nUsing the score obtained by smoothing, train the model with the SDAR algorithm\nBased on the trained time series model, calculate the likelihood that the data points at the next time point will appear\nCalculate the logarithmic loss and use it as an change score","5394b0af":"# Modeling : What are we trying to solve ?"}}