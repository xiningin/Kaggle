{"cell_type":{"f6f9f85c":"code","c1c4d251":"code","44ace69e":"code","7b68ba5f":"code","5de1834c":"code","a3773646":"code","a4f28c7f":"code","a3398df2":"code","6a2ef5e7":"code","b75477ad":"code","d1e28ba3":"code","1cb6d712":"code","edaad752":"code","c1ce0ebe":"code","fa4e7c50":"code","176831b7":"code","44ce61e7":"markdown","bbe4fa6d":"markdown","11adadd1":"markdown","ebd54931":"markdown","e99b8a15":"markdown","a3f13427":"markdown","6d0f6c73":"markdown","5336870e":"markdown","b8fe8540":"markdown"},"source":{"f6f9f85c":"import os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('fivethirtyeight')\nfrom tqdm import tqdm\n\nprint(os.listdir(\"..\/input\"))","c1c4d251":"from keras import Sequential\nfrom keras import optimizers\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential,Model\nfrom keras.layers import LSTM, Dense, Bidirectional, Input,Dropout,BatchNormalization,CuDNNLSTM, GRU, CuDNNGRU, Embedding, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import roc_curve, auc","44ace69e":"train = pd.read_json('..\/input\/train.json')\ntest = pd.read_json('..\/input\/test.json')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')","7b68ba5f":"train.shape, test.shape, sample_submission.shape","5de1834c":"train.head()","a3773646":"# https:\/\/www.kaggle.com\/qqgeogor\/keras-lstm-attention-glove840b-lb-0-043\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","a4f28c7f":"def get_model():\n    model = Sequential()\n    model.add(BatchNormalization(momentum=0.98, input_shape=(10, 128)))\n    model.add(Bidirectional(CuDNNGRU(128, return_sequences=True)))\n    model.add(Attention(10))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.001), metrics=['accuracy'])\n    return model","a3398df2":"# Make data for training\nxtrain = [k for k in train['audio_embedding']]\ntest_data = test['audio_embedding'].tolist()\nytrain = train['is_turkey'].values\n\n# Pad the audio features so that all are \"10 seconds\" long\nx_train = pad_sequences(xtrain, maxlen=10)\ny_train = np.asarray(ytrain)","6a2ef5e7":"kf = KFold(n_splits=10, shuffle=True, random_state=42069)\ntest_data = pad_sequences(test_data)\noof_preds = []\naucs = 0\n\nfor n_fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n    x_train_f = x_train[train_idx]\n    y_train_f = y_train[train_idx]\n    x_val_f = x_train[val_idx]\n    y_val_f = y_train[val_idx]\n    \n    # Get model\n    model = get_model()\n    \n    # Fit\n    model.fit(x_train_f, y_train_f,\n              batch_size=256,\n              epochs=12,\n              verbose=0,\n              validation_data=(x_val_f, y_val_f))\n\n    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n    preds_val = model.predict([x_val_f], batch_size=512)\n    oof_preds.append(model.predict(test_data))\n\n    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n    aucs += auc(fpr,tpr)\n    print('Fold {}, AUC = {}'.format(n_fold, auc(fpr, tpr)))\n\nprint(\"Cross Validation AUC = {}\".format(aucs\/10))","b75477ad":"preds = np.asarray(oof_preds)[...,0]\npreds = np.mean(preds, axis=0)\nsubmission = pd.DataFrame({'vid_id':test['vid_id'].values,'is_turkey':preds})\nprint(submission.head(5))","d1e28ba3":"def get_pseudo_data(sub_df, x_train, y_train, pos_threshold=0.99, neg_threshold=0.01):\n    pred_probs = sub_df.is_turkey.values\n    pseudo_index = np.argwhere(np.logical_or(pred_probs > pos_threshold, pred_probs < neg_threshold ))[:,0]\n    \n    pseudo_x_train = test_data[pseudo_index]\n    pseudo_y_train = pred_probs[pseudo_index]\n    pseudo_y_train[pseudo_y_train > 0.5] = 1\n    pseudo_y_train[pseudo_y_train <= 0.5] = 0\n    \n    X = np.concatenate([x_train, pseudo_x_train], axis=0)\n    y = np.concatenate([y_train, pseudo_y_train])\n    \n    return X, y","1cb6d712":"x_train, y_train = get_pseudo_data(submission, x_train, y_train)","edaad752":"kf = KFold(n_splits=10, shuffle=True, random_state=42069)\ntest_data = pad_sequences(test_data)\noof_preds = []\naucs = 0\n\nfor n_fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n    x_train_f = x_train[train_idx]\n    y_train_f = y_train[train_idx]\n    x_val_f = x_train[val_idx]\n    y_val_f = y_train[val_idx]\n    \n    # Get model\n    model = get_model()\n    \n    # Fit\n    model.fit(x_train_f, y_train_f,\n              batch_size=256,\n              epochs=12,\n              verbose=0,\n              validation_data=(x_val_f, y_val_f))\n\n    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n    preds_val = model.predict([x_val_f], batch_size=512)\n    oof_preds.append(model.predict(test_data))\n\n    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n    aucs += auc(fpr,tpr)\n    print('Fold {}, AUC = {}'.format(n_fold, auc(fpr, tpr)))\n\nprint(\"Cross Validation AUC = {}\".format(aucs\/10))","c1ce0ebe":"# Get submisison 1 using threshold\npreds = np.asarray(oof_preds)[...,0]\npreds = np.mean(preds, axis=0)\nsub_df = pd.DataFrame({'vid_id':test['vid_id'].values,'is_turkey':preds})\nsub_df.to_csv('submission1.csv', index=False)\nprint(sub_df.head(10))\nprint(sub_df.shape)","fa4e7c50":"n_bags = 20\nn_folds = 10\nrandom_state = 0\nbag_oof_preds = []\naucs = 0\n\nfor n_bag in range(n_bags):\n\n    random_state += n_bag\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n    test_data = pad_sequences(test_data)\n    oof_preds = []\n    \n    print('--> OOB #{}'.format(n_bag))\n\n    # Out-of-Fold Method\n    for n_fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n        x_train_f = x_train[train_idx]\n        y_train_f = y_train[train_idx]\n        x_val_f = x_train[val_idx]\n        y_val_f = y_train[val_idx]\n\n        # Get model\n        model = get_model()\n\n        # Fit\n        model.fit(x_train_f, y_train_f,\n                  batch_size=256,\n                  epochs=12,\n                  verbose=0,\n                  validation_data=(x_val_f, y_val_f))\n\n        # Get accuracy of model on validation data. It's not AUC but it's something at least!\n        preds_val = model.predict([x_val_f], batch_size=512)\n        oof_preds.append(model.predict(test_data))\n\n        fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n        aucs += auc(fpr,tpr)\n        print('Fold {}, AUC = {}'.format(n_fold, auc(fpr, tpr)))\n\n    bag_oof_preds.append(oof_preds)\n\nprint(\"Full Cross Validation AUC = {}\".format(aucs\/(n_bags*n_folds)))","176831b7":"oob_preds = np.asarray(bag_oof_preds)[...,0]\nmean_preds = np.mean(oob_preds, axis=0)\npreds = np.mean(mean_preds, axis=0)\nsubmission = pd.DataFrame({'vid_id':test['vid_id'].values,'is_turkey':preds})\nsubmission.to_csv('submission2.csv', index=False)\nprint(submission.head(5))\nprint(submission.shape)","44ce61e7":"# Helpers","bbe4fa6d":"## OOF with 10 folds","11adadd1":"## Finally, OOB","ebd54931":"## Threshold","e99b8a15":"# Load data","a3f13427":"# Load packages","6d0f6c73":"# Notes\n\n- OOBag with k-folds using CuDa GRU with an Attention layer on top.\n\n**Anyway, Happy Thanksgiving!**","5336870e":"# Modeling","b8fe8540":"## Re-OOF with 10 folds with pseudo_data"}}