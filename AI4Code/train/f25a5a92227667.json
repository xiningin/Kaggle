{"cell_type":{"29cc3f85":"code","89f75d02":"code","6623b1e8":"code","537d8c03":"code","d6b3ae3a":"code","960ad743":"code","c729b53a":"code","627f17b9":"code","f9ce23b4":"code","1b1c1e24":"code","f66904fc":"code","18669879":"code","6f737641":"code","44fa7f56":"code","f41fc7b6":"code","35323df6":"code","e0ace903":"code","c8e474ab":"code","0cd94bac":"code","740e7a26":"code","7c3deee6":"code","9687a6f4":"code","47521bc9":"code","2c32db44":"code","99a6f7b3":"code","eec9b698":"code","9798cd03":"code","f10053fe":"code","3ddfe456":"code","d45187f5":"code","5889c4c7":"code","ef234b2d":"code","627c8986":"code","8614bc93":"code","39fa8c0b":"code","f7efb56d":"code","cb6a2875":"code","bda70885":"code","bce8dc9d":"code","7ce6af04":"code","9b242757":"code","c5c96fb3":"code","3c56a447":"code","2aa28e3d":"code","a902ce4c":"code","9805afe6":"code","13cdbcce":"code","e7e86948":"code","42060d7a":"code","5035636c":"code","bad71854":"code","389de0d3":"code","aa8df998":"code","76898752":"code","b2c0764f":"code","762f2614":"code","4c4a8f96":"code","89b14935":"code","23eb4a8a":"code","44f11fec":"code","5aab8865":"code","42512cb0":"code","6f0ba634":"code","8b7a9a2b":"code","1224747c":"code","1d4ed07b":"code","ed28227b":"code","e153193d":"code","042a26a7":"code","43d437d1":"markdown","d7ee83f2":"markdown","71ca1aca":"markdown","480b9bd9":"markdown","deca14a3":"markdown","1e957cb2":"markdown","f3227f99":"markdown","ffff353a":"markdown","72590156":"markdown","e01fe5b4":"markdown","f148bcc2":"markdown","0cfc01f6":"markdown","f5bd194c":"markdown","0f68dd83":"markdown","5bbdfef3":"markdown","e998bb7b":"markdown","0a95e536":"markdown","988cfeac":"markdown","a5b1ccc4":"markdown","e88e3237":"markdown","c1de668e":"markdown","378fff98":"markdown","4e48d3af":"markdown","30eece21":"markdown","2af57088":"markdown","3b9d90f9":"markdown","912fe548":"markdown","0147b7a6":"markdown","e88fa10e":"markdown","b551e3ae":"markdown","8fd53043":"markdown","7df27cc1":"markdown","94f26279":"markdown","1af0d5ba":"markdown","802de693":"markdown","062ef5af":"markdown","48bfb0e4":"markdown","6c82c97e":"markdown","91c03473":"markdown","f720bc96":"markdown","54ae2c70":"markdown","0da9c2a4":"markdown","f5c2c3b7":"markdown","767090e0":"markdown","cb791bcd":"markdown","fbd76a38":"markdown","40ea4536":"markdown","d5bc8ab8":"markdown","d1a9dfd0":"markdown","a3656410":"markdown","f4847b7c":"markdown","84718f94":"markdown","e3ac1946":"markdown","08ae17d2":"markdown","18a96551":"markdown","56e0eaa9":"markdown","4a01bfb6":"markdown","c1636236":"markdown","b5e34c94":"markdown","989c6811":"markdown","1d50a16a":"markdown","8e0d5e9e":"markdown","5a4119bb":"markdown","ea9a429e":"markdown","a9c741d1":"markdown","76752d27":"markdown","fad536ec":"markdown"},"source":{"29cc3f85":"pip install pmdarima","89f75d02":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.dates as mdates\nimport scipy.stats\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport pylab\nsns.set(style='white')\nfrom pmdarima import auto_arima\nfrom statsmodels.tsa.stattools import adfuller\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport lightgbm as lgb","6623b1e8":"df=pd.read_csv('..\/input\/nifty50-stock-market-data\/RELIANCE.csv')","537d8c03":"df['Date']=pd.to_datetime(df['Date'])\ndf.set_index(['Date'],inplace=True)","d6b3ae3a":"df.head()","960ad743":"df.describe()","c729b53a":"df.shape","627f17b9":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","f9ce23b4":"missing_table=missing_values_table(df)\nmissing_table","1b1c1e24":"msno.matrix(df)","f66904fc":"df.Trades.plot()","18669879":"df.Trades[:2850]","6f737641":"# removing missing columns\n\ndf.drop(['Trades','Deliverable Volume','%Deliverble'],axis=1,inplace=True)\n","44fa7f56":"fig = go.Figure([go.Scatter(x=df.index, y=df['VWAP'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title='VWAP over time',\n    template=\"simple_white\",\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"VWAP\")\nfig.show()","f41fc7b6":"sns.kdeplot(df['VWAP'],shade=True)","35323df6":"fig = go.Figure([go.Scatter(x=df.loc['2019', 'VWAP'].index,y=df.loc['2020', 'VWAP'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title='VWAP in 2019',\n    template=\"simple_white\",\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"VWAP\")\n\nfig.show()","e0ace903":"fig = go.Figure([go.Scatter(x=df.loc['2020', 'VWAP'].index,y=df.loc['2020', 'VWAP'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title='VWAP in 2020',\n    template=\"simple_white\",\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"VWAP\")\nfig.show()","c8e474ab":"cols_plot = ['Open', 'Close', 'High','Low']\naxes = df[cols_plot].plot(figsize=(11, 9), subplots=True)\nfor ax in axes:\n    ax.set_ylabel('Daily trade')","0cd94bac":"fig = go.Figure([go.Scatter(x=df.index, y=df['Volume'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    template='simple_white',\n    title='Volume over time'\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"Volume\")\nfig.show()","740e7a26":"fig = go.Figure([go.Scatter(x=df.loc['2020', 'Volume'].index,y=df.loc['2020', 'Volume'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    template='simple_white',\n    title='Volume in 2020'\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"Volume\")\nfig.show()","7c3deee6":"scipy.stats.probplot(df.VWAP,plot=pylab)\npylab.show()","9687a6f4":"def dicky_fuller_test(x):\n    result = adfuller(x)\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n    if result[1]>0.05:\n        print(\"Fail to reject the null hypothesis (H0), the data is non-stationary\")\n    else:\n        print(\"Reject the null hypothesis (H0), the data is stationary.\")","47521bc9":"dicky_fuller_test(df['VWAP'])","2c32db44":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom dateutil.parser import parse\n\nplt.rcParams.update({'figure.figsize': (10,10)})\ny = df['VWAP'].to_frame()\n\n\n# Multiplicative Decomposition \nresult_mul = seasonal_decompose(y, model='multiplicative',period = 52)\n\n# Additive Decomposition\nresult_add = seasonal_decompose(y, model='additive',period = 52)\n\n# Plot\nplt.rcParams.update({'figure.figsize': (10,10)})\nresult_mul.plot().suptitle('Multiplicative Decompose', fontsize=22)\nresult_add.plot().suptitle('Additive Decompose', fontsize=22)\nplt.show()","99a6f7b3":"df['vwap_diff']=df['VWAP']-df['VWAP'].shift(1)","eec9b698":"fig = go.Figure([go.Scatter(x=df.index,y=df.VWAP)])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    template='simple_white',\n    title='VWAP over time ')\nfig.show()","9798cd03":"fig = go.Figure([go.Scatter(x=df.index,y=df.vwap_diff)])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    template='simple_white',\n    title='difference VWAP over time ')\nfig.show()","f10053fe":"sm.graphics.tsa.plot_acf(df['VWAP'].iloc[1:], lags=40,title='auto correlation of VWAP',zero=False)\nplt.show()","3ddfe456":"sm.graphics.tsa.plot_acf(df['vwap_diff'].iloc[7:], lags=40,title='auto correlation of difference VWAP',zero=False)\nplt.show()","d45187f5":"sm.graphics.tsa.plot_pacf(df['VWAP'].iloc[1:], lags=40,title='partial auto correlation of VWAP',zero=False)\nplt.show()","5889c4c7":"sm.graphics.tsa.plot_pacf(df['vwap_diff'].iloc[1:], lags=40,title='partial autocorrelation of difference VWAP  ',zero=False)\nplt.show()","ef234b2d":"df.head()","627c8986":"df=df.reset_index()","8614bc93":"lag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\",\"Close\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n\nfor feature in lag_features:\n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.fillna(df.mean(), inplace=True)\n\ndf.set_index(\"Date\", drop=False, inplace=True)","39fa8c0b":"\ndf.Date = pd.to_datetime(df.Date, format=\"%Y-%m-%d\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.week\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\n","f7efb56d":"df.head()","cb6a2875":"\ndf_train = df[df.Date < \"2019\"]\ndf_valid = df[df.Date >= \"2019\"]\n\nexogenous_features = [\"High_mean_lag3\", \"High_std_lag3\", \"Low_mean_lag3\", \"Low_std_lag3\",\n                      \"Volume_mean_lag3\", \"Volume_std_lag3\", \"Turnover_mean_lag3\",\n                      \"Turnover_std_lag3\",\"High_mean_lag7\", \"High_std_lag7\", \"Low_mean_lag7\", \"Low_std_lag7\",\n                      \"Volume_mean_lag7\", \"Volume_std_lag7\", \"Turnover_mean_lag7\",\n                      \"Turnover_std_lag7\",\"High_mean_lag30\", \"High_std_lag30\", \"Low_mean_lag30\", \"Low_std_lag30\",\n                      \"Volume_mean_lag30\", \"Volume_std_lag30\", \"Turnover_mean_lag30\",\n                      \"Close_mean_lag3\", \"Close_mean_lag7\",\"Close_mean_lag30\",\"Close_std_lag3\",\"Close_std_lag7\",\"Close_std_lag30\",\n                      \"Turnover_std_lag30\",\"month\",\"week\",\"day\",\"day_of_week\"]\n","bda70885":"df_valid['Date'].describe()","bce8dc9d":"from statsmodels.tsa.ar_model import AutoReg\nmodel = AutoReg(df_train.VWAP,lags=3, exog=df_train[exogenous_features])\nres = model.fit()\nprint(res.summary())\nprint(\"\u03bc={} ,\u03d5={}\".format(res.params[0],res.params[1]))","7ce6af04":"res = model.fit(cov_type=\"HC0\")\nprint(res.summary())\nprint(\"\u03bc={} ,\u03d5={}\".format(res.params[0],res.params[1]))","9b242757":"fig = plt.figure(figsize=(16,9))\nfig = res.plot_diagnostics(fig=fig, lags=30)","c5c96fb3":"model = auto_arima(df_train.VWAP, exogenous=df_train[exogenous_features], trace=True, error_action=\"ignore\", suppress_warnings=True)\nmodel.fit(df_train.VWAP, exogenous=df_train[exogenous_features])\n\nforecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])\ndf_valid[\"Forecast_ARIMAX\"] = forecast","3c56a447":"model.summary()","2aa28e3d":"df_valid[[\"VWAP\", \"Forecast_ARIMAX\"]].plot(figsize=(14, 7))","a902ce4c":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_ARIMAX)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_ARIMAX))","9805afe6":"from fbprophet import Prophet\n\nmodel_fbp = Prophet()\nfor feature in exogenous_features:\n    model_fbp.add_regressor(feature)\n\nmodel_fbp.fit(df_train[[\"Date\", \"VWAP\"] + exogenous_features].rename(columns={\"Date\": \"ds\", \"VWAP\": \"y\"}))\n\nforecast = model_fbp.predict(df_valid[[\"Date\", \"VWAP\"] + exogenous_features].rename(columns={\"Date\": \"ds\"}))\ndf_valid[\"Forecast_Prophet\"] = forecast.yhat.values","13cdbcce":"model_fbp.plot_components(forecast)","e7e86948":"df_valid[[\"VWAP\", \"Forecast_ARIMAX\", \"Forecast_Prophet\"]].plot(figsize=(14, 7))","42060d7a":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_ARIMAX)))\nprint(\"RMSE of Prophet:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_Prophet)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_ARIMAX))\nprint(\"MAE of Prophet:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_Prophet))","5035636c":"params = {\"objective\": \"regression\"}\n\ndtrain = lgb.Dataset(df_train[exogenous_features], label=df_train.VWAP.values)\ndvalid = lgb.Dataset(df_valid[exogenous_features])\n\nmodel_lgb = lgb.train(params, train_set=dtrain)\n\nforecast = model_lgb.predict(df_valid[exogenous_features])\ndf_valid[\"Forecast_LightGBM\"] = forecast","bad71854":"df_valid[[\"VWAP\", \"Forecast_ARIMAX\", \"Forecast_Prophet\", \"Forecast_LightGBM\"]].plot(figsize=(14, 7))","389de0d3":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_ARIMAX)))\nprint(\"RMSE of Prophet:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_Prophet)))\nprint(\"RMSE of LightGBM:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_LightGBM)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_ARIMAX))\nprint(\"MAE of Prophet:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_Prophet))\nprint(\"MAE of LightGBM:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_LightGBM))","aa8df998":"from sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import RNN\nfrom keras.layers import Dropout\nfrom keras.layers import *\nfrom keras.callbacks import EarlyStopping\nfrom math import sqrt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras","76898752":"fig = plt.figure(figsize = (17,25))\nax = fig.gca()\nhist=df.hist(ax = ax)","b2c0764f":"dataset = df[exogenous_features].values\ndataset = dataset.astype('float32')\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\ntrain_size = int(len(dataset) * 0.80)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nX_train = train[:,1:]\ny_train = train[:,0]\nX_test = test[:,1:]\ny_test = test[:,0]","762f2614":"batch_size = 1\ntimesteps = 1\nunits = 100\nnb_epoch = 70","4c4a8f96":"X_train = X_train.reshape(X_train.shape[0],timesteps,X_train.shape[1])\nX_test = X_test.reshape(X_test.shape[0],timesteps,X_test.shape[1])","89b14935":"model = Sequential()\nmodel.add(LSTM(units,batch_input_shape=(batch_size, X_train.shape[1], X_train.shape[2]), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nhistory=model.fit(X_train, y_train,epochs=nb_epoch,batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=21)],verbose=0,shuffle=False)","23eb4a8a":"model.summary()","44f11fec":"yhat = model.predict(X_test, batch_size=batch_size)\nrmse = sqrt(mean_squared_error(y_test, yhat))\nmae=mean_absolute_error(y_test, yhat)\nprint('rmse:{} MAE:{}'.format(rmse,mae))","5aab8865":"plt.figure(figsize=(8,4))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Test Loss')\nplt.title('LSTM model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(loc='upper right')\nplt.show();","42512cb0":"plt.figure(figsize=(8,4))\nplt.plot(y_test, marker='.', label=\"actual\")\nplt.plot(yhat, label=\"prediction\")\nplt.tick_params(left=False, labelleft=True)\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('Vwap', size=15)\nplt.xlabel('points', size=15)\nplt.legend(fontsize=15)\nplt.show();","6f0ba634":"model = Sequential()\nmodel.add(SimpleRNN(units,batch_input_shape=(batch_size, X_train.shape[1], X_train.shape[2]), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nhistory=model.fit(X_train, y_train,epochs=nb_epoch,batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=21)],verbose=0,shuffle=False)\nmodel.summary()","8b7a9a2b":"yhat2 = model.predict(X_test, batch_size=batch_size)\nrmse = sqrt(mean_squared_error(y_test, yhat))\nmae=mean_absolute_error(y_test, yhat2)\nprint('rmse:{} MAE:{}'.format(rmse,mae))","1224747c":"plt.figure(figsize=(8,4))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Test Loss')\nplt.title('RNN model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(loc='upper right')\nplt.show();","1d4ed07b":"plt.figure(figsize=(8,4))\nplt.plot(y_test, marker='.', label=\"actual\")\nplt.plot(yhat2, label=\"prediction\")\nplt.tick_params(left=False, labelleft=True)\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('Vwap', size=15)\nplt.xlabel('points', size=15)\nplt.legend(fontsize=15)\nplt.show();","ed28227b":"residuals=df_valid.VWAP-df_valid.Forecast_ARIMAX","e153193d":"dicky_fuller_test((residuals))","042a26a7":"residuals.plot()","43d437d1":"![image.png](attachment:image.png)","d7ee83f2":"* As you can see all the starting values are missing in columns Trades , Deliverable Volume\t and %Deliverble","71ca1aca":"* We don't actually need to convert the time series data into stationary data. For study purpose,I have explained how to check stationarity and how to convert non-stationary data into stationary data ","480b9bd9":"* There was many dips in year 2019 like in May-June , Aug-Sept and in end of the Sept month ","deca14a3":"<font size=\"+3\" color='#780404'><b> Data Preparation <\/b><\/font>","1e957cb2":"## Convert Stationary into Non Stationary","f3227f99":"Adding lag values of High, Low, Volume,Turnover, will use three sets of lagged values, one previous day, one looking back 7 days and another looking back 30 days as a proxy for last week and last month metrics.","ffff353a":"<font size=\"+3\" color='#053c96'><b> Reliance Industries Limited <\/b><\/font>","72590156":"<font size=\"+3\" color='#780404'><b> Evaluating Model <\/b><\/font>","e01fe5b4":"<font size=\"+3\" color='#780404'><b> AUTO-ARIMA Model <\/b><\/font>","f148bcc2":"Inspired from -\n\n* https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-time-series-using-pandas\n* https:\/\/www.kaggle.com\/rohanrao\/a-modern-time-series-tutorial","0cfc01f6":"### Differencing","f5bd194c":"## Converting Date into DateTime format ","0f68dd83":"## VWAP in 2019","5bbdfef3":"<font size=\"+3\" color='#053c96'><b> Introduction<\/b><\/font>\n\n","e998bb7b":"## Check Stationarity i.e Dicky Fuller Test","0a95e536":"## Stationarity ","988cfeac":"<font size=\"+3\" color='#780404'><b> Analyzing residuals <\/b><\/font>","a5b1ccc4":"* cyclic patter is shown in every 30 days (monthly)","e88e3237":"<font size=\"+3\" color='#053c96'><b> About Dataset<\/b><\/font>","c1de668e":"## data summary","378fff98":"## Facebook Prophet\nProphet is an open-source time series model developed by Facebook. It was released in early 2017. An exerpt from the homepage:\n\n> Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\nRead more about Prophet: https:\/\/facebook.github.io\/prophet\/\n\nI also shared a starter code [Prophet's Prophecy](https:\/\/www.kaggle.com\/rohanrao\/ashrae-prophet-s-prophecy) for using Prophet in the ASHRAE competition on Kaggle.\n\nNote that the default parameters are used for Prophet. They can be tuned to improve the results.","4e48d3af":"## Open,close,High,low prices over time ","30eece21":"## Check for missing values","2af57088":"# Deep Learning Models \nLet's explore some deep learning models on the dataset.\nWe'll try LTSM and RNN. RNN, a model designed for allowing information to persist in short term memory to predict subsequent values, should be best suited to our dataset, since we can see that autocorrelations don't show signs of seasonality, it tends to be manipulated by recent previous prices. We'll also apply LTSM, the special RNN model, which might reveal some interesting long term dependencies or relations in the dataset. Let's explore!","3b9d90f9":"> Note - If you want to know more about stationarity , you can refer this [link](http:\/\/https:\/\/towardsdatascience.com\/stationarity-in-time-series-analysis-90c94f27322#:~:text=In%20the%20most%20intuitive%20sense,not%20itself%20change%20over%20time.)","912fe548":"The Augmented Dickey-Fuller test is a type of statistical test called a unit root test.\n\nThe intuition behind a unit root test is that it determines how strongly a time series is defined by a trend\nIt uses an autoregressive model and optimizes an information criterion across multiple different lag values.\n\nThe null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary (has some time-dependent structure). The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n\n**Null Hypothesis (H0)**: If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n\n**Alternate Hypothesis (H1)**: The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n\nWe interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n\np-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\np-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","0147b7a6":"## Plotting VWAP(Volume Weighted Average Price) over time","e88fa10e":"<font size=\"+1\" color='#9b24a3'><b>I hope you enjoyed this kernel , Please don't forget to appreciate me with an Upvote.<\/b><\/font>","b551e3ae":"![image.png](attachment:image.png)","8fd53043":"<font size=\"+3\" color='#780404'><b>Exploratory Data Analysis<\/b><\/font>","7df27cc1":"<font size=\"+3\" color='#780404'><b> Feature Engineering <\/b><\/font>","94f26279":"# Features:\n1. Series: Here EQ stands for equity series of stock market.\n2. Prev Close: The closing price of the stock for the day before.\n3. Open,High, Low, Last, Close: The opening price, highest price, lowest price, last price and closing price of ICICI shares on the current day.\n4. **VWAP**: Volume Weighted Average Price,the **target variable** to predict. VWAP is a trading benchmark used by traders that gives the average price the stock has traded at throughout the day, based on both volume and price.\n5. Volume: Volume of shares traded on the current day.\n6. Turnover: It is a measure of stock liquidity calculated by dividing the total number of shares traded over a period by the average number of shares outstanding for the period. \n7. Trades: total number of trades on the current day.\n8. Deliverable Volume:  is the quantity of shares which actually move from one set of people to another set of people.\n9. Deliverable(%): Deliverable volume in percentage.","1af0d5ba":"* There is steady increase in prices upto year 2008 \n* Stock price fell after jan 2008  and attain pick again in may-june 2009  after that it fell again .","802de693":"# AutoRegressor:\nA model that uses the dependent relationship between an observation and some number of lagged observations.\n> \"We can use statistical measures to calculate the correlation between the output variable and values at previous time steps at various different lags. The stronger the correlation between the output variable and a specific lagged variable, the more weight that autoregression model can put on that variable when modeling.\"-https:\/\/machinelearningmastery.com\/autoregression-models-time-series-forecasting-python\/<br>\n\nAR(1) model<br>\nRt = \u03bc + \u03d5Rt-1 + \u03b5t<br>\n\nAs RHS has only one lagged value(Rt-1)this is called AR model of order 1 where \u03bc is mean and \u03b5 is noise at time t\nIf \u03d5 = 1, it is random walk. Else if \u03d5 = 0, it is white noise. Else if -1 < \u03d5 < 1, it is stationary. If \u03d5 is -ve, there is men reversion. If \u03d5 is +ve, there is momentum.<br>\n\nAR(2) model<br>\nRt = \u03bc + \u03d51Rt-1 + \u03d52Rt-2 + \u03b5t<br>\n\nAR(3) model<br>\nRt = \u03bc + \u03d51Rt-1 + \u03d52Rt-2 + \u03d53Rt-3 + \u03b5t<br>","062ef5af":"**Autocorrelation** and **partial autocorrelation** plots are heavily used in time series analysis and forecasting.\n\nThese are plots that graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps.\n\n**Statistical correlation** summarizes the strength of the relationship between two variables.\n\nWe can calculate the correlation for time series observations with observations with previous time steps, called lags. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a **serial correlation, or an autocorrelation.**\n\nA plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function, or the acronym ACF. This plot is sometimes called a **correlogram or an autocorrelation plot**.\n\n![image.png](attachment:image.png)\n","48bfb0e4":"* There are missing vales in Trades , Deliverable Volumne and % deliverable","6c82c97e":" ## Visualising using KDEs\n Summarizing the data with Density plots to see where the mass of the data is located","91c03473":"## VWAP in 2020","f720bc96":"## Plotting ACF and PACF ","54ae2c70":"In the most intuitive sense, stationarity means that the statistical properties of a process generating a time series do not change over time. It does not mean that the series does not change over time, just that the way it changes does not itself change over time. The algebraic equivalent is thus a linear function, perhaps, and not a constant one; the value of a linear function changes as \ud835\udc99 grows, but the way it changes remains constant \u2014 it has a constant slope; one value that captures that rate of change.","0da9c2a4":"* we don't have data of Trades on and before 31-05-2011 ","f5c2c3b7":"## Volume over Time ","767090e0":"* as you can see there is deep in the month of march and april.\n* Steady increase after month of april .","cb791bcd":"## Seasonal Decompose","fbd76a38":"## LightGBM\nTime series problems are popularly converted into a tabular i.i.d. structure and fed into boosting models like [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/) and [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/).\n\nThere is loss of information in terms of knowing the order of data points in the time series but it can be circumvented by the datetime features to capture this information to some extent.\n\nNote that the default parameters are used for LightGBM. They can be tuned to improve the results.","40ea4536":"The dataset used is stock market data of the Nifty-50 index from NSE (National Stock Exchange) India over the last 20 years (2000 - 2019)\n\nThe historic VWAP (Volume Weighted Average Price) is the target variable to predict. VWAP is a trading benchmark used by traders that gives the average price the stock has traded at throughout the day, based on both volume and price.\nRead more about the dataset: https:\/\/www.kaggle.com\/rohanrao\/nifty50-stock-market-data\n\nI am using Reliance stock prices .","d5bc8ab8":"Formally, the process {x\u1d62 ; i\u2208\u2124} is weakly stationary if:\n1. The first moment of x\u1d62 is constant; i.e. \u2200t, E[x\u1d62]=\ud835\udf07\n2. The second moment of x\u1d62 is finite for all t; i.e. \u2200t, E[x\u1d62\u00b2]<\u221e (which also implies of course E[(x\u1d62-\ud835\udf07)\u00b2]<\u221e; i.e. that variance is finite for all t)\n3. The cross moment \u2014 i.e. the auto-covariance \u2014 depends only on the difference u-v; i.e. \u2200u,v,a, cov(x\u1d64, x\u1d65)=cov(x\u1d64\u208a\u2090, x\u1d65\u208a\u2090)","d1a9dfd0":"ARIMA, Prophet and LightGBM models give a fair rmse and mae score. Let's see if deep learning models can do a better job in fitting to our dataset.","a3656410":"## Conclusions and Tips\n* Auto ARIMAX is a great baseline model but newer algorithms like Facebook's Prophet are extremely powerful and are getting cleverer by the day. Don't feel afraid to try out new techniques.\n* Setting up an appropriate validation framework is extremely important. It enables you to try and experiment various models and objectively compare them.\n* Lag-based features are very useful in providing trends information about the time series data. Rolling statistics are a common way of generating these.\n* Exogenous regressors help in providing external information about the time series. They tend to be very important in most models.\n* Boosting models like LightGBM are constrained to predict within the range of values of the target variable in the training data and don't extrapolate when there is strong trend.\n* Converting a time series to stationary and then modelling is a common approach for building solutions and can significantly improve results.","f4847b7c":"## Handling missing values ","84718f94":"## This notebook will the cover - \n\n* Data Preparation\n* Exploratory Data Analysis\n* Feature Engineering \n* AUTO-ARIMA Model\n* Analyzing residuals \n* Evaluating Model ","e3ac1946":"LSTM gives an excellent score on our dataset. Let's checkout a simple RNN model!","08ae17d2":"<img src=\"https:\/\/i.pinimg.com\/originals\/e2\/d7\/c7\/e2d7c71b09ae9041c310cb6b2e2918da.gif\">","18a96551":"## Import dataset","56e0eaa9":"Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.Whether we wish to predict the trend in financial markets or electricity consumption, time is an important factor that must now be considered in our models. For example, it would be interesting to forecast at what hour during the day is there going to be a peak consumption in electricity, such as to adjust the price or the production of electricity.\n","4a01bfb6":"A **partial autocorrelation** is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed.\n\nThe autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.\n\nIt is these indirect correlations that the partial autocorrelation function seeks to remove. Without going into the math, this is the intuition for the partial autocorrelation.\n\nA **partial autocorrelation** is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed.\n\nThe autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.\n\nIt is these indirect correlations that the partial autocorrelation function seeks to remove. Without going into the math, this is the intuition for the partial autocorrelation.\n\n![image.png](attachment:image.png)","c1636236":"The Auto ARIMAX model seems to do a fairly good job in predicting the stock price given data till the previous day. Can other models beat this benchmark?","b5e34c94":"* There are two picks in VWAP prices","989c6811":"RNN performs rather poorly, in comparison to LTSM model, Though the RMSE score turned out to be the same approximately, MAE has increased by 3-4%, as we can see the results from the graphs above. We can say that, when we move from RNN to LSTM, we are introducing more & more controlling knobs, which control the flow and mixing of Inputs as per trained Weights. And thus, bringing in more flexibility in controlling the outputs. Thus, we obtain better results in LTSM.","1d50a16a":"* All are following same pattern ","8e0d5e9e":"## Q-Q plot of VWAP \n\nused to determine whether dataset is distributed a certain way ","5a4119bb":"## Visualizing the locations of the missing data","ea9a429e":"## Import Libraries","a9c741d1":"Reliance Industries Limited (RIL) is an Indian multinational conglomerate company headquartered in Mumbai, Maharashtra, India. Reliance owns businesses across India engaged in energy, petrochemicals, textiles, natural resources, retail, and telecommunications. Reliance is one of the most profitable companies in India,the largest publicly traded company in India by market capitalization,and the largest company in India as measured by revenue after recently surpassing the government-controlled Indian Oil Corporation.On 22 June 2020, Reliance Industries became the first Indian company to exceed US$150 billion in market capitalization after its market capitalization hit \u20b911,43,667 crore on the BSE.\nThe company is ranked 96th on the Fortune Global 500 list of the world's biggest corporations as of 2020.It is ranked 8th among the Top 250 Global Energy Companies by Platts as of 2016. Reliance continues to be India's largest exporter, accounting for 8% of India's total merchandise exports with a value of \u20b91,47,755 crore and access to markets in 108 countries.Reliance is responsible for almost 5% of the government of India's total revenues from customs and excise duty. It is also the highest income tax payer in the private sector in India.\n\n~ *Source - wikipedia*","76752d27":"* data is not normally distributed , however this is what we usually expect from timeseries ","fad536ec":"## Volume in 2020"}}