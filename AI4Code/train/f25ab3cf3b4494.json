{"cell_type":{"40a7626f":"code","6b3c7abd":"code","f616605c":"code","8b81a4e5":"code","f93538a9":"code","3eff814e":"code","6739f309":"code","76f1730d":"code","ea396882":"code","fbc78cf6":"code","4a3dce71":"code","44931826":"markdown","2ae69168":"markdown"},"source":{"40a7626f":"import pandas as pd\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, f1_score, make_scorer\n\nimport hashlib","6b3c7abd":"# Set path for project files\nfpath = ('..\/input\/should-this-loan-be-approved-or-denied\/')","f616605c":"# Let's read CSV and take a look at our data\n# We drop unique ID and borrower's organization name right away as they are useless or even noisy as features;\n# Also we drop ChgOffDate, ChgOffPrinGr because they can directly tell us that the loan is charged-off\n# 'ApprovalDate', 'ApprovalFY', 'DisbursementDate' are dropped to make the model time-independent\ndata = pd.read_csv(fpath + 'SBAnational.csv').drop(columns=['LoanNr_ChkDgt', 'Name', 'ChgOffDate', 'ChgOffPrinGr',\n                                                            'ApprovalDate', 'ApprovalFY', 'DisbursementDate'])\nlen_data = len(data)\ndata","8b81a4e5":"# Let's convert the strings styled as '$XXXX.XX' to float values\nmoney_cols = ['DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv']\n\nfor col in money_cols:\n  data[col] = [float(val[1:].replace(',', '')) for val in data[col].values]","f93538a9":"# Let's check our data for missing values and fill NAs with mode\nfor col in data.drop(columns=['MIS_Status']).columns:\n  if data[col].isna().any():\n    data[col] = data[col].fillna(data[col].mode().iloc[0])","3eff814e":"# We have many columns with Object dtype; let's apply one hot encoding\n# (if the number of unique values is relatively small)\n# or hashing if there are many uniques\n# The only exception is MIS_Status (our target) variable: it is 'PIF' if the loan is returned\n# and 'CHGOFF' if the borrower had a debt\ncols_to_drop = []\n\nfor col in data.drop(columns=['MIS_Status']).columns:\n  if data[col].dtype == 'object':\n    print(f'Column {col} has {data[col].nunique()} values among {len_data}')\n\n    if data[col].nunique() < 25:\n      print(f'One-hot encoding of {col}')\n      one_hot_cols = pd.get_dummies(data[col])\n      for ohc in one_hot_cols.columns:\n        data[col + '_' + ohc] = one_hot_cols[ohc]\n    else:\n      print(f'Hashing of {col}')\n      data[col + '_hash'] = data[col].apply(lambda row: int(hashlib.sha1((col + \"_\" + str(row)).encode('utf-8')).hexdigest(), 16) % len_data)\n\n    cols_to_drop.append(col)","6739f309":"# Converting target variable from string to binary\ndata = data.drop(columns=cols_to_drop)\n\ndata['Defaulted'] = [1 if app == 'CHGOFF' else 0 for app in data.MIS_Status.values]\ndata = data.drop(columns=['MIS_Status'])","76f1730d":"# Finally, our data looks like this:\ndata","ea396882":"# The dataset is quite imbalanced: the amount of non-defaulted loans is 5x of that of defaulted ones\nprint(data.Defaulted.value_counts())","fbc78cf6":"# Let's fit and cross-validate a balanced random forest; first, divide the data to X and Y...\nX = data.drop(columns=['Defaulted'])\nY = data.Defaulted","4a3dce71":"# ...and apply stratified 5-fold validation\nrfc = RandomForestClassifier(class_weight='balanced', random_state=42)\nf1_scorer = make_scorer(f1_score)\nauc_scorer = make_scorer(roc_auc_score)\ncross_validate(rfc, X, Y, cv=StratifiedKFold(random_state=42, shuffle=True), scoring=['f1_weighted', 'roc_auc'],\n               n_jobs=-1, verbose=10)","44931826":"This notebook provides a baseline loan applications classifier based on Scikit-Learn's Random Forest. It doesn't include complex feature engineering, but provides high F1 and ROC AUC score on 5-fold cross-validation.","2ae69168":"As we can see, this model provides average F1 of 0.94 and average ROC AUC of 0.97, which is close to 1 and, thus, efficient to detect potentially risky loan applications. To improve the baseline solution, we can:\n- dive deeper into the problem and create new informative features;\n- apply more sophisticated methods, such as boosting or deep learning;\n- try to use oversampling techniques.\n\nThanks for your attention :)"}}