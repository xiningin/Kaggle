{"cell_type":{"b1778ad1":"code","c252ab23":"code","58671021":"code","f07040c5":"code","23f54c98":"code","dbf7a4db":"code","b1265aa7":"code","8eee4518":"code","6864281e":"code","9ba555ab":"code","943da150":"code","60b4f0a3":"code","81ae7faa":"code","639ad9da":"code","a01100bf":"code","26243ff3":"code","18bb8b70":"code","bfd60530":"code","133391b6":"code","7f173967":"code","4ec10cd0":"code","e94af0af":"code","4af96914":"code","f5e357b6":"markdown","31996bc3":"markdown","2efd3f86":"markdown","e4b88c06":"markdown","efb0c3d2":"markdown","2254125c":"markdown","8fb4d954":"markdown","44a5c091":"markdown","b80918ea":"markdown","7933b37a":"markdown","1597453f":"markdown","0013119c":"markdown","a5d2c522":"markdown","26feb6cd":"markdown","e7e3d779":"markdown","e93edb1b":"markdown","b686f115":"markdown","064aba11":"markdown","193c609a":"markdown","ac5721ec":"markdown","0e4c253a":"markdown","63db2fe8":"markdown","f857bcd2":"markdown","fd0d22ce":"markdown","53954638":"markdown","2dec7407":"markdown"},"source":{"b1778ad1":"# Input path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c252ab23":"# Import necessary libraries\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\nimport skopt","58671021":"# Read the data\n\ntrain = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv')\ntest  = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv')","f07040c5":"print('train shape:', train.shape)\nprint('test shape:', test.shape)","23f54c98":"train.head(2)","dbf7a4db":"test.head(2)","b1265aa7":"# Finding missing values in train and test data\n\ndef func(df):\n    a = df.isnull().sum()\n    b = df.count()\n    c = (a\/b) * 100\n    d = pd.DataFrame(a, columns = ['Missingvalue%'])\n    return d['Missingvalue%'].sum()","8eee4518":"print('missing values in train data:', func(train))\nprint('missing values in test data:', func(test))","6864281e":"train.describe()","9ba555ab":"test.describe()","943da150":"features = ['var_0', 'var_1','var_2','var_3', 'var_4', 'var_5', 'var_6', 'var_7', 'var_8', 'var_9',]  \n\ni = 0\nfig, ax = plt.subplots(figsize=(12,12))\n\nfor feature in features:    \n    i = i + 1\n    plt.subplot(4,4,i)     \n    plt.scatter(train[feature], test[feature])","60b4f0a3":"# Y distribution in train data\n\nsns.countplot(train.target)","81ae7faa":"print('% of 1 in train data:', (train.target.value_counts()[1]\/train.shape[0]) * 100)","639ad9da":"# function to generate subplots for ('X' variables vs 'Y')\n\ndef plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","a01100bf":"# Lets plot for few variables.\n\ndf1 = train.loc[train['target'] == 0]\ndf2 = train.loc[train['target'] == 1]\nfeatures = train.columns.values[2:102]\nplot_feature_distribution(df1, df2, '0', '1', features)","26243ff3":"correlations = train[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations.head()","18bb8b70":"features = train.columns.values[2:202]\n\nunique_max_train = []\nfor feature in features:\n    values = train[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])","bfd60530":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Count', 'Value'])).\\\n            sort_values(by = 'Count', ascending=False).head(10))","133391b6":"idx = features = train.columns.values[2:202]\nfor df in [test, train]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['avg'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)     \n    df['med'] = df[idx].median(axis=1)","7f173967":"train.head(2)","4ec10cd0":"# X columns\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target']]\n\n# Y volumn\ny = train['target']","e94af0af":"params = {      'learning_rate': 0.01,\n                'max_depth': -1,\n                'num_leaves': 12,\n                'feature_fraction': 0.1,\n                'subsample': 0.2,\n                'objective': 'binary',\n                 'metric': 'auc',\n                 'is_unbalance': True,\n                 'bagging_freq': 5,\n                 'boosting': 'gbdt' }                 ","4af96914":"folds = StratifiedKFold(n_splits = 5, shuffle = False)\n\noof = np.zeros(len(train))\n#predictions = np.zeros(len(test))\n\nfor fold_, (idxT, idxV) in enumerate(folds.split(train.values, y.values)):\n    print(\"Fold {}\".format(fold_))\n    \n    X_train = train.iloc[idxT][features]\n    y_train = y.iloc[idxT] \n    X_val =   train.iloc[idxV][features] \n    y_val = y.iloc[idxV]\n        \n    train_data = lgb.Dataset(X_train, y_train)\n    val_data   = lgb.Dataset(X_val, y_val)\n    \n    clf = lgb.train(params =  params ,                    \n                    train_set = train_data, \n                    valid_sets = [train_data, val_data], \n                    num_boost_round = 20000,\n                    verbose_eval = 1000, \n                    early_stopping_rounds = 5000)\n    \n    oof[idxV] = clf.predict(X_val, num_iteration=clf.best_iteration)\n    \nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y, oof)))   ","f5e357b6":"### <a id='5'>2.1 Missing values<\/a>","31996bc3":"# <a id='0'>Content<\/a>\n\n- <a href='#1'>1. Read the data<\/a>\n- <a href='#2'>2. Data Understanding<\/a>\n - <a href='#5'>2.1 Missing values<\/a>\n - <a href='#6'>2.2 Statistics<\/a>\n- <a href='#3'>3. Data Exploration<\/a>\n - <a href='#7'>3.1 Distribution of Train vs Test<\/a>\n - <a href='#8'>3.2 Distribution of Y variable<\/a>\n - <a href='#9'>3.3 Distribution of X variables<\/a>\n - <a href='#10'>3.4 Correlation<\/a>\n - <a href='#11'>3.5 Repeated values<\/a>\n- <a href='#4'>4. Additional Features<\/a>\n- <a href='#5'>5. Model<\/a>","2efd3f86":"# Conclusion\n Validation accuracy is around 90% and can be further improved with below\n \n 1) Tuning Hyperparameters\n \n 2) Testing with other models\n \n 3) Ensemble of different models","e4b88c06":"# <a id='4'>4. Additional Features<\/a>","efb0c3d2":"### Summary:\nThere is no missing value in train and test dataset","2254125c":"#### Let's try to plot Train[variables] vs Test[variables] for few features","8fb4d954":"# <a id='5'>5. Model<\/a>","44a5c091":"### Summary:\nIt's clear that, relationship between X variables is very low.","b80918ea":"### Summary: \nmin, max, mean and std dev vaues seems to be similar for train and test data","7933b37a":"# <a id='1'>1. Read the data<\/a>","1597453f":"### Some of the parameters of 'LightGBM'","0013119c":"# <a id='2'>2. Data Understanding<\/a>","a5d2c522":"#### As we dont have much information on columns, lets try to create aggregated columns","26feb6cd":"### Summary: \nMost of the variables seems to be normally distributed.","e7e3d779":"### <a id='11'>3.5 Repeated values<\/a>","e93edb1b":"### <a id='10'>3.4 Correlation<\/a>","b686f115":"### <a id='6'>2.2 Statistics<\/a>","064aba11":"### <a id='9'>3.3 Distribution of X variables<\/a>","193c609a":"# <a id='3'>3. Data Exploration<\/a>","ac5721ec":"### Summary: \nData is highly imbalanced, only 10% of 1's in target(Y) column","0e4c253a":"### <a id='8'>3.2 Distribution of Y variable<\/a>","63db2fe8":"### LightGBM (Leaf-Wise growth)\n\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Features.html\n","f857bcd2":"### <a id='7'>3.1 Distribution of Train vs Test<\/a>","fd0d22ce":"# Introduction\nSantander always looking for ways to help their customers to understand financial health and identify which products and services might help them achieve their monetary goals.\n\nProblem Statement:\nis a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n\nDataset contains numeric feature variables, the binary target column, and a string ID_code column.\nThe task is to predict the value of target column in the test set.\n\nData: \n1. train.csv - the training set.\n2. test.csv - the test set.  \n3. sample_submission.csv  \n\nData reference: (https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data)","53954638":"#### Relationship between X variables","2dec7407":"<img src = \"https:\/\/www.santanderbank.com\/us\/documents\/22507\/2202391\/DebitCardPage-Debit-Prmier-Cards_437x336.png\/68eaaf59-35ec-44c7-82f8-2cf46a38d733?t=1554058009547\" width=\"400\"><\/img>"}}