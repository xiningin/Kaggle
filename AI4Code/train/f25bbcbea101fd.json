{"cell_type":{"86de77c1":"code","9eafce4f":"code","c4b16d70":"code","2f5611c9":"code","8e9919b3":"code","9bb3965d":"code","2f508a65":"code","53379cf6":"code","0519090d":"code","19693400":"code","10d609a5":"code","b964ae33":"code","8bff5322":"code","c24da5e4":"code","5b377867":"code","ef2d2932":"code","b2fea766":"code","51f3bf7d":"code","2d543a14":"code","7899477f":"code","ee3b5156":"code","a5fa60cc":"code","151fe47c":"code","11df5280":"code","fb32c20c":"code","472d6428":"code","5951529c":"code","12db7f86":"code","613f5930":"code","7813eadd":"code","b9dcf37e":"code","ba7638aa":"code","942f9d33":"code","2be01720":"markdown","a82471fd":"markdown","2a7349e0":"markdown","c20a446d":"markdown","94d57c70":"markdown","e5fe3131":"markdown","6ff09047":"markdown","856721f9":"markdown","3590c907":"markdown","2c0b24dd":"markdown","2f2c7d4a":"markdown","1ed201e1":"markdown","3645ee0e":"markdown","c50cdc64":"markdown","2f738159":"markdown","7c77ee22":"markdown","303d9af9":"markdown","ba101eb4":"markdown","9becd6b7":"markdown","b4ce263b":"markdown","bf08bdb8":"markdown","f35197f7":"markdown","a428c6fa":"markdown","c136ea3a":"markdown","6607ff45":"markdown","34732f94":"markdown","0c4fe8ae":"markdown","78a5c56f":"markdown","9a84cf4f":"markdown","60d73c5f":"markdown","2df5255e":"markdown","25e0f18b":"markdown","ec313e38":"markdown","0957aa19":"markdown","2d05ffaa":"markdown","0eae7a30":"markdown","ae9f51c5":"markdown","22fb6ac4":"markdown","000dbe28":"markdown","41527cbb":"markdown"},"source":{"86de77c1":"! pip install py7zr\nimport py7zr\nwith py7zr.SevenZipFile('\/kaggle\/input\/mercari-price-suggestion-challenge\/train.tsv.7z', mode='r') as z:\n    z.extractall()\n!unzip \/kaggle\/input\/mercari-price-suggestion-challenge\/sample_submission_stg2.csv.zip\n!unzip \/kaggle\/input\/mercari-price-suggestion-challenge\/test_stg2.tsv.zip\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.utils import shuffle\nimport pickle\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","9eafce4f":"train_data = pd.read_table('..\/working\/train.tsv')\ntrain_data = shuffle(train_data, random_state=0)\nprint(train_data.shape)\n# train_data = train_data[:100]\n# print(train_data.shape)\ntrain_data.head()","c4b16d70":"test_data = pd.read_table('..\/working\/test_stg2.tsv')\nprint(test_data.shape)\n# test_data = test_data[:100]\ntest_data.head()","2f5611c9":"print(\"Train dataset:\\n\" + str(train_data['item_condition_id'].value_counts() \/ train_data.shape[0]))\nprint(\"\\nTest dataset:\\n\" + str(test_data['item_condition_id'].value_counts() \/ test_data.shape[0]))","8e9919b3":"print(\"Train dataset:\\n\" + str(train_data['brand_name'].value_counts().head() \/ train_data.shape[0]))\nprint(\"\\nTest dataset:\\n\" + str(test_data['brand_name'].value_counts().head() \/ test_data.shape[0]))","9bb3965d":"print(\"Train dataset:\\n\" + str(train_data['shipping'].value_counts() \/ train_data.shape[0]))\nprint(\"\\nTest dataset:\\n\" + str(test_data['shipping'].value_counts() \/ test_data.shape[0]))","2f508a65":"import seaborn\n\nseaborn.distplot(train_data['price'])","53379cf6":"seaborn.distplot(np.log1p(train_data.price))","0519090d":"train_data['log_price'] = np.log1p(train_data.price)\ntrain_data.iloc[0]","19693400":"def split_cat(category_name):\n    try:\n        return category_name.split('\/')\n    except:\n        return ['Others', 'Others', 'Others']\ntrain_data['cat_top'], train_data['cat_sub'], train_data['cat_item'] = zip(*train_data['category_name'].apply(lambda x: split_cat(x)))\ntest_data['cat_top'], test_data['cat_sub'], test_data['cat_item'] = zip(*test_data['category_name'].apply(lambda x: split_cat(x)))","10d609a5":"train_data.isnull().sum()","b964ae33":"test_data.isnull().sum()","8bff5322":"train_data['category_name'] = train_data['category_name'].fillna(value='Null')\ntrain_data['brand_name'] = train_data['brand_name'].fillna(value='Null')\ntrain_data['item_description'] = train_data['item_description'].fillna(value='Null')\n\ntest_data['category_name'] = test_data['category_name'].fillna(value='Null')\ntest_data['brand_name'] = test_data['brand_name'].fillna(value='Null')\ntest_data['item_description'] = test_data['item_description'].fillna(value='Null')","c24da5e4":"import re\ndef clean_text(text):\n    \"\"\"\n    Applies some pre-processing on the given text.\n\n    Steps :\n    - Removing HTML tags\n    - Removing punctuation\n    - Lowering text\n    \"\"\"\n    \n    # remove HTML tags\n    text = re.sub(r'<.*?>', '', text)\n    \n    # remove the characters [\\], ['] and [\"] using the resub method:\n    text = re.sub(r'\\\\', '', text)\n\n    text = re.sub(r'\\\"', '', text)   \n\n    text = re.sub(r'\\'', '', text)    \n    \n    # convert text to lowercase\n    text = text.strip().lower()\n    \n    # replace punctuation characters with spaces\n    filters='!\"\\'#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n    translate_dict = dict((c, \" \") for c in filters)\n    translate_map = str.maketrans(translate_dict)\n    text = text.translate(translate_map)\n\n    return text\n\n# Example\nclean_text(\"<html>This is is not a\\\" sentence.<\\html>\").split()","5b377867":"train_data.nunique()","ef2d2932":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(stop_words=\"english\",\n                            preprocessor=clean_text)\n\n# train_name = vectorizer.fit_transform(train_data['name'])\n# test_name = vectorizer.transform(test_data['name'])\n# pickle.dump(train_name, open(\"train_name.pickle\", \"wb\"))\n# pickle.dump(test_name, open(\"test_name.pickle\", \"wb\"))\n\ntrain_name = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'train_name.pickle'), 'rb'))\ntest_name = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'test_name.pickle'), 'rb'))","b2fea766":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(stop_words=\"english\",\n                             preprocessor=clean_text,\n                             ngram_range=(1, 2))\n\n# train_des = tfidf.fit_transform(train_data['item_description'])\n# test_des = tfidf.transform(test_data['item_description'])\n# pickle.dump(train_des, open(\"train_des.pickle\", \"wb\"))\n# pickle.dump(test_des, open(\"test_des.pickle\", \"wb\"))\n\ntrain_des = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'train_des.pickle'), 'rb'))\ntest_des = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'test_des.pickle'), 'rb'))","51f3bf7d":"from sklearn.preprocessing import LabelBinarizer\n\nlb_brand = LabelBinarizer(sparse_output=True)\n\n# train_brand = lb_brand.fit_transform(train_data['brand_name'])\n# test_brand = lb_brand.transform(test_data['brand_name'])\n# pickle.dump(train_brand, open(\"train_brand.pickle\", \"wb\"))\n# pickle.dump(test_brand, open(\"test_brand.pickle\", \"wb\"))\n\ntrain_brand = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'train_brand.pickle'), 'rb'))\ntest_brand = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'test_brand.pickle'), 'rb'))","2d543a14":"lb_condition_id = LabelBinarizer(sparse_output=True)\n\n# train_condition_id = lb_condition_id.fit_transform(train_data['item_condition_id'])\n# test_condition_id = lb_condition_id.transform(test_data['item_condition_id'])\n# pickle.dump(train_condition_id, open(\"train_condition_id.pickle\", \"wb\"))\n# pickle.dump(test_condition_id, open(\"test_condition_id.pickle\", \"wb\"))\n\ntrain_condition_id = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'train_condition_id.pickle'), 'rb'))\ntest_condition_id = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'test_condition_id.pickle'), 'rb'))","7899477f":"lb_shipping = LabelBinarizer(sparse_output=True)\n\n# train_shipping = lb_shipping.fit_transform(train_data['shipping'])\n# test_shipping = lb_shipping.transform(test_data['shipping'])\n# pickle.dump(train_shipping, open(\"train_shipping.pickle\", \"wb\"))\n# pickle.dump(test_shipping, open(\"test_shipping.pickle\", \"wb\"))\n\ntrain_shipping = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'train_shipping.pickle'), 'rb'))\ntest_shipping = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'test_shipping.pickle'), 'rb'))","ee3b5156":"lb_cat_top = LabelBinarizer(sparse_output=True)\n\n# train_cat_top = lb_cat_top.fit_transform(train_data['cat_top'])\n# test_cat_top = lb_cat_top.transform(test_data['cat_top'])\n# pickle.dump(train_cat_top, open(\"train_cat_top.pickle\", \"wb\"))\n# pickle.dump(test_cat_top, open(\"test_cat_top.pickle\", \"wb\"))\n\ntrain_cat_top = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'train_cat_top.pickle'), 'rb'))\ntest_cat_top = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'test_cat_top.pickle'), 'rb'))","a5fa60cc":"lb_cat_sub = LabelBinarizer(sparse_output=True)\n\n# train_cat_sub = lb_cat_sub.fit_transform(train_data['cat_sub'])\n# test_cat_sub = lb_cat_sub.transform(test_data['cat_sub'])\n# pickle.dump(train_cat_sub, open(\"train_cat_sub.pickle\", \"wb\"))\n# pickle.dump(test_cat_sub, open(\"test_cat_sub.pickle\", \"wb\"))\n\ntrain_cat_sub = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'train_cat_sub.pickle'), 'rb'))\ntest_cat_sub = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'test_cat_sub.pickle'), 'rb'))","151fe47c":"lb_cat_item = LabelBinarizer(sparse_output=True)\n\n# train_cat_item = lb_cat_item.fit_transform(train_data['cat_item'])\n# test_cat_item = lb_cat_item.transform(test_data['cat_item'])\n# pickle.dump(train_cat_item, open(\"train_cat_item.pickle\", \"wb\"))\n# pickle.dump(test_cat_item, open(\"test_cat_item.pickle\", \"wb\"))\n\ntrain_cat_item = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'train_cat_item.pickle'), 'rb'))\ntest_cat_item = pickle.load(open(os.path.join('\/kaggle\/input\/trained-sparse-matrix', 'test_cat_item.pickle'), 'rb'))","11df5280":"def evaluate(preds, y_test):\n    return np.sqrt(np.mean(np.power(preds - y_test, 2)))","fb32c20c":"from scipy.sparse import hstack\nfrom sklearn.model_selection import train_test_split\n\ntrain_features = (train_name, train_des, train_brand, train_condition_id, train_shipping, train_cat_top, train_cat_sub, train_cat_item)\nX = hstack(train_features).tocsr()\nX_train, X_test, y_train, y_test = train_test_split(X, train_data['log_price'], test_size=0.2, random_state=0)\n\ntest_features = (test_name, test_des, test_brand, test_condition_id, test_shipping, test_cat_top,  test_cat_sub, test_cat_item)\ntest_features = hstack(test_features).tocsr()","472d6428":"from sklearn.linear_model import *\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\n\nrmsle_result = 100\nbest_model = ''","5951529c":"def train(model, file_name):\n    print('Start training ' + str(model).split('(')[0] + '...')\n    model.fit(X_train, y_train)\n    pickle.dump(model, open(file_name, 'wb'))\ndef loss(model, X_test, y_test):\n    print('Root Mean Squared Error: ', end = '')\n    result = evaluate(model.predict(X_test), y_test)\n    print(result)\n    return result\ndef load(file_name):\n    model = pickle.load(open(os.path.join('\/kaggle\/input\/trained-model', file_name), 'rb'))\n    return model","12db7f86":"# model = LinearRegression()\n# file_name = 'linear_regression.sav'\n\n## TRAINING\n# train(model, file_name)\n\n# model = load(file_name)\n\n## EVALUATE\n# result = loss(model, X_test, y_test)\n\n# if(result < rmsle_result):\n#     rmsle_result = result\n#     best_model = file_name","613f5930":"model = Ridge()\nfile_name = 'ridge.sav'\n\n## TRAINING\n# train(model, file_name)\n\nmodel = load(file_name)\n\n## EVALUATE\nresult = loss(model, X_test, y_test)\n\nif(result < rmsle_result):\n    rmsle_result = result\n    best_model = file_name","7813eadd":"model = SGDRegressor()\nfile_name = 'sgd_regressor.sav'\n\n## TRAINING\n# train(model, file_name)\n\nmodel = load(file_name)\n\n## EVALUATE\nresult = loss(model, X_test, y_test)\n\nif(result < rmsle_result):\n    rmsle_result = result\n    best_model = file_name","b9dcf37e":"import lightgbm as lgb\nmodel = lgb.LGBMRegressor()\nfile_name = 'lgbm.sav'\n\n## TRAINING\n# train(model, file_name)\n\nmodel = load(file_name)\n\n## EVALUATE\nresult = loss(model, X_test, y_test)\n\nif(result < rmsle_result):\n    rmsle_result = result\n    best_model = file_name","ba7638aa":"submission = pd.read_csv('..\/working\/sample_submission_stg2.csv')\n\nmodel = pickle.load(open(os.path.join('\/kaggle\/input\/trained-model', best_model), 'rb'))\nprint('Use ' + best_model + ' model')\npreds = model.predict(test_features)\npreds = np.exp(preds) - 1\nsubmission.loc[:, 'price'] = preds\nsubmission","942f9d33":"submission.to_csv('submission.csv', index=False)","2be01720":"**K\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1 c\u1ee7a model kh\u00f4ng \u0111\u01b0\u1ee3c t\u1ed1t!** \n\nRoot Mean Squared Error: 0.5341069414140582\n\n<a href=\"#Conclusion\">Gi\u1ea3i th\u00edch v\u1ec1 k\u1ebft qu\u1ea3 c\u1ee7a XGBoost<\/a>\n\n\nTr\u00ean th\u1ef1c t\u1ebf, c\u00f3 m\u1ed9t model \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1 t\u1ed1t h\u01a1n XGBoost trong vi\u1ec7c x\u1eed l\u00fd l\u01b0\u1ee3ng data l\u1edbn l\u00e0 Light GBM.\n\n### Light GBM\n\nLight GBM \u0111\u00e1nh b\u1ea1i t\u1ea5t c\u1ea3 c\u00e1c thu\u1eadt to\u00e1n kh\u00e1c khi t\u1eadp dataset c\u00f3 k\u00edch th\u01b0\u1edbc c\u1ef1c l\u1edbn. Th\u1ef1c t\u1ebf ch\u1ee9ng minh, n\u00f3 c\u1ea7n \u00edt th\u1eddi gian \u0111\u00ea x\u1eed l\u00fd h\u01a1n tr\u00ean t\u1eadp d\u1eef li\u1ec7u n\u00e0y. Nguy\u00ean nh\u00e2n s\u00e2u xa c\u1ee7a s\u1ef1 kh\u00e1c bi\u1ec7t n\u00e0y n\u1eb1m \u1edf c\u01a1 ch\u1ebf l\u00e0m vi\u00eac c\u1ee7a Light GBM. Trong khi c\u00e1c thu\u1eadt to\u00e1n kh\u00e1c s\u1eed d\u1ee5ng c\u01a1 ch\u1ebf level-wise th\u00ec n\u00f3 l\u1ea1i s\u1eed d\u1ee5ng leaf-wise.\n\n![](https:\/\/rohitgr7.github.io\/content\/images\/2019\/03\/Screenshot-from-2019-03-27-23-09-47-1.png)\n\nNh\u01b0 ch\u00fang ta th\u1ea5y, leaf-wise ch\u1ec9 m\u1edf r\u1ed9ng tree theo 1 trong 2 h\u01b0\u1edbng so v\u1edbi c\u1ea3 2 h\u01b0\u1edbng c\u1ee7a level-wise, t\u1ee9c l\u00e0 s\u1ed1 l\u01b0\u1ee3ng t\u00ednh to\u00e1n c\u1ee7a Light GBM ch\u1ec9 b\u1eb1ng 1\/2 so v\u1edbi XGBoost.","a82471fd":"## Data cleansing\n\nC\u1ed9t danh m\u1ee5c s\u1ea3n ph\u1ea9m (**category_name**) \u0111a s\u1ed1 \u1edf d\u1ea1ng top\/sub\/item.\n\nTa th\u00eam 3 c\u1ed9t v\u00e0o d\u1eef li\u1ec7u: **top**, **sub** v\u00e0 **item** \u0111\u1ec3 d\u1ec5 d\u00e0ng ph\u00e2n t\u00edch text.","2a7349e0":"2 gi\u00e1 tr\u1ecb **shipping** tr\u00ean t\u1eadp train v\u00e0 test c\u00f3 t\u1ef7 l\u1ec7 t\u01b0\u01a1ng \u0111\u1ed3ng.","c20a446d":"Number of unique elements in **brand**: 4810\n\nC\u00f3 th\u1ec3 coi \u0111\u00e2y kh\u00f4ng ph\u1ea3i m\u1ed9t s\u1ed1 l\u1edbn khi so v\u1edbi 1 tri\u1ec7u 4 \u0111i\u1ec3m d\u1eef li\u1ec7u.","94d57c70":"### C\u00e1c features ph\u00e2n lo\u1ea1i theo label\n\nS\u1ed1 l\u01b0\u1ee3ng gi\u00e1 tr\u1ecb unique c\u1ee7a c\u00e1c feature sau kh\u00e1 \u00edt khi so v\u1edbi s\u1ed1 \u0111i\u1ec3m d\u1eef li\u1ec7u (1,482,535) n\u00ean ta vector h\u00f3a n\u00f3 theo c\u00e1c label.","e5fe3131":"# Result\nD\u1ef1a v\u00e0o \u0111\u00e1nh gi\u00e1 tr\u00ean t\u1eadp valid, ch\u1ecdn model t\u1ed1t nh\u1ea5t \u0111\u1ec3 \u0111\u01b0a ra k\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng:","6ff09047":">  # Workflow\n1. <a href=\"#Problem-description\">Problem description<\/a> \n2. <a href=\"#Data-preprocessing\">Data preprocessing<\/a>\n    * <a href=\"#Compare-TRAIN-and-TEST-dataset\">Compare TRAIN and TEST dataset<\/a>\n    * <a href=\"#Price-distribution\">Price distribution<\/a>\n    * <a href=\"#Data-cleansing\">Data cleansing<\/a>\n    * <a href=\"#Preprocessor-function\">Preprocessor function<\/a>\n    * <a href=\"#Text-encoding\">Text encoding<\/a>\n    * <a href=\"#Evaluate\">Evaluate<\/a>\n3. <a href=\"#Model-training\">Model training<\/a>\n    * <a href=\"#Linear-Regression\">Linear Regression<\/a>     \n    * <a href=\"#Ridge-Regression\">Ridge Regression<\/a>     \n    * <a href=\"#SGD-Regressor\">SGD Regressor<\/a>\n    * <a href=\"#Ensemble-models-(Boosting-method)\">Ensemble models (Boosting method)<\/a>\n4. <a href=\"#Conclusion\">Conclusion<\/a>\n5. <a href=\"#Result\">Result<\/a>","856721f9":"## Evaluate\n\nV\u00ec b\u00e0i to\u00e1n gi\u1edd \u0111\u00e3 tr\u1edf th\u00e0nh d\u1ef1 \u0111o\u00e1n log() v\u00e0 h\u00e0m \u0111\u00e1nh gi\u00e1 c\u1ee7a kaggle l\u00e0 Root Mean Squared Log Error:\n\n<a href=\"https:\/\/www.codecogs.com\/eqnedit.php?latex=RMSLE&space;=&space;\\sqrt{&space;\\frac{1}{N}\\sum_{i=1}^{N}&space;(log(\\hat{y}_{i}&plus;1)-&space;log(y_{i}&plus;1))^2}\" target=\"_blank\"><img src=\"https:\/\/latex.codecogs.com\/gif.latex?RMSLE&space;=&space;\\sqrt{&space;\\frac{1}{N}\\sum_{i=1}^{N}&space;(log(\\hat{y}_{i}&plus;1)-&space;log(y_{i}&plus;1))^2}\" title=\"RMSLE = \\sqrt{ \\frac{1}{N}\\sum_{i=1}^{N} (log(\\hat{y}_{i}+1)- log(y_{i}+1))^2}\" \/><\/a>\n\ntrong \u0111\u00f3:\n\n* $\\hat{y}_{i}$: gi\u00e1 d\u1ef1 \u0111o\u00e1n\n\n* $y_{i}$: gi\u00e1 th\u1ef1c s\u1ef1\n\nn\u00ean h\u00e0m evaluate c\u00f3 d\u1ea1ng nh\u01b0 sau:","3590c907":"G\u00f3i c\u00e1c features ph\u1ee5c v\u1ee5 qu\u00e1 tr\u00ecnh training v\u00e0 testing.\n\n\u0110\u1ec3 \u0111\u00e1nh gi\u00e1 \u0111\u1ed9 hi\u1ec7u qu\u1ea3 c\u1ee7a model, ta l\u1ea5y 20% t\u1eadp train l\u00e0m t\u1eadp valid.","2c0b24dd":"H\u00e0m \u0111\u1ed3ng bi\u1ebfn log() s\u1ebd \u0111\u01b0a \u0111\u1ed3 th\u1ecb gi\u00e1 v\u1ec1 g\u1ea7n d\u1ea1ng ph\u00e2n b\u1ed1 chu\u1ea9n.","2f2c7d4a":"# Problem description\n\nD\u1ef1 \u0111o\u00e1n gi\u00e1 c\u1ee7a s\u1ea3n ph\u1ea9m.\n\n**Input:** D\u1eef li\u1ec7u d\u1ea1ng text v\u1ec1 **t\u00ean**, **t\u00ecnh tr\u1ea1ng**, **danh m\u1ee5c**, **nh\u00e3n hi\u1ec7u**, **shipping** v\u00e0 **nh\u1eadn x\u00e9t c\u1ee7a kh\u00e1ch h\u00e0ng** v\u1ec1 s\u1ea3n ph\u1ea9m.\n\n**Output:** Gi\u00e1 c\u1ee7a s\u1ea3n ph\u1ea9m (>0) v\u00e0 \u0111\u01b0\u1ee3c Kaggle \u0111\u00e1nh gi\u00e1 theo c\u00f4ng th\u1ee9c:\n\n<a href=\"https:\/\/www.codecogs.com\/eqnedit.php?latex=RMSLE&space;=&space;\\sqrt{&space;\\frac{1}{N}\\sum_{i=1}^{N}&space;(log(\\hat{y}_{i}&plus;1)-&space;log(y_{i}&plus;1))^2}\" target=\"_blank\"><img src=\"https:\/\/latex.codecogs.com\/gif.latex?RMSLE&space;=&space;\\sqrt{&space;\\frac{1}{N}\\sum_{i=1}^{N}&space;(log(\\hat{y}_{i}&plus;1)-&space;log(y_{i}&plus;1))^2}\" title=\"RMSLE = \\sqrt{ \\frac{1}{N}\\sum_{i=1}^{N} (log(\\hat{y}_{i}+1)- log(y_{i}+1))^2}\" \/><\/a>\n\ntrong \u0111\u00f3:\n\n* $\\hat{y}_{i}$: gi\u00e1 d\u1ef1 \u0111o\u00e1n\n\n* $y_{i}$: gi\u00e1 th\u1ef1c s\u1ef1\n\n**K\u1ebft lu\u1eadn:** d\u1ea1ng b\u00e0i to\u00e1n h\u1ed3i quy.","1ed201e1":"### Vectorizer\n\nM\u00f4 h\u00ecnh **Bag of words**: bi\u1ec3u di\u1ec5n m\u1ed7i m\u1eabu d\u1eef li\u1ec7u d\u01b0\u1edbi d\u1ea1ng m\u1ed9t vector s\u1ed1 trong \u0111\u00f3 m\u1ed7i chi\u1ec1u l\u00e0 m\u1ed9t t\u1eeb c\u1ee5 th\u1ec3 trong kho d\u1eef li\u1ec7u.\n\nKho d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o khi g\u1ecdi l\u1ec7nh fit_transform t\u1eeb t\u1eadp train.\n\n\u0110\u1ed1i v\u1edbi t\u1eadp test th\u00ec ch\u1ec9 vi\u1ec7c bi\u1ec3u di\u1ec5n vector theo kho d\u1eef li\u1ec7u \u0111\u00e3 c\u00f3 (transform).\n\nNh\u1eefng t\u1eeb nh\u01b0 and, a, the, ... (stopwords) l\u00e0 nh\u1eefng t\u1eeb kh\u00f4ng c\u00f3 \u00fd ngh\u0129a v\u00e0 xu\u1ea5t hi\u1ec7n nhi\u1ec1u khi\u1ebfn lu m\u1edd c\u00e1c t\u1eeb kh\u00e1c s\u1ebd \u0111\u01b0\u1ee3c lo\u1ea1i b\u1ecf.","3645ee0e":"Number of unique elements in **cat_sub**: 113","c50cdc64":"Number of unique elements in **condition_id**: 5","2f738159":"# Data preprocessing\nT\u1eadp train data v\u1edbi h\u01a1n 1 tri\u1ec7u \u0111i\u1ec3m d\u1eef li\u1ec7u g\u1ed3m 7 c\u1ed9t features v\u00e0 1 c\u1ed9t gi\u00e1 (**price**).\n\nTr\u1ed9n t\u1eadp d\u1eef li\u1ec7u v\u1edbi random_state=0.","7c77ee22":"# Conclusion\n\n**Tham kh\u1ea3o t\u1eeb:**\n\n[**Kaggle discussion**: \u0111i\u1ec3m m\u1ea1nh v\u00e0 \u0111i\u1ec3m y\u1ebfu c\u1ee7a XGBoost.](https:\/\/www.kaggle.com\/discussion\/196542)\n\n[**Medium post**: nh\u1eefng b\u00e0i to\u00e1n kh\u00f4ng ph\u1ea3i \u0111i\u1ec3m m\u1ea1nh c\u1ee7a XGBoost v\u00e0 c\u00e1ch gi\u1ea3i quy\u1ebft.](https:\/\/towardsdatascience.com\/why-xgboost-cant-solve-all-your-problems-b5003a62d12a)\n\n**V\u1ea5n \u0111\u1ec1:**\n\nXGBoost \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1 v\u00f4 c\u00f9ng m\u1ea1nh m\u1ebd khi c\u00f3 th\u1ec3 gi\u1ea3i quy\u1ebft c\u00e1c b\u00e0i to\u00e1n phi tuy\u1ebfn t\u00ednh c\u00f9ng c\u00e1c k\u1ef9 thu\u1eadt nh\u01b0 regularization \u0111\u1ec3 gi\u1ea3m overfiting d\u1eef li\u1ec7u.\n\nNh\u01b0ng b\u00ean c\u1ea1nh \u0111\u00f3, XGBoost n\u00f3i ri\u00eang v\u00e0 c\u00e1c thu\u1eadt to\u00e1n tree-based n\u00f3i chung v\u1eabn kh\u00f4ng tr\u00e1nh kh\u1ecfi m\u1ed9t v\u00e0i h\u1ea1n ch\u1ebf, \u0111\u1eb7c bi\u1ec7t l\u00e0 trong vi\u1ec7c gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n x\u1eed l\u00fd ng\u00f4n ng\u1eef.\n\nTr\u00edch nguy\u00ean v\u0103n t\u1eeb trang web tham kh\u1ea3o:\n\n***When to NOT use XGBoost?***\n* *Natural language processing*\n* *Regression tasks that involve predicting a continuous output.*\n\nB\u00e0i to\u00e1n c\u1ee7a ta bao g\u1ed3m c\u1ea3 2 nhi\u1ec7m v\u1ee5 tr\u00ean: x\u1eed l\u00fd ng\u00f4n ng\u1eef v\u00e0 d\u1ef1 \u0111o\u00e1n gi\u00e1 tr\u1ecb li\u00ean t\u1ee5c - \u0111\u1ec1u kh\u00f4ng ph\u1ea3i \u0111i\u1ec3m m\u1ea1nh c\u1ee7a XGBoost.\n\n**Gi\u1ea3i ph\u00e1p:**\n\nC\u00e1ch gi\u1ea3i quy\u1ebft \u0111\u01b0\u1ee3c \u0111\u1ec1 ra l\u00e0 s\u1eed d\u1ee5ng Neural Network v\u1edbi kh\u1ea3 n\u0103ng fit \u0111a d\u1ea1ng lo\u1ea1i d\u1eef li\u1ec7u v\u00e0 cho ph\u00e9p *capture complex trends in data.*\n\n*\u0110\u00e3 hu\u1ea5n luy\u1ec7n tr\u00ean model MLPRegressor nh\u01b0ng kh\u00f4ng th\u00e0nh c\u00f4ng do v\u01b0\u1ee3t qu\u00e1 memory.*\n\n**Future work:**\n\nS\u1ebd hu\u1ea5n luy\u1ec7n t\u1eadp d\u1ef1 li\u1ec7u n\u00e0y b\u1eb1ng Neural Network khi \u0111i\u1ec1u ki\u1ec7n v\u1ec1 thi\u1ebft b\u1ecb v\u00e0 th\u1eddi gian cho ph\u00e9p!","303d9af9":"Number of unique elements in **cat_item**: 870","ba101eb4":"## Compare TRAIN and TEST dataset\n\n**V\u1ea5n \u0111\u1ec1**: N\u1ebfu 2 t\u1eadp d\u1eef li\u1ec7u train v\u00e0 test qu\u00e1 kh\u00e1c nhau, vi\u1ec7c h\u1ecdc tr\u00ean t\u1eadp train s\u1ebd kh\u00f4ng mang l\u1ea1i \u00fd ngh\u0129a khi c\u1ea7n d\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp test.\n\nV\u1eady n\u00ean tr\u01b0\u1edbc khi ph\u00e2n t\u00edch s\u00e2u h\u01a1n, ta c\u1ea7n ki\u1ec3m tra \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed3ng c\u1ee7a t\u1eadp train v\u00e0 test.\n\n**Gi\u1ea3i ph\u00e1p**: So s\u00e1nh **ph\u1ea7n tr\u0103m** c\u00e1c gi\u00e1 tr\u1ecb chi\u1ebfm \u0111a s\u1ed1 trong m\u1ed9t v\u00e0i feature \u0111\u1ec3 c\u00f3 c\u00e1i nh\u00ecn c\u01a1 b\u1ea3n v\u1ec1 t\u1eadp d\u1eef li\u1ec7u ta c\u1ea7n l\u00e0m vi\u1ec7c c\u00f9ng.","9becd6b7":"T\u1eadp d\u1eef li\u1ec7u train gi\u1edd \u0111\u00e3 c\u00f3 th\u00eam c\u1ed9t log_price.","b4ce263b":"### K\u1ebft lu\u1eadn v\u1ec1 t\u1eadp d\u1eef li\u1ec7u\n\nT\u1eadp train v\u00e0 test c\u00f3 d\u1eef li\u1ec7u c\u00e2n b\u1eb1ng, thu\u1eadn l\u1ee3i cho vi\u1ec7c train model v\u00e0 d\u1ef1 \u0111o\u00e1n.\n\n**\u0110\u1ec3 chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u m\u1ed9t c\u00e1ch t\u1ed1t nh\u1ea5t, ta c\u1ea7n kh\u00e1i qu\u00e1t c\u00e1c features v\u00e0 \u0111\u1ec1 xu\u1ea5t c\u00e1ch x\u1eed l\u00fd:**\n* **name** v\u00e0 **item_description**: d\u00f9ng c\u00e1c k\u1ef9 thu\u1eadt x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean\n* **item_condition_id**, **shipping** v\u00e0 **brand_name**: c\u00e1c features n\u00e0y ch\u1ec9 xu\u1ea5t hi\u1ec7n m\u1ed9t v\u00e0i gi\u00e1 tr\u1ecb n\u00ean s\u1ebd chia th\u00e0nh c\u00e1c label\n* **category_name**: feature n\u00e0y c\u00f3 d\u1ea1ng A\/B\/C n\u00ean d\u1ef1 \u0111\u1ecbnh s\u1ebd t\u00e1ch ri\u00eang ra l\u00e0m 3 ph\u1ea7n r\u1ed3i x\u1eed l\u00fd theo d\u1ea1ng label","bf08bdb8":"**V\u1ea5n \u0111\u1ec1:** K\u1ebft qu\u1ea3 \u0111\u00e1nh gi\u00e1 c\u1ee7a kaggle d\u1ef1a tr\u00ean logarit c\u1ee7a gi\u00e1 s\u1ea3n ph\u1ea9m n\u00ean b\u1eaft bu\u1ed9c gi\u00e1 tr\u1ea3 v\u1ec1 ph\u1ea3i l\u1edbn h\u01a1n 0.\n\nQua m\u1ed9t s\u1ed1 l\u1ea7n th\u1eed nghi\u1ec7m th\u00ec s\u1ed1 l\u1ea7n d\u1ef1 \u0111o\u00e1n gi\u00e1 b\u1ecb \u00e2m kh\u00e1 nhi\u1ec1u, d\u1eabn \u0111\u1ebfn khi l\u1ea5y log() b\u1ecb l\u1ed7i.\n\n**Gi\u1ea3i ph\u00e1p:** d\u1ef1 \u0111o\u00e1n log() c\u1ee7a gi\u00e1 r\u1ed3i qua h\u00e0m exp() s\u1ebd tr\u1ea3 v\u1ec1 gi\u00e1 tr\u1ecb lu\u00f4n d\u01b0\u01a1ng.\n","f35197f7":"**H\u1ecd t\u00ean: V\u01b0\u01a1ng Tr\u00ed Thi\u00ean C\u00f4ng**\n\n**M\u00e3 sinh vi\u00ean: 18020240**\n\n**L\u1edbp: K63-K2**","a428c6fa":"## Price distribution\n\nKh\u1ea3o s\u00e1t gi\u00e1 s\u1ea3n ph\u1ea9m cho th\u1ea5y s\u1ef1 kh\u00f4ng c\u00e2n x\u1ee9ng (gi\u00e1 \u0111\u1ed3 ti\u00eau d\u00f9ng so v\u1edbi trang s\u1ee9c).","c136ea3a":"### NaN values \n\n**V\u1ea5n \u0111\u1ec1:** Gi\u00e1 tr\u1ecb NaN s\u1ebd g\u00e2y ra l\u1ed7i khi chuy\u1ec3n h\u00f3a d\u1eef li\u1ec7u d\u1ea1ng text v\u1ec1 vector.\n\n**Gi\u1ea3i ph\u00e1p:** T\u00ecm tr\u00ean 2 t\u1eadp d\u1eef li\u1ec7u nh\u1eefng features n\u00e0o ch\u1ee9a gi\u00e1 tr\u1ecb NaN r\u1ed3i thay th\u1ebf ch\u00fang.","6607ff45":"**V\u1ea5n \u0111\u1ec1:** \u0110\u1ed1i v\u1edbi ph\u1ea7n nh\u1eadn x\u00e9t c\u1ee7a kh\u00e1ch h\u00e0ng **item_description** th\u00ec c\u1ea7n c\u00e1ch x\u1eed l\u00fd kh\u00e1c Bag of words b\u1edfi:\n* Nh\u1eefng t\u1eeb xu\u1ea5t hi\u1ec7n v\u1edbi t\u1ea7n su\u1ea5t nhi\u1ec1u s\u1ebd l\u00e0m lu m\u1edd \u00fd ngh\u0129a c\u1ee7a nh\u1eefng t\u1eeb quan tr\u1ecdng.\n\n**Gi\u1ea3i ph\u00e1p:** C\u00e1ch x\u1eed l\u00fd l\u00e0 d\u00f9ng m\u00f4 h\u00ecnh **TF-IDF**: vector bi\u1ec3u di\u1ec5n \u0111i\u1ec3m d\u1eef li\u1ec7u s\u1ebd d\u1ef1a v\u00e0o \u0111\u1ed9 quan tr\u1ecdng c\u1ee7a t\u1eeb.\n\n\u0110\u1ed9 quan tr\u1ecdng \u0111\u01b0\u1ee3c t\u00ednh theo c\u00f4ng th\u1ee9c:\n\n<a href=\"https:\/\/www.codecogs.com\/eqnedit.php?latex=i&space;*log(\\frac{n}{1&space;&plus;&space;n_{i}})\" target=\"_blank\"><img src=\"https:\/\/latex.codecogs.com\/gif.latex?i&space;*log(\\frac{n}{1&space;&plus;&space;n_{i}})\" title=\"i *log(\\frac{n}{1 + n_{i}})\" \/><\/a>\n\ntrong \u0111\u00f3:\n* $i$: l\u00e0 s\u1ed1 l\u1ea7n t\u1eeb xu\u1ea5t hi\u1ec7n trong c\u00e2u\n* $n$: l\u00e0 t\u1ed5ng s\u1ed1 \u0111i\u1ec3m d\u1eef li\u1ec7u\n* $n_{i}$: l\u00e0 s\u1ed1 \u0111i\u1ec3m d\u1eef li\u1ec7u c\u00f3 t\u1eeb \u0111\u00f3\n\nThay v\u00ec t\u1ea1o kho d\u1eef li\u1ec7u v\u1edbi t\u1eeb ri\u00eang l\u1ebb (unigrams: 'do', 'not') th\u00ec s\u1ebd \u0111i k\u00e8m v\u1edbi 2 t\u1eeb (bigrams: 'do', 'not', 'do-not').","34732f94":"## Preprocessor function \n\nH\u00e0m l\u00e0m s\u1ea1ch text gi\u00fap c\u1ea3i thi\u1ec7n k\u00e9t qu\u1ea3 kh\u00e1 nhi\u1ec1u (gi\u1ea3m 0.1 loss \u1edf k\u1ebft qu\u1ea3 submib Kaggle ~ cao h\u01a1n 100 v\u1ecb tr\u00ed so v\u1edbi kh\u00f4ng d\u00f9ng)\n\nNhi\u1ec7m v\u1ee5:\n* X\u00f3a th\u1ebb tag v\u00e0 c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t.\n* Chuy\u1ec3n k\u00fd t\u1ef1 vi\u1ebft hoa th\u00e0nh th\u01b0\u1eddng.","0c4fe8ae":"Gi\u00e1 tr\u1ecb **brand_name** tr\u00ean t\u1eadp train v\u00e0 test c\u00f3 t\u1ef7 l\u1ec7 t\u01b0\u01a1ng \u0111\u1ed3ng.","78a5c56f":"## Ensemble models (Boosting method)\n\n**C\u01a1 ch\u1ebf ho\u1ea1t \u0111\u1ed9ng**:\n\nM\u1ed7i base model \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 m\u1ed9t weak learner. Ch\u00fang s\u1ebd kh\u00f4ng ho\u1ea1t \u0111\u1ed9ng t\u1ed1t tr\u00ean to\u00e0n b\u1ed9 t\u1eadp D, nh\u01b0ng khi k\u1ebft h\u1ee3p nhi\u1ec1u weak learners ta \u0111\u01b0\u1ee3c m\u1ed9t strong learner. Strong learner n\u00e0y ch\u1eafc ch\u1eafn s\u1ebd hi\u1ec7u qu\u1ea3 tr\u00ean t\u1eadp D.\n\n(M\u1ed9t model thu\u1ed9c nh\u00f3m **Bagging** l\u00e0 RandomForestRegressor \u0111\u00e3 \u0111\u01b0\u1ee3c th\u1eed nh\u01b0ng v\u01b0\u1ee3t qu\u00e1 9 ti\u1ebfng kaggle cho ph\u00e9p)","9a84cf4f":"Thay th\u1ebf NaN th\u00e0nh gi\u00e1 tr\u1ecb null tr\u00ean 3 features: **category**, **brand** v\u00e0 **item_description**","60d73c5f":"## Linear Regression\n\n\u0110\u1ea7u ti\u00ean ta s\u1ebd ch\u1ecdn lo\u1ea1i model ph\u1ed5 bi\u1ebfn v\u00e0 \u0111\u01a1n gi\u1ea3n nh\u1ea5t khi ti\u1ebfp c\u1eadn v\u1edbi b\u00e0i to\u00e1n regression: H\u1ed3i quy tuy\u1ebfn t\u00ednh (tr\u01b0\u1eddng h\u1ee3p n\u00e0y l\u00e0 h\u1ed3i quy \u0111a b\u1ed9i: Multi linear regression).\n\n![Multi linear regression](https:\/\/i.stack.imgur.com\/nqI6I.png)\n\n\u00c1p d\u1ee5ng cho nghi\u00ean c\u1ee9u m\u1ed1i quan h\u1ec7 c\u1ee7a nhi\u1ec1u bi\u1ebfn \u0111\u1ed9c l\u1eadp v\u00e0 m\u1ed9t bi\u1ebfn ph\u1ee5 thu\u1ed9c.\n\nPh\u01b0\u01a1ng tr\u00ecnh t\u1ed5ng qu\u00e1t:\n\n<a href=\"https:\/\/www.codecogs.com\/eqnedit.php?latex=y&space;=&space;b_{0}&space;&plus;&space;b_{1}&space;*&space;x_{1}&space;&plus;&space;b_{2}&space;*&space;x_{2}&space;&plus;&space;...&space;&plus;&space;s\" target=\"_blank\"><img src=\"https:\/\/latex.codecogs.com\/gif.latex?y&space;=&space;b_{0}&space;&plus;&space;b_{1}&space;*&space;x_{1}&space;&plus;&space;b_{2}&space;*&space;x_{2}&space;&plus;&space;...&space;&plus;&space;s\" title=\"y = b_{0} + b_{1} * x_{1} + b_{2} * x_{2} + ... + s\" \/><\/a>\n\ntrong \u0111\u00f3: \n* $y$: bi\u1ebfn ph\u1ee5 thu\u1ed9c (bi\u1ebfn ta s\u1ebd d\u1ef1 \u0111o\u00e1n gi\u00e1 tr\u1ecb)\n* $x_{1}, x_{2}, ...$: bi\u1ebfn \u0111\u1ed9c l\u1eadp (t\u00e1c \u0111\u1ed9ng l\u00ean y)\n* $b_{1}, b_{2}, ...$: \u0111\u1ed9 d\u1ed1c (m\u1ee9c \u0111\u1ed9 thay \u0111\u1ed5i c\u1ee7a y khi x thay \u0111\u1ed5i 1 \u0111\u01a1n v\u1ecb)\n* $s$: sai s\u1ed1\n* $b_{0}$: gi\u00e1 tr\u1ecb ch\u1eb7n (intercept)\n\n![](https:\/\/www.mometrix.com\/blog\/wp-content\/uploads\/2020\/10\/unnamed.png)\n\n\n\nM\u1ed9t nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a m\u00f4 h\u00ecnh n\u00e0y l\u00e0 n\u00f3 r\u1ea5t nh\u1ea1y c\u1ea3m v\u1edbi d\u1eef li\u1ec7u b\u1ea5t th\u01b0\u1eddng (outliners). Trong tr\u01b0\u1eddng h\u1ee3p n\u00e0y \u0111\u00e3 \u0111\u01b0\u1ee3c kh\u1eafc ph\u1ee5c khi \u0111\u01b0a v\u1ec1 d\u1ef1 \u0111o\u00e1n log (\u0111\u1ed3 th\u1ecb kh\u1ea3o s\u00e1t gi\u00e1 \u0111\u00e3 l\u00e0m).\n\n*Update: do thi\u1ebft b\u1ecb kh\u00f4ng \u0111\u1ee7 RAM v\u00e0 kaggle gi\u1edbi h\u1ea1n th\u1eddi gian training \u0111\u1ec3 ch\u1ea1y to\u00e0n b\u1ed9 t\u1eadp d\u1eef li\u1ec7u n\u00ean m\u00f4 h\u00ecnh n\u00e0y ch\u01b0a th\u1ec3 cho ra k\u1ebft qu\u1ea3*.\n\n","2df5255e":"Performance c\u1ee7a Light GBM c\u00f2n k\u00e9m h\u01a1n c\u1ea3 XGBoost. C\u00f3 l\u1ebd Light GBM ch\u1ec9 m\u1ea1nh h\u01a1n XGBoost \u1edf t\u1ed1c \u0111\u1ed9 t\u00ednh to\u00e1n m\u00e0 th\u00f4i.","25e0f18b":"T\u1eadp test data th\u00ec kh\u00f4ng c\u00f3 c\u1ed9t **price** v\u00ec \u0111\u00e2y l\u00e0 gi\u00e1 tr\u1ecb c\u1ea7n d\u1ef1 \u0111o\u00e1n \u0111\u1ec3 submit.","ec313e38":"## Ridge Regression\n\nV\u1ec1 c\u01a1 b\u1ea3n c\u0169ng gi\u1ed1ng v\u1edbi Linear Regression truy\u1ec1n th\u1ed1ng, nh\u01b0ng c\u00f3 \u0111i\u1ec3m kh\u00e1c l\u00e0 c\u00f4ng th\u1ee9c t\u00ecm gi\u00e1 tr\u1ecb h\u1ec7 s\u1ed1 b th\u00ec b\u1ed5 sung th\u00eam r\u00e0ng bu\u1ed9c \u0111\u1ec3 sao cho c\u00e1c h\u1ec7 s\u1ed1 b nh\u1ecf nh\u1ea5t c\u00f3 th\u1ec3 \u0111\u1ebfn m\u1ee9c g\u1ea7n b\u1eb1ng 0, ngh\u0129a l\u00e0 c\u00e1c feature (x) \u00edt c\u00f3 \u1ea3nh h\u01b0\u1edfng t\u1edbi gi\u00e1 tr\u1ecb \u0111\u1ea7u ra.\n\nLambda c\u00f2n g\u1ecdi l\u00e0 tham s\u1ed1 Regularization, hay tham s\u1ed1 Penalty, l\u00e0 s\u1ed1 lu\u00f4n d\u01b0\u01a1ng (ngh\u0129a l\u00e0 h\u1ec7 s\u1ed1 b c\u00e0ng to loss c\u00e0ng l\u1edbn)\n\n![](https:\/\/www.analyticsinsight.net\/wp-content\/uploads\/2017\/09\/ridge_regression_geomteric.png)\n\nTr\u00ean h\u00ecnh l\u00e0 gi\u00e1 tr\u1ecb h\u1ec7 s\u1ed1 b1, b2 theo OLS (linear regression truy\u1ec1n th\u1ed1ng) v\u00e0 theo Ridge.\n\nC\u00e1c h\u1ec7 s\u1ed1 theo Ridge s\u1ebd nh\u1ecf h\u01a1n \u0111\u00e1ng k\u1ec3 so v\u1edbi OLS, d\u1eabn \u0111\u1ebfn \u0111a ph\u1ea7n training score s\u1ebd th\u1ea5p h\u01a1n nh\u01b0ng \u0111i\u1ec3m test th\u1ef1c t\u1ebf l\u1ea1i cao h\u01a1n (so v\u1edbi linear regression truy\u1ec1n th\u1ed1ng). V\u00ec Ridge \u0111\u00e3 lo\u1ea1i b\u1edbt \u0111\u01b0\u1ee3c s\u1ed1 l\u01b0\u1ee3ng c\u00e1c h\u1ec7 s\u1ed1, gi\u1ea3m b\u1edbt \u0111\u1ed9 ph\u1ee9c t\u1ea1p c\u1ee7a m\u00f4 h\u00ecnh, qua \u0111\u00f3 tr\u00e1nh \u0111\u01b0\u1ee3c t\u00ecnh tr\u1ea1ng overfitting, \u0111\u1ea1t \u0111\u01b0\u1ee3c genelization v\u1edbi d\u1eef li\u1ec7u test.","0957aa19":"## Text encoding\n\nD\u1ef1a v\u00e0o s\u1ed1 l\u01b0\u1ee3ng m\u1eabu c\u1ee7a c\u00e1c \u0111\u1eb7c t\u00ednh (unique number of each feature) ta chia d\u1eef li\u1ec7u th\u00e0nh 2 d\u1ea1ng \u0111\u1ec3 x\u1eed l\u00fd:\n* Bi\u1ebfn \u0111\u1ed5i v\u1ec1 d\u1ea1ng vector s\u1ed1: **name**, **item_description**.\n* Ph\u00e2n lo\u1ea1i theo c\u00e1c label: **item_condition_id**, **brand_name**, **shipping**, **cat_top**, **cat_sub**, **cat_item**.","2d05ffaa":"Number of unique elements in **cat_top**: 11","0eae7a30":"## SGD Regressor\n\nV\u1edbi default *loss='squared_loss'*, model \u0111ang s\u1eed d\u1ee5ng l\u00e0 Linear Regression truy\u1ec1n th\u1ed1ng v\u1edbi thu\u1eadt to\u00e1n t\u1ed1i \u01b0u h\u00e0m m\u1ea5t m\u00e1t Stochastic Gradient Descent.\n\nVi\u1ec7c t\u00ecm global minimum c\u1ee7a h\u00e0m m\u1ea5t m\u00e1t g\u1ea7n nh\u01b0 b\u1ea5t kh\u1ea3 thi. Thay v\u00e0o \u0111\u00f3 ta t\u00ecm \u0111i\u1ec3m local minimum c\u00f3 gi\u00e1 tr\u1ecb ch\u1ea5p nh\u1eadn \u0111\u01b0\u1ee3c r\u1ed3i coi \u0111\u00f3 l\u00e0 k\u1ebft qu\u1ea3 c\u1ee7a b\u00e0i to\u00e1n. H\u01b0\u1edbng ti\u1ebfp c\u1eadn ta \u0111\u1ec1 c\u1eadp \u0111\u1ebfn l\u00e0 Gradient Descent: xu\u1ea5t ph\u00e1t t\u1eeb m\u1ed9t \u0111i\u1ec3m m\u00e0 ch\u00fang ta coi l\u00e0 *g\u1ea7n* v\u1edbi nghi\u1ec7m c\u1ee7a b\u00e0i to\u00e1n, sau \u0111\u00f3 d\u00f9ng m\u1ed9t ph\u00e9p to\u00e1n l\u1eb7p \u0111\u1ec3 *ti\u1ebfn d\u1ea7n* \u0111\u1ebfn \u0111i\u1ec3m c\u1ea7n t\u00ecm, t\u1ee9c \u0111\u1ebfn khi \u0111\u1ea1o h\u00e0m g\u1ea7n v\u1edbi 0.\n\n![](https:\/\/1.bp.blogspot.com\/-GQOE2Jf92oA\/XNQDNLoQl7I\/AAAAAAAAA1I\/ifpzMryE2zgx7Y7SGy2IS4nTSNbE2EgggCLcBGAs\/s1600\/theta_new.png)\n\nH\u00e0m m\u1ea5t m\u00e1t c\u1ee7a Linear Regression l\u00e0:\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/89\/Linear_Regression_Loss_Function.png)\n\nGradient Descent\n\nT\u00ecm local minimum b\u1eb1ng c\u00e1ch c\u1eadp nh\u1eadt $w_{t}$ (vector c\u00e1c h\u1ec7 s\u1ed1), d\u1ea5u tr\u1eeb th\u1ec3 hi\u1ec7n vi\u1ec7c ph\u1ea3i \u0111i ng\u01b0\u1ee3c v\u1edbi \u0111\u1ea1o h\u00e0m: \n\n<a href=\"https:\/\/www.codecogs.com\/eqnedit.php?latex=w_{t&plus;1}&space;=&space;w_{t}&space;-&space;\\Delta&space;*&space;{loss}'(w)\" target=\"_blank\"><img src=\"https:\/\/latex.codecogs.com\/gif.latex?w_{t&plus;1}&space;=&space;w_{t}&space;-&space;\\Delta&space;*&space;{loss}'(w)\" title=\"w_{t+1} = w_{t} - \\Delta * {loss}'(w)\" \/><\/a>\n\ntrong \u0111\u00f3:\n* $\\Delta$: t\u1ed1c \u0111\u1ed9 h\u1ecdc (learning rate)\n* ${loss}'(w)$: \u0111\u1ea1o h\u00e0m c\u1ee7a h\u00e0m loss theo w\n\nStochastic Gradient Descent\n\nThay v\u00ec t\u00ednh \u0111\u1ea1o h\u00e0m h\u00e0m loss tr\u00ean to\u00e0n b\u1ed9 t\u1eadp d\u1eef li\u1ec7u r\u1ed3i m\u1edbi c\u1eadp nh\u1eadt w, gi\u1edd \u0111\u00e2y ta c\u1eadp nh\u1eadt tr\u00ean m\u1ed7i \u0111i\u1ec3m d\u1eef li\u1ec7u. Vi\u1ec7c c\u1eadp nh\u1eadt t\u1eebng \u0111i\u1ec3m m\u1ed9t nh\u01b0 th\u1ebf n\u00e0y c\u00f3 th\u1ec3 l\u00e0m gi\u1ea3m \u0111i t\u1ed1c \u0111\u1ed9 th\u1ef1c hi\u1ec7n 1 epoch (m\u1ed9t l\u1ea7n duy\u1ec7t qua to\u00e0n b\u1ed9 c\u00e1c \u0111i\u1ec3m tr\u00ean t\u1eadp d\u1eef li\u1ec7u). Nh\u01b0ng m\u1eb7t kh\u00e1c, SGD ch\u1ec9 y\u00eau c\u1ea7u m\u1ed9t l\u01b0\u1ee3ng epoch r\u1ea5t nh\u1ecf. V\u00ec v\u1eady SGD ph\u00f9 h\u1ee3p v\u1edbi c\u00e1c b\u00e0i to\u00e1n c\u00f3 l\u01b0\u1ee3ng data l\u1edbn.","ae9f51c5":"### XGBoost\nS\u1eed d\u1ee5ng nhi\u1ec1u base model l\u00e0 Decision Tree, l\u00e0m m\u1ecbn Training Loss (Sai s\u1ed1 khi hu\u1ea5n luy\u1ec7n) v\u00e0 Regularization (Chu\u1ea9n h\u00f3a sai s\u1ed1, h\u1ec7 s\u1ed1 v\u00e0 s\u1ed1 bi\u1ebfn)\n\n\n\nMang nhi\u1ec1u \u01b0u \u0111i\u1ec3m v\u01b0\u1ee3t tr\u1ed9i nh\u01b0:\n* T\u1ed1c \u0111\u1ed9 x\u1eed l\u00fd\n* C\u01a1 ch\u1ebf regularization x\u1eed l\u00fd overfit\n* T\u1ef1 \u0111\u1ed9ng b\u1ecf qua nodes kh\u00f4ng mang gi\u00e1 tr\u1ecb t\u00edch c\u1ef1c trong vi\u1ec7c m\u1edf r\u1ed9ng Tree\n* ...\n\nCh\u00ednh v\u00ec nh\u1eefng \u01b0u \u0111i\u1ec3m \u0111\u00f3 m\u00e0 hi\u1ec7u n\u0103ng c\u1ee7a XGBoost t\u0103ng l\u00ean \u0111\u00e1ng k\u1ec3 so v\u1edbi c\u00e1c thu\u1eadt to\u00e1n ensemble learning kh\u00e1c.","22fb6ac4":"Number of unique elements in **shipping**: 2","000dbe28":"# Model training\n\n3 models c\u1ed5 \u0111i\u1ec3n \u0111\u01b0\u1ee3c [sklearn](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#classical-linear-regressors) h\u1ed7 tr\u1ee3.","41527cbb":"5 gi\u00e1 tr\u1ecb **item_condition_id** tr\u00ean t\u1eadp train v\u00e0 test c\u00f3 t\u1ef7 l\u1ec7 t\u01b0\u01a1ng \u0111\u1ed3ng."}}