{"cell_type":{"b34ae85a":"code","adbf89b9":"code","7feba6bf":"code","9e565ca7":"code","ed9b2ac8":"code","a26753ed":"code","3eda7bcc":"code","c33c26ec":"code","7d1ff595":"code","f8391dd0":"code","6d5660ec":"code","c5a482f7":"code","964fb619":"code","2da3af56":"code","f3526048":"code","8b2cec95":"code","3cdc0822":"code","45282fef":"code","f425fba4":"code","024c0f93":"code","6b590d78":"code","e7e964af":"code","af1a77d0":"code","be066f94":"code","92e25ea5":"code","ac362967":"code","3aa7de93":"code","2111b953":"code","577a5d7a":"code","b43588b5":"code","62a79103":"code","3409ce32":"code","a6330423":"code","1929900f":"code","ad78a229":"code","5ee0ad4d":"code","0033ccec":"code","e6e56f2a":"code","3b061314":"code","d43ac01e":"code","7212dbd8":"code","f83ab27f":"code","ee0bc1a6":"code","d03d21be":"code","75848e1b":"code","d4849598":"code","ca2074ca":"code","2a67106f":"code","3fb9002c":"code","64091d35":"code","65fbccda":"code","73674b92":"code","1e8a1b44":"code","feba6b17":"code","988e7f0c":"code","54d33565":"code","d231a56e":"code","32aaffd4":"code","8e93a91d":"code","a466cb55":"code","9377b027":"code","4328fd7f":"code","68bd5907":"code","f7718c2f":"code","9a8ae485":"code","5ba4eaf4":"code","2d7fa33b":"code","8fa51e2d":"code","6872ab62":"code","f5ff9ba9":"code","38844d34":"code","84cc7ff5":"code","0b2cff31":"code","824d6111":"code","e0eecc28":"code","2a8df38a":"code","43e0e363":"code","bfcef7e3":"code","f15d837f":"code","6cdaf0b3":"code","5234e284":"code","a0655a53":"code","d75d832f":"code","f4cf3a58":"code","5430c2a0":"code","da67d870":"markdown","e426225c":"markdown","2b195f0b":"markdown","bb630a95":"markdown","b201faca":"markdown","6e94b0c9":"markdown","caff4c34":"markdown","285ab500":"markdown","669488d7":"markdown","4371e105":"markdown","b3c66aa4":"markdown","b1a06b40":"markdown","79d0644c":"markdown","98e5ccea":"markdown","8c5286af":"markdown","1d1ce7d7":"markdown","0ddaebb4":"markdown","fa663b4f":"markdown","fef6d9d0":"markdown","9d78fae7":"markdown","eeb4d6fe":"markdown","92e0743c":"markdown","59f974cf":"markdown","64ca55e3":"markdown","d0b5df14":"markdown","a4ca9e0a":"markdown","3105039d":"markdown","17fc9dd7":"markdown","f1e6a534":"markdown","c5400711":"markdown","f1990236":"markdown","ba8b3d0d":"markdown","c5b3a956":"markdown","c4ec0da8":"markdown","a8f469e8":"markdown","5cd169be":"markdown","a6dd4045":"markdown","782c2d2a":"markdown","bf3f2c68":"markdown","2ad33745":"markdown","61538c40":"markdown","03b03408":"markdown","4f45c079":"markdown","dfd764dc":"markdown","92d8ea91":"markdown","87a34d1c":"markdown","94ae27b0":"markdown","6d13e36c":"markdown","63539e9e":"markdown","d63eda69":"markdown","f61ae337":"markdown","4531fd01":"markdown","991e1aa3":"markdown"},"source":{"b34ae85a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","adbf89b9":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","7feba6bf":"\n\ntrain.head()\n\n","9e565ca7":"train.shape","ed9b2ac8":"\n\ntrain.describe()\n\n","a26753ed":"train.describe(include=['O'])","3eda7bcc":"train.info()","c33c26ec":"train.isnull().sum()","7d1ff595":"test.shape\n","f8391dd0":"test.head()","6d5660ec":"\n\ntest.info()\n\n","c5a482f7":"survived = train[train['Survived'] == 1]\nnot_survived = train[train['Survived'] == 0]\n\nprint (\"Survived: %i (%.1f%%)\"%(len(survived), float(len(survived))\/len(train)*100.0))\nprint (\"Not Survived: %i (%.1f%%)\"%(len(not_survived), float(len(not_survived))\/len(train)*100.0))\nprint (\"Total: %i\"%len(train))","964fb619":"train.Pclass.value_counts()","2da3af56":"train.groupby('Pclass').Survived.value_counts()","f3526048":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()","8b2cec95":"sns.barplot(x='Pclass', y='Survived', data=train)","3cdc0822":"train.Sex.value_counts()","45282fef":"train.groupby('Sex').Survived.value_counts()","f425fba4":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()","024c0f93":"sns.barplot(x='Sex', y='Survived', data=train)","6b590d78":"tab = pd.crosstab(train['Pclass'], train['Sex'])\nprint (tab)\n\ntab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\nplt.xlabel('Pclass')\nplt.ylabel('Percentage')","e7e964af":"sns.factorplot('Sex', 'Survived', hue='Pclass', size=4, aspect=2, data=train)","af1a77d0":"sns.factorplot(x='Pclass', y='Survived', hue='Sex', col='Embarked', data=train)","be066f94":"train.Embarked.value_counts()","92e25ea5":"train.groupby('Embarked').Survived.value_counts()","ac362967":"train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean()","3aa7de93":"sns.barplot(x='Embarked', y='Survived', data=train)","2111b953":"train.Parch.value_counts()","577a5d7a":"train.groupby('Parch').Survived.value_counts()","b43588b5":"train[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean()","62a79103":"sns.barplot(x='Parch', y='Survived', ci=None, data=train) # ci=None will hide the error bar","3409ce32":"train.SibSp.value_counts()","a6330423":"train.groupby('SibSp').Survived.value_counts()","1929900f":"train[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean()","ad78a229":"sns.barplot(x='SibSp', y='Survived', ci=None, data=train) # ci=None will hide the error bar","5ee0ad4d":"train.Age.value_counts()","0033ccec":"train.groupby('Age')['Survived'].mean()","e6e56f2a":"total_survived = train[train['Survived']==1]\ntotal_not_survived = train[train['Survived']==0]\nmale_survived = train[(train['Survived']==1) & (train['Sex']==\"male\")]\nfemale_survived = train[(train['Survived']==1) & (train['Sex']==\"female\")]\nmale_not_survived = train[(train['Survived']==0) & (train['Sex']==\"male\")]\nfemale_not_survived = train[(train['Survived']==0) & (train['Sex']==\"female\")]","3b061314":"plt.figure(figsize=(15,6))\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=0.6, square=True, annot=True)","d43ac01e":"train_test_data = [train, test] # combining train and test dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.')","7212dbd8":"\n\ntrain.head()\n\n","f83ab27f":"pd.crosstab(train['Title'], train['Sex'])","ee0bc1a6":"for dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', \\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n","d03d21be":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","75848e1b":"\n\ntrain.head()\n\n","d4849598":"for dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n","ca2074ca":"train.head()","2a67106f":"train.Embarked.unique()","3fb9002c":"train.Embarked.value_counts()","64091d35":"for dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","65fbccda":"train.head()","73674b92":"for dataset in train_test_data:\n    \n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","1e8a1b44":"train.head()","feba6b17":"for dataset in train_test_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ntrain['AgeBand'] = pd.cut(train['Age'], 5)\n\nprint (train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean())","988e7f0c":"train.head()","54d33565":"for dataset in train_test_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n","d231a56e":"train.head()","32aaffd4":"for dataset in train_test_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())","8e93a91d":"train['FareBand'] = pd.qcut(train['Fare'], 4)\nprint (train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean())","a466cb55":"train.head()","9377b027":"for dataset in train_test_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)","4328fd7f":"train.head()","68bd5907":"for dataset in train_test_data:\n    dataset['FamilySize'] = dataset['SibSp'] +  dataset['Parch'] + 1\n\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","f7718c2f":"for dataset in train_test_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \nprint (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","9a8ae485":"train.head(1)","5ba4eaf4":"test.head(1)","2d7fa33b":"features_drop = ['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'FamilySize']\ntrain = train.drop(features_drop, axis=1)\ntest = test.drop(features_drop, axis=1)\ntrain = train.drop(['PassengerId', 'AgeBand', 'FareBand'], axis=1)","8fa51e2d":"train.head()","6872ab62":"test.head()","f5ff9ba9":"# Defining training and testing set\n\nX_train = train.drop('Survived', axis=1)\ny_train = train['Survived']\nX_test = test.drop(\"PassengerId\", axis=1).copy()\n\nX_train.shape, y_train.shape, X_test.shape","38844d34":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nimport xgboost as xgb","84cc7ff5":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\ny_pred_log_reg = clf.predict(X_test)\nacc_log_reg = round( clf.score(X_train, y_train) * 100, 2)\nprint (str(acc_log_reg) + ' percent')","0b2cff31":"clf = SVC()\nclf.fit(X_train, y_train)\ny_pred_svc = clf.predict(X_test)\nacc_svc = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_svc)","824d6111":"clf = LinearSVC()\nclf.fit(X_train, y_train)\ny_pred_linear_svc = clf.predict(X_test)\nacc_linear_svc = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_linear_svc)","e0eecc28":"clf = KNeighborsClassifier(n_neighbors = 3)\nclf.fit(X_train, y_train)\ny_pred_knn = clf.predict(X_test)\nacc_knn = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_knn)","2a8df38a":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred_decision_tree = clf.predict(X_test)\nacc_decision_tree = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_decision_tree)","43e0e363":"clf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred_random_forest = clf.predict(X_test)\nacc_random_forest = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_random_forest)","bfcef7e3":"clf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred_gnb = clf.predict(X_test)\nacc_gnb = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_gnb)","f15d837f":"clf = Perceptron(max_iter=5, tol=None)\nclf.fit(X_train, y_train)\ny_pred_perceptron = clf.predict(X_test)\nacc_perceptron = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_perceptron)","6cdaf0b3":"clf = SGDClassifier(max_iter=5, tol=None)\nclf.fit(X_train, y_train)\ny_pred_sgd = clf.predict(X_test)\nacc_sgd = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_sgd)","5234e284":"xgb_classifier = xgb.XGBClassifier()\nxgb_classifier.fit(X_train, y_train)\ny_pred_xgb = xgb_classifier.predict(X_test)\nacc_xgb = round(xgb_classifier.score(X_train, y_train) * 100, 2)\nprint (acc_xgb)","a0655a53":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred_random_forest_training_set = clf.predict(X_train)\nacc_random_forest = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy: %i %% \\n\"%acc_random_forest)\n\nclass_names = ['Survived', 'Not Survived']\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_train, y_pred_random_forest_training_set)\nnp.set_printoptions(precision=2)\n\nprint ('Confusion Matrix in Numbers')\nprint (cnf_matrix)\nprint ('')\n\ncnf_matrix_percent = cnf_matrix.astype('float') \/ cnf_matrix.sum(axis=1)[:, np.newaxis]\n\nprint ('Confusion Matrix in Percentage')\nprint (cnf_matrix_percent)\nprint ('')\n\ntrue_class_names = ['True Survived', 'True Not Survived']\npredicted_class_names = ['Predicted Survived', 'Predicted Not Survived']\n\ndf_cnf_matrix = pd.DataFrame(cnf_matrix, \n                             index = true_class_names,\n                             columns = predicted_class_names)\n\ndf_cnf_matrix_percent = pd.DataFrame(cnf_matrix_percent, \n                                     index = true_class_names,\n                                     columns = predicted_class_names)\n\nplt.figure(figsize = (15,5))\n\nplt.subplot(121)\nsns.heatmap(df_cnf_matrix, annot=True, fmt='d')\n\nplt.subplot(122)\nsns.heatmap(df_cnf_matrix_percent, annot=True)","d75d832f":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', 'Linear SVC', \n              'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes', \n              'Perceptron', 'Stochastic Gradient Decent', 'XgBoost'],\n    \n    'Score': [acc_log_reg, acc_svc, acc_linear_svc, \n              acc_knn,  acc_decision_tree, acc_random_forest, acc_gnb, \n              acc_perceptron, acc_sgd, acc_xgb]\n    })\n\nmodels.sort_values(by='Score', ascending=False)\n\n","f4cf3a58":"test.head()","5430c2a0":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred_random_forest\n    })\n\nsubmission.to_csv('gender_submission.csv', index=False)","da67d870":"## Classification & Accuracy","e426225c":"## Gaussian Naive Bayes","2b195f0b":"After that, we convert the categorical Title values into numeric form.","bb630a95":"## Support Vector Machine (SVM)","b201faca":"## Create Submission File to Kaggle","6e94b0c9":"We are done with Feature Selection\/Engineering. Now, we are ready to train a classifier with our feature set.","caff4c34":"describe(include = ['O']) will show the descriptive statistics of object data types.","285ab500":"Create FareBand. We divide the Fare into 4 category range.","669488d7":"## Stochastic Gradient Descent (SGD)","4371e105":"\n\nFrom the above plot, it can be seen that:\n\n*     Women from 1st and 2nd Pclass have almost 100% survival chance.\n*     Men from 2nd and 3rd Pclass have only around 10% survival chance.\n\n","b3c66aa4":"\n\nWe can see that Age value is missing for many rows.\n\nOut of 891 rows, the Age value is present only in 714 rows.\n\nSimilarly, Cabin values are also missing in many rows. Only 204 out of 891 rows have Cabin values.\n","b1a06b40":"## Linear SVM","79d0644c":"\n## Sex vs. Survival\n\nFemales have better survival chance.\n","98e5ccea":"\n### Correlating Features\n\nHeatmap of Correlation between different features:\n\n    Positive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.\n\n    Negative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the Survived feature.\n","8c5286af":"\n\nThere are many classifying algorithms present. Among them, we choose the following Classification algorithms for our problem:\n\n    Logistic Regression\n    Support Vector Machines (SVC)\n    Linear SVC\n    k-Nearest Neighbor (KNN)\n    Decision Tree\n    Random Forest\n    Naive Bayes (GaussianNB)\n    Perceptron\n    Stochastic Gradient Descent (SGD)\n    XgBoost\n\nHere's the training and testing procedure:\n\n    First, we train these classifiers with our training data.\n\n    After that, using the trained classifier, we predict the Survival outcome of test data.\n\n    Finally, we calculate the accuracy score (in percentange) of the trained classifier.\n\nPlease note: that the accuracy score is generated based on our training dataset.\n\n","1d1ce7d7":"## Random Forest","0ddaebb4":"The number of passengers with each Title is shown above.\n\nWe now replace some less common titles with the name \"Other\".","fa663b4f":"\n## Total rows and columns\n\nWe can see that there are 891 rows and 12 columns in our training dataset.\n","fef6d9d0":"## k-Nearest Neighbors","9d78fae7":"\n## Feature Selection\n\nWe drop unnecessary columns\/features and keep only the useful ones for our experiment. Column PassengerId is only dropped from Train set because we need PassengerId in Test set while creating Submission file to Kaggle.\n","eeb4d6fe":"We now convert the categorical value of Embarked into numeric. We represent 0 as S, 1 as C and 2 as Q","92e0743c":"\n## **Logistic Regression**","59f974cf":"\n## Comparing Models\n\nLet's compare the accuracy score of all the classifier models used above.\n","64ca55e3":"There are missing entries for Age in Test dataset as well.\n\nOut of 418 rows in Test dataset, only 332 rows have Age value.","d0b5df14":"Now, we map Age according to AgeBand.","a4ca9e0a":"## Embarked vs. Survived","3105039d":"\n## Pclass vs. Survival\n\nHigher class passengers have better survival chance.\n","17fc9dd7":"## Decision Tree","f1e6a534":"We find that category \"S\" has maximum passengers. Hence, we replace \"nan\" values with \"S\".","c5400711":"## SibSp vs. Survival","f1990236":"As you can see above, we have added a new column named Title in the Train dataset with the Title present in the particular passenger name","ba8b3d0d":"\n## Fare Feature\n\nReplace missing Fare values with the median of Fare.\n","c5b3a956":"## Relationship between Features and Survival\n\nIn this section, we analyze relationship between different features with respect to Survival. We see how different feature values show different survival chance. We also plot different kinds of diagrams to visualize our data and findings.","c4ec0da8":"\n\nAbout data shows that:\n\n*     Having FamilySize upto 4 (from 2 to 4) has better survival chance.\n*     FamilySize = 1, i.e. travelling alone has less survival chance.\n*     Large FamilySize (size of 5 and above) also have less survival chance","a8f469e8":"## Xgboost","5cd169be":"## Describing training dataset\n\ndescribe() method can show different values like count, mean, standard deviation, etc. of numeric data types.\n","a6dd4045":"## Perceptron","782c2d2a":"\n\nFrom the above plot, it can be seen that:\n\n*     Almost all females from Pclass 1 and 2 survived.\n*     Females dying were mostly from 3rd Pclass.\n*     Males from Pclass 1 only have slightly higher survival chance than Pclass 2 and 3.\n\n","bf3f2c68":"## Sex Feature\n\nWe convert the categorical value of Sex into numeric. We represent 0 as female and 1 as male.","2ad33745":"## Embarked Feature\n\nThere are empty values for some rows for Embarked column. The empty values are represented as \"nan\" in below list.","61538c40":"## SibSp & Parch Feature\n\nCombining SibSp & Parch feature, we create a new feature named FamilySize.","03b03408":"\n## Feature Extraction\n\nIn this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form.\nName Feature\n\nLet's first extract titles from Name column.\n","4f45c079":"This shows that travelling alone has only 30% survival chance.","dfd764dc":"## Confusion Matrix","92d8ea91":"## Age Feature\n\nWe first fill the NULL values of Age with a random number between (mean_age - std_age) and (mean_age + std_age).\n\nWe then create a new column named AgeBand. This categorizes age into 5 different age range.","87a34d1c":"## Parch vs. Survival","94ae27b0":"## Age vs. Survival","6d13e36c":"Map Fare according to FareBand","63539e9e":"Let's create a new feature named IsAlone. This feature is used to check how is the survival chance while travelling alone as compared to travelling with family.","d63eda69":"## Pclass, Sex & Embarked vs. Survival","f61ae337":"We use info() method to see more information of our train dataset","4531fd01":"\n## Looking into the training dataset\n\nPrinting first 5 rows of the train dataset.\n","991e1aa3":"\n## Pclass & Sex vs. Survival\n\nBelow, we just find out how many males and females are there in each Pclass. We then plot a stacked bar diagram with that information. We found that there are more males among the 3rd Pclass passengers.\n"}}