{"cell_type":{"561ad4a5":"code","8b6d0637":"code","22a8aa61":"code","818eb137":"code","2b4e94dc":"code","0a07e0e6":"code","4a2321bd":"code","6b22b5ae":"code","ebc4f182":"code","9a15adcf":"code","83be1275":"code","456b08b9":"code","84e3cba6":"code","45f2953c":"code","d5c8f697":"code","0aae8bba":"code","3a6abf93":"code","0508d28c":"code","7b727ee9":"markdown","a16bd513":"markdown","6976e62d":"markdown","383843b9":"markdown","9b6c7dc3":"markdown","8ef1cb70":"markdown","48db7805":"markdown","27e8c0a8":"markdown","d0482d7d":"markdown","498fce94":"markdown","295c0553":"markdown","c74a9da1":"markdown","57cf4edb":"markdown","f05fbea8":"markdown"},"source":{"561ad4a5":"import pandas as pd\nimport tensorflow as tf\nimport keras\nfrom keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.applications import InceptionV3\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Flatten,BatchNormalization, Dropout, Lambda, Conv2D, MaxPool2D\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2","8b6d0637":"data = pd.read_pickle(\"..\/input\/traffic-signs-preprocessed\/data8.pickle\")\n","22a8aa61":"data.keys()","818eb137":"print(\"x train shape:\", data[\"x_train\"].shape)\nprint(\"y train shape:\", data[\"y_train\"].shape)\nprint(\"x test shape:\", data[\"x_test\"].shape)\nprint(\"y test shape:\", data[\"y_test\"].shape)\nprint(\"x validation shape:\", data[\"x_validation\"].shape)\nprint(\"y validation shape:\", data[\"y_validation\"].shape)","2b4e94dc":"x_train = data[\"x_train\"]\nx_test = data[\"x_test\"]\nx_val = data[\"x_validation\"]\ny_train = data[\"y_train\"]\ny_val = data[\"y_validation\"]","0a07e0e6":"x_train = x_train.swapaxes(1,2)\nx_train.shape","4a2321bd":"x_train = x_train.swapaxes(2,3)\nx_train.shape","6b22b5ae":"x_val = x_val.swapaxes(1,2)\nx_val = x_val.swapaxes(2,3)\nprint(\"x val shape:\", x_val.shape)","ebc4f182":"plt.figure(figsize=(10,10)) \n\nfor i in range(16):\n    plt.subplot(4,4,i+1)   \n    plt.imshow(x_train[i], cmap = \"gray\")\n    plt.axis(\"off\")\n\nplt.show()","9a15adcf":"img_list = [1,2,17]\nfor i in img_list:\n    plt.imshow(x_train[i],cmap = \"gray\")\n    plt.axis(\"off\")\n    plt.show()","83be1275":"def resize(img):\n    numberofImage = img.shape[0]\n    new_array = np.zeros((numberofImage,64,64,1))\n    for i in range(numberofImage):\n        new_array[i] = tf.image.resize(img[i],(64,64))\n    return new_array","456b08b9":"x_train_resized = resize(x_train)\nx_val_resized = resize(x_val)\nprint(\"x train resized shape:\", x_train_resized.shape)\nprint(\"x validation resized shape:\", x_val_resized.shape)","84e3cba6":"NumberofClass = 43\ny_train = to_categorical(y_train, num_classes = NumberofClass)\ny_val = to_categorical(y_val, num_classes = NumberofClass)","45f2953c":"plt.imshow(x_train[2], cmap = \"gray\")\nplt.title(\"32x32 Image\")\nplt.axis(\"off\")\nplt.show()\n\nplt.figure()\n\nplt.imshow(x_train_resized[2], cmap = \"gray\")\nplt.title(\"64x64 Image\")\nplt.axis(\"off\")\nplt.show()\n","d5c8f697":"\nmodel = Sequential()\n\n\nmodel.add(Conv2D(filters = 128, kernel_size = (4,4), padding = \"Same\", activation = \"relu\", input_shape = (64,64,1)))\n\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (4,4), padding = \"Same\", activation = \"relu\" ))\n\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (4,4), padding = \"Same\", activation = \"relu\" ))\n\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\nmodel.add(Conv2D(filters = 16, kernel_size = (4,4), padding = \"Same\", activation = \"relu\" ))\n\nmodel.add(MaxPool2D(pool_size = (2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units = 512, activation = \"relu\"))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units = NumberofClass, activation = \"softmax\"))\n","0aae8bba":"model.compile(optimizer = \"rmsprop\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])","3a6abf93":"hist = model.fit(x_train_resized, y_train, batch_size = 512,\n                    epochs = 10, validation_data = (x_val_resized, y_val))\n\n","0508d28c":"plt.style.use('seaborn')\nplt.figure(figsize=(6,6))\nplt.plot(hist.history['loss'], color='b', label=\"Training loss\")\nplt.plot(hist.history['val_loss'], color='r', label=\"Validation loss\")\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.figure(figsize=(6,6))\nplt.plot(hist.history['accuracy'], color='b', label=\"Training accuracy\")\nplt.plot(hist.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nplt.legend()\nplt.show()","7b727ee9":"## Visualization","a16bd513":"## Compiling the Model","6976e62d":"## Data Processing","383843b9":"This notebook consists of the following topics:\n\n- About Dataset\n- Import Libraries\n- Loading Data\n- Data Processing\n- Visualization\n- Reshaping\n- CNN Model\n- Compiling the Model\n- Fitting\n- Evaluating","9b6c7dc3":"## Fitting ","8ef1cb70":"![](http:\/\/![image.png](attachment:8ed3a354-6988-45cb-a304-1b55bff21aa6.png))","48db7805":"## Evaluating","27e8c0a8":"## Import Libraries","d0482d7d":"## Loading Data","498fce94":"The dataset to be used in this notebook consists of 86989 train images, 12630 test images, and 4410 validation traffic sign images. The images in the dataset are 32x32 in size and have rgb color channel.","295c0553":"## CNN Model","c74a9da1":"# Traffic Signs Classification","57cf4edb":"## Reshaping","f05fbea8":"## About Dataset"}}