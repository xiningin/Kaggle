{"cell_type":{"b5a2f6fe":"code","596fc92c":"code","ca276aec":"code","9a180135":"code","94bb1183":"code","c64d0c04":"code","1b2a4c7f":"code","f3f336e1":"code","a8989e05":"code","d5e0d8cd":"code","10198932":"code","96d8256e":"code","1e85d27d":"code","c1d11c30":"code","da297646":"code","5d768c59":"code","028f4892":"code","63925506":"code","0665b4f9":"code","36ab0bd5":"code","f0f078d9":"code","431ca83c":"code","03bf151a":"code","3e12c548":"code","8a8238c0":"code","551523c8":"markdown","8089cd86":"markdown","d861c1d8":"markdown","15aced90":"markdown","57faa364":"markdown","d261ab72":"markdown","5ad098d1":"markdown","b05b8bda":"markdown","76b2149b":"markdown","e4d5bb7b":"markdown","34ab2c72":"markdown","918b83ac":"markdown","6f58b824":"markdown","0723c268":"markdown","610b5b63":"markdown","dc6a68d4":"markdown","2250b422":"markdown","ed87f4c7":"markdown","248f0037":"markdown","926c9912":"markdown","39685fd6":"markdown","2c8f3c92":"markdown","37cb22f6":"markdown","058259e7":"markdown","167b3574":"markdown","2376ddea":"markdown","473365ef":"markdown","b3f3d399":"markdown","ad745a1c":"markdown"},"source":{"b5a2f6fe":"# For downloading resources (pre-trained models and dictionaries) from Google Drive\n!conda install -y gdown\n\n# For named entity recognition (NER)\n!pip install transformers\n!pip install seqeval\n!pip install spacy-lookup\n\n# For knowledge graph generation and querying using Grakn \n!pip install ijson\n!pip install grakn-client\n\n# For open information extraction (NER)\n!pip uninstall -y typing\n!pip uninstall -y allennlp\n!pip install git+https:\/\/github.com\/allenai\/allennlp-models.git@a9be0f0236c1e6d16f17f5c0e27d35ad7f221a72#egg=allennlp_models \n!pip install git+https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.2.5\/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm\n!pip install git+https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz#egg=en_core_sci_lg\n!pip install git+https:\/\/github.com\/schlevik\/tmdm\n\n# For visualisation \n!pip install pyvis\n\n","596fc92c":"import csv\nimport spacy\nimport gdown\nimport tarfile\nimport codecs\nimport json\nimport os\nimport string\n\nfrom typing import Tuple, List\nfrom collections import OrderedDict\nfrom transformers import pipeline\nfrom spacy_lookup import Entity\nfrom gensim.models import Word2Vec\n\nnlp = spacy.load('en_core_web_sm')","ca276aec":"class Paper:\n    paper_id = ''\n    title = ''\n    abstract = ''\n    sections = OrderedDict()\n    \n    def __init__(self, json_object):\n        if json_object is not None:\n            self.paper_id = json_object['paper_id']\n            self.title = json_object['metadata']['title']\n\n            #paper abstract\n            abstract_text = ''\n            if 'abstract' in json_object:\n              paper_abstract_array = json_object['abstract']\n              for i in range(0,len(paper_abstract_array)):\n                  span = paper_abstract_array[i]\n                  abstract_text = abstract_text + span['text']\n                  if i < len(paper_abstract_array)-1:\n                      abstract_text = abstract_text + ' '\n            self.abstract = abstract_text\n\n            #paper sections\n            body_sections = OrderedDict()\n            if 'body_text' in json_object:\n              body_text_array = json_object['body_text']\n              for i in range(0,len(body_text_array)):\n                  span = body_text_array[i]\n                  section_heading = span['section']\n                  if section_heading in body_sections:\n                      section_text = body_sections[section_heading]\n                      body_sections[section_heading] = section_text + ' ' + span['text']\n                  else:\n                      body_sections[section_heading] = span['text']\n            self.sections = body_sections\n        \n    def get_whole_body_text(self, include_section_headings):\n        body_text = ''\n        for section in self.sections:\n            if include_section_headings == True:\n                body_text = body_text + section + '\\n'\n            body_text = body_text + self.sections[section] + '\\n'\n        return body_text\n\nclass Sentence:\n  sentence_text = ''\n  nes = set()\n  paper_id = ''\n  section = ''\n    \nclass NE():\n  begin = 0\n  end = 0\n  ne_type = ''\n  \n  def __init__(self, begin, end, ne_type):\n    self.begin = begin\n    self.end = end\n    self.ne_type = ne_type\n\n  def __hash__(self):\n    return hash(str(self.begin) + '-'+ str(self.end) + ':' + self.ne_type)\n\n  def __eq__(self, other):\n    if isinstance(other, NE):\n      if self.begin == other.begin and self.end == other.end and self.ne_type == other.ne_type:\n        return True\n      else:\n        return False\n    else:\n      return False\n\n  def __str__(self):\n    return self.ne_type + ': ' + str(self.begin) + ', ' + str(self.end)\n\nclass NEToken:\n  word = ''\n  label = ''\n  score = ''\n  def __init__(self, word, label):\n    self.word = word\n    self.label = label\n\n  def __str__(self):\n    return self.label + '\\t' + self.word","9a180135":"def load_abstracts(file_path):\n    abstracts_dict = dict()\n    csvfile = open(file_path, 'r')\n    \n    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n    next(reader)\n    i = 1\n    for row in reader:\n        abstract = Paper(None)\n        abstract.paper_id = row[0]\n        abstract.title = row[3]\n        temp_abstract = row[8]\n        if temp_abstract.startswith('Abstract '):\n          temp_abstract = temp_abstract.replace('Abstract ', '', 1)\n        abstract.abstract = temp_abstract\n        abstracts_dict[row[0]] = abstract\n        i = i+ 1\n    csvfile.close()\n    return abstracts_dict\n\npapers = load_abstracts('..\/input\/CORD-19-research-challenge\/metadata.csv')\n\n# Function for sentence splitting\ndef get_sentences(text):  \n    doc = nlp(text.strip())\n    return list(doc.sents)","94bb1183":"# Entity types of interest\nDRUG_LABEL = 'Chemical'\nDISEASE_LABEL = 'Disease'\n\n# Download and load the deep learning-based NER model trained using Huggingface Transformers\nurl = 'https:\/\/drive.google.com\/uc?id=1TbSTVz-WqOZGkOzbMCTBWOAyPwfGrpC-'\noutput = 'cdr-model.tar.gz'\ngdown.download(url, output, quiet=False)\n\nmy_tar = tarfile.open('cdr-model.tar.gz')\nmy_tar.extractall() \nmy_tar.close()\nos.remove('cdr-model.tar.gz')\n\nCDR_MODEL_PATH = '.\/cdr-model\/'\ncd_ner = pipeline('ner', model=CDR_MODEL_PATH + 'tf_model.h5', config=CDR_MODEL_PATH + 'config.json', tokenizer='allenai\/scibert_scivocab_uncased', device=0)\n\n# Getting the right labels in the model\ndef map_labels(model_dir_path):\n  # load labels file\n  labels_filename = model_dir_path + 'labels.txt'\n  labels_file = open(labels_filename, 'r')\n  labels = labels_file.readlines()\n  labels_file.close()\n\n  label_list = []\n  for label in labels:\n    label_list.append(label.strip())\n\n\n  # load config file\n  config_filename = model_dir_path + 'config.json'\n  json_file = open(config_filename, 'r')\n  json_object = json.loads(json_file.read())\n  json_file.close()\n\n  label_dict = json_object['label2id']\n  return label_dict, label_list\n\ncd_dict, cd_labels = map_labels(CDR_MODEL_PATH)\n\n","c64d0c04":"# Load dictionary from a flat list\ndef load_dictionary(file_path):\n  entity_list = []\n  file = open(file_path , 'r', encoding = 'utf-8')\n  lines = file.readlines()\n  for line in lines:\n    entity_list.append(line.strip())\n  file.close()\n  return entity_list\n\nurl = 'https:\/\/drive.google.com\/uc?id=1ATez8emmXUD4ifHxx9nDf5kqOMGD_Ybp'\noutput = 'Covid-19_Drugs.txt'\ngdown.download(url, output, quiet=False)\n\nurl = 'https:\/\/drive.google.com\/uc?id=1-6ki6GTjTJx0ppPqaIDPA4_qHlne1x2-'\noutput = 'Covid-19_Synonyms.txt'\ngdown.download(url, output, quiet=False)\n\ncovid_names = load_dictionary('.\/Covid-19_Drugs.txt')\ncovid_drugs = load_dictionary('.\/Covid-19_Synonyms.txt')\n\ndisease_entity = Entity(keywords_list=covid_names, label=DISEASE_LABEL)\ndrug_entity = Entity(keywords_list=covid_drugs, label=DRUG_LABEL)","1b2a4c7f":"# Label tokens of recognised named entities using the BIO scheme\ndef get_tokens(ner_results, lookup_dict, labels_list):\n  ne_toks = []\n  for ner_result in ner_results:\n    ne_token = NEToken(ner_result['word'], ner_result['entity'])\n    ne_token.score = ner_result['score']\n    entity_label = ner_result['entity']\n    value = lookup_dict[entity_label]\n    if value < len(labels_list):\n      ne_token.label = labels_list[value]\n    else:  \n      ne_token.label = 'O'\n    ne_toks.append(ne_token)\n  return ne_toks\n\n# Function for correcting NE labels: making sure an entity's first token is tagged as 'B'\n# and propagating the label of an entity's first token to its subwords\ndef postprocess(tokens):\n  new_toks = []\n  previous_label = 'O'\n  for token in tokens:\n    if '[CLS]' not in token.word and '[SEP]' not in token.word:\n      if token.label.startswith('I-') and previous_label=='O':\n        token.label = 'B-' + token.label.split('-')[1]\n      if token.word.startswith('##'):\n        if '-' in previous_label:\n          token.label = 'I-' + previous_label.split('-')[1]\n        else:\n          token.label = previous_label\n      else:\n        previous_label = token.label\n      new_toks.append(token)\n  \n  #go backwards to ensure each tokens in a BI sequence have the same entity type\n  previous_entity_type = ''\n  for i in range(len(new_toks),0, -1):\n    current_tok = new_toks[i-1]\n    if current_tok.label.startswith('I-'):\n      if previous_entity_type != 'O' and previous_entity_type != '':\n        current_tok.label = 'I-' + previous_entity_type\n        previous_entity_type = current_tok.label.split('-')[1]\n      else:\n        previous_entity_type = current_tok.label.split('-')[1]\n    elif current_tok.label.startswith('B-') and previous_entity_type != 'O' and previous_entity_type != '':\n      current_tok.label = 'B-' + previous_entity_type\n      previous_entity_type = 'O'\n    elif current_tok.label == 'O':\n      previous_entity_type = 'O'\n\n  return new_toks\n\n# Function for consolidating consituent subwords (character n-grams) into a token\ndef consolidate(bert_tokenized):\n  result = []\n\n  for ne_token in bert_tokenized:\n      if ne_token.word.startswith('##'):\n          result[-1].word += ne_token.word[2:]\n      else:\n          result.append(ne_token)\n  return result\n\n# Function for merging entity labels coming from two different NE recognisers\ndef merge(toks1, toks2, to_debug):\n  merged_toks = []\n  if to_debug:\n    print(len(toks1))\n    print(len(toks2))\n  if len(toks1) == len(toks2):\n    for tok in toks1:\n      merged_toks.append(tok)\n    for i in range(0,len(toks2)):\n      if merged_toks[i].label == 'O':\n        merged_toks[i].label = toks2[i].label\n  else:\n    for tok in toks1:\n      merged_toks.append(tok)\n  if to_debug:\n    for m in merged_toks:\n      print(m.word, m.label)\n  return merged_toks\n\ndef to_tuples(tokens):\n  tuple_list = []\n  for token in tokens:\n    tuple_list.append((token.word, token.label))\n  return tuple_list\n\n# Function for obtaining character offsets of tokens of reconised named entities\ndef get_offsets(text: str, annotation: List[Tuple[str, str]], init_offset=0, return_last_match=False):\n    result = []\n    offset = 0\n    text = text.lower()\n\n    for token, label in annotation:\n        try:\n            start = text[offset:].index(token.lower()) + offset\n            end = start + len(token)\n\n        except ValueError:\n            printable_text = ''.join(c for c in text[offset:] if c in string.printable)\n            try:\n                matched_index = printable_text.index(token.lower())\n            except ValueError:\n                return []\n            start = matched_index + offset\n            matched_string = printable_text[matched_index:matched_index + len(token)]\n            char_iter = iter(text[start:])\n            diff = 0\n            for t in matched_string:\n                original_char = next(char_iter)\n                while t != original_char:\n                    original_char = next(char_iter)\n                    diff += 1\n\n            end = start + len(token) + diff\n            \n\n        if not label == \"O\":\n            tag, category = label.split('-')\n            if tag == \"B\":\n                result.append((start + init_offset, end + init_offset, category))\n            elif tag == \"I\":\n                old_start, *rest = result[-1]\n                result[-1] = (old_start, end + init_offset, category)\n        offset = end\n    if return_last_match:\n        return result, offset + init_offset\n    else:\n        return result\n\n\ndef get_offsets_from_sentences(text: str, annotation: List[List[Tuple[str, str]]]):\n    offset = 0\n    result = []\n    for sent in annotation:\n        sent_result, offset = get_offsets(text[offset:], sent, init_offset=offset, return_last_match=True)\n        result.extend(sent_result)\n    return result\n\n","f3f336e1":"# Recognise named entities in a given sentence using dictionaries\ndef apply_dictionary(nlp, text, entity_type, entity_label):\n  resulting_tokens = []\n  nlp.add_pipe(entity_type, first=False, name=entity_label, last=True)\n  doc = nlp(text)\n  previous_label = 'O'\n  for token in doc:\n    if token._.is_entity:\n      if previous_label == 'O':\n        resulting_tokens.append(NEToken(token.text, 'B-' + entity_label))\n        previous_label = 'B-' + entity_label\n      elif previous_label == 'B-' + entity_label or previous_label == 'I-' + entity_label:\n        resulting_tokens.append(NEToken(token.text, 'I-' + entity_label))\n        previous_label = 'I-' + entity_label\n    else:\n      resulting_tokens.append(NEToken(token.text, 'O'))\n      previous_label = 'O'\n  nlp.remove_pipe(entity_label)\n  return resulting_tokens\n\n# Recognise named entities in a given sentence using a deep learning-based model\ndef apply_ner_model(ner, text, label_dict, label_list, to_debug):\n  results = ner(text)\n  toks = get_tokens(results, label_dict, label_list)\n  if to_debug: \n    print('ORIG RESULTS')\n    for tok in toks:\n      print(tok)\n  clean_toks = postprocess(toks) \n  if to_debug:\n    print('AFTER POSTPROCESSING')\n    for clean_tok in clean_toks:\n      print(clean_tok)\n  consolidated_toks = consolidate(clean_toks)\n  if to_debug:\n    print('AFTER CONSOLIDATING')\n    for consolidated_tok in consolidated_toks:\n      print(consolidated_tok)\n  return consolidated_toks\n\n# Attach recognised entities to the sentence\ndef add_nes(sentence, ne_offsets):\n    for ne_offset in ne_offsets:\n      if ne_offset[1] - ne_offset[0] > 1:\n        ne = NE(ne_offset[0], ne_offset[1], ne_offset[2])\n        sentence.nes.add(ne)\n    return sentence","a8989e05":"# Load pre-trained BioWordVec embeddings\nfile_dict = {'pubmed_mesh_test.tar.gz':'https:\/\/drive.google.com\/uc?id=17KLR5bPKtlLlMnrdmcYTAMtbIAPnubmy',\n             'pubmed_mesh_test.trainables.syn1neg.npy.tar.gz':'https:\/\/drive.google.com\/uc?id=17NfQzGJACGLD8zXpPTyaXrAYV3qlj0wl', \n             'pubmed_mesh_test.trainables.vectors_ngrams_lockf.npy.tar.gz':'https:\/\/drive.google.com\/uc?id=1y2XgHAwm922oun5Vpojojwr-4rnEqb76',\n             'pubmed_mesh_test.trainables.vectors_vocab_lockf.npy.tar.gz':'https:\/\/drive.google.com\/uc?id=10Td9U8-R_b2QxSDQTGp5my48eqMZ6Fqz',\n             'pubmed_mesh_test.wv.vectors.npy.tar.gz':'https:\/\/drive.google.com\/uc?id=1Oc5-QT9qRz_SnNDGRFJLEfR7WqheSevf',\n             'pubmed_mesh_test.wv.vectors_ngrams.npy.tar.gz':'https:\/\/drive.google.com\/uc?id=1fncdQHoxMSMzfi1tVlEvvu57TDfZmoxa',\n             'pubmed_mesh_test.wv.vectors_vocab.npy.tar.gz':'https:\/\/drive.google.com\/uc?id=1EPSOZjtkQXLtqLc953ZNYlRUTAsJaD1t'}\n\nfor file in file_dict:\n    url = file_dict[file]\n    output = file\n    gdown.download(url, output, quiet=False)\n\n    my_tar = tarfile.open(file)\n    my_tar.extractall('.\/title_abstract\/') \n    my_tar.close()\n    os.remove(file)\n\nbiowordvec_model = Word2Vec.load('.\/title_abstract\/pubmed_mesh_test')","d5e0d8cd":"# Function for transforming terms from Medical Subject Headings (MeSH) into a more natural form\n# For example: \"Lung Inflammation, Experimental\" will be transformed into \"Experimental Lung Inflammation\" as the latter is more likely to be used in papers.\ndef transform(name):\n    result = name\n    if result.count(', ') > 0:\n        tokens = result.split(', ')\n        result = ''\n        for i in range(len(tokens)-1,-1,-1):\n            result = result + tokens[i] + ' '\n    return result.strip().lower()\n\n# Load MeSH descriptors (main headings)\nurl = 'https:\/\/drive.google.com\/uc?id=19PI1slvrMGvWoAv8iS7GJmevhsIxx5vM'\noutput = 'd2020.bin'\ngdown.download(url, output, quiet=False)\n\nmesh_file = open('.\/d2020.bin', 'r', encoding='utf-8')\nmesh_lines = mesh_file.readlines()\nmesh_file.close()\nos.remove('d2020.bin')\n\nuid_to_terms = dict()\nterm_to_uid = dict()\n\nrecord_ctr = 0\ncache = []\ntokens = []\n\nfor line in mesh_lines:\n    if line.startswith('*NEWRECORD'):\n        cache = []\n        record_ctr = record_ctr + 1\n    else:\n        tokens = line.strip().split(' = ')\n        \n    if line.startswith('MH ='):\n        cache.append(transform(tokens[1]))\n    elif line.startswith('PRINT ENTRY ='):\n        subtokens = tokens[1].split('|')\n        if transform(subtokens[0]) not in cache:\n            cache.append(transform(subtokens[0]))\n    elif line.startswith('ENTRY ='):\n        subtokens = tokens[1].split('|')\n        if transform(subtokens[0]) not in cache:\n            cache.append(transform(subtokens[0]))\n        \n    elif line.startswith('UI ='):\n        uid = tokens[1]\n        uid_to_terms[uid] = cache\n        for term in cache:\n            if term not in term_to_uid:\n                term_to_uid[term] = uid\n\n\n# Load MeSH supplementary concept record terms\nurl = 'https:\/\/drive.google.com\/uc?id=1Sk1nrpgDLOwu3V9217F3g3-itCauEcKo'\noutput = 'c2020.bin'\ngdown.download(url, output, quiet=False)\n\nmesh_file = open('.\/c2020.bin', 'r', encoding='utf-8')\nmesh_lines = mesh_file.readlines()\nmesh_file.close()\nos.remove('c2020.bin')\n\nuid_to_concepts = dict()\n\nrecord_ctr = 0\ncache = []\ntokens = []\n\nfor line in mesh_lines:\n    if line.startswith('*NEWRECORD'):\n        cache = []\n        record_ctr = record_ctr + 1\n    else:\n        tokens = line.strip().split(' = ')\n        \n    if line.startswith('NM ='):\n        cache.append(transform(tokens[1]))\n    elif line.startswith('N1 ='):\n        if transform(tokens[1]) not in cache:\n            cache.append(transform(tokens[1]))\n    elif line.startswith('RN ='):\n        if tokens[1] != '0' and tokens[1].startswith('EC')==False:\n            cache.append(tokens[1])\n    elif line.startswith('SY ='):\n        subtokens = tokens[1].split('|')\n        if transform(subtokens[0]) not in cache:\n            cache.append(transform(subtokens[0]))\n        \n    elif line.startswith('UI ='):\n        uid = tokens[1]\n        uid_to_concepts[uid] = cache\n        for concept in cache:\n            if concept not in term_to_uid:\n                term_to_uid[concept] = uid","10198932":"# Function for getting the text span given the begin and end offsets within a sentence\ndef get_ne_span(sentence_text, ne):\n    begin_offset = ne.begin\n    end_offset = ne.end\n    return (sentence_text[begin_offset:end_offset])\n\n# Function for retrieving the unique ID\n# First, exact string matching against MeSH entries is performed\n# If no match is found, the most similar term--according to trained word embeddings--is returned\n# However, a similar term is accepted only if similarity above or equal to 0.90\ndef get_closest_match(named_entity):\n  named_entity = named_entity.lower()\n  closest_match = 'NOT FOUND'\n  word_vectors = biowordvec_model.wv\n  if named_entity in term_to_uid:\n    closest_match = term_to_uid[named_entity] + ':' + named_entity\n  else:\n    try:\n      synonyms = word_vectors.most_similar(named_entity)\n      for synonym in synonyms:\n        if synonym[1] >= 0.90:\n          if synonym[0] in term_to_uid:\n            closest_match = term_to_uid[synonym[0]] + ':' + synonym[0]\n            break\n    except:\n      closest_match = 'NOT FOUND'\n  return closest_match","96d8256e":"NUM_DOCS = 500\nnlp = spacy.load('en_core_web_sm')\nnlp.remove_pipe('ner')\n\nall_docs_dict = dict()\noie_input_json_file = codecs.open('.\/oie_inputs.json', 'w+', encoding='utf-8')\n\nctr = 0\nfor p in papers:\n    doc_dict = dict() \n    doc_dict['title'] = []\n    doc_dict['body'] = []\n\n    body_index = 0\n    sentences = []\n    paper = papers[p]\n    # Sentence splitting:\n    for s in get_sentences(paper.title):\n      sentences.append(s.text)\n    \n    body_index = len(sentences)\n\n    for s in get_sentences(paper.abstract):\n      sentences.append(s.text)\n\n    for s in get_sentences(paper.get_whole_body_text(True)):\n      sentences.append(s.text)\n\n    # For each sentence, apply the NER models\n    sent_no = 0\n    for text in sentences:\n      #create Sentence \n      sentence = Sentence()\n      sentence.paper_id = paper.paper_id\n      sentence.sentence_text = text\n      if sent_no < body_index:\n        sentence.section = 'title'\n      else:\n        sentence.section = 'body'\n      sentence.nes = set()\n\n      to_debug = False\n      #Machine learning-based model\n      merged_toks = apply_ner_model(cd_ner, text, cd_dict, cd_labels, to_debug)\n      tuples = to_tuples(merged_toks)\n      ne_offsets = get_offsets(text, tuples, 0, False)\n      new_sentence = add_nes(sentence, ne_offsets)\n\n      #Dictionary-based matching\n      disease_dict_toks = apply_dictionary(nlp, text, disease_entity, DISEASE_LABEL)\n      drug_dict_toks = apply_dictionary(nlp, text, drug_entity, DRUG_LABEL)\n\n      disease_dict_tuples = to_tuples(disease_dict_toks)\n      drug_dict_tuples = to_tuples(drug_dict_toks)\n\n      disease_dict_offsets = get_offsets(text, disease_dict_tuples, 0, False)\n      drug_dict_offsets = get_offsets(text, drug_dict_tuples, 0, False)\n\n      new_sentence = add_nes(new_sentence, disease_dict_offsets)\n      new_sentence = add_nes(new_sentence, drug_dict_offsets)\n\n      #Saving annotations in JSON for OIE\n      sentence_object = dict()\n      sentence_object['text'] = new_sentence.sentence_text\n      ne_list = []\n      for ne in new_sentence.nes:\n        ne_span = get_ne_span(new_sentence.sentence_text, ne)\n        most_similar_span = get_closest_match(ne_span)\n        ne_list.append((ne.begin, ne.end, ne.ne_type, most_similar_span))\n      if len(ne_list) > 0:\n        sentence_object['named_entities'] = ne_list\n\n      if new_sentence.section in doc_dict:\n        section_object = doc_dict[new_sentence.section]\n        section_object.append(sentence_object)\n        doc_dict[new_sentence.section] = section_object\n      else:\n        section_object = [sentence_object]\n        doc_dict[new_sentence.section] = section_object\n      \n      sent_no = sent_no + 1\n    all_docs_dict[paper.paper_id] = doc_dict\n    ctr = ctr + 1\n    if ctr >= NUM_DOCS:\n        break\n\njson_string = json.dumps(all_docs_dict, ensure_ascii=False, indent = 4).encode('utf-8')\noie_input_json_file.write(json_string.decode())\noie_input_json_file.close()\n","1e85d27d":"def chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n\nfrom collections import defaultdict\nfrom tqdm.notebook import tqdm\n\nfrom tmdm.allennlp.oie import get_oie_provider\nfrom tmdm.util import load_file, save_file\n\nimport math\n\nfrom tmdm.pipe.tokenizer import IDTokenizer\nfrom scispacy.custom_tokenizer import combined_rule_tokenizer\nfrom tmdm.util import OneSentSentencizer\n\nimport en_core_sci_lg\nfrom tmdm.main import default_one_sent_getter\n\nnlp = en_core_sci_lg.load(disable=['ner','parser'])\nnlp.tokenizer = IDTokenizer(combined_rule_tokenizer(nlp), getter=default_one_sent_getter)\nnlp.add_pipe(OneSentSentencizer())\n\n# how many to annotate with OIE in parallel\nbatch_size = 100\n# only process those sentences with named entities in them\ndata_prep = [\n        (f\"{k}\/{i}\",  # id\n         l['text'])  # data\n        for k, v in all_docs_dict.items() for i, l in enumerate(v['body']) if 'named_entities' in l\n]\nprint(\"Number of sentences to process: \", len(data_prep))\n\noutputs = defaultdict(dict)\n\nprint(\"Tokenizing...\")\n# tokenize sentences to process\ndocs = list(tqdm(nlp.pipe(data_prep), total=len(data_prep)))\n# cuda=0: run on GPU. -1 for CPU\np = get_oie_provider(\"https:\/\/storage.googleapis.com\/allennlp-public-models\/openie-model.2020.03.26.tar.gz\", cuda=0)\n\nprint(\"Annotating...\")\nfor batch in tqdm(chunks(docs, batch_size), total=math.ceil(len(docs) \/ batch_size)):\n    annotations = p.annotate_batch(batch)\n    for doc, annotations in zip(batch, annotations):\n        doc._.oies = annotations","c1d11c30":"for doc in tqdm(docs):\n    idx, sent_id = doc._.id.rsplit(\"\/\", 1)\n    outputs[idx][int(sent_id)] = doc._._oies\n\n    \nner_file = 'oie_inputs.json'\n\ndata_ner = load_file(f'{ner_file}')\nfinal = {}\nfor id, article in tqdm(data_ner.items()):\n    for i, sent in enumerate(article['body']):\n        if \"named_entities\" in sent:\n            try:\n                oies = outputs[id][i]\n            except Exception as e:\n                raise e\n                oies = [[], []]\n            if oies != [[], []]:\n                predicates = (sum(1 for _, _, x in oies[0] if x.startswith(\"V\")))\n                if len(sent['named_entities']) >= 2:\n                    final[f'{id}\/{i}'] = {\n                        'ner': sent['named_entities'],\n                        'oie': oies,\n                        'text': sent['text']\n                    }\n","da297646":"import glob\nimport os\n\nimport sys\nfrom typing import List\n\nfrom tmdm.main import tmdm_one_sent_pipeline, change_getter\nfrom tmdm.model.oie import Argument\nfrom tmdm.util import load_file, save_file\n\nimport loguru\n\n\ndef is_predicate_between_arguments(predicate, arguments):\n    is_any_before = any(a[0].i < predicate[0].i for a in arguments)\n    is_any_after = any(a[0].i > predicate[0].i for a in arguments)\n    is_inbetween = is_any_before and is_any_after\n    return is_inbetween\n\n\ndef get_args_with_nes_and_el(oie):\n    args_with_ne = [x for x in oie.arguments if len([x for x in x._.get_nes() if \"NOT FOUND\" not in x.label_])]\n    return args_with_ne\n\ndef getter(input_data):\n    idx, d = input_data\n    return idx, d['text']\n\nnlp = en_core_sci_lg.load(disable=['ner', 'parser'])\nnlp.tokenizer = IDTokenizer(combined_rule_tokenizer(nlp), getter=default_one_sent_getter)\nnlp.add_pipe(OneSentSentencizer())\n\nchange_getter(nlp, getter)\nprint(\"Tokenizing...\")\ndocs = list(tqdm(nlp.pipe(list(final.items())), total=len(final)))\nprint(f\"Tokenized {len(docs)} docs.\")\nprint(\"Annotating...\")\n\nfailed_ner_align = 0\nfailed_oie_align = 0\nfiltered = []\n\nfor doc in tqdm(docs):\n    if doc:\n        id = doc._.id\n        d = final[id]\n        ne = [(s, e, f\"{l}|{el}\") for s, e, l, el in d['ner']]\n        try:\n            doc._.oies = d['oie']\n        except:\n            failed_oie_align += 1\n        try:\n            doc._.nes = ne\n        except:\n            failed_ner_align += 1\n    if doc._.nes and doc._.oies:\n        for oie in doc._.oies:\n            args_with_ne: List[Argument] = get_args_with_nes_and_el(oie)\n            if len(args_with_ne) > 1 and is_predicate_between_arguments(oie, args_with_ne):\n                res = {\n                    \"text\": str(doc),\n                    \"predicate\": str(oie)\n                }\n                for argument in args_with_ne:\n                    nes = [(str(ne), *str(ne.label_).rsplit(\"|\", 1)) for ne in argument._.get_nes() if\n                           \"NOT FOUND\" not in ne.label_]\n                    res[f'ARG-{argument.order}'] = {\n                        \"type\": argument.label_,\n                        \"text\": str(argument),\n                        \"ner\": [{\n                            \"text\": text,\n                            \"type\": ner_type,\n                            \"id\": el_id,\n                        } for text, ner_type, el_id in nes]\n                    }\n                filtered.append(res)\n\nprint(f\"{failed_ner_align} NER misaligned.\")\nprint(f\"{failed_oie_align} OIE misaligned.\")\nprint(f\"Have {len(filtered)} extractions after filtering.\")\n\nnew_d = []\nfor datum in filtered:\n         new_datum = {}\n         new_datum['text'] = datum['text']\n         new_datum['predicate'] = datum['predicate']\n         for i,(k, v) in enumerate(((k,v) for k,v in datum.items() if k.startswith(\"ARG\"))):\n             new_datum[f'ARG{i}'] = {}\n             new_datum[f'ARG{i}']['type'] = v['type']\n             new_datum[f'ARG{i}']['text'] = v['text']\n             new_datum[f'ARG{i}']['ner_text'] = '; '.join(f\"{n['type']}: {n['text']}\" for n in v['ner'])\n             new_datum[f'ARG{i}']['ner_id'] = '; '.join(n['id'] for n in v['ner'])\n             new_datum[f'ARG{i}']['ner_type'] = '; '.join(n['type'] for n in v['ner'])\n             new_datum[f\"ARG{i}\"]['ner_nested'] = v['ner']\n             new_d.append(new_datum)\n\n\ndef filter_identical(data):\n        ctr = 0\n        new_data = []\n        for datum in data:\n            nes = [v['ner_text'] for k,v in datum.items() if k.startswith('ARG')]\n            nes = [n for n in nes if n]\n            if  not len(set(nes)) > 1:\n                ctr += 1\n            else:\n                new_data.append(datum)\n        return(new_data)\n    \n\nnew_data = filter_identical(new_d)\nsave_file(new_data, f\"oie_outputs.json\")","5d768c59":"nlp = spacy.load('en_core_web_sm')\nfrom nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer(language='english')\n\n\n# Predicates extracted by OpenIE are normalised\n# First, we use lemmatisation to obtain the lemmatised form of the predicate; if the predicate is a phrase, the root is used\n# The stem (obtained using Snowball Stemming) of the lemmatised form is then taken as the normalised predicate\ndef normalise_predicate(original_predicate):\n  normalised = ''\n  preprocessed = nlp(original_predicate)\n  for w in preprocessed:\n    if w.dep_ == 'ROOT':\n      lemma = w.lemma_\n      stem = stemmer.stem(lemma)\n      normalised = stem\n      break\n  return normalised","028f4892":"pair_frequency_dict = dict()\npair_predicate_dict = dict()\nall_predicates = []\n\nfor item in new_data:\n    predicate = normalise_predicate(item['predicate'])\n    arg0 = item['ARG0']\n    arg1 = item['ARG1']\n      \n    arg0_types = arg0['ner_type'].split(';')\n    arg1_types = arg1['ner_type'].split(';')\n\n    arg0_ids = arg0['ner_id'].split(';')\n    arg1_ids = arg1['ner_id'].split(';')\n\n    for i in range(0,len(arg0_types)):\n        for j in range(0,len(arg1_types)):\n            ne_types = [arg0_types[i].strip(), arg1_types[j].strip()]\n            if DISEASE_LABEL in ne_types and DRUG_LABEL in ne_types:\n                arg0_id = arg0_ids[i].split(':')[0].strip()\n                arg1_id = arg1_ids[j].split(':')[0].strip()\n                pair_str = arg0_id + '-' + arg1_id\n                if arg0_id != arg1_id:\n                    if predicate not in all_predicates:\n                        all_predicates.append(predicate)\n                    if pair_str in pair_predicate_dict:\n                        pair_predicates = pair_predicate_dict[pair_str]\n                        if predicate not in pair_predicates:\n                            pair_predicates[predicate] = 1\n                            pair_predicate_dict[pair_str] = pair_predicates\n                        else:\n                            predicate_count = pair_predicates[predicate]\n                            predicate_count = predicate_count + 1\n                            pair_predicates[predicate] = predicate_count\n                            pair_predicate_dict[pair_str] = pair_predicates\n                    else:\n                        pair_predicates = dict()\n                        pair_predicates[predicate] = 1\n                        pair_predicate_dict[pair_str] = pair_predicates\n\n                    if pair_str in pair_frequency_dict:\n                        pair_count = pair_frequency_dict[pair_str]\n                        pair_count = pair_count + 1\n                        pair_frequency_dict[pair_str] = pair_count\n                    else:\n                        pair_frequency_dict[pair_str] = 1\n\n\nsorted_freq = sorted(pair_frequency_dict.items(), key=lambda x: x[1], reverse=True)\n\nsorted_pair_predicate_dict = dict()\nfor pair in pair_predicate_dict:\n    pair_predicates = pair_predicate_dict[pair]\n    sorted_predicates = sorted(pair_predicates.items(), key=lambda x: x[1], reverse=True)\n    sorted_pair_predicate_dict[pair] = sorted_predicates","63925506":"url = 'https:\/\/drive.google.com\/uc?id=1YsyvBkAcz9b5YyWQxTNOB3HTwaxlG1J5'\noutput = '.\/Treatment_Predicates_Clustering.txt'\ngdown.download(url, output, quiet=False)\ntreatment_predicates = load_dictionary(output)\n\nurl = 'https:\/\/drive.google.com\/uc?id=19umZ5Uy__4ozafP71xlrbIBQBMcnN13D'\noutput = '.\/mesh_to_dosage.json'\ngdown.download(url, output, quiet=False)\ndosage_file = open(output, 'r')\ndosage_dict = json.load(dosage_file)\ndosage_file.close()\n\n","0665b4f9":"# Function for getting the preferred name of a MeSH entry, given a MeSH identifier\ndef get_name(mesh_id):\n    preferred_name = ''\n    if mesh_id in uid_to_terms:\n        preferred_name = uid_to_terms[mesh_id][0]\n    elif mesh_id in uid_to_concepts:\n        preferred_name = uid_to_concepts[mesh_id][0]\n    return preferred_name\n\n\nchemical_dict = dict()\ncondition_dict = dict()\n\nchemical_json_array = []\ncondition_json_array = []\ntreatment_json_array = []\n\nMIN_PAIR_FREQ = 2\nMIN_PRED_FREQ = 2\n\nfor freq in sorted_freq:\n    if freq[1] < MIN_PAIR_FREQ:\n        break\n    mesh_ids = freq[0].split('-')\n    chem_id = ''\n    disease_id = ''\n    chem_found = False\n    disease_found = False\n    for mesh_id in mesh_ids:\n        if mesh_id in dosage_dict:\n            if dosage_dict[mesh_id] != 'Not applicable' and dosage_dict[mesh_id]!= '':\n                chem_id = mesh_id\n                chem_found = True\n        else:\n            disease_id = mesh_id\n            disease_found = True\n    \n    if chem_found == True and disease_found == True:\n        preds = sorted_pair_predicate_dict[freq[0]]\n        for pred in preds:\n            if pred[1] < MIN_PRED_FREQ:\n                break\n            if pred[0] in treatment_predicates:\n                if chem_id not in chemical_dict:\n                    chem_name = get_name(chem_id)\n                    chemical_dict[chem_id] = chem_name\n                    chem_object = dict()\n                    chem_object['mesh_id'] = chem_id\n                    chem_object['chemical_name'] = chem_name\n                    dosage_form = dosage_dict[chem_id]\n                    chem_object['dosage_form'] = dosage_form\n                    chemical_json_array.append(chem_object)\n                if disease_id not in condition_dict:\n                    disease_name = get_name(disease_id)\n                    condition_dict[disease_id] = disease_name\n                    disease_object = dict()\n                    disease_object['mesh_id'] = disease_id\n                    disease_object['condition_name'] = disease_name\n                    condition_json_array.append(disease_object)\n\n                treatment_object = dict()\n                treatment_object['medication_id'] = chem_id\n                treatment_object['disorder_id'] = disease_id\n                treatment_json_array.append(treatment_object)\n\n#write the json files to disk\nwith open('chemicals.json', 'w') as outfile: \n    json.dump(chemical_json_array, outfile) \n\nwith open('conditions.json', 'w') as outfile: \n    json.dump(condition_json_array, outfile) \n\nwith open('treatments.json', 'w') as outfile: \n    json.dump(treatment_json_array, outfile) ","36ab0bd5":"from grakn.client import GraknClient\n\nwith GraknClient(uri=\"167.99.203.112:48555\") as client:\n    with client.session(keyspace = \"cord\") as session:\n        with session.transaction().write() as transaction:\n            #delete existing treatment relations\n            delete_query = 'match $c isa chemical; $d isa condition; $t (medication: $c, disorder: $d) isa treatment; delete $t;' \n            transaction.query(delete_query)\n            \n            #delete existing mesh-id attribute values\n            delete_query = 'match $c isa chemical, has mesh-id $mi; delete $mi;' \n            transaction.query(delete_query)\n            \n            #delete existing chemical-name attribute values \n            delete_query = 'match $c isa chemical, has chemical-name $cn; delete $cn;' \n            transaction.query(delete_query)\n            \n            #delete existing dosage-from attribute values\n            delete_query = 'match $c isa chemical, has dosage-form $df; delete $df;' \n            transaction.query(delete_query)\n            \n            #delete existing mesh-id attribute values\n            delete_query = 'match $c isa condition, has mesh-id $mi; delete $mi;' \n            transaction.query(delete_query)\n            \n            #delete existing condition-name attribute values \n            delete_query = 'match $c isa condition, has condition-name $cn; delete $cn;' \n            transaction.query(delete_query)\n            \n            #delete existing chemicals\n            delete_query = 'match $c isa chemical; delete $c;' \n            transaction.query(delete_query)\n            \n            #delete existing conditions\n            delete_query = 'match $c isa condition; delete $c;' \n            transaction.query(delete_query)\n\n            transaction.commit()\n            \n            ","f0f078d9":"import ijson\n\ndef condition_template(condition):\n    graql_insert_query = 'insert $condition isa condition, has condition-name \"' + condition[\"condition_name\"] + '\"'\n    graql_insert_query += ', has mesh-id \"' + condition[\"mesh_id\"] + '\"'\n    graql_insert_query += \";\"\n    return graql_insert_query\n\ndef chemical_template(chemical):\n    graql_insert_query = 'insert $chemical isa chemical, has chemical-name \"' + chemical[\"chemical_name\"] + '\"'\n    graql_insert_query += ', has mesh-id \"' + chemical[\"mesh_id\"] + '\"'\n    graql_insert_query += ', has dosage-form \"' + chemical[\"dosage_form\"] + '\"'\n    graql_insert_query += \";\"\n    return graql_insert_query\n\ndef treatment_template(treatment):\n    graql_insert_query = 'match $disorder isa condition, has mesh-id \"' + treatment[\"disorder_id\"] + '\";'\n    graql_insert_query += ' $medication isa chemical, has mesh-id \"' + treatment[\"medication_id\"] + '\";'    \n    graql_insert_query += \" insert (disorder: $disorder , medication: $medication) isa treatment;\"\n    return graql_insert_query\n\ndef parse_data_to_dictionaries(input):\n    items = []\n    with open(input[\"data_path\"] + \".json\") as data:\n        for item in ijson.items(data, \"item\"):\n            items.append(item)\n    return items\n\ndef load_data_into_grakn(input, session):\n    items = parse_data_to_dictionaries(input)\n\n    for item in items:\n        with session.transaction().write() as transaction:\n            graql_insert_query = input[\"template\"](item)\n            print(\"Executing Graql Query: \" + graql_insert_query)\n            transaction.query(graql_insert_query)\n            transaction.commit()\n\n    print(\"\\nInserted \" + str(len(items)) + \" items from [ \" + input[\"data_path\"] + \"] into Grakn.\\n\")\n\n\ndef build_covid19_graph(inputs):\n    with GraknClient(uri=\"167.99.203.112:48555\") as client:\n        with client.session(keyspace = \"cord\") as session:\n            for input in inputs:\n                print(\"Loading from [\" + input[\"data_path\"] + \"] into Grakn ...\")\n                load_data_into_grakn(input, session)\n\ninputs = [\n    {\n        \"data_path\": \"conditions\",\n        \"template\": condition_template\n    },\n    {\n        \"data_path\": \"chemicals\",\n        \"template\": chemical_template\n    },\n    {\n        \"data_path\": \"treatments\",\n        \"template\": treatment_template\n    }    \n]\n\nbuild_covid19_graph(inputs)","431ca83c":"# Function that can be called to query the knowledge graph\ndef query_graph(drug='', disease=''):\n    answers = []\n    chem_name = '$chemname'\n    chem_name_query = ''\n    if drug != '':\n        chem_name_query = '  $chemname \"' + drug.lower() + '\";'\n        \n    cond_name = '$condname'\n    cond_name_query = ''\n    if disease != '':\n        cond_name_query = '  $condname \"' + disease.lower() + '\";'\n    \n    with GraknClient(uri=\"167.99.203.112:48555\") as client:\n        with client.session(keyspace = \"cord\") as session:\n            with session.transaction().read() as transaction:\n                query = [\n                    'match ',\n                    '  $chem isa chemical, has mesh-id $chemid, has chemical-name $chemname, has dosage-form $doseform;',\n                    '  $dose-form contains \"Oral\";',\n                    chem_name_query ,\n                    '  $cond isa condition, has mesh-id $condid, has condition-name $condname;',\n                    cond_name_query ,\n                    '  (medication: $chem, disorder: $cond) isa treatment;',\n                    'get $chemid, $chemname, $doseform, $condid, $condname;'\n                ]\n\n                print(\"\\nQuery:\\n\", \"\\n\".join(query))\n                query = \"\".join(query)\n                iterator = transaction.query(query)\n\n                for answer in iterator:\n                    chemname = answer.get(\"chemname\")\n                    chemid = answer.get(\"chemid\")\n                    doseform = answer.get(\"doseform\")\n                    condname = answer.get(\"condname\")\n                    condid = answer.get(\"condid\")\n                    answers.append((chemid.value(), chemname.value(), doseform.value(), condid.value(), condname.value()))\n\n    return answers\n","03bf151a":"# This will return the entire knowledge graph\nanswers = query_graph()\nprint(answers)\n\n# This will return diseases treated by the given drug\n#answers = query_graph(drug='oxygen')\n\n# This will return drugs that have been investigated to treat the given disease\n#answers = query_graph(disease='cough')","3e12c548":"import networkx as nx\n\n# Function for transforming the results of querying the knowledge graph above, into a graph\ndef to_graph(answers):\n    graph = nx.Graph()\n    for answer in answers:\n        graph.add_node(answer[0], name=answer[1], type='oral_drug')\n        graph.add_node(answer[3], name=answer[4], type='condition')\n        graph.add_edge(answer[0], answer[3])\n    return graph\n","8a8238c0":"import os\nimport shutil\nimport tempfile\nfrom pyvis.network import Network\nfrom typing import List\n\nclass PyVisPrinter:\n    \"\"\"Class to visualise a (serialized) dataset entry.\"\"\"\n\n    def __init__(self, path=None):\n        self.path = tempfile.mkdtemp(prefix='vis-', dir='.') or path\n        \n    def clean(self):\n        shutil.rmtree(self.path)\n\n    def print_graph(self, graph: nx.Graph, filename):\n\n        vis = Network(bgcolor=\"#222222\",\n                      width='100%',\n                      font_color=\"white\", notebook=False)\n        \n        for idx, (node, data) in enumerate(graph.nodes(data=True)):\n            vis.add_node(\n                node,\n                title=data['name'],\n                label=data['name'],\n                color='yellow' if data['type'] == 'condition' else 'green' if data['type'] == 'oral_drug' else 'blue'\n            )\n\n        for i, (source, target) in enumerate(graph.edges()):\n            if source in vis.get_nodes() and target in vis.get_nodes():\n                vis.add_edge(source, target)\n            else:\n                print(\"source or target not in graph!\")\n\n        name = os.path.join(self.path, filename)\n        return vis\n    \n\ngraph = to_graph(answers)\np = PyVisPrinter()\nv = p.print_graph(graph, 'cord_graph.html')\nv.show('cord_graph.html')","551523c8":"The cell below is a class for generating the visualisation of the results of querying the knowledge graph.\nIt will generate an HTML file that contains interactive visualisation.","8089cd86":"Below, we prepare the knowledge graph by emptying any previouly added entries","d861c1d8":"## Helper function for normalising predicates","15aced90":"# Open Information Extraction (Open IE)\nHere, we apply an off the shelf open information extraction tool on those sentences that contain at least two recognised named entities, in order to model the relations between them.\n\nEach processed sentence yields at least one `n`- tuple of the form\n`(predicate, arg0, ..., argM)` where `argN` are syntactic arguments of a predicate decected in the sentence.","57faa364":"Our knowledge graph can be explored in three ways:\n* by specifying a disease, answering the question \"What has been investigated as a treatment for disease C?\"\n* by specifying a drug, answering the question \"What does drug D treat?\"\n* by not specifying either a drug nor a disease, which would retrieve the entire knowledge graph","d261ab72":"# Classes that represent text processing units","5ad098d1":"# Named Entity Recognition (NER) preparation\nOur approach to named entity recognition is based on a combination of both deep learning-based and dictionary-based methods.\n","b05b8bda":"## Preparing the graph data\nThe cell below will generate JSON files that will be loaded onto our Grakn knowledge graph","76b2149b":"# Visualisation","e4d5bb7b":"## Dictionary-based entity linking resources\nAll of the main headings\/descriptors and concepts in MeSH are included in our vocabulary.","34ab2c72":"# Knowledge Graph Construction\nTo address the task of curating information on which oral medications could potentially work for patients of Covid-19, we seek to construct a knowledge graph containing information on treatment relations, i.e., relationships between drugs (which are available in oral form) and diseases. \n","918b83ac":"## Helper functions for entity linking","6f58b824":"# Introduction\nThis notebook is an implementation of a text mining pipeline that aims to address the question \"What has been published about medical care?\"\n\nSpecifically, we seek to analyse the CORD data set to automatically extract knowledge on \"Oral medications that might potentially work\".\n\nOur pipeline consists of the following components:\n* Named Entity Recognition (NER)\n* Entity Linking\n* Open Information Extraction (OpenIE)\n* Knowledge Graph Construction\n* Visualisation","0723c268":"Below, we prepare two resources:\n* A dictionary of predicates resulting from unsupervised clustering, that are associated with the treatment relationship\n* A JSON file containing a mapping between drugs (identified by their MeSH identifiers) and their dosage forms","610b5b63":"## Deep learning-based entity linking resources\nWe made use of [BioWordVec](https:\/\/www.nature.com\/articles\/s41597-019-0055-0) to train our own biomedical word embeddings on the titles and abstracts of all papers in Version 12 of the CORD data set.\n\nThese embeddings allow us to link a named entity to a MeSH term, even if the named entity does not have any exact string match.","dc6a68d4":"## Knowledge Graph Querying","2250b422":"We then build the knowledge graph by loading the JSON files generated above.","ed87f4c7":"## Deep-learning based NER resources\nThe deep learning-based method is underpinned by a sequence labelling model following a [transformer architecture](https:\/\/github.com\/huggingface\/transformers), trained on the [BioCreative V CDR Corpus](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4860626\/) which contains gold standard named entity labels for drugs and diseases.","248f0037":"# Document set loading and pre-processing\nSince it is not ideal to run a process for very long using a notebook, we use only the CORD abstracts for now.","926c9912":"## Dictionary-based NER resources\nWe manually compiled two small dictionaries: one containing names denoting Covid-19, and the other containing names of drugs which have recently mentioned in relation to Covid-19. These resources are to account for the fact that the corpus that our deep learning-based model was trained on, does not contain any examples of drug and disease names that have emerged only recently.","39685fd6":"## Demonstration of querying","2c8f3c92":"# Processing document set with NER and entity linking\nWe process only a few hundreds of documents to keep processing time within less than an hour. This number is currently set to 500. Change the value of NUM_DOCS below to process more documents.\n\nThe cell below will produce a JSON file named oie_inputs.json with the results of both NER and entity linking.","37cb22f6":"In the cell below, we process the results of Open IE to take a subject-object pair only if one argument is a chemical\/drug and the other is a disease. We sort the pairs according to their co-occurrence frequency over the document set. \n\nFurthermore, the predicates extracted for these subject-object pairs are stored and ordered according to frequency.","058259e7":"# Installation of dependencies","167b3574":"We further filter the OpenIE extractions to reduce the noise from low-quality extractions.\nWe only keep an extraction, if:\n\n* it has at least two recognized named entities in two different arguments of its predicate\n* the named entities are distinct from each other\n* the named entities are linked against the Medical Subject Headings (MeSH) vocabulary\n* the predicate is in between those two arguments in the corresponding sentence text\n\nThe cell below will generate a JSON file named oie_outputs.json containing the OpenIE results.","2376ddea":"## Helper functions for post-processing results from NER","473365ef":"# Preparation for entity linking\nWe seek to link each automatically recognised named entity to a unique identifier in a vocabulary, in our case, Medical Subject Headings (MeSH).\n\nOur approach to entity linking is based on a combination of both deep learning-based and dictionary-based methods.\n","b3f3d399":"# Import of required classes","ad745a1c":"## Functions that apply NER"}}