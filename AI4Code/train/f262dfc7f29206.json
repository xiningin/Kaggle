{"cell_type":{"c87e2fd7":"code","3c835d8b":"code","84c2b74c":"code","92f7ebec":"code","030aafe2":"code","76dbb88e":"code","aa4e4e0d":"code","3b88d46d":"code","387d91a4":"code","ec063cfe":"code","2c873b3d":"code","26f1a5c2":"code","089d0ec6":"code","c90e9ed3":"code","aba68563":"code","242e7424":"markdown","9184ae8e":"markdown","611d4eb8":"markdown","fc8d220e":"markdown","f400f022":"markdown","df9fb848":"markdown"},"source":{"c87e2fd7":"!pip install --upgrade transformers\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport transformers\nprint(\"TF version: \", tf.__version__)\nprint(\"Hub version: \", hub.__version__)\nprint(\"Transformers version: \", transformers.__version__)","3c835d8b":"import numpy as np\nimport pandas as pd\n\n# Transformers models\n# You can add to these how ever you like\n\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom transformers import BertTokenizer, TFBertModel\nfrom transformers import XLNetTokenizer, TFXLNetModel\n\nfrom tensorflow.keras.optimizers import Adam\nimport os\n# For text preprocessing\nimport string\nimport regex as re\n\n#from sklearn.model_selection import StratifiedKFold","84c2b74c":"test = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip', sep = '\\t')\ntrain = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip', sep = '\\t')\ntest_ids = test.PhraseId","92f7ebec":"train.head(3)","030aafe2":"# minimal cleaning because we're using a deep learning model that can\n# learn from different variations and shapes of words\ndef clean(t, punc = True, lower = True):\n\n    if lower is True:\n        t = t.lower()\n    # punctuation removal\n    if punc is True:\n        t = t.translate(str.maketrans('', '', string.punctuation))\n        \n    # removing extra space and letters\n    t = re.sub(\"\\s+\", ' ', t)\n    t = re.sub(\"\\b\\w\\b\", '', t)\n    return t\n# delete the unwanted columns\ndef delete(df_list, columns):\n    for df in df_list:\n        df.drop(columns = columns, inplace = True)\n\ntrain['cleaned_text'] = train.Phrase.apply(lambda x: clean(x, punc = True, lower = True))\ntest['cleaned_text'] = test.Phrase.apply(lambda x: clean(x, punc = True, lower = True))\n\ndelete([train, test], ['Phrase', 'PhraseId', 'SentenceId'])\n","76dbb88e":"\nmodels = {'roberta-large':(RobertaTokenizer,'roberta-large',TFRobertaModel),\n          #'roberta-base':(RobertaTokenizer,'roberta-base',TFRobertaModel),\n          #'bert-large':(BertTokenizer, 'bert-large-uncased', TFBertModel),\n          #'bert-base':(BertTokenizer, 'bert-base-uncased', TFBertModel),\n          #'xlnet':(XLNetTokenizer, 'xlnet-large-cased', TFXLNetModel)\n         }\n\ntokenizer, model_type, model_name = models['roberta-large']","aa4e4e0d":"def make_inputs(tokenizer, model_type, serie, max_len= 70):\n\n    tokenizer = tokenizer.from_pretrained(model_type, lowercase=True )\n    tokenized_data = [tokenizer.encode_plus(text, max_length=max_len, \n                                            padding='max_length', \n                                            add_special_tokens=True,\n                                            truncation = True) for text in serie]\n\n    \n    input_ids = np.array([text['input_ids'] for text in tokenized_data])\n    attention_mask = np.array([text['attention_mask'] for text in tokenized_data])\n    \n    return input_ids, attention_mask\n\ninput_ids_train, attention_mask_train = make_inputs(tokenizer, model_type, train.cleaned_text)","3b88d46d":"##### TPU or no TPU\ndef init_model(model_name, model_type, num_labels, Tpu = 'on', max_len = 70):\n# ------------------------------------------------ with TPU --------------------------------------------------------------#\n    if Tpu == 'on':\n        # a few lines of code to get our tpu started and our data distributed on it\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(resolver)\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        # print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n        with strategy.scope():\n\n            model_ = model_name.from_pretrained(model_type)\n            # inputs\n            input_ids = tf.keras.Input(shape = (max_len, ), dtype = 'int32')\n            attention_masks = tf.keras.Input(shape = (max_len,), dtype = 'int32')\n            \n            outputs = model_([input_ids, attention_masks])\n\n            if 'xlnet' in model_type:\n                # cls is the last token in xlnet tokenization\n                outputs = outputs[0]\n                cls_output = tf.squeeze(outputs[:, -1:, :], axis=1)\n            else:\n                cls_output = outputs[1]\n\n            final_output = tf.keras.layers.Dense(num_labels, activation = 'softmax')(cls_output)\n            model = tf.keras.Model(inputs = [input_ids, attention_masks], outputs = final_output)\n            model.compile(optimizer = Adam(lr = 1e-5), loss = 'categorical_crossentropy',\n                        metrics = ['accuracy'], )\n# ------------------------------------------------ without TPU --------------------------------------------------------------#\n    else:\n        model_ = model_name.from_pretrained(model_type)\n        # inputs\n        input_ids = tf.keras.Input(shape = (max_len, ), dtype = 'int32')\n        attention_masks = tf.keras.Input(shape = (max_len,), dtype = 'int32')\n        \n        outputs = model_([input_ids, attention_masks])\n\n        if 'xlnet' in model_type:\n            # cls is the last token in xlnet tokenization\n            outputs = outputs[0]\n            cls_output = tf.squeeze(outputs[:, -1:, :], axis=1)\n        else:\n            cls_output = outputs[1]\n\n        \n        final_output = tf.keras.layers.Dense(num_labels, activation = 'softmax')(cls_output)\n\n        model = tf.keras.Model(inputs = [input_ids, attention_masks], outputs = final_output)\n\n        model.compile(optimizer = Adam(lr = 1e-5), loss = 'categorical_crossentropy',\n                    metrics = ['accuracy'])\n    return model\nmodel = init_model(model_name, model_type, num_labels = 5, Tpu = 'on', max_len = 70)","387d91a4":"model.summary()","ec063cfe":"train_y = tf.keras.utils.to_categorical(train.Sentiment, num_classes=5)\ndel train","2c873b3d":"model.fit([input_ids_train, attention_mask_train], train_y,\n          validation_split=0.2, epochs = 3, batch_size = 16,\n          shuffle = True, verbose=2)","26f1a5c2":"input_ids_test, attention_mask_test = make_inputs(tokenizer, model_type, test.cleaned_text, max_len= 70)\n\ny_pred = model.predict([input_ids_test, attention_mask_test])\npred = np.argmax(y_pred, axis = 1)\npred\nsub = pd.DataFrame(np.c_[test_ids, pred], columns = ['PhraseId', 'Sentiment'])\nsub.head()","089d0ec6":"sub.to_csv('RoBERTa_3epochs_16bs_1e-5lr.csv', index = False)","c90e9ed3":"!git clone https:\/\/github.com\/WittmannF\/LRFinder.git\nfrom LRFinder.keras_callback import LRFinder","aba68563":"lr_finder = LRFinder(min_lr=1e-7, max_lr=1e-4)\nmodel.fit([input_ids_train, attention_mask_train], train_y,\n          validation_split=0.2, epochs = 1, batch_size = 16,\n          shuffle = True, callbacks=[lr_finder], verbose = 2)\n","242e7424":"Because TPU can help us alot and process the text 20 times faster I've writtern the code in a way that you can choose whether or not you want to work with TPU. On Kaggle and Colab there is a 12 hour limitation for using TPUs so you can switch between these modes by setting the value to on or off. ","9184ae8e":"Transformers models, no matter which one you choose, need a specific type of input to be able to process the data. The overall shape of the input is usually very similar, so it's not so hard to comprehend either.","611d4eb8":"We import the data and try to clean it as much as possible, withh minimum loss of meaning. We're using a deep learning model we don't need to clean a lot of words out, because the deep learning model can find out about each word itself even it's never seen it before in its vocabulary (I've included all the explaination between the code, but make sure to ask me if anything wasn't clear to you.)","fc8d220e":"# **Text Classification with TensorFlow and \ud83e\udd17 Transformers**\n\n**Goal:** In this notebook I try to summerize all different approaches and come up with very simple generalized functions that can help you get state of the art results with the TensorFlow version of Transformers models. Although I'll be using the large RoBERTa model, I've provided the code and structer that you will need if you choose to use other models as well. What we want to acomplish here is a very generalized approach that can work perfectly with all different models and different text preprocessing required by different models. I've also the written the code in Colab and you can use whichever you prefer. I also used this framework to come up with [sentiment analysis on the first and second presidential debates of United States](https:\/\/www.kaggle.com\/mitramir5\/complete-visualization-and-analysis-of-2020debates), so if you're interested make sure to check it out!Also here is the [Colab notebook](https:\/\/colab.research.google.com\/drive\/1XgJdIrSRB6_N7k3dIkXt0eUVUcG0yGNw?usp=sharing) which is a bit sloppier replica of this kernel. Work with which ever you find more understandalbe.\n\nAs there are [alot of different models](https:\/\/huggingface.co\/transformers\/pretrained_models.html#pretrained-models) to choose from, it's good to take a look at the table below for better a intuition and understanding of some of the most prevalent ones([reference of photo](https:\/\/towardsdatascience.com\/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8)):\n\n![download.png](attachment:download.png)\n\n\ud83e\udd17Transformers brings all these models together and makes it very easy to use each with only a few lines of code. In fact they even provide us with cool tools like [pipelines](https:\/\/huggingface.co\/transformers\/main_classes\/pipelines.html) or [live demo](https:\/\/huggingface.co\/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you) that we can classify our text without any training or long periods of coding. But as you can geuss these simple and ready to use models have their weaknesses. For example, you can't classify the text with them with the number of labels you want because they've been pretrained on a text with specific labels. Also not all models used by them are as strong and accurate as we want them to be(for example the default model for sentiment analysis is uncased distillbert which is not the best model we can find out there). With all these in mind, we want to train \ud83e\udd17Transformers models on our own data with the models that we prefer.\n\n\nThanks to this notebook on [fastai and Transformers](https:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta) that got me thinking of creating one for TensorFlow.\n\nMake sure all your packages are up-to-date and then get going!","f400f022":"# **Optional**\n\nYou can go on and experiment with different things as learning rate, batchsize, etc. Here as an example I wanted to find the best learning rate and I could find a great package that helped me with this task by using the fit property and fitting the model to the data after using several different lrs. The actuall phylosiphy and approach is explained in [this paper](https:\/\/arxiv.org\/abs\/1506.01186). Just remember to do it first hand before training so you use the best lr from the beggining.","df9fb848":"<h1 style=\"border:2px solid blue; text-align: center\">Thanks for reading! Don't forget to upvote!<\/h1>"}}