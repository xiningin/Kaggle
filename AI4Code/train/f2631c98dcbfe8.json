{"cell_type":{"96a923eb":"code","5b4d242f":"code","bd672591":"code","f39869a6":"code","f71cc6e3":"code","93959e2a":"code","fd658992":"code","7729b06b":"code","17013bff":"code","bc76a5de":"code","83c70b10":"code","2b74cd31":"code","2888411d":"code","22f793bd":"code","2e33ccaa":"code","b3990ea6":"code","d972fb10":"code","6e0470de":"code","2858ab53":"code","548e5272":"code","98f82ace":"code","7b62317d":"markdown","30195b4b":"markdown","08c4d91f":"markdown","2b97907f":"markdown","b4cf6923":"markdown","82d27be2":"markdown","c806ae60":"markdown","e4f7261e":"markdown","bdaa8f72":"markdown","f99c71c4":"markdown","5ee681f5":"markdown","6bc12764":"markdown","a7098860":"markdown","1711d45f":"markdown","91d608c2":"markdown","355de5b5":"markdown","9a124daa":"markdown","8a6ee6df":"markdown","fdc6f05d":"markdown","bb176f3c":"markdown","5650427e":"markdown","27e0cd33":"markdown","9c8e384e":"markdown"},"source":{"96a923eb":"import pandas as pd\nimport numpy as mp\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv', index_col='PassengerId')\ntest = pd.read_csv('..\/input\/titanic\/test.csv', index_col='PassengerId')\n\ntrain.head()","5b4d242f":"# 'male' to 0\n# 'female' to 1\ntrain['Sex'] = train['Sex'].map({'male':0,'female':1})\ntest['Sex'] = test['Sex'].map({'male':0,'female':1})\n\n# 'Southampton' to 1\n# 'Cherborg' to 2\n# 'Queenstown' to 3\ntrain['Embarked'] = train['Embarked'].map({'S':1,'C':2,'Q':3})\ntest['Embarked'] = test['Embarked'].map({'S':1,'C':2,'Q':3})","bd672591":"print('Training Data:')\n\nmissing_train_data = train.isnull().sum()\nprint(missing_train_data)\n\nfor i in ['Age', 'Cabin', 'Embarked']:\n    \n    print(f'Missing {missing_train_data[i]\/len(train):.2%} of {i} data')\n\nprint('\\nTesting Data:')\n\nmissing_test_data = test.isnull().sum()\nprint(missing_test_data)\n    \nfor i in ['Age', 'Fare', 'Cabin']:\n    \n    print(f'Missing {missing_test_data[i]\/len(test):.2%} of {i} data')","f39869a6":"train.drop(columns='Cabin', inplace=True)\ntest.drop(columns='Cabin', inplace=True)","f71cc6e3":"test[test.Fare.isnull()]","93959e2a":"avg_fare = train.groupby(['Pclass']).Fare.mean()\nprint(avg_fare.round(2))\n\ntest.Fare.fillna(avg_fare[3], inplace=True)\n\n# Fun Fact: a $13.68 ticket in 1912 would cost $378.02 today.","fd658992":"train[train.Embarked.isnull()]","7729b06b":"train.Embarked.fillna(1, inplace=True)\n\ntrain.drop(columns=['Name', 'Ticket'], inplace=True)\ntest.drop(columns=['Name', 'Ticket'], inplace=True)","17013bff":"full_df = train.append(test)\n\nfull_df['Fcount'] = full_df['SibSp'] + full_df['Parch'] + 1\n\nfull_df.Fcount.value_counts(normalize=True)","bc76a5de":"full_df['Fsize'] = full_df['Fcount']\n\nfull_df['Fsize'].loc[full_df['Fcount'] == 1] = 0\nfull_df['Fsize'].loc[full_df['Fcount'] == 2] = 1\nfull_df['Fsize'].loc[full_df['Fcount'] == 3] = 1\nfull_df['Fsize'].loc[full_df['Fcount'] > 3] = 2","83c70b10":"# Very Cheap (0), Cheap(1), Average(2), Expensive(3), Very Expensive(4)\nfare_categories = pd.qcut(full_df['Fare'], 5, labels=[0,1,2,3,4])\nfull_df['FareCat'] = fare_categories.values.astype('int8')\nfull_df\n","2b74cd31":"plt.figure(figsize=(12,10))\nsns.heatmap(full_df.corr(), annot=True)","2888411d":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\n# subset of df where we always know age\nknown_ages = full_df.dropna()\n\ndef average_approach():\n    '''Splits data into training and control groups.\n    Fills missing Age data with mean average age.\n    Prints MAE between prediction and actual.'''\n    \n    X = known_ages\n    y = known_ages.Age\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n    \n    mean_age = known_ages.Age.mean()\n   \n    avg_predictions = [mean_age for each in y_test]\n    avg_mae = mean_absolute_error(y_test, avg_predictions)\n    \n    print(f'Accuracy of predictions is (+-) {avg_mae.round(2)} years.')\n\naverage_approach()","22f793bd":"\n\ndef jedds_categorizer():\n    '''Splits data into training and control groups.\n    Sorts passengers by class, gender, and Fsize.\n    Average age of each group is stored in a dictionary.\n    Unknown ages are classified by group.\n    Predictions are drawn from said dictionary.\n    Prints MAE between prediction and actual.'''\n    \n    X = known_ages\n    y = known_ages.Age\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n    # gets the averages by group and stores them as a series\n    avg_ages = known_ages.groupby(['Pclass', 'Sex', 'Fsize'])['Age'].mean()\n    \n    # each key in the dictionary is a three digit number indicating class, sex, family size\n    # example: 100 - first class, male, solo\n    # example: 312 - third class, female, large family\n    \n    jedds_categorizer.age_dictionary = {\n        '100': avg_ages.iloc[0],\n        '101': avg_ages.iloc[1],\n        '102': avg_ages.iloc[2],\n        '110': avg_ages.iloc[3],\n        '111': avg_ages.iloc[4],\n        '112': avg_ages.iloc[5],\n        '200': avg_ages.iloc[6],\n        '201': avg_ages.iloc[7],\n        '202': avg_ages.iloc[8],\n        '210': avg_ages.iloc[9],\n        '211': avg_ages.iloc[10],\n        '212': avg_ages.iloc[11],\n        '300': avg_ages.iloc[12],\n        '301': avg_ages.iloc[13],\n        '302': avg_ages.iloc[14],\n        '310': avg_ages.iloc[15],\n        '311': avg_ages.iloc[16],\n        '312': avg_ages.iloc[17],}\n    \n    age_predictions = []\n    for row in X_test.itertuples():\n        sig = f'{row[2]}{row[3]}{row[10]}'\n        age_predictions.append(jedds_categorizer.age_dictionary[sig])\n    \n    # returns the MAE of our predictions vs the test data\n    jedds_cat_mae = mean_absolute_error(y_test, age_predictions)\n    \n    print(f'Accuracy of predictions is (+-) {jedds_cat_mae.round(2)} years.')\n\njedds_categorizer()","2e33ccaa":"from sklearn import svm\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Ridge\n\nhigh_corr_variables = ['Pclass', 'Fsize', 'FareCat']\n\ndef svr_regressor():\n    '''Splits data into training and control groups.\n    Runs several Regression estimators to generate predictions.\n    Prints MAE between prediction and actual.'''\n    \n    X = known_ages[high_corr_variables]\n    y = known_ages.Age\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n    \n    for test_model in [svm.SVR(), SGDRegressor(), Lasso(), ElasticNet(), Ridge()]:\n        model = test_model\n        model.fit(X_train, y_train)\n        model_prediction = model.predict(X_test)\n        model_mae = mean_absolute_error(y_test, model_prediction)\n\n        print(f'Mean Average Error of {model} is (+-) {model_mae.round(2)} years.')\n\nsvr_regressor()","b3990ea6":"for row in full_df[full_df.Age.isnull()].itertuples():\n    sig = f'{row[2]}{row[3]}{row[10]}'\n    # row[0] is index (passenger id)\n    full_df.loc[row[0], 'Age'] = jedds_categorizer.age_dictionary[sig].round(1)\n\nfull_df.isnull().sum()","d972fb10":"train, test = full_df[:891], full_df[891:]\n\ntest = test.drop(columns='Survived')","6e0470de":"plt.figure(figsize=(12,10))\nsns.heatmap(train.corr(), annot=True)","2858ab53":"prediction_variables = ['Sex', 'Pclass', 'FareCat']","548e5272":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\n\nX = train[prediction_variables]\ny = train.Survived\n    \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n\nfor test_model in [svm.SVC(), svm.LinearSVC(), LogisticRegression(), GaussianNB(), \n                   KNeighborsClassifier(), SGDClassifier(), DecisionTreeClassifier(), RandomForestClassifier()]:\n    \n    model = test_model\n    model.fit(X_train, y_train)\n    model_prediction = model.predict(X_test)\n    model_rmse = mean_squared_error(y_test, model_prediction)\n    model_accuracy = accuracy_score(y_test, model_prediction)\n\n    print(f'Mean Squared Error of {model} is {model_rmse.round(4)}')\n    print(f'Accuracy Score of {model} is {model_accuracy:.2%}\\n')","98f82ace":"model = DecisionTreeClassifier()\nmodel.fit(X, y)\npredictions = model.predict(test[prediction_variables])\n\nfinal_submission = pd.DataFrame({'PassengerId': test.index, 'Survived': predictions.astype('int32')})\nfinal_submission.to_csv('submission.csv', index=False)","7b62317d":"Next let's proceed to check for missing values.","30195b4b":"### Knowing the Ropes\n\nOur data is a subset of the *Titanic*'s manifest, with each row representing a passenger of the ill-fated vessel.  \nThe columns in the dataframe represent the following:\n\n   - Survived  \n      value of 1 if the passenger lived; value of 0 if the passenger died\n      \n   - Pclass  \n      1 for First Class, 2 for Second Class, and 3 for Third Class tickets\n      \n   - Name  \n      name and title\n      \n   - Sex  \n      binary gender\n      \n   - Age  \n      age in years\n      \n   - SibSp  \n      number of other *Titanic* passengers who are siblings or spouses to this passenger\n      \n   - Parch  \n      number of other *Titanic* passengers who are parents or children of this passenger\n      \n   - Ticket  \n      ticket number\n      \n   - Fare  \n      amount, in dollars, paid for ticket\n   - Cabin  \n      passenger's room number   \n   - Embarked  \n      port from which the passenger embarked\n           \nFrom this information we can see that some of our columns are categorical (Survived, Pclass, Sex, Embarked) and some are numerical (Age, SibSp, Parch, Fare).  \nThis will later influence how we select which machine learning model to use.  \n\n### Swabbing the Deck\n\nWe have some cleaning to do so that our data is in ship-shape.\n\nFirst let's encode our categorical data into a format that computers can understand.","08c4d91f":"In both datasets we are missing over 75% of cabin numbers.  \nTo avoid false making false correlations with the little cabin data that does exist, we will remove it entirely from our analysis.\n\n","2b97907f":"In the training dataset we are missing two values from the Embarked column.","b4cf6923":"We will assign the average cost of a 3rd Class ticket to this cell.","82d27be2":"We can see that the highest correlations for determining Survival are gender, class, and fare category.\n\nThese are the metrics we will use in training our predictive models.","c806ae60":"In our testing dataset we have one null value for Fare.","e4f7261e":"Our best performing model is a Decision Tree Classifier, predicting passenger survival with approximately 80% accuracy.  \n\nWe will write the predictions for our testing data to a CSV file and submit it for scoring.","bdaa8f72":"While these regressors are usually more accurate than a simple approach, we must consider that for all of our testing data we do not know if the passenger lived or died; this means that we cannot use Survival as a metric upon which to train our model when filling Ages from the testing data. Further contributing to error could be the fact that our dataset is relatively small.  \n\nAs the models do not outperform our simple categorical approach, we will proceed by using our average-age-by-grouping method to fill in the missing Age data.\n\nPerhaps an estimator will be of more use to us *after* we have filled in the blanks.","f99c71c4":"It seems that 60% of passengers were traveling solo, 30% with one or two family members, and cumulatively 10% with a family of 4 or larger.\n\nLet's categorize this observation with a simple designation: 0 for solo, 1 for small family, and 2 for large family.","5ee681f5":"Now we will run our data through several Classification estimators to determine Survival.","6bc12764":"Now let's create a new column that represents the number of members in a given family by using the SibSp and Parch data.\n\n\nFirst, we can combine our training and testing datasets to maximize the number of observations.","a7098860":"Now the only missing data that remains is approximately 20% of passenger ages.  \nLet's examine a few different ways of dealing with this.  \n\nWe can explore how strongly correlated our different variables are with the Seaborn heatmap method.","1711d45f":"At long last, we have accounted for all missing data - talk about running a tight ship!  \n\nLet's also remember to return our full dataframe to it's original two sets.","91d608c2":"Another categorization we can make is to encode our Fare prices into ranges.","355de5b5":"##### Using an Estimator\n\nThere are a number of estimators available in the scikit-learn library.  \nBecause we trying to predict a numeric value, we will compare the results of several Regression estimators.","9a124daa":"### Making Waves\n\nOur solution ranks in the top 5% of submissions!  \n\nThis project encouraged me to understand how machine learning algorithms function. The fundamental differences in methodology between ML techniques, vocabulary, and the application of the scikit-learn library are essential tools for a budding data analyist.  \n\nAlong the way I picked up a some handy Python\/Pandas methods, and I discovered a few new strategies for dealing with missing data.  \n\nI would highly recommend this exercise for any person interested in ML.","8a6ee6df":"The variables which most influence Age are passenger class, family count, and fare price.\n\nWith this in mind, let's explore a few different ways of predicting our missing Age values.","fdc6f05d":"## Exploratory Data Analysis and Predictive Modeling - Featuring Data from RMS *Titanic*\n\n---\n\n### All Aboard!\n\nThis project is for the Kaggle Titanic ML Competition, a popular challenge for those interested in machine learning and data science.  \n\nOur task is to create a model that predicts if a given passenger survived the sinking of the *Titanic*.  \n\nWe will analyze the provided data to determine which factors correlate highly with chances of survival and then build a machine learning model around the most relevant features.  \n\nLet's begin by importing the necessary libraries and loading our data.","bb176f3c":"As these two passengers were survivors, it is known that they [borded together from Southampton.](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/amelia-icard.html)  \n\nWe will ammend the rows to include this information.  \n\nFurthermore, for the sake of this analysis, we will be removing passenger names and ticket numbers, as well.","5650427e":"##### Catergorical Approach\n\nAnother approach would be to sort passengers by class, gender, and family size - then take the average age of passengers in each subgroup.","27e0cd33":"### On an Even Keel\n\nNow that we have a complete set of training data on which to build our predictions, let's again vist the heatmap to determine which variables are most highly correlated to a passenger's survival.","9c8e384e":"\n##### Average Approach (naive)\n\nOne possibility is to fill the missing values with the average age value.\n"}}