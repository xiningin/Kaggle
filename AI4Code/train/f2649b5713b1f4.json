{"cell_type":{"d73bb287":"code","b45d1b95":"code","9cf42f82":"code","d593c768":"code","fe6304ac":"code","caf67aaa":"code","e442c15e":"code","95194fde":"code","fd7c8637":"code","3c110f6e":"code","8b60b8ee":"code","f927d77f":"code","7d46d691":"code","0c30210f":"code","8526dd2d":"code","5146bcd2":"code","c2ac33c1":"code","075523ab":"code","0ff7e589":"code","a29925e8":"code","fad92b7b":"code","d13fa4d4":"code","eb09ee59":"code","8328b25d":"code","440861a0":"code","6b5e6fc9":"code","ee64f52f":"code","956940a3":"code","88dc3e8f":"code","7173bd3d":"code","e04dd36b":"code","195437da":"code","93b98834":"code","643580e3":"markdown","24044731":"markdown","b60e3938":"markdown","57f47375":"markdown","ae75a55c":"markdown","eaba6fa8":"markdown","59b4bae1":"markdown","127331be":"markdown","51e9b05e":"markdown","d0f2ce74":"markdown","64e1647f":"markdown","0558caee":"markdown","fa5e1a16":"markdown","4220e113":"markdown","f6e433de":"markdown","e21118cb":"markdown","bd730720":"markdown","0b5b7a7f":"markdown","a985c342":"markdown","6a7af0b3":"markdown","492279a6":"markdown","40c5a81a":"markdown","0e44969f":"markdown","f624b8b0":"markdown","e26b5398":"markdown","4a1a1f29":"markdown","e3de9564":"markdown","1e0a4204":"markdown","69f28539":"markdown","e0025680":"markdown","492f58c0":"markdown","cd25e4ca":"markdown","e6b8039f":"markdown","8d9addbb":"markdown","34386037":"markdown","9cd6f9ce":"markdown","684c80e8":"markdown"},"source":{"d73bb287":"!pip install pycaret --quiet","b45d1b95":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n#import regression module\nfrom pycaret.regression import *","9cf42f82":"BASE_PATH = '..\/input\/trends-assessment-prediction'\n\nfnc_df = pd.read_csv(f\"{BASE_PATH}\/fnc.csv\")\nloading_df = pd.read_csv(f\"{BASE_PATH}\/loading.csv\")\nlabels_df = pd.read_csv(f\"{BASE_PATH}\/train_scores.csv\")","d593c768":"fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\nlabels_df[\"is_train\"] = True\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\nprint(f'Shape of train data: {df.shape}, Shape of test data: {test_df.shape}')","fe6304ac":"target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\ndf.drop(['is_train'], axis=1, inplace=True)\ntest_df = test_df.drop(target_cols + ['is_train'], axis=1)\n\n\n# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1\/500\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","caf67aaa":"def get_train_data(target):\n    other_targets = [tar for tar in target_cols if tar != target]\n    train_df = df.drop( other_targets, axis=1)\n    return train_df","e442c15e":"\ntarget = 'age'\n\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)","95194fde":"# There are few models which take a lot of time, ignoring those models to make demo faster.\nblacklist_models = ['ransac', 'tr', 'rf', 'et', 'ada', 'gbr', 'xgboost', 'catboost']","fd7c8637":"compare_models(\n    blacklist = blacklist_models,\n    fold = 5,\n    sort = 'MAE', ## competition metric\n    turbo = True\n)\n","3c110f6e":"lgbm_age = create_model(\n    estimator='lightgbm',\n    fold=5\n)","8b60b8ee":"# here we are tuning the above created model\ntuned_lgbm_age = tune_model(\n    estimator='lightgbm',\n    fold=5\n)","f927d77f":"# plot_model(estimator = None, plot = \u2018residuals\u2019)\nplot_model(estimator = tuned_lgbm_age, plot = 'learning')","7d46d691":"plot_model(estimator = tuned_lgbm_age, plot = 'residuals')","0c30210f":"plot_model(estimator = tuned_lgbm_age, plot = 'feature')","8526dd2d":"evaluate_model(estimator=tuned_lgbm_age)","5146bcd2":"interpret_model(\n    estimator=tuned_lgbm_age,\n    plot = 'summary',\n    feature = None,\n    observation = None\n)","c2ac33c1":"predictions =  predict_model(tuned_lgbm_age, data=test_df)","075523ab":"predictions.head()","0ff7e589":"target = target_cols[0]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    blacklist = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","a29925e8":"target = target_cols[1]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    blacklist = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","fad92b7b":"target = target_cols[2]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    blacklist = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","d13fa4d4":"target = target_cols[3]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    blacklist = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","eb09ee59":"target = target_cols[4]\ntrain_df = get_train_data(target)\n\nsetup_reg = setup(\n    data = train_df,\n    target = target,\n    train_size=0.8,\n    numeric_imputation = 'mean',\n    silent = True\n)\n\ncompare_models(\n    blacklist = blacklist_models,\n    fold = 7,\n    sort = 'MAE',\n    turbo = True\n)","8328b25d":"# mapping targets to their corresponding models\n\nmodels = []\n\ntarget_models_dict = {\n    'age': 'br',\n    'domain1_var1':'ridge',\n    'domain1_var2':'svm',\n    'domain2_var1':'ridge',\n    'domain2_var2':'svm',\n}\n\ndef tune_and_ensemble(target):\n    train_df = get_train_data(target)    \n    exp_reg = setup(\n        data = train_df,\n        target = target,\n        train_size=0.8,\n        numeric_imputation = 'mean',\n        silent = True\n    )\n    \n    model_name = target_models_dict[target]\n    tuned_model = tune_model(model_name, fold=7)\n    model = ensemble_model(tuned_model, fold=7)\n    return model\n","440861a0":"target = target_cols[0]\nmodel = tune_and_ensemble(target)\nmodels.append(model)","6b5e6fc9":"target = target_cols[1]\nmodel = tune_and_ensemble(target)\nmodels.append(model)","ee64f52f":"target = target_cols[2]\nmodel = tune_and_ensemble(target)\nmodels.append(model)","956940a3":"target = target_cols[3]\nmodel = tune_and_ensemble(target)\nmodels.append(model)","88dc3e8f":"target = target_cols[4]\nmodel = tune_and_ensemble(target)\nmodels.append(model)","7173bd3d":"### create a pipelie or functio to run for all targets\n\ndef finalize_model_pipeline(model, target):\n    # this will train the model on houldout data\n    finalize_model(model)\n    save_model(model, f'{target}_{target_models_dict[target]}', verbose=True)\n    # making predictions on test data\n    predictions = predict_model(model, data=test_df)\n    test_df[target] = predictions['Label'].values","e04dd36b":"for index, target in enumerate(target_cols):\n    model = models[index]\n    finalize_model_pipeline(model,target)","195437da":"sub_df = pd.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\n\nsub_df.to_csv(\"submission1.csv\", index=False)","93b98834":"sub_df.head(10)","643580e3":"### 3.4 Tune Model Hyperparameters\n\n* `tune_model` function tunes the hyperparameters of a model and scores it using K-fold Cross Validation. The output prints the score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold (by default = 10 Folds). This function returns a trained model object.  \n\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#tune-model\n","24044731":"### 5.5 Tuning Support Vector Machines Model for `domain2_var2`","b60e3938":"### 4.3. comapring models for `domain1_var2`","57f47375":"# References\n\n* https:\/\/pycaret.org\/regression\/\n* https:\/\/towardsdatascience.com\/machine-learning-in-power-bi-using-pycaret-34307f09394a","ae75a55c":"For list of all available estimators and their abbreviations please visit https:\/\/pycaret.org\/regression\/","eaba6fa8":"### 3.2 Compare Models\n\n* `compare_models` function uses all models in the model library and scores them using K-fold Cross Validation. The output prints a score grid that shows MAE, MSE, RMSE, R2, RMSLE and MAPE by fold (default CV = 10 Folds) of all the available models in model library.\n\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#compare-models","59b4bae1":"### 3.5.1 Plotting learning curve","127331be":"### 4.5. comapring models for `domain2_var2`","51e9b05e":"## 3. Let's proceed with PyCaret for regression","d0f2ce74":"### 3.8 Predict Model\n\n* `predict_model` function is used to predict new data using a trained estimator. It accepts an estimator created using one of the function in pycaret that returns a trained  model object or a list of trained model objects created using stack_models() or create_stacknet(). New unseen data can be passed to data param as pandas Dataframe.  If data is not passed, the test \/ hold-out set separated at the time of setup() is used to generate predictions.\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#predict-model\n\n\n","64e1647f":"### 5.1 Tuning Bayesian Ridge Model for `age`","0558caee":"### 3.1 Setup our dataset (For demo just using `age` target)\n\n* `setup` function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. setup() must called before executing any other function in pycaret. It takes two mandatory parameters: dataframe {array-like, sparse matrix} and name of the target column. All other parameters are optional.\n\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#setup","fa5e1a16":"# END NOTES\n\n* This notebook is work in progress.  I will kepp updating this kernel with more and more info.\n* Feel free to use this kernel as the starting point. Happy kaggling:)\n\n**<span style=\"color:Red\">Please upvote this kernel if you like it . It motivates me to produce more quality content :)**  ","4220e113":"### Observations:\n\nOn close observation of ablove model comparisons, we have made following observations:\n* `age`: Bayesian Ridge\thas the minimum MAE\n* `domain1_var1`: Ridge Regression has the minimum MAE\n* `domain1_var2`: Support Vector Machines has the minimum MAE\n* `domain2_var1`: Ridge Regression has the minimum MAE\n* `domain2_var2`: Support Vector Machines has the minimum MAE\n\n> **Note: For demo purpose i have taken the model with lowest MAE in each target category, feel free to experiment with lowest 2 or 3 models and also try ensemble of various models in each category for better results.**","f6e433de":"## 2.1 Load Data","e21118cb":"### 5.4 Tuning Ridge Regression Model for `domain2_var1`","bd730720":"### 4.2. comapring models for `domain1_var1`","0b5b7a7f":"### 5.2 Tuning Ridge Regression Model for `domain1_var1`","a985c342":"# 1. Introduction to PyCaret\n\nPyCaret is an open source, **low-code** machine learning library in Python that aims to reduce the cycle time from hypothesis to insights. It is well suited for **seasoned data scientists** who want to increase the productivity of their ML experiments by using PyCaret in their workflows or for **citizen data scientists** and those **new to data science** with little or no background in coding. PyCaret allows you to go from preparing your data to deploying your model within seconds using your choice of notebook environment. Please click [this](https:\/\/pycaret.org\/guide\/) link to continue learning more about PyCaret. \n\n\n**<span style=\"color:Red\">Please upvote this kernel if you like it . It motivates me to produce more quality content :)**\n","6a7af0b3":"## 2 Importing Libraries","492279a6":"![pycaret](https:\/\/miro.medium.com\/max\/1400\/1*Q34J2tT_yGrVV0NU38iMig.jpeg)","40c5a81a":"### 3.5 Plot Model\n\n* `plot_model` function takes a trained model object and returns a plot based on the test \/ hold-out set. The process may require the model to be re-trained in certain cases. See list of plots supported below. Model must be created using create_model() or tune_model().\n\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#plot-model\n\n","0e44969f":"## 6. Finalize, save model and Inference","f624b8b0":"### 3.7 Interpret Model\n\n\n* `interpret_model` function takes a trained model object and returns an interpretation plot based on the test \/ hold-out set. It only supports tree based algorithms. This function is implemented based on the SHAP (SHapley Additive exPlanations), which is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations.\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#interpret-model\n\n","e26b5398":"## Create Submission","4a1a1f29":"### 3.6 Evaluate Model\n\n* `evaluate_model` function displays a user interface for all of the available plots for a given estimator. It internally uses the plot_model() function.\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#evaluate-model\n","e3de9564":"### 4.1. comapring models for `age`","1e0a4204":"### 3.5.3 plotting feature importance","69f28539":"### 5.3 Tuning Support Vector Machines Model for `domain1_var2`","e0025680":"### 4.4. comapring models for `domain2_var1`","492f58c0":"## 2.2 Utils","cd25e4ca":"## 4. Let's Proceed With Other Targets","e6b8039f":"Before proceeding let me clear few things:\n* Currently `PyCaret` do not have support for multitarget regression. So, instead of 1 model for our 5 targets, we need to create individual model for each target.","8d9addbb":"### 3.3 Create Model\nFor demo purpose let's create a Light Gradient Boosting Machine\n\n* `create_model` function creates a model and scores it using K-fold Cross Validation. (default = 10 Fold). The output prints a score grid that shows MAE, MSE, RMSE, RMSLE, R2 and MAPE. This function returns a trained model object. setup() function must be called before using create_model()\n\n\n\nFor more info please visit https:\/\/pycaret.org\/regression\/#create-model","34386037":"## 1.1 Installation (Let's install PyCaret)","9cd6f9ce":"### 3.5.2 plotting residuals","684c80e8":"## 5. Tuning Selected Models"}}