{"cell_type":{"186fa2cb":"code","0bf551f2":"code","8fc1a04c":"code","91674b16":"code","97975587":"code","b7a7a34e":"code","392daf47":"code","98b03e35":"code","534c27c5":"code","c68bd47e":"code","c6632a3c":"code","789ebc01":"code","1f49b8eb":"code","e5880219":"code","3b19af42":"markdown"},"source":{"186fa2cb":"!pip install -U git+https:\/\/github.com\/albu\/albumentations --no-cache-dir --quiet","0bf551f2":"!pip install fastai --upgrade --quiet","8fc1a04c":"!pip install pretrainedmodels --quiet\n!pip install efficientnet_pytorch --quiet\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom torchvision.models import *\n\nfrom torch.distributions.beta import Beta\nfrom fastai.vision.all import *\nfrom fastai.callback.mixup import *\nimport pretrainedmodels\nimport seaborn as sns\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.vision.models import *\nfrom fastai.vision.learner import model_meta\n\nimport fastai\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.functional\")\n\nimport sys\nimport pandas as pd\nimport torch\nimport numpy as np\nprint(fastai.__version__)\n\nimport torchvision.models as models\nimport torch.nn as nn\n\nimport os\nfrom efficientnet_pytorch import EfficientNet\nmodel = EfficientNet.from_pretrained('efficientnet-b7')\n\n\n","91674b16":"from torch.nn import Linear\n\nmodel._fc = Linear(2560, 2)\nnn.init.kaiming_normal_(model._fc.weight)\nmodel = model.to(torch.device(\"cuda\"))","97975587":"path = \"..\/input\/cat-and-dog\"","b7a7a34e":"import albumentations\n\nclass AlbumentationsTransform(RandTransform):\n    split_idx, order = None, 2\n    def __init__(self, train_aug, valid_aug):\n        store_attr()\n    \n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)","392daf47":"def get_train_aug(sz):\n    return albumentations.Compose([\n        albumentations.RandomResizedCrop(sz,sz),\n        albumentations.Transpose(p=0.5),\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.VerticalFlip(p=0.5),\n        albumentations.ShiftScaleRotate(p=0.5),\n        albumentations.HueSaturationValue(\n            hue_shift_limit=0.2,\n            sat_shift_limit=0.2,\n            val_shift_limit=0.2,\n            p=0.5\n        ),\n        albumentations.RandomBrightnessContrast(\n            brightness_limit=(-0.1, 0.1),\n            contrast_limit=(-0.1,0.1),\n            p=0.5\n        ),\n        albumentations.CoarseDropout(p=0.5),\n        albumentations.Cutout(p=0.5)\n    ])\n\ndef get_valid_aug(sz):\n    return albumentations.Compose([\n        albumentations.CenterCrop(sz, sz, p=1.),\n        albumentations.Resize(sz,sz)\n    ], p=1.)","98b03e35":"def get_dls(sz, bs):\n    item_tfms = AlbumentationsTransform(get_train_aug(sz), get_valid_aug(sz))\n    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n    dls = ImageDataLoaders.from_folder(path, valid_pct=0.2, seed=999, bs=bs, item_tfms=item_tfms, batch_tfms=batch_tfms )\n    return dls","534c27c5":"dls = get_dls(255,16)","c68bd47e":"dls.show_batch()","c6632a3c":"!pip install timm --quiet","789ebc01":"from fastai.vision.all import *\nfrom fastai.callback.mixup import *\nfrom torch.distributions.beta import Beta\nimport timm\nset_seed(314)","1f49b8eb":"class CutMix(MixUp):\n    def __init__(self, alpha=1.): self.distrib = Beta(tensor(alpha), tensor(alpha))\n    def before_batch(self):\n        lam = self.distrib.sample().squeeze().to(self.x.device)\n        shuffle = torch.randperm(self.y.size(0)).to(self.x.device)\n        self.yb1 = tuple(L(self.yb).itemgot(shuffle))\n        nx_dims = len(self.x.size())\n        bs, c, h, w = self.x.shape\n        rx, ry = w*self.distrib.sample(), h*self.distrib.sample()\n        rw, rh = w*(1-lam).sqrt(), h*(1-lam).sqrt()\n        x1 = (rx-rw\/2).clamp(min=0).round().to(int)\n        x2 = (rx+rw\/2).clamp(max=w).round().to(int)\n        y1 = (ry-rh\/2).clamp(min=0).round().to(int)\n        y2 = (ry+rh\/2).clamp(max=h).round().to(int)\n        self.learn.xb[0][:,:,y1:y2,x1:x2] = self.learn.xb[0][shuffle,:,y1:y2,x1:x2]\n        self.lam = 1- float(x2-x1)*(y2-y1)\/(h*w)\n        \n        if not self.stack_y:\n            ny_dims = len(self.y.size())\n            self.learn.yb = tuple(L(self.yb1,self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))","e5880219":"mixup = CutMix()\nwith Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=mixup) as learn:\n    learn.epoch,learn.training = 0,True\n    learn.dl = dls.train\n    b = dls.one_batch()\n    learn._split(b)\n    learn('before_batch')\n\n_,axs = plt.subplots(3,3, figsize=(9,9))\ndls.show_batch(b=(mixup.xb[0],mixup.y), ctxs=axs.flatten())","3b19af42":"from : https:\/\/www.kaggle.com\/ritesh2000\/cutmix-aug-amp-using-fastai but quieter."}}