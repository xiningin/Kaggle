{"cell_type":{"c94370ab":"code","5841488c":"code","8381c8df":"code","c0126b35":"code","c4fc3120":"code","54afcd16":"code","3fbd142b":"code","40ce3616":"code","0d30946f":"code","74fc5477":"code","4483775a":"code","785a0e0b":"code","e9d1c42b":"code","6dbdc7fa":"code","1f4f2f8f":"code","6e103411":"code","851f5079":"code","6fb71a59":"code","e7ee4033":"code","8c7abfa9":"code","90cd9817":"code","ffb03141":"code","154b2222":"code","4bae7009":"code","136749d3":"code","4193ec19":"code","d0efae9f":"code","2cc74844":"code","34d133ec":"code","138bcf32":"code","c8d3cfaf":"code","ede2397d":"code","94a5c172":"code","328f8824":"code","92b001a7":"code","096a8287":"code","fd9ec1cf":"markdown","c6d1d657":"markdown","a218efe3":"markdown","3a29f97b":"markdown","9e76303d":"markdown","a8c095e2":"markdown","b35b4740":"markdown","2e6837fd":"markdown","996460b2":"markdown","364c9bef":"markdown","7a5838a4":"markdown","0a4d3e41":"markdown","1e07f18e":"markdown","82637dd7":"markdown","0d4407fb":"markdown","1d985a85":"markdown","2332ad9d":"markdown","edd08344":"markdown","6f66642d":"markdown","fa5c8dea":"markdown","ad1c359e":"markdown","242bef3a":"markdown","bb4362fe":"markdown","27f7f756":"markdown","71d6f7a8":"markdown","411fadf1":"markdown","66e517c4":"markdown","72c435b0":"markdown","bb07d17b":"markdown","28b707a1":"markdown","1c9096f4":"markdown","d14b09ca":"markdown","61063435":"markdown","7f70c994":"markdown","0eb941cf":"markdown","a2efc361":"markdown","2e9e5d64":"markdown","f6811a62":"markdown","8b167123":"markdown","38bc8344":"markdown"},"source":{"c94370ab":"import re\nimport eli5\nimport spacy\nimport nltk as nl\nimport pandas as pd\nfrom sklearn.base import clone\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import hstack\nfrom nltk.corpus import stopwords\nfrom ml_helper.helper import Helper\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom scikitplot.metrics import plot_confusion_matrix\nfrom sklearn.preprocessing import FunctionTransformer\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.metrics import accuracy_score as metric_scorer\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nnl.download('stopwords')\n%matplotlib inline","5841488c":"KEYS = {\n    \"SEED\": 1,\n    \"DATA_PATH\": \"..\/input\/fake_or_real_news.csv\",\n    \"TARGET\": \"label\",\n    \"METRIC\": \"accuracy\",\n    \"TIMESERIES\": False,\n    \"SPLITS\": 3,\n    \"ESTIMATORS\": 150,\n    \"ITERATIONS\": 500,\n}\n\nhp = Helper(KEYS)","8381c8df":"df = pd.read_csv(KEYS[\"DATA_PATH\"], header=0, names=[\"id\", \"title\", \"text\", \"label\"])\ntrain, test = train_test_split(df, test_size=0.20, random_state=KEYS[\"SEED\"])","c0126b35":"train.dtypes","c4fc3120":"train.head()","54afcd16":"hp.missing_data(df)","3fbd142b":"train.iloc[10,2]","40ce3616":"hp.target_distribution(train)","0d30946f":"train[\"concat\"] = train[\"title\"] + train[\"text\"]\ntest[\"concat\"] = test[\"title\"] + test[\"text\"]\ntrain.head()","74fc5477":"count_vect = CountVectorizer()\nbase_train = count_vect.fit_transform(train[\"concat\"])\nprint(base_train.shape)","4483775a":"tfidf_transformer = TfidfTransformer()\nbase_train = tfidf_transformer.fit_transform(base_train)\nprint(base_train.shape)","785a0e0b":"pcaed = TruncatedSVD(n_components=2).fit_transform(base_train)\npcaed = pd.concat([pd.DataFrame(pcaed).reset_index(drop=True), train[\"label\"].reset_index(drop=True)], axis=1)\npcaed.head()","e9d1c42b":"pctrue=pcaed[pcaed[\"label\"]==\"REAL\"]\npcfake=pcaed[pcaed[\"label\"]==\"FAKE\"]\nplt.figure(figsize = (12,8))\nplt.xlabel('Principal Component 1', fontsize = 15)\nplt.ylabel('Principal Component 2', fontsize = 15)\nplt.title('2 Component PCA on TF-IDF Representation', fontsize = 20)\nplt.scatter(pctrue[0], pctrue[1], color=\"blue\")\nplt.scatter(pcfake[0], pcfake[1], color=\"red\")\nplt.legend([\"REAL\", \"FAKE\"])","6dbdc7fa":"basepipe = Pipeline([\n    ('vect', TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), sublinear_tf=True))\n])\n    \n\nmodels = [\n    {\"name\": \"naive\", \"model\": MultinomialNB()},\n    {\"name\": \"logistic_regression\", \"model\": LogisticRegression(solver=\"lbfgs\", max_iter=KEYS[\"ITERATIONS\"], random_state=KEYS[\"SEED\"])},\n    {\"name\": \"svm\", \"model\": SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=KEYS[\"SEED\"])},\n    {\"name\": \"pac\", \"model\":  PassiveAggressiveClassifier(max_iter=1000, random_state=KEYS[\"SEED\"], tol=1e-3)},\n]\n\nall_scores = hp.pipeline(train[[\"concat\", \"label\"]], models, basepipe, note=\"Base models\")","1f4f2f8f":"def stemmer(df, stem = \"snow\"):\n    if stem == \"port\":\n        stemmer = PorterStemmer()\n    else:\n        stemmer = SnowballStemmer(language='english')\n    df = df.apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split()]).lower())\n    return df","6e103411":"train.iloc[3:4][\"concat\"]","851f5079":"stemmer(train[\"concat\"].iloc[3:4])","6fb71a59":"nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\ndoc = nlp(train['concat'][3])\nprint(doc.text[:50])\nprint('----------------------------------------------------')\nfor token in doc[:5]:\n    print(f'Token: {token.text}, Lemma: {token.lemma_}, POS: {token.pos_}')","e7ee4033":"def tokenizer(text):\n    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n    return [token.lemma_.lower().strip() + token.pos_ for token in nlp(text)]","8c7abfa9":"def strict_tokenizer(text):\n    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n    return [token.lemma_.lower().strip() + token.pos_ for token in nlp(text)\n        if \n            not token.is_stop and not nlp.vocab[token.lemma_].is_stop\n            and not token.is_punct\n            and not token.is_digit\n    ]","90cd9817":"snow_pipe = Pipeline([\n    ('snow_stem', FunctionTransformer(stemmer, validate=False)),\n    ('vect', TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), sublinear_tf=True)),\n])","ffb03141":"port_pipe = Pipeline([\n    ('port_stem', FunctionTransformer(stemmer, kw_args={\"stem\": \"port\"}, validate=False)),\n    ('vect', TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), sublinear_tf=True)),\n])","154b2222":"lemm_pipe = Pipeline([\n    ('lemma_vect', TfidfVectorizer(analyzer = 'word', max_df=0.99, min_df=0.01, ngram_range=(1,2), tokenizer=tokenizer))\n])","4bae7009":"strict_lemm_pipe = Pipeline([\n    ('strict_lemma_vect', TfidfVectorizer(analyzer = 'word', max_df=0.99, min_df=0.01, ngram_range=(1,2), tokenizer=strict_tokenizer))\n])","136749d3":"models = [\n    {\"name\": \"logistic_regression\", \"model\": LogisticRegression(solver=\"lbfgs\", max_iter=KEYS[\"ITERATIONS\"], random_state=KEYS[\"SEED\"])},\n    {\"name\": \"pac\", \"model\":  PassiveAggressiveClassifier(max_iter=KEYS[\"ITERATIONS\"], random_state=KEYS[\"SEED\"], tol=1e-3)},\n]","4193ec19":"all_scores = hp.pipeline(train[[\"concat\", \"label\"]], models, snow_pipe, all_scores=all_scores, quiet = True)\nall_scores = hp.pipeline(train[[\"concat\", \"label\"]], models, port_pipe, all_scores=all_scores, quiet = True)\nall_scores = hp.pipeline(train[[\"concat\", \"label\"]], models, lemm_pipe, all_scores=all_scores, quiet = True)\nall_scores = hp.pipeline(train[[\"concat\", \"label\"]], models, strict_lemm_pipe, all_scores=all_scores)","d0efae9f":"hp.plot_models(all_scores)","2cc74844":"hp.show_scores(all_scores, top=True)","34d133ec":"grid = {\n    \"pac__C\": [1.0, 10.0],\n    \"pac__tol\": [1e-2, 1e-3],\n    \"pac__max_iter\": [500, 1000],\n}\n\nfinal_scores, pipe = hp.cross_val(train[[\"concat\", \"label\"]], model=clone(hp.top_pipeline(all_scores)), grid=grid)\nfinal_scores","138bcf32":"print(pipe.best_params_)\nfinal_pipe = pipe.best_estimator_","c8d3cfaf":"eli5.show_weights(final_pipe, top=30, target_names=train.label)","ede2397d":"test.concat.iloc[2]","94a5c172":"eli5.show_prediction(final_pipe.named_steps['pac'], test.concat.iloc[2], vec = final_pipe.named_steps['lemma_vect'], top=30, target_names=train.label)","328f8824":"predictions = final_pipe.predict(test[\"concat\"])\nmetric_scorer(test[\"label\"], predictions)","92b001a7":"print(classification_report(test[\"label\"], predictions))","096a8287":"plot_confusion_matrix(test[\"label\"], predictions)","fd9ec1cf":"## Data Loading and Processing\n\nHere we load the necessary data, splitting 80% for training and the remaining 20% for testing. We also review its types and print its first rows.","c6d1d657":"## Normalized TF\/TF-IDF Representation\n\nNow we transform the matrix of token counts to a normalized tf or tf-idf representation, where tf represents term frequency and tf-idf represents the frequency times the inverse document frequency, that way the importance\/scale of certain repeated tokens throughout the text is reduced.","a218efe3":"# NLP for Fake News Classification\n\nA dataset from [Real or Fake](https:\/\/www.kaggle.com\/rchitic17\/real-or-fake) is provided containing different news articles.\n\nWe want to build a  model that can classify if a given article is considered fake or not. We will use a subset of the data for training and the remaining for testing our model.\n\n## Outline\n\nWe separate the project in 3 steps:\n\n**Data Loading and Processing:** Load the data and analyze it to obtain an accurate picture of it, its features, its values (and whether they are incomplete or wrong), its data types among others. We also do the required processing.\n\n**Feature Engineering \/ Modeling:** Once we have the data, we create some features and then the modeling stage begins, we set multiple baselines making use of different models, we will hopefully produce a model that fits our expectations of performance. Once we have that model, a process of tuning it to the training data would be performed.\n\n**Results and Conclusions:** Finally, with our tuned model, we  predict against the test set, and finally, outline our conclusions.","3a29f97b":"### Original","9e76303d":"### Setting Key Values\n\nThe following values are used throught the code, this cell gives a central source where they can be managed.","a8c095e2":"## Hyperparameter Tuning\n\nNow that we have the best performing model which is a Passive Agressive Classifier with lemmatization and POS tagging of the concatenated text and title, we will do a 5 fold cross validated randomized grid search over it to get the best parameters for the model.","b35b4740":"### Pipeline Performance by Model","2e6837fd":"### Pipeline 3: Lemmatizer with POS Tagging","996460b2":"### Pipeline 4: Strict Lemmatizer with POS Tagging","364c9bef":"### Stemmed","7a5838a4":"### Sample News","0a4d3e41":"# Conclusions\n\nThe classification report obtained from our final model on a 20% holdout of the data shows its accuracy, precision (how often the predictions are correct) and the recall (how many of the total observations in the set are correctly classified), also its f1-score (harmonic average of both). The weighted average for all of them stands at 96% which means that it can classify which articles are fake with great efficacy.\n\nThis information is extremely useful to multiple actors, including social networks and end consumers since it can help them differentiate between real and fake stories, which often lead to skewed views of current world events.","1e07f18e":"### Data Types","82637dd7":"### 2 Component PCA on TF-IDF Representation\nNow we will reduce dimensionality on the TF-IDF Representation to 2 components to see how they differ.","0d4407fb":"## Pipelines\nNow we define 2 different pipelines that will be tested, one with Stemming and another one with Lemmatization.","1d985a85":"### Top Pipelines per Model\n\nHere we show the top pipelines per model.","2332ad9d":"### Defining our own Tokenizer\n\nWe define our own tokenizer, which lemmatizes, lowercases and adds POS tagging. We also try removing stop words, punctuations and digits with a second tokenizer.","edd08344":"## Lemmatizing and POS Tagging with Spacy","6f66642d":"Testing all pipelines consecutively.","fa5c8dea":"The distribution is perfect, therefore no resampling is needed.","ad1c359e":"### Check target variable balance\nWe review the distribution of values in the target variable.","242bef3a":"### Stemming\nHere we stem the news text with two different stemmers Porter and Snowball. Overall, we perform the following transformations:\n\n- Stemming of the words\n\n- Removing punctuation, stop words and other characters\n\n- Convert to Lower case and split string into words (tokenization)","bb4362fe":"## Explaining the model\nHere we see the associated weights to the different tokens and wether they are positive or negative with regards to classifying a news as REAL.\n\nIt seems like proper punctuation ('s in the end of words, use of commas, etc) really help the model to see if it is a real or fake news, also having many exclamation signs does the inverse.","27f7f756":"From the weights of the different positive and negative tokens the news has we can see how it would predict the article.","71d6f7a8":"## Predictions and Results on Test Set\n\nNow with our final pipeline we perform preditions on the entire test set.\n\nTo review the performance of the model, accuracy is not enough, therefore we plot a confusion matrix and print a classification report.\n\n### Accuracy","411fadf1":"Stemming is closely related to lemmatization. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.","66e517c4":"And these are the models we will test with the different pipelines, based on the results obtained from our baselines","72c435b0":"We can see that the Passive Aggressive classifier was the best performing one during our baseline.","bb07d17b":"Using Spacy we will do the following transformations:\n\n- Lemmatization of the words\n\n- POS Tagging of the words\n\n- Removing punctuation, stop words and other characters\n\n- Convert to Lower case and split string into words (tokenization)\n\n- We remove words with a frequency of less than 0.01% or with more than 0.99%\n\nHere is a sample of the lemmatization and tagging:","28b707a1":"## Feature Engineering","1c9096f4":"We can see that they are very similar and that there is no clear distinction between the two.","d14b09ca":"### Missing Data\n\nWe check if there is any missing data.","61063435":"### Pipeline 2: Porter Stemmer","7f70c994":"## Tokenizing data\nWe convert the different articles to a matrix of token counts","0eb941cf":"### First Rows of the Data","a2efc361":"### Explained prediction on one news article\nUsign the same library we will attempt to understand how a specific news item from the test set is predicted.","2e9e5d64":"### Pipeline 1: Snowball Stemmer","f6811a62":"### Classification Report","8b167123":"## Baseline\n\nIn order to test the performance of our feature engineering steps, we will create several initial baseline models, that way we will see how our efforts increase the models predictive power.\n\n### Train Function\nHere we define the train function which will be used with the different models, it performs a cross validation score on 80% of the training data and a final validation on the remaining 20%.","38bc8344":"### Confusion Matrix"}}