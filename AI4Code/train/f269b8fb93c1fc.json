{"cell_type":{"5cf61410":"code","b7bad942":"code","c01b1288":"code","40795596":"code","d5dddce4":"code","e251c367":"code","bb669fac":"code","0ad5617d":"code","d5bd0bfe":"code","3bdea00a":"code","b950b49f":"code","790ff6a0":"code","89141f17":"code","d57c5366":"markdown","a40898da":"markdown","5bd7775e":"markdown","57073c58":"markdown"},"source":{"5cf61410":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b7bad942":"import spacy\nimport re\nimport string\nfrom spacy.lang.en import English\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport nltk\ndef getSentences(text):\n    nlp = English()\n    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n    document = nlp(text)\n    return [sent.string.strip() for sent in document.sents]\n\ndef printToken(token):\n    print(token.text, \"->\", token.dep_)\n    \ndef appendChunk(original, chunk):\n    return original + ' ' + chunk\ndef isRelationCandidate(token):\n    deps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\n    return any(subs in token.dep_ for subs in deps)\ndef isConstructionCandidate(token):\n    deps = [\"compound\", \"prep\", \"conj\", \"mod\"]\n    return any(subs in token.dep_ for subs in deps)\ndef processSubjectObjectPairs(tokens):\n    subject = ''\n    object = ''\n    relation = ''\n    subjectConstruction = ''\n    objectConstruction = ''\n    for token in tokens:\n        printToken(token)\n        if \"punct\" in token.dep_:\n            continue\n        if isRelationCandidate(token):\n            relation = appendChunk(relation, token.lemma_)\n        if isConstructionCandidate(token):\n            if subjectConstruction:\n                subjectConstruction = appendChunk(subjectConstruction, token.text)\n            if objectConstruction:\n                objectConstruction = appendChunk(objectConstruction, token.text)\n        if \"subj\" in token.dep_:\n            subject = appendChunk(subject, token.text)\n            subject = appendChunk(subjectConstruction, subject)\n            subjectConstruction = ''\n        if \"obj\" in token.dep_:\n            object = appendChunk(object, token.text)\n            object = appendChunk(objectConstruction, object)\n            objectConstruction = ''\n            \n    print (subject.strip(), \",\", relation.strip(), \",\", object.strip())\n    return (subject.strip(), relation.strip(), object.strip())\n\n\ndef processSentence(sentence):\n    tokens = nlp_model(sentence)\n    return processSubjectObjectPairs(tokens)","c01b1288":"\ndf_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', dtype={'id': np.int16, 'target': np.int8},nrows=100)\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', dtype={'id': np.int16},nrows=100)\n","40795596":"df_train.head()","d5dddce4":"def printGraph(triples):\n    G = nx.Graph()\n    for triple in triples:\n        G.add_node(triple[0])\n        G.add_node(triple[1])\n        G.add_node(triple[2])\n        G.add_edge(triple[0], triple[1])\n        G.add_edge(triple[1], triple[2])\n\n    pos = nx.spring_layout(G,k=2, iterations=50)\n    plt.figure(figsize=(10, 10))\n    nx.draw(G, pos, edge_color='black', width=1, linewidths=1,\n            node_size=5000, node_color='lightblue', alpha=0.9,font_size=10,\n            labels={node: node for node in G.nodes()})\n    plt.axis('off')\n    plt.show()","e251c367":"def clean_text(x):\n    text = re.sub('(\\d+)','',x)   \n    text = text.lower()\n    return text\ndef remove_url(x):\n    text = re.sub('(https?:\\\/\\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})\\\/([a-zA-Z0-9_]+]*)',' ',x)\n    return text\ndef remove_punct(x):\n    text_without_puct = [t for t in x if t not in string.punctuation]\n    text_without_puct = ''.join(text_without_puct)\n    return text_without_puct\nstop_words = nltk.corpus.stopwords.words('english')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndf_train['text'] = df_train['text'].apply(clean_text)\ndf_train['text'] = df_train['text'].apply(remove_url)\ndf_train['text'] = df_train['text'].apply(remove_punct)\ndf_train['text'] = df_train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","bb669fac":"#nlp = spacy.load('en')\n#df_train['text'] = df_train['text'].apply(nlp)\ntext = df_train['text'].tolist()\nread_1 = text[:10]\nread_1","0ad5617d":"if __name__ == \"__main__\":\n    \n    text = 'deeds reason earthquake may allah forgive us'\n\n    sentences = getSentences(text)\n    nlp_model = spacy.load('en_core_web_sm')\n\n    triples = []\n    print (text)\n    for sentence in sentences:\n        triples.append(processSentence(sentence))\n\n","d5bd0bfe":"printGraph(triples)","3bdea00a":"if __name__ == \"__main__\":\n    \n    text = 'people receive wildfires evacuation orders california'\n\n    sentences = getSentences(text)\n    nlp_model = spacy.load('en_core_web_sm')\n\n    triples = []\n    print (text)\n    for sentence in sentences:\n        triples.append(processSentence(sentence))\n\n","b950b49f":"printGraph(triples)","790ff6a0":"if __name__ == \"__main__\":\n    \n    text = 'flood disaster heavy rain causes flash flooding streets manitou colorado springs areas'\n\n    sentences = getSentences(text)\n    nlp_model = spacy.load('en_core_web_sm')\n\n    triples = []\n    print (text)\n    for sentence in sentences:\n        triples.append(processSentence(sentence))\n\n","89141f17":"printGraph(triples)","d57c5366":"## Information extraction and knowledge graphs\n\n* Information extraction is a technique of extracting structured information from unstructured text. This means taking a raw text(say an article) and processing it in such way that we can extract information from it in a format that a computer understands and can use. This is a very difficult problem in NLP because human language is so complex and lots of words can have a different meaning when we put it in a different context.\n\n* Information extraction consists of several, more focused subfields, each of them having difficult problems to solve. For example, here's an approach to using information extraction techniques for performing named entity recognition. Another subfield which has gained much interest from the community is keywords extraction.\n\n## Graph\n\n* A knowledge graph is a way of storing data that resulted from an information extraction task. Many basic implementations of knowledge graphs make use of a concept we call triple, that is a set of three items(a subject, a predicate and an object) that we can use to store information about something.","a40898da":"# **Objective : To build a small knowledge graph that will contain structured information extracted from unstructured text.**","5bd7775e":"Based on - https:\/\/programmerbackpack.com\/python-nlp-tutorial-information-extraction-and-knowledge-graphs\/","57073c58":"* Import our dependencies\n* Use spaCy to split the text in sentences\n* For each sentence, use spaCy to figure out what kind of word is every word in that sentence: is it a subject, an object, a predicate and so on\n* Use the information from above to figure out where in the triple we should put a word\n* Finally build the triples\n* Build and show the knowledge graph"}}