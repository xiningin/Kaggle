{"cell_type":{"a4eab666":"code","6da936a9":"code","f1d46062":"code","e27d2364":"code","236c2896":"code","c56bcfc3":"code","591be44b":"code","b7ab8469":"code","fc7bcc34":"code","c8e0a507":"code","30cebb9d":"code","4ed391e3":"code","61bf4e29":"code","bd2049e7":"code","967f121d":"code","cf215135":"code","4379c403":"code","b5916c8f":"code","3f78cef5":"code","e211be16":"code","28a15b69":"code","840ac27a":"code","19db4a59":"code","b9e5a84d":"code","e8598141":"markdown","02784a57":"markdown","e0c7d778":"markdown","ac4b1199":"markdown","6236386a":"markdown","0d6910b1":"markdown"},"source":{"a4eab666":"%matplotlib inline\nimport os\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport keras\nimport keras.backend as K\nfrom skimage.util.montage import montage2d\ndata_dir = os.path.join('..', 'input', 'eye-gaze')\nhelen_eye_dir = '..\/input\/getting-all-the-eye-balls\/'\nnorm_stack = lambda x: np.clip((x-127.0)\/127.0, -1, 1)\ndef norm_stack(x):\n    # calculate statistics on first 20 points\n    mean = np.mean(x[:20])\n    std = np.std(x[:20])\n    return (1.0*x-mean)\/(2*std)","6da936a9":"# load the data file and extract dimensions\nmontage_rgb = lambda x: np.clip(0.5*np.stack([montage2d(x[..., i]) for i in range(x.shape[-1])], -1)+0.5, 0, 1) \nwith h5py.File(os.path.join(helen_eye_dir,'eye_balls_rgb.h5'),'r') as t_file:\n    real_image_stack = norm_stack(t_file['image'].value)\nplt.imshow(montage_rgb(real_image_stack[0:16, :, :]))","f1d46062":"# load the data file and extract dimensions\nwith h5py.File(os.path.join(data_dir,'gaze.h5'),'r') as t_file:\n    print(list(t_file.keys()))\n    assert 'image' in t_file, \"Images are missing\"\n    assert 'look_vec' in t_file, \"Look vector is missing\"\n    look_vec = t_file['look_vec'].value\n    assert 'path' in t_file, \"Paths are missing\"\n    print('Images found:',len(t_file['image']))\n    for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n        print('image',ikey,'shape:',ival.shape)\n        img_width, img_height = ival.shape\n    syn_image_stack = norm_stack(np.expand_dims(np.stack([a for a in t_file['image'].values()],0), -1))\n    print(syn_image_stack.shape, 'loaded')\nplt.matshow(montage2d(syn_image_stack[0:9, :, :, 0]), cmap = 'gray')","e27d2364":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\nax1.hist(syn_image_stack[::10].ravel());\nax1.set_title('Synthetic Data')\nax2.hist(real_image_stack[::10].ravel());\nax2.set_title('Real Data')","236c2896":"from sklearn.model_selection import train_test_split\ntrain_X, test_X = train_test_split(syn_image_stack, \n                                   test_size = 0.25, \n                                   random_state = 2018)\ntrain_Y, test_Y = train_test_split(real_image_stack,\n                                   test_size = 0.25,\n                                   random_state = 2018)\nprint('Fake Images', train_X.shape, test_X.shape, train_X.max(), train_X.min(), train_X.mean(), train_X.std())\nprint('Real Images', train_Y.shape, test_Y.shape, train_Y.max(), train_Y.min(), train_Y.mean(), train_Y.std())","c56bcfc3":"from keras.layers import Input, concatenate, Conv2D, MaxPool2D, UpSampling2D, Flatten, Dense, Dropout, GaussianNoise, add, ZeroPadding2D, Cropping2D, Conv2DTranspose\nfrom keras import models, layers\nfrom collections import defaultdict\ngauss_noise_level = 1e-3\nleakiness = 0.1\ndef make_gen(depth=16, layer_count=2, use_dilation=False, use_add=False):\n    in_lay = Input(shape = (train_X.shape[1:4]), name = 'Generator_Input')\n    padding_size = ((2, 3), (2,3))\n    padding_size = ((6, 7), (4, 5))\n    gn = ZeroPadding2D(padding_size)(in_lay)\n    gn = GaussianNoise(gauss_noise_level)(gn)\n    c1 = Conv2D(depth, (3,3), padding = 'same')(gn)\n    out_layers = []\n    # dilation\n    if use_dilation:\n        for i in range(layer_count):\n            out_layers += [Conv2D(depth, (3,3), padding = 'same', dilation_rate=(2**i, 2**i))(c1)]\n            out_layers += [Conv2D(depth, (1,3), padding = 'same', dilation_rate=(1, 2**i))(c1)]\n        c2 = concatenate(out_layers)\n    else:\n        layer_db = defaultdict(lambda : [])\n        x = c1\n        layer_db[c1._keras_shape[1:3]] += [c1]\n        for i in range(layer_count):\n            x = Conv2D(depth*2**i, (3,3), padding = 'same', activation='linear')(x)\n            x = layers.BatchNormalization()(x)\n            x = layers.LeakyReLU(leakiness)(x)\n            x = MaxPool2D((2, 2))(x)\n            layer_db[x._keras_shape[1:3]] += [x]\n        for idx, i in enumerate(reversed(range(layer_count))):\n            if idx>0:\n                x = Conv2D(depth*2**i, (1,1), padding = 'same', activation='linear')(x)\n                x = layers.BatchNormalization()(x)\n                x = layers.LeakyReLU(leakiness)(x)\n            x = Conv2DTranspose(depth, (4, 4), strides = (2,2), padding = 'same')(x)\n            x = concatenate([x] + layer_db.get(x._keras_shape[1:3]))\n        c2 = x\n    \n    if use_add:\n        c_out = Conv2D(train_Y.shape[3], (1,1), padding = 'same', activation = 'tanh')(c2)\n        c_out = add([gn, c_out])\n    else:\n        c_out = Conv2D(train_Y.shape[3], (1,1), padding = 'same', activation = 'tanh')(c2)\n    c_out = Cropping2D(padding_size)(c_out)\n    \n    # try to make a grayscale downsampled image match the input well\n    ds_img_out = layers.Conv2D(1, (1, 1), strides=(1, 1), activation='tanh', padding='same')(c2)\n    ds_img_out = Cropping2D(padding_size)(ds_img_out)\n    ds_img_out = layers.AvgPool2D((2, 2), name='DS_Image')(ds_img_out)\n    \n    return models.Model(inputs = [in_lay], outputs = [c_out, ds_img_out], name = 'Generator')\n\ndef make_disc(depth=4, layer_count=3):\n    in_lay = Input(shape = (train_Y.shape[1:4]), name = 'Disc_Input')\n    gn = GaussianNoise(gauss_noise_level)(in_lay)\n    x = Conv2D(depth, (5,5), padding = 'valid', activation='linear')(gn)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(leakiness)(x)\n    for i in range(layer_count):\n        x = Conv2D(depth*2**i, (3,3), strides=(1, 1), padding = 'same', activation='linear')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.LeakyReLU(leakiness)(x)\n        x = Conv2D(depth*2**i, (3,3), strides=(2, 2), padding = 'same', activation='linear')(x)\n        x = layers.LeakyReLU(leakiness)(x)\n    \n    c_out = layers.concatenate([layers.GlobalMaxPool2D()(x), layers.GlobalAvgPool2D()(x)])\n    c_out = Dropout(0.5)(c_out)\n    c_out = Dense(2, activation = 'softmax')(c_out)\n    return models.Model(inputs = [in_lay], outputs = [c_out], name = 'Discriminator')","591be44b":"simple_gen = make_gen(64, layer_count=3)\nsimple_disc = make_disc(32)\ndef make_full(gen_mod, disc_model):\n    raw_img_in = Input(shape = (train_X.shape[1:4]), name = 'Image_In')\n    ref_img_out, ds_img_out = simple_gen(raw_img_in)\n    ref_disc_score = simple_disc(ref_img_out)\n    return models.Model(inputs=[raw_img_in], outputs=[ref_disc_score, ds_img_out]) \nfull_gen_model = make_full(simple_gen, simple_disc)","b7ab8469":"from IPython.display import Image\nfrom keras.utils.vis_utils import model_to_dot\nd = model_to_dot(simple_gen, show_shapes=True)\nd.set_rankdir('UD')\nImage(d.create_png())","fc7bcc34":"# show the discriminator\nImage(model_to_dot(simple_disc, show_shapes=True).create_png())","c8e0a507":"from keras.optimizers import Adam\nBASE_LR_RATE = 1e-3\n\ndef compile_generator(lr = BASE_LR_RATE): \n    simple_disc.trainable = False\n    full_gen_model.layers[-1].set_weights(simple_disc.get_weights())\n    full_gen_model.layers[-1].trainable = False\n    full_gen_model.compile(optimizer=Adam(lr=lr), \n                           loss = ['categorical_crossentropy', 'mean_absolute_error'],\n                           loss_weights = [1, 0.5],\n                           metrics = ['accuracy'])\n\ndef compile_discriminator(lr = BASE_LR_RATE): \n    simple_disc.trainable = True\n    simple_disc.compile(optimizer=Adam(lr=lr), \n                           loss = 'categorical_crossentropy', \n                           metrics = ['accuracy'])","30cebb9d":"compile_generator()\nfull_gen_model.summary()","4ed391e3":"compile_discriminator()\nsimple_disc.summary()","61bf4e29":"fake_score, f_img = full_gen_model.predict(train_X[0:2])\nprint(fake_score)\nplt.imshow(montage2d(f_img[:, :, :, 0]))","bd2049e7":"from keras.preprocessing.image import ImageDataGenerator\nfrom scipy.ndimage import zoom\ndg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 5, \n                  width_shift_range = 0.1, \n                  height_shift_range = 0.1, \n                  shear_range = 0.01,\n                  zoom_range = [0.8, 1.2],  \n                  horizontal_flip = True, \n                  vertical_flip = False,\n                  fill_mode = 'reflect',\n               data_format = 'channels_last')\n\nimage_gen = ImageDataGenerator(**dg_args)\ndef make_train_gen_batch(in_X, batch_size = 512):\n    # improve generator\n    for x in image_gen.flow(in_X, batch_size=batch_size):\n        out_vec = np.zeros((x.shape[0], 2))\n        out_vec[:, 1] = 1.0\n        yield x, [out_vec, zoom(x, [1, 1\/2.05, 1\/2.05, 1], order=1)]","967f121d":"gen_train = make_train_gen_batch(train_X)\ngen_valid = make_train_gen_batch(test_X)\na, (b, c) = next(gen_train)\nprint(a.shape, b.shape, c.shape)\nfig, (ax1) = plt.subplots(1, 1, figsize = (20, 10))\nax1.imshow(montage2d(a[:, :, :, 0]), cmap = 'bone')\nax1.set_title('Synth Images')","cf215135":"def show_status(seed = None, img_cnt = 9):\n    if seed is not None:\n        np.random.seed(seed)\n    syn_block = np.random.permutation(syn_image_stack)[0:img_cnt]\n    real_block = np.random.permutation(real_image_stack)[0:img_cnt]\n    bins = np.linspace(-1, 1, 30)\n    fig, ((ax1, ax2, ax3), (ax1h, ax2h, ax3h))  = plt.subplots(2, 3, figsize = (24, 12))\n    ax1.imshow(montage2d(syn_block[:, :, :, 0]), cmap = 'gray')\n    ax1h.hist(syn_block[:, :, :, 0].flatten(), bins)\n    ax1.set_title('Simulated Images')\n    gen_stack, _ = simple_gen.predict(syn_block)\n    ax2.imshow(montage_rgb(gen_stack[: , :, :]))\n    ax2h.hist(gen_stack[:, :, :, 0].flatten(), bins)\n    ax2.set_title('Generated Images\\nReal: %2.2f%%' % (np.mean(simple_disc.predict(gen_stack)[:, 1])*100))\n    ax3.imshow(montage_rgb(real_block[:, :, :]))\n    ax3h.hist(real_block[:, :, :, 0].flatten(), bins)\n    ax3.set_title('Real Images\\nReal: %2.2f%%' % (np.mean(simple_disc.predict(real_block)[:, 1])*100))\n    return fig\nshow_status();","4379c403":"def make_train_disc_batch(in_fake, in_real, batch_size = 256, refine_images=True):\n    \"\"\"we create batches consisting of a 50\/50 split between\n    fake and real images. The fake images are processed using the refiner (refine_images=True), but\n    in future we plan to provide fake images from many different generations of\n    the generator model to 'stabilize training'  \"\"\"\n    while True:\n        real_img = image_gen.flow(in_real, batch_size=batch_size)\n        fake_img = image_gen.flow(in_fake, batch_size=batch_size)\n        for (c_real, c_fake) in zip(real_img, fake_img):\n            real_cat = np.zeros((c_real.shape[0], 2))\n            real_cat[:, 1] = 1.0 # real\n            refined_cat = np.zeros((c_fake.shape[0], 2))\n            refined_cat[:, 0] = 1.0 # learn that they are fake\n\n            if refine_images:\n                c_refined, _ = simple_gen.predict(c_fake)\n            else:\n                c_fake = c_fake\n            yield np.concatenate([c_real, c_refined], 0), np.concatenate([real_cat, refined_cat])\ndisc_train = make_train_disc_batch(train_X, train_Y)\ndisc_valid = make_train_disc_batch(test_X, test_Y)","b5916c8f":"compile_discriminator()\nprint('Improving Discriminator')\nsimple_disc.fit_generator(disc_train, steps_per_epoch=100)","3f78cef5":"compile_generator()\nprint('Improving Generator')\nfull_gen_model.fit_generator(gen_train, steps_per_epoch=200)","e211be16":"show_status(2002, 25).savefig('pretraining_image_gen.png', dpi = 300)","28a15b69":"from IPython.display import clear_output, display\nt_steps = 25\nm_epochs = 2\nv_steps = 0\nepochs = 25\ntrain_history = []\nfrom keras.callbacks import EarlyStopping\nes_callback = lambda : EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=1)\nfor i in range(epochs):\n    cur_lr = BASE_LR_RATE*(0.8**(i))\n    # we might be required to precompute images at some point here\n    disc_train = make_train_disc_batch(train_X, train_Y)\n    disc_valid = make_train_disc_batch(test_X, test_Y)\n    compile_discriminator(cur_lr)\n    print('Improving Discriminator')\n    if v_steps>0:\n        v_args = dict(validation_data=disc_valid, validation_steps=v_steps)\n    else:\n        v_args = {}\n    train_history+=[\n        simple_disc.fit_generator(disc_train, steps_per_epoch=t_steps, epochs=m_epochs, **v_args)\n    ]\n    \n    plt.close('all')\n    clear_output()\n    display(show_status(2018, 9))\n    \n    print('Improving Generator ({:2.2g})'.format(cur_lr))\n    compile_generator(cur_lr)\n    if v_steps>0:\n        v_args = dict(validation_data=gen_valid, validation_steps=v_steps)\n    else:\n        v_args = {}\n    train_history+=[\n        full_gen_model.fit_generator(gen_train, \n                                 steps_per_epoch=t_steps, epochs=m_epochs, **v_args)\n    ]\n    display(show_status(2018, 9))","840ac27a":"from itertools import chain\ncomb_hist = [(np.ones((len(h.history['loss']),)), \n              np.power(-1*np.ones((len(h.history['loss']),)),i), \n              h.history['loss']) for i, h in enumerate(train_history)]\nepoch_vec = np.cumsum(list(chain(*[a for a, b, c in comb_hist])))\ncycle_vec = np.array(list(chain(*[b for a, b, c in comb_hist])))\nloss_vec = np.array(list(chain(*[c for a, b, c in comb_hist])))\nfig, (ax1) = plt.subplots(1, 1, figsize=(6, 3))\nax1.semilogy(epoch_vec, loss_vec)\nax1.semilogy(epoch_vec[cycle_vec==-1], loss_vec[cycle_vec==-1], 'r+', label='Generator')\nax1.legend()","19db4a59":"show_status(2002, 25).savefig('image_gen.png', dpi = 300)","b9e5a84d":"show_status(2003, 25);","e8598141":"# Build Models","02784a57":"# Overview\nThe notebook implements a simpler version of the model discussed in [Learning from Simulated and Unsupervised Images through Adversarial Training](https:\/\/arxiv.org\/abs\/1612.07828). \n### The initial focus is to \n- load the datasets correctly\n- create the refiner and discriminator models\n- use data augmentation on the real and fake images\n- train for a few epochs\n- use the simpler training approach\n\n### Training\n- Unity Images - $x$\n- Real images $y$\n- Refiner Model $\\mathcal{R}$\n- Discriminator Model $\\mathcal{D}$\n### Training Loop (one epoch)\n1. Improve Generator: minimize $-\\log(\\mathcal{D}(\\mathcal{R}(x)))+||\\mathcal{R}(x)-x||$ by updating parameters in $\\mathcal{R}$\n1. Improve Discriminator: maximize $-\\log(\\mathcal{D}(y)+\\log(1-\\mathcal{D}(\\mathcal{R}(x)))$ by updating parameters in $\\mathcal{D}$","e0c7d778":"# Prepare Training Data","ac4b1199":"# Load Real Data","6236386a":"# Load Synthetic Data\nGenerated using Unity and UnityEyes Tools","0d6910b1":"# Big Training\nHere we run a number of loops \n- improve the generator\n- improve the discriminator\n- decrease the learning rate of both\n- show results on fixed images\n- repeat"}}