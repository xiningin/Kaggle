{"cell_type":{"fabbf3d0":"code","f308db5a":"code","76b79ae9":"code","86cb52b7":"code","e136c949":"code","af5cb4bd":"code","67d7da5f":"code","c7f3ed8f":"code","bb5a594c":"code","e03bbba2":"code","e2080b3c":"code","84138e3e":"code","6a67ac49":"code","46369235":"code","b77ac9c4":"code","ad7fec22":"code","cb212a55":"code","e91367d0":"code","2d2af7f8":"code","062e1dbb":"code","8ee1155b":"code","abc0297b":"code","205dd722":"code","9537882e":"code","33c96060":"code","c2a68bf9":"code","381c24ba":"code","5f4db9e5":"code","dde7f37e":"code","240fd226":"code","eb2fe165":"code","9aae711d":"code","d71dc41d":"code","0b77a14a":"markdown","9a28bae1":"markdown","183daac1":"markdown","5d1d2c29":"markdown","70df2b43":"markdown","bfc779af":"markdown","569c17d6":"markdown","7bc12610":"markdown","0c457876":"markdown","448de271":"markdown","b72308d8":"markdown","98392bde":"markdown","105fb3cf":"markdown","366c7024":"markdown","fd8516f5":"markdown","0d2ce05e":"markdown"},"source":{"fabbf3d0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f308db5a":"np.random.seed(123)","76b79ae9":"N_TRIAL = 2000\nN_ARMS = 16\nN_FEATURE  = 5\nBEST_ARMS = [3, 7, 9, 15]","86cb52b7":"def make_design_matrix(n_trial, n_arms, n_feature):\n    available_arms = np.arange(n_arms)\n    X = np.array([[np.random.uniform(low=0, high=1, size=n_feature) for _ in available_arms] for _ in np.arange(n_trial)])\n    return X","e136c949":"D = make_design_matrix(N_TRIAL, N_ARMS, N_FEATURE)","af5cb4bd":"D.shape","67d7da5f":"def make_theta(n_arms, n_feature, best_arms, bias = 1):\n    true_theta = np.array([np.random.normal(size=n_feature, scale=1\/4) for _ in np.arange(n_arms)])\n    true_theta[best_arms] = true_theta[best_arms] + bias\n    return true_theta","c7f3ed8f":"theta = make_theta(N_ARMS, N_FEATURE, BEST_ARMS, bias=1)","bb5a594c":"def generate_reward(arm, x, theta, scale_noise = 1\/10):\n    signal = theta[arm].dot(x)\n    noise = np.random.normal(scale=scale_noise)\n    return (signal + noise)","e03bbba2":"def make_regret(payoff, oracle):\n    return np.cumsum(oracle - payoff)","e2080b3c":"X = make_design_matrix(n_trial=N_TRIAL, n_arms= N_ARMS, n_feature=N_FEATURE)\ntrue_theta = make_theta(n_arms = N_ARMS, n_feature=N_FEATURE, best_arms=BEST_ARMS)","84138e3e":"# Average reward\nave_reward = np.mean([[generate_reward(arm=arm, x=X[t, arm], theta= true_theta) for arm in np.arange(N_ARMS)] for t in np.arange(N_TRIAL)], axis=0)","6a67ac49":"ave_reward","46369235":"f, (left, right) = plt.subplots(1, 2, figsize=(15, 10))\nf.suptitle(t=\"Visualizing of simulated parameters: true theta and average reward\", fontsize=20)\n# True theta\nleft.matshow(true_theta)\nf.colorbar(left.imshow(true_theta), ax = left)\nleft.set_xlabel(\"feature number\")\nleft.set_ylabel(\"arm number\")\nleft.set_yticks(np.arange(N_ARMS))\nleft.set_title(\"True theta matrix\")\n# Average reward\nright.bar(np.arange(N_ARMS), ave_reward)\nright.set_title(\"Average reward per arm\")\nright.set_xlabel(\"arm number\")\nright.set_ylabel(\"average reward\")\nplt.show()","b77ac9c4":" A = np.array([np.diag(np.ones(shape=6)) for _ in np.arange(16)])","ad7fec22":"A","cb212a55":"b = np.array([np.zeros(shape=5) for _ in np.arange(16)])","e91367d0":"b","2d2af7f8":"def linUCB_disjoint(alpha, X, generate_reward, true_theta):\n    print(\"linUCB disjoint with exploration parameter alpha: \", alpha)\n    n_trial, n_arms, n_feature = X.shape\n    # 1. Initialize object\n    # 1.1. Output object\n    arm_choice = np.empty(n_trial) # store arm choice (integer) for each trial\n    r_payoff = np.empty(n_trial) # store payoff (float) for each trial\n    theta = np.empty(shape=(n_trial, n_arms, n_feature)) # record theta over each trial (n_arms, n_feature) per trial\n    p = np.empty(shape = (n_trial, n_arms)) # predictions for reward of each arm for each trial\n    # 1.2 Intermediate object\n    A = np.array([np.diag(np.ones(shape=n_feature)) for _ in np.arange(n_arms)])\n    b = np.array([np.zeros(shape=n_feature) for _ in np.arange(n_arms)])\n    # 2. Algo\n    for t in np.arange(n_trial):\n        # Compute estimates (theta) and prediction (p) for all arms\n        for a in np.arange(n_arms):\n            inv_A = np.linalg.inv(A[a])\n            theta[t, a] = inv_A.dot(b[a])\n            p[t, a] = theta[t, a].dot(X[t, a]) + alpha * np.sqrt(X[t, a].dot(inv_A).dot(X[t, a]))\n        # Choosing best arms\n        chosen_arm = np.argmax(p[t])\n        x_chosen_arm = X[t, chosen_arm]\n        r_payoff[t] = generate_reward(arm=chosen_arm, x=x_chosen_arm, theta=true_theta)\n\n        arm_choice[t] = chosen_arm\n        \n        # update intermediate objects (A and b)\n        A[chosen_arm] += np.outer(x_chosen_arm, x_chosen_arm.T)\n        b[chosen_arm] += r_payoff[t]*x_chosen_arm\n    return dict(theta=theta, p=p, arm_choice=arm_choice, r_payoff=r_payoff)","062e1dbb":"oracle = np.array([np.max([generate_reward(arm=arm, x=X[t, arm], theta = true_theta) for arm in np.arange(N_ARMS)]) for t in np.arange(N_TRIAL)])","8ee1155b":"len(oracle)","abc0297b":"oracle[0]","205dd722":"payoff_random = np.array([generate_reward(arm=np.random.choice(N_ARMS), x= X[t, np.random.choice(N_ARMS)], theta = true_theta) for t in np.arange(X.shape[0])])","9537882e":"payoff_random","33c96060":"regret_random = make_regret(payoff=payoff_random, oracle=oracle)","c2a68bf9":"alpha_to_test = [0, 1, 2.5, 5, 10, 20]\nresults_dict = {alpha: linUCB_disjoint(alpha=alpha, X=X, generate_reward=generate_reward, true_theta = true_theta) for alpha in alpha_to_test}","381c24ba":"def plot_regrets(results, oracle):\n    [plt.plot(make_regret(payoff=x['r_payoff'], oracle=oracle), label=\"alpha: \"+str(alpha)) for (alpha, x) in results.items()]","5f4db9e5":"def plot_estimates(x, alpha, true_theta=None, abs_ylim = None, ncol = 4):\n    print(\"Estimates plot for alpha: \", alpha)\n    if true_theta is not None:\n        print(\"Parameter true_theta has been supplied. Plotting convergence\")\n    for i, arm in enumerate(np.arange(N_ARMS)):\n        plt.subplot(np.ceil(N_ARMS\/ncol), ncol, 1+i)\n        if true_theta is not None:\n            data_to_plot = pd.DataFrame(x[alpha][\"theta\"][:, arm, :]) - true_theta[arm]\n        else:\n            data_to_plot = pd.DataFrame(x[alpha][\"theta\"][:, arm, ])\n        plt.plot(data_to_plot)\n        \n        if (arm in BEST_ARMS):\n            title = 'Arm: ' + str(arm) + \" (best)\"\n        else:\n            title = \"Arm: \" + str(arm)\n        plt.title(title)\n        \n        if abs_ylim is not None:\n            plt.ylim([-abs_ylim, abs_ylim])\n    plt.legend([\"c\"+str(feature) for feature in np.arange(N_FEATURE)])","dde7f37e":"def plot_selected_arms(x, bar_width=0.15):\n    for (i, alpha) in enumerate(x):\n        xi, yi = np.unique(x[alpha][\"arm_choice\"], return_counts=True)\n        plt.bar(xi + i*bar_width, yi, label=\"alpha: \" + str(alpha), width=bar_width)\n    \n    plt.xticks(np.arange(N_ARMS) + round(len(x)\/2)*bar_width, np.arange(N_ARMS))\n    plt.legend()","240fd226":"plt.figure(figsize=(12.5, 7.5))\nplot_regrets(results_dict, oracle)\nplt.plot(make_regret(payoff=payoff_random, oracle=oracle), label = \"random\", linestyle='--')\nplt.legend()\nplt.title(\"Regrets for various levels of alpha\")\nplt.show()","eb2fe165":"plt.figure(figsize=(12.5, 17.5))\nplot_estimates(results_dict, alpha=2.5, true_theta = true_theta, abs_ylim=3\/4)","9aae711d":"plt.figure(figsize=(12.5, 17.5))\nplot_estimates(results_dict, alpha=20, true_theta=true_theta, abs_ylim=3\/4)","d71dc41d":"plt.figure(figsize=(15, 5))\nplot_selected_arms(results_dict)","0b77a14a":"Learned from: A Contextual-Bandit Approach to Personalized News Article Recommendation (paper from Li et al.)\n## Problem formulation\n\nA contextual bandit A proceeds in discrete trials t = 1, 2, 3, ... In trial t\n1. The algorithm observes current user $u_t$ and a set $A_t$ of arms (actions). These two made up $x_{t, a}$ for every $a \\in A_t$, and is called the $context$.\n2. Based on the observed payoffs in previous trials, $A$ chooses an arm $a_t \\in A_t$, and receives a payoff $r_{t,a_t}$ whose expectation depends on both the user $u_t$ and the arm $a_t$.\n3. $A$ improves arm-selection strategy with the new observation $(x_{t, a_t}, a_t, r_{t, a_t})$.\n4. The total T-trial payoff of $A$ is defined as $\\sum_{t=1}^{T}r_{t, a_t}$\n5. Optimal expected T-trial payoff is defined as $E[\\sum_{t=1}^Tr_{t, a_t^*}]$, where $a_t^*$ is the arm with maximum expected payoff at trial $t$.\n6. The goal for $A$ is to maximize payoff or minimize the regret, which is defined as:\n\\begin{equation}\n    R_A(T) = E[\\sum_{t=1}^Tr_{t, a_t^*}] - E[\\sum_{t=1}^Tr_{t, a_t}]\n\\end{equation}\n\n## The algorithm\nAssume that the payoff of an arm $a$ is linear in its d-dimensional feature x_{t, a} with some unknown coefficient vector $\\theta_a^*$:\n\\begin{equation}\nE[r_{t, a} | x_{t, a}] = x_{t, a}^T\\theta_a^*\n\\end{equation}\n\nLet $D_a$ be a design matrix of dimension $m\\times d$ at trial $t$, $m$ is the training inputs (e.g., m contexts that are observed previously for article a), and $c_a \\in R^m$ be the corresponding response vector (e.g., the corresponding $m$ click\/no-click user feedback). Applying ridge regression to the training data ($D_a$, $c_a$) gives an estimate of the coefficients:\n\n\\begin{equation}\n\\hat\\theta_a = (D_a^TD_a + I_d)^{-1}D_a^Tc_a\n\\end{equation}\n\nIt can be proved that, with probability at least $1-\\delta$:\n\\begin{equation}\n|x_{t, a}^T\\hat\\theta_a - E[r_{t, a}|x_{t, a}]| \\leq \\alpha \\sqrt{x_{t, a}^T(D_a^TDa + I_d)^{-1}x_{t, a}}\n\\end{equation}\nfor any $\\delta>0$ and $x_{t, a} \\in R^d$, where $\\alpha = \\sqrt{ln(2\/\\delta)\/2}$ is a constant.\n\nIn other words, the inequality above gives a reasonably tight UCB for the expected payoff of arm a, from which a UCB-type arm-selection strategy can be derived: at each trial t, choose:\n\\begin{equation}\na_t = arg\\,\\underset{a\\in A_t}{max}(x_{t, a}^T + \\alpha\\sqrt{x_{t, a}^T A_a^{-1}x_{t, a}})\n\\end{equation}\nwhere $A_a = D_a^TD_a + I_d$\n\nAlso, the expected payoff $x_{t, a}^T\\theta_a^*$ can be calculated as $x_{t, a}^TA_a^{-1}$, and then $\\sqrt{x_{t, a}^TA_a^{-1}x_{t, a}}$ becomes the standard deviation.\n\nTherefore the upper bound should be:\n\\begin{equation}\n    \\hat\\theta_{t, a} = A_a^{-1}b_a \\\\\n    p_{t, a} = \\hat\\theta_{t, a}x_{t, a} + \\alpha*\\sqrt{x_{t, a}A^{-1}x_{t, a}}\n\\end{equation}","9a28bae1":"### Algorithm testing for various alpha","183daac1":"#### Algorithm definition","5d1d2c29":"Low exploration parameter (alpha = 1) prevents the algorithm from visiting highly rentable arm (the arm 15 for instnace). No exploration causes the algorithm to get stuck in the first highly rentable arm found.","70df2b43":"### Simulation of design matrix and weight vector (theta)","bfc779af":"For each arm, feature vector is i.i.d and simulated according to U(\\[0, 1\\]).\n\nTheta vector is simulated according to $N(0_d, cI_d)$ with $c=1\/4$.\n\nHighly profitable arms are created adding positive bias to $\\theta$. These arms are defined by the constant BEST_ARMS.\n\nBy hypothesis, the expected payoff is linear in its feature vector $x_{t, a}$ with some unknown coefficient vector $\\theta^*_a$:\n\n\\begin{equation*}\nE[r_{t, a}|x_{t, a}] = x_{t, a}^T\\theta^*_a\n\\end{equation*}\n\nRegret is defined as $R_A(T) = E[\\sum_{t=1}^T r_{t, a_t^*}] - E[\\sum_{t=1}^T r_{t, a_t}]$\n","569c17d6":"## 2. Algorithm (disjoint version)","7bc12610":"# Contextual Bandit\nLearned from: https:\/\/github.com\/etiennekintzler\/bandits_algorithm\/blob\/master\/linUCB.ipynb","0c457876":"### Defining constants","448de271":"### Defining oracle and random payoff","b72308d8":"### Function definition","98392bde":"## 3. Analyzing regrets, coefficients estimates and chosen arm","105fb3cf":"### Graphical representation of average payoff per arm","366c7024":"## 3.1. Representing regret according to various level of exploration value","fd8516f5":"## 1. Problem setting\n### Setting problems parameters","0d2ce05e":"On average, the reward is much higher for the best arms. In this example, the arm 9 is the most profitable.\n\nShould note on whether the features are positive? Because if there are negative features high positive theta will lead to low rewards."}}