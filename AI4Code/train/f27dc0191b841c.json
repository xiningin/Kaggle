{"cell_type":{"b0146be3":"code","97dd3b0e":"code","f23fe0ea":"code","8ca58bf7":"code","81b01e47":"code","9a8cc1fb":"code","a1f586be":"code","00445096":"code","7c453c46":"code","b90aa614":"code","75557665":"code","388f8b82":"code","34490f4d":"code","7cd438da":"code","9b7cf9d4":"code","901089d1":"code","c388b9c8":"code","1542741b":"code","48042efd":"code","ecc41c28":"code","7776080d":"code","81b0dd31":"code","b4c5bcd9":"code","3f270dcf":"code","863cf2dd":"code","54badcc8":"code","ddd78ee4":"code","27c88fac":"code","92072a61":"code","e2222f72":"code","2450ad07":"code","b7650d4d":"code","cd332f9e":"code","46faa788":"code","e01e3cc6":"code","2a497fad":"code","501a4061":"code","b7816fb6":"code","a937a5b2":"code","c53dab61":"code","56f32af6":"code","1af8f020":"code","83b39163":"code","920c2471":"code","6b91885d":"code","aac52a64":"code","a75de104":"code","a58f05f5":"code","0c4d6e04":"code","0710e728":"code","95d60c3a":"code","04212ab0":"code","905315e9":"code","282a1875":"code","cbfa9798":"code","7e98f0f3":"code","acae11c7":"code","34ace70d":"code","5ad44239":"code","890f62fc":"code","109bf553":"code","c89baccd":"code","f20963e1":"code","e05e13b3":"code","ca4eb62b":"code","ae8ee838":"code","3a1cab8c":"code","77663abb":"code","225f050b":"code","25774f47":"code","d8c67c30":"code","e51d729e":"code","88af75a3":"code","fbf001cb":"code","50b331a9":"code","4034c465":"code","740568e2":"code","730ff9df":"code","c615f3dd":"code","e437734c":"code","37f4233f":"code","dc4a41b2":"code","09bb0c71":"code","c7a099cd":"code","4f4260a9":"code","05f80859":"code","8a2fce4a":"code","f3f02fd3":"code","17b2e54b":"code","69c57786":"code","2a18153e":"code","5dd34af8":"code","1f257e49":"code","30e8b6eb":"code","6bd70c6a":"code","9d74d053":"code","91ee658e":"code","f20d8bc2":"code","4242d87c":"code","afff8215":"code","e6d21e67":"code","306bbf63":"code","4536b10d":"code","5944a086":"code","7241323b":"code","ab306325":"code","20a5e90f":"code","f2883d4d":"code","2307f289":"code","abacd261":"code","374902a1":"code","0ea208af":"code","e95c5a03":"code","a70517c2":"markdown","d156c863":"markdown","f04062eb":"markdown","4c1850a3":"markdown","baa870a8":"markdown","6eac0258":"markdown","6686f489":"markdown","75a75b95":"markdown","efef23db":"markdown","b58ad6ca":"markdown","0aad0025":"markdown","73ff290d":"markdown","0405d714":"markdown","8e018433":"markdown","68394c6c":"markdown","2324313f":"markdown","92501a9c":"markdown","edc34202":"markdown","72ae808c":"markdown","036dcc28":"markdown","e8c4b57f":"markdown","3f396388":"markdown","9544bf15":"markdown","bcd2169d":"markdown","4caa2ade":"markdown","def491a3":"markdown","3ef90735":"markdown","505dc6d4":"markdown","66a5dd69":"markdown","df913635":"markdown","dfe4c05d":"markdown","beaa0877":"markdown","3c6358c8":"markdown","66e1bf29":"markdown","289fd97a":"markdown","a800f0fa":"markdown","877f0e91":"markdown","251606bf":"markdown","48a20ba9":"markdown","7751daa3":"markdown","5af872dc":"markdown","e03b5662":"markdown","fce02e97":"markdown","3e3f0ba7":"markdown","0fee93e9":"markdown","93843933":"markdown","9f3f751d":"markdown","b5a8b5e7":"markdown","1981bddc":"markdown","b0bfd633":"markdown","c1c1deb1":"markdown","b74a21b2":"markdown","4c48bda5":"markdown","247aa3b5":"markdown","fdb50920":"markdown","9f98c4f0":"markdown","7ee389a1":"markdown","0bfbaf91":"markdown","de465e33":"markdown","cbf69d2d":"markdown","0f949d72":"markdown","c622cb0a":"markdown","62dc0cbe":"markdown","34295069":"markdown","c12aca5b":"markdown","0dae6a85":"markdown","84f6eb8b":"markdown","994503aa":"markdown","1d1dcfda":"markdown","01625429":"markdown","e15c01e7":"markdown","3b7cf1c6":"markdown","8c73eebb":"markdown","0ad8934c":"markdown","7dc94005":"markdown","b0dc5601":"markdown"},"source":{"b0146be3":"# from google.colab import drive\n# drive.mount('\/content\/drive')","97dd3b0e":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n\n# from plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()\nfrom collections import Counter\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(\"DONE ---------------------------------------\")","f23fe0ea":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8ca58bf7":"# project_data = pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/train_data.csv') \n# resource_data = pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/resources.csv')\nproject_data = pd.read_csv('\/kaggle\/input\/donorschoosedataset\/train_data.csv') \nresource_data = pd.read_csv('\/kaggle\/input\/donorschoosedataset\/resources.csv')\n# project_data = pd.read_csv('..\/train_data.csv') \n# resource_data = pd.read_csv('..\/resources.csv')\n\nprint(\"Done\")","81b01e47":"project_data.shape","9a8cc1fb":"project_data.columns.values","a1f586be":"prefixlist=project_data['teacher_prefix'].values\nprefixlist=list(prefixlist)\ncleanedPrefixList = [x for x in project_data['teacher_prefix'] if x != float('nan')] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\nlen(cleanedPrefixList)\n# print(len(prefixlist))","00445096":"## Converting to Nan and Droping -> https:\/\/stackoverflow.com\/a\/29314880\/4433839\n\n# df[df['B'].str.strip().astype(bool)] \/\/ for deleting EMPTY STRINGS.\nproject_data.dropna(subset=['teacher_prefix'], inplace=True)\nproject_data.shape","7c453c46":"project_data['teacher_prefix'].head(10)","b90aa614":"print(\"Number of data points in train data\", resource_data.shape)\nprint(resource_data.columns.values)\nresource_data.head(2)","75557665":"catogories = list(project_data['project_subject_categories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\ncat_list = []\nfor i in catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_') # we are replacing the & value into \n    cat_list.append(temp.strip())\n    \nproject_data['clean_categories'] = cat_list\nproject_data.drop(['project_subject_categories'], axis=1, inplace=True)\n\nfrom collections import Counter\nmy_counter = Counter()\nfor word in project_data['clean_categories'].values:\n    my_counter.update(word.split())\n\ncat_dict = dict(my_counter)\nsorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")\n","388f8b82":"sub_catogories = list(project_data['project_subject_subcategories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\n\nsub_cat_list = []\nfor i in sub_catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_')\n    sub_cat_list.append(temp.strip())\n\nproject_data['clean_subcategories'] = sub_cat_list\nproject_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\n\n# count of all the words in corpus python: https:\/\/stackoverflow.com\/a\/22898595\/4084039\nmy_counter = Counter()\nfor word in project_data['clean_subcategories'].values:\n    my_counter.update(word.split())\n    \nsub_cat_dict = dict(my_counter)\nsorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")","34490f4d":"# this code removes \" \" and \"-\". ie Grades 3-5 -> grage3to5\n#  remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\nclean_grades=[]\nfor project_grade in project_data['project_grade_category'].values:\n    project_grade=str(project_grade).lower().strip().replace(' ','').replace('-','to')\n    \n    clean_grades.append(project_grade.strip())\n\nproject_data['clean_project_grade_category']=clean_grades\nproject_data.drop(['project_grade_category'],axis=1,inplace=True)\n\nmy_counter = Counter()\nfor word in project_data['clean_project_grade_category'].values:\n    my_counter.update(word.split())\n    \ngrade_dict = dict(my_counter)\nsorted_project_grade_cat_dict = dict(sorted(grade_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")\n","7cd438da":"# merge two column text dataframe: \nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","9b7cf9d4":"project_data.head(2)","901089d1":"#### Text PreProcessing Functions","c388b9c8":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","1542741b":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","48042efd":"# Combining all the above stundents \nfrom tqdm import tqdm\npreprocessed_essays = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['essay'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n    preprocessed_essays.append(sent.lower().strip())","ecc41c28":"project_data.columns","7776080d":"## new column added as PreProcessed_Essays and older unProcessed essays column is deleted\nproject_data['preprocessed_essays'] = preprocessed_essays\nproject_data.drop(['essay'], axis=1, inplace=True)","81b0dd31":"project_data.columns","b4c5bcd9":"# after preprocesing\npreprocessed_essays[20000]","3f270dcf":"# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\nfrom tqdm import tqdm\npreprocessed_titles = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['project_title'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_titles.append(sent.lower().strip())","863cf2dd":"#https:\/\/stackoverflow.com\/questions\/26666919\/add-column-in-dataframe-from-list\/3849072\nproject_data['preprocessed_titles'] = preprocessed_titles\n# project_data.drop(['project_title'], axis=1, inplace=True)","54badcc8":"project_data.columns","ddd78ee4":"project_data.shape","27c88fac":"price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nprice_data.head(2)","92072a61":"project_data = pd.merge(project_data, price_data, on='id', how='left')","e2222f72":"project_data.shape","2450ad07":"project_bkp=project_data.copy()","b7650d4d":"project_bkp.shape","cd332f9e":"## taking random samples of 100k datapoints\nproject_data = project_bkp.sample(n = 100000) \n# resource_data = pd.read_csv('..\/resources.csv')\n\nproject_data.shape\n\n# y_value_counts = row1['project_is_approved'].value_counts()\ny_value_counts = project_data['project_is_approved'].value_counts()\nprint(\"Number of projects thar are approved for funding:     \", y_value_counts[1],\" -> \",round(y_value_counts[1]\/(y_value_counts[1]+y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects thar are not approved for funding: \", y_value_counts[0],\" -> \",round(y_value_counts[0]\/(y_value_counts[1]+y_value_counts[0])*100,2),\"%\")","46faa788":"# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test=train_test_split(\n#     project_data.drop('project_is_approved', axis=1),\n#     project_data['project_is_approved'].values,\n#     test_size=0.3,\n#     random_state=42,\n#     stratify=project_data[['project_is_approved']])\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(\n    project_data,\n    project_data['project_is_approved'],\n    test_size=0.2,\n    random_state=42,\n    stratify=project_data[['project_is_approved']])\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test : \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test : \",y_test.shape)","e01e3cc6":"# x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.25, stratify=y_train)\n\n# print(\"x_train: \",x_train.shape)\n# print(\"y_train: \",y_train.shape)\n# print(\"x_cv   : \",x_cv.shape)\n# print(\"x_cv   : \",y_cv.shape)\n# print(\"x_test : \",x_test.shape)\n# print(\"y_test : \",y_test.shape)","2a497fad":"# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_TRAIN-------------------------\")\nx_train_y_value_counts = x_train['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_train_y_value_counts[1],\" -> \",round(x_train_y_value_counts[1]\/(x_train_y_value_counts[1]+x_train_y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects that are not approved for funding \",x_train_y_value_counts[0],\" -> \",round(x_train_y_value_counts[0]\/(x_train_y_value_counts[1]+x_train_y_value_counts[0])*100,2),\"%\")\nprint(\"\\n\")\n# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_TEST--------------------------\")\nx_test_y_value_counts = x_test['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_test_y_value_counts[1],\" -> \",round(x_test_y_value_counts[1]\/(x_test_y_value_counts[1]+x_test_y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects that are not approved for funding \",x_test_y_value_counts[0],\" -> \",round(x_test_y_value_counts[0]\/(x_test_y_value_counts[1]+x_test_y_value_counts[0])*100,2),\"%\")\nprint(\"\\n\")\n# y_value_counts = row1['project_is_approved'].value_counts()\n# print(\"X_CV----------------------------\")\n# x_cv_y_value_counts = x_cv['project_is_approved'].value_counts()\n# print(\"Number of projects that are approved for funding    \", x_cv_y_value_counts[1],\" -> \",round(x_cv_y_value_counts[1]\/(x_cv_y_value_counts[1]+x_cv_y_value_counts[0])*100),2,\"%\")\n# print(\"Number of projects that are not approved for funding \",x_cv_y_value_counts[0],\" -> \",round(x_cv_y_value_counts[0]\/(x_cv_y_value_counts[1]+x_cv_y_value_counts[0])*100),2,\"%\")\n# print(\"\\n\")","501a4061":"# from sklearn.utils import resample\n\n# ## splitting x_train in their respective classes\n# x_train_majority=x_train[x_train.project_is_approved==1]\n# x_train_minority=x_train[x_train.project_is_approved==0]\n\n# print(\"No. of points in the Training Dataset : \", x_train.shape)\n# print(\"No. of points in the majority class 1 : \",len(x_train_majority))\n# print(\"No. of points in the minority class 0 : \",len(x_train_minority))\n\n# print(x_train_majority.shape)\n# print(x_train_minority.shape)\n\n# ## Resampling with replacement\n# x_train_minority_upsampled=resample(\n#     x_train_minority,\n#     replace=True,\n\n#     n_samples=len(x_train_majority),\n#     random_state=123)\n\n# print(\"Resampled Minority class details\")\n# print(\"Type:  \",type(x_train_minority_upsampled))\n# print(\"Shape: \",x_train_minority_upsampled.shape)\n# print(\"\\n\")\n# ## Concatinating our Upsampled Minority class with the existing Majority class\n# x_train_upsampled=pd.concat([x_train_majority,x_train_minority_upsampled])\n\n# print(\"Upsampled Training data\")\n# print(\"Total number of Class labels\")\n# print(x_train_upsampled.project_is_approved.value_counts())\n# print(\"\\n\")\n# print(\"Old Training IMBALANCED Dataset Shape         : \", x_train.shape)\n# print(\"New Training BALANCED Upsampled Dataset Shape : \",x_train_upsampled.shape)\n\n# x_train_upsampled.to_csv ('x_train_upsampled_csv.csv',index=False)","b7816fb6":"# yy_train=x_train_upsampled['project_is_approved'].copy()","a937a5b2":"# yy_train.shape","c53dab61":"# x_train_upsampled.shape","56f32af6":"x_train.shape","1af8f020":"x_test.shape","83b39163":"# x_cv.shape","920c2471":"project_data.columns","6b91885d":"# we use count vectorizer to convert the values into one \nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer_sub = CountVectorizer(vocabulary=list(sorted_cat_dict.keys()), lowercase=False, binary=True)\n\nvectorizer_sub.fit(x_train['clean_categories'].values)\n\nx_train_categories_one_hot = vectorizer_sub.transform(x_train['clean_categories'].values)\n# x_cv_categories_one_hot    = vectorizer_sub.transform(x_cv['clean_categories'].values)\nx_test_categories_one_hot  = vectorizer_sub.transform(x_test['clean_categories'].values)\n\n\nprint(vectorizer_sub.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> categories: x_train: \",x_train_categories_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> categories: x_cv   : \",x_cv_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> categories: x_test : \",x_test_categories_one_hot.shape)","aac52a64":"# we use count vectorizer to convert the values into one \nvectorizer_sub_sub = CountVectorizer(vocabulary=list(sorted_sub_cat_dict.keys()), lowercase=False, binary=True)\n\nvectorizer_sub_sub.fit(x_train['clean_subcategories'].values)\n\nx_train_sub_categories_one_hot = vectorizer_sub_sub.transform(x_train['clean_subcategories'].values)\n# x_cv_sub_categories_one_hot    = vectorizer_sub_sub.transform(x_cv['clean_subcategories'].values)\nx_test_sub_categories_one_hot  = vectorizer_sub_sub.transform(x_test['clean_subcategories'].values)\n\nprint(vectorizer_sub_sub.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_train: \",x_train_sub_categories_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> sub_categories: x_cv   : \",x_cv_sub_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_test : \",x_test_sub_categories_one_hot.shape)","a75de104":"my_counter = Counter()\nfor state in project_data['school_state'].values:\n    my_counter.update(state.split())","a58f05f5":"school_state_cat_dict = dict(my_counter)\nsorted_school_state_cat_dict = dict(sorted(school_state_cat_dict.items(), key=lambda kv: kv[1]))","0c4d6e04":"from scipy import sparse ## Exporting Sparse Matrix to NPZ File -> https:\/\/stackoverflow.com\/questions\/8955448\/save-load-scipy-sparse-csr-matrix-in-portable-data-format\nstatelist=list(project_data['school_state'].values)\nvectorizer_state = CountVectorizer(vocabulary=set(statelist), lowercase=False, binary=True)\n\nvectorizer_state.fit(x_train['school_state'])\n\nx_train_school_state_one_hot = vectorizer_state.transform(x_train['school_state'].values)\n# x_cv_school_state_one_hot    = vectorizer_state.transform(x_cv['school_state'].values)\nx_test_school_state_one_hot  = vectorizer_state.transform(x_test['school_state'].values)\n\nprint(vectorizer_state.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> school_state: x_train: \",x_train_school_state_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> school_state: x_cv   : \",x_cv_school_state_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> school_state: x_test : \",x_test_school_state_one_hot.shape)\n# school_one_hot = vectorizer.transform(statelist)\n# print(\"Shape of matrix after one hot encodig \",school_one_hot.shape)\n# print(type(school_one_hot))\n# sparse.save_npz(\"school_one_hot_export.npz\", school_one_hot) \n# print(school_one_hot.toarray())","0710e728":"# prefixlist=project_data['teacher_prefix'].values\n# prefixlist=list(prefixlist)\n# cleanedPrefixList = [x for x in prefixlist if x == x] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\n# ## preprocessing the prefix to remove the SPACES,- else the vectors will be just 0's. Try adding - and see\n# prefix_nospace_list = []\n# for i in cleanedPrefixList:\n#     temp = \"\"\n#     i = i.replace('.','') # we are placeing all the '.'(dot) with ''(empty) ex:\"Mr.\"=>\"Mr\"\n#     temp +=i.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n#     prefix_nospace_list.append(temp.strip())\n\n# cleanedPrefixList=prefix_nospace_list\n\n# vectorizer = CountVectorizer(vocabulary=set(cleanedPrefixList), lowercase=False, binary=True)\n# vectorizer.fit(cleanedPrefixList)\n# print(vectorizer.get_feature_names())\n# prefix_one_hot = vectorizer.transform(cleanedPrefixList)\n# print(\"Shape of matrix after one hot encodig \",prefix_one_hot.shape)\n# prefix_one_hot_ar=prefix_one_hot.todense()\n\n# ##code to export to csv -> https:\/\/stackoverflow.com\/a\/54637996\/4433839\n# # prefixcsv=pd.DataFrame(prefix_one_hot.toarray())\n# # prefixcsv.to_csv('prefix.csv', index=None,header=None)","95d60c3a":"my_counter = Counter()\nfor teacher_prefix in project_data['teacher_prefix'].values:\n    teacher_prefix = str(teacher_prefix).lower().replace('.','').strip()\n    \n    my_counter.update(teacher_prefix.split())\nteacher_prefix_cat_dict = dict(my_counter)\nsorted_teacher_prefix_cat_dict = dict(sorted(teacher_prefix_cat_dict.items(), key=lambda kv: kv[1]))","04212ab0":"sorted_teacher_prefix_cat_dict.keys()","905315e9":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer_prefix = CountVectorizer(vocabulary=list(sorted_teacher_prefix_cat_dict.keys()), lowercase=False, binary=True)\n\nvectorizer_prefix.fit(x_train['teacher_prefix'].values)\n\nx_train_prefix_one_hot = vectorizer_prefix.transform(x_train['teacher_prefix'].values)\n# x_cv_prefix_one_hot    = vectorizer_prefix.transform(x_cv['teacher_prefix'].values)\nx_test_prefix_one_hot  = vectorizer_prefix.transform(x_test['teacher_prefix'].values)\n\nprint(vectorizer_prefix.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> prefix: x_train: \",x_train_prefix_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> prefix: x_test : \",x_test_prefix_one_hot.shape)","282a1875":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer_grade = CountVectorizer(vocabulary=list(sorted_project_grade_cat_dict.keys()), lowercase=False, binary=True)\n\nvectorizer_grade.fit(x_train['clean_project_grade_category'].values)\n\nx_train_grade_category_one_hot = vectorizer_grade.transform(x_train['clean_project_grade_category'].values)\n# x_cv_grade_category_one_hot    = vectorizer_grade.transform(x_cv['clean_project_grade_category'].values)\nx_test_grade_category_one_hot  = vectorizer_grade.transform(x_test['clean_project_grade_category'].values)\n\nprint(vectorizer_grade.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_train : \",x_train_grade_category_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> project_grade: x_cv    : \",x_cv_grade_category_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_test  : \",x_test_grade_category_one_hot.shape)\n","cbfa9798":"type(x_train_grade_category_one_hot)","7e98f0f3":"x_train_grade_category_one_hot.toarray()","acae11c7":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['price'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_price_normalized = normalizer.transform(x_train['price'].values.reshape(1, -1)).reshape(-1,1)\nx_test_price_normalized  = normalizer.transform(x_test['price'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\nprint(\"Shape of matrix after normalization -> Teacher Prefix: x_train: \",x_train_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\nprint(\"Shape of matrix after normalization -> Teacher Prefix: x_test : \",x_test_prefix_one_hot.shape)","34ace70d":"type(x_train_price_normalized)","5ad44239":"x_train_price_normalized","890f62fc":"# # check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# # standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# # from sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import Normalizer\n\n# # price_standardized = standardScalar.fit(project_data['price'].values)\n# # this will rise the error\n# # ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# # Reshape your data either using array.reshape(-1, 1)\n# # transformer = Normalizer().fit(X)\n# normalizer = Normalizer()\n\n# normalizer.fit(x_train_upsampled['price'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n# # print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# # Now normalize the data\n\n# x_train_price_normalized = normalizer.transform(x_train_upsampled['price'].values.reshape(-1, 1))\n# x_test_price_normalized  = normalizer.transform(x_test['price'].values.reshape(-1, 1))\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(-1, 1))\n\n\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_train: \",x_train_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_test : \",x_test_prefix_one_hot.shape)","109bf553":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nteacher_previous_proj_normalizer = Normalizer()\n# normalizer = Normalizer()\n\nteacher_previous_proj_normalizer.fit(x_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {teacher_previous_proj_scalar.mean_[0]}, Standard deviation : {np.sqrt(teacher_previous_proj_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nx_train_teacher_previous_proj_normalized = teacher_previous_proj_normalizer.transform(x_train['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\nx_test_teacher_previous_proj_normalized  = teacher_previous_proj_normalizer.transform(x_test['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\n# x_cv_teacher_previous_proj_normalized    = teacher_previous_proj_normalizer.transform(x_cv['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\n\nprint(\"Shape of matrix after normalization -> Teachers Previous Projects: x_train:  \",x_train_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teachers Previous Projects: x_cv   :  \",x_cv_prefix_one_hot.shape)\nprint(\"Shape of matrix after normalization -> Teachers Previous Projects: x_test :  \",x_test_prefix_one_hot.shape)\n","c89baccd":"# We are considering only the words which appeared in at least 10 documents(rows or projects).\nvectorizer_essay_bow = CountVectorizer(min_df=10)\n\nvectorizer_essay_bow.fit(x_train['preprocessed_essays'])\n\nx_train_essays_bow = vectorizer_essay_bow.transform(x_train['preprocessed_essays'])\n# x_cv_essays_bow    = vectorizer_essay_bow.transform(x_cv['preprocessed_essays'])\nx_test_essays_bow  = vectorizer_essay_bow.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after BOW -> Essays: x_train: \",x_train_essays_bow.shape)\n# print(\"Shape of matrix after BOW -> Essays: x_cv   : \",x_cv_essays_bow.shape)\nprint(\"Shape of matrix after BOW -> Essays: x_test : \",x_test_essays_bow.shape)","f20963e1":"vectorizer_title_bow = CountVectorizer(min_df=10)\n\nvectorizer_title_bow.fit(x_train['preprocessed_titles'])\n\nx_train_titles_bow = vectorizer_title_bow.transform(x_train['preprocessed_titles'])\n# x_cv_titles_bow    = vectorizer_title_bow.transform(x_cv['preprocessed_titles'])\nx_test_titles_bow  = vectorizer_title_bow.transform(x_test['preprocessed_titles'])\n\nprint(\"Shape of matrix after BOW -> Title: x_train: \",x_train_titles_bow.shape)\n# print(\"Shape of matrix after BOW -> Title: x_cv   : \",x_cv_titles_bow.shape)\nprint(\"Shape of matrix after BOW -> Title: x_test : \",x_test_titles_bow.shape)","e05e13b3":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_essay_tfidf = TfidfVectorizer(min_df=10)\n\nvectorizer_essay_tfidf.fit(x_train['preprocessed_essays'])\n\nx_train_essays_tfidf = vectorizer_essay_tfidf.transform(x_train['preprocessed_essays'])\n# x_cv_essays_tfidf    = vectorizer_essay_tfidf.transform(x_cv['preprocessed_essays'])\nx_test_essays_tfidf  = vectorizer_essay_tfidf.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after TF-IDF -> Essay: x_train: \",x_train_essays_tfidf.shape)\n# print(\"Shape of matrix after TF-IDF -> Essay: x_cv   : \",x_cv_essays_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Essay: x_test : \",x_test_essays_tfidf.shape)","ca4eb62b":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_title_tfidf = TfidfVectorizer(min_df=10)\n\nvectorizer_title_tfidf.fit(x_train['preprocessed_titles'])\n\nx_train_titles_tfidf = vectorizer_title_tfidf.transform(x_train['preprocessed_titles'])\n# x_cv_titles_tfidf    = vectorizer_title_tfidf.transform(x_cv['preprocessed_titles'])\nx_test_titles_tfidf  = vectorizer_title_tfidf.transform(x_test['preprocessed_titles'])\n\nprint(\"Shape of matrix after TF-IDF -> Title: x_train: \",x_train_titles_tfidf.shape)\n# print(\"Shape of matrix after TF-IDF -> Title: x_cv   : \",x_cv_titles_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Title: x_test : \",x_test_titles_tfidf.shape)\n\n# Code for testing and checking the generated vectors\n# v1 = vectorizer.transform([preprocessed_titles[0]]).toarray()[0]\n# text_title_tfidf=pd.DataFrame(v1)\n# text_title_tfidf.to_csv('text_title_tfidf.csv', index=None,header=None)","ae8ee838":"# print(STOP)","3a1cab8c":"from scipy.sparse import hstack\n\nx_train_onehot = hstack((x_train_categories_one_hot, x_train_sub_categories_one_hot, x_train_school_state_one_hot, x_train_prefix_one_hot, x_train_grade_category_one_hot, x_train_price_normalized, x_train_teacher_previous_proj_normalized))\n# x_cv_onehot    = hstack((x_cv_categories_one_hot, x_cv_sub_categories_one_hot, x_cv_school_state_one_hot, x_cv_prefix_one_hot, x_cv_grade_category_one_hot,x_cv_price_normalized, x_cv_teacher_previous_proj_normalized ))\nx_test_onehot  = hstack((x_test_categories_one_hot, x_test_sub_categories_one_hot, x_test_school_state_one_hot, x_test_prefix_one_hot, x_test_grade_category_one_hot, x_test_price_normalized, x_test_teacher_previous_proj_normalized))\n\nprint(\"Type -> One Hot -> x_train: \",type(x_train_onehot))\nprint(\"Type -> One Hot -> x_test : \",type(x_test_onehot))\n# print(\"Type -> One Hot -> x_cv        : \",type(x_cv_onehot))\nprint(\"\\n\")\nprint(\"Shape -> One Hot -> x_train: \",x_train_onehot.shape)\nprint(\"Shape -> One Hot -> x_test : \",x_test_onehot.shape)\n# print(\"Shape -> One Hot -> x_cv         : \",x_cv_onehot.shape)","77663abb":"x_train_onehot.shape\n","225f050b":"x_train_onehot_bow = hstack((x_train_onehot,x_train_titles_bow,x_train_essays_bow)).tocsr()### Merging all ONE HOT features\n# x_cv_onehot_bow    = hstack((x_cv_onehot, x_cv_titles_bow, x_cv_essays_bow)).tocsr()### Merging all ONE HOT features\nx_test_onehot_bow  = hstack((x_test_onehot, x_test_titles_bow, x_test_essays_bow)).tocsr()### Merging all ONE HOT features\nprint(\"Type -> One Hot BOW -> x_train_cv_test: \",type(x_train_onehot_bow))\n# print(\"Type -> One Hot BOW -> cv             : \",type(x_cv_onehot_bow))\nprint(\"Type -> One Hot BOW -> x_test         : \",type(x_test_onehot_bow))\nprint(\"\\n\")\nprint(\"Shape -> One Hot BOW -> x_train_cv_test: \",x_train_onehot_bow.shape)\n# print(\"Shape -> One Hot BOW -> cv             : \",x_cv_onehot_bow.shape)\nprint(\"Shape -> One Hot BOW -> x_test         : \",x_test_onehot_bow.shape)","25774f47":"x_train_onehot_tfidf = hstack((x_train_onehot,x_train_titles_tfidf, x_train_essays_tfidf)).tocsr()\n# x_cv_onehot_tfidf    = hstack((x_cv_onehot,x_cv_titles_tfidf, x_cv_essays_tfidf)).tocsr()\nx_test_onehot_tfidf  = hstack((x_test_onehot,x_test_titles_tfidf, x_test_essays_tfidf)).tocsr()\nprint(\"Type -> One Hot TFIDF -> x_train_cv_test: \",type(x_train_onehot_tfidf))\n# print(\"Type -> One Hot TFIDF -> cv             : \",type(x_cv_onehot_tfidf))\nprint(\"Type -> One Hot TFIDF -> x_test         : \",type(x_test_onehot_tfidf))\nprint(\"\\n\")\nprint(\"Shape -> One Hot TFIDF -> x_train_cv_test: \",x_train_onehot_tfidf.shape)\n# print(\"Shape -> One Hot TFIDF -> cv             : \",x_cv_onehot_tfidf.shape)\nprint(\"Shape -> One Hot TFIDF -> x_test         : \",x_test_onehot_tfidf.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","d8c67c30":"# yy_train.shape","e51d729e":"# stop","88af75a3":"# def batch_predict(clf, data):\n#     # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n#     # not the predicted outputs\n\n#     y_data_pred = []\n#     tr_loop = data.shape[0] - data.shape[0]%1000\n#     # consider you X_tr shape is 49041, then your tr_loop will be 49041 - 49041%1000 = 49000\n#     # in this for loop we will iterate unti the last 1000 multiplier\n#     for i in range(0, tr_loop, 1000):\n#         y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n#     # we will be predicting for the last data points\n#     if data.shape[0]%1000 !=0:\n#         y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n#     return y_data_pred","fbf001cb":"print(\"DONE TILL HERE\")","50b331a9":"# import matplotlib.pyplot as plt\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.metrics import roc_auc_score\n# import math\n# train_auc = []\n# cv_auc = []\n# log_alphas = []\n# alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\n# for i in tqdm(alphas):\n#     nb = MultinomialNB(alpha = i)\n#     nb.fit(x_train_onehot_bow, yy_train)\n#     y_train_pred = batch_predict(nb, x_train_onehot_bow)\n#     y_cv_pred = batch_predict(nb, x_cv_onehot_bow)\n# # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# # not the predicted outputs\n#     train_auc.append(roc_auc_score(yy_train,y_train_pred))\n#     cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\n    \n# for a in tqdm(alphas):\n# #     b = math.log(a)\n#     b = np.log10(a)\n#     log_alphas.append(b)\n    \n# plt.figure(figsize=(10,5))\n# plt.plot(log_alphas, train_auc, label='Train AUC')\n# plt.plot(log_alphas, cv_auc, label='CV AUC')\n# plt.scatter(log_alphas, train_auc, label='Train AUC points')\n# plt.scatter(log_alphas, cv_auc, label='CV AUC points')\n# plt.legend()\n# plt.xlabel(\"log(alpha): hyperparameter\")\n# plt.ylabel(\"AUC\")\n# plt.title(\"alpha: hyperparameter v\/s AUC\")\n# plt.grid()\n# plt.show()","4034c465":"# log_alphas","740568e2":"# print(nb.classes_)","730ff9df":"# len(train_auc)","c615f3dd":"from sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score\nimport math\n","e437734c":"from sklearn.model_selection import GridSearchCV\nmnb_bow = MultinomialNB(class_prior=[0.5, 0.5])\nparameters = {'alpha':[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]}\nclf = GridSearchCV(mnb_bow, parameters, cv= 10, scoring='roc_auc',verbose=1,return_train_score=True)\n# clf.fit(x_cv_onehot_bow, y_cv)\nclf.fit(x_train_onehot_bow,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestAlpha_1=clf.best_params_['alpha']\nbestScore_1=clf.best_score_\nprint(\"BEST ALPHA: \",clf.best_params_['alpha'],\" BEST SCORE: \",clf.best_score_) #clf.best_estimator_.alpha","37f4233f":"alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,5))\nplt.plot(log_alphas, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.3,color='darkblue')\nplt.plot(log_alphas, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.3,color='darkorange')\nplt.scatter(log_alphas, train_auc, label='Train AUC points')\nplt.scatter(log_alphas, cv_auc, label='CV AUC points')\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"alpha: hyperparameter v\/s AUC\")\nplt.grid()\nplt.show()","dc4a41b2":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nmnb_bow_testModel = MultinomialNB(alpha = bestAlpha_1,class_prior=[0.5, 0.5])\nmnb_bow_testModel.fit(x_train_onehot_bow, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_bow_testModel, x_train_onehot_bow)\ny_train_pred=mnb_bow_testModel.predict_proba(x_train_onehot_bow)[:,1]\n# y_test_pred = batch_predict(mnb_bow_testModel, x_test_onehot_bow)\ny_test_pred=mnb_bow_testModel.predict_proba(x_test_onehot_bow)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set1_train=auc(train_fpr, train_tpr)\nauc_set1_test=auc(test_fpr, test_tpr)\n\n\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","09bb0c71":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","c7a099cd":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","4f4260a9":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","05f80859":"from sklearn.model_selection import GridSearchCV\nmnb_tfidf = MultinomialNB(class_prior=[0.5, 0.5])\nparameters = {'alpha':[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]}\nclf = GridSearchCV(mnb_tfidf, parameters, cv= 10, scoring='roc_auc',verbose=1,return_train_score=True)\n# clf.fit(x_cv_onehot_tfidf, y_cv)\nclf.fit(x_train_onehot_tfidf,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestAlpha_2=clf.best_params_['alpha']\nbestScore_2=clf.best_score_\nprint(\"BEST ALPHA: \",clf.best_params_['alpha'],\" BEST SCORE: \",clf.best_score_) #clf.best_estimator_.alpha","8a2fce4a":"alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,5))\nplt.plot(log_alphas, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.3,color='darkblue')\nplt.plot(log_alphas, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.3,color='darkorange')\n\nax = plt.subplot()\nax.scatter(log_alphas, train_auc, label='Train AUC points')\nax.scatter(log_alphas, cv_auc, label='CV AUC points')\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"alpha: hyperparameter v\/s AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","f3f02fd3":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nmnb_tfidf_testModel = MultinomialNB(alpha = bestAlpha_2,class_prior=[0.5, 0.5])\nmnb_tfidf_testModel.fit(x_train_onehot_tfidf, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_tfidf_testModel, x_train_onehot_tfidf)\n# y_test_pred = batch_predict(mnb_tfidf_testModel, x_test_onehot_tfidf)\ny_train_pred=mnb_tfidf_testModel.predict_proba(x_train_onehot_tfidf)[:,1]\ny_test_pred=mnb_tfidf_testModel.predict_proba(x_test_onehot_tfidf)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set2_train=auc(train_fpr, train_tpr)\nauc_set2_test=auc(test_fpr, test_tpr)\n\n# plt.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\n# plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\n\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","17b2e54b":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","69c57786":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","2a18153e":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","5dd34af8":"all_feature_names_bow=[]\n","1f257e49":"## FOR SET 1 and SET 2\nfor i in vectorizer_sub.get_feature_names():\n    all_feature_names_bow.append(i)   \n\nfor i in vectorizer_sub_sub.get_feature_names():\n    all_feature_names_bow.append(i)    \n\nfor i in vectorizer_state.get_feature_names():\n    all_feature_names_bow.append(i)    \n\nfor i in vectorizer_prefix.get_feature_names():\n    all_feature_names_bow.append(i)   \n\nfor i in vectorizer_grade.get_feature_names():\n    all_feature_names_bow.append(i)   \n\nfor i in vectorizer_title_bow.get_feature_names():\n    all_feature_names_bow.append(i)    \n\nfor i in vectorizer_essay_bow.get_feature_names():\n    all_feature_names_bow.append(i)   \n\nall_feature_names_bow.append(\"price\")\n\nall_feature_names_bow.append(\"prev_proj\")","30e8b6eb":"print(len(all_feature_names_bow))","6bd70c6a":"totalFeatureNamesBow=len(all_feature_names_bow)","9d74d053":"x_train_onehot_bow.shape","91ee658e":"totalFeatureNamesBow","f20d8bc2":"nb_bow=MultinomialNB(alpha=0.5,class_prior=[0.5,0.5])\n# nb_bow.fit(X-tr,y_train)\nnb_bow.fit(x_train_onehot_bow,y_train)\n\n# x_train_onehot\n# x_cv_onehot\n# x_test_onehot","4242d87c":"bow_features_probs_neg = {}\nfor a in range(totalFeatureNamesBow) :\n# for a in range(101) :\n    bow_features_probs_neg[a] = nb_bow.feature_log_prob_[0,a]","afff8215":"# len(bow_features_probs)","e6d21e67":"final_bow_features_neg = pd.DataFrame({'feature_prob_estimates' : list(bow_features_probs_neg.values()),\n'feature_names' : list(all_feature_names_bow)})","306bbf63":"a = final_bow_features_neg.sort_values(by = ['feature_prob_estimates'], ascending = False)","4536b10d":"print(\"TOP 10 Negative features - BOW\")\na.head(10)","5944a086":"bow_features_probs_pos = {}\nfor a in range(totalFeatureNamesBow) :\n# for a in range(101) :\n    bow_features_probs_pos[a] = nb_bow.feature_log_prob_[1,a]\n\nlen(bow_features_probs_pos)\n\nfinal_bow_features_pos = pd.DataFrame({'feature_prob_estimates' : list(bow_features_probs_pos.values()),\n'feature_names' : list(all_feature_names_bow)})\n\nb = final_bow_features_pos.sort_values(by = ['feature_prob_estimates'], ascending = False)\n\nprint(\"TOP 10 Positive features - BOW\")\nb.head(10)","7241323b":"all_feature_names_tfidf=[]","ab306325":"## FOR TFIDF SET 2\nfor i in vectorizer_sub.get_feature_names():\n    all_feature_names_tfidf.append(i)\n\nfor i in vectorizer_sub_sub.get_feature_names():\n    all_feature_names_tfidf.append(i)\n\nfor i in vectorizer_state.get_feature_names():\n    all_feature_names_tfidf.append(i)\n\nfor i in vectorizer_prefix.get_feature_names():\n    all_feature_names_tfidf.append(i)\n\nfor i in vectorizer_grade.get_feature_names():\n    all_feature_names_tfidf.append(i)\n\nfor i in vectorizer_essay_tfidf.get_feature_names():\n    all_feature_names_tfidf.append(i)\n\nfor i in vectorizer_title_tfidf.get_feature_names():\n    all_feature_names_tfidf.append(i)\n\nall_feature_names_tfidf.append(\"price\")\nall_feature_names_tfidf.append(\"prev_proj\")","20a5e90f":"nb_tfidf=MultinomialNB(alpha=0.5,class_prior=[0.5,0.5])\nnb_tfidf.fit(x_train_onehot_tfidf,y_train)","f2883d4d":"totalFeatureNamesTfidf=len(all_feature_names_tfidf)","2307f289":"tfidf_features_probs_neg = {}\nfor a in range(totalFeatureNamesTfidf) :\n# for a in range(101) :\n    tfidf_features_probs_neg[a] = nb_tfidf.feature_log_prob_[0,a]","abacd261":"len(tfidf_features_probs_neg)","374902a1":"final_tfidf_features_neg = pd.DataFrame({'feature_prob_estimates' : list(tfidf_features_probs_neg.values()),\n'feature_names' : list(all_feature_names_tfidf)})\n\nc = final_tfidf_features_neg.sort_values(by = ['feature_prob_estimates'], ascending = False)\n\nprint(\"TOP 10 Negative features -  TFIDF\")\nc.head(10)","0ea208af":"tfidf_features_probs_pos = {}\nfor a in range(totalFeatureNamesTfidf) :\n# for a in range(101) :\n    tfidf_features_probs_pos[a] = nb_tfidf.feature_log_prob_[1,a]\n\nlen(tfidf_features_probs_pos)\n\nfinal_tfidf_features_pos = pd.DataFrame({'feature_prob_estimates' : list(tfidf_features_probs_pos.values()),\n'feature_names' : list(all_feature_names_tfidf)})\n\nd = final_tfidf_features_pos.sort_values(by = ['feature_prob_estimates'], ascending = False)\n\nprint(\"TOP 10 Positive features - TFIDF\")\nd.head(10)","e95c5a03":"from prettytable import PrettyTable\n    \nx = PrettyTable()\n\nx.field_names = [\"Vectorizer\", \"Model\", \"Hyperparameter: Alpha\", \"Train AUC\", \"Test AUC\"]\nauc_set2_train=auc(train_fpr, train_tpr)\nauc_set2_test=auc(test_fpr, test_tpr)\n\nx.add_row([\"BOW\", \"Multinomial Naive Bayes\", bestAlpha_1, round(auc_set1_train,2),round(auc_set1_test,2)])\nx.add_row([\"TF-IDF\", \"Multinomial Naive Bayes\", bestAlpha_2, round(auc_set2_train,2),round(auc_set2_test,2)])\n\nprint(x)","a70517c2":"# 10. TOP 10 FEATURES FOR SET 2","d156c863":"# 9. TOP 10 FEATURES FOR SET 1","f04062eb":"# 2: PRE-PROCESSING","4c1850a3":"## -> 1.1: REMOVING NaN:<br>\n**As it is clearly metioned in the dataset details that TEACHER_PREFIX has NaN values, we need to handle this at the very beginning to avoid any problems in our future analysis.**","baa870a8":"## ->->-> 6.3.1.1: BOW: Essays (Train, CV, Test)","6eac0258":"**Observation:**\n1. The proportion of Majority class of 85% and Minority class of 15% is maintained in Training, CV and Testing dataset.","6686f489":"According to Andrew Ng, in the Coursera MOOC on Introduction to Machine Learning, the general rule of thumb is to partition the data set into the ratio of ***3:1:1 (60:20:20)*** for training, validation and testing respectively.","75a75b95":"## ->-> 8.1.1: <font color='red'> SET 2<\/font> Hyper parameter tuning to find best 'alpha' using GRIDSEARCHCV","efef23db":"## -> 2.1: Preprocessing: Project Subject Categories","b58ad6ca":"# Naive Bayes: DonorsChoose Dataset\n\n### A. DATA INFORMATION\n### B. OBJECTIVE\n## 1. READING THE DATASET\n- **1.1 Removing Nan**\n\n## 2. PREPROCESSING \n- **2.1 Preprocessing: Project Subject Categories**\n- **2.2 Preprocessing: Project Subject Sub Categories**\n- **2.3 Preprocessing: Project Grade**\n\n## 3. TEXT PROCESSING\n- **3.1 Text Preprocessing: Essays**\n- **3.2 Text Preprocessing: Title**\n\n## 4. SAMPLING\n- **4.1 Taking Sample from the complete dataset.**\n- **4.2 Splitting the dataset into Train, CV and Test datasets. (60:20:20)**\n    - 4.2.1: Splitting the FULL DATASET into Training and Test sets. (80:20)\n    - 4.2.2: Splitting the TRAINING Dataset further into Training and CV sets. (Not required  as we are using GridSearch)\n- **4.3 Details of our Training, CV and Test datasets.**\n\n## 5. UPSAMPLING THE MINORITY CLASS WITH REPLACEMENT STRATEGY (Not required  as we are using GridSearch)\n\n## 6. PREPARING DATA FOR MODELS\n- **6.1: VECTORIZING CATEGORICAL DATA**\n    - 6.1.1: Vectorizing Categorical data: Clean Subject Categories.\n    - 6.1.2: Vectorizing Categorical data: Clean Subject Sub-Categories.\n    - 6.1.3: Vectorizing Categorical data: School State.\n    - 6.1.4: Vectorizing Categorical data: Teacher Prefix.\n    - 6.1.5: Vectorizing Categorical data: Project Grade\n        \n- **6.2: VECTORIZING NUMERICAL DATA**\n    - 6.2.1: Standarizing Numerical data: Price\n    - 6.2.2: Standarizing Numerical data: Teacher's Previous Projects\n        \n- **6.3: VECTORIZING TEXT DATA**\n    - **6.3.1: BOW**\n        - 6.3.1.1: BOW: Essays (Train, CV, Test)\n        - 6.3.1.2: BOW: Title (Train, CV, Test)\n            \n    - **6.3.2: TF-IDF**\n        - 6.3.2.1: TF-IDF: Essays (Train, CV, Test)\n        - 6.3.2.2: TF-IDF: Title (Train, CV, Test)\n                \n## 7. MERGING FEATURES\n- 7.1: Merging all ONE HOT features.\n- 7.2: SET 1: Merging All ONE HOT with BOW (Title and Essay) features.\n- 7.3: SET 2: Merging All ONE HOT with TF-IDF (Title and Essay) features.\n    \n## 8. NAIVE BAYES\n- **8.1: SET 1 Applying Multinomial Naive Bayes on BOW.**\n    - 8.1.1: SET 1 Hyper parameter tuning to find the best Alpha.\n    - 8.1.2: SET 1 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.1.3: SET 1 Confusion Matrix\n        - 8.1.3.1: SET 1 Confusion Matrix: Train\n        - 8.1.3.2: SET 1 Confusion Matrix: Test\n            \n- **8.2: SET 2 Applying Multinomial Naive Bayes on TF-IDF.**\n    - 8.2.1: SET 2 Hyper parameter tuning to find the best Alpha.\n    - 8.2.2: SET 2 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.2.3: SET 2 Confusion Matrix\n        - 8.2.3.1: SET 2 Confusion Matrix: Train\n        - 8.2.3.2: SET 2 Confusion Matrix: Test\n    \n## 9. Top 10 Features of Set 1\n- **9.1: Negative Class\n- **9.2: Positive Class\n\n## 10. Top 10 Features of Set 2\n- **10.1: Negative Class\n- **10.2: Positive Class\n\n## 11. CONCLUSION ","0aad0025":"## SET 1:  POSITIVE CLASS","73ff290d":"## -> 7.3: SET 2:  Merging All ONE HOT with TF-IDF (Title and Essay) features","0405d714":"## ->-> 6.1.1: Vectorizing Categorical data: Clean Subject Categories","8e018433":"**Conclusion:** Now the number of rows reduced from 109248 to 109245 in project_data.","68394c6c":"## -> 7.1: Merging all ONE HOT features","2324313f":"# 6. PREPARING DATA FOR MODELS","92501a9c":"# -> 8.2:<font color='red'> SET 2<\/font>  Applying Naive Bayes on TFIDF.","edc34202":"## ->->-> 6.3.1.2: BOW: Title (Train, CV, Test)","72ae808c":"## A. DATA INFORMATION \n### <br>About the DonorsChoose Data Set\n\nThe `train.csv` data set provided by DonorsChoose contains the following features:\n\nFeature | Description \n----------|---------------\n**`project_id`** | A unique identifier for the proposed project. **Example:** `p036502`   \n**`project_title`**    | Title of the project. **Examples:**<br><ul><li><code>Art Will Make You Happy!<\/code><\/li><li><code>First Grade Fun<\/code><\/li><\/ul> \n**`project_grade_category`** | Grade level of students for which the project is targeted. One of the following enumerated values: <br\/><ul><li><code>Grades PreK-2<\/code><\/li><li><code>Grades 3-5<\/code><\/li><li><code>Grades 6-8<\/code><\/li><li><code>Grades 9-12<\/code><\/li><\/ul>  \n **`project_subject_categories`** | One or more (comma-separated) subject categories for the project from the following enumerated list of values:  <br\/><ul><li><code>Applied Learning<\/code><\/li><li><code>Care &amp; Hunger<\/code><\/li><li><code>Health &amp; Sports<\/code><\/li><li><code>History &amp; Civics<\/code><\/li><li><code>Literacy &amp; Language<\/code><\/li><li><code>Math &amp; Science<\/code><\/li><li><code>Music &amp; The Arts<\/code><\/li><li><code>Special Needs<\/code><\/li><li><code>Warmth<\/code><\/li><\/ul><br\/> **Examples:** <br\/><ul><li><code>Music &amp; The Arts<\/code><\/li><li><code>Literacy &amp; Language, Math &amp; Science<\/code><\/li>  \n  **`school_state`** | State where school is located ([Two-letter U.S. postal code](https:\/\/en.wikipedia.org\/wiki\/List_of_U.S._state_abbreviations#Postal_codes)). **Example:** `WY`\n**`project_subject_subcategories`** | One or more (comma-separated) subject subcategories for the project. **Examples:** <br\/><ul><li><code>Literacy<\/code><\/li><li><code>Literature &amp; Writing, Social Sciences<\/code><\/li><\/ul> \n**`project_resource_summary`** | An explanation of the resources needed for the project. **Example:** <br\/><ul><li><code>My students need hands on literacy materials to manage sensory needs!<\/code<\/li><\/ul> \n**`project_essay_1`**    | First application essay<sup>*<\/sup>  \n**`project_essay_2`**    | Second application essay<sup>*<\/sup> \n**`project_essay_3`**    | Third application essay<sup>*<\/sup> \n**`project_essay_4`**    | Fourth application essay<sup>*<\/sup> \n**`project_submitted_datetime`** | Datetime when project application was submitted. **Example:** `2016-04-28 12:43:56.245`   \n**`teacher_id`** | A unique identifier for the teacher of the proposed project. **Example:** `bdf8baa8fedef6bfeec7ae4ff1c15c56`  \n**`teacher_prefix`** | Teacher's title. One of the following enumerated values: <br\/><ul><li><code>nan<\/code><\/li><li><code>Dr.<\/code><\/li><li><code>Mr.<\/code><\/li><li><code>Mrs.<\/code><\/li><li><code>Ms.<\/code><\/li><li><code>Teacher.<\/code><\/li><\/ul>  \n**`teacher_number_of_previously_posted_projects`** | Number of project applications previously submitted by the same teacher. **Example:** `2` \n\n<sup>*<\/sup> See the section <b>Notes on the Essay Data<\/b> for more details about these features.\n\nAdditionally, the `resources.csv` data set provides more data about the resources required for each project. Each line in this file represents a resource required by a project:\n\nFeature | Description \n----------|---------------\n**`id`** | A `project_id` value from the `train.csv` file.  **Example:** `p036502`   \n**`description`** | Desciption of the resource. **Example:** `Tenor Saxophone Reeds, Box of 25`   \n**`quantity`** | Quantity of the resource required. **Example:** `3`   \n**`price`** | Price of the resource required. **Example:** `9.95`   \n\n**Note:** Many projects require multiple resources. The `id` value corresponds to a `project_id` in train.csv, so you use it as a key to retrieve all resources needed for a project:\n\nThe data set contains the following label (the value you will attempt to predict):\n\nLabel | Description\n----------|---------------\n`project_is_approved` | A binary flag indicating whether DonorsChoose approved the project. A value of `0` indicates the project was not approved, and a value of `1` indicates the project was approved.","036dcc28":"**Conlusion**\n1. **UPSAMPLING** needs to be done on the Minority class to avoid problems related to Imbalanced dataset.\n1. Upsampling will be done by _**\"Resample with replacement strategy\"**_","e8c4b57f":"# 5. UPSAMPLING THE MINORITY CLASS WITH REPLACEMENT STRATEGY","3f396388":"## -> 3.2: Text Preprocessing: Title","9544bf15":"### ->->-> 8.1.3.2: <font color='red'> SET 2<\/font> Confusion Matrix: Test","bcd2169d":"### -> -> 4.2.2: Splitting the TRAINING Dataset further into Training and CV sets.","4caa2ade":"## 1. READING DATA","def491a3":"# TESTING WITH BEST HYPERPARAMETER VALUE ON SET 1","3ef90735":"**Query 1.1: PreProcessing Teacher Prefix Done <br>\nAction Taken: Removed '.' from the prefixes and converted to lower case**","505dc6d4":"##  SET 2: POSITIVE CLASS","66a5dd69":"### ->->-> 8.1.3.1: <font color='red'> SET 2<\/font> Confusion Matrix: Train","df913635":"## -> 3.1: Text Preprocessing: Essays","dfe4c05d":"# 7. MERGING FEATURES","beaa0877":"**Observation:** \n1. 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245.","3c6358c8":"# ->-> 6.3.2: TF-IDF","66e1bf29":"## ->-> 6.2.1: Normalizing Numerical data: Price","289fd97a":"## SET 1: NEGATIVE CLASS","a800f0fa":"### ->->-> 8.1.3.2: <font color='red'> SET 1<\/font> Confusion Matrix: Test","877f0e91":"### -> 4.3: Details of our Training, CV and Test datasets.","251606bf":"## -> 2.2: Preprocessing: Project Subject Sub Categories","48a20ba9":"**Conclusion:**\n1. Resampling is performed on the Training data.\n1. Training data in now **BALANCED**.","7751daa3":"## ->-> 6.1.2: Vectorizing Categorical data: Clean Subject Sub-Categories","5af872dc":"## -> 2.3: Preprocessing: Project Grade Category","e03b5662":"# 3. TEXT PROCESSING","fce02e97":"# GRIDSEARCHCV","3e3f0ba7":"## ->-> 6.2.2: Normalizing Numerical data: Teacher's Previous Projects","0fee93e9":"#### Checking total number of enteries with NaN values","93843933":"# B. OBJECTIVE\nThe primary objective is to implement the k-Nearest Neighbor Algo on the DonorChoose Dataset and measure the accuracy on the Test dataset.","9f3f751d":"## ->-> 6.1.5 Vectorizing Categorical data: Project Grade","b5a8b5e7":"# -> 6.2: VECTORIZING NUMERICAL DATA","1981bddc":"**Observation:**\n1. Dataset is highly **IMBALANCED**.\n1. Approved Class (1) is the Majority class. And the Majority class portion in our sampled dataset: ~85%\n1. Unapproved class (0) is the Minority class. And the Minority class portion in our sampled dataset: ~15%","b0bfd633":"#### -> Merging Price with Project Data","c1c1deb1":"###    -> -> 4.2.1: Splitting the FULL DATASET into Training and Test sets. (80:20)","b74a21b2":"# 4. SAMPLING\n## -> 4.1: Taking Sample from the complete dataset\n## NOTE: A sample of 100000 Datapoints is taken due to lack computational resource.","4c48bda5":"**Observation:** 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245. <br>\n**Action:** We can safely delete these, as 3 is very very small and its deletion wont impact as the original dataset is very large. <br>\nStep1: Convert all the empty strings with Nan \/\/ Not required as its NaN not empty string <br> \nStep2: Drop rows having NaN values","247aa3b5":"## ->-> 8.1.1: <font color='red'> SET 1<\/font> Hyper parameter tuning to find best 'alpha'","fdb50920":"#### Merging Project Essays 1 2 3 4 into Essays","9f98c4f0":"# ->-> 6.3.1: BOW","7ee389a1":"## UPSAMPLED Balanced training dataset is now of 10,262 Datapoints.\n## CV and Test is 8,000 data points. Therefore total dataset = 18,262 Datapoints.","0bfbaf91":"# 11. CONCLUSION","de465e33":"## -> 4.2: Splitting the dataset into Train, CV and Test datasets. (60:20:20)","cbf69d2d":"we are going to consider\n\n       - school_state : categorical data\n       - clean_categories : categorical data\n       - clean_subcategories : categorical data\n       - project_grade_category : categorical data\n       - teacher_prefix : categorical data\n       \n       - project_title : text data\n       - text : text data\n       - project_resource_summary: text data (optinal)\n       \n       - quantity : numerical (optinal)\n       - teacher_number_of_previously_posted_projects : numerical\n       - price : numerical","0f949d72":"## -> 7.2: SET 1:  Merging All ONE HOT with BOW (Title and Essay) features","c622cb0a":"## ->->-> 6.3.2.2: TF-IDF: Title (Train, CV, Test)","62dc0cbe":"# TESTING WITH BEST HYPERPARAMETER VALUE ON SET 2","34295069":"# -> 6.1: VECTORIZING CATEGORICAL DATA","c12aca5b":"# 8. NAIVE BAYES\n","0dae6a85":"# -> 8.1:<font color='red'> SET 1<\/font>  Applying Naive Bayes on BOW.","84f6eb8b":"### ->->-> 8.1.3.1: <font color='red'> SET 1<\/font> Confusion Matrix: Train","994503aa":"## ->-> 6.1.3 Vectorizing Categorical data: School State","1d1dcfda":"**Query 1.2: PreProcessing Project Grade Done <br>\nAction Taken: Removed ' ' and '-' from the grades and converted to lower case**","01625429":"## ->->-> 6.3.2.1: TF-IDF: Essays (Train, CV, Test)","e15c01e7":"**Teacher Prefix has NAN values, that needs to be cleaned.\nRef: https:\/\/stackoverflow.com\/a\/50297200\/4433839**","3b7cf1c6":"# -> 6.3: VECTORIZING TEXT DATA","8c73eebb":"## ->-> 6.1.4 Vectorizing Categorical data: Teacher Prefix","0ad8934c":"- https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/handling-categorical-and-numerical-features\/","7dc94005":"Reference: https:\/\/elitedatascience.com\/imbalanced-classes","b0dc5601":"## SET 2:  NEGATIVE CLASS"}}