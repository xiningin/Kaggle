{"cell_type":{"92a13437":"code","9cc0bbe9":"code","bbfd02c2":"code","848a9278":"code","cccb1cf4":"code","2ce1f2ec":"code","b9e96425":"code","e42bf021":"code","eee530e5":"code","15dff250":"code","17c2bb90":"code","eb4ffdb3":"code","a4632a3c":"code","6d1c9b14":"code","c8c8fa9e":"code","5846539a":"code","f8dfbc88":"code","2f7ce319":"code","6fe8b03b":"code","6a7eda03":"code","b4fe8fd0":"code","03dd81cf":"code","9e18e043":"code","2121d972":"code","3ec2c3df":"markdown","148e86e8":"markdown","f86fbe0c":"markdown","ceb2a3da":"markdown","16f26c09":"markdown","e1ad6fed":"markdown"},"source":{"92a13437":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9cc0bbe9":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_original = train.copy()\n\ntarget = train['SalePrice']\ntrain.drop('SalePrice', axis=1, inplace=True)\n\ntest_index = test['Id']","bbfd02c2":"missing_threshold = 0.05\n\ndef get_missing_ratios(data, ratio=missing_threshold):\n    missing_ratios = pd.Series({col: train[col].isnull().sum() \/ train.shape[0] for col in train.columns}).sort_values(ascending=False)\n    \n    return missing_ratios[missing_ratios > ratio]","848a9278":"(set(get_missing_ratios(train, missing_threshold).index).union(\n    set(get_missing_ratios(train, missing_threshold).index))).difference(set(get_missing_ratios(train, missing_threshold).index))","cccb1cf4":"drop_cols = set(get_missing_ratios(train, missing_threshold).index)\ndrop_cols = drop_cols.union(set(['Id']))\n\ntrain.drop(drop_cols, axis=1, inplace=True)\ntest.drop(drop_cols, axis=1, inplace=True)","2ce1f2ec":"def show_cat_counts(train, test, field):\n    plt.figure(figsize=(6, 2))\n    plt.subplot(1,2, 1)\n    plt.bar(train[field].value_counts().index.values, train[field].value_counts())\n    plt.title(field + ' train')\n    plt.subplot(1, 2, 2)\n    plt.bar(test[field].value_counts().index.values, test[field].value_counts())\n    plt.title(field + ' test')","b9e96425":"train.isnull().sum().sort_values(ascending=False)","e42bf021":"test.isnull().sum().sort_values(ascending=False)","eee530e5":"#loops = train.shape[1] \/\/ 5\n#for j in range(loops):\n#    for col in train.columns[j*5:(j+1)*5]:\n#        show_cat_counts(train, test, col)","15dff250":"col_cat = [col for (col, typ) in train.dtypes.iteritems() if typ == 'O']\ncol_num = [col for (col, typ) in train.dtypes.iteritems() if typ != 'O']","17c2bb90":"col_size = 4\nrows = len(col_cat) \/\/ col_size\ncols = col_size","eb4ffdb3":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n#plt.figure(figsize=(20, 20))\n\n#for i in range(len(col_cat)):\n#    col = col_cat[i]\n#    #row = i \/\/ 5 + 1\n#    #col_index = i % 5 + 1\n#    plt.subplot(rows, cols, i + 1)\n#    sns.barplot(train[col].value_counts().index, train[col].value_counts())\n    \n#plt.show()","a4632a3c":"from sklearn.impute import SimpleImputer\n\ndef Impute(train, test, cols, strategy='mean'):\n    simpleImputer = SimpleImputer(strategy=strategy)\n    \n    train_imp = train.copy()\n    test_imp = test.copy()\n\n    train_imp[cols] = pd.DataFrame(simpleImputer.fit_transform(train[cols]))\n    test_imp[cols] = pd.DataFrame(simpleImputer.transform(test[cols]))\n    \n    train_imp.columns = train.columns\n    test_imp.columns = test.columns\n    \n    return (train_imp, test_imp)\n\ntrain_imp = train.copy()\ntest_imp = test.copy()\n\n(train_imp, test_imp) = Impute(train_imp, test_imp, col_num, 'mean')\n(train_imp, test_imp) = Impute(train_imp, test_imp, col_cat, 'most_frequent')\n","6d1c9b14":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef LabelEncode(train, test, cols):\n    le = LabelEncoder()\n    \n    train_enc = train.copy()\n    test_enc = test.copy()\n\n    for col in cols:\n        train_enc[col] = le.fit_transform(train_enc[col])\n        test_enc[col] = le.transform(test_enc[col])\n        \n    return (train_enc, test_enc)\n\ndef OneHotEncode(train, test, cols):\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    \n    train_enc = train.copy()\n    test_enc = test.copy()\n\n    train_ohe = pd.DataFrame(ohe.fit_transform(train_enc[cols]))\n    test_ohe = pd.DataFrame(ohe.transform(test_enc[cols]))\n    \n    train_ohe.index = train_enc.index\n    test_ohe.index = test_enc.index\n    \n    train_enc = pd.concat([train_enc, train_ohe], axis=1)\n    test_enc = pd.concat([test_enc, test_ohe], axis=1)\n    \n    train_enc.drop(cols, axis=1, inplace=True)\n    test_enc.drop(cols, axis=1, inplace=True)\n        \n    return (train_enc, test_enc)\n\n\n(train_enc, test_enc) = LabelEncode(train_imp, test_imp, col_cat)\n#(train_enc, test_enc) = OneHotEncode(train_imp, test_imp, col_cat)","c8c8fa9e":"train_full = pd.concat([train_enc, target], axis=1)\ncorr = train_full.corr()['SalePrice']\ncols_to_keep = (corr[abs(corr) > 0.1].drop(['SalePrice'], axis=0)).index.values","5846539a":"#train_enc = train_enc[cols_to_keep]\n#test_enc = test_enc[cols_to_keep]","f8dfbc88":"from sklearn.model_selection import train_test_split\n\ny = target\nX = train_enc\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","2f7ce319":"from sklearn.metrics import mean_absolute_error\n\ndef TestRandomForest(X_train, X_valid, y_train, y_valid, n_estimators):\n    my_model = RandomForestRegressor(n_estimators=n_estimators, random_state=0)\n    my_model.fit(X_train, y_train)\n    predictions = my_model.predict(X_valid)\n    print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))\n\ndef CheckRandomForestEstimators():\n    from sklearn.ensemble import RandomForestRegressor\n    \n    for i in [100, 200, 230]:\n        TestRandomForest(X_train, X_valid, y_train, y_valid, i)\n\n\ndef ApplyRandomForest():\n    from sklearn.ensemble import RandomForestRegressor\n    \n    my_model = RandomForestRegressor(n_estimators=200, random_state=0)\n    my_model.fit(X_train, y_train)\n    \n    return my_model\n","6fe8b03b":"def ApplyXGB():\n    from xgboost import XGBRegressor\n\n    my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n    my_model.fit(X_train, y_train, \n                 early_stopping_rounds=5, \n                 eval_set=[(X_valid, y_valid)], \n                 verbose=True)\n    \n    return my_model","6a7eda03":"def ApplyDeepLearning(epochs=4, batch_size=16):\n    import keras\n    from keras import layers\n    from keras import models\n        \n    mean = X.mean(axis=0)\n    std = X.std(axis=0)\n    _X = (X - mean) \/ std\n        \n    _X_train, _X_valid, _y_train, _y_valid = train_test_split(_X.values, y.values, train_size=0.8, test_size=0.2,\n                                                                random_state=0)    \n    \n    my_model = models.Sequential()\n    my_model.add(layers.Dense(128, activation='relu',\n                           input_shape=(_X_train.shape[1],)))\n    my_model.add(layers.Dropout(0.5))\n    my_model.add(layers.Dense(128, activation='relu'))\n    my_model.add(layers.Dropout(0.5))\n    my_model.add(layers.Dense(1))\n    \n    my_model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    \n    history = my_model.fit(_X_train, _y_train, batch_size=batch_size, epochs=epochs, validation_data=(_X_valid, _y_valid), verbose=1)\n    #my_model.fit(_X, y, batch_size=16, epochs=100, validation_split=0.2)\n    \n    return (my_model, _X_valid, mean, std, history)","b4fe8fd0":"#my_model = ApplyRandomForest()\nmy_model = ApplyXGB()\npredictions = my_model.predict(X_valid)\n\n#epochs=100\n#batch_size=32\n#(my_model, _X_valid, mean, std, history) = ApplyDeepLearning(epochs=epochs, batch_size=batch_size)\n#predictions = my_model.predict(_X_valid)","03dd81cf":"plt.figure()\nplt.plot(range(1, epochs+1), history.history['loss'], label='loss')\nplt.plot(range(1, epochs+1), history.history['val_loss'], label='val_loss')\nplt.legend()\n","9e18e043":"from sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","2121d972":"# used for only Deep Learning\n#test_enc = (test_enc - mean) \/ std\n\npreds_test = my_model.predict(test_enc)\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': test_index,\n                       'SalePrice': preds_test.ravel()})\noutput.to_csv('submission.csv', index=False)","3ec2c3df":"We see how the features (both numerical and categorical) are distributed","148e86e8":"# Checking Correlations","f86fbe0c":"# Dropping Features\nI am thinking of removing the features with more than missing_threshold% missing values.","ceb2a3da":"# Handling Missing Values with Imputation\nI will use SimpleImputer for starters","16f26c09":"In this notebook, I will use and compare Random Forest, XGBoost and Deep Learning.","e1ad6fed":"Checking if the missing features are different in train than test set"}}