{"cell_type":{"70199b13":"code","ce76ac75":"code","88f420a8":"code","2f4a1d54":"code","c26b1102":"code","8bc1d39e":"code","506d2672":"code","bae472c9":"code","c48e3ed4":"code","9f77d1d5":"code","627e6f42":"code","98632f56":"code","d58290d2":"code","a7a7306a":"code","42227e26":"code","4bda75f8":"code","0b0bae96":"code","d79ebef7":"code","0246fd4e":"code","67df68e5":"code","89a28db6":"markdown","6ddadd7d":"markdown","876c3869":"markdown","851440bd":"markdown","e0c7986c":"markdown","4371f451":"markdown","85f2b55e":"markdown","a368d0a2":"markdown"},"source":{"70199b13":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\nfrom sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom fastai.losses import LabelSmoothingCrossEntropy, LabelSmoothingCrossEntropyFlat\nfrom fastai.layers import Mish\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\nfrom tqdm import tqdm\nfrom torchmetrics import AUROC\nimport gc, sys, random\ngc.enable()","ce76ac75":"train_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_df =  pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\n\ntrain_df.set_index('id', inplace=True)\ntest_df.set_index('id', inplace=True)","88f420a8":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\n        \nseed_all(2021)","2f4a1d54":"train_df.claim.value_counts()","c26b1102":"# null values in generated data shown to be useful\ntrain_df.isna().sum()","8bc1d39e":"train_df['sum_na'] = train_df.isna().sum(axis=1)\ntest_df['sum_na'] = test_df.isna().sum(axis=1)","506d2672":"class TabModel(nn.Module):\n    def __init__(self, act_fn = nn.SiLU(), dropout_num = 1):\n        super().__init__()\n        self.emb = nn.Embedding(96,18)\n        self.fc = nn.Linear(119*18, 30)\n        self.dropouts = nn.ModuleList([nn.Dropout(0.4) \n                                       for _ in range(dropout_num)])\n        self.fc1 = nn.Linear(119,30)\n        self.fc2 = nn.Linear(30*2,30) # concat layers\n        self.out = nn.Linear(30,1)\n        self.act_fn = act_fn\n        \n        torch.nn.init.xavier_normal_(self.out.weight)\n        torch.nn.init.xavier_normal_(self.emb.weight)\n        torch.nn.init.xavier_normal_(self.fc.weight)\n        torch.nn.init.xavier_normal_(self.fc1.weight)\n        torch.nn.init.xavier_normal_(self.fc2.weight)\n\n    def forward(self, x_bin, x):\n        x_bin = self.emb(x_bin)\n        x_bin = x_bin.view(-1,119*18)\n        x_bin = self.act_fn(self.fc(x_bin))\n        \n        x = self.act_fn(self.fc1(x))\n        x = torch.cat([x_bin,x], -1)\n        x = self.act_fn(self.fc2(x))\n        \n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                out = dropout(x)\n                out = self.out(out)\n        \n            else:\n                temp_out = dropout(x)\n                temp_out = self.out(temp_out)\n                out += temp_out\n                \n        out \/= len(self.dropouts)\n\n        return torch.sigmoid(out)","bae472c9":"def preprocess_dataset(x, x_test, target = None):\n    if target:\n        x = x.copy().drop(target, 1)\n    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n    qt = QuantileTransformer(n_quantiles=96, output_distribution='normal')\n    bin_cat = KBinsDiscretizer(n_bins=96, encode='ordinal',strategy='uniform')\n    \n    x = imp.fit_transform(x)\n    x = qt.fit_transform(x)\n    x_bin = bin_cat.fit_transform(x)\n    \n    x_test = imp.transform(x_test)\n    x_test = qt.transform(x_test)\n    x_test_bin = bin_cat.transform(x_test)\n    \n    return x, x_bin, x_test, x_test_bin","c48e3ed4":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","9f77d1d5":"class EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001, verbose = None):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        self.verbose = verbose\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            if self.verbose:\n                print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            if self.verbose:\n                print('Validation score improved ({:.4f} --> {:.4f}). Saving model!'.format(self.val_score, epoch_score))\n                \n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","627e6f42":"x_train, x_bin, x_test, x_test_bin = preprocess_dataset(train_df, test_df, target = 'claim')\ny_train = train_df.claim.values","98632f56":"class TabDataset(Dataset):\n    def __init__(self, x, x_bin, target = None):\n        super().__init__()\n        self.x = x\n        self.x_bin = x_bin\n        self.target = target\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        x = self.x[idx, :]\n        x_bin = self.x_bin[idx, :]\n        \n        _dict = {'x': torch.tensor(x, dtype = torch.float),\n                 'x_bin': torch.tensor(x_bin, dtype = torch.long)}\n        \n        if self.target is not None:\n            target = self.target[idx].item()\n            _dict.update({'target': torch.tensor(target, dtype = torch.float)})\n        \n        return _dict","d58290d2":"class Trainer:\n    def __init__(self, model, device, loss_fn, opt, scheduler = None):\n        self.model = model\n        self.device = device\n        self.loss_fn = loss_fn\n        self.opt = opt\n        self.scheduler = scheduler\n        \n    def fit_one_epoch(self, dl):\n        self.model.train()\n        losses = AverageMeter()\n        prog_bar = tqdm(enumerate(dl), total = len(dl), file=sys.stdout, leave = False)\n        \n        for bi, d in prog_bar:\n            x = d[\"x\"].to(self.device)\n            x_bin = d['x_bin'].to(self.device)\n            target = d['target'].to(self.device)\n            \n            out = self.model(x_bin, x)\n            loss = self.loss_fn(out.squeeze(-1), target)\n            prog_bar.set_description('loss: {:.2f}'.format(loss.item()))\n            losses.update(loss.item(), x.size(0))\n            loss.backward()\n            self.opt.step()\n            \n            if self.scheduler: \n                self.scheduler.step()\n                    \n            self.opt.zero_grad()\n            \n    def eval_one_epoch(self, dl, **kwargs):\n        self.model.eval()\n        losses = AverageMeter()\n        metric = AUROC()\n        prog_bar = tqdm(enumerate(dl), total = len(dl), file=sys.stdout, leave = False)\n        \n        for bi, d in prog_bar:  \n            x = d[\"x\"].to(self.device)\n            x_bin = d['x_bin'].to(self.device)\n            target = d['target'].to(self.device)\n            \n            with torch.no_grad():\n                out = self.model(x_bin, x)\n                loss = self.loss_fn(out.squeeze(-1), target)\n                if metric:\n                    auroc = metric(out.squeeze(-1), target.int())\n                \n                losses.update(loss.item(), x.size(0))\n        auroc = metric.compute()\n        print(f\"F{kwargs['fold']} E{kwargs['epoch']}  Valid Loss: {losses.avg:.4f}  AUROC Score: {auroc:.4f}\")\n        return auroc.cpu() if metric else losses.avg","a7a7306a":"class cfg:\n    bs = 1024\n    n_splits = 8\n    seed = 2021\n    epochs = 4\n    lr = 1e-4\n    checkpoint = lambda fold: f'model_{fold}.pt'\n    \nkfold = StratifiedKFold(n_splits = cfg.n_splits, \n                        random_state = cfg.seed, \n                        shuffle = True)\nsplits = [*kfold.split(X = x_train, y = y_train)]","42227e26":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","4bda75f8":"def create_dataloaders(fold):\n    train_idx, valid_idx = splits[fold]\n    \n    _xtr, _xtr_bins, _ytr = x_train[train_idx], x_bin[train_idx], y_train[train_idx]\n    _xval, _xval_bins, _yval = x_train[valid_idx], x_bin[valid_idx], y_train[valid_idx]\n    \n    train_ds = TabDataset(x = _xtr, x_bin = _xtr_bins, target = _ytr)\n    valid_ds = TabDataset(x = _xval, x_bin = _xval_bins, target = _yval)\n                          \n    train_dl = DataLoader(train_ds, batch_size = cfg.bs, shuffle = True)\n    valid_dl = DataLoader(valid_ds, batch_size = cfg.bs, shuffle = False)\n    \n    return train_dl, valid_dl","0b0bae96":"def train_fold(fold, epochs):\n    train_dl, valid_dl = create_dataloaders(fold)\n    es = EarlyStopping(patience = 7, mode=\"max\", verbose = False)\n    \n    model = TabModel(dropout_num = 1).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr = cfg.lr)\n    scheduler = OneCycleLR(opt, \n                           max_lr=1e-3, \n                           steps_per_epoch=len(train_dl),\n                           epochs = epochs)\n\n    trainer = Trainer(model, \n                      device, \n                      loss_fn=nn.BCELoss(), \n                      opt = opt,\n                      scheduler = scheduler,\n                     )\n    \n    for epoch in range(epochs):\n        trainer.fit_one_epoch(train_dl)\n        valid_loss = trainer.eval_one_epoch(valid_dl, fold = fold, epoch = epoch)\n        \n        es(valid_loss, trainer.model, model_path = cfg.checkpoint(fold))\n        \n        if es.early_stop:\n            break","d79ebef7":"for fold in range(cfg.n_splits):\n    train_fold(fold, cfg.epochs)","0246fd4e":"y_pred = torch.zeros(len(x_test), 1).to(device)\ntest_ds = TabDataset(x_test, x_test_bin)\ntest_dl = DataLoader(test_ds, batch_size = cfg.bs, shuffle = False)\n\nwith torch.no_grad():\n    for fold in range(cfg.n_splits):\n        preds = []\n        model = TabModel(dropout_num = 1).to(device)\n        state_dict = cfg.checkpoint(fold)\n        model.load_state_dict(torch.load(state_dict))\n        model.eval()\n        \n        for d in test_dl:\n            x = d[\"x\"].to(device)\n            x_bin = d['x_bin'].to(device)\n            out = model(x_bin, x)\n            preds.append(out)\n            \n        preds = torch.vstack(preds)\n        y_pred += preds \/ cfg.n_splits","67df68e5":"sub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub.iloc[:,1] = y_pred.cpu()\nsub = sub.set_index('id')\nsub.to_csv('submission.csv')","89a28db6":"## Model Architecture","6ddadd7d":"## Dataset","876c3869":"## Trainer","851440bd":"## Utilities","e0c7986c":"This notebook is based on `lukaszborecki's` great work in this competition:  \nhttps:\/\/www.kaggle.com\/lukaszborecki\/pytorch-fork-of-tps-09-nn  \n\n  This notebook plays around with:\n   - Adding scheduler\/boilerplate\n   - Adding a Trainer object for training\/evaluation\n   - Concating x and x_bin inputs\n   - Adding multi-sample dropout","4371f451":"## Exploration and Feature Engineering","85f2b55e":"## Prediction","a368d0a2":"## Training"}}