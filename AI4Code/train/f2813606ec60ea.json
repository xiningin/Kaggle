{"cell_type":{"fa76296c":"code","43e06516":"code","450bd61d":"code","e4e5eb19":"code","58823cfa":"code","9ab49075":"code","a274e7c1":"code","b5fd0b58":"code","95048bea":"code","0ecfe932":"code","b023b8a5":"markdown","e7cfae65":"markdown","df8982c0":"markdown","4f6be314":"markdown","122f4b48":"markdown","3c3de632":"markdown","dd0ebafb":"markdown","7fdfe4e7":"markdown","c46e121e":"markdown","6d449202":"markdown","7f380fdc":"markdown","9726c38f":"markdown","e43b7ce9":"markdown","c9a73fb2":"markdown","f5297303":"markdown","84e0a3d4":"markdown","b07417dd":"markdown","404a4324":"markdown"},"source":{"fa76296c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","43e06516":"# Titanic reading files\ntitanic_file_path = '\/kaggle\/input\/titanic\/train.csv'\ntitanic_data = pd.read_csv(titanic_file_path)\n\n# Show dataset columns\ntitanic_data.columns","450bd61d":"# This single column is stored in a Series, is like a DataFrame with only a single column of data\ny = titanic_data.Survived","e4e5eb19":"titanic_features = [\"Sex\"]\n_X = titanic_data[titanic_features] \n\n# Shows some interesting statistics about our data\nprint(_X.describe())\n\n# Shows the top few rows\n_X.head()","58823cfa":"def clean_X(X):\n    return pd.get_dummies(X)","9ab49075":"X = clean_X(_X)\n\nprint(X[:5])","a274e7c1":"from sklearn.tree import DecisionTree... as Model\n\n# help(Model)\n\n#Define model\ntitanic_model = Model(random_state=1)\n\n# Fit model\ntitanic_model.fit(X, y)\n\n# Show some info for visual validation\nprint(\"Making predictions for the following 5 passengers\")\nprint(_X[:5])\nprint(\"And the predictions are...\")\nprint(titanic_model.predict(X[:5]))","b5fd0b58":"from sklearn.metrics import mean_absolute_error\n\npredicted_passengers = titanic_model.predict(X)\nmean_absolute_error(y, predicted_passengers)","95048bea":"from sklearn.model_selection import train_test_split\n\n# Split data into training and validation data, for both features and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n# Define model\ntitanic_model = Model()\n# Fit model\ntitanic_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nval_predictions = titanic_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))","0ecfe932":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = Model(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\n# Compare MAE with differing values of max_leaf_nodes\nmin_mae = -1\nbest_max_leaf_nodes = 0\nfor max_leaf_nodes in [5, 10, 50, 100, 500, 1000, 5000]:\n    current_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)    \n    if min_mae < 0 or min_mae > current_mae:\n        min_mae = current_mae\n        best_max_leaf_nodes = max_leaf_nodes    \n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, current_mae))\n    \nprint(\"Best max leaf nodes: %d\" %(best_max_leaf_nodes))","b023b8a5":"Para nuestro vector caracter\u00edstico, seleccionamos \u00fanicamente el sexo que sera la columna a partir de la cu\u00e1l nuestro modelo podr\u00e1 realizar las predicciones.","e7cfae65":"El proximo paso ser\u00e1 tomarnos un tiempo para utilizar lo que aprendimos hasta ahora en un excelente tutorial de Kaggle acerca de los [sobrevivientes del Titanic](https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial) (les suena el tema no?).","df8982c0":"Repasemos r\u00e1pidamente algunos de los tipos de aprendizaje que existen:\n\n## Tipos de Aprendizaje\n\n### Supervised\n\nEl dataset es un conjuto de ejemplos representados como tuplas (vector caracer\u00edstico, etiqueta) donde por un lado, cada dimensi\u00f3n del vector describe el ejemplo (si tenemos un dataset de personas, cada elemento es una persona que puede ser descrita por su altura, peso, edad) y por el otro las etiquetas que pueden ser un n\u00famero real, un elemento de un set finito de clases (por ejemplo spam \/ not spam).\n\nEl objetivo de este tipo de aprendizaje es armar un ___modelo___ que dado cierto vector caracter\u00edstico X produzca ciertas salidas que permitan deducir como etiquetar dicho vector.\n\n### Semi-Supervised\nSimilar al supervisado. La gracia (o diferencia respecto al anterior) de este tipo es poder encontrar un mejor modelo sin tener todos los datos etiquetados.\n\n### Unsupervised\n> El aprendizaje no supervisado es el paradigma que permite producir conocimiento \u00fanicamente de los datos que se proporcionan como entrada. - Dot CSV [\u00bfQu\u00e9 es el Aprendizaje Supervisado y No Supervisado?](https:\/\/www.youtube.com\/watch?v=oT3arRRB2Cw)\n\nEn este caso, los ejemplos del dataset no est\u00e1n etiquetados y se trata de encontrar patrones desconocidos en los datos.\nUna de las aplicaciones de este paradigma es la de __clustering__, es decir, agrupar los datos en base a los patrones aprendidos.\n\n### Reinforcement\nEn este subcampo, tenemos una m\u00e1quina viviendo en cierto entorno, el cual percibe y lo usa de vector caracter\u00edstico. A partir del mismo, la m\u00e1quina puede ejecutrar diferentes __acciones__, que traer\u00e1n diferentes recompensas, dependiendo el __estado__ del contexto. El objetivo es poder encontrar una funci\u00f3n que, dado el vector de estado, produzca la acci\u00f3n \u00f3ptima a realizar, esta funci\u00f3n es denominada __pol\u00edtica__.","4f6be314":"## \u00bfC\u00f3mo evitarlos?\n\nAhora que ya sabemos lo que son el overfitting y el underfitting la pregunta que surge es c\u00f3mo hacer para evitarlos.\n\nEl underfitting resulta (relativamente) f\u00e1cil de evitar, s\u00f3lo basta con tener la suficiente cantidad y variedad de datos para entrenar al modelo.\n\nEl overfitting, sin embargo, resulta m\u00e1s complicado ya que no conocemos de antemano los resultados de las cosas que queremos predecir en un futuro.\n\nUna forma de resolver este problema es separar los datos de entrenamiento en dos grupos. El primero lo usaremos para entrenar al modelo, mientras que el segundo lo usaremos para realizar predicciones y comparar los valores predichos con los esperados mediante una funci\u00f3n de error (MAE por ejemplo).\n\nUna vez que conozcamos los hiperpar\u00e1metros que parecen funcionar mejor para nuestro modelo podemos utilizar el dataset completo para entrenar a la red.","122f4b48":"## Pandas para familiarizarnos con los datos\n\nEl primer paso en cualquier proyecto de Machine Learning es familiarizarnos con los datos. Una buena forma de introducrinos en este universo sin morir en el intento es usando una de las bibliotecas m\u00e1s utilizadas por los Data Scientists. __Pandas__.","3c3de632":"# Par\u00e1metros vs. Hiperpar\u00e1metros\n\nLos Hiperpar\u00e1metros son propiedades, en general num\u00e9ricas, de nuestro algoritmo que influencian la manera en el que el mismo trabaja. Estos valores no son encontrados \/ aprendidos por el modelo por si s\u00f3lo, si no que son seteados en general por los analistas que estan corriendo el algoritmo.\n\nPor otro lado, los par\u00e1metros son variables que definen el modelo y que son encontradas \/ aprendidas por el propio algoritmo. En base a los datos de entrenamiento, los mismos van siendo directamente modificados por este \u00faltimo. El objetivo es encontrar los valores que hagan que el modelo sea \u00f3ptimo.","dd0ebafb":"Obtenemos las X limpias y visualizamos c\u00f3mo quedan las primeras 5 filas.","7fdfe4e7":"## Nuestro primer modelo\n\nAhora, con el dataset ya le\u00eddo, vamos a implementar nuestro primer modelo siguiendo los siguientes pasos:\n- Definici\u00f3n: modelo a utilizar, par\u00e1metros e hiperpar\u00e1metros que recibir\u00e1 (\u00c1rbol de Decisi\u00f3n, en este caso).\n- Entrenamiento \/ adapataci\u00f3n (fit): es la parte fundamental donde capturamos los patrones que se encuentran en nuestros datos.\n- Predicci\u00f3n\n- Evaluaci\u00f3n: determinar la precisi\u00f3n de nuestro modelo.\n\nEn primer lugar necesitamos determinar cual es la columna que queremos predecir, Survived en este caso.","c46e121e":"# \ud83d\ude0e var hackateca = \"Machine Learning\" \ud83d\ude0e\n\nEmpezemos por sacarnos de la cabeza la idea de que la computadora \"aprende\" a trav\u00e9s de algoritmos de ___Machine Learning___. Lo que pasa en realidad es que nos esforzamos por encontrar una f\u00f3rmula matem\u00e1tica la cual dado un conjunto de inputs nos devuelve una salida esperada. El objetivo est\u00e1 en poder hallar aquella f\u00f3rmula que produzca la mayor cantidad de salidas \"correctas\" para la mayor cantidad de entradas (que tienen una distribuci\u00f3n similar).\n\nPara aproximarnos un poco a una definici\u00f3n m\u00e1s formal, podemos decir que Machine Learning es un proceso con el cu\u00e1l trataremos de resolver un problema:\n\n1. Recolecci\u00f3n de la informaci\u00f3n.\n2. Construcci\u00f3n de un algoritmo que se aproxime a un modelo estad\u00edstico basado en un el conjunto de datos con el que contamos.","6d449202":"# Y si era tan ~~listo~~ bueno \u00bfpor qu\u00e9 se muri\u00f3?\n![](https:\/\/i.ytimg.com\/vi\/de-Oi1kX7is\/maxresdefault.jpg)\nAhora que ya tenemos un primer modelo en funcionamiento vamos a intentar determinar cuantitativamente la calidad del mismo. Una medida interesante que a todos nos interesa observar es que tan cercano a la realidad esta nuestro modelo, que tan __preciso__ es a la hora de predecir.\n\nUn error muy com\u00fan que solemos cometer es tratar de predecir utilizando los datos de entrenamiento y luego comparar las predicciones contra esos mismos datos, en la pr\u00f3xima secci\u00f3n vemos porque y como resolverlo.\nExisten algunas m\u00e9tricas que permiten que nos desliguemos de problemas como el mencionado en el p\u00e1rrafo anterior, como por ejemplo el __Mean Absolute Error (MAE)__ siendo `error = |valor actual - valor predicho|`.","7f380fdc":"### Links de utilidad\n- [The 100-page Machine Learning Book](http:\/\/themlbook.com\/wiki\/doku.php?id=start)\n- [Micro-cursos de Kaggle (je)](https:\/\/www.kaggle.com\/learn\/overview)\n- [Un repo de un wachin](https:\/\/practicalai.me\/)\n- [Cursos de AWS](https:\/\/aws.amazon.com\/es\/training\/?nc2=h_ql_ce_tc)\n- [Dot CSV - Canal de IA y Data Science en general](https:\/\/www.youtube.com\/channel\/UCy5znSnfMsDwaLlROnZ7Qbg)","9726c38f":"![](http:\/\/66.media.tumblr.com\/tumblr_lqkhm7pYoj1qj0x9ro1_500.png)\n\nLos desafiamos a mejorar todav\u00eda m\u00e1s las predicciones del modelo de titanic. Pueden usar otros modelos, cambiar los hiperpar\u00e1metros del modelo existente, u otras cosas que no se nos ocurrieron a nosotros.\n\nSolo se necesita:\n- Todo lo que aprendimos hasta ahora.\n- Un poco de creatividad.\n- La [documentaci\u00f3n de scikit-learn](https:\/\/scikit-learn.org\/stable\/user_guide.html).\n- ~~El conocimiento para implementar una red neuronal en assembler.~~","e43b7ce9":"# Overfitting y Underfitting\nPara entender la importancia de contar con un buen dataset para entrenar al modelo hay que comprender primero qu\u00e9 son el ___underfitting___ y el ___overfitting___.\n\nCuando nuestro modelo responde lo suficientemente bien ante los datos de entrenamiento, pero realmente mal cuando se utilizan datos de validaci\u00f3n u otros estamos ante un caso de __overfitting__. \u00bfQu\u00e9 pasa? Parece como si nuestro modelo estar\u00eda encontrando patrones en base a datos \"falsos\", por decirlo de alguna manera. Por otro lado, cuando el modelo falla en la captura de patrones bastante importantes, notorios y falla hasta con los datos de entrenamiento tenemos una situaci\u00f3n de __underfitting__.\nHaciendo una analog\u00eda con los humanos, si la tarea a resolver por el modelo fuera un examen, entonces un modelo con __underfitting__ ser\u00eda como un alumno que no estudi\u00f3 lo suficiente, mientras que uno con __overfitting__ es un alumno que aprendi\u00f3 mec\u00e1nicamente los ejercicios del examen y no puede resolver problemas distintos a los que ya conoce.\n\nEntonces, lo que buscamos es un modelo que sea lo suficientemente flexible y haya entrenado lo suficiente para ser capaz de obtener un resultado satisfactorio teniendo como entrada los datos de entrenamiento, pero tambi\u00e9n necesitamos que sea capaz de ___generalizar___ ese conocimiento para datos de entrada nuevos y esta capacidad se pierde cuando el modelo se ajusta demasiado a la entrada.\n\n![underfitting-overfitting](https:\/\/i0.wp.com\/www.aprendemachinelearning.com\/wp-content\/uploads\/2017\/12\/generalizacion-machine-learning.png?resize=560%2C225)","c9a73fb2":"# \u00bfC\u00f3mo podemos mejorarlo?\n\nSi observamos algunos de los hiperpar\u00e1metros que existen para el modelo que elegimos, nos vamos a encontrar con por ejemplo la profunidad del \u00c1rbol. Existen algunas alternativas que nos permitir\u00e1n controlar la misma para poder combatir los dos problemas mencionados anteriormente. Mientras mayor profundidad tenga el \u00e1rbol, m\u00e1s nos movemos desde el \u00e1rea de __underfitting__ a la de __overfitting__.\n\nPodemos, por ejemplo, iterar con diferentes opciones de profundidad para comparar los diferentes errores que genera cada una y quedarnos con la que m\u00e1s le sirva al modelo.","f5297303":"Utilizando la ~~librer\u00eda~~ biblioteca __Scikit-Learn__ crearemos, por fin, nuestro modelo.\n\nTe pedimos que reemplaces los tres puntos del import por el tipo de \u00e1rbol de decisi\u00f3n que consideres apropiado para este problema (___Regressor___ o ___Classifier___).","84e0a3d4":"Definimos una funci\u00f3n auxiliar para limpiar los datos. Realizar\u00e1 transformaciones sobre las X para evitar valores nulos y de tipo String.\n\nEn este caso realizaremos una tabla de doble entrada donde las columnas ser\u00e1n \"Sex_female\" y \"Sex_male\", indicando con un 1 el valor correspondiente y con un 0 el otro.","b07417dd":"## Tipos de problema\n\n### Clasificaci\u00f3n\n\nEs un problema en el que asignamos una __etiqueta__ a un ejemplo que no la contiene. Por ejemplo si queremos detectar de todos nuestros mails cuales son o no spam. La variable a predecir es __discreta__.\n\n### Regresi\u00f3n\n\nEn este caso tratamos de predecir lo que se denominan __objetivos__ a partir de un ejemplo no etiquetado. Un ejemplo posile podr\u00eda ser estimar el precio de una casa basado en algunas __caracter\u00edsticas__ como la cantidad de habitaciones, ubicaci\u00f3n, superficie. La variable a predecir es __continua__.","404a4324":"## \u00bfComo funcionan nuestros modelos?\n\nPara comenzar podemos basarnos en un \u00c1rbol de decisi\u00f3n que si bien no es el modelo m\u00e1s preciso, es bastante simple de entender. \n\nSupongamos que contamos con los datos de todas las personas que estuvieron a bordo del Titanic y que en promedio sobrevivieron muchas m\u00e1s mujeres que hombres.\n\n![](https:\/\/i.gyazo.com\/57dd6ebe47345c20cc47b3eb35363b77.png)\n\nUsamos los datos para decidir como dividir, en este caso, las personas en grupos para luego usar estos grupos para predecir el \"destino\" de cada uno. Este proceso de b\u00fasqueda de patrones es muy com\u00fan y hasta tiene un nombre fancy,  se llama entrenamiento __(fitting, training)__ del modelo, y los datos que se utilizan en este proceso se denominan __training data__.\n\nCon el modelo entrenado, contamos con la posibilidad de poder aplicarlo para poder predecir qui\u00e9n hubiera sobrevivido."}}