{"cell_type":{"1c4be51f":"code","bb08bc2e":"code","3643be41":"code","a35fdf7d":"code","fa9714c9":"code","ccd9df92":"code","80622790":"code","5040c82b":"code","85275007":"code","af81d56c":"code","2b43d940":"code","75d6be14":"code","46f60b72":"code","7d963854":"code","97e088f7":"code","74469294":"code","01751a2f":"code","6c0ccda3":"code","bb7d6343":"code","f2639ebd":"code","ba901562":"code","438b4d35":"code","e8175ae6":"code","ef404021":"code","c5d2e0a6":"code","d1f1b1f8":"code","f657ec3d":"code","2dad9f3f":"code","676753e5":"code","52db5610":"code","ec32c028":"code","042c8c00":"code","74b576d8":"code","1dee98c6":"code","30eb2a74":"markdown","2f92bf14":"markdown","f20aff66":"markdown","d784d6af":"markdown","b77bdbbc":"markdown","bea94c8d":"markdown","6306a9c8":"markdown","38a5d800":"markdown","0592be7a":"markdown","1cc9a1a1":"markdown","c40d01d1":"markdown","2127d113":"markdown","0a029ae3":"markdown","3c352879":"markdown","01d6958d":"markdown","739704bb":"markdown","7749206e":"markdown"},"source":{"1c4be51f":"# import the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport plotly.express as px\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_curve, roc_auc_score, classification_report, precision_score, recall_score, ConfusionMatrixDisplay\nfrom sklearn.manifold import TSNE\n\n!pip install umap-learn\nimport umap\nfrom sklearn.ensemble import IsolationForest\nimport warnings\nwarnings.filterwarnings('ignore')","bb08bc2e":"data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","3643be41":"data.head()","a35fdf7d":"data","fa9714c9":"# Check for missing values\ndata.isna().sum()","ccd9df92":"data.describe()","80622790":"# Check data imbalance\nfig = px.histogram(data[\"Class\"], width=600, height=400, labels={'value':'Class'}, template='plotly_dark+presentation')\nfig.show()","5040c82b":"sns.distplot(data['Time']);","85275007":"# Split the data according to class\nclass_0 = data[data[\"Class\"] == 0]\nclass_1 = data[data[\"Class\"] == 1]","af81d56c":"# Visualize features\nfeatures = list(data.columns[1:29])\nfig, ax = plt.subplots(nrows=14, ncols=2, figsize=(16, 45))\nc = 1\nfor i in features:\n    plt.subplot(14, 2, c)\n    sns.kdeplot(data=class_0, x=i, color='red')\n    sns.kdeplot(data=class_1, x=i, color='blue')\n    c = c + 1","2b43d940":"# Visualize amounts\nplt.figure(figsize=(13, 5))\nsns.kdeplot(data=class_0, x='Amount', color='red')\nsns.kdeplot(data=class_1, x='Amount', color='yellow')","75d6be14":"#from sklearn.manifold import TSNE\n# from cuml.manifold import TSNE","46f60b72":"def visualize_tsne(features, target, n_neighbors):\n    \n    # Perform TSNE\n    for neighbor in n_neighbors:\n        \n        x_reduced = umap.UMAP(n_components=2, n_neighbors=neighbor).fit_transform(features)\n        print(\"Done...\")\n\n        # Visualize TSNE\n        print(\"Creaitng plot\")\n        df  = pd.DataFrame({'x':x_reduced[:, 0], 'y':x_reduced[:, 1], 'label':target})\n\n        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, palette='Set1')\n        #plt.title(f\"Perplexity: {perplexity} and max iter: {n_iter}\")\n        plt.show()\n        print(\"Done\")","7d963854":"features = data.drop('Class', axis=1)\ntarget = data[\"Class\"]\nvisualize_tsne(features, target, n_neighbors=[10, 15, 20, 30])","97e088f7":"from sklearn.preprocessing import RobustScaler","74469294":"preprocess_feat = ['Time', 'Amount']\nfor feat in preprocess_feat:\n    scaler = RobustScaler()\n    data[feat] = scaler.fit_transform(X=data[feat].values.reshape(-1, 1))","01751a2f":"X = data.drop('Class', axis=1)\ny = data[\"Class\"]","6c0ccda3":"def Stratified_CV(X, y, classifier, params, folds=5):\n    scores = []\n    \n    # Using Stratified K-fold CV for preserving the percentage of samples for the two classes\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n    for fold, (tr_idx, ts_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n        x_ts, y_ts = X.iloc[ts_idx], y.iloc[ts_idx]\n\n        clf = classifier(**params)\n        clf.fit(x_tr, y_tr,\n                eval_set=[(x_ts, y_ts)],\n                early_stopping_rounds=100,\n                verbose=False)\n        \n        pred = clf.predict_proba(x_ts)[:, 1]\n        score = roc_auc_score(y_ts, pred)\n        precision = precision_score(y_ts, clf.predict(x_ts))\n        recall = recall_score(y_ts, clf.predict(x_ts))\n        scores.append([score, precision, recall])\n        print(f\"ROC AUC Score: {score}\")\n        print()\n        print(classification_report(y_ts, clf.predict(x_ts)))\n        print()\n        \n        print(\"-\"*60)\n    \n    return clf, scores","bb7d6343":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import learning_curve, train_test_split","f2639ebd":"estimators = {\n    'KNN':KNeighborsClassifier(),\n    'Logistic Regression':LogisticRegression(),\n    'SVM': SVC(),\n    'Random Forest':RandomForestClassifier(),\n    'LightGBM':LGBMClassifier(),\n    'XGBoost':XGBClassifier()\n}","ba901562":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","438b4d35":"for name, clf in estimators.items():\n    print(name)\n    clf.fit(x_train, y_train)\n    pred = clf.predict(x_test)\n    \n    print(classification_report(y_test, pred))\n    print('-'*50)","e8175ae6":"xgb_params = {\n    \"random_state\":42,\n    'use_label_encoder':False,\n    'eval_metric':'auc'\n}","ef404021":"model_xgb, score = Stratified_CV(X, y, XGBClassifier, xgb_params, 5)","c5d2e0a6":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split","d1f1b1f8":"def tf_CV(model, X, y, folds=5):\n    \n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n    for fold, (tr_idx, ts_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n        x_ts, y_ts = X.iloc[ts_idx], y.iloc[ts_idx]\n\n        model.compile(\n            loss='binary_crossentropy',\n            optimizer='adam',\n            metrics=['Recall', 'accuracy']\n        )\n\n        es = tf.keras.callbacks.EarlyStopping(monitor='val_recall', patience=5, restore_best_weights=True)\n        lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_recall')\n\n        model.fit(x_tr, \n                y_tr,\n                validation_data=(x_ts, y_ts),\n                verbose=2,\n                epochs=30,\n                batch_size=128,\n                callbacks=[es, lr])\n        \n        print(\"-\"*100)\n    \n    return model","f657ec3d":"def create_model():\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(30, activation='relu', input_shape=(30, )),\n        tf.keras.layers.Dense(30, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    return model","2dad9f3f":"clf_nn = create_model()\ntf_CV(clf_nn, X, y)","676753e5":"umap = pd.read_csv(\"..\/input\/umap-embedding\/UMAP_embed.csv\")\ndbscan_pred = pd.read_csv(\"..\/input\/dbscan-predictions-credit-card-fraud-detection\/DBSCAN_Pred.csv\")\numap[\"target\"] = y\numap[\"dbscan_pred\"] = dbscan_pred[\"pred\"]","52db5610":"umap","ec32c028":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(17, 8))\nsns.scatterplot(x=umap['x_component'], y=umap[\"y_component\"], hue=umap[\"target\"], ax=ax[0]).set_title(\"Actual Labels\")\nsns.scatterplot(x=umap['x_component'], y=umap[\"y_component\"], hue=umap[\"dbscan_pred\"], ax=ax[1]).set_title(\"DBSCAN predicted clusters\")\nfig.show()","042c8c00":"IF_pred = IsolationForest().fit_predict(X)","74b576d8":"umap[\"Isolation_forest\"] = IF_pred","1dee98c6":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(17, 8))\nsns.scatterplot(x=umap[\"x_component\"], y=umap[\"y_component\"], hue=umap[\"target\"], ax=ax[0]).set_title(\"Actual Labels\")\nsns.scatterplot(x=umap['x_component'], y=umap['y_component'], hue=umap[\"Isolation_forest\"], ax=ax[1]).set_title(\"Isolation Forest Clusters\")\nfig.show()","30eb2a74":"* Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for     visualisation similarly to t-SNE. T-SNE takes a lot of time to complete so I have used UMAP.\n\n* UMAP is fast and preserves the global structure i.e. the distance between data points within clusters and between clusters are preserved.\n\n* Check out the below blogs for UMAP: \n\n    https:\/\/towardsdatascience.com\/how-exactly-umap-works-13e3040e1668\n    \n    https:\/\/towardsdatascience.com\/tsne-vs-umap-global-structure-4d8045acba17","2f92bf14":"Observations:\n> 1. From the above garph we can see that most of the features of both the classes differ by a large margin.","f20aff66":"Both the classes are not separated properly.","d784d6af":"## TSNE","b77bdbbc":"## XGB with Cross validation","bea94c8d":"## UMAP","6306a9c8":"From the boosting and neural network we are getting approximately 80% recall.","38a5d800":"### Visualize each features using TSNE","0592be7a":"## Preprocessing","1cc9a1a1":"# Neural Networks","c40d01d1":"## Isolation Forest","2127d113":"## Visualization","0a029ae3":"From the above resutls XGBoost has highest precision and recall. We will continue with xgboost model.","3c352879":"# Credit Card Fraud Detection\n\n**Problem statement**: Identifying the fraud transaction\n\n**Description**: Credit card frauds belongs mainly to two groups of application and behavioural fraud.\n\n* **Application Fraud**: Application fraud takes place when, fraudsters apply new cards from bank or issuing\ncompanies using false or other's information. Multiple applications may be submitted by one user with one set of user details or different user with identical details. \n\n* **Behavioural Fraud**: In this method it takes user behaviour into account. Any activity that is different\nfrom the legitimate user behaviour will be considered as a possible fraud. It does not search for specific patterns\n\n**Dataset**: This dataset is highly imbalance. Only 0.172% of the transaction are fraud.\n\n**Modelling**: \n* Supervised Learning:\n\n    1. Boosting and Tree Algorithms\n    2. Neural networks\n\n* Unsupervised Learning:\n     \n    1. DBSCAN\n    2. IsolationForest\n    \n**Metric**:\n\n* Since this dataset is highly unbalance the dominant class can be easily classified. Therefore *precision* and *recall* are the best metric to use.\n\n![Precision and Recall](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/d37e557b5bfc8de22afa8aad1c187a357ac81bdb)\n\n\n**Other Techniques**\n\n* Oversampling\n* Undersampling\n   \n","01d6958d":"## Work in progress \u2692 ","739704bb":"## Modelling","7749206e":"## Baseline Models"}}