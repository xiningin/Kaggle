{"cell_type":{"a15f7e7d":"code","ac5cccad":"code","67b2b0b4":"code","a95a9a8a":"code","44f327dc":"code","f6778bdd":"code","c2cd0360":"code","2f11c796":"code","9d710779":"code","83176119":"code","da13e18c":"code","6c759494":"code","ad00992a":"code","0d48e166":"code","fd4a6530":"code","b903518c":"code","574c757c":"code","6a462f2a":"code","c237879e":"code","6777250b":"code","1589e196":"code","c4c11cb1":"markdown","a5057f0b":"markdown","a267702e":"markdown","10392035":"markdown","6e9f952e":"markdown","9481ecb7":"markdown","eab4d718":"markdown","c1f83a8d":"markdown","29354a3a":"markdown"},"source":{"a15f7e7d":"from numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)\nimport pandas as pd\nimport pyarrow.parquet as pq\nfrom tqdm import trange,tqdm\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport keras\nimport keras.backend as K\nfrom keras.layers import LSTM,Dropout,Dense,TimeDistributed,Conv1D,MaxPooling1D,Flatten,GlobalAveragePooling1D,AveragePooling1D,GlobalMaxPooling1D,BatchNormalization,Activation,Bidirectional\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import Sequential\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy import signal\nimport gc","ac5cccad":"compression_bucket_size = 300","67b2b0b4":"# Load Data\ndef load_data(parquet_data, csv_metadata):\n    pq_data = np.array(pq.read_pandas(('..\/input\/'+parquet_data)).to_pandas().T)\n    metadata = pd.read_csv('..\/input\/'+csv_metadata) \n    target = metadata['target'][:len(pq_data)].values\n    return pq_data,target\n\nfrom numpy.fft import *\ndef filter_signal(signal, threshold=1e8):\n    fourier = rfft(signal)\n    frequencies = rfftfreq(signal.size, d=20e-3\/signal.size)\n    fourier[frequencies > threshold] = 0\n    return irfft(fourier)\n\n# Subtract de-noised data from the raw signal data to process, or normalize(?), data. \ndef signal_processing(data):\n    return abs(np.round((data-filter_signal(data,threshold=1e3)),2))\n\n\ndef train_validate_split(data,data_target,validate_size):\n    metadata = pd.read_csv('..\/input\/metadata_train.csv') \n    signal_id_1 = list(metadata[metadata['target']==1]['signal_id'])\n    signal_id_0 = list(metadata[metadata['target']==0]['signal_id'])\n    train_1 = signal_id_1[0:int(len(signal_id_1)*(1-validate_size))]\n    validate_1 = signal_id_1[int(len(signal_id_1)*(1-validate_size)):]\n    train_0 = signal_id_0[0:int(len(signal_id_0)*(1-validate_size))]\n    validate_0 = signal_id_0[int(len(signal_id_0)*(1-validate_size)):]\n    \n    data_train = data[sorted(np.concatenate((train_0,train_1)))]\n    data_train_target = data_target[sorted(np.concatenate((train_0,train_1)))]\n    data_validate = data[sorted(np.concatenate((validate_0,validate_1)))]\n    data_validate_target = data_target[sorted(np.concatenate((validate_0,validate_1)))]  \n    \n    return data_train, data_train_target, data_validate, data_validate_target\n\n# Reduce sample size from 800000 to 800000\/bucket_size while not losing information by extracting features : std, mean, max, min\ndef compress_data_and_extract_features(data,bucket_size):\n    data_bucket_std, data_bucket_mean, data_bucket_percentile_0, data_bucket_percentile_1, data_bucket_percentile_25, data_bucket_percentile_50, data_bucket_percentile_75, data_bucket_percentile_99, data_bucket_percentile_100 = [],[],[],[],[],[],[],[],[]\n    \n        \n    for i in trange(data.shape[0]):\n        holder_std, holder_mean, holder_percentile,holder_0,holder_1,holder_25,holder_50,holder_75,holder_99,holder_100  = [],[],[],[],[],[],[],[],[],[]\n        #percentile_threshhold = np.percentile(abs(data[i]),99.97)\n        for j in range(0,data.shape[1],bucket_size):\n            holder_std.append(abs(data[i][j:(j+bucket_size)]).std())\n            holder_mean.append(abs(data[i][j:(j+bucket_size)]).mean())\n            holder_percentile=np.percentile(abs(data[i][j:(j+bucket_size)]),[0, 1, 25, 50, 75, 99, 100])\n            holder_0.append(holder_percentile[0])\n            holder_1.append(holder_percentile[1])\n            holder_25.append(holder_percentile[2])\n            holder_50.append(holder_percentile[3])\n            holder_75.append(holder_percentile[4])\n            holder_99.append(holder_percentile[5])\n            holder_100.append(holder_percentile[6])\n            #holder_peaks.append(sum(abs(data[i][j:(j+bucket_size)])>percentile_threshhold))           \n            \n        data_bucket_std.append(holder_std)\n        data_bucket_mean.append(holder_mean)\n        data_bucket_percentile_0.append(holder_0)\n        data_bucket_percentile_1.append(holder_1)\n        data_bucket_percentile_25.append(holder_25)\n        data_bucket_percentile_50.append(holder_50)\n        data_bucket_percentile_75.append(holder_75)\n        data_bucket_percentile_99.append(holder_99)\n        data_bucket_percentile_100.append(holder_100)\n        #data_bucket_peaks.append(holder_peaks)        \n    return np.asarray(data_bucket_std), np.asarray(data_bucket_mean), np.asarray(data_bucket_percentile_0), np.asarray(data_bucket_percentile_1), np.asarray(data_bucket_percentile_25), np.asarray(data_bucket_percentile_50), np.asarray(data_bucket_percentile_75), np.asarray(data_bucket_percentile_99), np.asarray(data_bucket_percentile_100)\n\n# Reshape Input Data of multiple features into single input for LSTM Input\ndef LSTM_reshape_dstack(combined_data_list):      \n    for i in range(len(combined_data_list)):\n        combined_data_list[i]=combined_data_list[i].reshape(combined_data_list[i].shape[0],combined_data_list[i].shape[1],1)        \n    return np.dstack(combined_data_list)","a95a9a8a":"# singal = 6\n# #  3,    4,    5,  201,  202,  228,  229,  230,  270,  271\n\n# plt.figure(figsize=(10,3))\n# t = np.arange(0,len(train[0]))\n# plt.plot(t,train[singal])\n# plt.show()","44f327dc":"train,target_train = load_data('train.parquet','metadata_train.csv')\nfor i in trange(len(train)):  \n    train[i]=signal_processing(train[i])\n#     train_min = min(train[i])\n#     train_max = max(train[i])\n#     train[i]= (train[i]-train_min)\/(train_max-train_min)*2-1\ndata_train, data_train_target, data_validate, data_validate_target = train_validate_split(train,target_train,0.3)\ndel train,target_train\ngc.collect()\n","f6778bdd":"#train_std,train_mean,train_max,train_25,train_50,train_75,train_peaks = compress_data_and_extract_features(data_train,compression_bucket_size)\ntrain_std,train_mean,train_0,train_1,train_25,train_50,train_75,train_99,train_100 = compress_data_and_extract_features(data_train,compression_bucket_size)\n#train_LSTM = LSTM_reshape_dstack([train_std,train_mean,train_max,train_25,train_50,train_75,train_peaks])\ntrain_LSTM = LSTM_reshape_dstack([train_std,train_mean,train_0,train_1,train_25,train_50,train_75,train_99,train_100])\ntrain_LSTM_backup = train_LSTM.copy()\ntrain_LSTM = train_LSTM.reshape(train_LSTM.shape[0],1,train_LSTM.shape[1],train_LSTM.shape[2])\n\ndel train_std,train_mean,train_0,train_1,train_25,train_50,train_75,train_99,train_100#,train_peaks\ngc.collect()\n\n#validate_std,validate_mean,validate_max,validate_25,validate_50,validate_75,validate_peaks = compress_data_and_extract_features(data_validate,compression_bucket_size)\nvalidate_std,validate_mean,validate_0,validate_1,validate_25,validate_50,validate_75,validate_99,validate_100 = compress_data_and_extract_features(data_validate,compression_bucket_size)\n#validate_LSTM = LSTM_reshape_dstack([validate_std,validate_mean,validate_0,validate_1,validate_25,validate_50,validate_75,validate_99,validate_100])\nvalidate_LSTM = LSTM_reshape_dstack([validate_std,validate_mean,validate_0,validate_1,validate_25,validate_50,validate_75,validate_99,validate_100])\nvalidate_LSTM_backup = validate_LSTM.copy()\nvalidate_LSTM = validate_LSTM.reshape(validate_LSTM.shape[0],1,validate_LSTM.shape[1],validate_LSTM.shape[2])\n\n#del validate_std,validate_mean,validate_max,validate_25,validate_50,validate_75,validate_peaks\ndel validate_std,validate_mean,validate_0,validate_1,validate_25,validate_50,validate_75,validate_99,validate_100\ngc.collect()\n\n# For easier readability\nnum_signals = train_LSTM.shape[0]\nnum_timesteps = train_LSTM.shape[2]\nnum_features = train_LSTM.shape[3]","c2cd0360":"def keras_auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc\ndef matthews_correlation(y_true, y_pred):\n    '''Calculates the Matthews correlation coefficient measure for quality\n    of binary classification problems.\n    '''\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    y_neg = 1 - y_pos\n\n    tp = K.sum(y_pos * y_pred_pos)\n    tn = K.sum(y_neg * y_pred_neg)\n\n    \n    fp = K.sum(y_neg * y_pred_pos)\n    fn = K.sum(y_pos * y_pred_neg)\n\n    numerator = (tp * tn - fp * fn)\n    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    return numerator\/(denominator+K.epsilon())","2f11c796":"#num_signals, num_timesteps, num_features = train_LSTM.shape[2]\n\nmodel = Sequential()\n# num_timesteps = 800000\n# num_features = 6\n\n# Define CNN Model\nmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=6), input_shape=(None,num_timesteps,num_features)))\nmodel.add(TimeDistributed(Activation('relu')))\nmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=6)))\nmodel.add(TimeDistributed(Activation('relu')))\nmodel.add(TimeDistributed(GlobalMaxPooling1D()))\n\n\nmodel.add(TimeDistributed(Flatten()))\n# Define LSTM Model\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\ncallbacks = [EarlyStopping(monitor='val_matthews_correlation', patience=20),\n            ModelCheckpoint(filepath='best_model.h5', monitor='val_matthews_correlation', save_best_only=True,mode='max')]\n#callbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='val_matthews_correlation', save_best_only=True,mode='max')]","9d710779":"\nmodel.fit(train_LSTM, data_train_target, validation_data=(validate_LSTM[0:1308],data_validate_target[0:1308]),epochs=200, batch_size=128, verbose=1,callbacks=callbacks)","83176119":"\nmodel.fit(train_LSTM, data_train_target, validation_data=(validate_LSTM[1308:],data_validate_target[1308:]),epochs=200, batch_size=128, verbose=1,callbacks=callbacks)","da13e18c":"callbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='matthews_correlation', save_best_only=True,mode='max')]\n\nmodel.fit(np.concatenate((train_LSTM,validate_LSTM)), np.concatenate((data_train_target,data_validate_target)),epochs=10, batch_size=128, verbose=1,callbacks=callbacks)","6c759494":"# #num_signals, num_timesteps, num_features = train_LSTM.shape[2]\n\n# model = Sequential()\n# # num_timesteps = 800000\n# # num_features = 6\n\n# # Define CNN Model\n# model.add(TimeDistributed(Conv1D(filters=32, kernel_size=2), input_shape=(None,num_timesteps,num_features)))\n# model.add(TimeDistributed(Activation('relu')))\n# model.add(TimeDistributed(MaxPooling1D(pool_size=6)))\n\n\n# model.add(TimeDistributed(Conv1D(filters=16, kernel_size=2)))\n# model.add(TimeDistributed(Activation('relu')))\n# model.add(TimeDistributed(MaxPooling1D(pool_size=6)))\n\n# model.add(TimeDistributed(Conv1D(filters=8, kernel_size=2)))\n# model.add(TimeDistributed(Activation('relu')))\n# model.add(TimeDistributed(MaxPooling1D(pool_size=6)))\n\n# model.add(TimeDistributed(Conv1D(filters=4, kernel_size=2)))\n# model.add(TimeDistributed(Activation('relu')))\n# model.add(TimeDistributed(MaxPooling1D(pool_size=6)))\n\n\n\n# # model.add(TimeDistributed(GlobalMaxPooling1D()))\n\n# # model.add(TimeDistributed(Conv1D(filters=16, kernel_size=4)))\n# # model.add(TimeDistributed(Activation('relu')))\n# # #model.add(TimeDistributed(BatchNormalization()))\n# # model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n# # model.add(Dropout(0.2))\n\n# # model.add(TimeDistributed(Conv1D(filters=16, kernel_size=2)))\n# # model.add(TimeDistributed(Activation('relu')))\n# # #model.add(TimeDistributed(BatchNormalization()))\n# # model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n# # model.add(Dropout(0.2))\n\n\n\n# model.add(TimeDistributed(Flatten()))\n# # Define LSTM Model\n# model.add(LSTM(128))\n# model.add(Dropout(0.5))\n# model.add(Dense(32, activation='relu'))\n# model.add(Dropout(0.5))\n# model.add(Dense(1, activation='sigmoid'))\n# model.summary()\n\n# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n\n# #callbacks = [EarlyStopping(monitor='val_matthews_correlation', patience=50),\n# #             ModelCheckpoint(filepath='best_model.h5', monitor='val_matthews_correlation', save_best_only=True,mode='max')]\n# callbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='val_matthews_correlation', save_best_only=True,mode='max')]\n# model.fit(train_LSTM, data_train_target, validation_data=(validate_LSTM,data_validate_target),epochs=300, batch_size=64, verbose=1,callbacks=callbacks)\n","ad00992a":"del train_LSTM, data_train_target, validate_LSTM, data_validate_target\ngc.collect()\n\n# model.save_weights('model1.hdf5')\nmodel.load_weights('best_model.h5')","0d48e166":"def divide_test_data(start,end):\n    test_metadata = pd.read_csv('..\/input\/metadata_test.csv') \n    test_index_list = test_metadata['signal_id'][start:end].values\n    test_index_lists = []\n    for i in range(len(test_index_list)):\n        test_index_lists.append(str(test_index_list[i]))\n    return test_index_lists\n    test_index_lists_master.append(test_index_lists)\n    \n# Process test data in 6 chunks due to memory issues\ntest_index_lists_master=[]\ntest_index_lists_master.append(divide_test_data(0,3390))\ntest_index_lists_master.append(divide_test_data(3390,6780))\ntest_index_lists_master.append(divide_test_data(6780,10170))\ntest_index_lists_master.append(divide_test_data(10170,13560))\ntest_index_lists_master.append(divide_test_data(13560,16950))\ntest_index_lists_master.append(divide_test_data(16950,20337))","fd4a6530":"y_pred = []\nfor i in range(len(test_index_lists_master)):\n    test = np.array(pq.read_pandas(('..\/input\/test.parquet'),columns=test_index_lists_master[i]).to_pandas().T)    \n    for i in range(len(test)):\n        test[i]=signal_processing(test[i])    \n#         test_min = min(test[i])\n#         test_max = max(test[i])\n#         test[i]= (test[i]-test_min)\/(test_max-test_min)*2-1    \n    \n    \n    #test_std,test_mean,test_max,test_25,test_50,test_75,test_peaks = compress_data_and_extract_features(test,compression_bucket_size)\n    test_std,test_mean,test_0,test_1,test_25,test_50,test_75,test_99,test_100 = compress_data_and_extract_features(test,compression_bucket_size)\n    del test\n    #test_LSTM = LSTM_reshape_dstack([test_std,test_mean,test_max,test_25,test_50,test_75,test_peaks])\n    test_LSTM = LSTM_reshape_dstack([test_std,test_mean,test_0,test_1,test_25,test_50,test_75,test_99,test_100])\n    #del test_std,test_mean,test_max,test_25,test_50,test_75,test_peaks\n    del test_std,test_mean,test_0,test_1,test_25,test_50,test_75,test_99,test_100\n    test_LSTM = test_LSTM.reshape(test_LSTM.shape[0],1,test_LSTM.shape[1],test_LSTM.shape[2])\n    y_pred.append(model.predict(test_LSTM))\n    del test_LSTM\n    gc.collect()   ","b903518c":"y_pred_final=np.concatenate((y_pred[0],y_pred[1],y_pred[2],y_pred[3],y_pred[4],y_pred[5]))\n# Do this temporarily since target and number of signals didn't match.\n#y_pred_final=np.append(y_pred_final,0)\n","574c757c":"test_metadata = pd.read_csv('..\/input\/metadata_test.csv') \ntest_metadata['target']=y_pred_final\ntest_metadata=test_metadata.drop(columns=['phase','id_measurement'])","6a462f2a":"threshhold=0.5\ntest_metadata['target'][test_metadata['target']>=threshhold]=1\ntest_metadata['target'][test_metadata['target']<threshhold]=0","c237879e":"test_metadata['target']=test_metadata.target.astype(int)","6777250b":"test_metadata.to_csv('submission.csv',index=False)","1589e196":"print(len(y_pred[0]))\nprint(sum(y_pred[0]))\n\nprint(len(y_pred[1]))\nprint(sum(y_pred[1]))\n\nprint(len(y_pred[2]))\nprint(sum(y_pred[2]))\n\nprint(len(y_pred[3]))\nprint(sum(y_pred[3]))\n\nprint(len(y_pred[4]))\nprint(sum(y_pred[4]))\n\nprint(len(y_pred[5]))\nprint(sum(y_pred[5]))\n\n","c4c11cb1":"Explanation on prepararing data for LSTM training.  \nWe have four features, or dimensions, of 1D signals. In order to stack them, you need to use [numpy.dstack](https:\/\/docs.scipy.org\/doc\/numpy-1.15.0\/reference\/generated\/numpy.dstack.html). Per dstack's document, 1D signals first need to be reshaped to (1,timesteps,1) to (timesteps,).  \nBut, LSTM takes data in 4D, so we actually need (num_samples,1,timesteps,dimensions). But if we have 4D form first, dstack wouldn't work. Therefore, we first stack the four features using numpy.dstack, then we reshape the data once more to (num_samples,1,timesteps,dimensions) for LSTM.  \n* num_samples = number of signals \n* 1 = 1 because it's 1D signal. If it was a 2D case, it wouldn't been width or height of the image, and not 1\n* timesteps = 800000 or whatever it's reduced to per feature extraction.\n* dimensions = will be result of numpy.dstack depending on how many features you have","a5057f0b":"# Define Model\n[TimeDistributed Documentation](https:\/\/keras.io\/layers\/wrappers\/)  \n[Convolutional Layers Documentation](https:\/\/keras.io\/layers\/convolutional\/)  \n[Pooling Layers](https:\/\/keras.io\/layers\/pooling\/)  \n[How to use TimeDistributed wrapper for LSTM](https:\/\/machinelearningmastery.com\/timedistributed-layer-for-long-short-term-memory-networks-in-python\/)  \n[CNN LSTM Tutorial](https:\/\/machinelearningmastery.com\/cnn-long-short-term-memory-networks\/)  \n[Number of LSTM Units](https:\/\/datascience.stackexchange.com\/questions\/16350\/how-many-lstm-cells-should-i-use\/18049)","a267702e":"# Train Validate Split","10392035":"[PHD Thesis](http:\/\/dspace.vsb.cz\/bitstream\/handle\/10084\/133114\/VAN431_FEI_P1807_1801V001_2018.pdf)  \n[Preprint](https:\/\/www.dropbox.com\/s\/2ltuvpw1b1ms2uu\/A%20Complex%20Classification%20Approach%20of%20Partial%20Discharges%20from%20Covered%20Conductors%20in%20Real%20Environment%20%28preprint%29.pdf?dl=0)  \n[Advance Pandas Tricks and Techniques](https:\/\/www.kaggle.com\/ashishpatel26\/advance-pandas-tricks-and-techniques)  \n[CNN + LSTM for Signal Classification LB 0.513](https:\/\/www.kaggle.com\/afajohn\/cnn-lstm-for-signal-classification-lb-0-513)","6e9f952e":"# Functions","9481ecb7":"# Execute Data Preparation for LSTM Input","eab4d718":"# Predict using Test Data","c1f83a8d":"# Prepare Data for LSTM Input\n[Prepare input data for LSTM layer](https:\/\/machinelearningmastery.com\/reshape-input-data-long-short-term-memory-networks-keras\/)  \n[How to Prepare Univariate Time Series Data for Long Short-Term Memory Networks](https:\/\/machinelearningmastery.com\/prepare-univariate-time-series-data-long-short-term-memory-networks\/)","29354a3a":"# Import Libraries"}}