{"cell_type":{"3cd6eb0d":"code","cc40c6f5":"code","c24c63fa":"code","d433452a":"code","789c59cd":"code","da4d9e59":"code","0c9cb462":"code","e549c7ba":"code","d72d19ef":"code","932988e5":"code","acba2ece":"code","bd1f007a":"code","8c41d028":"code","7ef13bb3":"code","9ea19b4e":"code","4f86c4b1":"code","4925ad96":"code","154f4a17":"code","d748c55c":"code","4bfa93f0":"code","4d5d00f2":"markdown","2c8a6a26":"markdown","f1b2ef25":"markdown","cac8bcc7":"markdown","3ce3b6b6":"markdown","4a35f6e2":"markdown","d0c9f16f":"markdown","ca94e6d4":"markdown","97cf664f":"markdown","2f728dd7":"markdown","f5b8081a":"markdown","c900a7f6":"markdown","545364ab":"markdown","3d170cf2":"markdown","9224446a":"markdown","1009579c":"markdown","155b65a0":"markdown","08bab37f":"markdown","89b34dd3":"markdown"},"source":{"3cd6eb0d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc40c6f5":"#!pip install libsvm2csv","c24c63fa":"#import libsvm2csv\n#from libsvm2csv import convert\n#convert('\/kaggle\/input\/tv-news-channel-commercial-detection-dataset\/BBC.txt','BBC.csv')","d433452a":"val = pd.read_csv('..\/input\/bbc-television-advertisementcsv\/BBC.csv')","789c59cd":"val.head()","da4d9e59":"val['target'].value_counts()","0c9cb462":"val['target'] = val['target'].replace(-1,0)","e549c7ba":"val['target'].value_counts()","d72d19ef":"Y = val['target']\nX = val.drop('target',axis = 1)","932988e5":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfor col in val.columns:\n    if col in ['target','index','qid']:\n        continue\n    plt.figure()\n    sns.distplot(X[col],kde = False)","acba2ece":"from sklearn.model_selection import train_test_split\nX_train,X_valid,\\\ny_train,y_valid = train_test_split(X,Y,test_size=0.2,random_state=42)","bd1f007a":"import numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nX_train_arr = np.array(X_train)\nY_train_arr = np.array(y_train)\nX_valid_arr = np.array(X_valid)\nY_valid_arr = np.array(y_valid)\n\nfrom sklearn.svm import SVC\nclf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nclf.fit(X_train_arr, Y_train_arr)","8c41d028":"from sklearn.metrics import classification_report\nY_pred_valid = clf.predict(X_valid_arr)\nprint(classification_report(y_valid,Y_pred_valid))","7ef13bb3":"from sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.svm import NuSVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler","9ea19b4e":"clf = make_pipeline(StandardScaler(), LinearSVC(class_weight = 'balanced',\n                                                dual = False,\n                                                max_iter = 1000))\nclf.fit(X_train_arr, Y_train_arr)","4f86c4b1":"from sklearn.metrics import classification_report\nY_pred_valid = clf.predict(X_valid_arr)\nprint(classification_report(y_valid,Y_pred_valid))","4925ad96":"clf = make_pipeline(StandardScaler(), SVC(kernel='poly',\n                                          degree = 3,\n                                          gamma='auto',\n                                          class_weight = 'balanced'\n                                          ))\nclf.fit(X_train_arr, Y_train_arr)\n\nY_pred_valid = clf.predict(X_valid_arr)\nprint(classification_report(y_valid,Y_pred_valid))","154f4a17":"clf = make_pipeline(StandardScaler(), SVC(kernel='poly',\n                                          degree = 2,\n                                          gamma='auto',\n                                          class_weight = 'balanced'\n                                          ))\nclf.fit(X_train_arr, Y_train_arr)\nY_pred_valid = clf.predict(X_valid_arr)\nprint(classification_report(y_valid,Y_pred_valid))","d748c55c":"clf = make_pipeline(StandardScaler(), NuSVC(nu = 0.2, gamma = 'auto'))\nclf.fit(X_train_arr, Y_train_arr)\n\nY_pred_valid = clf.predict(X_valid_arr)\nprint(classification_report(y_valid,Y_pred_valid))","4bfa93f0":"clf = make_pipeline(StandardScaler(), NuSVC(nu = 0.9,kernel = 'sigmoid',gamma = 'auto'))\nclf.fit(X_train_arr, Y_train_arr)\n\nY_pred_valid = clf.predict(X_valid_arr)\nprint(classification_report(y_valid,Y_pred_valid))","4d5d00f2":"So, degree three even performs worse than the linear one. Let's try degree = 2 once.\n### second stop: degree = 2","2c8a6a26":"So as we have seen upto this that the rbf kernel is the best performer. We will try to improve upon that by trying out NuSVC and trying to tune the nu parameter.","f1b2ef25":"nu = 0.2 F-score 0.89 for class 0, 0.86 for class 1, 0.88 macro-average accuracy<br\/>\nnu = 0.25 F-score 0.90 for class 0, 0.87 for class 1,0.88 macro-average accuracy<br\/>\nnu = 0.5 F-score 0.86 for class 0, 0.83 for class 1, 0.84 macro-average accuracy<br\/>\nnu = 0.3 F-score 0.89 for class 0, 0.87 for class 1, 0.88 macro-average accuracy<br\/>\nnu = 0.35 F-score 0.89 for class 0, 0.87 for class 1, 0.88 macro-average accuracy<br\/>\n\nSo around nu = 0.2 gives the best performance.","cac8bcc7":"First we are trying out 'rbf' kernel:","3ce3b6b6":"### <a id = 'section1'>What is SVM?<\/a>\n![SVM hyperplane picture](https:\/\/d2h0cx97tjks2p.cloudfront.net\/blogs\/wp-content\/uploads\/sites\/2\/2019\/07\/introduction-to-SVM.png)\nSupport Vector Machines are a type of supervised machine learning algorithm that helps you in data classification and regression analysis. While they can be used for regression, SVM is mostly used for classification. In SVM, we consider the features in a vector space, with n-dimension if there are n-features. Then the value of each feature also serves as the value of the specific coordinate. Then, to classify this n-dimensional data, we find the ideal hyperplane that differentiates between the two classes.<br\/>\nSo in this n-dimensional space, we try to find support vectors, which are basically lines in the space which are closer to points from both classes. After finding such support vectors, we optimize a hyperplane somewhat from the middle of them; such that the distance between the hyperplane and the support vectors are at maximum.<br\/>\n![support vectors and distance with hyperplane](https:\/\/d2h0cx97tjks2p.cloudfront.net\/blogs\/wp-content\/uploads\/sites\/2\/2019\/07\/SVM-Frontier-method.png)\nThis distance optimization is the main problem in SVM; as well as here is the trick with different kernels help; as different kernels correspond to different ways to measure the distance and therefore different values in the distance optimization.<br\/>\nFor our work, this much understanding is enough to begin with. For a much better understanding of how SVM is implemented and how it works; refer to this [data flair article](https:\/\/data-flair.training\/blogs\/svm-support-vector-machine-tutorial\/). <br\/>\nAlso, as we are going to handle a number of different kernels; i.e. 'rbf','sigmoid','polynomial' and 'linear'; so here are some articles to read up about the different kernels and how they work.<br\/>\n### References:\n[what are the different kernels in sklearn?](https:\/\/scikit-learn.org\/stable\/auto_examples\/svm\/plot_svm_kernels.html)<br\/>\n[major kernel functions in svm](https:\/\/www.geeksforgeeks.org\/major-kernel-functions-in-support-vector-machine-svm\/)<br\/>\nNow, with the tune set up for svm in theory; let's begin our practical experimentation.","4a35f6e2":"### <a id = 'poly'>Let's try a polynomial kernel<\/a>:\nIn SVM, we can try polynomial kernels to try and improve the accuracy. As we can see, the rbf kernel did a bit better (2% better F1-score) than the linear version. Now, we will try out polynomial kernels of varying degrees.","d0c9f16f":"### <a id = 'linearsvc'>Try LinearSVC on this model<\/a>\nFirst we tried with 10000 iterations. The liblinear solver didn't converge. So we increased the time to 20000 iterations. That also didn't resolve the issue. Then I read this [stackoverflow answer](https:\/\/stackoverflow.com\/questions\/52670012\/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati); and realized that dual has to be set to False; and that resolved the issue.","ca94e6d4":"### SVC results:\n               precision    recall  f1-score   support\n\n           0       0.85      0.93      0.89      1876\n           1       0.91      0.82      0.86      1668\n\n    accuracy                           0.88      3544\n    macro avg      0.88      0.87      0.88      3544\n    weighted avg   0.88      0.88      0.88      3544\n","97cf664f":"## <a id = 'section2'>modeling approach<\/a>:\nAs I told in the dataset discussion the original paper where this dataset was explored used svm techniques. We will also use some svm techniques to explore this dataset and do some basic modeling. Later on we can try other features.<br\/>\nHere are the following models we are going to try out in this notebook:<br\/>\n(1) [Linearsvc](#linearsvc)<br\/>\n(2) [Nusvc](#nusvc)<br\/>\n(3) [polynomial svm](#poly)<br\/>\nYou can read about [SVM sklearn here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html).<br\/>\n","2f728dd7":"### First step: degree = 3","f5b8081a":"## SVM exploration with television-commercial detection:\nThis notebook is mainly about exploring the bbc-television-advertisement dataset. In this notebook, we explore the different versions and variations of **Support Vector Machine**, analyze the results and comment about how the different versions react in the way they do.<br\/>\nContents:<br\/>\n(1) [What is SVM? a theoretical introduction](#section1)<br\/>\n(2) [Modeling approaches](#section2)<br\/>","c900a7f6":"### read more about libsvm2csv [here](https:\/\/github.com\/andribas404\/libsvm2csv)","545364ab":"### <a id = 'nusvc'>Trying out NuSVC<\/a>","3d170cf2":"Trying out 'sigmoid' kernel:","9224446a":"### loading the bbc dataset","1009579c":"So sigmoid kernel performed at around 80%. Not much better. So we will say, rbf kernel performs best with NuSVC with nu = 0.2 - 0.3 and achieves 88% F1-score.","155b65a0":"the original bbc.csv as turned from libsvm format gives tokenization error. Therefore I downloaded the dataset; and checked it. Issue is that first line will have one less element in the created csv and therefore creates tokenization error in csv file reading. therefore we are doing<br\/> skiprows =\\[1\\] to read the csv properly. that reads the file incorrectly too. So we finally have uploaded the file from our local and using it.","08bab37f":"### LinearSVC results:\n\n                precision  recall   f1-score   support\n\n           0       0.84      0.91      0.87      1876\n           1       0.89      0.80      0.84      1668\n\n    accuracy                           0.86      3544\n    macro avg      0.86      0.85      0.86      3544\n    weighted avg   0.86      0.86      0.86      3544\n","89b34dd3":"So degree 2 also didn't work out nice. It achieved the same result as the linear one. we will not try the polynomial kernel anymore; as clearly the boundary is almost linear.\nResult below:<br\/>\n          \n                precision   recall  f1-score   support\n\n           0       0.85      0.90      0.87      1876\n           1       0.88      0.82      0.85      1668\n\n    accuracy                           0.86      3544\n    macro avg      0.87      0.86      0.86      3544\n    weighted avg   0.86      0.86      0.86      3544"}}