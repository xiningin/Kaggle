{"cell_type":{"4118c26a":"code","35cee7a2":"code","d0f8ecd0":"code","6e843d2f":"code","8258a1a5":"code","3ccb27ce":"code","b315a120":"code","5566c9c6":"code","ab76efab":"code","5bf3737b":"code","db241fa3":"code","647763d4":"code","258638a5":"code","d65e32e8":"code","3c6b0e8d":"code","1dc2c91f":"code","7d1c76d3":"code","fec505b2":"code","f99f052f":"code","e23fe814":"code","311bd182":"code","3b41dc84":"code","2ab25042":"code","f096abff":"code","10f21b7c":"markdown","363b590f":"markdown","5f467c39":"markdown","0d1aaffc":"markdown","0362c678":"markdown","31df30a3":"markdown","66592911":"markdown","d7a22710":"markdown","fcae36d8":"markdown","fa2c3b5d":"markdown","70881e2c":"markdown","1b684b37":"markdown","06160f85":"markdown","6a36b062":"markdown","43e375e8":"markdown","149ee830":"markdown","264c2e22":"markdown","f46d410e":"markdown","17570662":"markdown","69a6b048":"markdown","157d6550":"markdown","60cf7627":"markdown","f1b85f95":"markdown","4ce11c83":"markdown","23b4b1a1":"markdown","2fb0ff32":"markdown","bb985df0":"markdown","63a94859":"markdown"},"source":{"4118c26a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score","35cee7a2":"# read in data\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","d0f8ecd0":"df['Class'].value_counts()","6e843d2f":"print(\"Data count Belongs to class 0 :\",df.Class[df['Class']==0].count())\nprint(\"Data count Belongs to class 1 :\",df.Class[df['Class']==1].count())","8258a1a5":"sns.countplot(data=df,x='Class')","3ccb27ce":"y = df.Class\nX = df.drop('Class', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n\n# DummyClassifier to predict only target 0\ndummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\ndummy_pred = dummy.predict(X_test)\n\n# checking unique labels\nprint('Unique predicted labels: ', (np.unique(dummy_pred)))\n\n# checking accuracy\nprint('Test score: ', accuracy_score(y_test, dummy_pred))","b315a120":"dr1=pd.DataFrame(dummy_pred)\nprint(int(dr1[dr1==1].count()))\ndr2=pd.DataFrame(y_test)\nprint(int(dr2[dr2==1].count()))","5566c9c6":"lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n \n# Predict on training set\nlr_pred = lr.predict(X_test)\n\n# Checking accuracy\nprint(\"accuracy :\",accuracy_score(y_test, lr_pred))","ab76efab":"dr1=pd.DataFrame(lr_pred)\nprint(int(dr1[dr1==1].count()))\ndr2=pd.DataFrame(y_test)\nprint(int(dr2[dr2==1].count()))","5bf3737b":"print(accuracy_score(y_test, lr_pred))\nprint(f1_score(y_test, lr_pred))\nprint(recall_score(y_test, lr_pred))","db241fa3":"from xgboost import XGBClassifier\n\nxgb_model = XGBClassifier().fit(X_train, y_train)\n\n# predict\nxgb_y_predict = xgb_model.predict(X_test)\n\n# accuracy score\nxgb_score = accuracy_score(xgb_y_predict, y_test)\n\nprint('Accuracy score is:', xgb_score)\n","647763d4":"print(accuracy_score(y_test, xgb_y_predict))\nprint(f1_score(y_test, lr_pred))\nprint(recall_score(y_test, xgb_y_predict))","258638a5":"from sklearn.ensemble import RandomForestClassifier\n\n# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)","d65e32e8":"print(accuracy_score(y_test, rfc_pred))\nprint(f1_score(y_test, rfc_pred))\nprint(recall_score(y_test, rfc_pred))","3c6b0e8d":"# class count\ndata=df.copy()\nclass_count_0, class_count_1 = data['Class'].value_counts()\n\n# Separate class\nclass_0 = data[data['Class'] == 0]\nclass_1 = data[data['Class'] == 1]# print the shape of the class\nprint('class 0:', class_0.shape)\nprint('class 1:', class_1.shape)","1dc2c91f":"class_0_under = class_0.sample(class_count_1)\n\ntest_under = pd.concat([class_0_under, class_1], axis=0)\n\nprint(\"total class of 1 and0:\",test_under['Class'].value_counts())# plot the count after under-sampeling\ntest_under['Class'].value_counts().plot(kind='bar', title='count (target)')","7d1c76d3":"class_1_over = class_1.sample(class_count_0, replace=True)\n\ntest_over = pd.concat([class_1_over, class_0], axis=0)\n\nprint(\"total class of 1 and 0:\",test_under['Class'].value_counts())# plot the count after under-sampeling\ntest_over['Class'].value_counts().plot(kind='bar', title='count (target)')","fec505b2":"data","f99f052f":"df.groupby(by='Class').count()['Amount']","e23fe814":"\nfrom imblearn.under_sampling import RandomUnderSampler\ndata=df.copy()\ny=data['Class']\nx=data.drop('Class',axis=1)\nrus = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable\nx_rus, y_rus = rus.fit_resample(x,y)\n","311bd182":"y_rus.plot(kind='hist')","3b41dc84":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=42)\n\nvariablex_ros, y_ros = ros.fit_resample(x, y)\ndf2 = pd.concat([variablex_ros, y_ros], axis=1)\n\ny_ros.plot(kind='hist')","2ab25042":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedKFold\n# generate 2 class dataset\n\nX, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# enumerate the splits and summarize the distributions\nfor train_ix, test_ix in kfold.split(X, y):\n\t# select rows\n\ttrain_X, test_X = X[train_ix], X[test_ix]\n\ttrain_y, test_y = y[train_ix], y[test_ix]\n\t# summarize train and test composition\n\ttrain_0, train_1 = len(train_y[train_y==0]), len(train_y[train_y==1])\n\ttest_0, test_1 = len(test_y[test_y==0]), len(test_y[test_y==1])\n\tprint('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))","f096abff":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedKFold\n# generate 2 class dataset\nX, y = make_classification(n_samples=1000, n_classes=2, weights=[0.5, 0.5] , random_state=1)\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# enumerate the splits and summarize the distributions\nfor train_ix, test_ix in kfold.split(X, y):\n\t# select rows\n\ttrain_X, test_X = X[train_ix], X[test_ix]\n\ttrain_y, test_y = y[train_ix], y[test_ix]\n\t# summarize train and test composition\n\ttrain_0, train_1 = len(train_y[train_y==0]), len(train_y[train_y==1])\n\ttest_0, test_1 = len(test_y[test_y==0]), len(test_y[test_y==1])\n\tprint('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))","10f21b7c":"One way to fight imbalance data is to generate new samples in the minority classes. The most naive strategy is to generate new samples by randomly sampling with replacement of the currently available samples. The RandomOverSampler offers such a scheme.","363b590f":"The solution is to not split the data randomly when using k-fold cross-validation or a train-test split.\n\nSpecifically, we can split a dataset randomly, although in such a way that maintains the same class distribution in each subset. This is called stratification or stratified sampling and the target variable (y), the class, is used to control the sampling process.\n\nFor example, we can use a version of k-fold cross-validation that preserves the imbalanced class distribution in each fold. It is called stratified k-fold cross-validation and will enforce the class distribution in each split of the data to match the distribution in the complete training dataset.","5f467c39":"# 3.4. Random over-sampling with imblearn","0d1aaffc":"# 3.3. Random under-sampling with imblearn\n\nRandomUnderSampler is a fast and easy way to balance the data by randomly selecting a subset of data for the targeted classes. Under-sample the majority class(es) by randomly picking samples with or without replacement.","0362c678":"# Imbalanced classes \nare a common problem in machine learning classification where there are a disproportionate ratio of observations in each class. Class imbalance can be found in many different areas including medical diagnosis, spam filtering, and fraud detection.","31df30a3":"While in every machine learning problem, it\u2019s a good rule of thumb to try a variety of algorithms, it can be especially beneficial with imbalanced datasets. Decision trees frequently perform well on imbalanced data. They work by learning a hierarchy of if\/else questions and this can force both classes to be addressed.","66592911":"# what is imbalence dataset\n\nWhen observation in one class is higher than the observation in other classes then there exists a class imbalance. Example: To detect fraudulent credit card transactions. As you can see in the below graph fraudulent transaction is around 400 when compared with non-fraudulent transaction around 90000.\n\n","d7a22710":"1. Use the right evaluation metrics\n2. Select the ensamble based Model\n3. Resample the dataset\n4. Stratified k-fold cross-validation","fcae36d8":"# 1. Change the performance metric","fa2c3b5d":"The easiest way to successfully generalize a model is by using more data. The problem is that out-of-the-box classifiers like logistic regression or random forest tend to generalize by discarding the rare class. One easy best practice is building n models that use all the samples of the rare class and n-differing samples of the abundant class. Given that you want to ensemble 10 models, you would keep e.g. the 1.000 cases of the rare class and randomly sample 10.000 cases of the abundant class. Then you just split the 10.000 cases in 10 chunks and train 10 different models.\n\n![](https:\/\/www.kdnuggets.com\/wp-content\/uploads\/imbalanced-data-2.png)\n\nit can e done in three way:\n1. Rare and Frequent class have same proportions\n2. Requent Class may have ratio in increaseing order like ,1:3 then 1:5, 1:7 somting like that\n3. Frequent class can  be cluster in diffrent diffrent group and the can be send to ensamble technic","70881e2c":"# The Problem with Accuracy\n\nHere we can use the DummyClassifier to always predict \u201cnot fraud\u201d just to show how misleading accuracy can be.","1b684b37":"![](https:\/\/miro.medium.com\/max\/725\/1*H6XodlitlGDl9YdbwaZLMw.png)","06160f85":"**Two type:**\n\n**3.1. Up Sampling**\n\n**3.2. Down Sampling**","6a36b062":"# 4. Stratified k-fold cross-validation","43e375e8":"A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and\/or adding more examples from the minority class (over-sampling).","149ee830":"# 2. Change the algorithm\n","264c2e22":"# The Problem with Imbalanced Classes\n\nMost machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce error.","f46d410e":"As we saw above, accuracy is not the best metric to use when evaluating imbalanced datasets as it can be very misleading. Metrics that can provide better insight include:\n\n\n**1. Confusion Matrix:** \na table showing correct predictions and types of incorrect predictions.\n\n**2. Precision:** \nthe number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier\u2019s exactness. Low precision indicates a high number of false positives.\n\n**3. Recall:** \nthe number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier\u2019s completeness. Low recall indicates a high number of false negatives.\n\n**4. F1 Score:** \nthe weighted average of precision and recall.\nLet\u2019s see what happens when we apply these F1 and recall scores to our logistic regression from above.","17570662":"These scores don\u2019t look quite so impressive. Let\u2019s see what other methods we might try to improve our new metrics.","69a6b048":"# 2.1 Ensemble different resampled datasets","157d6550":"Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch).\n\nThe simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfiting.\n\nIn under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n\nLet\u2019s implement this with the credit card fraud detection example.\n\nWe will start by separating the class that will be 0 and class 1.","60cf7627":"Another example of these kind technique is Randomforest, lets have a look....","f1b85f95":"Other then random sampling number of more sophisticated resampling techniques have been proposed in the scientific literature.\n\nFor example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.","4ce11c83":"Balance data with the imbalanced-learn python module\n","23b4b1a1":"# 3.1. Random Under-Sampling\n\nUndersampling can be defined as removing some observations of the majority class. This is done until the majority and minority class is balanced out.\n\nUndersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback to undersampling is that we are removing information that may be valuable.","2fb0ff32":"# 3. Resampling Techniques \u2014 ","bb985df0":"# 3.2. Random Over-Sampling\n\nOversampling can be defined as adding more copies to the minority class. Oversampling can be a good choice when you don\u2019t have a ton of data to work with.\n\nA con to consider when undersampling is that it can cause overfitting and poor generalization to your test set.","63a94859":"**Let\u2019s take a look at some popular methods for dealing with class imbalance.**"}}