{"cell_type":{"7a73d549":"code","f32c089b":"code","bf5b4f1f":"code","ab5e78cd":"code","e1500e82":"code","857771ad":"code","a1c3faea":"code","d4a725ed":"code","774d4a22":"code","e0c85b61":"code","1dfc9a5e":"code","5ef7c4ce":"code","eeabbdfa":"code","37aebe4a":"code","3e99f6cf":"code","dbc1f453":"code","cc33b5b9":"code","a9a085d2":"markdown","dbb5ef2b":"markdown","0c49a1e7":"markdown","74e60303":"markdown","ce17efb1":"markdown","6f80bffc":"markdown","c73be28e":"markdown","5bd062a6":"markdown","11fbc20c":"markdown","61bd6606":"markdown","8948bcc3":"markdown","2df25fe1":"markdown","5240ff04":"markdown"},"source":{"7a73d549":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom numpy import concatenate\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.python.keras.layers import Dense, LSTM , Dropout\nfrom tensorflow.python.keras import Sequential\nfrom math import sqrt;","f32c089b":"#dataset preprocessing\n#datset loading dropping na values\n\ndataset= pd.read_csv('..\/input\/beijing-pm25-data-data-set\/PRSA_data_2010.1.1-2014.12.31.csv');\ndataset=dataset.dropna();","bf5b4f1f":"dataset=dataset.drop('No',axis=1);\ndataset=dataset.drop('year',axis=1);\ndataset=dataset.drop('month',axis=1);\ndataset=dataset.drop('day',axis=1);\ndataset=dataset.drop('hour',axis=1);\ndataset.head();\n\nvalues=dataset.values;","ab5e78cd":"# specify columns to plot\ngroups = [0, 1, 2, 3, 5, 6, 7]\ni = 1\n# plotting each column\npyplot.figure()\nfor group in groups:\n\tpyplot.subplot(len(groups), 1, i)\n\tpyplot.plot(values[:, group])\n\tpyplot.title(dataset.columns[group], y=0.5, loc='right')\n\ti += 1\npyplot.show()","e1500e82":"# integer encode direction\nencoder = LabelEncoder()\nvalues[:,4] = encoder.fit_transform(values[:,4])\n# ensure all data is float\nvalues = values.astype('float32')","857771ad":"# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)","a1c3faea":"# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg","d4a725ed":"# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\nprint(reframed.head())","774d4a22":"# split into train and test sets\nvalues = reframed.values\nn_train_hours = 365 * 24*2\ntrain = values[:n_train_hours, :]\ntest = values[n_train_hours:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :8], train[:, 8]\ntest_X, test_y = test[:, :8], test[:, 8]","e0c85b61":"train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)","1dfc9a5e":"# design network\nmodel = Sequential()\nmodel.add(LSTM(100, return_sequences = True, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dropout(0.3))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1,activation='linear'))\n\nmodel.compile(loss='mse', optimizer='adam')","5ef7c4ce":"model.summary()","eeabbdfa":"# fit network\nhistory = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)","37aebe4a":"# plot history\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","3e99f6cf":"# make a prediction\nyhat = model.predict(test_X)\ntest_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n\n# invert scaling for forecast\ninv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]","dbc1f453":"# calculate RMSE and MAE\nrmse = sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)\nmae = (mean_absolute_error(inv_y, inv_yhat))\nprint('Test MAE: %.3f' % mae)","cc33b5b9":"print('Actual :', inv_y)\nprint('Predicted:', inv_yhat)\n# plot history\npyplot.plot(inv_y, label='Actual')\npyplot.plot(inv_yhat, label='Predicted')\npyplot.legend()\npyplot.show()","a9a085d2":"# **3. Data Normalization**\nData Normalization is done using MinMaxScaler function of sklearn.","dbb5ef2b":"# **Plotting the graph of Train Loss and Test Loss**","0c49a1e7":"# **Designing Network**\nWe will define the LSTM with 100 neurons in the first hidden layer and a Dropout Layer of 0.3,Next there will be another hidden layer of 50 neurons and a Dropout of 0.2. Similarly, there will two more hidden layers with respective 0.2 Dropouts and 50 neurons. In Final Layer, 1 neuron in the output layer for predicting pollution. In the activation Function we used linear function, because of sequential dataset. In the batch size we used three days (24*3 Hours) data. Optimizer function we used Adam method. Loss function we used mean squared error. By monitoring the value of test data loss function, stop the training model when it is not decreasing, and save the current best model.","74e60303":"# **Calculating the RMSE and MAE values**","ce17efb1":"# **4. Converting to Time Series Data**\nSince we use the LSTM neural network, we must sort the data according to the time. The dataset is transformed into a supervised learning problem. The weather variables for the hour to be predicted (t) are then removed. So, we have features for previous timestep (t-1) and for prediction of pollution PM2.5 taking its current timestep (t) data.","6f80bffc":"# **1. Feature Selection**\nThere are 8 features important for the forecast: PM2.5, dew point, temperature, pressure, wind direction, wind speed and the cumulative number of hours of snow and rain. Hence, dropping other features.","c73be28e":"# **Making Prediction\/Forecasting**\nForecasting the results and invert the scaling of the prediction and test data to check.","5bd062a6":"# Splitting into train and test sets\nWe will only fit the model on the first 2 years(365* 24 * 2 hours) of data, then evaluate it on the remaining 3 years of data. ","11fbc20c":"# **2.** **Label** **Encoding**\nThe Wind direction doesn\u2019t contain numerical values so label encoding is done.\n","61bd6606":"# reshaping input to be 3D [samples, timesteps, features]\n","8948bcc3":"# **Plotting the Graph of Actual vs Predicted**","2df25fe1":"# **Fitting the Network**\nNetwork is fit with epochs size of 50 , batch size of 72.","5240ff04":"**DATASET PREPROCESSING**\n\n\n\n\nDataset Loading and dropping the rows with NA values.Plotting the graph for each columns. Further preprocessing includes:\n\n\n\n\n1. Feature Selection\n2. Label Encoding\n3. Data Normalization\n4. Converting to Time Series data"}}