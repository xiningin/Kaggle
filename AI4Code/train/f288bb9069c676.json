{"cell_type":{"0f7c41b8":"code","9b3cef20":"code","bb06648f":"code","ba4c49a3":"code","49f31fb2":"code","bb889b96":"code","637d93b3":"code","ea8cfc63":"code","642b8762":"code","f5b479b3":"code","a0fc18b1":"code","4cfefe3e":"code","757aeb2d":"code","5e250fb7":"code","6eeae448":"code","68eec546":"code","a537a141":"code","45786b24":"code","0f7553f5":"code","c8e25edd":"code","d1696f14":"markdown","08a6dc30":"markdown","f78e6803":"markdown"},"source":{"0f7c41b8":"# importing libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\n\n# To view all columns in the dataframe\npd.options.display.max_columns=None\n\nsns.set(style = \"whitegrid\")","9b3cef20":"# Loading the data, setting id column as the index\n\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)","bb06648f":"# Previewing the data\n\ntrain.head()","ba4c49a3":"# Size of the train dataframe\n\ntrain.shape","49f31fb2":"# Info on dataframe columns\n\ntrain.info()","bb889b96":"# Separating the target column from the features\n\ny = train.target\nfeatures = train.drop(['target'], axis=1)\n\n# Identifying the categorical columns\n\ncat_cols = [column for column in train.columns if 'cat' in column]\n\n#Identifying the numerical columns\n\nnum_cols = [col for col in features if features[col].dtypes == 'float64']\n\nfeatures.head()","637d93b3":"# Description of the numerical columns\n\ntrain.describe().transpose()","ea8cfc63":"# Unique values in the categorical columns\n\nfor col in cat_cols:\n    print('Unique values in column {} : {}'.format(col,train[col].nunique()))","642b8762":"# Categorical features\n\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(12,15), dpi=100)\n\ni = 0\nfig.suptitle('Frequency plot of categorical variables', fontsize=20)\nfor row in range(5):\n    for col in range(2):\n        sns.countplot(x=features[cat_cols[i]], ax=axes[row, col], palette='magma')\n        i += 1\n        \nfig.subplots_adjust(left=None,\n    bottom=None,\n    right=None,\n    top=None,\n    wspace=0.5,\n    hspace=0.5,)","f5b479b3":"# Numerical features\n\nnum_cols = [col for col in features if features[col].dtypes == 'float64']\n\nfig, axes = plt.subplots(nrows=7, ncols=2, figsize=(12,15), dpi=100)\n\ni = 0\nfig.suptitle('Distribution of numerical features', fontsize=20)\nfor row in range(7):\n    for col in range(2):\n        sns.histplot(x=features[num_cols[i]], ax=axes[row, col],kde=True, bins=10, color='m')\n        i += 1\n        \nfig.subplots_adjust(left=None,\n    bottom=None,\n    right=None,\n    top=None,\n    wspace=0.5,\n    hspace=0.5,)","a0fc18b1":"fig, axes = plt.subplots(nrows=7, ncols=2, figsize=(12,16), dpi=100)\n\ni = 0\nfig.suptitle('Distribution of numerical variables', fontsize=20)\nfor row in range(7):\n    for col in range(2):\n        sns.boxplot(x=features[num_cols[i]], ax=axes[row, col], color='m')\n        i += 1\n        \nfig.subplots_adjust(left=None,\n    bottom=None,\n    right=None,\n    top=None,\n    wspace=0.3,\n    hspace=0.7,)","4cfefe3e":"# Encoding the categorical columns using ordinal encoder\nX = features.copy() \nX_test = test.copy()\n\nordinal_encoder = OrdinalEncoder()\nX[cat_cols] = ordinal_encoder.fit_transform(features[cat_cols])\nX_test[cat_cols] = ordinal_encoder.transform(test[cat_cols])","757aeb2d":"X.head()","5e250fb7":"# Splitting the data into train and validation\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=0)","6eeae448":"# Parameters for XGBRegressor\n\nn_estimators = 1000\nearly_stopping_rounds = 5\nlearning_rates = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.07, 0.08, 0.09, 0.1]","68eec546":"# Training the model for different learning rate\n\nmean_sqaured_score = {}\n\nfor learning_rate in learning_rates:\n    xgb_model = XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n    xgb_model.fit(X_train, y_train,\n                 early_stopping_rounds=5,\n                 eval_set=[(X_valid, y_valid)],\n                 verbose=False)\n    \n    preds_valid = xgb_model.predict(X_valid)\n    \n    # Storing the mean squared error for each learning rate\n    mean_sqaured_score[learning_rate] = mean_squared_error(y_valid, preds_valid, squared=False)","a537a141":"mean_sqaured_score","45786b24":"min(mean_sqaured_score, key=mean_sqaured_score.get)","0f7553f5":"# Building the model with learning rate 0.05 for which the mean squared error is minimum\n\nmodel = XGBRegressor(n_estimators=n_estimators, \n                     learning_rate=0.05, \n                     random_state=0)\nmodel.fit(X_train, y_train)","c8e25edd":"predictions = model.predict(X_test)\n\n# Saving the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","d1696f14":"# Model Training","08a6dc30":"# Exploratory Data Analysis","f78e6803":"# Data Preparation"}}