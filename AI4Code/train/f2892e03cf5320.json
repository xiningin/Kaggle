{"cell_type":{"423e00ae":"code","b0a2ee8d":"code","8d55a320":"code","aa4f705b":"code","b0083d5c":"code","dcdf147a":"code","8cad75a9":"code","281d0cc5":"code","bbef2447":"code","10fcb273":"code","b4933c01":"code","fa43274f":"code","da8be495":"code","89ed6350":"code","422ae33f":"code","c98cadce":"code","512d6990":"code","6320e0c3":"code","97aed6c7":"code","9c736854":"code","fb41f68d":"code","0e37ecdd":"code","b5417538":"code","00c32083":"code","67fac018":"code","4ab620f9":"code","d664a85b":"code","4ae92c13":"code","bcb82846":"code","87b55d7b":"code","9ff35921":"code","9690d42d":"code","6c79a4be":"code","c7a3692b":"code","f93a4e2e":"code","ea84d0f5":"code","fe06d929":"code","0d4ffd4a":"code","f313c3b2":"code","6ad3a9f1":"code","dd7e3536":"code","b1d88e77":"code","2cdbf9f7":"code","97eab5ab":"code","c6e9cf2f":"code","8c22609c":"code","cc649cd1":"code","14d1505e":"code","36f376f4":"code","8daabbaa":"code","b542feab":"code","491de65b":"code","25f64653":"code","d43424be":"code","324dbcdb":"code","3409e44c":"code","97aebb3d":"code","69deaf34":"code","b633f83b":"code","2f8beac5":"code","94f63e2b":"markdown","5d542eb8":"markdown","c37d3557":"markdown","33dc07b2":"markdown","85405aa0":"markdown","bed5d8df":"markdown","e3b6fce6":"markdown","fb774e4a":"markdown"},"source":{"423e00ae":"import os\nfrom dateutil import parser\nimport pandas as pd\nimport numpy as np\nimport re\nimport email","b0a2ee8d":"%matplotlib inline","8d55a320":"import sys\n#reload(sys)\n#sys.setdefaultencoding('utf8')","aa4f705b":"\npath = ''\n#file = open('\/Users\/millerc\/Dropbox\/CODE\/BLDG-SIM Analysis\/2012-December.txt')\nfilelist = os.listdir(path)\nos.chdir(path)\n\n","b0083d5c":"filelist","dcdf147a":"DataList = []\nBodyList = []\n\n#Open all the files one at a time\nfor file in filelist:\n    i=0\n    openfile = open(file)\n    linelist = [line for line in openfile.readlines()]\n    linelistmax=len(linelist)\n\n    #Iterate through each file and find the From:, Date:, and Subject: for each email\n    body = []\n    while i < linelistmax:\n        \n        try:\n            if linelist[i][:5] == 'From:':\n                if linelist[i+2][:8] == 'Subject:':              \n                    WholeDate = parser.parse(linelist[i+1][6:]).replace(tzinfo=None)\n                    #Date = WholeDate.date()\n#                    YearMonth = WholeDate.replace(day=0)\n                    DataEntry = (linelist[i][5:], WholeDate, linelist[i+2][19:])\n                    DataList.append(DataEntry)\n                    BodyList.append(\" \".join(str(x) for x in body))\n                    body = []\n                else:\n                    body.append(linelist[i-1])\n            else:\n                body.append(linelist[i-1])\n        except ValueError:\n            print 'Bad Date: '+linelist[i+1]\n        i+=1\n    #BodyList.append(body)\n    print \"Finished Loading \"+file\nBodyList.append(\" \".join(str(x) for x in body))","8cad75a9":"len(BodyList)","281d0cc5":"body_df = pd.DataFrame({\"Body\":BodyList[1:]})","bbef2447":"body_df.iloc[-1].Body.strip()","10fcb273":"BodyList[1]","b4933c01":"body_df.info()","fa43274f":"\nbldgsimdata = pd.DataFrame(data=DataList,columns=['From','DateTime','Subject'])\nbldgsimdata = pd.concat([bldgsimdata, body_df], axis=1)\n\n# NumberPerDay = bldgsimdata.groupby('Date').size()\n# NumberPerDayIndex = pd.DatetimeIndex(NumberPerDay.index)\n# NumberPerDay=NumberPerDay.reindex(index=NumberPerDayIndex)\n# NumberPerMonth = NumberPerDay.resample('M').sum()\n\n# AggSubjects = bldgsimdata.groupby('Subject').size()\n\n# PeopleFreq = bldgsimdata.groupby('From').size()\n# # PeopleFreq = PeopleFreq.sort()\n\n# monthgrouped = NumberPerDay.groupby(lambda x: x.month)\n# monthlystats = monthgrouped.agg({'monthlysum': np.sum, 'dailymean': np.mean})","da8be495":"#AggSubjects","89ed6350":"bldgsimdata.info()","422ae33f":"#PeopleFreq.sort_values()","c98cadce":"bldgsimdata","512d6990":"#bldgsimdata.sort_values(\"Date\")","6320e0c3":"# bldgsimdata['name'] = bldgsimdata.From.apply(lambda x: re.findall('\\(([^)]+)', x))\n# bldgsimdata['name'] = bldgsimdata['name']","97aed6c7":"#type(bldgsimdata.name.ix[1])","9c736854":"from mailbox import mbox\nimport csv\n\ndef store_content(message, body=None):\n    if not body:\n        body = message.get_payload(decode=True)\n    if len(message):\n        contents = {\n            \"subject\": message['subject'] or \"\",\n            \"body\": body,\n            \"from\": message['from'],\n            \"to\": message['to'],\n            \"date\": message['date'],\n            \"labels\": message['X-Gmail-Labels'],\n            \"epilogue\": message.epilogue,\n        }\n        return df.append(contents, ignore_index=True)\n\n# Create an empty DataFrame with the relevant columns\ndf = pd.DataFrame(\n    columns=(\"subject\", \"body\", \"from\", \"to\", \"date\", \"labels\", \"epilogue\"))\n\n# Import your downloaded mbox file\nbox = mbox('\/Users\/nus\/bldgsim-topicmodeling\/mbox-to-txt-master\/2015-05-06_2018-11-08-bldg-sim-onebuilding.org.mbox')\n\nfails = []\nfor message in box:\n    try:\n        if message.get_content_type() == 'text\/plain':\n            df = store_content(message)\n        elif message.is_multipart():\n            # plaintext from multipart messages\n            for part in message.get_payload():\n                if part.get_content_type() == 'text\/plain':\n                    df = store_content(message, part.get_payload(decode=True))\n                    break\n    except:\n        fails.append(message)","fb41f68d":"df.date = pd.to_datetime(df.date)","0e37ecdd":"# df['from']","b5417538":"df.body[1]","00c32083":"# out_array = []\n# snippets = []\n\n\n# for file in filelist:\n#     i=0\n#     openfile = open(file)\n#     linelist = [line for line in openfile.readlines()]\n#     linelistmax=len(linelist)\n#     print \"Processing \"+file\n\n#     with open(file, mode=\"r\") as bigfile:\n#         for line in bigfile:\n#             try:\n#                 if line.startswith(\"From:\"):\n#                     out_array.append(line)\n#                     snippets.append( line[0:min(len(line),100)] )\n#                 else:\n#                     out_array[-1] += line\n#             except:\n#                 print line","67fac018":"bldgsimdata","4ab620f9":"bldgsimdata_new = df[[\"from\",\"date\",\"subject\",\"body\"]]","d664a85b":"bldgsimdata_new.columns = [\"From\",\"DateTime\",\"Subject\",\"Body\"]","4ae92c13":"bldgsimdata_new","bcb82846":"total_bldgsim = pd.concat([bldgsimdata, bldgsimdata_new], axis=0).reset_index(drop=True)","87b55d7b":"total_bldgsim","9ff35921":"total_bldgsim.index = total_bldgsim.DateTime","9690d42d":"total_bldgsim = total_bldgsim.sort_index()","6c79a4be":"total_bldgsim.resample('M').count().plot(figsize=(20,3))","c7a3692b":"total_bldgsim.truncate(before='2014-05-08 16:00', after='2014-05-08 17:00')#.resample('D').count().plot(figsize=(20,3))","f93a4e2e":"total_bldgsim.truncate(before='2014-05-08 16:00', after='2014-05-08 17:00').index[0]#.resample('D').count().plot(figsize=(20,3))\n","ea84d0f5":"total_bldgsim = total_bldgsim.drop([total_bldgsim.truncate(before='2014-05-08 16:00', after='2014-05-08 17:00').index[0]])\n","fe06d929":"total_bldgsim.resample('M').count().plot(figsize=(20,3))","0d4ffd4a":"total_bldgsim.truncate(before='2017-05-08 18:01:20', after='2017-05-08 18:35').Subject[0]#.resample('D').count().plot(figsize=(20,3))\n","f313c3b2":"total_bldgsim = total_bldgsim[~total_bldgsim.Subject.str.contains(\"rodrigo.cerqueira@mra.qc.ca\")]","6ad3a9f1":"total_bldgsim.resample('M').count().plot(figsize=(20,3))","dd7e3536":"total_bldgsim.truncate(before='2013-01-09', after='2013-01-11')#.resample('D').count().plot(figsize=(20,3))\n","b1d88e77":"total_bldgsim.to_pickle(\"\/Users\/nus\/twenty-years-of-bldgsim-textmining\/processed_data\/total_email_data.pkl\")","2cdbf9f7":"#df = df.fillna(\"x\")","97eab5ab":"#bldgsimdata.info()","c6e9cf2f":"out_array = np.array(total_bldgsim.fillna(\"x\").Body)","8c22609c":"#out_array_new = np.array(df['from']+\" \"+df[\"date\"].astype(\"str\")+\" \"+df[\"subject\"]+\" \"+df[\"body\"])","cc649cd1":"#len(out_array_old)","14d1505e":"#out_array = np.concatenate([out_array_old,out_array_new])","36f376f4":"len(out_array)","8daabbaa":"os.chdir(\"\/Users\/nus\/twenty-years-of-bldgsim-textmining\/processed_data\/\")","b542feab":"custom_stop_words = []\nwith open( \"stopwords.txt\", \"r\" ) as fin:\n    for line in fin.readlines():\n        custom_stop_words.append( line.strip() )\n# note that we need to make it hashable\nprint(\"Stopword list has %d entries\" % len(custom_stop_words) )","491de65b":"from sklearn.feature_extraction.text import CountVectorizer\n# use a custom stopwords list, set the minimum term-document frequency to 20\nvectorizer = CountVectorizer(stop_words = custom_stop_words, min_df = 20, encoding='latin-1')\nA = vectorizer.fit_transform(out_array)\nprint( \"Created %d X %d document-term matrix\" % (A.shape[0], A.shape[1]) )","25f64653":"# extract the resulting vocabulary\nterms = vectorizer.get_feature_names()\nprint(\"Vocabulary has %d distinct terms\" % len(terms))","d43424be":"from sklearn.externals import joblib\njoblib.dump((A,terms), \"emails-raw.pkl\") ","324dbcdb":"from sklearn.feature_extraction.text import TfidfVectorizer\n# we can pass in the same preprocessing parameters\nvectorizer = TfidfVectorizer(stop_words=custom_stop_words, min_df = 20, encoding='latin-1')\nA = vectorizer.fit_transform(out_array)\nprint(\"Created %d X %d TF-IDF-normalized document-term matrix\" % (A.shape[0], A.shape[1]) )","3409e44c":"import operator\ndef rank_terms( A, terms ):\n    # get the sums over each column\n    sums = A.sum(axis=0)\n    # map weights to the terms\n    weights = {}\n    for col, term in enumerate(terms):\n        weights[term] = sums[0,col]\n    # rank the terms by their weight over all documents\n    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)","97aebb3d":"ranking = rank_terms( A, terms )\nfor i, pair in enumerate( ranking[0:200] ):\n    print( \"%s\" % (pair[0]) )","69deaf34":"#ranking","b633f83b":"#snippets[:1]","2f8beac5":"joblib.dump((A,terms), \"emails-tfidf.pkl\") ","94f63e2b":"# Get data from .mbox\n\nCode example from: https:\/\/gist.github.com\/shkesar\/be9009d83b82548edd16cd5a7d300457\n\nTo download the mbox file used here: https:\/\/www.dropbox.com\/s\/4gzc3a6a7wgsm01\/mbox_files_fordownload_bldgsim.zip?dl=0","5d542eb8":"# Stop words from the bodies","c37d3557":"## Looks like there is an anomalous day in 2014\n","33dc07b2":"# Extract Data from all the 1999-2015 flat files found in the online archive\n\nInstead, we will use the `.mbox` files sent by Jason later","85405aa0":"# TFIDF Weighting","bed5d8df":"# Create an Array for the tfidf process","e3b6fce6":"# Merge the data and output as a pickle that can be used in further analysis","fb774e4a":"Get the message contents from files - old way that's not used anymore"}}