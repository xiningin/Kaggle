{"cell_type":{"afaea07b":"code","77e9918e":"code","c5c6340a":"code","e89ea6cd":"code","99375bef":"code","7e363ebe":"code","36f1e55d":"code","67a2b86f":"code","8339aeec":"code","0785405c":"code","7a8f5054":"code","cdf0731c":"code","222f252f":"code","46c989c3":"code","f9a06f4c":"code","97f82ea7":"code","46373e59":"code","2ec10133":"code","1d28f9b1":"code","a549af78":"code","331b8a32":"code","b32b1742":"code","77930806":"code","7d99e81b":"code","b1bf96a1":"code","b3c60051":"code","a9b56a21":"markdown","61991c2a":"markdown","2dee5c1e":"markdown","85611a91":"markdown","38e628d0":"markdown","b314681a":"markdown","f24daafb":"markdown","da2f92f4":"markdown","cd599999":"markdown","bfd0ef81":"markdown","8fa9808d":"markdown","f03aa7da":"markdown","498f53e2":"markdown","8f0d650b":"markdown","3121b87f":"markdown","7dae2921":"markdown","c5257e1c":"markdown","9a374b4c":"markdown","a32949a1":"markdown"},"source":{"afaea07b":"import pandas as pd\nimport warnings  \nwarnings.filterwarnings('ignore') # to ignore the warnings","77e9918e":"data = pd.read_csv('..\/input\/churn-modelling\/Churn_Modelling.csv')\ndata.head()","c5c6340a":"# encoding the categorical columns and getting rid of the redundant columns\ngeog = pd.get_dummies(data['Geography'], drop_first=True)\ngend = pd.get_dummies(data[\"Gender\"], drop_first=True)","e89ea6cd":"# converting these columns to 'int'\ngeog = geog.astype(int)\ngend = gend.astype(int)","99375bef":"# concatenating these encoded variables to the original dataset\ndata1 = pd.concat([data, gend, geog], axis=1)","7e363ebe":"# seperating the independent and dependent variables\n\n# taking only the important variables(columns names) from the orignal dataset\nfeature_cols = ['CreditScore', 'Age', 'Tenure', 'Balance',\n                'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Male', 'Germany', 'Spain']\n\nx = data1[feature_cols]\ny = data1['Exited']","36f1e55d":"# splitting the data into training and testing\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)","67a2b86f":"# scaling the data to crush the impact of variable with larger weight in the analysis\n# this method equalizes range and variability in the dataset\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)","8339aeec":"# importing the required libraries to form an Artificial Neural Network\nfrom keras.models import Sequential     # required to initialize the neural network coz ANN is a sequence of layers\nfrom keras.layers import Dense          # to build the layers in ANN","0785405c":"# initializing the ANN\nann_classifier = Sequential()","7a8f5054":"ann_classifier.add(Dense(units=6, kernel_initializer = 'uniform', activation='relu', input_dim=11))","cdf0731c":"ann_classifier.add(Dense(units=6, kernel_initializer = 'uniform', activation='relu'))","222f252f":"ann_classifier.add(Dense(units=1, kernel_initializer = 'uniform', activation='sigmoid'))","46c989c3":"ann_classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","f9a06f4c":"ann_classifier.fit(x_train, y_train, batch_size=10, epochs=100)","97f82ea7":"y_pred = ann_classifier.predict(x_test)\ny_pred = y_pred > 0.5       ","46373e59":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"The accuracy obtained on testing set is\", round((accuracy_score(y_test, y_pred) * 100), 2), '%')","2ec10133":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score","1d28f9b1":"def build_classifier():\n    from keras.models import Sequential\n    from keras.layers import Dense\n    ann_classifier = Sequential()\n    ann_classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))\n    ann_classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n    ann_classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n    ann_classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return ann_classifier","a549af78":"ann_classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100)\naccuracies = cross_val_score(estimator=ann_classifier, X=x_train, y=y_train, cv=7, n_jobs=-1)\n# will contain 10 accuracies returned by kfold cv\nprint(\"The average of the accuracies is\", round((accuracies.mean() * 100), 2), '%')\nprint(\"The standard deviation of the accuracies is \", accuracies.std())","331b8a32":"from sklearn.model_selection import GridSearchCV\ndef build_classifier(optimizer):\n    from keras.models import Sequential\n    from keras.layers import Dense\n    ann_classifier = Sequential()\n    ann_classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))\n    ann_classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n    ann_classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n    ann_classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    return ann_classifier","b32b1742":"# here we set what parameters to pass to check for the optimal values suggested by this method\nann_classifier = KerasClassifier(build_fn = build_classifier)\n\n# we pass these arguments of parameters as a list\nparams = {'batch_size': [25, 32], 'nb_epoch': [100, 200, 300], 'optimizer': ['adam', 'rmsprop']}\n\ngrid_search = GridSearchCV(estimator=ann_classifier, param_grid=params, cv=10, scoring='accuracy')\ngrid_search = grid_search.fit(x_train, y_train)\nbest_parameters = grid_search.best_params_      # will give the best parameters\nbest_accuracy = grid_search.best_score_         # will give the best accuracy score","77930806":"# checking the parameters obtained by the grid search mechanism\nprint(best_parameters)\nprint(best_accuracy)","7d99e81b":"# defining the layers\nann_classifier2 = Sequential()\nann_classifier2.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))\nann_classifier2.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\nann_classifier2.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\nann_classifier2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nann_classifier2.fit(x_train, y_train, batch_size=25, epochs=100)","b1bf96a1":"# predicting the result\ny_pred2 = ann_classifier2.predict(x_test)\ny_pred2 = y_pred2 > 0.5","b3c60051":"print('The accuracy obtained after tuning the ANN is', round((accuracy_score(y_test, y_pred2) * 100), 2), '%')","a9b56a21":"* Here we set a threshold of 0.5 \n* People having this score greater than 0.5 means a probability of leaving the bank\n* Hence we apply a trick here that if values are less than 0.5 then it would return False and if greater than 0.5 it would return True\n* Then we plot the Confusion Matrix for the same","61991c2a":"## **Step 1** : Pre-processing","2dee5c1e":"## **Step - 6 :** Running the ANN again based on parameters obtained above","85611a91":"**Step 2.1 :** Adding the input layer and the 1st hidden layer","38e628d0":"**Step 4.1 :** Wrapping k-fold cross validation into keras model","b314681a":"## **Step 5 :** Tuning the ANN \n* This is usually done for the sake of ease in choosing the best parameters for the ANN instead of manually imputing them over and over\n* This method also saves time by avoiding trial and error\n* We use the Grid Search method for this task","f24daafb":"**Arguments used -** \n* `optimizer` = name of the algorithm we want to apply, usually SGD algorithm known by 'adam'\n* `loss` = it is a loss function within SGD algorithm, or the function we need to optimize to find optimal weights usually based on the activation function used for the o\/p layer, or the type of dependent variable\n* `metrics` parameter has [ ] coz it expects a list of values as the weights have been calculated after each observation or each batch of observations. Hence the algorithm uses this parameter to calculate the accuracy to improve the model performance","da2f92f4":"## **Step 4 :** Evluating the ANN (Cross Validation)","cd599999":"**Step 2.5 :** Fitting the ANN to the training set\n\n**Arguments used -**\n* `batch_size` means after how many observations the weights should be updated\n* `epochs` means how many times you want to run through the network\n* `1` epoch would signify that whole data has been passed through the network once","bfd0ef81":"**Step 4.3 -** Performing the cross validation","8fa9808d":"## **Step - 2** : Building the Artificial Neural Network","f03aa7da":"**Step 2.2 :** Adding the 2nd hidden layer\n\nThis time, there is no need to specify the input layer as the operation above tells this layer what input to expect","498f53e2":"**Step 4.2 -** Building a function to initialize the ANN and its respective layers","8f0d650b":"**Step 2.3 :** Adding the output layer","3121b87f":"A small change we do while building this model is that the 'optimizer' argument is passed while calling the function so that it can use the optimizers provided in the list below","7dae2921":"## **Step 3 :** Predicting the results for test set","c5257e1c":"**Arguments used -**\n* `units` = no. of nodes in hidden layer, generally half of the total of all variables\n* `kernel_initializer` = 'uniform' means assigining weights between 0 and 1 in a uniform manner\n* `activation='relu'` means assigning rectifier function at the hidden layer\n* `input_dim=11` means no. of input neurons in the input layer (no. of variables in training set)","9a374b4c":"**Step 2.4 :** Compiling the ANN","a32949a1":"# Artificial Neural Network Simplified (Churn Dataset)\n\n### **Goals of the project -** \n* To understand the basic implemetation of the ANN\n* To build the ANN layer by layer and understanding the significance of each layer and the arguments used\n* To understand how to cross validate the results of ANN\n* Learn to fine tune the ANN using Grid Search Mechanism\n"}}