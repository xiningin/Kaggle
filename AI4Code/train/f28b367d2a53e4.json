{"cell_type":{"f7575f34":"code","53ef0153":"code","36fa535c":"code","d1b7ac43":"code","5229013d":"code","37a60d71":"code","6db0e3f5":"code","baabd860":"code","27a1d360":"code","87c1ba69":"code","45c8f7b3":"code","2700f508":"code","2f4433b3":"code","85a3d4d8":"code","a2389a5c":"code","8905cf53":"code","199467a3":"markdown","686faaf7":"markdown","baf02e80":"markdown","c70e4bb4":"markdown","8c5f0ece":"markdown","8eaa1a97":"markdown","ebf9d709":"markdown","81c33b6a":"markdown"},"source":{"f7575f34":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\nimport warnings\nwarnings.filterwarnings('ignore')","53ef0153":"#Set up Screen output\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', 100)","36fa535c":"data_train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ndata_test = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nID = data_test['id']\ndata_train.head()","d1b7ac43":"data_train.isnull().sum()","5229013d":"data_train.describe()","37a60d71":"#Drop columns not use\n\ndata_train = data_train.drop(['id'], axis=1)\ndata_test = data_test.drop(['id'], axis=1)\ndata_train.head()","6db0e3f5":"X = data_train.drop(['target'],axis=1)\ny = data_train['target']","baabd860":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX= sc.fit_transform(X)\ndata_test = sc.fit_transform(data_test)","27a1d360":"from sklearn.decomposition import PCA  #Import PCA\npca = PCA().fit(X)   \n\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, 51, step=1) #Assume x is number of component, \ny1 = np.cumsum(pca.explained_variance_ratio_) #In this data, we have 51 colums\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y1, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 60, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","87c1ba69":"pca = PCA(n_components = 47)\nX = pca.fit_transform(X)\ndata_test = pca.transform(data_test)","45c8f7b3":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 0)","2700f508":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'max_depth': np.arange(1, 10),\n 'criterion':['entropy','gini']}\nmodel = GridSearchCV(DecisionTreeClassifier(), param_grid)\nmodel.fit(X_train, y_train)\nmodel.best_estimator_","2f4433b3":"best_model = DecisionTreeClassifier(max_depth = 3,criterion = 'entropy')\nbest_model.fit(X_train,y_train)\n\ny_pred = best_model.predict_proba(data_test)","85a3d4d8":"predictions = pd.DataFrame(y_pred, columns=['class_1','class_2', 'class_3', 'class_4'])\nsubmission = pd.concat([ID,predictions], axis=1)","a2389a5c":"submission.head()","8905cf53":"submission.to_csv('submission.csv', index=False)","199467a3":"# Train Test Split","686faaf7":"From the graph. The dimension required is 47. Then we use n_components = 47","baf02e80":"# Import Libralies","c70e4bb4":"Before I train model. I will find best parameters for model by using GridSerachCV.","8c5f0ece":"# Expolre data ","8eaa1a97":"Data has 51 columns. \n* 1 column for ID\n* 1 column for targets\n* 49 columns for feature","ebf9d709":"# Import Model","81c33b6a":"**Using PCA**\n\nBecause of many features. I will plot a graph of Cumulative vs No.of components.\nSelect at 95% ."}}