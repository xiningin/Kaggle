{"cell_type":{"8d590e12":"code","c118731d":"code","c0730e0c":"code","05981bbb":"code","7bc28318":"code","0edd8760":"code","5cdd1234":"code","76ba28bb":"code","3d0378d1":"code","bb0582ef":"code","2d981b55":"code","da00726f":"code","51d774f6":"code","b4f21f13":"code","79b3773e":"code","2992870c":"code","4bcea0b8":"code","3c724abd":"code","0a11908c":"code","cd8271ef":"code","54443a7d":"code","028b95a3":"code","53c75c4a":"code","65563011":"code","2290100d":"code","8da5498f":"code","d7b718fc":"code","bb251b4e":"code","961e5957":"code","1d70ee9e":"code","99b40076":"code","6ddfc01c":"markdown","6fd643a6":"markdown","463b84fb":"markdown","f650a0c0":"markdown","1384742b":"markdown","5559bba5":"markdown","3bde452b":"markdown","20738901":"markdown","bdd4ffbd":"markdown","4914441a":"markdown","4aa1d05d":"markdown","659168a8":"markdown","77b1c564":"markdown","d83ed075":"markdown","e9b2ce11":"markdown","63c4573b":"markdown","8103c149":"markdown","cb431d26":"markdown","647ead6e":"markdown","69a3155e":"markdown","81cbe556":"markdown","d17fa80b":"markdown","76288112":"markdown"},"source":{"8d590e12":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras.layers import Input,Dense\nfrom tensorflow.keras.models import Sequential\nfrom keras.models import Model\nimport tensorflow as tf\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","c118731d":"df = pd.read_csv('..\/input\/ny-property-data\/NY property data.csv')\ndf.head()","c0730e0c":"df = df[['BBLE','FULLVAL', \n         'AVLAND', \n         'AVTOT', \n         'LTFRONT', \n         'LTDEPTH', \n         'BLDFRONT', \n         'BLDDEPTH', \n         'STORIES', \n         'ZIP', \n         'TAXCLASS', \n         'B',\n         'BLOCK',\n         'BLDGCL']]\ndf['BLOCK'] = df['BLOCK'].astype('category')\ndf['BLDGCL'] = df['BLDGCL'].astype('category')\n#combine 0 and NA\ndf.replace(0, np.nan, inplace=True)","05981bbb":"#fill ZIP nan\ndf['ZIP']=df.groupby(['B','BLOCK'])['ZIP'].transform(lambda x: x.fillna(x.median()))\ndf['ZIP']=df.groupby(['B'])['ZIP'].transform(lambda x: x.fillna(x.median()))\na=df['ZIP'].isnull().sum()\n#fill NAs: FULLVAL, AVLAND, AVTOT\nHV=['FULLVAL', 'AVLAND', 'AVTOT']\nfor i in HV:\n    df[i]=df.groupby(['BLDGCL'])[i].transform(lambda x: x.fillna(x.median()) if len(x)>=5 else x)\n    df[i]=df.groupby(['B'])[i].transform(lambda x: x.fillna(x.median()))\nb=df[HV].isnull().sum()\n#Filling NAs: LTFRONT, LTDEPTH,BLDFRONT, BLDDEPTH\nHP=['LTFRONT', 'LTDEPTH', 'BLDFRONT', 'BLDDEPTH']\nfor hpi in HP:\n    df[hpi]=df.groupby('BLDGCL')[hpi].transform(lambda x: x.fillna(x.median()))\n    df[hpi]=df.groupby('B')[hpi].transform(lambda x: x.fillna(x.median()))\n\ndf[HP].isnull().sum()\nc=df[HP].isnull().sum()\n# Filling NAs: STORIES\ndf['STORIES'] = df.groupby('BLDGCL')['STORIES'].transform(lambda x: x.fillna(x.median()))\ndf['STORIES'] = df.groupby('B')['STORIES'].transform(lambda x: x.fillna(x.median()))\nd=df['STORIES'].isnull().sum()\nprint(a,b,c,d)","7bc28318":"from IPython.display import Image\nImage(\"..\/input\/create-variables\/Screen Shot 2019-02-22 at 10.24.17 PM.png\")","0edd8760":"df['lotarea']=df['LTFRONT']*df['LTFRONT']\ndf['bldarea']=df['BLDFRONT']*df['BLDDEPTH']\ndf['bldvol']=df['bldarea']*df['STORIES']\n\nnew_cols=['lotarea','bldarea','bldvol']\nnorm_vols=['FULLVAL','AVLAND', 'AVTOT']\n\nfor n in norm_vols:\n    for j in new_cols:\n        df[n+'\/'+j]=df[n]\/df[j]\n# create variables\ndf['zip3'] = df['ZIP']\/\/100\ndf['zip5'] = df['ZIP']\/\/1\n\n#scale value\nscale_value=['FULLVAL\/lotarea',\n 'FULLVAL\/bldarea',\n 'FULLVAL\/bldvol',\n 'AVLAND\/lotarea',\n 'AVLAND\/bldarea',\n 'AVLAND\/bldvol',\n 'AVTOT\/lotarea',\n 'AVTOT\/bldarea',\n 'AVTOT\/bldvol',]\nscale_facter=['zip3','zip5','TAXCLASS','B']\nfor i in scale_value:\n    df[i+'_scale by_all']=df[i]\/df[i].mean()\n    for j in scale_facter:\n        df[i+'_scale by_'+j]=df.groupby(j)[i].apply(lambda x: x\/(x.mean()))\n#clean process data\ndf=df.drop(['FULLVAL',\n 'AVLAND',\n 'AVTOT',\n 'LTFRONT',\n 'LTDEPTH',\n 'BLDFRONT',\n 'BLDDEPTH',\n 'STORIES',\n 'ZIP',\n 'TAXCLASS',\n 'B',\n 'BLOCK',\n 'BLDGCL',\n 'lotarea',\n 'bldarea',\n 'bldvol',\n 'FULLVAL\/lotarea',\n 'FULLVAL\/bldarea',\n 'FULLVAL\/bldvol',\n 'AVLAND\/lotarea',\n 'AVLAND\/bldarea',\n 'AVLAND\/bldvol',\n 'AVTOT\/lotarea',\n 'AVTOT\/bldarea',\n 'AVTOT\/bldvol',\n 'zip3',\n 'zip5'], axis=1)\ndf.head()","5cdd1234":"df.shape","76ba28bb":"feature_values=df.columns.values.tolist()\nfeature_values.pop(0)","3d0378d1":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndef scaleColumns(df, feature_values):\n    for col in feature_values:\n        df[col] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(df[col])),columns=[col])\n    return df\nscaled_df = scaleColumns(df,feature_values)","bb0582ef":"scaled_df.head()","2d981b55":"scaled_df.set_index('BBLE', inplace=True)\nscaled_df.head()","da00726f":"from sklearn.decomposition import PCA\nnpc=20\npca = PCA(n_components=npc)\nprincipalComponents = pca.fit_transform(scaled_df[feature_values].values)\npca.get_covariance()\nexplained_variance=pca.explained_variance_ratio_\nexplained_variance\nX=list(range(1,npc+1))\nplt.bar(X,explained_variance,label='individual explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","51d774f6":"values = list(explained_variance)\ntotal  = 0\nsums   = []\n\nfor v in values:\n  total = total + v\n  sums.append(total)\nsums","b4f21f13":"npc=8\npca = PCA(n_components=npc)\nprincipalComponents = pca.fit_transform(scaled_df.values)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', \n                          'principal component 2', \n                          'principal component 3', \n                          'principal component 4', \n                          'principal component 5', \n                          'principal component 6', \n                          'principal component 7', \n                          'principal component 8'],index=scaled_df.index)\nprincipalDf['BBLE']=principalDf.index\nprincipalDf.reset_index(drop=True, inplace=True)\nprincipalDf.head()","79b3773e":"feature_values2=principalDf.columns.values.tolist()\nfeature_values2.pop(-1)","2992870c":"scaled_principalDf= scaleColumns(principalDf,feature_values2)","4bcea0b8":"scaled_principalDf.head()","3c724abd":"Image(\"..\/input\/score-image\/score1.png\",width=200, height=600)","0a11908c":"n=8\nscaled_principalDf['zscore']=0\nfor pc in feature_values2:\n    scaled_principalDf['zscore']=scaled_principalDf['zscore']+(scaled_principalDf[pc])**n\nscaled_principalDf['zscore']=(scaled_principalDf['zscore'])**(1\/n)\nscaled_principalDf.head()","cd8271ef":"Image(\"..\/input\/score-image\/score2.2.png\",width=300, height=800)","54443a7d":"Image(\"..\/input\/score-image\/score2.1.png\",width=200, height=600)","028b95a3":"from keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\ninput_data=scaled_principalDf[feature_values2]\ninput_dim = 8\nencoding_dim = 4\n\ninput_layer = Input(shape=(input_dim, ))\nencoder = Dense(encoding_dim, activation=\"tanh\", \n                activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoder = Dense(int(encoding_dim \/ 2), activation=\"relu\")(encoder)\ndecoder = Dense(int(encoding_dim \/ 2), activation='tanh')(encoder)\ndecoder = Dense(input_dim, activation='relu')(decoder)\nautoencoder = Model(inputs=input_layer, outputs=decoder)","53c75c4a":"nb_epoch = 1\n#batch_size = \nautoencoder.compile(optimizer='adam', \n                    loss='mean_squared_error', \n                    metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath=\"model.h5\",\n                               verbose=0,\n                               save_best_only=True)\ntensorboard = TensorBoard(log_dir='.\/logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=True)\nhistory = autoencoder.fit(input_data, input_data,\n                    epochs=nb_epoch,\n                    shuffle=True,\n                    verbose=1,\n                    callbacks=[checkpointer, tensorboard]).history\npredictions = autoencoder.predict(input_data)","65563011":"autoencoded_data = pd.DataFrame(predictions,columns=['enPC1','enPC2','enPC3','enPC4','enPC5','enPC6','enPC7','enPC8'])\nautoencoded_data.head()","2290100d":"n=8\nfor i in range(n):\n    scaled_principalDf[i]=abs(autoencoded_data.iloc[:,i]-scaled_principalDf.iloc[:,i])\n#data.iloc[:,1] # second column of data frame (last_name)","8da5498f":"scaled_principalDf.head()","d7b718fc":"n=8\ndiff=list(range(n))\nscaled_principalDf['autoencoder']=0\nfor pc in diff:\n    scaled_principalDf['autoencoder']=scaled_principalDf['autoencoder']+(scaled_principalDf[pc])**n\nscaled_principalDf['autoencoder']=(scaled_principalDf['autoencoder'])**(1\/n)","bb251b4e":"scaled_principalDf.head()","961e5957":"df=scaled_principalDf\ndf['Zscore_Rank'] = df['zscore'].rank(ascending=True)\ndf['Autoencoder_Rank'] = df['autoencoder'].rank(ascending=True)\ndf_final=df[['BBLE','zscore','autoencoder','Zscore_Rank','Autoencoder_Rank']]\ndf_final['Final_Score']=df_final['zscore']+df_final['autoencoder']\ndf_final['Final_Rank']=0.7*df_final['Autoencoder_Rank'] +0.5*df_final['Zscore_Rank']\ndf_final = df_final.sort_values(by=['Final_Rank'], ascending=False)\ndf_final.head(10)","1d70ee9e":"fraud=df_final['BBLE'].tolist()[:10]\ndf.loc[df['BBLE'].isin(fraud)]","99b40076":"import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#xs = np.random.normal(size=int(1e6))\nfig, ax = plt.subplots(1, 3, figsize=(16,5))\nax[0].hist(df_final['Final_Score'], bins=30)\nax[0].set_yscale('log')\nax[1].hist(df_final['zscore'], bins=30)\nax[1].set_yscale('log')\nax[2].hist(df_final['autoencoder'], bins=30)\nax[2].set_yscale('log')\n","6ddfc01c":"# New York Property Data Fraud Analysis\n[Yulia Niu](https:\/\/xyniu0829.wixsite.com\/yulia) - February 2019","6fd643a6":"### z scale\nZ-scaling is a common method used for normalization. The standard score of sample x in z scale is calculated as:\n\n**z = (x - u) \/ s**","463b84fb":"## Final Score","f650a0c0":"## Heuristic Function\nI use these 8 PCs to get all first score----zscore","1384742b":"As we could see, several records have U.S. Government or NY City as owners. The characteristics of such property is that it tends to have a large value, zero building frontage and building depth, and almost full exemption of tax.\n\nProperties owned by individuals or without an owner were also considered as fraudulent records by our models, since their property values are either 0 or very low. They displayed many signs indicating them as fraudulent records.","5559bba5":"# Variable Creation(45)\n\nProcess of variable creation:\n","3bde452b":"# Summary\nI built unsupervised fraud models on the NY Property Data in order to identify fraudulent events. \n\nI first took the time to understand the data and the business problem, which was determining which data records are fraudulent in the NY property dataset. The approach was to find anomalies within the dataset by building unsupervised fraud models. After cleaning the data and filling in missing fields using values that would not cause unwanted dramatic changes in the records, I created 45 expert variables. I then z-scaled the variables to give them equal importance and conducted the Principal Component Analysis to reduce the number of dimensionalities to eight, as well as reducing the correlation between variables. Next, I built a heuristic function of z-score model and an autoencoder model that generated two fraud scores, which I combined into a final fraud score. A higher score indicates a higher probability of fraud. With this score, I then rank-ordered all the entries and found the fraud records.\n\nIn the end, the results indicated that my models were effective in identifying fraudulent records for a few reasons. First, the fraud score distributions shared similar trends and displayed reasonable shapes. Second, the top 20 records identified as potential fraud did indeed demonstrate anomalies in some area, upon manual investigation. Interestingly, several of the records appeared to be government properties, and I believe that some industry expertise can help identify the validity of these records.","20738901":"# Algorithms","bdd4ffbd":"# Date cleaning\n## Feature Selection","4914441a":"I selected some features from the original data. 'BBLE' will be used to identify each record and others will be used for calculation later.","4aa1d05d":"# Description of Data\nThe name of the Dataset is Property Valuation and Assessment Data, and it is updated annually. This dataset contains New York City Property valuation and assessment data provided by the Department of Finance (DOF). The data is primarily used to calculate property tax, grant eligible properties exemptions and\/or abatements. It covers the time between year 2010 and 2011. In total, the data has 32 fields and 1070994 records.","659168a8":"we can see that there is an obvious reduction between the 8th PC and the 9th PC. Therefore, we decided to select the first eight PCs.","77b1c564":"### Second time of Z scale\nAfter PCA, we used the z-scale method again for the selected 8 PCs to make sure that all of them have same weights going into next steps.","d83ed075":"I already did data exploration and data quality report in another document. Here started with feature selections, data cleaning feature creations since I was already familiar with all the data features. \n\nSome data types are not reasonal in the original dataset, so we change them to what they should be. For example, 'BLDGCL' represents the Building Class and it should be a category feature, but in original dataset it is numerical feature. \n\nAlso, there are some features seem should be changed into Category features but did not being changed, such as ZIP, B and TAXCLASS, since we need to use them for calculation later.\n\nAfter Data exploration, I can figure out that there are some meaningless data with value 0. I just replaced value '0' with NAN for making fill missing values more convenient.","e9b2ce11":"## Autoencoder\nTo build the autoencoder, we employed a neural network model. Here we use Keras to create autoemcoder. Autoencoder are made by trainning and it try to model then pattern of most number. So We use autoencoder to help find outliers.\n\n In the decoding process, the autoencoder decodes the data in the hidden layer by using the same encoding function in the opposite way, reproducing each data point to make it as similar to the original data as possible. This process is demonstrated below:","63c4573b":"I fill missing value based on group by.","8103c149":"### Selecting Principal Components\nIn order to determine exactly how many eigenvectors we should keep, we first selected 20 PCs and graphed the relationship between number of PCs and the cumulative explained variance. ","cb431d26":"Calculate difference between PCs and enPCs","647ead6e":"## Fill missing value","69a3155e":"## Dimensionality Reduction\n\nAfter variable creation, we had 45 fields, which were too many for further processing. Therefore, we took the necessary steps in Python to reduce dimensionality. There were three essential steps in the dimensionality reduction process:\n\n*  Using the z-scale method to normalize 45 variables.\n*  Performing the Principal Component Analysis (PCA) to reduce variables to eight eigenvectors.\n*  Z-scaling again to assign equal weight to all eight eigenvectors.","81cbe556":"One epoch means that you have trained all dataset(all records) once,if you have 384 records,one epoch means that you have trained your model for all on all 384 records.","d17fa80b":"## Principal Component Analysis (PCA)\nThe main idea of principal component analysis (PCA) is to reduce the dimensionality of a dataset consisting of many variables correlated with each other, while retaining the variation present in the dataset. PCA transforms the variables to a new set of variables, known as the principal components (or simply, the PCs). The 1st PC retains maximum variation that was present in the original components.","76288112":"I calculated the differences between the original data and the reproduced data in order to examine and identify which data points failed to be accurately reproduced."}}