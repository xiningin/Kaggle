{"cell_type":{"fecc64fb":"code","fbba76da":"code","028867a0":"code","53d9d278":"code","9b255914":"code","04ca14c2":"code","7a10c2a5":"code","b82568e6":"code","a1676523":"code","b2d7eaba":"code","59f13e44":"code","12e6bef3":"code","a039b88f":"code","aa1bb63a":"code","ad7d6044":"code","69b35843":"code","d0ed8c49":"code","cc549cd1":"code","4297441f":"code","cab0bd75":"code","40367458":"code","689cddbf":"code","b63133cc":"code","cd98eaae":"code","c4db4b82":"code","ee5020fe":"code","78e38742":"code","9014451b":"code","3758aa8f":"code","60cba302":"code","4d5ace87":"code","6f2fa233":"code","fee99ce0":"markdown","3dbd0b1b":"markdown","eb8ec314":"markdown","12ecf001":"markdown","7b1b79ae":"markdown","f71a555b":"markdown","5929dfe8":"markdown","d1ce6430":"markdown","0fd732ad":"markdown","f0990d71":"markdown","390ac617":"markdown","9d476386":"markdown","86624b98":"markdown","f7523306":"markdown","49089d86":"markdown","4f60cc9f":"markdown","d9eaaef5":"markdown","c99c7487":"markdown","b165dadf":"markdown","40255b60":"markdown","15f4f207":"markdown","1527df36":"markdown","f9a0917b":"markdown","d5bd09ac":"markdown","132be5e5":"markdown"},"source":{"fecc64fb":"# importing all the necessary libraries\nimport numpy as np\nimport pandas as pd\n# plotly for data visualization\nimport plotly.offline as pyo\npyo.init_notebook_mode()\nimport plotly.graph_objects as go\n\nfrom sklearn.feature_selection import chi2  \nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nfrom sklearn import tree\nfrom sklearn import linear_model\nfrom sklearn import ensemble\n\n# sweetviz a popular EDA library \n!pip install sweetviz\nimport sweetviz\n# pandas_profiling another popular EDA library\nfrom pandas_profiling import ProfileReport","fbba76da":"def createFolds():\n    # read the train CSV\n    df = pd.read_csv(\"..\/input\/framingham-heart-study-dataset\/framingham.csv\")\n    # adding a kfold column with value -1\n    df['kfold'] = -1\n    # Random Sample the data with df.sample\n    # Reset the indices and the drop the Index column \n    df = df.sample(frac=1).reset_index(drop=True)\n    # Using Stratified K fold\n    # Stratified ensures equal distribution of all classes in each fold\n    kf =  model_selection.StratifiedKFold(n_splits=5,shuffle=False,random_state=42)\n\n    for fold,(trainId,valId) in enumerate(kf.split(X=df,y=df.TenYearCHD.values)):\n        df.loc[valId,'kfold'] = fold\n    #save the new csv file     \n    df.to_csv('framinghamFolds.csv',index=False)\n","028867a0":"createFolds()","53d9d278":"# read the csv file and display the head \ndf = pd.read_csv(\"framinghamFolds.csv\")\ndf.head()","9b255914":"report = sweetviz.analyze([df,\"Complete DataSet\"],target_feat='TenYearCHD')","04ca14c2":"# savinf the EDA.html file\nreport.show_html(\"EDA_SweetViz.html\")","7a10c2a5":"profile = ProfileReport(df,title=\"Framingham Kaggle Data EDA Report\")\nprofile.to_widgets()","b82568e6":"profile.to_file(\"EDA_PandasProfling.html\")","a1676523":"targetColumn = 'TenYearCHD'\nnumericalColumns = ['age',\n                   'cigsPerDay',\n                    'totChol',\n                    'sysBP',\n                    'diaBP',\n                    'BMI',\n                    'heartRate',\n                    'glucose']\ncategoricalColumns = [ column for column in df.columns if column not in numericalColumns + ['kfold', targetColumn]] ","b2d7eaba":"# dropping column education\n# df = df.drop(['education'], axis=1)\n# df.head()","59f13e44":"# for all the numerical variables fill the missing values with the mean\n# make a list of features we are interested in  \n# kfold and targetColumn is something we should not alter  \nfeatures = [x for x in df.columns if x not in ['kfold', targetColumn]]\nfor feat in features:\n    if feat in numericalColumns:\n        df[feat] = df[feat].replace(np.NaN,df[feat].mean())\ndf.isnull().sum()","12e6bef3":"# Now we can see there are no null values in our data\ndf.dropna(inplace = True)\ndf.isnull().sum()","a039b88f":"# create a Dataframe only for categorical variables\ncategoricalDF = df[categoricalColumns]\n# select only Top 3 variables \nselector = SelectKBest(chi2,k=5)\n# give the targetcolumn and the rest of the data to the scalar to fit\nselector.fit(categoricalDF,df[targetColumn])\n# get the indicies of the selected columns\ncols = selector.get_support(indices=True)\n\n# For display purpose Only\ndfscores = pd.DataFrame(selector.scores_)\ndfcolumns = pd.DataFrame(categoricalDF.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Features','Score']  #naming the dataframe columns\nfeatureScores = featureScores.sort_values(by='Score', ascending=False)\nprint(featureScores)\n# plotting in plotly\n# defining data\ntrace = go.Bar(x=featureScores['Features'],y=featureScores['Score'])\ndata=[trace]\n# defining layout\nlayout = go.Layout(title='Chi-Score TEST For Categorical Features',xaxis=dict(title='Feature Name'),\n                  yaxis=dict(title='Score'),hovermode='closest')\n# defining figure and plotting\nfigure = go.Figure(data=data,layout=layout)\npyo.iplot(figure)","aa1bb63a":"# create a new dataframe from the selected columns\nselectedCategoricalDF = categoricalDF.iloc[:,cols]\nselectedCategoricalDF.head()","ad7d6044":"finalDF = pd.concat([selectedCategoricalDF,df[numericalColumns]],axis=1)\nfinalDF","69b35843":"scaler = MinMaxScaler(feature_range=(0,1)) \n\nnormalizedDF = pd.DataFrame(scaler.fit_transform(finalDF), \n                         columns=finalDF.columns)\n# add the target and kfold column to the new dataframe\nnormalizedDF[targetColumn] = df[targetColumn].to_list()\nnormalizedDF['kfold'] = df['kfold'].to_list()\nnormalizedDF.head()","d0ed8c49":"models = {  \"decision_tree_gini\": tree.DecisionTreeClassifier(criterion=\"gini\"),  \n          \"decision_tree_entropy\": tree.DecisionTreeClassifier(criterion=\"entropy\"),\n          \"logistic_regression\" : linear_model.LogisticRegression(), \n          \"random_forest\" : ensemble.RandomForestClassifier(n_jobs=-1)\n         }","cc549cd1":"def run(fold,df,model):\n    \n    # get the Training and validation data for this fold\n    # training data is where the kfold is not equal to the fold\n    # validation data is where the kfold is equal to the fold\n    trainDF = df[df.kfold != fold].reset_index(drop=True)\n    valDF = df[df.kfold==fold].reset_index(drop=True)\n    \n    # drop the kfold and TenYearCHD column    \n    # convert it into a numpy array\n    xTrain = trainDF.drop(['kfold','TenYearCHD'],axis=1).values\n    yTrain = trainDF.TenYearCHD.values\n    \n    # perform the same for validation\n    xVal = valDF.drop(['kfold','TenYearCHD'],axis=1).values\n    yVal = valDF.TenYearCHD.values\n    \n    # fetch the model from the model dispatcher\n    clf = models[model]\n    \n    #fit the model on the training data\n    clf.fit(xTrain,yTrain)\n    \n    # create probabilities for validation samples\n    preds = clf.predict_proba(xVal)[:,1]\n\n    # get roc auc score\n    auc = metrics.roc_auc_score(yVal,preds)\n    \n    print(f\"Fold={fold}, AUC SCORE={auc}\") ","4297441f":"# select a model from dispacther\nmodel = 'decision_tree_entropy'\n# give the fold number , the dataframe and the model\nprint(model)\nfor fold in range(5):\n    run(fold,normalizedDF,model)","cab0bd75":"# select a model from dispacther\nmodel = 'logistic_regression'\n# give the fold number , the dataframe and the model\nprint(model)\nfor fold in range(5):\n    run(fold,normalizedDF,model)","40367458":"# select a model from dispacther\nmodel = 'random_forest'\n# give the fold number , the dataframe and the model\nprint(model)\nfor fold in range(5):\n    run(fold,normalizedDF,model)","689cddbf":"# select a model from dispacther\nmodel = 'logistic_regression'\nprint(model)\n\n# train the model on a single fold\n# record all the metrics for different threshold.\nfold = 0\n\ntrainDF = normalizedDF[normalizedDF.kfold != fold].reset_index(drop=True)\nvalDF = normalizedDF[normalizedDF.kfold==fold].reset_index(drop=True)\n\nxTrain = trainDF.drop(['kfold','TenYearCHD'],axis=1).values\nyTrain = trainDF.TenYearCHD.values\n\nxVal = valDF.drop(['kfold','TenYearCHD'],axis=1).values\nyVal = valDF.TenYearCHD.values\n\nclf = models[model]\nclf.fit(xTrain,yTrain)\npreds = clf.predict_proba(xVal)[:,1]\nauc = metrics.roc_auc_score(yVal,preds) \n# we will run for only one fold.\n# as the objective here is to look at ROC CURVE.\nfpr,tpr,thresholds = metrics.roc_curve(yVal, preds)\nprint(f\"Fold={fold}, AUC SCORE={auc}\") \n#draw the ROC Curve.     \n# plotting in plotly\n# defining data\ntrace = go.Scatter(x=fpr,y=tpr,text=thresholds)\ndata=[trace]\n# defining layout\nlayout = go.Layout(title='ROC CURVE',xaxis=dict(title='FPR'),\n                  yaxis=dict(title='TPR'),hovermode='closest')\n# defining figure and plotting\nfigure = go.Figure(data=data,layout=layout)\npyo.iplot(figure)\n# create blank lists\ntnList=[]\nfpList=[]\nfnList=[]\ntpList=[]\nprecisionList =[]\nrecallList = []\n# lets make a metics dataframe for all the thresholds\nfor threshold in thresholds:\n    #since our predictions are in probability\n    # convert them to 0 or 1 based on the threshold     \n    tempPred = [1 if x >= threshold else 0 for x in preds] \n    tn, fp, fn, tp = metrics.confusion_matrix(yVal,tempPred).ravel()\n    precision = metrics.precision_score(yVal,tempPred)\n    recall = metrics.recall_score(yVal,tempPred)\n    #append all the values to the appropriate list\n    tnList.append(tn)\n    fpList.append(fp)\n    fnList.append(fn)\n    tpList.append(tp)\n    precisionList.append(precision)\n    recallList.append(recall)\n\nmetricsDF = pd.DataFrame(\n{'Threshold': thresholds,\n 'TN': tnList,\n 'FP': fpList,\n 'FN' : fnList,\n 'TP' : tpList,\n 'Precision' : precisionList,\n 'Recall' : recallList,\n 'TPR' : tpr,\n 'FPR' : fpr\n})\nprint(metricsDF)","b63133cc":"trace = go.Scatter(x=metricsDF['FP'],y=metricsDF['FN'],marker=dict(color=thresholds,\n                                                                   colorscale='viridis',\n                                                                   showscale = True),text=thresholds,mode='lines+markers')\ndata=[trace]\n# defining layout\nlayout = go.Layout(title='False Positive vs False Negative',xaxis=dict(title='False Positive'),\n                  yaxis=dict(title='False Negative'),hovermode='closest')\n# defining figure and plotting\nfigure = go.Figure(data=data,layout=layout)\npyo.iplot(figure)","cd98eaae":"trace = go.Scatter(x=metricsDF['Precision'],y=metricsDF['Recall'],marker=dict(color=thresholds,\n                                                                   colorscale='viridis',\n                                                                   showscale = True),text=thresholds,mode='lines+markers')\ndata=[trace]\n# defining layout\nlayout = go.Layout(title='Precision vs Recall',xaxis=dict(title='Precision'),\n                  yaxis=dict(title='Recall'),hovermode='closest')\n# defining figure and plotting\nfigure = go.Figure(data=data,layout=layout)\npyo.iplot(figure)","c4db4b82":"metricsDF[metricsDF['Threshold'].between(0.12 , 0.16,inclusive=True).to_list()]","ee5020fe":"import joblib","78e38742":"# dump the model\njoblib.dump(clf,\"classifier.pkl\")\n# dump the threshold\n# selecting a random value between 0.13-0.15 which was our preferred range\nthreshold = 0.145\njoblib.dump(threshold,\"threshold.pkl\")\n# dump the features except kfold,TenYearCHD\ndumpFeatures = normalizedDF.drop(['kfold','TenYearCHD'],axis=1).columns.to_list()\njoblib.dump(dumpFeatures,\"features.pkl\")\n# dump the MinMaxScaler\njoblib.dump(scaler,'scaler.pkl')","9014451b":"# create a Dataframe only for categorical variables\nnumericalDF = df[numericalColumns]\n# select only Top 3 variables \nselector = SelectKBest(f_classif,k=4)\n# give the targetcolumn and the rest of the data to the scalar to fit\nselector.fit(numericalDF,df[targetColumn])\n# get the indicies of the selected columns\ncols = selector.get_support(indices=True)\n\n# For display purpose Only\ndfscores = pd.DataFrame(selector.scores_)\ndfcolumns = pd.DataFrame(numericalDF.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Features','Score']  #naming the dataframe columns\nfeatureScores = featureScores.sort_values(by='Score', ascending=False)\nprint(featureScores)\n# plotting in plotly\n# defining data\ntrace = go.Bar(x=featureScores['Features'],y=featureScores['Score'])\ndata=[trace]\n# defining layout\nlayout = go.Layout(title='ANOVA F-TEST For Numerical Features',xaxis=dict(title='Feature Name'),\n                  yaxis=dict(title='Score'),hovermode='closest')\n# defining figure and plotting\nfigure = go.Figure(data=data,layout=layout)\npyo.iplot(figure)","3758aa8f":"# create a new dataframe from the selected columns\nselectedNumericalDF = numericalDF.iloc[:,cols]\nselectedNumericalDF.head()","60cba302":"# combine the best numerical and categorical variables\nfinalDF = pd.concat([selectedCategoricalDF,selectedNumericalDF],axis=1)\nfinalDF","4d5ace87":"# normalize the data\nscaler = MinMaxScaler(feature_range=(0,1)) \n\nnormalizedDF = pd.DataFrame(scaler.fit_transform(finalDF), \n                         columns=finalDF.columns)\n# add the target and kfold column to the new dataframe\nnormalizedDF[targetColumn] = df[targetColumn].to_list()\nnormalizedDF['kfold'] = df['kfold'].to_list()\nnormalizedDF.head()","6f2fa233":"# select a linear regression and see the auc score\nmodel = 'logistic_regression'\n# give the fold number , the dataframe and the model\nprint(model)\nfor fold in range(5):\n    run(fold,normalizedDF,model)","fee99ce0":"## ANOVA F-TEST\n- We will perform ANOVA F-TEST to select the best numerical variables.\n- Combine the best Numerical and Catergorical Variables and evalute the model again.\n- If we get a better AUC SCORE than we can decide to go ahead with the new model or else we will use the old one.","3dbd0b1b":"# Reporting Observations From EDA\n- Since this is a Medical Data that need us to predict the chances of TenYearCHD, I am willing to accept __more FP than FN__. In essence I dont want my model to say that __The Patient is not sick when they are sick__. This is a personal choice that I have taken, generally this is done with the help of a domain expert in this case let's say a Doctor . But for the purpose of this exercise I have made this decision.\n- Since we have a __classification problem__ and  __imbalance target variable__ we will be using __roc auc__ as our evaluation metric to select the best threshold among the best algorithm.\n- Although the total count of __Missing Values__ is low, most of them are present in the numerical variables which can be easily handled by filling in with the mean. Remainig can be dropped for the sake of simplicity.\n- __Dropping__ the __Education__ Column. As I am not able to understand that what this column represents. Moreover the __low correlation factor__ with the target variable and putting this kind of features for a health related data can result to very subjective outcomes.\n- Initial Findings __Current Smoker, Heart Rate, totChol and Glucose__ seems to have __low correlation factor__ with TenYearCHD. We will have the final conclusion in feature selection part. ","eb8ec314":"- Now After collecting all the Metrics for all the thresholds. It's Observation time.\n- As stated above I am willing to accept __more FP than FN__. So will plot FP vs FN with thresholds.\n- We will also plot Precision vs Recall with Thresholds.\n- The ColorScale in based on Threshold values.\n","12ecf001":"- From the above three Graphs: ROC Curve, FP vs FN and Precision vs Recall we can say that the best threshold for our use lies between 0.17 - 0.14 for the trained model.\n- Keep in mind this threshold value can change if the model is trained again.\n- So let's look at the metrics for these threshold value.","7b1b79ae":"- Just a Dislcaimer!! Tree models are presented in the workspace to show the ease of trainig and modularity of the code.\n- But for this particular Data I will be trying to improve the Performance of Logistic Regression Only.","f71a555b":"- Create a final DataFrame combining both the finalilzed Numnerical and Categorical Features","5929dfe8":"## Numerical Variables\n- First step will be fill the NAN values with the mean for that variable.\n- We are going to see more about the Numerical Feature Selection in later sections after a BaseLine Performance.","d1ce6430":"- Select a model.\n- Evaluate that model for all the folds.\n- Compare different models. ","0fd732ad":"## Categorical Variables\n- We will use the SelectKBest with chi-squared test for feature selection . We will select the 10 Best features.\n- The chi-square test is a statistical test of independence to determine the dependency of two categorical variables. \n- As you can see __Education__ will be dropped of here automatically. since we are selecting only top 5 variables.","f0990d71":"# Model Dispatcher & Training\n- After selecting the desired Features, let's run different algorithm on the data and note the Base Performance.\n- Based on this performance we can perform some Feature Engineering. Remember we said this earlier in Numerical Variables.\n- We can select from various models present in the __model dictionary__.\n- After that we will call the __run__ function . \n- It will train the selected model for the given fold on the data and print the __roc auc value__","390ac617":"\n# Framingham Kaggle Data\n# Author : Yash Agarwal\n- The following sequence of code is extracted from the ML Framework that I use for my personal project.\n- Reason for using that Framework : It was developed keeping in mind that it will help me to use the same code for any ML problem statements with minimal changes. So this problem statement fits the bill.\n- [Github Repo](https:\/\/github.com\/yash276\/mlTemplate) for that Framework. But that repo is still incomplete and I have to update it.\n- So my goal is to make a code that can give you a BaseLine performace on different models very quickly just my providing few inputs and no or minimal changes to the Code.\n- Ofcourse based on that performance we can make a dedicated code for that use case.\n","9d476386":"# CONCLUSION\n- All the Categorical Variables eliminated after the Feature selection have suggest a significant role in predicting the TenYearCHD.\n- For Numerical Variables,if we decrease the count we see a deacrese in AUC Score. ALthough if we exclude less significant variables such Heart Rate,Cigsperday as per ANOVA F-Test there is a small decrease in AUC Score. So we decided to let them stay(Can be removed but will have neglegible effect).\n- Our Earlier hypothesis from the EDA i.e. __Current Smoker, Heart Rate, totChol and Glucose__ seems to have __low correlation factor__ was found to be __True__ after Feature Selection. Except for Glucose which ranked 4th in best Numerical Variable.\n- The AUC score of the model is around 71 which is satisfactory.\n- For our trained model keeping Threshold between 0.13 - 0.15 will yield the expected results i.e. __high FP low FN, high Recall low Precision__ \n- Ofcourse the size of the DataSet was very novel around 4000 samples. The above reported performances can be improved with increasing the number of samples.","86624b98":" - As we can see after filling the missing values for numerical variables we are left with very few missing values.\n - So I decied to drop those rows.","f7523306":"## WHY PANDAS_PROFILING?\n- Yes I know this is redundancy. Running this yields somewhat similar result same as SweetViz.\n- But after looking through the EDA_SweetViz.Html many times I realized there are few things that are more refined in Pandas_Profiling and I kind of need those things.\n- For example better and more Correlation Chart with a better visula aspect for better understanding, individual variable interaction with other variables, color scheme to depict missing values and much more.\n- This was included after many iterations of not being completely satisfied with SweetViz.\n- __Why not Remove Sweetviz?__ I thought I will be good to compare both the libraries head on with pros and con. And I hope with his I was able to tell you something new if you were not aware of it :)\n- __Disclaimer__ : We need to run the Pandas Profiling first and then SweetViz. If we reverse the order I was getting some memory issue while running Pandas Profiling. Could not understand this particular problem.\n- This causes the numbers adjacent to Code block be a up and down.","49089d86":"- The above experiment was performed multiple time by varying the count of BEST Numerical Variables.\n- Following Observation was noted :  __decreasing the count of Numerical Varibales decreases the AUC SCORE__","4f60cc9f":"# Feature Selection and Engineering\n- I like to seperate my Numerical and Categorical Variables List.\n- This allows me to apply specifc Feature Engineering for both the categories.\n- This also allows my code to be modular which helps me to use the same code for any ML problem statements with minimal changes.\n- These pieces of code are yet to be updated on my Githu Repo.","d9eaaef5":"- After Selecting the Features we want to train on.\n- Normalize the Features i.e. excluding Target Column and kfold data before training.\n- Add back the 'kfold' and TargetColumn.","c99c7487":"# Storing \n- So before we wrap up, we need to store few things so that we can apply our model to the Test\/Live Data without any bias. Following is the list\n        __model__ The trained and final model.\n        __Threshold__ The selected Threshold value\n        __Features__ Features\/Variables name that were used at the time of training. So that we can drop any   other columns in test data\n        __Normalizing Parameters__ i.e. the mean and std or min and max of these Features, so that we can  normalize the test data in short the MinMaxScaler.","b165dadf":"# Shuffle and Splitting The Data\n- The following piece of code reads the input CSV , add a column named __kfold__ , randomly shuffle the data , applies StratifiedKFold Cross Validation and makes a new CSV with name __inputFolds.csv__ and saves in the input directory.\n- The value of __kfold__ indicates the samples that will be used for __Validation__ while training that fold.\n- __WHY I AM PERFORMING THIS STEP NOW?__   We do not need any prior information on the data to shuffle and split it. So complete that step now and focus on the individual data from now on.","40255b60":"- The above Threshold values looks statisfactory and as per our expectations.\n- We can see we have achieved our goal of haing more FP than FN hence higher Recall.\n- We can use any one of these Threshold values for our Test\/Live Data.\n- We can conclude this experiment now. But I am not happy with the Precision values . Let's try incresing that, for that I want to perform Feature Selection on Numerical Variables and see if we can increase our performace.","15f4f207":"# EDA\n- Now we will work with __framinghamFolds.csv__ \n- Once you impoprt it you will see __kfolds__ as the last column. With the values depecting which sample is used as __Validation__ for that fold while training.","1527df36":"- I will not be using this step in my final version.\n- As dropping an specific column kind of takes the code away for being modular and generating a baseline performance very quickly.","f9a0917b":"- The following function trains the given data on the given model for the specified fold to report the accuracy.\n- Inputs : \n            __fold__ number to select the specific data for train and validation,\n            __df__ the dataframe which will used for training and validation,\n            __model__ which you want to evaluate should be selected from models dictionary defined above ","d5bd09ac":"## WHY SWEETVIZ?\n- Sweetviz is an EDA Library that allows a programmer to extract all the imformation about the data in just __One Line of Code__.\n- It combines all the funcationaly of matplotlib, seaborn , pandas profiling.\n- It outputs count of Numerical and Categorical Variables , Associatations between variables , each varaible max value, min value, std, mean, median, missing values count and much more. You can see the HTML file for more.\n- It runs pretty fast for large datasets as well for eaxmple Samples > 1M. And just one line of code!!!\n- More importantly you can export all that statistics in a __HTML File__ and present it to the stakeholders for better understanding. \n    ","132be5e5":"- The Data Specific coding Starts.\n- As stated I am willing to accept __more FP than FN__. Let's have a look at the ROC Curve for logistic Regression.\n- This will help us understand How to select our Threshold Value for predictions on the Test\/Live Data when the model is deployed.\n- Hover over the line you will see(fpr,tpr) and below that the threshold value for that."}}