{"cell_type":{"c4418305":"code","257fea96":"code","99f38375":"code","4f2c4336":"code","088657be":"code","f8bd4391":"code","d62ffaec":"code","cab445c9":"code","3855d303":"code","88114699":"code","05ffc794":"code","353f95a2":"code","a72825c2":"code","c8e72f5e":"code","efda284d":"code","8f776f25":"code","660e67c5":"code","f028fb67":"code","c716dd65":"code","0fa23f6a":"code","26cd4b6c":"code","a0225333":"code","49bef154":"code","3eea03a7":"code","2d7b6d68":"code","a25a260d":"code","308af825":"code","8879e040":"code","4d8dab07":"markdown","053b29aa":"markdown","ca56c571":"markdown","aff49063":"markdown","02adbfbf":"markdown","4a8f89a1":"markdown","342d8ca7":"markdown","3321a61b":"markdown","a574edf2":"markdown"},"source":{"c4418305":"##import nltk\nimport numpy as np\nimport pandas as pd\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport re\nimport string\nimport tensorflow\nfrom tensorflow.keras import layers\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport os\nimport pickle\n\n##nltk.download(\"wordnet\")\n##nltk.download(\"stopwords\")\n##nltk.download(\"punkt\")\n##nltk.download(\"omw-1.4\")","257fea96":"df = pd.read_csv(\"..\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv\")\ndf","99f38375":"df[\"Category\"].value_counts()","4f2c4336":"df.isnull().sum()","088657be":"len(df) - len(df.drop_duplicates())","f8bd4391":"df.drop_duplicates(inplace = True)","d62ffaec":"ratio_spam_and_ham = df[\"Category\"].value_counts()\nratio_spam_and_ham.plot.pie(autopct = '%1.1f%%')","cab445c9":"from wordcloud import WordCloud\ndef visualize(label):\n    words=''\n    for msg in df[df['Category'] == label]['Message']:\n        msg = msg.lower()\n        words+=msg + ''\n    wordcloud = WordCloud(width=600, height=400).generate(words)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\nprint(\"Featured words in spam messeges:\")\nvisualize('spam')\n\nprint(\"Featured words in non-spam messeges:\")\nvisualize('ham')","3855d303":"def preprocess(text):\n    text = text.lower()\n    text = re.sub(r'https?:\/\/\\S+|www.\\S+', '', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = re.sub(r'[^a-zA-Z]+', ' ', text)\n    text = re.sub(r'[0-9]', '', text)\n    translator = str.maketrans(\"\", \"\", string.punctuation)\n    text = text.translate(translator)\n    words = word_tokenize(text)\n    words = [word for word in words if word not in stopwords.words(\"english\")]\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in words]\n    text = \" \".join(words)\n    return text","88114699":"df[\"Message\"] = df.Message.map(preprocess) ","05ffc794":"MAX_NB_WORDS = 3000\nMAX_SEQUENCE_LENGTH = 300\nEMBEDDING_DIM = 100\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['Message'].values)\nwith open(\".\/tokenizer.pickle\", \"wb\") as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","353f95a2":"X = tokenizer.texts_to_sequences(df['Message'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","a72825c2":"Y = pd.get_dummies(df['Category']).values\nprint('Shape of label tensor:', Y.shape)","c8e72f5e":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","efda284d":"import tensorflow \nfrom tensorflow.keras import layers\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","8f776f25":"model = tensorflow.keras.models.Sequential()\nmodel.add(layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(layers.SpatialDropout1D(0.2))\nmodel.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(layers.Dense(2, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nepochs = 5\nbatch_size = 64\n\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n","660e67c5":"print(history.model.summary())","f028fb67":"plt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","c716dd65":"plt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","0fa23f6a":"predictions = model.predict(X_test)","26cd4b6c":"eval_results = []\nfor i in range(0,len(predictions)):\n    eval_results.append(int(np.argmax(predictions[i])))","a0225333":"Y_test_true = []\nfor i in range(0,len(Y_test)):\n    if(Y_test[i][0] == 0):\n        Y_test_true.append(1)\n    else:\n        Y_test_true.append(0)","49bef154":"from sklearn.metrics import classification_report\ntarget_names = ['ham','spam']\nprint(classification_report(Y_test_true, eval_results, target_names=target_names))","3eea03a7":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test_true, eval_results)\ncm_df = pd.DataFrame(cm,\n                     index = target_names, \n                     columns = target_names)\ncm_df","2d7b6d68":"from sklearn import metrics\nprint(\"AUC:\",metrics.roc_auc_score(Y_test_true, eval_results))\ncutoff_grid = np.linspace(0.0,1.0,100)\nTPR = []\nFPR = []\ncutoff_grid\nFPR, TPR, cutoffs = metrics.roc_curve(Y_test_true, eval_results,pos_label=1)\nplt.plot(FPR,TPR,c='red',linewidth=1.0)\nplt.xlabel('False Positive')\nplt.ylabel('True Positive')\nplt.title('ROC Curve')\nplt.show()","a25a260d":"fpr_en, tpr_en, thresholds_en = metrics.roc_curve(Y_test_true, eval_results)\nroc_auc_en = metrics.auc(fpr_en, tpr_en)\nprecision_en, recall_en, th_en = metrics.precision_recall_curve(Y_test_true, eval_results)\nplt.plot([1, 0], [0, 1], 'k--')\nplt.plot(fpr_en, tpr_en, label='rnn (area = %0.3f)' % roc_auc_en)\nplt.plot(recall_en,precision_en , label='Recall rnn')\nplt.title('Precision vs. Recall')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend(loc='best')\nplt.show()","308af825":"model.save(\".\/Final Model.h5\")","8879e040":"from keras.models import load_model\nloadedModel = load_model(\".\/Final Model.h5\")\ntestText = str(input(\"Please , Enter Message\\n\"))\ntestText = preprocess(testText)\ntestText = [testText]\nwith open(\".\/tokenizer.pickle\", \"rb\") as handle:\n    tokenizer = pickle.load(handle)\ntestText = tokenizer.texts_to_sequences(testText)\ntestText = pad_sequences(testText, maxlen=MAX_SEQUENCE_LENGTH)\npred = loadedModel.predict(testText)\neval_results = []\nfor i in range(0,len(pred)):\n    eval_results.append(int(np.argmax(pred[i])))\nif eval_results[0] == 1:\n    print(\"Spam\")\nelif eval_results[0] == 0:\n    print(\"Ham\")","4d8dab07":"## Model Testing","053b29aa":"## Data Visualization","ca56c571":"## Save Model","aff49063":"## Model Training","02adbfbf":"## Done","4a8f89a1":"## Model Evaluation","342d8ca7":"## Check Missing Data and Duplicates","3321a61b":"## Data PreProcessing (NLP)","a574edf2":"## Import Libraries"}}