{"cell_type":{"7ccd1a83":"code","77da2c0a":"code","169d748c":"code","1aa21e6d":"code","9e0d8ea2":"code","c3a1bcaf":"code","ced9cd0a":"code","d237e7cf":"code","903e4b13":"code","176ad01c":"code","adc3fdf7":"code","55015412":"code","ebc5bdf4":"code","2e36e16a":"code","864755bb":"code","df541619":"code","9a3823b8":"code","a166a865":"code","32629f4c":"code","59d65f47":"code","88f79a39":"code","e272a324":"code","5beac5f9":"code","d833baeb":"code","958a42a2":"code","9ca3f390":"code","4ba2ada2":"code","7267cdf2":"code","3e91610a":"code","f78f0074":"code","29e1db05":"code","76e2d208":"code","5db88be3":"code","c719b27b":"code","9bb43ca4":"code","43a1218c":"code","130acbe9":"code","7040b154":"code","9551c073":"code","4a76d211":"code","cf823bad":"code","d4c882cf":"code","5c9afd3b":"code","dd50528d":"code","fbd1f046":"code","35bdfe08":"code","b549f4c1":"code","30fe45a4":"code","95c7790c":"code","28da14b9":"code","d92295ee":"code","2f43ca04":"code","84381d25":"code","83adee1d":"code","9c30d7e0":"code","e9361f08":"code","5a41852a":"code","03896fc2":"code","96dedd5f":"code","0504cff0":"code","f547f283":"markdown","942e0953":"markdown","7c0d5589":"markdown","bbb4debb":"markdown","fffb2549":"markdown","13f7532c":"markdown","df3d1f12":"markdown","179dc28a":"markdown","ba9e4d71":"markdown","660d74df":"markdown","2752d080":"markdown","522625f3":"markdown","0c72df52":"markdown","6d2b4183":"markdown","c486bafd":"markdown","0bc8dcb4":"markdown","dce3b411":"markdown","cfc1b844":"markdown","38e5bf1e":"markdown","de75392b":"markdown","30528fcf":"markdown","83f28815":"markdown","965dc291":"markdown","cd301914":"markdown","2469247c":"markdown","acba7e5a":"markdown","4b692d6d":"markdown","227fff0f":"markdown"},"source":{"7ccd1a83":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\nfrom collections import  Counter\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nstop=set(stopwords.words('english'))\n\nfrom tqdm.notebook import tqdm\n\nimport os\nimport re\nimport time\nimport string\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\n\nfrom torchtext import data, datasets\nfrom torchtext.vocab import Vectors, GloVe","77da2c0a":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","169d748c":"# Import Data\n\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","1aa21e6d":"train.head()","9e0d8ea2":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","c3a1bcaf":"fig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\ntrain.groupby('target').count()['id'].plot(kind='pie', ax=axes[0], labels=['Not Disaster (57%)', 'Disaster (43%)'])\nsns.countplot(x=train['target'], hue=train['target'], ax=axes[1])\n\naxes[0].set_ylabel('')\naxes[1].set_ylabel('')\naxes[1].set_xticklabels(['Not Disaster (4342)', 'Disaster (3271)'])\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Target Distribution in Training Set', fontsize=13)\naxes[1].set_title('Target Count in Training Set', fontsize=13)\n\nplt.show()","ced9cd0a":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n\n# No Disaster Tweets\ntrain_len = train[train['target']==0]['text'].str.len()\nax1.hist(train_len,color='green')\nax1.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\n\n# Disaster Tweets\ntrain_len = train[train['target']==1]['text'].str.len()\nax2.hist(train_len,color='red')\nax2.set_title('Disaster tweets')\n\nplt.show()","d237e7cf":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\ntrain_len = train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax1.hist(train_len,color='green')\nax1.set_title('Not disaster tweets')\n\ntrain_len = train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax2.hist(train_len,color='red')\nax2.set_title('Disaster tweets')\n\nfig.suptitle('Words in a tweet')\nplt.show()","903e4b13":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\ncorpus0 = create_corpus(0)\ncorpus1 = create_corpus(1)\nlen(corpus0)","176ad01c":"dic=defaultdict(int)\nfor word in corpus0:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \nx,y=zip(*top)\nplt.bar(x,y, color=\"green\")","adc3fdf7":"dic=defaultdict(int)\nfor word in corpus1:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \nx,y=zip(*top)\nplt.bar(x,y)","55015412":"plt.figure(figsize=(10,5))\n\ndic=defaultdict(int)\n\nspecial = string.punctuation\nfor i in corpus1:\n    if i in special:\n        dic[i]+=1\n        \nx,y = zip(*dic.items())\nplt.bar(x,y)","ebc5bdf4":"plt.figure(figsize=(10,5))\n\ndic=defaultdict(int)\n\nspecial = string.punctuation\nfor i in (corpus0):\n    if i in special:\n        dic[i]+=1\n        \nx,y = zip(*dic.items())\nplt.bar(x,y,color='green')","2e36e16a":"counter = Counter(corpus1)\nmost_common = counter.most_common()\nx=[]\ny=[]\nfor word,count in most_common[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","864755bb":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","df541619":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(train['text'], 10)\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","9a3823b8":"df=pd.concat([train,test])\ndf.shape","a166a865":"df.drop(columns=['keyword','location'], inplace=True)\ndf.head()","32629f4c":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\")","59d65f47":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\nremove_html(\"<h1>Real or Fake<\/h1>\")","88f79a39":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","e272a324":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nremove_punct(\"I am a #king\")","5beac5f9":"def clean_data(text):\n    text = text.apply(lambda x : remove_URL(x))\n    text = text.apply(lambda x : remove_html(x))\n    text = text.apply(lambda x : remove_emoji(x))\n    text = text.apply(lambda x : remove_punct(x))\n    return text","d833baeb":"df.text = clean_data(df.text)","958a42a2":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(df['text'], 10)\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","9ca3f390":"keywords = train.keyword.unique()[1:]\nkeywords = list(map(lambda x: x.replace('%20', ' '), keywords))\n\nwnl = WordNetLemmatizer()\n\ndef lemmatize_sentence(sentence):\n    sentence_words = sentence.split(' ')\n    new_sentence_words = list()\n    \n    for sentence_word in sentence_words:\n        sentence_word = sentence_word.replace('#', '')\n        new_sentence_word = wnl.lemmatize(sentence_word.lower(), wordnet.VERB)\n        new_sentence_words.append(new_sentence_word)\n        \n    new_sentence = ' '.join(new_sentence_words)\n    new_sentence = new_sentence.strip()\n    \n    return new_sentence","4ba2ada2":"df['text'] = df['text'].apply(lambda x: lemmatize_sentence(x))","7267cdf2":"# save train, test and validation datasets into separate csv files.\ndef prepare_csv(df_train, df_test, seed=27, val_ratio=0.3):\n    idx = np.arange(df_train.shape[0])\n    \n    np.random.seed(seed)\n    np.random.shuffle(idx)\n    \n    val_size = int(len(idx) * val_ratio)\n    \n    if not os.path.exists('cache'):\n        os.makedirs('cache')\n    \n    df_train.iloc[idx[val_size:], :][['id', 'target', 'text']].to_csv(\n        'cache\/dataset_train.csv', index=False\n    )\n    \n    df_train.iloc[idx[:val_size], :][['id', 'target', 'text']].to_csv(\n        'cache\/dataset_val.csv', index=False\n    )\n    \n    df_test[['id', 'text']].to_csv('cache\/dataset_test.csv',\n                   index=False)","3e91610a":"# wrapper for iterating through TabularDataset\ndef get_iterator(dataset, batch_size, train=True,\n                 shuffle=True, repeat=False):\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                          else 'cpu')\n    \n    dataset_iter = data.Iterator(\n        dataset, batch_size=batch_size, device=device,\n        train=train, shuffle=shuffle, repeat=repeat,\n        sort=False\n    )\n    \n    return dataset_iter","f78f0074":"import logging\nfrom copy import deepcopy\n\nLOGGER = logging.getLogger('tweets_dataset')\n\ndef get_dataset(fix_length=100, lower=False, vectors=None):\n    \n    if vectors is not None:\n        lower=True\n        \n    LOGGER.debug('Preparing CSV files...')\n    prepare_csv(train, test)\n    \n    TEXT = data.Field(sequential=True, \n#                       tokenize='spacy', \n                      lower=True, \n                      include_lengths=True, \n                      batch_first=True, \n                      fix_length=25)\n    LABEL = data.Field(use_vocab=True,\n                       sequential=False,\n                       dtype=torch.float16)\n    ID = data.Field(use_vocab=False,\n                    sequential=False,\n                    dtype=torch.float16)\n    \n    \n    LOGGER.debug('Reading train csv files...')\n    \n    train_temp, val_temp = data.TabularDataset.splits(\n        path='cache\/', format='csv', skip_header=True,\n        train='dataset_train.csv', validation='dataset_val.csv',\n        fields=[\n            ('id', ID),\n            ('target', LABEL),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Reading test csv file...')\n    \n    test_temp = data.TabularDataset(\n        path='cache\/dataset_test.csv', format='csv',\n        skip_header=True,\n        fields=[\n            ('id', ID),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Building vocabulary...')\n    \n    TEXT.build_vocab(\n        train_temp, val_temp, test_temp,\n        max_size=20000,\n        min_freq=10,\n        vectors=GloVe(name='6B', dim=300)  # We use it for getting vocabulary of words\n    )\n    LABEL.build_vocab(\n        train_temp\n    )\n    ID.build_vocab(\n        train_temp, val_temp, test_temp\n    )\n    \n    word_embeddings = TEXT.vocab.vectors\n    vocab_size = len(TEXT.vocab)\n    \n    train_iter = get_iterator(train_temp, batch_size=32, \n                              train=True, shuffle=True,\n                              repeat=False)\n    val_iter = get_iterator(val_temp, batch_size=32, \n                            train=True, shuffle=True,\n                            repeat=False)\n    test_iter = get_iterator(test_temp, batch_size=32, \n                             train=False, shuffle=False,\n                             repeat=False)\n    \n    \n    LOGGER.debug('Done preparing the datasets')\n    \n    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter","29e1db05":"TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter = get_dataset()","76e2d208":"class LSTMClassifier(torch.nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights):\n        super(LSTMClassifier, self).__init__()\n        \n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.word_embeddings = torch.nn.Embedding(vocab_size,\n                                                  embedding_dim)\n        self.word_embeddings.weight = torch.nn.Parameter(weights,\n                                                         requires_grad=False)\n        \n        self.dropout_1 = torch.nn.Dropout(0.3)\n        self.lstm = torch.nn.LSTM(embedding_dim,\n                                  hidden_dim,\n                                  n_layers,\n                                  dropout=0.3,\n                                  batch_first=True)\n        \n        self.dropout_2 = torch.nn.Dropout(0.3)\n        self.label_layer = torch.nn.Linear(hidden_dim, output_size)\n        \n        self.act = torch.nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        \n        x = self.word_embeddings(x)\n        \n        x = self.dropout_1(x)\n        \n        lstm_out, hidden = self.lstm(x, hidden)\n                \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout_2(lstm_out)\n        out = self.label_layer(out)    \n        \n        out = out.view(batch_size, -1, self.output_size)\n        out = out[:, -1, :]\n\n        out = self.act(out)\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        \n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        \n        return hidden","5db88be3":"def train_model(model, train_iter, val_iter, optim, loss, num_epochs, batch_size=32):\n    h = model.init_hidden(batch_size)\n    \n    clip = 5\n    val_loss_min = np.Inf\n    \n    total_train_epoch_loss = list()\n    total_train_epoch_acc = list()\n        \n    total_val_epoch_loss = list()\n    total_val_epoch_acc = list()\n        \n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                           else 'cpu')\n    \n    for epoch in range(num_epochs):\n\n        model.train()\n        \n        train_epoch_loss = list()\n        train_epoch_acc = list()\n        \n        val_epoch_loss = list()\n        val_epoch_acc = list()\n        \n        for idx, batch in enumerate(tqdm(train_iter)):\n            h = tuple([e.data for e in h])\n\n            text = batch.text[0]\n            target = batch.target\n            target = target - 1\n            target = target.type(torch.LongTensor)\n\n            text = text.to(device)\n            target = target.to(device)\n\n            optim.zero_grad()\n            \n            if text.size()[0] is not batch_size:\n                continue\n            \n            prediction, h = model(text, h)\n                \n            loss_train = loss(prediction.squeeze(), target)\n            loss_train.backward()\n\n            num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n            acc = 100.0 * num_corrects \/ len(batch)\n\n            train_epoch_loss.append(loss_train.item())\n            train_epoch_acc.append(acc.item())\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n            \n            optim.step()\n    \n        print(f'Train Epoch: {epoch}, Training Loss: {np.mean(train_epoch_loss):.4f}, Training Accuracy: {np.mean(train_epoch_acc): .2f}%')\n\n        model.eval()\n\n        with torch.no_grad():\n            for idx, batch in enumerate(tqdm(val_iter)):\n                val_h = tuple([e.data for e in h])\n\n                text = batch.text[0]\n                target = batch.target\n                target = target - 1\n                target = target.type(torch.LongTensor)\n                \n                text = text.to(device)\n                target = target.to(device)\n                \n                if text.size()[0] is not batch_size:\n                    continue\n\n                prediction, h = model(text, h)\n                loss_val = loss(prediction.squeeze(), target)\n\n                num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n                acc = 100.0 * num_corrects \/ len(batch)\n\n                val_epoch_loss.append(loss_val.item())\n                val_epoch_acc.append(acc.item())\n                \n            print(f'Vadlidation Epoch: {epoch}, Training Loss: {np.mean(val_epoch_loss):.4f}, Training Accuracy: {np.mean(val_epoch_acc): .2f}%')\n                \n            if np.mean(val_epoch_loss) <= val_loss_min:\n#                 torch.save(model.state_dict(), 'state_dict.pth')\n                print('Validation loss decreased ({:.6f} --> {:.6f})'.\n                      format(val_loss_min, np.mean(val_epoch_loss)))\n                \n                val_loss_min = np.mean(val_epoch_loss)\n                \n        total_train_epoch_loss.append(np.mean(train_epoch_loss))\n        total_train_epoch_acc.append(np.mean(train_epoch_acc))\n    \n        total_val_epoch_loss.append(np.mean(val_epoch_loss))\n        total_val_epoch_acc.append(np.mean(val_epoch_acc))\n    \n    return (total_train_epoch_loss, total_train_epoch_acc,\n            total_val_epoch_loss, total_val_epoch_acc)","c719b27b":"lr = 1e-4\nbatch_size = 32\noutput_size = 2\nhidden_size = 128\nembedding_length = 300\n\nmodel = LSTMClassifier(vocab_size=vocab_size, \n                       output_size=output_size, \n                       embedding_dim=embedding_length,\n                       hidden_dim=hidden_size,\n                       n_layers=2,\n                       weights=word_embeddings\n)\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available()\n                      else 'cpu')\n    \nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nloss = torch.nn.CrossEntropyLoss()\n    \ntrain_loss, train_acc, val_loss, val_acc = train_model(model=model,\n                                                       train_iter=train_iter,\n                                                       val_iter=val_iter,\n                                                       optim=optimizer,\n                                                       loss=loss,\n                                                       num_epochs=20,\n                                                       batch_size=batch_size)\n    ","9bb43ca4":"plt.figure(figsize=(10, 6))\nplt.title('Loss')\nsns.lineplot(range(len(train_loss)), train_loss, label='train')\nsns.lineplot(range(len(val_loss)), val_loss, label='test')","43a1218c":"plt.figure(figsize=(10, 6))\nplt.title('Accuracy')\nsns.lineplot(range(len(train_acc)), train_acc, label='train')\nsns.lineplot(range(len(val_acc)), val_acc, label='test')","130acbe9":"results_target = list()\n\nwith torch.no_grad():\n    for batch in tqdm(test_iter):\n        for text, idx in zip(batch.text[0], batch.id):\n            text = text.unsqueeze(0)\n            res, _ = model(text, hidden=None)\n\n            target = np.round(res.cpu().numpy())\n            \n            results_target.append(target[0][1])","7040b154":"sample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsample_submission['target'] = list(map(int, results_target))\nsample_submission.head()","9551c073":"sample_submission.to_csv('submission_lstm.csv', index=False)","4a76d211":"import tokenizers\nimport transformers","cf823bad":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\ndf=pd.concat([train,test])\ndf.drop(columns=['keyword','location'], inplace=True)\ndf.text = clean_data(df.text)","d4c882cf":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self,df,y=None, max_len=128):\n        self.df = df\n        self.y = y\n        self.max_len= max_len\n        self.tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n        \n    def __getitem__(self,index):\n        row = self.df.iloc[index]\n        ids,masks = self.get_input_data(row)\n        data = {}\n        data['ids'] = ids\n        data['masks'] = masks\n        if self.y is not None:\n            data['out'] = torch.tensor(self.y.iloc[index],dtype=torch.float32)\n        return data\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self,row):\n        row = self.tokenizer.encode(row,add_special_tokens=True,add_prefix_space=True)\n        padded = row + [0] * (self.max_len - len(row))\n        padded = torch.tensor(padded, dtype=torch.int64)\n        mask = torch.where(padded != 0 , torch.tensor(1), torch.tensor(0))\n        return padded, mask","5c9afd3b":"train_df = df[df.target.isnull() == False]\ntest_df = df[df.target.isnull() == True]","dd50528d":"train_x, val_x, train_y, val_y = train_test_split(train_df.text, train_df.target, test_size=0.2, stratify=train_df.target)","fbd1f046":"batch_size = 32\nnum_workers = 2\n\ntrain_loader = torch.utils.data.DataLoader(\n    Dataset(train_x,train_y),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers\n)\n\nval_loader = torch.utils.data.DataLoader(\n    Dataset(val_x,val_y),\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers\n)","35bdfe08":"next(iter(train_loader))","b549f4c1":"class RobertaClassificationModel(nn.Module):\n    def __init__(self):\n        super(RobertaClassificationModel,self).__init__()\n        self.distilBert = transformers.RobertaModel.from_pretrained('roberta-base')\n        self.l0 = nn.Linear(768,512)\n        self.d0 = nn.Dropout(0.5)\n        self.l1 = nn.Linear(512,256)\n        self.d1 = nn.Dropout(0.5)\n        self.l2 = nn.Linear(256,1)\n        self.d2 = nn.Dropout(0.5)\n        \n#         nn.init.normal_(self.l0.weight,std=0.2)\n#         nn.init.normal_(self.l1.weight,std=0.2)\n#         nn.init.normal_(self.l2.weight,std=0.2)\n    \n    def forward(self,ids,masks):\n        hid = self.distilBert(ids,attention_mask=masks)\n        hid = hid[0][:,0]\n        x = self.d0(hid)\n        x = self.l0(x)\n        x = F.leaky_relu(x)\n        x = self.d1(x)\n        x = self.l1(x)\n        x = F.leaky_relu(x)\n        x = self.d2(x)\n        x = self.l2(x)\n        return x","30fe45a4":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = RobertaClassificationModel().to(device)","95c7790c":"model","28da14b9":"def accuracy_score(outputs,labels):\n    outputs = torch.round(torch.sigmoid(outputs))\n    correct = (outputs == labels).sum().float()\n    return correct\/labels.size(0)","d92295ee":"def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs):\n    model.cuda()\n\n    losses = {'train': list(), 'val': list()}\n    accuracies = {'train': list(), 'val': list()}\n    \n    for epoch in range(num_epochs):\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            epoch_loss = 0.0\n            epoch_acc = 0.0\n            \n            tk0 = tqdm(dataloaders_dict[phase], total=len(dataloaders_dict[phase]))\n            \n            for i, data in enumerate(tk0):\n\n                ids = data['ids'].cuda()\n                masks = data['masks'].cuda()\n                labels = data['out'].cuda()\n                labels = labels.unsqueeze(1)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    x = model(ids, masks)\n                    loss = criterion(x, labels)\n                    \n                    if phase == 'train':\n                        loss.backward(retain_graph=True)\n                        optimizer.step()\n\n                    epoch_loss += loss.item() * len(ids)\n                    epoch_acc += accuracy_score(x, labels)\n\n            epoch_loss = epoch_loss \/ len(dataloaders_dict[phase])\n            epoch_acc = epoch_acc \/ len(dataloaders_dict[phase])\n            \n            losses[phase].append(epoch_loss)\n            accuracies[phase].append(epoch_acc.item())\n\n            print('Epoch {}\/{} | {:^5} | Loss: {:.4f} | Accuracy: {:.4f}'.format(\n                epoch + 1, num_epochs, phase, epoch_loss, epoch_acc))\n            \n\n    torch.save(model.state_dict(), 'model.pth')\n    \n    return losses, accuracies","2f43ca04":"dataloaders_dict = {'train': train_loader, 'val': val_loader}\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\ncriterion = nn.BCEWithLogitsLoss()","84381d25":"num_epochs = 5\n\nlosses, accuracies = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs)","83adee1d":"plt.figure(figsize=(10, 6))\nplt.title('Loss')\nsns.lineplot(range(len(losses['train'])), losses['train'], label='train')\nsns.lineplot(range(len(losses['val'])), losses['val'], label='val')","9c30d7e0":"plt.figure(figsize=(10, 6))\nplt.title('Accuracy')\nsns.lineplot(range(len(accuracies['train'])), accuracies['train'], label='train')\nsns.lineplot(range(len(accuracies['val'])), accuracies['val'], label='val')","e9361f08":"def make_predictions(test_loader):\n    predictions = []\n    \n    model = RobertaClassificationModel()\n    model.cuda()\n    model.load_state_dict(torch.load('\/kaggle\/working\/model.pth'))\n    model.eval()\n    \n    tk = tqdm(test_loader, total=len(test_loader))\n    \n    for _, data in enumerate(tk):\n        ids = data['ids'].cuda()\n        masks = data['masks'].cuda()\n        \n        with torch.no_grad():\n            outputs = model(ids,masks)\n            predictions += outputs.cpu().detach().numpy().tolist()\n        \n    \n    predictions = np.round(1\/(1 + np.exp(-np.array(predictions))))\n    predictions = np.array(predictions, dtype=np.uint8)\n    return predictions","5a41852a":"test_loader = torch.utils.data.DataLoader(\n    Dataset(test_df['text'],y=None),\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers)","03896fc2":"next(iter(test_loader))","96dedd5f":"predictions = make_predictions(test_loader)","0504cff0":"submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission['target'] = predictions\nsubmission.to_csv('submission_roberta.csv', index=False)\nsubmission.head()","f547f283":"Just by looking at the most common words, we can see that a lot of data cleaning is needed before applying any kind of model if we wish to achieve a good accuracy.","942e0953":"## Common Stopwords in Tweets\n\nIn computing, stop words are words which are filtered out before or after processing of natural language data. [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Stop_word)\n\u201cstop words\u201d usually refers to the most common words in a language. There is no universal list of \u201cstop words\u201d that is used by all NLP tools in common. Stopwords are the words in any language which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence.\n\nHere, we'll use NLTK, The Natural Language Toolkit, to process stopwords in the tweets.\n\nFirst, we'll analyze tweets with class `0`, i.e. No real disaster.","7c0d5589":"## Imports","bbb4debb":"We're only interested in `text` and `target` columns in this case, so we'll drop the other columns.","fffb2549":"It seems to be working fine. Most of the junk values have been cleaned up. Now it's time to vectorize our corpus, and for that, we'll use GloVe.","13f7532c":"## Number of words in Tweets","df3d1f12":"## Word Embeddings\nOne way to feed our model text data is to treat each word in our vocabulary as a separate feature and one hot encode them (**Bag of Words encoding**). This works and performs decently, but there is a major drawback to this approach\n\n#### Mathematical intuition:\nWe are interested in constructing a vector space to represent our words. Suppose our vocabulary is: 'cat', 'dog', 'plant', 'leaf', 'man', woman'. Then we can form a vector space via:\n\n$$ cat = (1, 0, 0, 0, 0, 0) $$\n$$ dog = (0, 1, 0, 0, 0, 0) $$\n$$ plant = (0, 0, 1, 0, 0, 0) $$\n$$ leaf = (0, 0, 0, 1, 0, 0) $$\n$$ man = (0, 0, 0, 0, 1, 0) $$\n$$ woman = (0, 0, 0, 0, 0, 1) $$\n \nBut these vectors form an orthogonal basis, so when we take the dot product of them, we get `0`:\n\n$$ cat \\cdot dog = (1, 0, 0, 0, 0, 0) \\cdot (0, 1, 0, 0, 0, 0) = 0$$\n \nThis means that all of these vectors are as far as possible from each other in the vector space, i.e., they are not similar. But words like cat and dog are similar in meaning, so it would be great if our word encoddings could somehow capture this similarity\n\nSo, instead of following the Bag of Word approach, we will import the pre-trained words from GloVe and use them to construct our word embeddings. But what is GloVe?\n\n\"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\"\n\nFor example, we can take the vector for King and subtract the vector for Man and the resulting vector is remarkably close to the vector for Queen. Using these word encodings as opposed to the Bag of Word encodings will substantially improve our classification accuracy.\n\nFor embeddings, here we are using [GloVe](https:\/\/nlp.stanford.edu\/projects\/glove\/):\n\n> Glove produces dense vector embeddings of words, where words that occur together are close in the resulting vector space.","179dc28a":"Now,we will analyze tweets with class `1`.","ba9e4d71":"### Analyzing Punctuation\nFirst let's check tweets indicating real disaster.","660d74df":"## Number of characters in Tweets","2752d080":"### Class Distribution\nBefore going any further, let's quickly check the class distribution. There are only two classe, `0` (No disaster) and `1` (Disaster). ","522625f3":"Let's quickly see the most common bigrams now and see if data cleaning helped or not.","0c72df52":"### Dataset Class\nDataset class is inherited from `torch.utils.data.Dataset` class. It is mandatory to overwrite `getitem()` and `len()` functions. This class will preprocess all data needed for the model. The text is tokenized using `roberta-base` vocabulary. RoBERTa model for text classification expects the input of the model be in form: \n> [cls_token] [......token_ids_of_tokenized_text.......] [sep_token]\n\nWe have taken `max_len=96` here. If a text is less than `max_len` we will pad them with 1's and set the corresponding mask 0.","6d2b4183":"## Common Words\nLet's take a look at the most common words in the dataset.","c486bafd":"### Most common bigrams","0bc8dcb4":"## Predictions","dce3b411":"There are more tweets with class `0` than with class `1`.","cfc1b844":"# LSTM Model\nFirst, we'll create LSTM model in PyTorch, and see how well it performs.","38e5bf1e":"Now,we will move on to class `0`.","de75392b":"# Natural Language Processing with Disaster Tweets\nPredict which Tweets are about real disasters and which ones are not","30528fcf":"## PyTorch LSTM Model","83f28815":"## Predictions using LSTM","965dc291":"The character distribution is almost the same in both cases. We can also see that no tweets exceeds 160 characters. ","cd301914":"## Exploratory Data Analysis","2469247c":"# RoBerta Transformer","acba7e5a":"## PyTorch Dataset","4b692d6d":"Good thing to do is [lemmatizing](https:\/\/en.wikipedia.org\/wiki\/Lemmatisation). We can do it using [nltk](http:\/\/www.nltk.org\/book\/) library.\n\n> Lemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.","227fff0f":"# Data Cleaning"}}