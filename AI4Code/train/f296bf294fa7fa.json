{"cell_type":{"8f79ac32":"code","09e06390":"code","36bcc32a":"code","3a903290":"code","bd97fe32":"code","2221af71":"code","7a23cc8c":"code","e7a68f51":"code","66a0bdfa":"code","222c094a":"code","690e4f60":"code","59d9d404":"code","8866a20c":"code","f6981eef":"code","dcf13b79":"code","ef83df99":"code","5199589a":"code","d0dbee15":"code","a9bdcff6":"code","555f1581":"code","bf75b95f":"code","428a7ed1":"code","538de9a6":"code","8d075409":"code","397d6f12":"code","c323ffa7":"code","e9e216f9":"code","2b003399":"code","36b7af7d":"code","f5873d22":"code","6d9adb6d":"code","30d75980":"markdown","0d8b9a51":"markdown","86265da5":"markdown"},"source":{"8f79ac32":"import numpy as np\nimport tensorflow as tf\nimport keras\n\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Conv2D ,LSTM, MaxPooling2D , Flatten , Dropout \nfrom keras.layers import BatchNormalization,TimeDistributed\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (15,5)\nimport seaborn as sns\nimport pandas as pd\nimport os\nimport random\nimport time\nimport os\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom scipy import signal\nfrom scipy.fft import fftshift\n\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn; sn.set(font_scale=1.4)\nfrom sklearn.utils import shuffle  \n","09e06390":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils.np_utils import to_categorical","36bcc32a":"ACTIONS = [\"kiri\", \"maju\",\"idle\",\"kanan\"]\nreshape = (-1,8, 60,1)\nreshape2 = (-1,8, 60)\n","3a903290":"def create_data(starting_dir=\"..\/input\/gabungan\/gabungan\"):\n    training_data = {}\n    for action in ACTIONS:\n        if action not in training_data:\n            training_data[action] = []\n        data_dir = os.path.join(starting_dir,action)\n        for item in os.listdir(data_dir):\n            data = np.load(os.path.join(data_dir, item))\n            for item in data:\n                training_data[action].append(item)\n\n    lengths = [len(training_data[action]) for action in ACTIONS]\n    print(lengths)\n\n    for action in ACTIONS:\n        np.random.shuffle(training_data[action])  \n        training_data[action] = training_data[action][:min(lengths)]\n\n    lengths = [len(training_data[action]) for action in ACTIONS]\n    print(lengths)\n    combined_data = []\n    for action in ACTIONS:\n        for data in training_data[action]:\n            if action == \"kiri\":\n                combined_data.append([data, [1, 0, 0,0]])\n            elif action == \"maju\":\n                combined_data.append([data, [0, 1, 0, 0]])\n            elif action == \"idle\":\n                combined_data.append([data, [0, 0, 1, 0]])\n            elif action == \"kanan\":\n                combined_data.append([data, [0, 0, 0, 1]])\n\n    np.random.shuffle(combined_data)\n    print(\"length:\",len(combined_data))\n    return combined_data\n","bd97fe32":"print(\"creating training data\")\ntraindata = create_data(starting_dir=\"..\/input\/gabungan\/gabungan\")\ntrain_X = []\ntrain_y = []\n\nfor X, y in traindata:\n    train_X.append(X)\n    train_y.append(y)\n\ntrain_X = np.array(train_X).reshape(-1,60,8,1)\ntrain2_X = np.array(train_X).reshape(reshape2)\n\ntrain_y = np.array(train_y)\nlabels=np.argmax(train_y, axis=1)\n\ntrain_X.shape,labels.shape","2221af71":"ye=pd.DataFrame(train_y)\n\nye.columns=[\"kiri\", \"maju\",\"idle\",\"kanan\"]\ncategories = list(ye.columns.values)\n\nsns.set(font_scale = 1)\nplt.figure(figsize=(10,8))\n\nax= sns.barplot(categories, ye.iloc[:,0:].sum().values)\nplt.title(\"Number of samples labeled as active (1) out of {0} length data\".format((ye.shape[0])),fontsize=20)\nplt.ylabel('Number of events', fontsize=18)\nplt.xlabel('Event Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = ye.iloc[:,0:].sum().values\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom', fontsize=10)\nplt.show()\n","7a23cc8c":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience = 5, verbose=1,factor=0.5, min_lr=0.0001)","e7a68f51":"\ndef cnn_model():    \n\n    model = Sequential()\n    \n    model.add(Conv2D(filters = 64, kernel_size = (8,8), \n                     padding = \"same\",activation = \"relu\", input_shape=train_X.shape[1:]))\n    model.add(MaxPooling2D(pool_size = (4,4)))\n    model.add(Dropout(0.1))\n    model.add(BatchNormalization())\n    \n    model.add(Conv2D(filters = 64, kernel_size = (4,4), \n                     padding = \"same\", activation = \"relu\",input_shape=train_X.shape[1:]))\n    model.add(MaxPooling2D(pool_size = (2,2)))\n    model.add(Dropout(0.1))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(filters = 64, kernel_size = (2,2), \n                     padding = \"same\", activation = \"relu\",input_shape=train_X.shape[1:]))\n    model.add(MaxPooling2D(pool_size = (1,1)))\n    model.add(Dropout(0.1))\n    model.add(BatchNormalization())\n    \n\n    model.add(Flatten())\n    \n    model.add(Dense(64, activation = \"relu\"))\n    model.add(Dense(4, activation = \"softmax\"))\n    \n    opt = keras.optimizers.Adam(learning_rate=0.0001)\n    \n    model.compile(optimizer =\"adam\", loss = \"categorical_crossentropy\", \n                  metrics = ['accuracy',\n                             #tf.keras.metrics.AUC(),\n                             #tf.keras.metrics.Precision(),\n                             #tf.keras.metrics.PrecisionAtRecall(0.5),\n                             tf.keras.metrics.SpecificityAtSensitivity(0.5),\n                             tf.keras.metrics.SensitivityAtSpecificity(0.5)\n                             ])\n    \n    return model\n","66a0bdfa":"model = cnn_model()\nmodel.summary()\n","222c094a":"\n#set early stopping criteria\npat = 5\nn_folds=5\nepochs=100\nbatch_size=32\n\n#this is the number of epochs with no improvment after which the training will stop\nearly_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=1)\n\n#define the model checkpoint callback -> this will keep on saving the model as a physical file\nmodel_checkpoint = ModelCheckpoint('subjek1CNN.h5', verbose=1, save_best_only=True)\n\n#define a function to fit the model\ndef fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=epochs, BATCH_SIZE=batch_size):\n    model = None\n    model = cnn_model() \n    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, \n                        callbacks=[\n                                   #learning_rate_reduction,\n                                   early_stopping,\n                                   model_checkpoint], \n              verbose=1, validation_split=0.1)   \n    print(\"Val Score: \", model.evaluate(val_x, val_y))\n    \n    return results\n\n\n\n#save the model history in a list after fitting so that we can plot later\nmodel_history = [] \n\nfor i in range(n_folds):\n    print(\"Training on Fold: \",i+1)\n    t_x, val_x, t_y, val_y = train_test_split(train_X, train_y,\n                                              test_size=0.2, \n                                            random_state = np.random.randint(1,1000, 1)[0]\n                                             )\n    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n   \n    print(\"=======\"*12, end=\"\\n\\n\\n\")","690e4f60":"keras.utils.plot_model(model, show_shapes=True)","59d9d404":"\nprint(\"data train x\",t_x.shape)\nprint(\"data train y\",t_y.shape)\nprint(\"data tes x\",val_x.shape)\nprint(\"data tes y\",val_y.shape)\n\nmodel = load_model('subjek1CNN.h5')\n\na = model.evaluate(val_x, val_y)\n","8866a20c":"x_train,x_test,y_train,y_test=train_test_split(train_X,train_y,test_size=0.2)\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","f6981eef":"rounded_predictions = model.predict_classes(x_test,batch_size=32, verbose=1)\n\nrounded_labels=np.argmax(y_test, axis=1)\nrounded_labels[1],rounded_predictions[1]\nprint(classification_report(rounded_labels, rounded_predictions))\n\npredictions = model.predict_classes(x_train)\ny_tn=np.argmax(y_train, axis=1)\npredictions[1]\n\n\nsign = ['kiri', 'maju','idle','kanan']\nencoder = LabelEncoder()\nencoder_y = encoder.fit_transform(labels)\ntrain_labels = to_categorical(encoder_y,num_classes=None)\n\nplt.figure(figsize = (10,10))\ncm = confusion_matrix(y_tn,predictions )\nsns.heatmap(cm,cmap= \"Blues\",xticklabels=sign, yticklabels=sign, \n            linecolor = 'black' , linewidth = 1 , annot = True, fmt='')\n\n","dcf13b79":"fig, (ax1, ax2) =  plt.subplots( ncols=2, sharex=True)\nax1.plot(model_history[0].history['accuracy'], label='Training Fold 1 accuration')\nax1.plot(model_history[1].history['accuracy'], label='Training Fold 2 accuration')\nax1.plot(model_history[2].history['accuracy'], label='Training Fold 3 accuration')\nax1.plot(model_history[3].history['accuracy'], label='Training Fold 4 accuration')\nax1.plot(model_history[4].history['accuracy'], label='Training Fold 5 accuration')\nax1.legend()\nax2.plot(model_history[1].history['loss'], label='Training Fold 1 loss')\nax2.plot(model_history[1].history['loss'], label='Training Fold 2 loss')\nax2.plot(model_history[2].history['loss'], label='Training Fold 3 loss')\nax2.plot(model_history[3].history['loss'], label='Training Fold 4 loss ')\nax2.plot(model_history[4].history['loss'], label='Training Fold 5 loss')\nax2.legend()\nplt.show()","ef83df99":"\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[0].history['accuracy'], label='Train Accuracy Fold 1', color='black')\nplt1.plot(model_history[0].history['val_accuracy'], label='Val Accuracy Fold 1', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[0].history['loss'], label='Train Loss Fold 1', color='black')\nplt2.plot(model_history[0].history['val_loss'], label='Val Loss Fold 1', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[1].history['accuracy'], label='Train Accuracy Fold 2', color='red')\nplt1.plot(model_history[1].history['val_accuracy'], label='Val Accuracy Fold 2', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[1].history['loss'], label='Train Loss Fold 2', color='red')\nplt2.plot(model_history[1].history['val_loss'], label='Val Loss Fold 2', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[2].history['accuracy'], label='Train Accuracy Fold 3', color='green')\nplt1.plot(model_history[2].history['val_accuracy'], label='Val Accuracy Fold 3', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[2].history['loss'], label='Train Loss Fold 3', color='green')\nplt2.plot(model_history[2].history['val_loss'], label='Val Loss Fold 3', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[3].history['accuracy'], label='Train Accuracy Fold 4', color='blue')\nplt1.plot(model_history[3].history['val_accuracy'], label='Val Accuracy Fold 4', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[3].history['loss'], label='Train Loss Fold 4', color='blue')\nplt2.plot(model_history[3].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[4].history['accuracy'], label='Train Accuracy Fold 5', color='purple')\nplt1.plot(model_history[4].history['val_accuracy'], label='Val Accuracy Fold 5', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[4].history['loss'], label='Train Loss Fold 4', color='purple')\nplt2.plot(model_history[4].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()","5199589a":"\nMODEL_NAME ='subjek1CNN.h5' \n\n#CLIP = True # if your model was trained with np.clip to clip  values\nCLIP = False\nCLIP_VAL = 8  # if above, what was the value +\/-\n\nmodel = tf.keras.models.load_model(MODEL_NAME)\n\nVALDIR = '..\/input\/gabungan\/gabungan'\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nPRED_BATCH = 32\n\n\ndef get_val_data(valdir, action, batch_size):\n\n   # argmax_dict = {0: 0, 1: 0, 2: 0,3:0}\n    argmax_dict = {2: 0, 0: 0, 1: 0,3:0}\n    raw_pred_dict = {0: 0, 1: 0, 2: 0,3:0}\n\n    action_dir = os.path.join(valdir, action)\n    for session_file in os.listdir(action_dir):\n        filepath = os.path.join(action_dir,session_file)\n        if CLIP:\n            data = np.clip(np.load(filepath), -CLIP_VAL, CLIP_VAL) \/ CLIP_VAL\n            #print(data)\n        else:\n            data = np.load(filepath) \n        preds = model.predict([data.reshape(-1, 8, 60,1)], batch_size=batch_size)\n        \n        for pred in preds:\n            argmax = np.argmax(pred)\n            argmax_dict[argmax] += 1\n            for idx,value in enumerate(pred):\n                raw_pred_dict[idx] += value\n    \n    argmax_pct_dict = {}\n    for i in argmax_dict:\n        total = 0\n        correct = argmax_dict[i]\n        for ii in argmax_dict:\n            total += argmax_dict[ii]\n        argmax_pct_dict[i] = round(correct\/total, 4)\n    return argmax_dict, raw_pred_dict, argmax_pct_dict\n\n\ndef make_conf_mat(none,left,forward, right):\n    action_dict = {\"idle\":none,\"maju\": forward, \"kiri\": left, \"kanan\": right}\n    action_conf_mat = pd.DataFrame(action_dict)\n    actions = [i for i in action_dict]\n\n    fig = plt.figure(figsize=(15, 6))\n    ax = fig.add_subplot(111)\n    ax.matshow(action_conf_mat, cmap=plt.cm.RdYlGn)\n    ax.set_xticklabels([\"\"]+actions)\n    ax.set_yticklabels([\"\"]+actions)\n\n    for idx, i in enumerate(action_dict):\n        for idx2, ii in enumerate(action_dict[i]):\n            ax.text(idx, idx2, f\"{round(float(action_dict[i][ii]),2)}\", va='center', ha='center')\n    # Rotate the tick labels and set their alignment.\n    \n    plt.title(\"Matrik data\")\n    plt.ylabel(\"Predicted Action\")\n    fig.tight_layout()\n    plt.show()\n\n\n\nforward_argmax_dict, forward_raw_pred_dict, forward_argmax_pct_dict = get_val_data(VALDIR, \"maju\", PRED_BATCH)\nnone_argmax_dict, none_raw_pred_dict, none_argmax_pct_dict = get_val_data(VALDIR, \"idle\", PRED_BATCH)\nleft_argmax_dict, left_raw_pred_dict, left_argmax_pct_dict = get_val_data(VALDIR, \"kiri\", PRED_BATCH)\nright_argmax_dict, right_raw_pred_dict, right_argmax_pct_dict = get_val_data(VALDIR, \"kanan\", PRED_BATCH)\nmake_conf_mat(none_argmax_pct_dict,forward_argmax_pct_dict,left_argmax_pct_dict,  right_argmax_pct_dict)\n\n","d0dbee15":"x_train,x_test,y_train,y_test=train_test_split(train_X,train_y,test_size=0.2)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","a9bdcff6":"from sklearn.metrics import roc_curve, auc\n\n#y_score = clf.decision_function(xval)\ny_score = model.predict(x_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(4):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    \n    \nprint(\"roc_auc:\",sum(roc_auc.values())\/4)\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nfor i in range(0,4):\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(10,10))\n    plt.plot(fpr[i], tpr[i],linewidth=3)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.05, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n    #plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % ACTIONS[i])\n    plt.title('%s -ROC curve (area = %0.2f)' % (ACTIONS[i],roc_auc[i]))\n    \n    plt.legend(loc=\"lower right\")\n    plt.show()\n    ","555f1581":"def lstm_model():\n    model = Sequential()\n    model.add(LSTM(64,input_shape = (train2_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.1))\n    model.add(BatchNormalization())\n\n    model.add(LSTM(64,input_shape = (train2_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.1))\n    model.add(BatchNormalization())\n\n    model.add(LSTM(64,input_shape = (train2_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.1))\n    model.add(BatchNormalization())\n \n    model.add(LSTM(64))\n    \n    model.add(Flatten())\n    model.add(Dense(32, activation = \"relu\"))\n    \n    #model.add(BatchNormalization())\n    model.add(Dense(4, activation='softmax'))\n   \n\n    model.compile(optimizer =\"adam\", loss = \"binary_crossentropy\", \n                  metrics = ['accuracy',tf.keras.metrics.AUC(),\n                             #tf.keras.metrics.Precision(),\n                             #tf.keras.metrics.PrecisionAtRecall(0.5),\n                             tf.keras.metrics.SpecificityAtSensitivity(0.5),\n                             tf.keras.metrics.SensitivityAtSpecificity(0.5)\n\n                             ])\n\n    return model","bf75b95f":"model = lstm_model()\nmodel.summary()","428a7ed1":"#set early stopping criteria\npat = 5\n#this is the number of epochs with no improvment after which the training will stop\nearly_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=1)\n\n#define the model checkpoint callback -> this will keep on saving the model as a physical file\nmodel_checkpoint = ModelCheckpoint('subjek1LSTM.h5', verbose=1, save_best_only=True)\n\n#define a function to fit the model\ndef fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=100, BATCH_SIZE=32):\n    model = None\n    model = lstm_model() \n    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, \n                        callbacks=[learning_rate_reduction,\n                                    early_stopping, \n                                    model_checkpoint], \n              verbose=1, validation_split=0.1)   \n    print(\"Val Score: \", model.evaluate(val_x, val_y))\n    return results\n    ","538de9a6":"n_folds=5\nepochs=100\nbatch_size=32\n\n#save the model history in a list after fitting so that we can plot later\nmodel_history = [] \nfor i in range(n_folds):\n    print(\"Training on Fold: \",i+1)\n    t_x, val_x, t_y, val_y = train_test_split(train2_X, train_y, test_size=0.2, \n                                               random_state = np.random.randint(1,1000, 1)[0])\n    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n   \n    print(\"=======\"*12, end=\"\\n\\n\\n\")\n","8d075409":"\nprint(\"data train x\",t_x.shape)\nprint(\"data train y\",t_y.shape)\n\nprint(\"data tes x\",val_x.shape)\nprint(\"data tes y\",val_y.shape)\n\nmodel = load_model('subjek1LSTM.h5')\nb = model.evaluate(val_x, val_y)\n\nprint('loss',b[0])\nprint('Accuracy',b[1]*100)\n\n\n","397d6f12":"x_train,x_test,y_train,y_test=train_test_split(train2_X,train_y,test_size=0.2)\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","c323ffa7":"rounded_predictions = model.predict_classes(x_test, batch_size=32, verbose=1)\nrounded_labels=np.argmax(y_test, axis=1)\nrounded_labels[1],rounded_predictions[1]\nprint(classification_report(rounded_labels, rounded_predictions))\n\npredictions = model.predict_classes(x_train, batch_size=32, verbose=1)\ny_tn=np.argmax(y_train, axis=1)\npredictions[1],labels[1]\n\nsign = ['kiri', 'maju','idle','kanan']\nencoder = LabelEncoder()\nencoder_y = encoder.fit_transform(labels)\ntrain_labels = to_categorical(encoder_y,num_classes=None)\n\nplt.figure(figsize = (10,10))\ncm = confusion_matrix(y_tn,predictions )\nsns.heatmap(cm,cmap= \"Blues\",xticklabels=sign, yticklabels=sign, \n            linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","e9e216f9":"fig, (ax1, ax2) =  plt.subplots( ncols=2, sharex=True)\nax1.plot(model_history[0].history['accuracy'], label='Training Fold 1 accuration')\nax1.plot(model_history[1].history['accuracy'], label='Training Fold 2 accuration')\nax1.plot(model_history[2].history['accuracy'], label='Training Fold 3 accuration')\nax1.plot(model_history[3].history['accuracy'], label='Training Fold 4 accuration')\nax1.plot(model_history[4].history['accuracy'], label='Training Fold 5 accuration')\nax1.legend()\nax2.plot(model_history[1].history['loss'], label='Training Fold 1 loss')\nax2.plot(model_history[1].history['loss'], label='Training Fold 2 loss')\nax2.plot(model_history[2].history['loss'], label='Training Fold 3 loss')\nax2.plot(model_history[3].history['loss'], label='Training Fold 4 loss ')\nax2.plot(model_history[4].history['loss'], label='Training Fold 5 loss')\nax2.legend()\nplt.show()","2b003399":"\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[0].history['accuracy'], label='Train Accuracy Fold 1', color='black')\nplt1.plot(model_history[0].history['val_accuracy'], label='Val Accuracy Fold 1', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[0].history['loss'], label='Train Loss Fold 1', color='black')\nplt2.plot(model_history[0].history['val_loss'], label='Val Loss Fold 1', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[1].history['accuracy'], label='Train Accuracy Fold 2', color='red')\nplt1.plot(model_history[1].history['val_accuracy'], label='Val Accuracy Fold 2', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[1].history['loss'], label='Train Loss Fold 2', color='red')\nplt2.plot(model_history[1].history['val_loss'], label='Val Loss Fold 2', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[2].history['accuracy'], label='Train Accuracy Fold 3', color='green')\nplt1.plot(model_history[2].history['val_accuracy'], label='Val Accuracy Fold 3', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[2].history['loss'], label='Train Loss Fold 3', color='green')\nplt2.plot(model_history[2].history['val_loss'], label='Val Loss Fold 3', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[3].history['accuracy'], label='Train Accuracy Fold 4', color='blue')\nplt1.plot(model_history[3].history['val_accuracy'], label='Val Accuracy Fold 4', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[3].history['loss'], label='Train Loss Fold 4', color='blue')\nplt2.plot(model_history[3].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[4].history['accuracy'], label='Train Accuracy Fold 5', color='purple')\nplt1.plot(model_history[4].history['val_accuracy'], label='Val Accuracy Fold 5', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[4].history['loss'], label='Train Loss Fold 4', color='purple')\nplt2.plot(model_history[4].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()","36b7af7d":"\nMODEL_NAME ='subjek1LSTM.h5' \n\n#CLIP = True # if your model was trained with np.clip to clip  values\nCLIP = False\nCLIP_VAL = 8  # if above, what was the value +\/-\n\nmodel = tf.keras.models.load_model(MODEL_NAME)\n\nVALDIR = '..\/input\/gabungan\/gabungan'\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nPRED_BATCH = 32\n\n\ndef get_val_data(valdir, action, batch_size):\n\n   # argmax_dict = {0: 0, 1: 0, 2: 0,3:0}\n    argmax_dict = {2: 0, 0: 0, 1: 0,3:0}\n    raw_pred_dict = {0: 0, 1: 0, 2: 0,3:0}\n\n    action_dir = os.path.join(valdir, action)\n    for session_file in os.listdir(action_dir):\n        filepath = os.path.join(action_dir,session_file)\n        if CLIP:\n            data = np.clip(np.load(filepath), -CLIP_VAL, CLIP_VAL) \/ CLIP_VAL\n            #print(data)\n        else:\n            data = np.load(filepath) \n        preds = model.predict([data.reshape(-1, 8, 60)], batch_size=batch_size)\n        \n        for pred in preds:\n            argmax = np.argmax(pred)\n            argmax_dict[argmax] += 1\n            for idx,value in enumerate(pred):\n                raw_pred_dict[idx] += value\n    \n    argmax_pct_dict = {}\n    for i in argmax_dict:\n        total = 0\n        correct = argmax_dict[i]\n        for ii in argmax_dict:\n            total += argmax_dict[ii]\n        argmax_pct_dict[i] = round(correct\/total, 4)\n    return argmax_dict, raw_pred_dict, argmax_pct_dict\n\n\ndef make_conf_mat(none,left,forward, right):\n    action_dict = {\"idle\":none,\"maju\": forward, \"kiri\": left, \"kanan\": right}\n    action_conf_mat = pd.DataFrame(action_dict)\n    actions = [i for i in action_dict]\n\n    fig = plt.figure(figsize=(15, 6))\n    ax = fig.add_subplot(111)\n    ax.matshow(action_conf_mat, cmap=plt.cm.RdYlGn)\n    ax.set_xticklabels([\"\"]+actions)\n    ax.set_yticklabels([\"\"]+actions)\n\n    for idx, i in enumerate(action_dict):\n        for idx2, ii in enumerate(action_dict[i]):\n            ax.text(idx, idx2, f\"{round(float(action_dict[i][ii]),2)}\", va='center', ha='center')\n    # Rotate the tick labels and set their alignment.\n    \n    plt.title(\"Matrik data\")\n    plt.ylabel(\"Predicted Action\")\n    fig.tight_layout()\n    plt.show()\n\n\n\nforward_argmax_dict, forward_raw_pred_dict, forward_argmax_pct_dict = get_val_data(VALDIR, \"maju\", PRED_BATCH)\nnone_argmax_dict, none_raw_pred_dict, none_argmax_pct_dict = get_val_data(VALDIR, \"idle\", PRED_BATCH)\nleft_argmax_dict, left_raw_pred_dict, left_argmax_pct_dict = get_val_data(VALDIR, \"kiri\", PRED_BATCH)\nright_argmax_dict, right_raw_pred_dict, right_argmax_pct_dict = get_val_data(VALDIR, \"kanan\", PRED_BATCH)\nmake_conf_mat(none_argmax_pct_dict,forward_argmax_pct_dict,left_argmax_pct_dict,  right_argmax_pct_dict)\n\n","f5873d22":"x_train,x_test,y_train,y_test=train_test_split(train2_X,train_y,test_size=0.2)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","6d9adb6d":"from sklearn.metrics import roc_curve, auc\n\n#y_score = clf.decision_function(xval)\ny_score = model.predict(x_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(4):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    \n    \nprint(\"roc_auc:\",sum(roc_auc.values())\/4)\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nfor i in range(0,4):\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(10,10))\n    plt.plot(fpr[i], tpr[i],linewidth=3)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.05, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n    plt.title('%s -ROC curve (area = %0.2f)' % (ACTIONS[i],roc_auc[i]))\n    \n    plt.legend(loc=\"lower right\")\n    plt.show()\n    ","30d75980":"# CNN","0d8b9a51":"# DATA","86265da5":"# LSTM"}}