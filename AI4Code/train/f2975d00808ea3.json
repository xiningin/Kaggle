{"cell_type":{"a9de5921":"code","995d0a0f":"code","f258ed05":"code","266857aa":"code","8fbfc496":"code","8f28315e":"code","d183ba4e":"code","72a0c92b":"code","a7f4f5f6":"code","0ca70da0":"code","8b3974f9":"code","82a97e5e":"code","f7c09232":"markdown","9d0a51d7":"markdown","37395d95":"markdown","fee74abf":"markdown","804f9e0f":"markdown","6dbd4d45":"markdown","fa7e9795":"markdown","9d292015":"markdown"},"source":{"a9de5921":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","995d0a0f":"data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\",encoding=\"latin1\")\ndata=pd.concat([data.text,data.target],axis=1)\ndata.dropna(axis=0,inplace=True)","f258ed05":"CleanData=[]\ndef prepare(data,out):\n    for descr in data.text:\n        descr=re.sub(\"[^a-zA-Z]\",\" \",descr) #harf olMAyanlar\u0131 bul\n        descr=descr.lower()\n        descr = nltk.word_tokenize(descr)\n        descr = [word for word in descr if not word in set(stopwords.words(\"english\"))] \n        lemma = nltk.WordNetLemmatizer()\n        descr = [lemma.lemmatize(word) for word in descr]\n        descr = \" \".join(descr)\n        out.append(descr)\n    \nprepare(data,CleanData)","266857aa":"max_features= 300\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words=\"english\")\nsparce_matrix=count_vectorizer.fit_transform(CleanData).toarray()\nprint(\"Most used {} word: {}\".format(max_features,count_vectorizer.get_feature_names()))\n#%%\ny= data.iloc[:,1].values # Disaster or not\nx=sparce_matrix #Texts for training\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.01,random_state=42)","8fbfc496":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)","8f28315e":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state=42,max_iter=100, C=10)\nlogreg.fit(x_train,y_train.T)","d183ba4e":"#Used callback function to save best weights\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers.core import Dropout\nfrom keras.callbacks import ModelCheckpoint\ncheckpoint = ModelCheckpoint(\"best.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacklists=[checkpoint]\nmodel = Sequential()\nmodel.add(Dense(input_dim=x_train.shape[1],\n                output_dim = 1,\n                init =   'uniform',\n                activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, kernel_initializer='uniform'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, kernel_initializer='uniform'))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1, kernel_initializer='uniform'))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"acc\"])\nmodel.fit(x_train,\n          y_train,\n          epochs = 35,\n          batch_size = 50,\n          validation_data = (x_test,y_test),\n          callbacks=callbacklists,\n          verbose=1)\nmodel.load_weights(\"best.hdf5\")","72a0c92b":"print(model.summary())","a7f4f5f6":"print(\"Test accuracy of naive bayes: \",nb.score(x_test,y_test))\nprint(\"Test accuracy of Logistic Regression:  \",logreg.score(x_test,y_test.T))\nprint(\"Test loss and accuracy of Neural Network: \",model.evaluate(x_test,y_test))","0ca70da0":"#%%Small Visualization\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ny_axis=[nb.score(x_test,y_test),logreg.score(x_test,y_test.T),model.evaluate(x_test,y_test)[1]]\nx_axis=[\"Naive Bayes\",\"Logistic Regression\",\"Neural Network\"]\n\nfig,ax=plt.subplots(figsize=(10,6))\nax.bar(x_axis,y_axis)\nplt.show()\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (5,5), fontsize=15):\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\",cmap='Blues')\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return fig\n\nresult_Float=model.predict(x_test)\nresult_bin=[]\nfor num in result_Float:\n    result_bin.append(int(num>=0.5))\n\ncf_matrix=confusion_matrix(y_test,result_bin)\nprint_confusion_matrix(cf_matrix,['1','2'])","8b3974f9":"sub=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nx_lastTest=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nCleanTestData=[]\nprepare(x_lastTest,CleanTestData)\nx_lastTest=count_vectorizer.transform(CleanTestData).toarray()","82a97e5e":"sub[\"target\"]=nb.predict(x_lastTest)\nsub.set_index([\"id\"],inplace=True)\nsub.to_csv(\"cevap.csv\")","f7c09232":"**Using Logistic Regeression Classification**","9d0a51d7":"**Preparing data with lemmatizing, tokenizing and lowering words.**","37395d95":"**Printing and Visualizing Results and Comparing Classification Models**","fee74abf":"**Vectorizing and Spliting data**","804f9e0f":"**Using Neural Network for Classification**","6dbd4d45":"**IMPORTING DATA**","fa7e9795":"**Applying for submission**","9d292015":"**Using Naive Bayes Classification**"}}