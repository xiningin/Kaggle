{"cell_type":{"7c34c50f":"code","a09738fa":"code","4e6b289e":"code","a42bc09d":"code","55d5516f":"code","116d40f4":"code","fd7d9573":"code","3ff9cab3":"code","615157d0":"code","2f53e9ad":"code","6b676e19":"code","dbb724b1":"code","7d26bf8a":"code","f6f72fcb":"markdown","c13238b9":"markdown","7ce8d8bf":"markdown"},"source":{"7c34c50f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt; plt.rcdefaults()\nimport tensorflow as tf\n\n# Feature Engineering\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\n# Restrating the tensor graph\ndef reset_graph(seed=42):\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)    \nreset_graph()\n\ntrain_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")\ntrain_data.head()","a09738fa":"X_train, X_test, y_train, y_test = train_test_split(train_data, train_data[\"Survived\"], test_size=0.20, random_state=42)","4e6b289e":"# I created one preprocessing pipelines for processing both numeric and categorical data.\nnumeric_features = ['Age','Fare', 'SibSp', 'Parch']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()) #Please note all the numerical values are scaled using StandardScaler().\n])\n\ncategorical_features = ['Sex', 'Pclass', 'Embarked']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='S')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])","a42bc09d":"X_train_prepared = preprocessor.fit_transform(X_train)\n\nX_test_prepared = preprocessor.fit_transform(X_test)","55d5516f":"feature_columns = [tf.feature_column.numeric_column(\"X\", shape=(X_train_prepared.shape[1],1))]","116d40f4":"dnn_clf = tf.estimator.DNNClassifier(\n    feature_columns=feature_columns,\n    hidden_units=[10, 9, 10, 10, 10],\n    n_classes=2,\n    activation_fn=tf.nn.elu,\n    batch_norm=False,\n    dropout=0.5,\n    optimizer=tf.train.ProximalGradientDescentOptimizer(\n        learning_rate=0.01,\n        l1_regularization_strength=0.1,\n        l2_regularization_strength=0.1),                                      \n#     optimizer=tf.train.MomentumOptimizer(\n#       learning_rate=0.1,\n#       momentum=0.2,\n#       use_nesterov=True)\n)","fd7d9573":"train_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"X\": np.array(X_train_prepared)},\n    y=np.array(y_train.values.reshape((len(y_train),1))),\n    num_epochs=100, \n    shuffle=True\n)\ndnn_clf.train(input_fn=train_input_fn)","3ff9cab3":"test_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"X\": np.array(X_test_prepared)},\n    y=np.array(y_test.values.reshape((len(y_test),1))),\n    num_epochs=1, #We just want to use one epoch since this is only to score.\n    shuffle=False  #It isn't necessary to shuffle the cross validation \n)","615157d0":"# Evaluate accuracy.\naccuracy_score = dnn_clf.evaluate(input_fn=test_input_fn)\nprint(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score['accuracy']))","2f53e9ad":"test_set_prepared = preprocessor.fit_transform(test_data)","6b676e19":"prediction_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"X\": np.array(test_set_prepared)},\n    y=None,\n    num_epochs=1, #We just want to use one epoch since this is only to score.\n    shuffle=False  #It isn't necessary to shuffle the cross validation \n)\npred = dnn_clf.predict(input_fn=prediction_input_fn)","dbb724b1":"predictions = np.array([])\npred_list = list(pred)\nfor p in pred_list:\n    predictions = np.append(predictions,p['class_ids'][0])\n#cast from string to integer\npredictions = predictions.astype(int)","7d26bf8a":"result = pd.DataFrame(columns=[\"PassengerId\", \"Survived\"])\nresult[\"PassengerId\"] = test_data['PassengerId']\nresult[\"Survived\"] = predictions\nresult.to_csv(\"Submission-tf1.csv\", index=False)","f6f72fcb":"The test data does not contain survival labels: your goal is to train the best model you can using the training data, then make your predictions on the test data and upload them to Kaggle to see your final score. This code gives you slightly more than 80% accuracy.\n\nWe use the validation set to test out the performance of the training models. The simplest way to create a validation set is to use train_test_split function and specify the percentage of data that you would want to allocate for validation. This will splits the training data into two sets of training and validation. For example, the code below creates a validation set with 20% of the data.","c13238b9":"Maryam Ashoori \n\n_Requirements: Scikit-learn 0.20_\n\nThis notebook provides an easy implementation of a deep neural net for survival prediction","7ce8d8bf":"# Deep Neural Net"}}