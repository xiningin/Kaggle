{"cell_type":{"39b001fb":"code","482113f7":"code","5c920f78":"code","358f4b1f":"code","e4bdd7f4":"code","621b2e1c":"code","db4a39ae":"code","04b8e5c2":"code","7a5e8030":"code","902e3d62":"code","b6f53f90":"code","764dc1d0":"code","d6098909":"code","80b76de5":"code","b6d0d424":"code","24dac320":"code","d9e4ca5b":"code","86ca511b":"code","591cd8b8":"code","e64bea6f":"code","0ccc7320":"code","7f09f9a8":"code","e14f291a":"code","c5442216":"code","cc79f63c":"markdown","76390e45":"markdown","6e4807af":"markdown","45160f39":"markdown","db0d60c2":"markdown","457918bc":"markdown","a012b865":"markdown","4f9f0b41":"markdown"},"source":{"39b001fb":"## Most Important\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom pathlib import Path\nfrom PIL import Image\n\n## less Important\nfrom functools import partial\nimport os\nfrom scipy import stats\nimport missingno as msno\nimport joblib\nimport tarfile\nimport shutil\nimport urllib\n\n## Sklearn\nfrom sklearn import datasets\n## Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n## Metrics\nfrom sklearn.metrics import accuracy_score\n\n## tensorflow & Keras\nimport tensorflow as tf    ## i will use tf for every thing and for keras using tf.keras","482113f7":"train_labels = pd.read_csv('..\/input\/arabic-hwr-ai-pro-intake1\/train.csv')\ntrain_images = Path(r'..\/input\/arabic-hwr-ai-pro-intake1\/train')\n\n## read these all training images paths as Series\ntrain_images_paths = pd.Series(sorted(list(train_images.glob(r'*.png'))), name='Filepath').astype(str)\n\ntrain_images_paths.head()","5c920f78":"train_labels['label'].unique()","358f4b1f":"train_labels['label'] = train_labels['label'] - 1\ntrain_labels['label'].unique()","e4bdd7f4":"img_key_value = {}\nfor value in train_labels['label'].unique():\n    img_key_value[value] = train_labels[train_labels['label']==value].index[0]\n    \nimg_index = list(img_key_value.values())\nimg_label = list(img_key_value.keys())\n\nfig, ax = plt.subplots(4, 7, figsize=(12, 8))\n\ni = 0\nfor row in range(4):\n    for col in range(7):\n        plt.sca(ax[row, col])\n        plt.title(f'label = {img_label[i]}')\n        img = plt.imread(train_images_paths.iloc[img_index[i]])\n        plt.imshow(img)\n        plt.axis('off')\n        i+=1","621b2e1c":"np.asarray(plt.imread(train_images_paths.iloc[0])).shape\n\nfrom PIL import Image\nimg = Image.open(train_images_paths.iloc[0]).convert('L')\nnp.asarray(img)\/255","db4a39ae":"print('Number of Instances in train_set =>', len(train_images_paths))\nprint('Number of Instances in train_labels =>', len(train_labels))\n\nprint()\n\nimg = plt.imread(train_images_paths.iloc[img_index[20]])\nprint('shape of each Image is =>', img.shape)\n\nimg","04b8e5c2":"train_full_labels = train_labels['label'].values\ntrain_full_set = np.empty((13440, 64, 64), dtype=np.float32)\nnewsize = (64, 64)\n\nfor idx, path in enumerate(train_images_paths):\n    img = Image.open(path).convert('L')\n    img = img.resize(newsize)\n    img = np.asarray(img)\/255\n    train_full_set[idx] = img\n    \ntrain_full_set = train_full_set.reshape(train_full_set.shape[0], train_full_set.shape[1],\\\n                                        train_full_set.shape[2], 1)\nprint('train_full_set.shape =>', train_full_set.shape)\nprint('train_full_labels.shape =>', train_full_labels.shape)","7a5e8030":"X_train, X_valid, y_train, y_valid = train_test_split(train_full_set, train_full_labels,\\\n                                        test_size=0.1, stratify=train_full_labels, random_state=42)\n\nprint('X_train.shape =>', X_train.shape)\nprint('X_valid.shape =>', X_valid.shape)\nprint('y_train.shape =>', y_train.shape)\nprint('y_valid.shape =>', y_valid.shape)","902e3d62":"y_valid[y_valid == 1].shape","b6f53f90":"np.random.seed(10)","764dc1d0":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Dropout, Dense\n\ndef create_model(optimizer='adam', kernel_initializer='he_normal', activation='relu'):\n  # create model\n  model = Sequential()\n  model.add(Conv2D(filters=16, kernel_size=3, padding='same', input_shape=(64, 64, 1), kernel_initializer=kernel_initializer, activation=activation))\n  model.add(BatchNormalization())\n  model.add(MaxPooling2D(pool_size=2))\n  model.add(Dropout(0.1))\n\n  model.add(Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n  model.add(BatchNormalization())\n  model.add(MaxPooling2D(pool_size=2))\n  model.add(Dropout(0.1))\n\n  model.add(Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n  model.add(BatchNormalization())\n  model.add(MaxPooling2D(pool_size=2))\n  model.add(Dropout(0.1))\n\n  model.add(Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n  model.add(BatchNormalization())\n  model.add(MaxPooling2D(pool_size=2))\n  model.add(Dropout(0.1))\n  model.add(GlobalAveragePooling2D())\n  \n  #Fully connected final layer\n  model.add(Dense(28, activation='softmax'))\n    \n  # Compile model\n  model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'], optimizer=optimizer)\n  return model","d6098909":"model = create_model(optimizer='Adam', kernel_initializer='uniform', activation='relu')\nmodel.summary()","80b76de5":"from keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# using checkpoints to save model weights to be used later instead of training again on the same epochs.\ncheckpointer = ModelCheckpoint(filepath='weights6.hdf5', verbose=1, save_best_only=True)\nearly_stopp = EarlyStopping(patience=10, restore_best_weights=True)\nhistory = model.fit(X_train, y_train, \n                    validation_data=(X_valid, y_valid),\n                    epochs=100, batch_size=20, verbose=1)\n# history = model.fit(X_train, y_train, \n#                     validation_data=(X_valid, y_valid),\n#                     epochs=100, batch_size=20, verbose=1, callbacks=[checkpointer, early_stopp])","b6d0d424":"pd.DataFrame(history.history).plot(figsize=(10, 6));","24dac320":"loss_all_data, acc_all_data = model.evaluate(train_full_set, train_full_labels, verbose=0)\nprint('loss_all_data =>', loss_all_data)\nprint('acc_all_data =>', acc_all_data)","d9e4ca5b":"test_labels = pd.read_csv('..\/input\/arabic-hwr-ai-pro-intake1\/test.csv')\ntest_images = Path(r'..\/input\/arabic-hwr-ai-pro-intake1\/test')\n\n## read these all training images paths as Series\ntest_images_paths = pd.Series(sorted(list(test_images.glob(r'*.png'))), name='Filepath').astype(str)\n\ntest_images_paths.head()","86ca511b":"print('Number of Instances in test_set is', len(test_images_paths))","591cd8b8":"test_full_set = np.empty((3360, 64, 64), dtype=np.float32)\nnewsize = (64, 64)\n\nfor idx, path in enumerate(test_images_paths):\n    img = Image.open(path).convert('L')\n    img = img.resize(newsize)\n    img = np.asarray(img)\/255\n    test_full_set[idx] = img\n    \ntest_full_set = test_full_set.reshape(test_full_set.shape[0], test_full_set.shape[1],\\\n                                        test_full_set.shape[2], 1)\nprint('test_full_set.shape =>', test_full_set.shape)","e64bea6f":"y_preds_classes = np.argmax(model.predict(test_full_set) , axis=-1)","0ccc7320":"y_preds_classes = y_preds_classes + 1\ny_preds_classes.shape","7f09f9a8":"test_labels['label'] = y_preds_classes","e14f291a":"test_labels['label'].value_counts().plot(kind='bar')","c5442216":"test_labels[['id', 'label']].to_csv('\/kaggle\/working\/submission.csv', index=False)","cc79f63c":"## Explore the Data","76390e45":"## Split the Data","6e4807af":"## Evaluation on Testing DataSet","45160f39":"## Done :D","db0d60c2":"## Model Training","457918bc":"## Data Preprocessing","a012b865":"## Loading the Data and Look at the Big Picture","4f9f0b41":"`Only for training here`"}}