{"cell_type":{"6301ea88":"code","4367cdac":"code","63d64792":"code","2631d6e7":"code","9c9d76c2":"code","ab706fe5":"code","77c8ca4f":"code","3d27650d":"code","8454060a":"code","e8090b22":"code","7ae4bdcb":"code","eca3caa3":"code","456050bb":"code","22dd2c65":"code","bcf07efc":"code","6622e395":"code","24e2d146":"code","64cbe7c7":"code","56702fc0":"code","45c0bdca":"code","c824cc04":"code","c822aee8":"code","5c0d64e8":"code","eb082261":"code","45b0c188":"markdown","898add53":"markdown","a4e35a37":"markdown","e65b51ea":"markdown","0ee24e0f":"markdown"},"source":{"6301ea88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nimport IPython #pretty printing of dataframes in Jupyter notebook\nfrom IPython import display\nimport sklearn #collection of machine learning algorithms\n\n#misc libraries\nimport random\nimport time\n\n#ignore warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4367cdac":"#Load Modelling Algorithms\n#We will use the popular scikit-learn library to develop our machine learning algorithms. \n#In sklearn, algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the matplotlib and seaborn library. Below are common classes to load.\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#common model helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n\n#visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n\n#visualization defaults\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12, 8\n\n#lets first import our data\ndata_raw = pd.read_csv('..\/input\/train.csv')\ndata_val  = pd.read_csv('..\/input\/test.csv')\n\n#Make a deep copy, including a copy of the data and the indices. With deep=False neither the indices nor the data are copied.\ndata1 = data_raw.copy(deep= True)\n#in order to clean both datasets at once\ndata_cleaner = [data1, data_val]\n\n#preview data\n\ndata_raw.info()\n","63d64792":"data_raw.sample(10)","2631d6e7":"\nprint('Train columns with null values:\\n',data1.isnull().sum())\nprint('*'*20)\nprint('Test columns with null values:\\n', data_val.isnull().sum())","9c9d76c2":"data1.describe(include='all')","ab706fe5":"#COMPLETE or delete missing values in train and test\/validation dataset\nfor dataset in data_cleaner:\n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace= True)\n    #complete missing Embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\n    \n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n#delete the PassengerId, Cabin and Ticket feature to exclude in train dataset\ndrop_column = ['PassengerId', 'Cabin', 'Ticket']\ndata1.drop(drop_column, axis=1, inplace=True)\n    \nprint('Train columns dropped')\n\n    \n    ","77c8ca4f":"\n\nprint('Train columns with null values:\\n',data1.isnull().sum())\nprint('*'*20)\nprint('Test columns with null values:\\n', data_val.isnull().sum())","3d27650d":"drop_column = ['Cabin', 'Ticket']\ndata_val.drop(drop_column, axis = 1, inplace=True)\nprint('Test columns with null values:\\n', data_val.isnull().sum())","8454060a":"###CREATE: Feature Engineering for train and test\/validation dataset\nfor dataset in data_cleaner:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1\n    #If other family members are present IsAlone will be 0\n    dataset['IsAlone'].loc[dataset['FamilySize']>1] = 0\n    #extracting title from data\n    dataset['Title'] = dataset['Name'].str.split(', ', expand = True)[1].str.split(\".\", expand = True)[0]\n     \n    #With qcut, the bins will be chosen so that you have the same number of records in each bin \n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    \n    #cut will choose the bins to be evenly spaced according to the values themselves and not the frequency of those values\n\n    \n    dataset['AgeBin'] = pd.cut(dataset['Age'], 5)\nprint('Feature Engineering done for FareBin and AgeBin')","e8090b22":"stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\ntitle_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\ndata1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())\nprint(\"-\"*10)\n\n\n#preview data again\ndata1.info()\ndata_val.info()\ndata1.sample(10)","7ae4bdcb":"#CONVERT: convert objects to category using Label Encoder for train and test\/validation dataset\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n#define y variable aka target\/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name\/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w\/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\ndata1_dummy.head()","eca3caa3":"print('Train columns with null values:\\n',data1.isnull().sum())\nprint('*'*20)\nprint('Test columns with null values:\\n', data_val.isnull().sum())","456050bb":"data_raw.describe(include = 'all')","22dd2c65":"train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\n\ntrain1_x_bin.head()","bcf07efc":"for x in data1_x:\n    if data1[x].dtype != 'float64':\n        print('Survival correlation by ', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n\n        \n        ","6622e395":"print(pd.crosstab(data1['Title'], data1[Target[0]]))\n","24e2d146":"#graph distribution of quantitative data\nplt.figure(figsize=[16, 12])\nplt.subplot(231)\nplt.boxplot(x = data1['Fare'], showmeans = True, meanline = True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(232)\nplt.boxplot(x = data1['Age'], showmeans = True, meanline = True)\nplt.title('Age Boxplot')\nplt.ylabel('Age (years)')\n\nplt.subplot(233)\nplt.boxplot(x = data1['FamilySize'], showmeans = True, meanline = True)\nplt.title('Familysize')\nplt.ylabel('Familysize (Nos)')\n\nplt.subplot(234)\nplt.hist(x=[data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']],stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Fare histogram by survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x=[data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], stacked = True,color= ['g','b'], label = ['Survived', 'Dead'])\nplt.title('Age histogram by Survival')\nplt.xlabel('Age (years)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist(x=[data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], stacked = True, color = ['r','g'], label = ['Survived', 'Dead'], bins = 10)\nplt.title('Family size histogram by survival')\nplt.xlabel('Family Size')\nplt.ylabel('# of Passengers')\nplt.legend()\n","64cbe7c7":"fig, saxis = plt.subplots(2, 3, figsize = (16, 12))\nsns.barplot(x= 'Embarked', y='Survived', data= data1, ax= saxis[0,0])\nsns.barplot(x='Pclass', y='Survived', data=data1, ax=saxis[0,1])\nsns.barplot(x='IsAlone', y='Survived', data= data1, ax= saxis[0,2])\n\nsns.pointplot(x='FareBin', y='Survived', data=data1, ax= saxis[1,0])\nsns.pointplot(x='AgeBin', y='Survived', data=data1, ax= saxis[1,1])\nsns.pointplot(x='FamilySize', y='Survived', data=data1, ax= saxis[1,2])\n\n\n\n","56702fc0":"#we know sex mattered in survival, now let's compare sex and Embarked\n\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax = qaxis[1])\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data1, ax = qaxis[2])\n            \n\n","45c0bdca":"fig, qaxis = plt.subplots(1, 3, figsize = (14,12))\nsns.distplot(data1[\"Age\"],kde=True, ax =qaxis[0]) #without the kde\nsns.distplot(data1['Fare'], kde = True, ax=qaxis[1])\nsns.distplot(data1['FamilySize'], kde = True, ax=qaxis[2])\n","c824cc04":"def correlation_heatmap(df):\n    _, ax = plt.subplots(figsize=(16, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    _ = sns.heatmap(df.corr(), annot= True, cmap=colormap)\n    \n        \ncorrelation_heatmap(data1)","c822aee8":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest neighbour\n    neighbors.KNeighborsClassifier(),\n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability= True),\n    svm.LinearSVC(),\n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    XGBClassifier()\n\n]\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data1[Target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(data1[data1_x_bin], data1[Target])\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index+=1\n\n    \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","5c0d64e8":"sns.barplot(x= 'MLA Test Accuracy Mean', y='MLA Name', data = MLA_compare, color='m')\nplt.title('MLA Accuracy score')\nplt.xlabel('Accuracy score %')\nplt.ylabel('Algorithm name')","eb082261":"\nclf = svm.SVC()\nclf.fit(data1[data1_x_bin], data1[Target])\npred= clf.predict(data_val[data1_x_bin])\nsubmission = pd.DataFrame({\"PassengerId\": data_val['PassengerId'], \"Survived\": pred})\n#data_val.columns\nsubmission.to_csv(\"..\/working\/submission.csv\", index=False)\n\nsubmission.sample(10)","45b0c188":"**Perform Exploratory Analysis with Statistics**\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.","898add53":"Convert Formats\nWe will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.\n\nIn this step, we will also define our x (independent\/features\/explanatory\/predictor\/etc.) and y (dependent\/target\/outcome\/response\/etc.) variables for data modeling.","a4e35a37":"**Model Data**\nData Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don\u2019t worry, we only need a high-level overview, which we\u2019ll cover in this Kernel. Also, thanks to computer science, a lot of the heavy lifting is done for you. So, problems that once required graduate degrees in mathematics or statistics, now only take a few lines of code. Last, we\u2019ll need some business acumen to think through the problem. After all, like training a sight-seeing dog, it\u2019s learning from us and not the other way around.\n\nMachine Learning (ML), as the name suggest, is teaching the machine how-to think and not what to think. While this topic and big data has been around for decades, it is becoming more popular than ever because the barrier to entry is lower, for businesses and professionals alike. This is both good and bad. It\u2019s good because these algorithms are now accessible to more people that can solve more problems in the real-world. It\u2019s bad because a lower barrier to entry means, more people will not know the tools they are using and can come to incorrect conclusions. That\u2019s why I focus on teaching you, not just what to do, but why you\u2019re doing it. Previously, I used the analogy of asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible; or even worst, implements incorrect actionable intelligence. So now that I\u2019ve hammered (no pun intended) my point, I\u2019ll show you what to do and most importantly, WHY you do it.\n\nFirst, you must understand, that the purpose of machine learning is to solve human problems. Machine learning can be categorized as: supervised learning, unsupervised learning, and reinforced learning. Supervised learning is where you train the model by presenting it a training dataset that includes the correct answer. Unsupervised learning is where you train the model using a training dataset that does not include the correct answer. And reinforced learning is a hybrid of the previous two, where the model is not given the correct answer immediately, but later after a sequence of events to reinforce learning. We are doing supervised machine learning, because we are training our algorithm by presenting it with a set of features and their corresponding target. We then hope to present it a new subset from the same dataset and have similar results in prediction accuracy.\n\nThere are many machine learning algorithms, however they can be reduced to four categories: classification, regression, clustering, or dimensionality reduction, depending on your target variable and data modeling goals. We'll save clustering and dimension reduction for another day, and focus on classification and regression. We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. One side note, logistic regression, while it has regression in the name, is really a classification algorithm. Since our problem is predicting if a passenger survived or did not survive, this is a discrete target variable. We will use a classification algorithm from the sklearn library to begin our analysis. We will use cross validation and scoring metrics, discussed in later sections, to rank and compare our algorithms\u2019 performance.\n\nMachine Learning Selection:\n\nSklearn Estimator Overview\nSklearn Estimator Detail\nChoosing Estimator Mind Map\nChoosing Estimator Cheat Sheet\nNow that we identified our solution as a supervised learning classification algorithm. We can narrow our list of choices.\n\nMachine Learning Classification Algorithms:\n\nEnsemble Methods\nGeneralized Linear Models (GLM)\nNaive Bayes\nNearest Neighbors\nSupport Vector Machines (SVM)\nDecision Trees\nDiscriminant Analysis\nData Science 101: How to Choose a Machine Learning Algorithm (MLA)\nIMPORTANT: When it comes to data modeling, the beginner\u2019s question is always, \"what is the best machine learning algorithm?\" To this the beginner must learn, the No Free Lunch Theorem (NFLT) of Machine Learning. In short, NFLT states, there is no super algorithm, that works best in all situations, for all datasets. So the best approach is to try multiple MLAs, tune them, and compare them for your specific scenario. With that being said, some good research has been done to compare algorithms, such as Caruana & Niculescu-Mizil 2006 watch video lecture here of MLA comparisons, Ogutu et al. 2011 done by the NIH for genomic selection, Fernandez-Delgado et al. 2014 comparing 179 classifiers from 17 families, Thoma 2016 sklearn comparison, and there is also a school of thought that says, more data beats a better algorithm.\n\nSo with all this information, where is a beginner to start? I recommend starting with Trees, Bagging, Random Forests, and Boosting. They are basically different implementations of a decision tree, which is the easiest concept to learn and understand. They are also easier to tune, discussed in the next section, than something like SVC. Below, I'll give an overview of how-to run and compare several MLAs, but the rest of this Kernel will focus on learning data modeling via decision trees and its derivatives.\n","e65b51ea":"The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by \n1) correcting aberrant values and outliers, \n2) completing missing information, \n3) creating new features for analysis, \n4) converting fields to the correct format for calculations and presentation.\n\n1. Correcting: Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare. However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, for example age = 800 instead of 80, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n\n2. Completing: There are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values. Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. For this dataset, age will be imputed with the median, the cabin attribute will be dropped, and embark will be imputed with mode. Subsequent model iterations may modify this decision to determine if it improves the model\u2019s accuracy.\n\n3.Creating: Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.\n\n4.Converting: Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations. For this dataset, we will convert object datatypes to categorical dummy variables.\n\n","0ee24e0f":"1. The Survived variable is our outcome or dependent variable. It is a binary nominal datatype of 1 for survived and 0 for did not survive. All other variables are independent variables.\n2.The PassengerID and Ticket variables are assumed to be random unique identifiers, that have no impact on the outcome variable. Thus, they will be excluded from analysis.\n3. The Pclass variable is an ordinal datatype for the ticket class, a proxy for socio-economic status (SES), with 1 = upper class, 2 = middle class, and 3 = lower class.\n4. The Name variable is a nominal datatype. \nIt could be used in feature engineering to derive the SES from titles like doctor or master. \nSince these variables already exist, we'll make use of it to see if title, like master, makes a difference.\n5. The Sex and Embarked variables are a nominal datatype. They will be converted to dummy variables for mathematical calculations.\n6. The Age and Fare variable are continuous quantitative datatypes.\n7. The SibSp represents number of related siblings\/spouse aboard and Parch represents number of related parents\/children aboard. Both are discrete quantitative datatypes. This can be used for feature engineering to create a family size and is alone variable.\n8. The Cabin variable is a nominal datatype that have no impact on the outcome variable.Thus, they will be excluded from analysis."}}