{"cell_type":{"f79ed9ab":"code","493b6c29":"code","85be65d2":"code","6823ffbe":"code","ccba64aa":"code","f2443e88":"code","0cfa8aa4":"code","fb389dbc":"code","db0af8d0":"code","2ef7c384":"code","90f6c49f":"code","f4c500f5":"code","3b2848c8":"code","e9d5c3bf":"code","f1b3de2c":"code","f846b952":"code","a4544517":"code","a561902f":"code","048cfd52":"code","07e15a07":"code","bf061a6f":"code","22f23f5f":"code","ffa1804c":"code","d23e9e57":"code","332b0e51":"code","b250eabf":"code","41aab365":"code","b8dbf095":"code","a279ba06":"code","1d598f81":"code","e350150f":"code","b6c9b55e":"code","3745af57":"code","dbfb6c5a":"code","f426de97":"code","0adbe7a5":"code","67883dc8":"code","32d210a1":"code","b8f04ca2":"code","785898e9":"code","ee66b4aa":"code","38c8f569":"code","71425591":"code","bbf59ac0":"code","9f199da6":"code","2d8925a4":"code","4e7669ee":"code","10fd6632":"code","0f032d90":"code","1c0f63c7":"code","5628fca5":"code","a8950e2e":"code","1c4eb581":"code","7667b5d8":"code","3de05cc3":"code","87ecdf3b":"code","ad580484":"code","b66e8aa8":"code","b9965fa3":"code","5e924c5d":"code","b5fa3cd1":"code","84a67d8d":"code","3904e474":"code","844a3e13":"code","069966b8":"code","e1364ba3":"code","3ef68c44":"code","94ccbbfa":"code","b073f1db":"code","45ad99fe":"code","7ab920d1":"code","dadb4b96":"code","28662aa5":"code","07658a5a":"code","1cb80ab6":"code","9536a223":"code","dd30f6de":"code","78b1fb7e":"code","d0dbb563":"code","8917c74e":"code","97b21054":"code","7eea55a4":"code","59b3e872":"code","e8eb13a8":"code","f91a189c":"code","952abbbf":"code","9755b8be":"code","dd97cc39":"code","899b4a76":"code","6268b62a":"code","7d46d740":"code","ff165594":"code","c2af8086":"code","30caeced":"code","4ad0842d":"code","0a090fad":"code","7aa71e54":"code","893ce9c4":"code","17752bc2":"code","0bf31d2b":"code","49584981":"code","c0e053cb":"code","90b77ecb":"code","76fbdf16":"markdown","5d58b7ab":"markdown","a7a86372":"markdown","6d843157":"markdown","bb968b83":"markdown","90c06ce9":"markdown","1f8e1635":"markdown"},"source":{"f79ed9ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","493b6c29":"tweets = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding=\"ISO-8859-1\")","85be65d2":"tweets.columns = ['sentiment','id','date','query','special','text']","6823ffbe":"tweets.head()","ccba64aa":"def convert_labels(val):\n    if(val == 0):\n        return 'NEGATIVE'\n    elif(val == 2):\n        return 'NEUTRAL'\n    else:\n        return 'POSITIVE'","f2443e88":"tweets['output'] = tweets['sentiment'].apply(convert_labels)","0cfa8aa4":"tweets['output']","fb389dbc":"tweets.head()","db0af8d0":"import re","2ef7c384":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt    ","90f6c49f":"def keep_pattern(input_txt, pattern):\n    lis = input_txt.split(' ')\n    nlis = []\n    for word in lis:\n        r = re.findall(pattern, word)\n        nlis.append(''.join(r))\n    return ' '.join(nlis)","f4c500f5":"def to_lower(txt):\n    return txt.lower()","3b2848c8":"def remove_username(tweet):\n    return remove_pattern(tweet,\"@[\\w]*\")","e9d5c3bf":"def remove_hashtags(tweet):\n    return remove_pattern(tweet,\"#[\\w]*\")","f1b3de2c":"def remove_urls(tweet):\n    return remove_pattern(tweet,\"http\\S+\")","f846b952":"def remove_symbols(tweet):\n    return remove_pattern(tweet,'\\(\\(\\(')","a4544517":"def pre_process(tweet):\n    tweet = tweet.lower()\n    tweet = keep_pattern(tweet,'[a-zA-Z0-9]')\n    tweet = remove_username(tweet)\n    tweet = remove_hashtags(tweet)\n    tweet = remove_urls(tweet)\n    return tweet","a561902f":"remove_urls('A #python package to manage your #code #snippets use it from your terminal easily. To install type pip install codesnip and there you have this package installed. For more visit https:\/\/pypi.org\/project\/codesnip\/\u2026  its very useful for #developers #coders and its free.')","048cfd52":"tweets['filtered_tweets'] = tweets['text'].apply(pre_process)","07e15a07":"tweets.head()","bf061a6f":"from sklearn.model_selection import train_test_split","22f23f5f":"X_train, X_test, y_train, y_test = train_test_split(tweets['filtered_tweets'],tweets['output'],stratify=tweets['output'],test_size=0.2,random_state=1)","ffa1804c":"y_train.value_counts()","d23e9e57":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline","332b0e51":"tweet_clf_one = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', MultinomialNB()),\n])","b250eabf":"tweet_clf_one.fit(X_train,y_train)","41aab365":"predictions_one = tweet_clf_one.predict(X_test)","b8dbf095":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score","a279ba06":"accuracy_score(y_test,predictions_one)","1d598f81":"f1_score(y_test,predictions_one,pos_label='NEGATIVE')","e350150f":"f1_score(y_test,predictions_one,pos_label='POSITIVE')","b6c9b55e":"filename = 'finalized_model.sav'","3745af57":"import pickle","dbfb6c5a":"pickle.dump(tweet_clf_one, open(filename, 'wb'))","f426de97":"import pickle\nfilename = '\/kaggle\/input\/classify-tweet-sentiment\/finalized_model.sav'\ntweet_classify = pickle.load(open(filename, 'rb'))","0adbe7a5":"tweet_classify.predict(['Learning to live life by exploring and loving people'])","67883dc8":"!pip install tweepy","32d210a1":"def to_lower(text):\n  \"\"\"\n  Converts text to lowercase\n  \"\"\"\n  return text.lower()","b8f04ca2":"def extract_hashtags(text):\n  \"\"\"\n  Returns all hashtags in a tweet\n  \"\"\"\n  import re\n  regexp = '#[\\w+]*'\n  pattern = re.compile(regexp)\n  return pattern.findall(text)","785898e9":"def get_tweets(topic,max_count=1000):\n  \"\"\"\n  Returns 1000 tweets about a topic as a pandas dataframe.\n  change number by using max_count\n  \"\"\"\n  import tweepy\n  import csv\n  import unicodedata\n  api_key = \"J1MhAOtPirsUiacFFmgkSiJrn\"\n  api_secret = \"8ZPIL6tn1ycj4US19XnBdwEjj4SatHNl8eLw48MERDqh8INGYP\"\n  auth = tweepy.OAuthHandler(api_key, api_secret)\n  api = tweepy.API(auth,wait_on_rate_limit=True)\n  count = 0\n  date = []\n  tweets = []\n  df = pd.DataFrame(columns=['date','tweet'])\n  for tweet in tweepy.Cursor(api.search,q=topic,count=2000,\n                           lang=\"en\",\n                           tweet_mode='extended'\n                           ).items():\n    if(not tweet.full_text.startswith(\"RT @\")):\n      tweets.append(str(unicodedata.normalize('NFKD', tweet.full_text).encode('ascii','ignore')))\n      date.append(tweet.created_at)\n      count += 1\n\n    if(count > max_count):\n      break\n  \n  return pd.DataFrame(data={'date':date,'tweet':tweets})","ee66b4aa":"def make_edges(general_topic):\n  \"\"\"\n  Given a general_topic returns a dataframe with all possible edges\n  \n  \"\"\"\n  from itertools import combinations\n  tweets = get_tweets(general_topic)\n  tweets['tweet'] = tweets.tweet.apply(to_lower)\n  tweets['hashtags'] = tweets.tweet.apply(extract_hashtags)\n  tweets['no_of_hashtags'] = tweets.hashtags.apply(len)\n  tweets_filtered = tweets[tweets['no_of_hashtags'] > 1]\n  node1 = []\n  node2 = []\n  for tags in tweets_filtered.hashtags:\n    comb = combinations(tags,2)\n    for i in comb:\n      node1.append(i[0])\n      node2.append(i[1])\n  return pd.DataFrame(data={'node1':node1,'node2':node2})","38c8f569":"def make_hashmap(graph_df,col1='node1',col2='node2'):\n  \"\"\"\n  Used to make a networkx graph given a graph dataframe\n  pass column name if different from node1 and node2 in\n  col1 and col2\n\n  e.g. \n  make_hashmap(graph_df,col1='A',col2='B')\n  \"\"\"\n  import networkx as nx\n  G = nx.Graph()\n  for i in zip(graph_df[col1],graph_df[col2]):\n    if G.has_edge(i[0],i[1]):\n      G[i[0]][i[1]]['weight'] += 1\n    else:\n      G.add_edge(i[0],i[1],weight=1)\n  return G","71425591":"def make_community(hashmap,no=15):\n  \"\"\"\n  returns a asyn_fluid community given a graph, (its a list of set)\n  \"\"\"\n  from networkx.algorithms import community\n  comm_g = community.asyn_fluidc(hashmap,k=no)\n  lis = []\n  for i in comm_g:\n    lis.append(i)\n  return lis","bbf59ac0":"def get_community(topics,community):\n  \"\"\"\n  Given a list of topics and community list, returns most matching community\n  nodes\n  \"\"\"\n  topics = set(topics)\n  max = 0\n  lis = {}\n  for i in range(len(community)):\n    if(len(topics.intersection(community[i])) > max):\n      lis = community[i]\n      max = len(topics.intersection(community[i]))\n  return lis","9f199da6":"def get_sentiment(text,only=False):\n  \"\"\"\n  Given a text, returns sentiment.\n  only = True means only sentiment is returned and not subjectivity.\n  \"\"\"\n  from textblob import TextBlob\n  text_blob = TextBlob(text)\n  if(only):\n    sentiment = text_blob.sentiment\n    return sentiment[0]\n  return text_blob.sentiment","2d8925a4":"def node_value(adj,node):\n  \"\"\"\n  returns sum of weights\n  \"\"\"\n  tot = 0\n  for item in adj[node]:\n    tot += adj[node][item]['weight']\n  wt = tot\/len(adj[node])\n  return (node,tot)","4e7669ee":"def real_value(adj,node):\n  \"\"\"\n  returns a mix of len(adj[node])\n  \"\"\"\n  return (node, (len(adj[node]) * node_value(adj,node)[1]) \/ (len(adj[node]) + node_value(adj,node)[1]))","10fd6632":"def sort_lis_tup(tup):\n  tup.sort(key = lambda x: x[1],reverse=True)  \n  return tup","0f032d90":"def pick_topics(selected_community,adj):\n  node_values = [real_value(adj,topic) for topic in selected_community]\n  node_values = sort_lis_tup(node_values)\n  res = len(node_values) \/\/ 10\n  return node_values[:res]","1c0f63c7":"def sentiment_df(topic):\n  tweets = get_tweets(topic,max_count=200)\n  tweets['tweet'] = tweets.tweet.apply(to_lower)\n  tweets['sentiment'] = tweets.tweet.apply(get_sentiment,only=True)\n  return tweets","5628fca5":"def complete(general_topic, topic_list):\n  #Step 1: Make Graph\n  edges = make_edges(general_topic)\n  hashmap = make_hashmap(edges)\n  community = make_community(hashmap)\n  selected = get_community(topic_list, community)\n  adj = dict(hashmap.adjacency())\n  topics = pick_topics(selected,adj)\n  # Find sentiment for topic in topics\n  lis = []\n  for topic in topics:\n    lis.append(sentiment_df(topic[0]))\n  return pd.concat(lis)","a8950e2e":"def filter_zero(col,df):\n  filtered_df = df[df[col] != 0]\n  return filtered_df","1c4eb581":"def extract_weights(dic):\n  lis = []\n  for val in dic:\n    if(dic[val]['weight']>1):\n      lis.append((val,dic[val]['weight']))\n  return lis","7667b5d8":"def extract_related(topic,hashmap):\n  adj = dict(hashmap.adjacency())\n  if(topic.lower() in adj.keys()):\n    return extract_weights(adj[topic.lower()])","3de05cc3":"def fetch_related_topics(general_topic):\n  edges = make_edges(general_topic)\n  # print(edges.head())\n  hashmap = make_hashmap(edges)\n  related_topics = extract_related(general_topic,hashmap)\n  related_topics_sorted = sort_lis_tup(related_topics)\n  if(len(related_topics_sorted) > 10):\n    t = len(related_topics) \/\/ 10\n    return [i[0] for i in related_topics[:t]]\n  return [i[0] for i in related_topics]","87ecdf3b":"def new_related(topic,hashmap):\n    lis = []\n    for val in hashmap.neighbors(topic):\n        lis.append((val,hashmap[topic][val]['weight']))\n    sorted_rel = sort_lis_tup(lis)\n    if(len(sorted_rel) > 15):\n        return [i[0] for i in sorted_rel[:15]]\n    return [i[0] for i in sorted_rel]","ad580484":"def new_get_sentiment(classifier,tweet):\n    return classifier.predict([tweet])[0]","b66e8aa8":"def labeltoval(label):\n    t = {'NEGATIVE':-1,'POSITIVE':1}\n    return t[label]","b9965fa3":"def new_sentiment_topic(classifier,topic):\n    tweets = get_tweets(topic,max_count=1000)\n    tweets['tweet'] = tweets.tweet.apply(to_lower)\n    val = 0\n    count = 0\n    tweets['sentiment'] = tweets.tweet.apply(get_sentiment,only=True)\n    for tweet in tweets['tweet']:\n        x = labeltoval(new_get_sentiment(classifier,tweet))\n        val += x\n        count += 1\n    val = val\/count\n    print((topic,val))\n    return (topic,val)","5e924c5d":"def topics_sentiment(classifier,topics):\n    return [new_sentiment_topic(classifier,topic[0]) for topic in topics]","b5fa3cd1":"labeltoval('NEGATIVE')","84a67d8d":"topic_lis = fetch_related_topics('#biden')","3904e474":"print(topic_lis)","844a3e13":"general_topic = '#politics'","069966b8":"edges = make_edges(general_topic)\nhashmap = make_hashmap(edges)","e1364ba3":"community = make_community(hashmap,no=30)","3ef68c44":"len(community[0])","94ccbbfa":"print(community[11])","b073f1db":"related_topics = fetch_related_topics('#cricket')","45ad99fe":"print(related_topics)","7ab920d1":"selected = get_community(related_topics,community)","dadb4b96":"adj = dict(hashmap.adjacency())","28662aa5":"topics = pick_topics(selected,adj)","07658a5a":"print(topics)","1cb80ab6":"community = make_community(hashmap,no=15)","9536a223":"related_topics = new_related('#trump',hashmap)","dd30f6de":"print(related_topics)","78b1fb7e":"selected = get_community(related_topics,community)","d0dbb563":"adj = dict(hashmap.adjacency())","8917c74e":"topics = pick_topics(selected,adj)","97b21054":"print(topics)","7eea55a4":"import pickle\nfilename = '\/kaggle\/input\/classify-tweet-sentiment\/finalized_model.sav'\ntweet_classify = pickle.load(open(filename, 'rb'))","59b3e872":"new_get_sentiment(tweet_classify,'I suggest you to read harry potter but it gets dull and is stupid')","e8eb13a8":"new_sentiment_topic(tweet_classify,'#election2020')","f91a189c":"new_sentiment_topic(tweet_classify,'#trump')","952abbbf":"lis = topics_sentiment(tweet_classify,topics)","9755b8be":"lis","dd97cc39":"GENERAL_TOPIC = '#india'","899b4a76":"edges = make_edges(GENERAL_TOPIC)\nhashmap = make_hashmap(edges)","6268b62a":"community = make_community(hashmap,no=30)","7d46d740":"# Analyze nodes\nprint(community[20])","ff165594":"TOPIC_NODE = '#modi'","c2af8086":"related_topics = new_related(TOPIC_NODE,hashmap)\nselected = get_community(related_topics,community)\nadj = dict(hashmap.adjacency())\ntopics = pick_topics(selected,adj)","30caeced":"print(topics)","4ad0842d":"len(topics)","0a090fad":"# Classifier import\nimport pickle\nfilename = '\/kaggle\/input\/classify-tweet-sentiment\/finalized_model.sav'\nclassifier = pickle.load(open(filename, 'rb'))","7aa71e54":"result = topics_sentiment(classifier,topics[:])","893ce9c4":"print(GENERAL_TOPIC,TOPIC_NODE)","17752bc2":"#india #modi\n# ('#indian', 0.3246753246753247)\n# ('#kashmir', -0.1028971028971029)\n# ('#sardarvallabhbhaipatel', 0.6543456543456544)\n# ('#ironmanofindia', 0.8201798201798202)\n# ('#nationalunityday', 0.5784215784215784)\n# ('#sardarpatel', 0.6883116883116883)\n# ('#primeminister', 0.22792022792022792)\n# ('#unity', 0.8381618381618382)\n# ('#sardarpateljayanti', 0.6283716283716284)\n# ('#rashtriyaektadiwas', 0.5104895104895105)\n# ('#statueofunity', 0.7042957042957043)","0bf31d2b":"#politics #election\n# ('#vote', 0.4645354645354645)\n# ('#election', 0.06293706293706294)\n# ('#usa', 0.16283716283716285)\n# ('#donaldtrump', 0.16683316683316685)\n# ('#democrats', -0.022977022977022976)\n# ('#biden', 0.4745254745254745)\n# ('#republican', 0.4745254745254745)\n# ('#maga', 0.1108891108891109)\n# ('#bidenharris2020', 0.23276723276723277)\n# ('#covid', -0.32667332667332666)\n# ('#democrat', 0.12287712287712288)\n# ('#america', 0.20679320679320679)\n# ('#conservative', 0.2627372627372627)\n# ('#liberal', 0.4745254745254745)\n# ('#political', 0.08691308691308691)\n# ('#gop', 0.04095904095904096)\n# ('#bernie', -0.6523476523476524)\n# ('#trump2020', 0.21878121878121878)\n# ('#congress', 0.24475524475524477)\n# ('#bjp', 0.2987012987012987)\n# ('#p2', 0.6623376623376623)\n# ('#rt', 0.7382617382617382)","49584981":"#sports #sports\n# ('#nfl', 0.2967032967032967)\n# ('#betting', 0.6883116883116883)\n# ('#mlb', -0.058941058941058944)\n# ('#gaming', 0.5644355644355644)\n# ('#bettingtips', 0.32867132867132864)\n# ('#gamblingtwitter', 0.26873126873126874)\n# ('#espn', 0.5044955044955045)\n# ('#gambling', 0.2007992007992008)\n# ('#sportsbetting', 0.1848151848151848)\n# ('#nhl', -0.12087912087912088)","c0e053cb":"#cars #tesla\n# ('#electricvehicles', 0.26073926073926074)\n# ('#autos', 0.12352941176470589)\n# ('#tesla', 0.6223776223776224)\n# ('#electriccar', 0.37545126353790614)\n# ('#vw', 0.1188118811881188)\n# ('#electriccars', 0.2784992784992785)\n# ('#fiat', 0.7608409986859396)\n# ('#renault', 0.3734567901234568)","90b77ecb":"#invest #trade\n# ('#bitcoin', 0.08291708291708291)\n# ('#crypto', 0.3066933066933067)\n# ('#cryptocurrency', 0.42857142857142855)\n# ('#btc', -0.32667332667332666)\n# ('#blockchain', 0.5644355644355644)\n# ('#ethereum', 0.01898101898101898)\n# ('#forextrader', 0.4805194805194805)\n# ('#binaryoptions', 0.7362204724409449)\n# ('#eth', 0.45254745254745254)\n# ('#bitcoinmining', 0.13598074608904934)\n# ('#binary', 0.7263157894736842)\n# ('#bitcoins', 0.03940886699507389)\n# ('#bitcointrading', -0.06435643564356436)\n# ('#millionairemindset', 0.6906854130052724)\n# ('#binance', -0.24075924075924077)","76fbdf16":"# Related OLD","5d58b7ab":"# Related using adjacent","a7a86372":"# Complete","6d843157":"# Loading trained tweet classifier model","bb968b83":"# Various modules from colab","90c06ce9":"# Sentiment Analysis","1f8e1635":"# Make Hashmap"}}