{"cell_type":{"663d2431":"code","6a2ed6b4":"code","a9d7e588":"code","397ba4ac":"code","2b1d6eab":"code","0db609a2":"code","4f87d3a2":"code","020e07ed":"code","6df84735":"code","7e8573e3":"code","7354a4dc":"code","2cb97b3a":"code","e5c24acd":"code","6dd392f8":"code","6dd0a381":"code","db85f96d":"code","1a830fea":"code","59ec7980":"code","84a3eca9":"code","9507cadb":"code","c3ac35c1":"code","de688b0a":"code","0def40a5":"code","9ca91984":"code","e0a411fa":"code","b6821f8d":"code","ecc69485":"code","1798a077":"code","2d094140":"code","c32d6d1a":"code","97120a0a":"markdown","456ef8ed":"markdown","0e8e0e7a":"markdown","a4dcca4f":"markdown","44452150":"markdown","e401af25":"markdown","849ed233":"markdown"},"source":{"663d2431":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport re\nimport string\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a2ed6b4":"data = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\ndata.head()","a9d7e588":"data.shape","397ba4ac":"plt.figure(figsize = (10,6))\nplt.title(\"Data Distribution\")\nsns.countplot(x = \"sentiment\", data = data)\n\nplt.show()","2b1d6eab":"data['lenght'] = data['review'].apply(lambda x : len(x.split()))","0db609a2":"plt.figure(figsize = (10,6))\nplt.title(\"Text lenght\")\n\nsns.histplot(x=\"lenght\", data = data)\nplt.show()","4f87d3a2":"fig = plt.figure(figsize = (14,8))\nax1 = fig.add_subplot(121)\nplt.title(\"Positive text lenght\")\nsns.histplot(x=\"lenght\", data = data[data['sentiment'] == 'positive'], ax=ax1)\n\nax2 = fig.add_subplot(122)\nplt.title(\"Negative text lenght\")\nsns.histplot(x=\"lenght\", data = data[data['sentiment'] == 'negative'], ax=ax2)\n\nplt.show()","020e07ed":"# Example\ndata.iloc[1,0]","6df84735":"def clean_review(text):\n    clean_text = re.sub('<br\\s?\\\/>|<br>', '', text) \n    clean_text = re.sub('[^a-zA-Z\\']', ' ', clean_text)\n    clean_text = clean_text.lower()\n    return clean_text","7e8573e3":"data['review'] = data['review'].apply(lambda x : clean_review(x))\ndata.iloc[1,0]","7354a4dc":"# Splitting the data into train set and validation set\ntrain_data = data[:30000]\nval_data = data[30000:]","2cb97b3a":"import spacy\n\n# Create an empty model\nnlp = spacy.blank(\"en\")\n\n# Create custom TextCategorizer with exclusive classes and bag of words architecture\ntextcat = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"bow\"})\n\n# Add the TextCategorizer to the empty model\nnlp.add_pipe(textcat)\nprint(nlp.pipe_names)\ntextcat.add_label(\"positive\")\ntextcat.add_label(\"negative\")","e5c24acd":"# Data Preparation\ntrain_texts = train_data['review'].values\ntrain_labels = [{'cats': {'positive': label == 'positive','negative': label == 'negative'}} \n                for label in train_data['sentiment']]","6dd392f8":"from spacy.util import minibatch\nimport random\n\ndef model_train(model, train, optimizer):\n    losses = {}\n    random.seed(1)\n    random.shuffle(train)\n    \n    batches = minibatch(train, size=8)\n    for batch in batches:\n        texts, labels = zip(*batch)\n        model.update(texts, labels, sgd=optimizer, losses=losses)\n        \n    return losses","6dd0a381":"# 1st Iteration\nspacy.util.fix_random_seed(1)\nrandom.seed(1)\noptimizer = nlp.begin_training()\ntrain = list(zip(train_texts, train_labels))\nlosses = model_train(nlp, train, optimizer)\nprint(losses['textcat'])","db85f96d":"data.iloc[30001,:2]","1a830fea":"doc = nlp(data.iloc[30001,0])\nprint(doc.cats)","59ec7980":"# Predict list of reviews\ndef predict(nlp, texts): \n\n    docs = [nlp.tokenizer(text) for text in texts]    \n    # Use textcat to get the scores for each doc\n    textcat = nlp.get_pipe('textcat')\n    predicted_class = textcat.predict(docs)[0].argmin(axis=1)\n    \n    return predicted_class","84a3eca9":"data.iloc[30001:30004,:2]","9507cadb":"# Negative review -> 0; positive review -> 1\npredict(nlp, list(data.iloc[30001:30004,0].values))","c3ac35c1":"from sklearn.metrics import accuracy_score, f1_score\nmapper = {'positive':1, 'negative':0}\nval_data['sentiment'] = val_data['sentiment'].apply(lambda x : mapper[x])\nval_data.sentiment.values","de688b0a":"def evaluate(model, texts, labels): \n    predicted_class = predict(model, texts)\n    accuracy = accuracy_score(predicted_class, labels)\n    fscore = f1_score(predicted_class, labels)\n    return accuracy, fscore","0def40a5":"accuracy, f1score = evaluate(nlp, list(val_data.review.values), val_data.sentiment.values)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1_score: {f1score:.4f}\")","9ca91984":"# Model Training \nn_iters = 6\nfor i in range(n_iters):\n    losses = model_train(nlp, train, optimizer)\n    accuracy, f1score = evaluate(nlp, list(val_data.review.values), val_data.sentiment.values)\n    print(f\"Loss: {losses['textcat']:.3f} \\t Accuracy: {accuracy:.3f} \\t F1_Score: {f1score:.3f}\")","e0a411fa":"from wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nstp = set(stopwords.words('english'))","b6821f8d":"poswords = ' '.join([text for text in train_data[train_data['sentiment'] == 'positive']['review']])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, stopwords=stp,\n                      background_color='white').generate(poswords)\n\nplt.figure(figsize=(8, 5))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","ecc69485":"del nlp, textcat, optimizer","1798a077":"nlp = spacy.blank(\"en\")\n\n# Create custom TextCategorizer with exclusive classes and cnn architecture\ntextcat = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"})\nnlp.add_pipe(textcat)\nprint(nlp.pipe_names)\ntextcat.add_label(\"positive\")\ntextcat.add_label(\"negative\")","2d094140":"def load_data(data):\n    # Splitting the data into train set and validation set\n    train_data = data[:10000]\n    val_data = data[10000:13000]\n    mapper = {'positive':1, 'negative':0}\n    val_data['sentiment'] = val_data['sentiment'].apply(lambda x : mapper[x])\n    train_texts = train_data['review'].values\n    train_labels = [{'cats': {'positive': label == 'positive','negative': label == 'negative'}} \n                for label in train_data['sentiment']]\n    return list(zip(train_texts, train_labels)), list(val_data.review.values), val_data.sentiment.values","c32d6d1a":"n_iters = 1\nspacy.util.fix_random_seed(1)\nrandom.seed(1)\noptimizer = nlp.begin_training()\ntrain, valrev, valsnt = load_data(data)\nfor i in range(n_iters):\n    losses = model_train(nlp, train, optimizer)\n    accuracy, f1score = evaluate(nlp, valrev, valsnt)\n    print(f\"Loss: {losses['textcat']:.3f} \\t Accuracy: {accuracy:.3f} \\t F1_Score: {f1score:.3f}\")","97120a0a":"Prediction:","456ef8ed":"For efficient processing we will disable some pipline component like parser and named entity recognition","0e8e0e7a":"### CNN","a4dcca4f":"## Data Exploration and Processing","44452150":"## Modelling\n### Bag Of Words","e401af25":"WordCloud of positive reviews, It is better to add some words to our stopwords set such as: movie, film, story ... because such words may appear in both positive and negative reviews.","849ed233":"Train a CNN architecture needs many resources (RAM), I will use a small subset of data."}}