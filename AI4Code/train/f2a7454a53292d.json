{"cell_type":{"df5d7bec":"code","6a2d5729":"code","db2df08a":"code","986a0445":"code","50d381b8":"code","b89b2f64":"code","41247f6d":"code","ff92682f":"code","8ca57f79":"code","9d7fa5be":"code","118f2256":"code","b0f6a22d":"code","f47794a9":"code","92219486":"code","2a7cb2ef":"code","3ae1e33a":"code","314d3208":"code","a7e88a9e":"code","8addf2d0":"code","02434f66":"code","b60b4eb1":"code","786ea852":"code","275bd6cd":"code","b6c32d9c":"markdown","71d00d12":"markdown","21a98479":"markdown","566b4df8":"markdown","612126e7":"markdown","c2d47206":"markdown"},"source":{"df5d7bec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6a2d5729":"from keras.layers import Input, Dense\nfrom keras.models import Model\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.noise import GaussianNoise\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.utils import np_utils\nfrom keras.callbacks import History\nfrom keras import regularizers\nfrom keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split\n\nimport pandas as pd \nimport numpy as np\n\ncolumns = [\n    \"title\",\n    \"year\",\n    \"lifetime_gross\",\n    \"ratingInteger\",\n    \"ratingCount\",\n    \"duration\",\n    \"nrOfWins\",\n    \"nrOfNominations\",\n    \"nrOfPhotos\",\n    \"nrOfNewsArticles\",\n    \"nrOfUserReviews\",\n    \"nrOfGenre\",\n    \"Action\",\n    \"Adult\" ,\n    \"Adventure\",\n    \"Animation\",\n    \"Biography\",\n    \"Comedy\",\n    \"Crime\",\n    \"Documentary\",\n    \"Drama\",\n    \"Family\",\n    \"Fantasy\",\n    \"Horror\",\n    \"Music\",\n    \"Musical\",\n    \"Mystery\",\n    \"News\",\n    \"RealityTV\",\n    \"Romance\",\n    \"SciFi\",\n    \"Short\",\n    \"Sport\",\n    \"TalkShow\",\n    \"Thriller\",\n    \"War\",\n    \"Western\"]\ndt = pd.read_csv('..\/input\/movies-example-for-machine-learning-activities\/MACHINE_LEARNING_FINAL.csv', header=None, names=columns, sep=\";\")\n\ndef plot_history(network_history):\n    plt.figure(figsize=(14,7))\n    ax = plt.subplot(1, 2, 1)\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.plot(x_plot, network_history.history['loss'])\n    plt.plot(x_plot, network_history.history['val_loss'])\n    plt.legend(['Training', 'Validation'])\n\n    ax = plt.subplot(1, 2, 2)\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.plot(x_plot, network_history.history['accuracy'])\n    plt.plot(x_plot, network_history.history['val_accuracy'])\n    plt.legend(['Training', 'Validation'], loc='lower right')\n\n    plt.show()\n\ndef plot_representation(x_test,encoded_imgs,codex,codey,decoded_imgs):\n    n = 14\n    plt.figure(figsize=(16, 3))\n    for i in range(n):\n        # display original\n        ax = plt.subplot(3, n, i + 1)\n        plt.imshow(x_test[i].reshape(28, 28))\n        plt.gray()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\n        ax = plt.subplot(3, n, i + 1 + n)\n        plt.imshow(encoded_imgs[i].reshape(codex, codey))\n        plt.gray()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\n        # display reconstruction\n        ax = plt.subplot(3, n, i + 1 + 2*n)\n        plt.imshow(decoded_imgs[i].reshape(28, 28))\n        plt.gray()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\n    plt.show()\n    \ndef preprocess_labels(labels, encoder=None, categorical=True):\n    if not encoder:\n        encoder = LabelEncoder()\n        encoder.fit(labels)\n    y = encoder.transform(labels).astype(np.int32)\n    if categorical:\n        y = np_utils.to_categorical(y)\n    return y, encoder","db2df08a":"dt.shape","986a0445":"dt.columns","50d381b8":"dati = dt.loc[1:, dt.columns != 'ratingInteger']\ndati = dati.loc[:, dati.columns != 'title']\nY = dt['ratingInteger']\nY = Y[1:]","b89b2f64":"dati.shape","41247f6d":"Y.shape","ff92682f":"train , test , labels , tlabels = train_test_split(dati, Y)\nprint(train.shape)\nprint(test.shape)","8ca57f79":"labels.value_counts()","9d7fa5be":"tlabels.value_counts()","118f2256":"labels, encoder = preprocess_labels(labels)\nlabels","b0f6a22d":"tlabels, tencoder = preprocess_labels(tlabels)\ntlabels","f47794a9":"n_epochs = 100\nb = 100\ndims=len(train.columns)\nnb_classes=len(Y.unique())\n\nx_plot = list(range(1,n_epochs+1))\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(dims,)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(nb_classes, activation='softmax'))\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\nnetwork_history = model.fit(train, labels, epochs=n_epochs, batch_size=b, shuffle=True, validation_data=(test, tlabels))\nmodel.summary()","92219486":"plot_history(network_history)\n\nscore = model.evaluate(test,tlabels, batch_size=b) #evaluating the models accuracy or loss,\nprint('test loss, test acc:', score)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\nprint(\"\\n%s: %.2f\" % (model.metrics_names[0], score[0]))","2a7cb2ef":"from sklearn import preprocessing\n\nx = dati.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndata = pd.DataFrame(x_scaled) # <-- Look! this is another variable\n\nseed = 2\nnp.random.seed(seed)","3ae1e33a":"train , test , labels , tlabels = train_test_split(data, Y)\nprint(train.shape)\nprint(test.shape)\nprint(labels.value_counts())\nprint(tlabels.value_counts())\n\nlabels, encoder = preprocess_labels(labels)\nprint(labels)\n\ntlabels, tencoder = preprocess_labels(tlabels)\nprint(tlabels)\n","314d3208":"n_epochs = 200\nb = 100\ndims=len(train.columns)\nnb_classes=len(Y.unique())\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(dims,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(nb_classes, activation='softmax'))\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\nnetwork_history = model.fit(train, labels, epochs=n_epochs, batch_size=b, shuffle=True, validation_data=(test, tlabels))\nmodel.summary()","a7e88a9e":"x_plot = list(range(1,n_epochs+1))\n\nplot_history(network_history)\n\nscore = model.evaluate(test,tlabels, batch_size=b) #evaluating the models accuracy or loss,\nprint('test loss, test acc:', score)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\nprint(\"\\n%s: %.2f\" % (model.metrics_names[0], score[0]))","8addf2d0":"n_epochs = 200\nb = 200\ndims=len(train.columns)\nnb_classes=len(Y.unique())\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(dims,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(nb_classes, activation='softmax'))\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\nnetwork_history = model.fit(train, labels, epochs=n_epochs, batch_size=b, shuffle=True, validation_data=(test, tlabels))\nmodel.summary()","02434f66":"x_plot = list(range(1,n_epochs+1))\n\nplot_history(network_history)\n\nscore = model.evaluate(test,tlabels, batch_size=b) #evaluating the models accuracy or loss,\nprint('test loss, test acc:', score)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\nprint(\"\\n%s: %.2f\" % (model.metrics_names[0], score[0]))","b60b4eb1":"n_epochs = 200\nb = 512\ndims=len(train.columns)\nnb_classes=len(Y.unique())\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(dims,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(nb_classes, activation='softmax'))\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\nnetwork_history = model.fit(train, labels, epochs=n_epochs, batch_size=b, shuffle=True, validation_data=(test, tlabels))\nmodel.summary()","786ea852":"x_plot = list(range(1,n_epochs+1))\n\nplot_history(network_history)\n\nscore = model.evaluate(test,tlabels, batch_size=b) #evaluating the models accuracy or loss,\nprint('test loss, test acc:', score)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\nprint(\"\\n%s: %.2f\" % (model.metrics_names[0], score[0]))","275bd6cd":"n_epochs = 200\nb = 512\ndims=len(train.columns)\nnb_classes=len(Y.unique())\n\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape=(dims,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(nb_classes, activation='softmax'))\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\nnetwork_history = model.fit(train, labels, epochs=n_epochs, batch_size=b, shuffle=True, validation_data=(test, tlabels))\nmodel.summary()\n\nx_plot = list(range(1,n_epochs+1))\n\nplot_history(network_history)\n\nscore = model.evaluate(test,tlabels, batch_size=b) #evaluating the models accuracy or loss,\nprint('test loss, test acc:', score)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\nprint(\"\\n%s: %.2f\" % (model.metrics_names[0], score[0]))","b6c32d9c":"MODEL 1","71d00d12":"# In the next notebook I try to use Fine-Tuning and Transfer-Learning to enanche the performance of this task. \n# Actually HP Optimization I can't do it due to a problem of installation pyGPGO packages.","21a98479":"Thanks for you attention","566b4df8":"SECOND APPROACH WITH DEEP LEARNING\n\n\nadd normalization of data before training and test and Dropout Regularization","612126e7":"MODEL 2\n\n","c2d47206":"FIRST APPROACH WITH DEEP LEARNING"}}