{"cell_type":{"b9b2aabe":"code","c3bb7b46":"code","61883a7c":"code","64adfe66":"code","713c70d2":"code","c4cd13dd":"code","d6c7be65":"code","3b64ec1a":"code","57682a6a":"code","8d7a8afc":"code","d662e269":"code","5a2d4a81":"code","97183415":"code","f653fc98":"code","6741d152":"code","ce4dd6c7":"code","4a1c11ad":"code","552956a2":"code","396092d4":"code","93bebe45":"code","e31a2d75":"code","0039a36e":"code","5a3a0820":"code","7740dec6":"code","5e066214":"code","ef2cc434":"code","197d7bb5":"code","badfba8d":"code","44203542":"code","392548ed":"code","92ed3caf":"code","33567d50":"code","ba74ed34":"code","7dc68969":"code","91e8a2a0":"code","041ed8ed":"code","8f3ef892":"code","e8c7a234":"code","a6dbe972":"code","a84fa257":"code","f4bcf413":"code","df92d61f":"code","dbdd58fd":"code","52c00c79":"code","04c05c51":"code","a66df92d":"code","39a26e85":"code","22fa3216":"code","bc04c099":"code","85ad3e6e":"code","0d079833":"code","8b675d6d":"code","f3020521":"code","716c1dd1":"code","bc9d5b7b":"code","281d90d5":"markdown","9fdf3084":"markdown","c8839066":"markdown","9d176c69":"markdown","876c491d":"markdown","121cd850":"markdown","e50ad35e":"markdown","a8a1adb9":"markdown","243c18e8":"markdown","6cbdf60b":"markdown","e2434f47":"markdown","147134ca":"markdown","b0d955df":"markdown","d0d5b876":"markdown","75ba488d":"markdown","d3a643ba":"markdown","add02f2e":"markdown","b6844992":"markdown","bbfb47ac":"markdown","e3dd3cb4":"markdown","88231d0f":"markdown","662580d4":"markdown","5b056d6d":"markdown","ef4b2ea7":"markdown","621e38ee":"markdown","d963be11":"markdown","5f00781b":"markdown","c51c4648":"markdown","a0da4f19":"markdown","48e27148":"markdown","1ee28f4b":"markdown","10bdc13f":"markdown","75e31c50":"markdown","4c554eca":"markdown","21538915":"markdown"},"source":{"b9b2aabe":"# data analysis\nimport numpy as np\nimport pandas as pd\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","c3bb7b46":"#import train and test CSV files\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nfull_data = [train, test]\n\nprint(train.columns.values)","61883a7c":"# preview the data\ntrain.head()","64adfe66":"test.head()","713c70d2":"train.info()","c4cd13dd":"test.info()","d6c7be65":"#see a summary of the training dataset\ntrain.describe(include = \"all\")","3b64ec1a":"test.describe(include=\"all\")","57682a6a":"#check for any other NaN\nprint(pd.isnull(train).sum())","8d7a8afc":"print(pd.isnull(test).sum())","d662e269":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5a2d4a81":"#draw a bar plot of survival by Pclass\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n\n#print percentage of people by Pclass that survived\nfor i in range(0,3):\n    print(\"survival rate:\", train[\"Survived\"][train[\"Pclass\"] == i+1].value_counts(normalize = True)[1]*100)","97183415":"train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f653fc98":"#draw a bar plot of survival by sex\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n\n#print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","6741d152":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","ce4dd6c7":"#draw a bar plot for SibSp vs. survival\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train)","4a1c11ad":"train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","552956a2":"#draw a bar plot for Parch vs. survival\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train)\nplt.show()","396092d4":"sns.set_style(\"whitegrid\")# opitional:darkgrid , whitegrid , dark , white ,\u548c ticks\ngird = sns.FacetGrid(train, col='Survived',size=2.8, aspect=1.5)\ngird.map(plt.hist, 'Age', bins=20)\ngird.add_legend()\nplt.show()","93bebe45":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train, row='Embarked', size=2.5, aspect=1.5)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()\nplt.show()","e31a2d75":"train = train.drop(['Ticket', 'Cabin'], axis=1)\ntest = test.drop(['Ticket', 'Cabin'], axis=1)\nfull_data = [train, test]","0039a36e":"freq_port = train.Embarked.dropna().mode()[0]\nfreq_port","5a3a0820":"for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","7740dec6":"#extract a title for each Name in the train and test datasets\nfor dataset in full_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","5e066214":"for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n     'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","ef2cc434":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","197d7bb5":"#drop the name feature since it contains no more useful information.\ntrain = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)\nfull_data = [train, test]","badfba8d":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in full_data:\n    \n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)\n\ntrain.head()","44203542":"for dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ntrain['AgeGroup'] = pd.cut(train['Age'], 5)\nprint(train[['AgeGroup', 'Survived']].groupby(['AgeGroup'], as_index=False).mean().sort_values(by='AgeGroup', ascending=True))\n\ntrain.head()","392548ed":"test.head()","92ed3caf":"for dataset in full_data:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\ntrain.head()","33567d50":"test.head()","ba74ed34":"train = train.drop(['AgeGroup'], axis=1)\nfull_data = [train, test]\ntrain.head()","7dc68969":"#fill in missing Fare value in test,only test has a missing value\ntest['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)\n#creat FareBand\ntrain['FareBand'] = pd.qcut(train['Fare'], 4)\ntrain[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","91e8a2a0":"for dataset in full_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n#drop the FareBand\ntrain = train.drop(['FareBand'], axis=1)\nfull_data = [train, test]\n\ntrain.head()","041ed8ed":"test.head()","8f3ef892":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)\n\ntrain.head()","e8c7a234":"test.head()","a6dbe972":"for dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False))","a84fa257":"for dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\nprint (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean().sort_values(by='Survived', ascending=False))","f4bcf413":"train = train.drop(['Parch', 'SibSp', 'FamilySize',], axis=1)\ntest = test.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\nfull_data = [train, test]\n#check train data\ntrain.head()","df92d61f":"#check test data\ntest.head()","dbdd58fd":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(predictors, target, random_state = 0)","52c00c79":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\nacc_knn = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_knn)","04c05c51":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\nacc_logreg = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_logreg)","a66df92d":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_test)\nacc_gaussian = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gaussian)","39a26e85":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nacc_svc = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_svc)","22fa3216":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_test)\nacc_linear_svc = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_linear_svc)","bc04c099":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_test)\nacc_perceptron = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_perceptron)","85ad3e6e":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_test)\nacc_decisiontree = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_decisiontree)","0d079833":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_randomforest)","8b675d6d":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_test)\nacc_sgd = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_sgd)","f3020521":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","716c1dd1":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","bc9d5b7b":"#set PassengerId as PassengerId and predict survival \nPassengerId = test['PassengerId']\npredictions = decisiontree.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : PassengerId, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","281d90d5":"### FamilySize Feature\nWe can create a new feature for FamilySize which combines Parch and SibSp. \nThis will enable us to drop Parch and SibSp from our datasets.","9fdf3084":"Let us replace Age with ordinals based on these groups.","c8839066":"## 6) Choosing the Best Model","9d176c69":"## 1) Import Necessary Libraries\nFirst of all, we need to import several Python libraries such as numpy, pandas, matplotlib and seaborn.","876c491d":"### Age Feature\nLet us create Age gruups and determine correlations with Survived.","121cd850":"we can quickly analyze our feature correlations by pivoting features against each other,and visualize each features.  ","e50ad35e":"Now we can safely drop the Name feature from training and testing datasets. ","a8a1adb9":"People with less than four parents or children aboard are more likely to survive than those with four or more. Again, people traveling alone are less likely to survive than those with 1-3 parents or children.","243c18e8":"### Fare Feature\nWe may also want round off the fare to two decimals as it represents currency.","6cbdf60b":"### Splitting the Training Data\nWe will use part of our training data (25% in this case) to test the accuracy of our different models.","e2434f47":"## Sources:\n* [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n* [Titanic Survival Predictions (Beginner)](https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner)","147134ca":"## 7) Creating Submission File\nIt's time to create a submission.csv file to upload to the Kaggle competition!","b0d955df":"Let's compare the accuracies of each model!","d0d5b876":"## 2)Acquire data\n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.","75ba488d":"We can replace many titles with a more common name or classify them as `Rare`.","d3a643ba":"I decided to use the Decision Tree model for the testing data.","add02f2e":"### Sex Feature\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.","b6844992":"### Testing Different Models\nI will be testing the following models with my training data(got the list from [here](http:\/\/https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)):\n* KNN or k-Nearest Neighbors\n* Logistic Regression\n* Gaussian Naive Bayes\n* Support Vector Machines\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\n\nFor each model, we set the model, fit it with 75% of our training data, predict for 25% of the training data and check the accuracy.","bbfb47ac":"Now we can safely drop the AgeGroup feature from training and testing datasets. ","e3dd3cb4":"**Question:Which features are categorical?**\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\n**Question:Which features are numerical?**\n- Continous: Age, Fare. Discrete: SibSp, Parch.\n**Question:Which features are Alphanumeric?**\n- Ticket, Cabin\n#### What are the data types for each feature?\n* Survived: int\n* Pclass: int\n* Name: string\n* Sex: string\n* Age: float\n* SibSp: int\n* Parch: int\n* Ticket: string\n* Fare: float\n* Cabin: string\n* Embarked: string","88231d0f":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.","662580d4":"## 4)Analyze by pivoting,visualizing features","5b056d6d":"### Name Feature\nWe decide to retain the new Title feature for model training.","ef4b2ea7":"### IsAlone Feature\nWe can create another feature called IsAlone.","621e38ee":"# Titanic Solution (Good work for Beginner)\nI am a newbie to data science and machine learning, and will be attempting to work my way through the Titanic: Machine Learning from Disaster dataset. Please consider upvoting if this is useful to you! :)\n\n### Contents:\n1. Import Necessary Libraries\n2. Acquire data \n3. Data Analysis(describing)\n4. Analyze by pivoting,visualizing features\n5. Cleaning Data\n6. Choosing the Best Model\n7. Creating Submission File\n\nAny and all feedback is welcome! ","d963be11":"people with higher socioeconomic class had a higher rate of survival.","5f00781b":"We can now convert the EmbarkedFill feature by creating a new numeric Port feature.","c51c4648":"In general, it's clear that people with more siblings or spouses aboard were less likely to survive. However, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)","a0da4f19":"### Embarked Feature","48e27148":"Complete and add Embarked feature to model training.","1ee28f4b":"## 5) Cleaning Data\nTime to clean our data to account for missing values and unnecessary information!","10bdc13f":"females have a much higher chance of survival than males. The Sex feature is essential in our predictions.","75e31c50":"## 3) Data Analysis\n1)Analyze by describing data","4c554eca":"Convert the Fare feature to ordinal values based on the FareBand.\nwe can safely drop the FareBand feature from training and testing datasets. ","21538915":"We can convert the categorical titles to ordinal."}}