{"cell_type":{"8840360d":"code","d2fc6f63":"code","2b4a753c":"code","322f921b":"code","e07f3631":"code","f337134f":"code","ebdcb10a":"code","e2b14748":"code","98211f33":"code","aa23f9c7":"code","66a21623":"code","77f5cf11":"code","26fcc1e7":"code","409d2d6a":"code","4f07fdb1":"code","274c0528":"code","12db1896":"code","b1d32628":"code","29f05334":"code","6fb31883":"code","61fc9134":"code","60558e78":"code","4bea7294":"code","fd5695cf":"code","c1cef702":"code","deb83288":"code","e7a35b07":"code","1e6e6585":"code","7f6205fd":"code","c7d3507e":"code","9c1ebb5e":"code","ee590471":"code","18adb02e":"code","8bca815a":"code","0bea5ca3":"code","a1c29ce9":"code","2d8e6d2f":"code","a01398f7":"code","cbc920f3":"code","dc9165d5":"code","9fc23ce1":"code","34838161":"code","ca0004bc":"code","d48011f1":"code","bb9801b5":"code","f1fa0b3c":"code","03641fc0":"code","302908e0":"code","6f8e7010":"code","1b280e09":"code","83dbd824":"code","4e23363b":"code","9b609a53":"code","dac28657":"code","a7db1050":"code","8474820c":"code","add36458":"code","397c6855":"code","a279fdb1":"code","a7e74cc6":"code","34138efa":"code","b8db7faa":"code","e15ce997":"code","56f90539":"code","936f5f40":"code","dcb3dc1f":"code","4089b814":"markdown","13b179cd":"markdown","117e540b":"markdown","9c3160d9":"markdown"},"source":{"8840360d":"import os\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport time\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm\n\npath = '..\/input\/covid19-global-forecasting-week-4\/'\ntrain = pd.read_csv(path + 'train.csv')\ntest  = pd.read_csv(path + 'test.csv')\nsub   = pd.read_csv(path + 'submission.csv')\n\ntrain['Date'] = train['Date'].apply(lambda x: (datetime.datetime.strptime(x, '%Y-%m-%d')))\ntest['Date'] = test['Date'].apply(lambda x: (datetime.datetime.strptime(x, '%Y-%m-%d')))\n#path_ext = '..\/input\/novel-corona-virus-2019-dataset\/'\n#ext_rec = pd.read_csv(path_ext + 'time_series_covid_19_recovered.csv').\\\n#        melt(id_vars=[\"Province\/State\", \"Country\/Region\", \"Lat\", \"Long\"], \n#            var_name=\"Date\", \n#            value_name=\"Recoveries\")\n#ext_rec['Date'] = ext_rec['Date'].apply(lambda x: (datetime.datetime.strptime(x+\"20\", '%m\/%d\/%Y')))\n#train = train.merge(ext_rec[['Province\/State', 'Country\/Region', 'Date', 'Recoveries']], how='left',\n#           left_on=['Province\/State', 'Country\/Region', 'Date'],\n#           right_on=['Province\/State', 'Country\/Region', 'Date'])\n\ntrain['days'] = (train['Date'].dt.date - train['Date'].dt.date.min()).dt.days\ntest['days'] = (test['Date'].dt.date - train['Date'].dt.date.min()).dt.days\n#train['isTest'] = train['Date'].dt.date >= datetime.date(2020, 3, 12)\n#train['isVal'] = np.logical_and(train['Date'].dt.date >= datetime.date(2020, 3, 11), train['Date'].dt.date <= datetime.date(9999, 3, 18))\ntrain.loc[train['Province_State'].isnull(), 'Province_State'] = 'N\/A'\ntest.loc[test['Province_State'].isnull(), 'Province_State'] = 'N\/A'\n\ntrain['Area'] = train['Country_Region'] + '_' + train['Province_State']\ntest['Area'] = test['Country_Region'] + '_' + test['Province_State']\n\nprint('train Date max',train['Date'].max())\nprint('test Date min',test['Date'].min())\nprint('train days max', train['days'].max())\nN_AREAS = train['Area'].nunique()\nAREAS = np.sort(train['Area'].unique())\nSTART_PUBLIC = test['days'].min()\nprint('public LB start day', START_PUBLIC)\nprint(' ')\n\n\nTRAIN_N = 77\nprint(train[train['days'] < TRAIN_N]['Date'].max())\nprint(train[train['days'] >= TRAIN_N]['Date'].min())\nprint(train[train['days'] >= TRAIN_N]['Date'].max())\ntrain.head()\n\ntest_orig = test.copy()","d2fc6f63":"print('test Date max',test['days'].max())","2b4a753c":"106-77","322f921b":"train_p_c_raw = train.pivot(index='Area', columns='days', values='ConfirmedCases').sort_index()\ntrain_p_f_raw = train.pivot(index='Area', columns='days', values='Fatalities').sort_index()\n\ntrain_p_c = np.maximum.accumulate(train_p_c_raw, axis=1)\ntrain_p_f = np.maximum.accumulate(train_p_f_raw, axis=1)\n\nf_rate = (train_p_f \/ train_p_c).fillna(0)\n\nX_c = np.log(1+train_p_c.values)[:,:TRAIN_N]\nX_f = train_p_f.values[:,:TRAIN_N]\n","e07f3631":"from sklearn.metrics import mean_squared_error\n\ndef eval1(y, p):\n    val_len = y.shape[1] - TRAIN_N\n    return np.sqrt(mean_squared_error(y[:, TRAIN_N:TRAIN_N+val_len].flatten(), p[:, TRAIN_N:TRAIN_N+val_len].flatten()))\n\ndef run_c(params, X, test_size=50):\n    \n    gr_base = []\n    gr_base_factor = []\n    \n    x_min = np.ma.MaskedArray(X, X<1)\n    x_min = x_min.argmin(axis=1) \n    \n    for i in range(X.shape[0]):\n        temp = X[i,:]\n        threshold = np.log(1+params['min cases for growth rate'])\n        num_days = params['last N days']\n        if (temp > threshold).sum() > num_days:\n            d = np.diff(temp[temp > threshold])[-num_days:]\n            w = np.arange(len(d))+1\n            w = w**5\n            w = w \/ np.sum(w)\n            gr_base.append(np.clip(np.average(d, weights=w), 0, params['growth rate max']))\n            d2 = np.diff(d)\n            w = np.arange(len(d2))+1\n            w = w**10\n            w = w \/ np.sum(w)\n            gr_base_factor.append(np.clip(np.average(d2, weights=w), -0.5, params[\"growth rate factor max\"]))\n        else:\n            gr_base.append(params['growth rate default'])\n            gr_base_factor.append(params['growth rate factor'])\n\n    gr_base = np.array(gr_base)\n    gr_base_factor = np.array(gr_base_factor)\n    #print(gr_base_factor)\n    #gr_base = np.clip(gr_base, 0.02, 0.8)\n    preds = X.copy()\n\n    for i in range(test_size):\n        delta = np.clip(preds[:, -1], np.log(2), None) + gr_base * (1 + params['growth rate factor']*(1 + params['growth rate factor factor'])**(i))**(np.log1p(i))\n        #delta = np.clip(preds[:, -1], np.log(2), None) + gr_base * (1 + gr_base_factor*(1 + params['growth rate factor factor'])**(i))**(i)\n        #delta = np.clip(preds[:, -1], np.log(2), None) + gr_base * (1 + params['growth rate factor']*(1 + params['growth rate factor factor'])**(i+X.shape[1]-x_min))**(i+X.shape[1]-x_min) \n        preds = np.hstack((preds, delta.reshape(-1,1)))\n\n    return preds\n\nparams = {\n    \"min cases for growth rate\": 0,\n    \"last N days\": 15,\n    \"growth rate default\": 0.2,\n    \"growth rate max\": 0.3,\n    \"growth rate factor max\": -0.1,\n    \"growth rate factor\": -0.3,\n    \"growth rate factor factor\": 0.01,\n}\n#x = train_p_c[train_p_c.index==\"Austria_N\/A\"]\n\nx = train_p_c\n\npreds_c = run_c(params, np.log(1+x.values)[:,:TRAIN_N])\n#eval1(np.log(1+x).values, preds_c)","f337134f":"\n\nfor i in range(N_AREAS):\n    if 'China' in AREAS[i] and preds_c[i, TRAIN_N-1] < np.log(31):\n        preds_c[i, TRAIN_N:] = preds_c[i, TRAIN_N-1]\n\n\n","ebdcb10a":"def lin_w(sz):\n    res = np.linspace(0, 1, sz+1, endpoint=False)[1:]\n    return np.append(res, np.append([1], res[::-1]))\n\n\ndef run_f(params, X_c, X_f, X_f_r, test_size=50):\n  \n    X_f_r = np.array(np.ma.mean(np.ma.masked_outside(X_f_r, 0.03, 0.5)[:,:], axis=1))\n    X_f_r = np.clip(X_f_r, params['fatality_rate_lower'], params['fatality_rate_upper'])\n    #print(X_f_r)\n    \n    X_c = np.clip(np.exp(X_c)-1, 0, None)\n    preds = X_f.copy()\n    #print(preds.shape)\n    \n    train_size = X_f.shape[1] - 1\n    for i in range(test_size):\n        \n        t_lag = train_size+i-params['length']\n        t_wsize = 5\n        d = np.diff(X_c, axis=1)[:, t_lag-t_wsize:t_lag+1+t_wsize]\n#         w = np.arange(d.shape[1])[::-1]+1\n#         w = w**1\n#         w = w \/ np.sum(w)\n        delta = np.average(d, axis=1)\n        #delta = np.average(np.diff(X_c, axis=1)[:, t_lag-t_wsize:t_lag+1+t_wsize], axis=1, weights=lin_w(t_wsize))\n        \n        delta = params['absolute growth'] + delta * X_f_r\n        \n        preds = np.hstack((preds, preds[:, -1].reshape(-1,1) + delta.reshape(-1,1)))\n\n    return preds\n\nparams = {\n    \"length\": 7,\n    \"absolute growth\": 0.02,\n    \"fatality_rate_lower\": 0.02,\n    \"fatality_rate_upper\": 0.3,\n}\n\npreds_f_1 = run_f(params, preds_c, X_f, f_rate.values[:,:TRAIN_N])\npreds_f_1 = np.log(1+preds_f_1)","e2b14748":"from torch.utils.data import Dataset\nfrom torch.nn import Parameter\nimport torch.nn as nn\nfrom torch.nn import init\nimport math \nimport torch\nimport time\n\nclass ZDatasetF(Dataset):\n    def __init__(self, X_c, X_f=None, hist_len=10):\n        self.X_c = X_c\n        self.X_f = X_f\n        self.hist_len = hist_len\n        self.is_test = X_f is None\n    def __len__(self):\n        return self.X_c.shape[1]\n    def __getitem__(self, idx):\n        if self.is_test:\n            return {'x_c':self.X_c[:, idx-self.hist_len:idx]}\n        else:\n            return {'x_c':self.X_c[:, idx-self.hist_len:idx],\n                    'x_f':self.X_f[:, idx-1],\n                    'y':np.log(1+self.X_f[:, idx])}\n\nclass PrLayer2(nn.Module):\n    def __init__(self, in_features1, in_features2):\n        super(PrLayer2, self).__init__()\n        self.weight0 = Parameter(torch.Tensor(1, 1, in_features2))\n        self.weight1 = Parameter(torch.Tensor(1, in_features1, in_features2))\n        self.reset_parameters()\n    def reset_parameters(self):\n        init.kaiming_uniform_(self.weight0, a=math.sqrt(5))\n        init.kaiming_uniform_(self.weight1, a=math.sqrt(5))\n    def forward(self, input):\n        return input * torch.sigmoid(self.weight0 + self.weight1)\n\n\n\nclass ZModelF(nn.Module):\n\n    def __init__(self, hist_len):\n        super(ZModelF, self).__init__()\n        self.l_conv = PrLayer2(len(X_c),hist_len-1)\n\n    def forward(self, x_c, x_f):\n        x = x_c[:,:,1:] - x_c[:,:,:-1]\n        res = torch.sum(self.l_conv(x), 2)\n        return {'preds': torch.log(1 + x_f + res)}        \n        \n\nclass DummySampler(torch.utils.data.sampler.Sampler):\n    def __init__(self, idx):\n        self.idx = idx\n    def __iter__(self):\n        return iter(self.idx)\n    def __len__(self):\n        return len(self.idx)\n    \n    \ndef _smooth_l1_loss(target):\n    t = torch.abs(target)\n    t = torch.where(t < 1, 0.5 * t ** 2, t - 0.5)\n    return torch.mean(t)\n\n\nn_epochs = 5000\nlr = 0.18\nbag_size = 4\ndevice = 'cpu'\nhist_len = 14\nloss_func = torch.nn.MSELoss()\nreg_loss_func = _smooth_l1_loss\nreg_factor = 0.035\n\n\ntrain_dataset = ZDatasetF(np.exp(X_c)-1, X_f, hist_len=hist_len)\ntest_dataset = ZDatasetF(np.exp(preds_c)-1, hist_len=hist_len)\n\n#trn_idx = np.arange(hist_len+1, len(train_dataset))\ntrn_idx = np.arange(hist_len+1, len(train_dataset))\ntrain_sampler = torch.utils.data.sampler.SubsetRandomSampler(trn_idx)\n#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, sampler=train_sampler, num_workers=0, pin_memory=True)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=len(trn_idx), sampler=train_sampler, num_workers=0, pin_memory=True)\n\ntest_idx = np.arange(TRAIN_N, len(test_dataset))\ntest_sampler = DummySampler(test_idx)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, sampler=test_sampler, num_workers=0, pin_memory=True)\n\n\n#gradient_accumulation = len(trn_idx)\ngradient_accumulation = 1\n\npreds_f = 0\n\nfor m_i in range(bag_size):\n    model_f = ZModelF(hist_len=hist_len).to(device)\n    optimizer_f = torch.optim.Adam(model_f.parameters(), lr=lr)\n    model_f.train()\n\n    start_time = time.time()\n    for epoch in range(n_epochs):\n\n        s = time.time()\n        avg_train_loss = 0\n        \n        optimizer_f.zero_grad()\n        for idx, data in enumerate(train_loader):\n\n            X1 = data['x_c'].to(device).float()\n            X2 = data['x_f'].to(device).float()\n            y = data['y'].to(device).float()\n            \n            preds = model_f(X1, X2)['preds'].float()\n\n            cond = X2 > np.log(10)\n            preds = preds[cond]\n            y = y[cond]\n            \n            loss = loss_func(preds, y)\n            \n            loss += reg_factor * reg_loss_func(model_f.l_conv.weight1)\n            \n            avg_train_loss += loss  \/ len(train_loader)\n            \n            loss.backward()\n            if (idx+1) % gradient_accumulation == 0 or idx == len(train_loader) - 1: \n                optimizer_f.step()\n                optimizer_f.zero_grad()\n                \n        if epoch % 1000 == 0:\n        \n            model_f.eval()\n            preds_f_delta = train_p_f.values[:,:TRAIN_N]\n\n            for idx, data in enumerate(test_loader):\n                X1 = data['x_c'].to(device).float()\n                temp = model_f(X1, torch.Tensor(preds_f_delta[:,-1]).unsqueeze(0))['preds']\n                temp = np.exp(temp.detach().cpu().numpy().reshape(-1,1)) - 1\n                preds_f_delta = np.hstack((preds_f_delta, temp))\n\n            preds_f_delta = np.log(1 + preds_f_delta)\n#             val_len = train_p_c.values.shape[1] - TRAIN_N\n\n#             m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, TRAIN_N:TRAIN_N+val_len]).flatten(), \\\n#                                             preds_f_delta[:, TRAIN_N:TRAIN_N+val_len].flatten()))\n#             print(f\"{epoch:2} train_loss {avg_train_loss:<8.4f} val_loss {m2:8.5f} {time.time()-s:<2.2f}\")\n                \n            model_f.train()\n        \n    model_f.eval()\n    preds_f_delta = train_p_f.values[:,:TRAIN_N]\n    \n    for idx, data in enumerate(test_loader):\n        X1 = data['x_c'].to(device).float()\n        temp = model_f(X1, torch.Tensor(preds_f_delta[:,-1]).unsqueeze(0))['preds']\n        temp = np.exp(temp.detach().cpu().numpy().reshape(-1,1)) - 1\n        preds_f_delta = np.hstack((preds_f_delta, temp))\n    preds_f += preds_f_delta \/ bag_size\n\npreds_f_2 = np.log(1 + preds_f)\n\nprint(\"Done\")\n","98211f33":"preds_f_2.shape","aa23f9c7":"preds_f = np.mean([preds_f_1, preds_f_2], axis=0)","66a21623":"from sklearn.metrics import mean_squared_error\n\nif False:\n    val_len = train_p_c.values.shape[1] - TRAIN_N\n    \n    for i in range(val_len):\n        d = i + TRAIN_N\n        m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, d]), preds_c[:, d]))\n        m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, d]), preds_f[:, d]))\n        print(f\"{d}: {(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n\n    print()\n\n    m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, TRAIN_N:TRAIN_N+val_len]).flatten(), preds_c[:, TRAIN_N:TRAIN_N+val_len].flatten()))\n    m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, TRAIN_N:TRAIN_N+val_len]).flatten(), preds_f[:, TRAIN_N:TRAIN_N+val_len].flatten()))\n    print(f\"{(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")","77f5cf11":"import gc\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression, Ridge\n\ndef get_cpmp_sub(save_oof=False, save_public_test=False):\n    train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv')\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n#     train = train[train.day < 86]\n    train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train\n\n    test = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n    train = train[train.day < TRAIN_N]\n    \n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['geo'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['geo'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('geo')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n    num_geo = geo_data.shape[0]\n    geo_data\n\n    geo_id = {}\n    for i,g in enumerate(geo_data.index):\n        geo_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def get_dataset(start_pred, num_train, lag_period):\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        geo_ids = np.vstack([geo_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_geo_ids = geo_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_geo_ids, new_country_ids, new_days\n\n    def fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], geo_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alpha = 3\n        lr_death = Ridge(alpha=alpha, fit_intercept=False)\n        lr_case = Ridge(alpha=alpha, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) \/ 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) \/ 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    geo_ids_base = np.arange(num_geo).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    geo_ids_base = 0.1 * ohe.fit_transform(geo_ids_base)\n    geo_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 5\n    num_lag_case = 14\n    lag_period = max(start_lag_death, num_lag_case)\n\n    def get_oof(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n\n    if save_oof:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13']):\n            print(date, end=' ')\n            oof = get_oof(start_val_delta)\n            oof.to_csv('..\/submissions\/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 14\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    \n    return sub","26fcc1e7":"cpmp_sub = get_cpmp_sub()\ncpmp_sub['ForecastId'] = cpmp_sub['ForecastId'].astype('int')\ncpmp_preds = test_orig.merge(cpmp_sub, on='ForecastId')\ncpmp_p_c = cpmp_preds.pivot(index='Area', columns='days', values='ConfirmedCases').sort_index()\ncpmp_p_f = cpmp_preds.pivot(index='Area', columns='days', values='Fatalities').sort_index()\npreds_c_cpmp = np.log1p(cpmp_p_c.values)\npreds_f_cpmp = np.log1p(cpmp_p_f.values)\n\n'shape cpmp preds', preds_c_cpmp.shape, preds_f_cpmp.shape","409d2d6a":"\nfrom sklearn.metrics import mean_squared_error\n\nif False:\n    val_len = train_p_c.values.shape[1] - TRAIN_N\n    m1s = []\n    m2s = []\n    for i in range(val_len):\n        d = i + TRAIN_N\n        m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, d]), preds_c_cpmp[:, d-START_PUBLIC]))\n        m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, d]), preds_f_cpmp[:, d-START_PUBLIC]))\n        print(f\"{d}: {(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n        m1s += [m1]\n        m2s += [m2]\n    print()\n\n    \n    m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, TRAIN_N:TRAIN_N+val_len]).flatten(), preds_c_cpmp[:, TRAIN_N-START_PUBLIC:TRAIN_N-START_PUBLIC+val_len].flatten()))\n    m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, TRAIN_N:TRAIN_N+val_len]).flatten(), preds_f_cpmp[:, TRAIN_N-START_PUBLIC:TRAIN_N-START_PUBLIC+val_len].flatten()))\n    print(f\"{(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n\n","4f07fdb1":"import matplotlib.pyplot as plt\n\nfor _ in range(5):\n    plt.style.use(['default'])\n    fig = plt.figure(figsize = (15, 5))\n\n    idx = np.random.choice(N_AREAS)\n    print(AREAS[idx])\n\n    plt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='darkblue')\n    plt.plot(preds_c[idx], linestyle='--', color='darkblue', label = 'pdd pred cases')\n    plt.plot(np.pad(preds_c_cpmp[idx],(START_PUBLIC,0)),label = 'cpmp pred cases', linestyle='-.', color='darkblue')\n    plt.legend()\n    plt.show()","274c0528":"import matplotlib.pyplot as plt\n\nfor _ in range(5):\n    plt.style.use(['default'])\n    fig = plt.figure(figsize = (15, 5))\n\n    idx = np.random.choice(N_AREAS)\n    print(AREAS[idx])\n\n    plt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='darkblue')\n    plt.plot(preds_f[idx], linestyle='--', color='darkblue', label = 'pdd pred fatalities')\n    plt.plot(np.pad(preds_f_cpmp[idx],(START_PUBLIC,0)),label = 'cpmp pred fatalities', linestyle='-.', color='darkblue')\n    plt.legend()\n    plt.show()","12db1896":"import datetime as dt\n\nCOMP = '..\/input\/covid19-global-forecasting-week-4'\nDATEFORMAT = '%Y-%m-%d'\n\ndef get_comp_data(COMP):\n    train = pd.read_csv(f'{COMP}\/train.csv')\n    test = pd.read_csv(f'{COMP}\/test.csv')\n    submission = pd.read_csv(f'{COMP}\/submission.csv')\n    print(train.shape, test.shape, submission.shape)\n    \n    \n    \n    train['Country_Region'] = train['Country_Region'].str.replace(',', '').fillna('N\/A')\n    test['Country_Region'] = test['Country_Region'].str.replace(',', '').fillna('N\/A')\n\n    train['Location'] = train['Country_Region'].fillna('') + '-' + train['Province_State'].fillna('N\/A')\n\n    test['Location'] = test['Country_Region'].fillna('') + '-' + test['Province_State'].fillna('N\/A')\n\n    train['LogConfirmed'] = to_log(train.ConfirmedCases)\n    train['LogFatalities'] = to_log(train.Fatalities)\n    train = train.drop(columns=['Province_State'])\n    test = test.drop(columns=['Province_State'])\n\n    country_codes = pd.read_csv('..\/input\/covid19-metadata\/country_codes.csv', keep_default_na=False)\n    train = train.merge(country_codes, on='Country_Region', how='left')\n    test = test.merge(country_codes, on='Country_Region', how='left')\n    \n    train['continent'] = train['continent'].fillna('')\n    test['continent'] = test['continent'].fillna('')\n\n    train['DateTime'] = pd.to_datetime(train['Date'])\n    test['DateTime'] = pd.to_datetime(test['Date'])\n    \n    return train, test, submission\n\n\ndef process_each_location(df):\n    dfs = []\n    for loc, df in tqdm(df.groupby('Location')):\n        df = df.sort_values(by='Date')\n        df['Fatalities'] = df['Fatalities'].cummax()\n        df['ConfirmedCases'] = df['ConfirmedCases'].cummax()\n        df['LogFatalities'] = df['LogFatalities'].cummax()\n        df['LogConfirmed'] = df['LogConfirmed'].cummax()\n        df['LogConfirmedNextDay'] = df['LogConfirmed'].shift(-1)\n        df['ConfirmedNextDay'] = df['ConfirmedCases'].shift(-1)\n        df['DateNextDay'] = df['Date'].shift(-1)\n        df['LogFatalitiesNextDay'] = df['LogFatalities'].shift(-1)\n        df['FatalitiesNextDay'] = df['Fatalities'].shift(-1)\n        df['LogConfirmedDelta'] = df['LogConfirmedNextDay'] - df['LogConfirmed']\n        df['ConfirmedDelta'] = df['ConfirmedNextDay'] - df['ConfirmedCases']\n        df['LogFatalitiesDelta'] = df['LogFatalitiesNextDay'] - df['LogFatalities']\n        df['FatalitiesDelta'] = df['FatalitiesNextDay'] - df['Fatalities']\n        dfs.append(df)\n    return pd.concat(dfs)\n\n\ndef add_days(d, k):\n    return dt.datetime.strptime(d, DATEFORMAT) + dt.timedelta(days=k)\n\n\ndef to_log(x):\n    return np.log(x + 1)\n\n\ndef to_exp(x):\n    return np.exp(x) - 1\n\ndef get_beluga_sub():\n\n    \n    train, test, submission = get_comp_data(COMP)\n    train.shape, test.shape, submission.shape\n    \n    TRAIN_START = train.Date.min()\n    TEST_START = test.Date.min()\n    TRAIN_END = train.Date.max()\n    # TRAIN_END = \"2020-03-21\"\n    TEST_END = test.Date.max()\n    TRAIN_START, TRAIN_END, TEST_START, TEST_END\n    \n    train_clean = process_each_location(train)\n    \n    train_clean['Geo#Country#Contintent'] = train_clean.Location + '#' + train_clean.Country_Region + '#' + train_clean.continent\n    \n    DECAY = 0.93\n    DECAY ** 7, DECAY ** 14, DECAY ** 21, DECAY ** 28\n\n    confirmed_deltas = train.groupby(['Location', 'Country_Region', 'continent'])[[\n        'Id']].count().reset_index()\n\n    GLOBAL_DELTA = 0.11\n    confirmed_deltas['DELTA'] = GLOBAL_DELTA\n\n    confirmed_deltas.loc[confirmed_deltas.continent=='Africa', 'DELTA'] = 0.14\n    confirmed_deltas.loc[confirmed_deltas.continent=='Oceania', 'DELTA'] = 0.06\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Korea South', 'DELTA'] = 0.011\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='US', 'DELTA'] = 0.15\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='China', 'DELTA'] = 0.01\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Japan', 'DELTA'] = 0.05\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Singapore', 'DELTA'] = 0.05\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Taiwan*', 'DELTA'] = 0.05\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Switzerland', 'DELTA'] = 0.05\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Norway', 'DELTA'] = 0.05\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Iceland', 'DELTA'] = 0.05\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Austria', 'DELTA'] = 0.06\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Italy', 'DELTA'] = 0.04\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Spain', 'DELTA'] = 0.08\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Portugal', 'DELTA'] = 0.12\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Israel', 'DELTA'] = 0.12\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Iran', 'DELTA'] = 0.08\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Germany', 'DELTA'] = 0.07\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Malaysia', 'DELTA'] = 0.06\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Russia', 'DELTA'] = 0.18\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Ukraine', 'DELTA'] = 0.18\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Brazil', 'DELTA'] = 0.12\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Turkey', 'DELTA'] = 0.18\n    confirmed_deltas.loc[confirmed_deltas.Country_Region=='Philippines', 'DELTA'] = 0.18\n    confirmed_deltas.loc[confirmed_deltas.Location=='France-N\/A', 'DELTA'] = 0.1\n    confirmed_deltas.loc[confirmed_deltas.Location=='United Kingdom-N\/A', 'DELTA'] = 0.12\n    confirmed_deltas.loc[confirmed_deltas.Location=='Diamond Princess-N\/A', 'DELTA'] = 0.00\n    confirmed_deltas.loc[confirmed_deltas.Location=='China-Hong Kong', 'DELTA'] = 0.08\n    confirmed_deltas.loc[confirmed_deltas.Location=='San Marino-N\/A', 'DELTA'] = 0.03\n\n\n    confirmed_deltas.shape, confirmed_deltas.DELTA.mean()\n\n    confirmed_deltas[confirmed_deltas.DELTA != GLOBAL_DELTA].shape, confirmed_deltas[confirmed_deltas.DELTA != GLOBAL_DELTA].DELTA.mean()\n    confirmed_deltas[confirmed_deltas.DELTA != GLOBAL_DELTA]\n    confirmed_deltas.describe()\n    \n    daily_log_confirmed = train_clean.pivot('Location', 'Date', 'LogConfirmed').reset_index()\n    daily_log_confirmed = daily_log_confirmed.sort_values(TRAIN_END, ascending=False)\n    daily_log_confirmed.to_csv('daily_log_confirmed.csv', index=False)\n\n    for i, d in tqdm(enumerate(pd.date_range(add_days(TRAIN_END, 1), add_days(TEST_END, 1)))):\n        new_day = str(d).split(' ')[0]\n        last_day = dt.datetime.strptime(new_day, DATEFORMAT) - dt.timedelta(days=1)\n        last_day = last_day.strftime(DATEFORMAT)\n        for loc in confirmed_deltas.Location.values:\n            confirmed_delta = confirmed_deltas.loc[confirmed_deltas.Location == loc, 'DELTA'].values[0]\n            daily_log_confirmed.loc[daily_log_confirmed.Location == loc, new_day] = daily_log_confirmed.loc[daily_log_confirmed.Location == loc, last_day] + \\\n                confirmed_delta * DECAY ** i\n            \n    confirmed_prediciton = pd.melt(daily_log_confirmed, id_vars='Location')\n    confirmed_prediciton['ConfirmedCases'] = to_exp(confirmed_prediciton['value'])\n    \n    confirmed_prediciton_cases = confirmed_prediciton.copy()\n    \n    latest = train_clean[train_clean.Date == TRAIN_END][[\n    'Geo#Country#Contintent', 'ConfirmedCases', 'Fatalities', 'LogConfirmed', 'LogFatalities']]\n    daily_death_deltas = train_clean[train_clean.Date >= '2020-03-17'].pivot(\n        'Geo#Country#Contintent', 'Date', 'LogFatalitiesDelta').round(3).reset_index()\n    daily_death_deltas = latest.merge(daily_death_deltas, on='Geo#Country#Contintent')\n    \n    death_deltas = train.groupby(['Location', 'Country_Region', 'continent'])[[\n    'Id']].count().reset_index()\n\n    GLOBAL_DELTA = 0.11\n    death_deltas['DELTA'] = GLOBAL_DELTA\n\n    death_deltas.loc[death_deltas.Country_Region=='China', 'DELTA'] = 0.005\n    death_deltas.loc[death_deltas.continent=='Oceania', 'DELTA'] = 0.08\n    death_deltas.loc[death_deltas.Country_Region=='Korea South', 'DELTA'] = 0.04\n    death_deltas.loc[death_deltas.Country_Region=='Japan', 'DELTA'] = 0.04\n    death_deltas.loc[death_deltas.Country_Region=='Singapore', 'DELTA'] = 0.05\n    death_deltas.loc[death_deltas.Country_Region=='Taiwan*', 'DELTA'] = 0.06\n\n\n\n    death_deltas.loc[death_deltas.Country_Region=='US', 'DELTA'] = 0.17\n\n    death_deltas.loc[death_deltas.Country_Region=='Switzerland', 'DELTA'] = 0.15\n    death_deltas.loc[death_deltas.Country_Region=='Norway', 'DELTA'] = 0.15\n    death_deltas.loc[death_deltas.Country_Region=='Iceland', 'DELTA'] = 0.01\n    death_deltas.loc[death_deltas.Country_Region=='Austria', 'DELTA'] = 0.14\n    death_deltas.loc[death_deltas.Country_Region=='Italy', 'DELTA'] = 0.07\n    death_deltas.loc[death_deltas.Country_Region=='Spain', 'DELTA'] = 0.1\n    death_deltas.loc[death_deltas.Country_Region=='Portugal', 'DELTA'] = 0.13\n    death_deltas.loc[death_deltas.Country_Region=='Israel', 'DELTA'] = 0.16\n    death_deltas.loc[death_deltas.Country_Region=='Iran', 'DELTA'] = 0.06\n    death_deltas.loc[death_deltas.Country_Region=='Germany', 'DELTA'] = 0.14\n    death_deltas.loc[death_deltas.Country_Region=='Malaysia', 'DELTA'] = 0.14\n    death_deltas.loc[death_deltas.Country_Region=='Russia', 'DELTA'] = 0.2\n    death_deltas.loc[death_deltas.Country_Region=='Ukraine', 'DELTA'] = 0.2\n    death_deltas.loc[death_deltas.Country_Region=='Brazil', 'DELTA'] = 0.2\n    death_deltas.loc[death_deltas.Country_Region=='Turkey', 'DELTA'] = 0.22\n    death_deltas.loc[death_deltas.Country_Region=='Philippines', 'DELTA'] = 0.12\n    death_deltas.loc[death_deltas.Location=='France-N\/A', 'DELTA'] = 0.14\n    death_deltas.loc[death_deltas.Location=='United Kingdom-N\/A', 'DELTA'] = 0.14\n    death_deltas.loc[death_deltas.Location=='Diamond Princess-N\/A', 'DELTA'] = 0.00\n\n    death_deltas.loc[death_deltas.Location=='China-Hong Kong', 'DELTA'] = 0.01\n    death_deltas.loc[death_deltas.Location=='San Marino-N\/A', 'DELTA'] = 0.05\n\n\n    death_deltas.shape\n    death_deltas.DELTA.mean()\n\n    death_deltas[death_deltas.DELTA != GLOBAL_DELTA].shape\n    death_deltas[death_deltas.DELTA != GLOBAL_DELTA].DELTA.mean()\n    death_deltas[death_deltas.DELTA != GLOBAL_DELTA]\n    death_deltas.describe()\n    \n    daily_log_deaths = train_clean.pivot('Location', 'Date', 'LogFatalities').reset_index()\n    daily_log_deaths = daily_log_deaths.sort_values(TRAIN_END, ascending=False)\n    daily_log_deaths.to_csv('daily_log_deaths.csv', index=False)\n\n    for i, d in tqdm(enumerate(pd.date_range(add_days(TRAIN_END, 1), add_days(TEST_END, 1)))):\n        new_day = str(d).split(' ')[0]\n        last_day = dt.datetime.strptime(new_day, DATEFORMAT) - dt.timedelta(days=1)\n        last_day = last_day.strftime(DATEFORMAT)\n        for loc in death_deltas.Location:\n            death_delta = death_deltas.loc[death_deltas.Location == loc, 'DELTA'].values[0]\n            daily_log_deaths.loc[daily_log_deaths.Location == loc, new_day] = daily_log_deaths.loc[daily_log_deaths.Location == loc, last_day] + \\\n                death_delta * DECAY ** i#\n            \n    confirmed_prediciton = pd.melt(daily_log_deaths, id_vars='Location')\n    confirmed_prediciton['Fatalities'] = to_exp(confirmed_prediciton['value'])\n    \n    confirmed_prediciton_fatalities = confirmed_prediciton.copy()\n    \n    return confirmed_prediciton_cases, confirmed_prediciton_fatalities\n    \npreds_beluga_cases, preds_beluga_fatalities = get_beluga_sub()","b1d32628":"p_f_beluga = preds_beluga_fatalities.pivot(index='Location', columns='Date', values='Fatalities').sort_values(\"Location\")\np_c_beluga = preds_beluga_cases.pivot(index='Location', columns='Date', values='ConfirmedCases').sort_values(\"Location\")","29f05334":"locs = p_f_beluga.index.values","6fb31883":"p_f_beluga = np.log1p(p_f_beluga.values[:].copy())\np_c_beluga = np.log1p(p_c_beluga.values[:].copy())\np_f_beluga.shape, p_c_beluga.shape","61fc9134":"import copy\nimport warnings\nwarnings.filterwarnings('ignore')","60558e78":"FIRST_TEST = test_orig['Date'].apply(lambda x: x.dayofyear).min()","4bea7294":"# day_before_valid = FIRST_TEST-1 # 3-11 day  before of validation\n# day_before_public = FIRST_TEST-1 # 3-18 last day of train\n# day_before_private = df_traintest['day'][pd.isna(df_traintest['ForecastId'])].max() # last day of train","fd5695cf":"def do_aggregation(df, col, mean_range):\n    df_new = copy.deepcopy(df)\n    col_new = '{}_({}-{})'.format(col, mean_range[0], mean_range[1])\n    df_new[col_new] = 0\n    tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n    df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n    df_new[col_new][pd.isna(df_new[col_new])] = 0\n    return df_new[[col_new]].reset_index(drop=True)\n\ndef do_aggregations(df):\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [15,21]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [15,21]).reset_index(drop=True)], axis=1)\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}cases'.format(threshold)] = tmp\n\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n\n    # process China\/Hubei\n    if df['place_id'][0]=='China\/Hubei':\n        df['days_since_1cases'] += 35 # 2019\/12\/8\n        df['days_since_10cases'] += 35-13 # 2019\/12\/8-2020\/1\/2 assume 2019\/12\/8+13\n        df['days_since_100cases'] += 4 # 2020\/1\/18\n        df['days_since_1fatal'] += 13 # 2020\/1\/9\n    return df","c1cef702":"def feature_engineering_oscii():\n\n    # helper fucntions\n    \n    def fix_area(x):\n        try:\n            x_new = x['Country_Region'] + \"\/\" + x['Province_State']\n        except:\n            x_new = x['Country_Region']\n        return x_new\n    \n    def fix_area2(x):\n        try:\n            x_new = x['Country\/Region'] + \"\/\" + x['Province\/State']\n        except:\n            x_new = x['Country\/Region']\n        return x_new\n\n\n    \n    def encode_label(df, col, freq_limit=0):\n        df[col][pd.isna(df[col])] = 'nan'\n        tmp = df[col].value_counts()\n        cols = tmp.index.values\n        freq = tmp.values\n        num_cols = (freq>=freq_limit).sum()\n        print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n\n        col_new = '{}_le'.format(col)\n        df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n        for i, item in enumerate(cols[:num_cols]):\n            df_new[col_new][df[col]==item] = i\n\n        return df_new\n\n    def get_df_le(df, col_index, col_cat):\n        df_new = df[[col_index]]\n        for col in col_cat:\n            df_tmp = encode_label(df, col)\n            df_new = pd.concat([df_new, df_tmp], axis=1)\n        return df_new\n    \n    def to_float(x):\n        x_new = 0\n        try:\n            x_new = float(x.replace(\",\", \"\"))\n        except:\n            x_new = np.nan\n        return x_new\n    \n    df_train = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\n    df_test = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n    df_traintest = pd.concat([df_train, df_test])\n    \n    print('process_date')\n    df_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\n    df_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\n    day_min = df_traintest['day'].min()\n    df_traintest['days'] = df_traintest['day'] - day_min\n    \n    df_traintest.loc[df_traintest['Province_State'].isnull(), 'Province_State'] = 'N\/A'\n    df_traintest['place_id'] = df_traintest['Country_Region'] + '_' + df_traintest['Province_State']\n    \n    #df_traintest['place_id'] = df_traintest.apply(lambda x: fix_area(x), axis=1)\n    \n    print('add lat and long')\n    df_latlong = pd.read_csv(\"..\/input\/smokingstats\/df_Latlong.csv\")\n    \n    df_latlong.loc[df_latlong['Province\/State'].isnull(), 'Province_State'] = 'N\/A'\n    df_latlong['place_id'] = df_latlong['Country\/Region'] + '_' + df_latlong['Province\/State']\n    \n    # df_latlong['place_id'] = df_latlong.apply(lambda x: fix_area2(x), axis=1)\n    df_latlong = df_latlong[df_latlong['place_id'].duplicated()==False]\n    df_traintest = pd.merge(df_traintest, df_latlong[['place_id', 'Lat', 'Long']], on='place_id', how='left')\n    \n    places = np.sort(df_traintest['place_id'].unique())\n    \n    print('calc cases, fatalities per day')\n    df_traintest2 = copy.deepcopy(df_traintest)\n    df_traintest2['cases\/day'] = 0\n    df_traintest2['fatal\/day'] = 0\n    tmp_list = np.zeros(len(df_traintest2))\n    for place in places:\n        tmp = df_traintest2['ConfirmedCases'][df_traintest2['place_id']==place].values\n        tmp[1:] -= tmp[:-1]\n        df_traintest2['cases\/day'][df_traintest2['place_id']==place] = tmp\n        tmp = df_traintest2['Fatalities'][df_traintest2['place_id']==place].values\n        tmp[1:] -= tmp[:-1]\n        df_traintest2['fatal\/day'][df_traintest2['place_id']==place] = tmp\n\n    print('do agregation')\n    df_traintest3 = []\n    for place in places[:]:\n        df_tmp = df_traintest2[df_traintest2['place_id']==place].reset_index(drop=True)\n        df_tmp = do_aggregations(df_tmp)\n        df_traintest3.append(df_tmp)\n    df_traintest3 = pd.concat(df_traintest3).reset_index(drop=True)\n    \n    \n    print('add smoking')\n    df_smoking = pd.read_csv(\"..\/input\/smokingstats\/share-of-adults-who-smoke.csv\")\n    df_smoking_recent = df_smoking.sort_values('Year', ascending=False).reset_index(drop=True)\n    df_smoking_recent = df_smoking_recent[df_smoking_recent['Entity'].duplicated()==False]\n    df_smoking_recent['Country_Region'] = df_smoking_recent['Entity']\n    df_smoking_recent['SmokingRate'] = df_smoking_recent['Smoking prevalence, total (ages 15+) (% of adults)']\n    df_traintest4 = pd.merge(df_traintest3, df_smoking_recent[['Country_Region', 'SmokingRate']], on='Country_Region', how='left')\n    SmokingRate = df_smoking_recent['SmokingRate'][df_smoking_recent['Entity']=='World'].values[0]\n    # print(\"Smoking rate of the world: {:.6f}\".format(SmokingRate))\n    df_traintest4['SmokingRate'][pd.isna(df_traintest4['SmokingRate'])] = SmokingRate\n    \n    print('add data from World Economic Outlook Database')\n    # https:\/\/www.imf.org\/external\/pubs\/ft\/weo\/2017\/01\/weodata\/index.aspx\n    df_weo = pd.read_csv(\"..\/input\/smokingstats\/WEO.csv\")\n    subs  = df_weo['Subject Descriptor'].unique()[:-1]\n    df_weo_agg = df_weo[['Country']][df_weo['Country'].duplicated()==False].reset_index(drop=True)\n    for sub in subs[:]:\n        df_tmp = df_weo[['Country', '2019']][df_weo['Subject Descriptor']==sub].reset_index(drop=True)\n        df_tmp = df_tmp[df_tmp['Country'].duplicated()==False].reset_index(drop=True)\n        df_tmp.columns = ['Country', sub]\n        df_weo_agg = df_weo_agg.merge(df_tmp, on='Country', how='left')\n    df_weo_agg.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_weo_agg.columns]\n    df_weo_agg.columns\n    df_weo_agg['Country_Region'] = df_weo_agg['Country']\n    df_traintest5 = pd.merge(df_traintest4, df_weo_agg, on='Country_Region', how='left')\n    \n    print('add Life expectancy')\n    # Life expectancy at birth obtained from http:\/\/hdr.undp.org\/en\/data\n    df_life = pd.read_csv(\"..\/input\/smokingstats\/Life expectancy at birth.csv\")\n    tmp = df_life.iloc[:,1].values.tolist()\n    df_life = df_life[['Country', '2018']]\n    def func(x):\n        x_new = 0\n        try:\n            x_new = float(x.replace(\",\", \"\"))\n        except:\n    #         print(x)\n            x_new = np.nan\n        return x_new\n\n    df_life['2018'] = df_life['2018'].apply(lambda x: func(x))\n    df_life = df_life[['Country', '2018']]\n    df_life.columns = ['Country_Region', 'LifeExpectancy']\n    df_traintest6 = pd.merge(df_traintest5, df_life, on='Country_Region', how='left')\n    \n    print(\"add additional info from countryinfo dataset\")\n    df_country = pd.read_csv(\"..\/input\/countryinfo\/covid19countryinfo.csv\")\n    df_country['Country_Region'] = df_country['country']\n    df_country = df_country[df_country['country'].duplicated()==False]\n    df_traintest7 = pd.merge(df_traintest6, \n                             df_country.drop(['tests', 'testpop', 'country'], axis=1), \n                             on=['Country_Region',], how='left')\n    \n\n\n    df_traintest7['id'] = np.arange(len(df_traintest7))\n    df_le = get_df_le(df_traintest7, 'id', ['Country_Region', 'Province_State'])\n    df_traintest8 = pd.merge(df_traintest7, df_le, on='id', how='left')\n\n    \n    \n    print('covert object type to float')\n\n    cols = [\n        'Gross_domestic_product__constant_prices', \n        'Gross_domestic_product__current_prices', \n        'Gross_domestic_product__deflator', \n        'Gross_domestic_product_per_capita__constant_prices', \n        'Gross_domestic_product_per_capita__current_prices', \n        'Output_gap_in_percent_of_potential_GDP', \n        'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP', \n        'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP', \n        'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total', \n        'Implied_PPP_conversion_rate', 'Total_investment', \n        'Gross_national_savings', 'Inflation__average_consumer_prices', \n        'Inflation__end_of_period_consumer_prices', \n        'Six_month_London_interbank_offered_rate__LIBOR_', \n        'Volume_of_imports_of_goods_and_services', \n        'Volume_of_Imports_of_goods', \n        'Volume_of_exports_of_goods_and_services', \n        'Volume_of_exports_of_goods', 'Unemployment_rate', 'Employment', 'Population', \n        'General_government_revenue', 'General_government_total_expenditure', \n        'General_government_net_lending_borrowing', 'General_government_structural_balance', \n        'General_government_primary_net_lending_borrowing', 'General_government_net_debt', \n        'General_government_gross_debt', 'Gross_domestic_product_corresponding_to_fiscal_year__current_prices', \n        'Current_account_balance', 'pop'\n    ]\n    df_traintest8['cases\/day'] = df_traintest8['cases\/day'].astype(np.float)\n    df_traintest8['fatal\/day'] = df_traintest8['fatal\/day'].astype(np.float)   \n    for col in cols:\n        df_traintest8[col] = df_traintest8[col].apply(lambda x: to_float(x))  \n    # print(df_traintest8['pop'].dtype)\n    \n    return df_traintest8","deb83288":"df_traintest = feature_engineering_oscii()\ndf_traintest.shape","e7a35b07":"# day_before_valid = FIRST_TEST -1 # 3-11 day  before of validation\n# day_before_public = FIRST_TEST -1 # 3-18 last day of train\n# day_before_launch = 85 # 4-1 last day before launch","1e6e6585":"def calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score","7f6205fd":"# train model to predict fatalities\/day\n# params\nSEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }","c7d3507e":"# train model to predict fatalities\/day\n# features are selected manually based on valid score\ncol_target = 'fatal\/day'\ncol_var = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n#     'days_since_10cases', \n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal', 'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n#     'cases\/day_(8-14)',  \n#     'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n    'fatal\/day_(1-7)', \n    'fatal\/day_(8-14)', \n    'fatal\/day_(15-21)', \n    'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', 'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n    'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]\ncol_cat = []\ndf_train = df_traintest[(pd.isna(df_traintest['ForecastId'])) & (df_traintest['days']<TRAIN_N)]\ndf_valid = df_traintest[(pd.isna(df_traintest['ForecastId'])) & (df_traintest['days']<TRAIN_N)]\n# df_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 340 \nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration","9c1ebb5e":"# y_true = df_valid['fatal\/day'].values\n# y_pred = np.exp(model.predict(X_valid))-1\n# score = calc_score(y_true, y_pred)\n# print(\"{:.6f}\".format(score))","ee590471":"# train model to predict fatalities\/day\ncol_target2 = 'cases\/day'\ncol_var2 = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n    'days_since_10cases', #selected\n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal',\n#     'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n    'cases\/day_(8-14)',  \n    'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n#     'fatal\/day_(1-7)', \n#     'fatal\/day_(8-14)', \n#     'fatal\/day_(15-21)', \n#     'recov\/day_(1-1)', 'recov\/day_(1-7)', \n#     'recov\/day_(8-14)',  'recov\/day_(15-21)',\n#     'active_(1-1)', \n#     'active_(1-7)', \n#     'active_(8-14)',  'active_(15-21)', \n#     'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', \n#     'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n#     'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]","18adb02e":"# train model to predict cases\/day\ndf_train = df_traintest[(pd.isna(df_traintest['ForecastId'])) & (df_traintest['days']<TRAIN_N)]\ndf_valid = df_traintest[(pd.isna(df_traintest['ForecastId'])) & (df_traintest['days']<TRAIN_N)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration","8bca815a":"# y_true = df_valid['cases\/day'].values\n# y_pred = np.exp(model2.predict(X_valid))-1\n# score = calc_score(y_true, y_pred)\n# print(\"{:.6f}\".format(score))","0bea5ca3":"places = AREAS.copy()","a1c29ce9":"# remove overlap for public LB prediction\n\ndf_tmp = df_traintest[((df_traintest['days']<TRAIN_N)  & (pd.isna(df_traintest['ForecastId'])))  | ((TRAIN_N<=df_traintest['days']) & (pd.isna(df_traintest['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n    'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest9 = []\nfor i, place in tqdm(enumerate(places[:])):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\n#df_traintest9[df_traintest9['days']>TRAIN_N-2].head()","2d8e6d2f":"# predict test data in public\n# predict the cases and fatatilites one day at a time and use the predicts as next day's feature recursively.\ndf_preds = []\nfor i, place in tqdm(enumerate(places[:])):\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['days']<TRAIN_N).sum()\n    #len_unknown = (TRAIN_N<=df_interest['day']).sum()\n    len_unknown = 30\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model.predict(X_valid)\n        pred_c = model2.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n            'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds.append(df_interest)\ndf_preds = pd.concat(df_preds)","a01398f7":"df_preds.shape","cbc920f3":"# df_preds['cases\/day']","dc9165d5":"# df_preds['Area'] = df_preds['Country_Region'] + '_' + df_preds['Province_State']","9fc23ce1":"p_f_oscii = df_preds.pivot(index='place_id', columns='days', values='fatal_pred').sort_index()\np_c_oscii = df_preds.pivot(index='place_id', columns='days', values='cases_pred').sort_index()\np_f_oscii","34838161":"preds_f_oscii = np.log1p(p_f_oscii.values[:].copy())\npreds_c_oscii = np.log1p(p_c_oscii.values[:].copy())\npreds_f_oscii.shape, preds_c_oscii.shape","ca0004bc":"START_PUBLIC, TRAIN_N","d48011f1":"if False:\n    val_len = train_p_c.values.shape[1] - TRAIN_N\n    #val_len = 12\n    m1s = []\n    m2s = []\n    for i in range(val_len):\n        d = i + TRAIN_N\n        m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, d]), preds_c_oscii[:, d-START_PUBLIC]))\n        m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, d]), preds_f_oscii[:, d-START_PUBLIC]))\n        print(f\"{d}: {(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n        m1s += [m1]\n        m2s += [m2]\n    print()\n\n    \n    m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, TRAIN_N:TRAIN_N+val_len]).flatten(), preds_c_oscii[:, TRAIN_N-START_PUBLIC:TRAIN_N-START_PUBLIC+val_len].flatten()))\n    m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, TRAIN_N:TRAIN_N+val_len]).flatten(), preds_f_oscii[:, TRAIN_N-START_PUBLIC:TRAIN_N-START_PUBLIC+val_len].flatten()))\n    print(f\"{(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")","bb9801b5":"preds_c_oscii.shape, preds_c_cpmp.shape, preds_c.shape","f1fa0b3c":"# preds_c_blend = np.log1p(np.average([np.expm1(p_c_beluga[:,64:107]),np.expm1(preds_c_cpmp[:]),np.expm1(preds_c[:,64:107])],axis=0, weights=[2,1,2]))\n# preds_f_blend = np.log1p(np.average([np.expm1(p_f_beluga[:,64:107]),np.expm1(preds_f_cpmp[:]),np.expm1(preds_f[:,64:107])],axis=0, weights=[2,1,2]))","03641fc0":"\n\npreds_c_blend = np.log1p(np.average([np.expm1(preds_c_oscii[:,64:107]),np.expm1(preds_c_cpmp[:]),np.expm1(p_c_beluga[:,64:107]),np.expm1(preds_c[:,64:107])],axis=0, weights=[8,1,1,8]))\npreds_f_blend = np.log1p(np.average([np.expm1(preds_f_oscii[:,64:107]),np.expm1(preds_f_cpmp[:]),np.expm1(p_f_beluga[:,64:107]),np.expm1(preds_f[:,64:107])],axis=0, weights=[8,1,1,8]))","302908e0":"preds_c_cpmp.shape, preds_c[:,64:107].shape","6f8e7010":"val_len = 13\n#val_len = 12\nm1s = []\nm2s = []\nfor i in range(val_len):\n    d = i + 64\n    m1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, d]), preds_c_blend[:, i]))\n    m2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, d]), preds_f_blend[:, i]))\n    print(f\"{d}: {(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")\n    m1s += [m1]\n    m2s += [m2]\nprint()\n\n\nm1 = np.sqrt(mean_squared_error(np.log(1 + train_p_c_raw.values[:, 64:64+val_len]).flatten(), preds_c_blend[:, :val_len].flatten()))\nm2 = np.sqrt(mean_squared_error(np.log(1 + train_p_f_raw.values[:, 64:64+val_len]).flatten(), preds_f_blend[:, :val_len].flatten()))\nprint(f\"PUBLIC LB {(m1 + m2)\/2:8.5f} [{m1:8.5f} {m2:8.5f}]\")","1b280e09":"import matplotlib.pyplot as plt\n\nfor _ in range(5):\n    plt.style.use(['default'])\n    fig = plt.figure(figsize = (15, 5))\n\n    idx = np.random.choice(N_AREAS)\n    print(AREAS[idx], places[idx])\n\n    plt.plot(np.log(1+train_p_f.values[idx]), label=AREAS[idx], color='darkblue')\n    plt.plot(preds_f[idx], linestyle='--', color='darkblue', label = 'pdd fat ')\n    plt.plot(np.pad(preds_f_cpmp[idx],(START_PUBLIC,0)),label = 'cpmp fat ', linestyle='-.', color='darkblue')\n    plt.plot(np.pad(preds_f_oscii[idx],(0,0)),label = 'oscii fat ', linestyle='-.', color='red')\n    plt.plot(np.pad(p_f_beluga[idx],(0,0)),label = 'beluga fat ', linestyle='-.', color='orange')\n    plt.plot(np.pad(preds_f_blend[idx],(START_PUBLIC,0)),label = 'blend fat ', linestyle='-.', color='darkgreen')\n    plt.legend()\n    plt.show()","83dbd824":"import matplotlib.pyplot as plt\n\nfor _ in range(5):\n    plt.style.use(['default'])\n    fig = plt.figure(figsize = (15, 5))\n\n    idx = np.random.choice(N_AREAS)\n    print(AREAS[idx], places[idx])\n\n    plt.plot(np.log(1+train_p_c.values[idx]), label=AREAS[idx], color='darkblue')\n    plt.plot(preds_c[idx], linestyle='--', color='darkblue', label = 'pdd  cases')\n    plt.plot(np.pad(preds_c_cpmp[idx],(START_PUBLIC,0)),label = 'cpmp  cases', linestyle='-.', color='darkblue')\n    plt.plot(np.pad(preds_c_oscii[idx],(0,0)),label = 'oscii  cases', linestyle='-.', color='orange')\n    plt.plot(np.pad(p_c_beluga[idx],(0,0)),label = 'beluga fat ', linestyle='-.', color='red')\n    plt.plot(np.pad(preds_c_blend[idx],(START_PUBLIC,0)),label = 'blend  cases', linestyle='-.', color='darkgreen')\n    plt.legend()\n    plt.show()","4e23363b":"EU_COUNTRIES = ['Austria', 'Italy', 'Belgium', 'Latvia', 'Bulgaria', 'Lithuania', 'Croatia', 'Luxembourg', 'Cyprus', 'Malta', 'Czechia', \n                'Netherlands', 'Denmark', 'Poland', 'Estonia', 'Portugal', 'Finland', 'Romania', 'France', 'Slovakia', 'Germany', 'Slovenia', \n                'Greece', 'Spain', 'Hungary', 'Sweden', 'Ireland']\nEUROPE_OTHER = ['Albania', 'Andorra', 'Bosnia and Herzegovina', 'Liechtenstein', 'Monaco', 'Montenegro', 'North Macedonia',\n                'Norway', 'San Marino', 'Serbia', 'Switzerland', 'Turkey', 'United Kingdom']\nAFRICA = ['Algeria', 'Burkina Faso', 'Cameroon', 'Congo (Kinshasa)', \"Cote d'Ivoire\", 'Egypt', 'Ghana', 'Kenya', 'Madagascar',\n                'Morocco', 'Nigeria', 'Rwanda', 'Senegal', 'South Africa', 'Togo', 'Tunisia', 'Uganda', 'Zambia']\nNORTH_AMERICA = ['US', 'Canada', 'Mexico']\nSOUTH_AMERICA = ['Argentina', 'Bolivia', 'Brazil', 'Chile', 'Colombia', 'Ecuador', 'Paraguay', 'Peru', 'Uruguay', 'Venezuela']\nMIDDLE_EAST = ['Afghanistan', 'Bahrain', 'Iran', 'Iraq', 'Israel', 'Jordan', 'Kuwait', 'Lebanon', 'Oman', 'Qatar', 'Saudi Arabia', 'United Arab Emirates']\nASIA = ['Bangladesh', 'Brunei', 'Cambodia', 'India', 'Indonesia', 'Japan', 'Kazakhstan', 'Korea, South', 'Kyrgyzstan', 'Malaysia',\n                'Pakistan', 'Singapore', 'Sri Lanka', 'Taiwan*', 'Thailand', 'Uzbekistan', 'Vietnam']","9b609a53":"non_china_mask = np.array(['China' not in a for a in AREAS]).astype(bool)\nnon_china_mask.shape\n","dac28657":"preds_c2 = preds_c.copy()\npreds_f2 = preds_f.copy()\npreds_c2[non_china_mask,64:107] = preds_c_blend[non_china_mask]\npreds_f2[non_china_mask,64:107] = preds_f_blend[non_china_mask]","a7db1050":"import matplotlib.pyplot as plt\n\ndef plt1(ar, ar2, ax, col='darkblue', linew=0.2):\n    ax.plot(ar2, linestyle='--', linewidth=linew\/2, color=col)\n    ax.plot(np.log(1+ar), linewidth=linew, color=col)\n\nplt.style.use(['default'])\nfig, axs = plt.subplots(3, 2, figsize=(18, 15), sharey=True)\n\nX = train_p_c.values\n#X = train_p_f.values\n\nfor ar in range(X.shape[0]):\n    \n    temp = X[ar]\n    temp2 = preds_c2[ar]\n    if 'China' in AREAS[ar]:\n        plt1(temp, temp2, axs[0,0])\n    elif AREAS[ar].split('_')[0] in NORTH_AMERICA:\n        plt1(temp, temp2, axs[0,1])\n    elif AREAS[ar].split('_')[0] in EU_COUNTRIES + EUROPE_OTHER:\n        plt1(temp, temp2, axs[1,0])\n    elif AREAS[ar].split('_')[0] in SOUTH_AMERICA + AFRICA:\n        plt1(temp, temp2, axs[1,1])\n    elif AREAS[ar].split('_')[0] in MIDDLE_EAST + ASIA:\n        plt1(temp, temp2, axs[2,0])\n    else:\n        plt1(temp, temp2, axs[2,1])\n\nprint(\"Confirmed Cases\")\naxs[0,0].set_title('China')\naxs[0,1].set_title('North America')\naxs[1,0].set_title('Europe')\naxs[1,1].set_title('Africa + South America')\naxs[2,0].set_title('Asia + Middle East')\naxs[2,1].set_title('Other')\nplt.show()","8474820c":"import matplotlib.pyplot as plt\n\ndef plt1(ar, ar2, ax, col='darkblue', linew=0.2):\n    ax.plot(ar2, linestyle='--', linewidth=linew\/2, color=col)\n    ax.plot(np.log(1+ar), linewidth=linew, color=col)\n\nplt.style.use(['default'])\nfig, axs = plt.subplots(3, 2, figsize=(18, 15), sharey=True)\n\n#X = train_p_c.values\nX = train_p_f.values\n\nfor ar in range(X.shape[0]):\n    \n    temp = X[ar]\n    temp2 = preds_f2[ar]\n    if 'China' in AREAS[ar]:\n        plt1(temp, temp2, axs[0,0])\n    elif AREAS[ar].split('_')[0] in NORTH_AMERICA:\n        plt1(temp, temp2, axs[0,1])\n    elif AREAS[ar].split('_')[0] in EU_COUNTRIES + EUROPE_OTHER:\n        plt1(temp, temp2, axs[1,0])\n    elif AREAS[ar].split('_')[0] in SOUTH_AMERICA + AFRICA:\n        plt1(temp, temp2, axs[1,1])\n    elif AREAS[ar].split('_')[0] in MIDDLE_EAST + ASIA:\n        plt1(temp, temp2, axs[2,0])\n    else:\n        plt1(temp, temp2, axs[2,1])\n\nprint(\"Fatalities\")\naxs[0,0].set_title('China')\naxs[0,1].set_title('North America')\naxs[1,0].set_title('Europe')\naxs[1,1].set_title('Africa + South America')\naxs[2,0].set_title('Asia + Middle East')\naxs[2,1].set_title('Other')\nplt.show()","add36458":"preds_c.shape, preds_c_blend.shape\n","397c6855":"# preds_c2 = preds_c.copy()\n# preds_f2 = preds_f.copy()\n# preds_c2[:,64:107] = preds_c_blend\n# preds_f2[:,64:107] = preds_f_blend","a279fdb1":"\ntemp = pd.DataFrame(np.clip(np.exp(preds_c2) - 1, 0, None))\ntemp['Area'] = AREAS\ntemp = temp.melt(id_vars='Area', var_name='days', value_name=\"ConfirmedCases\")\n\ntest = test_orig.merge(temp, how='left', left_on=['Area', 'days'], right_on=['Area', 'days'])\n\ntemp = pd.DataFrame(np.clip(np.exp(preds_f2) - 1, 0, None))\ntemp['Area'] = AREAS\ntemp = temp.melt(id_vars='Area', var_name='days', value_name=\"Fatalities\")\n\ntest = test.merge(temp, how='left', left_on=['Area', 'days'], right_on=['Area', 'days'])\ntest.head()","a7e74cc6":"test.to_csv(\"submission.csv\", index=False, columns=[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"])","34138efa":"test.days.nunique()","b8db7faa":"for i, rec in test.groupby('Area').last().sort_values(\"ConfirmedCases\", ascending=False).iterrows():\n    print(f\"{rec['ConfirmedCases']:10.1f} {rec['Fatalities']:10.1f}  {rec['Country_Region']}, {rec['Province_State']}\")\n","e15ce997":"print(f\"{test.groupby('Area')['ConfirmedCases'].last().sum():10.1f}\")\nprint(f\"{test.groupby('Area')['Fatalities'].last().sum():10.1f}\")","56f90539":"test_p_c = test.pivot(index='Area', columns='days', values='ConfirmedCases').sort_index().values\ntest_p_f = test.pivot(index='Area', columns='days', values='Fatalities').sort_index().values\ndates = test.Date.dt.strftime('%d.%m.%Y').unique()","936f5f40":"print(\"Confirmed Cases\")\nfor i in [7,14,21,28,35,42]:\n    print(f'week{i\/\/7-1}  ', dates[i],  f'   {round(test_p_c[:,i].sum(),0):,}')","dcb3dc1f":"print(\"Fatalities\")\nfor i in [7,14,21,28,35,42]:\n    print(f'week{i\/\/7-1}  ', dates[i],  f'   {round(test_p_f[:,i].sum(),0):,}', )","4089b814":"## oscii model","13b179cd":"## BLEND","117e540b":"## beluga model","9c3160d9":"## cpmp model"}}