{"cell_type":{"e46e4942":"code","7aa58e7e":"code","67da76a6":"code","f242cc11":"code","2ac4f4a7":"code","cb85559b":"code","a2afc343":"code","49e380f6":"code","14607e0c":"code","2cd06e3c":"code","0d3a153a":"code","b65c047a":"code","13621b29":"code","d311fab3":"code","f959e3c9":"code","5c9a8bde":"code","4db0d069":"code","1bc352c6":"code","60264e00":"code","41f6784d":"code","30568310":"code","c10947a8":"code","eeef6fc7":"code","d97c776b":"code","8c32f3f8":"code","a383a0b8":"code","2474e74b":"code","9ce6db18":"code","7cd9a0f2":"code","38c2634f":"code","9a1ac6f0":"code","5a68b139":"code","51d8d2e3":"code","6ea496e5":"code","83083687":"code","5866def8":"code","4e04e2bd":"code","3330580d":"code","10203431":"code","2c46f78b":"code","1ed69783":"code","1d82f4ae":"code","9b160e9b":"code","03a5b166":"code","52e069b3":"markdown","62a2e5c3":"markdown","14b6b885":"markdown","5a64b304":"markdown","4bbce0cc":"markdown","44a578cc":"markdown","ab1cb64a":"markdown","9ac54e45":"markdown","33378c5f":"markdown"},"source":{"e46e4942":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7aa58e7e":"import tensorflow as tf\nfrom numpy import random\nimport matplotlib.pyplot as plt\nimport numpy as np\nrandom.seed(25)\ntf.random.set_seed(25)\nnp.random.seed(2)","67da76a6":"df = pd.read_csv(\"\/kaggle\/input\/minst-for-coursera\/MNIST_train_sample.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/minst-for-coursera\/MNIST_test_samplev2.csv\")","f242cc11":"df = df.drop(\"Unnamed: 0\", axis=1)","2ac4f4a7":"df.head()\n#28*28 = 784","cb85559b":"df[\"label\"].value_counts()","a2afc343":"import seaborn as sns\nsns.countplot(x=\"label\", data=df)","49e380f6":"# Drop 'label' column\nX = df.drop(labels = [\"label\"],axis = 1) \ny = df[\"label\"]","14607e0c":"X_test = test.drop(labels = [\"label\"],axis = 1) \ny_test = test[\"label\"]","2cd06e3c":"from keras.utils.np_utils import to_categorical\n\n# Gray Scale Normalization\nX = X \/ 255\nX_test = X_test\/ 255\n\n# one-hot-encoding\ny = to_categorical(y, 10)\ny_test = to_categorical(y_test, 10)","0d3a153a":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25,stratify=y)","b65c047a":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_dim=784))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","13621b29":"from tensorflow.keras.utils import plot_model\nplot_model(\n    model,\n    show_shapes=True,\n)","d311fab3":"history = model.fit(X_train, y_train,\n                    batch_size=128,\n                    epochs=10,\n                    verbose=1,\n                    validation_data=(X_val, y_val))","f959e3c9":"score = model.evaluate(X_test, y_test)\nprint(score[0])\nprint(score[1])","5c9a8bde":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","4db0d069":"df = pd.read_csv(\"\/kaggle\/input\/minst-for-coursera\/MNIST_train_sample.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/minst-for-coursera\/MNIST_test_samplev2.csv\")","1bc352c6":"df = df.drop(\"Unnamed: 0\", axis=1)","60264e00":"# Drop 'label' column\nX = df.drop(labels = [\"label\"],axis = 1) \ny = df[\"label\"]","41f6784d":"X.shape","30568310":"#If you need to turn it into a 28X28 image (for example if you create a convolutional neural net)\nX = X.values.reshape(-1,28,28)","c10947a8":"X.shape","eeef6fc7":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X[i])\nplt.show()","d97c776b":"#Before Normalization\nplt.figure()\nplt.imshow(X[0])\nplt.colorbar()\nplt.grid(False)\nplt.show()","8c32f3f8":"# Gray Scale Normalization\nX = X \/ 255.0","a383a0b8":"#After Normalization\nplt.figure()\nplt.imshow(X[0])\nplt.colorbar()\nplt.grid(False)\nplt.show()","2474e74b":"from keras.utils import to_categorical\n#We'll use a utility in Keras to turn Y_train into dummies:\n# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\ny = to_categorical(y, num_classes = 10)\nprint(y.shape)","9ce6db18":"# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\nX = X.reshape(-1,28,28,1)\nX.shape","7cd9a0f2":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","38c2634f":"from tensorflow.keras.utils import plot_model\nplot_model(\n    model,\n    show_shapes=True,\n)","9a1ac6f0":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25,stratify=y)","5a68b139":"from keras.optimizers import RMSprop\n\nfrom keras.callbacks import ReduceLROnPlateau\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n                    batch_size=128,\n                    epochs=11,\n                    verbose=1,\n                    validation_data=(X_val, y_val),\n                    callbacks=[learning_rate_reduction])","51d8d2e3":"# Drop 'label' column\nX_test = test.drop(labels = [\"label\"],axis = 1) \ny_test = test[\"label\"] \n#If you need to turn it into a 28X28 image (for example if you create a convolutional neural net)\nX_test = X_test.values.reshape(-1,28,28,1)\n# Gray Scale Normalization\nX_test = X_test \/ 255.0\n\ny_test = to_categorical(y_test, num_classes = 10)\nprint(y_test.shape)","6ea496e5":"score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","83083687":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","5866def8":"\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","4e04e2bd":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","3330580d":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","10203431":"# Fit the model\nhistory = model.fit_generator(datagen.flow(X_train,y_train, batch_size=128),\n                              epochs = 10, validation_data = (X_val,y_val),\n                              verbose = 1)","2c46f78b":"score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","1ed69783":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","1d82f4ae":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","9b160e9b":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","03a5b166":"# I was unable to implement this so need to look for more information regarding this","52e069b3":"# Convolutional neural network","62a2e5c3":"In order to make the optimizer converge faster and closest to the global minimum of the loss function, i used an annealing method of the learning rate (LR).\n\nThe LR is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima.\n\nIts better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.\n\nTo keep the advantage of the fast computation time with a high LR, i decreased the LR dynamically every X steps (epochs) depending if it is necessary (when accuracy is not improved).\n\nWith the ReduceLROnPlateau function from Keras.callbacks, i choose to reduce the LR by half if the accuracy is not improved after 3 epochs.","14b6b885":"# Neural Network","5a64b304":"# Error Analysis","4bbce0cc":"# **Data augmentation**\n\nImage data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n\nTraining deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n\nThe Keras deep learning neural network library provides the capability to fit models using image data augmentation via the ImageDataGenerator class.","44a578cc":"# Transer Learning \nTransfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.\n\nIt is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks given the vast compute and time resources required to develop neural network models on these problems and from the huge jumps in skill that they provide on related problems.","ab1cb64a":"For the data augmentation, I choose to :\n\n* Randomly rotate some training images by 10 degree\n* Randomly Zoom by 10% some training images\n* Randomly shift images horizontally by 10% of the width\n* Randomly shift images vertically by 10% of the height\n* I did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9.\n\nOnce our model is ready, we fit the training dataset .","9ac54e45":"As a result, this is much better than the normal multilayer neural network. Having said that, this loss plot illustrates that this might be overfitting after 8 epoch","33378c5f":"As you can see, this is underfitting. It is simply because our neural network is not so deep so I am going to make more sophisticated model bu using CNN"}}