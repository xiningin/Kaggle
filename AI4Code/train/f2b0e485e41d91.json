{"cell_type":{"f0c3b9bb":"code","80909c55":"code","00c25b14":"code","18ca9269":"code","f842f7de":"code","71fc32e2":"code","acec511f":"code","bf2f612f":"code","19f9e85f":"code","bb53065f":"code","6cfeeb7c":"code","2dfde1e1":"code","de1dc7e7":"code","b0c669f7":"code","17f97951":"code","779a89d9":"code","c290375d":"code","8960cc2f":"code","f30cf2a5":"code","2a406a0a":"code","00807ba1":"code","d2acf7f3":"code","411c3424":"code","b6038636":"code","32757866":"code","416afc78":"code","9419e6e2":"code","b84ff680":"code","311557c7":"code","c7dcb7a3":"code","cfdecd14":"code","1f4a1fcf":"markdown","f928469c":"markdown","b6e8ac79":"markdown","1a2b81dc":"markdown","8dfb53e7":"markdown","0f407370":"markdown","f350e394":"markdown","2efc7267":"markdown","c0341ba3":"markdown","4ae09239":"markdown","effdf382":"markdown","f1a45d34":"markdown","9c371f9b":"markdown","211e7227":"markdown","29836af2":"markdown","58f579a4":"markdown","d39d6b3f":"markdown","822e37a0":"markdown","d4e74703":"markdown","1e7b4637":"markdown","3a9747a3":"markdown","f63e948e":"markdown","a1f796b9":"markdown","0f5592d0":"markdown","ad5fec36":"markdown","9c1ab628":"markdown","fe6be495":"markdown","f3d4b4a9":"markdown","4cef8899":"markdown","8287f333":"markdown","8cfbd13e":"markdown","9976be13":"markdown","38e1fa4a":"markdown","fb5755e5":"markdown","b7f0e0a5":"markdown","db684d35":"markdown","25b1a528":"markdown","230a54de":"markdown","8e4adb82":"markdown","7abaf973":"markdown","2c03ff73":"markdown","697214cd":"markdown","37b0798f":"markdown","ac504bfc":"markdown","52cd263f":"markdown","3c4eb7a6":"markdown"},"source":{"f0c3b9bb":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)","80909c55":"from kaggle_datasets import KaggleDatasets\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path()\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [180, 180]\nEPOCHS = 500","00c25b14":"from sklearn.model_selection import train_test_split\n\nfilenames = tf.io.gfile.glob(str(GCS_PATH + '\/chest_xray\/train\/*\/*'))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH + '\/chest_xray\/val\/*\/*')))\ntest_filenames = tf.io.gfile.glob(str(GCS_PATH + '\/chest_xray\/test\/*\/*'))\n\ntrain_filenames, val_filenames = train_test_split(filenames, test_size=0.2)\n\ntrain_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\nval_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\ntest_list_ds = tf.data.Dataset.from_tensor_slices(test_filenames)","18ca9269":"COUNT_NORMAL = len([filename for filename in train_filenames if \"NORMAL\" in filename])\nprint(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n\nCOUNT_PNEUMONIA = len([filename for filename in train_filenames if \"PNEUMONIA\" in filename])\nprint(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))","f842f7de":"import os\n\ndef parse_image(filename):\n    parts = tf.strings.split(filename, os.sep)\n    label = parts[-2]\n    image = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(image, channels = 3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, [180, 180])\n    if label == 'PNEUMONIA':\n        label = 1\n    else:\n        label = 0\n    return image, label\n\ndef augment(image,label):\n    image = tf.image.random_brightness(image, max_delta=0.2)\n    image = tf.clip_by_value(image, 0, 1)\n    return image, label\n\n\ntrain_dataset = (\n    train_list_ds\n    .map(parse_image, num_parallel_calls=AUTOTUNE)\n    .map(augment, num_parallel_calls=AUTOTUNE)\n)\n\n\nval_dataset  = (\n    val_list_ds \n    .map(parse_image, num_parallel_calls=AUTOTUNE)\n)\n\ntest_dataset  = (\n    test_list_ds \n    .map(parse_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n)","71fc32e2":"def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n    # This is a small dataset, only load it once, and keep it in memory.\n    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n    # fit in memory.\n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n\n    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n\n    # Repeat forever\n    ds = ds.repeat()\n\n    ds = ds.batch(BATCH_SIZE)\n\n    # `prefetch` lets the dataset fetch batches in the background while the model\n    # is training.\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n\n    return ds","acec511f":"train_dataset = prepare_for_training(train_dataset)\nval_dataset = prepare_for_training(val_dataset)\n\nimage_batch, label_batch = next(iter(train_dataset))","bf2f612f":"import matplotlib.pyplot as plt\ndef show_batch(image_batch, label_batch):\n    plt.figure(figsize=(20,20))\n    for n in range(10):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        if label_batch[n]:\n            plt.title(\"PNEUMONIA\")\n        else:\n            plt.title(\"NORMAL\")\n        plt.axis(\"off\")","19f9e85f":"show_batch(image_batch.numpy(), label_batch.numpy())","bb53065f":"TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\nprint(\"Training images count: \" + str(TRAIN_IMG_COUNT))\n\nVAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\nprint(\"Validating images count: \" + str(VAL_IMG_COUNT))\n\nTEST_IMAGE_COUNT = tf.data.experimental.cardinality(test_list_ds).numpy()\nprint(\"Testing images count: \" + str(TEST_IMAGE_COUNT))","6cfeeb7c":"from keras.layers import Input, Conv2D, MaxPooling2D\nfrom keras.layers import Dense, Flatten, Dropout, BatchNormalization\nfrom keras.models import Model\n\ndef build_model():\n    input_img = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)) \n\n    conv1  = Conv2D(filters=16, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(input_img)\n    conv2  = Conv2D(filters=16, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv1)\n    pool1  = MaxPooling2D((2, 2),strides=(2,2))(conv2)\n\n    conv3  = Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\n    conv4  = Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv3)\n    norm1  = BatchNormalization()(conv4)\n    pool2  = MaxPooling2D((2, 2),strides=(2,2))(norm1)\n    drop1  = Dropout(0.3)(pool2)\n\n    conv5  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(drop1)\n    conv6  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv5)\n    norm2  = BatchNormalization()(conv6)\n    pool3  = MaxPooling2D((2, 2),strides=(2,2))(norm2)\n\n    conv7  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\n    conv8  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv7)\n    norm3  = BatchNormalization()(conv8)\n    pool4  = MaxPooling2D((2, 2),strides=(2,2))(norm3)\n    drop2  = Dropout(0.3)(pool4)\n\n    conv9  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(drop2)\n    conv10 = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv9)\n    norm4  = BatchNormalization()(conv10)\n    pool3  = MaxPooling2D((2, 2),strides=(2,2))(norm4)\n\n    flat   = Flatten()(pool3)\n    dense  = Dense(512, activation=\"relu\")(flat)\n    norm4  = BatchNormalization()(dense)\n    drop3  = Dropout(0.6)(norm4)\n    dense2 = Dense(128, activation=\"relu\")(drop3)\n    norm5  = BatchNormalization()(dense2)\n    drop4  = Dropout(0.4)(norm5)\n    dense3 = Dense(64, activation=\"relu\")(drop4)\n    norm6  = BatchNormalization()(dense3)\n    drop5  = Dropout(0.3)(norm6)\n    output = Dense(1, activation=\"sigmoid\")(drop5)\n\n    model  = Model(input_img,output)\n    return model","2dfde1e1":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    mode='min',\n    min_delta=0.001,\n    patience=200,\n    restore_best_weights=True,\n)","de1dc7e7":"def scheduler(epoch, lr):\n    if epoch < 100:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.05)\n    \nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)","b0c669f7":"weight_for_0 = (1 \/ COUNT_NORMAL)*(TRAIN_IMG_COUNT)\/2.0 \nweight_for_1 = (1 \/ COUNT_PNEUMONIA)*(TRAIN_IMG_COUNT)\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","17f97951":"with strategy.scope():\n    model = build_model()\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    model.compile(optimizer=opt, loss='binary_crossentropy',metrics=METRICS)\n    \nmodel.summary()\n\nmodel.fit(train_dataset,\n          epochs=EPOCHS,\n          steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n          callbacks=[early_stopping,lr_scheduler],\n          validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n          validation_data=val_dataset,\n          class_weight=class_weight,\n          verbose = 0)","779a89d9":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom mlxtend.plotting import plot_confusion_matrix\nimport numpy as np\n\npreds = model.predict(test_dataset)\npreds = np.around(preds)\n\n\ntrue_labels = []\n_ = test_dataset.unbatch()\n_ = _.take(TEST_IMAGE_COUNT)\n_ = list(_.as_numpy_iterator())\nfor i in range(len(_)):\n    true_labels.append(_[i][1])\n    \ncm = confusion_matrix(true_labels, preds)\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","c290375d":"tn, fp, fn, tp = cm.ravel()\nprecision = tp\/(tp+fp)\nrecall = tp\/(tp+fn)\n\nprint(\"Recall of the model is {:.2f}\".format(recall))\nprint(\"Precision of the model is {:.2f}\".format(precision))","8960cc2f":"from sklearn.metrics import roc_curve\nfpr,tpr,thresholds = roc_curve(true_labels,preds)\n\ndef plot_roc_curve(fpr,tpr,label=None):\n    plt.plot(fpr,tpr,linewidth=2,label=label)\n    plt.plot([0,1],[0,1],'k--')\n    plt.ylim((0,1))\n    plt.xlim((0,1))\n    plt.xlabel(\"False positive rate\")\n    plt.ylabel(\"True positive rate\")\n    \nplot_roc_curve(fpr,tpr)\nplt.show()","f30cf2a5":"from sklearn.metrics import roc_auc_score\nprint(\"AUC score of the model is {:.2f}\".format(roc_auc_score(true_labels,preds)))","2a406a0a":"from keras.models import Sequential\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.applications import VGG16\n\nwith strategy.scope():\n    vgg16_base_model = VGG16(input_shape=(180,180,3),include_top=False,weights='imagenet')\n\n    vgg16_model = tf.keras.Sequential([\n        vgg16_base_model,\n        GlobalAveragePooling2D(),\n        Dense(512, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.6),\n        Dense(128, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.4),\n        Dense(64,activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.3),\n        Dense(1,activation=\"sigmoid\")\n    ])\n\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    vgg16_model.compile(optimizer=opt,loss='binary_crossentropy',metrics=METRICS)\n    \nvgg16_model.fit(train_dataset,\n          epochs=EPOCHS,\n          steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n          callbacks=[early_stopping,lr_scheduler],\n          validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n          validation_data=val_dataset,\n          class_weight=class_weight,\n          verbose = 0)","00807ba1":"preds = vgg16_model.predict(test_dataset)\npreds = np.around(preds)\ntrue_labels = []\n_ = test_dataset.unbatch()\n_ = _.take(TEST_IMAGE_COUNT)\n_ = list(_.as_numpy_iterator())\nfor i in range(len(_)):\n    true_labels.append(_[i][1])\n\n    \ncm = confusion_matrix(true_labels, preds)\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","d2acf7f3":"tn, fp, fn, tp = cm.ravel()\nprecision = tp\/(tp+fp)\nrecall = tp\/(tp+fn)\n\nprint(\"Recall of the VGG16 model is {:.2f}\".format(recall))\nprint(\"Precision of the VGG 16 model is {:.2f}\".format(precision))\nprint(\"AUC score of the VGG 16 model is {:.2f}\".format(roc_auc_score(true_labels,preds)))\n\nfpr,tpr,thresholds = roc_curve(true_labels,preds)\nplot_roc_curve(fpr,tpr)\nplt.show()","411c3424":"from keras.applications import ResNet50\nwith strategy.scope():\n    resnet_base_model = ResNet50(input_shape=(180,180,3),include_top=False,weights='imagenet')\n\n    resnet_model = tf.keras.Sequential([\n        resnet_base_model,\n        GlobalAveragePooling2D(),\n        Dense(512, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.6),\n        Dense(128, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.4),\n        Dense(64,activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.3),\n        Dense(1,activation=\"sigmoid\")\n    ])\n\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    resnet_model.compile(optimizer=opt,loss='binary_crossentropy',metrics=METRICS)\n    \nresnet_model.fit(train_dataset,\n          epochs=EPOCHS,\n          steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n          callbacks=[early_stopping,lr_scheduler],\n          validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n          validation_data=val_dataset,\n          class_weight=class_weight,\n          verbose = 0)","b6038636":"preds = resnet_model.predict(test_dataset)\npreds = np.around(preds)\ntrue_labels = []\n_ = test_dataset.unbatch()\n_ = _.take(TEST_IMAGE_COUNT)\n_ = list(_.as_numpy_iterator())\nfor i in range(len(_)):\n    true_labels.append(_[i][1])\n\n    \ncm = confusion_matrix(true_labels, preds)\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","32757866":"tn, fp, fn, tp = cm.ravel()\nprecision = tp\/(tp+fp)\nrecall = tp\/(tp+fn)\n\nprint(\"Recall of the ResNet model is {:.2f}\".format(recall))\nprint(\"Precision of the ResNet model is {:.2f}\".format(precision))\nprint(\"AUC score of the ResNet model is {:.2f}\".format(roc_auc_score(true_labels,preds)))\n\nfpr,tpr,thresholds = roc_curve(true_labels,preds)\nplot_roc_curve(fpr,tpr)\nplt.show()","416afc78":"from keras.applications import InceptionV3\n\nwith strategy.scope():\n    inception_base_model = InceptionV3(input_shape=(180,180,3),include_top=False,weights='imagenet')\n\n    inception_model = tf.keras.Sequential([\n        inception_base_model,\n        GlobalAveragePooling2D(),\n        Dense(512, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.6),\n        Dense(128, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.4),\n        Dense(64,activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.3),\n        Dense(1,activation=\"sigmoid\")\n    ])\n\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    inception_model.compile(optimizer=opt,loss='binary_crossentropy',metrics=METRICS)\n\ninception_model.fit(train_dataset,\n          epochs=EPOCHS,\n          steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n          callbacks=[early_stopping,lr_scheduler],\n          validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n          validation_data=val_dataset,\n          class_weight=class_weight,\n          verbose = 0)","9419e6e2":"preds = inception_model.predict(test_dataset)\npreds = np.around(preds)\ntrue_labels = []\n_ = test_dataset.unbatch()\n_ = _.take(TEST_IMAGE_COUNT)\n_ = list(_.as_numpy_iterator())\nfor i in range(len(_)):\n    true_labels.append(_[i][1])\n\n    \ncm = confusion_matrix(true_labels, preds)\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","b84ff680":"tn, fp, fn, tp = cm.ravel()\nprecision = tp\/(tp+fp)\nrecall = tp\/(tp+fn)\n\nprint(\"Recall of the Inception model is {:.2f}\".format(recall))\nprint(\"Precision of the Inception model is {:.2f}\".format(precision))\nprint(\"AUC score of the Inception model is {:.2f}\".format(roc_auc_score(true_labels,preds)))\n\nfpr,tpr,thresholds = roc_curve(true_labels,preds)\nplot_roc_curve(fpr,tpr)\nplt.show()","311557c7":"from keras.applications import Xception\n\nwith strategy.scope():\n    xception_base_model = Xception(input_shape=(180,180,3),\n                                                   include_top=False,\n                                                   weights='imagenet')\n\n    xception_model = tf.keras.Sequential([\n        xception_base_model,\n        GlobalAveragePooling2D(),\n        Dense(512, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.6),\n        Dense(128, activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.4),\n        Dense(64,activation=\"relu\"),\n        BatchNormalization(),\n        Dropout(0.3),\n        Dense(1,activation=\"sigmoid\")\n    ])\n    \n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    METRICS = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall')\n    ]\n    xception_model.compile(optimizer=opt,loss='binary_crossentropy',metrics=METRICS)\n    \nxception_model.fit(train_dataset,\n          epochs=EPOCHS,\n          steps_per_epoch=TRAIN_IMG_COUNT \/\/ BATCH_SIZE,\n          callbacks=[early_stopping,lr_scheduler],\n          validation_steps=VAL_IMG_COUNT \/\/ BATCH_SIZE,\n          validation_data=val_dataset,\n          class_weight=class_weight,\n          verbose = 0)","c7dcb7a3":"preds = xception_model.predict(test_dataset)\npreds = np.around(preds)\ntrue_labels = []\n_ = test_dataset.unbatch()\n_ = _.take(TEST_IMAGE_COUNT)\n_ = list(_.as_numpy_iterator())\nfor i in range(len(_)):\n    true_labels.append(_[i][1])\n    \ncm = confusion_matrix(true_labels, preds)\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","cfdecd14":"tn, fp, fn, tp = cm.ravel()\nprecision = tp\/(tp+fp)\nrecall = tp\/(tp+fn)\n\nprint(\"Recall of the Xception model is {:.2f}\".format(recall))\nprint(\"Precision of the Xception model is {:.2f}\".format(precision))\nprint(\"AUC score of the Xception model is {:.2f}\".format(roc_auc_score(true_labels,preds)))\n\nfpr,tpr,thresholds = roc_curve(true_labels,preds)\nplot_roc_curve(fpr,tpr)\nplt.show()","1f4a1fcf":"![image.png](attachment:image.png)","f928469c":"See the [Research Paper on GoogleNet](https:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/pubs\/archive\/43022.pdf)","b6e8ac79":"We can use Computer Vision to determine whether a person is affected by pneumonia or not.","1a2b81dc":"# Pneumonia Detection with Convolutional Neural Networks","8dfb53e7":"The stride parameters determines the distance between each filters. The padding one determines if we ignore the borderline pixels or not (adding zeros helps the neural network to get information on the border)","0f407370":"We need to apply an EarlyStopping to prevent overfitting","f350e394":"The outputs are then concatened in Dense layers","2efc7267":"![ann.PNG](attachment:ann.PNG)","c0341ba3":"# Data Exploration & Data Preparation","4ae09239":"XceptionNet is a version of the GoogleNet architecture which uses depthwise separable convolutions instead of classic inception modules","effdf382":"See the full explanation and schemes in the [Research Paper on Deep Residual Learning](https:\/\/arxiv.org\/pdf\/1512.03385.pdf) ","f1a45d34":"We will need to use data augmentation to train an accurate model. It also help to prevent overfitting.","9c371f9b":"![image.png](attachment:image.png)","211e7227":"![image.png](attachment:image.png)","29836af2":"From [wikipedia:](https:\/\/en.wikipedia.org\/wiki\/Computer_vision)\n\nComputer vision is an interdisciplinary scientific field that deals with how computers can gain a high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.","58f579a4":"# Famous CNN architectures","d39d6b3f":"The dataset is composed of 5,863 lungs X-ray images splitted in 2 categories (Normal or Pneumonia)","822e37a0":"Also known as GoogleNet, this architecture presents sub-networks called inception modules, which allows fast training computing, complex patterns detection, and optimal use of parameters","d4e74703":"# InceptionNet","1e7b4637":"![image.png](attachment:image.png)","3a9747a3":"Since there are only 16 images in the validation folder, we regroup the two folders in one and shuffle the result with a 80\/20 distribution","f63e948e":"By using a sigmoid activation, the neural network determines which class the image belongs to","a1f796b9":"We make predictions on test data and get the confusion matrix, recall and precision of the model","0f5592d0":"# Computer Vision","ad5fec36":"Inspired from [@amyjang's work](https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays) and [@shivamb's notebook](https:\/\/www.kaggle.com\/shivamb\/cnn-architectures-vgg-resnet-inception-tl)","9c1ab628":"Features are the enhanced with MaxPool layers","fe6be495":"We generate infinite batched datasets","f3d4b4a9":"![image.png](attachment:image.png)","4cef8899":"# Launching TPU Acceleration and Defining Constant Variables","8287f333":"![image.png](attachment:image.png)","8cfbd13e":"This function keeps the initial learning rate for the first 100 epochs and decreases it exponentially after that. ","9976be13":"# XceptionNet","38e1fa4a":"# ResNet","fb5755e5":"We can also print the ROC curve and observe the AUC score","b7f0e0a5":"Features are then detected using the reLu activation on each destination pixel.","db684d35":"The data is imbalanced, therefore we will need to make use of the class_weight parameter later","25b1a528":"# VGG16","230a54de":"Let's see how famous CNN architectures perform on our dataset","8e4adb82":"The convolution process is illustrated below","7abaf973":"# What is Pneumonia ?\n\nFrom [Mayo Clinic's Article on pneumonia](https:\/\/www.mayoclinic.org\/diseases-conditions\/pneumonia\/symptoms-causes\/syc-20354204)\n\nPneumonia is an infection that inflames the air sacs in one or both lungs. The air sacs may fill with fluid or pus (purulent material), causing cough with phlegm or pus, fever, chills, and difficulty breathing. A variety of organisms, including bacteria, viruses and fungi, can cause pneumonia.\n\nPneumonia can range in seriousness from mild to life-threatening. It is most serious for infants and young children, people older than age 65, and people with health problems or weakened immune systems.","2c03ff73":"See the [Research Paper on XceptionNet](https:\/\/arxiv.org\/pdf\/1610.02357.pdf) and this nice article on [depthwise separable convolutions ](https:\/\/towardsdatascience.com\/a-basic-introduction-to-separable-convolutions-b99ec3102728)","697214cd":"Computer Vision can be realized using Convolutional neural networks (CNN)\nThey are neural networks making features extraction over an image before classifying it.\nThe feature extraction performed consists of three basic operations:\n\n* Filter an image for a particular feature (convolution)\n* Detect that feature within the filtered image (using the ReLU activation)\n* Condense the image to enhance the features (maximum pooling)\n","37b0798f":"![image.png](attachment:image.png)","ac504bfc":"Using convolution filters with different dimensions or values results in differents features extracted","52cd263f":"Presented in 2014, VGG16 has a very simple and classical architecture, with blocks of 2 or 3 convolutional layers followed by a pooling layer, plus a final dense network composed of 2 hidden layers (of 4096 nodes each) and one output layer (of 1000 nodes). Only 3x3 filters are used.","3c4eb7a6":"# Building a CNN model"}}