{"cell_type":{"93c67771":"code","3d885d5e":"code","0269dfec":"code","8bd5c21b":"code","fd21a221":"code","f3a57ef1":"code","896ff096":"code","d6c01a6e":"code","0e60ccba":"code","5657f823":"code","5cf9bb6c":"code","9bf7bbb5":"code","ea11ca0d":"code","3a0636a9":"code","1d236df3":"code","aa2afd57":"code","b0213abf":"code","e8f94d34":"code","f6c832f8":"code","ff1173ca":"code","7f563985":"code","bd3eeb89":"code","d32ce406":"code","d02cd71f":"code","9fdd2924":"code","cd7afcbd":"code","67dad981":"code","8bd6c68f":"code","084844c6":"code","2c4c029d":"markdown","b904d868":"markdown","32b7250a":"markdown","72b60ac4":"markdown","ca88ab33":"markdown","ac0c7bf2":"markdown","21148699":"markdown","c5e37ac0":"markdown","1c1ddfe4":"markdown","0505fcaf":"markdown","d225f9e2":"markdown","ad7a1c09":"markdown","955582b2":"markdown","13d3e4a6":"markdown","fb2a2ba2":"markdown","d933e057":"markdown","1e6e16db":"markdown","8b0724a4":"markdown","9557f440":"markdown","5afcc1d0":"markdown","f216d673":"markdown","5eea7cf0":"markdown","36db3e86":"markdown","1d8a11d8":"markdown","99679d4f":"markdown","5d97ff8d":"markdown","ab0b44d7":"markdown","89aaec37":"markdown","bc47f3a9":"markdown","ec7fdafd":"markdown","9a024b09":"markdown","1657f4d6":"markdown","d509ca99":"markdown"},"source":{"93c67771":"import os\n\nimport pandas as pd\nimport numpy as np\n\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n\nfrom tqdm.notebook import tqdm\n\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt","3d885d5e":"KAGGLE_FLAG = True\nSEED = 199\nNUM_FOLDS = 5","0269dfec":"train_input_path = ('..\/input\/30-days-of-ml\/train.csv' if KAGGLE_FLAG else '\/home\/rapela\/Downloads\/kaggle\/30_days\/data\/train.csv')\ntest_input_path = ('..\/input\/30-days-of-ml\/test.csv' if KAGGLE_FLAG else '\/home\/rapela\/Downloads\/kaggle\/30_days\/data\/test.csv')\nsubmission_input_path = ('..\/input\/30-days-of-ml\/sample_submission.csv' if KAGGLE_FLAG else '\/home\/rapela\/Downloads\/kaggle\/30_days\/data\/sample_submission.csv')","8bd5c21b":"train = pd.read_csv(train_input_path).drop(['id'], axis=1)\ntest = pd.read_csv(test_input_path).drop(['id'], axis=1)\nsubmission = pd.read_csv(submission_input_path)","fd21a221":"train.head()","f3a57ef1":"test.head()","896ff096":"submission.head()","d6c01a6e":"train.dtypes","0e60ccba":"test.dtypes","5657f823":"cat_cols = [x for x in test.columns if 'cat' in x]\nnum_cols = [x for x in test.columns if 'cont' in x]","5cf9bb6c":"cat_cols","9bf7bbb5":"num_cols","ea11ca0d":"train[num_cols].describe().T","3a0636a9":"test[num_cols].describe().T","1d236df3":"train.isna().sum()","aa2afd57":"test.isna().sum()","b0213abf":"# This plot is useful to understand the categorical values of our data! \n# for col in cat_cols:\n#     df = pd.DataFrame(train[col].value_counts(normalize=True)).reset_index()\n#     df.columns = ['category','percentage']\n#     print(df)\n#     df.plot(x ='category', y='percentage', kind = 'bar', sort_columns=True)","e8f94d34":"\nencoder = LabelEncoder()\n\nfor col in cat_cols:\n    \n    encoder.fit(pd.concat([train[col], test[col]], axis=0))\n    train[col] = encoder.transform(train[col])\n    test[col] = encoder.transform(test[col])","f6c832f8":"X =  train[num_cols+cat_cols]\nX_test = test[num_cols+cat_cols]\ny = train.target","ff1173ca":"lgb_params = {\n    \n    'learning_rate': 0.028752712882542178,\n\n    'n_estimators': 13460,\n\n    'num_leaves': 781,\n\n    'max_depth': 6,\n\n    'reg_alpha': 11.204053023177407,\n\n    'reg_lambda': 18.58812972996122,\n\n    'colsample_bytree': 0.12924180099983043,\n\n    'min_child_samples': 1063,\n\n    'max_bin': 503,\n\n    'min_data_per_group': 214,\n\n    'n_jobs': -1,\n\n    'random_state': SEED,\n    'bagging_seed': SEED,\n    'feature_fraction_seed': SEED,\n    \n    # 'boosting' : 'dart', \n    \n    \"objective\": \"regression\",\n    \n    \"metric\": \"rmse\",\n}","7f563985":"def cross_valid(model, train, target, test, num_folds=10, random_state=42):\n\n    train_oof = np.zeros((len(train)))\n    test_preds = 0\n    ret_models = []\n    \n    kf = KFold(n_splits=num_folds, random_state=SEED, shuffle=True)\n    scores=[]\n\n    for f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        \n        train_target, val_target = target[train_ind], target[val_ind]\n        \n        model.fit(train_df, \n        train_target)\n        \n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(test[columns])\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/num_folds\n        \n        scores.append(mean_squared_error(val_target, temp_oof, squared=False))\n\n        print(f'Fold {f}: {mean_squared_error(val_target, temp_oof, squared=False)}')\n        ret_models.append(model)\n    print(\"Mean of MSE: \", np.mean(scores))\n    \n    return train_oof, test_preds, np.mean(scores), ret_models","bd3eeb89":"%%time\ncolumns = X_test.columns\n\nclf = LGBMRegressor(**lgb_params)\n\ntrain_oof_1, test_preds_1, score_oof_1, models = cross_valid(clf, X, y, X_test, num_folds=NUM_FOLDS, random_state=SEED)","d32ce406":"np.save('train_oof_lgbm', train_oof_1)\nnp.save('test_preds_lgbm', test_preds_1)","d02cd71f":"print(f'MSE: {mean_squared_error(y, train_oof_1, squared=False)}')","9fdd2924":"submission['target'] = test_preds_1\nsubmission.to_csv('submission_output_lgbm.csv', index=False)\nprint(submission)","cd7afcbd":"lgbm.create_tree_digraph(models[0])","67dad981":"print(models[0].feature_name_)","8bd6c68f":"models[0].feature_importances_","084844c6":"lgbm.plot_importance(models[0])","2c4c029d":"Import useful libs :D","b904d868":"![LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/_images\/LightGBM_logo_black_text.svg)\n\n\n# In this notebook you will have a full pipeline to play and learn about LGBM. This is a work in progress... \n\nWhat do you will find here?\n\n* Encode & preprocessing if its necessary (Understand the data) \n* KFOLD\n* LGBM (CPU)\n\nFor more information about LGBM: https:\/\/lightgbm.readthedocs.io\/en\/latest\/index.html","32b7250a":"<a id='5'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">5. Final Notes <\/p>","72b60ac4":"Open train, test and submission files","ca88ab33":"Here we are going to filter the categorical features with LabelEncoder() from scikit-learn. We could also try another encoding type because we do not have many different values for each categorical column. I also decided to use the encoder .fit() on train & test data together to prevent an unrecognized value from the test.","ac0c7bf2":"The first \"Hello world\" of data science is to call the function .head() on the data. This function is default from pandas data frame, and its shows the first five rows of your data frame. So, let's do it for the train, test and submission files:","21148699":"To understand the types of each column, you can call the function dtypes from a pandas dataframe. In the train data we have our 'target' that will be used as a supervision for our machine learning model.","c5e37ac0":"## About the data\n\n> \"The dataset is used for this competition is synthetic (and generated using a CTGAN), but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features\".\n\nMore information: https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview\/description\n","1c1ddfe4":"Check if everything is okay!","0505fcaf":"I am defining the LGBM parameters. I will tune them in the future, but right now, they are from another private notebook from another competition. This part is important because you need to adjust it using your data. I like the Optuna hyperparameter tuner. It is easier to understand and has many good tutorials here on Kaggle.","d225f9e2":"Let's look some cool graphs! :)","ad7a1c09":"Check if everything is okay!","955582b2":"Make the submission using the preds","13d3e4a6":"Save the oof to use in the future in bleding or stacking algorithms, and also the preds to be possible to submit the ensemble.","fb2a2ba2":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\"> [30 Days of ML] Yet Another LGBM Notebook \ud83d\udd25 <\/p>","d933e057":"For cross validation I am using the KFold from scikit-learn with 5 folds defined by NUM_FOLDS on the beginning of this notebook.","1e6e16db":"We are going to get the categorical columns and the numerical columns to filter each part separately if necessary.","8b0724a4":"<a id='4'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">4. LGBM <\/p>","9557f440":"Thanks for the attention! Attention is all you need to learn and start playing with data in Kaggle! Just keep practicing and learning from other notebooks and build your own!\n\nHere we had a full LGBM pipeline with KFold. We used the LGBM with the scikit-learn API -> LGBMRegressor (.fit(), .predict()).\n\nIf you will copy the parameters, please give the credits! :)","5afcc1d0":"To analyze some statistics of the numerical data we can use the pandas dataframe .describe() function and transpose (.T) it to show each column of our data as a row. ","f216d673":"Define some parameters that will be useful","5eea7cf0":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">1. Import & Open the data <\/p>","36db3e86":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">3. Preprocessing the data <\/p>","1d8a11d8":"Open the data (here we can choose to open on kaggle or from another path e.g. you can download the data and run locally)","99679d4f":"We can see that the distribution of the train & test data are similar. :)","5d97ff8d":"Using the following functions .isna() and .sum() we can see if there are nan values per column in our data.","ab0b44d7":"Here, I just rename the variables to use on our model.","89aaec37":"References\n\n*  https:\/\/lightgbm.readthedocs.io\/en\/latest\/index.html\n*  https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.create_tree_digraph.html#lightgbm.create_tree_digraph","bc47f3a9":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Table of Content<\/p>\n\n* [1. Import & Open the data](#1)\n* [2. Understand the data](#2)\n* [3. Preprocessing the data](#3)\n* [4. LGBM](#4)\n* [5. Final Notes](#5)","ec7fdafd":"The mse from y and the train_oof is a good estimation about your mse, you can submit the preds and compare with this local mse.","9a024b09":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">2. Understand the data <\/p>","1657f4d6":"The first graph is the tree digraph that representes the path from root to each leaf of the tree and if you follow the path using one sample you can understantd the result of your model for that sample based on the features values. \"Each node in the graph represents a node in the tree. Non-leaf nodes have labels like cont11 < 0.357, which means \u201cthis node splits on the feature named \u201ccont11\u201d, with threshold 0.357\u201d.\"","d509ca99":"The categorical variables have the type object, and the numerical ones have the type float64, and the target is also a float64. We are dealing with a regression problem in which we need to provide a continuous value as the output of our model."}}