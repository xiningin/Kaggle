{"cell_type":{"932f78f8":"code","f08e0807":"code","e07ef595":"code","8a307c76":"code","bb844434":"code","27b4d0ea":"code","1ecd7794":"code","90456ebe":"code","acb8c51f":"code","ab614a40":"code","c69d6377":"code","826f729e":"code","cd64387b":"code","1a5852ce":"code","e1669fad":"code","e77bd71e":"code","347cc9ef":"code","2dd13d55":"code","c0a595f6":"code","f6834af7":"code","494a6ed2":"code","4b4ff7d2":"code","7556e54d":"code","db0f8ea5":"code","929cbd68":"code","473b9f17":"code","556b0c3b":"code","a7984169":"code","c1e70ef3":"code","2f74b534":"code","892f049f":"code","528ab408":"code","bea1940e":"code","4f86d482":"code","8de88a97":"code","95af6fe8":"code","accd1e34":"code","4dc5ca5e":"code","7f574a56":"code","b31dffa0":"code","81ddc5e5":"code","80b9587c":"code","e11502b5":"code","3b113476":"code","4551f36a":"code","20a54acf":"code","77061e26":"code","6638a1ac":"code","38605f22":"code","83fb6c55":"code","7a9aa856":"code","9a0bb7f8":"code","4766aad0":"code","2f8bc106":"code","1968a46c":"code","f35316f6":"code","b1ca7006":"code","51b60f7a":"code","a7da3a8a":"markdown","6eadb253":"markdown","f29774ce":"markdown","9a194346":"markdown","a8ee2fbf":"markdown","2b26ca6a":"markdown","0c23e3e0":"markdown","bc8fcb91":"markdown","6a7e1d52":"markdown","aa884a50":"markdown","b993160d":"markdown","4f9e760b":"markdown","5bf2763b":"markdown","38aff626":"markdown","345c070b":"markdown","2a9708a8":"markdown","0dc64cf3":"markdown","ce9931cb":"markdown","855fcfb7":"markdown","a4bf4793":"markdown","9041f62f":"markdown","630ce1d7":"markdown","3ac533c9":"markdown","1c05a604":"markdown","3c8c63aa":"markdown","ca0c7d83":"markdown","675d1445":"markdown","d3170029":"markdown","d95d7670":"markdown","e572075b":"markdown","9fb56b19":"markdown","d75ef205":"markdown","5b86a8dd":"markdown","caa257bf":"markdown","22fe787d":"markdown","4402ef1b":"markdown","0c5ea571":"markdown","80e3a1c9":"markdown","444769ab":"markdown","6eddd398":"markdown","2d7dc289":"markdown","df81bdeb":"markdown","ec69bc16":"markdown","4e2ec222":"markdown","eba91e0e":"markdown","e8087546":"markdown","bc8a1521":"markdown","bbe0a33b":"markdown","4be09811":"markdown","2f8da371":"markdown","6890b7fe":"markdown","1cd3559f":"markdown","e52697ba":"markdown","e3f227df":"markdown","07e80767":"markdown","9394d484":"markdown"},"source":{"932f78f8":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow_datasets as tfds\nimport pathlib\nimport os\nimport seaborn as sns\nimport random\nfrom PIL import Image\nimport cv2 as cv\nimport requests\nfrom sklearn.metrics import confusion_matrix","f08e0807":"training_data_path = pathlib.Path(r\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\")\nvalidatoin_data_path = pathlib.Path(r\"..\/input\/chest-xray-pneumonia\/chest_xray\/val\")\ntesting_data_path = pathlib.Path(r\"..\/input\/chest-xray-pneumonia\/chest_xray\/test\")\n","e07ef595":"# loading the data for exploration \n# This code will generate a DataFrame with 2 columns \n#one of then is the path of the image and the other is the label NORMAL or PNEUMONIA\n\nnormal_data = [img_path for img_path in os.listdir(training_data_path\/'NORMAL') if img_path[-5:] == '.jpeg']\nNORMAL = ['NORMAL']*len(normal_data)\nnormal_df = pd.DataFrame(\n    {'path': normal_data,\n     'label': NORMAL\n    })\n\n\npneumonia_data = [img_path for img_path in os.listdir(training_data_path\/'PNEUMONIA') if img_path[-5:] == '.jpeg']\nPNEUMONIA = ['PNEUMONIA']*len(pneumonia_data)\npneumonia_df = pd.DataFrame(\n      {'path': pneumonia_data,\n       'label': PNEUMONIA\n    })\n\npath_label_df = pd.concat([normal_df, pneumonia_df])\npath_label_df.index = np.arange(5216)","8a307c76":"f,ax=plt.subplots(1,2,figsize=(18,8))\npath_label_df['label'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('NORMAL Vs. PNEUMONIA')\nsns.countplot(x=path_label_df['label'])\nax[1].set_title('NORMAL Vs. PNEUMONIA')\nplt.show()","bb844434":"#Visualization of normal images\n#sampling different 4 normal images randomly \n\nrand_normal_imgs = random.sample(normal_data, 4)\n\nplt.figure(figsize=(7, 7))\nfor i in range(4):\n    norm_img = Image.open(training_data_path\/'NORMAL'\/rand_normal_imgs[i])\n    norm_img.resize(size=(180,180))\n    ax = plt.subplot(2, 2, i + 1)\n    plt.imshow(norm_img, cmap='gray')\n    label = 'NORMAL'\n    plt.title(\"NAORMAL\")\n    plt.axis(\"off\")","27b4d0ea":"#Visualization of pneumonia images\n#sampling different 4 pneumonia images randomly \n\nrand_pneumonia_imgs = random.sample(pneumonia_data, 4)\n\nplt.figure(figsize=(7, 7))\nfor i in range(4):\n    pneumonia_img = Image.open(training_data_path\/'PNEUMONIA'\/rand_pneumonia_imgs[i])\n    pneumonia_img.resize(size=(180,180))\n    ax = plt.subplot(2, 2, i + 1)\n    plt.imshow(pneumonia_img, cmap='gray')\n    label = 'PNEUMONIA'\n    plt.title(\"PNEUMONIA\")\n    plt.axis(\"off\")","1ecd7794":"img_height = 224\nimg_width = 224\n\n\n\ntraining_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255.0,\n                                                                   rotation_range=0.2,\n                                                                   zoom_range=0.2,\n                                                                   horizontal_flip=True,\n                                                                   vertical_flip=True)\n\ntraining_generator = training_data_gen.flow_from_directory(training_data_path,\n                                                          target_size=(img_height, img_width),\n                                                          batch_size=32  ,\n                                                          class_mode='binary')\n#------------------------------------------------------------------------------------------------------\nval_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255.0)\nvalidation_generator = val_data_gen.flow_from_directory(testing_data_path,\n                                                        target_size=(img_height, img_width),\n                                                        batch_size=8,\n                                                        class_mode='binary')\n#------------------------------------------------------------------------------------------------------\ntest_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255.0)\ntest_generator = test_data_gen.flow_from_directory(validatoin_data_path,\n                                                   target_size=(img_height, img_width),\n                                                   batch_size=16,\n                                                   class_mode='binary')\n#------------------------------------------------------------------------------------------------------\n","90456ebe":"baseline_model = tf.keras.models.Sequential()\n\nbaseline_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1),\n                                          activation='relu', input_shape=(224, 224, 3) ))\nbaseline_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n\nbaseline_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),\n                                          strides=(1, 1), activation='relu'))\nbaseline_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n\nbaseline_model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3),\n                                          strides=(1, 1), activation='relu'))\nbaseline_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n\nbaseline_model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),\n                                          strides=(1, 1), activation='relu'))\nbaseline_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))","acb8c51f":"baseline_model.summary()","ab614a40":"baseline_model.add(tf.keras.layers.Flatten())\n\nbaseline_model.add(tf.keras.layers.Dense(256, activation='relu',\n                                        kernel_initializer='he_normal'))\nbaseline_model.add(tf.keras.layers.BatchNormalization())\nbaseline_model.add(tf.keras.layers.Dropout(0.4))\n\nbaseline_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))","c69d6377":"baseline_model.summary()","826f729e":"def decayed_learning_rate(epoch):\n    return 0.01 * 0.1 ** (epoch \/ 20)\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(decayed_learning_rate)\n\n#------------------------------------------------------------------------------------------------------\n\nearly_stopting = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n                                                 restore_best_weights=True,\n                                                 patience=6)","cd64387b":"baseline_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n                       loss=tf.keras.losses.BinaryCrossentropy(),\n                       metrics=['accuracy'])\n\nbaseline_history = baseline_model.fit(training_generator,\n                                      steps_per_epoch=163,\n                                      epochs=10, \n                                      validation_data=validation_generator,\n                                      validation_steps=78, \n                                      callbacks=[lr_scheduler, early_stopting])","1a5852ce":"baseline_train_loss = baseline_history.history['loss']\nbaseline_val_loss = baseline_history.history['val_loss']\n\n\nplt.plot(baseline_history.epoch, baseline_train_loss, label='Training Loss')\nplt.plot(baseline_history.epoch, baseline_val_loss, label='Validation Loss')\nplt.grid(True)\nplt.legend()","e1669fad":"baseline_train_acc = baseline_history.history['accuracy']\nbaseline_val_acc = baseline_history.history['val_accuracy']\n\nplt.plot(baseline_history.epoch, baseline_train_acc, label='Training Accuracy')\nplt.plot(baseline_history.epoch, baseline_val_acc, label='Validation Accuracy')\nplt.grid(True)\nplt.legend()","e77bd71e":"baseline_testset_loss, baseline_testest_acc = baseline_model.evaluate(test_generator)\nprint(\"The test set loss: {}, Test set Accuracy: {}\".format(baseline_testset_loss, baseline_testest_acc))","347cc9ef":"print(\"And by now we have a baseline model to beat, The accuracy of the baseline model on the testset is {}\".format(baseline_testest_acc))","2dd13d55":"class ResidualUnit(tf.keras.layers.Layer):\n    \n    def __init__(self,filters, strides, activation, **kwargs):\n        super().__init__(**kwargs)\n        \n        self.activation = tf.keras.activations.get(activation)\n        \n        self.main_layers = [\n            tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, \n                                  padding='same', use_bias=False),\n            tf.keras.layers.BatchNormalization(),\n            self.activation,\n            tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=1, \n                                  padding='same', use_bias=False),\n            tf.keras.layers.BatchNormalization()]\n        \n        self.skip_con_layers = []\n        if strides > 1:\n            self.skip_con_layers = [\n                tf.keras.layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, \n                                       padding='same', use_bias=False),\n                tf.keras.layers.BatchNormalization()]\n        \n        \n        def call(self, inputs):\n            \n            z = inputs \n            for layer in self.main_layers:\n                z = layer(z)\n                \n            skip_z = inputs\n            for layer in self.skip_con_layers:\n                skip_z = layer(skip_z)\n            \n            return self.activation(z + skip_z)","c0a595f6":"simple_ResNet_model = tf.keras.models.Sequential()\n\n\nsimple_ResNet_model.add(tf.keras.layers.Conv2D(64, 7, strides=2, input_shape=(224,224,3),\n                                       use_bias=False))\nsimple_ResNet_model.add(tf.keras.layers.MaxPool2D(pool_size=3))\nsimple_ResNet_model.add(tf.keras.layers.BatchNormalization())\nsimple_ResNet_model.add(tf.keras.layers.Activation('relu'))\nsimple_ResNet_model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))\n\n\nsimple_ResNet_model.add(ResidualUnit(filters=64, strides=1, activation='relu'))\nsimple_ResNet_model.add(ResidualUnit(filters=64, strides=1, activation='relu'))\n\nsimple_ResNet_model.add(ResidualUnit(filters=128, strides=2, activation='relu'))\nsimple_ResNet_model.add(ResidualUnit(filters=128, strides=1, activation='relu'))\n\n\nsimple_ResNet_model.add(tf.keras.layers.Flatten())\nsimple_ResNet_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))","f6834af7":"tf.keras.utils.plot_model(simple_ResNet_model,\n                         show_shapes=True,\n                         show_dtype=True,\n                         show_layer_names=True,\n                         expand_nested=True)","494a6ed2":"simple_ResNet_model.summary()","4b4ff7d2":"from tensorflow.keras import backend as K\ndef F1_score(y_true, y_pred): \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","7556e54d":"METRICS = [\n    tf.keras.metrics.TruePositives(name='tp'),\n    tf.keras.metrics.FalsePositives(name='fp'),\n    tf.keras.metrics.TrueNegatives(name='tn'),\n    tf.keras.metrics.FalseNegatives(name='fn'), \n    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n    tf.keras.metrics.Precision(name='precision'),\n    tf.keras.metrics.Recall(name='recall'),\n    tf.keras.metrics.AUC(name='auc'),\n    tf.keras.metrics.AUC(name='prc', curve='PR'),\n    F1_score]","db0f8ea5":"initial_learning_rate = 0.001\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=750,\n    decay_rate=0.92,\n    staircase=True)","929cbd68":"simple_ResNet_model.compile(optimizer=tf.keras.optimizers.Adam(lr_schedule),\n                           loss=tf.keras.losses.BinaryCrossentropy(),\n                           metrics=METRICS)\n\nsimple_ResNet_history = simple_ResNet_model.fit(training_generator,\n                                                steps_per_epoch=163,\n                                                epochs=15, \n                                                validation_data=validation_generator,\n                                                validation_steps=78, \n                                                callbacks=[early_stopting])","473b9f17":"simple_ResNet_mode_results = simple_ResNet_model.evaluate(test_generator)\n\nfor name, value in zip(simple_ResNet_model.metrics_names, simple_ResNet_mode_results):\n    print(name, ': ', value)\nprint()  ","556b0c3b":"actual_positive = len(os.listdir(training_data_path\/'PNEUMONIA'))\nactual_negative = len(os.listdir(training_data_path\/'NORMAL'))\nsimple_ResNet_train_precision = simple_ResNet_history.history['precision'][0]\nsimple_ResNet_train_recall = simple_ResNet_history.history['recall'][0]\nsimple_ResNet_train_ROC_AUC = simple_ResNet_history.history['auc'][0]\nsimple_ResNet_train_prc = simple_ResNet_history.history['prc'][0]\nsimple_ResNet_train_F1_score = simple_ResNet_history.history['F1_score'][0]\n\nprint(\"The FalsePositives was {} out of {} that are truly negative that were calssified as positive.\".format(simple_ResNet_history.history['fp'][0], actual_negative))\nprint(\"so we have a precesion of {}\".format(simple_ResNet_train_precision))\nprint(\"\\n\")\n\nprint(\"The FalseNegatives was {} out of {} that are truly positive that were calssified as negative.\".format(simple_ResNet_history.history['fn'][0], actual_positive))\nprint(\"so we have a recall of {}\".format(simple_ResNet_train_recall))\nprint(\"\\n\")\n\nprint(\"The F1-score: {}\".format(simple_ResNet_train_F1_score))\nprint(\"\\n\")\n\nprint(\"The Area Under the Curve of a Receiver Operating Characteristic curve ROC-AUC: {}\".format(simple_ResNet_train_ROC_AUC))\nprint(\"And the Area Under the Curve of the Precision-Recall Curve prc: {}\".format(simple_ResNet_train_prc))\nprint(\"\\n\")\n","a7984169":"actual_positive = len(os.listdir(testing_data_path\/'PNEUMONIA'))\nactual_negative = len(os.listdir(testing_data_path\/'NORMAL'))\nsimple_ResNet_val_precision = simple_ResNet_history.history['val_precision'][0]\nsimple_ResNet_val_recall = simple_ResNet_history.history['val_recall'][0]\nsimple_ResNet_val_ROC_AUC = simple_ResNet_history.history['val_auc'][0]\nsimple_ResNet_val_prc = simple_ResNet_history.history['val_prc'][0]\nsimple_ResNet_val_F1_score = simple_ResNet_history.history['val_F1_score'][0]\n\nprint(\"The FalsePositives was {} out of {} that are truly negative that were calssified as positive.\".format(simple_ResNet_history.history['val_fp'][0], actual_negative))\nprint(\"so we have a precesion of {}\".format(simple_ResNet_val_precision))\nprint(\"\\n\")\n\nprint(\"The FalseNegatives was {} out of {} that are truly positive that were calssified as negative.\".format(simple_ResNet_history.history['val_fn'][0], actual_positive))\nprint(\"so we have a recall of {}\".format(simple_ResNet_val_recall))\nprint(\"\\n\")\n\nprint(\"The F1-score: {}\".format(simple_ResNet_val_F1_score))\nprint(\"\\n\")\n\nprint(\"The Area Under the Curve of a Receiver Operating Characteristic curve ROC-AUC: {}\".format(simple_ResNet_val_ROC_AUC))\nprint(\"And the Area Under the Curve of the Precision-Recall Curve prc: {}\".format(simple_ResNet_val_prc))\nprint(\"\\n\")","c1e70ef3":"base_model = tf.keras.applications.ResNet50V2(weights='imagenet', input_shape=(224,224,3), include_top=False)","2f74b534":"TL_model = tf.keras.models.Sequential()\n\n# step 1: Add your custom network on top of an already-trained base network\nTL_model.add(base_model)\nTL_model.add(tf.keras.layers.Flatten())\nTL_model.add(tf.keras.layers.Dense(256, activation = 'relu'))\nTL_model.add(tf.keras.layers.Dropout(0.3))\nTL_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n# step 2: Freeze the base netweork\nbase_model.trainable = False","892f049f":"# step 3: Train the part you added\n\nTL_model.compile(optimizer=tf.keras.optimizers.Adam(lr_schedule),\n                loss=tf.keras.losses.BinaryCrossentropy(),\n                metrics=METRICS)\n\nTL_model_history = TL_model.fit(training_generator,\n                                steps_per_epoch=163,\n                                epochs=12, \n                                validation_data=validation_generator,\n                                validation_steps=78)","528ab408":"baseline_results = TL_model.evaluate(test_generator)\n\nfor name, value in zip(TL_model.metrics_names, baseline_results):\n    print(name, ': ', value)\nprint()  ","bea1940e":"# step 4: Un freeze some layers in the base network\n\nbase_model.trainable = True \n\nset_trainable = False\n\nfor layer in base_model.layers:\n    if layer.name == 'conv5_block3_preact_bn':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else :\n        layer.trainable = False","4f86d482":"# step 5: Jointly train both these layers and the part you added\n\nTL_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-5),\n                loss=tf.keras.losses.BinaryCrossentropy(),\n                metrics=METRICS)\n\nTL_model_history = TL_model.fit(training_generator,\n                                steps_per_epoch=163,\n                                epochs=15, \n                                validation_data=validation_generator,\n                                validation_steps=78,\n                                callbacks=[early_stopting])","8de88a97":"baseline_results = TL_model.evaluate(test_generator)\n\nfor name, value in zip(TL_model.metrics_names, baseline_results):\n    print(name, ': ', value)\nprint()","95af6fe8":"actual_positive = len(os.listdir(training_data_path\/'PNEUMONIA'))\nactual_negative = len(os.listdir(training_data_path\/'NORMAL'))\nTL_model_train_precision = TL_model_history.history['precision'][0]\nTL_model_train_recall = TL_model_history.history['recall'][0]\nTL_model_train_ROC_AUC = TL_model_history.history['auc'][0]\nTL_model_train_prc = TL_model_history.history['prc'][0]\nTL_model_train_F1_score = TL_model_history.history['F1_score'][0]\n\nprint(\"The FalsePositives was {} out of {} that are truly negative that were calssified as positive.\".format(TL_model_history.history['fp'][0], actual_negative))\nprint(\"so we have a precesion of {}\".format(TL_model_train_precision))\nprint(\"\\n\")\n\nprint(\"The FalseNegatives was {} out of {} that are truly positive that were calssified as negative.\".format(TL_model_history.history['fn'][0], actual_positive))\nprint(\"so we have a recall of {}\".format(TL_model_train_recall))\nprint(\"\\n\")\n\nprint(\"The F1-score: {}\".format(TL_model_train_F1_score))\nprint(\"\\n\")\n\nprint(\"The Area Under the Curve of a Receiver Operating Characteristic curve ROC-AUC: {}\".format(TL_model_train_ROC_AUC))\nprint(\"And the Area Under the Curve of the Precision-Recall Curve prc: {}\".format(TL_model_train_prc))\nprint(\"\\n\")\n","accd1e34":"actual_positive = len(os.listdir(testing_data_path\/'PNEUMONIA'))\nactual_negative = len(os.listdir(testing_data_path\/'NORMAL'))\nTL_model_val_precision = TL_model_history.history['val_precision'][0]\nTL_model_val_recall = TL_model_history.history['val_recall'][0]\nTL_model_val_ROC_AUC = TL_model_history.history['val_auc'][0]\nTL_model_val_prc = TL_model_history.history['val_prc'][0]\nTL_model_val_F1_score = TL_model_history.history['val_F1_score'][0]\n\nprint(\"The FalsePositives was {} out of {} that are truly negative that were calssified as positive.\".format(TL_model_history.history['val_fp'][0], actual_negative))\nprint(\"so we have a precesion of {}\".format(TL_model_val_precision))\nprint(\"\\n\")\n\nprint(\"The FalseNegatives was {} out of {} that are truly positive that were calssified as negative.\".format(TL_model_history.history['val_fn'][0], actual_positive))\nprint(\"so we have a recall of {}\".format(TL_model_val_recall))\nprint(\"\\n\")\n\nprint(\"The F1-score: {}\".format(TL_model_val_F1_score))\nprint(\"\\n\")\n\nprint(\"The Area Under the Curve of a Receiver Operating Characteristic curve ROC-AUC: {}\".format(TL_model_val_ROC_AUC))\nprint(\"And the Area Under the Curve of the Precision-Recall Curve prc: {}\".format(TL_model_val_prc))\nprint(\"\\n\")","4dc5ca5e":"def plot_cm(labels, predictions, p=0.5):\n    cm = confusion_matrix(labels, predictions > p)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')","7f574a56":"predictions = TL_model.predict(test_generator)\nplot_cm(test_generator.labels, predictions, p=0.5)","b31dffa0":"predictions = TL_model.predict(validation_generator)\nplot_cm(validation_generator.labels, predictions, p=0.5)","81ddc5e5":"test_generator.class_indices","80b9587c":"BUFFER_SIZE = 256\n\n#reading the images from PNEUMONIA and NORMAL directories \n#training_data_path = pathlib.Path(r\"H:\\AI\\DataSets\\Chest X-Ray Images (Pneumonia)\\chest_xray\\chest_xray\\train\")\npos_training_data_path = training_data_path\/'PNEUMONIA'\nneg_training_data_path = training_data_path\/'NORMAL'\n\n#reading the images from validation_data_path directory \n#validatoin_data_path = pathlib.Path(r\"H:\\AI\\DataSets\\Chest X-Ray Images (Pneumonia)\\chest_xray\\chest_xray\\val\")\ntest_image_count = len(list(validatoin_data_path.glob(r'*\/*.jpeg')))\ntest_list_ds = tf.data.Dataset.list_files(str(validatoin_data_path\/'*\/*'), shuffle=False)\ntest_list_ds = test_list_ds.shuffle(test_image_count, reshuffle_each_iteration=False)\n\n\n#reading the images from testing_data_path directory \n#testing_data_path = pathlib.Path(r\"H:\\AI\\DataSets\\Chest X-Ray Images (Pneumonia)\\chest_xray\\chest_xray\\test\")\nval_image_count = len(list(testing_data_path.glob(r'*\/*.jpeg')))\nval_list_ds = tf.data.Dataset.list_files(str(testing_data_path\/'*\/*'), shuffle=False)\nval_list_ds = val_list_ds.shuffle(val_image_count, reshuffle_each_iteration=False)\n\n\n#reading the images from PNEUMONIA and NORMAL directories \ndef make_ds(path):\n    ds = tf.data.Dataset.list_files(str(path\/'*.jpeg'), shuffle=False)\n    ds = ds.shuffle(BUFFER_SIZE).repeat()\n    return ds\n\npos_ds = make_ds(pos_training_data_path)\nneg_ds = make_ds(neg_training_data_path)","e11502b5":"# converts a file path to an (img, label) pair\n\ndef process_path(file_path):\n    class_indices = {'NORMAL': 0, 'PNEUMONIA': 1}\n    img_height=224\n    img_width = 224\n    \n    parts = tf.strings.split(file_path, os.path.sep)\n    \n    if parts[-2] == 'NORMAL':\n        label = class_indices['NORMAL']\n    else:\n        label = class_indices['PNEUMONIA']\n    \n    \n    img = tf.io.read_file(file_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [img_height, img_width])\n\n    return img, label\n","3b113476":"pos_ds = pos_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\nneg_ds = neg_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)","4551f36a":"resampled_ds = tf.data.experimental.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\nresampled_ds = resampled_ds.batch(64).prefetch(2)","20a54acf":"neg, pos = len(os.listdir(training_data_path\/'NORMAL')), len(os.listdir(training_data_path\/'PNEUMONIA'))","77061e26":"resampled_steps_per_epoch = np.ceil(2.0*neg\/64)\nresampled_steps_per_epoch","6638a1ac":"#converts a file path to an (img, label) pair\n\ndef test_val_process_path(file_path):\n    \n    img_height, img_width = 224, 224\n    parts = tf.strings.split(file_path, os.path.sep)\n    one_hot = parts[-2] == class_names\n    label = tf.argmax(one_hot)\n    \n    img = tf.io.read_file(file_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [img_height, img_width])\n\n    return img, label","38605f22":"class_names = np.array(sorted([item.name for item in training_data_path.glob('*') if item.name !=\".DS_Store\"]))\nnum_classes = len([item for item in training_data_path.glob('*') if item.name !=\".DS_Store\"])\nprint(\"The number of classes is: {}\".format(num_classes))","83fb6c55":"val_ds = val_list_ds.map(test_val_process_path, num_parallel_calls=tf.data.AUTOTUNE)\ntest_ds = test_list_ds.map(test_val_process_path, num_parallel_calls=tf.data.AUTOTUNE)","7a9aa856":"def configure_for_performance(ds):\n    ds = ds.cache()\n    ds = ds.shuffle(buffer_size=1024)\n    ds = ds.batch(64)\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds\n\nval_ds = configure_for_performance(val_ds)\ntest_ds = configure_for_performance(test_ds)","9a0bb7f8":"test_ds.take(1)","4766aad0":"resampled_model = tf.keras.models.Sequential()\n\nresampled_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1),\n                                          activation='relu', input_shape=(224, 224, 3) ))\nresampled_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n\nresampled_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),\n                                          strides=(1, 1), activation='relu'))\nresampled_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n\nresampled_model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3),\n                                          strides=(1, 1), activation='relu'))\nresampled_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n\nresampled_model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),\n                                          strides=(1, 1), activation='relu'))\nresampled_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\n\nresampled_model.add(tf.keras.layers.Flatten())\n\nresampled_model.add(tf.keras.layers.Dense(256, activation='relu',\n                                        kernel_initializer='he_normal'))\nresampled_model.add(tf.keras.layers.BatchNormalization())\nresampled_model.add(tf.keras.layers.Dropout(0.4))\n\nresampled_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))","2f8bc106":"resampled_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n                       loss=tf.keras.losses.BinaryCrossentropy(),\n                       metrics=METRICS)","1968a46c":"resampled_history = resampled_model.fit(resampled_ds, \n                                        epochs=20, \n                                        steps_per_epoch=resampled_steps_per_epoch,\n                                        validation_data=test_ds)","f35316f6":"resampled_train_acc = resampled_history.history['accuracy']\nresampled_val_acc = resampled_history.history['val_accuracy']\n\nplt.plot(resampled_history.epoch, resampled_train_acc, label='Training Accuracy')\nplt.plot(resampled_history.epoch, resampled_val_acc, label='Validation Accuracy')\nplt.grid(True)\nplt.legend()","b1ca7006":"resampled_train_acc = resampled_history.history['F1_score']\nresampled_val_acc = resampled_history.history['val_F1_score']\n\nplt.plot(resampled_history.epoch, resampled_train_acc, label='Training F1_score')\nplt.plot(resampled_history.epoch, resampled_val_acc, label='Validation F1_score')\nplt.grid(True)\nplt.legend()","51b60f7a":"resampled_train_acc = resampled_history.history['prc']\nresampled_val_acc = resampled_history.history['val_prc']\n\nplt.plot(resampled_history.epoch, resampled_train_acc, label='Training prc')\nplt.plot(resampled_history.epoch, resampled_val_acc, label='Validation prc')\nplt.grid(True)\nplt.legend()","a7da3a8a":"To use this dataset, you'll need the number of steps per epoch.\n\nThe definition of \"epoch\" in this case is less clear. \nSay it's the number of batches required to see each negative example once:\n\n**Source:** <a href=\"https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data#oversampling\">Tensorflow Oversampling\/<a>","6eadb253":"Here we are taking the convolutional base without the densly connected layer by setting `include_top`  to False\n\nbecause the representations learned by the convolutional base are more genaric and reusable, which will be usefule \nregardless tho computer vision problem at hand, while the representations learned by the classifier are \nmore specific to the classes which the model was trained. \n\n**source:** Deep Learning with Python by: Francois Chollet","f29774ce":"**Thank you for reading, I hope you enjoyed and benefited from it.** <br>\n \n**If you have any questions or notes please leave it in the commont section.** <br>\n\n**If you like this notebook please press upvote and thanks again.** <br>","9a194346":"The final layer I will use the softmax activations that used for multicalss classification, \nfor the  layer that preceding the final layer I will use 512 units to avoid information bottleneck.\n\n\nAfter the first two layers I will add `BatchNormalization` layer to reduce the danger of **Vanishing\/Exploding Gradient** \nproblems ","a8ee2fbf":"# 4- Implementing a simple ResNet CNN","2b26ca6a":"`.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\nThis will ensure the dataset does not become a bottleneck while training your model.\nIf your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n\n`.prefetch()` overlaps data preprocessing and model execution while training.\n\n        \n**Source:** <a href=\"https:\/\/www.tensorflow.org\/tutorials\/load_data\/images#configure_the_dataset_for_perfor\">Configure dataset for performance\/<a>","0c23e3e0":"**In this section we will do some basic EDA on the trainig data to explore and understand the data we have**","bc8fcb91":"Here we use a **very low learning rate** to limit the magnitude of the modifications we make to the representations \nof the layers that we are fine tuning.\n\n**source:** Deep Learning with Python by: Francois Chollet","6a7e1d52":"**define the convolutional base using a common pattern: a stack of `Conv2D` and `MaxPooling2D` layers.**\n\nThe CNN takes a tensor of shape (height, width, channels) 3D tensor (R, G, B) without givving it the batch size.\nW will give the input shape to your first layer (180, 180, 3) by passing the argument `input_shape`.\n\n**Source:** <a href=\"https:\/\/www.tensorflow.org\/tutorials\/images\/cnn\">Tensorflow Convolutional Neural Network (CNN)\/<a>","aa884a50":"**Fine Tuning**","b993160d":"**For Validation data** ","4f9e760b":"**For validation data**","5bf2763b":"## 1.1- what is the Pneumonia?\n\nPneumonia is a disease that causes the air sacs in the lungs to become inflamed. \nThese air sacs of the infected person may be filled with pus,\nwhich leads to coughing with phlegm and difficulty breathing, \nand this pneumonia may be caused by bacteria, viruses and fungi.Pneumonia is most dangerous for children,\npeople over 65 years of age and those with health problems\n\n**Symptoms** vary depending on the person's overall health, which often resemble those of a cold or flu,\nsome of which are: chest pain when breathing or coughing, fever, sweating, and chills,\nnausea and vomiting or diarrhea, and shortness of breath.\n\nSoure: https:\/\/www.mayoclinic.org\/diseases-conditions\/pneumonia\/symptoms-causes\/syc-20354204","38aff626":"**Content**\n\n\nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia\/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia\/Normal).\n\nChest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients\u2019 routine clinical care.\n\nFor the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.","345c070b":"**Compile and train the model**","2a9708a8":"<hr>","0dc64cf3":"The data is stored as a jped files and it's should be formated as a floating point numbers befor feeding them to the \nnetwork, so we should read the files, decoding then into RGB grids of pixels, convert them into floating point tensors, \nand rescaling them. The class `ImageDataGenerator` can automatically turn image files on disk into batches of floating point \nnumbers ready for training using python generator.\n\nWe can get more data and fight overfitting by using data augmantation, by appling random transformations to the data using \nthe `ImageDataGenerator` class also. \n\n\n`rotation_range`: randomly rotating the image. <br>\n`zoom_range`: randomly zooming inside pictures. <br>\n`horizontal_flip` , `vertical_flip`: randomly flipping half the half the images horizontally and vertically. <br>\n\n**source:** Deep Learning with python by: Francois Chollet","ce9931cb":"As we can see from the above images, we **can not differentiate** between an infected and uninfected person by our **naked eyes**, \nso the task here to build a good classifier that will help to detect the infected person, this model will help save lives.","855fcfb7":"#  1- Pneumonia Dataset","a4bf4793":"As we see that in the above figure we have an imbalanced dataset.\nSo we need different ways to measure the performance of the data. \nI will present some of the usefull tools in this situation where we have imbalanced dataset.","9041f62f":"# 6- evaluation","630ce1d7":"**Configure dataset for performance** <br>\n\nTo train a model with this dataset you will want the data:<br>\n\n- To be well shuffled.<br>\n- To be batched.<br>\n- Batches to be available as soon as possible.<br>\n\nThese features can be added using the tf.data API. For more details:\n\n**Source:** <a href=\"https:\/\/www.tensorflow.org\/tutorials\/load_data\/images#using_tfdata_for_finer_control\">Configure dataset for performance\/<a>","3ac533c9":"Resample the dataset by oversampling the minority class, you can balance the dataset manually\nby choosing the right number of random indices from the positive examples.\n\n\nIf you're using tf.data the easiest way to produce balanced examples is to start with a positive and a negative dataset,\nand merge them.\n\n**Source:** <a href=\"https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data#oversampling\">Tensorflow Oversampling\/<a>","1c05a604":"**We can use the confusion matrix to find other useful metrics:**<br>\n\n**Precision:** is the accuracy of the positive predictions, (out of the instances that classified as positive, how many of them are truly positive)<br>\n- Precision = TP \/ TP + FP<br>\n\n**Recall** (other names: sensitivity, true positive rate(TPR)): out of the instances that are actually positive how many of them are classified correctly as positive class.<br>\n- Recall = TP \/ TP + FN<br>\n\n\nIf we need a single value to compare two classifiers, we can use the **F1-score** which is the harmonic mean of precision and recall:<br>\n- F1 = 2 \/ ( 1\/precision + 1\/recall)<br>\n\nOther metrics also will be usefull **precision_recall_curve()**, **average_precision_score**, \n**Receiver Operating Characteristic (ROC) curve**, and **the area under the curve (AUC) of the roc_curve**","3c8c63aa":"<hr>\n","ca0c7d83":"positive: PNEUMONIA <br>\nnegative: NORMAL","675d1445":"We can get best results using transfer learning and thats the next section topic.\n\nBut for this kind of problem diagnoses of illnesses, **it's better to have as lowest as possible of FalseNegatives**,\nFalseNegatives is where the patient is actually have the disease (positive) but he diagnoses as negative (he don't have it)\nbecause if the patient is trully have the disease we have to diagnoses his correctly and to undergo treatment. \n\nbut in the other side it's okay ho have some FalsePositives where the patient is actually don't have the disease (negative) but he diagnoses as positive (he have it), because after more investigation the truth will come out. \n\nBut mental health is also important, if we have a high FalsePositives and we tell a healthy patient that \nhe has the disease we may harm his mental health for no reason. \n\n\nso we have to have a single metric that we can concentrate on which is the F1 score. \n","d3170029":"# 8- Thank you ","d95d7670":"As I mentioned this is an imabalanced data set and I explain in section 2 that we need to deal with this problem with \ndifferent tools. \n\nAs a metrics to measure I will use the following: `TruePositives`, `FalsePositives`, `TrueNegatives`, `FalseNegatives`,\n`BinaryAccuracy`, `Precision`, `Recall`, and `AUC` to measure the performance of the model.","e572075b":"# 3- creating a simple model as a baseline","9fb56b19":"**For training data:**","d75ef205":"<hr>","5b86a8dd":"## 1.2- Pneumonia Dataset","caa257bf":"**Add Dense layers on top**\n\nFinally we will feed the last output tensor from the the convolutional base (of shape (20, 20, 64)) into Dense layers\nthat will perform the classification. First we need to Flatten the 3D tensor to 1D a `Flatten` layer, then we will add \none or more dense layers on top of the flatten layer, we have 275 classes so the final dense layer \nwill have 275 units.\n\n**Source:** <a href=\"https:\/\/www.tensorflow.org\/tutorials\/images\/cnn\">Tensorflow Convolutional Neural Network (CNN)\/<a>","22fe787d":"You can find the full details about Imbalanced Dataset in my discussion at: \n\n**Source:** <a href=\"https:\/\/www.kaggle.com\/general\/253378\">Imbalanced Dataset Metrics in binary classification problem explained\/<a>","4402ef1b":"**steps for Fine-Tuning  a network:**\n<ol>\n<li>Add your custom network on top of an already-trained base network<\/li>\n<li>Freeze the base netweork<\/li>\n<li>Train the part you added<\/li>\n<li>Un freeze some layers in the base network<\/li>\n<li>Jointly train both these layers and the part you added<\/li>\n<\/ol>\n\n\n**Why we do the first 3 steps? why we just Un freeze some layers in the base network and directly Jointly train both these layers and the part you added?**\n\nfreezing the base netweork weights means preventing them from being updating during training, if we do not do this \nthe presentations that was previesly learned by the base netweorkwill modified during trainin. \n\nThe densw layers on the top that we added are randomly inialized and very large weight updated will be propagated through the \nnetwork destroying the presentations that was previesly learned.\n\n**source:** Deep Learning with Python by: Francois Chollet","0c5ea571":"# 7- (Trying) Oversampling (Building Input Pipeline using tf.data)","80e3a1c9":"# 5- Using pretrained models (Transfer learning (ResNet50V2))","444769ab":"Merge the two together using `experimental.sample_from_datasets`","6eddd398":"# 2- Basic Exploratory Data Analysis","2d7dc289":"**For the training data**","df81bdeb":"For a binary classification problem, sometimes the dataset is imbalanced, where we have for example a 99% of the data\nfrom a class (negative) and the reminding 1% from the other class (positive) we call this dataset an Imbalanced Dataset.\n\n\nf we use the accuracy metric in these kind of problem we will get an optimistic misleading results, if we use a classifier that always predict the negative class we will get a model with 99% accuracy!!!\n\n\n\nIn these kind of problems we use a **confusion matrix** and other related metrics to measure the performance so what is a confusion matrix?\n\n\nA confusion matrix is a 2 x 2 matrix (for binary classification) where the rows represent the actual classes and the columns represent the predicted classes.\n\nyou can see an example of confusion matrix at the link bellow:\nhttps:\/\/cdn-images-1.medium.com\/max\/950\/1*PPgItHcPSaskyjLMWFC-Kw.png\n\n- **True Positive (TP):** they are a positive class instances and correctly classified as positive.<br>\n- **True Negative (TN):** they are a negative class instances and correctly classified as negative.<br>\n- **False Positive (FP):** they are a negative class instances and incorrectly classified as positive.<br>\n- **False Negative (FN):** they are a positive class instances and incorrectly classified as negative.<br>","ec69bc16":"As you can see this method was unstable durng training for the validation data: <br>\n\n- The accuracy was between 0.6 and 0.9 <br>\n- The F1_score was between 0.7 and 0.9 <br>\n- The prc  was between 0.65 and 0.95  <br>\n\nWhile on the training set all three metrics was doing very well it's an indication of **overfitting** \nwe can solve this problem by **Image augmentation**, **regularization methods** and other \n**hyperparameters tuning**. \n\nI will stop here for this method of Oversampling and in the next notebook I will dig deeply to solve it. \n\n\nAlso next notebook I'll try the Class weights","4e2ec222":"---","eba91e0e":"**Source:** <a href=\"https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data\">Link to Souce of the METRICS code\/<a>","e8087546":"positive: PNEUMONIA <br>\nnegative: NORMAL","bc8a1521":"# 3- ImageDataGenerator","bbe0a33b":"**For test data:**","4be09811":"Residual Network or (RezNet) won the ILSVRC 2015 challenge bt Kaiming He et al. the idea behind the RezNet is using a skip \nconnections (shortcut connections), it works by adding the input signal the fed to the layer with the output of the layer, by these \nskip connections the signal can make its way across the network. The RezNet can be seen as a deep stack of Residual Units.\n\nThe Residual Unit composed of two Conv layers without pooling, BatchNormalization, Relu activation, 3 x 3 kernels, stride 1 and \n\"same\" padding. \n\n\n**source:** hands-on machine learning with scikit-learn, keras, and tensorflow: concepts, tools, and techniques to build intelligent systems\nby: Aur\u00e9lien G\u00e9ron\n\nby using the Residual Units we can train deeper network without suffering a problem of vanishing\/Exploding gradient.","2f8da371":"**Imbalanced DataSet**","6890b7fe":"**Train on the oversampled data**","1cd3559f":"<hr>\n","e52697ba":"There are **two ways to use a pretrained models**:\n\n- Feature Extraction <br>\n- Fine Tuning <br>\n\n**source:** Deep Learning with Python by: Francois Chollet\n\nI will used the Fine Tuning method. ","e3f227df":"Transfer learning is the process of taking the knowledge of pretrained model in different task or different domain \nto another domain or task. Task is defined by the input and the expected output (for example image classification \nand image detection are different tasks), while the domain (different data distributions but the same tasks),\nfor example when the task is image classification images taken fom the web and images by the user camera. \n\n**source:** Hands-On Computer Vision with TensorFlow 2 by: Benjamin Planche, Eliot Andres\n\nAs we know the first layers in the ConvNet learned local genaric features while higher layers extract more \nabstract concept, we can we can use this information and apply transfer learning.","07e80767":"**Source:** <a href=\"https:\/\/aakashgoel12.medium.com\/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\">Link to Souce of the F1_score code\/<a>","9394d484":"The output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). as we can see\nthe height and the width shrink as we go depper in the network, we can control the number of output channels \nby for each Conv2D by the first argument (e.g., filters=32)\n\n**Source:** <a href=\"https:\/\/www.tensorflow.org\/tutorials\/images\/cnn\">Tensorflow Convolutional Neural Network (CNN)\/<a>"}}