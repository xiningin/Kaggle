{"cell_type":{"32ffdc15":"code","df5ddace":"code","7c2e2f24":"code","74523ca1":"markdown","2964fffd":"markdown"},"source":{"32ffdc15":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ndef reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings            \n          \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n            \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return df, NAlist\n\ntrain = pd.read_csv('..\/input\/train_transaction.csv')\ntrain1, NAlist = reduce_mem_usage(train)","df5ddace":"train = pd.read_csv('..\/input\/train_transaction.csv')\n\nfor i in range(len(train)):\n    if not np.isnan(train.V314[i]):\n        if train.V314[i]!=train1.V314[i]:\n            print(i, train.V314[i], train1.V314[i])\n    if i > 1000:\n        break","7c2e2f24":"# By the way did you know that np.nan != np.nan?\nnp.nan == np.nan","74523ca1":"Whenever the size of dataset goes above 1.5GB there are some memory issues when working with Kaggle Kernels, particlarly when you want to fit everything in one kernel. In the public kernls of this competition I saw a very commonly used function `reduce_mem_usage()` to reduce memory usage introduced in [here](https:\/\/www.kaggle.com\/mjbahmani\/reducing-memory-size-for-ieee) that is basically using the function first introduced in [here](https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65). The same (or very similar function) was being used in Predicting Molecular Properties competition as well. \n\nAs cool as it seems to use this function, it is not the best idea to use a function blindly. First of all, this function automatically fills in your null values for you! that is not exactly what you asked for and is actually a big deal. Moreover, there are some hidden pitfalls in using that function as described in [here](https:\/\/www.kaggle.com\/c\/champs-scalar-coupling\/discussion\/96655#latest-566225). I think that is the reason why Pandas (with all of its genious developers) doesn't have this basic function built-in.\n\nThis kernel runs a simple check on one column to see if `reduce_mem_usage()` results in percision loss.","2964fffd":"## Turned out it does\n* Fill in your `NaN` values for you that is not what exactly what you asked for.\n* Result in percision loss in some columns\n* Probably result in other hidden bugs that are not easily detectable"}}