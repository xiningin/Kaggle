{"cell_type":{"1bacd3a4":"code","91af7831":"code","724f41d6":"code","5710775f":"code","196d8c9e":"code","565c4d40":"code","9f5235f1":"code","b7de80b5":"code","3db57e5a":"code","d77edaa5":"code","ee36402f":"code","a2e20ba6":"code","c49f4f58":"code","52c0b037":"code","d7b19ef4":"code","23742ef5":"code","4f334ddf":"code","b3cf658c":"code","d0b0c915":"code","f5167d84":"code","baf87299":"code","3a0a484c":"code","6ce7febd":"code","e0ab6522":"code","0a529cfa":"code","f18c567b":"code","34498b7a":"code","df194049":"code","67d26c84":"code","35e36539":"code","8304f974":"code","55a9b5bf":"code","f7a1d244":"code","500d88ec":"code","419bfde2":"code","0cd503d2":"code","e11faef8":"code","e712a625":"code","dbfc913e":"code","46ed9f34":"code","ab1b7058":"code","cfd27835":"code","4fa90b32":"code","67aac500":"markdown","389c6b10":"markdown","dc5e4451":"markdown"},"source":{"1bacd3a4":"!pip install fastai==0.7.0\n!pip install torchtext==0.2.3","91af7831":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","724f41d6":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","5710775f":"from fastai.imports import *\nfrom fastai.structured import *\n\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\n\nfrom sklearn import metrics","196d8c9e":"PATH = \"..\/input\/\"","565c4d40":"!ls {PATH}","9f5235f1":"df_raw = pd.read_csv(f'{PATH}train\/Train.csv', low_memory=False, parse_dates=[\"saledate\"])","b7de80b5":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","3db57e5a":"display_all(df_raw.tail().T)","d77edaa5":"display_all(df_raw.describe(include='all').T)","ee36402f":"df_raw.SalePrice = np.log(df_raw.SalePrice)","a2e20ba6":"add_datepart(df_raw, 'saledate')\ndf_raw.saleYear.head()","c49f4f58":"train_cats(df_raw)\n","52c0b037":"df_raw.UsageBand.cat.categories","d7b19ef4":"df_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)","23742ef5":"df_raw.UsageBand = df_raw.UsageBand.cat.codes","4f334ddf":"display_all(df_raw.isnull().sum().sort_index()\/len(df_raw))","b3cf658c":"os.makedirs('tmp', exist_ok=True)\ndf_raw.to_feather('tmp\/bulldozers-raw')","d0b0c915":"df_raw = pd.read_feather('tmp\/bulldozers-raw')","f5167d84":"df, y, nas = proc_df(df_raw, 'SalePrice')","baf87299":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(df, y)\nm.score(df,y)","3a0a484c":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\nn_valid=12000\nn_trn = len(df) - n_valid\nraw_train, raw_valid = split_vals(df_raw, n_trn)\nX_train,X_valid = split_vals(df, n_trn)\ny_train,y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","6ce7febd":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","e0ab6522":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","0a529cfa":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice', subset=30000, na_dict=nas)\nX_train, _ = split_vals(df_trn, 20000)\ny_train, _ = split_vals(y_trn, 20000)","f18c567b":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(X_train, y_train)\nprint_score(m)","34498b7a":"preds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0]","df194049":"preds.shape","67d26c84":"plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);","35e36539":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)","8304f974":"set_rf_samples(20000)\n","55a9b5bf":"m = RandomForestRegressor(n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","f7a1d244":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","500d88ec":"reset_rf_samples()","419bfde2":"def dectree_max_depth(tree):\n    children_left = tree.children_left\n    children_right = tree.children_right\n\n    def walk(node_id):\n        if (children_left[node_id] != children_right[node_id]):\n            left_max = 1 + walk(children_left[node_id])\n            right_max = 1 + walk(children_right[node_id])\n            return max(left_max, right_max)\n        else: # leaf\n            return 1\n    root_node_id = 0\n    return walk(root_node_id)","0cd503d2":"m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","e11faef8":"t=m.estimators_[0].tree_","e712a625":"dectree_max_depth(t)","dbfc913e":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","46ed9f34":"t=m.estimators_[0].tree_","ab1b7058":"dectree_max_depth(t)","cfd27835":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","4fa90b32":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","67aac500":"We can no longer submit to this competition - but we can at least see that we're getting similar results to the winners based on the dataset we have.","389c6b10":"For working on Fast.ai [**Introduction to Machine Learning for Coders**](http:\/\/course18.fast.ai\/ml) Lesson 1 and 2, based on the instructions given [here](https:\/\/forums.fast.ai\/t\/wiki-thread-lesson-1\/6825) we need to install older version of fastai-0.7.x. Refer actual notebook by [fastai](https:\/\/github.com\/fastai\/fastai\/blob\/master\/courses\/ml1\/lesson1-rf.ipynb). Kaggle comes with the latest version of Fastai i.e. v.1.x.\n\nIf you are stuck anywhere writing code directly from this course into the Kaggle Kernel or working on this [Lesson 1 - Kaggle kernel](https:\/\/www.kaggle.com\/miwojc\/fast-ai-machine-learning-lesson-1\/) You have missed out the below pip installations.\n\nNote (for beginners like me in Kaggle): Though it may look silly but please make sure you have **turned On** Internet on the right side of the kaggle kernel.\n\nPlease watch this [fastai video by Jeremy Howard](http:\/\/course18.fast.ai\/lessonsml1\/lesson1.html) and refer to the the extra ordinary [lesson notes](https:\/\/medium.com\/@hiromi_suenaga\/machine-learning-1-lesson-1-84a1dc2b5236) to get a clear understanding of how the below code works : ","dc5e4451":"**Tree Building Parameters**"}}