{"cell_type":{"e3d44cc4":"code","5d01838c":"code","1cae7fa8":"code","2677aee4":"code","98a3e691":"code","b2a8e4ce":"code","05333fab":"code","348d5573":"code","8957ed39":"code","381b4daf":"code","86c4c8a2":"code","67691afc":"code","dc249c0f":"code","6ae85ba1":"code","28363ac3":"code","e94a424b":"code","8d482b27":"code","ab7f1352":"code","a8c00721":"code","c686d04b":"code","1a77cd75":"code","c1797ce9":"code","e060184c":"code","13e8e560":"code","9602518c":"code","74d9911d":"code","c78c822b":"code","2146ea72":"code","17cbe97e":"code","ed2f163e":"code","1366cadc":"code","68a1b0ec":"code","802890f0":"code","a91cb206":"code","1784cf02":"code","c553c0a6":"code","16d999da":"code","ef8cf312":"code","47fd3023":"code","f9af5ad1":"code","025f453b":"code","184a1c58":"code","d45eb049":"code","46daab9c":"code","3c0f2f36":"code","08ceb2ba":"code","9ef3de88":"code","c99df8b9":"code","a3119753":"code","8a21be55":"code","31eda383":"code","05b8914a":"code","60c8b450":"code","1981767f":"code","621198e6":"code","62409dbc":"code","bf937425":"code","247be108":"code","384840b3":"markdown","508a48af":"markdown","cde421ee":"markdown","8487e36c":"markdown","47a2a4dc":"markdown","c26e4404":"markdown","4894a68e":"markdown","145ff25c":"markdown","4739ae2d":"markdown","32ac4ce9":"markdown","4e8b98f3":"markdown","c1ba8175":"markdown","1fd1c95f":"markdown","a47fdcb8":"markdown","89666437":"markdown","dc38edd8":"markdown","66abfae5":"markdown","f72286bb":"markdown","4f4efdc2":"markdown","2e843cda":"markdown","8193691a":"markdown","5ddae266":"markdown","6a09ee0f":"markdown","82b04428":"markdown","36a18063":"markdown"},"source":{"e3d44cc4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d01838c":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score, r2_score, mean_squared_error\nfrom math import sqrt\n\nimport numpy as np\nimport pandas as pd\nimport requests\nimport json\nimport csv\nimport io","1cae7fa8":"# Constants\nCASES_URL = 'https:\/\/usafactsstatic.blob.core.windows.net\/public\/data\/covid-19\/covid_confirmed_usafacts.csv'\nDEATHS_URL = 'https:\/\/usafactsstatic.blob.core.windows.net\/public\/data\/covid-19\/covid_deaths_usafacts.csv'\nPOPS_URL = 'https:\/\/usafactsstatic.blob.core.windows.net\/public\/data\/covid-19\/covid_county_population_usafacts.csv'","2677aee4":"# import tensorflow as tf\n\n# # update_data -> None\n# # updates the cases, deaths, and pops json files to have the most recent data from usafacts.org.\n# def update_data():\n    \n#     # Get Cases\n#     cases_data = {}\n#     cases_file_path = tf.keras.utils.get_file(\"cases.csv\", CASES_URL)\n#     cases_reader = csv.DictReader(cases_file_path)\n    \n#     for rows in cases_reader:\n#         idx = list(rows.keys())[0]\n#         key = rows[idx]\n#         cases_data[key] = rows\n#     with open('.\/cases.json', 'w', encoding='utf-8') as jsonf:\n#         jsonf.write(json.dumps(cases_data, indent=4))\n        \n#     # Get Deaths\n#     deaths_data = {}\n#     deaths_file_path = tf.keras.utils.get_file(\"deaths.csv\", DEATHS_URL)\n#     deaths_reader = csv.DictReader(deaths_file_path)\n    \n#     for rows in deaths_reader:\n#         idx = list(rows.keys())[0]\n#         key = rows[idx]\n#         deaths_data[key] = rows\n        \n#     with open('.\/deaths.json', 'w', encoding='utf-8') as jsonf:\n#         jsonf.write(json.dumps(deaths_data, indent=4))\n        \n#     # Get Populations\n#     pops_data = {}\n#     pops_file_path = tf.keras.utils.get_file(\"pops.csv\", POPS_URL)\n#     pops_reader = csv.DictReader(pops_file_path)\n    \n#     for rows in pops_reader:\n#         idx = list(rows.keys())[0]\n#         key = rows[idx]\n#         pops_data[key] = rows\n        \n#     with open('.\/pops.json', 'w', encoding='utf-8') as jsonf:\n#         jsonf.write(json.dumps(pops_data, indent=4))\n#     print(\"Finished downloading to json files\")\n# update_data()","98a3e691":"# get_dataframes -> (DataFrame, DataFrame, DataFrame)\n# reads the cases, deaths, and populations json files in and converts them to pandas dataframes.\ndef get_dataframes():\n    cases_df = ''\n    deaths_df = ''\n    pops_df = ''\n\n    with open('..\/input\/cases-deaths-and-population-json-datas\/cases.json') as file:\n        cases_data = json.load(file)\n        cases_df = pd.DataFrame(cases_data).transpose()\n    \n    with open('..\/input\/cases-deaths-and-population-json-datas\/deaths.json') as file:\n        deaths_data = json.load(file)\n        deaths_df = pd.DataFrame(deaths_data).transpose()\n        \n    with open('..\/input\/cases-deaths-and-population-json-datas\/pops.json') as file:\n        pops_data = json.load(file)\n        pops_df = pd.DataFrame(pops_data).transpose()\n    print(\"Finished opening json files\")\n        \n    return (cases_df, deaths_df, pops_df)\n        \n    \ncases_df, deaths_df, pops_df = get_dataframes()","b2a8e4ce":"# new_cases -> DataFrame\n# Calculates the number of new cases in a given day based on the previous day's value and replaces the corresponding\n# cell with that value.\ndef new_cases():\n    df = cases_df.copy()\n    ignored_columns = [0, 1, 2, 3, 4]\n    num_cols = len(df.columns)\n    \n    for index, row in df.iterrows():\n        for col_idx in reversed(range(num_cols)):\n            if col_idx not in ignored_columns:\n                curr_col = df.columns[col_idx]\n                last_col = df.columns[col_idx-1]\n                df.at[index, curr_col] = int(df.at[index, curr_col]) - int(df.at[index, last_col])\n    print(\"finished new_cases preprocessing\")            \n    return df\n\nnew_cases_df = new_cases()\nnew_cases_df","05333fab":"# new_deaths -> DataFrame\n# Calculates the number of new deaths in a given day based on the previous day's value and replaces the corresponding\n# cell with that value.\ndef new_deaths():\n    df = deaths_df.copy()\n    ignored_columns = [0, 1, 2, 3, 4]\n    num_cols = len(df.columns)\n    \n    for index, row in df.iterrows():\n        for col_idx in reversed(range(num_cols)):\n            if col_idx not in ignored_columns:\n                curr_col = df.columns[col_idx]\n                last_col = df.columns[col_idx-1]\n                df.at[index, curr_col] = int(df.at[index, curr_col]) - int(df.at[index, last_col])\n                \n    print(\"finished new_deaths preprocessing\")                        \n    return df\n\nnew_deaths_df = new_deaths()\nnew_deaths_df","348d5573":"# Creates a new dataframe with the following specifications (to be used in model training):\n# Columns: [CountyFIPS, Population, Total Cases, New Cases, Total Deaths, New Deaths, Cases in 7 Days]\n# Index: [Date]\ndef master_dataframe():\n    c_df = cases_df.copy().iloc[:,:-7]\n    d_df = deaths_df.copy().iloc[:,:-7]\n    n_c_df = new_cases_df.copy().iloc[:,:-7]\n    n_d_df = new_deaths_df.copy().iloc[:,:-7]\n    \n    dates = []\n    counties = []\n    populations = []\n    total_cases = []\n    new_cases = []\n    total_deaths = []\n    new_deaths = []\n    num_cases_in_seven_days = []\n    ignored_columns = [0, 1, 2, 3]\n    \n    row_index = 0\n    for index, row in c_df.iterrows():\n        countyFIPS = c_df.at[index, c_df.columns[0]]\n        if countyFIPS in pops_df['\u00ef\u00bb\u00bfcountyFIPS']:\n            population = pops_df.loc[pops_df['\u00ef\u00bb\u00bfcountyFIPS'] == countyFIPS, 'population'].iloc[0]\n        else:\n            population = 0\n        \n        for col_idx in range(len(c_df.columns)):\n            if col_idx not in ignored_columns:\n                date = cases_df.columns[col_idx]\n                dates.append(date)\n                counties.append(countyFIPS)\n                populations.append(population)\n                total_cases.append(c_df.iloc[int(row_index)][date])\n                new_cases.append(n_c_df.iloc[int(row_index)][date])\n                total_deaths.append(d_df.iloc[int(row_index)][date])\n                new_deaths.append(n_d_df.iloc[int(row_index)][date])\n                num_cases_in_seven_days.append(cases_df.iloc[int(row_index)][cases_df.columns[col_idx + 7]])\n        row_index += 1\n        \n    df = pd.DataFrame(list(zip(counties, populations, total_cases, new_cases, total_deaths, new_deaths, num_cases_in_seven_days)), \n                      index = dates, \n                      columns = ['CountyFIPS', 'Population', 'Total Cases', 'New Cases', 'Total Deaths', 'New Deaths', 'Cases in 7 Days'])\n    \n    print(\"finished rest of preprocessing\")            \n    return df\n    \nmaster_df = master_dataframe()\nmaster_df","8957ed39":"# cases_bar_graph_by_state_by_date -> None\n# Takes a state and a list of dates as input and creates a bar graph of \n# the number of cases per county in the given state for the given dates.\ndef cases_bar_graph_by_state_by_date(state, dates):\n    graphing_df = cases_df.copy()\n    graphing_df = graphing_df.loc[graphing_df['State'] == state]\n    for date in dates:\n        graphing_df[date] = graphing_df[date].astype(int)\n    \n    ax = graphing_df.plot.barh(x='County Name', y=dates, figsize=(15,8)).set(xlabel='Number of Cases')\n    \n# cases_bar_graph_by_state_by_date('VT', ['10\/20\/20'])","381b4daf":"# deaths_bar_graph_by_state_by_date -> None\n# Takes a state and a list of dates as input and creates a bar graph of \n# the number of deaths per county in the given state for the given dates.\ndef deaths_bar_graph_by_state_by_date(state, date):\n    graphing_df = deaths_df.copy()\n    graphing_df = graphing_df.loc[graphing_df['State'] == state]\n    graphing_df[date] = graphing_df[date].astype(int)\n    \n    ax = graphing_df.plot.barh(x='County Name', y=date, figsize=(15,8)).set(xlabel='Number of Deaths')\n    \n# deaths_bar_graph_by_state_by_date('ME', '10\/20\/20')","86c4c8a2":"# cases_by_state -> None\n# Takes a state and generates a line graph that shows the number of cases for each\n# county in that state over the entire date range.\ndef cases_by_state(state):\n    graphing_df = cases_df.copy()\n    graphing_df = graphing_df.set_index('County Name')\n    graphing_df = graphing_df.loc[graphing_df['State'] == state]\n    graphing_df = graphing_df.drop(columns=graphing_df.columns[[0,1,2]])\n    \n    for column in graphing_df:\n        graphing_df[column] = graphing_df[column].astype(int)\n        \n    graphing_df.transpose().plot(figsize=(15,8))\n    \n# cases_by_state('VT')","67691afc":"# deaths_by_state -> None\n# Takes a state and generates a line graph that shows the number of deaths for each\n# county in that state over the entire date range.\ndef deaths_by_state(state):\n    graphing_df = deaths_df.copy()\n    graphing_df = graphing_df.set_index('County Name')\n    graphing_df = graphing_df.loc[graphing_df['State'] == state]\n    graphing_df = graphing_df.drop(columns=graphing_df.columns[[0,1,2]])\n    \n    for column in graphing_df:\n        graphing_df[column] = graphing_df[column].astype(int)\n        \n    graphing_df.transpose().plot(figsize=(15,8))\n    \n# deaths_by_state('VT')","dc249c0f":"from __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual, Button, HBox, VBox\nimport ipywidgets as widgets\n\n@interact\ndef choseFewOptions(state = [\"VT\", \"WY\", \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"], \n                    date=widgets.DatePicker(value=pd.to_datetime('10\/31\/20'))\n                   ):\n    today = date.today()\n    if(date < today):\n        dateFormatted = (\"%s\/%s\/%s\" % (date.month,date.day,(str(date.year)[2:])))\n        print(\"Deaths by state (state used: \", state,\") by date (date used: \", dateFormatted, \")\")\n        deaths_bar_graph_by_state_by_date(state, dateFormatted)\n        print(\"Deaths by state (state used: \", state,\")\")\n        deaths_by_state(state)\n        print(\"Cases by state  (state used: \", state,\") by date (date used: \", dateFormatted, \")\")\n        cases_bar_graph_by_state_by_date(state, [dateFormatted])\n        print(\"Cases by state  (state used: \", state,\")\")\n        cases_by_state(state)\n    else:\n        print(\"We don't have data for this date yet\")","6ae85ba1":"# test_dataframe -> DataFrame\n# This is a modification of the master_dataframe function that creates the same dataframe\n# for the instances within a given state. This returned dataframe is used for testing\n# since the master_dataframe is very large and often crashes our Jupyter notebooks.\ndef test_dataframe(state):\n    c_df = cases_df.copy().iloc[:,:-7]\n    d_df = deaths_df.copy().iloc[:,:-7]\n    n_c_df = new_cases_df.copy().iloc[:,:-7]\n    n_d_df = new_deaths_df.copy().iloc[:,:-7]\n    \n    dates = []\n    counties = []\n    populations = []\n    total_cases = []\n    new_cases = []\n    total_deaths = []\n    new_deaths = []\n    num_cases_in_seven_days = []\n    ignored_columns = [0, 1, 2, 3]\n    \n    row_index = 0\n    for index, row in c_df.iterrows():\n        if row['State'] == state:\n            countyFIPS = c_df.at[index, c_df.columns[0]]\n            if countyFIPS in pops_df['\u00ef\u00bb\u00bfcountyFIPS']:\n                population = pops_df.loc[pops_df['\u00ef\u00bb\u00bfcountyFIPS'] == countyFIPS, 'population'].iloc[0]\n            else:\n                population = 0\n\n            for col_idx in range(len(c_df.columns)):\n                if col_idx not in ignored_columns:\n                    date = cases_df.columns[col_idx]\n                    dates.append(date)\n                    counties.append(countyFIPS)\n                    populations.append(population)\n                    total_cases.append(c_df.iloc[int(row_index)][date])\n                    new_cases.append(n_c_df.iloc[int(row_index)][date])\n                    total_deaths.append(d_df.iloc[int(row_index)][date])\n                    new_deaths.append(n_d_df.iloc[int(row_index)][date])\n                    num_cases_in_seven_days.append(cases_df.iloc[int(row_index)][cases_df.columns[col_idx + 7]])\n            row_index += 1\n        \n    df = pd.DataFrame(list(zip(counties, populations, total_cases, new_cases, total_deaths, new_deaths, num_cases_in_seven_days)), \n                      index = dates, \n                      columns = ['CountyFIPS', 'Population', 'Total Cases', 'New Cases', 'Total Deaths', 'New Deaths', 'Cases in 7 Days'])\n    print(\"Finished model training\")\n    return df\n    \ntest_df = test_dataframe('VT')\ntest_df","28363ac3":"# remove unnecessary column for Decision Tree (CountyFIPS)\ntest_df = test_df.drop('CountyFIPS', axis=1)\n\n# split dataframe into X and Y\ncols = test_df.shape[1]\nX = test_df.iloc[:,0:cols-1]\nY = test_df.iloc[:,cols-1:cols]\n\n# split X and Y into training and testing dataframes\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)","e94a424b":"# Create a decision tree object and fit it to our training data\ndtree = DecisionTreeRegressor(max_depth=20, min_samples_leaf=0.01, random_state=3)\ndtree.fit(X_train, Y_train)","8d482b27":"\n# Predict using our training and test data, and output the RMSE and R^2 values.\n# Expected - low RMSE, high R^2 percentage value.\npred_train_tree= dtree.predict(X_train)\nprint(np.sqrt(mean_squared_error(Y_train, pred_train_tree)))\nprint(r2_score(Y_train, pred_train_tree))\n\npred_test_tree= dtree.predict(X_test)\nprint(np.sqrt(mean_squared_error(Y_test, pred_test_tree))) \nprint(r2_score(Y_test, pred_test_tree))","ab7f1352":"# Using a grid search to tune our hyperparameters\ngrid_search_params = {\n    'max_depth': [1,3,4,5,6,8,10,20,30],\n    'min_samples_leaf': [.001, .01, .1, 1],\n    'random_state': [1,2,3,4,5],\n    'max_leaf_nodes': [5,10,15,20]\n}\n\ngridSearchCVForDecTreeReg = GridSearchCV(dtree, grid_search_params, cv = 5)\ngridSearchCVForDecTreeReg.fit(X,Y)\nprint(\"best parameters are \", gridSearchCVForDecTreeReg.best_params_)","a8c00721":"dtree_opt = DecisionTreeRegressor(max_depth=5, max_leaf_nodes=15, min_samples_leaf=0.001, random_state=4)\ndtree_opt.fit(X_train, Y_train)\npred_values = dtree_opt.predict(X_test)\n\n\npred_test_tree= dtree_opt.predict(X_test)\nprint(np.sqrt(mean_squared_error(Y_test, pred_test_tree))) \nprint(r2_score(Y_test, pred_test_tree))\n# print(pred_values)\n# print(Y_test)","c686d04b":"pred_df = pd.DataFrame(pred_values, index=Y_test.index)\nact_df = Y_test.astype(int)\n\n\ngraphing_df = pd.concat([pred_df, act_df], axis=1)\n# print(graphing_df.shape)\ngraphing_df.columns = ['Predicted Values', 'Actual Values']\ngraphing_df.plot(figsize=(15,8))\n","1a77cd75":"df_last_month_pred_values = pred_df.tail(30) \ndf_last_month_act_values = act_df.tail(30) \ngraphing_df = pd.concat([df_last_month_pred_values, df_last_month_act_values], axis=1)\n# print(graphing_df.shape)\ngraphing_df.columns = ['Predicted Values', 'Actual Values']\ngraphing_df.plot(figsize=(15,8))","c1797ce9":"# get_dataframes -> (DataFrame, DataFrame, DataFrame)\n# reads the cases, deaths, and populations json files in and converts them to pandas dataframes.\ndef get_dataframes():\n    cases_df = ''\n    deaths_df = ''\n    pops_df = ''\n\n    with open('..\/input\/cases-deaths-and-population-json-datas\/cases.json') as file:\n        cases_data = json.load(file)\n        cases_df = pd.DataFrame(cases_data).transpose()\n    \n    with open('..\/input\/cases-deaths-and-population-json-datas\/deaths.json') as file:\n        deaths_data = json.load(file)\n        deaths_df = pd.DataFrame(deaths_data).transpose()\n        \n    with open('..\/input\/cases-deaths-and-population-json-datas\/pops.json') as file:\n        pops_data = json.load(file)\n        pops_df = pd.DataFrame(pops_data).transpose()\n        \n    return (cases_df, deaths_df, pops_df)\n        \n    \ncases_df, deaths_df, pops_df = get_dataframes()\n\n# new_cases -> DataFrame\n# Calculates the number of new cases in a given day based on the previous day's value and replaces the corresponding\n# cell with that value.\ndef new_cases():\n    df = cases_df.copy()\n    ignored_columns = [0, 1, 2, 3, 4]\n    num_cols = len(df.columns)\n    \n    for index, row in df.iterrows():\n        for col_idx in reversed(range(num_cols)):\n            if col_idx not in ignored_columns:\n                curr_col = df.columns[col_idx]\n                last_col = df.columns[col_idx-1]\n                df.at[index, curr_col] = int(df.at[index, curr_col]) - int(df.at[index, last_col])\n                \n    return df\n\nnew_cases_df = new_cases()\n\n# new_deaths -> DataFrame\n# Calculates the number of new deaths in a given day based on the previous day's value and replaces the corresponding\n# cell with that value.\ndef new_deaths():\n    df = deaths_df.copy()\n    ignored_columns = [0, 1, 2, 3, 4]\n    num_cols = len(df.columns)\n    \n    for index, row in df.iterrows():\n        for col_idx in reversed(range(num_cols)):\n            if col_idx not in ignored_columns:\n                curr_col = df.columns[col_idx]\n                last_col = df.columns[col_idx-1]\n                df.at[index, curr_col] = int(df.at[index, curr_col]) - int(df.at[index, last_col])\n                \n    return df\n\nnew_deaths_df = new_deaths()","e060184c":"# master_dataframe -> DataFrame\n# Creates a new dataframe with the following specifications (to be used in model training):\n# Columns: [CountyFIPS, Population, Total Cases, New Cases, Total Deaths, New Deaths, Cases in 7 Days]\n# Index: [Date]\ndef master_dataframe():\n    c_df = cases_df.copy().iloc[:,:-7]\n    d_df = deaths_df.copy().iloc[:,:-7]\n    n_c_df = new_cases_df.copy().iloc[:,:-7]\n    n_d_df = new_deaths_df.copy().iloc[:,:-7]\n    \n    dates = []\n    counties = []\n    populations = []\n    total_cases = []\n    new_cases = []\n    total_deaths = []\n    new_deaths = []\n    num_cases_in_seven_days = []\n    ignored_columns = [0, 1, 2, 3]\n    \n    row_index = 0\n    for index, row in c_df.iterrows():\n        countyFIPS = c_df.at[index, c_df.columns[0]]\n        if countyFIPS in pops_df['\u00ef\u00bb\u00bfcountyFIPS']:\n            population = pops_df.loc[pops_df['\u00ef\u00bb\u00bfcountyFIPS'] == countyFIPS, 'population'].iloc[0]\n        else:\n            population = 0\n        \n        for col_idx in range(len(c_df.columns)):\n            if col_idx not in ignored_columns:\n                date = cases_df.columns[col_idx]\n                dates.append(date)\n                counties.append(countyFIPS)\n                populations.append(population)\n                total_cases.append(c_df.iloc[int(row_index)][date])\n                new_cases.append(n_c_df.iloc[int(row_index)][date])\n                total_deaths.append(d_df.iloc[int(row_index)][date])\n                new_deaths.append(n_d_df.iloc[int(row_index)][date])\n                num_cases_in_seven_days.append(cases_df.iloc[int(row_index)][cases_df.columns[col_idx + 7]])\n        row_index += 1\n        \n    df = pd.DataFrame(list(zip(counties, populations, total_cases, new_cases, total_deaths, new_deaths, num_cases_in_seven_days)), \n                      index = dates, \n                      columns = ['CountyFIPS', 'Population', 'Total Cases', 'New Cases', 'Total Deaths', 'New Deaths', 'Cases in 7 Days'])\n    return df\n    \nmaster_df = master_dataframe()","13e8e560":"# test_dataframe -> DataFrame\n# This is a modification of the master_dataframe function that creates the same dataframe\n# for the instances within a given state. This returned dataframe is used for testing\n# since the master_dataframe is very large and often crashes our Jupyter notebooks.\ndef test_dataframe(states):\n    c_df = cases_df.copy().iloc[:,:-7]\n    d_df = deaths_df.copy().iloc[:,:-7]\n    n_c_df = new_cases_df.copy().iloc[:,:-7]\n    n_d_df = new_deaths_df.copy().iloc[:,:-7]\n    \n    dates = []\n    counties = []\n    populations = []\n    total_cases = []\n    new_cases = []\n    total_deaths = []\n    new_deaths = []\n    num_cases_in_seven_days = []\n    ignored_columns = [0, 1, 2, 3]\n    \n    row_index = 0\n    for index, row in c_df.iterrows():\n        if row['State'] in states:\n            countyFIPS = c_df.at[index, c_df.columns[0]]\n            if countyFIPS in pops_df['\u00ef\u00bb\u00bfcountyFIPS']:\n                population = pops_df.loc[pops_df['\u00ef\u00bb\u00bfcountyFIPS'] == countyFIPS, 'population'].iloc[0]\n            else:\n                population = 0\n\n            for col_idx in range(len(c_df.columns)):\n                if col_idx not in ignored_columns:\n                    date = cases_df.columns[col_idx]\n                    dates.append(date)\n                    counties.append(countyFIPS)\n                    populations.append(population)\n                    total_cases.append(c_df.iloc[int(row_index)][date])\n                    new_cases.append(n_c_df.iloc[int(row_index)][date])\n                    total_deaths.append(d_df.iloc[int(row_index)][date])\n                    new_deaths.append(n_d_df.iloc[int(row_index)][date])\n                    num_cases_in_seven_days.append(cases_df.iloc[int(row_index)][cases_df.columns[col_idx + 7]])\n        row_index += 1\n        \n    df = pd.DataFrame(list(zip(counties, populations, total_cases, new_cases, total_deaths, new_deaths, num_cases_in_seven_days)), \n                      index = dates, \n                      columns = ['CountyFIPS', 'Population', 'Total Cases', 'New Cases', 'Total Deaths', 'New Deaths', 'Cases in 7 Days'])\n    return df\n    \ntest_df = test_dataframe(['VT', 'ME', 'NH', 'MA'])\ntest_df[test_df['CountyFIPS'] == '50007']","9602518c":"# remove unnecessary column for Decision Tree (CountyFIPS)\ntest_df = test_df.drop('CountyFIPS', axis = 1)\n\n# split dataframe into X and Y\ncols = test_df.shape[1]\nX = test_df.iloc[:,0:cols-1]\nY = test_df.iloc[:,cols-1:cols]\n\n# split X and Y into training and testing dataframes\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n\n# split X_train and Y_train into testing and validation dataframes\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)","74d9911d":"# Create a decision tree object and fit it to our training data\ndtree = DecisionTreeRegressor() # max_depth=20, min_samples_leaf=0.01, random_state=3\ndtree.fit(X_train, Y_train)","c78c822b":"# Using a grid search to tune our hyperparameters\ngrid_search_params = {\n    'max_depth': [1,3,4,5,6,8,10,20,30],\n    'min_samples_leaf': [.001, .01, .1, 1],\n    'random_state': [1,2,3,4,5],\n    'max_leaf_nodes': [5,10,15,20]\n}\n\ngridSearchCVForDecTreeReg = GridSearchCV(dtree, grid_search_params, cv = 5)\ngridSearchCVForDecTreeReg.fit(X_train, Y_train)\nprint(\"best parameters are \", gridSearchCVForDecTreeReg.best_params_)","2146ea72":"# Apply best hyperparameters found in grid search\ndtree = DecisionTreeRegressor(max_depth=5, max_leaf_nodes=20, min_samples_leaf=1, random_state=2)\ndtree.fit(X_train, Y_train)\n\n# Predict using our training and validation data, and output the RMSE and R^2 values.\n# Expected - low RMSE, high R^2 percentage value.\npred_train_tree= dtree.predict(X_train)\nprint(np.sqrt(mean_squared_error(Y_train, pred_train_tree)))\nprint(r2_score(Y_train, pred_train_tree))\n\npred_val_tree= dtree.predict(X_val)\nprint(np.sqrt(mean_squared_error(Y_val, pred_val_tree))) \nprint(r2_score(Y_val, pred_val_tree))","17cbe97e":"# Import the necessary modules and libraries\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n\nprint(type(X))\nprint(X.shape)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, s=20, edgecolor=\"black\",\n            c=\"darkorange\", label=\"data\")\nplt.plot(X_test, y_1, color=\"cornflowerblue\",\n         label=\"max_depth=2\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()","ed2f163e":"# The following graph shows the effect of max_depth on the predicted values of a Decision Tree Regressor. \ndt_x = X_val[0:120:5]\ndt_y = Y_val[0:120:5]\n\ntest = dt_x.merge(dt_y, how='outer', left_index=True, right_index=True)\nprint(test)\n# test['New Cases'] = test['New Cases'].astype(int)\ntest.sort_values(by=['New Cases'])\n\nprint(test)","1366cadc":"y = Y_val.to_numpy().ravel()\npredicted = pred_val_tree\n\nfig, ax = plt.subplots()\nax.scatter(y, predicted)\nax.plot([y.min(), y.max()], [predicted.min(), predicted.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()","68a1b0ec":"# Create a random forest object and fit it to our training data\nrfr = RandomForestRegressor()\nrfr.fit(X_train, np.ravel(Y_train))","802890f0":"\n# Iterate through many different sizes of trees, find the optimal n_estimators\nns = []\nr2_scores = []\n\nfor tree_size in range(1, 1001, 100):\n    rfr = RandomForestRegressor(n_estimators = tree_size)\n    rfr.fit(X_train, np.ravel(Y_train))\n    y_pred_rfr = rfr.predict(X_val)\n    ns.append(tree_size)\n    r2_scores.append(r2_score(Y_val, y_pred_rfr) * 100)\n    \nplt.plot(np.array(ns), np.array(r2_scores))\nplt.xlabel('n_estimator')\nplt.ylabel('R2 Score')\nplt.title('n_estimator vs. R2 Score')","a91cb206":"CV_rfr = GridSearchCV(estimator=RandomForestRegressor(), param_grid={'n_estimators': [i for i in range (1, 401, 10)]})\nCV_rfr.fit(X_train, np.ravel(Y_train))\nbest_estimator = CV_rfr.best_estimator_\n\n# Using the above potential n_estimators, GridSearchCV found 71 as the optimal value of n_estimators (accuracy: 95%).","1784cf02":"CV_rfr","c553c0a6":"from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_predict\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, r2_score, mean_squared_error, confusion_matrix\nfrom sklearn import tree\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom math import sqrt\n\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras.backend as K\nimport numpy as np\nimport pandas as pd\nimport requests\nimport graphviz\nimport json\nimport csv\nimport io","16d999da":"# get_dataframes -> (DataFrame, DataFrame, DataFrame)\n# reads the cases, deaths, and populations json files in and converts them to pandas dataframes.\ndef get_dataframes():\n    cases_df = ''\n    deaths_df = ''\n    pops_df = ''\n\n    with open('..\/input\/cases-deaths-and-population-json-datas\/cases.json') as file:\n        cases_data = json.load(file)\n        cases_df = pd.DataFrame(cases_data).transpose()\n    \n    with open('..\/input\/cases-deaths-and-population-json-datas\/deaths.json') as file:\n        deaths_data = json.load(file)\n        deaths_df = pd.DataFrame(deaths_data).transpose()\n        \n    with open('..\/input\/cases-deaths-and-population-json-datas\/pops.json') as file:\n        pops_data = json.load(file)\n        pops_df = pd.DataFrame(pops_data).transpose()\n        \n    return (cases_df, deaths_df, pops_df)\n        \n    \ncases_df, deaths_df, pops_df = get_dataframes()\ncases_df = cases_df.rename(columns={ cases_df.columns[0]: 'CountyFIPS' })\ndeaths_df = deaths_df.rename(columns={ deaths_df.columns[0]: 'CountyFIPS' })\npops_df = pops_df.rename(columns={ pops_df.columns[0]: 'CountyFIPS' })\n\n# new_cases -> DataFrame\n# Calculates the number of new cases in a given day based on the previous day's value and replaces the corresponding\n# cell with that value.\ndef new_cases():\n    df = cases_df.copy()\n    ignored_columns = [0, 1, 2, 3, 4]\n    num_cols = len(df.columns)\n    \n    for index, row in df.iterrows():\n        for col_idx in reversed(range(num_cols)):\n            if col_idx not in ignored_columns:\n                curr_col = df.columns[col_idx]\n                last_col = df.columns[col_idx-1]\n                df.at[index, curr_col] = int(df.at[index, curr_col]) - int(df.at[index, last_col])\n                \n    return df\n\nnew_cases_df = new_cases()\n\n# new_deaths -> DataFrame\n# Calculates the number of new deaths in a given day based on the previous day's value and replaces the corresponding\n# cell with that value.\ndef new_deaths():\n    df = deaths_df.copy()\n    ignored_columns = [0, 1, 2, 3, 4]\n    num_cols = len(df.columns)\n    \n    for index, row in df.iterrows():\n        for col_idx in reversed(range(num_cols)):\n            if col_idx not in ignored_columns:\n                curr_col = df.columns[col_idx]\n                last_col = df.columns[col_idx-1]\n                df.at[index, curr_col] = int(df.at[index, curr_col]) - int(df.at[index, last_col])\n                \n    return df\n\nnew_deaths_df = new_deaths()","ef8cf312":"# custom_test_train_split -> DataFrame\n# This is a modification of the master_dataframe function that creates the same dataframe\n# for the instances within a given state and county. This returned dataframe is used for testing\n# since the master_dataframe is very large and often crashes our Jupyter notebooks.\ndef custom_test_train_split():\n    c_df = cases_df.copy().iloc[:,:-7]\n    d_df = deaths_df.copy().iloc[:,:-7]\n    n_c_df = new_cases_df.copy().iloc[:,:-7]\n    n_d_df = new_deaths_df.copy().iloc[:,:-7]\n    \n    train_dates = []\n    test_dates = []\n    train_counties = []\n    test_counties = []\n    train_populations = []\n    test_populations = []\n    train_total_cases = []\n    test_total_cases = []\n    train_new_cases = []\n    test_new_cases = []\n    train_total_deaths = []\n    test_total_deaths = []\n    train_new_deaths = []\n    test_new_deaths = []\n    train_num_cases_in_seven_days = []\n    test_num_cases_in_seven_days = []\n    ignored_columns = [0, 1, 2, 3]\n    \n    unique_fips = c_df['CountyFIPS'].unique()\n    random_fips_idxs = [np.random.randint(0, len(unique_fips) - 1) for _ in range(10)]\n    random_fips = [unique_fips[i] for i in random_fips_idxs]\n    \n    row_index = 0\n    for index, row in c_df.iterrows():\n        if row['CountyFIPS'] not in random_fips:\n            countyFIPS = c_df.at[index, c_df.columns[0]]\n            if countyFIPS in pops_df['CountyFIPS']:\n                population = pops_df.loc[pops_df['CountyFIPS'] == countyFIPS, 'population'].iloc[0]\n            else:\n                population = 0\n            for col_idx in range(len(c_df.columns)):\n                if col_idx not in ignored_columns:\n                    date = cases_df.columns[col_idx]\n                    train_dates.append(date)\n                    train_counties.append(countyFIPS)\n                    train_populations.append(population)\n                    train_total_cases.append(c_df.iloc[int(row_index)][date])\n                    train_new_cases.append(n_c_df.iloc[int(row_index)][date])\n                    train_total_deaths.append(d_df.iloc[int(row_index)][date])\n                    train_new_deaths.append(n_d_df.iloc[int(row_index)][date])\n                    train_num_cases_in_seven_days.append(cases_df.iloc[int(row_index)][cases_df.columns[col_idx + 7]])\n        else:\n            countyFIPS = c_df.at[index, c_df.columns[0]]\n            if countyFIPS in pops_df['CountyFIPS']:\n                population = pops_df.loc[pops_df['CountyFIPS'] == countyFIPS, 'population'].iloc[0]\n            else:\n                population = 0\n            for col_idx in range(len(c_df.columns)):\n                if col_idx not in ignored_columns:\n                    date = cases_df.columns[col_idx]\n                    test_dates.append(date)\n                    test_counties.append(countyFIPS)\n                    test_populations.append(population)\n                    test_total_cases.append(c_df.iloc[int(row_index)][date])\n                    test_new_cases.append(n_c_df.iloc[int(row_index)][date])\n                    test_total_deaths.append(d_df.iloc[int(row_index)][date])\n                    test_new_deaths.append(n_d_df.iloc[int(row_index)][date])\n                    test_num_cases_in_seven_days.append(cases_df.iloc[int(row_index)][cases_df.columns[col_idx + 7]])\n        row_index += 1\n        \n    train_df = pd.DataFrame(list(zip(train_counties, train_populations, train_total_cases, train_new_cases, train_total_deaths, train_new_deaths, train_num_cases_in_seven_days)), \n                      index = train_dates, \n                      columns = ['CountyFIPS', 'Population', 'Total Cases', 'New Cases', 'Total Deaths', 'New Deaths', 'Cases in 7 Days'])\n    train_cols = train_df.shape[1]\n    X_train = train_df.iloc[:,0:train_cols-1]\n    Y_train = train_df.iloc[:,train_cols-1:train_cols]\n    \n    test_df = pd.DataFrame(list(zip(test_counties, test_populations, test_total_cases, test_new_cases, test_total_deaths, test_new_deaths, test_num_cases_in_seven_days)), \n                      index = test_dates, \n                      columns = ['CountyFIPS', 'Population', 'Total Cases', 'New Cases', 'Total Deaths', 'New Deaths', 'Cases in 7 Days'])\n    test_cols = test_df.shape[1]\n    X_test = test_df.iloc[:,0:test_cols-1]\n    Y_test = test_df.iloc[:,test_cols-1:test_cols]\n    \n    \n    return X_train, X_test, Y_train, Y_test\n\nX_train, X_test, Y_train, Y_test = custom_test_train_split()","47fd3023":"# Normalizing\nX_train = X_train.astype(int)\nY_train = Y_train.astype(int)\nX_test = X_test.astype(int)\nY_test = Y_test.astype(int)\n\nif 'CountyFIPS' in X_train.columns:\n    X_train = X_train.drop(['CountyFIPS'], axis=1)\nif 'CountyFIPS' in X_test.columns:\n    X_test = X_test.drop(['CountyFIPS'], axis=1)\n\nnormalizer = preprocessing.Normalization()\nnormalizer.adapt(np.array(X_train.astype(int)))","f9af5ad1":"first = np.array(X_train[:70000])\n\nwith np.printoptions(precision=2, suppress=True):\n    print('First Example: ', first)\n    print()\n    print('Normalized: ', normalizer(first).numpy())","025f453b":"scaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(X_train)\nprint(X_train_scaled)","184a1c58":"def split_test(X, Y):\n    num_counties = 10\n    \n    X_list = np.array_split(X, num_counties)\n    Y_list = np.array_split(Y, num_counties)\n    \n    return X_list, Y_list\n    \n    \nX_test_list, Y_test_list = split_test(X_test_scaled, Y_test)","d45eb049":"# Model creation\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n\nes = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=5)\n\nhistory = model.fit(X_train_scaled, Y_train,\n                    epochs=50,\n                    verbose=2,\n                    validation_split=0.1,\n                    callbacks=[es])","46daab9c":"def plot_loss(hist):\n    plt.plot(hist.history['loss'], label='loss')\n    plt.plot(hist.history['val_loss'], label='val_loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef plot_mae(hist):\n    plt.plot(hist.history['mae'], label='loss')\n    plt.plot(hist.history['val_mae'], label='val_loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Mean Average Error')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \nplot_loss(history)\nplot_mae(history)","3c0f2f36":"def accuracy_score(predicted, actual, threshold):\n    correct = 0\n    incorrect = 0\n    \n    for idx in range(len(predicted)):\n        pred = predicted[idx]\n        act = actual[idx]\n        \n        diff = abs(act - pred)\n        diff_pct = diff\/(act + 0.01)\n        \n        if diff_pct <= threshold:\n            correct += 1\n        else:\n            incorrect += 1\n            \n    accuracy = (correct \/ len(predicted)) * 100\n    \n    print('Weighted Accuracy w\/ Threshold of ' + str(threshold * 100) + '%: ' + str(accuracy) + '%.')","08ceb2ba":"def graph_predicted_vs_actual_nn(X_input, Y_input, predictions):\n\n    X = Y_input.index.values\n    Y = Y_input[Y_input.columns[0]].to_numpy()\n    \n    Y_min = np.amin(Y)\n    pred_min = np.amin(predictions)\n    Y_min = np.minimum(Y_min, pred_min)\n\n    Y_max = np.amax(Y)\n    pred_max = np.amax(predictions)\n    Y_max = np.maximum(Y_max, pred_max)\n\n    plt.figure(figsize=(15,8))\n    plt.scatter(X, Y, label='Actual')\n    plt.scatter(X, predictions, label='Predicted')\n    plt.ylim(Y_min, Y_max)\n    plt.xlabel('Date')\n    plt.ylabel('Number of Cases')\n    plt.xticks(X[1::20], rotation=45)\n    plt.title(\"Predicted vs. Actual for Neural Network Regression\")\n    plt.legend()\n    plt.show()\n    \n    \n# for county_idx in range(len(X_test_list)):\n#     predictions = model.predict(X_test_list[county_idx]).flatten()\n#     graph_predicted_vs_actual_nn(X_test_list[county_idx], Y_test_list[county_idx], predictions)\n    \n#     print('R2 Score: ' + str(r2_score(predictions, np.ravel(Y_test_list[county_idx].to_numpy()))))\n#     accuracy_score(predictions, np.ravel(Y_test_list[county_idx].to_numpy()), 0.1)","9ef3de88":"# Model creation\nmodel_2 = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\n\nmodel_2.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n\nes = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=5)\n\nhistory_2 = model_2.fit(X_train_scaled, Y_train,\n                    epochs=30,\n                    verbose=2,\n                    validation_split=0.1,\n                    callbacks=[es])","c99df8b9":"plot_loss(history_2)\nplot_mae(history_2)","a3119753":"# Model creation\nmodel_3 = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\n\nmodel_3.compile(loss='mse', optimizer='adam', metrics=['mae'])\n\nhistory_3 = model_3.fit(X_train_scaled, Y_train,\n                    epochs=30,\n                    verbose=2,\n                    validation_split=0.1)","8a21be55":"plot_loss(history_3)\nplot_mae(history_3)","31eda383":"# data = [[30, 25, 50, 20],\n# [40, 23, 51, 17],\n# [35, 22, 45, 19]]\n# X = np.arange(4)\n# fig = plt.figure()\n# ax = fig.add_axes([0,0,1,1])\n# ax.bar(X + 0.00, data[0], color = 'b', width = 0.25)\n# ax.bar(X + 0.25, data[1], color = 'g', width = 0.25)\n# ax.bar(X + 0.50, data[2], color = 'r', width = 0.25)\n\n# #graph R2 and RMSE values of each model - maybe one bar graph at the end that has two bars for each algorithm - R2 and RMSE. ","05b8914a":"idNum = 0\npredictions = model.predict(X_test_list[idNum]).flatten()\ngraph_predicted_vs_actual_nn(X_test_list[idNum], Y_test_list[idNum], predictions)\n\nprint('R2 Score: for  ' + str(r2_score(predictions, np.ravel(Y_test_list[idNum].to_numpy()))))\naccuracy_score(predictions, np.ravel(Y_test_list[idNum].to_numpy()), 0.1)","60c8b450":"    predictions = model_3.predict(X_test_list[idNum]).flatten()\n    graph_predicted_vs_actual_nn(X_test_list[idNum], Y_test_list[idNum], predictions)\n\n    print('R2 Score: ' + str(r2_score(predictions, np.ravel(Y_test_list[idNum].to_numpy()))))\n    accuracy_score(predictions, np.ravel(Y_test_list[idNum].to_numpy()), 0.1)","1981767f":"idNum2 = 2\npredictions = model_2.predict(X_test_list[idNum2]).flatten()\ngraph_predicted_vs_actual_nn(X_test_list[idNum2], Y_test_list[idNum2], predictions)\n    \nprint('R2 Score: ' + str(r2_score(predictions, np.ravel(Y_test_list[idNum2].to_numpy()))))\naccuracy_score(predictions, np.ravel(Y_test_list[idNum2].to_numpy()), 0.1)","621198e6":"vermontCases = {'countyFIPS': [50001, 50003, 50005, 50007, 50009, 50011, 50013, 50015, 50017, 50019, 50021, 50023, 50025, 50027], 'cases': [203, 235, 157, 1724, 62, 304, 46, 176, 248, 176, 271, 761, 281, 241]}\n","62409dbc":"from urllib.request import urlopen\nimport json\nwith urlopen('https:\/\/raw.githubusercontent.com\/plotly\/datasets\/master\/geojson-counties-fips.json') as response:\n    counties = json.load(response)\n\ncounties[\"features\"][0]","bf937425":"\nimport plotly.express as px\n\nfig = px.choropleth_mapbox(vermontCases, geojson=counties, locations='countyFIPS', color='cases',\n                           color_continuous_scale=\"orrd\",\n                           range_color=(0, 1700),\n                           mapbox_style=\"carto-positron\",\n                           zoom=6.5, center = {\"lat\": 43.955178, \"lon\": -72.903028},\n                           opacity=0.5,\n                           labels={'cases':'Cases as of 12\/7\/2020'}\n                          )\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","247be108":"df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/lcharlesb\/254FinalProject\/master\/TestingCSVFile.csv\",\n                   dtype={\"fips\": str})\n\nimport plotly.express as px\n\nfig = px.choropleth_mapbox(df, geojson=counties, locations='countyFIPS', color='cases',\n                           color_continuous_scale=\"orrd\",\n                           range_color=(0, 5000),\n                           mapbox_style=\"carto-positron\",\n                           zoom=6, center = {\"lat\": 43.955178, \"lon\": -72.903028},\n                           opacity=0.5,\n                           labels={'cases':'Cases as of 12\/7\/2020'}\n                          )\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","384840b3":"# Neural Networks","508a48af":"Model 3","cde421ee":"Now let's look at the best result so far (Sometimes our kaggle version looks a bit different live versus the editor so this might now show it. But our best model prediction was with model 2 with around 70-75% of the predictions being within 10% of actual cases for some counties.","8487e36c":"# Retrieving the data","47a2a4dc":"# Random Forest","c26e4404":"Random Forest Training","4894a68e":"# Model Evaluation","145ff25c":"# Model Training","4739ae2d":"Decision Tree Evaluation","32ac4ce9":"Normalizing","4e8b98f3":"Model 2","c1ba8175":"Master dataframe","1fd1c95f":"New cases","a47fdcb8":"Let's compare 2 models with the same county","89666437":"New deaths","dc38edd8":"# Preprocessing","66abfae5":"Decision Tree Training","f72286bb":"Model 1","4f4efdc2":"Random Forest Hyperparameter Optimization","2e843cda":"# Visualizing the Data","8193691a":"# Map Visualization","5ddae266":"Below I've hardcoded in the values for the cases but this was to test if we could take our predicted cases and put it into a map to visualize it","6a09ee0f":"Decision Tree Hyperparameter Optimization","82b04428":"# Neural Networks","36a18063":"# Hyperparameter Tuning"}}