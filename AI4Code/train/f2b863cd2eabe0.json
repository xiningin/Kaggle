{"cell_type":{"d0bd1876":"code","c6b1bd8f":"code","07797d37":"code","c3102dd0":"code","fd2b549a":"code","14f0665b":"code","6cd08ce1":"code","0fb9eccc":"code","d6371c60":"code","acaf4c77":"code","1322ce14":"code","4847f249":"code","90197e0d":"code","72c090e2":"code","1ac30a31":"code","bb4e56c2":"code","fb6138e3":"code","8da3a52b":"code","06ee1dcc":"code","6c61a818":"code","38f4413f":"code","9128d8f4":"code","867ad79c":"code","113618b9":"code","7e0ffe33":"code","e2584881":"code","57ab9d45":"code","05ea1db7":"code","c674c6f8":"code","4a6fa59f":"code","87cb1aaa":"code","86748ab0":"code","9c0fba97":"code","01d59418":"code","6cac7249":"code","11a9e14b":"code","bc4097c6":"code","48101b0c":"code","609b1c91":"markdown","2576c11e":"markdown","0824dfa4":"markdown","e0eb4891":"markdown","d04c3bfc":"markdown","cee719b9":"markdown","0a66d81a":"markdown","0eca7541":"markdown","1eac7c06":"markdown","db856c85":"markdown","e4a581e0":"markdown","36083f7e":"markdown","e9282124":"markdown","b50f4c42":"markdown","b6db08cf":"markdown","bc2dc71b":"markdown","4ac781f2":"markdown","ff2135f0":"markdown","8e61eba1":"markdown"},"source":{"d0bd1876":"import numpy as np\nimport pandas as pd\nimport keras\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom collections import Counter","c6b1bd8f":"from imblearn.over_sampling import SMOTE","07797d37":"from sklearn.datasets import make_classification\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_classes=2, weights=[0.2, 0.8], class_sep=0.95, random_state=0)","c3102dd0":"plt.figure(figsize=(12, 8))\nplt.title('Repartition before SMOTE')\nplt.scatter(X[y==1][:, 0], X[y==1][:, 1], label='class 1')\nplt.scatter(X[y==0][:, 0], X[y==0][:, 1], label='class 0')\nplt.legend()\nplt.grid(False)\nplt.show()","fd2b549a":"smt = SMOTE()\nX_smote, y_smote = smt.fit_resample(X, y)","14f0665b":"plt.figure(figsize=(12, 8))\nplt.title('Repartition after SMOTE')\nplt.scatter(X_smote[y_smote==1][:, 0], X_smote[y_smote==1][:, 1], label='class 1')\nplt.scatter(X_smote[y_smote==0][:, 0], X_smote[y_smote==0][:, 1], label='class 0')\nplt.legend()\nplt.grid(False)\nplt.show()","6cd08ce1":"df = pd.read_csv(\"..\/input\/train.csv\")\nprint(\"Number of texts: \", df.shape[0])","0fb9eccc":"df = df.sample(30000)","d6371c60":"plt.figure(figsize = (10, 8))\nsns.countplot(df['target'])\nplt.show()","acaf4c77":"print(Counter(df['target']))","1322ce14":"max_len = 50\nlen_voc = 40000","4847f249":"from sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size=0.5)","90197e0d":"def make_tokenizer(texts, len_voc):\n    from keras.preprocessing.text import Tokenizer\n    t = Tokenizer(num_words=len_voc)\n    t.fit_on_texts(texts)\n    return t","72c090e2":"tokenizer = make_tokenizer(df['question_text'], len_voc)","1ac30a31":"X_train = tokenizer.texts_to_sequences(df_train['question_text'])\nX_test = tokenizer.texts_to_sequences(df_test['question_text'])","bb4e56c2":"from keras.preprocessing.sequence import pad_sequences\n\nX_train = pad_sequences(X_train, maxlen=max_len, padding='post', truncating='post')\nX_test = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')","fb6138e3":"y_train = df_train['target'].values\ny_test = df_test['target'].values","8da3a52b":"def get_coefs(word,*arr): \n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embedding(file):\n    if file == '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n    return embeddings_index","06ee1dcc":"def make_embedding_matrix(embedding, tokenizer, len_voc):\n    all_embs = np.stack(embedding.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    word_index = tokenizer.word_index\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n    \n    for word, i in word_index.items():\n        if i >= len_voc:\n            continue\n        embedding_vector = embedding.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","6c61a818":"glove = load_embedding('..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt')","38f4413f":"embed_mat = make_embedding_matrix(glove, tokenizer, len_voc)","9128d8f4":"X_train_emb = embed_mat[X_train]\nX_test_emb = embed_mat[X_test]","867ad79c":"train_size, max_len, embed_size = X_train_emb.shape\nX_train_emb_r = X_train_emb.reshape(train_size, max_len*embed_size)","113618b9":"smt = SMOTE(sampling_strategy=0.2)\nX_smote, y_smote = smt.fit_sample(X_train_emb_r, y_train)","7e0ffe33":"X_smote = X_smote.reshape((X_smote.shape[0], max_len, embed_size))","e2584881":"plt.figure(figsize = (10, 8))\nplt.subplot(1, 2, 1)\nsns.countplot(y_train)\nplt.title('Reparition before SMOTE')\nplt.subplot(1, 2, 2)\nsns.countplot(y_smote)\nplt.title('Reparition after SMOTE')\nplt.show()","57ab9d45":"from keras.models import Model\nfrom keras.layers import Dense, Bidirectional, CuDNNGRU, GlobalMaxPool1D, Input, Dropout\nfrom keras.optimizers import Adam","05ea1db7":"def make_model(max_len, len_voc=50000, embed_size=300):\n    inp = Input(shape=(max_len, 300))\n    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(inp)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n    return model","c674c6f8":"model = make_model(max_len)\nmodel_smote = make_model(max_len)","4a6fa59f":"model.summary()","87cb1aaa":"from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=2, verbose=1, min_lr=0.000001)\ncheckpoints = ModelCheckpoint('weights.hdf5', monitor=\"val_acc\", mode=\"max\", verbose=True, save_best_only=True)\n\nreduce_lr_smote = ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=2, verbose=1, min_lr=0.000001)\ncheckpoints_smote = ModelCheckpoint('smote_weights.hdf5', monitor=\"val_acc\", mode=\"max\", verbose=True, save_best_only=True)","86748ab0":"model.fit(X_train_emb, y_train, batch_size=128, epochs=3, validation_data=[X_test_emb, y_test], callbacks=[checkpoints, reduce_lr])","9c0fba97":"model_smote.fit(X_smote, y_smote, batch_size=128, epochs=3, validation_data=[X_test_emb, y_test], callbacks=[checkpoints_smote, reduce_lr_smote])","01d59418":"model.load_weights('weights.hdf5')\nmodel_smote.load_weights('smote_weights.hdf5')","6cac7249":"pred_test = model.predict([X_test_emb], verbose=1)\npred_test_smote = model_smote.predict([X_test_emb], batch_size=256, verbose=1)","11a9e14b":"def tweak_threshold(pred, truth):\n    from sklearn.metrics import f1_score\n    scores = []\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        score = f1_score(truth, (pred>thresh).astype(int))\n        scores.append(score)\n    return round(np.max(scores), 4)","bc4097c6":"print(f\"Scored {tweak_threshold(pred_test, y_test)} without SMOTE (test data)\")","48101b0c":"print(f\"Scored {tweak_threshold(pred_test_smote, y_test)} with SMOTE (test data)\")","609b1c91":"### Tokenizing","2576c11e":"## Now let us train a model","0824dfa4":"### Fitting","e0eb4891":"## Loading data","d04c3bfc":"## Class imbalance","cee719b9":"## Oversampling","0a66d81a":" ## Conclusion\n\nIt appears that SMOTE does not help improve the results. However, it makes the network learning faster.\n\n**Moreover, there is one big problem, this method is not compatible larger datasets.**\n\nYou have to apply SMOTE on embedded sentences, which takes way too much memory. \n\nA solution is to use a generator for our training, which realizes oversampling on batches. I've tried it, but my generator was very slow.\n\nSo I'm going to stick with these results for now, and try another data augmentation technique.\n\nIf you have any improvement idea feel free to let me know.\n\n#### Thanks for reading ! \n ","0eca7541":"### Making model","1eac7c06":"### Tweaking threshold","db856c85":"### Predictions","e4a581e0":"## How does SMOTE work ?\n\n> \" The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any\/all of the k minority class nearest neighbors \"\n\n> \" Synthetic samples are generated in the following way: Take the di\ufb00erence between the feature vector (sample) under consideration and its nearest neighbor. Multiply this di\ufb00erence by a random number between 0 and 1, and add it to the feature vector under consideration. This causes the selection of a random point along the line segment between two speci\ufb01c features. This approach e\ufb00ectively forces the decision region of the minority class to become more general. \"\n\nI am using the class from imblearn,  see https:\/\/imbalanced-learn.org\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html","36083f7e":"## Making Data for the network\nWe apply the following steps :\n* Splitting\n* Tokenizing\n* Padding","e9282124":"### Train\/Test split\nIt is important to split before oversampling ! ","b50f4c42":"### Embeddings","b6db08cf":"There is way more 0s than 1s in our dataset, data is very unbalanced and one should consider using oversampling or undersampling.\n\nI don't recommand undersampling in Kaggle competitions, because you want to have as much data as possible for your training. ","bc2dc71b":" # Dealing with Class Imbalance with SMOTE\n\n### In this kernel, I will use a simple Deep Learning model and compare its performance on normal data and data augmented with SMOTE\n\n> Check https:\/\/arxiv.org\/pdf\/1106.1813.pdf\n\nI use SMOTE to add **sentence level** noise to our data.\n\n#### The model is the following one :\n* GloVe Embedding\n* Bidirectional GRU\n* MaxPool\n* Dense \n* Probably some Dropouts\n\n\n#### Feel free to give any feedback, it is always appreciated.","4ac781f2":"### Padding","ff2135f0":"### Callbacks","8e61eba1":"### Targets"}}