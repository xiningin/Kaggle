{"cell_type":{"08d26d0a":"code","6504ee12":"code","99544590":"code","6216fd5f":"code","15af3f02":"code","94708fc7":"code","9634d796":"code","65bdc386":"code","eff11f08":"code","194ce10f":"code","20a87218":"code","672d39a4":"code","6b665bd5":"code","ea969a45":"code","2d4f906f":"code","54ffeeca":"code","aa5ac1e6":"code","d195fe40":"code","469e1e4c":"code","f056d706":"code","29ef74e0":"code","23a1de59":"code","cc4b8591":"code","e040bb8a":"code","07d74d7d":"code","29d8554f":"code","d71fc9eb":"code","f779eace":"code","c6ecfc1e":"code","f0cf684f":"code","4e5aa30c":"code","e0f948e2":"code","7f4bb542":"markdown"},"source":{"08d26d0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6504ee12":"df_target_train = pd.read_csv('\/kaggle\/input\/mlclass-dubai-by-ods-lecture-5-hw\/df_target_train.csv')\nprint('df_target_train:', df_target_train.shape)\n\ndf_sample_submit = pd.read_csv('\/kaggle\/input\/mlclass-dubai-by-ods-lecture-5-hw\/df_sample_submit.csv')\nprint('df_sample_submit:', df_sample_submit.shape)\n\ndf_tracks = pd.read_csv('\/kaggle\/input\/mlclass-dubai-by-ods-lecture-5-hw\/df_tracks.csv')\nprint('df_tracks:', df_tracks.shape)\n\ndf_genres = pd.read_csv('\/kaggle\/input\/mlclass-dubai-by-ods-lecture-5-hw\/df_genres.csv')\nprint('df_genres:', df_genres.shape)\n\ndf_features = pd.read_csv('\/kaggle\/input\/mlclass-dubai-by-ods-lecture-5-hw\/df_features.csv')\nprint('df_features:', df_features.shape)","99544590":"df_target_train","6216fd5f":"df_sample_submit","15af3f02":"df_tracks.track_id","94708fc7":"df_genres","9634d796":"df_features","65bdc386":"from tqdm.notebook import tqdm\nfrom collections import defaultdict\n# extract tracks for each genre\ngenre2tracks = defaultdict(list)\nfor _, row in tqdm(df_target_train.iterrows(), total=df_target_train.shape[0]):\n    for g_id in row['track:genres'].split(' '):\n        genre2tracks[int(g_id)].append(row['track_id']) ","eff11f08":"list(genre2tracks)","194ce10f":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(50,20))\ndf_tmp = pd.DataFrame(\n    [(k, len(v)) for (k, v) in genre2tracks.items()], \n    columns=['genre', 'n_tracks']).sort_values(['n_tracks'],ascending=False)\n\nsns.barplot(\n    data=df_tmp, \n    x='genre',\n    y='n_tracks',\n    order=df_tmp['genre'])\nplt.xticks(rotation=45)\nplt.title('Disribution of tracks per genre')\nplt.show()","20a87218":"sns.heatmap(data=df_tmp,yticklabels=False,annot=False, cbar=False)","672d39a4":"df_features.set_index(['track_id'], inplace=True)\ndf_target_train.set_index(['track_id'], inplace=True)","6b665bd5":"sns.heatmap(df_features.corr(),cbar=False)","ea969a45":"# # from sklearn.decomposition import KernelPCA\n# from sklearn.decomposition import PCA\n# from sklearn.preprocessing import StandardScaler\n# # Scale the features and set the values to a new variable\n# scaler = StandardScaler()\n# scaled_train_features = scaler.fit_transform(df_features)\n# pca = PCA().fit(scaled_train_features)\n# # Plotting the Cumulative Summation of the Explained Variance\n# plt.figure()\n# plt.plot(np.cumsum(pca.explained_variance_ratio_), c='r')\n# plt.xlabel('Number of Components')\n# plt.ylabel('Variance (%)')  # for each component\n# plt.title('Scaled data Explained Variance')\n# plt.axhline(y=0.9, linestyle='dashdot')\n# plt.axvline(x=151, linestyle='dashdot')\n# plt.show()\n# print('Explained variation per principal component:\\n {}'.format(\n#     pca.explained_variance_ratio_))\n# # assign pca dataframe\n# pca = PCA(n_components=.9)\n# pca_features = PCA(n_components=.9).fit_transform(scaled_train_features)  # assign the x values for pca models\n# print('PCA datashape:\\n', pca_features.shape)","2d4f906f":"# # scaling df without target\n# from sklearn.preprocessing import QuantileTransformer\n# df_scaled = pd.DataFrame(\n#     QuantileTransformer().fit_transform(df_features), columns=df_features.columns,index=df_features.index)\n# # df_scaled.hist(color='r', alpha=0.5, figsize=(15, 13))\n# df_scaled.head()","54ffeeca":"df_features.head()","aa5ac1e6":"# df_features_scaled = df_scaled","d195fe40":"from collections import defaultdict\nr = defaultdict(int)\nfor _, row in df_target_train.iterrows():\n    for x in row['track:genres'].split(' '):\n        r[int(x)] += 1\n        \nlabels = list(sorted(r.keys()))","469e1e4c":"# from sklearn.preprocessing import StandardScaler\n# df_features = pd.DataFrame(StandardScaler().fit_transform(df_features.values),\n#                            columns=df_features.columns, index=df_features.index)","f056d706":"df_features","29ef74e0":"from sklearn.preprocessing import QuantileTransformer\ndf_features = pd.DataFrame(\n    QuantileTransformer().fit_transform(df_features.values), columns=df_features.columns,index=df_features.index)","23a1de59":"from sklearn.metrics import accuracy_score\n# from sklearn.linear_model import LogisticRegression\n# from catboost import CatBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n# from sklearn.ensemble import RandomForestClassifier\nfrom functools import reduce\nfrom sklearn.metrics import f1_score\nimport pickle\n\nval_ratio = 0.3\nc_grid = [0.01, 0.1, 1]\nmodels = {}\n\nfor g_id, positive_samples in tqdm(genre2tracks.items()):\n    if len(positive_samples) < 1000:\n        continue\n    # construct train\/val using negative sampling\n    negative_samples = list(set(reduce(lambda a, b: a + b,\n                                       [v for (k, v) in genre2tracks.items() if k != g_id])).difference(positive_samples))\n    negative_samples = np.random.choice(\n        negative_samples,\n        size=len(positive_samples),\n        replace=len(negative_samples) < len(positive_samples))\n    \n    train_positive_samples = np.random.choice(\n        positive_samples,\n        size=int((1 - val_ratio)*len(positive_samples)),\n        replace=False)\n    \n    val_positive_samples = list(set(positive_samples).difference(train_positive_samples))\n    \n    train_negative_samples = np.random.choice(\n        negative_samples,\n        size=int((1 - val_ratio)*len(negative_samples)),\n        replace=False)\n    \n    val_negative_samples = list(set(negative_samples).difference(train_negative_samples))\n    \n    # train a models and pick one\n    models[g_id] = {\n        'acc': -1}\n    \n    for c in c_grid:\n        model = KNeighborsClassifier()\n        model = model.fit(\n            df_features.loc[np.hstack([train_positive_samples, train_negative_samples])],\n            [1.0]*len(train_positive_samples) + [0.0]*len(train_negative_samples))\n        \n        p_val = model.predict_proba(df_features.loc[np.hstack([val_positive_samples, val_negative_samples])])[:, 1]\n        y_val = [1.0]*len(val_positive_samples) + [0.0]*len(val_negative_samples)\n        \n        # choose threshold\n        best_t = -1\n        best_acc = -1\n        for t in np.linspace(0.01, 0.99, 99):\n            acc = accuracy_score(y_val, (p_val >= t).astype(np.float))\n            if acc > best_acc:\n                best_acc = acc\n                best_t = t\n        \n        if models[g_id]['acc'] < best_acc:\n            models[g_id]['acc'] = best_acc\n            models[g_id]['t'] = best_t\n            models[g_id]['model'] = model\n            models[g_id]['c'] = c","cc4b8591":"with open('.\/models.pkl', 'wb') as f:\n    pickle.dump(models, f)","e040bb8a":"# correct test preditions to equalize median number of genres per track\n\ndef get_test(k=1.0):\n    g_prediction = {}\n    for g_id, d in tqdm(models.items()):\n        p = d['model'].predict_proba(df_features.loc[df_sample_submit['track_id'].values])[:, 1]\n        g_prediction[g_id] = df_sample_submit['track_id'].values[p > k*d['t']]\n\n    track2genres = defaultdict(list)\n    for g_id, tracks in g_prediction.items():\n        for t_id in tracks:\n            track2genres[t_id].append(g_id)\n            \n    return track2genres\n\ntrack2genres = get_test(k=1.0)","07d74d7d":"# median number of genres per track in test\nnp.median([len(v) for v in track2genres.values()])","29d8554f":"# median number of genres per track in train\nz = df_target_train['track:genres'].apply(lambda s: len([int(x) for x in s.split(' ')])).median()\n\nz","d71fc9eb":"for k in np.linspace(1, 2, 11):\n    track2genres = get_test(k=k)\n    print(k, np.median([len(v) for v in track2genres.values()]))","f779eace":"track2genres = get_test(k=1.55)","c6ecfc1e":"df_sample_submit['track:genres'] = df_sample_submit.apply(lambda r: ' '.join([str(x) for x in track2genres[r['track_id']]]), axis=1)","f0cf684f":"# add top-2\ndf_sample_submit['track:genres'] = df_sample_submit['track:genres'].apply(lambda r: r + ' 15 38' if len(r) > 0 else '15 38')","4e5aa30c":"df_sample_submit.to_csv('.\/submit.csv', index=False)","e0f948e2":"!head .\/submit.csv","7f4bb542":"Applying dimensionalty reduction techniques"}}