{"cell_type":{"587c39be":"code","6cd1fcd1":"code","f53c94c5":"code","539e36ce":"code","52271043":"code","ee524b25":"code","3d3d7dc6":"code","97a445de":"code","3b15e08c":"code","4cbb7d35":"code","f94b1aad":"code","54cdd9aa":"code","376d63c2":"code","71365f4f":"code","595def14":"code","432de350":"code","41968be6":"code","882bf324":"code","fb1beea0":"code","0653605d":"code","9a8e6f02":"code","23f4bab2":"code","dc811647":"code","c50fe0e9":"code","d6524bb9":"code","22a022a9":"code","e5085a93":"code","fb8f6304":"code","f04d4605":"code","6f223e88":"code","33b98c9e":"code","f17f18f8":"code","fe7844be":"code","7aec5772":"code","171602ea":"code","ac7d7099":"code","492d62c8":"code","a57d4383":"code","674ddea7":"code","8aa4179b":"code","ac16aa40":"code","0e7f8925":"code","d86bee99":"code","bd4b23f7":"code","675c442d":"code","85a01775":"code","be2bb2b4":"code","b4ad6775":"code","c1e57c24":"code","5051eb87":"code","8c924232":"code","04843c2b":"code","1241812c":"code","e2e419ea":"code","1099e803":"code","50f62ab5":"code","38bdb3f3":"code","f35370cc":"code","6dfeee04":"code","b490994f":"markdown","7a70e4b4":"markdown","1533ca30":"markdown","ead9afc2":"markdown","dd2685f4":"markdown","cd16ca88":"markdown","25a713aa":"markdown","989c47ef":"markdown","78cd01bc":"markdown"},"source":{"587c39be":"import pandas as pd\nimport numpy as np\n!pip install xlrd\n\ndata = pd.read_excel('..\/input\/uci-cardiotocography\/CTG.xls', sheet_name = 1, skiprows = 1)","6cd1fcd1":"data","f53c94c5":"data.drop(data.iloc[:, :10], inplace = True, axis = 1) ","539e36ce":"data.drop(data.iloc[:, 22:33], inplace = True, axis = 1)","52271043":"data = data.drop(['Unnamed: 31', 'Unnamed: 44'], axis = 1)","ee524b25":"data.info()","3d3d7dc6":"data.isna().sum()","97a445de":"data = data.dropna()","3b15e08c":"data = data.drop_duplicates()","4cbb7d35":"data.describe()","f94b1aad":"for col in data.columns:\n    print(data[col].value_counts())","54cdd9aa":"import plotly.express as px\n\nfig = px.box(data, y = data.columns )\nfig.show()","376d63c2":"X = data.drop(['CLASS'], axis=1)\ny = data['CLASS']","71365f4f":"#outliers calculation\n\nQ1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIQR = Q3 - Q1\n((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).sum()","595def14":"#uncomment if you want to remove outliers\n\n#data = data[(data >= (Q1 - 1.5*IQR)) & (data <= (Q1 + 1.5*IQR))]","432de350":"data.describe()","41968be6":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)","882bf324":"print(y_train.value_counts())","fb1beea0":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state = 42)\n           \nX_train, y_train = sm.fit_sample(X_train, y_train) ","0653605d":"print(y_train.value_counts())","9a8e6f02":"from sklearn.preprocessing import StandardScaler\n#from sklearn.preprocessing import Normalizer\n\nscaler = StandardScaler()\n\nstandardized_X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\nstandardized_X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)","23f4bab2":"from sklearn.metrics import log_loss\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ntraining_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=i, random_state = 42)\n    \n    \n    clf_gini.fit(standardized_X_train, y_train)\n    y_pred_gini = clf_gini.predict_proba(standardized_X_test)\n    y_pred_train_gini = clf_gini.predict_proba(standardized_X_train)\n     \n\n    training_loss.append(log_loss(y_train, y_pred_train_gini))\n\n    test_loss.append(log_loss(y_test, y_pred_gini))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","dc811647":"import matplotlib.pyplot as plt\n\ny = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('max depth')\n# Set the y axis label of the current axis.\nplt.ylabel('log-loss')\n# Set a title of the current axes.\nplt.title('Two or more lines on same plot with suitable legends ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","c50fe0e9":"from sklearn.model_selection import cross_val_score\n\nclf_gini = DecisionTreeClassifier(criterion='gini', max_depth=6, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_gini, standardized_X_train, y_train, cv=20)))","d6524bb9":"clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=6, random_state = 42)\n\nclf_gini.fit(standardized_X_train, y_train)","22a022a9":"y_pred_gini = clf_gini.predict(standardized_X_test)","e5085a93":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred_gini))","fb8f6304":"import seaborn as sns\n%matplotlib inline\nimport graphviz\nfrom sklearn import tree\n\n\n\ndot_data = tree.export_graphviz(clf_gini, out_file=None, \n                              feature_names=standardized_X_train.columns,  \n                              class_names= y_train.apply(str),  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","f04d4605":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_gini)\n\nprint('Confusion matrix\\n\\n', cm)\n\nplt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(y_test.unique())\nax.yaxis.set_ticklabels(y_test.unique())","6f223e88":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_gini))","33b98c9e":"training_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=i, random_state = 42)\n    \n    \n    clf_en.fit(standardized_X_train, y_train)\n    y_pred_en = clf_en.predict_proba(standardized_X_test)\n    y_pred_train_en = clf_en.predict_proba(standardized_X_train)\n    \n\n    training_loss.append(log_loss(y_train, y_pred_train_en))\n\n    test_loss.append(log_loss(y_test, y_pred_en))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","f17f18f8":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","fe7844be":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_en, standardized_X_train, y_train, cv=20)))","7aec5772":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state = 42)\n\nclf_en.fit(standardized_X_train, y_train)","171602ea":"y_pred_en = clf_en.predict(standardized_X_test)","ac7d7099":"print(\"Accuracy:\",accuracy_score(y_test, y_pred_en))","492d62c8":"dot_data = tree.export_graphviz(clf_en, out_file=None, \n                              feature_names=standardized_X_train.columns,  \n                              class_names=y_train.apply(str),  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","a57d4383":"cm = confusion_matrix(y_test, y_pred_en)\n\nprint('Confusion matrix\\n\\n', cm)\n\nplt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(y_test.unique())\nax.yaxis.set_ticklabels(y_test.unique())","674ddea7":"print(classification_report(y_test, y_pred_en))","8aa4179b":"from sklearn.ensemble import GradientBoostingClassifier\n\ntraining_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_gb = GradientBoostingClassifier( max_depth=i, random_state = 42)\n    \n    \n    clf_gb.fit(standardized_X_train, y_train.values.ravel())\n    y_pred_gb = clf_gb.predict_proba(standardized_X_test)\n    y_pred_train_gb = clf_gb.predict_proba(standardized_X_train)\n    \n\n    training_loss.append(log_loss(y_train, y_pred_train_gb))\n\n\n    test_loss.append(log_loss(y_test, y_pred_gb))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","ac16aa40":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","0e7f8925":"clf_gb = GradientBoostingClassifier( max_depth=2, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_gb, standardized_X_train, y_train.values.ravel(), cv=10)))","d86bee99":"clf_gb = GradientBoostingClassifier( max_depth=2, random_state = 42)\n\nclf_gb.fit(standardized_X_train, y_train.values.ravel())","bd4b23f7":"y_pred_gb = clf_gb.predict(standardized_X_test)","675c442d":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_gb.score(standardized_X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_gb.score(standardized_X_test, y_test)))","85a01775":"dot_data = tree.export_graphviz(clf_gb.estimators_[0, 0], out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=y_train.apply(str),  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","be2bb2b4":"cm = confusion_matrix(y_test, y_pred_gb)\n\nprint('Confusion matrix\\n\\n', cm)","b4ad6775":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(pd.unique(y_test.values.ravel()))\nax.yaxis.set_ticklabels(pd.unique(y_test.values.ravel()))","c1e57c24":"print(classification_report(y_test, y_pred_gb))","5051eb87":"import xgboost as xgb","8c924232":"training_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_xgb = xgb.XGBClassifier( max_depth=i, random_state = 42, n_jobs = 4)\n    \n    \n    clf_xgb.fit(standardized_X_train, y_train.values.ravel())\n    y_pred_xgb = clf_xgb.predict_proba(standardized_X_test)\n    y_pred_train_xgb = clf_xgb.predict_proba(standardized_X_train)\n    \n     \n    print('Max Depth:' ,i)    \n    print('Training set score: {:.4f}'.format(clf_xgb.score(standardized_X_train, y_train)))\n    training_loss.append(log_loss(y_train, y_pred_train_xgb))\n\n    print('Test set score: {:.4f}'.format(clf_xgb.score(standardized_X_test, y_test)))\n    test_loss.append(log_loss(y_test, y_pred_xgb))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","04843c2b":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","1241812c":"clf_xgb = xgb.XGBClassifier( max_depth=2, random_state = 42, n_jobs = 4)\n\nclf_xgb.fit(standardized_X_train, y_train.values.ravel())","e2e419ea":"y_pred_xgb = clf_xgb.predict(standardized_X_test)","1099e803":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_xgb.score(standardized_X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_xgb.score(standardized_X_test, y_test)))","50f62ab5":"fig, ax = plt.subplots(figsize=(30, 30))\nxgb.plot_tree(clf_xgb, num_trees=2, ax=ax)\nplt.show()","38bdb3f3":"cm = confusion_matrix(y_test, y_pred_xgb)\n\nprint('Confusion matrix\\n\\n', cm)","f35370cc":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(pd.unique(y_test.values.ravel()))\nax.yaxis.set_ticklabels(pd.unique(y_test.values.ravel()))","6dfeee04":"print(classification_report(y_test, y_pred_xgb))","b490994f":"<h1 id=\"Decision_Tree:_Gini_Criterion\">\n2. Decision Tree: Gini Criterion\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Decision_Tree:_Gini_Criterion\">\u00b6<\/a>\n<\/h1>","7a70e4b4":"<h1 id=\"Gradient_Boosting\">\n4. Gradient Boosting\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Gradient_Boosting\">\u00b6<\/a>\n<\/h1>","1533ca30":"Let's drop the unecessary columns","ead9afc2":"<h1 id=\"Exploratory_Data_Analysis\">\n1. Exploratory Data Analysis\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Exploratory_Data_Analysis\">\u00b6<\/a>\n<\/h1>","dd2685f4":"## In this notebook I will try to do an exploratory data analysis and classification for the Cardiotocography dataset, found in the UCI machine learning repository.","cd16ca88":"<h1 id=\"XGB\">\n5. XGBOOST\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#XGB\">\u00b6<\/a>\n<\/h1>","25a713aa":"<h1 id=\"Decision_Tree:_Entropy_Criterion\">\n3. Decision Tree: Entropy Criterion\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Decision_Tree:_Entropy_Criterion\">\u00b6<\/a>\n<\/h1>","989c47ef":"We will apply Standard Scaling to our data ","78cd01bc":"We will produce synthetic data to balance the classes"}}