{"cell_type":{"2ee70976":"code","51ef1ae9":"code","2416da41":"code","d071bd11":"code","739a4fed":"code","624127c1":"code","99bca553":"code","d64ab8d7":"code","0feed530":"code","e1f49a6a":"code","84e680d3":"code","802856b5":"code","90b9119f":"code","e8f4497c":"code","5d9d3bd0":"code","8ca7bb94":"code","4b5b8e15":"code","eeececef":"code","acd54053":"code","cf9568a5":"code","40026110":"markdown","196c829c":"markdown","e373d326":"markdown","e8550ade":"markdown","8b342524":"markdown","3d8f2363":"markdown","4a6ac5d5":"markdown","c0d0a45f":"markdown","47a0b5df":"markdown","86b57d85":"markdown","b955b762":"markdown","593a86d7":"markdown","c0f3124c":"markdown","b77b4cac":"markdown","d7f12a6a":"markdown","4819c64e":"markdown","382b5e44":"markdown","eafdb28b":"markdown","ccded53f":"markdown","76682fc6":"markdown","a771d5a6":"markdown","32221ae4":"markdown","e0a4aa1e":"markdown","7a943b09":"markdown","8a1a38b4":"markdown"},"source":{"2ee70976":"import pandas as pd\nimport numpy as np\nfrom glob import glob\nfrom tqdm import tqdm\nimport cv2, os, shutil","51ef1ae9":"shutil.copytree('\/kaggle\/input\/dhakaai-3-fold-cv-yolo-dataset\/dhakaai', '\/kaggle\/working\/dhakaai')","2416da41":"# shutil.rmtree('\/kaggle\/working\/yolov5')\nos.chdir('\/kaggle\/working')","d071bd11":"# !git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n!git clone https:\/\/github.com\/awsaf49\/yolov5.git\n!pip install -qr yolov5\/requirements.txt  # install dependencies (ignore errors)\nos.chdir('\/kaggle\/working\/yolov5')\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\nfrom utils.google_utils import gdrive_download  # to download models\/datasets\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","739a4fed":"from os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ndef format_dataset_config(val_fold = 0,\n                cwd   = '\/kaggle\/working',\n                folds = [0, 1, 2]):\n    \n    path = join(cwd, 'dhakaai\/images\/')\n    ## processing\n    train_files = []\n    val_files   = []\n    test_files  = []\n    for fold in folds:\n        mypath = path + 'fold' + str(fold);\n        if fold == val_fold:       \n            val_files = [join(mypath,f) for f in listdir(mypath) if isfile(join(mypath, f))]\n        else:\n            onefoldfiles = [join(mypath,f) for f in listdir(mypath) if isfile(join(mypath, f))]\n            train_files += onefoldfiles\n    # test\n    mypath = path + 'test';\n    test_files += [join(mypath,f) for f in listdir(mypath) if isfile(join(mypath, f))]\n\n    print('\\nTotal Dataset:')\n    print(f'  train: {len(train_files)}')\n    print(f'  val: {len(val_files)}')\n    print(f'  test: {len(test_files)}')\n    \n    with open(join( cwd , 'train.txt'), 'w') as f:\n        for path in train_files:\n            f.write(path+'\\n')\n\n    with open(join( cwd , 'val.txt'), 'w') as f:\n        for path in val_files:\n            f.write(path+'\\n')\n            \n    data = dict(\n        train =  join( cwd , 'train.txt\/') ,\n        val   =  join( cwd , 'val.txt\/' ),\n        nc    = 21,\n        names = open(join( cwd , 'dhakaai\/classes.txt'), 'r').read().split('\\n')\n        )\n\n    with open(join( cwd , 'dhakaai.yaml'), 'w') as outfile:\n        yaml.dump(data, outfile, default_flow_style=False)\n        \n    f = open(join( cwd , 'dhakaai.yaml'), 'r')\n    print('\\nyaml:')\n    print(f.read())","624127c1":"hyp = \"\"\"\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum\/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+\/- deg)\ntranslate: 0.1  # image translation (+\/- fraction)\nscale: 0.5  # image scale (+\/- gain)\nshear: 0.0  # image shear (+\/- deg)\nperspective: 0.0  # image perspective (+\/- fraction), range 0-0.001\nflipud: 0.0  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 1.0  # image mosaic (probability)\nmixup: 0.0  # image mixup (probability)\n\"\"\"\nwith open('\/kaggle\/working\/hyp.yaml', 'w') as f:\n    yaml.dump(yaml.load(hyp, Loader = yaml.FullLoader), f)","99bca553":"fold = 0\nformat_dataset_config(val_fold = fold)\n\n# training\n!python train.py --epochs 50\\\n--img-size 640\\\n--batch-size 16\\\n--cfg models\/yolov5s.yaml\\\n--weights yolov5s.pt\\\n--data \/kaggle\/working\/dhakaai.yaml\\\n--cache\\\n--hyp '\/kaggle\/working\/hyp.yaml' ","d64ab8d7":"glob('runs\/train\/**\/*')","0feed530":"import matplotlib.pyplot as plt\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch0.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch1.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch2.jpg'))\n","e1f49a6a":"fig, ax = plt.subplots(3, 2, figsize = (2*5*1.8,3*2.5*2), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'runs\/train\/exp\/test_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'runs\/train\/exp\/test_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'runs\/train\/exp\/test_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'runs\/train\/exp\/test_batch{row}_pred.jpg', fontsize = 12)","84e680d3":"fold = 1\nformat_dataset_config(val_fold = fold)\n\n# training\n!python train.py --epochs 50\\\n--img-size 640\\\n--batch-size 16\\\n--cfg models\/yolov5x.yaml\\\n--weights yolov5s.pt\\\n--data \/kaggle\/working\/dhakaai.yaml\\\n--cache\\\n--hyp '\/kaggle\/working\/hyp.yaml' ","802856b5":"import matplotlib.pyplot as plt\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp2\/train_batch0.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp2\/train_batch1.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp2\/train_batch2.jpg'))\n","90b9119f":"fig, ax = plt.subplots(3, 2, figsize = (2*5*1.5,3*2.5*2), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'runs\/train\/exp2\/test_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'runs\/train\/exp2\/test_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'runs\/train\/exp2\/test_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'runs\/train\/exp2\/test_batch{row}_pred.jpg', fontsize = 12)","e8f4497c":"fold = 2\nformat_dataset_config(val_fold = fold)\n\n# training\n!python train.py --epochs 50\\\n--img-size 640\\\n--batch-size 16\\\n--cfg models\/yolov5s.yaml\\\n--weights yolov5s.pt\\\n--data \/kaggle\/working\/dhakaai.yaml\\\n--cache\\\n--hyp '\/kaggle\/working\/hyp.yaml' ","5d9d3bd0":"import matplotlib.pyplot as plt\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp3\/train_batch0.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp3\/train_batch1.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp3\/train_batch2.jpg'))\n","8ca7bb94":"fig, ax = plt.subplots(3, 2, figsize = (2*5*1.5,3*2.5*2), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'runs\/train\/exp3\/test_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'runs\/train\/exp3\/test_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'runs\/train\/exp3\/test_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'runs\/train\/exp3\/test_batch{row}_pred.jpg', fontsize = 12)","4b5b8e15":"!python detect.py --weights 'runs\/train\/exp\/weights\/best.pt' 'runs\/train\/exp2\/weights\/best.pt' 'runs\/train\/exp3\/weights\/best.pt'\\\n--img-size 640\\\n--conf 0.2\\\n--iou 0.5\\\n--source \/kaggle\/working\/dhakaai\/images\/test\\\n--dhakaai # for DhakaAI Submission format, submission.csv will be saved at the current directory","eeececef":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('runs\/detect\/exp\/*')\nfor _ in range(3):\n    row = 4\n    col = 4\n    grid_files = random.sample(files, row*col)\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","acd54053":"import pandas as pd\nsub = pd.read_csv('submission.csv')\nprint('Bboxes:', sub.shape[0])\nsub.to_csv('\/kaggle\/working\/submission.csv',index = False) # bring the submission.csv to the kaggle default current directory","cf9568a5":"shutil.rmtree('\/kaggle\/working\/dhakaai')\nshutil.rmtree('runs\/detect\/exp')\nos.remove('\/kaggle\/working\/dhakaai.yaml')\nos.remove('\/kaggle\/working\/val.txt')\nos.remove('\/kaggle\/working\/train.txt')","40026110":"## Parameters:\n* `--epochs` : number of epochs\n* `--img-size` : image_size\n* `--batch-size` : batch_size\n* `--cfg` : config of model # you may customize it for custom model\n* `--weights` : weights for the model , default value like `yolov5x.pt`will automatically be downloaded\n* `--data`: config of dataset\n* `--cache`: load all images into the memory, will speed up the training\n* `--hyp` : hyperparameters like augmentation","196c829c":"# Clone YOLOv5 [Repo](https:\/\/github.com\/ultralytics\/yolov5)\n* Turn ON the `Internet`\n* Select `GPU` as `Accelerator`\n\n**Official repo has some issues for kaggle:**\n* recently added `wandb` logger creates error\n* doesn't show all `logger` messages\n* included `DhakaAI` submission format in the `detect.py` (infer)\n","e373d326":"# Dataset\n\n### Credit\n* **Official Dataset:** [Link](https:\/\/www.kaggle.com\/rifat963\/dhakaai-dhaka-based-traffic-detection-dataset)\n\n### Fold Map:\n```\n{'Dipto': 0,\n'Asraf': 0,\n'Navid': 1,\n'Unknown': 1,\n'Pias': 2,\n'Numan': 2}\n```\n### Data Distribution in Fold:\n\n* **Fold0 :**   ` 918`\n* **Fold1 :**  `1042`\n* **Fold2 :**  `1040`\n\n\n### Class Map:\n```\n{'ambulance': 0,\n 'army vehicle': 1,\n 'auto rickshaw': 2,\n 'bicycle': 3,\n 'bus': 4,\n 'car': 5,\n 'garbagevan': 6,\n 'human hauler': 7,\n 'minibus': 8,\n 'minivan': 9,\n 'motorbike': 10,\n 'pickup': 11,\n 'policecar': 12,\n 'rickshaw': 13,\n 'scooter': 14,\n 'suv': 15,\n 'taxi': 16,\n 'three wheelers (CNG)': 17,\n 'truck': 18,\n 'van': 19,\n 'wheelbarrow': 20}\n```","e8550ade":"# Selecting Models\nIn this notebok I'm using `v5s`. To select your prefered model just replace `--cfg models\/yolov5s.yaml --weights yolov5s.pt` with the following command:\n* `v5s` : `--cfg models\/yolov5s.yaml --weights yolov5s.pt`\n* `v5m` : `--cfg models\/yolov5m.yaml --weights yolov5m.pt`\n* `v5l` : `--cfg models\/yolov5l.yaml --weights yolov5l.pt`\n* `v5x` : `--cfg models\/yolov5x.yaml --weights yolov5x.pt`","8b342524":"# Augmentation","3d8f2363":"### Train Batch Images","4a6ac5d5":"# Submission\n* Commit the notebook(`Save Version`)\n* After the training is done, download the `submission.csv` from the `output`\n","c0d0a45f":"# Training\nBefore we start please keep these points on mind,\n1. After each run a new `exp` will be created on `yolov5\/runs`. So, make sure you're selecting the correct one\n2. If you've performed experimented `3` then weights of that experiment will be in `yolov5\/runs\/exp3\/weights\/`\n3. GPU Notebook runtime is `9 hours` so choose your `model` and `img-size` accordingly\n4. If you're planning to train `v5x` on large `img-size` then you should train each fold seperately in seprate notebooks for `runtime` limitation\n3. If you're having unusual issues you may look [here](https:\/\/github.com\/ultralytics\/yolov5\/issues)","47a0b5df":"# Inferance`[Ensemble of Folds]`\n> Please select the weights correctly\n> Though the competition metric `Precision` and `COCO` metric aren't same but they're quite relatable\n> You can ensemble more then 3 models just adding their weights path\n\n* `--weights` : weights path to weights, to ensemble use multiple weights path side by side\n* `--img-size` : image_size to infer\n* `--conf` : minimum confidence threshold of bbox\n* `--iou` : IoU threshold for performing `nms`\n* `--source` : directory to take images for inference\n* `--dhakaai` : will create the submission file which you can submit directly to the website :)","86b57d85":"## Fold 1","b955b762":"# Data Config File\nThe Dataset has been rearanged for 3 Fold Cross Validation. Folds were created by considering annotator\nFor more details go to this [link](https:\/\/github.com\/ultralytics\/yolov5\/wiki\/Train-Custom-Data)\n","593a86d7":"### Train Batch Images","c0f3124c":"## Fold 2","b77b4cac":"### Train Batch Images","d7f12a6a":"### GT and Pred","4819c64e":"# Plot","382b5e44":"# Fix Dataset for Kaggle\n* Dataset is already in` YOLOv5` format\n* We need to copy the dataset to current directory for `write access` otherwise we'll get error\n* For Colab we don't need to run the following cell","eafdb28b":"## Fold 0","ccded53f":"### GT and Pred","76682fc6":"## Pretrained Checkpoints:\n\n| Model | AP<sup>val<\/sup> | AP<sup>test<\/sup> | AP<sub>50<\/sub> | Speed<sub>GPU<\/sub> | FPS<sub>GPU<\/sub> || params | FLOPS |\n|---------- |------ |------ |------ | -------- | ------| ------ |------  |  :------: |\n| [YOLOv5s](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 37.0     | 37.0     | 56.2     | **2.4ms** | **416** || 7.5M   | 13.2B\n| [YOLOv5m](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 44.3     | 44.3     | 63.2     | 3.4ms     | 294     || 21.8M  | 39.4B\n| [YOLOv5l](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 47.7     | 47.7     | 66.5     | 4.4ms     | 227     || 47.8M  | 88.1B\n| [YOLOv5x](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | **49.2** | **49.2** | **67.7** | 6.9ms     | 145     || 89.0M  | 166.4B\n| | | | | | || |\n| [YOLOv5x](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0) + TTA|**50.8**| **50.8** | **68.9** | 25.5ms    | 39      || 89.0M  | 354.3B\n| | | | | | || |\n| [YOLOv3-SPP](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0) | 45.6     | 45.5     | 65.2     | 4.5ms     | 222     || 63.0M  | 118.0B","a771d5a6":"# [YOLOv5](https:\/\/github.com\/ultralytics\/yolov5)\n![](https:\/\/user-images.githubusercontent.com\/26833433\/98699617-a1595a00-2377-11eb-8145-fc674eb9b1a7.jpg)\n![](https:\/\/user-images.githubusercontent.com\/26833433\/90187293-6773ba00-dd6e-11ea-8f90-cd94afc0427f.png)","32221ae4":"# Helpful Links:\n* Please go to this link for [Custom Data Training Tutorial](https:\/\/github.com\/ultralytics\/yolov5\/wiki\/Train-Custom-Data)\n* For better understanding go this [Tutorial](https:\/\/github.com\/ultralytics\/yolov5#tutorials)","e0a4aa1e":"# Removing Unnecessary files to keep the output short","7a943b09":"### GT and Pred","8a1a38b4":"# Importing Necessary Packages"}}