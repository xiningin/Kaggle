{"cell_type":{"b4abe41e":"code","84ab1729":"code","74647846":"code","c455a97d":"code","57929d42":"code","6289d70d":"code","c7f29808":"code","c7594f6a":"code","77b5938b":"code","cb8e8606":"code","31290acf":"code","54a87024":"code","7d6233d8":"markdown","c0bbb16e":"markdown"},"source":{"b4abe41e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84ab1729":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","74647846":"data = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\nprint('Data Size:', data.shape)\nprint('Any null values?:', data.isnull().sum().where(lambda row: row > 0).dropna().to_dict())\nprint('Feature Statistics:', '\\n', data.describe().T.iloc[:,1:])\n\n# remove highly correlated features\nDROP_THRESHOLD = 0.9\ncorr_matrix = data.corr()\none_sided_corr = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\ndrop_cols = [col for col in one_sided_corr.columns if any(one_sided_corr[col] > DROP_THRESHOLD)]\nprint('Columns to drop (highly correlated):', drop_cols)","c455a97d":"# convert categorical string labels into one-hot encoded vectors (or binary label for output) \nfrom sklearn.preprocessing import LabelEncoder\ndata['class'] = LabelEncoder().fit_transform(data['class'])\ndata = pd.get_dummies(data)","57929d42":"# scale input \"data\" for better linear relationship\nfrom sklearn.preprocessing import StandardScaler\ny = data['class']\ndata = pd.DataFrame(StandardScaler().fit_transform(data.drop('class', axis=1)), columns=data.drop('class', axis=1).columns.values)\ndata['class'] = y","6289d70d":"from sklearn.preprocessing import normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport random\n\ndata_norm = pd.DataFrame(normalize(data), columns=data.columns.values)\nkmeans = [KMeans(n_clusters=i+2, random_state=random.randrange(43)).fit(data_norm) for i in range(10)]\nscores = [silhouette_score(data_norm, km.labels_, metric='cosine') for km in kmeans]\noptimal_score = max(scores)\nclustering_index = scores.index(optimal_score)\n\nprint('How many clusters is optimal?', clustering_index+2)\nprint('Clustering Score:', optimal_score)","c7f29808":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n\ndata_norm = pd.DataFrame(MinMaxScaler().fit_transform(data_norm), columns=data_norm.columns.values)\npca = PCA(n_components=0.95, svd_solver='full').fit(data_norm)\ndata_norm_pca = pca.transform(data_norm)\n\nwith plt.style.context('dark_background'):\n    fig, ax = plt.subplots(1, 2, figsize=(10,5))\n    sns.scatterplot(ax=ax[0], x=data_norm_pca[:,0], y=data_norm_pca[:,1], hue=kmeans[clustering_index].labels_, palette='Set2')\n    ax[1].plot(np.arange(1, pca.n_components_+1), np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--', color='b')\n    plt.axis('tight')\n    ax[0].set_xlabel('PC0')\n    ax[0].set_ylabel('PC1')\n    ax[1].set_xlabel('Number of Components')\n    ax[1].set_ylabel('Cumulative Explained Variance (%)')\n    ax[0].get_legend().remove()","c7594f6a":"# extract most important feature variances for each cluster (by variance)\ndata_norm['cluster'] = kmeans[clustering_index].labels_\ndata_norm_cluster_means = data_norm.groupby('cluster').mean()\ndata_norm_feature_variances = pd.DataFrame([[col, np.var(data_norm_cluster_means[col])] for col in data_norm_cluster_means.columns[1:]], columns=['feature', 'variance'])\nfeatures = list(data_norm_feature_variances.sort_values('variance', ascending=False).head(7)['feature'].values)\nfeature_variances = data_norm[features + ['cluster']].melt(id_vars='cluster')\n\n# extract feature importances with random forest classifier (cluster label is output!)\nfrom sklearn.ensemble import RandomForestClassifier\nX, y = data_norm.drop('cluster', axis=1), data_norm['cluster']\nclf = RandomForestClassifier(n_estimators=100).fit(X,y)\nimportances = pd.DataFrame(np.array([clf.feature_importances_, X.columns]).T, columns=['importance', 'feature'])\nfeatures = list(importances.sort_values('importance', ascending=False).head(7)['feature'].values)\nfeature_importances = data_norm[features + ['cluster']].melt(id_vars='cluster')\n\nwith plt.style.context('dark_background'):\n    fig, ax = plt.subplots(2, 1, sharex=True, figsize=(10,10))\n    sns.barplot(ax=ax[0], x='cluster', y='value', hue='variable', data=feature_importances)\n    sns.barplot(ax=ax[1], x='cluster', y='value', hue='variable', data=feature_variances)\n    plt.axis('tight')\n    ax[0].set_title('Top Random Forest Feature Importances to k-Means Clustering Labels')\n    ax[1].set_title('Top Feature Variances to k-Means Clustering Labels')\n    plt.xlabel('Cluster')","77b5938b":"# split \"data\" into input and output (X, y)\nX = data.drop('class', axis=1)\ny = data['class'] # this is the \"class\" feature as output","cb8e8606":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","31290acf":"from sklearn.metrics import classification_report, roc_curve, auc\n\ndef predict(model):\n    print('Best Parameters:', model.best_params_)\n    print('Best CV Accuracy Score:', model.best_score_)\n    y_prob = model.predict_proba(X_test)[:,1]\n    y_pred = np.where(y_prob > 0.5, 1, 0)\n    model.score(X_test, y_pred)\n\n    class_rept = classification_report(y_test, y_pred)\n    fp_rate, tp_rate, _ = roc_curve(y_test, y_pred)\n    roc_auc = auc(fp_rate, tp_rate)\n    \n    print('Model Classification Report:', '\\n', class_rept)\n    with plt.style.context('dark_background'):\n        plt.plot(fp_rate, tp_rate, color='red', linestyle='--', label=f'AUC = {roc_auc:.2f}')\n        plt.axis('tight')\n        plt.title('Receiver Operating Characteristic')\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.legend(loc='lower right')","54a87024":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nESTIMATORS = {\n    'lr':{\n        'model':LogisticRegression,\n        'params':[\n            {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000], \n             'penalty':['l2'], \n             'solver':['lbfgs']},\n            {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000], \n             'penalty':['l1'], \n             'solver':['liblinear']}]},\n    'nb':{\n        'model':GaussianNB,\n        'params':{\n            'var_smoothing':np.logspace(0,-9,num=100)}},\n    'svm':{\n        'model':SVC,\n        'params':[\n            {'C':[1, 10, 100, 500, 1000], \n             'kernel':['linear', 'rbf']},\n            {'C':[1, 10, 100, 500, 1000], \n             'gamma':[1, 0.1, 0.01, 0.001, 0.0001], \n             'kernel':['rbf']},\n            {'C':[1, 10, 100, 500, 1000], \n             'degree':[2, 3, 4, 5, 6], \n             'kernel':['poly']}]},\n    'rf':{\n        'model':RandomForestClassifier,\n        'params':{\n            'n_estimators':range(10,100,10),\n            'min_samples_leaf':range(10,100,10),\n            'max_depth':range(5,15,5),\n            'max_features':['auto', 'sqrt', 'log2']}},\n    'tree':{\n        'model':DecisionTreeClassifier,\n        'params':{\n            'criterion':['gini', 'entropy'],\n            'max_features':['auto', 'sqrt', 'log2'],\n            'min_samples_leaf':range(1,100,1),\n            'max_depth':range(1,50,1)}},\n    'mlp':{\n        'model':MLPClassifier,\n        'params':{\n            'hidden_layer_sizes':range(1,200,10),\n            'activation':['tanh', 'logistic', 'relu'],\n            'alpha':[0.0001, 0.001, 0.01, 0.1, 1, 10],\n            'max_iter':range(50,200,50)}}}\n\nfor estimator in ESTIMATORS.values():\n    grid = RandomizedSearchCV(\n        estimator['model'](), \n        estimator['params'], \n        cv=10, scoring='accuracy', n_jobs=-1, n_iter=20)\n    grid.fit(X_train, y_train)\n    predict(grid)","7d6233d8":"# **SUPERVISED LEARNING ALGORITHMS**","c0bbb16e":"# **Clustering Comparison**"}}