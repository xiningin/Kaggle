{"cell_type":{"b5f58c6a":"code","70775223":"code","58a2af1a":"code","7eaefdde":"code","b93840e3":"code","75960552":"code","0143330e":"code","126bf5aa":"code","13fd7779":"code","f92233b5":"code","084ede26":"code","f9578242":"code","b00ffacb":"code","6b0420a9":"code","1500c5a0":"code","8711c26c":"code","0fe1a972":"code","dc049dcf":"code","aac8e1a4":"code","ede43fa3":"code","30b31589":"code","0f9daf68":"code","f3d0cbbb":"code","43bb71df":"code","ed08bd97":"code","1367cfa2":"code","d330c7d6":"code","abf96d79":"code","da8d5fe6":"code","c4d46164":"code","551babfd":"code","2390f58c":"code","a2f5ebda":"code","d01030f5":"code","64aa8988":"code","ff8111b1":"code","06c57a0b":"code","b56c462c":"code","6589f665":"code","1300e302":"code","064d9f6f":"code","dc1811db":"code","6702e92c":"code","aae19a64":"code","8119d335":"code","9d829345":"code","778c051a":"code","a6ed75ea":"code","fc24966a":"code","feccd600":"code","9073baa5":"code","237933e7":"code","ab957275":"code","cb4db43d":"code","9360d3a9":"code","49cf0df8":"code","2e94a07c":"code","0f58aa44":"code","bdb7f3d1":"code","4fbd51ba":"code","45f77f12":"code","f1e63b51":"code","0996c9ae":"code","c5ae5c1a":"code","ec757910":"code","6410b2f9":"code","0b08083f":"code","d58605fa":"code","6867edc9":"code","18a4d945":"code","be5b2f06":"code","13fd7f7c":"code","54b88b71":"code","a646fe9c":"code","f65924e7":"code","45cd4bbf":"code","60922761":"code","b792f0a5":"code","96372681":"code","cebe9719":"code","a866a406":"code","f87d6c8a":"code","818c9326":"code","87006804":"code","a38acb41":"code","519038d3":"code","c490356d":"code","91dbfc6c":"code","882d292c":"code","f770c34c":"code","3515ff3e":"code","8d85f4f8":"code","de3d25c4":"code","d56121fe":"code","4325c9c9":"markdown","89f77d6f":"markdown","c69d07b8":"markdown","664fe657":"markdown","6da129a2":"markdown","849de512":"markdown","5c0b94d6":"markdown","2b981218":"markdown","7ecd2692":"markdown","a8b023e8":"markdown","4cb60c6b":"markdown","120e5321":"markdown","ee73a0bf":"markdown","8947f2c2":"markdown","4d26b9c2":"markdown","368cde0d":"markdown","56750bd2":"markdown","bf44854f":"markdown","4e26cb9f":"markdown","249410ee":"markdown","b110884d":"markdown","a3cecc31":"markdown","2c356f10":"markdown","08fff4a8":"markdown","f834440c":"markdown","1d91e732":"markdown","a7dce9e1":"markdown","f2fb35d9":"markdown","ef30481f":"markdown","87de9336":"markdown","3ad02812":"markdown","29f5ec22":"markdown","2bb87260":"markdown"},"source":{"b5f58c6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","70775223":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nfrom collections import Counter","58a2af1a":"y_2018 = pd.read_csv(\"..\/input\/world-happiness\/2018.csv\")\ny_2019 = pd.read_csv(\"..\/input\/world-happiness\/2019.csv\")\n\ndata = pd.concat([y_2018,y_2019], sort = False)\ndata","7eaefdde":"data.isnull().sum()","b93840e3":"data.describe().T #statistical information about the data set ","75960552":"data.info() ","0143330e":"data.rename(columns={\n    \"Overall rank\": \"rank\",\n    \"Country or region\": \"country\",\n    \"Score\": \"score\",\n    \"GDP per capita\": \"gdp\",\n    \"Social support\": \"social\",\n    \"Healthy life expectancy\": \"healthy\",\n    \"Freedom to make life choices\": \"freedom\",\n    \"Generosity\": \"generosity\",\n    \"Perceptions of corruption\": \"corruption\"\n},inplace = True)\ndel data[\"rank\"]","126bf5aa":"data.columns[data.isnull().any()]","13fd7779":"data.isnull().sum()","f92233b5":"data[data[\"corruption\"].isnull()]","084ede26":"avg_data_corruption = data[data[\"score\"] > 6.774].mean().corruption\ndata.loc[data[\"corruption\"].isnull(),[\"corruption\"]] = avg_data_corruption\ndata[data[\"corruption\"].isnull()]","f9578242":"df = data.copy()\ndf = df.select_dtypes(include=[\"float64\",\"int64\"])\ndf.head()","b00ffacb":"column_list = [\"score\",\"gdp\",\"social\",\"healthy\",\"freedom\",\"generosity\",\"corruption\"]","6b0420a9":"column_list = [\"score\",\"gdp\",\"social\",\"healthy\",\"freedom\",\"generosity\",\"corruption\"]\nsns.heatmap(df[column_list].corr(), annot = True, fmt = \".2f\") #annot=True dersek minik karelerin i\u00e7inde coorelation skorlar\u0131n\u0131 da g\u00f6rm\u00fc\u015f oluruz (daha kolay anlayabilmek i\u00e7in)\nplt.show()","1500c5a0":"g = sns.factorplot(x = \"score\", y = \"gdp\", data = df, kind = \"bar\", size = 5)\ng.set_ylabels(\"GDP per capita\")\nplt.show()","8711c26c":"for col in column_list:\n    sns.boxplot(x = df[col])\n    plt.xlabel(col)\n    plt.show()","0fe1a972":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers\n\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1","dc049dcf":"lower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"lower bound is\" + str(lower_bound))\nprint(\"upper bound is\" + str(upper_bound))\nprint(\"Q1: \", Q1)\nprint(\"Q3: \", Q3)","aac8e1a4":"df.loc[detect_outliers(df,[\"score\",\"gdp\",\"social\",\"healthy\",\"freedom\",\"generosity\",\"corruption\"])]","ede43fa3":"# for corruption\ndf_table = df[\"corruption\"]\n\nQ1 = df_table.quantile(0.25)\nQ3 = df_table.quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"lower bound is \" + str(lower_bound))\nprint(\"upper bound is \" + str(upper_bound))\nprint(\"Q1: \", Q1)\nprint(\"Q3: \", Q3)","30b31589":"outliers_vector = (df_table < (lower_bound)) | (df_table > (upper_bound))\noutliers_vector","0f9daf68":"outliers_vector = df_table[outliers_vector]\noutliers_vector.index.values","f3d0cbbb":"df_table = data.copy()\ndf_table[\"corruption\"].iloc[outliers_vector.index.values] = df_table[\"corruption\"].mean()\ndf_table[\"corruption\"].iloc[outliers_vector.index.values]\n","43bb71df":"data = df_table","ed08bd97":"sns.jointplot(x = \"gdp\", y = \"score\", data = df_table, kind = \"reg\")\nplt.show()","1367cfa2":"from sklearn.linear_model import LinearRegression\n\nX = data[[\"gdp\"]]\nX.head","d330c7d6":"y = data[[\"score\"]]\ny.head","abf96d79":"reg = LinearRegression()\nmodel = reg.fit(X,y)\nprint(\"intercept: \", model.intercept_)\nprint(\"coef: \", model.coef_)\nprint(\"rcore. \", model.score(X,y))","da8d5fe6":"# prediction\nplt.figure(figsize = (10,8))\ng = sns.regplot(x = data[\"gdp\"], y = data[\"score\"], ci = None, scatter_kws = {'color':'r','s':9})\ng.set_title(\"Model Equation\")\ng.set_xlabel(\"gdp\")\ng.set_ylabel(\"score\")\nplt.show()","c4d46164":"model.predict([[1.50]])","551babfd":"gdb_list = [[0.25],[0.50],[0.75],[1.00],[1.25],[1.50]]\nmodel.predict(gdb_list)\nfor g in gdb_list:\n    print(\"The happiness value of the country with a gdp value of \",g,\": \",model.predict([g]))","2390f58c":"sns.jointplot(x = \"social\", y = \"score\", data = df_table, kind = \"reg\")\nplt.show()","a2f5ebda":"def linear_reg(col,text,prdctn):\n    \n    sns.jointplot(x=col,y=\"score\",data=df_table,kind=\"reg\")\n    plt.show()\n    \n    X = data[[col]]\n    y = data[[\"score\"]]\n    reg = LinearRegression()\n    model = reg.fit(X,y)\n    \n    # prediction\n    plt.figure(figsize=(12,6))\n    g = sns.regplot(x=data[col],y=data[\"score\"],ci=None,scatter_kws = {'color':'r','s':9})\n    g.set_title(\"Model Equation\")\n    g.set_ylabel(\"score\")\n    g.set_xlabel(col)\n    plt.show()\n    \n    print(text,\": \", model.predict([[prdctn]]))","d01030f5":"linear_reg(\"social\",\"The happiness value of the country whose sociability value is 2:\",2)","64aa8988":"linear_reg(\"healthy\",\"The happiness value of the country whose healthiest value is 1.20:\",1.20)","ff8111b1":"linear_reg(\"freedom\",\"The happiness value of the country whose freedom value is 1.20:\",1.20)","06c57a0b":"import statsmodels.api as sms\n\nX = df.drop(\"score\", axis = 1)\ny = df[\"score\"]\n\n# OLS (dependent,independent)\nlm = sms.OLS(y,X)\nmodel = lm.fit()\nmodel.summary()","b56c462c":"lm = LinearRegression()\nmodel = lm.fit(X,y)\nprint(\"constant: \", model.intercept_)\nprint(\"coefficient: \", model.coef_)","6589f665":"# PREDICTION\n# Score = 0.929921*gdp + 1.06504217*social + 0.94321492*healthy + 1.40426054*freedom + 0.52070628*generosity + 0.88114008*corruption\n\nnew_data = [[1],[2],[1.25],[1.75],[1.50],[0.75]]\nnew_data = pd.DataFrame(new_data).T\nnew_data","1300e302":"model.predict(new_data)","064d9f6f":"# calculating the amount of error\n\nfrom sklearn.metrics import mean_squared_error\n\nMSE = mean_squared_error(y,model.predict(X))\nRMSE = np.sqrt(MSE)\n\nprint(\"MSE: \", MSE)\nprint(\"RMSE: \", RMSE)","dc1811db":"from sklearn.model_selection import train_test_split\nX = df.drop(\"score\", axis = 1)\ny = df[\"score\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nX_train.head()","6702e92c":"X_test.head()","aae19a64":"y_train.head()","8119d335":"y_test.head()","9d829345":"lm = LinearRegression()\nlm.fit(X_train, y_train)\nprint(\"Training error: \", np.sqrt(mean_squared_error(y_train, model.predict(X_train))))\nprint(\"Test Error: \", np.sqrt(mean_squared_error(y_test, model.predict(X_test))))","778c051a":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")","a6ed75ea":"cvs_avg_mse = np.mean(-cross_val_score(model, X_train, y_train, cv = 20, scoring = \"neg_mean_squared_error\"))\ncvs_avg_rmse = np.sqrt(cvs_avg_mse)\n\nprint(\"Cross Val Score MSE : \",cvs_avg_mse)\nprint(\"Cross Val Score RMSE : \",cvs_avg_rmse)","fc24966a":"import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV","feccd600":"X = df.drop(\"score\", axis = 1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nridge_model = Ridge(alpha = 0.1).fit(X_train, y_train)\nridge_model","9073baa5":"ridge_model.coef_","237933e7":"ridge_model.intercept_","ab957275":"lambdas = 10**np.linspace(10,-2,100)*0.5 # Creates random numbers\nridge_model =  Ridge()\ncoefs = []\n\nfor i in lambdas:\n    ridge_model.set_params(alpha=i)\n    ridge_model.fit(X_train,y_train)\n    coefs.append(ridge_model.coef_)\n    \nax = plt.gca()\nax.plot(lambdas, coefs)\nax.set_xscale(\"log\")","cb4db43d":"ridge_model = Ridge().fit(X_train,y_train)\n\ny_pred = ridge_model.predict(X_train)\n\nprint(\"predict: \", y_pred[0:10])\nprint(\"real: \", y_train[0:10].values)","9360d3a9":"RMSE = np.mean(mean_squared_error(y_train,y_pred))\nprint(\"train error: \", RMSE)","49cf0df8":"from sklearn.model_selection import cross_val_score","2e94a07c":"Verified_RMSE = np.sqrt(np.mean(-cross_val_score(ridge_model, X_train, y_train, cv=20, scoring=\"neg_mean_squared_error\")))\nprint(\"Verified_RMSE: \", Verified_RMSE)","0f58aa44":"#test error\ny_pred = ridge_model.predict(X_test)\nRMSE = np.mean(mean_squared_error(y_test, y_pred))\nprint(\"test error: \", RMSE)","bdb7f3d1":"ridge_model = Ridge(10).fit(X_train, y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","4fbd51ba":"ridge_model = Ridge(30).fit(X_train, y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","45f77f12":"ridge_model = Ridge(90).fit(X_train, y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","f1e63b51":"lambdas1 = 10**np.linspace(10,-2,100)\nlambdas2 = np.random.randint(0,1000,100)\n\nridgeCV = RidgeCV(alphas = lambdas1,scoring = \"neg_mean_squared_error\", cv=10, normalize=True)\nridgeCV.fit(X_train,y_train)","0996c9ae":"ridgeCV.alpha_","c5ae5c1a":"# final model\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train, y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","ec757910":"# for lambdas2\nridgeCV = RidgeCV(alphas = lambdas2, scoring = \"neg_mean_squared_error\", cv = 10, normalize = True)\nridgeCV.fit(X_train, y_train)\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train, y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","6410b2f9":"# Required Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV, LassoCV","0b08083f":"x = df.drop(\"score\", axis = 1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\nlasso_model = Lasso().fit(X_train, y_train)","d58605fa":"print(\"intercept: \", lasso_model.intercept_)\nprint(\"coef: \", lasso_model.coef_)","6867edc9":"# coefficients for different lambda values\n\nalphas = np.random.randint(0,10000,10)\nlasso = Lasso()\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(X_train,y_train)\n    coefs.append(lasso.coef_)","18a4d945":"ax = plt.gca()\nax.plot(alphas,coefs)\nax.set_xscale(\"log\")","be5b2f06":"lasso_model","13fd7f7c":"lasso_model.predict(X_train)[0:5]","54b88b71":"lasso_model.predict(X_test)[0:5]","a646fe9c":"y_pred = lasso_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","f65924e7":"r2_score(y_test,y_pred)","45cd4bbf":"lasso_cv_model = LassoCV(cv = 10, max_iter = 100000).fit(X_train, y_train)\nlasso_cv_model","60922761":"lasso_cv_model.alpha_","b792f0a5":"lasso_tuned = Lasso().set_params(alpha= lasso_cv_model.alpha_).fit(X_train,y_train)\ny_pred = lasso_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","96372681":"# Required Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge,Lasso,ElasticNet\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV","cebe9719":"X = df.drop(\"score\",axis=1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nenet_model = ElasticNet().fit(X_train,y_train)","a866a406":"enet_model.coef_","f87d6c8a":"enet_model.intercept_","818c9326":"#prediction\nenet_model.predict(X_train)[0:10]","87006804":"enet_model.predict(X_test)[0:10]","a38acb41":"y_pred = enet_model.predict(X_test)","519038d3":"np.sqrt(mean_squared_error(y_test, y_pred))","c490356d":"r2_score(y_test, y_pred)","91dbfc6c":"from sklearn.linear_model import ElasticNetCV","882d292c":"enet_cv_model = ElasticNetCV(cv = 10, random_state = 0).fit(X_train, y_train)","f770c34c":"enet_cv_model.alpha_","3515ff3e":"enet_cv_model","8d85f4f8":"enet_tuned = ElasticNet(alpha = enet_cv_model.alpha_).fit(X_train, y_train)","de3d25c4":"y_pred = enet_tuned.predict(X_test)","d56121fe":"np.sqrt(mean_squared_error(y_test, y_pred))","4325c9c9":"### score -- freedom","89f77d6f":"### Lasso Regression Model Tuning","c69d07b8":"### Variable Description","664fe657":"### Lasso Regression - Model Tuning","6da129a2":"### Missing Value","849de512":"* Hello! We will practice about Linear Regression for myself-improvement. I hope this notebook will be useful to you. Lets get start :)","5c0b94d6":"<a id = \"1\"><\/a><br>\n## Load and Check Data","2b981218":"### Ridge Regression - Prediction","7ecd2692":"* Every time we change the random_state value we defined at first, a different result is returned. We need to find out which of these returns the best result. For this we need to do the following.","a8b023e8":"# Simple Linear Regression","4cb60c6b":"### Data Preparation","120e5321":"# ElasticNet Regression","ee73a0bf":"### Lasso Regression - Prediction ","8947f2c2":"# Introduction","4d26b9c2":"1. Overall rank: Overall rank: Ranking of countries by happiness level\n1. Country or region: Country or region names\n1. Score: Happiness scores\n1. GDP per capita: Value representing the country's income and expense levels\n1. Social support\n1. Healthy life expectancy\n1. Freedom to make life choices\n1. Generosity\n1. Perceptions of corruption ","368cde0d":"* R-squared   :   Percentages of independent variables that explain the change in dependent variables.\n* F-statistic :   Expresses the significance of the model.\n* Coef        :   Refers to coefficients.\n* Std Err     :   Standard errors.","56750bd2":"# Ridge Regression\n<br>\n* The aim is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients.\n<br>\n* It is resistant to over learning.\n* It is biased but its variance is low.\n* It is better than OLS when there are too many parameters.\n* Builds a model with all variables. It does not exclude the unrelated variables from the model, it approximates its coefficients to zero.\n","bf44854f":"#### Create model with sckit learn","4e26cb9f":"### score -- social","249410ee":"* If gdp score is 1.50 , happines score is 6.74","b110884d":"* gdp feature used here describes 63% of the data.","a3cecc31":"* We observed outlier detection with boxplot in corruption and social features. But we can observed this features with outlier detection.  ","2c356f10":"# Lasso Regression -- Model","08fff4a8":"# Simple Linear & Multiple Linear Regression - Model Tuning","f834440c":"# Lasso Regression","1d91e732":"### Ridge Model -- Model Tuning","a7dce9e1":"### score -- healthy","f2fb35d9":"* Let's create a class and make the job easier.","ef30481f":"* Lets change the column names for convenience.","87de9336":"* We can find out which value will work better by trial and error. But with the method we will use below, we can find the most appropriate value more easily and quickly.","3ad02812":"# Multiple Linear Regression","29f5ec22":"# Contents\n1. [Load and Check Data](#1)","2bb87260":"### score -- gdp"}}