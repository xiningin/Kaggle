{"cell_type":{"b0690f7e":"code","d17bab36":"code","e9a8137b":"code","7375e75a":"code","7c249177":"code","95caf374":"code","dce3c115":"code","3cbfe3be":"code","a59442ab":"code","b62dd72d":"code","35e6a7b0":"code","d5ff558e":"markdown","8ee68519":"markdown","73d2510f":"markdown","461dc12e":"markdown","d57b5e08":"markdown","721f7cca":"markdown"},"source":{"b0690f7e":"%%capture\n!pip install pycocotools\n!pip install --upgrade wandb\n\nimport multiprocessing as mp\nimport random\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\n\n# from deepspeed.ops.adam import FusedAdam\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import io, transforms\nimport torchvision.datasets as dset\nfrom tqdm.auto import tqdm\nfrom transformers import EncoderDecoderModel, GPT2Tokenizer, ViTFeatureExtractor\n\n# Wandb login:\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=secret_value)","d17bab36":"VIT_MODEL = \"google\/vit-base-patch16-224-in21k\"\nGPT2 = \"gpt2\"\nDISTIL_GPT2 = \"distilgpt2\"\n\nDATA_PATH = \"\/kaggle\/input\/coco-2017-dataset\/coco2017\/\"\nMODEL_PATH = \"\/kaggle\/working\/models\/\"\nANNOTATION_PATH = DATA_PATH + \"annotations\/captions_train2017.json\"\n\nVALID_LOSS = \"Validation loss\"\n\n\nMEAN = 0.5\nSTD = 0.5\nIMAGE_SIZE = (224, 224)\n\nTRAIN_PCT = 0.95\nNUM_WORKERS = mp.cpu_count()\nBATCH_SIZE = 16\nEPOCHS = 10\nLR = 1e-4\n\nMAX_TEXT_LENGTH = 32\n\nLABEL_MASK = -100\n\nTOP_K = 1000\nTOP_P = 0.95","e9a8137b":"tfms = transforms.Compose(\n    [\n        transforms.Resize(IMAGE_SIZE),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomApply([transforms.RandomRotation(degrees=20)], p=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=MEAN, std=STD)\n   ]\n)\ndescale = transforms.Compose(\n    [\n        transforms.Normalize(\n            mean = [ 0., 0., 0. ],\n            std = 1 \/ STD\n        ),\n        transforms.Normalize(\n            mean = -MEAN,\n            std = [ 1., 1., 1. ]\n        ),                           \n    ]\n)\n\ntarget_tfm = lambda x: random.choice(x)\n\ncoco_dataset = dset.CocoCaptions(root = DATA_PATH + \"train2017\/\",\n                        annFile = ANNOTATION_PATH,\n                        transform=tfms,\n                        target_transform=target_tfm,\n)\n\nprint('Number of samples: ', len(coco_dataset))\nimg, caption = coco_dataset[3] # load 4th sample","7375e75a":"print(caption)\ntransforms.ToPILImage()(descale(img))","7c249177":"train_len = int(TRAIN_PCT * len(coco_dataset))\ntrain_data, valid_data = random_split(coco_dataset, [train_len, len(coco_dataset) - train_len])\ntrain_dl = DataLoader(\n    train_data, \n    BATCH_SIZE, \n    pin_memory=True, \n    shuffle=True, \n    num_workers=NUM_WORKERS, \n    drop_last=True\n)\nvalid_dl = DataLoader(\n    valid_data, \n    BATCH_SIZE, \n    pin_memory=True, \n    shuffle=False, \n    num_workers=NUM_WORKERS, \n    drop_last=False\n)\n\nimages, captions = next(iter(train_dl))\nimages.shape, images.min(), images.max(), images.mean(), images.std()","95caf374":"# make sure GPT2 appends EOS in begin and end\ndef build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n    return outputs\n    \nGPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\ngpt2_tokenizer = GPT2Tokenizer.from_pretrained(DISTIL_GPT2)\n# set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\ngpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n\ngpt2_tokenizer_fn = lambda x: gpt2_tokenizer(\n    x,\n    max_length=MAX_TEXT_LENGTH,\n    truncation=True,\n    padding=True,\n    return_tensors=\"pt\",\n)","dce3c115":"def top_k_top_p_filtering(\n    next_token_logits: torch.FloatTensor,\n    top_k: Optional[float]=None, \n    top_p: Optional[float]=None,\n    device: Union[str, torch.device]=\"cpu\",\n) -> torch.FloatTensor:\n    if top_k is None:\n        top_k = next_token_logits.shape[-1]\n    if top_p is None:\n        top_p = 1.0\n        \n    p, largest_p_idx = F.softmax(next_token_logits, dim=-1).topk(top_k, dim=-1)\n    cumulative_p = p.cumsum(dim=-1)\n    threshold_repeated = top_p + torch.zeros((len(p),1)).to(device)\n    idx = torch.searchsorted(cumulative_p, threshold_repeated).clip(max=top_k-1).squeeze()\n    cutoffs = cumulative_p[torch.arange(len(cumulative_p)), idx]\n    censored_p = (cumulative_p <= cutoffs[:, None]) * p\n    renormalized_p = censored_p \/ censored_p.sum(dim=-1, keepdims=True)\n    \n    final_p = torch.zeros_like(next_token_logits)\n    row_idx = torch.arange(len(p)).unsqueeze(1).repeat(1,top_k).to(device)\n    final_p[row_idx, largest_p_idx] = renormalized_p.to(final_p.dtype)\n\n    return final_p\n\ndef generate_sentence_from_image(model, encoder_outputs, tokenizer, max_text_length: int, device)-> List[str]:\n    generated_so_far = torch.LongTensor([[tokenizer.bos_token_id]]*len(encoder_outputs.last_hidden_state)).to(device)\n    with torch.no_grad():\n        for _ in tqdm(range(max_text_length)):\n            attention_mask = torch.ones_like(generated_so_far)\n            decoder_out = model(\n                decoder_input_ids=generated_so_far, \n                decoder_attention_mask=attention_mask,\n                encoder_outputs=encoder_outputs\n            )\n\n            next_token_logits = decoder_out[\"logits\"][:, -1, :]\n            filtered_p = top_k_top_p_filtering(next_token_logits, top_k=TOP_K, top_p=TOP_P, device=device)\n            next_token = torch.multinomial(filtered_p, num_samples=1)\n            generated_so_far = torch.cat((generated_so_far, next_token), dim=1)\n\n    return [tokenizer.decode(coded_sentence) for coded_sentence in generated_so_far]","3cbfe3be":"class LightningModule(pl.LightningModule):\n    def __init__(\n        self,\n        model: nn.Module,\n        tokenizer,\n        lr: float,\n    ):\n        super().__init__()\n        self.model = model\n        self.tokenizer = tokenizer\n        self.lr = lr\n        \n        for name, param in self.model.named_parameters():\n            if \"crossattention\" not in name:\n                param.requires_grad = False\n        \n    def common_step(self, batch: Tuple[torch.FloatTensor, List[str]]) -> torch.FloatTensor:\n        images, captions = batch\n        tokenized_captions = {\n            k: v.to(self.device) for k, v in \n            self.tokenizer(\n                captions,\n                max_length=MAX_TEXT_LENGTH,\n                truncation=True,\n                padding=True,\n                return_tensors=\"pt\",\n            ).items()\n        }\n        labels = tokenized_captions[\"input_ids\"].clone()\n        labels[tokenized_captions[\"attention_mask\"]==0] = LABEL_MASK\n        encoder_outputs = self.model.encoder(pixel_values=images)\n        outputs = self.model(\n            encoder_outputs=encoder_outputs,\n            decoder_input_ids=tokenized_captions[\"input_ids\"],\n            decoder_attention_mask=tokenized_captions[\"attention_mask\"],\n            labels=labels,\n            return_dict=True,\n        )\n        \n        return outputs[\"loss\"]\n    \n    def training_step(self, batch: Tuple[torch.FloatTensor, List[str]], batch_idx: int) -> torch.FloatTensor:\n        loss = self.common_step(batch)\n        self.log(name=\"Training loss\", value=loss, on_step=True, on_epoch=True)\n        \n        return loss\n        \n    def validation_step(self, batch: Tuple[torch.FloatTensor, List[str]], batch_idx: int):\n        loss = self.common_step(batch)\n        self.log(name=VALID_LOSS, value=loss, on_step=True, on_epoch=True)\n\n        images, actual_sentences = batch\n        \n        if batch_idx == 0:\n            encoder_outputs = self.model.encoder(pixel_values=images.to(self.device))\n            generated_sentences = generate_sentence_from_image(\n                self.model, \n                encoder_outputs, \n                self.tokenizer, \n                MAX_TEXT_LENGTH,\n                self.device\n            )\n            images = [wandb.Image(transforms.ToPILImage()(descale(image))) for image in images]\n            data = list(map(list, zip(images, actual_sentences, generated_sentences)))\n            columns = [\"Images\", \"Actual Sentence\", \"Generated Sentence\"]\n            table = wandb.Table(data=data, columns=columns)\n            self.logger.experiment.log({f\"epoch {self.current_epoch} results\": table})\n                        \n    def on_after_backward(self):\n        if self.trainer.global_step % 50 == 0:  # don't make the tf file huge\n            for name, param in self.model.named_parameters():\n                if \"crossattention\" in name and not \"norm\" in name and param.requires_grad:\n                    self.logger.experiment.log(\n                        {f\"{name}_grad\": wandb.Histogram(param.grad.detach().cpu())}\n                    )\n                    self.logger.experiment.log(\n                        {f\"{name}\": wandb.Histogram(param.detach().cpu())}\n                    )\n                    \n    def on_train_epoch_end(self, *args):\n        if self.current_epoch == 0:\n            for p in self.model.parameters(): p.requires_grad = True\n            print(\"unfroze base\")\n            \n            self.optimizers().optimizer.param_groups[0][\"lr\"] = self.lr \/ 10\n\n    def configure_optimizers(self) -> torch.optim.Optimizer:\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n        return optimizer\n","a59442ab":"!mkdir -p \/kaggle\/working\/logs\nvit2gpt2 = EncoderDecoderModel.from_encoder_decoder_pretrained(VIT_MODEL, DISTIL_GPT2)\nlightning_module = LightningModule(\n    vit2gpt2,\n    gpt2_tokenizer,\n    LR\n)\n\ncheckpoint_callback = pl.callbacks.ModelCheckpoint(\n    monitor=VALID_LOSS,\n    dirpath=MODEL_PATH,\n    filename=\"{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=3,\n    mode=\"min\",\n    save_weights_only=True,\n)\nearly_stopping_callback = pl.callbacks.EarlyStopping(\n    monitor=VALID_LOSS,\n    mode=\"min\",\n    verbose=True,\n)\ncallbacks = [checkpoint_callback, early_stopping_callback]\nlogger = WandbLogger(\"Frozen\", \"\/kaggle\/working\/logs\/\", project=\"Vit2GPT2\")\n\ntrainer = pl.Trainer(\n#     max_epochs=EPOCHS,\n    max_epochs=1,\n    gpus=torch.cuda.device_count(),\n    gradient_clip_val=1.0,\n    logger=logger,\n    precision=16,\n    callbacks=callbacks,\n    limit_train_batches=5,\n    limit_val_batches=5,\n    num_sanity_val_steps=0,\n)\ntrainer.fit(lightning_module, train_dl, valid_dl)","b62dd72d":"logger.experiment.log_artifact(\n    artifact_or_path=callbacks[0].best_model_path,\n    name=\"vit2gpt2_model\",\n    type=\"model\",\n)","35e6a7b0":"trained_state_dict = torch.load(callbacks[0].best_model_path)[\"state_dict\"]\nclass Model(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        \n    def forward(self, x):\n        return self.model(x)\n    \nmodel_wrapper = Model(vit2gpt2)\nmodel_wrapper.load_state_dict(trained_state_dict)","d5ff558e":"Below shows how to load the model","8ee68519":"## Config","73d2510f":"## Nucleus Sampling\n[Paper](https:\/\/arxiv.org\/pdf\/1904.09751.pdf)","461dc12e":"## Training Module (PyTorch Lightning)","d57b5e08":"## Data","721f7cca":"## Model"}}