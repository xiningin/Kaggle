{"cell_type":{"f7b3a69c":"code","caf29276":"code","a0e5332e":"code","f0186a30":"code","4c4f5cfd":"code","daa96b17":"code","d6a0ec14":"code","ce1d5953":"code","a0531397":"code","b036fb95":"code","488e22a7":"code","22442599":"code","e33fc2c6":"code","44a33f8a":"code","d0e680ef":"code","33dbb42c":"code","fca6c450":"code","2e8e060c":"code","180f362e":"code","a1322bb3":"code","c5b16a71":"code","f00417bd":"code","b1edaa39":"code","8d9a57bd":"code","2efd52e3":"code","8eee0394":"code","85fa1f32":"code","6074c56f":"markdown","769cb08e":"markdown","d600d3fb":"markdown","10321b61":"markdown"},"source":{"f7b3a69c":"import pandas as pd","caf29276":"dataset_train1 = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', index_col='id')\ndataset_test1 = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', index_col='id')\ny = dataset_train1.claim\ny = pd.DataFrame(y)\ndataset_train = dataset_train1.drop(['claim'], axis=1)","a0e5332e":"dataset_train['f10_ratio_f9'] = dataset_train['f10']\/dataset_train['f9']\ndataset_train['f92_ratio_f73'] = dataset_train['f92']\/dataset_train['f73']\ndataset_train['f78_ratio_f73'] = dataset_train['f78']\/dataset_train['f73']\ndataset_train['f89_ratio_f73'] = dataset_train['f89']\/dataset_train['f73']\ndataset_train['f90_ratio_f74'] = dataset_train['f90']\/dataset_train['f74']\ndataset_train['f43_ratio_f74'] = dataset_train['f43']\/dataset_train['f74']\ndataset_train['f36_ratio_f73'] = dataset_train['f36']\/dataset_train['f73']\ndataset_train['f32_ratio_f35'] = dataset_train['f32']\/dataset_train['f35']\ndataset_train['f5_ratio_f86'] = dataset_train['f5']\/dataset_train['f86']\ndataset_train['f14_ratio_f26'] = dataset_train['f14']\/dataset_train['f26']\ndataset_train['f63_ratio_f26'] = dataset_train['f63']\/dataset_train['f26']","f0186a30":"dataset_test1['f10_ratio_f9'] = dataset_test1['f10']\/dataset_test1['f9']\ndataset_test1['f92_ratio_f73'] = dataset_test1['f92']\/dataset_test1['f73']\ndataset_test1['f78_ratio_f73'] = dataset_test1['f78']\/dataset_test1['f73']\ndataset_test1['f89_ratio_f73'] = dataset_test1['f89']\/dataset_test1['f73']\ndataset_test1['f90_ratio_f74'] = dataset_test1['f90']\/dataset_test1['f74']\ndataset_test1['f43_ratio_f74'] = dataset_test1['f43']\/dataset_test1['f74']\ndataset_test1['f36_ratio_f73'] = dataset_test1['f36']\/dataset_test1['f73']\ndataset_test1['f32_ratio_f35'] = dataset_test1['f32']\/dataset_test1['f35']\ndataset_test1['f5_ratio_f86'] = dataset_test1['f5']\/dataset_test1['f86']\ndataset_test1['f14_ratio_f26'] = dataset_test1['f14']\/dataset_test1['f26']\ndataset_test1['f63_ratio_f26'] = dataset_test1['f63']\/dataset_test1['f26']","4c4f5cfd":"dataset_train['nan_count'] = dataset_train.isnull().sum(axis=1)\ndataset_train['nan_count']  = dataset_train['nan_count']\/dataset_train['nan_count'].max()","daa96b17":"dataset_test1['nan_count'] = dataset_test1.isnull().sum(axis=1)\ndataset_test1['nan_count']  = dataset_test1['nan_count']\/dataset_test1['nan_count'].max()","d6a0ec14":"dataset_train['count'] = dataset_train.notnull().sum(axis=1)\ndataset_train['count']  = dataset_train['count']\/dataset_train['count'].max()","ce1d5953":"dataset_test1['count'] = dataset_test1.notnull().sum(axis=1)\ndataset_test1['count']  = dataset_test1['count']\/dataset_test1['count'].max()","a0531397":"dataset_train['sum'] = dataset_train.sum(axis=1)\ndataset_train['sum'].head()","b036fb95":"dataset_test1['sum'] = dataset_test1.sum(axis=1)\ndataset_test1['sum'].head()","488e22a7":"import tensorflow as tf","22442599":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","e33fc2c6":"from sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer, PowerTransformer","44a33f8a":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')\ndataset_train = imputer.fit_transform(dataset_train)\ndataset_test = imputer.transform(dataset_test1)","d0e680ef":"#pt = PowerTransformer()\n#dataset_train = pt.fit_transform(dataset_train)\n#dataset_test = pt.transform(dataset_test)","33dbb42c":"qt = QuantileTransformer(n_quantiles=256, output_distribution='uniform')\ndataset_train = qt.fit_transform(dataset_train)\ndataset_test = qt.transform(dataset_test)","fca6c450":"#kb = KBinsDiscretizer(n_bins=1024, encode='ordinal',strategy='uniform')\n#dataset_train = kb.fit_transform(dataset_train)\n#dataset_test = kb.transform(dataset_test)","2e8e060c":"from sklearn.preprocessing import MinMaxScaler\nx_scaler = MinMaxScaler()\ndataset_train_sc = x_scaler.fit_transform(dataset_train)\ndataset_test_sc = x_scaler.transform(dataset_test)","180f362e":"import keras\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\n#from tensorflow.keras.engine.input_layer import Input\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization","a1322bb3":"def model_builder(lr):\n    \"\"\"\u041c\u043e\u0434\u0435\u043b\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u0434\u043b\u044f \u0430\u0433\u0435\u043d\u0442\u0430\"\"\"\n    inputA = keras.Input(shape=(132))\n    line = Reshape((132,1))(inputA)\n    #line = Conv1D(filters=16, kernel_size=1, activation='relu')(line)\n    #line = BatchNormalization()(line)\n    line = Conv1D(filters=32, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.2)(line)\n    \n    line = Conv1D(filters=64, kernel_size=2, activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.5)(line)\n    \n    line = Flatten()(line)\n    line = Dense(64, activation='relu')(line)\n    line = Dropout(0.5)(line)\n    #line = Dense(64, activation='relu')(line)\n    outputA = Dense(units=1, activation=\"sigmoid\", kernel_regularizer=keras.regularizers.l1(0.01))(line)\n    model = Model(inputs=inputA, outputs=outputA)\n    #model = keras.models.load_model('models\/model2')\n    model.compile(loss = 'binary_crossentropy', optimizer = Adam(lr=lr), metrics=[tf.keras.metrics.AUC(name='auc'), 'binary_accuracy'],)\n    return model","c5b16a71":"lr=0.0005\nwith strategy.scope():\n    model = model_builder(lr)","f00417bd":"model.summary()","b1edaa39":"checkpoint_filepath = 'best.h5'\nsave_model_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_auc',\n    mode='max',\n    verbose=1,\n    save_best_only=True)","8d9a57bd":"#model = load_model('\/models\/best.h5')","2efd52e3":"EPOCHS = 500\n#EPOCHS = 1\nmodel.fit(dataset_train_sc, y, validation_split=0.2, epochs=EPOCHS, callbacks=[save_model_callback,], batch_size=4096)","8eee0394":"model = load_model('best.h5')","85fa1f32":"preds = model.predict(dataset_test_sc)\noutput = pd.DataFrame({'Id': dataset_test1.index,'claim': preds[:,0]})\npath = 'sample_submission.csv'\noutput.to_csv(path, index=False)\noutput ","6074c56f":"nan count","769cb08e":"not nan count","d600d3fb":"sum all cols","10321b61":"ratio"}}