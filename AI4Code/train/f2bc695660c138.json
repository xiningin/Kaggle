{"cell_type":{"5ab9421c":"code","32843301":"code","8d8bdd78":"code","a6aa1c77":"code","3af04b8e":"code","9bdb243e":"code","4ea0b764":"code","dca239ae":"code","a9a2cdbf":"code","ad62ca5f":"code","8a0c6e3c":"code","d6be4c95":"code","ab03891f":"code","784388c7":"code","2cc5d6df":"code","d1ff0251":"code","2b927ee8":"code","e53bd4f1":"code","a5bbf72e":"code","73ce0f7d":"code","1ec5dec5":"code","c64a8ac3":"code","e1a82fb7":"code","6f9b94f9":"code","6b5c71a8":"markdown","8f9275da":"markdown","081d5078":"markdown","276f681b":"markdown","c6955e1a":"markdown","6bb66fe6":"markdown","32a1117d":"markdown","44abd8f4":"markdown"},"source":{"5ab9421c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32843301":"# import libraries\nimport datetime\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport seaborn as sns \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler , StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LinearRegression,SGDClassifier, RidgeClassifier\n","8d8bdd78":"# read dataset\ndf = pd.read_csv(\"\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\ndf.head()","a6aa1c77":"## check Nan value\nfor i in df.columns:\n    print (i+\": \"+str(df[i].isna().sum()))","3af04b8e":"#check correlation in feature if correlation in independent feature is more then It may affect on final output of dataset.\ncorrelation  = df.corr()\n\n\nplt.figure(figsize=(15,9))\nsns.heatmap(correlation,annot = True,cmap = 'Blues')","9bdb243e":"#ejection fraction and serum sodium are showing more correlation as compared to other dataset, will see how these two features are correlated to each others.\nplt.figure(figsize=(16,8)) # Adding size to the graph- width by height\n# Use `+` as marker; color set as `g` (green); size proportion to Y values\nplt.scatter(x = df['ejection_fraction'], y = df.serum_sodium, c='r') \n# set x\/y labels\nplt.xlabel('ejection_fraction')\nplt.ylabel('serum_sodium')\n# set title\nplt.title('ejection_fraction vs serum_sodium')","4ea0b764":"#Assign feature variable\nx = df.drop(\"DEATH_EVENT\",axis=1)\nx.head()\n\n","dca239ae":"# Assign target variable\ny = df[\"DEATH_EVENT\"]\ny.head()","a9a2cdbf":"# split data into train and test format\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20,random_state = 40)\nprint(\"training data:{}\".format(x_train.shape))\nprint(\"test data:{}\".format(x_test.shape))","ad62ca5f":"df.DEATH_EVENT.value_counts()","8a0c6e3c":"ax = sns.countplot(x=\"DEATH_EVENT\", data=df, palette=\"Set3\")","d6be4c95":"#will check how sex parameter affect on death_event.\nplt.figure(figsize = (15,8))\nax = sns.barplot(x=\"DEATH_EVENT\", y=\"sex\", data=df)","ab03891f":"#smoking is giving balance data interms in death event\nplt.figure(figsize = (15,8))\nax = sns.barplot(x=\"DEATH_EVENT\", y=\"smoking\", data=df)","784388c7":"!pip install plotly\nimport plotly.express as px","2cc5d6df":"fig = px.scatter(df, x=\"serum_creatinine\", y=\"age\", color=\"DEATH_EVENT\",\n                 facet_col=\"DEATH_EVENT\", title=\"serum_creatinine, age Vs death_event\")\n\nfig.update_xaxes(showgrid=False)\n\nfig.show()","d1ff0251":"# Use feature scaling to normailize data each feature contributes approximately proportionately to the final output.\nstandard = StandardScaler()\n\nstd_x = standard.fit_transform(x)","2b927ee8":"results = []","e53bd4f1":"clf_1 = SGDClassifier()\n\n\n\nclf_1.fit(x_train,y_train)\ny_predicted = clf_1.predict(x_test)\nscore = clf_1.score(x_test,y_test)\n\n\nprint(score)\nresults.append(score)","a5bbf72e":"clf_1 = RidgeClassifier()\n\n\n\nclf_1.fit(x_train,y_train)\ny_predicted = clf_1.predict(x_test)\nscore = clf_1.score(x_test,y_test)\n\n\nprint(score)\nresults.append(score)","73ce0f7d":"clf = LogisticRegression()\n\nclf.fit(x_train,y_train)\ny_predicted = clf.predict(x_test)\nscore = clf.score(x_test,y_test)\n\n\nprint(score)\nresults.append(score)","1ec5dec5":"cnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","c64a8ac3":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","e1a82fb7":"classes = df['DEATH_EVENT'].value_counts()\n\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=classes.index,\n                      title='Confusion matrix, without normalization')\n# With normalization\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes= classes.index, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","6f9b94f9":"result_df = pd.DataFrame({\"ML Models\":['SGDClassifier',\"Ridge classifier\",\"Logistic Regression\"],\"Score\":results})\nresult_df","6b5c71a8":"We already split data using train test split, we also used feature scaling now we will use model to train dataset here we will use default parameter.","8f9275da":"# 5.Prepare Model","081d5078":"##### Confusion matrix shows us true positve, true negative, false positve, false negative value","276f681b":"# 6. Confusion matrix","c6955e1a":"# 1. Import Libraries","6bb66fe6":"# 4. Data Preprocessing","32a1117d":"# 2. Import Data","44abd8f4":"# 3. Data Cleaning and Visualization"}}