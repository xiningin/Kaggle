{"cell_type":{"f9e05541":"code","389255cb":"code","4910b292":"code","f9dbed4c":"code","193ee302":"code","84c56833":"code","3b32c99f":"code","56157c9b":"code","511989ee":"code","108f8923":"code","f019cfd0":"markdown","81cec221":"markdown","29f1a917":"markdown","58a3d4d6":"markdown","4c266b24":"markdown","00fc7918":"markdown","50e33b14":"markdown","8ca18991":"markdown","230f1ee3":"markdown","7b1292d0":"markdown","0b2e6e45":"markdown","b3685c1a":"markdown","c537e9dc":"markdown","ef84b071":"markdown"},"source":{"f9e05541":"import numpy as np \nimport pandas as pd\nfrom sklearn import preprocessing\nimport tensorflow as tf","389255cb":"#read the data\nraw_csv_data = np.loadtxt('..\/input\/audiobooks-data\/Audiobooks_data.csv', delimiter = ',')\n\n# X features \nunscaled_inputs_all = raw_csv_data[:,:-1]\n\n# y feature\ntargets_all = raw_csv_data[:,-1]","4910b292":"# Balance the dataset\nnum_one_targets = int(np.sum(targets_all))\nprint(f'the percentage of class 1 before balancing the dataset: {round(num_one_targets \/ targets_all.shape[0] * 100, 1)}') # the data set is clearly imbalanced\n\n# zero counter \nzero_targets_counter = 0\n\n# variable to recall the indicies to be removed\nindices_to_remove = []\n\n#iterate over the dataset and balance it\nfor i in range(targets_all.shape[0]):\n    if targets_all[i] == 0:\n        zero_targets_counter += 1\n        if zero_targets_counter > num_one_targets:\n            indices_to_remove.append(i)\n            \n# the balanced data\nunscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\ntargets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0)\n\nnum_one_targets_equal_priors = int(np.sum(targets_equal_priors))\nprint(f'the percentage of class 1 after balancing the dataset: {num_one_targets_equal_priors \/ targets_equal_priors.shape[0] * 100}') # the data set is clearly imbalanced","f9dbed4c":"# standerdize the inputs\nscaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)","193ee302":"# shuffle the data\n# Since we will use batch gradient descent, we have to shuffle the data\nshuffled_indices = np.arange(scaled_inputs.shape[0])\nnp.random.shuffle(shuffled_indices)\n\nshuffled_inputs = scaled_inputs[shuffled_indices]\nshuffled_targets = targets_equal_priors[shuffled_indices]","84c56833":"#split the dataset in train, validation, and test\nsamples_counts = shuffled_inputs.shape[0]\n\ntrain_samples_counts = int(0.8*samples_counts)\nvalidation_samples_counts = int(0.1*samples_counts)\ntest_samples_counts = int(0.1*samples_counts)\n\ntrain_inputs = shuffled_inputs[:train_samples_counts]\ntrain_targets = shuffled_targets[:train_samples_counts]\n\nvalidation_inputs = shuffled_inputs[train_samples_counts:train_samples_counts+validation_samples_counts]\nvalidation_targets = shuffled_targets[train_samples_counts:train_samples_counts+validation_samples_counts]\n\ntest_inputs = shuffled_inputs[train_samples_counts+validation_samples_counts:]\ntest_targets = shuffled_targets[train_samples_counts+validation_samples_counts:]","3b32c99f":"print(f'Training: the percentage of class 1: {round(np.sum(train_targets)\/train_targets.shape[0] * 100, 1)}')\nprint(f'Validation: the percentage of class 1: {round(np.sum(validation_targets)\/validation_targets.shape[0] * 100, 1)}')\nprint(f'Test: the percentage of class 1: {round(np.sum(test_targets)\/test_targets.shape[0] * 100, 1)}')","56157c9b":"np.savez(\"Audiobooks_data_train\", inputs = train_inputs, targets = train_targets)\nnp.savez(\"Audiobooks_data_validation\", inputs = validation_inputs, targets = validation_targets)\nnp.savez(\"Audiobooks_data_test\", inputs = test_inputs, targets = test_targets)","511989ee":"# model building\ninput_size = 10\noutput_size = 2\nhidden_layer_size = 50\n\nmodel = tf.keras.Sequential([\n                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n                            tf.keras.layers.Dense(hidden_layer_size, activation = 'softmax'),\n                            ])\nmodel.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n\nbatch_size = 100\nmax_epochs = 20\n\nearly_stopping = tf.keras.callbacks.EarlyStopping()\n\nmodel.fit(train_inputs,\n          train_targets,\n          batch_size = batch_size,\n          epochs = max_epochs,\n          validation_data = (validation_inputs, validation_targets),\n          #callbacks = [early_stopping],\n          verbose = 2)","108f8923":"test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)","f019cfd0":"### Split the dataset in train, validation, and test","81cec221":"## Project Overview ","29f1a917":"### Balance the dataset","58a3d4d6":"## Data Preprocessing","4c266b24":"## Build the deep learning algorithm","00fc7918":"### Imports ","50e33b14":"The Network will have 4 layers total:\n1. input layer: 10 units (we have 10 features)\n2. first hidden layer: 50 hidden units\n3. second hidden layer: 50 hidden units\n4. output layer: 2 units (0 or 1)\n\n50 hidden units will provide enough sofistication","8ca18991":"In this project I tried to predict customer behavior, specifically I tried to answer the following question: of the customers who purchased at least once, who is most likely to purchase again?\nanswering this question will drive so much business value to the audiobooks company through targetting the right customers (those who are most likely to come back), thus maximizing the return of each dollar spent on marketting.\n\nI will focus on building the deep leaning model directly, I will skip the data cleaning and exploratory data analysis phases because I already covered them in detail in many other projects (you can find these projects [here](https:\/\/www.kaggle.com\/ahmedmohameddawoud\/code))\n\nI eliminated the head of each column to leave only numeric data for model training. In general each column represent a feature that will help the model understand customer behavior. For example, the data has feature about customer review, minutes played, last time the customer visited the website, and for sure whether he bought again or not (this is our target\/outcome variable).\n\nLets now jump into the important stuff, to create a machine\/deep learning model from scratch we have to:\n1. Preprocess the data\n    - Balance the dataset: if the 90% of the outcome variable is 0 and just 10% is 1, then the data needs to be balanced or at least to be taken into consideration while building the model  \n    - Divide the dataset in training, validation, and test\n    - Save the data in a tensor friendly format (.npz)\n2. Build the deep learning algorithm\n3. Evaluate the performance of the model\n    ","230f1ee3":"### Save the dataset","7b1292d0":"### Read the dataset","0b2e6e45":"### Standerdize the inputs","b3685c1a":"## Evaluate the performance of the model","c537e9dc":"### Check the balance of each dataset again","ef84b071":"### Shuffle the data"}}