{"cell_type":{"e43c791a":"code","ddf5cfb6":"code","d08ec541":"code","1bfda2ff":"code","1e301e6c":"code","b20ba9f3":"code","5ced13af":"code","953ad5ce":"code","87fc9a43":"code","b93b74ca":"code","5b927f8a":"code","21a4ee55":"code","0c72ad44":"code","49700dcc":"code","e91c293e":"code","bc444dfa":"code","39fa5fe1":"code","69617416":"code","1375af90":"code","a8cef8fe":"code","409e9d52":"code","58e72feb":"code","9a495089":"code","a4bc77a6":"code","9e713677":"code","99c3cca6":"code","3c29a85d":"code","711db3be":"code","685c7fc8":"code","86e26875":"code","288493d7":"code","7a93b658":"code","f66ea220":"code","2e3dc55e":"code","7e2b7b56":"code","c52adce6":"code","5ade6267":"code","a2e1c203":"code","f39a6d2c":"markdown","dd3ea0d7":"markdown","d9368db1":"markdown","6f5b2fa1":"markdown"},"source":{"e43c791a":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","ddf5cfb6":"data = pd.read_csv('..\/input\/churn-modelling\/Churn_Modelling.csv')","d08ec541":"data.head()","1bfda2ff":"# independent feature:\nX = data.iloc[:, 3:13]","1e301e6c":"# dependent feature :\ny = data.iloc[:, 13]","b20ba9f3":"X.head()","5ced13af":"y.head()","953ad5ce":"# Create dummy variables :\ngeography=pd.get_dummies(X[\"Geography\"],drop_first=True)","87fc9a43":"gender=pd.get_dummies(X['Gender'],drop_first=True)","b93b74ca":"# Concatenate the Data Frames : \nX=pd.concat([X,geography,gender],axis=1)","5b927f8a":"X.head()","21a4ee55":"# Drop Unnecessary columns :\nX=X.drop(['Geography','Gender'],axis=1)","0c72ad44":"from sklearn.model_selection import train_test_split","49700dcc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","e91c293e":"# Feature Scaling :\nfrom sklearn.preprocessing import StandardScaler","bc444dfa":"sc = StandardScaler()","39fa5fe1":"X_train = sc.fit_transform(X_train)","69617416":"X_test = sc.transform(X_test)","1375af90":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout","a8cef8fe":"# Initialising the ANN\nclassifier = Sequential()","409e9d52":"# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu',input_dim = 11))","58e72feb":"# Adding the second hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu'))","9a495089":"# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))","a4bc77a6":"classifier.summary()","9e713677":"# Compiling the ANN\nclassifier.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","99c3cca6":"# Fitting the ANN to the Training set\nmodel_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 10, epochs= 100)","3c29a85d":"# list all data in history\nmodel_history.history.keys()","711db3be":"# summarize history for accuracy\nplt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","685c7fc8":"# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","86e26875":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)","288493d7":"y_pred = (y_pred > 0.5)","7a93b658":"y_pred","f66ea220":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix","2e3dc55e":"cm = confusion_matrix(y_test, y_pred)","7e2b7b56":"cm","c52adce6":"# Calculate the Accuracy\nfrom sklearn.metrics import accuracy_score","5ade6267":"score=accuracy_score(y_pred,y_test)","a2e1c203":"score","f39a6d2c":"# IMPORTING THE DATASET","dd3ea0d7":"# TRAIN TEST SPLIT","d9368db1":"*we have to predict the customer in the bank that will be exited in the future or not.*","6f5b2fa1":"# IMPORTING THE LIBRARIES"}}