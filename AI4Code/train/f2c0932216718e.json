{"cell_type":{"6e583b2d":"code","596e83c8":"code","d99e6020":"code","96a06aa0":"code","bb0b7694":"code","f4664382":"code","2b603215":"code","7ebd2643":"code","9573fa9d":"code","e187ea7e":"code","d7a8f25e":"code","953c16f7":"code","e9c461de":"code","31d7d67a":"markdown","c8ee9b63":"markdown","1c7eed66":"markdown","6d50f68a":"markdown","cbfbeee5":"markdown","548e3d1f":"markdown","2005d60b":"markdown"},"source":{"6e583b2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","596e83c8":"import pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns',None)\nfrom itertools import product\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nimport lightgbm as lgb","d99e6020":"train=pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')","96a06aa0":"y=train['target']\ntrain.drop(['id','target'],axis=1,inplace=True)\ntest_id=test['id']\ntest.drop(['id'],axis=1,inplace=True)\ncol_cat=train.select_dtypes('object').columns.to_list()\ncol_num=train.select_dtypes('number').columns.to_list()\n\ntrain.shape,y.shape,test.shape","bb0b7694":"class SeparateEncoder():\n    def cat_transform(self,df,cat_cols,encoding_list):\n        df_mod=pd.DataFrame()\n        for idx,cat in zip(encoding_list,cat_cols):\n            if idx==0:#One Hot\n                df_oh=pd.get_dummies(df[cat],prefix=cat,prefix_sep='_')\n                df_mod=pd.concat([df_mod,df_oh],axis=1)\n            elif idx==1:#LabelEncoder\n                le=LabelEncoder()\n                df_le=pd.DataFrame(le.fit_transform(df[cat]),columns=[cat])\n                df_mod=pd.concat([df_mod,df_le],axis=1)\n        (_,cat_feature_count)=df_mod.shape\n        return df_mod,cat_feature_count\n    \n    def num_transform(self,df,num_cols,add_pca=True,n_components=5):\n        df_num=df[num_cols]\n        SS=StandardScaler()\n        df_num=pd.DataFrame(SS.fit_transform(df_num),columns=num_cols)\n        if add_pca:\n            pca=PCA(n_components=n_components,random_state=0)\n            df_pca=pd.DataFrame(pca.fit_transform(df_num))\n            df_num=pd.concat([df_num,df_pca],axis=1)\n        else:\n            pass\n        return df_num  \n    \n    def merge_transform(self,df,cat_cols,num_cols,encoding_list,add_pca=True,n_components=5):\n        df_cat,cat_feature_count=self.cat_transform(df,cat_cols,encoding_list)\n        df_num=self.num_transform(df,num_cols,add_pca=add_pca,n_components=n_components)\n        df_merge=pd.concat([df_cat,df_num],axis=1)\n        return df_merge,cat_feature_count","f4664382":"lgbm_params = {'max_depth': 16,\n               'subsample': 0.8, \n               'colsample_bytree': 0.2,\n               'learning_rate': 0.01,\n               'reg_lambda': 10,\n               'reg_alpha': 17,\n               'min_child_samples': 31, \n               'num_leaves': 66,\n               'max_bin': 522,\n               'cat_smooth': 81,\n               'cat_l2': 0.03,\n               'metric': 'auc',\n               'objective':'binary',\n               'n_jobs': -1, \n               'n_estimators': 100,\n               'force_col_wise':True,\n               'verbosity':-2\n              }","2b603215":"n_splits=5\nskf=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=0)\nse=SeparateEncoder()\nresults=pd.DataFrame(columns=['encoding_list','score'])","7ebd2643":"%%time\nfor i in range(len(col_cat)+1):\n    print(f'-----Cat{i}')\n    encoding_list=np.ones(len(col_cat))\n    if i==len(col_cat):pass\n    else:encoding_list[i]=0\n    df_merge,cat_feature_count=se.merge_transform(train,col_cat,col_num,encoding_list,add_pca=False)\n    score=0\n    for k,(train_idx,val_idx) in enumerate(skf.split(df_merge,y)):\n        print(f'------Fold{k+1}')\n        X_train,y_train=df_merge.iloc[train_idx,:],y[train_idx]\n        X_val,y_val=df_merge.iloc[val_idx,:],y[val_idx]\n\n        l_train=lgb.Dataset(X_train,y_train)\n        l_val=lgb.Dataset(X_val,y_val)\n        model = lgb.train(params=lgbm_params,\n                      num_boost_round=1000,\n                      early_stopping_rounds=400,\n                      train_set=l_train,\n                      valid_sets=[l_val,l_train],\n                      verbose_eval=500)         \n        val_pred=model.predict(X_val)\n        score+=roc_auc_score(y_val,val_pred)\/n_splits\n\n    results.loc[i,'encoding_list']=encoding_list\n    results.loc[i,'score']=score","9573fa9d":"results.sort_values('score',ascending=False)","e187ea7e":"results_2=pd.DataFrame(columns=['encoding_list','score'])","d7a8f25e":"%%time\nfrom itertools import product\nfor i,e_list in enumerate(product([0,1],repeat=5)):\n    if sum(e_list)==5 or sum(e_list)==4: continue\n\n    print(f'-----Cat{i}')\n    encoding_list=np.ones(len(col_cat))\n    for l,m in zip([1,5,6,10,16],list(range(6))):\n        encoding_list[l]=e_list[m]\n\n    df_merge,cat_feature_count=se.merge_transform(train,col_cat,col_num,encoding_list,add_pca=False)\n    score=0\n    for k,(train_idx,val_idx) in enumerate(skf.split(df_merge,y)):\n        print(f'------Fold{k+1}')\n        X_train,y_train=df_merge.iloc[train_idx,:],y[train_idx]\n        X_val,y_val=df_merge.iloc[val_idx,:],y[val_idx]\n\n        l_train=lgb.Dataset(X_train,y_train)\n        l_val=lgb.Dataset(X_val,y_val)\n        model = lgb.train(params=lgbm_params,\n                      num_boost_round=1000,\n                      early_stopping_rounds=400,\n                      train_set=l_train,\n                      valid_sets=[l_val,l_train],\n                      verbose_eval=500)         \n        val_pred=model.predict(X_val)\n        score+=roc_auc_score(y_val,val_pred)\/n_splits\n\n    results_2.loc[i,'encoding_list']=encoding_list\n    results_2.loc[i,'score']=score","953c16f7":"results_2.sort_values('score',ascending=False).head(10)","e9c461de":"best_idx=results_2.sort_values('score',ascending=False).head(1).index[0]\nbest_encoding_list=results_2.loc[best_idx,'encoding_list']\nbest_encoding_list","31d7d67a":"If I try every combination (i.e. cat0: LabelEncoding\/OneHotEncoding, cat1: LabelEncoding\/OneHotEncoding, etc.), it will take tremendous amount of time.\n\nTherefore, in the first trial, I've chosen one categorical feature to apply OneHotEncoding, to see whether these choice improve the score or not","c8ee9b63":"**Thanks for reading!**\n\n**I'm sorry for being messy at some parts.**\n\n**Please let me know if you have any questions and advice.**","1c7eed66":"Since index=19 is controll (all LabelEncoder), applying OneHotEncoder to most of the categories (except cat3,9) might improve the score.\n\nFrom now, I'd like to focus on the top 5 categories (cat1,5,6,10,16) to figure out the exact combination of LabelEncoder or OneHotEncoder in those categories.","6d50f68a":"Loading Data (though I will now use the test data in this notebook)","cbfbeee5":"The \"best_encoding_list\" shows that \n\n\n# OneHotEncoder for cat1, cat5, cat10\n\ncan slightly boost the score.","548e3d1f":"Function to encode categorical features separately (also applicable to adding PCA columns of numerical features)","2005d60b":"# Purpose\n\nIn this notebook, I'd like to compare LabeEncoder and OneHotEncoder in the LightGBM model.\n\nThank you [@maostack](http:\/\/) and I referred to [https:\/\/www.kaggle.com\/maostack\/tps-mar-baseline](https:\/\/www.kaggle.com\/maostack\/tps-mar-baseline) to build the LightGBM model.\n\n\n---Conclusion---\n\n**OneHotEncoder for cat1,cat5 and cat10** can slightly boost the score."}}