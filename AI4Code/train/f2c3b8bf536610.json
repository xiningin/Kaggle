{"cell_type":{"a66d02ac":"code","e531f358":"code","247ba999":"code","aca683ee":"code","a7134259":"code","5210478d":"code","34828731":"code","9bf6296e":"code","506a4de3":"code","7a031616":"code","d85172b8":"code","2304e90e":"code","55dd537a":"code","0852af11":"code","83e25073":"code","dfef0e21":"code","bf6905f3":"code","bee21e1e":"code","5d7004ad":"code","0bb6bd6d":"code","9a336092":"code","b26f44a3":"code","48afa448":"code","74b0c9a6":"code","748b5dfb":"code","81ba8006":"code","62ff2fe9":"code","4e29360e":"code","dac082a2":"code","8f453cce":"code","fe727192":"code","f19970e4":"code","c4d4494c":"code","9479183b":"code","f0709b46":"code","975e9061":"code","5befd949":"code","539fe90c":"code","8f12c104":"code","05ddb938":"code","4040d9cd":"code","b02a4f71":"code","39ff0f53":"code","11a60219":"markdown","80511d60":"markdown","101528ec":"markdown","7e1edca7":"markdown","99119d4d":"markdown","e9f83558":"markdown","e030dae1":"markdown","5cff60aa":"markdown","ca132b2c":"markdown"},"source":{"a66d02ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e531f358":"#Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","247ba999":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport datetime as dt\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","aca683ee":"# read the dataset\nretail_df = pd.read_csv(\"\/kaggle\/input\/online-retail-customer-clustering\/OnlineRetail.csv\", sep=\",\", encoding=\"ISO-8859-1\", header=0)\nretail_df.head()","a7134259":"# basics of the df\nretail_df.info()","5210478d":"# missing values\nround(100*(retail_df.isnull().sum())\/len(retail_df), 2)","34828731":"# drop all rows having missing values\nretail_df = retail_df.dropna()\nretail_df.shape","9bf6296e":"retail_df.head()","506a4de3":"# new column: amount \nretail_df['amount'] = retail_df['Quantity']*retail_df['UnitPrice']\nretail_df.head()","7a031616":"# monetary\ngrouped_df = retail_df.groupby('CustomerID')['amount'].sum()\ngrouped_df = grouped_df.reset_index()\ngrouped_df.head()","d85172b8":"# frequency\nfrequency = retail_df.groupby('CustomerID')['InvoiceNo'].count()\nfrequency = frequency.reset_index()\nfrequency.columns = ['CustomerID', 'frequency']\nfrequency.head()","2304e90e":"# merge the two dfs\ngrouped_df = pd.merge(grouped_df, frequency, on='CustomerID', how='inner')\ngrouped_df.head()","55dd537a":"retail_df.head()","0852af11":"# recency\n# convert to datetime\nretail_df['InvoiceDate'] = pd.to_datetime(retail_df['InvoiceDate'], \n                                          format='%d-%m-%Y %H:%M')","83e25073":"retail_df.head()","dfef0e21":"# compute the max date\nmax_date = max(retail_df['InvoiceDate'])\nmax_date","bf6905f3":"# compute the diff\nretail_df['diff'] = max_date - retail_df['InvoiceDate']\nretail_df.head()","bee21e1e":"# recency\nlast_purchase = retail_df.groupby('CustomerID')['diff'].min()\nlast_purchase = last_purchase.reset_index()\nlast_purchase.head()","5d7004ad":"# merge\ngrouped_df = pd.merge(grouped_df, last_purchase, on='CustomerID', how='inner')\ngrouped_df.columns = ['CustomerID', 'amount', 'frequency', 'recency']\ngrouped_df.head()","0bb6bd6d":"# number of days only\ngrouped_df['recency'] = grouped_df['recency'].dt.days\ngrouped_df.head()","9a336092":"# 1. outlier treatment\nplt.boxplot(grouped_df['recency'])","b26f44a3":"# two types of outliers:\n# - statistical\n# - domain specific","48afa448":"# removing (statistical) outliers\nQ1 = grouped_df.amount.quantile(0.05)\nQ3 = grouped_df.amount.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.amount >= Q1 - 1.5*IQR) & (grouped_df.amount <= Q3 + 1.5*IQR)]\n\n# outlier treatment for recency\nQ1 = grouped_df.recency.quantile(0.05)\nQ3 = grouped_df.recency.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.recency >= Q1 - 1.5*IQR) & (grouped_df.recency <= Q3 + 1.5*IQR)]\n\n# outlier treatment for frequency\nQ1 = grouped_df.frequency.quantile(0.05)\nQ3 = grouped_df.frequency.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.frequency >= Q1 - 1.5*IQR) & (grouped_df.frequency <= Q3 + 1.5*IQR)]\n\n","74b0c9a6":"# 2. rescaling\nrfm_df = grouped_df[['amount', 'frequency', 'recency']]\n\n# instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df)\nrfm_df_scaled.shape","748b5dfb":"rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['amount', 'frequency', 'recency']\nrfm_df_scaled.head()","81ba8006":"# k-means with some arbitrary k\nkmeans = KMeans(n_clusters=4, max_iter=50)\nkmeans.fit(rfm_df_scaled)","62ff2fe9":"kmeans.labels_","4e29360e":"# help(KMeans)","dac082a2":"# elbow-curve\/SSD\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\n# ssd\nplt.plot(ssd)","8f453cce":"# silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))\n    \n    ","fe727192":"# final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(rfm_df_scaled)","f19970e4":"kmeans.labels_","c4d4494c":"# assign the label\ngrouped_df['cluster_id'] = kmeans.labels_\ngrouped_df.head()","9479183b":"# plot\nsns.boxplot(x='cluster_id', y='amount', data=grouped_df)","f0709b46":"rfm_df_scaled.head()","975e9061":"grouped_df.head()","5befd949":"# single linkage\nmergings = linkage(rfm_df_scaled, method=\"single\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","539fe90c":"# complete linkage\nmergings = linkage(rfm_df_scaled, method=\"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","8f12c104":"# 3 clusters\ncluster_labels = cut_tree(mergings, n_clusters=3).reshape(-1, )\ncluster_labels","05ddb938":"# assign cluster labels\ngrouped_df['cluster_labels'] = cluster_labels\ngrouped_df.head()","4040d9cd":"# plots\nsns.boxplot(x='cluster_labels', y='recency', data=grouped_df)","b02a4f71":"# plots\nsns.boxplot(x='cluster_labels', y='frequency', data=grouped_df)","39ff0f53":"# plots\nsns.boxplot(x='cluster_labels', y='amount', data=grouped_df)","11a60219":"## Finding the Optimal Number of Clusters\n\n### SSD","80511d60":"## Hierarchical Clustering","101528ec":"# 1. Read and visualise the data","7e1edca7":"# 3. Prepare the data for modelling","99119d4d":"- R (Recency): Number of days since last purchase\n- F (Frequency): Number of tracsactions\n- M (Monetary): Total amount of transactions (revenue contributed)","e9f83558":"### Silhouette Analysis\n\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","e030dae1":"## K-Means Clustering","5cff60aa":"# 4. Modelling","ca132b2c":"# 2. Clean the data"}}