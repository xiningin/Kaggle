{"cell_type":{"e0c4582a":"code","8978c2e6":"code","507c3a4f":"code","671de1b7":"code","87af3a76":"code","858ba585":"code","66238ae3":"code","4c10edea":"code","f5f94698":"code","b53e210a":"code","d32fdfa7":"code","8f67f61a":"code","7a651048":"code","19bcf1c0":"code","e9801100":"code","1654fe7c":"code","93870c03":"code","ec9a1d5b":"code","62768841":"code","a871f134":"code","5e96b7ca":"markdown","e1bd178a":"markdown","41ed2ba6":"markdown"},"source":{"e0c4582a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8978c2e6":"df = pd.read_csv('..\/input\/fishmarket\/fishes.csv')","507c3a4f":"sns.pairplot(df, hue=\"Species\") ","671de1b7":"sns.countplot(data=df, x=\"Species\").set_title(\"Species Outcome\")","87af3a76":"sns.relplot(data=df, x=\"Weight\", y=\"Height\", hue=\"Species\", palette=\"bright\", height=6)","858ba585":"sns.relplot(data=df, x=\"Length1\", y=\"Width\", hue=\"Species\", palette=\"bright\", height=6)","66238ae3":"# Check if there are any null values\ndf.isnull().values.any()","4c10edea":"# Remove null values\ndf = df.dropna()","f5f94698":"# Check if there are any null values\ndf.isnull().values.any()","b53e210a":"#Drop not needed columns \/ features\ndf.drop('Id', axis=1, inplace=True)\ndf.head()","d32fdfa7":"# Import required libraries for performance metrics\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.model_selection import train_test_split","8f67f61a":"def get_performance_measures(actual, prediction):\n    matrix = confusion_matrix(actual, prediction)\n    FP = matrix.sum(axis=0) - np.diag(matrix)  \n    FN = matrix.sum(axis=1) - np.diag(matrix)\n    TP = np.diag(matrix)\n    TN = matrix.sum() - (FP + FN + TP)\n\n    return(TP, FP, TN, FN)","7a651048":"#Custom Scorers\n\n# # Sensitivity, hit rate, recall, or true positive rate\n# TPR = TP\/(TP+FN)\n# # Specificity or true negative rate\n# TNR = TN\/(TN+FP) \n# # Precision or positive predictive value\n# PPV = TP\/(TP+FP)\n# # Negative predictive value\n# NPV = TN\/(TN+FN)\n# # Fall out or false positive rate\n# FPR = FP\/(FP+TN)\n# # False negative rate\n# FNR = FN\/(TP+FN)\n# # False discovery rate\n# FDR = FP\/(TP+FP)\n\n# # Overall accuracy\n# ACC = (TP+TN)\/(TP+FP+FN+TN)\n\n# Also remember:\n# specificity = true negative rate\n# sensitivity = true positive rate\n\ndef sensitivity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TPR = (TP\/(TP+FN)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TPR = (TP\/(TP+FN))[1] # Since the [0] part is the index\n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TPR\n\ndef specificity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TNR = (TN\/(TN+FP)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TNR = (TN\/(TN+FP))[1]\n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TNR","19bcf1c0":"# Define dictionary with performance metrics\n# To know what everaging to use: https:\/\/stats.stackexchange.com\/questions\/156923\/should-i-make-decisions-based-on-micro-averaged-or-macro-averaged-evaluation-mea#:~:text=So%2C%20micro%2Daveraged%20measures%20add,is%20more%20like%20an%20average.\n\n\nscoring = {\n            'accuracy':make_scorer(accuracy_score), \n            'precision':make_scorer(precision_score, average='weighted'),\n            'f1_score':make_scorer(f1_score, average='weighted'),\n            'recall':make_scorer(recall_score, average='weighted'), \n            'sensitvity':make_scorer(sensitivity_score, mode=\"multiclass\"), \n            'specificity':make_scorer(specificity_score, mode=\"multiclass\"), \n           }","e9801100":"# Import required libraries for machine learning classifiers\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes\nfrom sklearn.linear_model import LogisticRegression #Logistic Regression\nfrom sklearn.svm import LinearSVC # Support Vector Machine\nfrom sklearn.neighbors import KNeighborsClassifier #K-nearest Neighbors\nfrom sklearn.cluster import KMeans #K-means\n\n# Instantiate the machine learning classifiers\ndecisionTreeClassifier_model = DecisionTreeClassifier()\ngaussianNB_model = GaussianNB()\nlogisticRegression_model = LogisticRegression(max_iter=10000)\nlinearSVC_model = LinearSVC(dual=False)\nkNeighbors_model = KNeighborsClassifier()","1654fe7c":"# features = data frame set that contain your features that will be used as input to see if prediction is equal to actual result\n# target = data frame set (1 column usually) that will contain your target or actual results.\n# folds = this is added so we can easily change the number of folds we want to do with our data set.\n# folding is a technique to minimise overfitting and therefore make our model more accurate.\ndef models_evaluation(features, target, folds):    \n    # Perform cross-validation to each machine learning classifier\n    decisionTreeClassifier_result = cross_validate(decisionTreeClassifier_model, features, target, cv=folds, scoring=scoring)\n    gaussianNB_result = cross_validate(gaussianNB_model, features, target, cv=folds, scoring=scoring)\n    logisticRegression_result = cross_validate(logisticRegression_model, features, target, cv=folds, scoring=scoring)\n    linearSVC_result = cross_validate(linearSVC_model, features, target, cv=folds, scoring=scoring)\n    kNeighbors_result = cross_validate(kNeighbors_model, features, target, cv=folds, scoring=scoring)\n    # kMeans_result = cross_validate(kMeans_model, features, target, cv=folds, scoring=scoring)\n\n    # Create a data frame with the models perfoamnce metrics scores\n    models_scores_table = pd.DataFrame({\n      'Decision Tree':[\n                        decisionTreeClassifier_result['test_accuracy'].mean(),\n                        decisionTreeClassifier_result['test_precision'].mean(),\n                        decisionTreeClassifier_result['test_recall'].mean(),\n                        decisionTreeClassifier_result['test_sensitvity'].mean(),\n                        decisionTreeClassifier_result['test_specificity'].mean(),\n                        decisionTreeClassifier_result['test_f1_score'].mean()\n                       ],\n\n      'Gaussian Naive Bayes':[\n                                gaussianNB_result['test_accuracy'].mean(),\n                                gaussianNB_result['test_precision'].mean(),\n                                gaussianNB_result['test_recall'].mean(),\n                                gaussianNB_result['test_sensitvity'].mean(),\n                                gaussianNB_result['test_specificity'].mean(),\n                                gaussianNB_result['test_f1_score'].mean()\n                              ],\n\n      'Logistic Regression':[\n                                logisticRegression_result['test_accuracy'].mean(),\n                                logisticRegression_result['test_precision'].mean(),\n                                logisticRegression_result['test_recall'].mean(),\n                                logisticRegression_result['test_sensitvity'].mean(),\n                                logisticRegression_result['test_specificity'].mean(),\n                                logisticRegression_result['test_f1_score'].mean()\n                            ],\n\n      'Support Vector Classifier':[\n                                    linearSVC_result['test_accuracy'].mean(),\n                                    linearSVC_result['test_precision'].mean(),\n                                    linearSVC_result['test_recall'].mean(),\n                                    linearSVC_result['test_sensitvity'].mean(),\n                                    linearSVC_result['test_specificity'].mean(),\n                                    linearSVC_result['test_f1_score'].mean()\n                                   ],\n\n       'K-nearest Neighbors':[\n                        kNeighbors_result['test_accuracy'].mean(),\n                        kNeighbors_result['test_precision'].mean(),\n                        kNeighbors_result['test_recall'].mean(),\n                        kNeighbors_result['test_sensitvity'].mean(),\n                        kNeighbors_result['test_specificity'].mean(),\n                        kNeighbors_result['test_f1_score'].mean()\n                       ],\n\n      },\n\n      index=['Accuracy', 'Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', ])\n    \n    # Return models performance metrics scores data frame\n    return(models_scores_table)","93870c03":"# Let's try to look at our data frame again one last time\ndf.head()","ec9a1d5b":"# Specify features columns\n# Actually what we are doing here is that we are just dropping the Species column since that is our class\n# and the remaining columns will then be our features (eg. inputs to come up to a class)\n# axis 0 basically means to drop all of that column\nfeatures = df.drop(columns=\"Species\", axis=0)\n\n# Now let's see what features looks like\nfeatures\n\n# Don't mind the left hand side, those are just index mainly used for viewing","62768841":"evaluationResult = models_evaluation(features, target, 5)\nview = evaluationResult\nview = view.rename_axis('Test Type').reset_index() #Add the index names to the column. This will be used for our presentation\n\n# https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.melt.html\n# Re-Organizing our dataframe to fit our view need\nview = view.melt(var_name='Classifier', value_name='Value', id_vars='Test Type')\n# result\nsns.catplot(data=view, x=\"Test Type\", y=\"Value\", hue=\"Classifier\", kind='bar', palette=\"bright\", alpha=0.8, legend=True, height=5, margin_titles=True, aspect=2)","a871f134":"# In here we just add a new column to our raw data frame, that gets the result for the highest\n# scoring classifier in every score test.\nevaluationResult['Best Score'] = evaluationResult.idxmax(axis=1)\nevaluationResult","5e96b7ca":"**Classifier Setups and Build Model**","e1bd178a":"# Data Preparation, Balancing and Cleanup","41ed2ba6":"****Data Analysis and Exploration********"}}