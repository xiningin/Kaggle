{"cell_type":{"4e20e476":"code","60c9af3e":"code","1a439b5f":"code","3a10d36d":"code","59e98418":"code","ba3cc202":"code","ccc6cb45":"code","4d970b48":"code","2ce1ef06":"code","2d8b2de4":"code","33544714":"code","f692d840":"code","15682eb9":"code","5f80cce0":"code","c87471f6":"code","3bd60c66":"code","784985be":"code","6d747c22":"code","b77df084":"markdown","eba35fc8":"markdown","e68e64b6":"markdown","7751df3a":"markdown","a5909512":"markdown"},"source":{"4e20e476":"# %tensorflow_version 2.x\nimport tensorflow as tf","60c9af3e":"import pandas as pd\nimport numpy as np\nimport os\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Dropout\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam","1a439b5f":"base_model=MobileNet(weights='imagenet',include_top=False)","3a10d36d":"# base_model.summary()","59e98418":"CLASSES = 2\nx = base_model.output\nx = GlobalAveragePooling2D(name='avg_pool')(x)\nx = Dropout(0.4)(x)\npredictions = Dense(CLASSES, activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=predictions)\nmodel.summary()","ba3cc202":"for layer in base_model.layers:\n    layer.trainable = False\n    \nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","ccc6cb45":"WIDTH = 299\nHEIGHT = 299\nBATCH_SIZE = 32\nTRAIN_DIR = r'\/kaggle\/input\/cat-and-dog\/training_set\/training_set'\nTEST_DIR = r'..\/input\/cat-and-dog\/test_set\/test_set'\n\n# DATA PREP\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\nvalidation_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    target_size=(HEIGHT, WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical')\n    \nvalidation_generator = validation_datagen.flow_from_directory(\n    TEST_DIR,\n    target_size=(HEIGHT, WIDTH),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical')","4d970b48":"imgs, labels = next(train_generator)\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 10, figsize=(200,200))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\nplotImages(imgs)\nprint(labels)\n","2ce1ef06":"data=next(train_generator)","2d8b2de4":"EPOCHS = 5\nBATCH_SIZE = 32\nSTEPS_PER_EPOCH = train_generator.n\/\/train_generator.batch_size\nVALIDATION_STEPS=validation_generator.n\/\/train_generator.batch_size","33544714":"STEPS_PER_EPOCH,VALIDATION_STEPS","f692d840":"history = model.fit(\n    train_generator,\n    epochs = EPOCHS,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    validation_data = validation_generator,\n    validation_steps = VALIDATION_STEPS, \n    verbose=5\n)","15682eb9":"from tensorflow.keras.preprocessing import image","5f80cce0":"\nimg = image.load_img(r'..\/input\/cat-and-dog\/test_set\/test_set\/cats\/cat.4004.jpg', target_size=(299,299))\n","c87471f6":"x = image.img_to_array(img)\nx = preprocess_input(x)\nx = np.expand_dims(x, axis=0)\npred = model.predict(x)[0]","3bd60c66":"pred","784985be":"img1 = image.load_img(r'..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/dog.4003.jpg', target_size=(299,299))\nx = image.img_to_array(img1)\nx = preprocess_input(x)\nx = np.expand_dims(x, axis=0)\npred = model.predict(x)[0]\npred","6d747c22":"train_generator.class_indices","b77df084":"is a streamlined architecture that uses depthwise separable convolution to construct lightweight deep convolution neural networks and provides an efficient model for mobile and embedded vision applications. the structure of MobileNet is based on depthwise separable filter","eba35fc8":"### flow_from_directory\n\nthis method is useful when the images are sorted and  placed in there respective class\/label folders. this method will identify classes automatically from the folder name","e68e64b6":"### please upvote notebook","7751df3a":"### preprocess_input\nkeras works with batches of images. so the first dimension is used for the number of sample(or images) you have. when you load a single image, you get the shape of one image, which s (size1, size2, channels). the **preprocess_input** function is meant to adequate your image to the format the model requres","a5909512":"# MobileNet "}}