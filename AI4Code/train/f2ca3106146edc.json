{"cell_type":{"93b3d08d":"code","cff0f637":"code","dfeae0f6":"code","52f2cf0c":"code","37864289":"code","097e901f":"code","5c947195":"code","b31acb0c":"code","98af10b1":"code","87a7d421":"code","bad5809e":"code","e8913692":"code","6b4a6aad":"code","87c28a49":"code","d6d635e2":"code","e0aabb39":"code","65c75ab8":"code","0dc3b02c":"code","36f73171":"markdown","d498bbe4":"markdown","debe6125":"markdown","b032f02f":"markdown","8a5fdb55":"markdown","1cfce15f":"markdown","4f24ae64":"markdown","9c9bffba":"markdown","805918b2":"markdown","45e23e13":"markdown","e7ba5487":"markdown","5d2a95eb":"markdown","8a4b42f8":"markdown","4f888c0c":"markdown","269f18b0":"markdown","6939d032":"markdown","67d3bfb5":"markdown","4634f1df":"markdown","b2932799":"markdown","ae3eb3de":"markdown","9a2dfee8":"markdown"},"source":{"93b3d08d":"import random, re, math\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nimport tensorflow as tf, tensorflow.keras.backend as K\nfrom kaggle_datasets import KaggleDatasets\nfrom IPython.display import Image\nfrom tensorflow.keras.utils import plot_model\nprint('Tensorflow version ' + tf.__version__)\nfrom sklearn.model_selection import KFold\nimport gc\nfrom scipy import stats\nimport gc\nfrom collections import Counter\n\n!pip install -q efficientnet\n\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications import DenseNet201\nfrom tensorflow.keras.applications import InceptionV3\nfrom efficientnet.tfkeras import EfficientNetB7\nfrom tensorflow.keras.applications import ResNet152V2\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications import InceptionResNetV2","cff0f637":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","dfeae0f6":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nIMAGE_SIZE = [224, 224]\nEPOCHS = 30\nFOLDS = 3\nSEED = 777\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","52f2cf0c":"MIXED_PRECISION = False\nXLA_ACCELERATE = False\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","37864289":"# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\n\nGCS_PATH_SELECT = { # available image sizes\n    192: GCS_DS_PATH + '\/tfrecords-jpeg-192x192',\n    224: GCS_DS_PATH + '\/tfrecords-jpeg-224x224',\n    331: GCS_DS_PATH + '\/tfrecords-jpeg-331x331',\n    512: GCS_DS_PATH + '\/tfrecords-jpeg-512x512'\n}\n\nGCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec') + tf.io.gfile.glob(GCS_PATH + '\/val\/*.tfrec')\n\nTRAINING_FILENAMES_ONLY = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec')\nVAL_FILENAMES_ONLY =  tf.io.gfile.glob(GCS_PATH + '\/val\/*.tfrec')\n\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/test\/*.tfrec') # predictions on this dataset should be submitted for the competition\n\nCLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']   \n\n\n# Learning rate schedule for TPU, GPU and CPU.\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\n\nLR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\n    \ndef scheduler(epoch):\n    if epoch < 4:\n        return 0.0005\n    elif epoch < 8:\n        return 0.0002\n    elif epoch < 12:\n        return 0.0001\n    elif epoch < 16:\n        return 0.00005\n    elif epoch < 20:\n        return 0.00002\n    else:\n        return 0.00001\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose = True)\n\nrng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\ny = [scheduler(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n\n","097e901f":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled = True, ordered = False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n    \n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n    return dataset\n\ndef data_augment(image, label):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    return image, label   \n\ndef get_training_dataset(dataset,do_aug=True):\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    if do_aug: dataset = dataset.map(transform, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(dataset):\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nNUM_TRAINING_IMAGES = int( count_data_items(TRAINING_FILENAMES) * (FOLDS-1.)\/FOLDS )\nNUM_VALIDATION_IMAGES = int( count_data_items(TRAINING_FILENAMES) * (1.\/FOLDS) )\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\n\nprint('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","5c947195":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n\ndef transform(image,label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3]),label","b31acb0c":"row = 3; col = 4;\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","98af10b1":"row = 3; col = 4;\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","87a7d421":"row = 3; col = 4;\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","bad5809e":"row = 3; col = 4;\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch(row*col)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","e8913692":"# Create Test, TRain and validation Data\n\ntrain_dataset_all = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES})['TRAINING_FILENAMES']), labeled = True)\ntest_dataset_all = load_dataset(list(pd.DataFrame({'TEST_FILENAMES': TEST_FILENAMES})['TEST_FILENAMES']), labeled = True, ordered=True)\n\ntrain_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES_ONLY': TRAINING_FILENAMES_ONLY})['TRAINING_FILENAMES_ONLY']), labeled = True)\nval_dataset = load_dataset(list(pd.DataFrame({'VAL_FILENAMES_ONLY': VAL_FILENAMES_ONLY})['VAL_FILENAMES_ONLY']), labeled=True, ordered=True)\n\n\ntrain_all = get_training_dataset(train_dataset_all) #Both validation and train concatenated for final fitting\ntest = get_test_dataset(test_dataset_all)\ntest_images_ds = test.map(lambda image, idnum: image)\n\ntrain = get_training_dataset(train_dataset)\nval = get_validation_dataset(val_dataset)\n\ndef Xception_model():\n    with strategy.scope():\n        rnet = Xception(\n            input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n            weights='imagenet',\n            include_top=False\n        )\n        # trainable rnet\n        rnet.trainable = True\n        model = tf.keras.Sequential([\n            rnet,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n        ])\n    model.compile(\n        optimizer='adam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n    )\n    return model\n\ndef VGG16_model():\n    with strategy.scope():\n        rnet = VGG16(\n            input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n            weights='imagenet',\n            include_top=False\n        )\n        # trainable rnet\n        rnet.trainable = True\n        model = tf.keras.Sequential([\n            rnet,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n        ])\n    model.compile(\n        optimizer='adam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n    )\n    return model\n\ndef DenseNet201_model():\n    with strategy.scope():\n        rnet = DenseNet201(\n            input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n            weights='imagenet',\n            include_top=False\n        )\n        # trainable rnet\n        rnet.trainable = True\n        model = tf.keras.Sequential([\n            rnet,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n        ])\n    model.compile(\n        optimizer='adam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n    )\n    return model\n\ndef InceptionV3_model():\n    with strategy.scope():\n        rnet = InceptionV3(\n            input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n            weights='imagenet',\n            include_top=False\n        )\n        # trainable rnet\n        rnet.trainable = True\n        model = tf.keras.Sequential([\n            rnet,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n        ])\n    model.compile(\n        optimizer='adam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n    )\n    return model\n\ndef EfficientNetB7_model():\n    with strategy.scope():\n        rnet = EfficientNetB7(\n            input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n            weights='imagenet',\n            include_top=False\n        )\n        # trainable rnet\n        rnet.trainable = True\n        model = tf.keras.Sequential([\n            rnet,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n        ])\n    model.compile(\n        optimizer='adam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n    )\n    return model\n\ndef ResNet152V2_model():\n    with strategy.scope():\n        rnet = ResNet152V2(\n            input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n            weights='imagenet',\n            include_top=False\n        )\n        # trainable rnet\n        rnet.trainable = True\n        model = tf.keras.Sequential([\n            rnet,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n        ])\n    model.compile(\n        optimizer='adam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n    )\n    return model\n\ndef MobileNetV2_model():\n    with strategy.scope():\n        rnet = MobileNetV2(\n            input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n            weights='imagenet',\n            include_top=False\n        )\n        # trainable rnet\n        rnet.trainable = True\n        model = tf.keras.Sequential([\n            rnet,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n        ])\n    model.compile(\n        optimizer='adam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n    )\n    return model\n\ndef InceptionResNetV2_model():\n    with strategy.scope():\n        rnet = InceptionResNetV2(\n            input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n            weights='imagenet',\n            include_top=False\n        )\n        # trainable rnet\n        rnet.trainable = True\n        model = tf.keras.Sequential([\n            rnet,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32')\n        ])\n    model.compile(\n        optimizer='adam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n    )\n    return model\n\nmodels = {\n            'Xception' : Xception_model,\n          'VGG16' : VGG16_model,\n          'DenseNet201' : DenseNet201_model,\n          'InceptionV3' : InceptionV3_model,\n#          'EfficientNetB7' : EfficientNetB7_model, \n          'ResNet152V2' : ResNet152V2_model, \n          'MobileNetV2' : MobileNetV2_model, \n          'InceptionResNetV2' : InceptionResNetV2_model\n         }\nhistorys = {}\npredictions = {}\npredictions_val = {}\npredictions_prob = {}\n\nMODELS_NUMBER = 7 #By RAM constraints i took only 5 models","6b4a6aad":"for name, model_ in models.items() :\n    print('Running ' + name)\n    model = model_()\n    plot_model(model, to_file= name+'.png', show_shapes=True)\n\n    history = model.fit(\n        train, \n        steps_per_epoch = STEPS_PER_EPOCH,\n        epochs = EPOCHS,\n        callbacks = [lr_callback],#, early_stopping],\n        validation_data = val,\n        verbose = 3\n    )\n    historys[name] = history # Save historys\n    predictions_val = np.argmax(model.predict(val), axis=-1)\n#     Train on Train and validation Data for prediction\n    del model\n    gc.collect()\n\n    model = model_()\n    history = model.fit(\n        train_all, \n        steps_per_epoch = STEPS_PER_EPOCH,\n        epochs = EPOCHS,\n        callbacks = [lr_callback],#, early_stopping],\n        verbose = 2\n    )\n    \n    predictions_prob[name] = model.predict_proba(test_images_ds)\n    predictions[name] = np.argmax(model.predict(test_images_ds), axis=-1)","87c28a49":"df = pd.DataFrame(predictions)\npred = []\nfor i in range(0, 7382) :\n    if df.loc[i,:].unique().shape[0] < MODELS_NUMBER :\n        pred.append(stats.mode(df.loc[i,:].values)[0][0])\n    else :\n        pred.append(df.loc[i,'Xception'])\n        \ndf = pd.DataFrame(predictions_val)\npred_val = []\nfor i in range(0, 3712) :\n    if df.loc[i,:].unique().shape[0] < MODELS_NUMBER :\n        pred_val.append(stats.mode(df.loc[i,:].values)[0][0])\n    else :\n        pred_val.append(df.loc[i,'Xception'])","d6d635e2":"avg_prob = predictions_prob['Xception'] + predictions_prob['VGG16'] + predictions_prob['DenseNet201'] + predictions_prob['InceptionV3'] + predictions_prob['EfficientNetB7'] \npred_avg = pd.DataFrame(np.argmax(avg_prob, axis=-1))","e0aabb39":"import matplotlib.image as mpimg\n\ni = 1\nfig = plt.figure(figsize = [20,20])\nfor name, history in historys.items() :\n    \n    plt.subplot(8, 3, i)\n    i += 1\n#     display(Image(filename=name+'.png') )\n    img=mpimg.imread(name+'.png')\n    plt.title(name + ' Architecture')\n    plt.imshow(img)\n    \n    plt.subplot(8, 3, i)\n    i += 1\n    plt.plot(history.history['sparse_categorical_accuracy'])\n    plt.plot(history.history['val_sparse_categorical_accuracy'])\n    plt.title(name + ' accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n#     plt.show()\n    \n    plt.subplot(8, 3, i)\n    i += 1\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title(name + ' loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n#     plt.show()\n    \nplt.tight_layout()","65c75ab8":"def display_confusion_matrix(cmat, score, precision, recall):\n    plt.figure(figsize=(45,45))\n    ax = plt.gca()\n    ax.matshow(cmat, cmap='Reds')\n    ax.set_xticks(range(len(CLASSES)))\n    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 18})\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n    ax.set_yticks(range(len(CLASSES)))\n    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 18})\n    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    titlestring = \"EfficientNet B7 with noisy-student weights\"\n    if score is not None:\n        titlestring += '\\n f1 = {:.3f} '.format(score)\n    if precision is not None:\n        titlestring += '\\n precision = {:.3f} '.format(precision)\n    if recall is not None:\n        titlestring += '\\n recall = {:.3f} '.format(recall)\n    if len(titlestring) > 0:\n        ax.text(101, 1, titlestring, fontdict={'fontsize': 30, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n    plt.show()\n\ncmdataset = val\nimages_ds = cmdataset.map(lambda image, label: image)\nlabels_ds = cmdataset.map(lambda image, label: label).unbatch()\ncm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() \n# cm_probabilities = model1.predict(images_ds)\ncm_predictions = pred_val\ncmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\nscore = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\nprecision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\nrecall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\ncmat = (cmat.T \/ cmat.sum(axis=1)).T \ndisplay_confusion_matrix(cmat, score, precision, recall)\nprint('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))","0dc3b02c":"test_ids_ds = test.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\nnp.savetxt('submission_vote.csv', np.rec.fromarrays([test_ids, pred]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, np.argmax(avg_prob, axis=-1)]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')","36f73171":"# 3. Data Augmentation <a id=\"3\"><\/a>","d498bbe4":"# 6. Models Performance <a id=\"6\"><\/a>","debe6125":"# 4. Pretrained Models Creation <a id=\"4\"><\/a>","b032f02f":"**Pretrained models used for voting :**\n\n* Xception\n* VGG16\n* DenseNet201\n* InceptionV3\n* EfficientNetB7\n* ResNet152V2\n* MobileNetV2\n* InceptionResNetV2\n\n<a href=\"https:\/\/keras.io\/api\/applications\/\">Keras reference<\/a>\n\n![a88.jpg](attachment:a88.jpg)","8a5fdb55":"# Contents\n\n* [<font size=4>1. Libraries<\/font>](#1)\n* [<font size=4>2. Configuration and Data Access<\/font>](#2)\n* [<font size=4>3. Data Augmentation<\/font>](#3)\n* [<font size=4>4. Pretrained Models Creation<\/font>](#4)\n* [<font size=4>5. Transfer Learning and Prediction<\/font>](#5)\n *     [Apply voting to all models output](#5.1)\n* [<font size=4>6. Models Performance<\/font>](#6)\n *     [Accuracy \/ Loss Evolution](#6.1)\n *     [Confusion Matrix](#6.2)\n* [<font size=4>7. Submit predictions<\/font>](#6)","1cfce15f":"**Referances :**\n\n<a href=\"https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96\">Rotation Augmentation GPU\/TPU - [0.96+]<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/sgladysh\/flowers-tpu-efficientnet-b7-b6-b5-b4\">Flowers@TPU EfficientNet B7+B6+B5+B4<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/philculliton\/a-simple-petals-tf-2-2-notebook\">A Simple Petals TF 2.2 notebook<\/a>","4f24ae64":"Models Voting seems perform well =)","9c9bffba":"## Solution Architecture","805918b2":" <FONT size=\"5pt\"><p style=\"color:red\">If you like this approach please Upvote it will motivate me to continue =D, I hope you Enjoyed ;-)<\/p><\/FONT>","45e23e13":"# 1. Libraries <a id=\"1\"><\/a>","e7ba5487":"<img src=\"https:\/\/i.postimg.cc\/R0qN7VJy\/1.jpg\" alt=\"Flowers\" class=\"center\">","5d2a95eb":"Big Thanks to Chris Deotte for this\n\n<a href=\"https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96\">Rotation Augmentation GPU\/TPU - [0.96+]<\/a>","8a4b42f8":"**<p style=\"color:red\">If you like this approach please Upvote it will motivate me to continue =D Enjoy.<\/p>**","4f888c0c":"![architechture.png](attachment:architechture.png)","269f18b0":"**In this notebook i will experiment transfer learning from 8 most popular pre trained convolutional neural networks for identify the type of flowers in a dataset of images (for simplicity, we\u2019re sticking to just over 100 types).**\n\n**I will made an assemblage using this 8 architectures by creating fair voting system between them, at the end every CNN architecture will return type of flower and i will considertate the choice of the majority.**\n\n**Fair play =)**\n\n**I will take advantage of the powerful Tensor Processing Units (TPUs) provided by Kaggle in cloud, it allows to greatly increase the learning speed and also it pushes the limits of RAM as what happens for gpu which allows to consider high resolution images.**\n\n**i will also do data augmentation on pictures by zoom, rotate, inverse, shift and share them. it decrease overfitting.**","6939d032":"# 2. Configuration and Data Access <a id=\"2\"><\/a>","67d3bfb5":"## Apply voting to all models output <a id=\"5.1\"><\/a>","4634f1df":"## Accuracy \/ Loss Evolution <a id=\"6.1\"><\/a>","b2932799":"## Confusion Matrix <a id=\"6.2\"><\/a>\n\nInspired from <a href=\"https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu\">Getting started with 100+ flowers on TPU<\/a>\n","ae3eb3de":"## 7. Submit predictions <a id=\"7\"><\/a>","9a2dfee8":"# 5. Transfer Learning and Prediction <a id=\"5\"><\/a>"}}