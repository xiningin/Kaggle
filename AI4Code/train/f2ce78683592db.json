{"cell_type":{"f4d4f1a4":"code","fa2a3086":"code","e906b26c":"code","0f73d41e":"code","b853e4fd":"code","0e108306":"code","9fab1828":"code","ceb11edd":"code","700bded1":"code","80bc4ab7":"code","27f7e9e8":"markdown","4c38550d":"markdown","07810a5d":"markdown","eb79fce4":"markdown","94ee1a11":"markdown"},"source":{"f4d4f1a4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nfrom gensim.models import Word2Vec\nimport nltk\nnltk.download('wordnet')\nstemmer = SnowballStemmer('english')\nimport unicodedata\nfrom numpy import dot\nfrom numpy.linalg import norm\n\nimport cv2\n%matplotlib inline\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt","fa2a3086":"train_df = pd.read_csv('\/kaggle\/input\/shopee-product-matching\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/shopee-product-matching\/test.csv')\nDATA_PATH = '\/kaggle\/input\/shopee-product-matching\/'\nprint(gensim.__version__)","e906b26c":"def cleanData(dataParse):\n    data = unicodedata.normalize('NFKC', dataParse)\n    data = re.sub(r'\u3010.*\u3011', '', data)\n    data = re.sub(r'\\[.*\\]', '', data)\n    data = re.sub(r'\u300c.*\u300d', '', data)\n    data = re.sub(r'\\(.*\\)', '', data)\n    data = re.sub(r'\\<.*\\>', '', data)\n    data = re.sub(r'[\u203b@\u25ce].*$', '', data)\n    return data.lower() #Returns the parsed tweets\n\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(cleanData(text), pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            if token == 'xxxx':\n                continue\n            result.append(lemmatize_stemming(token))\n    \n    return result","0f73d41e":"processed_docs = train_df['title'].map(preprocess)\nprocessed_docs =list(processed_docs)","b853e4fd":"processed_docs[:5] # clean document","0e108306":"w2v_model = Word2Vec.load(\"\/kaggle\/input\/train-model\/word2vec_model.model\")","9fab1828":"# Generate the average word2vec for the each title description\n\ndef vectors_test(): #test_df\n    \n    # Creating a list for storing the vectors (description into vectors)\n    global word_embeddings_test\n    word_embeddings_test = []\n    test_processed_docs = test_df['title'].map(preprocess)\n    \n    # Reading the each book description \n    for line in test_processed_docs:\n        avgword2vec = None\n        count = 0\n        for word in line:\n            if word in w2v_model.wv:\n                count += 1\n                if avgword2vec is None:\n                    avgword2vec = w2v_model.wv[word]\n                else:\n                    avgword2vec = avgword2vec + w2v_model.wv[word]\n                \n        if avgword2vec is not None:\n            avgword2vec = avgword2vec \/ count\n            word_embeddings_test.append(avgword2vec)\n        else:\n            word_embeddings_test.append(np.array([0]*50, dtype='float32'))","ceb11edd":"vectors_test()#train_df\n# The Top 50 similar matches\ndef similarity(title):\n    \n    # finding cosine similarity for the vectors\n    cosine_similarities = cosine_similarity(word_embeddings_test, word_embeddings_test)\n\n    cntx = test_df[['title', 'posting_id']]\n    #Reverse mapping of the index\n    indices = pd.Series(test_df.index, index = test_df['title']).drop_duplicates()\n         \n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_similarities[idx]))\n    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n    uplimit = len(test_df['title']) if len(test_df['title']) < 50 else 50\n    sim_scores = sim_scores[0:uplimit]\n    post_indices = [i[0] for i in sim_scores if i[1] > .9]\n    recommend = cntx.iloc[post_indices]\n    output=[]\n    output.append(cntx.iloc[idx,1])\n    for index, row in recommend.iterrows():\n        output.append(row['posting_id'])\n\n    return ' '.join( np.unique(output))\n    ","700bded1":"test_df['matches']=test_df['title'].apply(similarity)\n    \ntest_df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","80bc4ab7":"test_df[['posting_id', 'matches']].head()","27f7e9e8":"# Finding the similarity between title and product text using word2vec and cosine similarity","4c38550d":"# keep learning and enrich your intuition.Don't forgot to upvote !! :)","07810a5d":"# Find the similarity between two vector","eb79fce4":"# Data preprocessing","94ee1a11":"# Load model"}}