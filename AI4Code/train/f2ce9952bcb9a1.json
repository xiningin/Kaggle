{"cell_type":{"7ae62b9a":"code","aa5bbc8d":"code","102377b8":"code","92692f8c":"code","9e19b2dd":"code","e572dac1":"code","e99f6606":"code","a1a32f0e":"code","23f84a94":"code","e7c7a21d":"code","718cd38a":"code","0a2c71f1":"code","b66969e7":"code","610061de":"code","e7cf2653":"code","4292adef":"code","d06376f8":"code","a64cd69f":"code","9957bcea":"code","8753a37d":"code","3737ef51":"code","fa9398c3":"code","b1094ebc":"code","6ee8120f":"code","4d79d74d":"code","36758d46":"code","b10c98ae":"code","68cac437":"markdown","ecc64cc6":"markdown","d66bfe1d":"markdown","3fce2f5e":"markdown","0f379fa9":"markdown","f4d2158c":"markdown","10bf1b1d":"markdown","73f30146":"markdown","82021fe6":"markdown","4798f488":"markdown","4014198c":"markdown"},"source":{"7ae62b9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","aa5bbc8d":"# read csv (comma separated value) into data\ndata = pd.read_csv('..\/input\/column_2C_weka.csv')\nprint(plt.style.available) # look at available plot styles\nplt.style.use('ggplot')","102377b8":"# to see features and target variable\ndata.head()","92692f8c":"# Well know question is is there any NaN value and length of this data so lets look at info\ndata.info()","9e19b2dd":"data.describe()","e572dac1":"#heatmap\nf,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(data.corr(),annot=True,linewidths=0.6,linecolor=\"blue\",fmt=\".1f\",ax=ax)\nplt.show()","e99f6606":"data[\"class\"].value_counts().unique","a1a32f0e":"sns.countplot(x=\"class\", data=data)\nplt.title(\"classes\", color=\"green\")\nplt.show()","23f84a94":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","e7c7a21d":"pelvic=data[\"pelvic_incidence\"]\npelvic_tilt=data[\"pelvic_tilt numeric\"]\nlumbar=data[\"lumbar_lordosis_angle\"]\nsacral_s=data[\"sacral_slope\"]\nradius=data[\"pelvic_radius\"]\n\nplt.subplots(figsize=(12,12))\n\nplt.subplot(5,1,1)\nplt.title(\"pelvic_incidence-pelvic_tilt numeric-lumbar_lordosis_angle-sacral_slope-pelvic_radius subplot\")\nplt.plot(radius,color=\"b\",label=\"pelvic_radius\")\nplt.legend()\nplt.grid()\n\nplt.subplot(5,1,2)\nplt.plot(lumbar,color=\"g\",label=\"lumbar_lordosis_angle\")\nplt.legend()\nplt.grid()\n\nplt.subplot(5,1,3)\nplt.plot(pelvic,color=\"r\",label=\"pelvic_incidence\")\nplt.legend()\nplt.grid()\n\nplt.subplot(5,1,4)\nplt.plot(radius,color=\"lime\",label=\"pelvic_radius\")\nplt.legend()\nplt.grid()\n\nplt.subplot(5,1,5)\nplt.plot(pelvic_tilt,color=\"b\",label=\"pelvic_tilt numeric\")\nplt.legend()\nplt.grid()\n\n\nplt.show()","718cd38a":"# import graph objects as \"go\"\nimport plotly.graph_objs as go\n\ndata2=data.copy()\n\n# creating trace1\ntrace1 =go.Scatter(\n                    x = np.arange(0,310),\n                    y = data2[data2['class']=='Normal'].sacral_slope,\n                    mode = \"markers\",\n                    name = \"Normal\",\n                    marker = dict(color = 'rgba(0, 100, 255, 0.8)'),\n                    text= data2['class'])\n# creating trace2\ntrace2 =go.Scatter(\n                    x = np.arange(0,310),\n                    y = data2[data2['class']=='Abnormal'].sacral_slope,\n                    mode = \"markers\",\n                    name = \"Abnormal\",\n                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n                    text= data2['class'])\n\ndf = [trace1, trace2]\nlayout = dict(title = 'sacral_slope',\n              xaxis= dict(title= 'index',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Values',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = df, layout = layout)\niplot(fig)","0a2c71f1":"# pelvic_radius vs class scatter plot\n# import graph objects as \"go\"\nimport plotly.graph_objs as go\n\n# creating trace1\ntrace1 =go.Scatter(\n                    x = np.arange(0,310),\n                    y = data2[data2['class']=='Normal'].pelvic_radius,\n                    mode = \"markers\",\n                    name = \"Normal\",\n                    marker = dict(color = 'rgba(16, 112, 2, 0.8)'),\n                    text= data2['class'])\n# creating trace2\ntrace2 =go.Scatter(\n                    x = np.arange(0,310),\n                    y = data2[data2['class']=='Abnormal'].pelvic_radius,\n                    mode = \"markers\",\n                    name = \"Abnormal\",\n                    marker = dict(color = 'rgba(125, 12, 255, 0.6)'),\n                    text= data2['class'])\n\ndf = [trace1, trace2]\nlayout = dict(title = 'pelvic_radius',\n              xaxis= dict(title= 'index',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Values',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = df, layout = layout)\niplot(fig)","b66969e7":"pie1_list=data[\"class\"].value_counts().values\nlabels = data[\"class\"].value_counts().index\n# figure\nfig = {\n  \"data\": [\n    {\n      \"values\": pie1_list,\n      \"labels\": labels,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"class\",\n      \"hoverinfo\":\"label+percent\",\n      \"hole\": .3,\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Class Type\",\n        \"annotations\": [\n            { \"font\": { \"size\": 15},\n              \"showarrow\": False,\n              \"text\": \"class\",\n                \"x\": 0.20,\n                \"y\": 1\n            },\n        ]\n    }\n}\niplot(fig)","610061de":"dataa = data[data['class'] =='Abnormal']\nx=dataa[\"pelvic_incidence\"].values.reshape(-1,1)\ny=dataa[\"sacral_slope\"].values.reshape(-1,1)\n\n#plot\nplt.figure(figsize=(10,10))\nplt.scatter(x=x,y=y)\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacral_slope\")\nplt.title(\"Abnormal Class\")\nplt.grid()\nplt.show()","e7cf2653":"#linear regression\n#sklearn library\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n#regression\nlinear_reg=LinearRegression()\n\n#fit\nlinear_reg.fit(x,y)\n\n#prediction\n# we need these values to plot the regression line.\nx_=np.linspace(min(x), max(x)).reshape(-1,1) # \ny_head=linear_reg.predict(x_)\n\n#R2 score with LinearRegression library\nprint(\"R_square score: \",linear_reg.score(x,y))\n# R2 score with sklearn.metrics\nprint(\"R_2 score with sklearn.metrics library: \",r2_score(y,linear_reg.predict(x)))\n\n# Plot regression line and scatter\nplt.subplots(figsize=(10,10))\nplt.plot(x_, y_head, color='green', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.grid()\nplt.show()","4292adef":"A=data[data[\"class\"]==\"Abnormal\"]\nN=data[data[\"class\"]==\"Normal\"]","d06376f8":"# pelvic_incidence vs sacral_slope scatter plot in terms of class type \nplt.figure(figsize=(8,8))\nplt.scatter(A.pelvic_incidence,A.sacral_slope,color=\"red\",label=\"abnormal\",alpha=0.5)\nplt.scatter(N.pelvic_incidence, N.sacral_slope,color=\"green\",label=\"normal\",alpha=0.5)\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacral_slope\")\nplt.legend()\nplt.grid()\nplt.show()","a64cd69f":"# determine the values\ndata[\"class\"]=[1 if i==\"Abnormal\" else 0 for i in data[\"class\"]]\n\ny=data[\"class\"].values\nx_data=data.drop([\"class\"],axis=1)","9957bcea":"# normalize the values\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","8753a37d":"# train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)","3737ef51":"#knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nprediction=knn.predict(x_test)","fa9398c3":"prediction","b1094ebc":"y_test","6ee8120f":"print(\"{} nn score: {}\".format(3,knn.score(x_test,y_test)))","4d79d74d":"# find the convenient k value for range (1,21)\nscore_list=[]\nfor i in range(1,25):\n    knn2=KNeighborsClassifier(n_neighbors=i)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\nplt.figure(figsize=(10,10))   \nplt.plot(range(1,25),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","36758d46":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=21)\nknn.fit(x_train,y_train)\nprediction=knn.predict(x_test)","b10c98ae":"print(\"{} nn score: {}\".format(21,knn.score(x_test,y_test)))","68cac437":"<a id=\"2\"><\/a> <br>\n## EDA\n","ecc64cc6":"<a id=\"6\"><\/a> <br>\n### How does the KNN algorithm work?\n\nLet\u2019s take a simple case to understand this algorithm. Following is a spread of red circles (RC) and green squares (GS) :\n\n<a href=\"https:\/\/ibb.co\/gJjn6d7\"><img src=\"https:\/\/i.ibb.co\/BBz1wGP\/2.png\" alt=\"2\" border=\"0\"><\/a>\n\nYou intend to find out the class of the blue star (BS) . BS can either be RC or GS and nothing else. The \u201cK\u201d is KNN algorithm is the nearest neighbors we wish to take vote from. Let\u2019s say K = 3. Hence, we will now make a circle with BS as center just as big as to enclose only three datapoints on the plane. Refer to following diagram for more details:\n\n<a href=\"https:\/\/ibb.co\/VvL4qfN\"><img src=\"https:\/\/i.ibb.co\/9qp5rQv\/3.png\" alt=\"3\" border=\"0\"><\/a>\n\nThe three closest points to BS is all RC. Hence, with good confidence level we can say that the BS should belong to the class RC. Here, the choice became very obvious as all three votes from the closest neighbor went to RC. The choice of the parameter K is very crucial in this algorithm. Next we will understand what are the factors to be considered to conclude the best K.","d66bfe1d":"<a id=\"3\"><\/a> <br>\n## Linear Regression\n","3fce2f5e":"\n* Referances :\nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/03\/introduction-k-neighbours-algorithm-clustering\/\nhttps:\/\/www.kaggle.com\/kanncaa1\/machine-learning-tutorial-for-beginners\n","0f379fa9":"<a id=\"7\"><\/a> <br>\n### How do we choose the factor K?\n\nFirst let us try to understand what exactly does K influence in the algorithm. If we see the last example, given that all the 6 training observation remain constant, with a given K value we can make boundaries of each class. These boundaries will segregate RC from GS. The same way, let\u2019s try to see the effect of value \u201cK\u201d on the class boundaries. Following are the different boundaries separating the two classes with different values of K.\n\n<a href=\"https:\/\/ibb.co\/tcY8CYv\"><img src=\"https:\/\/i.ibb.co\/ZKcWTcF\/4.png\" alt=\"4\" border=\"0\"><\/a><br \/><a target='_blank' href='https:\/\/freeonlinedice.com\/'>dice rolling website<\/a><br \/>\n<a href=\"https:\/\/ibb.co\/hCXtJVz\"><img src=\"https:\/\/i.ibb.co\/rvb1RHr\/5.png\" alt=\"5\" border=\"0\"><\/a>\n\nIf you watch carefully, you can see that the boundary becomes smoother with increasing value of K. With K increasing to infinity it finally becomes all blue or all red depending on the total majority.  The training error rate and the validation error rate are two parameters we need to access on different K-value. Following is the curve for the training error rate with varying value of K :\n\n<a href=\"https:\/\/ibb.co\/6W1Zf0P\"><img src=\"https:\/\/i.ibb.co\/DR8bSfC\/6.png\" alt=\"6\" border=\"0\"><\/a>\n\nAs you can see, the error rate at K=1 is always zero for the training sample. This is because the closest point to any training data point is itself.Hence the prediction is always accurate with K=1. If validation error curve would have been similar, our choice of K would have been 1. Following is the validation error curve with varying value of K:\n\n<a href=\"https:\/\/ibb.co\/y8W8xhQ\"><img src=\"https:\/\/i.ibb.co\/MRMRJ8C\/7.jpg\" alt=\"7\" border=\"0\"><\/a>\n\nThis makes the story more clear. At K=1, we were overfitting the boundaries. Hence, error rate initially decreases and reaches a minima. After the minima point, it then increase with increasing K. To get the optimal value of K, you can segregate the training and validation from the initial dataset. Now plot the validation error curve to get the optimal value of K. This value of K should be used for all predictions.","f4d2158c":"pd.plotting.scatter_matrix:\n\n* green: normal and red: abnormal\n* c: color\n* figsize: figure size\n* diagonal: histohram of each features\n* alpha: opacity\n* s: size of marker\n* marker: marker type","10bf1b1d":"# Introduction\nIn this kernel, we will use Linear Regression and KNN Algortihm with \"Biomedical features of orthopedic patients\" dataset.\n\n * [Importing Dataset](#1)\n * [EDA](#2)\n      * Subplots\n      * Countplot\n      * Scatterplots\n * [Linear Regression](#3)\n * [KNN Algorithm](#4)\n     * [When Do we use KNN Algorithm](#5)\n     * [How Does the KNN Algorithm work?](#6)\n     * [How Do we choose the factor K?](#7)\n * [Conclusion](#8)","73f30146":"<a id=\"1\"><\/a> <br>\n## Importing Dataset","82021fe6":"<a id=\"5\"><\/a> <br>\n### When do we use KNN algorithm?\n\nKNN can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry. To evaluate any technique we generally look at 3 important aspects:\n\n1. Ease to interpret output\n\n2. Calculation time\n\n3. Predictive Power\n\nLet us take a few examples to  place KNN in the scale :\n<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/i.ibb.co\/SBqYbS9\/1.png\" alt=\"1\" border=\"0\"><\/a>\nKNN algorithm fairs across all parameters of considerations. It is commonly used for its easy of interpretation and low calculation time.","4798f488":"<a id=\"8\"><\/a> <br>\n### Conclusion\nwhen k=21, we have the most corrected results.","4014198c":"<a id=\"4\"><\/a> <br>\n## KNN Algorithm"}}