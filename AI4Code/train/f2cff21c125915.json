{"cell_type":{"d840ee8e":"code","22c8e844":"code","6a5f8eb9":"code","c2689691":"code","652e887f":"code","d454b2f5":"code","b8901d3e":"markdown","6935609e":"markdown","f91755ce":"markdown","c6a556c3":"markdown","7145c96a":"markdown"},"source":{"d840ee8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk                         # import NLTK to handle simple NL tasks like tokenization.\nfrom nltk.util import ngrams\nnltk.download('punkt')\nimport math\nfrom collections import Counter     # import the Counter module.\n!pip3 install 'sacrebleu'           # install the sacrebleu package.\nimport sacrebleu                    # import sacrebleu in order compute the BLEU score.\nimport matplotlib.pyplot as plt    \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","22c8e844":"reference = \"The NASA Opportunity rover is battling a massive dust storm on planet Mars.\"\ncandidate_1 = \"The Opportunity rover is combating a big sandstorm on planet Mars.\"\ncandidate_2 = \"A NASA rover is fighting a massive storm on planet Mars.\"\n\ntokenized_ref = nltk.word_tokenize(reference.lower())\ntokenized_cand_1 = nltk.word_tokenize(candidate_1.lower())\ntokenized_cand_2 = nltk.word_tokenize(candidate_2.lower())\n\nprint(f\"{reference} -> {tokenized_ref}\")\nprint(\"\\n\")\nprint(f\"{candidate_1} -> {tokenized_cand_1}\")\nprint(\"\\n\")\nprint(f\"{candidate_2} -> {tokenized_cand_2}\")","6a5f8eb9":"def brevity_penalty(candidate, reference):\n    ref_length = len(reference)\n    can_length = len(candidate)\n\n    # Brevity Penalty\n    if ref_length < can_length: # if reference length is less than candidate length\n        BP = 1 # set BP = 1\n    else:\n        penalty = 1 - (ref_length \/ can_length) # else set BP=exp(1-(ref_length\/can_length))\n        BP = np.exp(penalty)\n\n    return BP","c2689691":"def clipped_precision(candidate, reference):\n    \"\"\"\n    Clipped precision function given a original and a machine translated sentences\n    \"\"\"\n\n    clipped_precision_score = []\n    \n    for i in range(1, 5):\n        ref_n_gram = Counter(ngrams(reference,i))\n        cand_n_gram = Counter(ngrams(candidate,i))\n\n        c = sum(cand_n_gram.values())\n        \n        for j in cand_n_gram: # for every n-gram up to 4 in candidate text\n            if j in ref_n_gram: # check if it is in the reference n-gram\n                if cand_n_gram[j] > ref_n_gram[j]: # if the count of the candidate n-gram is bigger\n                                                   # than the corresponding count in the reference n-gram,\n                    cand_n_gram[j] = ref_n_gram[j] # then set the count of the candidate n-gram to be equal\n                                                   # to the reference n-gram\n            else:\n                cand_n_gram[j] = 0 # else set the candidate n-gram equal to zero\n\n        clipped_precision_score.append(sum(cand_n_gram.values())\/c)\n\n    weights =[0.25]*4\n\n    s = (w_i * math.log(p_i) for w_i, p_i in zip(weights, clipped_precision_score))\n    s = math.exp(math.fsum(s))\n    return s","652e887f":"def bleu_score(candidate, reference):\n    BP = brevity_penalty(candidate, reference)\n    precision = clipped_precision(candidate, reference)\n    return BP * precision","d454b2f5":"print(\n    \"Results reference versus candidate 1 our own code BLEU: \",\n    round(bleu_score(tokenized_cand_1, tokenized_ref) * 100, 1),\n)\nprint(\n    \"Results reference versus candidate 2 our own code BLEU: \",\n    round(bleu_score(tokenized_cand_2, tokenized_ref) * 100, 1),\n)","b8901d3e":"# 1.Importing Library","6935609e":"# **Computing the Brevity Penalty**\n\n We add brevity penalty to handle too short translations.\n Brevity Penalty(BP) will be 1.0 when the candidate translation length is the same as any reference translation length. The closest reference sentence length is the \u201cbest match length.\u201d\n  With the brevity penalty, we see that a high-scoring candidate translation will match the reference translations in length, in word choice, and word order.","f91755ce":"## 2. Example Calculations of the BLEU score","c6a556c3":"BLEU is computed using a couple of ngram modified precisions. Specifically,\n\n$$BLEU = BP\\Bigl(\\prod_{i=1}^{4}precision_i\\Bigr)^{(1\/4)}$$\n\nwith the Brevity Penalty and precision defined as:\n\n$$BP = min\\Bigl(1, e^{(1-({ref}\/{cand}))}\\Bigr)$$\n\n$$precision_i = \\frac {\\sum_{snt \\in{cand}}\\sum_{i\\in{snt}}min\\Bigl(m^{i}_{cand}, m^{i}_{ref}\\Bigr)}{w^{i}_{t}}$$\nwhere pn is the modified precision for ngram, the base of log is the natural base e, wn is weight between 0 and 1 for logpn and \u2211Nn=1wn=1, and BP is the brevity penalty to penalize short machine translations.\n\n* $m^{i}_{cand}$, is the count of i-gram in candidate matching the reference translation.\n* $m^{i}_{ref}$, is the count of i-gram in the reference translation.\n* $w^{i}_{t}$, is the total number of i-grams in candidate translation.","7145c96a":" # **BLEU SCORE**\n  BLEU (Bilingual Evaluation Understudy) is a measurement of the differences between an automatic translation and one or more human-created reference translations of the same source sentence.\n  \n  The BLEU algorithm compares consecutive phrases of the automatic translation with the consecutive phrases it finds in the reference translation, and counts the number of matches, in a weighted fashion. These matches are position independent. A higher match degree indicates a higher degree of similarity with the reference translation, and higher score. Intelligibility and grammatical correctness are not taken into account.\n\n"}}