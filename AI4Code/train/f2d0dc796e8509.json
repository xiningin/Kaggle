{"cell_type":{"0210281e":"code","44ca3616":"code","a7c786aa":"code","aa2af961":"code","bd10a34a":"code","4de48a18":"code","1b0b00e6":"code","89668471":"code","e69ece34":"code","d7f724bc":"code","78fef200":"code","706998b0":"code","5487e6d6":"code","2bdcadbc":"code","de225008":"code","b26453ee":"code","9a36e074":"code","77f890a5":"code","dab2d827":"code","d2ea7889":"code","a894f2a0":"code","7231d795":"code","77716042":"code","5cdcc946":"code","fac8b3d6":"markdown","419c95d9":"markdown","412f0920":"markdown","a2db14af":"markdown","8ee42698":"markdown","a772a72c":"markdown"},"source":{"0210281e":"!pip install -qq --upgrade wandb","44ca3616":"import os\nimport gc\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pathlib import Path\nfrom argparse import Namespace\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow import keras\n\nfrom scipy import stats\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\n\nimport lightgbm as lgb\n\nimport wandb\nfrom wandb.lightgbm import log_summary, wandb_callback\nfrom wandb.keras import WandbCallback","a7c786aa":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=secret_value_0)\n\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","aa2af961":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","bd10a34a":"df = pd.read_pickle('..\/input\/ubiquant-market-prediction-half-precision-pickle\/train.pkl')\ndf.head()","4de48a18":"features = [f'f_{i}' for i in range(300)]\ntarget = 'target'\nEPOCHS = 100","1b0b00e6":"def make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    if mode == \"train\":\n        ds = ds.shuffle(buffer_size=batch_size*8)\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds","89668471":"def get_investment_lookup(investment_id_df):\n    investment_ids = list(investment_id_df.unique())\n    investment_id_size = len(investment_ids) + 1\n    investment_id_lookup_layer = IntegerLookup(max_tokens=investment_id_size)\n    investment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\": investment_ids}))\n    \n    return investment_id_lookup_layer, investment_id_size\n\ndef get_model(investment_id_df):\n    investment_id_inputs = Input((1, ), dtype=tf.uint16)\n    features_inputs = Input((300, ), dtype=tf.float16)\n    \n    investment_id_lookup_layer, investment_id_size = get_investment_lookup(investment_id_df)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = Reshape((-1, ))(investment_id_x)\n    investment_id_x = Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = Dense(256, activation='swish')(features_inputs)\n    feature_x = Dense(256, activation='swish')(feature_x)\n    feature_x = Dense(256, activation='swish')(feature_x)\n    \n    x = Concatenate(axis=1)([investment_id_x, feature_x])\n    x = Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = Dense(1)(x)\n    model = Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    \n    rmse = tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', rmse])\n\n    return model","e69ece34":"tf.keras.backend.clear_session()\nmodel = get_model(df[\"investment_id\"])\nmodel.summary()","d7f724bc":"kfold = StratifiedKFold(5, shuffle=True, random_state=42)\nmodels = []\n\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n\nfor fold, (train_indices, valid_indices) in enumerate(kfold.split(df[features], df[\"investment_id\"])):\n    # Prepare dataset\n    X_train, y_train = df[features].iloc[train_indices], df[\"target\"].iloc[train_indices]\n    X_val, y_val = df[features].iloc[valid_indices], df[\"target\"].iloc[valid_indices]\n    invest_train, invest_val = df[\"investment_id\"][train_indices], df[\"investment_id\"][valid_indices]\n    \n    # Get Dataloaders\n    train_ds = make_dataset(X_train, invest_train, y_train)\n    valid_ds = make_dataset(X_val, invest_val, y_val, mode=\"valid\")\n\n    # Get Model\n    model = get_model(df[\"investment_id\"])\n    \n    # Initialize W&B run\n    run = wandb.init(project='ubiquant_kaggle', group='DNN', job_type='train')\n    \n    # Train model\n    _ = model.fit(train_ds,\n                  epochs=EPOCHS,\n                  validation_data=valid_ds,\n                  callbacks=[WandbCallback(save_model=False), early_stop])\n    \n    # Evaluate\n    preds = model.predict(valid_ds)\n    \n    # Save the model\n    model.save(f'DNN\/models\/model_{fold}')\n    \n    # Get rmse score\n    rmse_score = np.sqrt(mean_squared_error(y_val.values, preds.ravel()))\n    wandb.log({'oof_rmse': rmse_score})\n\n    # Get pearson score\n    pearson_score = stats.pearsonr(preds.ravel(), y_val.values)[0]\n    wandb.log({'oof_pearsonr': pearson_score})\n    \n    # Clear W&B run\n    wandb.finish()\n    \n    del invest_train, invest_val, X_train, X_val, y_train, y_val, train_ds, valid_ds\n    gc.collect()","78fef200":"run = wandb.init(project='ubiquant_kaggle', group='DNN', job_type='save_model')\nmodel_artifact = wandb.Artifact(name='Baseline_DNN', type='dnn_model')\nmodel_artifact.add_dir('DNN\/models')\nrun.log_artifact(model_artifact)\nwandb.finish()","706998b0":"def setup_cv(df, X, y, groups, splits=5):\n    kf = GroupKFold(n_splits=splits)\n    for f, (t_, v_) in enumerate(kf.split(X=X, y=y, groups=groups)):\n            df.loc[v_, 'fold'] = f\n\n    return df","5487e6d6":"def get_rnn_v2():\n    f300_in = Input(shape=(300,), name='300 feature input')\n    x = BatchNormalization(name='batch_norm1')(f300_in)\n    x = Dense(256, activation='swish', name='dense1')(x)\n    x = Dropout(0.1, name='dropout1')(x)\n    x = Reshape((1, -1), name='reshape1')(x)\n    x = BatchNormalization(name='batch_norm2')(x)\n    x = LSTM(128, dropout=0.3, recurrent_dropout=0.3, return_sequences=True, activation='relu', name='lstm1')(x)\n    x = LSTM(16, dropout=0.1, return_sequences=False, activation='relu', name='lstm2')(x)\n    output_layer = Dense(1, name='output')(x)\n\n    model = Model([f300_in], \n                    [output_layer])\n\n    rmse = tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                  loss='mse', metrics=['mse', rmse])\n\n    return model\n\nclass UbiquantRNNV2:\n    def __init__(self, df: pd.DataFrame, feature_cols: list=None, target: str='target'):\n\n        self.model = get_rnn_v2()\n\n        self.df = df\n\n        if feature_cols is not None:\n            self.feature_cols = feature_cols\n        else:\n            self.feature_cols = [f\"f_{i}\" for i in range(300)]\n\n        self.target_col = target\n\n    def train_one_fold(self, f: int, max_epochs=10, log_wandb=True):\n        X_train = self.df[self.df.fold!=f][self.feature_cols]\n        X_valid = self.df[self.df.fold==f][self.feature_cols]\n\n        y_train = self.df[self.df.fold!=f][self.target_col]\n        y_valid = self.df[self.df.fold==f][self.target_col]\n        \n        if log_wandb:\n            run = wandb.init(project='ubiquant_kaggle', group='RNN')\n\n        self.model.fit(X_train, y_train,\n                       validation_data=(X_valid, y_valid),\n                       batch_size=512, epochs=EPOCHS,\n                       callbacks=[\n                         WandbCallback(save_model=False),\n                         tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min'),\n                         tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', baseline=None, restore_best_weights=True)\n            ])\n\n        oof = self.model.predict(X_valid)\n        \n        if log_wandb:\n            wandb.log({'oof_rmse': self.compute_rmse(y_valid, oof)})\n            wandb.log({'oof_pearsonr': self.compute_pearsonr(oof.ravel(), y_valid.values)})\n            wandb.finish()\n            \n        del X_train, X_valid, y_train, y_valid\n        _ = gc.collect()\n\n    def predict(self, X: np.ndarray):\n        preds = self.model.predict(X)\n        return preds\n\n    def save(self, path: str, f: int):\n        self.model.save(f'{path}\/model_{f}.h5')\n        \n    def oof_save(self):\n        self.df[['target', 'preds']].to_csv('rnn_oof.csv', index=False)\n        \n    def compute_rmse(self, y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n    \n    def compute_pearsonr(self, y_true, y_pred):\n        return pearsonr(y_pred, y_true)[0]","2bdcadbc":"uneven_group = np.sort(np.random.randint(0, 1000, len(df)))\nfold_df = setup_cv(df, df[features], df[\"investment_id\"], uneven_group)","de225008":"tf.keras.backend.clear_session()\ntrain_ubiquant_rnn = UbiquantRNNV2(df)","b26453ee":"for fold in range(5):\n    train_ubiquant_rnn.train_one_fold(fold)\n    train_ubiquant_rnn.save('RNN\/models', fold)","9a36e074":"run = wandb.init(project='ubiquant_kaggle', group='RNN', job_type='save_model')\nmodel_artifact = wandb.Artifact(name='Baseline_RNN', type='rnn_model')\nmodel_artifact.add_dir('RNN\/models')\nrun.log_artifact(model_artifact)\nwandb.finish()","77f890a5":"args = Namespace(\n    debug=False,\n    seed=21,\n    folds=5,\n    workers=4,\n    min_time_id=None, \n    num_bins=16,\n    data_path=Path(\"parquets\"),\n)","dab2d827":"time_id_df = (\n    df.filter(regex=r\"^(?!f_).*\")\n    .groupby(\"investment_id\")\n    .agg({\"time_id\": [\"min\", \"max\"]})\n    .reset_index()\n)\ntime_id_df[\"time_span\"] = time_id_df[\"time_id\"].diff(axis=1)[\"max\"]\n\ntrain = df.merge(time_id_df.drop(columns=\"time_id\").droplevel(level=1, axis=1), on=\"investment_id\")\ntrain.time_span.hist(bins=args.num_bins, figsize=(16,8))\ndel time_id_df\ngc.collect()","d2ea7889":"train[\"fold\"] = -1\n_target = pd.cut(train.time_span, args.num_bins, labels=False)\nskf = StratifiedKFold(n_splits=args.folds)\nfor fold, (train_index, valid_index) in enumerate(skf.split(_target, _target)):\n    train.loc[valid_index, 'fold'] = fold\n    \nfig, axs = plt.subplots(nrows=args.folds, ncols=1, sharex=True, figsize=(16,8), tight_layout=True)\nfor ax, (fold, df) in zip(axs, train[[\"fold\", \"time_span\"]].groupby(\"fold\")):\n    ax.hist(df.time_span, bins=args.num_bins)\n    ax.text(0, 40000, f\"fold: {fold}, count: {len(df)}\", fontsize=16)\nplt.show()\ndel _target, train_index, valid_index\n_=gc.collect()","a894f2a0":"cat_features = [\"investment_id\"]\nnum_features = list(train.filter(like=\"f_\").columns)\nfeatures = num_features + cat_features\n\ntrain = train.drop(columns=\"time_span\")\ntrain[[\"investment_id\", \"time_id\"]] = train[[\"investment_id\", \"time_id\"]].astype(np.uint16)\ntrain[\"fold\"] = train[\"fold\"].astype(np.uint8)\ngc.collect()\nfeatures += [\"time_id\"] # https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/302429\nlen(features)","7231d795":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef feval_rmse(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'rmse', rmse(y_true, y_pred), False\n\n# https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/302480\ndef feval_pearsonr(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'pearsonr', pearsonr(y_true, y_pred)[0], True\n\ndef run():    \n    params = {\n        'learning_rate':0.05,\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        'boosting_type': \"gbdt\",\n        'verbosity': -1,\n        'n_jobs': -1, \n        'seed': args.seed,\n        'lambda_l1': 2.7223413643193285e-08, \n        'lambda_l2': 0.009462714717237544, \n        'num_leaves': 108, \n        'feature_fraction': 0.5298125662824026, \n        'bagging_fraction': 0.7279540797730281, \n        'bagging_freq': 6, \n        'max_depth': 10, \n        'max_bin': 487, \n        'min_data_in_leaf': 158,\n        'n_estimators': 1000, \n    }\n    \n    y = train['target']\n    train['preds'] = -1000\n    scores = defaultdict(list)\n    features_importance= pd.DataFrame()\n    \n    for fold in range(args.folds):\n        print(f\"=====================fold: {fold}=====================\")\n        trn_ind, val_ind = train.fold!=fold, train.fold==fold\n        print(f\"train length: {trn_ind.sum()}, valid length: {val_ind.sum()}\")\n        train_dataset = lgb.Dataset(train.loc[trn_ind, features], y.loc[trn_ind], categorical_feature=cat_features)\n        valid_dataset = lgb.Dataset(train.loc[val_ind, features], y.loc[val_ind], categorical_feature=cat_features)\n        \n        run = wandb.init(project='ubiquant_kaggle', group='LGBM')\n        \n        model = lgb.train(\n            params,\n            train_set = train_dataset, \n            valid_sets = [train_dataset, valid_dataset], \n            verbose_eval=100,\n            early_stopping_rounds=50,\n            feval = feval_pearsonr,\n            callbacks=[wandb_callback()]\n        )\n        joblib.dump(model, f'LGBM\/lgbm_seed{args.seed}_{fold}.pkl')\n\n        preds = model.predict(train.loc[val_ind, features])\n        train.loc[val_ind, \"preds\"] = preds\n        \n        scores[\"rmse\"].append(rmse(y.loc[val_ind], preds))\n        scores[\"pearsonr\"].append(pearsonr(y.loc[val_ind], preds)[0])\n        \n        fold_importance_df= pd.DataFrame({'feature': features, 'importance': model.feature_importance(), 'fold': fold})\n        features_importance = pd.concat([features_importance, fold_importance_df], axis=0)\n        \n        wandb.log({f'oof_rmse': rmse(y.loc[val_ind], preds),\n                   f'oof_pearsonr': pearsonr(y.loc[val_ind], preds)[0]})\n        \n        del train_dataset, valid_dataset, model\n        gc.collect()\n    print(f\"lgbm {args.folds} folds mean rmse: {np.mean(scores['rmse'])}, mean pearsonr: {np.mean(scores['pearsonr'])}\")\n    train.filter(regex=r\"^(?!f_).*\").to_csv(\"preds.csv\", index=False)\n    return features_importance","77716042":"features_importance = run()\ndf = train[[\"target\", \"preds\"]].query(\"preds!=-1000\")\nprint(f\"lgbm {args.folds} folds mean rmse: {rmse(df.target, df.preds)}, mean pearsonr: {pearsonr(df.target, df.preds)[0]}\")\ndel df, train\ngc.collect()","5cdcc946":"run = wandb.init(project='ubiquant_kaggle', group='LGBM', job_type='save_model')\nmodel_artifact = wandb.Artifact(name='Baseline_LGBM', type='lgbm_model')\nmodel_artifact.add_dir('LGBM\/')\nrun.log_artifact(model_artifact)\nwandb.finish()","fac8b3d6":"# LightGBM","419c95d9":"# RNN\n\nSource: https:\/\/www.kaggle.com\/ravishah1\/ubiquant-rnn-training-pipeline-and-inference\/notebook","412f0920":"# Imports and Setup","a2db14af":"# DNN\n\nSource: https:\/\/www.kaggle.com\/lonnieqin\/ubiquant-market-prediction-with-dnn\/notebook","8ee42698":"In this kernel, I have compared **three of the best baseline kernels** we have seen in this competition so far. \n\nBeing a beginner in modeling time-series data, I decided to learn from the experienced and setup a useful training+inference workflow in the process. Thus most of the credits for the code goes to the authors of those kernels. \n\nI have used the following baseline kernels:\n* [Ubiquant Market Prediction with DNN](https:\/\/www.kaggle.com\/lonnieqin\/ubiquant-market-prediction-with-dnn\/notebook) by [Lonnie](https:\/\/www.kaggle.com\/lonnieqin).\n* [Ubiquant RNN - Training Pipeline and Inference](https:\/\/www.kaggle.com\/ravishah1\/ubiquant-rnn-training-pipeline-and-inference\/notebook) by  [Ravi Shah](https:\/\/www.kaggle.com\/ravishah1).\n* [Ubiquant LGBM Baseline](https:\/\/www.kaggle.com\/valleyzw\/ubiquant-lgbm-baseline) by [valley](https:\/\/www.kaggle.com\/valleyzw).\n\nThis notebook uses [Weights and Biases](https:\/\/wandb.ai\/site) to compare the three modeling techniques. If you are using these baselines, this kernel can be a good place to learn how W&B can be used.","a772a72c":"# Load Dataset\n\nWe will be using the [Ubiquant Market Prediction half precision Pickle](https:\/\/www.kaggle.com\/lonnieqin\/ubiquant-market-prediction-half-precision-pickle) by Lonnie. You can find more information about the creation of this dataset in this [kernel](https:\/\/www.kaggle.com\/lonnieqin\/reduce-the-dataset-to-1-8g)."}}