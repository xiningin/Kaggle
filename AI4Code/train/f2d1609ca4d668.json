{"cell_type":{"e06fd126":"code","16cc0f8a":"code","67fb06ae":"code","923a50b7":"code","111e819f":"code","c1b67eb4":"code","beb52a6b":"code","8fdc0121":"code","bbe481ff":"code","9e76c05d":"code","98337a29":"code","8522191d":"code","df17155b":"code","6ee972dc":"code","3eb234c6":"code","4cfa54e1":"code","338451f9":"code","724b16b9":"code","fc772888":"markdown","758bbf40":"markdown","124b85f1":"markdown","adc6444c":"markdown","02128fb1":"markdown","b8526c91":"markdown","7551e662":"markdown","f0dcf56f":"markdown","1ba04d50":"markdown","8b5657c2":"markdown","a86dc80e":"markdown","b9563320":"markdown","0a9a26d2":"markdown"},"source":{"e06fd126":"import os\nimport sys\nimport torch\nimport random\nimport numpy as np\nimport torchvision\nimport pandas as pd\nimport seaborn as sns\nfrom PIL import Image\nfrom PIL import Image\nfrom sklearn import cluster\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset as Dataset\nfrom torch.utils.data import DataLoader as DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\nsys.path.append(\"..\/input\/maskrcnn-utils\/\")\nfrom transforms import ToTensor, RandomHorizontalFlip, Compose\n\ntorch.cuda.empty_cache()\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\nsns.set_style(\"darkgrid\")","16cc0f8a":"IMG_W ,IMG_H = 520, 704\nroot = \"..\/input\/sartorius-cell-instance-segmentation\/\"\n\ndef rle_decode(mask_rle, shape, color=1):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    \n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)\n\ndef test_model(model, dataloader,n):\n    \n    for i in range(n):\n        fig = plt.figure(figsize=(10,10))\n        img, targets = next(iter(dataloader))\n        img = img[0]\n        targets = targets[0]\n        plt.imshow(img.numpy().transpose((1,2,0)))\n        plt.grid(None)\n        plt.title(f\"Image : {i}\")\n        plt.show()\n    \n        fig = plt.figure(figsize=(10,10))\n        all_masks = np.zeros((IMG_W ,IMG_H))\n        for mask in targets['masks']:\n            all_masks = np.logical_or(all_masks, mask)\n        plt.imshow(img.numpy().transpose((1,2,0)))\n        plt.imshow(all_masks, alpha=0.3)\n        plt.grid(None)\n        plt.title(f\"Target : {i}\")\n        plt.show()\n    \n        fig = plt.figure(figsize=(10,10))\n        model.eval()\n        with torch.no_grad():\n            preds = model([img.to(device)])[0]\n\n        plt.imshow(img.cpu().numpy().transpose((1,2,0)))\n        all_preds_masks = np.zeros((IMG_W ,IMG_H))\n        for mask in preds['masks'].cpu().detach().numpy():\n            all_preds_masks = np.logical_or(all_preds_masks, mask[0])\n        plt.imshow(all_preds_masks, alpha=0.4)\n        plt.grid(None)\n        plt.title(f\"Predictions : {i}\")\n        plt.show()\n        \n\n\n# Stolen from: https:\/\/www.kaggle.com\/arunamenon\/cell-instance-segmentation-unet-eda\n# Run-length encoding stolen from https:\/\/www.kaggle.com\/rakhlin\/fast-run-length-encoding-python\n# Modified by me\n\ndef rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\n\ndef does_overlap(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            return True\n    return False\n\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask","67fb06ae":"class SatoriusDataset(torch.utils.data.Dataset):\n    def __init__(self, transforms, root = 'data\/',train = True):\n        self.root = root\n        self.transforms = transforms\n        self.w , self.h = 520 , 704\n        info = pd.read_csv(self.root+'train.csv')[[\"id\",\"annotation\"]]\n        info = info.groupby('id')['annotation'].agg(lambda x: list(x)).reset_index()\n        validation =  6\n        self.data_info = 0\n\n        if train == True:\n            self.data_info = info[validation:].reset_index(drop=True)\n        else:\n            self.data_info = info[:validation].reset_index(drop=True)\n            \n    def __getitem__(self, idx):\n        img = Image.open(self.root+'train\/'+self.data_info['id'][idx]+'.png').convert(\"RGB\")\n        mask = np.zeros((len(self.data_info['annotation'][idx]), self.w, self.h), dtype=int)\n        \n        num_objs = len(self.data_info['annotation'][idx])\n        \n        for i in range(num_objs):\n            nth_mask = rle_decode(self.data_info['annotation'][idx][i], (self.w, self.h))\n            nth_mask = np.array(nth_mask) > 0\n            mask[i, :, :] = nth_mask\n    \n\n        \n        boxes = []\n        new_masks = []\n\n        for i in range(num_objs):\n            pos = np.where(mask[i, :, :])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n            new_masks.append(mask[i, :, :])\n        \n        nmx = np.zeros((num_objs ,self.w, self.h), dtype=int)\n        \n        for i in range(num_objs):\n            nmx[i, :, :] = new_masks[i]\n        \n            \n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(nmx, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img,target = self.transforms(img,target)\n            \n        return img, target\n\n    def __len__(self):\n        return len(self.data_info)\n\n\n    \ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(RandomHorizontalFlip(0.5))\n    return Compose(transforms)\n\n\ntrain_dataset = SatoriusDataset(transforms=get_transform(train=True),root =root,train = True)\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n\nvalidation_dataset = SatoriusDataset(transforms=get_transform(train=False),root =root,train = False)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))","923a50b7":"train_df = train_dataset.data_info\nvalidation_df = validation_dataset.data_info\n\npd.set_option('max_colwidth', 125)\ndisplay(train_df.head())\ndisplay(validation_df)","111e819f":"sns.set_palette(\"pastel\")\n\nn_annotations = [[],[]] #[[train], [validation]]\nfor i in train_df['annotation']:\n    n_annotations[0].append(len(i))\n    \nfor i in validation_df['annotation']:\n    n_annotations[1].append(len(i))\n\nx_axis,y_axis = 'Number Of Annotations' , 'Number Of Images'\n\nfig = plt.figure(figsize=(8,8))\np = sns.histplot(data = n_annotations[0])\np.set_xlabel(x_axis, fontsize = 15)\np.set_ylabel(y_axis, fontsize = 15)\n\n\nplt.show()\n\nfig = plt.figure(figsize=(8,8))\np = sns.histplot(data = n_annotations[1])\np.set_xlabel(x_axis, fontsize = 15)\np.set_ylabel(x_axis, fontsize = 15)\nplt.show()","c1b67eb4":"import cv2\n\nimg, targets = next(iter(validation_dataloader))\nimage = np.array(img[0])\ntargets = targets[0]\nimage = cv2.cvtColor(image.transpose((1,2,0)), cv2.COLOR_BGR2RGB)\nall_masks = np.zeros((IMG_W ,IMG_H))\nfor mask in targets['masks']:\n    all_masks = np.logical_or(all_masks, mask)\n    \nplt.figure(figsize=(10, 10))\nplt.imshow(image)\nplt.axis(\"off\")\nplt.figure(figsize=(10, 10))\nplt.imshow(image)\nplt.imshow(all_masks, alpha=0.5)\nplt.axis(\"off\")\nplt.figure(figsize=(10, 10))\nplt.imshow(all_masks)\nplt.axis(\"off\")\n    \nplt.show()","beb52a6b":"def clustered_img(x):\n    kmeans = cluster.KMeans(2)\n    dims = np.shape(x)\n    pixel_matrix = np.reshape(x, (dims[0] * dims[1], dims[2]))\n    clustered = kmeans.fit_predict(pixel_matrix)\n    clustered_img = np.reshape(clustered, (dims[0], dims[1]))\n    return clustered_img\n\nfor i in range(2):\n    fig = plt.figure(figsize=(10,10))\n    img, targets = next(iter(validation_dataloader))\n    img = img[0]\n    targets = targets[0]\n    plt.imshow(clustered_img(img.numpy().transpose((1,2,0))))\n    plt.grid(None)\n    plt.title(\"K-Means\")\n    plt.show()\n\n    fig = plt.figure(figsize=(10,10))\n    all_masks = np.zeros((IMG_W ,IMG_H))\n    for mask in targets['masks']:\n        all_masks = np.logical_or(all_masks, mask)\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.imshow(all_masks, alpha=0.3)\n    plt.grid(None)\n    plt.title(\"Target\")\n    plt.show()","8fdc0121":"!mkdir -p \/root\/.cache\/torch\/hub\/checkpoints\/\n!cp ..\/input\/cocopre\/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth \/root\/.cache\/torch\/hub\/checkpoints\/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth","bbe481ff":"def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n#     model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)  #If to train newly\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model","9e76c05d":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    # our dataset has two classes only - background and cell\nnum_classes = 2\n\nmodel = get_model_instance_segmentation(num_classes)\n\n    # move model to the right device\nmodel.to(device)\n\n    # construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\nepoch = 15\nloss_history = [[],[]]\ntrain_n_minibatches = train_dataloader.__len__()\nvalidation_n_minibatches = validation_dataloader.__len__()","98337a29":"model.train()\nlog_idx = 100\n\nfor e in range(epoch):\n    for batch_idx , (x ,y) in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        x = list(image.to(device) for image in x)\n        y = [{k: v.to(device) for k, v in t.items()} for t in y]\n        loss_dict = model(x, y)\n        \n        loss = sum(loss for loss in loss_dict.values())\n        loss.backward()\n        optimizer.step()\n        loss_history[0].append(float(loss.detach()))\n        \n        if batch_idx % log_idx == 0:\n            # Printing Log\n            print(f'LOSS for EPOCH {e+1} BATCH {batch_idx+1}\/{train_n_minibatches} TRAIN LOSS : {loss_history[0][-1]}',end = ' ')\n            with torch.no_grad():\n                # Calculating loss and accuracy for validation\n                for _batch_idx_ , (x ,y) in enumerate(validation_dataloader):\n                    x = list(image.to(device) for image in x)\n                    y = [{k: v.to(device) for k, v in t.items()} for t in y]\n                    loss_dict = model(x, y)\n                    validation_loss = sum(loss for loss in loss_dict.values())\n                    loss_history[1].append(float(validation_loss.detach()))\n                                      \n                print(f'VALIDATION LOSS : {sum(loss_history[1][-1:-validation_n_minibatches-1:-1])\/validation_n_minibatches}')\n\n    torch.save(model.state_dict(),'masked_rcnn_ss')\n    #Log for e+1th epoch\n    print(f'---------------------------------------EPOCH {e+1}-------------------------------------------')\n    print(f'Loss for EPOCH {e+1}  TRAIN LOSS : {sum(loss_history[0][-1:-train_n_minibatches-1:-1])\/train_n_minibatches}')\n    n_validation_losses = int(train_n_minibatches\/log_idx)*validation_n_minibatches\n    print(f'VALIDATION LOSS for EPOCH {e+1} : {sum(loss_history[1][-1:-1*n_validation_losses-1:-1])\/n_validation_losses}',end = '\\n')\n    print('---------------------------------------------------------------------------------------------')","8522191d":"# Plotting Loss per epoch\nloss_per_epoch = [[],[]]\nfor i in range(epoch):\n    temp = 0\n    for j in loss_history[0][i*train_n_minibatches:(i+1)*train_n_minibatches]:\n        temp = temp + j\n    loss_per_epoch[0].append(temp\/train_n_minibatches)\n    temp = 0\n    for j in loss_history[1][i*n_validation_losses:(i+1)*n_validation_losses]:\n        temp = temp + j\n    loss_per_epoch[1].append(temp\/n_validation_losses)    \n\nsns.lineplot(x=range(len(loss_per_epoch[0])),y=loss_per_epoch[0])\nsns.lineplot(x=range(len(loss_per_epoch[1])),y=loss_per_epoch[1])\nplt.show()","df17155b":"model = get_model_instance_segmentation(num_classes)\n    # move model to the right device\nmodel.to(device)\nmodel.load_state_dict(torch.load('masked_rcnn_ss', map_location='cuda'))","6ee972dc":"test_model(model, validation_dataloader,n=3)","3eb234c6":"class TestDataset(Dataset):\n    def __init__(self, root,transforms = None):\n        self.root = root\n        self.transforms = transforms\n        self.img_name = []\n        for i in os.listdir(root+'test\/'):\n            self.img_name.append(i[:-4])\n        self.w , self.h = 520 , 704\n\n            \n    def __getitem__(self, idx):\n        image = Image.open(self.root+'test\/'+self.img_name[idx]+'.png').convert(\"RGB\")\n        if self.transforms is not None:\n            image, _ = self.transforms(image=image, target=None)\n        \n        return image , self.img_name[idx]\n\n    def __len__(self):\n        return len(self.img_name)\n    \n","4cfa54e1":"td = TestDataset(root, transforms=get_transform(train=False))","338451f9":"DROP_OVERLAPPING = False\n\nsubmission = []\ncounter = 0\n\nmin_precision = 0.5\n\nmodel.eval()\n\nfor img,img_name in td:\n    \n    with torch.no_grad():\n        result = model([img.to(device)])[0]\n        \n    if len(result[\"masks\"]) != 0:\n        previous_masks = []\n        for j, m in enumerate(result[\"masks\"]):\n            original_mask = result[\"masks\"][j][0].cpu().numpy()\n            if DROP_OVERLAPPING and does_overlap(original_mask, previous_masks):\n                continue\n            else:\n                original_mask = remove_overlapping_pixels(original_mask, previous_masks)\n                previous_masks.append(original_mask)\n                rle = rle_encoding(original_mask > min_precision)\n                submission.append([img_name, rle])\n    else:\n        submission.append([img_name, \"\"])\n\ndf_sub = pd.DataFrame(submission, columns=['id', 'predicted'])\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","724b16b9":"df_sub.groupby('id')['predicted'].agg(lambda x: list(x)).reset_index()","fc772888":"#### 2.Visualizing Dataset <a class=\"anchor\" id=\"3.2\"><\/a>","758bbf40":"## **Contents**\n-  [Introduction](#i)\n-  [1.Importing Libraries](#1)\n-  [2.Helper Functions](#2)\n-  [3.Dataset Managament](#3)\n    -  [3.1.Dataset and DataLoaders](#3.1)\n    -  [3.2.Visualizing Dataset](#3.2)\n-  [4.Initializing pre-trained model](#4)\n-  [5.Training](#5)\n-  [6.Plotting Graphs](#6)\n    -  [6.1.Plotting Loss vs Epoch](#6.1)\n-  [7.Loading and Testing](#7)","124b85f1":"# 1.Plotting Loss vs Epoch<a class=\"anchor\" id=\"6.1\"><\/a>","adc6444c":"## **Introduction** <a class=\"anchor\" id=\"i\"><\/a>\n\n\nIn this notebook I am visualizing the dataset and also fine-tuning pre-trained [Mask-RCNN](https:\/\/arxiv.org\/abs\/1703.06870) model using [Pytorch](https:\/\/pytorch.org\/) library.Mask R-CNN is a popular deep learning instance segmentation technique that performs pixel-level segmentation on detected objects.\n\nThe Mask R-CNN algorithm can accommodate multiple classes and overlapping objects.Mask R-CNN extends Faster R-CNN to solve instance segmentation tasks. It achieves this by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. In principle, Mask R-CNN is an intuitive extension of Faster R-CNN, but constructing the mask branch properly is critical for good results. \n\nInstance segmentation treats multiple objects of the same class as distinct individual instances which give this model slight advantage over other Semantic segmentation models. \n\n\nRead more about Mask-RCNN [here](https:\/\/www.analyticsvidhya.com\/blog\/2019\/07\/computer-vision-implementing-mask-r-cnn-image-segmentation\/)\n\nSome of the helper functions and dataloader classes are the derivatives of [this](https:\/\/www.kaggle.com\/julian3833\/sartorius-starter-torch-mask-r-cnn-lb-0-173#%F0%9F%A6%A0-Sartorius---Starter-Torch-Mask-R-CNN) notebook.\n\n<img src= \"attachment:e21e8027-d32b-49eb-8685-18e763bda774.png\"  style='width: 800px;'>","02128fb1":"### **5.Training** <a class=\"anchor\" id=\"5\"><\/a>","b8526c91":"### **8.Predictions**<a class=\"anchor\" id=\"8\"><\/a>","7551e662":"#### 1.Downloading and Extracting Dataset <a class=\"anchor\" id=\"3.1\"><\/a>","f0dcf56f":"### **1.Importing Libraries** <a class=\"anchor\" id=\"1\"><\/a>","1ba04d50":"### **7.Loading and Testing**<a class=\"anchor\" id=\"7\"><\/a>","8b5657c2":"### **6.Plotting Graphs** <a class=\"anchor\" id=\"6\"><\/a>","a86dc80e":"### **3.Dataset Managament** <a class=\"anchor\" id=\"3\"><\/a>","b9563320":"### **4.Initializing pre-trained model** <a class=\"anchor\" id=\"4\"><\/a>","0a9a26d2":"### **2.Helper Functions** <a class=\"anchor\" id=\"2\"><\/a>"}}