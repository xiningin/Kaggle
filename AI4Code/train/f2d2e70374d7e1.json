{"cell_type":{"b002655b":"code","18a34b24":"code","c936999e":"code","68457793":"code","c07ab191":"code","462ac1cd":"code","e6749552":"code","242b5c9d":"code","221abbce":"code","01c2d866":"code","314a3b3b":"code","b5b04c21":"code","1e64daea":"code","96c1a9d5":"code","03dbd22f":"code","c5ff97e1":"code","f4a9d571":"code","0d286145":"code","2fb409f1":"code","14bdccb5":"code","f7735102":"code","b2a80eed":"code","f0989ea1":"code","d2e8424f":"code","2572a1b0":"code","ea95715e":"code","ac162450":"code","90b49afb":"markdown","57f93e03":"markdown","93f1092f":"markdown","df8a3d7e":"markdown","519dca59":"markdown","ff44c34b":"markdown"},"source":{"b002655b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18a34b24":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","c936999e":"df = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","68457793":"df.head()","c07ab191":"df.shape","462ac1cd":"df.dtypes","e6749552":"df['DEATH_EVENT'].value_counts()","242b5c9d":"X = df.drop('DEATH_EVENT', axis=1)\nX.head()","221abbce":"y = df['DEATH_EVENT']","01c2d866":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","314a3b3b":"import statsmodels.api as sm\nlogit_model = sm.Logit(y_train, X_train).fit()\nlogit_model.summary()","b5b04c21":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nLR = LogisticRegression().fit(X_train,y_train)\nyhat = LR.predict(X_test)\nLR.score(X_test, y_test)","1e64daea":"X = df[['age', 'ejection_fraction', 'serum_creatinine']]\nX.corr()","96c1a9d5":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","03dbd22f":"logit_model = sm.Logit(y_train, X_train, axis=1).fit()\nlogit_model.summary()","c5ff97e1":"LR = LogisticRegression().fit(X_train,y_train)\nyhat = LR.predict(X_test)\nLR.score(X_test, y_test)","f4a9d571":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, yhat)","0d286145":"df.head()","2fb409f1":"df['DEATH_EVENT'].value_counts()","14bdccb5":"X = df.drop('DEATH_EVENT', axis=1).values\ny = df['DEATH_EVENT'].values","f7735102":"# for the ML try with all variables vs only the good variables","b2a80eed":"from sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(X).transform(X)","f0989ea1":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","d2e8424f":"from sklearn.neighbors import KNeighborsClassifier","2572a1b0":"k = 7\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nyhat = neigh.predict(X_test)\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))","ea95715e":"k_range = 30\nmean_acc = np.zeros((k_range-1))\nConfustionMx = [];\nfor n in range(1, k_range):\n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\nmean_acc","ac162450":"plt.plot(range(1,k_range),mean_acc,'g')\nplt.legend(('Accuracy ', '+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (k)')\nplt.tight_layout()\nplt.show()","90b49afb":"(interpret the confusion matrix here)","57f93e03":"According to the data, 203 patients were not deceased on follow-up and 96 were. Since the output has a binary outcome (deceased \/ not deceased), I wanted to start with logistic regression to see which variables were important to the prediction of the outcome.","93f1092f":"This is a dataset that keeps track of several variables including blood pressure and reports whether or not the patient had passed away by their next follow-up appointment.","df8a3d7e":"After keeping only age, ejection_fraction, and serum_creatinine, the accuracy had a small improvement from 0.733 to 0.75. Based on the coefficients, higher age, higher serum creatinine level, and lower ejection fraction is associated with a greater likelihood of heart failure.","519dca59":"This model had an accuracy of 0.733. Above is the summary output for the logistic regression with all of the predictors included. Based on the summary, the only predictors with a p-value below 0.05 were age, ejection_fraction, serum_creatinine, and time. \n\nThe time variable measures the number of days between these measurements being taken and the patient's follow-up appointment. For prediction purposes, the time variable may not actually be appropriate to include in the model since it is not a measurement that can be taken on the patient.\n\nNext, all of the predictors with a p-value greater than 0.05 and the time variable will be dropped.","ff44c34b":"(multicollinearity?)"}}