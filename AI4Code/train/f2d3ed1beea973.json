{"cell_type":{"0842790f":"code","46ef8144":"code","1bd6d07f":"code","cebdeb0c":"code","2ff7cd11":"code","e1c40e18":"code","5b8379eb":"code","023702ce":"code","9650ad29":"code","336c0260":"code","4b0fa549":"code","fb4849bd":"code","152e119f":"code","e1e23ffe":"code","14d21786":"code","e3c43c87":"code","ab9e45be":"code","aaa10dbd":"code","849569b7":"code","b0d5273f":"code","731707a0":"code","13e5cb6f":"code","98362877":"code","0d094f77":"code","3cc04474":"code","bc774200":"code","4ce8595b":"code","2cc56452":"code","9208e9fe":"code","61f33e24":"code","be8ca341":"code","d6198f7b":"code","e862dbc5":"code","287de15f":"markdown","6ac87036":"markdown"},"source":{"0842790f":"import pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\n\nmelbourne_file_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(melbourne_data.describe())","46ef8144":"# Drop houses where the target is missing\ntrain_data = melbourne_data.copy()\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\ntarget = melbourne_data.SalePrice\n\ncols_with_missing = [col for col in train_data.columns \n                                 if train_data[col].isnull().any()]                                  \n","1bd6d07f":"## One hot encoded Part ##\ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\ncandidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n# \"cardinality\" means the number of unique values in a column.\n# We use it as our only way to select categorical columns here. This is convenient, though\n# a little arbitrary.\nlow_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].nunique() < 10 and\n                                candidate_train_predictors[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_cardinality_cols + numeric_cols\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]\n\n\none_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\none_hot_encoded_test_predictors = one_hot_encoded_test_predictors.drop(['LotArea'],axis=1)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)","cebdeb0c":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mae(X, y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50), \n                                X, y, \n                                scoring = 'neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n\nmae_without_categoricals = get_mae(predictors_without_categoricals, target)\n\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n\nprint('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))","2ff7cd11":"## DecisionTree Part ##\ny = melbourne_data.SalePrice\nmelbourne_predictors = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = melbourne_data[melbourne_predictors]\n\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n\n# Fit model\nmelbourne_model.fit(X, y)","e1c40e18":"print(\"Making predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"The predictions are\")\nprint(melbourne_model.predict(X.head()))\nprint(y.head())","5b8379eb":"from sklearn.metrics import mean_absolute_error\n\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)","023702ce":"from sklearn.model_selection import train_test_split\n# split data into training and validation data, for both predictors and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.3, random_state = 0)\n# Define model\nmelbourne_model = DecisionTreeRegressor()\n# Fit model\nmelbourne_model.fit(train_X, train_y)\n\n# get predicted prices on validation data\nval_predictions = melbourne_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))","9650ad29":"def get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)","336c0260":"for max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","4b0fa549":"## RandomForest Part ##\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor()\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))","fb4849bd":"# test and submission\ntest_file = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\ntest_data = pd.read_csv(test_file) \n\nmelbourne_predictors = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nt_X = test_data[melbourne_predictors]\npredicted_prices = forest_model.predict(t_X)\n#print(predicted_prices)\n#my_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': predicted_prices})\n# you could use any filename. We choose submission here\n#my_submission.to_csv('submission.csv', index=False)","152e119f":"# Drop Columns with Missing Values\ntest_file = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\ntest_data = pd.read_csv(test_file) \noriginal_data = test_data.copy()\ncols_with_missing = [col for col in original_data.columns \n                                 if original_data[col].isnull().any()]\nredued_original_data = original_data.drop(cols_with_missing, axis=1)\nreduced_test_data = test_data.drop(cols_with_missing, axis=1)\n#print(reduced_test_data.columns)","e1e23ffe":"## Imupte Part ## just for learning\nfrom sklearn.impute import SimpleImputer\noriginal_data = test_data.copy()\nmelbourne_predictors = ['LotFrontage','LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\noriginal_data = original_data[melbourne_predictors]\n#print(original_data.isnull().sum())\n\nmy_imputer = SimpleImputer()\ndata_with_imputed_values = pd.DataFrame(my_imputer.fit_transform(original_data))\ndata_with_imputed_values.columns = original_data.columns\n#print(data_with_imputed_values)\n#print(data_with_imputed_values.isnull().sum())","14d21786":"# make copy to avoid changing original data (when Imputing)\nnew_data = original_data.copy()\n\n# make new columns indicating what will be imputed\ncols_with_missing = (col for col in new_data.columns \n                                 if new_data[col].isnull().any())\nfor col in cols_with_missing:\n    new_data[col + '_was_missing'] = new_data[col].isnull()\n#print(new_data)\n\n# Imputation\nmy_imputer = SimpleImputer()\n#data_with_imputed_values = my_imputer.fit_transform(new_data)\ndata_with_imputed_values = pd.DataFrame(my_imputer.fit_transform(new_data))\ndata_with_imputed_values.columns = new_data.columns\n#print(data_with_imputed_values)","e3c43c87":"# test the impute performance for the training data\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\ny = melbourne_data.SalePrice\nmelbourne_predictors = ['LotFrontage','LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = melbourne_data[melbourne_predictors]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","ab9e45be":"cols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\nprint(cols_with_missing)\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_test  = X_test.drop(cols_with_missing, axis=1)\nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nprint(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))","aaa10dbd":"my_imputer = SimpleImputer()\nimputed_X_train = my_imputer.fit_transform(X_train)\nimputed_X_test = pd.DataFrame(my_imputer.transform(X_test))\nprint(\"Mean Absolute Error from Imputation:\")\nprint(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))\n","849569b7":"# submission\ntest_file = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\ntest_data = pd.read_csv(test_file) \nmelbourne_predictors = ['LotFrontage','LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nt_X = test_data[melbourne_predictors]\nimputed_X_test = pd.DataFrame(my_imputer.transform(t_X))\n\nmodel = RandomForestRegressor()\nmodel.fit(imputed_X_train, y_train)\npreds = model.predict(imputed_X_test)\n#my_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': preds})\n# you could use any filename. We choose submission here\n#my_submission.to_csv('submission.csv', index=False)","b0d5273f":"## XGBoost Part ##\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = data.SalePrice\nX = data.drop(['Id','SalePrice'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25)\n\nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)\n\n\nmy_model = XGBRegressor()\n# Add silent=True to avoid printing out updates with each cycle\nmy_model.fit(train_X, train_y, verbose=False)\nX.columns","731707a0":"my_model = XGBRegressor(n_estimators=1000)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)], verbose=False)\n","13e5cb6f":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(train_X, train_y, early_stopping_rounds=20, \n             eval_set=[(test_X, test_y)], verbose=False)\n#print(train_X.columns)\npredictions = my_model.predict(test_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))","98362877":"from xgboost import plot_importance\nfrom matplotlib import pyplot\n\nfig,ax = pyplot.subplots(figsize=(15,15))\nplot_importance(my_model,\n                height=0.5,\n                ax=ax,\n                max_num_features=64)\npyplot.show()","0d094f77":"print(X.columns)\nxs = pd.Series(X.columns)","3cc04474":"f_num = [15, 2, 8, 5, 11, 4, 3, 6, 1, 13, 12, 26, 24]\nf_cl = [cl for x, cl in enumerate(xs) if x in f_num]\nprint(f_cl)\n#cols = ['OverallQual','GrLivArea', 'GarageCars','TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']","bc774200":"f_X = X.copy()\nf_X = f_X[f_cl]\nprint(f_X.columns)\ntrain_X, test_X, train_y, test_y = train_test_split(f_X, y, test_size=0.25)\n\nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(train_X, train_y, early_stopping_rounds=20, \n             eval_set=[(test_X, test_y)], verbose=False)\n#print(train_X.columns)\npredictions = my_model.predict(test_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))","4ce8595b":"# test and submission\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nt_X = test_data.select_dtypes(exclude=['object'])\nt_X = t_X.drop(['Id'], axis=1)\n# select \nt_X = t_X[f_cl]","2cc56452":"im_t_X = my_imputer.transform(t_X)\npredictions = my_model.predict(im_t_X)\nmy_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': predictions})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","9208e9fe":"## one hot encoded and impute part ## final model\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# one hot encoded\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = data.SalePrice\nX = data.drop(['Id','SalePrice'], axis=1)\n\none_hot_encoded_X = pd.get_dummies(X)\ntrain_X, test_X, train_y, test_y = train_test_split(one_hot_encoded_X, y, test_size=0.25)\n\nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)\n","61f33e24":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=5, colsample_bytree=0.7, subsample=0.7)\nmy_model.fit(train_X, train_y, early_stopping_rounds=20, \n             eval_set=[(test_X, test_y)], verbose=False)","be8ca341":"predictions = my_model.predict(test_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))","d6198f7b":"# test and submission\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nt_X = test_data.copy()\n\none_hot_encoded_test = pd.get_dummies(t_X)\nfinal_train, final_test = one_hot_encoded_X.align(one_hot_encoded_test,\n                                                                    join='left', \n                                                                    axis=1)\n","e862dbc5":"im_t_X = my_imputer.transform(final_test)\npredictions = my_model.predict(im_t_X)\nmy_submission = pd.DataFrame({'Id': test_data.Id, 'SalePrice': predictions})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","287de15f":"\n**If you have any questions or hit any problems, come to the [Learn Discussion](https:\/\/www.kaggle.com\/learn-forum) for help. **\n\n**Return to [ML Course Index](https:\/\/www.kaggle.com\/learn\/machine-learning)**","6ac87036":"# Introduction\n**This will be your workspace for the [Machine Learning course](https:\/\/www.kaggle.com\/learn\/machine-learning).**\n\nYou will need to translate the concepts to work with the data in this notebook, the Iowa data. Each page in the Machine Learning course includes instructions for what code to write at that step in the course.\n\n# Write Your Code Below"}}