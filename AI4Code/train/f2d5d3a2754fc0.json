{"cell_type":{"5d90eb74":"code","ef4df293":"code","bf4d02ea":"code","fa784c39":"code","4c4dbdc5":"code","78ecbd58":"code","3bb3817d":"code","368defd9":"code","dc12dac9":"code","cc90899c":"code","d27471b7":"code","3359ab71":"code","cd3eb5fb":"markdown","54b5cca8":"markdown","4fcbaa2c":"markdown","034398a1":"markdown","13d21376":"markdown","2073f447":"markdown","9b13678e":"markdown","41b826c0":"markdown","5479a2bd":"markdown"},"source":{"5d90eb74":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets, decomposition, linear_model, preprocessing, model_selection\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","ef4df293":"# Load Boston housing Dataset from sklearn \ndata_boston = datasets.load_boston()\nX = pd.DataFrame(data = data_boston.data, columns = data_boston.feature_names)\ny = pd.DataFrame(data = data_boston.target, columns = ['target'])\n","bf4d02ea":"# Preview the data\nX.head()","fa784c39":"# Scale data before performing PCA - to zero mean and unit variance\nX_scaled = preprocessing.scale(X)\n\n# Convert to DataFrame\nX_scaled = pd.DataFrame(X_scaled, columns = X.columns)\nX_scaled.describe()","4c4dbdc5":"# Initialize PCA transformer\npca = decomposition.PCA()\n\n# Fit it to data\npca.fit(X_scaled)","78ecbd58":"# Plot the cumulative variance\nfig = plt.figure(figsize = (10,4))\nplt.plot(np.arange(1, X.shape[1]+1),100 * np.cumsum(pca.explained_variance_ratio_), marker = 'o',\n         color = 'teal', alpha = .8)\nplt.xticks(np.arange(1, X.shape[1]+1),np.arange(1, X.shape[1]+1))\nplt.xlabel('Number of Components')\nplt.ylabel('Expained Variance %')\nplt.grid(linestyle = '--')","3bb3817d":"# Transform data\npca = decomposition.PCA().fit(X_scaled)\nX_pca = pca.transform(X_scaled)","368defd9":"# Save the pca weights in a dataframe\npca_component_directions = pd.DataFrame(pca.components_, columns = X.columns, \n                                        index = np.arange(1, X_pca.shape[1]+1))\n\n# Make a heatmap to show the contribution of each feature to each principal component\nfig = plt.figure(figsize = (12, 9))\nsns.heatmap(pca_component_directions, linewidth = .2, annot = True, cmap = 'coolwarm',\n            vmax = 1, vmin = -1)\nplt.ylabel('Components', fontsize = 13)\nplt.xlabel('Features', fontsize = 13)","dc12dac9":"# Extract the first 2 principal components\nX_2comps = decomposition.PCA(n_components = 2).fit_transform(X_scaled)\n\nfig = plt.figure(figsize = (10,7))\nplt.plot(X_2comps[:,0], X_2comps[:,1], marker = 'o', color = 'teal', alpha = .75, linewidth = 0)\nplt.xlabel('Principal Component 1', fontsize = 14)\nplt.ylabel('Principal Component 2', fontsize = 14)\nplt.grid(linestyle = '--')\n","cc90899c":"# Find the median value of y.\nthreshold = y.median()\nthreshold","d27471b7":"# Store binary y and X in a dataframe \nplot_2comps = pd.DataFrame(X_2comps, columns = ['PC1', 'PC2'])\n\n# Binarize y: replace y with 0 where y < threshold and with 1 where y>=threshold\ny_copy = y.copy()\ny_copy[y_copy < threshold] = 0\ny_copy[y_copy >= threshold] = 1\n\nplot_2comps['target'] = y_copy","3359ab71":"# Colour code the 2 Principal Component plot depending on whether target is 1 or 0\nX_2comps_0 = plot_2comps[plot_2comps.target == 0][['PC1', 'PC2']].copy()\nX_2comps_1 = plot_2comps[plot_2comps.target == 1][['PC1', 'PC2']].copy()\n\n# Teal Data points correspond to target < 21.2 and salmon points correspond to target >=21.2\nfig = plt.figure(figsize = (10,7))\nplt.plot(X_2comps_0['PC1'], X_2comps_0['PC2'], marker = 'o', color = 'teal', alpha = .75,\n         linewidth = 0, label = 'target = 0')\nplt.plot(X_2comps_1['PC1'], X_2comps_1['PC2'], marker = 'o', color = 'salmon', alpha = .75,\n         linewidth = 0, label = 'target = 1')\nplt.xlabel('Principal Component 1', fontsize = 14)\nplt.ylabel('Principal Component 2', fontsize = 14)\nplt.legend()\nplt.grid(linestyle = '--')","cd3eb5fb":"In the beginning we had defined X and y. Let us Binarize y such that low values of y are mapped to 0 and high values to 1. Then, we will colour code the above scatter plot to show which data points correspond to zero and 1.","54b5cca8":"Let us now look at how to visualize the entire data in 2 dimensions. For doing so, we will represent the data using its first 2 principal components and then make a scatter plot of the 2 components. Sometimes, in classification problems, a scatter plot of the first 2 principal components clearly shows the distinct classes. \n\nIt is also possible in classification problems to colour code the scatter plot to show the data corresponding to each class in a different colour. But first, let us pick the first 2 principal components and plot them.","4fcbaa2c":"Such plots can be made for multiclass cases also; It is a good way to visualize which data points belong to which class. In the above plot, we can see that most of the high target values are on the left side(salmon colour)  and most of the low target values(teal colour) are on the right. We can see what kind of a decision boundary must be required to classify the data. We must remember, however, that the above figure is only in 2 dimensions where the original data was in 13 dimensions. So, we are missing higher dimensional interactions where a better decision boundary might be possible but it is good to draw the above plot for a visualization of the data. These plots are also good for explaining to someone the nature of the data in terms of the target.","034398a1":"PCA is a method of representing data using fewer dimensions. If there is large collinearity in the data, PCA can be used to transform data to a feature space where there is no collinearity. However, PCA can be used to check if collinearity exists in data which will be explained later. \nPCA is also used to shrink the feature space - sometimes by orders of magnitude. \n\n","13d21376":"So, INDUS, NOX and TAX contribute the most to the first principal component. This is good for having a first understanding of the Principal Components. ","2073f447":"\n\n\nThe above cumulative variance graph shows that 10 components end up explaining just a little more than 95% of the data. At this point, we need to decide if we want to retain the 11th, 12th, 13th component. \nUsually, it is a good idea to keep the number of components that explain just more than 95% of data but another way could be to train a model on the selected features and retain the number of principal components that correspond to the lowest regression\/classification error(using k fold crossvalidation). The reason PCA could reduce model prediction error is that sometimes, the components explaining least variance(the last components), are just noise - they are variations that do not matter.  ","9b13678e":"### PCA Description and Visualization","41b826c0":"First, we check how many dimensions we want to transform our data to. We plot the cumulative \nvariance to understand how many dimensions we need to retain. However first, it is important to scale the data. PCA gives directions in feature space in along which variance of the data exists. Scaling the data before performing PCA is important because, scaling ensures that all the features are of comparable magnitude; otherwise, if one feature has large values, PCA will be biased towards that feature and will show maximum variance of data along that feature.","5479a2bd":"So mathematically, each Principal Component is a linear combination of all the features. Let us now see how much each feature contributes to each PCA component."}}