{"cell_type":{"c35ac1df":"code","88db0122":"code","2790f0d0":"code","2d78ff37":"code","15f53b95":"code","e3615d63":"code","43786909":"code","6f4cfbce":"code","b17fb951":"code","3d6c9ff3":"code","9d798056":"code","1801da82":"code","8f2c28e5":"code","fbf81d9f":"code","170de492":"code","b7aa38dc":"code","261ca4ce":"code","3c3ce9a5":"markdown","cc34979c":"markdown","b9f88d75":"markdown","a4facd67":"markdown","a7764529":"markdown","58457f0f":"markdown","2b3b51c9":"markdown","a48ff331":"markdown"},"source":{"c35ac1df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88db0122":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D,InputLayer, Conv2DTranspose, Dropout, BatchNormalization, Input, Concatenate, Activation, concatenate ,RepeatVector ,Reshape ,UpSampling2D\nfrom keras.initializers import RandomNormal\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import plot_model\nimport numpy as np\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nimport cv2\nimport PIL\nfrom skimage import transform\nfrom PIL import Image\nimport random\nimport h5py\nimport os\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n","2790f0d0":"images_gray = np.load(\"..\/input\/image-colorization\/l\/gray_scale.npy\")\nimages_ab1 = np.load(\"..\/input\/image-colorization\/ab\/ab\/ab1.npy\")\nimages_ab2 = np.load(\"..\/input\/image-colorization\/ab\/ab\/ab2.npy\")\nimages_ab3 = np.load(\"..\/input\/image-colorization\/ab\/ab\/ab3.npy\")\n","2d78ff37":"#taking training data of 100\nX_train = (images_gray[:250,:,:].astype('float')).reshape(250,224,224,1) #reshaping the input gray images\nY = (images_ab1[:250,:,:].astype('float'))\nX_test = (images_gray[300:550,:,:].astype('float')).reshape(250,224,224,1)\nY_test = (images_ab1[300:550,:,:].astype('float'))\n","15f53b95":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nfrom tensorflow.keras.applications.xception import Xception,decode_predictions,preprocess_input\nxception = Xception(weights='imagenet', include_top=True)\nxception.graph = tf.get_default_graph()","e3615d63":"def create_Xception_embedding(grayscaled_rgb):\n    import tensorflow.compat.v1 as tf\n    tf.disable_v2_behavior()\n    grayscaled_rgb_resized = []\n    for i in grayscaled_rgb:\n        i = resize(i, (299, 299, 3), mode='constant')\n        grayscaled_rgb_resized.append(i)\n    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n    with xception.graph.as_default():\n        embed = xception.predict(grayscaled_rgb_resized)\n    return embed","43786909":"xcept_em = create_Xception_embedding(X_train)\nembeddings = RepeatVector(28 * 28)(xcept_em)\nlayer_embedding_train = Reshape(([28, 28, 1000]))(embeddings)","6f4cfbce":"model_path = \".\/color_model.h5\"\ncheckpoint = ModelCheckpoint(model_path,\n                            monitor = \"val_loss\",\n                            mode=\"min\",\n                            save_best_only = True,\n                            verbose = 1)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","b17fb951":"#Encoder\nembed_input = Input(shape=(28, 28, 1000))\nencoder_input = Input(shape=(224, 224, 1,))\nencoder_1 = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\nencoder_2 = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_1)\nencoder_3 = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_2)\nencoder_4 = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_3)\nencoder_5 = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_4)\nencoder_6 = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_5)\nencoder_7 = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_6)\nencoder_output= Conv2D(256, (3,3), activation='relu', padding='same')(encoder_7)\n#Fusion layer\nfusion_output = concatenate([encoder_output, embed_input], axis=3) \nfusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\nfusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n#Decoder layer\ndecoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\ndecoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\ndecoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\ndecoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\ndecoder_output = Conv2D(2, (3, 3), activation='relu', padding='same')(decoder_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\nmodel = Model(inputs=[encoder_input,embed_input], outputs=decoder_output)\nmodel.summary()","3d6c9ff3":"model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\nhistory = model.fit(x=[X_train,layer_embedding_train] ,y=Y,callbacks = [checkpoint,es], batch_size=5, epochs=2500,steps_per_epoch=5)","9d798056":"plt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()\n","1801da82":"plt.plot(history.history['acc'])\nplt.title('Training Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('epoch')\nplt.show()","8f2c28e5":"xcept_em = create_Xception_embedding(X_test)\nembeddings = RepeatVector(28 * 28)(xcept_em)\nlayer_embedding_test = Reshape(([28, 28, 1000]))(embeddings)","fbf81d9f":"loss, acc = model.evaluate([X_test,layer_embedding_test] ,Y_test,steps=3)\nprint()\nprint(\"Test accuracy = \", acc)","170de492":"output = model.predict([X_test,layer_embedding_test],steps=3)\n","b7aa38dc":"def get_LAB(image_l, image_ab  ):\n       \n    image_l = image_l.reshape((224, 224, 1))\n    image_lab = np.concatenate((image_l, image_ab), axis=2)\n    image_lab = image_lab.astype(\"uint8\")\n \n    image_rgb = cv2.cvtColor(image_lab, cv2.COLOR_LAB2RGB)\n    image_rgb = Image.fromarray(image_rgb)\n    return image_rgb\ndef get_LAB1(image_l  ):\n    image_ab =  np.ones((224,224,2))*128\n    image_l = image_l.reshape((224, 224, 1))\n    image_lab = np.concatenate((image_l, image_ab), axis=2)\n    image_lab = image_lab.astype(\"uint8\")\n \n    image_rgb = cv2.cvtColor(image_lab, cv2.COLOR_LAB2RGB)\n    image_rgb = Image.fromarray(image_rgb)\n    return image_rgb","261ca4ce":"for i in range(50,100):\n    pred = get_LAB(X_test[i],output[i])\n    real = get_LAB(X_test[i],Y_test[i])\n    original = get_LAB1(X_test[i])\n    f, axarr = plt.subplots(1,3)\n    axarr[0].title.set_text('Black and white')  \n    axarr[1].title.set_text('Prediction')  \n    axarr[2].title.set_text('original')  \n    axarr[0].imshow(original)\n    axarr[1].imshow(pred)\n    axarr[2].imshow(real)","3c3ce9a5":"# Loss","cc34979c":"# Training.....","b9f88d75":"# Creating samples for training and testing","a4facd67":"# Loading pretrained Model ..... Xception","a7764529":"# ****** Loading the Data......**","58457f0f":"# The prediction (results)","2b3b51c9":"# Accuracy on training Data","a48ff331":"# THE MODEL"}}