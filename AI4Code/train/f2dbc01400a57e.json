{"cell_type":{"6df78bc9":"code","4aedb33f":"code","6022921c":"code","0932a5b6":"code","989bece5":"code","5f42d8c0":"code","85527909":"code","8ed9593a":"code","637d534d":"code","ca103b67":"code","a19930d6":"code","59321af4":"code","1b0ba9c1":"code","686317bc":"code","b76c8b4b":"code","8f7ea052":"code","cad9390f":"code","e5b540e1":"code","0524b41f":"code","7b0b3835":"code","e159c788":"code","8837c217":"code","f965e245":"code","1461a65a":"code","d5ec29b8":"code","821e592e":"code","36fec38c":"markdown","65a0fb52":"markdown","39b11b9a":"markdown","fa3aaf9c":"markdown","5b78872e":"markdown","995ca3ff":"markdown","5dd6fd08":"markdown","868e7c47":"markdown","bab03850":"markdown","93223cb2":"markdown"},"source":{"6df78bc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nprint(tf.__version__)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4aedb33f":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df =  pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","6022921c":"train_df.head(10)","0932a5b6":"print(len(train_df))\ntrain_df = train_df.drop_duplicates('text', keep='last')\nprint(len(train_df))","989bece5":"sns.countplot(train_df['target'])\n","5f42d8c0":"train_df['target'].value_counts()","85527909":"plt.figure(figsize=(15,40))\nprint(f'Unique keywords num={len(train_df.keyword.unique())}')\nprint(f'Unique keywords num={len(test_df.keyword.unique())}')\nsns.countplot(y=train_df['keyword'], color=(0,0,1), label='Train')\nsns.countplot(y=test_df['keyword'], color=(1,0,0), label='Test')\nplt.legend()","8ed9593a":"plt.figure(figsize=(15,100))\nsns.countplot(data=train_df, y='keyword', hue='target')","637d534d":"import re\n\ndef preprocess_text(text):\n    text = re.sub(r\"http\\S+\", \"\", text)\n    return text","ca103b67":"train_df.head(15)\n","a19930d6":"\n!pip install -U tensorflow_text==2.3","59321af4":"!pip install -q tf-models-official==2.3.0","1b0ba9c1":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization","686317bc":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(train_df['text'].tolist(),\\\n                                                      train_df['target'].tolist(),\\\n                                                      test_size=0.01,\\\n                                                      stratify = train_df['target'].tolist(),\\\n                                                      random_state=0)\n","b76c8b4b":"batch_size = 32\nseed = 42\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(batch_size)\nvalid_ds = tf.data.Dataset.from_tensor_slices((X_valid,y_valid)).batch(batch_size)\n#print(train_ds)","8f7ea052":"#@title Choose a BERT model to fine-tune\n\nbert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert\/bert_en_uncased_L-2_H-128_A-2\", \"small_bert\/bert_en_uncased_L-2_H-256_A-4\", \"small_bert\/bert_en_uncased_L-2_H-512_A-8\", \"small_bert\/bert_en_uncased_L-2_H-768_A-12\", \"small_bert\/bert_en_uncased_L-4_H-128_A-2\", \"small_bert\/bert_en_uncased_L-4_H-256_A-4\", \"small_bert\/bert_en_uncased_L-4_H-512_A-8\", \"small_bert\/bert_en_uncased_L-4_H-768_A-12\", \"small_bert\/bert_en_uncased_L-6_H-128_A-2\", \"small_bert\/bert_en_uncased_L-6_H-256_A-4\", \"small_bert\/bert_en_uncased_L-6_H-512_A-8\", \"small_bert\/bert_en_uncased_L-6_H-768_A-12\", \"small_bert\/bert_en_uncased_L-8_H-128_A-2\", \"small_bert\/bert_en_uncased_L-8_H-256_A-4\", \"small_bert\/bert_en_uncased_L-8_H-512_A-8\", \"small_bert\/bert_en_uncased_L-8_H-768_A-12\", \"small_bert\/bert_en_uncased_L-10_H-128_A-2\", \"small_bert\/bert_en_uncased_L-10_H-256_A-4\", \"small_bert\/bert_en_uncased_L-10_H-512_A-8\", \"small_bert\/bert_en_uncased_L-10_H-768_A-12\", \"small_bert\/bert_en_uncased_L-12_H-128_A-2\", \"small_bert\/bert_en_uncased_L-12_H-256_A-4\", \"small_bert\/bert_en_uncased_L-12_H-512_A-8\", \"small_bert\/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-12_H-768_A-12\/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_L-12_H-768_A-12\/3',\n    'small_bert\/bert_en_uncased_L-2_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-2_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-2_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-2_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-4_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-4_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-4_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-4_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-6_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-6_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-6_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-6_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-6_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-8_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-8_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-8_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-8_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-8_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-10_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-10_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-10_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-10_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-10_H-768_A-12\/1',\n    'small_bert\/bert_en_uncased_L-12_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-128_A-2\/1',\n    'small_bert\/bert_en_uncased_L-12_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-256_A-4\/1',\n    'small_bert\/bert_en_uncased_L-12_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-512_A-8\/1',\n    'small_bert\/bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-768_A-12\/1',\n    'albert_en_base':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_base\/2',\n    'electra_small':\n        'https:\/\/tfhub.dev\/google\/electra_small\/2',\n    'electra_base':\n        'https:\/\/tfhub.dev\/google\/electra_base\/2',\n    'experts_pubmed':\n        'https:\/\/tfhub.dev\/google\/experts\/bert\/pubmed\/2',\n    'experts_wiki_books':\n        'https:\/\/tfhub.dev\/google\/experts\/bert\/wiki_books\/2',\n    'talking-heads_base':\n        'https:\/\/tfhub.dev\/tensorflow\/talkheads_ggelu_bert_en_base\/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-2_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-2_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-2_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-2_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-4_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-4_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-4_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-4_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-6_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-6_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-6_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-6_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-8_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-8_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-8_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-8_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-10_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-10_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-10_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-10_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-12_H-128_A-2':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-12_H-256_A-4':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-12_H-512_A-8':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'small_bert\/bert_en_uncased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_preprocess\/2',\n    'albert_en_base':\n        'https:\/\/tfhub.dev\/tensorflow\/albert_en_preprocess\/2',\n    'electra_small':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'electra_base':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'experts_pubmed':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'experts_wiki_books':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n    'talking-heads_base':\n        'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')","cad9390f":"\ndef build_classifier_model():\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(8,activation =\"relu\",name =\"hidden\")(net)\n    net = tf.keras.layers.Dense(1, activation= \"sigmoid\" , name='classifier')(net)\n    return tf.keras.Model(text_input, net)\n    \n    \n   ","e5b540e1":"classifier_model = build_classifier_model()\n\ntf.keras.utils.plot_model(classifier_model)\nfor layer in classifier_model.layers[:-3]:\n      layer.trainable = False","0524b41f":"loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nmetrics = tf.metrics.BinaryAccuracy()","7b0b3835":"epochs = 30\nsteps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 2e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                      num_train_steps=num_train_steps,\n                                      num_warmup_steps=num_warmup_steps,\n                                      optimizer_type='adamw')\nclassifier_model.compile(optimizer=optimizer,\n                     loss=loss,\n                     metrics=metrics)","e159c788":"print(f'Training model with {tfhub_handle_encoder}')\nhistory = classifier_model.fit(x=train_ds, validation_data=valid_ds, epochs=epochs)\n","8837c217":"probs = classifier_model.predict(test_df[\"text\"]) \nthreshold = 0.4\npreds = np.where(probs[:,] > threshold, 1, 0)","f965e245":"#print(preds)","1461a65a":"submission=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","d5ec29b8":"submission[\"target\"]=preds","821e592e":"submission.to_csv('submission.csv', index=False, header=True)","36fec38c":"I will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model,  a Dropout layer followed by a Dense layer and the final output Dense layer","65a0fb52":"# Loading models from TensorFlow Hub\nHere you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n\nBERT-Base, Uncased and seven more models with trained weights released by the original BERT authors.\nSmall BERTs have the same general architecture but fewer and\/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\nALBERT: four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\nBERT Experts: eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\nElectra has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\nBERT with Talking-Heads Attention and Gated GELU [base, large] has two improvements to the core of the Transformer architecture.\n\n\n# The preprocessing model\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n\nThe preprocessing model must be the one referenced by the documentation of the BERT model, which you can read at the URL printed above. ","39b11b9a":"Training the model","fa3aaf9c":"Distribution of keyword across the target classes","5b78872e":"train_df['text'] = train_df['text'].apply(preprocess_text)\ntest_df ['text'] = test_df['text'].apply(preprocess_text)","995ca3ff":"* The distribution of target classes","5dd6fd08":"Simple preprocessing .Just removing URLs","868e7c47":"For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as AdamW.\n\nFor the learning rate (init_lr), we use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (num_warmup_steps). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5).","bab03850":"In this notebook, i will use Tf-hub BERT finetuning  for disaster tweet classification. I have adapated the code from this notebook:https:\/\/www.tensorflow.org\/tutorials\/text\/classify_text_with_bert","93223cb2":"Since this is a binary classification problem and the model outputs a probability (a single-unit layer), you'll use losses.BinaryCrossentropy loss function."}}