{"cell_type":{"50047dfb":"code","9c49554d":"code","2ea501b3":"code","f62d0536":"code","0d9d6a5d":"code","06974816":"code","c472036c":"code","3877b2bb":"code","771e45c0":"code","8d4fcf91":"code","5dd44092":"code","833822a1":"code","14e03298":"code","8e72602b":"code","3c625ec8":"code","dd1da785":"code","d947a6ea":"code","4a5f6792":"code","525508c0":"code","305107c9":"code","d77555b4":"code","606ea1aa":"code","cd17c480":"code","71d560c0":"code","dcd755e9":"code","7691fccf":"code","4029975e":"code","f500e899":"code","3ce35604":"code","c8c5af95":"code","33492354":"code","f2f528c2":"code","460b3e0f":"code","da3aa8c4":"code","5312af90":"code","5beaf50a":"code","19c56815":"code","88c890b6":"markdown","6eb79c25":"markdown","71aa62f1":"markdown","f2fed67c":"markdown","e49b534a":"markdown","a72fcc36":"markdown","430882a3":"markdown","6c2913e0":"markdown","22e8b1f6":"markdown","8143fada":"markdown","946105d2":"markdown","856532c1":"markdown","678d3f57":"markdown","91cf7b66":"markdown","153db69a":"markdown","76be212e":"markdown","8728a26f":"markdown","99fce054":"markdown","36fe1280":"markdown","c909d229":"markdown","5604786a":"markdown","371972a1":"markdown","e7ddfbf7":"markdown","f4546911":"markdown","f9588a85":"markdown","9f060e0c":"markdown","14add578":"markdown","1550ef33":"markdown","30a48818":"markdown","c88452c1":"markdown","0d9d8369":"markdown","a912ace5":"markdown"},"source":{"50047dfb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\nimport os\nos.listdir()","9c49554d":"df=pd.read_csv('..\/input\/add.csv',low_memory=False,header=0)\n","2ea501b3":"df=df.drop('Unnamed: 0',axis=1)\n","f62d0536":"df.columns=df.columns.astype('int')","0d9d6a5d":"df.dtypes.head(3)","06974816":"df.info() # data set information","c472036c":"df.iloc[:,0:3].info()","3877b2bb":"df[0][10]","771e45c0":"newdf=df.iloc[:,[0,1,2,3]]\nnewdf=newdf.applymap(lambda x:'?' in x)\nplt.figure(figsize=(7,5))\nsns.heatmap(newdf,cbar=False,yticklabels=False,cmap='viridis')","8d4fcf91":"for i in (newdf):\n    print('column['+str(i)+'] has missing values -'+str(sum(newdf[i])))","5dd44092":"def replace_missing(df):\n    for i in df:\n        df[i]=df[i].replace('[?]',np.NAN,regex=True).astype('float')\n        df[i]=df[i].fillna(df[i].mean())\n    return df\n","833822a1":"df[[0,1,2,3]]=replace_missing(df.iloc[:,[0,1,2,3]].copy()).values","14e03298":"df[3]=df[3].apply(lambda x:round(x))","8e72602b":"df[[0,1,2,3]].describe()","3c625ec8":"\nfig,ax=plt.subplots(nrows=1,ncols=3)\nfig.set_figheight(5)\nfig.set_figwidth(13)\nsns.distplot(df[0],ax=ax[0])\nsns.distplot(df[1],ax=ax[1])\nsns.distplot(df[2],ax=ax[2])","dd1da785":"sns.pairplot(data=df.iloc[:,[0,1,2,3,1558]])","d947a6ea":"fig,ax=plt.subplots(nrows=3,ncols=1)\nfig.set_figheight(15)\nfig.set_figwidth(18)\nsns.stripplot(y=1558,x=0,data=df,ax=ax[0])\nsns.stripplot(y=1558,x=1,data=df,ax=ax[1])\nsns.stripplot(y=1558,x=2,data=df,ax=ax[2])","4a5f6792":"sns.countplot(data=df,y=1558,palette='husl')","525508c0":"plt.figure(figsize=(15,10))\nsns.boxplot(x=1558,y=1,data=df)\nplt.xlabel('label-add\/non-ad')\nplt.ylabel('width')\n","305107c9":"df.iloc[:,-1]=df.iloc[:,-1].replace(['ad.','nonad.'],[1,0])","d77555b4":"x=df.iloc[:,:-1]\ny=df.iloc[:,-1]","606ea1aa":"from sklearn.preprocessing import StandardScaler\nscaled=StandardScaler()\nx=scaled.fit_transform(x)","cd17c480":"sns.pairplot(data=df.iloc[:,[0,1,2,-1]],hue=1558)","71d560c0":"from sklearn.cross_validation import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.30,random_state=8)","dcd755e9":"import collections\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import  cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\n\ndef fit_models(classifiers,xtrain,ytrain):\n    \"\"\"This function fit multiple models by sklearn and return the dictionary with values as  objects of models\"\"\"\n    models=collections.OrderedDict()\n    for constructor in classifiers:\n        obj=constructor()\n        obj.fit(xtrain,ytrain)\n        models[str(constructor).split(':')[0]]=obj\n    return models\n\ndef classification_multi_report(ytest,models_array):\n    \"\"\"This function generate classification accuracy report for given input model objects\"\"\"\n    for i in models_array:\n        print('__________________________________________________')\n        print('the model - '+str(i))\n        print(classification_report(ytest,models_array[i].predict(xtest)))\ndef cross_Fucntion(models,cv):\n    \"\"\"This function return cross validated accuray and the variance of given input model obejects\"\"\"\n    accuracy={}\n    for model in models:\n        cross_val_array=cross_val_score(models[model],xtrain,ytrain,scoring='accuracy',cv=cv)\n        accuracy[model]=[np.mean(cross_val_array),np.std(cross_val_array)]\n    return accuracy\n\ndef multi_grid_search(param_grid_array,estimator_list,x,y):\n    \"\"\"This function calculate the grid search parameters and accuracy  for given input modles and return dictionary with each tupple containing accuracy and best parameters\"\"\"\n    d={}\n    count=0\n    for i in estimator_list:\n        gc=GridSearchCV(estimator=estimator_list[i],param_grid=param_grid_array[count],scoring ='accuracy',cv=5).fit(x,y)\n        d[i]=(gc.best_params_,gc.best_score_)\n        count+=1\n    return d","7691fccf":"classifiers=[GaussianNB,SVC,KNeighborsClassifier]\n\nmodel_list=fit_models(classifiers,xtrain,ytrain)\n\nclassification_multi_report(ytest,model_list)","4029975e":"obj=cross_Fucntion(model_list,cv=20)\nfor model in obj:\n    print('the model -'+str(model)+'has \\n || crosss validated accuracy as  -> '+str(obj[model][0])+' | variance - '+str(obj[model][1])+' ||' )\n    print('______________________________________________________________________________________________________________')","f500e899":"\nparam_grid_svm=[\n    {\n        'kernel':['linear'],'random_state':[0]\n    },\n     {\n        'kernel':['rbf'],'random_state':[0]\n     },\n    \n    {\n        'kernel':['poly'],'degree':[1,2,3,4],'random_state':[0]\n    }\n]\n\nparam_grid_knn=[\n\n    {   \n        'n_neighbors':np.arange(1,50),\n        'p':[2]\n        \n    }\n]\n\nparam_grid_nb=[\n    {}\n]\n\nparam_grid_array=[param_grid_nb,param_grid_svm,param_grid_knn]\nmulti_grid_search(param_grid_array,model_list,xtrain,ytrain)","3ce35604":"classifier=SVC(kernel='poly',degree=1,random_state=0)","c8c5af95":"classifier.fit(xtrain,ytrain)\n","33492354":"sns.heatmap(pd.crosstab(ytest,classifier.predict(xtest)),cmap='coolwarm')\nplt.xlabel('predicted')\nplt.ylabel('actual')\n","f2f528c2":"print(classification_report(ytest,classifier.predict(xtest)))","460b3e0f":"df.head()","da3aa8c4":"x=df.iloc[:,:-1].values","5312af90":"from sklearn.cluster import KMeans\ndef best_knumber_cluster(x,iter_number):\n    wwss=[]\n    for i in range(1,iter_number+1):\n        kmeans=KMeans(n_clusters=i)\n        kmeans.fit(x)\n        wwss.append(kmeans.inertia_)\n    plt.figure(figsize=(20,10))\n    c=plt.plot(np.arange(1,iter_number+1),wwss,marker='o',markersize=10,markerfacecolor='black')\n    plt.xlabel('number of clusters')\n    plt.ylabel('wwss')\n    plt.title('Elbow Curve')\n    \n    return plt.show()\n\n\nbest_knumber_cluster(x,15)","5beaf50a":"kmeans=KMeans(n_clusters=3,init='k-means++',max_iter=300,n_init=100)\nkmeans.fit(x)\nnewdf=df.copy()\nnewdf['Cluster']=kmeans.predict(x)\n","19c56815":"fig = plt.figure(figsize=(20,12))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(kmeans.cluster_centers_[0,0],kmeans.cluster_centers_[0,1],kmeans.cluster_centers_[0,2],c='yellow',marker='x',s=300)\nax.scatter(kmeans.cluster_centers_[1,0],kmeans.cluster_centers_[1,1],kmeans.cluster_centers_[1,2],c='yellow',marker='x',s=300,label='centroid')\nax.scatter(kmeans.cluster_centers_[2,0],kmeans.cluster_centers_[2,1],kmeans.cluster_centers_[2,2],c='yellow',marker='x',s=300)\n\nax.scatter(newdf[newdf['Cluster']==0][0],newdf[newdf['Cluster']==0][1],newdf[newdf['Cluster']==0][2],c='blue',s=newdf[2]*10,label='Cluster-1')\nax.scatter(newdf[newdf['Cluster']==1][0],newdf[newdf['Cluster']==1][1],newdf[newdf['Cluster']==1][2],c='red',s=newdf[2]*10,label='Cluster-2')\nax.scatter(newdf[newdf['Cluster']==2][0],newdf[newdf['Cluster']==2][1],newdf[newdf['Cluster']==2][2],c='green',s=newdf[2]*10,label='Cluster-3')\nax.legend()\nplt.xlabel('height')\nplt.ylabel('width')\nax.set_zlabel('aspect ratio')","88c890b6":"####  Frequency of missing values","6eb79c25":"## Confusion matrix\n","71aa62f1":"####  Information of First three continous variables\n- 28% data is missing for three continous attributes","f2fed67c":"#### Feature Engineering","e49b534a":"This dataset represents a set of possible advertisements on Internet pages. \n<h3>The features encode :-<\/h3> \n- the geometry of the image (if available) \n- phrases occuring in the URL\n- the image's URL and alt text\n- the anchor text, \n- words occuring near the anchor text\n<h4 style=\"color:red\"> The task is to predict whether an image is an advertisement (\"ad\") or not (\"nonad\")<\\h4>\n\n\n-The aim is to classify based on the given features given the features mentioned\n\n\n","a72fcc36":"# Model Selection\n- Splitting data \n- applying models\n- k fold cross vaidations\n- building classification confusion matrix","430882a3":">    How classes are in frequency","6c2913e0":"Statsitical approach\n\n- data is right skewed","22e8b1f6":"> Most optimal number of clusters ","8143fada":"## Exporatory  Data Analysis(E.D.A)","946105d2":"Encoding last column(class variable)\n* '0' - non advertisement image\n* '1' - addvertisement image","856532c1":"Fitting model with best hyperparmater and cv score","678d3f57":"> #### *Plotting discrete data","91cf7b66":"### applying kfold cross validation to check biase and variance in the model","153db69a":"- how data and central tendency is distributed \n- Boxplot helps to see the difference in quartiles,mean and the outliers","76be212e":"#### Preparing features for model","8728a26f":"Relations between three continous variables","99fce054":"**Fitting the train data **","36fe1280":"### Exploring the dataset","c909d229":"- we can see that EDA becomes more clear after scaling data ","5604786a":"## Clustering-\n- we applied kmeans clustering to find out clusters based upon IMAGE   AND FEATURES","371972a1":"#### Filling missing data with mean of each attribute\n","e7ddfbf7":"#### Information regarding data as general\n","f4546911":"<img src=\"http:\/\/phoenix-tec.com\/wp-content\/uploads\/2015\/01\/Online-Advertising-8.png\">","f9588a85":"Since we can see that missing values are there so first we have to replace missing values\n- yellow strips represent the missing data","9f060e0c":"#### Attribute Information:\n\n(3 continous; others binary; this is the \"STANDARD encoding\" mentioned in the [Kushmerick, 99].) \n\nOne or more of the three continous features are missing in 28% of the instances; missing values should be interpreted as \"unknown\".\n\n","14add578":"\n- First three colums are important as most of the variation between results are because of first three continuous varibles\n\n* column1-> height of image\n* column2->widht of image\n* column30->aspect ratio","1550ef33":"3 clusters are most optimal","30a48818":"### Importing resources and toolkits","c88452c1":"#                                  Advertisements Images detection -U.C.I","0d9d8369":"#### we can see that dimetions,aspect ratio  form 3 different cluster for given image","a912ace5":"> ### Scaleing data\n- Since the data is highly varied and hence mikowski mertric would have a large varition of distances given a small change\n- gradient decent will not work effective unless all input attributes are scaled to a same version"}}