{"cell_type":{"7d4adb7d":"code","a169acc8":"code","cabe2e1d":"code","52b90650":"code","45f3f5f0":"code","9bbf2de4":"code","a4eb9ed6":"code","4a1225b2":"code","2798062b":"code","ed0c93ed":"code","9f94795e":"code","13ed350c":"code","a4b90a64":"code","1f3b058a":"code","35062467":"code","18a46f64":"code","e069d33e":"code","8e797624":"markdown","524a131e":"markdown","93d6f4de":"markdown","c5a76c10":"markdown","588deece":"markdown","bf424bca":"markdown","07cb2d96":"markdown","7a52dfc3":"markdown","60dcea71":"markdown","d8c50c78":"markdown","a664d76a":"markdown","ba2f7836":"markdown","a7784f9e":"markdown","e9f513fd":"markdown","c6f0617d":"markdown","6e961704":"markdown","777f48f5":"markdown","a01bc5e5":"markdown","822f8a38":"markdown"},"source":{"7d4adb7d":"import numpy as np\nimport pandas as pd\n\nfrom random import seed\nfrom random import randrange\nfrom math import sqrt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score","a169acc8":"path = '..\/input\/glass\/glass.csv'\ndf = pd.read_csv(path)\ndf.head()","cabe2e1d":"features = df.drop('Type', axis=1)\nlabels = df['Type'].values\nlabels = labels.reshape(len(labels), 1)","52b90650":"features.describe().T","45f3f5f0":"scaler = StandardScaler()\nfeatures = scaler.fit_transform(features)","9bbf2de4":"dataset = np.append(features, labels, axis=1).tolist()","a4eb9ed6":"def kfold(dataset, n_folds):\n    dataset_split = list()\n    dataset_copy = list(dataset)\n    fold_size = int(len(dataset) \/ n_folds)\n    \n    for i in range(n_folds):\n        fold = list()\n        while len(fold) < fold_size:\n            index = randrange(len(dataset_copy))\n            fold.append(dataset_copy.pop(index))\n        dataset_split.append(fold)\n        \n    return dataset_split","4a1225b2":"n_folds = 5\n\nfolds = kfold(dataset, n_folds)","2798062b":"def euclidean_distance(row1, row2):\n    distance = 0.0\n    for i in range(len(row1)-1):\n        distance += (row1[i] - row2[i])**2\n    return sqrt(distance)","ed0c93ed":"def random_subset(train):\n    n_records = len(train)\n    n_features = len(train[0])\n    subsets = [train[randrange(n_records)][i] for i in range(n_features)]\n    return subsets","9f94795e":"def best_match(subsets, test_row):\n    distances = list()\n    \n    for subset in subsets:\n        dist = euclidean_distance(subset, test_row)\n        distances.append((subset, dist))\n        \n    distances.sort(key=lambda tup: tup[1])\n    return distances[0][0]","13ed350c":"def lvq(train_set, n_subsets, lrate, epochs):\n    subsets = [random_subset(train_set) for i in range(n_subsets)]\n\n    for epoch in range(epochs):\n        rate = lrate * (1.0-(epoch\/float(epochs)))\n        \n        for row in train_set:\n            bmu = best_match(subsets, row)\n            \n            for i in range(len(row)-1):\n                error = row[i] - bmu[i]\n                if bmu[-1] == row[-1]:\n                    bmu[i] += rate * error\n                else:\n                    bmu[i] -= rate * error\n                    \n    return subsets","a4b90a64":"def accuracy(actual, predicted):\n    correct = 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            correct += 1\n    return correct \/ float(len(actual)) * 100.0","1f3b058a":"def train_test_split(folds, fold):\n    train_set = list(folds)\n    train_set.remove(fold)\n    train_set = sum(train_set, [])\n    test_set = list()\n    return train_set, test_set","35062467":"lrate = 0.1\nepochs = 500\nn_subsets = 50","18a46f64":"scores = list()\n\nfor fold in folds:\n    train_set, test_set = train_test_split(folds, fold)\n\n    for row in fold:\n        test_set.append(list(row))\n\n    subsets = lvq(train_set, n_subsets, lrate, epochs)\n    y_hat = list()\n    \n    for test_row in test_set:\n        output = best_match(subsets, test_row)[-1]\n        y_hat.append(output)\n    \n    y = [row[-1] for row in fold]\n    scores.append(accuracy(y, y_hat))","e069d33e":"print('Accuracy per fold: {:}'.format(scores))\nprint('Max Accuracy: {:.3f}'.format(max(scores)))","8e797624":"## Accuracy function","524a131e":"LVQ can be understood as a special case of an artificial neural network, more precisely, it applies a winner-take-all Hebbian learning-based approach. It is a precursor to self-organizing maps (SOM) and related to neural gas, and to the k-nearest neighbor algorithm (k-NN). LVQ was invented by Teuvo Kohonen.\n\nAn LVQ system is represented by prototypes W = ( w ( i ) , . . . , w ( n ) ) which are defined in the feature space of observed data. In winner-take-all training algorithms one determines, for each data point, the prototype which is closest to the input according to a given distance measure. The position of this so-called winner prototype is then adapted, i.e. the winner is moved closer if it correctly classifies the data point or moved away if it classifies the data point incorrectly.\n\nAn advantage of LVQ is that it creates prototypes that are easy to interpret for experts in the respective application domain. LVQ systems can be applied to multi-class classification problems in a natural way. It is used in a variety of practical applications. See the 'Bibliography on the Self-Organizing Map (SOM) and Learning Vector Quantization (LVQ)'.\n\nA key issue in LVQ is the choice of an appropriate measure of distance or similarity for training and classification. Recently, techniques have been developed which adapt a parameterized distance measure in the course of training the system, see e.g. (Schneider, Biehl, and Hammer, 2009) and references therein.\n\nReference: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Learning_vector_quantization)","93d6f4de":"<h1 id=\"training\" style=\"color:#0132bd; background:#c2d5e6; border:0.5px dotted #00b9f2;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","c5a76c10":"<h1 id=\"folds\" style=\"color:#0132bd; background:#c2d5e6; border:0.5px dotted #00b9f2;\"> \n    <center>Folds\n        <a class=\"anchor-link\" href=\"#folds\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","588deece":"## Results","bf424bca":"## Training","07cb2d96":"<h1 id=\"euclidean\" style=\"color:#0132bd; background:#c2d5e6; border:0.5px dotted #00b9f2;\"> \n    <center>Euclidean distance\n        <a class=\"anchor-link\" href=\"#euclidean\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","7a52dfc3":"## Parameters","60dcea71":"## Standardize the features","d8c50c78":"## Merge the dataset","a664d76a":"## Load dataset","ba2f7836":"## Learning Vector Quantization training function","a7784f9e":"## Best match unit using euclidean distance","e9f513fd":"<div width=\"100%\">\n    <img width=\"100%\" src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/738\/1370\/0a1267f68de353bec843f056c8343009\/dataset-cover.jpg\"\/>\n<\/div>","c6f0617d":"In mathematics, the Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, therefore occasionally being called the Pythagorean distance. These names come from the ancient Greek mathematicians Euclid and Pythagoras, although Euclid did not represent distances as numbers, and the connection from the Pythagorean theorem to distance calculation was not made until the 18th century. \n\nReference: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Euclidean_distance)","6e961704":"## Train\/Test dataset split","777f48f5":"<h1 id=\"dataset\" style=\"color:#0132bd; background:#c2d5e6; border:0.5px dotted #00b9f2;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","a01bc5e5":"## Features described","822f8a38":"<h1 id=\"lvq\" style=\"color:#0132bd; background:#c2d5e6; border:0.5px dotted #00b9f2;\"> \n    <center>Learning vector quantization\n        <a class=\"anchor-link\" href=\"#lvq\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}