{"cell_type":{"ca104afd":"code","b0f78a04":"code","a8b9c41c":"code","fa1ba5f1":"code","a6010b91":"code","94e953ff":"code","d08f2ae4":"code","5c95ab15":"code","f2db1830":"code","b577ff75":"code","09319bf7":"code","11d5e00c":"code","a6c59ca0":"code","1f53525c":"code","8f374f26":"code","5f93a5e1":"code","b5706c10":"code","0c3efccc":"code","9e4273c8":"code","88722544":"code","14cba7e8":"code","5b177f6e":"code","44a33680":"code","28062838":"code","769f8d72":"code","2e4ab20c":"code","d91b7754":"code","4b8cd59d":"code","ed73eb32":"code","7b6da754":"code","fb9eb333":"code","69081f9b":"code","b2edeeda":"code","162a4466":"code","ddc0eb85":"code","561b95b1":"code","06b6ee3a":"markdown"},"source":{"ca104afd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","b0f78a04":"def read_files(filepath):\n    with open(filepath) as f:\n        str_txt = f.read()\n        \n    return str_txt","a8b9c41c":"read_files('..\/input\/moby_dick_four_chapters.txt')","fa1ba5f1":"import spacy\nnlp = spacy.load('en', disable=['parser','tagger','ner'])\nnlp.max_length = 1198623","a6010b91":"def separate_punc(doc_text):\n    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\ \\n \\n\\\\n \\\\n\\\\n\\\\n!\\\"-#$%&()--.*+,-\/:;<=>?@[\\\\\\\\]^_`{|}~\\\\t\\\\n ']","94e953ff":"d = read_files('..\/input\/moby_dick_four_chapters.txt')\ntokens = separate_punc(d)\nlen(tokens)","d08f2ae4":"train_len = 25+1\ntext_sequences = []\n\nfor i in range(train_len,len(tokens)):\n    seq = tokens[i-train_len:i]\n    text_sequences.append(seq)","5c95ab15":"' '.join(text_sequences[0])","f2db1830":"from keras.preprocessing.text import Tokenizer","b577ff75":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_sequences)\nsequences = tokenizer.texts_to_sequences(text_sequences)","09319bf7":"for i in sequences[0]:\n    print(f\"{i} : {tokenizer.index_word[i]}\")","11d5e00c":"tokenizer.word_counts","a6c59ca0":"vocabulary_size = len(tokenizer.word_counts)\nvocabulary_size","1f53525c":"type(sequences)","8f374f26":"import numpy as np","5f93a5e1":"sequences = np.array(sequences)\nsequences","b5706c10":"from keras.utils import to_categorical","0c3efccc":"X = sequences[:,:-1]\ny = sequences[:,-1]\ny","9e4273c8":"y = to_categorical(y,num_classes=vocabulary_size+1)\ny","88722544":"seq_len = X.shape[1]","14cba7e8":"seq_len","5b177f6e":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM","44a33680":"def create_model(vocabulary_size,seq_len):\n    model = Sequential()\n    model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\n    model.add(LSTM(150, return_sequences=True))\n    model.add(LSTM(150))\n    model.add(Dense(vocabulary_size, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    model.summary()\n    \n    return model","28062838":"model = create_model(vocabulary_size+1, seq_len)","769f8d72":"from pickle import dump, load","2e4ab20c":"model.fit(X, y, batch_size=150, epochs=200, verbose=1)","d91b7754":"model.save('my_mobydick_model.h5')\ndump(tokenizer, open('my_simpletokenizer','wb'))","4b8cd59d":"from keras.preprocessing.sequence import pad_sequences","ed73eb32":"def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n    output_text = [] \n    input_text = seed_text\n    \n    for i in range(num_gen_words):\n        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n        pre_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n        pred_word = tokenizer.index_word[pre_word_ind]\n        input_text += ' '+pred_word\n        output_text.append(pred_word)\n        \n    return ' '.join(output_text) ","7b6da754":"text_sequences[0]","fb9eb333":"import random\nrandom.seed(101)\nrandom_pick = random.randint(0, len(text_sequences))","69081f9b":"random_seed_text = text_sequences[random_pick]","b2edeeda":"random_seed_text","162a4466":"seed_text = ' '.join(random_seed_text)","ddc0eb85":"seed_text","561b95b1":"generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25)","06b6ee3a":"25 words ---> network predict #26"}}