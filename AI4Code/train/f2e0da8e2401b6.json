{"cell_type":{"c83c7273":"code","c0d6fa23":"code","78d53dfe":"code","2c1933af":"code","df5e4f7e":"code","de46165d":"code","1f1ed7ed":"code","56ef6c30":"code","6c145369":"code","cb6b68a0":"code","dd11b187":"code","55642f17":"code","57ae920b":"code","17294d98":"code","ab25a904":"code","4f71830c":"code","29f9921a":"code","3bc8744a":"code","d7049b86":"code","b08f80c7":"code","3dbe3fd8":"code","0c809bb3":"code","0010801b":"code","e4fdc4fd":"code","a0393a8f":"code","9db3c04c":"code","3e66950b":"code","8b8ac6bb":"code","c3e47606":"code","39cd8976":"code","62d61b74":"code","5752bc83":"code","3a76ce61":"code","2df9150b":"code","35c1c2bb":"code","6404d3d0":"code","58ccb2a1":"code","a55192cd":"code","0d8fb1f3":"code","69895b61":"code","77857fb9":"code","a0a29415":"code","edf0a8b1":"code","a6e38403":"code","e98e0976":"code","7a98f9c7":"code","c19b1036":"code","cb90bf06":"code","92d2ba2e":"code","31af2111":"code","33b345e1":"code","7e2e782a":"code","e4e026b2":"code","0eded692":"code","e28d8dac":"code","19a936e7":"code","3eb71a45":"code","2e7d91fb":"code","c9959ac3":"code","beb79631":"code","6e634391":"code","32efa659":"code","1eb1fc15":"code","800f0b64":"code","a0c78d03":"code","b81c7bde":"code","543c0dfc":"code","d244b775":"code","a7ac733a":"code","ae4c5427":"code","78b15c49":"code","f7349f84":"code","dea74a50":"code","93df68ac":"code","dd1a542a":"code","5c1e5d4e":"code","43cfb782":"code","a48bc632":"code","fa6f3134":"code","8cbb041a":"code","8784387f":"code","801f14cc":"code","b502f842":"code","922b016b":"code","9c734e9b":"code","f8ea2a1f":"code","2afab6f6":"code","f437c35b":"code","9645d0b2":"code","03a2fe93":"code","2a0d65be":"code","0bf92b5c":"code","fbf681e4":"code","b7922bd5":"code","3163078a":"code","1430af48":"code","b56db1c7":"code","4a307b22":"markdown","fcd392c8":"markdown","667c366f":"markdown","9d9c93f9":"markdown","ed431b1c":"markdown","7afa5929":"markdown","f61ab7fe":"markdown","45d1883c":"markdown","baf02332":"markdown","916d84bd":"markdown","c62bdb0b":"markdown","0e512709":"markdown","a2b0421b":"markdown","8820cfe0":"markdown","496f3f7b":"markdown","61a90e89":"markdown","4b680be8":"markdown","1d8c2644":"markdown","7e9fbc4b":"markdown","126c4c6f":"markdown","770a0572":"markdown","12fa615b":"markdown","3b2df505":"markdown","bdbfcc9e":"markdown","2bbb590d":"markdown","f9f15773":"markdown","d30fbcdc":"markdown","49de1cf3":"markdown","f3261ac9":"markdown","05a00728":"markdown","a4587186":"markdown","d0f6f978":"markdown","9f9c0123":"markdown","ba97fbe9":"markdown","ad1eb542":"markdown","5d2a5b1e":"markdown","18eb3dfe":"markdown","1f8b149d":"markdown","5b0917a9":"markdown","69b69d09":"markdown","483adee4":"markdown"},"source":{"c83c7273":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c0d6fa23":"ecommerce_df = pd.read_csv('\/kaggle\/input\/ecommerce-customers\/Ecommerce Customers.csv')\necommerce_df.head()","78d53dfe":"# Rename Cols for ease of manipulate\necommerce_df.columns = [re.sub('[.\\s]+', '_', c.lower()) for c in ecommerce_df.columns] #.str.lower() #.replace(r'[+]','_')","2c1933af":"ecommerce_df['address'][0].splitlines()[1].split()","df5e4f7e":"ecommerce_df['state'] = ecommerce_df['address'].map(lambda x: x.splitlines()[1].split()[-2])\necommerce_df['zip_code'] = ecommerce_df['address'].map(lambda x: x.splitlines()[1].split()[-1])\necommerce_df['email_domain'] = ecommerce_df['email'].apply(lambda email: email[email.find(\"@\")+1:].split('.')[0])\necommerce_df['avatar_theme'] = [re.findall(r'[A-Z][^A-Z]*', a)[-1] for a in ecommerce_df.avatar] ","de46165d":"ecommerce_df.head()","1f1ed7ed":"ecommerce_df.avatar.value_counts().plot.bar(figsize=(20, 4), title='Distribution by Avatars')","56ef6c30":"ecommerce_df.avatar_theme.value_counts().plot.bar(figsize=(15, 4), title='Distribution by Email Themes')","6c145369":"## Simplify \necommerce_df['avatar_theme3'] = ecommerce_df['avatar_theme'].apply(lambda x: x if x == 'Blue' or x == 'Green' else 'Others')\necommerce_df.avatar_theme3.value_counts().plot.bar(figsize=(15, 4), title='Distribution by Avatar Themes - Simplified')","cb6b68a0":"ecommerce_df.state.value_counts().plot.bar(figsize=(15, 4), title='Distribution by States')","dd11b187":"ecommerce_df.email_domain.value_counts().sort_values(ascending=False).head(20).plot.bar(figsize=(15, 4), title='Distribution by Email Domains')","55642f17":"## Simplify \necommerce_df['email_domain4'] = ecommerce_df['email_domain'].apply(lambda x: x if x in ['hotmail','gmail','yahoo'] else 'Others')\necommerce_df.email_domain4.value_counts().plot.bar(figsize=(15, 4), title='Distribution by Email Domains - Simplified')","57ae920b":"sns.pairplot(ecommerce_df)","17294d98":"sns.heatmap(ecommerce_df.corr(), annot=True)","ab25a904":"ecommerce_df.groupby('email_domain').yearly_amount_spent.median().sort_values(ascending=False).head(70).plot.bar(figsize=(15, 4))","4f71830c":"ecommerce_df.groupby('email_domain4').yearly_amount_spent.median().sort_values() #.head(20).plot.bar(figsize=(15, 4))","29f9921a":"ecommerce_df.groupby('avatar_theme').yearly_amount_spent.median().sort_values(ascending=False).plot.bar(figsize=(15, 4))","3bc8744a":"ecommerce_df.groupby('avatar_theme3').yearly_amount_spent.median().sort_values()","d7049b86":"ecommerce_df.groupby('state').yearly_amount_spent.median().sort_values(ascending=False).plot.bar(figsize=(15, 4), title='Avg. Yearly Spent x States')","b08f80c7":"# quantity of yearly_amount_spent (numeric) is the target of regression problem\nplt.figure(figsize = (6,4))\nsns.distplot(ecommerce_df['yearly_amount_spent'])","3dbe3fd8":"# create the binary variable (=1 if yearly_amount_spent > q0.8)\necommerce_df['yearly_amount_spent'].quantile(0.8)\necommerce_df['is_highval'] = ecommerce_df['yearly_amount_spent'] > ecommerce_df['yearly_amount_spent'].quantile(0.8)\necommerce_df['is_highval'].value_counts()","0c809bb3":"# bin plots (The bin into percentiles make the linearity clearer)\nx_var = 'time_on_app' # Try other: avg_session_length, time_on_app, time_on_website, length_of_membership\ny_var = 'yearly_amount_spent'\necommerce_df[x_var + '_10bin'] = pd.qcut(ecommerce_df[x_var], q=10, labels=False) # divide to 10 bins equals in n\necommerce_df.groupby(x_var + '_10bin')[y_var].mean().plot(title='Linearity check: {} vs. {}'.format(x_var, y_var))","0010801b":"# Split cat and numeric variables\ncat_cols = ['avatar_theme3','email_domain4','state']\nnum_cols = ['avg_session_length','time_on_app','time_on_website','length_of_membership']\ny_col = ['yearly_amount_spent']\ny_class = ['is_highval']\none_hot_df = pd.get_dummies(ecommerce_df[cat_cols])\none_hot_cols = one_hot_df.columns\ndf_model = pd.merge(ecommerce_df, one_hot_df, left_index=True, right_index=True)\ndf_model = df_model[num_cols + list(one_hot_cols) + y_class + y_col]\ndf_model.head()","e4fdc4fd":"# Split train set - test set\nX = df_model.iloc[:,:-2]\ny = df_model.iloc[:,-1]\ny_class = df_model.iloc[:,-2]","a0393a8f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","9db3c04c":"X_train, X_test, y_class_train, y_class_test = train_test_split(X, y_class, test_size=0.3, random_state=42)","3e66950b":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nscaled_X_train = StandardScaler().fit_transform(X_train.iloc[:,:4]) #[:,:4]\nkmeans = KMeans(init=\"random\",\n                n_clusters=3,\n                n_init=10,\n                max_iter=300,\n                random_state=42)\nkmeans.fit(scaled_X_train)","8b8ac6bb":"print('The lowest SSE value: {}'.format(kmeans.inertia_))\nprint('Number of Iterations to Converge: {}'.format(kmeans.n_iter_))\nprint('--- Locations of the Centroid ---')\nprint(kmeans.cluster_centers_)","c3e47606":"kmeans.labels_[:5]","39cd8976":"kmeans_kwargs = {\n    \"init\": \"random\",\n    \"n_init\": 10,\n    \"max_iter\": 300,\n    \"random_state\": 42,\n}\n\n# A list holds the SSE values for each k\nsse = []\nfor k in range(1, 30):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(scaled_X_train)\n    sse.append(kmeans.inertia_)","62d61b74":"plt.figure(figsize = (15,6))\nplt.plot(range(1, 30), sse)\nplt.xticks(range(1, 30))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()","5752bc83":"!pip install kneed","3a76ce61":"# Determine by kneed\nfrom kneed import KneeLocator\nkl = KneeLocator(range(1, 30), sse, curve=\"convex\", direction=\"decreasing\")\nprint('K-mean Elbow: {}'.format(kl.elbow))","2df9150b":"from sklearn.metrics import silhouette_score\n# A list holds the silhouette coefficients for each k\nsilhouette_coefficients = []\n\n# Notice you start at 2 clusters for silhouette coefficient\nfor k in range(2, 11):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(scaled_X_train)\n    score = silhouette_score(scaled_X_train, kmeans.labels_)\n    silhouette_coefficients.append(score)","35c1c2bb":"plt.figure(figsize = (15,6))\nplt.plot(range(2, 11), silhouette_coefficients)\nplt.xticks(range(2, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()","6404d3d0":"features = ['avg_session_length', 'time_on_app','time_on_website','length_of_membership']\nkmeans_3 = KMeans(n_clusters=3, **kmeans_kwargs).fit(ecommerce_df[features])\necommerce_df['cluster'] = kmeans_3.labels_\nsns.pairplot(data=ecommerce_df, hue=\"cluster\")","58ccb2a1":"ecommerce_df.groupby(['cluster'])[features].median()","a55192cd":"g = sns.FacetGrid(data=ecommerce_df, col='cluster')\ng.map(sns.distplot, 'yearly_amount_spent')\ng = sns.FacetGrid(data=ecommerce_df, col='cluster')\ng.map(sns.regplot, 'yearly_amount_spent','avg_session_length')\ng = sns.FacetGrid(data=ecommerce_df, col='cluster')\ng.map(sns.regplot, 'yearly_amount_spent','length_of_membership')\ng = sns.FacetGrid(data=ecommerce_df, col='cluster')\ng.map(sns.regplot, 'avg_session_length','length_of_membership')","0d8fb1f3":"from sklearn.decomposition import PCA\npca = PCA(n_components=2, whiten=True)\npca.fit(ecommerce_df[features])\nprint('Explained Variance Ratio:' + str(pca.explained_variance_ratio_))","69895b61":"dataPCA = pca.transform(ecommerce_df[features])\ndataPCA = pd.DataFrame(dataPCA, columns= ['PC1','PC2'])\ndataPCA['yearly_amount_spent'] = ecommerce_df['yearly_amount_spent']\ndataPCA.head()","77857fb9":"plt.figure(figsize=(10,5))\nsns.relplot(x=\"PC1\", y=\"PC2\", size=\"yearly_amount_spent\",\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            height=6, data=dataPCA)","a0a29415":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nclusterer = Pipeline([(\n    'kmeans', KMeans(\n        n_clusters=3,\n        init=\"k-means++\",\n        n_init=50,\n        max_iter=500,\n        random_state=42,\n    ),\n)])\npreprocessor = Pipeline([\n    ('minmax_scaler', MinMaxScaler()), \n    ('pca', PCA(n_components=2, random_state=42)),\n])\n\npipe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('clusterer', clusterer)\n])","edf0a8b1":"pipe.fit(ecommerce_df[features])","a6e38403":"preprocessed_data = pipe['preprocessor'].transform(ecommerce_df[features])\npredicted_labels = pipe['clusterer']['kmeans'].labels_\nprint('Silhouette Score: {}'.format(silhouette_score(preprocessed_data, predicted_labels)))","e98e0976":"data = ecommerce_df[features]\npcadf = pd.DataFrame(pipe[\"preprocessor\"].transform(data), columns=[\"PC1\", \"PC2\"])\npcadf['k_cluster'] = pipe[\"clusterer\"][\"kmeans\"].labels_\npcadf['k_cluster'] = pcadf['k_cluster'].apply(str)\npcadf['yearly_amount_spent'] = ecommerce_df['yearly_amount_spent']\npcadf.head()","7a98f9c7":"pcadf.info()","c19b1036":"plt.figure(figsize=(10,5))\nsns.relplot(x=\"PC1\", y=\"PC2\", hue='k_cluster', size=\"yearly_amount_spent\",\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            data=pcadf)","cb90bf06":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('std_scalar', StandardScaler())\n])\n\npca_pipeline = Pipeline([\n    ('minmax_scaler', MinMaxScaler()), \n    ('pca', PCA(n_components=2, random_state=42))\n])\n\nX_train = pipeline.fit_transform(X_train)\nX_train_pca = pca_pipeline.fit_transform(X_train)\n\nX_test = pipeline.transform(X_test)\nX_test_pca = pca_pipeline.transform(X_test)","92d2ba2e":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val(model):\n    pred = cross_val_score(model, X, y, cv=10)\n    return pred.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square","31af2111":"def eval_class(true, predicted):\n    acc = metrics.accuracy_score(true, predicted)\n    precision = metrics.precision_score(true, predicted)\n    recall = metrics.recall_score(true, predicted)\n    f1 = metrics.f1_score(true, predicted)\n    log_loss = metrics.log_loss(true, predicted)\n    auc = metrics.roc_auc_score(true, predicted)\n    return acc, precision, recall, f1, log_loss, auc","33b345e1":"def plot_feature_importance(importance,names,model_type):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","7e2e782a":"import shap\n# Summary bee-swarm plot to show the global importance of each features and distribution of effect size\ndef plot_shap_feature_importance(model, X, feature_names):\n    shap_values = shap.TreeExplainer(xgb).shap_values(X)\n    shap.summary_plot(shap_values, X, feature_names)\n    \n# Dependence Plot to investigate the interaction between features\ndef plot_shap_feature_interaction(model, X, feature_names, f_orders = (1,2)):\n    shap_interaction_values = shap.TreeExplainer(xgb).shap_interaction_values(X)\n    shap.dependence_plot(f_orders, shap_interaction_values, X,feature_names)","e4e026b2":"from sklearn.linear_model import LinearRegression\n\n# All features\nlin_reg1 = LinearRegression(normalize=True)\nlin_reg1.fit(X_train,y_train)","0eded692":"# print the intercept\nprint(lin_reg1.intercept_)\ncoeff_df = pd.DataFrame(lin_reg1.coef_, X.columns, columns=['Coefficient'])\ncoeff_df.head()","e28d8dac":"pred = lin_reg1.predict(X_test)\n\nplt.figure(figsize = (6,4))\nsns.jointplot(x= y_test,y= pred,kind='reg')\n#plt.title('Scatterplot between Y_test and Ytest_pred')\nplt.grid()","19a936e7":"plt.figure(figsize = (15,6))\nx_ax = range(len(X_test))\nplt.plot(x_ax, y_test, lw=1, color=\"blue\", label=\"original\")\nplt.plot(x_ax, pred, lw=0.8, color=\"red\", label=\"predicted\",marker=\"o\", markersize=4)\nplt.legend()\nplt.grid()\nplt.show()","3eb71a45":"sns.distplot((y_test - pred), bins=50);","2e7d91fb":"test_pred = lin_reg1.predict(X_test)\ntrain_pred = lin_reg1.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","c9959ac3":"eval_metrics = ['MAE', 'MSE', 'RMSE', 'R2_Square']\neval_m_train = [m + '_train' for m in eval_metrics]\neval_m_test = [m + '_test' for m in eval_metrics]\ndis_m = ['Model','RMSE_train'] + eval_m_test + ['diff_RMSE_train_test']\nresults_df = pd.DataFrame(data=[[\"Linear Regression\",*evaluate(y_train, train_pred),*evaluate(y_test, test_pred), cross_val(LinearRegression())]], \n                          columns=[['Model'] + eval_m_train + eval_m_test + ['CV']])\nresults_df['diff_RMSE_train_test'] = results_df.apply(lambda x: (x.RMSE_test - x.RMSE_train)\/x.RMSE_train, axis=1)\nresults_df[dis_m]","beb79631":"# Try the model specifications to remove the cat variables\n# All features\nlin_reg2 = LinearRegression(normalize=True)\nfeatures_list = X.columns[:4]\nlin_reg2.fit(X_train[:,:4],y_train) ## First 4 cols are numeric\n# print the intercept\nprint(lin_reg2.intercept_)\ncoeff_df = pd.DataFrame(lin_reg2.coef_, features_list, columns=['Coefficient'])\ncoeff_df","6e634391":"## Prediction\ntest_pred2 = lin_reg2.predict(X_test[:,:4])\ntrain_pred2 = lin_reg2.predict(X_train[:,:4])","32efa659":"## Evaluation\nresults_df2 = pd.DataFrame(data=[[\"Linear Regression - Numeric\",*evaluate(y_train, train_pred2),*evaluate(y_test, test_pred2), 0]], \n                          columns=[['Model'] + eval_m_train + eval_m_test + ['CV']])\nresults_df2['diff_RMSE_train_test'] = results_df2.apply(lambda x: (x.RMSE_test - x.RMSE_train)\/x.RMSE_train, axis=1)\nresults_df = results_df.append(results_df2, ignore_index=True)\nresults_df[dis_m]","1eb1fc15":"## Fitting\nlin_reg2b = LinearRegression(normalize=True)\nlin_reg2b.fit(X_train_pca,y_train) ## First 4 cols are numeric\n\n## Prediction\ntest_pred2b = lin_reg2b.predict(X_test_pca)\ntrain_pred2b = lin_reg2b.predict(X_train_pca)","800f0b64":"## Evaluation\nresults_df2b = pd.DataFrame(data=[[\"Linear Regression - PCA\",*evaluate(y_train, train_pred2b),*evaluate(y_test, test_pred2b), 0]], \n                          columns=[['Model'] + eval_m_train + eval_m_test + ['CV']])\nresults_df2b['diff_RMSE_train_test'] = results_df2b.apply(lambda x: (x.RMSE_test - x.RMSE_train)\/x.RMSE_train, axis=1)\nresults_df = results_df.append(results_df2b, ignore_index=True)\nresults_df[dis_m]","a0c78d03":"from sklearn.preprocessing import PolynomialFeatures\n## Processing\npoly_reg = PolynomialFeatures(degree=2)\nX_train_2d = poly_reg.fit_transform(X_train[:,:4])\nX_test_2d = poly_reg.transform(X_test[:,:4])\n## Fitting\nlin_reg3 = LinearRegression(normalize=True)\nlin_reg3.fit(X_train_2d,y_train)\n## Prediction\ntest_pred3 = lin_reg3.predict(X_test_2d)\ntrain_pred3 = lin_reg3.predict(X_train_2d)","b81c7bde":"## Evaluation\nresults_df3 = pd.DataFrame(data=[[\"Polynomial Regression\",*evaluate(y_train, train_pred3),*evaluate(y_test, test_pred3), 0]], \n                          columns=[['Model'] + eval_m_train + eval_m_test + ['CV']])\nresults_df3['diff_RMSE_train_test'] = results_df3.apply(lambda x: (x.RMSE_test - x.RMSE_train)\/x.RMSE_train, axis=1)\nresults_df = results_df.append(results_df3, ignore_index=True)\nresults_df[dis_m]","543c0dfc":"from sklearn.svm import SVR # Support Vector Regression\n\n## Fitting\nsvm_reg = SVR(kernel='rbf', C=1, epsilon=20)\nsvm_reg.fit(X_train[:,:4], y_train)\n## Prediction\ntest_pred4 = svm_reg.predict(X_test[:,:4])\ntrain_pred4 = svm_reg.predict(X_train[:,:4])","d244b775":"## Evaluation\nresults_df4 = pd.DataFrame(data=[[\"SVM Regressor\",*evaluate(y_train, train_pred4),*evaluate(y_test, test_pred4), 0]], \n                          columns=[['Model'] + eval_m_train + eval_m_test + ['CV']])\nresults_df4['diff_RMSE_train_test'] = results_df4.apply(lambda x: (x.RMSE_test - x.RMSE_train)\/x.RMSE_train, axis=1)\nresults_df = results_df.append(results_df4, ignore_index=True)\nresults_df[dis_m]","a7ac733a":"import plotly.graph_objects as go # for data visualization\nimport plotly.express as px # for data visualization\nx_var = 'avg_session_length'\ny_var = 'yearly_amount_spent'\n# Note, we need X to be a 2D array, hence reshape\nX=df_model[x_var].values.reshape(-1,1)\ny=df_model[y_var].values\n# ------- SVR vs. LR -------\nlr = LinearRegression().fit(X, y)\nsvr = SVR(kernel='rbf', C=10, epsilon=20).fit(X, y)\n# ------- Predict a range of values based on the models for visualization -------\n# Create 100 evenly spaced points from smallest X to largest X\nx_range = np.linspace(X.min(), X.max(), 100)\n# Predict y values for our set of X values\ny_lr = lr.predict(x_range.reshape(-1, 1)) # Linear regression\ny_svr = lr.predict(x_range.reshape(-1, 1)) # SVR\n\nfig = px.scatter(df_model, x=df_model[x_var], y=df_model[y_var], opacity=0.8, color_discrete_sequence=['black'])\n\n# Add a best-fit line\nfig.add_traces(go.Scatter(x=x_range, y=y_lr, name='Linear Regression', line=dict(color='limegreen')))\nfig.add_traces(go.Scatter(x=x_range, y=y_svr, name='Support Vector Regression', line=dict(color='red')))\nfig.add_traces(go.Scatter(x=x_range, y=y_svr+20, name='+epsilon', line=dict(color='red', dash='dot')))\nfig.add_traces(go.Scatter(x=x_range, y=y_svr-20, name='-epsilon', line=dict(color='red', dash='dot')))\n\n# Change chart background color\nfig.update_layout(dict(plot_bgcolor = 'white'))\n# Update axes lines\nfig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n                 showline=True, linewidth=1, linecolor='black')\n\nfig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n                 showline=True, linewidth=1, linecolor='black')\n\n# Set figure title\nfig.update_layout(title=dict(text=\"Yearly Amount Spent Prediction\", \n                             font=dict(color='black')))\n# Update marker size\nfig.update_traces(marker=dict(size=3))\n\nfig.show()","ae4c5427":"from sklearn.ensemble import RandomForestRegressor\n\n# Fitting\nrf_reg = RandomForestRegressor(n_estimators=1000, max_depth=3)\nrf_reg.fit(X_train[:,:4], y_train)\n\n# Prediction\ntest_pred5 = rf_reg.predict(X_test[:,:4])\ntrain_pred5 = rf_reg.predict(X_train[:,:4])\n\n## Evaluation\nresults_df5 = pd.DataFrame(data=[[\"RF Regressor\",*evaluate(y_train, train_pred5),*evaluate(y_test, test_pred5), 0]], \n                          columns=[['Model'] + eval_m_train + eval_m_test + ['CV']])\nresults_df5['diff_RMSE_train_test'] = results_df5.apply(lambda x: (x.RMSE_test - x.RMSE_train)\/x.RMSE_train, axis=1)\nresults_df = results_df.append(results_df5, ignore_index=True)\nresults_df[dis_m]","78b15c49":"plt.figure(figsize = (6,4))\nplot_feature_importance(rf_reg.feature_importances_,['avg_session_length', 'time_on_app','time_on_website','length_of_membership'],'RF Regressors')","f7349f84":"!pip install dtreeviz","dea74a50":"from sklearn import tree\nfrom dtreeviz.trees import dtreeviz # will be used for tree visualization\nfrom matplotlib import pyplot as plt\nplt.rcParams.update({'figure.figsize': (10, 20)})\nplt.rcParams.update({'font.size': 14})\nviz = dtreeviz(rf_reg.estimators_[0], X_train[:,:4], y_train, feature_names=df_model.columns[:4], target_name=\"yearly_amount_spent\")\nviz","93df68ac":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance\n## Fitting\nxgb = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n# Add silent=True to avoid printing out updates with each cycle\nxgb.fit(X_train[:,:4], y_train, early_stopping_rounds=5, eval_set=[(X_test[:,:4], y_test)], verbose=False)\n# Prediction\ntest_pred6 = xgb.predict(X_test[:,:4])\ntrain_pred6 = xgb.predict(X_train[:,:4])\n\n## Evaluation\nresults_df6 = pd.DataFrame(data=[[\"XGB Regressor\",*evaluate(y_train, train_pred6),*evaluate(y_test, test_pred6), 0]], \n                          columns=[['Model'] + eval_m_train + eval_m_test + ['CV']])\nresults_df6['diff_RMSE_train_test'] = results_df5.apply(lambda x: (x.RMSE_test - x.RMSE_train)\/x.RMSE_train, axis=1)\nresults_df = results_df.append(results_df6, ignore_index=True)\nresults_df[dis_m]","dd1a542a":"xgb.feature_names = ['avg_session_length', 'time_on_app','time_on_website','length_of_membership']\nplt.figure(figsize = (15,6))\nplot_feature_importance(xgb.feature_importances_, xgb.feature_names, 'XGB')","5c1e5d4e":"feature_names = ['avg_session_length', 'time_on_app','time_on_website','length_of_membership']\nplot_shap_feature_importance(xgb, X_test[:,:4], feature_names)","43cfb782":"plot_shap_feature_interaction(xgb, X_test[:,:4],['avg_session_length', 'time_on_app','time_on_website','length_of_membership'], (3,0))","a48bc632":"results_df[dis_m]","fa6f3134":"from sklearn.tree import DecisionTreeClassifier \nclf = DecisionTreeClassifier(random_state=1234, max_depth=3)\nclf_model = clf.fit(X_train[:,:4], y_class_train)","8cbb041a":"plot_feature_importance(clf_model.feature_importances_,['avg_session_length', 'time_on_app','time_on_website','length_of_membership'],'Decision Tree Classifier')","8784387f":"viz = dtreeviz(clf, X_train[:,:4], y_class_train,\n                target_name='is_highval',\n                feature_names=['avg_session_length','time_on_app','time_on_website','length_of_membership'],\n                class_names=[0,1])\n\nviz","801f14cc":"# Prediction\ntest_pred_class1 = clf_model.predict(X_test[:,:4])\ntrain_pred_class1 = clf_model.predict(X_train[:,:4])\n# Confusion Matrix\nmetrics.confusion_matrix(test_pred_class1, y_class_test)","b502f842":"eval_clm_metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'Log_loss','AUC']\neval_clm_train = [m + '_train' for m in eval_clm_metrics]\neval_clm_test = [m + '_test' for m in eval_clm_metrics]\ndis_clm = ['Model','Accuracy_train'] + eval_clm_test + ['diff_Acc_train_test']\nres_clm = pd.DataFrame(data=[[\"Decision Tree Classifier\",*eval_class(y_class_train, train_pred_class1),\n                                 *eval_class(y_class_test, test_pred_class1)]],\n                          columns=[['Model'] + eval_clm_train + eval_clm_test])\nres_clm['diff_Acc_train_test'] = res_clm.apply(lambda x: (x.Accuracy_test - x.Accuracy_train)\/x.Accuracy_train, axis=1)\nres_clm[dis_clm]","922b016b":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold # Technique for tunning the model\n\n# Training\nlogit_model = LogisticRegression(random_state=5, class_weight='balanced')\nlogit_model = logit_model.fit(X_train[:,:4], y_class_train)\n\n# Prediction\ntest_pred_class2 = logit_model.predict(X_test[:,:4])\ntrain_pred_class2 = logit_model.predict(X_train[:,:4])\n# Confusion Matrix\nmetrics.confusion_matrix(test_pred_class2, y_class_test)","9c734e9b":"# Evaluation on Train & Test set\nres_clm2 = pd.DataFrame(data=[[\"Logistic Model\",*eval_class(y_class_train, train_pred_class2),\n                                 *eval_class(y_class_test, test_pred_class2)]],\n                          columns=[['Model'] + eval_clm_train + eval_clm_test])\nres_clm2['diff_Acc_train_test'] = res_clm2.apply(lambda x: (x.Accuracy_test - x.Accuracy_train)\/x.Accuracy_train, axis=1)\nres_clm = res_clm.append(res_clm2, ignore_index=True)\nres_clm[dis_clm]","f8ea2a1f":"from sklearn.ensemble import GradientBoostingClassifier\n\n# Training\ngbm_cmodel = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01)\ngbm_cmodel = gbm_cmodel.fit(X_train[:,:4], y_class_train)\n\n# Prediction\ntest_pred_class3 = gbm_cmodel.predict(X_test[:,:4])\ntrain_pred_class3 = gbm_cmodel.predict(X_train[:,:4])\n\n# Confusion Matrix\nmetrics.confusion_matrix(test_pred_class3, y_class_test)","2afab6f6":"# Evaluation on Train & Test set\nres_clm3 = pd.DataFrame(data=[[\"GBM Classifier\",*eval_class(y_class_train, train_pred_class3),\n                                 *eval_class(y_class_test, test_pred_class3)]],\n                          columns=[['Model'] + eval_clm_train + eval_clm_test])\nres_clm3['diff_Acc_train_test'] = res_clm3.apply(lambda x: (x.Accuracy_test - x.Accuracy_train)\/x.Accuracy_train, axis=1)\nres_clm = res_clm.append(res_clm3, ignore_index=True)\nres_clm[dis_clm]","f437c35b":"from lightgbm import LGBMClassifier\n\n# Training\nlgbm_cmodel = LGBMClassifier()\nlgbm_cmodel = lgbm_cmodel.fit(X_train[:,:4], y_class_train)\n\n# Prediction\ntest_pred_class4 = lgbm_cmodel.predict(X_test[:,:4])\ntrain_pred_class4 = lgbm_cmodel.predict(X_train[:,:4])\n# Confusion Matrix\nmetrics.confusion_matrix(test_pred_class4, y_class_test)","9645d0b2":"# Evaluation on Train & Test set\nres_clm4 = pd.DataFrame(data=[[\"LightGBM Classifier\",*eval_class(y_class_train, train_pred_class4),\n                                 *eval_class(y_class_test, test_pred_class4)]],\n                          columns=[['Model'] + eval_clm_train + eval_clm_test])\nres_clm4['diff_Acc_train_test'] = res_clm4.apply(lambda x: (x.Accuracy_test - x.Accuracy_train)\/x.Accuracy_train, axis=1)\nres_clm = res_clm.append(res_clm4, ignore_index=True)\nres_clm[dis_clm]","03a2fe93":"from xgboost import XGBClassifier\n\n## Fitting\nxgb_cmodel = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n# Add silent=True to avoid printing out updates with each cycle\nxgb_cmodel.fit(X_train[:,:4], y_class_train, early_stopping_rounds=5, eval_set=[(X_test[:,:4], y_class_test)], verbose=False)\n\n# Prediction\ntest_pred_class5 = xgb_cmodel.predict(X_test[:,:4])\ntrain_pred_class5 = xgb_cmodel.predict(X_train[:,:4])\n# Confusion Matrix\nmetrics.confusion_matrix(test_pred_class5, y_class_test)","2a0d65be":"# Evaluation on Train & Test set\nres_clm5 = pd.DataFrame(data=[[\"XGBoost Classifier\",*eval_class(y_class_train, train_pred_class5),\n                                 *eval_class(y_class_test, test_pred_class5)]],\n                          columns=[['Model'] + eval_clm_train + eval_clm_test])\nres_clm5['diff_Acc_train_test'] = res_clm5.apply(lambda x: (x.Accuracy_test - x.Accuracy_train)\/x.Accuracy_train, axis=1)\nres_clm = res_clm.append(res_clm5, ignore_index=True)\nres_clm[dis_clm]","0bf92b5c":"from sklearn.ensemble import RandomForestClassifier\n\n## Fitting\nrf_cmodel = RandomForestClassifier(n_estimators=1000, max_depth=3)\n# Add silent=True to avoid printing out updates with each cycle\nrf_cmodel.fit(X_train[:,:4], y_class_train)\n\n# Prediction\ntest_pred_class6 = rf_cmodel.predict(X_test[:,:4])\ntrain_pred_class6 = rf_cmodel.predict(X_train[:,:4])\n# Confusion Matrix\nmetrics.confusion_matrix(test_pred_class6, y_class_test)","fbf681e4":"# Evaluation on Train & Test set\nres_clm6 = pd.DataFrame(data=[[\"RF Classifier\",*eval_class(y_class_train, train_pred_class6),\n                                 *eval_class(y_class_test, test_pred_class6)]],\n                          columns=[['Model'] + eval_clm_train + eval_clm_test])\nres_clm6['diff_Acc_train_test'] = res_clm6.apply(lambda x: (x.Accuracy_test - x.Accuracy_train)\/x.Accuracy_train, axis=1)\nres_clm = res_clm.append(res_clm6, ignore_index=True)\nres_clm[dis_clm]","b7922bd5":"from sklearn.naive_bayes import GaussianNB\n\n## Fitting\nnb_cmodel = GaussianNB()\n# Add silent=True to avoid printing out updates with each cycle\nnb_cmodel.fit(X_train[:,:4], y_class_train)\n\n# Prediction\ntest_pred_class7 = nb_cmodel.predict(X_test[:,:4])\ntrain_pred_class7 = nb_cmodel.predict(X_train[:,:4])\n# Confusion Matrix\nmetrics.confusion_matrix(test_pred_class7, y_class_test)","3163078a":"# Evaluation on Train & Test set\nres_clm7 = pd.DataFrame(data=[[\"NBayes Classifier\",*eval_class(y_class_train, train_pred_class7),\n                                 *eval_class(y_class_test, test_pred_class7)]],\n                          columns=[['Model'] + eval_clm_train + eval_clm_test])\nres_clm7['diff_Acc_train_test'] = res_clm7.apply(lambda x: (x.Accuracy_test - x.Accuracy_train)\/x.Accuracy_train, axis=1)\nres_clm = res_clm.append(res_clm7, ignore_index=True)\nres_clm[dis_clm]","1430af48":"res_clm[dis_clm]","b56db1c7":"#Predicting proba\nytest_pred_prob1 = clf_model.predict_proba(X_test[:,:4])[:,1]\nytest_pred_prob2 = logit_model.predict_proba(X_test[:,:4])[:,1]\nytest_pred_prob3 = gbm_cmodel.predict_proba(X_test[:,:4])[:,1]\nytest_pred_prob4 = lgbm_cmodel.predict_proba(X_test[:,:4])[:,1]\nytest_pred_prob5 = xgb_cmodel.predict_proba(X_test[:,:4])[:,1]\nytest_pred_prob6 = rf_cmodel.predict_proba(X_test[:,:4])[:,1]\nytest_pred_prob7 = nb_cmodel.predict_proba(X_test[:,:4])[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_class_test, ytest_pred_prob1)\nfpr2, tpr2, thresholds2 = metrics.roc_curve(y_class_test, ytest_pred_prob2)\nfpr3, tpr3, thresholds3 = metrics.roc_curve(y_class_test, ytest_pred_prob3)\nfpr4, tpr4, thresholds4 = metrics.roc_curve(y_class_test, ytest_pred_prob4)\nfpr5, tpr5, thresholds5 = metrics.roc_curve(y_class_test, ytest_pred_prob5)\nfpr6, tpr6, thresholds6 = metrics.roc_curve(y_class_test, ytest_pred_prob6)\nfpr7, tpr7, thresholds7 = metrics.roc_curve(y_class_test, ytest_pred_prob7)\n\n# Plot ROC curve\nplt.figure(figsize = (15,6))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr1, tpr1)\nplt.plot(fpr2, tpr2)\nplt.plot(fpr3, tpr3)\nplt.plot(fpr4, tpr4)\nplt.plot(fpr5, tpr5)\nplt.plot(fpr6, tpr6)\nplt.plot(fpr7, tpr7)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve: High-Value Users Classifiers')\nplt.legend(['Baseline','DecisionTreeClassifier','Logit','GBM', 'LightGBM', 'XGBoost', 'RF','Naive Bayes'], loc='lower right')\nplt.show()","4a307b22":"# \u2714\ufe0f Evaluation\n> Reference: What eval metrics, scikit offers [HERE](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html)\n\n### Regression\n* **MAE**: Mean Absolute Error => Easiest to understand, just average error (take absolute)\n* **MSE**: Mean Square Error => More popular, because MSE \"punishes\" larger errors, useful in the real world\n* **RMSE**: even more popular in MSE, because RMSE is interpretable in the \"y\" units\n\n\n### Classification\n> Referece: [Performance Evaluation Metrics for Classification](https:\/\/www.kdnuggets.com\/2020\/04\/performance-evaluation-metrics-classification.html)\n\nAll these coming from the confusion matrix:\n\n![image.png](attachment:76f3d46d-1cb1-471d-a403-bb491d1c4aee.png)\n\n* **Accuracy**: How accurate the model classify to the right class (cautious with imbalance)\n* **Precision**: More False Positive, the lower precision (False Positive is tolerant for high-risk problems, i.e. COVID diagnostic)\n* **Recall (Sensitivity)**: More False Negative, the lower recall (False Negative is tolerant, Spam)\n* **F1**: single score balances both the concerns of precision and recall. Good F1-score = low Precision, low Recall. Perfect = 1, Failure = 0\n* **ROC Curve**: Plot of tpr (true positive rate) vs fpr (false positive rate) => Higher area under the curve better the models\n\n![image.png](attachment:810cb181-c6ba-4067-9ca4-956f9a0d7058.png)","fcd392c8":"# Reference\n1. [Predicting Credit Risk - Model Pipeline](https:\/\/www.kaggle.com\/kabure\/predicting-credit-risk-model-pipeline): A notebook with ML pipeline for classification\n2. [k-mean Clustering Pipeline](https:\/\/realpython.com\/k-means-clustering-python\/): Pipeline in using clustring, and PCA to processing data","667c366f":"# \ud83d\udcca EDA: Continuous\n* The very first variable to pay attention is `yearly_amount_spent`, this is highly correlated with `length_of_membership` => Intuitively speaking, the longer you stay the more value you generated\n* EDA would aim to look into distribution of each variables, and the interactions among them (especially the interaction with target variables)\n* Though the relationship with other variables seems unclear. But we observe a positive correlation (in heatmap) between `yearly_amount_spent` vs. `time_on_app` and `session_length`\n> Note: Or check out this Streamlit Data Explore App [HERE](https:\/\/share.streamlit.io\/anhdanggit\/streamlit-data-glimpse\/main\/app.py), built on the top of Pandas-profiling","9d9c93f9":"# Common ML Algorithms in One Notebook\nThe purpose of this All-in-one Notebook is to illustrate the modelling workflow of common ML techniques (Processing, Modeling, Predicting, Evaluating).\nThe tuning is not focused. You are recommended to do further research on it.\n\nThere are 2 main problems in ML:\n* **Regressions**: Predicting a quantity (In this example: `yearly_amount_spent`)\n* **Classifications**: Predicting a class (For example: High-value users with `yearly_amount_spent` > q0.8)\n\n> *Note: Though it might not show in this notebook: SVM, Boosting (XGBoost, GBM, LightGBM), Tree-based (Decision-Tree, Random Forest) => Can work on borht problems with Regressor and Classifier, respectively (Fore example, in this notebook `XGBRegressor` and `XGBClassifier`*\n\nThere are 3 main types of ML algorithms:\n1. **Supervised Learning**: Have an observed label\/target\/outcome to be predicted from a set of predictors. We generate a functions to map inputs to the desired outputs (Linear Regressions, RF, XGBoost, etc). There are 2 main problems in ML:\n    - Regressions: Predicting a quantity - For example: `yearly_amount_spent`\n    - Classification: Classify the observations into class (label) - For example: `High-Value Users` (one with `yearly_amount_spent` > q0.8)\n2. **Unsupervised Learning**: No target, but we cluster the sample\/population into different groups that within each group they are quite similar, but between groups they are different (K-means) (Read More: [Clustering](https:\/\/realpython.com\/k-means-clustering-python\/)). Clustering have 2 key purposes:\n    - Analytics cluster: expland the knowledge and give more information about the data (customer segmentation)\n    - Useful cluster: use as an intermediate step in a data pipeline\n3. **Reinforcement Learning**: model is trained to make specific decisions, exposed to the environment that it trains itself continually using trial-and-error (Markov Decision Process)\n\n> Reference: [Learn ML Kaggle Series](https:\/\/www.kaggle.com\/dansbecker\/learn-machine-learning)","ed431b1c":"## \u2714\ufe0f Check Linearity (For Linear Regression)","7afa5929":"## \u2714\ufe0f PCA for K-Means\n\nHere we apply the Pipeline from sklearn.pipeline to make the progress become smoother (and more elegant).\nWe apply PCA, then doing the K-Means on top of it. Which recreate look-much-better graph.","f61ab7fe":"# \ud83d\udcca EDA: Categorical\n* Similar we look into the relationship of Categorical Variables and Continuous Target Variables by graphing the median values of target variables on different categories\n* The more meaningful categorical variables would split the data into distinguished groups with more differences in Target variables (here summarised by Median or Mean)","45d1883c":"We believe that there are some interesting info in the `email`, `address`, `avatar`.\nFor example:\n* Customer with company email might be more legit => Extract the email domain info\n* Different states might have different consumption behaviors\n* Avatar colors implies the genders and preferences\n\n> Note: Use [Regex](https:\/\/www.w3schools.com\/python\/python_regex.asp) for this string manipulation ","baf02332":"# \ud83e\uddec LightGBM Classifier","916d84bd":"# \u2714\ufe0f Target for Regressions & Classifications","c62bdb0b":"## \u2714\ufe0f Linear Regression with PCA","0e512709":"# \ud83e\uddec XGBoost Classifier","a2b0421b":"## \u2714\ufe0f Visualize Clusters","8820cfe0":"# \ud83d\udcc8 Regressors Comparison","496f3f7b":"# \u2714\ufe0f Data Processing","61a90e89":"## \u2714\ufe0f Prediction","4b680be8":"# \ud83d\udcbe Data\nWe use [Ecommerce Customer](https:\/\/www.kaggle.com\/srolka\/ecommerce-customers) dataset, with `Yearly Amount Spent` is a continues variable, as a target variable (y) for our modelings. The data contains the following columns:\n\n* `Email`, `Address`, `Avatar`: customers metadata\n* `Avg. Session Length`, `Time on App`, `Time on Website`, `Length of Membership`: User behaviors on the Ecommerce platform","1d8c2644":"# \ud83e\uddec Logit Regression","7e9fbc4b":"# \ud83e\uddec Linear Regressions\n> * **Linear Asusumptions**: Relationship between your input and output is linear (`check by bins plot, transform: polynomial, absolute, log on exponential relationship`)\n> * **Remove Noise**: Assume that input and output not noisy (`check by distribution plot, transform: Remove outliers on output\/input if possible`)\n> * **Remove Collinearity**: Over-fit the data when you add highly correlated input variables (`check by pairwise correlation`)\n> * **Gaussian Distribution**: More reliable when input\/output is gaussian distribution (`check by distribution, transform: Boxcox or log`)\n> * **Rescale inputs**: rescale input variables using standardization\/normalisation (`improve the above assumptions`)\n\n![image.png](attachment:0b62e037-5fc4-4e18-9a92-fddb31c72c97.png)\n\n> **Reference:** [Linear Regressions House Price](https:\/\/www.kaggle.com\/faressayah\/linear-regression-house-price-prediction#%F0%9F%92%BE-Data)","126c4c6f":"# \ud83d\udce4 Import Libraries","770a0572":"# \ud83e\uddec GBM Classifier\n> **Reference**\n> * [Gradient Boosting with Scikit-Learn](https:\/\/machinelearningmastery.com\/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost\/)\n> * [scikit-learn.GradientBoostingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html)\n\n**Parameters:**\n* `loss{\u2018deviance\u2019, \u2018exponential\u2019}, default=\u2019deviance\u2019`: Loss function to be optimized (Deviance = logit)\n* `learning_rate`: shrunks the contribution of each tree (trade off between n_estimators and learning_date)\n* `n_estimators`: Number of boosting stage (GB fairly robust to over-fitting => large number can boost the performance)\n* `max_depth`, `min_impurity_decrease`","12fa615b":"# \u2714\ufe0f Train-Test Data","3b2df505":"# \ud83e\uddec PCA\n\n**Principal Components Analysis** is the technique to reduce the dimensions (cols of the data).\nFrom the orignal data with 2+ dimensions (features), we can reduce it to 2-dim which allow us:\n* Do visualize easily\n* As a step of processing data","bdbfcc9e":"# \ud83e\uddec Decision Tree Classifier\n> Source: [scikit-learn.DecisionTreeClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)\n\n**Parameters:**\n* `criterion{\u201cgini\u201d, \u201centropy\u201d}, default=\u201dgini\u201d`: Function to measure the quality of split\n* `max_depth`: how many layers the tree has (to avoid too complex tree => overfitting)\n* `min_samples_split`: minimum numbers of samples for each split (to avoid splitting to too small groups => overfitting)\n* `min_samples_leaft`: minimum numbers of samples for each leaf node (to avoid splitting to too small groups => overfitting)\n* `min_impurity_decrease`: early stops the tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.","2bbb590d":"## \u2714\ufe0f Diagnostics","f9f15773":"# \ud83e\uddec RF Classifier","d30fbcdc":"# \u2714\ufe0f Model Explainer\n* For Random Forest, GBM, XGBoost\n* Rank the features by the importance to the model\n* SHAP - SHApley Additive exPlanations ([HERE](https:\/\/towardsdatascience.com\/explain-any-models-with-the-shap-values-use-the-kernelexplainer-79de9464897a))","49de1cf3":"## \u2714\ufe0f Silhouette Coefficient\nA measure of cluster cohesion and separation. It quantifies how well the data poitns fit to the centroid:\n* How close the data points to others belong to same cluster\n* How far the data points from others in other clusters\n\n=> Select the `n_clusters` have the highest silhouette score","f3261ac9":"See how useful this regex manipulation could help us: \n* Simplify the categories of avatar (remember if you have to many groups, it's clumsy for one-hot encoding to process the data for modeling)\n* For the distribution of categorical, we might change the way we bucketize the variables => In the way, that we simplify and reduce the buckets","05a00728":"## \u2714\ufe0f Data Processing","a4587186":"## \u2714\ufe0f Elbow Method\nPlot SSE curve as a function of cluster, SSE will decrease with the increase in k (more centroids are addedm the distance from each points to the centroid will decrease) => The **Sweet Spot** is elbow points, where the improvement in SSE is diminished with the additional centroids","d0f6f978":"# \ud83e\uddec Random Forest Regressor\n**Random Forest** is an esemble of Decision Trees.\n> **Reference**: [How to visualize a single Decision Tree from the RF in Python](https:\/\/mljar.com\/blog\/visualize-tree-from-random-forest\/)","9f9c0123":"# \ud83e\uddec Polynomial Regression\nWe can use linear model to trained nonlinear data. Linear model has the advantages of fast training, and fit a wide range of data.\nLinear Regressions could be fitted on polynomial features to capture the non-linear relationship.\n> **Reference:** \n> [ML Polynomial Regresison with Python](https:\/\/towardsdatascience.com\/machine-learning-polynomial-regression-with-python-5328e4e8a386)\n\n![image.png](attachment:16d97789-84f9-42df-9196-f49d32f9342d.png)","ba97fbe9":"# \ud83d\udcc8 Classifiers Comparison","ad1eb542":"# \ud83d\udea7 Data Manipulation: Regex & Categorical","5d2a5b1e":"# \ud83e\uddec Support Vector Regressors\n> **Reference**: [SVR - The most flexible yet robost prediction](https:\/\/towardsdatascience.com\/support-vector-regression-svr-one-of-the-most-flexible-yet-robust-prediction-algorithms-4d25fbdaca60)\n\n![image.png](attachment:d8c00cb3-c62b-4b09-870b-477a4f4eb1fd.png)\n\nSource: [www.saedsayad.com\/support_vector_machine_reg](https:\/\/www.saedsayad.com\/support_vector_machine_reg.htm)","18eb3dfe":"## \u2714\ufe0f Fitting","1f8b149d":"## \u2714\ufe0f Over-fitting\nBelow we also try the Linear Regression, but remove all categorical variables => Set-up: `Linear Regression - Numeric`.\nLook into the report of evaluation:\n* **Linear Regression (All Variables)**: RMSE in train is low but in test is higher (increase 28%) => Not stable, due to the overfiting (?) the performance in test is not great\n* **Linear Regression (Numeric Variables)**: RMSE in train is higher than All variables but in test is RMSE is better (increase 3.8%) => More stable\n\n![image.png](attachment:094718c2-b9c9-4a11-ba91-432e91fca294.png)","5b0917a9":"# \ud83e\uddec Naive Bayes\n> **Source**: [Naive Bayers Classifier from Scratch Python](https:\/\/machinelearningmastery.com\/naive-bayes-classifier-scratch-python\/)\n\nNaive Bayes methods are base on Bayes' theorem with the \"naive\" assumption of conditional independece between every pair of featrues given the class variables. \nDespite the over-simplified assumptions, Naive Bayers worked quite well in many real-world situations (famous for document classification and spam filtering). \n* Require a small amout of training data\n* Fast training comparing to sophisticated methods","69b69d09":"# \ud83e\uddec XGBoost Regressor\n\n**XGBoost** goes through the modeling cycle for iterations, repeately bulding new models then combine them into an ensemble model.\n* Start the cycle to calculate errors for each observations in dataset\n* Build the next model to predict errors\n* Add predictions from the error-predicting model to the \"ensemble of models\"\n\n**Model Tuning**\nHyperparameters that dramatically affecy the accuracy and training speed:\n* `n_estimators`: How many times to go through the modeling cycle. Too low `n_estimators`, model is underfitting (low accuracy in both training and testing). Typical range: 100-1000. Depends also on `learning_rate`.\n* `early_stopping_rounds`: model stops iterating when validation score stops improving, even if we don't set `n_estimators`. If the value = 5, means we stop after 5 straight rounds that the validation score not improve.\n* **==> Strategy**: Set high `n_estimators` and use `early_stopping_rounds` to find the optimal time to stop iterating (cautious with local optimum)\n* `learning_rate`: instead of only adding up the predictions from new models, we multiply prediction by a small number before add up. This means the new tree\/model add to the ensemble helps us less => Reduce the propensity of overfit (penalty the complexity). So that, we can have high `n_estimators` and control by this. But, it will take longer to train (more iterations)\n* `n_jobs`: for large datasets, parallelism to build the model faster (n_jobs is the numbers of cores on the machines) => To speed up modeling time.\n> **Reference**: [XGBoost Kaggle Tutorial ](https:\/\/www.kaggle.com\/dansbecker\/xgboost)","483adee4":"# \ud83e\uddec Clustering K-Means\n\nThe k-means clustering method is an unsupervised ML techniques to identify clusters of data objects in a dataset. There are many different clustering techniques, but k-means is the oldest and most approachable. \n\n**Algorithms**\n1. Specify the number of cluster (k) to assign\n2. Randomly initialize k centroids\n3. Repeat\n4. Expectation: Assign each data points to the nearest centroid\n5. Maximization: Compute the new centroid (mean) of each cluster\n6. Until the centroid position do not change\n\n**Evaluation**\n* Sum of Squared Error (SSE) => Centroid Coverge"}}