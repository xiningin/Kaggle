{"cell_type":{"e278f867":"code","e27c30a5":"markdown"},"source":{"e278f867":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ntrain_x = pd.read_csv('..\/input\/X_train.csv')\ntrain_y = pd.read_csv('..\/input\/y_train.csv')\n\nnum_test=500\n\ndef prepare_data(t):\n    def f(d):\n        d=d.sort_values(by=['measurement_number'])\n        return pd.DataFrame({\n         'lx':[ d['linear_acceleration_X'].values ],\n         'ly':[ d['linear_acceleration_Y'].values ],\n         'lz':[ d['linear_acceleration_Z'].values ],\n         'ax':[ d['angular_velocity_X'].values ],\n         'ay':[ d['angular_velocity_Y'].values ],\n         'az':[ d['angular_velocity_Z'].values ],\n         'ox':[ d['orientation_X'].values ],\n         'oy':[ d['orientation_Y'].values ],\n         'oz':[ d['orientation_Z'].values ],\n         'ow':[ d['orientation_W'].values ],\n        })\n\n    t= t.groupby('series_id').apply(f)\n    return t\n\n\ndef split_shuffle_groups(t):\n    t= t.copy()\n\n    # select randomly some groups (should be weighted by # of samples)\n\n    aggcol='surface' # arbitrary; just to get size\n    gstat= t.groupby('group_id')[aggcol].agg(np.size)\n    gstat= gstat.reset_index()\n\n    import random\n    random.shuffle\n\n    groups = list(zip(gstat['group_id'].values, gstat[aggcol].values))\n    random.shuffle(groups)\n    \n    test_groups= set()\n    c=0\n    for gid,len in groups:\n        if c>=num_test: break\n        c+=len\n        test_groups.add(gid)\n    print(\"test groups:\", test_groups)\n\n    ctest = [ i for i,gid in enumerate(t['group_id']) if (gid in test_groups) ]\n    ctrain = [ i for i,gid in enumerate(t['group_id']) if not (gid in test_groups) ]\n\n    random.shuffle(ctrain)\n    random.shuffle(ctest)\n\n    return t.iloc[ctrain], t.iloc[ctest]\n\n\ntrain= prepare_data(train_x)\n\n# merge\ntrain=pd.merge(train,train_y[['series_id','group_id','surface']],on='series_id')\n\ntrain_part_df, validation_part_df= split_shuffle_groups(train)\n\nprint(\"training part of training data set:\", train_part_df.describe())\nprint(\"validation part of training data set:\",validation_part_df.describe())\n","e27c30a5":"This is a simple kernel, that splits the training data set into a real training set and a validation data set, so that no groups are split between them. This is useful to get realistic validation measurements, because the data within a group is correlated."}}