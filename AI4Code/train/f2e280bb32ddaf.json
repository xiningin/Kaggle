{"cell_type":{"92674d36":"code","9d8c0620":"code","601e6f29":"code","63d5b74e":"code","145b9e71":"code","fe709ac9":"code","143c2b21":"code","ad3fa5ae":"code","d84ee38d":"code","9712524a":"code","a7929f8d":"code","4f3cf22f":"code","78c49f2c":"code","f8e687bb":"code","b9f462cb":"code","c1cc5c80":"code","813d3342":"code","ba513f15":"code","0ea16cb6":"code","b7b3ab44":"code","6edd479b":"code","87b82db7":"code","b8c71070":"code","75ef66c0":"code","5410485d":"code","5453cdb1":"code","3c328c71":"code","bd073cc6":"code","340c8309":"code","05d9d7dc":"code","2673c056":"code","693af9bc":"code","fc0297f8":"code","f47a269a":"code","39b8062b":"code","08998976":"code","cbf4611f":"code","08fcab00":"code","2a34ad1c":"code","4ce62c49":"code","b709d4c1":"code","425f9d59":"code","ecbd35ee":"code","749c01cd":"code","2a68d500":"code","05d1aab8":"code","14bd1595":"markdown","d39c0a26":"markdown","ead3507d":"markdown","c4bf971b":"markdown","64138b61":"markdown","e0f62d52":"markdown","761b87cc":"markdown","b1011343":"markdown","042c07a8":"markdown","8eac21b0":"markdown","eedddfe2":"markdown","e9620fc4":"markdown","84c190f8":"markdown","68a08b42":"markdown","86a0216c":"markdown","129a061b":"markdown","4c0be87f":"markdown","1b28e97a":"markdown","ef297d7a":"markdown","31bb90ad":"markdown"},"source":{"92674d36":"# Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics","9d8c0620":"# Importing the dataset\ndata = pd.read_csv('..\/input\/Iris.csv')","601e6f29":"# Printing the 1st 5 columns\ndata.head()","63d5b74e":"# Printing the dimenions of data\ndata.shape","145b9e71":"# Viewing the column heading\ndata.columns","fe709ac9":"# Inspecting the target variable\ndata.Species.value_counts()","143c2b21":"data.dtypes","ad3fa5ae":"# Identifying the unique number of values in the dataset\ndata.nunique()","d84ee38d":"# Checking if any NULL values or any inconsistancies are present in the dataset\ndata.isnull().sum()","9712524a":"# See rows with missing values\ndata[data.isnull().any(axis=1)]","a7929f8d":"# Viewing the data statistics\ndata.describe()","4f3cf22f":"data.info()","78c49f2c":"# Dropping the Id column as it is unnecessary for our model\ndata.drop('Id',axis=1,inplace=True)","f8e687bb":"fig = data[data.Species=='Iris-setosa'].plot(kind='scatter', x='SepalLengthCm', y='SepalWidthCm', color='blue', label='Setosa')\ndata[data.Species=='Iris-versicolor'].plot(kind='scatter', x='SepalLengthCm', y='SepalWidthCm', color='orange', label='Versicolor', ax=fig)\ndata[data.Species=='Iris-virginica'].plot(kind='scatter', x='SepalLengthCm', y='SepalWidthCm', color='green', label='Virginica', ax=fig)\nfig.set_xlabel(\"Sepal Length\")\nfig.set_ylabel(\"Sepal Width\")\nfig.set_title(\"Sepal Length vs Sepal Width\")\nfig=plt.gcf()\n# fig.set_size_inches(20,10)\nplt.show()","b9f462cb":"fig = data[data.Species=='Iris-setosa'].plot(kind='scatter', x='PetalLengthCm', y='PetalWidthCm', color='blue', label='Setosa')\ndata[data.Species=='Iris-versicolor'].plot(kind='scatter', x='PetalLengthCm', y='PetalWidthCm', color='orange', label='Versicolor', ax=fig)\ndata[data.Species=='Iris-virginica'].plot(kind='scatter', x='PetalLengthCm', y='PetalWidthCm', color='green', label='Virginica', ax=fig)\nfig.set_xlabel(\"Petal Length\")\nfig.set_ylabel(\"Petal Width\")\nfig.set_title(\"Petal Length vs Petal Width\")\nfig=plt.gcf()\n# fig.set_size_inches(20,10)\nplt.show()","c1cc5c80":"data.hist(edgecolor='black', linewidth=2)\nfig=plt.gcf()\nfig.set_size_inches(12,10)\nplt.show()","813d3342":"sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=data)","ba513f15":"sns.boxplot(x=\"Species\", y=\"PetalWidthCm\", data=data)","0ea16cb6":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='Species',y='PetalLengthCm',data=data)\nplt.subplot(2,2,2)\nsns.violinplot(x='Species',y='PetalWidthCm',data=data)\nplt.subplot(2,2,3)\nsns.violinplot(x='Species',y='SepalLengthCm',data=data)\nplt.subplot(2,2,4)\nsns.violinplot(x='Species',y='SepalWidthCm',data=data)","b7b3ab44":"sns.stripplot(x=\"Species\", y=\"PetalLengthCm\", data=data, jitter=True, edgecolor=\"gray\")","6edd479b":"# Distribution density plot KDE (kernel density estimate)\nsns.FacetGrid(data, hue=\"Species\", size=6).map(sns.kdeplot, \"PetalLengthCm\").add_legend()","87b82db7":"# Plotting bivariate relations between each pair of features (4 features x4 so 16 graphs) with hue = \"Species\"\nsns.pairplot(data, hue=\"Species\", size=4)","b8c71070":"# Finding out the correlation between the features\ncorr = data.corr()\ncorr.shape","75ef66c0":"# Plotting the heatmap of correlation between features\nplt.figure()\nsns.heatmap(corr, cbar=True, square= True, fmt='.2f', annot=True, annot_kws={'size':15}, cmap = 'Greens')","5410485d":"# Spliting target variable and independent variables\nX = data.drop(['Species'], axis = 1)\ny = data['Species']","5453cdb1":"# Splitting the data into training set and testset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\nprint(\"Size of training set:\", X_train.shape)\nprint(\"Size of training set:\", X_test.shape)","3c328c71":"# Logistic Regression\n\n# Import library for LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Create a Logistic regression classifier\nlogreg = LogisticRegression()\n\n# Train the model using the training sets \nlogreg.fit(X_train, y_train)","bd073cc6":"# Prediction on test data\ny_pred = logreg.predict(X_test)","340c8309":"# Calculating the accuracy\nacc_logreg = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Logistic Regression model : ', acc_logreg )","05d9d7dc":"# Gaussian Naive Bayes\n\n# Import library of Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n# Create a Gaussian Classifier\nmodel = GaussianNB()\n\n# Train the model using the training sets \nmodel.fit(X_train,y_train)","2673c056":"# Prediction on test set\ny_pred = model.predict(X_test)","693af9bc":"# Calculating the accuracy\nacc_nb = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Gaussian Naive Bayes model : ', acc_nb )","fc0297f8":"# Decision Tree Classifier\n\n# Import Decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create a Decision tree classifier model\nclf = DecisionTreeClassifier(criterion = \"gini\" , min_samples_split = 100, min_samples_leaf = 10, max_depth = 50)\n\n# Train the model using the training sets \nclf.fit(X_train, y_train)","f47a269a":"# Prediction on test set\ny_pred = clf.predict(X_test)","39b8062b":"# Calculating the accuracy\nacc_dt = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Decision Tree model : ', acc_dt )","08998976":"# Random Forest Classifier\n\n# Import library of RandomForestClassifier model\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a Random Forest Classifier\nrf = RandomForestClassifier()\n\n# Train the model using the training sets \nrf.fit(X_train,y_train)","cbf4611f":"# Prediction on test data\ny_pred = rf.predict(X_test)","08fcab00":"# Calculating the accuracy\nacc_rf = round( metrics.accuracy_score(y_test, y_pred) * 100 , 2 )\nprint( 'Accuracy of Random Forest model : ', acc_rf )","2a34ad1c":"# SVM Classifier\n\n# Creating scaled set to be used in model to improve the results\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","4ce62c49":"# Import Library of Support Vector Machine model\nfrom sklearn import svm\n\n# Create a Support Vector Classifier\nsvc = svm.SVC()\n\n# Train the model using the training sets \nsvc.fit(X_train,y_train)","b709d4c1":"# Prediction on test data\ny_pred = svc.predict(X_test)","425f9d59":"# Calculating the accuracy\nacc_svm = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of SVM model : ', acc_svm )","ecbd35ee":"# Random Forest Classifier\n\n# Import library of RandomForestClassifier model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create a Random Forest Classifier\nknn = KNeighborsClassifier()\n\n# Train the model using the training sets \nknn.fit(X_train,y_train)","749c01cd":"# Prediction on test data\ny_pred = knn.predict(X_test)","2a68d500":"# Calculating the accuracy\nacc_knn = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of KNN model : ', acc_knn )","05d1aab8":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'Support Vector Machines', \n              'K - Nearest Neighbors'],\n    'Score': [acc_logreg, acc_nb, acc_dt, acc_rf, acc_svm, acc_knn]})\nmodels.sort_values(by='Score', ascending=False)","14bd1595":"## Evaluation and comparision of all the models","d39c0a26":"As we can see that the Petal Features are giving a better cluster division compared to the Sepal features. This is an indication that the Petals can help in better and accurate Predictions over the Sepal.","ead3507d":"#### Once the data is cleaned, we split the data into training set and test set to prepare it for our machine learning model in a suitable proportion.","c4bf971b":"# Iris EDA","64138b61":"### Decision Tree Classifier","e0f62d52":"### K - Nearest Neighbors","761b87cc":"### Read the data from the dataset using the read_csv() function from the pandas library.","b1011343":"### Import all the necessary header files as follows:\n\n**pandas** : An open source library used for data manipulation, cleaning, analysis and visualization. <br>\n**numpy** : A library used to manipulate multi-dimensional data in the form of numpy arrays with useful in-built functions. <br>\n**matplotlib** : A library used for plotting and visualization of data. <br>\n**seaborn** : A library based on matplotlib which is used for plotting of data. <br>\n**sklearn.metrics** : A library used to calculate the accuracy, precision and recall. <br>","042c07a8":"### Inspecting and cleaning the data","8eac21b0":"### Random Forest Classifier","eedddfe2":"## Data Visualization","e9620fc4":"KDE: Kernel Density Estimate. This shows the distribution density more clearly. We use a FacetGrid with hue = 'Species'. .add_legend() adds the legend on the top rights.","84c190f8":"### Support Vector Machine","68a08b42":"## Dataset: [Iris Dataset](https:\/\/www.kaggle.com\/uciml\/iris)","86a0216c":"The violinplot shows density of the length and width in the species. The thinner part denotes that there is less density whereas the fatter part conveys higher density","129a061b":"### Logistic Regression","4c0be87f":"## Hence Naive Bayes classification works perfectly on this dataset.","1b28e97a":"### Please upvote if you found this kernel useful! :) <br>\n### Feedbacks appreciated.","ef297d7a":"The Sepal Width and Length are not correlated The Petal Width and Length are highly correlated.","31bb90ad":"### Gaussian Naive Bayes"}}