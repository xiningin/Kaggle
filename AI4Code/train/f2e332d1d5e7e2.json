{"cell_type":{"1eebe53c":"code","5d7233eb":"code","47789090":"code","2d0406a6":"code","bf4334f8":"code","ca94f73e":"code","eb56b56c":"code","6c55826b":"code","534aecb8":"code","d18fe7e7":"code","c434e17a":"code","b3e1c7d2":"code","38cb89f7":"code","d95407b6":"code","a9e841ef":"code","340ef4e0":"code","6c58c58a":"code","5f796215":"code","1e7d6282":"code","bafc02b4":"code","8d0cd572":"code","2ebc2a7c":"code","7523a3fa":"code","a897276d":"code","40fab674":"code","940d9d62":"code","6ae0be54":"code","858fa1a0":"markdown","9f5d9948":"markdown","7a0f8ab8":"markdown","5f9ba9ba":"markdown","dd36d343":"markdown","979915fa":"markdown","9875ff25":"markdown","5b6be7a8":"markdown","d15f0552":"markdown","1b635d61":"markdown"},"source":{"1eebe53c":"import warnings # tf needs to learn to stfu\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=UserWarning)\nwarnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)","5d7233eb":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (15, 10)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})\nplt.rcParams['image.cmap'] = 'gray' # grayscale looks better\nfrom itertools import cycle\nprop_cycle = plt.rcParams['axes.prop_cycle']\ncolors = prop_cycle.by_key()['color']","47789090":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport os\nfrom skimage.io import imread as imread\nfrom skimage.util import montage\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\nfrom skimage.color import label2rgb\nimage_dir = Path('..') \/ 'input' \/ 'skin-cancer-mnist-ham10000'\nmapping_file = Path('..') \/ 'input' \/ 'skin-images-to-features' \/ 'color_features.json'\nskin_df = pd.read_json(mapping_file)\n#skin_df['image_path'] = skin_df['image_path'].map(lambda x: image_dir \/ 'subset' \/ x) \nprint(skin_df['image_path'].map(lambda x: Path(x).exists()).value_counts())\nskin_df.sample(2)","2d0406a6":"from sklearn.model_selection import train_test_split\nraw_train_df, valid_df = train_test_split(skin_df, \n                 test_size = 0.3, \n                  # hack to make stratification work                  \n                 stratify = skin_df['dx_id'])\nprint(raw_train_df.shape[0], 'training masks')\nprint(valid_df.shape[0], 'validation masks')","bf4334f8":"GAUSSIAN_NOISE = 0.05\n# number of validation images to use\nVALID_IMG_COUNT = 1500\nBASE_MODEL='MobileNet' # ['VGG16', 'RESNET52', 'InceptionV3', 'Xception', 'DenseNet169', 'DenseNet121']\nIMG_SIZE = (224, 224) # [(224, 224), (384, 384), (512, 512), (640, 640)]\nBATCH_SIZE = 64 # [1, 8, 16, 24]\nDROPOUT = 0.5\nDENSE_COUNT = 256\nSAMPLE_PER_GROUP = 2200\nLEARN_RATE = 2e-4\nEPOCHS = 25\nFLATTEN = True\nRGB_FLIP = 1 # should rgb be flipped when rendering images","ca94f73e":"train_df = raw_train_df.\\\n     groupby('dx_name').\\\n     apply(lambda x: x.sample(SAMPLE_PER_GROUP\/\/2, replace=True)).\\\n     reset_index(drop=True)\ntrain_df.shape[0]","eb56b56c":"from keras.preprocessing.image import ImageDataGenerator\nif BASE_MODEL == 'MobileNet':\n    from keras.applications.mobilenet_v2 import MobileNetV2 as PTModel, preprocess_input\nelif BASE_MODEL=='VGG16':\n    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\nelif BASE_MODEL=='RESNET52':\n    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\nelif BASE_MODEL=='InceptionV3':\n    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\nelif BASE_MODEL=='Xception':\n    from keras.applications.xception import Xception as PTModel, preprocess_input\nelif BASE_MODEL=='DenseNet169': \n    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\nelif BASE_MODEL=='DenseNet121':\n    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\nelse:\n    raise ValueError('Unknown model: {}'.format(BASE_MODEL))","6c55826b":"from keras.preprocessing.image import ImageDataGenerator\ndg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 45, \n                  width_shift_range = 0.1, \n                  height_shift_range = 0.1, \n                  shear_range = 0.01,\n                  zoom_range = [0.9, 1.25],  \n                  brightness_range = [0.7, 1.3],\n                   \n                  horizontal_flip = True, \n                  vertical_flip = False,\n                  fill_mode = 'reflect',\n                   data_format = 'channels_last',\n              preprocessing_function = preprocess_input)\n\nvalid_args = dict(fill_mode = 'reflect',\n                   data_format = 'channels_last',\n                  preprocessing_function = preprocess_input)\n\ncore_idg = ImageDataGenerator(**dg_args)\nvalid_idg = ImageDataGenerator(**valid_args)","534aecb8":"def flow_from_dataframe(img_data_gen, raw_df, path_col, y_col, **dflow_args):\n    \"\"\"Keras update makes this much easier\"\"\"\n    in_df = raw_df.copy()\n    in_df[path_col] = in_df[path_col].map(str)\n    in_df[y_col] = in_df[y_col].map(lambda x: np.array(x))\n    df_gen = img_data_gen.flow_from_dataframe(in_df, \n                                              x_col=path_col,\n                                              y_col=y_col,\n                                    class_mode = 'raw',\n                                    **dflow_args)\n    # posthoc correction\n    df_gen._targets = np.stack(df_gen.labels, 0)\n    return df_gen","d18fe7e7":"train_gen = flow_from_dataframe(core_idg, train_df, \n                             path_col = 'image_path',\n                            y_col = 'dx_id', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = BATCH_SIZE)\n\n# used a fixed dataset for evaluating the algorithm\nvalid_x, valid_y = next(flow_from_dataframe(valid_idg, \n                               valid_df, \n                             path_col = 'image_path',\n                            y_col = 'dx_id', \n                            target_size = IMG_SIZE,\n                             color_mode = 'rgb',\n                            batch_size = VALID_IMG_COUNT)) # one big batch\nprint(valid_x.shape, valid_y.shape)","c434e17a":"t_x, t_y = next(train_gen)\nprint('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\nprint('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\nfig, (ax1) = plt.subplots(1, 1, figsize = (10, 10))\nax1.imshow(montage_rgb((t_x-t_x.min())\/(t_x.max()-t_x.min()))[:, :, ::RGB_FLIP])\nax1.set_title('images')","b3e1c7d2":"plt.hist(t_y.T)","38cb89f7":"base_pretrained_model = PTModel(input_shape =  t_x.shape[1:], \n                              include_top = False, weights = 'imagenet')\nbase_pretrained_model.trainable = False","d95407b6":"from keras import models, layers\nfrom keras.optimizers import Adam\nimg_in = layers.Input(t_x.shape[1:], name='Image_RGB_In')\nimg_noise = layers.GaussianNoise(GAUSSIAN_NOISE)(img_in)\npt_features = base_pretrained_model(img_noise)\npt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\nbn_features = layers.BatchNormalization()(pt_features)\nfeature_dropout = layers.SpatialDropout2D(DROPOUT)(bn_features)\nif FLATTEN:\n    flat_layer = layers.Flatten()(bn_features)\n    collapsed_layer = layers.Dropout(DROPOUT)(flat_layer)\nelse:\n    collapsed_layer = layers.GlobalAvgPool2D()(bn_features)\ndr_steps = layers.Dropout(DROPOUT)(layers.Dense(DENSE_COUNT, activation = 'relu')(collapsed_layer))\nout_layer = layers.Dense(train_df['dx_id'].max()+1, activation = 'softmax')(dr_steps)\n\nskin_model = models.Model(inputs = [img_in], outputs = [out_layer], name = 'full_model')\n\nskin_model.compile(optimizer = Adam(lr=LEARN_RATE), \n                   loss = 'sparse_categorical_crossentropy',\n                   metrics = ['sparse_categorical_accuracy'])\n\nskin_model.summary()","a9e841ef":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('skin_cancer_detector')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=15) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","340ef4e0":"from IPython.display import clear_output\ntrain_gen.batch_size = BATCH_SIZE\nfit_results = skin_model.fit_generator(train_gen, \n                            steps_per_epoch = train_gen.samples\/\/BATCH_SIZE,\n                      validation_data = (valid_x, valid_y), \n                      epochs = EPOCHS, \n                      callbacks = callbacks_list,\n                      workers = 3)\nclear_output()","6c58c58a":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.plot(fit_results.history['loss'], label='Training')\nax1.plot(fit_results.history['val_loss'], label='Validation')\nax1.legend()\nax1.set_title('Loss')\nax2.plot(fit_results.history['sparse_categorical_accuracy'], label='Training')\nax2.plot(fit_results.history['val_sparse_categorical_accuracy'], label='Validation')\nax2.legend()\nax2.set_title('Binary Accuracy')\nax2.set_ylim(0, 1)","5f796215":"skin_model.load_weights(weight_path)\nskin_model.save('full_skin_cancer_model.h5')","1e7d6282":"for k, v in zip(skin_model.metrics_names, \n        skin_model.evaluate(valid_x, valid_y)):\n    if k!='loss':\n        print('{:40s}:\\t{:2.1f}%'.format(k, 100*v))","bafc02b4":"dx_lookup_dict = train_df[['dx_name', 'dx_id']].drop_duplicates().set_index('dx_id').to_dict()['dx_name']","8d0cd572":"t_x, t_y = next(train_gen)\nt_yp = skin_model.predict(t_x)\nfig, (m_axs) = plt.subplots(4, 4, figsize = (20, 20))\nfor i, c_ax in enumerate(m_axs.flatten()):\n    c_ax.imshow(((t_x[i]-t_x.min())\/(t_x.max()-t_x.min()))[:, ::RGB_FLIP])\n    c_title = '{}\\nPred: {:2.1f}%'.format(dx_lookup_dict[t_y[i]], 100*t_yp[i, t_y[i]])\n    c_ax.set_title(c_title)\n    c_ax.axis('off')","2ebc2a7c":"t_x, t_y = valid_x, valid_y\nt_yp = skin_model.predict(t_x, batch_size=8, verbose=True)\nfig, (m_axs) = plt.subplots(4, 4, figsize = (20, 20))\nfor i, c_ax in enumerate(m_axs.flatten()):\n    c_ax.imshow(((t_x[i]-t_x.min())\/(t_x.max()-t_x.min()))[:, ::RGB_FLIP])\n    c_title = '{}\\nPred: {:2.1f}%'.format(dx_lookup_dict[t_y[i]], 100*t_yp[i, t_y[i]])\n    c_ax.set_title(c_title)\n    c_ax.axis('off')","7523a3fa":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\nskin_conditions = [dx_lookup_dict[k][:20] for k in range(len(dx_lookup_dict))]\nt_y_ohe = ohe.fit_transform(t_y.reshape(-1, 1))\npred_df = pd.concat([\n    pd.DataFrame(t_yp, columns=skin_conditions).assign(source='Prediction').assign(id=range(t_yp.shape[0])),\n    pd.DataFrame(t_y_ohe, columns=skin_conditions).assign(source='Ground-truth').assign(id=range(t_yp.shape[0]))\n])\nflat_pred_df = pd.melt(pred_df, id_vars=['source', 'id']).pivot_table(index=['id', 'variable'], columns='source', values='value').reset_index()\nflat_pred_df['Ground-truth'] = flat_pred_df['Ground-truth'].map(lambda x: 'Positive' if x>0.5 else 'Negative')\nsns.catplot(data=flat_pred_df, x='Ground-truth', y='Prediction', col='variable', kind='swarm',  col_wrap=4)","a897276d":"fig, ax1 = plt.subplots(1, 1, figsize=(12, 5))\nsns.swarmplot(data=flat_pred_df, hue='Ground-truth', y='Prediction', x='variable', size=2.0, ax=ax1)","40fab674":"fig, ax1 = plt.subplots(1, 1, figsize=(12, 5))\nsns.boxplot(data=flat_pred_df, hue='Ground-truth', y='Prediction', x='variable', ax=ax1)","940d9d62":"from sklearn.metrics import roc_curve, roc_auc_score\nfig, ax1 = plt.subplots(1, 1, figsize=(10, 10))\nfor i, c_all in enumerate(skin_conditions):\n    tpr, fpr, thresh = roc_curve(y_true=t_y_ohe[:, i], y_score=t_yp[:, i])\n    auc_roc = roc_auc_score(y_true=t_y_ohe[:, i], y_score=t_yp[:, i])\n    ax1.plot(tpr, fpr, '.-', label='{} (AUC:{:2.1%})'.format(c_all, auc_roc), lw=2)\nax1.legend()","6ae0be54":"skin_model.get_input_at(0), skin_model.get_output_at(0)","858fa1a0":"# Goal\nThe goal is to make a simple model that can go from an image (taken with a smartphone) to a prediction of how likely different skin-conditions are based on a picture of your skin. It is not designed for medical use and serves as a fun toy to see how image processing works (and fails) in real conditions.\n## Setup\nWe basically take a pretrained model (MobileNet in this case) and add a few layers that we train ourselves in order to determine if the food contains any of the 7 different conditions. We try to create a balanced training group and a realistic validation group to know if the model is learning anything useful","9f5d9948":"### Show the labels\nHere we show the labels for the batch items and can see how frequent each one is","7a0f8ab8":"# Build a Model","5f9ba9ba":"Split up the groups so we can validate our model on something besides the direct training data","dd36d343":"## Detailed Performance by group","979915fa":"# Prepare for Model","9875ff25":"# Export the Model\nWe can export the model to tensorflowjs to build a web-app that can automatically predict what allergens are in a given image ","5b6be7a8":"## Class-level ROC curves","d15f0552":"# Validation Data Results\nHere we show the results on validation data","1b635d61":"# Model Parameters"}}