{"cell_type":{"3f744863":"code","d2d18520":"code","7de7cc30":"code","50553aa8":"code","a89850ed":"code","e84d9288":"code","7af2b42f":"code","b388cc1c":"code","4e0a8793":"code","0a67b4ea":"code","4ab7ac1d":"code","70c4d41f":"code","065fc7d3":"code","6bebb85b":"code","e25ad53e":"code","c2dcc7aa":"code","722658a7":"code","23e775f6":"code","446704b6":"code","9cfa1f75":"code","16e9d4e8":"code","03129993":"code","a57c6e6b":"code","204d68f1":"code","08e47250":"code","bbeb0caa":"code","52f39d81":"code","128cb728":"code","e06346bb":"code","b24980a6":"code","2cb580b3":"code","63701261":"code","d693d291":"code","bd24b965":"code","0d5cc09a":"code","61579938":"code","3b75dfdd":"code","a8f06887":"code","19d0aca5":"code","85f5853e":"code","ad761c7b":"code","3c86a8dc":"code","e2cca295":"code","0cda3a14":"code","cf59f8f5":"code","27f82f16":"code","5d805078":"code","f54ed2ab":"code","be6e5fe5":"code","4401db66":"code","66579acf":"code","a9ec6f6f":"code","c319786d":"code","21c9bd95":"code","50bd107f":"code","881e8c0f":"code","44074fe0":"code","1772ca5d":"code","b1adfbb0":"code","89ae3fdb":"code","55befe55":"code","927a8013":"code","82272d41":"code","e3cd2db9":"code","33d82968":"code","c3ec9094":"code","f183a6f7":"code","6ce88318":"code","7277eccd":"code","a400439f":"code","e8f01348":"code","56fb0afb":"code","dda3584a":"code","48a02417":"markdown","780b37c9":"markdown","4c35a280":"markdown","537dfaa0":"markdown","5f5f15d5":"markdown","2d488948":"markdown","e968317c":"markdown","f0330c5c":"markdown","c6312c17":"markdown","63c0bbba":"markdown","081a0b62":"markdown","4e4c39a7":"markdown","0c2814d2":"markdown","b8aaba21":"markdown","797d3fd2":"markdown","61eb837d":"markdown","b0a023ac":"markdown","44c0ee2e":"markdown","293a796f":"markdown","1b9010f2":"markdown","0db57c87":"markdown","bfd37688":"markdown","27aea7a2":"markdown","b577fd35":"markdown","ac0e3d23":"markdown","76d4bcef":"markdown","3e58440a":"markdown","348f4be5":"markdown","1fff3089":"markdown","a2c43dec":"markdown","775ac0d1":"markdown","c3c02943":"markdown","3b5070ec":"markdown","7b7ad0a2":"markdown"},"source":{"3f744863":"import numpy as np\nimport pandas as pd","d2d18520":"base_url = '\/kaggle\/input\/web-traffic-time-series-forecasting\/'\n\nkey_1 = pd.read_csv(base_url+'key_1.csv')\ntrain_1 = pd.read_csv(base_url+'train_1.csv')\nsample_submission_1 = pd.read_csv(base_url+'sample_submission_1.csv')","7de7cc30":"print(train_1.shape, key_1.shape, sample_submission_1.shape)","50553aa8":"train_1.info()","a89850ed":"train_1.head()","e84d9288":"# Creating a list of wikipedia main sites \nsites = [\"wikipedia.org\", \"commons.wikimedia.org\", \"www.mediawiki.org\"]\n\n# Function to create a new column having the site part of the article page\ndef filter_by_site(page):\n    for site in sites:\n        if site in page:\n            return site\n\n# Creating a new column having the site part of the article page\ntrain_1['Site'] = train_1.Page.apply(filter_by_site)","7af2b42f":"train_1['Site'].value_counts(dropna=False)","b388cc1c":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nplt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Sites\", fontsize=\"18\")\ntrain_1['Site'].value_counts().plot.bar(rot=0);","4e0a8793":"# Checking which country codes exist in the article pages\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-3:].value_counts().index.to_list()","0a67b4ea":"# Creating a list of country codes\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-2:].value_counts().index.to_list()[0:7]","4ab7ac1d":"# Checking which agents + access exist in the article pages and creating a list with them\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,1].str[1:].value_counts().index.to_list()","70c4d41f":"# Creating the list of country codes and agents\ncountries = train_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-2:].value_counts().index.to_list()[0:7]\nagents = train_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,1].str[1:].value_counts().index.to_list()\n\n# Function to create a new column having the country code part of the article page\ndef filter_by_country(page):\n    for country in countries:\n        if \"_\"+country+\".\" in page:\n            return country\n\n# Creating a new column having the country code part of the article page\ntrain_1['Country'] = train_1.Page.apply(filter_by_country)\n\n# Function to create a new column having the agent + access part of the article page\ndef filter_by_agent(page):\n    for agent in agents:\n        if agent in page:\n            return agent\n\n# Creating a new column having the agent part of the article page\ntrain_1['Agent'] = train_1.Page.apply(filter_by_agent)","065fc7d3":"# Understanding what are the NaN values for the Country column\n# It seems that the URL page does not contain the country code for those cases\n\npd.DataFrame(train_1.Page[train_1['Country'].isna() == True]).sample(10)","6bebb85b":"plt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Country\", fontsize=\"18\")\ntrain_1['Country'].value_counts(dropna=False).plot.bar(rot=0);","e25ad53e":"train_1['Agent'].value_counts(dropna=False)","c2dcc7aa":"plt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Agents\/Access\", fontsize=\"18\")\ntrain_1['Agent'].value_counts().plot.bar(rot=0);","722658a7":"# Creating a sample dataset from the Train dataset for analysis\ntrain_1_sample = train_1.drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample","23e775f6":"# Transposing the sample dataset to have Date Time at the index\ntrain_1_sampleT = train_1_sample.drop('Page', axis=1).T\ntrain_1_sampleT.columns = train_1_sample.Page.values\ntrain_1_sampleT.shape","446704b6":"train_1_sampleT.head()","9cfa1f75":"# Plotting the Series from the sample dataset \nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT[v].plot()\n\nplt.tight_layout();","16e9d4e8":"# Plotting the Series from the sample dataset at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT.columns:\n    plt.plot(train_1_sampleT[v])\n    plt.legend(loc='upper center');","03129993":"# Plotting the histograms for the Series from the sample dataset\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    sns.distplot(train_1_sampleT[v])\n\nplt.tight_layout();","a57c6e6b":"# Checking that the number of visits to the Wikipedia Articles have Gaussian Distribution (p-value=0)\nfrom scipy.stats import kstest, ks_2samp\n\npages = list(train_1_sampleT.columns)\n\nprint(\"Kolgomorov-Smirnov - Normality Test\")\nprint()\n\nfor p in pages:\n    print(p,':', kstest(train_1_sampleT[p], 'norm', alternative = 'less'))    ","204d68f1":"# List of the main Wikipedia Article sites\nsites","08e47250":"# Creating sample datasets from the train dataset and filtering them by sites\ntrain_1_sample_site0 = train_1[train_1['Site'] == sites[0]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample_site1 = train_1[train_1['Site'] == sites[1]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample_site2 = train_1[train_1['Site'] == sites[2]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\n\n# Transposing them to have the Date Time as index\ntrain_1_sampleT_site0 = train_1_sample_site0.drop('Page', axis=1).T\ntrain_1_sampleT_site0.columns = train_1_sample_site0.Page.values\ntrain_1_sampleT_site1 = train_1_sample_site1.drop('Page', axis=1).T\ntrain_1_sampleT_site1.columns = train_1_sample_site1.Page.values\ntrain_1_sampleT_site2 = train_1_sample_site2.drop('Page', axis=1).T\ntrain_1_sampleT_site2.columns = train_1_sample_site2.Page.values","bbeb0caa":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site0.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site0[v].plot()\n\nplt.tight_layout();","52f39d81":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site0.columns:\n    plt.plot(train_1_sampleT_site0[v])\n    plt.legend(loc='upper center');","128cb728":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site1.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site1[v].plot()\n\nplt.tight_layout();","e06346bb":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site1.columns:\n    plt.plot(train_1_sampleT_site1[v])\n    plt.legend(loc='upper center');","b24980a6":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site2.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site2[v].plot()\n\nplt.tight_layout();","2cb580b3":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site2.columns:\n    plt.plot(train_1_sampleT_site2[v])\n    plt.legend(loc='upper center');","63701261":"train_1_sampleT_site2.columns[4]","d693d291":"# List of the Wikipedia Article country codes\ncountries","bd24b965":"# Creating a sample dataset from the train dataset for countries having \"de\" code\ntrain_1_sample_de = train_1[train_1['Country'] == countries[2]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\n\n# Transposing the sample dataset to have Date Time at the index\ntrain_1_sampleT_de = train_1_sample_de.drop('Page', axis=1).T\ntrain_1_sampleT_de.columns = train_1_sample_de.Page.values","0d5cc09a":"# Plotting the Series from the sample dataset\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_de.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_de[v].plot()\n\nplt.tight_layout();","61579938":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_de.columns:\n    plt.plot(train_1_sampleT_de[v])\n    plt.legend(loc='upper center');","3b75dfdd":"# Picked up one Time Series for the prophet modeling\ntrain_1_sampleT.columns[1]","a8f06887":"# Creating a dataframe for the Time Series from the train_1 samples dataset\ndata = pd.DataFrame(train_1_sampleT.iloc[:,1].copy())\ndata.columns = ['y']\ndata.head()","19d0aca5":"plt.figure(figsize=(15, 7))\nplt.plot(data.y.values, label=\"actual\", linewidth=2.0);","85f5853e":"# Adding the lag of the target variable from 1 step back up to 7\nfor i in range(1, 8):\n    data[\"lag_{}\".format(i)] = data.y.shift(i)","ad761c7b":"data.tail()","3c86a8dc":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import TimeSeriesSplit ","e2cca295":"# for time-series cross-validation set 3 folds\n# ~180 days by fold from total of 550 days\ntscv = TimeSeriesSplit(n_splits=3)","0cda3a14":"def timeseries_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    \n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test = X.iloc[test_index:]\n    y_test = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test","cf59f8f5":"y = data.dropna().y\nX = data.dropna().drop(['y'], axis=1)\n\n# reserve 33% of data for testing\n# so test size would be ~180 days\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.33)","27f82f16":"print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","5d805078":"# Function for the MAPE error\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\n# Function for the SMAPE error\ndef smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred))\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return 200 * np.mean(diff)","f54ed2ab":"def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        Plots modelled vs fact values, prediction intervals and anomalies\n    \n    \"\"\"\n    \n    prediction = model.predict(X_test)\n    \n    plt.figure(figsize=(15, 7))\n    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n    \n    if plot_intervals:\n        cv = cross_val_score(model, X_train, y_train, \n                                    cv=tscv, \n                                    scoring=\"neg_mean_absolute_error\")\n        mae = cv.mean() * (-1)\n        deviation = cv.std()\n        \n        scale = 1.96\n        lower = prediction - (mae + scale * deviation)\n        upper = prediction + (mae + scale * deviation)\n        \n        plt.plot(lower, \"r--\", label=\"upper bond \/ lower bond\", alpha=0.5)\n        plt.plot(upper, \"r--\", alpha=0.5)\n        \n        if plot_anomalies:\n            anomalies = np.array([np.NaN]*len(y_test))\n            anomalies[y_test<lower] = y_test[y_test<lower]\n            anomalies[y_test>upper] = y_test[y_test>upper]\n            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    mape_error = mean_absolute_percentage_error(prediction, y_test)\n    smape_error = smape(prediction, y_test)\n    plt.title(\"MAPE: \"+str(mape_error)+\"\\n\"+\"SMAPE: \"+str(smape_error))\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    plt.grid(True);\n    \ndef plotCoefficients(model):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    \n    coefs = pd.DataFrame(model.coef_, X_train.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    plt.figure(figsize=(15, 7))\n    coefs.coef.plot(kind='bar')\n    plt.grid(True, axis='y')\n    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","be6e5fe5":"from sklearn.linear_model import LinearRegression\n\n# Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)","4401db66":"plotModelResults(lr, plot_intervals=True)\nplotCoefficients(lr)","66579acf":"data.index = pd.to_datetime(data.index)\ndata[\"weekday\"] = data.index.weekday\ndata['is_weekend'] = data.weekday.isin([5,6])*1\ndata.tail(7)","a9ec6f6f":"plt.figure(figsize=(16, 5))\nplt.title(\"Encoded features\")\n#data.weekday.plot()\ndata.is_weekend.plot()\nplt.grid(True);","c319786d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","21c9bd95":"y = data.dropna().y\nX = data.dropna().drop(['y'], axis=1)\n\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.33)","50bd107f":"print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","881e8c0f":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","44074fe0":"print(X_train_scaled.shape, y_train.shape, X_test_scaled.shape, y_test.shape)","1772ca5d":"# Linear Regression using Scaled Data\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)","b1adfbb0":"plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\nplotCoefficients(lr)","89ae3fdb":"plt.figure(figsize=(10, 8))\nsns.heatmap(X_train.corr());","55befe55":"from sklearn.linear_model import LassoCV, RidgeCV\n\nridge = RidgeCV(cv=tscv)\nridge.fit(X_train_scaled, y_train)\n\nplotModelResults(ridge, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, plot_anomalies=True)\nplotCoefficients(ridge)","927a8013":"lasso = LassoCV(cv=tscv)\nlasso.fit(X_train_scaled, y_train)\n\nplotModelResults(lasso, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, plot_anomalies=True)\nplotCoefficients(lasso)","82272d41":"from xgboost import XGBRegressor \n\nxgb = XGBRegressor()\nxgb.fit(X_train_scaled, y_train);","e3cd2db9":"plotModelResults(xgb, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)","33d82968":"# Creating a dataframe for the Time Series from the train_1 samples dataset\ndata = pd.DataFrame(train_1_sampleT.iloc[:,1].copy())\ndata.columns = ['y']\ndata.index = pd.to_datetime(data.index)\nfuture = pd.DataFrame(index=pd.date_range(start='2017-01-01', end='2017-03-01'), columns=data.columns).fillna(0)\ndata_future = data.append(future)","c3ec9094":"for i in range(1, 61):\n    data_future[\"lag_{}\".format(i)] = data_future.y.shift(i)\n\ndata_future[\"weekday\"] = data_future.index.weekday\ndata_future['is_weekend'] = data_future.weekday.isin([5,6])*1\ndata_future.tail(7)","f183a6f7":"data_future.shape","6ce88318":"X_train = data_future.iloc[:550,:].dropna().drop(['y'], axis=1)\ny_train = data_future.iloc[:550,:].dropna().y\n\nX_test = data_future.iloc[550:,:].dropna().drop(['y'], axis=1)\ny_test = data_future.iloc[550:,:].dropna().y\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","7277eccd":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","a400439f":"lr = LinearRegression()\nlr.fit(X_train_scaled, y_train)","e8f01348":"prediction = lr.predict(X_test)","56fb0afb":"plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\nplotCoefficients(lr)","dda3584a":"# train_1_sampleT.columns[1]+\"_\"+\"2017-01-01\"\n# train_1_sampleT.columns[1]+\"_\"+\"2017-01-01\" in list(key_1.Page.values)","48a02417":"### Scaling Data","780b37c9":"I could be using Multivariate Time Series (MTS) instead of the univariate models against all Time Series.  \nFollowing this approach, below are some ideas I could try in the future:\n\n- Vector Auto Regression (VAR)\n  - Johansen\u2019s test for checking the stationarity of any multivariate time series data  \n    (statsmodels.tsa.vector_ar.vecm import coint_johansen)\n  - Fit the model using VAR model from statsmodel library  \n    (from statsmodels.tsa.vector_ar.var_model import VAR)  \n- Random Forest  \n- Recurrent Neural Networs (RNN)  \n\nSources:  \n\n<a href=\"https:\/\/link.medium.com\/miaEiLC0c1\">A Multivariate Time Series Guide to Forecasting and Modeling (with Python codes)<\/a>)  \n<a href=\"https:\/\/towardsdatascience.com\/multivariate-time-series-forecasting-using-random-forest-2372f3ecbad1\">Multivariate Time Series Forecasting Using Random Forest<\/a>)  \n<a href=\"https:\/\/link.medium.com\/XFbTA4O0c1\">Interpreting recurrent neural networks on multivariate time series<\/a>","4c35a280":"Notes:\n\nFor all the sites samples, some series presented missing data (NaNs).\n\nFor one of the WWW.MEDIAWIKI.ORG Series sample, noticed there was no data at all.  \nFor this series, the URL contains the IP address instead of DNS name and it starts with \"User:\"","537dfaa0":"### Exploring a Group of Time Series for a Specific Country - DE","5f5f15d5":"## Modeling with XGboost","2d488948":"### Time Series - Lags","e968317c":"## Modeling with Machine Learning","f0330c5c":"## Regularization - Ridge and Lasso","c6312c17":"Running the Linear Regression Model.","63c0bbba":"**Conclusion:** the results shows p-values equals to zero, hence visits to the wikipedia articles for the extracted sample have normal distributions.","081a0b62":"**Time Series of \"WWW.MEDIAWIKI.ORG\" sites only**","4e4c39a7":"**Time Series of \"COMMONS.WIKIMEDIA.ORG\" sites only**","0c2814d2":"Plotting the results.","b8aaba21":"### Prediction for the next 60 days","797d3fd2":"## Submitting to Kaggle","61eb837d":"## Aditional Feature Engineering","b0a023ac":"**Time Series of \"WIKIPEDIA.ORG\" sites only**","44c0ee2e":"Plotting the results","293a796f":"## Feature Engineering","1b9010f2":"## Multiple Time Series in parallel  \n\nAnother idea could be the use of Python multiprocessing package to forecast multiple Time Series in parallel.  \n\nSource:  \n\n<a href=\"https:\/\/medium.com\/spikelab\/forecasting-multiples-time-series-using-prophet-in-parallel-2515abd1a245\">Forecasting multiple time-series using Prophet in parallel<\/a>","0db57c87":"### Train Test Split","bfd37688":"* First we need to define the evaluation metric functions.\n\n**MAPE - Mean Absolute Percentage Error:**\n\n$$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$$\n\n**SMAPE - Symmetric Mean Absolute Percentage Error:**\n\n$$ SMAPE = \\frac{100\\%}{n} \\sum_{t=1}^{n} \\frac{\\left|F_t - A_t\\right|}{(\\left|A_t\\right|+\\left|F_t\\right|)\/2} $$","27aea7a2":"### Linear Regression","b577fd35":"### Exploring Groups of Time Series for Different Sites     ","ac0e3d23":"Below are the function to plot the Model results and its coeficients.","76d4bcef":"# Web Traffic Time Series Forecasting\n\n**Forecast future traffic to Wikipedia pages**","3e58440a":"## Linear Regression Model plus Additional Features","348f4be5":"**Conclusion:**\n\nRegularization did not change neither helped to improve the results in this case.","1fff3089":"## Multivariate Time Series models","a2c43dec":"**Conclusion:**\n\nThe MAPE and SMAPE errors were a litte bit bigger, increasing from 17.30% and 17.90% to 18.03% and 18.68%.","775ac0d1":"## Collecting DATA","c3c02943":"**Conclusion:**  \n\nThe MAPE and SMAPE errors decreased a litte bit, from 18.18% and 18.81% to 17.30% and 17.90%.  \n\nAlso the \"**is_weekend**\" added feature showed up as useful resource, while the \"**weekday**\" added feature not contributed so much. ","3b5070ec":"## Exploratory Data Analisys (EDA)","7b7ad0a2":"After adding \"weekday\" and \"is_weekend\" features, we have different scales in data values.  \n\nHence, we need to tranform data to the same scale to continue the analysis."}}