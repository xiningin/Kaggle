{"cell_type":{"27c17ea0":"code","d85a64d6":"code","21c29678":"code","1fec574f":"code","1db81e50":"code","6401474c":"code","6677785a":"code","2e78d3bd":"code","36f48450":"code","a87a68f3":"code","ad4e0da2":"code","df8f819c":"code","dd89a9b0":"code","62277682":"code","5c719cb5":"code","b0eb31f9":"code","87c93aeb":"code","f8ec8aeb":"code","e5ff36ec":"code","bad2533c":"code","338d26a7":"code","1607775e":"code","7a35b878":"code","8af4d7ac":"code","0322d3fe":"code","dcb04cf7":"code","9ef9f9a2":"code","a20d40a1":"code","c0cb1257":"markdown","3c440c1c":"markdown","9b313a87":"markdown","3bb4e930":"markdown","457a85c5":"markdown","14c3f29c":"markdown","8b0a2e2f":"markdown","139d2e62":"markdown","d6dcad98":"markdown","26149a8b":"markdown","a0c7c927":"markdown","69d7bed8":"markdown","1d66716f":"markdown","7c7cf39e":"markdown","dda8b250":"markdown","836d304c":"markdown","5844b696":"markdown","17ae65d0":"markdown","98c205b2":"markdown","717f67d0":"markdown","f0991dae":"markdown","244c1fc5":"markdown","c186080d":"markdown","556e52ec":"markdown","7bc4f6b4":"markdown","1f561dfc":"markdown","f872c9eb":"markdown","6c104556":"markdown","9a6e0c7d":"markdown","42081459":"markdown","f051c26e":"markdown"},"source":{"27c17ea0":"# import packages\nimport os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\n# setting up options\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n# import datasets\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","d85a64d6":"train_df.head()","21c29678":"print(f'Number of rows: {train_df.shape[0]};  Number of columns: {train_df.shape[1]}; No of missing values: {sum(train_df.isna().sum())}')","1fec574f":"train_df.describe()","1db81e50":"test_df.head()","6401474c":"print(f'Number of rows: {test_df.shape[0]};  Number of columns: {test_df.shape[1]}; No of missing values: {sum(test_df.isna().sum())}')","6677785a":"submission.head()","2e78d3bd":"missing_train_df = pd.DataFrame(train_df.isna().sum())\nmissing_train_df = missing_train_df.drop(['id', 'claim']).reset_index()\nmissing_train_df.columns = ['feature', 'count']\n\nmissing_train_percent_df = missing_train_df.copy()\nmissing_train_percent_df['count'] = missing_train_df['count']\/train_df.shape[0]\n\nmissing_test_df = pd.DataFrame(test_df.isna().sum())\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'count']\n\nmissing_test_percent_df = missing_test_df.copy()\nmissing_test_percent_df['count'] = missing_test_df['count']\/test_df.shape[0]\n\nfeatures = [feature for feature in train_df.columns if feature not in ['id', 'claim']]\nmissing_train_row = train_df[features].isna().sum(axis=1)\nmissing_train_row = pd.DataFrame(missing_train_row.value_counts()\/train_df.shape[0]).reset_index()\nmissing_train_row.columns = ['no', 'count']\n\nmissing_test_row = test_df[features].isna().sum(axis=1)\nmissing_test_row = pd.DataFrame(missing_test_row.value_counts()\/test_df.shape[0]).reset_index()\nmissing_test_row.columns = ['no', 'count']","36f48450":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(2, 15), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=1.5, hspace=0.05)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514']*120)\n\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0_sns = sns.barplot(ax=ax0, y=missing_train_df['feature'], x=missing_train_df['count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\",fontsize=3, weight='bold')\nax0_sns.set_ylabel(\"features\",fontsize=3, weight='bold')\nax0_sns.tick_params(labelsize=3, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.text(0, -1.8, 'Train Dataset', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(0, -1.105, 'Number of missing value are around 15,000 or 1.6%', fontsize=2.5, ha='left', va='top')\nax0.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f} | {(p.get_width()\/train_df.shape[0]):,.1%}'\n    x = p.get_x() + p.get_width() + 1000\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='left', va='center', fontsize=2, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))\n    \nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ff355d']*120)\n    \nax3 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\"]:\n    ax3.spines[s].set_visible(False)\nax3.set_facecolor(background_color)\nax3_sns = sns.barplot(ax=ax3, y=missing_test_df['feature'], x=missing_test_df['count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax3_sns.set_xlabel(\"missing values\",fontsize=3, weight='bold')\nax3_sns.set_ylabel(\"features\",fontsize=3, weight='bold')\nax3_sns.tick_params(labelsize=3, width=0.5, length=1.5)\nax3_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax3_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax3.text(0, -1.8, 'Test Dataset', fontsize=4, ha='left', va='top', weight='bold')\nax3.text(0, -1.105, 'Number of missing value are around 7,000 - 8,000 or 1.6%', fontsize=2.5, ha='left', va='top')\nax3.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n# data label\nfor p in ax3.patches:\n    value = f'{p.get_width():,.0f} | {(p.get_width()\/test_df.shape[0]):,.1%}'\n    x = p.get_x() + p.get_width() + 500\n    y = p.get_y() + p.get_height() \/ 2 \n    ax3.text(x, y, value, ha='left', va='center', fontsize=2, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))","a87a68f3":"background_color = \"#f6f5f5\"\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(6, 2), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 1):\n    for col in range(0, 2):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nsns.barplot(ax=ax0, x=missing_train_row['no'], y=missing_train_row['count'], saturation=1, zorder=2, color='#ffd514')\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.set_ylabel('')\nax0.set_xlabel('train dataset', fontsize=4, fontweight='bold')\nax0.tick_params(labelsize=4, width=0.5)\nax0.xaxis.offsetText.set_fontsize(4)\nax0.yaxis.offsetText.set_fontsize(4)\n\nax0.text(-0.5, 0.44, 'Missing Values - Row Basis', fontsize=6, ha='left', va='top', weight='bold')\nax0.text(-0.5, 0.415, 'Train and test dataset have quite a same distribution', fontsize=4, ha='left', va='top')\n\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_height():.2f}'\n    x = p.get_x() + p.get_width() - 0.4\n    y = p.get_y() + p.get_height() + 0.01 \n    ax0.text(x, y, value, ha='center', va='center', fontsize=2.5, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))\n\nsns.barplot(ax=ax1, x=missing_test_row['no'], y=missing_test_row['count'], saturation=1, zorder=2, color='#ff355d')\nax1.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1.set_ylabel('')\nax1.set_xlabel('test dataset', fontsize=4, fontweight='bold')\nax1.tick_params(labelsize=4, width=0.5)\nax1.xaxis.offsetText.set_fontsize(4)\nax1.yaxis.offsetText.set_fontsize(4)\n\n# data label\nfor p in ax1.patches:\n    value = f'{p.get_height():.2f}'\n    x = p.get_x() + p.get_width() - 0.4\n    y = p.get_y() + p.get_height() + 0.01 \n    ax1.text(x, y, value, ha='center', va='center', fontsize=2.5, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))\n\nplt.show()","ad4e0da2":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nfeatures = list(train_df.columns[1:26])\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","df8f819c":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nfeatures = list(train_df.columns[26:51])\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","dd89a9b0":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nfeatures = list(train_df.columns[51:76])\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","62277682":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nfeatures = list(train_df.columns[76:101])\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","5c719cb5":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(4, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 4):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nfeatures = list(train_df.columns[101:119])\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nax18.remove()\nax19.remove()\n    \nplt.show()","b0eb31f9":"claim_df = pd.DataFrame(train_df['claim'].value_counts()).reset_index()\nclaim_df.columns = ['claim', 'count']\n\nclaim_percent_df = pd.DataFrame(train_df['claim'].value_counts()\/train_df.shape[0]).reset_index()\nclaim_percent_df.columns = ['claim', 'count']\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 1), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.3, hspace=0.05)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514']*120)\n\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0_sns = sns.barplot(ax=ax0, y=claim_df['claim'], x=claim_df['count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"count\",fontsize=3, weight='bold')\nax0_sns.set_ylabel(\"\",fontsize=3, weight='bold')\nax0_sns.tick_params(labelsize=3, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.text(0, -0.8, 'Claim', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(0, -0.65, 'Both of 0 and 1 has almost the same numbers', fontsize=2.5, ha='left', va='top')\nax0.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 10000\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='left', va='center', fontsize=2, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))\n    \nax1 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1.set_facecolor(background_color)\nax1_sns = sns.barplot(ax=ax1, y=claim_percent_df['claim'], x=claim_percent_df['count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax1_sns.set_xlabel(\"percentage\",fontsize=3, weight='bold')\nax1_sns.set_ylabel(\"\",fontsize=3, weight='bold')\nax1_sns.tick_params(labelsize=3, width=0.5, length=1.5)\nax1_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1.text(0, -0.8, 'Claim in %', fontsize=4, ha='left', va='top', weight='bold')\nax1.text(0, -0.65, 'Both of 0 and 1 distributrion are alomost the same of 50%', fontsize=2.5, ha='left', va='top')\n# data label\nfor p in ax1.patches:\n    value = f'{p.get_width():.2f}'\n    x = p.get_x() + p.get_width() + 0.01\n    y = p.get_y() + p.get_height() \/ 2 \n    ax1.text(x, y, value, ha='left', va='center', fontsize=2, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))","87c93aeb":"features = [feature for feature in train_df.columns if feature not in ['id', 'claim']]\ntrain_df['no_missing'] = train_df[features].isna().sum(axis=1)\ntest_df['no_missing'] = test_df[features].isna().sum(axis=1)\n\nmissing_target = pd.DataFrame(train_df.groupby('no_missing')['claim'].agg('mean')).reset_index()\nmissing_target.columns = ['no', 'mean']\n\nbackground_color = \"#f6f5f5\"\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(6, 2), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0.3, hspace=0.3)\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\nfor s in [\"top\",\"right\"]:\n    ax0.spines[s].set_visible(False)\n\nsns.barplot(ax=ax0, x=missing_target['no'], y=missing_target['mean'], saturation=1, zorder=2, color='#ffd514')\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.set_ylabel('')\nax0.set_xlabel('train dataset', fontsize=4, fontweight='bold')\nax0.tick_params(labelsize=4, width=0.5)\nax0.xaxis.offsetText.set_fontsize(4)\nax0.yaxis.offsetText.set_fontsize(4)\n\nax0.text(-0.5, 0.95, 'Target & missing value', fontsize=6, ha='left', va='top', weight='bold')\nax0.text(-0.5, 0.9, 'Observations that has no missing value has the lowest probability to claim', fontsize=4, ha='left', va='top')\n\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_height():.2f}'\n    x = p.get_x() + p.get_width() - 0.4\n    y = p.get_y() + p.get_height() + 0.02\n    ax0.text(x, y, value, ha='center', va='center', fontsize=2.5, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))\n\nplt.show()","f8ec8aeb":"# import packages\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import boxcox\nfrom xgboost import XGBClassifier \nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n# import datasets\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\n\nfolds = 5\nfeatures = list(train_df.columns[1:119])","e5ff36ec":"train_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['claim']\n    y_valid = X_valid['claim']\n    X_train = X_train.drop('claim', axis=1)\n    X_valid = X_valid.drop('claim', axis=1)\n\n    model = XGBClassifier(random_state=42, verbosity=0, tree_method='gpu_hist')\n\n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))","bad2533c":"train_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['claim']\n    y_valid = X_valid['claim']\n    X_train = X_train.drop('claim', axis=1)\n    X_valid = X_valid.drop('claim', axis=1)\n\n    model = LGBMClassifier(random_state=42)\n\n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))","338d26a7":"train_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['claim']\n    y_valid = X_valid['claim']\n    X_train = X_train.drop('claim', axis=1)\n    X_valid = X_valid.drop('claim', axis=1)\n\n    model = CatBoostClassifier(random_state=42)\n\n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))","1607775e":"train_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['claim']\n    y_valid = X_valid['claim']\n    X_train = X_train.drop('claim', axis=1)\n    X_valid = X_valid.drop('claim', axis=1)\n    \n    X_train = np.log(X_train)\n    X_valid = np.log(X_valid)\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))","7a35b878":"train_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['claim']\n    y_valid = X_valid['claim']\n    X_train = X_train.drop('claim', axis=1)\n    X_valid = X_valid.drop('claim', axis=1)\n    \n    X_train['min'] = X_train.min(axis=1)\n    X_valid['min'] = X_valid.min(axis=1)\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))","8af4d7ac":"train_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['claim']\n    y_valid = X_valid['claim']\n    X_train = X_train.drop('claim', axis=1)\n    X_valid = X_valid.drop('claim', axis=1)\n    \n    X_train['max'] = X_train.max(axis=1)\n    X_valid['max'] = X_valid.max(axis=1)\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))","0322d3fe":"train_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['claim']\n    y_valid = X_valid['claim']\n    X_train = X_train.drop('claim', axis=1)\n    X_valid = X_valid.drop('claim', axis=1)\n    \n    X_train['sum'] = X_train.sum(axis=1)\n    X_valid['sum'] = X_valid.sum(axis=1)\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))","dcb04cf7":"train_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['claim']\n    y_valid = X_valid['claim']\n    X_train = X_train.drop('claim', axis=1)\n    X_valid = X_valid.drop('claim', axis=1)\n    \n    X_train['multiply'] = 1\n    X_valid['multiply'] = 1\n    for feature in features:\n        X_train['multiply'] = X_train[feature] * X_train['multiply']\n        X_valid['multiply'] = X_valid[feature] * X_valid['multiply']\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))","9ef9f9a2":"train_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['claim']\n    y_valid = X_valid['claim']\n    X_train = X_train.drop('claim', axis=1)\n    X_valid = X_valid.drop('claim', axis=1)\n    \n    X_train['sum'] = X_train[features].sum(axis=1)\n    X_valid['sum'] = X_valid[features].sum(axis=1)\n    for feature in features:\n        X_train[feature+'_prorate'] = X_train[feature] \/ X_train['sum']\n        X_valid[feature+'_prorate'] = X_valid[feature] \/ X_valid['sum']\n    X_train = X_train.drop('sum', axis=1)\n    X_valid = X_valid.drop('sum', axis=1)\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))","a20d40a1":"train_df = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['claim'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['claim']\n    y_valid = X_valid['claim']\n    X_train = X_train.drop('claim', axis=1)\n    X_valid = X_valid.drop('claim', axis=1)\n    \n    X_train = np.exp(X_train)\n    X_valid = np.exp(X_valid)\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['claim'], train_oof))","c0cb1257":"<a id=\"6.1.2\"><\/a>\n### 6.1.2 LGBM Classifier","3c440c1c":"<a id=\"4.1.2\"><\/a>\n### 4.1.2 Individual features\nCount how many missing values in each features on `train` and `test` dataset to see if there any similiarity between them.\n\n**Observations:**\n- Every features in `train` and `test` dataset has a missing value of around `1.6%`.\n- `train` dataset has a missing value of around `15,000` for each feature.\n- There are around `7,000 - 8,000` missing values for each feature in `test` dataset.","9b313a87":"[back to top](#table-of-contents)\n<a id=\"6\"><\/a>\n# 6 Model\nEvaluate the performance of base model. Models will be evaluated using five cross validation without any hyperparameters tuning. *(to see the packages used, please expand)*","3bb4e930":"[back to top](#table-of-contents)\n<a id=\"2\"><\/a>\n# 2 Preparations\nPreparing packages and data that will be used in the analysis process. Packages that will be loaded are mainly for data manipulation, data visualization and modeling. There are 2 datasets that are used in the analysis, they are train and test dataset. The main use of train dataset is to train models and use it to predict test dataset. While sample submission file is used to informed participants on the expected submission for the competition. *(to see the details, please expand)*","457a85c5":"<a id=\"6.2.4\"><\/a>\n### 6.2.4 Sum of features\nCreate a new feature `sum` that summing up all features values in a row. It seems adding up the `sum` of all features slightly decrease the model performance from `0.80151796` to `0.80151795`.","14c3f29c":"<a id=\"4.1.3\"><\/a>\n### 4.1.3 Individual rows\nCount how many missing values in each rows on `train` and `test` dataset to see if there any similiarity between them.\n\n**Observations:**\n- The maximum of missing value in an observation is `14` and the lowest is `no missing value`.\n- Interestingly, the missing value distribution (row basis) is quite the same between `train` and `test` dataset.\n- Though there is around 2% of missing value in each features, there are around `38%` of the observations (row basis) that has no missing values.\n- In reverse, there are `62%` observations that has missing value.\n- `1 to 3` missing values in the a observations constitute around `41%` of total observations. ","8b0a2e2f":"### 4.2.2 Features f26 - f50","139d2e62":"[back to top](#table-of-contents)\n<a id=\"5\"><\/a>\n# 5 Target\n\n<a id=\"5.1\"><\/a>\n## 5.1 Distribution\nTarget variable has a value of `0` to `1` which indicate people that not claim and claim the insurance. Let's see how the distribution of the `claim` variable.\n\n**Observations:**\n- The number of people that not claim and claim (`0` and `1`) are almost the same of `480,404` and `477,515`, respectively.\n- In term of percentage both of people that claim and not claim are around 50%.","d6dcad98":"[back to top](#table-of-contents)\n<a id=\"3.3\"><\/a>\n## 3.3 Submission\nThe submission file is expected to have an `id` and `claim` columns.\n\nBelow is the first 5 rows of submission file:","26149a8b":"[back to top](#table-of-contents)\n<a id=\"6.2\"><\/a>\n## 6.2 Base model & feature engineering\nThis section will `blindly` try feature engineering using previous created notebook [TPS Feb 2021 Base Model & Features Engineering](https:\/\/www.kaggle.com\/dwin183287\/tps-feb-2021-base-model-features-engineering), to see if there are any new features that are useful. This section will use `LGBM Classifier` as the base model.\n\n**Observations:**\n- Adding up `multiply` feature increased the model performance which can be seen in `6.2.5 Multiplication of features`.\n- Calculate minimum of all features in a row and put it in a new column, slightly increased the model performance, which can be seen in `6.2.2 Minimum of features`.\n- Others feature engineering attempts decrease the model performance.\n\n<a id=\"6.2.1\"><\/a>\n### 6.2.1 Log\nIt seems converting all the features into a log decrease the OOF AUC substantialy from `0.801` to `0.701`.","a0c7c927":"<a id=\"4.1.4\"><\/a>\n### 4.1.4 Dealing with missing value (reference)\nSome references on how to deal with missing value:\n- [Missing Values](https:\/\/www.kaggle.com\/alexisbcook\/missing-values) by [Alexis Cook](https:\/\/www.kaggle.com\/alexisbcook)\n- [Data Cleaning Challenge: Handling missing values](https:\/\/www.kaggle.com\/rtatman\/data-cleaning-challenge-handling-missing-values) by [Rachael Tatman](https:\/\/www.kaggle.com\/rtatman)\n- [A Guide to Handling Missing values in Python ](https:\/\/www.kaggle.com\/parulpandey\/a-guide-to-handling-missing-values-in-python) by [Parul Pandey](https:\/\/www.kaggle.com\/parulpandey)\n\nSome models that have capability to handle missing value by default are:\n- XGBoost: https:\/\/xgboost.readthedocs.io\/en\/latest\/faq.html\n- LightGBM: https:\/\/lightgbm.readthedocs.io\/en\/latest\/Advanced-Topics.html\n- Catboost: https:\/\/catboost.ai\/docs\/concepts\/algorithm-missing-values-processing.html","69d7bed8":"<a id=\"6.1.3\"><\/a>\n### 6.1.3 Catboost Classifier","1d66716f":"[back to top](#table-of-contents)\n<a id=\"6.1\"><\/a>\n## 6.1 Base model\nModels that will be evaluated are `XGBoost Classifier`, `LGBM Classifier` and `Catboost Classifier`.\n\n**Observations:**\n- All 3 models have quite a same AUC result at around `0.8`. The differences are very small among the models.\n- `Catboost Classifier` has the best result with `0.803`.\n- `XGBoost Classifier` is the worst performing model with `0.799`.\n- The second place is hold by `LGBM Classifier` with `0.801`.","7c7cf39e":"<a id=\"6.2.7\"><\/a>\n### 6.2.7 Exponential of features\nIt seems converting all the features into a exponential decrease the OOF AUC from `0.8015` to `0.8011`.","dda8b250":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n# 3 Dataset Overview\nThe intend of the overview is to get a feel of the data and its structure in train, test and submission file. An overview on train and test datasets will include a quick analysis on missing values and basic statistics, while sample submission will be loaded to see the expected submission.\n\n<a id=\"3.1\"><\/a>\n## 3.1 Train dataset\nAs stated before, train dataset is mainly used to train predictive model as there is an available target variable in this set. This dataset is also used to explore more on the data itself including find a relation between each predictors and the target variable.\n\n**Observations:**\n- `claim` column is the target variable which is only available in the `train` dataset.\n- There are `120` columns: `118` features, `1` target variable `claim` and `1` column of `id`.\n- `train` dataset contain `957,919` observation with `1,820,782` missing values which need to be treated carefully.\n\n\n### 3.1.1 Quick view\nBelow is the first 5 rows of train dataset:","836d304c":"Thank you for reading. If you have any critics or find anything wrong, please let me know. I hope you enjoy it.","5844b696":"<a id=\"6.2.6\"><\/a>\n### 6.2.6 Prorate of features\nCreate new `prorate` of each feature that calculate every feature contribution to the sum of all features. Adding `prorate` for each features doesn't improve the model performance. It is decreasing the model from `0.8015` to `0.8014`.","17ae65d0":"# Table of Contents\n<a id=\"table-of-contents\"><\/a>\n- [1 Introduction](#1)\n- [2 Preparations](#2)\n- [3 Datasets Overview](#3)\n    - [3.1 Train dataset](#3.1)\n    - [3.2 Test dataset](#3.2)\n    - [3.3 Submission](#3.3)\n- [4 Features](#4)\n    - [4.1 Missing values](#4.1)\n       - [4.1.1 Preparation](#4.1.1)\n       - [4.1.2 Individual features](#4.1.2)\n       - [4.1.3 Individual rows](#4.1.3)\n       - [4.1.3 Dealing with missing values (reference)](#4.1.4)\n    - [4.2 Distribution](#4.2)\n- [5 Target](#5)\n    - [5.1 Distribution](#5.1)\n    - [5.2 Target & missing value](#5.2)\n- [6 Model](#6)\n    - [6.1 Base model](#6.1)\n        - [6.1.1 XGBoost Classifier](#6.1.1)\n        - [6.1.2 LGBM Classifier](#6.1.2)\n        - [6.1.3 Catboost Classifier](#6.1.3)\n    - [6.2 Base model & feature engineering](#6.2)\n        - [6.2.1 Log](#6.2.1)\n        - [6.2.2 Minimum of features](#6.2.2)\n        - [6.2.3 Maximum of features](#6.2.3)\n        - [6.2.4 Sum of features](#6.2.4) \n        - [6.2.5 Multiplication of feature](#6.2.5) \n        - [6.2.6 Prorate of features](#6.2.6) \n        - [6.2.7 Exponential of features](#6.2.7) \n    ","98c205b2":"<a id=\"6.2.2\"><\/a>\n### 6.2.2 Minimum of features\nCreate a new feature `min` that calculate the minimum value in a row. There is a very small improvement in the model from `0.801517` to `0.801538`.","717f67d0":"[back to top](#table-of-contents)\n<a id=\"3.2\"><\/a>\n## 3.2 Test dataset\nTest dataset is used to make a prediction based on the model that has previously trained. Exploration in this dataset is also needed to see how the data is structured and especially on it\u2019s similiarity with the train dataset.\n\n**Observations:**\n- There are `119` columns: `118` features and `1` column of `id`.\n- `train` dataset contain `493,474` observation with `936,218` missing values which need to be treated carefully.\n\n### 3.2.1 Quick view\nBelow is the first 5 rows of test dataset:","f0991dae":"### 4.2.4 Features f76 - f100","244c1fc5":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n# 1 Introduction\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.\n\nThis competition will asked to predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.\n\nSubmissions are evaluated on **area under the ROC curve** between the predicted probability and the observed target.","c186080d":"[back to top](#table-of-contents)\n<a id=\"4.2\"><\/a>\n## 4.2 Distribution\nShowing distribution on each feature that are available in train and test dataset. As there are 118 features, it will be broken down into 25 features for each sections. `Yellow` represents train dataset while `pink` will represent test dataset\n\n**Observations:**\n- All features distribution on train and test dataset are almost similar.\n\n### 4.2.1 Features f1 - f25","556e52ec":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n# 4 Features\nNumber of features available to be used to create a prediction model are `118`.\n\n<a id=\"4.1\"><\/a>\n## 4.1 Missing values\nCounting number of missing value and it's relative with their respective observations between train & test dataset.\n\n<a id=\"4.1.1\"><\/a>\n### 4.1.1 Preparation\nPrepare train and test dataset for data analysis and visualization. *(to see the details, please expand)*","7bc4f6b4":"<a id=\"6.1.1\"><\/a>\n### 6.1.1 XGBoost Classifier","1f561dfc":"<a id=\"6.2.3\"><\/a>\n### 6.2.3 Maximum of features\nCreate a new feature `max` that calculate the maximum value in a row. Adding `max` to the model doesn't change the model performance.","f872c9eb":"### 4.2.3 Features f51 - f75","6c104556":"<a id=\"6.2.5\"><\/a>\n### 6.2.5 Multiplication of features\nCreate a new feature `multiply` that multiply all features values in a row. Adding up `multiply` column increase the model to `0.8019` from `0.8015`.","9a6e0c7d":"<a id=\"5.2\"><\/a>\n## 5.2 Target & missing value\nAs missing value constitute of `62%` of the observations, it's a good idea to explore if higher numbers of missing values in a observation relates to higher probability of claim. An assumption that customers that disclose every information required may be less likely to cheat and they may be more honest.\n\n**Observations:**\n- An observation that has no missing value has the lowest probability to claim with only `14%`.\n- Observation that has a missing value (`1`) increased the probability to claim to `58%`.\n- A missing value between `2 to 13` has probability to claim above `70%`, while it drop for a missing value of `14` to `50%`.\n- This may be used for `target encoding` in `feature engineering`","42081459":"### 3.1.2 Basic statistics\nBelow is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.","f051c26e":"### 4.2.5 Features f101 - f118"}}