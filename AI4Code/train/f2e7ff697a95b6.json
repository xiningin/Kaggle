{"cell_type":{"a32ac8a2":"code","49ccc36e":"code","4a0a8c56":"code","0d8f3878":"code","85a856cc":"code","fddd5d32":"code","95856183":"code","31d62c96":"code","425c6afe":"code","0eb89360":"code","52a12f0a":"code","ae5894fd":"code","9b8745b9":"code","4abaa1b7":"code","6a37d40a":"code","9550a115":"code","b9a1f121":"code","c02569c3":"code","2bce53b8":"code","427ca713":"code","9dafccc5":"code","227b6417":"code","24ec847a":"code","d67fb95f":"code","c1266ec0":"code","74ab9f5d":"code","0fd3e07b":"markdown","b83a6673":"markdown"},"source":{"a32ac8a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","49ccc36e":"import matplotlib.pyplot as plt\nimport math\nimport random","4a0a8c56":"#loading the dataset\ndataset = pd.read_csv(\"..\/input\/Restaurant_Reviews.tsv\", delimiter = \"\\t\", quoting = 3)","0d8f3878":"print(dataset.info())","85a856cc":"print(dataset.head())","fddd5d32":"print(dataset.describe())","95856183":"#cleaning the texts\n#importing re as its is most used library to clean text.\nimport re\nreview = re.sub(\"[^a-zA-Z]\", \" \",  dataset[\"Review\"][0])\n#here re.sub sort the text on by the letter from A to Z and get free of all the other punctuation, nubers, etc.\n\"\"\"here our input to the sub command is \"[^a-zA-Z]\" which means we want everything to remove or sort expect the letter\nfrom a to z and also capital letters from A to Z, also here zA is not seperated and this create a problem and\nso to sort that we add a new paramer space \"\"\"","31d62c96":"print(review)","425c6afe":"# the next process will be to convert all the letter to a lower case letter\nreview = review.lower()\nprint(review)","0eb89360":"# now in the next process we will get rid of word which are of no use to us to judge the review like article, prepositions, etc.\n#we need a library which contain most functions of NLP calles nltk\nimport nltk\n\n\"\"\" now we will run a for loop for all the review for the same process we done above for 1st review\"\"\"\nreview =review.split()\nprint(review)","52a12f0a":"from nltk.corpus import stopwords\nreview = [word for word in review if not word in set(stopwords.words(\"english\"))]\n#the set function is used to speedup the algo as it guide fot the set of words in the list\nprint(review)","ae5894fd":"#now appling stemming. we need to import porterstemmer\nfrom nltk.stem.porter import PorterStemmer\n#we will redesign our algorithem as the above process was to make understanding and now we can have a fun algorithem here.\nps = PorterStemmer()\nreview = [ps.stem(word) for word in review if not word in set(stopwords.words(\"english\"))]\nprint(review)","9b8745b9":"#we will make a list containg all the words and so we use join command with space as to seperate the words for join in single word.\nreview = \" \".join(review)\nprint(review)","4abaa1b7":"#the same process we will perform for all 1000 review.\n#we will make a list for all 1000 reviews names corpus\ncorpus = []\nfor i in range(0, 1000):\n    review_1 = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n    review_1 = review_1.lower()\n    review_1 = review_1.split()\n    ps = PorterStemmer()\n    review_1 = [ps.stem(word) for word in review_1 if not word in set(stopwords.words('english'))]\n    review_1 = ' '.join(review_1)\n    corpus.append(review_1)","6a37d40a":"corpus[:10]","9550a115":"#creating a bag of word model, appluing the machine learning model and tranforming and predicting.\n\"\"\" we will make a table containing all the review in one column. And then we choose a specific word and count the \nnumber of times the specific word appeared in the review \"\"\"\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\n#we can do use stemming , sorting and many other things in countvectorizer but can be more efficient doing it manually.\nX = cv.fit_transform(corpus).toarray()","b9a1f121":"print(X[:10])","c02569c3":"print(X.shape)","2bce53b8":"#here we can see that the 1565 words are taken from the reviews and so we add a new parameter in countVectorizer a max_feature.\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\ny = dataset.iloc[:, 1].values\n","427ca713":"print(X.shape)","9dafccc5":"\"\"\" we will train our model to predict of the review. The most common model used in Natual Language Processing are\nnaive bayes decision tree and random forest. We will use here naive bayes first\"\"\" \n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n\n# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","227b6417":"print(cm)","24ec847a":"from sklearn.metrics import classification_report\ntarget =[\"negative add\", \"positive add\"]\nprint(classification_report(y_test, y_pred, target_names=target))","d67fb95f":"a = (48+86)\/200\nprint(\"accuracy {}\\n\".format(a))","c1266ec0":"#Now we will train random forest to the dataset\nfrom sklearn.ensemble import RandomForestClassifier\nSEED = 1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\nrf = RandomForestClassifier(n_estimators=400, min_samples_leaf=0.12,max_depth = 5,  random_state=SEED)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm_rf = confusion_matrix(y_test, y_pred_rf)\nprint(cm_rf)\n","74ab9f5d":"from sklearn.metrics import classification_report\ntarget =[\"negative add\", \"positive add\"]\nprint(classification_report(y_test, y_pred_rf, target_names=target))","0fd3e07b":"Natural Language Processing is all obout analysing text and this can be book, text doc, html web page, etc. And it is a branch of Machine learning we do some predective analysis on text. Here we analyse the text review and sort the if the review is positive or negative. This will be the general algorith and we can also apply the same to analyse the book and other sources.","b83a6673":"\"\"\" cleaning the dataset is like preparing our dataset to apply our machine learning model. The step include:\n\n1.Get rid of the punctuations as this create many confusions of the words and difficult to understand.\n2.steaming the dataset like sorting some words like loved, liked and converting them to ove and like respectively.\n3.converting all the uppercase letter to lower case letter.\n4.get reid of the nuumbers, unless they are not relevent.\n5.we will construct columns for each associated word and will count if it appears in the review or not. this will create more zeros in our columns.\n6.we will use sparse matrix as most of the part we will be having 0's as most are new words\"\"\""}}