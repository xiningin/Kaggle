{"cell_type":{"d6db53c0":"code","e8cc9340":"code","a610df71":"code","a19bd97c":"code","5cf96c8c":"code","3c54abff":"code","9d7c2681":"code","d9c5d26a":"code","d7123a06":"code","25efb9b0":"code","824fd402":"code","b318f55b":"code","0a2fb69b":"code","922f0856":"code","43d089e5":"code","6d09bdd8":"code","a72e9ac6":"code","faf73bb3":"code","d1e9ea3b":"code","71398f35":"code","b67ebacc":"code","ad52910a":"code","32173c37":"code","e223a6a5":"code","54610383":"code","84f5b522":"markdown","939a4012":"markdown","aceeb6ad":"markdown","ff84364e":"markdown","dc036f1b":"markdown","42648969":"markdown","eec815b3":"markdown","b3719172":"markdown","ac687b7c":"markdown","16fc9866":"markdown","22c56343":"markdown","21fdc30c":"markdown","dfbf7ebf":"markdown","2ef7067a":"markdown","fa3eb3ce":"markdown","822e95a3":"markdown","87da5d5c":"markdown","c5d2a1e3":"markdown"},"source":{"d6db53c0":"# setup\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_context('notebook')\nsns.set_style('whitegrid')\nsns.set_palette('Blues_r')\n\n# dataset\ndf = pd.read_csv('..\/input\/kickstarter-projects\/ks-projects-201801.csv')\n\n# clean up column names that contain whitespace\ndf.columns = df.columns.str.strip()\n\n# view\nprint(df.info())\ndf.head()","e8cc9340":"df['state'].value_counts().plot(kind='bar')\nplt.ylabel('count');\n\n# filter dataset to 'failed' and 'successful' campaigns only\ndf = df.loc[(df['state'] == 'failed') | (df['state'] == 'successful')]\ndf.reset_index(inplace=True, drop=True)","a610df71":"df.info()","a19bd97c":"# datetime conversions\ndf['deadline'] = pd.to_datetime(df['deadline'])\ndf['launched'] = pd.to_datetime(df['launched'])\n\n# 'state' to binary: 0 = failed, 1 = successful\ndf['state'] = df['state'].replace({'failed':0, 'successful':1})\n\n# view finished product\ndf.head()","5cf96c8c":"# number of unique values for object columns \n## uninformative to keep unique identifiers or variables with too many categories for encoding\ndf.select_dtypes(include = 'object').nunique()","3c54abff":"# drop as outlined above\ndf.drop(columns=['ID', 'category', 'currency'], inplace=True)","9d7c2681":"# identify columns with null values\ndf.isnull().sum().sort_values(ascending=False)","d9c5d26a":"# drop problematic columns in favor of using \"real\" values\ndf.drop(columns=['usd pledged', 'goal', 'pledged'], inplace=True)\n\n# fill null titles\ndf['name'] = df['name'].fillna(value='and')","d7123a06":"df.info()","25efb9b0":"# fraction of goal met\ndf = df.assign(fraction_goal = df['usd_pledged_real']\/df['usd_goal_real'])\n\n# years from datetimes\ndf['deadline_year'] = df['deadline'].dt.year\ndf['launched_year'] = df['launched'].dt.year\n\n# length of name\ndf = df.assign(length_name = df['name'].str.len())\n\n# now drop datetime columns\ndf.drop(columns=['deadline', 'launched'], inplace=True)\n\n# view df so far\ndf.head()","824fd402":"# encoding categorical variables\nfrom sklearn.preprocessing import OneHotEncoder\n\n## remove `name` column for now, as we'll do NLP in next section on that\ndf1 = df.drop(columns=['name'])\n\n## list of categorical columns\ncat_cols = df1.select_dtypes(include='object').columns\n\n## df of categorical variables only\ndf1_cat = df1[cat_cols]\n\n## one-hot encoding\nenc = OneHotEncoder(handle_unknown='ignore', sparse=False)\nenc.fit(df1_cat)\ndf1_enc = pd.DataFrame(enc.transform(df1_cat))\n\n## add back column names\ndf1_enc.columns = enc.get_feature_names(cat_cols)\n\n## join with numerical columns\ndf1_all = pd.concat([df1.drop(columns=cat_cols), df1_enc], axis=1)\n\n## shape \nprint(\"Dimensions of dataframe before encoding\", df.shape)\nprint(\"Dimensions of dataframe after encoding: \", df1_all.shape)","b318f55b":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\n# create pipeline: bag of words count vectorizer, tfidf transformer, and multinomial NB classifier\npipeline = Pipeline([\n    ('bow', CountVectorizer(stop_words = 'english', analyzer = 'word')),\n    ('tfidf', TfidfTransformer()),\n    ('classifier', MultinomialNB())\n])\n\n# train\/test split\nfrom sklearn.model_selection import train_test_split\n\nX = df['name']\ny = df['state']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# test the nlp pipeline\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n\n# evaluate nlp\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","0a2fb69b":"sns.countplot(df['state'])\nplt.ylabel('count')\nplt.title(\"Categorical target variable 'state'\\n 0: failed, 1: successful\", size=16);","922f0856":"print(\"Percent sucessful: \", round((df[df['state'] == 1].shape[0]\/df.shape[0])*100, 1))","43d089e5":"sns.boxplot(x='state', y='fraction_goal', data=df)\nplt.title(\"Continuous target variable 'fraction_goal' by 'state'\\n 0: failed, 1: successful\", size=16);","6d09bdd8":"g = sns.FacetGrid(df, col='state', sharey=False, height=4, aspect=0.8)\ng.map(sns.boxplot, 'state', 'usd_goal_real')\ng.set(xlabel='', xticklabels='')\ng.fig.suptitle(\"Goal amounts by 'state'\\n 0: failed, 1: successful\", y=1.1);\n","a72e9ac6":"# plot the rows we're going to filter out due to goal amount not meeting the cutoff\n## (see notes in markdown above for explanation)\n\ndata_to_plot = df[df['usd_goal_real'] < 1000]\n\ng = sns.FacetGrid(data_to_plot, col='state', sharey=False, height=4, aspect=0.8)\ng.map(sns.boxplot, 'state', 'usd_goal_real')\ng.set(xlabel='', xticklabels='')\ng.fig.suptitle(\"Goal amounts less than $1000 by 'state'\\n 0: failed, 1: successful\", y=1.1);","faf73bb3":"# number of observations before filtering\nprint(\"Number of rows before filtering: \", df.shape[0])\n\n# cutoff value\ncutoff = 1000\n\n# filter the df\ndf = df[df['usd_goal_real'] >= cutoff]\n\n# filter encoded df\ndf1_all = df1_all[df1_all['usd_goal_real'] >= cutoff]\n\n# number of observations after filtering\nprint(\"Number of rows after filtering: \", df.shape[0])","d1e9ea3b":"sns.countplot(df['state'])\nplt.ylabel('count')\nplt.title(\"Categorical target variable 'state'\\n 0: failed, 1: successful\", size=16);","71398f35":"print(\"Percent sucessful after removing campaigns with unreasonably small goals: \", round((df[df['state'] == 1].shape[0]\/df.shape[0])*100, 1))","b67ebacc":"sns.boxplot(x='state', y='fraction_goal', data=df)\nplt.title(\"Continuous target variable 'fraction_goal' by 'state'\\n 0: failed, 1: successful\", size=16);","ad52910a":"# calculate kendall correlations\ncorrs = df1_all.corr(method='kendall')\n\n# clustered heatmap\nsns.clustermap(corrs, cmap='coolwarm', center=0, figsize=(14,14), cbar_pos=(-0.05, 0.8, 0.05, 0.18));","32173c37":"# drop from one-hot encoded data\ndf1_all.drop(columns=['backers', 'usd_pledged_real', 'deadline_year'], inplace=True)\n\nprint(\"The final dataframe for machine learning models:\\n\")\nprint(df1_all.info())","e223a6a5":"# # gather X and y data\n# X = pd.concat([df1_all.drop(columns = ['state', 'fraction_goal']), df['name']], axis=1) # drop the target variables, add in the project title data\n# y = df1_all['state']\n\n# # column transformer for text data\n# from sklearn.compose import ColumnTransformer\n\n# text_transformer = Pipeline([\n#     ('bow', CountVectorizer(stop_words = 'english', analyzer = 'word')),\n#     ('tfidf', TfidfTransformer())\n# ])\n\n# text_preprocessor = ColumnTransformer(\n#     transformers = [\n#         ('text', text_transformer, 'name') # column on which perform text preprocessing is df['name']\n#     ]\n# )\n\n# # classification model using all data\n# from sklearn.ensemble import RandomForestClassifier\n\n# model = RandomForestClassifier()\n\n# # overall pipeline for text transformation + classifier on all data\n# transform_and_model = Pipeline([\n#     ('text_transformation', text_preprocessor),\n#     ('classifier', model)\n# ])\n\n# # train\/test split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# # train model\n# transform_and_model.fit(X_train, y_train)\n\n# # predictions\n# y_pred = transform_and_model.predict(X_test)\n\n# # evaluate\n# print(classification_report(y_test, y_pred))","54610383":"# import eli5\n# from eli5.sklearn import PermutationImportance\n\n# perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\n# eli5.show_weights(perm, feature_names = X_test.columns.tolist())","84f5b522":"# Exploratory data analysis\n\n## Explore the target variables\n* For a classification model: `state`\n* For a regression model: `fraction_goal`\n    - Observation: There are many outliers in `fraction goal` for `state == 1`. Many of these outliers are well above 1.0, which should be the `fraction_goal` cutoff value for a successful campaign (i.e. achieved 100% of the goal). Let's investigate these further, as some of these sucesses may be false positives if the goal was set unreasonably low.","939a4012":"### Drop uninformative columns\n\n* `ID` is unique identifier\n* `category` has too many unique values to be encoded for machine learning\n* `currency` would be heavily tied to `country`, so drop the former\n* Although it has a lot of unique values, we will keep `name` and perform natural language processing","aceeb6ad":"### Replot the target variables after removing campaigns with unreasonably small goals","ff84364e":"### Feature engineering\n\n* Extract year from datetimes:\n    - `deadline_year` from `deadline`\n    - `launched_year` from `launched`\n* Fraction of goal met: *We can use this for a regression model*\n    - `fraction_goal` = `usd_pledged_real \/ usd_goal_real`\n* Length of `name`\n* Encoding:\n    - One-hot encoding of categorical variables","dc036f1b":"* Cleaned dataset summary:","42648969":"# Machine Learning models\n\n## Classification of `state`\n\n* Combine NLP and numeric data modeling as follows:\n    - Column transformer for BOW\/TFIDF of project titles\n    - Classification of `state` from numeric data + transformed text data\n* Test the model using a 70\/30 train\/test split\n* Evaluate with classification report\n* WORK IN PROGRESS...","eec815b3":"# Kickstarter Campaign Success Prediction\n\nWill a kickstarter campaign meet its goal? Let's use machine learning to predict kickstarter campaign sucess based on the campaign's features.","b3719172":"## Identify correlations\n\n* Identify correlated variables in clustered heatmap (Kendall non-parametric correlation values): Positive correlations appear red, negative correlations are blue, and no correlation is gray\n* Findings of interest:\n    1. Highly positively correlated cluster: `state`, `fraction_goal`, `backers` (the number of people who pledge money), `usd_pledged_real`\n        - **Action:** As this cluster is so closely related to the target variable, and indeed can only be determined after the campaign is successful or failed, including these in machine learning models may cause target leakage. Therefore, we will drop `backers` and `usd_pledged_real` from machine learning models that seek to predict `state` or `fraction_goal`.\n    1. The features `launched_year` and `deadline_year` have a near-100% positive correlation\n        - **Action:** Drop one of the two features (we will drop `deadline_year`), since we don't need two features that tell us the same thing.\n    1. Negative correlation between `usd_goal_real` and the target variables `state` and `fraction_goal`\n        - **Interpretation:** This makes sense, as it is more challenging to be successful when aiming for a higher goal value.\n    1. Positive or negative correlatinos seen between several `main_category` variables and the target variables `state` and `fraction_goal`\n        - **Interpretations:** For example, Technology products are less likely to succeed and Film & Video products are more likely to succeed.","ac687b7c":"### Investigate whether some campaigns successful due to unreasonably small goal\n\n* It is possible that some successes are \"fake\", because the goal was set unreasonably low\n* Plot goal amounts by state (failed, successful)\n    - Findings: \n        - Both failed and successful campaigns have a wide range for `usd_goal_real`, meaning that some successful campaigns did in fact raise meaningful amounts of money.\n        - However, both failed and successful campaigns have a majority of observations in the very low end of the range. \n    - Action:\n        - Let's remove any campaign that set a goal lower than \\$1000, as it's hard to imagine that a product could be brought to market for lesser amounts.","16fc9866":"# Natural language processing\n\n* *What's in a name?* ...Does the title of the kickstarter project affect its chance at success? \n* Perform NLP on `name` feature with 30\/70 train\/test split \n    - Findings: The project title can predict success with 64% accuracy\n        - Not great, but may be useful in the final model","22c56343":"* Drop columns prior to implementing machine learning models, as outlined by the **Action** items for the correlations above:","21fdc30c":"### Filter and encode the target variable\n\n* `state` is the target variable for campaign success\/failure: plot number of observations for each outcome\n    - We will focus on predicting `state == 'failed'` *vs.* `state == 'successful'` for the remainder of this analysis","dfbf7ebf":"* Insights on feature contributions to model with permutation importance:","2ef7067a":"# Dataset preprocessing\n\n* Load the dataset","fa3eb3ce":"### Transformations\n\n* Transform data types\n    - `deadline` and `launched` to datetime\n    - `state` to binary (0: failed, 1: successful)","822e95a3":"### Filter dataset to remove campaigns with unreasonably small goal\n\n* As detailed in the section above, we will remove any campaign that set a goal lower than \\$1000.","87da5d5c":"### Clean null values\n\n* Drop `usd pledged` in favor of using `usd_pledged_real`\n    * For uniformity, also:\n        - drop `goal` in favor of using `usd_goal_real`\n        - drop `pledged` in favor of using `usd_pledged_real`\n* Fill null values in `name` with a stopword, so it will be ignored during NLP steps","c5d2a1e3":"* Filtered dataset summary:"}}