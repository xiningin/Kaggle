{"cell_type":{"be17fe5d":"code","e9eddaba":"code","db822ffb":"code","3a76e6c8":"code","c3280249":"code","c1ece0ef":"code","872c8dc0":"code","2af166f2":"code","5c96521f":"code","63c36f81":"code","1fd7e22e":"code","e65240c6":"code","80199ea1":"code","172629df":"code","5d6b88b5":"code","aea9badf":"code","998440bd":"code","c4afeeee":"code","79b007fc":"code","b04b68da":"code","545260e3":"code","7c5679d6":"code","30a1027c":"code","7055561f":"code","9a3a327b":"code","2a1cdbec":"code","d71dae61":"code","043872eb":"code","21a1fde3":"code","7c3645d3":"code","9f25f502":"code","f49be298":"code","a907921f":"code","53a8247f":"code","b5564078":"code","c0ada2db":"code","6ebe8684":"code","b3737974":"code","6cddb27e":"code","e573969f":"code","573fb263":"code","3a15736d":"code","63d1dfe0":"code","d10b9c61":"code","5a9c5dcd":"code","71f31f9d":"code","682d3a08":"code","e1354d2b":"code","dad3e378":"code","a15bad63":"code","6aba42f8":"code","2b06d7e4":"code","75b44d75":"code","88dc6e67":"code","38657aa1":"code","6d06eefc":"code","4965eb1a":"code","d863be40":"code","8dc29f41":"code","09039505":"code","488eefb0":"code","df3406be":"code","a7921080":"code","ce0a8d42":"code","9d30fff3":"code","4c7d06d8":"code","97e1618a":"code","a544d053":"code","90fe6fdc":"code","37e1d9f0":"code","6b62e210":"code","dca3100d":"code","427b587e":"code","e0f872a6":"code","76740a90":"code","5650ea8f":"code","60053169":"code","14d7e5dd":"code","18d49479":"code","89b880bb":"code","973decf5":"code","86621f1e":"code","a2f3b3f5":"code","b6585ca9":"code","008ecf2e":"code","db8476f3":"code","4d99cea7":"code","0e1432d4":"code","806ef4a7":"code","73d02b3a":"code","3b52ecbd":"code","fec66eff":"code","fee4e5ad":"code","4957460a":"code","b77de950":"markdown","d13da35b":"markdown","37309a36":"markdown","b0fddfc8":"markdown","a22d08dd":"markdown","e7ba0a89":"markdown","1b35f0cf":"markdown","13e6158f":"markdown","be926070":"markdown","b192c122":"markdown","1ca1a64f":"markdown","6e51ae7e":"markdown","19c9957f":"markdown","f5907925":"markdown","4a707876":"markdown","13a37173":"markdown","365e8d6f":"markdown","0194c5d0":"markdown","c01bec74":"markdown","8a3c0a79":"markdown","c3f60e06":"markdown","eac31c27":"markdown","0a718b76":"markdown","79a4c211":"markdown","db1b14dc":"markdown","c370f353":"markdown","1219144a":"markdown","0e3c67fc":"markdown","a2dd06f5":"markdown","27307ed3":"markdown","5a32523f":"markdown","d3e21879":"markdown","3057a191":"markdown","ebed0fc9":"markdown","eababb36":"markdown","b9a514cb":"markdown","4854e68e":"markdown","93406e5c":"markdown","11b4f74e":"markdown","d0434efb":"markdown","0a08cd68":"markdown","77315b94":"markdown","e7c02cff":"markdown","0070ef13":"markdown","c25ebeb1":"markdown","0debac3e":"markdown","f286b937":"markdown","ae6588ba":"markdown","3c8c6ae8":"markdown","0fd86bc5":"markdown","05c71fc8":"markdown","82d17368":"markdown","2d6101fb":"markdown","b35398e0":"markdown","d2b22ff0":"markdown","510bcc35":"markdown","54b6f848":"markdown","0f3acb4d":"markdown","cedbca34":"markdown","55089334":"markdown","48cf76dd":"markdown","b26fd9a5":"markdown","95f5ec0a":"markdown","a98ef79e":"markdown","a29059df":"markdown","295d3b54":"markdown","6961c82d":"markdown","b51dd45f":"markdown","1d728cdc":"markdown","2483ca9d":"markdown","8c7ecb3b":"markdown","a3f6d23e":"markdown","eca9f689":"markdown","4450c0e3":"markdown","b45d6805":"markdown","dc43bcfd":"markdown","59f5e240":"markdown","92a47f14":"markdown","65bbfc30":"markdown","5f5af321":"markdown","1bf5d483":"markdown","3736b3a1":"markdown","ac9a3edd":"markdown","9bbeff02":"markdown","8f6bacbb":"markdown","caa86cf6":"markdown","c07d96aa":"markdown","d4358304":"markdown","648c5320":"markdown","0a1d173b":"markdown","045f5546":"markdown","5653014c":"markdown","fbd75eba":"markdown","0fb8c071":"markdown","2c2e51c6":"markdown","bf2679bb":"markdown","1be24abf":"markdown"},"source":{"be17fe5d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e9eddaba":"df=pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")","db822ffb":"df.info() ","3a76e6c8":"df.columns = df.columns.str.strip().str.replace('-', '_')","c3280249":"df=df.rename(columns={\"class\": \"m_class\"})","c1ece0ef":"df.head() ","872c8dc0":"df.describe()","2af166f2":"for i in df:\n    unique=np.unique(df[i])\n    print('{}:{}'.format(i,unique))","5c96521f":"df = df.drop(['veil_type'], axis=1)","63c36f81":"df2=df.copy()","1fd7e22e":"list_df_features=[]\nfor i in df:\n    df_features=i\n    list_df_features.append(df_features)\nlist_df_features","e65240c6":"#xtick olarak atamak i\u00e7in\nlist_unique_real=[]\nfor i in df:\n    unique=np.unique(df[i]).tolist()\n    list_unique_real.append(unique)\nlist_unique_real","80199ea1":"print(df['m_class'].value_counts())","172629df":"#ratio of poisonous and edible mushrooms..\n#plt.style.use('bmh')\n%matplotlib inline\n#\nf,ax=plt.subplots(1,2,figsize=(10,4))\ndf['m_class'].value_counts().plot.pie(explode=[0.1,0],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('m_class')\nax[0].set_ylabel('')\n#\nsns.countplot('m_class',data=df,ax=ax[1])\nax[1].set_title('m_class')\nplt.show()","5d6b88b5":"#for i in df:\n#    print(df[i].value_counts(),'\\n')","aea9badf":"#for i in df:\n#    print(df.groupby([i,'m_class'])['m_class'].count())","998440bd":"#Counts of values in each features descending from ascending..\nf,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    df[list_df_features[i]].value_counts().plot.bar(color=sns.color_palette(\"rocket\"),ax=ax[i])\n    ax[i].set_title(list_df_features[i])","c4afeeee":"#Counts of poisonous and edible mushrooms in each features.\n#plt.style.use('fivethirtyeight')\nf,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    sns.countplot(list_df_features[i],hue='m_class',data=df,ax=ax[i])","79b007fc":"f,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    df[list_df_features[i+6]].value_counts().plot.bar(color=sns.color_palette(\"rocket\"),ax=ax[i])\n    ax[i].set_title(list_df_features[i+6])","b04b68da":"f,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    sns.countplot(list_df_features[i+6],hue='m_class',data=df,ax=ax[i])","545260e3":"f,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    df[list_df_features[i+12]].value_counts().plot.bar(color=sns.color_palette(\"rocket\"),ax=ax[i])\n    ax[i].set_title(list_df_features[i+12])","7c5679d6":"f,ax=plt.subplots(1,6,figsize=(19,4))\nfor i in range(0,6):\n    sns.countplot(list_df_features[i+12],hue='m_class',data=df,ax=ax[i])","30a1027c":"f,ax=plt.subplots(1,4,figsize=(19,4))\nfor i in range(0,4):\n    df[list_df_features[i+18]].value_counts().plot.bar(color=sns.color_palette(\"rocket\"),ax=ax[i])\n    ax[i].set_title(list_df_features[i+18])","7055561f":"f,ax=plt.subplots(1,4,figsize=(19,4))\nfor i in range(0,4):\n    sns.countplot(list_df_features[i+18],hue='m_class',data=df,ax=ax[i])","9a3a327b":"le=LabelEncoder()\nfor col in df.columns:\n    df[col] = le.fit_transform(df[col])","2a1cdbec":"df.head()","d71dae61":"list_unique_encoded=[]\nfor i in df:\n    unique=np.unique(df[i]).tolist()\n    list_unique_encoded.append(unique)\nlist_unique_encoded","043872eb":"for col in df:\n    print('{}:{}'.format(col,np.unique(df[col])))\nprint(\"\\n\")\nfor i in df2:\n    print('{}:{}'.format(i,np.unique(df2[i])))","21a1fde3":"df3=df.copy()","7c3645d3":"df.describe()","9f25f502":"sns.relplot(x=\"bruises\",y=\"odor\",col=\"m_class\",data=df)","f49be298":"sns.relplot(x=\"spore_print_color\",y=\"odor\",col=\"m_class\",data=df)","a907921f":"sns.relplot(x=\"population\",y=\"odor\",col=\"m_class\",data=df)","53a8247f":"sns.relplot(x=\"habitat\",y=\"odor\",col=\"m_class\",data=df)","b5564078":"gd_df=pd.get_dummies(df,columns=df.columns)\ngd_df","c0ada2db":"gd_df.loc[3:5, 'm_class_0':'cap_shape_5']","6ebe8684":"df.corr()","b3737974":"f,ax=plt.subplots(figsize=(15,15))\nsns.heatmap(df.corr(), annot=True, lineWidth=.5, fmt='.2f', ax=ax)\nplt.show()","6cddb27e":"color_columns= []\nfor i in df.columns:\n    if 'color' in i:\n        color_columns.append(i)\ndf_color = df[color_columns]\ndf_color.head()","e573969f":"list_color_features=[]\nfor i in df_color:\n    color_features=i\n    list_color_features.append(color_features)\nlist_color_features","573fb263":"f,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(df_color.corr(), annot=True, lineWidth=.5, fmt='.2f',cmap=\"YlGnBu\", ax=ax)\nplt.show()","3a15736d":"#number of poisonous mushroom in every odor types.\nodor_list=list(df['odor'].unique())\nfor i in odor_list:\n    x=df[df['odor']==i]\n    print('{}:{}'.format(i,sum(x.m_class)))","63d1dfe0":"#number of edible or poisonous mushroom in every odor types\nodor_list_2=list(df['odor'].unique())\nfor i in odor_list_2:\n    x=df[df['odor']==i]\n    print('{}:{}'.format(i,x.groupby('m_class').size()))\n    print('\\n')","d10b9c61":"#number of mushrooms in every odor types\ndf.odor.value_counts()","5a9c5dcd":"#total poisonous mushroom number\nprint(len(df[df.m_class == 1]))","71f31f9d":"#total edible mushroom number\nprint(len(df[df.m_class == 0]))","682d3a08":"df.odor.plot(kind='hist', bins=20, figsize=(8,8), color=\"gray\") \nplt.show()","e1354d2b":"\nodor_list=list(df['odor'].unique())\nm_class_ratio=[]\nfor i in odor_list:\n    x=df[df['odor']==i]\n    m_class_rate=sum(x.m_class)\/len(x)\n    m_class_ratio.append(m_class_rate)\n\nplt.figure(figsize=(15,10))\nsns.barplot(x=odor_list, y=m_class_ratio, palette=\"Greens_d\")\nplt.xticks(rotation= 360)\nplt.xlabel('Odors')\nplt.ylabel('Poison Rate')\nplt.title('Poison Rate Given Odors')","dad3e378":"df.cap_shape.plot(kind='hist', bins=20, figsize=(8,8), color=\"gray\") \nplt.show()","a15bad63":"cap_shape_list=list(df['cap_shape'].unique())\nm_class_ratio=[]\nfor i in cap_shape_list:\n    x=df[df['cap_shape']==i]\n    m_class_rate=sum(x.m_class)\/len(x)\n    m_class_ratio.append(m_class_rate)\n\nplt.figure(figsize=(15,10))\nsns.barplot(x=cap_shape_list, y=m_class_ratio, palette = sns.cubehelix_palette(len(x)))\nplt.xticks(rotation= 360)\nplt.xlabel('Cap Shapes')\nplt.ylabel('Poison Rate')\nplt.title('Poison Rate Given Cap Shapes')","6aba42f8":"y = df.m_class.values\nx_df = df.drop([\"m_class\"],axis=1)","2b06d7e4":"x = (x_df - np.min(x_df))\/(np.max(x_df)-np.min(x_df)).values","75b44d75":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)","88dc6e67":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"logistic regression test accuracy {}\".format(lr.score(x_test,y_test)))\n","38657aa1":"y_pred =lr.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","6d06eefc":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(lr,x,y, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","4965eb1a":"lr2 = LogisticRegression(C=1000.0, penalty=\"l1\", random_state=1)\nlr2.fit(x_train,y_train)\nprint(\"logistic regression test accuracy with Grid Search Cross Validation {}\".format(lr2.score(x_test,y_test)))","d863be40":"y_pred =lr2.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","8dc29f41":"# %% parameter initialize and sigmoid function\ndimension = 22\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n\nw,b = initialize_weights_and_bias(22)\n\ndef sigmoid(z):\n    \n    y_head = 1\/(1+ np.exp(-z))\n    return y_head\nprint(sigmoid(0))","09039505":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","488eefb0":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        \n        cost_list2.append(cost)\n        index.append(i)\n        print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","df3406be":"#%%  # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","a7921080":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0] \n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train.T, y_train.T, x_test.T, y_test.T,learning_rate = 4, num_iterations = 300)    ","ce0a8d42":"p=df[df.m_class== 1]\ne=df[df.m_class== 0]","9d30fff3":"plt.scatter(p.odor,p.cap_shape,color=\"red\",label=\"poisonous\")\nplt.scatter(e.odor,e.cap_shape,color=\"green\",label=\"edible\")\nplt.xlabel(\"odors\")\nplt.ylabel(\"cap_shapes\")\nplt.legend()\nplt.show()","4c7d06d8":"plt.scatter(p.veil_color,p.gill_attachment,color=\"red\",label=\"poisonous\")\nplt.scatter(e.veil_color,e.gill_attachment,color=\"green\",label=\"edible\")\nplt.xlabel(\"veil_color\")\nplt.ylabel(\"gill_attachment\")\nplt.legend()\nplt.show()","97e1618a":"df[\"gill_attachment\"].value_counts()","a544d053":"df[\"veil_color\"].value_counts()","90fe6fdc":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors =3)\nknn.fit(x_train,y_train)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","37e1d9f0":"y_pred =knn.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","6b62e210":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=10, random_state=42, shuffle=False)\nfor trainkf, testkf in kf.split(x):\n    #print(\"%s %s\" % (trainkf, testkf))\n    print(\"trainkf:\",len(trainkf))\n    print(\"testkf:\",len(testkf))","dca3100d":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(knn,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","427b587e":"# find k value\nscore_list = []\nfor each in range(1,50):\n    knn3 = KNeighborsClassifier(n_neighbors = each)\n    knn3.fit(x_train,y_train)\n    score_list.append(knn3.score(x_test,y_test))\n    \nplt.plot(range(1,50),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","e0f872a6":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\nprint(\"svm score: {} \".format(svm.score(x_test,y_test)))","76740a90":"y_pred = svm.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","5650ea8f":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(svm,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","60053169":"from sklearn.model_selection import cross_val_score\nsvm2 = SVC(C=1,gamma=1)\naccuracies = cross_val_score(svm2,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))","14d7e5dd":"#Naive bayes \nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"accuracy of naive bayes algo: \",nb.score(x_test,y_test))","18d49479":"y_pred = nb.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","89b880bb":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(nb,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","973decf5":"from sklearn.model_selection import GridSearchCV\ngrid = {'var_smoothing': np.logspace(0,-9, num=100)} \nnb = GaussianNB()\nnb_cv = GridSearchCV(nb,grid,cv = 10)\nnb_cv.fit(x_train,y_train)\nprint(\"tuned hyperparameters: (best parameters): \",nb_cv.best_params_)\nprint(\"accuracy: \",nb_cv.best_score_)","86621f1e":"nb2 = GaussianNB(var_smoothing=0.0023101297000831605)\nnb2.fit(x_train,y_train)\nprint(\"score: \", nb2.score(x_test,y_test))","a2f3b3f5":"y_pred = nb2.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","b6585ca9":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\nnb2 = GaussianNB(var_smoothing=0.0023101297000831605)\naccuracies = cross_val_score(nb2,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","008ecf2e":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"score: \", dt.score(x_test,y_test))","db8476f3":"y_pred = dt.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","4d99cea7":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(dt,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","0e1432d4":"from sklearn.model_selection import GridSearchCV\ngrid = {'criterion': ['gini', 'entropy'],\n             'max_depth': [1, 2, 3, 4, 5, 6, 7, 8],\n             'min_samples_split': [2, 3]}\ndt = DecisionTreeClassifier()\ndt_cv = GridSearchCV(dt,grid,cv = 10)\ndt_cv.fit(x_train,y_train)\nprint(\"tuned hyperparameters: (best parameters): \",dt_cv.best_params_)\nprint(\"accuracy: \",dt_cv.best_score_)\n","806ef4a7":"from sklearn.model_selection import cross_val_score\ndt2 = DecisionTreeClassifier(criterion='gini', max_depth= 8, min_samples_split= 2)\naccuracies = cross_val_score(dt2,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","73d02b3a":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))","3b52ecbd":"y_pred = rf.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n#visualization\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","fec66eff":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(rf,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","fee4e5ad":"#K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\nrf2 = RandomForestClassifier(bootstrap= True, criterion='gini', n_estimators= 100)\naccuracies = cross_val_score(rf2,x,y, cv = 10)\nprint(accuracies)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n#devam","4957460a":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2, whiten= True )  # whitten = normalize\npca.fit(df3)\n\ndf3_pca = pca.transform(df3)\n\nprint(\"variance ratio: \", pca.explained_variance_ratio_)\n\nprint(\"sum: \",sum(pca.explained_variance_ratio_))","b77de950":"In habitat value is 0(woods),1(grasses),3(meadows),4(paths) and odor is 3(anise), mushrooms are edible.And all values in habitat except 3(meadows),6(waste) if odor is 2(foul),mushrooms are poisonous.","d13da35b":"All of features are non-null object so I have not missing values for this dataset.","37309a36":"Output:\n\n* tuned hyperparameters: (best parameters):  {'C': 1000.0, 'penalty': 'l1'}\n* accuracy:  0.9663025080481659","b0fddfc8":"I removed the below code because it take a long time during running.But I detected best parameter values with this code.","a22d08dd":"* If gill_spacing is crowded(w) mushrooms mostly edible(91%).\n* If gill_size is narrow(n) moshrooms are mostly poisonous(%88).\n* gill_color is brown(n)(89%=936 sample) or purple(u)(%90=444 sample),mushrooms are mostly edible.\n* gill_color is buff(b) colored, mushrooms are exactly poisonous(1728 sample).","e7ba0a89":"**CONTENT OF THE NOTEBOOK:**\n* EXPLORATORY DATA ANALYS\u0130S AND FEATURE ENGINEERING\n* CLASSIFICATION ALGORITHMS\n* LOGISTIC REGRESSION CLASSIFICATION\n* Confusion Matrix of Logistic Regression\n* Logistic Regression with K-Fold Cross Validation\n* Logistic Regression With Grid Search Cross Validation\n* Confusion Matrix of Logistic Regression With Grid Search CV\n* KNN (K Nearest Neighbour) CLASSIFICATION\n* Confusion Matrix of KNN\n* KNN With K-Fold Cross Validation\n* SVM (Support Vector Machine)\n* Confusion Matrix of SVM\n* SVM With K-Fold Cross Validation\n* SVM With Grid Search Cross Validation\n* SVM With Grid Search Cross Validation and K-Fold\n* NAIVE BAYES CLASSIFICATION\n* Confusion Matrix of Naive Bayes\n* Naive Bayes With K-Fold Cross Validation\n* Naive Bayes With Grid Search Cross Validation\n* Confusion Matrix of Naive Bayes With Grid Search Cross Validation\n* Naive Bayes With Grid Search Cross Validation and K-Fold\n* DECISION TREE CLASSIFICATION\n* Confusion Matrix of Decision Tree Classification\n* Decision Tree Classification With K-Fold Cross Validation\n* Decision Tree Classification With Grid Search Cross Validation\n* Decision Tree Classification With Grid Search CV and K-Fold\n* RANDOM FOREST CLASSIFICATION\n* Confusion Matrix of Random Forest Classification\n* Random Forest Classification With K-Fold Cross Validation\n* Random Forest Classification With Grid Search Cross Validation\n* Random Forest Classification With Grid Search CV and K-Fold\n* PCA (PRINCIPAL COMPONENT ANALYSIS)","1b35f0cf":"**NAIVE BAYES CLASSIFICATION**","13e6158f":"**KNN With K Fold Cross Validation**","be926070":"Checking the values..","b192c122":"I can say that if there isn't bruises(0) and odor is not anise(3) or pungent(6), mushrooms are mostly poisonous.","1ca1a64f":"**SVM (Support Vector Machine)**","6e51ae7e":"Preparer-RUNLIY","19c9957f":"I want to show visualization about correlation.","f5907925":"* If stalk_surface_above_ring is silky(k), mushrooms are mostly poisonous with 94% rate(2228 sample).\n* If stalk_surface_below_ring is silky(k), mushrooms are mostly poisonous with 93% rate(2160 sample).\n* If stalk_color_above_ring and stalk_color_below_ring are gray(g) colored muhrooms are completely edible.(576 sample)\n* If stalk_color_above_ring and stalk_color_below_ring are buff(b) colored mushrooms are completely poisonous(432 sample)","4a707876":"**Logistic Regression With Grid Search Cross Validation**","13a37173":"* Q3-Q1=IQR\n* Q1-1.5(IQR) -> OUTLIER\n* Q3+1.5(IQR) -> OUTLIER\n* describe() already calculate these automatically","365e8d6f":"**Decision Tree Classification With Grid Search Cross Validation and K Fold**","0194c5d0":"**Inferences:**","c01bec74":"Review dataset..","8a3c0a79":"I step up the accuracy to 95.32 from 94.95","c3f60e06":"**DECISION TREE CLASSIFICATION**","eac31c27":"**Decision Tree Classification With Grid Search Cross Validation**","0a718b76":"I found variety of values in features","79a4c211":"**KNN (K Neirest Neighbour) CLASSIFICATION**","db1b14dc":"**Confusion Matrix of Logistic Regression**","c370f353":"**Mushroom analysing according to only odor types**","1219144a":"**Inferences:**","0e3c67fc":"corr() method uses to show the correlation between the values.","a2dd06f5":"**SVM With K Fold Cross Validation**","27307ed3":"**Confusion Matrix of Logistic Regression With Grid Search Cross Validation**","5a32523f":"Dataset\u2019s path is put(csv file).","d3e21879":"In all values of population, if odor is 5(none) and population isn't 1(clustered) or 4(several) mushrooms are edible.And if odor is 0(almond) and 3(anise) and population is 2(numerous),3(scattered),4(several) or 5(solitary), mushrooms are edible.","3057a191":"I stored m_class values in y variable and other features in x_df variable.Because I try to find next mushroom is poisonous or edible and I'll show step by step.","ebed0fc9":"**Logistic Regression With K Fold Cross Validation**","eababb36":"* cv:Determines the cross-validation splitting strategy.","b9a514cb":"In this research,it is tried to learn which features are mostly indicative of a poisonous mushroom and what types of machine learning models perform best on this dataset.The Mushroom\u2019s dataset contains 8124 mushrooms both edible and poisonous in several species.The data contains attributes are class, cap shape, cap surface, cap color, bruises,  odor, gill attachment, gill spacing, gill size, gill color, stalk shape, stalk root, stalk surface above ring, stalk surface below ring, stalk color above ring, stalk color below ring, veil type, veil color, ring number, ring type, spore print color, population, habitat. According to these attributes, it was found out which factors are effective on whether the mushroom is poisonous or not.And some classification models used to categorize the mushroom type is edible or poisonous to identify the class to which a new data will fall under.","4854e68e":"**Naive Bayes With K Fold Crosss Validation**","93406e5c":"**Confusion Matrix of Naive Bayes**","11b4f74e":"And normalization..","d0434efb":"Hold the feature names in list.I occur this list.Because I will use in visualizations with for loops.","0a08cd68":"**Confusion Matrix of KNN**","77315b94":"*Output:\n\ntuned hyperparameters: (best parameters):  {'C': 1, 'gamma': 1}\n\naccuracy:  1.0\n","e7c02cff":"I calculated poisonous mushroom ratio according to odors.But it is misleading.For example above histogram 4(musty odor type) count's is too few but below it seem %100 poisonous.So we can not generalize. ","0070ef13":"**get_dummies**","c25ebeb1":"**Confusion Matrix of SVM**","0debac3e":"* If bruises t(true) mushrooms are edible overwhelmingly(%82 edible)\n* Mushrooms have almond(a) and anise(l) odors are completely edible (with 400 sample)\n* Mushrooms that hasn't an odor(n) are mostly edible(Only 3% are poisonous)\n* Foul(f)(2160),fishy(y)(576),spicy(s)(576),creosote(c)(192) odor types completely poisonous.","f286b937":"**Random Forest Classification With Grid Search Cross Validation**","ae6588ba":"**Naive Bayes With Grid Search Cross Validation**","3c8c6ae8":"**Let's some comments about mushrooms over relplots.. **","0fd86bc5":"I used this method to convert all values into 1 and 0.It isn't necessary,sometimes it makes easier normalization but I prefer different method instead of this.","05c71fc8":"I compared unique values between with label encoder and without label encoder in columns to make interpretation easy.","82d17368":"**Confusion Matrix of Naive Bayes With Grid Search Cross Validation**","2d6101fb":"Hold the unique values in list.","b35398e0":"For object data, the result\u2019s index will include count, unique, top, and freq.The top is the most common value. The freq is the most common value\u2019s frequency.","d2b22ff0":"I run below code before and found the best C and penalty values but I removed the code because it take a long time during running.","510bcc35":"And 'class' is a keyword in python, if I don't change it, it will cause some problems.\nSo I changed with m_class that's mean mushroom class","54b6f848":"from sklearn.model_selection import GridSearchCV\n\ngrid = {'n_estimators': [100, 300, 500],\n    'criterion': ['gini', 'entropy'],\n    'bootstrap': [True, False]}  \n    \nrf = RandomForestClassifier()\n\nrf_cv = GridSearchCV(rf,grid,cv = 10)\n\nrf_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameters: (best parameters): \",rf_cv.best_params_)\n\nprint(\"accuracy: \",rf_cv.best_score_)\n\nOutput:\n\ntuned hyperparameters: (best parameters):  {'bootstrap': True, 'criterion': 'gini', 'n_estimators': 100}\n\naccuracy:  1.0","0f3acb4d":"* p=poisonous mushroom  \n* e=edible mushromm","cedbca34":"**SVM With Grid Search Cross Validation and K Fold**","55089334":"* Histogram is about frequency of data with visualization.\n* bins= thinness of graphic columns and number of bar\n* figsize=magnitude of graphic frame\n\nfor i in df:\n\n   df[i].plot(kind='hist', bins=20, figsize=(5,5), color='red', grid=True)\n    \n   plt.xlabel(\"{}\".format(i), fontsize=15)\n    \n   plt.show()\n    \nI can use above codes for histogram to learn counts of values in each features but I prefer other technique.\n","48cf76dd":"I calculated poisonous mushroom ratio according to cap shapes.And it is misleading too with same reason.","b26fd9a5":" I want to stop Pandas showing Future Warning messages again and again so used import warnings","95f5ec0a":"**Random Forest Classification With Grid Search Cross Validation and K Fold**","a98ef79e":"* If ring_type is large(l),mushrooms are poisonous with 1296 samples.\n* If spore_print_color is chocolate(h),mushrooms are mostly poisonous(97% = 1584 samples).\n* If spore_print_color is brown(n),mushrooms are mostly edible(89% = 1744 samples).\n* If spore_print_color is black(k),mushrooms are mostly edible(88% = 1584 samples).\n* If habitat is path(p), mushroom are mostly poisonous(88%= 1008 samples).","a29059df":"**LOGISTIC REGRESSION CLASSIFICATION**","295d3b54":"After that time, my works will be easier..","6961c82d":"I don't need to use columns, dtypes, isnull and shape methods, because info() method already contains all of them.","b51dd45f":"**Decision Tree Classification With K Fold Cross Validation **","1d728cdc":"**Random Forest Classification With K Fold Cross Validation**","2483ca9d":"> from sklearn.model_selection import GridSearchCV\n\n> grid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}  # l1 = lasso ve l2 = ridge\n\n> lr = LogisticRegression()\n\n> lr_cv = GridSearchCV(lr,grid,cv = 10)\n\n> lr_cv.fit(x_train,y_train)\n\n> print(\"tuned hyperparameters: (best parameters): \",lr_cv.best_params_)\n\n> print(\"accuracy: \",lr_cv.best_score_)\n","8c7ecb3b":"**Converting Object Types Into Integer Types and Label Encoder**","a3f6d23e":"from sklearn.model_selection import GridSearchCV\n\ngrid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n              \nsvm = SVC()\n\nsvm_cv = GridSearchCV(svm,grid,cv = 10)\n\nsvm_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameters: (best parameters): \",svm_cv.best_params_)\n\nprint(\"accuracy: \",svm_cv.best_score_)","eca9f689":"**PCA (PRINCIPAL COMPONENT ANALYSIS)**","4450c0e3":"Rate of edible mushrooms are much more than ratio of poisonous by 3.6% percent difference(equal to 292 sample).","b45d6805":"* And I noticed that all mushrooms have the same veil-type which is 'p'(partial), none of them are 'u' (universal).So I can drop this feature, it is unnecessary.\n* axis=0 represents rows and axis=1 represents columns.","dc43bcfd":"Let's look at our data what percentage of mushrooms are poisonous(p) and  what percentage of mushrooms are edible(e).And following visualize them.","59f5e240":"Then I apply C and penalty parameters for logistic regression.","92a47f14":"I copied data to df2, just a precaution.If df gets out of hand, I will use df2.","65bbfc30":"Two comment code cells in below(comment row because I don't want take up a lot of space in my kernel), I learnt counts of values in each features and counts of poisonous and edible mushrooms in each features.And after this code cells, I visualized them to be understandable usually six by six.","5f5af321":"**Naive Bayes With Grid Search Cross Validation and K Fold**","1bf5d483":"**Inferences:**","3736b3a1":"There is overfitting,algorithm memorized my datas.So I will apply K Fold Cross Validation to avoid overfitting. ","ac9a3edd":"**SVM With Grid Search Cross Validation**","9bbeff02":"I need numeric values for thoroughly analyze my data,but all features are in object form.I used LabelEncoder() method to transform non-numerical labels to numerical labels.\n* fit_transform() -> Fit label encoder and return encoded labels","8f6bacbb":"* figsize= magnitude of boxes\n* annot=visibility of numbers on boxes\n* lineWidth=distance between boxes\n* fmt=determine how many number will show in fractional part","caa86cf6":"Hold the df_color feature names in list.","c07d96aa":"* principle component=0.33699013\n* second component=0.16553877\n* I can protect my datas 0.5025289008187602 ratio.","d4358304":"**Confusion Matrix of Random Forest Classification**","648c5320":"**Confusion Matrix of Decision Tree Classification**","0a1d173b":"I create new dataframe that includes only color features in df.","045f5546":"**RANDOM FOREST CLASSIFICATION**","5653014c":"I see that veil-color and gill-attachment are directly proportional(0.90),the correlation is positively correlated.","fbd75eba":"For logistic regression above codes enough.But if I calculate detailed, I write below codes.","0fb8c071":"In spore_print_color values are 0,1,2,3,4,8 and if odor is 5(none), mushrooms are edible.","2c2e51c6":"**Inferences:**","bf2679bb":"Feature names had seperated with hyphen(-),this situation cause some syntax error later,so I changed hyphen with underscore(_) below code line.","1be24abf":"Describe() method is useful now because of the label encoder."}}