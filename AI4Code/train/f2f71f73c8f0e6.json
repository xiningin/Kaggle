{"cell_type":{"c627222f":"code","ecfae554":"code","307c81ee":"code","979d93a9":"code","7ce8833c":"code","10bcbd01":"code","255e32bf":"code","138ca051":"code","b0a25a16":"code","e66fc02c":"code","f764a20b":"code","e212a23c":"code","2e166325":"code","bd5f658e":"code","5dac1665":"code","1327d577":"code","c0dba842":"code","ec9d4650":"code","ab70cafc":"code","5cc2ddf1":"code","394101e6":"code","4dbc5d24":"code","b0c06f35":"code","b124d3f0":"code","645e336a":"code","ef87e81a":"code","183d7a1c":"code","0ab58148":"code","6cb0a475":"code","2c08e5c9":"markdown","f57a0c2c":"markdown","1fd7bbbe":"markdown","a2dfd73c":"markdown","9674759b":"markdown","b8d367ec":"markdown","e5319c31":"markdown","bc7b9086":"markdown","d5d38bd1":"markdown","71aed298":"markdown","507b45fd":"markdown","c0e1ef26":"markdown","fa8563d6":"markdown"},"source":{"c627222f":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ecfae554":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","307c81ee":"train_df.info()","979d93a9":"train_df.head()","7ce8833c":"# check the class distribution for the target label in train_df?\ntrain_df['target'].value_counts()","10bcbd01":"X = train_df['text']\ny = train_df['target']","255e32bf":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=123)\nprint(X_train.shape, y_train.shape, X_val.shape, y_val.shape)","138ca051":"# examine the class distribution in y_train and y_test\nprint(y_train.value_counts(),'\\n', y_val.value_counts())","b0a25a16":"# import and instantiate CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = CountVectorizer(lowercase=True, stop_words='english', token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\;|\\:')\nvect","e66fc02c":"# learn the vocabulary in the training data, then use it to create a document-term matrix\nX_train_dtm = vect.fit_transform(X_train)\n# examine the document-term matrix created from X_train\nX_train_dtm","f764a20b":"# transform the test data using the earlier fitted vocabulary, into a document-term matrix\nX_val_dtm = vect.transform(X_val)\n# examine the document-term matrix from X_test\nX_val_dtm","e212a23c":"# import and instantiate the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=8)\nlogreg","2e166325":"# tune hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid_values = {'C':[0.01, 0.1, 1.0, 3.0, 5.0]}\ngrid_logreg = GridSearchCV(logreg, param_grid=grid_values, scoring='neg_log_loss', cv=5)\ngrid_logreg.fit(X_train_dtm, y_train)\ngrid_logreg.best_params_","bd5f658e":"# set with recommended parameter\nlogreg = LogisticRegression(C=1.0, random_state=8)\n# train the model using X_train_dtm & y_train\nlogreg.fit(X_train_dtm, y_train)","5dac1665":"# make class predictions for X_test_dtm\ny_pred_val = logreg.predict(X_val_dtm)","1327d577":"# compute the accuracy of the predictions\nfrom sklearn import metrics\nmetrics.accuracy_score(y_val, y_pred_val)","c0dba842":"# compute the accuracy of predictions with the training data\ny_pred_train = logreg.predict(X_train_dtm)\nmetrics.accuracy_score(y_train, y_pred_train)","ec9d4650":"# look at the confusion matrix for y_test\nmetrics.confusion_matrix(y_val, y_pred_val)","ab70cafc":"# compute the predicted probabilities for X_test_dtm\ny_pred_prob = logreg.predict_proba(X_val_dtm)\ny_pred_prob[:10]","5cc2ddf1":"# compute the log loss number\nmetrics.log_loss(y_val, y_pred_prob)","394101e6":"# Learn the vocabulary in the entire training data, and create the document-term matrix\nX_dtm = vect.fit_transform(X)\n# Examine the document-term matrix created from X_train\nX_dtm","4dbc5d24":"# Train the Logistic Regression model using X_dtm & y\nlogreg.fit(X_dtm, y)","b0c06f35":"# Compute the accuracy of training data predictions\ny_pred_train = logreg.predict(X_dtm)\nmetrics.accuracy_score(y, y_pred_train)","b124d3f0":"test = test_df['text']\n# transform the test data using the earlier fitted vocabulary, into a document-term matrix\ntest_dtm = vect.transform(test)\n# examine the document-term matrix from X_test\ntest_dtm","645e336a":"# make author (class) predictions for test_dtm\nLR_y_pred = logreg.predict(test_dtm)\nprint(LR_y_pred)","ef87e81a":"# calculate predicted probabilities for test_dtm\nLR_y_pred_prob = logreg.predict_proba(test_dtm)\nLR_y_pred_prob[:10]","183d7a1c":"submission['target'] = LR_y_pred","0ab58148":"submission","6cb0a475":"# Generate submission file in csv format\nsubmission.to_csv('submission.csv', index=False)","2c08e5c9":"# Create submission file","f57a0c2c":"### Thank you for reading this.\n### Please upvote if you find it useful. Cheers!","1fd7bbbe":"# Build and evaluate the disaster tweet classification model using Logistic Regression","a2dfd73c":"* The class distribution looks quite balanced, with about 40% 'disaster' tweets.","9674759b":"# Vectorize the data","b8d367ec":"# Define X and y from train data for use in tokenization by Vectorizers","e5319c31":"# Train the Logistic Regression model with the entire dataset from \"train.csv\"","bc7b9086":"##### Comments:\n* In this kernel, I use logistic regression as the binary classifier.\n* I shall start with the twitter text only. My purpose is to create a baseline model. Further on, I will explore adding other features, and using other models to see how much improvement can be made to the classification task.\n* To process the text data, I will simply use Count Vectorizer.","d5d38bd1":"# Disasters, real or fake?\n# Logistic regression baseline\nStarted on 16 Jan 2020","71aed298":"# Make predictions on the test data and compute the probabilities for submission","507b45fd":"# Examine the train data","c0e1ef26":"# Read \"train.csv\" and \"test.csv into pandas","fa8563d6":"# Split train data into a training and a validation set"}}