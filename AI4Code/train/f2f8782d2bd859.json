{"cell_type":{"9ac163e5":"code","121ab35b":"code","df8ddd4f":"code","db2994d5":"code","c758f683":"code","b1c64a3a":"code","9a154b10":"code","14a7987e":"code","e0a3747f":"code","dc6b31f3":"markdown","4233fa41":"markdown","486cc6f4":"markdown"},"source":{"9ac163e5":"!pip install -U sitq","121ab35b":"# Load ml-20m dataset\n\nfrom sklearn.model_selection import train_test_split\n\ndef iter_data(path='..\/input\/rating.csv'):\n    with open(path, 'rt') as f:\n        for line in f:\n            (user_id, movie_id, _, _) = line.split(',')\n            yield (user_id, movie_id)\n        \nuids_train, uids_test, iids_train, iids_test = \\\n    train_test_split(*zip(*iter_data()), test_size=0.2, random_state=0)","df8ddd4f":"# Train recommender model\n\nimport implicit\nimport numpy as np\nfrom scipy import sparse\n\nuid_idxs = {uid: idx for idx, uid in enumerate(set(uids_train))}\niid_idxs = {iid: idx for idx, iid in enumerate(set(iids_train))}\n\ndef train_als():\n    X = sparse.coo_matrix((np.ones(len(uids_train)), \n                           ([iid_idxs[iid] for iid in iids_train],\n                            [uid_idxs[uid] for uid in uids_train])),\n                          shape=(len(iid_idxs), len(uid_idxs))\n                         ).tocsr()\n    model = implicit.als.AlternatingLeastSquares(factors=16)\n    model.fit(X)\n    return model, X\n\nals, X_train = train_als()","db2994d5":"# Stats\n\nprint('Number of train data: {}'.format(len(uids_train)))\nprint('Number of test data: {}'.format(len(uids_test)))\nprint('')\nprint('Number of trained users: {}'.format(len(als.user_factors)))\nprint('Number of trained items: {}'.format(len(als.item_factors)))","c758f683":"# Define functions for evaluation\n\nfrom collections import defaultdict\nfrom functools import lru_cache, partial\nfrom timeit import default_timer as timer\n\ndef _get_precision(recommended_items, true_items):\n    return len(np.intersect1d(recommended_items, true_items)) \/ len(recommended_items)\n\ndef _get_recall(recommended_items, true_items):\n    return len(np.intersect1d(recommended_items, true_items)) \/ len(true_items)\n\n@lru_cache()\ndef _get_idcg(topn):\n    return sum(1 \/ np.log2(i + 2) for i in range(topn))\n\ndef _get_ndcg(recommended_items, true_items):\n    dcg = sum(1 \/ np.log2(i + 2)\n              for i, item in enumerate(recommended_items) if item in true_items)\n    return dcg \/ _get_idcg(len(true_items))\n\ndef _get_recommendations(recommender):\n    recommendations_by_user = []\n    for user_vector, train_items in zip(als.user_factors, X_train.T):\n        train_item_idxs = train_items.nonzero()[1]\n        recommendations_by_user.append(recommender(user_vector, excluded_idxs=train_item_idxs))\n    return recommendations_by_user\n    \ndef evaluate(recommender):\n    start = timer()\n    recommendations_by_user = _get_recommendations(recommender)\n    end = timer()\n    print('Time to recommend: {:.1f} sec'.format(end - start))\n    \n    iid_idxs_by_uid_idx = defaultdict(list)\n    for (uid, iid) in zip(uids_test, iids_test):\n        uid_idx = uid_idxs.get(uid)\n        iid_idx = iid_idxs.get(iid)\n        if uid_idx is None or iid_idx is None:\n            continue\n        iid_idxs_by_uid_idx[uid_idx].append(iid_idx)\n        \n    precision = recall = ndcg = 0\n    for (uid_idx, _iid_idxs) in iid_idxs_by_uid_idx.items():\n        recommended_idxs = recommendations_by_user[uid_idx]\n        precision += _get_precision(recommended_idxs, _iid_idxs)\n        recall += _get_recall(recommended_idxs, _iid_idxs)\n        ndcg += _get_ndcg(recommended_idxs, _iid_idxs)\n    print('precision: {:.3f}'.format(precision \/ len(iid_idxs_by_uid_idx)))\n    print('recall: {:.3f}'.format(recall \/ len(iid_idxs_by_uid_idx)))\n    print('ndcg: {:.3f}'.format(ndcg \/ len(iid_idxs_by_uid_idx)))","b1c64a3a":"# brute-force\n\ndef recommend_by_als(query_vector, excluded_idxs, limit):\n    _limit = limit + len(excluded_idxs)\n    scores = als.item_factors.dot(query_vector)\n    item_idxs = np.argpartition(scores, -_limit)[-_limit:]\n    item_idxs = item_idxs[np.argsort(scores[item_idxs])[::-1]]\n    return np.setdiff1d(item_idxs, excluded_idxs, assume_unique=True)[:limit]\n\ndef evaluate_als():\n    evaluate(partial(recommend_by_als, limit=10))","9a154b10":"evaluate_als()","14a7987e":"# SITQ\n\nfrom sitq import Mips\n\ndef recommend_by_sitq(mips, query_vector, excluded_idxs, limit, distance):\n    item_idxs, _ = mips.search(query_vector, \n                               limit=(limit + len(excluded_idxs)),\n                               distance=distance,\n                               require_items=True,\n                               sort=True)\n    return np.setdiff1d(item_idxs, excluded_idxs, assume_unique=True)[:limit]\n\ndef evaluate_sitq(signature_size, distance):\n    start = timer()\n    mips = Mips(signature_size=signature_size)\n    mips.fit(als.item_factors)\n    end = timer()\n    print('Time to train SITQ: {:.3f} sec'.format(end - start))\n    evaluate(partial(recommend_by_sitq, mips, limit=10, distance=distance))","e0a3747f":"evaluate_sitq(signature_size=8, distance=0)","dc6b31f3":"Benchmark\n----------------","4233fa41":"Preparation\n----------------","486cc6f4":"SITQ: Learning to Hash for Maximum Inner Product Search\n=============================================\n\nSITQ is a fast algorithm for approximate Maximum Inner Product Search (MIPS). It can find items which are likely to maximize inner product against a query in sublinear time.\n\nAbout package\n---------------------\n\n`sitq` package provides two classes: `Sitq` and `Mips`. \n\n`Sitq` is the core of this package. It offers methods which convert a vector into a signature. \nSignature is binary array with arbitrary number of bits. `Sitq` needs to train parameters with all items In order to calculate signatures for items and queries. \nThe important property of SITQ is: items whose signatures are similar to query signature are likely to maximize inner product among all items.\n\n`Mips` offers methods for searching items which are likely to maximize inner product against an arbitrary query.\nIt uses `Sitq` internally for fast approximate search.\n\nMore information is available at https:\/\/github.com\/shiroyagicorp\/sitq\n\nAbout this notebook\n----------------------------\n\nThis notebook compares performance of SITQ and brute-force for recommendation using `MovieLens 20M Dataset`.\n"}}