{"cell_type":{"a01ee0bf":"code","828962c1":"code","f369c8ef":"code","74723284":"code","07d3debf":"code","8c21c20d":"code","b2f37176":"code","9d0336c1":"code","2f9052c4":"code","051046b6":"code","97f2b6d3":"code","d5655330":"code","d0a1edf2":"code","9e8523bc":"code","ce5a4580":"code","a058da84":"code","e4d228bd":"code","6d77bcb5":"code","8cc6d47d":"code","0ceafc03":"code","cce0f0cb":"code","d2e94f2b":"code","745b07b1":"code","51777aa5":"markdown","85d2539d":"markdown","d8db5e61":"markdown","8ab21228":"markdown","7d3242e4":"markdown","21b30629":"markdown","d36831da":"markdown","9e1019e8":"markdown","4be245bb":"markdown","06e165aa":"markdown","a1c7e4a0":"markdown","ad775329":"markdown"},"source":{"a01ee0bf":"import ast\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\npd.options.display.max_colwidth = 200","828962c1":"DATA_PATH = \"\/kaggle\/input\/nbme-score-clinical-patient-notes\/\"\n\npatient_notes = pd.read_csv(DATA_PATH + \"patient_notes.csv\")\nfeatures = pd.read_csv(DATA_PATH + \"features.csv\")\ndf_train = pd.read_csv(DATA_PATH + \"train.csv\")","f369c8ef":"df_train['annotation'] = df_train['annotation'].apply(ast.literal_eval)\ndf_train['location'] = df_train['location'].apply(ast.literal_eval)\n\ndf_train = df_train.merge(features, how=\"left\", on=[\"case_num\", \"feature_num\"])\n\ndf_train.head()","74723284":"df_train_grouped = df_train.groupby(['case_num', 'pn_num']).agg(list)\npatient_notes = patient_notes.merge(df_train_grouped, how=\"left\", on=['case_num', 'pn_num'])\n\npatient_notes = patient_notes.dropna(axis=0).reset_index(drop=True)\npatient_notes = patient_notes[['case_num', 'pn_num', 'pn_history', 'annotation', 'location', 'feature_text', 'feature_num']]","07d3debf":"from sklearn.model_selection import StratifiedKFold\n\nK = 5\nSEED = 2222\n\nskf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)\nsplits = list(skf.split(X=patient_notes, y=patient_notes['case_num']))","8c21c20d":"folds = np.zeros(len(patient_notes), dtype=int)\nfor i, (train_idx, val_idx) in enumerate(splits):\n    folds[val_idx] = i\n    df_val = patient_notes.iloc[val_idx]\n    print(f'   -> Fold {i}')\n    print('- Number of samples :', len(df_val))\n    print('- Case repartition :', dict(Counter(df_val['case_num'])), '\\n')","b2f37176":"patient_notes['fold'] = folds\npatient_notes[['case_num', 'pn_num', 'fold']].to_csv('folds.csv', index=False)","9d0336c1":"from sklearn.metrics import f1_score\n\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    \n    return f1_score(truths, preds)","2f9052c4":"preds = [[0, 0, 1], [0, 0, 0]]\ntruths = [[0, 0, 1], [1, 0, 0]]\n\nmicro_f1(preds, truths)","051046b6":"def spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    \n    return binary","97f2b6d3":"spans_to_binary([[0, 5], [10, 15]])","d5655330":"def span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n        \n    bin_preds = []\n    bin_truths = []\n    \n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n        \n    return micro_f1(bin_preds, bin_truths)","d0a1edf2":"spans = patient_notes['location'][0]\nspans = [[list(np.array(s.split(' ')).astype(int)) for s in span] for span in spans if len(span)]\n\npred = spans\ntruth = [span[:2] for span in spans]\n\nprint(pred)\nprint(truth)","9e8523bc":"span_micro_f1(pred, truth)","ce5a4580":"def location_to_span(location):\n    spans = []\n    for loc in location:\n        if \";\" in loc:\n            loc = loc.split(';')\n        else:\n            loc = [loc]\n        \n        for l in loc:\n            spans.append(list(np.array(l.split(' ')).astype(int)))\n    \n    return spans","a058da84":"df = df_train.copy()\npatient_notes = pd.read_csv(DATA_PATH + \"patient_notes.csv\")\n\ndf = df.merge(patient_notes, how=\"left\")","e4d228bd":"df_folds = pd.read_csv('folds.csv')\ndf = df.merge(df_folds, how=\"left\", on=[\"case_num\", \"pn_num\"])","6d77bcb5":"df['span'] = df['location'].apply(location_to_span)","8cc6d47d":"for fold in range(K):\n    \n    print(f\"\\n-------------   Fold {fold + 1} \/ {K}  -------------\\n\")\n    \n    df_train = df[df['fold'] != fold].reset_index(drop=True)\n    df_val = df[df['fold'] == fold].reset_index(drop=True)\n    \n    matching_dict = df_train[['case_num', 'feature_num', 'annotation']].groupby(['case_num', 'feature_num']).agg(list).T.to_dict()\n    matching_dict = {k: np.concatenate(v['annotation']) for k, v in matching_dict.items()}\n    matching_dict = {k: np.unique([v_.lower() for v_ in v]) for k, v in matching_dict.items()}\n    \n    preds = []\n    for i in range(len(df_val)):\n        \n        key = (df_val['case_num'][i], df_val['feature_num'][i])\n#         print(key)\n        \n        candidates = matching_dict[key]\n        \n        text = df_val['pn_history'][i].lower()\n        \n        spans = []\n        for c in candidates:\n            start = text.find(c)\n            if start > -1:\n                spans.append([start, start + len(c)])\n        \n        preds.append(spans)\n        \n    score = span_micro_f1(preds, df_val['span'])\n\n    print(f\"-> F1 score: {score :.3f}\")\n    \n#     break","0ceafc03":"df_test = pd.read_csv(DATA_PATH + \"test.csv\")\ndf_test = df_test.merge(patient_notes, how=\"left\")","cce0f0cb":"df_train = df.copy()\n\nmatching_dict = df_train[['case_num', 'feature_num', 'annotation']].groupby(['case_num', 'feature_num']).agg(list).T.to_dict()\nmatching_dict = {k: np.concatenate(v['annotation']) for k, v in matching_dict.items()}\nmatching_dict = {k: np.unique([v_.lower() for v_ in v]) for k, v in matching_dict.items()}\n\npreds = []\nfor i in range(len(df_test)):\n    key = (df_test['case_num'][i], df_test['feature_num'][i])\n\n    candidates = matching_dict[key]\n\n    text = df_test['pn_history'][i].lower()\n\n    spans = []\n    for c in candidates:\n        start = text.find(c)\n        if start > -1:\n            spans.append([start, start + len(c)])\n    preds.append(spans)","d2e94f2b":"def preds_to_location(preds):\n    locations = []\n    for pred in preds:\n        loc = \";\".join([\" \".join(np.array(p).astype(str)) for p in pred])\n        locations.append(loc)\n    return locations","745b07b1":"sub = pd.read_csv(DATA_PATH + 'sample_submission.csv')\nsub['location'] = preds_to_location(preds)\n\nsub.to_csv('submission.csv', index=False)\nsub","51777aa5":"# Intro\n\nThis notebook setups a naive solution for the problem, which achieves LB 0.568.\n\nIt consists of three main parts :\n- Creating folds\n- Building the evaluation metric\n- Training & evaluation the baseline\n\n\nUpdates :\n- v5 : lower strings before matching","85d2539d":"# Data","d8db5e61":"# Baseline\n\nWe basically perform string matching on all the data.","8ab21228":"*Thanks for reading !*","7d3242e4":"### Evaluation","21b30629":"# Metric\n\nFrom the [evaluation page](https:\/\/www.kaggle.com\/c\/nbme-score-clinical-patient-notes\/overview\/evaluation) :\n- This competition is evaluated by a micro-averaged F1 score.\n- We score each character index as:\n - TP if it is within both a ground-truth and a prediction,\n - FN if it is within a ground-truth but not a prediction, and,\n - FP if it is within a prediction but not a ground truth.\n- Finally, we compute an overall F1 score from the TPs, FNs, and FPs aggregated across all instances.","d36831da":"Now we need to convert predicted spans to binary arrays indcating whether each character is predicted of not.","9e1019e8":"### Inference","4be245bb":"### Submission","06e165aa":"We generate spans from a train example.","a1c7e4a0":"# Folds\nThere are two possibilities that come to my mind for splitting the data : \n- A k-fold on features stratified by `case_num`\n- A k-fold on features grouped by `case_num`\n\nFrom my understanding, clinical cases will be the same in the train and test data, hence I'm going with the first option.","ad775329":"### Preparation"}}