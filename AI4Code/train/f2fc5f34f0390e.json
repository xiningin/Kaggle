{"cell_type":{"787a4de2":"code","7d3208d2":"code","f3bc897d":"code","792030d3":"code","f81a4c88":"code","c368aa46":"code","048f97b9":"code","eafb77a2":"code","ffc60f6e":"code","86e3b93a":"code","36a99f26":"code","7b708587":"code","ef981855":"code","147f42b5":"code","537d2028":"code","9fe89da7":"code","37c38a30":"code","873b09e4":"code","e77693cf":"code","95c95165":"code","5b0e6b62":"code","653ad4b6":"code","a43888b9":"code","3585a069":"code","18d0bd7f":"code","2efbc9a0":"code","5c92586b":"code","55a10a30":"code","e532b31e":"code","53d7f3c8":"code","ef18b581":"code","f5443eb2":"code","bed95569":"code","e12f59ae":"code","8ad85c14":"code","99d5ca16":"code","dea99ced":"code","81003c38":"code","e478d969":"code","a14f6b1c":"code","a9599dd4":"code","f65dbb3c":"code","64940366":"code","d2bb4eb3":"code","e1cae616":"code","fbe917f8":"code","0aa10bcf":"code","9bea4fcb":"code","114dfe8d":"code","0f94b2c8":"code","e06822be":"code","0749f99f":"code","98b2dea6":"code","7c423474":"code","933d2a20":"code","d1f39534":"code","e65abbb7":"code","ad49ce21":"code","83ececb5":"code","439a7839":"code","8967bffd":"code","3e8e28b1":"code","c0f98bcb":"code","260872dd":"code","9abbc17e":"code","e4d27ffe":"code","792b1767":"code","f847fc8f":"code","4eecbddd":"code","1aa26015":"code","b84bb3dc":"code","0fc1ca85":"code","85b212cc":"code","892ea4e4":"code","5be5b499":"code","57b415c5":"code","5e764cc5":"code","45d82e19":"code","0713a1ac":"code","e8b966bc":"code","e32d6d5e":"code","dceff3fe":"code","2cc9c038":"code","0d621238":"code","417c1f19":"code","53524188":"code","c9140d9b":"code","90c129b5":"code","3a23b903":"code","723f601c":"code","8f777c3e":"code","05a5feea":"markdown","3b73f5ee":"markdown","eb186806":"markdown","da1cfc6d":"markdown","90956ee4":"markdown","3d2a96e6":"markdown","cf7f0f46":"markdown","c9f45029":"markdown","1dfbc372":"markdown","a15df888":"markdown","23fbd6c3":"markdown","8da456cf":"markdown","a6c6a991":"markdown","3faa7d94":"markdown","8121fa6e":"markdown","233b4a61":"markdown","26622403":"markdown","83a737f8":"markdown","ed4abff8":"markdown","01654345":"markdown","36bb0298":"markdown","c5de52ce":"markdown","661b565c":"markdown","12036a5b":"markdown","cabf4753":"markdown"},"source":{"787a4de2":"# Data analysis and wrangling\nimport pandas as pd\nimport numpy as np","7d3208d2":"# Data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno","f3bc897d":"# Dates\nimport datetime\nfrom matplotlib.dates import DateFormatter","792030d3":"# Text analysis\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist as fdist\nimport re","f81a4c88":"# Statistical analysis\nfrom scipy.stats import ttest_ind","c368aa46":"# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')","048f97b9":"customerData = pd.read_csv(\"..\/input\/qvi-purchase-and-transaction\/QVI_purchase_behaviour.csv\")\ntransactionData = pd.read_csv(\"..\/input\/qvi-purchase-and-transaction\/QVI_transaction_data.csv\")","eafb77a2":"transactionData.head()","ffc60f6e":"len(transactionData)","86e3b93a":"transactionData['TXN_ID'].nunique()","36a99f26":"# Look for duplicated TXN_ID\n\ntransactionData[transactionData.duplicated(['TXN_ID'])].head()","7b708587":"# Select the first duplicated TXN_ID\n\ntransactionData.loc[transactionData['TXN_ID'] == 48887, :]","ef981855":"transactionData.info()","147f42b5":"# Missing values in transactionData\n\nmissingno.matrix(transactionData)","537d2028":"# Now let's explore the features in both dataset starting with 'transactionData'\n\nlist(transactionData.columns)","9fe89da7":"transactionData['DATE'].head()","37c38a30":"# Date is not in the right format\n\n# Function that converts Excel integer into yyyy-mm-dd format\ndef xlseriesdate_to_datetime(xlserialdate):\n    excel_anchor = datetime.datetime(1900, 1, 1)\n    if(xlserialdate < 60):\n        delta_in_days = datetime.timedelta(days = (xlserialdate - 1))\n    else:\n        delta_in_days = datetime.timedelta(days = (xlserialdate - 2))\n    converted_date = excel_anchor + delta_in_days\n    return converted_date","873b09e4":"# Apply function to Date feature in transactionData dataset\n\ntransactionData['DATE'] = transactionData['DATE'].apply(xlseriesdate_to_datetime)","e77693cf":"# Check the first 5 rows of the new date feature\n\ntransactionData['DATE'].head()","95c95165":"# Date is now in the right format\n\ntransactionData.head()","5b0e6b62":"# First 5 rows of PROD_NAME\n\ntransactionData['PROD_NAME'].head()","653ad4b6":"# Extract weights out of 'PROD_NAME'\n\ntransactionData['PACK_SIZE'] = transactionData['PROD_NAME'].str.extract(\"(\\d+)\")\ntransactionData['PACK_SIZE'] = pd.to_numeric(transactionData['PACK_SIZE'])\ntransactionData.head()","a43888b9":"# Create text cleaning function for PROD_NAME feature\ndef clean_text(text):\n    text = re.sub('[&\/]', ' ', text) # remove special characters '&' and '\/'\n    text = re.sub('\\d\\w*', ' ', text) # remove product weights\n    return text\n\n# Apply text cleaning function to PROD_NAME column\ntransactionData['PROD_NAME'] = transactionData['PROD_NAME'].apply(clean_text)","3585a069":"# Create one giant string and apply 'word_tokenize' to separate the words\n\ncleanProdName = transactionData['PROD_NAME']\nstring = ''.join(cleanProdName)\nprodWord = word_tokenize(string)","18d0bd7f":"# Apply 'fdist' function which computes the frequency of each token and put it into a dataframe\n\nwordFrequency = fdist(prodWord)\nfreq_df = pd.DataFrame(list(wordFrequency.items()), columns = [\"Word\", \"Frequency\"]).sort_values(by = 'Frequency', ascending = False)","2efbc9a0":"# Let's see the top 5 most frequent words\n\nfreq_df.head()","5c92586b":"# Drop rows with salsa word in PROD_NAME\n\ntransactionData['PROD_NAME'] = transactionData['PROD_NAME'].apply(lambda x: x.lower())\ntransactionData = transactionData[~transactionData['PROD_NAME'].str.contains(\"salsa\")]\ntransactionData['PROD_NAME'] = transactionData['PROD_NAME'].apply(lambda x: x.title())\n","55a10a30":"transactionData.head()","e532b31e":"# Value counts of PROD_QTY\n\ntransactionData['PROD_QTY'].value_counts()","53d7f3c8":"transactionData.loc[transactionData['PROD_QTY'] == 200, :]","ef18b581":"transactionData.loc[transactionData['LYLTY_CARD_NBR'] == 226000, :]","f5443eb2":"transactionData.drop(transactionData.index[transactionData['LYLTY_CARD_NBR'] == 226000], inplace = True)\ncustomerData.drop(customerData.index[customerData['LYLTY_CARD_NBR'] == 226000], inplace = True)","bed95569":"# Make sure it has been dropped \n\ntransactionData.loc[transactionData['LYLTY_CARD_NBR'] == 226000]","e12f59ae":"# Now let's examine the number of transactions over time to see if there are any obvious data issues e.g. missing data\n\ntransactionData['DATE'].nunique()","8ad85c14":"# Look for the missing date \n\npd.date_range(start = '2018-07-01', end = '2019-06-30').difference(transactionData['DATE'])","99d5ca16":"# Create a new dataframe which contains the total sale for each date\n\na = pd.pivot_table(transactionData, values = 'TOT_SALES', index = 'DATE', aggfunc = 'sum')\na.head()","dea99ced":"b = pd.DataFrame(index = pd.date_range(start = '2018-07-01', end = '2019-06-30'))\nb['TOT_SALES'] = 0\nlen(b)","81003c38":"c = a + b\nc.fillna(0, inplace = True)","e478d969":"c.head()","a14f6b1c":"c.index.name = 'Date'\nc.rename(columns = {'TOT_SALES': 'Total Sales'}, inplace = True)\nc.head() ","a9599dd4":"timeline = c.index\ngraph = c['Total Sales']\n\nfig, ax = plt.subplots(figsize = (10, 5))\nax.plot(timeline, graph)\n\ndate_form = DateFormatter(\"%Y-%m\")\nax.xaxis.set_major_formatter(date_form)\nplt.title('Total Sales from July 2018 to June 2019')\nplt.xlabel('Time')\nplt.ylabel('Total Sales')\n\nplt.show()\n","f65dbb3c":"# Confirm the date where sales count equals to zero\n\nc[c['Total Sales'] == 0]","64940366":"# Let's look at the December month only\n\nc_december = c[(c.index < \"2019-01-01\") & (c.index > \"2018-11-30\")]\nc_december.head()","d2bb4eb3":"plt.figure(figsize = (15, 5))\nplt.plot(c_december)\nplt.xlabel('Date')\nplt.ylabel('Total Sales')\nplt.title('Total Sales in December')","e1cae616":"# Reset index\n\nc_december.reset_index(drop = True, inplace = True)\nc_december.head()","fbe917f8":"# Relabel Date\n\nc_december['Date'] = c_december.index + 1\nc_december.head()","0aa10bcf":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'Date', y ='Total Sales', data = c_december)","9bea4fcb":"transactionData['PACK_SIZE'].head()","114dfe8d":"transactionData['PACK_SIZE'].unique()","0f94b2c8":"# Check the distribution of PACK_SIZE\n\nplt.figure(figsize = (10, 5))\nplt.hist(transactionData['PACK_SIZE'])     \nplt.xlabel('Pack Size')\nplt.ylabel('Frequency')\nplt.title('Pack Size Histogram')","e06822be":"# Extract brand name from PROD_NAME and create new column called BRAND\n\npart = transactionData['PROD_NAME'].str.partition()\ntransactionData['BRAND'] = part[0]\ntransactionData.head()","0749f99f":"transactionData['BRAND'].unique()","98b2dea6":"# Rename brand names for consistency\n\ntransactionData['BRAND'].replace('Ncc', 'Natural', inplace = True)\ntransactionData['BRAND'].replace('Ccs', 'CCS', inplace = True)\ntransactionData['BRAND'].replace('Smith', 'Smiths', inplace = True)\ntransactionData['BRAND'].replace(['Grain', 'Grnwves'], 'Grainwaves', inplace = True)\ntransactionData['BRAND'].replace('Dorito', 'Doritos', inplace = True)\ntransactionData['BRAND'].replace('Ww', 'Woolworths', inplace = True)\ntransactionData['BRAND'].replace('Infzns', 'Infuzions', inplace = True)\ntransactionData['BRAND'].replace(['Red', 'Rrd'], 'Red Rock Deli', inplace = True)\ntransactionData['BRAND'].replace('Snbts', 'Sunbites', inplace = True)\n\ntransactionData['BRAND'].unique()","7c423474":"# Which brand had the most sales?\n\ntransactionData.groupby('BRAND').TOT_SALES.sum().sort_values(ascending = False)","933d2a20":"list(customerData.columns)","d1f39534":"customerData.head()","e65abbb7":"# Missing values in customerData\n\nmissingno.matrix(customerData)","ad49ce21":"len(customerData)","83ececb5":"customerData['LYLTY_CARD_NBR'].nunique()","439a7839":"# How many unique lifestages?\n\ncustomerData['LIFESTAGE'].nunique()","8967bffd":"# What are those lifestages?\n\ncustomerData['LIFESTAGE'].unique()","3e8e28b1":"# Value counts for lifestages\n\ncustomerData['LIFESTAGE'].value_counts().sort_values(ascending = False)","c0f98bcb":"sns.countplot(y = customerData['LIFESTAGE'], order = customerData['LIFESTAGE'].value_counts().index)","260872dd":"# How many unique premium customer categories?\n\ncustomerData['PREMIUM_CUSTOMER'].nunique()","9abbc17e":"# Value counts for each premium customer category\n\ncustomerData['PREMIUM_CUSTOMER'].value_counts().sort_values(ascending = False)","e4d27ffe":"plt.figure(figsize = (12, 7))\nsns.countplot(y = customerData['PREMIUM_CUSTOMER'], order = customerData['PREMIUM_CUSTOMER'].value_counts().index)\nplt.xlabel('Number of Customers')\nplt.ylabel('Premium Customer')","792b1767":"# Merge transactionData and customerData together\n\ncombineData = pd.merge(transactionData, customerData)","f847fc8f":"print(\"Transaction data shape: \", transactionData.shape)\nprint(\"Customer data shape: \", customerData.shape)\nprint(\"Combined data shape: \", combineData.shape)","4eecbddd":"# Check for null values\n\ncombineData.isnull().sum()","1aa26015":"# Total sales by PREMIUM_CUSTOMER and LIFESTAGE\n\nsales = pd.DataFrame(combineData.groupby(['PREMIUM_CUSTOMER', 'LIFESTAGE']).TOT_SALES.sum())\nsales.rename(columns = {'TOT_SALES': 'Total Sales'}, inplace = True)\nsales.sort_values(by = 'Total Sales', ascending = False, inplace = True)\nsales","b84bb3dc":"# Visualise\n\nsalesPlot = pd.DataFrame(combineData.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER']).TOT_SALES.sum())\nsalesPlot.unstack().plot(kind = 'bar', stacked = True, figsize = (12, 7), title = 'Total Sales by Customer Segment')\nplt.ylabel('Total Sales')\nplt.legend(['Budget', 'Mainstream', 'Premium'], loc = 2)","0fc1ca85":"# Number of customers by PREMIUM_CUSTOMER and LIFESTAGE\n\ncustomers = pd.DataFrame(combineData.groupby(['PREMIUM_CUSTOMER', 'LIFESTAGE']).LYLTY_CARD_NBR.nunique())\ncustomers.rename(columns = {'LYLTY_CARD_NBR': 'Number of Customers'}, inplace = True)\ncustomers.sort_values(by = 'Number of Customers', ascending = False).head(10)","85b212cc":"# Visualise\n\ncustomersPlot = pd.DataFrame(combineData.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER']).LYLTY_CARD_NBR.nunique())\ncustomersPlot.unstack().plot(kind = 'bar', stacked = True, figsize = (12, 7), title = 'Number of Customers by Customer Segment')\nplt.ylabel('Number of Customers')\nplt.legend(['Budget', 'Mainstream', 'Premium'], loc = 2)","892ea4e4":"# Average units per customer by PREMIUM_CUSTOMER and LIFESTAGE\n\navg_units = combineData.groupby(['PREMIUM_CUSTOMER', 'LIFESTAGE']).PROD_QTY.sum() \/ combineData.groupby(['PREMIUM_CUSTOMER', 'LIFESTAGE']).LYLTY_CARD_NBR.nunique()\navg_units = pd.DataFrame(avg_units, columns = {'Average Unit per Customer'})\navg_units.sort_values(by = 'Average Unit per Customer', ascending = False).head()","5be5b499":"# Visualise \n\navgUnitsPlot = pd.DataFrame(combineData.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER']).PROD_QTY.sum() \/ combineData.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER']).LYLTY_CARD_NBR.nunique())\navgUnitsPlot.unstack().plot(kind = 'bar', figsize = (12, 7), title = 'Average Unit by Customer Segment')\nplt.ylabel('Average Number of Units')\nplt.legend(['Budget', 'Mainstream', 'Premium'], loc = 2)","57b415c5":"# Average price per unit by PREMIUM_CUSTOMER and LIFESTAGE\n\navg_price = combineData.groupby(['PREMIUM_CUSTOMER', 'LIFESTAGE']).TOT_SALES.sum() \/ combineData.groupby(['PREMIUM_CUSTOMER', 'LIFESTAGE']).PROD_QTY.sum()\navg_price = pd.DataFrame(avg_price, columns = {'Price per Unit'})\navg_price.sort_values(by = 'Price per Unit', ascending = False).head()","5e764cc5":"# Visualise \n\navgPricePlot = pd.DataFrame(combineData.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER']).TOT_SALES.sum() \/ combineData.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER']).PROD_QTY.sum())\navgPricePlot.unstack().plot(kind = 'bar', figsize = (12, 7), title = 'Average Price by Customer Segment', ylim = (0, 6))\nplt.ylabel('Average Price')\nplt.legend(['Budget', 'Mainstream', 'Premium'], loc = 2)","45d82e19":"# Perform an independent t-test between mainstream vs non-mainstream midage and young singles\/couples to test this difference\n\n# Create a new dataframe pricePerUnit\npricePerUnit = combineData\n\n# Create a new column under pricePerUnit called PRICE\npricePerUnit['PRICE'] = pricePerUnit['TOT_SALES'] \/ pricePerUnit['PROD_QTY']\n\n# Let's have a look\npricePerUnit.head()","0713a1ac":"# Let's group our data into mainstream and non-mainstream\n\nmainstream = pricePerUnit.loc[(pricePerUnit['PREMIUM_CUSTOMER'] == 'Mainstream') & ( (pricePerUnit['LIFESTAGE'] == 'YOUNG SINGLES\/COUPLES') | (pricePerUnit['LIFESTAGE'] == 'MIDAGE SINGLES\/COUPLES') ), 'PRICE']\nnonMainstream = pricePerUnit.loc[(pricePerUnit['PREMIUM_CUSTOMER'] != 'Mainstream') & ( (pricePerUnit['LIFESTAGE'] == 'YOUNG SINGLES\/COUPLES') | (pricePerUnit['LIFESTAGE'] == 'MIDAGE SINGLES\/COUPLES') ), 'PRICE']","e8b966bc":"# Compare histograms of mainstream and non-mainstream customers\n\nplt.figure(figsize = (10, 5))\nplt.hist(mainstream, label = 'Mainstream')\nplt.hist(nonMainstream, label = 'Premium & Budget')\nplt.legend()\nplt.xlabel('Price per Unit')","e32d6d5e":"print(\"Mainstream average price per unit: ${:.2f}\".format(np.mean(mainstream)))\nprint(\"Non-mainstream average price per unit: ${:.2f}\".format(np.mean(nonMainstream)))\nif np.mean(mainstream) > np.mean(nonMainstream):\n    print(\"Mainstream customers have higher average price per unit. \")\nelse:\n    print(\"Non-mainstream customers have a higher average price per unit. \")","dceff3fe":"# Perform t-test \n\nttest_ind(mainstream, nonMainstream)","2cc9c038":"target = combineData.loc[(combineData['LIFESTAGE'] == 'YOUNG SINGLES\/COUPLES') & (combineData['PREMIUM_CUSTOMER'] == 'Mainstream'), :]\nnonTarget = combineData.loc[(combineData['LIFESTAGE'] != 'YOUNG SINGLES\/COUPLES' ) & (combineData['PREMIUM_CUSTOMER'] != 'Mainstream'), :]\ntarget.head()","0d621238":"# Target Segment\ntargetBrand = target.loc[:, ['BRAND', 'PROD_QTY']]\ntargetSum = targetBrand['PROD_QTY'].sum()\ntargetBrand['Target Brand Affinity'] = targetBrand['PROD_QTY'] \/ targetSum\ntargetBrand = pd.DataFrame(targetBrand.groupby('BRAND')['Target Brand Affinity'].sum())\n\n# Non-target segment\nnonTargetBrand = nonTarget.loc[:, ['BRAND', 'PROD_QTY']]\nnonTargetSum = nonTargetBrand['PROD_QTY'].sum()\nnonTargetBrand['Non-Target Brand Affinity'] = nonTargetBrand['PROD_QTY'] \/ nonTargetSum\nnonTargetBrand = pd.DataFrame(nonTargetBrand.groupby('BRAND')['Non-Target Brand Affinity'].sum())","417c1f19":"# Merge the two dataframes together\n\nbrand_proportions = pd.merge(targetBrand, nonTargetBrand, left_index = True, right_index = True)\nbrand_proportions.head()","53524188":"brand_proportions['Affinity to Brand'] = brand_proportions['Target Brand Affinity'] \/ brand_proportions['Non-Target Brand Affinity']\nbrand_proportions.sort_values(by = 'Affinity to Brand', ascending = False)","c9140d9b":"# Target segment \ntargetSize = target.loc[:, ['PACK_SIZE', 'PROD_QTY']]\ntargetSum = targetSize['PROD_QTY'].sum()\ntargetSize['Target Pack Affinity'] = targetSize['PROD_QTY'] \/ targetSum\ntargetSize = pd.DataFrame(targetSize.groupby('PACK_SIZE')['Target Pack Affinity'].sum())\n\n# Non-target segment\nnonTargetSize = nonTarget.loc[:, ['PACK_SIZE', 'PROD_QTY']]\nnonTargetSum = nonTargetSize['PROD_QTY'].sum()\nnonTargetSize['Non-Target Pack Affinity'] = nonTargetSize['PROD_QTY'] \/ nonTargetSum\nnonTargetSize = pd.DataFrame(nonTargetSize.groupby('PACK_SIZE')['Non-Target Pack Affinity'].sum())","90c129b5":"# Merge the two dataframes together\n\npack_proportions = pd.merge(targetSize, nonTargetSize, left_index = True, right_index = True)\npack_proportions.head()","3a23b903":"pack_proportions['Affinity to Pack'] = pack_proportions['Target Pack Affinity'] \/ pack_proportions['Non-Target Pack Affinity']\npack_proportions.sort_values(by = 'Affinity to Pack', ascending = False)","723f601c":"# Which brand offers 270g pack size?\n\ncombineData.loc[combineData['PACK_SIZE'] == 270, :].head(10)","8f777c3e":"# Is Twisties the only brand who sells 270g pack size?\n\ncombineData.loc[combineData['PACK_SIZE'] == 270, 'BRAND'].unique()","05a5feea":"## Affinity to brand","3b73f5ee":"Since the number of rows in customerData is equal to number of unique loyalty card number, we conclude that loyalty card numbers are unique to each row.","eb186806":"## Affinity to pack size","da1cfc6d":"# Conclusion\n\n- Sales are highest for (Budget, OLDER FAMILIES), (Mainstream, YOUNG SINGLES\/COUPLES) and (Mainstream, RETIREES)\n- We found that (Mainstream, YOUNG SINGLES\/COUPLES) and (Mainstream, RETIREES) are mainly due to the fact that there are more customers in these segments\n- (Mainstream, YOUNG SINGLES\/COUPLES) are more likely to pay more per packet of chips than their premium and budget counterparts\n- They are also more likely to purchase 'Tyrrells' and '270g' pack sizes than the rest of the population","90956ee4":"# Transaction data","3d2a96e6":"We have found quite a few interesting insights that we can dive deeper into. For example, we might want to target customers segments that contribute the most to sales to retain them to further increase sales. Let's examine mainstream young singles\/couples against the rest of the cutomer segments to see if they prefer any particular brand of chips.","cf7f0f46":"Mainstream customers have higher average price per unit than that of non-mainstream customers.","c9f45029":"Mainstream young singles\/couples are more likely to purchase Tyrrells chips compared to other brands.","1dfbc372":"Twisties is the only brand that offers 270g pack size.","a15df888":"\nOlder families and young families buy more chips per customer.","23fbd6c3":"# Libraries","8da456cf":"# Customer Data","a6c6a991":"We have a missing date on Christmas Day. This makes sense because most retail stores are closed that day. ","3faa7d94":"We have two occurences of 200 in the dataset. This seems odd so let's explore further.","8121fa6e":"TXN_ID is not unique to each row. This is because there can be sales of chips with different brands in a single transaction.","233b4a61":"# Import and read data","26622403":"This customer has only made two transactions over the entire year so unlikely to be a retail customer. He\/she is most likely purchasing for commercial purposes so it is safe for us to drop these this customer from both 'transactionData' and 'customerData' dataset.","83a737f8":"It looks like mainstream singles\/couples are more likely to purchase a 270g pack size compared to other pack sizes.","ed4abff8":"\nMainstream midage and young singles and couples are more willing to pay more per packet of chips compared to their budget and premium counterparts. This may be due to premium shoppers being more likely to buy healthy snacks and when they do buy chips, it is mainly for entertainment purposes rather than their own consumption. This is also supported by there being fewer premium midage and young singles and couples buying chips compared to their mainstream counterparts.","01654345":"\nWe can see that sales spike up during the December month and zero sale on Christmas Day.","36bb0298":"\nTop 3 sales come from budget older families, mainstream young singles\/couples and mainstream retirees.","c5de52ce":"Both these transactions have been made by the same person at the same store. Let's see all the transactions this person has made by tracking his loyalty card number.","661b565c":"Now, we move on to PACK_SIZE that we created at the beginning by extracting the weight from the PROD_NAME column.","12036a5b":"\nThere are more mainstream young singles\/couples and retirees. This contributes to to more chips sales in these segments however this is not the major driver for the budget older families segment.","cabf4753":"# Data analysis on customer segments\n\nNow that our data is ready for analysis, we can define some metrics of interest to the client:\n\n- Who spends the most on chips, describing customers by lifestage and how premium their general purchasing behaviour is\n- How many customers are in each segment\n- How many chips are bought per customer by segment\n- What is the average chip price by customer segment\n    "}}