{"cell_type":{"fd752b8e":"code","a62bddc4":"code","afbff4e2":"code","5fa624a2":"code","af1fce72":"code","c9ce2cb5":"code","16675178":"code","f7f18d21":"code","240d9402":"code","f1348bf8":"code","870153e4":"code","3acacc62":"code","23b25419":"code","b398b90e":"code","8104a2fa":"code","2e4bd7b0":"code","4f02c9c5":"code","17a18dfa":"code","05ecae3e":"code","9f953f66":"code","fc0295d5":"code","e958c507":"code","a8754db3":"code","1c52f444":"code","2d926b2b":"code","220e708b":"code","234d792f":"code","e6c5fea8":"code","1bb1e4a4":"code","c97e0446":"code","b3481196":"code","ae99f851":"code","abee6a67":"code","28ecd32f":"code","fdd50b4c":"code","7d300910":"code","b1c82c8e":"code","63960ee5":"code","87704a46":"code","e9c57db8":"code","65f88d01":"code","23094c29":"code","6cae2f0a":"code","1de8a544":"code","222a5047":"code","629e72f8":"code","f1d8c0d0":"code","431df10a":"code","4d2b79aa":"code","e29f380c":"code","3a3e3dd2":"code","745949f3":"code","124a3f37":"code","613c2bc0":"code","a49f4647":"code","a6527d86":"code","e328039e":"code","ce8279a1":"code","e07852ad":"code","f1f961df":"code","ea93dcdf":"code","22238be4":"code","00242cd5":"code","efbb3867":"code","90591c9b":"code","2e546d65":"code","56ec7b17":"code","ec53eea6":"code","ff1008f7":"code","c2f7b850":"code","bacecedb":"code","15902e94":"code","92340492":"code","18a83010":"code","e9d7f3f3":"code","1f553967":"code","78f84028":"code","310c5df9":"code","d2331133":"code","3e6f9188":"code","429e6af7":"code","ce48c868":"code","6aac0193":"code","23b650cf":"code","8ee04b4c":"code","cb763d3e":"code","536ad144":"code","9f095ad0":"code","6762dba5":"code","7eb7f199":"code","53a8c73c":"code","36889719":"code","b0bdbb06":"code","9a63faed":"code","022cbf14":"code","7c30c5c5":"code","3c3528d3":"code","8021eeef":"code","039179a7":"code","31a1e77b":"code","145ed87d":"code","2f75dbe4":"code","1c06ecd2":"code","438b1c40":"code","c4bdc83f":"code","32a463f7":"code","3f68f5bc":"code","201fd167":"code","2fe624c6":"code","3d6f9f0f":"code","8196bb7d":"code","ceca0623":"code","8eaddfe3":"code","834abc59":"code","925ccb28":"code","405392cb":"code","59c91e0f":"code","fb088b5c":"code","6bffeafc":"code","c4b1fe2e":"code","5cbfa384":"code","253e11a8":"code","94120929":"code","4858490f":"code","efc5de84":"code","d79f2daa":"code","441c4e43":"code","d5b81926":"code","c5c2cf66":"code","5020aa7c":"code","2e632f0a":"markdown","067f9347":"markdown","f90e621e":"markdown","a0fa080d":"markdown","6cc021c3":"markdown","adfa9236":"markdown","efdfb52f":"markdown","681a349e":"markdown","c0479114":"markdown","6ae26c52":"markdown","d556ed50":"markdown","26eab3db":"markdown","abd3a474":"markdown","9bef837a":"markdown","37bd0e3b":"markdown","aa298a95":"markdown","ec89dc88":"markdown","cadc1a4d":"markdown","dd71ec03":"markdown","9bb9258a":"markdown","4211b45e":"markdown","55a5a814":"markdown","4378c0d5":"markdown","a94bb7bb":"markdown","5eb57d44":"markdown","2ccd4e9b":"markdown","13016dc8":"markdown","1f858524":"markdown","b197cd53":"markdown","5346aa58":"markdown","502f329a":"markdown","767c76c4":"markdown","9336d1d4":"markdown","c93efd51":"markdown","66d0291e":"markdown","9af945f0":"markdown","e7b9a4db":"markdown","36f9f7cc":"markdown","3c52700f":"markdown","52a578c5":"markdown","e8267ba9":"markdown","24f4056d":"markdown","fb43a7c9":"markdown","1d04b48d":"markdown","b0888b01":"markdown","c9f09a2e":"markdown","4480a24d":"markdown","8d707b68":"markdown","fa250e39":"markdown","666a352c":"markdown","34457065":"markdown","f4f7f231":"markdown","31399dca":"markdown","c65dc79d":"markdown","a60f9741":"markdown","7dba4dfc":"markdown","fb71d8a3":"markdown","9ece64fa":"markdown","6f99069f":"markdown","005f4055":"markdown","e81f2131":"markdown","0e839c0a":"markdown","84c6db08":"markdown","f0d3aa13":"markdown","0fc8c021":"markdown","012d7124":"markdown","22b96f7e":"markdown","7dc30d1b":"markdown","bd6db7b3":"markdown","e14cea14":"markdown","1ec35680":"markdown","5344822f":"markdown","81ae0a54":"markdown","5ae9d7c2":"markdown","9dc3ac3b":"markdown","555a232c":"markdown","e6bb19e5":"markdown","af773ffe":"markdown","eb63acc5":"markdown","575b0eea":"markdown","9590576b":"markdown","7b7a17d4":"markdown","99c3bd7b":"markdown","3aa72e52":"markdown","d390c860":"markdown","8dcbc00b":"markdown"},"source":{"fd752b8e":"# import all the required libraries and dependencies for dataframe\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom datetime import datetime, timedelta\n\n# import all libraries and dependencies for data visualization\npd.options.display.float_format='{:.4f}'.format\nplt.rcParams['figure.figsize'] = [8,8]\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', -1) \nsns.set(style='darkgrid')\nimport matplotlib.ticker as plticker\n%matplotlib inline\n\n# import all the required libraries and dependencies for machine learning\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import metrics\nimport statsmodels.api as sm\nimport pickle\nimport gc \nfrom sklearn import svm\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve,roc_auc_score, precision_recall_curve, average_precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","a62bddc4":"# Local file path.Please change the path accordingly.\n\npath = '..\/input\/credit-card-fraud\/creditcard.csv'\n","afbff4e2":"# Reading the Credit Card file on which analysis needs to be done\n\ndf_card = pd.read_csv(path)\ndf_card.head()","5fa624a2":"# Shape of the Credit card dataframe\n\ndf_card.shape","af1fce72":"# Data Description\n\ndf_card.describe()","c9ce2cb5":"# Data Information\n\ndf_card.info()","16675178":"# Calculating the Missing Value% in the DF\n\ndf_null = df_card.isnull().mean()*100\ndf_null.sort_values(ascending=False).head()","f7f18d21":"# Datatype check for the dataframe\n\ndf_card.dtypes","240d9402":"plt.figure(figsize=(13,7))\nplt.subplot(121)\nplt.title('Fraudulent BarPlot', fontweight='bold',fontsize=14)\nax = df_card['Class'].value_counts().plot(kind='bar')\ntotal = float(len(df_card))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.5f}'.format(height\/total),\n            ha=\"center\") \n\n\nplt.subplot(122)\ndf_card[\"Class\"].value_counts().plot.pie(autopct = \"%1.5f%%\")\nplt.show()","f1348bf8":"classes=df_card['Class'].value_counts()\nnormal_share=classes[0]\/df_card['Class'].count()*100\nfraud_share=classes[1]\/df_card['Class'].count()*100\nprint(normal_share)\nprint(fraud_share)","870153e4":"# Box Plot of amount for both classes\nplt.figure(figsize = (7, 6))\na=sns.boxplot(x = 'Class', y = 'Amount',hue='Class', data = df_card,showfliers=False) \nplt.setp(a.get_xticklabels(), rotation=45)","3acacc62":"# KDE plot to visualize the distribution of Amount for both the classes\nplt.rcParams['figure.figsize'] = [10,6]\nsns.kdeplot(df_card.loc[df_card['Class'] == 0, 'Amount'], label = 'Non Fraud')\nsns.kdeplot(df_card.loc[df_card['Class'] == 1, 'Amount'], label = 'Fraud')\nplt.title('Distribution of Amount by Target Value')\nplt.xlabel('Amount')\nplt.ylabel('Density')","23b25419":"# Time Distribution plot for transactions \nplt.figure(figsize=(15,7))\n\nplt.title('Distribution of Transaction Time')\nsns.distplot(df_card['Time'].values\/(60*60))","b398b90e":"# Storing Fraud and non-Fraud transactions \n\ndf_nonfraud = df_card[df_card.Class == 0]\ndf_fraud = df_card[df_card.Class == 1]","8104a2fa":"#Scatter plot between Time and Amount\n\nfig = plt.figure(figsize = (8,8))\nplt.scatter(df_nonfraud.Amount, df_nonfraud.Time.values\/(60*60),alpha=0.5,label='Non Fraud')\nplt.scatter(df_fraud.Amount, df_fraud.Time.values\/(60*60),alpha=1,label='Fraud')\nplt.xlabel('Amount')\nplt.ylabel('Time')\nplt.title('Scatter plot between Amount and Time ')\nplt.show()","2e4bd7b0":"# Plot of high value transactions($200-$2000)\n\nbins = np.linspace(200, 2000, 100)\nplt.hist(df_nonfraud.Amount, bins, alpha=1, density=True, label='Non-Fraud')\nplt.hist(df_fraud.Amount, bins, alpha=1, density=True, label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Amount by percentage of transactions (transactions \\$200-$2000)\")\nplt.xlabel(\"Transaction amount (USD)\")\nplt.ylabel(\"Percentage of transactions (%)\")\nplt.show()","4f02c9c5":"# Plot of transactions in 48 hours\n\nbins = np.linspace(0, 48, 48)\nplt.hist((df_nonfraud.Time\/(60*60)), bins, alpha=1,label='Non-Fraud')\nplt.hist((df_fraud.Time\/(60*60)), bins, alpha=0.6,label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Percentage of transactions by hour\")\nplt.xlabel(\"Transaction time from first transaction in the dataset (hours)\")\nplt.ylabel(\"Percentage of transactions (%)\")\nplt.show()","17a18dfa":"# Putting the feature variable into X\n\nX = df_card.drop(['Class'],axis = 1)\nX.head(2)","05ecae3e":"# Putting the Target variable to y\n\ny = df_card['Class']","9f953f66":"from sklearn.model_selection import StratifiedShuffleSplit","fc0295d5":"# Splitting the data into Train and Test set\nkfold = 4\nsss = StratifiedShuffleSplit(n_splits=kfold, test_size=0.3, random_state=9487)\nfor train_index, test_index in sss.split(X, y):\n        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        X_train, X_test = X.iloc, X.iloc\n        y_train, y_test = y[train_index], y[test_index]","e958c507":"# Checking Skewness of data\n\nplt.rcParams['figure.figsize'] = [10,8]\nplt.hist(df_card['Amount'],edgecolor='k',bins = 5)\nplt.title('Transaction Amount')\nplt.xlabel('Amount in USD') \nplt.ylabel('Count')","a8754db3":"from sklearn import preprocessing\nfrom sklearn.preprocessing import PowerTransformer","1c52f444":"pt = preprocessing.PowerTransformer(copy=False)\nPWTR_X = pt.fit_transform(X)","2d926b2b":"# Splitting dataset into test and train sets in 70:30 ratio after applying Power Transform\n\nkfold = 4\nsss = StratifiedShuffleSplit(n_splits=kfold, test_size=0.3, random_state=9487)\nfor train_index, test_index in sss.split(PWTR_X, y):\n        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        X_train, X_test = PWTR_X[train_index], PWTR_X[test_index]\n        y_train, y_test = y[train_index], y[test_index]","220e708b":"from sklearn.linear_model import LogisticRegression\n\n# Fit a logistic regression model to train data\nmodel_lr = LogisticRegression()\nmodel_lr.fit(X_train, y_train)\n","234d792f":"# Predict on test data\ny_predicted = model_lr.predict(X_test)","e6c5fea8":"# Evaluation Metrics\n\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","1bb1e4a4":"# Function for roc_curve\ndef plot_roc_curve(fpr,tpr,roc_auc):\n    plt.plot(fpr, tpr, linewidth=5, label='AUC = %0.3f'% roc_auc)\n    plt.plot([0,1],[0,1], linewidth=5)\n    plt.xlim([-0.01, 1])\n    plt.ylim([0, 1.01])\n    plt.legend(loc='upper right')\n    plt.title('Receiver operating characteristic curve (ROC)')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","c97e0446":"# tpr and fpr\nfpr, tpr, threshold = roc_curve(y_test, y_predicted)\nroc_auc = roc_auc_score(y_test, y_predicted)","b3481196":"# Plotting the roc curve \nplt.rcParams['figure.figsize'] = [6,6]\nplot_roc_curve(fpr,tpr,roc_auc)","ae99f851":"from imblearn.metrics import sensitivity_specificity_support","abee6a67":"# Number of folds\n\nn_folds = 5\n# parameters \nparams ={'C': [0.1, 0.5, 1, 2, 3, 4, 5, 10], 'penalty': ['l1', 'l2']}\n\nlrh = LogisticRegression()\n\nmodel_lrh = GridSearchCV(estimator=lrh, cv=n_folds, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=1)","28ecd32f":"# Fitting the model\n\nmodel_lrh.fit(X_train,y_train)","fdd50b4c":"pd.DataFrame(model_lrh.cv_results_)","7d300910":"print(\"Logistic Regression with PCA Best AUC : \", model_lrh.best_score_)\nprint(\"Logistic Regression with PCA Best hyperparameters: \", model_lrh.best_params_)","b1c82c8e":"# Passing the best parameteres\nmodel_lrh_tuned = LogisticRegression(penalty='l2',C=0.1)","63960ee5":"# Predicting on test data\n\nmodel_lrh_tuned.fit(X_train,y_train)\ny_predicted = model_lrh_tuned.predict(X_test)","87704a46":"#Evaluation Metrices\n\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","e9c57db8":"# Create true and false positive rates\nfpr, tpr, threshold = roc_curve(y_test, y_predicted)\nroc_auc = roc_auc_score(y_test, y_predicted)\n# Plot the roc curve \nplt.rcParams['figure.figsize'] = [6,6]\nplot_roc_curve(fpr,tpr,roc_auc)","65f88d01":"#Initializing Random forest and creating model\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel_rfc = RandomForestClassifier(n_jobs=-1, \n                             random_state=2018,\n                             criterion='gini',\n                             n_estimators=100,\n                             verbose=False)","23094c29":"# Fitting the model on Train data and Predicting on Test data\n\nmodel_rfc.fit(X_train,y_train)\ny_predicted = model_rfc.predict(X_test)","6cae2f0a":"# Evaluation Metrics\n\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","1de8a544":"# Create true and false positive rates\nfpr, tpr, threshold = roc_curve(y_test, y_predicted)\nroc_auc = roc_auc_score(y_test, y_predicted)\n# Plot the roc curve \nplt.rcParams['figure.figsize'] = [6,6]\nplot_roc_curve(fpr,tpr,roc_auc)","222a5047":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV","629e72f8":"# Defining Parameters\nparams = { \n    'n_estimators': [200, 400],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","f1d8c0d0":"# Stratified K Fold\ncross_val = StratifiedKFold(n_splits=3)\nindex_iterator = cross_val.split(X_train, y_train)\nclf = RandomForestClassifier()\nclf_random = RandomizedSearchCV(estimator = clf, param_distributions = params, n_iter = 50, cv = cross_val,\n                                verbose=2, random_state=42, n_jobs = -1,scoring='roc_auc')\n","431df10a":"# Fitting the model on train data\n# clf_random.fit(X_train, y_train)","4d2b79aa":"# Scores of RandomizedSearchCV\n#scores = clf_random.cv_results_\n#pd.DataFrame(scores).head()","e29f380c":"#print(clf_random.best_score_)\n#print(clf_random.best_params_)\n#print(clf_random.best_index_)","3a3e3dd2":"# Passing the best parameteres based on Randomized Search CV\nmodel_rfc_tuned = RandomForestClassifier(bootstrap=True,\n                               class_weight={0:1, 1:12}, # 0: non-fraud , 1:fraud\n                               criterion='gini',\n                               max_depth=5,\n                               max_features='sqrt',\n                               min_samples_leaf=10,\n                               n_estimators=200,\n                               n_jobs=-1, \n                               random_state=5)","745949f3":"# Fitting the model on Train data and Predicting on Test Data\n\nmodel_rfc_tuned.fit(X_train,y_train)\ny_predicted = model_rfc_tuned.predict(X_test)","124a3f37":"# Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","613c2bc0":"# Create true and false positive rates\nfpr, tpr, threshold = roc_curve(y_test, y_predicted)\nroc_auc = roc_auc_score(y_test, y_predicted)\n# Plot the roc curve \nplt.rcParams['figure.figsize'] = [6,6]\nplot_roc_curve(fpr,tpr,roc_auc)","a49f4647":"#Initializing Random forest and creating model\nmodel_xgb = XGBClassifier()","a6527d86":"# Fitting the model on Train data and Predicting on Test data\nmodel_xgb.fit(X_train,y_train)\ny_predicted = model_xgb.predict(X_test)","e328039e":"# Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","ce8279a1":"# Create true and false positive rates\nfpr, tpr, threshold = roc_curve(y_test, y_predicted)\nroc_auc = roc_auc_score(y_test, y_predicted)\n# Plot the roc curve \nplt.rcParams['figure.figsize'] = [6,6]\nplot_roc_curve(fpr,tpr,roc_auc)","e07852ad":"# Defining parameters\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","f1f961df":"# Stratified K Fold\ncross_val = StratifiedKFold(n_splits=5)\nindex_iterator = cross_val.split(X_train, y_train)\n\n\nxgb_cross = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic',\n                    silent=True, nthread=1) \n\n\nxgb_random = RandomizedSearchCV(estimator = xgb_cross, param_distributions = params, n_iter =30 , cv = cross_val,\n                                verbose=2, random_state=42, n_jobs = -1,scoring='roc_auc')","ea93dcdf":"# Fitting the model on train data\n#xgb_random.fit(X_train, y_train)","22238be4":"# scores of RandomizedSearchCV\n#scores = xgb_random.cv_results_\n#pd.DataFrame(scores).head()","00242cd5":"#print(xgb_random.best_score_)\n#print(xgb_random.best_params_)\n#print(xgb_random.best_index_)","efbb3867":"# Passing the best parameteres based on Randomized Search CV\nmodel_xgb_tuned = XGBClassifier(min_child_weight= 5,\n        gamma= 1.5,\n        subsample= 1.0,\n        colsample_bytree= 0.6,\n        max_depth= 5)","90591c9b":"# Fitting the model on Train data and Predicting on Test data\nmodel_xgb_tuned.fit(X_train,y_train)\ny_predicted = model_xgb_tuned.predict(X_test)","2e546d65":"# Evaluation metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","56ec7b17":"# Create true and false positive rates\nfpr, tpr, threshold = roc_curve(y_test, y_predicted)\nroc_auc = roc_auc_score(y_test, y_predicted)\n# Plot the roc curve \nplt.rcParams['figure.figsize'] = [6,6]\nplot_roc_curve(fpr,tpr,roc_auc)","ec53eea6":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import ADASYN\n\n# Resample training data\nros = RandomOverSampler()\nsmote = SMOTE(random_state=5)\nadasyn = ADASYN(random_state=5)\n\nX_train_ros, y_train_ros = ros.fit_sample(X_train,y_train)\nX_train_smote, y_train_smote = smote.fit_sample(X_train,y_train)\nX_train_adasyn, y_train_adasyn =adasyn.fit_sample(X_train,y_train)","ff1008f7":"# Fit a logistic regression model to our data\nfrom sklearn.linear_model import LogisticRegression\n\nmodel_lr = LogisticRegression()\nmodel_lr.fit(X_train_ros, y_train_ros)\n\n# Obtain model predictions\ny_predicted = model_lr.predict(X_test)","c2f7b850":"# Evaluation Metrics\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","bacecedb":"# Create true and false positive rates\nfpr, tpr, threshold = roc_curve(y_test, y_predicted)\nroc_auc = roc_auc_score(y_test, y_predicted)\n# Plot the roc curve \nplt.rcParams['figure.figsize'] = [6,6]\nplot_roc_curve(fpr,tpr,roc_auc)","15902e94":"# Fit a logistic regression model to our data\nfrom sklearn.linear_model import LogisticRegression\n\nmodel_lr = LogisticRegression()\nmodel_lr.fit(X_train_smote, y_train_smote)\n\n# Obtain model predictions\ny_predicted = model_lr.predict(X_test)","92340492":"# Evaluation Metrics\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","18a83010":"# Create true and false positive rates\nfpr, tpr, threshold = roc_curve(y_test, y_predicted)\nroc_auc = roc_auc_score(y_test, y_predicted)\n# Plot the roc curve \nplt.rcParams['figure.figsize'] = [6,6]\nplot_roc_curve(fpr,tpr,roc_auc)","e9d7f3f3":"# Fit a logistic regression model to our data\nfrom sklearn.linear_model import LogisticRegression\n\nmodel_lr = LogisticRegression()\nmodel_lr.fit(X_train_adasyn, y_train_adasyn)\n\n# Obtain model predictions\ny_predicted = model_lr.predict(X_test)","1f553967":"# Evaluation Metrics\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","78f84028":"# Insantiate Model\nmodel_rfc = RandomForestClassifier(bootstrap=True,\n                               class_weight={0:1, 1:12}, # 0: non-fraud , 1:fraud\n                               criterion='entropy',\n                               max_depth=10, # Change depth of model\n                               min_samples_leaf=10, # Change the number of samples in leaf nodes\n                               n_estimators=20, # Change the number of trees to use\n                               n_jobs=-1, \n                               random_state=5)","310c5df9":"# Fit the model on train data and predict on test data \nmodel_rfc.fit(X_train_ros,y_train_ros)\ny_predicted = model_rfc.predict(X_test)","d2331133":"# Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","3e6f9188":"# Fit the model on train data and predict on test data \nmodel_rfc.fit(X_train_smote,y_train_smote)\ny_predicted = model_rfc.predict(X_test)","429e6af7":"# Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","ce48c868":"# Fit the model on train data and predict on test data \nmodel_rfc.fit(X_train_adasyn,y_train_adasyn)\ny_predicted = model_rfc.predict(X_test)","6aac0193":"# Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","23b650cf":"params = { \n    'n_estimators': [200, 400],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","8ee04b4c":"cross_val = StratifiedKFold(n_splits=3)\nindex_iterator = cross_val.split(X_train_ros, y_train_ros)\nclf = RandomForestClassifier()\nclf_random = RandomizedSearchCV(estimator = clf, param_distributions = params, n_iter = 50, cv = cross_val,\n                                verbose=2, random_state=42, n_jobs = -1,scoring='roc_auc')","cb763d3e":"#clf_random.fit(X_train_ros, y_train_ros)","536ad144":"# Scores of RandomizedSearchCV\n#scores = clf_random.cv_results_\n#pd.DataFrame(scores).head()","9f095ad0":"#print(clf_random.best_score_)\n#print(clf_random.best_params_)\n#print(clf_random.best_index_)","6762dba5":"# Insanitiate Model on best params\nmodel_rfc_tuned = RandomForestClassifier(bootstrap=True,\n                               class_weight={0:1, 1:12}, \n                               criterion='entropy',\n                               max_depth=8, \n                               max_features='auto',\n                               n_estimators=200,\n                               n_jobs=-1)","7eb7f199":"#Fit the model on train data and predict the model on test data\nmodel_rfc_tuned.fit(X_train_ros,y_train_ros)\ny_predicted = model_rfc_tuned.predict(X_test)","53a8c73c":"# Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","36889719":"model_xgb_ros = XGBClassifier()","b0bdbb06":"#Fit the model on train data and predict the model on test data\nmodel_xgb_ros.fit(X_train_ros,y_train_ros)\ny_predicted = model_xgb_ros.predict(X_test)","9a63faed":"# Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","022cbf14":"# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","7c30c5c5":"cross_val = StratifiedKFold(n_splits=4)\nindex_iterator = cross_val.split(X_train_ros, y_train_ros)\n\n\nxgb_cross = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic',\n                    silent=True, nthread=1) \n\n\nxgb_random = RandomizedSearchCV(estimator = xgb_cross, param_distributions = params, n_iter =30 , cv = cross_val,\n                                verbose=2, random_state=42, n_jobs = -1,scoring='roc_auc')","3c3528d3":"#xgb_random.fit(X_train_ros, y_train_ros)","8021eeef":"# scores of RandomizedSearchCV\n#scores = xgb_random.cv_results_\n#pd.DataFrame(scores).head()","039179a7":"#print(xgb_random.best_score_)\n#print(xgb_random.best_params_)\n#print(xgb_random.best_index_)","31a1e77b":"model_xgb_tuned_ros = XGBClassifier(min_child_weight= 5,\n        gamma= 1.5,\n        subsample= 1.0,\n        colsample_bytree= 0.6,\n        max_depth= 5)","145ed87d":"#Fit the model on train data and predict the model on test data\nmodel_xgb_tuned_ros.fit(X_train_ros,y_train_ros)\ny_predicted = model_xgb_tuned_ros.predict(X_test)","2f75dbe4":"# Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","1c06ecd2":"model_xgb_smote = XGBClassifier()","438b1c40":"#Fit the model on train data and predict the model on test data\nmodel_xgb_smote.fit(X_train_smote,y_train_smote)\ny_predicted = model_xgb_smote.predict(X_test)","c4bdc83f":"# Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","32a463f7":"# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 5, 10,15],\n        'gamma': [0.5, 1, 1.5, 2, 5,8],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0,1.2],\n        'max_depth': [3, 4, 5,6,7]\n        }","3f68f5bc":"cross_val = StratifiedKFold(n_splits=5)\nindex_iterator = cross_val.split(X_train_smote, y_train_smote)\n\n\nxgb_cross = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic',\n                    silent=True, nthread=1) \n\n\nxgb_random = RandomizedSearchCV(estimator = xgb_cross, param_distributions = params, n_iter =40 , cv = cross_val,\n                                verbose=2, random_state=42, n_jobs = -1,scoring='roc_auc')","201fd167":"#xgb_random.fit(X_train_smote, y_train_smote)","2fe624c6":"# scores of RandomizedSearchCV\n#scores = xgb_random.cv_results_\n#pd.DataFrame(scores).head()","3d6f9f0f":"#print(xgb_random.best_score_)\n#print(xgb_random.best_params_)\n#print(xgb_random.best_index_)","8196bb7d":"model_xgb_tuned_smote = XGBClassifier(min_child_weight= 10,\n        gamma= 1.5,\n        subsample= 0.6,\n        colsample_bytree= 0.6,\n        max_depth= 5)","ceca0623":"#Fit the model on train data and predict the model on test data\nmodel_xgb_tuned_smote.fit(X_train_smote,y_train_smote)\ny_predicted = model_xgb_tuned.predict(X_test)","8eaddfe3":"#Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","834abc59":"model_xgb_adasyn = XGBClassifier()","925ccb28":"#Fit the model on train data and predict the model on test data\nmodel_xgb_adasyn.fit(X_train_adasyn,y_train_adasyn)\ny_predicted = model_xgb_adasyn.predict(X_test)","405392cb":"#Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","59c91e0f":"# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","fb088b5c":"cross_val = StratifiedKFold(n_splits=5)\nindex_iterator = cross_val.split(X_train_adasyn, y_train_adasyn)\n\n\nxgb_cross = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic',\n                    silent=True, nthread=1) \n\n\nxgb_random = RandomizedSearchCV(estimator = xgb_cross, param_distributions = params, n_iter =30 , cv = cross_val,\n                                verbose=2, random_state=42, n_jobs = -1,scoring='roc_auc')","6bffeafc":"#xgb_random.fit(X_train_adasyn, y_train_adasyn)","c4b1fe2e":"# scores of RandomizedSearchCV\n#scores = xgb_random.cv_results_\n#pd.DataFrame(scores).head()","5cbfa384":"#print(xgb_random.best_score_)\n#print(xgb_random.best_params_)\n#print(xgb_random.best_index_)","253e11a8":"model_xgb_tuned_adasyn = XGBClassifier(min_child_weight= 10,\n        gamma= 1.5,\n        subsample= 0.6,\n        colsample_bytree= 0.6,\n        max_depth= 5)","94120929":"#Fit the model on train data and predict the model on test data\nmodel_xgb_tuned_adasyn.fit(X_train_adasyn,y_train_adasyn)\ny_predicted = model_xgb_tuned_adasyn.predict(X_test)","4858490f":"#Evaluation Metrices\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))","efc5de84":"#Predicting on the test data using the best model\ny_predicted = model_xgb_smote.predict(X_test)","d79f2daa":"# Create true and false positive rates\nfpr, tpr, thresholds = roc_curve(y_test, y_predicted)\nroc_auc = roc_auc_score(y_test, y_predicted)","441c4e43":"# Printing Evaluation Metrices\nprint('Classification report for XGBoost Smote:\\n', classification_report(y_test, y_predicted))\nprint(\"Logistic Regression Accuracy: \",accuracy_score(y_test,y_predicted))\nprint('ROC AUC : ', roc_auc_score(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\nthreshold = thresholds[np.argmax(tpr-fpr)]\nprint(\"Threshold:\",threshold)","d5b81926":"# Plotting the roc curve \nplt.rcParams['figure.figsize'] = [6,6]\nplot_roc_curve(fpr,tpr,roc_auc)","c5c2cf66":"target = 'Class'\npca_comp = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount', 'Time']","5020aa7c":"tmp = pd.DataFrame({'Feature': pca_comp, 'Feature importance': model_xgb_smote.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()  ","2e632f0a":"## Problem Statement\n\nThe problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.\n\nIn this project, you will analyse customer-level data which has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group. \n\nThe dataset is taken from the Kaggle website and it has a total of 2,84,807 transactions, out of which 492 are fraudulent. Since the dataset is highly imbalanced, so it needs to be handled before model building.","067f9347":"#### Inference:\nHour `zero` corresponds to the hour the first transaction happened and not necessarily `12-1 AM`. Given the heavy decrease in normal transactions from hours `1` to `8` and again roughly at hours `24` to `32`, \nit seems fraud tends to occur at higher rates during the night.","f90e621e":"### Random Forest","a0fa080d":"#### Model 8 : Logistic Regression on SMOTE Balanced Data","6cc021c3":"## Business Problem Overview\n\nFor many banks, retaining high profitable customers is the number one business goal. Banking fraud, however, poses a significant threat to this goal for different banks. In terms of substantial financial losses, trust and credibility, this is a concerning issue to both banks and customers alike.\n\nIt has been estimated by Nilson report that by 2020 the banking frauds would account to $30 billion worldwide. With the rise in digital payment channels, the number of fraudulent transactions is also increasing with new and different ways. \n\nIn the banking industry, credit card fraud detection using machine learning is not just a trend but a necessity for them to put proactive monitoring and fraud prevention mechanisms in place. Machine learning is helping these institutions to reduce time-consuming manual reviews, costly chargebacks and fees, and denials of legitimate transactions.","adfa9236":"#### Model 11 : Random Forest on SMOTE Balanced Data","efdfb52f":"#### Inference:\n\n- The distribution of amount for Fraud transactions is much higher than non-fradualent transactions. ","681a349e":"<a id='6'><\/a><br>\n## Step 6: Final Analysis","c0479114":"#### Model 14 : XGB on ROS Balanced Data","6ae26c52":"<a id='4'><\/a><br>\n## Step 4: Splitting the Data into Training and Testing Sets\n\nAs we know, the first basic step for regression is performing a train-test split.","d556ed50":"### XG Boost","26eab3db":"#### Inference:\n- Precision : 0.54\n- Recall : 0.77\n- F1-score : 0.64\n- Accuracy : 0.99\n- ROC AUC : 0.88","abd3a474":"#### Inference:\n- Precision : 0.06\n- Recall : 0.84\n- F1-score : 0.12\n- Accuracy : 0.97\n- ROC AUC : 0.91","9bef837a":"### Random Forest","37bd0e3b":"#### Inference:\n- Precision : 0.91\n- Recall : 0.76\n- F1-score : 0.83\n- Accuracy : 0.99\n- ROC AUC : 0.87","aa298a95":"#### Inference:\n- Precision : 0.94\n- Recall : 0.70\n- F1-score : 0.80\n- Accuracy : 0.99\n- ROC AUC : 0.85","ec89dc88":"#### Inference:\n- Precision : 0.20\n- Recall : 0.79\n- F1-score : 0.32\n- Accuracy : 0.99\n- ROC AUC : 0.89","cadc1a4d":"#### Inference:\n- Precision : 0.86\n- Recall : 0.60\n- F1-score : 0.70\n- Accuracy : 0.85\n- ROC AUC : 0.80","dd71ec03":"#### Inference:\n- Precision : 0.76\n- Recall : 0.76\n- F1-score : 0.76\n- Accuracy : 0.99\n- ROC AUC : 0.87","9bb9258a":"#### Important Features","4211b45e":"#### Best Model considering various parameters and scenarios","55a5a814":"Model 6 : XGB on Imbalanced Data with K-Fold and Hyperparamater Tuning","4378c0d5":"#### Inference:\n- Precision : 0.95\n- Recall : 0.72\n- F1-score : 0.82\n- Accuracy : 0.99\n- ROC AUC : 0.85","a94bb7bb":"We have build a logistic regression model based on the transaction data provided to us.<br>\nThe data provided to us was very imbalanced data set. Hence, for building a proper logistic model on top of that we have used some balancing techniques like (ROS,SMOTE etc) to balance the data and applied some of very popular logistic regression models\nlike Random Forest, Logistic regression and some boosting techniques like XGBoost to catch any frud transactions.<br>\nIn our scenario Accuracy was not a concerning Evaluation criteria and we focussed more on Recall and AUC.<br>\nWe finally able to build a proper logistic model and predicted on test data and the results were satisfying.<br>\nWe were also able to figure out the variables which will be important in detecting any fraud transactions.","5eb57d44":"#### Inference:\n- Precision : 0.02\n- Recall : 0.91\n- F1-score : 0.04\n- Accuracy : 0.91\n- ROC AUC : 0.91","2ccd4e9b":"## Imbalanced Data Set","13016dc8":"#### Inference:\n- Precision : 0.95\n- Recall : 0.74\n- F1-score : 0.83\n- Accuracy : 0.99\n- ROC AUC : 0.87","1f858524":"#### Inference:\n- Precision : 0.03\n- Recall : 0.84\n- F1-score : 0.06\n- Accuracy : 0.95\n- ROC AUC : 0.89","b197cd53":"In this scenario accuracy score to evaluate our classification algorithm will not be correct.Just using accuracy as the evaluation metric will predit every case as `0` Non Fraud and hence it would be wrong.","5346aa58":"#### Model 17 : Hyper Tuning XGB on SMOTE Balanced Data","502f329a":"#### Inference:\nWe got `284807` records and `31` columns in our dataset.","767c76c4":"### If this Kernel helped you in any way, some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated.","9336d1d4":"#### Model 16 : XGB on SMOTE Balanced Data","c93efd51":"#### Inference:\n- Precision : 0.46\n- Recall : 0.79\n- F1-score : 0.58\n- Accuracy : 0.99\n- ROC AUC : 0.89","66d0291e":"#### Model 2 : Logistic Regression on Imbalanced Data with K-Fold and Hypertuning","9af945f0":"#### Hyperparameter Tuning Random Forest","e7b9a4db":"#### Model 7 : Logistic Regression on ROS Balanced Data","36f9f7cc":"#### Model 3 : Random Forest on Imbalanced Data","3c52700f":"#### Hyperparameter Tuning on Smote Balanced data","52a578c5":"#### Model 12 : Random Forest on ADASYN Balanced Data","e8267ba9":"#### Model 18 : XGB on ADASYN Balanced Data","24f4056d":"<a id='1'><\/a><br>\n## Step 1: Reading and Understanding the Data","fb43a7c9":"#### Model 4 : Random Forest on Imbalanced Data with K-Fold and Hyperparamater Tuning","1d04b48d":"#### Hyperparameter Tuning XGB","b0888b01":"# Capstone Project: Credit Card Fraud Detection","c9f09a2e":"#### Inference:\n- Precision : 0.81\n- Recall : 0.71\n- F1-score : 0.76\n- Accuracy : 0.99\n- ROC AUC : 0.85","4480a24d":"#### Inference:\nWe found out that PCA converted variables like V15, V5 are able to explain the maximum variance and hence we can target these variables to detect a fraud.","8d707b68":"<a id='3'><\/a><br>\n## Step 3 : Data Visualization","fa250e39":"#### Hyperparameter Tuning Random Forest on ROS Data","666a352c":"#### Model 9 : Logistic Regression on ADASYN Balanced Data","34457065":"#### If this Kernel helped you in any way, some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated.","f4f7f231":"#### Inference:\n- Precision : 0.82\n- Recall : 0.76\n- F1-score : 0.79\n- Accuracy : 0.99\n- ROC AUC : 0.88","31399dca":"#### Inference:\n- Precision : 0.06\n- Recall : 0.91\n- F1-score : 0.11\n- Accuracy : 0.97\n- ROC AUC : 0.93","c65dc79d":"Here,instead of `Accuracy` we are very much interested in the recall score, because that is the metric that will help us try to capture the most fraudulent transactions.\nIncrease of Recall comes at a price of Precision. In this case predicting a transaction fradulant which actually is not is not a big concern.","a60f9741":"#### Inference:\n\n- In the long tail, fraud transaction happened more frequently.\n\n- It seems It would be hard to differentiate fraud from normal transactions by transaction amount alone.","7dba4dfc":"<a id='2'><\/a><br>\n## Step 2 : Data Cleansing","fb71d8a3":"#### Visualizing the Distribution of `Fraudulent` Variable","9ece64fa":"#### Hyperparameter Tuning Logisitic Regression","6f99069f":"### Logistic Regression","005f4055":"#### This kernel is based on the assignment by IIITB collaborated with upgrad.","e81f2131":"We observed in the dataset has no null values and Hence, no Null treatment is required.","0e839c0a":"#### Model 5 : XG Boost on Imbalanced Data","84c6db08":"#### Inference:\n- Precision : 0.95\n- Recall : 0.72\n- F1-score : 0.82\n- Accuracy : 0.99\n- ROC AUC : 0.85","f0d3aa13":"#### Inference:\n- Precision : 0.06\n- Recall : 0.91\n- F1-score : 0.11\n- Accuracy : 0.97\n- ROC AUC : 0.94","0fc8c021":"## Project Pipeline\nThe project pipeline can be briefly summarized in the following four steps:\n\n**Data Understanding:** Here, you need to load the data and understand the features present in it. This would help you choose the features that you will need for your final model.<br>\n\n**Exploratory data analytics (EDA):** Normally, in this step, you need to perform univariate and bivariate analyses of the data, followed by feature transformations, if necessary. For the current data set, because Gaussian variables are used, you do not need to perform Z-scaling. However, you can check if there is any skewness in the data and try to mitigate it, as it might cause problems during the model-building phase.<br>\n\n**Train\/Test Split:** Now you are familiar with the train\/test split, which you can perform in order to check the performance of your models with unseen data. Here, for validation, you can use the k-fold cross-validation method. You need to choose an appropriate k value so that the minority class is correctly represented in the test folds.<br>\n\n**Model-Building\/Hyperparameter Tuning:** This is the final step at which you can try different models and fine-tune their hyperparameters until you get the desired level of performance on the given dataset. You should try and see if you get a better model by the various sampling techniques.<br>\n\n**Model Evaluation:** Evaluate the models using appropriate evaluation metrics. Note that since the data is imbalanced it is is more important to identify which are fraudulent transactions accurately than the non-fraudulent. Choose an appropriate evaluation metric which reflects this business goal.","012d7124":"#### Hyperparameter Tuning on ROS Balanced data","22b96f7e":"#### Model 15 : Hyper Tuning XGB on ROS Balanced Data","7dc30d1b":"#### Model 13 : Hyper Tuning  model Random Forest on ROS Balanced Data","bd6db7b3":"### Logistic Regression","e14cea14":"#### Inference:\n- Precision : 0.82\n- Recall : 0.76\n- F1-score : 0.79\n- Accuracy : 0.99\n- ROC AUC : 0.88","1ec35680":"## Balanced Data Set","5344822f":"#### Model 19: Hyperparameter Tuning on Adasyn Balanced data","81ae0a54":"#### Inference:\nNone of the columns have inconsistent datatype.Hence, no conversion is required.","5ae9d7c2":"In nutshell rather than aiming for overall accuracy on the entire dataset, we cared more about detecting most of the fraud cases (recall), whilst keeping the cost at which this is achieved under control (precision).We have applied XGBoost on Smote data and got the best evaluation metrices.","9dc3ac3b":"#### Inference:\n- Precision : 0.92\n- Recall : 0.78\n- F1-score : 0.84\n- Accuracy : 0.99\n- ROC AUC : 0.88","555a232c":"#### Model 1 : Logistic Regression on Imbalanced Data","e6bb19e5":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQiifUVxP8n7i_j5QSAodaQxje4bQboAwKZ2e0u8g87mrV-Nwak&s)","af773ffe":"Let's do a Hyperparameter tuning on ROS data as it gave a better Recall and Precision","eb63acc5":"#### Below are the steps which we will be basically following:\n\n1. [Step 1: Reading and Understanding the Data](#1)\n1. [Step 2: Data Cleansing](#2)\n   - Null Check\n   - Data type check\n1. [Step 3: Data Visualization](#3)\n   - Imbalanced Data check\n   - Data Distribution Plots\n1. [Step 4: Splitting the Data into Training and Testing Sets](#4)\n   - Stratified Train Test Split\n   - Skewness Check and Fix\n1. [Step 5:  Building a Logistic Model](#5)\n   - Logistic Regression on Imbalanced Data\n   - Random Forest on Imbalanced Data\n   - XGBoost on Imbalanced Data\n   - Logistic Regression on Balanced Data (Random Over Sampling,SMOTE,ADASYN)\n   - Random Forest on Balanced Data (Random Over Sampling,SMOTE,ADASYN)\n   - XGBoost on Balanced Data (Random Over Sampling,SMOTE,ADASYN)\n   - Stratified K-Fold and Hyperparameter Tuning\n   - Classification Report\n   - Confusion Matrix\n   - ROC AUC Curve\n1. [Step 6: Final Analysis](#6)\n   - Best Model Selection\n   - Features Importance\n1. [Step 7: Closing Statement](#7)\n","575b0eea":"<a id='7'><\/a><br>\n## Step 7: Closing Statement","9590576b":"![](https:\/\/www.xenonstack.com\/wp-content\/uploads\/xenonstack-credit-card-fraud-detection.png)","7b7a17d4":"#### Inference:\n- Precision : 0.85\n- Recall : 0.59\n- F1-score : 0.70\n- Accuracy : 0.99\n- ROC AUC : 0.79","99c3bd7b":"We need to perform basic cleansing check in order to feed correct data to the model.","3aa72e52":"### XG Boost","d390c860":"<a id='5'><\/a><br>\n## Step 5: Building a Logistic Model","8dcbc00b":"#### Model 10 : Random Forest on ROS Balanced Data"}}