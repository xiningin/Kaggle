{"cell_type":{"4bc683d4":"code","0ae10629":"code","6ad62612":"code","a0507d11":"code","0b836e87":"code","8f36677f":"code","8c237016":"code","9ea33915":"code","bb660606":"code","66bb8bf8":"code","e32ccbc2":"code","e2948e1d":"code","12512efa":"code","b07cbd2b":"code","befe56de":"code","46748b12":"code","1e3f4298":"code","767dbaeb":"code","d71f088a":"code","0ca9b04b":"code","8207036b":"code","c239dd49":"code","2f291347":"code","d28c253d":"code","be7f507f":"code","1c824a90":"code","7674dcba":"code","16aa557b":"code","2746e694":"code","fdafd2da":"code","d148d968":"code","e2b5f8fa":"code","688c82ff":"code","8a025cab":"code","3ec8253e":"code","4ae944ae":"code","8905fb96":"code","2216095a":"code","c86bea4c":"code","141f6aac":"code","ae5e65dd":"code","29602a5e":"code","c454da43":"code","b0ef3e6d":"code","0e7de17f":"code","094ee35b":"code","a8ebf700":"code","81bcf744":"code","a2614e51":"code","d603f0c6":"code","3e867c03":"code","9955d592":"code","320d7110":"code","317df9fa":"code","4e8029d4":"code","ff3659a7":"code","67a24f59":"code","a7a550f2":"code","15de0d6f":"code","b343d380":"code","d2e75c47":"code","221d829f":"code","b789c116":"code","26c60d92":"code","7811ceed":"code","f307d47c":"code","d757091a":"code","a85bf13d":"code","efa6ed71":"markdown","b5699cb3":"markdown","8c5c567f":"markdown","833b56dc":"markdown","f619f40e":"markdown","ea2e8534":"markdown","ea9953eb":"markdown","0dddfa49":"markdown","cce898c4":"markdown","2233da3c":"markdown","90b2987f":"markdown","499f064a":"markdown","e008b07a":"markdown","4d760c1e":"markdown","5cdde2f1":"markdown","e28ae85c":"markdown","f230c98b":"markdown","07f7e7e0":"markdown","e10b3cfc":"markdown","de001a40":"markdown","51adc062":"markdown","74f8efcc":"markdown","fefa27b0":"markdown","d192fed5":"markdown","f90296af":"markdown","dd81e62a":"markdown","3c6c58ed":"markdown","8ed437e0":"markdown","22a32255":"markdown","a75bbf2d":"markdown","786575ad":"markdown","f94b1e36":"markdown","b2309182":"markdown","5e55bcb1":"markdown","46e4ff18":"markdown","631cf4ca":"markdown","9f288c3e":"markdown","a3e3f30f":"markdown","e0d1f0a0":"markdown","168521a7":"markdown","0faa7324":"markdown","3d644f02":"markdown","b599e2d2":"markdown","03babe12":"markdown","60666063":"markdown","1b687f2e":"markdown","dcf59b29":"markdown","14bf7ccf":"markdown","4a03b97a":"markdown","a720f3d0":"markdown","138d5b63":"markdown","177545d0":"markdown","f06f46bd":"markdown","65b40932":"markdown","9c941bc4":"markdown","e11ac50c":"markdown","d4df7c13":"markdown","5cbbda1d":"markdown","30b72c85":"markdown","927a21ae":"markdown","99f9fb67":"markdown","e5d3a77f":"markdown","bf0a2884":"markdown","2e7f3cae":"markdown","951f2e60":"markdown","ce4c0c43":"markdown","a4077205":"markdown","38455e26":"markdown","7680e881":"markdown","decdd233":"markdown"},"source":{"4bc683d4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom sklearn.manifold import TSNE\nfrom scipy.stats import chi2_contingency \nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder, MaxAbsScaler, LabelEncoder\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.feature_selection import mutual_info_classif, RFE\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\nfrom imblearn import under_sampling\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.ensemble import EasyEnsembleClassifier, RUSBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nimport optuna\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.pipeline import Pipeline as imbpipeline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0ae10629":"data = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')","6ad62612":"data.head().T","a0507d11":"data.shape","0b836e87":"data.drop(columns='id',inplace=True)","8f36677f":"data.dtypes","8c237016":"categorical_variables = list(data.select_dtypes('object').columns)\n\nnumeric_variables = list(data.select_dtypes('float64').columns)\n\ntarget = list(data.select_dtypes('int64').columns)[0]\n\nprint(f'Categorical Variables ({len(categorical_variables)}):\\n{categorical_variables}\\n\\nNumeric Variables ({len(numeric_variables)}):\\n{numeric_variables}\\n\\nTarget:\\n{target}')","9ea33915":"data.describe().T","bb660606":"data[numeric_variables].apply(lambda x: (x.std()\/x.median())*100)","66bb8bf8":"data.isnull().mean()","e32ccbc2":"X = data.iloc[:,:-1].copy()\ny = data['target'].astype(np.int8).copy()\n\ncol_transform = ColumnTransformer(transformers=[['ordinal_encoder',OrdinalEncoder(),categorical_variables]],remainder='passthrough')\n\nX = col_transform.fit_transform(X).copy()\nmin_max = MinMaxScaler()\nX = min_max.fit_transform(X).copy()\n\npca = PCA(n_components=2,random_state=11)\nX_pca = pca.fit_transform(X).copy()\nX_pca = pd.DataFrame(X_pca,columns=[\"component_1\",\"component_2\"])\nX_pca['y'] = y\n\nnp.cumsum(pca.explained_variance_ratio_)","e2948e1d":"fig = plt.figure(figsize=(8,8))\nsns.scatterplot(data=X_pca,x='component_1',y=\"component_2\",hue='y',alpha=0.2,palette=['black','lightgray']);","12512efa":"X = data.iloc[:,:-1].copy()\ny = data['target'].astype(np.int8).copy()\n\ncol_transform = ColumnTransformer(transformers=[['ohe',OneHotEncoder(),categorical_variables]],remainder='passthrough')\n\nX = col_transform.fit_transform(X).copy()\nmax_abs = MaxAbsScaler()\nX = max_abs.fit_transform(X).copy()\n\ntsvd = TruncatedSVD(n_components=2,random_state=11)\nX_svd = tsvd.fit_transform(X).copy()\nX_svd = pd.DataFrame(X_svd,columns=[\"component_1\",\"component_2\"])\nX_svd['y'] = y\n\nnp.cumsum(tsvd.explained_variance_ratio_)","b07cbd2b":"fig = plt.figure(figsize=(8,8))\nsns.scatterplot(data=X_svd,x='component_1',y=\"component_2\",hue='y',alpha=0.2,palette=['black','lightgray']);","befe56de":"data[numeric_variables].plot(kind='kde',figsize=(15,20),subplots=True,layout=(6,2),color='black');","46748b12":"print(\"h0:Sample comes from a normal distribution\\nh1:Sample doesn't come from a normal distribution\\n\\n\")\n\nfor i in numeric_variables:\n    print(f\"{i}: {'Non-Gaussian' if (stats.normaltest(data[i])[1])<0.05 else 'Gaussian'}\")","1e3f4298":"for n,i in enumerate(numeric_variables):\n    stats.probplot(data[i],plot=plt)\n    plt.title(i)\n    plt.show()","767dbaeb":"data[numeric_variables].plot(kind='box',subplots=True,layout=(6,2),figsize=(15,20),color='black');","d71f088a":"for i in numeric_variables:\n    print(f\"{i}: {'Skewed' if (stats.skewtest(data[i])[1])<0.05 else 'Not Skewed'}  {stats.skew(data[i])}\")","0ca9b04b":"fig = plt.figure(figsize=(10,10))\nsns.heatmap(data[numeric_variables].corr(),mask=np.triu(data[numeric_variables].corr()),\n            annot=True,fmt='.2f',\n            cbar=False,cmap=['white'],linewidths=0.01,linecolor='black',square=True);","8207036b":"r = c = 0\nfig,ax = plt.subplots(6,2,figsize=(14,35))\nfor n,i in enumerate(numeric_variables):\n    med = data[[i,'target']].groupby('target').median().copy()\n    sns.violinplot(x=target,y=i,data=data,ax=ax[r,c],palette=[\"gray\",\"lightgray\"])\n    med.plot(ax=ax[r,c],color='black',linewidth=3,linestyle=\"--\",legend=False)\n    for x,y in zip(list(med.index),med[i]):\n        ax[r,c].text(x=x+0.05,y=y+0.01,s=np.round(y,2),fontsize=10,color='white',backgroundcolor='black')\n    ax[r,c].set_title(i.upper()+\" by \"+\"TARGET\")\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","c239dd49":"for i in categorical_variables:\n    val_cnt = pd.DataFrame(np.round(data[i].value_counts(normalize=True)*100,2))\n    val_cnt.columns = ['Proportion']\n    print(f'{i}: {data[i].nunique()} unique categories\\n{val_cnt}\\n')","2f291347":"for i in categorical_variables:\n    ct = pd.crosstab(columns=data[i],index=data[\"target\"])\n    stat, p, dof, expected = chi2_contingency(ct) \n    print(f\"\\n{'-'*len(f'Chi-Square test between {i} & Target')}\")\n    print(f'Chi-Square test between {i} & Target')\n    print(f\"{'-'*len(f'Chi-Square test between {i} & Target')}\")\n    print(f\"\\nH0: THERE IS NO RELATIONSHIP BETWEEN TARGET & {i.upper()}\\nH1: THERE IS RELATIONSHIP BETWEEN TARGET & {i.upper()}\")\n    print(f\"\\nP-VALUE: {np.round(p,2)}\")\n    print(\"REJECT H0\" if p<0.05 else \"FAILED TO REJECT H0\")","d28c253d":"r = c = 0\nfig,ax = plt.subplots(7,2,figsize=(24,32))\nnew_cat = categorical_variables.copy()\nnew_cat.remove('cat3')\nnew_cat.remove('cat10')\nnew_cat.remove('cat5')\nnew_cat.remove('cat7')\nnew_cat.remove('cat8')\nfor n,i in enumerate(new_cat):\n    ct = pd.crosstab(columns=data[i],index=data['target'],normalize=\"columns\")\n    ct.T.plot(kind=\"bar\",stacked=True,color=[\"black\",\"gray\"],ax=ax[r,c])\n    ax[r,c].set_ylabel(\"% of observations\")\n    ax[r,c].set_xlabel(\"\")\n    ax[r,c].set_title(f'{i} vs Target')\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nplt.show()","be7f507f":"non_rare = pd.DataFrame()\n\ncut_offs = [0.2,0.02,0.02,0.02,0.02,0.03,0.01,0.01,0.005,0.02,0.001,0.1,0.1,0.02,0.4,0.04,0.02,0.085,0.072]\n\nfor n,i in enumerate(categorical_variables):\n    var_dist = data[i].value_counts(normalize=True).copy()\n    non_rare = pd.concat([non_rare,pd.DataFrame({i:var_dist[var_dist>cut_offs[n]].index})],axis=1).copy()\n\nnon_rare.to_csv('.\/non_rare_categories.csv',index=False) #storing non-rare categories in a csv for further use\nnon_rare","1c824a90":"new_data = data.copy()\nfor i in non_rare.columns:\n    new_data.loc[(new_data[i].isin(non_rare[i]) == False), i] = \"Rare\"\n    \n    \n\nfor n,i in enumerate(non_rare.columns):\n    if i in ['cat5', 'cat7', 'cat8', 'cat10']:\n        continue\n    \n    cat_dist = data[i].value_counts().sort_values().copy()\n    cat_dist = np.round((cat_dist \/ cat_dist.sum()) * 100,1).copy()\n    fig,ax = plt.subplots(1,2,figsize=(15, len(cat_dist)\/1.5))\n    cat_dist.plot(kind=\"barh\",ax=ax[0],sharey=False,title=i + \" Before Adding Rare Label\",color='black')\n    for n,j in enumerate(cat_dist.index):\n        ax[0].text(y=n,x=cat_dist[j]+0.1,s=str(cat_dist[j]) + \"%\")\n    \n    \n    new_cat_dist = new_data[i].value_counts().sort_values().copy()\n    new_cat_dist = np.round((new_cat_dist \/ new_cat_dist.sum()) * 100,1).copy()\n    new_cat_dist.plot(kind=\"barh\",ax=ax[1],sharey=False,title=i + \" After Adding Rare Label\",color='black')\n    for n,j in enumerate(new_cat_dist.index):\n        ax[1].text(y=n,x=new_cat_dist[j]+0.1,s=str(new_cat_dist[j]) + \"%\")\n    \n    \nplt.show()","7674dcba":"def mark_rare_categories(df):\n    df = df.copy()\n    non_rare = pd.read_csv('.\/non_rare_categories.csv')\n    for i in non_rare.columns:\n        df.loc[(df[i].isin(non_rare[i]) == False), i] = \"Rare\"\n    return df","16aa557b":"data[target].unique()","2746e694":"sns.countplot(data=data,x=target,palette=['black','gray']);\n\ntgt_cnt = data[target].value_counts()\n\ntgt_prop = np.round(data[target].value_counts(normalize=True)*100,1)\n\nplt.text(x=-0.2,y=tgt_cnt[0]+2500,s=f'{tgt_cnt[0]:,} ({tgt_prop[0]}%)')\n\nplt.text(x=0.8,y=tgt_cnt[1]+3000,s=f'{tgt_cnt[1]:,} ({tgt_prop[1]}%)');","fdafd2da":"def mutual_info(df,mark_rare=False):\n    df = df.copy()\n    if mark_rare == True:\n        df = mark_rare_categories(df).copy()\n    \n    #X = df.iloc[:,:-1].copy()\n    X = df.drop(columns=['target']).copy()\n    y = df['target'].values.copy()\n\n    for i in categorical_variables:\n        le = LabelEncoder()\n        X[i] = le.fit_transform(X[i])\n\n    mutual_info = mutual_info_classif(X=X,y=y,discrete_features=(X.dtypes == np.int64),random_state=11)\n    mutual_info_df = pd.DataFrame({'feature':X.columns,'MI':mutual_info}).sort_values(by='MI',ascending=False)\n    mutual_info_df.sort_values(by='MI').plot(x='feature',y='MI',kind='barh',figsize=(15,12),color='black')\n    for n,k in enumerate(range((len(mutual_info_df)-1),-1,-1)):\n        plt.text(y=n-0.2,x=mutual_info_df.iloc[k,1],s=np.round(mutual_info_df.iloc[k,1],4))\n    plt.show()\n    if mark_rare == True:\n        mutual_info_df.columns = ['Feature','MI_mark_rare_T']\n    else:\n        mutual_info_df.columns = ['Feature','MI_mark_rare_F']\n    return mutual_info_df","d148d968":"mi_mark_rare_F = mutual_info(data).copy()","e2b5f8fa":"mi_mark_rare_T = mutual_info(df=data,mark_rare=True).copy()","688c82ff":"marked_rare_cols = mark_rare_categories(data).copy()\nmarked_rare_cols = (((marked_rare_cols[categorical_variables] == 'Rare').sum()).astype('bool')).reset_index()\nmarked_rare_cols.columns = ['Feature','is_marked_rare']\n\nmi_mark_rare_F = mi_mark_rare_F.merge(mi_mark_rare_T,on='Feature',how='left').copy()\nmi_mark_rare_F = mi_mark_rare_F.merge(marked_rare_cols,on='Feature',how='left').copy()\n\nmi_mark_rare_F['mi_percent_change'] = (mi_mark_rare_F['MI_mark_rare_T'] \/ mi_mark_rare_F['MI_mark_rare_F'] -1)*100\nmi_mark_rare_F","8a025cab":"def prepare_x_y(df,mark_rare=False):\n    df = df.copy()\n    if mark_rare == True:\n        df = mark_rare_categories(df).copy()\n\n    #X = df.iloc[:,:-1].copy()\n    X = df.drop(columns=['target']).copy()\n    y = df['target'].copy()\n\n    col_transform = ColumnTransformer(transformers=[['ordinal_encoder',OrdinalEncoder(),categorical_variables]],\n                                      remainder='passthrough')\n\n    X = col_transform.fit_transform(X).copy()\n    return (X,y)","3ec8253e":"X,y = prepare_x_y(data)\n\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=11)\n\nrf = RandomForestClassifier(random_state=11)\n\n#cv = cross_validate(estimator=rf,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc',return_train_score=True)","4ae944ae":"X,y = prepare_x_y(data,mark_rare=True)\n\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=11)\n\nrf = RandomForestClassifier(random_state=11)\n\n#cv = cross_validate(estimator=rf,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc',return_train_score=True)","8905fb96":"data['c15_16'] = data['cat15'] + '-' + data['cat16']\ndata['c15_18'] = data['cat15'] + '-' + data['cat18']\ndata['c16_18'] = data['cat16'] + '-' + data['cat18']\ndata['c15_16_18'] = data['cat15'] +  '-' + data['cat16'] + '-' + data['cat18']\n\n\n#NEW\ndata['c15_1'] = data['cat15'] + '-' + data['cat1']\ndata['c16_1'] = data['cat16'] + '-' + data['cat1']\ndata['c18_1'] = data['cat18'] + '-' + data['cat1']\n\ndata['c15_16_1'] = data['cat15'] +  '-' + data['cat16'] + '-' + data['cat1']\ndata['c15_18_1'] = data['cat15'] +  '-' + data['cat18'] + '-' + data['cat1']\ndata['c16_18_1'] = data['cat16'] +  '-' + data['cat18'] + '-' + data['cat1']\ndata['c15_16_18_1'] = data['cat15'] +  '-' + data['cat16'] +  '-' + data['cat18'] + '-' + data['cat1']","2216095a":"data.head().T","c86bea4c":"#categorical_variables.extend(['c15_16','c15_18','c16_18','c15_16_18'])\ncategorical_variables.extend(['c15_16','c15_18','c16_18','c15_16_18','c15_1','c16_1','c18_1',\n                            'c15_16_1','c15_18_1','c16_18_1','c15_16_18_1'])","141f6aac":"mi_mark_rare_F = mutual_info(data).copy()","ae5e65dd":"X,y = prepare_x_y(data)\n\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=11)\n\nrf = RandomForestClassifier(random_state=11)\n\n#cv = cross_validate(estimator=rf,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc',return_train_score=True)","29602a5e":"# X = data.drop(columns=['target']).copy()\n# y = data['target'].copy()\n\n# col_transform = ColumnTransformer(transformers=[['ordinal_encoder',OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),categorical_variables]],\n#                                    remainder='passthrough')\n\n# X = col_transform.fit_transform(X).copy()\n\n# kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=11)\n\n# final_model = xgb.XGBClassifier(random_state=11,\n#                                 eval_metric='auc',\n#                                 tree_method='gpu_hist',\n#                                 n_jobs=-1,\n#                                 use_label_encoder=False,\n#                                 scale_pos_weight=2.78)\n\n# # cv = cross_validate(estimator=final_model,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc',return_train_score=True)","c454da43":"# final_model.fit(X,y)","b0ef3e6d":"def model_train_test(X,y,estimator,param_distributions,n_trials,n_splits):\n    \n    kfold = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=11)\n    \n    optuna_search = optuna.integration.OptunaSearchCV(estimator=estimator,\n                                                  param_distributions=param_distributions,\n                                                  cv=kfold,\n                                                  scoring='roc_auc',\n                                                  n_jobs=-1,\n                                                  n_trials=n_trials,\n                                                  random_state=11,\n                                                  verbose=1)\n    \n    optuna_search.fit(X,y)\n    \n    result = {'optuna_search':optuna_search,\n              'best_params':optuna_search.best_params_,\n              'best_score':optuna_search.best_score_}\n    \n    return result","0e7de17f":"#scale_pos_weight = no. of neg samples \/ no. of pos samples\n\nX = data.drop(columns=['target']).copy()\ny = data['target'].copy()\n\ncol_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n                                                 OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n                                                 categorical_variables]],\n                                  remainder='passthrough')\n\nX = col_transform.fit_transform(X).copy()\n\nxgb_classifier = xgb.XGBClassifier(random_state=11,\n                                   eval_metric='auc',\n                                   tree_method='gpu_hist',\n                                   n_jobs=-1,\n                                   use_label_encoder=False,\n                                   scale_pos_weight=2.8)\n\n\npipeline = Pipeline(steps=[['rfe',RFE(estimator=xgb_classifier)],\n                           ['classifier',xgb_classifier]])\n\n\n\n\nparam_distributions = {'rfe__n_features_to_select':optuna.distributions.IntUniformDistribution(20, X.shape[1]),\n                       'classifier__n_estimators':optuna.distributions.IntUniformDistribution(100, 700),\n                       'classifier__max_depth':optuna.distributions.IntUniformDistribution(3, 15),\n                       'classifier__reg_lambda':optuna.distributions.LogUniformDistribution(500, 100000),\n                       'classifier__colsample_bylevel':optuna.distributions.LogUniformDistribution(0.5,1)\n                      }\n\n    \n#result = model_train_test(X,y,pipeline,param_distributions,300,3)\n\n#result","094ee35b":"X = data.drop(columns=['target']).copy()\ny = data['target'].copy()\n\ncol_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n                                                 OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n                                                 categorical_variables]],\n                                  remainder='passthrough')\n\nX = col_transform.fit_transform(X).copy()\n\nxgb_classifier = xgb.XGBClassifier(random_state=11,\n                                   eval_metric='auc',\n                                   tree_method='gpu_hist',\n                                   n_jobs=-1,\n                                   use_label_encoder=False,\n                                   scale_pos_weight=2.8,\n                                   n_estimators=670,\n                                   max_depth=10,\n                                   reg_lambda=9738.763400431715,\n                                   colsample_bylevel=0.6323793308207895)\n\n\nfinal_model = Pipeline(steps=[['rfe',RFE(n_features_to_select=41,\n                                         estimator=xgb.XGBClassifier(\n                                         random_state=11,\n                                         eval_metric='auc',\n                                         tree_method='gpu_hist',\n                                         n_jobs=-1,\n                                         use_label_encoder=False,\n                                         scale_pos_weight=2.8))],\n                           ['classifier',xgb_classifier]])\n\nfinal_model.fit(X,y)","a8ebf700":"# #scale_pos_weight = no. of neg samples \/ no. of pos samples\n\n# X = data.drop(columns=['target']).copy()\n# y = data['target'].copy()\n\n# col_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n#                                                  OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n#                                                  categorical_variables]],\n#                                   remainder='passthrough')\n\n# X = col_transform.fit_transform(X).copy()\n\n# lgb_classifier = LGBMClassifier(random_state=11,\n#                                 metric='auc',\n#                                 device_type='gpu',\n#                                 n_jobs=-1,\n#                                 scale_pos_weight=2.8)\n\n\n# pipeline = Pipeline(steps=[['rfe',RFE(estimator=lgb_classifier)],\n#                            ['classifier',lgb_classifier]])\n\n\n\n\n# param_distributions = {'rfe__n_features_to_select':optuna.distributions.IntUniformDistribution(10, X.shape[1]),\n#                        'classifier__n_estimators':optuna.distributions.IntUniformDistribution(100, 700),\n#                        'classifier__max_depth':optuna.distributions.IntUniformDistribution(2, 15),\n#                        'classifier__reg_lambda':optuna.distributions.LogUniformDistribution(500, 100000),\n#                        'classifier__feature_fraction_bynode':optuna.distributions.LogUniformDistribution(0.5,1)\n#                       }\n\n    \n# #result = model_train_test(X,y,pipeline,param_distributions,150,3)\n\n# #result","81bcf744":"# X = data.drop(columns=['target']).copy()\n# y = data['target'].copy()\n\n# col_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n#                                                  OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n#                                                  categorical_variables]],\n#                                   remainder='passthrough')\n\n# X = col_transform.fit_transform(X).copy()\n\n\n# xgb_model = xgb.XGBClassifier(random_state=11,\n#                                    eval_metric='auc',\n#                                    tree_method='gpu_hist',\n#                                    n_jobs=-1,\n#                                    use_label_encoder=False,\n#                                    scale_pos_weight=2.8,\n#                                    n_estimators=636,\n#                                    max_depth=8,\n#                                    reg_lambda=10495.566650340597,\n#                                    colsample_bylevel=0.8940079532042917)\n\n# lgb_model = LGBMClassifier(random_state=11,\n#                                 metric='auc',\n#                                 device_type='gpu',\n#                                 n_jobs=-1,\n#                                 scale_pos_weight=2.8,\n#                                 n_estimators=550,\n#                                 max_depth=13,\n#                                 num_leaves = 2^(13),\n#                                 reg_lambda=789.5237167418961,\n#                                 feature_fraction_bynode=0.6711243271129161\n#                                )\n\n# voting_classifier = VotingClassifier(estimators = [('xgb',xgb_model),('lgb',lgb_model)],\n#                                     voting='soft')\n\n# voting_classifier.fit(X,y)","a2614e51":"# final_model = voting_classifier","d603f0c6":"# X = data.drop(columns=['target']).copy()\n# y = data['target'].copy()\n\n# col_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n#                                                  OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n#                                                  categorical_variables]],\n#                                   remainder='passthrough')\n\n# X = col_transform.fit_transform(X).copy()\n\n# xgb_classifier = xgb.XGBClassifier(random_state=11,\n#                                    eval_metric='auc',\n#                                    tree_method='gpu_hist',\n#                                    n_jobs=-1,\n#                                    use_label_encoder=False,\n#                                    n_estimators=636,\n#                                    max_depth=8,\n#                                    reg_lambda=10495.566650340597,\n#                                    colsample_bylevel=0.8940079532042917)\n\n# kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=11)\n\n# model = EasyEnsembleClassifier(base_estimator = xgb_classifier,\n#                                random_state=11,\n#                                verbose=11,\n#                                n_jobs=-1)\n    \n# cv = cross_validate(estimator=model,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc',return_train_score=True)\n    \n    \n# print(f'\\ntrain_scores\\n{cv[\"train_score\"]}\\nmean train score\\n{np.mean(cv[\"train_score\"])}' + \n#      f'\\ntest_scores\\n{cv[\"test_score\"]}\\nmean test score\\n{np.mean(cv[\"test_score\"])}')\n","3e867c03":"# final_model = EasyEnsembleClassifier(base_estimator = xgb_classifier,\n#                                random_state=11,\n#                                verbose=11,\n#                                n_jobs=-1)\n\n# final_model.fit(X,y)","9955d592":"# X = data.drop(columns=['target']).copy()\n# y = data['target'].copy()\n\n# col_transform = ColumnTransformer(transformers=[['ordinal_encoder',\n#                                                  OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1),\n#                                                  categorical_variables]],\n#                                   remainder='passthrough')\n\n# X = col_transform.fit_transform(X).copy()\n\n# xgb_classifier = xgb.XGBClassifier(random_state=11,\n#                                    eval_metric='auc',\n#                                    tree_method='gpu_hist',\n#                                    n_jobs=-1,\n#                                    use_label_encoder=False,\n#                                    n_estimators=426,\n#                                    max_depth=7,\n#                                    reg_lambda=4905.085039913591)\n\n# under_samplers = [under_sampling.TomekLinks(n_jobs=-1),\n#                   under_sampling.CondensedNearestNeighbour(random_state=11,n_jobs=-1),\n#                   under_sampling.EditedNearestNeighbours(n_jobs=-1),\n#                   under_sampling.RepeatedEditedNearestNeighbours(n_jobs=-1),\n#                   under_sampling.NearMiss(n_jobs=-1),\n#                   under_sampling.NeighbourhoodCleaningRule(n_jobs=-1)]\n\n# for i in under_samplers:\n    \n#     kfold = StratifiedKFold(n_splits=3,shuffle=True,random_state=11)\n    \n    \n#     model = imbpipeline(steps=[['under_sampler',i],\n#                                ['classifier',xgb_classifier]])\n    \n#     cv = cross_validate(estimator=model,X=X,y=y,cv=kfold,n_jobs=-1,verbose=11,scoring='roc_auc')\n    \n    \n#     print(f'{i}\\ntest_scores\\n{cv[\"test_score\"]}\\nmean test score\\n{np.mean(cv[\"test_score\"])}')\n\n","320d7110":"test = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')","317df9fa":"test_ids = test['id'].copy()","4e8029d4":"test.drop(columns='id',inplace=True)","ff3659a7":"test.dtypes","67a24f59":"categorical_variables = list(test.select_dtypes('object').columns)\n\nnumeric_variables = list(test.select_dtypes('float64').columns)","a7a550f2":"print(f'Categorical Variables ({len(categorical_variables)}):\\n{categorical_variables}\\n\\nNumeric Variables ({len(numeric_variables)}):\\n{numeric_variables}')","15de0d6f":"test['c15_16'] = test['cat15'] + '-' + test['cat16']\ntest['c15_18'] = test['cat15'] + '-' +  test['cat18']\ntest['c16_18'] = test['cat16'] + '-' +  test['cat18']\ntest['c15_16_18'] = test['cat15'] + '-' +  test['cat16'] + '-' +  test['cat18']\n\n#NEW\ntest['c15_1'] = test['cat15'] + '-' + test['cat1']\ntest['c16_1'] = test['cat16'] + '-' + test['cat1']\ntest['c18_1'] = test['cat18'] + '-' + test['cat1']\n\ntest['c15_16_1'] = test['cat15'] +  '-' + test['cat16'] + '-' + test['cat1']\ntest['c15_18_1'] = test['cat15'] +  '-' + test['cat18'] + '-' + test['cat1']\ntest['c16_18_1'] = test['cat16'] +  '-' + test['cat18'] + '-' + test['cat1']\ntest['c15_16_18_1'] = test['cat15'] +  '-' + test['cat16'] +  '-' + test['cat18'] + '-' + test['cat1']","b343d380":"test.head().T","d2e75c47":"#categorical_variables.extend(['c15_16','c15_18','c16_18','c15_16_18'])\ncategorical_variables.extend(['c15_16','c15_18','c16_18','c15_16_18','c15_1','c16_1','c18_1',\n                              'c15_16_1','c15_18_1','c16_18_1','c15_16_18_1'])","221d829f":"for i in categorical_variables:\n    print(f'{i}\\n{np.sum(test[i].isin(data[i])==False)}')","b789c116":"test.dtypes","26c60d92":"test = col_transform.transform(test).copy()","7811ceed":"prediction = final_model.predict_proba(test)[:,1]","f307d47c":"submission = pd.DataFrame({'id':test_ids,'target':prediction})","d757091a":"submission","a85bf13d":"submission.to_csv('submission_11.csv',index=False)","efa6ed71":"<h3>Excluded few plots as they were cluttered.","b5699cb3":"<h3>All the newly added features have high MI scores, but their individual score is close to 0. Hence, adding them may not improve the model' performance to a great extent. However, adding them and employing a feature selection method might work.","8c5c567f":"<h2>Normality Test","833b56dc":"<h1><section id='tgtdist'>8. Target Distribution&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","f619f40e":"<h1><section id=\"preda\">1. Preliminary EDA&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","ea2e8534":"<h2>EasyEnsembleClassifier (no improvement in submission score)","ea9953eb":"<h3>Dropping 'id' column","0dddfa49":"<h3>Variables cont8, cont9, cont10 have outliers<\/h3><br>\n<h2>Skewness test","cce898c4":"<h2>Let's one hot encode categorical variables and use Truncated SVD on the sparse data","2233da3c":"<h2>Checking for zero\/near-zero variance in relation with the median for a clearer picture","90b2987f":"<h1><section id='rare'>7. Handling Rare Categories in Categorical Variables&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","499f064a":"<h2>RF with rare labels marked","e008b07a":"<h2>Identifying non-rare categories in category variables with different cut-offs for each variable and saving them in a csv.","4d760c1e":"<h2>Trying OptunaSearchCV (ExperimentalWarning: OptunaSearchCV is still experimental phase)","5cdde2f1":"cv['train_score']<br>\narray([1., 1., 1., 1., 1.])<br><br>\n\ncv['test_score']<br>\narray([0.88089911, 0.88598508, 0.88015966, 0.88502024, 0.88199494])<br>\n\nnp.mean(cv['test_score'])<br>\n0.8828118051395147<br>","e28ae85c":"<h1><section id='catrel'>6. Categorical Variables vs Target&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","f230c98b":"<h2>We'll reduce the dimensionality of the data using PCA\/SVD to get a rough idea on how the classes are separated.<br><br>First using ordinal encoding for categorical variables and using PCA","07f7e7e0":"<h3>Class in imbalanced","e10b3cfc":"<h1><section id='catdist'>5. Distribution of Categorical Variables&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","de001a40":"<h2>RF without marking rare labels","51adc062":"<h3>No numeric variable has zero\/near-zero variance<\/h3><br>\n<h2>Checking for Missing Values","74f8efcc":"<h1>Vanilla XGB","fefa27b0":"<h3>Few numeric features are highly correlated with each other","d192fed5":"<h3>Few variables have multiple peaks in their distribution and almost all of them look skewed. However, a statistical test can confirm these observations","f90296af":"<h3>No missing values<\/h3>","dd81e62a":"<h3>Defining a function to mark rare labels","3c6c58ed":"<h2>Relationship among numeric variables","8ed437e0":"<h3>scale_pos_weight = no. of neg samples \/ no. of pos samples","22a32255":"<h1><section id='numvstgt'>4. Numeric Variables vs Target&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","a75bbf2d":"<h2>cat16, cat15 and cat18 have the highest MI scores","786575ad":"<h3>No numeric variable has a Gaussian\/Normal Distribution","f94b1e36":"<h1><section id='subm'>13. Submission&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","b2309182":"<h1><section id='csep'>2. Visualizing Class Separation&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","5e55bcb1":"<h3>Calculating MI scores by including the newly added features","46e4ff18":"<section id = 'top'><\/section>\n<h2>\n    <ol>\n        <li><a href=#preda>Preliminary EDA<\/a><\/li>\n        <li><a href=#csep>Visualizing Class Separation<\/a><\/li>\n        <li><a href=#numdist>Distribution of Numeric Variables<\/a><\/li>\n        <li><a href=#numvstgt>Numeric Variables vs Target<\/a><\/li>\n        <li><a href=#catdist>Distribution of Categorical Variables<\/a><\/li>\n        <li><a href=#catrel>Categorical Variables vs Target<\/a><\/li>\n        <li><a href=#rare>Handling Rare Categories in Categorical Variables<\/a><\/li>\n        <li><a href=#tgtdist>Target Distribution<\/a><\/li>\n        <li><a href=#mutinfo>Mutual Information<\/a><\/li>\n        <li><a href=#base>Random Forest Baseline Model<\/a><\/li>\n        <li><a href=#nfc>New Feature Creation<\/a><\/li>\n        <li><a href=#build>Model Building<\/a><\/li>\n        <li><a href=#subm>Submission<\/a><\/li>\n    <\/ol><\/h2>\n        \n","631cf4ca":"<h1><section id='base'>10. Random Forest Baseline Model&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","9f288c3e":"<h1>WORK IN PROGRESS","a3e3f30f":"<h3>cont7, cont8, cont9 & cont10 have slightly higher right-skewness. Other variables have slight skewness (cont5 is slightly left-skewed)","e0d1f0a0":"<h1><section id='build'>12. Model Building&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","168521a7":"<h3>Plots of few variables with high cardinality as hidden","0faa7324":"<h1><section id='mutinfo'>9. Mutual Information&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","3d644f02":"<h2>Vanilla RF on data with newly added features","b599e2d2":"cv['train_score']<br>\narray([1., 1., 1., 1., 1.])<br><br>\n\ncv['test_score']<br>\narray([0.88190414, 0.88619879, 0.88114262, 0.88497447, 0.88220613])<br>\n\nnp.mean(cv['test_score'])<br>\n0.8832852299999999<br>\n\n<h2>Marking rare labels showed no difference in model' performance\n","03babe12":"cv['train_score']<br>\narray([1., 1., 1., 1., 1.])<br><br>\ncv['test_score']<br>\narray([0.88257123, 0.88632475, 0.88164209, 0.88505834, 0.88238509])\n\nnp.mean(cv['test_score'])<br>\n0.8835963\n","60666063":"<h1>Tabular Playground Series - Mar 2021","1b687f2e":"<h3>MARKING RARE LABELS IS RESULTING IN REDUCED MI SCORE. REDUCING THE CUT-OFF FOR MARKING RARE LABELS IS IMPROVING THE MI SCORE. HENCE, THE MORE VARIABLES MARKED AS RARE, LOWER WILL BE THE MI SCORE.","dcf59b29":"<h2>VotingClassifier (Not yet trained)","14bf7ccf":"<h1><section id='numdist'>3. Distribution of Numeric Variables&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","4a03b97a":"<h2>Q-Q plots","a720f3d0":"<h3>Both PCA and SVD show that the classes are fairly separated (though they are not reliable due to very low explained variance, they give us a fair idea). Trying it after feature engineering may result in better explained variance ratio.","138d5b63":"cv['train_score']<br>\narray([0.91757861, 0.91684923, 0.91634894, 0.91599782, 0.91700579])<br><br>\n\ncv['test_score']<br>\narray([0.88787838, 0.89218227, 0.88705092, 0.89156957, 0.8876197 ])<br>\n\nnp.mean(cv['test_score'])<br>\n0.8892601661007467<br>","177545d0":"<h2>XGB + Optuna + RFE (Used as Final Model)","f06f46bd":"<h3>Calculating mutual information scores with and without marking rare categories. This helps in understanding the effect of marking rare categories","65b40932":"<h3>cat16, cat15, cat18 and cat1 have the highest MI scores. Hence, we'll combine them to create new features.","9c941bc4":"train_scores\n[0.90357855 0.90318018 0.90274322 0.90319348 0.90365007 0.90314774\n 0.90269818 0.90347099 0.90326053 0.90343125]<br><br>\nmean train score<br>\n0.9032354202610001<br><br>\ntest_scores<br>\n[0.88959868 0.89332921 0.8974062  0.89291028 0.88805739 0.8933892\n 0.89770895 0.89028771 0.89207586 0.89045199]<br><br>\nmean test score<br>\n0.892521546427905\n\n","e11ac50c":"<h2>LGBMClassifier using same hyperparameters tuned for XGB (Need to train)","d4df7c13":"<h3>Data type of all the variables looks good.<\/h3><br>\n<h2>Storing Categorical, Numeric and Target variables in separate variables for simpler EDA","5cbbda1d":"'best_params': {'rfe__n_features_to_select': 41,<br>\n  'classifier__n_estimators': 670,<br>\n  'classifier__max_depth': 10,<br>\n  'classifier__reg_lambda': 9738.763400431715,<br>\n  'classifier__colsample_bylevel': 0.6323793308207895},<br>\n 'best_score': 0.8926917680409953}","30b72c85":"<h3>No numeric variable has 0 as minimum, hence ruling out the presence of masked NaNs. No numeric varable has missing values<\/h3>","927a21ae":"<h3>Count of rows in test data with categories not in training data","99f9fb67":"<h3>Building RF with and without marking rare categories helps to identify the effect of marking rare categories on a model' performance","e5d3a77f":"<h2>Evaluating few under-sampling techniques (Not yet trained)","bf0a2884":"<h3>From the above MI plots, we need to assess if marking rare lables is resulting in any loss of information","2e7f3cae":"<h2>Plotting categorical variables before and after marking rare categories","951f2e60":"<h2>Box Plot for outlier detection","ce4c0c43":"<h3>Most of the continous variables have a similar distribution for both the classes of target variables. However, their median shows some sort of relationship with the target.","a4077205":"<h3>Though the explained variance of PCA is low, the plot above gives us a fair idea (not very much reliable) that the classes are fairly separated. Let's try SVD","38455e26":"<h1><section id='nfc'>11. New Feature Creation&nbsp;&nbsp;<a href=#top>Top<\/a><\/section><\/h1>","7680e881":"<h3>Adding New features has not improved the performance of the model. However, hyperparameter tuning may help","decdd233":"<h2>Few categorical variables have high cardinality therefore, rare labels."}}