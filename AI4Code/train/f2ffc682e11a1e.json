{"cell_type":{"24082848":"code","0def49ba":"code","0bdc5fdc":"code","c401ee19":"code","34c51711":"code","f864445c":"code","13a04c58":"code","4e92ff37":"code","d39dd93d":"code","79bdd24c":"code","19f47924":"code","a0371581":"code","5308b7e7":"code","a804f6a4":"markdown","f1276170":"markdown","31bc6014":"markdown","944fdac4":"markdown","83f9dabf":"markdown","1cbd3dc2":"markdown","cdeabd7f":"markdown","68ba9024":"markdown","2f11b499":"markdown","a9e58572":"markdown","2d4b6abe":"markdown","e72742cb":"markdown","d40aae13":"markdown","7791406c":"markdown","68e76040":"markdown","2eae23e6":"markdown","3bff4dc0":"markdown","84639bde":"markdown","c6ff5868":"markdown","a400994d":"markdown","99507cb3":"markdown","a081841e":"markdown","52dc7984":"markdown","3434fbf9":"markdown","c5dd1d76":"markdown","5fd1d749":"markdown","a75be859":"markdown"},"source":{"24082848":"!pip install openpyxl\n!pip install wordcloud\n\n# Modules for data processing\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport sys\nfrom datetime import datetime\nimport calendar\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# REFERENCE: https:\/\/www.kaggle.com\/dmitryuarov\/eda-covid-19-impact-on-digital-learning\nSTATE_ABBR = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'American Samoa': 'AS', 'Arizona': 'AZ', 'Arkansas': 'AR',\n    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'District of Columbia': 'DC', 'District Of Columbia': 'DC',\n    'Florida': 'FL', 'Georgia': 'GA', 'Guam': 'GU', 'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL',\n    'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME',\n    'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH',\n    'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR', 'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN',\n    'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virgin Islands': 'VI', 'Virginia': 'VA', 'Washington': 'WA',\n    'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n}\nSTATE_NAME = dict([(y, x) for x, y in STATE_ABBR.items()])\n\n# ########################################################################\n# ### analyze_dataset\n# ########################################################################\n# #\n# # Simple function to help quickly analyze information & usability of\n# # a dataset. Provides information about shape, null values, unique\n# # values & basic statistical features.\n# #\n# # Inputs:\n# #   1.  df_path (string) -> Dataset path (if available)\n# #   2.  df (pandas dataframe) -> Dataset (if available)\n# #   3.  direct_df (boolean) -> Whether dataset path or dataset is\n# #       being provided\n# #   4.  processing_func (function) -> If dataset needs to be processed\n# #       before analyzing\n# #   5.  Other arguments for pd.read_csv(...) if dataset path is being\n# #       provided\n# #\n# # Return:   Either dataframe itself (if path provided) or head of\n# #           dataframe (if dataframe provided)\n# #\n# ########################################################################\n\n# def analyze_dataset(df_path = None, df = None, direct_df = False, processing_func = lambda x: x, **read_csv_args):\n    \n#     if(direct_df == False):\n#         df = pd.read_csv(df_path, **read_csv_args)\n#     df = processing_func(df)\n    \n#     num_rows, num_cols = df.shape\n#     dtypes = dict(df.dtypes.items())\n#     print(\"*****************\")\n#     print(\"Basic Info:\")\n#     print(\"*****************\\n\")\n#     print(f\"Shape of Dataset: {num_rows} rows, {num_cols} cols\")\n#     print(\"Columns:\")\n#     for col_idx, col in enumerate(df.columns):\n#         print(f\"\\t{col_idx+1}. {col}\\n\\t\\t\\t\\t\\t\\t\\t\\t{dtypes[col]}\")\n    \n#     print(\"\\n\\n\\n*****************\")\n#     print(\"Null Values:\")\n#     print(\"*****************\\n\")\n#     nulls = pd.isnull(df).sum()\n#     print(f\"Total Nulls: {nulls.sum()}\")\n#     nulls = nulls[nulls > 0]\n#     nulls = list(sorted(nulls.items(), key = lambda x: x[1], reverse = True))\n#     print(\"Columns with missing values:\")\n#     for col_idx, (col_name, col_missin_num) in enumerate(nulls):\n#         print(f\"\\t{col_idx + 1}. {col_name}\\n\\t\\t\\t\\t\\t\\t\\t\\t{col_missin_num} missing ({col_missin_num \/ num_rows * 100:.1f}%)\")\n    \n#     print(\"\\n\\n\\n*****************\")\n#     print(\"Column-specific:\")\n#     print(\"*****************\\n\")\n#     print(\"Unique values in columns:\")\n#     idx = 1\n#     for col in df.columns:\n#         nunique = df[col].nunique()\n#         if(nunique < 10):\n#             unique_vals = [\"'\" + str(x) + \"'\" for x in df[col].unique()]\n#             print(f\"{idx}. {col} has {nunique} unique values\")\n#             idx += 1\n#             print(f\"\\t[ {', '.join(unique_vals)} ]\")\n#     print(\"\\n\\nStatistical Features:\")\n#     print(df.describe())\n    \n#     print(\"\\n\\n\")\n#     if(direct_df == True):\n#         return df.head()\n#     else:\n#         return df\n\n########################################################################\n### load_main_dataset\n########################################################################\n#\n# Function to load the main processed & merged dataset for\n# engagement. Has options to merge selected datasets. The processing\n# has been done separately in another function `process_main_dataset`\n#\n# Inputs:\n#   1.  whether_merge_district (boolean) -> Whether to merge districts\n#       data\n#   2.  whether_merge_products (boolean) -> Whether to merge products\n#       data\n#   3.  whether_merge_dates (boolean) -> Whether to merge dates data\n#\n# Return:   Engagement data merged with other relevant datasets\n#\n########################################################################\n\ndef load_main_dataset(whether_merge_districts = True, whether_merge_products = True, whether_merge_dates = True):\n    def reduce_dtype_size(df):\n        numeric_cols = [x for x in df.columns if (df[x].dtype != object) & ('datetime' not in str(df[x].dtype))]\n        for numeric_col in numeric_cols:\n            if('float' in str(df[numeric_col].dtype)):\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'float')\n            elif(('uint' in str(df[numeric_col].dtype)) | ('bool' in str(df[numeric_col].dtype))):\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'unsigned')\n            else:\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'signed')\n        return df\n    \n    def merge_districts_data(engagement_data):\n        districts_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/districts_data.csv')\n        districts_data = reduce_dtype_size(districts_data)\n        merged_engagement_data = pd.merge(engagement_data, districts_data, how = 'left', on = 'district_id')\n        return merged_engagement_data\n\n    def merge_products_data(engagement_data):\n        products_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/products_data.csv')\n        products_data = reduce_dtype_size(products_data)\n        merged_engagement_data = pd.merge(engagement_data, products_data, how = 'left', left_on = 'lp_id', right_on = 'LP ID')\n        merged_engagement_data = merged_engagement_data.drop('LP ID', axis = 1)\n        return merged_engagement_data\n\n    def merge_dates_data(engagement_data):\n        dates_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/dates_data.csv', parse_dates = ['date'])\n        dates_data = reduce_dtype_size(dates_data)\n        merged_engagement_data = pd.merge(engagement_data, dates_data, how = 'left', left_on = 'time', right_on = 'date')\n        merged_engagement_data = merged_engagement_data.drop('date', axis = 1)\n        return merged_engagement_data\n\n    engagement_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/engagement_data.csv', parse_dates = ['time'])\n    engagement_data = reduce_dtype_size(engagement_data)\n    if(whether_merge_districts == True):\n        engagement_data = merge_districts_data(engagement_data)\n    if(whether_merge_products == True):\n        engagement_data = merge_products_data(engagement_data)\n    if(whether_merge_dates == True):\n        engagement_data = merge_dates_data(engagement_data)\n    \n    return engagement_data\n\n\n# ########################################################################\n# ### process_main_dataset\n# ########################################################################\n# #\n# # Function to process the main processed & save it for\n# # loading later from another function `load_main_dataset`.\n# #\n# # Inputs:\n# #   1.  whether_load_url_html_data (boolean) -> Whether to process\n# #       and save URL's HTML data\n# #\n# # Return:   None\n# #\n# ########################################################################\n\n# def process_main_dataset(whether_load_url_html_data = False):\n#     def get_all_na_idx(df):\n#         all_na_idx = df.isnull().all(axis=1)\n#         return all_na_idx[all_na_idx == True].keys()\n\n#     def add_dummys(df, dummy_cols, remove_orig_dummy_cols = False):\n#         dummy_df = df[dummy_cols]\n#         dummy_df = pd.get_dummies(dummy_df)\n        \n#         df = pd.concat([df, dummy_df], axis = 1)\n#         if(remove_orig_dummy_cols == True):\n#             df = df.drop(dummy_cols, axis = 1)\n        \n#         return df\n\n#     def reduce_dtype_size(df):\n#         numeric_cols = [x for x in df.columns if (df[x].dtype != object) & ('datetime' not in str(df[x].dtype))]\n#         for numeric_col in numeric_cols:\n#             if('float' in str(df[numeric_col].dtype)):\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'float')\n#             elif(('uint' in str(df[numeric_col].dtype)) | ('bool' in str(df[numeric_col].dtype))):\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'unsigned')\n#             else:\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'signed')\n#         return df\n    \n#     def load_districts_data():\n        \n#         def districts_data_preprocessing(districts_data):\n            \n#             def process_lower_upper_bounds(df_series):\n#                 processed_lower_series = []\n#                 processed_upper_series = []\n                \n#                 for row in df_series:                \n#                     if(pd.isnull(row) == True):\n#                         processed_lower_series.append(row)\n#                         processed_upper_series.append(row)\n#                     else:\n#                         assert(len(row[1:-1].split(', ')) == 2)\n#                         lower_val, upper_val = row[1:-1].split(', ')\n#                         lower_val = float(lower_val)\n#                         upper_val = float(upper_val)\n#                         processed_lower_series.append(lower_val)\n#                         processed_upper_series.append(upper_val)\n                \n#                 return processed_lower_series, processed_upper_series\n            \n#             for col in ['pct_black\/hispanic', 'pct_free\/reduced', 'county_connections_ratio', 'pp_total_raw']:\n#                 lower_series, upper_series = process_lower_upper_bounds(districts_data[col])\n#                 districts_data[col + '_lower_bound'] = pd.Series(lower_series, index = districts_data.index)\n#                 districts_data[col + '_upper_bound'] = pd.Series(upper_series, index = districts_data.index)\n#                 districts_data[col + '_bound_avg'] = pd.Series(np.add(lower_series, upper_series) \/ 2.0, index = districts_data.index)\n            \n#             districts_data = districts_data.drop(['pct_black\/hispanic', 'pct_free\/reduced', 'county_connections_ratio', 'pp_total_raw'], axis = 1)\n#             return districts_data\n        \n#         districts_data = pd.read_csv('\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\n        \n#         districts_data = districts_data_preprocessing(districts_data)\n#         districts_data = districts_data.drop(get_all_na_idx(districts_data.drop('district_id', axis = 1))).reset_index(drop = True)\n#         districts_data = add_dummys(districts_data, ['locale'], remove_orig_dummy_cols = False)\n        \n#         #all_states = districts_data['state'].unique()\n#         #district_id_state_map = dict(districts_data[['district_id', 'state']].values)\n#         #state_district_id_map = dict([(x, [y for y in district_id_state_map if district_id_state_map[y] == x]) for x in all_states])\n#         #districts_data = districts_data.drop('state', axis = 1)\n        \n#         districts_data = reduce_dtype_size(districts_data)\n#         return districts_data\n\n#     districts_data = load_districts_data()\n    \n#     # URL Information Extraction\n#     #   For July 2021\n#     #   Using similarweb.com\n#     #   Avg Duration - in seconds\n#     #   Total Visits - in 1000s\n#     def load_url_html_data():\n#         url_html_dict = {}\n\n#         all_html_content = \"\"\n#         with open(f'.\/Data\/url_info\/combined_url_info_data.txt', 'r') as html_file:\n#             all_html_content = html_file.read()\n\n#         tot_num_files = len([x for x in all_html_content.split('---') if len(x.strip()) != 0])\n#         print(f\"Total No. of files: {tot_num_files}\\n\\n\")\n\n#         for html_content_idx, html_content in enumerate(all_html_content.split('---')):\n            \n#             html_content = html_content.strip()\n#             if(len(html_content) == 0):\n#                 print(\"ERROR: URL not found\")\n#                 sys.exit(\"\")\n            \n#             url_name = html_content.split('<')[0].strip()\n#             html_content = '<'.join(html_content.split('<')[1:])\n#             print(f\"{html_content_idx + 1}. File: {url_name}\\n\")\n#             if(url_name in url_html_dict):\n#                 print(\"ERROR: Name already exists\")\n#                 sys.exit(\"\")\n#             url_html_dict[url_name] = {}\n            \n#             # Global Rank\n#             global_rank = re.findall('\\\"GlobalRank\":\\[\\d+,\\d+,-?\\d+,\\d+\\]', html_content)\n#             if(len(global_rank) != 1):\n#                 print(\"ERROR: Global Rank\")\n#                 print(global_rank)\n#                 sys.exit(\"\")\n#             global_rank = int(global_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['global_rank'] = global_rank\n\n#             # Country\n#             if('<img class=\"websiteRanks-titleIconImg\" src=\"\/images\/flags-svg\/flag-icon-us.svg\">' in html_content):\n#                 country = 'USA'\n#             else:\n#                 print(\"Country: Not USA!\")\n#                 country = 'Not_USA'\n#             url_html_dict[url_name]['country'] = country\n            \n#             # Country Rank\n#             country_rank = re.findall('\\\"CountryRanks\":\\{\"\\d+\":\\[\\d+,\\d+,-?\\d+,\\d+\\]\\}', html_content)\n#             if(len(country_rank) != 1):\n#                 print(\"ERROR: Country Rank\")\n#                 print(country_rank)\n#                 sys.exit(\"\")\n#             country_rank = int(country_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['country_rank'] = country_rank\n            \n#             # Category\n#             category = re.findall('<a class=\"websiteRanks-nameText\" data-analytics-category=\"Internal Link\" data-analytics-label=\"Category Rank\/.+\" href=\"\/top-websites\/category\/.+\" itemprop=\"significantLink\">.+<\/a>', html_content)\n#             if(len(category) != 1):\n#                 print(\"ERROR: Category\")\n#                 print(category)\n#                 url_html_dict[url_name]['main_category'] = np.nan\n#                 url_html_dict[url_name]['sub_category'] = np.nan\n#             else:\n#                 category = category[0].split('Category Rank\/')[1].split('\"')[0]\n#                 main_category = category.split('\/')[0]\n#                 url_html_dict[url_name]['main_category'] = main_category\n#                 if(len(category.split('\/')) != 1):\n#                     sub_category = category.split('\/')[-1]\n#                 else:\n#                     sub_category = \"\"\n#                 url_html_dict[url_name]['sub_category'] = sub_category\n\n#             # Category Rank\n#             category_rank = re.findall('\\\"CategoryRank\\\":\\[\\d+,\\d+,-?\\d+,\\d+\\]', html_content)\n#             if(len(category_rank) != 1):\n#                 print(\"ERROR: Category Rank\")\n#                 print(category_rank)\n#                 sys.exit(\"\")\n#             category_rank = int(category_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['category_rank'] = category_rank\n\n#             # Total Visits\n#             total_visits = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">.*\\d+[KMB]<\/span>', html_content)\n#             if(len(total_visits) != 1):\n#                 print(\"ERROR: Total Visits\")\n#                 print(total_visits)\n#                 url_html_dict[url_name]['total_visits'] = np.nan\n#             else:\n#                 total_visits = total_visits[0].split('>')[1].split('<')[0]\n#                 units = total_visits[-1]\n#                 total_visits = float(''.join([x for x in total_visits if x.isdigit()]))\n#                 if(units == 'K'):\n#                     total_visits = total_visits * 1\n#                 elif(units == 'M'):\n#                     total_visits = total_visits * 1000\n#                 elif(units == 'B'):\n#                     total_visits = total_visits * 1000000\n#                 url_html_dict[url_name]['total_visits'] = total_visits\n\n#             # Avg Duration\n#             avg_duration = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+:\\d+:\\d+<\/span>', html_content)\n#             if(len(avg_duration) != 1):\n#                 print(\"ERROR: Avg Duration\")\n#                 print(avg_duration)\n#                 url_html_dict[url_name]['avg_duration'] = np.nan\n#             else:\n#                 avg_duration = avg_duration[0].split('>')[1].split('<')[0]\n#                 hr_val, min_val, sec_val = avg_duration.split(':')\n#                 avg_duration = 3600 * int(hr_val) + 60 * int(min_val) + int(sec_val)\n#                 url_html_dict[url_name]['avg_duration'] = avg_duration\n\n#             # Page Visits\n#             page_visits = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+\\.\\d+<\/span>', html_content)\n#             if(len(page_visits) != 1):\n#                 print(\"ERROR: Page Visits\")\n#                 print(page_visits)\n#                 url_html_dict[url_name]['page_visits'] = np.nan\n#             else:\n#                 page_visits = float(page_visits[0].split('>')[1].split('<')[0])\n#                 url_html_dict[url_name]['page_visits'] = page_visits\n\n#             # Bounce Rate\n#             bounce_rate = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+.\\d+%<\/span>', html_content)\n#             if(len(bounce_rate) != 1):\n#                 print(\"ERROR: Bounce Rate\")\n#                 print(bounce_rate)\n#                 url_html_dict[url_name]['bounce_rate'] = np.nan\n#             else:\n#                 bounce_rate = float(bounce_rate[0].split('>')[1].split('%')[0])\n#                 url_html_dict[url_name]['bounce_rate'] = bounce_rate\n            \n#             # Description\n#             description = re.findall('<p itemprop=\"description\" class=\"websiteHeader-companyDescription js-companyDescription\">.+<\/p>', html_content)\n#             if(len(description) != 1):\n#                 url_html_dict[url_name]['description'] = np.nan\n#             else:\n#                 description = description[0].split('>')[1].split('<')[0]\n#                 url_html_dict[url_name]['description'] = description\n\n#         url_html_df = pd.DataFrame.from_dict(url_html_dict, orient = 'index').reset_index(drop = False)\n#         url_html_df.columns = ['URL'] + [*url_html_df.columns][1:]\n\n#         url_html_df['global_rank'] = url_html_df['global_rank'].replace({0: np.nan})\n#         url_html_df['country_rank'] = url_html_df['country_rank'].replace({0: np.nan})\n#         url_html_df['category_rank'] = url_html_df['category_rank'].replace({0: np.nan})\n\n#         additional_data = pd.read_csv('.\/Data\/url_info\/url_info_mobile.csv')\n#         url_html_df = pd.concat([url_html_df, additional_data], axis = 0)\n\n#         def find_subpage_level(url):\n#             level = url.split(':\/\/')[1]\n#             level = level.split('\/')\n#             level = [x for x in level if len(x.strip()) > 0]\n#             return len(level) - 1\n#         url_html_df['URL_subpage_level'] = url_html_df['URL'].apply(find_subpage_level)\n#         url_html_df['URL_subpage_visits'] = url_html_df.apply(lambda x: x['total_visits'] * (((100 - x['bounce_rate']) \/ 100) ** x['URL_subpage_level']), axis = 1)\n#         url_html_df['URL_page_duration'] = url_html_df['avg_duration'] \/ url_html_df['page_visits']\n#         url_html_df['URL_subpage_total_browsing_days'] = url_html_df['URL_subpage_visits'] * url_html_df['URL_page_duration'] \/ 60 \/ 60 \/ 24\n#         url_html_df['URL_subpage_avg_browsing_days'] = url_html_df['URL_subpage_total_browsing_days'] \/ 31\n#         url_html_df.to_csv('.\/Data\/url_info\/final_url_data.csv', index = False)\n    \n#     def load_products_data():\n    \n#         def products_data_preprocessing(products_data):\n\n#             products_data['Primary Category'] = products_data['Primary Essential Function'].apply(lambda x: x.split(' - ')[0] if pd.isna(x) == False else np.nan)\n#             products_data['Primary Category'] = products_data['Primary Category'].map({'LC': 'LC', 'CM': 'CM', 'SDO': 'SDO', 'LC\/CM\/SDO': 'Other'})\n#             products_data['Primary Essential Function'] = products_data['Primary Essential Function'].apply(lambda x: x.split(' - ')[1] if pd.isna(x) == False else np.nan)\n            \n#             def sector_map(sectors):\n                \n#                 sector_prek12 = []\n#                 sector_higher_ed = []\n#                 sector_corporate = []\n                \n#                 for sector in sectors:\n#                     if(pd.isna(sector) == True):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12; Higher Ed'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12; Higher Ed; Corporate'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(1)\n#                     elif(sector == 'Corporate'):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(1)\n#                     elif(sector == 'Higher Ed; Corporate'):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(1)\n#                     else:\n#                         print(f\"***\\nUnknown sector detected! {sector}\\n***\")\n                \n#                 return sector_prek12, sector_higher_ed, sector_corporate\n            \n#             sector_prek12, sector_higher_ed, sector_corporate = sector_map(products_data['Sector(s)'])\n#             products_data = products_data.assign(Sector_prek12 = sector_prek12)\n#             products_data = products_data.assign(Sector_higher_ed = sector_higher_ed)\n#             products_data = products_data.assign(Sector_corporate = sector_corporate)\n            \n#             products_data['Primary Essential Function'] = products_data['Primary Essential Function'].replace({\"Sites, Resources & References\": \"Sites, Resources & Reference\"})\n            \n#             # Correcting small mistakes\n#             products_data['URL'] = products_data['URL'].replace({'https:\/\/fligprid.com': 'https:\/\/flipgrid.com'})\n\n#             if(whether_load_url_html_data == True):\n#                 load_url_html_data()\n#             url_info_data = pd.read_csv('.\/Data\/url_info\/final_url_data.csv')\n#             url_info_data.columns = ['mainURL_' + x if x != 'URL' else x for x in url_info_data.columns]\n#             products_data = pd.merge(products_data, url_info_data, how = 'left', on = 'URL')\n            \n#             products_data['mainURL_country_rank'] = products_data.apply(lambda x: x if x['mainURL_country'] == 'USA' else np.nan, axis = 1)\n            \n#             return products_data\n        \n#         products_data = pd.read_csv('.\/Data\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')\n        \n#         products_data = products_data_preprocessing(products_data)\n#         products_data = add_dummys(products_data, ['Sector(s)', 'Primary Category', 'Primary Essential Function', 'mainURL_main_category'], remove_orig_dummy_cols = False)\n        \n#         products_data = reduce_dtype_size(products_data)\n#         return products_data\n\n#     products_data = load_products_data()\n\n#     # Not focusing on vacation dates since time-analysis is not priority\n\n#     def load_dates_data():\n#         days_2020 = pd.date_range(datetime(2020, 1, 1), datetime(2020, 12, 31))\n#         dates_data = pd.DataFrame.from_dict({'date': days_2020})\n        \n#         dates_data['month'] = dates_data['date'].apply(lambda x: x.month)\n#         dates_data['day'] = dates_data['date'].apply(lambda x: x.day)\n#         dates_data['day_of_week'] = dates_data['date'].apply(lambda x: calendar.day_name[x.weekday()])\n#         dates_data['is_weekend'] = dates_data['day_of_week'].apply(lambda x: 1 if (x == 'Saturday') | (x == 'Sunday') else 0)\n        \n#         us_holidays = pd.read_csv('.\/Data\/US Holiday Dates (2004-2021).csv', usecols = ['Date', 'Holiday'], parse_dates = ['Date'])\n#         dates_data = pd.merge(dates_data, us_holidays, how = 'left', left_on = 'date', right_on = 'Date')\n#         dates_data = dates_data.drop('Date', axis = 1)\n#         dates_data['is_holiday'] = dates_data['Holiday'].apply(lambda x: 0 if pd.isnull(x) == True else 1)\n        \n#         dates_data = add_dummys(dates_data, ['day_of_week', 'Holiday'], remove_orig_dummy_cols = False)\n#         dates_data = reduce_dtype_size(dates_data)\n#         return dates_data\n\n#     dates_data = load_dates_data()\n\n#     def load_engagement_data():\n\n#         engagement_data = pd.DataFrame()\n#         districts = []\n        \n#         for x in os.listdir('.\/Data\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/'):\n#             data_x = pd.read_csv(f'.\/Data\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/{x}', parse_dates = ['time'])\n#             engagement_data = pd.concat([engagement_data, data_x], axis = 0)\n#             districts.extend([int(x.split('.')[0])] * data_x.shape[0])\n        \n#         engagement_data['district_id'] = pd.Series(districts, index = engagement_data.index)\n        \n#         top_products_id = list(products_data['LP ID'].unique())\n#         districts_id = list(districts_data['district_id'].unique())\n#         engagement_data = engagement_data[engagement_data['lp_id'].isin(top_products_id)]\n#         engagement_data = engagement_data[engagement_data['district_id'].isin(districts_id)]\n        \n#         same_url_map = {\n#             33562: 75206,\n#             87841: 35971\n#         }\n#         engagement_data['lp_id'] = engagement_data['lp_id'].replace(same_url_map)\n#         engagement_data = engagement_data.groupby(['time', 'lp_id', 'district_id'])[['pct_access', 'engagement_index']].aggregate(np.nansum).reset_index()\n        \n#         engagement_data = reduce_dtype_size(engagement_data)\n#         return engagement_data\n\n#     engagement_data = load_engagement_data()\n\n#     # # Saving all datasets\n#     districts_data.to_csv('.\/Data\/Processed_Dataset\/districts_data.csv', index = False)\n#     products_data.to_csv('.\/Data\/Processed_Dataset\/products_data.csv', index = False)\n#     dates_data.to_csv('.\/Data\/Processed_Dataset\/dates_data.csv', index = False)\n#     engagement_data.to_csv('.\/Data\/Processed_Dataset\/engagement_data.csv', index = False)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nCOLOR_WHITE = '#F8F1FF'\nCOLOR_BLACK = '#231942'\nCOLOR_DARK_BLUE = '#156BB7'\nCOLOR_LIGHT_BLUE = '#63D1DF'\nCOLOR_GREEN = '#30DB8D'\nCOLOR_DARK_GREEN = '#0DAB6C'\nCOLOR_ORANGE = '#FBAB60'\nCOLOR_YELLOW = '#F8E16C'\nCOLOR_RED = '#DA4167'\n\nPLOT_THEME_LIGHT = {\n    'text': COLOR_BLACK,\n    'axis': COLOR_BLACK,\n    'subtitle': COLOR_DARK_BLUE,\n    'color+1': COLOR_DARK_GREEN,\n    'color+2': COLOR_YELLOW,\n    'color+3': COLOR_ORANGE,\n    'color+4': COLOR_DARK_BLUE,\n    'color-1': COLOR_RED,\n    'bg': COLOR_LIGHT_BLUE,\n    'inv': COLOR_WHITE,\n    'color+1_lower': '#064B30',\n    'color+1_higher': '#2EEFA2',\n    'gray': '#676076',\n}\nPLOT_THEME_LIGHT['groups'] = [PLOT_THEME_LIGHT[x] for x in ['color+1', 'color-1', 'color+3', 'color+4', 'color+2']]\n\nPLOT_THEME_DARK = {\n    'text': COLOR_WHITE,\n    'axis': COLOR_WHITE,\n    'subtitle': COLOR_LIGHT_BLUE,\n    'color+1': COLOR_GREEN,\n    'color+2': COLOR_YELLOW,\n    'color+3': COLOR_ORANGE,\n    'color+4': COLOR_LIGHT_BLUE,\n    'color-1': COLOR_RED,\n    'bg': COLOR_DARK_BLUE,\n    'inv': COLOR_BLACK,\n    'color+1_lower': '#188B57',\n    'color+1_higher': '#86EABD',\n    'gray': '#D6CFDB',\n}\nPLOT_THEME_DARK['groups'] = [PLOT_THEME_DARK[x] for x in ['color+1', 'color-1', 'color+3', 'color+4', 'color+2']]\n\ndef create_fig(nrows = 1, ncols = 1, width = 10, height = 5):\n    fig, ax = plt.subplots(nrows, ncols, figsize = (width, height))\n    return fig, ax\n\ndef remove_spines(ax, theme = {}):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_color(theme['axis'])\n    ax.spines['left'].set_color(theme['axis'])\n    return ax\n\ndef remove_all_spines(ax):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    return ax\n\ndef set_titles_and_labels(fig, ax, fig_title = \"\", title = \"\", xlabel = \"\", ylabel = \"\", theme = {}):\n    fig.suptitle(fig_title, fontsize = 30, color = theme['text'])\n    ax.set_title(title, fontsize = 20, color = theme['subtitle'])\n    ax.set_xlabel(xlabel, fontsize = 15, color = theme['text'])\n    ax.set_ylabel(ylabel, fontsize = 15, color = theme['text'])\n    return fig, ax\n\ndef set_ticks(ax, theme):\n    ax.tick_params(axis = 'x', colors = theme['axis'])\n    ax.tick_params(axis = 'y', colors = theme['axis'])\n    return ax\n\ndef set_xticklabels(ax, labels, rotate_x = 0, theme = {}):\n    ax.set_xticks(np.arange(len(labels)))\n    ax.set_xticklabels(labels, color = theme['text'], rotation = rotate_x)\n    return ax\n\ndef set_yticklabels(ax, labels, rotate_y = 0, theme = {}):\n    ax.set_yticks(np.arange(len(labels)))\n    ax.set_yticklabels(labels, color = theme['text'], rotation = rotate_y)\n    return ax\n\ndef set_bg(fig, ax, theme):\n    fig.set_facecolor(theme['bg'])\n    ax.set_facecolor(theme['bg'])\n    return fig, ax\n\ndef select_theme(theme):\n    if(theme == 'DARK'):\n        return PLOT_THEME_DARK\n    else:\n        return PLOT_THEME_LIGHT\n\ndef set_legend(ax, theme):\n    ax.legend(loc = 'best')\n    return ax\n\ndef plot_decoration():\n    return \"\"\"\n    fig, ax = set_bg(fig, ax, theme); ax = set_ticks(ax, theme); ax = remove_spines(ax, theme); fig, ax = set_titles_and_labels(fig, ax, suptitle, title, xlabel, ylabel, theme);\n    \"\"\".strip()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_lineplot(x_vals, y_vals, width = 15, height = 7, labels = [], suptitle = \"Lineplot\", title = \"Demo\", xlabel = \"\", ylabel = \"\", theme = 'DARK'):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    if(len(y_vals) > len(theme['groups'])):\n        group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(y_vals))]\n    else:\n        group_colors = theme['groups'][:len(y_vals)]\n    for y_val_idx, y_val in enumerate(y_vals):\n        if(len(labels) == 0):\n            ax.plot(x_vals, y_val, lw = 3, color = group_colors[y_val_idx], label = f'line #{y_val_idx + 1}')\n        else:\n            ax.plot(x_vals, y_val, lw = 3, color = group_colors[y_val_idx], label = labels[y_val_idx])\n    ax = set_legend(ax, theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_barplot(x_names, y_vals, cats = [], width = 15, height = 7, suptitle = \"Barplot\", title = \"Demo\", xlabel = '', ylabel = '', theme = 'DARK', rotate_x = 0, rotate_y = 0):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    x_vals = np.arange(len(x_names))\n    if(len(cats) > 0):\n        uniq_cats = list(sorted(pd.Series(cats).unique()))\n        if(len(uniq_cats) > len(theme['groups'])):\n            group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\n        else:\n            group_colors = theme['groups'][:len(uniq_cats)]\n        group_colors = [group_colors[uniq_cats.index(x)] for x in cats]\n    else:\n        if(len(y_vals) > len(theme['groups'])):\n            group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(y_vals))]\n        else:\n            group_colors = theme['groups'][:len(y_vals)]\n    ax.bar(x_vals, y_vals, color = group_colors)\n    ax = set_xticklabels(ax, x_names, rotate_x = rotate_x, theme = theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_scatterplot(x_vals, y_vals, cats = [1], width = 15, height = 7, suptitle = \"Scatterplot\", title = 'Demo', xlabel = '', ylabel = '', theme = 'DARK', annotate = False, annotate_texts = []):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    no_cats_passed = False\n    if(len(cats) == 1):\n        cats = np.ones(len(x_vals))\n        no_cats_passed = True\n    uniq_cats = pd.Series(cats).unique()\n    if(len(uniq_cats) > len(theme['groups'])):\n        group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\n    else:\n        group_colors = theme['groups'][:len(uniq_cats)]\n    for cat_idx, cat in enumerate(uniq_cats):\n        ax.scatter(x_vals[cats == cat], y_vals[cats == cat], color = group_colors[cat_idx], label = cat)\n    if(annotate == True):\n        for idx in range(len(x_vals)):\n            ax.annotate(annotate_texts[idx], (x_vals[idx], y_vals[idx]), color = theme['text'])\n    if(no_cats_passed == False):\n        ax = set_legend(ax, theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_us_map(state_vals_df, title, val_col, val_label, range_min_val = 0, range_max_val = 1, theme = 'DARK', state_col = 'STATE_ABBR'):\n    \n    theme = select_theme(theme)\n\n    layout = dict(\n        font_family = 'Source Sans Pro',\n        font_color = theme['text'],\n        title_text = title,\n        # To change\n        title_font = dict(\n            family = \"Source Sans Pro\",\n            size = 25,\n            color = theme['axis']\n        ),\n        geo_scope = 'usa',\n        paper_bgcolor = theme['bg'],\n        geo_bgcolor = theme['bg'],\n        geo = dict(\n            landcolor = theme['inv'],\n            subunitcolor = theme['gray'],\n            lakecolor = theme['bg'],\n        ),\n    )\n\n    fig = px.choropleth(\n        state_vals_df,\n        locations = state_col,\n        color = val_col,\n        color_continuous_scale = [theme['color-1'], theme['color+1_higher']],\n        range_color = (range_min_val, range_max_val),\n        locationmode = \"USA-states\",\n        labels = {val_col : val_label, state_col: 'State'},\n    )\n\n    fig.update_layout(layout)\n    fig.update_layout(margin = {\"r\": 0, \"l\": 0, \"b\": 15})\n    fig.show()\n    \nDATA = load_main_dataset()","0def49ba":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport sys\nimport time\n\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import pearsonr\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.factorplots import interaction_plot\n\nfrom IPython.display import HTML, display\nimport tabulate\n\ndata1 = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/USBroadbandUsagePercentages_copy.csv')\ndata1 = data1.groupby('ST')[['BROADBAND AVAILABILITY PER FCC', 'BROADBAND USAGE']].aggregate(np.nanmean).reset_index()\n\ndata2 = pd.read_excel('\/kaggle\/input\/learnplatform-analysis-data\/publicdata_imls_metrics.xlsx', sheet_name = 'State Data REV')\ndata2 = data2.drop('GEO_ID', axis = 1)\ndata2['State_Code'] = data2['State'].map(STATE_ABBR)\n\ndata3 = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/broadband_data_opendatachallenge.csv')\ndata3 = data3.groupby('State')[[x for x in data3.columns if x not in ['Zip', 'County', 'State']]].aggregate(np.nanmean).reset_index()\ndata3 = data3[data3['State'] != 'Palau']\ndata3['State_Code'] = data3['State'].map(STATE_ABBR)\n\ndata = pd.merge(data2, data3, how = 'inner', on = 'State_Code')\ndata = pd.merge(data, data1, how = 'inner', left_on = 'State_Code', right_on = 'ST')\ndata = data.drop(['ST', 'State_x', 'State_y'], axis = 1)\n\nus_state_area_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/us_state_area.csv', delimiter = '\\t')\nus_state_area_data['Sq. Mi.'] = us_state_area_data['Sq. Mi.'].apply(lambda x: int(x.replace(',', '')))\nus_state_area_data = us_state_area_data.drop('Sq. Km.', axis = 1)\nus_state_area_data['State and other areas'] = us_state_area_data['State and other areas'].map(STATE_ABBR)\ndata = pd.merge(data, us_state_area_data, how = 'inner', left_on = 'State_Code', right_on = 'State and other areas')\n\ncols_requiring_density = [\n    'Number of Broadband providers (2019)', 'WiredCount_2020', 'Fwcount_2020', 'AllProviderCount_2020',\n    'Wired25_3_2020', 'Wired100_3_2020', 'All25_3_2020', 'All100_3'\n]\nfor col in data.columns:\n    if(col in cols_requiring_density):\n        data[col + '_density'] = data[col] \/ np.log10(data['Sq. Mi.'])\ndata = data.drop(cols_requiring_density, axis = 1)\n\ndata = data.rename({\n    'Number of Broadband providers (2019)_density': 'Number of Broadband providers',\n    'Population for whom broadband available, 2019 (%)': 'Population_broadband available',\n    'Percent without health insurance (2018)': 'Percent without health insurance',\n    'Percent with no home computer (2018)': 'Percent with no computer',\n    'Percent with no home Internet (2018)': 'Percent with no Internet',\n    'Lowest broadband cost per month, 2019 ($)': 'Lowest broadband cost',\n    'Lowest Priced Terrestrial Broadband Plan': 'Lowest Priced Terrestrial',\n    'BROADBAND AVAILABILITY PER FCC': 'FCC_Availability'\n}, axis = 1)\n\navailability_feats = [\n    'Number of Broadband providers', 'Population_broadband available',\n    'WiredCount_2020_density', 'Fwcount_2020_density', 'AllProviderCount_2020_density',\n    'Wired25_3_2020_density', 'Wired100_3_2020_density', 'All25_3_2020_density', 'All100_3_density',\n    'AverageMbps', 'FastestAverageMbps', 'FCC_Availability'\n]\naffordability_feats = [\n    'Unemployment rate 2019', 'Percent without health insurance', 'Poverty Rate (%)',\n    'Percent received SNAP (2018)', 'Percent with no computer',\n    'Percent with no Internet', 'Percent with home Broadband', 'BROADBAND USAGE',\n    'Lowest broadband cost', 'Lowest Priced Terrestrial',\n]\n\n# IMPORTANT:\n# Alaska not considered in analysis since the extremely large area of Alaska dominates over other features\n\nfrom sklearn.decomposition import PCA\n\nbroadband_metrics_names = []\nbroadband_metrics = []\n\ncats = [\n    [1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1],\n    [1, 1, 0, 1, 0, 0, 0, 0, 1, 1]\n]\nfeat_names = ['Availability', 'Affordability']\n\nfor feat_idx, feats in enumerate([availability_feats, affordability_feats]):\n\n    data_for_pca = data[data['State_Code'] != 'AK'][feats]\n    for col in data_for_pca.columns:\n        data_for_pca[col] = (data_for_pca[col] - data_for_pca[col].mean()) \/ (data_for_pca[col].std() + 0.0000000000001)\n\n    pca = PCA(n_components = 10)\n    pca_orig_data = pca.fit_transform(data_for_pca)\n    for pca_idx in range(10):\n        pca_data = pca.components_[pca_idx]\n        expl_var = np.sum(pca.explained_variance_ratio_[:pca_idx + 1]) * 100\n        if(feat_idx == 0):\n            if(pca_data.sum() < 0):\n                pca_data = -pca_data\n                broadband_metrics.append(-pca_orig_data[:, pca_idx])\n            else:\n                broadband_metrics.append(pca_orig_data[:, pca_idx])\n        else:\n            if(pca_data.sum() > 0):\n                pca_data = -pca_data\n                broadband_metrics.append(-pca_orig_data[:, pca_idx])\n            else:\n                broadband_metrics.append(pca_orig_data[:, pca_idx])\n        \n        if(feat_idx == 0):\n            broadband_metrics_names.append(f'availability_{pca_idx+1}')\n        elif(feat_idx == 1):\n            broadband_metrics_names.append(f'affordability_{pca_idx+1}')\n\n        if(pca_idx == 0):\n            plot_barplot(\n                [x for x in feats], pca_data, cats = cats[feat_idx],\n                width = 15, height = 7,\n                suptitle = f\"Broadband {feat_names[feat_idx]}\", title = f\"PCA Component {pca_idx + 1}, Explained Variance = {expl_var:.2f}%\",\n                xlabel = \"Features\", ylabel = \"PCA Weights\",\n                rotate_x = 60,\n                theme = 'DARK',\n            )\n\n    #     sns.heatmap(pd.DataFrame(data_for_pca).corr())\n    #     plt.show()\n    \n        if(expl_var >= 70):\n            break","0bdc5fdc":"broadband_metrics = pd.DataFrame(np.asarray(broadband_metrics).transpose(), columns = broadband_metrics_names)\n#print(data.shape, social_vulnerability_metrics.shape)\nbroadband_data = pd.concat([data[['State_Code']], broadband_metrics], axis = 1)\nbroadband_data = broadband_data.groupby('State_Code')[broadband_metrics_names].aggregate(np.nanmean).reset_index(drop = False)\n\nplot_us_map(broadband_data, \"Broadband Availability - Statewise View\", \"availability_1\", \"PCA Component 1\", -10, 10, state_col = 'State_Code', theme = 'DARK')\nplot_us_map(broadband_data, \"Broadband Affordability - Statewise View\", \"affordability_1\", \"PCA Component 1\", -10, 10, state_col = 'State_Code', theme = 'DARK')","c401ee19":"engagement_data = DATA.groupby('state')[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\nengagement_data['state'] = engagement_data['state'].replace(STATE_ABBR)\nbroadband_engagement_data = pd.merge(engagement_data, broadband_data, how = 'inner', left_on = 'state', right_on = 'State_Code')\ntarget_cols = ['pct_access', 'engagement_index']\ncorrs = np.zeros((len(broadband_metrics_names), len(target_cols)))\ncorrs_pval = np.zeros((len(broadband_metrics_names), len(target_cols)))\nfor col_idx, col in enumerate(broadband_metrics_names):\n    for target_col_idx, target_col in enumerate(target_cols):\n        corrs[col_idx, target_col_idx], corrs_pval[col_idx, target_col_idx] = pearsonr(broadband_engagement_data[target_cols[target_col_idx]], broadband_engagement_data[broadband_metrics_names[col_idx]])\n\ntable = [\n    ['FEATURE', 'CORRELATION WITH PCT_ACCESS', 'CORRELATION WITH ENGAGEMENT_INDEX']\n]\nfor col_idx, col in enumerate(broadband_metrics_names):\n    table_row = []\n    table_row.append(f\"{col.split('_')[0]} (PCA-{col.split('_')[1]})\")\n    table_row.append(f\"{corrs[col_idx][0]:.2f} (pval = {corrs_pval[col_idx][0]:.2f})\")\n    table_row.append(f\"{corrs[col_idx][1]:.2f} (pval = {corrs_pval[col_idx][1]:.2f})\")\n    table.append(table_row)\n\ndisplay(HTML(tabulate.tabulate(table, tablefmt='html')))        \n\nfor row_idx in range(corrs.shape[0]):\n    for col_idx in range(corrs.shape[1]):\n        if(corrs_pval[row_idx, col_idx] < 0.05):\n            print(broadband_metrics_names[row_idx])\n            print(target_cols[col_idx])\n            print(corrs[row_idx, col_idx])\n            print(corrs_pval[row_idx, col_idx])\n            print()\n#corr_check_cols = ['E_UNEMP', 'EPL_UNEMP', 'F_POV', 'F_PCI', 'F_NOHSDP', 'F_THEME1']\ncorr_check_cols = availability_feats\ngrouped_data = data.groupby('State_Code')[corr_check_cols].aggregate(np.nanmean).reset_index(drop = False)\ndata2 = pd.merge(engagement_data, grouped_data, how = 'inner', left_on = 'state', right_on = 'State_Code')\nfor col in corr_check_cols:\n    pearson_corr = pearsonr(data2['pct_access'], data2[col])\n    #print(col, pearson_corr)\n    if(pearson_corr[1] < 0.05):\n        print(col, pearson_corr)\nfor col in corr_check_cols:\n    pearsonr_corr = pearsonr(data2['engagement_index'], data2[col])\n    #print(col, pearson_corr)\n    if(pearson_corr[1] < 0.05):\n        print(col, pearson_corr)\n#corr_check_cols = ['E_UNEMP', 'EPL_UNEMP', 'F_POV', 'F_PCI', 'F_NOHSDP', 'F_THEME1']\ncorr_check_cols = affordability_feats\ngrouped_data = data.groupby('State_Code')[corr_check_cols].aggregate(np.nanmean).reset_index(drop = False)\ndata2 = pd.merge(engagement_data, grouped_data, how = 'inner', left_on = 'state', right_on = 'State_Code')\nfor col in corr_check_cols:\n    pearson_corr = pearsonr(data2['pct_access'], data2[col])\n    #print(col, pearson_corr)\n    if(pearson_corr[1] < 0.05):\n        print(col, pearson_corr)\nfor col in corr_check_cols:\n    pearsonr_corr = pearsonr(data2['engagement_index'], data2[col])\n    #print(col, pearson_corr)\n    if(pearson_corr[1] < 0.05):\n        print(col, pearson_corr)","34c51711":"engagement_data = DATA.groupby('state')[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\nengagement_data['state'] = engagement_data['state'].replace(STATE_ABBR)\n\ndef select_website_categories(x):\n    if(x['mainURL_main_category'] == 'science-and-education'):\n        return 'Education'\n    elif(x['mainURL_main_category'] == 'arts-and-entertainment'):\n        return 'Entertainment'\n    elif(x['mainURL_sub_category'] == 'social-networks-and-online-communities'):\n        return 'Social Media'\n    elif(x['mainURL_main_category'] == 'games'):\n        return 'Games'\n    else:\n        return 'Other'\n\nengagement_data = pd.DataFrame()\nfor state in DATA['state'].unique():\n    engagement_state_data = DATA[DATA['state'] == state][['state', 'mainURL_main_category', 'mainURL_sub_category', 'pct_access', 'engagement_index']]\n    engagement_state_data['URL_category'] = engagement_state_data.apply(select_website_categories, axis = 1)\n    engagement_state_data = engagement_state_data.groupby(['state', 'URL_category'])[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\n    engagement_state_data['state'] = engagement_state_data['state'].replace(STATE_ABBR)\n    engagement_data = pd.concat([engagement_data, engagement_state_data], axis = 0)\n\nbroadband_engagement_data = pd.merge(engagement_data, broadband_data, how = 'inner', left_on = 'state', right_on = 'State_Code')\nbroadband_engagement_data['broadband_not_available'] = (broadband_engagement_data['availability_1'] < -1.5)\n\nfig, ax = plt.subplots(2, 2, figsize = (20, 10))\n\nmodel = ols('engagement_index ~ C(URL_category)*C(broadband_not_available)', broadband_engagement_data).fit()\n# print(f\"Overall model F({model.df_model: .0f},{model.df_resid: .0f}) = {model.fvalue: .3f}, p = {model.f_pvalue: .4f}\")\nres = sm.stats.anova_lm(model, typ= 2)\n#display(res)\n\nax[0][0] = interaction_plot(x=broadband_engagement_data['URL_category'], trace=broadband_engagement_data['broadband_not_available'], response=broadband_engagement_data['engagement_index'], ax = ax[0][0], colors = [COLOR_DARK_BLUE, COLOR_GREEN])\n\nmodel = ols('pct_access ~ C(URL_category)*C(broadband_not_available)', broadband_engagement_data).fit()\n# print(f\"Overall model F({model.df_model: .0f},{model.df_resid: .0f}) = {model.fvalue: .3f}, p = {model.f_pvalue: .4f}\")\nres = sm.stats.anova_lm(model, typ= 2)\n#display(res)\n\nax[1][0] = interaction_plot(x=broadband_engagement_data['URL_category'], trace=broadband_engagement_data['broadband_not_available'], response=broadband_engagement_data['pct_access'], ax = ax[1][0], colors = [COLOR_DARK_BLUE, COLOR_GREEN])\n\nbroadband_engagement_data['broadband_not_affordable'] = (broadband_engagement_data['affordability_1'] < -1.5)\nbroadband_engagement_data.head()\n\nmodel = ols('engagement_index ~ C(URL_category)*C(broadband_not_affordable)', broadband_engagement_data).fit()\n# print(f\"Overall model F({model.df_model: .0f},{model.df_resid: .0f}) = {model.fvalue: .3f}, p = {model.f_pvalue: .4f}\")\nres = sm.stats.anova_lm(model, typ= 2)\n# display(res)\n\nax[0][1] = interaction_plot(x=broadband_engagement_data['URL_category'], trace=broadband_engagement_data['broadband_not_affordable'], response=broadband_engagement_data['engagement_index'], ax = ax[0][1], colors = [COLOR_DARK_BLUE, COLOR_GREEN])\n\nmodel = ols('pct_access ~ C(URL_category)*C(broadband_not_affordable)', broadband_engagement_data).fit()\n# print(f\"Overall model F({model.df_model: .0f},{model.df_resid: .0f}) = {model.fvalue: .3f}, p = {model.f_pvalue: .4f}\")\nres = sm.stats.anova_lm(model, typ= 2)\n# display(res)\n\nax[1][1] = interaction_plot(x=broadband_engagement_data['URL_category'], trace=broadband_engagement_data['broadband_not_affordable'], response=broadband_engagement_data['pct_access'], ax = ax[1][1], colors = [COLOR_DARK_BLUE, COLOR_GREEN])\n\nax[0][0].axes[0].set_xlabel(\"\")\nax[0][1].axes[1].set_xlabel(\"\")\nax[1][0].axes[2].set_xlabel(\"\")\nax[1][1].axes[3].set_xlabel(\"\")\nfig.suptitle(\"Engagement vs. Broadband Access (for different kinds of EdTech apps)\", fontsize = 15)\nplt.show()","f864445c":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport sys\n\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import ConvexHull\n\nfrom scipy import stats\nfrom statsmodels.stats import weightstats as stests\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom scipy.spatial import Delaunay\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nproducts_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/products_data.csv')\n\ndef check_inside_polygon(x, y, labels):\n    keep_idx = []\n    for idx in range(len(x)):\n        if(np.isnan(x[idx]) == False and np.isnan(y[idx]) == False):\n            keep_idx.append(idx)\n    non_na_x = x[keep_idx]\n    non_na_y = y[keep_idx]\n    non_na_labels = labels[keep_idx]\n    convex_hulls = []\n    uniq_labels = list(pd.Series(labels).unique())\n    for label in uniq_labels:\n        cat_x = non_na_x[non_na_labels == label]\n        cat_y = non_na_y[non_na_labels == label]\n        p = np.c_[cat_x, cat_y]\n        convex_hulls.append(Delaunay(p))\n    num_clusters = 0\n    for idx in range(len(non_na_x)):\n        for hull in convex_hulls:\n            num_clusters += (hull.find_simplex([non_na_x[idx], non_na_y[idx]]) != -1)\n    return num_clusters \/ len(non_na_x)\n\n# plot_scatterplot(\n#     np.log10(products_data['mainURL_URL_subpage_visits'].values),\n#     np.log10(products_data['mainURL_URL_page_duration'].values),\n#     width = 18, height = 10,\n#     cats = products_data['Primary Category'].fillna('Other').values,\n#     suptitle = 'EdTech Products Popularity (using given categories)', title = \"Poor segregation\", xlabel = \"Visits\", ylabel = \"Duration\",\n#     annotate = False, annotate_texts = products_data['Product Name'].values,\n#     theme = 'DARK'\n# )\n\nx_vals = np.log10(products_data['mainURL_URL_subpage_visits'].values)\ny_vals = np.log10(products_data['mainURL_URL_page_duration'].values)\ncats = [1]\nwidth = 18\nheight = 10\nsuptitle = \"EdTech Products Popularity\"\ntitle = '(Based on stats crawled from similarweb.com)'\nxlabel = 'log (Monthly Visits)'\nylabel = 'log (Visit Duration in seconds)'\ntheme = 'DARK'\nannotate = False\nannotate_texts = []\n\nannotate = True\nannotate_texts = products_data['Product Name'].values.copy()\nfor idx in range(len(annotate_texts)):\n    if((x_vals[idx] >= 2.5) & (x_vals[idx] <= 8) & (y_vals[idx] <= 2.2) & (y_vals[idx] >= 1.25)):\n        annotate_texts[idx] = ''\n\ntheme = select_theme(theme)\nfig, ax = create_fig(1, 1, width, height)\nno_cats_passed = False\nif(len(cats) == 1):\n    cats = np.ones(len(x_vals))\n    no_cats_passed = True\nuniq_cats = pd.Series(cats).unique()\nif(len(uniq_cats) > len(theme['groups'])):\n    group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\nelse:\n    group_colors = theme['groups'][:len(uniq_cats)]\nfor cat_idx, cat in enumerate(uniq_cats):\n    ax.scatter(x_vals[cats == cat], y_vals[cats == cat], color = group_colors[cat_idx], label = cat)\nif(annotate == True):\n    for idx in range(len(x_vals)):\n        ax.annotate(annotate_texts[idx], (x_vals[idx], y_vals[idx]), color = theme['text'])\nif(no_cats_passed == False):\n    ax = set_legend(ax, theme)\nexec(plot_decoration())\n\nplt.show()","13a04c58":"# plot_scatterplot(\n#     np.log10(products_data['mainURL_URL_subpage_visits'].values),\n#     np.log10(products_data['mainURL_URL_page_duration'].values),\n#     width = 18, height = 10,\n#     cats = products_data['Primary Category'].fillna('Other').values,\n#     suptitle = 'EdTech Products Popularity (using given categories)', title = \"Poor segregation\", xlabel = \"Visits\", ylabel = \"Duration\",\n#     annotate = False, annotate_texts = products_data['Product Name'].values,\n#     theme = 'DARK'\n# )\n\nx_vals = np.log10(products_data['mainURL_URL_subpage_visits'].values)\ny_vals = np.log10(products_data['mainURL_URL_page_duration'].values)\ncats = products_data['Primary Category'].fillna('Other').values\nwidth = 18\nheight = 10\nsuptitle = \"EdTech Products Popularity (using provided categories)\"\nxlabel = 'log (Monthly Visits)'\nylabel = 'log (Visit Duration in seconds)'\ntheme = 'DARK'\nannotate = False\nannotate_texts = []\n\ntitle = f\"Overlap Score: {check_inside_polygon(x_vals, y_vals, cats):.2f}\"\n\n# https:\/\/stackoverflow.com\/questions\/44575681\/how-do-i-encircle-different-data-sets-in-scatter-plot\ndef encircle(x,y, ax, **kw):\n    keep_idx = []\n    for idx in range(len(x)):\n        if(np.isnan(x[idx]) == False and np.isnan(y[idx]) == False):\n            keep_idx.append(idx)\n    x = x[keep_idx]\n    y = y[keep_idx]\n    p = np.c_[x,y]\n    hull = ConvexHull(p)\n    poly = plt.Polygon(p[hull.vertices,:], **kw)\n    ax.add_patch(poly)\n\ntheme = select_theme(theme)\nfig, ax = create_fig(1, 1, width, height)\nno_cats_passed = False\nif(len(cats) == 1):\n    cats = np.ones(len(x_vals))\n    no_cats_passed = True\nuniq_cats = list(sorted(pd.Series(cats).unique()))\nif(len(uniq_cats) > len(theme['groups'])):\n    group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\nelse:\n    group_colors = theme['groups'][:len(uniq_cats)]\nfor cat_idx, cat in enumerate(uniq_cats):\n    ax.scatter(x_vals[cats == cat], y_vals[cats == cat], color = group_colors[cat_idx], label = cat)\n    encircle(x_vals[cats == cat], y_vals[cats == cat], ax = ax, ec = group_colors[cat_idx], fc = group_colors[cat_idx], alpha = 0.5)\nif(annotate == True):\n    for idx in range(len(x_vals)):\n        ax.annotate(annotate_texts[idx], (x_vals[idx], y_vals[idx]), color = theme['text'])\nif(no_cats_passed == False):\n    ax = set_legend(ax, theme)\nexec(plot_decoration())\n\nplt.show()","4e92ff37":"def select_website_categories(x):\n    if(x['mainURL_main_category'] == 'science-and-education'):\n        return 'Education'\n    elif(x['mainURL_main_category'] == 'arts-and-entertainment'):\n        return 'Entertainment'\n    elif(x['mainURL_sub_category'] == 'social-networks-and-online-communities'):\n        return 'Social Media'\n    elif(x['mainURL_main_category'] == 'games'):\n        return 'Games'\n    else:\n        return 'Other'\ncats = products_data.apply(select_website_categories, axis = 1)\n\n# plot_scatterplot(\n#     np.log10(products_data['mainURL_URL_subpage_visits'].values),\n#     np.log10(products_data['mainURL_URL_page_duration'].values),\n#     width = 18, height = 18,\n#     cats = cats.fillna('Other').values,\n#     suptitle = 'EdTech Products Popularity (using newly extracted categories)', title = \"Much better segregation\", xlabel = \"Visits\", ylabel = \"Duration\",\n#     annotate = False, annotate_texts = products_data['Product Name'].values,\n#     theme = 'DARK'\n# )\n\nx_vals = np.log10(products_data['mainURL_URL_subpage_visits'].values)\ny_vals = np.log10(products_data['mainURL_URL_page_duration'].values)\ncats = cats.fillna('Other').values\nwidth = 18\nheight = 10\nsuptitle = \"EdTech Products Popularity (using new categories)\"\ntitle = ''\nxlabel = 'log (Monthly Visits)'\nylabel = 'log (Visit Duration in seconds)'\ntheme = 'DARK'\nannotate = False\nannotate_texts = []\n\ntitle = f\"Overlap Score: {check_inside_polygon(x_vals, y_vals, cats):.2f}\"\n\n# https:\/\/stackoverflow.com\/questions\/44575681\/how-do-i-encircle-different-data-sets-in-scatter-plot\ndef encircle(x,y, ax, **kw):\n    keep_idx = []\n    for idx in range(len(x)):\n        if(np.isnan(x[idx]) == False and np.isnan(y[idx]) == False):\n            keep_idx.append(idx)\n    x = x[keep_idx]\n    y = y[keep_idx]\n    p = np.c_[x,y]\n    hull = ConvexHull(p)\n    poly = plt.Polygon(p[hull.vertices,:], **kw)\n    ax.add_patch(poly)\n\nannotate = True\nannotate_texts = products_data['Product Name'].values.copy()\nfor idx in range(len(annotate_texts)):\n    if((x_vals[idx] >= 2.5) & (x_vals[idx] <= 8) & (y_vals[idx] <= 2.2) & (y_vals[idx] >= 1.25)):\n        annotate_texts[idx] = ''\n\ntheme = select_theme(theme)\nfig, ax = create_fig(1, 1, width, height)\nno_cats_passed = False\nif(len(cats) == 1):\n    cats = np.ones(len(x_vals))\n    no_cats_passed = True\nuniq_cats = list(sorted(pd.Series(cats).unique()))\nif(len(uniq_cats) > len(theme['groups'])):\n    group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\nelse:\n    group_colors = theme['groups'][:len(uniq_cats)]\nfor cat_idx, cat in enumerate(uniq_cats):\n    ax.scatter(x_vals[cats == cat], y_vals[cats == cat], color = group_colors[cat_idx], label = cat)\n    encircle(x_vals[cats == cat], y_vals[cats == cat], ax = ax, ec = group_colors[cat_idx], fc = group_colors[cat_idx], alpha = 0.5)\nif(annotate == True):\n    for idx in range(len(x_vals)):\n        ax.annotate(annotate_texts[idx], (x_vals[idx], y_vals[idx]), color = theme['text'])\nif(no_cats_passed == False):\n    ax = set_legend(ax, theme)\nexec(plot_decoration())\nplt.show()","d39dd93d":"products_data['popularity_index'] = products_data.apply(lambda x: np.log10(x['mainURL_URL_subpage_visits']) + np.log10(x['mainURL_URL_page_duration']), axis = 1)\nproduct_engagement = DATA.groupby('lp_id')['engagement_index'].aggregate(np.nanmean).reset_index()\nproduct_engagement_popularity = pd.merge(product_engagement.dropna(), products_data[['LP ID', 'Product Name', 'popularity_index', 'mainURL_main_category', 'mainURL_sub_category']].dropna(), how = 'inner', left_on = 'lp_id', right_on = 'LP ID')\nproduct_engagement_popularity['engagement_to_popularity_ratio'] = product_engagement_popularity['engagement_index'] \/ product_engagement_popularity['popularity_index']\n\ncats = product_engagement_popularity.apply(select_website_categories, axis = 1)\n\nx_vals = np.log10(product_engagement_popularity['engagement_index'].values)\ny_vals = product_engagement_popularity['popularity_index'].values\ncats = cats\nwidth = 15\nheight = 15\nsuptitle = \"Engagement vs. Popularity\"\ntitle = \"\"\nxlabel = \"log (Engagement Index)\"\nylabel = \"log (Popularity Index)\"\nannotate = True\nannotate_texts = product_engagement_popularity['Product Name'].values\ntheme = 'DARK'\n\nannotate = True\nannotate_texts = product_engagement_popularity['Product Name'].values.copy()\nfor idx in range(len(annotate_texts)):\n    # Live up to the reputation\n    if((x_vals[idx] > 1.5) & (y_vals[idx] > 7)):\n        if(5 * x_vals[idx] + 2.5 * y_vals[idx] <= 37.5):\n            annotate_texts[idx] = ''\n    # Dark Horses\n    elif((x_vals[idx] > 1.5) & (y_vals[idx] < 7)):\n        if(5 * x_vals[idx] - 2.5 * y_vals[idx] <= -1):\n            annotate_texts[idx] = ''\n    # Underperformers\n    elif((x_vals[idx] < 1.5) & (y_vals[idx] > 7)):\n        if(5 * x_vals[idx] - 2.5 * y_vals[idx] >= -24):\n            annotate_texts[idx] = ''\n    # Flops\n    else:\n        if(5 * x_vals[idx] + 2.5 * y_vals[idx] >= 12):\n            annotate_texts[idx] = ''\n\ntheme = select_theme(theme)\nfig, ax = create_fig(1, 1, width, height)\nno_cats_passed = False\nif(len(cats) == 1):\n    cats = np.ones(len(x_vals))\n    no_cats_passed = True\nuniq_cats = list(sorted(pd.Series(cats).unique()))\nif(len(uniq_cats) > len(theme['groups'])):\n    group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\nelse:\n    group_colors = theme['groups'][:len(uniq_cats)]\nfor cat_idx, cat in enumerate(uniq_cats):\n    ax.scatter(x_vals[cats == cat], y_vals[cats == cat], color = group_colors[cat_idx], label = cat)\nif(annotate == True):\n    for idx in range(len(x_vals)):\n        ax.annotate(annotate_texts[idx], (x_vals[idx], y_vals[idx]), color = theme['text'])\nif(no_cats_passed == False):\n    #ax = set_legend(ax, theme)\n    ax.legend(loc = 'upper left')\nexec(plot_decoration())\n\nax.axhline(7, color = 'white', lw = 1)\nax.axvline(1.5, color = 'white', lw = 1)\n\nt1 = ax.text(1.75, 7.5, \"    Live up to\\nthe Reputation\", fontsize = 20, color = COLOR_WHITE)\nt1.set_bbox(dict(facecolor=COLOR_ORANGE, alpha=0.75, edgecolor=COLOR_BLACK))\n\nt2 = ax.text(0.35, 7.5, \"    Under\\nperformers\", fontsize = 20, color = COLOR_WHITE)\nt2.set_bbox(dict(facecolor=COLOR_ORANGE, alpha=0.75, edgecolor=COLOR_BLACK))\n\nt3 = ax.text(1.75, 6, \"  Dark\\nHorses\", fontsize = 20, color = COLOR_WHITE)\nt3.set_bbox(dict(facecolor=COLOR_ORANGE, alpha=0.75, edgecolor=COLOR_BLACK))\n\nt4 = ax.text(0.35, 6, \"Unknown &\\n   Unused\", fontsize = 20, color = COLOR_WHITE)\nt4.set_bbox(dict(facecolor=COLOR_ORANGE, alpha=0.75, edgecolor=COLOR_BLACK))\n\nplt.show()","79bdd24c":"cats = product_engagement_popularity.apply(select_website_categories, axis = 1)\nproduct_engagement_popularity['engagement_index'] = product_engagement_popularity['engagement_index'].apply(np.log10)\nproduct_engagement_popularity['URL_category'] = cats\n\nres2 = pairwise_tukeyhsd(product_engagement_popularity['engagement_index'], product_engagement_popularity['URL_category'])\nprint(res2.summary())\nres2.plot_simultaneous('Games')\nplt.show()\n\n# res2 = pairwise_tukeyhsd(product_engagement_popularity['popularity_index'], product_engagement_popularity['URL_category'])\n# print(res2.summary())\n# res2.plot_simultaneous('Games')\n# plt.show()","19f47924":"product_categories_popularity = pd.merge(product_engagement.dropna(), products_data[['LP ID', 'Product Name', 'popularity_index', 'mainURL_main_category', 'mainURL_sub_category', 'mainURL_description']].dropna(), how = 'inner', left_on = 'lp_id', right_on = 'LP ID')\n\nsuptitle = \"Wordmaps for Provided Categories\"\n\ntheme = select_theme('LIGHT')\nfig, axs = create_fig(1, 3, 20, 8)\n\ndescriptions = ''\nfor description in products_data['mainURL_description'].dropna().values:\n    descriptions += f\" {description} \"\ndescriptions = re.sub('amp', 'and', descriptions)\ndescriptions = descriptions.lower().strip()\ndescriptions = re.sub(\"[^a-z ]\", \"\", descriptions)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n    width = 800, height = 800,\n    background_color = COLOR_BLACK,\n    stopwords = stopwords,\n    min_font_size = 50\n).generate(descriptions)\naxs[0].imshow(wordcloud)\naxs[0].set_title(f\"Overall\", color = theme['subtitle'], fontsize = 20)\n\ndescriptions = ''\nfor description in product_categories_popularity.sort_values('popularity_index', ascending = False).iloc[:25]['mainURL_description'].dropna().values:\n    descriptions += f\" {description} \"\ndescriptions = re.sub('amp', 'and', descriptions)\ndescriptions = descriptions.lower().strip()\ndescriptions = re.sub(\"[^a-z ]\", \"\", descriptions)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n    width = 800, height = 800,\n    background_color = COLOR_BLACK,\n    stopwords = stopwords,\n    min_font_size = 50\n).generate(descriptions)\naxs[1].imshow(wordcloud)\naxs[1].set_title(f\"Most Popular\", color = theme['subtitle'], fontsize = 20)\n\ndescriptions = ''\nfor description in product_categories_popularity.sort_values('engagement_index', ascending = False).iloc[:25]['mainURL_description'].dropna().values:\n    descriptions += f\" {description} \"\ndescriptions = re.sub('amp', 'and', descriptions)\ndescriptions = descriptions.lower().strip()\ndescriptions = re.sub(\"[^a-z ]\", \"\", descriptions)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n    width = 800, height = 800,\n    background_color = COLOR_BLACK,\n    stopwords = stopwords,\n    min_font_size = 50\n).generate(descriptions)\naxs[2].imshow(wordcloud)\naxs[2].set_title(f\"Most Engaging for Students\", color = theme['subtitle'], fontsize = 20)\n\nfor ax in axs.flatten():\n    #fig, ax = set_bg(fig, ax, theme)\n    ax = set_ticks(ax, theme)\n    ax = remove_all_spines(ax)\n    fig.suptitle(suptitle, fontsize = 30, color = theme['text'])\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.tight_layout(h_pad = 5)\nplt.show()","a0371581":"suptitle = \"Wordmaps for Provided Categories\"\n\ntheme = select_theme('LIGHT')\nfig, axs = create_fig(1, 3, 15, 6)\nfor cat_idx, cat in enumerate(products_data['Primary Category'].replace({'Other': np.nan}).dropna().unique()):\n    descriptions = ''\n    for description in products_data[products_data['Primary Category'] == cat]['mainURL_description'].dropna().values:\n        descriptions += f\" {description} \"\n    descriptions = re.sub('amp', 'and', descriptions)\n    descriptions = descriptions.lower().strip()\n    descriptions = re.sub(\"[^a-z ]\", \"\", descriptions)\n    stopwords = set(STOPWORDS)\n    wordcloud = WordCloud(\n        width = 800, height = 800,\n        background_color = COLOR_BLACK,\n        stopwords = stopwords,\n        min_font_size = 40\n    ).generate(descriptions)\n    axs[cat_idx].imshow(wordcloud)\n    axs[cat_idx].set_title(f\"Category: {cat}\", color = theme['subtitle'], fontsize = 20)\nfor ax in axs.flatten():\n    #fig, ax = set_bg(fig, ax, theme)\n    ax = set_ticks(ax, theme)\n    ax = remove_all_spines(ax)\n    fig.suptitle(suptitle, fontsize = 30, color = theme['text'])\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.tight_layout(h_pad = 3)\nplt.show()","5308b7e7":"suptitle = \"Wordmaps for New Extracted Categories\"\n\ncats = products_data.apply(select_website_categories, axis = 1).replace({'Other': np.nan})\n\ntheme = select_theme('LIGHT')\nfig, axs = create_fig(1, 4, 15, 5)\nfor cat_idx, cat in enumerate(cats.dropna().unique()):\n    descriptions = ''\n    for description in products_data[cats == cat]['mainURL_description'].dropna().values:\n        descriptions += f\" {description} \"\n    descriptions = re.sub('amp', 'and', descriptions)\n    descriptions = descriptions.lower().strip()\n    descriptions = re.sub(\"[^a-z ]\", \"\", descriptions)\n    stopwords = set(STOPWORDS)\n    wordcloud = WordCloud(\n        width = 800, height = 800,\n        background_color = COLOR_BLACK,\n        stopwords = stopwords,\n        min_font_size = 40\n    ).generate(descriptions)\n    axs[cat_idx].imshow(wordcloud)\n    axs[cat_idx].set_title(f\"{cat}\", color = theme['subtitle'], fontsize = 20)\nfor ax in axs.flatten():\n    #fig, ax = set_bg(fig, ax, theme)\n    ax = set_ticks(ax, theme)\n    ax = remove_all_spines(ax)\n    fig.suptitle(suptitle, fontsize = 30, color = theme['text'])\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.tight_layout(h_pad = 3)\nplt.show()","a804f6a4":"<div>\n<font size='+1'>\n    Naturally, products like YouTube, Facebook & Netflix have much better popularity than other Educational apps. And frankly that is because these are not Educational apps but more of Entertainment apps. Therefore, I wanted to segregate such apps from the genuine Educational apps.\n    <br><br>\n    In the provided dataset, the products have been categorized into Learning & Curriculum (LC), Classroom Management (CM) & School and District Operations (SDO). But when using these categories, the segregation is extremely poor as shown below. All categories tend to overlap onto each other.\n    <br><br>\n    To measure this overlap, I define a metric called <b>Overlap Score<\/b> which calculates how many groups\/clusters a point is within (on average). In the ideal case, this score should be 1 indicating that there is absolutely no overlap between clusters. In the below case, a point is expected to be in almost 3.5 clusters on average, which is high.\n<\/font>\n<\/div>","f1276170":"<div align='center'>\n    <font size='+2' color='#75D345'>\n        <a style=\"background-color:#63D1DF; padding: 15px;\" href='https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-3-sleepy-noons' target=\"_blank\">Next: <b>Sleepy Noons<\/b><\/a>\n    <\/font>\n<\/div>","31bc6014":"<div>\n<font size='+1'>\n    Finally, coming back to the original question, do these popular websites naturally have more student engagement? To answer this, we need a <b>Popularity Metric<\/b> & Engagement Metric. We already have engagement_index as a good candidate to represent student engagement. For popularity, I decided to use the following metric - (Total visits x Avg Visit Duration).\n    <br><br>\n    <b>NOTE: The total visits & visit duration have actually only been given for main webpages (eg. - abc.com\/) but not subpages (eg. abc.com\/xyz\/). However, we know the average number of pages visited on the website & also how many users do not check out the subpages (bounce rate), hence the total visits & visit duration can be approximated for subpages using this formula.<\/b>\n    <br><br>\n    Popularity Index = Page Visits x Page Visit Duration\n    <br>\n    Page Visits = Total Visits x (100% - Bounce Rate) ^ (Subpage Depth: How many '\/' are present in subpage URL)\n    <br>\n    Page Visit Duration = Total Visit Duration \/ Avg number of pages visited\n    <br><br>\n    Using the Popularity & Engagement metric, I plotted a scatterplot. In this plot, apps can be classified as -\n    <ul>\n        <li><font color='#FD7121'><b>Live up to the reputation<\/b><\/font> - Highly popular & highly used by students (YouTube, Google Classroom, Google Forms)<\/li>\n        <li><font color='#FD7121'><b>Underperformers<\/b><\/font> - Highly popular but less used by students (Google Chrome, Google Books)<\/li>\n        <li><font color='#FD7121'><b>Dark Horses<\/b><\/font> - Less popular but highly used by students (Kahoot!, Canvas, ST Math)<\/li>\n        <li><font color='#FD7121'><b>Unknown & Unused<\/b><\/font> - Less popular & less used by students<\/li>\n    <\/ul>\n<\/font>\n<\/div>","944fdac4":"<div style=\"background-color: #F8F1FF; padding: 20px 50px;\">\n<span style=\"color:#156BB7;\">\n<font size='+2.5'>\n    <b>9 AM:<\/b>\n<\/font>\n<br>\n<br>\n<font size='+1.5'>\n    After an hour of her Maths class, Grace felt like a prisoner on the day of release. She was confident that whatever happens next, cannot be worse than what just happened.\n    <br><br>\n    \"<b>Science<\/b> class is next. I like science,\" she nodded with a slight smile on her face. \"You should like it,\" I said. \"After all, the most revolutionary changes on Earth in the last few hundred years have been through science & technology. Cars, vaccines, bulbs, mobiles and so on. Ah, and don't forget the internet which has connected each and every one of us together.\"\n    <br><br>\n    Suddenly, a boy started talking from Grace's laptop. \"Ma'am,\" he said. \"Adam won't be attending today's class. The internet in his area is down.\"\n<\/font>\n<\/span>\n<\/div>","83f9dabf":"<div>\n<font size='+1'>\n    I then decided to use the categories which were available on similarweb.com itself. I chose the Education, Entertainment, Games, Social Media & Other categories. These categories provided a lower overlap score of 3 and these clusters visually also look better segregated than before.\n    <br><br>\n    In addition, one can clearly see that Entertainment & Social Media apps are much more popular than Educational apps. This is the reason I will continue with these categories for future analysis.\n<\/font>\n<\/div>","1cbd3dc2":"<div>\n<font size='+1'>\n    <b>HYPOTHESIS: Gaming apps have better engagement than regular Educational apps<\/b>\n    <br>\n    Null Hypothesis: Gaming apps & Educational apps have the same student engagement\n    <br><br>\n    To test this hypothesis, I used the Tukey test and found that there is no significant difference between Gaming apps & Educational apps. Hence we cannot reject the Null Hypothesis. \n<\/font>\n<\/div>","cdeabd7f":"![example.png](attachment:f785cf8d-cfd7-4ed8-acf9-adf7e81eecb2.png)","68ba9024":"<div>\n<font size='+0.5'>\n    Let's start off by collecting data about broadband access. To improve the quality of this analysis, I used three different datasets which encompass different aspects of broadband access. After going through the combined features available, I realized that almost all features indicated either how available broadband is in a state, or how affordable broadband is for the people. This is why I decided to categorize the features based on whether it explains <font color='#FD7121'>Broadband Availability<\/font> or <font color='#FD7121'>Braodband Affordability<\/font>.\n    <br><br>\n<\/font>\n<\/div>","2f11b499":"<div>\n<font size='+1'>\n    The wordmaps present a fun way to understand more about the EdTech products. Using the description queried from similarweb.com, I made wordmaps to know more about different kind of EdTech products. Here are a few fun short summaries (almost purely from the wordmaps) -\n    <ul>\n        <li>EdTech products (overall): <b>Free online learning for students<\/b><\/li>\n        <li>EdTech products (most popular): <b>Sharing + creating + movies, TV & shows<\/b><\/li>\n        <li>EdTech products (most engaging): <b>Learning through reading, books, instructions & videos<\/b><\/li>\n    <\/ul>\n<\/font>\n<\/div>","a9e58572":"![LearnPlatform_Experimental(2).png](attachment:cfaa4f23-efc5-44e9-be77-e66245e242fd.png)","2d4b6abe":"<div>\n<font size='+0.5'>\n    However, these broadband metrics <font color='#FD7121'>do not seem to have a significant correlation<\/font> with the two provided engagement metrics - pct_access & engagement_index. This comes as a bit of a surprise since one would expect more challenges in areas having less available\/affordable broadband access and hence lesser engagement. But it appears students from such areas are still managing to keep up sufficiently good engagement levels.\n<\/font>\n<\/div>","e72742cb":"<div>\n<font size='+1'>\n    Grace never had a problem with speaking English. But some of her friends who were non-English speakers do face a lot of difficulty, even in regular conversation. Firstly identifying all the words, then understanding the meaning of each word and then finally putting all the words together to understand the meaning of a sentence - it was tough for them.\n    <br><br>\n    Right then I suddenly remembered a Natural Language Processing project that I had worked on a couple of years ago. Just to make sure a machine understands a sentence properly, researchers have spent countless hours, incredible effort and humongous bundles of cash.\n    <br><br>\n    I have neither of the three, so why don't I myself make sense of the textual descriptions of the different EdTech products. Maybe there exists some collective meaning within the bunch of words. When I tried that, this is what I found -\n<\/font>\n<\/div>","d40aae13":"<div>\n<font size='+1'>\n    The first step for me would be to categorize the large number of EdTech products that I have data for. But while going through the products, I identified several well-known apps like YouTube, Facebook, Instagram. Though not technically an educational app, my first intuition was that since these products are extremely popular, they might naturally be used more. This would indeed create a bias based on the popularity of the product.\n    <br><br>\n    Hence, I decided to first try and quantify the popularity of each and every product. To do this, I had to create a new dataset myself by web crawling using the <a href='similarweb.com'>similarweb.com<\/a> site. This site provides several web traffic related statistics (like total visits, avg visit duration, bounce rate, number of pages, global ranks, etc.) which can help in determining how popular a website is.\n    <br><br>\n    So after several hours of hardwork (and after many hacks to prevent myself from being identified as a bot), this is what I found -\n<\/font>\n<\/div>","7791406c":"<div style=\"background-color: #F8F1FF; padding: 20px 50px;\">\n<span style=\"color:#156BB7;\">\n<font size='+2.5'>\n    <b>11 AM:<\/b>\n<\/font>\n<br>\n<br>\n<font size='+1.5'>\n    With the lunch break arriving in an hour, Grace was eager for her <b>English<\/b> class to end. \"I know English. I am speaking to you in English. There is no need for me to learn more English,\" she complained to me.\n    <br><br>\n    \"You know very basic English, Grace. You need to learn more about English so that one day you can stop asking me what each long word means in a newspaper,\" I scolded. \"And then you need to be able to put all of those long words together and understand the meaning of that news report.\"\n<\/font>\n<\/span>\n<\/div>","68e76040":"<div>\n    <font size='+2' color='#0DAB6C'>\n        <b>Detailed Analysis<\/b>\n    <\/font>\n<\/div>","2eae23e6":"<div>\n<font size='+0.5'>\n    <br><br>\n    Further, since there were many features for each category, I applied PCA to reduce the dimensions until the explained variance was atleast 70%. Out of these components, shown below are the weights of the <b>first PCA component<\/b> for each category.\n    <br><br>\n    <a style=\"background-color:#30DB8D; padding: 10px;\">Green Bars<\/a> - Features which have high absolute weights (these features have more influence over the PCA component)\n    <br><br>\n    <a style=\"background-color:#DA4167; padding: 10px;\">Red Bars<\/a> - Features which have relatively less absolute weights (these features do not influence the PCA component as much as others) \n    <br><br>\n    The main aim of analyzing the first PCA component is to answer the question - if I had to represent all these several features under a single feature, then what exactly would that single feature majorly represent. In the case of Broadband Availability, the component increases in value with increase in density of Broadband providers; hence higher this value, more available broadband is. For Broadband Affordability features, the component decreases with poverty rate and increases with % of population having home broadband. Hence higher the value of this component for a state, higher are the chances of people in that state being able to afford broadband.\n<\/font>\n<\/div>","3bff4dc0":"<div>\n    <font size='+2' color='#0DAB6C'>\n        <b>Detailed Analysis<\/b>\n    <\/font>\n<\/div>","84639bde":"<div style=\"background-color: #F8F1FF; padding: 20px 50px;\">\n<span style=\"color:#156BB7;\">\n<font size='+2.5'>\n    <b>10 AM:<\/b>\n<\/font>\n<br>\n<br>\n<font size='+1.5'>\n    It is 10 AM. Grace should be starting with her <b>Computer Science<\/b> class now, but her teacher allows the students a ten-minute break.\n    <br><br>\n    Usually around this time, Grace would also start getting bored at looking at the laptop screen. \"Would you get bored after two classes if you were at school?\" I asked. \"Never, I would have friends to spend my time with. If classes seem long, our chit-chatting would also be long,\" she replied, gleaming as she remembered those moments. \"But I can't do that in these online meetings.\"\n    <br><br>\n    \"You should be thankful that these online meetings exist in the first place. Even while all of your friends are sitting in different parts of the city, you all can still get the same education,\" I said.\n<\/font>\n<\/span>\n<\/div>","c6ff5868":"<div>\n<font size='+1'>\n    It is true that online meetings do not have the 'human touch' that Grace misses so much; but to be honest, if this pandemic would have hit us maybe a 100 years ago, continuing education would have been much more challenging in the absence of such advanced technology.\n    <br><br>\n    But this got me wondering, Grace has so many online tools available for her study - meetings, videos, e-books, exam portals and others. How does student engagement compare to that? Do certain kinds of EdTech products boast a better engagement metric than others? After exploring this question, here is what I found -\n<\/font>\n<\/div>","a400994d":"<div>\n    <font size='+2' color='#0DAB6C'>\n        <b>Detailed Analysis<\/b>\n    <\/font>\n<\/div>","99507cb3":"<div>\n<font size='+1'>\n    Alright, maybe internet has not connected 'each and every one' of us. But why? Maybe broadband is not available in some remote areas. Maybe broadband is not affordable for some. But does having unstable or no internet connectivity cost students like Adam their ability to learn digitally. Surely yes, I thought. So I explored more and this is what I found -\n<\/font>\n<\/div>","a081841e":"![LearnPlatform_Experimental(5).png](attachment:6202274a-bf15-47ea-bc4b-e2b3a1ec0767.png)","52dc7984":"![LearnPlatform_Experimental(3).png](attachment:0f7e6ede-ab48-4dd5-9748-4ba2a8e7cc2b.png)","3434fbf9":"<div>\n<font size='+0.5'>\n    Now that we have developed a metric which represents how available & affordable broadband is, it might be a good idea to apply these metrics on the US map.\n    <br><br>\n    As can be seen, Delaware seems to be the state with most available broadband relative to its size while states like Missouri have much less availability of Broadband connections. When it comes to affordability, Nevada seems to have the most population who can easily afford broadband while people in states like Minnesota find it difficult to afford broadband.\n<\/font>\n<\/div>","c5dd1d76":"<div>\n<font size='+1'>\n    Wordmaps have also been made for different categories of EdTech products, which could be used as a fun way to learn more about the characteristics of each category.\n<\/font>\n<\/div>","5fd1d749":"<div>\n<font size='+0.5'>\n    In a bid to explore further the relationship between broadband access & engagement, I decided to involve a third variable - the type of educational app. The main intention here was to see whether certain kinds of educational apps are being used less in regions where broadband is an issue. To do this, I used the two-factor ANOVA test, where interactions between broadband access & type of educational app can also be explored against engagement.\n    <br><br>\n    <b>NOTE: The category of EdTech apps (Education, Entertainment, Games, Others & Social Media) is a new feature that I have created. More information about these new categories of EdTech apps will be explained in the next sections.<\/b>\n    <br><br>\n    Ultimately, the p-value suggested that the broadband access - educational app interaction was not significant. However, when taking a look at the interaction plot, an interesting observation was made. <font color='#FD7121'>Gaming apps<\/font> seemed to be the only kind of educational apps where engagement decreased with broadband access. Another confusing aspect of the plot was that most EdTech apps (apart from gaming) seemed to have more engagement in regions with lesser broadband access. This obviously goes against common logic. Perhaps the reason why this is happening is because we are analyzing at a state-level which could be generalizing the observations. Perhaps at the county-level, the overall accuracy of these analyses might increase.\n<\/font>\n<\/div>","a75be859":"<div align='center'>\n    <font size='+3' color='#75D345'>\n        <b>A Day of Digital Learning<\/b>\n    <\/font>\n    <br>\n    <font size='+2.5' color='#FBAB60'>\n        <b>Part 2: Morning Mania<\/b>\n    <\/font>\n    <br>\n    <br>\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-1-wake-up\">\n        <font size='+1'>\n            &#9202; Wake Up!\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-2-morning-mania\">\n        <font size='+1'>\n            <b>&#127748; Morning Mania<\/b>\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-3-sleepy-noons\">\n        <font size='+1'>\n            &#127774; Sleepy Noons\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-4-finally-done\">\n        <font size='+1'>\n            &#127769; Finally Done!\n        <\/font>\n    <\/a>\n<\/div>"}}