{"cell_type":{"4b5d5e0a":"code","2bd1a6f1":"code","cd1844dc":"code","d1f4288e":"code","a130542c":"code","4cc76ac5":"code","1c16a11a":"code","b6265ed8":"code","b53e6664":"code","a66f1b04":"code","145ab30a":"code","09e8201b":"code","70a42f23":"code","678313b1":"code","a91f45ce":"code","7bf35878":"code","f7510f7d":"code","676aee9b":"code","2ac2a7c0":"code","2cd04590":"code","aec6a135":"code","f22f9182":"code","188cb9dc":"markdown","248ae2bb":"markdown","2b5d28ab":"markdown","334dd7e0":"markdown","ca0e9294":"markdown","3d184895":"markdown","34701c64":"markdown","6e06ff4c":"markdown","b404e2fc":"markdown","c477d27f":"markdown","4e778ddc":"markdown","f957ffd2":"markdown","fc786a56":"markdown","40e0820a":"markdown"},"source":{"4b5d5e0a":"!ls ..\/input\/sarcasm\/","2bd1a6f1":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\n\n# plotting\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom matplotlib import pyplot as plt","cd1844dc":"train_df = pd.read_csv('..\/input\/sarcasm\/train-balanced-sarcasm.csv')","d1f4288e":"train_df.head()","a130542c":"train_df.info()","4cc76ac5":"train_df.dropna(subset=['comment'], inplace=True)","1c16a11a":"train_df['label'].value_counts()","b6265ed8":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)","b53e6664":"# Shortcuts for the data with label 1 and label 0\ntrain1_df = train_df[train_df[\"label\"]==1]\ntrain0_df = train_df[train_df[\"label\"]==0]","a66f1b04":"# How many ARE sarcasm?\n# -> \"balanced\" dataset\nprint(train_df['label'].value_counts())","145ab30a":"# Word cloud\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_df[\"comment\"], title=\"Word Cloud of comments\")","09e8201b":"from collections import defaultdict\nimport string # for punctuation\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1): # TODO: optimize and don't strip twice\n    token = [token.strip(string.punctuation) # remove leading\/trailing punctuation\n             for token in text.lower().split(\" \") # for words in text; lower cased\n             if token.strip(string.punctuation) != \"\" if token not in STOPWORDS] # except empty and stop words\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"comment\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(30), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"comment\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(30), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of sincere comments\", \n                                          \"Frequent words of sarcasm comments\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n","70a42f23":"# BIGRAMS\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"comment\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(30), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"comment\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(30), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of sincere comments\", \n                                          \"Frequent bigrams of sarcasm comments\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","678313b1":"# TRIGRAMS\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"comment\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(30), 'green')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"comment\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(30), 'green')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of sincere comments\", \n                                          \"Frequent bigrams of sarcasm comments\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","a91f45ce":"train1_df['comment'].str.len().apply(np.log1p).hist(label='sarcastic', alpha=.5, density=True)\ntrain0_df['comment'].str.len().apply(np.log1p).hist(label='normal', alpha=.5, density=True)\nplt.legend();","7bf35878":"## Group by to analyze subreddits\nsub_df = train_df.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\nsub_df[sub_df['size'] > 1000].sort_values(by='mean', ascending=False).head(10)","f7510f7d":"# build bigrams, put a limit on maximal number of features\n# and minimal word frequency\ntf_idf = TfidfVectorizer(ngram_range=(1, 2),  # unigrams and bigrams\n                         max_features=50000, \n                         min_df=2) # inogre a feature if it is encounter only 2 times or less\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(C=1, #inverse regularization strength; smaller -> higher reguralization\n                           n_jobs=4, # parallelize\n                           solver='lbfgs', \n                           random_state=17, # for shuffling the data\n                           verbose=1) \n# sklearn's pipeline\n# Sequentially apply a list of transforms and a final estimator. \n# Intermediate steps of the pipeline must be \u2018transforms\u2019, that is, they must implement fit and transform methods.\n#The final estimator only needs to implement fit.\ntfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n                                 ('logit', logit)] # Steps\n                               )\n\ntfidf_logit_pipeline","676aee9b":"## Train\n# Fit all the transforms one after the other and transform the data,\n# then fit the transformed data using the final estimator.\ntfidf_logit_pipeline.fit(train_texts, # training data\n                         y_train) # labels","2ac2a7c0":"## test\nvalid_pred = tfidf_logit_pipeline.predict(valid_texts)\n\n## accuracy score - fraction of correct ones\naccuracy_score(y_valid, valid_pred)","2cd04590":"import eli5\neli5.show_weights(estimator=tfidf_logit_pipeline.named_steps['logit'],\n                  vec=tfidf_logit_pipeline.named_steps['tf_idf'])","aec6a135":"## play with parameters:\n# Don't use rare words (<10)\n# build bigrams, put a limit on maximal number of features\n# and minimal word frequency\n\ntf_idf = TfidfVectorizer(ngram_range=(1, 2),  # unigrams and bigrams\n                        # max_features=50000, # build only from top features, ordered by encounter frequency\n                         min_df=20,  # inogre a feature if it is encountered only 2 times or less\n                         max_df=0.95) # inogre a feature if it is encountered too often\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(C=1, #inverse regularization strength; smaller -> higher reguralization\n                           n_jobs=4, # parallelize\n                           solver='lbfgs', \n                           random_state=17, # for shuffling the data\n                           verbose=1) \n# sklearn's pipeline\n# Sequentially apply a list of transforms and a final estimator. \n# Intermediate steps of the pipeline must be \u2018transforms\u2019, that is, they must implement fit and transform methods.\n#The final estimator only needs to implement fit.\ntfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n                                 ('logit', logit)] # Steps\n                               )\n\ntfidf_logit_pipeline.fit(train_texts, # training data\n                         y_train) # labels\n\n## test\nvalid_pred = tfidf_logit_pipeline.predict(valid_texts)\n\n## accuracy score - fraction of correct ones\naccuracy_score(y_valid, valid_pred)","f22f9182":"eli5.show_weights(estimator=tfidf_logit_pipeline.named_steps['logit'],\n                  vec=tfidf_logit_pipeline.named_steps['tf_idf'], \n                  top=(10,10))","188cb9dc":"Frequent words?","248ae2bb":"* ## Part 1.1 - analyze metadata","2b5d28ab":"## Part 4 - improve results\n\n- Try cutting rare words or words which are way too frequent","334dd7e0":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","ca0e9294":"Some comments are missing, so we drop the corresponding rows.","3d184895":"Almost equal distribution","34701c64":"TODO:\n- subreddit?\n- number of up\/down votes\n- as a function of up\/down-votes ratio","6e06ff4c":"We notice that the dataset is indeed balanced","b404e2fc":"## Part 3 - explain results","c477d27f":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words\/bigrams which a most predictive of sarcasm (you can use [eli5](https:\/\/github.com\/TeamHG-Memex\/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n\n## Links:\n  - Machine learning library [Scikit-learn](https:\/\/scikit-learn.org\/stable\/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-2-classification) and its applications to [text classification](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https:\/\/github.com\/TeamHG-Memex\/eli5) to explain model predictions\n  \n  ---\n  \n## DONE\n- Word counts\n- Strip punctuation\n- Would be interesting to see if there is a correlation with the up\/down votes ratio, but \n  - I don't understand why there are negative counts of up or down votes","4e778ddc":"## <center> Assignment 4. Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https:\/\/arxiv.org\/abs\/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https:\/\/www.kaggle.com\/danofer\/sarcasm).\n\nSarcasm detection is easy. \n<img src=\"https:\/\/habrastorage.org\/webt\/1f\/0d\/ta\/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" \/>","f957ffd2":"We split data into training and validation parts.","fc786a56":"Observations:\n\n- 'yeah', 'well' -> sarcasm\n- 'good thing', 'everyone knows' -> sarcasm\n- Many combinations - for both sarcasm and non-sarcasm\n- Many triplets - weird","40e0820a":"## Part 2 - training the model\n\n- TFIDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus \n  - `min_df=2` - "}}