{"cell_type":{"14210e99":"code","9a787b5d":"code","5545615b":"code","5b96c134":"code","0783facf":"code","b844fbba":"code","819829dc":"code","e262b436":"code","d4578f8a":"code","d698de36":"code","d6a8a9e7":"markdown","d484a1ea":"markdown","c24a2440":"markdown","52bec9f4":"markdown","026368c4":"markdown","383ab7a3":"markdown","a08cd6f0":"markdown","bf840077":"markdown","0d380f9d":"markdown","5a359169":"markdown","69616886":"markdown","6f59c781":"markdown","674f80fb":"markdown"},"source":{"14210e99":"import pickle\nimport os\n\nimport keras\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SpatialDropout1D, LSTM, Conv1D, Dense, GlobalMaxPooling1D, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\n\nos.chdir('\/kaggle\/input\/electrical-power-quality-meter-dataset')\nprint(os.listdir('.'))\n        \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a787b5d":"data = pd.read_csv('power-quality-meter.csv')\n\n# Dropping Phase angle values as they have little effect\ninput_data = data.drop(['Date','Time','Cos Phi AN Avg','Cos Phi BN Avg','Cos Phi CN Avg','Cos Phi Total Avg'], axis = 1)\ninput_data.drop(input_data.columns[0], inplace=True, axis=1)\n\n# Dropping outliers\ninput_data = input_data[(np.abs(stats.zscore(input_data)) < 3).all(axis=1)]\n\nprint(list(input_data.columns.values))\n\n# Dump a pickle of dataset labels\nwith open('\/kaggle\/working\/labels.pickle', \"wb+\") as pickle_out:\n    pickle.dump(list(input_data.columns.values), pickle_out)\n","5545615b":"# Scaling data\nscaler = StandardScaler()\nscaler.fit(input_data[:3*len(input_data)\/\/4]) # 0.75 because train_size is 75% of given data\ncopy = scaler.transform(input_data)\n\ntimestep = 10\n\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n\n    if dropnan:\n        agg.dropna(inplace=True)   \n        \n    return agg\n\ntrain = series_to_supervised(copy).values\nprint(np.array(train).shape)","5b96c134":"X_train = []\ny_train = []\n\nfor i in range(timestep, len(input_data)-1):\n    X_train.append(train[i-timestep:i, :len(input_data.columns)])\n    y_train.append(train[i-timestep, len(input_data.columns):])\n    \nX_train, y_train = np.array(X_train), np.array(y_train)\n\ndata_dump = X_train, y_train\n\nwith open(\"\/kaggle\/working\/data.pickle\",\"wb+\") as pickle_out:\n    pickle.dump(data_dump, pickle_out)\n\nwith open(\"\/kaggle\/working\/scaler.pickle\",\"wb+\") as pickle_out:\n    pickle.dump(scaler, pickle_out)","0783facf":"with open(\"\/kaggle\/working\/data.pickle\", \"rb\") as f:\n    X_train, y_train = pickle.load(f)","b844fbba":"model = Sequential()\nmodel.add(LSTM(512, activation = 'tanh', recurrent_activation = 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias = True, return_sequences = True, input_shape=(X_train.shape[1],X_train.shape[2])))\nmodel.add(LSTM(256, activation = 'tanh', recurrent_activation = 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias = True, return_sequences = True))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(1024))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(X_train.shape[2]))\n\nmodel.compile(loss = 'mae', optimizer = Adam(lr = 1e-3))","819829dc":"# Model checkpoint callback to save model with lowest validation loss\ncp_callbacks = ModelCheckpoint(filepath = \"\/kaggle\/working\/model.h5\", monitor = \"val_loss\", mode = 'min', save_best_only = True, verbose = 1)\n\n# Fit model\nhistory = model.fit(X_train, y_train, epochs = 90, batch_size = 256, validation_split = 0.25, callbacks = [cp_callbacks])","e262b436":"model = tf.keras.models.load_model(\"\/kaggle\/working\/model.h5\")\n\nvalidation_target = y_train[3*len(X_train)\/\/4:]\nvalidation_predictions = []\nerror = []\n\n# index of first validation input\ni = 3*len(X_train)\/\/4\n\nwhile len(validation_predictions) < len(validation_target) - 1:\n  p = model.predict(X_train[i].reshape(1, X_train.shape[1], X_train.shape[2]))[0] \n  i += 1\n  error.append(mean_absolute_error(p,y_train[i]))\n\n  # update the predictions list\n  validation_predictions.append(p)","d4578f8a":"fig = plt.figure()\nfig.set_size_inches(18.5, 10.5)\nfig.suptitle('Mean Absolute Error', size=20)\nplt.plot(error, label='Error')\nplt.xticks(fontsize=18)\nplt.ylim(0, 10)\nplt.legend(prop={'size': 20})\nplt.show()","d698de36":"with open('\/kaggle\/working\/scaler.pickle', 'rb') as f:\n    scaler = pickle.load(f)\n    \nwith open('\/kaggle\/working\/labels.pickle', 'rb') as f:\n    labels = pickle.load(f)\n\nfor i in range(validation_target.shape[1]):\n    fig = plt.figure()\n    fig.set_size_inches(18.5, 10.5)\n    fig.suptitle(\"Fig \"+str(i+1)+\": Predicted and actual values for \"+labels[i], size=30)\n    plt.plot(scaler.inverse_transform(validation_target)[:, i], label='Actual')\n    plt.plot(scaler.inverse_transform(np.array(validation_predictions))[:, i], '--', label='Predicted')\n    plt.xticks(fontsize=18)\n    plt.yticks(fontsize=18)\n    plt.legend(prop={'size': 20})\n    plt.show()","d6a8a9e7":"## Train-test split and dump data","d484a1ea":"# Training\n------------------\nThis section of the kernel focuses on training and validating the model","c24a2440":"## Importing libraries","52bec9f4":"## Fit model","026368c4":"## Load preprocessed data","383ab7a3":"## Validation of data","a08cd6f0":"## Preprocessing and conversion to supervised learning problem","bf840077":"## Define and compile model","0d380f9d":"## Plot mean absolute error","5a359169":"## Plot predicted vs actual values for all parameters","69616886":"## Loading and cleaning data","6f59c781":"# Conclusion\n\nThat's it for the kernel. Feel free to fork and edit, download the model and use it from the outputs. Enjoy yourself.","674f80fb":"# Preprocessing\n-------------------\n\nThis part of the notebook focuses on preprocessing data."}}