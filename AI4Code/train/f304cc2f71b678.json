{"cell_type":{"dc17b9c5":"code","6cef18c9":"code","eb07b194":"code","6fc235aa":"code","0e06ce2d":"code","d36db605":"code","ea9f915c":"code","aecf8f9e":"code","c3ebd854":"code","6e8b577e":"code","fbf8d00b":"code","addc6886":"code","8b0ddc2c":"code","e6c9f8c7":"code","21f1e814":"code","fd336271":"code","100a481b":"code","fd90efad":"code","a7779d2c":"code","44a12e99":"code","2427a59d":"code","d53919f0":"code","9265e056":"code","ca49c967":"code","edee2a97":"code","42fc66e4":"code","2026338c":"code","b2f6e495":"code","e8cad7de":"code","fac443f4":"code","fb40ddc8":"code","a3610981":"code","3a103ae9":"code","edb9b629":"code","24b71d2a":"code","f22e6b82":"code","f8ffaa2f":"code","fa4b3cb1":"code","765dfa7d":"code","2eced179":"code","4daa0161":"code","2a8c66bf":"code","083b3165":"code","a8400e14":"code","17e2bf7e":"code","d69fe3fd":"code","c8b012d5":"code","99a58f0c":"code","ebaaea29":"code","02f74d2d":"code","9ce50ca1":"code","2e72fa50":"code","7a0ca2db":"code","67d61200":"code","5e32017b":"code","895b32b1":"code","692546bd":"code","fd61d3e6":"code","bac7c195":"code","6e28e47b":"code","5baaf944":"code","f4dc745c":"code","4a7a5314":"code","5ded180d":"markdown","d9dcb0aa":"markdown","0f6b4292":"markdown","3ed6f6df":"markdown","5633a0a7":"markdown","22c0beeb":"markdown","c011b55f":"markdown","93e7c71c":"markdown","8690dcc7":"markdown","7e33f08b":"markdown","566c55ae":"markdown","68b78978":"markdown","5c577344":"markdown","1b536daf":"markdown","08f23a05":"markdown","c7c7b80a":"markdown","cafdbd11":"markdown","69ee7923":"markdown","94a46c9a":"markdown","7fea35c8":"markdown","5064c2e9":"markdown","92d79f42":"markdown","e4760b23":"markdown","130b71e5":"markdown","a0213a0c":"markdown","2a7fa9e0":"markdown","32810b9a":"markdown","0e0b5082":"markdown","3c0f63e3":"markdown","466f8f76":"markdown","9a4123a8":"markdown","5388d826":"markdown","ade9d5cf":"markdown","a88a8c0d":"markdown","545e88bd":"markdown","b532d4ed":"markdown","654abcd6":"markdown","d5b1cff0":"markdown","70702875":"markdown","205ba7c8":"markdown","96d920ee":"markdown","ddbce52c":"markdown","5f2a8561":"markdown","8a1842f4":"markdown","32419b73":"markdown","b34dda52":"markdown","2508dacd":"markdown","631b5aba":"markdown"},"source":{"dc17b9c5":"import pandas as pd\nimport numpy as np\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import Perceptron\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nimport featuretools as ft\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import ExtraTreesRegressor\n","6cef18c9":"def GetBasedModel():\n    models = []\n   #models.append(('LN', linear_model.LinearRegression())) #(Regression - Supervised)\n    #models.append(('RID', Ridge())) #(Regression - Supervised)\n    #models.append(('LSO', Lasso())) #(Regression - Supervised)\n    #models.append(('EN',  ElasticNet())) #(Regression - Supervised)\n    #models.append(('KNNR', KNeighborsRegressor())) #(Regression - Supervised)\n    #models.append(('CARTR', DecisionTreeRegressor())) #(Regression - Supervised)\n    #models.append(('SVR', SVR())) #(Regression - Supervised)\n    models.append(('PER', Perceptron( max_iter=1000,tol=1e-3,n_jobs=-1)))  #(Binary-Classification-Supervised)\n    models.append(('XGB', XGBClassifier( n_jobs=-1))) #(Regression-Classification-binary-multi-Supervised)\n    models.append(('LSVC',  LinearSVC(max_iter=10000))) #(Classification-Supervised)\n    models.append(('SGDC',  SGDClassifier(max_iter=1000,tol=1e-3,n_jobs=-1))) #(Classification-Supervised)\n    models.append(('LR', LogisticRegression(solver='lbfgs',max_iter=1000, multi_class='auto',n_jobs=-1))) #(Classification-Binary-Supervised)\n    models.append(('LDA', LinearDiscriminantAnalysis())) #(Classification-binary-multi-Supervised)\n    models.append(('KNN', KNeighborsClassifier(n_jobs=-1))) #(Classification-Supervised)\n    models.append(('CART', DecisionTreeClassifier())) #(Clasification-Supervised)\n    models.append(('NB', GaussianNB())) #(Clasification-Supervised)\n    models.append(('SVM', SVC(gamma='scale',max_iter=1000))) #(Clasification-Supervised)\n    models.append(('AB', AdaBoostClassifier())) #(Clasification-Supervised)\n    models.append(('GBM', GradientBoostingClassifier())) #(Clasification-Supervised)\n    models.append(('RF', RandomForestClassifier(n_estimators=100,n_jobs=-1))) #(Clasification-Supervised)\n    models.append(('ET', ExtraTreesClassifier(n_estimators=100,n_jobs=-1))) #(Clasification-Supervised)\n\n    \n    return models","eb07b194":"def BasedLine2(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = 'accuracy'\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = KFold(n_splits=num_folds, random_state=42)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n    return names, results","6fc235aa":"class PlotBoxR(object):\n    \n    \n    def __Trace(self,nameOfFeature,value): \n    \n        trace = go.Box(\n            y=value,\n            name = nameOfFeature,\n            marker = dict(\n                color = 'rgb(0, 128, 128)',\n            )\n        )\n        return trace\n\n    def PlotResult(self,names,results):\n        \n        data = []\n\n        for i in range(len(names)):\n            data.append(self.__Trace(names[i],results[i]))\n\n\n        py.iplot(data)","0e06ce2d":"def ScoreDataFrame(names,results):\n    def floatingDecimals(f_val, dec=3):\n        prc = \"{:.\"+str(dec)+\"f}\" \n    \n        return float(prc.format(f_val))\n\n    scores = []\n    for r in results:\n        scores.append(floatingDecimals(r.mean(),4))\n\n    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n    return scoreDataFrame","d36db605":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\n\n\ndef GetScaledModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n    elif nameOfScaler == 'Robust':\n        scaler= RobustScaler()\n    elif nameOfScaler == 'normalizer':\n        scaler= Normalizer()\n        \n    pipelines = []\n    #pipelines.append((nameOfScaler+'LN'  , Pipeline([('Scaler', scaler),('LN',  linear_model.LinearRegression())])))\n    #pipelines.append((nameOfScaler+'RID'  , Pipeline([('Scaler', scaler),('RID',  Ridge())])))\n    #pipelines.append((nameOfScaler+'LSO'  , Pipeline([('Scaler', scaler),('LSO',  Lasso())])))\n    #pipelines.append((nameOfScaler+'EN'  , Pipeline([('Scaler', scaler),('EN',  ElasticNet())])))\n    #pipelines.append((nameOfScaler+'kNNR'  , Pipeline([('Scaler', scaler),('KNNR',  KNeighborsRegressor())])))\n    #pipelines.append((nameOfScaler+'CARTR'  , Pipeline([('Scaler', scaler),('CARTR',  DecisionTreeRegressor())])))\n    #pipelines.append((nameOfScaler+'SVR'  , Pipeline([('Scaler', scaler),('SVR',  SVR())])))\n    pipelines.append((nameOfScaler+'PER'  , Pipeline([('Scaler', scaler),('PER', Perceptron( max_iter=1000,tol=1e-3,n_jobs=-1))])))\n    pipelines.append((nameOfScaler+'XGB'  , Pipeline([('Scaler', scaler),('XGB', XGBClassifier(n_jobs=-1))])))\n    pipelines.append((nameOfScaler+'LSVC'  , Pipeline([('Scaler', scaler),('LSVC',  LinearSVC(max_iter=1000))])))\n    pipelines.append((nameOfScaler+'SGDC'  , Pipeline([('Scaler', scaler),('SGDC',  SGDClassifier(max_iter=1000,tol=1e-3,n_jobs=-1))])))\n    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression(solver='lbfgs',max_iter=1000, multi_class='auto',n_jobs=-1))])))\n    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier(n_jobs=-1))])))\n    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC(gamma='scale',max_iter=1000))])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier(n_estimators=100,n_jobs=-1))])))\n    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier(n_estimators=100,n_jobs=-1))])))\n    \n    return pipelines ","ea9f915c":"class RandomSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def RandomSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = RandomizedSearchCV(self.model,\n                                 self.hyperparameters,\n                                 random_state=1,\n                                 n_iter=100,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.RandomSearch()\n        pred = best_model.predict(X_test)\n        return pred","aecf8f9e":"class GridSearch(object):\n    \n    def __init__(self,X_train,y_train,model,hyperparameters):\n        \n        self.X_train = X_train\n        self.y_train = y_train\n        self.model = model\n        self.hyperparameters = hyperparameters\n        \n    def GridSearch(self):\n        # Create randomized search 10-fold cross validation and 100 iterations\n        cv = 10\n        clf = GridSearchCV(self.model,\n                                 self.hyperparameters,\n                                 cv=cv,\n                                 verbose=0,\n                                 n_jobs=-1,\n                                 )\n        # Fit randomized search\n        best_model = clf.fit(self.X_train, self.y_train)\n        message = (best_model.best_score_, best_model.best_params_)\n        print(\"Best: %f using %s\" % (message))\n\n        return best_model,best_model.best_params_\n    \n    def BestModelPridict(self,X_test):\n        \n        best_model,_ = self.GridSearch()\n        pred = best_model.predict(X_test)\n        return pred","c3ebd854":"df_train= pd.read_csv(\"..\/input\/\/train.csv\")\ndf_validation= pd.read_csv(\"..\/input\/test.csv\")","6e8b577e":"df_train.head(5)","fbf8d00b":"df_validation.head(5)","addc6886":"df_train.shape, df_validation.shape","8b0ddc2c":"missing_data=df_train.isnull()\nmissing_data.sum()","e6c9f8c7":"df_train.describe(include='O')","21f1e814":"agmean= df_train[\"Age\"].median()\nagmean","fd336271":"df_train[\"Age\"].replace(np.nan,agmean,inplace=True)","100a481b":"df_train[\"Embarked\"].replace(np.nan,\"S\",inplace=True)\n","fd90efad":"df_train=df_train.drop(['Cabin'],axis=1)","a7779d2c":"missing_data2= df_train.isnull()\nmissing_data2.sum()","44a12e99":"df_train.columns","2427a59d":"df_train=df_train[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Fare', 'Embarked']]","d53919f0":"df_train.hist(bins=10, figsize=(20,15))\nplt.show()","9265e056":"df_train.plot(kind='box', subplots=True, layout=(5,5), sharex=False, sharey=False, figsize=(20,20))\nplt.show()","ca49c967":"scatter_matrix(df_train,figsize=(20,20))\nplt.show()","edee2a97":"df_train = pd.get_dummies(df_train)","42fc66e4":"df_train.corr()","2026338c":"corr_matrix= df_train.corr()","b2f6e495":"corr_matrix['Survived'].sort_values(ascending=False)","e8cad7de":"sns.heatmap(df_train.corr(), vmin=-1, vmax=1.0, annot=True)\nplt.show()","fac443f4":"X =  df_train.drop(['Survived'],axis=1)\ny = df_train['Survived']  ","fb40ddc8":"model = ExtraTreesClassifier(n_estimators=100,n_jobs=-1)\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","a3610981":"feat_importances.nlargest(75)","3a103ae9":"feat_importances","edb9b629":"df_clean_train = df_train","24b71d2a":"agmean= df_validation[\"Age\"].median()\nfaremean=df_validation['Fare'].median()\n\ndf_validation[\"Age\"].replace(np.nan,agmean,inplace=True)\ndf_validation[\"Fare\"].replace(np.nan,faremean,inplace=True)\ndf_validation[\"Embarked\"].replace(np.nan,\"S\",inplace=True)\n\n\ndf_validation=df_validation.drop(['Cabin'],axis=1)\n\nmissing_data2= df_validation.isnull()\nmissing_data2.sum()","f22e6b82":"df_clean_test=df_validation","f8ffaa2f":"df_clean_test=pd.get_dummies(df_clean_test)","fa4b3cb1":"X =  df_clean_train.drop(['Survived'],axis=1).values\ny = df_clean_train['Survived'].values","765dfa7d":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=42)","2eced179":"X_train.shape, X_test.shape , y_train.shape, y_test.shape","4daa0161":"models = GetBasedModel()\nnames,results = BasedLine2(X_train, y_train,models)\nfig = pyplot.figure()\nfig.suptitle( ' Algorithm Comparison ' )\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","2a8c66bf":"basedLineScore = ScoreDataFrame(names,results)\nbasedLineScore.sort_values(by='Score', ascending=False)","083b3165":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, y_train,models)\n\nscaledScoreStandard = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard], axis=1)\ncompareModels","a8400e14":"models = GetScaledModel('minmax')\nnames,results = BasedLine2(X_train, y_train,models)\n\n\nscaledScoreMinMax = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard,\n                          scaledScoreMinMax], axis=1)\ncompareModels","17e2bf7e":"models = GetScaledModel('normalizer')\nnames,results = BasedLine2(X_train, y_train,models)\n\n\nscaledScoreNormal = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard,\n                          scaledScoreMinMax,scaledScoreNormal], axis=1)\ncompareModels","d69fe3fd":"models = GetScaledModel('Robust')\nnames,results = BasedLine2(X_train, y_train,models)\n\n\nscaledScoreRobust = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard,\n                          scaledScoreMinMax,scaledScoreNormal,scaledScoreRobust], axis=1)\ncompareModels","c8b012d5":"def floatingDecimals(f_val, dec=10):\n        prc = \"{:.\"+str(dec)+\"f}\" #first cast decimal as str\n    #     print(prc) #str format output is {:.3f}\n        return float(prc.format(f_val))","99a58f0c":"X= df_clean_train[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_female','Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']].values\ny= df_clean_train['Survived'].values","ebaaea29":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state =0)","02f74d2d":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)","9ce50ca1":"rescaledValidationX = scaler.transform(X_test)","2e72fa50":"model = XGBClassifier()","7a0ca2db":"max_depth=[3] \nlearning_rate=[0.1,0.2,0.3] \nn_estimators=[100,200,300] \n#verbosity=[2]\n#silent=[None]\nobjective=['binary:logistic']\nbooster=['gbtree', 'gblinear','dart']\nn_jobs=[-1]\nnthread=[None]\ngamma=[0,0.1,0.2,1]\nmin_child_weight=[1,2,3]\nmax_delta_step=[0,1,2,3]\nsubsample=[0,1]\n#colsample_bytree=[0,1]\n#colsample_bylevel=[0,1]\n#colsample_bynode=[0,1]\nreg_alpha=[0,1,2]\nreg_lambda=[1,2,3]\nscale_pos_weight=[1,2]\nbase_score=[0.5,0.6,0.7,0.8,0.9]\nrandom_state=[42]\nseed=[None] \nmissing=[None]\n\n\nhyperparameters= dict(max_depth=max_depth, \nlearning_rate=learning_rate, \nn_estimators=n_estimators,\n#verbosity=verbosity,\n#silent=silent,\nobjective=objective,\nbooster=booster,\nn_jobs=n_jobs,\nnthread=nthread,\ngamma=gamma,\nmin_child_weight=min_child_weight,\nmax_delta_step=max_delta_step,\nsubsample=subsample,\n#colsample_bytree=colsample_bytree,\n#colsample_bylevel=colsample_bylevel,\n#colsample_bynode=colsample_bynode,\nreg_alpha=reg_alpha,\nreg_lambda=reg_lambda,\nscale_pos_weight=scale_pos_weight,\nbase_score=base_score,\nrandom_state=random_state,\nseed=seed, \nmissing=missing\n)","67d61200":"RandSearch = RandomSearch(rescaledX,y_train,model,hyperparameters)\n# LR_best_model,LR_best_params = LR_RandSearch.RandomSearch()\nPrediction =RandSearch.BestModelPridict(rescaledValidationX)","5e32017b":"X= df_clean_train[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_female','Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']].values\ny= df_clean_train['Survived'].values","895b32b1":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state =0)","692546bd":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = XGBClassifier(subsample= 1, seed= None, scale_pos_weight=1, reg_lambda= 1, reg_alpha=2, random_state=42, objective= 'binary:logistic', nthread=None, n_jobs=-1, n_estimators= 200, missing=None, min_child_weight= 2, max_depth= 3, max_delta_step= 0, learning_rate=0.2, gamma= 0.2, booster='gbtree', base_score=0.9)\nmodel.fit(rescaledX, y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = model.predict(rescaledValidationX)\n","fd61d3e6":"model.score(rescaledX, y_train)\n","bac7c195":"model.score(rescaledValidationX, y_test)","6e28e47b":"final=df_clean_test[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_female','Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']].values","5baaf944":"rescaledFinal = scaler.transform(final)","f4dc745c":"predictions = model.predict(rescaledFinal)","4a7a5314":"output = pd.DataFrame({'PassengerId': df_validation.PassengerId,'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\noutput.head()\n","5ded180d":"#### Min-Max Scaler","d9dcb0aa":"#### Define X and y","0f6b4292":"### Feature Importance","3ed6f6df":"## Feauture selection\n","5633a0a7":"# Let's Start Playing ","22c0beeb":"### Now lest use the validation data set","c011b55f":"What we can do to deal with missing data?","93e7c71c":"## Kfold ","8690dcc7":"#### Standard Scaler","7e33f08b":"### Scores Comparison ","566c55ae":"### Scores Comparison After Feature Scaling","68b78978":"#### Normalizer Scaler","5c577344":"## XGBClassifier","1b536daf":"### Plotting Histograms","08f23a05":"## Load Data Sets","c7c7b80a":" # Load Funtions","cafdbd11":"# Feature Selection Using  Feature Importance","69ee7923":"# Algorithm Tunning","94a46c9a":"## Spot-Check Algorithms ","7fea35c8":" 1-drop data\n \n    a. drop the whole row\n    b. drop the whole column\n    \n 2-replace data\n \n    a. replace it by mean\n    b. replace it by frequency\n    c. replace it based on other functions\n","5064c2e9":"## Algorithm Tunning","92d79f42":"## Define X_train, y_train,X_test","e4760b23":"## Missing data fix","130b71e5":"#### Robust Scaler","a0213a0c":"## Basic EDA","2a7fa9e0":"### Ploting Box Plots","32810b9a":" This part needs to be analisy carefully because we need to know why we are droping and replacing the null values","0e0b5082":"### Grid Search","3c0f63e3":"## Check for Categorical Values","466f8f76":"I used a lot of the code from this amazing work  https:\/\/www.kaggle.com\/pouryaayria\/a-complete-ml-pipeline-tutorial-acu-86, thank you for sharing.","9a4123a8":"# Time to play with Algorithms","5388d826":"#  Understand Your Data With Descriptive Statistics and Visualizations and cleaning it.","ade9d5cf":"#  Load Libraries","a88a8c0d":"#### Model","545e88bd":"## Score Data Frame","b532d4ed":"## Algorithm Comparison ","654abcd6":"# All the changes that you did to the training set, needs to be done in the validation set.","d5b1cff0":"## Evaluation for missing values","70702875":"## Feature Scaling","205ba7c8":"## Dealing with missing data","96d920ee":"###  Select your features and start playing with your algorithms ","ddbce52c":"## Now we Fit our model","5f2a8561":"### Random Searh","8a1842f4":"## PlotBox","32419b73":"# Finalize the model","b34dda52":"If it is a Classification problem, scoring can be 'accuracy', 'neg_log_loss' , 'roc_auc' also we can use  confusion_matrix(Y_test, predicted) and classification_report(Y_test, predicted)\n\nIf it is a Regression problem, scoring can be 'neg_mean_absolute_error' , 'neg_mean_squared_error', 'r2'.","2508dacd":"### Analysing Correlation ","631b5aba":"### All together with a scaler plot matrix"}}