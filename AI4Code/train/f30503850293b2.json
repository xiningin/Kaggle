{"cell_type":{"e42bb82f":"code","adb59e34":"code","f71e7740":"code","ddb6b319":"code","13b85573":"code","4de52972":"code","11e5ff5d":"code","7e0952b4":"code","38eae4b1":"code","d8863866":"code","fce2e085":"code","35224619":"code","3f6ed825":"code","07d86027":"code","7181a708":"code","108f4b89":"code","8d0dd5e3":"code","60c7043c":"code","f841d5a1":"code","f237a570":"code","db0abef9":"code","cf3269f6":"markdown","a68a033a":"markdown","1d04e667":"markdown","af46cc24":"markdown","c4391a8c":"markdown","87919b46":"markdown","70ea7fc3":"markdown","327a950f":"markdown","b2956572":"markdown","3da6efeb":"markdown","192c102e":"markdown","beb68048":"markdown","e24d523b":"markdown","e09038fa":"markdown","a7d24d83":"markdown","c246349e":"markdown","6ee095e7":"markdown","2f7e224c":"markdown","fa3e4aec":"markdown","918ac620":"markdown","d066e746":"markdown","c64df1e2":"markdown","00195023":"markdown","faaefdc5":"markdown","bb3aae03":"markdown","85ec9bd0":"markdown","4f192447":"markdown","00ce8162":"markdown","0327267f":"markdown","2f5e9e96":"markdown","12c5e199":"markdown","93de23bd":"markdown","f2f8e96d":"markdown"},"source":{"e42bb82f":"# We shall see some examples\nimport pandas as pd\ntitanic_data = pd.read_csv('..\/input\/titanic\/train.csv')\nprint(\"Example of some data objects: Each row represents a data object:\")\ntitanic_data.head(3)\n","adb59e34":"# The attributes or features in the dataset describing each data object or tuple\nprint(list(titanic_data.columns))","f71e7740":"# You may have already guessed nominal attributes in the titanic dataset by seeing the rows above\n# Here are the columns which represent nominal attributes\nprint(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin','Embarked'])\nprint(\"You may have noticed that Pclass contains numeric values but is still nominal.\")","ddb6b319":"# The 'Survived' attribute is a binary attribute\nprint(titanic_data['Survived'].value_counts())\nprint(\"Clearly only two classes, 0 and 1, exist for 'Survived' attribute. Again it contains numeric values but is actually binary nominal.\")","13b85573":"# In our Titanic dataset, we have following numeric attributes\nprint(['Age','Fare'],\": Numeric attributes\")\nprint(\"Some values of 'Age' are:\")\nprint(titanic_data['Age'][0:3])\nprint(\"Some values of 'Fare' are:\")\nprint(titanic_data['Fare'][0:3])","4de52972":"# In the titanic dataset, 'Pclass' is an ordinal attribute.\nprint(\"It is obvious that class 1, 2 and 3 have an order among themselves.\")\nprint(titanic_data['Pclass'].value_counts())\nprint(\"Here 3, 2, 1 are Pclass values while numbers in front of them are their frequency in the dataset\")","11e5ff5d":"# In our Titanic dataset 'Fare' is continuous and 'Pclass', 'SibSp', 'Survived', etc. are discrete.\nprint(\"An example of continous attribute:\")\nprint(titanic_data['Fare'][0:3])\nprint(\"An example of discrete attribute:\")\nprint(titanic_data['Pclass'][0:3])","7e0952b4":"# We shall use predefined mean function to calculate the mean, which employs same mathematical procedure\nprint(\"Mean of Fare: \",titanic_data['Fare'].mean())\nprint(\"Note:Outliers and distribution are not dealt with for sake of understanding.\")","38eae4b1":"# Let's compute the median for 'Pclass'(ordinal) and 'Age' (numeric)\nprint(\"Median for Pclass:\", titanic_data['Pclass'].median())\nprint(\"Median for Age:\", titanic_data['Age'].median())","d8863866":"# We shall see the mode of 'Pclass' and 'Survived'\nprint(\"Mode of Pclass:\", (titanic_data['Pclass'].mode())[0])\nprint(\"Mode of Survived:\", (titanic_data['Survived'].mode())[0])","fce2e085":"# We will calculate range of 'Age' in Titanic dataset\nprint(\"Range of Age:\", titanic_data['Age'].max()-titanic_data['Age'].min())","35224619":"# Lets see the distribution of 'Age' in titanic dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns # seaborn is a popular visualization library\nsns.distplot(titanic_data['Age'])\nplt.show()\nprint(\"The distribution is slightly positivly skewed.\")","3f6ed825":"# Let's plot box plot\n# Box plot can also be used for outlier detection. It also shows Interquartile range, median, Q1, Q3, etc.\nsns.boxplot(x=titanic_data['Survived'], y=titanic_data['Age'])\nplt.show()\nprint(\"Box plots show outliers, IQR, Q1, Q3, median and min and max in 1.5xIQR on both sides\")","07d86027":"# Kurtosis can be calculated using inbuilt methods\nprint(\"Kurtosis of Age:\", titanic_data['Age'].kurtosis())\nprint(\"Note:- This inbuilt method considers kurtosis of normal distribuiton as 0.0 (Fisher's method)\")","7181a708":"# We can easily compute variance and standard deviation of data using inbuilt methods\n# We shall use a new dataset which contains several different types of attributes.\nhp_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","108f4b89":"# This dataset has an attribute 'SalePrice' which contains prices of houses\n# We shall calculate the variance and standard deviation of this attribute\n# This is custom formulae to calculate variance and standard deviation\nimport numpy as np\nprint(\"Custom formulae to calculate variance and standard deviation.\")\nn=(hp_data['SalePrice'].shape)[0]\nmean = (hp_data['SalePrice'].sum())\/n\nv = (abs(hp_data['SalePrice']-mean))**2 # This is called broadcasting in pyhton. Read more in documentation\nvar = sum(v)\/n\nprint(\"Variance:\", var)\nprint(\"Standard deviation:\" , np.sqrt(var))","8d0dd5e3":"# Inbuilt functions can also be used for same purpose.\nprint(\"Variance and standard deviation using inbuilt numpy functions.\")\nprint(\"Variance of SalePrice:\",np.var(hp_data['SalePrice']))\nprint(\"Standard Deviation of SalePrice:\",np.std(hp_data['SalePrice']))","60c7043c":"# Distribution plot of 'SalePrice' attribute in house price data\nplt.figure(figsize=(8,6))\nplt.xlabel(\"Sale Price\")\nsns.distplot(hp_data['SalePrice'])\nplt.show()\nprint(\"It is clear from the figure that distribution is positively skewed.\")","f841d5a1":"# Following is an example of histogram of attribute 'MSZoning' is house-price data\nplt.figure(figsize=(8,6))\nsns.countplot(hp_data['MSZoning'])\nplt.show()","f237a570":"# Here is a scatter plot of 'close\/last' and 'open' of macdonald-stock-price dataset\nmcd_stock = pd.read_csv('..\/input\/eda-and-cleaning-mcdonald-s-stock-price-data\/final.csv')\nsns.scatterplot(x='Close\/Last', y='Open', data=mcd_stock)\nplt.show()\nprint(\"The plot shows high positive correlation and there a no possible outliers\")","db0abef9":"# Let's create a box plot for some attributes\nsns.boxplot(y='Age',x='Sex',data=titanic_data)\nplt.show()\nprint(\"The points above 'male' can be possible outliers and the line in box is the median\")","cf3269f6":"<a id=\"prox_binary\"><\/a>\n### 4.3 Proximity measures for Binary Attributes\nSince binary attributes are similar to nominal attributes, proximity measures for binary attributes are also similar to that of nominal attributes. For symmetric binary attributes, the process is same i.e. <br> $$d(i,j)=\\frac{p-m}{p}$$ <br>\nHowever, for asymmetric binary attributes, we drop the number of matched zeros (where an attribute of both tuples is zero). Let $s$ be the cases where matched attributes are both zero then,\n<br> $$d(i,j)=\\frac{p-m}{p-s}$$ <br>\nWe can calculate similarity as, <br>\n$$s(i,j) = 1-d(i,j)$$","a68a033a":"<a id=\"matrices\"><\/a>\n### 4.1 Data Matrix and Dissimilarity Matrix\nData matrix is nothing but all the data tuples stacked as a matrix. **Data matrix is tuple vs attribute matrix.**\n<br><br>\n$$\n\\begin{bmatrix}\nx_{11} & ... & x_{1f} & ... & x_{1p} \\\\\n... & ... & ... & ... & ... \\\\\nx_{i1} & ... & x_{if} & ... & x_{ip} \\\\\n... & ... & ... & ... & ... \\\\\nx_{n1} & ... & x_{nf} & ... & x_{np}\n\\end{bmatrix}\n$$\n<br>\nDissimilarity matrix is a matrix of pairwise dissimilarity among the data tuples. It is often desirable to keep only lower triangle or upper triangle of a dissimilarity matrix. **Dissimilarity matrix is a tuple vs tuple matrix.**\n<br><br>\n$$\n\\begin{bmatrix}\n0 \\\\\nd(2,1) & 0\\\\\nd(3,1) & d(3,2) & 0\\\\\n. & . & . \\\\\n. & . & . \\\\\nd(n,1) & d(n,2) & ... & ... & 0\n\\end{bmatrix}\n$$","1d04e667":"<a id=\"data_viz\"><\/a>\n### 3.7 Basic Data Vizualizations\nData analysis is incomplete without data visualization and often visualizations help in recognizing some properties and pattern of data that couldn't be recognized by looking at the raw data.\n<br><br>\nThere are a large number of data visualzation techniques however, **distribution plots, histograms, scatter plots and box and whiskers plots** are very common. Here, a brief introduction of each of the above mentioned visualization technique is given along with the sample code.","af46cc24":"# Types of Data, Statistics and Proximity Measures\n\n##### Although its tempting to jump straight into models and play around with data, it is a pitfall to which almost everyone as a beginner must have fallen. Creating a robust algorithm is important but to be able to do so, it is equally important to know your data. Real world data is usually messy and comes from a number of different sources. It can be large, it can be arbitrarily complex and it can be really messy. Therefore it is important to know everything about data before doing anything else. In this notebook, we shall see the types of data, basic statistics, measures of dispersion, proximity measures, etc. which will prove to be highly important later when dealing with real world data. This notebook aims at providing basic knowledge needed to be known by every data scientist which ultimately helps in better results. Without knowing data, it is difficult to design efficient solutions. \n\n#####  Please hit upvote if you find the notebook useful.\n\n### Table of content\n*  **[Data Objects and Attribute Types](#obj_attr)**\n    * [Nominal Attributes](#nominal)\n    * [Binary Attributes](#binary)\n    * [Numeric Attribites](#numeric)\n    * [Ordinal Attributes](#ordinal)\n    * [Continuous and Discrete Attributes](#cont_dis)\n* **[Measures of Central Tendency](#central_tendency)**\n    * [Mean](#mean)\n    * [Median](#median)\n    * [Mode](#mode)\n* **[Measures of Dispersion of data](#dispersion)**\n    * [Range](#range)\n    * [Quantiles](#quantile)\n    * [Quartiles, Interquartile Range and Percentile](#quartile)\n    * [Skewness](#skew)\n    * [Kurtosis](#kurtosis)\n    * [Variance and Standard Deviation](#var_std)\n    * [Basic data visualizations](#data_viz)\n        * [Distribution plots](#distplot)\n        * [Histograms](#hist)\n        * [Scatter plots](#scatter)\n        * [Box and whiskers plots](#boxplot)\n* **[Proximity Measures](#proximity)**\n    * [Data Matrix and Dissimilarity Matrix](#matrices)\n    * [Proximity Measures for Nominal Attributes](#prox_nominal)\n    * [Proximity Measures for Binary Attributes](#prox_binary)\n    * [Proximity Measures for Numeric Data:Minkowski Distance](#prox_numeric)\n    * [Proximity Measures for Ordinal Data](#prox_ordinal)\n    * [Proximity measures for mixed attribute type data](#prox_mix)\n    * [Cosine similarity](#prox_cosine)\n\n\nReferred TextBook - [DataMining:Concepts and Techniques by Jiawei Han, Micheline Kamber and Jian Pei](https:\/\/github.com\/mohtashim-nawaz\/Books\/tree\/master\/Data%20Science)\n<br><br>\nFor basics of numpy: [Basics of Numpy](https:\/\/www.kaggle.com\/mohtashimnawaz\/numpy-with-jokes-and-funs) <br>\nFor basics of Pandas: [Basics of Pandas](https:\/\/www.kaggle.com\/mohtashimnawaz\/easy-peasy-pandas-with-jokes)\n<br><br>\n**Note: Since notebook contains latex mathemetical formulae, it may take some time to load the notebook properly.**\n**All the figures have been taken from the reffered book.**","c4391a8c":"<a id=\"binary\"><\/a>\n### 1.2. Binary Attributes\nBinary attributes are a special type of nominal attributes which holds only two possible values often pertainig to a truth and false. All other properties of nominal attributes are applicable to binary attributes. For example, in *Titanic Dataset*, the attribute *Survived* is a binary attribute.\n<br><br>\nBinary attributes are of two types, ***symmetric*** binary attributes and ***asymmetric*** attributes. The attributes whose binary values are not biased towards any class are symmetric attributes while others are asymmetric attributes.\n<br><br> \nAlthough, binary attributes are often associated with positive and negative class, they should not be confused to have any order. Like nominal attributes, these class names simply denote two different classed which have no order.","87919b46":"<a id=\"distplot\"><\/a>\n#### 3.7.1 Distribution plots\nDistribution plots are used to visualized the **distribution of numeric attributes**. Such plots are very helpful to get the sense of overall data distribution and to know the status of **skewness in data**.\n<br><br>\nDistribution plots are used for univariate analysis (invloving a single variable\/attribute\/feature).","70ea7fc3":"<a id=\"quantile\"><\/a>\n### 3.2 Quantiles\nLet's suppose the data is sorted in increasing order, we can choose some points which can split the data distribution into approximately equal sized consecutive sets.These points are called **quantiles**.\n<br><br>\nWhen a single data point is choosen dividing the data distribution into two equal halves, it is called 2-Quantile, when two data points are choosen it is called 3-Quantile and so on. There are $k-1$ data split points for $k$-quantiles.\n<br><br> \nA $k$th $q$-quantile for a given data distribution is a value $x$ such the at most $\\frac{k}{q}$ values are lesser than $x$ and at most $\\frac{(k-q)}{q}$ values are greater than $x$. For e.g. in a 4-quantile Q1 splits data into 25% and 75%, Q2 divides data into 50% each and Q3 divides into 75% and 25% each.\n<br><br>\n**2-Quantile corresponds to median.**","327a950f":"<a id=\"quartile\"><\/a>\n### 3.3 Quartiles, Interquartile Range and Percentile\nQuartile is nothing but a specific case of quantiles. **4-quantiles are reffered as quartiles**. In the quatiles, Q1, Q2 and Q3 are respective dividing points where **Q2 corresponds to median**. Quartiles provide an indication of center, shape and spread of data distribution.\n<br><br>\n**Interquartile range is simply the distance (difference) between Q3 and Q1 of quartiles**.\nso the IQR can be defined as,\n<br>$$IQR = Q3-Q1$$<br>\n**Percentile is simply 100-quantiles** which divides the data distribution into 100 essentially equal parts.\n<br><br>\n***Five Number Summary is yet another useful concept which is simply a collection of five numbers which are min, Q1, median, Q3, max of a given data.***\n<br><br> Figure below summarizes the concepts.\n![bell_curve.png](attachment:bell_curve.png)","b2956572":"<a id=\"central_tendency\"><\/a>\n## 2. Measures of Central Tendency\nAfter knowing the attribute type and their properties, it is often helpful to know measures of central tendency. The measures of central tendency denote the central location of the data distribution and are therefore helpful in providing a  general overview. Also, mean, median, mode are often used in data preprocessing tasks which is the next step after knowing the data.\n<br><br>\nWe shall see the most common measures of central tendency which are mean, median and mode.","3da6efeb":"<a id=\"proximity\"><\/a>\n## 4. Proximity Measures\nThere are applications like clutering, outlier analysis and nearest neighbor classification in machine learning and data mining where we need to asses the similarity\/dissimilarity of data tuples. **Proximity measures are mathemetical techniques and formulae to asses the similarity\/dissimilarity of data tuples**.\n<br><br>\n**Similarity and dissimilarity are related where given dissimilarity, we can calculate similarity easliy by subtracting dissimilarity value from 1 ( if in range $[0,1]$ )**.\n<br><br>\nProximity measures are different for different types of attributes. We shall discuss proximty measure for all types of attributes seen previously. We shall then see proximity measure for mixed attribute type data and cosine similarity. Howeve, before that it is important to know data matrix and dissimilarity matrix.","192c102e":"<a id=\"range\"><\/a>\n### 3.1 Range\nRange is simply the **difference of maximum and minimum** data value among all data values. It can be simply calculated by finding maximum and minimum values of a data.","beb68048":"<a id=\"cont_dis\"><\/a>\n### 1.5. Continuous and Discrete Attributes\nThere is yet one more way of classification of attributes. The attributes having a number of ***countable*** different values are called discrete attributes. Other type of attributes are called continuous attributes which have infinite number of ***non-countable*** values.\n<br><br>\nDiscrete attributes can have infinite number of values but the values are countable e.g. an attribute having natural numbers as values. On the other hand, continuous attributes are often represented with real numbers.","e24d523b":"<a id=\"prox_cosine\"><\/a>\n### 4.7 Cosine Proximity\nCosine similarity is not as popular proximity measure as above described methods but is important for comparing documents. Let $x$ and $y$ be two term frequecy vectors representing two documents, cosine similarity can be computed as, <br><br>\n$$sim(x,y) = \\frac{x.y}{||x|||y||}$$\n<br>\nwhere $||x||$ is euclidean norm of $x$.<br><br>\nCosine similarity is actually based on dot product of vectors $sim(x,y)=0$ implies that two vectors are orthogonal, or have no match. As value becomes closer to $0$, similarity between $x$ and $y$ increases.","e09038fa":"<a id=\"mean\"><\/a>\n### 2.1 Mean\nMean or average is the most common and effective measure of central tendency which is calculated as the summation of all data values devided by number of data values. \n<br><br>\nMean can be calculated as,<br><br>\n$$\\bar{x}=\\frac{\\sum_{i=1}^N x_i}{N} = \\frac{x_1 + x_2 +...+x_N}{N}$$\n<br><br>\nIf certain weights are given to each attribute, weighted mean can be calculated as<br><br>\n$$\\bar{x}=\\frac{\\sum_{i=1}^N w_i x_i}{\\sum_{i=1}^N w_i} = \\frac{w_1x_1 + w_2x_2 +...+w_Nx_N}{w_1+w_2+...+w_N}$$\n<br><br>\nAlthough mean is very commonly used, it is **less effective in presence of outliers**. In other words, mean is more sensitive to outliers.","a7d24d83":"##### While data objects and attribute types familiarized with the type of data which is found in real world, measures of central tendency and measures for dispersion of data provide ways to gain insights about data. It is really important to know everything about data before proceeding to next step (i.e. EDA and Preprocessing) failing which leads to difficulty in further steps. In the end, proximity measures provide mathemetics involved to calculate dissimilarity\/similarity which is used in complex algorithms like kNN, clustering algorithms, etc.\n\n**PS- I have started a medium publication where we shall be publishing Data Science Learning Material in a sequential way. You are invited to contribute. **\nPlease visit [here](https:\/\/medium.com\/scratch-data-science) for further details.","c246349e":"<a id=\"numeric\"><\/a>\n### 1.3. Numeric Attributes\nNumeric attributes are used to measure a quantity and are therefore *quantitative*. It is a measurable quantity specified as real numbers. All the mathematical operations are applicable on numeric attributes. For example mean, median, ditributions, etc. are often calculated on numeric data. However, mode may not be of much use for numeric data.\n<br><br>\nNumeric data is of two types: ***interval-scaled*** numeric attributes and ***ratio-scaled*** numeric attributes.","6ee095e7":"<a id=\"skew\"><\/a>\n### 3.4 Skewness\nSkewness is related to the distribution of data. If a distribution curve is perfectly symmetric, it is called normally distributed and such a distribution is not skewed.\n<br><br>\nData distribution can be either **positively skewed if the tail ends towards right** or **negatively skewed if the tail ends towards left**. The details can be found in figure.\n<br><br>\nFor positively skewed data, median is lesser than mean while for negatively skewed data, median is greater than mean.\n<br><br>\nSkeweness is often visualized and analyzed with the help of distribution curves however, box and whiskers plots can also be used for the same task. Box plots can also be used to detect outliers.\n<br>\n![skew.png](attachment:skew.png)","2f7e224c":"<a id=\"nominal\"><\/a>\n### 1.1. Nominal Attributes\nNominal stands for \"name like\". The value of nominal attributes is name of a thing or a symbol which denotes some category, code or state. They are thus also reffered as ***categorical attributes***. Nominal attrbutes do not have any order and therefore are not applicable for any mathemetical calculations like mean, median, standard deviation, etc. However, mode, the most commonly occuring value can be calculated.\n<br><br>\nAlthough nominal attributes represent a category, they can have numeric values which may represent some code or state, etc. However, the numeric value should still be considered as categorical. For e.g. *average*, *good*, *excellant* can be a grading criteria where values to each category are given as 0,1 and 2 respectively. However, they should not be misunderstood as numeric data. Often such type of data is called false numeric and should be carefully dealt. ","fa3e4aec":"<a id = \"obj_attr\"><\/a>\n## 1. Data Objects and Attribute Types\nDataset is often organized in form of data objects where each data object represents an entity. In different literatures these data objects have different names like ***tuples***, ***objects***, or even ***rows***.\n<br><br>\nA data object represents an entity on the basis of certain characterstics which are called as ***attributes***. Likewise, attributes also have different names in different literature like ***feature***, ***dimension*** and ***variable***. Since each data object is a vector of attributes, it is sometimes reffered to as ***feature vector***.\n<br><br>\nThe attributes are classified in different categories on the basis of different properties. Also, division on the basis on different properties may not be disjoint.","918ac620":"<a id=\"prox_ordinal\"><\/a>\n### 4.5 Proximity measures for ordinal attributes\nOrdinal attributes have a meaningful order among their attributes values therefore, they are **treated similar to numeric attributes**. However, to do so, it is important to convert the states to numbers where each state of an ordinal attribute is assigned a number corresponding to the order of attribute values.\nFor e.g if a grading system have grades as A, B and C, then the number can be given as C=1, B=2 and A=3.\n<br><br>\nSince number of states can be different for different ordinal attributes, it is therefore **required to scale the values to common range** e.g $[0,1]$. This can be done using given formula, <br><br>\n$$z_{if} = \\frac{r_{if}-1}{M_f-1}$$\n<br>\nwhere $M$ is maximum number assigned to states and $r$ is rank(numeric value) of a patricular object.\n<br><br>\nAfter the scaling is done, we can simply apply same distance metrics as given for numeric attributes. The similarity can be calculated as:\n<br><br>$$s(i,j) = 1-d(i,j)$$","d066e746":"<a id=\"hist\"><\/a>\n#### 3.7.2 Histograms\nHistograms are a classic way to graphically represent **distribution of an attribute by counting frequency** of different items in data where taller bars represent more data falls into that particular category.\n<br><br> \nFor non-numeric data, frequency for different items is counted and displayed. However, for numeric data, data needs to be discritized. \n<br><br>\nHistograms are used for univariate analysis.","c64df1e2":"<a id=\"kurtosis\"><\/a>\n### 3.5 Kurtosis\nKurtosis is a measure of how much tails of a distribution differ from a normal distribution. In other words, kurtosis identifies if tails of a distribution contain extreme values.<br><br>\nType of kurtosis depends on excess kurtosis which is calculated as,<br><br>\n$$excess = kurtosis-3$$\n<br>\nWhere $3$ corresponds to kurtosis of normal\/gaussian bell shaped distribution.\n<br><br>\nWhen excess kurtosis is zero or close to zero, distribution is said to be **mesokurtic**. Such type of ditribution is very similar to normal distribution.<br>\nWhen excess kurtosis is positive, distribution is said to be **leptokurtic**. This type of distribution contains extreme values at tails.<br>\nWhen excess kurtosis is negative, distribution is said to be **platykurtic**. This type of distribution indicates absence of extreme values.","00195023":"<a id=\"mode\"><\/a>\n### 2.3 Mode\nMode simply means the **most commonly occuring** value among all the data values. It is defined for all types of attributes but is of little use for continuous attributes.\n<br><br>\nDataset having a single mode is called unimodal, having two modes is called bimodal ans so on. Generally, dataset having more than three modes is called polymodal. \n<br><br>\nFor a unimodal and moderately skewed data, a relation  among mean, median and mode exists which is given as,<br><br>\n$$mean-mode \\approx 3 \\times (mean-median) $$","faaefdc5":"<a id=\"median\"><\/a>\n### 2.2 Median\nMedian is also one of the most commonly used measure of central tendency. Median can be calculated by sorting the data and then finding the middle value. \n<br><br>\nWhen the number of data values are odd, a single middle value exists, however, when the number of data values is even, two values are in the middle of data. Everything occuring between these two middle values is a median. However, most commonly, mean of these values is taken to be the median. \n<br><br>\nMedian is expensive to compute but can be approximated easily when the data is divided into intervals. The formula used is given as,<br><br>\n$$median = L_1 + \\left(\\frac{ N\/2 - (\\sum freq)_l}{freq_median} \\right)width$$\n<br>\nwhere $L_1$ is lower boundary of median interval, $N$ is number of values in entire dataset, $\\sum{(freq)_l}$ is sum of frequencies of all intervals lower than median interval, $freq_{median}$ is frequency of median interval and $width$ is width of median interval.\n<br><br>\nMedian is **not defined for nominal and binary attributes** and can be found for numeric and ordinal attributes.","bb3aae03":"<a id=\"var_std\"><\/a>\n### 3.6 Variance and Standard Deviation\nVariance and standard deviation are the measure of spread and indicate how data distribution is spread. A high standard deviation means data values are spread over a wide range while low value denotes that data values are close to mean. \n<br><br>\nVariance and standard deviation can be calculated as,<br><br>\n$$\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N \\left( x_i - \\bar{x}\\right)^2 = \\left(\\frac{1}{N}\\sum_{i=1}^N x_i^2 \\right) - \\bar{x}^2$$\n<br>\n**Standard deviation is the positive square root of variance.** \n<br>\n$$\\sigma = +\\sqrt{\\sigma^2}$$\n<br>Standard deviation is less sensitive to outliers as compared to variance.","85ec9bd0":"<a id=\"prox_mix\"><\/a>\n### 4.6 Proximity Measures for Mixed Attribute Types\nReal world data is often described by a mixture of different types of attributes, so it is important to define proximity measure for such data. \n<br><br>\nApproach is to combine all the attributes into a single dissimilarity matrix, bringing all meaningful attributes to a common scale of $[0,1]$.<br><br>\n$$d(i,j) = \\frac{\\sum_{f=1}^p \\delta_{ij}^{(f)} d_{ij}^{(f)}}{\\sum_{f=1}^p \\delta_{ij}^{(f)}}$$\n<br> where $\\delta_{ij}^{(f)} = 0$ if <br>\n(1) $x_{if}$ or $x_{jf}$ is missing, <br>\n(2) $x_{if} = x_{jf} = 0$ and attribute $f$ is asymmetric binary.\nOtherwise, $\\delta_{ij}^{(f)} = 1$.\n<br><br>\n$d_{ij}^{(f)}$ depends on type of attribute and can be calculated as:<br><br>\n(1) If $f$ is numeric $d_{ij}^{(f)} = \\frac{|{x_{if}-x_{jf}}|}{max_hx_{hf}-min_hx_{hf}}$ \nwhere $h$ runs over all non-missing objects for attribute $f$.<br>\n(2) If $f$ is nominal or binary, $d_{ij}^{(f)} = 0$, if $x_{if}=x_{jf}$, otherwise $d_{ij}^{(f)}=1$.<br>\n(3) If $f$ is ordinal, compute the ranks(i.e. assign values), $r_{if}$ and $z_{if} = \\frac{r_{if}-1}{M_f-1}$ and treat $z_{if}$ as numeric.\n<br><br>\nAll the steps are very similar to what we have already seen and numeric attributes are normalized to $[0,1]$ for same reasons described above.","4f192447":"<a id=\"dispersion\"><\/a>\n## 3. Measures of Dispersion of data\nDispersion or spread of numeric data is very useful when analysing the data. Most commonly used measures to asses the dispersion of data are discussed below.","00ce8162":"<a id=\"prox_nominal\"><\/a>\n### 4.2 Proximity measures for Nominal Attributes\nNominal attributes can have two or more different states e.g. an attribute 'color' can have values like 'Red', 'Green', 'Yellow', etc. Dissimilarity for nominal attributes is calculated as the ratio of total number of mismatches between two data tuples to the total number of attributes.\n<br><br>\nLet $M$ be the total number of states of a nominal attribute. Then the states can be numbered from 1 to $M$. However, the numbering does not denote any kind of ordering and can not be used for any mathemetical operations.\n<br><br>\nLet $m$ be total number of matches between two tuple attributes and $p$ be total number of attributes, then the dissimilarity can be calculated as,\n$$d(i,j)=\\frac{p-m}{p}$$\n<br>\nWe can calculate similarity as, <br>\n$$s(i,j) = 1-d(i,j)$$","0327267f":"More information on distribution plot and box and whiskers plot can be found in referred book and [here(distplots)](https:\/\/seaborn.pydata.org\/tutorial\/distributions.html) and [here(boxplots)](https:\/\/seaborn.pydata.org\/generated\/seaborn.boxplot.html).","2f5e9e96":"<a id=\"prox_numeric\"><\/a>\n### 4.4 Proximity Measures for Numeric Data : Minkowski Distance\n**Distance or dissimilarity between two numric attributes is commonly measured using minkowski distance, manhattan or euclidean distance.**\n<br><br>\nIt is important to scale the data point to a common range usually $[0,1]$ or $[-1,1]$. This is to avoid attributes having high values from outweighing those with lower values.\n<br><br>\n**Euclidean distance is most popular distance metric for numeric attributes also known as straight line distance**. It can be calculated as,<br><br>\n$$d(i,j) = \\sqrt{|x_{i1}-x_{j1}|^2+|x_{i2}-x_{j2}|^2+...+|x_{ip}-x_{jp}|^2}$$\n<br>\nAnother popular distance measure is **manhattan or city block distance** which is calculated as,<br><br>\n$$d(i,j)=|x_{i1}-x_{j1}|+|x_{i2}-x_{j2}|+...+|x_{ip}-x_{jp}|$$\n<br>\nBoth above mentioned distance measures follow the following properties: <br>\n<br>$$d(i,j) >= 0$$\n<br>$$d(i,i) = 0$$\n<br>$$d(i,j) = d(j,i)$$\n<br>$$d(i,k) <= d(i,j) + d(j,k)$$<br>\nA generalized distance measure is **minkowski distance** given as,<br><br>\n$$d(i,j) = \\sqrt[{h}]{|x_{i1}-x_{j1}|^h+|x_{i2}-x_{j2}|^h+...+|x_{ip}-x_{jp}|^h}$$\n<br>\n**For $h$=1, minkowski distance corresponds to manhattan distance.**\n<br>**For $h$=2, minkowski distance corresponds to euclidean distance.**\n<br>**For $h\\to \\infty$, the minkowski distance corresponds to $L_{\\infty}$ norm or uniform norm** which is given as,<br><br>\n$$d(i,j)=\\lim_{h\\to \\infty}\\left(\\sum_{f=1}^p|x_{if}-x_{jf}|^h\\right)^{\\frac{1}{h}} = \\max_f^p |x_{if}-x_{jf}|$$\n<br>\nAll the above mentioned distance measures correspond to unweighted attributes, however, sometimes attributes can be assigned weights. In such case, respective attribute terms are multiplied to weights. The formula thus becomes,<br><br>\n$$d(i,j) = \\sqrt{w_1|x_{i1}-x_{j1}|^2+w_2|x_{i2}-x_{j2}|^2+...+w_m|x_{ip}-x_{jp}|^2}$$\n<br>\nThe above mentioned distance measures are crucial to the algorithms like K-Nearest Neighbors, Clustering algorithms, etc. and are very widely used in machine learning and data mining.","12c5e199":"<a id=\"boxplot\"><\/a>\n#### 3.7.4 Box and whiskers plots\nBox and whiskers plots are very popular plots mainly used for **plotting data groups using quartiles**. Box plots are used for getting a sense of **data distribution and spread**, to know possible outliers and to plot the **five number sumary (min, Q1, median, Q3, max)**. **The length of box denotes IQR**.","93de23bd":"<a id=\"ordinal\"><\/a>\n### 1.4. Ordinal Attributes\nOrdinal attributes are a type of nominal attributes with an order among the data values. For e.g students are often graded as A, B, C which have inherent order among the data values. \n<br><br>\nOrdinal attributes, though are nominal, a number of mathemetical operations are possible on them like median, mode however mean is not defined. Ordinal attributes can also be obtained by discretization of numeric data.","f2f8e96d":"<a id=\"scatter\"><\/a>\n#### 3.7.3 Scatter plots\nScatter plots are most commonly used plots for **bivariate analysis**. Two attributes are simply plotted on a 2-D plot against each other where each data point is plotted as a seperate point.\n<br><br>\nScatter plots are often helpful in recognizing clusters and outliers and can provide a sense of how and where data lies. A point that is far away from most of the other points is possibly an outlier .Scatter plots are also used for correlation analysis. \n![scatter.png](attachment:scatter.png)\n<br>\nPlot (a) shows positve correlation while plot (b) shows negative correlation between two plotted attributes. Following image shows cases of no correlation.\n![scatter2.png](attachment:scatter2.png)"}}