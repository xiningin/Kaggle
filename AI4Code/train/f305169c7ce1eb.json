{"cell_type":{"c4814711":"code","729b5db5":"code","772d050b":"code","16fd5289":"code","cc681551":"code","e4055b31":"code","21462a92":"code","19635e87":"markdown","13cdfc1d":"markdown"},"source":{"c4814711":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport logging\nplt.style.use('fivethirtyeight')\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM, GRU\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.datasets import imdb","729b5db5":"print(\"reading data..\")\ndata = pd.read_csv('\/kaggle\/input\/imdb-review-dataset\/imdb_master.csv',encoding='ISO-8859-1')","772d050b":"class IMDBSentiMentAnalysis:\n    \n    def __init__(self, data, maxlen=100, num_words=10000):\n        \n        self.model = None\n        self.history = None\n        self.data = data\n        self.maxlen = maxlen\n        self.num_words = num_words\n        \n    def process_data(self):\n        print(\"processing data...\")\n        data = self.data[self.data.label!='unsup']\n        sns.countplot(x='label',data=data)\n        data['out'] = data['label']\n        \n        data['out'][data.out=='neg']=0\n        data['out'][data.out=='pos']=1\n        # Another way data['out'] = data['out'].map({1:'pos',0:'neg'})\n        np.unique(data.out)\n        #data['out'] = data['label'].map({1:'pos',0:'neg'})\n        \n        req_data = data[['review','out']]\n\n        self.texts = np.array(req_data.review)\n        self.labels = np.array(req_data.out)\n        self.convert_data_to_padded_sequence()\n            \n            \n    def convert_data_to_padded_sequence(self):\n        print(\"Converting data to Sequences\")\n        # num_words: Top No. of words to be tokenized. Rest will be marked as unknown or ignored.\n        tokenizer = Tokenizer(num_words=self.num_words) \n        \n        # tokenizing based on \"texts\". This step generates the word_index and map each word to an integer other than 0.\n        tokenizer.fit_on_texts(self.texts)\n        \n        # generating sequence based on tokenizer's word_index. Each sentence will now be represented by combination of numericals\n        # Example: \"Good movie\" may be represented by [22, 37]\n        seq = tokenizer.texts_to_sequences(self.texts)\n        \n        self.word_index = tokenizer.word_index\n        # padding each numerical representation of sentence to have fixed length.\n\n        self.padded_seq = np.array(pad_sequences(seq,maxlen=self.maxlen))\n        print(\"Data converted to Sequences...\")\n        \n    \n    \n    def plot_model_output(self):\n        history = self.history\n        epochs = self.epochs\n        plt.figure()\n        plt.plot(range(epochs,),history.history['loss'],label = 'training_loss')\n        plt.plot(range(epochs,),history.history['val_loss'],label = 'validation_loss')\n        plt.legend()\n        plt.figure()\n        plt.plot(range(epochs,),history.history['acc'],label = 'training_accuracy')\n        plt.plot(range(epochs,),history.history['val_acc'],label = 'validation_accuracy')\n        plt.legend()\n        plt.show()\n\n    def init_model(self, model = None, gru=False):\n        \n        if model is None:\n            print(\"Initialising default model\")\n            model = Sequential()\n            embedding = Embedding(self.num_words, 32, input_length = self.maxlen, name='embedding')\n            model.add(embedding)\n            if gru:\n                model.add(GRU(32))\n            else:\n                model.add(LSTM(32))\n            model.add(Flatten())\n            model.add(Dense(1,activation='sigmoid'))\n            self.model = model\n        else:\n            print(\"Initialising model passed\")\n            self.model = model\n\n        return self.model.summary()\n\n    def run_the_model(self,optimizer = 'rmsprop', epochs = 10, validation_split=0.2):\n        \n        self.model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['acc'])\n        self.epochs = epochs\n\n        self.history = self.model.fit(self.padded_seq,np.asarray(self.labels).astype(np.uint8),epochs=epochs,validation_split=validation_split)\n        \n        self.plot_model_output()\n    ","16fd5289":"print(\"Initialising IMDB object\")\n\nimdb_deep_learning = IMDBSentiMentAnalysis(data,)\n\nimdb_deep_learning.process_data()","cc681551":"imdb_deep_learning.init_model(gru=True)","e4055b31":"imdb_deep_learning.run_the_model(epochs = 10)","21462a92":"imdb_deep_learning.init_model(gru=False)\nimdb_deep_learning.run_the_model(epochs = 10)","19635e87":"# USING LSTM","13cdfc1d":"# Using GRU"}}