{"cell_type":{"1c08b716":"code","168c16fe":"code","49950f83":"code","a32f8062":"code","70cd6a3f":"code","41334bea":"code","5a720fc8":"code","caf055b2":"code","123954df":"markdown","5915cd87":"markdown","bd1949c5":"markdown","2b3d1e7d":"markdown","85ec7814":"markdown","d35e3e38":"markdown"},"source":{"1c08b716":"!pip install blimpy\n# Note this full resolution file is over 14GB in size. Moving this to colab it downloads in <5 minutes\n# !wget \thttp:\/\/blpd7.ssl.berkeley.edu\/dl2\/GBT_57532_09539_HIP56445_fine.h5\n# Time resolution file\n# !wget \thttp:\/\/blpd13.ssl.berkeley.edu\/dl\/GBT_57532_09539_HIP56445_time.h5\n# This is the mid resolution notebook.\n!wget http:\/\/blpd7.ssl.berkeley.edu\/dl2\/GBT_57532_09539_HIP56445_mid.h5","168c16fe":"import numpy as np\nfrom scipy import stats, interpolate\nfrom blimpy import Waterfall\nfrom matplotlib import pyplot as plt\nfrom bisect import bisect_left\nfrom tqdm import tqdm\nimport dask.array as da\nimport h5py as h5\nimport os\nimport pandas as pd\nimport pylab as plt\nfrom copy import deepcopy","49950f83":"%matplotlib inline\nobs = Waterfall(\".\/GBT_57532_09539_HIP56445_mid.h5\", load_data=False)\nobs.info()\n# We are taking a small selection of the spectrum between \n# 1023.92mHz - 1028.92mHz. \n# obs = Waterfall(\"\/content\/GBT_57532_09539_HIP56445_fine.h5\",  f_stop=1023.92578125+20, max_load=5)\nobs = Waterfall(\".\/GBT_57532_09539_HIP56445_mid.h5\", max_load=5)\ndata = obs.data\n# Channel width of 1033216 np index. \nchannel_len  = np.int(np.round(187.5\/64\/abs(obs.header['foff'])))\nprint(data.shape)\nprint(\"Channel Width: \"+str(channel_len))\nobs.plot_spectrum()","a32f8062":"%matplotlib inline\n# Set up arrays to hold the data manipulations \naverage_power = np.zeros((data.shape[2]))\nshifted_power = np.zeros((int(data.shape[2]\/8)))\nx=[]\nspl_order = 2\nprint(\"Fitting Spline\")\ndata_adjust = np.zeros(data.shape)\naverage_power = data.mean(axis=0)\ncoarse_channel_width = channel_len\nfor i in tqdm(range(0, data.shape[2],coarse_channel_width)):\n  # For each coarse channel we preform a spline fit and subtract the fit from the data.\n    average_channel = average_power[0,i:i+coarse_channel_width]\n    x = np.arange(0,coarse_channel_width,1)\n    knots = np.arange(0, coarse_channel_width, coarse_channel_width\/\/spl_order+1)\n    tck = interpolate.splrep(x, average_channel, s=knots[1:])\n    xnew = np.arange(0, coarse_channel_width,1)\n    ynew = interpolate.splev(xnew, tck, der=0)\n    data_adjust[:,0,i:i+coarse_channel_width]  = data[:,0,i:i+coarse_channel_width] - ynew\n\n# Plot the newly adjusted average power across time per frequency. \nplt.figure()\nplt.plot( data_adjust.mean(axis=0)[0,:])\nplt.title('Spline Fit - adjusted')\nplt.xlabel(\"Fchans\")\nplt.ylabel(\"Power\")","70cd6a3f":"# Computes normal distribution of the signal.\n# Noise is assumed to be Gaussian  \ndef norm_test(arr):\n  # Uses scipy lib to compute the P-values and Z scores\n    return stats.normaltest(arr.flatten())\ndef top(arr, top = 10):\n  newarr=deepcopy(arr)\n  candidate = []\n  for i in range(top):\n    # We add 1 as the 0th index = period of 1 not 0\n    index = np.argmax(newarr)\n    candidate.append(index)\n    newarr[index]=0\n  return candidate","41334bea":"res = []\nslice_length = 32\n# slice_length = int(1033216\/2**4)\nnum_chan = int(data.shape[2]\/channel_len)\ndata_temp=data_adjust[:,0,:]\nfor chan in tqdm(range(num_chan)):\n  res.append([])\n  window = data_temp[:, channel_len*(chan):channel_len*(chan+1)]\n  for i in range(0, channel_len-slice_length, 1):\n    test_data = window[:, i:i+slice_length]\n    s, p = norm_test(test_data)\n    # We filter out p_values by a threshold to reject or accept the\n    # null hypothesis \n    if p < 1e-25:\n      res[chan].append((i, s, p))\n    else:\n      res[chan].append((i, 0, p))\nchan_hits = res","5a720fc8":"result = np.zeros(((len(res))*(len(res[1])), 3))\nfor i in range(len(res)):\n  for k in range(len(res[0])):\n    result[i*(channel_len- slice_length)+k,0]=i*(channel_len- slice_length)+k\n    result[i*(channel_len- slice_length)+k,1]=res[i][k][1]\n    result[i*(channel_len- slice_length)+k,2]=res[i][k][2]","caf055b2":"# slice just the z_scores of the signals\ns_val = result[:,1]\ncandidates = []\ncandidates = top(s_val, top=2)\n\nfor i in range(len(candidates)):\n  max_index = candidates[i]\n  print(\"Fchan (MHz): \"+str(max_index*(-obs.header['foff'])+(obs.header['fch1']+obs.header['foff']*data.shape[2]))+\" S_VAL: \"+str(result[max_index,1]))\n  f_low = max_index - int(slice_length\/2)\n  f_high = max_index + int(slice_length\/2)\n  fig = plt.figure(figsize=(15, 6))\n  plt.imshow(data_adjust[:250,0,f_low:f_high], aspect='auto')\n  plt.colorbar()","123954df":"# Compute Statistical Likelihood of Signal\n\nWhen given an observation we want to compute the statistical likelihood of the signal occuring due to noise given its intensity. Since the noise picked up during observation is said to be Gaussian, all we need to do is compute the normal distribution of a window of the spectrogram and then compute the Z-scores of the signals. The greater the signal is away from the central mean the more statistically \"anomalous\" it is in terms of its energy.\n\n<p align=\"center\" height=\"200px\"> \n    <img src=\"https:\/\/desktop.arcgis.com\/en\/arcmap\/10.3\/tools\/spatial-statistics-toolbox\/GUID-CBF63B74-D1B2-44FC-A316-7AC2B1C1D464-web.png\">\n<\/p>\n\n\nGiven the Z-score calculations we also want to compute the p-values of each window slice. The p-value threshold `p < 1e-25` is used to help reject a null-hypothesis where a statistical anomaly would likely appear given a non-anomalous situation. If it is unlikely that during a non-anomalous situation an anomalous event could occur, then we accept the alternative hypothesis. The alternative being that this signal isn't random noise. \n\nIn other words we are asking, how likely is this 'weird' event to occur assuming there it came from noise. P-values help use judge the statistical significance of a given event. ","5915cd87":"# Concluding Result\n\nAfter filtering out statistically insignificant events. We then look for the greatest Z scores given by the `s values` from our calculations. We picked the top 3 to show to illustrate the types of signals it has found. ","bd1949c5":"# Bandpass Removal \n### Preprocessing\nThe goal of this process is to clean the data of its artifacts created by combining multiple bands. Our data is created by taking sliding windows of the raw voltage data and computing an FFT of that sliding window. With these FFTs (each containing frequency information about a timestamp) for each coarse channel, we use a bandpass filter to cut off frequencies that don\u2019t belong to that coarse channel\u2019s frequency range. But we can\u2019t make a perfect cut, and that\u2019s why there's a falling off at the edges.\n\nThey\u2019re called band-pass because they only allow signals in a particular frequency range, called a band, to pass-through. When we assemble the products we see these dips in the spectrogram. In other words - they aren't real signals.\n\nTo remove the bandpass features, we use spline lines to fit each channel to get a model of the bandpass of that channel. By using splines, we can fit the bandpass without fitting the more significant signals.\n\nIf you want more details on this check out [Yuhong's work](https:\/\/github.com\/FX196\/SETI-Energy-Detection) for a detailed explanation. \n","2b3d1e7d":"# Breakthrough Listen Data and How To Get It \n\nBreakthrough Listen data is typically stored in 3 types of radio spectrogram resolutions used for different types of searches. \n\n\n1. `Fine Resolution` - Sacrifices time resolution for high-frequency resolution\n\n2. `Time Resolution` - Sacrifices high-frequency resolution for high time resolution\n\n3. `Mid Resolution` - inbetween case with an 'inbetween' time, and frequency resolution.\n\n\n\nFor a formal documentation on the data products [check out the paper](https:\/\/arxiv.org\/pdf\/1906.07391.pdf). To get the open sourced data, check out the archive [data](https:\/\/arxiv.org\/pdf\/1906.07391.pdf)\n\n**IMPORTANT**: Ideally we want to work with `Fine Resolution` data as we are searching for narrowband signals that would require a high frequency resolution that may be within a few `KHz` in width. Furthermore, since we care about the energy of the signal, more so then the actual modulation of the signal, we can afford to sacrifice the time resolution of the data. *However we will be working with a* `mid resolution` *product as an example. This makes this notebook run fast enough for most computers and COLAB.*\n\n# Blimpy I\/O Library \n\nIn order to interface with this data we have a python library `BLIMPY` where it makes it easier to work with +10GB of data.\n\n**NOTE**: We will be working with very large datasets that may be resource-intensive. Advise you to port this notebook to a COLAB enviroment.\n\n\n","85ec7814":"# Loading Data\n\nHere we load the data using `BLIMPY` object. Note, we are using the `mid` resolution data for some target `HIP56445`\n\n`obs.info()` gives us important header information . The header information gives vital information about the observational setup of the telescope. For example, the coarse channel width or the observation time and duration, etc.\n\nWe also want to compute the coarse channel width. Which is shown in `channel_len`","d35e3e38":"# SETI Energy Detection \n\n#### Original Code By Yuhong | [Original Code](https:\/\/github.com\/FX196\/SETI-Energy-Detection) \n#### Notebook By Peter Ma | [Contact](https:\/\/PETERMA.CA) \n\nWhen conducting an ETI search we often look for certain hallmarks of an engineered signal. Of the many possible ETI signal characteristics, an energized narrowband signal is an important indicator. Loosely speaking, this is based on the assumption that if an ETI had the intention to be found, they'd likely produce a 'loud signal' (high intensity and a high SNR) in order to 'get our attention'. \n\nIn this notebook we will show a statistical method in detecting a highly energized signal.\n"}}