{"cell_type":{"2fb8dfb7":"code","f899a8aa":"code","b28ac39b":"code","be5f4240":"code","33771823":"code","539fcd77":"code","0b33af92":"code","aef4193e":"code","15dd19c5":"code","53372fcc":"code","3c18bf4a":"code","af7a3490":"code","875a4399":"code","0699080c":"code","1991deb9":"code","726f1c71":"code","79ade8d9":"code","912e7e19":"code","435085c7":"code","0a5fa9d6":"code","72caca37":"code","2bd97420":"code","b1d15298":"code","6737a4b0":"code","e2ae6f61":"code","ba2e911d":"code","8c6f312e":"code","526b08f4":"markdown","95a88d01":"markdown","41045412":"markdown","aba000ea":"markdown","ca3a8755":"markdown","93c0fa6a":"markdown","832642d0":"markdown","a7b9e685":"markdown","91854518":"markdown","f370127e":"markdown","fbee70db":"markdown","d4e1db7e":"markdown","154dab5e":"markdown","1fb22b38":"markdown","e1d067c3":"markdown","dea71f26":"markdown","6f476379":"markdown","d89dba59":"markdown","46082982":"markdown","dec323b3":"markdown","f3e04ee7":"markdown","6e58baf1":"markdown","93c9cc2f":"markdown"},"source":{"2fb8dfb7":"import shap\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom tqdm import tqdm\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nshap.initjs()","f899a8aa":"RANDOM_STATE = 2021","b28ac39b":"train = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\")","be5f4240":"sns.countplot(x = 'target', data = train)","33771823":"train_C14 = train[(train.target == 'Class_1') | (train.target == 'Class_4')]\ntrain_C23 = train[(train.target == 'Class_2') | (train.target == 'Class_3')]\ntrain_C12 = train[(train.target == 'Class_1') | (train.target == 'Class_2')]\ntrain_C24 = train[(train.target == 'Class_2') | (train.target == 'Class_4')]","539fcd77":"sns.countplot(x = 'target', data = train_C14)","0b33af92":"lencoder = LabelEncoder()\n\ntrain_C14['target'] = lencoder.fit_transform(train_C14['target'])\ntrain_C23['target'] = lencoder.fit_transform(train_C23['target'])\ntrain_C12['target'] = lencoder.fit_transform(train_C12['target'])\ntrain_C24['target'] = lencoder.fit_transform(train_C24['target'])\n","aef4193e":"X_train_C14, X_valid_C14, y_train_C14, y_valid_C14 = train_test_split(train_C14.drop(['id','target'], axis = 1), \n                                                                      train_C14.target,  \n                                                                      stratify=train_C14.target, \n                                                                      test_size=0.1, \n                                                                      random_state= RANDOM_STATE)\n\nX_train_C23, X_valid_C23, y_train_C23, y_valid_C23 = train_test_split(train_C23.drop(['id','target'], axis = 1), \n                                                                      train_C23.target,  \n                                                                      stratify=train_C23.target, \n                                                                      test_size=0.1, \n                                                                      random_state= RANDOM_STATE)\n\nX_train_C12, X_valid_C12, y_train_C12, y_valid_C12 = train_test_split(train_C12.drop(['id','target'], axis = 1), \n                                                                      train_C12.target,  \n                                                                      stratify=train_C12.target, \n                                                                      test_size=0.1, \n                                                                      random_state= RANDOM_STATE)\n\nX_train_C24, X_valid_C24, y_train_C24, y_valid_C24 = train_test_split(train_C24.drop(['id','target'], axis = 1), \n                                                                      train_C24.target,  \n                                                                      stratify=train_C24.target, \n                                                                      test_size=0.1, \n                                                                      random_state= RANDOM_STATE)","15dd19c5":" params = { \n        'objective': 'binary', \n        'boosting_type' : 'gbdt', \n        'metric': 'binary_logloss' \n    } ","53372fcc":"def train_group(X, y, Xv):    \n    test_preds = None\n    train_rmse = 0\n    val_rmse = 0\n    n_splits = 5\n    \n    model =  LGBMClassifier(**params)\n    #model = CatBoostClassifier() # there is no difference - you can try it \n    \n    skf = StratifiedKFold(n_splits = n_splits, shuffle = True,  random_state = 0)\n    \n    for tr_index , val_index in tqdm(skf.split(X.values , y.values), total=skf.get_n_splits(), desc=\"k-fold\"):\n\n        x_train_o, x_val_o = X.iloc[tr_index] , X.iloc[val_index]\n        y_train_o, y_val_o = y.iloc[tr_index] , y.iloc[val_index]\n        \n        eval_set = [(x_val_o, y_val_o)]\n        \n        model.fit(x_train_o, y_train_o, eval_set = eval_set, early_stopping_rounds=100, verbose=False)\n\n        train_preds = model.predict(x_train_o)\n        train_rmse += mean_squared_error(y_train_o ,train_preds , squared = False)\n\n        val_preds = model.predict(x_val_o)\n        val_rmse += mean_squared_error(y_val_o , val_preds , squared = False)\n        \n        if test_preds is None:\n            test_preds = model.predict_proba(Xv.values)\n        else:\n            test_preds += model.predict_proba(Xv.values)\n\n    print(f\"\\nAverage Training RMSE : {train_rmse \/ n_splits}\")\n    print(f\"Average Validation RMSE : {val_rmse \/ n_splits}\\n\")\n\n    return model, test_preds","3c18bf4a":"def experiment(exp_title, X_train, y_train, X_valid, y_valid):\n    \n    model, preds = train_group(X_train, y_train, X_valid)\n    \n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_valid)\n    shap.summary_plot(shap_values, X_valid)\n    \n    y_preds = np.argmax(preds, axis=1)\n    print(f'MSE Score: {mean_squared_error(y_valid, y_preds)}\\n')\n    print(classification_report(y_valid, y_preds))\n    \n    sns.heatmap(pd.DataFrame(confusion_matrix(y_valid, y_preds)), annot=True, linewidths=.5, fmt=\"d\")\n    \n    return shap_values, explainer","af7a3490":"shap_values, explainer = experiment('CLASS 1 - 4', X_train_C14, y_train_C14, X_valid_C14, y_valid_C14)","875a4399":"# Let's look from Class_4 perspective (our 1) \nshap.summary_plot(shap_values[1], X_valid_C14)","0699080c":"sns.histplot(x = 'feature_16', data = X_valid_C14, bins=10)","1991deb9":"X_valid_C14.feature_16.value_counts().head(5)","726f1c71":"selected_features = [\"feature_16\", \"feature_25\", \"feature_31\", \"feature_37\"]\n\nplt.figure(figsize=(20,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","79ade8d9":"for name in selected_features:\n    shap.dependence_plot(name, shap_values[1], X_valid_C14)","912e7e19":"shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_valid_C14.iloc[0,:])","435085c7":"shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_valid_C14.iloc[1,:])","0a5fa9d6":"shap.force_plot(explainer.expected_value[0], shap_values[0][:500,:], X_valid_C14.iloc[:500,:])","72caca37":"shap_values, explainer = experiment('CLASS 2 - 3', X_train_C23, y_train_C23, X_valid_C23, y_valid_C23)","2bd97420":"shap.summary_plot(shap_values[0], X_valid_C23)","b1d15298":"shap.dependence_plot(14, shap_values[0], X_valid_C23)","6737a4b0":"selected_features = [\"feature_14\", \"feature_15\", \"feature_6\", \"feature_34\"]\n\nplt.figure(figsize=(20,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","e2ae6f61":"shap_values, explainer = experiment('CLASS 1 - 2', X_train_C12, y_train_C12, X_valid_C12, y_valid_C12)","ba2e911d":"shap_values, explainer = experiment('CLASS 2 - 4', X_train_C24, y_train_C24, X_valid_C24, y_valid_C24)","8c6f312e":"zero_data = ((train.drop('id', axis = 1).iloc[:,:50]==0).sum() \/ len(train) * 100)[::-1]\n_, ax = plt.subplots(1,1,figsize=(10, 20))\n\nax.barh(zero_data.index, 100, color='#dadada', height = 1, edgecolor = '#FFFFFF')\nbarh = ax.barh(zero_data.index, zero_data, height = 1)\nax.bar_label(barh, fmt='%.02f %%')\n  \nplt.show()","526b08f4":"### Let's look then on feature_16 ....","95a88d01":"#### As I wrote in the introduction. Let's simplify the problem. Only 2nd class. Nothing more. Let's build a simple classifier.","41045412":"According to Slundberg (https:\/\/slundberg.github.io\/shap\/notebooks\/plots\/dependence_plot.html)\n\nEach dot is a single prediction (row) from the dataset.\n* The x-axis is the value of the feature (from the X matrix).\n* The y-axis is the SHAP value for that feature, which represents how much knowing that feature's value changes the output of the model for that sample's prediction. \n* The color corresponds to a second feature that may have an interaction effect with the feature we are plotting (by default this second feature is chosen automatically). If an interaction effect is present between this other feature and the feature we are plotting it will show up as a distinct vertical pattern of coloring. ","aba000ea":"<div class=\"alert alert-success\">\n  <strong>Conclutions:<\/strong>\n    <ul>\n        <li>Shap show us that BIG BLOB (0 value) drive our model to Class_4 (1) (but we have some blues on the left side as well this is why not all 1482 zeros were classified to Class_4) .... <\/li>\n        <li>from the very beginning our model tries to do everything possible to assign observations to Class_4.<\/li>\n        <li>BAD performance of model ..... - look on confusion matrix<\/li>\n    <\/ul>\n<\/div>","ca3a8755":"## DEFINE SIMPLE CLASSIFIER AND K-FOLDED TRAINING LOOP ","93c0fa6a":"### I know begging for a vote here is not right. But I really spent a lot of time figuring out this competition. I am sharing my discoveries with you. Please appreciate my work - notebooks and this dataset. Thank you!","832642d0":"## EXPERIMENTS","a7b9e685":"In the plot above, the bold 1.62 is the model\u2019s score for this observation. Higher scores lead the model to predict 1 and lower scores lead the model to predict 0. The features that were important to making the prediction for this observation are shown in red and blue, with red representing features that pushed the model score higher, and blue representing features that pushed the score lower. Features that had more of an impact on the score are located closer to the dividing boundary between red and blue, and the size of that impact is represented by the size of the bar.\n\nSource: https:\/\/medium.com\/mlearning-ai\/shap-force-plots-for-classification-d30be430e195","91854518":"### Let's see one sample from class_1 ...","f370127e":"#### I separate 10% data for model validation ","fbee70db":"### 1. CLASS_1 AND CLASS_4 ACCURACY CLASSIFIER","d4e1db7e":"### Conclutions:\n- \n\n<div class=\"alert alert-success\">\n  <strong>Conclutions:<\/strong>\n    <ul>\n        <li>BAD ..... - Class_2 = 5750, Class_3 = 2142 - we have balanced dataset .... but model perform bad - it sees corectly - look on confusion matrix <\/li>\n    <\/ul>\n<\/div>\n","154dab5e":"# Let's look finaly on data ....... zeroes in dataset ....","1fb22b38":"<div class=\"alert alert-danger\">\n  <strong>You can make interactive analysis with this chart - please choose options on TOP and on the LEFT to see feature interaction .... <\/strong> \n<\/div>","e1d067c3":"<div class=\"alert alert-success\">\n  <strong>Conclutions:<\/strong>\n    <ul>\n        <li>We can experiment such way assuming that data is symthetic (unfortunately when we divide training data such way we lose some information about class coreleations but ... still this is only experiment).<\/li>\n        <li>As we can see class imbalance is not problem.<\/li>\n        <li>My hypotesis is that dealing with data sparsity is the biggest challange here ....  <\/li>\n    <\/ul>\n<\/div>\n\n### and ..... what do you think? .... it is possible to build good classifier (without overfitting) based on this data? :)))))))))))\n\n### in my opinion ....  hmmmm  ... \n","dea71f26":"### 2. CLASS_2 AND CLASS_3 ACCURACY CLASSIFIER","6f476379":"## Let's divide training dataset on groups \n\n- Group 1 - Class_1 and Class_4 - to avoid class imbalance \n- Group 2 - Class_2 and Class_3 - to avoid class imbalance\n- Group 3 - to check if we find something interesting\n- Group 4 - to check if we find something interesting","d89dba59":"## EXPERIMENT PREPARATION","46082982":"### ... and one sample from class_4","dec323b3":"# Hacking TPS-05 data and models .... next steps ...\n\n\n## Since we have syntetic data .... I decided to make some crazy experiment ... \n\n\n#### I asked myself. Is it possible to build a good model on the TPS-05 data at all? I decided to simplify the issue to exclude the influence of imbalanced data between classes ans see how out-of-the LightGBM ... box works on TPS-05 data. This is how a new idea was born.\n\n#### What if I only did a classification of two classes, eg Class_1 and Class_4. Both have the same amount of data, so the problem of the lack of balance is eliminated. In addition, the classifier should separate both classes well - it's a only 2 classes instead of 4, there is no problem of unbalance etc.\n\n#### I decided to conduct set of experiments. See what came out. \n\n<div class=\"alert alert-success\">\n  <strong>In this series my TPS-05 notebooks:<\/strong>\n    <ul>\n        <li><a href = \"https:\/\/www.kaggle.com\/remekkinas\/shap-lgbm-looking-for-best-features\">SHAP + LGBM - looking for best features<\/a><\/li>\n        <li><a href = \"https:\/\/www.kaggle.com\/remekkinas\/tps-5-weighted-training-xgb-rf-lr-smote\">Weighted training - XGB, RF, LR, ... SMOTE<\/a><\/li>\n    <\/ul>\n<\/div>\n\nThey are not so popular ... so either I'm right or ... I'm not;) But if I am wrong please comment this ... I would love to learn new things. My motivation is to build good model for TPS-05.\n\n<div class=\"progress\">\n  <div class=\"progress-bar progress-bar-warning\" role=\"progressbar\" aria-valuenow=\"70\"\n  aria-valuemin=\"0\" aria-valuemax=\"100\" style=\"width:100%\">\n      <strong>100% Complete<\/strong>\n  <\/div>\n<\/div>","f3e04ee7":"### 3. CLASS_1 AND CLASS_2 ACCURACY CLASSIFIER","6e58baf1":"### Let's look on the rest TOP features ....","93c9cc2f":"### 4. CLASS_2 AND CLASS_4 ACCURACY CLASSIFIER"}}