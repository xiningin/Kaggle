{"cell_type":{"5cfea874":"code","dba2cd10":"code","da82a626":"code","04bbe74c":"code","34c054f6":"code","201b2908":"code","e463b78a":"code","59bc16a2":"code","974f3037":"code","300c6ac0":"code","2784a722":"code","805a2da3":"code","8a3bd3e0":"code","07ba636f":"code","cdc6cbf6":"code","6cb7a49e":"code","378e5e1d":"code","b40f5000":"code","32500886":"code","716adc41":"code","c8b5dc81":"code","c7f5961f":"markdown","9c7c1a24":"markdown","985466de":"markdown","2df49fac":"markdown","4728dd44":"markdown","3aa4d2d7":"markdown","e7b2df41":"markdown"},"source":{"5cfea874":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dba2cd10":"import os , glob \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport skimage\nfrom skimage.io import imread , imread_collection \n\nfrom sklearn.metrics import accuracy_score , classification_report\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\n\nfrom sklearn.utils import shuffle\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Sequential\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.image import ImageDataGenerator\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import resample","da82a626":"base_path = \"..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/\"\nfolder = os.listdir(base_path)\nprint(\"No. of Patients:\",len(folder))\nimage_pathes ='..\/input\/breast-histopathology-images\/*\/'\n\npatient_list = list(glob.glob(image_pathes))\n\ntotal_images = 0\nfor n in range(len(folder)):\n    patient_id = folder[n]\n    for c in [0, 1]:\n        patient_path = base_path + patient_id\n        class_path = patient_path + '\/' + str(c) + '\/'\n        subfiles = os.listdir(class_path)\n        total_images += len(subfiles)\n        \nprint(\"Total Images in dataset: \", total_images )\n\ntest=[glob.glob(patient_list[i]+'\/*\/*') for i in range(0,len(patient_list))]\n","04bbe74c":"# Organizing the data into pandas data frame\ndata = pd.DataFrame(index=np.arange(0, total_images), columns=[\"patient_id\", \"path\", \"target\"])\n\nk = 0\nfor n in range(len(folder)):\n    patient_id = folder[n]\n    patient_path = base_path + patient_id \n    for c in [0,1]:\n        class_path = patient_path + \"\/\" + str(c) + \"\/\"\n        subfiles = os.listdir(class_path)\n        for m in range(len(subfiles)):\n            image_path = subfiles[m]\n            data.iloc[k][\"path\"] = class_path + image_path\n            data.iloc[k][\"target\"] = c\n            data.iloc[k][\"patient_id\"] = patient_id\n            k += 1  \n\ndata.head()","34c054f6":"cancer_perc = data.groupby(\"patient_id\").target.value_counts() \/ data.groupby(\"patient_id\").target.size()\ncanxer_perc = cancer_perc.unstack()\n\nfig, ax = plt.subplots(1, 3,figsize = (17,8))\nsns.histplot(data.groupby('patient_id').size(), ax=ax[0], color='Orange', kde=False, bins=30)\nax[0].set_xlabel('Number of patches')\nax[0].set_ylabel('Frequency')\nax[0].set_title('No. of Patches per patient?')\nsns.histplot(cancer_perc.loc[:, 1]*100, ax=ax[1], color=\"Tomato\", kde=False, bins=30)\nax[1].set_title(\"Percentage of an image covered by IDC?\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_xlabel(\"% of patches with IDC\");\nsns.countplot(x = data.target, palette=\"Set2\", ax=ax[2]);\nax[2].set_xlabel(\"no(0) versus yes(1)\")\nax[2].set_title(\"No. of Patches showing IDC?\");","201b2908":"## Visualizing sample from our dataset\nimg_sample = imread(test[0][0])\nplt.figure(figsize = (17,8))\nplt.title('patienID:'\n          +str(test[0][0].split('\/')[3])+'\\n'\n         'class:'+str(test[0][0].split('\/')[4]))\nplt.imshow(img_sample)\nplt.show()","e463b78a":"print(\"Displaying Cancer Tissue Samples\")\ncancer_selection = np.random.choice(data[data.target == 1].index.values, size=50, replace=False)\n\nfig, ax = plt.subplots(5, 10, figsize=(20, 10))\n\nfor n in range(5):\n    for m in range(10):\n        idx = cancer_selection[m + 10*n]\n        image = imread(data.loc[idx, \"path\"])\n        ax[n,m].imshow(image)\n        ax[n,m].grid(False)","59bc16a2":"print(\"Displaying Non- Cancer Tissue Samples\")\nnon_cancer_selection = np.random.choice(data[data.target == 0].index.values, size=50, replace=False)\n\nfig, ax = plt.subplots(5, 10, figsize=(20, 10))\n\nfor n in range(5):\n    for m in range(10):\n        idx = non_cancer_selection[m + 10*n]\n        image = imread(data.loc[idx, \"path\"])\n        ax[n,m].imshow(image)\n        ax[n,m].grid(False)","974f3037":"def plot_model_history(model_history):\n    fig, axs = plt.subplots(1,2,figsize=(15,5))\n    axs[0].plot(range(1,len(model_history.history['accuracy'])+1),model_history.history['accuracy'])\n    axs[0].plot(range(1,len(model_history.history['val_accuracy'])+1),model_history.history['val_accuracy'])\n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy')\n    axs[0].set_xlabel('Epoch')\n    axs[0].legend(['train', 'val'], loc='best')\n    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss')\n    axs[1].set_xlabel('Epoch')\n    axs[1].legend(['train', 'val'], loc='best')\n    plt.show()","300c6ac0":"from sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom sklearn.utils import shuffle\nfrom keras.utils import to_categorical\n\nimport cv2\n# Down-sample Majority Class\ndata_samples = data[:153000]\ntrain_val, test = train_test_split(data_samples, test_size=0.2)\n\nclass0 = train_val[train_val[\"target\"]==0]\nclass1 = train_val[train_val[\"target\"]==1]\ndf_majority_upsampled = resample(class1, replace=False, n_samples=class1.shape[0],random_state=123)\ndf_minority_upsampled = resample(class0, replace=False, n_samples=class1.shape[0],random_state=123)\n\ndf_downsampled  = pd.concat([df_minority_upsampled, df_majority_upsampled])\ntest = test[:int(df_downsampled.shape[0]\/4)]\nData,data_output= shuffle(df_downsampled[\"path\"],df_downsampled[\"target\"])\nimages=list()\nfor img in Data:\n    images.append(cv2.resize(cv2.imread(img),(50,50),interpolation=cv2.INTER_CUBIC))\n","2784a722":"from keras.utils import to_categorical\n\ndata_output_encoded =to_categorical(data_output, num_classes=2)\n\nData_test,data_output_test= shuffle(test[\"path\"],test[\"target\"])\nimages_test=list()\nfor img in Data_test:\n    images_test.append(cv2.resize(cv2.imread(img),(50,50),interpolation=cv2.INTER_CUBIC))\ndata_output_encoded_test =to_categorical(data_output_test, num_classes=2)\n\nX_train, X_test, Y_train, Y_test = np.array(images), np.array(images_test), data_output_encoded, data_output_encoded_test\nprint(\"Size of train\",X_train.shape[0])\nprint(\"Size of test\",X_test.shape[0])\nprint(\"Size of train target\",len(Y_train))\nprint(\"Size of test target\",len(Y_test))","805a2da3":"X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.25)\nprint(\"Training Data Shape:\", X_train.shape)\nprint(\"Validation Data Shape:\", X_valid.shape)\nprint(\"Testing Data Shape:\", X_test.shape)\nprint(\"Training Label Data Shape:\", Y_train.shape)\nprint(\"Validation Label Data Shape:\", Y_valid.shape)\nprint(\"Testing Label Data Shape:\", Y_test.shape)","8a3bd3e0":"data_generator = ImageDataGenerator(rotation_range=40,width_shift_range=0.2,\n                             height_shift_range=0.2, zoom_range=0.2,\n                             rescale=1\/255.0)\nX_valid_e=X_valid\/255.0\nX_test_e=X_test\/255.0\nX_train_e=X_train\/255.0\nmodel = Sequential()\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),strides=2,padding='same',activation='relu',input_shape=X_train.shape[1:]))\nmodel.add(Dropout(0.1))\nmodel.add(MaxPooling2D(pool_size=2,strides=2))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(filters=128,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(filters=512,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()","07ba636f":"model.compile(loss='categorical_crossentropy', optimizer='AdaDelta', metrics=['accuracy'])\nmodel_info = model.fit_generator(data_generator.flow(X_train, Y_train, 64), \n          validation_data=(X_valid_e, Y_valid), steps_per_epoch=500,\n          epochs=30, verbose=1)","cdc6cbf6":"plot_model_history(model_info)","6cb7a49e":"predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in X_test_e]\ntest_acc = accuracy_score(np.argmax(Y_test, axis=1),np.array(predictions))\npredictions_train = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in X_train_e]\ntrain_acc = accuracy_score(np.argmax(Y_train, axis=1),np.array(predictions_train))\npredictions_val = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in X_valid_e]\nval_acc = accuracy_score(np.argmax(Y_valid, axis=1),np.array(predictions_val))\ncnf_matrix=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions))","378e5e1d":"dict_characters = {0: 'IDC:Benign(-)', 1: 'IDC:Malignant(+)'}\ndf_cm = pd.DataFrame(cnf_matrix, index = dict_characters.values(), columns = dict_characters.values())\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')\nplt.title(\"Test Confusion Matrix\")\nplt.show()\n\nresults=pd.DataFrame(columns=['Set', 'Accuracy'])\nresults.loc[0]=['Train',train_acc]\nresults.loc[1]=['Validation',val_acc]\nresults.loc[2]=['Test',test_acc]\ndisplay(results)","b40f5000":"from keras.applications.vgg16 import VGG16, preprocess_input\nvgg_model = VGG16(include_top=False,weights='imagenet', input_tensor=None, input_shape=None, pooling=None,)\nvgg_train=vgg_model.predict(preprocess_input(X_train),batch_size=50,verbose=1)\nvgg_valid=vgg_model.predict(preprocess_input(X_valid),batch_size=50,verbose=1)\nvgg_test=vgg_model.predict(preprocess_input(X_test),batch_size=50,verbose=1)\n\nmodel_transfer = Sequential()\nmodel_transfer.add(GlobalAveragePooling2D(input_shape=vgg_train.shape[1:]))\nmodel_transfer.add(Dense(32,activation='relu'))\nmodel_transfer.add(Dropout(0.25))\nmodel_transfer.add(Dense(64,activation='relu'))\nmodel_transfer.add(Dropout(0.3))\nmodel_transfer.add(Dense(128,activation='relu'))\nmodel_transfer.add(Dropout(0.35))\nmodel_transfer.add(Dense(256,activation='relu'))\nmodel_transfer.add(Dropout(0.45))\nmodel_transfer.add(Dense(512,activation='relu'))\nmodel_transfer.add(Dropout(0.55))\nmodel_transfer.add(Dense(2, activation='softmax'))\nmodel_transfer.summary()\n\n\nmodel_transfer.compile(loss='categorical_crossentropy', optimizer='AdaDelta', metrics=['accuracy'])\nmodel_info = model_transfer.fit(vgg_train, Y_train, 64, validation_data=(vgg_valid, Y_valid), epochs=30, verbose=1)","32500886":"plot_model_history(model_info)","716adc41":"predictions_transfer = [np.argmax(model_transfer.predict(np.expand_dims(feature, axis=0))) for feature in vgg_test]\ntransfer_test_acc = accuracy_score(np.argmax(Y_test, axis=1),np.array(predictions_transfer))\npredictions_transfer_train = [np.argmax(model_transfer.predict(np.expand_dims(feature, axis=0))) for feature in vgg_train]\ntransfer_train_acc = accuracy_score(np.argmax(Y_train, axis=1),np.array(predictions_transfer_train))\npredictions_transfer_val = [np.argmax(model_transfer.predict(np.expand_dims(feature, axis=0))) for feature in vgg_valid]\ntransfer_val_acc = accuracy_score(np.argmax(Y_valid, axis=1),np.array(predictions_transfer_val))\n\ncnf_matrix_transfer=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_transfer))\ndf_cm = pd.DataFrame(cnf_matrix_transfer, index = dict_characters.values(), columns = dict_characters.values())\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')\nplt.title(\"Test Confusion Matrix\")\nplt.show()\n\nresults=pd.DataFrame(columns=['Set', 'Accuracy'])\nresults.loc[0]=['Train',transfer_train_acc]\nresults.loc[1]=['Validation',transfer_val_acc]\nresults.loc[2]=['Test',transfer_test_acc]\ndisplay(results)","c8b5dc81":"tp, tn = 0, 0\nfor i in range(0,len(Y_test)):\n    if(np.argmax(Y_test[i])==1):\n        tp+=1\nconfusion_Arg_s=cnf_matrix[1][1]\/tp *100\nconfusion_transfer_s=cnf_matrix_transfer[1][1]\/tp*100\nfor i in range(0,len(Y_test)):\n    if(np.argmax(Y_test[i])==1): \n        tp+=1\nfor i in range(0,len(Y_test)):\n    if(np.argmax(Y_test[i])==0):\n        tn+=1\n\nresults=pd.DataFrame(columns=['Models', 'Senstivity', 'Specificity', 'Accuracy'])\nresults.loc[0]=['model',confusion_Arg_s,cnf_matrix[0][0]\/tn *100,test_acc]\nresults.loc[1]=['transfer model',confusion_transfer_s,cnf_matrix_transfer[0][0]\/tn*100,transfer_test_acc]\ndisplay(results)\n","c7f5961f":"- Cancer Tissur appears to be more viloet.\n\n- But some non-caner tissue is also violet.","9c7c1a24":"###### Load the dataset","985466de":"## Using VGG","2df49fac":"- The number of image patches per patient varie's a lot.\n\n- Some patients have more than 80 % patches that show IDC! Consequently the tissue is full of cancer or only a part of the breast was covered by the tissue slice that is focused on the IDC cancer.\n\n- The classes of IDC versus no IDC are imbalanced.","4728dd44":"### Exploring the data","3aa4d2d7":"# Breast Histopathology Images Classification\n\n198,738 IDC(-) image patches; 78,786 IDC(+) image patches\n\nDuctal carcinoma is a common type of breast cancer that starts in cells that line the milk ducts, which carry breast milk to the nipple. There are two types: Invasive ductal carcinoma (IDC) Ductal carcinoma in situ (DCIS), also called intraductal carcinoma.\n\n\n### Context:\n\nInvasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample, pathologists typically focus on the regions which contain the IDC. As a result, one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide.\n\n## About the dataset:\n\nThe original dataset consisted of 162 whole mount slide images of Breast Cancer (BCa) specimens scanned at 40x. From that, 277,524 patches of size 50 x 50 were extracted (198,738 IDC negative and 78,786 IDC positive). Each patch\u2019s file name is of the format: u_xX_yY_classC.png \u2014 > example 10253_idx5_x1351_y1101_class0.png . Where u is the patient ID (10253_idx5), X is the x-coordinate of where this patch was cropped from, Y is the y-coordinate of where this patch was cropped from, and C indicates the class where 0 is non-IDC and 1 is IDC.\n\n## Inspiration:\n\nBreast cancer is the most common form of cancer in women, and invasive ductal carcinoma (IDC) is the most common form of breast cancer. Accurately identifying and categorizing breast cancer subtypes is an important clinical task, and automated methods can be used to save time and reduce error.\n\nAdrian Rosebrock of PyImageSearch has this wonderful tutorial on this same topic as well. Be sure to check that out if you have not. I decided to use the fastai library and to see if I could improve the predictive performance by incorporating modern deep learning practices.\n\nLet's take a look at the class distribution of the dataset again - >\n\n198,738 negative examples (i.e., no breast cancer)\n78,786 positive examples (i.e., indicating breast cancer was found in the patch)\n0 indicates no IDC (no breast cancer) while 1 indicates IDC (breast cancer)\n\nAs we can see, this is a clear example of class-imbalance. But we will start simple and do a lot of experimentation for taking major decisions for the model training and tricking.\n","e7b2df41":"#### Import the required libraries"}}