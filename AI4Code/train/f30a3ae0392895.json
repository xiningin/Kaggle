{"cell_type":{"d598762b":"code","09e2f7a8":"code","edcf5ff5":"code","478e2285":"code","2223a766":"code","37554640":"code","de3a4957":"code","ac40f6bd":"code","6e0f9d4c":"code","6ea097d0":"code","889e3e4e":"code","d1a72e7c":"code","9cbbc90f":"code","1011a847":"code","151de245":"code","c108b268":"code","d8c27d99":"code","c400a182":"code","f2f49f39":"code","117384a2":"code","80eedf75":"code","391d45a8":"markdown","b449d8e2":"markdown","245c3025":"markdown","a016164a":"markdown","470111f0":"markdown"},"source":{"d598762b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09e2f7a8":"!pip install \/kaggle\/input\/whl-datasets\/iterative_stratification-0.1.6-py3-none-any.whl\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport seaborn as sns\nfrom numpy import mean, std\nimport seaborn as sns\nfrom matplotlib import *\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as mpatches\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","edcf5ff5":"trainFeatures = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntestFeatures = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrainTargetsS = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrainTargetsNS = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n\nfor file in (trainFeatures, testFeatures, trainTargetsS, trainTargetsNS):\n    file.columns = file.columns.str.lower().str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n    \nprint('Train features shape: ', trainFeatures.shape)\nprint('Test features shape: ', testFeatures.shape)\nprint('Train Targets (Scored) shape: ', trainTargetsS.shape)\nprint('Train Targets (Non Scored) shape: ', trainTargetsNS.shape)\n\nprint('------- Train Features view -------')\ntrainFeatures.head()","478e2285":"print('------- Train Targets (Scored) view -------')\ntrainTargetsS.head()","2223a766":"print('------- Train Targets (Non Scored) view -------')\ntrainTargetsNS.head()","37554640":"# The Sample Treatment is heavily skewed:\n\nprint('Compound Treatment', round(trainFeatures['cp_type'].value_counts()[0]\/len(trainFeatures) * 100, 2), '% of the dataset')\nprint('Control Perturbation Treatment', round(trainFeatures['cp_type'].value_counts()[1]\/len(trainFeatures) * 100, 2), '% of the dataset')","de3a4957":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('cp_type', data = trainFeatures, palette = colors)\nplt.title('Sample Treatment Distribution \\n (trt_cp: Compound Treatment || clt_vehicle: Control Perturbation Treatment)', fontsize = 14)\nplt.show()","ac40f6bd":"# The Treatment Dose is almost equally distributed:\n\nprint('High Dose', round(trainFeatures['cp_dose'].value_counts()[0]\/len(trainFeatures) * 100, 2), '% of the dataset')\nprint('Low Dose', round(trainFeatures['cp_dose'].value_counts()[1]\/len(trainFeatures) * 100, 2), '% of the dataset')\n\ncolors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('cp_dose', data = trainFeatures, palette = colors)\nplt.title('Treatment Dose Distribution \\n (High Dose || Low Dose)', fontsize = 14)\nplt.show()","6e0f9d4c":"# The Treatment Duration is almost equally distributed:\n\nduration = pd.DataFrame(trainFeatures['cp_time'].value_counts()).reset_index()\n\nprint('24 hrs. of treatment doses', round(duration.loc[0][1]\/len(trainFeatures) * 100, 2), '% of the dataset')\nprint('48 hrs. of treatment doses', round(duration.loc[1][1]\/len(trainFeatures) * 100, 2), '% of the dataset')\nprint('72 hrs. of treatment doses', round(duration.loc[2][1]\/len(trainFeatures) * 100, 2), '% of the dataset')\n\ncolors = [\"#0101DF\", \"#DF0101\", \"#008000\"]\n\nsns.countplot('cp_time', data = trainFeatures, palette = colors)\nplt.title('Treatment Duration Distribution \\n (24 || 48 || 72 units of hours)', fontsize = 14)\nplt.show()","6ea097d0":"fig, ax = plt.subplots(1, 4, figsize = (22, 4))\n\ngene3 = trainFeatures['g-3'].values\ngene2 = trainFeatures['g-2'].values\ngene1 = trainFeatures['g-1'].values\ngene0 = trainFeatures['g-0'].values\n\nsns.distplot(gene3, ax = ax[0], color = 'r')\nax[0].set_title('Distribution of gene3', fontsize = 14)\nax[0].set_xlim([min(gene3), max(gene3)])\n\nsns.distplot(gene2, ax = ax[1], color = 'b')\nax[1].set_title('Distribution of gene2', fontsize = 14)\nax[1].set_xlim([min(gene2), max(gene2)])\n\nsns.distplot(gene1, ax = ax[2], color = 'g')\nax[2].set_title('Distribution of gene1', fontsize = 14)\nax[2].set_xlim([min(gene1), max(gene1)])\n\nsns.distplot(gene0, ax = ax[3], color = 'y')\nax[3].set_title('Distribution of gene0', fontsize = 14)\nax[3].set_xlim([min(gene0), max(gene0)])\n\nplt.show()","889e3e4e":"Skewness = pd.DataFrame(trainFeatures.skew()).reset_index()\nSkewness.columns = ['column', 'skewness']\nSkewness = Skewness.sort_values('skewness')\nSkewness.head()","d1a72e7c":"cell_cols = [col for col in trainFeatures if col.startswith('c-')]\ngene_cols = [col for col in trainFeatures if col.startswith('g-')]\n\ncellMeans = trainFeatures[cell_cols].mean()\ncellMeans = pd.DataFrame(cellMeans).reset_index()\ncellMeans.columns = ['column', 'mean']\ngeneMeans = trainFeatures[gene_cols].mean()\ngeneMeans = pd.DataFrame(geneMeans).reset_index()\ngeneMeans.columns = ['column', 'mean']\n\n\nfig, ax = plt.subplots(1, 2, figsize = (22, 4))\n\ncell = cellMeans['mean'].values\ngene = geneMeans['mean'].values\n\nsns.distplot(cell, ax = ax[0], color = 'r')\nax[0].set_title('Distribution of means of cell variables', fontsize = 14)\nax[0].set_xlim([min(cell), max(cell)])\n\nsns.distplot(gene, ax = ax[1], color = 'b')\nax[1].set_title('Distribution of means of gene variables', fontsize = 14)\nax[1].set_xlim([min(gene), max(gene)])\n\nplt.show()","9cbbc90f":"cellMins = trainFeatures[cell_cols].max()\ncellMins = pd.DataFrame(cellMins).reset_index()\ncellMins.columns = ['column', 'MAX']\ngeneMins = trainFeatures[gene_cols].max()\ngeneMins = pd.DataFrame(geneMins).reset_index()\ngeneMins.columns = ['column', 'MAX']\n\n\nfig, ax = plt.subplots(1, 2, figsize = (22, 4))\n\ncell = cellMins['MAX'].values\ngene = geneMins['MAX'].values\n\nsns.distplot(cell, ax = ax[0], color = 'r')\nax[0].set_title('Distribution of maximums of cell variables', fontsize = 14)\nax[0].set_xlim([min(cell), max(cell)])\n\nsns.distplot(gene, ax = ax[1], color = 'b')\nax[1].set_title('Distribution of maximums of gene variables', fontsize = 14)\nax[1].set_xlim([min(gene), max(gene)])\n\nplt.show()","1011a847":"plt.figure(figsize = (20, 6))\ntargetTypes = []\nfor column in trainTargetsS.columns:\n    try:\n        targetTypes.append(column.rsplit('_', 1)[1])\n    except:\n        targetTypes.append(column.rsplit('_', 1)[0])\ntargetTypes = list(dict.fromkeys(targetTypes))\n\ntargetTypes.remove('id')\ntargetTypes.remove('b')\n\ntargets = {}\nfor i in targetTypes:\n    targets[i] = 0\n\nfor column in trainTargetsS.columns:\n    try:\n        col = column.rsplit('_', 1)[1]\n        if col not in ['id', 'b']:\n            targets[col] += 1\n        else: pass\n                \n    except:\n        col = column.rsplit('_', 1)[0]\n        if col not in ['id', 'b']:\n            targets[col] += 1\n        else: pass\ntargets = pd.DataFrame.from_dict(targets, orient = 'index')\ntargets = targets.reset_index()\ntargets.columns = ['target_type', 'types']\nsns.barplot(x = 'target_type', y = 'types', data = targets)\nplt.xticks(rotation = 90)\nplt.title('Types of Target variables')\nplt.show()","151de245":"correlations = trainFeatures.corr()\ncorrelationsM = correlations.abs()\nkot = correlations[(correlationsM >= .9) & (correlationsM != 1)]\nkot = kot.dropna(axis = 1, how = 'all')\nkot = kot.dropna(axis = 0, how = 'all')\nplt.figure(figsize = (20, 20))\nsns.heatmap(kot, cmap = \"Greens\")","c108b268":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ntrain = preprocess(trainFeatures)\ntest = preprocess(testFeatures)\n\ndel trainTargetsS['sig_id']\n\ntrainTargetsS = trainTargetsS.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain = train.loc[train['cp_type'] == 0].reset_index(drop = True)","d8c27d99":"def create_model(num_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period = 10),\n                  loss = 'binary_crossentropy',\n                  )\n    return model","c400a182":"seed = 42\n\nfrom typing import Tuple, List, Callable, Any\n\nfrom sklearn.utils import check_random_state  # type: ignore\n\n### from eli5\ndef iter_shuffled(X, columns_to_shuffle=None, pre_shuffle=False,\n                  random_state=None):\n    rng = check_random_state(random_state)\n\n    if columns_to_shuffle is None:\n        columns_to_shuffle = range(X.shape[1])\n\n    if pre_shuffle:\n        X_shuffled = X.copy()\n        rng.shuffle(X_shuffled)\n\n    X_res = X.copy()\n    for columns in tqdm(columns_to_shuffle):\n        if pre_shuffle:\n            X_res[:, columns] = X_shuffled[:, columns]\n        else:\n            rng.shuffle(X_res[:, columns])\n        yield X_res\n        X_res[:, columns] = X[:, columns]\n\n\n\ndef get_score_importances(\n        score_func,  # type: Callable[[Any, Any], float]\n        X,\n        y,\n        n_iter=5,  # type: int\n        columns_to_shuffle=None,\n        random_state=None\n    ):\n    rng = check_random_state(random_state)\n    base_score = score_func(X, y)\n    scores_decreases = []\n    for i in range(n_iter):\n        scores_shuffled = _get_scores_shufled(\n            score_func, X, y, columns_to_shuffle=columns_to_shuffle,\n            random_state=rng, base_score=base_score\n        )\n        scores_decreases.append(scores_shuffled)\n\n    return base_score, scores_decreases\n\n\n\ndef _get_scores_shufled(score_func, X, y, base_score, columns_to_shuffle=None,\n                        random_state=None):\n    Xs = iter_shuffled(X, columns_to_shuffle, random_state=random_state)\n    res = []\n    for X_shuffled in Xs:\n        res.append(-score_func(X_shuffled, y) + base_score)\n    return res\n\ndef metric(y_true, y_pred):\n    metrics = []\n    for i in range(y_pred.shape[1]):\n        if y_true[:, i].sum() > 1:\n            metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float)))\n    return np.mean(metrics)   \n\nperm_imp = np.zeros(train.shape[1])\nall_res = []\nfor n, (tr, te) in enumerate(KFold(n_splits=7, random_state=0, shuffle=True).split(trainTargetsS)):\n    print(f'Fold {n}')\n\n    model = create_model(len(train.columns))\n    checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n    model.fit(train.values[tr],\n                  trainTargetsS.values[tr],\n                  validation_data=(train.values[te], trainTargetsS.values[te]),\n                  epochs=35, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt], verbose=2\n                 )\n        \n    model.load_weights(checkpoint_path)\n        \n    def _score(X, y):\n        pred = model.predict(X)\n        return metric(y, pred)\n\n    base_score, local_imp = get_score_importances(_score, train.values[te], trainTargetsS.values[te], n_iter=1, random_state=0)\n    all_res.append(local_imp)\n    perm_imp += np.mean(local_imp, axis=0)\n    print('')\n    \ntop_feats = np.argwhere(perm_imp < 0).flatten()\ntop_feats","f2f49f39":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in trainTargetsS.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)","117384a2":"N_STARTS = 7\ntf.random.set_seed(42)\n\nres = trainTargetsS.copy()\nsubmission.loc[:, trainTargetsS.columns] = 0\nres.loc[:, trainTargetsS.columns] = 0\n\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=7, random_state=seed, shuffle=True).split(trainTargetsS, trainTargetsS)):\n        print(f'Fold {n}')\n    \n        model = create_model(len(top_feats))\n        checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n        model.fit(train.values[tr][:, top_feats],\n                  trainTargetsS.values[tr],\n                  validation_data=(train.values[te][:, top_feats], trainTargetsS.values[te]),\n                  epochs=35, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt], verbose=2\n                 )\n        \n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(test.values[:, top_feats])\n        val_predict = model.predict(train.values[te][:, top_feats])\n        \n        submission.loc[:, trainTargetsS.columns] += test_predict\n        res.loc[te, trainTargetsS.columns] += val_predict\n        print('')\n    \nsubmission.loc[:, trainTargetsS.columns] \/= ((n+1) * N_STARTS)\nres.loc[:, trainTargetsS.columns] \/= N_STARTS","80eedf75":"print(f'OOF Metric: {metric(trainTargetsS, res)}')\nsubmission.loc[test['cp_type'] == 1, trainTargetsS.columns] = 0\nsubmission.to_csv('submission.csv', index = False)\nsubmission.head()","391d45a8":"**Observation**: \n- The means of cells and genes have close to normal distributions. \n- The max of cells are centered around 3.5 - 4 and 10 that of genes","b449d8e2":"### Visualising different types of target columns:","245c3025":"### Below are the most common target classes:\n\n- Inhibotor\n- Agonist\n- Antagonist\n- Activator\n- Analgesic","a016164a":"Model based upon: https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2\/","470111f0":"**Observation**: Gene variables' means have a close to normal distribution, with very slight skewness in most of the gene variables"}}