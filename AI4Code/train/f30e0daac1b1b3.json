{"cell_type":{"dc9547e7":"code","6e010b31":"code","61ab258e":"code","ff5d6f34":"code","8627db46":"code","c033655d":"code","cc38ae5a":"code","611a69d9":"code","f9296c53":"code","6d6d9da1":"code","54f348c9":"code","6273cbfa":"code","1a7dceb5":"code","26682d23":"code","8b9a6435":"code","dbbf07a2":"code","9bb1782f":"code","c694556c":"code","f9fd5137":"code","484cb606":"code","eb1ddcf9":"code","ae792718":"code","9c6a81e1":"markdown","e0dab32e":"markdown","4d9282c6":"markdown","a1a8ea40":"markdown","e7852ca5":"markdown","c620ace4":"markdown","bf177293":"markdown","2c7d81a6":"markdown","50b88448":"markdown"},"source":{"dc9547e7":"!pip install -q fastai==2.2.5 fastcore==1.3.19 fast-tabnet==0.2.0","6e010b31":"from fastai.tabular.all import *\nfrom fast_tabnet.core import *\n\nSEED = 42\nset_seed(SEED, reproducible=True)","61ab258e":"path = Path('\/kaggle\/input\/tabular-playground-series-jan-2021')\npath.ls()","ff5d6f34":"train_df = pd.read_csv(path\/'train.csv')\ntrain_df.head()","8627db46":"y_names = ['target']\ncont_names = list(train_df.columns.values)[1:-1]\ncat_names = []\nprocs = [Categorify, FillMissing, Normalize]\nsplits = RandomSplitter(seed=SEED)(range_of(train_df))\nbs = 256","c033655d":"db = TabularPandas(\n    train_df, \n    procs=procs, \n    cat_names=cat_names, \n    cont_names=cont_names, \n    y_names=y_names, \n    y_block=RegressionBlock(),\n    splits=splits,\n)","cc38ae5a":"dls = db.dataloaders(bs=bs)\ndls.show_batch()","611a69d9":"model_name = 'tabnet'","f9296c53":"model = TabNetModel(get_emb_sz(db), len(db.cont_names), dls.c, n_d=64, n_a=64, n_steps=5, virtual_batch_size=256)","6d6d9da1":"# save the best model so far\ncbs = [SaveModelCallback(monitor='_rmse', comp=np.less, fname=model_name+'_best')]","54f348c9":"learn = Learner(dls, model, loss_func=MSELossFlat(), metrics=rmse, cbs=cbs)","6273cbfa":"learn.lr_find()","1a7dceb5":"learn.fit_one_cycle(20, 5e-2)","26682d23":"learn.show_results()","8b9a6435":"learn.load(model_name+'_best')","dbbf07a2":"preds, targs = learn.get_preds()\npreds = preds.squeeze(1)","9bb1782f":"rmse(preds, targs)","c694556c":"test_df = pd.read_csv(path\/'test.csv')\ntest_df.head()","f9fd5137":"test_dl = dls.test_dl(test_df)","484cb606":"preds, _ = learn.get_preds(dl=test_dl)\npreds = preds.squeeze(1)","eb1ddcf9":"submit = pd.read_csv(path\/'sample_submission.csv')\nsubmit['target'] = preds\nsubmit.head()","ae792718":"submit.to_csv('submission.csv', index=False)","9c6a81e1":"## Import libraries & data\n- `fastai` releases updates frequently, so I won't guarantee this notebook will work with versions later than the one specified here\n- This notebook is a follow-up from my previous attempt [**link**](https:\/\/www.kaggle.com\/nguyncaoduy\/fastai-tabular-regression-model-nn-xgb) \n- This notebook demonstrates how to use **TabNet (Attention-based network for tabular data)** in `fastai`. The original paper https:\/\/arxiv.org\/pdf\/1908.07442.pdf. ","e0dab32e":"## Make predictions on test data","4d9282c6":"| Model    | Min RMSE (Validation) |\n|----------|----------|\n| tabnet    | 0.7147   |","a1a8ea40":"## Model Training","e7852ca5":"## Submit to Kaggle\n- Download the `submission.csv` file and submit","c620ace4":"Remember to try modifying the hyperparameters to get the best performance! Here I just used the default values","bf177293":"## Process data","2c7d81a6":"## Evaluate on validation data","50b88448":"### TabNet architecture\n\n`model = TabNetModel(emb_szs, n_cont, out_sz, embed_p=0., y_range=None, \n                     n_d=8, n_a=8,\n                     n_steps=3, gamma=1.5, \n                     n_independent=2, n_shared=2, epsilon=1e-15,\n                     virtual_batch_size=128, momentum=0.02)`\n\nParameters `emb_szs, n_cont, out_sz, embed_p, y_range` are the same as for fastai TabularModel.\n\n- n_d : int\n    Dimension of the prediction  layer (usually between 4 and 64)\n- n_a : int\n    Dimension of the attention  layer (usually between 4 and 64)\n- n_steps: int\n    Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n- gamma : float\n    Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n- momentum : float\n    Float value between 0 and 1 which will be used for momentum in all batch norm\n- n_independent : int\n    Number of independent GLU layer in each GLU block (default 2)\n- n_shared : int\n    Number of independent GLU layer in each GLU block (default 2)\n- epsilon: float\n    Avoid log(0), this should be kept very low"}}