{"cell_type":{"de0cb7b0":"code","2071031c":"code","0191ed36":"code","e17fea00":"code","d2c61485":"code","bf824321":"code","cdf5eb2b":"code","42f2c346":"code","c2f6b295":"code","c4cb35d7":"code","b2781715":"markdown","ddae814a":"markdown","5bda5b2a":"markdown","8bb0f327":"markdown","2c3243b4":"markdown"},"source":{"de0cb7b0":"import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.utils import save_image, make_grid\nimport torch.nn.functional as F","2071031c":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","0191ed36":"img_size = 128\n# VGG networks are trained on images with each channel normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]\nstats = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\ntransformer = transforms.Compose([\n    transforms.Resize((img_size,img_size),Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize(*stats)  \n])\n\ndef load_image(path):\n    image = Image.open(path).convert('RGB')\n    image = transformer(image).unsqueeze(0)\n    # add extra dim to fit input dim\n    return image\n\nstyle_img = load_image('..\/input\/nstdataset\/style-img.jpg')\ncontent_img = load_image('..\/input\/nstdataset\/content-img.jpg')","e17fea00":"def denorm(img_batch):\n    return img_batch * stats[1][0] + stats[0][0]\n    # Rescale the normalised images to (0,1)\n    \ndef images_preview(content_img,style_img,generated_image):\n    fig, ax = plt.subplots(figsize=(8, 10))\n    ax.axis('off')\n    grid1 = make_grid(denorm(content_img), nrow=1)    \n    grid2 = make_grid(denorm(style_img), nrow=1)\n    grid3 = make_grid(denorm(generated_image), nrow=1)    \n    img_grid = torch.cat((grid1, grid2, grid3), -1)\n    ax.imshow(make_grid(img_grid, nrow=1).permute(1, 2, 0))\n    \ngenerated_image = content_img.clone()\nimages_preview(content_img,style_img,generated_image.detach())\n# For first iter generated image will be the same as content image","d2c61485":"vgg_model = models.vgg19(pretrained = True).features\nprint(vgg_model)","bf824321":"for name, layer in vgg_model._modules.items():\n    print(name,layer)","cdf5eb2b":"class VGG_5(nn.Module):\n    def __init__(self):\n        super(VGG_5, self).__init__()\n        # content_layer : conv_4\n        # style_layers : conv_1, conv_2, conv_3, conv_4, conv_5\n        # as mentioned in the paper we will need only upto 5 conv blocks i.e. upto 28 layers\n        # i.e. layer: 0, 5, 10, 19, 28\n        self.vgg_model = models.vgg19(pretrained = True).features[:29]\n        \n    def forward(self,image):\n        x = image.to(device)\n        features = []\n        for num, layer in self.vgg_model._modules.items():            \n            x = layer(x)\n            if num in ['0', '5', '10', '19', '28']:\n                features.append(x)\n                \n                \n        return features","42f2c346":"def compute_gram_mat(feature):\n    _,channel,height,width = feature.size()\n    feature = feature.view(channel,height*width)\n    # multiply h*w so that we will multiply feature accd to each channel\n    gram_mat = torch.mm(feature,feature.t())\n    # multiply with transpose\n    return gram_mat","c2f6b295":"EPOCHS = 30\noptimizer = torch.optim.LBFGS([generated_image.requires_grad_()])\nvgg_model = VGG_5().to(device).eval()\ncontent_weight = 1  # alpha\nstyle_weight = 0.0001  # beta","c4cb35d7":"for epoch in range(EPOCHS):\n    def closure():\n        optimizer.zero_grad()\n        content_features = vgg_model(content_img)\n        style_features = vgg_model(style_img)\n        generated_features = vgg_model(generated_image)\n\n        style_loss, content_loss = 0, 0\n        \n        for content_feature,style_feature,gen_feature in zip(content_features,style_features,generated_features):\n            content_loss += torch.mean((gen_feature-content_feature)**2)\n            gram_gen = compute_gram_mat(gen_feature)\n            gram_style = compute_gram_mat(style_feature)\n            style_loss += torch.mean((gram_gen-gram_style)**2)\n        \n        total_loss = content_weight * content_loss + style_weight * style_loss\n        total_loss.backward()\n            \n\n        if epoch % 10 == 0 :            \n            print('Content Loss: {}, Style Loss: {}'.format(content_loss.item(),style_loss.item()))\n            images_preview(content_img,style_img,generated_image.detach().cpu())\n                    \n        return total_loss\n            \n    optimizer.step(closure)","b2781715":"## Helper Functions","ddae814a":"## Gram Matrix","5bda5b2a":"## Get Features From VGG Model","8bb0f327":"## Import Libraries and Data","2c3243b4":"## VGG19"}}