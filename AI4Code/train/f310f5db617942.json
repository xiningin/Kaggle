{"cell_type":{"70bfb147":"code","21bfa220":"code","cad36cad":"code","b899e984":"code","c1406197":"code","b86e8977":"code","0ebe3326":"code","1b868056":"code","0ae3ebf9":"code","3abc9d95":"code","445f3a63":"code","c2a84b39":"code","5b0dd8f5":"code","f5258248":"code","8e52fe8b":"code","0d2e8602":"code","865a612b":"code","658dd775":"code","0a30f280":"code","b4fa7ad5":"code","0336884b":"code","86b6e291":"code","03fbb4e8":"code","dd8a3c47":"code","5238befb":"code","529b9cb0":"code","81881437":"code","99b1b041":"code","d1298e79":"code","0a6749a1":"code","c27b4a3d":"code","f0926863":"code","a699f07f":"code","80fa91fa":"code","6ffdf942":"code","78ff85cd":"code","724c4ade":"code","4dda7b3d":"code","42ca55de":"code","8f5890a5":"code","6df8c9ee":"code","6fd72cf9":"code","22172764":"code","12090df0":"code","72fdbb30":"code","39c52bf3":"code","59aad955":"code","266077ad":"code","b2e4b365":"code","c5d13248":"code","15be5084":"code","af6258d9":"markdown","e7d25c79":"markdown","59fe2fa2":"markdown","82fa5662":"markdown","17303eb5":"markdown","f944dc86":"markdown","742bee07":"markdown","f0fa5723":"markdown","6b0f4b08":"markdown","73f834e0":"markdown","9fed21ff":"markdown","52393013":"markdown","363e2fea":"markdown","f445feb4":"markdown","ae015395":"markdown","62781df8":"markdown","03d12646":"markdown","e9a8be05":"markdown","643a2814":"markdown","420ff748":"markdown","09d280f3":"markdown","82a3d959":"markdown","2254874b":"markdown","1a3dc602":"markdown","3e380fe0":"markdown","bb40eb85":"markdown","8cc9d459":"markdown","a620880a":"markdown","d5c7fe0e":"markdown","37bb8ae7":"markdown","0c96348c":"markdown","69e3acfe":"markdown","12466bf1":"markdown","57da381d":"markdown","b9b26d46":"markdown","cfb1460d":"markdown","8028b52b":"markdown","920ef137":"markdown","641e989f":"markdown","ed9d0fc1":"markdown","0cf0ca67":"markdown","fc5d8df4":"markdown","9314e972":"markdown","c30ccae7":"markdown","717290e0":"markdown","51cf2eaa":"markdown","a68a8f6c":"markdown","8f0c659b":"markdown","153a6c95":"markdown","e0d38802":"markdown","f64dbbda":"markdown","3212fe0e":"markdown","43bf70a1":"markdown"},"source":{"70bfb147":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","21bfa220":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","cad36cad":"train = pd.read_csv(\"..\/input\/home-credit-default-risk\/application_train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/home-credit-default-risk\/application_test.csv\", index_col=0)","b899e984":"display(train.head())\ndisplay(train.tail())","c1406197":"display(test.head())\ndisplay(test.tail())","b86e8977":"columns = [\"TARGET\", \"CODE_GENDER\", \"NAME_CONTRACT_TYPE\", \"FLAG_OWN_CAR\",\n           \"FLAG_OWN_REALTY\", \"CNT_CHILDREN\",\"NAME_TYPE_SUITE\",\n           \"NAME_HOUSING_TYPE\", \"OCCUPATION_TYPE\", \"EMERGENCYSTATE_MODE\",\"FLAG_DOCUMENT_2\",\n           \"FLAG_DOCUMENT_3\",\"FLAG_DOCUMENT_4\",\"FLAG_DOCUMENT_5\",\"FLAG_DOCUMENT_6\",\n           \"FLAG_DOCUMENT_7\",\"FLAG_DOCUMENT_8\",\"FLAG_DOCUMENT_9\",\"FLAG_DOCUMENT_10\",\n           \"FLAG_DOCUMENT_11\",\"FLAG_DOCUMENT_12\",\"FLAG_DOCUMENT_13\",\n           \"FLAG_DOCUMENT_14\",\"FLAG_DOCUMENT_15\",\"FLAG_DOCUMENT_16\",\n           \"FLAG_DOCUMENT_17\",\"FLAG_DOCUMENT_18\",\"FLAG_DOCUMENT_19\",\n           \"FLAG_DOCUMENT_20\",\"FLAG_DOCUMENT_21\"]\nfor column in columns:\n  df = pd.DataFrame(train[column].value_counts())\n  plt.figure(figsize=(12,12))\n  plt.pie(df[column], labels=df.index, autopct='%2.1f%%')\n  plt.title(f\" {column}\")\n  plt.show()","0ebe3326":" male_zero = train.loc[(train[\"CODE_GENDER\"]==\"M\")&(train[\"TARGET\"]==0), :].count()[0]\n male_one =  train.loc[(train[\"CODE_GENDER\"]==\"M\")&(train[\"TARGET\"]==1), :].count()[0]\n female_zero =  train.loc[(train[\"CODE_GENDER\"]==\"F\")&(train[\"TARGET\"]==0), :].count()[0]\n female_one =  train.loc[(train[\"CODE_GENDER\"]==\"F\")&(train[\"TARGET\"]==1), :].count()[0]\n\nplt.figure(figsize=(10,5))\nsns.countplot(data=train,x=\"CODE_GENDER\", hue=\"TARGET\", )\nplt.text(-0.3,100000,male_zero)\nplt.text(0.08,15000,male_one)\nplt.text(0.7,190000,female_zero)\nplt.text(1.1,20000,female_one)\nplt.show()","1b868056":"plt.figure(figsize=(10,5))\nsns.countplot(data=train,x=\"FLAG_OWN_CAR\", hue=\"TARGET\")\nplt.show()","0ae3ebf9":"plt.figure(figsize=(10,4))\nsns.countplot(data=train,x=pd.cut(train.DAYS_BIRTH\/-365.25, bins=3,precision=0, right=True,retbins=False), hue=\"TARGET\")\nplt.xlabel(\"YEARS_BIRTH\")\nplt.show()","3abc9d95":"plt.figure(figsize=(10,4))\n(train[\"DAYS_EMPLOYED\"]\/-365.25).plot.hist(bins=10)\nplt.xlabel(\"YEARS_EMPLOYED\")\nplt.show()","445f3a63":"plt.figure(figsize=(14,10))\nsns.countplot(data=train,x=pd.cut(train.DAYS_EMPLOYED\/-365.25, bins=5,precision=0, right=True,retbins=False), hue=\"TARGET\")\nplt.xlabel(\"YEARS_EMPLOYED\")\nplt.show()","c2a84b39":"plt.figure(figsize=(15,10))\nsns.countplot(data=train, x=\"NAME_INCOME_TYPE\", hue=\"TARGET\")\nplt.show()","5b0dd8f5":"plt.figure(figsize=(15,10))\nsns.countplot(data=train,x=\"NAME_EDUCATION_TYPE\", hue=\"TARGET\", )\nplt.show()\n","f5258248":"plt.figure(figsize=(15,10))\nsns.countplot(data=train,x=\"NAME_FAMILY_STATUS\", hue=\"TARGET\", )\nplt.show()\n","8e52fe8b":"plt.figure(figsize=(15,8))\nsns.countplot(data=train,x=pd.cut(train.DAYS_LAST_PHONE_CHANGE\/-365.25, bins=12,precision=0, right=True,retbins=False), hue=\"TARGET\")\nplt.xlabel(\"YEARS_LAST_PHONE_CHANGE\")\nplt.show()","0d2e8602":"name_income_type=train.NAME_INCOME_TYPE.unique()\nfor name in name_income_type:\n  plt.figure(figsize=(7,5))\n  data=train.loc[(train.NAME_INCOME_TYPE==name), :]\n  sns.countplot(data = data, x= \"NAME_INCOME_TYPE\", hue=train[\"TARGET\"])\n  plt.show()\n","865a612b":"pd.DataFrame(train.isnull().sum(), columns=[\"MISSING_VALUES\"]).sort_values(by=\"MISSING_VALUES\", ascending=False).head(30)","658dd775":"#categorical columns\npd.DataFrame(train.select_dtypes('object').apply(pd.Series.nunique, axis = 0),\n             columns=[\"Categ_Data\"]).sort_values(by=\"Categ_Data\", ascending=False)","0a30f280":"#categorical data in test data set\npd.DataFrame(train.select_dtypes('object').apply(pd.Series.nunique, axis = 0),\n             columns=[\"Categ_Data\"]).sort_values(by=\"Categ_Data\", ascending=False)","b4fa7ad5":"colomns = [\"HOUR_APPR_PROCESS_START\", \"EXT_SOURCE_1\", \"APARTMENTS_MODE\",\n           \"YEARS_BEGINEXPLUATATION_MODE\",\"DAYS_LAST_PHONE_CHANGE\",\n           \"REGION_POPULATION_RELATIVE\", \"DAYS_REGISTRATION\", \"DAYS_ID_PUBLISH\",\n           \"DAYS_EMPLOYED\", \"AMT_ANNUITY\", \"AMT_CREDIT\", \"AMT_INCOME_TOTAL\"\n           ]\n\nfor colomn in colomns:\n  plt.figure(figsize=(12,6))\n  plt.subplot(1,2,1)\n  train[colomn].plot(kind = \"box\")\n  plt.subplot(1,2,2)\n  train[colomn].plot(kind = \"hist\")\n  plt.show()","0336884b":"train.AMT_INCOME_TOTAL.sort_values(ascending=False)","86b6e291":"useless_columns = [\"HOUSETYPE_MODE\", \"WALLSMATERIAL_MODE\",\"FLAG_DOCUMENT_2\",\n                     \"FLAG_DOCUMENT_4\",\"FLAG_DOCUMENT_5\",\"FLAG_DOCUMENT_7\",\n                     \"FLAG_DOCUMENT_9\",\"FLAG_DOCUMENT_10\",\"FLAG_DOCUMENT_12\",\n                     \"FLAG_DOCUMENT_15\",\"FLAG_DOCUMENT_17\",\"FLAG_DOCUMENT_19\",\n                   \"FLAG_DOCUMENT_20\", \"FLAG_DOCUMENT_21\",\"AMT_REQ_CREDIT_BUREAU_HOUR\",\n                   \"YEARS_BUILD_MODE\",\"ELEVATORS_MODE\",\"ENTRANCES_MODE\", \"FLOORSMAX_MODE\",\n                     \"COMMONAREA_MEDI\", \"ELEVATORS_MEDI\", \"ENTRANCES_MEDI\",\n                   \"FLOORSMAX_MEDI\", \"FLOORSMIN_MEDI\",\"ELEVATORS_AVG\",\"ENTRANCES_AVG\",\n                   \"FLOORSMAX_AVG\",\"FLOORSMIN_AVG\",\"LIVINGAPARTMENTS_AVG\",\n                   \"NONLIVINGAPARTMENTS_AVG\",\"ORGANIZATION_TYPE\"]","03fbb4e8":"train = train.drop(columns = useless_columns)\ntest = test.drop(columns=useless_columns)\ntrain.shape, test.shape","dd8a3c47":"#Fill the missing categorical data with the most frequent velue\ntrain_categorical = train.select_dtypes(include ='object') \ntrain_categorical = train_categorical.apply(lambda x: x.fillna(x.value_counts().index[0]))\n\n#Fill the missing numerical data with the median \ntrain_float = train.select_dtypes(include ='float64') \ntrain_float = train_float.fillna(train_float.median())\n\ntrain_int = train.select_dtypes(include ='int64') \ntrain_int = train_int.fillna(train_int.median())\n\ntrain_final = pd.concat([train_categorical,train_float,train_int], axis=1)\ntrain_final[\"TARGET\"] = train[\"TARGET\"]\ntrain=train_final","5238befb":"#fill missing categorical values in test data set with most frequent values in \n#training data set (don't use any info from test data set)\n\ncategorical_columns = test.select_dtypes(include ='object').columns\ntest_categorical = test.select_dtypes(include ='object') \n\nfor column in categorical_columns:\n  test_categorical[column] = test_categorical[column].fillna(train[column].value_counts().index[0])\n\n#for numerical missing values in test data set, fill them with median value in \n#the training data set (don't use any info from test data set)\ntest_float = test.select_dtypes(include ='float64') \ntest_float = test_float.fillna(train_float.median())\n\ntest_int = test.select_dtypes(include ='int64') \ntest_int = test_int.fillna(train_int.median())\n\ntest_final = pd.concat([test_categorical,test_float,test_int], axis=1)\ntest=test_final","529b9cb0":"train.shape,test.shape","81881437":"train.loc[train.DAYS_EMPLOYED==train.DAYS_EMPLOYED.max(), :].NAME_INCOME_TYPE.unique()","99b1b041":"train.DAYS_EMPLOYED.max()","d1298e79":"#replace anomalies with the extreme value in range of normal data:\n#for years of employment, the maximum number should be 50 years for pensioners,\n#and 0 for unemployed clients\ntrain_dataframe=train.copy()\ntest_dataframe = test.copy()\ndef unemployed_anomaly(x):\n  if x==365243:\n    x= 0\n  return x\n\ndef pensioner_anomaly(x):\n  if x==365243:\n    x=-18263\n  return x\n\ntrain_unemployed = train_dataframe.loc[train_dataframe.NAME_INCOME_TYPE==\"Unemployed\",:]\ntrain_pensioner = train_dataframe.loc[train_dataframe.NAME_INCOME_TYPE==\"Pensioner\",:]\ntrain_other = train_dataframe.loc[(train_dataframe.NAME_INCOME_TYPE!=\"Unemployed\")&(train_dataframe.NAME_INCOME_TYPE!=\"Pensioner\"),:]\ntrain_unemployed.DAYS_EMPLOYED = train_unemployed.DAYS_EMPLOYED.apply(unemployed_anomaly)\ntrain_pensioner.DAYS_EMPLOYED=train_pensioner.DAYS_EMPLOYED.apply(pensioner_anomaly)\ntrain_dataframe=pd.concat([train_unemployed,train_pensioner,train_other],axis=0)\n\ntest_unemployed = test_dataframe.loc[test_dataframe.NAME_INCOME_TYPE==\"Unemployed\",:]\ntest_pensioner = test_dataframe.loc[test_dataframe.NAME_INCOME_TYPE==\"Pensioner\",:]\ntest_other = test_dataframe.loc[(test_dataframe.NAME_INCOME_TYPE!=\"Unemployed\")&(test_dataframe.NAME_INCOME_TYPE!=\"Pensioner\"),:]\ntest_unemployed.DAYS_EMPLOYED=test_unemployed.DAYS_EMPLOYED.apply(unemployed_anomaly)\ntest_pensioner.DAYS_EMPLOYED=test_pensioner.DAYS_EMPLOYED.apply(pensioner_anomaly)\ntest_dataframe=pd.concat([test_unemployed,test_pensioner,test_other],axis=0)\n\ndisplay(test_dataframe.head())\ndisplay(train_dataframe.head())","0a6749a1":"train_dataframe.DAYS_EMPLOYED.min()\/365.25","c27b4a3d":"train=train_dataframe\ntest=test_dataframe","f0926863":"#RATIOS:\n#1- amount of annual payment to annual income:\ntrain_df = train.copy()\ntrain_df[\"PAYMENT_RATIO\"] = train_df[\"AMT_ANNUITY\"]\/train_df[\"AMT_INCOME_TOTAL\"]\ntrain_df = train_df.drop(columns=[\"AMT_ANNUITY\"])\ntest_df = test.copy()\ntest_df[\"PAYMENT_RATIO\"] = test_df[\"AMT_ANNUITY\"]\/test_df[\"AMT_INCOME_TOTAL\"]\ntest_df = test_df.drop(columns=[\"AMT_ANNUITY\"])\n","a699f07f":"train_df.head()","80fa91fa":"#Years Instead of Days!\ndef year_convertor(dataframe):\n  df = dataframe.copy()\n  df[\"AGE\"] = df[\"DAYS_BIRTH\"].apply(lambda x: np.int(-x\/365))\n  df[\"YEARS_EMPLOYED\"] = df[\"DAYS_EMPLOYED\"].apply(lambda x: np.int(-x\/365))\n  df[\"YEARS_REGISTERED\"] = df[\"DAYS_REGISTRATION\"].apply(lambda x: np.int(-x\/365))\n  df[\"YEARS_ID_PUBLISHED\"] = df[\"DAYS_ID_PUBLISH\"].apply(lambda x: np.int(-x\/365))\n  df[\"YEARS_LAST_PHONE_CHANGE\"]=df[\"DAYS_LAST_PHONE_CHANGE\"].apply(lambda x: np.int(-x\/365))\n  df = df.drop(columns = [\"DAYS_BIRTH\", \"DAYS_EMPLOYED\", \"DAYS_REGISTRATION\", \"DAYS_ID_PUBLISH\",\"DAYS_LAST_PHONE_CHANGE\"])\n  return df","6ffdf942":" training_df = year_convertor(train_df)\n testing_df = year_convertor(test_df)\ntraining_df.head()","78ff85cd":"train=training_df\ntest=testing_df","724c4ade":"train_1 = train.copy()\ntest_1 = test.copy()\n\n#Binary Categorical Data (Y\/N,F\/M):\ntrain_1['FLAG_OWN_CAR'] = train_1['FLAG_OWN_CAR'].replace(to_replace = ['Y', 'N'], value = [1,0] )\ntrain_1['FLAG_OWN_REALTY'] = train_1['FLAG_OWN_REALTY'].replace(['Y', 'N'], [1, 0])\ntrain_1['EMERGENCYSTATE_MODE'] = train_1['EMERGENCYSTATE_MODE'].replace(['Yes', 'No'], [1, 0])\ntrain_1['CODE_GENDER'] = train_1['CODE_GENDER'].replace(['M', 'F'], [1, 0])\n\ntest_1['FLAG_OWN_CAR'] = test_1['FLAG_OWN_CAR'].replace(['Y', 'N'], [1, 0])\ntest_1['FLAG_OWN_REALTY'] = test_1['FLAG_OWN_REALTY'].replace(['Y', 'N'], [1, 0])\ntest_1['EMERGENCYSTATE_MODE'] = test_1['EMERGENCYSTATE_MODE'].replace(['Yes', 'No'], [1, 0])\ntest_1['CODE_GENDER'] = test_1['CODE_GENDER'].replace(['M', 'F'], [1, 0])\ntrain_1.head()","4dda7b3d":"train=train_1\ntest=test_1","42ca55de":"#one-hot coding on categorical columns:\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)\ntrain.head()","8f5890a5":"train.shape,test.shape","6df8c9ee":"#saving the targets firs:\ntrain_labels = train['TARGET']\n\n# Align the training and testing data\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target\ntrain['TARGET'] = train_labels","6fd72cf9":"train.shape,test.shape","22172764":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","12090df0":"X_train, X_test, y_train, y_test = train_test_split(train.drop(columns=\"TARGET\"),\n                                                    train[\"TARGET\"], test_size=0.10,\n                                                    random_state=101)","72fdbb30":"X_train.shape,X_test.shape","39c52bf3":"reg_algorithm = LogisticRegression()\nreg_algorithm.fit(X_train,y_train)\npredictions = reg_algorithm.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","59aad955":"dt_model=DecisionTreeClassifier()\ndt_model.fit(X_train,y_train)\ndt_pred = dt_model.predict(X_test)\nprint(confusion_matrix(y_test,dt_pred))\nprint(classification_report(y_test,dt_pred))","266077ad":"rf_model= RandomForestClassifier(n_estimators=300)\nrf_model.fit(X_train,y_train)\nrf_pre=rf_model.predict(X_test)\nprint(confusion_matrix(y_test,rf_pre))\nprint(classification_report(y_test,rf_pre))","b2e4b365":"xgb_model = XGBClassifier(n_estimators=300)\nxgb_model.fit(X_train,y_train)\nxg_pred = xgb_model.predict(X_test)\nprint(confusion_matrix(y_test,xg_pred))\nprint(classification_report(y_test,xg_pred))","c5d13248":"X=train.drop(columns=\"TARGET\")\ny=train.TARGET\ntuned_parameters = [{'n_estimators': [200,350], 'max_depth' : [5,10]}]\nclf = GridSearchCV(XGBClassifier(), tuned_parameters, cv=5, scoring='roc_auc', return_train_score=True)\nclf.fit(X, y)\nprint(clf.classes_)\nprint (\"best score = \", clf.best_score_)\nprint(\"Best parameters set found on development set: \", clf.best_params_)\nmaxDepth=clf.best_params_[\"max_depth\"]\nnEstimator = clf.best_params_[\"n_estimators\"]\n\nxgb_model=XGBClassifier(max_depth=maxDepth,n_estimators=nEstimator)\nX = train.drop(columns=\"TARGET\")\ny = train.TARGET\nxgb_model=xgb_model.fit(X,y)\ntest_labels = xgb_model.predict_proba(test)\n","15be5084":"submission = pd.DataFrame({\n    \"SK_ID_CURR\": test.index,\n    \"TARGET\" : test_labels[:,1]\n})\nsubmission.to_csv('submission.csv', index=False)\n","af6258d9":"As we can see, there is a **greater chance** for ** youger applicants** to fail to pay their loans which is a good information for taking precautionary measures.","e7d25c79":"We can see that the anomalies for the employment days were those who were either pensioner or umemployed.\nI chose to impute the pensioners employment history anomalies with 50 (the max years) and the unemployed ones with 0 \n(because the don't have any employment history claimed)","59fe2fa2":"### Based on Income type:","82fa5662":"### Gender-based :","17303eb5":"It seems the algorithm doesn't learn any thing from the data. That's probably because of the imbalanced labels.","f944dc86":"## 4.  Imputation","742bee07":"### XGBoost Classifier:","f0fa5723":"### \"Marriage status\"-based:","6b0f4b08":"## 5. Feature Engineering","73f834e0":"Let's see our data anomalies by plotting some distributions:","9fed21ff":"### Anomalies","52393013":"## 3. Data Cleaning","363e2fea":"#### 2- Years instead of days!","f445feb4":"### distribution of the employment history:","ae015395":"### Categorical values:","62781df8":"### Education-bsed:","03d12646":"We are all good :)","e9a8be05":"I chose to fill the categorical missing values with the most frequent ones, and the numerical values with their median.\n(I chose median instead of mean to avoid affected by the anomalies)","643a2814":"###  Logistic Regression:","420ff748":"### One-Hot Coding:","09d280f3":"### Owning a Car:","82a3d959":"Before one-hot coding, I tried to replace binary categorical values simply with 0 and 1 to prevent more dimention increase:","2254874b":"#### 1- Payment Ratio","1a3dc602":"### Ratios:","3e380fe0":"This project aims to perform a useful EDA on the data, suggest some future engineering and finally find the best model for the ML part.","bb40eb85":"### Mssing Values:","8cc9d459":"Because of more categories in one or more columns in the training dataset, the one-hot coding produced more columns for the training dataset. we can simply align these data sets together","a620880a":"### Grid Search Cross Validation","d5c7fe0e":"### Decision Tree Classifier:","37bb8ae7":"### Splitting the data set ","0c96348c":"Now that we deleted the useless columns, let's impute missing values and anomalies with reasonable values:","69e3acfe":"## 7. Model Optimization","12466bf1":"Now it's aligned","57da381d":"### based on the last phone number change:","b9b26d46":"# 2. EDA\nLet's do some charting!","cfb1460d":"Let's delet these columns:","8028b52b":"As we saw in the EDA part, some of the columns don't provide any data (as most of \"FLAG_DOCUMENT_... ones). Some of the columns also don't contain any useful data (such as the walls materials, the elevator type and area and etc...)","920ef137":"Wow! what a disaster in data collection! Seems we have lots of work to do!","641e989f":"### Age-based:","ed9d0fc1":"I tried to do some feature engineering based on  domain knowledge and not the feature importance","0cf0ca67":"And now we are set for the one-hot coding ","fc5d8df4":"## 6. ML model building","9314e972":"### \"Employment history\"-based:","c30ccae7":"The other point was that I chose to not to impute the test dataset with its own values and get the help of the median values in the training data. that's how we can prevent overfitting","717290e0":"Our training dataset now has 90 columns which is not too bad!","51cf2eaa":"As we can see, more than **10%** of the **male applicants** **failed** to pay theri loans while **less than 10%** of **the female applicants** did so.","a68a8f6c":"### Random Forest Classifier:","8f0c659b":"### Career-based:","153a6c95":"These are the columns which we need to one-hot code before ML model building","e0d38802":"# 1. Importing libraries and reading data","f64dbbda":"### As we can see:\n\n 1- the target column **(labels)** in the training dataset is significantly **imbalanced**\n    \n 2- Around  **66%** of the applicants are **women** and the rest are men\n    \n 3- Around **90%** are the loan applications are **cash loans** and the rest are revolving loans\n    \n 4- **34%** of the applicants already **own a car**\n    \n 5- **30%** of the applicants already **own a home or flat**\n    \n 6- **70%** of the applicants **don't have a child**, **20%** have **1**, and **9%** have **2 children**\n    \n 7- **81%** of the applicant were **living alone** while **13%** were **family**\n \n 8- Most of the applicants were **laborers (26%)** which were about **twice of the sales staff and core staff**\n \n 9- **1.4%** of the applicants had an **emergency situation** while applying for the loan\n \n 10- Most of the **columns named \"Documents\"** **don't have any useful data** and we can delet them in the data cleaning part\n    ","3212fe0e":"Let's plot some charts based on the number of those who did or didn't pay their loans:","43bf70a1":"### Replacing anomalies with reasonable values"}}