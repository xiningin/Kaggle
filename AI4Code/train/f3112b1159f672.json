{"cell_type":{"9ac955cc":"code","86a046b7":"code","6e4c6c1e":"code","dd59d6ad":"code","5813e803":"code","e7791aa4":"code","156e67f6":"code","0d08c310":"code","cc0ca3a3":"code","469edb94":"code","bcd7cb57":"code","c5496c13":"code","679bb48c":"code","bb4e147e":"code","b3449d88":"code","cbc9678f":"code","56857c5f":"code","a46c8950":"code","7da3bc86":"code","547d7449":"markdown","cc0b49c3":"markdown","c8bb273f":"markdown","e706e5d5":"markdown","627cab3d":"markdown","fa2d915c":"markdown","f91446b9":"markdown","ecc4c233":"markdown","69d40a5d":"markdown","c23457f8":"markdown","993b74ce":"markdown","c9c7fa17":"markdown"},"source":{"9ac955cc":"import cv2\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize\nfrom IPython.display import clear_output\nfrom matplotlib.pyplot import imshow\nimport pandas as pd\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.metrics import *\nfrom sklearn.model_selection import train_test_split\nimport tensorflow.keras as keras\nimport tensorflow as tf\nfrom tensorflow.keras import backend as k\nimport datetime","86a046b7":"train_df = pd.read_csv(\"..\/input\/ucf101\/UCF101TrainTestSplits-RecognitionTask\/ucfTrainTestlist\/trainlist01.txt\",sep = \" \",header = None,names = ['path','class'])","6e4c6c1e":"content = []\ni = 0\nn = 50\nwhile len(content) < 10 and i < 102:\n    if len(train_df[train_df['class']==i]) >= n:\n        df_temp = train_df[train_df['class']==i].iloc[:n]\n        path = df_temp['path']\n        content.append(path)\n        \n    i += 1\n    \n\ncontent = np.array(content)","dd59d6ad":"content = content.reshape(500,)\ny = np.array([i\/\/50 for i in range(0,500)])","5813e803":"def read_frames(root_folder,arr,each_nth=10):\n    videos=[]\n    for j  in range(len(arr)):\n        clear_output()\n        print(np.round(100*j\/len(arr),3))\n            \n        vcap=cv2.VideoCapture(root_folder+arr[j])\n        success=True\n  \n        frames=[]\n        cnt=0\n        while success:\n            try:\n              success,image=vcap.read()\n              cnt+=1\n              if cnt%each_nth==0:\n                image=resize(image,(128,192))\n                frames.append(image)\n            except Exception as e:\n                print(e)\n        videos.append(frames)\n    \n    return videos","e7791aa4":"def select_frames(frames_arr , n=10):\n    videos=[]\n    for i in range(len(frames_arr)):\n        frames=[]\n        for t in np.linspace(0, len(frames_arr[i])-1, num=n):\n            frames.append(frames_arr[i][int(t)])\n        videos.append(frames)\n        \n    videos = np.array(videos)\n    print(videos.shape)\n    return videos","156e67f6":"X_frames = read_frames(\"..\/input\/ucf101\/UCF101\/UCF-101\/\",content)\nX_frames = select_frames(X_frames, 10)","0d08c310":"xtr, xte, ytr , yte = train_test_split(X_frames , y , shuffle=True,test_size = 0.2,random_state = 42)","cc0ca3a3":"ytr = to_categorical(ytr, 10)\nyte = to_categorical(yte,10)","469edb94":"fig = plt.figure(figsize=(32,8))\n\nrandom_video_index = np.random.randint(0,len(X_frames))\n\nfor i,image in enumerate(X_frames[random_video_index]):\n    ax = plt.subplot(2,5,i+1)\n    imshow(image)","bcd7cb57":"def list_to_stack(xs):\n  xs=tf.stack(xs, axis=1)\n  s = tf.shape(xs)\n \n  return xs","c5496c13":"ish=(10, 128, 192, 3)\n  \nxs=[]\n\n\ninp = Input(ish)\n\nfor slice_indx in range(0,10,1):\n  x=Lambda(lambda x: x[:, slice_indx])(inp)\n  x=BatchNormalization(momentum=0.8)(x)\n  x=Conv2D(filters=20, kernel_size=3, padding='same', activation='relu')(x)\n  x=BatchNormalization(momentum=0.8)(x)\n  x=MaxPooling2D(pool_size=2)(x)\n  \n  x=Conv2D(filters=30, kernel_size=3, padding='same', activation='relu')(x)\n  x=BatchNormalization(momentum=0.8)(x)\n  x=MaxPooling2D(pool_size=2)(x)\n  x=Conv2D(filters=30, kernel_size=3, padding='same', activation='relu')(x)\n    \n  xs.append(x)\n  \n\nt=Lambda(list_to_stack)(xs)\nt=Conv3D(50,3,padding='same')(t)\nt=BatchNormalization(momentum=0.8)(t)\ntarget_shape=(10,32*48*50)\nt=Reshape(target_shape)(t)\nt=GRU(25, return_sequences=True)(t)\nt=GRU(50, return_sequences=False,dropout=0.5)(t)\n\nt=Dense(100,'relu')(t)\nout=Dense(10, activation='softmax')(t)\n\nmodel = Model(inputs=inp, outputs=out)\nopt = tf.keras.optimizers.SGD(lr=0.0087)\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\nmodel.summary()","679bb48c":"from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)","bb4e147e":"history = model.fit(xtr, ytr, epochs=5, batch_size=10,validation_data=(xte, yte),shuffle  = True)","b3449d88":"fig = plt.figure(figsize=(14,6))\n\nax = plt.subplot(1,2,1)\nplt.plot(history.history['loss'], label='Training loss')\nax.legend(loc=\"upper left\")\nplt.ylabel('Loss')\nplt.xlabel('No. epoch')\nax = plt.subplot(1,2,2)\nplt.plot(history.history['val_loss'], label='Vallidation loss')\nax.legend(loc=\"upper left\")\nfig.suptitle('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('No. epoch')\nplt.show()","cbc9678f":"fig = plt.figure(figsize=(14,6))\n\nax = plt.subplot(1,2,1)\nplt.plot(history.history['accuracy'], label='Training accuracy')\nax.legend(loc=\"upper left\")\nplt.ylabel('Accuracy')\nplt.xlabel('No. epoch')\nax = plt.subplot(1,2,2)\nplt.plot(history.history['val_accuracy'], label='Vallidation accuracy')\nax.legend(loc=\"upper left\")\nfig.suptitle('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('No. epoch')\nplt.show()","56857c5f":"classes = []\nfor i in range(len(content)):\n    class_name = content[i].split('\/')[0]\n    if class_name not in classes:\n        classes.append(class_name)\n        \nnum_classes = [i for i in range(10)]\nclass_map = dict(zip(num_classes,classes))","a46c8950":"random_videos = np.random.randint(0,len(xte),size=(10))\npredicted  = model.predict(xte[random_videos],batch_size = 10)\npredicted  = np.argmax(predicted,axis=1)","7da3bc86":"fig = plt.figure(figsize=(30,10))\n\nfor i,rand_indx in enumerate(random_videos):\n    ax = plt.subplot(2,5,i+1)\n    video = xte[rand_indx]\n    frame = video[np.random.randint(0,10)]\n    ax.set_title(class_map[predicted[i]])\n    imshow(frame)","547d7449":"<h1 style=\"text-align: center\"> Random Sample Visualization <\/h1>","cc0b49c3":"<p style=\"font-size: 120%\"> <b> Training model <\/b><\/p>","c8bb273f":"<h1 style=\"text-align: center\"> Model demonstration <\/h1> ","e706e5d5":"<h2> 10 classes - 500 samples <\/h2>","627cab3d":"<p style=\"font-size: 120%;\"> <b>Training model <\/b><\/p>","fa2d915c":"<h1 style=\"text-align: center\"> Model performance Visualization <h1> ","f91446b9":"<p style=\"font-size: 120%\"> We will take 10 random videos from test set and show predicted activity on them. For demonstration purposes we will show only 1 frame from each video <\/p>","ecc4c233":"<h1 style=\"text-align: center\"> Building model <\/h1>","69d40a5d":"<p style=\"font-size: 120%\"><b> Model design <\/b><\/p>","c23457f8":"not perfect as you can see.","993b74ce":"<p style=\"font-size: 110%;text-align: left;\"> Due to limited size of RAM, we would like to take less number of classes and samples for demonstration purposes. It might affect performance, but it is enough in this context.<\/p>","c9c7fa17":"<h1 style=\"text-align: center\"> Reading data <\/h1>"}}