{"cell_type":{"fd769eb2":"code","1fc4066d":"code","c91e0fb5":"code","a462e40a":"code","8ee23256":"code","848d6792":"code","e3e5d008":"code","62aabd93":"code","98c9500a":"code","d3900f98":"code","994a82b7":"code","62d3e0aa":"code","10b350f7":"code","4bd411e6":"code","2909a4a2":"code","335a0ff7":"code","f46dad9f":"code","001eeb10":"code","86b613ca":"code","98812804":"code","284dd1cb":"code","6d4cb413":"code","659e5f51":"code","fe272f74":"code","7817bee9":"code","dcbbbbd8":"code","15030223":"code","2f96d8fc":"code","0d75e325":"code","ca1ccaec":"code","69c57c49":"code","caaaa13a":"code","7b7c78c3":"code","4056d684":"code","6724f883":"code","f22befe9":"code","4152af50":"code","490a267d":"code","d2bde28f":"code","769afb7a":"code","705f6692":"code","7ec38c4f":"code","197a0a8e":"code","e23602f0":"code","f0bb8888":"code","fc458633":"code","43a12de8":"code","6b913a63":"code","217a6eb5":"code","732c43cd":"code","c55573c6":"code","1d2854f3":"code","6483bb4e":"code","ed0e912e":"code","4f6b885e":"code","570e4308":"code","48eb69f6":"code","0121e98e":"code","cc64398f":"code","59d746f6":"code","53c6e6e4":"code","3fbea25a":"code","d508f3d7":"code","aa10e947":"code","59f4a627":"code","95e46b5a":"code","e3fe993b":"code","754ad7b1":"code","8244de3c":"code","998066cc":"code","5a36fdce":"code","1694ba35":"code","89455d16":"code","c5ade8a6":"code","10e7c28f":"code","593dbf7e":"code","841a4634":"code","d14355d3":"markdown","02389b92":"markdown","86c059d9":"markdown","f1d48c47":"markdown","5272b640":"markdown","f860cc74":"markdown","cef20191":"markdown","9e21c88f":"markdown","d844731a":"markdown","34a01994":"markdown","321686aa":"markdown","a8b0e7a8":"markdown","1912c106":"markdown","88f551fd":"markdown"},"source":{"fd769eb2":"import pandas as pd\nimport numpy as np","1fc4066d":"train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')","c91e0fb5":"train.head()","a462e40a":"test.head()","8ee23256":"train_title = train.copy()\ntest_title = test.copy()","848d6792":"# Extrai apenas os pronomes e coloquei em uma nova coluna chamada Title.\ntrain_title['Title'] = train_title['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_title['Title'] = test_title['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","e3e5d008":"train_title.head()","62aabd93":"train_title['Title'].value_counts()","98c9500a":"title_mapped = {'Mr': 0, 'Miss': 1, 'Mrs': 2, \n                'Master': 3, 'Dr': 3, 'Rev': 3, 'Mlle': 3, 'Ms': 3, 'Capt': 3, \n                'Col': 3, 'Countess': 3, 'Jonkheer': 3, 'Sir': 3, 'Mme': 3}","d3900f98":"train_title['Title'] = train_title['Title'].map(title_mapped)\ntrain_title['Title'].unique()","994a82b7":"test_title['Title'] = test_title['Title'].map(title_mapped)\ntest_title['Title'].unique()","62d3e0aa":"# Retirei os valores NAN do dataframe de testes\ntest_title.update(test_title['Title'].fillna(3))","10b350f7":"test_title['Title'].isnull().sum()","4bd411e6":"train_title.drop(['Name'], inplace=True, axis=1)\ntest_title.drop(['Name'], inplace=True, axis=1)","2909a4a2":"train_title.head()","335a0ff7":"train_ticket = train_title.copy()\ntest_ticket = test_title.copy()","f46dad9f":"train_ticket.groupby(['Ticket']).count().sort_values('Survived', ascending=False).head(20)","001eeb10":"# Na coluna Ticket tem um registro sem n\u00famero, substitu\u00ed por '0' para que eu pudesse tratar depois retirando\n# as palavras e deixando os n\u00fameros.\ntrain_ticket = train_ticket.replace('LINE', '0')\ntest_ticket = test_ticket.replace('LINE', '0')","86b613ca":"split_train_ticket = train_ticket['Ticket'].str.split()\nsplit_train_ticket = np.array(split_train_ticket.values)\n\nsplit_test_ticket = test_ticket['Ticket'].str.split()\nsplit_test_ticket = np.array(split_test_ticket.values)","98812804":"# Retiro as palavras e salvo apenas os n\u00fameros dos tickets\nnew_ticket_train = []\nfor i in split_train_ticket:\n    if np.shape(i) == (1,):\n        new_ticket_train.append(i[0])\n    else:\n        new_ticket_train.append(i[-1])\n\nnew_ticket_test = []\nfor i in split_test_ticket:\n    if np.shape(i) == (1,):\n        new_ticket_test.append(i[0])\n    else:\n        new_ticket_test.append(i[-1])","284dd1cb":"new_ticket_train[0:5]","6d4cb413":"new_ticket_test[0:5]","659e5f51":"new_ticket_train = np.array(new_ticket_train, dtype='int')\nnew_ticket_test = np.array(new_ticket_test, dtype='int')","fe272f74":"train_ticket['Ticket'] = new_ticket_train\n\ntest_ticket['Ticket'] = new_ticket_test","7817bee9":"train_ticket.head()","dcbbbbd8":"test_ticket.head()","15030223":"train_ticket.isnull().sum()","2f96d8fc":"test_ticket.isnull().sum()","0d75e325":"train_cabin = train_ticket.copy()\ntest_cabin = test_ticket.copy()","ca1ccaec":"train_cabin.drop(['Cabin'], axis=1, inplace=True)\ntest_cabin.drop(['Cabin'], axis=1, inplace=True)","69c57c49":"train_embarked_sex = train_cabin.copy()\ntest_embarked_sex = test_cabin.copy()","caaaa13a":"train_embarked_sex.groupby(['Survived', 'Embarked']).count()","7b7c78c3":"train_embarked_sex.head()","4056d684":"train_embarked_sex['Embarked'].unique()","6724f883":"train_embarked_sex.loc[train_embarked_sex['Embarked'].isnull(), 'Embarked'] = train_embarked_sex.mode()['Embarked'][0]","f22befe9":"train_embarked_sex.isnull().sum()","4152af50":"test_embarked_sex.isnull().sum()","490a267d":"train_embarked_sex = pd.get_dummies(train_embarked_sex)\ntest_embarked_sex = pd.get_dummies(test_embarked_sex)","d2bde28f":"train_embarked_sex.head()","769afb7a":"test_embarked_sex.head()","705f6692":"train_embarked_sex.corr()","7ec38c4f":"train_embarked_sex.drop(['Sex_male', 'Embarked_Q'], axis=1, inplace=True)\ntest_embarked_sex.drop(['Sex_male', 'Embarked_Q'], axis=1, inplace=True)","197a0a8e":"train_embarked_sex.head()","e23602f0":"test_embarked_sex.head()","f0bb8888":"train_age = train_embarked_sex.copy()\ntest_age = test_embarked_sex.copy()","fc458633":"train_age.loc[train_age['Age'].isnull(), 'Age'] = train_age.mean()['Age']\n\ntest_age.loc[test_age['Age'].isnull(), 'Age'] = test_age.mean()['Age']","43a12de8":"train_age.head()","6b913a63":"test_age.head()","217a6eb5":"train_age.corr()['Survived']","732c43cd":"train_mean_std_max = train_age.copy()\ntest_mean_std_max = test_age.copy()","c55573c6":"colunas = ['Pclass', 'Fare', 'Title', 'Sex_female', 'Embarked_C', 'Embarked_S']","1d2854f3":"train_mean_std_max['Mean'] = train_mean_std_max[colunas].mean(axis=1).values\ntrain_mean_std_max['Std'] = train_mean_std_max[colunas].std(axis=1).values\ntrain_mean_std_max['Max'] = train_mean_std_max[colunas].max(axis=1).values\n\ntest_mean_std_max['Mean'] = test_mean_std_max[colunas].mean(axis=1).values\ntest_mean_std_max['Std'] = test_mean_std_max[colunas].std(axis=1).values\ntest_mean_std_max['Max'] = test_mean_std_max[colunas].max(axis=1).values","6483bb4e":"train_mean_std_max.corr()","ed0e912e":"features = ['Pclass', 'Fare', 'Title', 'Sex_female', 'Embarked_C', 'Embarked_S', 'Mean', 'Std', 'Max']","4f6b885e":"X = train_mean_std_max[features]\nY = train_mean_std_max['Survived']","570e4308":"X","48eb69f6":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=1)","0121e98e":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","cc64398f":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc","59d746f6":"from sklearn.ensemble import RandomForestClassifier","53c6e6e4":"model_forest = RandomForestClassifier(random_state=1)\nmodel_forest.fit(x_train, y_train)","3fbea25a":"y_pred_train = model_forest.predict(x_train)","d508f3d7":"accuracy_score(y_train, y_pred_train)","aa10e947":"y_pred_train = model_forest.predict(x_test)\ny_pred_train_proba = model_forest.predict_proba(x_test)","59f4a627":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\n\n%matplotlib inline\nrcParams['figure.figsize'] = 8, 4","95e46b5a":"cm = confusion_matrix(y_test, y_pred_train)\n\nsns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\nplt.title('Matriz de Confus\u00e3o')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","e3fe993b":"accuracy_score(y_test, y_pred_train)","754ad7b1":"print(classification_report(y_test, y_pred_train))","8244de3c":"fp, tp, thresholds = roc_curve(y_test, y_pred_train_proba[:, 1])","998066cc":"plt.plot(fp, tp)\n\nplt.plot([0, 1], [0, 1], '--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.xlabel('Falso Positivo')\nplt.ylabel('Verdadeiro Positivo')\nplt.title('Curva ROC')","5a36fdce":"auc(fp, tp)","1694ba35":"model_forest.fit(X, Y)","89455d16":"end_test = test_mean_std_max[features]\ny_pred_test_proba = model_forest.predict_proba(end_test)","c5ade8a6":"y_pred_test_proba = y_pred_test_proba[:, 1]","10e7c28f":"passageiro_id = test['PassengerId']","593dbf7e":"resultado = pd.concat([passageiro_id, pd.DataFrame(y_pred_test_proba, columns=['Survived'])], axis=1)","841a4634":"resultado.to_csv('submission.csv', index=False)","d14355d3":"### Voc\u00ea \u00e9 capaz de prever quem vai sobreviver ao naufr\u00e1gio do Titanic?\n\n###### Este notebook cont\u00e9m a minha solu\u00e7\u00e3o para o desafio proposto pela [1\u00aa Competi\u00e7\u00e3o Data Train no Kaggle](https:\/\/www.kaggle.com\/c\/data-train-competicao-ml-1-titanic) onde consegui a 4\u00aa melhor posi\u00e7\u00e3o na competi\u00e7\u00e3o. O notebook pode ser encontrado tamb\u00e9m no meu [Github](https:\/\/github.com\/lucas-or-ramon\/Titanic_Data_Train) <br>\n\n#### Conhe\u00e7a a comunidade:\n##### A Data Train \u00e9 uma comunidade aberta voltada para a \u00e1rea de dados e intelig\u00eancia artificial que tem como principal objetivo fomentar e contribuir com a transforma\u00e7\u00e3o data-driven de pessoas, empresas e sociedade civil atrav\u00e9s de meetups, eventos, cursos e um bom bate-papo entre profissionais.\n\n##### Torne-se membro da comunidade atrav\u00e9s dos [canais](https:\/\/linktr.ee\/datatrain).\n\n##### [Aqui](https:\/\/github.com\/wandersondsm\/Competicao-ML-1---Data-Train) voc\u00ea encontra todo o material disponibilizado para a competi\u00e7\u00e3o.","02389b92":"##### Coluna Ticket\n* Na coluna `Ticket` tamb\u00e9m pode ser analisada, vejamos.","86c059d9":"# Random Forest","f1d48c47":"#### Cria\u00e7\u00e3o de features Mean, Std e Max","5272b640":"##### Dados\n* Farei o mesmo tratamento tanto para os dados de treino quanto para os dados de teste.","f860cc74":"# Resultado","cef20191":"##### Coluna Name\n* Ao analisar a coluna `Name` pude ver que em todos os registros h\u00e1 um forma de tratamento para cada passageiro.","9e21c88f":"##### Desempenho com Conjunto de Valida\u00e7\u00e3o","d844731a":"##### Split","34a01994":"#####  Colunas Embarked e Sex\n* Para as colunas `Embarked` e `Sex` fiz a transforma\u00e7\u00e3o `one-hot-encoding`.","321686aa":"##### Coluna Age\n* Os valores nulos da coluna `Age` foram substitu\u00eddos pela m\u00e9dia das idades.","a8b0e7a8":"##### Desempenho com Conjunto de Treino","1912c106":"##### Coluna Cabin\n* Para esta an\u00e1lise descartarei a coluna `Cabin`.","88f551fd":"* Depois fiz o `label encoding` ou `integer encoding`."}}