{"cell_type":{"ad779a58":"code","0bf07278":"code","48798625":"code","a1495931":"code","9e2eb415":"code","2b0de5f5":"code","41051f02":"code","e592074c":"code","af4cf6a9":"code","f10d3c2d":"code","6eb307ac":"code","4dd5e393":"code","ef01d5f6":"code","44fe354c":"code","812d9f26":"code","fd5548a7":"code","b5ddc04a":"code","9a86a7a8":"code","e242cb5f":"code","3688ab78":"code","fb16900f":"code","ec55b609":"code","f966bd88":"code","ddbb9b4f":"code","b719fe25":"code","512ccc16":"code","ede15231":"code","9d08d099":"code","a7ec8c30":"code","1a6fc508":"code","4ea40483":"code","c688d9b5":"code","d6cd1fa1":"code","b974e32c":"code","e1a5bbe5":"code","8fbd9a14":"markdown","f4c077aa":"markdown","ef968b54":"markdown"},"source":{"ad779a58":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0bf07278":"import matplotlib.pyplot as plt\nplt.style.use('bmh')\nplt.rcParams['figure.figsize'] = (10, 10)\ntitle_config = {'fontsize': 20, 'y': 1.05}\ntrain = pd.read_csv('..\/input\/train.csv')\ntrain.head()","48798625":"X = train.iloc[:, 2:].values.astype('float64')\nY = train['target'].values\n\ntrain['target'].unique()","a1495931":"#pd.DataFrame(X[Y == 0]).plot.kde(ind=100, legend=False, figsize=(10, 10))\n#plt.title('Likelihood KDE Plots for the Negative Class (y = 0)', fontsize=20, y=1.05);","9e2eb415":"#pd.DataFrame(X[Y == 1]).plot.kde(ind=100, legend=False, figsize=(10, 10))\n#plt.title('Likelihood KDE Plots for the Positive Class (y = 1)', fontsize=20, y=1.05);","2b0de5f5":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nscaled = pd.DataFrame(scaler.transform(X))\nprint(\"Done\")","41051f02":"#scaled[Y == 0].plot.kde(ind=100, legend=False, figsize=(10, 10))\n#plt.title('Likelihood KDE Plots for the Negative Class (y = 0) after Standardization', fontsize=20, y=1.05);","e592074c":"#scaled[Y == 1].plot.kde(ind=100, legend=False, figsize=(10, 10))\n#plt.title('Likelihood KDE Plots for the Positive Class (y = 1) after Standardization', fontsize=20, y=1.05);","af4cf6a9":"from sklearn.preprocessing import QuantileTransformer\nquantileScaler = QuantileTransformer(output_distribution='normal')\nquantileScaler.fit(X)\ntransformed = pd.DataFrame(quantileScaler.transform(X))\nprint('done')","f10d3c2d":"#transformed[Y == 0].plot.kde(ind=100, legend=False, figsize=(10, 10))\n#plt.title('Likelihood KDE Plots for the Negative Class (y = 0) after Quantile Transformation', fontsize=20, y=1.05);","6eb307ac":"#transformed[Y == 1].plot.kde(ind=100, legend=False, figsize=(10, 10));\n#plt.title('Likelihood KDE Plots for the Positive Class (y = 1) after Quantile Transformation', fontsize=20, y=1.05);","4dd5e393":"#plt.figure(figsize=(10, 10))\n#plt.imshow(transformed.corr())\n#plt.colorbar()\n#plt.title('Correlation Matrix Plot of the Features', fontsize=20, y=1.05);","ef01d5f6":"def GetModel(init_mode='uniform'):\n    from keras.models import Sequential\n    from keras.layers import Dense, Activation, Dropout\n    from keras.callbacks import EarlyStopping\n    from keras import regularizers\n    model = Sequential()\n    model.add(Dense(128,activation='relu', kernel_initializer=init_mode,input_dim=200))\n    model.add(Dropout(0.5))\n    model.add(Dense(64,activation='relu',kernel_initializer=init_mode))\n    model.add(Dropout(0.5))\n    model.add(Dense(32,activation='relu',kernel_initializer=init_mode))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid',kernel_initializer=init_mode))\n\n    # For a binary classification problem\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    print(model.summary())\n    return model\n\nmodel = GetModel()","44fe354c":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nX = transformed.values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n\nclassifier = KerasClassifier(build_fn=GetModel, verbose=1)\nbatch_size = [128]\nepochs = [15]\n\n#param_grid = dict(batch_size=batch_size, epochs=epochs)\n\ninit_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\nparam_grid = dict()\nparam_grid['init_mode'] =init_mode\nparam_grid['batch_size'] =batch_size\nparam_grid['epochs'] =epochs\n\nscore = 'precision'\ngrid = GridSearchCV(estimator=classifier, param_grid=param_grid, n_jobs=-1,cv=5,scoring='%s_macro' % score)\nprint('start')\ngrid_result = grid.fit(X_test, Y_test)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","812d9f26":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.callbacks import EarlyStopping\nX = transformed.values\n\nmodel = GetModel('he_normal')\n\nbatch_size = 128\nfolds = StratifiedKFold(n_splits=10, shuffle=False, random_state=44000)\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X, Y)):\n    print(\"Fold {}\".format(fold_))\n    X_fold_train, X_fold_test = X[trn_idx], X[val_idx]\n    Y_fold_train, Y_fold_test = Y[trn_idx], Y[val_idx]\n    model.fit(X_fold_train, Y_fold_train, epochs = 15, batch_size=batch_size, verbose = 2,validation_data=(X_fold_test, Y_fold_test), callbacks = [EarlyStopping(monitor='val_acc', patience=4)])\n    \n\nscore,acc = model.evaluate(X, Y, verbose = 1, batch_size = batch_size)\nprint(\"loss: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))\n\nfrom sklearn.metrics import roc_curve, auc\nY_test_pridict = model.predict(X,batch_size=1,verbose = 1)\nfpr, tpr, thr = roc_curve(Y, Y_test_pridict)\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Plot', **title_config)\nauc(fpr, tpr)","fd5548a7":"#print(len(XTemp))\n#XTemp = X\n#X = transformed.values\n#X = scaled.values\n#print('done')","b5ddc04a":"#print(X[0])\n#print(transformed[0])\n#print(X.shape)\n#print(transformed.shape)\n#print(type(X))\n#print(type(transformed))\n#print('done')","9a86a7a8":"#from sklearn.model_selection import train_test_split\n","e242cb5f":"#X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)","3688ab78":"#print(X_train.shape,Y_train.shape)\n#print(X_test.shape,Y_test.shape)\n#print(type(X_train))","fb16900f":"\n#validation_size = int(len(X_test) * 0.3)\n#print(validation_size)\n\n\n#X_validate = X_test[-validation_size:]\n#Y_validate = [Y_test[-validation_size:]]\n#X_test = X_test[:-validation_size]\n#Y_test = [Y_test[:-validation_size]]","ec55b609":"#print(len(Y_test[0]))","f966bd88":"#print(len(X_test))\n#batch_size = 128\n#model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1,validation_data=(X_validate, Y_validate), callbacks = [EarlyStopping(monitor='val_acc', patience=3)])","ddbb9b4f":"#score,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\n#print(\"loss: %.2f\" % (score))\n#print(\"acc: %.2f\" % (acc))","b719fe25":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    from sklearn.utils.multiclass import unique_labels\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","512ccc16":"'''from sklearn.metrics import confusion_matrix\nY_test_pridict = model.predict(X_test,batch_size=1,verbose = 1)\nprint(Y_test_pridict[0])\n#Y_test_pridict = np.argmax(Y_test_pridict,axis=1)\nprint(Y_test_pridict[0])\n#cm = confusion_matrix(Y_test[0], Y_test_pridict)\nplot_confusion_matrix(Y_test[0], Y_test_pridict,np.array(['1','0']))'''","ede15231":"'''from sklearn.metrics import roc_curve, auc\nY_test_pridict = model.predict(X,batch_size=1,verbose = 1)\nfpr, tpr, thr = roc_curve(Y, Y_test_pridict)\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Plot', **title_config)\nauc(fpr, tpr)'''","9d08d099":"testData = pd.read_csv(\"..\/input\/test.csv\")\ntestData.head()","a7ec8c30":"#print(testData.head())\nY_Id_code = testData['ID_code'].values\ntestData = testData.iloc[:,1:].values.astype('float64')\ntestData.shape","1a6fc508":"#print(type(testData))\ntData = pd.DataFrame(quantileScaler.transform(testData))\n#tData = pd.DataFrame(scaler.transform(testData))\ntData.head()","4ea40483":"tData = tData.values\nprint(len(tData))\nresult = model.predict(tData,batch_size=1,verbose = 1)\n","c688d9b5":"#print(len(result))","d6cd1fa1":"print(Y_Id_code.shape)\nprint(result.shape)\ndfResult = pd.DataFrame({\"ID_code\": Y_Id_code[:], \"target\": result[:,0]})\ndfResult.head()\n    ","b974e32c":"dfResult.to_csv(\"submission10.csv\", index=False)","e1a5bbe5":"print(os.listdir())\n#!pip install kaggle\n#!kaggle competitions submit -c santander-customer-transaction-prediction -f submission.csv -m \"Message\"","8fbd9a14":"**Predicting Result**","f4c077aa":"**Grid Search Preparation**","ef968b54":"**Confutsion Metrix and ROC chart**"}}