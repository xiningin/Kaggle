{"cell_type":{"c237d99a":"code","6a68602d":"code","6dd5fe48":"code","4e337afc":"code","11901bab":"code","9b711bcc":"code","1cb5fbc1":"code","d488c7e0":"code","f26ddeef":"code","66863ded":"code","020fad23":"code","d8656b39":"code","8aab8a4d":"code","e6a156b7":"code","f887ff25":"code","44f2c5cb":"code","04a30102":"code","fb478a93":"code","3888a2ba":"code","5d30b69a":"code","ffe12cfb":"code","5e9caa55":"code","d8b255fd":"code","c9f8d2e8":"code","7bf5139b":"code","5f84c594":"code","cb8ea64e":"code","10b72623":"code","a8ae7931":"code","2b645afa":"code","c5cce742":"code","5ba0b0c8":"code","334f94b1":"code","72ebed40":"code","40244d05":"code","cdef9ce4":"code","021dc0bd":"code","c38f4a4e":"code","49b62b37":"code","02f47293":"code","f70a4f03":"code","aa66136d":"code","dc29df03":"code","e7da8ec8":"code","08c44e8d":"code","67a4ad3d":"code","7acbc45d":"code","de503691":"code","a1b2f5b3":"code","df21a85e":"code","9c08eb44":"code","370cd11b":"code","c40a8ce2":"code","0ec2405d":"code","8089cb13":"code","f05e7da1":"code","c05bdd52":"code","bd365b8b":"code","66fe4efa":"code","94bb2069":"code","3a341d7b":"code","c4d3cf8c":"code","059ece1e":"code","772a3ac4":"code","97d39441":"code","b16b8389":"code","31bf9343":"code","ad55e18c":"code","d0c05b36":"code","c6302aed":"code","34d44447":"code","f988341f":"code","55e2b469":"code","a82f6ec7":"code","60979673":"code","78668068":"code","2008a21b":"code","8bc098d9":"code","15a4bb74":"code","765cfb79":"code","6c103ae2":"code","2f0548b9":"code","957cef03":"code","1bc71359":"code","f9470a33":"code","2e89bf8a":"code","f7bec56f":"code","55a6f6de":"code","9d5b5b69":"markdown","331b8526":"markdown","e98e5232":"markdown","b33389dd":"markdown","cf6d29bf":"markdown","75bf5d57":"markdown","c1398509":"markdown","1530a524":"markdown","521dbb45":"markdown","201ead46":"markdown","4ac051b2":"markdown","6ed4143c":"markdown","6b83312b":"markdown","b674dcdb":"markdown","52fbcefc":"markdown","6a201484":"markdown","eedb698c":"markdown","e49e51f1":"markdown","6224ec58":"markdown","15019468":"markdown","567477d8":"markdown","02ffbd2e":"markdown","9252ec19":"markdown","860ee0e2":"markdown","2aa65aa6":"markdown","e9f359aa":"markdown","8e2102de":"markdown","c549bc02":"markdown","9e13dfb0":"markdown","07499aa2":"markdown","2dc43960":"markdown","48d29f05":"markdown","90f5caa4":"markdown","8abf01b8":"markdown","2f48e275":"markdown","c7e44e2b":"markdown"},"source":{"c237d99a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, recall_score, roc_auc_score, roc_curve","6a68602d":"os.listdir()","6dd5fe48":"path = r'..\/input\/amazon-fine-food-reviews\/Reviews.csv'\ndata = pd.read_csv(path, sep=',')","4e337afc":"data.head()","11901bab":"data.describe()","9b711bcc":"data.info()","1cb5fbc1":"data['Score'].value_counts().plot(kind='bar')\nplt.xlabel(\"Rating\")\nplt.ylabel(\"People Count\")\nplt.show()","d488c7e0":"filtered_data = data[data['Score'] != 3]\nfiltered_data.loc[data.Score >3,'Score'] = 1    # Positive review\nfiltered_data.loc[data.Score <3,'Score'] = 0    # Negative review","f26ddeef":"filtered_data['Score'].value_counts().plot(kind='bar')\nplt.xlabel(\"Rating\")\nplt.ylabel(\"People Count\")\nplt.show()","66863ded":"filtered_data.shape","020fad23":"# filtered_data.duplicated(subset=[\"UserId\",\"ProfileName\",\"Time\",\"Summary\",\"Text\"]).value_counts()\n# filtered_data.duplicated(subset=[\"Time\",\"Summary\",\"Text\"]).value_counts()\nfiltered_data.duplicated(subset=[\"UserId\", \"Summary\", \"Text\", \"Time\"]).value_counts()","d8656b39":"# Sort according to ProductId\nsorted_data = filtered_data.sort_values(\n    by=[\"ProductId\"], axis=0, ascending=True, inplace=False, kind='mergesort', na_position='last'\n)\n\n# drop duplicated data\nfinal = sorted_data.drop_duplicates(subset={\"UserId\", \"Summary\", \"Text\", \"Time\"}, keep='first', inplace=False)\nfinal.shape","8aab8a4d":"final = final[final.HelpfulnessNumerator <= final.HelpfulnessDenominator]\nfinal.shape","e6a156b7":"# drop the columns\n# final.drop(['Id', 'UserId', 'ProductId', 'ProfileName', 'Summary'], axis=1, inplace=True)\nfinal = final[['Text','Score','Time']]","f887ff25":"final.head(5)","44f2c5cb":"final.isna().sum()","04a30102":"stopwords = nltk.corpus.stopwords.words('english')\nprint(stopwords)\nlen(stopwords)","fb478a93":"stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \\\n             \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \\\n             'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\\\n             'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', \\\n             'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', \\\n             'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n             'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', \\\n             'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \\\n             'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', \\\n             'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \\\n             'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", \\\n             'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \\\n             \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\",\n             'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\" \\\n            ]","3888a2ba":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub(r\"^cause\", \"because\", phrase)\n    \n    return phrase","5d30b69a":"def sentance_clean(sentance):\n    # change sentence to lower case\n    sentance = sentance.lower()\n    # removing URL from sentence\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    # removing HTML tags\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    # removing contraction of words from sentence   # call decontracted funtion for it\n    sentance = decontracted(sentance)\n    # removing digits\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    # removing special character\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n\n    return sentance","ffe12cfb":"# stemming and lemmatisation object\nstemmer = SnowballStemmer('english')\nwordnet_lemmatizer = WordNetLemmatizer()","5e9caa55":"# Performing the preprocessing steps on all messages\n\n# add stemming and lemmatisation in the preprocess function\ndef preprocess(document, stem=True):\n    preprocessed_reviews = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(document):\n        # call sentance_clean function to clean text\n        sentance = sentance_clean(sentance)\n        \n        # tokenize into words\n        words = word_tokenize(sentance)\n        # remove stop words\n        sentance = [word for word in words if word not in stopwords]\n        \n        if stem:\n            # stemming word\n            sentance = [stemmer.stem(word) for word in sentance]\n        else:\n            # stemming word\n            sentance = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in sentance]\n\n        # join words to make sentence\n        sentance = \" \".join(sentance)\n\n        preprocessed_reviews.append(sentance.strip())\n        \n    return preprocessed_reviews","d8b255fd":"final = final.sample(frac=0.80, replace=True, random_state=123)\nfinal.shape","c9f8d2e8":"# final = final.iloc[:15000]\ndocument = final['Text'].values","7bf5139b":"%%time\npreprocessed_reviews = preprocess(document, stem=True)","5f84c594":"print(\"Before preprocess\\n\", document[1])\nprint(\"***\"*40)\nprint(\"After preprocess\\n\", preprocessed_reviews[1])","cb8ea64e":"final['Text'] = preprocessed_reviews","10b72623":"final.head()","a8ae7931":"positive_reviews = final[final['Score'] == 1]['Text'].values\nnegative_reviews = final[final['Score'] == 0]['Text'].values\n\nprint(\"Postive reviews count:\",len(positive_reviews))\nprint(\"Negative reviews count:\",len(negative_reviews))","2b645afa":"%%time\npositive_reviews = \" \".join(word for word in positive_reviews)\nnegative_reviews = \" \".join(word for word in negative_reviews)\n\npositive_reviews_cloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(positive_reviews)\nnegative_reviews_cloud = WordCloud(stopwords=STOPWORDS, background_color=\"black\").generate(negative_reviews)","c5cce742":"def show_word_cloud(cloud, title):\n    plt.figure(figsize = (15, 10))\n    plt.imshow(cloud, interpolation='bilinear')\n    plt.title(title, fontsize=25)\n    plt.axis(\"off\")\n    plt.show()","5ba0b0c8":"show_word_cloud(positive_reviews_cloud, \"Common Words in Positive Reviews\")","334f94b1":"show_word_cloud(negative_reviews_cloud, \"Common Words in Negative Reviews\")","72ebed40":"X = final[['Text','Time']].sort_values('Time',axis=0).drop('Time',axis=1)\n\nY = final[['Score','Time']].sort_values('Time',axis=0).drop('Time',axis=1)","40244d05":"X_train, X_test, y_train, y_test = train_test_split(X, Y.Score, test_size=0.20, random_state=123)","cdef9ce4":"print(\"X_train size:\", len(X_train))\nprint(\"X_test size:\", len(X_test))","021dc0bd":"bow_vectorizer = CountVectorizer(min_df=40)","c38f4a4e":"# fit data\nX_train_bow = bow_vectorizer.fit_transform(X_train.values.astype('U').ravel())\n\n# scale data\nX_train_bow = StandardScaler(with_mean=False).fit_transform(X_train_bow)\n\nprint(X_train_bow.shape)","49b62b37":"# fit data\nX_test_bow = bow_vectorizer.transform(X_test.values.astype('U').ravel())\n\n# scale data\nX_test_bow = StandardScaler(with_mean=False).fit_transform(X_test_bow)\n\nprint(X_test_bow.shape)","02f47293":"tfidf_vectorizer = TfidfVectorizer(min_df=40)","f70a4f03":"# fit data\nX_train_tf = tfidf_vectorizer.fit_transform(X_train.values.astype('U').ravel())\n\n# scale data\nX_train_tf = StandardScaler(with_mean=False).fit_transform(X_train_tf)\n\nprint(X_train_tf.shape)","aa66136d":"# fit data\nX_test_tf = tfidf_vectorizer.transform(X_test.values.astype('U').ravel())\n\n# scale data\nX_test_tf = StandardScaler(with_mean=False).fit_transform(X_test_tf)\n\nprint(X_test_tf.shape)","dc29df03":"# defining a class object for model evaluation\n\nclass ModelEvaluation:\n\n    def __init__(self, model_instance, X_train, X_test, y_train, y_test):\n        self.X_train = X_train\n        self.X_test = X_test\n        self.y_train = y_train\n        self.y_test = y_test\n        \n        self.model_instance = model_instance\n        self.y_train_proba = self.model_instance.predict_proba(self.X_train)[:, 1]\n        self.y_test_proba = self.model_instance.predict_proba(self.X_test)[:, 1]\n        self.y_test_predicted = None\n    \n    def pred_test(self):\n        self.y_train_proba = self.model_instance.predict_proba(self.X_train)[:, 1]\n        self.y_test_proba = self.model_instance.predict_proba(self.X_test)[:, 1]\n    \n    def model_evaluation(self, threshold, params, roc=False):\n        '''\n        This method is for calculating the evaluation metrics.\n        '''\n        # params type {'Vectorizer': , 'Hypreparameter':, 'Algorithm':}\n        metric = {}\n        \n        metric['Algorithm'] = params['Algorithm']\n        metric['Vectorizer'] = params['Vectorizer']        \n        \n        # prediction  based on threshhold (probabilty)\n        self.y_test_predicted = pd.Series(self.y_test_proba).map(lambda x: 1 if x > threshold else 0)        \n        # function call to calculate specificity, sensitivity\n        accuracy, specificity, sensitivity = self.calcualte_sensi_speci(self.y_test, self.y_test_predicted)\n        \n        accuracy_test = round(accuracy_score(self.y_test, self.y_test_predicted), 2)\n        print(f\"Accuracy = {accuracy_test}\")\n        metric['Accuracy'] = accuracy_test\n        \n        auc_test = round(roc_auc_score(self.y_test, self.y_test_proba), 2)\n        print(f\"AUC = {auc_test}\")\n        metric['AUC'] = auc_test\n        \n        print(f\"Specificity = {specificity}\")\n        metric['Specificity'] = specificity\n        \n        print(f\"Sensitivity = {sensitivity}\\n\")\n        metric['Sensitivity'] = sensitivity\n        \n        metric['Hypreparameter'] = params['Hypreparameter']\n        # call function to draw roc\n        if roc:\n            plt.figure(figsize=(6, 6))\n            self.draw_roc()\n            plt.show()\n        # call function to plot confusion_matrix\n        self.plot_confusion_matrix()\n        \n        return metric\n        \n    def draw_roc(self):\n        '''\n        This method plots the ROC curve of the model for the test data\n        '''\n\n        fpr, tpr, thresholds = roc_curve(self.y_test, self.y_test_proba,\n                                         drop_intermediate = False )\n        auc_score = roc_auc_score(self.y_test, self.y_test_proba)\n        \n        # plt.subplot(1, 2, i)\n        plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n        plt.ylabel('True Positive Rate')\n        plt.title(f'Receiver operating characteristic for test set')\n        plt.legend(loc=\"lower right\")\n\n\n    @staticmethod\n    def calcualte_sensi_speci(y, y_predicted):\n        '''\n        This calculates the accuracy, specificity and sensitivity\n        '''\n        confusion = confusion_matrix(y, y_predicted)\n        TP = confusion[1,1] # true positive \n        TN = confusion[0,0] # true negatives\n        FP = confusion[0,1] # false positives\n        FN = confusion[1,0] # false negatives\n        \n        sensitivity = round(TP \/ float(TP+FN), 2)\n        specificity = round(TN \/ float(TN+FP), 2)\n        accuracy = round((TP+TN)\/(TP+TN+FP+FN), 2)\n        \n        return accuracy, specificity, sensitivity\n    \n    def senisitivity_specificity_report(self):\n\n        '''\n        This method calculates accuracy sensitivity and specificity for various probability cutoffs.\n        '''\n\n        cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\n        y_train_pred_final = pd.DataFrame({'Score':self.y_train, 'Score_Prob':self.y_train_proba})\n\n        num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n\n        for i in num:\n            y_train_pred_final[i] = y_train_pred_final['Score_Prob'].map(lambda x: 1 if x > i else 0)\n\n        for i in num:\n            accuracy, speci, sensi = self.calcualte_sensi_speci(self.y_train, y_train_pred_final[i])\n            cutoff_df.loc[i] = [i, accuracy, sensi, speci]\n\n        print(cutoff_df)\n        \n        return cutoff_df\n\n    @staticmethod\n    def sensi_speci_trade_off(df, x, y):\n        '''\n        This method plots accuracy sensitivity and specificity for various probabilities.\n        '''\n        \n        df.plot.line(x='prob', y=['accuracy','sensi','speci'], figsize=[8, 6])\n        plt.vlines(x=x, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n        plt.hlines(y=y, xmax=0.99, xmin=0, colors=\"g\", linestyles=\"--\")\n        plt.show()\n\n        print(f'\\nOptimal Threshold probability = {x}')\n        \n        return x\n\n    def plot_confusion_matrix(self):\n        '''\n        This method plots confusion matrix.\n        '''\n\n        y_actual = self.y_test\n        y_pred = self.y_test_predicted\n        accuracy = accuracy_score(y_actual, y_pred)\n        print(\"Accuracy\", \"{:2.3}\".format(accuracy))\n\n        conf_matrix = confusion_matrix(y_actual, y_pred)\n\n        fig, ax = plt.subplots(figsize=(4, 4))\n        ax.matshow(conf_matrix, cmap=plt.cm.Reds, alpha=0.3)\n        for i in range(conf_matrix.shape[0]):\n            for j in range(conf_matrix.shape[1]):\n                ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center')\n\n        plt.xlabel('Predictions')\n        plt.ylabel('Actuals')\n        plt.title('Confusion Matrix', fontsize=12)\n        plt.show()\n\n    @classmethod\n    def metrics_table(self, metrics_dict):\n        '''\n        Metrics evolution table.\n        '''   \n        df = []\n        for key, value in metrics_dict.items():\n            df.append(metrics_dict[key])\n        df = pd.DataFrame(df)\n        df.set_index('Algorithm', inplace=True)\n        return df\n    ","e7da8ec8":"def lambda_LR1(dict_params):\n    # GridSearchCV\n    model_lr_final = GridSearchCV(estimator=dict_params['model'], param_grid=dict_params['params'],\n                                  cv = 5, scoring='recall', n_jobs = -1, verbose=1)\n    # fit cv\n    model_lr_final = model_lr_final.fit(dict_params['X_train'], dict_params['y_train'])\n    # best_estimator\n    best_estimator = model_lr_final.best_estimator_\n    best_params = model_lr_final.best_params_\n    print(\"Best parameters\", best_params)\n    \n    return best_estimator, best_params","08c44e8d":"#  logistic regression object\nlr_model = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=123)\n\n# parameters for cv\nparams = {'penalty': ['l1', 'l2'], 'C': [0.0001, 0.001, 0.01, 0.1, 1]}\ndict_params = {'model':lr_model, 'params': params, 'X_train':X_train_tf, 'y_train':y_train}","67a4ad3d":"%%time\nlr_model_tf, best_params = lambda_LR1(dict_params)","7acbc45d":"# instantiating the class model evaluation\nlr_eva_tfidf = ModelEvaluation(lr_model_tf, X_train_tf, X_test_tf, y_train, y_test)","de503691":"logit_cut_off = lr_eva_tfidf.senisitivity_specificity_report()","a1b2f5b3":"rf_threshold = lr_eva_tfidf.sensi_speci_trade_off(logit_cut_off, 0.5, 0.91)","df21a85e":"# test data\n# calculating the evaluation metrics by calling the method\nLR_TF_IDF = {'Vectorizer': 'TF-IDF', 'Hypreparameter':[best_params], 'Algorithm': 'Logistic_Regression_1'}\nLR_TF_IDF = lr_eva_tfidf.model_evaluation(rf_threshold, LR_TF_IDF, roc=True)","9c08eb44":"metrics_dict = {}","370cd11b":"metrics_dict['Logistic_Regression_TF_IDF'] = LR_TF_IDF","c40a8ce2":"#  logistic regression object\nlr_model = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=123)\n\n# parameters for cv\nparams = {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1]}\ndict_params = {'model':lr_model, 'params': params, 'X_train':X_train_bow, 'y_train':y_train}","0ec2405d":"%%time\nlr_model_bow, best_params = lambda_LR1(dict_params)","8089cb13":"%%time\n# instantiating the class model evaluation\nlr_eva_bow = ModelEvaluation(lr_model_bow, X_train_bow, X_test_bow, y_train, y_test)","f05e7da1":"logit_cut_off = lr_eva_bow.senisitivity_specificity_report()","c05bdd52":"rf_threshold = lr_eva_bow.sensi_speci_trade_off(logit_cut_off, 0.47, 0.92)","bd365b8b":"# test data\n# calculating the evaluation metrics by calling the method\nLR_TF_BOW = {'Vectorizer': 'BOW', 'Hypreparameter':[best_params], 'Algorithm': 'Logistic_Regression_2'}\nLR_TF_BOW = lr_eva_bow.model_evaluation(rf_threshold, LR_TF_BOW, roc=True)\nmetrics_dict['Logistic_Regression_BOW'] = LR_TF_BOW","66fe4efa":"#  naive_bayes object\nnv_model = MultinomialNB()\nparams = {'alpha': (0.00005, 0.00001, 0.0001, 0.001, 0.01, 0.1)}\ndict_params = {'model':nv_model, 'params': params, 'X_train':X_train_tf, 'y_train':y_train}","94bb2069":"nb_model_tf, best_params = lambda_LR1(dict_params)","3a341d7b":"# instantiating the class model evaluation\nnb_eva_tfidf = ModelEvaluation(nb_model_tf, X_train_tf, X_test_tf, y_train, y_test)","c4d3cf8c":"logit_cut_off = nb_eva_tfidf.senisitivity_specificity_report()","059ece1e":"rf_threshold = nb_eva_tfidf.sensi_speci_trade_off(logit_cut_off, 0.40, 0.87)","772a3ac4":"# calculating the evaluation metrics by calling the method\nNB_TF_TFIDF = {'Vectorizer': 'TF-IDF', 'Hypreparameter':[best_params], 'Algorithm': 'Naive_Bayes_1'}\nNB_TF_TFIDF = nb_eva_tfidf.model_evaluation(rf_threshold, NB_TF_TFIDF, roc=True)","97d39441":"metrics_dict['Naive_Bayes_TF_IDF'] = NB_TF_TFIDF","b16b8389":"#  naive_bayes object\nnv_model = MultinomialNB()\n\nparams = {'alpha': (0.0001, 0.001, 0.01, 0.1, 1, 2, 5, 10, 50, 100)}\ndict_params = {'model':nv_model, 'params': params, 'X_train':X_train_bow, 'y_train':y_train}","31bf9343":"nb_model_bow, best_params = lambda_LR1(dict_params)","ad55e18c":"# instantiating the class model evaluation\nnb_eva_bow = ModelEvaluation(nb_model_bow, X_train_bow, X_test_bow, y_train, y_test)","d0c05b36":"logit_cut_off = nb_eva_bow.senisitivity_specificity_report()","c6302aed":"rf_threshold = nb_eva_bow.sensi_speci_trade_off(logit_cut_off, 0.9, 0.85)","34d44447":"# calculating the evaluation metrics by calling the method\nNB_TF_BOW = {'Vectorizer': 'BOW', 'Hypreparameter':[best_params], 'Algorithm': 'Naive_Bayes_2'}\nNB_TF_BOW = nb_eva_bow.model_evaluation(rf_threshold, NB_TF_BOW, roc=True)\nmetrics_dict['Naive_Bayes_BOW'] = NB_TF_BOW","f988341f":"from sklearn.ensemble import RandomForestClassifier","55e2b469":"# random forest object\nrf_model = RandomForestClassifier(class_weight='balanced', random_state=123)\nparams = {\n    'max_depth': [6,9,12],\n}\ndict_params = {'model':rf_model, 'params': params, 'X_train':X_train_tf, 'y_train':y_train}","a82f6ec7":"rf_model_tf, best_params = lambda_LR1(dict_params)","60979673":"# instantiating the class model evaluation\nrf_eva_tfidf = ModelEvaluation(rf_model_tf, X_train_tf, X_test_tf, y_train, y_test)","78668068":"logit_cut_off = rf_eva_tfidf.senisitivity_specificity_report()","2008a21b":"rf_threshold = rf_eva_tfidf.sensi_speci_trade_off(logit_cut_off, 0.49, 0.89)","8bc098d9":"# calculating the evaluation metrics by calling the method\nRF_TF_TFIDF = {'Vectorizer': 'TF-IDF', 'Hypreparameter':[best_params], 'Algorithm': 'Random_Forest_1'}\nRF_TF_TFIDF = rf_eva_tfidf.model_evaluation(rf_threshold, RF_TF_TFIDF, roc=True)","15a4bb74":"metrics_dict['Random_Forest_TF_IDF'] = RF_TF_TFIDF","765cfb79":"from sklearn.ensemble import RandomForestClassifier","6c103ae2":"# random forest object\nrf_model = RandomForestClassifier(class_weight='balanced', random_state=123)\nparams = {\n    'max_depth': [16, 20],\n    'n_estimators': [50, 200],\n}\ndict_params = {'model':rf_model, 'params': params, 'X_train':X_train_bow, 'y_train':y_train}","2f0548b9":"rf_model_bow, best_params = lambda_LR1(dict_params)","957cef03":"# instantiating the class model evaluation\nrf_eva_bow = ModelEvaluation(rf_model_bow, X_train_bow, X_test_bow, y_train, y_test)","1bc71359":"logit_cut_off = rf_eva_bow.senisitivity_specificity_report()","f9470a33":"rf_threshold = rf_eva_bow.sensi_speci_trade_off(logit_cut_off, 0.50, 0.85)","2e89bf8a":"# calculating the evaluation metrics by calling the method\nRF_TF_BOW = {'Vectorizer': 'BOW', 'Hypreparameter':[best_params], 'Algorithm': 'Random_Forest_2'}\nRF_TF_BOW = rf_eva_bow.model_evaluation(rf_threshold, RF_TF_BOW, roc=True)","f7bec56f":"metrics_dict['Random_Forest_BOW'] = RF_TF_BOW","55a6f6de":"metrics_table = ModelEvaluation.metrics_table(metrics_dict)\nmetrics_table","9d5b5b69":"As we can see that we have 160678 duplicate customers with same Summary and Text.\n\nLet's remove those customers. Sort the data according to ProductId and keep the first items review and delete the others.","331b8526":"### 5.5 Random Forest with TFIDF","e98e5232":"### 1.1 Filter Review as Positive and Negative","b33389dd":"### 2.3 Performing the preprocessing steps on all messages","cf6d29bf":"#### 3.1.1. Positive Word Cloud","75bf5d57":"### 1.2 Checking duplicates\nReviews might contain duplicate entries.  So, we need to remove the duplicate entries so that we get unbiased data for Analysis.\n\nCheck duplicate customers with same Summary and Text.","c1398509":"### 5.3 Naive Bayes with TFIDF","1530a524":"### 5.1 Logistic Regression with TF-IDF","521dbb45":"No null values are present","201ead46":"## Data Collection\n\n1. [Dataset is taken from Kaggle](https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews)\n\n2. This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n\nData includes:\n\n- Reviews from Oct 1999 - Oct 2012\n- 568,454 reviews\n- 256,059 users\n- 74,258 products\n- 260 users with > 50 reviews\n\nColumns:\n\n1. Id\n2. ProductId - Unique identifier for the product\n3. UserId - Unqiue identifier for the user\n4. ProfileName - Profile name of user\n5. HelpfulnessNumerator - Number of users who found the review helpful\n6. HelpfulnessDenominator - Number of users who indicated whether they found the review helpful or not\n7. Score - Rating between 1 and 5\n8. Time - Timestamp for the review\n9. Summary - Brief summary of the review\n10. Text - Text of the review\n","4ac051b2":"### 4.1 BAG OF WORDS Vectorization","6ed4143c":"## 2. Preprocessing Review Text","6b83312b":"#### 3.1.2. Negative Word Cloud","b674dcdb":"We can see that we have 5-star rating system. We can filter onlt positive and negitive reviews. \n\n- Review with score>3 : Positive ratings\n- Review with score<3 : Negative ratings\n- Review with score=3 : Neutral rating (we can skip those ratings)","52fbcefc":"## Conclusion:\n- Logistic regression with TF-IDF and BOW model are giving  more accurate result.\n- Also Random Forest with BOW model is giving better result as compare to Naive Bayes.\n","6a201484":"**For Train Data**","eedb698c":"`HelpfulnessNumerator` : Number of users who found the review helpful.\n\n`HelpfulnessDenominator` : Number of users who indicated whether they found the review helpful or not.\n\n**HelpfulnessNumerator should be less than HelpfulnessDenominator.**","e49e51f1":"### 3.1 Word Cloud for positive and negative review","6224ec58":"### 5.5 Random Forest with BOW","15019468":"## 3. Word Cloud","567477d8":"## Reading Data","02ffbd2e":"### 5.4 naive_bayes with BOW","9252ec19":"## 4. Vectorization","860ee0e2":"**For Test Data**","2aa65aa6":"**For Train Data**","e9f359aa":"**Remove `'no', 'nor', 'not'` from stopwords**","8e2102de":"### 2.1 Decontractions\n\nContractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe.","c549bc02":"## 5. Model","9e13dfb0":"### 5.2 Logistic Regression with BOW","07499aa2":"### 1.3 Checking null values.","2dc43960":"## Evolution Metrics","48d29f05":"## Creating a train and test set","90f5caa4":"### 2.2 Remove HTML tag, URL, digits, special character","8abf01b8":"**For Test Data**","2f48e275":"## 1. Data Cleaning","c7e44e2b":"### 4.2 TFIDF Vectorization"}}