{"cell_type":{"062cfd09":"code","f4661948":"code","8cea2b62":"code","638eff99":"code","152c5e4c":"code","e4293768":"code","085a548d":"code","253dda14":"code","f8896098":"code","4b5c08e3":"code","54e80747":"code","1c8881eb":"code","2483e836":"code","1ce4a7af":"code","7468dc7e":"code","4480e214":"code","2d4bca1a":"code","7e7927aa":"code","b4a6f3a5":"code","eb7f15bc":"code","4bee1601":"code","6bc1431b":"code","94f1ca2e":"code","ba7b7fbf":"code","4111ffca":"code","13269c0f":"code","c5736f4e":"code","d4466305":"code","7f3cd6b0":"code","50f87bb7":"code","ae2e9035":"code","48d854a1":"code","e23de1ac":"code","83034594":"code","8548573b":"code","34bde259":"code","b2d829eb":"code","bf97c031":"code","9cc89b72":"code","696a87e0":"code","4b75ca4c":"code","c8b1932d":"code","3e5f1074":"code","2ebf934c":"code","1c79cfd2":"code","f5998691":"code","3fe1fa6b":"code","6af83f7a":"code","e5cb046d":"code","99646155":"code","3082c4b9":"code","4df57f89":"code","b5d0621c":"code","ca257470":"code","525b7089":"code","fc4d58fc":"code","3ea28914":"code","95725b1b":"code","cf3444f3":"code","8838820f":"code","e563448b":"code","10043b73":"code","b6b75cc6":"code","631504c3":"code","3efea057":"code","a56f8701":"code","66548617":"code","80e5a8cf":"code","3e26e9c4":"code","51f3b815":"code","f754be1c":"code","12170b02":"code","1da0ab1d":"code","dab4d438":"code","2db5e93d":"code","18f76944":"code","9aaa3d15":"code","b76fe70f":"code","d14d5392":"code","5b574826":"code","3d457f4a":"code","a7b3c4d6":"code","6658aa0b":"code","d8b22942":"code","bbce5847":"code","52ade34c":"code","ab4faba8":"code","190b2b37":"code","d231b824":"code","a7dfad57":"code","b1b8731c":"code","a148ed33":"code","df2beccc":"code","40082bff":"code","3ac2f555":"markdown","6d2ec013":"markdown","db28954b":"markdown","4eedd62d":"markdown","c32e4938":"markdown","450ca74d":"markdown","93e01369":"markdown","ea6d8205":"markdown","0add82fb":"markdown","86f918d3":"markdown","d11da291":"markdown","c897939d":"markdown","ea1a2b56":"markdown","c0445772":"markdown","e0e885d4":"markdown","dcb9dd89":"markdown","1316e436":"markdown","a43c6705":"markdown","48e883a2":"markdown","c9cd3d9d":"markdown","320e8866":"markdown","0a323879":"markdown","1eea00d8":"markdown","84c0d009":"markdown","87e16ec4":"markdown","00aa1c2d":"markdown","ec4b13db":"markdown","617fb9df":"markdown","248d9aa4":"markdown","ba2ee2dc":"markdown","523bedd3":"markdown","c8364864":"markdown","c7a80f52":"markdown","786f436c":"markdown","bd277e12":"markdown","19b07a49":"markdown","1b050762":"markdown","2081d963":"markdown","19b6100c":"markdown","96f93b71":"markdown","556a6fcf":"markdown","7c4ed59d":"markdown"},"source":{"062cfd09":"import numpy as np\nimport pandas  as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","f4661948":"df=pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head(3)","8cea2b62":"df.shape","638eff99":"df.describe()","152c5e4c":"df.isnull().sum()","e4293768":"plt.figure(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","085a548d":"# dropping nulls in bmi from the dataset\ndf=df.dropna().reset_index(drop=True)","253dda14":"# sanity check\ndf.isnull().sum()","f8896098":"# dropping id column because it is unique for each row\ndf=df.drop(columns=['id'])","4b5c08e3":"plt.figure(figsize=(12,8))\ndf.boxplot()\nplt.show()","54e80747":"for i in df.select_dtypes(include=np.number).columns:\n    sns.boxplot(df[i])\n    plt.show()","1c8881eb":"# type conversion of hypertension, heart disease and stroke\ndf['hypertension']=df['hypertension'].astype(object)\ndf['heart_disease']=df['heart_disease'].astype(object)\ndf['stroke']=df['stroke'].astype(object)\n","2483e836":"# converting data into numerical and categorical\ndf_int=df.select_dtypes(include=np.number)\ndf_cat=df.select_dtypes(exclude=np.number)","1ce4a7af":"# sanity check \ndf_int.head()","7468dc7e":"# sanity check\ndf_cat.head()","4480e214":"# Countplot of our Categorical Variables\nfor i in df_cat:\n    sns.countplot(df[i])\n    plt.show()","2d4bca1a":"pd.crosstab(df['ever_married'],df['stroke']).plot(kind='bar',stacked=True)\nplt.show()","7e7927aa":"pd.crosstab(df['work_type'],df['stroke']).plot(kind='bar',stacked=True)\nplt.show()","b4a6f3a5":"plt.figure(figsize=(12,8))\nsns.kdeplot(df[df['stroke']==0]['age'],shade=True,label='no_stroke')\nsns.kdeplot(df[df['stroke']==1]['age'],shade=True,label='stroke')\nplt.xlabel('Age')\nplt.title('Stroke Density vs Age')\nplt.legend()\n\nplt.show()","eb7f15bc":"plt.figure(figsize=(12,8))\nsns.kdeplot(df[df['stroke']==0]['bmi'],shade=True,label='no_stroke')\nsns.kdeplot(df[df['stroke']==1]['bmi'],shade=True,label='stroke')\nplt.legend()\nplt.title('Stroke Density vs BMI ')\nplt.show()","4bee1601":"# Separating Dependent and Predictor variables\nX=pd.get_dummies(df,columns=df_cat.columns,drop_first=True).iloc[:,:-2]\ny=pd.to_numeric(df['stroke'])","6bc1431b":"# splitting into train and test sets into stratified sets\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,random_state=8)","94f1ca2e":"# sanity check\ny_train.value_counts()","ba7b7fbf":"# sanity check\ny_test.value_counts()","4111ffca":"# Applying SMOTE for treating imbalance in our data\nfrom imblearn.over_sampling import SMOTE\nsmt = SMOTE(random_state=8)\nX_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\nprint(y_train_sm.value_counts())","13269c0f":"from sklearn.linear_model import LogisticRegression","c5736f4e":"log_reg=LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train,y_train)\nprint('Train:',log_reg.score(X_train,y_train))\nprint('Test:',log_reg.score(X_test,y_test))\n\ny_pred_lr=log_reg.predict(X_test)","d4466305":"pd.DataFrame(y_pred_lr).value_counts()","7f3cd6b0":"from sklearn.metrics import classification_report,roc_auc_score,roc_curve\nprint(classification_report(y_test,y_pred_lr))","50f87bb7":"log_regsm=LogisticRegression(max_iter=1000)\nlog_regsm.fit(X_train_sm,y_train_sm)\nprint('Train:',log_regsm.score(X_train_sm,y_train_sm))\nprint('Test:',log_regsm.score(X_test,y_test))\n\ny_pred_lrsm=log_regsm.predict(X_test)","ae2e9035":"print(classification_report(y_test,y_pred_lrsm))","48d854a1":"from sklearn.tree import DecisionTreeClassifier\ndtc=DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\nprint('Train:',dtc.score(X_train,y_train))\nprint('Test:',dtc.score(X_test,y_test))\n\ny_pred_dt=dtc.predict(X_test)","e23de1ac":"print(classification_report(y_test,y_pred_dt))","83034594":"from sklearn.tree import DecisionTreeClassifier\ndtcsm=DecisionTreeClassifier()\ndtcsm.fit(X_train_sm,y_train_sm)\nprint('Train:',dtcsm .score(X_train_sm,y_train_sm))\nprint('Test:',dtcsm.score(X_test,y_test))\n\ny_pred_dtsm=dtcsm.predict(X_test)","8548573b":"print(classification_report(y_test,y_pred_dtsm))","34bde259":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()\nknn.fit(X_train,y_train)\nprint('Train:',knn.score(X_train,y_train))\nprint('Test:',knn.score(X_test,y_test))\n\ny_pred_knn=knn.predict(X_test)\n","b2d829eb":"print(classification_report(y_test,y_pred_knn))","bf97c031":"from sklearn.neighbors import KNeighborsClassifier\nknnsm=KNeighborsClassifier()\nknnsm.fit(X_train_sm,y_train_sm)\nprint('Train:',knnsm.score(X_train_sm,y_train_sm))\nprint('Test:',knnsm.score(X_test,y_test))\n\ny_pred_knnsm=knnsm.predict(X_test)","9cc89b72":"print(classification_report(y_test,y_pred_knnsm))","696a87e0":"from sklearn.naive_bayes import GaussianNB\nnaive=GaussianNB()\nnaive.fit(X_train,y_train)\nprint('Train:',naive.score(X_train,y_train))\nprint('Test:',naive.score(X_test,y_test))\n\ny_pred_gnb=naive.predict(X_test)","4b75ca4c":"print(classification_report(y_test,y_pred_gnb))","c8b1932d":"from sklearn.naive_bayes import GaussianNB\nnaivesm=GaussianNB()\nnaivesm.fit(X_train_sm,y_train_sm)\nprint('Train:',naivesm.score(X_train_sm,y_train_sm))\nprint('Test:',naivesm.score(X_test,y_test))\n\ny_pred_naivesm=naivesm.predict(X_test)","3e5f1074":"print(classification_report(y_test,y_pred_naivesm))","2ebf934c":"from sklearn.model_selection import cross_val_score\nscore=cross_val_score(LogisticRegression(max_iter=1000),X_train,y_train,cv=10,scoring='accuracy')\nprint(score)","1c79cfd2":"from sklearn.model_selection import cross_val_score\nscore=cross_val_score(dtc,X_train,y_train,cv=10,scoring='accuracy')\nprint(score)","f5998691":"from sklearn.metrics import roc_auc_score,roc_curve\n\ndef roc_curve1(model):\n    pred_proba=model.predict_proba(X_test)\n    fpr,tpr,th = roc_curve(y_test,pred_proba[:,1])\n    plt.plot(fpr,tpr)\n    plt.xlim([0.0,1.0])\n    plt.ylim([0.0,1.0])\n    plt.plot([0,1],[0,1],'r--')\n    plt.title('ROC curve for Classifier')\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, pred_proba[:,1]),4)))\n    plt.grid(True)\n    plt.show()","3fe1fa6b":"roc_curve1(log_reg)","6af83f7a":"#### ROC curve along with scores for comparing 2 algorithms\n## 1st classifier\n\ny_pred_proba= log_reg.predict_proba(X_test)[:,1]\n\ny_pred_probasm= log_regsm.predict_proba(X_test)[:,1]\n\nplt.figure(figsize = (8, 8))\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.plot([0, 1], [0, 1],'r--')\nplt.title('ROC-curves for Logistic Regression with and without SMOTE', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score for log_reg model:',round(roc_auc_score(y_test, y_pred_proba),4)))\n## 2nd classifier\nfpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred_probasm)\nplt.plot(fpr1, tpr1)\nplt.text(x = 0.02, y = 0.8, s = ('AUC Score for log_reg_sm model:',round(roc_auc_score(y_test, y_pred_probasm),4)))\nplt.grid(True)","e5cb046d":"from warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn.model_selection import validation_curve\n\nC_param_range = [0.001,0.01,0.1,1,10,100,1000]\n\nplt.figure(figsize=(15, 10))\n\n# Logistic Regression validation curve\ntrain_scores, test_scores = validation_curve(estimator=log_reg,X=X_train,y=y_train ,param_name='C',param_range=C_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nplt.subplot(2,2,1)\nplt.semilogx(C_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\n    \nplt.semilogx(C_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \n    \nplt.xlabel('C_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.5,1])","99646155":"dtc_param_range = np.arange(1,13)\n\nplt.figure(figsize=(15, 10))\n\n# Decision Tree validation curve\ntrain_scores, test_scores = validation_curve(estimator=dtc,X=X_train,y=y_train ,param_name='max_depth',param_range=dtc_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.semilogx(dtc_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(dtc_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)    \nplt.semilogx(dtc_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(dtc_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\nplt.xlabel('max_depth_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.5,1])","3082c4b9":"dtc_param_range = np.arange(1,25)\n\nplt.figure(figsize=(15, 10))\n\n# Decision Tree(SMOTE) validation curve\ntrain_scores, test_scores = validation_curve(estimator=dtcsm,X=X_train_sm,y=y_train_sm ,param_name='max_depth',param_range=dtc_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.semilogx(dtc_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(dtc_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)    \nplt.semilogx(dtc_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(dtc_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\nplt.xlabel('max_depth_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.5,1])","4df57f89":"from sklearn.model_selection import GridSearchCV\n\nparams={'criterion':['entropy', 'gini'],'max_depth': range(2, 10),'min_samples_split' : range(1,5)}\ndt=DecisionTreeClassifier()\ngrid=GridSearchCV(dt,params,cv=5)\ngrid.fit(X_train,y_train)\n\nprint('The best value of hyperparameters \"criterion\", \"max_depth\", and \"min_samples_split\"')\nprint(grid.best_params_)","b5d0621c":"dt_tuned=DecisionTreeClassifier(criterion='entropy',max_depth=2,min_samples_split=2)\ndt_tuned.fit(X_train,y_train)\n\ny_pred_dttuned= dt_tuned.predict(X_test)\n\ndt_tuned.score(X_train,y_train)","ca257470":"print(classification_report(y_test,y_pred_dttuned))","525b7089":"dt_tuned.score(X_test,y_test)","fc4d58fc":"from sklearn.model_selection import GridSearchCV\n\nParameter_Trials={'max_depth': [11,12,13,14,15,16]}\n\nGrid_Search = GridSearchCV(dtcsm, Parameter_Trials, cv=5, n_jobs=1)\nGridSearchResults=Grid_Search.fit(X_train_sm,y_train_sm)\n\nprint('The best value of hyperparameters \"max_depth\" :')\nprint(Grid_Search.best_params_)","3ea28914":"dtsm_tuned=DecisionTreeClassifier(criterion='entropy',max_depth=16)\ndtsm_tuned.fit(X_train_sm,y_train_sm)\n\ny_pred_dtsmtuned= dtsm_tuned.predict(X_test)\n\ndtsm_tuned.score(X_train_sm,y_train_sm)","95725b1b":"print(classification_report(y_test,y_pred_dtsmtuned))","cf3444f3":"# ROC-Curve for Decision Tree\nroc_curve1(dtc)","8838820f":"# ROC-Curve for tuned Decision Tree\nroc_curve1(dt_tuned)","e563448b":"y_pred_probad= dtc.predict_proba(X_test)[:,1]\n\ny_pred_probadsm= dtcsm.predict_proba(X_test)[:,1]\n\ny_pred_probad_tuned= dt_tuned.predict_proba(X_test)[:,1]\n\ny_pred_probadsm_tuned= dtsm_tuned.predict_proba(X_test)[:,1]\n\nplt.figure(figsize = (12, 8))\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_probad)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.plot([0, 1], [0, 1],'r--')\nplt.title('ROC curve for different Models', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score for DT model:',round(roc_auc_score(y_test, y_pred_probad),4)))\n## 2nd classifier\nfpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred_probadsm)\nplt.plot(fpr1, tpr1)\nplt.text(x = 0.02, y = 0.8, s = ('AUC Score for DT_SMOTE model:',round(roc_auc_score(y_test, y_pred_probadsm),4)))\nplt.grid(True)\n\n## 3rd classifier\nfpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred_probad_tuned)\nplt.plot(fpr2, tpr2)\nplt.text(x = 0.02, y = 0.7, s = ('AUC Score for tuned_DT model:',round(roc_auc_score(y_test, y_pred_probad_tuned),4)))\nplt.grid(True)\n\n## 4th classifier\nfpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred_probadsm_tuned)\nplt.plot(fpr3, tpr3)\nplt.text(x = 0.02, y = 0.6, s = ('AUC Score for tuned_DT_SMOTE model:',round(roc_auc_score(y_test, y_pred_probadsm_tuned),4)))\nplt.grid(True)","10043b73":"from sklearn.ensemble import RandomForestClassifier\n\nrf=RandomForestClassifier()\n\nrf.fit(X_train,y_train)\n\nrf.score(X_train,y_train)","b6b75cc6":"y_pred_rf=rf.predict(X_test)","631504c3":"print(classification_report(y_test,y_pred_rf))","3efea057":"rf.score(X_test,y_test)","a56f8701":"# Validation Curve for Random Forest without SMOTE\nrf_param_range = np.arange(1,1000,100)\n\nplt.figure(figsize=(12,8))\n\ntrain_scores, test_scores = validation_curve(estimator=rf,X=X_train,y=y_train ,param_name='n_estimators',param_range=rf_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.plot(rf_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(rf_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\n\nplt.plot(rf_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(rf_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\n    \nplt.xlabel('n_estimators_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.9,1])","66548617":"# Validation Curve for Random Forest without SMOTE\nrf_param_range = np.arange(1,13)\n\nplt.figure(figsize=(15, 10))\n\ntrain_scores, test_scores = validation_curve(estimator=rf,X=X_train,y=y_train ,param_name='max_depth',param_range=rf_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.plot(rf_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(rf_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\n\nplt.plot(rf_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(rf_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\n    \nplt.xlabel('max_depth_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.9,1])","80e5a8cf":"params={'criterion':['entropy', 'gini'],'max_depth': range(2,8),'n_estimators' : range(1,200,50)}\n\ngrid_rf=GridSearchCV(rf,params,cv=5)\ngrid_rf.fit(X_train,y_train)\n\nprint('The best value of hyperparameters \"criterion\", \"max_depth\", and \"min_samples_split\"')\nprint(grid_rf.best_params_)","3e26e9c4":"rf_tuned=RandomForestClassifier(n_estimators=1,max_depth=2,criterion='entropy')\n\nrf_tuned.fit(X_train,y_train)\nrf_tuned.score(X_train,y_train)\n\ny_pred_tunedrf=rf_tuned.predict(X_test)","51f3b815":"rf_tuned.score(X_test,y_test)","f754be1c":"\nprint(classification_report(y_test,y_pred_tunedrf))","12170b02":"rf_sm=RandomForestClassifier()\n\nrf_sm.fit(X_train_sm,y_train_sm)\nrf_sm.score(X_train_sm,y_train_sm)","1da0ab1d":"y_pred_rfsm=rf_sm.predict(X_test)","dab4d438":"print(classification_report(y_test,y_pred_rfsm))","2db5e93d":"# Validation Curve for Random Forest model with SMOTE\nrfsm_param_range = np.arange(1,40)\n\nplt.figure(figsize=(15, 10))\n\ntrain_scores, test_scores = validation_curve(estimator=rf_sm,X=X_train_sm,y=y_train_sm ,param_name='max_depth',param_range=rfsm_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.plot(rfsm_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(rfsm_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\n\nplt.plot(rfsm_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(rfsm_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\n    \nplt.xlabel('max_depth_parameters')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.9,1])","18f76944":"# Validation Curve for Random Forest model with SMOTE\nrfsm_param_range = np.arange(1,1000,100)\n\nplt.figure(figsize=(15, 10))\n\ntrain_scores, test_scores = validation_curve(estimator=rf_sm,X=X_train_sm,y=y_train_sm ,param_name='n_estimators',param_range=rfsm_param_range)\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nlw=0.5\nplt.subplot(2,2,1)\nplt.plot(rfsm_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\nplt.fill_between(rfsm_param_range, train_mean - train_std,\n                 train_mean + train_std, alpha=0.2,\n                 color=\"darkorange\", lw=lw)\n\nplt.plot(rfsm_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \nplt.fill_between(rfsm_param_range, test_mean - test_std,\n                 test_mean + test_std, alpha=0.2,\n                 color=\"navy\", lw=lw)\n\n    \nplt.xlabel('n_estimators_parameters')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.9,1])","9aaa3d15":"from sklearn.model_selection import GridSearchCV\nParameter_Trials={'criterion':['gini','entropy'],\n                  'n_estimators': range(100,500,100),'max_depth':[15,16,17,18,19,20,21,22,23]}\n \nGrid_Search = GridSearchCV(rf_sm, Parameter_Trials, cv=5, n_jobs=1)\nGridSearchResults=Grid_Search.fit(X,y)\nGrid_Search.best_params_","b76fe70f":"rf_smtuned=RandomForestClassifier(criterion='gini',n_estimators=300,max_depth=17)\n\nrf_smtuned.fit(X_train_sm,y_train_sm)\nrf_smtuned.score(X_train_sm,y_train_sm)\n\ny_pred_rfsmtuned=rf_smtuned.predict(X_test)","d14d5392":"print(classification_report(y_test,y_pred_rfsmtuned))","5b574826":"y_pred_probarf= rf.predict_proba(X_test)[:,1]\n\ny_pred_probarfsm= rf_sm.predict_proba(X_test)[:,1]\n\ny_pred_probarf_tuned= rf_tuned.predict_proba(X_test)[:,1]\n\ny_pred_probadrfsm_tuned= rf_smtuned.predict_proba(X_test)[:,1]\n\nplt.figure(figsize = (12, 8))\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_probarf)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.plot([0, 1], [0, 1],'r--')\nplt.title('ROC curve for Different Models', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score for rf model:',round(roc_auc_score(y_test, y_pred_probarf),4)))\n## 2nd classifier\nfpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred_probarfsm)\nplt.plot(fpr1, tpr1)\nplt.text(x = 0.02, y = 0.8, s = ('AUC Score for rf_sm model:',round(roc_auc_score(y_test, y_pred_probarfsm),4)))\nplt.grid(True)\n\n## 3rd classifier\nfpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred_probarf_tuned)\nplt.plot(fpr2, tpr2)\nplt.text(x = 0.02, y = 0.7, s = ('AUC Score for rf_tuned model:',round(roc_auc_score(y_test, y_pred_probarf_tuned),4)))\nplt.grid(True)\n\n## 4th classifier\nfpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred_probadrfsm_tuned)\nplt.plot(fpr3, tpr3)\nplt.text(x = 0.02, y = 0.6, s = ('AUC Score for rfsm_tuned model:',round(roc_auc_score(y_test, y_pred_probadrfsm_tuned),4)))\nplt.grid(True)","3d457f4a":"from xgboost import XGBClassifier\n\nX_train_xgb=X_train.astype(np.number)\nX_test_xgb=X_test.astype(np.number)\ny_train_xgb=y_train.astype(np.number)\ny_test_xgb=y_test.astype(np.number)\n\nxgb=XGBClassifier()\nxgb.fit(X_train_xgb,y_train_xgb)\n\nxgb.score(X_train_xgb,y_train_xgb)\n\ny_pred_xgb=xgb.predict(X_test)\n\n","a7b3c4d6":"print(classification_report(y_test,y_pred_xgb))","6658aa0b":"xgb.score(X_test_xgb,y_test_xgb)","d8b22942":"from sklearn.metrics import classification_report,accuracy_score,precision_score,confusion_matrix\n\ny_pred_xgb= xgb.predict(X_test_xgb)\ny_proba_xgb= xgb.predict_proba(X_test_xgb)\nprint('Roc_auc score:' ,roc_auc_score(y_test_xgb,y_proba_xgb[:,1]))\nprint('Classification Report:')\nprint(classification_report(y_test_xgb,y_pred_xgb))\n\nprint('precision_score')\nprint(precision_score(y_test_xgb,y_pred_xgb))\n\nprint('confusion_matrix')\nprint(confusion_matrix(y_test_xgb,y_pred_xgb))","bbce5847":"from sklearn.feature_selection import SelectKBest,f_classif\nselect_features = SelectKBest(f_classif,k='all')","52ade34c":"from sklearn.ensemble import AdaBoostClassifier\n\nad = AdaBoostClassifier(n_estimators=100)\nad.fit(X_train,y_train)\nprint(\"What is the Testing Accuracy\")\nprint(ad.score(X_test,y_test))\nprint(\"What is the Training Accuracy\")\nprint(ad.score(X_train,y_train))\n\ny_pred_ad=ad.predict(X_test)","ab4faba8":"print(classification_report(y_test,y_pred_ad))","190b2b37":"from sklearn.ensemble import ExtraTreesClassifier\net=ExtraTreesClassifier()\n\net.fit(X_train,y_train)\net.score(X_train,y_train)\n\ny_pred_et=et.predict(X_test)","d231b824":"y_pred_et=et.predict(X_test)\nprint(classification_report(y_test,y_pred_ad))","a7dfad57":"et.score(X_test,y_test)","b1b8731c":"from sklearn.ensemble import BaggingClassifier\n\nbg = BaggingClassifier(n_estimators=2)\nbg.fit(X_train,y_train)\nprint(\"What is the Testing Accuracy\")\nprint(bg.score(X_test,y_test))\nprint(\"What is the Training Accuracy\")\nprint(bg.score(X_train,y_train))","a148ed33":"y_pred_bg=bg.predict(X_test)\nprint(classification_report(y_test,y_pred_bg))","df2beccc":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train,y_train)\n\nprint(\"What is the Training Accuracy\")\nprint(gb.score(X_train,y_train))\n\nprint(\"What is the Testing Accuracy\")\nprint(gb.score(X_test,y_test))\ngb.feature_importances_","40082bff":"y_pred_gb=et.predict(X_test)\nprint(classification_report(y_test,y_pred_gb))","3ac2f555":"#### Reading data from the files","6d2ec013":"## Validation Curve for Logistic Regression Model without SMOTE ","db28954b":"## Comparing AUC Score along with ROC curve for Decision Tree, tuned Decision Tree, Decision Tree with SMOTE and tuned Decision Tree with SMOTE ","4eedd62d":"### We can see that people with age between 60-90 have most likely to have a stroke","c32e4938":"## Applying Ensemble Techniques ","450ca74d":"### From this we infer that our predictor is very highly imbalanced. To treat imbalance we could go with Oversampling or Undersampling techniques(like SMOTE), but further in this problem we have used SMOTE(Oversampling)","93e01369":"### Decision tree without SMOTE","ea6d8205":"## Tuning Decision Tree with SMOTE by taking Hyperparameter range as per Validation Curve","0add82fb":"# Bivariate Analysis","86f918d3":"# Random Forest without SMOTE","d11da291":"### Boxplot of Numerical features in the data shows the presence of outliers in avg_glucose_level and bmi","c897939d":"## Fitting Decision Tree algo without SMOTE using the hyperparameters deduced above","ea1a2b56":"## Decision Tree with SMOTE","c0445772":"### Logistic Regression with SMOTE gives better Precision,Recall and F1-SCore as compared to Logistic Regression without SMOTE","e0e885d4":"### We can easily visualize that people who have ever been maried have a larger number of stroke than compared to the non married","dcb9dd89":"### Boxplot of Numerical features in the data shows the presence of outliers in avg_glucose_level and bmi","1316e436":"### Decision Tree with SMOTE gives better Precision,Recall and F1-SCore as compared to Decision Tree without SMOTE","a43c6705":"### We can easily visualize that people who have been working in private sector have larger possibility of stroke than any other profession","48e883a2":"### We can easily  see that bmi is not affecting the target that much","c9cd3d9d":"## Validation Curve for Decision Tree Model with SMOTE ","320e8866":"# Applying few Ensemble Techniques","0a323879":"## Tuning Decision Tree without SMOTE by taking Hyperparameter range as per Validation Curve","1eea00d8":"## KNN without SMOTE","84c0d009":"### Logistic Regression with SMOTE","87e16ec4":"#### Exploratory Data Analysis","00aa1c2d":"## Applying Random Forest with SMOTE ","ec4b13db":"# Extra Trees Classifier","617fb9df":"# AdaBoost Classifier","248d9aa4":"# XGBoost Classifier","ba2ee2dc":"## Naive Bayes without SMOTE","523bedd3":"### Naive Bayes with SMOTE gives lower F1-SCore as compared to Naive Bayes without SMOTE","c8364864":"# Gradient Boosting","c7a80f52":"# Applying Cross validation to check Sampling Bias","786f436c":"# Heart Stroke Prediction using XGBoost and Random Forest","bd277e12":"## Tuning Random Forest without SMOTE by taking Hyperparameter range as per Validation Curve","19b07a49":"### Logistic regression without SMOTE","1b050762":"## Comparing AUC Score along with ROC curve for Random Forest, tuned Random Forest, Random Forest with SMOTE and tuned Random Forest with SMOTE ","2081d963":"# Bagging Classifier","19b6100c":"## Naive Bayes with SMOTE","96f93b71":"### KNN with SMOTE","556a6fcf":"## Validation Curve for Decision Tree Model without SMOTE ","7c4ed59d":"### KNN with SMOTE gives better Recall and F1-SCore as compared to KNN without SMOTE"}}