{"cell_type":{"94e27036":"code","76ecf9c3":"code","2bd65743":"code","0426f2e0":"code","c1a76233":"code","fc29eaa5":"code","3766e758":"code","81bdcab9":"code","b7350de2":"code","4495c272":"code","1852caae":"code","e67ff942":"code","ce024cac":"code","e9bb470c":"markdown","e69e039e":"markdown","eb841ae4":"markdown","44711b4c":"markdown","d29a13c1":"markdown","cb82f3af":"markdown","f6e35125":"markdown"},"source":{"94e27036":"# loading all needed libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import Sequential, layers\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom time import time\nfrom keras.models import load_model","76ecf9c3":"# Loading training data\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_data","2bd65743":"# Loading test data\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_data","0426f2e0":"train_data = train_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ntest_data = test_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)","c1a76233":"# Info about training data\ntrain_data.info()","fc29eaa5":"# Info about test data\ntest_data.info()","3766e758":"# Desribe training data\ntrain_data.describe()","81bdcab9":"# Desribe test data\ntest_data.describe()","b7350de2":"# The correlation between numerical features\nmask = np.triu(np.ones_like(train_data._get_numeric_data().corr(), dtype=np.bool))\ncorr_map = sns.heatmap(data=train_data._get_numeric_data().corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')","4495c272":"for data in [train_data, test_data]:\n    # replacing with \"None\" for categoricals\n    str_cols = data.select_dtypes(include=['object']).columns\n    data.loc[:, str_cols] = data.loc[:, str_cols].fillna('None')\n    # replacing with median for numericals\n    data.fillna(data.median(axis=0), inplace=True)","1852caae":"# train and test split\ntrain_X, train_y = train_data.values[:, 1:], train_data.values[:, 0]\ntest_X = test_data.values\n# One hot encoding\nenc = OneHotEncoder(sparse=False, handle_unknown='ignore')\ntrain_X_enc = enc.fit_transform(train_X)\ntest_X_enc = enc.transform(test_X)\n# change type of data for nn to work\ntrain_X_enc=np.asarray(train_X_enc).astype(np.float32)\ntrain_y_enc=np.asarray(train_y).astype(np.float32)\ntest_X_enc=np.asarray(test_X_enc).astype(np.float32)","e67ff942":"from sklearn.model_selection import train_test_split, StratifiedKFold\nimport scipy\nfirst_col = True\ncross_fold = StratifiedKFold(n_splits = 5, shuffle=True)\nfor train_index, test_index in cross_fold.split(train_X_enc, train_y_enc):\n    validation_X, validation_y = train_X_enc[test_index], train_y_enc[test_index]\n    train_X, train_y = train_X_enc[train_index], train_y_enc[train_index]\n    \n    # determine the number of input features\n    n_features = train_X_enc.shape[1]\n    # define model\n    model = Sequential()\n    model.add(layers.Dense(512, kernel_initializer='he_normal', input_shape=(n_features,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation('elu'))\n    model.add(layers.Dropout(0.5))\n#     model.add(layers.Dense(512, kernel_initializer='he_normal'))\n#     model.add(layers.BatchNormalization())\n#     model.add(layers.Activation('elu'))\n#     model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(2, activation='softmax'))\n    # compile the model\n    model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n    #callbacks\n    # simple early stopping\n    es = EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.0001, verbose=1, patience=30)\n    mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n\n    tensorboard = TensorBoard(log_dir=\"logs\/{}\".format(time()))\n    \n    rlrop = ReduceLROnPlateau(monitor='accuracy', factor=0.5, patience=5, verbose=1)\n    \n    # fit model and save the best\n    history = model.fit(x=train_X, y=train_y, \n#                         steps_per_epoch=steps_per_epoch, \n                        batch_size=8, \n                        epochs=300, \n                        validation_data=(validation_X, validation_y), \n#                         validation_steps=validation_steps, \n                        shuffle=True, \n                        callbacks=[tensorboard, es, mc, rlrop]\n                       )\n    saved_model = load_model('best_model.h5')\n    \n    probs = saved_model.predict(test_X_enc)\n    predict = probs.argmax(axis=1)\n    if first_col:\n        pr_values = np.array(predict, ndmin=2)\n        pr_values = np.transpose(pr_values)\n        first_col = False\n    else:\n        pr_values = np.insert(pr_values, -1, predict, axis=1)\npr_values= scipy.stats.mode(pr_values, axis=1)","ce024cac":"# saving results\nsub_data = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nsub_data['Survived'] = [x[-1] for x in pr_values[0]]\nsub_data.to_csv('nn_sub1.csv', index=False)","e9bb470c":"### As we saw before we have missing values in data. My approach here is to replace them with \"None\" for all object-type features and with median values for all numerical ones","e69e039e":"### Hello everyone. In this notebook (my first on kaggle actually) I will show you how to build a simple neural network to predict survivals of titanic passengers with ~ top 10% result. Upvotes and discussion are highly appreciated.","eb841ae4":"### Our data has missing values, so we'll need to deal with it later","44711b4c":"### Next, some quick EDA","d29a13c1":"### And, finally, the most important step is building the model. Here I use a simple Multi-layer Perceptron with just one hidden layer containing 512 neurons. But to make it perform well, I use several techniques: \n### 1. Using Batch normalization after the layer\n### 2. Using \"elu\" activation function instead of more common \"relu\".\n### 3. Adding 50% Dropout to combat overfitting.\n### 4. Halving learning rate after 5 epochs with no improvement.\n### 5. Using rather small batch size for better performance\n### 6. Cross-validation mode with 5 folds. The model is built 5 times with early stopping after 30 epochs without improvement on validation data. After all the modal value of all 5 predictions is computed as our final prediction","cb82f3af":"### Some of the features in our data are not informative for our prediction, so we can drop them","f6e35125":"### The next step is to perform one-hot encoding because we have some categorical features"}}