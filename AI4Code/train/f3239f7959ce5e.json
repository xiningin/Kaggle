{"cell_type":{"8c1e9995":"code","2dd835d2":"code","283a8e9a":"code","113c7b2e":"code","66e82eea":"code","7a921ba4":"code","2ec27760":"code","9c77654c":"code","f182bd0f":"code","31997d5b":"code","eb50d361":"code","d9162ec3":"code","91cd5bab":"code","d1d79eb7":"code","a0fc117f":"code","1fc1ae03":"code","909d4ca5":"markdown","e757c59f":"markdown","5d1daf61":"markdown","68b0e97a":"markdown","c682bcb8":"markdown","2734b084":"markdown","0515d00f":"markdown","e14cf1b3":"markdown","594c7749":"markdown","ecb674fc":"markdown","5b99486e":"markdown","950bf2b4":"markdown","6f7d58f0":"markdown","3fc61a2b":"markdown","ed895d0c":"markdown"},"source":{"8c1e9995":"import math\nimport os\nimport pickle\nfrom dataclasses import dataclass\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data.dataset import Dataset\nfrom transformers import AdamW, BertForSequenceClassification, BertTokenizer, DataCollatorWithPadding\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm as tqdm_notebook","2dd835d2":"data = pd.read_csv('\/kaggle\/input\/imdb-ptbr\/imdb-reviews-pt-br.csv', index_col=0)\ndata.head(1)","283a8e9a":"data.info()","113c7b2e":"data['sentiment'].value_counts().plot.pie(autopct='%.2f', explode=[0.01, 0])","66e82eea":"test_dev_size = int(0.05*data.shape[0])\ntrain_dev, test = train_test_split(data, test_size=test_dev_size, random_state=42, stratify=data['sentiment'])\ntrain, dev = train_test_split(train_dev, test_size=test_dev_size, random_state=42, stratify=train_dev['sentiment'])\nprint('Training samples:', train.shape[0])\nprint('Dev samples:     ', dev.shape[0])\nprint('Test samples:    ', test.shape[0])","7a921ba4":"class ImdbPt(Dataset):\n    ''' Loads IMDB-pt dataset. \n    \n    It will tokenize our inputs and cut-off sentences that exceed 512 tokens (the pretrained BERT limit)\n    '''\n    def __init__(self, tokenizer, X, y):\n        X = list(X)\n        y = list(y)\n        tokenized_data = tokenizer(X, truncation=True, max_length=512)\n        samples = [\n            {\n                **{key: tokenized_data[key][i] for key in tokenized_data},\n                'labels': y[i]\n            }\n             \n            for i in range(len(X))\n        ]\n        self.samples = samples\n    \n    def __getitem__(self, i):\n        return self.samples[i]\n    \n    def __len__(self):\n        return len(self.samples)","2ec27760":"def send_inputs_to_device(inputs, device):\n    return {key:tensor.to(device) for key, tensor in inputs.items()}\n","9c77654c":"tokenizer = BertTokenizer.from_pretrained('neuralmind\/bert-base-portuguese-cased')\ntrain_dataset = ImdbPt(tokenizer, train['text_pt'], (train['sentiment'] == 'pos').astype(int))\ndev_dataset   = ImdbPt(tokenizer, dev['text_pt'], (dev['sentiment'] == 'pos').astype(int))\ntest_dataset  = ImdbPt(tokenizer, test['text_pt'], (test['sentiment'] == 'pos').astype(int))\n","f182bd0f":"train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=DataCollatorWithPadding(tokenizer))\ndev_loader = DataLoader(dev_dataset, batch_size=16, collate_fn=DataCollatorWithPadding(tokenizer))\ntest_loader = DataLoader(test_dataset, batch_size=16, collate_fn=DataCollatorWithPadding(tokenizer))\n","31997d5b":"model = BertForSequenceClassification.from_pretrained(\n    'neuralmind\/bert-base-portuguese-cased')\nmodel.train()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-6)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9997)\n\n\nfor param in model.base_model.parameters():\n    param.requires_grad = False","eb50d361":"def evaluate(model, dev_loader, device):\n    with torch.no_grad():\n        model.eval()\n        dev_losses = []\n        tp, tn, fp, fn = [], [], [], []\n        for inputs in dev_loader:\n            inputs = send_inputs_to_device(inputs, device)\n            loss, scores = model(**inputs)[:2]\n            dev_losses.append(loss.cpu().item())\n\n            _, classification = torch.max(scores, 1)\n            labels = inputs['labels']\n            tp.append(((classification==1) & (labels==1)).sum().cpu().item())\n            tn.append(((classification==0) & (labels==0)).sum().cpu().item())\n            fp.append(((classification==1) & (labels==0)).sum().cpu().item())\n            fn.append(((classification==0) & (labels==1)).sum().cpu().item())\n\n        tp_s, tn_s, fp_s, fn_s = sum(tp), sum(tn), sum(fp), sum(fn)\n        print('Dev loss: {:.2f}; Acc: {:.2f}; tp: {}; tn: {}; fp: {}; fn: {}'.format( \n              np.mean(dev_losses), (tp_s+tn_s)\/(tp_s+tn_s+fp_s+fn_s), tp_s, tn_s, fp_s, fn_s))\n\n        model.train()","d9162ec3":"epoch_bar = tqdm_notebook(range(1))\nloss_acc = 0\nalpha = 0.95\nfor epoch in epoch_bar:\n    batch_bar = tqdm_notebook(enumerate(train_loader), desc=f'Epoch {epoch}', total=len(train_loader))\n    for idx, inputs in batch_bar:\n        if (epoch * len(train_loader) + idx) == 800:\n            for param in model.base_model.parameters():\n                param.requires_grad = True\n\n        inputs = send_inputs_to_device(inputs, device)\n        optimizer.zero_grad()\n        loss, logits = model(**inputs)[:2]\n        \n        loss.backward()\n        optimizer.step()\n        if epoch == 0 and idx == 0:\n            loss_acc = loss.cpu().item()\n        else:\n            loss_acc = loss_acc * alpha + (1-alpha) * loss.cpu().item()\n        batch_bar.set_postfix(loss=loss_acc)\n        if idx%200 == 0:\n            del inputs\n            del loss\n            evaluate(model, dev_loader, device)\n\n        scheduler.step()\n    os.makedirs('\/kaggle\/working\/checkpoints\/epoch'+str(epoch))\n    model.save_pretrained('\/kaggle\/working\/checkpoints\/epoch'+str(epoch))   \n\n","91cd5bab":"with torch.no_grad():\n    model.eval()\n    pred = []\n    labels = []\n    for inputs in tqdm_notebook(dev_loader):\n        inputs = send_inputs_to_device(inputs, device)\n        _, scores = model(**inputs)[:2]\n        pred.append(F.softmax(scores, dim=1)[:, 1].cpu())\n        labels.append(inputs['labels'].cpu())\npred = torch.cat(pred).numpy()\nlabels = torch.cat(labels).numpy()\n","d1d79eb7":"fpr, tpr, thresholds = metrics.roc_curve(labels, pred, pos_label=1)\nroc_auc = metrics.auc(fpr, tpr)\n\nfig = px.scatter(\n    x=fpr, y=tpr, color=thresholds, \n    labels={'x': 'False positive rate', 'y': 'True positive rate'},\n    title='Curva ROC')\nfig.show()","a0fc117f":"acc = []\nfor th in thresholds:\n    acc.append(metrics.accuracy_score(pred > th, labels))\n\nfig2 = px.scatter(\n    x=thresholds, y=acc, labels={'x': 'threshold', 'y': 'acur\u00e1cia'}, \n    title='Acur\u00e1cia em diferentes thresholds')\nfig2.show()  \n    ","1fc1ae03":"with torch.no_grad():\n    model.eval()\n    pred = []\n    labels = []\n    for inputs in tqdm_notebook(test_loader):\n        inputs = send_inputs_to_device(inputs, device)\n        _, scores = model(**inputs)[:2]\n        pred.append(F.softmax(scores, dim=1)[:, 1].cpu())\n        labels.append(inputs['labels'].cpu())\npred = torch.cat(pred).numpy()\nlabels = torch.cat(labels).numpy()\n\nprint('Acc:', metrics.accuracy_score(pred>0.67, labels))","909d4ca5":"Aqui, n\u00f3s vamos utilizar o DataCollatorWithPadding. Uma vez que os nossos exemplos podem ter comprimentos diferentes, precisamos colocar padding para junt\u00e1-los em um batch. O DataCollatorWithPadding j\u00e1 faz isso para n\u00f3s.","e757c59f":"\n## Definindo a classe que vai carregar o nosso dataset\n\nEssa classe vai tokenizar os exemplos e fornecer os pares de inputs e labels para o modelo.\n\n","5d1daf61":"# Treinamento\n","68b0e97a":"## Preparando os dados para o treino","c682bcb8":"## Treinando o modelo\n\nAqui vamos fazer a avalia\u00e7\u00e3o no devset a cada 200 passos. \nNo passo 800 vamos soltar os pesos do modelo base do BERT pra aprender.","2734b084":"## Avalia\u00e7\u00e3o final\n\nFinalmente vamos avaliar o modelo no conjunto de test com o threshold escolhido","0515d00f":"## Definindo fun\u00e7\u00e3o auxiliares","e14cf1b3":"## Validando o modelo\n\nAgora vamos verificar o desempenho no dev set. Vamos ver a curva ROC e tb qual o melhor threshold de classifica\u00e7\u00e3o com respeito a nossa m\u00e9trica escolhida, acur\u00e1cia","594c7749":"Ok, o dataset est\u00e1 bem balanceado, o que nos conduz a utilizar acur\u00e1cia como m\u00e9trica.\n","ecb674fc":"## Criando os splits\n\n90\/5\/5 splits","5b99486e":"# Conclus\u00e3o\n\nNesse notebook mostramos como usar o BERT para fazer an\u00e1lise de sentimento. \nSe voc\u00ea gostou do notebook, n\u00e3o esquece de soltar o upvote. \n\nVlw!","950bf2b4":"Vamos ver a distribui\u00e7\u00e3o de classes","6f7d58f0":"# Introdu\u00e7\u00e3o\n\nOl\u00e1, j\u00e1 que este dataset \u00e9 em portugu\u00eas, vou fazer o markdown em portugu\u00eas. Nesse notebook n\u00f3s vamos prever o sentimento (positivo ou negativo) de um coment\u00e1rio no IMDB. Para tanto, faremos uso do poderoso BERT.\n\nO BERT \u00e9 um modelo de linguagem baseado no Transformer. Ele \u00e9 essencialmente o encoder do Transformer, e \u00e9 muito adequado em casos onde voc\u00ea j\u00e1 tem a mensagem inteira para processar, o que \u00e9 o caso da an\u00e1lise de sentimentos (mas n\u00e3o \u00e9 o caso da gera\u00e7\u00e3o de texto, por exemplo).\n\nVamos utilizar a implementa\u00e7\u00e3o de Hugginface. ","3fc61a2b":"## Carregando o modelo e otimizador\n\nAqui n\u00f3s definimos uma coisa extra que \u00e9 come\u00e7ar o modelo com a parte pr\u00e9 treinada do BERT congelada. Assim a gente permite aos pesos do layer de classifica\u00e7\u00e3o se acomodarem antes de come\u00e7ar a fazer o fine-tune na rede toda.","ed895d0c":"## Lendo e entendendo os dados"}}