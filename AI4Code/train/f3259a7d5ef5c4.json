{"cell_type":{"baacb6e6":"code","2e949f94":"code","71c02c6e":"code","79ddbbe6":"code","cef9e383":"code","a11e9a03":"code","b52313f0":"code","6619f1cb":"code","fb8a1e9b":"code","05111c03":"code","1191fa47":"code","86e18fc6":"code","ba8b9413":"code","f500a546":"code","e32a6efe":"code","10377994":"code","bb4dd8da":"code","c470b678":"code","69de5e7d":"code","51ec33b6":"code","1caa5959":"code","ef6d639a":"code","a7f24c32":"code","ea8f8d8f":"code","a95d6dfb":"code","5bf1c551":"code","441cae86":"code","0d3a5c2b":"code","a4db82ce":"code","bccf2d85":"code","2797c5c1":"code","560c8ed8":"code","fbbfee32":"code","bb4ae36f":"code","df6acd60":"code","5ce053a8":"code","aba8b270":"code","2214fe50":"code","dd576cf0":"code","45496fcb":"code","a6745ee4":"code","6d37f15d":"code","d97243d6":"code","ecc4ea4b":"code","a34dfda8":"code","8fa7dbcb":"code","3c29b243":"code","e745c87f":"markdown","e4467491":"markdown","0c18e64a":"markdown","cd9efdf0":"markdown","54dbf0fc":"markdown","4110f4a3":"markdown","0d2e3b8e":"markdown","0fd53f7d":"markdown","c5206538":"markdown","ccd57683":"markdown","a67167c6":"markdown","a02593a8":"markdown","70ba19cd":"markdown","64c1d4f4":"markdown","f20da6f4":"markdown","fe35dc0e":"markdown","2ec7dc4f":"markdown","523ad5ef":"markdown","9ea09800":"markdown","bda12ad4":"markdown","4ee37e37":"markdown","12f2562e":"markdown","0fd08ed7":"markdown","d8b2ed24":"markdown","3d4c02bb":"markdown","ed63dd61":"markdown","b1e0c12f":"markdown","3c05817f":"markdown","e8715428":"markdown","3843cda0":"markdown","2549fcbf":"markdown","bb8f5366":"markdown","c46eaa74":"markdown","8a9cad12":"markdown","b33327e5":"markdown","6057adfe":"markdown","3c2022cd":"markdown","e93e68dd":"markdown","b0055a04":"markdown","01019fb0":"markdown"},"source":{"baacb6e6":"import pandas as pd, numpy as np, os, gc, matplotlib.pyplot as plt, seaborn as sb, re, warnings, calendar, sys\nfrom numpy import arange\nget_ipython().run_line_magic('matplotlib', 'inline')\nwarnings.filterwarnings('ignore'); np.set_printoptions(suppress=True); pd.options.mode.chained_assignment = None\npd.set_option('display.float_format', lambda x: '%.3f' % x)\nglobal directory; directory = '..\/input'\n\ndef files(): return os.listdir(directory)\n\ndef read_clean(data):\n    data.columns = [str(x.lower().strip().replace(' ','_')) for x in data.columns]\n    seen = {}; columns = []; i = 0\n    for i,x in enumerate(data.columns):\n        if x in seen: columns.append(x+'_{}'.format(i))\n        else: columns.append(x)\n        seen[x] = None\n        \n    for x in data.columns[data.count()\/len(data) < 0.0001]: del data[x];\n    gc.collect();\n    try: data = data.replace({'':np.nan,' ':np.nan});\n    except: pass;\n    \n    if len(data) < 10000: l = len(data);\n    else: l = 10000;\n    sample = data.sample(l);size = len(sample);\n    \n    for x in sample.columns:\n        ints = pd.to_numeric(sample[x], downcast = 'integer', errors = 'coerce')\n        if ints.count()\/size > 0.97:\n            minimum = ints.min()\n            if minimum > 0: data[x] = pd.to_numeric(data[x], downcast = 'unsigned', errors = 'coerce')\n            else: data[x] = pd.to_numeric(data[x], downcast = 'integer', errors = 'coerce')\n        else:\n            floats = pd.to_numeric(sample[x], downcast = 'float', errors = 'coerce')\n            if floats.count()\/size > 0.97: data[x] = pd.to_numeric(data[x], downcast = 'float', errors = 'coerce')\n            else:\n                dates = pd.to_datetime(sample[x], errors = 'coerce')\n                if dates.count()\/size > 0.97: data[x] = pd.to_datetime(data[x], errors = 'coerce')\n    return data.reset_index(drop = True)\n\ndef read(x):\n    '''Kaggle Reading in CSV files.\n    Just type read('file.csv'), and you'll get back a Table.'''\n    \n    file = '{}\/{}'.format(directory,x)\n    try:     data = pd.read_csv(file)\n    except:  data = pd.read_csv(file, encoding = 'latin-1')\n    return read_clean(data)\n\ndef tally(column, minimum = 0, top = None, graph = False, percent = False, multiple = False, lowercase = False, min_count = 1):\n    '''Provides a tally count of all values in a COLUMN.\n        1. minimum  =  (>0)          Least count of item to show.\n        2. top      =  (-1,>0)       Only show top N objects\n        3. graph    =  (False,True)  Show bar graph of results\n        4. percent  =  (False,>0)    Instead of showing counts, show percentages of total count\n        \n       multiple = False\/True.\n       If True, counts and tallies objects in list of lists (Count Vectorizer)\n       \n       lowercase = True \/ False.\n       If True, lowers all text firsrt. So A == a\n       \n       min_count >= 1\n       If a column sum for tag has less than min_count, discard whole column\n    '''\n    if multiple == False:\n        counts = column.value_counts().astype('uint')\n        counts = counts[counts >= minimum][:top]\n        counts = pd.DataFrame(counts).reset_index()\n        counts.columns = [column.name, 'tally']\n        if percent: \n            counts['tally'] \/= counts['tally'].sum()\/100\n            counts['tally'] = counts['tally']\n        if graph:\n            C = counts[::-1]\n            C.plot.barh(x = column.name, y = 'tally', legend = False); plt.show();\n        return counts\n    else:\n        from sklearn.feature_extraction.text import CountVectorizer\n        column = column.fillna('<NAN>')\n        if type(column.iloc[0]) != list: column = column.apply(lambda x: [x])\n        counter = CountVectorizer(lowercase = lowercase, tokenizer = lambda x: x, dtype = np.uint32, min_df = min_count)\n        counter.fit(column)\n        counts = pd.DataFrame(counter.transform(column).toarray())\n        counts.columns = [column.name+'_('+str(x)+')' for x in counter.get_feature_names()]\n        return counts\n    \n    \ndef describe(data):\n    '''Provides an overview of your data\n        1. dtype    =  Column type\n        2. missing% =  % of the column that is missing\n        3. nunique  =  Number of unique values in column\n        4. top3     =  Top 3 most occuring items\n        5. min      =  Minimum value. If not a number column, then empty\n        6. mean     =  Average value. If not a number column, then empty\n        7. median   =  Middle value. So sort all numbers, and get middle. If not a number column, then empty\n        8. max      =  Maximum value. If not a number column, then empty\n        9. sample   =  Random 2 elements\n        10. name    =  Column Name\n    '''\n    dtypes = dtype(data)\n    length = len(data)\n    missing = ((length - data.count())\/length*100)\n    \n    N = [];    most3 = []\n    for dt,col in zip(dtypes,data.columns):\n        if dt != 'datetime':\n            U = data[col].value_counts()\n            N.append(len(U))\n            if U.values[0] > 1: most3.append(U.index[:3].tolist())\n            else: most3.append([]);\n        else: N.append(0); most3.append([]);\n            \n    df = pd.concat([dtypes, missing], 1)\n    df.columns = ['dtype','missing%']\n    df['nunique'] = N; df['top3'] = most3\n    \n    numbers = list(data.columns[df['dtype'].isin(('uint','int','float'))])\n    df['min'] = data.min()\n    df['mean'] = data[numbers].mean()\n    df['median'] = data[numbers].median()\n    df['max'] = data.max()\n    df['sample'] = data.apply(lambda x : x.sample(2).values.tolist())\n    df['name'] = list(data.columns)\n    return df.sort_values(['missing%', 'nunique', 'dtype'], ascending = [False, False, True]).reset_index(drop = True)\n\n\ndef Checker(x):\n    if type(x) is pd.DataFrame: return 0\n    elif type(x) is pd.Series: return 1\n    else: return -1\n\ndef columns(data): return list(data.columns)\ndef rows(data): return list(data.index)\ndef index(data): return list(data.index)\ndef head(data, n = 10): return data.head(n)\ndef tail(data, n = 10): return data.tail(n)\ndef sample(data, n = 10): return data.sample(n)\n\ndef dtype(data):\n    what = Checker(data)\n    if what == 0:\n        dtypes = data.dtypes.astype('str')\n        dtypes = dtypes.str.split(r'\\d').str[0]\n    else:\n        dtypes = str(data.dtypes)\n        dtypes = re.split(r'\\d', dtypes)[0]\n    return dtypes\n\ndef mean(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].mean()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.mean()\n        else: return np.nan\n    else:\n        try:     return np.nanmean(data)\n        except:  return np.nan\n        \ndef std(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].std()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.std()\n        else: return np.nan\n    else:\n        try:     return np.nanstd(data)\n        except:  return np.nan\n        \ndef var(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].var()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.var()\n        else: return np.nan\n    else:\n        try:     return np.nanvar(data)\n        except:  return np.nan\n        \ndef log(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        x = np.log(data[numbers])\n        x[np.isinf(x)] = np.nan\n        return pd.Series(x)\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        else: return np.nan\n    else:\n        try:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        except:  return np.nan\n        \ndef median(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].median()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.median()\n        else: return np.nan\n    else:\n        try:     return np.nanmedian(data)\n        except:  return np.nan\n        \ndef minimum(data):\n    what = Checker(data)\n    if what == 0:      return data.min()\n    elif what == 1:    return data.min()\n    else:              return np.min(data)\n        \ndef maximum(data):\n    what = Checker(data)\n    if what == 0:      return data.max()\n    elif what == 1:    return data.max()\n    else:              return np.max(data)\n    \ndef missing(data):\n    what = Checker(data)\n    if what >= 0:      return pd.isnull(data)\n    else:              return np.isnan(data)\n    \ndef count(data):\n    what = Checker(data)\n    if what >= 0:      return data.count()\n    else:              return len(data)\n    \ndef nunique(data):\n    what = Checker(data)\n    if what >= 0:      return data.nunique()\n    else:              return len(np.unique(data))\n    \ndef unique(data):\n    if type(data) is pd.DataFrame:\n        uniques = []\n        for x in data.columns:\n            uniques.append(data[x].unique())\n        df = pd.Series(uniques)\n        df.index = data.columns\n        return df\n    elif type(data) is pd.Series: return data.unique()\n    else:              return np.unique(data)\n    \ndef total(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].sum()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.sum()\n        else: return np.nan\n    else:\n        try:     return np.nansum(data)\n        except:  return np.nan\n        \ndef time_number(date): return hours(date)+minutes(date)\/60+seconds(date)\/60**2\ndef hours_minutes(date): return hours(date)+minutes(date)\/60\ndef hours(date): return date.dt.hour\ndef minutes(date): return date.dt.minute\ndef seconds(date): return date.dt.second\ndef month(date): return date.dt.month\ndef year(date): return date.dt.year\ndef day(date): return date.dt.day\ndef weekday(date): return date.dt.weekday\ndef leap_year(date): return year(date).apply(calendar.isleap)\ndef date_number(date): return year(date)+month(date)\/12+day(date)\/(365+leap_year(date)*1)\ndef year_month(date): return year(date)+month(date)\/12\n\ndef hcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 1)\n\ndef vcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 0)\n\ndef melt(data, columns):\n    '''Converts a dataset into long form'''\n    return data.melt(id_vars = columns)\n    \ndef tabulate(*columns, method = 'count'):\n    '''Splits columns into chunks, and counts the occurences in each group.\n        Remember - tabulate works on the LAST column passed.\n        Options:\n            1. count            = Pure Count in group\n            2. count_percent    = Percentage of Count in group\n            3. mean             = Mean in group\n            4. median           = Median in group\n            5. max              = Max in group\n            6. min              = Min in group\n            7. sum_percent      = Percentage of Sum in group\n        Eg:\n            Apple | 1\n            ---------\n            Orange| 3\n            ---------\n            Apple | 2\n            ---------\n        Becomes:\n            Apple | 1 | 1\n            -------------\n                  | 2 | 1\n            -------------\n            Orange| 3 | 1\n        \n        NOTE --------\n            method can be a list of multiple options.\n    '''\n    if type(method) in (list, tuple):\n        xs = []\n        for x in method:\n            g = tabulate(*columns, method = x)\n            xs.append(g)\n        xs = hcat(xs)\n        xs = xs.T.drop_duplicates().T\n        return read_clean(xs)        \n    else:\n        def percent(series):\n            counts = series.count()\n            return counts.sum()\n\n        data = hcat(*columns)\n        columns = data.columns.tolist()\n\n        if method in ('count', 'count_percent'):\n            groups = data.groupby(data.columns.tolist()).apply(lambda x: x[data.columns[-1]].count())\n\n            if method == 'count_percent':\n                groups = groups.reset_index()\n                groups.columns = list(groups.columns[:-1])+['Group_Count']\n                right = data.groupby(columns[:-1]).count().reset_index()\n                right.columns = list(right.columns[:-1])+['Group_Sum']\n\n                groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n                groups['Percent%'] = groups['Group_Count']\/groups['Group_Sum']*100\n                groups = groups[columns+['Percent%']]\n                return groups\n\n        elif method == 'mean': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].mean())\n        elif method == 'median': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].median())\n        elif method == 'max': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].max())\n        elif method == 'min': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].min())\n        elif method == 'sum_percent':\n            groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].sum()).reset_index()\n            groups.columns = list(groups.columns[:-1])+['Group_Count']\n            right = data.groupby(columns[:-1]).sum().reset_index()\n            right.columns = list(right.columns[:-1])+['Group_Sum']\n\n            groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n            groups['Sum%'] = groups['Group_Count']\/groups['Group_Sum']*100\n            groups = groups[cols+['Sum%']]\n            return groups\n        else:\n            print('Method does not exist. Please choose count, count_percent, mean, median, max, min, sum_percent.'); return None;\n        #except: print('Method = {}'.format(method)+' cannot work on Object, Non-Numerical data. Choose count.'); return None;\n\n        groups = pd.DataFrame(groups)\n        groups.columns = [method]\n        groups.reset_index(inplace = True)\n        return groups\n\n\ndef sort(data, by = None, how = 'ascending', inplace = False):\n    ''' how can be 'ascending' or 'descending' or 'a' or 'd'\n    It can also be a list for each sorted column.\n    '''\n    replacer = {'ascending':True,'a':True,'descending':False,'d':False}\n    if by is None and type(data) is pd.Series:\n        try:    x = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n        return data.sort_values(ascending = x, inplace = inplace)\n    elif type(how) is not list:\n        try:    how = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    else:\n        for x in how: \n            try:    x = replacer[x]\n            except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    return data.sort_values(by, ascending = how, inplace = inplace)\n\ndef keep(data, what, inplace = False):\n    '''Keeps data in a column if it's wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple,np.array,np.ndarray): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[~need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[~need] = np.nan\n        return df\n\ndef remove(data, what, inplace = False):\n    '''Deletes data in a column if it's not wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[need] = np.nan\n        return df\n    \n    \ndef ternary(data, condition, true, false = np.nan, inplace = False):\n    '''C style ternary operator on column.\n    Condition executes on column, and if true, is filled with some value.\n    If false, then replaced with other value. Default false is NAN.'''\n    try:\n        execute = 'data {}'.format(condition)\n        series = eval(execute)\n        try: series = series.map({True:true, False:false})\n        except: series = series.replace({True:true, False:false})\n        return series\n    except: print('Ternary accepts conditions where strings must be enclosed.\\nSo == USD not allowed. == \"USD\" allowed.'); return False;\n\n    \ndef locate(data, column):\n    '''Use ternary to get result and then filter with notnull'''\n    if dtype(column) == 'bool': return data.loc[column]\n    return data.loc[column.notnull()]\n    \ndef query(data, column = None, condition = None):\n    '''Querying data based on conditions'''\n    def Q(data, column, condition):\n        if column is not None:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data[column], tuple(condition))\n                cond = (cond.notnull())\n            else: cond = ternary(data[column], condition, True, False)\n            return data.loc[cond]\n        else:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data, tuple(condition))\n            else: cond = ternary(data, condition, True, False)\n            return data.loc[cond]\n    try:\n        return Q(data, column, condition)\n    except:\n        condition = condition.replace('=','==')\n        return Q(data, column, condition)\n        \ndef keep_top(x, n = 5):\n    '''Keeps top n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:n].values)\n    return df\n\ndef keep_bot(x, n = 5):\n    '''Keeps bottom n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:-n].values)\n    return df\n\n\ndef remove_outlier(x, method = 'iqr', range = 1.5):\n    '''Removes outliers in column with methods:\n        1. mean     =    meean+range (normally 3.5)\n        2. median   =    median+range (normally 3.5)\n        3. iqr      =    iqr+range (normally 1.5)\n    '''\n    i = x.copy()\n    if method == 'iqr':\n        first = np.nanpercentile(x, 0.25)\n        third = np.nanpercentile(x, 0.75)\n        iqr = third-first\n        i[(i > third+iqr*range) | (i < first-iqr*range)] = np.nan\n    else:\n        if method == 'mean': mu = np.nanmean(x)\n        else: mu = np.nanmedian(x)\n        std = np.nanstd(x)\n        i[(i > mu+std*range) | (i < mu-std*range)] = np.nan\n    return i\n\n\ndef cut(x, bins = 5, method = 'range'):\n    '''Cut continuous column into parts.\n        Method options:\n            1. range\n            2. quantile (number of quantile cuts)'''\n    if method == 'range': return pd.cut(x, bins = bins, duplicates = 'drop')\n    else: return pd.qcut(x, q = bins, duplicates = 'drop')\n    \n    \ndef plot(x, y = None, colour = None, column = None, data = None, size = 5, top = 10, wrap = 4, \n         subset = 5000, method = 'mean', quantile = True, bins = 10,\n         style = 'lineplot', logx = False, logy = False, logc = False, power = 1):\n    '''Plotting function using seaborn and matplotlib\n        Options:\n        x, y, colour, column, subset, style, method\n        \n        Plot styles:\n            1. boxplot\n            2. barplot\n            3. tallyplot (counting number of appearances)\n            4. violinplot (boxplot just fancier)\n            5. lineplot (mean line plot)\n            6. histogram\n            7. scatterplot (X, Y must be numeric --> dates will be converted)\n            8. bivariate (X, Y must be numeric --> dates will be converted)\n            9. heatmap (X, Y will be converted into categorical automatically --> bins)\n            10. regplot (X, Y must be numeric --> dates will be converted)\n    '''\n    if type(x) in (np.array,np.ndarray): x = pd.Series(x); x.name = 'x';\n    if type(y) in (np.array,np.ndarray): y = pd.Series(y); y.name = 'y';\n    if type(column) in (np.array,np.ndarray): column = pd.Series(column); column.name = 'column';\n    if type(colour) in (np.array,np.ndarray): colour = pd.Series(colour); colour.name = 'colour';\n        \n    if type(x) == pd.Series: \n        data = pd.DataFrame(x); x = x.name\n        if type(x) is not str:\n            data.columns = [str(x)]\n            x = str(x)\n    if method == 'mean': estimator = np.nanmean\n    elif method == 'median': estimator = np.nanmedian\n    elif method == 'min': estimator = np.min\n    elif method == 'max': estimator = np.max\n    else: print('Wrong method. Allowed = mean, median, min, max'); return False;\n    #----------------------------------------------------------\n    sb.set(rc={'figure.figsize':(size*1.75,size)})\n    dtypes = {'x':None,'y':None,'c':None,'col':None}\n    names = {'x':None,'y':None,'c':None,'col':None}\n    xlim = None\n    #----------------------------------------------------------\n    if data is not None:\n        if type(x) is str: x = data[x];\n        if type(y) is str: y = data[y]; \n        if type(colour) is str: colour = data[colour]; \n        if type(column) is str: column = data[column]; \n    if type(x) is str: print('Please specify data.'); return False;\n    #----------------------------------------------------------\n    if x is not None:\n        dtypes['x'] = dtype(x); names['x'] = x.name\n        if dtypes['x'] == 'object': x = keep_top(x, n = top)\n        elif dtypes['x'] == 'datetime': x = date_number(x)\n        if logx and dtype(x) != 'object': x = log(x)\n    if y is not None: \n        dtypes['y'] = dtype(y); names['y'] = y.name\n        if dtypes['y'] == 'object': y = keep_top(y, n = top)\n        elif dtypes['y'] == 'datetime': y = date_number(y)\n        if logy and dtype(y) != 'object': y = log(y)\n    if colour is not None:\n        dtypes['c'] = dtype(colour); names['c'] = colour.name\n        if dtypes['c'] == 'object': colour = keep_top(colour, n = top)\n        elif dtypes['c'] == 'datetime': colour = date_number(colour)\n        if logc and dtype(colour) != 'object': colour = log(colour)\n    if column is not None:\n        dtypes['col'] = dtype(column); names['col'] = column.name\n        if dtypes['col'] == 'object': column = keep_top(column, n = top)\n        elif dtypes['col'] == 'datetime': column = date_number(column)\n    #----------------------------------------------------------\n    df = hcat(x, y, colour, column)\n    if subset > len(df): subset = len(df)\n    df = sample(df, subset)\n    #----------------------------------------------------------\n    if column is not None:\n        if dtype(df[names['col']]) not in ('object', 'uint',' int') and nunique(df[names['col']]) > top: \n            if quantile: df[names['col']] = cut(df[names['col']], bins = bins, method = 'quantile')\n            else: df[names['col']] = cut(df[names['col']], bins = bins, method = 'range')\n    \n    try: df.sort_values(names['y'], inplace = True);\n    except: pass;\n    #----------------------------------------------------------\n    replace = {'boxplot':'box', 'barplot':'bar', 'tallyplot':'count', 'violinplot':'violin', \n               'lineplot': 'point', 'histogram':'lv'}\n    \n    if style == 'histogram' and y is None:\n        plot = sb.distplot(df[names['x']].loc[df[names['x']].notnull()], bins = bins)\n    elif style == 'lineplot' and y is None:\n        plot = plt.plot(df[names['x']]);\n        plt.show(); return;\n    elif style == 'barplot' and y is None:\n        plot = df.sort_values(names['x']).plot.bar();\n        plt.show(); return;\n    elif style in replace.keys():\n        if dtype(df[names['x']]) not in ('object', 'uint',' int') and nunique(df[names['x']]) > top: \n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n        \n        if names['col'] is not None:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator, col_wrap = wrap)\n        else:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator)\n            \n        for ax in plot.axes.flatten(): \n            for tick in ax.get_xticklabels(): \n                tick.set(rotation=90)\n    \n    elif style == 'heatmap':\n        if dtype(df[names['x']]) != 'object'and nunique(df[names['x']]) > top:\n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n                \n        if dtype(df[names['y']]) != 'object'and nunique(df[names['y']]) > top:\n            if quantile: df[names['y']] = cut(df[names['y']], bins = bins, method = 'quantile')\n            else: df[names['y']] = cut(df[names['y']], bins = bins, method = 'range')     \n\n        df = tabulate(df[names['x']], df[names['y']]).pivot(index = names['x'], columns = names['y'], values = 'count')\n        plot = sb.heatmap(df, cmap=\"YlGnBu\")\n\n        \n    elif dtype(df[names['x']]) == 'object' or dtype(df[names['y']]) == 'object':\n            print('{} can only take X = number and Y = number.'.format(style)); return False;\n        \n    elif style  in ('regplot', 'scatterplot'):\n        if column is None: col_wrap = None\n        else: col_wrap = wrap\n        if style == 'regplot': reg = True\n        else: reg = False\n        \n        plot = sb.lmplot(x = names['x'], y = names['y'], hue = names['c'], data = df, col = names['col'],\n                             n_boot = 2, size = size, ci = None, scatter_kws={\"s\": 50,'alpha':0.5},\n                        col_wrap = col_wrap, truncate = True, fit_reg = reg, order = power)\n        plot.set_xticklabels(rotation=90)\n        \n    elif style == 'bivariate':\n        plot = sb.jointplot(x = names['x'], y = names['y'], data = df, dropna = True, size = size, kind = 'reg',\n                           scatter_kws={\"s\": 50,'alpha':0.5}, space = 0)\n    plt.show()\n    \n    \ndef match_pattern(x, pattern, mode = 'find'):\n    '''Regex pattern finds in data and returns only match\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        \n        Modes =\n            1. find:   True\/False if find or not\n            2. keep:   Output original string if match, else NAN\n            3. match:  Output only the matches in the string, else NAN\n        '''\n    pattern = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n\n    regex = re.compile(r'{}'.format(pattern))\n    \n    def patternFind(i):\n        try: j = re.match(regex, i).group(); return True\n        except: return False;\n    def patternKeep(i):\n        try: j = re.match(regex, i).group(); return i\n        except: return np.nan;\n    def patternMatch(i):\n        try: j = re.match(regex, i).group(); return j\n        except: return np.nan;\n    \n    if mode == 'find':        return x.apply(patternFind)\n    elif mode == 'keep':      return x.apply(patternKeep)\n    elif mode == 'match':     return x.apply(patternMatch)\n    \n    \ndef split(x, pattern):\n    '''Regex pattern finds in data and returns match. Then, it is splitted accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        \\S = most symbols including spaces but not apostrophes\n        '''\n    pattern2 = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)').replace('\\S','[.!, \"\\(\\)\\?\\*\\&\\^%$#@:\/\\\\_;\\+\\-\\\u2026]')\n    \n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n    \n    regex = re.compile(r'{}'.format(pattern2))\n    if pattern == pattern2: return x.str.split(pattern)\n    else: return x.apply(lambda i: re.split(regex, i))\n    \ndef replace(x, pattern, with_ = None):\n    '''Regex pattern finds in data and returns match. Then, it is replaced accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        '''\n    if type(pattern) is list:\n        d = {}\n        for l in pattern: d[l[0]] = l[1]\n        try:\n            return x.replace(d)\n        except:\n            return x.astype('str').replace(d)\n            \n    pattern2 = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    \n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n    \n    regex = re.compile(r'{}'.format(pattern2))\n    if pattern == pattern2: return x.str.replace(pattern, with_)\n    else: return x.apply(lambda i: re.sub(regex, with_, i))\n    \ndef remove(x, what):\n    return replace(x, what, '')\n    \ndef notnull(data, loc = None):\n    '''Returns the items that are not null in a column \/ dataframe'''\n    if loc is not None:\n        return data.loc[loc.notnull()]\n    else:\n        return data.loc[data.notnull().sum(1) == data.shape[1]]\n    \n    \ndef exclude(data, col):\n    '''Only returns a dataframe where the columns in col are not included'''\n    if type(col) is str: col = [col]\n    columns = list(data.columns)\n    leave = list(set(columns) - set(col))\n    return data[leave]\n\n################### -----------------------------------------------------------------#######################\n#Recommendation Systems\ndef pivot(index, columns, values):\n    '''Creates a table where rows = users, columns = items, and cells = values \/ ratings'''\n    from scipy.sparse import dok_matrix\n    S = dok_matrix((nunique(index), nunique(columns)), dtype=np.float32)\n    \n    mins = np.abs(np.min(values))+1\n    indexM = {}\n    for i,x in enumerate(unique(index)): indexM[x] = i;\n    columnsM = {}\n    for i,x in enumerate(unique(columns)): columnsM[x] = i;\n        \n    for i,c,v in zip(index, columns, values+mins): S[indexM[i],columnsM[c]] = v;\n    \n    S = S.toarray(); S[S == 0] = np.nan; S -= mins\n    S = pd.DataFrame(S)\n    S.index = indexM.keys(); S.columns = columnsM.keys();\n    return S\n\ndef row_operation(data, method = 'sum'):\n    '''Apply a function to a row\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n    '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(1)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(1)'.format(method.split('_')[0]))\n        x \/= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(1)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 1)\n    x.name = 'row_operation'\n    return x\n\n\ndef col_operation(data, method = 'sum'):\n    '''Apply a function to a column\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n        '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(0)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(0)'.format(method.split('_')[0]))\n        x \/= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(0)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 0)\n    x.name = 'col_operation'\n    return x\n\n    \ndef random(obj, n = 1, p = None):\n    if p is not None:\n        if type(p) is pd.Series: p = p.values\n        if p.sum() > 2: p \/= 100\n    return list(np.random.choice(obj, size = n, replace = False, p = p))\n\ndef row(data, n): return data.loc[n]\n\ndef distances(source, target):\n    '''Returns all distances between target and source (L2)'''\n    Y = np.tile(target.values, (source.shape[0],1))\n    nans = np.isnan(Y)\n    X = source.values; X[np.isnan(X)] = 0;\n    Y[nans] = 0;\n    diff = X - Y;\n    diff[nans] = 0;\n    d = np.linalg.norm(diff, axis = 1)\n    j = pd.Series(d)\n    j.index = source.index\n    return j\n\n################### -----------------------------------------------------------------#######################\n#Natural Language Processing & Machine Learning\n\ndef multiply(left, right):\n    ''' Multiplies 2 tables or columns together.\n        Will do automatic type casting'''\n\n    if len(left.shape) == 1:\n        try: return left.values.reshape(-1,1)*right\n        except: return left.reshape(-1,1)*right\n    elif len(right.shape) == 1:\n        try: return right.values.reshape(-1,1)*left\n        except: return right.reshape(-1,1)*left\n    else:\n        return left*right\n    \n    \ndef clean(data, missing = 'mean', remove_id = True):\n    '''Cleans entire dataset.\n    1. missing =\n        mean, max, median, min\n        Fills all missing values with column mean\/median etc\n\n    2. remove_id = True\/False\n        Checks data to see if theres an ID column.\n        Removes it (not perfect)\n    '''\n    x = data[data.columns[dtype(data) != 'object']].copy()\n    for c in x.columns[x.count()!=len(x)]:\n        x[c] = eval('x[c].fillna(x[c].{}())'.format(missing))\n    if remove_id:\n        for c in x.columns[(dtype(x) == 'int')|(dtype(x) == 'uint')]:\n            if x[c].min() >= 0:\n                j = (x[c] - x[c].min()).sort_values().diff().sum()\n                if j <= 1.001*len(x) and j >= len(x)-1: x.pop(c);\n    return x\n\n\ndef scale(data):\n    columns = data.columns\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler().fit(data)\n    X = pd.DataFrame(scaler.transform(data))\n    X.columns = columns\n    return X\n\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom keras.models import Sequential, load_model\nfrom sklearn.linear_model import LassoLarsIC\nfrom sklearn.linear_model import Ridge\nfrom sklearn.utils import class_weight\nfrom keras.layers import Dense, Activation, GaussianNoise, BatchNormalization, Dropout\nfrom keras.initializers import glorot_normal\nfrom keras.callbacks import *\nfrom keras.optimizers import Nadam, SGD\n\nclass LinearModel(BaseEstimator, RegressorMixin):\n\n    def __init__(self, lasso = False, scale = True, logistic = False, layers = 0, activation = 'tanh', epochs = 50,\n                    time = None, shift = 1, test_size = 0.2, early_stopping = 7, lr = 0.1):\n        self.scale = scale; self.logistic = logistic; self.lasso = lasso; self.layers = layers;\n        assert activation in ['tanh','relu','sigmoid','linear']\n        assert shift > 0;\n        self.activation = activation; self.epochs = epochs; self.time = time; self.shift = shift\n        if logistic or (self.logistic == False and self.layers > 0): self.model = Sequential()\n        elif lasso: self.model = LassoLarsIC()\n        else: self.model = Ridge()\n        self.mapping = {}; self.test_size = test_size; self.early_stopping = early_stopping\n        self.lr = lr\n\n    def fit(self, X, Y):\n        print('Model now fitting...')\n        X = self._process_X(X.copy())\n        X = self._time_transform(X)\n        \n        self.uniques, self.columns = X.apply(self._range_unique), list(X.columns)\n        if self.scale: X, self.means, self.stds = self._scaler(X)\n        self.uniques_scale = X.apply(self._range_unique)\n        \n        if self.logistic:\n            Y = self._process_Y(Y)\n            self._fit_keras(X, Y)\n        else:\n            try: \n                if self.layers == 0: self._fit_sklearn(X, Y)\n                else: \n                    self.out = 1\n                    if Y.min() >= 0:\n                        if Y.max() <= 1: self.activ = 'sigmoid'\n                        else: self.activ = 'relu'\n                    elif Y.min() >= -1 and Y.max() <= 1:\n                        self.activ = 'tanh'\n                    else: self.activ = 'linear'\n                    self.loss = 'mse'\n                    self._fit_keras(X, Y)\n            except: \n                print('Y is not numeric. Choose logistic = True for classification'); return None\n\n        self._store_coefficients()\n        self._df = self._store_analysis(X)\n        print('Model finished fitting')\n\n        \n    def predict(self, X):\n        X = self._process_X(X.copy())\n        X = self._time_transform(X)\n        X = self._transform(X)\n        if self.logistic:\n            prob = self.model.predict(X)\n            if self.activ == 'sigmoid': prob = prob.round().astype('int').flatten()\n            else: prob = prob.argmax(1).astype('int').flatten()\n            prob = pd.Series(prob).replace(self.mapping)\n            return prob\n        else: return self.model.predict(X).flatten()\n\n\n    def predict_proba(self, X):\n        if self.logistic:\n            X = self._process_X(X.copy())\n            X = self._time_transform(X)\n            X = self._transform(X)\n            prob = self.model.predict(X).flatten()\n            return prob\n        else: print('Predict Probabilities only works for logisitc models.'); return None;\n\n        \n    def coefficients(self, plot = False, top = None):\n        df = self.coef\n        if self.layers == 0:\n            if top is not None: df = df[:top]\n            if plot:\n                df = df.fillna('')\n                if len(self.mapping) > 2:\n                    df = df.style.bar(subset = [x for x in df.columns if 'Y=(' in x], align='mid', color=['#d65f5f', '#5fba7d'])\n                else:\n                    df = df.style.bar(subset = ['Coefficient'], align='mid', color=['#d65f5f', '#5fba7d'])\n        return df\n    \n    \n    def plot(self, predictions, real_Y):\n        if self.logistic:\n            from sklearn.metrics import confusion_matrix\n            conf = pd.DataFrame(confusion_matrix(real_Y, predictions))\n            try: \n                conf.index = [f'True({x}\/{i})' for i,x in zip(self.mapping.keys(), self.mapping.values())]\n                conf.columns = [f'{x}\/{i}' for i,x in zip(self.mapping.keys(), self.mapping.values())]\n            except: \n                conf.index = [f'True({x})' for x in range(nunique(real_Y))]\n            conf = conf.divide(conf.sum(1), axis = 'index')*100\n            return sb.heatmap(conf, cmap=\"YlGnBu\", vmin = 0, vmax = 100, annot = True)\n        else:\n            return plot(x = predictions, y = real_Y, style = 'regplot')\n    \n    \n    def score(self, predictions, real_Y):\n        if self.logistic:\n            from sklearn.metrics import matthews_corrcoef\n            coef = matthews_corrcoef(real_Y, predictions)\n            if np.abs(coef) < 0.3: print('Model is not good. Score is between (-0.3 and 0.3). A score larger than 0.3 is good, or smaller than -0.3 is good.') \n            else: print('Model is good.')\n            return coef\n        else:\n            from sklearn.metrics import mean_squared_error\n            error = np.abs(np.sqrt(mean_squared_error(real_Y, predictions))\/np.mean(real_Y))\n            if error > 0.4: print('Model is not good. Score is larger than 40%. Smaller than 40% relative error is good.')\n            else: print('Model is good.')\n            return error\n    \n    \n    def analyse(self, column = None, plot = False, top = 20):\n        if self.layers == 0:\n            df = self._df.round(2)\n            if self.logistic:\n                if column is not None: df = df.loc[column]\n                else: df = df[:top]\n                def color_negative_red(val):\n                    color = 'lightgreen' if val == 'Add 1' else 'pink'\n                    return 'background-color: %s' % color\n\n                def highlight_max(s):\n                    is_max = s == s.max()\n                    return ['color: lightgreen' if v else '' for v in is_max]\n\n                if plot:\n                    df = df.fillna('')\n                    if self.activ == 'sigmoid':\n                        df = df.style.bar(align = 'mid', width = 75, color = ['gray'])\\\n                                    .applymap(color_negative_red, subset = ['Effect'])\n                    else:\n                        df = df.style.bar(align = 'mid', width = 75, color = ['gray'])\\\n                                    .applymap(color_negative_red, subset = ['Effect'])\\\n                                    .apply(highlight_max, subset = df.columns[2:], axis = 1)\n            else:\n                if column is not None: df = pd.DataFrame(df.loc[[column]])\n                else: df = df[:top]\n                if plot:\n                    cols = list(df.columns); cols.remove('If Stays'); cols.remove('Change if Removed')\n                    df[cols] = df[cols].fillna('')\n                    def color_negative_red(val):\n                        if val == True: color = 'cyan'\n                        elif val == False: color = 'pink'\n                        else: color = ''\n                        return 'background-color: %s' % color\n\n                    df = df.style.bar(subset = ['Coefficient','If Stays','Change if Removed','Best Addon',\n                                               'Worst Reduced'], align='mid', color=['#d65f5f', '#5fba7d'])\\\n                            .applymap(color_negative_red, subset = ['Stay'])\\\n                            .bar(subset = ['Best Contrib','Worst Contrib'], align='mid', color=['pink', 'cyan'])\n            return df\n        else:\n            print(\"Can't analyse since it's a neural network. I can only give you the model layout and loss graphs\")\n            print(self.model.summary()); history = self.history\n            plt.plot(history.history['loss']); plt.plot(history.history['val_loss'])\n            plt.title('model loss'); plt.ylabel('loss');plt.xlabel('epoch')\n            plt.legend(['train', 'test'], loc='upper left')\n            plt.show()\n        \n        \n    def degrees(self, prediction, real_Y):\n        '''The closer the degree of the fit line to 45*, the better!'''\n        if not self.logistic:\n            from sklearn.linear_model import Ridge as modeller\n            models = modeller().fit(prediction.reshape(-1,1),real_Y)\n            deg = np.round((np.arctan(models.coef_[0]))\/np.pi*180, 3)\n            if deg <= 50 and deg > 45: print('Prediction seems good, but probably overpredicting')\n            elif deg > 50: print(\"Prediction doesn't seem good. It's overpredicting\")\n            elif deg == 45: print(\"Prediction looks ideal! It's quite smooth\")\n            elif deg <= 45 and deg > 40: print(\"Prediction seems good, but probably underpredicting\")\n            else: print(\"Prediction doesn't seem good. It's underpredicting\")\n            return deg\n        else: print('Model is not regression. Use score instead'); return None;\n        \n        \n    def _process_X(self, X):\n        try: X.shape[1]\n        except: X = X.reshape(-1,1)\n        if type(X) is not pd.DataFrame: X = pd.DataFrame(X)\n        try: X = X[self.columns]\n        except: pass\n        return X\n\n\n    def _process_Y(self, Y):\n        if type(Y) is not pd.Series: Y = pd.Series(Y)\n        n = nunique(Y); Y = Y.astype('category')\n        self.mapping = dict(enumerate(Y.cat.categories))\n        self.reverse_mapping = dict(zip(self.mapping.values(), self.mapping.keys()))\n        Y = Y.cat.codes\n        \n        class_weights = class_weight.compute_class_weight('balanced', list(self.mapping.keys()), Y)\n        self.class_weights = dict(enumerate(class_weights))\n        \n        if n == 2:\n            self.activ, self.loss, self.out = 'sigmoid', 'binary_crossentropy', 1\n        else:\n            self.activ, self.loss = 'softmax', 'categorical_crossentropy'\n            Y = pd.get_dummies(Y); self.out = Y.shape[1]\n        return Y\n    \n    \n    def _time_transform(self, X):\n        if self.time is not None:\n            X.sort_values(self.time, inplace = True)\n            alls = [X]\n            for s in range(1,self.shift+1):\n                ss = X.shift(s); ss.columns = [x+f'({-s})' for x in ss.columns]\n                alls.append(ss)\n            X = pd.concat(alls, 1)\n            X.fillna(method = 'backfill', inplace = True); X.sort_index(inplace = True)\n        return X\n    \n        \n    def _store_coefficients(self):\n        if self.logistic:\n            if self.layers == 0:\n                coefs = pd.DataFrame(self.coef_)\n                if len(self.mapping) > 2: coefs.columns, coefs.index = [f'Y=({x})' for x in self.mapping.values()], self.columns\n                else: coefs.columns, coefs.index = ['Coefficient'], self.columns\n                coefs['Abs'] = np.abs(coefs).sum(1)\n                coefs['Mean'], coefs['Std'], coefs['Range'], coefs['Scale'] = self.means, self.stds, self.uniques, self.uniques_scale\n                coefs.sort_values('Abs', inplace = True, ascending = False); coefs.pop('Abs');\n                self.coef = coefs\n            else: self.coef = self.coef_\n        else:\n            if self.layers == 0:\n                df = pd.DataFrame({'Coefficient':self.coef_ , 'Abs' : np.abs(self.coef_),\n                                    'Mean':self.means, 'Std':self.stds, 'Range':self.uniques, 'Scale':self.uniques_scale})\n                df.index = self.columns; df.sort_values('Abs', ascending = False, inplace = True)\n                df.pop('Abs');\n                self.coef = df\n            else: self.coef = self.coef_\n                \n\n    def _store_analysis(self, X):\n        if self.logistic:\n            if self.layers == 0:\n                coefs = pd.DataFrame(self.coef_)\n                if self.activ == 'sigmoid':\n                    col = 'Probability (Y={})'.format(max(list(self.mapping.values())))\n                    coefs.columns = [col]\n                    coefs.index = self.columns\n                    exponential = np.exp(1*coefs + self.bias_)\n                    exponential = exponential.divide(exponential + 1)*100\n                    exponential['Effect'] = 'Add 1'\n\n                    neg_exponential = np.exp(-1*coefs + self.bias_)\n                    neg_exponential = neg_exponential.divide(neg_exponential + 1)*100\n                    neg_exponential['Effect'] = 'Minus 1'\n\n                    coefs = pd.concat([exponential, neg_exponential]).round(2)\n                    coefs.reset_index(inplace = True); coefs.columns = ['Column']+list(coefs.columns[1:])\n                    coefs.sort_values(col, ascending = False, inplace = True)\n                else:\n                    coefs.columns, coefs.index = [f'Y=({x})' for x in self.mapping.values()], self.columns\n                    exponential = np.exp(1*coefs + self.bias_)\n                    exponential = exponential.divide(exponential.sum(1), axis = 0)*100\n                    exponential['Effect'] = 'Add 1'\n\n                    neg_exponential = np.exp(-1*coefs + self.bias_)\n                    neg_exponential = neg_exponential.divide(neg_exponential.sum(1), axis = 0)*100\n                    neg_exponential['Effect'] = 'Minus 1'\n\n                    coefs = pd.concat([exponential, neg_exponential])\n                    coefs.reset_index(inplace = True)\n                    coefs.columns = ['Column']+list(coefs.columns[1:])\n                    coefs = coefs[['Column','Effect']+list(coefs.columns)[1:-1]].round(2)\n\n                    coefs['Max'] = coefs.max(1); coefs.sort_values('Max', ascending = False, inplace = True); del coefs['Max'];\n                return coefs\n            else: return None\n        else:\n            if self.layers == 0:\n                full = X*self.coef_\n                transformed = full.sum(1) + self.bias_\n                selects, unselects, worst, best, W, B, L, original_G, original_B, overall = [],[],[],[],[],[],[],[],[],[]\n\n                for i, (col, mu) in enumerate(zip(self.columns, self.means)):\n                    if np.isnan(mu):\n                        cond = (X[col]!=0)\n                        select = transformed.loc[cond]\n                        unselect = transformed.loc[~cond]\n                        selects.append(select.mean())\n                        unselects.append(unselect.mean())\n\n                        original = X.loc[cond].mean(0)\n                        d = full.loc[cond].mean(0)\n                        dx = full.loc[~cond].mean(0)\n\n                        d = pd.DataFrame({col: d, 'Abs': np.abs(d)}).sort_values('Abs', ascending = False)[col]\n                        s = (d.index == col)\n                        d = d.loc[~s].sort_values(ascending = False)\n                        first = d.index[0]; end = d.index[-1]\n                        best.append(first)\n                        B.append(d[0]-dx.loc[first])\n                        worst.append(d.index[-1])\n                        W.append(d[-1]-dx.loc[end])\n                        L.append(len(select))\n\n                        original_G.append(original.loc[first])\n                        original_B.append(original.loc[end])\n                    else:\n                        selects.append(np.nan); unselects.append(np.nan); L.append(np.nan)\n\n                        gt = (full.gt(full[col], axis = 'index')*full)\n                        gt[gt == 0] = np.nan; gt_means = gt.mean(0).sort_values(ascending = False)\n                        changes = gt.subtract(full[col], axis = 'index').mean(0)\n                        b = gt_means.index[0]; b_add = changes.loc[b]; b_contrib = gt_means.iloc[0]\n                        best.append(b); B.append(b_add); original_G.append(b_contrib)\n\n                        lt = (full.lt(full[col], axis = 'index')*full)\n                        lt[lt == 0] = np.nan; lt_means = lt.mean(0).sort_values(ascending = True)\n                        changes = lt.subtract(full[col], axis = 'index').mean(0)\n                        w = lt_means.index[0]; w_add = changes.loc[w]; w_contrib = lt_means.iloc[0]\n                        worst.append(w); W.append(w_add); original_B.append(w_contrib)\n\n\n                df = pd.DataFrame({'Coefficient':self.coef_, 'N':L,'If Stays':selects, 'Removed':unselects, 'Change if Removed': 0, 'Stay' : 0,\n                                  'Best Combo':best, 'Best Addon':B,'Best Contrib':original_G,'Worst Combo':worst, 'Worst Reduced':W, 'Worst Contrib':original_B})\n\n                df['Change if Removed'] = df['Removed'] - df['If Stays']\n                df['Stay'] = (df['Change if Removed'] < 0); df['Abs'] = np.abs(df['Change if Removed'])\n                df.loc[df['N'].isnull(), 'Stay'] = np.nan\n                df['Abs_Coef'] = np.abs(df['Coefficient'])\n                df.index = self.columns\n                df.sort_values(['Abs','Abs_Coef'], ascending = [False,False], inplace = True)\n                df.pop('Abs'); df.pop('Removed'); df.pop('Abs_Coef');\n                return df\n            else: return None\n\n    def _fit_keras(self, X, Y):\n        self.model.add(GaussianNoise(0.01, input_shape = (X.shape[1],)))\n        \n        for l in range(self.layers):\n            self.model.add(Dense(X.shape[1], kernel_initializer = glorot_normal(seed = 0)))\n            self.model.add(Activation(self.activation))\n            self.model.add(BatchNormalization())\n            self.model.add(Dropout(0.15))\n            self.model.add(GaussianNoise(0.01))\n            \n        self.model.add(Dense(self.out, kernel_initializer = glorot_normal(seed = 0)))\n        self.model.add(Activation(self.activ))\n    \n        earlyStopping = EarlyStopping(monitor = 'val_loss', patience = int(self.early_stopping*(self.layers\/2+1)), verbose = 0, mode = 'min')\n        reduce_lr_loss = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = int(1*(self.layers\/2+1)), verbose = 0, epsilon = 1e-4, mode = 'min')\n        cycle = CyclicLR(base_lr = 0.0005, max_lr = self.lr, step_size = 2000, mode = 'exp_range')\n        checkpoint = ModelCheckpoint('Best_Model.hdf5', save_best_only = True)\n        \n        self.metrics = ['acc']        \n        if not self.logistic: self.class_weights = None; self.metrics = None\n        \n        self.model.compile(optimizer = Nadam(), loss = self.loss, metrics = self.metrics)\n\n        if len(X) < 100: bs = 10\n        elif len(X) < 200: bs = 20\n        elif len(X) < 300: bs = 30\n        else: bs = 32\n\n        self.history = self.model.fit(X, Y, epochs = self.epochs, batch_size = bs, verbose = 2, validation_split = self.test_size, shuffle = True,\n                    callbacks = [earlyStopping, TerminateOnNaN(), reduce_lr_loss, cycle, checkpoint], \n                   class_weight = self.class_weights)\n        self.model = load_model('Best_Model.hdf5')\n        if self.layers == 0: self.coef_, self.bias_ = self.model.get_weights()\n        else: self.coef_ = self.model.get_weights()\n        self.lr = cycle\n        \n        \n    def _fit_sklearn(self, X, Y):\n        self.model.fit(X, Y)\n        self.coef_, self.bias_ = self.model.coef_, self.model.intercept_\n\n\n    def _range_unique(self, x):\n        s = x.sort_values(ascending = True).values\n\n        mins, maxs = np.round(s[0], 2), np.round(s[-1], 2)\n        length = len(s)\/4\n        qtr1, qtr3 = np.round(s[int(length)], 2), np.round(s[int(3*length)], 2)\n        return sorted(set([mins, qtr1, qtr3, maxs]))\n\n\n    def _scaler(self, X):\n        result = []; means = []; stds = []\n        \n        for col in X.columns:\n            df = X[col]\n            if df.nunique() == 2 and df.min() == 0 and df.max() == 1:\n                result.append(df); means.append(np.nan); stds.append(np.nan)\n            else:\n                mu, std = df.mean(), df.std()\n                means.append(mu); stds.append(std)\n                result.append((df-mu)\/std)\n        return pd.concat(result, 1), np.array(means), np.array(stds)\n\n\n    def _transform(self, X):\n        if self.scale:\n            final = []\n            for col, mu, std in zip(self.columns, self.means, self.stds):\n                if np.isnan(mu): final.append(X[col])\n                else: final.append((X[col]-mu)\/std)\n            X = pd.concat(final, 1)\n        return X\n\n    \nclass CyclicLR(Callback):\n    \"\"\"(https:\/\/arxiv.org\/abs\/1506.01186). https:\/\/github.com\/bckenstler\/CLR\/blob\/master\/clr_callback.py\"\"\"\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=0.9999, scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n        self.base_lr = base_lr; self.max_lr = max_lr\n        self.step_size = step_size; self.mode = mode; self.gamma = gamma\n\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn, self.scale_mode = lambda x: 1., 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn, self.scale_mode = lambda x: 1\/(2.**(x-1)), 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn, self.scale_mode = lambda x: gamma**(x), 'iterations'\n        else:\n            self.scale_fn, self.scale_mode = scale_fn, scale_mode\n\n        self.clr_iterations = 0.; self.trn_iterations = 0.; self.history = {}\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,new_step_size=None):\n        if new_base_lr != None: self.base_lr = new_base_lr\n        if new_max_lr != None: self.max_lr = new_max_lr\n        if new_step_size != None: self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle': return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else: return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n        if self.clr_iterations == 0: K.set_value(self.model.optimizer.lr, self.base_lr)\n        else: K.set_value(self.model.optimizer.lr, self.clr())        \n\n    def on_batch_end(self, epoch, logs=None):\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n        for k, v in logs.items(): self.history.setdefault(k, []).append(v)\n        K.set_value(self.model.optimizer.lr, self.clr())\n        \n##----NATURAL LANG PROCESSING\ndef lower(x):\n    if type(x) is pd.Series: return x.str.lower()\n    else: return [y.lower() for y in x]\n\ndef upper(x):\n    if type(x) is pd.Series: return x.str.upper()\n    else: return [y.upper() for y in x]\n    \ndef remove_space(x):\n    '''Removes duplicate spaces'''\n    return x.str.replace('[\\s]{2,}', ' ')\n\ndef keep_length(x, length = 2):\n    '''Removes objects in lists spaces'''\n    return x.apply(lambda x: [y for y in x if len(re.sub(r'[0-9a-zA-Z\\']','',y)) > 0 or len(y) > length])\n\ndef clean_up(x):\n    '''Deletes \\n \\r'''\n    return x.str.replace('\\n',' ').str.replace('\\r',' ').str.lstrip().str.rstrip()","2e949f94":"print(\"Hello! Welcome to MARK5826!\")","71c02c6e":"print(1+2)","79ddbbe6":"1+2","cef9e383":"(1+2)\/3","a11e9a03":"print(hello)","b52313f0":"hello = 2\nhello","6619f1cb":"2 == 2","fb8a1e9b":"2 > -1","05111c03":"4 < 10","1191fa47":"100 >= -1","86e18fc6":"200 >= 234.234234","ba8b9413":"apple = 0\nprint(apple)","f500a546":"apple + 1","e32a6efe":"apple = apple+1","10377994":"apple","bb4dd8da":"print(\"Hello 1\")\nprint(\"Hello 2\")\nprint(\"Hello 3\")\nprint(\"Hello 4\")","c470b678":"counter = 1\nwhile counter < 5:\n    print(\"Hello {}\".format(counter))\n    counter = counter + 1","69de5e7d":"a = 20\nb = 10\nc = 204\nprint(\"Hello, a = {}, b = {}, c = {}\".format(a,b,c))","51ec33b6":"counter = 10\nwhile counter < 50:\n    print(\"Hello {}\".format(counter))\n    counter = counter * 2","1caa5959":"for counter in range(1,5):\n    print(\"Hello {}\".format(counter))","ef6d639a":"counter = 1\nwhile counter < 5:\n    print(\"Hello {}\".format(counter))\n    counter = counter + 1","a7f24c32":"counter = 10\nwhile counter < 50:\n    print(\"Hello {}\".format(counter))\n    counter = counter * 2","ea8f8d8f":"for counter in range(10,50,10):\n    print(\"Hello {}\".format(counter))","a95d6dfb":"fruits = ['apples', 'bananas', 'oranges']\nfruits","5bf1c551":"random_data = [1,2,0.23,\"apple\",'data',24]\nrandom_data","441cae86":"random_data[0]","0d3a5c2b":"random_data[5]","a4db82ce":"random_data = [1,2,0.23,\"apple\",'data',24]\n\ni = 0\nwhile i < len(random_data):\n    print(random_data[i])\n    i = i + 1","bccf2d85":"print(random_data[0])\nprint(random_data[1])\nprint(random_data[2])\nprint(random_data[3])\nprint(random_data[4])\nprint(random_data[5])","2797c5c1":"len(random_data)","560c8ed8":"random_data = [1,2,0.23,\"apple\",'data',24]\nfor item in random_data:\n    print(item)","fbbfee32":"empty_list = []\nempty_list","bb4ae36f":"empty_list.append('Hello')","df6acd60":"empty_list","5ce053a8":"empty_list.append('Hello again')\nempty_list","aba8b270":"fruit  = \"apple\"\nfruit","2214fe50":"fruit = 'appl'\nfruit","dd576cf0":"if fruit == 'apple':\n    print(\"Correct!\")\nelif fruit == 'appl':\n    print(\"You're missing a letter e\")\nelse:\n    print(\"Wrong!\")","45496fcb":"if fruit == 'apple':\n    print(\"Correct!\")\n    print(1+1)\n    print(\"hey\")\nprint(\"hey1\")\nprint(2)","a6745ee4":"if fruit == \"apple\":\n    print(\"Yep!\")\n    print(\"ok\")\nelif fruit == 'appl':\n    print('No, missing 1 letter')\nelse:\n    print('Hmmm')","6d37f15d":"if fruit == 'apple':\n    print('apple?')\n    print('hmm')\nelif fruit == 'appl':\n    print('missing 1 letter')\nelif fruit == 'app' or fruit == 'ale':\n    print('missing {} letters'.format(2))\nelif fruit == 'ap' and len(fruit) == 2:\n    print('You are missing {} letters, which is {} good'.format(3, 'not'))\nelse:\n    print('hmmmm ok not an apple at all')","d97243d6":"fruit = 'apple'\n\nfruit == 'apple' and len(fruit) == 5","ecc4ea4b":"fruit == 'apple' or len(fruit) == 6","a34dfda8":"fruit == 'apple' and len(fruit) == 5 and fruit[0] == 'a'","8fa7dbcb":"empty_list = []\n# You code goes here","3c29b243":"# Your code goes here","e745c87f":"**Lesson 1: Printing in Python**\n\nFirst, we need to print some statements in Python.\n\nIn Kaggle online kernels, press CTRL + ENTER to execute \/ run the cell.","e4467491":"Lets go back to the WHILE loop. Let's change it up.","0c18e64a":"You can see how the = operator added 1 to it, Now apple is NOT 0 but 1.","cd9efdf0":"<a id='Content'><\/a>\n<h1> 1. Python Fundamentals <\/h1>","54dbf0fc":"Above, you can see the use of multiple ELIF statements. In Python, the code is executed from top to bottom.\n\nThat means if one IF statement or ELIF statement fails, then it will check downwards.\n\nIn the end, if all statements fail, then the final ELSE statement is executed.\n\n<a id='Binary'><\/a>\n<h1> 5. Binary Operations <\/h1>\n\n**OR and AND**\n\nYou can also see we used the OR and AND commands. This essentially means intuitively if a condition has an AND statement or an OR statement.\n\nSo: a check:","4110f4a3":"You can see in action the AND and OR statements. AND means both statements need to be TRUE for the entire one to be TRUE.\n\nOR means either one needs to be TRUE.\n\nMultiple ANDS and ORS work","0d2e3b8e":"The goal here is say someone wrote the word wrong.\n\nSay fruit = \"appl\" and not = \"apple\"\n\nHow do we check if it's wrong?","0fd53f7d":"You can see the error on top when I type print(hello). You can see theres no speech marks \"\" \"\".\n\nIn Python, *string* (text) must be enclosed in either double quotes (\"...\") or single quotes ('...').\n\nhello failed, since Python thought you were trying to print a *variable*.","c5206538":"The code above uses i as a variable and prints out each location. So in essence, what is occuring is:","ccd57683":"You can also do the above. Using a FOR loop, we removed the counter, and immediately called items within the LIST.\n\n\"For every item in the LIST, print the item.\"","a67167c6":"In Python, comments can be placed inside code (text that is not executed). You need to place the # symbol.","a02593a8":"(1) In this exercise, you need to:\n\n1. Start with an EMPTY LIST.\n2. Add to it multiples of 5 starting from 0 all the way to 20.\n3. During each procedure of appending, print the resultant list out.\n\nIt should look like this:\n\n[]\n\n[0]\n\n[0,5]\n\n[0,5,10]\n\n[0,5,10,15]\n\n[0,5,10,15,20]","70ba19cd":"The function LEN(...) just says how long or how many elements is in the LIST.\n\nIn this case:","64c1d4f4":"<a id='Lab'><\/a>\n<h1> 5. Lab Questions <\/h1>\n\n**Using the MARK5826 Library Functions**\n\nThere is a documentation available for you which outlines what functions do and how they perform.\n\nJust a reminder - **ask us if you have issues. No mark penalties**.\n\nAlso, you can click the OPEN DISCUSSION: [CLICK HERE FOR OPEN DISCUSSION](https:\/\/docs.google.com\/document\/d\/1wYTOEgpdBPlM8OXlTKHZ5FrDJSBj50DLphdgbcgRaIs\/edit?usp=sharing)\n\nWe will link you to it later.","f20da6f4":"What a FOR LOOP does is exactly the same as a WHILE:","fe35dc0e":"In Python, assigning or saving data to a variable (eg, hello) is allowed using the = operator.\n\nNote --> in Python, = is ASSIGN. == is COMPARE IF = to.\n\n**VARIABLE NAMES**\n\nmust be all connected by underscores (eg, *fav_colour*). NO dashes - is allowed (eg, *fav-colour*). NO spaces is allowed (eg, *fav colour*).\n\nnumbers are fine, but NO NUMBERS at the start (eg, *fav_colour1* is fine, but not *1fav_colour*).","2ec7dc4f":"LISTs are also iterable. This means you can loop in it.\n\nYou can use both WHILE or FOR loops.","523ad5ef":"HOWEVER, a FOR LOOP cannot be used when u are multiplying.","9ea09800":"Getting a specific data in a list is done by indexing:\n\n(Python indexing starts at 0)","bda12ad4":"Also, notice the AUTOMATIC INDENTING.\n\nIn python, in a IF statement, FOR, WHILE, any operation, if u want to exit that command, you have to unindent","4ee37e37":"FOR LOOP:       range(START, END, ADDING_INCREMENT)","12f2562e":"Also, adding to a list also works.\n\nUse .append(....)","0fd08ed7":"Clearly this is not good. Say you wanted to go to \"Hello 100\".\n\nSo, a faster way is to use a LOOP.\n\nA LOOP is a method which calls itself until a condition is met.","d8b2ed24":"Just use an IF checker:","3d4c02bb":"The WHILE loop says \"while counter is less than 5, then do\".\n\nYou can see the counter getting updated.\n\nYou can also see I used {} and format. This just places the variable counter inside the string \/ text during printing.","ed63dd61":"(2) In this exercise, you need to:\n\n1. Start from 0.\n2. Continue adding 10 to it until 50.\n3.. If the number == 0, print \"Starting at 0\"\n4. If number == 20: print \"Kinda half way\"\n5. If number == 40: print \"Nearly at 50\"\n7. Anything else: print \"At {}\" where {} is replaced with the number.\n\nIt should look like this:\n\n\nStarting at 0\n\nAt 10\n\nKinda half way\n\nAt 30\n\nNearly at 50\n\nAt 50","b1e0c12f":"Now, we reassigned counter by multiplying by 2!","3c05817f":"<a id='Loops'><\/a>\n<h1> 2. Python Loops <\/h1>\n\nNow, let us introduce WHILE and FOR loops\n\nSay you want to print \"Hello 1\", \"Hello 2\", \"Hello 3\", \"Hello 4\".\n\nYou can do this:","e8715428":"<img src=\"https:\/\/previews.123rf.com\/images\/christianchan\/christianchan1503\/christianchan150300425\/37144675--now-it-s-your-turn-note-pinned-on-cork-.jpg\" style=\"width: 300px;\"\/>","3843cda0":"Sometimes, its better to use a FOR LOOP.\n\nSay you don't like to keep a counter. Instead, the same syntax can be:","2549fcbf":"<a id='Lists'><\/a>\n<h1> 3. Lists and Objects <\/h1>\n\nNow, let us introduces LISTS.\n\nA LIST is a collection of data or objects.","bb8f5366":"Once you are done, and satisfied with your work, let the tutor mark you.\n\nNote, if you can't get it, it's fine. Marks are awarded for trying the questions out. We don't mind if the output is wrong","c46eaa74":"<a id='If'><\/a>\n<h1> 4. If, Else, Elif <\/h1>\n\n**If, Else, Elif Procedure Flow**\n\nIn programming, it's very important to understand IF, ELSE and ELIF statements\n\nSay a variable fruit is assigned to \"apple\".","8a9cad12":"[<h1>CLICK to SKIP BELOW CODE TO CONTENT<\/h1>](#Content)","b33327e5":"Also, a confusing part for students is the variable update.\n\nSay you store a variable apples = 0.\n\nYou want to add 1 to it.","6057adfe":"Python 3 can easily do mathematical operations. Don't forget CTRL + ENTER.\n\nAlso, Kaggle AUTOSAVES. No need to save your work.","3c2022cd":"IMPORTANT - Python starts indexing at 0 and not 1.\n\nSo its 0,1,2,...,5\nand NOT 1,2,3,...,6","e93e68dd":"You can see above the 3 lines are NOT executed since apple = 'appl' != 'apple'\n\nThe two lines are executed instead\n\nNote == means \"is equal to\"\n!= means \"is NOT equal to\"\nalso >=, >, <=, <","b0055a04":"<h2> Welcome to MARK5826 Week 1!<\/h2>\n\n<h3> Thanks for taking this first time ever offered course! I hope you'll love it! <\/h3>\n\nLecturer In Charge: Junbum Kwon;Teaching Assistant: Daniel Han-Chen & James Lin\n\n<h2>AIM<\/h2>\n\nIn week 1, we want you to be exposed to the basics of programming, and the basics of Python.\n\nDuring the weeks, we will be using a Python pre-made package specifically designed for this course.\n\nWe have tried to limit code in order to make everyone be able to code properly and quickly.\n\n<h2> OPEN DISCUSSION <\/h2>\n\nIn MARK5826, we will have an **open discussion format**. We know students have issues, but sometimes they are shy or scared to say it out.\n\nTo solve this problem, you can anonymously write your questions in the public Google Doc: [CLICK HERE TO ACCESS](https:\/\/docs.google.com\/document\/d\/1wYTOEgpdBPlM8OXlTKHZ5FrDJSBj50DLphdgbcgRaIs\/edit?usp=sharing)\n\n<h2>Week Topics<\/h2>\n\n(You can click the links below)\n\n[SKIP BELOW CODE TO CONTENT](#Content)\n<hr>\n1.[Python Fundamentals](#Content)\n\n2.[Python Loops](#Loops)\n\n3.[Lists and Objects](#Lists)\n\n4.[If, Else, Elif](#If)\n\n5.[Binary Operations](#Binary)\n\n6.[Lab Questions](#Lab)\n","01019fb0":"You can see the use of IF, ELIF (else if) and ELSE.\n\nWhat this means is IF something is TRUE, then execute the below indented code.\n\nIf NOT, then check the ELIF command. If that fails, keep checking the ELIF parts.\n\nOr ELSE --> do the below indented code.\n\nMultiple ELIFs are acceptable"}}