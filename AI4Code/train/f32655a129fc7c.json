{"cell_type":{"0d7d536d":"code","b8426686":"code","22be3902":"code","6bd573a6":"code","11ecbea1":"code","f535c0f1":"code","d0fcd811":"code","8b9e29a4":"code","55cc29ee":"code","0ed3b33b":"code","284dd559":"code","b7ad6894":"code","f89b91ef":"code","d5de1b2e":"code","b70edc3d":"code","5c023413":"code","cb14f412":"code","3bc0b467":"code","0758d9f4":"code","7cef6629":"code","c079dfc0":"code","8da7d151":"code","b09b5c81":"code","7e1f7fe9":"code","9c246fd6":"code","96136b10":"code","2574f007":"code","e3fde1e6":"code","3e5051db":"code","26cafe52":"code","181c7212":"code","8a5e9d72":"code","e940f133":"code","86c72484":"code","21764aff":"code","76cc3510":"code","b08a66aa":"code","6ffbf5df":"markdown","6434e18d":"markdown","af94dbc3":"markdown","6f556349":"markdown","7b5f4440":"markdown","9162b8ce":"markdown","adb1fcc1":"markdown","4ef3dccc":"markdown","a62e5980":"markdown"},"source":{"0d7d536d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8426686":"import tensorflow\nimport keras","22be3902":"BASE_PATH = \"\/kaggle\/input\"\nEXPORT_PATH = '\/kaggle\/working'\nEXPORT_NAME = 'airplane_box_data.csv'\n\nIMAGES_PATH = os.path.sep.join([BASE_PATH, \"airplanes-images\/airplanes\"])\nANNOTS_PATH = os.path.sep.join([BASE_PATH, \"airplane-annotations\/Airplanes_Side_2\"])\nDATASET_PATH = os.path.sep.join([EXPORT_PATH, EXPORT_NAME])","6bd573a6":"def filelist(root, file_type):\n    \"\"\"Returns a fully-qualified list of filenames under root directory\"\"\"\n    return [os.path.join(directory_path, f) for directory_path, directory_name, \n            files in os.walk(root) for f in files if f.endswith(file_type)]","11ecbea1":"num_found = 0\nimage_anno_grouped = []\n\na = filelist(ANNOTS_PATH, '.mat')\nb = filelist(IMAGES_PATH, '.jpg')\n\nprint('Finding annotations for images')\nfor annotation in a:\n    number = annotation.split('\/')[-1].split('_')[-1].split('.')[0]\n    image_path = IMAGES_PATH + f'\/image_{number}.jpg'\n#     print(image_path)\n    if image_path in b:\n#         print('Found!')\n        num_found += 1\n        image_anno_grouped.append([image_path, annotation])\n    else:\n        print('Not found.')\nprint(f'Found: {num_found}\/{len(a)}')\n","f535c0f1":"from scipy.io import loadmat\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches","d0fcd811":"def plot_nth_image(image_anno_grouped: list, n: int = 0, debug: bool = False):\n    image_path = image_anno_grouped[n][0]\n    anno_path = image_anno_grouped[n][1]\n    \n    if debug:\n        print(anno_path)\n        print(image_path)\n    \n    matfile = loadmat(anno_path)\n    box_coords = list(matfile['box_coord'][0])\n    \n    if debug:\n        print(box_coords)\n        \n    im = plt.imread(image_path)\n\n    # Display the image\n    plt.imshow(im)\n\n    # Get the current reference\n    ax = plt.gca()\n\n    # Create a Rectangle patch\n    point_1 = box_coords[2]\n    point_2 = box_coords[0]\n    point_3 = box_coords[3] - box_coords[2]\n    point_4 = box_coords[1] - box_coords[0]\n    # rect = patches.Rectangle((52,27),346-52,124-27,linewidth=1,edgecolor='r',facecolor='none')\n    rect = patches.Rectangle((point_1, point_2),point_3,point_4,linewidth=1,edgecolor='r',facecolor='none')\n\n    # Add the patch to the Axes\n    ax.add_patch(rect)\n    \n    return box_coords\n    ","8b9e29a4":"box_coords = plot_nth_image(image_anno_grouped, n=85, debug=False)\nprint(box_coords)","55cc29ee":"import pandas as pd","0ed3b33b":"data = np.array([]).reshape(0,5)\n\nfor item in image_anno_grouped:\n    image_path = item[0]\n    anno_path = item[1]\n    \n    matfile = loadmat(anno_path)\n    box_coords = list(matfile['box_coord'][0])\n    \n    row = np.array([image_path, *box_coords]).reshape(1, 5)\n    \n    data = np.vstack([data, row])\n\nheaders = ['filename', 'startX', 'startY', 'endX', 'endY'] \npdf = pd.DataFrame(data, columns=headers)","284dd559":"# export_path = f'{EXPORT_PATH}\/{EXPORT_NAME}'\n\nprint(f'Exporting csv as: {EXPORT_NAME}')\npdf.to_csv(EXPORT_NAME, index=False)\nprint('Done.')","b7ad6894":"headers = ['filename', 'startX', 'startY', 'endX', 'endY'] \npdf = pd.read_csv(DATASET_PATH)","f89b91ef":"pdf","d5de1b2e":"# import the necessary packages - from tut\n# from pyimagesearch import config\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2","b70edc3d":"data_np = np.array(pdf)\ndata = []\ntargets = []\nfilenames = []\n\nfor row in data_np:\n    (imagePath, startX, startY, endX, endY) = row\n    \n    image = cv2.imread(imagePath)\n    (h, w) = image.shape[:2]\n    \n    # scale the bounding box coordinates relative to the spatial\n    # dimensions of the input image\n    startX = float(startX) \/ w\n    startY = float(startY) \/ h\n    endX = float(endX) \/ w\n    endY = float(endY) \/ h\n    \n    # load the image and preprocess it\n    image = load_img(imagePath, target_size=(224, 224))\n    image = img_to_array(image)\n    # update our list of data, targets, and filenames\n    data.append(image)\n    targets.append((startX, startY, endX, endY))\n    filenames.append(imagePath.split('\/')[-1])","5c023413":"# convert the data and targets to NumPy arrays, scaling the input\n# pixel intensities from the range [0, 255] to [0, 1]\ndata = np.array(data, dtype=\"float32\") \/ 255.0\ntargets = np.array(targets, dtype=\"float32\")\n\n# partition the data into training and testing splits using 90% of\n# the data for training and the remaining 10% for testing\nsplit = train_test_split(data, targets, filenames, test_size=0.10, random_state=42)\n\n# unpack the data split\n(trainImages, testImages) = split[:2]\n(trainTargets, testTargets) = split[2:4]\n(trainFilenames, testFilenames) = split[4:]\n\n# write the testing filenames to disk so that we can use then\n# when evaluating\/testing our bounding box regressor\nprint(\"[INFO] saving testing filenames...\")\nf = open('test_images.txt', \"w\")\nf.write(\"\\n\".join(testFilenames))\nf.close()\nprint('Done')","cb14f412":"MODEL_NAME = 'detector.h5'\n\nINIT_LR = 1e-4\nNUM_EPOCHS = 3\nBATCH_SIZE = 32","3bc0b467":"# load the VGG16 network, ensuring the head FC layers are left off\nvgg = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n\n# freeze all VGG layers so they will *not* be updated during the training process\nvgg.trainable = False\n\n# flatten the max-pooling output of VGG\nflatten = vgg.output\nflatten = Flatten()(flatten)\n\n# construct a fully-connected layer header to output the predicted bounding box coordinates\nbboxHead = Dense(128, activation=\"relu\")(flatten)\nbboxHead = Dense(64, activation=\"relu\")(bboxHead)\nbboxHead = Dense(32, activation=\"relu\")(bboxHead)\nbboxHead = Dense(4, activation=\"sigmoid\")(bboxHead)\n\n\n# construct the model we will fine-tune for bounding box regression\nmodel = Model(inputs=vgg.input, outputs=bboxHead)","0758d9f4":"# initialize the optimizer, compile the model, and show the model summary\nopt = Adam(lr=INIT_LR)\nmodel.compile(loss=\"mse\", optimizer=opt)\nprint(model.summary())","7cef6629":"# train the network for bounding box regression\nprint(\"[INFO] training bounding box regressor...\")\nH = model.fit(\n    trainImages, trainTargets,\n    validation_data=(testImages, testTargets),\n    batch_size=BATCH_SIZE,\n    epochs=NUM_EPOCHS,\n    verbose=1)","c079dfc0":"# serialize the model to disk\nprint(\"[INFO] saving object detector model...\")\nmodel.save(MODEL_NAME, save_format=\"h5\")","8da7d151":"# plot the model training history\nN = NUM_EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.title(\"Bounding Box Regression Loss on Training Set\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=\"lower left\")\nplt.savefig('plot.png')","b09b5c81":"!pip install imutils","7e1f7fe9":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\n\nimport imutils\nimport cv2","9c246fd6":"model_path = f'..\/input\/bounding-box-detector\/{MODEL_NAME}'\nmodel = load_model(model_path)\nmodel.summary()","96136b10":"testImages[0].shape","2574f007":"img_number = 4\n\ntest_image = pdf.iloc[img_number]['filename']\ntest_box = [pdf.iloc[img_number]['startX'], pdf.iloc[img_number]['startY'], pdf.iloc[img_number]['endX'], pdf.iloc[img_number]['endY']]","e3fde1e6":"image = load_img(test_image, target_size=(224, 224))\nimage = img_to_array(image) \/ 255.0\nimage = np.expand_dims(image, axis=0)","3e5051db":"image.shape","26cafe52":"preds = model.predict(image)[0]\n(startX, startY, endX, endY) = preds","181c7212":"preds_squashed = list(np.array([startX, startY, endX, endY]) * 224)\npreds_squashed","8a5e9d72":"pred_box = preds_squashed","e940f133":"image = cv2.imread(test_image)\nimage = imutils.resize(image)\n(h, w) = image.shape[:2]","86c72484":"(h, w)","21764aff":"startX = int(startX * w)\nstartY = int(startY * h)\nendX = int(endX * w)\nendY = int(endY * h)\n\npred_box = [startX, startY, endX, endY]","76cc3510":"print(f' test coords: {test_box}')\nprint(f' pred coords: {pred_box}')","b08a66aa":"# box_coords = [startX, endX, startY, endY]\n# box_coords = test_box\nbox_coords = pred_box\n\nprint(box_coords)\n        \nim = plt.imread(test_image)\n# im = image[0]\n\n# Display the image\nplt.imshow(im)\n\n# Get the current reference\nax = plt.gca()\n\n# Create a Rectangle patch\npoint_1 = box_coords[2]\npoint_2 = box_coords[0]\npoint_3 = box_coords[3] - box_coords[2]\npoint_4 = box_coords[1] - box_coords[0]\n# rect = patches.Rectangle((52,27),346-52,124-27,linewidth=1,edgecolor='r',facecolor='none')\nrect = patches.Rectangle((point_1, point_2),point_3,point_4,linewidth=1,edgecolor='r',facecolor='none')\n\n# Add the patch to the Axes\nax.add_patch(rect)","6ffbf5df":"## Read in data from CSV","6434e18d":"### Model Predictions","af94dbc3":"## Extract box-coords and export with image paths","6f556349":"[](http:\/\/)","7b5f4440":"## Link images to annotations","9162b8ce":"## Plot image with box","adb1fcc1":"So it looks like something is wrong with the scaling of the image. The start x and y coords look accurate, but all the boxes need to be wider. \n\nIn the model training step, we train on squashed (square) images, so it might have something to do with that. However, we have played around with post-scoring scaling without any succeess. \n\nIt can also simply be that we are not plotting the box coords correctly, since we need to do some transforms on the points to get it to the \"box shape\". Although, the method used above seems to work on the training data.","4ef3dccc":"## Scale data","a62e5980":"## Prepare model"}}