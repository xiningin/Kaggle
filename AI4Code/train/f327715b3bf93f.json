{"cell_type":{"518f5af6":"code","9e3a0255":"code","1bfdceed":"code","1223c4b1":"code","245d89d5":"code","838d94b4":"code","d8982c72":"code","35eaaae2":"code","588d1ac9":"code","94e62853":"code","9df9609b":"code","f12a57cb":"code","9644a395":"code","b0594bf6":"code","24a03db1":"markdown","17d1cbb5":"markdown","9cffc421":"markdown","2f93796d":"markdown","2b9fce11":"markdown","2b36dc99":"markdown","96c97d2c":"markdown","095eed35":"markdown","bf31e065":"markdown","da60d458":"markdown","f1c5ff4c":"markdown","eab9008b":"markdown","5ba70d52":"markdown","a029ff1d":"markdown","d0a8ce17":"markdown","04d88e52":"markdown","67b253fd":"markdown"},"source":{"518f5af6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","9e3a0255":"from keras.datasets import mnist\n(train_x, train_y), (test_x, test_y) = mnist.load_data()","1bfdceed":"fig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(10):\n    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n    ax.imshow(train_x[idx], cmap='Blues_r')\n    ax.set_title(str(train_y[idx]),fontsize=25)","1223c4b1":"img = train_x[0]\n\nfig = plt.figure(figsize = (12,12)) \nax = fig.add_subplot(111)\nax.imshow(img, cmap='Blues_r')\nwidth, height = img.shape\nthresh = img.max()\/2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y],2) if img[x][y] !=0 else 0\n        ax.annotate(str(val), xy=(y,x),\n                    horizontalalignment='center',\n                    verticalalignment='center',\n                    color='white' if img[x][y]<thresh else 'black')","245d89d5":"# Reshape to 2D data\ntrain_x = train_x.reshape(train_x.shape[0], -1)\nprint(train_x.shape)\n\nsample_size = 5000\n# Use only the top 1000 data for training\ntrain_x = pd.DataFrame(train_x[:sample_size, :])\ntrain_y = train_y[:sample_size]","838d94b4":"from sklearn.decomposition import PCA\n\npca = PCA()\nx_pca = pca.fit_transform(train_x)\nmarkers=['o','v','^','<','>','8','s','P','*','X']\n# plot in 2D by class\nplt.figure(figsize=(10,10))\nfor i,marker in enumerate(markers):\n    mask = train_y == i\n    plt.scatter(x_pca[mask, 0], x_pca[mask, 1], label=i, s=10, alpha=1,marker=marker)\nplt.legend(bbox_to_anchor=(1.00, 1), loc='upper left',fontsize=15)\n","d8982c72":"from sklearn.decomposition import TruncatedSVD\n\ntsvd = TruncatedSVD()\nx_tsvd = tsvd.fit_transform(train_x)\nmarkers=['o','v','^','<','>','8','s','P','*','X']\n# plot in 2D by class\nplt.figure(figsize=(10,10))\nfor i,marker in enumerate(markers):\n    mask = train_y == i\n    plt.scatter(x_tsvd[mask, 0], x_tsvd[mask, 1], label=i, s=10, alpha=1,marker=marker)\nplt.legend(bbox_to_anchor=(1.00, 1), loc='upper left',fontsize=15)\n","35eaaae2":"from sklearn.decomposition import NMF\n\nnmf = NMF(n_components=2, init='random', random_state=0)\nx_nmf = nmf.fit_transform(train_x)\nmarkers=['o','v','^','<','>','8','s','P','*','X']\n# plot in 2D by class\nplt.figure(figsize=(10,10))\nfor i,marker in enumerate(markers):\n    mask = train_y == i\n    plt.scatter(x_nmf[mask, 0], x_nmf[mask, 1], label=i, s=10, alpha=1,marker=marker)\nplt.legend(bbox_to_anchor=(1.00, 1), loc='upper left',fontsize=15)","588d1ac9":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nlda = LDA(n_components=2)\nx_lda = lda.fit_transform(train_x, train_y)\n\nplt.figure(figsize=(10,10))\nfor i,marker in enumerate(markers):\n    mask = train_y == i\n    plt.scatter(x_lda[mask, 0], x_lda[mask, 1], label=i, s=10, alpha=1,marker=marker)\nplt.legend(bbox_to_anchor=(1.00, 1), loc='upper left',fontsize=15)","94e62853":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2)\nx_tsne = tsne.fit_transform(train_x)\n\nplt.figure(figsize=(10,10))\nfor i,marker in enumerate(markers):\n    mask = train_y == i\n    plt.scatter(x_tsne[mask, 0], x_tsne[mask, 1], label=i, s=10, alpha=1,marker=marker)\nplt.legend(bbox_to_anchor=(1.00, 1), loc='upper left',fontsize=15)","9df9609b":"import umap\n\num = umap.UMAP()\nx_umap = um.fit_transform(train_x)\n\nplt.figure(figsize=(10,10))\nfor i,marker in enumerate(markers):\n    mask = train_y == i\n    plt.scatter(x_umap[mask, 0], x_umap[mask, 1], label=i, s=10, alpha=1,marker=marker)\nplt.legend(bbox_to_anchor=(1.00, 1), loc='upper left',fontsize=15)\n","f12a57cb":"import umap.plot\nmapper = umap.UMAP().fit(train_x)\numap.plot.connectivity(mapper, show_points=True)","9644a395":"umap.plot.points(mapper, labels=train_y, theme='fire')","b0594bf6":"import plotly\nimport plotly.express as px\nfrom umap import UMAP\n\numap_3d = UMAP(n_components=3, init='random', random_state=0)\nx_umap = umap_3d.fit_transform(train_x)\numap_df = pd.DataFrame(x_umap)\ntrain_y_sr = pd.Series(train_y,name='label')\nprint(type(x_umap))\nnew_df = pd.concat([umap_df,train_y_sr],axis=1)\nfig = px.scatter_3d(\n    new_df, x=0, y=1, z=2,\n    color='label', labels={'color': 'number'}\n)\nfig.update_traces(marker_size=1)\nfig.show()","24a03db1":"---------------------------------------------------------------------\n# Loading library and dataset","17d1cbb5":"The effect of each dimension reduction is identified using the MNIST dataset. ","9cffc421":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction\n\n![](https:\/\/miro.medium.com\/max\/1156\/1*Tom80X_DVuAhT4p6SiKLIA.gif) \n\nPicture Credit: https:\/\/miro.medium.com\n\n**The Curse of Dimensionality**\n\n> Dimensionally cursed phenomena occur in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Curse_of_dimensionality","2f93796d":"------------------------------------------\n# AutoEncoder\n\nAutoEncoder is a dimension reduction method using a neural network. By learning a neural network that outputs the same value as the input using an intermediate layer smaller than the input dimension, a lower-dimensional representation that can reproduce the original data can be learned.\n\nThere are several types of AutoEncoders.\n**The note below summarizes the usage examples of AutoEncoder. Reading it will help you a lot in understanding AutoEncoder.**\n\n* Denoising AutoEncoder: https:\/\/www.kaggle.com\/ohseokkim\/simple-denoising-autoencoder\n* Anomaly Dection by AutoEncoder: https:\/\/www.kaggle.com\/ohseokkim\/anomaly-dectection-by-autoencoder\n\n<hr style=\"border: solid 3px blue;\">","2b9fce11":"-------------------------------------------------------\n# UMAP\nUMAP (Uniform Manifold Approximation and Projection), which is faster than t-SNE and separates the data space well, has been proposed for nonlinear dimensionality reduction. In other words, it can process very large datasets quickly and is suitable for sparse matrix data. Furthermore, compared to t-SNE, it has the advantage of being able to embed immediately when new data comes in from other machine learning models.","2b36dc99":"__________________________________________________________________\n# PCA\nPCA is the most representative method of dimensionality reduction. This is a method of re-axis of multidimensional data in the direction of large variance. The greater the dependence between variables, the smaller the principal component can represent the original data.\nHowever, since it is assumed that each feature follows a normal distribution, it is not appropriate to apply a variable with a distorted distribution to PCA.","96c97d2c":"------------------------------------------------------------------------------------------\n# Checking train dataset","095eed35":"# Truncated SVD\nTruncated SVD is a method of extracting and decomposing only the upper part of the diagonal elements in the sigma matrix, that is, the upper part of the singular values.\nWith this decomposition, the original matrix cannot be accurately restored because it artificially decomposes $U\u2211V^T$ of smaller dimensions.\nHowever, despite the data information being compressed and decomposed, it is possible to approximate the original matrix to a considerable degree.","bf31e065":"The figure above was drawn by reducing the dimension to 3D with UMAP.\n\nIf you compare it with the previous two-dimensional plot, you can see that it is visually complex and the points are distributed sparsely in space.\n**If the dimension is increased further, the above phenomenon will become more severe.**\n\n**It's a small experiment, but we've experienced the curse of a dimension.**","da60d458":"**Dimension reduction** is to create a new dimension data set by reducing the dimension of a multidimensional data set composed of very many features. In general, as the dimension increases, the distance between data points becomes exponentially farther, and it has a sparse structure. In the case of a data set consisting of hundreds or more features, the prediction reliability is lower than that of a model trained on relatively few dimensions. Also, if there are many features, the correlation between individual features is likely to be high. In a linear model such as linear regression, when the correlation between input variables is high, the predictive performance of the model decreases due to the multicollinearity problem.\n\nIn this notebook, we summarize the dimensionality reduction methods that can interpret data more intuitively by reducing features by reducing the dimensions of multidimensional features.\n\nThe dataset uses the commonly used **MNIST dataset**. The MNIST dataset we commonly use has a size of $28 x 28$. When looking at this dataset through dimensionality reduction, we will check if there is any regularity and how labels are clustered according to each dimensionality reduction method.","f1c5ff4c":"## UMAP connectivity plot\n\n> UMAP works by constructing an intermediate topological representation of the approximate manifold the data may have been sampled from. In practice this structure can be simplified down to a weighted graph. Sometimes it can be beneficial to see how that graph (representing connectivity in the manifold) looks with respect to the resulting embedding. It can be used to better understand the embedding, and for diagnostic purposes.\n\nRef: https:\/\/umap-learn.readthedocs.io","eab9008b":"## Another UMAP plot","5ba70d52":"------------------------------------------------------------\n# UMAP 3D plot","a029ff1d":"---------------------------------------------\n# t-SNE\nt-SNE is often used for visualization purposes by compressing data on a two-dimensional plane. Points that are close to the original feature space are also expressed in a two-dimensional plane after compression. Since the nonlinear relationship can be identified, the model performance can be improved by adding the compression results expressed by these t-SNEs to the original features. However, since the computation cost is high, it is not suitable for compression exceeding two or three dimensions.","d0a8ce17":"It looks like an orderly universe. A few galaxies seem to be visible as well.","04d88e52":"------------------------------------------\n# LDA\nLDA is a method of dimensionality reduction in the classification problem of supervised learning. It finds a low-dimensional feature space that can classify the training data well, and reduces the dimensionality by projecting the original features into that space.","67b253fd":"-------------------------------------------------------------------\n# NMF\n\nNMF is a variant of the Low-Rank Approximation method like SVD. However, it must be guaranteed that the values \u200b\u200bof all elements in the source matrix are positive. NMF decomposes a matrix into W and H matrices. The W matrix indicates how well the values \u200b\u200bof the latent elements correspond to the source matrix. The H matrix represents how this latent element is composed of sour features."}}