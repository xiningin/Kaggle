{"cell_type":{"918e71fc":"code","e5533395":"code","79af5a37":"code","8c82a6f1":"code","a54c55c8":"code","41cb5ee2":"code","f11aacad":"code","d7737621":"code","e65fe1fd":"code","02024b45":"code","29d96427":"code","fc3f035d":"code","c74f81b4":"code","1993b5e6":"code","40a1ab71":"code","58b81650":"code","61d8f152":"code","9dead309":"code","96140cd0":"code","da6c5194":"code","b0f15c03":"code","607672be":"code","9ed32505":"code","5e705662":"code","e5f78d59":"code","d1bf0745":"code","8560df68":"code","1df64ec7":"code","811ff1a9":"code","362dcb65":"code","52a66698":"code","bd418e64":"markdown","514961de":"markdown","0ff1fd1c":"markdown","4ef57ed5":"markdown","40446026":"markdown","b15c1464":"markdown","26be7a92":"markdown","6abb3945":"markdown","52f57d5b":"markdown","9649aa3e":"markdown","a40f9a9f":"markdown","97db9f84":"markdown"},"source":{"918e71fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n%matplotlib inline\nplt.style.use('fivethirtyeight')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5533395":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","79af5a37":"df.info()","8c82a6f1":"features = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\ntarget = ['Outcome']","a54c55c8":"print('No. of Missing Values:\\n' + str(df.isnull().isnull().sum()))","41cb5ee2":"plt.figure(figsize = (20,16))\nn = 0\nfor i in features + target:\n    n += 1\n    plt.subplot(3,3,n)\n    plt.subplots_adjust(hspace = 0.4,wspace = 0.2)\n    sns.boxplot(x=df[i])\nplt.show()","f11aacad":"print('Dataset shape before outlier rejection: ',df.shape)\ndf = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\ndf = df.reset_index().drop(['index'],axis = 1)\nprint('Dataset shape after outlier rejection: ',df.shape)","d7737621":"df.value_counts()","e65fe1fd":"plt.figure(figsize = (20,16))\nn = 0\nfor i in features + target:\n    n += 1\n    plt.subplot(3,3,n)\n    plt.subplots_adjust(hspace = 0.4,wspace = 0.2)\n    sns.boxplot(x=df[i])\nplt.show()","02024b45":"sns.set(style = 'whitegrid')\nsns.pairplot(df)","29d96427":"corr = df[features].corr()\nplt.figure(figsize=(8,8))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features, yticklabels= features, alpha = 0.7,   cmap= 'coolwarm')\nplt.show()","fc3f035d":"corrMatrix = pd.DataFrame(df[df.columns[0:]].corr()['Outcome'][:-1])\nplt.figure(1,figsize =(20,6))\nsns.set(style=\"whitegrid\")\nsns.barplot(x = corrMatrix.index,y = corrMatrix['Outcome'],data = corrMatrix)\nplt.title('Correlation of Outcome to other features')\nplt.ylabel('Correlation with Outcome')\nplt.xlabel('Features')\nplt.show()","c74f81b4":"plt.figure(figsize = (20,16))\nn = 0\nfor hues_crit in ['Glucose','BMI']:\n    for i,j in zip(['Pregnancies','SkinThickness','BMI','BloodPressure'],['Age','Insulin','SkinThickness','Age']):\n        n += 1\n        plt.subplot(4,2,n)\n        plt.subplots_adjust(hspace = 0.4,wspace = 0.2)\n        sns.scatterplot(x = df[i].where(df['Outcome'] == 1),y = df[j].where(df['Outcome'] == 1),hue = df[hues_crit])\nplt.legend()\nplt.show()","1993b5e6":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nscaler = StandardScaler()","40a1ab71":"def plot_history(history,n):\n    print(\"PLOT FOR {}\".format(n))\n    plt.plot(history.history['binary_accuracy'])\n    plt.plot(history.history['val_binary_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'valid'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'valid'], loc='upper left')\n    plt.show()","58b81650":"def grid_search(est,param_grid):\n    fin_model = GridSearchCV(estimator=est,param_grid=param_grid,cv = 10,n_jobs = -1)\n    return fin_model","61d8f152":"def neigh(X_train,y_train,X_test,y_test):\n    pipe = Pipeline([('neigh', KNeighborsClassifier())])\n    param_grid = {'neigh__n_neighbors':[i for i in range(1,20)],\n                 'neigh__weights':['uniform', 'distance']}\n    pipe = grid_search(pipe,param_grid)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    print('KNN_CLASSIFIER: ')\n    print(classification_report(y_test, y_pred,zero_division = 1))\n    print('PIPE SCORE: ', pipe.score(X_test, y_test))\n    print('ACCURACY SCORE: ',accuracy_score(y_test, y_pred))\n    print(pipe.best_params_)","9dead309":"def forest(X_train,y_train,X_test,y_test):\n    #Parameter Grid\n    pipe = Pipeline([('forest', RandomForestClassifier(random_state = 0))])\n    param_grid = {'forest__n_estimators':[10,50,140],\n                 'forest__max_depth' : [1,2,3,4],\n                 'forest__max_features':['sqrt', 'log2']}\n    pipe = grid_search(pipe,param_grid)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    #Evaluation Phase\n    print('RANDOM FOREST: ')\n    print(classification_report(y_test, y_pred,zero_division = 1))\n    print('PIPE SCORE: ', pipe.score(X_test, y_test))\n    print('ACCURACY SCORE: ',accuracy_score(y_test, y_pred))\n    print(pipe.best_params_)","96140cd0":"def svm(X_train,y_train,X_test,y_test):\n    pipe = Pipeline([('svm', SVC(gamma = 'auto',random_state = 0))])\n    param_grid = {'svm__kernel':['poly', 'rbf'],\n                 'svm__degree':[1,2,3],\n                 'svm__C' : [1,2,3]}\n    pipe = grid_search(pipe,param_grid)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    print('SVM: ')\n    print(classification_report(y_test, y_pred,zero_division = 1))\n    print('PIPE SCORE: ', pipe.score(X_test, y_test))\n    print('ACCURACY SCORE: ',accuracy_score(y_test, y_pred))\n    print(pipe.best_params_)","da6c5194":"# Scaling the features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# Separating out the features\nX = df.loc[:, features].values\n# Separating out the target\ny = df.loc[:,target].values\n# Standardizing the features\nX = scaler.fit_transform(X)\nfrom sklearn.model_selection import train_test_split as tts\nX_train,X_test,y_train,y_test = tts(X,y,test_size = 0.2,random_state = 7)\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","b0f15c03":"neigh(X_train,y_train,X_test,y_test)","607672be":"forest(X_train,y_train,X_test,y_test)","9ed32505":"svm(X_train,y_train,X_test,y_test)","5e705662":"from xgboost import XGBClassifier\nxgb = XGBClassifier(learning_rate = 0.001)\nxgb.fit(X_train,y_train)\n#make predictions for test data\ny_pred = xgb.predict(X_test)\nprint(classification_report(y_test, y_pred,zero_division = 1))\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","e5f78d59":"res = pd.DataFrame(columns = ['loss','acc','lr','opt','layers','m_value','n_value','epochs','add_dropout'])\npd.set_option('max_colwidth', 400)","d1bf0745":"import tensorflow as tf\nfrom keras.layers import Dense,Dropout,BatchNormalization\nfrom keras import Sequential\nfrom keras.optimizers import Adam,SGD,RMSprop,Nadam,Adadelta\nimport gc","8560df68":"def model(X_train,y_train,X_test,y_test,lr = 0.001,opt = Adam,layers = 3,m=10,n=100,epochs = 10,add_dropout = False):\n    model = Sequential()\n    model.add(Dense(units = 10, activation = 'relu',input_shape = (8,)))\n    for i in range(layers):\n        model.add(Dense(units = m, activation = 'relu'))\n        model.add(Dense(units = n, activation = 'relu'))\n        if(add_dropout == True):\n            model.add(Dropout(0.2))\n    model.add(Dense(units = 1,activation = 'sigmoid'))\n    model.compile(optimizer = opt(lr = lr),loss = 'binary_crossentropy',metrics = ['binary_accuracy'])\n    model.fit(X_train,y_train,epochs= epochs,validation_split = 0.1,verbose = 0)\n    loss,acc = model.evaluate(X_test,y_test)\n    return {'loss':loss,\n            'acc':acc,\n            'lr':lr,\n            'opt':str(opt),\n            'layers':(layers*2 + 2),#Excluding dropouts\n            'm_value':m,\n            'n_value':n,\n            'epochs': epochs,\n            'add_dropout': add_dropout}","1df64ec7":"#for lr in [0.005,0.001,0.0005]:\n#    for opt in [RMSprop,Adam]:\n#        for layers in [2,3,4]:\n#            for m in [10,30,50,100]:\n#                for n in [10,30,50,100]:\n#                    for epochs in [5,10,20]:\n#                        for add_dropout in [False,True]:\n#                            res = res.append(model(X_train,y_train,X_test,y_test,lr,opt,layers,\n#                                                   m,n,epochs,add_dropout),ignore_index = True)","811ff1a9":"ad_f = pd.read_csv('..\/input\/final-search-result\/ad_f.csv')\nad_t = pd.read_csv('..\/input\/final-search-result\/ad_t.csv')\nrms_f = pd.read_csv('..\/input\/final-search-result\/rms_f.csv')\nrms_t = pd.read_csv('..\/input\/final-search-result\/rms_t.csv')\nfin = ad_f\nfin = fin.append([ad_t,rms_f,rms_t])\nfin = fin.reset_index().drop(['index','Unnamed: 0'],axis = 1)\nfin","362dcb65":"fin.where(fin['loss']<0.45).dropna()","52a66698":"fin.where(fin['acc'] > 0.79).dropna()","bd418e64":"* **Pregnancies** : No. of Pregnancies the person been through\n\n* **Glucose** (or) Blood Sugar Level: Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\nA blood sugar level less than 140 mg\/dL (7.8 mmol\/L) is normal. A reading of more than 200 mg\/dL (11.1 mmol\/L) after two hours indicates diabetes. A reading between 140 and 199 mg\/dL (7.8 mmol\/L and 11.0 mmol\/L) indicates prediabetes.\n\n* **BloodPressure** : Diastolic blood pressure (mm Hg).\nA normal blood pressure level is less than 120\/80 mmHg. No matter your age, you can take steps each day to keep your blood pressure in a healthy range.","514961de":"# DATA\n\n**Context**\n\nThis dataset is originally from the **National Institute of Diabetes and Digestive and Kidney Diseases**. The objective of the dataset is to **diagnostically predict whether or not a patient has diabetes**, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, **all patients here are females at least 21 years old of Pima Indian heritage**.\n\n**Content**\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n**Acknowledgements**\n\nSmith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988).** Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265).** IEEE Computer Society Press.\n\n**Inspiration**\n\nCan you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?","0ff1fd1c":"![image.png](attachment:image.png)","4ef57ed5":"Outcome is Binary...\n\nNot a very evenly distributed dataset, we have quite a lot of outliers.","40446026":"# Model Building","b15c1464":"## Distribution","26be7a92":"### Neural Nets","6abb3945":"# Maximum Test Accuracy achieved = 80.4348% \n# Minimum Loss = 0.446542\t","52f57d5b":"* **SkinThickness** : Triceps skin fold thickness (mm). Skin fold thickness measurement provides an estimated size of the subcutaneous fat, which is the layer of subcutaneous tissue and composed of adipocytes. Subcutaneous fat is the major determinant of insulin sensitivity and has a strong association with insulin resistance. [Cite](https:\/\/www.semanticscholar.org\/paper\/Skin-Fold-Thickness-in-Diabetes-Mellitus%3A-A-Simple-Selvi-Pavithra\/5d68b7a7391272feb9a737f4d69539483deb2556)\n\n* **Insulin** : 2-Hour serum insulin (mu U\/ml). Insulin is a peptide hormone produced by beta cells of the pancreatic islets; it is considered to be the main anabolic hormone of the body. It regulates the metabolism of carbohydrates, fats and protein by promoting the absorption of glucose from the blood into liver, fat and skeletal muscle cells. [Cite](https:\/\/www.google.com\/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjp_O2S8bHtAhVAzzgGHeJwBhAQmhMwHXoECC8QAg&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FInsulin&usg=AOvVaw1shvsn0GOvC5LJCRsIorHn)\n\n* **BMI** : Body mass index (weight in kg\/(height in m)^2)\n\n* **DiabetesPedigreeFunction** : It provides some data on diabetes mellitus history in relatives and the genetic relationship of those relatives to the patient. This measure of genetic influence gave us an idea of the hereditary risk one might have with the onset of diabetes mellitus. \n\n* **Age** : Age (years)\n\n* **Outcome** : Class variable (0 or 1) 268 of 768 are 1, the others are 0","9649aa3e":"# Exhaustive Hyperparameter Search","a40f9a9f":"### Correlated:\n* Pregnancies and Age\n* Skin Thickness and Insulin\n* BMI and Skin Thickness\n* Blood Pressure and Age\n\n* *High Glucose & BMI levels are most likely the reasons of Diabetes*\n\nBut no two variables are correlated enought to perform PCA\n\n### Plotting these variables to study only members who got Diabetes","97db9f84":"# Exploratory Data Analysis"}}