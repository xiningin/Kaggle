{"cell_type":{"d7a2385d":"code","25619e5a":"code","35efcd17":"code","6b2722b8":"code","bb5a035b":"code","efd7b32c":"code","664e5fc0":"code","7d4b5dbc":"code","04c5f54f":"code","c1dd1620":"code","8514ca3c":"code","bf91c694":"code","b1397c10":"code","e5939aea":"code","f01a0d07":"code","4d9260c6":"code","a16168eb":"code","51cad95e":"code","d17c8b9e":"code","a87db29b":"code","e5ac0aa4":"code","6feb5980":"code","974d3afc":"code","1d0b7a7f":"code","b808e4b6":"code","78c6393f":"code","dc89727f":"code","42779847":"code","da6f017a":"code","278493e2":"code","41496d3e":"code","1c2e50ef":"markdown","1e2f914b":"markdown","601cb649":"markdown","2c3a64ab":"markdown","b63fc46b":"markdown","14fbf3aa":"markdown","0b4ebd6c":"markdown","d24c76ce":"markdown","891a8351":"markdown","9074388a":"markdown","6b02a555":"markdown","35c28eea":"markdown","5b182475":"markdown","bcd38762":"markdown","118f5762":"markdown","8127e943":"markdown","cfbccba9":"markdown","6e9923ed":"markdown","0c7f2cba":"markdown","b753d014":"markdown","d97f41d3":"markdown","6637bbc0":"markdown","115b6fbb":"markdown","13558cdf":"markdown","366cc72c":"markdown","0f6b223a":"markdown","071391f2":"markdown","36d18deb":"markdown","435a9d59":"markdown","986e0c45":"markdown","18a19070":"markdown","dadbc2fe":"markdown","0cef37a5":"markdown","06230a5c":"markdown","e09875ec":"markdown","8adec0d7":"markdown","509795c2":"markdown","51c8493b":"markdown","f8aaee76":"markdown"},"source":{"d7a2385d":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nimport squarify\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport squarify\nfrom sklearn.decomposition import PCA\nimport difflib \nfrom textwrap import wrap\nfrom mlxtend.preprocessing import minmax_scaling\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn import metrics, mixture, cluster\nimport mpl_toolkits.mplot3d.axes3d as p3\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(\"Libraries loaded correctly.\")","25619e5a":"df = pd.read_csv('..\/input\/sanfranciso-crime-dataset\/Police_Department_Incidents_-_Previous_Year__2016_.csv')\ndf_aux = pd.read_csv('..\/input\/offences-in-us-with-penalties\/AmericanOffencesPenalty.csv')\ndf_aux['Offence'] = df_aux['Offence'].str.upper()\n\nprint(\"The number of rows: \" + format(df.shape[0]) + \"The number of factors: \" + format(df.shape[1]))","35efcd17":"C = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\n\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\nMissing_Percentage = (df.isnull().sum()).sum()\/np.product(df.shape)*100\nprint(\"The number of missing entries before cleaning: \" + str(round(Missing_Percentage,7)) + \" %\")","6b2722b8":"df['PdDistrict'].fillna(df['PdDistrict'].mode()[0], inplace = True)\n\nMissing_Percentage = (df.isnull().sum()).sum()\/np.product(df.shape)*100\nprint(\"The number of missing entries before cleaning: \" + str(round(Missing_Percentage,5)) + \" %\")","bb5a035b":"def highlight_cols(s, coldict):\n    if s.name in coldict.keys():\n        return ['background-color: {}'.format(coldict[s.name])] * len(s)\n    return [''] * len(s)\n\ndef ExtractColumn(lst,j): \n    return [item[j] for item in lst] ","efd7b32c":"coldict = {'Category':'lightcoral','Descript':'lightcoral','PdDistrict':'lightcoral','Resolution':'lightcoral','DayOfWeek':'lightcoral','Address':'lightcoral', \n           'Date':'lightsalmon','Time':'lightsalmon','X':'lightsalmon','Y':'lightsalmon'}\n\ndf = df[['PdId','Category','Descript','PdDistrict','Resolution','DayOfWeek','Address','X','Y','Time','Date']]\n\ndf.iloc[0:5].style.apply(highlight_cols, coldict=coldict)","664e5fc0":"df['Date'] = pd.Series(pd.to_datetime(df['Date']), index=df.index)\ndf['Minute'] = pd.Series(pd.to_datetime(df['Time']).dt.minute, index=df.index)+pd.Series(pd.to_datetime(df['Time']).dt.hour, index=df.index)*60\ndf['Hour'] = pd.Series(pd.to_datetime(df['Time']).dt.hour, index=df.index)\ndf['Time'] = pd.Series(pd.to_datetime(df['Time']).dt.time, index=df.index)\n\ndf['Month'] = pd.Series(df['Date'].dt.month, index=df.index)\ndf['Day'] = pd.Series(df['Date'].dt.day, index=df.index)\n\ndf['Date'] = pd.Series(pd.to_datetime(df['Date']), index=df.index)\n\nC = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\n\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\ncoldict = {'Category':'lightcoral','Descript':'lightcoral','PdDistrict':'lightcoral','Resolution':'lightcoral','DayOfWeek':'lightcoral','Address':'lightcoral', \n           'Date':'tomato','Time':'tomato','X':'lightsalmon','Y':'lightsalmon', 'Minute':'tomato', 'Hour':'tomato',  'Month':'tomato', 'Day':'tomato'}\n\ndf.iloc[0:5].style.apply(highlight_cols, coldict=coldict)","7d4b5dbc":"y = df['Category'].value_counts().head(25)\n    \nplt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('bmh')\n\nlabels = [ '\\n'.join(wrap(l, 20)) for l in y.index]\n\ncolor = plt.cm.Blues(np.linspace(0, 1, 15))\nsquarify.plot(sizes = y.values, label = labels, alpha=.8, color = color)\nplt.title('Top 25 categories', fontsize = 20)\n\nplt.axis('off')\nplt.show()","04c5f54f":"y = df['Descript'].value_counts().head(30)\n    \nplt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('bmh')\n\nlabels = [ '\\n'.join(wrap(l, 20)) for l in y.index]\n\ncolor = plt.cm.Greens(np.linspace(0, 1, 15))\nsquarify.plot(sizes = y.values, label = labels, alpha=.8, color = color)\nplt.title('Top 30 descripts', fontsize = 20)\n\nplt.axis('off')\nplt.show()","c1dd1620":"print(\"30 top used descripts corresponds to: \" + format(round(sum(df['Descript'].value_counts().head(30))\/sum(df['Descript'].value_counts()),3)*100)\n     + \"% of all descripts,\" +\n    \" while 100 top used descripts corresponds to: \" + format(round(sum(df['Descript'].value_counts().head(100))\/sum(df['Descript'].value_counts()),3)*100)\n     + \"% of all descripts.\")","8514ca3c":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8), dpi=70, facecolor='w', edgecolor='k')\nax0, ax1= ax.flatten()\n\nsns.set('talk', palette='colorblind')\n\nfont = {'family' : 'normal','weight' : 'normal','size'   : 12}\n\nmatplotlib.rc('font', **font)\n\ncolor = plt.cm.twilight(np.linspace(0, 5, 100))\n\ny = df['Resolution'].value_counts().head(5)\nax0.bar(y.index,y.values,color=color)\nplt.setp(ax0.xaxis.get_majorticklabels(), rotation=75)\nax0.set_ylabel('Count');\n\ny = df['PdDistrict'].value_counts()\nax1.bar(y.index,y.values,color=color)\nplt.setp(ax1.xaxis.get_majorticklabels(), rotation=75)\nax1.set_ylabel('Count');\n\nplt.tight_layout()\nplt.show()","bf91c694":"DayOfWeek_map  = {'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7}\n\ndf['DayOfWeek'] = df['DayOfWeek'].map(DayOfWeek_map)\nprint(DayOfWeek_map)","b1397c10":"FrequentAddress = pd.DataFrame({'Frequent_Address':np.array(df.Address.value_counts()), 'Address':np.array(df.Address.value_counts().index)})\ndf = pd.merge(df,FrequentAddress,on='Address')\ndf=df.set_index('PdId')\nFrequentAddress","e5939aea":"xxx = pd.DataFrame(df.Descript.value_counts()\/df.Descript.count()).head(10)\nTopDescripts = list(xxx.index)\nTopDescripts","f01a0d07":"#df_aux['Offence'] = df_aux['Offence'].apply(lambda x: difflib.get_close_matches(x, df['Descript'],cutoff =0.6,n=1))\n#df_dummy.merge(df_aux)","4d9260c6":"Serious_Crime_map  = {'WEAPON LAWS':1,'WARRANTS':1,'NON-CRIMINAL':0, 'ASSAULT':2,\n       'OTHER OFFENSES':0, 'MISSING PERSON':0, 'LARCENY\/THEFT':1, 'BURGLARY':1,\n       'STOLEN PROPERTY':0, 'ROBBERY':1, 'FRAUD':1, 'DRUG\/NARCOTIC':2,\n       'VEHICLE THEFT':1, 'RECOVERED VEHICLE':0, 'VANDALISM':1, 'ARSON':1,\n       'PROSTITUTION':1, 'SECONDARY CODES':0, 'SUSPICIOUS OCC':0, 'DRUNKENNESS':0,\n       'TRESPASS':1, 'SEX OFFENSES, NON FORCIBLE':1, 'SEX OFFENSES, FORCIBLE':2,\n       'RUNAWAY':0, 'KIDNAPPING':2, 'DISORDERLY CONDUCT':0,\n       'DRIVING UNDER THE INFLUENCE':1, 'FORGERY\/COUNTERFEITING':1,\n       'EMBEZZLEMENT':1, 'BRIBERY':1, 'FAMILY OFFENSES':1, 'GAMBLING':1,\n       'SUICIDE':0, 'LIQUOR LAWS':0, 'EXTORTION':1, 'LOITERING':0, 'TREA':0,\n       'BAD CHECKS':1, 'PORNOGRAPHY\/OBSCENE MAT':1}\n\ndf['Serious_Crime'] = df['Category'].map(Serious_Crime_map)","a16168eb":"df['Resolution_binary'] = np.where((df['Resolution'] == 'ARREST, BOOKED') | (df['Resolution'] == 'ARREST, CITED') | (df['Resolution'] == 'JUVENILE BOOKED') | (df['Resolution'] == 'PSYCHOPATHIC CASE'),1,0)","51cad95e":"C = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\n\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index) #+ ['Time']\n\nOrdinalVariables = ['DayOfWeek']\nNominalVariables = [x for x in CategoricalVariables if x not in OrdinalVariables]\nExcludeVariables = ['Descript','Address','Date','Time']\nNominalVariables = [x for x in NominalVariables if x not in ExcludeVariables]\n\ndf_dummy = pd.get_dummies(df[NominalVariables], columns=NominalVariables)\ndf_numeric = df[NumericVariables]\ndf_numeric = minmax_scaling(df_numeric, columns=df_numeric.columns)\ndf_final = pd.merge(df_numeric,df_dummy,on='PdId')\n\nprint(\"Dummy transformation was successful\")","d17c8b9e":"SpearmanCorr = df_final[NumericVariables].corr(method=\"spearman\")\nmatplotlib.pyplot.figure(figsize=(7,7))\nsns.heatmap(SpearmanCorr, vmax=.9, square=True, annot=True, linewidths=.3, cmap=\"YlGnBu\", fmt='.1f')","a87db29b":"List_Resolution = ['Resolution_binary','Resolution_ARREST, BOOKED', 'Resolution_ARREST, CITED',\n       'Resolution_CLEARED-CONTACT JUVENILE FOR MORE INFO',\n       'Resolution_COMPLAINANT REFUSES TO PROSECUTE',\n       'Resolution_EXCEPTIONAL CLEARANCE', 'Resolution_JUVENILE BOOKED',\n       'Resolution_JUVENILE CITED', 'Resolution_JUVENILE DIVERTED',\n       'Resolution_LOCATED', 'Resolution_NONE', 'Resolution_NOT PROSECUTED',\n       'Resolution_PROSECUTED BY OUTSIDE AGENCY',\n       'Resolution_PSYCHOPATHIC CASE', 'Resolution_UNFOUNDED']\nSpearmanCorr = df_final[List_Resolution].corr(method=\"spearman\")\nx = SpearmanCorr[['Resolution_binary']]\nmatplotlib.pyplot.figure(figsize=(7,7))\nsns.heatmap(x, vmax=.9, square=True, annot=True, linewidths=.3, cmap=\"YlGnBu\", fmt='.1f')","e5ac0aa4":"df_spatial = df_final[['Frequent_Address','X','Y','PdDistrict_BAYVIEW',\n       'PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', 'PdDistrict_MISSION',\n       'PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND',\n       'PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN']]\nX = df_spatial.values\n\nGM_n_components = np.arange(1, 10)\nGM_models = [mixture.GaussianMixture(n, covariance_type='full', random_state=0).fit(X) for n in GM_n_components]\n\nplt.figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='r')\nplt.plot(GM_n_components, [m.aic(X) for m in GM_models], label='AIC')\nplt.tight_layout()\nplt.legend(loc='best')\nplt.xlabel('n_components');","6feb5980":"GM_n_classes = 6\n\nGMcluster = mixture.GaussianMixture(n_components=GM_n_classes, covariance_type='full',random_state = 0)\nGMcluster_fit = GMcluster.fit(df_spatial)\nGMlabels = GMcluster_fit.predict(df_spatial)\n\nprint('Number of clusters: ' + format(len(np.unique(GMlabels))))","974d3afc":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), facecolor='w', edgecolor='k')\nax = p3.Axes3D(fig)\nax.view_init(10, 40)\nfor l in np.unique(GMlabels):\n    ax.scatter(X[GMlabels == l, 0], X[GMlabels == l, 1], X[GMlabels == l, 2],color=plt.cm.jet(float(l) \/ np.max(GMlabels + 1)),s=20, edgecolor='k')\nplt.title('Expectation-maximization algorithm for spatial features clustering' )\n\nplt.show()","1d0b7a7f":"df_final[['Spatial_Cluster']] = list(GMlabels)","b808e4b6":"df_time = df_final[['DayOfWeek', 'Minute', 'Hour', 'Month', 'Day', 'Frequent_Address']]\nX = df_time.values\n\nGM_n_classes = 4\n\nGMcluster = mixture.GaussianMixture(n_components=GM_n_classes, covariance_type='full',random_state = 0)\nGMcluster_fit = GMcluster.fit(df_time)\nGMlabels = GMcluster_fit.predict(df_time)\n\ndf_final[['Time_Cluster']] = list(GMlabels)","78c6393f":"pca = PCA().fit(df_final)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5), dpi=70, facecolor='w', edgecolor='k')\nax0, ax1 = axes.flatten()\n\nsns.set('talk', palette='colorblind')\n\nfont = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 12}\n\nmatplotlib.rc('font', **font)\n\nax0.plot(np.cumsum(pca.explained_variance_ratio_)[0:20], marker='.')\nax0.set_xlabel('Number of components')\nax0.set_ylabel('Cumulative explained variance');\n\nax1.bar(range(20),pca.explained_variance_ratio_[0:20] )\nax1.set_xlabel('Number of components')\nax1.set_ylabel('Explained variance');\n\nplt.tight_layout()\nplt.show()","dc89727f":"CompOne = pd.DataFrame(list(zip(df_final.columns,pca.components_[0])),columns=('Name','Contribution to Component 1'),index=range(1,len(df_final.columns)+1,1))\nCompOne = CompOne[(CompOne['Contribution to Component 1']>0.05) | (CompOne['Contribution to Component 1']< -0.05)]\nCompTwo = pd.DataFrame(list(zip(df_final.columns,pca.components_[1])),columns=('Name','Contribution to Component 2'),index=range(1,len(df_final.columns)+1,1))\nCompTwo = CompTwo[(CompTwo['Contribution to Component 2']>0.05) | (CompTwo['Contribution to Component 2']< -0.05)]\nCompThree = pd.DataFrame(list(zip(df_final.columns,pca.components_[2])),columns=('Name','Contribution to Component 3'),index=range(1,len(df_final.columns)+1,1))\nCompThree = CompThree[(CompThree['Contribution to Component 3']>0.05) | (CompThree['Contribution to Component 3']< -0.05)]\n#CompFour = pd.DataFrame(list(zip(df_final.columns,pca.components_[3])),columns=('Name','Contribution to Component 4'),index=range(1,len(df_final.columns)+1,1))\n#CompFour = CompFour[(CompFour['Contribution to Component 4']>0.05) | (CompFour['Contribution to Component 4']< -0.05)]\n#CompFive = pd.DataFrame(list(zip(df_final.columns,pca.components_[4])),columns=('Name','Contribution to Component 5'),index=range(1,len(df_final.columns)+1,1))\n#CompFive = CompFive[(CompFive['Contribution to Component 5']>0.05) | (CompFive['Contribution to Component 5']< -0.05)]\n#CompSix = pd.DataFrame(list(zip(df_final.columns,pca.components_[5])),columns=('Name','Contribution to Component 6'),index=range(1,len(df_final.columns)+1,1))\n#CompSix = CompSix[(CompSix['Contribution to Component 6']>0.05) | (CompSix['Contribution to Component 6']< -0.05)]\nPCA_top = pd.concat([CompOne,CompTwo,CompThree])\nPCA_top","42779847":"sorted(list(PCA_top.Name.unique()))","da6f017a":"n_PCA_70 = np.size(np.cumsum(pca.explained_variance_ratio_)>0.7) - np.count_nonzero(np.cumsum(pca.explained_variance_ratio_)>0.7)\nn_PCA_90 = np.size(np.cumsum(pca.explained_variance_ratio_)>0.9) - np.count_nonzero(np.cumsum(pca.explained_variance_ratio_)>0.9)\nprint(\"Already \" + format(n_PCA_70) + \" components cover 70% of variance. \" + format(n_PCA_90) + \" components cover 90%.\")","278493e2":"pca = PCA(4).fit((df_final[NumericVariables]))\n\nX_pca=pca.transform((df_final[NumericVariables])) \n\nplt.matshow(pca.components_,cmap='viridis')\nplt.yticks([0,1,2,3,4],['1st Comp','2nd Comp','3rd Comp','4th Comp'],fontsize=12)\nplt.colorbar()\nplt.xticks([0,1,2,3,4,5,6,7,8,9],NumericVariables,fontsize=10,rotation=30)\nplt.tight_layout()\nplt.show()","41496d3e":"coldict = {'Category':'lightcoral','Descript':'lightcoral','PdDistrict':'lightcoral','Resolution':'lightcoral','DayOfWeek':'tomato','Address':'lightcoral', \n           'Date':'tomato','Time':'tomato','X':'lightsalmon','Y':'lightsalmon', 'Minute':'tomato', 'Hour':'tomato',  'Month':'tomato', 'Day':'tomato',\n          'Frequent_Address':'orange','Serious_Crime':'orange','Resolution_binary':'orange','Spatial_Cluster':'yellow','Time_Cluster':'yellow'}\n\ndf_final.iloc[0:10].style.apply(highlight_cols, coldict=coldict)","1c2e50ef":"Last, I apply the same method but for time using time-related variables like: 'DayOfWeek', 'Minute', 'Hour', 'Month', 'Day', 'Frequent_Address'. At the end also merging.","1e2f914b":"# 5.Principial component analysis\n\nThe useful method to understand variability in the data set  (PCA topic is presented in my [other notebook](https:\/\/www.kaggle.com\/jjmewtw\/total-analysis-of-pca-sparse-pca-nmf-kernel-pca)).  \n\nFirst, I fit regular PCA on the total data:","601cb649":"Second, check the frequency of occurence for given addresses and match it to the data:","2c3a64ab":"Next, in the previous chapter we looked at the number of possible descripts, I will highlight top ten of them:","b63fc46b":"# 4.Clustering\n\nOne of useful methods to understand the data is clustering (Clustering topic is presented in my [other notebook](https:\/\/www.kaggle.com\/jjmewtw\/clustering-k-means-hierarchical-debscan-ema)).  ","14fbf3aa":"A bit more problematic, because there is no pre-defined form of descript, namely it depends on the fantasy of police officer.","0b4ebd6c":"**Please upvote if you liked it.**","d24c76ce":"Apparently,already 3 components give us good feeling about variability. I look what variables contribute the most to these 3 components:","891a8351":"Above methiod allows for estimation how many clusters are optimal for this method. Apaprently 6 is good choice.","9074388a":"Two functions for making data presentation neater:","6b02a555":"Some changes for date-related factors. Extracting minute and hour from 'time' and transofmring to float:","35c28eea":"Let's apply clustering. For this I chose 'Expectation-maximization algorithm' and decided to build spatial cluster by use of: 'Frequent_Address','X','Y','PdDistrict_BAYVIEW','PdDistrict_CENTRAL', 'PdDistrict_INGLESIDE', 'PdDistrict_MISSION','PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_RICHMOND','PdDistrict_SOUTHERN', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN'.","5b182475":"Last modification, resolution binary defined on the basis of police verdict regarding the crime. '1' for offences finished with arrest, '0' for offences without it.","bcd38762":"# Introduction\nThe goal of this notebook is to **prepare the data set and extract as much infromation** as possible. The underlying data is set of San Francisco criminal records. We see two types of variables:\n* numeric variables like event occurence hour, latitude, longitude - these factors can be imemdiately used just after small preparation\n* string factors like category and descript - I need to put more work for them to extract as much infromation as possible\n\nThe result of the nutebook will be cleaned, enriched, transformed data set with clusters attached.\n\n**For clarity some code was hidde, click 'unhide' to see it.**","118f5762":"# 6.Final data description","8127e943":"# 1.Data Preparation\n\nI will prepare the data for further use (Cleansing topic is presented in my [other notebook](https:\/\/www.kaggle.com\/jjmewtw\/prices-cleaning-analysis-estimation-in-stages)).  First, loading some libraries:","cfbccba9":"Last two variables which I find good to show: 'resolution' on the left and 'disctricts' on the right:","6e9923ed":"I look at the results by plotting 3D graph:","0c7f2cba":"First, matching string day of the week to ordinal variables from 1 to 7:","b753d014":"And finally I merge this cluster to the data.","d97f41d3":"Next, the self-defined variable 'serious crime' with three integer value {0,1,2}. Value '2' for crimes with more than 10 years in jail, '0' for offences with no jail.","6637bbc0":"Not a lot, but still needs to be cleaned. Only 'disctrict' variables has NA's, apply mode for it:","115b6fbb":"# 2.One-way analysis\n\nBasic look at the dispersion of exposure regarding the factors groups (One-way topic is presented in my [other notebook](https:\/\/www.kaggle.com\/jjmewtw\/actuarial-study-eda-pca-cluster-estimation-0-88)).","13558cdf":"![image.png](attachment:image.png)","366cc72c":"![image.png](attachment:image.png)","0f6b223a":"At this stage, I have cleaned, enriched data with plotted dependencies and trends. Please find the data set below:\n* first group are time-related factors\n* second group are factor defined by me, engineered ones\n* third group (X and Y) are variables latitude and longitude\n* fourth big group without color are dummy variables\n* at the very end fifth group are two clusters which were defined","071391f2":"Let's check how many components I need:","36d18deb":"First, let's look at the dependencies between numeric variables in this data set (Correlation topic is presented in my [other notebook](https:\/\/www.kaggle.com\/jjmewtw\/yt-pearson-spearman-distance-corr-rv-coef)).  ","435a9d59":"And last let's plot contribution of numeric variables (you can do it as well with all variables, but it will look messy).","986e0c45":"Obvious dependency between 'hour' and 'minute' as the first one is just a general version of the second one. Some dependecies between geographical factors. Below, the look on the dependency between resolution binary and its dummy versions:","18a19070":"Mark categorical and numeric variables, check number of NA's:","dadbc2fe":"Preferebly, we would apply fuzzy matching to connect our offences with the penalty size from American Law, in that way we would achieve the integer variable in palce of string strcuture. But this fuzzy matching is very heavy computation-wise:","0cef37a5":"Second the data:","06230a5c":"And presented factors alphabetically:","e09875ec":"# 3.Features engineering\n\nLet's boost this data set by adding some new variables (Feature engineering topic is presented in my [other notebook](https:\/\/www.kaggle.com\/jjmewtw\/prices-cleaning-analysis-estimation-in-stages)). ","8adec0d7":"For this data set, we can put every factor out-of-bag and treat as response. \n\nIt may then look like calculator, where user puts characteristics of situation and tries to find the respective results. \n\n**In the next notebook I will use following variables as response in separate analyzes:**\n* Minute - I predict the time of the given crime. I exclude for this 'time cluster' and 'hour' but leave 'day of week' and 'day' which corresponds to the day of month\n* Resolution_binary - I predict whether the given crime will finish with arrest or not. I leave out all resolution-related dummies\n* Serious_Crime - I will check whther the crime can be defined as serious leaving out all category-related variables\n* Spatial_Cluster - I will predict what type of crime appears in the given location. All spatial variables will be excluded","509795c2":"Finally, I transform the nominal variables into dummy set to keep their information.","51c8493b":"My cleaned data set:","f8aaee76":"Categories can be neatly divided. Let's look at descripts:"}}