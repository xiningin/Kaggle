{"cell_type":{"e5fa1524":"code","79ce414c":"code","caf0a06c":"code","adaa4f07":"code","a0ce0ae0":"code","683aa284":"code","9584874e":"code","5e8e3630":"code","01cd80d7":"code","b7fef8ee":"code","9210ec0f":"code","79c148cc":"code","5a1afa43":"code","016de0b6":"code","15fff993":"code","0e4a6d39":"code","cf708173":"code","bb583994":"code","5d4dfb2b":"code","883aef65":"code","853bae96":"code","fa8a04d5":"code","af4b467a":"code","759c8219":"code","3242e52a":"code","e5306935":"code","2f2bd0b6":"code","f94ea789":"code","212c5ddd":"code","54c35aa5":"code","7c927c36":"code","5bb03abb":"code","7ff97d8f":"code","8805cba0":"code","d0ca6444":"code","f4678fae":"code","7b00e292":"code","cb8592da":"code","8b6925b2":"code","18f3361b":"code","e244df69":"code","b59cabe1":"code","5e2cb6c9":"code","512dd699":"code","c377dc7a":"code","f927e8e7":"code","ccb7afc8":"code","2d3db81d":"code","79bf32eb":"code","5afd8158":"code","f7e889d2":"code","e73a38db":"code","1163099a":"code","bb17bfcd":"code","7cac49de":"code","30254a65":"code","dff2a054":"code","a2eff53e":"code","80aedffb":"code","c5ae0ba8":"code","95b76370":"code","b64d22d3":"code","3a86c085":"code","f6444b01":"code","b9705188":"code","80f22508":"code","9da21efd":"code","8fb8455f":"code","c45b0ac8":"code","38107d9d":"markdown","249b03b4":"markdown","1844ec66":"markdown","b8515e3f":"markdown","1b709773":"markdown","b2b6c371":"markdown","98ba6bcd":"markdown","f4822505":"markdown"},"source":{"e5fa1524":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79ce414c":"pip install -U scikit-learn","caf0a06c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\n%matplotlib inline\n","adaa4f07":"df= pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\", sep= ',')","a0ce0ae0":"df","683aa284":"sns.countplot(df['Class'])\nplt.show()","9584874e":"from imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.metrics import classification_report_imbalanced","5e8e3630":"#let's do some EDA\ndf.info()","01cd80d7":"df.isnull().sum()","b7fef8ee":"df.describe()","9210ec0f":"#let's visualize classes\nd1= pd.DataFrame(df.groupby(['Class'])['Class'].count())\nfig,axes=plt.subplots(figsize= (12,10), dpi= 70)\naxes.barh([0],d1.Class[0], height= 0.7, color= 'black')\nplt.text(290000,0.08, '99.8%',{'fontname':'Serif','weight':'bold' ,'size':'16','color':'black'})\naxes.barh([1], d1.Class[1] ,height= 0.7, color= 'red')\nplt.text(3900,1, '0.17%',{'fontname':'Serif', 'weight':'bold','size':'16','color':'red'})\n\naxes.axes.get_xaxis().set_visible(False)\naxes.axes.get_yaxis().set_visible(False)\naxes.spines['right'].set_visible(False)\naxes.spines['top'].set_visible(False)\nplt.show()\n","79c148cc":"#let's do it without sampling and with oversampling\nsns.distplot(df['Time'])\nplt.show()","5a1afa43":"sns.distplot(df['Amount'])\nplt.show()","016de0b6":"sns.scatterplot(x= 'Time', y='Amount', hue= 'Class', data = df)\nplt.show()","15fff993":"#let's checl correrlation now\nplt.figure(figsize= (19,10))\nsns.heatmap(df.corr(), annot=True)\nplt.show()","0e4a6d39":"#becuasue of outliers let;s scale it\nfrom sklearn.preprocessing import RobustScaler\n\nrbst= RobustScaler()\n\n","cf708173":"df['Amount']= rbst.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Time']= rbst.fit_transform(df['Time'].values.reshape(-1,1))","bb583994":"target = 'Class'\npredictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']","5d4dfb2b":"X= df.drop('Class', axis =1)\nY= df.Class.values","883aef65":"Y","853bae96":"X","fa8a04d5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier as Knn\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report , confusion_matrix\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import train_test_split","af4b467a":"X_train,X_test,y_train,y_test= train_test_split(X,Y,test_size= 0.2, random_state= 10)","759c8219":"# a fucntion to see things easi;ly\ndef predict(model,X_train, X_test, y_train, y_test):\n    model.fit(X_train,y_train)\n    preds=model.predict(X_test)\n    print(confusion_matrix(y_test,preds))\n    print(classification_report(y_test,preds))\n    print(roc_auc_score(y_test, preds))\n    fig, axes = plt.subplots(1,2,figsize=(27,12))\n    \n    axes[0].set_title(\"Confusion Matrix\")\n    sns.heatmap(confusion_matrix(y_test,preds),annot=True,fmt='0.0f',ax=axes[0])\n\n   \n    \n    return accuracy_score(y_test,preds)\n","3242e52a":"# a fucntion to see things easi;ly\ndef predict2(model,X_train, X_test, y_train, y_test):\n    model.fit(X_train,y_train)\n    preds=model.predict(X_test)\n    print(confusion_matrix(y_test,preds))\n    print(classification_report(y_test,preds))\n    print(\"roc_score\",roc_auc_score(y_test, preds))\n    fig, axes = plt.subplots(1,2,figsize=(27,12))\n    \n    axes[0].set_title(\"Confusion Matrix\")\n    sns.heatmap(confusion_matrix(y_test,preds),annot=True,fmt='0.0f',ax=axes[0])\n\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': model.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp,color='red',ax=axes[1])\n    axes[1].set_title(\"Feature Importance\")\n    plt.show()\n    return accuracy_score(y_test,preds)\n","e5306935":"#predict(LogisticRegression(), X_train, X_test, y_train, y_test)","2f2bd0b6":"predict2(xgb.XGBClassifier(silent=True,max_depth=2,tree_method = 'gpu_hist',predictor='gpu_predictor',eval_metric='auc'),X_train,X_test, y_train, y_test)","f94ea789":"predict2(RandomForestClassifier(n_estimators=100,n_jobs=4,verbose=False),X_train,X_test, y_train, y_test)","212c5ddd":"predict2(AdaBoostClassifier(random_state=42,\n                        algorithm='SAMME.R',\n                         learning_rate=0.8,),X_train,X_test, y_train, y_test)","54c35aa5":"predict2(lgb.LGBMClassifier(\n    objective='binary',\n        metric = 'auc',\n          learning_rate= 0.05,\n          num_leaves =  7,max_depth=4,min_child_samples= 100,  \n          max_bin=100,device= 'gpu'),X_train,X_test, y_train, y_test)","7c927c36":"smt = SMOTE(random_state=42)\n# adasyn = ADASYN(random_state=42)\nX_smote,Y_smote = smt.fit_resample(X_train,y_train)","5bb03abb":"predict(LogisticRegression(), X_smote, X_test, Y_smote, y_test)","7ff97d8f":"predict2(xgb.XGBClassifier(silent=True,max_depth=2,tree_method = 'gpu_hist',predictor='gpu_predictor',eval_metric='auc'),X_smote,X_test, Y_smote, y_test)","8805cba0":"predict2(RandomForestClassifier(n_estimators=100,n_jobs=4,verbose=False),X_smote,X_test, Y_smote, y_test)","d0ca6444":"predict2(AdaBoostClassifier(random_state=42,\n                        algorithm='SAMME.R',\n                         learning_rate=0.8,),X_smote,X_test, Y_smote, y_test)","f4678fae":"predict2(lgb.LGBMClassifier(\n    objective='binary',\n        metric = 'auc',\n          learning_rate= 0.05,\n          num_leaves =  7,max_depth=4,min_child_samples= 100,  \n          max_bin=100,device= 'gpu'),X_smote,X_test, Y_smote, y_test)","7b00e292":"model =xgb.XGBClassifier(silent=True,max_depth=2,tree_method = 'gpu_hist',predictor='gpu_predictor',eval_metric='auc')","cb8592da":"model.fit(X_smote,Y_smote)","8b6925b2":"slm = SelectFromModel(model, threshold = 0.2)","18f3361b":"X_train_XGB =slm.fit_transform(X_smote,Y_smote)","e244df69":"#need to select same features as train dataset\nX_test_XGB= np.array(X_test[X_test.columns[slm.get_support()]])","b59cabe1":"X_train_XGB","5e2cb6c9":"#preparing the train and test datasets\nd_train= xgb.DMatrix(pd.DataFrame(X_train_XGB), Y_smote)\nd_test= xgb.DMatrix(pd.DataFrame(X_test_XGB),y_test)\n\n##to monitor train and  test\nwatch_list= [(d_train,'train'), (d_test, 'test')]","512dd699":"params = {}\nparams['objective'] = 'binary:logistic'\nparams['silent'] = True\nparams['max_depth'] = 2\nparams['eval_metric'] = 'auc'\nparams['random_state'] = 50\nparams['tree_method'] = 'gpu_hist'\nparams['predictor']='gpu_predictor'\n","c377dc7a":"model1 = xgb.train(params,d_train,200, watch_list)","f927e8e7":"preds=model1.predict(d_test)","ccb7afc8":"xgb.plot_importance(model1, height=0.8, title=\"Feature importance \",color='red',show_values=False)","2d3db81d":"roc_auc_score(y_test ,preds)","79bf32eb":"#let's try with imbalanced data for now\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.models import Sequential","5afd8158":"n_inputs = len(X_train.columns)","f7e889d2":"model = Sequential()\n\n# First Hidden Layer \nmodel.add(Dense(50, input_dim = n_inputs, activation = 'relu', kernel_initializer = 'he_uniform'))\n\n# Define output layer\nmodel.add(Dense(1, activation = 'sigmoid'))\n","e73a38db":"model.compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'adam'\n)","1163099a":"y_pred1 = model.predict(X_test)","bb17bfcd":"print(roc_auc_score(y_test, y_pred1))","7cac49de":"#not bad now lets try with smote balanced one\nmodel.fit(X_smote, Y_smote, epochs = 20)","30254a65":"y_pred2 = model.predict(X_test)","dff2a054":"print(roc_auc_score(y_test, y_pred2))","a2eff53e":"#let's  try multi layer perceptron as well\nfrom sklearn.neural_network import MLPClassifier","80aedffb":"predict(MLPClassifier(solver='lbfgs', hidden_layer_sizes=(100,100), random_state=2),X_train,X_test,y_train,y_test)","c5ae0ba8":"predict(MLPClassifier(solver='lbfgs', hidden_layer_sizes=(100,100), random_state=2),X_smote,X_test,Y_smote,y_test)","95b76370":"#now let's try with some dense networks\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(30,), activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(16, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(8, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(4, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(1, activation='sigmoid'))","b64d22d3":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])","3a86c085":"history = model.fit(X_train, y_train, epochs = 10, batch_size=5, validation_split = 0.15, verbose = 0,\n                    )\nhistory_dict = history.history","f6444b01":"loss_values = history_dict['loss']\nval_loss_values=history_dict['val_loss']\nplt.plot(loss_values,'b',label='training loss')\nplt.plot(val_loss_values,'r',label='val training loss')\nplt.legend()\nplt.xlabel(\"Epochs\")","b9705188":"accuracy_values = history_dict['accuracy']\nval_accuracy_values=history_dict['val_accuracy']\nplt.plot(val_accuracy_values,'-r',label='val_accuracy')\nplt.plot(accuracy_values,'-b',label='accuracy')\nplt.legend()\nplt.xlabel(\"Epochs\")","80f22508":"y_pred_nn = model.predict_classes(X_test)","9da21efd":"import sklearn.metrics as metrics","8fb8455f":"print(\"Accuracy Neural Net:\",metrics.accuracy_score(y_test, y_pred_nn))\nprint(\"Precision Neural Net:\",metrics.precision_score(y_test, y_pred_nn))\nprint(\"Recall Neural Net:\",metrics.recall_score(y_test, y_pred_nn))\nprint(\"F1 Score Neural Net:\",metrics.f1_score(y_test, y_pred_nn))","c45b0ac8":"#AUC\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\ny_pred_nn_proba = model.predict_proba(X_test)\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred_nn_proba)\nauc_keras = auc(fpr_keras, tpr_keras)\nprint('AUC Neural Net: ', auc_keras)","38107d9d":"seeems like the data is imbalanced so we need imbalanced learn package","249b03b4":"##### importing relevant libraries\n","1844ec66":"looks like there are outliers","b8515e3f":"fun thing is all plots are showing that Amount really has small importance which is quite interesting but mostly it depends on \nV14, V10 , v4 and v17 in few cases\n","1b709773":"now le'ts try same after sampling\n","b2b6c371":"now let's try some feature selection to see if we can improve it a bit further using xgb","98ba6bcd":"### splitting training and testing set","f4822505":" The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions."}}