{"cell_type":{"2c75d629":"code","157fea1c":"code","ad1e7727":"code","d29d2f84":"code","bce1da85":"code","17d2c5dd":"code","f08b5291":"code","f73d815e":"markdown","d51f07e2":"markdown"},"source":{"2c75d629":"import numpy as np \nimport pandas as pd \nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn import neighbors, svm,metrics\nfrom sklearn.ensemble import RandomForestClassifier\n","157fea1c":"data = pd.read_csv('..\/input\/university-recommendation\/original_data.csv') #original data file\nscore_table= pd.read_csv('..\/input\/university-recommendation\/score.csv') # GRE score conversion table\n\n#Dropping unwanted columns\ndata = data.drop(['gmatA','gmatQ','gmatV','specialization','department','program','toeflEssay','userProfileLink','topperCgpa','termAndYear','userName','toeflScore','industryExp','internExp','confPubs','journalPubs','researchExp'],1)\n# Dropping missing values\ndata = data.dropna()\n\n#Only admitted data is required, dropping not admitted\ndata = data[data[\"admit\"] > 0]\ndata = data.drop(\"admit\", 1)\n\n# Dropping universites whose instances are lesser in number\nuniversity_list = list(set(data[\"univName\"].tolist()))\nfor i in range(len(university_list)):\n    if len(data[data[\"univName\"] == university_list[i]]) < 100:\n        data = data[data[\"univName\"] != university_list[i]]\n","ad1e7727":"def normalize_gpa(data, cgpa, totalcgpa):\n    '''\n    Utility function to normalize CGPA\n    '''\n    cgpa = data[cgpa].tolist()\n    totalcgpa = data[totalcgpa].tolist()\n    for i in range(len(cgpa)):\n        if totalcgpa[i] != 0:\n            cgpa[i] = cgpa[i] \/ totalcgpa[i]\n        else:\n            cgpa[i] = 0\n    data[\"cgpa\"] = cgpa\n    return data","d29d2f84":"def feature_extraction_categorical_variable1(data, feature):\n    '''\n    Utility function to preprocess categorical features\n    '''\n    feature_list = list(data[feature].astype(str))\n    student_id_for_feature = defaultdict(list)\n    for i in range(len(feature_list)):\n        feature_list[i] = str(feature_list[i])\n        feature_list[i] = feature_list[i].strip()\n        feature_list[i] = feature_list[i].replace(\"-\", \"\")\n        feature_list[i] = feature_list[i].replace(\".\", \"\")\n        feature_list[i] = feature_list[i].partition(\"\/\")[0]\n        feature_list[i] = feature_list[i].partition(\"(\")[0]\n        feature_list[i] = feature_list[i].replace(\" \", \"\")\n        feature_list[i] = feature_list[i].lower()\n    data[feature] = feature_list\n    return data","bce1da85":"def scoreConversion(feature):\n    '''\n    Utility function: Gre Old Score to New Score\n    '''\n    gre_score = list(data[feature])\n    for i in range(len(gre_score)):\n        if gre_score[i] > 170:\n            try:\n                if feature =='greV':\n                    gre_score[i]=score_table['newV'][gre_score[i]]\n                elif feature == 'greQ':\n                    gre_score[i]=score_table['newQ'][gre_score[i]]\n            except:\n                continue\n    return gre_score","17d2c5dd":"# Preprocessing each column\n\ndata = feature_extraction_categorical_variable1(data, \"ugCollege\")\ndata['ugCollege'] = data['ugCollege'].astype('category')\ndata['ugCollege_code'] = data['ugCollege'].cat.codes\n\ndata = feature_extraction_categorical_variable1(data, \"major\")\ndata['major'] = data['major'].astype('category')\ndata['major_code'] = data['major'].cat.codes\ndata = data.drop(['major','ugCollege'],1)\n\ndata = normalize_gpa(data, \"cgpa\", \"cgpaScale\")\n\ndata['greV'] = data['greV'].astype('int')\ndata['greQ'] = data['greQ'].astype('int')\nscore_table.set_index(['old'],inplace=True)\ndata['greV']=scoreConversion('greV')\ndata['greQ']=scoreConversion('greQ')\ndata = data[data['greV']<=170]\ndata = data[data['greQ']<=170]","f08b5291":"x = data.drop(['univName'], 1)\ny = data['univName']\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)\n\n\n# Random Forest Classifier\n\nclf=RandomForestClassifier(n_estimators=1000)\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\n\n\n# Support Vector Classifier\nclf = svm.SVC()\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\n\n\n# K Nearest Neighbours\nclf = neighbors.KNeighborsClassifier(300, weights='uniform')\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\n\n\n# XGBoost Classifier\n\nclf = XGBClassifier()\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\n\n\n# Light GBM Classifier\nclf = LGBMClassifier()\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\n","f73d815e":"## Modelling","d51f07e2":"## Data Preprocessing\n\n- Removing Unwanted Columns (Features)\n- Cleaning the categorical variables\n- GPA scale conversion\n- GRE scores conversion from old to new"}}