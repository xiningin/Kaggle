{"cell_type":{"6a57e741":"code","3cc5761c":"code","8db12991":"code","c4cc060c":"code","912bb14a":"code","22b003fe":"code","d6a08db7":"code","52e46c7f":"code","e033efe9":"code","dd71653d":"code","0450ba3d":"code","009c80e4":"code","c78d7873":"code","e5d06c68":"code","e171a132":"code","6f8afb44":"code","3c36a9fd":"code","6f685d0a":"code","20232b62":"code","44f4a860":"code","2c8bb3ed":"code","8622b184":"code","5fff363d":"code","fa0c1892":"code","beb2ca2f":"code","e724e35f":"code","baee25b6":"code","0620701d":"code","6120d76a":"code","7b4491c9":"code","f7c976f8":"code","21243a25":"code","2e2979b6":"code","8856dbd2":"code","26cbca9a":"code","bde938a7":"code","31c9b88e":"code","023e17b5":"code","ebd3a0d0":"code","bac3bae6":"code","342df334":"code","b6db43f6":"code","5d6a9ef7":"code","a77f7d0e":"code","567928bf":"code","df30a6b1":"code","988079ff":"code","cefb8fa1":"code","cc4aacbe":"code","6921d9e2":"code","6728a14e":"code","2e11c651":"code","b3014f1b":"code","2926478f":"code","640e9ae5":"code","51a21055":"markdown","513c6181":"markdown","6eeefb56":"markdown","523fad59":"markdown","e15b59b1":"markdown","ec76d408":"markdown","2de7c31d":"markdown","5fb22613":"markdown","6f0ad89c":"markdown","23fae32d":"markdown","6da7bb7e":"markdown","8105e9f1":"markdown","34dfe6fa":"markdown","de11f63a":"markdown","2b3df517":"markdown","d54f74dc":"markdown","580bd60d":"markdown","c682a28e":"markdown","9626f7cc":"markdown","c06ce687":"markdown","d4bf35a4":"markdown","d100664a":"markdown","a61ee30e":"markdown","7be75f58":"markdown","ff47cb49":"markdown","ddbcab9b":"markdown","fc5e2388":"markdown","b7f04b76":"markdown","179d07ab":"markdown","121e3228":"markdown","cf2aadc2":"markdown","4d4973e1":"markdown","bfaa8087":"markdown","5e78398d":"markdown","f8b66413":"markdown","c31cf274":"markdown","38aa0332":"markdown","080218fa":"markdown","d6d75304":"markdown","331da959":"markdown","bdb44a2f":"markdown","c47da5e1":"markdown","02a65da7":"markdown","95015a9d":"markdown","2fa16d97":"markdown","4abc46ba":"markdown"},"source":{"6a57e741":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3cc5761c":"import sklearn\nimport matplotlib.pyplot as plt \nimport plotly\nimport plotly.express as px\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n# machine learning algorithms \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","8db12991":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","c4cc060c":"train.info()","912bb14a":"# check missing values in Age feature in test set\nmissing_values = 0\nfor i in train['Embarked'].isnull():\n    if i == True:\n        missing_values +=1\nprint(missing_values)","22b003fe":"train['Pclass'].value_counts()","d6a08db7":"train['Age'].value_counts()","52e46c7f":"train.describe()","e033efe9":"# variances for age and fare\nprint('Age\\'s variance: {}'.format(train['Age'].var()))\nprint('Fare\\'s variance: {}'.format(train['Fare'].var()))","dd71653d":"# plot Age feature to check the disribution\ntrain[['Age']].iplot(kind='hist',bins=27,title='Age Distribution',xTitle='Age',yTitle='Frequency')","0450ba3d":"# plot Fare distribution \ntrain['Fare'].iplot(kind='hist',bins=30,xTitle='Fare',yTitle='Frequency',title='Fare Distribution')","009c80e4":"# plot sibsp to check the distribution\ntrain['SibSp'].iplot(kind='hist',xTitle='Sibsp',yTitle='frequency',title='Sibling or Spouses Distribution')","c78d7873":"# plot Parch to check the distribution\ntrain['Parch'].iplot(kind='hist',xTitle='Parch',yTitle='Frequency')","e5d06c68":"# check the number of passengers that has 3 or 4 or 5 parents or children\nx =0\nfor i in train['Parch'].values:\n    if i == 5:\n        x += 1\nprint(x)","e171a132":"train['Fare'].describe()","6f8afb44":"train.columns","3c36a9fd":"# create a copy of train data to be modified \ntrain_insight = train.copy()","6f685d0a":"# convert the values in Sex attribute\ntrain_insight = train_insight.replace({'Sex':{'female':1,'male':0}})","20232b62":"train['Embarked'].value_counts()","44f4a860":"# convert the values in Embarked attribute\ntrain_insight = train_insight.replace({'Embarked':{'C':0,'Q':2,'S':3}}) ","2c8bb3ed":"train_corr = train_insight.corr()\ntrain_corr['Survived'].sort_values(ascending=False)","8622b184":"print('Shapes train and test data ')\nprint('Before: train data{} --- test data{}'.format(train.shape, test.shape))\n# remove Ticket and Cabin attributes from train and test sets\ntrain = train.drop(['Ticket','Cabin','PassengerId'], axis=1)\ntest = test.drop(['Ticket','Cabin'], axis=1)\nprint('After: train data{} --- test data{}'.format(train.shape, test.shape))","5fff363d":"train['Family Size'] = train['Parch'] + train['SibSp'] + 1\ntest['Family Size'] = train['Parch'] + train['SibSp'] + 1","fa0c1892":"train_corr = train.corr()\ntrain_corr['Survived'].sort_values(ascending=False)","beb2ca2f":"# train set\ntrain['Is Alone'] = 0\ntrain.loc[train['Family Size']==1, 'Is Alone'] = 1\n\n# test set\ntest['Is Alone'] = 0\ntest.loc[test['Family Size']==1, 'Is Alone'] = 1","e724e35f":"train_corr = train.corr()\ntrain_corr['Survived'].sort_values(ascending=False)","baee25b6":"train = train.drop(['SibSp','Parch','Family Size'], axis=1)\ntest = test.drop(['SibSp','Parch','Family Size'],axis=1)","0620701d":"train.head()","6120d76a":"train['Title'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest['Title'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","7b4491c9":"pd.crosstab(train['Title'], train['Survived'])","f7c976f8":"combine = [train, test] # combine is a list\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Capt','Col','Countess','Don','Dr','Jonkheer','Lady','Major','Mme','Rev','Sir'],'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle','Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms','Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')\n\ntrain[['Title','Survived']].groupby(['Title'], as_index=False).mean()","21243a25":"title_mapping = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master':4, 'Rare':5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    dataset['Title'] = dataset['Title'].astype(int)","2e2979b6":"train = train.drop('Name', axis=1)\ntest = test.drop('Name', axis=1)","8856dbd2":"train_corr = train.corr()\ntrain_corr['Survived']","26cbca9a":"train['Fare Interval'] = pd.qcut(train['Fare'], 5)\ntrain[['Fare Interval','Survived']].groupby(['Fare Interval'], as_index=False).mean()","bde938a7":"# handle missing values in Fare attribute\ntest['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)\ntrain['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)","31c9b88e":"train[['Fare']].info()","023e17b5":"combine = [train, test]\nfor dataset in combine:\n    dataset.loc[dataset['Fare'] <= 7.854, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.854) & (dataset['Fare'] <= 10.5), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 10.5) & (dataset['Fare'] <= 21.679), 'Fare'] = 2\n    dataset.loc[(dataset['Fare'] > 21.679) & (dataset['Fare'] <= 39.688), 'Fare'] = 3\n    dataset.loc[(dataset['Fare'] > 39.688) & (dataset['Fare'] <= 512.329), 'Fare'] = 4\n    # convert float values in Fare attribute to int \n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n# remove Fare Interval attribute\ntrain = train.drop('Fare Interval', axis=1)\n","ebd3a0d0":"train['Age Interval'] = pd.cut(train['Age'], 5)\ntrain[['Age Interval', 'Survived']].groupby(['Age Interval'], as_index=False).mean()","bac3bae6":"# handle missing values in Age attribute\ntrain['Age'].fillna(train['Age'].dropna().mean(), inplace=True)\ntest['Age'].fillna(train['Age'].dropna().mean(), inplace=True)","342df334":"combine = [train, test]\nfor dataset in combine:\n    dataset.loc[dataset['Age'] <= 16.336, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16.336) & (dataset['Age'] <= 32.252), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32.252) & (dataset['Age'] <= 48.168), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48.168) & (dataset['Age'] <= 64.084), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 64.084) & (dataset['Age'] <= 80.0), 'Age'] = 4\n    dataset['Age'] = dataset['Age'].astype(int)\n\n# remove Age Interval\ntrain = train.drop('Age Interval', axis=1)","b6db43f6":"train['Embarked'].value_counts()","5d6a9ef7":"# fill NAN with 'S' because 'S' is for which the value occurs frequently\ntrain['Embarked'].fillna('S', inplace=True)","a77f7d0e":"combine = [train, test]\n# convert Embarked and Sex \nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].replace(['C','S','Q'],[0,1,2])\n    dataset['Sex'] = dataset['Sex'].replace(['female','male'],[0,1])\n    dataset['Embarked'] = dataset['Embarked'].astype(int)\n    dataset['Sex'] = dataset['Sex'].astype(int)","567928bf":"X_train = train.drop('Survived',axis=1)\ny_train = train['Survived']\nX_test = test.drop('PassengerId', axis=1).copy()","df30a6b1":"# create a model\nlogreg = LogisticRegression()\n# fit train data to model\nlogreg.fit(X_train, y_train)\n# predict based test data\nY_prediction = logreg.predict(X_test)\n# calculate the accuracy\nlogreg_accuracy = logreg.score(X_train, y_train)\nlogreg_accuracy","988079ff":"# create a model\ngaussian = GaussianNB()\n# fit it\ngaussian.fit(X_train, y_train)\n# predict\nY_prediction = gaussian.predict(X_test)\n# calculate the accuracy\ngaussian_accuracy = gaussian.score(X_train, y_train)\ngaussian_accuracy","cefb8fa1":"# create a model\nknn = KNeighborsClassifier()\n# fit it\nknn.fit(X_train, y_train)\n# predict\ny_prediction = knn.predict(X_test)\n# calculate the accuracy\nknn_accuracy = knn.score(X_train, y_train)\nknn_accuracy","cc4aacbe":"# create a model\nsvc = SVC()\n# fit it\nsvc.fit(X_train, y_train)\n# predict\ny_prediction = svc.predict(X_test)\n# calculate the accuracy\nsvc_accuracy = svc.score(X_train, y_train)\nsvc_accuracy","6921d9e2":"# create a model\ndecision_tree = DecisionTreeClassifier()\n# fit it\ndecision_tree.fit(X_train, y_train)\n# predict\ny_prediction = decision_tree.predict(X_test)\n# accuracy\ndecision_tree_accuracy = decision_tree.score(X_train, y_train)\ndecision_tree_accuracy","6728a14e":"model = pd.DataFrame({\n    'Model':['Logistics Regression','Naive Bayes Classifier','Nearest Neighbors', 'Support Vector Machine','Decision Trees'],\n    'Score':[logreg_accuracy, gaussian_accuracy, knn_accuracy, svc_accuracy, decision_tree_accuracy]\n})","2e11c651":"model.sort_values(by='Score', ascending=False)","b3014f1b":"test","2926478f":"submission = pd.DataFrame({\n    'PassengerId':test['PassengerId'],\n    'Survived': y_prediction\n})","640e9ae5":"submission.to_csv('..\/submission.csv', index=False)","51a21055":"These code can help you to know whether the feature is categorical or discrete ","513c6181":"Create an Age Interval","6eeefb56":"### Decision Trees","523fad59":"Seems like the correlation between Is Alone with Survived is more useful than SibSp, Parch, and Family Size. <br>\nSo I decide to remove SibSp, Parch and Family Size attributes.","e15b59b1":"## Info \nInfo is will be contains information of dataset like data types and whether there is missing values in each feature on training data.","ec76d408":"### Combine SipSp and Parch atributes\nI want to create a feature that name is Family Size, so the attribute have to add 1 which is the passenger itself.","2de7c31d":"### Nearest Neighbors","5fb22613":"Convert the Title attribute to ordinal type","6f0ad89c":"## Background\n","23fae32d":"These code can check whether the feature has missing values.","6da7bb7e":"## Validation\nCheck the validation all of values in every each features in train set and test set","8105e9f1":"## Load The Data","34dfe6fa":"Create Fare Interval attributes","de11f63a":"### Support Vector Machine","2b3df517":"#### Remove Name attribute","d54f74dc":"## Correlation between target variable and all of variables","580bd60d":"### Remove SibSp, Parch and Family Size attributes","c682a28e":"### Convert Embarked and Sex Attributes \nConvert Embarked and Sex to numeric.","9626f7cc":"#### Variances for Age and Fare\n- Age = 211\n- Fare = 2469","c06ce687":"Evidently the correlation Family Size with Survived is approaching to 0. So it's not useful to create a Family Size attribute.","d4bf35a4":"### Name attribute","d100664a":"### Scale Fare and Age","a61ee30e":"### Create an Is Alone attribute\nThis attribute contain an information whether not the passenger is alone.","7be75f58":"C = Cherbourg; Q = Queenstown; S = Southampton","ff47cb49":"### Remove PassengerId,Ticket and Cabin attribute","ddbcab9b":"## Model and Predict","fc5e2388":"### Features that have missing values in Training Data:\n* Age (177)\n* Cabin (687)\n* Embarked (2)\n\n### Features that have missing values in Test Data:\n* Age (86)\n* Cabin (327)","b7f04b76":"## Exploratory Data Analysis\nGain insight from datasets for prepare the data that fit the model","179d07ab":"### Distribution of Numerical Features\n* Age\n* Fare\n* Sibsp\n* Parch\n","121e3228":"## Feature Engineering\nWe gain some informations from Exploratory Data Analysis process then we get some conclusion like PassengerId, Ticket and Cabin attributes should be deleted and some question like whether Name attribute have impact with passengers safety and what if we combine some feature like SibSp and Parch.","cf2aadc2":"### Naive Bayes Classifier","4d4973e1":"#### Categorical Features\n1. Categorical \n    * Survived\n    * Embarked\n    * Sex \n2. Ordinal feature:\n    * Pclass","bfaa8087":"I replace many titles that have very few users with common name or classify them as Rare. <br>\nThe titles that I want to replace with Rare is:\n1. Capt\n2. Col\n3. Countess\n4. Don\n5. Dr\n6. Jonkheer\n7. Lady\n8. Major\n9. Mme\n10. Rev\n11. Sir","5e78398d":"Fare and Age is continuous so I have to convert them to ordinal attributes.","f8b66413":"Steps:\n1. Convert the values in Sex attribute and Embarked attribute to numeric\n2. Check the correlation of some attributes with survived attribute","c31cf274":"Seems like PassengerId attribute has very litle correlation with Target Variable.","38aa0332":"## Results","080218fa":"## Import Necessary Modules ","d6d75304":"Title have the biggest correlation with Survived attribute!","331da959":"### Statistics Descriptive","bdb44a2f":"I want to predict who will be survived in titanic disaster with some predictors in dataset. I'll performing Supervised Machine Learning and Classification task. The following are some of the algorithms I will try:\n1. Logistic Regression\n2. Naive Bayes Classifier\n3. Nearest Neighbor\n4. Support Vector Machines\n5. Decision Trees","c47da5e1":"### Prepare Train Set and Test Set","02a65da7":"### Logistic Regression","95015a9d":"#### Numerical Features\n1. Continuous \n    * Age\n    * Fare\n2. Discrete\n    * Sibsp\n    * Parch","2fa16d97":"### Datatypes","4abc46ba":"# Titanic Prediction"}}