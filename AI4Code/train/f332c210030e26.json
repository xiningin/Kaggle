{"cell_type":{"bc1c3560":"code","aa48ce92":"code","76909aa7":"code","54cb77fb":"code","4a0d0831":"code","561fba94":"code","beb2ef10":"code","94ce9991":"code","f271c993":"code","0ad607e6":"code","912190db":"code","291972eb":"code","11f1393c":"code","a2efc08e":"code","9dc3d3e8":"code","457fbb45":"code","30d56c9b":"code","49cbd5f6":"code","62170b4e":"code","4093581a":"code","1fd4b2c1":"code","820a5704":"code","052caa8c":"code","d15d75d3":"code","ab9c835a":"code","14021810":"code","6fbf6cfd":"code","505b52c9":"code","19b38f45":"code","95f85203":"code","6ae448f2":"code","3fde86f6":"code","8332d0a1":"code","c787bf44":"code","e017f028":"code","e80f6524":"code","88b1a979":"code","a4e75b58":"code","1c477d01":"code","3299342e":"code","96ec8200":"code","e769a76c":"code","dbaf3360":"code","25496ef2":"code","2104b552":"code","104b8620":"code","7bd3077d":"code","44dca01a":"code","84a2593c":"code","2e3231b3":"code","f56a8a74":"code","43c0d931":"code","20e5332a":"code","5b4a5625":"code","66ee4acf":"code","23148a27":"markdown","a5279f36":"markdown","6c40efb7":"markdown","540a75ff":"markdown","28e24924":"markdown","9e11af0d":"markdown","b6566f6b":"markdown","040eca77":"markdown","01f5a1e5":"markdown","7f4882ba":"markdown"},"source":{"bc1c3560":"import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport string\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB,BernoulliNB","aa48ce92":"glove = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\nparagram =  '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\nwiki_news = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'","76909aa7":"df = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape : \",df.shape)\nprint(\"Test shape : \",test.shape)","54cb77fb":"df.info()\ntest.info()","4a0d0831":"df.head(n=2)","561fba94":"df.shape","beb2ef10":"df.describe()","94ce9991":"df.isnull().sum()\ntest.isnull().sum()","f271c993":"df.where(df['target']==1).count()","0ad607e6":"df.where(df['target']==0).count()","912190db":"sincere_questions = df[df['target'] == 0]\ninsincere_questions = df[df['target'] == 1]\ninsincere_questions.tail(5)","291972eb":"question = df['question_text']\ni=0\nfor q in question[:5]:\n    i=i+1\n    print('sample '+str(i)+':' ,q)","11f1393c":"df[\"num_words\"] = df[\"question_text\"].apply(lambda x: len(str(x).split()))\n\nprint('maximum of num_words in train',df[\"num_words\"].max())\nprint('min of num_words in train',df[\"num_words\"].min())\n","a2efc08e":"df[\"num_unique_words\"] = df[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n#test[\"num_unique_words\"] = test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\nprint('maximum of num_unique_words in train',df[\"num_unique_words\"].max())\nprint('mean of num_unique_words in train',df[\"num_unique_words\"].mean())\n#print(\"maximum of num_unique_words in test\",test[\"num_unique_words\"].max())\n#print('mean of num_unique_words in train',test[\"num_unique_words\"].mean())","9dc3d3e8":"df[\"num_stopwords\"] = df[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#test[\"num_stopwords\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\nprint('maximum of num_stopwords in train',df[\"num_stopwords\"].max())\n#print(\"maximum of num_stopwords in test\",test[\"num_stopwords\"].max())","457fbb45":"df[\"num_punctuations\"] =df['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n#test[\"num_punctuations\"] =test['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nprint('maximum of num_punctuations in train',df[\"num_punctuations\"].max())\n#print(\"maximum of num_punctuations in test\",test[\"num_punctuations\"].max()","30d56c9b":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nfrom nltk.stem import WordNetLemmatizer\n\nlemm_ = WordNetLemmatizer()\nst = PorterStemmer()\nstops = set(stopwords.words(\"english\"))\ndef cleanData(text, lowercase = True, remove_stops = True, stemming = False, lemma = True):\n\n    txt = str(text)\n    txt = re.sub(r'[^a-zA-Z. ]+|(?<=\\\\d)\\\\s*(?=\\\\d)|(?<=\\\\D)\\\\s*(?=\\\\d)|(?<=\\\\d)\\\\s*(?=\\\\D)',r'',txt)\n    txt = re.sub(r'\\n',r' ',txt)\n    \n    #converting to lower case\n    if lowercase:\n        txt = \" \".join([w.lower() for w in txt.split()])\n    \n    # removing stop words\n    if remove_stops:\n        txt = \" \".join([w for w in txt.split() if w not in stops])\n    \n    # stemming\n    if stemming:\n        txt = \" \".join([st.stem(w) for w in txt.split()])\n        \n    if lemma:\n        txt = \" \".join([lemm_.lemmatize(w) for w in txt.split()])\n\n    return txt\n","49cbd5f6":"df['clean_question_text'] = df['question_text'].map(lambda x: cleanData(x))\ntest['clean_question_text'] = test['question_text'].map(lambda x: cleanData(x))\n","62170b4e":"test['clean_question_text']","4093581a":"max_features = 50000  ##More than this would filter in noise also\ntfidf_vectorizer = TfidfVectorizer(ngram_range =(2,4) , max_df=0.90, min_df=5, max_features=max_features) ##4828 features found\n#tfidf_feature_names = tfidf_vectorizer.get_feature_names()","1fd4b2c1":"X = tfidf_vectorizer.fit_transform(df['clean_question_text'])\nX_te = tfidf_vectorizer.transform(test['clean_question_text'])\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\n","820a5704":"from gensim.models import KeyedVectors\n\nnews_path = '..\/input\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)","052caa8c":"y = df[\"target\"]","d15d75d3":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3,random_state=42)","ab9c835a":"# Classification and prediction\nclf = LogisticRegression(C=10, penalty='l1')\nclf.fit(X_train, y_train)","14021810":"clf.score(X_val, y_val)","6fbf6cfd":"p_test = clf.predict_proba(X_te)[:, 0]\ny_te = (p_test > 0.5).astype(np.int)","505b52c9":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport nltk\n\nstop = stopwords.words('english')\nstemmer = PorterStemmer()\nlem = nltk.WordNetLemmatizer()\neng_stopwords = set(stopwords.words(\"english\"))","19b38f45":"#df['question_text'] = df['question_text'].apply(lambda x: x.lower())\ndf['question'] = df['question_text'].str.replace(r\"[^a-z0-9 ]\", '')\ndf['tokens'] = df['question'].apply(word_tokenize)\ndf['tokens'] = df['tokens'].map(lambda x: [word for word in x if word not in eng_stopwords])\ndf['lems'] = df['tokens'].map(lambda x: [lem.lemmatize(word) for word in x])","95f85203":"df['lems']","6ae448f2":"from gensim.corpora import Dictionary\n\ndicti = Dictionary(df['lems'])\n\nbow = [dicti.doc2bow(line) for line in df['lems']]","3fde86f6":"# TODO: Compute TF-IDF\nfrom gensim.models import TfidfModel\n\ntfmodel = TfidfModel(bow)\n\ntfidf = tfmodel[bow]","8332d0a1":"from gensim.models import LsiModel\n\nlsa = LsiModel(corpus = tfidf, num_topics=10, id2word = dicti)","c787bf44":"from pprint import pprint\n\npprint(lsa.print_topics(num_words=4))","e017f028":"from gensim.models import LdaModel\nlda = LdaModel(corpus = tfidf, num_topics=6, id2word = dicti, passes=5)","e80f6524":"pprint(lda.print_topics(num_words=4))","88b1a979":"import pyLDAvis\n\nimport pyLDAvis.gensim\n\n# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda, bow, dicti)\nvis","a4e75b58":"ax=sns.countplot(x='target',hue=\"target\", data=df  ,linewidth=1,edgecolor=sns.color_palette(\"dark\", 3))\nplt.title('Data set distribution');","1c477d01":"from sklearn.model_selection import train_test_split \nimport nltk","3299342e":"def build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n","96ec8200":"def clean_text(x):\n\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    return x","e769a76c":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport nltk\n\nstop = stopwords.words('english')\nstemmer = PorterStemmer()\nlem = nltk.WordNetLemmatizer()\neng_stopwords = set(stopwords.words(\"english\"))","dbaf3360":"df[\"question_text\"] = df[\"question_text\"].apply(lambda x: clean_numbers(x))\ndf[\"question_text\"] = df[\"question_text\"].apply(lambda x: clean_text(x))\ndf['lowered_question'] = df['question_text'].apply(lambda x: x.lower())\ndf['question'] = df['lowered_question'].str.replace(r\"[^a-z0-9 ]\", '')\ndf['tokens'] = df['question'].apply(nltk.word_tokenize)\ndf['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in eng_stopwords])\ndf['lems'] = df['tokens'].apply(lambda x: [lem.lemmatize(word) for word in x])","25496ef2":"df_train = pd.DataFrame(df['lems'])\ndf_train","2104b552":"def clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","104b8620":"import re","7bd3077d":"sentences = df_train['lems'].apply(lambda x: x.split())\nvocab = build_vocab(sentences)","44dca01a":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","84a2593c":"oov = check_coverage(vocab,embeddings_index)","2e3231b3":"oov[:10]","f56a8a74":"train_text['lems'] = df['lems']\ntrain_text['lems']","43c0d931":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","20e5332a":"tfidf_vectorizer = TfidfVectorizer()\ntfidf_vectorizer.fit(all_text)\n\ncount_vectorizer = CountVectorizer()\ncount_vectorizer.fit(all_text)\n\ntrain_text_features_cv = count_vectorizer.transform(train_text)\ntest_text_features_cv = count_vectorizer.transform(test_text)\n\ntrain_text_features_tf = tfidf_vectorizer.transform(train_text)\ntest_text_features_tf = tfidf_vectorizer.transform(test_text)","5b4a5625":"kfold = KFold(n_splits = 5, shuffle = True, random_state = 2018)\ntest_preds = 0\noof_preds = np.zeros([df.shape[0],])\n\nfor i, (train_idx,valid_idx) in enumerate(kfold.split(df['lems'])):\n    x_train, x_valid = train_text_features_tf[train_idx,:], train_text_features_tf[valid_idx,:]\n    y_train, y_valid = train_target[train_idx], train_target[valid_idx]\n    classifier = LogisticRegression()\n    print('fitting.......')\n    classifier.fit(x_train,y_train)\n    print('predicting......')\n    print('\\n')\n    oof_preds[valid_idx] = classifier.predict_proba(x_valid)[:,1]\n    test_preds += 0.2*classifier.predict_proba(test_text_features_tf)[:,1]","66ee4acf":"pred_train = (oof_preds > .25).astype(np.int)\nf1_score(train_target, pred_train)","23148a27":"# Data Visualization","a5279f36":"#### Number of stopwords in the text","6c40efb7":"## Data Preprocessing","540a75ff":"## Explorer Dataset","28e24924":"#### Number of punctuations in the text","9e11af0d":"A large part of the data is unbalanced, but how can we solve it?","b6566f6b":"## Some Feature Engineering","040eca77":"## Exploreing Questions","01f5a1e5":"Few pointers I have is;\n\n1. Are they toxic with targetted words on race\/region\/religion?\n2. Do they contain obscene words ? Are these questions long or short?\n3. And, dividing them into clusters will help my model predict - what cluster of insincerity does an insincere question lies in .... etc\n\nSo, let's check the head frame now..","7f4882ba":"\nBasic Logistic Regression"}}