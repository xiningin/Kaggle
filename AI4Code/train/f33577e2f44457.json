{"cell_type":{"7c52f990":"code","4db87b4d":"code","c653b26c":"code","4b2615d0":"code","e0070ec6":"code","eed5826c":"code","d743e8f2":"code","0eea1972":"code","6bb1840f":"code","30f73f4a":"code","41e4401d":"code","01dc424b":"code","f27cbd50":"code","1c142838":"code","ad10320b":"code","13840fe9":"code","04f148e1":"code","3344aa46":"code","4b2af708":"code","53a27b05":"code","4e7fbec8":"code","aa960b87":"code","eb55f2c2":"code","5345e784":"code","adb35cf0":"code","0c22062b":"code","49284489":"code","04b11c5f":"code","be0d5dfa":"code","7664423b":"code","5b971222":"code","eb215675":"code","f360bc6d":"code","5b8b93b6":"code","8f2f4e03":"code","e43b87a1":"code","005eaf60":"code","e875c63f":"code","ef815048":"code","8bbdccdd":"code","bc0987c0":"code","05ea374b":"code","d931fb52":"code","915eee7c":"code","72a14874":"code","1bf8246d":"code","add8e963":"code","67af40cb":"code","34450d57":"code","b9fa5ea3":"code","3d67326a":"code","72615c3f":"code","780de7b2":"code","a17e6b72":"code","628cb409":"code","c2521b8b":"code","90fc135a":"code","f0e08d9c":"code","fab6cafe":"code","425d6d04":"code","c4c2d2e9":"code","1b24b923":"code","fcc98e97":"code","8095d0a7":"code","243bad0b":"code","9df8ebbf":"code","ce4e6b10":"code","46d9aad6":"code","86895047":"code","147c4c67":"code","9d490617":"code","6abca624":"code","cedb9a0a":"code","1f90ec65":"code","785c99ec":"code","5a2517af":"code","2c5e6257":"markdown","d0d986f4":"markdown","e2f645e4":"markdown","8acf034c":"markdown","8a899057":"markdown","809dc3f2":"markdown","301783a6":"markdown","4394558a":"markdown","c1180f42":"markdown"},"source":{"7c52f990":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n! pip install geocoder\nimport geocoder\nimport tensorflow_hub as hub","4db87b4d":"train=pd.read_csv(r'..\/input\/twitter-prediction-disaster\/train.csv')","c653b26c":"train.head()","4b2615d0":"test=pd.read_csv(r\"\\test.csv\")","e0070ec6":"test.head()","eed5826c":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","d743e8f2":"# Class distribution\n\nx=train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","0eea1972":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len_prob =train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len_prob,color='red')\nax1.set_title(\"Disaster_tweets\")\ntweet_len_noprob=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len_noprob,color='green')\nax2.set_title(\"Non_disastor_tweets\")","6bb1840f":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len_word_dis = train[train['target']==1]['text'].str.split().map(lambda x:len(x))\nax1.hist(tweet_len_word_dis,color='red')\nax1.set_title(\"Disastor_tweets\")\ntweet_len_word_nodis = train[train['target']==0]['text'].str.split().map(lambda x:len(x))\nax2.hist(tweet_len_word_nodis,color='green')\nax2.set_title(\"No_Disastor_tweets\")","30f73f4a":"fig,(ax1,ax2)= plt.subplots(1,2,figsize=(10,5))\nword=train[train['target']==1]['text'].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word.map(lambda x:np.mean(x)),ax=ax1,color='red')\nax1.set_title(\"disastor_tweet\")\nword_nodis =train[train['target']==0]['text'].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word.map(lambda x:np.mean(x)),ax=ax2,color='green')\nax2.set_title(\"no_disastor_tweet\")","41e4401d":"def create_corpus(target):\n    corpus=[]\n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return(corpus)","01dc424b":"corpus=create_corpus(0)\nlen(corpus)","f27cbd50":"dic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\ndic","1c142838":"top=sorted(dic.items(),key=lambda x:x[1],reverse=True)[:10]\ntop","ad10320b":"x,y=zip(*top)\nplt.bar(x,y)","13840fe9":"corpus1=create_corpus(1)\ndic1=defaultdict(int)\nfor word in corpus1:\n    if word in stop:\n        dic1[word]+=1\n","04f148e1":"top1 = sorted(dic1.items(),key=lambda x:x[1],reverse=True)[:10]","3344aa46":"top1","4b2af708":"x1,y1=zip(*top1)\nplt.bar(x1,y1)","53a27b05":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\ndic=defaultdict(int)\nimport string\nspecial=string.punctuation\nfor i in corpus:\n    if i in special:\n        dic[i]+=1\nx,y=zip(*dic.items())\nplt.bar(x,y)","4e7fbec8":"corpus=create_corpus(0)\ndic=defaultdict(int)\nfor i in corpus:\n    if i in special:\n        dic[i]+=1\nx,y=zip(*dic.items())\nplt.bar(x,y)","aa960b87":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)","eb55f2c2":"sns.barplot(x=x,y=y)","5345e784":"df=train.append(test).reset_index(drop=True)","adb35cf0":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","0c22062b":"def remove_url(text):\n    url = re.compile(r'https:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\nremove_url(example)","49284489":"df['text']=df['text'].apply(lambda x : remove_url(x))","04b11c5f":"def remove_html(text):\n    html=re.compile('<.*?>')\n    return html.sub(r'',text)\n","be0d5dfa":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"\n\nprint(remove_html(example))","7664423b":"df['text']=df['text'].apply(lambda x : remove_html(x))","5b971222":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","eb215675":"remove_emoji(\"Omg #another Earthquake \ud83d\ude14\ud83d\ude14\")","f360bc6d":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","5b8b93b6":"def remove_punct(text):\n    try:\n        table =str.maketrans(\"\",\"\",string.punctuation)\n        return text.translate(table)\n    except:\n        return(text)","8f2f4e03":"print(remove_punct(\"is , an the value\"))","e43b87a1":"df['text']=df['text'].apply(lambda x: remove_punct(x))","005eaf60":"from spellchecker import SpellChecker","e875c63f":"spell=SpellChecker()\ndef spell_correction(text):\n    try:\n        spelled =[]\n        uncorrect =spell.unknown(text.split())\n        for word in text.split():\n            if word in uncorrect:\n                spelled.append(spell.correction(word))\n            else:\n                spelled.append(word)\n        return(\" \".join(spelled))\n    except:\n        return(text)","ef815048":"example=\"help me plese\"\nspell_correction(example)","8bbdccdd":"def ascii_remover(text):\n    try:\n        all_ascii = ''.join(char for char in text if ord(char) < 128)\n        return(all_ascii)\n    except:\n        return(text)","bc0987c0":" ascii_remover(\"45\u00c3\u00a5\u00c2\u00a1 5'12.53N   14\u00c3\u00a5\u00c2\u00a1 7'24.93E\")\n","05ea374b":"df['text']=df['text'].apply(lambda x:ascii_remover(x))","d931fb52":"df['location']=df['location'].apply(lambda x:ascii_remover(x))","915eee7c":"def replace(text):\n    try:\n        \n        text_replace=text.replace(\"T: \", \"\")\n        return(text_replace)\n    except:\n        return(text)","72a14874":"df['location']=df['location'].apply(lambda x:replace(x))","1bf8246d":"#df['location']=df['location'].apply(lambda x: remove_punct(x))","add8e963":"from geopy.geocoders import Nominatim","67af40cb":"def reverse_geocoder(text):\n    \n    geolocator = Nominatim()\n    try:\n        location = geolocator.reverse(text)\n        address=location.address\n        if address is None:\n            return(text)\n        else:\n            return(address)\n    except:\n        return(text)\n    ","34450d57":"df1=df[df['id']==9662]\ndf1","b9fa5ea3":"df1['location']=df1['location'].apply(lambda x:reverse_geocoder(x))\ndf1","3d67326a":"df['location']=df['location'].apply(lambda x:reverse_geocoder(x))","72615c3f":"df['location']=df['location'].apply(lambda x:spell_correction(x))","780de7b2":"def string_capitalize(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(word.capitalize())\n        return(\" \".join(word1))\n        \n    except:\n        return(text)","a17e6b72":"df['location']=df['location'].apply(lambda x:string_capitalize(x))\ndf['text']=df['text'].apply(lambda x:string_capitalize(x))\ndf['text']=df['text'].apply(lambda x:spell_correction(x))","628cb409":"from geotext import GeoText","c2521b8b":"def geo_cities(text):\n    try:\n        places=GeoText(text)\n        city=places.cities\n        country=places.countries\n\n        if len(city)>0:\n            \n            return(city[0])\n        elif len(country)>0:\n            return(country[0])\n        else :\n            return(text)\n    except:\n        return(text)","90fc135a":"df['location']=df['location'].apply(lambda x: geo_cities(x))","f0e08d9c":"df['location']=df['location'].apply(lambda x: remove_punct(x))","fab6cafe":"df['keyword']=df['keyword'].apply(lambda x:spell_correction(x))","425d6d04":"#nltk.download('averaged_perceptron_tagger')","c4c2d2e9":"from pattern.text.en import singularize\ndef singular(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(singularize(word))\n        return(\" \".join(word1))\n        \n    except:\n        return(text)\n        ","1b24b923":"df['text']=df['text'].apply(lambda x:singular(x))","fcc98e97":"def string_lower(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(word.lower())\n        return(\" \".join(word1))\n        \n    except:\n        return(text)","8095d0a7":"df['text']=df['text'].apply(lambda x:string_lower(x))","243bad0b":"\ndef keyword(text):\n    disastor=['flooding','wildfire','bombed','bagging','kill','dead','apocalypse','calamity','catastrophe','collapse','crash','debacle','defeat','emergency','failure','fiasco','flood','harm','hazard','holocaust','mishap','setback','tragedy','woe','adversity','affliction','bale','bane','blight','blow','bust','casualty','cataclysm','collision','depression','exigency','fall','flop','grief','misadventure','mischance','misfortune','reverse','rock','rough','ruin','ruination','slip','stroke','undoing','upset','washout','act of God','bad luck','bad news','fell stroke','hard luck','hot water','ill luck','the worst','crack-up','disaster','fender-bender','fluke','pileup','rear ender','smash','smashup','stack-up','total','wrack-up','bad break','bummer','can of worms','clutch','contretemps','crunch','difficulty','distress','downer','drag','evil eye','hard knocks','hard times','hardship','hurting','ill fortune','jam','jinx','kiss of death','misery','on the skids','pain in the neck','poison','sorrow','suffering','tough luck','trial','trouble','annoyance','bad trip','disappointment','irritation','bum trip','depressing experience','raw deal','rotten hand','unhappy situation','unpleasant experience','unpleasant situation','unpleasent','burden','b\u00eate noir','curse','despair','destruction','downfall','fatal attraction','nuisance','pest','plague','scourge','torment','venom','balk','bolt from the blue','bombshell','chagrin','comedown','disgruntlement','frustration','jolt','letdown','shock','chance','contingency','accident','alluvion','culmination','curtains','denouement','desolation','devastation','end','fatality','finale','havoc','ill','infliction','meltdown','termination','upshot','waterloo','wreck','embarrassment','predicament','agitation','clamor','commotion','ferment','furor','outcry','quaking','rocking','seism','shaking','tottering','trembling','tumult','turbulence','upheaval','upturn','big trouble','change','climacteric','climax','confrontation','corner','crossroad','crux','deadlock','dilemma','dire straits','entanglement','extremity','height','hot potato','hour of decision','imbroglio','impasse','juncture','mess','moment of truth','necessity','pass','perplexity','pickle','pinch','plight','point of no return','pressure','puzzle','quandary','situation','stew','strait','trauma','turning point','urgency','Cancer','cross','evil','hydra','ordeal','pestilence','tribulation','vexation','voodoo','beating','blue ruin','breakdown','defeasance','dissolution','drubbing','licking','overthrow','reversal','rout','shellacking','trouncing','vanquishment','bitter pill','blind alley','blunder','bringdown','discouragement','dud','error','false alarm','faux pas','fizzle','flash in the pan','inefficacy','lemon','miscalculation','mistake','obstacle','old one-two','destitution','hard time','holy mess','indigence','need','poverty','privation','rigor','rotten luck','scrape','straits','throe','ticklish spot','tough break','unholy mess','vicissitude','want','Judgment Day','Moira','annihilation','circumstance','conclusion','condemnation','death','decree','destination','destiny','fixed future','foreordination','fortune','handwriting on wall','judgment','karma','kismet','lap of the gods','lot','opinion','portion','predestination','predetermination','sentence','verdict','way the ball bounces','way the cookie crumbles','acme','acuteness','apex','apogee','border','bound','boundary','brim','brink','butt','consummation','crisis','depth','edge','excess','extreme','extremes','frontier','last','margin','maximum','nadir','outside','pinnacle','pole','remote','rim','terminal','terminus','tip','top','verge','vertex','zenith','abasement','capitulation','degradation','diminution','dive','drop','humiliation','loss','resignation','surrender','tumble','deadliness','destructiveness','dying','inevitability','lethality','lethalness','mortality','necrosis','noxiousness','poisonousness','virulence','abortion','botched situation','dumb thing to do','dumb trick','farce','flap','miscarriage','route','screwup','stunt','bomb','loser','nonstarter','Herculean task','asperity','austerity','case','danger','discomfort','drudgery','fatigue','grievance','injury','labor','oppression','peril','persecution','rainy day','toil','travail','uphill battle','worry','ache','black and blue','boo-boo','bruise','chop','detriment','disadvantage','disservice','down','gash','ill-treatment','mark','mischief','nick','ouch','outrage','pain','pang','prejudice','scratch','sore','soreness','wound','wrong','bereavement','cost','damage','debit','debt','deficiency','depletion','deprivation','disappearance','dispossession','forfeiture','hurt','impairment','losing','mislaying','misplacing','perdition','retardation','sacrifice','shrinkage','squandering','waste','wreckage','lapse','ill-fortune','rear-ender','crime','crying shame','regret','shame','sin','clobbering','confusion','flight','hiding','retreat','romp','shambles','shutout','thrashing','trashing','walkover','waxing','whipping','about-face','alteration','convulsion','disorder','disruption','disturbance','eruption','explosion','flip-flop','new ball-game','new deal','outbreak','outburst','revolution','shakeout','stirring','switch','temblor','tremor','turmoil','turnaround','punishment','retribution','downpour','erosion','gully','agony','anguish','bemoaning','blues','care','dejection','deploring','dole','gloom','grieving','headache','heartache','heartbreak','lamentation','melancholy','rain','rue','sadness','unhappiness','wretchedness','attack','armageddon','aftershock','typhoon','asteroid','tsunami','natural disasters','volcano','tornado','avalanche','earthquake','blizzard','drought','bushfire','dust storm','magma','twister','windstorm','heat wave','cyclone','forest fire','fire','hailstorm','lava','lightning','high-pressure','hail','hurricane','seismic','whirlpool','Richter scale','whirlwind','cloud','thunderstorm','barometer','gale','blackout','gust','force','low-pressure','volt','snowstorm','rainstorm','storm','nimbus','violent storm','sandstorm','Beaufort scale','fatal','cumulonimbus','lost','money','tension','uproot','underground','destroy','arsonist','wind scale','arson','rescue','permafrost','fault','shelter','ablaze'\n]\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            if word in disastor:\n                word1.append(word)\n        return(\",\".join(word1))\n    \n    except:\n        return(\"\")","9df8ebbf":"y1 = df.loc[df['keyword']=='']\nindex_y1=list(y1.index.values)\nprint(index_y1)\n\nfor i in index_y1:\n    df['keyword'].iat[i]=keyword(df['text'].iat[i])","ce4e6b10":"df['keyword'].iat[3]","46d9aad6":"df['text']=df['text'].apply(lambda x:string_capitalize(x))","86895047":"from nltk import word_tokenize, pos_tag, ne_chunk\nfrom nltk import Tree\ndef get_continuous_chunks(text, label):\n    try:\n        chunked = ne_chunk(pos_tag(word_tokenize(text)))\n        prev = None\n        continuous_chunk = []\n        current_chunk = []\n\n        for subtree in chunked:\n            if type(subtree) == Tree and subtree.label() == label:\n                current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n            elif current_chunk:\n                named_entity = \" \".join(current_chunk)\n                if named_entity not in continuous_chunk:\n                    continuous_chunk.append(named_entity)\n                    current_chunk = []\n            else:\n                continue\n\n        return continuous_chunk\n    except:\n        return(\"\")","147c4c67":"get_continuous_chunks(\"Haha South Tampa Is Getting Flooded Hah Wait A Second I Live In South Tampa What Am I Gonna Do What Am I Gonna Do Fvck Flooding\",'GPE')","9d490617":"y = df.loc[df['location']==\"\"]\nindex_y=list(y.index.values)\nprint(index_y)\n\nfor i in index_y:\n    df['location'].iat[i]=get_continuous_chunks(df['text'].iat[i],'LOCATION')","6abca624":"y = df.loc[df['location']==\"\"]\nindex_y=list(y.index.values)\nprint(index_y)\n\nfor i in index_y:\n    df['location'].iat[i]=get_continuous_chunks(df['text'].iat[i],'GPE')","cedb9a0a":"df.head(100)","1f90ec65":"def remove_ing(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(word.replace(\"ing\",\"\"))\n        return(\" \".join(word1))\n    \n    except:\n        return(\"\")","785c99ec":"df['text']=df['text'].apply(lambda x:remove_ing(x))\ny1 = df.loc[df['keyword']=='']\nindex_y1=list(y1.index.values)\nprint(index_y1)\n\nfor i in index_y1:\n    df['keyword'].iat[i]=keyword(df['text'].iat[i])","5a2517af":"df.to_csv(r'\\Twitter Prediction\\output_datafile.csv', index=False)","2c5e6257":"## Spread of Punctuations","d0d986f4":"## Character length distribution","e2f645e4":"Certain feature entries have \"T:\" as a suffix within column location, so remove them using customized function [ it's just one of the way to resolve it]","8acf034c":"## Perc distribution of word length per tweet","8a899057":"Dataset used belongs to the \"twitter-disastor-prediction\" competition , with target is binary suggestion if the features actually indiactes a disastor or not ","809dc3f2":"**Reverse Geocoding** - A technique to convert the lat\/long entries to the textual geolocation address [Building No,Block No,Town\/City,Country]","301783a6":"## Word Length Distribution","4394558a":"Data Cleansing - Observed there are certain ASCII characters present with the lat\/long entries , need to clean them before we can utilise those entries.","c1180f42":"Created a custom list \"disastor\" with a lot of synonyms or indicators and performed a grid search with in the text data column and if presented filled within empty cells of \"keyword\" column"}}