{"cell_type":{"02eef385":"code","bf6045fe":"code","c6dbfbbe":"code","a69f364a":"code","158ebaec":"code","4d94f181":"code","2edd5f38":"code","1ee73ff5":"code","8543ac45":"code","a26418a6":"code","ad51f170":"code","57d24cd7":"code","0cf9c5cb":"code","f3bbd4fb":"code","833b601a":"code","f48066c6":"code","15b12c56":"code","20915f8d":"code","9d0839ef":"code","3677ef24":"code","d233c581":"code","aba8b14d":"code","6d1618a2":"code","255246e8":"code","56961dc8":"code","7e75a6fe":"code","8efac9fb":"code","73a60174":"code","2044c7ba":"code","7508b3ae":"code","44d04f9d":"code","850963ad":"code","63a3bf88":"code","edce3a62":"code","91c6639d":"code","8c057b76":"code","f9541d8f":"code","32c58a19":"code","61a6513b":"code","a40b0508":"code","062af27c":"code","3c5206b9":"code","4c68d350":"code","a0afe3d3":"code","7962db0f":"code","f5f72937":"code","69fdfe88":"code","93c0d215":"code","629b228d":"code","b988eb10":"code","397dce83":"code","ae49f280":"code","23a93051":"code","29dca2bf":"code","d541e9b0":"code","a7e38c82":"code","fb4aed0a":"code","1e8ccee7":"code","e0698d98":"code","87388bf9":"code","e3f9bf23":"code","6c4506e9":"code","f9a7248f":"code","f46baf5f":"code","f63c8a72":"code","f4d5b762":"code","69e7e0af":"code","9138ea4f":"code","61699039":"code","d79051d7":"code","a74ea74d":"code","d41cb4a6":"code","beb74b7a":"code","6a5a8440":"code","1340787b":"code","99603fab":"code","63d6344b":"code","a7a4e460":"code","86984a51":"code","d0f4c81b":"code","f293d146":"code","ce01ca25":"code","594f93fc":"code","c187380c":"code","94885c8c":"code","1f40c091":"code","e5d4047b":"code","ec0e86c3":"code","2a2a64a6":"code","b84966bf":"code","b20444d0":"markdown","f5b5329c":"markdown","89ebb4b1":"markdown","5cd9b42e":"markdown","4d9402b1":"markdown","4ec7daa8":"markdown","10107a59":"markdown","9925b17c":"markdown","39ff9711":"markdown","90ec06cd":"markdown","9d59c3e6":"markdown","5aa391a2":"markdown","2d14b3aa":"markdown","d903ab1f":"markdown","e83f20c6":"markdown","22ba0917":"markdown","8808a271":"markdown","f3a0bd92":"markdown","1062bfbd":"markdown","dada48ae":"markdown","85ea63e8":"markdown","360ee190":"markdown","be23c0b2":"markdown","dcbbf5c4":"markdown","dd6cd8ec":"markdown","8b4b1f9d":"markdown","62f1c227":"markdown","5ecc464d":"markdown","f618fadf":"markdown","4b39a7b6":"markdown","83138d53":"markdown","836c9c17":"markdown","a6fd56e8":"markdown","e24e2dce":"markdown","92fde09d":"markdown","84536327":"markdown","3256e6d3":"markdown","abb64b88":"markdown","ff8d1ab1":"markdown","77bddcc2":"markdown","31c999e6":"markdown"},"source":{"02eef385":"# importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\nfrom collections import Counter, defaultdict\n%matplotlib inline","bf6045fe":"# display 100 columns\npd.set_option('display.max_columns', 100)","c6dbfbbe":"# loading the dataset\ntrain_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","a69f364a":"train_df.head(5)","158ebaec":"train_df.info()","4d94f181":"test_df.info()","2edd5f38":"print(f\"Rows | Columns\\n{train_df.shape}\")\nprint(test_df.shape)","1ee73ff5":"table = plt.table(cellText=[[train_df.location.isnull().sum()], [test_df.location.isnull().sum()]],\n         rowLabels=['Train Data', 'Test Data'],\n         colLabels=['Number of Missing values'],\n         loc='top')\nplt.box(on=None)\nplt.axis('off')\n# plt.subplots_adjust(top = 1, bottom = 0.1, right = 1, left = 0, \n#             hspace = 0, wspace = 0)\n#plt.margins(0,0)\ntable.set_fontsize(14);","8543ac45":"# checking for missing values\nfig, (ax1, ax2) = plt.subplots(2,1, figsize=(8,5))\n\nsns.heatmap(train_df.isnull(),yticklabels=False,cbar=False,cmap='flare', ax = ax1)\nax1.tick_params(axis='x', labelsize=13, rotation = 45)\nax1.set_title('train data')\n\nsns.heatmap(test_df.isnull(),yticklabels=False,cbar=False,cmap='flare', ax = ax2)\nax2.tick_params(axis='x', labelsize=13, rotation = 45)\nax2.set_title('test data')\nfig.tight_layout();","a26418a6":"# we notice many missing values for the location column","ad51f170":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain_df.at[train_df['id'].isin(ids_with_target_error),'target'] = 0\ntrain_df[train_df['id'].isin(ids_with_target_error)]","57d24cd7":"# checking for duplicates\nprint(train_df.duplicated(subset=['text']).sum())\nprint(test_df.duplicated(subset=['text']).sum())","0cf9c5cb":"print(train_df.duplicated(subset=['text','target']).sum())","f3bbd4fb":"set1 = set(train_df[train_df.duplicated(subset=['text'])]['id'].values)","833b601a":"set2 = (train_df[train_df.duplicated(subset=['text','target'])]['id'].values)","f48066c6":"tweet_ids_possible_wrong = set1.difference(set2)\n# those are the tweets which have duplicated text but different label\nprint(train_df[train_df['id'].isin(tweet_ids_possible_wrong)].text.values)\nprint(train_df[train_df['id'].isin(tweet_ids_possible_wrong)].target.values)","15b12c56":"# setting the argument keep=False, drops all the duplicates\ntrain_df.drop_duplicates(subset=['text'], keep=False, inplace=True)","20915f8d":"train_df.shape","9d0839ef":"fig = plt.figure(figsize=(20,5))\ntrain_df.keyword.value_counts().head(25).plot(kind = 'bar')\nplt.tick_params(axis='x', labelsize=11, rotation = 45)\nplt.title('Top keywords');","3677ef24":"fig = plt.figure(figsize=(20,5))\ntrain_df.location.value_counts().head(25).plot(kind = 'bar')\nplt.tick_params(axis='x', labelsize=9, rotation = 45)\nplt.title('Top Locations');","d233c581":"fig = plt.figure(figsize=(7,5))\nsns.countplot(data = train_df, x = \"target\")\nplt.title('Non-actual vs actual distater tweets counts');","aba8b14d":"fig = plt.figure(figsize=(7,5))\n(train_df.groupby(['target']).count() \/ len(train_df['target']))['id'].plot(kind='bar', width = 0.85, color = ['tomato', 'steelblue'])\nplt.title('Non-actual vs actual distater tweets percentage');","6d1618a2":"# Somes actual tweets (non-disaster)\nprint(train_df.text[train_df['target'] == 0][:10].values)","255246e8":"print(train_df.text[train_df['target'] == 1][:10].values)","56961dc8":"from wordcloud import WordCloud","7e75a6fe":"wordcloud_non_dis = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(train_df.text[train_df['target'] == 0]))\nwordcloud_dis = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(train_df.text[train_df['target'] == 1]))","8efac9fb":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,14))\n\nax1.imshow(wordcloud_non_dis, interpolation='bilinear')\nax1.set_title('Non disaster')\nax1.axis(\"off\")\nax2.imshow(wordcloud_dis, interpolation='bilinear')\nax2.set_title('Disaster')\nax2.axis(\"off\");","73a60174":"train_df['location'] = train_df['location'].str.lower()\ntrain_df['location'] = train_df['location'].str.strip()\ntest_df['location'] = train_df['location'].str.lower()\ntest_df['location'] = train_df['location'].str.strip()","2044c7ba":"loc_dict = {'united states':'usa',\n                            'us':'usa',\n                            'united kingdom':'uk',\n                              'nyc':'new york',\n                             'london, uk': 'london',\n                              'london, england':'london',\n                            'new york, ny':'new york',\n                            'everywhere':'worldwide'}","7508b3ae":"train_df['location'].replace(loc_dict, inplace=True)\ntest_df['location'].replace(loc_dict, inplace=True)","44d04f9d":"fig, axes = plt.subplots(1,2,figsize=(25,6))\nax1 = train_df[train_df['target']==0]['location'].value_counts().head(25).plot(kind = 'bar', color = 'b', alpha = 0.5, ax=axes[0])\nax1.tick_params(axis='x', labelsize=9, rotation = 45)\nax1.set_title('Non-Disaster')\nax2 = train_df[train_df['target']==1]['location'].value_counts().head(25).plot(kind = 'bar', color = 'r', alpha = 0.5, ax=axes[1])\nax2.tick_params(axis='x', labelsize=9, rotation = 45)\nax2.set_title('Disaster');","850963ad":"fig, axes = plt.subplots(1,2,figsize=(25,6))\nax1 = train_df[train_df['target']==0]['keyword'].value_counts().head(25).plot(kind = 'bar', color = 'b', alpha = 0.5, ax=axes[0])\nax1.tick_params(axis='x', labelsize=9, rotation = 45)\nax1.set_title('Top Keywords (non disaster)')\nax2 = train_df[train_df['target']==1]['keyword'].value_counts().head(25).plot(kind = 'bar', color = 'r', alpha = 0.5, ax=axes[1])\nax2.tick_params(axis='x', labelsize=9, rotation = 45)\nax2.set_title('Top Keywords (disaster)');","63a3bf88":"import string\nimport emoji\nimport nltk\nimport re","edce3a62":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","91c6639d":"def convert_abb(x):\n    word_list = x.split()\n    r_string = []\n    for word in word_list:\n        r_string.append(abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word)\n    return ' '.join(r_string)\n\ntest = 'afk hello world!'\nconvert_abb(test)","8c057b76":"train_df['text'] = train_df.text.apply(convert_abb)\ntest_df['text'] = test_df.text.apply(convert_abb)","f9541d8f":"train_df['clean_text'] = train_df.text.apply(lambda x: re.sub('https?:\/\/\\S+|www\\.\\S+', '', x))\ntest_df['clean_text'] = test_df.text.apply(lambda x: re.sub('https?:\/\/\\S+|www\\.\\S+', '', x))","32c58a19":"train_df['clean_text'] = train_df.clean_text.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\ntest_df['clean_text'] = test_df.clean_text.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))","61a6513b":"non_alpha = string.punctuation + '0123456789'","a40b0508":"train_df['clean_text'] = train_df.clean_text.apply(lambda x: x.translate(str.maketrans('','',non_alpha)))\ntest_df['clean_text'] = test_df.clean_text.apply(lambda x: x.translate(str.maketrans('','',non_alpha)))","062af27c":"train_df['token_text'] = train_df.clean_text.str.lower()\ntrain_df['token_text'] = train_df.token_text.apply(lambda x: nltk.word_tokenize(x))\ntest_df['token_text'] = test_df.clean_text.str.lower()\ntest_df['token_text'] = test_df.token_text.apply(lambda x: nltk.word_tokenize(x))","3c5206b9":"stopwords = nltk.corpus.stopwords.words(\"english\")\naddingStopWords = ['im','get','dont','got','amp']\nstopwords.extend(addingStopWords)\ntrain_df['token_text'] = train_df['token_text'].apply(lambda x: [word for word in x if word not in stopwords])\ntest_df['token_text'] = test_df['token_text'].apply(lambda x: [word for word in x if word not in stopwords])","4c68d350":"# since we have a dataset containing text from social media, there might be many spelling mistakes and words which cannot be found in the word lemmatizer corpus, so they would be remained untouched.\n# To this end we would use the Porter Stemming which just removes affixes of the words\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ntrain_df['token_text'] = train_df['token_text'].apply(lambda x: [ps.stem(word) for word in x ])\ntest_df['token_text'] = test_df['token_text'].apply(lambda x: [ps.stem(word) for word in x ])","a0afe3d3":"train_df.reset_index(inplace=True, drop=True)","7962db0f":"from sklearn.feature_extraction.text import TfidfVectorizer","f5f72937":"def dummy(doc):\n    return doc\ntfidf_vect = TfidfVectorizer(analyzer=dummy)","69fdfe88":"tfidf_fit = tfidf_vect.fit(train_df['token_text'])","93c0d215":"matrix_train = tfidf_fit.transform(train_df['token_text'])\nmatrix_test = tfidf_fit.transform(test_df['token_text'])","629b228d":"print(matrix_train.shape)\nprint(matrix_test.shape)","b988eb10":"counts_df_train = pd.DataFrame(matrix_train.toarray())\ncounts_df_test = pd.DataFrame(matrix_test.toarray())","397dce83":"train_df['length'] = train_df.text.apply(lambda x: len(x) - x.count(' '))\ntest_df['length'] = test_df.text.apply(lambda x: len(x) - x.count(' '))","ae49f280":"train_df['punct_perc'] = train_df.text.apply(lambda x: sum([1 for char in x if char in non_alpha])\/(len(x) - x.count(' '))*100)\ntest_df['punct_perc'] = test_df.text.apply(lambda x: sum([1 for char in x if char in non_alpha])\/(len(x) - x.count(' '))*100)","23a93051":"train_df['word_count'] = train_df.token_text.apply(len)\ntest_df['word_count'] = train_df.token_text.apply(len)","29dca2bf":"fig, axes = plt.subplots(1,2,figsize=(17,5))\nsns.histplot(data = train_df, \n             x= 'length',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0])\nsns.boxplot(data = train_df, x = 'target', y = 'length',ax=axes[1]);","d541e9b0":"train_df['length_int'] =pd.cut(train_df.length, 14, include_lowest=True)\n                               #bins=[0, 15, 30, 40, 50,60, 80, 100, 120, 140, 180]\ntest_df['length_int'] =pd.cut(test_df.length, 14, include_lowest=True)\n                              #bins=[0, 15, 30, 40, 50,60, 80, 100, 120, 140, 180]","a7e38c82":"fig = plt.figure(figsize=(7,5))\ntrain_df[train_df['target']==0]['length_int'].value_counts(sort=False).plot(kind='bar', alpha = 0.5, color='blue', label = 'No')\ntrain_df[train_df['target']==1]['length_int'].value_counts(sort=False).plot(kind='bar', alpha = 0.5, color='orange', label = 'Yes')\nplt.legend(title='Actual Disaster', fontsize=11, title_fontsize=12);\n                             ","fb4aed0a":"fig, axes = plt.subplots(1,2,figsize=(17,5))\nsns.histplot(data = train_df, \n             x= 'punct_perc',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0])\nsns.boxplot(data = train_df, x = 'target', y = 'punct_perc',ax=axes[1]);","1e8ccee7":"fig, axes = plt.subplots(1,2,figsize=(17,5))\nsns.histplot(data = train_df, \n             x= 'word_count',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0])\nsns.boxplot(data = train_df, x = 'target', y = 'word_count',ax=axes[1]);","e0698d98":"## data transformation\nplt.hist(train_df[train_df.target==1]['length']**2.3, bins = 40, color = 'blue', alpha=0.5)\nplt.hist(train_df[train_df.target==0]['length']**2.3, bins = 40, color = 'red', alpha=0.5);\n## by transforming the distribution into a bimodal we can notice that the data are more separated","87388bf9":"train_df['length'] = train_df['length']**2.3\ntest_df['length'] = train_df['length']**2.3","e3f9bf23":"plt.hist(train_df['punct_perc']**(1\/3), bins = 40);","6c4506e9":"train_df['punct_perc'] = train_df['punct_perc']**(1\/3)\ntest_df['punct_perc'] = train_df['punct_perc']**(1\/3)","f9a7248f":"!pip install text2emotion","f46baf5f":"import text2emotion as te","f63c8a72":"# assign an emotion to each tweet\ntrain_df['emotion'] = train_df.text.apply(lambda x: te.get_emotion(x))\ntest_df['emotion'] = test_df.text.apply(lambda x: te.get_emotion(x))","f4d5b762":"# exploding the dictionary into 4 different columns, based on the dictionary keys\ntrain_df = pd.concat([train_df, pd.DataFrame(train_df['emotion'].tolist())], axis =1)\ntest_df = pd.concat([test_df, pd.DataFrame(test_df['emotion'].tolist())], axis =1)","69e7e0af":"total_emotions = train_df[['Happy', 'Angry', 'Surprise', 'Sad', 'Fear','target']].groupby('target').sum()","9138ea4f":"mean_emotions = train_df[['Happy', 'Angry', 'Surprise', 'Sad', 'Fear','target']].groupby('target').mean()","61699039":"fig, axes = plt.subplots(1,2,figsize=(14,5))\nax1 = total_emotions.plot(kind='bar', ax = axes[0])\nax1.set_title('Total values of emotion scores per target class')\nax2 = mean_emotions.plot(kind='bar', ax = axes[1])\nax2.set_title('Mean Scores of emotions per target class');","d79051d7":"fig, axes = plt.subplots(2,2,figsize=(20,10))\nsns.histplot(data = train_df, \n             x= 'Happy',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0,0])\nsns.boxplot(data = train_df, x = 'target', y = 'Happy',ax=axes[0,1])\nsns.histplot(data = train_df, \n             x= 'Fear',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[1,0])\nsns.boxplot(data = train_df, x = 'target', y = 'Fear',ax=axes[1,1])","a74ea74d":"# From the graphs and tables above we notice that, in cases of actual disasters the tweets have greater fear score\n# while for the non-disaster the mean score of 'Happy' is higher.\n# however those variables appear not to have a great importance. They might be dropped from the model.","d41cb4a6":"!pip install twython\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()","beb74b7a":"train_df['sentiment'] = train_df.text.astype(str).apply(lambda x: sia.polarity_scores(x))\ntest_df['sentiment'] = test_df.text.astype(str).apply(lambda x: sia.polarity_scores(x))","6a5a8440":"train_df = pd.concat([train_df, pd.DataFrame(train_df['sentiment'].tolist())], axis =1)\ntest_df = pd.concat([test_df, pd.DataFrame(test_df['sentiment'].tolist())], axis =1)","1340787b":"mean_sentiment = train_df[['neg', 'neu', 'pos', 'compound','target']].groupby('target').mean()\ntotal_sentiment = train_df[['neg', 'neu', 'pos', 'compound','target']].groupby('target').sum()","99603fab":"fig, axes = plt.subplots(1,2,figsize=(14,5))\nax1 = total_sentiment.plot(kind='bar', ax = axes[0])\nax1.set_title('Total values of emotion scores per target class')\nax2 = mean_sentiment.plot(kind='bar', ax = axes[1])\nax2.set_title('Mean Scores of emotions per target class');","63d6344b":"fig, axes = plt.subplots(1,2,figsize=(17,5))\nsns.histplot(data = train_df, \n             x= 'compound',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0])\nsns.boxplot(data = train_df, x = 'target', y = 'compound',ax=axes[1]);","a7a4e460":"fig, axes = plt.subplots(2,2,figsize=(20,10))\nsns.histplot(data = train_df, \n             x= 'neg',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[0,0])\nsns.boxplot(data = train_df, x = 'target', y = 'neg',ax=axes[0,1])\nsns.histplot(data = train_df, \n             x= 'pos',  \n             hue = 'target',\n             element='step',\n             stat='probability',\n            bins=40,\n            ax=axes[1,0])\nsns.boxplot(data = train_df, x = 'target', y = 'pos',ax=axes[1,1])","86984a51":"#full_train_df = pd.concat([train_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int'], axis=1), counts_df_train], axis=1)\nfull_train_df = pd.concat([train_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int','Happy', 'Angry', 'Surprise', 'Sad', 'Fear'], axis=1), counts_df_train], axis=1)","d0f4c81b":"#full_test_df = pd.concat([test_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int'], axis=1), counts_df_test], axis=1)\nfull_test_df = pd.concat([test_df.drop(['location','keyword','text','clean_text','token_text','sentiment','neg','neu','pos','word_count','length_int','Happy', 'Angry', 'Surprise', 'Sad', 'Fear'], axis=1), counts_df_test], axis=1)","f293d146":"# deleting unnecessary dataframes to save memory\ndel train_df\ndel test_df\ndel counts_df_train\ndel counts_df_test","ce01ca25":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","594f93fc":"forest =  RandomForestClassifier()\nparam = {'n_estimators':[200, 500],\n        'max_depth':[200, 300]}","c187380c":"gs = GridSearchCV(forest, param, cv=3)","94885c8c":"gs_fit = gs.fit(full_train_df.drop(['target','id'], axis=1), full_train_df['target'])","1f40c091":"pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)","e5d4047b":"rf =  RandomForestClassifier(n_estimators=200, max_depth=200)","ec0e86c3":"rf.fit(full_train_df.drop(['target','id'], axis=1), full_train_df['target'])","2a2a64a6":"predictions_rf = rf.predict(full_test_df.drop(['id'], axis=1))","b84966bf":"output = pd.DataFrame({'id': full_test_df.id, 'target': predictions_rf})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","b20444d0":"From the above we notice the following:\n1. According to the sample tweets we read there is a significant difference between non-disaster and disaster tweets\n2. According to the word clouds, non-disaster tweets involve words such us \"love\", while disaster have natural disaster events (storm, flood) and other words such us \"suicide\", \"bomber\", \"killed\".\n3. There are common words such us, \"death\" but with different fequencies.\n\nWe have to perfom additional cleaning, and stemming\/lemmatization to have a cleaner picture","f5b5329c":"#### VADER","89ebb4b1":"The graph above gives as a cleaner picture. We could argue that the smaller the tweet, fewer the possibilities to refer to an actual disaster.\nHowever longer the tweet, the higher the entropy.","5cd9b42e":"#### Locations","4d9402b1":"## EDA","4ec7daa8":"## Sentiment Analysis","10107a59":"We notice that the variance of punct_perc for actual disaster tweets is much less","9925b17c":"#### Target variable","39ff9711":"Lets see, if there is any possible connection between tweets length and percentage of punctuations and the target variable","90ec06cd":"## Data Cleaning","9d59c3e6":"From the above plots we can notice the following:\n* The top keywords have similar counts\n* The majority of the disasters have taken place in U.S.A.\n* There needs to be done cleaning in both these variables since, there are variable names which can group together - (USA, United States), (weapon, weapons). ","5aa391a2":"We can notice that we have more non-actual disaster tweets than actual ones.\nFurthermore, we could say that we do not have an unbalanced data set.","2d14b3aa":"#### Abbreviations","d903ab1f":"#### Location and target","e83f20c6":"By checking some of the above tweets we notice that some are labeled wrong. However for our convinience we will just drop them all.","22ba0917":"### Feature evaluation\n#### length","8808a271":"In order to explore this relationship we have to clean this location variable, by assigning together similar locations","f3a0bd92":"We could say that it appears to be a very small proprtion of actual disaster tweets with length between 0-40, while for 80-100 the gap is more narrow.\nHowever, both distributions are similar so no significant relatioship between length and the target variable can be dirived. Also actual disaster tweets are in avarage longer, however show a smaller variance, since the IQR is between 90-135, while for non-disaster ones from 70-130.\n\u200b\nLets use the cut function to bin values into discrete intervals, with this way we might spot some relationships.","1062bfbd":"#### Remove emojis","dada48ae":"#### Stemming","85ea63e8":"#### Remove Punctuation and numbers","360ee190":"Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-bert-keras?scriptVersionId=31186559 Some data is wrong. Some tweets in the training dataset are labeled wrong. 328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226 are given as 1 whereas they are obviously 0, since they are not related to disaster.\n\nWe change them to 0","be23c0b2":"### Missing values ","dcbbf5c4":"We can notice the following:\n1. Many non-disaster tweets have 0 negative sentiment\n2. Many disaster tweets have 0 positive sentiment","dd6cd8ec":"### Features Relationships","8b4b1f9d":"## Vectorising\n### Tfidf","62f1c227":"#### Keyword and target","5ecc464d":"Converting all the abbreviations to their full form. \nSpecial thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-bert-keras?scriptVersionId=31186559","f618fadf":"The distributions are pretty similar, however actual disaster tweets tend to have more words.\nWe are not going to use this variable.","4b39a7b6":"#### Punctuation percentage","83138d53":"#### Text2Emotion","836c9c17":"We notice that some of the duplicated text, has different target values. Meaning that some are labeled wrongly.","a6fd56e8":"### Feature Engineering","e24e2dce":"We are going to use the text2emotion package which gives an emotion score to each tweet<br>\n*see https:\/\/pypi.org\/project\/text2emotion\/*","92fde09d":"#### Tokenizing","84536327":"#### Keywords","3256e6d3":"#### Removing stopwords","abb64b88":"#### Remove links","ff8d1ab1":"## RandomForestClassifier CV","77bddcc2":"### Exploring the variables","31c999e6":"#### Word count"}}