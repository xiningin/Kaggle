{"cell_type":{"71b5a7da":"code","728dac99":"code","e35d80c3":"code","9b1a2bf6":"code","79f48a2d":"code","20afc468":"code","880c29f8":"code","bc36031c":"code","c660ddc9":"code","6787768d":"code","f0e04310":"code","29a3b1fe":"code","63e302f6":"code","c70dcfc7":"code","5084dce9":"code","0107dde9":"code","bf3e04bd":"code","3b3a57e7":"code","0759dc56":"code","2416b158":"code","a4548ade":"code","68d86731":"code","1b091a46":"code","b8e2d9d3":"code","5ce96608":"code","f60d8de1":"code","223d41d0":"code","be9fe527":"code","d59a696c":"code","a65e6e6a":"code","a3bbcfc9":"code","44e48081":"code","678bee05":"code","89e26d4d":"code","e1bbcaa8":"code","b91a574d":"code","8ad1cc07":"code","697eee7c":"code","a3fd3f43":"code","4070c16a":"code","005faba2":"code","250e4d55":"code","0ceaa5b0":"code","9727a722":"code","fda324cc":"code","542410be":"code","dcaf55d1":"code","acdafd5f":"code","0ab164a8":"markdown","c2674497":"markdown","3d2b062c":"markdown","44d8e858":"markdown","0c526c79":"markdown","0a6a744a":"markdown","c24234c3":"markdown","bd664bf1":"markdown","6947b8c8":"markdown","c78764e0":"markdown","3a15d2ac":"markdown","e0768a1a":"markdown","380d4a44":"markdown","fdffc0e0":"markdown","df3b6b54":"markdown","071db52c":"markdown","4436beaf":"markdown","6b146e7e":"markdown","43efec53":"markdown","4fd98503":"markdown","89ad8d1a":"markdown","b9455500":"markdown","28454afc":"markdown","78975e64":"markdown","59ab4e12":"markdown","db94693f":"markdown","d52a1ba4":"markdown"},"source":{"71b5a7da":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom plotly.offline import iplot\nimport plotly as py\nimport plotly.tools as tls\nimport cufflinks as cf\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","728dac99":"pd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',None)","e35d80c3":"cm = sns.light_palette(\"green\", as_cmap=True)","9b1a2bf6":"py.offline.init_notebook_mode(connected = True)\ncf.go_offline()\ncf.set_config_file(theme='solar')\nplt.style.use('ggplot')","79f48a2d":"df = pd.read_csv('..\/input\/forest-fire-prediction\/forestfires.csv')\ndf.head().style.set_properties(**{'background-color': 'black',\n                           'color': 'lawngreen',\n                           'border-color': 'white'})\n# .style.background_gradient(cmap='Reds')\n\n","20afc468":"df.info()","880c29f8":"numerical_features = [features for features in df.columns if df[features].dtypes != 'O']\nprint('Number of Numerical variables are: ', len(numerical_features))\nprint('Numerical features are: ', numerical_features)\ndf[numerical_features].head().style.background_gradient(cmap=cm)","bc36031c":"discrete_feature = [features for features in numerical_features if len(df[features].unique())< 20]\nprint(f\"length of discrete numerical variables are: {len(discrete_feature)}\")\nprint(f\"And the discreate features are: {discrete_feature}\")\n# lets see the head of the data frame consists of discrete numerical values\ndf[discrete_feature].head().style.background_gradient(cm).highlight_null('green')","c660ddc9":"df['X'].value_counts()","6787768d":"df['Y'].value_counts()","f0e04310":"df['rain'].value_counts()","29a3b1fe":"# lets see the different values in each discreate variables\nprint(df['X'].value_counts())\nprint('\\n')\nprint(df['Y'].value_counts())\nprint('\\n')\nprint(df['rain'].value_counts())","63e302f6":"#  lets search for year feature\nyear_feature = [features for features in numerical_features if 'Yr' in features or 'Year' in features or 'yr' in features or 'year' in features]\nprint(f\"year features are : {year_feature}\")","c70dcfc7":"continuous_feature=[features for features in numerical_features if features not in discrete_feature]\nprint(f\"Continuous feature Count {len(continuous_feature)}\")\nprint(f\"Continuous feature are: {continuous_feature}\")\n\n# lets see the head\ndf[continuous_feature].head().style.background_gradient(cmap=cm)","5084dce9":"categorical_features = [features for features in df.columns if df[features].dtypes =='O']\nprint(f\"Now categorical variables are: {categorical_features}\")\nprint(f\"number of categorical variables are: {categorical_features}\")\n\n# see the head\n# CANT COLOR A CATEGORICAL VARIABLE\ndf[categorical_features].head()","0107dde9":"df['month'].describe()","bf3e04bd":"df['day'].describe()","3b3a57e7":"# lets see the different values in each categorical variables\nprint(df['month'].value_counts())\nprint('\\n')\nprint(df['day'].value_counts())\nprint('\\n')","0759dc56":"df.describe().style.background_gradient(cmap='Reds')","2416b158":"df['area'].iplot(kind = 'scatter' , mode = 'markers',title=\"Scatter plot of area\",\n                            yTitle='area',xTitle = 'id')","a4548ade":"df['area'].iplot(title=\"Line plot of area\",\n                            yTitle='area',xTitle = 'id')","68d86731":"import plotly.figure_factory as ff\nimport numpy as np\nnp.random.seed(1)\n\n\nx = np.array(df['area'])\nhist_data = [x]\ngroup_labels = ['area'] \n\nfig = ff.create_distplot(hist_data, group_labels)\nfig.show()","1b091a46":"pd.DataFrame(df[\"area\"]).iplot(kind=\"histogram\", \n                bins=40, \n                theme=\"solar\",\n                title=\"Histogram of area\",\n                xTitle='area', \n                yTitle='Count',\n                asFigure=True)","b8e2d9d3":"import plotly.express as px\n\n# df = px.data.tips()\nfig = px.violin(df, y=\"area\", box=True, # draw box plot inside the violin\n                points='all', # can be 'outliers', or False\n               )\nfig.show()","5ce96608":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\nfor i in categorical_features:\n    df[i]=label_encoder.fit_transform(df[i])","f60d8de1":"all_fe = ['X','Y','month','day','FFMC','DMC','DC','ISI','temp','RH','wind','rain','area']","223d41d0":"corr_new_train=df.corr()\nplt.figure(figsize=(10,20))\nsns.heatmap(corr_new_train[['area']].sort_values(by=['area'],ascending=False).head(60),vmin=-1, cmap='seismic', annot=True)\nplt.ylabel('features')\nplt.xlabel('Target')\nplt.title(\"Corelation of different fitures with target\")\nplt.show()","be9fe527":"fs1 = ['X','Y','month','FFMC','DMC','DC','temp','area'] ","d59a696c":"df_fs1 = df[fs1]\ndf_fs1.head()","a65e6e6a":"SEED = 42\n\ndata = df_fs1.copy()\ny = data['area']\nx = data.drop(['area'],axis=1)\n\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_val,y_train,y_val = train_test_split(x,y,test_size = 0.2,random_state = SEED)","a3bbcfc9":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n# from sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.ensemble import RandomForestRegressor","44e48081":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","678bee05":"reg_rf_rscv = RandomForestRegressor()","89e26d4d":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nrandom_search_rf = RandomizedSearchCV(reg_rf_rscv, random_grid,n_iter=5, n_jobs=1, cv=5,verbose=2)","e1bbcaa8":"random_search_rf.fit(x_train,y_train)","b91a574d":"random_search_rf.best_params_","8ad1cc07":"base_model = RandomForestRegressor(n_estimators= 1200,\n                                     min_samples_split= 10,\n                                     min_samples_leaf= 2,\n                                     max_features= 'auto',\n                                     max_depth= 20,\n                                     bootstrap= True,\n                                    random_state = SEED)\nbase_model.fit(x_train, y_train)\n# base_accuracy = evaluate(base_model, x_val,y_val)","697eee7c":"y_pred_rf_rscv = base_model.predict(x_val)","a3fd3f43":"def MSE(model_preds, ground_truths):\n  return mean_squared_error(model_preds, ground_truths)\n\ndef MAE(model_preds, ground_truths):\n  return mean_absolute_error(model_preds, ground_truths)\n\ndef Other_Err(model_preds, ground_truths):\n  return r2_score( ground_truths,model_preds)\n\ndef RMSE(model_preds, ground_truths):\n  return np.sqrt(mean_squared_error(model_preds, ground_truths))","4070c16a":"print(f\"mean squared error: {MSE(y_pred_rf_rscv,y_val)}\")\nprint(f\"mean absolute error: {MAE(y_pred_rf_rscv,y_val)}\")\nprint(f\"r2 error: {Other_Err(y_pred_rf_rscv,y_val)}\")\nprint(f\"root mean squared error: {RMSE(y_pred_rf_rscv,y_val)}\")","005faba2":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","250e4d55":"score = cross_val_score(reg_rf_rscv, x_train, y_train, cv=k_fold, n_jobs=1, scoring='r2')\nprint(score)","0ceaa5b0":"print('train r2 %2f' %(1 * score.mean()))","9727a722":"score_val = cross_val_score(reg_rf_rscv, x_val, y_val, cv=k_fold, n_jobs=1, scoring='r2')\nprint(score)","fda324cc":"print('train r2 %2f' %(1 * score_val.mean()))","542410be":"pd.DataFrame(score).iplot(title=\"R2 score of diferent CV for training data\",xTitle = \"count\",yTitle=\"R2 Score\")","dcaf55d1":"pd.DataFrame(score_val).iplot(title=\"R2 score of diferent CV for validation data\",xTitle = \"count\",yTitle=\"R2 Score\")","acdafd5f":"import pickle\nfilename = 'finalized_model.pkl'\npickle.dump(reg_rf_rscv,open(filename,'wb'))","0ab164a8":"## Line plot of Area:","c2674497":"## Histogram of Area:","3d2b062c":"### Lable Encoding:\nUsing label encoding to encode the categorical variables.","44d8e858":"### Few observations:\n-  Now this is an interesting plot,why? cause in this 1 plot you can see most of the commonly use statistical terms has been ploted.\n- You can see the quantiles, max value, min value, median, kde every thing. So, Hoover over the plot and you will get to know what is the max value what is the median and how the kde is changing.","0c526c79":"# Metric Reports\n \n\n| Metrics \t| Values \t| \n|-\t|-\t|\n| MSE \t| 11311.77\t|\n| RMSE \t| 106.35 \t|\n| MAE \t| 25.40 \t|\n| R2 Score \t| 0.04 \t|\n\n","0a6a744a":"## what are the Numerical features?","c24234c3":"# Analysis on the data","bd664bf1":"### Model using selected fetures:","6947b8c8":"## Lets bring some color and draw some graphs\n\n|Type of variable|Column name|\n|--|--|\n|Numerical Variables|'X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'area'|\n|Year variable\/features|No year variables|\n|Discrete Variables|'X', 'Y', 'rain'|\n|Continuous Variables|'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'area'|\n|Categorical Variables|'month', 'day'|\n","c78764e0":"# Utils:","3a15d2ac":"### **Few observations:**\n- This shows the differnt values of the target variable in the form of dots\/bubbles Hoover over and what are the values of each points.\n- It shows that there are some extream values in the area column and most of the values are near to zero. Which really make sense, because we can see forsts getting burned-out but most of the cases fire cant spread enough. And from the statistical table we can also varify that the Huge fire spread outs are really few in number as the mean of area column is 12.84.\n- And the interesting thing is the max(top 3) values  of area dont really have any pattern.Like- we could say if FFDM is around this much then there is a high probablity of having huge burn of area. But this kind of pattern makes no sence in this data for any value of area.","e0768a1a":"### Few observations:\n- Now this is the density aka PDF plot for area you might need to zoom to see the ditails.\n- This plot show that the mean value for area is near to zero(but not zero) and there are fewer huge burned areas.","380d4a44":"![dsc-logo](https:\/\/raw.githubusercontent.com\/divyake\/Cysec-Hacktoberfest\/dcc84465cfcff73981f8fcb5c8fe3b1710c007e1\/assets\/logo.svg)\n\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/d\/d8\/Deerfire_high_res_edit.jpg' width='1200px' style=\"vertical-align:middle\"\/>","fdffc0e0":"### Cross Validation CV:","df3b6b54":"## Violin Plot of Area:","071db52c":"### Lets check Corelation of different features with target:","4436beaf":"**Hello** reader, In this notebook I have covered the model building of Forest Fire Prediction and the Uni-variate analysis of the target variable. I have plotted different charts and graphs inorder to describe how the target variable is changing and what are the properties of that.\n\n<br>\n\n### Table of Content:\n1. Analysis on the data\n      * Basic info about data\n      * what are the Numerical features?\n      * what are the Continuous Numerical feafeatures?\n      * what are the Discrete Numerical feafeatures?\n      * what are the Categorical Features?And quantitative analysis on those features.\n<br>\n2. Statistical insights of all the features\n<br>\n3. Uni-variate analysis of Area\/Target variable\n      * Scatter Plot\n      * Line Plot\n      * PDF (Probability density function) Plot\n      * CDF (cumulative distribution function) Plot\n      * Histogram Plot\n      * Violin Plot\n<br>\n4. Data Preprocessing\n      * Encoding of data\n      * Feature selection\n      * Train test split\n<br>     \n5. Model building\n      * Model Building using Selected features\n      * Cross Validation\n      * Metric Reports\n      \n","6b146e7e":"## Scatter Plot of Area:","43efec53":"## Density(PDF) Plot of Area:","4fd98503":"## Tran Test(val) Split:","89ad8d1a":"# Saved Model:\n\nmodel link - https:\/\/drive.google.com\/file\/d\/1isIoiZRKjQLzTdb2YiGa-nAnMUGvMmh6\/view?usp=sharing","b9455500":"## Uni-variate analysis of Area:\n\n\n\n\n\n","28454afc":"I have tried normal Random forest, Random forest with no Randomized Search CV,xgboost and stacking of multiple ml models and results are something like this.\n\n|reg      |\trmse\t|\tmse\t| r2\t    |\n|-|-|-|-|  \n|simple_rf =|109.5595|12003.2882|-0.0182|\n|rscv_rf = |108.3183|11732.8603|0.00465|\n|rscv_xgboost = |108.7711|11831.1615|-0.00368|\n|stacking = |109.0897|11900.57185|-0.0095|\n|rf with rscv and feature selection =|106.35|11311.77|0.04|\n\n<br>\nPS: rscv is Randomized Search CV and rf is random forest.\n\nSo, the improvements in **rf with rscv and feature selection** major thats why I am keeping it.\n\n\n","78975e64":"### Taking top 6 features for model building.","59ab4e12":"## Model Building using Selected features:","db94693f":"### Few observations:\n- Histograms are often called as Distibution plot, as this show the distribution of a perticular feature. Now by distribution I mean how many points of a feature lies in a perticular range.\n- If you observe the plot you will understand that 465 points whaich are having values from -25(actually 0,but mentioned in the plot) to 24.9, then there are 37 points which ranges from 25 to 74.9 and so on.\n- This thing again shows that there are few burn outs which covers less area. \n- Just think, the area is in hector so can you imagine how huge 1090.84 hector would be.","d52a1ba4":"### Few observations:\n- In this plot how the area is changing row wise.\n- Its a normal line plot to make you understand how the area is changing(Scatter plot could be enough but some time line plot make more sense to people and some times scatter plot, both shows kind of same things but different way).\n- This plot also shows that the target is continuous."}}