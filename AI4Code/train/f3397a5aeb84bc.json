{"cell_type":{"396e5c1c":"code","ea3acf41":"code","bee9e165":"code","7b5a0add":"code","86079348":"code","b1e07eec":"code","69308bf6":"code","4d359835":"code","355490bf":"code","4027aafb":"code","e7e3b3e7":"code","355c7166":"code","aaa80ebb":"code","7447673b":"code","58fd8e45":"code","d63a773d":"code","76ed4663":"code","c8298944":"code","63a79e49":"code","3f9f9e7f":"code","79c0640a":"code","f08cf642":"code","71ffa5a3":"code","e991a2bb":"code","344ce637":"code","d521c981":"code","3a349669":"code","f4a17258":"code","2038e9d9":"code","693a2661":"code","568a532b":"code","f9e10efd":"code","3a00d8a5":"code","0961b24f":"code","36885864":"code","4839f172":"code","57c63c87":"code","fc5fdf9e":"code","d5fb4c50":"code","300624ba":"code","52aeec63":"code","5fc32068":"code","54f4edfd":"code","18398c9a":"code","34925adb":"code","5101e132":"code","f71a9c06":"markdown","a82d732c":"markdown","f3de4856":"markdown","39e7d0f7":"markdown","d9df8faf":"markdown","1d96b674":"markdown","5d5cc482":"markdown","05cf425b":"markdown","d1c7941d":"markdown","965d24a9":"markdown","f5342233":"markdown","f4a3b07e":"markdown","c1440d45":"markdown","9c10c9b3":"markdown","5891da43":"markdown","7395e71d":"markdown","1a31e853":"markdown","3d7c3df9":"markdown","5b35ad01":"markdown","06f3638c":"markdown","58ea2417":"markdown","70157394":"markdown","2c9302d3":"markdown","18f3bacb":"markdown","c631b3e5":"markdown","237469ee":"markdown","28d3d7c8":"markdown","e10fb2de":"markdown","221a2f66":"markdown","71cdc5a3":"markdown","a9a623ca":"markdown","2f539fc4":"markdown","566aa1a5":"markdown","4ded8102":"markdown","d50d5e20":"markdown","c7c1d1fc":"markdown","69c901c0":"markdown","48ed7510":"markdown","5f070461":"markdown","5ddb0482":"markdown","099916ce":"markdown","59471752":"markdown","91901ea4":"markdown","ebf4bc18":"markdown","1fd03bc6":"markdown","c25b8b65":"markdown","e5987eb9":"markdown","b1243fd2":"markdown","26168493":"markdown","d5f29782":"markdown","2cf8a713":"markdown","fbfb44ae":"markdown","213ede4e":"markdown","3e43bb9d":"markdown","faf76cbd":"markdown","274f5c73":"markdown","fc1264e8":"markdown","e02b7cc0":"markdown","50b8036f":"markdown","874525d1":"markdown"},"source":{"396e5c1c":"# import necessary package\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","ea3acf41":"# load data\nseattle_calendar = pd.read_csv('..\/input\/calendar.csv')\nseattle_listing = pd.read_csv('..\/input\/listings.csv')\nseattle_review = pd.read_csv('..\/input\/reviews.csv')","bee9e165":"seattle_calendar.head()","7b5a0add":"seattle_calendar.info()","86079348":"#  If the available values are f, the price values seems to be NaN. But it is only a hypothesis, is it true all data?\ncalendar_q1_df = seattle_calendar.groupby('available')['price'].count().reset_index()\ncalendar_q1_df.columns = ['available', 'price_nonnull_count']\ncalendar_q1_df","b1e07eec":"#  How many rows per each listing_id?\ncalendar_q2_df = seattle_calendar.groupby('listing_id')['date'].count().reset_index()\ncalendar_q2_df['date'].value_counts()","69308bf6":"# process data\ncalendar_q3_df = seattle_calendar.copy(deep=True)\ncalendar_q3_df.dropna(inplace=True)\ncalendar_q3_df['date'] = pd.to_datetime(calendar_q3_df['date'])\ncalendar_q3_df['price'] = calendar_q3_df['price'].map(lambda x: float(x[1:].replace(\",\", \"\")))\n\n# apply aggregation\ncalendar_q3_df = calendar_q3_df.groupby('date')['price'].mean().reset_index()\n\n# plot avg listings prices over time.\nplt.figure(figsize=(15, 8))\nplt.plot(calendar_q3_df.date, calendar_q3_df.price, color='b', marker='.', linewidth=0.9)\nplt.title(\"Average listing price by date\")\nplt.xlabel('date')\nplt.ylabel('average listing price')\nplt.grid()","4d359835":"# plot more narrow range\nplt.figure(figsize=(15, 8))\nplt.plot(calendar_q3_df.date.values[:15], calendar_q3_df.price.values[:15], color='b', marker='o', linewidth=1.5)\nplt.title(\"Average listing price by date\")\nplt.xlabel('date')\nplt.ylabel('average listing price')\nplt.grid()","355490bf":"# create weekday column\ncalendar_q3_df[\"weekday\"] = calendar_q3_df[\"date\"].dt.weekday_name\n\n# boxplot to see price distribution\nplt.figure(figsize=(15, 8))\nsns.boxplot(x = 'weekday',  y = 'price', data = calendar_q3_df, palette=\"Blues\", width=0.6)\nplt.show()","4027aafb":"seattle_listing.head()","e7e3b3e7":"print(list(seattle_listing.columns.values))","355c7166":"print(\"Num of listings: \", seattle_listing.id.count())\nprint(\"Num of rows: \", seattle_listing.shape[0])","aaa80ebb":"seattle_listing['review_scores_rating'].describe().reset_index()","7447673b":"# cleaning data\nlistings_q1_df = seattle_listing['review_scores_rating'].dropna()\n\n# plot histgram\nplt.figure(figsize=(15, 8))\nplt.hist(listings_q1_df.values, bins=80, color='b')\nplt.grid()","58fd8e45":"# cleaning data\nlistings_q2_df = seattle_listing.copy(deep=True)\nlistings_q2_df = listings_q2_df['price'].dropna().reset_index()\nlistings_q2_df['price'] = listings_q2_df['price'].map(lambda x: float(x[1:].replace(',', '')))\n\nlistings_q2_df['price'].describe().reset_index()","d63a773d":"plt.figure(figsize=(15, 8))\nplt.hist(listings_q2_df.price, bins=100, color='b')\nplt.grid()","76ed4663":"seattle_listing['maximum_nights'].describe().reset_index()","c8298944":"# eliminate outliers because maximum values are very large.\nlistings_q3_df = seattle_listing[seattle_listing['maximum_nights'] <= 1500]\n\nplt.figure(figsize=(15, 8))\nplt.hist(listings_q3_df.maximum_nights, bins=100, color='b')\nplt.xlabel('maximum nights')\nplt.ylabel('listings count')\nplt.grid()","63a79e49":"seattle_review.head()","3f9f9e7f":"seattle_review.info()","79c0640a":"print(\"sample 1: \", seattle_review.comments.values[0], \"\\n\")\nprint(\"sample 2: \", seattle_review.comments.values[3])","f08cf642":"# convert date column's data type to date from object\nreview_q1_df = seattle_review.copy(deep=True)\nreview_q1_df.date = pd.to_datetime(review_q1_df.date)\n\nreview_q1_df = review_q1_df.groupby('date')['id'].count().reset_index()\n\n# plot avg listings prices over time.\nplt.figure(figsize=(15, 8))\nplt.plot(review_q1_df.date, review_q1_df.id, color='b', linewidth=0.9)\nplt.title(\"Number of reviews by date\")\nplt.xlabel('date')\nplt.ylabel('number of reviews')\nplt.grid()","71ffa5a3":"# create rolling mean column\nreview_q1_df[\"rolling_mean_30\"] = review_q1_df.id.rolling(window=30).mean()\n\n# plot avg listings prices over time.\nplt.figure(figsize=(15, 8))\nplt.plot(review_q1_df.date, review_q1_df.rolling_mean_30, color='b', linewidth=2.0)\nplt.title(\"Number of reviews by date\")\nplt.xlabel('date')\nplt.ylabel('number of reviews')\nplt.grid()","e991a2bb":"review_q1_df[\"year\"] = review_q1_df.date.dt.year\nyears = review_q1_df.year.unique()\n\nfor year in years:\n    if year >= 2010 and year < 2016:\n        year_df = review_q1_df[review_q1_df.year == year]\n        max_value = year_df.rolling_mean_30.max()\n        max_date = year_df[year_df.rolling_mean_30 == max_value].date.dt.date.values[0]\n        print(year, max_date, np.round(max_value, 1))","344ce637":"listings_q3_df[\"min_max_night_diff\"] = listings_q3_df.maximum_nights - listings_q3_df.minimum_nights\n\nplt.figure(figsize=(15, 8))\nplt.plot(listings_q3_df.maximum_nights, listings_q3_df.minimum_nights, color='b', marker='o', linewidth=0, alpha=0.25)\nplt.xlabel('maximum nights')\nplt.ylabel('minimum nights')\nplt.grid()","d521c981":"review_q2_df = review_q1_df[review_q1_df.year == 2015]\n\nplt.figure(figsize=(15, 8))\nplt.plot(review_q2_df.date, review_q2_df.rolling_mean_30, color='b', linewidth=2.0)\nplt.title(\"Number of reviews by date\")\nplt.grid()","3a349669":"prepare_df = seattle_listing.copy(deep=True)","f4a17258":"# check null count\ndf_length = prepare_df.shape[0]\n\nfor col in prepare_df.columns:\n    null_count = prepare_df[col].isnull().sum()\n    if null_count == 0:\n        continue\n        \n    null_ratio = np.round(null_count\/df_length * 100, 2)\n    print(\"{} has {} null values ({}%)\".format(col, null_count, null_ratio))","2038e9d9":"# detect need drop columns\ndrop_cols = [col for col in prepare_df.columns if prepare_df[col].isnull().sum()\/df_length >= 0.3]\n\n# drop null\nprepare_df.drop(drop_cols, axis=1, inplace=True)\nprepare_df.dropna(subset=['host_since'], inplace=True)\n\n# check after\nfor col in prepare_df.columns:\n    null_count = prepare_df[col].isnull().sum()\n    if null_count == 0:\n        continue\n        \n    null_ratio = np.round(null_count\/df_length * 100, 2)\n    print(\"{} has {} null values ({}%)\".format(col, null_count, null_ratio))","693a2661":"drop_cols = ['listing_url', 'scrape_id', 'last_scraped', 'name', 'summary', 'space', 'description', 'neighborhood_overview',\n                'transit', 'medium_url', 'picture_url', 'xl_picture_url', 'host_id', 'host_url', 'host_name', 'host_about', 'host_thumbnail_url',\n                'host_picture_url', 'street', 'city', 'state', 'zipcode', 'market', 'smart_location', 'country_code', 'country', 'latitude', 'longitude',\n                'calendar_updated', 'calendar_last_scraped', 'first_review', 'last_review', 'amenities', 'host_verifications']\n\nprepare_df.drop(drop_cols, axis=1, inplace=True)","568a532b":"prepare_df.columns","f9e10efd":"drop_cols = []\nfor col in prepare_df.columns:\n    if prepare_df[col].nunique() == 1:\n        drop_cols.append(col)\n        \nprepare_df.drop(drop_cols, axis=1, inplace=True)\nprepare_df.columns","3a00d8a5":"# available days count each listings\nlisting_avalilable = seattle_calendar.groupby('listing_id')['price'].count().reset_index()\nlisting_avalilable.columns = [\"id\", \"available_count\"]\n\n# merge\nprepare_df = prepare_df.merge(listing_avalilable, how='left', on='id')\n\n# create target column\nprepare_df['host_since_year'] = pd.to_datetime(prepare_df['host_since']).dt.year\nprepare_df[\"easily_accomodated\"] = prepare_df.accommodates \/ (prepare_df.available_count+1) \/ (2017 - prepare_df.host_since_year)","0961b24f":"print(\"Before: {} columns\".format(prepare_df.shape[1]))\n\ndrop_cols = ['host_since', 'accommodates', 'availability_30', 'availability_60', 'availability_90', 'availability_365',\n                'number_of_reviews', 'review_scores_rating', 'available_count', 'reviews_per_month', 'host_since_year', 'review_scores_value']\n\nprepare_df.drop(drop_cols, axis=1, inplace=True)\nprint(\"After: {} columns\".format(prepare_df.shape[1]))","36885864":"# convert true or false value to 1 or 0\ndummy_cols = ['host_is_superhost', 'require_guest_phone_verification', 'require_guest_profile_picture', 'instant_bookable', \n              'host_has_profile_pic', 'host_identity_verified', 'is_location_exact']\n\nfor col in dummy_cols:\n    prepare_df[col] = prepare_df[col].map(lambda x: 1 if x == 't' else 0)\n\n# create dummy valuables\ndummy_cols = ['host_location', 'host_neighbourhood', 'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed',\n             'property_type', 'room_type', 'bed_type', 'cancellation_policy', 'host_response_time']\n\nprepare_df = pd.get_dummies(prepare_df, columns=dummy_cols, dummy_na=True)","4839f172":"df_length = prepare_df.shape[0]\n\nfor col in prepare_df.columns:\n    null_count = prepare_df[col].isnull().sum()\n    if null_count == 0:\n        continue\n        \n    null_ratio = np.round(null_count\/df_length * 100, 2)\n    print(\"{} has {} null values ({}%)\".format(col, null_count, null_ratio))","57c63c87":"prepare_df[\"is_thumbnail_setted\"] = 1 - prepare_df.thumbnail_url.isnull()\nprepare_df.drop('thumbnail_url', axis=1, inplace=True)\nprepare_df.host_response_rate = prepare_df.host_response_rate.fillna('0%').map(lambda x: float(x[:-1]))\nprepare_df.host_acceptance_rate = prepare_df.host_acceptance_rate.fillna('0%').map(lambda x: float(x[:-1]))\nprepare_df.bathrooms.fillna(0, inplace=True)\nprepare_df.bedrooms.fillna(0, inplace=True)\nprepare_df.beds.fillna(0, inplace=True)\nprepare_df.cleaning_fee.fillna('$0', inplace=True)\nprepare_df.review_scores_accuracy.fillna(0, inplace=True)\nprepare_df.review_scores_cleanliness.fillna(0, inplace=True)\nprepare_df.review_scores_checkin.fillna(0, inplace=True)\nprepare_df.review_scores_communication.fillna(0, inplace=True)\nprepare_df.review_scores_location.fillna(0, inplace=True)","fc5fdf9e":"for col in prepare_df.columns:\n    if prepare_df[col].dtypes == 'object':\n        print(col)","d5fb4c50":"prepare_df.price = prepare_df.price.map(lambda x: float(x[1:].replace(',', '')))\nprepare_df.cleaning_fee = prepare_df.cleaning_fee.map(lambda x: float(x[1:].replace(',', '')))\nprepare_df.extra_people = prepare_df.extra_people.map(lambda x: float(x[1:].replace(',', '')))","300624ba":"X = prepare_df.drop(['id', 'easily_accomodated'], axis=1)\ny = prepare_df.easily_accomodated.values\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=5)\nscores = cross_val_score(rf, X, y, cv=5)","52aeec63":"scores","5fc32068":"rf.fit(X, y)\npredictions = rf.predict(X)","54f4edfd":"plt.figure(figsize=(8, 8))\n\nplt.plot((0, 4), (0, 4), color='gray')\nplt.plot(y, predictions, linewidth=0, marker='o', alpha=0.5)\nplt.grid()\nplt.xlim((-0.2, 4.2))\nplt.ylim((-0.2, 4.2))\nplt.xlabel(\"Actual values\")\nplt.ylabel(\"Predicted values\")\nplt.show()","18398c9a":"X = prepare_df.drop(['id', 'easily_accomodated'], axis=1)\ny = np.log(prepare_df.easily_accomodated.values)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=5)\nscores = cross_val_score(rf, X, y, cv=5)\nprint(scores)","34925adb":"rf.fit(X, y)\npredictions = rf.predict(X)","5101e132":"plt.figure(figsize=(8, 8))\n\nplt.plot((-10, 10), (-10, 10), color='gray')\nplt.plot(y, predictions, linewidth=0, marker='o', alpha=0.5)\nplt.grid()\nplt.xlim((-8.2, 2))\nplt.ylim((-8.2, 2))\nplt.xlabel(\"Actual values\")\nplt.ylabel(\"Predicted values\")\nplt.show()","f71a9c06":"Let's begin with looking at first 5 row of the data and columns information.","a82d732c":"This is interesting.  \nThere are two trend of the data.\n\n1. The average price rise from 2016\/1 to 2016\/7, and reach peak for three months, and getting lower. And the average proce of 2017\/1 is higher than 1 years ago.\n2. There is periodic small peak.","f3de4856":"There are many columns, so I can't explore each columns here.  \nHere I'll look at some columns of my interest.","39e7d0f7":"Good. Now I almost finished remove columns that is not used for anallysis. So next, I create target valuable.","d9df8faf":"#### price","1d96b674":"The closer to the gray line, the more accurate the predicted value is.  \nThen, at first glance I can understand the predocted values are not accurate.  \nIn addition, it is also understood that the prediction is pulled to a small value near 0, and a small prediction is made for the larger value.","5d5cc482":"#### Summary\n\n* The listings data has 92 columns.\n* The `review_scores_rating` has right skewed distribution, and almost values are over 90 points.\n* The `price` has long tail distribution, almost values are around 100\\$ but some values are much higher than other values.  \n* The `maximum_nights` has very special distribution. Their are two segments, one is about 3 years, the other is around 1week.\n\nOK, let's look at last data.","05cf425b":"Next, I want to see the time series change of the number of comments.","d1c7941d":"Above, I can answer my question. The answer is\n\n***If the available values are f, the price values seems to be NaN. But it is only a hypothesis, is it true all data?***  \n-> true !!\n\n***How many rows per each listing_id?***  \n-> 365 days record. This is equal a year.","965d24a9":"It's hard to say clearly when to begin and when to end.  \nBut, from here it may be able to say the busy season is One month before and after from September.","f5342233":"The first trend can be split into two foctors. One is seasonal factor, and the other is overall factor.  \nThe second trend looks like a weekly trend, so let's close look at!!","f4a3b07e":"Good.  \nNext, we will delete the column that seems not to be a feature of ease of borrowing. Since we do not include natural language processing this time, we will also delete comment-based columns.","c1440d45":"#### maximum_nights","9c10c9b3":"### reviews","5891da43":"Now, I almost understood the features of the data.  \nFinally, I'll research is there any trend of the listings price.","7395e71d":"It looks like a weekly trend as I thought.  \nThen, which does weekday have high price? ","1a31e853":"This shows the each rows represents unique listings.","3d7c3df9":"I tried thirty days (about 1 month) window.  \nThe graph became smooth and the trend became clear, and my belief that the peaks were in the same place became stronger.  \nNext, I extract when the peak comes in each year.","5b35ad01":"From here, I want to answer the last question, **\"Are there any trends of popular rooms?\"**.  \nSo, Let's begin to clean and process the listing data.","06f3638c":"As you can see, it seems that there are columns 90% or more of the values are null, but most of the columns have between 0-30% of null_ratio. Therefore, here, I decided to excluded from analysis the columns with 30% or more null ratio.\n\nAlso, it seems that there are only two null values in `host_since` used to calculate the target variable. This needs to be removed. ","58ea2417":"#### Summary\n\n* Each listings has `365` days record in this data.\n* If `available` values are `f`, the `price` values are `NaN`.\n* There is the weekly trend which the listing prices in weekend are higher than other weekday.","70157394":"Finally, since the value to be acted as a numeric value is recognized as a string, so let's convert it. ","2c9302d3":"This is very right skewed distribution.  \nThe 75% or more values are 90 points. And the most common thing is 100 points.  \nI can say the low score listings are minolity.","18f3bacb":"First, we need to define the target variable 'Popular of listings'. I thought this could be defined as follows.\n\n[Actual number of times rent] \/ ([Available days from 2016 to 2017] * (2017 - [Year of the listings open]))\n\nSince the number of times the listings are actually rent is considered to be proportional to the number of days the property is available, it needs to be scaled, and Furthermore, since the number is considered to increase the earlier the listings is released to the public, we need to scale there as well.\n\nOne thing I can not consider here is the period when the listings was actually rented. The longer the period of rending, the smaller the number of times of rending. However, there are no data to consider this, so I will assume that most of Airbnb users are short-term use. ","c631b3e5":"Up to this point, I can answer two of the three questions mentioned at the beginning. \nFirst of all, let's answer from the question.\n**How long is the period available for lending by rooms?**\n\nThis was shown when investigating listing data. There were two groups in listings. It's a listing available at spots with a maximum nights less than a week and a listing of a sense of renting available for up to three years. \n\nFor a further discussion, plot a scatter plot of maximum nights and minimum nights.","237469ee":"From here, I will turn the data into a form that the model can learn.  \nFirst, I will convert the category valuables to dummy valuables.","28d3d7c8":"It is little noisy, but we can see an increase in the number of Airbnb users. (and the date range is wide than calendar data)  \nAnd I realize it seems to have a peak at about the same time of each year.  \nSo, let's use moving averages to smooth the graph.","e10fb2de":"From the above, the review score seems not to be included.","221a2f66":"The weekend, Friday and Saturday has high prices. ","71cdc5a3":"There are six columns, such as listing_id that received review, id of reviews, when review submitted, and so on.  \nI'm concerned that there are no review scores here. I think it might be in comments, so let's confirm this.","a9a623ca":"## Data Understanding\n\nWe have three data.\n\n* `listings`: including full descriptions and average review score\n* `calendar`: including listing id and the price and availability for that day\n* `reviews`: including unique id for each reviewer and detailed comments\n\nIn this part, I'll make some visualization and aggregation to understand the charactoristics of the data.","2f539fc4":"Let's begin with looking at first 5 row of the data and columns information.","566aa1a5":"It has improved somewhat, but the values are still too dense and many predicted values have been pulled into that part.\nTo improve this, I think two method are effective.\n\n* Review the logic of the objective variable; this definition maybe too complex, it maybe good with more simple definition of target valuables.  \n* Do downsampling in dense areas.\n\nThis analysis doesn't cover that much, but I'm going to add it if there is something in the future.","4ded8102":"Next, I am interested in below columns.\n\n* review_scores_rating\n* price\n* maximum_nights\n\nWhat is the distribution of these values in each columns? Is the distribution skewed or normal?  \nLet's look at!","d50d5e20":"### Answer my Question","c7c1d1fc":"This problem is regression.\nI'll use random forest regressor here, and use cross validation to evaluation.","69c901c0":"## Modeling, Evaluation","48ed7510":"#### review_scores_rating","5f070461":"### calendar","5ddb0482":"This is the project of Udacity's data scientist nanodegree program (Term2, Project1).  \nIn this project, we are required to analysis data with **CRISP-DM** process. The CRISP-DM process is below.\n\n#### CRISP-DM (Cross-Industry Standard Process for Data Mining)\n- Business Understanding\n- Data Understanding\n- Data Preparation\n- Modeling\n- Evaluation\n- Deployment\n\nSo, in this kernel I analysis data with following this process.","099916ce":"Let's try logarithmic transformation of the target variable.","59471752":"Let's look first 5 row of the data and column information.","91901ea4":"Furthermore, delete columns that have only a single value, because the feature value does not make sense.","ebf4bc18":"In response to the result, now I have two question .\n\n1. If the available values are f, the price values seems to be NaN. But it is only a hypothesis, is it true all data?\n2. How many rows per each listing_id?\n\nLet's answer these questions with exploring data.","1fd03bc6":"First, we start with a check for null values. This is because we can not use columns that has too null values.","c25b8b65":"From here, it can be seen that the minimum nights is almost constant regardless of the maximum nights.\nIn other words, it can be seen that listings with a long maximum nights are not rented exclusively for rental, but are widely handled from spot use to long-term stay.","e5987eb9":"Let's answer the second question.  \n**Is there a busy season?**\n\nIt can not be said exactly because the actual duration of the user's stay are not included in the data, but the number of reviews is considered to be a guide.\nIn addition, since periodical peaks appear in the number of reviews annually, it may be considered that the neighborhood is a busy season.\n\nWe found that the biggest busy season was the beginning of September, but how long will it be the busy season? Let's look in more detail!","b1243fd2":"## Business Understanding\n\nAirbnb is a platform of accommodation which match the needs of staying and of lending.  \nTheir main source of income is **fee for host**. Basically, as the number of transactions between the host and the guest increases, their profit also increases.  \nSo, It is important to their business and I expect it to be one of their KPIs.\n\n\n<img src=\"https:\/\/bmtoolbox.net\/wp-content\/uploads\/2016\/06\/airbnb.jpg\" width=700>\n\nref: https:\/\/bmtoolbox.net\/stories\/airbnb\/\n\n#### What can we do to increase the transactions?\nI considered three below questions to explore its way.\n\n* **How long is the period available for lending by rooms?**  \nIs there rooms which is available all for years? or almost rooms are available on spot like one day or one week?  \nHere, I want to know the trend in the outline of the data.\n\n* **Is there a busy season?**  \nIf the demand for accommodation is more than the number of rooms available for lending, it leads to the loss of business opportunities.  \nSo, I want to know whether is there the busy season. If this is true, we must create a mechanism to increase the number of rooms available for lending during the busy season.\n\n* **Are there any trends of popular rooms?**  \nIf this question's answer is true, we can suggest host to ways make the room easier to rent.  \nIn this part, I'll use machine learning technique.","26168493":"## Data Preparation","d5f29782":"This is long tail distribution.  \nAlmost values are from 0 to 200.","2cf8a713":"My hypothesis is correct.  \nThe peak seems to be towards the beginning of September!!\nIs this summer vacation?","fbfb44ae":"## Summary","213ede4e":"### listings","3e43bb9d":"This notebook uses data from the Seattle area of Airbnb and has been analyzed to answer the following questions.\nHere we summarize the answers to those questions.\n\n* **How long is the period available for lending by rooms?**  \nThe histogram of maximum nights shows that there are two groups.    \nOne is a listing that can be used at spots such as the maximum number of nights within a week.  \nThe other is a listing that supports a wide range of stay from the super long-term stay of the maximum number of stays for three years or more and the minimum number of nights for around two days to the spot use. \n\n\n* **Is there a busy season?**   \nThe answer is **Yes**.  \nApart from the increase in the number of Airbnb users, there was definitely a timely increase in the number of reviews at the same time each year.  \nIt is thought that it is about one month around early September and overlaps with the summer vacation time. It is important that the number of properties that can be provided at this time exceeds demand.\n\n\n* **Are there any trends of popular rooms?**  \nI could not derive it from my analysis.  \nHowever, I learned that the score improves by logarithmic transformation. I will find time in the future and try to improve.","faf76cbd":"First, I'll investigate how many listings are in the data.","274f5c73":"This is very surprising because I expect it would be a week at most.  \nIn fact, almost `maxmum_night` values are setted 1125.   \nI have not used Airbnb so I don't know, but maybe there may be something like the default value.  \nOr there maybe two segments, one is `spot available listings`, the other is `long term listings like normal rent`. ","fc1264e8":"I can not predict at all. Why?  \nTo consider why, let's make a scatter plot of predicted and actual values.","e02b7cc0":"There are 4 columns.  \nHere, I found some charactoristics of the data.\n\n* Not only available days are stored in data, it seems to be stored not available days.\n* If the `available` values are `f`, the `price` values seems to be `NaN`. \n* The `price` values are stored as object, not integer. This is caused the value stored like `$xx.xx`, and it is necessary to transform this column.","50b8036f":"Next, delete the column that seems to be directly related to the objective variable (ex. num_review)","874525d1":"Next, handle any remaining null values."}}