{"cell_type":{"f6e48432":"code","c5735215":"code","21aef959":"code","a72eefda":"code","7d9f172b":"code","329de0f1":"code","059582af":"code","128650bb":"code","d1f47bde":"code","29107844":"code","551dbc6b":"code","6fef24c7":"code","82bb1dec":"code","5059974c":"code","96d28a70":"code","2e91b38c":"code","6317067f":"code","3b242dd9":"code","70e77c78":"code","46e868d2":"code","f800ac31":"code","07b62c2f":"code","f2cdf3c2":"code","d20be0c2":"code","d6d099e3":"code","d233b859":"code","b469a8ce":"code","7fdb5796":"code","0267a876":"code","a04bd9b4":"code","4af37627":"code","d00a1e24":"code","3df573da":"code","dd895989":"code","9f78c7fd":"code","5b8a401b":"code","132e08a5":"code","b33f8b6c":"markdown","d91bb735":"markdown","76c10285":"markdown","697d92e1":"markdown"},"source":{"f6e48432":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(color_codes=True)\nfrom scipy import stats\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn import metrics\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c5735215":"data1 = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndata1 = pd.DataFrame(data1)","21aef959":"data1.info()","a72eefda":"#number of records and features in the dataset\ndata1.shape","7d9f172b":"#Check duplicate rows in data\nduplicate_rows = data1[data1.duplicated()]\nprint(\"Number of duplicate rows :: \", duplicate_rows.shape)","329de0f1":"#we have one duplicate row.\n#Removing the duplicate row\n\ndata1 = data1.drop_duplicates()\nduplicate_rows = data1[data1.duplicated()]\nprint(\"Number of duplicate rows :: \", duplicate_rows.shape)\n\n#Number of duplicate rows after dropping one duplicate row","059582af":"#Check if the other data is consistent\ndata1.shape","128650bb":"#Looking for null values\nprint(\"Null values :: \")\nprint(data1.isnull() .sum())","d1f47bde":"#As there are no null values in data, we can proceed with the next steps.\n#Detecting Outliers\n\n# 1. Detecting Outliers using IQR (InterQuartile Range)\nsns.boxplot(x=data1['age'])","29107844":"#No Outliers observed in 'age'\nsns.boxplot(x=data1['sex'])","551dbc6b":"#No outliers observed in sex data\nsns.boxplot(x=data1['cp'])","6fef24c7":"#No outliers in 'cp'\nsns.boxplot(x=data1['trtbps'])","82bb1dec":"#Some outliers are observed in 'trtbps'. They will be removed later\nsns.boxplot(x=data1['chol'])","5059974c":"#Some outliers are observed in 'chol'. They will be removed later\nsns.boxplot(x=data1['fbs'])","96d28a70":"sns.boxplot(x=data1['restecg'])","2e91b38c":"sns.boxplot(x=data1['thalachh'])","6317067f":"#Outliers present in thalachh\nsns.boxplot(x=data1['exng'])","3b242dd9":"sns.boxplot(x=data1['oldpeak'])","70e77c78":"#Outliers are present in 'OldPeak'\nsns.boxplot(x=data1['slp'])","46e868d2":"sns.boxplot(x=data1['caa'])","f800ac31":"#Outliers are present in 'caa'\nsns.boxplot(x=data1['thall'])","07b62c2f":"#Find the InterQuartile Range\nQ1 = data1.quantile(0.25)\nQ3 = data1.quantile(0.75)\n\nIQR = Q3-Q1\nprint('*********** InterQuartile Range ***********')\nprint(IQR)","f2cdf3c2":"# Remove the outliers using IQR\ndata2 = data1[~((data1<(Q1-1.5*IQR))|(data1>(Q3+1.5*IQR))).any(axis=1)]\ndata2.shape","d20be0c2":"#Removing outliers using Z-score\nz = np.abs(stats.zscore(data1))\ndata3 = data1[(z<3).all(axis=1)]\ndata3.shape","d6d099e3":"#Finding the correlation between variables\npearsonCorr = data3.corr(method='pearson')\nspearmanCorr = data3.corr(method='spearman')","d233b859":"#Finding the correlation between variables\npearsonCorr = data3.corr(method='pearson')\nspearmanCorr = data3.corr(method='spearman')","b469a8ce":"fig = plt.subplots(figsize=(14,8))\nsns.heatmap(pearsonCorr, vmin=-1,vmax=1, cmap = \"Greens\", annot=True, linewidth=0.1)\nplt.title(\"Pearson Correlation\")","7fdb5796":"fig = plt.subplots(figsize=(14,8))\nsns.heatmap(spearmanCorr, vmin=-1,vmax=1, cmap = \"Blues\", annot=True, linewidth=0.1)\nplt.title(\"Spearman Correlation\")","0267a876":"#Create mask for both correlation matrices\n\n#Pearson corr masking\n#Generating mask for upper triangle\nmaskP = np.triu(np.ones_like(pearsonCorr,dtype=bool))\n\n#Adjust mask and correlation\nmaskP = maskP[1:,:-1]\npCorr = pearsonCorr.iloc[1:,:-1].copy()\n\n#Setting up a diverging palette\ncmap = sns.diverging_palette(0, 200, 150, 50, as_cmap=True)\n\nfig = plt.subplots(figsize=(14,8))\nsns.heatmap(pCorr, vmin=-1,vmax=1, cmap = cmap, annot=True, linewidth=0.3, mask=maskP)\nplt.title(\"Pearson Correlation\")\n","a04bd9b4":"#Create mask for both correlation matrices\n\n#Spearson corr masking\n#Generating mask for upper triangle\nmaskS = np.triu(np.ones_like(spearmanCorr,dtype=bool))\n\n#Adjust mask and correlation\nmaskS = maskS[1:,:-1]\nsCorr = spearmanCorr.iloc[1:,:-1].copy()\n\n#Setting up a diverging palette\ncmap = sns.diverging_palette(0, 250, 150, 50, as_cmap=True)\n\nfig = plt.subplots(figsize=(14,8))\nsns.heatmap(sCorr, vmin=-1,vmax=1, cmap = cmap, annot=True, linewidth=0.3, mask=maskS)\nplt.title(\"Spearman Correlation\")\n","4af37627":"#From this we observe that the minimum correlation between output and other features in\n#fbs,trtbps and chol\n\n\nx = data3.drop(\"output\", axis=1)\ny = data3[\"output\"]\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)","d00a1e24":"#Building classification models\nnames = ['Age', 'Sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall']\n\n#   ****************Logistic Regression*****************\nlogReg = LogisticRegression(random_state=0, solver='liblinear')\nlogReg.fit(x_train, y_train)\n\n#Check accuracy of Logistic Regression\ny_pred_logReg = logReg.predict(x_test)\n#Model Accuracy\nprint(\"Accuracy of logistic regression classifier :: \" ,metrics.accuracy_score(y_test,y_pred_logReg))\n\n#Removing the features with low correlation and checking effect on accuracy of model\nx_train1 = x_train.drop(\"fbs\",axis=1)\nx_train1 = x_train1.drop(\"trtbps\", axis=1)\nx_train1 = x_train1.drop(\"chol\", axis=1)\nx_train1 = x_train1.drop(\"restecg\", axis=1)\n\nx_test1 = x_test.drop(\"fbs\", axis=1)\nx_test1 = x_test1.drop(\"trtbps\", axis=1)\nx_test1 = x_test1.drop(\"chol\", axis=1)\nx_test1 = x_test1.drop(\"restecg\", axis=1)\n\nlogReg1 = LogisticRegression(random_state=0, solver='liblinear').fit(x_train1,y_train)\ny_pred_logReg1 = logReg1.predict(x_test1)\nprint(\"\\nAccuracy of logistic regression classifier after removing features:: \" ,metrics.accuracy_score(y_test,y_pred_logReg1))\n","3df573da":"# ***********************Decision Tree Classification***********************\ndecTree = DecisionTreeClassifier(max_depth=6, random_state=0)\ndecTree.fit(x_train,y_train)\n\ny_pred_decTree = decTree.predict(x_test)\n\nprint(\"Accuracy of Decision Trees :: \" , metrics.accuracy_score(y_test,y_pred_decTree))\n\n#Remove features which have low correlation with output (fbs, trtbps, chol)\nx_train_dt = x_train.drop(\"fbs\",axis=1)\nx_train_dt = x_train_dt.drop(\"trtbps\", axis=1)\nx_train_dt = x_train_dt.drop(\"chol\", axis=1)\nx_train_dt = x_train_dt.drop(\"age\", axis=1)\nx_train_dt = x_train_dt.drop(\"sex\", axis=1)\n\nx_test_dt = x_test.drop(\"fbs\", axis=1)\nx_test_dt = x_test_dt.drop(\"trtbps\", axis=1)\nx_test_dt = x_test_dt.drop(\"chol\", axis=1)\nx_test_dt = x_test_dt.drop(\"age\", axis=1)\nx_test_dt = x_test_dt.drop(\"sex\", axis=1)\n\ndecTree1 = DecisionTreeClassifier(max_depth=6, random_state=0)\ndecTree1.fit(x_train_dt, y_train)\ny_pred_dt1 = decTree1.predict(x_test_dt)\n\nprint(\"Accuracy of decision Tree after removing features:: \", metrics.accuracy_score(y_test,y_pred_dt1))","dd895989":"# Using Random forest classifier\n\nrf = RandomForestClassifier(n_estimators=500)\nrf.fit(x_train,y_train)\n\ny_pred_rf = rf.predict(x_test)\n\nprint(\"Accuracy of Random Forest Classifier :: \", metrics.accuracy_score(y_test, y_pred_rf))\n\n#Find the score of each feature in model and drop the features with low scores\nf_imp = rf.feature_importances_\nfor i,v in enumerate(f_imp):\n    print('Feature: %s, Score: %.5f' % (names[i],v))\n","9f78c7fd":"#Removing the following features : fbs(score=0.006), sex(score=0.02), trtbps(score=0.072), chol(score=0.078), \n#restecg(score=0.02), exng(score=0.06), slp(score=0.06)\n\n#names = ['Age', 'Sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall']\nnames1 = ['Age', 'Sex', 'cp', 'trtbps', 'chol','restecg', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall']\n\nx_train_rf2 = x_train.drop(\"fbs\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"sex\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"restecg\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"slp\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"exng\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"trtbps\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"chol\",axis=1)\n#x_train_rf2 = x_train_rf2.drop(\"age\",axis=1)\n\nx_test_rf2 = x_test.drop(\"fbs\", axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"sex\", axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"restecg\",axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"slp\",axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"exng\",axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"trtbps\",axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"chol\",axis=1)\n#x_test_rf2 = x_test_rf2.drop(\"age\",axis=1)\n\nrf2 = RandomForestClassifier(n_estimators=500)\nrf2.fit(x_train_rf2,y_train)\n\ny_pred_rf2 = rf2.predict(x_test_rf2)\nprint(\"Accuracy of Random Forest Classifier after removing features with low score :\")\nprint(\"New Accuracy :: \" , metrics.accuracy_score(y_test,y_pred_rf2))\nprint(\"\\n\")\nprint(\"---------------------------------------------------------------------------------------------\")\n\nf_imp = rf2.feature_importances_\nfor i,v in enumerate(f_imp):\n    print('Feature: %s, Score: %.5f' % (names1[i],v))\nprint(\"---------------------------------------------------------------------------------------------\")\n","5b8a401b":"#K Neighbours Classifier\n\nknc =  KNeighborsClassifier()\nknc.fit(x_train,y_train)\n\ny_pred_knc = knc.predict(x_test)\n\nprint(\"Accuracy of K-Neighbours classifier :: \", metrics.accuracy_score(y_test,y_pred_knc))","132e08a5":"#Models and their accuracy\n\nprint(\"*****************Models and their accuracy*****************\")\nprint(\"Logistic Regression Classifier :: \", metrics.accuracy_score(y_test,y_pred_logReg1))\nprint(\"Decision Tree :: \", metrics.accuracy_score(y_test,y_pred_dt1))\nprint(\"Random Forest Classifier :: \", metrics.accuracy_score(y_test, y_pred_rf))\nprint(\"K Neighbours Classifier :: \", metrics.accuracy_score(y_test,y_pred_knc))","b33f8b6c":"Import all the required Libraries","d91bb735":"About this dataset\nAge : Age of the patient\n\nSex : Sex of the patient\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n\nValue 1: typical angina\nValue 2: atypical angina\nValue 3: non-anginal pain\nValue 4: asymptomatic\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg\/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n\nValue 0: normal\nValue 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\nValue 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\nthalach : maximum heart rate achieved\n\ntarget : 0= less chance of heart attack 1= more chance of heart attack\n\nDataset URL: https:\/\/www.kaggle.com\/rashikrahmanpritom\/heart-attack-analysis-prediction-dataset\n","76c10285":"Import the data from local machine","697d92e1":"Thus, the logistic regression gives us a highest accuracy amongst other algorithms used."}}