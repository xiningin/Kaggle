{"cell_type":{"8dcbdf10":"code","ab981dc7":"code","f5f1dc34":"code","2a331dcd":"code","1c340703":"code","36ee6e89":"code","f1376842":"code","d64822a4":"code","1683f316":"code","5af24722":"code","4bddc618":"code","5033c8b8":"code","4899ed88":"code","50147ff9":"code","c26242bd":"code","b4c5ac90":"code","6749cd63":"code","a3a6d1fa":"code","e8067627":"code","93034fd9":"code","1e6bd614":"markdown","b04b9059":"markdown","7d3584ac":"markdown","37f8ff0b":"markdown","09ddae5b":"markdown","47291d8c":"markdown","5a56f438":"markdown","4f87f112":"markdown","fa5a4844":"markdown"},"source":{"8dcbdf10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error as mse\n#syntax: mse(y_true, y_prediction)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab981dc7":"from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","f5f1dc34":"train_data = pd.read_csv(\"..\/input\/cap-4611-2021-fall-assignment-1\/train.csv\", low_memory = False)\ntrain_data = train_data[train_data['Group'] == 'By Week']\ntrain_data = train_data[train_data['HHS Region'] == 'United States']\ndroppedColumns = ['Data As Of', 'End Date', 'Group', 'Year', 'Month', 'Week-Ending Date', 'HHS Region', 'Total Deaths', 'Footnote']\ntrain_data = train_data.drop(droppedColumns, axis = 1)\n#Convert 'MMWR Week' and 'COVID-19 Deaths' to ints\ntrain_data['MMWR Week'] = train_data['MMWR Week'].astype('int')\ntrain_data['COVID-19 Deaths'] = train_data['COVID-19 Deaths'].astype('int')\n#Convert 'Start Date' to datetime\ntrain_data['Start Date'] = pd.to_datetime(train_data['Start Date'])\ntrain_data","2a331dcd":"train_data.isna().sum() #Checking for missing data","1c340703":"#Same with the test_data\ntest_data = pd.read_csv(\"..\/input\/cap-4611-2021-fall-assignment-1\/test.csv\", low_memory = False)\ndroppedColumns2 = ['Data As Of', 'End Date', 'Group', 'Year', 'Month', 'Week-Ending Date', 'HHS Region']\ntest_data = test_data.drop(droppedColumns2, axis = 1)\ntest_data['MMWR Week'] = test_data['MMWR Week'].astype('int')\ntest_data['Total Deaths'] = test_data['Total Deaths'].astype('int')\ntest_data['Start Date'] = pd.to_datetime(test_data['Start Date'])\ntest_data.rename(columns = {'Total Deaths' : 'COVID-19 Deaths'}, inplace = True)\ntest_data","36ee6e89":"#Convert date to indexes (same dates = same index)\ntrain_data['Start Date'] = train_data.groupby('Start Date').ngroup()\ntest_data['Start Date'] = test_data.groupby('Start Date').ngroup()\ntest_data['Start Date'] = test_data['Start Date']","f1376842":"#Creating binary columns for race and age\ntrain_data_age = pd.get_dummies(train_data['Age Group'])\ntrain_data_race = pd.get_dummies(train_data['Race and Hispanic Origin Group'])\ntrain_data = pd.concat([train_data, train_data_age, train_data_race], axis = 1)\ntrain_data = train_data.drop(['Age Group', 'Race and Hispanic Origin Group'], axis = 1)\n\ntest_data_age = pd.get_dummies(test_data['Age Group'])\ntest_data_race = pd.get_dummies(test_data['Race and Hispanic Origin Group'])\ntest_data = pd.concat([test_data, test_data_age, test_data_race], axis = 1)\ntest_data = test_data.drop(['Age Group', 'Race and Hispanic Origin Group'], axis = 1)","d64822a4":"train_data","1683f316":"test_data","5af24722":"train_data['COVID-19 Deaths'] = train_data['COVID-19 Deaths'].diff()\ntrain_data.dropna(inplace = True)\nplt.plot(train_data['Start Date'], train_data['COVID-19 Deaths'])\nplt.show()","4bddc618":"X = train_data.drop(['COVID-19 Deaths'], axis=1)\ny = train_data['COVID-19 Deaths']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, shuffle=False)","5033c8b8":"pipe_lr = Pipeline([('scaler', StandardScaler()), ('lr', LinearRegression())])\nols_params = {\n    'lr__fit_intercept': ['True', 'False']\n}\ngrid_lr = GridSearchCV(estimator=pipe_lr, param_grid=ols_params, scoring= make_scorer(mse), n_jobs = -1, cv=TimeSeriesSplit(n_splits = 45))\ngrid_lr.fit(X_train, y_train)\n\nlr_predict = grid_lr.predict(X_test)\nlr_predict[lr_predict < 0] = 0\nlr_score = mse(y_test, lr_predict)\n\nprint(lr_score)","4899ed88":"pipe_ridge = Pipeline([('scaler', StandardScaler()), ('ridge', Ridge())])\nridge_params = {\n    'ridge__alpha': np.linspace(.1, 10, num=20),\n}\ngrid_ridge = GridSearchCV(estimator=pipe_ridge, param_grid=ridge_params, scoring = make_scorer(mse), n_jobs = -1, cv=TimeSeriesSplit(n_splits = 45))\ngrid_ridge.fit(X_train, y_train)\n\nridge_predict = grid_ridge.predict(X_test)\nridge_predict[ridge_predict < 0] = 0\nridge_score = mse(y_test, ridge_predict)\n\nprint(ridge_score)","50147ff9":"pipe_lasso = Pipeline([('scaler', StandardScaler()), ('lasso', Lasso())])\nlasso_params = {\n    'lasso__alpha' : np.linspace(1, 10, num=20),\n    'lasso__warm_start': ['True', 'False'],\n}\ngrid_lasso = GridSearchCV(estimator=pipe_lasso, param_grid=lasso_params, scoring = make_scorer(mse), n_jobs = -1, cv=TimeSeriesSplit(n_splits = 45))\ngrid_lasso.fit(X_train, y_train)\n\nlasso_predict = grid_lasso.predict(X_test)\nlasso_predict[lasso_predict < 0] = 0\nlasso_score = mse(y_test, lasso_predict)\n\n\nprint(lasso_score)","c26242bd":"pipe_en = Pipeline([('scaler', StandardScaler()), ('en', ElasticNet())])\nen_params = {\n    'en__alpha': np.linspace(1, 10, num=20),\n    'en__warm_start': ['True', 'False'],\n}\ngrid_en = GridSearchCV(estimator=pipe_en, param_grid=en_params, scoring = make_scorer(mse), n_jobs = -1, cv = TimeSeriesSplit(n_splits=45))\ngrid_en.fit(X_train, y_train)\n\nen_predict = grid_en.predict(X_test)\nen_predict[en_predict < 0] = 0\nen_score = mse(y_test, en_predict)\n\nprint(en_score)","b4c5ac90":"res = pd.DataFrame({'ols': lr_predict, 'ridge': ridge_predict, 'lasso': lasso_predict,'elastic_net': en_predict})\nres.describe()","6749cd63":"#Picking best model\nbestmodel = grid_lr\nif lr_score < ridge_score:\n    bestmodel = grid_ridge\nelif ridge_score < lasso_score:\n    bestmodel = grid_lasso\nelif lasso_score < en_score:\n    bestmodel = grid_en","a3a6d1fa":"test_predict = grid_ridge.predict(test_data.drop(['id'], axis=1))\ntest_predict[test_predict < 0] = 0 #Getting rid of values below zero (cant have negative deaths)\ntest_predict = test_predict.round() #Turing values into ints\nprint(test_predict)","e8067627":"df_test_predict = pd.DataFrame(test_predict, columns=['COVID-19 Deaths'])\ndf_test_predict['rev'] = df_test_predict['COVID-19 Deaths'].shift(1) + df_test_predict['COVID-19 Deaths']\ndf_test_predict.fillna(np.nanmedian(df_test_predict['rev']), inplace=True)\ntest_predict = df_test_predict['rev']\nprint(test_predict)","93034fd9":"output = pd.DataFrame({'id': test_data.id, 'COVID-19 Deaths': test_predict})\noutput.to_csv('submission.csv', index=False)\nprint(\"Done!\")","1e6bd614":"# Elastic Net Regression Model","b04b9059":"# Ridge Regression Model","7d3584ac":"# Creating submission.csv","37f8ff0b":"# Ordinary Least Squares Model","09ddae5b":"From looking at the data in Excel it is clear we need to drop multiple columns off the bat.\n\nI am filtering all data not gathered 'By Week' by 'Group'\n\nThen, I am dropping the columns:\n* 'Data As Of': missing values, useless information\n* 'End Date': already have 'Start Date', just add 7?\n* 'Group': not needed, all data is 'By Week' by now\n* 'Year','Month','Week-Ending Date': redundant\n* 'HHS Region': not needed, all data is 'United States' by now\n* 'Total Deaths': redundant\n* 'Footnote': useless","47291d8c":"# Lasso Regression Model","5a56f438":"# Building Models","4f87f112":"# Feature Engineering","fa5a4844":"# Loading Data"}}