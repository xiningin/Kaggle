{"cell_type":{"0306ccc7":"code","b1a6a99a":"code","d1358e75":"code","9a1fedb4":"code","9cc45687":"code","e1dcd3ab":"code","00dda3a8":"code","ce0e472d":"code","8c4e6e53":"markdown","4a44a02a":"markdown","46e41885":"markdown","6eef3db0":"markdown"},"source":{"0306ccc7":"# Import packages\nimport pandas as pd, numpy as np\nimport tensorflow as tf\nassert tf.__version__ >= '2.0'\n\nfrom itertools import islice\n\n# Keras\nfrom keras.layers import Dense, Embedding, LSTM, Dropout, MaxPooling1D, Conv1D\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model, Sequential\nfrom keras.preprocessing import sequence\nfrom keras.datasets import imdb\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Suppress warnings\nimport warnings; warnings.filterwarnings('ignore')\n\nrandom_state = 42\nnp.random.seed(random_state)\ntf.random.set_seed(random_state)","b1a6a99a":"vocab_size = 10000\nmaxlen = 300\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = vocab_size)\n\nx_train = pad_sequences(x_train, maxlen = maxlen, padding = 'pre')\nx_test =  pad_sequences(x_test, maxlen = maxlen, padding = 'pre')\n\nX = np.concatenate((x_train, x_test), axis = 0)\ny = np.concatenate((y_train, y_test), axis = 0)\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state, shuffle = True)\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2, random_state = random_state, shuffle = True)\n\nprint('---'*20, f'\\nNumber of rows in training dataset: {x_train.shape[0]}')\nprint(f'Number of columns in training dataset: {x_train.shape[1]}')\nprint(f'Number of unique words in training dataset: {len(np.unique(np.hstack(x_train)))}')\n\n\nprint('---'*20, f'\\nNumber of rows in validation dataset: {x_valid.shape[0]}')\nprint(f'Number of columns in validation dataset: {x_valid.shape[1]}')\nprint(f'Number of unique words in validation dataset: {len(np.unique(np.hstack(x_valid)))}')\n\n\nprint('---'*20, f'\\nNumber of rows in test dataset: {x_test.shape[0]}')\nprint(f'Number of columns in test dataset: {x_test.shape[1]}')\nprint(f'Number of unique words in test dataset: {len(np.unique(np.hstack(x_test)))}')\n\n\nprint('---'*20, f'\\nUnique Categories: {np.unique(y_train), np.unique(y_valid), np.unique(y_test)}')","d1358e75":"def decode_review(x, y):\n  w2i = imdb.get_word_index()                                \n  w2i = {k:(v + 3) for k, v in w2i.items()}\n  w2i['<PAD>'] = 0\n  w2i['<START>'] = 1\n  w2i['<UNK>'] = 2\n  i2w = {i: w for w, i in w2i.items()}\n\n  ws = (' '.join(i2w[i] for i in x))\n  print(f'Review: {ws}')\n  print(f'Actual Sentiment: {y}')\n  return w2i, i2w\n\nw2i, i2w = decode_review(x_train[0], y_train[0])\n\n# get first 50 key, value pairs from id to word dictionary\nprint('---'*30, '\\n', list(islice(i2w.items(), 0, 50)))","9a1fedb4":"# Model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 256, input_length = maxlen))\nmodel.add(Dropout(0.25))\nmodel.add(Conv1D(256, 5, padding = 'same', activation = 'relu', strides = 1))\nmodel.add(Conv1D(128, 5, padding = 'same', activation = 'relu', strides = 1))\nmodel.add(MaxPooling1D(pool_size = 2))\nmodel.add(Conv1D(64, 5, padding = 'same', activation = 'relu', strides = 1))\nmodel.add(MaxPooling1D(pool_size = 2))\nmodel.add(LSTM(75))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nprint(model.summary())\n\n# Adding callbacks\nes = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 0)  \nmc = ModelCheckpoint('imdb_model.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)","9cc45687":"# Fit the model\nmodel.fit(x_train, y_train, validation_data = (x_valid, y_valid), epochs = 3, batch_size = 64, verbose = True, callbacks = [es, mc])\n\n# Evaluate the model\nscores = model.evaluate(x_test, y_test, batch_size = 64)\nprint('Test accuracy: %.2f%%' % (scores[1]*100))","e1dcd3ab":"y_pred = model.predict_classes(x_test)\nprint(f'Classification Report:\\n{classification_report(y_pred, y_test)}')","00dda3a8":"sample_x_test = x_test[np.random.randint(10000)]\nfor layer in model.layers:\n\n    model_layer = Model(inputs = model.input, outputs = model.get_layer(layer.name).output)\n    output = model_layer.predict(sample_x_test.reshape(1,-1))\n    print('\\n','--'*20, layer.name, 'layer', '--'*20, '\\n')\n    print(output)","ce0e472d":"decode_review(x_test[10], y_test[10])\nprint(f'Predicted sentiment: {y_pred[10][0]}')","8c4e6e53":"## build LSTM using keras","4a44a02a":"# EDA","46e41885":"## Conclusion\n- Sentiment classification task on the IMDB dataset, on test dataset,\n- Accuracy: ~ 90%\n- F1-score: ~ 90%\n- Loss of 0.25","6eef3db0":"## Get word index and create a key-value pair for word and word id"}}