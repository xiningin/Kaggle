{"cell_type":{"f4d5eb43":"code","73e6d949":"code","959d1cc6":"code","ae6a44fc":"code","f7ea40a5":"code","1fbb3c64":"code","c0658e4b":"code","9d8438f7":"code","48fd8143":"code","9560fdac":"code","eac82158":"code","cb2f38c1":"code","aa1cda38":"code","a65f9994":"code","6292f6ad":"code","f22949be":"markdown","b93adc69":"markdown","5ea27655":"markdown","52623827":"markdown","350f77dd":"markdown","be8b1561":"markdown","1a113571":"markdown"},"source":{"f4d5eb43":"!pip install category_encoders","73e6d949":"pip install sklearn-genetic","959d1cc6":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import zero_one_loss\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import balanced_accuracy_score\n\nfrom collections import OrderedDict\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.feature_selection import RFE\nfrom genetic_selection import GeneticSelectionCV\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport category_encoders as ce","ae6a44fc":"treino = pd.read_csv('\/content\/train_full.csv')\nteste = pd.read_csv('\/content\/test_without_label.csv')\n\ndef remove_space(df):\n    df.columns = [x.strip() for x in df.columns]\n\nremove_space(teste)\nremove_space(treino)","f7ea40a5":"def team(dfIN):\n\n    df = dfIN.copy()\n    time_c = df['H_Team']\n    \n    times_antigos = {\"New Jersey Nets\": \"Brooklyn Nets\",\n                     \"Charlotte Bobcats\": \"Charlotte Hornets\",\n                     \"New Orleans Hornets\": \"New Orleans Pelicans\"}\n    \n    time_c = time_c.replace(times_antigos)\n\n    return(time_c)","1fbb3c64":"def convert(dfIN):\n\n  df = dfIN.copy()\n  Resultado = {'W': 1,\n               'L': 0}\n  conv = df['WinOrLose'].replace(Resultado)\n\n  return(conv)","c0658e4b":"def select_features(dfIN1,dfIN2):\n\n  data = dfIN1.copy()\n  data1 = dfIN2.copy()\n  #observando as variaveis que possuem os maiores valores de correla\u00e7\u00e3o com o target\n  n = data.shape[1]\n  nomes = ['']*n\n  correlacoes = []\n  for i in range(0,n):\n    corr,vp = stats.spearmanr(data.iloc[:,-1],data.iloc[:,i])\n    correlacoes.append(abs(corr))\n    nomes[i] = data.columns[i]\n  \n  obj = {'Atributos': nomes, 'Correla\u00e7\u00f5es': correlacoes}\n  df = pd.DataFrame(data=obj)\n  df.drop(df.tail(1).index,inplace=True)\n  media = np.mean(correlacoes)\n  df = df[(df['Correla\u00e7\u00f5es'] > media)] #quantas variaveis tem correla\u00e7\u00e3o maior que a m\u00e9dia das correla\u00e7\u00f5es com y\n  select = list(df['Atributos'])\n\n  data = data[data.columns.intersection(select)]\n  data1 = data1[data1.columns.intersection(select)]\n\n  corr_features = set()\n  corr_matrix = data.corr()\n\n  for i in range(len(corr_matrix.columns)):\n      for j in range(i):\n          if abs(corr_matrix.iloc[i, j]) > 0.8:\n              colname = corr_matrix.columns[i]\n              corr_features.add(colname)\n              \n  data.drop(labels=corr_features, axis=1, inplace=True)\n  data1.drop(labels=corr_features, axis=1, inplace=True)\n\n  return(data,data1)","9d8438f7":"def string_encoder(dfIN1, dfIN2):\n    df1 = dfIN1.copy()\n    df2 = dfIN2.copy()\n\n    df1['H_Team'] = ce.TargetEncoder().fit_transform(df1['H_Team'], df1['WinOrLose'])\n    df2['H_Team'] = ce.TargetEncoder().fit_transform(df1['H_Team'], df1['WinOrLose'])\n    df1['Data'] = ce.TargetEncoder().fit_transform(df1['Data'], df1['WinOrLose'])\n    df2['Data'] = ce.TargetEncoder().fit_transform(df1['Data'], df1['WinOrLose'])\n    return(df1, df2)","48fd8143":"def pre_processing(dfIN1,dfIN2):\n    Hteam1 = team(dfIN1)\n    Hteam2 = team(dfIN2)\n    conv = convert(dfIN1)\n    treino,teste = select_features(dfIN1,dfIN2)\n    treino['H_Team'] = Hteam1\n    teste['H_Team'] = Hteam2\n    treino['WinOrLose'] = conv\n    final_treino, final_teste = string_encoder(treino, teste)\n    \n    X = final_treino.copy()\n    y = X['WinOrLose'].copy()\n    X.drop(['WinOrLose'], axis = 1, inplace = True)\n\n    col = X.columns\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    X = scaler.fit_transform(X)\n    final_teste = scaler.fit_transform(final_teste)\n    X = pd.DataFrame(X, columns=col)\n    final_teste = pd.DataFrame(final_teste, columns=col)\n\n    return(X,y ,final_teste)","9560fdac":"X, y, teste_use = pre_processing(treino, teste)","eac82158":"def alg_selection(train, test):\n    Xm = train.copy()\n    ym = test.copy()\n    \n    acuracia , auc , acuracia_b , algoritmos  = [] , [] , [] , []\n\n    # Definindo os valores para o n\u00famero de folds\n    num_folds = 5\n    seed = 11\n\n     # LISTA DE MODELOS\n    modelos = [('SVCL', SVC(kernel='linear')),\n            ('NBG', GaussianNB()),\n            ('RFC', RandomForestClassifier(n_jobs=-1))\n            ]\n\n    ## M\u00e9tricas Utilizadas\n    scor = {'accuracy' : make_scorer(accuracy_score), \n            'roc_auc_score' : make_scorer(roc_auc_score),\n            'balanced_accuracy_score' : make_scorer(balanced_accuracy_score)}\n\n    ## Avaliando modelos com as m\u00e9tricas estipuladas\n    for nome, modelo in modelos:\n        kfold = KFold(n_splits = num_folds, \n                    random_state = seed,\n                    shuffle = True)\n        cv_results = cross_validate(estimator=modelo,\n                                            X = Xm,\n                                            y = ym,\n                                            cv = kfold,\n                                            scoring = scor)\n        \n        algoritmos.append(nome)\n        acuracia += [cv_results['test_accuracy']]\n        auc += [cv_results['test_roc_auc_score']]\n        acuracia_b += [cv_results['test_balanced_accuracy_score']]\n\n    fig, ([ax1, ax2, ax3]) = plt.subplots(1, 3,figsize=(15,5))\n    ax1.title.set_text('Acuracia')\n    ax2.title.set_text('AUC')\n    ax3.title.set_text('Acuracia_b')\n    ax1.boxplot(acuracia)\n    ax2.boxplot(auc)\n    ax3.boxplot(acuracia_b)\n    ax1.set_xticklabels(algoritmos)\n    ax2.set_xticklabels(algoritmos)\n    ax3.set_xticklabels(algoritmos)\n\nalg_selection(X,y)","cb2f38c1":"p = 0.8\nx_train, x_test, y_train, y_test = train_test_split(X, y, train_size = p,shuffle = True,random_state = 1, stratify = y)","aa1cda38":"#Ajuste do Naive-Bayes\ncv = 10\np_grid = {\"var_smoothing\":[10**x for x in range(-5,5)]}\nmodel = GaussianNB()\n\ngrid_search_cv = GridSearchCV(model, p_grid, cv=cv, \n                                      scoring='balanced_accuracy', refit=True)\nresult = grid_search_cv.fit(x_train, y_train)\nbest_model = result.best_estimator_  ","a65f9994":"#Teste do Naive Bayes\nmodel_final = best_model\nmodel_final.fit(x_train,y_train)\ny_pred = model_final.predict(x_test)\nauc = np.round(roc_auc_score(y_test, y_pred),3)\nacc = np.round(balanced_accuracy_score(y_pred, y_test),3)\nprint('AUC:',auc,'Acur\u00e1cia balanciada:',acc)","6292f6ad":"y_pred_final = model_final.predict(teste_use)\nget_id = pd.read_csv('\/content\/test_without_label.csv')\n\naux = get_id[\"Game\"]\nResultado = {1 : 'W',\n             0 : 'L'}\n\n\nobj = {\"Game\": aux,\"WinOrLose\": y_pred_final}\nresp = pd.DataFrame(data = obj)\nresp['WinOrLose'] = resp['WinOrLose'].replace(Resultado)\nresp.to_csv(\"resposta.csv\", index = False)\nresp","f22949be":"# Envio","b93adc69":"# Pr\u00e9-Processamento\n","5ea27655":"Finalizamos com o modelo GaussianNB e para terminar voltamos aos \u00edndices 'W' e 'L' iniciais ","52623827":"# Modelo","350f77dd":"Para o pr\u00e9-processamento dos dados, utilizaremos t\u00e9cnicas para alterar o nome das franquias que mudaram ao longo dos anos para o nome atual, aplicar target encoder nos nomes dos times para transform\u00e1-lo em uma entrada num\u00e9rica e seleiconaremos as features mais correlacionadas com o target e menos correlacionadas entre si. (Para aplicar o target encoder foi necess\u00e1rio transformar W e L em 1 e 0, respectivamente. ","be8b1561":"Para a sele\u00e7\u00e3o do modelo, testamos medidas de desempenho para 3 diferentes e aquele que apresentar melhores resultados ser\u00e1 o selecionado paraa otimiza\u00e7\u00e3o dos hiper-par\u00e2metros e finalizar com a classifica\u00e7\u00e3o.","1a113571":"# Libs & Data"}}