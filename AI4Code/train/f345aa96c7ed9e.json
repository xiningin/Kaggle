{"cell_type":{"fc976171":"code","da51738a":"code","93c46834":"code","bae4812e":"code","8a147412":"code","27a7540c":"code","748fe896":"code","3cd692f5":"code","d6479106":"code","664748b2":"code","d72091ff":"code","cac9d2a3":"code","b1f4b8d3":"code","f679d0b6":"code","5e7b4641":"code","04167a23":"code","e9b1c57a":"code","2b6cbce1":"code","3f1cd86c":"code","312e7c2d":"code","cf64af76":"code","fe23de80":"code","02646020":"code","b91050f4":"code","42c6e831":"code","9653aee2":"code","79a25ac2":"code","6e4e75ef":"code","d4fb9c50":"code","4a5abab5":"code","7bba45f8":"code","966ffeb2":"code","6ae64390":"code","d824b357":"code","0f15ec09":"markdown","9f12e3b0":"markdown","44fe9281":"markdown"},"source":{"fc976171":"import numpy as np \nfrom pandas import *\nfrom matplotlib.pyplot import *\nimport seaborn as sns\n%matplotlib inline ","da51738a":"df = read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","93c46834":"df","bae4812e":"df.isnull().sum()","8a147412":"df.corr()","27a7540c":"df['Outcome'].value_counts()","748fe896":"x = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values","3cd692f5":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0)","d6479106":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","664748b2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","d72091ff":"Raw_models = [(LogisticRegression(),[{'C':[0.25,0.5,0.75,1],'random_state':[0]}]),\n             (KNeighborsClassifier(),[{'n_neighbors':[5,10,6,7]}]),\n             (SVC(),[{'C':[0.25,0.5,0.75,1],'kernel':['linear'],'random_state':[0]},{'C':[0.25,0.5,0.75,1],'kernel':['rbf'],'gamma':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],'random_state':[0]}]),\n             \n             (DecisionTreeClassifier(),[{'criterion':['gini','entropy'],'random_state':[0]}]),\n             (RandomForestClassifier(),[{'n_estimators':[10,100,50,150,200],'criterion':['gini','entropy'],'random_state':[0]}])]","cac9d2a3":"from sklearn.model_selection import GridSearchCV","b1f4b8d3":"for i,j in Raw_models:\n    grid = GridSearchCV(estimator=i,param_grid = j, scoring = 'accuracy',cv = 10)\n    grid.fit(x_train,y_train)\n    best_accuracy = grid.best_score_\n    best_param = grid.best_params_\n    print('{} Best Accuracy : {:.2f}%'.format(i,best_accuracy*100))\n    print('Best Parameters : ',best_param)","f679d0b6":"reg = GaussianNB()\nreg.fit(x_train,y_train)","5e7b4641":"y_NB = reg.predict(x_test)","04167a23":"from sklearn.metrics import confusion_matrix , accuracy_score\ncm = confusion_matrix(y_test,y_NB)\nprint(cm)\naccuracy_score(y_test,y_NB)","e9b1c57a":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = reg , X = x_train,y = y_train , scoring = 'accuracy',cv = 10)\nprint('Accuracy of NB : {:.2f}%'.format(accuracies.mean()*100))","2b6cbce1":"xg = XGBClassifier(use_label_encoder = False,eval_metric = 'error')\nxg.fit(x_train,y_train)","3f1cd86c":"y_xg = xg.predict(x_test)","312e7c2d":"cm = confusion_matrix(y_test,y_xg)\nprint(cm)\naccuracy_score(y_test,y_xg)","cf64af76":"accu = cross_val_score(estimator = xg , X = x_train,y = y_train , scoring = 'accuracy',cv = 10)\nprint('Accuracy of XGBoost: {:.2f}%'.format(accu.mean()*100))","fe23de80":"lr = LogisticRegression(C= 0.25, random_state = 0)\nlr.fit(x_train,y_train)","02646020":"y_lr = lr.predict(x_test)","b91050f4":"cm = confusion_matrix(y_test,y_lr)\nprint(cm)\naccuracy_score(y_test,y_lr)","42c6e831":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(x_train,y_train)","9653aee2":"sv = SVC(C= 0.25, gamma= 0.1, kernel= 'rbf', random_state= 0)","79a25ac2":"dt = DecisionTreeClassifier(criterion = 'gini',random_state = 0)","6e4e75ef":"rf = RandomForestClassifier(criterion = 'entropy',n_estimators = 200,random_state = 0)","d4fb9c50":"l = []\nl.append( knn)\nl.append(sv)\nl.append(dt)\nl.append(rf)\nl.append(lr)","4a5abab5":"for i in l:\n    i.fit(x_train,y_train)\n    pr = i.predict(x_test)\n    cm = confusion_matrix(y_test,pr)\n    print(i , cm)\n    a = accuracy_score(y_test,pr)\n    print(a)","7bba45f8":"df1 = DataFrame({'Model':['LogisticRegression','KNN','svm','Naive_Bayes','DecisionTree','RandomForest','XGBoost'],'Accuracy on Test Set':['82.46%','79.87%','81.81%','79.22%','76.62%','80.51%','81.81%'],\n                'Accuracy with K-Fold':['75.89%','72.80%','76.39%','74.27%','70.70%','76.07%','75.08%']})","966ffeb2":"df1","6ae64390":"figure(figsize = (12,8))\nsns.barplot(x = 'Model',y = 'Accuracy on Test Set',data = df1)","d824b357":"figure(figsize = (12,8))\nsns.barplot(x = 'Model',y = 'Accuracy with K-Fold',data = df1)","0f15ec09":"# **Conclusion**","9f12e3b0":"***From the above DataFrame and Barplots , it is evident that Logistic regression model gives us good accuracy on this test set . But , considering the accuracy obtained using K-Fold cross validation ,  SVC model with 'rbf' kernel function might perform better with new unseen data .***","44fe9281":"# **From the above table , we can infer that there are no correlations between the independent variables . So , we do not need to drop any parameter to eliminate multicollinearity .** "}}