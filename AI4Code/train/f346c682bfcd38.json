{"cell_type":{"fafc14b4":"code","26effbf4":"code","c7e1574a":"code","387ae3de":"code","2f410c72":"code","57e0c299":"code","5a58738a":"code","4674e94c":"code","f15c4356":"code","b1891898":"code","e4f402a8":"code","66156c85":"code","6a03286a":"code","45f5ec2e":"code","58592634":"markdown","1eb2facb":"markdown","9f853df5":"markdown","e1045840":"markdown","c9b3e424":"markdown","84c068f0":"markdown"},"source":{"fafc14b4":"import gc,os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional, Multiply\nfrom scipy.signal import hilbert, chirp\nfrom scipy.fft import fft, fftfreq\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nfrom IPython.display import display\n\nfrom tqdm import tqdm\nfrom tqdm.keras import TqdmCallback\nimport matplotlib.pyplot as plt","26effbf4":"TARGET_VAR='pressure'\nDEBUG = False","c7e1574a":"train_df = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\nprint(f\"train_df: {train_df.shape}\")\n\nif DEBUG:\n    train_df = train_df[:80*1000]\n    \ntrain_idx = train_df.id.values.tolist()","387ae3de":"test_df = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nprint(f\"test_df: {test_df.shape}\")","2f410c72":"all_pressure = np.sort(train_df.pressure.unique())\nprint('The first 25 unique pressures...')\nPRESSURE_MIN = all_pressure[0].item()\nPRESSURE_MAX = all_pressure[-1].item()\nprint(all_pressure[:25])\nprint('The differences between first 25 pressures...')\nPRESSURE_STEP = ( all_pressure[1] - all_pressure[0] ).item()\nall_pressure[1:26] - all_pressure[:25]\ndel all_pressure\ngc.collect()","57e0c299":"def add_features(df):\n    df['cross']= df['u_in'] * df['u_out']\n    df['cross2']= df['time_step'] * df['u_out']\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    #df[\"u_in_first\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"first\")\n    #df[\"u_in_last\"]  = df.groupby(\"breath_id\")[\"u_in\"].transform(\"last\")\n     \n    \n    print(\"Step-1...Completed\")\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    #df['u_in_lag5'] = df.groupby('breath_id')['u_in'].shift(5)\n    #df['u_out_lag5'] = df.groupby('breath_id')['u_out'].shift(5)\n    #df['u_in_lag_back5'] = df.groupby('breath_id')['u_in'].shift(-5)\n    #df['u_out_lag_back5'] = df.groupby('breath_id')['u_out'].shift(-5)\n    #df['u_in_lag6'] = df.groupby('breath_id')['u_in'].shift(6)\n    #df['u_out_lag6'] = df.groupby('breath_id')['u_out'].shift(6)\n    #df['u_in_lag_back6'] = df.groupby('breath_id')['u_in'].shift(-6)\n    #df['u_out_lag_back6'] = df.groupby('breath_id')['u_out'].shift(-6)\n    \n    \n    \n    df = df.fillna(0)\n    print(\"Step-2...Completed\")\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_in__mean'] = df.groupby(['breath_id'])['u_in'].transform('mean')\n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    print(\"Step-3...Completed\")\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    print(\"Step-4...Completed\")\n    \n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] \/df['count']\n    \n    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n    print(\"Step-5...Completed\")\n    \n    df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['ewm_u_in_mean'] = (df\\\n                           .groupby('breath_id')['u_in']\\\n                           .ewm(halflife=9)\\\n                           .mean()\\\n                           .reset_index(level=0,drop=True))\n    df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n                                                              .groupby('breath_id')['u_in']\\\n                                                              .rolling(window=15,min_periods=1)\\\n                                                              .agg({\"15_in_sum\":\"sum\",\n                                                                    \"15_in_min\":\"min\",\n                                                                    \"15_in_max\":\"max\",\n                                                                    \"15_in_mean\":\"mean\"})\\\n                                                               .reset_index(level=0,drop=True))\n    print(\"Step-6...Completed\")\n    \n    df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n    df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n    df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n    df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n    print(\"Step-7...Completed\")\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    print(\"Step-8...Completed\")\n    \n    ffta = lambda x: np.abs(fft(np.append(x.values,x.values[0]))[:80])\n    ffta.__name__ = 'ffta'\n\n    fftw = lambda x: np.abs(fft(np.append(x.values,x.values[0])*w)[:80])\n    fftw.__name__ = 'fftw'\n\n    #df['fft_u_in'] = df.groupby('breath_id')['u_in'].transform(ffta)\n    #df['fft_u_in_w'] = df.groupby('breath_id')['u_in'].transform(fftw)\n    df['analytical'] = df.groupby('breath_id')['u_in'].transform(hilbert)\n    df['envelope'] = np.abs(df['analytical'])\n    df['phase'] = np.angle(df['analytical'])\n    df['unwrapped_phase'] = df.groupby('breath_id')['phase'].transform(np.unwrap)\n    df['phase_shift1'] = df.groupby('breath_id')['unwrapped_phase'].shift(1).astype(np.float32)\n    df['IF'] = df['unwrapped_phase'] - df['phase_shift1'].astype(np.float32)\n    \n    df.drop(['analytical','phase', 'unwrapped_phase','phase_shift1'], axis=1, inplace=True)\n\n    df = df.fillna(0)\n    print(\"Step 9 ... Add evelope and IF features\")\n    return df\n\n\nprint(\"Train data...\\n\")\ntrain = add_features(train_df)\n\nprint(\"\\nTest data...\\n\")\ntest = add_features(test_df)\n\ndel train_df\ndel test_df\ngc.collect()","5a58738a":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\n\ntrain.drop(['pressure','id', 'breath_id','one','count',\n            'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n            'breath_id_lag2same'], axis=1, inplace=True)\n\ntest = test.drop(['id', 'breath_id','one','count','breath_id_lag',\n                  'breath_id_lag2','breath_id_lagsame',\n                  'breath_id_lag2same'], axis=1)\n\nprint(f\"train: {train.shape} \\ntest: {test.shape}\")","4674e94c":"scaler = RobustScaler()\ntrain = scaler.fit_transform(train)\n# Add index column .\ntrain = np.c_[train,train_idx]\ntest = scaler.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\n# -1 to compensate for extra Index column in test data \ntest = test.reshape(-1, 80, train.shape[-1]-1)\n\nprint(f\"train: {train.shape} \\ntest: {test.shape} \\ntargets: {targets.shape}\")\nCOLS=train.shape[-1]\nCOLS","f15c4356":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 512\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")","b1891898":"# Utils\n# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    return tpu, strategy\ntpu, strategy = get_hardware_strategy()\n\ndef plot_history(history):\n    # https:\/\/machinelearningmastery.com\/display-deep-learning-model-training-history-in-keras\/\n    #print(history.history.keys()) \n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    \ndef make_submission(name,final_predictions):\n    \"\"\"Makes submission for testing\"\"\"\n    sample_submission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\n    try:\n        os.remove(f\"submission_{name}.csv\")\n    except (OSError, IOError) as e:    \n        #gulp\n        print(f\"Gulp {name}\")\n    # https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/276138    \n    preds = np.column_stack(final_predictions)\n    sample_submission[TARGET_VAR] = np.mean(preds, axis=1)\n    sample_submission.to_csv(f\"submission_mean_{name}.csv\", index=False)\n    \n    sample_submission[TARGET_VAR] = np.median(preds, axis=1)\n    sample_submission.to_csv(f\"submission_median_{name}.csv\", index=False)\n    \n    # ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n    sample_submission[TARGET_VAR] = np.round( (sample_submission[TARGET_VAR] - PRESSURE_MIN)\/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n    sample_submission[TARGET_VAR] = np.clip(sample_submission[TARGET_VAR], PRESSURE_MIN, PRESSURE_MAX)\n    sample_submission.to_csv(f\"submission.csv\", index=False)","e4f402a8":"from tensorflow.keras.layers import Input, Dense, LSTM,  Conv1D,Dropout,Bidirectional,Multiply\nfrom tensorflow.keras.models import Model\n#from attention_utils import get_activations\nfrom tensorflow.keras.layers import Multiply,Permute,Flatten,Add \nfrom tensorflow.keras.models import *\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import backend as K\n\n\nSINGLE_ATTENTION_VECTOR = False\ndef attention_3d_block(inputs):\n    # inputs.shape = (batch_size, time_steps, input_dim)\n    input_dim = int(inputs.shape[2])\n    a = inputs\n    #a = Permute((2, 1))(inputs)\n    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n    a = Dense(input_dim, activation='softmax')(a)\n    if SINGLE_ATTENTION_VECTOR:\n        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n        a = RepeatVector(input_dim)(a)\n    a_probs = Permute((1, 2), name='attention_vec')(a)\n\n    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n    output_attention_mul = Multiply()([inputs, a_probs])\n    return output_attention_mul\n\nclass Attention(Layer):\n    \n    def __init__(self, return_sequences=True):\n        self.return_sequences = return_sequences\n        super(Attention,self).__init__()\n        \n    def build(self, input_shape):\n        \n        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n                               initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n                               initializer=\"zeros\")\n        \n        super(Attention,self).build(input_shape)\n        \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'return_sequences': self.return_sequences \n        })\n        return config    \n        \n    def call(self, x):\n        \n        e = K.tanh(K.dot(x,self.W)+self.b)\n        a = K.softmax(e, axis=1)\n        output = x*a\n        \n        if self.return_sequences:\n            return output\n        \n        return K.sum(output, axis=1)\n    \ndef dnn_model():\n    \n    x_input = Input(shape=(test.shape[-2:]))\n    \n    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n    \n    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n    \n    z31 = Multiply()([x3, z2])\n    z31 = BatchNormalization()(z31)\n    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n    \n    z41 = Multiply()([x4, z3])\n    z41 = BatchNormalization()(z41)\n    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n    \n    z51 = Multiply()([x5, z4])\n    z51 = BatchNormalization()(z51)\n    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n    \n    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n    \n    x = Dense(units=128, activation='selu')(x)\n    \n    x_output = Dense(units=1)(x)\n\n    model = Model(inputs=x_input, outputs=x_output, \n                  name='DNN_Model')\n    return model\n\ndef dnn_model_2():\n    \n    x_input = Input(shape=(train.shape[-2:]))\n    \n    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n    x3 = Bidirectional(LSTM(units=256, return_sequences=True))(x2)\n    \n    z2 = Bidirectional(GRU(units=256, return_sequences=True))(x2)\n    z3 = Bidirectional(GRU(units=128, return_sequences=True))(Add()([x3, z2]))\n    \n    x = Concatenate(axis=2)([x3, z2, z3])\n    x = Bidirectional(LSTM(units=192, return_sequences=True))(x)\n    \n    x = Dense(units=128, activation='selu')(x)\n    \n    x_output = Dense(units=1)(x)\n\n    model = Model(inputs=x_input, outputs=x_output, \n                  name='DNN_Model')\n    return model\n\ndef transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n    # Normalization and Attention\n    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n    x = layers.MultiHeadAttention(\n        key_dim=head_size, num_heads=num_heads, dropout=dropout\n    )(x, x)\n    x = layers.Dropout(dropout)(x)\n    res = x + inputs\n\n    # Feed Forward Part\n    x = layers.LayerNormalization(epsilon=1e-6)(res)\n    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"selu\")(x)\n    x = layers.Dropout(dropout)(x)\n    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n    return x + res\n\ndef transformer_model(\n    input_shape=train.shape[-2:],\n    head_size=512,\n    num_heads=8,\n    ff_dim=4,\n    num_transformer_blocks=4,\n    mlp_units=[128],\n    dropout=0,\n    mlp_dropout=0,\n):\n    inputs = Input(shape=input_shape)\n    x = inputs\n    for _ in range(num_transformer_blocks):\n        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n\n    #x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n    for dim in mlp_units:\n        x = layers.Dense(dim, activation=\"selu\")(x)\n        x = layers.Dropout(mlp_dropout)(x)\n    outputs = layers.Dense(units=1, activation=\"softmax\")(x)\n    return Model(inputs, outputs)\n\n\n\ndef transformer_model_bi(\n    input_shape=train.shape[-2:],\n    head_size=256,\n    num_heads=4,\n    ff_dim=4,\n    num_transformer_blocks=8,\n    mlp_units=[128],\n    dropout=0,\n    mlp_dropout=0,\n):\n    x_input = Input(shape=(test.shape[-2:]))\n    \n    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n    \n    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n    \n    z31 = Multiply()([x3, z2])\n    z31 = Attention(return_sequences=True)(z31)\n    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n    \n    z41 = Multiply()([x4, z3])\n    z41 =Attention(return_sequences=True)(z41)\n    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n    \n    z51 = Multiply()([x5, z4])\n    z51 = Attention(return_sequences=True)(z51)\n    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n    \n    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n    \n    x = Dense(units=128, activation='selu')(x)\n    \n    x_output = Dense(units=1)(x)\n\n    model = Model(inputs=x_input, outputs=x_output, \n                  name='DNN_Model')\n    return model\n\ndef transformer_model11(\n    input_shape=train.shape[-2:],\n    head_size=256,\n    num_heads=4,\n    ff_dim=4,\n    num_transformer_blocks=1,\n    mlp_units=[128],\n    dropout=0,\n    mlp_dropout=0,\n):\n    x_input = Input(shape=(train.shape[-2:]))\n    \n    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n    x3 = Bidirectional(LSTM(units=256, return_sequences=True))(x2)\n    #x2 = Attention(return_sequences=True)(x2)\n    z2 = Bidirectional(GRU(units=256, return_sequences=True))(x2)\n    z3 = Bidirectional(GRU(units=128, return_sequences=True))(Add()([x3, z2]))\n    \n    x = Concatenate(axis=2)([x3, z2, z3])\n    x = Bidirectional(LSTM(units=192, return_sequences=True))(x)\n    #x = attention_3d_block(x)\n    #x = Flatten()(x)\n    x = Dense(units=128, activation='selu')(x)\n    \n    x_output = Dense(units=1)(x)\n\n    model = Model(inputs=x_input, outputs=x_output, \n                  name='DNN_Model')\n    return model\n\ndef _3d_to_2d(arr):\n    return arr.reshape((arr.shape[0]*arr.shape[1]),arr.shape[-1])","66156c85":"#model = dnn_model()\n#model.summary()","6a03286a":"with strategy.scope():\n    \n    VERBOSE = 0\n    SEED = 2021\n    FOLDS = 10\n    test_preds = []\n    final_valid_predictions = {}\n    scores = []\n    name = \"dlstm_2021_ftrs\"\n    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        X_train, X_valid = train[train_idx][:,:,0:COLS-1], train[test_idx][:,:,0:COLS-1]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        #checkpoint_filepath=f\"..\/input\/gb-vpp-pulp-fiction\/Bidirect_LSTM_model_2021_1{fold+1}C.h5\"\n        checkpoint_filepath=f\".\/Bidirect_LSTM_model_10_{fold+1}C.h5\"\n        checkpoint_filepath_rmt = f\"..\/input\/vpp-blend-4-new-features\/Bidirect_LSTM_model_10_{fold+1}C.h5\"\n        if os.path.exists(checkpoint_filepath_rmt):\n            model = tf.keras.models.load_model(checkpoint_filepath_rmt) \n        else:\n            model = dnn_model()\n            model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mae\"])\n\n            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.85, \n                                   patience=7, verbose=VERBOSE)\n\n            save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\n            chk_point = ModelCheckpoint(f'.\/Bidirect_LSTM_model_10_{fold+1}C.h5', options=save_locally, \n                                        monitor='val_loss', verbose=VERBOSE, \n                                        save_best_only=True, mode='min')\n\n            es = EarlyStopping(monitor=\"val_loss\", patience=30, \n                               verbose=VERBOSE, mode=\"min\", \n                               restore_best_weights=True)               \n            history = model.fit(X_train, y_train, \n                      validation_data=(X_valid, y_valid), \n                      epochs=300,\n                      verbose=VERBOSE,\n                      batch_size=BATCH_SIZE, \n                      callbacks=[lr, chk_point, es,TqdmCallback(verbose=1)])\n            if DEBUG:\n                plot_history(history)\n            del history    \n                \n        y_true = y_valid.squeeze().reshape(-1, 1)\n        y_pred = model.predict(X_valid, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)\n        valid_idx = _3d_to_2d(train[test_idx])[:,COLS-1].astype(int) \n        print(len(valid_idx),len(y_pred))\n        final_valid_predictions.update(dict(zip(valid_idx, y_pred)))\n        del valid_idx\n        gc.collect()\n        score = mean_absolute_error(y_true, y_pred)\n        scores.append(score)\n        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n        del X_train\n        del y_train\n        del X_valid\n        del y_valid\n        del y_true\n        del y_pred\n        gc.collect()\n        test_preds.append(model.predict(test, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1).squeeze())\n    print(f\"Final Score : Median {np.median(scores)}, Mean {np.mean(scores)}\")\n    final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n    final_valid_predictions.columns = [\"id\", f\"pred_{name}\"]\n    final_valid_predictions.to_csv(f\"train_{name}_pred.csv\", index=False)\n    sample_submission_temp = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\n    sample_submission_temp[TARGET_VAR] = np.median(np.column_stack(test_preds), axis=1)\n    sample_submission_temp[TARGET_VAR] = np.round( (sample_submission_temp[TARGET_VAR] - PRESSURE_MIN)\/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n    sample_submission_temp[TARGET_VAR] = np.clip(sample_submission_temp[TARGET_VAR], PRESSURE_MIN, PRESSURE_MAX)\n    sample_submission_temp.columns = [\"id\", f\"pred_{name}\"]\n    sample_submission_temp.to_csv(f\"test_{name}_pred.csv\", index=False)","45f5ec2e":"make_submission(name,test_preds) \n# Baseline Final Score : Median 0.6182050890824652, Mean 0.6254582082423709\n# 10 Folds . Score : Median 0.6107518408661121, Mean 0.6115367082590414 \n# 7 folds Median 0.6255510868051626, Mean 0.6178064096651388\n# 7 fold with attention   Median 0.6253117080984856, Mean 0.6308399315111733\n# Final Score : Median 0.6842507432512323, Mean 0.6816680787380409\n# Final Score : Median 0.6226577897596568, Mean 0.6165830909037122 \n# With new Features : Final Score : Median 0.6238020513760956, Mean 0.6362880387969907 with new features\n# With Attention Layer : Final Score : Median 0.606840550218398, Mean 0.6142074864099231\n\n","58592634":"## Keras DNN Model","1eb2facb":"## Create submission file","9f853df5":"## Load source datasets","e1045840":"## Hardware config","c9b3e424":"## Feature Engineering","84c068f0":"## Import libraries"}}