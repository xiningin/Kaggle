{"cell_type":{"8744c879":"code","2bef83e8":"code","8e2e747c":"code","736e302b":"code","a0542358":"code","a544e9d4":"code","bc2b4885":"code","7b685bd2":"code","202c2757":"code","83c2b10c":"code","20e1875f":"code","3ee2b19c":"code","755f1195":"code","38b988dd":"code","d4e05375":"code","881855f6":"code","b9d9716e":"code","de6f38ff":"code","08f5fcae":"code","cdcf3a14":"code","69793c7a":"code","617bcc09":"code","eaebbcd8":"code","8e592f50":"code","d448e22d":"code","105ba4ca":"code","1d422f0e":"code","498f2355":"code","721d185a":"code","9dae4f1b":"code","ed8d8ecd":"code","da3b7bbb":"code","7953b8ce":"code","143008b9":"code","9744216e":"code","14e644c9":"code","99376d9e":"code","d6fef033":"code","40260772":"code","e0b4938a":"code","26fb1632":"code","65a8c488":"code","f22b32fc":"code","289c0eb8":"code","8263f976":"code","b35cc211":"code","7c4e4c27":"code","d6ea3757":"code","ced784de":"code","6cb3dbed":"markdown","dc1abf30":"markdown","2eaecf3b":"markdown","9a7eea4e":"markdown","8aaef10b":"markdown","8ce47280":"markdown","f117bd79":"markdown","59024a76":"markdown","b30e6d15":"markdown","9c90fb30":"markdown","d234fb12":"markdown","7eda323e":"markdown","2653a4b9":"markdown","11a517e4":"markdown","fd17a8fe":"markdown","71a1f1d3":"markdown","96874ca3":"markdown","ffd550c2":"markdown","6f36541e":"markdown","676d01bf":"markdown","c3ed6cb2":"markdown","b7f066fa":"markdown","903d440c":"markdown","70558be4":"markdown","1bbd9b9f":"markdown","f3919393":"markdown","d100a43e":"markdown","dffc2718":"markdown","b36ab8db":"markdown","160d304c":"markdown","aac96f7d":"markdown","95905fb7":"markdown","aca4ca65":"markdown","ea786684":"markdown"},"source":{"8744c879":"import numpy as np # linear algebra\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport gc\nimport time\n%matplotlib inline\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text  import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import average_precision_score, roc_curve, roc_auc_score\nfrom sklearn import metrics","2bef83e8":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\nprint(train.shape)\nprint(test_df.shape)\nprint(test_df.head())","8e2e747c":"train.head()","736e302b":"test_df.head()","a0542358":"train=train.iloc[:,1:3]\nprint(train.head())\ny_trainAll =  np.where(train['target']>=0.5, 1, 0)\nX_trainAll=train.drop('target',axis=1)","a544e9d4":"sum(y_trainAll)\/len(y_trainAll)","bc2b4885":"print(sum(y_trainAll))\nprint(len(y_trainAll))\n(len(y_trainAll) - sum(y_trainAll))\/sum(y_trainAll)","7b685bd2":"import seaborn as sns\nsns.countplot(y_trainAll)","202c2757":"import re, string, timeit, datetime\n\ndef clean(train_clean):\n    tic = datetime.datetime.now()\n    train_clean['comment_text']=train_clean['comment_text'].str.replace('[0-9]+',' ') ### remove numbers\n    train_clean['comment_text']=train_clean['comment_text'].apply(lambda x : x.lower()) ### to lower case\n    train_clean['comment_text']=train_clean['comment_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n    train_clean['comment_text']=train_clean['comment_text'].str.replace('[0-9]',' ') ### remove numbers\n    tac = datetime.datetime.now(); time = tac - tic; print(\"To lower time\" + str(time))\n    print(\"remove punct time\" + str(time))\n    gc.collect()\n    return(train_clean)\n\n\ntrain_cl=clean(X_trainAll)\ntrain_cl.head()","83c2b10c":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True, strip_accents='unicode',  analyzer='word',\n     stop_words='english', ngram_range=(1, 2),token_pattern=r'(?u)\\b[A-Za-z]+\\b',  #erhoehen auf 2\n     max_features=50000) \n\ntfidf_train = word_vectorizer.fit_transform(train_cl['comment_text'])\nprint(word_vectorizer.get_feature_names()[:10])\nprint( len( word_vectorizer.get_feature_names() ))\n\ngc.collect()","20e1875f":"out=word_vectorizer.vocabulary_ ; list(out)[1:10]","3ee2b19c":"n_size=len(y_trainAll); print(n_size\/10)\nsub_sample = np.random.choice(range(0, n_size), size=180487, replace=False).tolist()\n#sub_sample[:20]","755f1195":"zw_df=pd.concat([train_cl, pd.DataFrame(y_trainAll)], axis=1)\nzw_df.columns=['comment_text', 'target']\n#print(zw_df.shape)\nzw_df=zw_df.iloc[sub_sample,:]\n#print(zw_df.shape)\n\ntoxic_comments = zw_df[zw_df['target'] >= .5]['comment_text'].values\ntoxic_comments = ' '.join(toxic_comments)\n\nnon_toxic_comments = zw_df[zw_df['target'] < .5]['comment_text'].values\nnon_toxic_comments = ' '.join(non_toxic_comments)\ndel zw_df, test_df, out ; gc.collect() ","38b988dd":"import nltk\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\"))\n\nfrom wordcloud import WordCloud\nwordcloud_toxic = WordCloud(max_font_size=100, max_words=100, background_color=\"white\",  stopwords=stop_words).generate(toxic_comments)\nplt.figure(figsize=[15,5])    \n# Display the generated image:\nplt.title(\"Wordcloud: Toxic comments\")\nplt.imshow(wordcloud_toxic, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\ndel wordcloud_toxic,X_trainAll  , toxic_comments,   train_cl, word_vectorizer, train  ; \ngc.collect()","d4e05375":"wordcloud_non_toxic = WordCloud(max_font_size=100, max_words=100, background_color=\"white\",  stopwords=stop_words).generate(non_toxic_comments)\nplt.figure(figsize=[15,5])\nplt.title(\"Wordcloud: Non-Toxic comments\")\nplt.imshow(wordcloud_non_toxic, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\ndel wordcloud_non_toxic,  non_toxic_comments, stop_words, \ngc.collect()","881855f6":"from sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.metrics import average_precision_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold","b9d9716e":"def train_and_predictNB(alpha, train_x,train_y, valid_x,valid_y):\n    # Instantiate the classifier: nb_classifier\n    nb_classifier = MultinomialNB(alpha)# count_train: best auc=0.769 bei alpha = 0.0 on count_train\n    nb_classifier.fit(train_x,train_y)\n    pred = nb_classifier.predict_proba(valid_x); pred=pd.DataFrame(pred); #print(pred[:3])\n    auc = roc_auc_score(valid_y, pred[1]); #print(auc);# print(pred[1]) #print('AUC: %.3 f' % auc)\n    pred = nb_classifier.predict(valid_x)\n    score = metrics.accuracy_score(valid_y, pred)\n    del nb_classifier, pred\n    return [round(score,5), round(auc,5)]","de6f38ff":"X_train_tf, X_valid_tf, y_train, y_valid = train_test_split(tfidf_train, y_trainAll, test_size = 0.2, random_state = 53)","08f5fcae":"alpha=1.2\nprint('Alpha: ', alpha)\nout=train_and_predictNB(alpha, X_train_tf, y_train, X_valid_tf, y_valid)\nprint('Accuracy: ', out[0])                             \nprint('AUC: ',out[1])","cdcf3a14":"acc_out=[]; auc_out=[]\nnfold = 5\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=123)\ni = 1\nfor train_index, valid_index in skf.split(tfidf_train, y_trainAll):\n    print(\"\\nFold {}\".format(i)); i+=1\n    print(len(train_index));print(len(valid_index))\n    out=train_and_predictNB(alpha, tfidf_train[train_index], y_trainAll[train_index], tfidf_train[valid_index], y_trainAll[valid_index])\n    print(out)\n    acc_out.append(out[0]); auc_out.append(out[1])\nprint(acc_out)   ; print(auc_out) \nprint(\"Mean-Acc: \", round(np.mean(acc_out),5) )\nprint(\"Mean-AUC: \", round(np.mean(auc_out),5) )","69793c7a":"np.random.seed(seed=234)\ni_class0 = np.where(y_trainAll == 0)[0] ; i_class1 = np.where(y_trainAll == 1)[0]\nn_class0 = len(i_class0) ; n_class1 = len(i_class1)\ni_class0_downsampled = np.random.choice(i_class0, size=n_class1, replace=False)\nds_index=np.concatenate((i_class1,i_class0_downsampled))\nprint(n_class1); print(n_class0); print(len(ds_index))\n\ny_train_ds=y_trainAll[ds_index]; tfidf_train_ds =tfidf_train[ds_index]","617bcc09":"def downsample(x_orig, y_orig):\n    np.random.seed(seed=234)\n    i_class0 = np.where(y_orig == 0)[0] ; i_class1 = np.where(y_orig == 1)[0]\n    n_class0 = len(i_class0) ; n_class1 = len(i_class1)\n    if n_class0 > n_class1:\n        i_class0_downsampled = np.random.choice(i_class0, size=n_class1, replace=False);\n        ds_index=np.concatenate((i_class1,i_class0_downsampled))\n    else: \n        i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False);\n        ds_index=np.concatenate((i_class0,i_class1_downsampled)) \n    #print(n_class1); print(n_class0); print(len(ds_index))\n\n    y_ds=y_orig[ds_index]; X_ds =x_orig[ds_index]\n    return X_ds, y_ds\n    ","eaebbcd8":"acc_out=[]; auc_out=[]\nnfold = 5\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=123)\ni = 1\nfor train_index, valid_index in skf.split(tfidf_train, y_trainAll):\n    tfidf_train_ds, y_train_ds = downsample(tfidf_train[train_index], y_trainAll[train_index])\n    out=train_and_predictNB(alpha, tfidf_train_ds, y_train_ds, tfidf_train[valid_index], y_trainAll[valid_index])\n    acc_out.append(out[0]); auc_out.append(out[1])\nprint(acc_out)   ; print(auc_out) \nprint(\"Mean-Acc: \", round(np.mean(acc_out),5) )\nprint(\"Mean-AUC: \", round(np.mean(auc_out),5) )","8e592f50":"from sklearn.linear_model import LogisticRegression","d448e22d":"def train_and_predictLogR(cl,c_weight=None):                                                      #c=0.8; l1 =0.9444   l2 ist schlechter\n    logreg = LogisticRegression(C=cl,penalty='l1',class_weight=c_weight, solver='liblinear')    #class_weight : dict or \u2018balanced\u2019, optional (default=None)\n    logreg.fit(X_train_tf, y_train)\n    pred = logreg.predict_proba(X_valid_tf);pred=pd.DataFrame(pred)\n    \n    auc = roc_auc_score(y_valid, pred[1]); print('auc: ',auc)     \n    pred = logreg.predict(X_valid_tf)\n    score = metrics.accuracy_score(y_valid, pred)\n    del logreg, pred\n    return score\n\n#print('Score: ', train_and_predictLogR(1))   \n#classos = np.arange(0.001,3,.2)\n\nclassos =[.4,.6,.8 ]\n\nfor classo in classos:\n    print('classo: ', classo)\n    print('Score: ', train_and_predictLogR(classo))                              #0.8782946199369265\n    print()","105ba4ca":"def train_and_predictLogR(c_par, train_x,train_y, valid_x,valid_y, c_weight=None):\n    logreg = LogisticRegression(C=c_par,penalty='l1', solver='liblinear' , class_weight=c_weight)   \n    logreg.fit(train_x, train_y)\n    pred = logreg.predict_proba(valid_x);pred=pd.DataFrame(pred)        \n    auc = roc_auc_score(valid_y, pred[1]); #print(auc);# print(pred[1]) #print('AUC: %.3 f' % auc)\n    pred = logreg.predict(valid_x)\n    score = metrics.accuracy_score(valid_y, pred)\n    return [round(score,5), round(auc,5)]","1d422f0e":"c_par=0.6\nstart = time.time()\nacc_out=[]; auc_out=[]   ;         \nnfold = 5\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=123)\ni = 1\nfor train_index, valid_index in skf.split(tfidf_train, y_trainAll):\n    #print(\"\\nFold {}\".format(i)); i+=1  #print(len(train_index));print(len(valid_index))\n    out=train_and_predictLogR(c_par, tfidf_train[train_index], y_trainAll[train_index], tfidf_train[valid_index], y_trainAll[valid_index])    #print(out)\n    acc_out.append(out[0]); auc_out.append(out[1])\n    \n    \nprint(acc_out)   ; print(auc_out) \nprint(\"Mean-Acc: \", round(np.mean(acc_out),5) )\nprint(\"Mean-AUC: \", round(np.mean(auc_out),5) )   ;end = time.time(); print((end - start)\/60)","498f2355":"start = time.time()\nacc_out=[]; auc_out=[]   ;       \nnfold = 5\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=123)\ni = 1\nfor train_index, valid_index in skf.split(tfidf_train, y_trainAll):\n    tfidf_train_ds, y_train_ds = downsample(tfidf_train[train_index], y_trainAll[train_index])\n    out=train_and_predictLogR(c_par, tfidf_train_ds, y_train_ds, tfidf_train[valid_index], y_trainAll[valid_index])    #print(out)\n    acc_out.append(out[0]); auc_out.append(out[1])\n    \n    \nprint(acc_out)   ; print(auc_out) \nprint(\"Mean-Acc: \", round(np.mean(acc_out),5) )\nprint(\"Mean-AUC: \", round(np.mean(auc_out),5) )   ;end = time.time(); print((end - start)\/60)","721d185a":"start = time.time()\nacc_out=[]; auc_out=[]   ;       \nnfold = 5\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=123)\ni = 1\nfor train_index, valid_index in skf.split(tfidf_train, y_trainAll):\n    out=train_and_predictLogR(c_par, tfidf_train[train_index], y_trainAll[train_index], tfidf_train[valid_index], y_trainAll[valid_index],c_weight='balanced')    #print(out)\n    acc_out.append(out[0]); auc_out.append(out[1])\n    \n    \nprint(acc_out)   ; print(auc_out) \nprint(\"Mean-Acc: \", round(np.mean(acc_out),5) )\nprint(\"Mean-AUC: \", round(np.mean(auc_out),5) )  ;end = time.time(); print((end - start)\/60)","9dae4f1b":"import time\nimport lightgbm as lgb\ntrain_data = lgb.Dataset(X_train_tf, y_train)\nvalid_data = lgb.Dataset(X_valid_tf, y_valid ) #tfidf_test, reference=train_data)\n\nparam = {\n    'num_trees':5000,   #0.942217   ;   0.94268   #0.94338  (20; 0.1);0.94316 (30,0.1);  0.9434 (25,0.1\/32min); 0.943271 (25;0.05\/55)\n    'learning_rate':0.1,\n    \"objective\": \"binary\",\n    'num_leaves':25,\n    'metric': ['auc'],\n    \"num_threads\": -1,\n    \"early_stopping_rounds\":20,\n    \"verbose\":1,\n    'boost_from_average': False,    \n}\n\nstart = time.time()\nbdt = lgb.train(param, train_data, valid_sets=[valid_data], verbose_eval=100)  \nend = time.time(); print((end - start)\/60)","ed8d8ecd":"def train_and_predictLGBM01(train_x,train_y, valid_x,valid_y, num_trees=1):                  #[1126]\tvalid_0's auc: 0.943421\n    param = {\n    'num_trees':num_trees,    'learning_rate':0.1,  \"objective\": \"binary\",  'num_leaves':25,\n    'metric': ['auc'],   \"num_threads\": -1,   # \"early_stopping_rounds\":20,\n    \"verbose\":1,'boost_from_average': False,     #'is_unbalance': True,                       \n     #'scale_pos_weight': ch_weights,                        \n     }\n    train_data = lgb.Dataset(train_x, train_y)\n    bdt = lgb.train(param, train_data,  verbose_eval=500) \n    pred = bdt.predict(valid_x)  ;         \n    auc = roc_auc_score(valid_y, pred); #print(auc);# print(pred[1]) #print('AUC: %.3 f' % auc)\n    pred_dichotom=np.where(pred >=0.5, 1, 0); pred=pd.DataFrame(pred)\n    #pred = bdt.predict(valid_x)\n    score = metrics.accuracy_score(valid_y, pred_dichotom)\n    return [round(score,5), round(auc,5)]","da3b7bbb":"#train_and_predictLGBM01(X_train_tf, y_train, X_valid_tf, y_valid , num_trees=120)#\n","7953b8ce":"del train_data, X_train_tf, X_valid_tf,bdt,  valid_data, out, tfidf_train, y_trainAll\ngc.collect()","143008b9":"for name in dir():\n    if not name.startswith('_'):\n        del globals()[name]\n\nfor name in dir():\n    if not name.startswith('_'):\n        del locals()[name]\n\n%reset -f        \nimport gc        \ngc.collect()        ","9744216e":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\ntqdm.pandas()\nimport time\n\nfrom keras.preprocessing import text, sequence\nfrom keras import backend as K\nfrom keras.models import load_model\nimport keras\nimport pickle\nfrom sklearn.model_selection import train_test_split","14e644c9":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')","99376d9e":"def clean_text(x):\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    return x\n\ntrain[\"comment_text\"] = train[\"comment_text\"].progress_apply(lambda x: clean_text(x))","d6fef033":"train_data = train[\"comment_text\"]\nlabel_data = train.target.apply(lambda x: 0 if x < 0.5 else 1)\ntrain_data.shape, label_data.shape","40260772":"MAX_LEN = 200\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'\n\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(train_data) )\n\ntrain_data = tokenizer.texts_to_sequences(train_data)\ntrain_data = sequence.pad_sequences(train_data, maxlen=MAX_LEN)","e0b4938a":"x_train, x_val, y_train, y_val = train_test_split(train_data, label_data, test_size = 0.35, random_state = 53)","26fb1632":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n\n","65a8c488":"EMBEDDING_FILES = [ '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec']\n\nstart = time.time()\n\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n\nend = time.time(); elapsed = end - start; print(elapsed\/60)\ngc.collect()\nembedding_matrix.shape","f22b32fc":"from keras.models import Sequential, Model\nfrom keras.optimizers import  Adam\nfrom keras.layers import Flatten, Dense, Embedding, Dropout, Bidirectional, Input, add #,  CuDNNLSTM,\nfrom keras.layers import concatenate,  SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, LSTM, CuDNNLSTM\nfrom keras.utils import plot_model\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.metrics import roc_auc_score\nimport tensorflow as tf\nimport timeit\n\ndef auroc(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)","289c0eb8":"n_layers=64\n\ndef build_model(embedding_matrix):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=True)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = CuDNNLSTM(n_layers, return_sequences=True)(x)\n    x = CuDNNLSTM(n_layers, return_sequences=True)(x)\n    x = GlobalMaxPooling1D()(x)\n    \n\n    x = Dense(n_layers, activation='relu')(x)\n    x = Dense(64, activation='relu')(x)\n    result = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=words, outputs=[result])\n    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[\"accuracy\",auroc])\n    return model\n\n","8263f976":"#dir()","b35cc211":"del tokenizer, train, train_data, label_data\ngc.collect()","7c4e4c27":"start = time.time()\nmodel = build_model(embedding_matrix)\nhistory = model.fit(x_train, y_train,\n                    epochs=6,\n                    batch_size=1024,\n                    validation_data=(x_val, y_val))\n\nend = time.time(); elapsed = end - start; print(elapsed\/60)","d6ea3757":"def plot_accuracy(acc,val_acc):\n  # Plot training & validation accuracy values\n  plt.figure()\n  plt.plot(acc)\n  plt.plot(val_acc)\n  plt.title('Model accuracy')\n  plt.ylabel('Accuracy')\n  plt.xlabel('Epoch')\n  plt.legend(['Train', 'Test'], loc='upper left')\n  plt.show()\n\ndef plot_loss(loss,val_loss):\n  plt.figure()\n  plt.plot(loss)\n  plt.plot(val_loss)\n  plt.title('Model loss')\n  plt.ylabel('Loss')\n  plt.xlabel('Epoch')\n  plt.legend(['Train', 'Test'], loc='upper right')\n  plt.show()\n\ndef plot_auc(auroc,val_auroc):\n  plt.figure()\n  plt.plot(auroc)\n  plt.plot(val_auroc)\n  plt.title('Model AUC')\n  plt.ylabel('AUC')\n  plt.xlabel('Epoch')\n  plt.legend(['Train', 'Test'], loc='upper right')\n  plt.show()","ced784de":"plot_loss(history.history['loss'], history.history['val_loss'])\nplot_accuracy(history.history['acc'], history.history['val_acc'])\nplot_auc(history.history['auroc'], history.history['val_auroc'])","6cb3dbed":"# Final Thoughts <a class=\"anchor\" id=\"final\"><\/a>\n\nIn this notebook I compared several models for class-prediction on the jigsaw text data. In particular, I used the Naive Bayes model, the logistic regression model with regularization capabilities, the boosting model light-GBM, and a small neural network employing recurrent neural network layers. As performance measures I mainly considered the area-under-the-curve AUC , but also evaluated the corresponding accuracy. The data exhibit class-imbalance with approximately 11 times more none-toxic comments in the data-set than toxic messages. \n\nTo get a reliable estimate of the model performance and how it may generalize to new data I employed cross-validation using five folds. I additionally considered some techniques to deal with class-imbalances: downsampling and\/ or re-weighting. Results in mean-AUC showed often some improvements although they were often small relative to the underlying variation.\n\nNaive-Bayes models achieved the lowest performance with a mean-AUC of around 0.876. The logistic regression model employing (penalty based on) a L1-regularization showed a fairly large improvement with a mean-AUC of 0.943. Application of the light-GBM exhibited similar results with a mean-AUC of 0.942. A considerable improvement could then be achieved by using a small recurrent neural network with two LSTM- layers with an approximate performance of 0.961. Due to restrictions of notebook execution-time especially computationally expensive models had to be chosen rather small. \n\nThe performance measure used for the competition was an extention of the AUC taking into account performance over different classes. For some of the models considered here,  options to adjust to this extended measure may be by adjusting the sample weights or loss functions accordingly.\n\nFinally, there were several kernels I  benefited from and I want to express my gratitude. Some of those will be listed below (although there were many more).\n","dc1abf30":"### Cross-Validation","2eaecf3b":"# Gradient Boosting (LightGBM) <a class=\"anchor\" id=\"XBR\"><\/a>\n\nA class of machine-learning algorithms often applied are tree-based models. Two frequent choices are random forests and gradient-boosting models. Due to their good performance the second choice, boosting models, have become very popular. In the following I apply the light-GBM to the text data. \n\nTo find an appropriate model I allow for a high number of trees while at the same time applying 'early-stopping' which will stop further estimation if no improvement is made.","9a7eea4e":"For the current data I will use a small neural network. It includes the embedding-matrix, two LSTM-layers and two standard, dense layers. As there are time-computation restrictions for this nodebook I only use 64 nodes per layer.","8aaef10b":"So training data contain approximately 18 times more instances than the test set and, apart from an identifier, the test-set only contains the comment-text. In the following we consider the first lines of each of the two data sets","8ce47280":"# Model: Naive Bayes <a class=\"anchor\" id=\"NB\"><\/a>\n\nThe naive Bayes model is a popular model in machine learning frequently applied to text-mining applications. It is a simple probablistic classifier derived from the classic Bayesian theorem, where for certain dependence structures the simplifying assumption of independence was made (therefore naive). ","f117bd79":"For making many machine learning algorithms applicable to text data, text data have to be transormed into something these algorithms can operate on. A common way is to create a column for each particular word occuring in the texts; then for a given line the column-cell of a particular word is given the number equivalent to the number of times that particular word appears in the comment text while for column-cells of words not occuring in that text-line the value 0 is assigned. \n\nA variation of that concept is known as tf-idf (term-frequency inverse-document frequency). Here again for every word (included in the text analysis) a separate column is created. The term-frequency referes to the relative frequency of a partiuclar word in the text-line. This value is adjusted by the inverse-document frequency, where the term of adjustment is based on the inverse of the frequency the particular word occurs over all text-lines (in effect, this should down-weight words such as 'is' or 'the' occuring in very many documents which are therefore unlikely to help in distinguishing between the two classes). \n\nIn the following the tf-idf technique is applied; additionally to considering only single words also sequences of two words (2-grams) are considered. ","59024a76":"### Results\n\nThe results from 5-fold cross-validation show the logistic regression model to considerably improve prediction results with respect to the previously considered Naive Bayes approach. \n\nNo further improvement was reached applying downsampling or to apply different weights according to class-membership using (exact) class-rebalancing weights. These techniques, however, do show again a reduction in accuracy.\n\n|Scheme|AUC|ACC|\n|---|---|---|\n|5-CV |0.94339|0.94725\n|(Downsample) 5-CV |0.94154|0.89766\n| (balanced weights) 5-CV|0.94190|0.89964 \n","b30e6d15":"The training set therefore contains an identifying column (id), the comment made by the contributer with the corresponding text stored in the column 'comment_text', and the column 'target' representing a probability of how likely this comment is considered to be toxic. There are additional columns with ratings on how toxic this comment was considered by some human raters, as well as a rating of the identity and type of insult of the receiver of the message.The columns of the test set:","9c90fb30":"# Comparison of different Models on text data\n\nIn this notebook several machine learning algorithms are applied to the jicksaw text data and their performance in terms of AUC (area under the curve) is compared; additionally model accuracy is evaluated (while the extended auc used in the competition  is not considered). Models considered are naive-Bayes, logistic regression, light-GBM and a neural-network and their performance is evaluated using cross-validation. An aspect considered here is the inbalance of class frequencies. While for most algorithms in this notebook a feature matrix based on weights of particular words of the comment-text is employed (TF-idf), the neural-network approach is based on pre-trained word-embeddings.\n\n### Contents\n\n* [Introduction](#intro)\n* [The Data](#data)\n  * First exploratory analysis; Data Preparation; TF-idf\n* [Model: Naive Bayes](#NB)\n  * Model estimation; Cross-validation; Down-sampling\n* [Model: Logistic Regression](#LR)\n  * Cross-Validation; Downsampling; Rebalancing class-weights\n* [Model: Gradient Boosting (XBR)](#XBR)\n  * Cross-Validation; Rebalancing class-weights\n* [Model: Neural-Network (LTSM)](#LTSM)\n  * Cross-Validation; Rebalancing class-weights\n* [Final Thoughts](#final)\n\n_(author: T.Payer)_","d234fb12":"For making predictions I round all target-values with a toxicity-probabilty of 50% or more up to one, while values below are rounded to 0.\n","7eda323e":"The above model was then  estimated  using 5-fold cross-validation. For time-restrictions of the execution of this notebook this has been carried out  separately in a previous version. The model has then also been estimated using an internal balancing scheme of the algorithm, as well as some manual re-weighting of the toxic class by the factor 3. Results are shown in the subsequent talbe. With respect to mean-AUC for standard cross-validation we obtain a value of 0.9417. There is some indication that by re-weighting some slight improvements can be achieved.  \n\n|Light GBM |\tAUC |\tACC|\n|---|---|---|\n|5-CV \t|0.9417 |\t0.9474|\n|balanced 5-CV |\t0.9418| \t0.9028|\n|3 x pos-weight| \t0.9422 |\t0.9420| ","2653a4b9":"\n**EDA**\n\nhttps:\/\/www.kaggle.com\/kabure\/simple-eda-hard-views-w-easy-code\n\nhttps:\/\/www.kaggle.com\/ekhtiar\/unintended-eda-with-tutorial-notes\n\nhttps:\/\/www.kaggle.com\/nz0722\/simple-eda-text-preprocessing-jigsaw\n\nhttps:\/\/www.kaggle.com\/s7anmerk\/lean-import-to-save-ram-and-eda\n\n\n**Embeddings**\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part1-eda\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part2-usage\n\n\n**LTSM**\n\nhttps:\/\/www.kaggle.com\/thousandvoices\/simple-lstm?scriptVersionId=12514554\n\n\n\n**further refs**\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n\nhttps:\/\/fasttext.cc\/docs\/en\/english-vectors.html\n\nhttp:\/\/www.tfidf.com\/\n","11a517e4":"The graphs indicate that 2-epochs form a reasonable choice.\n\n### Cross-Validation and Re-Weighting\n\nFor the Neural-Network presented above I  previously carried out 5-fold cross-validation using GPU (see kernel-version 2). Results are presented in the table below: \n\n|Structure| \tAUC| \tACC|\n|---|---|---|\n|5-CV| \t0.9605| \t0.9512|\n|3\/1 - upweight 5-CV| \t0.9612| \t0.9381|\n\n\nThe table shows a mean-AUC estimate of 0.9605 which is a\nconsiderable improvement compared to previous models. \n\nApplying some re-weighting of the lower represented toxic comments by a factor of 3 indicates some slight improvement of the AUC estimate.","fd17a8fe":"So we see that there are approximately 11.5 times more non-toxic comments (labelled with 0) than there are toxic comments (labelled with 1). The graph below illustrates that difference.","71a1f1d3":"# Neural-Network (LTSM) <a class=\"anchor\" id=\"LTSM\"><\/a>\n\nIn the following I consider a neural network. The main concept of 'what' will be analysed is somewhat different. The previous methods have mainly considered if a particular word (or a particular sequence of words, n-grams) is present in the comment-text. It was then the combination of words present in the comment-text according to which cases where classified. \n\nIn the current situation sentences and their structura (apart from some cleaning) are maintained in the analysis. This allows that predictions may also take into account the structure of the sentence. In fact, words within a sentence are mapped into a d-dimensional space by the use of so-called embeddings; thus the new d-dimensional word-vectors, at least to a certain degree, represent 'meaning' rather than a partular word. The whole procedure can be further improved by using embeddings which have prviously already been trained on large amounts of text, for example on webpages of Wikipedia. This is the approach used subsequently.","96874ca3":"# Logistic Regression <a class=\"anchor\" id=\"LR\"><\/a>\n\nLogistic regression is a common binary classification model in which a linear combination of input-variables is appropriately transformed to the probability of a binary event. The version implemented in Python (sklearn) is an extension of the classic model in which a penalty-term is added to the objective function to constrain or regularize coefficient estimates; depending on the type of regularisation imposed, measuring size of estimates in absolute or quadratic terms, one refers to L1 or L2 regularisation. Application of this regularisation is very useful if the number of variables (features) is large relative to the number of observations.","ffd550c2":"Subsequently we see plots of different performance measures vs the corresponding epochs:","6f36541e":"### Preparation of the data\n\nBefore analysing the data some preliminary cleaning is applied to remove punctuation and numbers and transform all words to lower case.","676d01bf":"### Downsampling","c3ed6cb2":"for repeated application I turn this into a function","b7f066fa":"Some preliminary analysis have shown that parameter alpha of 1.2 is a reasonable choice. I will thus use this parameter value in the subsequent.","903d440c":"Many of the most important words in both classes appear to be similar. So from the word clouds classification may not appear imediately obvious.\n\nTo get more insight into the data further EDA would be useful. There are already many very good notebook-kernels treating EDA  in depth for this data-set; I will reference a few of them at the end of this noteboook. In the following I will continue with considering different prediction models","70558be4":"**Creation of a word-cloud**\n\nIn the following I will create a word-cloud for toxic and another one for non-toxic comments. Each of this clouds holds the most important words among its comments. To avoid memory  problems I will only consider a subset of 10% of all comments.","1bbd9b9f":"### Results\n\nThe subsequent table shows results of the 5-fold cross-validation estimations carried out above. The estimate of the (mean-) AUC indicates some slight improvement over the standard version (although improvement seems not to be large relative to variation). More striking is the difference in mean accuracy between the two different approaches.\n\n|Scheme|AUC|ACC|\n|---|---|---|\n|5-CV|0.87622|0.92784\n|5-CV (Downsample)|0.87647|0.75082","f3919393":"Tokenization and transforming the sentences; we allow a maximal length of sequences to have 200 words.","d100a43e":"# Introduction<a class=\"anchor\" id=\"first\"><\/a>\n\n# The Data <a class=\"anchor\" id=\"data\"><\/a>\n### First exploratory analysis","dffc2718":"In the following we will load the training data and the test data. The training data are used to train prediction models. Test data are only considered here to know the structure of information used to make future prediction (which is the comment text only). ","b36ab8db":"### Downsampling\n\nA common situation of concern is when data-sets are inbalanced, that is one group (such as non-toxic comments) are by far more present in the data-set than another group (such as toxic comments). Depending on the choice of perfomance measure estimation results may not be desirable.\n\nThere are different strategies to handle this situation. Among those are up-sampling and down-sampling. In downsampling only a (a randomly sampled) subgroup of the majority group's samples is included in the data for training the model, while upsampling includes resampling from the minority group to achieve a balanced training data-set. In the following I use the downsampling approach.","160d304c":"For a first analysis I will split the data-set into training and validation","aac96f7d":"As can be seen from the test set, the models we train for predictions (apart from an identifying column) contain only text for making predictions (and no further variables). I will therefore  focus on the text columns to make predictions and model comparisons. As such I extract from training data the target and the text column:","95905fb7":"We create functions to load embeddings and build the embedding matrix","aca4ca65":"In the following we carry out some basic cleaning of the comment-texts: some punctuation and special characters.","ea786684":"### Class-Weighted Approach\n\nAnother option to face class-inbalances is by adjusting the weights of each instance according to the class it belongs to. The logistic regression function of the sklearn-package offers such an option. This can be achieved by setting the 'class_weight' argument to _balanced_. Here the sklearn-documentation states  that \"The \u201cbalanced\u201d mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data\"."}}