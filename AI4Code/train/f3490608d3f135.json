{"cell_type":{"2232b243":"code","d0e5bcc0":"code","9dfe9b15":"code","2f67fb6c":"code","fb0ebd2a":"code","188653ba":"code","5df60f16":"code","ff7bbc74":"code","f665715a":"code","2257a1db":"code","cbdd1acc":"code","e70d13d2":"code","b182a9a8":"code","6aafe6b2":"code","a18cd657":"code","1a09fbb2":"code","1c61d69e":"code","a1e02c21":"code","84c7a9a0":"code","9cbdc723":"code","fb9eed33":"markdown","666e8592":"markdown","df70b0eb":"markdown","acfc53cf":"markdown","45700940":"markdown","6acbf13e":"markdown","582dfc79":"markdown","c09de60d":"markdown","5d70a084":"markdown","6ec93a7f":"markdown","158d2f10":"markdown","36fed9b3":"markdown","dff5ec9d":"markdown","fd65eba7":"markdown","4284d56f":"markdown","069ba9be":"markdown","c5c549e7":"markdown","9ed5a69e":"markdown","cc3d4ba0":"markdown","fce729cd":"markdown"},"source":{"2232b243":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import RobustScaler\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom sklearn.linear_model import ElasticNet, Ridge\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nfrom IPython.display import display, HTML\nimport warnings\nwarnings.filterwarnings('ignore')","d0e5bcc0":"display(HTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    }\n\"\"\"))","9dfe9b15":"#reading the train and test data\ntest = pd.read_csv('..\/input\/test.csv')\ntrain = pd.read_csv('..\/input\/train.csv')","2f67fb6c":"#plotting the salesprice distribution\nax1 = sns.distplot(train['SalePrice'])\nax1.set_title('Distplot Normal Distribution')\n\n#Creating QQ - plot\nfig = plt.figure()\nsx1 = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n\n#applying np.log to distribution\ntrain['SalePrice'] =np.log1p(train['SalePrice'])\n\n#New salesprice distribution(After np.log)\nax2 = sns.distplot(train['SalePrice'])\nax2.set_title('Distplot after log transformation')\n\n#New QQ plot Distribution(After np.log)\nfig = plt.figure()\nsx2 = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","fb0ebd2a":"#Creating y-train data\ny_train = train.SalePrice.values","188653ba":"#combining train and test data\ndf = pd.concat([train,test], sort=False)","5df60f16":"#Getting datatypes for each category\ndf.dtypes\n\n#classifying numerical and categorical features\nnumFeats = df.dtypes[df.dtypes != \"object\"].index\ncatFeats = df.dtypes[df.dtypes == \"object\"].index\nprint(numFeats)\nprint(catFeats)","ff7bbc74":"#log transform skewed numeric features:\nskewed_feats = train[numFeats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\ndf[skewed_feats] = np.log1p(df[skewed_feats])","f665715a":"#Counts percentage of null values for each column\nfor x in df:\n    if round(df[x].isna().mean() * 100, 2) > 0:\n        print(x,  round(df[x].isna().mean() * 100, 2),'%' )","2257a1db":"#Numerical value imputation\nfor x in df[numFeats]:\n    df[x].fillna((round(df[x].mean(),0)), inplace=True)\n    \n#MANUAL CATEGORICAL IMPUTATION\n#Most Frequent Value Imputation\ndf[ 'BsmtCond' ].fillna('TA' , inplace = True)\ndf[ 'BsmtExposure' ].fillna( 'No', inplace = True)\ndf[ 'Electrical' ].fillna('SBrkr' , inplace = True)\ndf[ 'Exterior1st' ].fillna('VinylSd' , inplace = True)\ndf[ 'Exterior2nd' ].fillna('VinylSd' , inplace = True)\ndf[ 'ExterCond' ].fillna('TA' , inplace = True)\ndf[ 'ExterQual' ].fillna('TA' , inplace = True)\ndf[ 'Functional' ].fillna('Typ' , inplace = True)\ndf[ 'KitchenQual' ].fillna('TA' , inplace = True)\ndf['MSZoning'].fillna('RL', inplace = True)\ndf[ 'SaleType' ].fillna( 'WD', inplace = True)\n\n\n#####New Category 'None' Imputation\ndf[ 'Alley' ].fillna('None' , inplace = True)\ndf[ 'BsmtFinType1' ].fillna( 'None', inplace = True)\ndf[ 'BsmtFinType2' ].fillna( 'None', inplace = True)\ndf[ 'BsmtQual' ].fillna('None' , inplace = True)\ndf[ 'Fence' ].fillna('None' , inplace = True)\ndf[ 'FireplaceQu' ].fillna('None' , inplace = True)\ndf[ 'Foundation' ].fillna('None' , inplace = True)\ndf[ 'GarageCond' ].fillna( 'None', inplace = True)\ndf[ 'GarageFinish' ].fillna( 'None', inplace = True)\ndf[ 'GarageQual' ].fillna( 'None', inplace = True)\ndf[ 'GarageType' ].fillna('None' , inplace = True)\ndf[ 'MasVnrType' ].fillna('None' , inplace = True)\ndf[ 'MiscFeature' ].fillna( 'None', inplace = True)\ndf[ 'PoolQC' ].fillna('None' , inplace = True)","cbdd1acc":"# Heatmap for correlations between variables\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);\n\n# Heatmap for most correlated variables\ncorrmat = df.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.35]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(df[top_corr_features].corr(),annot=True)","e70d13d2":"# Box plot for categorical variables\nli_cat_feats = ['Alley','LotShape','Exterior1st','Exterior2nd','BsmtFinType1', 'BsmtFinType2', 'LandContour', 'Neighborhood', 'Condition1', 'Condition2',\n      'HouseStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'Heating',\n      'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual','FireplaceQu', 'GarageType',\n      'GarageFinish', 'GarageQual','PavedDrive','SaleType', 'SaleCondition']\ntarget = 'SalePrice'\nnr_rows = 7\nnr_cols = 4\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(x=li_cat_feats[i], y=target, data=df, ax = axs[r][c])\nplt.tight_layout()    \nplt.show()","b182a9a8":"# Scatter Plot for numerical variables\nli_num_feats = ['3SsnPorch','LotFrontage', 'BedroomAbvGr','LotArea', 'OverallQual','YearBuilt', 'MasVnrArea', 'BsmtFinSF1','BsmtFinSF2',\n        'TotalBsmtSF','BsmtFullBath','BsmtHalfBath','BsmtUnfSF','GrLivArea', 'FullBath','Fireplaces', 'GarageArea', 'WoodDeckSF','OpenPorchSF', 'OverallCond','TotRmsAbvGrd','WoodDeckSF', 'PoolArea']   \ntarget = 'SalePrice'\nnr_rows = 6\nnr_cols = 4\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_num_feats):\n            sns.regplot(x=li_num_feats[i], y=target, data=df,  ax = axs[r][c])\n    \nplt.tight_layout()    \nplt.show()","6aafe6b2":"X = df[li_num_feats+li_cat_feats]","a18cd657":"X = pd.get_dummies(X, drop_first =True)","1a09fbb2":"#Split train and test data\nX_train = X.iloc[:1460, :]\nX_test = X.iloc[1460:, :]","1c61d69e":"sc=RobustScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","a1e02c21":"ridge = Ridge()\nridge.fit(X_train, y_train)\nridge_pred = ridge.predict(X_test)\nridge_pred = np.expm1(ridge_pred)","84c7a9a0":"ENet =ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\nENet.fit(X_train, y_train)\nENet_pred = ENet.predict(X_test)\nENet_pred = np.expm1(ENet_pred)","9cbdc723":"final_model = ((ridge_pred)*0.30 + (ENet_pred)*0.7)","fb9eed33":"The dataframe is seperated into train and test data.","666e8592":"Create the ElasticNet model Regressor and fit it to the training data.","df70b0eb":"We created a seperate variable to save the SalePrice data to train the model","acfc53cf":"## EDSA REGRESSION TEAM 8 ##\n\n","45700940":"Select the weighted averages to provide the Optimized final model. ","6acbf13e":"Create the Ridge model Regressor and fit it to the training data.","582dfc79":"A new dataframe is created with the selected columns","c09de60d":"We imported the data into a Train and Test dataframe. ","5d70a084":"Analysing the SalePrice data, we noticed that the data was Right-Skewed. This would make it harder to develop a reliable prediction model. We apply log transformation to normalize the distribution.","6ec93a7f":"ATTENDING TO MISSING VALUES\n\nWe analysed the data in the column to determine the amount of missing data in both sets. ","158d2f10":"We split the columns into categorical and numerical features based on the tyoe of data in the column.","36fed9b3":"Together with the data_description document provided in the data set, we analysed each feature for the most likely variable to enter into the NaN items. Missing data did not necessarily indicate that the information was not provided, in some cases, such as the PoolQC, it can indicate that that particular house does not have that feature, which will have to be filled with a new category, 'None'. Other features were more suited to be filled by the most common value in the case of categorical features and the mean in the case of the numerical variables. ","dff5ec9d":"We selected the Robust Scaler to scale the features.","fd65eba7":"The correlation graphs provide us with the features that are most likely to affect the final sale price, as well as features that will affect each other. Based on the Correlation graphs, the Box Plot for Categorical Variables and the RegPlots for the Numerical Variables, we selected columns we felt were important and influential to our final regression model.","4284d56f":"The Kaggle dataset below provides us with information on how to predict a house price based on a number of features. This data set was analyzed and modeled using regression methods in order to develop a procedure that used to accurately predict the selling price of a house. The model we used is a weighted combination of the ElasticNet and Ridge regression models. This provided our model with accuracy and stability in order to be consistent accross all future datasets.","069ba9be":"For the data exploration and imputation, we decided to combine the train and test data. This would allow for consistency on data handling and further manipulation","c5c549e7":"Fistly we importing all the relevant libraries,including the regression models that we will be using. Numpy, Pandas and SciPy are the libraries used for mathematical, statistical and dataframe manipulation. Pyplot and Seaborn allow us to generate the necessary plot \n\nAs stated below we will be using Elasticnet and Ridge regressors. ","9ed5a69e":"# House Price Prediciton Model #","cc3d4ba0":"The numerical variables showed that some of the numerical variables also showed a skewness to some degree. We applied the log transformation to the columns that showed a skewness of 75% or more.","fce729cd":"Dummy variables are generated to seperate the categorical variables. "}}