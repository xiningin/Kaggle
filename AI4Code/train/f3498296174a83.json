{"cell_type":{"8c88f73f":"code","01db684f":"code","09c0cab0":"code","46f9c189":"code","02ff40a1":"code","e8ce62c6":"code","511da50a":"code","e420bf58":"code","be3951bf":"code","ab645649":"code","60c35dff":"code","d2cdee18":"code","a09d9b7a":"code","20f75b39":"code","9bba596c":"code","573a0b6e":"code","92e5c3f6":"code","3e0ad79b":"code","97874b9d":"code","19b1e25c":"code","665f37b9":"code","9e8a0a22":"code","ecf2c3e7":"code","70977ae5":"code","fed27cd5":"code","a656e660":"code","7b38c400":"code","b77da8b9":"code","3b132bc2":"code","2fe77d6d":"code","a480b72b":"code","2141b21d":"code","1d132e8a":"code","b61d1443":"code","99516497":"code","67df37ce":"code","72fa3ee5":"code","e4852658":"code","eca335ad":"code","57c8bc17":"code","3e3ed2d8":"code","c14f0955":"code","b4ad171b":"code","fd890d05":"code","4c55a919":"code","1b2301f5":"code","ab8a72ab":"code","83f7e4d0":"markdown","be574d8f":"markdown","6c2c68d9":"markdown"},"source":{"8c88f73f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# u! Hope this notebook can be helpful for getting a better understanding about the data.\n\nimport os\n\nfrom PIL import Image, ImageDraw\nimport cv2\nimport re\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\nfrom IPython.display import Video, display\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01db684f":"dataset = {\n    'root_dir': '..\/input\/tensorflow-great-barrier-reef',\n    'train_csv': '..\/input\/tensorflow-great-barrier-reef\/train.csv',\n    'test_csv': '..\/input\/tensorflow-great-barrier-reef\/test.csv',\n    'sample_submission_csv': '..\/input\/tensorflow-great-barrier-reef\/example_sample_submission.csv',\n    'video_img_dir': '..\/input\/tensorflow-great-barrier-reef\/train_images'\n}","09c0cab0":"train_csv = pd.read_csv(dataset['train_csv'])\ntest_csv = pd.read_csv(dataset['test_csv'])","46f9c189":"train_csv.head()","02ff40a1":"print(\"number of frames:\", len(train_csv))","e8ce62c6":"frame_counts = train_csv['video_id'].value_counts().sort_values().to_frame()\nframe_counts.head()","511da50a":"print(\"number of records in video_0 matched: \", frame_counts.loc[0]['video_id'] == len(os.listdir(os.path.join(dataset['video_img_dir'], 'video_0'))))\nprint(\"number of records in video_0 matched: \", frame_counts.loc[1]['video_id'] == len(os.listdir(os.path.join(dataset['video_img_dir'], 'video_1'))))\nprint(\"number of records in video_0 matched: \", frame_counts.loc[2]['video_id'] == len(os.listdir(os.path.join(dataset['video_img_dir'], 'video_2'))))","e420bf58":"sequence_counts = train_csv['sequence'].value_counts().sort_values().reset_index()\nsequence_counts.columns = [['sequence', 'num_frames']]\nprint(\"number of sequences:\", len(sequence_counts))\nsequence_counts.head()","be3951bf":"test_csv","ab645649":"num_no_obj_frame = train_csv[train_csv.annotations == '[]']['annotations'].count()\nprint(\"number of frames without objects:\", num_no_obj_frame)","60c35dff":"num_with_obj_frame = train_csv[train_csv.annotations != '[]']['annotations'].count()\nprint(\"number of frames with objects:\", num_with_obj_frame)","d2cdee18":"train_csv[train_csv.annotations != '[]'].head()","a09d9b7a":"print('ratio of frames with objects:', num_with_obj_frame \/ len(train_csv))\n\nfig, axes = plt.subplots(1,1, figsize=(12, 6))\n\nsns.barplot(ax=axes, x=['Number of Frames with Objects', 'Number of Frames with No Objects'], y=[num_with_obj_frame, num_no_obj_frame])\naxes.set_title(\"Distribution of Frames with\/without Objects\")\naxes.set_xlabel(\"Frame Types\")\naxes.set_ylabel(\"Count\")\n\nplt.show()","20f75b39":"def decode_annotation(annot_line):\n    # annot_line example: [{'x': 540, 'y': 310, 'width': 113, 'height': 105}, {'x': 657, 'y': 501, 'width': 95, 'height': 56}]\n    boxes = []\n    \n    box_pattern = r'\\{\\'\\w\\'\\:\\s\\d+\\,\\s\\'\\w\\'\\:\\s\\d+\\,\\s\\'\\w+\\'\\:\\s\\d+\\,\\s\\'\\w+\\'\\:\\s\\d+\\}'\n    val_pattern = r'\\d+'\n    \n    annotations = re.findall(box_pattern, annot_line)\n    for annot in annotations:\n        x, y, width, height = re.findall(val_pattern, annot)\n        x, y, width, height = float(x), float(y), float(width), float(height)\n        confidence = 1.0\n        \n        box = [x, y, width, height, confidence]\n        boxes.append(box)\n        \n    return boxes\n\ndef count_boxes(annot_line):\n    \n    annot_line  = annot_line[1:-1]\n    box_pattern = r'\\{\\'\\w\\'\\:\\s\\d+\\,\\s\\'\\w\\'\\:\\s\\d+\\,\\s\\'\\w+\\'\\:\\s\\d+\\,\\s\\'\\w+\\'\\:\\s\\d+\\}'\n    val_pattern = r'\\d+'\n    \n    annotations = re.findall(box_pattern, annot_line)\n    \n    return len(annotations)\n\n\ndef test_decode_annotation(annot_line):\n    print(\"sample:\", annot_line)\n    boxes = decode_annotation(annot_line)\n    for i, box in enumerate(boxes):\n        print(f\"box {i}:\", box)","9bba596c":"test_samples = [\n    \"[{'x': 540, 'y': 310, 'width': 113, 'height': 105}, {'x': 657, 'y': 501, 'width': 95, 'height': 56}, {'x': 257, 'y': 101, 'width': 42, 'height': 59}]\",\n    \"[{'x': 540, 'y': 310, 'width': 113, 'height': 105}, {'x': 657, 'y': 501, 'width': 95, 'height': 59}]\",\n    \"[{'x': 12, 'y': 250, 'width': 143, 'height': 82}]\",\n    \"[]\"\n]\n\nfor i, sample in enumerate(test_samples):\n    num_boxes = count_boxes(sample)\n    print(f\"Test {i+1}:\", f\"found {num_boxes} boxes\")\n    \n    test_decode_annotation(sample)\n    print(\"\")","573a0b6e":"train_csv['num_boxes'] = train_csv['annotations'].apply(count_boxes)\n","92e5c3f6":"train_csv[train_csv.annotations != '[]'].head()","3e0ad79b":"boxes_dist = train_csv[train_csv.annotations != '[]']['num_boxes'].value_counts().sort_values(ascending=False).reset_index()\nboxes_dist.columns = ['num_boxes', 'num_frames']\nboxes_dist","97874b9d":"fig = plt.figure(figsize=(24, 8))\nsns.barplot(x=boxes_dist.num_boxes, y=boxes_dist.num_frames)\n\nplt.title(\"Box Distribution\")\nplt.xlabel(\"Number of Boxes\")\nplt.ylabel(\"Frame Counts\")\n\nplt.show()","19b1e25c":"def gen_file_path(image_id):\n    # extract file path by using the image_id in the train file\n    video_id = image_id.split('-')[0]\n    image_id = image_id.split('-')[1]\n    return os.path.join(dataset['video_img_dir'], 'video_' + video_id, image_id + '.jpg')\n\ndef draw_boxes(image_path, annot_line):\n    \n    boxes = decode_annotation(annot_line)\n\n    coords = [] \n    for box in boxes: \n        coord = [] \n        coord.append(box[0]) \n        coord.append(box[1]) \n        coord.append(box[0] + box[2]) \n        coord.append(box[1] + box[3]) \n        coords.append(coord) \n\n    image = Image.open(image_path)\n    imgcp = image.copy()\n    imgcp_draw = ImageDraw.Draw(imgcp)\n\n    for coord in  coords:\n         imgcp_draw.rectangle(coord, fill = None, outline = \"red\", width=5)\n\n    return imgcp","665f37b9":"train_csv['file_path'] = train_csv['image_id'].apply(gen_file_path)","9e8a0a22":"train_csv.head()","ecf2c3e7":"samples = train_csv.groupby('num_boxes').first()","70977ae5":"plt.figure(figsize=(24, 36))\n\nr, c = 7, 3\nfor index, row in samples.iterrows():\n    image_path = row['file_path']\n    annot_line = row['annotations']\n    plt.subplot(r, c, index + 1)\n    dimg = draw_boxes(image_path, annot_line)\n    plt.imshow(dimg)\n    \nplt.tight_layout()\nplt.show()","fed27cd5":"all_boxes_xy = []\nall_boxes_wh = []\n\nfor index, row in tqdm(train_csv.iterrows(), total=len(train_csv)):\n    if row['annotations'] != '[]':\n        boxes = decode_annotation(row['annotations'])\n        \n        for box in boxes:\n            all_boxes_xy.append([box[0], box[1]])\n            all_boxes_wh.append([box[2], box[3]])\n            \nall_boxes_xy = np.array(all_boxes_xy)\nall_boxes_wh = np.array(all_boxes_wh)","a656e660":"box_center_df = pd.DataFrame.from_records(all_boxes_xy, columns=['x', 'y'])\n\nbox_shape_df  = pd.DataFrame.from_records(all_boxes_wh, columns=['width', 'height'])\nbox_shape_df['area'] = box_shape_df['width'] * box_shape_df['height']","7b38c400":"box_center_df.describe()","b77da8b9":"box_shape_df.describe()","3b132bc2":"plt.figure(figsize=(28, 16))\nplt.scatter(x=all_boxes_xy[:,0], y=all_boxes_xy[:,1], s=0.5, color = 'red')\nplt.title(\"Distribution of Box Center Coordinate on Image\")\nplt.xlabel(\"X value\")\nplt.ylabel(\"Y value\")\nplt.show()","2fe77d6d":"train_csv.groupby('sequence')['num_boxes'].sum().sort_values(ascending=False).to_frame().T","a480b72b":"train_csv.groupby('sequence')['image_id'].count().sort_values(ascending=False).to_frame().T\n","2141b21d":"sample_seq = train_csv[train_csv.sequence == 22643]\nsample_seq","1d132e8a":"from PIL import Image\nimg = Image.open(train_csv['file_path'][0])","b61d1443":"plt.imshow(np.array(img))\nimg.size","99516497":"import torch\nfrom torch.utils.data import DataLoader\n!pip install -qU torch_snippets\nfrom torch_snippets import *","67df37ce":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nlabel2target = {}\nlabel2target['starfish'] = 1\nlabel2target['background'] = 0\ntarget2label = {t:l for l,t in label2target.items()}\ndef preprocess_image(img):\n    img = torch.tensor(img).permute(2,0,1)\n    return img.to(device).float()\nclass OpenDataset(torch.utils.data.Dataset):\n    w, h = 1280 , 720\n    def __init__(self, df):\n#         self.image_dir = image_dir\n#         self.files = glob.glob(self.image_dir+'\/*')\n        self.df = df\n        self.image_infos = df['file_path'].values\n    def __getitem__(self, ix):\n        # load images and masks\n#         image_id = self.image_infos[ix]\n        img_path = self.image_infos[ix]\n        img = Image.open(img_path).convert(\"RGB\")\n        img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))\/255.\n#         data = self.df[self.df['Image_ID'] == image_id]\n#         if self.df['annotations'].values[ix] != '[]':\n        data = []\n        boxes = decode_annotation(self.df['annotations'].values[ix]) \n        for box in boxes:\n            data.append(pd.Series([box[0], box[1],box[0]+box[2],box[1]+box[3]]).astype(np.uint32).tolist())\n#             data = data.astype(np.uint32).tolist() # convert to absolute coordinates\n        # torch FRCNN expects ground truths as a dictionary of tensors\n\n        labels =['starfish']*len(data)\n        target = {}\n        target[\"boxes\"] = torch.Tensor(data).float()\n        target[\"labels\"] = torch.Tensor([1]*len(data)).long()\n        img = preprocess_image(img)\n        return img, target\n    def collate_fn(self, batch):\n        return tuple(zip(*batch)) \n\n    def __len__(self):\n        return len(self.image_infos)","72fa3ee5":"data = train_csv[train_csv.annotations != \"[]\"]\nfrom sklearn.model_selection import train_test_split\ntrn_ids, val_ids = train_test_split(data.image_id.unique(), test_size=0.1, random_state=99)\ntrn_df, val_df = data[data['image_id'].isin(trn_ids)], data[data['image_id'].isin(val_ids)]\n\ntrain_ds = OpenDataset(trn_df)\ntest_ds = OpenDataset(val_df)\ntrain_loader = DataLoader(train_ds, batch_size=4, collate_fn=train_ds.collate_fn, drop_last=True)\ntest_loader = DataLoader(test_ds, batch_size=4, collate_fn=test_ds.collate_fn, drop_last=True)\nlen(trn_df), len(val_df)","e4852658":"num_classes = 2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n\ndef get_model():\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model","eca335ad":"# Defining training and validation functions for a single batch\ndef train_batch(inputs, model, optimizer):\n    model.train()\n    input, targets = inputs\n    input = list(image.to(device) for image in input)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    optimizer.zero_grad()\n    losses = model(input, targets)\n    loss = sum(loss for loss in losses.values())\n    loss.backward()\n    optimizer.step()\n    return loss, losses\n\n@torch.no_grad() # this will disable gradient computation in the function below\ndef validate_batch(inputs, model):\n    model.train() # to obtain the losses, model needs to be in train mode only. # #Note that here we are not defining the model's forward method \n#and hence need to work per the way the model class is defined\n    input, targets = inputs\n    input = list(image.to(device) for image in input)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n    optimizer.zero_grad()\n    losses = model(input, targets)\n    loss = sum(loss for loss in losses.values())\n    return loss, losses","57c8bc17":"model = get_model().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\nn_epochs = 8\nlog = Report(n_epochs)","3e3ed2d8":"for epoch in range(n_epochs):\n    _n = len(train_loader)\n    for ix, inputs in enumerate(train_loader):\n        loss, losses = train_batch(inputs, model, optimizer)\n        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = \\\n            [losses[k] for k in ['loss_classifier','loss_box_reg','loss_objectness','loss_rpn_box_reg']]\n        pos = (epoch + (ix+1)\/_n)\n        log.record(pos, trn_loss=loss.item(), trn_loc_loss=loc_loss.item(), \n                   trn_regr_loss=regr_loss.item(), trn_objectness_loss=loss_objectness.item(),\n                   trn_rpn_box_reg_loss=loss_rpn_box_reg.item(), end='\\r')\n\n    _n = len(test_loader)\n    for ix,inputs in enumerate(test_loader):\n        loss, losses = validate_batch(inputs, model)\n        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = \\\n          [losses[k] for k in ['loss_classifier','loss_box_reg','loss_objectness','loss_rpn_box_reg']]\n        pos = (epoch + (ix+1)\/_n)\n        log.record(pos, val_loss=loss.item(), val_loc_loss=loc_loss.item(), \n                  val_regr_loss=regr_loss.item(), val_objectness_loss=loss_objectness.item(),\n                  val_rpn_box_reg_loss=loss_rpn_box_reg.item(), end='\\r')\n    if (epoch+1)%(n_epochs\/\/2)==0: log.report_avgs(epoch+1)","c14f0955":"log.plot_epochs(['trn_loss','val_loss'])","b4ad171b":"inputt = next(iter(test_loader))[0]\ninput1 = next(iter(test_loader))[1]","fd890d05":"model.eval()\nmodel(inputt[0].unsqueeze(0))","4c55a919":"from torchvision.ops import nms\ndef decode_output(output):\n    'convert tensors to numpy arrays'\n    bbs = output['boxes'].cpu().detach().numpy().astype(np.uint16)\n    labels = np.array([target2label[i] for i in output['labels'].cpu().detach().numpy()])\n    confs = output['scores'].cpu().detach().numpy()\n    ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n    bbs, confs, labels = [tensor[ixs] for tensor in [bbs, confs, labels]]\n\n    if len(ixs) == 1:\n        bbs, confs, labels = [np.array([tensor]) for tensor in [bbs, confs, labels]]\n    return bbs.tolist(), confs.tolist(), labels.tolist()","1b2301f5":"model.eval()\nfor ix, (images, targets) in enumerate(test_loader):\n    if ix==3: break\n    images = [im for im in images]\n    outputs = model(images)\n    for ix, output in enumerate(outputs):\n        bbs, confs, labels = decode_output(output)\n        info = [f'{l}@{c:.2f}' for l,c in zip(labels, confs)]\n        show(images[ix].cpu().permute(1,2,0), bbs=bbs, texts=labels, sz=7)","ab8a72ab":"PATH ='TF-OD1.pth'\nPATH1 ='TF-OD2.pth'\ntorch.save(model.state_dict(), PATH)\ntorch.save(model, PATH1)","83f7e4d0":"# Model Testing","be574d8f":"# Creating Dataset","6c2c68d9":"# Sequence"}}