{"cell_type":{"63d6b9a8":"code","72b6f090":"code","9cf11fbc":"code","d3b0a0cf":"code","f2d2ec33":"code","e592081d":"code","4ee65286":"code","bafbb9bb":"code","414ed4c8":"code","a6ef6547":"code","3c1d8d8a":"code","eaeada41":"code","c16d64c5":"code","09678dfc":"code","571614ad":"code","1576bb0f":"code","949a1496":"code","f825d4db":"code","a27156d2":"code","58b11c98":"code","9dcf91e7":"code","42728964":"code","477e62b9":"code","fc07c8bd":"code","b40b989c":"code","067a10a6":"code","7ec50801":"code","da2b4e97":"code","3f5bacf9":"code","ea02c470":"code","15357b90":"code","737f152f":"code","c9204f80":"code","61e90b97":"code","9d66da28":"code","1379483a":"code","71564aed":"code","4471d1ff":"code","e1b34450":"code","54e83b59":"code","980cfd85":"code","0d766b94":"code","fabaa22c":"code","dcddbf6b":"code","18988416":"code","0a9d4ecc":"code","2e9935f2":"code","8d4e21c7":"code","5fc0e057":"code","c1488d61":"code","4d97ab33":"code","260b9376":"code","03541305":"code","ab958102":"code","c84afd92":"code","a460f50b":"code","c2a21025":"code","26124dca":"code","3385a558":"code","f79ec5c4":"code","9f4b871f":"code","3697514a":"code","8260c508":"code","c5e1c336":"code","87471383":"code","80942ebd":"code","e2b4d0f6":"markdown","4fbe2751":"markdown","75c3486b":"markdown","479be2b2":"markdown","08c21665":"markdown","54e050ca":"markdown","d954bd27":"markdown","6c38c920":"markdown","32b36f1b":"markdown","b04339e1":"markdown","83e3d61b":"markdown","b989e133":"markdown","4663baed":"markdown","f40992fc":"markdown","f2677c9f":"markdown","17c259ea":"markdown","a4159fc3":"markdown","63fa72ec":"markdown","9b1987c5":"markdown","21a8da57":"markdown","1d690273":"markdown","c145c23e":"markdown","40412887":"markdown","c587eb9f":"markdown","ef9e7c3c":"markdown","1fc1c74f":"markdown","34a5e425":"markdown","d0fd08bc":"markdown","8ae4f493":"markdown","6bc05800":"markdown","ec5c11b1":"markdown","1e0abacb":"markdown","3ff8eb81":"markdown","7b1a274a":"markdown","7d9465de":"markdown","2ce34160":"markdown","e872d6f5":"markdown","016f070a":"markdown","b2aac7e6":"markdown","8cdfa846":"markdown","b2d4706c":"markdown","6f0cf262":"markdown","aacfa0d6":"markdown","e20b2f2e":"markdown","99b068b8":"markdown","67a85029":"markdown","04c814ae":"markdown","cde5d566":"markdown","359ebaaa":"markdown","fe4b17ea":"markdown","f8efa1d7":"markdown","bcb2097d":"markdown","a62ae419":"markdown","ec916178":"markdown","8b8f50d1":"markdown","4505bbe3":"markdown","09834c50":"markdown","e7de7b4e":"markdown","9402a993":"markdown","173ae84b":"markdown","c232395b":"markdown","854b39ab":"markdown","026a8924":"markdown","fce0816f":"markdown","bd7996aa":"markdown","4add19f4":"markdown","8598a06a":"markdown","04d8892f":"markdown","e59ace38":"markdown","21a215b4":"markdown","c49bbec3":"markdown","6da94dc8":"markdown","3ffa7c67":"markdown","0e4a5b27":"markdown","fae092cc":"markdown"},"source":{"63d6b9a8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics","72b6f090":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","9cf11fbc":"##display the first five rows of the train dataset.\ntrain.head(5)","d3b0a0cf":"#display the first five rows of the test dataset.\ntest.head(5)","f2d2ec33":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))","e592081d":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","4ee65286":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","bafbb9bb":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","414ed4c8":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","a6ef6547":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","3c1d8d8a":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","eaeada41":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","c16d64c5":"f, ax = plt.subplots(figsize=(10, 8))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","09678dfc":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","571614ad":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","1576bb0f":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","949a1496":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","f825d4db":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n","a27156d2":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n","58b11c98":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","9dcf91e7":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","42728964":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","477e62b9":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","fc07c8bd":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","b40b989c":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","067a10a6":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","7ec50801":"all_data = all_data.drop(['Utilities'], axis=1)\n","da2b4e97":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n","3f5bacf9":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n","ea02c470":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n","15357b90":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","737f152f":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n","c9204f80":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n","61e90b97":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","9d66da28":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","1379483a":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","71564aed":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","4471d1ff":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","e1b34450":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","54e83b59":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","980cfd85":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","0d766b94":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","fabaa22c":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","dcddbf6b":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","18988416":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","0a9d4ecc":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","2e9935f2":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","8d4e21c7":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","5fc0e057":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","c1488d61":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","4d97ab33":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","260b9376":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","03541305":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ab958102":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","c84afd92":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","a460f50b":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","c2a21025":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","26124dca":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","3385a558":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","f79ec5c4":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","9f4b871f":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","3697514a":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))\n","8260c508":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))\n","c5e1c336":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","87471383":"ensemble = stacked_pred*0.7 + xgb_pred*0.15 + lgb_pred*0.15","80942ebd":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)\n","e2b4d0f6":"Outliers removal is not always safe. We decided to delete these two as they are very huge and really bad ( extremely large areas for very low prices).","4fbe2751":"\n\n### Define a cross validation strategy\n","75c3486b":"**Skewed features**","479be2b2":"## More features engeneering:\n\n**Transforming some numerical variables that are really categorical**","08c21665":"## Imputing missing values","54e050ca":"**LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood, we can fill in missing values by the median LotFrontage of the neighborhood.","d954bd27":"### LASSO Regression :\n\n#### This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline\n","6c38c920":"### Averaged Base Models Score:\n","32b36f1b":"**MSSubClass** : Na most likely means No building class. We can replace missing values with None\n","b04339e1":"#### We use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x . \n\n#### Note that setting  \u03bb=0  is equivalent to log1p used above for the target variable.","83e3d61b":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them.","b989e133":"**GarageType, GarageFinish, GarageQual and GarageCond**: Replacing missing data with None","4663baed":"It remains no missing value.\n\n","f40992fc":"### **<span style=\"color:blue\">Please Upvote If You Like, Use Or Learn From This. If you have any idea that might improve this kernel, please be sure to comment, or fork and experiment as you like. If you don't understand any part, feel free to ask in the comment section.**\ud83d\ude4c\ud83d\ude4c\ud83d\ude4c","f2677c9f":"Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","17c259ea":"FireplaceQu : data description says NA means \"no fireplace\"","a4159fc3":"**Getting the new train and test sets.**","63fa72ec":"**PoolQC** : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.","9b1987c5":"**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.","21a8da57":"### Simplest Stacking approach : Averaging base models","1d690273":"**Adding one more important feature**\n\n**Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house**","c145c23e":"**Alley** : data description says NA means \"no alley access\"","40412887":"## Stacking Averaged Models Class","c587eb9f":"### Outliers","ef9e7c3c":"### LightGBM:\n","1fc1c74f":"We add **XGBoost and LightGBM** to the **StackedRegressor** defined previously.","34a5e425":"Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it","d0fd08bc":"### XGBoost:","8ae4f493":"**MiscFeature** : data description says NA means \"no misc feature\"","6bc05800":"### Missing Data\n","ec5c11b1":"### Import librairies\n","1e0abacb":"#### We begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation ","3ff8eb81":"#### We just average four models here **ENet, GBoost, KRR and Lasso**. Of course we could easily add more models in the mix.","7b1a274a":"**BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement","7d9465de":"## Final Training and Prediction","2ce34160":"### Gradient Boosting Regression :\n\n#### With **huber** loss that makes it robust to outliers\n","e872d6f5":"**Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","016f070a":"## Importing Data and Dependencies","b2aac7e6":"**Functional** : data description says NA means typical\n","8cdfa846":"### LightGBM:","b2d4706c":"**Fence**: data description says NA means \"no fence\"","6f0cf262":"**KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","aacfa0d6":"**SaleType** : Fill in again with most frequent which is \"WD\"","e20b2f2e":"### <span style=\"color:blue\">**Please Upvote If You Like, Use Or Learn From This.**\n\n### <span style=\"color:blue\">**Also Comment For Improvisation Of The Notebook.**\n\n### <span style=\"color:blue\">**Thank You!!!**\n\n","99b068b8":"## Stacking Averaged Models Score:","67a85029":"## Submission\n","04c814ae":"# House Prices Advanced Regression Techniques\n\n## Overview\nThere are several factors that influence the price a buyer is willing to pay for a house. Some are apparent and obvious and some are not. Nevertheless, a rational approach facilitated by machine learning can be very useful in predicting the house price. A large data set with 79 different features (like living area, number of rooms, location etc) along with their prices are provided for residential homes in Ames, Iowa. The challenge is to learn a relationship between the important features and the price and use it to predict the prices of a new set of houses.\n\n> In This project we are going to use some of the advanced learning algorithm to predict the housing prices. Because it is not easy task to fit the model from training data who has more than 80 features of single house, so we need to work with more complexity such as cleaning the data, label encoding, one hot encoding , dimensionality reduction, features scaling etc.Hence, based on there Accuracy and the r2_score we will be desciding which algorithm is best for fitting the model in this case.\n\n## Flow of Kernel:\n\n![](https:\/\/pic1.xuehuaimg.com\/proxy\/csdn\/https:\/\/img-blog.csdn.net\/20180729183002412?watermark\/2\/text\/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYWlndWNodW45NTAz\/font\/5a6L5L2T\/fontsize\/400\/fill\/I0JBQkFCMA==\/dissolve\/70)\n\n## Data Processing\n* Deleting the huge outliners from data.\n\n* Log - transformation of target variable\n\n* Feature Engineering for slection of better features for our data.\n\n* Data Correlation matrix for viwing the relationship of variables.\n\n* There are many missing values in data so we will fill those values as given in the competition rules.\n\n* Label Encoding some categorical variables\n\n* Using Box Cox Transformation of (highly) skewed features\n\n## Modelling\n\n* Importing all required libraries\n\n* Cross validation strategy definition\n\n### Base models used\n\n1. LASSO Regression\n\n2. Elastic Net Regression\n\n3. Kernel Ridge Regression\n\n4. Gradient Boosting Regression\n\n5. XGBoost\n\n6. LightGBM\n\n## Stacking Models\n* Averaged base models class method\n\n* Ensembling StackedRegressor, XGBoost and LightGBM\n\n* Ensemble prediction and submission.\n\n## About the ensembling methods used:\n* Stacking :\n[Guide to Model Stacking](https:\/\/towardsdatascience.com\/a-guide-to-ensemble-learning-d3686c9bed9a)\n\n* Average and weighted average ensembling:\n[Guide to Ensembling](https:\/\/mlwave.com\/kaggle-ensembling-guide\/)\n\n* Boosting:\n[Guide to Boosting Algorithms](https:\/\/hackernoon.com\/boosting-algorithms-adaboost-gradient-boosting-and-xgboost-f74991cad38c)","cde5d566":"### XGBoost:\n","359ebaaa":"### Averaged Base Models Class:\n","fe4b17ea":"### Stacked Regressor:","f8efa1d7":"## Target Variable","bcb2097d":"### Ensemble prediction:","a62ae419":"## Base Models scores","ec916178":"## Ensembling StackedRegressor, XGBoost and LightGBM","8b8f50d1":"### Elastic Net Regression :\n\n#### Again made robust to outliers\n","4505bbe3":"To make the two approaches comparable (by using the same number of models) , we just average **Enet KRR and Gboost**, then we add **lasso as meta-mode.**","09834c50":"**Box Cox Transformation of (highly) skewed features**","e7de7b4e":"The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","9402a993":"## Data Correlation","173ae84b":"## Base Models:","c232395b":"## Feature Engineering","854b39ab":"## Stacking models","026a8924":"MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.","fce0816f":"**Getting dummy categorical features**","bd7996aa":"## Log-transformation of the target variable","4add19f4":"#### We use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation","8598a06a":"## Data Processing","04d8892f":"**Label Encoding some categorical variables that may contain information in their ordering set**","e59ace38":"### Kernel Ridge Regression :","21a215b4":"**MSZoning (The general zoning classification)** : 'RL' is by far the most common value. So we can fill in missing values with 'RL'","c49bbec3":"## Modelling","6da94dc8":"**BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no basement.","3ffa7c67":"\n\nWe first define a rmsle evaluation function\n","0e4a5b27":"GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)","fae092cc":"### We impute them by proceeding sequentially through features with missing values"}}