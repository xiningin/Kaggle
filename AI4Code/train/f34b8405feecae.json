{"cell_type":{"f2026461":"code","928b923b":"code","1e28c0eb":"code","bd3a5cf9":"code","dd2eaeed":"code","0eb6b9cf":"code","eabf6b46":"code","66ebd54b":"code","de366c9b":"code","c3db4704":"code","bbda544a":"code","117de435":"code","101478ad":"code","e93ba5f8":"code","45148ae2":"code","5fdf261e":"code","8cb1962e":"code","4119a56b":"code","4dd8c28a":"code","29dcd521":"code","2f2d4c32":"code","bbb74c72":"code","231f6927":"code","df633738":"code","fb209286":"code","9ae453e4":"code","0427ecb9":"code","7dd19aa6":"code","2710a35d":"code","6516ad56":"code","3a64aa4f":"code","06d88548":"code","094cd931":"code","72e686c4":"code","47381933":"markdown","fb87bd08":"markdown","a332438e":"markdown","02f4f71e":"markdown","6b18e8c7":"markdown","324a8a06":"markdown","a0d778f1":"markdown","e75e324c":"markdown","d133ca8e":"markdown","b2dfb415":"markdown","7467f276":"markdown","7f5cfcd2":"markdown","dbe928bf":"markdown","8eee752e":"markdown","791b723e":"markdown","9ec9e242":"markdown","a2ad0147":"markdown","c0764304":"markdown","305d0ff2":"markdown","b531fbde":"markdown","4d1b03c6":"markdown","c2dede93":"markdown","03fb021e":"markdown","324fb3b3":"markdown","029ce796":"markdown"},"source":{"f2026461":"learners_names = ['fit', 'fit_one_cycle()', 'fit_one_cycle(5, max_lr)', 'fit_one_cycle(5, max_lr=[])', 'custom cosine annealing', 'custom linear annealing', 'custom NO annealing' ]","928b923b":"import numpy as np\nfrom copy import copy\nimport torch\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport PIL\nprint(PIL.PILLOW_VERSION)\n\nnp.random.seed(2)\n\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\nimport fastai\nfastai.__version__","1e28c0eb":"import os\nimport pandas as pd","bd3a5cf9":"train_folder = '..\/input\/images\/images'\nprint(os.listdir(train_folder)[:10])","dd2eaeed":"fnames = get_image_files(train_folder)\nprint(fnames[:5])\npat = re.compile(r'\/([^\/]+)_\\d+.jpg$') # we specify a regex for finding cat or dog images\nsz = 64\nbs = 64\ndata = ImageDataBunch.from_name_re(\n                                train_folder,\n                                fnames,\n                                pat,\n                                ds_tfms=get_transforms(),\n                                size=sz, bs=bs,\n                                valid_pct = 0.75, #_______________________________________________________\n                                num_workers = 0, # for code safety on kaggle\n).normalize(imagenet_stats)\ndata","0eb6b9cf":"print(data.classes)\ndata.show_batch()","eabf6b46":"learn = cnn_learner(\n    data,\n    models.resnet18,\n    metrics=error_rate,\n    model_dir=\"\/tmp\/model\/\"\n)","66ebd54b":"learn.unfreeze()","de366c9b":"learn1 = copy(learn)\nlearn2 = copy(learn)\nlearn3 = copy(learn)\nlearn4 = copy(learn)\nlearn5 = copy(learn)\nlearn6 = copy(learn)\nlearn7 = copy(learn)","c3db4704":"\ndef learner_plots(learner, name):\n    pltName = name\n    metricsName =  'error_rate'\n    lrec = learner.recorder\n    plt.plot(figsize=(2,9))\n\n    plt.subplot(4, 1, 1)\n    plt.plot( lrec.lrs, lrec.losses )\n    plt.title(pltName, fontdict={'fontsize':20})\n    plt.ylabel('Losses', fontdict={'fontsize':15})\n    plt.xlabel('LR')\n\n    plt.subplot(4, 1, 2)\n    plt.plot( lrec.lrs )\n    plt.ylabel('LR', fontdict={'fontsize':15})\n    plt.xlabel('Iterations')\n\n    plt.subplot(4, 1, 3)\n    iters = np.arange(len(lrec.lrs))\n    val_iter = np.cumsum(lrec.nb_batches)\n    plt.plot( iters, lrec.losses )\n    plt.plot( val_iter, lrec.val_losses )\n    plt.ylabel('Loss')\n    plt.yscale('log')\n    plt.xlabel('Iterations')\n\n    plt.subplot(4, 1, 4)\n    # plt.bar( \n    #     np.arange(len(learn1.recorder.metrics))+1,\n    #     np.array(learn1.recorder.metrics[0]).astype(int)\n    # )\n    plt.plot(lrec.metrics)\n    plt.ylabel(metricsName)\n    plt.xlabel('Iterations 1e-2')\n    plt.gcf().set_figheight(15)\n    plt.gcf().set_figwidth(15)\n    return","bbda544a":"def plotLearners(learners, learners_names, figsize=(20, 6)):\n    '''\n    edit: https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/basic_train.py#L524\n    '''\n    # figure(figsize=figsize, dpi=80, facecolor='w', edgecolor='k')\n    nl = len(learners)\n    fig, ax = plt.subplots(3,nl)\n    fig.set_figheight(figsize[0])\n    fig.set_figwidth(figsize[1])\n    for li in range(len(learners)):\n        lrec = learners[li].recorder\n        skip_start = 10\n        skip_end = 5\n        lrs = lrec._split_list(lrec.lrs, skip_start, skip_end)\n        losses = [x.item() for x in lrec._split_list(lrec.losses, skip_start, skip_end)]\n        val_losses =  learners[li].recorder.val_losses\n        # if 'k' in kwargs: losses = self.smoothen_by_spline(lrs, losses, **kwargs)\n        # plt.subplot(2,4,li);\n        # PLOT LR vs LOSS\n        ax[0, li].plot(lrs, losses);\n        if (li==0): ax[0, li+1].set_ylabel(\"Loss\")\n        ax[0, li].set_title(learners_names[li])\n        ax[0, li].set_xlabel(\"Learning Rate\")\n        ax[0, li].set_xscale('log')\n        show_moms=False; skip_start=0; skip_end=0; return_fig=None;\n        # PLOT LR vs ITERs\n        lrs = lrec._split_list(lrec.lrs, skip_start, skip_end)\n        iterations = lrec._split_list(range_of(lrec.lrs), skip_start, skip_end)\n        if show_moms: # not used right here\n            moms = lrec._split_list(self.moms, skip_start, skip_end)\n            fig, axs = plt.subplots(1,2, figsize=(12,4))\n            axs[0].plot(iterations, lrs)\n            axs[0].set_xlabel('Iterations')\n            axs[0].set_ylabel('Learning Rate')\n            axs[1].plot(iterations, moms)\n            axs[1].set_xlabel('Iterations')\n            axs[1].set_ylabel('Momentum')\n        else:\n            # plt.subplot(2,4, (li+1)+4)\n            ax[1, li].plot(iterations, lrs)\n            ax[1, li].set_xlabel('Iterations')\n            ax[1, li].set_ylabel('Learning Rate')\n        # PLOT LOSSES\n        losses = lrec._split_list(lrec.losses, skip_start, skip_end)\n        ax[2, li].plot(iterations, losses, label='Train') ## SUBPLOT 3\n        val_iter = lrec._split_list_val(np.cumsum(lrec.nb_batches), skip_start, skip_end)\n        val_losses = lrec._split_list_val(lrec.val_losses, skip_start, skip_end)\n        ax[2, li].plot(val_iter, val_losses, label='Validation')\n        ax[2, li].set_ylabel('Loss')\n        ax[2, li].set_xlabel('Batches processed')\n        ax[2, li].legend()\n        if ifnone(return_fig, defaults.return_fig): return fig\n        # PLOT METRICS\n#         val_iter = self._split_list_val(np.cumsum(self.nb_batches), skip_start, skip_end)\n#         axes = axes.flatten() if len(self.metrics[0]) != 1 else [axes]\n#         for i, ax in enumerate(axes):\n#             values = [met[i] for met in self.metrics]\n#             values = self._split_list_val(values, skip_start, skip_end)\n#             ax.plot(val_iter, values)\n#             ax.set_ylabel(str(self.metrics_names[i]))\n#             ax.set_xlabel('Batches processed') \n#     plt.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))","117de435":"%%time\nlearn1.lr_find()\nlearn2.lr_find()\nlearn3.lr_find()\nlearn4.lr_find()\n# learn5.lr_find()\n# learn6.lr_find()\n# learn7.lr_find()","101478ad":"learners = [learn1.recorder, learn2.recorder, learn3.recorder, learn4.recorder]\nplotLearners(learners, learners_names, figsize=(15,18))","e93ba5f8":"learn3.recorder.plot(suggestion=True)\nmng3 = learn3.recorder.min_grad_lr  #\/\/ Suggested-LR, https:\/\/docs.fast.ai\/callbacks.lr_finder.html","45148ae2":"learn1 = copy(learn)\nlearn2 = copy(learn)\nlearn3 = copy(learn)\nlearn4 = copy(learn)","5fdf261e":"learn1.fit(5)     # 10epochs, no cyclical_lr","8cb1962e":"# plotLearners([learn1], learners_names[:1], figsize=(20, 6))\nlearner_plots(learn1, 'fit(5)')","4119a56b":"learn2.fit_one_cycle(5)         # cyclical_lr","4dd8c28a":"learner_plots(learn2, 'fit_one_cycle(5)')","29dcd521":"learn3.lr_find()\nlearn3.recorder.plot(suggestion=True)\nmng3 = learn3.recorder.min_grad_lr  #\/\/ Suggested-LR, https:\/\/docs.fast.ai\/callbacks.lr_finder.html","2f2d4c32":"learn3.fit_one_cycle(5, max_lr=mng3); print('Training learn3 with min numerical gradient: ', mng3)  # cyclical_lr with max value","bbb74c72":"learner_plots(learn3, 'fit_one_cycle + max_lr')","231f6927":"learn4.fit_one_cycle(5, max_lr=slice(mng3*0.01, mng3))    # cyclical_lr + differential_lr","df633738":"learner_plots(learn4, 'fit_one_cycle + differential LR')","fb209286":"from fastai.callbacks.general_sched import *","9ae453e4":"def fit_custom_annealing(learn, anneal, n_cycles, lr, mom, cycle_len, cycle_mult):\n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n * (cycle_len * cycle_mult**i))\n                 .schedule_hp('lr', lr, anneal=anneal)\n                 .schedule_hp('mom', mom)) for i in range(n_cycles)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    if cycle_mult != 1:\n        total_epochs = int(cycle_len * (1 - (cycle_mult)**n_cycles)\/(1-cycle_mult)) \n    else: total_epochs = n_cycles * cycle_len\n    learn.fit(total_epochs)","0427ecb9":"anneal = annealing_cos\nfit_custom_annealing(learn5, anneal=anneal, n_cycles=3, lr=1e-3, mom=0.9, cycle_len=1, cycle_mult=2)","7dd19aa6":"learner_plots(learn5, 'Cosine annealing')","2710a35d":"anneal = annealing_linear\nfit_custom_annealing(learn6, anneal=anneal, n_cycles=3, lr=1e-3, mom=0.9, cycle_len=1, cycle_mult=2)","6516ad56":"learner_plots(learn6, 'Linear annealing')","3a64aa4f":"anneal = annealing_no\nfit_custom_annealing(learn7, anneal=anneal, n_cycles=3, lr=1e-3, mom=0.9, cycle_len=1, cycle_mult=2)","06d88548":"learner_plots(learn7, 'No annealing')","094cd931":"learners = [learn1, learn2, learn3, learn4]\nplotLearners(learners, learners_names,  figsize=(15,18))","72e686c4":"learners = [learn5, learn6, learn7]\nplotLearners(learners, learners_names,  figsize=(15,18))","47381933":"#### Cosine annealing","fb87bd08":"Further Readings   \n*  [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay](https:\/\/arxiv.org\/pdf\/1803.09820.pdf)  \n*  https:\/\/sgugger.github.io\/the-1cycle-policy.html  \n*  https:\/\/blog.floydhub.com\/ten-techniques-from-fast-ai\/  \n*  [Introduction to Cyclical Learning Rates](https:\/\/www.datacamp.com\/community\/tutorials\/cyclical-learning-neural-nets)  ","a332438e":"https:\/\/docs.fast.ai\/basic_train.html#Learner\nhttps:\/\/docs.fast.ai\/basic_train.html#lr_find","02f4f71e":"#### Load Data","6b18e8c7":"### <a id='overaall_glance'>A overall glance<\/a>","324a8a06":"Annealing customization.\nThe LR annealing can be implemented through different types of functions. Let's see some examples.\n\nTo do so, we need to set the traing phase callback, inside the scheduler of 1cycle fitter.  \n![](http:\/\/)ref: \nhttps:\/\/docs.fast.ai\/callbacks.general_sched.html#TrainingPhase   \nhttps:\/\/docs.fast.ai\/callbacks.one_cycle.html#OneCycleScheduler  ","a0d778f1":"We create our 7 learners","e75e324c":"Cycle length\nhttps:\/\/docs.fast.ai\/basic_train.html#fit_one_cycle","d133ca8e":"_______","b2dfb415":"About Cyclical Learning Rates, according to the fastAI docs:  \n>Unpublished work has shown even better results by using only two phases: the same phase 1, followed by a second phase where we do a cosine annealing from lr_max to 0. The momentum goes from mom_min to mom_max by following the symmetric cosine (see graph a bit below).\n>\n([What is 1cycle?, callbacks, fastAI docs](https:\/\/docs.fast.ai\/callbacks.one_cycle.html#What-is-1cycle?))","7467f276":"### <a id=\"custom_annealing\">Custom annealing<\/a>","7f5cfcd2":"https:\/\/docs.fast.ai\/callback.html#Annealing-functions  \n","dbe928bf":"#### Linear annealing","8eee752e":"#### No annealing","791b723e":"# Learning Rate Annealing with FastAI\n\n\nin this kernel I did several tests in order to understand the learning annealing and how this is available from fastAI library.\n\nas [Smith](https:\/\/arxiv.org\/pdf\/1803.09820.pdf) says:\n>If the learning rate (LR) is too small, overfitting can occur. Large learning rates help to regularize\nthe training but if the learning rate is too large, the training will diverge. Hence a grid search of short\nruns to find learning rates that converge or diverge is possible but there is an easier way.\nCyclical learning rates (CLR) and the learning rate range test (LR range test) were first proposed by\nSmith (2015) and later updated in Smith (2017) as a recipe for choosing the learning rate.\n>\n\n>To use CLR, one specifies minimum and maximum learning rate boundaries and a stepsize\n>","9ec9e242":"Using the [Recorder](https:\/\/docs.fast.ai\/basic_train.html#Recorder) class","a2ad0147":"#### Further improvements and experiments\nI hope it will be useful. If you have any suggestions please leave a comment \ud83d\ude0a, you're more than welcome.","c0764304":"### Model definition","305d0ff2":"We will knead a tray of learners to ~~serve~~ train with different lr approaches  \nThis will be our set of experiments\n\n1. [fit](#fit)\n    * fit(5)\n2. [fit_one_cycle](#fit_one_cycle)\n    * fit_one_cycle(5),  _using 1-cycle policy_\n    * fit_one_cycle( 5, max_lr=*suggestedByLRFinder* )\n    * fit_one_cycle( 5, max_lr=slice(*suggestedByLRFinder*) ),  _using differential learning rate_\n3. [custom cycles fits](#custom_annealing)\n    * custom cycles, cosine annealing\n    * custom cycles, linear annealing\n    * custom cycles, **no** annealing\n    \nFinally we'll [compare](#overaall_glance) all the results.\n\n\n","b531fbde":"I report here the results of previous experiments.","4d1b03c6":"Run the lr_find to some of them, just to see that it's more or less the same behaviour","c2dede93":"### <a id='fit'>fit<\/a>","03fb021e":"Now reset the learners, so to have their recorders clean.","324fb3b3":"We define some helper functions for plotting","029ce796":"### <a id='fit_one_cycle'>fit_one_cycle<\/a>\n##### The one-cycle policy\nhttps:\/\/docs.fast.ai\/callbacks.one_cycle.html#What-is-1cycle?\n\nLooking to the plots, please pay attention to the number of iterations varying the epochs number (first argument)."}}