{"cell_type":{"559e7666":"code","b5b84572":"code","b31a17c0":"code","14e4bc89":"code","79256f5a":"code","89889f0b":"code","7facf9dd":"code","b2de1a42":"code","2ac4551c":"code","5b6be239":"code","3ee31df8":"code","c8b7cd7a":"code","c63c276e":"code","36446bd3":"code","ff4b24f2":"code","0f23b518":"code","0d90a0fb":"code","9c262318":"code","13d5d52b":"code","7f6990ce":"markdown","0bb29db7":"markdown","46709395":"markdown","c0971527":"markdown","70388129":"markdown","22c4c058":"markdown","f274fb37":"markdown","6e723394":"markdown","a0c1b641":"markdown","fc08e312":"markdown","48078193":"markdown","9338eeda":"markdown","0a644217":"markdown","93fde735":"markdown","d4c36851":"markdown","66164e47":"markdown"},"source":{"559e7666":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model, Sequential\nimport numpy as np\nimport pandas as pd\nimport shutil\nimport time\nimport cv2 as cv2\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport os\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom IPython.core.display import display, HTML","b5b84572":"target_size=(224,224)\nfpath=r'..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/000006.jpg'\nplt.figure(figsize=(10, 10))\nimg=plt.imread(fpath)\nimg=cv2.resize(img, target_size)\nprint (img.shape)\nplt.imshow(img)","b31a17c0":"def scaler(img): # crops the image to an region of interest based on the attribute selected\n    # since this process many images use values based on input image shape\n    shape= img.shape\n    ycenter=int(shape[0]\/2.04) # adjust this point to set Y location of region of interest based on the type of attribute\n    xcenter=int(shape[1]\/2)\n    deltay=int(shape[0]\/7.5 )# adjust delta to set height and width of cropped image\n    deltax=int(shape[1]\/3.66)\n    ytop=ycenter-deltay\n    ybottom=ycenter+ deltay\n    xleft=xcenter-deltax\n    xright=xcenter+ deltax  \n    print(ycenter, xcenter, deltay, deltax)\n    new_img= img[ytop:ybottom, xleft:xright] # this is the xropped image    \n    new_img=cv2.resize(new_img, (shape[1], shape[0])) # must return original image size or else get an exception\n    new_img=new_img\/127.5-1 # scale pixels from -1 to + 1\n    return new_img\ncropped_img=scaler(img)\n# image was scaled between -1 and +1 so rescale back to 0 to 1\ncropped_img=(cropped_img + 1)\/2\nplt.figure(figsize=(10, 10))\nplt.imshow(cropped_img)","14e4bc89":"csvpath=r'..\/input\/celeba-dataset\/list_attr_celeba.csv'\ndf=pd.read_csv(csvpath)\ncolumns=df.columns\nfor column in columns:\n    if column !='image_id':\n        df[column]=df[column]. replace({-1: '0', 1:'1'}) # replace -1 value with '0' and 1 value  with'1'\n","79256f5a":"for column in df.columns:\n    print (column)\ntarget='Eyeglasses' # select a target attribute to classify\nprint (df[target].value_counts())","89889f0b":"columns=df.columns\nfor column in columns:\n    if column !='image_id' and column != target:\n        df=df.drop(column, axis=1)\nprint(df.head())","7facf9dd":"train_split=.98\ntest_split=.01\ndummy_split=test_split\/(1-train_split)\ntrain_df, dummy_df=train_test_split(df, train_size=train_split, shuffle=True, random_state=123)\ntest_df, valid_df=train_test_split(dummy_df, train_size=dummy_split, shuffle=True, random_state=123)\nprint ('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))","b2de1a42":"min_train_entries=3000  # specify the number of entries for each label class\nmin_tv_entries=600\nfor i, dataframe in enumerate([train_df, test_df, valid_df]):\n    if i==0:\n        min_entries=min_train_entries\n    else:\n        min_entries= min_tv_entries\n    balance=dataframe[target].value_counts()  \n    balance_list=list(balance)    \n    balance_min= np.min(balance) # find out the minimun number of labels\n    if balance_min>= min_entries:\n        entries=min_entries\n    else:\n        entries=balance_min\n\n    group=dataframe.groupby([target])\n    sample_list=[]\n\n    for label in dataframe[target].unique():\n        sample=group.get_group(label).sample(entries)    \n        sample_list.append(sample)       \n    if i==0:\n        train_df=pd.concat(sample_list, axis=0).reset_index(drop=True)\n    elif i == 1:\n        test_df=pd.concat(sample_list, axis=0).reset_index(drop=True)\n    else:\n        valid_df=pd.concat(sample_list, axis=0).reset_index(drop=True)\nprint('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '   valid_df length: ', len(valid_df))\nprint (train_df.head())  \n\n","2ac4551c":"img_dir=r'..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba'\nheight=128\nwidth=128\nchannels=3\nbatch_size=40\nimg_shape=(height, width, channels)\nimg_size=(height, width)\nlength=len(test_df)\ntest_batch_size=sorted([int(length\/n) for n in range(1,length+1) if length % n ==0 and length\/n<=batch_size],reverse=True)[0]  \ntest_steps=int(length\/test_batch_size)\nprint ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps)\ndef scalar(img):# use this as preprocessing function in ImageDataGenerator if no cropping is desired\n    return img\/127.5-1  # scale pixel between -1 and +1\ndef scaler(img): # crops the image to an region of interest based on the attribute selected\n    # since this process many images use values based on input image shape\n    shape= img.shape\n    ycenter=int(shape[0]\/2.04) # adjust this point to set Y location of region of interest based on the type of attribute\n    xcenter=int(shape[1]\/2)\n    deltay=int(shape[0]\/7.5 )# adjust delta to set height and width of cropped image\n    deltax=int(shape[1]\/3.66)\n    ytop=ycenter-deltay\n    ybottom=ycenter+ deltay\n    xleft=xcenter-deltax\n    xright=xcenter+ deltax   \n    new_img= img[ytop:ybottom, xleft:xright] # this is the xropped image    \n    new_img=cv2.resize(new_img, (shape[1], shape[0])) # must return original image size or else get an exception\n    new_img=new_img\/127.5-1 # scale pixels from -1 to + 1\n    return new_img\ngen=ImageDataGenerator(preprocessing_function=scaler)\ntrain_gen=gen.flow_from_dataframe( train_df, img_dir,x_col='image_id', y_col=target, target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\ntest_gen=gen.flow_from_dataframe( test_df, img_dir, x_col='image_id', y_col=target, target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\nvalid_gen=gen.flow_from_dataframe( valid_df, img_dir, x_col='image_id', y_col=target, target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\nclasses=list(train_gen.class_indices.keys())\nprint (classes)\nclass_count=len(classes)\n\n\n### create function to show some image examples","5b6be239":"def show_image_samples(gen ):\n    test_dict=test_gen.class_indices\n    classes=list(test_dict.keys())    \n    images,labels=next(gen) # get a sample batch from the generator \n    plt.figure(figsize=(20, 20))\n    length=len(labels)\n    if length<25:   #show maximum of 25 images\n        r=length\n    else:\n        r=25\n    for i in range(r):\n        plt.subplot(5, 5, i + 1)\n        image=(images[i]+1 )\/2 # scale images between 0 and 1 becaue pre-processor set them between -1 and +1\n        plt.imshow(image)\n        index=np.argmax(labels[i])\n        class_name=classes[index]\n        plt.title(class_name, color='blue', fontsize=16)\n        plt.axis('off')\n    plt.show()","3ee31df8":"show_image_samples(train_gen)","c8b7cd7a":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","c63c276e":"model_name='InceptionResNetV2'\nbase_model=tf.keras.applications.InceptionResNetV2(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \nx=base_model.output\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(1024, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx = Dense(128, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=.45, seed=123)(x)\n\noutput=Dense(class_count, activation='softmax')(x)\nmodel=Model(inputs=base_model.input, outputs=output)\nmodel.compile(Adamax(lr=.001), loss='categorical_crossentropy', metrics=['accuracy']) ","36446bd3":"class LRA(keras.callbacks.Callback):\n    reset=False\n    count=0\n    stop_count=0\n    tepochs=0\n    def __init__(self,model, patience,stop_patience, threshold, factor, dwell, model_name, freeze, initial_epoch):\n        super(LRA, self).__init__()\n        self.model=model\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.stop_patience=stop_patience\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.dwell=dwell\n        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it in self.lr\n        self.highest_tracc=0.0 # set highest training accuracy to 0\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity\n        #self.count=0 # initialize counter that counts epochs with no improvement\n        #self.stop_count=0 # initialize counter that counts how manytimes lr has been adjustd with no improvement  \n        self.initial_epoch=initial_epoch \n        #self.epochs=epochs\n        best_weights=self.model.get_weights() # set a class vaiable so weights can be loaded after training is completed        \n        msg=' '\n        if freeze==True:\n            msgs=f' Starting training using  base model { model_name} with weights frozen to imagenet weights initializing LRA callback'\n        else:\n            msgs=f' Starting training using base model { model_name} training all layers '            \n        print_in_color (msgs, (244, 252, 3), (55,65,80)) \n        \n    def on_epoch_begin(self,epoch, logs=None):\n        self.now= time.time()\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        later=time.time()\n        duration=later-self.now        \n        if epoch== self.initial_epoch or LRA.reset==True:  \n            LRA.reset=False           \n            msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^11s}{8:^8s}'.format('Epoch', 'Loss', 'Accuracy','V_loss','V_acc', 'LR', 'Next LR', 'Monitor', 'Duration')\n            print_in_color(msg, (244,252,3), (55,65,80)) \n            \n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        current_lr=lr\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        acc=logs.get('accuracy')  # get training accuracy \n        v_acc=logs.get('val_accuracy')\n        loss=logs.get('loss')\n        #print ( '\\n',v_loss, self.lowest_vloss, acc, self.highest_tracc)\n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            monitor='accuracy'\n            if acc>self.highest_tracc: # training accuracy improved in the epoch                \n                self.highest_tracc=acc # set new highest training accuracy\n                LRA.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n                self.count=0 # set count to 0 since training accuracy improved\n                self.stop_count=0 # set stop counter to 0\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss\n                color= (0,255,0)\n                self.lr=lr\n            else: \n                # training accuracy did not improve check if this has happened for patience number of epochs\n                # if so adjust learning rate\n                if self.count>=self.patience -1:\n                    color=(245, 170, 66)\n                    self.lr= lr* self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    self.stop_count=self.stop_count + 1\n                    if self.dwell:\n                        self.model.set_weights(LRA.best_weights) # return to better point in N space                        \n                    else:\n                        if v_loss<self.lowest_vloss:\n                            self.lowest_vloss=v_loss                                    \n                else:\n                    self.count=self.count +1 # increment patience counter                    \n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            monitor='val_loss'\n            if v_loss< self.lowest_vloss: # check if the validation loss improved \n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                LRA.best_weights=self.model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved  \n                self.stop_count=0  \n                color=(0,255,0)\n                self.lr=lr\n            else: # validation loss did not improve\n                if self.count>=self.patience-1:\n                    color=(245, 170, 66)\n                    self.lr=self.lr * self.factor # adjust the learning rate\n                    self.stop_count=self.stop_count + 1 # increment stop counter because lr was adjusted \n                    self.count=0 # reset counter\n                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    if self.dwell:\n                        self.model.set_weights(LRA.best_weights) # return to better point in N space\n                else: \n                    self.count =self.count +1 # increment the patience counter                    \n                if acc>self.highest_tracc:\n                    self.highest_tracc= acc\n        msg=f'{str(epoch+1):^3s}\/{str(LRA.tepochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{self.lr:^9.5f}{monitor:^11s}{duration:^8.2f}'\n        print_in_color (msg,color, (55,65,80))\n        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n            print_in_color(msg, (0,255,0), (55,65,80))\n            self.model.stop_training = True # stop training","ff4b24f2":"epochs =20\npatience= 1 # number of epochs to wait to adjust lr if monitored value does not improve\nstop_patience =3 # number of epochs to wait before stopping training if monitored value does not improve\nthreshold=.9 # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\nfactor=.5 # factor to reduce lr by\ndwell=True # experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch\nfreeze=False # if true free weights of  the base model\n\ncallbacks=[LRA(model=model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n                   factor=factor,dwell=dwell, model_name=model_name, freeze=freeze, initial_epoch=0 )]\nLRA.tepochs=epochs  # used to determine value of last epoch for printing\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","0f23b518":"def tr_plot(tr_data, start_epoch):\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)   \n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()\n","0d90a0fb":"def print_info( test_gen, preds, print_code, save_dir, subject ):\n    class_dict=test_gen.class_indices\n    labels= test_gen.labels\n    file_names= test_gen.filenames \n    error_list=[]\n    true_class=[]\n    pred_class=[]\n    prob_list=[]\n    new_dict={}\n    error_indices=[]\n    y_pred=[]\n    for key,value in class_dict.items():\n        new_dict[value]=key             # dictionary {integer of class number: string of class name}\n    # store new_dict as a text fine in the save_dir\n    classes=list(new_dict.values())     # list of string of class names\n    dict_as_text=str(new_dict)\n    dict_name= subject + '-' +str(len(classes)) +'.txt'  \n    dict_path=os.path.join(save_dir,dict_name)    \n    with open(dict_path, 'w') as x_file:\n        x_file.write(dict_as_text)    \n    errors=0      \n    for i, p in enumerate(preds):\n        pred_index=np.argmax(p)        \n        true_index=labels[i]  # labels are integer values\n        if pred_index != true_index: # a misclassification has occurred\n            error_list.append(file_names[i])\n            true_class.append(new_dict[true_index])\n            pred_class.append(new_dict[pred_index])\n            prob_list.append(p[pred_index])\n            error_indices.append(true_index)            \n            errors=errors + 1\n        y_pred.append(pred_index)    \n    if print_code !=0:\n        if errors>0:\n            if print_code>errors:\n                r=errors\n            else:\n                r=print_code           \n            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')\n            print_in_color(msg, (0,255,0),(55,65,80))\n            for i in range(r):                \n                split1=os.path.split(error_list[i])                \n                split2=os.path.split(split1[0])                \n                fname=split2[1] + '\/' + split1[1]\n                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(fname, pred_class[i],true_class[i], ' ', prob_list[i])\n                print_in_color(msg, (255,255,255), (55,65,60))\n                #print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])               \n        else:\n            msg='With accuracy of 100 % there are no errors to print'\n            print_in_color(msg, (0,255,0),(55,65,80))\n    if errors>0:\n        plot_bar=[]\n        plot_class=[]\n        for  key, value in new_dict.items():        \n            count=error_indices.count(key) \n            if count!=0:\n                plot_bar.append(count) # list containg how many times a class c had an error\n                plot_class.append(value)   # stores the class \n        fig=plt.figure()\n        fig.set_figheight(len(plot_class)\/3)\n        fig.set_figwidth(10)\n        plt.style.use('fivethirtyeight')\n        for i in range(0, len(plot_class)):\n            c=plot_class[i]\n            x=plot_bar[i]\n            plt.barh(c, x, )\n            plt.title( ' Errors by Class on Test Set')\n    y_true= np.array(labels)        \n    y_pred=np.array(y_pred)\n    if len(classes)<= 30:\n        # create a confusion matrix \n        cm = confusion_matrix(y_true, y_pred )        \n        length=len(classes)\n        if length<8:\n            fig_width=8\n            fig_height=8\n        else:\n            fig_width= int(length * .5)\n            fig_height= int(length * .5)\n        plt.figure(figsize=(fig_width, fig_height))\n        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n        plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.title(\"Confusion Matrix\")\n        plt.show()\n    clr = classification_report(y_true, y_pred, target_names=classes)\n    print(\"Classification Report:\\n----------------------\\n\", clr)","9c262318":"tr_plot(history,0)\nsave_dir=r'.\/'\nsubject='target'\nacc=model.evaluate( test_gen, batch_size=test_batch_size, verbose=1, steps=test_steps, return_dict=False)[1]*100\nmsg=f'accuracy on the test set is {acc:5.2f} %'\nprint_in_color(msg, (0,255,0),(55,65,80))\nsave_id=str (model_name +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')\nsave_loc=os.path.join(save_dir, save_id)\nmodel.save(save_loc)","13d5d52b":"print_code=0\npreds=model.predict(test_gen) \nprint_info( test_gen, preds, print_code, save_dir, subject )  ","7f6990ce":"### make predictions on test set and generate confusion matrix and classification report","0bb29db7":"### reduce the train_df in such a way as to balance the labels in the target column\n### first find out what value has the minimum number of representation in the target column","46709395":"### input an image and show as a large image with a grid. If you elect to crop images using the scaler\n### preprocessing function in the ImageDataGenerator you can view this image to determine the region of\n### interest to set the cropping paramaters. Note the ImageDataGenerator resizes the input images to\n### target_size (height,width) in mycase 224 by 224 so resize the image to that size","c0971527":"### create train, test, valid  generators. Use preprocessing_function  scaler in ImageDataGeneraror to\n### crop the image to a region of interest based on the attribute you selected to classify.","70388129":"### evaluate model on the test set then save the model","22c4c058":"### define function to generate the confusion matrix and classification report","f274fb37":"### read in attribute csv file","6e723394":"### Here is an example of using the scaler function to crop an image to a region of interest\nAssume you have selected the attribute 'eyeglasses' to classify. From the above image shape[0] is\n224 and shape[1]= 224. The center height of the eyes is at about 110 and the center x location is\nabout 110. Now this function will process many images in the ImageDataGenerator and not all images\nwill have these exact centers. So when setting the height of the cropped image make it larger than\nwhat the above image would indicate. Use for example a cropped image height of 60 and a cropped image\nwidth of say 100. So deltay=30 and deltax=50\nSo xcenter= 90 and ycenter= 110. \nIn the scaler function set ycenter=int(shape[0]\/2.04  )=int( 224\/2.03)  = 110. Similarly\nxcenter=int(shape[1]\/2)=int(224\/2.03)= 110,  deltax=int(shape[1]\/3.66)= int(224\/4.48 ) =50  and deltay=int(shape[0]\/7.5)= int(224\/7.47)=29\n","a0c1b641":"### define function to plot the training data","fc08e312":"### define function to print text in RGB foreground and background colors","48078193":"### target will probably be unbalaced as is the case for Pointy_Nose attribute balance make equal number of samples for each label\n### create a train_df, test_df and valid_df","9338eeda":"### select a column as a target to classify","0a644217":"### create the model","93fde735":"### instantiate the custom callback and train the model","d4c36851":"### create a subclass of callbacks to control learning rate and print training results for each epoch","66164e47":"### drop all attribute columns exclusive of the target column"}}