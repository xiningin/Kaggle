{"cell_type":{"7d98e973":"code","e28bac75":"code","2d16d63e":"code","53f3faf1":"code","4fb83c43":"code","56daa975":"code","496f9dcf":"code","689b1b9c":"code","7bbef81e":"code","bbfe4177":"code","7f86dce0":"code","1728aa94":"code","5a1471af":"code","5f38ee05":"code","90d0a4d7":"code","11006b37":"code","d4966717":"code","d95ee337":"code","1226ec03":"code","56b2482f":"code","7a602b0b":"code","5e3776cc":"code","3be2c9ad":"code","15cf623c":"code","f5044c83":"code","8418917f":"code","178ceb4e":"code","b9d82acb":"code","9cfd6826":"code","2cbcc67e":"code","9c39ef2f":"code","8a0daec5":"code","95431801":"code","7c8de3d0":"code","8816bd5e":"markdown","73b97f27":"markdown","075bb3dc":"markdown","752bf09d":"markdown","3505bf6c":"markdown","3e533cc4":"markdown","1af47d31":"markdown","81527a45":"markdown","23c6e33c":"markdown","b8f39a79":"markdown","ee4fe95d":"markdown","76c4524d":"markdown","b68c7d49":"markdown","c9e13d2d":"markdown","16f37462":"markdown"},"source":{"7d98e973":"#Importing the important libraries for algebra and data processing\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # statistical visualization\n\nimport matplotlib.pyplot as plt #matlab plots\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n\n#Loading data\ndata = pd.read_csv(\"..\/input\/student-mat.csv\", sep = ';') #Load the clean training data. Splitter is the semi-colon character","e28bac75":"data.head()","2d16d63e":"print ('The data has {0} rows and {1} columns'.format(data.shape[0],data.shape[1]))","53f3faf1":"data.info()","4fb83c43":"data.describe() #to look at the numerical fields and their describing mathematical values.","56daa975":"sns.distplot(data['G3']) #Plotting the distribution of the final grades.","496f9dcf":"corr = data.corr() # only works on numerical variables.\nsns.heatmap(corr)","689b1b9c":"print (corr['G3'].sort_values(ascending=False), '\\n')","7bbef81e":"groupColumns = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup'\n               , 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']\n\navgColumns = ['G3', 'G2', 'G1']","bbfe4177":"school = data.groupby(groupColumns[0])[avgColumns].mean()\nschool.head()","7f86dce0":"sex = data.groupby(groupColumns[1])[avgColumns].mean()\nsex.head()","1728aa94":"address = data.groupby(groupColumns[2])[avgColumns].mean()\naddress.head()","5a1471af":"famsize = data.groupby(groupColumns[3])[avgColumns].mean()\nfamsize.head()","5f38ee05":"Pstatus = data.groupby(groupColumns[4])[avgColumns].mean()\nPstatus.head()","90d0a4d7":"Mjob = data.groupby(groupColumns[5])[avgColumns].mean()\nMjob.head() #interesting results here. Children of fathers working in the health industry are doing significantly better than children\n            #of fathers at home or other.","11006b37":"Fjob = data.groupby(groupColumns[6])[avgColumns].mean()\nFjob.head()","d4966717":"reason = data.groupby(groupColumns[7])[avgColumns].mean()\nreason.head()","d95ee337":"guardian = data.groupby(groupColumns[8])[avgColumns].mean()\nguardian.head()","1226ec03":"schoolsup = data.groupby(groupColumns[9])[avgColumns].mean()\nschoolsup.head()","56b2482f":"famsup = data.groupby(groupColumns[10])[avgColumns].mean()\nfamsup.head()","7a602b0b":"paid = data.groupby(groupColumns[11])[avgColumns].mean()\npaid.head()","5e3776cc":"activities = data.groupby(groupColumns[12])[avgColumns].mean()\nactivities.head()","3be2c9ad":"nursery = data.groupby(groupColumns[13])[avgColumns].mean()\nnursery.head()","15cf623c":"higher = data.groupby(groupColumns[14])[avgColumns].mean()\nhigher.head() #another interesting field. ","f5044c83":"internet = data.groupby(groupColumns[15])[avgColumns].mean()\ninternet.head()","8418917f":"romantic = data.groupby(groupColumns[16])[avgColumns].mean()\nromantic.head()","178ceb4e":"focusGroupColumns = ['internet', 'guardian', 'Fjob']\naggs = data.groupby(focusGroupColumns)[avgColumns].mean()\nprint(aggs.to_string())","b9d82acb":"X = data.drop('G3', axis=1)\nY = data.G3\nX = pd.get_dummies(X) # to convert categorical data to a format that can be used in regression. This isn't the best method to use as it increases the\n                      # dimensionality of the dataset but it is a valid place to start\nX.info()","9cfd6826":"from sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .20, random_state = 42) # splitting data into 80% test and 20% train since the data is quite small. Usually it's best to use 60:40 or something similar\n                                                                                              # with the possibility of validation data for certain types of regression models to avoid overfitting.","2cbcc67e":"from sklearn import linear_model\n\nregr = linear_model.LinearRegression()\nregr.fit(X_train, Y_train)\n\npredicted = regr.predict(X_test)\nerr = Y_test - predicted\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Grade',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","9c39ef2f":"from sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor(max_features='auto')\ndtr.fit(X_train, Y_train)\npredicted = dtr.predict(X_test)\nerr = Y_test - predicted\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Grade',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","8a0daec5":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\n\ngbr = GradientBoostingRegressor(loss ='huber', max_depth=6)\ngbr.fit (X_train, Y_train)\npredicted = gbr.predict(X_test)\nerr = Y_test - predicted\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Grade',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","95431801":"from sklearn import neighbors\nknn = neighbors.KNeighborsRegressor(n_neighbors=6)\nknn.fit(X_train, Y_train)\n\npredicted = knn.predict(X_test)\nerr = Y_test - predicted\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Grade',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","7c8de3d0":"#Exporting\nfrom sklearn.externals import joblib\n#joblib.dump(knn, 'model.pkl') #This will produce a model file that we can import later in a web based python script and possibly take input from a web\/mobile application and predict the G3 score","8816bd5e":"From this we can tell that the distribution of the grades is decent and doesn't require any further skewness correction yet. We can go with this distribution for now to analyze the data and create a primitive model and it's error rate first. We can look into data processing of the G3 field afterwards if the results aren't satisfactory. ","73b97f27":"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n**Model**\nNow we will start working on a regression model that we can train and use to predict future records.","075bb3dc":"Lets start with a general linear regression model and see how it goes from there","752bf09d":"Since the KNN Model was the best model we had so far with a variance score of 0.79, we can select it as our current prediction model.","3505bf6c":"Decision Tree","3e533cc4":"KNN Regression","1af47d31":"Next up, we will take the fields (columns) one by one to analyze their importance and effect on the G3 value:\n\n1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n2 sex - student's sex (binary: 'F' - female or 'M' - male)\n3 age - student's age (numeric: from 15 to 22)\n4 address - student's home address type (binary: 'U' - urban or 'R' - rural)\n5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n16 schoolsup - extra educational support (binary: yes or no)\n17 famsup - family educational support (binary: yes or no)\n18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n19 activities - extra-curricular activities (binary: yes or no)\n20 nursery - attended nursery school (binary: yes or no)\n21 higher - wants to take higher education (binary: yes or no)\n22 internet - Internet access at home (binary: yes or no)\n23 romantic - with a romantic relationship (binary: yes or no)\n24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n29 health - current health status (numeric: from 1 - very bad to 5 - very good)\n30 absences - number of school absences (numeric: from 0 to 93)","81527a45":"Now we will start exploring the data to see what it has and look at what we can do with it.","23c6e33c":"In this kernel, I will look into a dataset containing student performance data acquired from the University of Minho, Portugal. I will be using the student-mat portion of the Dataset.#","b8f39a79":"Now that we have analyzed the numerical data slightly and figured out the most correlated fields, we now have to take a look at the categorical data to figure out how useful the fields may be and how to introduce them into the prediction model. The simplest way to analyze those fields is to compare the means accross the categories. ","ee4fe95d":"From this, we see that Gabriel Pereira students generally do better than Mousinho da Silveira students. The same analysis can be done for a few more fields:","76c4524d":"From the info available on the data, we can tell that the quality of the data is quite decent as there aren't any columns with null values and every cell has a single piece of data. This will significanlty simplify the processing stage of the data as we would not be required to compensate for null values or split dynamic data. However, there are many categorical fields in the data set and that requires some additional processing to generate better results from.","b68c7d49":"From the correlation graph above, we can look at the numerical fields to know the values that affect the end result the most. Obviously G2 and G1 are the most correlated fields to G3 as they are part of the calculation formula for G3 so they will have the greatest effect on our prediction. \nAnother thing we can see is the negative correlation between failures and the G3 result. This also makes quite a lot of sense as more failures tend to negatively affect your end score.\nAbsences and free time seem to not be very relevant in the dataset that are analyzing which can be a flag that may help us further understand the data in the future. ","c9e13d2d":"We can also generate an aggregate summary of the means of the most valuable fields we found: internet, guardian and Fjob","16f37462":"XGBooster Regression"}}