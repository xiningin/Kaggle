{"cell_type":{"effb9e80":"code","6e49d761":"code","7f8e143f":"code","9c08e757":"code","68c70ffb":"code","1d56e845":"code","b0a8603f":"code","756115b0":"code","c9ed9319":"code","81061868":"code","7f00dc98":"code","875a8878":"code","cf5179d0":"code","450f968a":"code","ffa4a7ff":"code","2b31c194":"code","b790b99e":"code","2e61905d":"code","368247ec":"code","adf37c6c":"code","0fd769ca":"code","2f85f24b":"code","95325539":"code","6ce888d5":"code","54b04e9d":"code","00cc3675":"code","ac738b81":"code","dc13d443":"code","fb93ec2f":"code","9b9d033a":"code","a4fab5d3":"code","2e9251c2":"code","c2a81891":"code","e108ea6e":"code","f73cbb6c":"code","e46c15a9":"code","a8de3762":"code","5235197f":"code","8b825319":"code","b78dac7b":"code","46ac9a54":"code","66f03145":"code","8d82dd71":"code","bf6d9185":"code","64b15ad4":"code","7552c3e6":"code","afef9551":"code","297c2df1":"code","c6859727":"code","4093dc08":"code","4b2d0ebb":"code","2f18ef17":"code","dd6ff570":"code","74f067e0":"code","da940a6f":"code","bfebf07a":"code","a0888e5a":"code","ddb29ae1":"code","1901b944":"code","5393b3c4":"code","37eb8081":"code","20cf38b3":"code","8d9bf616":"code","dcd75446":"code","3265e253":"code","6d033bc7":"code","f0c66832":"code","3b7c90c4":"code","b8b425f1":"code","515d51bc":"markdown","bab615b6":"markdown","54fa3115":"markdown","10781b06":"markdown","ed8d7ac2":"markdown","06a543ba":"markdown","eeddc3ef":"markdown","15842d48":"markdown","0ebc7448":"markdown","8c4188d5":"markdown","1169a0b5":"markdown","d72c899d":"markdown","1489d2b9":"markdown","1606c6fe":"markdown","0f56312b":"markdown","3ed1a596":"markdown","3748b833":"markdown","fa536ceb":"markdown","206af58d":"markdown","a50182a4":"markdown"},"source":{"effb9e80":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nROW_NUMS = 10000\n# raw_train = pd.read_csv(\"..\/input\/kagglecamp2022\/train.csv\",nrows=ROW_NUNS)\nraw_train = pd.read_csv(\"..\/input\/kagglecamp2022\/train.csv\", nrows=ROW_NUMS)","6e49d761":"raw_train","7f8e143f":"category_cols = []\ncolunq_dict = dict()\nfor col in raw_train.columns:\n    colunq_dict[col] = len(raw_train[col].unique())\n    \n    if 'Cover_Type' == col:\n        continue\n    \n    if colunq_dict[col] < 40:\n        category_cols.append(col)\ncolunq_dict","9c08e757":"raw_train['Cover_Type'].value_counts() \/ raw_train.shape[0]","68c70ffb":"from sklearn.preprocessing import OneHotEncoder\ndef ohe_encoding(input_df,OheEncoder_dict = None):\n    \n    \n    isTrain = False\n    df = input_df\n    category_cols = []\n    \n    for col in df.columns:\n        if col in ['id','Cover_Type']:\n            continue\n\n        if len(df[col].unique()) < 40:\n            category_cols.append(col)\n    \n    \n    if OheEncoder_dict is None:\n        print(\"OheEncoder_list\uac00 \ube44\uc5c8\uc2b5\ub2c8\ub2e4.\")\n        print(\"Training mode is activate!\")\n        isTrain = True\n    \n    \n    if isTrain:\n        input_ids = df['id'] \n        targets = df['Cover_Type']\n        df = df.drop(columns = ['id','Cover_Type'])\n        \n        \n        \n        ohe_dict = dict()\n        ohe_df = raw_train\n        for col in category_cols:\n\n            \n            ohe = OneHotEncoder()\n            ohe_df = ohe.fit_transform(df[[col]]).toarray().astype(int)\n            ohe_df = pd.DataFrame(ohe_df, columns=[str(col)+\"_\" + str(value) for value in ohe.categories_[0]])\n            df = df.join(ohe_df)\n            df = df.drop(columns = col)\n\n            ohe_dict[col] = ohe\n            \n            print(col,'is onehot encoded')\n        df['id'] = input_ids\n        df['Cover_Type'] = targets\n        return ohe_dict, df\n    \n    else:\n        print(\"Validation(Test) mode is activate!\")\n        input_ids = df['id']\n        df = df.drop(columns = 'id')\n        \n        \n        for col in category_cols:\n            if col not in ['id','Cover_Type']:\n                ohe_df = OheEncoder_dict[col].transform(df[[col]]).toarray().astype(int)\n                ohe_df = pd.DataFrame(ohe_df, columns=[str(col)+\"_\" + str(value) for value in OheEncoder_dict[col].categories_[0]])\n                df = df.join(ohe_df)\n                df = df.drop(columns = col)            \n            print(col,'is onehot encoded')\n        df['id'] = input_ids\n        return OheEncoder_dict, df","1d56e845":"ohe_list, ohe_train = ohe_encoding(raw_train)","b0a8603f":"ohe_list","756115b0":"ohe_train.info()","c9ed9319":"ohe_train.info()","81061868":"from sklearn.preprocessing import StandardScaler\n\ndef std_encoding(input_df,scaler_list = None):\n    category_cols = []\n    df = input_df\n    for col in df.columns:\n        if col in ['id','Cover_Type']:\n            continue\n\n        if len(df[col].unique()) < 40:\n            category_cols.append(col)\n\n            \n            \n    df = input_df\n    isTrain = False\n    \n    \n            \n    if scaler_list is None:\n        isTrain = True\n        \n    if isTrain: #Train set\uc744 \uc0ac\uc6a9\ud560 \ub54c\n        scalers = dict()\n        input_ids = df['id']\n        targets = df['Cover_Type']\n        df = df.drop(columns = ['id','Cover_Type'])\n        \n        \n        for col in df:\n            if col not in category_cols:\n                series = df[[col]]\n                Scaler = StandardScaler()\n                df[col] = pd.DataFrame(\n                    Scaler.fit_transform(series),\n                    index=series.index\n                )\n                print(col,\"is scaled!\")\n                scalers[col] = Scaler\n                \n        df['id'] = input_ids\n        df['Cover_Type'] = targets\n        return scalers, df\n\n    else: # \ud6c8\ub828\ub41c \uc778\ucf54\ub354\ub85c val set, test set\uc744 \uc778\ucf54\ub529\ud560 \ub54c (\ud55c\ubc88\uc5d0 1\uac1c\uc529)\n        if scaler_list is None:\n            print(\"\ud6c8\ub828\ub41c StandardScaler list\ub97c \uc785\ub825\ud574\uc8fc\uc138\uc694!\")\n            return\n\n        input_ids = df['id']\n        df = df.drop(columns = 'id')\n        \n        for col in df:\n            if col not in category_cols:\n                df[col] = scaler_list[col].transform(df[[col]])\n                print(col,\"is scaled!\")\n\n\n        df['id'] = input_ids\n        \n        return scaler_list, df","7f00dc98":"scalers, std_train = std_encoding(ohe_train)","875a8878":"scalers","cf5179d0":"std_train.describe()","450f968a":"def impute_na_from_norm(input_df,MeanDict = None):\n    '''\n    fill missing value from norm(mean = col's mean. std = col's sd) groupby Target('label') values\n    return mean & std (for impute missing value at Test data set), imputed_data\n    '''\n    df = input_df\n    meanstd_dict = dict()\n    isTrain = False\n    \n    category_cols = []\n    for col in df.columns:\n        if col in ['id','Cover_Type']:\n            continue\n\n        if len(df[col].unique()) < 40:\n            category_cols.append(col)\n    \n    \n    \n    \n    if MeanDict is None:\n        print('mean std dict is None')\n        isTrain = True\n    \n    \n    \n    \n    \n    if isTrain:\n        \n        input_ids = df['id']\n        df = df.drop(columns = 'id')\n        \n        \n        for col in df:\n            if col not in category_cols and col != 'Cover_Type':\n                meanstd_dict[col] = df[[col,'Cover_Type']].groupby(by='Cover_Type').agg(['mean','std'])\n                for label_value in range(1, 4+ 1): # num of lable's unique value\n                    df[col] = df[col].apply(lambda x : np.random.normal(meanstd_dict[col][(col,'mean')][label_value], meanstd_dict[col][(col,'std')][label_value]) if x != x else x) \n                    # if x is nan -> x != x is return True\n    \n        df['id'] = input_ids\n        \n        return meanstd_dict, df       \n    else:\n        meanstd_dict = MeanDict\n        \n        input_ids = df['id']\n        df = df.drop(columns = 'id')\n        \n        for col in df:\n            if col not in category_cols:\n                for label_value in range(1, 4+ 1):\n                    df[col] = df[col].apply(lambda x : np.random.normal(meanstd_dict[col][(col,'mean')][label_value], meanstd_dict[col][(col,'std')][label_value]) if x != x else x) \n                    # if x is nan -> x != x is return True\n    \n        df['id'] = input_ids\n        return meanstd_dict, df       \n    ","ffa4a7ff":"meanstd_train, impute_train = impute_na_from_norm(std_train)","2b31c194":"len(meanstd_train), impute_train.shape","b790b99e":"std_train.isnull().sum()","2e61905d":"impute_train.isnull().sum()","368247ec":"from plotly.offline import iplot\nimport plotly.graph_objs as go\n\nfor col in impute_train:       \n    \n    if 'id' != col and 'Soil_' not in col and 'Wilderness_Area_' not in col:\n        trace1 = go.Histogram(\n        x=std_train[col],\n        opacity=0.5,\n        name = col+\"_scaled\")\n\n\n        trace2 = go.Histogram(\n        x=impute_train[col],\n        opacity=0.5,\n        name = col+\"_imputed\")\n        \n        data = [trace1, trace2]\n        layout = go.Layout(barmode='overlay',\n                           title='Distribution for imputing missing value',\n                           xaxis=dict(title=col),\n                           yaxis=dict(title='Count')\n                            )\n\n        fig = go.Figure(data=data, layout=layout)\n        iplot(fig)\n","adf37c6c":"impute_train['Distance_To_Hydrology'] = ( impute_train['Horizontal_Distance_To_Hydrology']**2 + impute_train['Vertical_Distance_To_Hydrology']**2 )**0.5\nimpute_train[['Distance_To_Hydrology','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology']].head()","0fd769ca":"data = []\n\nfor col in impute_train:\n    if \"Distance\" in col:\n\n        trace1 = go.Histogram(\n        x=impute_train[col],\n        opacity=0.5,\n        name = col)\n        data.append(trace1)\n        \nlayout = go.Layout(barmode='overlay',\n                   title='Distance features',\n                   xaxis=dict(title=\"Distance\"),\n                   yaxis=dict(title='Count')\n                    )\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)\n","2f85f24b":"def feautre_engineering(input_df):\n    df = input_df\n    df['Distance_To_Hydrology'] = ( df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2 )**0.5\n    fe_columns = ['Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Distance_To_Hydrology']\n\n    df['fe_Distance_sum'] = df[fe_columns].sum(axis=1)\n    df['fe_Distance_mean'] = df[fe_columns].mean(axis=1)\n    df['fe_Distance_max'] = df[fe_columns].max(axis=1)\n    df['fe_Distance_min'] = df[fe_columns].min(axis=1)\n    \n#     df = df.drop(columns = fe_columns)\n#     df = df.drop(columns = ['fe_Distance_mean','fe_Distance_max','fe_Distance_min'])\n    \n    return df","95325539":"fe_train = feautre_engineering(impute_train)","6ce888d5":"data = []\n\nfor col in fe_train:\n    if \"fe\" in col:\n\n        trace1 = go.Histogram(\n        x=impute_train[col],\n        opacity=0.5,\n        name = col)\n        data.append(trace1)\n        \nlayout = go.Layout(barmode='overlay',\n                   title='Feature Engineering from [sum, mean, max, min]',\n                   xaxis=dict(title=\"scaled fe Value\"),\n                   yaxis=dict(title='Count')\n                    )\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","54b04e9d":"def feautre_engineering(input_df):\n    df = input_df\n    df['Distance_To_Hydrology'] = ( df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2 )**0.5\n    fe_columns = ['Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Distance_To_Hydrology']\n\n    df['fe_Distance_sum'] = df[fe_columns].sum(axis=1)\n    df['fe_Distance_mean'] = df[fe_columns].mean(axis=1)\n    df['fe_Distance_max'] = df[fe_columns].max(axis=1)\n    df['fe_Distance_min'] = df[fe_columns].min(axis=1)\n    \n    df = df.drop(columns = fe_columns)\n    df = df.drop(columns = ['fe_Distance_mean','fe_Distance_max','fe_Distance_min'])\n    df = df.drop(columns = ['Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Hydrology'])\n    \n    \n    return df","00cc3675":"data = []\n\nfor col in fe_train:\n    if \"Distance\" in col:\n\n        trace1 = go.Histogram(\n        x=fe_train[col],\n        opacity=0.5,\n        name = col)\n        data.append(trace1)\n\nlayout = go.Layout(barmode='overlay',\n               title='row_source',\n               xaxis=dict(title=\"scaled Value\"),\n               yaxis=dict(title='Count')\n                )\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)\n","ac738b81":"from scipy.stats import skew, kurtosis\nskew_dict = dict() \n\nfor col in fe_train:\n    if 'Distance' in col:\n        # \uc65c\ub3c4\n        skew_dict[col] = skew(impute_train[col])","dc13d443":"skew_dict","fb93ec2f":"fe_train.columns","9b9d033a":"corr_cols = []\nfor col in fe_train.columns:\n    if 'Soil_Type_' not in col and 'Wilderness_Area_' not in col:\n        corr_cols.append(col)","a4fab5d3":"corr_cols","2e9251c2":"raw_corr = impute_train[corr_cols].corr()\nfig = plt.figure(figsize=(20,6))\nimport numpy as np\nmask = np.triu(raw_corr)\nimport seaborn as sns\nsns.heatmap(raw_corr, \n               annot = True,      \n               cmap = 'RdYlBu_r',  \n               vmin = -1, vmax = 1, \n               mask=mask\n              )\nplt.show()","c2a81891":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nraw_train = pd.read_csv(\"..\/input\/kagglecamp2022\/train.csv\")","e108ea6e":"ohe_list, ohe_train = ohe_encoding(raw_train)","f73cbb6c":"scalers, std_train = std_encoding(ohe_train)","e46c15a9":"meanstd_train, impute_train = impute_na_from_norm(std_train)","a8de3762":"fe_train = feautre_engineering(impute_train)","5235197f":"del raw_train, ohe_train, std_train, impute_train","8b825319":"fe_train.shape","b78dac7b":"raw_test = pd.read_csv(\"..\/input\/kagglecamp2022\/test.csv\")","46ac9a54":"ohe_list","66f03145":"ohe_list, ohe_test = ohe_encoding(raw_test,ohe_list)","8d82dd71":"del raw_test","bf6d9185":"ohe_test.shape","64b15ad4":"scalers","7552c3e6":"scalers, std_test = std_encoding(ohe_test,scalers)","afef9551":"del ohe_test","297c2df1":"std_test","c6859727":"meanstd_train.keys()","4093dc08":"meanstd_train, impute_test = impute_na_from_norm(std_test,meanstd_train)","4b2d0ebb":"del std_test","2f18ef17":"impute_test.isnull().sum().sum()","dd6ff570":"impute_test.head()","74f067e0":"fe_test = feautre_engineering(impute_test)","da940a6f":"del impute_test","bfebf07a":"fe_train.shape ,fe_test.shape","a0888e5a":"inter_cols = list(set(fe_train.columns).intersection(fe_test.columns))\nlen(inter_cols)","ddb29ae1":"train_label = fe_train['Cover_Type']","1901b944":"train,test = fe_train.align(fe_test, join='inner', axis=1)","5393b3c4":"train.shape, test. shape","37eb8081":"from sklearn.model_selection import train_test_split\nX_train_input = train.drop(columns = 'id')\nX_train_input.shape","20cf38b3":"X_test_ids = test['id']\nX_test_input = test.drop(columns = 'id')\nX_test_input.shape","8d9bf616":"X_train, X_val, y_train, y_val = train_test_split(X_train_input, train_label,\n                                                  stratify = train_label,\n                                                  test_size = 0.2)","dcd75446":"X_train.shape","3265e253":"from sklearn.metrics import accuracy_score, confusion_matrix,f1_score, make_scorer\n\n# f1_scorer = make_scorer(f1_score,average='macro')","6d033bc7":"from sklearn.neighbors import KNeighborsClassifier","f0c66832":"knn_clf = KNeighborsClassifier(n_neighbors=len(train_label.unique()), algorithm='auto',weights='distance')","3b7c90c4":"# knn_clf.fit(X_train,y_train)\nknn_clf.fit(X_train_input,train_label)","b8b425f1":"submission = pd.read_csv(\"..\/input\/kagglecamp2022\/sample_submission.csv\")\nknn_pred = knn_clf.predict(X_test_input)\nsubmission['Cover_Type'] = knn_pred\nsubmission.to_csv(\"submission.csv\",index=False)","515d51bc":"## Standard Scaling","bab615b6":"## A-1. One hot encoding for Categorical features","54fa3115":"## A-4. Feature Engineering","10781b06":"## OneHot Encoding","ed8d7ac2":"## Feature Engineering","06a543ba":"- -2 < skew < +2 : Data mostly are located at center\n    - Distance_To_Hydrology have a tail","eeddc3ef":"# Test data preprocessing","15842d48":"- missing value is imputed well from Norm(TargetMean, Target's std) ","0ebc7448":"# \ub370\uc774\ud130 \uc14b \ubd84\ub9ac","8c4188d5":"# A-2 StandardScaling for every numeric features","1169a0b5":"## Impute missing value","d72c899d":"## Macth train and test set's features\n- If Train's some features not in Test's features, drop it\n    - Trainining process must be trained by Training distribution\n    - Because <We don't have a REAL(FUTURE) data>\n    - just Do handle test set with same way for training set","1489d2b9":"- fe_sum similar to Norm!!!","1606c6fe":"- Distance_To_Hydrology looks like Normal distributation\n    - so, used it instead of  Horizontal_Distance_To_Hydrology & Vertical_Distance_To_Hydrology","0f56312b":"# Train data set Load","3ed1a596":"- most categorical feature's uniaue value are under 40!","3748b833":"# A. Preprocessing with Distance & Hillshade Features","fa536ceb":"Both Training set and test set had some features(50 columns)","206af58d":"### To Make Euclidean  Distance with ( Horizontal_Distance_To_Hydrology & Vertical_Distance_To_Hydrology )","a50182a4":"## A-3 Fill missing value with Normal distribution (Target Mean)\nfill missing value from norm(mean = col's mean. std = col's sd) groupby Target('label') values"}}