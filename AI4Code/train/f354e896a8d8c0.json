{"cell_type":{"3ad7cb99":"code","c76ab245":"code","606e46ab":"code","0987ba46":"code","0150e78a":"code","d06bb5ed":"code","54566189":"code","16500029":"code","eb352e30":"code","03c00f18":"code","882aa708":"code","bb615da5":"code","c6045de4":"code","9d57eab8":"code","a6c41069":"code","9370a02e":"code","22ee1ef2":"code","31cef23f":"code","fff81a3e":"code","d90dacff":"code","8a35a53d":"code","432c62da":"code","bbb0151c":"code","32d07ef9":"code","a23e28b0":"code","0501a636":"code","cfb48435":"code","21a6d418":"code","f419a9ee":"code","4b22b3f6":"code","14f17340":"code","da02f051":"code","f3f84c29":"code","3e67d34a":"code","a9260998":"code","44eb16f7":"code","549b7e0b":"code","7b7b6e24":"markdown","e9191b58":"markdown","1d805192":"markdown","dbb61004":"markdown","35e9cdb7":"markdown","b8eebd75":"markdown","a4076d3a":"markdown","0a9a8f54":"markdown","c34687e8":"markdown","0bc315c2":"markdown","7843d278":"markdown","2f53f52c":"markdown","cf091790":"markdown","da48f47d":"markdown","4bb7661b":"markdown","17ffb56a":"markdown","029ea3aa":"markdown","3f22663a":"markdown","27a8e53e":"markdown","a0acdfb5":"markdown","c50f4b35":"markdown","62d4e6f5":"markdown","6f508149":"markdown","55b51929":"markdown","72495550":"markdown","fe229d95":"markdown","a74f7433":"markdown","0590220a":"markdown","5a4c57d6":"markdown","040dad51":"markdown","3730bcd8":"markdown","697c5318":"markdown","03299018":"markdown","4a5775e9":"markdown","1bac8356":"markdown","9a8b2cf3":"markdown","00070276":"markdown","f212b077":"markdown","8c669077":"markdown"},"source":{"3ad7cb99":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/Ob (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\nimport seaborn as sns\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","c76ab245":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport time\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter('ignore', UserWarning)\n\nimport gc\ngc.enable()","606e46ab":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')","0987ba46":"categorical_feats = [\n    f for f in train.columns if train[f].dtype == 'object'\n]\n\ncategorical_feats\nfor f_ in categorical_feats:\n    train[f_], _ = pd.factorize(train[f_])\n    # Set feature type as categorical\n    train[f_] = train[f_].astype('category')","0150e78a":"def get_feature_importances(data, shuffle, seed=None):\n    # Gather real features\n    train_features = [f for f in data if f not in ['target', 'id']]\n    # Go over fold and keep track of CV score (train and valid) and feature importances\n    \n    # Shuffle target if required\n    y = data['target'].copy()\n    if shuffle:\n        # Here you could as well use a binomial distribution\n        y = data['target'].copy().sample(frac=1.0)\n    \n    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n    lgb_params = {\n        'objective': 'binary',\n        'boosting_type': 'rf',\n        'subsample': 0.623,\n        'colsample_bytree': 0.7,\n        'num_leaves': 127,\n        'max_depth': 8,\n        'seed': seed,\n        'bagging_freq': 1,\n        'n_jobs': 4\n    }\n    \n    # Fit the model\n    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats)\n\n    # Get feature importances\n    imp_df = pd.DataFrame()\n    imp_df[\"feature\"] = list(train_features)\n    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n    imp_df['trn_score'] = roc_auc_score(y, clf.predict(data[train_features]))\n    \n    return imp_df","d06bb5ed":"\n# Seed the unexpected randomness of this world\nnp.random.seed(123)\n# Get the actual importance, i.e. without shuffling\nactual_imp_df = get_feature_importances(data=train, shuffle=False)","54566189":"actual_imp_df.head()","16500029":"%%time\nnull_imp_df = pd.DataFrame()\nnb_runs = 80\nimport time\nstart = time.time()\ndsp = ''\nfor i in range(nb_runs):\n    # Get current run importances\n    imp_df = get_feature_importances(data=train, shuffle=True)\n    imp_df['run'] = i + 1 \n    # Concat the latest importances with the old ones\n    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n    # Erase previous message\n    for l in range(len(dsp)):\n        print('\\b', end='', flush=True)\n    # Display current run and time used\n    spent = (time.time() - start) \/ 60\n    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n    print(dsp, end='', flush=True)","eb352e30":"null_imp_df.head()","03c00f18":"def display_distributions(actual_imp_df_, null_imp_df_, feature_):\n    plt.figure(figsize=(13, 6))\n    gs = gridspec.GridSpec(1, 2)\n    # Plot Split importances\n    ax = plt.subplot(gs[0, 0])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n    # Plot Gain importances\n    ax = plt.subplot(gs[0, 1])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())","882aa708":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='bin_0')","bb615da5":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='bin_1')","c6045de4":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='bin_2')","9d57eab8":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='bin_3')","a6c41069":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='bin_4')","9370a02e":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='nom_0')","22ee1ef2":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='nom_1')","31cef23f":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='nom_2')","fff81a3e":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='nom_3')","d90dacff":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='nom_4')","8a35a53d":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='nom_5')","432c62da":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='nom_6')","bbb0151c":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='nom_7')","32d07ef9":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='nom_8')","a23e28b0":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='nom_9')","0501a636":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='ord_0')","cfb48435":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='ord_1')","21a6d418":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='ord_2')","f419a9ee":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='ord_3')","4b22b3f6":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='ord_4')","14f17340":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='ord_5')","da02f051":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='day')","f3f84c29":"display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='month')","3e67d34a":"%%time\nfeature_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n    gain_score = np.log(1e-10 + f_act_imps_gain \/ (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n    split_score = np.log(1e-10 + f_act_imps_split \/ (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n    feature_scores.append((_f, split_score, gain_score))\n\nscores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n\nplt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()","a9260998":"# null_imp_df = pd.read_csv('null_importances_distribution_rf.csv')\n# actual_imp_df = pd.read_csv('actual_importances_ditribution_rf.csv')\n\n# actual_imp_df = actual_imp_df[['feature','importance_gain','importance_split','trn_score']].copy()\n# null_imp_df = null_imp_df[['feature','importance_gain','importance_split','trn_score']].copy()","44eb16f7":"%%time\ncorrelation_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values\n    gain_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() \/ f_null_imps.size\n    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values\n    split_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() \/ f_null_imps.size\n    correlation_scores.append((_f, split_score, gain_score))\n\ncorr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])\n\nfig = plt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='split_score', y='feature', data=corr_scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=corr_scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()\nplt.suptitle(\"Features' split and gain scores\", fontweight='bold', fontsize=16)\nfig.subplots_adjust(top=0.93)","549b7e0b":"%%time\ndef score_feature_selection(df=None, train_features=None, cat_feats=None, target=None):\n    # Fit LightGBM \n    dtrain = lgb.Dataset(df[train_features], target, free_raw_data=False, silent=True)\n    lgb_params = {\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'learning_rate': .1,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'num_leaves': 31,\n        'max_depth': -1,\n        'seed': 13,\n        'n_jobs': 4,\n        'min_split_gain': .00001,\n        'reg_alpha': .00001,\n        'reg_lambda': .00001,\n        'metric': 'auc'\n    }\n    \n    # Fit the model\n    hist = lgb.cv(\n        params=lgb_params, \n        train_set=dtrain, \n        num_boost_round=2000,\n        categorical_feature=cat_feats,\n        nfold=5,\n        stratified=True,\n        shuffle=True,\n        early_stopping_rounds=50,\n        verbose_eval=0,\n        seed=17\n    )\n    # Return the last mean \/ std values \n    return hist['auc-mean'][-1], hist['auc-stdv'][-1]\n\nfor threshold in [0, 5, 10, 15 ,20 , 25]:\n    split_feats = [_f for _f, _score, _ in correlation_scores if _score >= threshold]\n    split_cat_feats = [_f for _f, _score, _ in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n    gain_feats = [_f for _f, _, _score in correlation_scores if _score >= threshold]\n    gain_cat_feats = [_f for _f, _, _score in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n                                                                                             \n    print('Results for threshold %3d' % threshold)\n    split_results = score_feature_selection(df=train, train_features=split_feats, cat_feats=split_cat_feats, target=train['target'])\n    print('\\t SPLIT : %.6f +\/- %.6f' % (split_results[0], split_results[1]))\n    gain_results = score_feature_selection(df=train, train_features=gain_feats, cat_feats=gain_cat_feats, target=train['target'])\n    print('\\t GAIN  : %.6f +\/- %.6f' % (gain_results[0], gain_results[1]))","7b7b6e24":"#### display_distributions - nom_7","e9191b58":"#### display_distributions - nom_6","1d805192":"#### display_distributions - nom_2","dbb61004":"> Save data","35e9cdb7":"# Final","b8eebd75":"#### display_distributions - ord_3","a4076d3a":"## Categorical Feature Encoding Challenge WITH PYTHON\n[Crisl\u00e2nio Mac\u00eado](https:\/\/medium.com\/sapere-aude-tech) -  Kernel Created - December, 31th, 2019. Last Update- January, 08th, 2019\n\n\ud83d\udc31 Cat with Null Importance - Target Permutation: [\u26a1\ud83d\udd0cCat with Null Importance - Target Permutation](https:\/\/www.kaggle.com\/caesarlupum\/cat-with-null-importance-target-permutation)\n\n\n- [**Github**](https:\/\/github.com\/crislanio)\n- [**Linkedin**](https:\/\/www.linkedin.com\/in\/crislanio\/)\n- [**Medium**](https:\/\/medium.com\/sapere-aude-tech)\n- [**Quora**](https:\/\/www.quora.com\/profile\/Crislanio)\n- [**Ensina.AI**](https:\/\/medium.com\/ensina-ai\/an%C3%A1lise-dos-dados-abertos-do-governo-federal-ba65af8c421c)\n- [**Hackerrank**](https:\/\/www.hackerrank.com\/crislanio_ufc?hr_r=1)\n- [**Blog**](https:\/\/medium.com\/@crislanio.ufc)\n- [**Personal Page**](https:\/\/crislanio.wordpress.com\/about)\n- [**Twitter**](https:\/\/twitter.com\/crs_macedo)\n\n----------\n----------","0a9a8f54":"#### display_distributions - ord_5","c34687e8":"### Feature selecture using target permutation\n##### Feature selection process using target permutation tests actual importance significance against the distribution of feature importances when fitted to noise (shuffled target). The notebook implements the following steps :\n\n- Create the null importances distributions : these are created fitting the model over several runs on a shuffled version of the target. This shows how the model can make sense of a feature irrespective of the target.\n\n- Fit the model on the original target and gather the feature importances. This gives us a benchmark whose significance can be tested against the Null Importances Distribution for each feature test the actual importance:\n\n- Compute the probabability of the actual importance wrt the null distribution. I will use a very simple estimation using occurences while the article proposes to fit known distribution to the gathered data. In fact here I'll compute 1 - the proba so that things are in the right order.\n\n- Simply compare the actual importance to the mean and max of the null importances. This will give sort of a feature importance that allows to see major features in the dataset. Indeed the previous method may give us lots of ones.\nInspired by:\n\nLook: https:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances (upvote this !) Not only useful but also valuable","0bc315c2":"#### Create a scoring function\n> Scoring function uses LightGBM in RandomForest mode fitted on the full dataset","7843d278":"#### display_distributions - month","2f53f52c":"#### display_distributions - ord_1","cf091790":"#### display_distributions - bin_2","da48f47d":"#### display_distributions - nom_1","4bb7661b":"# Display distribution examples\n#### A few plots are better than any words","17ffb56a":"#### display_distributions - bin_1","029ea3aa":"Null Importance","3f22663a":"### Build Null Importances distribution","27a8e53e":"\n<html>\n<body>\n\n<p><font size=\"5\" color=\"blue\">If you like my kernel please consider upvoting it<\/font><\/p>\n<p><font size=\"4\" color=\"purple\">Remember the upvote button is next to the fork button, and it's free too! ;)<\/font><\/p>\n\n<\/body>\n<\/html>\n","a0acdfb5":"#### display_distributions - day","c50f4b35":"#### display_distributions - ord_2","62d4e6f5":"Actual Importance","6f508149":"#### display_distributions - nom_5","55b51929":"#### display_distributions - nom_1","72495550":"### Feature Importances","fe229d95":"\n<html>\n<body>\n\n<p><font size=\"5\" color=\"blue\">If you like my kernel please consider upvoting it<\/font><\/p>\n<p><font size=\"4\" color=\"purple\">Remember the upvote button is next to the fork button, and it's free too! ;)<\/font><\/p>\n\n<\/body>\n<\/html>\n","a74f7433":"#### display_distributions - ord_4","0590220a":"The notebook uses a procedure described in this [article](https:\/\/academic.oup.com\/bioinformatics\/article\/26\/10\/1340\/193348).\n\nFeature selection process using target permutation tests actual importance significance against the distribution of feature importances when fitted to noise (shuffled target).\n\nThe notebook implements the following steps :\n\n- Create the null importances distributions : these are created fitting the model over several runs on a shuffled version of the target. This shows how the model can make sense of a feature irrespective of the target.\n\n- Fit the model on the original target and gather the feature importances. This gives us a benchmark whose significance can be tested against the Null - Importances Distribution\n\n- for each feature test the actual importance:\n\n    1 - Compute the probabability of the actual importance wrt the null distribution. I will use a very simple estimation using occurences while the article proposes to fit known distribution to the gathered data. In fact here I'll compute 1 - the proba so that things are in the right order.\n    \n    2 - Simply compare the actual importance to the mean and max of the null importances. This will give sort of a feature importance that allows to see major features in the dataset. Indeed the previous method may give us lots of ones.\n    \n<\/body>\n<\/html>\n\n","5a4c57d6":"#### display_distributions - nom_8","040dad51":"#### display_distributions - ord_0","3730bcd8":"#### display_distributions - nom_3","697c5318":"# Score feature removal for different thresholds","03299018":"#### display_distributions - nom_9","4a5775e9":"#### display_distributions - nom_4","1bac8356":"# Check the impact of removing uncorrelated features\n> Here I'll use a different metric to asses correlation to the target","9a8b2cf3":"## About this Competition\n\n![](https:\/\/media.giphy.com\/media\/H4DjXQXamtTiIuCcRU\/giphy.gif)\n\n> #### In this competition, you will be predicting the probability [0, 1] of a binary target column.\n\nThe data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.\nSince the purpose of this competition is to explore various encoding strategies, the data has been simplified in that (1) there are no missing values, and (2) the test set does not contain any unseen feature values (See this). (Of course, in real-world settings both of these factors are often important to consider!)\n\n#### Files\n- train.csv - the training set\n- test.csv - the test set; you must make predictions against this data\n- sample_submission.csv - a sample submission file in the correct format\n\n","00070276":"#### display_distributions - bin_0","f212b077":"#### display_distributions - bin_4","8c669077":"#### display_distributions - bin_3"}}