{"cell_type":{"0f4dec1e":"code","6e078d67":"code","68150d82":"code","bcc7a5b9":"code","841cf9bb":"code","a68e8d32":"code","0595a62d":"code","06c4ef77":"code","1d00bab4":"code","3ec8a7cd":"code","623c5607":"code","02b075ee":"code","1f9c8c01":"code","3376f4e2":"code","04523f94":"code","7fb0177d":"code","8571ee4b":"code","09893f5e":"code","54d8bfe6":"code","848a23c7":"code","65ccc591":"code","f92fd3c9":"code","22c4dd25":"code","c75e434a":"code","c3eb9522":"code","e9eb8f59":"code","96fd54bc":"code","e6bbb63e":"code","3bc239fd":"code","9d52d7c3":"code","38c1901c":"code","02dc4243":"code","7ed1565c":"code","de1013df":"code","5e79faaa":"code","b89a77c1":"code","883179c7":"code","0d989527":"code","c7079206":"code","c66415c6":"code","e7d7afbe":"code","ff714a8b":"code","9a6923dd":"code","f35da499":"code","51cef9bf":"code","2e5e9cac":"code","adb08d5d":"code","beffb7f3":"code","86774470":"code","14877da9":"markdown","57d4251b":"markdown","ff5e8c90":"markdown","53ecb57b":"markdown","a3300507":"markdown","3f9d8555":"markdown","7003a88b":"markdown","fb7c68a9":"markdown","66cd46ea":"markdown","191593eb":"markdown","344a9017":"markdown","76b8d23b":"markdown","3e8d2164":"markdown"},"source":{"0f4dec1e":"# code for run on google colab\n\"\"\"\n!pip install git+https:\/\/github.com\/Kaggle\/kaggle-api.git --upgrade\nimport os\ncredentials = {\"username\":\"kaggle_username\",\"key\":\"Kaggle_API_keys\"}\nos.environ['KAGGLE_USERNAME']=credentials[\"username\"]\nos.environ['KAGGLE_KEY']=credentials[\"key\"]\n!kaggle datasets download -d moltean\/fruits\n!unzip fruits.zip\n\"\"\"","6e078d67":"# Librairies\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import random_split\nfrom torch.utils.data.dataloader import DataLoader\nimport matplotlib.pyplot as plt\n%matplotlib inline","68150d82":"# Import dataset for Google Colab\n\"\"\"\ndata_dir = '\/content\/fruits-360'\nprint('Folders :', os.listdir(data_dir))\nclasses = os.listdir(data_dir + \"\/Training\")\nprint('131 classes :', classes)\n\"\"\"","bcc7a5b9":"# Import dataset for Kaggle. \n\ndata_dir = '..\/input\/fruits\/fruits-360'\nprint('Folders :', os.listdir(data_dir))\nprint(\"******************************************************************************\")\nclasses = os.listdir(data_dir + \"\/Training\")\nprint('131 classes :', classes)","841cf9bb":"dataset = ImageFolder(data_dir + '\/Training', transform=ToTensor())\nprint('Size of training dataset :', len(dataset))\ntest = ImageFolder(data_dir + '\/Test', transform=ToTensor())\nprint('Size of test dataset :', len(test))","a68e8d32":"# view one image shape of the dataset.\nimg, label = dataset[599]\nprint(img.shape)\n","0595a62d":"# function for the showing the image.\ndef show_image(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))\n    \nshow_image(*dataset[599])","06c4ef77":"show_image(*dataset[23480])","1d00bab4":"torch.manual_seed(20)\nval_size = len(dataset)\/\/10\ntrain_size = len(dataset) - val_size","3ec8a7cd":"train_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds) # train_ds length = dataset length - val_ds length","623c5607":"batch_size = 64\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test, batch_size*2, num_workers=4, pin_memory=True)","02b075ee":"for images, labels in train_loader:\n    fig, ax = plt.subplots(figsize=(18,10))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n    break","1f9c8c01":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","3376f4e2":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","04523f94":"torch.cuda.is_available()","7fb0177d":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","8571ee4b":"device = get_default_device()\ndevice","09893f5e":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","54d8bfe6":"input_size = 3*100*100\noutput_size = 131 # Number of classe","848a23c7":"class Model(ImageClassificationBase):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        # hidden layer\n        self.in_layer = nn.Linear(input_size, 8384)\n        self.hidden1 = nn.Linear(8384, 4192)\n        self.hidden2 = nn.Linear(4192, 2096)\n        self.hidden3 = nn.Linear(2096, 1048)\n        self.out_layer = nn.Linear(1048, output_size)\n        \n    def forward(self, xb):\n        # Flatten images into vectors\n        out = xb.view(xb.size(0), -1)\n        # Apply layers & activation functions\n        # Input layer\n        out = self.in_layer(out)\n        # Hidden layers w\/ ReLU\n        out = self.hidden1(F.relu(out))\n        out = self.hidden2(F.relu(out))\n        out = self.hidden3(F.relu(out))\n        # Class output layer\n        out = self.out_layer(F.relu(out))\n        return out","65ccc591":"model = to_device(Model(input_size, output_size), device)","f92fd3c9":"history = [evaluate(model, val_loader)]\nhistory","22c4dd25":"history += fit(7, 0.01, model, train_loader, val_loader)","c75e434a":"history += fit(8, 0.001, model, train_loader, val_loader)","c3eb9522":"history += fit(3, 0.0001, model, train_loader, val_loader)","e9eb8f59":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n    plt.show()\n    \n    \ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs')\n    plt.show()","96fd54bc":"plot_accuracies(history)","e6bbb63e":"plot_losses(history)","3bc239fd":"evaluate(model, test_loader)","9d52d7c3":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","38c1901c":"class CnnModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 100, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(100, 150, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 150 x 16 x 16\n\n            nn.Conv2d(150, 200, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(200, 200, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 200 x 8 x 8\n\n            nn.Conv2d(200, 250, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(250, 250, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 250 x 4 x 4\n\n            nn.Flatten(), \n            nn.Linear(36000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 500),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(500, 131))\n        \n    def forward(self, xb):\n        return self.network(xb)","02dc4243":"model = CnnModel()\nmodel.cuda()","7ed1565c":"for images, labels in train_loader:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    #print('out[0]:', out[0])\n    break","de1013df":"device = get_default_device()\ndevice","5e79faaa":"train_dl = DeviceDataLoader(train_loader, device)\nval_dl = DeviceDataLoader(val_loader, device)\nto_device(model, device)","b89a77c1":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","883179c7":"model = to_device(CnnModel(), device)","0d989527":"history=[evaluate(model, val_loader)]\nhistory","c7079206":"num_epochs = 3\nopt_func = torch.optim.Adam\nlr = 0.001","c66415c6":"history+= fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","e7d7afbe":"history+= fit(num_epochs, lr\/10, model, train_dl, val_dl, opt_func)","ff714a8b":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n    plt.show()\n    \n    \ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs')\n    plt.show()","9a6923dd":"plot_accuracies(history)","f35da499":"plot_losses(history)","51cef9bf":"evaluate(model, test_loader)","2e5e9cac":"!pip install jovian --upgrade --quiet","adb08d5d":"import jovian\njovian.reset()","beffb7f3":"jovian.log_metrics(val_loss=history[-1]['val_loss'], \n                   val_accuracy=history[-1]['val_acc'],\n                   train_loss=history[-1]['train_loss'])","86774470":"project_name='fruits-360'\njovian.commit(project=project_name, environment=None)","14877da9":"### Exploring the fruits-360 dataset\nThe dataset is extracted to the directory data\/fruits-360. It contains 2 folders (train and test), containing the training set (67,692 images) and test set (22,688 images) respectively. Each of them contains 131 folders, one for each class of images :","57d4251b":"### Evaluation\nWe will plot of the losses & accuracies and evaluate the model on the test set.","ff5e8c90":"## Feedforward Neural Network","53ecb57b":"### Preparing the data for training\nThe dataset is split into 3 parts :\n\n- Training set : used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent .\n- Validation set : used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n- Test set : used to compare different models, or different types of modeling approaches, and report the final accuracy of the model. Since there's no predefined validation set, we can set aside a small portion (about 10% of the training dataset) to be used as the validation set. We'll use the random_split helper method from PyTorch to do this. To ensure that we always create the same validation set, we'll also set a seed for the random number generator.","a3300507":"Your Suggestion are requested in Comments.\n\n**Please upvote the notebook if you like and find it useful.**\n\n### Thankyou","3f9d8555":"We can now create data loaders for training and validation, to load the data in batches","7003a88b":"We can look at batches of images from the dataset using the make_grid method from torchvision.\n\n**Note:-** Each time the following code is run, we will get a different batch, since the sampler shuffles the indices before creating batches.","fb7c68a9":"Each element from the training dataset is a tuple, containing a image tensor and a label. Since the data consists of 100x100 pexel color images with 3 channels (RGB), each image tensor has the shape (3, 100, 100).","66cd46ea":"### Evaluating the model\nWe will plot of the losses & accuracies and evaluate the model on the test set.","191593eb":"### Training","344a9017":"### Result\n\n- In Feed Forward Neural Network accuracy is about 89.69% in 18 epochs on test data.\n- In Convolutional Neural Network accuracy is about 94.98% in 6 epochs on test data.","76b8d23b":"## Convolutional Neural Networks","3e8d2164":"Display of an image and its label using matplotlib, after changing the tensor dimensions to (100,100,3)"}}