{"cell_type":{"5454012e":"code","30208f73":"code","7ba8bbbd":"code","11d22d73":"code","9d447a23":"code","88e1223a":"code","4f9239ad":"code","89fb355b":"code","d1c05c0a":"code","07677008":"code","055ff3cd":"code","bb8616bd":"code","c814478b":"code","1732690f":"code","ddb2e946":"code","c2de714b":"code","45f64d59":"code","70e10ffa":"markdown","566bc032":"markdown","a5242a6d":"markdown","d79f8184":"markdown","5d6727d6":"markdown","9f217d1d":"markdown","cc29d299":"markdown","e6210c71":"markdown","abb39679":"markdown"},"source":{"5454012e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","30208f73":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nfrom statistics import mean\nimport glob\nimport os\nimport cv2\nimport sys\nfrom sklearn.model_selection import train_test_split","7ba8bbbd":"##########################################################\n# 2. LOAD DATASET\n##########################################################\ndata = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\",header=0)# header 0 means the first row is name of the coloumn \n\n# Delete unused columns\ndata.drop([\"Unnamed: 32\",\"id\"], axis=1, inplace=True)\n\n# Change label M(ganas = malignant) = 1 dan B(jinak = benign) = 0\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n\n# Test select malignant data\nm_data = data.loc[data['diagnosis'] == 1]\n\n# Test select benign data\nb_data = data.loc[data['diagnosis'] == 0]\n\n# View sample data\nb_data.head(10) ","11d22d73":"print(data.shape)","9d447a23":"##########################################################\n# SHARE TO TEST AND TRAIN DATA\n##########################################################\nx = data.iloc[:, 1:]\ny = data['diagnosis']\nprint(x)\n\n# Share test and train data\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)","88e1223a":"x_train = x_train.values.reshape((x_train.shape[0], 30, 30, 1))\nx_test = x_test.values.reshape((x_test.shape[0], 30, 30, 1))\n\nprint(x_train.shape)\n\n\n\n","4f9239ad":"# Check again Summarize & Shape Dataset\nprint('Check shape Training Data: X=%s, y=%s' % (x_train.shape, y_train.shape))\nprint('Check shape Testing Data: X=%s, y=%s' % (x_test.shape, y_test.shape))","89fb355b":"x_train.shape","d1c05c0a":"#x_train2 = x_train\/255\n#x_test2 = x_test\/255\nx_train2 = x_train\/255\nx_test2 = x_test\/255\n\n# Convert class labels to one-hot encoded\ny_train2 = keras.utils.to_categorical(y_train)\ny_test2 = keras.utils.to_categorical(y_test)\n\n# Print labels\ny_train2","07677008":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\nfrom keras.layers.advanced_activations import LeakyReLU","055ff3cd":"# Create a model function\ndef make_model():\n    model = Sequential()\n\n    model.add(Conv2D(filters = 16, kernel_size = (3, 3), padding='same', input_shape=(30, 30, 1)))\n    model.add(LeakyReLU(0.1))\n    model.add(Conv2D(filters = 32, kernel_size = (3, 3), padding='same'))\n    model.add(LeakyReLU(0.1))\n    \n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Conv2D(filters = 32, kernel_size = (3, 3), padding='same'))\n    model.add(LeakyReLU(0.1))\n    model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding='same'))\n    model.add(LeakyReLU(0.1))\n\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Flatten())\n    \n    model.add(Dense(256))\n    model.add(LeakyReLU(0.1))\n\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(10))\n    model.add(Activation(\"softmax\"))\n    \n    return model","bb8616bd":"# Summarize a model\nmodel = make_model()\nmodel.summary()","c814478b":"INIT_LR = 1e-2 # Learning rate, is default schedule in all Keras Optimizers\nBATCH_SIZE = 32 # Batch size is a hyperparameter of gradient descent that controls the number of training samples\nEPOCHS = 80 # Number of epochs\n\n# Create object of deep learning model\nmodel = make_model()\n\n# Define the loss function, the optimizer and the metrics\nmodel.compile(\n    loss='categorical_crossentropy',  \n    optimizer=keras.optimizers.Adamax(lr=INIT_LR), \n    metrics=['accuracy']  # report accuracy during training\n)\n\n# Scheduler of learning rate (decay with epochs)\ndef lr_scheduler(epoch):\n    return INIT_LR * 0.9 ** epoch\n\n# Callback for printing of actual learning rate used by optimizer\nclass LrHistory(keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs={}):\n        print(\"Learning rate:\", K.get_value(model.optimizer.lr))","1732690f":"import tensorflow_addons as tfa\ntqdm_callback = tfa.callbacks.TQDMProgressBar()\n\nmodel.fit(\n    x_train2, y_train2,  # Prepared data\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler), \n               LrHistory(), \n               tqdm_callback],\n    shuffle=True,\n    verbose=0,\n    initial_epoch=0\n)","ddb2e946":"# Predict using testing data without labels\/classes\ny_pred_test = model.predict(x_test2)\ny_pred_test_classes = np.argmax(y_pred_test, axis=1) # Change to normal classes\ny_pred_test_classes","c2de714b":"# Create the same format for actual classes\ny_actual_test_classes = np.argmax(y_test2, axis=1) # Change to normal classes\ny_actual_test_classes","45f64d59":"# 6. CONFUSION MATRIX\n##########################################################\n# Actual and predicted classes\nlst_actual_class = y_test\nlst_predicted_class = y_pred\n\n# label M(ganas = malignant) = 1 dan B(jinak = benign) = 0\nlst_classes = [0, 1]\n\n# Compute binary-class confusion matrix \ntn, fp, fn, tp = confusion_matrix(lst_actual_class, lst_predicted_class, labels=lst_classes).ravel()\nsensitivity = round(tp\/(tp+fn)*100, 4);\nspecificity = round(tn\/(tn+fp)*100, 4);\naccuracy = round((tp+tn)\/(tp+fp+tn+fn)*100, 4);\nbalanced_accuracy = round(((sensitivity+specificity)\/2),4)\nprecision = round(tp\/(tp+fp), 4)*100;\nf1Score = round((2*tp\/(2*tp + fp + fn))*100, 4);\nmcc = round(((tp*tn)-(fp*fn))\/(math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))), 4);\n\nprint(\"TP={0}, FP={1}, TN={2}, FN={3}\".format(tp, fp, tn, fn));\nprint(\"Sensitivity: {0}%\".format(sensitivity));\nprint(\"Specificity: {0}%\".format(specificity));\nprint(\"Accuracy: {0}%\".format(accuracy));\nprint(\"Balanced Accuracy: {0}%\".format(balanced_accuracy));\nprint(\"Precision: {0}%\".format(precision));\nprint(\"F1-Score: {0}%\".format(f1Score));\nprint(\"MCC: {0}\".format(mcc));","70e10ffa":"# **Define the loss function, the optimizer and the metrics**","566bc032":"# **Import Necessary CNN Building Blocks**","a5242a6d":"# **Reshape Dataset to Have a Single Channel**","d79f8184":"# **Import All Packages**","5d6727d6":"# **Define\/Create CNN Architecture**","9f217d1d":"# **Fit the Deep Learning Mode**","cc29d299":"# **Normalize Input**","e6210c71":"# **Load the Dataset**","abb39679":"# **Predict New Samples**"}}