{"cell_type":{"1e284379":"code","1c9d2408":"code","c02d73be":"code","9e531053":"code","339f0641":"code","74d8b193":"code","27c653f0":"code","f34724cd":"code","8b401eef":"code","9032f52f":"code","816c218d":"code","27a1576f":"code","35baffae":"code","da2cb7f6":"code","d54ba805":"code","c68a0cf0":"code","014cbbcf":"code","406fd133":"code","6cdbf243":"markdown","a06d623b":"markdown","22a24251":"markdown","9076b449":"markdown","f0a73968":"markdown","6c950069":"markdown","c3139777":"markdown","63ca6e19":"markdown","fe633886":"markdown","9d9134c0":"markdown","fac0bbe4":"markdown","dcc85047":"markdown","e1fb4121":"markdown","d391d41c":"markdown","31b9c5cd":"markdown","0904b939":"markdown","f60f6702":"markdown","1dfd878b":"markdown","bf61ea5c":"markdown","4117bbfe":"markdown","64cde003":"markdown","fdf912e9":"markdown","0c799afa":"markdown","d9896fbc":"markdown","7d489e0c":"markdown"},"source":{"1e284379":"import seaborn as sns; sns.set()\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score, precision_recall_fscore_support\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom imblearn.over_sampling import ADASYN, SMOTE\n\nimport plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\n\n\ninit_notebook_mode(connected=True)\ncf.set_config_file(sharing='public',theme='white',offline=True)\n\n!pip install -Uqq fastbook\n\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastai.tabular.all import *\nfrom fastbook import *\n\nwarnings.filterwarnings(action='ignore', category=UserWarning)","1c9d2408":"#import data and print first five rows of dataframe\ndf = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndf = df.iloc[:, :-2]\ndf.head()","c02d73be":"df.info()","9e531053":"#Age histogram\nage_att = df.loc[df['Attrition_Flag'] == 'Attrited Customer', 'Customer_Age']\nage_ex = df.loc[df['Attrition_Flag'] == 'Existing Customer', 'Customer_Age']\nconc_age = pd.concat([age_att, age_ex], axis=1)\nconc_age.columns = ['Attrited Customer', 'Existing Customer']\nconc_age.iplot(kind='hist', keys=['Attrited Customer', 'Existing Customer'],\n           colors=['blue', 'orange'], histnorm='percent', opacity=0.5, bins=30,\n           title='Customers\\' age', xTitle='Age', yTitle='% customers')\n\n#Marital_Status normalized bar plot\nmar_count = df.groupby(['Marital_Status', 'Attrition_Flag']).count()['CLIENTNUM'].unstack()\nmar_count = mar_count.loc[['Single', 'Married', 'Divorced', 'Unknown']]\nmar_count = mar_count \/ mar_count.sum()\n\nmar_count.iplot(kind='bar', orientation='h',\n                title='Customers\\' marital status', xTitle='normalized count', yTitle='Marital status',                \n                bargap=0.5, colors=['blue', 'orange'])\n\n#Education_Level normalized bar plot\nedu_count = df.groupby(['Education_Level', 'Attrition_Flag']).count()['CLIENTNUM'].unstack()\nedu_count = edu_count.loc[['Uneducated', 'High School', 'College', 'Graduate', 'Post-Graduate', 'Doctorate','Unknown']]\nedu_count = edu_count \/ edu_count.sum()\n\nedu_count.iplot(kind='bar', orientation='h', \n                title='Customers\\' education', xTitle='normalized count', yTitle='Education level',                \n                bargap=0.5, colors=['blue', 'orange'])\n\n#Income_Category normalized bar plot\ninc_count = df.groupby(['Income_Category', 'Attrition_Flag']).count()['CLIENTNUM'].unstack()\ninc_count = inc_count.loc[['Less than $40K','$40K - $60K', '$60K - $80K' , '$80K - $120K', '$120K +', 'Unknown']]\ninc_count.index = inc_count.index.str.replace('$', '')\ninc_count = inc_count \/ inc_count.sum()\n\ninc_count.iplot(kind='bar', orientation='h',\n                title='Customers\\' annual income', xTitle='normalized count', yTitle='Income range ($)',                \n                bargap=0.5, colors=['blue', 'orange'])","339f0641":"#Normalized histogram of annual transactions\ndf_att = df.loc[df['Attrition_Flag'] == 'Attrited Customer', 'Total_Trans_Amt']\ndf_ex = df.loc[df['Attrition_Flag'] == 'Existing Customer', 'Total_Trans_Amt']\nconc = pd.concat([df_att, df_ex], axis=1)\nconc.columns = ['Attrited Customer', 'Existing Customer']\n\nprint(\"Churned customers' mean total transactions' amount: {:.2f}$\".format(conc['Attrited Customer'].mean()))\nprint(\"Non-churned customers' mean total transactions' amount: {:.2f}$\".format(conc['Existing Customer'].mean()))\n\nconc.iplot(kind='hist', keys=['Attrited Customer', 'Existing Customer'],\n           colors=['blue', 'orange'], histnorm='percent', opacity=0.6,\n           title='Annual transactions', xTitle='Total transactions\\' amount ($)', yTitle='% customers')\n\n#Scatter plot: annual transactions' amount vs number\ndf.iplot(kind='scatter', x='Total_Trans_Amt', y='Total_Trans_Ct', categories='Attrition_Flag', size=6,\n        xTitle='Total annual transactions\\' amount ($)', yTitle='no. annual transactions')\n\ndf_att = df.loc[df['Attrition_Flag'] == 'Attrited Customer', 'Total_Ct_Chng_Q4_Q1']\ndf_ex = df.loc[df['Attrition_Flag'] == 'Existing Customer', 'Total_Ct_Chng_Q4_Q1']\nconc = pd.concat([df_att, df_ex], axis=1)\nconc.columns = ['Attrited Customer', 'Existing Customer']\n\n#Box plot of ratio of change in transactions' count over the last year\nconc.iplot(kind='box', keys=['Attrited Customer', 'Existing Customer'],\n           colors=['blue', 'orange'], opacity=0.5, boxpoints='all', legend=False,\n           title='Change in transactions count Q4\/Q1 -- Box Plot', yTitle='change (ratio)')","74d8b193":"#Boxplot of revolving balance\ndf_att = df.loc[df['Attrition_Flag'] == 'Attrited Customer', 'Total_Revolving_Bal']\ndf_ex = df.loc[df['Attrition_Flag'] == 'Existing Customer', 'Total_Revolving_Bal']\nconc = pd.concat([df_att, df_ex], axis=1)\nconc.columns = ['Attrited Customer', 'Existing Customer']\n\nconc.iplot(kind='box', keys=['Attrited Customer', 'Existing Customer'],\n           colors=['blue', 'orange'], opacity=0.5, legend=False,\n           title='Revolving balance', yTitle='Total revolving balance ($)')","27c653f0":"income_ord = 'Less than $40K','$40K - $60K', '$60K - $80K' , '$80K - $120K', '$120K +'\nedu_ord = 'Uneducated', 'High School', 'College', 'Graduate', 'Post-Graduate', 'Doctorate'\ncard_ord = 'Blue', 'Silver', 'Gold', 'Platinum'\nmarital_status = 'Single', 'Married', 'Divorced'\n\ndf['Income_Category'] = df['Income_Category'].astype('category')\ndf['Income_Category'].cat.set_categories(income_ord, ordered=True, inplace=True)\ndf['Education_Level'] = df['Education_Level'].astype('category')\ndf['Education_Level'].cat.set_categories(edu_ord, ordered=True, inplace=True)\ndf['Card_Category'] = df['Card_Category'].astype('category')\ndf['Card_Category'].cat.set_categories(card_ord, ordered=True, inplace=True)\ndf['Marital_Status'] = df['Marital_Status'].astype('category')\ndf['Marital_Status'].cat.set_categories(marital_status, ordered=False, inplace=True)\n\ndep_var = 'Attrition_Flag'\ndf[dep_var] = (df[dep_var] == 'Attrited Customer').astype(int)\n\nprint(\"# existing customers: {}\\n\".format(len(df.loc[df[dep_var] == 0])))\nprint(\"# attrited customers: {}\\n\".format(len(df.loc[df[dep_var] == 1])))","f34724cd":"cont, cat = cont_cat_split(df, max_card=1, dep_var='Attrition_Flag')\nprocs = [Categorify, FillMissing]\nto_rf1 = TabularPandas(df, procs, cat, cont, y_names=dep_var, y_block=CategoryBlock, splits=RandomSplitter()(range_of(len(df))))\n\ntrain_X, train_y = to_rf1.train.xs, to_rf1.train.y\nvalid_X, valid_y = to_rf1.valid.xs, to_rf1.valid.y\n\nrf1 = RandomForestClassifier(n_estimators=500, oob_score=True)\nrf1.fit(train_X, train_y)\npred_y = rf1.predict(valid_X)\n\nprint(\"OOB score: {:.3f}\\n\".format(rf1.oob_score_))\n\nprint(classification_report(valid_y, pred_y))\n\nsns.heatmap(confusion_matrix(valid_y, pred_y), annot=True, fmt='d')\nplt.xlabel('Predicted flag')\nplt.ylabel('True flag')\nplt.title('Confusion Matrix -- RandomForest')\n\n#ROC curve and AUC\nprobs = rf1.predict_proba(valid_X)\nfpr, tpr, thr = roc_curve(valid_y, probs[:, 1])\n\nfig, ax = plt.subplots(figsize=(7, 5))\nplt.plot([0,1],[0,1],\"k--\")\nplt.plot(fpr, tpr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC curve - AUC={:.4f}\".format(roc_auc_score(valid_y, probs[:, 1])))","8b401eef":"def test_model_oversamp(df, model, ovs_method, norm=False, n_iter=5, n_epochs=15, thr_mod=1):\n    res = []\n    roc = []\n    procs=[Categorify, FillMissing]\n    if norm == True:\n        procs = procs + [Normalize]\n    ind0 = df[df['Attrition_Flag'] == 0].index.to_list()\n    ind1 = df[df['Attrition_Flag'] == 1].index.to_list()\n    np.random.shuffle(ind0)\n    np.random.shuffle(ind1)\n    #Split the dataset into n_iter folds, with a 3:1 balancing of the dependent variable\n    test_chks = [ind1[i::n_iter] for i in range(n_iter)]\n    train_chks = [ind0[i::3*n_iter] for i in range(n_iter)]\n    for i in range(n_iter):\n        #Validation fold\n        test_idx = test_chks[i] + train_chks[i]\n        test_idx.sort()\n        #Merge the training folds\n        train_idx = [a for a in range(len(df)) if (a not in test_idx)]\n\n        cont, cat = cont_cat_split(df, max_card=1, dep_var='Attrition_Flag')\n        to = TabularPandas(df, procs, cat, cont, y_names='Attrition_Flag', y_block=CategoryBlock, splits=(train_idx, test_idx))\n        train_X, train_y = to.train.xs, to.train.y\n        test_X, test_y = to.valid.xs, to.valid.y\n\n        #Upsampling of training set\n        ovsX, ovsy = ovs_method.fit_resample(train_X, train_y) \n            \n        #Neural Network setup using FastAI\n        if model == nn:\n            ovs_concat = pd.concat([to.valid.items, pd.concat([ovsX, pd.DataFrame({'Attrition_Flag':ovsy})], axis=1)]).reset_index().drop('index', axis=1)\n            dls = TabularDataLoaders.from_df(ovs_concat, y_names='Attrition_Flag', y_block=CategoryBlock, valid_idx=range(len(test_idx)), bs=512)\n            metrics = RocAucBinary()\n            learn = tabular_learner(dls, metrics=metrics)\n            learn.fit_one_cycle(n_epochs, 1e-2)\n            probs, _ = learn.get_preds()\n            probs = np.array(probs)\n        else:\n            ovsX = np.array(ovsX)\n            test_X = np.array(test_X)\n            model.fit(ovsX, ovsy)\n            probs = model.predict_proba(test_X)\n        \n        #Choose probability threshold using ROC\n        fpr, tpr, thr = roc_curve(test_y, probs[:, 1])\n        best_thr = thr[np.argmin(fpr - tpr)]\n\n        #Predictions of the model\n        preds = np.where(probs[:, 1] > best_thr\/thr_mod, 1, 0)\n\n        #Accuracy and precision\/recall scores\n        acc = accuracy_score(test_y, preds)\n        prec_rec = precision_recall_fscore_support(test_y, preds)\n        res.append((acc, prec_rec[:2]))\n        roc.append(roc_auc_score(test_y, probs[:, 1]))\n        print(\"Iteration {} concluded\".format(i+1))\n  \n    accu = np.mean([a for a,_ in res])\n    accu_std = np.std([a for a,_ in res])\n    auc = np.mean(roc)\n    prec0 = np.mean([pr[0][0] for _,pr in res])\n    prec0_std = np.std([pr[0][0] for _,pr in res])\n    prec1 = np.mean([pr[0][1] for _,pr in res])\n    prec1_std = np.std([pr[0][1] for _,pr in res])\n    rec0 = np.mean([pr[1][0] for _,pr in res])\n    rec0_std = np.std([pr[1][0] for _,pr in res])\n    rec1 = np.mean([pr[1][1] for _,pr in res])\n    rec1_std = np.std([pr[1][1] for _,pr in res])\n    print(\"\\nMean AUC score: {:.3f}\".format(auc))\n    print(\"\\nMean accuracy: {0:.3f} (std: {1:.3f})\".format(accu, accu_std))\n    print(\"\\nCLASS 0: \\n \\t mean precision: {0:.3f} (std: {1:.3f}), \\t mean recall: {2:.3f} (std: {3:.3f})\".format(prec0, prec0_std, rec0, rec0_std))\n    print(\"CLASS 1: \\n \\t mean precision: {0:.3f} (std: {1:.3f}), \\t mean recall: {2:.3f} (std: {3:.3f})\".format(prec1, prec1_std, rec1, rec1_std))\n    return res, roc  ","9032f52f":"rf = RandomForestClassifier(n_estimators=300, max_features='auto')\novs_method = ADASYN()\nres, roc = test_model_oversamp(df, rf, ovs_method=ovs_method)","816c218d":"model_xgb = xgb.XGBClassifier(use_label_encoder=False, max_depth=25, eval_metric='logloss')\novs_method = ADASYN()\nres, roc = test_model_oversamp(df, model_xgb, ovs_method=ovs_method)","27a1576f":"to_all = TabularPandas(df, procs, cat, cont, y_names=dep_var, y_block=CategoryBlock)\ntrain_X, train_y = to_all.train.xs, to_all.train.y\n\nparam_tuning = {\n        'learning_rate': [0.01, 0.1, 0.5],\n        'max_depth': [5, 10, 25, 40],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 400]\n        }\n\nmodel= xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n\ngrid = RandomizedSearchCV(estimator=model, param_distributions=param_tuning)\ngrid.fit(train_X, train_y)\n\nbest_xgb = grid.best_estimator_\n\nprint(grid.best_params_)\n\novs_method = ADASYN()\nres, roc = test_model_oversamp(df, best_xgb, ovs_method=ovs_method)","35baffae":"to_all = TabularPandas(df, procs, cat, cont, y_names=dep_var, y_block=CategoryBlock)\n\ntrain_X, train_y = to_all.train.xs, to_all.train.y\n\nrf_all = RandomForestClassifier(n_estimators=500, max_features='auto', oob_score=True)\nrf_all.fit(train_X, train_y)\n\nfi = pd.DataFrame({'feature':train_X.columns, 'feature_imp': rf_all.feature_importances_}).sort_values('feature_imp', ascending=False)\nplt.subplots(figsize=(13,8))\nplt.barh(fi.feature, fi.feature_imp)\nplt.title(\"Feature importance\")\n\nprint(\"OOB score: {:.3f}\\n\".format(rf_all.oob_score_))","da2cb7f6":"feat_filt = fi.loc[fi['feature_imp'] > 0.02, 'feature'].to_list()\nx_filt = train_X[feat_filt]\n\nrf_filt = RandomForestClassifier(n_estimators=500, oob_score=True)\nrf_filt.fit(x_filt, train_y)\n\nprint(\"OOB score: {:.3f}\\n\".format(rf_filt.oob_score_))\nprint(\"Number of features: {}\".format(len(feat_filt)))\n\ncluster_columns(x_filt)","d54ba805":"x_filt2 = train_X[feat_filt].drop(['Avg_Open_To_Buy', 'CLIENTNUM'], axis=1)\n\nrf_filt2 = RandomForestClassifier(n_estimators=500, oob_score=True)\nrf_filt2.fit(x_filt2, train_y)\n\nprint(\"OOB score: {:.3f}\\n\".format(rf_filt2.oob_score_))","c68a0cf0":"df_fin = df[feat_filt + ['Attrition_Flag']].drop(['Avg_Open_To_Buy', 'CLIENTNUM'], axis=1)\n\nmodel = best_xgb\novs_method = ADASYN()\nres, roc = test_model_oversamp(df_fin, model, ovs_method=ovs_method)","014cbbcf":"#All features\nres, roc = test_model_oversamp(df, nn, norm=True, ovs_method=ADASYN())","406fd133":"#Selected features\nres, roc = test_model_oversamp(df_fin, nn, norm=True, ovs_method=ADASYN())","6cdbf243":"Attrited customers seems to have a slightly higher education level and sliglty lower annual income, but overall from the demographic standpoint there are no striking differences between customers who have churned and customers who have not. Let us look at credit card usage and spending habits.","a06d623b":"## 2. EDA and visualization <a id=2><\/a>","22a24251":"### 3.2 Dataset balancing <a id=3.2><\/a>\n \nWe can balance the dataset by adding synthetic data. We do so by using an upsampling algorithm like SMOTE or ADASYN. However, before upsampling we remove from the dataset a test set which we'll use for validation (and we repeat this over multiple folds). This is to prevent data leakage: we construct the synthetic datapoints (which will constitute almost half of the final training set) without them seeing the validation set, aiming for a more robust predictor.\nWe choose the probability threshold for predicting `Attrition_Flag` = 0\/1 using ROC.","9076b449":"The OOB score has not decreased. We restrict to the remaining 12 features and check the upsampled XGB algorithm CV performance as before.","f0a73968":"We see that XGB gives better overall results than RandomForest. We perform a randomized grid search for hyperparameter tuning.","6c950069":"## 0. Introduction <a id=0><\/a>\n\nCredit card churning is a widespread phenomenon where people apply for multiple credit cards (or, more generally, open multiple credit lines) to take advntage of signup bonuses, with no intention of keeping all of them active in the long term. Credit institutions have therefore a strong interest in identifying churning customers and, more generally, predicting whether a client is gonna cancel their credit card or not. We perform data exploration on a dataset of 10000 customers with a view towards this goal and then build a predicting model.","c3139777":"### 3.4 Neural Networks? <a id=3.4><\/a> \n\nLet us try using a neural network for the classification model, both with the total set of features and the filtered one. We train a 2-layers NN for 15 epochs and cross-validate as before.","63ca6e19":"We notice a major difference when analizying the revolving balance of the customers' credit lines: more than half of the churned customers have paid off all their debt. It is unclear whether this measure for the revolving balance is conducted before or after the termination of the credit line and whether customers who cancel their credit cards are asked to pay off all their standing debt. If this is the case, this feature has no real predicting value and should not be used for classification, as it is implicitly biased. However, since it was included in the dataset, we will use it in our models.","fe633886":"As before, using only the 12 most important features gives better results, but the NN approach does not perform as good as  XGB.","9d9134c0":"## 1. Preparation <a id=1><\/a>\n\n### 1.1 Packages <a id=1.1><\/a>","fac0bbe4":"## 1.3 Data dictionary <a id=1.3><\/a>","dcc85047":"0. [Introduction](#0)\n\n1.  [Preparation](#1)\n\n    1.1 [Packages](#1.1)\n    \n    1.2 [Data](#1.2)\n    \n    1.3 [Data dictionary](#1.3)\n    \n2. [EDA and visualization](#2)\n\n3. [Classification model](#3)\n\n    3.1 [Imputation, label encoding and baseline](#3.1)\n    \n    3.2 [Dataset balancing](#3.2)\n    \n    3.3 [Feature selection](#3.3)\n    \n    3.4 [Neural Networks?](#3.4)","e1fb4121":"As it can be expected, we notice a direct correlation between the number of annual transactions and their total amount. More interestingly, it appears that we can segment the dataset into three different clusters of clients, one of which (high spenders, i.e. `Total_Trans_Amt` > 11k) contains no attrited customers at all. Churned customers used their credit card less often and spent significantly less money. They are also characterized by a more significant reduction in the usage of their credit card during the last 12 months.","d391d41c":"Pretty good! With upsampling, the recall for the positive class has jumped to 94%, with a still good overall accuracy.\nLet us try using a different model, namely XGBoost.","31b9c5cd":"### 1.2 Data <a id=1.2><\/a>\n\nAs specified in the original dataset, we can drop the last two columns.","0904b939":"### 3.3 Feature selection <a id=3.3><\/a>\n\nUp until now we have worked with all features in the original dataset. It is worth looking at whether we can restrict to a subset of features (thus improving interpretability of the model) without losing prediction accuracy. To do so, we first train a RandomForest on all the dataset and take a look at the corresponding features importance.","f60f6702":"Our dataset is **heavily unbalanced**: only 16% of the datapoints have `Attrition_Flag` = 1. This poses a problem which needs to be addressed, both while training classification models (to avoid bias) and while evaluating their performances (a naive model which always predicts `Attrition_Flag` = 0 will have an accuracy of almost 84%). In particular, as metric we will need to look separately at precision and recall for each of the two levels of the dependent variable, not just at the overall accuracy. Since the bank is especially interested in predicting which customers will churn, we will prioritize the recall for the positive class: this will be reflected in the choices fo the ROC thresholds.\n\nAs baseline, let us train a RandomForest classifier using all variables in the unbalanced dataset. We reserve a stratified 20% of data for validation.","1dfd878b":"There are four categorical features, namely `Education_Level`, `Income_Category`, `Card_Category` and `Marital_Status`, in addition to the dependent variable `Attrition_Flag`. Even though the non-null count does not show any missing data, by looking at the different levels we see that there there are entries in the columns `Education_Level`, `Income_Category` and `Marital_Status` which are marked as *Unknown*.","bf61ea5c":"# Credit churn - data exploration and modeling","4117bbfe":"Passing from 20 features to just 14, the OOB score has actually slightly improved! Incidentally, we notice that we have tossed out all categorical features. Looking at the hierarchical feature clustering, it seems that we might also spare one feature between `Credit_Limit` and `Avg_Open_To_Buy` (we discard the second one, which has smaller importance). We also try removing `CLIENTNUM`, which we expect has no significant predicting value.","64cde003":"## 3. Classification model <a id=3><\/a>\n\n### 3.1 Imputation, label encoding and baseline\n\nWe change the *Unknown* entries in the categorical features to NaN and then we impute such missing data, while label encoding all categorical features (we can treat `Education_Level`, `Income_Category` and `Card_Category` as ordinal features, and follow the obvious ordering in the choice of the labels). For the dependent variable, we will consider *Attrited Customer* as the positive class (label = 1).","fdf912e9":"`CLIENTNUM` - ID of the customer holding the credit card.\n\n`Customer_Age` - Age of the customer.\n\n`Gender` - Sex of the customer.\n\n`Dependent_count` - Number of dependents of the customer.\n\n`Education_Level` - Educational qualification of the customer.\n\n`Marital_Status` - Civil status of the customer.\n\n`Income_Category` - Annual income range of the customer.\n\n`Card_Category` - Type of card owned by the customer.\n\n`Months_on_book` - Number of months elapsed since the account opening.\n\n`Total_Relationship_Count` - Total number of products held by the customer.\n\n`Months_Inactive_12_mon` - Number of months with no transactions in the last year.\n\n`Contacts_Count_12_mon` - Number of contacts with the bank in the last year.\n\n`Credit_Limit` - Credit limit on the credit card.\n\n`Total_Revolving_Bal` - Total revolving balance on the credit card.\n\n`Avg_Open_To_Buy` - Average card \"Open To Buy\" (=credit limit - account balance) in the last year.\n\n`Total_Amt_Chng_Q4_Q1` - Change in transaction amount over the last year (Q4 over Q1).\n\n`Total_Trans_Amt` - Total amount of transactions made in the last year.\n\n`Total_Trans_Ct` - Number of transactions made in the last year.\n\n`Total_Ct_Chng_Q4_Q1` - Change in transaction number over the last year (Q4 over Q1).\n\n`Avg_Utilization_Ratio` - Average card \"Utilization ratio\" (=account balance \/ credit limit) in the last year.\n\n`Attrition_Flag` - Target variable. \"Attrited Customer\" if the customer closed their account, otherwise \"Existing Customer\".","0c799afa":"As we see from the classification report, the recall for the positive level of the dependent variable is significantly lower than the overall accuracy. This could be of course increased by lowering the probability threshold in the binary classification, at the cost of a precision reduction.","d9896fbc":"The overall performance has improved passing from the original 20 features to the 12 most significant ones, reaching an **AUC score of 99.2% and a recall for the positive class of over 97%**.","7d489e0c":"We notice that all categorical features have low importance. The most signficant variables (namely `Total_Trans_Amt`, `Total_Trans_Ct`, `Total_Revolving_Bal` and `Total_Ct_Chng_Q4_Q1`) have been analyzed in the preliminary dataset exploration. Let's toss out the less important features and check how the OOB score changes."}}