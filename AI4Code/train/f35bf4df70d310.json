{"cell_type":{"5da4d65a":"code","9dc58b59":"code","68191855":"code","21d3f16d":"code","a40c3af0":"code","1c3471a9":"code","022313e5":"code","ae05b822":"code","da662005":"code","58b6aa60":"code","ff4af22d":"code","4643bcbb":"code","72bd0106":"code","0b17a2df":"code","f23efdde":"code","59ad32f2":"code","b21a2ca7":"code","76bdb8b5":"code","c426de0d":"code","5b7eed81":"code","3aed81d8":"code","2ee65138":"code","e5c8d812":"code","69c0f9b9":"code","8a233610":"code","c7134537":"markdown","27056213":"markdown","ecb2667f":"markdown","4474c179":"markdown","21e54195":"markdown","f380d192":"markdown","40858e6d":"markdown","6e4ab709":"markdown","b69dcdff":"markdown","1b2184ea":"markdown","d16beb82":"markdown","2cc9ca27":"markdown","c1ef60d5":"markdown","be1ee66a":"markdown","615b3840":"markdown","fa3ace92":"markdown","d2dd0f10":"markdown","dc9d60f5":"markdown","a0068391":"markdown","60edb554":"markdown","2040e169":"markdown","1db2564b":"markdown","4d59fc39":"markdown","ac22c64c":"markdown"},"source":{"5da4d65a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","9dc58b59":"from keras.datasets import mnist # MNIST dataset is included in Keras","68191855":"# The MNIST data is split between 60,000 training images and 10,000 test image\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nprint(\"X_train shape\", X_train.shape)\nprint(\"y_train shape\", y_train.shape)\nprint(\"X_test shape\", X_test.shape)\nprint(\"y_test shape\", y_test.shape)","21d3f16d":"fig = plt.figure(figsize=(6, 6))\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\nfor i in range(20):\n    ax = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n    ax.imshow(X_train[i], cmap=plt.cm.binary, interpolation='nearest')\n    # label the image with the target value\n    ax.text(0, 7, str(y_train[i]))","a40c3af0":"# Reshaping the dataset\nX_train = X_train.reshape(60000, 784) \nX_test = X_test.reshape(10000, 784)  \n\n# Change integers to 32-bit floating point numbers\nX_train = X_train.astype('float32')   \nX_test = X_test.astype('float32')\n\nprint(\"Training matrix shape\", X_train.shape)\nprint(\"Testing matrix shape\", X_test.shape)","1c3471a9":"# Apply standarization to both train and test data\nfrom sklearn.preprocessing import StandardScaler\nX_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)","022313e5":"# initializing the pca\nfrom sklearn.decomposition import PCA","ae05b822":"pca = PCA(n_components=.95)\npca.fit(X_train)","da662005":"print(f'Total number of components used after PCA : {pca.n_components_}')","58b6aa60":"train_img = pca.transform(X_train)\ntest_img = pca.transform(X_test)\n\nprint(f'train_img shape : {train_img.shape}')\nprint(f'test_img shape : {test_img.shape}')","ff4af22d":"from sklearn.neural_network import MLPClassifier","4643bcbb":"# all parameters not specified are set to their defaults\n# default solver is incredibly slow thats why we change it\n# solver = 'lbfgs'\nclf = MLPClassifier(solver = 'lbfgs')","72bd0106":"clf.fit(train_img, y_train)","0b17a2df":"print('Training score : ', clf.score(train_img, y_train))\nprint('Testing score  : ', clf.score(test_img, y_test))","f23efdde":"# The data used is the original train and test data\nX_train.shape","59ad32f2":"from sklearn.decomposition import PCA\nlist_variance = [.85, .90, .95, .99]\ntrain_data = []\ntest_data = []\ni = 0\n\nfor variance in list_variance:\n  print(f'Dataset dimensional reduction with {variance*100}% variance retained')\n  print('='*60)\n\n  pca = PCA(n_components=variance)\n  pca.fit(X_train)\n\n  train_data.append(pca.transform(X_train))\n  test_data.append(pca.transform(X_test))\n\n  print(f'> Total number of components for variance {variance} : {pca.n_components_}')\n  print(f'> Train data shape : {train_data[i].shape}')\n  print(f'> Test data shape  : {test_data[i].shape}\\n')\n  i += 1","b21a2ca7":"from sklearn.neural_network import MLPClassifier\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","76bdb8b5":"start_time = time.time()\nclf = MLPClassifier(solver = 'lbfgs', max_iter=100)\nclf.fit(X_train, y_train)\nend_time = time.time()\n\naccuracy = []\ntest_score = clf.score(X_test, y_test)\naccuracy.append(test_score)\n\ntrain_time = []\nx = end_time - start_time\ntrain_time.append(x)\n\nprint(f'Control result with original dataset (100% variance)')\nprint('='*60)\nprint(f'Training score : ', clf.score(X_train, y_train))\nprint(f'Testing score  : ', clf.score(X_test, y_test))\nprint(f'Time (seconds) : {x} \\n')","c426de0d":"for i in range(0, 4):\n    start_time = time.time()\n    clf = MLPClassifier(solver = 'lbfgs', max_iter=100)\n    clf.fit(train_data[i], y_train)\n    end_time = time.time()\n    \n    x = end_time - start_time\n    train_time.append(x)  \n    \n    train_score = clf.score(train_data[i], y_train)\n    \n    test_score = clf.score(X_test, y_test)\n    accuracy.append(test_score)\n      \n    print(f'Dataset with {list_variance[i]*100}% variance retained')\n    print('='*40)\n    print(f'Training score : ', train_score)\n    print(f'Testing score  : ', test_score)\n    print(f'Time (seconds) : {x} \\n')","5b7eed81":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    if axes is None:\n        _, axes = plt.subplots(1, 2, figsize=(8, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot fit_time vs score\n    axes[1].grid()\n    axes[1].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[1].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[1].set_xlabel(\"fit_times\")\n    axes[1].set_ylabel(\"Score\")\n    axes[1].set_title(\"Performance of the model\")\n\n    return plt","3aed81d8":"cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n\nfig, axes = plt.subplots(2, 4, figsize=(25, 15))\n\nfor i in range(0,4):\n  title = \"Performance of model with \" + str(list_variance[i]) + \" data variance\"\n  estimator = MLPClassifier(solver = 'lbfgs', max_iter=100)\n  plot_learning_curve(estimator, title, test_data[i], y_test, axes=axes[:, i],\n                        ylim=(0.7, 1.01), cv=cv, n_jobs=-1)\n\nplt.show()","2ee65138":"# Calculate component reduction percentage\n\ncomponents = [185, 236, 331, 543, 784]\ncomponents_reduction = []\n\nfor cc in components:\n  z = (1.0 - (cc \/ 784)) * 100\n  z = round(z, 2)\n  components_reduction.append(z)\n\ncomponents_reduction","e5c8d812":"# Calculate training time reduction percentage\n\ntrain_time_reduction = []\n\nfor time in train_time:\n  reduction = (1.0 - (time \/ 76.19)) * 100\n  reduction = round(reduction, 2)\n  train_time_reduction.append(reduction)\n\ntrain_time_reduction","69c0f9b9":"# Calculate accuracy difference\n\naccuracy_diff = []\n\nfor acc in accuracy:\n  x = 0.9744 - acc\n  x = round(x, 4)\n  accuracy_diff.append(x)\n\naccuracy_diff","8a233610":"result = {'Data Variance' : ['85%', '90%', '95%', '99%', '100%'], \n          'Components' : components,\n          'Components Reduction (%)' : components_reduction,\n          'Accuracy' : accuracy,\n          'Accuracy Difference' : accuracy_diff,\n          'Train Time': train_time,\n          'Train time Reduction (%)' : train_time_reduction,\n          }\nres_df = pd.DataFrame(result)\nres_df","c7134537":"#### Testing\n\nIndependent variables : Data variance\n\nDependent variables : Training score, Testing score, Train time","27056213":"Apply the mapping to the training & test set of data","ecb2667f":"Now let's experiment with using multiple dimension input and see which \nPCA settings is the most optimal for reducing fit times while retaining high accuracy","4474c179":"## 1. Loading the datasets","21e54195":"### Reshaping the dataset","f380d192":"### Splitting the data into train and test set","40858e6d":"Step 0 : Import the PCA model from the sklearn library and do dimentional reduction on the data","6e4ab709":"### Display Sample Data","b69dcdff":"### Standardizing the data","1b2184ea":"For this task, I'm going to use the **Multi Layered Perceptron** model","d16beb82":"## 0. Importing Library","2cc9ca27":"## 4. Experiment","c1ef60d5":"#### Prepare the data\n\nLet's try to reduce the data dimension while retaining **85%, 90%, 95%, and 99%** of the variance","be1ee66a":"### Generate Training accuracy & Validation accuracy curves","615b3840":"From our experiment, the settings which produce the highest score is \n> **PCA while maintaining 90% of data variance**\n\n\nThis approach has reduce \n\n* 35.42% of the training time\n* 69.9% of the components\n\nwhile maintaining the **same accuracy score**","fa3ace92":"## 2. PCA on MNIST Digit Recognition","d2dd0f10":"#### Control Result \n\nControl result is the accuracy and train time that is taken by the model using the original data without dimentional reduction\n\nFor experimentation purpose, I set some control variables which are \n\n* **Solver** which set to the type of 'lbfgs' which can converge faster [reference here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html)\n* **Max iteration** to 100","dc9d60f5":"# Exploring Principal Component Analysis (PCA)\n\nA notebook by Jonathan Kristanto\n\nAll rights reserved\n\n*PS : If you found this notebook helpful, please consider to upvote this notebook. Thank you :D*","a0068391":"## Conclusion","60edb554":"#### Function to plot training & validation accuracy curves\n\nSource : https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html","2040e169":"Analysis by : Jonathan Kristanto &copy; 20 Mei 2021","1db2564b":"## Result Analysis","4d59fc39":"Fit the PCA on the training set and set the model to retain 95% of the variance","ac22c64c":"## 3. Train the Multilayer Neural Network model"}}