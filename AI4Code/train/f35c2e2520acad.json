{"cell_type":{"913fe872":"code","ce882d1e":"code","f2956028":"code","438a13c6":"code","3f76ab49":"code","530f2abe":"code","05a5984a":"code","5ead8f0b":"code","e105eb45":"code","11f008f0":"code","c81cdca1":"code","8359b776":"code","56037dd4":"code","862ebf8c":"code","ef52b802":"code","d70ef7c3":"code","9e58c012":"code","a444d476":"code","5715ffd8":"code","dea58aa4":"code","5d012325":"code","fbf4f5f5":"code","3b2a0d56":"code","0cab9649":"markdown","852b8787":"markdown","e570631f":"markdown","836a5dda":"markdown","ddfc2b98":"markdown","d0803dc9":"markdown","6493686f":"markdown","74d9c9c0":"markdown","38037a05":"markdown","950c01a1":"markdown","307468bf":"markdown","4a64a271":"markdown","fa7fb05d":"markdown","61d9ac38":"markdown","9328594c":"markdown","277dcc9e":"markdown","e73ee49b":"markdown","b0af4c7c":"markdown","e84ca509":"markdown"},"source":{"913fe872":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce882d1e":"SHOPEE_ROOT = \"\/kaggle\/input\/shopee-product-matching\"\nSHOPEE_TRAIN_IMAGES = SHOPEE_ROOT+\"\/\"+\"train_images\"\nRANDOM_STATE = 42\n","f2956028":"#seeding numpy randomizer\nnp.random.seed(RANDOM_STATE)\n","438a13c6":"shopee_train_df = pd.read_csv(SHOPEE_ROOT+\"\/\"+\"train.csv\")","3f76ab49":"shopee_train_df.shape","530f2abe":"#dropping entries with identical posting_id values\nshopee_train_df.drop_duplicates([\"posting_id\"], inplace=True)","05a5984a":"shopee_train_df.shape","5ead8f0b":"shopee_train_df.drop_duplicates([\"image\", \"image_phash\", \"title\"], inplace=True)","e105eb45":"shopee_train_df.shape","11f008f0":"#just taking a look\nshopee_train_df","c81cdca1":"print(\"Number unique label_groups = {}\".format( len(shopee_train_df[\"label_group\"].unique()) ))","8359b776":"hash_of_label_group_images = {}\nhash_of_image_count_per_label = {}\n\nfor mylabel in shopee_train_df[\"label_group\"]:\n    hash_of_label_group_images[mylabel]  = shopee_train_df[ shopee_train_df[\"label_group\"]==mylabel]\n    hash_of_image_count_per_label[mylabel] =  len(shopee_train_df[ shopee_train_df[\"label_group\"]==mylabel])\n    #print(\"for label_group = {} number of images is {}\".format(mylabel,len(hash_of_label_group_images)))\n    ","56037dd4":"#sorting the keys (the label_groups) by the values (thenumber of images per label_group)\nsorted_label_groups = sorted(hash_of_image_count_per_label,key=hash_of_image_count_per_label.__getitem__)","862ebf8c":"#10 least populated (by distinct image count) label groups\nlabel_group_image_counts = []\nfor i,my_label in enumerate(sorted_label_groups):\n    if i > 9:\n        break\n    label_group_image_counts.append(hash_of_image_count_per_label[my_label])\n    print(\"label {} has {} images\".format(my_label,hash_of_image_count_per_label[my_label] ))","ef52b802":"#10 most populated (by distinct image count) label groups\nfor i in range(len(sorted_label_groups)-11,len(sorted_label_groups)-1,1):\n    label_group_image_counts.append(hash_of_image_count_per_label[ sorted_label_groups[i]  ])\n    print(\"label {} has {} images\".format(sorted_label_groups[i],hash_of_image_count_per_label[sorted_label_groups[i]  ] ))","d70ef7c3":"#plotting the whole data set is ugly\n#will plot the first 20 label groups: The label_groups with the 10 lowest and 10 highest number of images per said label group\nlabel_groups_to_plot = sorted_label_groups[:10]\nlabel_groups_to_plot.extend(sorted_label_groups[-11:-1])\nlabel_groups_to_plot = [str(mylabel) for mylabel in label_groups_to_plot]\nprint(label_groups_to_plot)","9e58c012":"plt.figure(figsize=(40,40))\nplt.grid()\nplt.barh(label_groups_to_plot, np.log10(label_group_image_counts))\nplt.ylabel(\"label_group\",fontsize=30)\nplt.xlabel(\"log(number_of_images_per_label_group)\",fontsize=30)\nplt.title(\"The 10 Most and Least Populated label_groups and their Distinct Image Counts\",fontsize=35)\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\n\nplt.show()","a444d476":"#first just pick a random set of label_groups\nlabel_groups = np.array(sorted_label_groups)\nnp.random.shuffle(label_groups)\nlabel_group_subset = label_groups[:5]\nprint(label_group_subset)","5715ffd8":"fig, ax = plt.subplots(nrows=5,ncols=2,figsize=(30,30))\nfor i, mylabel in enumerate(label_group_subset):\n    image_names =  shopee_train_df[ shopee_train_df[\"label_group\"] == mylabel ][\"image\"]\n    image_titles = shopee_train_df[ shopee_train_df[\"label_group\"] == mylabel ][\"title\"]\n    \n    for j, image_name in enumerate(image_names):\n        if j>=2:\n            break\n        img = plt.imread(SHOPEE_TRAIN_IMAGES+\"\/\"+image_name)\n        ax[i,j].set_title(str(image_titles.iloc[j])+\"\\nlg: \"+str(mylabel))\n        ax[i,j].imshow(img)\n","dea58aa4":"unique_phash = shopee_train_df[\"image_phash\"].unique()\nprint(\"number of unique_phashes is {}\".format(len(unique_phash)))","5d012325":"#number of phashes to randomly select\nNUM_OF_PHASHES = 10\n\nunique_phash = np.array(unique_phash)\nnp.random.shuffle(unique_phash)\nunique_phash_subset = []\ni = 0\n\nfor myphash in unique_phash:\n    numb_images = len(shopee_train_df[shopee_train_df[\"image_phash\"] == myphash][\"image\"])\n    #print(\"phash {} has {} images\".format(myphash,numb_images))\n    if numb_images > 1:\n        i += 1\n        unique_phash_subset.append(myphash)\n        \n    if i >= NUM_OF_PHASHES:\n        break\n        \nprint(\"unique_phash_subset = {}\".format(unique_phash_subset))","fbf4f5f5":"fig, ax = plt.subplots(nrows=NUM_OF_PHASHES,ncols=2,figsize=(20,80))\n\n\nfor i,myphash in enumerate( unique_phash_subset):\n    myimages = shopee_train_df[shopee_train_df[\"image_phash\"] == myphash][\"image\"]\n    mytitles = shopee_train_df[shopee_train_df[\"image_phash\"] == myphash][\"title\"]\n    #print(\"number of images at phash = {} is {}\".format(len(myimages),myphash))\n  \n    for j, myimage in enumerate(myimages):\n        if j >= 2:\n            break\n        ax[i,j].set_title(mytitles.iloc[j]+\"\\n phash: \"+ str(myphash)+\"\\n image: \"+str(myimage),fontsize=10)\n        img = plt.imread(SHOPEE_TRAIN_IMAGES+\"\/\"+myimage)\n        ax[i,j].imshow(img)\n        ","3b2a0d56":"fig, ax = plt.subplots(nrows=5,ncols=2,figsize=(40,40))\n#ax = ax.flatten()\n\nfor i, mylabel in enumerate(label_group_subset):\n    image_names =  shopee_train_df[ shopee_train_df[\"label_group\"] == mylabel ][\"image\"]\n    image_phashes = shopee_train_df[ shopee_train_df[\"label_group\"] == mylabel ][\"image_phash\"]\n    image_titles = shopee_train_df[ shopee_train_df[\"label_group\"] == mylabel ][\"title\"]\n\n    for j,myphash1 in enumerate(image_phashes):\n        for k, myphash2 in enumerate(image_phashes):\n            if myphash1 != myphash2:\n                img1 = plt.imread(SHOPEE_TRAIN_IMAGES + \"\/\" + image_names.iloc[j])\n                ax[i,0].set_title(str(image_titles.iloc[j]+\"\\nphash: \"+str(myphash1) + \"\\nlg: \"+str(mylabel)))\n                ax[i,0].imshow(img1)\n                img2 = plt.imread(SHOPEE_TRAIN_IMAGES + \"\/\" + image_names.iloc[k])\n                ax[i,1].set_title(str(image_titles.iloc[k]+\"\\nphash: \"+str(myphash2) + \"\\nlg: \"+str(mylabel)))\n                ax[i,1].imshow(img2)","0cab9649":"**Finding**: Yes there are many images whose `image_phash` are not identical yet these images belong to the same  `label_group`. So sometimes people do submit different pictures of the same product (presumably).","852b8787":"**Finding**: Looks like the objects in the images that belong to the same `label_group` are the same. We can trust the labeling.","e570631f":"I leave the line 7 commented out. You can comment it out if you want to see the very verbose printout","836a5dda":"# Conclusions\n\n* There are no duplicates (either by `posting_id` or `(image, image_phash,title)`\n* There is pretty large difference in number of distinct images between the most and least populus `label_group`\n* All `label_groups` have at least 2 images. I think this is good for training a CNN.\n* Images that belong to the same `label_group` so far appear to be images of the same object. So currently no concern of mislabled entries in `train.csv`. But this is not conclusive\n* Seems there are copies of the same image under in `train.csv` that have different `image` names. ","ddfc2b98":"Hypothesis all images with the same phash are the same object","d0803dc9":"Which are the top 10 most populus and least populus `label_groups`?","6493686f":"**Findings**: Images that have the same phash are images of the same object. Seems that image files can have different `image` file names, but yet are the same image (AFAICT). I notice that people submit the same thing but call it something different (`titles` are different). I also notice that two different images can be different (slighlty) but have the same phash (see \"MUKENA DEWASA\" product images). \n\ud83e\udd14","74d9c9c0":"Any entries that have the same `(image, image_phash,title)` tuple can be for the purposes of creating a training set considered redundant(?)","38037a05":"How many images are there per label group?","950c01a1":"Now I'm going to try to validate or better understand the data set by making some hypotheses and then examine the data set to prove\/disprove them. These are not 100% conclusive findings but I feel give me an OK idea of the nature of the data set. If I am lucky I may find mislabeled data which will be helpful to know before I start doing long training and cross validation runs. I'm open to suggestions on more comprehensive ways to validate a data set. I image for data sets where the relationship between features follows some rule, a checker can be coded up to read each row. In this case I can't think of a rule-based checker so I'm hoping to luck out by randomly picking rows that have certain relationships and hoping to _see_ images that don't fit with my expectation.","307468bf":"# Shopee Dataset EDA\nDoing some exploratory data analysis. WIll make some hypethesis and then generate data to either prove or disprove hypothesis.","4a64a271":"Hypothesis: All images belonging to the same `label_group` are of the same object","fa7fb05d":"Nothing dropped yet. Data doesn't have redundante entries \u263a\ufe0f","61d9ac38":"There is a little over an order of magnitude difference in the number of images in the largest `label_group` when compared to the smallest `label_group`","9328594c":"Nothing dropped","277dcc9e":"Hypothesis: Not all images that belong to the same `label_group` have identical `image_phash` values","e73ee49b":"# Constants\nSet of variables I use throughout the notebook. Not sure if these cause submissions to fail.","b0af4c7c":"## Data Cleaning\nIt's not a good idea to just straightaway use the data set to train and test a model.\nDatasets can be dirty:\n\n* They could can contain duplicate entries\n* They could contain entries that are invalid\n\nSo I will look for and remove any two entries that have the same `posting_id`","e84ca509":"How many unique `label_group` exist in `train.csv`?"}}