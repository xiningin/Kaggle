{"cell_type":{"6cc10ce3":"code","053729d5":"code","f7e24726":"code","5373aba5":"code","2d9c6716":"code","e0928ab3":"code","2f8398b7":"code","64d66970":"code","2f70f740":"code","c041da9f":"code","93aff7c4":"code","85cabb0c":"code","815e383b":"code","36e93cf9":"code","3bd53142":"code","2622fbb3":"code","4e058464":"code","5e15d1ba":"code","e6e11e57":"code","ac39a834":"code","a81f28de":"code","dc53defb":"code","a00e4ca7":"code","d5762ff6":"code","8cbf3180":"markdown","e58810e7":"markdown","9286d3c1":"markdown","e88f04cb":"markdown","606ad463":"markdown","bdd0f383":"markdown","f1050ced":"markdown","44a78eb1":"markdown","a885966f":"markdown"},"source":{"6cc10ce3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","053729d5":"import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer","f7e24726":"sms = pd.read_csv('\/kaggle\/input\/spam-or-ham\/spam.csv',encoding='latin-1')\nsms.head()","5373aba5":"sms.info()","2d9c6716":"sms.shape","e0928ab3":"sms.describe()","2f8398b7":"round(100*(sms.isnull().sum()\/len(sms)),2)","64d66970":"sms=sms.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\nsms=sms.rename(columns={\"v1\":\"label\",\"v2\":\"message\"})","2f70f740":"sms.head()","c041da9f":"sms['length']=sms['message'].apply(len)\nsms.head()","93aff7c4":"sms['label']=sms['label'].map({\"ham\":0,\"spam\":1})","85cabb0c":"def preprocess(document):\n    # change sentence to lower case\n    document = document.lower()\n\n    # tokenize into words\n    words = word_tokenize(document)\n\n    # remove stop words\n    words = [word for word in words if word not in stopwords.words(\"english\")]\n\n    # join words to make sentence\n    document = \" \".join(words)\n    \n    return document","815e383b":"# extract the messages from the dataframe\nmessages = sms.message\nprint(messages)","36e93cf9":"# convert messages into list\nmessages = [message for message in messages]\n#print(messages)","3bd53142":"# preprocess messages using the preprocess function\nmessages = [preprocess(message) for message in messages]","2622fbb3":"# bag of words model\nvectorizer = CountVectorizer()\nbow_model = vectorizer.fit_transform(messages)\nprint(bow_model.toarray())","4e058464":"print(bow_model.shape)\nprint(vectorizer.get_feature_names())","5e15d1ba":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstemmer = PorterStemmer()\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# adding stemming and lemmatisation in the preprocess function\ndef preprocess(document, stem=True):\n    # change sentence to lower case\n    document = document.lower()\n\n    # tokenize into words\n    words = word_tokenize(document)\n\n    # remove stop words\n    words = [word for word in words if word not in stopwords.words(\"english\")]\n    \n    if stem:\n        words = [stemmer.stem(word) for word in words]\n    else:\n        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n\n    # join words to make sentence\n    document = \" \".join(words)\n    \n    return document","e6e11e57":"# stem messages\nmessages = [preprocess(message, stem=True) for message in sms.message]\n\n# bag of words model\nvectorizer = CountVectorizer()\nbow_model = vectorizer.fit_transform(messages)","ac39a834":"# look at the dataframe\npd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())","a81f28de":"# token names\nprint(vectorizer.get_feature_names())","dc53defb":"# lemmatise messages\nmessages = [preprocess(message, stem=False) for message in sms.message]\n\n# bag of words model\nvectorizer = CountVectorizer()\nbow_model = vectorizer.fit_transform(messages)","a00e4ca7":"# look at the dataframe\npd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())","d5762ff6":"# token names\nprint(vectorizer.get_feature_names())","8cbf3180":"### Creating bag of words model using count vectorizer function","e58810e7":"## Note:\nSometimes we will be confused in whether to use a stemmer or a lemmatizer in our application. so keep in mind below points to help you with.\n- Stemmer is a rule based technique,Therefore it is much faster than the lemmatizer. so we need to choose the techniques wisely.\n- you want to consider a stemmer rather than a lemmatiser if you notice that POS tagging is inaccurate.","9286d3c1":"## Preprocess function Using Stemming and lemmatising","e88f04cb":"## Stemming and lemmatising","606ad463":"# Introduction\nif we create a model with out Stemming and Lemmatization. This will result in an inefficient model. Stemming makes sure that different variations of a word, say \u2018warm\u2019, warmer\u2019, \u2018warming\u2019 etc\u2019 are represented by a single token - \u2018warm\u2019, because they all represent the same information. Another similar preprocessing step is lemmatisation.\nwe will see below these two techniques that will help us to deal with the problem of redundant tokens:\n1. Stemming\n2. Lemmatization","bdd0f383":"## Let's try lemmatizing the messages.","f1050ced":"## Bag of words model on stemmed messages","44a78eb1":"## Observations:\n- A lot of duplicate tokens such as 'win'and 'winner'; 'reply' and 'replying'; 'want' and 'wanted' etc.","a885966f":"## Preprocess function with out Stemming and Lemmatising"}}