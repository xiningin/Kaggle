{"cell_type":{"23e24b90":"code","70114fb3":"code","7d1148cc":"code","f8b11836":"code","a5fbfe7c":"code","ae68e154":"code","85e63fb6":"code","9739f2d3":"markdown","80661aad":"markdown"},"source":{"23e24b90":"#Important libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n# Pretty display for notebooks\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans ","70114fb3":"#Read_Data\ndata = pd.read_csv('..\/input\/customer-segmentaion\/customers.csv' , sep = ',' , encoding ='utf8')","7d1148cc":"def preprocessing (data):\n    data['fresh']  = np.log(data['Fresh'])\n    data['milk']   = np.log(data['Milk'])\n    data['grocery']= np.log(data['Grocery'])\n    data[\"frozen\"] = np.log(data[\"Frozen\"])\n    data[\"detergents_Paper\"] = np.log(data[\"Detergents_Paper\"])\n    data[\"delicatessen\"]     = np.log(data[\"Delicatessen\"])\n    \n    #Drop Columns after log transform\n    data.drop(['Fresh' , 'Milk','Grocery' , 'Frozen' , 'Detergents_Paper' , 'Delicatessen'],axis=1 ,inplace = True)\n    #convert to object to be more readable\n    #Channel:{Hotel\/Restaurant\/Cafe - 1, Retail - 2}\n    data['Channel'].replace({1:\"h-r-c\" , 2:\"Retail\"} , inplace=True)\n    #Region:{Lisbon - 1, Oporto - 2, or Other - 3} \n    data['Region'].replace({1:\"Lisbon\" , 2:\"Oporto\" , 3:\"Other Zone\"}, inplace=True)  \n    #We have transformed categorical columns to dummy.\n    data= pd.concat([data, pd.get_dummies(data[\"Channel\"], drop_first=True),pd.get_dummies(data[\"Region\"])], axis=1)\n    data.drop(columns=[\"Channel\", \"Region\"], axis=1, inplace=True)\n    #drop for Retail columns as the same column of h-r-c\n\n    outliers_list = []\n# For each feature find the data points with extreme high or low values\n    for feature in data.keys():\n        # Calculate Q1 (25th percentile of the data) for the given feature\n        Q1 = np.percentile(data[feature], 25)\n        # Calculate Q3 (75th percentile of the data) for the given feature\n        Q3 = np.percentile(data[feature], 75)\n        # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n        step = (Q3 - Q1) * 1.5\n        outliers = list(data[~((data[feature] >= Q1 - step) & (data[feature] <= Q3 + step))].index.values)\n        outliers_list.extend(outliers)\n    duplicate_outliers_list = list(set([x for x in outliers_list if outliers_list.count(x) >= 2]))\n    # Remove the outliers\n    outliers  = duplicate_outliers_list\n    new_data = data.drop(data.index[outliers]).reset_index(drop = True)\n#Before clustering, we transform features from original version to standardize version\n#as after dummy for two columns has zero and ones \n#and another columns has data by milliones \n\n    scaler= StandardScaler()\n    std_data= scaler.fit_transform(new_data)\n    mean_vec = np.mean(std_data, axis=0)\n    cov_mat = (std_data - mean_vec).T.dot((std_data - mean_vec)) \/ (std_data.shape[0]-1)    \n    cov_mat = np.cov(std_data.T)\n    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n    tot = sum(eig_vals)\n    var_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\n    cum_var_exp = np.cumsum(var_exp)\n    # Apply PCA by fitting the new data with the same number of dimensions as features\n\n#svd_solver auto , full , arpack\n#n_component is number of new features\n#n_component is 4 as important component\n    pca = PCA(n_components=4, copy=True , svd_solver='full' , random_state=0 , iterated_power='auto' ,whiten = False)\n    reduced_data = pca.fit_transform(std_data)\n    # Create a DataFrame for the reduced data\n    reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2','Dimension 3', 'Dimension 4'])\n    \n    return reduced_data\n","f8b11836":"new_data = preprocessing(data)\nnew_data.head()","a5fbfe7c":"KMeanModel = KMeans(n_clusters= 3, init='k-means++' , random_state=33 , algorithm='auto')\n#algorithm is auto , full or elkan\ndef train(data_pro):\n    #Fitting Model\n    KMeanModel.fit(data_pro)\n    return 'train is Done successful:)'\n\n#calling function\ntrain()","ae68e154":"def predict(data_pro):\n    y_predict=KMeanModel.predict(data_pro)\n    return y_predict\n#call function\npredict()","85e63fb6":"def evaluation (data_pro):\n    labels  = KMeanModel.labels_\n    silhouette_Score = silhouette_score(data_pro, labels)\n    return silhouette_Score\n#call function\nevaluation()","9739f2d3":"**General Information Of Data**\n* The customer segments data is included as a selection of 440 data points collected on data found from clients of a wholesale distributor in Lisbon, Portugal.\n\n **Features**\n* Fresh: annual spending (m.u.) on fresh products (Continuous);\n* Milk: annual spending (m.u.) on milk products (Continuous);\n* Grocery: annual spending (m.u.) on grocery products (Continuous);\n* Frozen: annual spending (m.u.) on frozen products (Continuous);\n* Detergents_Paper: annual spending (m.u.) on detergents and paper products (Continuous);\n* Delicatessen: annual spending (m.u.) on and delicatessen products (Continuous);\n* Channel: {Hotel\/Restaurant\/Cafe - 1, Retail - 2} (Nominal)\n* Region: {Lisbon - 1, Oporto - 2, or Other - 3} (Nominal)","80661aad":"# Clustering Model"}}