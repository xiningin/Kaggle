{"cell_type":{"e445e604":"code","b4736a48":"code","437f6145":"code","b782abcc":"code","bd6a767f":"code","b39768af":"code","927e7be4":"code","0625efa9":"code","6e478675":"code","7b7665a0":"code","fafeaaf9":"code","0df7f1c4":"code","fba77aaa":"code","41cbe972":"code","4508ace2":"code","7cad1b3d":"code","01a511d4":"code","ec848bb8":"code","722ec654":"markdown","49904844":"markdown"},"source":{"e445e604":"!pip install sklearn_evaluation\n!pip install xgboost\n!pip install lightgbm\n\n#=====================================#","b4736a48":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nimport matplotlib.pyplot as plt # plotting\nfrom sklearn_evaluation import plot\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split,GridSearchCV\n\nfrom sklearn import preprocessing\nfrom sklearn.externals import joblib\n\n\n%matplotlib inline\n#=====================================#","437f6145":"cols = \"\"\"\n    duration,\nprotocol_type,\nservice,\nflag,\nsrc_bytes,\ndst_bytes,\nland,\nwrong_fragment,\nurgent,\nhot,\nnum_failed_logins,\nlogged_in,\nnum_compromised,\nroot_shell,\nsu_attempted,\nnum_root,\nnum_file_creations,\nnum_shells,\nnum_access_files,\nnum_outbound_cmds,\nis_host_login,\nis_guest_login,\ncount,\nsrv_count,\nserror_rate,\nsrv_serror_rate,\nrerror_rate,\nsrv_rerror_rate,\nsame_srv_rate,\ndiff_srv_rate,\nsrv_diff_host_rate,\ndst_host_count,\ndst_host_srv_count,\ndst_host_same_srv_rate,\ndst_host_diff_srv_rate,\ndst_host_same_src_port_rate,\ndst_host_srv_diff_host_rate,\ndst_host_serror_rate,\ndst_host_srv_serror_rate,\ndst_host_rerror_rate,\ndst_host_srv_rerror_rate\"\"\"\ncols = [c.strip() for c in cols.split(\",\") if c.strip()]\ncols.append('target')\n#print(len(cols))\n\n#=====================================#","b782abcc":"attacks_type = {\n'normal': 'normal',\n'back': 'dos',\n'buffer_overflow': 'u2r',\n'ftp_write': 'r2l',\n'guess_passwd': 'r2l',\n'imap': 'r2l',\n'ipsweep': 'probe',\n'land': 'dos',\n'loadmodule': 'u2r',\n'multihop': 'r2l',\n'neptune': 'dos',\n'nmap': 'probe',\n'perl': 'u2r',\n'phf': 'r2l',\n'pod': 'dos',\n'portsweep': 'probe',\n'rootkit': 'u2r',\n'satan': 'probe',\n'smurf': 'dos',\n'spy': 'r2l',\n'teardrop': 'dos',\n'warezclient': 'r2l',\n'warezmaster': 'r2l',\n    }\n\n#=====================================#","bd6a767f":"hajar_to_cup = {\n    'is_hot_login' : 'is_host_login',\n'urg' : 'urgent',\n'protocol' : 'protocol_type',\n'count_sec' : 'count',\n'srv_count_sec' : 'srv_count',\n'serror_rate_sec' : 'serror_rate',\n'srv_serror_rate_sec' : 'srv_serror_rate',\n'rerror_rate_sec' : 'rerror_rate',\n'srv_error_rate_sec' : 'srv_rerror_rate',\n'same_srv_rate_sec' : 'same_srv_rate',\n'diff_srv_rate_sec' : 'diff_srv_rate',\n'srv_diff_host_rate_sec' : 'srv_diff_host_rate',\n'count_100' : 'dst_host_count',\n'srv_count_100' : 'dst_host_srv_count',\n'same_srv_rate_100' : 'dst_host_same_srv_rate',\n'diff_srv_rate_100' : 'dst_host_diff_srv_rate',\n'same_src_port_rate_100' : 'dst_host_same_src_port_rate',\n'srv_diff_host_rate_100' : 'dst_host_srv_diff_host_rate',\n'serror_rate_100' : 'dst_host_serror_rate',\n'srv_serror_rate_100' : 'dst_host_srv_serror_rate',\n'rerror_rate_100' : 'dst_host_rerror_rate',\n'srv_rerror_rate_100' : 'dst_host_srv_rerror_rate',\n}\n\n#=====================================#","b39768af":"selcted_features  = ['duration', 'protocol_type', 'flag', 'src_bytes', 'dst_bytes', 'hot',\n       'logged_in', 'num_compromised', 'count', 'srv_count', 'serror_rate',\n       'diff_srv_rate', 'dst_host_count', 'dst_host_srv_count',\n       'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n       'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n       'dst_host_serror_rate']\n#=====================================#","927e7be4":"needed_cols_dump = []\nfor l in selcted_features:\n    if l in hajar_to_cup.values():\n        for k, v in hajar_to_cup.items():\n            if v == l:\n                needed_cols_dump.append(k)\n    else:\n        needed_cols_dump.append(l)\nprint(len(needed_cols_dump), len(selcted_features))\nprint(needed_cols_dump)","0625efa9":"def standardize_columns(df, cols_map=hajar_to_cup):\n    \"\"\"\n    1- Delete the 'service' column.\n    2- Verify if TCPDUMP columns exists, then they will renamed\n    \"\"\"\n    if 'service' in df.columns:\n        df = df.drop(['service'], axis = 1)\n    df = df.rename(columns = cols_map)\n    return df\n\ndef do_what_we_want(X, \n                    scaler_1, \n                    le_X_cols, \n                    selcted_features, \n                    map_cols,\n                    rdf_clf,\n                    xgb_clf,\n                    PathX=False):\n    if PathX:\n        X = pd.read_csv(PathX, names=cols, nrows=30000)\n    X = standardize_columns(X, cols_map=map_cols) # Rename the columns, and delete the \n    X[['dst_bytes','src_bytes']] = scaler_1.fit_transform(X[['dst_bytes','src_bytes']])\n    X = X[selcted_features]\n    for c in X.columns:\n        if str(X[c].dtype) == 'object': \n            le_X = le_X_cols[c]\n            X[c] = le_X.transform(X[c])\n            \n    #====\n    res = {\n        'rd_prd_prb': rdf_clf.predict_proba(X),\n        'rd_prd': rdf_clf.predict(X),\n        'xgb_prd_prb': xgb_clf.predict_proba(X),\n        'xgb_prd': xgb_clf.predict(X),\n        \n    }\n    \n    return res\n\n#=====================================#","6e478675":"scaler_1 = joblib.load('..\/input\/first-start-kernel\/scaler_1.pkl') \nle_X_cols = joblib.load('..\/input\/first-start-kernel\/le_X_cols.pkl') \nle_y = joblib.load('..\/input\/first-start-kernel\/le_y.pkl') \nxgb_clf = joblib.load('..\/input\/first-start-kernel\/xgboost_classifier.pkl') \nrdf_clf = joblib.load('..\/input\/first-start-kernel\/random_forest_classifier.pkl') \n\n#=====================================#","7b7665a0":"X = pd.read_csv(\"..\/input\/kdd-cup-1999-data\/kddcup.data_10_percent\/kddcup.data_10_percent\", names=cols, nrows=100000)\nY = X.target.apply(lambda r: attacks_type[r[:-1]])\n\n#=====================================#","fafeaaf9":"X.tail(5)","0df7f1c4":"print(np.unique(Y))\nY = le_y.transform(Y.values)\nprint(np.unique(Y))\nprint(le_y.inverse_transform(Y))\n#=====================================#","fba77aaa":"res = do_what_we_want(X, \n                    scaler_1, \n                    le_X_cols, \n                    selcted_features, \n                    hajar_to_cup,\n                    rdf_clf,\n                    xgb_clf,\n                    PathX=False)\nres.keys()\n#=====================================#","41cbe972":"# print(len(res['rd_prd']), np.unique(res['rd_prd']))\n# sum(np.argmax(res['rd_prd_prb'], axis=1) == res['rd_prd'])\n# type(res['rd_prd_prb'])","4508ace2":"atks = ['dos', 'normal', 'probe', 'r2l', 'u2r']\nrd_prd_df = pd.DataFrame(data=res['rd_prd_prb'])\nrd_prd_df= rd_prd_df.rename(columns = {l:'rd_'+atks[l] for l in range(len(atks))})\nxg_prd_df = pd.DataFrame(data=res['xgb_prd_prb'])\nxg_prd_df= xg_prd_df.rename(columns = {l:'xg_'+atks[l] for l in range(len(atks))})\n\ndf = pd.concat([rd_prd_df, xg_prd_df], axis=1)\ndf.head()\n#=====================================#","7cad1b3d":"params={\"C\":np.logspace(-7,7,7), \"penalty\":[\"l2\"], \"multi_class\":['auto','ovr']}\nlg = LogisticRegression(C=4.5, random_state = 42, multi_class = 'ovr', solver = 'lbfgs', max_iter = 1000)\nclf = GridSearchCV(lg, params, cv=3)\nclf.fit(df[:20000], Y[:20000])\nprint(\"Train score is:\", clf.score(df[:20000], Y[:20000]))\nprint(\"Test score id:\",clf.score(df[20000:], Y[20000:]))# New data, not included in Training data\n#=====================================#","01a511d4":"tcpdump_cols =\"num_conn, startTimet, orig_pt, resp_pt, orig_ht, resp_ht, duration, protocol, resp_pt_2, flag, src_bytes, dst_bytes, land, wrong_fragment, urg, hot, num_failed_logins, logged_in, num_compromised, root_shell, su_attempted, num_root, num_file_creations, num_shells, num_access_files, num_outbound_cmds, is_hot_login, is_guest_login, count_sec, srv_count_sec, serror_rate_sec, srv_serror_rate_sec, rerror_rate_sec, srv_error_rate_sec, same_srv_rate_sec, diff_srv_rate_sec, srv_diff_host_rate_sec, count_100, srv_count_100, same_srv_rate_100, diff_srv_rate_100, same_src_port_rate_100, srv_diff_host_rate_100, serror_rate_100, srv_serror_rate_100, rerror_rate_100, srv_rerror_rate_100\"\ntcpdump_cols = tcpdump_cols.split(\", \")\nneeded_cols_dump = ['duration', 'protocol', 'flag', 'src_bytes', 'dst_bytes', 'hot', 'logged_in', 'num_compromised', 'count_sec', 'srv_count_sec', 'serror_rate_sec', 'diff_srv_rate_sec', 'count_100', 'srv_count_100', 'same_srv_rate_100', 'diff_srv_rate_100', 'same_src_port_rate_100', 'srv_diff_host_rate_100', 'serror_rate_100']\n\nTCP_DUMP = pd.read_csv(\"..\/input\/tcpdumpdatakddcup\/tcpdump.csv\", sep=' ', lineterminator='\\n', names=tcpdump_cols)\nprint(TCP_DUMP.shape)\nTCP_DUMP.head()","ec848bb8":"res = do_what_we_want(TCP_DUMP, \n                    scaler_1, \n                    le_X_cols, \n                    selcted_features, \n                    hajar_to_cup,\n                    rdf_clf,\n                    xgb_clf,\n                    PathX=False)\ndf = pd.DataFrame(index=TCP_DUMP.index)\ndf['typeAttack'] = le_y.inverse_transform(res['rd_prd'])\ndf['isAnomaly'] = (df['typeAttack']!='normal').astype('int')\nprint(\"Nb anomalies  is :\",sum(df['isAnomaly']))\nprint(df['typeAttack'].value_counts())\ndf[1740:1750]","722ec654":"### **Ensembling by unsig LogistcRegression to stacking predictions results **","49904844":"### Test TcpDumpData"}}