{"cell_type":{"b8fc1bc9":"code","8ca7cf3a":"code","5f8e4c5e":"code","9223788f":"code","2f25138d":"code","547c5d0d":"code","a03caaa6":"code","612ac871":"code","edeb7c1c":"code","ad3e0dbc":"code","6faf988e":"code","3c16c68d":"code","24ebfc87":"code","962b3610":"code","d0b85bd5":"code","1d107586":"code","56a83aba":"code","246ddeca":"code","ced83088":"code","6f549c35":"code","d98d341c":"code","c75c9646":"code","371c0668":"code","ef856cbd":"code","ae040fee":"code","0e1e8b64":"code","e6dff018":"code","fe5c2142":"code","4e6da285":"code","c7e56bfe":"code","559cc46e":"code","263cfe20":"code","65773b70":"code","e493be96":"code","b8dec5ef":"code","39c037db":"code","77ac027a":"code","6bb3926b":"code","d432fa9b":"markdown","4af09eea":"markdown","28f58982":"markdown","24612ac4":"markdown","e53a9757":"markdown","f66a199b":"markdown","7ba01ed6":"markdown","30994575":"markdown","d2df7b30":"markdown","b9e0dcb5":"markdown","9f1016a5":"markdown","dcc714d5":"markdown","83e13c5f":"markdown","bd4847c6":"markdown","4c6e20c8":"markdown","ccb7cd7c":"markdown","a6fabebe":"markdown","cd77a52c":"markdown","a54ca5e1":"markdown","4bc908ef":"markdown","ae124aae":"markdown","e6dbc25c":"markdown"},"source":{"b8fc1bc9":"# import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, re, time, math, tqdm, itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.offline as pyo\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neural_network import MLPClassifier\nimport keras\nfrom keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.callbacks import CSVLogger, ModelCheckpoint","8ca7cf3a":"# check the available data\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5f8e4c5e":"!nvidia-smi","9223788f":"%%time\n# load the data into memory\nnetwork_data = pd.read_csv('..\/input\/ids-intrusion-csv\/02-14-2018.csv')","2f25138d":"# check the shape of data\nnetwork_data.shape","547c5d0d":"# check the number of rows and columns\nprint('Number of Rows (Samples): %s' % str((network_data.shape[0])))\nprint('Number of Columns (Features): %s' % str((network_data.shape[1])))","a03caaa6":"network_data.head(4)","612ac871":"# check the columns in data\nnetwork_data.columns","edeb7c1c":"# check the number of columns\nprint('Total columns in our data: %s' % str(len(network_data.columns)))","ad3e0dbc":"network_data.info()","6faf988e":"# check the number of values for labels\nnetwork_data['Label'].value_counts()","3c16c68d":"# make a plot number of labels\nsns.set(rc={'figure.figsize':(12, 6)})\nplt.xlabel('Attack Type')\nsns.set_theme()\nax = sns.countplot(x='Label', data=network_data)\nax.set(xlabel='Attack Type', ylabel='Number of Attacks')\nplt.show()","24ebfc87":"# make a scatterplot\npyo.init_notebook_mode()\nfig = px.scatter(x = network_data[\"Bwd Pkts\/s\"][:100000], \n                 y=network_data[\"Fwd Seg Size Min\"][:100000])\nfig","962b3610":"%%time\nsns.set(rc={'figure.figsize':(12, 6)})\nsns.scatterplot(x=network_data['Bwd Pkts\/s'][:50000], y=network_data['Fwd Seg Size Min'][:50000], \n                hue='Label', data=network_data)","d0b85bd5":"# check the dtype of timestamp column\n(network_data['Timestamp'].dtype)","1d107586":"# check for some null or missing values in our dataset\nnetwork_data.isna().sum().to_numpy()","56a83aba":"# drop null or missing columns\ncleaned_data = network_data.dropna()\ncleaned_data.isna().sum().to_numpy()","246ddeca":"# encode the column labels\nlabel_encoder = LabelEncoder()\ncleaned_data['Label']= label_encoder.fit_transform(cleaned_data['Label'])\ncleaned_data['Label'].unique()","ced83088":"# check for encoded labels\ncleaned_data['Label'].value_counts()","6f549c35":"# make 3 seperate datasets for 3 feature labels\ndata_1 = cleaned_data[cleaned_data['Label'] == 0]\ndata_2 = cleaned_data[cleaned_data['Label'] == 1]\ndata_3 = cleaned_data[cleaned_data['Label'] == 2]\n\n# make benign feature\ny_1 = np.zeros(data_1.shape[0])\ny_benign = pd.DataFrame(y_1)\n\n# make bruteforce feature\ny_2 = np.ones(data_2.shape[0])\ny_bf = pd.DataFrame(y_2)\n\n# make bruteforceSSH feature\ny_3 = np.full(data_3.shape[0], 2)\ny_ssh = pd.DataFrame(y_3)\n\n# merging the original dataframe\nX = pd.concat([data_1, data_2, data_3], sort=True)\ny = pd.concat([y_benign, y_bf, y_ssh], sort=True)","d98d341c":"y_1, y_2, y_3","c75c9646":"print(X.shape)\nprint(y.shape)","371c0668":"# checking if there are some null values in data\nX.isnull().sum().to_numpy()","ef856cbd":"from sklearn.utils import resample\n\ndata_1_resample = resample(data_1, n_samples=20000, \n                           random_state=123, replace=True)\ndata_2_resample = resample(data_2, n_samples=20000, \n                           random_state=123, replace=True)\ndata_3_resample = resample(data_3, n_samples=20000, \n                           random_state=123, replace=True)","ae040fee":"train_dataset = pd.concat([data_1_resample, data_2_resample, data_3_resample])\ntrain_dataset.head(2)","0e1e8b64":"# viewing the distribution of intrusion attacks in our dataset \nplt.figure(figsize=(10, 8))\ncircle = plt.Circle((0, 0), 0.7, color='white')\nplt.title('Intrusion Attack Type Distribution')\nplt.pie(train_dataset['Label'].value_counts(), labels=['Benign', 'BF', 'BF-SSH'], colors=['blue', 'magenta', 'cyan'])\np = plt.gcf()\np.gca().add_artist(circle)","e6dff018":"test_dataset = train_dataset.sample(frac=0.1)\ntarget_train = train_dataset['Label']\ntarget_test = test_dataset['Label']\ntarget_train.unique(), target_test.unique()","fe5c2142":"y_train = to_categorical(target_train, num_classes=3)\ny_test = to_categorical(target_test, num_classes=3)","4e6da285":"train_dataset = train_dataset.drop(columns = [\"Timestamp\", \"Protocol\",\"PSH Flag Cnt\",\"Init Fwd Win Byts\",\"Flow Byts\/s\",\"Flow Pkts\/s\", \"Label\"], axis=1)\ntest_dataset = test_dataset.drop(columns = [\"Timestamp\", \"Protocol\",\"PSH Flag Cnt\",\"Init Fwd Win Byts\",\"Flow Byts\/s\",\"Flow Pkts\/s\", \"Label\"], axis=1)","c7e56bfe":"# making train & test splits\nX_train = train_dataset.iloc[:, :-1].values\nX_test = test_dataset.iloc[:, :-1].values\nX_test","559cc46e":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","263cfe20":"# reshape the data for CNN\nX_train = X_train.reshape(len(X_train), X_train.shape[1], 1)\nX_test = X_test.reshape(len(X_test), X_test.shape[1], 1)\nX_train.shape, X_test.shape","65773b70":"# making the deep learning function\ndef model():\n    model = Sequential()\n    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n                    padding='same', input_shape=(72, 1)))\n    model.add(BatchNormalization())\n    \n    # adding a pooling layer\n    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n    \n    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n                    padding='same', input_shape=(72, 1)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n    \n    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n                    padding='same', input_shape=(72, 1)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n    \n    model.add(Flatten())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","e493be96":"model = model()\nmodel.summary()","b8dec5ef":"logger = CSVLogger('logs.csv', append=True)\nhis = model.fit(X_train, y_train, epochs=30, batch_size=32, \n          validation_data=(X_test, y_test), callbacks=[logger])","39c037db":"# check the model performance on test data\nscores = model.evaluate(X_test, y_test)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))","77ac027a":"# check history of model\nhistory = his.history\nhistory.keys()","6bb3926b":"epochs = range(1, len(history['loss']) + 1)\nacc = history['accuracy']\nloss = history['loss']\nval_acc = history['val_accuracy']\nval_loss = history['val_loss']\n\n# visualize training and val accuracy\nplt.figure(figsize=(10, 5))\nplt.title('Training and Validation Accuracy (CNN)')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.plot(epochs, acc, label='accuracy')\nplt.plot(epochs, val_acc, label='val_acc')\nplt.legend()\n\n# visualize train and val loss\nplt.figure(figsize=(10, 5))\nplt.title('Training and Validation Loss(CNN)')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.plot(epochs, loss, label='loss', color='g')\nplt.plot(epochs, val_loss, label='val_loss', color='r')\nplt.legend()","d432fa9b":"Most of the network intrusions in our data are benign, as output from above code cell. ","4af09eea":"From the graphs, we came to know that:\n* Most of the attacks made by intruders are malignant (almost 700k)\n* **FTP-BruteFore** and **SSH-BruteForce** type attacks are less in numbers (less than 200k)\n* Most of the intruders try to make a malignant attack on network systems","28f58982":"The output of above cell shows that there are no null values in our data, and the data can now be used for model fitting. We have two types of datasets, normal and abnormal, and they'll be used for model fitting.","24612ac4":"## Data Argumentation\nTi avoid biasing in data, we need to use data argumentation on it so that we can remove bias from data and make equal distributions.","e53a9757":"## Data Visualizations\nAfter getting some useful information about our data, we now make visuals of our data to see how the trend in our data goes like. The visuals include bar plots, distribution plots, scatter plots, etc.","f66a199b":"## Making X & Y Variables (CNN)","7ba01ed6":"## Data Splicing\nThis stage involves the data split into train & test sets. The training data will be used for training our model, and the testing data will be used to check the performance of model on unseen dataset. We're using a split of **80-20**, i.e., **80%** data to be used for training & **20%** to be used for testing purpose.","30994575":"## Visualization of Results (CNN)\nLet's make a graphical visualization of results obtained by applying CNN to our data.","d2df7b30":"We have a lot of data available to deal with in this notebook. We will perform analysis, preprocessing and modeling on one of the datasets and will conclude the results at the end.","b9e0dcb5":"The dataset is huge. We have a total of **80** columns in our data.","9f1016a5":"# Conclusion after CNN Training\nAfter training our deep CNN model on training data and validating it on validation data, it can be interpreted that:\n* Model was trained on 50 epochs and then on 30 epochs\n* CNN performed exceptionally well on training data and the accuracy was **99%**\n* Model accuracy was down to **83.55%** on valiadtion data after **50** iterations, and gave a good accuracy of **92%** after **30** iterations. Thus, it can be interpreted that optimal number of iterations on which this model can perform are **30**.","dcc714d5":"All features in the data have no null or missing values, except one feature that contains **2277** missing values. We need to remove this column from our data, so that our data may get cleaned.","83e13c5f":"## Shaping the data for CNN\nFor applying a convolutional neural network on our data, we will have to follow following steps:\n* Seperate the data of each of the labels\n* Create a numerical matrix representation of labels\n* Apply resampling on data so that can make the distribution equal for all labels\n* Create X (predictor) and Y (target) variables\n* Split the data into train and test sets\n* Make data multi-dimensional for CNN\n* Apply CNN on data","bd4847c6":"# Importing Libraries\nFirst, we will import libraries that we need to start our workflow. The libraries we are using are:\n* NumPy\n* Pandas\n* Matplotlib\n* Scikit-learn\n* Keras\n* TensorFlow","4c6e20c8":"We have a total of **1 million+** samples and **80** features in data.","ccb7cd7c":"# Loading the Data\nFirst step is to load the available data into our memory.","a6fabebe":"The following information tells us that:\n* We have a huge amount of data, containing **1 million+** entries (samples)\n* There are a total of **80** columns belinging to each sample\n* There are missing values in our data, which need to be filled or dropped for proper modelling\n* The memory consumption of data is **700 MB**","cd77a52c":"# Network Intrusion Detection Using Machine Learning\/Deep Learning\nThis notebook involves the making of machine learning & deep learning models to classify the given data of obtained as a network intrusion into differen classes (malignant or benign). Given a sample point, the objective of machine learning model will be to classify that whether the intrusion made is  **Benign** or is a **BruteForce** (either FTP or SSH).","a54ca5e1":"# Data Preprocessing\nData preprocessing plays an important part in the process of data science, since data may not be fully clean and can contain missing or null values. In this step, we are undergoing some preprocessing steps that will help us if there is any null or missing value in our data. ","4bc908ef":"### Label Encoding\nThe Label feature in the data contains 3 labels as **Benign**, **BruteForceFTP** and **BruteForceSSH**. All these are in string format. For our neural network, we need to convert them into numbers so that our NN may understand their representations.","ae124aae":"After removing the missing valued column in our data, we have now no feature that contains any missing or null value. Data is cleaned now.","e6dbc25c":"# EDA (Exploratory Data Analysis)\nFor making a proper undertanding of dataset we are using, we will perform a bief EDA (Exploratory Data Analysis). The EDA is sub-divided into:\n* Data Visuals\n* Data Understanding\n* Data Analysis"}}