{"cell_type":{"73c98952":"code","10e3314c":"code","f15da04c":"code","e8fe15de":"code","1eb9664d":"code","10e8adf7":"code","0b72feeb":"code","82b3d9eb":"code","66183c0f":"code","5e12478f":"code","67b029fb":"code","a4e66897":"code","3d24146b":"code","e57d005e":"code","ec809c21":"code","accab8a3":"code","3d369903":"markdown","9330fae4":"markdown","14fad9a7":"markdown","3daba985":"markdown","70119196":"markdown","989f549b":"markdown","ce086298":"markdown","32334560":"markdown","efc342b0":"markdown","fab2b302":"markdown","96f031c4":"markdown","8e67a9c2":"markdown","f66d255a":"markdown","11f8f137":"markdown"},"source":{"73c98952":"#!\/usr\/bin\/env python\n# coding: utf-8\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport csv\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","10e3314c":"batch_size = 32\nimg_height = img_width = 28\nlearning_rate = 0.001\nepochs = 200","f15da04c":"dataset = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nprint(f\"Dataset shape: {dataset.shape}\")","e8fe15de":"testset = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nprint(f\"Testset shape: {testset.shape}\")","1eb9664d":"record_defaults = [tf.int32]\nlabel_data = tf.data.experimental.CsvDataset(\"\/kaggle\/input\/digit-recognizer\/train.csv\", record_defaults, header=True, select_cols=[0])","10e8adf7":"unique, counts = np.unique([x[0] for x in label_data.as_numpy_iterator()], return_counts=True)\nfreq = dict(zip(unique, counts))\nplt.bar(list(freq.keys()), list(freq.values()))","0b72feeb":"train_x_all = dataset.iloc[:, 1:785].values.reshape(-1, 28, 28, 1)  # loc(row, col) - reshape to make 3D\ntrain_y_all = tf.keras.utils.to_categorical(dataset.iloc[:, 0])\nprint(train_y_all[6])\nplt.imshow(tf.squeeze(train_x_all[6]))","82b3d9eb":"train_x, val_x, train_y, val_y = train_test_split(train_x_all, train_y_all, test_size=0.20)\n\ntestset = testset.values.reshape(-1, 28, 28, 1)\nprint(f\"Train Set Shape: {train_x.shape, train_y.shape}\")\nprint(f\"Val Set Shape: {val_x.shape, val_y.shape}\")","66183c0f":"index = 1\nplt.imshow(tf.squeeze(train_x[index]))\nprint(f\"Training label: {train_y[index]}\")","5e12478f":"def basic_model():\n    inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"tanh\")(inputs)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"tanh\")(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"tanh\")(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"tanh\")(x)\n    x = tf.keras.layers.MaxPooling2D((3, 3))(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n    outputs = tf.keras.layers.Dense(10, activation=\"softmax\")(x)\n    return tf.keras.models.Model(inputs=inputs, outputs=outputs)","67b029fb":"model = basic_model()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy'])\nprint(model.summary())","a4e66897":"dataset_len = dataset.shape[0]\ntraining_set = tf.data.Dataset.from_tensors((train_x, train_y))\nvalidation_set = tf.data.Dataset.from_tensors((val_x, val_y))\nprint(f\"Training set size: {tf.data.experimental.cardinality(training_set)}\")\nprint(f\"Validation set size: {tf.data.experimental.cardinality(validation_set)}\")\nhistory = model.fit(training_set, validation_data=validation_set, epochs=epochs)","3d24146b":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","e57d005e":"predictions = model.predict(testset)\nprint(predictions)","ec809c21":"def check(index):\n    print(tf.math.argmax(predictions[index]))\n    plt.imshow(test_data[index])\ncheck(68)","accab8a3":"with open('submissions.csv', 'w', newline='') as csvfile:\n    output_writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n    output_writer.writerow(['ImageId', 'Label'])\n    for i, pred in enumerate(integerfied_preds):\n        output_writer.writerow([i+1, pred.numpy()])","3d369903":"# Predictions","9330fae4":"# Plot model performance","14fad9a7":"# Build a simple model\nThe model alternates convolutions with pooling with dropout","3daba985":"First massage the data set to train_x and train_y - note we don't have to do the 80\/20 split for a validation set as the ImageDataGenerator already does that for us.\n\nWe also need to reshape the image to the 28x28x1 resolution (Grayscale, 1 color channel) in order to use it with the NN tensorflow stuff\n\nNote: the labels are 0 - 9, so we convert the labels into a one-hot encoding representation (binary representation in array form). This will make labeling easier later.","70119196":"## Get labels - first column","989f549b":"# Import and explore training set","ce086298":"# Write predictions to submissions.csv","32334560":"## Explore test set","efc342b0":"Take a look at the data set to make sure we are getting the right images","fab2b302":"And then visualize it - this essentially provides a frequency diagram on the distribution of numbers in the dataset","96f031c4":"Retrieve the label data from the train data set.","8e67a9c2":"# Training\nSplit the test set and train on 80\/20 with a basic CNN with 4 hidden layers.","f66d255a":"# Define variables","11f8f137":"# Train the model"}}