{"cell_type":{"5d72ed8c":"code","ad0800bf":"code","0c4c6b46":"code","a6b58748":"code","a203aece":"code","1e02d526":"code","525bda49":"code","cf382910":"code","db184054":"code","7970f049":"code","196ceace":"code","b5f07097":"code","c5bfc874":"code","7688ba9e":"code","5f3e9433":"code","918c17bd":"code","cb4a6b5c":"code","c3e22f7a":"code","d350e415":"code","ccbddd01":"code","bb8ece2b":"code","c4133ff8":"code","91cfda8f":"code","c8197795":"code","d582dba9":"code","e0091164":"markdown","20f6be1b":"markdown","45b514f4":"markdown","e3e84803":"markdown","dbd2691e":"markdown","a2009538":"markdown"},"source":{"5d72ed8c":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import RMSprop, adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.utils import plot_model\n\n%matplotlib inline","ad0800bf":"train_df = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")\ntest_df = pd.read_csv(\"..\/input\/fashion-mnist_test.csv\")\nprint(\"training set shape:\", train_df.shape)\nprint(\"testing set shape:\", test_df.shape)","0c4c6b46":"print(\"Missing Data in training data:\", train_df.isnull().values.any())\nprint(\"Missing Data in testing data:\", test_df.isnull().values.any())","a6b58748":"train_df.head()","a203aece":"X_train = train_df.drop([\"label\"], axis = 1)\nY_train = train_df.label\n\nX_test = test_df.drop([\"label\"], axis = 1)\nY_test = test_df.label\n\ndel train_df, test_df","1e02d526":"print(\"No. of training observations:\", len(X_train))\nprint(\"No. of testing observations:\", len(X_test))\nprint(\"No. of distict classes available in training set:\", len(set(Y_train)))\nprint(\"No. of distict classes available in testing set:\", len(set(Y_test)))","525bda49":"sns.countplot(Y_train)","cf382910":"sns.countplot(Y_test)","db184054":"Y_train = to_categorical(Y_train, 10)\n#Y_test = to_categorical(Y_test, 10)","7970f049":"X_train = X_train \/ 255.0\nX_test = X_test \/ 255.0","196ceace":"X_train.head()","b5f07097":"X_test.head()","c5bfc874":"X_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1)","7688ba9e":"X_train.shape","5f3e9433":"random_seed = 100\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = random_seed)","918c17bd":"plt.imshow(X_train[0][:,:,0])","cb4a6b5c":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', \n                 input_shape = (28,28,1), name = \"CONV_1\"))\nmodel.add(BatchNormalization(name = \"BN_1\"))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', name = \"CONV_2\"))\nmodel.add(BatchNormalization(name = \"BN_2\"))\nmodel.add(MaxPool2D(pool_size=(2,2), name = \"MAXPOOL_1\"))\nmodel.add(Dropout(0.20, name = \"DROP_1\"))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), activation ='relu', name = \"CONV_3\"))\nmodel.add(BatchNormalization(name = \"BN_3\"))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), activation ='relu', name = \"CONV_4\"))\nmodel.add(BatchNormalization(name = \"BN_4\"))\nmodel.add(MaxPool2D(pool_size=(2,2), name = \"MAXPOOL_2\"))\nmodel.add(Dropout(0.20, name = \"DROP_2\"))\n\nmodel.add(Flatten(name = \"FLAT_1\"))\nmodel.add(Dense(128, activation = \"relu\", name = \"FC_1\"))\nmodel.add(BatchNormalization(name = \"BN_5\"))\nmodel.add(Dropout(0.20, name = \"DROP_3\"))\nmodel.add(Dense(10, activation = \"softmax\", name = \"FC_2\"))\nmodel.summary()","c3e22f7a":"plot_model(model, to_file='model.png', show_shapes = True, show_layer_names = True)\nImage(\"model.png\")","d350e415":"optimizer = adam(lr = 0.001, epsilon = 1e-08, decay = 0.0)\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","ccbddd01":"callbacks = [\n    EarlyStopping(\n        monitor = 'val_acc', \n        patience = 10,\n        mode = 'max',\n        verbose = 1),\n    ReduceLROnPlateau(\n        monitor = 'val_acc', \n        patience = 3, \n        verbose = 1, \n        factor = 0.5, \n        min_lr = 0.00001)]","bb8ece2b":"epochs = 50\nbatch_size = 64","c4133ff8":"datagen = ImageDataGenerator(\n        rotation_range = 10, \n        zoom_range = 0.1, \n        width_shift_range = 0.1,  \n        height_shift_range = 0.1)  \n\ndatagen.fit(X_train)","91cfda8f":"his = model.fit_generator(datagen.flow(X_train, \n                                 Y_train, \n                                 batch_size = batch_size),\n                    epochs = epochs, \n                    validation_data = (X_val,Y_val),\n                    verbose = 1, \n                    steps_per_epoch = X_train.shape[0] \/\/ batch_size,\n                    callbacks = callbacks)","c8197795":"fig, ax = plt.subplots(2,1)\nax[0].plot(his.history['loss'], color = 'b', label = \"Training loss\")\nax[0].plot(his.history['val_loss'], color = 'r', label = \"validation loss\" ,axes = ax[0])\nlegend = ax[0].legend(loc = 'best', shadow = True)\n\nax[1].plot(his.history['acc'], color = 'b', label = \"Training accuracy\")\nax[1].plot(his.history['val_acc'], color = 'r',label = \"Validation accuracy\")\nlegend = ax[1].legend(loc = 'best', shadow = True)","d582dba9":"Y_train = np.argmax(Y_train, axis = 1)\ntrainResults = model.predict(X_train)\ntrainResults = np.argmax(trainResults, axis = 1)\ntrainResults = pd.Series(trainResults, name = \"Label\")\n\nY_val = np.argmax(Y_val, axis = 1)\nvalidationResults = model.predict(X_val)\nvalidationResults = np.argmax(validationResults, axis = 1)\nvalidationResults = pd.Series(validationResults, name = \"Label\")\n\ntestResults = model.predict(X_test)\ntestResults = np.argmax(testResults, axis = 1)\ntestResults = pd.Series(testResults, name = \"Label\")\n\ntrainAccuracy = (sum(Y_train == trainResults)\/len(Y_train)) * 100\nvalAccuracy = (sum(Y_val == validationResults)\/len(Y_val)) * 100\ntestAccuracy = (sum(Y_test == testResults)\/len(Y_test)) * 100\n\nprint(\"training Accuracy:\", round(trainAccuracy, 2))\nprint(\"validation Accuracy:\", round(valAccuracy, 2))\nprint(\"testing Accuracy:\", round(testAccuracy, 2))","e0091164":"# scale the data by dividing by 255, as pixels range between 0-255","20f6be1b":"# Well, not bad for the 1st go...Little overfitting\n\n","45b514f4":"# Reshape the data for Model building","e3e84803":"# All the classes are well represented in training data without bias to any particular class ","dbd2691e":"# Check for missing values","a2009538":"# Same is the case with testing data."}}