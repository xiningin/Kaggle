{"cell_type":{"9019d329":"code","df11c329":"code","ca6bd523":"code","308a0d90":"code","69f07726":"code","263cf0cc":"code","2f15f809":"code","b93692c9":"code","9829ee65":"code","fe77cf70":"code","a838b5ad":"code","64844702":"code","eac4daa3":"code","4bf7f8d1":"code","21b193f8":"code","47526673":"code","d0584d47":"code","4d0e45db":"code","770a08ec":"code","1023b35a":"code","0476fa9e":"code","befe3016":"code","37aa2922":"code","726a88c0":"code","1516de95":"code","4abe8d03":"code","787a028a":"code","b743d06b":"code","f461e57e":"code","63c9ff29":"code","cfbf19c4":"code","893ebad4":"code","b5aeb95b":"code","b7080084":"code","44f0b8ea":"code","edc242e4":"code","b7b12389":"code","f44c6612":"code","a73008a8":"code","36ab70c7":"code","c9974673":"code","f3e1413e":"code","e7042ca7":"code","3fe9dfb4":"code","97eb1b1e":"code","9d07e857":"code","3200f624":"code","968bcf87":"code","a25baefd":"code","e9687dd0":"code","bbeb81ec":"code","d17d6f1b":"code","7dd9815e":"code","758eb412":"code","fb03c545":"code","5a2899e0":"code","fa793f1a":"code","4086ca87":"code","50cdedef":"code","fa11293f":"code","5a64a044":"code","2d3273d4":"code","338ca533":"code","f886e58e":"code","a703a3aa":"code","3016b042":"code","f09f7079":"code","e6f5ef1d":"code","19ec2acc":"code","1a426052":"code","990e295f":"code","85c8e526":"code","8bdd495f":"code","9c82adb7":"code","66081061":"code","7118ea65":"code","157c7ce6":"code","ecc14891":"markdown","ed56fcb3":"markdown","297f3746":"markdown","6db06035":"markdown","5a72f566":"markdown","835efb8f":"markdown","108ffe01":"markdown","82f6bfc4":"markdown","05b627a0":"markdown","14d8491b":"markdown","b218347a":"markdown","6efe7f46":"markdown","34ea102c":"markdown","e651541d":"markdown","e268cc41":"markdown","5ff76f07":"markdown","a2c8ce0d":"markdown","f6d51914":"markdown","0aba4df4":"markdown","817db14a":"markdown","8518b4c0":"markdown","5d8a2233":"markdown","e792385c":"markdown","f7c4b584":"markdown","b0c23e02":"markdown","ac306238":"markdown","7e49a803":"markdown","0ceae8f2":"markdown","efff9241":"markdown","82dd4e5f":"markdown","d9cca130":"markdown","5cf3b447":"markdown","f39ff22e":"markdown","164a35b2":"markdown","d985a859":"markdown","16ac21ff":"markdown","204a9bcc":"markdown","6839fd00":"markdown","89f2f5c0":"markdown","1bcccfdf":"markdown","0f0d060f":"markdown","cf40a535":"markdown"},"source":{"9019d329":"from IPython.display import Image\nImage(url= \"https:\/\/media.nationalgeographic.org\/assets\/photos\/000\/273\/27302.jpg\")","df11c329":"import pandas as pd\nimport numpy as np","ca6bd523":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","308a0d90":"print(\"Train Shape:\",train.shape)\nprint(\"Test Shape:\",test.shape)","69f07726":"train.info()","263cf0cc":"test.info()","2f15f809":"train.head(10)","b93692c9":"train.describe()","9829ee65":"test.describe()","fe77cf70":"train.isnull().sum()","a838b5ad":"test.isnull().sum()","64844702":"test.isnull().sum()\ntest[\"Survived\"] = \"\"\ntest.head()","eac4daa3":"import matplotlib.pyplot as plt # Plot the graphs\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots","4bf7f8d1":"def bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","21b193f8":"bar_chart('Sex')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Sex'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Sex'].value_counts())","47526673":"bar_chart('Pclass')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Pclass'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Pclass'].value_counts())","d0584d47":"bar_chart('SibSp')\nprint(\"Survived :\\n\",train[train['Survived']==1]['SibSp'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['SibSp'].value_counts())","4d0e45db":"bar_chart('Parch')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Parch'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Parch'].value_counts())","770a08ec":"bar_chart('Embarked')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Embarked'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Embarked'].value_counts())","1023b35a":"corr = train.corr()\ng = sns.heatmap(corr,  vmax=.3, center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f', cmap='coolwarm')\nsns.despine()\ng.figure.set_size_inches(12,8)\n    \nplt.show()","0476fa9e":"corr=train.corr()#[\"Survived\"]\nplt.figure(figsize=(10, 10))\n\nsns.heatmap(corr, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\nplt.title('Correlation between features');","befe3016":"train.head()","37aa2922":"train.head(10)","726a88c0":"train_test_data = [train,test] # combine dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","1516de95":"train['Title'].value_counts()","4abe8d03":"test['Title'].value_counts()","787a028a":"title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset[\"Title\"].map(title_mapping)","b743d06b":"dataset.head()","f461e57e":"test.head()","63c9ff29":"bar_chart('Title')","cfbf19c4":"# delete unnecessary feature from dataset\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","893ebad4":"train.head()","b5aeb95b":"sex_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)","b7080084":"bar_chart('Sex')","44f0b8ea":"test.head()","edc242e4":"train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace= True)\ntest[\"Age\"].fillna(test.groupby('Title')['Age'].transform(\"median\"), inplace= True)","b7b12389":"train.head(30)\n#train.groupby(\"Title\")[\"Age\"].transform(\"median\")","f44c6612":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend() \nplt.show()\n\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend() \nplt.xlim(10,50)\n","a73008a8":"train.info()\ntest.info()","36ab70c7":"train.head()","c9974673":"for dataset in train_test_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0,\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1,\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2,\n    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3,\n    dataset.loc[ dataset['Age'] > 62, 'Age'] = 4\n# for dataset in train_test_data:\n#     dataset.loc[]\n#train[train['Age'].isin([23])]","f3e1413e":"train.head()\nbar_chart('Age')","e7042ca7":"Pclass1 = train[train['Pclass'] == 1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass'] == 2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass'] == 3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1,Pclass2,Pclass3])\ndf.index = ['1st Class','2nd Class','3rd Class']\ndf.plot(kind = 'bar', stacked =  True, figsize=(10,5))\nplt.show()\nprint(\"Pclass1:\\n\",Pclass1)\nprint(\"Pclass2:\\n\",Pclass2)\nprint(\"Pclass3:\\n\",Pclass3)","3fe9dfb4":"for dataset in train_test_data:\n    dataset['Embarked'] =  dataset['Embarked'].fillna('S')","97eb1b1e":"train.head()","9d07e857":"embarked_mapping = {'S':0,'C':1,'Q':2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","3200f624":"# train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"])\n# train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)\n# test[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)\n# train.head(50)\n\n\n# fill missing Fare with median fare for each Pclass\ntrain[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntrain.head(50)\n","968bcf87":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4 )\nfacet.map(sns.kdeplot, 'Fare', shade = True)\nfacet.set(xlim = (0, train['Fare'].max()))\nfacet.add_legend()\nplt.show()","a25baefd":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\nplt.xlim(0, 20)","e9687dd0":"for dataset in train_test_data:\n    dataset.loc[dataset['Fare'] <= 17, 'Fare'] = 0,\n    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1,\n    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2,\n    dataset.loc[dataset['Fare'] >= 100, 'Fare'] = 3","bbeb81ec":"train.head()","d17d6f1b":"train.Cabin.value_counts()","7dd9815e":"for dataset in train_test_data:\n    dataset['Cabin'] =  dataset['Cabin'].str[:1]","758eb412":"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))","fb03c545":"cabin_mapping = {\"A\": 0, \"B\": 0.4, \"C\": 0.8, \"D\": 1.2, \"E\": 1.6, \"F\": 2, \"G\": 2.4, \"T\": 2.8}\nfor dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)","5a2899e0":"# fill missing Fare with median fare for each Pclass\ntrain[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\ntest[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)","fa793f1a":"train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1","4086ca87":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'FamilySize',shade= True)\nfacet.set(xlim=(0, train['FamilySize'].max()))\nfacet.add_legend()\nplt.xlim(0)","50cdedef":"family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\nfor dataset in train_test_data:\n    dataset['FamilySize'] = dataset['FamilySize'].map(family_mapping)","fa11293f":"train.head()","5a64a044":"features_drop = ['Ticket','SibSp','Parch']\ntrain = train.drop(features_drop, axis = 1)\ntest = test.drop(features_drop,axis=1)\ntrain = train.drop(['PassengerId'], axis=1)","2d3273d4":"train_data = train.drop('Survived', axis = 1)\ntarget = train['Survived']\ntrain_data.shape, target.shape","338ca533":"train_data.head(10)","f886e58e":"# Importing Classifier Modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport numpy as np","a703a3aa":"train.info()","3016b042":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","f09f7079":"X = train_data.copy()\nY = target.copy()","e6f5ef1d":"## Ensembling ##\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process,model_selection\nimport xgboost\nfrom xgboost import XGBClassifier\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    #gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost\n    XGBClassifier()    \n    ]\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nMLA_predict = Y\nrow_index = 0\nX1 = X.copy()","19ec2acc":"for alg in MLA:\n    X = train_data.copy()\n    Y = target.copy()\n    MLA_name = alg.__class__.__name__\n    print(\"Evaluating \",MLA_name)\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    cv_results = model_selection.cross_validate(alg, X, Y, cv  = cv_split)\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n    alg.fit(X, Y)\n    MLA_predict[MLA_name] = alg.predict(X)\n    row_index+=1\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)","1a426052":"MLA_compare","990e295f":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","85c8e526":"clf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","8bdd495f":"#learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\nclf = [KNeighborsClassifier(n_neighbors = 13),DecisionTreeClassifier(),\n       RandomForestClassifier(n_estimators=13),GaussianNB(),SVC(),ExtraTreeClassifier(),\n      GradientBoostingClassifier(n_estimators=10, learning_rate=1,max_features=3, max_depth =3, random_state = 10),AdaBoostClassifier(),ExtraTreesClassifier()]\ndef model_fit():\n    scoring = 'accuracy'\n    for i in range(len(clf)):\n        score = cross_val_score(clf[i], train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\n        print(\"Score of Model\",i,\":\",round(np.mean(score)*100,2))\n#     round(np.mean(score)*100,2)\n#     print(\"Score of :\\n\",score)\nmodel_fit()","9c82adb7":"clf1 = SVC()\nclf1.fit(train_data, target)\ntest\ntest_data = test.drop(['Survived','PassengerId'], axis=1)\nprediction = clf1.predict(test_data)\n# test_data\n","66081061":"prediction","7118ea65":"test_data['Survived'] = prediction\nsubmission = pd.DataFrame(pd.concat([test['PassengerId'],test_data['Survived']],axis=1))\nsubmission.set_index('PassengerId',inplace = True)\nsubmission","157c7ce6":"submission.to_csv(\"Submission.csv\")","ecc14891":"### 1.3 Data Dictionary<a id=\"datadict\"><\/a>\n\n* Survived: 0 = No, 1 = Yes\n* pclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n* sibsp: # of siblings \/ spouses aboard the Titanic\n* parch: # of parents \/ children aboard the Titanic\n* ticket: Ticket number\n* cabin: Cabin number\n* embarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n**Total rows and columns**\n\nWe can see that there are 891 rows and 12 columns in our training dataset.","ed56fcb3":"### 3.4 Pclass Mappping <a id=\"pclass\"><\/a>","297f3746":"## 3. Feature extraction & Feature Engineering <a id=\"fe\"><\/a>","6db06035":"*P.S: This Notebook is designed to help people break into ML competitions involving predictive modelling and expose beginners to concepts such as feature extraction, binning, simple to intermediate visualizations and ensembling techniques, all through the most user friendly and insightful dataset*","5a72f566":"**Binning**\n\nBinning\/Converting Numerical Age to Categorical Variable\n\nfeature vector map:\n* child: 0\n* young: 1\n* adult: 2\n* mid-age: 3\n* senior: 4","835efb8f":"* [Initial review and preprocessing](#prepro)\n\n    - [Reading the dataset](#reading)\n    - [Understanding the structure](#structure)\n    - [Data Dictionary](#datadict)\n    - [Identifying Nans or Nulls](#nulls)\n    \n    \n* [Data Visualization and Exploratory Data Analysis](#dv)\n\n    - [Bar charts](#bar)\n    - [Correlation Maps](#corr)\n    \n    \n* [Feature Extraction and Feature Engineering](#fe)\n    \n    - [Title Mapping](#tit)\n    - [Gender Mapping](#gend)\n    - [Age Mapping & Binning](#age)\n    - [Pclass Mapping](#pclass)\n    - [Embarked Mapping](#emb)\n    - [Fare and Pclass Mapping](#fare)\n    - [Cabin Mapping](#cabin)\n    - [Family Size Mapping and Binning](#fam)\n    - [Dropping Unnecessary features](#drop)\n    \n    \n* [Predictive Modelling](#models)\n\n    - [Introducing K-Fold Cross Validation](#kfold)\n    - [Ensembling with 20 ML algorithms](#ensemble)\n    - [K- Nearest Neighbour Classifier](#knn)\n    - [Support Vector Machine Classifier](#svc)\n    \n    \n* [Submission of final predictions](#submit)","108ffe01":"Find here an Ensemble of 20 Benchmark ML models including tree classifiers, GBDTs, SVCs, Naive Bayes and LDA Classifiers","82f6bfc4":"The Chart confirms a **person aboarded with more than 2 parents or children more likely survived.**  \nThe Chart confirms a **person aboarded alone more likely dead**","05b627a0":"# A Beginner's Guide to Predictive Modelling","14d8491b":"## 2. Data Visualizations and Exploratory Data Analysis<a id=\"dv\"><\/a>","b218347a":"### 3.3 Age mapping & Binning <a id=\"age\"><\/a>","6efe7f46":"The Chart confirms **Women more likely survivied than Men**.","34ea102c":"### 3.7 Cabin mapping <a id=\"cabin\"><\/a>","e651541d":"### 4.4 SVC Classifer <a id=\"svc\"><\/a>","e268cc41":"# 4. Predictive Modelling with several ML Algorithms and techniques <a id=\"models\"><\/a>","5ff76f07":"## 1. Initial Review and Preprocessing<a id=\"prepro\"><\/a>","a2c8ce0d":"### 3.2 Gender mapping <a id=\"gend\"><\/a>","f6d51914":"### 4.1 Introducing Cross Validation(k-fold) <a id=\"kfold\"><\/a>","0aba4df4":"### 2.2 Correlation charts and heatmaps <a id=\"corr\"><\/a>","817db14a":"Those who were **20 to 30 years old** were **more dead and more survived.**","8518b4c0":"### That's it!\n### Hope this notebook helped you navigate your way through predictive modelling techniques and EDA methods!\n\n### If this notebook piqued your interest in ML or helped you out in whatever way, kindly upvote :-) !","5d8a2233":"### 1.2 Understanding the structure<a id=\"structure\"><\/a>","e792385c":"### 2.1 Bar Chart for Categorical Features <a id=\"bar\"><\/a>\n\n* Pclass\n* Sex\n* SibSp ( # of siblings and spouse)\n* Parch ( # of parents and children)\n* Embarked\n* Cabin","f7c4b584":"### 3.6 Fare and Pclass mapping with binning<a id=\"fare\"><\/a>","b0c23e02":"### 1.4 Identifying the NaNs or Nulls<a id=\"nulls\"><\/a>","ac306238":"The Chart confirms a **person aboarded with more than 2 siblings or spouse** more likely survived.  \nThe Chart confirms a **person aboarded without siblings or spouse** more likely dead","7e49a803":"## Find here data analysis, exploratory data analysis and several predictive modelling approaches implemented on the ever-famous titanic dataset.","0ceae8f2":"### 3.8 Dropping unnecessary features <a id=\"drop\"><\/a>","efff9241":"# Contents of this notebook","82dd4e5f":"### 3.8 Family Size mapping <a id=\"fam\"><\/a>","d9cca130":"more than 50 % of 1st class are from S embark.  \nmore than 50 % of 2st class are from S embark.   \nmore than 50 % of 3st class are from S embark.  \n\n**fill out missing embark with S embark**","5cf3b447":"## 5. Making Submission files ready<a id=\"submit\"><\/a>","f39ff22e":"### 3.5 Embarked mapping <a id=\"emb\"><\/a>","164a35b2":"## *We next choose the 2 best classifiers in terms of preformance on validation set and use one of them to generate our test predictions*","d985a859":"### 4.3 K-NN Classifier <a id=\"knn\"><\/a>","16ac21ff":"Feature engineering is the process of using domain knowledge of the data\nto create features (**feature vectors**) that make machine learning algorithms work.  \n\nfeature vector is an n-dimensional vector of numerical features that represent some object.\nMany algorithms in machine learning require a numerical representation of objects,\nsince such representations facilitate processing and statistical analysis.","204a9bcc":"### 1.1 Reading the dataset<a id=\"reading\"><\/a>","6839fd00":"The Chart confirms **1st class** more likely survivied than **other classes**.  \nThe Chart confirms **3rd class** more likely dead than **other classes**","89f2f5c0":"The Chart confirms a **person aboarded from C** slightly more likely survived.  \nThe Chart confirms a **person aboarded from Q** more likely dead.  \nThe Chart confirms a **person aboarded from S** more likely dead.  ","1bcccfdf":"### 4.2 Ensembling techniques <a id=\"ensemble\"><\/a>","0f0d060f":"#### Title Map\n\nMr : 0   \nMiss : 1  \nMrs: 2  \nOthers: 3  ","cf40a535":"### 3.1 Title Mapping <a id=\"tit\"><\/a>"}}