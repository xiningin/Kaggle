{"cell_type":{"c84aec55":"code","55104e54":"code","94645ba5":"code","bc35b242":"code","b31791ee":"code","dd1fcb32":"code","0ae5d82d":"code","9683745b":"code","6115c4c6":"code","e9171d5c":"code","1f5dea25":"code","e55e22ae":"code","395da12b":"code","72b53229":"code","263e144a":"code","7baa66c3":"code","05b76e3c":"code","f96cc2da":"code","3729eb02":"code","e1ad240b":"code","04037e14":"code","747b6053":"code","7ca7f051":"markdown"},"source":{"c84aec55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        pass\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","55104e54":"import seaborn as sns\nimport matplotlib.pyplot as plt\n","94645ba5":"train_df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\", index_col='Id')\ntest_df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\", index_col='Id')","bc35b242":"train_df","b31791ee":"sns.histplot(train_df.Pawpularity, bins=20)","dd1fcb32":"train_df.apply(lambda col:col.unique()) # check out unique values","0ae5d82d":"train_df.isna().sum() ## no null values","9683745b":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xg\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import LinearSVR","6115c4c6":"X = train_df.iloc[:, :-1]\ny = train_df.iloc[:, -1]","e9171d5c":"X","1f5dea25":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","e55e22ae":"rf_reg1 = RandomForestRegressor() ## rf regressor\nrf_reg1.fit(X_train, y_train)\ny_pred_vanilla = rf_reg1.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test, y_pred_vanilla)))","395da12b":"xg_reg1 = xg.XGBRegressor()  #xgb regressor\nxg_reg1.fit(X_train, y_train)\ny_pred_vanilla = xg_reg1.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_pred_vanilla, y_test)))","72b53229":"np.sqrt(mean_squared_error(LinearRegression(normalize=True).fit(X_train, y_train).predict(X_test), y_test))","263e144a":"np.sqrt(mean_squared_error(LinearSVR(C=0.01, epsilon=0.01, max_iter=50, loss=\"squared_epsilon_insensitive\").fit(X_train, y_train).predict(X_test), y_test))","7baa66c3":"## using rf \nparams = {\n    \"n_estimators\":[100,300,500],\n    \"max_depth\":[2,4,6],\n    \"min_samples_split\":[2,4,6],\n    \"oob_score\":[True, False]\n}","05b76e3c":"# grid_rf_reg = GridSearchCV(rf_reg1, params, n_jobs=-1)\n# grid_rf_reg.fit(X_train, y_train)","f96cc2da":"# grid_rf_reg.best_params_","3729eb02":"# rf_reg2 = RandomForestRegressor(**grid_rf_reg.best_params_).fit(X_train, y_train)\n# y_pred_opt = rf_reg2.predict(X_test)\n# print(np.sqrt(mean_squared_error(y_test, y_pred_opt)))","e1ad240b":"# submission = pd.DataFrame({\"Id\": test_df.index, \"Pawpularity\":rf_reg2.predict(test_df)})\nsubmission = pd.DataFrame({\"Id\": test_df.index, \"Pawpularity\":LinearRegression(normalize=True).fit(X_train, y_train).predict(test_df)})","04037e14":"submission.set_index('Id', inplace=True)","747b6053":"submission.to_csv(\"submission.csv\")","7ca7f051":"Linear model is giving better result than RF in this case, eh - lets see.."}}