{"cell_type":{"0fde029e":"code","1e1345c1":"code","ce9f9d35":"code","26438367":"code","5b4b9810":"code","65cea07c":"code","0ca55d9a":"code","61b5d2f7":"code","ac1ec723":"code","9541d399":"code","5ea1df6a":"code","22b80375":"code","f2ebdbb8":"code","5678fb46":"code","99daa57b":"code","875d03e8":"code","9617a4bb":"code","ed533019":"code","e1b51985":"code","f5872426":"code","e6d42b40":"code","c1a7da55":"code","204b5b80":"code","5e8289d5":"code","d1be8d57":"code","09792ab2":"code","ce60bbda":"code","b78eff48":"code","99cdc15e":"code","ec9bd240":"code","ec11f725":"code","168dc879":"code","58880f65":"code","087d74a6":"code","0b544ce9":"code","d3acd0a6":"code","96d93a36":"code","ddb48cf1":"code","1b9dba14":"code","56d0e4a0":"code","e7d06465":"code","5111cbde":"code","8d82fd73":"code","ffc4ab28":"code","1ec43df8":"code","98539ec2":"code","866f0d70":"code","5cfd92c2":"code","1676a5e0":"code","8e41d6e9":"markdown","668fadf1":"markdown","3f14d38e":"markdown","6f5740d9":"markdown","15dcad0c":"markdown","783d5c03":"markdown","fc3313cc":"markdown","7e985f73":"markdown","5318609a":"markdown","b8d0516f":"markdown","63ff214c":"markdown","a2962d9b":"markdown","3ccdd755":"markdown","dbffae3c":"markdown","2dbab88e":"markdown","99eb85b4":"markdown","6e8fabb3":"markdown","bb3768ef":"markdown","c240ca5b":"markdown","c23d0a5c":"markdown","acfcc671":"markdown"},"source":{"0fde029e":"import warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nimport os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nfrom collections import defaultdict\nfrom textblob import TextBlob\nfrom functools import partial\nimport importlib\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\n\nimport nltk\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nnlp.max_length = 4000000\nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom tqdm.autonotebook import tqdm\nimport string\n\n%matplotlib inline\n\nos.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/')","1e1345c1":"!pip install datasets --no-index --find-links=file:\/\/\/kaggle\/input\/coleridge-packages\/packages\/datasets\n!pip install ..\/input\/coleridge-packages\/seqeval-1.2.2-py3-none-any.whl\n!pip install ..\/input\/coleridge-packages\/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ..\/input\/coleridge-packages\/transformers-4.5.0.dev0-py3-none-any.whl","ce9f9d35":"literal_matching = True\npattern_matching = True\nBert_prediction = True\nsentence_selection = True\n\nif sentence_selection:\n    bag_size = 130\n\nMAX_SAMPLE = None # set a small number for experimentation, set None for production.","26438367":"# reading csv files and train & test file paths\ntrain_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","5b4b9810":"train_df.head(6)","65cea07c":"train_df.info()","0ca55d9a":"[print(f\"{col}:{len(train_df[col].unique())}\") for col in train_df.columns]   #finding unique values in each column","61b5d2f7":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","ac1ec723":"%%time\ntqdm.pandas()   #tqdm is used to show any code running with a progress bar. \ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","9541d399":"train_df.head()","5ea1df6a":"#print(train_df['dataset_label'][16000])\n#print(train_df['cleaned_label'][16000])\n#print(train_df['text'][16000])","22b80375":"#print(f\"{'dataset_label'}:{train_df['cleaned_label'].unique()}\")","f2ebdbb8":"%%time\ntqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))","5678fb46":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special characters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('r[^\\w\\s]', ' ', str(text).lower()).strip()\n    lem = nltk.stem.wordnet.WordNetLemmatizer()\n    text = lem.lemmatize(text)\n#     text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n#     text = re.sub(\"\/'+\/g\", ' ', text)\n    \n    return text","99daa57b":"print(text_cleaning('bats'))","875d03e8":"%%time\ntqdm.pandas()\ntrain_df['cleaned_text'] = train_df['text'].progress_apply(text_cleaning)","9617a4bb":"# %%time \n# tqdm.pandas()\n# sample_sub['text'] = sample_sub['text'].progress_apply(text_cleaning)","ed533019":"words =list( train_df['cleaned_label'].values)\nstopwords=['ourselves', 'hers','the','of','and','in', 'between', 'yourself', 'but', 'again','of', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\nsplit_words=[]\nfor word in words:\n    lo_w=[]\n    list_of_words=str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\nfor wordlist in split_words:\n    allwords += wordlist","e1b51985":"mostcommon = FreqDist(allwords).most_common(100)\nwordcloud = WordCloud(width=1600, height=800, background_color='white', stopwords=STOPWORDS).generate(str(mostcommon))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Top 100 Most Common Words in cleaned_label', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()\n\nmostcommon_small = FreqDist(allwords).most_common(25)\nx, y = zip(*mostcommon_small)\nplt.figure(figsize=(50,30))\nplt.margins(0.02)\nplt.bar(x, y)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.tight_layout(pad=0)\nplt.title('Freq of 25 Most Common Words in cleaned_label', fontsize=60)\nplt.show()","f5872426":"# def prepare_text(text, nlp=nlp):\n#     '''\n#     Returns the text after stop-word removal and lemmatization.\n#     text - Sentence to be processed\n#     nlp - Spacy NLP model\n#     '''\n#     doc = nlp(text)\n#     lemma_list = [token.lemma_ for token in doc if not token.is_stop]\n#     lemmatized_sentence = ' '.join(lemma_list)\n    \n#     return lemmatized_sentence","e6d42b40":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","c1a7da55":"# %%time\n# tqdm.pandas()\n# train_df['text'] = train_df['text'].progress_apply(prepare_text)","204b5b80":"import re\nalphabets= \"([A-Za-z])\"\nprefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\nsuffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\nstarters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\nacronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\nwebsites = \"[.](com|net|org|io|gov)\"\n\ndef split_into_sentences(text):\n    text = \" \" + text + \"  \"\n    text = text.replace(\"\\n\",\" \")\n    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n    text = re.sub(websites,\"<prd>\\\\1\",text)\n    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n    if \"\u201d\" in text: text = text.replace(\".\u201d\",\"\u201d.\")\n    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n    text = text.replace(\".\",\".<stop>\")\n    text = text.replace(\"?\",\"?<stop>\")\n    text = text.replace(\"!\",\"!<stop>\")\n    text = text.replace(\"<prd>\",\".\")\n    sentences = text.split(\"<stop>\")\n    sentences = sentences[:-1]\n    sentences = [s.strip() for s in sentences]\n    return sentences","5e8289d5":"#print(train_df.head(6))\n#sorted_df = train_df.sort_values(by=['pub_title'])\n#print(sorted_df.head(6))\n#print(len(train_df['pub_title'].unique()))","d1be8d57":"#This cell will be for finding the sentence structures most often used. \n\"\"\"\nsentence_list = []\nsample_text = ''\nprev_title = ''\nthis_labels = []\nforLoops = 0\nfor index, row in tqdm(sorted_df.iterrows()):\n    if row['pub_title'] == prev_title and prev_title != '':\n        if row['dataset_label'] not in this_labels:\n            this_labels.append(row['dataset_label'])\n    else:\n        sentences = split_into_sentences(sample_text)\n        for item in sentences:\n            for label in this_labels:\n                if label in item:\n                    sentence_list.append(item.replace(label,'XXXX'))\n        forLoops += 1\n        sample_text = row['text']\n        this_labels = [row['dataset_label']]\n        prev_title = row['pub_title']\nprint(len(sentence_list))\nprint(forLoops)\n\"\"\"","09792ab2":"#commons = FreqDist(sentence_list).most_common(25)\n#print(commons)","ce60bbda":"def create_patterns():\n    patterns = []\n    #The following pattern gets stuck at the last part, no clue yet why\n    #patterns.append(re.compile(r'(?:from|From) (?:the |a )*(([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*)(?: \\(.*\\)){0,1} (?:database|dataset|data base|data set|records)'))\n    patterns.append(re.compile(r'(?:used|uses|use|using|Using) (?:data|cohorts|samples) (?:from|in) (?:the )*(([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]* (Study|Database|Data))'))\n    #The difference between the following two patterns is that one assumes any amount of words, if within parentheses\n    #The other assumes a maximum of 5 words with or without parentheses\n    patterns.append(re.compile(r'(?:using|Using) (?:data|cohorts|samples) (?:from|in) (?:the )*(([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*)(?: [a-z\\(]+[A-Za-z0-9\\(\\)]*){0,5}, we'))\n    patterns.append(re.compile(r'(?:using|Using) (?:data|cohorts|samples) (?:from|in) (?:the )*(([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*)\\s*(?:\\(.*\\)){0,1}, we'))\n    patterns.append(re.compile(r'(?:using|Using) (?:data|cohorts|samples) (?:from|in) (?:the )*([A-Z]{3,})+\\s*(?:\\(.*\\)){0,1}, we'))\n    \n    #Based on the freqdist of sentences:\n    patterns.append(re.compile(r'contributed to the design and implementation of (([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*) and\/or provided data but'))\n    patterns.append(re.compile(r'(?:Data|data) used in (?:the preparation of this )*article were obtained from the (([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*) database'))\n    patterns.append(re.compile(r'(?:The|the) primary goal of (([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*) has been to test whether'))\n    patterns.append(re.compile(r'A complete listing of (([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*) investigators can be found at'))\n    #Pattern below again causes delays\n    #patterns.append(re.compile(r'(([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*) is the result of efforts of many co-investigators from '))\n    patterns.append(re.compile(r'(?:Subjects|subjects) originally recruited for (([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*)-(?:[0-9]|GO)'))\n    patterns.append(re.compile(r'duration of each group is specified in the protocols for (([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*)-(?:[0-9]|GO)'))\n    \n    #Patterns below are experimental\n    #patterns.append(re.compile(r'(([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]* (Study|Database|Data))'))\n    patterns.append(re.compile(r'(?:\\w)* database used (?:\\w)* (([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*)'))\n    #patterns.append(re.compile(r'(([A-Z]+[a-zA-Z0-9\\-]*(\\'s)*( of| and| Of| And)*\\s*)+ [A-Z]+[a-zA-Z0-9\\-]*) (?:study|database|data)'))\n\n    return patterns\n    \npatterns = create_patterns()\n\n\ndef my_pat_match(labels_list, patterns, text):\n    found_list = []\n    \n    for ind,pat in enumerate(patterns):\n        matches = pat.findall(text)\n        found_list.append(matches)\n    \n    found_list = [item for sublist in found_list for item in sublist]\n    #Need the if-else below, some patterns do not return tuples as matches.\n    found_list = [x[0]  if len(x[0]) is not 1 else x for x in found_list]\n    for item in found_list:\n        labels_list.append(item)\n    return list(set(labels_list))\n\nprint(set(my_pat_match([\"BehaViou-ra262's and Apes\"], patterns, \"blalb,laalba\")))","b78eff48":"#Here, I aim to just test the amount of matches found in the training data. \n#Only if I feel this is useful\n\"\"\"\nlables_list_t = []\nfor index, row in tqdm(train_df.iterrows()):\n    #print('new')\n        \n    sample_text = row['text']\n    #Takes only the rows where cleaned text matches the sample submission. \n    #Aka only when a train file is identical to a test file.  \n    #The for loop below (because we take a set later), is only for new files. \n    \n    cleaned_labels = []\n    sentences = split_into_sentences(sample_text)\n    for item in sentences:\n        cleaned_labels = my_pat_match(cleaned_labels, patterns, item)\n    #cleaned_labels = my_pat_match(cleaned_labels, patterns, sample_text)\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list_t.append('|'.join(cleaned_labels))\n    if index % 100 == 0:\n        print(index, len(cleaned_labels))\n\"\"\"","99cdc15e":"MAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nPREDICT_BATCH = 64000 \n\nPRETRAINED_PATH = '..\/input\/coleridge-bert-models\/output'\nTEST_INPUT_SAVE_PATH = '.\/input_data'\nTEST_NER_DATA_FILE = 'test_ner_input.json'\nTRAIN_PATH = '..\/input\/scibertmodel\/train_ner_new.json'\nVAL_PATH = '..\/input\/scibertmodel\/train_ner_new.json'\n\nPREDICTION_SAVE_PATH = '.\/pred'\nPREDICTION_FILE = 'test_predictions.txt'","ec9bd240":"train = train_df.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')\nprint(train.head())","ec11f725":"def clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences","168dc879":"paper_train_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\npapers = {}\nfor paper_id in train_df['Id'].unique():\n    with open(f'{paper_train_folder}\/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper\n\ntemp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)","58880f65":"#Try to create data format for bag of words\n\n\ntagged_sentences = [] # test data in NER format\npaper_length = [] # store the number of sentences each paper has\nlabels = []\nindices = np.random.permutation(len(train['Id']))[:int(len(train['Id'])\/2)]\nfor paper_id in train['Id'][indices]: #make random\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = [clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ]\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n        \n    # collect all sentences in json\n    for sentence in sentences:\n        dummy_tag = False\n        for label in existing_labels:\n            if label in sentence.lower():\n                dummy_tag = True\n        tagged_sentences.append(sentence)\n        labels.append(dummy_tag)\n    \n    # track which sentence belongs to which data point\n    paper_length.append(len(sentences))\n    \nprint(f'total number of sentences: {len(tagged_sentences)}')\nprint(f'found number of sentences that mentioned dataset: {len([value for value in labels if value==True])}')","087d74a6":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords \nmatrix = CountVectorizer(max_features = bag_size, stop_words=stopwords.words('english'))\nX = matrix.fit_transform(tagged_sentences).toarray()\ndel tagged_sentences #safe memory space\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, labels)\n\n# Naive Bayes \nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB([0.3, 0.7])\nclassifier.fit(X_train, y_train)\n\n# Predict Class\ny_pred = classifier.predict(X_test)\nprint(X_test.shape)\nprint(len([value for value in y_pred if value==True]))\nprint(len([value for value in y_test if value==True]))\n\n# Accuracy \nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_pred)\nprint(accuracy)\n\n#Confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(classifier, X_test, y_test)\n\n#Visualizing what is learned:\nprint(matrix.get_feature_names())","0b544ce9":"test_rows = [] # test data in NER format\npaper_length = [] # store the number of sentences each paper has\n\nfor paper_id in sample_sub['Id']:\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = [clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ]\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    if sentence_selection:\n        sentences = [sentence for sentence in sentences if classifier.predict(matrix.transform([sentence]).toarray())]\n    else: \n        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n        \n    # collect all sentences in json\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        dummy_tags = ['O']*len(sentence_words)\n        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n    \n    # track which sentence belongs to which data point\n    paper_length.append(len(sentences))\n    \nprint(f'total number of sentences: {len(test_rows)}')","d3acd0a6":"os.environ[\"MODEL_PATH\"] = f\"{PRETRAINED_PATH}\"\nos.environ[\"TRAIN_FILE\"] = f\"{TRAIN_PATH}\"\nos.environ[\"VALIDATION_FILE\"] = f\"{VAL_PATH}\"\nos.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}\/{TEST_NER_DATA_FILE}\"\nos.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\"","96d93a36":"# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp \/kaggle\/input\/coleridge-packages\/my_seqeval.py .\/\n\n# make necessart directories and files\nos.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)","ddb48cf1":"def bert_predict():\n    !python ..\/input\/kaggle-ner-utils\/kaggle_run_ner.py \\\n    --model_name_or_path \"$MODEL_PATH\" \\\n    --train_file \"$TRAIN_FILE\" \\\n    --validation_file \"$VALIDATION_FILE\" \\\n    --test_file \"$TEST_FILE\" \\\n    --output_dir \"$OUTPUT_DIR\" \\\n    --report_to 'none' \\\n    --seed 123 \\\n    --do_predict","1b9dba14":"bert_outputs = []\n\nfor batch_begin in range(0, len(test_rows), PREDICT_BATCH):\n    # write data rows to input file\n    with open(f'{TEST_INPUT_SAVE_PATH}\/{TEST_NER_DATA_FILE}', 'w') as f:\n        for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\n            json.dump(row, f)\n            f.write('\\n')\n    \n    # remove output dir\n    !rm -r \"$OUTPUT_DIR\"\n    \n    # do predict\n    bert_predict()\n    \n    # read predictions\n    with open(f'{PREDICTION_SAVE_PATH}\/{PREDICTION_FILE}') as f:\n        this_preds = f.read().split('\\n')[:-1]\n        bert_outputs += [pred.split() for pred in this_preds]","56d0e4a0":"# get test sentences\ntest_sentences = [row['tokens'] for row in test_rows]\n\ndel test_rows","e7d06465":"bert_dataset_labels = [] # store all dataset labels for each publication\n\nfor length in paper_length:\n    labels = set()\n    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n        curr_phrase = ''\n        for word, tag in zip(sentence, pred):\n            if tag == 'B': # start a new phrase\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n                curr_phrase = word\n            elif tag == 'I' and curr_phrase: # continue the phrase\n                curr_phrase += ' ' + word\n            else: # end last phrase (if any)\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n        # check if the label is the suffix of the sentence\n        if curr_phrase:\n            labels.add(curr_phrase)\n            curr_phrase = ''\n    \n    # record dataset labels for this publication\n    bert_dataset_labels.append(labels)\n    \n    del test_sentences[:length], bert_outputs[:length]","5111cbde":"bert_dataset_labels[:5]","8d82fd73":"def jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) \/ union\n\nfiltered_bert_labels = []\n\nfor labels in bert_dataset_labels:\n    filtered = []\n    \n    for label in sorted(labels, key=len):\n        label = clean_text(label)\n        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n            filtered.append(label)\n    \n    filtered_bert_labels.extend(filtered)","ffc4ab28":"filtered_bert_labels[:5]","1ec43df8":"temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)\nid_list = []\nlables_list = []\nfor index, row in tqdm(sample_sub.iterrows()):\n    #print('new')\n    sample_text = row['text']\n    #print(sample_text)\n    row_id = row['Id']\n    #Takes only the rows where cleaned text matches the sample submission. \n    #Aka only when a train file is identical to a test file.  \n    temp_df = train_df[train_df['cleaned_text'] == text_cleaning(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    #The for loop below (because we take a set later), is only for new files. \n    if literal_matching:\n        for known_label in existing_labels:\n            if known_label in sample_text.lower():\n                cleaned_labels.append(clean_text(known_label))\n            \n    #TODO: Add to cleaned_labels, only the ones that match patterns. \n    #Only when the length is still 0 at this point. \n    if pattern_matching:\n        sentences = split_into_sentences(sample_text)\n        for item in sentences:\n            cleaned_labels = my_pat_match(cleaned_labels, patterns, item)\n    \n    if Bert_prediction:\n        cleaned_labels.extend(filtered_bert_labels)\n    #cleaned_labels = my_pat_match(cleaned_labels, patterns, sample_text)\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","98539ec2":"print(lables_list[0])","866f0d70":"submission = pd.DataFrame()\nsubmission['Id'] = id_list\nsubmission['PredictionString'] = lables_list","5cfd92c2":"# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\nsubmission.head()","1676a5e0":"submission.to_csv('submission.csv', index=False)","8e41d6e9":"## Adaptation notice\n\nThis notebook has been copied from a notebook provided by a separate user. We as a group participate in this competition as part of an assignment for a course at our university. Credit for this initial notebook goes to the original author. We aim to use this notebook as a baseline, and to attempt to improve the score from that point","668fadf1":"## Hurray! We are done with the submission and model. Hope you like this kernel. If so, don't forget to upvote and leave your valuable comment. Thank you\ud83d\ude0a","3f14d38e":"# <p style=\"font-family:newtimeroman; text-align:center; fontsize:150%\">Coleridge Initiative - Show US the Data<br>Discover how data is used for the public good<\/p>\n![CI_logo.jpg](attachment:CI_logo.jpg)","6f5740d9":"We are provided with 4 main pieces of data:\n\n* `train.csv:` The CSV file containing all the metadata of the publications, such as their title and the dataset they utilize.\n* `train:` The directory containing the actual publications that are referenced in train.csvin JSON format.\n* `test:` The directory containing the actual publications that will be used for testing purposes (thus, with no ground truth CSV file available).\n* `sample_submission.csv:` The CSV file containing all the publications IDs in the test set, for which we'll have to populate the prediction column.","15dcad0c":"<a id='1'><\/a>\n## <p style=\"text-align:center;\">Data Description<\/p>\ntrain.csv - labels and metadata for the training set train\/test directory - the full text of the training\/test set's publications in JSON format, broken into sections with section titles\n\n* `id` - publication id - note that there are multiple rows for some training documents, indicating multiple mentioned datasets.\n* `pub_title` - title of the publication (a small number of publications have the same title).\n* `dataset_title` - the title of the dataset that is mentioned within the publication.\n* `dataset_label` - a portion of the text that indicates the dataset.\n* `cleaned_label` - the dataset_label, as passed through the clean_text function from the Evaluation page.\n\nsample_submission.csv - a sample submission file in the correct format.\n* `Id` - publication id.\n* `PredictionString` - To be filled with equivalent of cleaned_label of train data.","783d5c03":"### Special thanks to helper notebooks \ud83d\ude4f\ud83c\udffb:- \n1. [Tabular Data Preparation, Basic EDA and Baseline](https:\/\/www.kaggle.com\/manabendrarout\/tabular-data-preparation-basic-eda-and-baseline)\n2. [Coleridge - Data Loading, EDA & Simple Submission](https:\/\/www.kaggle.com\/poornap\/coleridge-data-loading-eda-simple-submission)\n3. [Coleridge Initiative - EDA + Na\u00efve Submission \ud83d\udcda](https:\/\/www.kaggle.com\/josephassaker\/coleridge-initiative-eda-na-ve-submission\/data)","fc3313cc":"It takes time!\ud83d\ude44","7e985f73":"<a id='4'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. Data Vizualization\ud83c\udfa8<\/p>","5318609a":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">1. Importing necessary modules and libraries\ud83d\udcda<\/p>","b8d0516f":"<a id='1'><\/a>\n# <p style=\"background-color:red; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 10px 25px;\">Table of Content<\/p>\n* [1. Importing necessary modules and libraries\ud83d\udcda](#1)\n* [2. Data Exploration\ud83d\udd0d](#2)\n* [3. Data Cleaning\ud83d\udd27](#3)\n* [4. Data Vizualization\ud83c\udfa8](#4)\n* [5. Baseline model and Submission\ud83d\udcdd](#5)","63ff214c":"Great! we don't have any null values.","a2962d9b":"<a id='5'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">5. Baseline model and Submission\ud83d\udcdd<\/p>","3ccdd755":"## Adding BERT","dbffae3c":"We have our text appended in our train dataframe.","2dbab88e":"## Hey There! \ud83d\ude4c\ud83c\udffb\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f\nIn this notebook basically we have to predict text for some strings by using nlp techniques.\n*** \n>The objective of the competition is to identify the mention of datasets within scientific publications.\n\nThis competition challenges data scientists to show how publicly funded data are used to serve science and society. Evidence through data is critical if government is to address the many threats facing society, including; pandemics, climate change, Alzheimer\u2019s disease, child hunger, increasing food production, maintaining biodiversity, and addressing many other challenges. Yet much of the information about data necessary to inform evidence and science is locked inside publications.\n\nThe Coleridge Initiative is a not-for-profit organization originally established at New York University. It was set up in order to inform the decision-making of the Commission on Evidence-based Policymaking and has since worked with dozens of government agencies at the federal, state, and local levels to ensure that data are more effectively used for public decision-making.\n\nIt achieves this goal by working with the agencies to create value for the taxpayer from the careful use of data by building new technologies to enable secure access to and sharing of confidential microdata and by training agency staff to acquire modern data skills.","99eb85b4":"We have our data cleaned!","6e8fabb3":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Cleaning\ud83d\udd27<\/p>","bb3768ef":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. Data Exploration\ud83d\udd0d<\/p>","c240ca5b":"### 100 Most common words (cleaned_label) - WordCloud","c23d0a5c":"Also, we have the text of for the sample_submission file","acfcc671":"<a id='0'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Coleridge Initiative\ud83d\udd8b\ud83d\udcdd - EDA\ud83d\udcda & Baseline Model\ud83c\udfaf <\/p>"}}