{"cell_type":{"4938041e":"code","1f7ce11c":"code","70a256f5":"code","862f2a99":"code","450e1510":"code","420f7d84":"code","2a679169":"code","b9c872ac":"code","d9042434":"code","1f8cf7f2":"code","4534560f":"code","31451df8":"code","7e695b96":"code","ddb6c495":"code","8e26e406":"code","7b344cd7":"code","1a0be638":"code","95855b72":"code","9230be8b":"code","8c046a7a":"code","7b25fc2e":"markdown","81ef2b21":"markdown","600e5fd8":"markdown","002a82ff":"markdown","c10975f7":"markdown","07af131e":"markdown","80d9eaef":"markdown","c7d721e0":"markdown"},"source":{"4938041e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfrom sklearn.metrics import r2_score\nimport glob\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport warnings\nwarnings.filterwarnings('ignore')","1f7ce11c":"train = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/test.csv\")\ntrain.head()","70a256f5":"test.head()","862f2a99":"mean = np.mean(train['target'])\nprint(f\"Mean : {mean}\")\n\nplt.figure(figsize=(8, 5))\nsns.distplot(train['target'], bins=50)\nplt.title('Target Distribution')\nplt.show()","450e1510":"print(f\"Target count greater than 0.02 : {train['target'][train['target'] >= 0.02].count()}\")\nprint(f\"Percentage of total: {(train['target'][train['target'] >= 0.02].count() \/ train.shape[0]) * 100} %\")","420f7d84":"print(f\"Number of shares: {train.shape[0]}\")\nfor col in train.columns:\n    print(f\" {col}: {len(train[col].unique())}\")","2a679169":"stock = train.groupby('stock_id')['target'].agg(['mean', 'sum']).reset_index()\nprint(f\"Mean: {stock['mean'].mean()}\")\nprint(f\"Max value: {stock['sum'].mean()}\")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nsns.histplot(x=stock['mean'], ax=ax1)\nsns.histplot(x=stock['sum'], ax=ax2)\nax1.set_title('Target mean distribution', size=12)\nax2.set_title('Target sum distribution', size=12)\nplt.legend()\nplt.show()","b9c872ac":"book_train = pd.read_parquet(\"..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0\")\nbook_test = pd.read_parquet(\"..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/stock_id=0\")\n\ntrade_train = pd.read_parquet(\"..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0\")\ntrade_test = pd.read_parquet(\"..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\/stock_id=0\")\n\nbook_train.head()","d9042434":"df_book = book_train[book_train['time_id'] == 5]\ndf_book.head()","1f8cf7f2":"plt.figure(figsize=(15, 5))\nfor col in ['bid_price1', 'bid_price2', 'ask_price1', 'ask_price2']:\n    sns.lineplot(x='seconds_in_bucket', y=col, data=df_book, label=col)\nplt.legend()\nplt.show()","4534560f":"df_trade= trade_train[trade_train['time_id'] == 5]\ndf_trade.head()","31451df8":"plt.figure(figsize=(15, 5))\nfor col in ['bid_price1', 'bid_price2', 'ask_price1', 'ask_price2']:\n    sns.lineplot(x='seconds_in_bucket', y=col, data=df_book, label=col)\n    \nsns.lineplot(x='seconds_in_bucket', y='price', data=df_trade, linewidth=3, color='black', label='price', )\nplt.legend()\nplt.show()","7e695b96":"df_book['wap'] = (df_book['bid_price1'] * df_book['ask_size1']+df_book['ask_price1'] * df_book['bid_size1'])  \/ (df_book['bid_size1'] + df_book['ask_size1'])\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndf_book.loc[:,'log_return'] = log_return(df_book['wap'])\ndf_book = df_book[~df_book['log_return'].isnull()]","ddb6c495":"def realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\nrealized_vol = realized_volatility(df_book['log_return'])\nprint(f'Realized volatility for stock_id 0 on time_id 5 is {realized_vol}')","8e26e406":"list_order_book_file_train = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*')","7b344cd7":"train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n\nmodel_dict = {}\n\ndef realized_volatility_per_time_id_linear(file_path, prediction_column_name, train_test = True):\n    df_book_data = pd.read_parquet(file_path)\n    df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  \/ (\n                                      df_book_data['bid_size1']+ df_book_data[\n                                  'ask_size1'])\n    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n    stock_id = file_path.split('=')[1]\n    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    \n    poly = PolynomialFeatures(degree=3)\n    \n    if train_test:\n        \n        df_realized_vol_per_stock_joined = train.merge(df_realized_vol_per_stock[['row_id',prediction_column_name]], on = ['row_id'], how = 'right')\n\n        weights = 1\/np.square(df_realized_vol_per_stock_joined.target)\n\n        X = np.array(df_realized_vol_per_stock_joined[[prediction_column_name]]).reshape(-1, 1)\n        X_ = poly.fit_transform(X)\n        y = df_realized_vol_per_stock_joined.target\n\n\n        reg = LinearRegression().fit(X_, y, sample_weight = weights)\n        df_realized_vol_per_stock[[prediction_column_name]] = reg.predict(X_)\n\n        model_dict[stock_id] = reg\n\n    else: \n        \n        reg = model_dict[stock_id]\n        \n        X = np.array(df_realized_vol_per_stock[[prediction_column_name]]).reshape(-1, 1)\n        X_ = poly.fit_transform(X)\n        df_realized_vol_per_stock[[prediction_column_name]] = reg.predict(X_)\n    \n    return df_realized_vol_per_stock[['row_id',prediction_column_name]]","1a0be638":"def past_realized_volatility_per_stock_linear(list_file,prediction_column_name, train_test = True):\n    df_past_realized = pd.DataFrame()\n    for file in list_file:\n        df_past_realized = pd.concat([df_past_realized,\n                                     realized_volatility_per_time_id_linear(file,prediction_column_name,train_test)])\n    return df_past_realized\n\ndf_past_realized_train = past_realized_volatility_per_stock_linear(list_file=list_order_book_file_train,prediction_column_name='pred')","95855b72":"train = train[['row_id','target']]\ndf_joined = train.merge(df_past_realized_train[['row_id','pred']], on = ['row_id'], how = 'left')","9230be8b":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\nR2 = round(r2_score(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\nRMSPE = round(rmspe(y_true = df_joined['target'], y_pred = df_joined['pred']),3)\nprint(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')","8c046a7a":"list_order_book_file_test = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*')\ndf_naive_pred_test = df_past_realized_train = past_realized_volatility_per_stock_linear(list_file=list_order_book_file_test,\n                                                           prediction_column_name='target', train_test = False)\ndf_naive_pred_test.to_csv('submission.csv',index = False)","7b25fc2e":"<b> So the mean value 0.003 which is close to 0 and Max value is on 14.8.<\/b>","81ef2b21":"### Let's see How many values are greater than 0.02","600e5fd8":"### book_[train\/test].parquet: \nA parquet file partitioned by stock_id. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level.\n\n* <b>stock_id - <\/b>ID code for the stock. Not all stock IDs exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; you may wish to convert it to int8.\n* <b>time_id - <\/b>ID code for the time bucket. Time IDs are not necessarily sequential but are consistent across all stocks.\n* <b>seconds_in_bucket - <\/b>Number of seconds from the start of the bucket, always starting from 0.\n* <b>bid_price[1\/2] - <\/b>Normalized prices of the most\/second most competitive buy level.\n* <b>ask_price[1\/2] - <\/b>Normalized prices of the most\/second most competitive sell level.\n* <b>bid_size[1\/2] - <\/b>The number of shares on the most\/second most competitive buy level.\n* <b>ask_size[1\/2] - <\/b>The number of shares on the most\/second most competitive sell level.\n\n### trade_[train\/test].parquet:\nA parquet file partitioned by stock_id. Contains data on trades that actually executed. Usually, in the market, there are more passive buy\/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book.\n\n* <b>stock_id - <\/b>Same as above.\n* <b>time_id - <\/b>Same as above.\n* <b>seconds_in_bucket - <\/b>Same as above. Note that since trade and book data are taken from the * same time window and trade data is more sparse in general, this field is not necessarily starting from 0.\n* <b>price - <\/b>The average price of executed transactions happening in one second. Prices have been normalized and the average has been weighted by the number of shares traded in each transaction.\n* <b>size - <\/b>The sum number of shares traded.\n* <b>order_count - <\/b>The number of unique trade orders taking place.\n\n### train.csv:\nThe ground truth values for the training set.\n* <b>stock_id - <\/b>Same as above, but since this is a csv the column will load as an integer instead of categorical.\n* <b>time_id - <\/b>Same as above.\n* <b>target - <\/b>The realized volatility computed over the 10 minute window following the feature data under the same stock\/time_id. There is no overlap between feature and target data. You can find more info in our tutorial notebook.\n\n### test.csv\nProvides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download.\n\n* <b>stock_id - <\/b>Same as above.\n* <b>time_id - <\/b>Same as above.\n* <b>row_id - <\/b>Unique identifier for the submission row. There is one row for each existing time ID\/stock ID pair. Each time window is not necessarily containing every individual stock.\n\n### sample_submission.csv\nA sample submission file in the correct format.\n\n* <b>row_id - <\/b>Same as in test.csv.\n* <b>target - <\/b>Same definition as in train.csv. The benchmark is using the median target value from train.csv.\n","002a82ff":"## Submission","c10975f7":"# Optiver Realized Volatility Prediction : EDA + Linear Regression","07af131e":"<b> So there are different 112 stock ids, 3830 time ids and 414287 target. <\/b>","80d9eaef":"## Dataset","c7d721e0":"## Target Distribution:\nTarget is skewed on left side"}}