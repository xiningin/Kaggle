{"cell_type":{"5cf51820":"code","61f10f6b":"code","55b94f63":"code","3ae5c1d3":"code","6d67ad43":"code","2e826e6b":"code","8f3fe383":"code","dd13e547":"code","dfe4a388":"code","302a168b":"code","1ca579e1":"code","65ca91d9":"code","d3ed51e1":"code","00e5cf60":"code","98cea106":"code","30b159f2":"code","c942c7d6":"code","b71408cd":"code","4a82ea68":"markdown","88c271a5":"markdown","d34aa22a":"markdown","305ba858":"markdown","69f62b3c":"markdown","7509d459":"markdown","e9effd7c":"markdown","7c8ff246":"markdown","f3361a00":"markdown","276eabed":"markdown","b309d0b4":"markdown","4ff6621c":"markdown","c4f17993":"markdown","1b73a9e8":"markdown","3c1d14f0":"markdown","80db530c":"markdown","44de5f0f":"markdown","0255bd85":"markdown","105c4e04":"markdown","ddf74a20":"markdown","99ce7383":"markdown","914f917f":"markdown","6b33e7fe":"markdown","4d6453a4":"markdown","34071960":"markdown","e11db22d":"markdown"},"source":{"5cf51820":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","61f10f6b":"columns = [\n    'neighbourhood_group', 'room_type', 'latitude', 'longitude',\n    'minimum_nights', 'number_of_reviews','reviews_per_month',\n    'calculated_host_listings_count', 'availability_365',\n    'price'\n]\n\ndf = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv', usecols=columns)\ndf.reviews_per_month = df.reviews_per_month.fillna(0)","55b94f63":"df['price'] = np.log1p(df.price.values)\n\nfrom sklearn.model_selection import train_test_split\n\ndf_full_train, df_test = train_test_split(df, test_size = 0.2, random_state = 1)\ndf_train, df_val = train_test_split(df_full_train, test_size = 0.25, random_state = 1)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ny_train = df_train.price.values\ndel df_train['price']\n\ny_val = df_val.price.values\ndel df_val['price']\n\ny_test = df_test.price.values\ndel df_test['price']","3ae5c1d3":"from sklearn.feature_extraction import DictVectorizer\ndv = DictVectorizer(sparse = False)\n\ntrain_dict = df_train.to_dict(orient = 'records')\nX_train = dv.fit_transform(train_dict)\n\nval_dict = df_val.to_dict(orient = 'records')\nX_val = dv.fit_transform(val_dict)","6d67ad43":"from sklearn.tree import DecisionTreeRegressor, export_text\n \nmodel = DecisionTreeRegressor(max_depth = 1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_val)\n\n\nprint(export_text(model, feature_names = dv.get_feature_names()))","2e826e6b":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmodel = RandomForestRegressor(n_estimators=10, random_state=1, n_jobs=-1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_val)\nrmse = mean_squared_error(y_val, y_pred)** 0.5","8f3fe383":"print(rmse)","dd13e547":"for i in range(10, 201, 10):\n    model = RandomForestRegressor(n_estimators = i, random_state=1, n_jobs = -1)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    rmse = mean_squared_error(y_val, y_pred) ** 0.5 \n    print(\"For estimators %s, RMSE is %.3f\" % (i, rmse))","dfe4a388":"for depth in [10,15,20,25]:\n    print('For max_depth %s \\n' % depth)\n    for i in range(10, 201, 10):\n        model = RandomForestRegressor(n_estimators = i, random_state=1, n_jobs = -1, max_depth = depth)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_val)\n        rmse = mean_squared_error(y_val, y_pred) ** 0.5 \n        print(\"For estimators %s, RMSE is %.3f\" % (i, rmse))\n    ","302a168b":"# Let's use a model with the same parameters except for seed\nmodel1 = RandomForestRegressor(n_estimators = 10, random_state=1, n_jobs = -1, max_depth= 20)\nmodel1.fit(X_train, y_train)\ny_pred = model.predict(X_val)\nrmse = mean_squared_error(y_val, y_pred) ** 0.5 \nprint(rmse)","1ca579e1":"model2 = RandomForestRegressor(n_estimators = 10, random_state=2, n_jobs = -1, max_depth =20)\nmodel2.fit(X_train, y_train)\ny_pred = model.predict(X_val)\nrmse = mean_squared_error(y_val, y_pred) ** 0.5 \nprint(rmse)","65ca91d9":"model = RandomForestRegressor(n_estimators = 10, max_depth = 20, random_state=1, n_jobs = -1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_val)","d3ed51e1":"d = {'feature': dv.get_feature_names(), 'values': model.feature_importances_}\nfeature_info_values = pd.DataFrame(data = d)\nfeature_info_values.sort_values('values', ascending = False)","00e5cf60":"import xgboost as xgb\nfeatures = dv.get_feature_names()\ndtrain = xgb.DMatrix(X_train, label = y_train, feature_names = features)\ndval = xgb.DMatrix(X_val, label = y_val, feature_names = features)","98cea106":"watchlist = [(dtrain, 'train'), (dval, 'val')]","30b159f2":"xgb_params = {\n    'eta': 0.3, \n    'max_depth': 6,\n    'min_child_weight': 1,\n\n    'objective': 'reg:squarederror',\n    'nthread': 8,\n\n    'seed': 1,\n    'verbosity': 1,\n}\n\nmodel = xgb.train(xgb_params, dtrain, num_boost_round = 100)\ny_pred = model.predict(dval)\nrmse = mean_squared_error(y_pred,y_val) ** 0.5\nprint(rmse)","c942c7d6":"xgb_params = {\n    'eta': 0.1, \n    'max_depth': 6,\n    'min_child_weight': 1,\n\n    'objective': 'reg:squarederror',\n    'nthread': 8,\n\n    'seed': 1,\n    'verbosity': 1,\n}\n\nmodel = xgb.train(xgb_params, dtrain, num_boost_round = 100)\ny_pred = model.predict(dval)\nrmse = mean_squared_error(y_pred,y_val) ** 0.5\nprint(rmse)","b71408cd":"xgb_params = {\n    'eta': 0.01, \n    'max_depth': 6,\n    'min_child_weight': 1,\n\n    'objective': 'reg:squarederror',\n    'nthread': 8,\n\n    'seed': 1,\n    'verbosity': 1,\n}\n\nmodel = xgb.train(xgb_params, dtrain, num_boost_round = 100)\ny_pred = model.predict(dval)\nrmse = mean_squared_error(y_pred,y_val) ** 0.5\nprint(rmse)","4a82ea68":"## Question 2\n\nTrain a random forest model with these parameters:\n\n* `n_estimators=10`\n* `random_state=1`\n* `n_jobs=-1`  (optional - to make training faster)","88c271a5":"After which value of `n_estimators` does RMSE stop improving?\n\n- 10\n- 50\n- 70\n- 120","d34aa22a":"## 6.10 Homework\n\nThe goal of this homework is to create a tree-based regression model for prediction apartment prices (column `'price'`).\n\nIn this homework we'll again use the New York City Airbnb Open Data dataset - the same one we used in homework 2 and 3.\n\nYou can take it from [Kaggle](https:\/\/www.kaggle.com\/dgomonov\/new-york-city-airbnb-open-data?select=AB_NYC_2019.csv)\nor download from [here](https:\/\/raw.githubusercontent.com\/alexeygrigorev\/datasets\/master\/AB_NYC_2019.csv)\nif you don't want to sign up to Kaggle.\n\nLet's load the data:","305ba858":"## Submit the results\n\n\nSubmit your results here: https:\/\/forms.gle\/wQgFkYE6CtdDed4w8\n\nIt's possible that your answers won't match exactly. If it's the case, select the closest one.\n\n\n## Deadline\n\n\nThe deadline for submitting is 20 October 2021, 17:00 CET (Wednesday). After that, the form will be closed.\n\n","69f62b3c":"Answer 1: room_type","7509d459":"Bonus question (not graded):\n\nWill the answer be different if we change the seed for the model?","e9effd7c":"## Question 3\n\nNow let's experiment with the `n_estimators` parameter\n\n* Try different values of this parameter from 10 to 200 with step 10\n* Set `random_state` to `1`\n* Evaluate the model on the validation dataset","7c8ff246":"What's the most important feature? \n\n* `neighbourhood_group=Manhattan`\n* `room_type=Entire home\/apt`\t\n* `longitude`\n* `latitude`","f3361a00":"What's the RMSE of this model on validation?\n\n* 0.059\n* 0.259\n* 0.459\n* 0.659","276eabed":"Which feature is used for splitting the data?\n\n* `room_type`\n* `neighbourhood_group`\n* `number_of_reviews`\n* `reviews_per_month`","b309d0b4":"Bonus Question:\n\nAnswer will remain the same if we change the seed for the model itself","4ff6621c":"Question 6: ETA: 0.1 ","c4f17993":"Question 2 Answer: 0.459 ","1b73a9e8":"Answer to Question 5: room_type = Entire home\/apt","3c1d14f0":"## Question 5\n\nWe can extract feature importance information from tree-based models. \n\nAt each step of the decision tree learning algorith, it finds the best split. \nWhen doint it, we can calculate \"gain\" - the reduction in impurity before and after the split. \nThis gain is quite useful in understanding what are the imporatant features \nfor tree-based models.\n\nIn Scikit-Learn, tree-based models contain this information in the `feature_importances_` field. \n\nFor this homework question, we'll find the most important feature:\n\n* Train the model with these parametes:\n    * `n_estimators=10`,\n    * `max_depth=20`,\n    * `random_state=1`,\n    * `n_jobs=-1` (optional)\n* Get the feature importance information from this model","80db530c":"Now change `eta` first to `0.1` and then to `0.01`","44de5f0f":"Now, use `DictVectorizer` to turn train and validation into matrices:","0255bd85":"## Question 1\n\nLet's train a decision tree regressor to predict the price variable. \n\n* Train a model with `max_depth=1`","105c4e04":"## Question 6","ddf74a20":"What's the best eta?\n\n* 0.3\n* 0.1\n* 0.01","99ce7383":"What's the best `max_depth`:\n\n* 10\n* 15\n* 20\n* 25","914f917f":"Question 3 Answer: 120 ","6b33e7fe":"* Apply the log tranform to `price`\n* Do train\/validation\/test split with 60%\/20%\/20% distribution. \n* Use the `train_test_split` function and set the `random_state` parameter to 1","4d6453a4":"Question 4 Answer: max_depth = 15","34071960":"Now let's train an XGBoost model! For this question, we'll tune the `eta` parameter\n\n* Install XGBoost\n* Create DMatrix for train and validation\n* Create a watchlist\n* Train a model with these parameters for 100 rounds:\n\n```\nxgb_params = {\n    'eta': 0.3, \n    'max_depth': 6,\n    'min_child_weight': 1,\n    \n    'objective': 'reg:squarederror',\n    'nthread': 8,\n    \n    'seed': 1,\n    'verbosity': 1,\n}\n```","e11db22d":"## Question 4\n\nLet's select the best `max_depth`:\n\n* Try different values of `max_depth`: `[10, 15, 20, 25]`\n* For each of these values, try different values of `n_estimators` from 10 till 200 (with step 10)\n* Fix the random seed: `random_state=1`"}}