{"cell_type":{"a5a3b1e7":"code","d595fdd2":"code","0e1a72a2":"code","f4087736":"code","5507a4e8":"code","08c35e33":"code","b82ac1f1":"code","0037a14c":"code","5c76b701":"code","5e693259":"code","28b5daa8":"code","cdb877c0":"code","32ca3e19":"code","d9beb5fa":"code","ca2c73ce":"code","698252f0":"code","fdcba540":"code","226594dc":"code","1af49046":"code","c954d126":"code","e4c5b143":"code","924f2307":"code","9b23e4e6":"code","6114934d":"code","c3b9ffc9":"code","f106db9c":"code","0ada5072":"code","e8e36135":"code","81e69e01":"code","f2dac57e":"code","e1477ef3":"code","a3b358df":"code","da3e55eb":"code","5b049ac5":"code","699acf02":"code","eae8aa1c":"code","0940c9d7":"code","8ea1e166":"code","e48ca503":"code","51cee08c":"code","a1c72984":"code","fcc15d0f":"code","55ff83ef":"code","c79ae9d8":"code","f9a1f42a":"code","397c01c2":"code","d95e066a":"code","36f25ef9":"code","4171c2f6":"code","efc61028":"code","b3ff65a8":"code","251f1fc2":"code","6ba87c86":"code","41d0f4cf":"code","d4c91084":"code","467deeaf":"code","3acabb9c":"code","c1c06a3c":"code","2d5f0428":"code","d1347b0c":"code","7780694d":"code","555114e1":"code","a873df7c":"code","e1d74cfa":"code","f51182a5":"code","b512c75e":"code","1ae66fc4":"code","4e672af3":"markdown","b16f657a":"markdown","944013b6":"markdown","9cee7c1b":"markdown","1f4fbaca":"markdown","aa88a857":"markdown","8f56139d":"markdown","090b110f":"markdown","8e3bdb25":"markdown","52b1a208":"markdown","addaf6f5":"markdown","5531e902":"markdown","2c514feb":"markdown","913e538f":"markdown","c1551ae4":"markdown","09221af2":"markdown","dd419b78":"markdown","01376962":"markdown","11cb5ab6":"markdown","b8de76e5":"markdown","9ca3a824":"markdown","697ea10f":"markdown","516005e1":"markdown","3b3eef30":"markdown","ca415249":"markdown","09084aef":"markdown","2bf171e7":"markdown","7bbfafe6":"markdown","fece4471":"markdown","9ca11d7d":"markdown","e65fbfa7":"markdown","c2c1d38d":"markdown","22cafa59":"markdown","5f2ec52a":"markdown","130f1b04":"markdown","4556f219":"markdown","b2da50a1":"markdown","cb9dd8e2":"markdown","2fa8c07c":"markdown","18ee67bd":"markdown","712b14d3":"markdown"},"source":{"a5a3b1e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d595fdd2":"import matplotlib.pyplot as plt\nimport seaborn as sns","0e1a72a2":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","f4087736":"data = train.copy()","5507a4e8":"data.head(3)","08c35e33":"data.dtypes","b82ac1f1":"data.drop('Name',axis=1,inplace=True)","0037a14c":"# first let's see the distribution of target variable\nsns.distplot(data['Survived'])","5c76b701":"data.isnull().sum()","5e693259":"# Imputing age with the best possible technique\nsns.distplot(data['Age'])\nplt.show()","28b5daa8":"median = data.Age.median()\nmedian","cdb877c0":"def impute_median(variable):\n    data[variable+'_median'] = data[variable].fillna(data[variable].median())\n    \nimpute_median('Age')","32ca3e19":"data['Age_median'].isna().sum()","d9beb5fa":"sns.distplot(data['Age'],hist=False,color='red')\nsns.distplot(data['Age_median'],hist=False,color='green')\nplt.show()","ca2c73ce":"# i am going to read the data again with only some column and we will best find the imputation technique\ndf = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',usecols=['Age','Fare','Survived'])","698252f0":"df.head()\n# here i am going to perform the random sampling imputation","fdcba540":"df.Age.sample()\n# we can see that it takes the random sample from the Age everytime we run it","226594dc":"df['Age'].dropna().sample(df.Age.isna().sum(),random_state=1)","1af49046":"def impute_nan(df,variable,median):\n    df[variable+'_median'] = df[variable].fillna(median)\n    df[variable+'_random'] = df[variable]\n    random_sample = data['Age'].dropna().sample(data[variable].isna().sum(),random_state=1)\n    \n    random_sample.index = df[df[variable].isnull()].index\n    df.loc[df[variable].isnull(), variable + '_random'] = random_sample\n","c954d126":"median = df.Age.median()\nimpute_nan(df,'Age',median)","e4c5b143":"df.head()","924f2307":"sns.distplot(df['Age'],hist=False,color='red')\nsns.distplot(df['Age_median'],hist=False,color='green')\nsns.distplot(df['Age_random'],hist=False,color='blue')\nplt.show()","9b23e4e6":"df['Age_nan'] = np.where(df['Age'].isnull(),1,0)\n# where the value is null place 1 otherwise 0","6114934d":"df.head()","c3b9ffc9":"# sns.distplot(df['Age'])\ndf['Age'+'Zeros'] = df['Age'].fillna(0)\ndf['Age'+'hund'] = df['Age'].fillna(100)","f106db9c":"extreme = df['Age'].mean() + 3*df['Age'].std()\nprint(extreme)\ndf['Age'+'_end'] = df['Age'].fillna(extreme)","0ada5072":"sns.distplot(df['Age'])","e8e36135":"sns.distplot(df['Age_end'])\nplt.show()","81e69e01":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","f2dac57e":"categorical_features= [feature for feature in data.columns if data[feature].dtype == 'O']\nprint(\"len of cat feat:\",len(categorical_features))","e1477ef3":"data[categorical_features].head()","a3b358df":"data[categorical_features].isna().sum()","da3e55eb":"data['Embarked'].value_counts()\n# S has highest frequency","5b049ac5":"def impute_freq_cat(df,variable):\n    freq_cat = df[variable].value_counts().index[0]\n    df[variable].fillna(freq_cat,inplace=True)\n    \nimpute_freq_cat(data,'Embarked')","699acf02":"data['Embarked'].isna().sum()\n# Ok it's been imputed correctly","eae8aa1c":"data['cabin_nan'] = np.where(data['Cabin'].isna(),1,0)\ndata.head()","0940c9d7":"data['Cabin'] = data['Cabin'].fillna('Missing')\ndata.head()","8ea1e166":"data.head()","e48ca503":"data[categorical_features].head()","51cee08c":"pd.get_dummies(data['Sex'])","a1c72984":"# here the one column is suitable to represent the other column as well so there is no need og it\nsex = pd.get_dummies(data['Sex'],drop_first=True)\n#sex","fcc15d0f":"data['Embarked'].unique()","55ff83ef":"def create_cat_mapping(df,variable):\n    return {k: i for i, k in enumerate(df[variable].unique(),0)}","c79ae9d8":"mapping = create_cat_mapping(data,'Embarked')\nmapping\n# nice it creates the dictonary in which we can mapp it easily","f9a1f42a":"data['embarked_impute'] = data['Embarked'].map(mapping)","397c01c2":"data['Embarked'].value_counts(normalize=True)","d95e066a":"counts = data['Embarked'].value_counts().to_dict()\ndata['Embarked_valcount'] = data['Embarked'].map(counts)","36f25ef9":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',usecols=['Cabin','Survived'])\ndf.head()","4171c2f6":"df['Cabin'].fillna('Missing',inplace=True)","efc61028":"df['Cabin'] = df['Cabin'].astype(str).str[0]","b3ff65a8":"df.head(3)","251f1fc2":"df['Cabin'].unique()","6ba87c86":"df.groupby('Cabin')['Survived'].mean()","41d0f4cf":"order_labels = df.groupby('Cabin')['Survived'].mean().sort_values().index\norder_labels","d4c91084":"mapping_dict = {k: i for i, k in enumerate(order_labels,0)}\nmapping_dict","467deeaf":"df['Cabin_impute'] = df['Cabin'].map(mapping_dict)\ndf.head()","3acabb9c":"mean_ordinal = df.groupby('Cabin')['Survived'].mean().to_dict()\nmean_ordinal","c1c06a3c":"df['Cabin_mean'] = df['Cabin'].map(mean_ordinal)\ndf.head()","2d5f0428":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',usecols=['Cabin','Survived'])\ndf.head()","d1347b0c":"df['Cabin'].fillna('Missing',inplace=True)\ndf['Cabin'] = df['Cabin'].astype(str).str[0]","7780694d":"prob_df = df.groupby('Cabin')['Survived'].mean()\nprob_df","555114e1":"prob_df = pd.DataFrame(prob_df)\nprob_df","a873df7c":"prob_df['Not-Survived'] = 1 - prob_df['Survived']\nprob_df","e1d74cfa":"prob_df['prob_ratio'] = prob_df['Survived'] \/ prob_df['Not-Survived']\nprob_df.head()","f51182a5":"prob_encoded = prob_df['prob_ratio'].to_dict()\n#prob_encoded","b512c75e":"df['Cabin_encoded'] = df['Cabin'].map(prob_encoded)\ndf.head()","1ae66fc4":"# i am performing directly the steps because in the above line we have calculated the probability ratio\ndf['cabin_woe'] = np.log(df['Cabin_encoded'])\ndf.head()","4e672af3":"**Thanks to everyone who has followed me till the end of this session. In the next kernel I will be continuing this Feature Engineering by performing the techniques as Feature Transformation, Feature Scalling and Treatment of outliers**\n\n**I am open to all the corrections if anything was done wrong by me, in any of the case. I am a enthusiast learner and beginning my carrier in Data Science field so if you find it usefull then please upvote and use the comment section and if anything was wrong or you didn't understand then also please comment. i will surely be very motivated by your upvoting this notebook**\n\n**Thanks A lot**","b16f657a":"### 4) Target Guided Encoding\n**this technique replaces the categorical features with the integers 1 to n. where n is no. of distinct categories.**\n**It uses the target mean to decide how to assign these numbers**","944013b6":"### 5) End of Distribution\n**Most of the observations (~99%) of a normally-distributed variable lie within the mean plus\/minus three times standard deviations\u2014for that the `selected value = mean \u00b1 3 \u00d7standard deviations`.**\n\n**Skewed distributions**\n** The general approach is to calculate the quantiles, and then the inter-quantile range (IQR), as follows:\nIQR = 75th Quantile \u201325th Quantile.**\n\n`Upper limit = 75th Quantile + IQR \u00d73`\n.\n`Lower limit = 25th Quantile - IQR \u00d73`. \n\n**So the selected value for imputation is the previously calculated upper limit or the lower limit.**","9cee7c1b":"**ADVANTAGES**\n1. here also Nan values are provided a great importance.\n2. easy to impliment\n\n**DISADVANTAGES**\n1. there is not any kind of change or increase in correlation because importance is privided to missing values and we want in respect to target variable\n","1f4fbaca":"### Count of frequency Encoding\n**In this we encode the categorical values with its number of observation or by its percentage in the data**","aa88a857":"* here you can see what a great method it is all the values are nicely encoded.","8f56139d":"**ADVANTAGES**\n1. It is easy to impliment\n2. fast\n\n**DISADVANTAGES**\n1. There can been over-representation of a single one value if there are more missing values.\n2. Due to this it can also generate the rare categories in your data,if more missing value are there.\n3. It distorts the relation of frequent category","090b110f":"## 2) Add a missing Indicator to NAN\n**In this technique we try to some some extra importance to nan values**","8e3bdb25":"**3) Capturing Nan as a important feature**","52b1a208":"* mostly the passengers who were in titanic were between the age of 20 to 40","addaf6f5":"* Age is having 177 missing values and cabin as 687 and embarked as 2","5531e902":"### 2) Random Sampling Imputation**\n\n** In this we take the randmom observation from the left of the rows and fill in place of NAN values **","2c514feb":"## Categorical Imputation Technique","913e538f":"**ADVANTAGES**\n1. In this technique we try to some some extra importance to nan values\n\n**DISADVANTAGES**\n1. it creates the extra feature space\n2. when there are less missing value then we cannot use this(eg-Embarked)","c1551ae4":"* This technique is very much powerful to handle the outliers and thus we can see here\n* But it distorts the distribution of feature","09221af2":"**ADVANTAGES**\n1. does not expand the feature space.\n2. straight forward to impliment\n3. create the monotonic relationship between categories and target.\n\n**DISADVANTAGES**\n1. it may lead to overfitting\n2. It may lead to loss of information if 2 categorical features gives the same mean.","dd419b78":"* Yes it's nicely encoded we can match it.","01376962":"*** I will introduce the each Missing value Imputation Technique in this notebook **","11cb5ab6":"### 2) Integer(Label) Encoding","b8de76e5":"* Now I will moving with basic analyticsl part and all the techniques for feature engineering\n**In this particular notebook I will showing you all the techniques being used for Missing Data imputation for numerical and Categorical Features aswell the techniques for categorical Encoding**\n1. Missing Data Imputation\n2. Categorical Encoding\n","9ca3a824":"## Categorical Encoding","697ea10f":"### 1) Mean, Median Imputation**","516005e1":"### 1) Frequent Category Imputation\n**It can also be refferf as mode imputation in which whatever values occus the most of the times in the observation is been imputed inplace of issing value**\n\n* I will be applying this technique on embarked column as there are less missing value","3b3eef30":"**ADVANTAGES**\n1. Straight forward implimentation\n2. does not expand the feature space\n3. works well in tree-based algorithms\n\n**DISADVANTAGES**\n1. we can loose the valuable information in this case, if 2 categorical features have the same frequency which can also lead to distortion of relationship.\n2. does not work on linear-models\n3. does not handle new category automatically.","ca415249":"**ADVANTAGES**\n1. Creates the monotonic ralationship between target and variables\n2. order the categories on a Log Scale which is natural for Logistic Regression\n3. we can compare the transformed variables because they are on same scale. Therefore, its easy to determine which one is more predictive.\n\n**DISADVANTAGES**\n1. may lead to over-fitting\n2. not defined when denominator is 0","09084aef":"### 4) Random Sampling Imputation\n**this we have already covered, it can also be applied on categorical features as mode imputation**","2bf171e7":"**ADVANTAGES**\n1. Straight forward implimentation\n2. does not expand the feature space\n3. works well in tree-based algorithms\n4. Allows agile benchmarking of machine learning models.\n\n**DISADVANTAGES**\n1. Does not create any additional information while encoding.\n2. Not suitable on linear models\n3. create an order relationship.\n4. cannot handle the new categories in data automatically.","7bbfafe6":"**ADVANTAGES**\n1. easy and fast to impliment\n2. canbe used in production\n\n**DisAdvantages**\n1. it cannot be used when we have more categories because it creates a extra feature space(eg- if we have 10 diff cate in column then it will create 9 extra \ncolumn)","fece4471":"### 4) Arbitrary Value imputation\n**In this technique we manuaaly assign the value to missing value towards the edge lower or upper means null are assigned with the value less then minimum or greater then mximum**","9ca11d7d":"### 3) Replacing NAN with new Category variable \n**In this we can replace the NAN with any category like UNKNOWN, MISSING OR anything.**","e65fbfa7":"**Now that we have done perform the 5 Missing Value Imputation Technique on numerical variable. You can choose anyone as per there performance and distribution**\n* here in the case I think the median is only working quite fine according to distribution","c2c1d38d":"### 5) Mean Encoding\n**Mean encoding means replacing the category with the mean target value for that category.**","22cafa59":"**Hello friends, I am glad to have you on my notebook. Please go thoroughly to the full notebook. I have talked about each technique for Missing data imputation and categorical encoding with their advantages and limitations and In which situation which technique can be used**\n\n**If you find any corrections or missing then please use the comment section to let me know. I will appreciate myself to learn something from your side. if you find the notebook usefull please upvote it to help others**","5f2ec52a":"**ADVANTAGES**\n1. does not expand the feature space.\n2. straight forward to impliment\n3. create the monotonic relationship between categories and target.\n\n**DISADVANTAGES**\n1. it may lead to overfitting","130f1b04":"* Here I do not have any meaning with Name so I will drop it. and Ticket is also there but we will try to use it in some way. ","4556f219":"* Nice we can see the imputed age variable(Median) is lying more towards the normal distribution(mean)","b2da50a1":"### Weight of Evidence Encoding\n**It is the technique used to categorical encoding for Binary classification Problems which is somewhat same as Probability Ratio Encoding**\n\n**In this also we calculate the probability ration and perform the log of that ratio**\n\n**Here is a mathematic formula : `WOE = ln (p(1) \/ p(0)).`**","cb9dd8e2":"### 1) One-Hot Encoding","2fa8c07c":"* Random impution gives the exact the original distribution as of variable ","18ee67bd":"### 6) Probability Ratio Encoding\n**It is the technique used to categorical encoding for Binary classification Problems**\n\n**Steps to be followed**\n1. probability of survived based on Cabin\n2. probanility of not survived = `1-P(survived)`\n3. `prob(survived) \/ prob(not-surv)`\n4. Dictionary to map the cabin with prob ratio\n5. replace the category featuures","712b14d3":"## DATA Cleaning\n**First we will treat the Missing value and tried the every possible imputation method to cover in this**"}}