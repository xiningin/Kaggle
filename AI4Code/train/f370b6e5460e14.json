{"cell_type":{"0a95aa64":"code","6acc3148":"code","c7db035f":"code","4816663c":"code","39e1020a":"code","475168bc":"code","31eb3409":"code","26ce471d":"code","0a9d559d":"code","8f0d60d7":"code","8a35c24f":"code","9fdf445e":"code","9a7a3e86":"code","5c4a1de2":"code","650fee21":"code","413937e4":"markdown","3843aed0":"markdown","013c0c87":"markdown","df9d9a53":"markdown","af1c5cab":"markdown","cf964e31":"markdown"},"source":{"0a95aa64":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport string\nimport re\nfrom wordcloud import WordCloud\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6acc3148":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv(\"..\/input\/medium-articles-dataset\/medium_data.csv\")","c7db035f":"top20_articles = df.sort_values([\"claps\"],ascending=0)[0:20][[\"title\",\"claps\",\"publication\",\"url\"]]\ntop20_articles.reset_index(drop=True, inplace=True)\ntop20_articles\n# As we can see the most popular articles are mainly from The Startup. There is only one\n# article from Towards Data Science. Go ahead and explore them. They are great articles.","4816663c":"# What are the article distribution by publications?\nplt.figure(figsize=(20,5))\nsns.countplot(\"publication\", data=df, palette=\"bright\")\n# Towards Data Science rank No.2","39e1020a":"# Average claps and responses for different publications. \navg_by_publication = df.groupby(by=\"publication\").mean()[[\"claps\"]]\navg_by_publication.sort_values([\"claps\"],ascending=False)\n# Medium bloggers should start to write articles in Better Humans if they want \n# more reactions from readers.","475168bc":"#Firstly lets see the wordcloud for towards data science.\nnltk.download('stopwords')\ndf_data_science = df[df[\"publication\"]==\"Towards Data Science\"]\ntitle_data = \"\".join(str(x) for x in df_data_science[\"title\"])\nsubtitle_data = \"\".join(str(x) for x in df_data_science[\"subtitle\"])\ntitle_data = title_data+subtitle_data\nstop_words = set(nltk.corpus.stopwords.words(\"english\"))\nword_cloud = WordCloud(stopwords=stop_words, width=2000, height=1000,\\\n                            max_font_size=160, min_font_size=30).generate(title_data)\nplt.figure(figsize=(12,6), facecolor=\"k\")\nplt.imshow(word_cloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show() #Aha, we saw a lot of familar topics.We should start to figure out these topics now!","31eb3409":"import unicodedata\ndef remove_punct(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    text = re.sub('[0-9]+', '', text)\n    return text\n\ndef remove_stopwords(text):\n    filtered_text = []\n    for i in text.split():\n        i = i.strip().lower()\n        if i not in stop_words:\n            filtered_text.append(i)\n    filtered_text = ' '.join(filtered_text)    \n    return filtered_text\n\ndef normalize_accented_characters(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf8')\n    return text\n\ndef normalize_text(text):\n    text = remove_punct(text)\n    text = remove_stopwords(text)\n    text = normalize_accented_characters(text)\n    return text    ","26ce471d":"df_data_science = df[df[\"publication\"]==\"Towards Data Science\"]\ndf_data_science[\"title\"] = df_data_science[\"title\"].astype(str) + df_data_science[\"subtitle\"].astype(str)\ndf_data_science.drop(columns=[\"subtitle\",\"id\",\"url\",\"image\",\"publication\"],inplace=True)\ndf_data_science[\"title\"] = df_data_science[\"title\"].apply(normalize_text)\ndf_data_science","0a9d559d":"import sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(max_df=0.6,min_df=2)\n# max_df is the maximum precentage this word show in the documents.\n# I tweaked this parameter for topic modelling because some words like \"Data Science\"\n# are too frequent in the documents.\n# min_df means at least it shows min_df times in the documents.\ndoc_term_matrix = count_vect.fit_transform(df_data_science[\"title\"].values)\ndoc_term_matrix","8f0d60d7":"from sklearn.decomposition import LatentDirichletAllocation\n# LDA requires us to specify the number of topics. So that will be hp to tweak.\nnumber_topics = 3\nnumber_words = 20\nLDA = LatentDirichletAllocation(n_components=number_topics, n_jobs=-1)\nLDA.fit(doc_term_matrix)\n#A helper function. You can save this snippet for future use.\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\nprint(\"Topics:\")\nprint_topics(LDA, count_vect, number_words)        ","8a35c24f":"!pip install pyLDAvis #visualizing LDA","9fdf445e":"import pyLDAvis\nimport pyLDAvis.sklearn\nfrom pyLDAvis import sklearn as sklearn_lda\n#prepare to display result in the Jupyter notebook\npyLDAvis.enable_notebook()\nLDAvis = sklearn_lda.prepare(LDA, doc_term_matrix, count_vect)\n#run the visualization [mds is a function to use for visualizing the \"distance\" between topics]\nLDAvis","9a7a3e86":"#Allocate articles into topics.\ntopic_values = LDA.transform(doc_term_matrix)\ndf_data_science['topic'] = topic_values.argmax(axis=1)\ndf_data_science.head(10)\n#Ok, lets humanly test the first 10 -.-\n#[1,1,0,1,1,1,0,0,1,1] 7 out of 10 are correct. ","5c4a1de2":"df_data_science[\"topic\"].replace(0,\"Deep Learning and AI\",inplace=True)\ndf_data_science[\"topic\"].replace(1,\"General Data Analysis\",inplace=True)\ndf_data_science[\"topic\"].replace(2,\"Machine Learning Modeling\",inplace=True)\navg_claps_by_topic = df_data_science.groupby(by=[\"topic\"]).mean()[\"claps\"].reset_index()\navg_reading_time_by_topic = df_data_science.groupby(by=[\"topic\"]).mean()[\"reading_time\"].reset_index()","650fee21":"print(avg_claps_by_topic) # No significant difference.\nprint(avg_reading_time_by_topic) # More reasonable.","413937e4":"Genrally:\n\nTopic0 : Deep Learning and Neural Networks\n\nTopic1 : General Data Analysis and Python tutorial\n\nTopic2 : Machine Learning Modeling\n\nLDA did a bad job when there are 4 topics(or more). \n\nWhy it does not work as great when the num_topics goes up.\n\nIt cannot distingush some data science topics properly. For example, the Tensorflow, ML, and DL are seperated into different topics. \n\nI summarize the reasons behind this:\n\nSo firstly, what is LDA?\n\nLantent stands for distribution of words and topics, which is unknow.\n\nDirichlet stands for a dirichlet probability distribution. It is an assumption of LDA: they follow this distribution.\n\nAllocation means getting the topics in documents and words in topics. \n\nIf \"machine learning\" tends to appear 10 times in TOPIC1, and only 1 time in other topics.Then it difines TOPIC1.\n\nBecause titles are all short strings and authors are more likely to use general words for title. You can solve this problem by tweak the \"max_df\" parameter. However, this will cause info loss. Another awesome way is to scrape data based on URL(It would be awesome). It aslo would be a lot more easier if the \"tag\" data was scraped. Meanwhile, if we use the entire dataset for topic modeling, the result will be much better. \n\n\n\n","3843aed0":"## Topic Modeling","013c0c87":"## What are the top popular articles?","df9d9a53":"### What topics are more likely to get attention?","af1c5cab":"Is it worthy writing blogs on towards data science ?","cf964e31":"## Text Cleaning"}}