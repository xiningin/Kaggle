{"cell_type":{"aca482d9":"code","7ce4372c":"code","db8892f3":"code","e91bbfca":"code","b4c9c53b":"code","0f8f1a75":"code","5f565559":"code","e0052751":"code","f3f0195c":"code","7ed1ff30":"code","9aec681c":"code","e7b91909":"code","9c6650e3":"code","f1367c47":"code","3f1a4633":"code","e8ef9969":"code","f87e44bf":"code","821745a9":"code","33904088":"code","1aa47868":"code","47a9ced2":"code","427fee58":"code","526d7f28":"code","16101683":"code","4ed5d960":"code","5490e257":"code","d61e1f24":"code","13c94f8a":"code","ab6c2cae":"code","93bb781f":"code","d6d7dd8b":"code","681e6500":"markdown","83700733":"markdown","d8c852ec":"markdown","4f3b3d63":"markdown","ab1c6e47":"markdown","3bcfe366":"markdown","228b2275":"markdown","ee8eaa96":"markdown","5f4c9c3f":"markdown","ceb3f733":"markdown","d005d0eb":"markdown","c6c66bae":"markdown","c9e11888":"markdown","42b59f9e":"markdown","ff9ef0ff":"markdown","2c5b325a":"markdown","1bd6175f":"markdown","6b403648":"markdown","32de9dec":"markdown","4e8337a9":"markdown","434316ee":"markdown"},"source":{"aca482d9":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport imageio","7ce4372c":"df = pd.read_csv('..\/input\/fractal-analysis\/intern_dataset.csv')\nprint(len(df))\ndf.tail()","db8892f3":"df['Label'].unique()","e91bbfca":"# the amount of data is a lot so, first thousand values will be reviwed.\nsns.lineplot(data = df[911500:912000], x = 'Time', y = 'Signal1', hue = 'Label')","b4c9c53b":"# coversion function that converts the data for using in compute_Hc\n# and also plots it for you. taken from the Github resource mentioned above.\ndef calc_rms(x, scale):\n    \"\"\"\n    windowed Root Mean Square (RMS) with linear detrending.\n    \n    Args:\n    -----\n      *x* : numpy.array\n        one dimensional data vector\n      *scale* : int\n        length of the window in which RMS will be calculaed\n    Returns:\n    --------\n      *rms* : numpy.array\n        RMS data in each window with length len(x)\/\/scale\n    \"\"\"\n    # making an array with data divided in windows\n    shape = (x.shape[0]\/\/scale, scale)\n    X = np.lib.stride_tricks.as_strided(x,shape=shape)\n    # vector of x-axis points to regression\n    scale_ax = np.arange(scale)\n    rms = np.zeros(X.shape[0])\n    for e, xcut in enumerate(X):\n        coeff = np.polyfit(scale_ax, xcut, 1)\n        xfit = np.polyval(coeff, scale_ax)\n        # detrending and computing RMS of each window\n        rms[e] = np.sqrt(np.mean((xcut-xfit)**2))\n    return rms\n\ndef dfa(x, scale_lim=[7,12], scale_dens=0.125, show=False):\n    \"\"\"\n    Detrended Fluctuation Analysis - measures power law scaling coefficient\n    of the given signal *x*.\n    More details about the algorithm you can find e.g. here:\n    Hardstone, R. et al. Detrended fluctuation analysis: A scale-free \n    view on neuronal oscillations, (2012).\n    Args:\n    -----\n      *x* : numpy.array\n        one dimensional data vector\n      *scale_lim* = [5,9] : list of length 2 \n        boundaries of the scale, where scale means windows among which RMS\n        is calculated. Numbers from list are exponents of 2 to the power\n        of X, eg. [5,9] is in fact [2**5, 2**9].\n        You can think of it that if your signal is sampled with F_s = 128 Hz,\n        then the lowest considered scale would be 2**5\/128 = 32\/128 = 0.25,\n        so 250 ms.\n      *scale_dens* = 0.25 : float\n        density of scale divisions, eg. for 0.25 we get 2**[5, 5.25, 5.5, ... ] \n      *show* = False\n        if True it shows matplotlib log-log plot.\n    Returns:\n    --------\n      *scales* : numpy.array\n        vector of scales (x axis)\n      *fluct* : numpy.array\n        fluctuation function values (y axis)\n      *alpha* : float\n        estimation of DFA exponent\n    \"\"\"\n    # cumulative sum of data with substracted offset\n    y = np.cumsum(x - np.mean(x))\n    scales = (2**np.arange(scale_lim[0], scale_lim[1], scale_dens)).astype(np.int)\n    fluct = np.zeros(len(scales))\n    # computing RMS for each window\n    for e, sc in enumerate(scales):\n        fluct[e] = np.sqrt(np.mean(calc_rms(y, sc)**2))\n    # fitting a line to rms data\n    coeff = np.polyfit(np.log2(scales), np.log2(fluct), 1)\n    if show:\n        fluctfit = 2**np.polyval(coeff,np.log2(scales))\n        plt.loglog(scales, fluct, 'bo')\n        plt.loglog(scales, fluctfit, 'r', label=r'$\\alpha$ = %0.2f'%coeff[0])\n        plt.title('DFA')\n        plt.xlabel(r'$\\log_{10}$(time window)')\n        plt.ylabel(r'$\\log_{10}$<F(t)>')\n        plt.legend()\n        plt.show()\n        print(coeff)\n    return scales, fluct, coeff[0]\n","0f8f1a75":"# compute_Hc will help in computing the hurst component of the data for a label A, B, C and separately\n# code below to separate the data \ndf_A = df[df['Label'] == 'A']\nprint(len(df_A))\ndf_B = df[df['Label'] == 'B']\nprint(len(df_B))\ndf_C = df[df['Label'] == 'C']\nprint(len(df_C))\n# the dataframe is separated","5f565559":"series_A = np.array(df_A['Signal1'])\nscales, fluct, alpha = dfa(series_A, show=1)\nprint(scales)\nprint(fluct)\nprint(\"DFA exponent: {}\".format(alpha))","e0052751":"# Performing DFA for Signal-2 in label A\nseries_A = np.array(df['Signal2'])\nscales, fluct, alpha = dfa(series_A, show=True)\n# 12000 values suggest 20 minutes of data.\nprint(scales)\nprint(fluct)\nprint(\"DFA exponent: {}\".format(alpha))","f3f0195c":"# DFA for signal 1 label B\nseries_B = np.array(df_B['Signal1'])\nscales, fluct, alpha = dfa(series_B, show=1)\nprint(scales)\nprint(fluct)\nprint(\"DFA exponent: {}\".format(alpha))","7ed1ff30":"# DFA for label B signal 2\nseries_B = np.array(df_B['Signal2'])\nscales, fluct, alpha = dfa(series_B, show=1)\nprint(scales)\nprint(fluct)\nprint(\"DFA exponent: {}\".format(alpha))","9aec681c":"# DFA for signal C\nseries_C = np.array(df_C['Signal1'])\nscales, fluct, alpha = dfa(series_C, scale_dens=0.2 ,show=1)\nprint(scales)\nprint(fluct)\nprint(\"DFA exponent: {}\".format(alpha))","e7b91909":"# Label C, Signal 2\nseries_C = np.array(df_C['Signal2'])\nscales, fluct, alpha = dfa(series_C, show=1)\nprint(scales)\nprint(fluct)\nprint(\"DFA exponent: {}\".format(alpha))","9c6650e3":"plt.figure(dpi=1200)\nplt.grid(b=False)\nplt.axis(False)\nplt.plot(np.array(df_A['Signal1']), linewidth = 0.25)\nplt.show()","f1367c47":"plt.figure(dpi=1200)\nplt.grid(b=False)\nplt.axis(False)\nplt.plot(np.array(df_B['Signal1']), linewidth = 0.25)\nplt.show()","3f1a4633":"plt.figure(dpi=1200)\nplt.grid(b=False)\nplt.axis(False)\nplt.plot(np.array(df_C['Signal1']), linewidth = 0.25)\nplt.show()\n# The end at C suggest presence of a higher order trend in C.","e8ef9969":"# found this git repository to calculate the Fractal dimension\n# link to the original : https:\/\/github.com\/viniroger\/fractal\n\"\"\"\nvar1 = 'Signal1'\nplot_data(df_A[0:7500], var1)\n# It was taking longer than expected, so only 7500 values\nfrac = calc_Sdtau(df, var1)\nD = calc_dim(frac)\nprint('Dimension: %s' %D)\n\"\"\"\n# The above is taking too long, the plot it saved explains the Hurst values for A1. Uncomment if you want Fractal dimension","f87e44bf":"# quick demo of what I mean\nnp.random.seed(998)\ndemo = np.random.uniform(size = 10000)\nsns.lineplot(data  = demo)","821745a9":"\nscales, fluct, alpha = dfa(demo, show=1)\nprint(scales)\nprint(fluct)\nprint(\"DFA exponent: {}\".format(alpha))","33904088":"plt.figure(dpi = 1200)\nplt.grid(b=False)\nplt.axis(False)\nplt.plot(np.array(df_A['Signal2']),linewidth = 0.25)\nplt.savefig('A_2.png', dpi = 1200)\nplt.show()","1aa47868":"plt.figure(dpi = 1200)\nplt.grid(b=False)\nplt.axis(False)\nplt.plot(np.array(df_B['Signal2']), linewidth = 0.25)\nplt.savefig('B_2.png', dpi = 1200)\nplt.show()","47a9ced2":"plt.figure(dpi = 1200)\nplt.grid(b=False)\nplt.axis(False)\nplt.plot(np.array(df_C['Signal2']), linewidth = 0.25)\nplt.savefig('C_2.png', dpi = 1200)\nplt.show()","427fee58":"# Found this code on Github\n#https:\/\/github.com\/ErikRZH\/Fractal-Dimension\/blob\/master\/fractal-dimension.py\n\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\ndef fractal_dimension(Z, threshold=0.8):\n    \"\"\"Returns box-counting dimension of a 2D array.\n    Args:\n        Z: 2D array to be analysed.\n        threshold: Cutoff for converting values in Z to 1 and 0.\n    Returns:\n        The estimated box counting dimension.\n    \"\"\"\n\n    # Only for 2d image\n    assert(len(Z.shape) == 2)\n\n    # From https:\/\/github.com\/rougier\/numpy-100 (#87)\n    def boxcount(Z, k):\n        S = np.add.reduceat(\n            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n                               np.arange(0, Z.shape[1], k), axis=1)\n\n        # We count non-empty (0) and non-full boxes (k*k)\n        return len(np.where((S > 0) & (S < k*k))[0])\n\n\n    # Transform Z into a binary array\n    Z = (Z < threshold)\n\n    # Minimal dimension of image\n    p = min(Z.shape)\n\n    # Greatest power of 2 less than or equal to p\n    n = 2**np.floor(np.log(p)\/np.log(2))\n\n    # Extract the exponent\n    n = int(np.log(n)\/np.log(2))\n\n    # Build successive box sizes (from 2**n down to 2**1)\n    sizes = 2**np.arange(n, 1, -1)\n\n    # Actual box counting with decreasing size\n    counts = []\n    for size in sizes:\n        counts.append(boxcount(Z, size))\n\n    # Fit the successive log(sizes) with log (counts)\n    coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n    return -coeffs[0]","526d7f28":"# > Label A, Signal 2\nI = imageio.imread(\"A_2.png\", as_gray=\"True\")\/255.0    # Import the image in greyscale\nprint(\"Minkowski\u2013Bouligand dimension (computed): \", fractal_dimension(I))","16101683":"# > Label B, Signal 2\nI = imageio.imread(\"B_2.png\", as_gray=\"True\")\/255.0    # Import the image in greyscale\nprint(\"Minkowski\u2013Bouligand dimension (computed): \", fractal_dimension(I))","4ed5d960":"# > Label C, Signal 2\nI = imageio.imread(\"C_2.png\", as_gray=\"True\")\/255.0\n#print(np.min(I))    # Import the image in greyscale\nprint(\"Minkowski\u2013Bouligand dimension (computed): \", fractal_dimension(I))","5490e257":"\"\"\"\n# Trial for Label A Signal 2:\nseris_A = np.array(df_A['Signal2'])\nA_im = np.zeros((1000,318000))\nminA = np.min(seris_A)\nbound = np.max(seris_A) - minA\nfor i in range(318000):\n    X = series_A[i] - minA\n    X = X\/bound\n    X = int(X * 999)\n    A_im[X][i] = 1\n# the above should hold the whole data like I\nprint(\"Minkowski\u2013Bouligand dimension (computed): \", fractal_dimension(A_im))\n\"\"\"\n# This gave a result of 0.866\n# to avoid making the code slower I commented this.","d61e1f24":"def ApEn_new(U, m, r):\n    U = np.array(U)\n    N = U.shape[0]\n            \n    def _phi(m):\n        z = N - m + 1.0\n        x = np.array([U[i:i+m] for i in range(int(z))])\n        X = np.repeat(x[:, np.newaxis], 1, axis=2)\n        C = np.sum(np.absolute(x - X).max(axis=2) <= 3, axis=0) \/ z\n        return np.log(C).sum() \/ z\n    \n    return abs(_phi(m + 1) - _phi(m))","13c94f8a":"# using the above found code to calculate entropy,\nentropy = 0\n\"\"\"\nfor i in range(30):\n    entropy = entropy + (ApEn_new(np.array(df_A['Signal1'])[i*10000:(i+1)*10000], 2, 300000)) \nprint(abs(entropy))\n\"\"\"\nprint(ApEn_new(np.array(df_A['Signal1'])[0:10000], 2, 3)) # entropy : 0.0036\n# The entropy came out to be 0.0 for the whole series Signal 1 label A.","ab6c2cae":"print(ApEn_new(np.array(df_B['Signal1'])[0:10000], 2, 3)) # entropy : 0.00620","93bb781f":"print(ApEn_new(np.array(df_C['Signal1'])[0:10000], 2, 3)) # entropy : 0.00660","d6d7dd8b":"print(ApEn_new(np.array(df_A['Signal2'])[0:10000], 2, 3)) # entropy : 0 for all three\nprint(ApEn_new(np.array(df_B['Signal2'])[0:10000], 2, 3))\nprint(ApEn_new(np.array(df_C['Signal2'])[0:10000], 2, 3))","681e6500":"### The hurst component is more than 0.5, can correspond to persisent behaviour, and the data was repeating(looked very synthetic and unnatural)\n#### Also a point to note is that the line is not really capturing the points of the data, we can try quadratic fit, which is also known as the Higher order DFA \n## ","83700733":"### The images hence produced will be used for scanning later on to calculate fractal dimension by box counting method.\n*I am doing this because, compared to signal 1, signal 2 is varying very slowly over time, so much so that the DFA is not at all suitable for signal 2 , also the shape makes it apparent that some other method will be suitable.*","d8c852ec":"### Approximate entropy - calculating entropy for a time series\n#### Found this code on : https:\/\/gist.github.com\/DustinAlandzes\/a835909ffd15b9927820d175a48dee41\n#### which is in reference to the article on : https:\/\/en.wikipedia.org\/wiki\/Approximate_entropy","4f3b3d63":"### The data is large and it takes sometime to run the problem, but as we can see above that the hurst component of the data is more than 0.5, which suggests persistent behaviour, \n#### In all the above DFA's we can see the scales array which are nothing but all the time intervals we took to separate the data into epochs to perform DFA, reading the papers and online material gave a thorough understanding of what fractal analysis is, the only thing remaining is exploring more methods of analysis and come up with a solution of analysing Signal 2 of the Labels.\n#### One argument is sensible that why did I not pass the Time series of the data in the function, its because the time series is changing by only by 0.1 second, and if one wanted the real time intervals that I took multiply the Scales array by 0.1, and the resulting values will be the time in seconds taken for analysis.","ab1c6e47":"### Entropy what it means\n#### The above example is not the best way to assess, but it will roughly give a good idea of what is happening in the data, 0 entropy suggest that the time series not very random, and it may be due to the fact that Signal 2 is not changing very slowly with time and gets enropy zero, the previous values are very close to their next values. I divided the data into batch and calculated entropy for each batch and hit maximum of about 0.009(closer to 0.01).","3bcfe366":"### If we use the package as is, the limitation set pretty quickly, as suggested by the shape of I, the requirement is for a 2-D array which has intensities plotted between (0,1). The 0.3 million values in the dataset is brought down to 7200 courtsey of the above stunt. Converting the time to X and signal 2 to Y one can use the 2-D array produced as is in the fractal_dimension function.","228b2275":"## Label B","ee8eaa96":"## Label C","5f4c9c3f":"### Visualizing Signal 2 of the data","ceb3f733":"### Shape of the curve expalins a lot for the data, B does look like a persistent time series(natural), whereas A looks synthetic and compeletly unnatural\n### Hurst exponent A:0.53303\n### Hurst exponent B:0.52087 \n### Hurst exponent C:0.43018  (when Scale limit was (32, 430))\n#### I changed the scale array,for some graphs(A's component went below 0.49)\n#### The data as you can see, mean , median and mode will do little to nothing to explain the data and this increases the importance of fractal analysis, I took only first 700 value in each case because it was taking really long to make the graphs, but numbers dont lie, I have checked the DFA algorithm and the results produced are correct,(Found this online Hurst value more than one means unsuccesful detrending) so the Hurst exponent more than 0.5 suggests persistent behaviour, less than 0.5 suggests synthetic data(unnatural), or anti persistent. If a natural looking data(coast line, heartbeat, numpy random walk) is taken then the Hurst exponent will rise closer to one.(Criticality), time series which encapsulates \nnature have higher hurst component in fractal analysis.\n\n## Complexity of the classes\n#### As we can see Labels A and B, signal 1, has successful detrending, the hurst values in the correct range, changing the scale of the DFA had pronounced effect on DFA component, I changed the Scale array multiple times, Signal 1 in label A and B had very little effect, I conclude that the DFA was successful and the classes A and B are less complex when compared to C.\n#### class C showed unsuccesful detrending in DFA and the Hurst value is out of range for signal 2, but the shape of the regression line does not do justice to the points in the graph, I read about this in an article(Wikipedia) maybe a higher order DFA(rather than fitting a linear line, we can try to fit quadratic(or cubic) function in log-log plot)\n## Conclusion: Classes A and B are not as complex as Class C.","d005d0eb":"### A note for the reader\n#### The data suggests that it almost 31800 seconds which is roughly 8.8 hours, and a lot of articles suggested that DFA should be done for data which has atleast ten minutens of value, I tried calculating hurst component with different scales, and it changes depending on the value of the scales array which is used to detrend the data.\n#### I also found that larger the data better the accuracy is, and a general tip is to try to take the scale array to be large as possible.\n#### If I take very large values in the scale array, the mathematics in detrending will ignore very fine fluctuations in data.\n\n#### Here I found someone having a similar issue:\nhttps:\/\/www.researchgate.net\/post\/Can-anybody-help-with-Hurst-Exponent-algorithms-and-scaling-window\n\n#### Also I noticed that plot looks lot like it a quadratic, it has maybe a quadratic trend in it(label C).","c6c66bae":"### The arrays are so big that it will not be able to calculate the entropy correctly. I have to make the algorithm better. With some minor modifications the algorithm, will work for our case and not run into memory error, it takes a long time to calculate so the code will be commented with the results pre-calculated.","c9e11888":"### The above has failed because of inconsistency, the png method may not be that precise still gives a better value than above.","42b59f9e":"### Interpretation of the Hurst value and what it means for the data\n#### The hurst value for the label A signal 1: a look at the first 700 values, and anyone can conclude that this is a mean-reverting time series, but that would be nothing but an error, when I took into account the whole 318k+ values of the data and calculated the hurst component (0.533) it suggest that the time series is actually a brownian random motion(referred to as the drunkard's walk) that, there is no correlation in the future values of the data with the current observed one, it is equally likely for a increase or decrease in the time series.\n#### The interpretation for label B signal 1: I expected this to have more hurst component, atleast more than A1, but its close also a part of browinian random motion, no correlation between the future value and the current observed values.\n#### For label C and signal 1: This class and signal saw a lot of irregularities, the hurst values change rapidally with change in scale, but fitting the linear line would be an error as the curve suggests presence of higher order trend in data, it is more complex compared to class A and B.\n#### It would have been fun to get a hurst value more than 0.5 closer to 1, to watch a real natural phenomenon in the plots. Fractal dimension came up a lot in the material I read for this assignment and so I will calculate the fractal dimension of signal 1 all labels, cause I am curious.","ff9ef0ff":"### One intersting point to note is that, after this sudden drop happens in signal 2 in C, signal 1 completly changes behaviour.","2c5b325a":"### Fractal dimension\n#### One method is to make images of the Signal 2 for all classes and scan them (imread) and calculate fractal dimension, found a lot of packages on Github that scan images to find dimension. Limitation is that Scanning the whole data set is not possible(algorithm will be very slow)\n#### The Limitation of seaborn package are apparent, when using big data Matplotlib is actually faster, the dpi of the plots is kept high and the linewidth is kept low to capture each fluctuation with accuracy for image scanning, the images produced will be used for fractal dimension.","1bd6175f":"#### The above suggests random brownian motion, first I took 1000 values H was 0.43, 10000 value gave 0.51, 50000 gave H as 0.5009, The more data you have the better accuracy you will get, as the data is increased(50000) H tends to 0.5(geometric brownian motion)","6b403648":"### Signal 2 for all the classes\n#### I conclude that signal 2 would be unfit for DFA and box-counting method will be better.","32de9dec":"#### Found various repositories for hurst exponent, DFA and various packages for analysis of the data\n#### Some source I used for making concept more clear\n#### 3blue 1brown's video and https:\/\/www.youtube.com\/watch?v=-RmxLZF8adI for understanding\n#### First up: Hurst exponent after DFA, I referred to the above two videos for concept clarity. \n#### Hurst package is used: link: https:\/\/github.com\/Mottl\/hurst\n#### Found this package where the DFA calculation had been done before and also plotted. (link): https:\/\/github.com\/dokato\/dfa\/blob\/master\/dfa.py","4e8337a9":"## Fractal Analysis: A strech for a beginner\n#### I was given this problem by a propective employer, to perform a fractal anlysis on a time series, Fractal anlysis is a far-cry from standard geometry, moreover it challenges the standard rules of smoothness of a curve in standard algebra and geometry, and it is more interesting than it sounds, I had a ball working on this problem. Fractal analysis(Detrended Fluctuation Analysis) consists of methods which tries to quantify the roughness of a curve. I hope readers will enjoy this notebook just like I did.","434316ee":"## Fractal dimension for all the Labels in signal 2:\n#### For label A, Signal 2: 1.119\n#### For label B, Signal 2: 1.324\n#### For label C, Signal 2: 1.194\n#### This was expected, one look at the graph and Label B looks more fractal like compared A and C, where A and C seem to follow very smooth trends (uncharacteristic of a fractal), even the changes were not apparent until the whole curve was plotted."}}