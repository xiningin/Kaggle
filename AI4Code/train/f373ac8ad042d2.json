{"cell_type":{"29cb47bc":"code","b4aad3cb":"code","ce3eca46":"code","a0c094a3":"code","0293e0ec":"code","13a37730":"code","cedbf816":"code","db68f7a3":"code","5b81ed40":"code","496b1ccb":"code","9c2a6ce7":"code","e72460e4":"code","83aa28d7":"code","2270c99f":"code","6891ae25":"code","1ee7bbf1":"code","b8c5cf6e":"code","642585fb":"code","4a3aab95":"code","70bb9e86":"code","a8956569":"code","3b15d76b":"code","b0bc7081":"code","1f1ff8f6":"markdown","145d6e8b":"markdown","882e115f":"markdown","a57e14db":"markdown","2480b01b":"markdown","6bedd29c":"markdown","2f567188":"markdown","7fb0c276":"markdown","2b1731f2":"markdown","aecc37a7":"markdown","b8083a68":"markdown","c5b49fb7":"markdown","421b6042":"markdown","39045060":"markdown","57b8bbba":"markdown","9374b86b":"markdown","2b3d42eb":"markdown","d9877883":"markdown","baccbab0":"markdown","8aae75e5":"markdown","82d19e7e":"markdown","7eb57b95":"markdown","d301333e":"markdown","48ea760e":"markdown","6091e9b4":"markdown","aaed1a01":"markdown"},"source":{"29cb47bc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nimport time\n%matplotlib inline\n\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\nimport warnings\nwarnings.filterwarnings('ignore')","b4aad3cb":"admission = pd.read_csv(\"..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")","ce3eca46":"admission.head()","a0c094a3":"admission.describe()","0293e0ec":"admission.shape","13a37730":"admission.info(verbose=True)","cedbf816":"sns.set(color_codes=True)\nsns.pairplot(admission)","db68f7a3":"admission.drop(columns=['Serial No.'], inplace=True)\nplt.figure(figsize=(20,25))\ni = 0\n\nfor item in admission.columns:\n    i += 1\n    plt.subplot(4, 2, i)\n    sns.distplot(admission[item], rug=True, rug_kws={\"color\": \"m\"},kde=True,\n                 kde_kws={\"color\": \"red\", \"lw\": 3, \"label\": \"KDE\"},\n                 hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\"alpha\": 1, \"color\": \"orange\"},label=\"{0}\".format(item))\nplt.show()\n","5b81ed40":"plt.figure(figsize=(20,25))\nsns.jointplot(x=\"GRE Score\", y=admission['Chance of Admit '], data=admission, kind=\"reg\", height=12,color='m')\nplt.show()","496b1ccb":"plt.figure(figsize=(20,25))\nsns.jointplot(x=\"TOEFL Score\", y=admission['Chance of Admit '], data=admission, kind=\"reg\", height=12,color='m')\nplt.show()","9c2a6ce7":"plt.figure(figsize=(20,25))\nsns.jointplot(x=\"CGPA\", y=admission['Chance of Admit '], data=admission, kind=\"reg\", height=12,color='m')\nplt.show()","e72460e4":"sns.heatmap(admission.corr(), annot=True)","83aa28d7":"X = admission.iloc[:,:-1]\ny = admission.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\nlinear_regression_performance = {}","2270c99f":"def my_linear_model(X,y):\n  start = time.time()\n  # Augment the x vector\n  n,m = X.shape\n  linearX = np.c_[(np.ones((n,1)),X)]\n\n  # Compute beta\n  beta_linear = np.linalg.inv(np.transpose(linearX).dot(linearX)).dot(np.transpose(linearX)).dot(y)\n  end = time.time()\n  t= end-start\n  return beta_linear,t\n\nbeta_linear,t=my_linear_model(X_train,y_train)\nprint(f'The Bias: {beta_linear[0]}\\nThe Weights: {beta_linear[1:]}')\nprint(f'Mean Absolute Error: {mean_absolute_error(y_test,np.add(np.sum(np.multiply(X_test,beta_linear[1:]),axis=1),beta_linear[0]))}, Time(s): {t}')\n\nlinear_regression_performance[\"Explicit OLS\"] = [mean_absolute_error(y_test,np.add(np.sum(np.multiply(X_test,beta_linear[1:]),axis=1),beta_linear[0])),t]","6891ae25":"LR = LinearRegression()\nstart = time.time()\nLR.fit(X_train,y_train)\nend = time.time()\nprint(f'The Bias: {LR.intercept_}')\nprint(f'The Weights: {LR.coef_}')\nprint(f'Mean Absolute Error: {mean_absolute_error(y_test,LR.predict(X_test))}, Time(s): {end-start}')\nlinear_regression_performance[\"sklearn OLS\"] = [mean_absolute_error(y_test,LR.predict(X_test)),end-start]","1ee7bbf1":"start = time.time()\n# fit our linear regression model\nmodel1 = sm.OLS(y_train, X_train).fit()\n# apply trained\/learned model back to training data\npred1 = model1.predict(X_test)\nend = time.time()\n\n# print summary stats from model fit\nprint(model1.summary())\n\nprint(f'Mean Absolute Error: {mean_absolute_error(y_test,pred1)}, Time(s): {end-start}')\nlinear_regression_performance[\"statsmodels OLS\"] = [mean_absolute_error(y_test,pred1),end-start]","b8c5cf6e":"print(f'{round(beta_linear[1],4)}*GRE Score + {round(beta_linear[2],4)}*TOFEL Score + {round(beta_linear[3],4)}*University Rating + {round(beta_linear[4],4)}*SOP + {round(beta_linear[5],4)}*LOR + {round(beta_linear[6],4)}*CGPA + {round(beta_linear[7],4)}*Research {round(beta_linear[0],4)}')","642585fb":"LR = LinearRegression()\nstart = time.time()\nLR.fit(X_train.drop(['LOR ', \"SOP\"],axis=1),y_train)\nend = time.time()\nprint(f'The Bias: {LR.intercept_}')\nprint(f'The Weights: {LR.coef_}')\nprint(f'Mean Absolute Error: {mean_absolute_error(y_test,LR.predict(X_test.drop([\"LOR \", \"SOP\"],axis=1)))}, Time(s): {end-start}')\nlinear_regression_performance[\"sklearn feature selection OLS\"] = [mean_absolute_error(y_test,LR.predict(X_test.drop([\"LOR \", \"SOP\"],axis=1))),end-start]","4a3aab95":"print(f'{round(LR.coef_[0],4)}*GRE Score + {round(LR.coef_[1],4)}*TOFEL Score + {round(LR.coef_[2],4)}*University Rating + {round(LR.coef_[3],4)}*CGPA + {round(LR.coef_[4],4)}*Research {round(LR.intercept_,4)}')","70bb9e86":"pd.DataFrame(linear_regression_performance, index=['Mean absolute error','Time (s)']).T","a8956569":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\ntest_results = {}","3b15d76b":"optimizers = [tf.optimizers.Adam(learning_rate=0.1),tf.optimizers.SGD(learning_rate=0.1),tf.optimizers.RMSprop(learning_rate=0.1), tf.optimizers.Adagrad(learning_rate=0.1), tf.optimizers.Adamax(learning_rate=0.1)]\noptimizers_name = [\"Adam\", \"SGD\", \"RMSprop\",\"Adagrad\", \"Adamax\"]\n\nfor opt,name in zip(optimizers, optimizers_name):\n  start = time.time()\n  admissions_normalizer = preprocessing.Normalization(input_shape=[7,],axis=None)\n  admissions_normalizer.adapt(X_train.to_numpy())\n  admissions_model = tf.keras.Sequential([\n      admissions_normalizer,\n      layers.Dense(units=1)\n  ])\n  admissions_model.compile(\n      optimizer=opt,\n      loss='mean_absolute_error')\n\n  history = admissions_model.fit(\n      X_train, y_train,\n      epochs=25,\n      # suppress logging\n      verbose=0\n  )\n  end = time.time()\n  test_results[name] = [admissions_model.evaluate(X_test,y_test, verbose=0), end-start]","b0bc7081":"pd.DataFrame(test_results, index=['Mean absolute error','Time (s)']).T","1f1ff8f6":"## Import Dataset\n\nWe will be working with a UCLA Admission Criteria Dataset","145d6e8b":"## Import Libraries","882e115f":"## Check Out Dataset\n\nThis dataset has 9 columns (Only 7 columns could be useful as features)\n* Serial No.\n* GRE\n* TOEFL Score\n* University Rating\n* SOP\n* LOR\n* CGPA\n* Research\n* Chance of Admit\n\nWe will be trying to predict the \"Chance of Admit\" column\n\n### The Target Varriable\nIn **predictive models**, there is a response variable (also called dependent variable or **target variable**), which is the variable that we are interested in predicting.\n\n### The Features\nThe **independent variables** (the predictors, also called **features** in the machine learning community) are one or more numeric variables we are using to predict the response variable. Given we are using a linear regression model, we are assuming the relationship between the independent and dependent variables follow a straight line. ","a57e14db":"# CAIS Workshop 2: Linear Regression\n\nIn the hands-on portion of the Linear Regression workshop we will be using the [LinearRegression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html) model from scikit-learn and the [Ordinary Least Sqaures](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.regression.linear_model.OLS.html) model from statsmodels\n\nThis notebook uses code, data and information from the following sources:\n* https:\/\/www.kaggle.com\/ashishpatel26\/everything-about-ucla-admission-criteria\n* https:\/\/sungsoo.github.io\/2018\/04\/11\/predicting-car-prices.html\n\n\nTable of Contents\n* Importing a dataset\n* Checking out the dataset \n* Analyzing the dataset (Pair plots, distributions, correlation)\n* Training the Model (Two different ways)\n* Exploring Different Verisons of Gradient Descent","2480b01b":"We can use the `info()` method with the parameter `verbose=True` to see the data types of the columns in the data frame and other information","6bedd29c":"### Correlation\n\nSome features will be more useful than others in training a classifier. Some features can be redundant. A good way to determine this is to look at the **correlation** between the features and the target variable and between the features.","2f567188":"## Train the Model\nAs described in the slides there are two different ways to train a Linear Regression model. We can use the Ordinary Least Sqaures approach or gradient descent. \n\nLet's explore both below!\n\nFirst up is splitting the data. It is **very important** when training and testing a model that we split the data into train and test. Sometimes we can even split the data into three groups: train, validation and test. Where we train our model on the training set, apply it to the validation set and cycle between the two to tune out hyperparameter choices. And finally test on the test set to have an accurate estimation of the performance of our model.\n\nFor today, we will only be using train and test since we will not be performing hyperparameter tuning","7fb0c276":"TOEFL vs Chance of Admit","2b1731f2":"### Feature Distributions\n\nAnother way to look at the data is to plot histograms of the **distribution** of the features.","aecc37a7":"# That's all we got for today!\n\n**Make sure to check us out on Facebook, Instagram and Our [Website](https:\/\/carletonai.com\/)**\n\n","b8083a68":"### Interesting Bivariate Relationships\n\nAs seen in the pair plots the GRE score, TOEFL score and CGPA have a decent linear relationship with the Chance of admit score\n\nLet's look at some of those plots more closely","c5b49fb7":"#### Using the results from our explicit OLS we can form the equation of our linear model.","421b6042":"We can use the `shape` attribute to see the shape of the data frame","39045060":"#### Linear Regression: Using statsmodels' [Ordinary Least Squares](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.regression.linear_model.OLS.html)","57b8bbba":"## Exploring Different Variants of Gradient Descent\n**Can't be run without a GPU**\n\n\nThere are many different variants of gradient descent.\n\nWe are going to use five different kinds of gradient descent to solve for our Linear Regression parameters.\n\n* [Adam](https:\/\/keras.io\/api\/optimizers\/adam\/)\n* [SGD](https:\/\/keras.io\/api\/optimizers\/sgd\/)\n* [RMSprop](https:\/\/keras.io\/api\/optimizers\/rmsprop\/)\n* [Adagrad](https:\/\/keras.io\/api\/optimizers\/adagrad\/)\n* [Adamax](https:\/\/keras.io\/api\/optimizers\/adamax\/)\n\nTo make it easier we are going to create a one layer neural network with one fully connected unit and no activation function. This is essentially linear regression.","9374b86b":"Then we are going to iterate over the 5 optimization algorithms.\n\nWe will:\n1. Create the 1 layer, 1 unit Neural Network\n\n2. Compile the network with the optimization algorithm\n\n3. Then fit the model to the training data over 25 epochs\n\n4. We will then apply the trained model to the test data and store the Mean Absolute Error","2b3d42eb":"#### Linear Regression: Using sklearn's [Linear Regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html)","d9877883":"GRE vs Chance of Admit","baccbab0":"# Examine Raw Variables\nBefore we start the modeling exercise, it\u2019s good to take a  look at what we are trying to predict to see what it looks like.\n\nHere we can use the `describe()` method on our data frame to see some information about the data frame.\n\nWe can see:\n* Statistical attributes of the numerical data\n* How many non-null data points we have for each feature","8aae75e5":"#### Linear Regression: Ordinary Least Squares Explicitly\n\nWe are going to use the numpy package to perform matrix operations to solve the following equation: \n\n$\\beta = (X^TX)^{-1}X^Ty$","82d19e7e":"## Analyze Dataset\n###  Pair plots\nIt\u2019s good to take a **visual** look at what we are trying to predict to see what it looks like. A good way to visualize how our data behaves is to look are pair plots. With these plots we can see how each feature relates to the target variable and one another.\n\nLet's look at our data. When we visualize our data for linear regression, we are **looking for variables that form a _linear trend_ with the Chance of Admit**. ","7eb57b95":"First we import the packages we need","d301333e":"#### From the OLS summary we can determine that the SOP and LOR features are probably not useful for our model\n\nSo let's drop those features and try again","48ea760e":"Finally, here are our results","6091e9b4":"Next we are going to train our models on the training data and test the models on the testing data. To **test** the model we will need a metric to define performance. Today we will use the performance metric __Mean Absolute Error__ defined by the equation below:\n\n$MAE =\\left ( \\frac{1}{n} \\right) \\sum_{i=1}^{n}\\left | y_{i} - \\hat{y_{i}}\\right |$\n\nThis will allow us to average the amount of error between the model's predictions and the true value over all the data points.\n\nWe are also going to report the time it took to fit the models to the training data","aaed1a01":"CGPA vs Chance of Admit"}}