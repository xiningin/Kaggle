{"cell_type":{"c1f8881f":"code","560fbddf":"code","d5971a77":"code","9421b2ac":"code","9883e104":"code","68197145":"code","28ebd96f":"code","22b1964e":"code","2cae7def":"code","dc88f91e":"code","74a41a75":"code","cd589527":"code","3c34ffd6":"code","4dda1883":"code","2e06b6aa":"code","bdb32ac3":"code","c3248dfb":"code","7bd04d56":"code","056b24e8":"code","0ca0c9d5":"code","1c6fa8a5":"code","526c9133":"markdown","2e496465":"markdown","30bcc92a":"markdown","ce432e2e":"markdown","43708247":"markdown","903ff48d":"markdown","884ed065":"markdown","d94fa306":"markdown","08b96a5c":"markdown","043e00ba":"markdown","eb50641b":"markdown","9004efa6":"markdown"},"source":{"c1f8881f":"\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]\n\ndata = pd.read_csv('..\/input\/credit-card\/creditcard.csv',sep=',')\ndata.head()\n","560fbddf":"data.info()\n","d5971a77":"data.isnull().values.any()","9421b2ac":"count_classes = pd.value_counts(data['Class'], sort = True)\n\ncount_classes.plot(kind = 'bar', rot=0)\n\nplt.title(\"Transaction Class Distribution\")\n\nplt.xticks(range(2), LABELS)\n\nplt.xlabel(\"Class\")\n\nplt.ylabel(\"Frequency\")","9883e104":"## Get the Fraud and the normal dataset \n\nfraud = data[data['Class']==1]\n\nnormal = data[data['Class']==0]","68197145":"print(fraud.shape,normal.shape)","28ebd96f":"## We need to analyze more amount of information from the transaction data\n#How different are the amount of money used in different transaction classes?\nfraud.Amount.describe()","22b1964e":"normal.Amount.describe()","2cae7def":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","dc88f91e":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data, palette=\"PRGn\",showfliers=False)\nplt.show();","74a41a75":"\n## Take some sample of the data\n\ndata1= data.sample(frac = 0.1,random_state=1)\n\ndata1.shape\n","cd589527":"data.shape","3c34ffd6":"#Determine the number of fraud and valid transactions in the dataset\n\nFraud = data1[data1['Class']==1]\n\nValid = data1[data1['Class']==0]\n\noutlier_fraction = len(Fraud)\/float(len(Valid))","4dda1883":"print(outlier_fraction)\n\nprint(\"Fraud Cases : {}\".format(len(Fraud)))\n\nprint(\"Valid Cases : {}\".format(len(Valid)))","2e06b6aa":"## Correlation\nimport seaborn as sns\n#get correlations of each features in dataset\ncorrmat = data1.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","bdb32ac3":"sns.regplot(x=\"V2\",\n            y=\"Amount\",\n            ci=None,\n            data=fraud);\n","c3248dfb":"sns.regplot(x=\"V2\",\n            y=\"Amount\",\n            ci=None,\n            data=normal);\n","7bd04d56":"#Create independent and Dependent Features\ncolumns = data1.columns.tolist()\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n# Store the variable we are predicting \ntarget = \"Class\"\n# Define a random state \nstate = np.random.RandomState(42)\nX = data1[columns]\nY = data1[target]\nX_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n# Print the shapes of X & Y\nprint(X.shape)\nprint(Y.shape)","056b24e8":"\nclassifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X), \n                                       contamination=outlier_fraction,random_state=state, verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None, contamination=outlier_fraction),\n   \n   \n}","0ca0c9d5":"type(classifiers)","1c6fa8a5":"n_outliers = len(Fraud)\nfor i, (clf_name,clf) in enumerate(classifiers.items()):\n    #Fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X)\n        scores_prediction = clf.negative_outlier_factor_\n    else:    \n        clf.fit(X)\n        scores_prediction = clf.decision_function(X)\n        y_pred = clf.predict(X)\n    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != Y).sum()\n    # Run Classification Metrics\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(Y,y_pred))\n    print(\"Classification Report :\")\n    print(classification_report(Y,y_pred))\n    print(\"Confusion Matrix :\")\n    print(confusion_matrix(Y,y_pred))\n    ","526c9133":"**Visualization**\n\nA graph\/chart is one of the best methods to understand the data that we have.\n\nWe will start analyzing how many of the cases in this dataset are fraudulent and which are not.\n\n**Plot histograms for the frequency\/number of fraudulent and non-fraudulent transactions against Amount**","2e496465":"**Importing the credit card fraud data set **","30bcc92a":"**Building an outlier detection model for the data using the Isolation Forest and the Local Outlier Factor classifiers**","ce432e2e":"3h)\n\nObservations :\nIsolation Forest detected 73 errors versus Local Outlier Factor detecting 97 errors.\n\nIsolation Forest has a 99.74% more accurate than LOF of 99.65%.\n\nWhen comparing error precision & recall for 2 models , the Isolation Forest performed much better than the LOF as we can see that the detection of fraud cases is around 27 % versus LOF detection rate of just 2 %.\n\nSo overall Isolation Forest Method performed much better in determining the fraud cases which is around 30%.\n\nWe can also improve on this accuracy by increasing the sample size or use deep learning algorithms however at the cost of computational expense.\nWe can also use complex anomaly detection models to get better accuracy in determining more fraudulent cases","43708247":"**Draw boxplots showing summary statistics for the Amount column**\n\nA Box Plot is also known as Whisker plot is created to display the summary of the set of data values having properties like minimum, first quartile, median, third quartile and maximum. In the box plot, a box is created from the first quartile to the third quartile, a verticle line is also there which goes through the box at the median. Here x-axis denotes the data to be plotted while the y-axis shows the frequency distribution.","903ff48d":"**Exploring the data we imported**\n\nFirstly, we need to check if the dataset have any missing values. Pandas can only check for standard missing values which is null.","884ed065":"Looking at the histogram above, we can easily notice that the number of fraud cases were very few compared to the enormous number of non-fraudulent cases","d94fa306":"**Content of the dataset**\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","08b96a5c":"**Generate a correlation matrix illustrating using a heatmap the relationship between the different variables**","043e00ba":"**Analyzing the models using Errors, Confusion Matrix, Accuracy Score and Classification Report to identify the strengths and weaknesses of the models.\n\n\n**Classification Report**","eb50641b":"This is my submission for FIC4020 assignment submission\n\nKhushi Gupta: 655456\n\n**In your Notebook do the following:**\n1. Import the credit card fraud data set\n\n2. Plot histograms for the frequency\/number of fraudulent and non-fraudulent transactions against Amount\n\n3. Draw boxplots showing summary statistics for the Amount column\n\n4. Generate a correlation matrix illustrating using a heatmap the relationship between the different variables\n\n5. Generate a scatterplot for Amount and V2 showing a line of best fit using the equation of a straight line is y = mx + c, where m is the slope of the line and c is the y intercept\n\n6. Build an outlier detection model for your data using the Isolation Forest and the Local Outlier Factor\n\n7. Analyze the models using Errors, Confusion Matrix, Accuracy Score and Classification Report to identify the strengths and weaknesses of the models\n\n8. Discuss as a conclusion the best model and how to use it in the future in identifying fraudulent credit card transactions","9004efa6":"**Generating a scatterplot for Amount and V2 showing a line of best fit using the equation of a straight line is y = mx + c, where m is the slope of the line and c is the y intercept\n\n\n\n**Get the Fraud and the normal dataset**"}}