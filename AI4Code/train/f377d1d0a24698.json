{"cell_type":{"6f9a5429":"code","b9fd517a":"code","83d476b0":"code","4ea20a6a":"code","ca0eaab1":"code","6d17c430":"code","a934124d":"code","c0e909ae":"code","89e4af79":"code","b1424541":"code","73a25610":"code","c1fcff81":"code","7ddad98d":"code","ad9f82ee":"code","766d4c43":"code","3c9261d2":"code","fbf72ebc":"markdown","d2f54f7a":"markdown"},"source":{"6f9a5429":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom tensorflow.keras.layers import Attention\nimport gc; import os","b9fd517a":"from transformers.modeling_bert import BertConfig, BertEncoder","83d476b0":"path = \"..\/input\/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))\nsales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))","4ea20a6a":"def scaled_dot_product_attention(q, k, v, mask):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead) \n  but it must be broadcastable for addition.\n  \n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable \n          to (..., seq_len_q, seq_len_k). Defaults to None.\n    \n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n  \n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)  \n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output, attention_weights","ca0eaab1":"class MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n    \n    assert d_model % self.num_heads == 0\n    \n    self.depth = d_model \/\/ self.num_heads\n    \n    self.wq = tf.keras.layers.Dense(d_model)\n    self.wk = tf.keras.layers.Dense(d_model)\n    self.wv = tf.keras.layers.Dense(d_model)\n    \n    self.dense = tf.keras.layers.Dense(d_model)\n        \n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n  def call(self, v, k, q, mask):\n    batch_size = tf.shape(q)[0]\n    \n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n    \n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n    \n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n    \n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        \n    return output","6d17c430":"def point_wise_feed_forward_network(d_model, dff):\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])","a934124d":"def create_padding_mask(seq):\n  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n  # add extra dimensions to add the padding\n  # to the attention logits.\n  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)","c0e909ae":"def create_look_ahead_mask(size):\n  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n  return mask  # (seq_len, seq_len)","89e4af79":"def create_masks(inp, tar):\n  # Encoder padding mask\n  enc_padding_mask = create_padding_mask(inp)\n  \n  # Used in the 2nd attention block in the decoder.\n  # This padding mask is used to mask the encoder outputs.\n  dec_padding_mask = create_padding_mask(inp)\n  \n  # Used in the 1st attention block in the decoder.\n  # It is used to pad and mask future tokens in the input received by \n  # the decoder.\n  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n  dec_target_padding_mask = create_padding_mask(tar)\n  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  \n  return enc_padding_mask, combined_mask, dec_padding_mask","b1424541":"import time\n\nclass Encoder(tf.keras.Model):\n    layer_n = 64\n    kernel_size = 7\n    depth = 2\n    \n    def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n        # x = Attention(INPUT_SHAPE)(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        \n        return x\n\n    def se_block(x_in, layer_n):\n        x = GlobalAveragePooling1D()(x_in)\n        x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n        x = Dense(layer_n, activation=\"sigmoid\")(x)\n        x_out=Multiply()([x_in, x])\n        return x_out\n\n    def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n        x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n        x = cbr(x, layer_n, kernel, 1, dilation, 64)\n        if use_se:\n            x = se_block(x, layer_n)\n        x = Add()([x_in, x])\n        x = Attention(INPUT_SHAPE)(x)\n        return x  \n    \n    def EncoderWday():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderMonth():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderYear():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderEv1():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderT1():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderEV2():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderT2():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderItemId():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderDeptId():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderStoreId():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderCatId():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def EncoderStateId():\n        layer_n = 64\n        kernel_size = 7\n        depth = 2\n        def cbr(x, out_layer, kernel, stride, dilation, INPUT_SHAPE):\n            x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n            # x = Attention(INPUT_SHAPE)(x)\n            x = BatchNormalization()(x)\n            x = Activation(\"relu\")(x)\n        \n            return x\n\n        def se_block(x_in, layer_n):\n            x = GlobalAveragePooling1D()(x_in)\n            x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n            x = Dense(layer_n, activation=\"sigmoid\")(x)\n            x_out=Multiply()([x_in, x])\n            return x_out\n    \n        def resblock(x_in, layer_n, kernel, dilation, INPUT_SHAPE, use_se=True):\n            x = cbr(x_in, layer_n, kernel, 1, dilation, 64)\n            x = cbr(x, layer_n, kernel, 1, dilation, 64)\n            if use_se:\n                x = se_block(x, layer_n)\n            x = Add()([x_in, x])\n            # x = Attention(INPUT_SHAPE)(x)\n            return x  \n        \n        input_shape = (64, 1)\n        input_layer_a = Input(shape=input_shape)\n        input_layer_1 = AveragePooling1D(5)(input_layer_a)\n    \n        x = cbr(input_layer_a, layer_n, kernel_size, 1, 1, 64)#1000\n        for i in range(depth):\n            x = resblock(x, 64, kernel_size, 1, 64)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                temp_k = tf.constant([[10,0,0],\n                      [0,10,0],\n                      [0,0,10],\n                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n\n                temp_v = tf.constant([[   1,0],\n                      [  10,0],\n                      [ 100,5],\n                      [1000,6]], dtype=tf.float32)  # (4, 2)\n\n                temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n                print(\"Bi-GRU is not none, so carrying on.\")\n                \n                bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n                bi_gru = Bidirectional(GRU(128, return_sequences=True), merge_mode='concat')(bi_lstm)\n                x = Dense(64, activation=\"relu\")(bi_gru)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                x = Attention(x)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_0 = x\n        \n        x = cbr(x, layer_n*2, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*2, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:                \n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                # x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_1 = x\n        \n\n        x = cbr(x, layer_n*3, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*3, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = resblock(x, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_2 = x\n\n        x = cbr(x, layer_n*4, kernel_size, 5, 1, 64)\n        for i in range(depth):\n            x = resblock(x, layer_n*4, kernel_size, 1, 128)\n            bi_lstm = Bidirectional(LSTM(128, return_sequences=True), merge_mode='concat')(x)\n            bi_gru = Bidirectional(LSTM(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n            if bi_gru is not None:\n                print(\"Bi-GRU is not none, so carrying on.\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n            if bi_gru is None:\n                x = Attention(x)\n                x = resblock(bi_gru, layer_n, kernel_size, 1, 64)\n                print(\"BI-GRU Error: Using resblock\")\n                x = Dense(64, activation=\"relu\")(x)\n                x = Dense(1, activation=\"sigmoid\")(x)\n        out_3 = x\n        \n        out = Activation(\"softmax\")(x)\n        model = Model(input_layer_a, out)\n    \n        return model\n    \n    def Encoder(tf.keras.Model):\n        EncoderWday()\n        EncoderMonth()","73a25610":"from sklearn.preprocessing import OrdinalEncoder\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\", \"event_name_2\", \"event_type_2\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    to_ordinal = [\"event_name_1\", \"event_type_1\"] \n    df[to_ordinal] = df[to_ordinal].fillna(\"1\")\n    df[to_ordinal] = OrdinalEncoder(dtype=\"int\").fit_transform(df[to_ordinal]) + 1\n    to_int8 = [\"wday\", \"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\"] + to_ordinal\n    df[to_int8] = df[to_int8].astype(\"int8\")\n    return df\n\ndef zapsmall(z, tol=1e-6):\n    z[abs(z) < tol] = 0\n    return z\n    \ndef prep_selling_prices(df):\n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) \/ (1 + gr.cummax() - gr.cummin())\n    df[\"sell_price_roll_sd7\"] = zapsmall(gr.transform(lambda x: x.rolling(7).std()))\n    to_float32 = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_cumrel\", \"sell_price_roll_sd7\"]\n    df[to_float32] = df[to_float32].astype(\"float32\")\n         \n    return df\n\ndef reshape_sales(df, drop_d = None):\n    df = df.drop([\"d_\" + str(i+1) for i in range(drop_d-1)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n             var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int64\"))\n\n    return df\n\ndef prep_sales(df):\n    df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    df['rolling_mean_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    df['rolling_mean_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    df['rolling_mean_t60'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(60).mean())\n    df['rolling_mean_t90'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    df['rolling_mean_t180'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    df['rolling_std_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    df['rolling_std_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n  \n    to_float32 = ['lag_t28', 'rolling_mean_t7', 'rolling_mean_t30', 'rolling_mean_t60', \n                      'rolling_mean_t90', 'rolling_mean_t180', 'rolling_std_t7', 'rolling_std_t30']\n    df[to_float32] = df[to_float32].astype(\"float32\")\n\n    # Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings\n    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_t180))]\n \n    return df","c1fcff81":"selling_prices = prep_selling_prices(selling_prices)\ncalendar = prep_calendar(calendar)\nsales = reshape_sales(sales, 1000)\nsales = prep_sales(sales)\ngc.collect()","7ddad98d":"sales = sales.fillna(0)\nsales = sales.merge(calendar, how=\"left\", on=\"d\")\ngc.collect()\ndel calendar\n    \nsales = sales.merge(selling_prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])\nsales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()\ndel selling_prices\n    \nfor i, v in enumerate([\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]):\n    sales[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales[[v]]).astype(\"int16\") + 1\ngc.collect()\n    \nx = [\"wday\", \"month\", \"year\", \n     \"event_name_1\", \"event_type_1\",\n     \"snap_CA\", \"snap_TX\", \"snap_WI\",\n     \"sell_price\", \"sell_price_rel_diff\", \"sell_price_cumrel\", \"sell_price_roll_sd7\",\n     \"lag_t28\", \"rolling_mean_t7\", \"rolling_mean_t30\", \"rolling_mean_t60\", \n     \"rolling_mean_t90\", \"rolling_mean_t180\", \"rolling_std_t7\", \"rolling_std_t30\",\n     \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    \ntest = sales[sales.d >= 1914]\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                       F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))","ad9f82ee":"def Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n    x=[]\n    y=[]\n  \n    count=0\n    idx_1 = np.arange(len(input_dataset))\n    np.random.shuffle(idx_1)\n\n    while True:\n        for i in range(len(input_dataset)):\n            input_data = input_dataset[idx_1[i]]\n            target_data = target_dataset[idx_1[i]]\n                \n            x.append(input_data)\n            y.append(target_data)\n            count+=1\n            if count==batch_size:\n                x=np.array(x, dtype=np.float32)\n                y=np.array(y, dtype=np.float32)\n                inputs = x\n                targets = y       \n                x = []\n                y = []\n                count=0\n                yield inputs, targets","766d4c43":"def model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n    hist = model.fit_generator(\n        Datagen(train_inputs, train_targets, batch_size, is_train=True),\n        steps_per_epoch = len(train_inputs) \/\/ batch_size,\n        epochs = n_epoch,\n        validation_data=Datagen(val_inputs, val_targets, batch_size),\n        validation_steps = len(val_inputs) \/\/ batch_size,\n        callbacks = [lr_schedule, macroF1(model, val_inputs, val_targets)],\n        shuffle = False,\n        verbose = 1\n        )\n    return hist\n\n\ndef lrs(epoch):\n    if epoch<35:\n        lr = learning_rate\n    elif epoch<50:\n        lr = learning_rate\/10\n    else:\n        lr = learning_rate\/100\n    return lr\n","3c9261d2":"from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\n\nclass Encoder_H1():\n    learning_rate=0.0005\n    n_epoch=60\n    batch_size=32\n    \n    start_time = time.time()\n    model1 = Encoder()\n    elapsed = time.time() - start_time\n    print(f\"{elapsed} time has passed from encoders 1-12\")","fbf72ebc":"## Neural Network","d2f54f7a":"Function to reduce mem. is taken from @ragnar's kernel."}}