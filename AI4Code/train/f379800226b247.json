{"cell_type":{"a4990322":"code","2e10a1c5":"code","8d092ce6":"code","569c5239":"code","3192828d":"code","8e23370b":"code","29b42641":"code","5948deea":"code","cb9e50bc":"code","a6c61838":"code","45479363":"code","fd93d4d8":"code","91774677":"code","dad3287f":"code","8888d2fc":"code","158c0913":"markdown","c7be0644":"markdown","f8f85313":"markdown","18b217a8":"markdown","faa1262b":"markdown","c0bb9dfa":"markdown"},"source":{"a4990322":"# word-level one-hot encoding\n\nfrom keras.preprocessing.text import Tokenizer \n\nsamples = [\"I love learning deep learning.\",\n           \"Machine learning is the future of humanity.\"]\n\ntokenizer = Tokenizer(num_words=1000) # only tokenize 1000 common words\ntokenizer.fit_on_texts(samples)\n\nsequences = tokenizer.texts_to_sequences(samples)\n\none_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n\nword_index = tokenizer.word_index\nprint(\"Found %s unique tokens.\" % len(word_index))","2e10a1c5":"from keras.layers import Embedding\n\nembedding_layer = Embedding(1000, 64) \n# max 1000 tokens\/sequences, 64 dimensions\/length","8d092ce6":"# IMDB movie-review sentiment-prediction\n\nfrom keras.datasets import imdb\nfrom keras import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense\n\nmax_features = 10000\nmaxlen = 20\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)","569c5239":"model = Sequential()\n\nmodel.add(Embedding(10000, 8, input_length=maxlen))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.summary()","3192828d":"model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n\nhistory = model.fit(x_train, y_train,\n                   epochs=10,\n                   batch_size=32,\n                   validation_split=0.2)","8e23370b":"# RNN in Keras\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN, Dense\n\nmodel = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(SimpleRNN(32, return_sequences=True))\n\nmodel.summary()","29b42641":"from keras.datasets import imdb\nfrom keras.preprocessing import sequence\n\nmax_features = 10000\nmaxlen = 500\nbatch_size = 32\n\nprint(\"Loading data...\")\n(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(input_train), \"train sequences\")\nprint(len(input_test), \"test sequences\")\n\n\nprint(\"Pad sequences (samples x time)\")\ninput_train = sequence.pad_sequences(input_train, maxlen=maxlen)\ninput_test = sequence.pad_sequences(input_test, maxlen=maxlen)\nprint(\"input_train_shape: \", input_train.shape)\nprint(\"input_test_shape: \", input_test.shape)","5948deea":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n\nhistory = model.fit(input_train, y_train,\n                   epochs = 10,\n                   batch_size = 128,\n                   validation_split = 0.2)","cb9e50bc":"import matplotlib.pyplot as plt\n\nacc = history.history[\"acc\"]\nval_acc = history.history[\"val_acc\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, \"b\")\nplt.plot(epochs, val_acc, \"bo\")\nplt.title(\"Training and validation accuracy.\")\nplt.show()\n\nplt.plot(epochs, loss, \"r\")\nplt.plot(epochs, val_loss, \"ro\")\nplt.title(\"Training and validation loss.\")\nplt.show()","a6c61838":"from keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n\nhistory = model.fit(input_train, y_train,\n                   epochs=10,\n                   batch_size=128,\n                   validation_split=0.2)","45479363":"fname = \"..\/input\/weather-archive-jena\/jena_climate_2009_2016.csv\"\nf = open(fname)\n\ndata = f.read()\nf.close()\n\nlines = data.split(\"\\n\")\nheader = lines[0].split(\",\")\nlines = lines[1:]\n\nprint(header)\nprint(len(lines))","fd93d4d8":"# convert into a NumPy array\n\nimport numpy as np\n\nfloat_data = np.zeros((len(lines), len(header) - 1))\nfor i, line in enumerate(lines):\n    values = [float(x) for x in line.split(\",\")[1:]]\n    float_data[i, :] = values","91774677":"temp = float_data[:, 1]\nplt.plot(range(len(temp)), temp)","dad3287f":"plt.plot(range(1440), temp[:1440])","8888d2fc":"# to be updated","158c0913":"# ## Deep Learning for Text and Sequence Notes\n\n*This is my notebook to record important terms, codes, and concepts of applying deep learning to work with text data.*\n\n**Reference**: *Francois, C. (2017). Deep learning with Python.*\n\n#### The fundamental algorithms: \n- RNN\n- Conv1D\n\nVectorizing text is the process of transform text into numeric tensors.\n\nTokens: words, characters, n-grams: are the units that text is broken down to.\n\nTokenization: breaking text into tokens.\n\n#### Methods of associating a vector with a token:\n- One-hot encoding\n- Word embedding\n\nN-grams: groups of N consecutive words extracted from a sentence. Extracting N-grams is a form of feature engineering. Useful for shallow language-processing rather than in deep learning.\n\n#### One-hot encoding:\n\nThe vector is all zeros, except the n-th entry (encoded as 1).\n\nOne-hot hashing trick: used when the number of unique tokens in the vocabulary is too large to handle explicitly.\n\n- Binary\n- Sparse\n- High-dimensional","c7be0644":"### LSTM and GRU layers \n\nSimpleRNN is too simple. Vanishing gradients problem.\n\nLSTM and GRU can solve the problem.\n\nLSTM saves information for later (allows past information to be reinjected at a later time), thus preventing older signals from gradually vanishing.\n\nLSTM solves difficult NLP problems: Q&A, machine translation.","f8f85313":"#### Using GloVe embeddings (https:\/\/nlp.stanford.edu\/projects\/glove)","18b217a8":"### Recurrent Neural Networks\n\nDensely connected networks, convnet = feedforward networks.\n\nRNN:\n- Processes information incrementally while maintaining an internal model of what it's processing.\n- Built from past infor and constantly updated as new infor comes in.","faa1262b":"#### Word embeddings\n\n- Low-dimensional floating-point vectors (dense vectors)\n- Learned from data.\n- Pack more information into far fewer dimensions.\n\nWords meaning different things are embedded at points far away from each other.\n\nThe Embedding layer ~ a dictionary that maps integer indices to dense vectors.\n\nAll sequences in a batch must have the same length. Shorter sequence is **padded with zeros**, longer sequence is **truncated**.","c0bb9dfa":"Recurrent dropout: prevents overfitting in recurrent layers.\n\nStacking recurrent layers; increases the representational power of the network.\n\nBidirectional recurrent layers: increases accuracy and mitigates forgetting issues."}}