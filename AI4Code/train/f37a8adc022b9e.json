{"cell_type":{"391a54e9":"code","a6cd3025":"code","92cfeefa":"code","78041e13":"code","62a25be2":"code","482bc953":"code","976f492f":"code","d1edcb10":"code","c910d9e3":"code","59ee8a1f":"code","5c1f9f97":"code","b569265f":"code","71cccf19":"code","4c7ceb3f":"code","b92df508":"code","e9a05a3c":"code","ba1e87e5":"code","a9020ee1":"code","d5470699":"code","aedd75d7":"code","f6553ca7":"code","512379df":"code","d297b26e":"code","7de04a19":"code","b99976ca":"code","863e78a6":"code","0364a535":"code","4339350a":"code","18b1abc4":"code","e87d58e7":"code","6f5943d4":"code","efc7f85e":"code","2a185a01":"code","ebc3cf05":"code","ca091f6e":"code","e67edc5a":"code","06736136":"code","c461d314":"code","eb547a38":"code","8bd412dc":"code","4cf2cee8":"code","b9f01c5e":"code","842393bf":"code","60d78273":"code","e4335096":"code","920de166":"code","5a03e70d":"code","214ae800":"code","0b930a19":"code","483774d7":"code","3d3f8633":"code","2eb8fddc":"markdown","22884288":"markdown","ef2472d8":"markdown","cdf13f9b":"markdown","10164bbd":"markdown","bb7f3f09":"markdown","aeb6d87d":"markdown","87e2b4e3":"markdown","740b171a":"markdown","7187ab83":"markdown","060ce396":"markdown","cf0eb86e":"markdown","d9aa6870":"markdown","9b676276":"markdown","1b81544f":"markdown","057dd9c7":"markdown","90d985dd":"markdown","5b3d1fcc":"markdown","85ad0ef6":"markdown","b973985f":"markdown","07a0d67b":"markdown","7c7b38ae":"markdown","e4c217ec":"markdown","819eefd8":"markdown","f172ab24":"markdown","22793609":"markdown","7061cd24":"markdown","ba324a75":"markdown","f4b116bb":"markdown","831dd65c":"markdown","a7e93574":"markdown","fb7cd103":"markdown","281a67ea":"markdown"},"source":{"391a54e9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a6cd3025":"a = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nb = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nprint('The shape of the training set:', a.shape[0], 'houses and', a.shape[1], 'features.')\nprint('The shape of the testing set:', b.shape[0], 'houses and', b.shape[1], 'features.')\nprint('The testing set has 1 feature less than the training set which is the target (SalePrice) to predict.')","92cfeefa":"a.head()","78041e13":"num = a.select_dtypes(exclude='object')\nnumcorr = num[num.columns.difference(['Id'])].corr()\nf, ax = plt.subplots(figsize=(17,1))\nsns.heatmap(numcorr.sort_values(by=['SalePrice'], ascending=False).head(1), cmap='Blues')\nplt.title(\"Numerical feature correlations with the SalePrice\", weight='bold', fontsize=18)\nplt.show()","62a25be2":"def stylerfunc(df): \n    return df.style.background_gradient(cmap=sns.light_palette(\"cyan\", as_cmap=True))","482bc953":"stylerfunc(numcorr['SalePrice'].sort_values(ascending=False).head(10).to_frame().T)","976f492f":"def mvrateplot(df, title):\n    df = df.drop(df[df == 0].index).sort_values(ascending=True)\n    plt.figure(figsize=(10, 6))\n    df.plot.barh(color='purple')\n    plt.title(title, fontsize=20, weight='bold' )\n    plt.show()","d1edcb10":"mvrateplot(a.isnull().mean(),'Missing value average per feature: Train set')","c910d9e3":"mvrateplot(b.isnull().mean(),'Missing value average per feature: Test set')","59ee8a1f":"trainsize = a.shape[0] # the length of the train raw data set\ntestsize = b.shape[0]  # the length of the test raw data set\ny_train = a['SalePrice'].to_frame()\n#Combine train and test sets\nc = pd.concat((a, b), sort=False).reset_index(drop=True)\nc.name = 'c'\n#Drop the target \"SalePrice\" and Id columns\nc.drop(['SalePrice'], axis=1, inplace=True)\nc.drop(['Id'], axis=1, inplace=True)\nprint(f\"Total size of the combined dataframe {c.name} is:\", c.shape)\noldlen = c.shape[1]\noldset = set(c)","5c1f9f97":"c = c.dropna(thresh = len(c)*0.9, axis=1)\nc.name = 'c'\nprint(oldlen-c.shape[1], 'features with each more than 90% of missing values are dropped from the combined dataset.')\nprint('The dropped features are:', list(oldset-set(c)))\nprint(f\"--> Total size of the combined dataset {c.name} after dropping features with more than 90% M.V.:\",c.shape)","b569265f":"mvrateplot((c.isnull().sum()\/len(c)), 'Missing value average per feature')","71cccf19":"allna = (c.isnull().sum()\/len(c))\nallna = allna.drop(allna[allna == 0].index).sort_values(ascending=False)\n\nNA = c[allna.index]\nNAcat = NA.select_dtypes(include='object')\nNAnum = NA.select_dtypes(exclude='object')\nprint(f'There are {NAcat.shape[1]} categorical features with missing values')\nprint(f'There are {NAnum.shape[1]} numerical features with missing values')","4c7ceb3f":"NAnum.head()","b92df508":"c['MasVnrArea'] = c.MasVnrArea.fillna(0)\n\nc['GarageYrBlt'] = c[\"GarageYrBlt\"].fillna(c[\"GarageYrBlt\"].median())","e9a05a3c":"NAcat.head()","ba1e87e5":"stylerfunc(NAcat.isnull().sum().head(18).to_frame().sort_values(by=[0]).T)","a9020ee1":"mvthreshold = 4\n\nprint(f'The categorical features having a number of M.V. less or equal to {mvthreshold}:\\n', \n      list(NAcat.columns[NAcat.isnull().sum()<=mvthreshold]))","d5470699":"fewMVNAcat = list(NAcat.columns[NAcat.isnull().sum()<=mvthreshold])\nfor f in fewMVNAcat:\n    c[f] = c[f].fillna(method='ffill')","aedd75d7":"stylerfunc(c[fewMVNAcat].isnull().sum().to_frame().sort_values(by=[0]).T)","f6553ca7":"mvrateplot((NAcat.isnull().sum()\/len(NAcat)), 'Missing value average per categorical feature')","512379df":"mvrateplot((NAnum.isnull().sum()\/len(NAnum)), 'Missing value average per numerical feature')","d297b26e":"for col in NA.columns:\n    if c[col].dtype == \"object\":\n        c[col] = c[col].fillna(\"None\")\n    else:\n        c[col] = c[col].fillna(0)","7de04a19":"c.isnull().sum().sort_values(ascending=False).head()","b99976ca":"c['TotalArea'] = c['TotalBsmtSF'] + c['1stFlrSF'] + c['2ndFlrSF'] + c['GrLivArea'] + c['GarageArea']\n\nc['Bathrooms'] = c['FullBath'] + c['HalfBath']*0.5 \n\nc['YearAverage'] = (c['YearRemodAdd'] + c['YearBuilt'])\/2","863e78a6":"cb = pd.get_dummies(c) \nprint(\"The shape of the original dataset:\", c.shape)\nprint(\"The shape of the encoded dataset:\", cb.shape)\nprint(f'--> {cb.shape[1] - c.shape[1]} encoded features are added to the combined dataset.')","0364a535":"Train = cb[:trainsize]  #trainsize is the number of rows of the original training set\nTest = cb[trainsize:] \nprint(\"The shape of train dataset:\", Train.shape)\nprint(\"The shape of test dataset:\", Test.shape)","4339350a":"fig = plt.figure(figsize=(15,10))\nax1 = plt.subplot2grid((2,2),(0,0))\nplt.scatter(x=a['GrLivArea'], y=a['SalePrice'], color=('yellowgreen'))\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Ground living Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((2,2),(0,1))\nplt.scatter(x=a['TotalBsmtSF'], y=a['SalePrice'], color=('red'))\nplt.axvline(x=5900, color='r', linestyle='-')\nplt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((2,2),(1,0))\nplt.scatter(x=a['1stFlrSF'], y=a['SalePrice'], color=('deepskyblue'))\nplt.axvline(x=4000, color='r', linestyle='-')\nplt.title('First floor Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((2,2),(1,1))\nplt.scatter(x=a['MasVnrArea'], y=a['SalePrice'], color=('gold'))\nplt.axvline(x=1500, color='r', linestyle='-')\nplt.title('Masonry veneer Area - Price scatter plot', fontsize=15, weight='bold' )\nplt.show()","18b1abc4":"print(a['GrLivArea'].sort_values(ascending=False).head(2))\nprint('*'*30)\nprint(a['TotalBsmtSF'].sort_values(ascending=False).head(1))\nprint('*'*30)\nprint(a['MasVnrArea'].sort_values(ascending=False).head(1))\nprint('*'*30)\nprint(a['1stFlrSF'].sort_values(ascending=False).head(1))","e87d58e7":"train = Train[(Train['GrLivArea'] < 4600) & (Train['MasVnrArea'] < 1500)]\n\nprint(f'--> {Train.shape[0]-train.shape[0]} outliers are removed from the train dataset.')","6f5943d4":"target = a[['SalePrice']]\noutliers = [1298, 523, 297]\ntarget.drop(target.index[outliers], inplace=True)","efc7f85e":"print('Make sure that both train and target sets have the same row number after removing the outliers:')\nprint('Train:', train.shape[0], 'rows')\nprint('Target:', target.shape[0], 'rows')","2a185a01":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nplt.scatter(x=a['GrLivArea'], y=a['SalePrice'], color=('yellowgreen'), alpha=0.7)\nplt.title('Area-Price plot with outliers',weight='bold', fontsize=18)\nplt.axvline(x=4600, color='r', linestyle='-')\n#first row, second col\nax1 = plt.subplot2grid((1,2),(0,1))\nplt.scatter(x=train['GrLivArea'], y=target['SalePrice'], color='navy', alpha=0.7)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Area-Price plot without outliers',weight='bold', fontsize=18)\nplt.show()","ebc3cf05":"target[\"SalePrice\"] = np.log1p(target[\"SalePrice\"])","ca091f6e":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(8,12))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((2,1),(0,0))\nsns.distplot(np.expm1(target[\"SalePrice\"]), color='plum')\nplt.title('Before: Distribution of SalePrice',weight='bold', fontsize=18)\n#first row, second col\nax1 = plt.subplot2grid((2,1),(1,0))\nsns.distplot(target[\"SalePrice\"], color='tan')\nplt.title('After: Distribution of SalePrice',weight='bold', fontsize=18)\nplt.show()","e67edc5a":"from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingRegressor\nimport math\nimport time","06736136":"print(train.shape)\nprint(target.shape)\nprint(Test.shape)","c461d314":"x = train\ny = np.array(target)","eb547a38":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .33, random_state=0)","8bd412dc":"scaler = RobustScaler()\n# transform \"x_train\"\nx_train = scaler.fit_transform(x_train)\n# transform \"x_test\"\nx_test = scaler.transform(x_test)\n# transform \"x\"\nx = scaler.transform(x)\n# transform the test set\nX_test = scaler.transform(Test)","4cf2cee8":"def GridSearchCVmodels(modellist, x_train, y_train, x_test, y_test, x, y, cv): \n    results = {}\n    bestregressors = {}\n    for regmodel in modellist:\n        vars()[\"grid_%s\"%regmodel.__name__] = GridSearchCV(estimator=regmodel(), \n                                                           param_grid=param_grid.get(regmodel.__name__),\n                                                           cv=cv, scoring='neg_mean_squared_error', verbose=0, \n                                                           n_jobs=-1, return_train_score=True, iid=False).fit(x_train, y_train.ravel())\n        print(f'Regressor {modellist.index(regmodel)+1}\/{len(modellist)}:')\n        print(\"-> Best Mean RMSE: %f using %s with %s\" % (-vars()[\"grid_%s\"%regmodel.__name__].best_score_, \n                                                          regmodel.__name__, \n                                                          vars()[\"grid_%s\"%regmodel.__name__].best_params_))\n\n        results[regmodel.__name__] = [-max(vars()[\"grid_%s\"%regmodel.__name__].cv_results_['mean_train_score']),\n                                      -max(vars()[\"grid_%s\"%regmodel.__name__].cv_results_['mean_test_score']),\n                                      np.sqrt(mean_squared_error(vars()[\"grid_%s\"%regmodel.__name__].best_estimator_.predict(x_train), y_train)),\n                                      np.sqrt(mean_squared_error(vars()[\"grid_%s\"%regmodel.__name__].best_estimator_.predict(x_test), y_test)), \n                                      np.sqrt(mean_squared_error(vars()[\"grid_%s\"%regmodel.__name__].best_estimator_.predict(x), y)), \n                                     ]\n        \n        bestregressors[regmodel.__name__] = vars()[\"grid_%s\"%regmodel.__name__].best_estimator_\n    \n    return results, bestregressors","b9f01c5e":"param_grid_Ridge = {'alpha': np.arange(10.4, 10.6, 0.01), \n                    'fit_intercept': [True, False],\n                   }\n\nparam_grid_Lasso = {'alpha': np.arange(4.2, 4.4, 0.01)*1e-4,\n                    'fit_intercept': [True, False],\n                   }\n\nparam_grid_ElasticNet = {'random_state': [1],\n                         'alpha': np.arange(34, 35, 0.1)*1e-04,\n                         'l1_ratio': np.arange(84, 85, 0.1)*1e-03,\n                         'fit_intercept': [True, False], \n                        }\n\nparam_grid_XGBRegressor = {'objective': ['reg:squarederror'],\n                           'gamma': [0], #np.arange(0, 10, 1), \n                           'n_estimators': [4110], #np.arange(4000, 4500, 100),  \n                           'learning_rate': [0.01],\n                           'seed': [27],\n                          }\n\nparam_grid = {\"Ridge\": param_grid_Ridge,\n              \"Lasso\": param_grid_Lasso,\n              \"ElasticNet\": param_grid_ElasticNet,\n              \"XGBRegressor\": param_grid_XGBRegressor,\n              }\n\nmodellist = [Ridge, Lasso, ElasticNet, XGBRegressor]\n\nresults, bestregressors = GridSearchCVmodels(modellist, x_train, y_train, x_test, y_test, x, y, cv=5)\n\npd.DataFrame(results, index=['SubSubTrainData', 'SubSubTestData', 'SubTrainData', 'SubTestData', 'AllTrainData']).transpose()","842393bf":"def VotingRegressorCV(estimators_list, vote_params, x_train, y_train, x_test, y_test, x, y, cv):\n    estimator=VotingRegressor(estimators_list)\n    grid = GridSearchCV(estimator,\n                        param_grid=vote_params, \n                        cv=cv, scoring='neg_mean_squared_error', \n                        verbose=0, n_jobs=-1, return_train_score=True, iid=False).fit(x_train, y_train.ravel())\n\n    print(\"-> Best Mean RMSE: %f using %s with %s\" % (-grid.best_score_, \n                                                      type(estimator).__name__, \n                                                      grid.best_params_))\n    \n    results['%s'%type(estimator).__name__] = [-max(grid.cv_results_['mean_train_score']), \n                                              -max(grid.cv_results_['mean_test_score']), \n                                              np.sqrt(mean_squared_error(grid.best_estimator_.predict(x_train), y_train)), \n                                              np.sqrt(mean_squared_error(grid.best_estimator_.predict(x_test), y_test)), \n                                              np.sqrt(mean_squared_error(grid.best_estimator_.predict(x), y)), \n                                             ]\n    return grid, results","60d78273":"estimators_list = [('Ridge', bestregressors['Ridge']),  \n                   ('Lasso', bestregressors['Lasso']), \n                   ('ElasticNet', bestregressors['ElasticNet']), \n                   ('XGBRegressor', bestregressors['XGBRegressor']), \n                  ]\n\n#Set parameters for GridSearch\nvote_params = {'weights': [(1, 1, 1, 1),\n                           #(0, 1, 1, 1), \n                           #(1, 0, 1, 1), \n                           #(1, 1, 0, 1),\n                           #(1, 1, 1, 0),\n                          ], \n              }\n\ngrid_VotingRegressor, results = VotingRegressorCV(estimators_list, vote_params, x_train, y_train, x_test, y_test, x, y, cv=5)\n\npd.DataFrame(results, index=['SubSubTrainData', 'SubSubTestData', 'SubTrainData', 'SubTestData', 'AllTrainData']).transpose()","e4335096":"def StackingRegressorCV(regressors, meta_regressor, params, x_train, y_train, x_test, y_test, x, y, cv):\n    stackreg = StackingRegressor(regressors=regressors, meta_regressor=meta_regressor, use_features_in_secondary=True)\n    \n    grid = GridSearchCV(estimator=stackreg,\n                        param_grid=params, \n                        cv=cv, scoring='neg_mean_squared_error', \n                        verbose=0, n_jobs=-1, return_train_score=True, iid=False).fit(x_train, y_train.ravel())\n\n    print(\"-> Best Mean RMSE: %f using Stack_%s with %s\" % (-grid.best_score_, \n                                                         type(meta_regressor).__name__, \n                                                         grid.best_params_))\n    \n    results['Stack_%s'%type(meta_regressor).__name__] = [-max(grid.cv_results_['mean_train_score']), \n                                                         -max(grid.cv_results_['mean_test_score']), \n                                                         np.sqrt(mean_squared_error(grid.best_estimator_.predict(x_train), y_train)), \n                                                         np.sqrt(mean_squared_error(grid.best_estimator_.predict(x_test), y_test)), \n                                                         np.sqrt(mean_squared_error(grid.best_estimator_.predict(x), y)), \n                                                        ]\n    return grid, results","920de166":"params = {'meta_regressor__alpha': [63.8], #np.arange(63.8, 63.9, 0.01)*1e-0, \n          'meta_regressor__fit_intercept': [False], #[True, False], \n         } \n          \ngrid_Stack_Ridge, results = StackingRegressorCV(regressors = [bestregressors['Lasso'], \n                                                              bestregressors['ElasticNet'], \n                                                              grid_VotingRegressor.best_estimator_, \n                                                             ], \n                                                meta_regressor = Ridge(), \n                                                params = params, x_train = x_train, \n                                                y_train = y_train, x_test = x_test, \n                                                y_test = y_test, x = x, y = y, cv=5) \n          \npd.DataFrame(results, index=['SubSubTrainData', 'SubSubTestData', 'SubTrainData', 'SubTestData', 'AllTrainData']).transpose()","5a03e70d":"params = {'meta_regressor__random_state': [1],\n          'meta_regressor__alpha': [0.0034], #np.arange(34, 35, 0.1)*1e-04,\n          'meta_regressor__l1_ratio': [0.084], #np.arange(84, 85, 0.1)*1e-03,\n          'meta_regressor__fit_intercept': [True, False],\n         } \n\ngrid_Stack_ElasticNet, results = StackingRegressorCV(regressors = [bestregressors['Ridge'], \n                                                                   bestregressors['Lasso'],  \n                                                                   grid_VotingRegressor.best_estimator_, \n                                                                  ], \n                                                     meta_regressor = ElasticNet(), \n                                                     params = params, x_train = x_train, \n                                                     y_train = y_train, x_test = x_test, \n                                                     y_test = y_test, x = x, y = y, cv=5) \n          \npd.DataFrame(results, index=['SubSubTrainData', 'SubSubTestData', 'SubTrainData', 'SubTestData', 'AllTrainData']).transpose()","214ae800":"params = {'meta_regressor__seed': [27],\n          'meta_regressor__objective': ['reg:squarederror'],\n          'meta_regressor__gamma': [0], #np.arange(8.7, 9.2, 0.1)*1e-04,\n          'meta_regressor__n_estimators': [2500], #np.arange(72, 93, 5),\n         }\n\ngrid_Stack_XGBRegressor, results = StackingRegressorCV(regressors = [bestregressors['Ridge'], \n                                                                     bestregressors['Lasso'], \n                                                                     bestregressors['ElasticNet'], \n                                                                     grid_VotingRegressor.best_estimator_, \n                                                                    ], \n                                                       meta_regressor = XGBRegressor(), \n                                                       params = params, x_train = x_train, \n                                                       y_train = y_train, x_test = x_test, \n                                                       y_test = y_test, x = x, y = y, cv=5)\n\npd.DataFrame(results, index=['SubSubTrainData', 'SubSubTestData', 'SubTrainData', 'SubTestData', 'AllTrainData']).transpose()","0b930a19":"df_results = pd.DataFrame(results, index=['SubSubTrainData', 'SubSubTestData', 'SubTrainData', \n                                          'SubTestData', 'AllTrainData']).transpose()\ndf_results.plot.barh(rot=0)\nplt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\nplt.title(\"RMSE score average over datasets\", weight='bold', fontsize=15)\nplt.show()","483774d7":"def blend(df):\n    blenddf = pd.DataFrame({#'a': bestregressors['Lasso'].predict(df),\n                            'b': grid_VotingRegressor.best_estimator_.predict(df),\n                            'c': grid_Stack_XGBRegressor.best_estimator_.predict(df), \n                           })\n    return blenddf\n\nlinregtest = LinearRegression(fit_intercept='False', normalize='False').fit(blend(x_test), y_test)\nprint(linregtest.coef_)\nprint(np.sqrt(mean_squared_error(linregtest.predict(blend(x_test)), y_test)))","3d3f8633":"#Submission of the results predicted by the average of Lasso\/Voting\/Stacking\ndft = blend(X_test)\n\nfinalb1 = (np.expm1(dft)*(linregtest.coef_\/linregtest.coef_.sum())).sum(axis=1)\nfinalb2 = np.expm1(linregtest.predict(blend(X_test)).ravel())\n\nfinalb = linregtest.predict(np.expm1(blend(X_test))).ravel()\n\nsubmission = pd.DataFrame({\"Id\": b[\"Id\"], \n                           \"SalePrice\": finalb, \n                          })\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","2eb8fddc":"> ### Log tranform the target because it's skewed to the right\n\nWe will apply **np.log1p** to the skewed features. (To get the original values back, we will apply **np.expm1** at the end of the study after training and testing the models. ","22884288":"Checking if the dataframe ``c`` contains yet missing values:","ef2472d8":"# <a id='4'>4. Encoding Categorical Features<\/a>","cdf13f9b":"We isolate the missing values from the rest of the dataset to have a good idea of how to treat them. \n\nLet's create a NA dataframe composed of only features with M.V. Then, we split them to:\n* Categorical features\n* Numerical features","10164bbd":"As we can see, there are 18 categorical features and 10 numerical features to clean.\n\nWe start with the numerical features. First thing to do is to have a look at them to learn more about their distribution and decide how to clean them:\n- Most of the features are going to be filled with 0s because we assume that they don't exist, for example GarageArea, GarageCars with missing values are simply because the house lacks a garage.\n- GarageYrBlt: Year garage was built can't be filled with 0s, so we fill with the median (1979).","bb7f3f09":"Concerning the 18 Categorical features with missing values:\n* Some features have just 1 or 2 missing values, so we will just use the forward fill method because they are obviously values that can't be filled with 'None's\n* Features with many missing values are mostly basement and garage related (same as in numerical features) so as we did with numerical features (filling them with 0s), we will fill the categorical missing values with 'None's assuming that the houses lack basements and garages.","aeb6d87d":"Now for the rest of features, we will just fill 0s in the numerical features and 'None' in categorical features, assuming that the houses don't have basements, full bathrooms or garage.","87e2b4e3":"Here is a glimpse of what we will be dealing with:\n* Many features, \n* Many missing values, \n* One target feature: ``SalePrice`` which is the price of the houses we are supposed to predict.","740b171a":"![](https:\/\/www.reno.gov\/Home\/ShowImage?id=7739&t=635620964226970000)\n\n**Competition Description from Kaggle**  \nWith 80 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\nSince the aim is to predict the sale price of residential homes, we will practice feature engineering and regression algorithms to achieve the lowest prediction error (RMSE is the metric used in this competition).\n\nIf there are any recommendations\/changes you would like to see in this notebook, please leave a comment at the end of this kernel. Any feedback\/constructive criticism would be genuinely appreciated. If you like this notebook or find this notebook helpful, Please feel free to UPVOTE and\/or leave a comment.\nThis notebook is always a work in progress. So, please stay tuned for more to come.\n\nThis work is inspired by [Amin's analysis](https:\/\/www.kaggle.com\/amiiiney\/price-prediction-top-15-regularization-stacking) that I have to thank him very much.\n\n- <a href='#1'>1. Exploratory Data Analysis<\/a>\n- <a href='#2'>2. Data Cleaning<\/a>\n- <a href='#3'>3. Feature Engineering<\/a>\n- <a href='#4'>4. Encoding Categorical Features<\/a>\n- <a href='#5'>5. Detecting Outliers<\/a>\n- <a href='#6'>6. Building Machine Learning Models<\/a>","7187ab83":"# <a id='1'>1. Exploratory Data Analysis<\/a>","060ce396":"\nIf you like to discuss any other projects or just have a chat about data science topics, I'll be more than happy to connect with you on:\n\nLinkedIn: https:\/\/www.linkedin.com\/in\/baligh-mnassri\/\n\nThis kernel will always be a work in progress. If you have any idea\/suggestions about this notebook, please let me know. Any feedback about further improvements would be genuinely appreciated.\n\nIf this notebook helped you in any way or you liked it, please upvote and\/or leave a comment!! :)","cf0eb86e":"Let's display the number of missing values per categorical feature:","d9aa6870":"We are done with the cleaning, normalization and feature engineering. Now, we split the combined dataset to the original train and test sets.","9b676276":"* ``MasVnrArea``: (Masonry veneer area) is in square feet, the missing data means no veneer so we fill with 0.\n* ``GarageYrBlt`` (Year garage was built), we fill the gaps with the median: 1979.\n* For the rest of the columns: Bathroom, half bathroom, basement related columns and garage related columns, we will fill with 0's because they just mean that the hosue doesn't have a basement, bathrooms or a garage.","1b81544f":"The outliers are the points in the right that have a larger area but a very low sale price. We localize those points by sorting their respective columns\n\n* The outlier in \"basement\" and \"first floor\" features is the same as the first outlier in ground living area: **the outlier with index number 1298. **\n* We detect the outlier 297 in MasVnrArea.","057dd9c7":"> Most of the features are clean from missing values\n\n**We combine first the train and test datasets to run all the data munging and applying feature engineering on both of them.**","90d985dd":"We do with the same thing with ``SalePrice`` feature, we localize those rows with these index (297, 523, 1298) and make sure they are the right outliers to remove. ","5b3d1fcc":"Before cleaning the data, we zoom at the features with missing values which won't be treated iqually. Some features have barely 1 or 2 missing values, we will use the forward fill method to fill them.","85ad0ef6":"Let's check if these categorical features do not yet have M.V.","b973985f":"The **overall quality**, **the living area, basement area, garage cars and garage area** have the highest correlation values with the sale price, which is logical, better quality and bigger area = Higher price.\n* Also some features such as, **full bath** or **1st floor surface** have a higher correlation, those are luxury features, more luxury = Higher price.\n* and **Year built**, the newer buildings seem to have higher sale prices.\n\nLet's dig in more into the data, those are just the numerical features. I assume that categorical features will be very important, for example, the neighborhood feature will be important, maybe the most important, given that good locations nowadays cost good money.\n\nBut before going any further, we start by cleaning the data from missing values. For this, let's investigate and identify features with missing values.","07a0d67b":"# <a id='5'>5. Detecting Outliers<\/a>\n\nThis part of the kernel will be a little bit messy. I didn't want to deal with the outliers in the combined dataset to keep the shape of the original train and test datasets. Dropping them would shift the location of the rows.\n\nSo we go back to our original train dataset to visualize the important features \/ Sale price scatter plots to find outliers","7c7b38ae":"We finally end up with a clean dataset, next thing to do is create new features.\n\n# <a id='3'>3. Feature Engineering<\/a>\n\nSince the area is a very important variable, we will create a new feature \"**TotalArea**\" that sums the area of all the floors and the basement.\n* **Bathrooms**: All the bathroom in the ground floor\n* **Year average**: The average of the sum of the year in which the house was built and the year the house was remodeled.","e4c217ec":"* We already dealt with small missing values or values that can't be filled with \"0\" such as Garage year built.\n* The rest of features are mostly basement and garage related with 100s of missing values, we will just fill 0s in the numerical features and 'None' in categorical features, assuming that the houses don't have basements, full bathrooms or garage.","819eefd8":"We start with categorical features having just few missing values, as example the number of M.V. less than 5. \n\nLest's locate these features and then we fill the gap with forward fill method:","f172ab24":"Just by looking at the heatmap above we can see many dark colors. Indeed, many features have high correlation with the target.\n\nTo have a better idea, we sort the features according to their correlations with the ``SalePrice``.","22793609":"## Preprocessing\nWe start machine learning by setting the features and target:\n* Features: x\n* Target: y","7061cd24":"Feature engineering is very important to improve the model's performance, I will start in this kernel just with the TotalArea, Bathrooms and average year features and will keep updating the kernel by creating new features.","ba324a75":"# <a id='6'>6. Building Machine Learning Models<\/a>","f4b116bb":"# <a id='2'>2. Data Cleaning<\/a>\n\n> First thing to do is get rid of the features with more than 90% missing values. For example the PoolQC's missing values are probably due to the lack of pools in some buildings, which is very logical. But replacing those (more than 90%) missing values with \"no pool\" will leave us with a feature with low variance, and low variance features are uniformative for machine learning models. So we drop the features with more than 90% missing values.","831dd65c":"Let's have a look first at the correlation between numerical features and the target ``SalePrice``, in order to have a first idea of the connections between features.\n\n*It is not necessary to include the ``Id`` feature into the analysis!*","a7e93574":"Splitting the data to train and test datasets","fb7cd103":"We can safety remove those 3 outliers by creating a new dataframe without these index (297, 523, 1298)  ","281a67ea":"RobustScaler can be used to scale the data because it's powerful against outliers, we already detected some but there must be some other outliers out there."}}