{"cell_type":{"c93ef070":"code","8f672682":"code","37e63d78":"code","835bb9ca":"code","3261f9f5":"code","e0648265":"code","09f75d0b":"code","212e6ec6":"code","1ccd0a1c":"code","8e6f48dd":"code","3a9530ff":"code","b5d3a6cc":"code","1085363d":"code","163c079a":"code","578376e6":"code","8069baf2":"code","abc446ba":"code","5ef2468c":"code","77b5a4ad":"markdown","002b5173":"markdown","1e8cc1c3":"markdown","3ec2016c":"markdown","45f49278":"markdown","7065f261":"markdown","cfbde83a":"markdown","f2397b2b":"markdown","f4cce89c":"markdown","63fb258b":"markdown","a2ef1e5b":"markdown","e035c7c7":"markdown","46a01e5d":"markdown","ce574e03":"markdown","aff9b0f3":"markdown","652144aa":"markdown","654017bd":"markdown","3b9a4dde":"markdown","e8028696":"markdown","fe04e586":"markdown","09edc341":"markdown","f3247ea1":"markdown","d625ff27":"markdown","27ccb9bd":"markdown","c955e366":"markdown","b7713bb9":"markdown"},"source":{"c93ef070":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f672682":"data = pd.read_csv('..\/input\/iris-data\/Iris.csv')\ndata.head(5)","37e63d78":"Species = list(set(data['Species']))\nSpecie1 = data[data['Species']==Species[0]]\nSpecie2 = data[data['Species']==Species[1]]\nSpecie3 = data[data['Species']==Species[2]]","835bb9ca":"import matplotlib.pyplot as plt\nplt.scatter(Specie1['PetalLengthCm'], Specie1['PetalWidthCm'], label=Species[0])\nplt.scatter(Specie2['PetalLengthCm'], Specie2['PetalWidthCm'], label=Species[1])\nplt.scatter(Specie3['PetalLengthCm'], Specie3['PetalWidthCm'], label=Species[2])\nplt.xlabel('PetalLengthCM')\nplt.ylabel('PetalWidthCM')\nplt.legend()\nplt.title('Different Species Visualization')","3261f9f5":"req_data = data.iloc[:,1:]\nreq_data.head(5)","e0648265":"shuffle_index = np.random.permutation(req_data.shape[0])        #shuffling the row index of our dataset\nreq_data = req_data.iloc[shuffle_index]\nreq_data.head(5)","09f75d0b":"train_size = int(req_data.shape[0]*0.7)","212e6ec6":"train_df = req_data.iloc[:train_size,:] \ntest_df = req_data.iloc[train_size:,:]\ntrain = train_df.values\ntest = test_df.values\ny_true = test[:,-1]\nprint('Train_Shape: ',train_df.shape)\nprint('Test_Shape: ',test_df.shape)","1ccd0a1c":"from math import sqrt\ndef euclidean_distance(x_test, x_train):\n    distance = 0\n    for i in range(len(x_test)-1):\n        distance += (x_test[i]-x_train[i])**2\n    return sqrt(distance)","8e6f48dd":"def get_neighbors(x_test, x_train, num_neighbors):\n    distances = []\n    data = []\n    for i in x_train:\n        distances.append(euclidean_distance(x_test,i))\n        data.append(i)\n    distances = np.array(distances)\n    data = np.array(data)\n    sort_indexes = distances.argsort()             #argsort() function returns indices by sorting distances data in ascending order\n    data = data[sort_indexes]                      #modifying our data based on sorted indices, so that we can get the nearest neightbours\n    return data[:num_neighbors]               ","3a9530ff":"def prediction(x_test, x_train, num_neighbors):\n    classes = []\n    neighbors = get_neighbors(x_test, x_train, num_neighbors)\n    for i in neighbors:\n        classes.append(i[-1])\n    predicted = max(classes, key=classes.count)              #taking the most repeated class\n    return predicted","b5d3a6cc":"def predict_classifier(x_test):\n    classes = []\n    neighbors = get_neighbors(x_test, req_data.values, 5)\n    for i in neighbors:\n        classes.append(i[-1])\n    predicted = max(classes, key=classes.count)\n    print(predicted)\n    return predicted","1085363d":"def accuracy(y_true, y_pred):\n    num_correct = 0\n    for i in range(len(y_true)):\n        if y_true[i]==y_pred[i]:\n            num_correct+=1\n    accuracy = num_correct\/len(y_true)\n    return accuracy","163c079a":"y_pred = []\nfor i in test:\n    y_pred.append(prediction(i, train, 5))\ny_pred","578376e6":"accuracy = accuracy(y_true, y_pred)","8069baf2":"accuracy","abc446ba":"test_df.insert(5, 'Predicted_Species', y_pred, False)","5ef2468c":"test_df.sample(5)","77b5a4ad":"# KNN-Python from Scratch","002b5173":"# KNN in 3 Steps\n> 1 Measure distance (Euclidean Distance or Manhattan Distance)\n\n> 2 Get nearest neighbours\n\n> 3 Predict Classifier","1e8cc1c3":"Predicting the classifier of which our new data point belongs too","3ec2016c":"> Evaluating model performance","45f49278":"Measuring Distance using Euclidean Distance\n>Mathematical formula  \u221a (x2 \u2212 x1)2 + (y2 \u2212 y1)2","7065f261":"* We are getting pretty good accuracy\n* If your are getting low accuracy tune the value of k(nearest neighbours)","cfbde83a":"Predicting test data","f2397b2b":"# # Step 2","f4cce89c":"# # Visualising different species(Classifiers) on which we are going to work","63fb258b":"Here we are going to work with\n[iris-data](https:\/\/www.kaggle.com\/uciml\/iris)","a2ef1e5b":"Measuring the accuracy. So that we can know how accurate our model would predict new data samples","e035c7c7":"Getting the nearest neighbours","46a01e5d":"Removing Id column from data, which is unnecessary","ce574e03":"Shuffling the data, to avoid overFitting problem","aff9b0f3":"[KNN Introduction](https:\/\/towardsdatascience.com\/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)\n> K Nearest Neighbour is a simple algorithm that stores all the available cases and classifies the new data or case based on a similarity measure. It mostly used to classifies a data point based on how its neighbours are classified.","652144aa":"# # load the data","654017bd":"* Setting 70% data into training data\n* Therefore 30% data will be our test data","3b9a4dde":"# # # How KNN works\n* Let us consider we have 10 classifiers.\n* You want to predict new sample data point belongs to which classifier.\n* Here KNN comes into picture to solve your problem.\n\nConfused!?\nLet me explain \n\n* KNN measures the distance between new data point and all the available data.\n* 'K' in KNN refers to number of nearest neighbours, consider k = 5\n* Collect five nearest data points based on distace we measured.\n* Our new data point is classified by majority of votes from its five neighbours and new data point would be 4th classifier (among 10 classifiers) as four out of five nearest neighbours belong to 4th classifier\n\nGot an Idea!\n\n","e8028696":"* Independent Variables: ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n* Dependent Variables: ['Species']","fe04e586":"# # Here we are going to implement KNN algorithm from scratch without using any machine-learning libraries","09edc341":"# # Step 3","f3247ea1":"# # Step 1","d625ff27":"# Let's Start ","27ccb9bd":"* Choosing k value varies with dataset your working\n* Recommendation\n    *         k = sqrt(N)\n    *         where N is total number of samples","c955e366":"# # # Preprocessing Data","b7713bb9":"# # *Now our task is to predict the new data point belongs to which species based on sepalLength, sepalWidth, petalLength, petalWidth*"}}