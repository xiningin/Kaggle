{"cell_type":{"569d118d":"code","c30abf0a":"code","fdcad637":"code","db942e38":"code","1b9d9c76":"code","de3420d2":"markdown"},"source":{"569d118d":"import json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport gc","c30abf0a":"def zero_entropy(data):\n    const_cols = [c for c in data.columns if data[c].nunique(dropna=False) == 1]\n    if len(const_cols) > 0:\n        print(\"The following columns will be dropped since they have only one value: \\n\")\n        print(const_cols)\n        for col in const_cols:\n            del data[col]\n    return data\n\n\ndef na_dropper(data):\n    tot = data.shape[0]\n    for col in data.columns:\n        mis = data[col].isna().sum()\n        if ((mis\/tot) > 0.7) and ('transactionRevenue' not in col): # quick escape from making a mistake\n            print(\"The column {} will be dropped because more than 70% of the entries are missing\".format(col))\n            del data[col]\n    return data\n\n            \ndef light_import(data_path):\n    # first, simple columns\n    simple_cols = ['channelGrouping', 'fullVisitorId', 'sessionId', \n              'visitId', 'visitNumber', 'visitStartTime']\n    result = pd.read_csv(data_path, usecols=simple_cols, dtype={'fullVisitorId': 'str'})\n    # cleaning useless columns\n    result = zero_entropy(result)\n    result = na_dropper(result)\n    # then focus on the complex column\n    complex_cols = ['geoNetwork', 'device', 'totals', 'trafficSource']\n    for col in complex_cols:\n        print(\"Importing {}...\".format(col)) # to watch something happening\n        tmp = pd.read_csv(data_path, usecols=[col])\n        tmp = json_normalize(tmp[col].apply(json.loads))\n        tmp.columns = [f\"{col}_{subcolumn}\" for subcolumn in tmp.columns]\n        # cleaning columns\n        tmp = zero_entropy(tmp)\n        tmp = na_dropper(tmp)\n        # mergin what is left\n        result = result.merge(tmp, left_index=True, right_index=True)\n        # remove the garbage\n        del tmp\n        gc.collect()\n    return result","fdcad637":"%%time\ndf_train = light_import('..\/input\/train.csv')","db942e38":"%%time\ndf_test = light_import('..\/input\/test.csv')","1b9d9c76":"df_train.columns","de3420d2":"This kernel aims to give a way of importing fairly big datasets when there is not much RAM available. Or, more simply, when you want to use your RAM for something more than reading a csv file.\n\nThe idea is simply to import column by column and drop it if the column is not useful. Thus I will drop a column if it contains only 1 unique value or, but in this case it is just an example, if more than 70% of the entries are missing.\n\nIn this way, I was able to load both datasets in pandas DataFrames on a laptop with only 8 GB of RAM.\n\nA word of caution before going into the code: dropping missing data is not necessarily a good strategy because data might be missing for a reason.\n\nI thank Julian Peller for his [nice kernel](https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields) that showed me the core procedure."}}