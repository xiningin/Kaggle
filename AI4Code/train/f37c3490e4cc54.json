{"cell_type":{"6956782e":"code","a3fdc2cc":"code","8ab5c279":"code","c56ca505":"code","2e43d9ae":"code","5bf1e4ed":"code","ba2d594c":"code","6e45c423":"code","22faffae":"code","6b4223b7":"code","3a41ade9":"code","3324f306":"code","a2536267":"code","32ba6e48":"code","0ca746cb":"code","5d01f59f":"code","39c79fa3":"code","afa4b957":"markdown","aea9fec2":"markdown","87b19fba":"markdown","603c52e6":"markdown","a375129a":"markdown","5122aa7c":"markdown","de0b7211":"markdown","1ec81854":"markdown","7157c01a":"markdown","5019c58c":"markdown","3d3df08b":"markdown","99fe2077":"markdown","a9ed7682":"markdown","662441e1":"markdown","b757aae7":"markdown","1c3fb57d":"markdown","58ec4a4c":"markdown","86192fd1":"markdown","8f19be28":"markdown","f6b7534c":"markdown","e935f7df":"markdown","983fab04":"markdown","c889184b":"markdown","9443ad8f":"markdown","2e760d53":"markdown","a9009c33":"markdown","28295092":"markdown","a4ced6df":"markdown","861868f6":"markdown","7053c326":"markdown","92b47c63":"markdown","a15813b2":"markdown","493a5fd1":"markdown","aaf83904":"markdown","78ef5850":"markdown","03b44ade":"markdown"},"source":{"6956782e":"#importing necessary libraries and frameworks\n!pip install tensorflow_datasets\nimport numpy as np\nimport pandas as pd   \nimport os\nfrom pathlib import Path\nimport glob\nimport json\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport os\nimport nltk\nimport cv2\nimport matplotlib.pyplot as plt\nimport random\n\n#Check GPU is available for training or not Or whether the tensorflow version can utilize gpu \nphysical_devices = tf.config.list_physical_devices('GPU') \nprint(\"Number of GPUs :\", len(physical_devices)) \nprint(\"Tensorflow GPU :\",tf.test.is_built_with_cuda())\nif len(physical_devices)>0:\n    device=\"\/GPU:0\"\nelse:\n    device=\"\/CPU:0\"\n","a3fdc2cc":"#train_dataframe and val_dataframe stores the path to the images and respective questions and answers\nlenghtData = 20\n\ntrainList=[]\nwith open('\/kaggle\/input\/clevr-dataset\/CLEVR_v1.0\/questions\/CLEVR_train_questions.json') as f:\n    data = json.load(f)\n    for K in range(lenghtData):\n        i = data['questions'][random.randrange(20, 5000, 3)]\n        temp=[]\n        for path in glob.glob('\/kaggle\/input\/clevr-dataset\/CLEVR_v1.0\/images\/train\/'+i['image_filename']): \n            temp.append(path)\n        temp.append(i['question'])\n        temp.append(i['answer'])\n        trainList.append(temp)\nf.close()\nlabels=['Path','Question','Answer']\ntrain_dataframe = pd.DataFrame.from_records(trainList, columns=labels)#training Dataframe \ndel(data)\ndel(trainList)\n\nvalList=[]\nwith open('\/kaggle\/input\/clevr-dataset\/CLEVR_v1.0\/questions\/CLEVR_val_questions.json') as f:\n    data = json.load(f)\n    for k in range(lenghtData):\n        i = data['questions'][random.randrange(20, 5000, 3)]\n        temp=[]\n        for path in glob.glob('\/kaggle\/input\/clevr-dataset\/CLEVR_v1.0\/images\/val\/'+i['image_filename']): \n            temp.append(path)\n        temp.append(i['question'])\n        temp.append(i['answer'])\n        valList.append(temp)\nf.close()\nval_dataframe = pd.DataFrame.from_records(valList, columns=labels)#validation Dataframe\ndel(data)\ndel(valList)\nval_dataframe.head()","8ab5c279":"vocab_set=set()#set object used to store the vocabulary\n\ntokenizer = tfds.deprecated.text.Tokenizer()\n\nfor i in val_dataframe['Question']:\n    vocab_set.update(tokenizer.tokenize(i))\nfor i in train_dataframe['Question']:\n    vocab_set.update(tokenizer.tokenize(i))\nfor i in val_dataframe['Answer']:\n    vocab_set.update(tokenizer.tokenize(i))\nfor i in train_dataframe['Answer']:\n    vocab_set.update(tokenizer.tokenize(i))\n#\n#Creating an Encoder and a Function to preprocess the text data during the training and inference    \n    \nencoder=tfds.deprecated.text.TokenTextEncoder(vocab_set)\nindex=14\nprint(\"Testing the Encoder with sample questions - \\n \")\nexample_text=encoder.encode(train_dataframe['Question'][index])\nprint(\"Original Text = \"+train_dataframe['Question'][index])\nprint(\"After Encoding = \"+str(example_text))","c56ca505":"imgtest = tf.io.read_file(val_dataframe.iloc[0]['Path'])","2e43d9ae":"    IMG_SIZE=(200,200)\n    imgtest=tf.image.decode_jpeg(imgtest,channels=3) # how many colors have in chanel\n    imgtest=tf.image.resize(imgtest,IMG_SIZE)\n    ","5bf1e4ed":" imgtest=tf.math.divide(imgtest, 255)#","ba2d594c":"testConstant = tf.constant([[1, 2, 3], [4, 5, 6]])\npaddings = tf.constant([[1, 1,], [2, 2]])\ntf.pad(testConstant, paddings, \"CONSTANT\")  # [[0, 0, 0, 0, 0, 0, 0],\n                                 #  [0, 0, 1, 2, 3, 0, 0],\n                                 #  [0, 0, 4, 5, 6, 0, 0],\n                                 #  [0, 0, 0, 0, 0, 0, 0]]","6e45c423":"t = tf.constant([[1, 2, 3], [4, 5, 6]])\npaddings = tf.constant([[1, 1,], [2, 2]])\ntf.pad(t, paddings, \"REFLECT\")  # [[6, 5, 4, 5, 6, 5, 4],\n                                #  [3, 2, 1, 2, 3, 2, 1],\n                                #  [6, 5, 4, 5, 6, 5, 4],\n                                #  [3, 2, 1, 2, 3, 2, 1]]\n","22faffae":"t = tf.constant([[1, 2, 3], [4, 5, 6]])\npaddings = tf.constant([[1, 1,], [2, 2]])\ntf.pad(t, paddings, \"SYMMETRIC\")  # [[2, 1, 1, 2, 3, 3, 2],\n                                  #  [2, 1, 1, 2, 3, 3, 2],\n                                  #  [5, 4, 4, 5, 6, 6, 5],\n                                  #  [5, 4, 4, 5, 6, 6, 5]]","6b4223b7":"t = tf.constant([[1, 2], [3, 4]])\nds = tf.data.Dataset.from_tensors(t)   # [[1, 2], [3, 4]]","3a41ade9":"t = tf.constant([[1, 2], [3, 4]])\nds = tf.data.Dataset.from_tensor_slices(t)   # [1, 2], [3, 4]","3324f306":"BATCH_SIZE=50\nIMG_SIZE=(200,200)\n\n\n#Function that uses the encoder created to encode the input question and answer string\ndef encode_fn(text):\n    return np.array(encoder.encode(text.numpy()))\n\n\n#Function to load and decode the image from the file paths in the dataframe and use the encoder function\ndef preprocess(ip,ans):\n    img,ques=ip#ip is a list containing image paths and questions\n    img=tf.io.read_file(img)\n    img=tf.image.decode_jpeg(img,channels=3)\n    # quantos canais de cores tem \n    img=tf.image.resize(img,IMG_SIZE)\n    img=tf.math.divide(img, 255)# \n    #The question string is converted to encoded list with fixed size of 50 with padding with 0 value\n    ques=tf.py_function(encode_fn,inp=[ques],Tout=tf.int32)\n    paddings = [[0, 50-tf.shape(ques)[0]]]\n    ques = tf.pad(ques, paddings, 'CONSTANT', constant_values=0)\n    ques.set_shape([50])#Explicit shape must be defined in order to create the Input pipeline\n    \n    #The Answer is also encoded \n    ans=tf.py_function(encode_fn,inp=[ans],Tout=tf.int32)\n    ans.set_shape([1])\n    \n    return (img,ques),ans\n    \ndef create_pipeline(dataframe):\n    raw_df=tf.data.Dataset.from_tensor_slices(((dataframe['Path'],dataframe['Question']),dataframe['Answer']))\n    df=raw_df.map(preprocess)#Preprocessing function is applied to the dataset\n    df=df.batch(BATCH_SIZE)#The dataset is batched\n    return df\n\n#The training and validation Dataset objects are created\ntrain_dataset=create_pipeline(train_dataframe)\nvalidation_dataset=create_pipeline(val_dataframe)","a2536267":"#Creating the CNN model for image processing\n\n\nCNN_Input=tf.keras.layers.Input(shape=(200,200,3),name='image_input')\n\nmobilenetv2=tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(200,200,3), alpha=1.0, include_top=False,\n                                                      weights='imagenet', input_tensor=CNN_Input)\n\nCNN_model=tf.keras.models.Sequential()\nCNN_model.add(CNN_Input)\nCNN_model.add(mobilenetv2)\nCNN_model.add(tf.keras.layers.GlobalAveragePooling2D())\n\n\n\n#Creating the RNN model for text processing\nRNN_model=tf.keras.models.Sequential()\n\nRNN_Input=tf.keras.layers.Input(shape=(50),name='text_input')\nRNN_model.add(RNN_Input)\nRNN_model.add(tf.keras.layers.Embedding (len(vocab_set)+1,256))\nRNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,stateful=False,return_sequences=True,recurrent_initializer='glorot_uniform')))\nRNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,stateful=False,return_sequences=True,recurrent_initializer='glorot_uniform')))\nRNN_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,stateful=False,return_sequences=False,recurrent_initializer='glorot_uniform')))\n\n\nconcat=tf.keras.layers.concatenate([CNN_model.output,RNN_model.output])\ndense_out=tf.keras.layers.Dense(len(vocab_set),activation='softmax',name='output')(concat)\n\nmodel = tf.keras.Model(inputs=[CNN_Input,RNN_Input],\n                    outputs=dense_out)\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\nmodel.summary()","32ba6e48":"def scheduler(epoch):\n  if epoch < 1:\n    return 0.001\n  else:\n    return 0.001 * tf.math.exp(0.1 * (1 - epoch))\n\nLRS = tf.keras.callbacks.LearningRateScheduler(scheduler)\ncsv_callback=tf.keras.callbacks.CSVLogger(\n    \"Training Parameters.csv\", separator=',', append=False\n)\nteste =1\n\n#create a checkpoint to save the training\n\n# Include the epoch in the file name (uses `str.format`)\ncheckpoint_path = \"training_2\/cp-{epoch:04d}.ckpt\"\n#checkpoint_dir = os.path.dirname(checkpoint_path)\nepoch = 0\n\nepoch = 0\n\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\n  filepath='weights.{epoch:02d}.ckpt', \n    verbose=1, \n    save_weights_only=True,\n    save_freq=1000*BATCH_SIZE)\n\n\n\n#def FormatarEndereco(epoch):\n#    epoch = epoch+1\n#    return \"training_2\/cp-\"+ str(epoch) + \".ckpt\"\n\n#model.save_weights(FormatarEndereco(epoch))\n\nmodel.save_weights(checkpoint_path.format(epoch=0,val_loss=0))\n                   \nwith tf.device(device):\n   history =  model.fit(train_dataset,\n              validation_data=validation_dataset,\n              callbacks=[csv_callback,LRS,cp_callback],\n              epochs=2)","0ca746cb":"train_dataframe.iloc[index]['Path']","5d01f59f":"print(\"Predictions Are as follows = \")\n\nfor i in range(5):\n    index=1\n    fig,axis=plt.subplots(1,2,figsize=(25, 8))\n    im=cv2.imread(train_dataframe.iloc[index]['Path'])\n    im=cv2.resize(im,(200,200))\n    q=train_dataframe.iloc[index]['Question']\n    q=encoder.encode(q)\n    paddings = [[0, 50-tf.shape(q)[0]]]\n    q=tf.pad(q, paddings, 'CONSTANT', constant_values=0)\n    q=np.array(q)\n    print(im.shape)\n    print(q.shape)\n    ans=model.predict([[im],[q]])\n    question=\"\"\n    flag=0\n    for i,j in enumerate(train_dataframe.iloc[index]['Question']):\n        if (flag==1) and (j==' '):\n            question+='\\n'\n            flag=0\n        question+=j\n        if (i%40==0)and (i!=0):\n            flag=1\n    axis[0].imshow(im)\n    axis[0].axis('off')\n    axis[0].set_title('Image', fontsize=30)\n    axis[1].text(0.05,0.5,\n             \"Question = {}\\n\\nPredicted Answer = {}\\n\\nActual Answer ={}\".format(question,encoder.decode([np.argmax(ans)]),val_dataframe.iloc[index]['Answer']),\n             transform=plt.gca().transAxes,fontsize=19)\n    axis[1].axis('on')\n    axis[1].set_title('Question And Answers', fontsize=30)\n    \n","39c79fa3":"print(\"Predictions Are as follows = \")\nhist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhistory.history\nhist.tail()\n","afa4b957":"![image.png](attachment:image.png) \nimage -3","aea9fec2":"# Types of padding\n\nThis operation pads a tensor according to the paddings you specify. paddings is an integer tensor with shape [n, 2], where n is the rank of tensor. For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension. The main models implemented in the tensorflow will be described below. \n\n# Constant padding\n\nA type of padding that really resembles same padding is constant padding. Here, the outcome can be the same \u2013 the output will have the same shape as the input. However, rather than \u201czeros\u201d \u2013 which is what same padding does \u2013 constant padding allows you to pad with a user-specified constant value, that will be described in the image 5 and in the code -1 the implemention with tensorflow\n\n![image.png](attachment:image.png)\nimage-5\n\n","87b19fba":"# Strides\n\nStride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data. Stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video. For example, if a neural network's stride is set to 1, the filter will move one pixel, or unit,  at a time. The size of the filter affects the encoded output volume, so stride is often set to a whole integer, rather than a fraction or decimal.\n**\nHow does Stride work?**\n\nImagine a convolutional neural network is taking an image and analyzing the content. If the filter size is 3x3 pixels, the contained nine pixels will be converted down to 1 pixel in the output layer. Naturally, as the stride, or movement, is increased, the resulting output will be smaller. Stride is a parameter that works in conjunction with padding, the feature that adds blank, or empty pixels to the frame of the image to allow for a minimized reduction of size in the output layer. Roughly, it is a way of increasing the size of an image, to counteract the fact that stride reduces the size. Padding and stride are the foundational parameters of any convolutional neural network. \n\n![image.png](attachment:image.png)\nimage 12","603c52e6":"Figure 2 - Dataflow diagram of question encoding\n![image.png](attachment:image.png)\nSource : DESTA","a375129a":"# Replication padding \/ symmetric padding \n\nReplication padding looks like reflection padding, but  you simply take a copy, and mirror it. Like this:\n\n* You\u2019re at the first row again, at the right. You find a 1. What is the next value?\n* Simple: you copy the entire row, mirror it, and start adding it as padding values horizontally. So, for row 1 with [3,5,1], this will be [1,5,3] being added. As you can see, since we only pad 2 elements in width, there are 1 and 5, but 3 falls off the padding.\n\nIn the image 8 you have a example with images and below you have the code to implemation. \n\n![image.png](attachment:image.png)\n\nImage 8\n","5122aa7c":"# **Feature Map**\n\nConsider a image 5 x 5 whose image pixel values are 0, 1 and filter matrix 3 x 3 as shown in the image 9 . \n\n![image.png](attachment:image.png)\nImage 9","de0b7211":"# Convolutional neural network architecture\n\nA Convolutional Neural Network (CNN) is a deep learning algorithm that can recognize and classify features in images for computer vision. It is a multi-layer neural network designed to analyze visual inputs and perform tasks such as image classification, segmentThere are two main parts to a CNN:\n\nA convolution tool that splits the various features of the image for analysis\nA fully connected layer that uses the output of the convolution layer to predict the best description for the image action and object detection, which can be useful for autonomous vehicles. \n\nThere are two main parts to a CNN:\n\n* A convolution tool that splits the various features of the image for analysis\n\n* A fully connected layer that uses the output of the convolution layer to predict the best description for the image \n\n**Basic Convolutional Neural Network Architecture**\n\nCNN architecture is inspired by the organization and functionality of the visual cortex and designed to mimic the connectivity pattern of neurons within the human brain.\n\nThe neurons within a CNN are split into a three-dimensional structure, with each set of neurons analyzing a small region or feature of the image. In other words, each group of neurons specializes in identifying one part of the image. CNNs use the predictions from the layers to produce a final output that presents a vector of probability scores to represent the likelihood that a specific feature belongs to a certain class.\n\n**How a Convolutional Neural Network Works\u2501The CNN layers**\n\nA CNN is composed of several kinds of layers:\n\nConvolutional layer\u2501creates a feature map to predict the class probabilities for each feature by applying a filter that scans the whole image, few pixels at a time.\n\n* **Pooling layer (downsampling)**: scales down the amount of information the convolutional layer generated for each feature and maintains the most essential information (the process of the convolutional and pooling layers usually repeats several times).\n* **Fully connected input layer**: \u201cflattens\u201d the outputs generated by previous layers to turn them into a single vector that can be used as an input for the next layer.\n* **Fully connected layer\u2014applies**: weights over the input generated by the feature analysis to predict an accurate label.\n* **Fully connected output layer** : generates the final probabilities to determine a class for the image.\n\nThe image 8 ilustrate the process. \n\n![image.png](attachment:image.png)\n image 8\n","1ec81854":"# Import libraries \nFirst of all, need to import the components and verify the gpu process.","7157c01a":"# Data Set tensorflow\nfrom_tensors combines the input and returns a dataset with a single element, that will be implement  below ","5019c58c":"# Visual Question Answering ML Model on CLEVR dataset\n\n\n   Visual Question Answering (VQA) is a novel problem domain where multi-modal inputs must be processed in order to solve the task given in the form of a natural language. As the solutions inherently require to combine visual and natural language processing with abstract reasoning, the problem is considered as AI-complete. Recent advances indicate that using high-level, abstract facts extracted from the inputs might facilitate reasoning. Following that direction we decided to develop a solution combining state-of-the-art object detection and reasoning modules. \n   In this article we would like to validate is whether the operation on highlevel and abstract facts extracted from the image might improve the accuracy of the system in a neutural network system. \n\n","3d3df08b":"# Solution \n\nThe Architecture that is used on this post are  presented in Fig. 1. It extends the Encoder-Decoder architecture, which originally consisted of two RNNs, the first one used for encoding\na sequence of input symbols into a fixed length representation, and the other for  decoding that representation into another sequence of output symbols.To deployment architecture propose in this architecture we use a tensorflow and panda, that will be describe below . ","99fe2077":"from_tensor_slices creates a dataset with a separate element for each row of the input tensor, that will be implement below","a9ed7682":"Then the convolution of 5 x 5 image matrix multiplies with 3 x 3 filter matrix which is called \u201cFeature Map\u201d, that represent in image 10. \n\n![image.png](attachment:image.png)\n\nImage 10\n","662441e1":"In the image 7, you have the example of reflection padding on a image. \n\n![image.png](attachment:image.png)\nimage 7","b757aae7":"# What is a pixel exactly?\n\nA pixel is essentially just a point on an image, with a specific shade, color, and\/or opacity. We normally represent a pixel as a single integer or multiple integers. Pixels take a specific form based on the interpretation of the image, which is usually one of the following:\nGrayscale: Viewing the image as shades of black and white. Each pixel is an integer between 0\u2013255, where 0 is completely black and 255 is completely white.\nRGB: The default interpretation for color images. Each pixel is made up of 3 integers between 0\u2013255, where the integers represent the intensity of red, green, and blue, respectively, for the pixel.\nRGBA: An extension of RGB with an added alpha field. The alpha field represents the opacity of an image, and in this Lab we\u2019ll represent a pixel\u2019s alpha value as an integer from 0\u2013255 with 0 being fully transparent and 255 being fully opaque.\nWe can choose to interpret an image however we want, but there is usually one interpretation that is optimal. For example, we could interpret a black and white image with RGB pixel values, but it is more efficient to view it as a grayscale image (3x fewer integers used). On the other hand, it would be unwise to interpret a colored image using grayscale pixels, since the pixels won\u2019t be able to capture any of the actual colors.","1c3fb57d":"According Hochreiter , where W denotes the number of words constituting a given question and answer that we use  GloVe (Global Vectors for Word Representation) word embedding model to encode question words. Finally, we pass the encoded words one by one as inputs to the LSTM to produce a list of encoded output:\n![image.png](attachment:image.png)\n","58ec4a4c":"<center> Figure 1:  General architecture of the proposed system\n\n![image.png](attachment:image.png)\n\nSource : Dased on  DESTA, 2018  ","86192fd1":"# Architecture  mobilenet_v2\n\nMobileNetV2 is a general architecture and can be used for multiple use cases. Depending on the use case, it can use different input layer size and different width factors. This allows different width models to reduce the number of multiply-adds and thereby reduce inference cost on mobile devices.\n\nThe principal archicture of mobilenetve is the Convolutional Neural Network (CNN) architecture is inspired by the organization and functionality of the visual cortex and designed to mimic the connectivity pattern of neurons within the human brain.\n\nThere are two main parts to a CNN:\n\n  -> A convolution tool that splits the various features of the image for analysis\n  -> A fully connected layer that uses the output of the convolution layer to predict the best description for the image.\n\n**How a Convolutional Neural Network Works\u2501The CNN layers**\n\nA CNN is composed of several kinds of layers:\n\nConvolutional layer: creates a feature map to predict the class probabilities for each feature by applying a filter that scans the whole image, few pixels at a time.\nPooling layer (downsampling)\u2501scales down the amount of information the convolutional layer generated for each feature and maintains the most essential information (the process of the convolutional and pooling layers usually repeats several times).\nFully connected input layer: \u201cflattens\u201d the outputs generated by previous layers to turn them into a single vector that can be used as an input for the next layer.\nFully connected layer:applies weights over the input generated by the feature analysis to predict an accurate label.\nFully connected output layer :generates the final probabilities to determine a class for the image.\n\nThe neurons within a CNN are split into a three-dimensional structure, with each set of neurons analyzing a small region or feature of the image. In other words, each group of neurons specializes in identifying one part of the image. CNNs use the predictions from the layers to produce a final output that presents a vector of probability scores to represent the likelihood that a specific feature belongs to a certain class.\n\n\n\n\nThe architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers described in the Table 2. We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation. We always use kernel size 3 \u00d7 3 as is standard\nfor modern networks, and utilize dropout and batch normalization during training.\n\n\n\n\nMobileNetV2 is very similar to the original MobileNet, except that it uses inverted residual blocks with bottlenecking features. It has a drastically lower parameter count than the original MobileNet. MobileNets support any input size greater than 32 x 32, with larger image sizes offering better performance.\n\n\n\n\nThe parameters will be list below \n\n**input_shape**\tOptional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value. Default to None. input_shape will be ignored if the input_tensor is provided.\n\n**alpha**  Controls the width of the network. This is known as the width multiplier in the MobileNet paper. - If alpha < 1.0, proportionally decreases the number of filters in each layer. - If alpha > 1.0, proportionally increases the number of filters in each layer. - If alpha = 1, default number of filters from the paper are used at each layer. Default to 1.0.\n\n\n\nThe number of parameters and number of multiply-adds can be modified by using the alpha parameter, which increases\/decreases the number of filters in each layer. By altering the image size and alpha parameter, all 22 models from the paper can be built, with ImageNet weights provided.\n","8f19be28":"# Creating the Model","f6b7534c":"# Padding avoids the loss of spatial dimensions\n\n\nSometimes, however, you need to apply filters of a fixed size, but you don\u2019t want to lose width and\/or height dimensions in your feature maps. For example, this is the case when you\u2019re training an autoencoder. You need the output images to be of the same size as the input, yet need an activation function like e.g. Sigmoid in order to generate them.\n\nIf you would do so\nwith a Conv layer, this would become problematic, as you\u2019d reduce the size of your feature maps \u2013 and hence would produce outputs unequal in size to your inputs.\n\nThat\u2019s not what we want when we create an autoencoder. We want the original output and the original output only.\n\nPadding helps you solve this problem. Applying it effectively adds \u201cspace\u201d around your input data or your feature map \u2013 or, more precisely, \u201cextra rows and columns\u201d [with some instantiation] (Chollet, 2017).\n\nAdding the \u201cextra space\u201d now allows us to capture the position we previously couldn\u2019t capture, and allows us to detect features in the \u201cedges\u201d of your input, this desmontrated on the image-4.\n\n![image.png](attachment:image.png)\nimage-4","e935f7df":"\n# Activation function \n\nIn artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. \n\n","983fab04":"Convolution of an image with different filters can perform operations such as edge detection, blur and sharpen by applying filters. The below example shows various convolution image after applying different types of filters (Kernels).\n\n![image.png](attachment:image.png)\nImage 11","c889184b":"# **Image Type**\n\nNow that have learn how to load an image, it is time to decode the image data into pixel data using TensorFlow.\nThe decoding function that we use depends on the format of the image. For generic decoding (i.e. decoding any image format), we use tensorflow.image.decode_image but if the input is a JPEG image we use tensorflow.image.decode_jpeg.\nAnother reason is that tensorflow.image.decode_image supports GIF decoding, which results in an output shape of (num_frames, height, width, channels. Since the function can return data with different shapes, we can't use tensorflow.image.decode_image when we also need to resize the image with tensorflow.image.resize_images.\nWe can change the pixel format of the decoded image via the channels keyword argument. The channels argument represents the number of integers per pixel. The default value for channels is 0, which means the decoding function uses the interpretation specified from the raw data. Setting channels to 1 specifies a grayscale image, while setting channels to 3 specifies an RGB image. For PNG images we're also allowed to set channels to 4, corresponding to RGBA images. Setting channels to 2 is invalid.","9443ad8f":"In the image 6, have the exemple in the transformation of image. \n\n![image.png](attachment:image.png)\nimage 6","2e760d53":"# Loading the data from the JSON files \n\nLoading the json files using JSON module and using pandas to create a dataframe consisting of the path to the images and respective questions and answers to the images.\nTo store the information will be load by json files, will be use tow arrays, that will be transform by panda.  ","a9009c33":"# Reflective padding\n\nReflective padding seems to improve the empirical performance of your model . Possibly, this occurs because of how \u201czero\u201d based padding (i.e., the \u201csame\u201d padding) and \u201cconstant\u201d based padding alter the distribution of your dataset. Below you have the implemation, of this feature.","28295092":"# Text encoder\n\nThe diagram for question processing is shown in Fig. 2. We start with the question and answer consisting of several words:\n ![image.png](attachment:image.png)","a4ced6df":"# Related work\n\n   Research on VQA has resulted in many interesting solutions. Those, however, could not have been developed without the existence of proper datasets and metrics for evaluation and comparison of the results.\n   The efforts in joint embedding focus on the methods for combining multi-modal representations. As in VQA there are two distinct input modalities (image and text),\nwhich makes this problem similar to the problems found in other multi-modal domains. For example the projection of user and item embeddings into the common representation space in neural recommender systems . Exemplary approaches developed for the VQA problem domain include e.g. Multimodal Compact Bilinear pooling (MCB) method that performed joint embedding of visual and text features, or Relational Networks (RN) where embedded question was concatenated with features extracted from pairs of image regions, enabling the system to reason about the relation between objects being present in those regions.\n    The architecture and modelling this implementation is represent on the figure 1 . ","861868f6":"# What is padding and why do we need it?\n\nLet\u2019s first take a look at what padding is. From this, it gets clear straight away why we might need it for training our neural network. More specifically, our ConvNet, because that\u2019s where you\u2019ll apply padding pretty much all of time time.\n\nNow, in order to find out about how padding works, we need to study the internals of a convolutional layer first. Here you\u2019ve got one, although it\u2019s very generic:\n![image.png](attachment:image.png)\n\n\nWhat you see on the left is an RGB input image \u2013 **width W**, **height H** and **three channels**. Hence, this layer is likely the first layer in your model; in any other scenario, you\u2019d have feature maps as the input to your layer.\n\nNow, what is a feature map? That\u2019s the yellow block in the image. It\u2019s a collection of N one-dimensional \u201cmaps\u201d that each represent a particular \u201cfeature\u201d that the model has spotted within the image. This is why convolutional layers are known as feature extractors.\n\nNow, this is very nice \u2013 but how do we get from input (whether image or feature map) to a feature map? This is through kernels, or filters, actually. These filters \u2013 you configure some number N per convolutional layer \u2013 \u201cslide\u201d (strictly: convolve) over your input data, and have the same number of \u201cchannel\u201d dimensions as your input data, but have much smaller widths and heights. For example, for the scenario above, a filter may be 3 x 3 pixels wide and high, but always has 3 channels as our input has 3 channels too.\n\nNow, when they slide over the input \u2013 from left to right horizontally, then moving down vertically after a row has been fully captured \u2013 they perform element-wise multiplications between what\u2019s \u201ccurrently under investigation\u201d within the input data and the weights present within the filter. These weights are equal to the weights of a \u201cclassic\u201d neural network, but are structured in a different way. Hence, optimization a ConvNet involves computing a loss value for the model and subsequently using an optimizer to change the weights.\n\nThrough these weights, as you may guess, the model learns to detect the presence of particular features \u2013 which, once again, are represented by the feature maps. This closes the circle with respect to how a convolutional layer works.\n\n\n\n\n","7053c326":"References \n\nDesta, Mikyas T., Larry Chen, e Tomasz Kornuta. \u201cObject-Based Reasoning in VQA\u201d. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), 1814\u201323. Lake Tahoe, NV: IEEE, 2018. https:\/\/doi.org\/10.1109\/WACV.2018.00201.\n\nHochreiter, S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural computation, 9(8):1735\u20131780, 1997\n\n J. Pennington, R. Socher, and C. Manning.  Global\nvectors for word representation. In Proceedings of the 2014\nconference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.\n\nChollet, F. (2017). Deep Learning with Python. New York, NY: Manning Publications.\n\nKeras. (n.d.). Convolutional Layers. Retrieved from https:\/\/keras.io\/layers\/convolutional\/\n\nPyTorch. (n.d.). torch.nn.modules.padding. Retrieved from https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/nn\/modules\/padding.html\n\nThe Blog. -  Convolutions in Autoregressive Neural Networks. Retrieved from https:\/\/theblog.github.io\/post\/convolution-in-autoregressive-neural-networks\/\n\n","92b47c63":"# Conv layers might induce spatial hierarchy\n\n\nIf the width and\/or height of your kernels is >1, you\u2019ll see that the width and height of the feature map being output gets smaller. This occurs due to the fact that the feature map slides over the input and computes the element-wise multiplications, but is too large in order to inspect the \u201cedges\u201d of the input. This is illustrated in the image 2 , where the \u201cred\u201d position is impossible to take and the \u201cgreen\u201d one is part of the path of the convolution operation.\n\n![image.png](attachment:image.png)\nimage -2 \n\nAs it cannot capture the edges, it won\u2019t be able to effectively \u201cend\u201d at the final position of your row, resulting in a smaller output width and\/or height.\n\nWe call this a spatial hierarchy. Indeed, convolutional layers may cause a \u201chierarchy\u201d-like flow of data through the model. In the image 3, you have a schematic representation of a substantial hierarchy and a less substantial one \u2013 which is often considered to be less efficient:\n","a15813b2":"# Training the model on the prepared data","493a5fd1":"# **Normalization image in Python**\nNormalization refers to rescaling real-valued numeric attributes into a 00 to 1 range.\n\nData normalization is used in machine learning to make model training less sensitive to the scale of features. This allows our model to converge to better weights and, in turn, leads to a more accurate model. \n \nIn the images RGB , have a pixel is made up of 3 integers between 0\u2013255,to normalizate the values of image , i will share the matriz witt 255 .\n\n","aaf83904":"# Image recognition with Machine Learning \n\nIn this part of  article I will focus on image processing, specifically how we can convert images from JPEG or PNG files to usable data for our neural networks. The library I will use is TensorFlow 2.0 as it provides a variety of utility functions to obtain image data from files, resize the images, and even transform a large set of images all at once.\n\nIf you\u2019ve ever looked at an image file\u2019s properties before, it\u2019ll show the dimensions of the image, i.e. the height and width of the image. The height and width are based on number of pixels. For example, if the dimensions of an image are 400x300 (width x height), then the total number of pixels in the image is 120000.\n\nThe function tensorflow.io.read_file takes the file name as its required argument and returns the contents of the file as a tensor with type tensorflow.string. When the input file is an image, the output of tensorflow.io.read_file will be the raw byte data of the image file. Although the raw byte output represents the image's pixel data, it cannot be used directly. Let\u2019s first see the implementation in Python using a image of clever-database.","78ef5850":"# Tensorlfow\u2019s Neural Network Convolution\n\nTraining Convolutional Neural Networks means that your network is composed of two separate parts most of the times. The last part of your network, which often contains densely-connected layers but doesn\u2019t have to, generates a classification or regresses a value based on the inputs received by the first Dense layer. \n\nThe first part, however, serves as a \u201cfeature extraction\u201d mechanism \u2013 it transforms the original inputs into \u201cbits of information\u201d which ensures that the Dense layers perform better. By consequence, the system as a whole allows you to feed it raw inputs, which are processed internally, while you get a probability distribution over a set of classes in return.\n\nTypically, Convolutional layers are used as feature extractors. Through optimization, these layers learn \u201ckernels\u201d which slide (or convolve) over the input data, generating a number of \u201cfeature maps\u201d that can subsequently be used for detecting certain patterns in the data. This is achieved by element-wise multiplications between the slice of input data the filter is currently hovering over, and the weights present within the filter.\n\nThis, in return, effectively means that a spatial hierarchy is created: the more one moves towards the right when inspecting the model architecture, the smaller the inputs and hence feature maps become. Sometimes, though, you don\u2019t want your input to become smaller \u2013 in the case of an autoencoder, for example, where you just want to converge the feature maps into one Sigmoid activated output. This can be achieved with the \u201cpadding mechanism\u201d, which is precisely what we\u2019ll cover in this problem. \n\n","03b44ade":"In the next cell we prepare a vocabulary set for questions and answers present in the dataset, it will be used to create an encoder, conforme describe above."}}