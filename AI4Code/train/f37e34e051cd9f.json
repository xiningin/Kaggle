{"cell_type":{"ed1a6382":"code","1f156fbf":"code","72a6e9de":"code","2ae25bd4":"code","3b98b85c":"code","67bcdea5":"code","a35e869d":"code","1747b133":"code","b051eafd":"code","103a257e":"code","46d39c7b":"code","e047e00b":"code","b7de83ea":"code","fc961457":"code","4e588ab6":"code","7780faab":"code","56914ffe":"code","707448a5":"code","345a84e3":"code","6fcca53c":"code","397eb210":"code","03e600b6":"code","263ff2d6":"code","87bc4659":"markdown","48bda35d":"markdown","5381bb08":"markdown"},"source":{"ed1a6382":"from IPython.display import YouTubeVideo      \nYouTubeVideo('zZrGyQNGMJE')","1f156fbf":"from IPython.display import YouTubeVideo      \nYouTubeVideo('OAOQIOIwx7w')","72a6e9de":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.utils import to_categorical","2ae25bd4":"## loading the dataset\n\n(xtrain,ytrain),(xtest,ytest) = fashion_mnist.load_data()\nprint(xtrain.shape)\nprint(ytrain.shape)\nprint(xtest.shape)\nprint(ytest.shape)","3b98b85c":"class_names = [\"T-shirt\/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\nlen(class_names)","67bcdea5":"index = 50000\nplt.imshow(xtrain[index],cmap='gray')\nplt.show()\nprint(ytrain[index])\nprint(class_names[ytrain[index]])","a35e869d":"fig,axes = plt.subplots(nrows=10,ncols=5,figsize=(15,20))\naxes = axes.flatten()\nfor i,ax in zip(range(50),axes):\n  ax.imshow(xtrain[i],cmap='gray')  \n  ax.set_title(class_names[ytrain[i]])\n  ax.axis('off')\nplt.show()","1747b133":"## 2nd stage -Data preparation\nprint(xtrain.shape)\nprint(ytrain.shape)","b051eafd":"# reshape the image data to convert it to channel format\nxtrain = xtrain.reshape(60000,28,28,1)\nxtest = xtest.reshape(10000,28,28,1)\nprint(xtrain.shape)\nprint(xtest.shape)","103a257e":"## Scaling\nxtrain = xtrain\/255\nxtest = xtest\/255","46d39c7b":"plt.imshow(xtrain[0].reshape(28,28),cmap='gray')\nplt.show()","e047e00b":"# one hot encoding of labels\nprint(ytrain.shape)\nprint(ytest.shape)","b7de83ea":"ytrain","fc961457":"ytrain = to_categorical(ytrain)\nytest = to_categorical(ytest)\nprint(ytrain.shape)\nprint(ytest.shape)","4e588ab6":"ytrain[59999] ","7780faab":"ytrain","56914ffe":"from tensorflow.keras import models,layers\nfrom tensorflow.keras.utils import plot_model\n\nmodel = models.Sequential()\n#add the first convolution block\nmodel.add(layers.Conv2D(filters=6,kernel_size=(3,3),input_shape=(28,28,1),activation='relu')) \n# add the maxpooling layer\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\n# add the second covolutional block\nmodel.add(layers.Conv2D(filters=10,kernel_size=(3,3),activation='relu'))\n# add the maxpooling layer\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\n###---------- flatten layer\nmodel.add(layers.Flatten())\n###----------- Classification segment - fully connected network! ------------------\nmodel.add(layers.Dense(120,activation='relu'))\nmodel.add(layers.Dense(84,activation='relu'))\n# add the output layer\nmodel.add(layers.Dense(10,activation='softmax'))\n","707448a5":"plot_model(model)","345a84e3":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy']) \nmodel.summary()","6fcca53c":"#Train the model\nmodel.fit(xtrain,ytrain,epochs=20,batch_size=1000,verbose=True,validation_data=(xtest,ytest))","397eb210":"#Performance Analysis\nypred = model.predict_classes(xtest)  ## ypred is not one hot encoded\nypred2 = to_categorical(ypred)","03e600b6":"# classification report\nfrom sklearn.metrics import classification_report \nprint(classification_report(ytest,ypred2))","263ff2d6":"# confusion matrix\n\nfrom sklearn import metrics\n\n## converting ytest from onehot encoded to normal to check confusion matrix\nytest2 = np.array([np.argmax(i) for i in ytest]) \n\nimport pandas as pd \npd.DataFrame(metrics.confusion_matrix(ytest2,ypred),index=class_names,columns=class_names)","87bc4659":"Lets start with simple dataset, which helps to implement the process on CNN","48bda35d":"### Modelling - model the CNN","5381bb08":"Hello,\n\nHere the session is divided into 2 parts\n\nPart1 : Data Loading and Data Preparation with the logics behing\nPart2 : Model Building and performance factor\nPart3 : In progress"}}