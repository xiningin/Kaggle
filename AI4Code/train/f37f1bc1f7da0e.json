{"cell_type":{"7d01aab6":"code","768c5ac4":"code","c196c9e2":"code","9c8bedf1":"code","26f197d4":"code","8a524fdf":"code","b51d1cba":"code","df6531e2":"code","95d57e32":"code","29b3ba19":"code","f13198f0":"code","f0f001df":"code","a9f16c95":"code","1b0b699e":"code","66e5f058":"markdown"},"source":{"7d01aab6":"!pip install timm","768c5ac4":"import os\nimport PIL\nimport time\nimport timm\nimport math\nimport copy\nimport torch\nimport torchvision\nimport numpy as np\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport torch.nn as nn\nfrom PIL import Image\nimport itertools\nfrom pathlib import Path\nfrom copy import deepcopy\nfrom sklearn import metrics\nimport torch.optim as optim\nfrom torchvision import models\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport torch.utils.data as data\nfrom torch.optim import lr_scheduler\nfrom timm.models.layers.activations import *\n%config InlineBackend.figure_format = 'retina'\nfrom collections import OrderedDict, defaultdict\nfrom torchvision import transforms, models, datasets\nfrom torch.utils.data.sampler import SubsetRandomSampler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom sklearn.metrics import confusion_matrix,accuracy_score, classification_report","c196c9e2":"root_dir = '..\/input\/brain-tumor-classification-mri'\ndata_transforms = {\n    'train':transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], \n                             [0.229, 0.224, 0.225])\n    ]),\n    'test':transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], \n                             [0.229, 0.224, 0.225])\n    ])\n}\n\nimage_datasets = {}\nimage_datasets['train'] = datasets.ImageFolder(os.path.join(root_dir, 'Training'), data_transforms['train'])\nimage_datasets['test'] = datasets.ImageFolder(os.path.join(root_dir, 'Testing'), data_transforms['test'])\n\nn = len(image_datasets['train'])\nn_val = int(0.1*n)\nimage_datasets['train'], image_datasets['val'] = torch.utils.data.random_split(image_datasets['train'], [n-n_val, n_val])","9c8bedf1":"batch_size = 8\ndata_loader = {x:torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, num_workers=2, pin_memory=True) for x in ['train','val','test']}\ndataset_sizes = {x:len(image_datasets[x]) for x in ['train','val','test']}\nprint(dataset_sizes)","26f197d4":"class_names = image_datasets['test'].classes\nprint(class_names)\n_ = image_datasets['test'].class_to_idx\ncat_to_name = {_[i]:i for i in list(_.keys())}\nprint(cat_to_name)\n\nimages, labels = next(iter(data_loader['train']))\nprint(images.size())","8a524fdf":"def showimage(data_loader, number_images, cat_to_name):\n    dataiter = iter(data_loader)\n    images, labels = dataiter.next()\n    images = images.numpy() # convert images to numpy for display\n    # plot the images in the batch, along with the corresponding labels\n    fig = plt.figure(figsize=(number_images, 4))\n    for idx in np.arange(number_images):\n        ax = fig.add_subplot(2, number_images\/\/2, idx+1, xticks=[], yticks=[])\n        img = np.transpose(images[idx])\n        plt.imshow(img)\n        ax.set_title(cat_to_name[labels.tolist()[idx]])\n        \n#### to show some  images\nshowimage(data_loader['test'], 8, cat_to_name)","b51d1cba":"model = timm.create_model('swin_base_patch4_window7_224', num_classes = 4, pretrained=True)\nmodel.head","df6531e2":"for param in model.parameters():\n    param.requires_grad = True\n\ncriterion = LabelSmoothingCrossEntropy()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(\"The number of parameters of the model: \", count_params(model))","95d57e32":"from IPython.display import HTML, display\n\ndef progress(value, max=100):\n    return HTML(\"\"\"\n        <progress\n            value='{value}'\n            max='{max}',\n            style='width: 100%'\n        >\n            {value}\n        <\/progress>\n    \"\"\".format(value=value, max=max))","29b3ba19":"model = model.to(device)\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=20, path='model.pth'):\n    start = time.time()\n    pb_train = display(progress(0, num_epochs), display_id=True)\n    pb_phase = display(progress(0, 10), display_id=True)\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = math.inf\n    best_acc = 0.\n    \n    for epoch in range(num_epochs):\n        pb_train.update(progress(epoch+1, num_epochs))\n        \n        print('Epoch {}\/{}'.format(epoch+1, num_epochs))\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            running_corrects = 0\n            \n            for i, (inputs, labels) in enumerate(data_loader[phase]):\n                pb_phase.update(progress(i+1, len(data_loader[phase])))\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                if i%1000==999:\n                    print(f\"[{epoch+1}, {i} loss: {running_loss\/(i*inputs.size(0)):.4f}]\")\n                \n                with torch.set_grad_enabled(phase=='train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    if phase=='train':\n                        loss.backward()\n                        optimizer.step()\n                        \n                running_loss += loss.item()*inputs.size(0)\n                running_corrects += torch.sum(preds==labels.data)\n                \n            if phase=='train':\n                scheduler.step()\n            \n            epoch_loss = running_loss\/dataset_sizes[phase]\n            epoch_acc = running_corrects\/dataset_sizes[phase]\n            print('{} loss: {:.4f} acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            \n            if phase=='val' and epoch_acc>best_acc:\n                print(f'New acc: {epoch_acc:.4f}, previous acc: {best_acc:.4f}')\n                best_loss = epoch_loss\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save({'model_state_dict': model.state_dict(),\n                            'optimizer_state_dict': optimizer.state_dict(),\n                            'best_val_loss': best_loss,\n                            'best_val_accuracy': best_acc,\n                            'scheduler_state_dict' : scheduler.state_dict(),\n                            }, path)\n                print(f'New model is SAVED')\n                print()\n    time_elapsed = time.time()-start\n    print('Training complete in: {:.0f}m {:.0f}s'.format(time_elapsed\/\/60, time_elapsed%60))\n    print('Best val acc: {:.4f} Best val loss: {:.4f}'.format(best_acc, best_loss))\n    \n    model.load_state_dict(best_model_wts)\n    return model, best_loss, best_acc","f13198f0":"model, best_val_loss, best_val_acc = train_model(model,\n                                                 criterion,\n                                                 optimizer,\n                                                 scheduler,\n                                                 num_epochs = 20)","f0f001df":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='darkorange',\n         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    \n    plt.show()","a9f16c95":"checkpoint = torch.load('model.pth')\ndef load_model(path):                                \n    model.load_state_dict(checkpoint['model_state_dict'])\n    best_model_wts = copy.deepcopy(model.state_dict())\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    best_loss = checkpoint['best_val_loss']\n    best_acc = checkpoint['best_val_accuracy']\nload_model('model.pth')  \nmodel = model.to(device)","1b0b699e":"since = time.time()\nmodel.eval()\ny_test = []\ny_pred = []\nfor images, labels in data_loader['test']:\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = model(images)\n    _, predictions = outputs.max(1)\n    \n    y_test.append(labels.data.cpu().numpy())\n    y_pred.append(predictions.data.cpu().numpy())\n    \ny_test = np.concatenate(y_test)\ny_pred = np.concatenate(y_pred)\ntime_elapsed = time.time() - since\n\nprint('Testing complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n\nconfusion_mtx = confusion_matrix(y_test, y_pred)\n# plot the confusion matrix\nplot_labels = ['Gliomas', 'Meningioma', 'No_tumor', 'Pituitary']\n\nplot_confusion_matrix(confusion_mtx, plot_labels)\nreport = classification_report(y_test, y_pred, digits=4)\nprint(report)","66e5f058":"# Prediction on Test set"}}