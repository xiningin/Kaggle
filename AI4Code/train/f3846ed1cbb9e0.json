{"cell_type":{"4e2b2515":"code","5e123d18":"code","1230bc8d":"code","19e140dd":"code","e557d470":"code","6338e61b":"code","13eddeb1":"code","7f979a1b":"code","f111673f":"code","babcc59e":"code","f229ee95":"code","32d5f98c":"code","162c5e80":"code","50ffe516":"code","b5cfaee0":"code","c1ed6a6f":"code","0fa622fb":"code","ff5aee91":"code","ded27ae1":"code","e08f91c2":"code","5f293f74":"code","a603f7c8":"code","b2ed25a6":"code","fd4bde89":"code","e9d11c4c":"code","804ebefb":"code","e81fb695":"code","7235d43e":"code","050559c9":"code","a39debd4":"code","8586c61e":"code","facbfd89":"code","903fb8da":"code","713c0e9a":"code","fb9ff40d":"code","a5a1d8f1":"code","afdf4cff":"code","ab52dbd9":"code","673edf84":"code","102cb162":"code","c29313a4":"code","5daaaa35":"code","434bad40":"code","11b69555":"code","5d1f366e":"code","0da4e413":"code","e360798d":"code","10e5ed3e":"code","ca735361":"code","37cda639":"code","1b66f532":"code","ce31e9e4":"markdown","f587d203":"markdown","00e82326":"markdown","39fc051d":"markdown","edb2f8d3":"markdown","ca433d29":"markdown","1aee672d":"markdown","b38fdc74":"markdown","1ab5c722":"markdown","d594bb8a":"markdown","eaa2b7c4":"markdown","ef8ba5dc":"markdown","1267854a":"markdown","abced596":"markdown","14db187c":"markdown","ecc7ddfe":"markdown","989221b9":"markdown","4d9f109d":"markdown","c9a67f8d":"markdown","9b2bde0f":"markdown","24009021":"markdown","f02ea605":"markdown","a8901d3c":"markdown","ce8c5ed3":"markdown"},"source":{"4e2b2515":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport os\nimport os.path\nimport matplotlib.pyplot as plt\n!pip install klib\nimport klib\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, classification_report, average_precision_score\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score,f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","5e123d18":"#input data\nroot_dir = '\/kaggle\/input\/widsdatathon2021\/'\n\nsample_submission = pd.read_csv(os.path.join(root_dir, \n                    'SampleSubmissionWiDS2021.csv'))\nsolution_template = pd.read_csv(os.path.join(root_dir, \n                    'SolutionTemplateWiDS2021.csv'))\nData_dictionary = pd.read_csv(os.path.join(root_dir,   \n                   'DataDictionaryWiDS2021.csv'))\nUnlabled_data = pd.read_csv(os.path.join(root_dir,     \n                'UnlabeledWiDS2021.csv'))\nTraining_data = pd.read_csv(os.path.join(root_dir,     \n                'TrainingWiDS2021.csv'))","1230bc8d":"display(Data_dictionary.shape)\ndisplay(Unlabled_data.shape)\ndisplay(Training_data.shape)\ndisplay(solution_template.shape)","19e140dd":"Training_data.isna().any().any()","e557d470":"Training_data.drop('Unnamed: 0', axis = 1, inplace = True)","6338e61b":"display(Training_data.shape)","13eddeb1":"display(Unlabled_data.shape)\nUnlabled_data.head()","7f979a1b":"Unlabled_data.isna().any().any()","f111673f":"Unlabled_data.drop('Unnamed: 0', axis = 1, inplace = True)","babcc59e":"display(Unlabled_data.shape)\nUnlabled_data.head()","f229ee95":"Training_data.info(verbose=True, null_counts=True)","32d5f98c":"Training_data.dtypes.value_counts()","162c5e80":"Training_data.isnull().sum()","50ffe516":"def calc_missing_values(df_name):\n    \n    '''\n    Returns total number and percentage of missing value in each column of a\n    given dataframe.    \n    '''\n    # sum of missing values in each column\n    missing_values = df_name.isnull().sum() \n    \n    # percentage of missing values in each column\n    per_missing = df_name.isnull().sum() * 100 \/ len(df_name)\n    \n    # Table with sum and percentage of missing values\n    missing_table = pd.concat([missing_values, per_missing],axis = 1)\n        \n    # Assign column names\n    missing_table_rename = missing_table.rename(columns ={0: 'Missing Values', 1:'% of missing values'})\n    \n    # Sort it by percentage of missing values\n    \n    sorted_table = missing_table_rename[missing_table_rename.iloc[:,1] !=0].\\\n    sort_values('% of missing values', ascending = False).round(1)\n    \n    print('Out of ' + str(df_name.shape[1])+ ' columns in this dataframe '+ str(sorted_table.shape[0])+ \\\n                         ' columns have missing values')\n    \n    return sorted_table\n        ","b5cfaee0":"# Training data\nmissing_train = calc_missing_values(Training_data)\nmissing_train[:20].style.background_gradient(cmap='viridis')","c1ed6a6f":"# Test data\nmissing_test = calc_missing_values(Unlabled_data)\nmissing_test[:20].style.background_gradient(cmap='cividis')","0fa622fb":"train_df = klib.data_cleaning(Training_data) # removes duplicate and empty row\/col","ff5aee91":"test_df = klib.data_cleaning(Unlabled_data) # removes duplicate and empty row\/cols#","ded27ae1":"#train_df = Training_data\n#test_df = Unlabled_data","e08f91c2":"train_df['diabetes_mellitus'].value_counts(normalize = True)","5f293f74":"train_df['diabetes_mellitus'].astype(int).plot.hist();","a603f7c8":" train_df.dtypes","b2ed25a6":"cat_col_train = Training_data.select_dtypes('object').columns\ndisplay(len(cat_col_train))\ndisplay(cat_col_train)","fd4bde89":"cat_col_test = Unlabled_data.select_dtypes('object').columns\ndisplay(len(cat_col_test))\ndisplay(cat_col_test)","e9d11c4c":"#klib.cat_plot(test_df)","804ebefb":"cat_list = Training_data.select_dtypes('object').columns\ndisplay(cat_list)","e81fb695":"# Creating Label Encoder object\nle = LabelEncoder()\nfor ob in cat_list:\n    train_df[ob] = le.fit_transform(train_df[ob].astype(str))\n    test_df[ob] = le.fit_transform(test_df[ob].astype(str))\nprint(train_df.info())    \nprint(test_df.info()) ","7235d43e":"train_df.fillna(-9999,inplace = True)\ntrain_df.isnull().sum()","050559c9":"test_df.fillna(-9999,inplace = True)\ntest_df.isnull().sum()","a39debd4":"Target = 'diabetes_mellitus'\ntrain_labels = train_df[Target]\ntrain_df_NT = train_df.drop(columns = [Target])\nfeatures = list(train_df_NT.columns)\nprint('Training data shape:', train_df_NT.shape)\nprint('Test data shape:', test_df.shape)","8586c61e":"X, y = train_df_NT, train_labels","facbfd89":"# Tree-based feature selection\nclf_f = ExtraTreesClassifier(n_estimators=50)\nclf_f = clf_f.fit(X, y)\nmodel = SelectFromModel(clf_f, prefit=True)\nX_new = model.transform(X)\nX_test_new = model.transform(test_df)","903fb8da":"X_new.shape,X_test_new.shape","713c0e9a":"#create the train and validation set for cross-validation\nX_train, X_val, y_train, y_val = train_test_split(X_new, y, test_size=0.2, random_state=123)","fb9ff40d":"#dtrain = xgb.DMatrix(X_train, label=y_train)\n#dval = xgb.DMatrix(X_val, label=y_val)\n#dtest = xgb.DMatrix(X_train_lv)\n","a5a1d8f1":"# parameters\nparam = {\n    'max_depth': 5,\n    'subsample': 0.8,\n    'colsample_bytree': 0.7,\n    'colsample_bylevel': 0.7,\n    'scale_pos_weight': 1,\n    'min_child_weight': 1,\n    'reg_alpha': 4,\n    'n_jobs': 4, \n    'objective': 'binary:logistic',\n    'nthread': 4,\n    'gamma': 0.01,\n    'seed': 27,\n    }\nnum_round =100   # the number of training iterations\n","afdf4cff":"#bst = xgb.train(param, dtrain, num_round)","ab52dbd9":"#to see how the model looks you can also dump it in human readable form\n#bst.dump_model('dump.raw.txt')","673edf84":"#preds = bst.predict(dval)","102cb162":"#For each line you need to select that column where the probability is the highest\n#best_preds = np.asarray([np.argmax(line) for line in preds])","c29313a4":"#from sklearn.metrics import precision_score\n#print(precision_score(y_val, best_preds, average='macro'))","5daaaa35":"xgb_cls = xgb.XGBClassifier(\n    max_depth = 4,\n    subsample = 0.7,\n    colsample_bytree = 0.9,\n    colsample_bylevel = 0.9,\n    scale_pos_weight = 1,\n    min_child_weight = 1,\n    reg_alpha = 4,\n    n_jobs = 4, \n    objective = 'binary:logistic',\n    nthread=20,\n    gamma= 0.01,\n    seed = 27,\n    #n_estimators=1000,\n)","434bad40":"\nxgb_cls.fit(X_train,y_train)\n\ny_pred = xgb_cls.predict_proba(X_val)\n","11b69555":"y_scores = y_pred[:, 1]","5d1f366e":"fpr, tpr, _ = roc_curve(y_val, y_scores)\nroc_auc = auc(fpr, tpr)\naverage_precision = average_precision_score(y_val, y_scores)\nprecision, recall, _ = precision_recall_curve(y_val, y_scores)\nprint(roc_auc)","0da4e413":"xgb_cls_pred = xgb_cls.predict_proba(X_test_new)[:,1]","e360798d":"# plot importance\n#fig, ax = plt.subplots(figsize=(10,10))\n#xgb.plot_importance(bst, max_num_features=20, height=0.5,ax = ax,importance_type='weight')\n#plt.show();","10e5ed3e":"#plot the output tree\n#xgb.plot_tree(bst, num_trees=2);","ca735361":"#ypred = bst.predict(dtest)","37cda639":"#ypred ","1b66f532":"# Submission dataframe\nsubmit = test_df[['encounter_id']]\nsubmit['diabetes_mellitus'] = xgb_cls_pred\nsubmit.to_csv('xgb_cls.csv',index=False)\nsubmit.head()","ce31e9e4":"# Missing  data handling","f587d203":"Test data has 10234 entries and 179 variables which is 1 less than the training data due to the presence of TARGET column.","00e82326":"# Feature selection using SelectFromModel","39fc051d":"This column do not contain useful information so let's drop it.","edb2f8d3":"There are three unique datatypes.","ca433d29":"# Categorical features","1aee672d":"Training data has 130157 entries and 181 variables. ","b38fdc74":"## Training data","1ab5c722":"There are some missing data in training data.","d594bb8a":"## <span style='color:purple'>  Import libraries <\/span>","eaa2b7c4":"# Missing Values","ef8ba5dc":"plot the feature importances and see what features our model think are important for making prediction","1267854a":"### Column types in training and test data","abced596":"# Feature Selection","14db187c":"## Missing values","ecc7ddfe":"Training data has 130157 entries and 180 variables. ","989221b9":"## <span style='color:purple'> Data Exploration <\/span>\n","4d9f109d":"# <span style='color:purple'>  Women in Data Science Datathon 2021       <\/span>\n\n<div style=\"text-align: justify;\n             font-size:18px\">\nObjective of <span style='color:purple'> WiDS Datathon 2021  <\/span> is to develop models and make predictions to determine whether a patient admitted to ICU has been diagnosed with a particular type of diabetes, Diabetes Mellitus, using labeled training data from the first 24 hours of intensive care.\n    <\/div>","c9a67f8d":"## <span style='color:purple'> Input data <\/span>","9b2bde0f":"## Target Column in Training data","24009021":"### Number of unique column datatypes ","f02ea605":"### Columns","a8901d3c":"There is class imbalance in this dataset.","ce8c5ed3":"## Test data"}}