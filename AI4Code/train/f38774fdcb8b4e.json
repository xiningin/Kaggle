{"cell_type":{"9253d2b7":"code","17c0b8a4":"code","02e10fed":"code","c189c999":"code","51c51926":"code","a6de69cf":"code","5d432c82":"code","a63aa2da":"code","abaf6cda":"code","a085be6a":"code","5d8472b7":"code","6aafb672":"code","3589f2b3":"code","c8d07da1":"code","30e0d08a":"code","c06b98fe":"code","afc5e2f8":"code","5f7f31e9":"code","34467c29":"code","9a25b66e":"code","171b46f6":"code","02de4168":"code","cd3a04eb":"code","40bee5b2":"code","79b3fe4f":"code","87e9cb1b":"code","b6166010":"code","42f540a7":"code","11b9f599":"code","ddf159c1":"code","9598680a":"code","41c5986c":"code","1e80520c":"code","b042e043":"markdown","28b8000f":"markdown","6f6aa71d":"markdown","019df732":"markdown","ba373616":"markdown","1b617fb5":"markdown","ea33995e":"markdown","c8a89ef8":"markdown","c7f0415c":"markdown","1c630110":"markdown","cfa59f3d":"markdown","4f5c5eee":"markdown","07d3c0da":"markdown","a93de535":"markdown","9682c5cc":"markdown","9e397f10":"markdown","f5e5e93b":"markdown","a5d36b21":"markdown","1b58d095":"markdown","f623dc83":"markdown","7b13302b":"markdown","ef390422":"markdown","2539e7f8":"markdown","cdd88030":"markdown","e4868a01":"markdown","f029fce0":"markdown","c2e1dc9d":"markdown","b80c7a9d":"markdown","f6fda30f":"markdown"},"source":{"9253d2b7":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split,ShuffleSplit\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('whitegrid')\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import chain","17c0b8a4":"from plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.offline as py\nfrom plotly.graph_objs import Scatter, Layout\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff","02e10fed":"# Read the CSV File Using Pandas read_csv function\ndf = pd.read_csv('..\/input\/data.csv')\ndf.T.head(50)","c189c999":"# print the concise summery of the dataset\ndf.info()","51c51926":"#since the dataset can also contain null values\n#count total rows in each column which contain null values\ndf.isna().sum()","a6de69cf":"#'duplicated()' function in pandas return the duplicate row as True and othter as False\n#for counting the duplicate elements we sum all the rows\nsum(df.duplicated())","5d432c82":"#deleting useless columns\n#deleting the \"id\" column\ndf.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)","a63aa2da":"p = df.describe().T\np = p.round(4)\ntable = go.Table(\n    columnwidth=[0.8]+[0.5]*8,\n    header=dict(\n        values=['Attribute'] + list(p.columns),\n        line = dict(color='#506784'),\n        fill = dict(color='lightblue'),\n    ),\n    cells=dict(\n        values=[p.index] + [p[k].tolist() for k in p.columns[:]],\n        line = dict(color='#506784'),\n        fill = dict(color=['rgb(173, 216, 220)', '#f5f5fa'])\n    )\n)\npy.iplot([table], filename='table-of-mining-data')","abaf6cda":"B, M = df['diagnosis'].value_counts()\ns = [B,M]\nprint(df['diagnosis'].value_counts())\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar([0,1], s,align='center',\n            label='Count')\n    plt.ylabel('Count')\n    plt.xlabel('Target')\n    plt.legend(loc='best')\n    plt.tight_layout()","a085be6a":"data_dia = df['diagnosis']\ndata = df.drop('diagnosis',axis=1)\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([df['diagnosis'],data_n_2.iloc[:,0:15]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(14,5))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=45,fontsize=13)","5d8472b7":"mean_col = [col for col in df.columns if col.endswith('_mean')]\nfor i in range(len(mean_col)):\n    sns.FacetGrid(df,hue=\"diagnosis\",aspect=3,margin_titles=True).map(sns.kdeplot,mean_col[i],shade= True).add_legend()\n    #ax.set_title('lalala')","6aafb672":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\n#cmap = sns.diverging_palette( 240 , 10 , as_cmap = True )\nsns.heatmap(df.corr(), cmap='Blues',annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.xticks(fontsize=11,rotation=70)\nplt.show()","3589f2b3":"from pylab import rcParams\nrcParams['figure.figsize'] = 8,5\ncols = ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean','diagnosis']\nsns_plot = sns.pairplot(data=df[cols],hue='diagnosis')","c8d07da1":"palette ={'B' : 'lightblue', 'M' : 'gold'}\nedgecolor = 'grey'\n\n# Plot +\nfig = plt.figure(figsize=(12,12))\nsns.set_style('whitegrid')\nsns.color_palette(\"bright\")\ndef plot_scatter(a,b,k):\n    plt.subplot(k)\n    sns.scatterplot(x = df[a], y = df[b], hue = \"diagnosis\",\n                    data = df, palette = palette, edgecolor=edgecolor)\n    plt.title(a + ' vs ' + b,fontsize=15)\n    k+=1\n\n    \nplot_scatter('perimeter_mean','radius_worst',221)   \nplot_scatter('area_mean','radius_worst',222)   \nplot_scatter('texture_mean','texture_worst',223)   \nplot_scatter('area_worst','radius_worst',224)  ","30e0d08a":"fig = plt.figure(figsize=(12,12))\nplot_scatter('smoothness_mean','texture_mean',221)\nplot_scatter('radius_mean','fractal_dimension_worst',222)\nplot_scatter('texture_mean','symmetry_mean',223)\nplot_scatter('texture_mean','symmetry_se',224)","c06b98fe":"fig = plt.figure(figsize=(12,12))\nplot_scatter('area_mean','fractal_dimension_mean',221)\nplot_scatter('radius_mean','fractal_dimension_mean',222)\nplot_scatter('area_mean','smoothness_se',223)\nplot_scatter('smoothness_se','perimeter_mean',224)","afc5e2f8":"plt.style.use('ggplot')\nsns.set_style('whitegrid')\nplt.figure(figsize=(16,6))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,palette='Set1')\nplt.xticks(rotation=40)","5f7f31e9":"from collections import Counter\n\ndef detect_outliers(train_data,n,features):\n    outlier_indices = []\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(train_data[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(train_data[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR\n        outlier_list_col = train_data[(train_data[col] < Q1 - outlier_step) | (train_data[col] > Q3 + outlier_step )].index\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers\nlist_atributes = df.drop('diagnosis',axis=1).columns\nOutliers_to_drop = detect_outliers(df,2,list_atributes)","34467c29":"df.loc[Outliers_to_drop]","9a25b66e":"# Drop outliers\ndf = df.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","171b46f6":"df.shape","02de4168":"group_map = {\"M\": 1, \"B\": 0}\n\ndf['diagnosis'] = df['diagnosis'].map(group_map)","cd3a04eb":"target_pca = pd.DataFrame(df['diagnosis'])\ndata_pca = df.drop('diagnosis', axis=1)\n\n#To make a PCA, normalize data is essential\nX_pca = data_pca.values\nX_std = StandardScaler().fit_transform(X_pca)\n\npca = PCA(svd_solver='full')\npca_std = pca.fit(X_std, target_pca).transform(X_std)\n\npca_std = pd.DataFrame(pca_std)\npca_std = pca_std.merge(target_pca, left_index = True, right_index = True, how = 'left')","40bee5b2":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,5))\nplt.plot(np.cumsum(pca.explained_variance_ratio_),marker='o')\nplt.xlim(0,30,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.title('Cumulative Sum of Variance Vs Number Of Components')\nplt.show()","79b3fe4f":"var_pca = pd.DataFrame(pca.explained_variance_ratio_)\nlabels = []\nfor i in range(1,31):\n    labels.append('Col_'+str(i))\ntrace = go.Pie(labels = labels, values = var_pca[0].values, opacity = 0.8,\n               textfont=dict(size=15))\nlayout = dict(title =  'PCA : components and explained variance')\nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","87e9cb1b":"pca.explained_variance_ratio_","b6166010":"X = df.drop('diagnosis',axis=1).values\ny = df['diagnosis'].values\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\n\n#X = pd.DataFrame(preprocessing.scale(X))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","42f540a7":"from sklearn import metrics\ndef plot_confusion_metrix(y_test,model_test):\n    cm = metrics.confusion_matrix(y_test, model_test)\n    plt.figure(1)\n    plt.clf()\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n    classNames = ['Benign','Malignant']\n    plt.title('Confusion Matrix')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames)\n    plt.yticks(tick_marks, classNames)\n    s = [['TN','FP'], ['FN', 'TP']]\n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n    plt.show()\n    \nfrom sklearn.metrics import roc_curve, auc\ndef report_performance(model):\n\n    model_test = model.predict(X_test)\n\n    print(\"\\n\\nConfusion Matrix:\")\n    print(\"{0}\".format(metrics.confusion_matrix(y_test, model_test)))\n    print(\"\\n\\nClassification Report: \")\n    print(metrics.classification_report(y_test, model_test))\n    #cm = metrics.confusion_matrix(y_test, model_test)\n    plot_confusion_metrix(y_test, model_test)\n\ndef roc_curves(model):\n    predictions_test = model.predict(X_test)\n    fpr, tpr, _ = roc_curve(predictions_test,y_test)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \ndef accuracy(model):\n    pred = model.predict(X_test)\n    accu = metrics.accuracy_score(y_test,pred)\n    print(\"\\nAcuuracy Of the Model: \",accu,\"\\n\\n\")","11b9f599":"for i in ['linear','rbf']:\n    clf = SVC(kernel=i)\n    clf.fit(X_train,y_train)\n    print(\"On \"+ i + \" kernel:\" )\n    report_performance(clf)\n    roc_curves(clf)\n    accuracy(clf)","ddf159c1":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, scoring=None, obj_line=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    from sklearn.model_selection import learning_curve\n    from matplotlib import pyplot as plt\n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std  = np.std(train_scores, axis=1)\n    test_scores_mean  = np.mean(test_scores, axis=1)\n    test_scores_std   = np.std(test_scores, axis=1)\n    plt.grid(True)\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Testing score\")\n    plt.grid(True)\n    if obj_line:\n        plt.axhline(y=obj_line, color='blue')\n\n    plt.legend(loc=\"best\")\n","9598680a":"cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = SVC(kernel='linear')\nplot_learning_curve(estimator, 'Kernel = Linear', X, y, cv=cv)\nestimator = SVC(kernel='rbf')\nplot_learning_curve(estimator, 'kernel = RBF', X, y, cv=cv)","41c5986c":"fig = plt.figure(figsize=(16,5))\ndef plotlc(kernel=None,k=0):\n    plt.subplot(k)\n    cp = np.arange(1, 11)\n    train_accuracy = np.empty(len(cp))\n    test_accuracy = np.empty(len(cp))\n    for i, c in enumerate(cp):\n        clf = SVC(C=c,kernel = kernel)\n        clf.fit(X_train, y_train)\n        train_accuracy[i] = clf.score(X_train, y_train)\n        test_accuracy[i] = clf.score(X_test, y_test)\n\n        #plt.figure(figsize=(10,5))\n    plt.title('Learning curves for SVM( '+ kernel+' ): Varying Value of C', size=15)\n    plt.plot(cp, test_accuracy, marker ='o', label = 'Testing Accuracy')\n    plt.plot(cp, train_accuracy, marker ='o', label = 'Training Accuracy')\n    plt.legend(prop={'size':13})\n    plt.xlabel('Value of C (c)', size=13)\n    plt.ylabel('Accuracy', size=13)\n    plt.xticks(cp);\n#plt.show()\n\nplotlc('linear',121)\nplotlc('rbf',122)","1e80520c":"clf = SVC(kernel='rbf',C=1)\nclf.fit(X_train,y_train)\nreport_performance(clf)\nroc_curves(clf)\naccuracy(clf)","b042e043":"<a id='iqr'><\/a>\n> ### **Remove Outliers Using IQR**","28b8000f":"> Lets interpret the plot above together. For example, in texture_mean feature, median of the Malignant and Benign looks like separated so it can be good for classification. However, in fractal_dimension_mean feature, median of the Malignant and Benign does not looks like separated so it does not gives good information for classification","6f6aa71d":"<center><span>&#8226;<\/span>&nbsp; &nbsp;<a href='https:\/\/www.github.com\/deepak525'>Github<\/a>&nbsp; &nbsp;&nbsp; &nbsp;<span>&#8226;<\/span>&nbsp; &nbsp;<a href='https:\/\/www.kaggle.com\/deepak525'>Kaggle<\/a>&nbsp; &nbsp;&nbsp; &nbsp;<span>&#8226;<\/span>&nbsp; &nbsp;<a href='https:\/\/www.linkedin.com\/in\/deepak525\/'>Linkdin<\/a><center>","019df732":"<a id='pca'><\/a>\n## **Principal Component Analysis(PCA)**","ba373616":"<a id='sp'><\/a>\n> ### **Building Feature Set**<br>\n-  Preparing data for model building and checking. 30 percent of data is kept aside for validation purposes. We will also be performing scaling of data using sklearn's StandardScaler.","1b617fb5":"<a id='pac'><\/a>\n## Importing Packages and Loading Data","ea33995e":"<a id='toc'><\/a>\n# Table of Content\n\n* [Introduction](#intro)\n* [Importing packages and loading data](#pac)\n* [Data cleaning and data wrangling](#clean)\n* [Exploratory Data Analysis (EDA)](#eda)\n    * [Basic Statistical Details](#bsd)\n    * [Violin plot of features by diagnosis](#vpf)\n    * [Kde Plot For Each Mean Feature](#kde)\n    * [Relationship Between Features](#rbf)\n        * [Correlation Heatmap](#chm)\n        * [Feature Pair](#fp)\n        * [Positively Correlated Features](#pcf)\n        * [Un-Correlated Features](#ucf)\n        * [Negatively Correlated Features](#ncf)\n* [Statistical Analysis(Outliers)](#sa)\n    * [Box Plot](#bp)\n    * [Remove Outliers Using IQR](#iqr)\n* [Principal Component Analysis(PCA)](#pca)\n* [Machine Learning Classification](#ml)\n    * [Building Feature Set](#sp)\n    * [Support Vector Machine (SVM)](#svm)\n        * [Kernel Selection Using Learning Curve ](#ks)\n        * [Selection of Regularization parameter(C)](#srp)\n        * [Confusion Metrix and ROC Curve](#cmrc)\n* [Summary of models performance](#sum)","c8a89ef8":"<a id='srp'><\/a>\n> #### **Selection of Regularization parameter(C)**\n> - C is a regularization parameter that controls the trade off between the achieving a low training error and a low testing error that is the ability to generalize your classifier to unseen data. Consider the objective function of a linear SVM ","c7f0415c":"<a id='intro'><\/a>\n## Introduction<br>\n\nThe Breast Cancer (Wisconsin) Diagnosis dataset contains the diagnosis and a set of 30 features describing the characteristics of the cell nuclei present in the digitized image of a of a fine needle aspirate (FNA) of a breast mass.\n<br><br>\nTen real-valued features are computed for each cell nucleus:\n\n- radius (mean of distances from center to points on the perimeter);\n- texture (standard deviation of gray-scale values);\n- perimeter;\n- area;\n- smoothness (local variation in radius lengths);\n- compactness (perimeter^2 \/ area - 1.0);\n- concavity (severity of concave portions of the contour);\n- concave points (number of concave portions of the contour);\n- symmetry;\n- fractal dimension (\u201ccoastline approximation\u201d - 1).\n\n<br><br>\nThe mean, standard error (SE) and \u201cworst\u201d or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features.\n<br><br>\nWe will analyze the features to understand the predictive value for diagnosis. We will then create models using two different algorithms and use the models to predict the diagnosis.","1c630110":"<a id='ks'><\/a>\n> #### **Kernel Selection Using Learning Curve**","cfa59f3d":"<a id='ml'><\/a>\n## **Machine Learning Classification**","4f5c5eee":"> **observation:**  <br> \n- the radius, parameter and area are highly correlated as expected from their relation so from these we will use anyone of them\n- compactness_mean, concavity_mean and concavepoint_mean are highly correlated so we will use compactness_mean from here\n- so selected Parameter for use is perimeter_mean, texture_mean, compactness_mean, symmetry_mean","07d3c0da":"<a id='kde'><\/a>\n> ### **Kde Plot For Each Mean Feature** ","a93de535":"<a id='vpf'><\/a>\n> ### **Violin Plot of Features by Diagnosis**","9682c5cc":"<a id='rbf'><\/a>\n## **Relationship Between Features**  <br>\n> - We can say that two variables are related with each other, if one of them gives information about others\n- For example, price and distance. If you go long distance with taxi you will pay more. There fore we can say that price and distance are positively related with each other.\n- Scatter Plot:  Simplest way to check relationship between two variables\n- Lets look at relationship between radius mean and area mean. In scatter plot you can see that when radius mean increases, area mean also increases. Therefore, they are positively correlated with each other.\n- There is no correlation between area mean and fractal dimension se. Because when area mean changes, fractal dimension se is not affected by chance of area mean","9e397f10":"<a id='ncf'><\/a>\n> ### **Negatively Correlated Features**","f5e5e93b":"<div style=\"background: linear-gradient(to bottom, #200122, #6f0000); border: 2px; box-radius: 20px\"><h1 style=\"color: white; text-align: center\"><br> <center>Breast Cancer<center><br><\/h1><\/div>\n","a5d36b21":"![](http:\/\/)<a id='pcf'><\/a>\n> ### **Positively Correlated Features**","1b58d095":"<a id='fp'><\/a>\n> ### **Feature Pair**","f623dc83":"<a id='data'><\/a>\n### **Load the Data**","7b13302b":"<a id='bsd'><\/a>\n> ### **Basic Statistical Details**","ef390422":"<a id='sa'><\/a>\n\n## **Statistical Analysis(Outliers Detection)** <br>\n\n> While looking histogram as yok can see there are rare values in bening distribution (green in graph) <br>\nThere values can be errors or rare events.   <br>\nThese errors and rare events can be called outliers.  <br>\nCalculating outliers:   \n- first we need to calculate first quartile (Q1)(25%)  <br>\n- then find IQR(inter quartile range) = Q3-Q1  <br>\n- finally compute Q1 - 1.5IQR and Q3 + 1.5IQR   <br>\n- Anything outside this range is an outlier   <br>","2539e7f8":"<a id='clean'><\/a>\n## Data cleaning and data wrangling","cdd88030":"<a id='eda'><\/a>\n## **Exploratory Data Analysis (EDA)**","e4868a01":"<a id='svm'><\/a>\n> ### **Support Vector Machine(SVM)**","f029fce0":"[](http:\/\/)<a id='bp'><\/a>\n> ### **Box Plot**","c2e1dc9d":"> #### **Optimal Model**","b80c7a9d":"<a id='chm'><\/a>\n> ### **Correlation Heatmap**","f6fda30f":"<a id = 'ucf'><\/a>\n> ### **Un-Correlated Features** "}}