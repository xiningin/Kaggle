{"cell_type":{"2598d970":"code","366c8a4b":"code","2fb5ddb3":"code","2d680826":"code","782ff551":"code","0e522590":"code","d2f22077":"code","28cefc86":"code","286ac6ca":"code","6a66260e":"code","1955b025":"code","1a5bb527":"code","cf0b3906":"code","55d33c45":"code","bffa2e8c":"code","0ff416c6":"code","01c76fe4":"code","de3e2dc2":"code","aec3053b":"code","6046e2fb":"code","12a4343c":"code","8aa8dbf2":"code","9a938025":"code","b1105d73":"code","76673628":"code","94370fa0":"code","27cfd9e4":"markdown","dd5487f0":"markdown","96c6df32":"markdown","cc541ff9":"markdown","9bc9ee11":"markdown","01669d89":"markdown","42061bfc":"markdown","89d6eebb":"markdown","758060d8":"markdown","6c0bebde":"markdown","ce1e3c9a":"markdown","394c5a2a":"markdown","72adc945":"markdown","bcfd622c":"markdown","c6f3d2de":"markdown","6fe2f38a":"markdown","8fd240f0":"markdown","d35e518a":"markdown","a034f076":"markdown","a6665e9a":"markdown","0fc70574":"markdown","d12a7e98":"markdown","714942f1":"markdown","bd2657b4":"markdown","6fe35896":"markdown","c8d89546":"markdown","bfdc1f9f":"markdown","a57873a4":"markdown"},"source":{"2598d970":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport os\n\nfrom pandas import Series, DataFrame\nfrom pylab import rcParams\nfrom sklearn import preprocessing\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","366c8a4b":"met_df = pd.read_csv('\/kaggle\/input\/did-it-rain-in-seattle-19482017\/seattleWeather_1948-2017.csv')\nprint(met_df.head()); print(); print()\nmet_df.info()","2fb5ddb3":"met_df.describe(include = 'all')","2d680826":"met_df.isna().sum()","782ff551":"P_median = met_df.PRCP.median()\nR_mode   = met_df.RAIN.mode()[0]\n\nmet_df.PRCP.fillna(P_median, inplace = True)\nmet_df.RAIN.fillna(R_mode, inplace = True)\n\nmet_df.isna().sum()","0e522590":"from sklearn.preprocessing import LabelEncoder\nRAIN_encode = LabelEncoder().fit_transform(met_df.RAIN)\nRAIN_encode","d2f22077":"met_df['RAIN'] = RAIN_encode\n\nmet_df.describe(include = 'all')","28cefc86":"date = pd.to_datetime(met_df.DATE, format=\"%Y-%m-%d\")\nmet_df['DATE'] = date\nmet_df.head()","286ac6ca":"## T_RATIO\nT_RATIO =  met_df.TMIN \/ met_df.TMAX\nT_RATIO[T_RATIO > 1]  = 1 #correcting an error in initial data where TMIN > TMAX\n\n## PRCP in the past n days (here, n = 3)\nn = 3\nPRCP_n = np.empty(len(met_df.PRCP))\nPRCP_n[:] = 0\nPRCP_n = [sum(met_df.PRCP.values[i-n:i]) \/ n for i in range(n, len(met_df.PRCP.values))]\n\n## daily PRCP\nfrom datetime import datetime\nDoY_str = met_df.DATE.dt.strftime('%j')\nDay_of_Year = [int(a) for a in DoY_str]\n#group data based on \"day of year\"\ngroupD     = met_df.groupby(Day_of_Year)\nDoY_PRCP   = groupD['PRCP'].mean()  # daily climatological mean PRCP\ndaily_PRCP = [DoY_PRCP[a] for a in Day_of_Year]","6a66260e":"# Add all these new variables to DataFrame:\nT_RATIO_df     = pd.DataFrame(T_RATIO,     columns = ['T_RATIO'])\nPRCP_n_df      = pd.DataFrame(PRCP_n,      columns = ['PRCP_n'])\ndaily_PRCP_df  = pd.DataFrame(daily_PRCP,  columns = ['daily_PRCP'])\n\nmet_new_df = pd.concat([met_df['DATE'],met_df['PRCP'],PRCP_n_df,daily_PRCP_df,\n                        met_df['TMAX'],met_df['TMIN'],T_RATIO_df,met_df['RAIN']], axis = 1)\n\nmet_new_df.PRCP_n.fillna(met_new_df.PRCP.median(), inplace = True)\n\nmet_new_df.describe(include = 'all')","1955b025":"%matplotlib inline\nrcParams['figure.figsize'] = 10, 8\nsb.set_style('whitegrid')\n\nsb.pairplot(met_new_df, palette = 'husl', hue = 'RAIN')\nplt.show()","1a5bb527":"rcParams['figure.figsize'] = 10, 8\nsb.heatmap(met_new_df.corr(), vmin=-1, vmax=1, annot=True, cmap = 'RdBu_r')\nplt.show()","cf0b3906":"fig, axis = plt.subplots(2, 2,figsize=(12,10))\nsb.scatterplot(x = 'TMIN', y ='TMAX', data = met_new_df, hue = 'RAIN', ax = axis[0,0])\nsb.scatterplot(x = 'daily_PRCP', y ='T_RATIO', data = met_new_df, hue = 'RAIN', ax = axis[0,1])\nsb.scatterplot(x = 'daily_PRCP', y ='TMAX', data = met_new_df, hue = 'RAIN', ax = axis[1,0])\nsb.scatterplot(x = 'daily_PRCP', y ='TMIN', data = met_new_df, hue = 'RAIN', ax = axis[1,1])\nplt.show()","55d33c45":"fig, axis = plt.subplots(1, 3,figsize=(15,5))\nsb.boxplot(x = 'RAIN', y ='TMAX', data = met_new_df, ax = axis[0], showfliers = False, palette = 'hls')\nsb.boxplot(x = 'RAIN', y ='TMIN', data = met_new_df, ax = axis[1], showfliers = False, palette = 'hls')\nsb.boxplot(x = 'RAIN', y ='T_RATIO', data = met_new_df, ax = axis[2], showfliers = False)\nplt.show()","bffa2e8c":"fig, axis = plt.subplots(1, 3,figsize=(15,5))\nsb.boxplot(x = 'RAIN', y ='PRCP', data = met_new_df, ax = axis[0], showfliers = False, palette = 'husl')\nsb.boxplot(x = 'RAIN', y ='daily_PRCP', data = met_new_df, ax = axis[1], showfliers = False)\nsb.boxplot(x = 'RAIN', y ='PRCP_n', data = met_new_df, ax = axis[2], showfliers = False)\nplt.show()","0ff416c6":"met_new_df.drop(['DATE','PRCP','TMIN','TMAX'], inplace = True, axis=1)\nmet_new_df.head()","01c76fe4":"X_train, X_test, Y_train, Y_test = train_test_split(met_new_df.drop('RAIN', axis=1),\n                                                   met_new_df['RAIN'], test_size=0.2, random_state=10)                             \n","de3e2dc2":"all_classifiers = {'Gradient Boost': GradientBoostingClassifier(),\n                 'Ada Boost': AdaBoostClassifier(),\n                 'Random Forest': RandomForestClassifier(n_estimators=50, min_samples_leaf=2, min_samples_split=4, max_depth=6),\n                 'Logistic Regression': LogisticRegression(),\n                 'Decision Tree' : DecisionTreeClassifier(),\n                 'KNN': KNeighborsClassifier(),\n                 'Gaussian NB': GaussianNB(),\n                 'Beroulli  NB': BernoulliNB(),\n                  'SVC': SVC(probability = True)} ","aec3053b":"ML_name = []\nML_accuracy = []\nfor Name,classifier in all_classifiers.items():\n    classifier.fit(X_train,Y_train)\n    Y_pred = classifier.predict(X_test)\n    ML_accuracy.append(metrics.accuracy_score(Y_test,Y_pred)) \n    ML_name.append(Name) ","6046e2fb":"rcParams['figure.figsize'] = 8, 4\nplt.barh(ML_name, ML_accuracy, color = 'purple')\nplt.xlabel('Accuracy Score', fontsize = '14')\nplt.ylabel('Machine Learning Algorithms', fontsize = '14')\nplt.xlim([0.7, 0.84])\nplt.show()","12a4343c":"solve       = ['liblinear', 'sag', 'lbfgs']\nfit_interc  = [True, False]\ninterc_scal = [1, 2, 3]\nccc         = [1, 2, 3]\n\nmax_score = 0\n\nfor so in solve:\n    for fi in fit_interc:\n        for ins in interc_scal:\n            for c in ccc:\n                MLA = LogisticRegression(solver=so, fit_intercept=fi, intercept_scaling=ins, C=c)\n                MLA.fit(X_train,Y_train)\n                Y_pred = MLA.predict(X_test)\n                if metrics.accuracy_score(Y_test,Y_pred) > max_score:\n                    max_score, so_best, fi_best, ins_best, c_best = metrics.accuracy_score(Y_test,Y_pred), so, fi, ins, c\n\nprint('maximum accuracy score, solver, fit_intercept, intercept_scaling, C:')\nprint(max_score, so_best, fi_best, ins_best, c_best)","8aa8dbf2":"criteri       = ['gini', 'entropy']\nmin_samp_lf   = [1, 2, 5, 10]\nmin_samp_splt = [2, 4, 8, 12]\nmaxim_depth   = [2, 4, 8, 12, None]\n\nmax_score = 0\n\nfor c in criteri:\n    for ml in min_samp_lf:\n        for ms in min_samp_splt:\n            for md in maxim_depth:\n                MLA = DecisionTreeClassifier(criterion=c, min_samples_leaf=ml, min_samples_split=ms, max_depth=md)\n                MLA.fit(X_train,Y_train)\n                Y_pred = MLA.predict(X_test)\n                if metrics.accuracy_score(Y_test,Y_pred) > max_score:\n                    max_score, c_best, l_best, s_best, d_best = metrics.accuracy_score(Y_test,Y_pred), c, ml, ms, md\n\n\n\nprint('maximum accuracy score, criterion, min_samples_leaf, min_samples_split, max_depth:')\nprint(max_score, c_best, l_best, s_best, d_best)","9a938025":"learn_r       = [1, 2, 3]\nmin_samp_lf   = [1, 2, 5]\nmin_samp_splt = [2, 4, 8]\nmaxim_depth   = [3, 5, 10]\n\nmax_score = 0\n\nfor c in learn_r:\n    for ml in min_samp_lf:\n        for ms in min_samp_splt:\n            for md in maxim_depth:\n                MLA = GradientBoostingClassifier(learning_rate=c, min_samples_leaf=ml, min_samples_split=ms, max_depth=md)\n                MLA.fit(X_train,Y_train)\n                Y_pred = MLA.predict(X_test)\n                if metrics.accuracy_score(Y_test,Y_pred) > max_score:\n                    max_score, c_best, l_best, s_best, d_best = metrics.accuracy_score(Y_test,Y_pred), c, ml, ms, md\n\n\nprint('maximum accuracy score, learning_rate, min_samples_leaf, min_samples_split, max_depth:')\nprint(max_score, c_best, l_best, s_best, d_best)","b1105d73":"MLA = DecisionTreeClassifier(criterion='gini', min_samples_leaf=10, min_samples_split=2, max_depth=8)\nMLA.fit(X_train,Y_train)\nY_pred = MLA.predict(X_test)\nprint(metrics.classification_report(Y_test, Y_pred))","76673628":"CV_scores = cross_val_score(MLA, X_train, Y_train, cv=5)\nprint ('5-fold cross-validation: scores = ')\nprint(CV_scores)","94370fa0":"Y_train_pred = cross_val_predict(MLA, X_train, Y_train)\nrcParams['figure.figsize'] = 5, 4\nsb.heatmap(confusion_matrix(Y_train, Y_train_pred), annot=True, cmap='Purples')\nplt.show()","27cfd9e4":"- The result of confusion matrix is good when the values on the upper left and lower right are high and other values are low, which is the case here.\n- It shows that the cases of false predictions for NOT RAIN (2300) and RAIN (1200) are much lower than cases of true predictions for NOT RAIN (9500) and RAIN (7500).\n- This verifies that there is no systematic error that our MLA always predicts NOT RAIN when there is in fact RAIN.","dd5487f0":"## Data Cleaing:\n\n### Step 1: Correcting wrong values or outliers:","96c6df32":"- The left panel reinforces why PRCP should be dropped as a predictor. When precipitation amount is zero, certainly it is not rainy.\n- When the average of PRCP for today in all previous years is low, there is a good chance that today will not be rainy, and vice versa.\n- When the PRCP was low in the past n days (here, n = 3), it is very likely that today will not be rainy, and vice versa.\n---\n\n- So, we keep T_RATIO, PRCP_n, and daily_PRCP as predictors, and drop other variables (except RAIN, which is the outcome).","cc541ff9":"### Reading the data:","9bc9ee11":"# An Analysis regarding Rain Forecast in Seattle provided by an Atmospheric Scientist\n\n## Objective:\n\n- The input dataset (seattleWeather_1948-2017.csv) provides information on a few Meteorological variables in Seattle from 1948 to 2017. The customer would like to know the likelihood of predicting whether it rains or not on a specific day by using MLAs.\n\n- The input variables (predictors) are Date, PRCP, TMAX, and TMIN. The last three are numerical continuous variables.\n\n- The outcome or dependent variable is: RAIN (boolean)\n\n- I will use knowledge and experience in the field of Atmospheric Science to improve the methodology, but an in-depth use of science is not possible in this framework.\n\n- In part 1, I  only used the available predictors (as they are) and dropped the ones that are not independent. That way, I reached an accuracy score of 68%.\n\n- Here, (part 2), I will perform advanced feature engineering and will extract new variables based on the available predictors in order to improve the accuracy score.\n\n----\nVariable 'PRCP' (precipitation amount) should be used with caution:\n\nCorrect use of PRCP as a predictor: \n\n1) We know PRCP in the last few days (e.g. 1-3 Jan.). How can that help predict 'RAIN' (binary rain: did it rain or not) for today (e.g. 4 Jan.)?\n\n2) We know the PRCP for 4 Jan. for all the previous years 1948-2017. Can we use this data to predict 'RAIN' for 4 Jan. 2018?\n\nWrong use of PRCP:\n\nWe know PRCP for 4 Jan. 2018. Can we use it to predict 'RAIN' for 4 Jan. 2018?\n\nThis is cheating: MLA would provide a perfect answer when the outcome is included among the predictors: when MLA knows the amount of precipitation on a specific day, it can certainly say whether it rains or not. \n\nNote that the other solutions to this problem, that I found on this website, did this mistake and kept the variable PRCP to predict RAIN. This motivated me to provide a better methodology.\n\n \n\n### Importing important libraries:","01669d89":"### Logistic Regression:","42061bfc":"### Decision Tree:","89d6eebb":"The description and unit of each variable:\n- DATE = the date of the observation\n- PRCP = the amount of precipitation, in inches\n- TMAX = the maximum temperature for that day, in degrees Fahrenheit\n- TMIN = the minimum temperature for that day, in degrees Fahrenheit\n- RAIN = TRUE if rain was observed on that day, FALSE if it was not","758060d8":"## Discussion:\n\n- After feature engineering and tuning hyper-parameters for various MLAs, we reach an accuracy score of 83.5%, which means that our MLAs at best can predict rain correctly in 83.5% of times for the test dataset.\n\n- In part 1, we showed that an accuracy score of always selecting NOT RAIN was 57%, and an accuracy score of MLAs with no feature engineering was 68%. Here, we show that feature engineering improves accuracy score by 15.5%, and MLA with feature engineering leads to a total improvement of 26.6%. This seems very good considering the complex nature of weather and limited datasets and features.\n\n- Again I should mention the drawback in other solutions for this problem: they claimed they reached an accuracy of higher than 90%. This is an artifact and mistake because they did not drop the 'PRCP' variable from the predictor list when using MLAs.\n\n----\n\n### Future Work:\n\n- Can we predict the precipitation amount (PRCP) from TMAX and TMIN? My first guess is such a model would have a low score. There is a reason for super complex numerical weather prediction (NWP) models that use thousands of processes and mathematical models.\n\n-  Note that the limited number of predictors and the highly uncertain and non-linear nature of weather make it very difficult to predict PRCP accurately. However, I have some ideas to maximize climatological prediction, and that will be discussed in part 3.","6c0bebde":"- As expected, TMAX and TMIN are not independent: this means that there are factors such as season, weather systems and sky cloudiness that affect both TMAX and TMIN. Since TMAX and TMIN are dependent features, we should drop one of them.\n- Also, TMAX and TMIN are highly correlated with the newly-extracted feature (daily_PRCP), due to the fact that both TMAX and TMIN contain information on seasonality. T_RATIO, on the other hand, is not associated with seasonality, and therefore shows no relationship with daily_PRCP.\n- Since we extracted T_RATIO from TMAX and TMIN, and T_RATIO and daily_PRCP are independent, we keep these two and drop both TMAX and TMIN.","ce1e3c9a":"### Step 5: feature engineering:\n\n- Now, I am going to use subject matter expertise to extract new features (based on available data).\n- TMAX and TMIN are highly correlated, and at least one of them should be dropped, but is there a way to deduce new variables that have a higher correlation with RAIN and at the same time keep information from both TMAX and TMIN? I can think of (T_DIFF = TMAX - TMIN); and (T_RATIO = TMAX \/ TMIN). These new variables should have a higher correlation with 'RAIN', because when it rains there is less fluctuation in the daily change of Temperature, and TMAX and TMIN get closer. Therefore, T_DIFF (T_RATIO) becomes smaller (larger). This theory will be tested later. I will only use T_RATIO in this demo.\n- It takes about a week for a meteorological weather system to pass over a certain location. Consider a system is causing rain over Seattle for a week. I simply assume the average state, meaning that today is the 4th rainy day due to this system. In other words, the last 3 days were rainy and the next 3 days will also be rainy. So, I conclude that if I know the PRCP in the past 3 days, I can predict the RAIN for today. Therefore, I extract a variable \"average precipitation in the past n days\" (PRCP_n) where n = 3. For example, I know the average PRCP for 1-3 Jan. 2017. What is the predicted RAIN for Jan. 4 2017?\n- Imagine today is 4 Jan. 2018. If 4 Jan. was rainy in all previous years, there is a good chance that it will rain today. I would like to use average PRCP for 4 Jan. from 1948 to 2017 and predict RAIN for today. So, I calculate a new variable daily_PRCP (average PRCP on a specific day for all years).","394c5a2a":"- Both TMAX and TMIN have lower values during rain, because cloud cover blocks solar radiation and causes a decrease in surface warming.\n- T_RATIO is larger during rain, because the diurnal fluctuaion of temperature is smaller in cloudy days and TMIN and TMAX are closer.\n- TMAX predicts RAIN better than TMIN does, and T_RATIO predicts RAIN much better than the other two.","72adc945":"- The classification report shows that the NOT RAIN accuracy score is higher than RAIN accuracy score by 3%.\n- In our test set, the number of NOT RAIN cases is ~ 1.3 times the number of RAIN cases.","bcfd622c":"### Step 3: Converting boolean variable to dummy variable:\n- We should change RAIN from True\/False to 1\/0.\n- We then replace the new variable with the original one.","c6f3d2de":"The data description makes sense, and the mean, min, and max values of each variable is reasonable meaning there should not be a mistake in the data (such as a very large temperature of 200 F).","6fe2f38a":"- It seems that the new variabels extracted via feature engineering have high correlation with RAIN.\n- Compared to TMAX or TMIN, T_RATIO (equals to TMAX \/ TMIN) has higher correlation with RAIN.\n- PRCP_n (average precipitation in the past n days, here n = 3) and daily_PRCP (average PRCP on a specific day for all years) also have relatively high correlations with RAIN.","8fd240f0":"### K-fold cross-validation and confusion matrices:\n\nWe use this validation to make sure the accuracy score is not dependent on the train-test split. In other words, we want to verify that the train-test split is random.","d35e518a":"So, the scores from cross-validation do not vary much, and range between 0.823 and 0.839","a034f076":"## Further Validation for Decision Tree\n### Classification report without cross-validation:","a6665e9a":"### Step 4: Change DATE variable to datetime format:","0fc70574":"- Although tuning the hyper-parameters does not further increase the score for Gradient Boosting (which had a relatively high score by default) and Logistic Regression (which had a relatively low score by default), it does improve the accuracy score for Decision Tree from 0.77 to 0.83.","d12a7e98":"## Implementing MLAs:\n\n### Spliting the data into test and train sets:","714942f1":"## Exploratory Data Analysis","bd2657b4":"As expected, ensemble MLAs provide better accuracy.","6fe35896":"### Using a variety of MLAs to get the best results\n\n- Since the purpose of this project is determining the best rain prediction, we would like to maximize the accuracy score.\n- The outcome is binary, so we can use Logistic Regression, Decision Tree, or Naive Bayes.\n- We also use ensemble algorithms (such as Random Forest) to see if the accuracy score can be improved further.","c8d89546":"### Step 2: Imputing missing values:\n\nThere are only three missing data points for each PRCP and RAIN. So, we use median for PRCP and mode for RAIN to fill in the gaps.","bfdc1f9f":"### Gradient Boosting:","a57873a4":"## Optimizing model hyper-parameters:\n\n- Let's see if we can improve a model performance by changing the hyper parameters.\n- We are not going to test all the models, but just a few of them."}}