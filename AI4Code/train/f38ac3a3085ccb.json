{"cell_type":{"96f4282f":"code","0a7841df":"code","3f31bf14":"code","8dafec99":"code","cd4a4b1c":"code","54450f68":"code","0997ecbd":"code","017801e9":"code","965280a7":"code","976ade14":"code","566c40e0":"code","03d7ae9a":"code","fbf7c24b":"code","5858c195":"code","9ed2adf2":"code","e6611110":"code","a2ea76ee":"code","7b44e988":"code","267e74fb":"code","6ee0a52b":"code","722c67c2":"code","7097b417":"code","8078e9d8":"code","f3829390":"code","5ca610cd":"code","7c96513d":"code","fd13d6a1":"code","085184f7":"code","57c0aaac":"code","d77405c2":"markdown","bd19c220":"markdown","bf590470":"markdown","8a664c41":"markdown","825ae358":"markdown","b754bc7f":"markdown","55acf6ce":"markdown","38d31878":"markdown","2ccfc5a6":"markdown","799f70fa":"markdown","56528211":"markdown","348641aa":"markdown","f7bd4ad1":"markdown","898bf6d3":"markdown","14a8f2a1":"markdown","89edb9f7":"markdown","79cbcc84":"markdown","b2a9b79a":"markdown","34d59420":"markdown","78c83ccb":"markdown"},"source":{"96f4282f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\", font_scale=1.2)\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, PowerTransformer\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n","0a7841df":"def report_missing_unique(df):\n    \"\"\"\n    Enter your data frame. Returns a DataFrame object with the following columns:\n    \n    MissingCount: number of missing values per column\n    MissingPercent: percentage of missing values per column\n    DataType: data type of column\n    UniqueCount: number of unique values per column\n    UniqueList: list of unique values per column\n    \"\"\"\n    \n    count_missing_list = []\n    percent_missing_list = []\n    datatypes_list = []\n    count_unique_list = []\n    unique_values_list = []\n    features_list = []\n    \n    for col in df.columns:\n        count_missing_list.append(df[col].isna().sum())\n        percent_missing_list.append(round((df[col].isna().sum() \/ len(df[col]))*100, 3))\n        datatypes_list.append(df[col].dtypes)\n        count_unique_list.append(df[col].nunique())\n        unique_values_list.append(df[col].unique())\n        features_list.append(col)\n    \n    summary_df = pd.DataFrame({'MissingCount':count_missing_list, \n                               'MissingPercent':percent_missing_list, \n                               'DataType':datatypes_list, \n                               'UniqueCount':count_unique_list, \n                               'UniqueList':unique_values_list})\n    \n    summary_df.index = features_list\n    \n    return summary_df.sort_values(by=\"MissingCount\",ascending=False)\n","3f31bf14":"# Read the data\ntrain = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n# Set the target to the 'SalePrice' column of train\ny = train[['SalePrice']]\n# Show the initial number of rows and columns of the data sets train and test\nprint(\"train: \", train.shape)\nprint(\"test: \", test.shape)\n","8dafec99":"# concatenate the train and test features\nX = pd.concat([train.drop(\"SalePrice\", axis=1), test], axis=0)\nprint(\"X: \", X.shape)\n","cd4a4b1c":"# get a report on the missing and unique values of features with missing values\nmu_df = report_missing_unique(X)\nmu_df[mu_df.MissingCount > 0]\n","54450f68":"# 'MSSubClass' is actually a categorical column that is currently formatted as integer\nX['MSSubClass'] = X['MSSubClass'].astype(str)\ntrain['MSSubClass'] = train['MSSubClass'].astype(str)\ntest['MSSubClass'] = test['MSSubClass'].astype(str)\n","0997ecbd":"# Numerical features\ndf_num = X.select_dtypes(exclude=['object']).copy()\ndf_cat = X.select_dtypes(include=['object']).copy()\n\nprint(f\"There are {df_num.shape[1]} numerical columns.\")\nprint(f\"There are {df_cat.shape[1]} categorical columns.\")\n","017801e9":"# list discrete numerical variables\nlist_num_discrete = []\nfor i in df_num.columns:\n    if len(df_num[i].unique()) < 20:\n        list_num_discrete.append(i)\n\n# PoolArea is not a discrete continuous variable, so removing that from the list\nlist_num_discrete.remove('PoolArea')\n        \n# list continuous numerical variables\nlist_num_continuous = []\nfor i in df_num.columns:\n    if i not in list_num_discrete:\n        list_num_continuous.append(i)\n        \nprint(f\"There are {len(list_num_discrete)} discrete numerical columns.\")\nprint(f\"There are {len(list_num_continuous)} continuous numerical columns.\")\n","965280a7":"df_num_train = train.select_dtypes(exclude=['object']).copy()\ndf_cat_train = train.select_dtypes(include=['object']).copy()\n\n# previous step excluded 'SalePrice' from df_cat_train as the target is numeric, bring it back\ndf_cat_train = pd.concat([df_cat_train, train['SalePrice']], axis=1)\n\ndf_num_disc_train = df_num_train.drop(list_num_continuous, axis=1)\ndf_num_cont_train = df_num_train.drop(list_num_discrete, axis=1)\n","976ade14":"fig = plt.figure(figsize=(20,20))\nfor index in range(len(df_num_cont_train.columns)-1):\n    plt.subplot(6,4,index+1)\n    sns.scatterplot(x=df_num_cont_train.iloc[:,index], y='SalePrice', data=df_num_cont_train)\nfig.tight_layout(pad=1.0)\n","566c40e0":"# get the Id of this strange data point (from Extra above)\nX[X['GarageYrBlt'] >= 2020]['GarageYrBlt']\n","03d7ae9a":"# Id is high (higher than 1460) so this is from test data\n# this looks like a typo and the true value is likely 2007\ntest.loc[2593,'GarageYrBlt'] = 2007\nX.loc[2593,'GarageYrBlt'] = 2007\n","fbf7c24b":"fig = plt.figure(figsize=(16,10))\nfor index in range(len(df_num_disc_train.columns)-1):\n    plt.subplot(4,4,index+1)\n    sns.stripplot(x=df_num_disc_train.iloc[:,index], y='SalePrice', data=df_num_disc_train, jitter=True)\nfig.tight_layout(pad=1.0)\n","5858c195":"fig = plt.figure(figsize=(18,30))\nfor index in range(len(df_cat_train.columns)-1):\n    plt.subplot(11,4,index+1)\n    sns.stripplot(x=df_cat_train.iloc[:,index], y='SalePrice', data=df_cat_train, jitter=True)\nfig.tight_layout(pad=1.0)\n","9ed2adf2":"# Correlation matrix\nplt.figure(figsize=(14,12))\nsns.heatmap(df_num.corr(), mask = df_num.corr() <0.66, linewidth=1.5, cmap='Reds')\n","e6611110":"k = 10 #number of variables for heatmap\nplt.figure(figsize=(12,8))\ncorr_matrix = train.corr()\ncols = corr_matrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nhm = sns.heatmap(cm, cbar=True, annot=True, \n                square=True, fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=cols.values, xticklabels=cols.values)\n","a2ea76ee":"# Features with multicollinearity\nX.drop(['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageCars'], axis=1, inplace=True)\n# Features with a lot of missing values (>80% missing)\nX.drop(['PoolQC','MiscFeature','Alley','Fence'], axis=1, inplace=True)\n# Features unrelated to predict SalePrice\nX.drop(['MoSold', 'YrSold'], axis=1, inplace=True)\n# Features that have mostly just a single value (categorical)\nX.drop(['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating'], axis=1, inplace=True)\n# Features that have mostly just a single value (numerical)\nX.drop(['LowQualFinSF', 'PoolArea', 'MiscVal'], axis=1, inplace=True)","7b44e988":"##### NUMERIC FEATURES #####\n# numeric columns where missing values are to be replaced with 0 (exception is 'LotFrontage' as below)\nzero_list = ['MasVnrArea', 'BsmtHalfBath', 'BsmtFullBath', 'BsmtFinSF1', \n             'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea']\nfor col in zero_list:\n    X[col] = X[col].fillna(0)\n    \n# Group by neighborhood and fill in missing value by the median LotFrontage of Neighborhood\nX[\"LotFrontage\"] = X.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\n##### CATEGORICAL FEATURES #####\n# Missing values in columns in the na list are to be filled in with the value \"NA\"\nna_list = ['FireplaceQu', 'GarageCond', 'GarageQual', 'GarageFinish', \n           'GarageType', 'BsmtCond', 'BsmtExposure', 'BsmtQual', \n           'BsmtFinType2', 'BsmtFinType1']\nfor col in na_list:\n    X[col] = X[col].fillna('NA')\n\n# Missing values in columns in the none list are to be filled in with the value \"None\"\nnone_list = ['MasVnrType']\nfor col in none_list:\n    X[col] = X[col].fillna('None')\n\n# Missing values in columns in the most frequent list are to be filled in with the most commonly occurring value\nmf_list = ['MSZoning', 'Functional', 'Electrical', 'Exterior2nd', \n           'Exterior1st', 'SaleType', 'KitchenQual']\nfor col in mf_list:\n    X[col] = X[col].fillna(X[col].mode()[0])","267e74fb":"X['TotalWalledArea'] = X['TotalBsmtSF'] + X['GrLivArea']\nX['TotalPorchArea'] = X['OpenPorchSF'] + X['3SsnPorch'] + X['EnclosedPorch'] + X['ScreenPorch'] + X['WoodDeckSF']\nX['TotalOccupiedArea'] = X['TotalWalledArea'] + X['TotalPorchArea']","6ee0a52b":"map1 = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\nset1 = ['FireplaceQu', 'GarageCond', 'GarageQual', 'ExterQual', \n        'ExterCond', 'BsmtCond', 'BsmtQual', 'HeatingQC', 'KitchenQual']\nfor col in set1:\n    X[col] = X[col].replace(map1)\n\nmap2 = {'GLQ': 4,'ALQ': 3,'BLQ': 2,'Rec': 3,'LwQ': 2,'Unf': 1,'NA': 0}\nset2 = ['BsmtFinType1', 'BsmtFinType2']\nfor col in set2:\n    X[col] = X[col].replace(map2)    \n    \nmap3 = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\nX['BsmtExposure'] = X['BsmtExposure'].replace(map3)\n\nmap4 = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0}\nX['GarageFinish'] = X['GarageFinish'].replace(map4)\n\nmap5 = {'Y': 1, 'N': 0}\nX['CentralAir'] = X['CentralAir'].replace(map5)\n\nmap6 = {'Typ': 3.5, 'Min1': 3, 'Min2': 2.5, 'Mod': 2, 'Maj1': 1.5, 'Maj2': 1, 'Sev': 0.5, 'Sal': 0}\nX['Functional'] = X['Functional'].replace(map6)\n\nmap7 = {'Y': 1, 'P': 0.5, 'N': 0}\nX['PavedDrive'] = X['PavedDrive'].replace(map7)","722c67c2":"# let's have quick look if there are any missing values\nnew_mu_df = report_missing_unique(X)\nnew_mu_df[new_mu_df.MissingCount > 0]","7097b417":"# Select categorical columns\ncategorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n# Select numeric columns\nnumeric_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n\n# let's have a quick look how many unique entries there are for our (remaining) categorical features, as OneHotEncoder will need that many new columns\n# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X[col].nunique(), categorical_cols))\nd = dict(zip(categorical_cols, object_nunique))\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","8078e9d8":"# define the transformers\n\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', PowerTransformer(method='yeo-johnson', standardize=True))\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('cat_onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n    \npreprocessor = ColumnTransformer(transformers=[\n        ('num', numeric_transformer, numeric_cols),\n        ('cat', categorical_transformer, categorical_cols)], \n        remainder='passthrough')","f3829390":"# define the model\n\nmodel = XGBRegressor(n_estimators=1000, \n                     learning_rate = 0.02, \n                     max_depth = 3,\n                     subsample = 0.8,\n                     gamma = 1,\n                     random_state = 0)","5ca610cd":"# define the pipeline\n\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])","7c96513d":"# splitting the train and test data\ntrain = X[:train.shape[0]]\ntest = X[train.shape[0]:]","fd13d6a1":"# cross-validation with 5-folds\n\nscores = -cross_val_score(my_pipeline, train, y, cv=5, scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores: \", scores)","085184f7":"print(\"Average MAE score (across experiments): {:.3f}\".format(scores.mean()))","57c0aaac":"# fit model to the full train data\nmy_pipeline.fit(train, y)\n\n# Generate test predictions\npreds_test = my_pipeline.predict(test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': test.index, 'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)\nprint (\"Submission file is saved.\")","d77405c2":"### Some observations:\n\n1. There are some variables visible above that mostly take a **single value which is 0**: \n    ['MiscVal', 'PoolArea', 'ScreenPorch', '3SsnPorch', 'EnclosedPorch', 'LowQualFinSF', 'BsmtFinSF2']\n    \n2. There are some **outliers** we can visually identify. For instance, we see two data points where 'LotFrontage' > 300 which are too far away from other values of this feature. \n\n**Extra:** This is not something visible on the above charts, but there is something wrong with a single data point of 'GarageYrBuilt'. There is currently this data point in the *test* data that is greater than 2200. This column is supposed to hold the year when the garage (if any) was built. Obviously, there should not be any data point that is greater than the year when the data was collected.","bd19c220":"## 2.4 Numeric Discrete Features vs Target","bf590470":"## 2.5 Categorical Features vs Target","8a664c41":"## 3.3 Feature Engineering","825ae358":"# 3. Preprocessing","b754bc7f":"## 2.1 Identifying Missing and Unique Values","55acf6ce":"### 2.6.1. Correlation Matrix and Multicollinearity\n\nThrough **correlation matrix**, we show the correlation coefficients between variables.\n\n**Multicollinearity** is a concept we need to know. It refers to the condition in which two or more predictors are highly correlated with one another. There are no hard rules, but let's assume there is multicollinearity present if the correlation > 0.66.","38d31878":"# 2. Data Exploration","2ccfc5a6":"# 1. Preparation","799f70fa":"### Some observations:\n\nAbove, we see the following variables are highly correlated with each other:\n\n* GarageCars & GarageArea\n* GarageYrBlt & YearBuilt\n* TotRmsAbvGrd & GrLivArea\n* TotalBsmtSF & 1stFlrSF\n\nWhen there is multicollinearity present, statistical inferences to be made about the data may not be reliable. We will need to address this point in the preprocessing step.","56528211":"## 2.2 Identifying Numeric and Categorical Features","348641aa":"## 3.2 Imputation","f7bd4ad1":"## 3.4 Encoding Ordinal Features","898bf6d3":"### Some observations:\n\n1. There seems to be a strong positive relationship between 'OverallQual' and our target 'SalePrice'. There are also some other features that have a positive relationship with the target, such as 'OverallCond'.","14a8f2a1":"## 2.6 Correlation","89edb9f7":"## 2.3 Numeric Continuous Features vs Target","79cbcc84":"### 2.6.2. Correlation Between Predictors and Target\n\nTo understand which variables are important to predict SalePrice, we look at how strongly correlated the numerical variables are to our target.","b2a9b79a":"## 3.1 Dropping Not Useful Features","34d59420":"### Some observations:\n\n1. There are some variables visible above that mostly take a **single value**: \n    ['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating']\n    \n2. There are some variables that have a **lot of missing values** (something we had discovered before):\n    ['MiscFeature', 'PoolQC', 'Alley', 'Fence']","78c83ccb":"# 4. Modelling"}}