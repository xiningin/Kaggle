{"cell_type":{"1e4ecc83":"code","5611cea7":"code","f9706b44":"code","56b83829":"code","82ab440d":"code","463eaaaa":"code","1573007b":"code","9d6414ad":"code","9f2b0bf3":"code","c1055344":"code","4e74b244":"code","309455b1":"code","5a7ea975":"code","31c7a31e":"code","fd0fb370":"code","52342136":"code","ff112648":"code","2f576616":"code","9c227e0a":"code","09461004":"code","4bac18da":"markdown","571c046f":"markdown","e63d92ca":"markdown","cca6c796":"markdown","dd89d566":"markdown","b580fc0e":"markdown","9f8f97bc":"markdown","adcd1aac":"markdown"},"source":{"1e4ecc83":"import pandas as pd\nimport os\nos.chdir(\"..\/input\")","5611cea7":"meta_data = pd.read_csv(\"train.csv\")\nmeta_data.head()","f9706b44":"train_dir = \"train\/train\"\ntest_dir =  \"test\/test\"\nos.listdir(train_dir)[:5]\nprint(len(os.listdir(train_dir)))","56b83829":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n#######Wiithout data augumentation, just for experimental purposes...#########\n\ntrain_gen = ImageDataGenerator(rescale = 1\/255,\n                              horizontal_flip = True,\n                              height_shift_range = 0.2,\n                              width_shift_range = 0.2,\n                              brightness_range = [0.2,1.2],)\n                              #fill_mode=\"nearest\")\nvalid_gen = ImageDataGenerator(rescale=1\/255)\n\nmeta_data.has_cactus = meta_data.has_cactus.astype(str)\n\ntrain_generator = train_gen.flow_from_dataframe(\n    dataframe = meta_data[:15000],\n    target_size=(150,150),\n    directory = train_dir,\n    x_col=\"id\",\n    y_col=\"has_cactus\",\n    class_mode = \"binary\"\n)\n\nvalid_generator = valid_gen.flow_from_dataframe(\n    dataframe = meta_data[15000:],\n    target_size = (150,150),\n    directory = train_dir,\n    x_col = \"id\",\n    y_col = \"has_cactus\",\n    class_mode = \"binary\",\n)\n","82ab440d":"from tensorflow import keras\n\nbase_model = keras.applications.InceptionV3(include_top=False, weights='imagenet', input_tensor=None, input_shape=(150,150,3), pooling=None, classes=1)","463eaaaa":"# VGG19 is a state of the art model which has been trained on imagenet dataset\n# For our purposes we select the input shape as (32x32x3), here is the summary of the model.\n# We can see there are 5 blocks of convolution, and pooling layers.\nbase_model.summary()","1573007b":"#locking all the layers to prevent their training, as we only want to extend it by adding our own Dense layer classifier.\n\nfor layer in base_model.layers:\n    layer.trainable = False\n    \n#Extracting last layer, and collecting it's last output, which we will use to feed into our extended model.\n\nlast_layer = base_model.layers[-1]\nlast_output = last_layer.output\n\n#Adding out own extended version of the model. For simplicity lets take it to 512 neurons in FC layer. And 2 \n#neurons in the last layer for classification purpose.\n\nextend = keras.layers.Flatten()(last_output)\nextend = keras.layers.Dense(512, activation = \"relu\") (extend)\nextend = keras.layers.Dropout(0.4)(extend)\nextend = keras.layers.Dense(1, activation=\"sigmoid\")(extend)\n\n#Defining our extended model now\n\nmodel = keras.models.Model(base_model.input, extend)\n\n#All looks good, let's compile our model now. We'll use loss as categorical_crossentropy, and optimizer as adam\n\nmodel.compile(loss = \"binary_crossentropy\",\n             optimizer=keras.optimizers.Adam(lr = 1e-3),\n             metrics=[\"acc\"])\n\nmodel.summary()","9d6414ad":"\nmodel.fit_generator(\n    train_generator,\n    validation_data = valid_generator,\n    verbose = 1,\n    shuffle=True,\n    epochs = 10,\n)","9f2b0bf3":"history = model.history","c1055344":"acc = history.history[\"acc\"]\nloss = history.history[\"loss\"]\nval_acc = history.history[\"val_acc\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(len(acc))","4e74b244":"import matplotlib.pyplot as plt\n\n\nplt.plot(epochs, acc, label=\"Training Accuracy\")\nplt.plot(epochs, val_acc, label=\"Validation Accuracy\")\nplt.axis([0, 4, 0.7, 1])\nplt.title(\"Training vs Validation Accuracy\")\nplt.legend()\nplt.figure()","309455b1":"#model.save(\"my_model.h5\")","5a7ea975":"os.listdir(\"test\/test\")[:5]","31c7a31e":"import cv2\nimages = []\n\nfor image in os.listdir(\"test\/test\"):\n    images.append( cv2.imread(\"test\/test\/\" + image))\n","fd0fb370":"import numpy as np\nimage = np.asarray(images)","52342136":"image.resize(4000, 32, 32, 3)\nimage.shape","ff112648":"prediction = model.predict(image)\nprediction.resize(4000)","2f576616":"sub = pd.DataFrame({\"id\" : os.listdir(test_dir),\n                   \"has_cactus\" : prediction})","9c227e0a":"sub.head()","09461004":"sub.to_csv(\"..\/working\/samplesubmission.csv\", index=False)","4bac18da":"# Model training, without troubleshooting any overfitting, just for the sake of experimentation","571c046f":"# Preprocessing #\n","e63d92ca":"## Train-metadata excelsheet preview","cca6c796":"## Sorting training data, according to the label provided in the csv spreadsheet","dd89d566":"# First I trained model with different configs, here are the results\n# 512 nodes, adam approx  validation accuracy 97.65\n# 512 nodes, RMSprop with lr=0.001  validation accuracy 97.55\n# 512 nodes, RMSpop with lr= 0.000001  validation accuracy 97.62\n# 1024 nodes, adam approx validation accuracy = 97.75\n# 1024 nodes, first with adam then RMSprop with lr = 0.0000000001 validation_acc = 0.9812\n# 1024 nodes, first with adam then RMSprop with lr = 0.000001 validation_acc = 0.9812\n# 1024 nodes, first with adam then RMSprop with lr = 0.001 validation_acc = 0.9796\n# 1024=>512 nodes, adam validation accuracy = 0.9764-0.9816\n# 1024=>512 nodes, SGD validation accuracy = 0.9804\n# 1024=>512 nodes, SGD->Adam validation accuracy = 0.9824=>0.9800=>0.9875 and more degradation\n# 1024=>512=>dropout  Adam->SGD lr =0.001 validation accuracy = 0.9800\n# 1024=>512=>dropout  Adam->SGD lr = 0.0001 validatio accuracy = 0.9800\n# Selected pool layer from block 3 in an attempt to get a better result, but ended up at validation accuracy = 0.70\n# The training is kinda stuck between 97 to 98.25%, I think a more complex network can help in it, if not let's see how the obtained results perform on test images\n# 1024=>dropout=>512=>dropout=>256=>dropout, adam, validation accuracy =  0.9736\n","b580fc0e":"# Creating training, and validation generators for the purpose of feeding data into the network. 15000 to 2500.","9f8f97bc":"# Performing transfer learning on the state of the art model VGG19","adcd1aac":"# Predicting the test images"}}