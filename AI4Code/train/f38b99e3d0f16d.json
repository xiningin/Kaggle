{"cell_type":{"a07e72d7":"code","0b1f012e":"code","35b5ea44":"code","4826e28c":"code","a0b45d72":"code","9e534dbb":"code","c2f654ff":"code","a44775dc":"code","02bb1a17":"code","16e5ed3a":"code","202205ca":"code","c0d00e93":"code","316f2f26":"code","b5a1c87a":"code","3cff87b0":"code","a655db90":"code","9582bcf0":"code","61ca2b84":"code","5a163915":"code","5194ff8d":"code","398bf880":"code","7995e320":"code","4806dbe5":"code","0283f602":"code","dd5cab42":"code","748e4877":"markdown","4a485d30":"markdown","d15fdbcb":"markdown","702121ca":"markdown","ebe741bb":"markdown","2c7f8109":"markdown","1dc33973":"markdown","7549d29a":"markdown","799c45c4":"markdown","9eb1db5b":"markdown","1ba59ed9":"markdown","96baacc9":"markdown","9f2bdd26":"markdown","68edbbe9":"markdown","df66fb92":"markdown","cf9749db":"markdown"},"source":{"a07e72d7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing # package containing modules for modelling, data processing, etc.\nfrom sklearn import impute\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_log_error\nfrom xgboost import XGBRegressor\nimport seaborn as sns # visualization package #1\nimport matplotlib.pyplot as plt # visualization package #2\n# Configure visualisations\n%matplotlib inline","0b1f012e":"# Import files containing data\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# Convert .csv to dataframes\ntrain_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=\"Id\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=\"Id\")","35b5ea44":"# Remove rows with missing target, separate target from predictors\ntrain_data = train_df.copy()\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny_train_pip = train_data.SalePrice\nX_train_pip = train_data.drop(['SalePrice'], axis=1)\n\n# Select categorical columns\ncategorical_cols = [col for col in X_train_pip.columns if X_train_pip[col].nunique() < 10 and X_train_pip[col].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [col for col in X_train_pip.columns if X_train_pip[col].dtype in ['int64', 'float64']]\n\n# Joint numerical and categorical columns\nmy_cols = categorical_cols + numerical_cols\nX_train_pip = X_train_pip[my_cols].copy()\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_pip, y_train_pip, random_state=1)\n\n# Preprocessing for numerical data\nnumerical_transformer = impute.SimpleImputer(strategy='most_frequent')\n                  \n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[ ('imputer', impute.SimpleImputer(strategy='constant', fill_value='Empty')),\n                                           ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore')) ])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer( transformers=[ ('num', numerical_transformer, numerical_cols),\n                                                 ('cat', categorical_transformer, categorical_cols) ])\n\n# Define model\nmodel = XGBRegressor(n_estimators=500, learning_rate=0.1, random_state=1, objective ='reg:squarederror', \n                     early_stopping_rounds=5, eval_set=[(X_valid, y_valid)])\n\n# Bundle preprocessing and modeling code in a pipeline\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n\n# Fit the model \npipe.fit(X_train, y_train)\n# Use cross-validation to compute the average mae score\nscore = cross_val_score(pipe, X_train, y_train, scoring='neg_mean_squared_log_error', cv=4)","4826e28c":"# print scores and the mean score of the analysis\nprint(score)\nprint(\"Mean score: %.5f\" %(-1 * score.mean()))","a0b45d72":"# join both train and test sets so that the newly created features are incorporated in both sets\ndataset = pd.concat([train_df, test_df], sort=False)\ndataset.index.name = 'Id'\n# count number of missing entries\ncountmissing = dataset.isnull().sum().sort_values(ascending=False)\npercentmissing = (dataset.isnull().sum()\/dataset.isnull().count()).sort_values(ascending=False)\n# create a dataframe containing the missing values per feature and the %% of missing values\ndataset_na = pd.concat([countmissing,percentmissing], axis=1)\ndataset_na.head(36) # 36 entries so that all features containing missing values are shown","9e534dbb":"# the most frequent entry is used to fill the missing values of these features since a very small % is missing\ndataset['Utilities'] = dataset['Utilities'].fillna(\"AllPub\")\ndataset['Electrical'] = dataset['Electrical'].fillna(\"SBrkr\")\ndataset['Exterior1st'] = dataset['Exterior1st'].fillna(\"VinylSd\")\ndataset['Exterior2nd'] = dataset['Exterior2nd'].fillna(\"VinylSd\")\n\n# Missing integer values replaced with the median in order to return an integer\ndataset['BsmtFullBath']= dataset.BsmtFullBath.fillna(dataset.BsmtFullBath.median())\ndataset['BsmtHalfBath']= dataset.BsmtHalfBath.fillna(dataset.BsmtHalfBath.median())\ndataset['GarageCars']= dataset.GarageCars.fillna(dataset.GarageCars.median())\n\n# Missing float values were replaced with the mean for accuracy \ndataset['BsmtUnfSF']= dataset.BsmtUnfSF.fillna(dataset.BsmtUnfSF.mean())\ndataset['BsmtFinSF2']= dataset.BsmtFinSF2.fillna(dataset.BsmtFinSF2.mean())\ndataset['BsmtFinSF1']= dataset.BsmtFinSF1.fillna(dataset.BsmtFinSF1.mean())\ndataset['GarageArea']= dataset.GarageArea.fillna(dataset.GarageArea.mean())\ndataset['MasVnrArea']= dataset.MasVnrArea.fillna(dataset.MasVnrArea.mean())","c2f654ff":"# Infer missing values for the following features using information from other features\n# Garage was built at the earliest when the house was built, hence it should be a good approximation\ndataset.GarageYrBlt.fillna(dataset.YearBuilt, inplace=True)\n# A better approximation than just using the mean for the Basement Area is to assume that it is equal to the 1st Floor Area (which usually is what happens, excluding porches or balconies)\n# plot that might be used to check the inference of Basement Area values from 1st Floor Area\nsns.lmplot(x=\"TotalBsmtSF\", y=\"1stFlrSF\", data=dataset)\nplt.title(\"Basement Area vs 1st Floor Area\")\nplt.xlim(0,)\nplt.ylim(0,)\nplt.show()\n#Update values of Basement Area\ndataset.TotalBsmtSF.fillna(dataset['1stFlrSF'], inplace=True)","a44775dc":"# Regarding LotFrontage as there is no value for the depth of the lot, one may try to infer it from the LotArea\n# Let's use feature correlation to check the proposed theory\n# create a new dataframe containing only the features related to the lot\nlotfront_check = dataset[['LotArea','LotConfig','LotFrontage','LotShape']]\nlotfront_check = pd.get_dummies(lotfront_check)\nlotfront_check.corr()['LotFrontage'].sort_values(ascending=False)","02bb1a17":"# Let's assume that the lot is squared, hence the LotFrontage is going to be the square root of the LotArea. \nlotfront_check[\"LotAreaUnSq\"] = np.sqrt(lotfront_check['LotArea'])\n# Let's plot the newly created feature to check if the assumption is satisfatory or if we are thinking wrong...\nsns.regplot(x=\"LotAreaUnSq\", y=\"LotFrontage\", data=lotfront_check)\nplt.title(\"LotArea vs LotFrontage\")\nplt.xlim(0,)\nplt.ylim(0,)\nplt.show()","16e5ed3a":"# Update the LotFrontage of dataset dataframe\ndataset['LotFrontage']= dataset.LotFrontage.fillna(np.sqrt(dataset.LotArea))\ndataset['LotFrontage']= dataset['LotFrontage'].astype(int)\n# Plot the distribution of the imputed 'LotFrontage' to check its distribution. \n# The distribution should be similar to the original one - so that its properties are kept\n# Distribution of values after replacement of missing frontage\nplt.figure(figsize=(7,6))\nsns.kdeplot(dataset['LotFrontage'])\nsns.kdeplot(lotfront_check['LotFrontage'])\nsns.kdeplot(lotfront_check['LotAreaUnSq'])\nplt.title(\"Distribution of Lot Frontage\")\nplt.show() ","202205ca":"# Correlation of features with House SalePrice\ntrain_corr = pd.DataFrame(dataset.corr()['SalePrice']) # convert series to dataframe to allow sorting\n# correct column label from SalePrice to correlation\ntrain_corr.columns = [\"Correlation\"]\n# sort correlation\ntrain_corr = train_corr.sort_values(by=['Correlation'], ascending=False)\ntrain_corr.head(15)","c0d00e93":"# build a feature that includes all of the house area\ndataset['LivingTotalSF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF'] + dataset['GarageArea'] + dataset['WoodDeckSF'] + dataset['OpenPorchSF']\n# Total Living Area divided by LotArea\ndataset['PercentSQtoLot'] = dataset['LivingTotalSF'] \/ dataset['LotArea']\n# Total count of all bathrooms including full and half through the entire building\ndataset['TotalBaths'] = dataset['BsmtFullBath'] + dataset['BsmtHalfBath'] + dataset['HalfBath'] + dataset['FullBath']\n# Percentage of total rooms are bedrooms\ndataset['PercentBedrmtoRooms'] = dataset['BedroomAbvGr'] \/ dataset['TotRmsAbvGrd']\n# Number of years since last remodel, if there never was one it would be since it was built\ndataset['YearSinceRemodel'] = 2016 - ((dataset['YearRemodAdd'] - dataset['YearBuilt']) + dataset['YearBuilt'])","316f2f26":"# check the correlation values after newly created features\n# Correlation of features with House SalePrice\n#train_corr2 = pd.DataFrame(dataset['LivingTotalSF','PercentSQtoLot','TotalBaths','PercentBedrmtoRooms','YearSinceRemodel'].corr()['SalePrice']) # convert series to dataframe to allow sorting\ntrain_corr2 = pd.DataFrame(dataset.corr()['SalePrice']) # convert series to dataframe to allow sorting\n# correct column label from SalePrice to correlation\ntrain_corr2.columns = ['Correlation']\n# sort correlation\ntrain_corr2 = train_corr2.sort_values(by=['Correlation'], ascending=False)\ntrain_corr2.head(15)","b5a1c87a":"train_X.head()","3cff87b0":"train_X = dataset[dataset['SalePrice'].notnull()].copy()\ntrain_X.drop(['SalePrice'], axis=1, inplace=True)\ntrain_y = dataset[dataset['SalePrice'].notnull()]['SalePrice'].copy()\n\ntest_X =  dataset[dataset['SalePrice'].isnull()].copy()\ndel test_X['SalePrice']","a655db90":"# Select categorical columns\ncategorical_cols = [col for col in train_X.columns if train_X[col].nunique() < 10 and train_X[col].dtype == \"object\"]\n# Select numerical columns\nnumerical_cols = [col for col in train_X.columns if train_X[col].dtype in ['int64', 'float64']]\n# Joint numerical and categorical columns\nmy_cols = categorical_cols + numerical_cols\ntrain_X = train_X[my_cols].copy()\nX_train, X_valid, y_train, y_valid = train_test_split(train_X, train_y, random_state=1)\n# Preprocessing for numerical data\nnumerical_transformer = impute.SimpleImputer(strategy='most_frequent')           \n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[ ('imputer', impute.SimpleImputer(strategy='constant', fill_value='Empty')),('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore')) ])\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer( transformers=[ ('num', numerical_transformer, numerical_cols),('cat', categorical_transformer, categorical_cols) ])\n# Define model\nmodel = XGBRegressor(n_estimators=500, learning_rate=0.1, random_state=1, objective ='reg:squarederror', early_stopping_rounds=5, eval_set=[(X_valid, y_valid)])\n# Bundle preprocessing and modeling code in a pipeline\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n# Fit the model \npipe.fit(X_train, y_train)\n# Use cross-validation to compute the average mae score\nscore = cross_val_score(pipe, X_train, y_train, scoring='neg_mean_squared_log_error', cv=4)\nprint(\"Mean score: %.5f\" %(-1 * score.mean()))","9582bcf0":"# Iteration #1\n# Preprocessing for numerical data\nnumerical_transformer = impute.SimpleImputer(strategy='most_frequent')\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[ ('imputer', impute.SimpleImputer(strategy='most_frequent')),\n                                           ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore')) ])","61ca2b84":"# Iteration #2\n# Preprocessing for numerical data\nnumerical_transformer = impute.SimpleImputer(strategy='mean')\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[ ('imputer', impute.SimpleImputer(strategy = \"constant\", fill_value=\"Empty\")),\n                                           ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore')) ])","5a163915":"# Iteration #3\n# Preprocessing for numerical data\nnumerical_transformer = impute.SimpleImputer(strategy='median')\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[ ('imputer', impute.SimpleImputer(strategy = \"constant\", fill_value=\"Empty\")),\n                                           ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore')) ])","5194ff8d":"# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer( transformers=[ ('num', numerical_transformer, numerical_cols), ('cat', categorical_transformer, categorical_cols) ])\n# Define model\nmodel = XGBRegressor(n_estimators=500, learning_rate=0.1, random_state=1, objective ='reg:squarederror', early_stopping_rounds=5, eval_set=[(X_valid, y_valid)])\n# Bundle preprocessing and modeling code in a pipeline\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n# Fit the model \npipe.fit(X_train, y_train)\n# Use cross-validation to compute the average mae score\nscore = cross_val_score(pipe, X_train, y_train, scoring='neg_mean_squared_log_error', cv=4)\nprint(\"Mean score: %.5f\" %(-1 * score.mean()))","398bf880":"#Use GridSearchCV to find the best learning_rate\nmodelB = XGBRegressor(n_estimators=500, random_state=1, objective ='reg:squarederror', early_stopping_rounds=5, eval_set=[(X_valid, y_valid)])\n# Bundle preprocessing and modeling code in a pipeline\npipeB = Pipeline(steps=[('preprocessor', preprocessor), ('modelB', modelB)])\n# Fit the model \npipeB.fit(X_train_pip, y_train_pip)\n\n# command to check keys of pipeline, to use on GridSearchCv\n# sorted(pipe2.get_params().keys())\n\nparams = {\"modelB__learning_rate\" : [0.01, 0.025, 0.05, 0.1, 0.5]}\n\ngrid_search = GridSearchCV(pipeB, param_grid=params, scoring=\"neg_mean_squared_log_error\", cv=4)\ngrid_result = grid_search.fit(X_train_pip, y_train_pip)","7995e320":"# summarize GridSearchCv results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nfor mean, stdev, param in zip(means, stds, params):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))","4806dbe5":"#Use GridSearchCV to find the best learning_rate\nmodelC = XGBRegressor(random_state=1, learning_rate=0.05, objective ='reg:squarederror', early_stopping_rounds=5, eval_set=[(X_valid, y_valid)])\n# Bundle preprocessing and modeling code in a pipeline\npipeC = Pipeline(steps=[('preprocessor', preprocessor), ('modelC', modelC)])\n# Fit the model \npipeC.fit(X_train_pip, y_train_pip)\n\nparamsB = {\"modelC__n_estimators\" : [500, 750, 1000, 1250]}\n\ngrid_search2 = GridSearchCV(pipeC, param_grid=paramsB, scoring=\"neg_mean_squared_log_error\", cv=4)\ngrid_result2 = grid_search2.fit(X_train_pip, y_train_pip)","0283f602":"# summarize GridSearchCv results\nprint(\"Best: %f using %s\" % (grid_result2.best_score_, grid_result2.best_params_))\nmeansB = grid_result2.cv_results_['mean_test_score']\nstdsB = grid_result2.cv_results_['std_test_score']\nparamsB = grid_result2.cv_results_['params']\n\nfor mean, stdev, param in zip(meansB, stdsB, paramsB):\n\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))","dd5cab42":"# Preprocessing for numerical data\nnumerical_transformer = impute.SimpleImputer(strategy='median')\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[ ('imputer', impute.SimpleImputer(strategy = \"constant\", fill_value=\"Empty\")),\n                                           ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore')) ])\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer( transformers=[ ('num', numerical_transformer, numerical_cols), ('cat', categorical_transformer, categorical_cols) ])\n#Use GridSearchCV to find the best learning_rate\nmodelD = XGBRegressor(n_estimators=750, learning_rate= 0.05,random_state=1, objective ='reg:squarederror', early_stopping_rounds=5, eval_set=[(X_valid, y_valid)])\n# Bundle preprocessing and modeling code in a pipeline\npipeD = Pipeline(steps=[('preprocessor', preprocessor), ('modelD', modelD)])\n# Fit the model \npipeD.fit(X_train, y_train)\n\n# Preprocessing of test data\nX_test_pip = test_df[my_cols].copy()\n# Predict on the test data\npreds_test_pip = pipeD.predict(X_test_pip)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test_pip.index, 'SalePrice': preds_test_pip})\noutput.to_csv('submission.csv', index=False)","748e4877":"Indeed, the only feature that has some kind of meaningful correlation with `LotFrontage` is the `LotArea`, hence this connection shall be teste next.","4a485d30":"As it is possible to observe the newly created feature `LivingTotalSF` jumps to the top of the correlations table. Also, feature `TotalBaths` seems to be correlated with house `SalePrice`. Let's now rerun the Baseline model with the introduction of the new features and check if these made some difference in the error found. But before that, the train and test data sets have to be separated again.","d15fdbcb":"## Import data set .csv files and transform to dataframes","702121ca":"It surely looks like `LotFrontage` is correlated to the square of the `LotArea`. Let's then use this relationship to infer the missing values of `LotFrontage`. Lastly, the distribution of `LotFrontage` should be checked to ensure that the same properties are kept even after the transformation.","ebe741bb":"What about `LotFrontage`? More insight is needed to infer as best as possible given the available data.","2c7f8109":"# Parameter Optimization - Learning Rate\n\nTo assess if there is a better learning rate and number of estimators for the model defined, the `GridSearchCV` module is going to be applied.\nThe number of estimators of the baseline model is used in this analysis.","1dc33973":"# Studies on feature building:\n## a) Deal with feature missing values\nStart by looking at the amount of missing values inside the dataframe.\n","7549d29a":"We get an improvement of 0.00002! Amazing","799c45c4":"## c) Definition of new features\nTaking into account this list of correlations, it might be a good starting point to aggregate all of the livable areas ino a single feature, to find the total number of bathrooms (instead of having them split by floor), an aging parameter that checks how old is the house and if it has been refitted. Also, two cross-features are built: a ratio between the total area of the lot and the livable area; a ratio between the number of bedrooms and the total rooms of the house. ","9eb1db5b":"## Import Relevant Packages","1ba59ed9":"# House Prices Competition - XGBoost Regression model\n\nThis notebook will use model XGBoost Regressor to find House Prices. Initially, a baseline model with random parameters is going to be used to be able to compare later results with the first approach.\n\nSeveral approaches are analyzed, namely:\n* transformation of categorical features to numerical\n* imputation of values into the features\n* definition of new features\n* optimization of XGBoost Regression model parameters","96baacc9":"# Baseline: XGBoost Regressor Model\n\nConstruct an XGBoost Regressor model, using simple imputers and onehotencoders to transform the categorical features. Then build a pipeline to streamline the features' processing. Lastly use `cross_val_score` to determine the mean squared log error associated to the model.","9f2bdd26":"# Studies on module's parameters:\n\nTaking `learning_rate=0.01` and `n_estimators=500` the imputer's strategies are analyzed next (for both numerical and categorical features). Results of model with:\n* `Baseline` - categorical_transformer Simple Imputer `strategy = \"constant\", fill_value=\"Empty\"` : Mean score: 0.01830\n* `It#1` - categorical_transformer Simple Imputer `strategy = \"most frequent\"` : Mean score: 0.01929\n* `It#2` - `Baseline` and numerical_transformer = impute.SimpleImputer(strategy='mean') : Mean score: 0.01935\n* `It#3` - `Baseline` and numerical_transformer = impute.SimpleImputer(strategy='median') : Mean score: 0.01924","68edbbe9":"# Export .csv with submission","df66fb92":"## b) Feature Correlation","cf9749db":"# Parameter Optimization: number of iterators\n\nUse a loop to find the number of iterators that minimize mean squared log error. This time the better learning rate found in step 1 is used."}}