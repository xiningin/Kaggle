{"cell_type":{"a9770207":"code","07cd724e":"code","f9df2107":"code","d10b8cd1":"code","91b97da5":"code","5d90a7da":"code","2bc159e6":"code","33d842d8":"code","2b5340ff":"code","f85b8b6a":"code","bde88e58":"code","f3fbb1fc":"code","bd06532c":"code","59df47d8":"code","dbf58196":"code","e1376c4b":"code","fda87e4c":"code","ce914901":"code","c1490c52":"code","2f6def01":"code","0eaee5bd":"code","fbea1a16":"code","2a2ba90f":"code","5dc26b24":"code","b4155b7d":"code","7fd7bdd1":"code","fc4f782a":"code","4665a95d":"code","2de9bf3d":"code","c5388932":"code","3729069c":"code","dd64f094":"code","8cf41176":"code","c4de677e":"code","133a3023":"code","59ff991e":"code","42b57236":"code","e877c11f":"code","addb8679":"code","73b64449":"code","774cc021":"code","2e7416bd":"code","f26573b0":"code","dd11217c":"code","426a602e":"code","a8d5ee91":"code","96b61cb9":"code","d02b7b5d":"code","c61e09ac":"code","782be363":"code","04f85a26":"code","f5a7a2bb":"code","eef2f047":"code","33c08d85":"code","ee2bc5bf":"code","1a2279e2":"code","4045ff44":"code","ad413b9f":"code","98a592e8":"code","e2f3cbc2":"markdown","24de47a6":"markdown","9ed25b5d":"markdown","2762fa88":"markdown","7996ae9e":"markdown","b5d17ea6":"markdown","c6016fd4":"markdown","8b15f4b6":"markdown","f4d2b088":"markdown","72498148":"markdown","b555120b":"markdown","8c0c1f5d":"markdown","06d6e37d":"markdown","977d13f9":"markdown"},"source":{"a9770207":"import numpy as np\nimport pandas as pd","07cd724e":"import matplotlib.pyplot as plt\n\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom time import time","f9df2107":"#from tensorflow.keras.layers import  SimpleRNN ","d10b8cd1":"from tensorflow.keras import regularizers","91b97da5":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc","5d90a7da":"import tensorflow as tf;print(\"tensorflow:\",tf.__version__)\nfrom tensorflow.keras import layers","2bc159e6":"#directory = '\/kaggle\/input\/Polymerase512vec\/'\ndf1= pd.read_csv('\/kaggle\/input\/polymerase512vec\/Train1941_512vectorsWithTitles.csv')\ndf1.head()","33d842d8":"df1.shape","2b5340ff":"df1.dtypes","f85b8b6a":"df1.describe()","bde88e58":"# check missing values\ndf1.isnull().sum()\n","f3fbb1fc":"df1.hist(column='Group', bins=50)","bd06532c":"df_imputed=df1.drop(['title', 'abstract'], axis=1)\ndf_imputed\n","59df47d8":"# we want to predict the X and y data as follows:\n\nif 'Group' in df_imputed:\n    y = df_imputed['Group'].values # get the labels we want\n    del df_imputed['Group'] # get rid of the class label\n    X = df_imputed.values # use everything else to predict","dbf58196":"type(y)","e1376c4b":"type(X)","fda87e4c":"# features=","ce914901":"# #it's good practice to Scale Data\n# from sklearn.preprocessing import MinMaxScaler\n\n# scaler = MinMaxScaler(feature_range=(0, 1))\n# scaled_train = scaler.fit_transform(X)\n# scaled_train_df = pd.DataFrame(scaled_train, columns=features)","c1490c52":"# cross validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1776)","2f6def01":"type(X_test)","0eaee5bd":"\nprint(len(y_test))\nprint(len(y_train))","fbea1a16":"from tensorflow.keras.callbacks import TensorBoard\ntb = TensorBoard(log_dir=f\"logs\\\\{time()}\")\n","2a2ba90f":"tf.compat.v1.disable_eager_execution() #disable eager execution,since got error AttributeError: Tensor.graph is meaningless when eager execution is enabled.","5dc26b24":"FEATURES=512\nweight_decay=1e-5","b4155b7d":"model = tf.keras.Sequential([\n    layers.Dense(300, activation='relu',\n                 kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n                 #kernel_regularizer=regularizers.l2(0.001),\n                 input_shape=(FEATURES,)),\n    layers.Dropout(0.1),\n        layers.Dense(300, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(300, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(300, activation='relu'),\n    layers.Dropout(0.1),\n    #layers.Dense(units, activation=activation_func),\n    #layers.SimpleRNN(100,unroll=True),\n    layers.Dense(6 ,activation='softmax')\n])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='sparse_categorical_crossentropy', patience=10)\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=74, batch_size=100)","7fd7bdd1":"model2 = tf.keras.Sequential([\n    layers.Dense(300, activation='relu',\n                 kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n                 #kernel_regularizer=regularizers.l2(0.001),\n                 input_shape=(FEATURES,)),\n    layers.Dropout(0.1),\n        layers.Dense(300, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(300, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(300, activation='relu'),\n    layers.Dropout(0.1),\n    #layers.Dense(units, activation=activation_func),\n    #layers.SimpleRNN(100,unroll=True),\n    layers.Dense(6 ,activation='softmax')\n])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='sparse_categorical_crossentropy', patience=10)\n\nmodel2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=1000)","fc4f782a":"model3 = tf.keras.Sequential([\n    layers.Dense(300, activation='relu',\n                 kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n                 #kernel_regularizer=regularizers.l2(0.001),\n                 input_shape=(FEATURES,)),\n    layers.Dropout(0.1),\n        layers.Dense(300, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(300, activation='relu'),\n    layers.Dropout(0.1),\n    layers.Dense(300, activation='relu'),\n    layers.Dropout(0.1),\n    #layers.Dense(units, activation=activation_func),\n    #layers.SimpleRNN(100,unroll=True),\n    layers.Dense(6 ,activation='softmax')\n])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='sparse_categorical_crossentropy', patience=10)\n\nmodel3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=1000)","4665a95d":"X_test.shape","2de9bf3d":" \ny_pred_keras = model2.predict(X_test)[:, :]\ny_pred_keras","c5388932":"max_index = np.argmax(y_pred_keras, axis=1)\nmax_index","3729069c":"y_test","dd64f094":"max_index-y_test","8cf41176":"sum((max_index-y_test)==0)\/X_test.shape[0] #verify the method to calculate accuracy, confirm the number matches val loss output","c4de677e":"df2= pd.read_csv('\/kaggle\/input\/polymerase512vecholdout\/HoldOut388_512vectors2WithTitles.csv')\ndf2.head()","133a3023":"df_imputed2=df2.drop(['title', 'abstract'], axis=1)\ndf_imputed2","59ff991e":"x_test2=df_imputed2\nx_test2.shape","42b57236":"y_pred_keras2 = model2.predict(x_test2)[:, :]\ny_pred_keras2","e877c11f":"max_indexMLP = np.argmax(y_pred_keras2, axis=1)\nmax_indexMLP","addb8679":"## compare MLP with RF predictions\n\n# RF results:\n# preds[0:20] \n# Out[48]:\n# array([[0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 1, 0],\n#        [0, 0, 0, 0, 1, 0],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1]], dtype=uint8)\n# [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5],\n\n# MLP results 1-20\n# [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, ","73b64449":"# RF preds[21:40]\n\n# Out[51]:\n# array([[0, 1, 0, 0, 0, 0],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 1, 0],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 1, 0],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 1, 0, 0, 0, 0],\n#        [0, 0, 0, 0, 0, 1],\n#        [0, 0, 0, 0, 1, 0]], dtype=uint8)\n#([1, 1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4],\n\n# MLP results 21-40\n# 5, 1, 1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4,","774cc021":"max_indexRF=[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n       1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 1, 5, 5,\n       1, 4, 1, 0, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 3, 5, 5,\n       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 4, 1, 4, 4, 1, 4, 5, 1, 1, 5, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 5, 1, 1, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n       4, 1, 0, 4, 4, 1, 4, 4, 3, 0, 1, 0, 0, 4, 1, 0, 5, 1, 3, 1, 3, 1,\n       0, 5, 1, 5, 3, 5, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n       1, 0, 0, 3, 5, 2, 0, 1, 1, 0, 4, 1, 3, 0, 0, 5, 1, 1, 0, 0, 5, 1,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n       5, 0, 5, 1, 5, 1, 1, 5, 1, 5, 1, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n       5, 1, 1, 1, 5, 0, 5, 1, 1, 0, 1, 0, 1, 5, 1, 4, 5, 1, 0, 1, 1, 5,\n       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 5, 0, 5, 0, 1, 5, 5, 5, 0, 0, 1,\n       4, 1, 5, 5, 0, 0, 5, 1, 5, 5, 1, 0, 5, 4, 0, 5, 1, 5, 1, 1, 1, 0,\n       1, 1, 5, 5, 1, 5, 5, 1, 1, 0, 1, 0, 0, 5, 0, 5, 1, 1, 1, 5, 0, 0,\n       5, 0, 0, 0, 5, 1, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 1, 1, 0, 0, 5,\n       1, 5, 5, 1, 5, 0, 1, 1, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 0, 0, 5, 1,\n       1, 0, 0, 1, 5, 0, 0, 1, 1, 5, 1, 5, 5, 1, 5, 5, 0, 0, 0, 5, 1, 1,\n       0, 0, 0, 4, 5, 0, 5, 1, 0, 1, 0, 5, 0, 0]\nmax_indexRF","2e7416bd":"max_indexMLP-max_indexRF","f26573b0":"sum((max_indexMLP-max_indexRF)==0)\/x_test2.shape[0]","dd11217c":"max_indexXGB=[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n       1, 5, 5, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 4, 5, 5,\n       1, 4, 1, 2, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 1, 5, 5,\n       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 4, 1, 4, 4, 1, 4, 5, 1, 1, 5, 5,\n       5, 1, 1, 1, 1, 1, 1, 1, 5, 1, 5, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n       4, 1, 0, 4, 4, 1, 4, 4, 1, 0, 1, 4, 3, 4, 1, 0, 1, 1, 5, 1, 3, 1,\n       1, 5, 1, 5, 3, 5, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n       1, 0, 0, 1, 5, 1, 0, 1, 1, 0, 4, 1, 3, 0, 0, 5, 0, 1, 0, 0, 5, 1,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n       5, 0, 5, 1, 5, 1, 1, 1, 2, 5, 5, 0, 5, 1, 3, 0, 1, 4, 0, 1, 1, 5,\n       5, 1, 1, 1, 5, 0, 5, 1, 1, 3, 1, 0, 1, 5, 1, 4, 5, 1, 5, 1, 1, 5,\n       1, 5, 0, 0, 1, 2, 5, 0, 0, 5, 1, 5, 2, 5, 1, 1, 5, 5, 5, 1, 0, 1,\n       4, 1, 5, 5, 5, 5, 5, 1, 5, 1, 1, 1, 5, 4, 0, 5, 1, 5, 1, 2, 2, 0,\n       1, 1, 5, 5, 1, 5, 5, 5, 2, 4, 1, 5, 1, 5, 0, 5, 1, 2, 1, 5, 5, 2,\n       5, 5, 1, 2, 5, 4, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 5, 1, 3, 3, 5,\n       5, 5, 5, 1, 5, 5, 1, 1, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 1,\n       1, 5, 5, 1, 5, 5, 0, 1, 1, 3, 1, 5, 5, 1, 5, 5, 2, 5, 2, 5, 1, 1,\n       5, 5, 3, 4, 5, 5, 5, 1, 3, 1, 5, 5, 0, 0]","426a602e":"sum((max_indexMLP-max_indexXGB)==0)\/x_test2.shape[0]","a8d5ee91":"sum((max_indexRF-max_indexXGB)==0)\/x_test2.shape[0]","96b61cb9":"type(max_indexMLP)","d02b7b5d":"type(max_indexRF)","c61e09ac":"type(max_indexXGB)","782be363":"npRF= np.asarray(max_indexRF, dtype=np.float32)\ntype(npRF)","04f85a26":"sum((npRF-max_indexXGB)==0)\/x_test2.shape[0]","f5a7a2bb":"npRF-max_indexXGB","eef2f047":"dfAll = np.array([[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n       1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 1, 5, 5,\n       4, 4, 1, 1, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 3, 5, 5,\n       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 1, 1, 4, 4, 1, 4, 5, 1, 1, 5, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 5, 1, 1, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n       4, 1, 0, 4, 4, 1, 4, 4, 3, 0, 1, 3, 3, 4, 1, 0, 5, 1, 3, 0, 3, 1,\n       0, 5, 1, 5, 3, 1, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n       1, 0, 0, 3, 5, 2, 0, 1, 1, 0, 4, 1, 1, 0, 0, 5, 1, 1, 0, 0, 5, 1,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n       5, 0, 1, 1, 5, 1, 1, 5, 2, 5, 5, 0, 5, 1, 3, 0, 1, 4, 0, 1, 1, 5,\n       5, 0, 1, 1, 5, 0, 5, 1, 2, 3, 1, 0, 1, 5, 1, 4, 5, 1, 5, 1, 2, 5,\n       1, 5, 0, 0, 1, 2, 5, 0, 0, 5, 1, 5, 2, 5, 5, 1, 5, 5, 5, 3, 0, 1,\n       4, 1, 5, 5, 1, 5, 5, 1, 5, 1, 1, 1, 5, 4, 0, 5, 1, 5, 1, 2, 2, 0,\n       1, 1, 5, 5, 1, 5, 5, 3, 2, 4, 1, 1, 5, 5, 0, 5, 1, 2, 1, 5, 1, 2,\n       5, 5, 1, 2, 5, 4, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 1, 1, 3, 3, 3,\n       5, 5, 5, 1, 5, 5, 1, 1, 5, 5, 1, 5, 5, 1, 1, 5, 5, 5, 5, 5, 5, 1,\n       5, 5, 5, 1, 5, 5, 0, 1, 1, 3, 1, 1, 5, 1, 5, 5, 2, 5, 2, 5, 1, 1,\n       5, 5, 3, 3, 5, 5, 5, 1, 1, 1, 5, 5, 3, 0], [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n       1, 5, 1, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 1, 5, 5,\n       1, 4, 1, 0, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 3, 5, 5,\n       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 4, 1, 4, 4, 1, 4, 5, 1, 1, 5, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 5, 1, 1, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n       4, 1, 0, 4, 4, 1, 4, 4, 3, 0, 1, 0, 0, 4, 1, 0, 5, 1, 3, 1, 3, 1,\n       0, 5, 1, 5, 3, 5, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n       1, 0, 0, 3, 5, 2, 0, 1, 1, 0, 4, 1, 3, 0, 0, 5, 1, 1, 0, 0, 5, 1,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n       5, 0, 5, 1, 5, 1, 1, 5, 1, 5, 1, 0, 5, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n       5, 1, 1, 1, 5, 0, 5, 1, 1, 0, 1, 0, 1, 5, 1, 4, 5, 1, 0, 1, 1, 5,\n       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 5, 0, 5, 0, 1, 5, 5, 5, 0, 0, 1,\n       4, 1, 5, 5, 0, 0, 5, 1, 5, 5, 1, 0, 5, 4, 0, 5, 1, 5, 1, 1, 1, 0,\n       1, 1, 5, 5, 1, 5, 5, 1, 1, 0, 1, 0, 0, 5, 0, 5, 1, 1, 1, 5, 0, 0,\n       5, 0, 0, 0, 5, 1, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 1, 1, 0, 0, 5,\n       1, 5, 5, 1, 5, 0, 1, 1, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 0, 0, 5, 1,\n       1, 0, 0, 1, 5, 0, 0, 1, 1, 5, 1, 5, 5, 1, 5, 5, 0, 0, 0, 5, 1, 1,\n       0, 0, 0, 4, 5, 0, 5, 1, 0, 1, 0, 5, 0, 0],[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 4, 4, 1, 5, 5, 5, 1,\n       1, 5, 5, 5, 5, 4, 5, 1, 1, 1, 5, 1, 4, 1, 5, 1, 5, 4, 3, 4, 5, 5,\n       1, 4, 1, 2, 1, 4, 1, 1, 0, 4, 0, 1, 5, 0, 4, 1, 5, 1, 5, 1, 5, 5,\n       1, 5, 5, 1, 1, 1, 1, 1, 1, 4, 4, 4, 1, 4, 4, 1, 4, 5, 1, 1, 5, 5,\n       5, 1, 1, 1, 1, 1, 1, 1, 5, 1, 5, 1, 1, 5, 0, 0, 5, 4, 1, 0, 1, 1,\n       4, 1, 0, 4, 4, 1, 4, 4, 1, 0, 1, 4, 3, 4, 1, 0, 1, 1, 5, 1, 3, 1,\n       1, 5, 1, 5, 3, 5, 1, 1, 1, 5, 4, 0, 1, 1, 4, 4, 4, 0, 4, 1, 0, 0,\n       1, 0, 0, 1, 5, 1, 0, 1, 1, 0, 4, 1, 3, 0, 0, 5, 0, 1, 0, 0, 5, 1,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 4, 5, 0, 5, 1, 0, 1,\n       5, 0, 5, 1, 5, 1, 1, 1, 2, 5, 5, 0, 5, 1, 3, 0, 1, 4, 0, 1, 1, 5,\n       5, 1, 1, 1, 5, 0, 5, 1, 1, 3, 1, 0, 1, 5, 1, 4, 5, 1, 5, 1, 1, 5,\n       1, 5, 0, 0, 1, 2, 5, 0, 0, 5, 1, 5, 2, 5, 1, 1, 5, 5, 5, 1, 0, 1,\n       4, 1, 5, 5, 5, 5, 5, 1, 5, 1, 1, 1, 5, 4, 0, 5, 1, 5, 1, 2, 2, 0,\n       1, 1, 5, 5, 1, 5, 5, 5, 2, 4, 1, 5, 1, 5, 0, 5, 1, 2, 1, 5, 5, 2,\n       5, 5, 1, 2, 5, 4, 5, 0, 5, 5, 1, 0, 5, 5, 5, 1, 5, 5, 1, 3, 3, 5,\n       5, 5, 5, 1, 5, 5, 1, 1, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 1,\n       1, 5, 5, 1, 5, 5, 0, 1, 1, 3, 1, 5, 5, 1, 5, 5, 2, 5, 2, 5, 1, 1,\n       5, 5, 3, 4, 5, 5, 5, 1, 3, 1, 5, 5, 0, 0]])\n","33c08d85":"dfAll ","ee2bc5bf":"dfAll = pd.DataFrame(dfAll)\n","1a2279e2":"dfAll","4045ff44":"dfAll.mode(axis=0)","ad413b9f":"dfAll.mode(axis=0).isnull().sum()","98a592e8":"x_test2.shape[0]-dfAll.mode(axis=0).isnull().sum().sum()\/2","e2f3cbc2":"## Validation method 2\n## Compare MLP with RF predictions","24de47a6":"## Majority Vote\n### For this purpose, I will stack these three predictions and count the mode of each column.","9ed25b5d":"## XGB predictions match well with MLP predictions with 89.7% agreement.\n#### 0.8943298969072165  \/ 0.8969072164948454","2762fa88":"## Predict Hold Out dataset (new papers, no label)","7996ae9e":"## Introduction\nIn Part 9 KMeans Clustering vs LDA Topic Modeling. we did topic modeling on the 1941 papers mentioned Polymerase.\n\nLink to Part 9 KMeans Clustering vs LDA Topic Modeling\n#### https:\/\/www.kaggle.com\/leijiang1\/part-9-kmeans-clustering-vs-lda-topic-modeling\n\n\nThe topics identified as below:\n### Topics:\n#### G0 flu children clinical\n#### G1 gene protein cell \n#### G2 bat host phylogenetic \n#### G3 SARS COV MERS COV\n#### G4 diganosis asssay detection \n#### G5 RdRp RNA replication\n\n\n\n#### Topic Modeling can help us quickly gain the big picture of the major topics in a large collection of papers. It also helps by giving a rough index so you know where to look for the type of information you want (which cluster you should go). For example, if you are looking for information about diagnosis assay, you should probably go to \"Group 5 diagnosis assay detection\" to find information faster. If you are interested about \"children\" and \"clinical\" related information, you should go to \"Group 1 flu children clinical\".\n\nGroup these papers also helps with question answering tasks. For example, we know which subgroup of paper to go and ask a question based on the main topic of the question.\n\n## Interactive 3D visualization of Clusters\n\n#### In addition, I created an interactive 3D plot of the previous 6 clusters of papers in Java. \n#### You can drag the mouse to change angles of view. \n#### It is interesting to see how the papers distributed in space and view the relative positions to other papers.\n![image.png](attachment:image.png)\n\n#### Figure 1. 3D visual\nThe code is on my GitHub.\n\nhttps:\/\/github.com\/lj89\/Data-Visualization\/tree\/master\/3Djava","b5d17ea6":"### Therefore, I run a GXB. The result is as below:\n","c6016fd4":"## Future work:\n#### improvement consider it is a imbalance dataset use class weight","8b15f4b6":"## For the two methods, the first 20 predictions match perfectly!","f4d2b088":"## Code\n#### The code for MLP model is as below. The code for preparing the data and other classifiers are on GitHub:\n#### https:\/\/github.com\/lj89\/CORD19\/tree\/master\/Part%2010%20Classify%20a%20new%20paper%20with%20RF%2C%20XGB%20and%20MLP","72498148":"### From the calculation above, we know there are 10 instances that all three methods produce different predictions. So the majority vote would not work. For a problem like this with 6 classes, we need 7 different models to ensemble to make sure there is a majority vote result.","b555120b":"## How well MLP and RF predictions match?\n### 0.8144329896907216\n### 81.4%\n\n### If we have a third algo results, we can have a ensemble model with majority vote.","8c0c1f5d":"\n\n\n## References\n\n[1] Minaee et al. Deep Learning Based Text Classification: A Comprehensive Review\n\n[2]  COVID-19 therapeutics: design of inhibitors of the SARS-CoV-2 main protease\nhttps:\/\/www.sidneypaulymer.com\/posts\/covid-19-therapeutics-design-of-inhibitors-of-the-sars-cov-2-main-protease","06d6e37d":"## XGB vs RF: agree on 81.2% of the predictions","977d13f9":"\n\n\n\n## Method\n#### Use these topics as labels, we can use the 1941 papers (subset on March 26) as a training dataset to build a classification model that classify new papers. I use 388 papers newly updated papers (subet of April 12-subset on March 26) as a Hold Out test dataset to give these new papers a topic label.\n\n#### For this 6 classes classification problem:\n### Random Forest (RF), XGBoost (XGB) and Multilayer perceptron (MLP) classifiers with good prediction accuracies are built.\n#### In addition, ensemble model of all three is built to make the modle better generalizable.\n\n### Steps:\n#### Use Universal Sentence Encoder (USE) to vectorize abstracts to 512 dimension vectors.\n#### Use the G0-5 as the true labels to train classifers.\n#### Validate each classifier achieved good performance.\n#### Majority vote of RF, GXB, MLP to generate the final prediction.\n\n## Results\n\n#### Since the new papers are without label, I used the three models to kind of cross validate the results.\nXGB predictions match well with MLP predictions with 89.7% agreement.\n\nXGB vs RF: agree on 81.2% of the predictions.\n\nMLP vs RF: agree on 81.4% of the predictions. \n\n\n#### For the majority vote results, there are 10 instances that all three methods produce different predictions. So the majority vote would not work. For a problem like this with 6 classes, we need at least 7 different models to ensemble to make sure there is a majority vote result.\n\n\n#### The performance of each type of model varies. The MLP model by itself yielded 0.8329 accuracy on a validation dataset.\n\n![image.png](attachment:image.png)\n#### Figure 2. Confusion Matrix of the RF classifier for paper classification.\n\n\n#### The table below shows the RF classifer performance. Group 2,3 had not so ideal results due to they are less common in the training dataset.\n              precision    recall  f1-score   support\n\n           0       0.92      0.80      0.86       456\n           1       0.87      0.69      0.77       679\n           2       1.00      0.02      0.05        41\n           3       0.00      0.00      0.00       132\n           4       0.91      0.35      0.51       222\n           5       0.92      0.75      0.83       411\n\n   micro avg       0.90      0.63      0.74      1941\n   macro avg       0.77      0.44      0.50      1941\nweighted avg       0.84      0.63      0.70      1941\n samples avg       0.63      0.63      0.63      1941\n\n\n\n\n## Conclusion\n#### Now when there is a new paper come, we have a model that can predict which group the paper could be belong to with a pretty decent accuracy. \n\n\n## Future work\n#### For this classification task:\n#### To improve the models, consider it is a imbalance dataset use class weight.\n#### Parameters tuning for RF and XGB with a thorough Grid search.\n#### Neural Architecture search for MLP.\n\n## Discussion\nAs I have solid five years graduate level biomedical research experience and had read hundreds of papers in this domain in the past, I feel compelled need to contribute to this knowledge discovery project.  I start most analysis at abstract level because I think abstract (even the title)  is a good starting point for this project. First, abstract is the first thing we read in a paper in most cases. Additionally, according to my own experience and others\u2019 posts, the take home messages of a paper usually could be summarized from abstract and conclusion section. In addition, the best practice is also to confirm that in discussion.  When conducting research, for most papers we just read the abstracts and a few important figures and conclusions. Only a small amount of papers need to read thoroughly. \n\nIf we want a broader scope of an overview in the field, introduction is a good place to find this type of information. If we need to go deep, we can even go to the references to find out the related papers.\n\n### Future direction\nAlso search on well known authors in a particular field and systematically analyze most of their papers. It must be very interesting to  see how their interest and research developed.\n\nI already tried to generate summary of the papers from abstract and combine these summaries of papers about a certain topic to generate a more complete summary about the question of interest. The results from this process could be bias sometimes.  Thus, I also want to try utilize the information in the conclusion section to help make the summary more accurate.\n\nFurther down the road, I hope we can train some model or build some pipelines to read the experimental results and generate summaries on that.\n\nIn addition, the number of times a paper was cited can be a feature to predict how important this paper is. So try to integrate this information in the most related article recommender.\n\nThere are many challenges in this project. For example, what should we do when the conclusions in papers are contradicting? How can we teach our algorithms critical thinking?\n\n\n#### All in all, I learn a lot in this project.\n\n\n#### A good PubMed dataset was mentioned in a research paper [1] can be a good source for training a domain specific model for this data collection.\n> \u201cEach sentence in an abstract is labeled with its role in the abstract using one of the following classes: background, objective, method, result, or conclusion.\u201d\n\n#### It is so cool that Protease inhibitors could be designed once the target is chosen [2].\n\n\n\n"}}