{"cell_type":{"d3ebc74d":"code","0cbe7b65":"code","4f21cd25":"code","fef0efaa":"code","0a08f75f":"code","f0d130df":"code","62439c4f":"code","f7d9d497":"code","63893548":"code","bb4412de":"code","af102f9d":"code","3c7c8b0f":"code","50db085f":"code","a1f06fb2":"code","ecdf3829":"code","f94994dc":"code","c2163b17":"code","12829070":"code","468a4654":"code","15f8a1dd":"code","72aa6ea0":"code","d06beb0c":"code","32a670b8":"code","d5cab080":"code","874c49bf":"code","1e235fd9":"code","83872696":"code","57d5768b":"code","1b8fa452":"code","a387e33a":"code","532f0b3d":"markdown","89701fba":"markdown","cdbec0b7":"markdown","e00dbce9":"markdown","acc9ad31":"markdown","7a6afc75":"markdown","b43a63f3":"markdown","48a4caea":"markdown","a966048b":"markdown","98a2c455":"markdown","62645972":"markdown","6c28c75a":"markdown","dc24f392":"markdown","00dac70b":"markdown"},"source":{"d3ebc74d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0cbe7b65":"# let's start on mercedes car\ncclass = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/cclass.csv')\nfocus = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/focus.csv')\naudi = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/audi.csv')\ntoyota = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/toyota.csv')\nskoda = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/skoda.csv')\nford = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/ford.csv')\nvauxhall = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vauxhall.csv')\nbmw = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv')\nvw = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vw.csv')\nhyundai = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/hyundi.csv')\nmerc = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/merc.csv')\ndata = toyota.copy()\ndata.head()","4f21cd25":"data.info()","fef0efaa":"# check price distribution\nsns.histplot(data['price'], bins=30)","0a08f75f":"def plot_numerical(feature):\n    ax = sns.lmplot(x=feature, y='price', data=data)\n    ax.set_xticklabels(rotation=85)\n    plt.show()\n    \ndef plot_categorical(feature, figsize=None):\n    df = data.groupby([feature])['price'].describe()[['mean', '50%', 'min', 'count']]\n\n    labels = df.index.values\n    x = np.arange(len(labels))\n    width = 0.9\n    fig, ax1 = plt.subplots(figsize=(12, 5))\n\n    # plot bars for min, median and mean house price\n    rects1 = ax1.bar(x-width\/2, df['50%'], width\/3, label='median')\n    rects2 = ax1.bar(x-width\/6, df['mean'], width\/3, label='mean')\n    rects3 = ax1.bar(x+width\/6, df['min'], width\/3, label='min')\n\n    ax1.set_ylabel('price', fontsize=15)\n    ax1.set_title(feature, fontsize=18)\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(labels, rotation=85)\n    ax1.legend()\n\n    # plot counts of data points\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Counts', fontsize=15)\n    ax2.plot(x-width\/2, df['count'], color='red', linestyle='dashed')\n\n    # annotate counts of data points\n    for i, rect in enumerate(rects2):\n        height = int(round(rect.get_height()))\n        ax1.annotate('{}'.format(int(df['count'].iloc[i])),\n                     xy=(rect.get_x() + rect.get_width()\/2, height),\n                     xytext=(0, 3), textcoords=\"offset points\",\n                     ha='center', va='bottom', color='red')\n    plt.show()","f0d130df":"for feature in ['model', 'transmission', 'fuelType']:\n    plot_categorical(feature)","62439c4f":"for feature in ['year', 'mileage', 'tax', 'mpg', 'engineSize']:\n    plot_numerical(feature)","f7d9d497":"categorical_features = ['model', 'transmission', 'fuelType']\nnumerical_features = ['year', 'mileage', 'tax', 'mpg', 'engineSize']","63893548":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport joblib","bb4412de":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    le = LabelEncoder()\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    #print(feature)\n    \n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))","af102f9d":"plt.figure(figsize=(8, 5))\nsns.heatmap(round(data[numerical_features].corr(method='spearman'), 2), \n            annot=True, mask=None, cmap='GnBu')\nplt.show()","3c7c8b0f":"plt.figure(figsize=(10, 5))\nsns.heatmap(round(df[categorical_features+numerical_features+['price']].corr(method='spearman'), 2), annot=True,\n            mask=None, cmap='GnBu')\nplt.show()\n","50db085f":"from statsmodels.stats.outliers_influence import variance_inflation_factor","a1f06fb2":"# Calculating VIF\nvif = pd.DataFrame()\nvif[\"variables\"] = [feature for feature in categorical_features+numerical_features if feature not in ['year']]\nvif[\"VIF\"] = [variance_inflation_factor(df[vif['variables']].values, i) for i in range(len(vif[\"variables\"]))]\nprint(vif)","ecdf3829":"NumericData = data[['mileage']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","f94994dc":"NumericData = data[['year']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","c2163b17":"NumericData = data[['engineSize']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","12829070":"NumericData = data[['tax', 'mpg']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","468a4654":"# Percentage of outliers present in each variable\noutlier_percentage = {}\nfor feature in numerical_features:\n    tempData = data.sort_values(by=feature)[feature]\n    Q1, Q3 = tempData.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    Lower_range = Q1 - (1.5 * IQR)\n    Upper_range = Q3 + (1.5 * IQR)\n    outlier_percentage[feature] = round((((tempData<(Q1 - 1.5 * IQR)) | (tempData>(Q3 + 1.5 * IQR))).sum()\/tempData.shape[0])*100,2)\noutlier_percentage","15f8a1dd":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    \n    le = LabelEncoder()\n    ohe = OneHotEncoder(sparse=False)\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))\n    # load classes\n    columns = joblib.load(\n        open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'rb'))\n\n    if len(le.classes_)>2:\n        # perform one hot encoding\n        ohe.fit(df[[feature]])\n        # save the encoder\n        joblib.dump(ohe, \n                    open(os.path.join(path, \"TextEncoding\/ohe_{}.sav\".format(feature)), 'wb'))\n\n        # transfrom training data\n        # removing first column of encoded data to elude from dummy variable trap\n        tempData = ohe.transform(df[[feature]])[:, 1:]\n\n        # create Dataframe with columns as classes\n        tempData = pd.DataFrame(tempData, columns=columns)\n    else:\n        tempData = df[[feature]]\n    \n    # create dataframe with all the label encoded categorical features along with hot encoding\n    if i==0:\n        encodedData = pd.DataFrame(data=tempData, columns=tempData.columns.values.tolist())\n    else:\n        encodedData = pd.concat([encodedData, tempData], axis=1)","72aa6ea0":"# merge numerical features and categorical encoded features\ndf = df[numerical_features+['price']]\ndf = pd.concat([df, encodedData], axis=1)\ndf.info()","d06beb0c":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics, preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score","32a670b8":"train_data = df.copy()\nfeature_cols = [feature for feature in train_data.columns if feature not in(['price'])]\nprint('features used: ', feature_cols)\n\n# RESCALING\n#scaler = MinMaxScaler()\n#scaler.fit(train_data[feature_cols])\n#train_data[feature_cols] = scaler.transform(train_data[feature_cols])","d5cab080":"X = train_data[feature_cols]\ny = train_data['price']\n\nvalidation_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_size, random_state=0)","874c49bf":"model = LinearRegression()\nmodel.fit(X_train, y_train)","1e235fd9":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_train, y_pred)))\nprint('r2_score: ', round(r2_score(y_train, y_pred)*100, 2))\n\ny_pred = model.predict(X_test)\n\nprint('Validation metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('r2_score: ', round(r2_score(y_test, y_pred)*100, 2))","83872696":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(y_pred[-150:]))), y=y_pred[-150:],\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y_test[-150:]))), y=y_test[-150:],\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","57d5768b":"model = XGBRegressor( \n    n_estimators = 1000,\n    learning_rate=0.09, \n    min_child_weight=5,\n    max_depth = 3,\n    subsample = 0.75,\n    seed=7)\n\n\nmodel = model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    #early_stopping_rounds=10,\n    #eval_set=[(X_test, y_test)],\n    verbose=False)","1b8fa452":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_train, y_pred)))\nprint('r2_score: ', round(r2_score(y_train, y_pred)*100, 2))\n\ny_pred = model.predict(X_test)\n\nprint('Validation metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('r2_score: ', round(r2_score(y_test, y_pred)*100, 2))","a387e33a":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(y_pred[-150:]))), y=y_pred[-150:],\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y_test[-150:]))), y=y_test[-150:],\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","532f0b3d":"### Label encoding categorical features for correlation","89701fba":"# Handling Categorical Features (Label Encoding & One Hot Encoding)","cdbec0b7":"### Bivariate Analysis Correlation plot with the Categorical variables","e00dbce9":"# Analyzing features using VIF","acc9ad31":"# Model 1: Linear Regresssion ","7a6afc75":"# Looking at Outliers","b43a63f3":"* mpg - miles per gallon\n* tax - road tax","48a4caea":"# Training Model","a966048b":"# Model 2: XGB","98a2c455":"# CORRELATION","62645972":"**Observations-**\n* year - mileage -ve\n* mpg - tax -ve\n* year - mileage\n* transmission - fuelType\n* fuelType - engineSize -ve","6c28c75a":"# EDA","dc24f392":"### Bivariate Analysis Correlation plot for numerical features","00dac70b":"**Observations-**\n* model - Supra models are the costliest ones while Aygo and Yaris are the most popular ones\n* transmission - Manual has usually low cost\n* fuelType - Petrol models are the cheapest ones\n* year - new cars are sold at higher prices\n* mileage - lower the mileage or car travelled, higher the price\n* mpg - lower the mpg, higher the car price (usually heavy or luxury cars have lower mpg)\n* engineSize - bigger the enginer, higher the price\n* tax - generally higher the tax, higher the car price"}}