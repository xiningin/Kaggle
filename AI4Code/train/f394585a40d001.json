{"cell_type":{"921ff903":"code","4029728e":"code","c81d51cd":"code","4be4fc96":"code","07c3923c":"code","837a3452":"code","e4be90c3":"code","ac0f5ba0":"code","63ea634f":"code","1b62ca5b":"code","9453c2e7":"code","88fbb6c0":"code","acb18ab8":"code","a9b243ea":"code","c279445d":"code","2e53f5fb":"code","9d521130":"code","00cc817f":"code","621357d5":"code","40f1f204":"code","18a0141f":"code","025a40be":"code","befc7ccf":"code","9e8ff873":"code","097b4dca":"code","b649e5d6":"code","2f4dac91":"code","64ca78ca":"code","d14aa99f":"code","df243ca6":"code","b2b56861":"code","6dcc6599":"code","96a3532f":"code","f65fbcdf":"code","fec99404":"code","cad4b13b":"code","899ff98f":"code","7dc4d97e":"code","77872472":"code","d908f3c4":"code","8bfc1753":"code","2713dfd2":"markdown","bc725195":"markdown","30c1a414":"markdown","ffa30bfc":"markdown","4c164a0e":"markdown","ab71a3e6":"markdown","10e17c3f":"markdown","1110c81e":"markdown","a2d014ee":"markdown","60417e86":"markdown","297b65c1":"markdown","d37ce986":"markdown","40ce0590":"markdown","101c8675":"markdown","2aa7faf0":"markdown","e1008e38":"markdown","5e599c75":"markdown","b28d1b61":"markdown","8ba9ede7":"markdown","457541ae":"markdown","9a24529f":"markdown","cf01b93d":"markdown","08062b4c":"markdown","d36352c0":"markdown","684bc477":"markdown","82319d60":"markdown","e0e5341c":"markdown","d51176e7":"markdown","c10bdb79":"markdown","3be93ae4":"markdown","086c6c82":"markdown","d019e6d2":"markdown","8fe8f415":"markdown","11c1c33b":"markdown","64a6d88d":"markdown","251e631e":"markdown","5a3503a3":"markdown","d10aeffb":"markdown","32bf0d19":"markdown","3742c4ff":"markdown","4a364d7e":"markdown","fbd3ff60":"markdown","a17fa28f":"markdown","0f0ffc16":"markdown","6c346cde":"markdown","d532336e":"markdown","a2717134":"markdown"},"source":{"921ff903":"import sys\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport os","4029728e":"df = pd.read_csv(\"..\/input\/amazon-fine-food-corpus\/corpus.csv\")\ndf.head()","c81d51cd":"df1k = df.loc[:119999,:]\nprint(\"Shape:- \",df1k.shape)\nprint(df1k.head())\ndf1k['Score'].value_counts()","4be4fc96":"from sklearn.model_selection import TimeSeriesSplit\ndef timesplit(x,y):\n    ts = TimeSeriesSplit(n_splits = 4)\n    for train_index,test_index in ts.split(x):\n        x_train,x_test = x[train_index],x[test_index]\n        y_train,y_test = y[train_index],y[test_index]\n    return x_train,y_train,x_test,y_test\n\nx_train,y_train,x_test,y_test = timesplit(df1k[\"Text\"].values,df1k[\"Score\"].values)","07c3923c":"def imp_features(model,classifier):\n    voc = model.get_feature_names()\n    w = list(classifier.coef_[0])\n    pos_coef = []\n    neg_coef = []\n    pos_words = []\n    neg_words = []\n    for i,c in enumerate(w):\n        if c > 0:\n            pos_coef.append(c)\n            pos_words.append(voc[i])\n        if c < 0:\n            neg_coef.append(abs(c))\n            neg_words.append(voc[i])\n    pos_df = pd.DataFrame(columns = ['Words','Coef'])\n    neg_df = pd.DataFrame(columns = ['Words','Coef'])\n    pos_df['Words'] = pos_words\n    pos_df['Coef'] = pos_coef\n    neg_df['Words'] = neg_words\n    neg_df['Coef'] = neg_coef\n    pos_df = pos_df.sort_values(\"Coef\",axis = 0,ascending = False).reset_index(drop=True)\n    neg_df = neg_df.sort_values(\"Coef\",axis = 0,ascending = False).reset_index(drop=True)\n    print(\"Shape of Positive dataframe:- ,\",pos_df.shape)\n    print(\"Shape of Negative dataframe:- \",neg_df.shape)\n    print(\"Top ten positive predictors:- \\n\",pos_df.head(10))\n    print(\"\\nTop ten negative predictors:- \\n\",neg_df.head(10))","837a3452":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nbow_train = cv.fit_transform(x_train)\nprint(\"Shape of BOW vector:- \",bow_train.shape)","e4be90c3":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler(with_mean=False)\nbow_train = sc.fit_transform(bow_train)","ac0f5ba0":"%%time\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty = 'l2',solver = 'sag')\nparam_grid = {\"C\":[0.01,0.1,1,5,10,50]}\ngs = GridSearchCV(classifier,param_grid,cv = 5,scoring = 'f1_micro',n_jobs = -1)\ngs.fit(bow_train,y_train)\nprint(\"Best parameter:- \",gs.best_params_)\nprint(\"Best score:- \",gs.best_score_)","63ea634f":"%%time\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nbow_test = cv.transform(x_test)\nbow_test = sc.transform(bow_test)   # Standardizing the test data\nclassifier = LogisticRegression(C=1,penalty = 'l2',solver = 'sag',n_jobs = -1)\nclassifier.fit(bow_train,y_train)\ny_pred = classifier.predict(bow_test)\nprint(\"BOW test accuracy:- \",accuracy_score(y_test,y_pred))\nprint(\"F1 score:- \",f1_score(y_test,y_pred,average='micro'))\nprint(\"Training accuracy:- \",accuracy_score(y_train,classifier.predict(bow_train)))","1b62ca5b":"cm = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix:- \\n\",cm)\nsns.heatmap(cm,annot = True)","9453c2e7":"%%time\nfrom scipy.sparse import csr_matrix\ncoef = classifier.coef_    #weight vector of original classifier\ne = 0.02 # introducing small error in the training dataset\nbow_train_pert = csr_matrix(bow_train,dtype=np.float64)\nbow_train_pert[np.nonzero(bow_train_pert)]+=e\nclassifier_pert = LogisticRegression(C=1,penalty = 'l2',solver = 'sag',n_jobs = -1)\nclassifier_pert.fit(bow_train_pert,y_train)\ncoef_pert = classifier_pert.coef_\ncoef_diff = coef_pert - coef\nprint(\"Average difference in weight vectors:- \",np.mean(coef_diff))","88fbb6c0":"imp_features(cv,classifier)","acb18ab8":"y_pred_prob = classifier.predict_proba(bow_test)\nsns.set_style(\"whitegrid\")\nplt.figure(1,figsize = (12,5))\nplt.subplot(121)\ndelta0 = y_test[y_test == 0] - y_pred_prob[y_test==0,0]\nsns.kdeplot(np.array(delta0))\nplt.title(\"Error plot for class 0\")\nplt.subplot(122)\ndelta1 = y_test[y_test == 1] - y_pred_prob[y_test==1,1]\nsns.kdeplot(np.array(delta1))\nplt.title(\"Error plot for class 1\")\nplt.show()","a9b243ea":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True)\ntfidf_train = tfidf.fit_transform(x_train)\nprint(\"Shape of tfidf_train:- \",tfidf_train.shape)","c279445d":"sc = StandardScaler(with_mean = False)\ntfidf_train = sc.fit_transform(tfidf_train)","2e53f5fb":"%%time\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty = 'l1',solver = 'liblinear',class_weight = 'balanced')\nparam_grid = {\"C\":[0.01,0.1,1,10,50]}\ngs = GridSearchCV(classifier,param_grid,cv = 5,scoring = 'f1',n_jobs = -1)\ngs.fit(tfidf_train,y_train)\nprint(\"Best parameter:- \",gs.best_params_)\nprint(\"Best score:- \",gs.best_score_)","9d521130":"%%time\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\ntfidf_test = tfidf.transform(x_test)\ntfidf_test = sc.transform(tfidf_test)   # Standardizing the test data\nclassifier = LogisticRegression(C=0.01,penalty = 'l1',solver = 'liblinear',class_weight = 'balanced')\nclassifier.fit(tfidf_train,y_train)\ny_pred = classifier.predict(tfidf_test)\nprint(\"Tfdif test accuracy:- \",accuracy_score(y_test,y_pred))\nprint(\"F1 score:- \",f1_score(y_test,y_pred))\nprint(\"Training accuracy:- \",accuracy_score(y_train,classifier.predict(tfidf_train)))","00cc817f":"%%time\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\ntfidf_test = tfidf.transform(x_test)\ntfidf_test = sc.transform(tfidf_test)   # Standardizing the test data\nclassifier = LogisticRegression(C=0.01,penalty = 'l1',solver = 'liblinear')\nclassifier.fit(tfidf_train,y_train)\ny_pred = classifier.predict(tfidf_test)\nprint(\"Tfdif test accuracy:- \",accuracy_score(y_test,y_pred))\nprint(\"F1 score:- \",f1_score(y_test,y_pred))\nprint(\"Training accuracy:- \",accuracy_score(y_train,classifier.predict(tfidf_train)))","621357d5":"cm = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix:- \\n\",cm)\nsns.heatmap(cm,annot = True)","40f1f204":"cm = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix:- \\n\",cm)\nsns.heatmap(cm,annot = True)","18a0141f":"print(\"No. of features with zero coefficients:- \",tfidf_train.shape[1]-np.count_nonzero(classifier.coef_))\nimp_features(tfidf,classifier)","025a40be":"y_pred_prob = classifier.predict_proba(tfidf_test)\nsns.set_style(\"whitegrid\")\nplt.figure(1,figsize = (12,5))\nplt.subplot(121)\ndelta0 = y_test[y_test == 0] - y_pred_prob[y_test==0,0]\nsns.kdeplot(np.array(delta0))\nplt.title(\"Error plot for class 0\")\nplt.subplot(122)\ndelta1 = y_test[y_test == 1] - y_pred_prob[y_test==1,1]\nsns.kdeplot(np.array(delta1))\nplt.title(\"Error plot for class 1\")\nplt.show()","befc7ccf":"y_pred_prob = classifier.predict_proba(tfidf_test)\nsns.set_style(\"whitegrid\")\nplt.figure(1,figsize = (12,5))\nplt.subplot(121)\ndelta0 = y_test[y_test == 0] - y_pred_prob[y_test==0,0]\nsns.kdeplot(np.array(delta0))\nplt.title(\"Error plot for class 0\")\nplt.subplot(122)\ndelta1 = y_test[y_test == 1] - y_pred_prob[y_test==1,1]\nsns.kdeplot(np.array(delta1))\nplt.title(\"Error plot for class 1\")\nplt.show()","9e8ff873":"#Function to create list of sentences\ndef sent_list(x):\n    list_of_sent = []\n    for sent in tqdm(x):\n        words = []\n        for w in sent.split():\n            words.append(w)\n        list_of_sent.append(words)\n    return list_of_sent","097b4dca":"#implementing word2vec\nfrom gensim.models import Word2Vec\nsent_train = sent_list(x_train)\nw2v = Word2Vec(sent_train,size=50,min_count=2,workers=4)","b649e5d6":"#Function to create avg word2vec vector\ndef avgw2v(x):\n    avgw2v_vec = []\n    for sent in tqdm(x):\n        sent_vec = np.zeros(50)\n        count = 0\n        for word in sent:\n            try:\n                vec = w2v.wv[word]\n                sent_vec+=vec\n                count+=1\n            except:\n                pass\n        sent_vec\/=count\n        avgw2v_vec.append(sent_vec)\n    return avgw2v_vec","2f4dac91":"#Creating average word2vec training data\navgw2v_train = np.array(avgw2v(sent_train))\nprint(\"Shape of avg word2vec train data:- \",avgw2v_train.shape)","64ca78ca":"sc = StandardScaler()\navgw2v_train = sc.fit_transform(avgw2v_train)","d14aa99f":"%%time\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty = 'l2',solver='sag',class_weight = 'balanced')\nparam_grid = {\"C\":[0.01,0.1,1,10,50]}\ngs = GridSearchCV(classifier,param_grid,cv = 5,scoring = 'f1',n_jobs = -1)\ngs.fit(avgw2v_train,y_train)\nprint(\"Best parameter:- \",gs.best_params_)\nprint(\"Best score:- \",gs.best_score_)","df243ca6":"%%time\nsent_test = sent_list(x_test)\navgw2v_test = np.array(avgw2v(sent_test))\navgw2v_test = sc.transform(avgw2v_test)\nclassifier = LogisticRegression(C=1,penalty = 'l2',solver = 'sag',class_weight = 'balanced')\nclassifier.fit(avgw2v_train,y_train)\ny_pred = classifier.predict(avgw2v_test)\nprint(\"Avg Word2Vec test accuracy:- \",accuracy_score(y_test,y_pred))\nprint(\"F1 score:- \",f1_score(y_test,y_pred))\nprint(\"Training accuracy:- \",accuracy_score(y_train,classifier.predict(avgw2v_train)))","b2b56861":"%%time\nsent_test = sent_list(x_test)\navgw2v_test = np.array(avgw2v(sent_test))\navgw2v_test = sc.transform(avgw2v_test)\nclassifier = LogisticRegression(C=1,penalty = 'l2',solver = 'sag')\nclassifier.fit(avgw2v_train,y_train)\ny_pred = classifier.predict(avgw2v_test)\nprint(\"Avg Word2Vec test accuracy:- \",accuracy_score(y_test,y_pred))\nprint(\"F1 score:- \",f1_score(y_test,y_pred))\nprint(\"Training accuracy:- \",accuracy_score(y_train,classifier.predict(avgw2v_train)))","6dcc6599":"cm = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix:- \\n\",cm)\nsns.heatmap(cm,annot = True)","96a3532f":"cm = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix:- \\n\",cm)\nsns.heatmap(cm,annot = True)","f65fbcdf":"y_pred_prob = classifier.predict_proba(avgw2v_test)\nsns.set_style(\"whitegrid\")\nplt.figure(1,figsize = (12,5))\nplt.subplot(121)\ndelta0 = y_test[y_test == 0] - y_pred_prob[y_test==0,0]\nsns.kdeplot(np.array(delta0))\nplt.title(\"Error plot for class 0\")\nplt.subplot(122)\ndelta1 = y_test[y_test == 1] - y_pred_prob[y_test==1,1]\nsns.kdeplot(np.array(delta1))\nplt.title(\"Error plot for class 1\")\nplt.show()","fec99404":"#Function for creating tfidf weighted Word2Vec\ndef tfidfw2v(x):\n    dictionary = dict(zip(tfidf.get_feature_names(),list(tfidf.idf_)))\n    tfidf_w2v_vec = []\n    i=0\n    for sent in tqdm(x):\n        sent_vec = np.zeros(50)\n        weights = 0\n        for word in sent:\n            try:\n                vec = w2v.wv[word]\n                tfidf_value = dictionary[word]*sent.count(word)\n                sent_vec+=(tfidf_value*vec)\n                weights+=tfidf_value\n            except:\n                pass\n        sent_vec\/=weights\n        tfidf_w2v_vec.append(sent_vec)\n        i+=1\n    return tfidf_w2v_vec","cad4b13b":"tfidfw2v_train = np.array(tfidfw2v(sent_train))\nprint(\"Shape of tfidf avgw2v train vector:- \",tfidfw2v_train.shape)","899ff98f":"sc = StandardScaler()\ntfidfw2v_train = sc.fit_transform(tfidfw2v_train)","7dc4d97e":"%%time\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(penalty = 'l2',solver='sag',class_weight = 'balanced')\nparam_grid = {\"C\":[0.1,1,10,50,100]}\ngs = GridSearchCV(classifier,param_grid,cv = 5,scoring = 'f1',n_jobs = -1)\ngs.fit(tfidfw2v_train,y_train)\nprint(\"Best parameter:- \",gs.best_params_)\nprint(\"Best score:- \",gs.best_score_)","77872472":"%%time\nsent_test = sent_list(x_test)\ntfidfw2v_test = np.array(tfidfw2v(sent_test))\ntfidfw2v_test = sc.transform(tfidfw2v_test)\nclassifier = LogisticRegression(C=10,penalty = 'l2',solver = 'sag',class_weight = 'balanced')\nclassifier.fit(tfidfw2v_train,y_train)\ny_pred = classifier.predict(tfidfw2v_test)\nprint(\"Tfidf Word2Vec test accuracy:- \",accuracy_score(y_test,y_pred))\nprint(\"F1 score:- \",f1_score(y_test,y_pred))\nprint(\"Training accuracy:- \",accuracy_score(y_train,classifier.predict(tfidfw2v_train)))","d908f3c4":"cm = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix:- \\n\",cm)\nsns.heatmap(cm,annot = True)","8bfc1753":"y_pred_prob = classifier.predict_proba(tfidfw2v_test)\nsns.set_style(\"whitegrid\")\nplt.figure(1,figsize = (12,5))\nplt.subplot(121)\ndelta0 = y_test[y_test == 0] - y_pred_prob[y_test==0,0]\nsns.kdeplot(np.array(delta0))\nplt.title(\"Error plot for class 0\")\nplt.subplot(122)\ndelta1 = y_test[y_test == 1] - y_pred_prob[y_test==1,1]\nsns.kdeplot(np.array(delta1))\nplt.title(\"Error plot for class 1\")\nplt.show()","2713dfd2":"### Word2Vec","bc725195":"#### Error Plots:-","30c1a414":"### Error plot of model with class_weight as None","ffa30bfc":"### Error plots","4c164a0e":"### Testing the model with class_weight as None","ab71a3e6":"### ==================================================================================","10e17c3f":"## Results:-\n\n| Model | Hyperparameter(C) | Training Accuracy | Test Accuracy |\n| - | - | - | - |\n| Bag of Words | 1 | 89.7 | 83.6 |\n|Tfidf | 0.01 | 78, 89.7 | 65.57, 83.8 |\n| Avg Word2Vec | 1 | 50.98, 87.8 | 50, 84.5 |\n| Tfidf Word2Vec | 10 | 50.72 | 49.79 |","1110c81e":"### Testing the model on test data with optimal C.","a2d014ee":"### Feature Importance:- Top ten predictors of each class","60417e86":"### Confusion Matrix","297b65c1":"#### Standardizing the data","d37ce986":"## Since dataset is very large so taking out 120K points to work","40ce0590":"### Feature importance:- Top ten predictors from each class","101c8675":"### Performing Standardization on train data","2aa7faf0":"### Confusion Matrix of model with class_weight as None","e1008e38":"#### Confusion Matrix","5e599c75":"## Function for time based splitting into train and test dataset","b28d1b61":"### ===================================================================================","8ba9ede7":"### =====================================================================================","457541ae":"## Bag of Words Implementation","9a24529f":"### Error Plots of model with class_weight as 'balanced'","cf01b93d":"#### Applying GridSearchCV to find optimal hyperparameter","08062b4c":"#### Testing the model on the test data ","d36352c0":"# Logistic Regression on Amazon Fine Food Review","684bc477":"#### Differences in the coefficients of the peturbed model and original model are very less so weight vector of the classifier can be considered for the feature importance.","82319d60":"### Feature Importance Function","e0e5341c":"### Testing the model with class_weight as None","d51176e7":"### ===================================================================================","c10bdb79":"### Error Plots:-","3be93ae4":"## Loading already prepared corpus","086c6c82":"#### Standardizing the data","d019e6d2":"## Loading necessary libraries","8fe8f415":"### Checking Multicollinearity using peturbation test","11c1c33b":"### Parameter tuning using GridSearchCV","64a6d88d":"### Confusion matrix of model with class_weight as None","251e631e":"### Average Word2Vec implementation:-","5a3503a3":"### =====================================================================================","d10aeffb":"### Parameter tuning using GridSearchCV","32bf0d19":"### Tfidf implementation","3742c4ff":"### Tfidf Word2Vec implementation","4a364d7e":"### Applying the model on test data with optimal value of C","fbd3ff60":"#### Confusion Matrix of model with class_weight as 'balanced'.","a17fa28f":"#### Applying model on test data with optimal hyperparameter","0f0ffc16":"### Standardizing the data","6c346cde":"### Conclusion:-\n#### Losgistic Regression has given overall good results with Bag of Words and Tfidf vectors.\n\n#### Logisitic Regression is not at all performing good when applying on Word2Vec vectors. When i am not using class_weight='balanced' it is classifying all points as postive and when i am using class_weight='balanced' it is giving very less accuracy.","d532336e":"### Confusion Matrix of model with class_weight='balanced'","a2717134":"#### Applying GridSearchCV to find optimal hyperparameter"}}