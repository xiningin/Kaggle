{"cell_type":{"978a19f5":"code","54e35922":"code","18f0d9d5":"code","f3a66ea8":"code","0540a188":"code","56a94c0a":"code","c5b587c8":"code","edb82d3a":"code","ac0cbb5a":"code","dafe9f5f":"code","37a0b64c":"code","ff23c8e0":"code","4a89729c":"code","b076ef24":"code","e89f1973":"code","bf5fb955":"code","6804e3b9":"code","1c9c769a":"code","c89da761":"code","da9e9219":"code","6898da5b":"code","cf0104aa":"code","3a0942bd":"code","cf5e4541":"code","502729cc":"code","db195db4":"code","78f08ac0":"markdown","e3967e61":"markdown","58ad06e6":"markdown"},"source":{"978a19f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n##visual imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n##Missing data\nfrom sklearn.impute import SimpleImputer\n\n##Categorical Encoding\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n##Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n##Splitting data\nfrom sklearn.model_selection import train_test_split\n\n#Splitting Data\nfrom sklearn.model_selection import train_test_split\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n#Confusion Matrix & accuracy\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n######################\n### Classification ###\n######################\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54e35922":"train_data = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')","18f0d9d5":"train_data.head()","f3a66ea8":"train_data.info()","0540a188":"train_data.describe()","56a94c0a":"train_data.describe(include = [\"O\"])","c5b587c8":"train_data[\"Species\"].value_counts()","edb82d3a":"train_data.drop([\"Id\"], axis = 1,inplace = True)","ac0cbb5a":"correl = train_data.corr()\nsns.heatmap(correl, annot = True)","dafe9f5f":"sns.pairplot(train_data, hue = \"Species\")","37a0b64c":"X_train, X_test, y_train, y_test = train_test_split(train_data.drop('Species',axis=1), \n                                                    train_data['Species'], test_size=0.30, \n                                                    random_state=101)","ff23c8e0":"classifier = GaussianNB()\nclassifier.fit(X_train, y_train)","4a89729c":"y_pred = classifier.predict(X_test)","b076ef24":"from sklearn.metrics import classification_report\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"_\"*20)\nprint(classification_report(y_test,y_pred))","e89f1973":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","bf5fb955":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_validate\n\nMLA = [LogisticRegression(max_iter=500),KNeighborsClassifier(), SVC(), GaussianNB(),DecisionTreeClassifier(), RandomForestClassifier()]\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Accuracy Mean', 'MLA Test Accuracy 3*STD']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = pd.DataFrame() \nMLA_predict[\"Actual Results\"] = y_test\n\ncv_split = ShuffleSplit(n_splits = 10, test_size = 0.3, train_size = 0.6, random_state = 0 )","6804e3b9":"def MLA_Runner(MLA_Algo, MLA_Compare):\n    #index through MLA and save performance to table\n    row_index = 0\n    for alg in MLA:\n\n        \n        MLA_name = alg.__class__.__name__\n        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n\n        cv_results = cross_val_score(estimator = alg, X = X_train, y = y_train, cv = cv_split)\n\n        MLA_compare.loc[row_index, 'MLA Accuracy Mean'] = cv_results.mean()*100\n        MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results.std()*3\n\n        alg.fit(X_train, y_train)\n        MLA_predict[MLA_name] = alg.predict(X_test)\n\n        row_index +=1\n\n    MLA_compare.sort_values(by = [\"MLA Accuracy Mean\"], ascending = False, inplace = True)\n    MLA_compare\n    \n    plt.title(\"MLA Accuracy Rank\")\n    sns.barplot(x = \"MLA Accuracy Mean\", y = \"MLA Name\", data = MLA_compare)\n","1c9c769a":"MLA_Runner(MLA, MLA_compare)","c89da761":"svc = SVC()\nbase_result = cross_val_score(estimator = svc, X = X_train, y = y_train, cv = cv_split)\nsvc.fit(X_train, y_train)\nprint(\"Basic SVC Accuracy :{:.2f} %\".format(base_result.mean()*100))\nprint(\"_\"*30)\n\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = svc,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = cv_split,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Tuned SVC Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best SVC Parameters:\", best_parameters)","da9e9219":"KNC = KNeighborsClassifier()\nbase_result = cross_val_score(estimator = KNC, X = X_train, y = y_train, cv = cv_split)\nsvc.fit(X_train, y_train)\nprint(\"Basic KNeighborsClassifier Accuracy:{:.2f} %\".format(base_result.mean()*100))\nprint(\"_\"*30)\n\nparameters = [{'n_neighbors': [3, 5, 7], 'weights': ['uniform','distance'], \"algorithm\" : [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"], \"leaf_size\" :[20,30,40], \"p\" :[1,2] }]\ngrid_search = GridSearchCV(estimator = KNC,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = cv_split,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best KNeighborsClassifier Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best KNeighborsClassifier Parameters:\", best_parameters)","6898da5b":"LR = LogisticRegression(max_iter=500)\nbase_result = cross_val_score(estimator = LR, X = X_train, y = y_train, cv = cv_split)\nsvc.fit(X_train, y_train)\nprint(\"Basic LogisticRegression Accuracy:{:.2f} %\".format(base_result.mean()*100))\nprint(\"_\"*30)\n\nparameters = [{\"penalty\" : [\"l1\", \"l2\", \"elasticnet\",\"none\"], \"C\" : [1,5,10,15], \"dual\": [True, False], 'fit_intercept': [True, False],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \"max_iter\" : [200,250,300]}]\ngrid_search = GridSearchCV(estimator = LR,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = cv_split,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best LogisticRegression Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best LogisticRegression Parameters:\", best_parameters)","cf0104aa":"RFC = RandomForestClassifier()\nbase_result = cross_val_score(estimator = RFC, X = X_train, y = y_train, cv = cv_split)\nprint(\"Basic RandomForestClassifier Accuracy:{:.2f} %\".format(base_result.mean()*100))\nprint(\"_\"*30)\n\nparameters = [{\"n_estimators\":[10,50,100,200], \"criterion\":[\"gini\",\"entropy\"], \"max_features\" :[\"auto\", \"sqrt\", \"log2\"],\n            'max_depth': [2, 4, 6, 8, 10, None],\n            \"oob_score\": [True,False]}]\ngrid_search = GridSearchCV(estimator = RFC,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = cv_split,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best RandomForestClassifier Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best RandomForestClassifier Parameters:\", best_parameters)","3a0942bd":"DTC = DecisionTreeClassifier()\nbase_result = cross_val_score(estimator = DTC, X = X_train, y = y_train, cv = cv_split)\nprint(\"Basic DecisionTreeClassifier Accuracy:{:.2f} %\".format(base_result.mean()*100))\nprint(\"_\"*30)\n\nparameters = [{\"criterion\":[\"gini\",\"entropy\"], \"max_depth\":[5,10,None],\n               \"max_features\" :[\"auto\", \"sqrt\", \"log2\"]}]\ngrid_search = GridSearchCV(estimator = RFC,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = cv_split,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best DecisionTreeClassifier: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best DecisionTreeClassifier Parameters:\", best_parameters)","cf5e4541":"Optimal_MLA = MLA = [LogisticRegression(C= 1, dual = False, fit_intercept = True, max_iter = 200, penalty= 'l2', solver= 'sag'),\n                     KNeighborsClassifier(algorithm = 'auto', leaf_size = 20, n_neighbors = 7, p = 1, weights ='uniform'), \n                     SVC(C = 1, kernel ='linear'), GaussianNB(),\n                     DecisionTreeClassifier(criterion = 'gini', max_depth = 5, max_features = 'auto'), \n                     RandomForestClassifier(criterion='gini', max_depth= 2, max_features='sqrt', n_estimators = 200, oob_score = True)]","502729cc":"MLA_Runner(Optimal_MLA, MLA_compare)","db195db4":"MLA_compare","78f08ac0":"# Initial Findings\n* No missing values\n* 150 features\n* Id is the count for the rows, so this can be dropped. \n* Species is the outcome that we are looking to predict and has 3 unique values with 50 rows in each variable, so this needs to be removed and changed to numerical","e3967e61":"# Findings\n* Strong correlation in some features, so all features will be included in training the model\n* After plotting the data it is clear that there are clusters of Species within the each feature, this reinforces the need to use all features in the training of the model","58ad06e6":"# Training our the first model\nAfter a quick bit of EDA we can now use our data to train our first ML model. I have chosen Gaussian Naive Bayes as the first to run as it does not use too many hyper parameters and will give a benchmark for what to expect with the other models. "}}