{"cell_type":{"6691da4a":"code","473bc069":"code","262b72d7":"code","2c8754a0":"code","542d3454":"code","44b1ce57":"code","367ead7c":"code","65380498":"code","2e08e700":"code","0394ec0c":"code","a4f69982":"code","7ea6bfa5":"code","6234208f":"code","fe810d4f":"code","99ec2412":"code","cfcd39e6":"markdown","30ca289d":"markdown"},"source":{"6691da4a":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n# !curl -X PURGE https:\/\/pypi.org\/simple\/kaggle-environments\n\n# ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments>=0.1.6'","473bc069":"from kaggle_environments import evaluate, make, utils\n\nimport random\nfrom collections import namedtuple\nfrom itertools import count\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nenv = make(\"connectx\", debug=True)\nenv.render()","262b72d7":"Transition = namedtuple('Transition',\n                        ('state', 'action', 'next_state', 'reward'))\n\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n\n    def push(self, *args):\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        self.memory[self.position] = Transition(*args)\n        self.position = (self.position + 1) % self.capacity\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)","2c8754a0":"class DQN(nn.Module):\n\n    def __init__(self, rows, columns, inarow, outputs):\n        super(DQN, self).__init__()\n        \n        self.rows = rows\n        self.columns = columns\n        self.inarow = inarow\n        \n        # Set kernel size to minimum match length\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=inarow, stride = 1)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=1, stride = 1)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=1, stride = 1)\n        self.bn3 = nn.BatchNorm2d(32)\n\n        # Number of Linear input connections depends on output of conv2d layers\n        # and therefore the input array size, so compute it.\n        def conv2d_size_out(size, kernel_size=inarow, stride = 1):\n            return (size - (kernel_size - 1) - 1) \/\/ stride  + 1\n        convh = conv2d_size_out(rows)\n        convw = conv2d_size_out(columns + inarow - 1)\n        linear_input_size = convw * convh * 32\n        self.head = nn.Linear(linear_input_size, outputs)\n\n    # Called with either one element to determine next action, or a batch\n    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n    def forward(self, x):\n        x = torch.reshape(x, (-1, 1, self.rows, self.columns))\n        x = torch.cat((x, torch.zeros(x.shape[0], 1, self.rows, self.inarow-1)), dim=3)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        return self.head(x.view(x.size(0), -1))","542d3454":"BATCH_SIZE = 128\nGAMMA = 0.999\nEPS_START = 0.9\nEPS_END = 0.05\nEPS_DECAY = 200\nTARGET_UPDATE = 10\n\ncolumns = env.configuration['columns']\nrows = env.configuration['rows']\n\n# Number of actions is equal to number of columns\nn_actions = columns\ninarow = env.configuration['inarow']\n\npolicy_net = DQN(rows, columns, inarow, n_actions).to(device)\nadversary_net = DQN(rows, columns, inarow, n_actions).to(device)\ntarget_net = DQN(rows, columns, inarow, n_actions).to(device)\nadversary_net.load_state_dict(policy_net.state_dict())\nadversary_net.eval()\ntarget_net.load_state_dict(policy_net.state_dict())\ntarget_net.eval()\n\noptimizer = optim.RMSprop(policy_net.parameters())\nmemory = ReplayMemory(10000)\n\n\nsteps_done = 0\n\n\nepisode_durations = []","44b1ce57":"def select_action(observation, configuration):\n    global steps_done\n    state = torch.tensor(observation.board, dtype=torch.float)\n    sample = random.random()\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        np.exp(-1. * steps_done \/ EPS_DECAY)\n    steps_done += 1\n    if sample > eps_threshold:\n        with torch.no_grad():\n            return policy_net(state).max(1)[1].view(1, 1)\n    else:\n        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.float)","367ead7c":"def adversary_agent(observation, configuration):\n    state = torch.tensor(observation.board, dtype=torch.float)\n    \n    with torch.no_grad():\n        action = adversary_net(state).max(1)[1].view(1, 1)\n        \n    if observation.board[action] != 0:\n        return random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n        \n    return int(action[0][0].item())","65380498":"def optimize_model():\n    if len(memory) < BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    batch = Transition(*zip(*transitions))\n\n    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n                                          batch.next_state)), device=device, dtype=torch.bool)\n    non_final_next_states = torch.cat([s for s in batch.next_state\n                                                if s is not None])\n    state_batch = torch.cat(batch.state)\n    action_batch = torch.cat(tuple(ten.type(torch.long) for ten in batch.action))\n    reward_batch = torch.tensor(batch.reward)\n\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n\n    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n    \n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n\n    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n\n    optimizer.zero_grad()\n    loss.backward()\n    for param in policy_net.parameters():\n        param.grad.data.clamp_(-1, 1)\n    optimizer.step()","2e08e700":"# Play as first position against negamax agent.\ntrainer = env.train([None, \"negamax\"])\n\nnum_episodes = 1000\nfor i_episode in range(num_episodes):\n    observation = trainer.reset()\n    for t in count():\n        state = torch.tensor(observation.board, dtype=torch.float)\n        action = select_action(observation, env.configuration)\n        chosen_column = int(action[0][0].item())\n        \n        last_state = state\n        observation, reward, done, info = trainer.step(chosen_column)\n        \n        if not done:\n            next_state = torch.tensor(observation.board, dtype=torch.float)\n        else:\n            next_state = None\n            \n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        elif reward == None:\n            reward = 0\n\n        # Store the transition in memory\n        memory.push(last_state, action, next_state, reward)\n        \n        # Perform one step of the optimization (on the target network)\n        optimize_model()\n        if done:\n            break\n    \n#     # If the policy lost, switch to adversary\n#     if reward == 0:\n#         policy_net.load_state_dict(adversary_net.state_dict())\n#         policy_net.eval()\n        \n#     # Otherwise, update adversary\n#     else:\n#         adversary_net.load_state_dict(policy_net.state_dict())\n#         adversary_net.eval()\n    \n#     trainer = env.train([None, adversary_agent])\n    \n    # Update the target network, copying all weights and biases in DQN\n    if i_episode % TARGET_UPDATE == 0:\n        target_net.load_state_dict(policy_net.state_dict())\n        \nprint('Done')","0394ec0c":"#uncap state_dict size\ntorch.set_printoptions(profile=\"full\")\n\nagent = f\"\"\"\nimport torch\nimport numpy as np\nimport random\nfrom torch import nn, tensor\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n    \ndef my_agent(observation, configuration):\n    class DQN(nn.Module):\n        def __init__(self, rows, columns, inarow, outputs):\n            super(DQN, self).__init__()\n\n            self.rows = rows\n            self.columns = columns\n            self.inarow = inarow\n\n            self.conv1 = nn.Conv2d(1, 16, kernel_size=inarow, stride = 1)\n            self.bn1 = nn.BatchNorm2d(16)\n            self.conv2 = nn.Conv2d(16, 32, kernel_size=1, stride = 1)\n            self.bn2 = nn.BatchNorm2d(32)\n            self.conv3 = nn.Conv2d(32, 32, kernel_size=1, stride = 1)\n            self.bn3 = nn.BatchNorm2d(32)\n\n            def conv2d_size_out(size, kernel_size=inarow, stride = 1):\n                return (size - (kernel_size - 1) - 1) \/\/ stride  + 1\n            convh = conv2d_size_out(rows)\n            convw = conv2d_size_out(columns + inarow - 1)\n            linear_input_size = convw * convh * 32\n            self.head = nn.Linear(linear_input_size, outputs)\n\n        def forward(self, x):\n            x = torch.reshape(x, (-1, 1, self.rows, self.columns))\n            x = torch.cat((x, torch.zeros(x.shape[0], 1, self.rows, self.inarow-1)), dim=3)\n            x = F.relu(self.bn1(self.conv1(x)))\n            x = F.relu(self.bn2(self.conv2(x)))\n            x = F.relu(self.bn3(self.conv3(x)))\n            return self.head(x.view(x.size(0), -1))\n            \n    columns = configuration['columns']\n    rows = configuration['rows']\n\n    n_actions = columns\n    inarow = configuration['inarow']\n    \n    policy_net = DQN(rows, columns, inarow, n_actions)\n    \n    policy_net.load_state_dict({str(policy_net.state_dict())})\n    policy_net.eval()\n        \n    state = torch.tensor(observation.board, dtype=torch.float)\n    \n    with torch.no_grad():\n        action = policy_net(state).max(1)[1].view(1, 1)\n        \n    if observation.board[action] != 0:\n        return random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n        \n    return int(action[0][0].item())\n\"\"\"","a4f69982":"with open('submission.py', 'w') as f:\n    f.write(agent)","7ea6bfa5":"from submission import my_agent","6234208f":"# \"None\" represents which agent you'll manually play as (first or second player).\nenv.play([None, my_agent], width=500, height=450)","fe810d4f":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ max(0.001, sum(r[0] + r[1] for r in rewards))\n\n# Run multiple episodes to estimate agent's performance.\nprint(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\nprint(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=10)))\nprint(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))","99ec2412":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nout = sys.stdout\nsubmission = utils.read_file(\"\/kaggle\/working\/submission.py\")\nagent = utils.get_last_callable(submission)\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","cfcd39e6":"# Credits\n\nAdapted from [PyTorch: Reinforcement Learning (DQN) Tutorial](https:\/\/pytorch.org\/tutorials\/intermediate\/reinforcement_q_learning.html#input-extraction)","30ca289d":"# Install kaggle-environments"}}