{"cell_type":{"80c9e6d7":"code","566dedc4":"code","eb527916":"code","791b5e44":"code","9461e29a":"code","faa41408":"code","5f9e4c58":"code","6d17f6b6":"code","94b0f5a0":"code","a9149d40":"code","661cf40e":"code","6b18179c":"code","bfc9ea4a":"code","3793f3ab":"code","eecbbb5f":"code","2a483b33":"code","84be5ba3":"code","5063efdb":"code","37868fc4":"markdown","79e2219f":"markdown","de8de0ef":"markdown","f6411a0b":"markdown","49a498ff":"markdown","6928cbfd":"markdown","539b6aba":"markdown","4441f172":"markdown","a75bd753":"markdown","8aee3477":"markdown","05e7a534":"markdown","14b26322":"markdown","0b66e94b":"markdown","f8bb8c29":"markdown","f1c89f10":"markdown"},"source":{"80c9e6d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","566dedc4":"!pip install kaggle-environments --upgrade","eb527916":"import collections\nimport gym\n#!pip install 'tensorflow==1.15.0'\nimport tensorflow as tf\ntf.__version__\nimport tqdm\n\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers\nfrom typing import Any, List, Sequence, Tuple\n\nfrom kaggle_environments import make, evaluate\n\nfrom gym import spaces\n\nimport random","791b5e44":"class MABanditGym:   \n    \n    \n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"mab\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.nrounds = 2000\n        self.banditCount = ks_env.configuration.banditCount\n        self.prev_reward = 0\n        \n        # Learn about spaces here: http:\/\/gym.openai.com\/docs\/#spaces\n        self.action_space = spaces.Discrete(self.banditCount)\n        low = -np.ones((self.nrounds,), dtype=np.float32)\n        high = -low*(self.banditCount-1)\n        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10,1)\n        self.grid = -np.ones((self.nrounds,2))\n        self.obs=np.array(self.grid).reshape(self.nrounds*2)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        #print(env.obs)\n        self.env.reset()\n        self.grid = -np.ones((self.nrounds,2))\n        self.obs=np.array(self.grid).reshape(self.nrounds*2)\n        self.prev_reward = 0\n        return self.obs\n    def change_reward(self, old_reward, done):\n        if old_reward == 1000: # The agent won the game\n            return 0\n        elif done: # The opponent won the game\n            return -10\n        else: # Reward 1\/2000\n            return old_reward\n    def step(self, action):\n        _={}\n        # Check if agent's move is valid\n        is_valid = (int(action) in range(0,self.banditCount) )\n        #valid_moves = [bnd for bnd in range(config.banditCount)]\n       \n        if is_valid: # Play the \"move\"\n            current_obs = self.env.step(int(action))\n            \n            for pos in range(0,2):\n                #print(current_obs)\n                self.grid[current_obs[0]['step']-1][pos]=current_obs[0]['lastActions'][pos]\n            self.obs=np.array(self.grid).reshape(self.nrounds*2)\n            old_reward= current_obs[0]['reward']\n            done = (current_obs[0]['step']==self.nrounds-1 and current_obs[0]['reward']<600)#current_obs[1]['observation']['reward']\n            reward = old_reward- self.prev_reward  #self.change_reward(old_reward, done)\n            self.prev_reward=old_reward\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        #print(self.obs, reward, done, _  )    \n        return self.obs, reward, done, _    \n\n\n    \n    def seed(self, seed=None):\n        self.np_random, seed = gym.utils.seeding.np_random(seed)\n        return [seed]\n\n# Create the environment for training tasks\nenv = MABanditGym(agent2=\"random\")\n# Set seed for experiment reproducibility\nseed = 42\nenv.seed(seed)\nrandom.seed(seed)\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\n# Small epsilon value for stabilizing division operations\neps = np.finfo(np.float32).eps.item()\n\n\n","9461e29a":"class ActorCritic(tf.keras.Model):\n  \"\"\"Combined actor-critic network.\"\"\"\n\n  def __init__(\n      self, \n      num_actions: int, \n      num_hidden_units: int):\n    \"\"\"Initialize.\"\"\"\n    super().__init__()\n    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n    self.actor = layers.Dense(num_actions)\n    self.critic = layers.Dense(1)\n\n  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n    x = self.common(inputs)\n    return self.actor(x), self.critic(x)","faa41408":"num_actions = env.action_space.n  # 100\n#print(num_actions)\nnum_hidden_units = 128\n\nmodel = ActorCritic(num_actions, num_hidden_units)","5f9e4c58":"# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n# This would allow it to be included in a callable TensorFlow graph.\n\ndef env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n\n  state, reward, done, _ = env.step(action)\n  return (state.astype(np.float32), \n          np.array(reward, np.int32), \n          np.array(done, np.int32))\n\n\ndef tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n  return tf.numpy_function(env_step, [action], \n                           [tf.float32, tf.int32, tf.int32])","6d17f6b6":"def run_episode(\n    initial_state: tf.Tensor,  \n    model: tf.keras.Model, \n    max_steps: int) -> List[tf.Tensor]:\n  \"\"\"Runs a single episode to collect training data.\"\"\"\n\n  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n\n  initial_state_shape = initial_state.shape\n  state = initial_state\n\n  for t in tf.range(max_steps):\n    # Convert state into a batched tensor (batch size = 1)\n    state = tf.expand_dims(state, 0)\n\n    # Run the model and to get action probabilities and critic value\n    action_logits_t, value = model(state)\n\n    # Sample next action from the action probability distribution\n    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n    action_probs_t = tf.nn.softmax(action_logits_t)\n\n    # Store critic values\n    values = values.write(t, tf.squeeze(value))\n\n    # Store log probability of the action chosen\n    action_probs = action_probs.write(t, action_probs_t[0, action])\n\n    # Apply action to the environment to get next state and reward\n    state, reward, done = tf_env_step(action)\n    print(reward)\n    state.set_shape(initial_state_shape)\n\n     # Store reward\n    rewards = rewards.write(t, reward)\n\n    if tf.cast(done, tf.bool):\n      break\n\n  action_probs = action_probs.stack()\n  values = values.stack()\n  rewards = rewards.stack()\n\n  return action_probs, values, rewards","94b0f5a0":"def get_expected_return(\n    rewards: tf.Tensor, \n    gamma: float, \n    standardize: bool = True) -> tf.Tensor:\n  \"\"\"Compute expected returns per timestep.\"\"\"\n\n  n = tf.shape(rewards)[0]\n  returns = tf.TensorArray(dtype=tf.float32, size=n)\n\n  # Start from the end of `rewards` and accumulate reward sums\n  # into the `returns` array\n  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n  discounted_sum = tf.constant(0.0)\n  discounted_sum_shape = discounted_sum.shape\n  for i in tf.range(n):\n    reward = rewards[i]\n    discounted_sum = reward + gamma * discounted_sum\n    discounted_sum.set_shape(discounted_sum_shape)\n    returns = returns.write(i, discounted_sum)\n  returns = returns.stack()[::-1]\n\n  if standardize:\n    returns = ((returns - tf.math.reduce_mean(returns)) \/ \n               (tf.math.reduce_std(returns) + eps))\n\n  return returns","a9149d40":"huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n\ndef compute_loss(\n    action_probs: tf.Tensor,  \n    values: tf.Tensor,  \n    returns: tf.Tensor) -> tf.Tensor:\n  \"\"\"Computes the combined actor-critic loss.\"\"\"\n\n  advantage = returns - values\n\n  action_log_probs = tf.math.log(action_probs)\n  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n\n  critic_loss = huber_loss(values, returns)\n\n  return actor_loss + critic_loss","661cf40e":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n\n@tf.function\ndef train_step(\n    initial_state: tf.Tensor, \n    model: tf.keras.Model, \n    optimizer: tf.keras.optimizers.Optimizer, \n    gamma: float, \n    max_steps_per_episode: int) -> tf.Tensor:\n  \"\"\"Runs a model training step.\"\"\"\n  #print(\"Hello\")\n  with tf.GradientTape() as tape:\n\n    # Run the model for one episode to collect training data\n    action_probs, values, rewards = run_episode(\n        initial_state, model, max_steps_per_episode) \n\n    # Calculate expected returns\n    returns = get_expected_return(rewards, gamma)\n\n    # Convert training data to appropriate TF tensor shapes\n    action_probs, values, returns = [\n        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n\n    # Calculating loss values to update our network\n    loss = compute_loss(action_probs, values, returns)\n\n  # Compute the gradients from the loss\n  grads = tape.gradient(loss, model.trainable_variables)\n\n  # Apply the gradients to the model's parameters\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n  episode_reward = tf.math.reduce_sum(rewards)\n\n  return episode_reward","6b18179c":"%%time\n\nmax_episodes = 1\nmax_steps_per_episode = 2000\n\n# Problem is considered solved if average reward is >= 600 over 100 \n# consecutive trials\nreward_threshold = 600\nrunning_reward = 0\n\n# Discount factor for future rewards\ngamma = 0.99\n\nwith tqdm.trange(max_episodes) as t:\n  for i in t:\n    new_env=env.reset()\n    #print(new_env)\n    #print(type(new_env))\n    #print(new_env.shape)\n    \n    initial_state = tf.constant(new_env, dtype=tf.float32)\n    episode_reward = int(train_step(\n        initial_state, model, optimizer, gamma, max_steps_per_episode))\n    #print('episode_reward: ', episode_reward)\n    running_reward = episode_reward*0.01 + running_reward*.99\n\n    t.set_description(f'Episode {i}')\n    t.set_postfix(\n        episode_reward=episode_reward, running_reward=running_reward)\n\n    # Show average episode reward every 5 episodes\n    if i % 5 == 0:\n      print(f'Episode {i}: average reward: {running_reward}')\n\n    if running_reward > reward_threshold:  \n        break\n\nprint(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')","bfc9ea4a":"\nfilename = 'bandit_model'\nmodel.save(filename) # creates a HDF5 file 'bandit_model.h5'\n#del model  # deletes the existing model\n","3793f3ab":"from tensorflow.keras.models import load_model\n# returns a compiled model\n# identical to the previous one\nmodel_bandit = load_model('bandit_model')","eecbbb5f":"%%writefile agent_random_.py\nimport random\ndef agent_random_(obs, config):\n    #print(obs)\n    #print(config)\n    valid_moves = [bnd for bnd in range(config['banditCount'])]\n    return random.choice(valid_moves)","2a483b33":"%%writefile submission.py\n\nimport random\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import load_model\n# returns a compiled model\n# identical to the previous one\nmodel_bandit = load_model('bandit_model')\n\n\nclass MABanditPlayer:\n    global model_bandit\n    \n    \n    def __init__(self, observation, configuration):\n        \n        self.nrounds = configuration['episodeSteps']\n        self.banditCount = configuration['banditCount']\n        self.prev_reward = 0\n        self.grid = -np.ones((self.nrounds,2))\n        self.obs=np.array(self.grid).reshape(self.nrounds*2)\n        self.prev_reward = 0\n        # StableBaselines throws error if these are not defined\n        \n    def reset(self):\n        #print(env.obs)\n        self.env.reset()\n        self.grid = -np.ones((self.nrounds,2))\n        self.obs=np.array(self.grid).reshape(self.nrounds*2)\n        self.prev_reward = 0\n        return self.obs\n    \n    def play(self, observation, configuration):\n        bandit=0\n        if observation['step']>0:\n            \n        \n            for pos in range(0,2):\n                    #print(current_obs)\n                    self.grid[observation['step']-1][pos]=observation['lastActions'][pos]\n            new_reward= observation['reward']\n            reward = new_reward- self.prev_reward\n            self.prev_reward=new_reward\n            self.obs=np.array(self.grid).reshape(self.nrounds*2)\n            \n            # Convert state into a batched tensor (batch size = 1)\n            state = tf.expand_dims(self.obs,0)\n            # Run the model and to get action probabilities and critic value\n            action_logits_t, value = model_bandit(state)\n            # Sample next action from the action probability distribution\n            action = tf.random.categorical(action_logits_t, 1)[0, 0]\n            #bandit = model_bandit.predict_classes(state) #state = tf.expand_dims(state, 0)\n            with tf.compat.v1.Session() as sess:\n                bandit = action.numpy()\n            \n        else:\n            valid_moves = [bnd for bnd in range(configuration['banditCount'])]\n            bandit = np.dtype('int32').type(random.choice(valid_moves))\n\n        return(bandit)    \n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n# Create the environment\n\n\n# Set seed for experiment reproducibility\nseed = 2021\n#env.seed(seed)\nrandom.seed(seed)\n#tf.random.set_seed(seed)\n#np.random.seed(seed)\n\n\n\nobservation0 = [{'remainingOverageTime': 60,\n 'step': 0,\n 'agentIndex': 0,\n 'reward': 0,\n 'lastActions': []}]\nconfiguration0 = {'episodeSteps': 2000, 'actTimeout': 0.25, 'runTimeout': 1200, 'banditCount': 100, 'decayRate': 0.97, 'sampleResolution': 100}\n\nmab_player = MABanditPlayer(observation0, configuration0)\n\ndef keras_agent(observation, configuration):\n    #print(observation)\n    #print(configuration)\n    \n    global mab_player\n    bandit=0\n    bandit=(mab_player.play(observation, configuration)).item()  \n    return int(bandit)","84be5ba3":"env_test = make(\"mab\", debug=True)\n\nsteps = env_test.run([\"submission.py\", \"agent_random_.py\"])","5063efdb":"steps[-1]","37868fc4":"### 2. Computing expected returns","79e2219f":"## Training\nTo train the agent, we will follow these steps:\n \n1. Run the agent on the environment to collect training data per episode.\n2. Compute expected return at each time step.\n3. Compute the loss for the combined actor-critic model.\n4. Compute gradients and update network parameters.\n5. Repeat 1-4 until either success criterion or max episodes has been reached.","de8de0ef":"## Model\nThe Actor and Critic will be modeled using one neural network that generates the action probabilities and critic value respectively. We use model subclassing to define the model.","f6411a0b":"### 4. Defining the training step to update parameters","49a498ff":"### Random agent","6928cbfd":"# Introduction\nHopefully this code will help you create an agent based on a\nDeep reinforcement learning approach.\nI have created two classes to better interact with Gym and also in order to have a class that allows to exploit a model based on neural networks.\nYou will see that the implemented network is not really deep (among other details). But I think this code can be a good starting point to tackle the problem using neural networks.\nYou can choose to change the network, add layers, change the rewards and the criteria that determine the outcome of an episode, as well as the agent used to train the network.\nThanks to everyone who has shared their kernels. I have seen almost all of them and they have helped me to have a better idea of the problem and to learn a lot.\nFinally I would like to reference that I have used code from the following links: https:\/\/www.kaggle.com\/alexisbcook\/deep-reinforcement-learning and https:\/\/www.tensorflow.org\/tutorials\/reinforcement_learning\/actor_critic.\nYou can use those links to have a better explanation of what the code does. Happy Kaggling!","539b6aba":"## MABanditGym class (Open AI Gym)\nHere we create a class for better interaction with the Open AI Gym enviroment.","4441f172":"## [Huber loss](https:\/\/en.wikipedia.org\/wiki\/Huber_loss)","a75bd753":"## Save model","8aee3477":"### 5. Run the training loop","05e7a534":"### Testing the agent","14b26322":"### Load model","0b66e94b":"### Keras Agent (submission)\nAgent trained  using the implemented Actor-Critic method using TensorFlow. This is the submission file.","f8bb8c29":"### 1. Collecting training data","f1c89f10":"## **Setup**\nImport necessary packages and configure global settings."}}