{"cell_type":{"153b7f84":"code","bf4e97c0":"code","3fa51fd6":"code","88784458":"code","2c7cc2a0":"code","b98d1eb4":"code","b2dfa264":"code","35c9ff83":"code","9326a6fb":"code","74379c02":"code","344ea172":"code","7d05dcf8":"code","a307342e":"code","cfc7c2cd":"code","55a8eb24":"code","d8964501":"code","21839857":"code","ad172416":"code","91ea802d":"code","a011cbd2":"code","088bf610":"code","e45b1080":"code","1405acc5":"code","4b52a8f6":"code","4ba59f74":"code","e0f1e347":"code","b57c34fb":"code","43ce0336":"code","f28ebf12":"code","9331910a":"code","21debf53":"code","e8f2e45e":"code","b905865c":"markdown","3c2536ba":"markdown","54178074":"markdown","68d9a456":"markdown"},"source":{"153b7f84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf4e97c0":"!pip install xgboost\n!pip install lightgbm","3fa51fd6":"train_data = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/train.csv\" ) # reading the train data to a data frame \ntest_data = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv' ) # reading the test data into a data frame \nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv') # reading the test data into a data frame\nprint(\" data imported keep going....\")","88784458":"train_data.shape #7111,12\ntrain_data.info()","2c7cc2a0":"train_data.head()","b98d1eb4":"# daypart function\ndef daypart(hour):\n    if hour in [2,3,4,5]:\n        return \"dawn\"\n    elif hour in [6,7,8,9]:\n        return \"morning\"\n    elif hour in [10,11,12,13]:\n        return \"noon\"\n    elif hour in [14,15,16,17]:\n        return \"afternoon\"\n    elif hour in [18,19,20,21]:\n        return \"evening\"\n    else: return \"midnight\"","b2dfa264":"\n\n# from sklearn.preprocessing import StandardScaler\n# def datetime_temprature_fetures(data_frame,col_name):\n#     scaler = StandardScaler()\n#     cols_to_standrdise = ['sensor_1','sensor_2','sensor_3','sensor_4','sensor_5']\n#     scaler.fit(data_frame[cols_to_standrdise])\n#     if data_frame.dtypes[col_name] != 'datetime64[ns]':\n#         # convert date_time column to datetime type\n#         data_frame[col_name] = pd.to_datetime(data_frame[col_name],errors='coerce')\n\n#     else:\n#         print('It\\'salready datetime')\n\n#     data_frame['month'] = data_frame[col_name].dt.month # extracting month\n#     data_frame['year'] = data_frame[col_name].dt.year # extracting year\n#     data_frame['day_of_month'] = data_frame[col_name].dt.day # extracting day\n#     # first: extract the day name literal\n#     data_frame['day_name'] = data_frame[col_name].dt.day_name()\n#     data_frame['hour'] = data_frame[col_name].dt.hour\n#     data_frame['hour_categorical'] = data_frame.apply(lambda x : daypart(x['hour']),axis=1)\n#     data_frame['is_weekend'] = data_frame.apply(lambda x : 1 if x['day_name'] in ['Saturday','Sunday'] else 0,axis=1)\n#     standard_cols = pd.DataFrame(scaler.transform(data_frame[cols_to_standrdise]))\n#     data_frame = pd.concat([data_frame,standard_cols],axis=1)\n    \n   \n#     # check if columns exist or not if yes drop them\n#     if all(item  in list(train_data.columns) for item in cols_to_standrdise):\n#         print('not dropping')\n#         data_frame.drop(['sensor_1','sensor_2','sensor_3','sensor_4','sensor_5'],inplace=True,axis=1)\n    \n#     data_frame.rename(columns= {0:'sensor_1',\n#                       1:'sensor_2', 2:'sensor_3',3:'sensor_4',4:'sensor_5'},inplace=True)\n    \n#     return data_frame\n","35c9ff83":"# train_data = datetime_temprature_fetures(train_data,'date_time')\n# train_data.head()","9326a6fb":"# test_data = datetime_temprature_fetures(test_data,'date_time')\n# test_data.head()","74379c02":"for c in train_data.columns:\n    col_type = train_data[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        train_data[c] = train_data[c].astype('category')\n\nfor c in test_data.columns:\n    col_type = test_data[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        test_data[c] = test_data[c].astype('category')\n","344ea172":"# train_data.drop(['year','date_time'],axis=1,inplace=True)\n\n# test_data.drop(['year','date_time'],axis=1,inplace=True)","7d05dcf8":"print(train_data.shape)\nprint(test_data.shape)\n\ntargets = ['target_carbon_monoxide','target_benzene','target_nitrogen_oxides']\n\ny = train_data.loc[:,targets]\n\ntrain_data.drop(labels = targets,inplace=True,axis=1)","a307342e":"\ny1 = np.array(y.loc[:, list(y.columns)[0] ] )# first column for prediction\n\ny2 = np.array(y.loc[:, list(y.columns)[1] ]) # second column for prediction\n\ny3 = np.array(y.loc[:, list(y.columns)[2] ]) # third column for prediction","cfc7c2cd":"cat_cols = train_data.select_dtypes('category').columns.tolist()\ncat_cols","55a8eb24":"from sklearn.model_selection import StratifiedKFold ,KFold","d8964501":"\nskf = KFold(n_splits=10, shuffle=True,random_state=42)\n\ndef trainer(model, X,y,TEST_SET):\n    i=1\n    pred_val_final = np.zeros((1,7111))\n    pred_test_final = np.zeros((1,2247))\n    new_train_label_final = np.zeros((1,7111))\n    \n    fit_params={ 'feature_name':'auto',\n            'categorical_feature': 'auto' # that's actually the default\n           }\n    \n    for train_index, test_index in skf.split(X, y):\n        \n        print(f\"For Split Number {i}\")\n        X_train  = X.loc[train_index,:]\n        X_test = X.loc[test_index,:]\n        y_train = y[train_index]\n        y_test = y[test_index]\n\n#         print(f\"Training shape {X_train.shape} and {y_train.shape}\") # Training shape (6399, 14) and (6399,)\n#         print(f\"Testing shape {X_test.shape} and {y_test.shape}\")  # Testing shape (712, 14) and (712,)\n\n  \n        model.fit(X_train,y_train) # if lightgbm model is LGBM it can handle catgorical features on it's own given\n                                    # we are providing Pandas dataframe to it not numpy array\n\n        #storing predictions for train and test\n        pred_val= model.predict(X_test)\n\n        i+=1\n        pred_val = pred_val.reshape(-1,1).T\n\n\n        # new y for next training data\n        new_train_label = y_test.reshape(-1,1).T\n\n\n        new_train_label_final = np.hstack((new_train_label_final,new_train_label)) #new y\n        pred_val_final = np.hstack((pred_val_final,pred_val)) # new x\n\n\n#         print(pred_val_final.shape,pred_test_final.shape,i,new_train_label_final.shape)\n\n   \n    # new test data which we get after prediction after 10 cross folds we will use \n    # last model to predict our TEST DATA\n    model.fit(X,y)\n    pred_test=model.predict(TEST_SET)\n    pred_test = pred_test.reshape(-1,1).T\n    print(pred_test.shape,\"after predition\")\n    pred_test_final = np.hstack((pred_test_final,pred_test)) \n    print(pred_test_final.shape,\"after stacking\")\n    return pred_val_final,pred_test_final,new_train_label_final\n","21839857":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRFRegressor\nfrom lightgbm import LGBMRegressor,LGBMClassifier\nmodel=LGBMRegressor(num_leaves= 15, max_depth=-1, \n                         random_state=314, \n                         silent=True, \n                         metric='None', \n                         n_jobs=4, \n                         n_estimators=1000,\n                         colsample_bytree=0.9,\n                         subsample=0.9,\n                         learning_rate=0.1)\n\n\nnew_train_x_model_1, new_test_x_model_1, new_train_y_model_1 = trainer(model , train_data,y1,test_data)\nnew_train_x_model_2, new_test_x_model_2, new_train_y_model_2 = trainer(model , train_data,y2,test_data)\nnew_train_x_model_3, new_test_x_model_3, new_train_y_model_3 = trainer(model , train_data,y3,test_data)","ad172416":"\nprint(\"*\"*50,\"\\n\")\nprint(new_train_x_model_1.shape,\"---- validation  x shape for model 1\")\nprint(new_train_y_model_1.shape,\"---- validation  y shape for model 1\")\nprint(new_test_x_model_1.shape,\"---- test shape for model 1\")\n\n\nprint(\"*\"*50,\"\\n\")\nprint(new_train_x_model_2.shape,\"---- validation  x shape for model 2\")\nprint(new_train_y_model_2.shape,\"---- validation  y shape for model 2\")\nprint(new_test_x_model_2.shape,\"---- test shape for model 2\")\n\n\nprint(\"*\"*50,\"\\n\")\nprint(new_train_x_model_3.shape,\"---- validation  x shape for model 3\")\nprint(new_train_y_model_3.shape,\"---- validation  y shape for model 3\")\nprint(new_test_x_model_3.shape,\"---- test shape for model 3\")","91ea802d":"print(\"For model 1\")\nnew_train_x_model_1 = new_train_x_model_1[:,7111:] # removing zeroes column\nnew_train_y_model_1 = new_train_y_model_1[:,7111:] # new y column\n\nnew_test_x_model_1 = new_test_x_model_1[:,2247:]  # removing zeroes column\n\nprint(\"For model 2\")\nnew_train_x_model_2 = new_train_x_model_2[:,7111:] # removing zeroes column\nnew_train_y_model_2 = new_train_y_model_2[:,7111:] # new y column\n\nnew_test_x_model_2= new_test_x_model_2[:,2247:]  # removing zeroes column\n\nprint(\"For model 3\")\nnew_train_x_model_3 = new_train_x_model_3[:,7111:] # removing zeroes column\nnew_train_y_model_3 = new_train_y_model_3[:,7111:] # new y column\n\nnew_test_x_model_3= new_test_x_model_3[:,2247:]  # removing zeroes column","a011cbd2":"print(\"For model 1\")\nnew_train_x_model_1 = new_train_x_model_1.T\nnew_train_y_model_1 = new_train_y_model_1.T\nnew_test_x_model_1 = new_test_x_model_1.T\nprint(new_train_x_model_1.shape ,\" First ROW NEW TRAIN X\")\nprint(new_train_y_model_1.shape,\"FIRST NEW TRAIN Y \")\nprint(new_test_x_model_1.shape)\n\nprint(\"For model 2\")\nnew_train_x_model_2 = new_train_x_model_2.T\nnew_train_y_model_2 = new_train_y_model_2.T\nnew_test_x_model_2 = new_test_x_model_2.T\nprint(new_train_x_model_2.shape ,\" First ROW NEW TRAIN X\")\nprint(new_train_y_model_2.shape,\"FIRST NEW TRAIN Y \")\nprint(new_test_x_model_2.shape)\n\nprint(\"For model 1\")\nnew_train_x_model_3 = new_train_x_model_3.T\nnew_train_y_model_3 = new_train_y_model_3.T\nnew_test_x_model_3 = new_test_x_model_3.T\nprint(new_train_x_model_3.shape ,\" First ROW NEW TRAIN X\")\nprint(new_train_y_model_3.shape,\"FIRST NEW TRAIN Y \")\nprint(new_test_x_model_3.shape)","088bf610":"train_data_xgb = train_data.copy()\ntest_data_xgb = test_data.copy()\n\ntrain_data_xgb.drop(['date_time'],inplace=True,axis=1)\ntest_data_xgb.drop(['date_time'],inplace=True,axis=1)\n# # OHE for XGBOOST for training data\n# train_data_xgb =  pd.get_dummies(train_data_xgb, columns = ['hour_categorical','day_name'])\n\n# test_data_xgb =  pd.get_dummies(test_data_xgb, columns = ['hour_categorical','day_name'])\n","e45b1080":"#Model 2- XGBREgressor\nXgreg = XGBRFRegressor(random_state= 101)\nnew_train_x_xg_1 ,new_test_x_xg_1, new_train_y_xg_1 = trainer(Xgreg,train_data_xgb ,y1 ,test_data_xgb)\n\nnew_train_x_xg_2 ,new_test_x_xg_2, new_train_y_xg_2 = trainer(Xgreg,train_data_xgb ,y2 ,test_data_xgb)\n\nnew_train_x_xg_3 ,new_test_x_xg_3, new_train_y_xg_3 = trainer(Xgreg,train_data_xgb ,y3 ,test_data_xgb)","1405acc5":"\nprint(\"*\"*50,\"\\n\")\nprint(new_train_x_xg_1.shape,\"---- validation  x shape for model 1\")\nprint(new_train_y_xg_1.shape,\"---- validation  y shape for model 1\")\nprint(new_test_x_xg_1.shape,\"---- test shape for model 1\")\n\n\nprint(\"*\"*50,\"\\n\")\nprint(new_train_x_xg_2.shape,\"---- validation  x shape for model 2\")\nprint(new_train_y_xg_2.shape,\"---- validation  y shape for model 2\")\nprint(new_test_x_xg_2.shape,\"---- test shape for model 2\")\n\n\nprint(\"*\"*50,\"\\n\")\nprint(new_train_x_xg_3.shape,\"---- validation  x shape for model 3\")\nprint(new_train_y_xg_3.shape,\"---- validation  y shape for model 3\")\nprint(new_test_x_xg_3.shape,\"---- test shape for model 3\")","4b52a8f6":"\n\n\nprint(\"For model 1\")\nnew_train_x_xg_1 = new_train_x_xg_1[:,7111:] # removing zeroes column\nnew_train_y_xg_1 = new_train_y_xg_1[:,7111:] # new y column\n\nnew_test_x_xg_1 = new_test_x_xg_1[:,2247:]  # removing zeroes column\n\nprint(\"For model 2\")\nnew_train_x_xg_2 = new_train_x_xg_2[:,7111:] # removing zeroes column\nnew_train_y_xg_2 = new_train_y_xg_2[:,7111:] # new y column\n\nnew_test_x_xg_2= new_test_x_xg_2[:,2247:]  # removing zeroes column\n\nprint(\"For model 3\")\nnew_train_x_xg_3 = new_train_x_xg_3[:,7111:] # removing zeroes column\nnew_train_y_xg_3 = new_train_y_xg_3[:,7111:] # new y column\n\nnew_test_x_xg_3= new_test_x_xg_3[:,2247:]  # removing zeroes column","4ba59f74":"print(\"For model 1\")\nnew_train_x_xg_1 = new_train_x_xg_1.T\nnew_train_y_xg_1 = new_train_y_xg_1.T\nnew_test_x_xg_1 = new_test_x_xg_1.T\nprint(new_train_x_xg_1.shape ,\" First ROW NEW TRAIN X\")\nprint(new_train_y_xg_1.shape,\"FIRST NEW TRAIN Y \")\nprint(new_test_x_xg_1.shape)\n\nprint(\"For model 2\")\nnew_train_x_xg_2 = new_train_x_xg_2.T\nnew_train_y_xg_2 = new_train_y_xg_2.T\nnew_test_x_xg_2 = new_test_x_xg_2.T\nprint(new_train_x_xg_2.shape ,\" First ROW NEW TRAIN X\")\nprint(new_train_y_xg_2.shape,\"FIRST NEW TRAIN Y \")\nprint(new_test_x_xg_2.shape)\n\nprint(\"For model 1\")\nnew_train_x_xg_3 = new_train_x_xg_3.T\nnew_train_y_xg_3 = new_train_y_xg_3.T\nnew_test_x_xg_3 = new_test_x_xg_3.T\nprint(new_train_x_xg_3.shape ,\" First ROW NEW TRAIN X\")\nprint(new_train_y_xg_3.shape,\"FIRST NEW TRAIN Y \")\nprint(new_test_x_xg_3.shape)","e0f1e347":"train_new_1 = np.hstack((new_train_x_model_1,new_train_x_xg_1))\ntest_new_1 = np.hstack((new_test_x_model_1,new_test_x_xg_1))\n\ntrain_new_y_1 =  np.hstack((new_train_y_model_1,new_train_y_xg_1))\n\ntrain_new_2 = np.hstack((new_train_x_model_2,new_train_x_xg_2))\ntest_new_2 = np.hstack((new_test_x_model_2,new_test_x_xg_2))\n\ntrain_new_y_2 =  np.hstack((new_train_y_model_2,new_train_y_xg_2))\n\ntrain_new_3 = np.hstack((new_train_x_model_3,new_train_x_xg_3))\ntest_new_3 = np.hstack((new_test_x_model_3,new_test_x_xg_3))\n\ntrain_new_y_3 =  np.hstack((new_train_y_model_3,new_train_y_xg_3))","b57c34fb":"print(\"model1\")\nprint(train_new_1.shape)\nprint(test_new_1.shape)\nprint(train_new_y_1.shape)\n\nprint(\"model 2\")\nprint(train_new_2.shape)\nprint(test_new_2.shape)\nprint(train_new_y_2.shape)\nprint(\"model 3\")\nprint(train_new_3.shape)\nprint(test_new_3.shape)\nprint(train_new_y_3.shape)","43ce0336":"\nfinal_model_1 = RandomForestRegressor(random_state= 101)\nfinal_model_1.fit(train_new_1,train_new_y_1[:,1])\n\n\nfinal_model_2 = RandomForestRegressor(random_state= 101)\nfinal_model_2.fit(train_new_2,train_new_y_2[:,1])\n\n\nfinal_model_3 = RandomForestRegressor(random_state= 101)\nfinal_model_3.fit(train_new_3,train_new_y_3[:,1])","f28ebf12":"y_pred_1 = final_model_1.predict(test_new_1)\n\ny_pred_2 = final_model_2.predict(test_new_2)\n\ny_pred_3 = final_model_3.predict(test_new_3)","9331910a":"test_sub = np.vstack([y_pred_1,y_pred_2,y_pred_3]).T","21debf53":"sample_submission[sample_submission.columns[1:]] = test_sub\nsample_submission.head()","e8f2e45e":"sample_submission.to_csv('submission.csv',index=False)","b905865c":"### NEW TRAINING DATA AND NEW TESTING DATA","3c2536ba":" ### XGBOOST","54178074":"### Dataset ready","68d9a456":"1. Now after analysing Training and Testing Data I found Year is different in both so droping  **Year** column\n2. Creating **Lag features** i.e value of sensor 1 hour ago , 1 day ago found \n\n   [https:\/\/www.kaggle.com\/c\/tabular-playground-series-jul-2021\/discussion\/250074#1374044](here)\n        "}}