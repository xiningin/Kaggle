{"cell_type":{"c14ae4e3":"code","505c3151":"code","627810ce":"code","f5cae976":"code","b01f1ff3":"code","4b3b07b4":"code","f5912d0d":"code","e4672e16":"code","ffcd2182":"code","1c8b16cf":"code","4ee54da2":"code","44a8bc93":"code","b35bd808":"code","7126218d":"code","63776782":"code","141082cb":"code","eaa90692":"markdown","8c9f2394":"markdown","bfd68956":"markdown","2082d620":"markdown","531c1c94":"markdown","cfac16b4":"markdown","6cb4e732":"markdown","38975e4a":"markdown","1674a6b8":"markdown"},"source":{"c14ae4e3":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nimport keras_tuner as kt\nimport gc","505c3151":"class Config:\n    validation_split = 0.2\n    dataset_name = \"tabular-playground-series-nov-2021\"\n    train_path = \"\/kaggle\/input\/%s\/train.csv\"%(dataset_name)\n    test_path = \"\/kaggle\/input\/%s\/test.csv\"%(dataset_name)\n    sample_submission_path = \"\/kaggle\/input\/%s\/sample_submission.csv\"%(dataset_name)\n    id_field = \"id\"\n    label_field = \"target\"\n    hyperparameter_tuning_trial = 50\n    epochs = 50\n    train_with_fulldataset = True\n    sample_rate = 0.05\n    model_path = \"model.h5\"\n    submission_path = \"submission.csv\"\n    batch_size = 1024\nconfig = Config()","627810ce":"train_features = pd.read_csv(config.train_path)\ntrain_features.head()","f5cae976":"train_features.pop(config.id_field)\ntrain_targets = train_features.pop(config.label_field)","b01f1ff3":"train_max = train_features.max()\ntrain_features = train_features \/ train_max","4b3b07b4":"X_train, X_val, y_train, y_val = train_test_split(train_features, train_targets, test_size=config.validation_split, random_state=42)","f5912d0d":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","e4672e16":"del train_features\ndel train_targets\ngc.collect()","ffcd2182":"train_indices = np.random.choice(X_train.shape[0], int(X_train.shape[0] * config.sample_rate))\nX_train_subset = X_train.iloc[train_indices]\ny_train_subset = y_train.iloc[train_indices]\nval_indices = np.random.choice(X_val.shape[0], int(X_val.shape[0] * config.sample_rate))\nX_val_subset = X_val.iloc[val_indices]\ny_val_subset = y_val.iloc[val_indices]","1c8b16cf":"def residual_block(x, filters, kernel_size):\n    residual = x\n    x = layers.Conv2D(filters, kernel_size, activation=\"relu\", padding=\"same\")(x)\n    x = layers.Conv2D(filters, kernel_size, activation=\"relu\", padding=\"same\")(x)\n    x = layers.AveragePooling2D(2, padding=\"same\")(x)\n    residual = layers.Conv2D(filters, 1, strides=2)(residual)\n    x = layers.add([x, residual])\n    return x","4ee54da2":"def build_model(hp):\n    width = hp.Choice('dnn_width', [16, 32, 64])\n    depth = hp.Choice('dnn_depth', [3, 6, 9])\n    dropout = hp.Choice('dropout', [0.1, 0.2, 0.3])\n    inputs = tf.keras.layers.Input((100))\n    cnn_x = tf.keras.layers.Reshape((10, 10, 1))(inputs)\n    for i in range(3):\n        filters =  16 * (2 ** (i + 1))\n        kernel_size = 5 if i == 0 else 3\n        cnn_x = residual_block(cnn_x, filters, kernel_size)\n    cnn_x = tf.keras.layers.GlobalAveragePooling2D()(cnn_x)\n    for i in range(depth):\n        if i == 0:\n            dnn_x = inputs\n        dnn_x = keras.layers.Dense(\n            width, \n            activation=\"swish\"\n        )(dnn_x)\n        if (i + 1) % 3 == 0:\n            dnn_x = keras.layers.BatchNormalization()(dnn_x)\n            dnn_x = keras.layers.Dropout(dropout)(dnn_x)\n            dnn_x = keras.layers.Concatenate()([dnn_x, inputs])\n    x = keras.layers.Concatenate()([cnn_x, dnn_x])\n    x = keras.layers.Dropout(dropout)(x)\n    output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs=inputs, outputs=output)\n    adam = keras.optimizers.Adam(learning_rate=hp.Float(\"learing_rate\", 1e-5, 5e-3))\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"accuracy\", keras.metrics.AUC()])\n    return model","44a8bc93":"tuner = kt.BayesianOptimization(\n    build_model,\n    objective=kt.Objective(\"val_auc\", direction=\"max\"),\n    max_trials=config.hyperparameter_tuning_trial,\n    directory=\"tps_cnn_dnn\"\n)\ntuner.search(x=X_train_subset, y=y_train_subset, epochs=5, batch_size=config.batch_size, validation_data=(X_val_subset, y_val_subset))\nbest_model = tuner.get_best_models()[0]\nkeras.utils.plot_model(best_model, show_shapes=True)","b35bd808":"# Some of best parameters\n# {'dnn_width': 64, 'dnn_depth': 9, 'dropout': 0.2, 'learing_rate': 0.005}\n# {'dnn_width': 64, 'dnn_depth': 6, 'dropout': 0.3, 'learing_rate': 0.005}\nbest_hp = tuner.get_best_hyperparameters()[0]\nbest_hp.get_config()[\"values\"]","7126218d":"if not config.train_with_fulldataset:\n    model = best_model\nelse:\n    keras.backend.clear_session()\n    model = tuner.hypermodel.build(best_hp)\n    early_stopping = keras.callbacks.EarlyStopping(patience=10)\n    model_checkpoint = keras.callbacks.ModelCheckpoint(config.model_path, save_best_only=True)\n    reduce_lr =  keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=1e-7)\n    history = model.fit(x=X_train, y=y_train, epochs=config.epochs, batch_size=config.batch_size, validation_data=(X_val, y_val), callbacks=[early_stopping, model_checkpoint, reduce_lr])\n    model.load_weights(config.model_path)\n    pd.DataFrame(history.history).plot()","63776782":"del X_train\ndel y_train\ndel X_val\ndel y_val\ngc.collect()","141082cb":"test = pd.read_csv(config.test_path)\n_ = test.pop(config.id_field)\ntest = test \/ train_max\n# Submit probabilities has higher score than labels\ny_pred = model.predict(test)\n#y_pred = np.array(model.predict(test).reshape(-1) > 0.5, dtype=int)\nprint(y_pred.shape)\nsample_submission = pd.read_csv(config.sample_submission_path)\nsample_submission[config.label_field] = y_pred\nsample_submission.to_csv(config.submission_path, index=False)","eaa90692":"Here are best parameters:","8c9f2394":"## Submission","bfd68956":"### Choose a small sample for hyperparameter tuning","2082d620":"## Import and preprocess datasets","531c1c94":"## Setup","cfac16b4":"## OverView\nIn this Notebook, I will create a TPS prediction Model using Both CNN and DNN architecutre with skip connections. I will do hyperparameter tuning using Keras Tuner. Since this dataset is so big, I only choose a small sample for hyperparameter tuning. After finding the best Model, I keep training this Model with full dataset.","6cb4e732":"## Configuration","38975e4a":"## Model Development","1674a6b8":"## TPS-11-21: CNN+DNN"}}