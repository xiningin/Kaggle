{"cell_type":{"085366e1":"code","6ea40135":"code","fbbcacc9":"code","26c9f071":"code","d8204f9f":"code","23b68376":"code","7af314c7":"code","2c8fe7cb":"code","d15a9e2c":"code","96e3931f":"code","44fad17b":"code","3cee6fdf":"code","1c8cbbac":"code","f8e8a95c":"code","525c886d":"code","a01b280b":"code","76459bd4":"code","5edbf512":"code","07098242":"code","bfdf8396":"code","d4c8d72a":"code","08179fcf":"code","e1acd330":"code","cd41ca15":"code","bdd86b33":"code","430958a8":"code","a7067d93":"code","065cd282":"code","bf19256c":"code","46235aca":"code","1e134655":"code","0915373d":"code","0444d55e":"code","dfb2e4f4":"code","cd1249a5":"code","ca786095":"markdown","a7e95586":"markdown","0303e872":"markdown","6eda3204":"markdown","113a39f2":"markdown","337f257c":"markdown","5f367e9f":"markdown","c213a8c3":"markdown","285d8fd4":"markdown","dbcdaeed":"markdown","bbf93827":"markdown","a5bbc038":"markdown"},"source":{"085366e1":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nprint(tf.__version__)\n\n\"\"\"\nhyperparameters are:\n\nNumber of nodes \nepochs\nlearning rate\n\n\"\"\"","6ea40135":"window_size = 30\nbatch_size = 32\nshuffle_buffer_size = 1000","fbbcacc9":"!ls \/kaggle\/input\/sunspots\/Sunspots.csv","26c9f071":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.title('Forecasting trend')\n    plt.xlabel('Date')\n    plt.ylabel('Monthly Mean Total Sunspot Number')\n    plt.grid(True)","d8204f9f":"file_path = '\/kaggle\/input\/sunspots\/Sunspots.csv'\ndf = pd.read_csv(file_path,usecols=['Date','Monthly Mean Total Sunspot Number'])\ndf.dropna(inplace=True)","23b68376":"time_data = df['Date'].to_numpy()\nseries = df['Monthly Mean Total Sunspot Number'].to_numpy()","7af314c7":"len(time_data)","2c8fe7cb":"test_df = df[(df['Date'] > '2018-08-01') & (df['Date'] <= '2019-12-31')] # date from aug 2018 to dec 2019 for testing ","d15a9e2c":"test_df.tail()","96e3931f":"plt.figure(figsize=(20,10))\nplt.title('Forecasting trend')\nplt.xlabel('Date')\nplt.ylabel('Monthly Mean Total Sunspot Number')\n\nplt.plot(df['Date'], df['Monthly Mean Total Sunspot Number'])\nplt.show()","44fad17b":"split_time = 3000\ntime_train = time_data[:split_time]\nx_train = series[:split_time]\n\ntime_valid = time_data[split_time:]\nx_valid = series[split_time:]\n\n","3cee6fdf":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    \"\"\"\n    this function creates windows from inputted series, like this fashion:\n    1: [1,2,3,4]. --> [4,3,2,1]\n    2: [2,3,4,5]. --> [3,4,5,2]\n    3: [3,4,5,6]. --> [6,3,5,4]\n    4: [6,7,8,9]...\n\n    \"\"\"\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size + 1, shift=2, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n    dataset = dataset.batch(batch_size).prefetch(1)\n    return dataset","1c8cbbac":"dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)","f8e8a95c":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(20, input_shape=[window_size], activation=\"relu\"), # to avoid vanishing gradient problem\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9))\n","525c886d":"model.summary()","a01b280b":"%%time\nhistory = model.fit(dataset, epochs=100, verbose=0)","76459bd4":"%%time \nforecast = []\n\nfor time in range(len(series) - window_size):\n    forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n\nforecast = forecast[split_time-window_size:]\nresults = np.array(forecast)[:, 0, 0]","5edbf512":"# Visualising the result from Aug 2018 to Dec 2019\nplt.figure(figsize=(20,10))\nplot_series(time_valid[-len(test_df):], x_valid[-len(test_df):])\nplot_series(time_valid[-len(test_df):], results[-len(test_df):])","07098242":"# Visualising on whole dataset\nplt.figure(figsize=(20,10))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)\nplt.show()","bfdf8396":"len(x_valid)","d4c8d72a":"# Evaluate the model using MAE\ntf.keras.metrics.mean_absolute_error(x_valid, results).numpy()","08179fcf":"tf.keras.backend.clear_session()\ntf.random.set_seed(0)\nnp.random.seed(0)\n\n# Prepare the training set\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)\n\ndataset = windowed_dataset(x_train, window_size=60, batch_size=64, shuffle_buffer=shuffle_buffer_size)\n\n# Define the model\nmodel_1 = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.Dense(80, activation=\"relu\"),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\nopt1 = tf.keras.optimizers.SGD(lr=1e-5)\n\nmodel_1.compile(loss=tf.keras.losses.Huber(),\n              optimizer=opt1,\n              metrics=[\"mae\"])","e1acd330":"model_1.summary()","cd41ca15":"%%time\nhistory = model_1.fit(dataset, epochs=100, verbose=1)","bdd86b33":"def model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","430958a8":"# Predict the result\nlstm_rnn_forecast = model_forecast(model_1, series[..., np.newaxis], window_size)\nlstm_rnn_forecast = lstm_rnn_forecast[split_time - window_size:-1, -1, 0]","a7067d93":"# Visualising the result from Aug 2018 to Dec 2019\nplt.figure(figsize=(20,10))\nplot_series(time_valid[-len(test_df):], x_valid[-len(test_df):])\nplot_series(time_valid[-len(test_df):], lstm_rnn_forecast[-len(test_df):])","065cd282":"# Visualising on whole dataset\nplt.figure(figsize=(20,10))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, lstm_rnn_forecast)\nplt.show()","bf19256c":"# Evaluate the model using MAE\ntf.keras.metrics.mean_absolute_error(x_valid, lstm_rnn_forecast).numpy()","46235aca":"tf.keras.backend.clear_session()\ntf.random.set_seed(0)\nnp.random.seed(0)\n\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)\n\ndataset = windowed_dataset(x_train, window_size=60, batch_size=64, shuffle_buffer=shuffle_buffer_size)\n\n# Define the model\n\nmodel_2 = tf.keras.models.Sequential([\n    tf.keras.layers.SimpleRNN(64, return_sequences=True, activation=\"relu\", input_shape=[None, 1]),\n    tf.keras.layers.SimpleRNN(64, return_sequences=True, activation=\"relu\"),\n    tf.keras.layers.Dense(32, activation=\"relu\"),\n    tf.keras.layers.Dense(16, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n])\n\n\nmodel_2.compile(loss=tf.keras.losses.Huber(),\n              optimizer='adam',\n              metrics=[\"mae\"])\n\nmodel_2.summary()","1e134655":"%%time \nhistory = model_2.fit(dataset, epochs=100, verbose=1)","0915373d":"# Predict the result\nrnn_forecast = model_forecast(model_2, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","0444d55e":"# Visualising the result\nplt.figure(figsize=(20,10))\nplot_series(time_valid[-len(test_df):], x_valid[-len(test_df):])\nplot_series(time_valid[-len(test_df):], rnn_forecast[-len(test_df):])\nplt.show()","dfb2e4f4":"# Visualising on whole dataset\nplt.figure(figsize=(20,10))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)\nplt.show()","cd1249a5":"# Evaluate the model using MAE\ntf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","ca786095":"# getting data testing purpose\n\ndata from data from **Aug 2018** to **Dec 2019** will be used for the purpose of **Testing** out Forecasting model (as per assignment)\n\n\n","a7e95586":"# training of LSTM Arch.\n\n","0303e872":"# 3) Simple RNN based Architecture\n\n","6eda3204":"# splitting series  data for train and validation datasets","113a39f2":"# Notes\n before running below codes, make sure you have csv file of sunspots which is to be used at current location i.e. location same as this notebook\n\n\n","337f257c":"# forecasting Predicted output on LSTM Arch.\n---\n","5f367e9f":"# Installing required dependencies","c213a8c3":"# Windowing Forecasting data ","285d8fd4":"# Visualizing Forecasting Data","dbcdaeed":"# 2) LSTM based Architecture ","bbf93827":"# Training of Deep Neural Network","a5bbc038":"## 1) Defining Simple DNN based sequential Network"}}