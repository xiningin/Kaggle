{"cell_type":{"b2241ed9":"code","11d5fa56":"code","d968168b":"code","7c17b370":"code","c2dba35c":"code","31daf01f":"code","3a98866c":"code","de2f7f5f":"code","941f569c":"code","281319f4":"code","b3a1ec6a":"code","cd49a121":"code","4d9416ba":"code","7b8f4dc7":"code","554d3330":"code","c57f0157":"code","73d0d613":"code","54a2ee83":"code","6361c3da":"code","f29954c5":"code","98a75d56":"code","ffad2e53":"code","59eff63c":"code","2b25cdaa":"code","23bd5d15":"code","4ed693bd":"code","29b85f55":"code","1ede1f09":"code","9a2cfbc6":"code","f2e9d4a6":"code","6b8460d2":"code","47d7dc06":"code","f688dfdc":"code","c83c3882":"code","b57fb7ab":"code","ee76d725":"code","ac3bf75f":"code","e4c265f0":"code","8ad05072":"code","e22b629a":"code","95e5c9f5":"code","39376229":"code","e3975f49":"markdown","529a597f":"markdown","6cfdcac2":"markdown","b21e0b58":"markdown","ac09f356":"markdown","3c33b6e4":"markdown","25579575":"markdown","e82ba302":"markdown","7d64aa59":"markdown","f88d665b":"markdown","b066f835":"markdown","c113d8f2":"markdown","a70868d5":"markdown","99bb5804":"markdown","78e1467e":"markdown","6a792d89":"markdown","9f59216b":"markdown","ae559477":"markdown","1e4eed39":"markdown","6d39b7e1":"markdown","c1bba8b2":"markdown","cd0ada4e":"markdown","f7d9c0ab":"markdown","81193b89":"markdown","373bfb83":"markdown","f530d776":"markdown","52066748":"markdown","facc3a8e":"markdown","76572254":"markdown","ad2050dc":"markdown","0289b2f1":"markdown","e8f1b74e":"markdown","2e0de101":"markdown","0aae86cf":"markdown","2695ed36":"markdown","52685114":"markdown","3f07cd6d":"markdown","0acf1c64":"markdown","e82e2229":"markdown","06a25f62":"markdown","63fa1846":"markdown"},"source":{"b2241ed9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","11d5fa56":"data=  pd.read_csv(\"\/kaggle\/input\/california-housing-prices\/housing.csv\")\ndata.head()","d968168b":"data.describe()","7c17b370":"# Libraries\nimport plotly.express as px","c2dba35c":"data.info()","31daf01f":"# Housing Median Age\nfig = px.histogram(data, x=\"housing_median_age\", nbins = 30)\nfig.show()","3a98866c":"# Total Rooms\nfig = px.histogram(data, x=\"total_rooms\")\nfig.show()","de2f7f5f":"# Total Bed Rooms\nfig = px.histogram(data, x=\"total_bedrooms\")\nfig.show()","941f569c":"# Population\nfig = px.histogram(data, x=\"population\")\nfig.show()","281319f4":"# Households\nfig = px.histogram(data, x=\"households\")\nfig.show()","b3a1ec6a":"# median_income\nfig = px.histogram(data, x=\"median_income\")\nfig.show()","cd49a121":"# median_house_value\nfig = px.histogram(data, x=\"median_house_value\")\nfig.show()","4d9416ba":"# Ocean Proximity\ndf = data.groupby('ocean_proximity')['ocean_proximity'].count().reset_index(name = 'count')\nfig = px.bar(df, x='ocean_proximity', y='count')\nfig.show()","7b8f4dc7":"fig = px.scatter(data, x=\"median_house_value\", y=\"ocean_proximity\", color = 'housing_median_age')\nfig.show()","554d3330":"fig = px.scatter(data, x=\"median_house_value\", y=\"housing_median_age\", color = 'ocean_proximity')\nfig.show()","c57f0157":"fig = px.scatter(data, x=\"longitude\", y=\"latitude\", color = 'ocean_proximity')\nfig.show()","73d0d613":"fig = px.scatter(data, x=\"longitude\", y=\"latitude\", color = 'median_house_value')\nfig.show()","54a2ee83":"df_corr = data.corr()\nfig = px.imshow(df_corr)\nfig.show()","6361c3da":"from pandas.plotting import scatter_matrix\nattr = ['median_house_value','median_income','total_rooms','housing_median_age']\nscatter_matrix(data[attr], figsize = (15,10))","f29954c5":"data[\"income_cat\"] = pd.cut(data[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","98a75d56":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data,data['income_cat']):\n    strat_train_set = data.loc[train_index]\n    strat_test_set = data.loc[test_index]\nprint(strat_train_set.shape)\nprint(strat_test_set.shape)\nprint(data.shape)","ffad2e53":"# Creating new features from existing\ndata['rooms_per_household'] = data['total_rooms'] \/ data['households']\ndata['bedrooms_per_room'] = data['total_bedrooms'] \/ data['total_rooms']\ndata['population_per_household'] = data['population'] \/ data['households']","59eff63c":"corr = data.corr()\ncorr['median_house_value'].sort_values(ascending = False)","2b25cdaa":"fig = px.imshow(corr)\nfig.show()","23bd5d15":"from sklearn.impute import SimpleImputer","4ed693bd":"imputer  = SimpleImputer(strategy = 'median')\nhousing_num = data.drop(\"ocean_proximity\", axis = 1)\nmedian = imputer.fit(housing_num)\nX = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns = housing_num.columns, index = housing_num.index)","29b85f55":"from sklearn.preprocessing import OneHotEncoder\nhousing_cat = data[['ocean_proximity']]\ncat_encoder = OneHotEncoder()\nhousing_onehot = cat_encoder.fit_transform(housing_cat)","1ede1f09":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","9a2cfbc6":"features = strat_train_set.columns.to_list()","f2e9d4a6":"# Custom Transformers to create some extra features.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household = X[:, population_ix] \/ X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","6b8460d2":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","47d7dc06":"# add the categorical columns with transformer\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","f688dfdc":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","c83c3882":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(lin_rmse)","b57fb7ab":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor().fit(housing_prepared, housing_labels)\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\nprint(tree_rmse)","ee76d725":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\ntree_rmse_scores = np.sqrt(-scores)\nprint(\"Scores : \",str(tree_rmse_scores))\nprint(\"Mean : \",str(tree_rmse_scores.mean()))\nprint(\"Standard Deviation : \",str(tree_rmse_scores.std()))","ac3bf75f":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\nlin_rmse_scores = np.sqrt(-lin_scores)\nprint(\"Scores : \",str(lin_rmse_scores))\nprint(\"Mean : \",str(lin_rmse_scores.mean()))\nprint(\"Standard Deviation : \",str(lin_rmse_scores.std()))","e4c265f0":"from sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor()\nforest.fit(housing_prepared, housing_labels)\n\nforest_scores = cross_val_score(forest, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint(\"Scores : \",str(forest_rmse_scores))\nprint(\"Mean : \",str(forest_rmse_scores.mean()))\nprint(\"Standard Deviation : \",str(forest_rmse_scores.std()))","8ad05072":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n    {'n_estimators':[10,30, 50, 70, 100],'max_features':[6,8,10,15]},\n    {'bootstrap':[False], 'n_estimators':[3,10],'max_features':[2, 3, 4]}]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, \n                           param_grid, \n                           cv = 5, \n                           scoring = 'neg_mean_squared_error', \n                           return_train_score = True)\ngrid_search.fit(housing_prepared, housing_labels)\nprint(grid_search.best_params_)","e22b629a":"grid_search.best_estimator_","95e5c9f5":"st_time = time.time()\nforest = RandomForestRegressor(max_features = 8)\nforest.fit(housing_prepared, housing_labels)\n\nforest_scores = cross_val_score(forest, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint(\"Scores : \",str(forest_rmse_scores))\nprint(\"Mean : \",str(forest_rmse_scores.mean()))\nprint(\"Standard Deviation : \",str(forest_rmse_scores.std()))\nend_time = time.time()\nprint(\"Total Time : \",str(round(end_time - st_time,2)))","39376229":"import time\nst_time = time.time()\nforest = RandomForestRegressor(max_features = 15)\nforest.fit(housing_prepared, housing_labels)\n\nforest_scores = cross_val_score(forest, housing_prepared, housing_labels, scoring = 'neg_mean_squared_error', cv = 10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint(\"Scores : \",str(forest_rmse_scores))\nprint(\"Mean : \",str(forest_rmse_scores.mean()))\nprint(\"Standard Deviation : \",str(forest_rmse_scores.std()))\nend_time = time.time()\nprint(\"Total Time : \",str(round(end_time - st_time,2)))","e3975f49":"<font color = 'Blue'>Ocean Proximity Map View:<\/font>\nYou can see the Ocean Proximity value in a clear way in the chart.","529a597f":"## Decision Tree","6cfdcac2":"**Pipelines**\n\nA sequence of actions like imputation, encoding the features, normalizing the values would be arranged and kept it as pipeline before the data feed into the ML algorithms.\n\nGenerally this pipelines helps to achieve the data preprocessing activities in a proper flow and it can be useable any time.","b21e0b58":"Random Forest model's result is acceptable","ac09f356":"### Creating New Features\n\n<b> How creation of new features would help model?<\/b>\n\nModel requires a lot of data or features to learn a pattern. \nIf current number of features is not enough, we can(if possible) create some more features from existing features. \nalso creating new feature would help to increase the accuracy of the model.","3c33b6e4":"This notebook is result of my reading \n<b> <font color = 'Red'>Hands-on Machine Learning using Scikit-Learn, Tensorflow, Keras by Aurelien Geron<\/font><\/b>\n\nI utilized his way of approaching the problem and my way of exploring the options in the Scikit-Learn.","25579575":"## 1. Objective\nPredict each districts median price of the house in California State. Target variable name : `median_house_value`","e82ba302":"## Cross Validation\n**Scikit learn's K-Fold Cross validation**\n\nEvery time this K-fold cross validation would pick a split of sample test data for example 10%, the remaining 90% would be Train data. Data present in the Train data would not be there in Test data. K- number of times it should be folded. 10 means, 10 times with different trained and tested.","7d64aa59":"we can see the difference of timing with 8 and 15 features. so we are using the gridsearch option to get the optimal hyperparameters.","f88d665b":"<font color = 'Blue'>Total Rooms: <\/font> Similar to Total number of Rooms, Total number of Bedrooms also less than 1500 for most of the districts. very few districts having more than 3000. It may be developed district.","b066f835":"<font color = 'Blue'>Median House Value:<\/font> The Median house value is start from 50K to 300K. very few house values are in 500K. It may be near to bay.","c113d8f2":"## 1. Exploratory Data Analysis","a70868d5":"## Data Cleaning","99bb5804":"Data with Null values or different ranges or missed values would not acceptable by the model. we need to provide quality data to get a better model with high accuracy. There are multiple options to do the data cleaning. lets explore one by one.","78e1467e":"<font color = 'Blue'>Population and Households<\/font> Similar to Total Rooms and Total Bedrooms, Population and Households are having the similar range of values.","6a792d89":"<font color = 'Blue'>House Value:<\/font>\nThe price of the houses in the bay area is too high compare to all other places.","9f59216b":"Compared the Linear Regression and Decision Tree Models. Linear Regression is performing better as Decision Tree overfits the training data. So will explore Ensemble model- Random Forest.\n\n## Random Forest","ae559477":"**Performance Measure**\nRMSE : Root Mean Squared Error","1e4eed39":"<font color = 'Blue'>Null check<\/font>: Except Total Bedrooms column all the columns doesnt have null","6d39b7e1":"Null Values:\nRemoving null values from the dataset causes data shortage. so better way is filling the values, if its numerical we can give a try with median or mean value, if its categorical mode value.\n\nInstead of traditional approach like fillna, we are going to try Sklearn's Imputer classes.","c1bba8b2":"Using the Pipeline, the abouve code, Imput the missing values especially in number of bedrooms, create 3 features(bedrooms_per_room, rooms_per_household, population_per_household) then normalized the values in different ranges.","cd0ada4e":"## Linear Regression","f7d9c0ab":"* Bedrooms_per_room is having negative correlation\n* rooms_per_household is having positive correlation","81193b89":"<font color = 'Blue'>Ocean Proximity: <\/font> High number of houses are in 1 hour distance from Ocean, followed by Inland. these areas might be little cheaper. Near Bay and Ocean is good number of Houses.","373bfb83":"<font color = 'Blue'>Features Correlation:<\/font>\nAlmost most of the features are having negative correlation with Median house value and few features are having positive correlation.","f530d776":"## Feature Engineering","52066748":"<font color = 'blue'>Total Rooms:<\/font> Since this data is collected in 1990, the total number of rooms in the district is by average 1800 and max 5000 rooms. very few districts hase more than 10000 rooms.","facc3a8e":"**Validate Model Performance**\n\nTo know the performance of the trained model, how good its generalize the new dataset or unknown dataset, there are two methods Root Mean Squred Error and Mean Absolute Error.\n\nIn this problem we are using Root Mean Squared Error.\n\nTo learn about RMSE: https:\/\/www.statisticshowto.com\/probability-and-statistics\/regression-analysis\/rmse-root-mean-square-error\/\n","76572254":"<font color='Blue'>Age vs House Value vs Ocean Proximity:<\/font> \n* Island is having very few houses which is also more than 50+ age and price is also little high.\n* Inland house values are very low and its spread in all the ages.\n* Near Ocean and 1H Ocean is having spread over all the range either Price or Age.\n* Near Bay is costly and its spread over all the age range.","ad2050dc":"Apply the GridSearch Result best parameters\n\nJust I have used the exact result of grid search","0289b2f1":"The previous correlation heatmap and the above scatter chart clearly mentions there is a high correlation between median house value and median income in the district. which means who are all getting good income, they are buying a costly house or because of them the real estate values are going high. Since Real estate is one of the investment option :)","e8f1b74e":"<font color = 'Blue'>Median Income:<\/font> Most district people are earning more then 2K and upt0 7K. very few districts has more than 10K income. But also based on the textbook, its capped with some range of values. ","2e0de101":"## GridSearchCV\n\nThis is one of the option to get the optimal values for Hyperparameters to train the RandomForest Model.","0aae86cf":"It plays a very important role in Machine Learning Model Building. Data is in raw format. even though we had completed the Analysis to understand the data and business we cant say this data is good to feed into a ML Model.\n\nFeature Engineering would convert the raw data into Machine consumable data such as proper matrix after Scaling different range of values into a fixed range of value, filling Null values, Encoding Text values into binary values.","2695ed36":"<font color = 'Blue'>House Median Age:<\/font> Almost a good number of houses having the age of above 15 and below 40. Also a good number of houses are having the age of 50+","52685114":"If you <font color = 'orange'>like<\/font> this kernel and want to <font color = 'orange'>fork<\/font> plz <font color = 'red'>UPVOTE.<\/font>\n\nIf you have suggestions to improve this kernel plz <font color = 'red'>COMMENT.<\/font>\n\n<font color = 'orange'>************************ Notebook is under construction ************************<\/font>","3f07cd6d":"let me change the max features to 10 and see the result","0acf1c64":"## Correlation charts","e82e2229":"**Data Set Split** \n\nHere we are spliting the dataset into two parts. 75% would be Train and 25% would be Test dataset. also to get the right combination of data in each features values.","06a25f62":"## Model Training","63fa1846":"**Categorical Features**\n\nGenerally ML algorithms accepts the numerical values to train or predict. there are multiple ways to convert the categorical values into numerical values. pandas provides pandas.dummies() to convert the categorical values to numerical values by creating new columns and placing yes means 1 and no means 0, male means 1 and female means 0 and so more.\n\nSklearn provides an easy way to handle this. OnehotEncoder. once its fits with data it remembers the respected values. so it would help to fill the data which is used to predict."}}