{"cell_type":{"4e4b7001":"code","61dfef7e":"code","df935d94":"code","0bb7b0ac":"code","b06e290e":"code","e23292aa":"code","094bac11":"code","20e83788":"markdown","983bb1b1":"markdown","bcad468a":"markdown","b7db851b":"markdown","dbfe35d9":"markdown","0df46ab4":"markdown","1d4f877e":"markdown","3c308962":"markdown","beb8513b":"markdown","e089f3a6":"markdown","accd44fc":"markdown","43fc2a1e":"markdown","521d1413":"markdown","55cc0298":"markdown"},"source":{"4e4b7001":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import datasets\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt # for creating plots\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","61dfef7e":"# for reproducibility of the example generation\nnp.random.seed(0)\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 1500\nnoise=0.05\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=noise)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=noise)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nno_structure = np.random.rand(n_samples, 2), [0]*n_samples\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with different variances\nvaried = datasets.make_blobs(n_samples=n_samples,\n                             cluster_std=[1.0, 2.5, 0.5],\n                             random_state=random_state)\n\n# collect into array\nexamples = [noisy_circles,noisy_moons,varied,aniso,blobs,no_structure]\n\n# now lets display this data to see what we are dealing with\n# color indicates class of that point\nfig, axes = plt.subplots(2, 3, figsize=(16,10))\n\nfor i in range(0,len(examples)):\n    sns.scatterplot(ax=axes[int(i\/3),i%3],x=examples[i][0][:,0],y=examples[i][0][:,1],hue=examples[i][1],legend=False)\n\nplt.show()","df935d94":"from sklearn.cluster import KMeans\n\nmodels = []\nclassification = []\nscore = []\nn_clusters = [2,2,3,3,3,3]\n\nfor i in range(0,len(examples)):\n    # fit the model and predict\n    models.append(KMeans(n_clusters=n_clusters[i],random_state = 47))\n    classification.append(models[i].fit_predict(examples[i][0]))\n    \n# plot results\nfig, axes = plt.subplots(2, 3, figsize=(16,10))\n\nfor i in range(0,len(examples)):\n    ax = axes[int(i\/3),i%3]\n    sns.scatterplot(ax=ax,x=examples[i][0][:,0],y=examples[i][0][:,1],hue=classification[i],legend=False)\n    \nplt.show()","0bb7b0ac":"from sklearn.mixture import GaussianMixture\n\nmodels = []\nclassification = []\nn_clusters = [2,2,3,3,3,3]\n\nfor i in range(0,len(examples)):\n    # create a KMeans clusterer and fit it\n    models.append(GaussianMixture(n_components=n_clusters[i],covariance_type='full',random_state = 47))\n    models[i].fit(examples[i][0])\n    \n    # make a prediction\n    classification.append(models[i].predict(examples[i][0]))\n    \n# plot results\nfig, axes = plt.subplots(2, 3, figsize=(16,10))\n\nfor i in range(0,len(examples)):\n    sns.scatterplot(ax=axes[int(i\/3),i%3],x=examples[i][0][:,0],y=examples[i][0][:,1],hue=classification[i],legend=False)\n\nplt.show()","b06e290e":"from sklearn.cluster import MeanShift\n\nmodels = []\nclassification = []\n\nfor i in range(0,len(examples)):\n    # create a Mean Shift clusterer and fit it\n    models.append(MeanShift(n_jobs=-1,bandwidth=.7))\n    classification.append(models[i].fit_predict(examples[i][0]))\n    \n# plot results\nfig, axes = plt.subplots(2, 3, figsize=(16,10))\n\nfor i in range(0,len(examples)):\n    sns.scatterplot(ax=axes[int(i\/3),i%3],x=examples[i][0][:,0],y=examples[i][0][:,1],hue=classification[i],legend=False)\n\nplt.show()","e23292aa":"from sklearn.cluster import AgglomerativeClustering\n\nmodels = []\nclassification = []\nnumber = [2,2,3,3,3,3]\n\nfor i in range(0,len(examples)):\n    # create an Agglomerative Cluterer clusterer and fit it\n    # linkage may be chosen between 'ward', 'complete', 'average', and 'single'\n    models.append(AgglomerativeClustering(n_clusters=number[i],linkage='ward'))\n    classification.append(models[i].fit_predict(examples[i][0]))\n    \n# plot results\nfig, axes = plt.subplots(2, 3, figsize=(16,10))\n\nfor i in range(0,len(examples)):\n    sns.scatterplot(ax=axes[int(i\/3),i%3],x=examples[i][0][:,0],y=examples[i][0][:,1],hue=classification[i],legend=False)\n\nplt.show()","094bac11":"from sklearn.cluster import DBSCAN\n\nmodels = []\nclassification = []\neps = [0.1,0.1,0.65,0.35,1,0.1]\n\nfor i in range(0,len(examples)):\n    # create an Agglomerative Cluterer clusterer and fit it\n    # linkage may be chosen between 'ward', 'complete', 'average', and 'single'\n    models.append(DBSCAN(eps=eps[i],min_samples=5))\n    classification.append(models[i].fit_predict(examples[i][0]))\n    \n# plot results\nfig, axes = plt.subplots(2, 3, figsize=(16,10))\n\nfor i in range(0,len(examples)):\n    sns.scatterplot(ax=axes[int(i\/3),i%3],x=examples[i][0][:,0],y=examples[i][0][:,1],hue=classification[i],legend=False)\n\nplt.show()","20e83788":"# Modelling Overview (Clustering)\n\n\nThis is a set of personal notes on clustering models used in data science for future reference. The goal is to discuss the main features of a number of clustering algorithms without going into too much detail, and demonstrate how they are implemented in python. Since this is intended for my own personal use, the questions and confusions I address will be unique to my own background and experience. Nonetheless, I hope that others studying to enter data science will find this a convenient reference in their own data science journeys. Questions and comments are always appreciated! For those that found this notebook useful, I also recommend refering to my notes on [classification](https:\/\/www.kaggle.com\/michaelgeracie\/modelling-overview-classification) and [regression](https:\/\/www.kaggle.com\/michaelgeracie\/modelling-overview-regression).\n\nWe will use the following conventions throughout. The training data is given by a set of $m$ numerical features $\\mathbf x_j \\in \\mathbb R^n$ where $j = \\{ 1 , \\dots , m \\}$. We will sometimes denote feature vectors using the bold face vector notation just given, and sometimes use components. The $i$th feature of the $j$th training sample is then $x_{ij} = (\\mathbf x_j )_i$ where $i \\in \\{ 1 , \\dots , n\\}$. The features may be real valued or categorical, and if the $i$th feature is categorical, that component will be labelled by some subset $ C \\subseteq \\mathbb Z$ of the integers. The goal will be to find a division of the feature space $\\mathbf R^n$ into regions of \"like\" points according to some criterion of what it means for points to be similar. Each algorithm will use different criterion and as such these algorithms are not as directly comparable as say classification and regression algorithms are. Which algorithm is appropriate depends very much on the problem at hand. One big distinguishing feature is that some algorithms will need to know ahead of time how many clusters to create while others can learn this themselves. \n\nTo algorithms we have covered to date are\n\n## 1) K-Means Clustering\n## 2) Gaussian Mixture Models\n## 3) Mean Shift\n## 4) Hierarchical Agglomerative Clustering\n## 5) DBSCAN\n\nThe following is the standard code to start up a notebook","983bb1b1":"# Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n\nA common problem in the above models is the strong tendency for models to prefer 'blob-shaped' regions when clustering. DBSCAN is an entirely different approach that defines clusters as areas of high density separated by low density regions. The clustering regions can then have arbitrary shape and have no tendency to form in a particular way. The algorithm also does not require prior knowledge of the number of clusters.\n\nUnlike the approaches discussed above, not all points are necessarily in a region of sufficiently high density to be classified as being in a cluster and so are left unclassified.\n\nThe algorithm is straightforward enough to describe in words. Given a metric on $\\mathbf R^n$, which we assume here to be euclidean (though others are possible in `sklearn`), the model requires two parameters, `eps`, the radius of a circular neighborhood, and `min_samples`, the minimum number of samples in said neighborhood to continue grouping things together. Together, these two parameters define what it means for a sample to be in a 'dense' region. A 'core sample' is any sample in the training data such that there are `min_samples` within a distance of `eps`, which are in turn called its 'neighbors'.\n\nThe algorithm proceeds by beginning with a core sample, grouping it together with its neighbors, then further adding neighbors of those samples that are also core samples, and so on, until there are no further neighbors in the clustering that are also core samples. This proceedure continues until all core samples are part of some cluster. Data points that are not neighbors of any points in these clusters are considered unclassified.","bcad468a":"# Gaussian Mixture Models (GMM)\n\nOur main source on GMMs is the wikipedia article on [expectation-maximization](https:\/\/en.wikipedia.org\/wiki\/Expectation%E2%80%93maximization_algorithm), but we try to present things in a more pedagogical way, at least to my own tastes.\n\nIn gaussian mixture models, we assume that the data is iid drawn from a known number of classes, say $k \\in \\{ 1, \\dots , K \\}$, each of which is gaussian distributed. Denote the joint probability distribution of the feature vector $\\mathbf x$ and the class $y$ as $p(x,y)$. On a draw, the probability of getting something from class $k$ is $p(y = k) = \\tau_k$, so that $\\sum_k \\tau_k = 1$, and given that we are in class $k$, the probability that we get feature vector $\\mathbf x$ is a gausian random variable, which we write as\n\\begin{align}\n    p ( \\mathbf x ; \\mathbf \\mu_k , \\Sigma_k ) \\equiv p ( \\mathbf x | y = k ) = \\frac{1}{\\sqrt{(2 \\pi)^n|\\Sigma_k|}} e^{- \\frac 1 2 ( \\mathbf x - \\mathbf \\mu_k )^T \\Sigma_k^{-1} ( \\mathbf x - \\mathbf \\mu_k )}\n\\end{align}\nHere $\\mu$ is the mean of the gaussian distribution and $\\Sigma$ is the covariance matrix. Our model is defined by the parameters $\\tau_k$, $\\mu_k$, and $\\Sigma_k$ and our goal is to find best estimates for them given the training data $x_{ij}$, with *no information on the classes that these training samples belong to*. This gives a probabilistic model which we can turn into a classifier by picking the class with highest probability at a given point $\\mathbf x$.","b7db851b":"## Example\n\nBelow we run the k-means algorithm on all 6 examples considered above. As seen below, k-means always divides the data into not only convex cells, but convex cells of comparable size. When the the true classifications are not convexly shaped, the algorithm fails entirely. When it is convexly shaped but drawn from classes with significantly different or even non-proportional-to-identity covariance structure, it works better, but will fail to capture this aspect.\n\nNote in the final example we fed the model the incorrect number of clusters to find to see how it performs in this case.","dbfe35d9":"## Expectation-maximization\n\nSo let's preceed with our changes to $L(\\theta;X,y)$ to make this problem easier. \nFirst, we could maximize the log-likelihood $\\log L ( \\theta ; X , \\mathbf y )$, which is a simple quadratic function in all of the parameters, but we again need to specify what to do with $\\mathbf y$. We can average over it again, taking the expectation value of $\\log L ( \\theta ; X , \\mathbf y )$ with respect to $\\mathbf y$. In taking the expectation, we need to specify the probability distribution for $y$. We take the conditional one based off of the training data,\n\\begin{align}\n    p ( y | \\mathbf x , \\theta) .\n\\end{align}\nfor a single $y$. As is, this puts us back where we started, losing quadratic-ness of the problem in $\\theta$ (we multiply this distribution with the log-liklihood before taking a sum).\n\nHence, we approach the problem iteratively: given some assumed values $\\theta^{(t)}$ for the parameters at the $t$th step, we calculate the expectation value of the log-likelihood $\\log L ( \\theta ; X , \\mathbf y )$, which is now quadratic in $\\theta$. The conditional probability distribution for $y$ is then given by\n\\begin{align}\n    p(y | \\mathbf x ; \\theta^{(t)} ) = \\frac{\\tau^{(t)}_y p ( \\mathbf x ; \\mathbf \\mu^{(t)}_y , \\Sigma^{(t)}_y )}{\\sum_{y'=1}^K \\tau_{y'} f(\\mathbf x ; \\mathbf \\mu^{(t)}_{y'} , \\Sigma^{(t)}_{y'})} \\equiv T^{(t)}_{y,\\mathbf x}\n\\end{align}\nusing Bayes' theorem. This is a simple quadratic problem with analytic solution that can be found in the [wiki article](https:\/\/en.wikipedia.org\/wiki\/Expectation%E2%80%93maximization_algorithm). We then update the assumed values of the parameters $\\theta^{(t+1)}$ to be this new maximum.\n\nTo summarize, we repeat the following steps until improvement in $Q$ dips below some set value $\\epsilon$. Begin by initializing $\\theta$ to some random values. Then, given $\\theta^{(t)}$, first compute\n\\begin{align}\n    Q ( \\theta , \\theta^{(t)} ) &= E_{\\mathbf y | X ; \\theta^{(t)}} \\left[ \\log L ( \\theta ; X , \\mathbf y ) \\right] \\nonumber \\\\\n        &= \\sum_{j=1}^m \\sum_{y=1}^K T^{(t)}_{y,\\mathbf x_j} \\left( \\log \\tau_y - \\frac 1 2 ( \\mathbf x_j - \\mathbf \\mu_y)^T \\Sigma_y^{-1} ( \\mathbf x_j - \\mathbf \\mu_y ) - \\frac 1 2 \\log | \\Sigma_j | - \\frac n 2 \\log ( 2 \\pi )\\right)\n\\end{align}\nThen maximize it with respect to $\\theta$. The new maximum is then $\\theta^{(t+1)}$. This is obviously quadratic in $\\theta$ and may be done analytically.\n","0df46ab4":"## Setting up the problem\n\nSo how do we find the \"best\" values of the parameters? For convenience, let's denote the model's parameters collectively by $\\theta = (\\tau_k, \\mathbf \\mu_k , \\Sigma_k)_{k=1}^K$, the training data $\\mathbf x_j$ by a matrix $X$ with matrix elements $X_{ij} = (\\mathbf x_j)_i$, and the class $y_j \\in \\{ 1 , \\dots , K \\}$ of each training example by a vector $\\mathbf y$. Crucially, $\\mathbf y$ is unknown: clustering is an unsupervised learning problem and we want the algorithm to learn what the best class assingments are.\n\nNow, if we had all the data $(X,\\mathbf y)$, we would want to maximize the likelihood that it was drawn from a distribution with the parameters $\\theta$\n\\begin{align}\n    L(\\theta ; X, \\mathbf y ) &= \\prod_{j=1}^m p(\\mathbf x_j , y_j) \\nonumber \\\\\n        &= \\prod_{j=1}^m p(y = y_j ) p(\\mathbf x_j | y = y_j) \\nonumber \\\\\n        &= \\prod_{j=1}^m \\tau_{y_j} p (\\mathbf x_j ; \\mathbf \\mu_{y_j} , \\Sigma_{y_j})\n\\end{align}\nas a function of $\\theta$. Similarly, if we knew $(\\theta, X)$ and were trying to find $y$, we would try to maximize this likelyhood as a function of $\\mathbf y$. We are in neither of these situations: all we have is $X$ and need to find $\\theta$ without knowledge of $\\mathbf y$. \n\nOne might want then want to maximize the so-called marginal likelihood, where we average over $y$'s\n\\begin{align}\n    L ( \\theta ; X ) = \\sum_{\\mathbf y} L ( \\theta ; X , \\mathbf y )\n\\end{align}\nbut apparently this problem is usually intractable analytically (it's a *sum* of a large number of gaussian exponentials). Let's make this task easier one step at a time by modifying the function we are maximizing.\n\nThe function we will be considering in the end, denoted $Q(\\theta, \\theta^{(t)})$ will be quite different from the function above which is of actual interest, but one can show that iterative improvements in the maximization of $Q$ lead to iterative increases in the marginal liklihood$L ( \\theta ; X )$ (see the [wiki article](https:\/\/en.wikipedia.org\/wiki\/Expectation%E2%80%93maximization_algorithm) for a proof). This algorithm is called expectation-maximization (EM), and supposedly is computationally more efficient than other numerical approaches to directly maximizing the marginal likelihood (otherwise we would just attack the marginal liklihood directly).","1d4f877e":"## Example\n\nNote that the model is highly sensitive to the value of epsilon that you feed it. Getting reasonable values, as in the example below, takes some trial and error.","3c308962":"# Preparing Examples\n\nBefore we move on to the algorithms, below we randomly generate data for 6 different clustering problems. These clustering problems involve differing notions of what a cluster 'should' be and so are a good way to see the advantages and disadvantages of each algorithm in various settings. For example, some involve blob shapped regions assumed to come from a gaussian distribution, others involve irregularly shaped regions where being part of the same cluster has more to with being part of a contiguous region of high density. The original code is pulled from `sklearn`'s [example](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_cluster_comparison.html) comparing various algorithms on clustering problems of different types. We will be doing essentially the same thing as them in this notebook, but for pedagogical purposes, I think it's good to look independently at each model, how it works, and then see its performance on the data.","beb8513b":"## Example\n\nhere we put an example that also shows the tree","e089f3a6":"# K-Means Clustering\n\nThe idea behind the k-means algorithm is to partition the feature space $\\mathbb R^n$ into a predetermined number $K$ of regions. Each region $R_k$, $k \\in \\{1, \\dots , K\\}$ is determined by a particular vector in $\\mathbf m_k \\in \\mathbb R^n$ and consists of all points that are closer to this vector than any other. The vector $m_k$ itself is to be the mean of all the training samples $\\mathbf x_i$ that are in $R_k$. The goal is to find a collection of $\\mathbf m_k$'s such that the average distance of sample points in $R_k$ to the vector $\\mathbf m_k$ definining that region is minimized. More briefly stated, the goal is to find\n\\begin{align}\n    \\text{arg min}_{\\mathbf m} \\sum_{k=1}^K \\sum_{\\mathbf x_i \\in R_k} || \\mathbf x_i - \\mathbf m_k ||^2 .\n\\end{align}\n\nSome obvious limitations of this formulation of the problem include\n* we must know the number of desired clusters ahead of time\n* the clustered regions $R_k$ will always be convex (the shape is called a 'voronoi cell' - see [wiki](https:\/\/en.wikipedia.org\/wiki\/Voronoi_diagram)). If the data contains more complicated cluster geometry, the model will provide a bad fit.\n\nFinding the global minimum is in principle a very difficult problem, however, there is a simple iterative algorithm called Lloyd's algorithm that converges to a local minimum of the above in $O(n)$ time. It is this algorithm, or Elkan's algorithm, that is used by `sklearn`'s implementation of k-means clustering. We will cover only Lloyd's algorithm here, our source mainly being the discussion on [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/K-means_clustering).\n\nLloyd's algorithm begins with a randomly selected collection of \"mean\" vectors $\\mathbf m^{(1)}_k$ and then iteratively updates these in the following way:\n\n1) given $\\mathbf m^{(t)}_k$, partition the feature space $\\mathbf R^n$ into $K$ subregions\n\\begin{align}\n    R^{(t)}_k = \\{ \\mathbf x \\in \\mathbb R^n : || \\mathbf x - \\mathbf m^{(t)}_k ||^2 \\leq || \\mathbf x - \\mathbf m^{(t)}_{k'}||^2 ~ \\text{for all} ~ k'\\}\n\\end{align}\n\n2) update the vectors $\\mathbf m_k$ to be the means of the training vectors beloning to these new sub-regions\n\\begin{align}\n    m^{(t+1)}_k = \\frac{1}{|R^{(t)}_k|} \\sum_{\\mathbf x_i \\in R^{(k)}_k} \\mathbf x_i\n\\end{align}\n\nAt each step in this process, the updated region contains new points, and a new collection of mean vectors is defined to be at the center of that new cluster. This iteratively moves the mean vectors toward the center of regions of higher density. The algorithm continues this process until the until the assignments of training data to clusters no longer changes, or until a fixed maximum number of iterations is reached.","accd44fc":"# Heirarchical Agglomerative Clustering\n\nHeirarchical agglomerative clustering, or HAC, is a general family of clustering algorithms that builds clusters iteratively by iteratively joining like clusters. There are also non-agglomerative algorithms that start from a single large cluster and divides them into most-like clusters, but we will consider only the agglomerative case here. The algorithm produces not only a clustering, but a heirarchy of clusterings: finer clusters created earlier in the proceedure are subclusters of those later on. In agglomerative clustering, all samples begin in their own unique cluster and are then joined until they are all members of a since cluster, the \"root\" of the tree that represents this heirarchy. The proceedure is simple, and can be found outlined in `sklearn`'s documentation [here](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#hierarchical-clustering).\n\nAt each step, the \"linkage\" between each cluster and all other clusters is evaluated, and the clusters are combined to either maximize the linkage of the clusters so combined, or of the whole collection. The measure of linkage can be selected from several options depending on the nature of the problem. `sklearn`'s implementation includes four to choose from: Ward, maximum, average, and single linkages. Furthermore, the distance metric on $\\mathbf R^n$ used in these linkage measures, called the 'affinity', may be selected. In `sklearn`, we have the choice between euclidean, $L^1$, $L^2$, manhattan, cosine, or user supplied metrics.\n* Ward: the sum of squared differences is minimized within clusters. That is at each step, we find the pair of clusters that leads to minimum in-cluster variance once merged. See [here](https:\/\/en.wikipedia.org\/wiki\/Ward%27s_method)\n* Maximum or complete linkage: join pairs of clusters with the minimum maximum distance between observations\n* Average linkage: join pairs of clusters with the minimum average distances between all pairs of observations across clusters\n* Single linkage: join piars of clusters with the minimum minimum distance between pairs of clusters.\n\nAccording to `sklearn`'s documentation, the 'single linkage' method has the greatest tendancy to result in very different cluster dizes, while Ward gives the most regular sizes. Single linkage is highly sensitive to noise, but is also very efficient and performs relatively well on 'non-globular' data.","43fc2a1e":"## Example\n\nBelow we fit a GMM to each of the 6 classification problems generated at the beginning of this notebook. As we see, this performs significantly better than KMeans when the data forms blobs of different shapes and sizes, but still does not genrealize well (at all) beyond blobs, and still need the user to tell it how many blobs there in fact are.","521d1413":"## Example\n\nWe can see these advantages and disadvantages clearly below. Note we do not need to feed the model an expected number of classes.","55cc0298":"# Mean Shift Clustering\n\nFor this section we mainly recap the information from `sklearn`'s [documentation](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#mean-shift) on how them implement mean shift. The algorithm is similar to k-means: we find a collection of special vectors $\\{\\mathbf m_k\\}_{k=1}^K$ in feature space, and a given feature vector $\\mathbf x$ is classified according to which of these it is closest to. These vectors are found iteratively by repeatedly taking the means of sample data within a given region.\n\nIt differs significantly from KMeans in that the region of interest is a circle $N (\\mathbf x)$ centered at $\\mathbf x$ of some given radius, not all points of the given classification. We repeatedly update this vector to be the average of all points in this region\n\\begin{align}\n    m^{(t+1)}_k = \\frac{\\sum_{\\mathbf x_j \\in N(\\mathbf m^{(t)}_k)}  \\mathbf x_j }{\\sum_{\\mathbf x_j \\in N(\\mathbf m^{(t)}_k)} 1} .\n\\end{align}\nUpon repeated iterations, this will shift toward regions of higher density.\nOne could also weight by a kernel, but it appears according to `sklearn`'s [documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.MeanShift.html) that `sklearn.cluster.MeanShift` does not do this. Rather, `sklearn`'s documentation appears to refer to the entire neighborhood $N(\\mathbf x)$, with uniform weighting, as the 'kernel'. Frankly, it's unclear to me how the model selects the radius of these kernels. Perhaps this is the `bandwidth` parameter?\n\nIn either case, this is run until the points in the kernel ceases to change, or until a given number of max iterations. Once complete, overlapping kernels are discarded in favor of that kernel with the greatest density of points. Once there are no overlapping kernels, points are classified according to which vector $\\mathbf m_k$ it is closest to. It appears that the algorithm begins with a kernel at every point in the training data, though there is an option `bin_seeding` that will bin these points together to speed up the algorithm.\n\nOne advantage of this method over k-means and GMM is that it dynamically learns the number of clusters it should use. It may perform better than k-means when clusters have well defined regions of highest density. Other than that, it seems to suffer from many of the disadvantages of k-means: clustered regions are convex and will be of comparable size, leaving little room for non-trivial covariance structure or non-blob-like shapes."}}