{"cell_type":{"e77d47f1":"code","3597ec15":"code","c64bf486":"code","3c99bb3d":"code","ff4cca72":"code","41b4f6aa":"markdown","864267de":"markdown","97048621":"markdown","b32b21b5":"markdown"},"source":{"e77d47f1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lightgbm\nfrom cycler import cycler\nfrom datetime import datetime\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc\nfrom sklearn.svm import LinearSVC\n","3597ec15":"# Read the data\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\nfeatures = [f for f in test_df.columns if f != 'Id' and f != 'Cover_Type']\n","c64bf486":"%%time\n# Start by defining the classification target, 'istest'\ntrain_df['istest'] = False\ntest_df['istest'] = True\n\n# Concatenate the two datasets and shuffle\nboth_df = pd.concat([train_df, test_df])\nboth_df = both_df.sample(frac=1)\n\n# Train and validate the two classifiers (LightGBM and LinearSVC)\nscore_list = []\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(both_df, y=both_df.istest)):\n    print(f\"Fold {fold}\")\n    X_tr = both_df.iloc[train_idx]\n    X_va = both_df.iloc[val_idx]\n    y_tr = X_tr.istest\n    y_va = X_va.istest\n    X_tr = X_tr[features]\n    X_va = X_va[features]\n\n    model = lightgbm.LGBMClassifier(n_estimators=600, learning_rate=0.7)\n    model.fit(X_tr, y_tr)\n    \n    y_va_pred = model.predict_proba(X_va)[:,1]\n    acc = accuracy_score(y_va, y_va_pred > 0.5)\n    auc_ = roc_auc_score(y_va, y_va_pred)\n    print(f\"Accuracy {acc:.5f}  AUC {auc_:.5f} {model}\")\n    score_list.append((acc, auc_))\n    \n    plt.figure(figsize=(8, 8))\n    fpr, tpr, _ = roc_curve(y_va, y_va_pred)\n    plt.plot(fpr, tpr, color='r', lw=1, label=f\"LGBM (auc = {auc(fpr, tpr):.5f})\") # roc curve\n    plt.plot([0, 1 - (acc*len(both_df) - len(test_df)) \/ len(train_df)], # iso-accuracy line\n             [(acc*len(both_df) - len(train_df)) \/ len(test_df), 1], color='r', lw=1, linestyle=':')\n\n    model2 = make_pipeline(StandardScaler(), LinearSVC(dual=False))\n    model2.fit(X_tr, y_tr)\n    y_va_pred2 = model2.decision_function(X_va) + 0.5 # for LinearSVC\n    acc2 = accuracy_score(y_va, y_va_pred2 > 0.5)\n    print(f\"Accuracy {acc2:.5f}  AUC {roc_auc_score(y_va, y_va_pred2):.5f} {model2}\")\n    fpr, tpr, _ = roc_curve(y_va, y_va_pred2)\n    plt.plot(fpr, tpr, color='orange', lw=1, label=f\"LinearSVC (auc = {roc_auc_score(y_va, y_va_pred2):.5f})\") # roc curve\n    plt.plot([0, 1 - (acc2*len(both_df) - len(test_df)) \/ len(train_df)], # iso-accuracy line\n             [(acc2*len(both_df) - len(train_df)) \/ len(test_df), 1], color='orange', lw=1, linestyle=':')\n    del model2\n\n    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n    plt.gca().set_aspect('equal')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver Operating Characteristic\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    break # One fold is enough\n    \ntrain_df['weight'] = model.predict_proba(train_df[features])[:,1]\n","3c99bb3d":"prop_cycle = plt.rcParams['axes.prop_cycle']\ncustom_cycler = cycler(color=[col + '80' for col in prop_cycle.by_key()['color']])\nplt.gca().set_prop_cycle(custom_cycler)\n\nplt.hist(model.predict_proba(X_va[features][~y_va])[:,1], bins=100, label='train')\nplt.hist(model.predict_proba(X_va[features][y_va])[:,1], bins=100, label='test')\nplt.legend()\nplt.show()\n","ff4cca72":"plt.figure(figsize=(24, 24))\nlightgbm.plot_tree(model.booster_, orientation='vertical', ax=plt.gca())\nplt.show(9)\n\nplt.figure(figsize=(12, 14))\nlightgbm.plot_importance(model.booster_, importance_type='gain', ax=plt.gca())\nplt.show()\n    \n","41b4f6aa":"# Train and test don't overlap\n\nAfter the experience with the November competition, I wanted to know whether in the December competition train and test have the same distribution. It turns out that a LightGBM classifier easily can separate the two with high accuracy, which means that there is little to no overlap between the two regions of the feature space.\n\nThe decision surface is not linear. We see this because an SVM cannot separate train from test. The SVM reaches an accuracy of only 80 %, which is easy because 80 % of the whole data is training data.","864267de":"The histogram shows that there is very little overlap between train and test data (if the regions overlapped in feature space, the histogram areas would overlap as well). The blue area is larger than the other one because there is four times more training data than test data:","97048621":"Now we try to classify the data into training and test, using two different classifiers.","b32b21b5":"The first tree of the model starts at Wilderness_Area3, where the difference between train and test is most conspcicuous, and this feature gets the highest importance:"}}