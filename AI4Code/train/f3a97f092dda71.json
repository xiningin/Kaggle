{"cell_type":{"81eca143":"code","4e4cd446":"code","3e113b1b":"code","02347d20":"code","c702f6fd":"code","3938ae9f":"code","db89d3e0":"code","163a64d8":"code","31907ca7":"code","ce074151":"code","8ced66de":"code","25bc5078":"code","d7f8496d":"code","c0a6078c":"code","76139c17":"code","6d7e7aca":"code","749eb88d":"code","0f21cc62":"code","cd256354":"code","eda826b3":"code","87f9537a":"code","453a6a3f":"code","34ca2f64":"code","3ead9180":"code","a36226ac":"code","b07c9eaf":"code","d09aba93":"markdown","1b023b06":"markdown","d7e90aea":"markdown","338f2f04":"markdown","45a16224":"markdown","707a8613":"markdown","0f58191b":"markdown","53730600":"markdown"},"source":{"81eca143":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.metrics import f1_score\nimport graphviz\nfrom sklearn import tree\nimport random\nfrom sklearn.metrics import confusion_matrix","4e4cd446":"test = pd.read_csv('..\/input\/liverpool-ion-switching\/test.csv')\ntrain = pd.read_csv('..\/input\/liverpool-ion-switching\/train.csv')\ntrain.head()","3e113b1b":"pd.value_counts(train['open_channels'])","02347d20":"train.shape","c702f6fd":"import tensorflow as tf\n\nimport numpy as np","3938ae9f":"train_val_signal = np.array(train['signal'])\ntrain_val_y = np.array(train['open_channels'])\n","db89d3e0":"train_signal = train_val_signal[:4800000]\ntrain_y = train_val_y[:4800000]","163a64d8":"val_signal = train_val_signal[4800000:]\nval_y = train_val_y[4800000:]","31907ca7":"from sklearn.preprocessing import scale\ntrain_signal = scale(train_signal, axis=0, with_mean=True, with_std=True, copy=True )","ce074151":"val_signal = scale(val_signal, axis=0, with_mean=True, with_std=True, copy=True )","8ced66de":"def data_generator(data_signal, data_y, batch_size, signal_size):\n    def g():\n        \n        start_index = random.randint(0,len(data_signal) - (signal_size + 1))\n         \n        x = data_signal[start_index:(start_index+signal_size)]\n        y = data_y[start_index + (signal_size \/\/ 2)]\n \n        return x,y\n            \n    while True:\n        x_batch = np.zeros(shape = (batch_size,signal_size))\n        y_batch = np.zeros(shape = (batch_size,1))\n        for k in range(batch_size):\n            x_batch[k],y_batch[k] = g()\n            \n        yield x_batch,y_batch\n        ","25bc5078":"train_gen = data_generator(train_signal, train_y, batch_size = 200, signal_size = 101)\nval_gen = data_generator(val_signal, val_y, batch_size = 200, signal_size = 101)","d7f8496d":"for x,y in val_gen:\n    print(y.shape)\n    break;\n    ","c0a6078c":"from tensorflow import keras\nfrom tensorflow.keras.layers import Input, Dense, Dropout,BatchNormalization\n\ninputs = Input(shape=(101,))\nxx = Dense(101, activation= 'softmax')(inputs)\nxx = BatchNormalization()(xx)\nxx = Dense(101, activation= 'softmax')(xx)\nxx = BatchNormalization()(xx)\nxx = Dense(101, activation= 'softmax')(xx)\n\n\noutputs = Dense(11, activation= 'softmax')(xx)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n","76139c17":"\nmodel.compile(optimizer='adam',\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n             metrics=['accuracy'])\n","6d7e7aca":"model.summary()","749eb88d":"model.fit(train_gen,\n            steps_per_epoch=1000,\n            epochs=10)\n\n","0f21cc62":"y_hat = []\ny_true = []\n\n\nfor x,y in train_gen:\n    y_true = y_true +  list(y)\n    y_hat = y_hat + list(np.argmax(model.predict(x), -1))\n\n    break;\n        \nprint(confusion_matrix(y_hat,y_true))\n","cd256354":"test_signal = test['signal']","eda826b3":"new_test = []\nbatch_size = 200\ninput_size = 101\nstart = 0\ntest_signal_list = list(test_signal)+ [0] * input_size\nwhile (start+101) < len(test_signal_list):\n    batch = []\n    for x in range(batch_size):\n        batch.append(test_signal_list[start:(start + 101)])\n        #batch.append([0]*101)\n        start = start + 1\n    new_test.append(batch)\n    if len(new_test) % 1000 == 0:\n        print(len(new_test))\n","87f9537a":"arrp = []\nfor k in range(len(new_test)):\n    arrp.append(np.argmax(model.predict(np.array(new_test[k])), -1))\n    if k % 200 == 0:\n        print(k)","453a6a3f":"h = [i for sublist in arrp for i in sublist]\nh = [0]* 50 + h\nprint(len(h))\nfor x in range((101 \/\/ 2) ):\n    h.pop()\n","34ca2f64":"pd.value_counts(h)","3ead9180":"sub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\n","a36226ac":"sub['open_channels'] = h","b07c9eaf":"sub.to_csv('submission.csv',index=False,float_format='%.4f')","d09aba93":"# Reshape test in batches\nThe new test should have the following shape\n\n(10000, 200, 101)\n\nwhere 10000 prediction are going to happend\nEach batch contains 200 lists for each we have 101 signals, output y will be a list of shape 200\n\n","1b023b06":"# Scale the data","d7e90aea":"# Load the data ","338f2f04":"# Build the model","45a16224":"# Create a data generator\nThis is important to load the data in patches otherwise, loading data will take long time\n\nHere, I am using 50 signals before and 50 signals after the point we want to predict its channels. That's why I am using signal size to be 101\n","707a8613":"The current score is .55 but this means the method works to some extent","0f58191b":"# Split to train and validation","53730600":"# Submition"}}