{"cell_type":{"aa6c6096":"code","218a1d74":"code","487c2bf0":"code","005f1d88":"code","084f87f6":"code","7a979963":"code","9f96abeb":"code","2ebba9c4":"code","b0ae9182":"code","fe07272a":"code","0a6c100c":"code","b1fbc888":"code","455e4121":"code","25ec5891":"code","1c9d88f2":"code","94d21745":"code","edcabf65":"code","4c337402":"code","79b61fe4":"code","454050fa":"code","96406795":"code","cea62afb":"code","0161a4e9":"code","0474e77e":"code","17de63ba":"code","e2d4a6a7":"code","6f73386e":"code","ec792c67":"code","23dce8d1":"code","95dcfeec":"code","6d5a251c":"code","f967fcdd":"code","6569ba7c":"code","1eef2d99":"code","0e1d79d2":"code","6902f4d1":"code","f0e9b4af":"code","b83fad55":"code","e493c34a":"code","1493773d":"code","13d87ab0":"code","cb87dc8b":"code","0cfa7814":"code","4818047e":"code","c0014168":"code","d8abad5d":"code","a2a7b1d0":"code","71899155":"code","bdd1cb0d":"code","c1c2a244":"code","eb5b82dd":"code","a31f0b0c":"code","765f0142":"code","71109e0a":"code","25470e5b":"code","e2216521":"code","6399efd0":"markdown","676b3068":"markdown","6dd1feb6":"markdown","be79b286":"markdown"},"source":{"aa6c6096":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","218a1d74":"train= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","487c2bf0":"train.head()","005f1d88":"train.target.unique()","084f87f6":"train.info()","7a979963":"train.describe()","9f96abeb":"c = train['target'].value_counts(ascending=True)\nprint(c)","2ebba9c4":"plt.rcParams['figure.figsize'] = (7, 5)\nplt.bar(10,c[1],3, label=\"Real\", color='blue')\nplt.bar(15,c[0],3, label=\"Not\", color='green')\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Propertion of examples')\nplt.show()","b0ae9182":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","fe07272a":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title('disaster')\nword=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","0a6c100c":"def remove_punctuation(text):\n    '''a function for removing punctuation'''\n    import string\n    # replacing the punctuations with no space, \n    # which in effect deletes the punctuation marks \n    translator = str.maketrans('', '', string.punctuation)\n    # return the text stripped of punctuation marks\n    return text.translate(translator)","b1fbc888":"train['text'] = train['text'].apply(remove_punctuation)\ntrain.head(10)","455e4121":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","25ec5891":"def create_corpus_df(tweet, target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","1c9d88f2":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n","94d21745":"# displaying the stopwords\nlist(stop)","edcabf65":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n","4c337402":"np.array(top)","79b61fe4":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","454050fa":"sns.barplot(x=x,y=y)","96406795":"def get_top_tweet_bigrams(corpus,r,n=None):\n    vec = CountVectorizer(ngram_range=(r, r)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","cea62afb":"plt.figure(figsize=(20,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(train['text'],1)[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=x,y=y)","0161a4e9":"plt.figure(figsize=(20,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(train['text'],2)[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=x,y=y)","0474e77e":"plt.figure(figsize=(20,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(train['text'],3)[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=x,y=y)","17de63ba":"df=pd.concat([train,test])\ndf.shape","e2d4a6a7":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","6f73386e":"df['text']=df['text'].apply(lambda x : remove_URL(x))","ec792c67":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","23dce8d1":"df['text']=df['text'].apply(lambda x : remove_html(x))","95dcfeec":"corpus_0=create_corpus_df(df,0)\nlen(corpus_0)","6d5a251c":"corpus_0[:10]","f967fcdd":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_0[:10]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","6569ba7c":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_0[:100]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","1eef2d99":"corpus_1=create_corpus_df(df,1)\nlen(corpus_1)","0e1d79d2":"corpus_1[:10]","6902f4d1":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_1[:10]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","f0e9b4af":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_1[:100]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","b83fad55":"# TF-IDF scores for all the words in the corpus\ndef tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","e493c34a":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['black','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='black', label='Not')\n            blue_patch = mpatches.Patch(color='blue', label='Real')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_tfidf, y_train)\nplt.show()","1493773d":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus","13d87ab0":"corpus=create_corpus(df)","cb87dc8b":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","0cfa7814":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","4818047e":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","c0014168":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec ","d8abad5d":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","a2a7b1d0":"model.summary()","71899155":"target = train['target'].values\ntrain=tweet_pad[:train.shape[0]]\ntest=tweet_pad[train.shape[0]:]","bdd1cb0d":"X_train,X_test,y_train,y_test=train_test_split(train,target,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","c1c2a244":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(train,target)\nplt.show()","eb5b82dd":"history=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=2)","a31f0b0c":"train_pred_GloVe = model.predict(test)\n#since the train_pred_GloVe value are propability we will round it of and convert it into integrer\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')","765f0142":"train_pred_GloVe","71109e0a":"pred = pd.DataFrame(train_pred_GloVe, columns=['preds'])\npred.plot.hist()","25470e5b":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = train_pred_GloVe_int\nsubmission.head(10)","e2216521":"submission.to_csv(\"submission.csv\", index=False, header=True)","6399efd0":"Fake Tweet","676b3068":"****Using Tfidf****\n\nTF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.","6dd1feb6":"**Using GloVe  https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove**","be79b286":"> *Lenght of Non disaster tweets are greater then the real one.. Makes sence people in actual disaster won't have that much time to write longer tweets... *"}}