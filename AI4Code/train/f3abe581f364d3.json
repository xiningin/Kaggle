{"cell_type":{"6973d17d":"code","850df20d":"code","dfc72c3b":"code","81c0938f":"code","f8693445":"code","d0c903d3":"code","03cbaca9":"code","9b04fb97":"code","5f609134":"code","2e99f985":"code","9882d66b":"code","09c4f2e1":"code","3ba19f06":"code","52a69f94":"code","8a6efc59":"code","101098b6":"code","743e1598":"code","fc918643":"code","f8c7e5c4":"code","8404f92e":"code","1dcfea42":"code","6c73a42a":"code","8e5fec8c":"code","d02943ae":"markdown","3d387e08":"markdown","889487cb":"markdown","1c85774a":"markdown","7e18b76c":"markdown","ace942f0":"markdown","9b79e89c":"markdown","2ddd1324":"markdown","09e0c87f":"markdown","0aef9db8":"markdown","78e40a45":"markdown","3d955697":"markdown","27e2456e":"markdown","d9e1c721":"markdown","c58a7702":"markdown","e14b8eb2":"markdown","0626eab0":"markdown"},"source":{"6973d17d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","850df20d":"'''Import the e-commerce csv to a pandas df'''\ndata = pd.read_csv('\/kaggle\/input\/ecommerce-data\/data.csv',encoding= 'ISO-8859-1')\n# data = pd.read_csv('data.csv',encoding= 'ISO-8859-1')\nglobal separator\nseparator = '\\n***********************************************************************\\n'","dfc72c3b":"'''Basic info,describe and memory analysis over the dataset'''\nprint(separator)\nprint(data.info())\nprint(separator)\nprint(data.describe())\nprint(separator)\nprint(data.head())\nprint(separator)\nprint(data.memory_usage(deep=True))\nprint(separator)","81c0938f":"'''Let's perform some memory optimisation by making Country, StockCode and Description as categorical columns.'''\n\ndata[\"Country\"] = data[\"Country\"].astype(\"category\")\ndata[\"StockCode\"] = data[\"StockCode\"].astype(\"category\")\ndata[\"Description\"] = data[\"Description\"].astype(\"category\")\nprint(separator)\nprint(data.memory_usage(deep=True))\nprint(separator)","f8693445":"print(separator)","d0c903d3":"'''Let's find out the missing(isnull) absolute and % of data.'''\n\nprint(data.isnull().sum().sort_values(ascending=False))\nprint(separator)\nprint(round (data.isnull().sum().sort_values(ascending=False)\/len(data)*100,2))","03cbaca9":"print(data[[\"InvoiceNo\", \"Country\"]].groupby('Country').count().sort_values(\"InvoiceNo\", ascending=False))","9b04fb97":"print(sns.kdeplot(data['Quantity'], color=\"green\"))","5f609134":"print(sns.kdeplot(data['Quantity'], clip=(-20000, 20000), color=\"blue\"))\nplt.figure()\nprint(data['Quantity'].plot(kind='hist',bins=5))","2e99f985":"data.dropna(inplace=True)\ndata.isnull().sum()","9882d66b":"'''Basic info and describe analysis over the dataset'''\nprint(data.info())\nprint(separator)\nprint(data.describe())\nprint(separator)\nprint(data.head())","09c4f2e1":"data[(data['Quantity']<=0) | (data['UnitPrice']<0)].count()","3ba19f06":"data=data[data['Quantity']>0]\ndata=data[data['UnitPrice']>=0]","52a69f94":"print(data.shape)","8a6efc59":"data.head()","101098b6":"data['TotalAmount']=data['Quantity']*data['UnitPrice']\ndata.head()","743e1598":"data[data['TotalAmount']==data['TotalAmount'].max()]","fc918643":"data[data['CustomerID']==16446.0].sort_values(by='InvoiceDate', ascending=False)","f8c7e5c4":"'''Convert InvoicdeDate to datetime'''\ndata['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\n\n'''Grouping the data based on months to get a feel of the monthly sales data'''\ndata_new = data.groupby(pd.Grouper(key='InvoiceDate',freq='M')).sum()\ndata_new.reset_index(level=0, inplace=True)\ndata_new","8404f92e":"print(data_new.plot(x='InvoiceDate', y='TotalAmount',kind='bar'))\nprint(data_new.plot(x='InvoiceDate', y='Quantity',kind='bar'))","1dcfea42":"'''Top 5 countries sales count wise in the cleaned up data.'''\ndata.Country.value_counts().head().plot(kind='bar')","6c73a42a":"'''Top 5 countries Total Gross Amount sales wise.'''\ndata_temp = data.groupby(['Country'])['TotalAmount'].agg('sum').reset_index().sort_values(by=['TotalAmount'],ascending=False).head()\nprint(data_temp)\nprint(data_temp.plot(x='Country', y='TotalAmount',kind='bar'))","8e5fec8c":"data.groupby(['Description']).size().reset_index(name='counts').sort_values(by=['counts'],ascending=False).head()","d02943ae":"Let's plot it out to get a better visual representation.","3d387e08":"Now, let's do some operation on the InvoiceDate column to help us study the dataset more.","889487cb":"Let's add a column to find the total Amount or price of the invoice\/order. That would be Quantity * UnitPrice","1c85774a":"The Quantity is mainly available across the -20000,20000 range.","7e18b76c":"This is an e-commerce orders history data set so it be 'should' safe to assume that InvoiceNo a depictor of total number of invoices issued\/orders placed\/entries in the data. \nThere are total 541909 non-null entries.\nDescription(540455) and CustomerID(406829) has some missing values.\nDescription is a meta-data column and we may assume that neither it's values nor does some of it being missing shouldn't affect our EDA. Nonetheless we shall explore possibilites to ignore\/impute it.\nCustomerID values missing is an interesting find, seems like a data-entry limitation by BSS personnel. Having said that imputation for these missing values\nfor CustomerID seems tough and not fruitful since it's supposed to be a unique data for a unique customer. Having said that, we shall revisit this point after some analysis. We never know what pattern we may find. ;)\n\nFrom the describe() we see that we have 3 numerical columns: Quantity, UnitPrice and CustomerID. This was expected.\nThe spread is vast for all 3, since means and medians are far apart saying these data points aren't normally distributed.\nQuanitiy and UnitPrice have min in -ve, that's interesting fact. It mostly should refer to the 'return orders' for which the company has to pay out cash from it's pocket to that customer\/customerID.","ace942f0":"Now that's a lot better!! :D","9b79e89c":"Thus, we conclude that Nov-2011 resulted in the highest sales both by TotalAmount and Quantity(this may be the case because of Christmas Shopping? ;) ), while Feb-2011 was the worst in terms of sales and the last quarter of the year was best among the 2011 quarters.","2ddd1324":"So the invoice num 581483 was the largest order received amounting to 168469.6. Order was placed on 12-9-2011 by a UK customer(16446) with PAPER CRAFT , LITTLE BIRDIE description.","09e0c87f":"Description has .27% missing data, I'm going to let it be and not do anything about it for now. Later if we find it co relating to some other column we may think again.\n\nCustomerID has close to 25% missing data, we need to do something about it.\n\nLet's dig deeper.","0aef9db8":"CustomerID     135080\nDescription      1454\n\nCustomerID     24.93%\nDescription     0.27%\n\nThese are the NA values in our data set. Let's drop them, since description is low in % and it doesn't matter much so we may care not to produce it. CustomerID on the other hand is not possible to find out since there appears to be no co direct relation with other column(s).","78e40a45":"Let's find the largest amount order. ","3d955697":"It's a UK based ecommerce website, and their sales\/transactions data tell the same story. Highest number in the UK, followed by the EU and then the rest of the world. OK, this confirms our intuition. And the periods of transaction were in 1st Dec 2010 to 9th Dec 2011.","27e2456e":" # EDA conclusion:-\n\n1. Top 5 countries in terms of highest counts of sale\/invoices are: UK, Germany, France, Ireland, Spain.\n2. Top 5 countries in terms of Total Gross Amount sales are: UK, Netherlands, Ireland, Germany, France.\n3. The data had negative quantity\/unit price, those might have been return orders. But anyways we have ignored those ones from our list.\n4. Invoice num 581483 was the largest single order received amounting to 168469.6. Order was placed on 12-9-2011 by a UK customer(16446) with PAPER CRAFT , LITTLE BIRDIE description.\n5. Nov-2011 resulted in the highest sales both by TotalAmount and Quantity(this may be the case because of Christmas Shopping? ;) ), while Feb-2011 was the worst in terms of sales and the last quarter of the year was best among the 2011 quarters.\n6. 'WHITE HANGING HEART T-LIGHT HOLDER' was the top Order descriptions in terms of highest number of invoices against it.","d9e1c721":"Perfect, now we have dropped the NA rows. Let's look back at the basic info once again on the new df.","c58a7702":"8905 rows have Quantity in negative or UnitPrice negative. This might mean they are return orders, or we are not sure of the reason why they are so. 9k out of 5L records, we may trim it out and be fine with the data we will have left. Let's proceed that way.","e14b8eb2":"Besides that huge order, this particular customer hasn't placed any significant orders.","0626eab0":"Let's check for the top 5 Order descriptions in terms of highest number of invoices against it."}}