{"cell_type":{"28143542":"code","64f7ec5e":"code","61abe7ed":"code","a020470a":"code","b0a8fdca":"code","4034c8b7":"code","a3a558b3":"code","eb60348e":"code","6eee4f29":"code","c992245c":"code","f878a33a":"code","520c9a98":"code","2d9b7518":"code","2b29942f":"code","d060d722":"code","b55340b9":"code","76f47d95":"code","6f19fee5":"code","759c01c5":"code","be06cf06":"code","2f66ab7e":"code","b05f9599":"code","71e33e05":"code","fe6a4a3b":"code","a8820f7d":"code","ef7ad463":"code","7f6f19bd":"code","fa97acd1":"code","8c999cb1":"code","d03edd74":"code","4811a33c":"code","04217b8b":"code","33de9091":"code","c0ff1ccb":"code","4cc366b5":"code","05f0c33f":"code","48b6e18c":"code","a3b7ccbb":"code","742c19d6":"code","99c256b6":"code","4b1e1ca0":"markdown","95115cdf":"markdown","ac1007a4":"markdown","b070c73d":"markdown","7724325f":"markdown","55497ab4":"markdown","fbcb8f03":"markdown","ac325736":"markdown","6f89bf68":"markdown","d44eaec8":"markdown","c785ea85":"markdown","0d756d06":"markdown","8c816a2a":"markdown","10b7d6bc":"markdown","b579ba23":"markdown","f1543af1":"markdown","ada010c8":"markdown","71a06eb4":"markdown","0556a9e5":"markdown","aaabee00":"markdown","9198832e":"markdown","2948cf85":"markdown","c29a0458":"markdown","0af909dd":"markdown"},"source":{"28143542":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64f7ec5e":"import warnings\nwarnings.filterwarnings('ignore')","61abe7ed":"train_path = '\/kaggle\/input\/titanic\/train.csv'\ntest_path = '\/kaggle\/input\/titanic\/test.csv'\n\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\ntrain_df.head()","a020470a":"test_df.head()","b0a8fdca":"train_df.info()","4034c8b7":"train_df.describe()","a3a558b3":"train_df.shape","eb60348e":"train_df.isnull().sum()","6eee4f29":"sns.set()\nsns.countplot(train_df['Embarked']);","c992245c":"# filling the missing value of Embarked with S because S is frequently used\ntrain_df = train_df.fillna({\"Embarked\": \"S\"})","f878a33a":"train_df['Embarked'].isnull().sum()","520c9a98":"# One hot encoding applied on Embarked column(no ordering)\ntrain_df = pd.get_dummies(train_df, columns = ['Embarked'])\ntrain_df.head()\n","2d9b7518":"sns.set()\nsns.countplot(train_df['Sex']);","2b29942f":"# One hot encoding applied on sex column\ntrain_df = pd.get_dummies(train_df, columns = ['Sex'])\ntrain_df.head()","d060d722":"features = ['Pclass', 'Sex_male', 'Sex_female', 'Embarked_C', 'Embarked_Q',\n           'Embarked_S', 'Parch', 'SibSp', 'Fare']\n\ntrain_label = train_df[['Survived']]\ntrain_features = train_df[features]\ntrain_features.head()","b55340b9":"train_features.isnull().sum()","76f47d95":"# Importing decision tree classifier from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Fitting the decision tree with default hyperparameters, apart from\n# max_depth which is 5 so that we can plot and read the tree.\ndt_default = DecisionTreeClassifier(max_depth=5, random_state=0)\ndt_default.fit(train_features, train_label)","6f19fee5":"# Let's check the evaluation metrics of our default model\n\n# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Making predictions\ny_pred_default = dt_default.predict(train_features)\n\n# Printing classification report\nprint(classification_report(train_label, y_pred_default))","759c01c5":"# Printing confusion Matrix and accuracy\nprint(confusion_matrix(train_label, y_pred_default))\nprint(accuracy_score(train_label, y_pred_default))","be06cf06":"from sklearn import tree\nplt.figure(figsize=(10, 8))\ntree.plot_tree(dt_default)","2f66ab7e":"# Importing required packages for visualization\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\nfeatures","b05f9599":"data = export_graphviz(dt_default, out_file=None, feature_names = features,\n                       class_names = 'Survived', filled = True,\n                      rounded = True, special_characters = True)\ngraph = graphviz.Source(data)\ngraph","71e33e05":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Specify number of folds for KFolds CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(1, 40)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion='gini',\n                              random_state = 100)\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters,\n                   cv = n_folds,\n                   scoring = 'accuracy')\ntree.fit(train_features, train_label)","fe6a4a3b":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","a8820f7d":"# plotting accuracy with max_depth\nplt.figure()\nplt.plot(scores['param_max_depth'],\n        scores['mean_test_score'],\n        label=('Training accuracy'))\n\nplt.xlabel(\"Max depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","ef7ad463":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Specify number of folds for KFolds CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion='gini',\n                              random_state = 100)\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters,\n                   cv = n_folds,\n                   scoring = 'accuracy')\ntree.fit(train_features, train_label)","7f6f19bd":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","fa97acd1":"# plotting accuracy with min_samples_leaf\nplt.figure()\nplt.plot(scores['param_min_samples_leaf'],\n        scores['mean_test_score'],\n        label=('Training accuracy'))\n\nplt.xlabel(\"Max depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","8c999cb1":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Specify number of folds for KFolds CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion='gini',\n                              random_state = 100)\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters,\n                   cv = n_folds,\n                   scoring = 'accuracy')\n\ntree.fit(train_features, train_label)","d03edd74":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","4811a33c":"# plotting accuracy with min_samples_split\nplt.figure()\nplt.plot(scores['param_min_samples_split'],\n        scores['mean_test_score'],\n        label=('Training accuracy'))\n\nplt.xlabel(\"Max depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","04217b8b":"# Create the parameter grid\nparam_grid = {\"max_depth\": range(5, 15, 5),\n             \"min_samples_leaf\": range(50, 150, 50),\n             \"min_samples_split\": range(50, 150, 50),\n             \"criterion\": ['gini', 'entropy']}\n\nn_folds = 5\n\n# Instantiate the grid search model\ndtree = DecisionTreeClassifier()\ngrid_search = GridSearchCV(estimator = dtree,\n                          param_grid = param_grid,\n                          cv = n_folds,\n                          verbose = 1)\n\n# Fit the grid Search to the data\ngrid_search.fit(train_features, train_label)","33de9091":"# cv results\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results.head(10)","c0ff1ccb":"# printing the optimal accuracy score and hyperparameters\nprint(\"best accuracy: \", grid_search.best_score_)\nprint(grid_search.best_estimator_)","4cc366b5":"# model with optimal hyperparameter\nclf_gini = DecisionTreeClassifier(criterion='gini',\n                                 random_state=100,\n                                 max_depth=5,\n                                 min_samples_leaf=50,\n                                 min_samples_split=50)\nclf_gini.fit(train_features, train_label)","05f0c33f":"# Plot the tree\ndata = export_graphviz(clf_gini, out_file=None, feature_names = features,\n                       class_names = 'Survived', filled = True,\n                      rounded = True, special_characters = True)\ngraph = graphviz.Source(data)\ngraph","48b6e18c":"test_df.isnull().sum()","a3b7ccbb":"test_df = pd.read_csv(test_path)\ntest_df.head()","742c19d6":"meanFare = train_df['Fare'].mean()\ntest_df = test_df.fillna({\"Fare\": meanFare})\n\ntest_df = pd.get_dummies(test_df, columns=['Sex'])\ntest_df = pd.get_dummies(test_df, columns=['Embarked'])\n\nids = test_df['PassengerId']\ntest_feature = test_df[features]\npredictions = clf_gini.predict(test_feature)\n\noutput = pd.DataFrame({'PassengerId': ids, 'Survived': predictions})\noutput.head()","99c256b6":"output.to_csv('submission.csv', index=False)","4b1e1ca0":"The default tree is quite complex, and we need to simplify it by tuning the hyperparameters. \n\nFirst, let's understand the parameters in a decision tree. You can read this in the documentation using ```help(DecisionTreeClassifier)```.\n\n\n- **criterion** (Gini\/IG or entropy): It defines the function to measure the quality of a split. Sklearn supports \u201cgini\u201d criteria for Gini Index & \u201centropy\u201d for Information Gain. By default, it takes the value \u201cgini\u201d.\n- **splitter**: It defines the strategy to choose the split at each node. Supports \u201cbest\u201d value to choose the best split & \u201crandom\u201d to choose the best random split. By default, it takes \u201cbest\u201d value.\n- **max_features**: It defines the no. of features to consider when looking for the best split. We can input integer, float, string & None value.\n    - If an integer is inputted then it considers that value as max features at each split.\n    - If float value is taken then it shows the percentage of features at each split.\n    - If \u201cauto\u201d or \u201csqrt\u201d is taken then max_features=sqrt(n_features).\n    - If \u201clog2\u201d is taken then max_features= log2(n_features).\n    - If None, then max_features=n_features. By default, it takes \u201cNone\u201d value.\n- **max_depth**: The max_depth parameter denotes maximum depth of the tree. It can take any integer value or None. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. By default, it takes \u201cNone\u201d value.\n- **min_samples_split**: This tells above the minimum no. of samples reqd. to split an internal node. If an integer value is taken then consider min_samples_split as the minimum no. If float, then it shows percentage. By default, it takes \u201c2\u201d value.\n- **min_samples_leaf**: The minimum number of samples required to be at a leaf node. If an integer value is taken then consider - -min_samples_leaf as the minimum no. If float, then it shows percentage. By default, it takes \u201c1\u201d value.\n- **max_leaf_nodes**: It defines the maximum number of possible leaf nodes. If None then it takes an unlimited number of leaf nodes. By default, it takes \u201cNone\u201d value.\n- **min_impurity_split**: It defines the threshold for early stopping tree growth. A node will split if its impurity is above the threshold otherwise it is a leaf.<br>\n\n","95115cdf":"The hyperparameter **min_samples_leaf** indicates the minimum number of samples required to be at a leaf.<br>\n\nSo if the values of min_samples_leaf is less, say 5, then the will be constructed even if a leaf has 5, 6 etc. observations (and is likely to overfit).<br>\n\nLet's see what will be the optimum value for min_samples_leaf.","ac1007a4":"## Grid Search to Find Optimal Hyperparameters\n\nWe can now use GridSearchCV to find multiple optimal hyperparameters together. Note that this time, we'll also specify the criterion (gini\/entropy or IG).","b070c73d":"## Hyperparameter Tuning","7724325f":"### Tuning min_samples_split\n\nThe hyperparameter **min_samples_split** is the minimum no. of samples required to split an internal node. Its default value is 2, which means that even if a node is having 2 samples it can be furthur divided into leaf nodes.","55497ab4":"This shows that as you increase the min_samples_split, the tree overfits lesser since the model is less complex.","fbcb8f03":"## Applying Decision Tree on this Dataset","ac325736":"You can see that as we increase the value of max_depth, training score increase till about max-depth = 10, after which the test score gradually reduces. Note that the scores are average accuracies across the 5-folds.\n\nThus, it is clear that the model is overfitting the training data if the max_depth is too high. Next, let's see how the model behaves with other hyperparameters.","6f89bf68":"# Decision Tree: Titanic - Machine Learning from Disaster","d44eaec8":"Now let's visualize how train score changes with max_depth.","c785ea85":"Titanic dataset is very popular in machine learning. In this case study we will predict if a passenger survived the sinking of the Titanic or not? For each in this test set we must predict 0 or 1 value for the variable. As the heading, we will do it with very famous machine learning algorithm Decision Tree. Lots of thing we have to cover, so, let's start.","0d756d06":"Now let's visualize how train score changes with min_samples_leaf.","8c816a2a":"### Create submission File\n","10b7d6bc":"### Tuning max_depth","b579ba23":"### Tuning min_samples_leaf","f1543af1":"Here, we are creating a dataframe with max_depth in range 1 to 80 and checking the accuracy score corresponding to each max_depth. \n\nTo reiterate, a grid search scheme consists of:\n\n    - an estimator (classifier such as SVC() or decision tree)\n    - a parameter space\n    - a method for searching or sampling candidates (optional) \n    - a cross-validation scheme, and\n    - a score function (accuracy, roc_auc etc.)","ada010c8":"## If you find this notebook quite helpful, please **upvote** it.","71a06eb4":"## Features selection","0556a9e5":"Running the model with best parameters obtained from grid search.","aaabee00":"You can see that at low values of min_samples_leaf, the tree gets a bit overfitted. At values > 100, however, the model becomes more stable and the training and test accuracy start to converge.","9198832e":"## **We can Visualize in better way**","2948cf85":"In the following sections, we will:\n- clean and prepare the data\n- build a decision tree with default hyperparameter\n- we will use criterion `gini-index` as well as `entropy` \n- understand all the hyperparameter that we can tune, and finally\n- choose the optimal hyperparameter using grid search cross validation","c29a0458":"## Understanding and Cleaning The Data","0af909dd":"Let's first try to find the optimum values for max_depth and understand how the value of max_depth affects the decision tree."}}