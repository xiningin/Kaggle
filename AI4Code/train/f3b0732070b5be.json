{"cell_type":{"da513f37":"code","c04c7252":"code","e1031af9":"code","aa534eff":"code","2959372d":"code","34b854a5":"code","8f2d682b":"code","da91577b":"code","6b67fffe":"code","e02dea26":"code","176ae51a":"code","02e8beb5":"code","8990cbe6":"code","7d70e82e":"code","d3860989":"code","f8e90b0c":"code","3c4847ec":"code","dee84096":"code","455be6eb":"code","9fde9bfd":"code","ecff430c":"code","685d8f1f":"code","5b230b48":"code","e3930390":"code","a75a4bbc":"code","19c9b616":"code","843cbcc5":"code","d07273f6":"code","547494cf":"code","92bee634":"code","ee855b76":"code","f3d4a4f1":"code","6b9c35bd":"code","d5bbd9f9":"code","4ad2fe51":"code","de78d7f0":"code","5ab4a433":"code","16ee1686":"code","6469be81":"code","98969d82":"markdown","7b443f0c":"markdown","f104ce9e":"markdown","94ee2d7e":"markdown","2f9e9668":"markdown","3c2ed603":"markdown","9e9ed207":"markdown","32d5d715":"markdown","0a4582df":"markdown","7de1157b":"markdown","44ebf5e7":"markdown","1d87173c":"markdown","fbeecba6":"markdown","aaca2113":"markdown","48253d77":"markdown","5ac08684":"markdown","4e3fb9cb":"markdown","80b15522":"markdown","1336e1f1":"markdown","40505a62":"markdown","f0a8bdec":"markdown","29da29ba":"markdown","8b1a6a83":"markdown","26aca931":"markdown","5215d2db":"markdown","784eb199":"markdown","06b10040":"markdown","b90d2d31":"markdown","209cbb07":"markdown","00342586":"markdown","55d57164":"markdown","ea79d569":"markdown","199cab9b":"markdown","4cf27a2b":"markdown","de97b306":"markdown","3f12af47":"markdown","2aee268d":"markdown","4ec25d5d":"markdown","78d3edf9":"markdown","55e43347":"markdown","c1195ee6":"markdown","87695c5c":"markdown","2a47eded":"markdown","547127c3":"markdown","85ce5b55":"markdown","b51aeb4c":"markdown","7302a47a":"markdown","161bf869":"markdown","17e23686":"markdown","86730072":"markdown","47eefaec":"markdown"},"source":{"da513f37":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","c04c7252":"import warnings\nwarnings.filterwarnings('ignore')","e1031af9":"traindata = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntestdata = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","aa534eff":"traindata.info()","2959372d":"#Before dropping the outliers\nfig, ax = plt.subplots()\nax.scatter(x = traindata['GrLivArea'], y = traindata['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.title(\"Before dropping the outliers\")\nplt.show()\n\n#Deleting outliers\ntraindata = traindata.drop(traindata[(traindata['GrLivArea']>4000) & (traindata['SalePrice']<300000)].index)\n\n#After dropping the outliers\nfig, ax = plt.subplots()\nax.scatter(traindata['GrLivArea'], traindata['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.title(\"After dropping the outliers\")\nplt.show()","34b854a5":"traindata['SalePrice'].describe()","8f2d682b":"#Histogram\nsns.distplot(traindata['SalePrice'])\n\n#QQ-Plot\nfig = plt.figure()\nres = stats.probplot(traindata['SalePrice'], plot=plt)","da91577b":"print(\"Skewness before transformation:\", round(traindata['SalePrice'].skew(),5))\nprint(\"Kurtosis before transformation:\", round(traindata['SalePrice'].kurt(),5))","6b67fffe":"traindata['SalePrice'] = np.log(traindata['SalePrice'])","e02dea26":"#Histogram\nsns.distplot(traindata['SalePrice'])\n\n#QQ-Plote\nfig = plt.figure()\nres = stats.probplot(traindata['SalePrice'], plot=plt)","176ae51a":"print(\"Skewness after transformation:\", round(traindata['SalePrice'].skew(),5))\nprint(\"Kurtosis after transformation:\", round(traindata['SalePrice'].kurt(),5))","02e8beb5":"ntrain = len(traindata)\ny = traindata['SalePrice']\n\ncombined_df = pd.concat([traindata,testdata], ignore_index=True)\ncombined_df.drop(columns= 'SalePrice', inplace = True)\nprint(\"Shape of combined data frame: \", combined_df.shape)","8990cbe6":"#This heatmap shows the missing data present in the data frame\nplt.figure(figsize=(20,3))\nsns.heatmap(combined_df.isnull(), yticklabels=False, cbar=False, cmap = 'viridis')","7d70e82e":"(combined_df.isnull().sum().sort_values(ascending = False) \/ len(combined_df)) * 100","d3860989":"combined_df['PoolQC'] = combined_df['PoolQC'].fillna('None')\ncombined_df['MiscFeature'] = combined_df['MiscFeature'].fillna('None')\ncombined_df['Alley'] = combined_df['Alley'].fillna('None')\ncombined_df['Fence'] = combined_df['Fence'].fillna('None')\ncombined_df['FireplaceQu'] = combined_df['FireplaceQu'].fillna('None')","f8e90b0c":"combined_df['LotFrontage'] = combined_df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","3c4847ec":"for columns in ('GarageType','GarageFinish', 'GarageQual','GarageCond','MSSubClass','BsmtQual','BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','MasVnrType'):\n    combined_df[columns] = combined_df[columns].fillna('None')","dee84096":"for columns in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    combined_df[columns] = combined_df[columns].fillna(0)","455be6eb":"for columns in ('MSZoning','Electrical', 'KitchenQual','Exterior1st','Exterior2nd','SaleType'):\n    combined_df[columns] =  combined_df[columns].fillna(combined_df[columns]).mode()[0]","9fde9bfd":"combined_df['Functional'] =  combined_df['Functional'].fillna('Typ')\ncombined_df.drop(columns='Utilities', axis = 1, inplace = True)","ecff430c":"(combined_df.isnull().sum().sort_values(ascending = False) \/ len(combined_df)) * 100","685d8f1f":"combined_df['MSSubClass'] =  combined_df['MSSubClass'].apply(str)\ncombined_df['OverallCond'] = combined_df['OverallCond'].astype(str)","5b230b48":"combined_df['TotalSF'] = combined_df['TotalBsmtSF'] + combined_df['1stFlrSF'] + combined_df['2ndFlrSF']","e3930390":"#Getting only the numerical features\nnumeric_features = combined_df.dtypes[combined_df.dtypes != 'object'].index\n\n#Checking the skewness of all the numerical features\nskewed_features = combined_df[numeric_features].apply(lambda x: x.skew()).sort_values(ascending = False)\nskewed_df = pd.DataFrame({'Skew': skewed_features})\ndisplay(skewed_df)","a75a4bbc":"from scipy.special import boxcox1p\n\nskewed_df = skewed_df[abs(skewed_df) > 0.75]\nskewed_feats = skewed_df.index\nlam = 0.15\nfor feats in skewed_feats:\n    combined_df[feats] = boxcox1p(combined_df[feats], lam)","19c9b616":"combined_df = pd.get_dummies(combined_df, drop_first=True)\ncombined_df.shape","843cbcc5":"X_train = combined_df[:traindata.shape[0]]\nX_test = combined_df[traindata.shape[0]:]\ny = traindata.SalePrice","d07273f6":"from sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse.mean())","547494cf":"from sklearn.linear_model import LinearRegression\n\nlm_rmse = rmse_cv(LinearRegression())\nprint(\"RMSE for Linear Regression: \", lm_rmse)","92bee634":"from sklearn.linear_model import Lasso, LassoCV\n\nlassocv = LassoCV(cv=5,random_state=1)\nlassocv.fit(X_train, y)\nbest_alpha = lassocv.alpha_\n\nlasso_model = make_pipeline(RobustScaler(), Lasso(alpha= best_alpha, random_state=1))\nlasso_rmse = rmse_cv(lasso_model)\nprint(\"RMSE for LASSO (L1 Regularization): \", lasso_rmse)","ee855b76":"from sklearn.linear_model import Ridge, RidgeCV\n\nridge_model = make_pipeline(RobustScaler(), RidgeCV(alphas=np.logspace(-10,10,100)))\nridge_rmse = rmse_cv(ridge_model)\nprint(\"RMSE for Ridge Regression (L2 Regularization): \", ridge_rmse)","f3d4a4f1":"from sklearn.linear_model import ElasticNet, ElasticNetCV\n\nelasticnet_cv = ElasticNetCV(l1_ratio=np.arange(0.1,1,0.1), cv=5, random_state=1)\nelasticnet_cv.fit(X_train, y)\nbest_l1_ratio = elasticnet_cv.l1_ratio_\nbest_aplha = elasticnet_cv.alpha_\n\nelasticnet_model = make_pipeline(RobustScaler(), ElasticNet(alpha=best_alpha, l1_ratio= best_l1_ratio, random_state=1))\nelasticnet_rmse = rmse_cv(elasticnet_model)\nprint(\"RMSE for Elastic Net(L1 and L2 Regularization): \", elasticnet_rmse)","6b9c35bd":"from sklearn.ensemble import RandomForestRegressor\n\nrandomforest_model = rmse_cv(RandomForestRegressor(random_state=1))\nprint(\"RMSE for Random Forest: \", randomforest_model)","d5bbd9f9":"from sklearn.ensemble import GradientBoostingRegressor\n\ngradientboost_model = GradientBoostingRegressor(learning_rate=0.1, loss='huber', n_estimators=3000, random_state=1)\ngradientboost_cv = rmse_cv(gradientboost_model)\nprint(\"RMSE for Gradient Boost: \", gradientboost_cv)","4ad2fe51":"from sklearn.ensemble import AdaBoostRegressor\n\nadaboost_lassomodel = AdaBoostRegressor(lasso_model, n_estimators=50, learning_rate=0.001, random_state=1)\nadaboost_lassocv = rmse_cv(adaboost_lassomodel)\nprint(\"RMSE for Adaptive Boosting with LASSO estimator: \", adaboost_lassocv)","de78d7f0":"from sklearn.ensemble import AdaBoostRegressor\n\nadaboost_enetmodel = AdaBoostRegressor(elasticnet_model, n_estimators=50, learning_rate=0.001, random_state=1)\nadaboost_enetcv = rmse_cv(adaboost_enetmodel)\nprint(\"RMSE for Adaptive Boosting with Elastic Net estimator: \", adaboost_enetcv)","5ab4a433":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\npca = make_pipeline(RobustScaler(), PCA(n_components = 3, random_state = 1))\npca.fit(X_train.transpose())\nprint(f\"Proportion of variance explained by the components: {pca.steps[1][1].explained_variance_ratio_}\")\n\n# we are using 3 components in this case\np_comps = pca.steps[1][1].components_.transpose()\n\npca_lm_rmse = np.sqrt(-cross_val_score(LinearRegression(), p_comps, y, cv = 5, scoring = \"neg_mean_squared_error\")).mean()\nprint(f\"RMSE for Linear Regression after PCA reduction: [{pca_lm_rmse}]\")","16ee1686":"adaboost_enetmodel.fit(X_train, np.exp(y))\nsubmission_predictions = adaboost_enetmodel.predict(X_test)","6469be81":"results = pd.DataFrame({'Id': testdata['Id'], 'SalePrice':submission_predictions})\nresults.to_csv(\"submission.csv\", index = False)","98969d82":"### Modelling!","7b443f0c":"#### Imputing the missing values\n\nWe see that **`PoolQC`**, **`MiscFeature`**, **`Alley`**, **`Fence`**, **`FireplaceQU`** have a high ratio of missing values (also seen in the map). But, according to the data description, NA in all of these features means that there is no such feature in the house. For example, NA in PoolQC means that there is no pool in the house, NA in alley means that there is no alley access etc. Therefore, we impute these values by filling **None**.","f104ce9e":"**Principal Component Analysis**\n\nTrying to reduce the dimensionality of the data to see if it helps in our predictions.","94ee2d7e":"### Submission","2f9e9668":"#### Feature Engineering","3c2ed603":"For all other features with less missing ratio, we also impute the missing values by filling it with **None**.","9e9ed207":"#### Checking out the features present in the data","32d5d715":"Splitting the concatenated data back to train and test set to start our modelling.","0a4582df":"Any feature which is related to 'area' is important in determing the house prices. We add a new feature for obtaining the 'total area' using TotalBsmtSF, 1stFlrSF, 2ndFlrSF. ","7de1157b":"Let's assign dummy variables to categorical features for modelling purposes(eg: Linear Regression requires the creation of dummy variables for modelling)","44ebf5e7":"**Prediction time!**","1d87173c":"#### LASSO\n\nLASSO requires to know the value of alpha. Alpha can be obtained through cross validation ","fbeecba6":"#### Linear Regression","aaca2113":"#### Transforming 'SalePrice' to make it normally distributed.\n\nUsually, log transforms help.","48253d77":"Concatenating the train and test data to a single dataframe - This step helps us to impute the missing values.","5ac08684":"**`LotFrontage`** is the linear feet of street connected to the property. This means that the area of street connected to each house will mostly be similar and hence houses in the neighbourhood also tend to have the same area of street. We can impute the missing values in this feature by **median LotFrontage** of the neighbourhood.","4e3fb9cb":"From the above histogram, 'SalePrice' is deviated and right skewed from normal distribution. According to the Central Limit Theorem, all the RV's tends to be normally distributed and hence we require the response variable to be transformed into a normally distributed variable.                           \nQQ-plot also shows that the response variable is not normally distributed where the data does not follow along the line that is normally distributed. ","80b15522":"### Exploratory Data Analysis","1336e1f1":"Skewness and kurtosis have significantly reduced.","40505a62":"#### Skewed Features\n\nAfter exploring the skewness in our response variable, now, exploring the skewness in the numerical features.","f0a8bdec":"#### Looking at how the variable is distributed, by plotting a histogram and QQ-Plot","29da29ba":"#### Adaptive Boosting with LASSO as the estimator","8b1a6a83":"#### Handling Missing values","26aca931":"The given training data contains 1460 rows and 81 columns. The first thing is to get the data in the right format before diving into predicting the models. My analysis involves the following steps:\n1. Understanding the response\/dependent variable - `SalePrice`\n2. Understanding the relation between the dependent varaible and independent variables.\n3. Cleaning the data - handling missing values, oultiers and categorical variables.","5215d2db":"#### Gradient Boosting","784eb199":"While analysing the data set, I found that there are quite a few outliers. Since, we have very less data (around 1460 rows), removing outliers with respect to all the features will reduce the data for our analysis. I am considering 'Ground Living Area' predictors outliers. So, just removing a few extremely affecting outliers is safe because removing all of them may greatly affect the models if there were also outliers in the test data. ","06b10040":"#### A little more feature engineering","b90d2d31":"Transforming the highly skewed features to make it Gaussian using Box-Cox transformation. ","209cbb07":"**Exploring outliers!**","00342586":"#### Elastic Net : Hybrid of LASSO and Ridge Regression","55d57164":"#### Ridge Regression","ea79d569":"This is my first kaggle competition and a stepping stone for my career in the field of Data Science!                      \nI have taken inspiration from [Stacked Regressions to predict House prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook). \n","199cab9b":"#### Supress the warnings that we get from sklearn and seaborn","4cf27a2b":"#### Importing the data from the .csv file","de97b306":"There are two outliers in the first subplot. They have an unconvincing low price but with larger GrLivArea and they do not follow the trend of the data, hence we can safely remove them.                    \nAlso, there are two more extreme point over $600000 which are not outliers, they are following the trend of the data, hence we retain them.","3f12af47":"For features that are quantitative, we fill the missing values with **0**. For example, missing values in GarageCars means there are no cars in the garage and hence we impute it with 0.","2aee268d":"#### Understanding the statistics of the response\/dependent variable - `'SalePrice'`","4ec25d5d":"For feautures **`MSZoning`**, **`Electrical`**, **`KitchenQual`**, **`Exterior1st`**, **`Exterior2nd`**, **`SaleType`**, we fill the missing values with the **mode** of the predictors as these features have values that occur most frequently. ","78d3edf9":"Calculating the missing ratio to see which features have a greater percentage of missing data.","55e43347":"PCA also doesn't help because the RMSE has increased after the PCA reduction when compared to before reduction. So, we choose Adaptive boosting with Elastic Net as the estimator and predict the prices.","c1195ee6":"#### Adaptive Boosting with Elastic Net as the estimator","87695c5c":"After applying all the models to our data, Adaptive Boosting with Elastic net as the estimator seems to give a better rmse when compared to all other models. ","2a47eded":"#### Categorical Variables","547127c3":"We notice that two of the numerical features are actually categorical. Let's transform them to categorical type.","85ce5b55":"Missing ratio is 0 for all the features. Hence, we can confirm that we do not have missing values in the data.","b51aeb4c":"#### Random Forest","7302a47a":"All our predictors are in 'X_train' and response is in 'y'.","161bf869":"#### Import Libraries","17e23686":"Checking to see if there are missing values in any other features.","86730072":"In **`Functional`** feature, we have 2 NA's. NA means 'Typical' according to the data description and hence we fill them with **Typ** and we drop the **`Utilities`** feature as it has all its values as 'AllPub' and is of no use to us.","47eefaec":"#### Plotting the transformed variable - Now, it should be normally distributed"}}