{"cell_type":{"5110c66b":"code","39e9d2e2":"code","9fbe38d3":"code","9f1a6f74":"code","384ba911":"code","ab8d60de":"markdown","bad949a1":"markdown","176875b5":"markdown","6901ad26":"markdown","6069aad9":"markdown","e1330eaf":"markdown"},"source":{"5110c66b":"!pip install -i https:\/\/test.pypi.org\/simple\/  litemort==0.1.7\nfrom LiteMORT import *\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport time\nimport sys\nimport gc\nimport pickle\nimport random\nimport os\n\n#from bayes_opt import BayesianOptimization\n","39e9d2e2":"#isMORT = len(sys.argv)>1 and sys.argv[1] == \"mort\"\nisMORT = True\nalg='MORT' if isMORT else 'XGB'\nprint(f\"gradient boosting lib={alg}\")\n","9fbe38d3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\n\n#some_rows = 5000\nsome_rows = None\ndata_root = '..\/input\/'\n#data_root = \"~\/Datasets\/future_sales\"\n\ntest  = pd.read_csv(f'{data_root}\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')\ndata = pd.read_pickle(f'{data_root}\/predict-future-sales\/data.pkl')\nif some_rows is not None:\n    nMost=data.shape[0]\n    random.seed(42)\n    subset = random.sample(range(nMost), some_rows)\n    data = data.iloc[subset, :].reset_index(drop=True)\n    print('====== Some Samples ... data={}'.format(data.shape))\n\ndata = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    #'date_shop_type_avg_item_cnt_lag_1',\n    #'date_shop_subtype_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    #'date_type_avg_item_cnt_lag_1',\n    #'date_subtype_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month',\n    'days',\n    'item_shop_last_sale',\n    'item_last_sale',\n    'item_shop_first_sale',\n    'item_first_sale',\n]]\n\nX_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\nprint(f\"X_train={X_train.shape} Y_train={Y_train.shape}\")\nprint(f\"X_valid={X_valid.shape} Y_valid={Y_valid.shape}\")\nprint(f\"X_test={X_test.shape} \")\ndel data\ngc.collect();","9f1a6f74":"params={'num_leaves': 550,   'n_estimators':1000,'early_stopping_rounds':20,\n        'feature_fraction': 1,     'bagging_fraction': 1,\n        'max_bin': 512,\n      # \"adaptive\":'weight1',\n    #\"learning_schedule\":\"adaptive\",\n     'max_depth': 10,\n     'min_child_weight': 300,    #'min_data_in_leaf': 300,\n     'learning_rate': 0.1,\n     'objective': 'regression',\n     'boosting_type': 'gbdt',\n     'verbose': 1,\n     'metric': {'rmse'}\n}\n\nif isMORT:\n    print(f\"Call LiteMORT... \")    \n    t0=time.time()\n    model = LiteMORT(params).fit(X_train,Y_train,eval_set=[(X_valid, Y_valid)])\n    print(f\"LiteMORT......OK time={time.time()-t0:.4g} model={model}\")\nelse:\n    model = XGBRegressor(\n        max_depth=8,\n        n_estimators=1000,\n        min_child_weight=300,\n        colsample_bytree=0.8,\n        subsample=0.8,\n        eta=0.3,\n        seed=42)\n\n    model.fit(\n        X_train,\n        Y_train,\n        eval_metric=\"rmse\",\n        eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n        verbose=True,\n        early_stopping_rounds = 10)\n    alg = 'xgboost'\n\nY_pred = model.predict(X_valid).clip(0, 20)\nscore = np.sqrt(mean_squared_error(Y_pred, Y_valid))\nY_test = model.predict(X_test).clip(0, 20)\nprint(f\"score={score}\")\n\ndef plot_features(booster, figsize):\n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nif not isMORT:\n    plot_features(model, (10, 14))\n\n","384ba911":"submission = pd.DataFrame({\n        \"ID\": test.index,\n        \"item_cnt_month\": Y_test\n    })\npath = f'submission.csv'\n#print(f\"submission......path={path}......\")\nsubmission.to_csv(path, index=False)\nprint(f\"......Save submit @{path}......\")\n","ab8d60de":"LiteMORT is a new open source gradient boosting lib( https:\/\/github.com\/closest-git\/LiteMORT).  \nIn this kernel, it's much faster than XGBoost with a little higher accuracy. (The LB of this notebook is 0.9058)\n","bad949a1":"Step 3 Set the parameter of litemort and testing.  \nLiteMORT use sklearn-like api interface.\nThe meaning of these parameters are similar to that of lightGBM or XGBOOST","176875b5":"Step 4 Get result  \nThe mse error in valid dataset is 0.9025 and the LB is 0.9058","6901ad26":"Step 1: Install litemort and import.  \n","6069aad9":"Step 2 Load data.  \nSome codes and the \"data.pkl\" are forked from https:\/\/www.kaggle.com\/dhimananubhav\/feature-engineering-xgboost.  \nFor the detail of feature engineering, please visit that notebook.","e1330eaf":"If \"*isMORT*\" is true, we will call litemort, otherwise XGBoost"}}