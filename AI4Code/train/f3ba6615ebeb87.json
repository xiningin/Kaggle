{"cell_type":{"6b81fdab":"code","c5f9a0f2":"code","0da3af03":"code","71b28754":"code","848b8446":"code","2934b51d":"code","0e7386a4":"code","29325f84":"code","20037865":"code","ad691372":"code","c8ea8b42":"code","77f66850":"code","66db9a27":"code","9b8f46c3":"code","2e1af0d6":"code","1eee3386":"code","d36b05ea":"code","713f1084":"code","f81a25ae":"code","12f59f03":"code","d33784ea":"code","f9b5c57e":"code","e5e0c535":"code","2069d44c":"code","e04f3e2f":"code","dae3b20c":"code","52e94a82":"code","9575076c":"code","96ab30c9":"code","45989ad1":"code","8b789bd5":"code","278fa6e7":"code","22467e97":"code","3798257c":"code","1dc374a2":"code","483f1fbc":"code","c957123a":"code","2e16219c":"code","9a861b69":"code","6781cd2d":"code","7c789df3":"code","4939701c":"code","325759c4":"code","6975ca21":"code","15fe6f80":"code","f9e4a109":"code","5b1ee931":"code","eba2abd0":"code","ee0789b0":"code","27d96516":"code","1111023e":"code","9984a4be":"code","39fa69fa":"markdown","f9a4f92c":"markdown"},"source":{"6b81fdab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5f9a0f2":"# Importing the libraries\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.offline as pyoff\nimport plotly.graph_objs as go\nimport nltk\nfrom collections import Counter\n\nfrom plotly import graph_objs as go\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport re\nimport nltk\nimport gensim\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout, Bidirectional, Conv2D\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf\nimport transformers\nfrom tokenizers import BertWordPieceTokenizer\nfrom keras.layers import LSTM,Dense,Bidirectional,Input\nfrom keras.models import Model\nimport torch\nimport transformers","0da3af03":"df = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin-1')\n#test = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv')\n\ndf.head()","71b28754":"sns.countplot(df['Sentiment'])","848b8446":"df.info()","2934b51d":"df['Location'].isna().sum()","0e7386a4":"location = df['Location'].value_counts().nlargest(n=15)\n\nfig = px.bar(y=location.values,\n       x=location.index,\n       orientation='v',\n       color=location.index,\n       text=location.values,\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Location\",\n                  yaxis_title=\"Count\",\n                  title=\"Top 15 Locations with tweet count\")\nfig.show()","29325f84":"# Get all hashtags\n\ndef extract_hash_tags(s):\n    hashes = re.findall(r\"#(\\w+)\", s)\n    return \" \".join(hashes)\ndf['hashtags'] = df['OriginalTweet'].apply(lambda x : extract_hash_tags(x))","20037865":"allHashTags = list(df[(df['hashtags'] != None) & (df['hashtags'] != \"\")]['hashtags'])\nallHashTags = [tag.lower() for tag in allHashTags]\nhash_df = dict(Counter(allHashTags))\ntop_hash_df = pd.DataFrame(list(hash_df.items()),columns = ['word','count']).reset_index(drop=True).sort_values('count',ascending=False)[:20]\ntop_hash_df.head()","ad691372":"fig = px.bar(x=top_hash_df['word'],y=top_hash_df['count'],\n       orientation='v',\n       color=top_hash_df['word'],\n       text=top_hash_df['count'],\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Word\",\n                  yaxis_title=\"Count\",\n                  title=\"Top #hashtags in Covid19 Tweets\")\nfig.show()","c8ea8b42":"# Get all mentions\n\ndef get_mentions(s):\n    mentions = re.findall(\"(?<![@\\w])@(\\w{1,25})\", s)\n    return \" \".join(mentions)\ndf['mentions'] = df['OriginalTweet'].apply(lambda x : get_mentions(x))","77f66850":"df['OriginalTweet'][0]","66db9a27":"df['mentions'][0]","9b8f46c3":"allMentions = list(df[(df['mentions'] != None) & (df['mentions'] != \"\")]['mentions'])\nallMentions = [tag.lower() for tag in allMentions]\nmentions_df = dict(Counter(allMentions))\ntop_mentions_df = pd.DataFrame(list(mentions_df.items()),columns = ['word','count']).reset_index(drop=True).sort_values('count',ascending=False)[:20]\ntop_mentions_df.head()","2e1af0d6":"fig = px.bar(x=top_mentions_df['word'],y=top_mentions_df['count'],\n       orientation='v',\n       color=top_mentions_df['word'],\n       text=top_mentions_df['count'],\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Word\",\n                  yaxis_title=\"Count\",\n                  title=\"Top #hashtags in Covid19 Tweets\")\nfig.show()","1eee3386":"# Data Cleaning\nstop = set(stopwords.words('english'))\n\ndef cleaner(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can't\", 'can not', phrase)\n  \n  # general\n    phrase = re.sub(r\"n\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'re'\",\" are\", phrase)\n    phrase = re.sub(r\"\\'s\",\" is\", phrase)\n    phrase = re.sub(r\"\\'ll\",\" will\", phrase)\n    phrase = re.sub(r\"\\'d\",\" would\", phrase)\n    phrase = re.sub(r\"\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'ve\",\" have\", phrase)\n    phrase = re.sub(r\"\\'m\",\" am\", phrase)\n    \n    return phrase\n\ncleaned_title = []\n\nfor sentance in tqdm(df['OriginalTweet'].values):\n    sentance = str(sentance)\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = cleaner(sentance)\n    sentance = re.sub(r'[?|!|\\'|\"|#|+]', r'', sentance)\n    sentance = re.sub('<.*?>','',sentance)\n    sentance = re.sub(r'@\\w+','',sentance)\n    sentance = re.sub(r'#\\w+','',sentance)\n    sentance = re.sub(r'[0-9]+','',sentance)\n    sentance = re.sub(r'[0-9]+','',sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop)\n    cleaned_title.append(sentance.strip())\n    \ndf['text'] = cleaned_title\ndf.head()","d36b05ea":"# WordClouds\n# Text that is displaying a positive sentiment\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Sentiment == 'Positive'].text))\nplt.imshow(wc , interpolation = 'bilinear')","713f1084":"# Text that is displaying a negative sentiment\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Sentiment == 'Negative'].text))\nplt.imshow(wc , interpolation = 'bilinear')","f81a25ae":"# Text that is displaying a neutral sentiment\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Sentiment == 'Neutral'].text))\nplt.imshow(wc , interpolation = 'bilinear')","12f59f03":"# Continuing with some n-gram analysis\n\ndef basic_clean(text):\n  \"\"\"\n  A simple function to clean up the data. All the words that\n  are not designated as a stop word is then lemmatized after\n  encoding and basic regex parsing are performed.\n  \"\"\"\n  wnl = nltk.stem.WordNetLemmatizer()\n  stopwords = nltk.corpus.stopwords.words('english')\n  text = (unicodedata.normalize('NFKD', text)\n    .encode('ascii', 'ignore')\n    .decode('utf-8', 'ignore')\n    .lower())\n  words = re.sub(r'[^\\w\\s]', '', text).split()\n  return [wnl.lemmatize(word) for word in words if word not in stopwords]","d33784ea":"# Uni-grams for Tweets\n\nHQ_words = basic_clean(''.join(str(df['text'].tolist())))\nunigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 1)).value_counts())[:20]\nunigram_HQ = pd.DataFrame(unigram_HQ)\nunigram_HQ['idx'] = unigram_HQ.index\nunigram_HQ['idx'] = unigram_HQ.apply(lambda x: '('+x['idx'][0]+')',axis=1)","f9b5c57e":"plot_data = [\n    go.Bar(\n        x=unigram_HQ['idx'],\n        y=unigram_HQ[0],\n        marker = dict(\n            color = 'Blue'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 20 uni-grams from Covid-19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='Uni-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n","e5e0c535":"# Bi-grams for Tweets\n\nHQ_words = basic_clean(''.join(str(df[df['Sentiment']=='Negative']['text'].tolist())))\nbigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 2)).value_counts())[:20]\nbigram_HQ = pd.DataFrame(bigram_HQ)\nbigram_HQ['idx'] = bigram_HQ.index\nbigram_HQ['idx'] = bigram_HQ.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+')',axis=1)","2069d44c":"plot_data = [\n    go.Bar(\n        x=bigram_HQ['idx'],\n        y=bigram_HQ[0],\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 20 bi-grams from Covid 19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","e04f3e2f":"# Tri-grams for Tweets\n\nHQ_words = basic_clean(''.join(str(df['text'].tolist())))\ntrigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 3)).value_counts())[:20]\ntrigram_HQ = pd.DataFrame(trigram_HQ)\ntrigram_HQ['idx'] = trigram_HQ.index\ntrigram_HQ['idx'] = trigram_HQ.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+', '+x['idx'][2]+')',axis=1)","dae3b20c":"plot_data = [\n    go.Bar(\n        x=trigram_HQ['idx'],\n        y=trigram_HQ[0],\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 20 Tri-grams from Covid 19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='Tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","52e94a82":"test = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='latin-1')\ntest.head()","9575076c":"cleaned_title = []\n\nfor sentance in tqdm(test['OriginalTweet'].values):\n    sentance = str(sentance)\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = cleaner(sentance)\n    sentance = re.sub(r'[?|!|\\'|\"|#|+]', r'', sentance)\n    sentance = re.sub('<.*?>','',sentance)\n    sentance = re.sub(r'@\\w+','',sentance)\n    sentance = re.sub(r'#\\w+','',sentance)\n    sentance = re.sub(r'[0-9]+','',sentance)\n    sentance = re.sub(r'[0-9]+','',sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop)\n    cleaned_title.append(sentance.strip())\n    \ntest['text'] = cleaned_title\ntest.head()","96ab30c9":"df['text'].replace('', np.nan, inplace=True)\ndf.dropna(subset=['text'], inplace=True)\n\ndf.head()","45989ad1":"df.columns","8b789bd5":"train = df.copy()\ntrain.drop(['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'hashtags', 'mentions'], axis=1, inplace=True)","278fa6e7":"train.head()","22467e97":"X = train.text","3798257c":"def target(label):\n    if label == 'Neutral': \n        return 0\n    if label == 'Positive' or label=='Extremely Positive':\n        return 1\n    else:\n        return -1","1dc374a2":"train['label'] = train['Sentiment'].apply(target)\ntrain.head()","483f1fbc":"X = train['text']\ny = train['label']","c957123a":"from sklearn.model_selection import train_test_split\nX_Train, X_test, y_Train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify = y)\n\nX_train, X_cross, y_train, y_cross = train_test_split(X_Train, y_Train, test_size=0.1, random_state=42, stratify = y_Train)","2e16219c":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntf_idf=TfidfVectorizer(use_idf=True,ngram_range=(1,2))\n\ntf_idf.fit(X_train)\nTrain_TFIDF = tf_idf.transform(X_train)\nCrossVal_TFIDF = tf_idf.transform(X_cross)\nTest_TFIDF= tf_idf.transform(X_test)","9a861b69":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n\nc=[0.0001,0.001,0.01,0.1,1,10,100,1000]\nTrain_AUC_TFIDF = []\nCrossVal_AUC_TFIDF = []\nfor i in c:\n  logreg = LogisticRegression(C=i,penalty='l2')\n  logreg.fit(Train_TFIDF, y_train)\n  Train_y_pred =  logreg.predict_proba(Train_TFIDF)[0:,]\n  Train_AUC_TFIDF.append(roc_auc_score(y_train ,Train_y_pred, multi_class='ovr'))\n  CrossVal_y_pred =  logreg.predict_proba(CrossVal_TFIDF)[0:,]\n  CrossVal_AUC_TFIDF.append(roc_auc_score(y_cross,CrossVal_y_pred, multi_class='ovr'))","6781cd2d":"C=[]\nfor i in range(len(c)):\n  C.append(np.math.log(c[i]))\n\nplt.plot(C, Train_AUC_TFIDF, label='Train AUC')\nplt.scatter(C, Train_AUC_TFIDF)\nplt.plot(C, CrossVal_AUC_TFIDF, label='CrossVal AUC')\nplt.scatter(C, CrossVal_AUC_TFIDF)\nplt.legend()\nplt.xlabel(\"lambda : hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","7c789df3":"optimal_inverse_lambda=c[CrossVal_AUC_TFIDF.index(max(CrossVal_AUC_TFIDF))]\nprint(pow(optimal_inverse_lambda,-1))","4939701c":"Classifier=LogisticRegression(C=optimal_inverse_lambda,penalty='l2')\nClassifier.fit(Train_TFIDF, y_train)\n\nauc_train_tfidf = roc_auc_score(y_train,Classifier.predict_proba(Train_TFIDF)[0:,], multi_class='ovr')\nprint (\"AUC for Train set\", auc_train_tfidf)\n\nauc_test_tfidf = roc_auc_score(y_test,Classifier.predict_proba(Test_TFIDF)[0:,], multi_class='ovr')\nprint (\"AUC for Test set\",auc_test_tfidf)","325759c4":"y_pred = Classifier.predict(Test_TFIDF)\nprint('Confusion Matrix of Test Data')\nTest_mat=confusion_matrix(y_test, y_pred)\nprint (Test_mat)\n\nprint('Accuracy Score on test: ', accuracy_score(y_test, y_pred))","6975ca21":"from sklearn import metrics\nprint(metrics.classification_report(y_test, y_pred, target_names = ['Negative', 'Neutral', 'Positive']))","15fe6f80":"# LSTM","f9e4a109":"import numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nnumpy.random.seed(7)\nfrom keras.layers import SpatialDropout1D\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing import text","5b1ee931":"MAX_NB_WORDS = 50000\nMAX_SEQUENCE_LENGTH = 300\nEMBEDDING_DIM = 100\ntokenizer = text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(X.values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","eba2abd0":"x = tokenizer.texts_to_sequences(X.values)\nx = sequence.pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', x.shape)\n\nX_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.3,stratify=y)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","ee0789b0":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nY_train=np.array(Y_train)\n\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(Y_train)\nprint(Y_train)\n\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\nprint(onehot_encoded)\nY_train=onehot_encoded","27d96516":"Y_test=np.array(Y_test)\n\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(Y_test)\n#print(Y_test)\n\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\nprint(onehot_encoded)\nY_test=onehot_encoded","1111023e":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=x.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nepochs = 10\nbatch_size = 64\n\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","9984a4be":"accr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","39fa69fa":"### 2. Model Building","f9a4f92c":"### 1. Basic EDA and Data cleaning"}}