{"cell_type":{"491317db":"code","98cfcead":"code","8dc4c78a":"code","dc999a0c":"code","3d44d53c":"code","14dd9fb6":"code","03d99b73":"code","c2a452de":"code","a13df860":"code","4b0b8c0a":"code","7525a0cd":"code","6fd7bdf2":"code","e3734dec":"code","e8f68366":"code","cc806848":"code","852f3495":"code","138aa9c2":"code","584b5365":"code","67bddf43":"code","945ae448":"code","f14be5b4":"code","1017bf8c":"code","534493ad":"code","fa77a93f":"code","2967e97e":"code","9fe9f288":"code","e28c738f":"code","0471a821":"code","1ac509d5":"code","8948a82b":"code","0bfaea03":"code","dd8699f8":"code","0e6f463f":"code","2215c7f8":"code","3b3df765":"code","ce893842":"code","d721a553":"code","966e4f95":"code","68135472":"code","bd69b301":"code","71ecc028":"code","f070d554":"code","1b562093":"code","a62126bb":"code","a67b7718":"code","b2d61750":"code","bf959afc":"code","74f50665":"code","9c4480c1":"code","6c58ac66":"code","dd467b24":"code","fa5af25d":"code","86bfec39":"code","c3bb613f":"code","87d5ab1f":"code","3759117b":"code","8735553e":"code","918a8b3b":"code","7443317a":"markdown","2182f0bf":"markdown","cb62e7c9":"markdown","e1d42a52":"markdown","3fc1d8a7":"markdown","c34f9ea9":"markdown","bd5d4710":"markdown","3200706d":"markdown","710730f9":"markdown","95d8e8f4":"markdown","f4df2c96":"markdown","bfe93231":"markdown","e15094c5":"markdown","b6f9c6fe":"markdown","3d343445":"markdown","0203b5eb":"markdown","a9087831":"markdown","d53b01c2":"markdown","979997a5":"markdown","c12ffde0":"markdown","f7e98053":"markdown","27ea0821":"markdown","87652aba":"markdown","cb23d4bc":"markdown","3b51825b":"markdown","b02e0377":"markdown","463f2b6b":"markdown","d4673626":"markdown","abc60dcb":"markdown","b97bc120":"markdown","24ca14d4":"markdown","8064be7a":"markdown","47b61121":"markdown","be48de15":"markdown","fbe01e42":"markdown","1364b78d":"markdown","2b86083a":"markdown","1d092b90":"markdown","f11a50d4":"markdown","96556d0a":"markdown","bcbcb9bc":"markdown","f13acc31":"markdown","b53f36ee":"markdown","86a1ce9f":"markdown","71361d09":"markdown"},"source":{"491317db":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","98cfcead":"data = pd.read_csv('..\/input\/board-games-prediction-data\/games.csv')\nprint(data.shape)\ndata.head()","8dc4c78a":"data_explore = data.copy()","dc999a0c":"drop_features = ['id', 'type', 'name', 'bayes_average_rating']\ndata_explore = data_explore.drop(columns=drop_features, axis=1)","3d44d53c":"data_explore.info()","14dd9fb6":"data_explore.describe()","03d99b73":"def plot_histogram(data):\n    ax = plt.gca()\n    counts, _, patches = ax.hist(data)\n    for count, patch in zip(counts, patches):\n        if count>0:\n            ax.annotate(str(int(count)), xy=(patch.get_x(), patch.get_height()+5))\n    if data.name:\n        plt.xlabel(data.name)","c2a452de":"plt.figure(figsize=(15, 25))\ni=1\nfor col in data_explore.columns:\n    plt.subplot(6, 3, i)\n    plot_histogram(data_explore[col])\n    i+=1","a13df860":"plt.title('Histogram of Average Ratings')\nplot_histogram(data_explore['average_rating'])","4b0b8c0a":"data_explore_zero_ratings = data_explore[data_explore['average_rating']==0]\ndata_explore_zero_ratings.describe()","7525a0cd":"data_explore = data_explore[data_explore['average_rating']>0]\ndata = data[data['average_rating']>0] # making this change in orignal dataframe","6fd7bdf2":"plt.title('Histogram of Average Ratings')\nplot_histogram(data_explore['average_rating'])","e3734dec":"plt.title('Histogram of Average Weight')\nplot_histogram(data_explore['average_weight'])","e8f68366":"data_explore = data_explore[data_explore['average_weight']>0]\ndata = data[data['average_weight']>0] # making this change in orignal dataframe","cc806848":"plot_histogram(data_explore['yearpublished'])","852f3495":"data_explore = data_explore[data_explore['yearpublished']>0]\nplot_histogram(data_explore['yearpublished'])","138aa9c2":"data_explore_1920 = data_explore.query('yearpublished > 1900 and yearpublished < 2000')\nplot_histogram(data_explore_1920['yearpublished'])","584b5365":"data_explore = data_explore[data_explore['yearpublished']>1950]\ndata = data[data['yearpublished']>1950]\nplot_histogram(data_explore['yearpublished'])","67bddf43":"data_7585 = data_explore.query('yearpublished >= 1975 and yearpublished <= 1985')\ndata_0515 = data_explore.query('yearpublished >= 2005 and yearpublished <= 2015')\n\nplt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(data_7585['yearpublished'], data_7585['average_rating'], s=data_7585['average_weight']*10)\nplt.title(\"1975-85 (Games {})\".format(data_7585['yearpublished'].count()))\nplt.xlabel('Published year')\nplt.ylabel('Average rating')\nplt.subplot(1, 2, 2)\nplt.scatter(data_0515['yearpublished'], data_0515['average_rating'], s=data_0515['average_weight']*10)\nplt.title(\"2005-15 (Games {})\".format(data_0515['yearpublished'].count()))\nplt.xlabel('Published year')\nplt.show()","945ae448":"columns = ['minplaytime', 'maxplaytime', 'minplayers', 'maxplayers', 'users_rated']\nplt.figure(figsize=(15, 8))\nsns.boxplot(data=data_explore[columns])\nplt.ylim(-100, 500)","f14be5b4":"Q1 = data_explore.quantile(0.25)\nQ3 = data_explore.quantile(0.75)\nIQR = Q3 - Q1\n((data_explore < (Q1 - 1.5 * IQR)) | (data_explore > (Q3 + 1.5 * IQR))).sum()","1017bf8c":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')","534493ad":"data_columns = data_explore.columns\ndata_explore = imputer.fit_transform(data_explore)\ndata_explore = pd.DataFrame(data=data_explore, columns=data_columns)","fa77a93f":"drop_features.append('playingtime')\ndata_explore = data_explore.drop(columns=['playingtime'], axis=1)","2967e97e":"plt.figure(figsize=(15, 10))\ncorr_matrix = data_explore.corr()\nsns.heatmap(corr_matrix, mask=np.zeros_like(corr_matrix, dtype=np.bool), square=True, annot=True)","9fe9f288":"corr_matrix['average_rating'].sort_values(ascending=False)","e28c738f":"data.shape","0471a821":"y = data[['average_rating']].copy()\nX = data.drop(columns=['average_rating'], axis=1)","1ac509d5":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","8948a82b":"feature_columns =[ feature for feature in list(X.columns) if feature not in drop_features ]","0bfaea03":"from sklearn.compose import ColumnTransformer\n\ndrop_feature_cols = ColumnTransformer(transformers=[('drop_columns', 'drop', drop_features)], remainder='passthrough')","dd8699f8":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","0e6f463f":"pre_process = Pipeline(steps=[('drop_features', drop_feature_cols),\n                              ('imputer', SimpleImputer(strategy=\"median\")),\n                              ('scaler', StandardScaler())])","2215c7f8":"X_train_transformed = pre_process.fit_transform(X_train)\nX_test_transformed = pre_process.transform(X_test)","3b3df765":"from sklearn.model_selection import cross_val_score\n\ndef cv_results(model, X, y):\n    scores = cross_val_score(model, X, y, cv = 7, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n    rmse_scores = np.sqrt(-scores)\n    print('CV Scores: ', rmse_scores)\n    print('rmse: {},  S.D.:{} '.format(np.mean(rmse_scores), np.std(rmse_scores)))","ce893842":"from sklearn.linear_model import LinearRegression\n\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train_transformed, y_train)","d721a553":"coefs = list(zip(feature_columns, linear_reg.coef_[0]))\ncoefs.sort(key= lambda x:x[1], reverse=True)\ncoefs","966e4f95":"print(\"Linear Regression Model Cross Validation Results\")\ncv_results(linear_reg, X_train_transformed, y_train)","68135472":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95)   # Keeping variance 95% so that we will not loose much information.\nX_train_reduced = pca.fit_transform(X_train_transformed)\nX_test_reduced = pca.transform(X_test_transformed)\npca.n_components_, X_train_reduced.shape[1]","bd69b301":"linear_reg.fit(X_train_reduced, y_train)","71ecc028":"print(\"Linear Regression Model Cross Validation Results\")\ncv_results(linear_reg, X_train_reduced, y_train)","f070d554":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor(criterion='mse', random_state=42)\ntree_reg.fit(X_train_transformed, y_train)","1b562093":"coefs = list(zip(feature_columns, tree_reg.feature_importances_))\ncoefs.sort(key= lambda x:x[1], reverse=True)\ncoefs","a62126bb":"print(\"Decision Tree Regression Model Cross Validation Results\")\ncv_results(tree_reg, X_train_transformed, y_train)","a67b7718":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor(criterion='mse', random_state=42, n_jobs=-1)\nforest_reg.fit(X_train_transformed, y_train.values.flatten())","b2d61750":"coefs = list(zip(feature_columns, forest_reg.feature_importances_))\ncoefs.sort(key= lambda x:x[1], reverse=True)\ncoefs","bf959afc":"print(\"Random Forest Regression Model Cross Validation Results\")\ncv_results(forest_reg, X_train_transformed, y_train.values.flatten())","74f50665":"from sklearn.model_selection import GridSearchCV\n\ngrid_parm=[{'n_estimators':[25, 50, 75, 100], 'max_depth':[4, 8, 12, 16]}]\ngrid_search = GridSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1), grid_parm, cv=5, scoring=\"neg_mean_squared_error\", return_train_score=True, n_jobs=-1)\ngrid_search.fit(X_train_transformed, y_train.values.flatten())","9c4480c1":"cvres = grid_search.cv_results_\nprint(\"Results for each run of Random Forest Regression...\")\nfor train_mean_score, test_mean_score, params in zip(cvres[\"mean_train_score\"], cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-train_mean_score), np.sqrt(-test_mean_score), params)","6c58ac66":"grid_search.best_params_, -grid_search.best_score_","dd467b24":"best_forest_reg = grid_search.best_estimator_\nbest_forest_reg.max_depth=12\nbest_forest_reg","fa5af25d":"print(\"Best Random Forest Cross Validation Results\")\ncv_results(best_forest_reg, X_test_transformed, y_test)","86bfec39":"y_train_pred = best_forest_reg.predict(X_train_transformed)\ny_test_pred = best_forest_reg.predict(X_test_transformed)","c3bb613f":"y_pred = np.concatenate((y_train_pred, y_test_pred), axis=0)\ny_pred.shape","87d5ab1f":"plt.figure(figsize=(15, 6))\nplt.subplot(1, 2, 1)\nplt.title('Histogram of Observed Average Ratings')\nplt.hist(data['average_rating'], bins=np.arange(1, 10), rwidth=0.85)\nplt.subplot(1, 2, 2)\nplt.title('Histogram of Predicted Average Ratings')\nplt.hist(y_pred, bins=np.arange(1, 10), rwidth=0.85)\nplt.show()","3759117b":"combine_data = pd.concat([X_train, X_test], axis=0)","8735553e":"plt.figure(figsize=(15, 6))\nplt.subplot(1, 2, 1)\nplt.scatter(data['yearpublished'], data['average_rating'],  c='green')\nplt.title('Distibution of Observed Average Rating')\nplt.subplot(1, 2, 2)\nplt.scatter(combine_data['yearpublished'], y_pred, c='red')\nplt.title('Distibution of Predicted Average Rating')\nplt.show()","918a8b3b":"plt.figure(figsize=(15, 6))\nplt.subplot(1, 2, 1)\nplt.scatter(data['average_weight'], data['average_rating'],  c='green')\nplt.title('Distibution of Observed Average Rating')\nplt.ylabel('Average Rating')\nplt.xlabel('Average Weight')\nplt.subplot(1, 2, 2)\nplt.scatter(combine_data['average_weight'], y_pred, c='red')\nplt.title('Distibution of Predicted Average Rating')\nplt.ylabel('Average Rating')\nplt.xlabel('Average Weight')\nplt.show()","7443317a":"- Decision Tree has given more importance to the total_wisher and yearpublished features than average_weight and total_wanters.\n- This is completely different from what we recieved from Linear Regression. Linear Regressor model has given very less weights to total_wishers and yearpublished.\n- This model has give more importance to total wishers and year in which game has been published.","2182f0bf":"We are going to build a pipeline which will take care of the data cleaning and model training.\n\nIn data cleaning process we will focus on following aspects of data:\n1. drop unnecessary features\n2. replace null values\n3. standardization of features","cb62e7c9":"## Step 6: Model Evaluation","e1d42a52":"In data exploration step we found out two key facts about the dataset:\n* Many features are correlated with each other.\n* Many feature contains outliers.\n\n\n- Having correlated features might affect performance of linear model but for tree-based models correlated feature is not a concern.\n- I will use following machine learning algorithms:\n    * Linear Regression\n    * Decision Tree\n    * Random Forest\n   \n   \n- RMSE will the performance metric to evaluate model's performance.","3fc1d8a7":"Now lets shift our focus on tree-based models. We will first implement Decision tree and then see if we get any improvement in result by implementing Random Forest.","c34f9ea9":"- We can see there any outliers in each of above features. Lets calculate total number of outliers present in each feature column.","bd5d4710":"Now lets see how features are correlated with average rating. ","3200706d":"## Step 5: Fine Tune a Model","710730f9":"## Step 3: Data Preprocessing","95d8e8f4":"## Step 1: Frame the problem","f4df2c96":"Again there not so many games prior to middle of 19th century. ","bfe93231":"## Step 2: Data Exploration","e15094c5":"We also saw that for some games 'year of published' is negative.","b6f9c6fe":"In orignal dataset we had more than 80000 records but because of many invalid values for some attributes we had to remove those records.","3d343445":"- As we can see, each column has many outliers present. \n- Since the data is collected by using web-scrapping, there is possibility that the outliers can be the results of mistake made in data-collection process\n- For now, we will not drop the outliers. Outliers not necessarily affect the models performance.\n- Since there are outliers, we will replace the null values by median.","0203b5eb":"- Observations:\n    - Minimum value of average rating and average weight is zero which is invalid as per the information given on BGG website. The expected minimum value for both these features should be 1. \n    - Minimum value in yearpublished column is negative indicating there are games with invalid value of published year.\n    - Minimum value in users_rated column indicates that there are games for which no user has given rating.\n    - Statistics of playingtime and maxplaytime columns are same. These are identical columns.\n    - There are games with zero maximum playing time, zero minimum and\/or maximum players required. These can be consider as invalid records.\n    - There is no categorical feature present in this dataset.\n    - By comparing max and 75% values of columns such as maxplaytime, minplaytime, users_rated maxplayers, minplayers etc. which tells us there are some outliers present in the dataset.\n    \nLet first focus on average rating.","a9087831":"Unfortunately we didn't got any improvement in results. The only improvement we got is reduced in training time, which not our concern for this problem.\n\nWhat effect the multicollinearity has on the model performance is well descibed on following link:\n<br>https:\/\/datascience.stackexchange.com\/questions\/24452\/in-supervised-learning-why-is-it-bad-to-have-correlated-features\n<br>\n\n- Few important learnings:\n    - In case of supervised learning for predictions, the only reason for removing the multicollinearity is improve the training time and reduce the storage.\n    - If we add so much correlated features to the model we may cause the model to consider unnecessary features and we may have curse of high dimensionality problem.\n    - Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don\u2019t need to understand the role of each independent variable, you don\u2019t need to reduce severe multicollinearity.","d53b01c2":"- Objective: Predict the average rating a board game will receive based on certain aspects such as number of participants in game, difficulty, playing time etc.\n\t\t\n- In current world we have access to wide variety of products from many possible domains. Be it shopping, watching movies or playing games we can do all these things online. With ability for consumer to share his\/her reviews about the product or service he\/she receives has made massive influence on shopping behaviours of consumers.\n\n- Products with very good rating high chances of ranking higher up in the Top product list. Having many positive reviews about product will give boost to the sell of product by attracting new consumers. Postive reviews gives customer a trust about quality of the product. But on same line, few negative reviews will repel the consumer from product. The major benifit of review or rating system can be seen on decline in malfunctioning.\n\n- On BoradGameGeek website, from where the data has been collected for this problem, ratings of games plays very crucial role in attracting new players towards those high rated game. The rating of games are on scale of 1 to 10 with 1 being 'Awful' and 10 being 'Outstanding'. For more information about the ratings, visit https:\/\/boardgamegeek.com\/wiki\/page\/Ratings&redirectedfrom=rating#\n\n\n- Data Description:\nThere are total 20 features columns for each game.\nFollowing are the features associated with each game:\n    - type: Type of game\n    - name: Name of game\n    - yearpublished: Year when game is published\n    - minplayers & maxplayers: Minimum and maximum number of players allowed to particiate in the game\n    - playingtime: Allowed playing time (maximum)\n    - minplayingtime & maxplayingtime: Minimum and maximum allowed playing time\n    - minage: Minimum age of player required to play\n    - users_rated: Total number of users given rating to the game\n    - average_rating: Avergae rating for game\n    - bayes_average_rating: Bayesian average rating for game\n    - total_owners: Total number of players who own this game\n    - total_traders: Total number of players who want to trade this game\n    - total_wanters: Total number of players who wants this game in trade\n    - total_wishers: Total number of players who added this game in their wishlist\n    - total_comments: Total number of comments\n    - total_weights: Total number of people given Game Play Weight to this game\n    - average_weight: Average weight of this game\n\nImportant Terms:\n- Rating: range 1-10. For more info: https:\/\/boardgamegeek.com\/wiki\/page\/Ratings&redirectedfrom=rating#\n- Game Play Weight: range 1-5. Community rating for how difficult a game is to understand. Lower rating (lighter weight) means easier. For more info: https:\/\/boardgamegeek.com\/wiki\/page\/Weight","979997a5":"- Observations:\n    - Average rating is correlated with complexity of game and year in which game was published. Average rating is less correlated with the number of users given rating to the game. I feel this less correlation is good indicator because the ratings should be more depend on what users think about game rather than how many users rate the game.\n    - Average rating is more correlated with number of people who want the game in trade rather than number of people who owns the game. \n    - Rating of game is clearly independend of number of players and playing time of game.\n    - From the correlation plot, we can see that there are many features having fairly strong correlation with other features.","c12ffde0":"Variance of 2 in the prediciton of rating is not very good. \nCan we reduce the variance by reducing the correlation among feature variables.\n\nLets apply PCA technique to remove the correlation among input features.","f7e98053":"### Random Forest","27ea0821":"# BGG Average Rating Prediction","87652aba":"## Step 7: Analysis of Model Performance","cb23d4bc":"We also saw earlier that the playingtime column is same as maxplaying time. So we will drop the playingtime column.","3b51825b":"Lets now get ready with training and testing datasets.","b02e0377":"We expect that the features which are strongly correlated with target variable to have high coefficient value compare to others.\n\nSome interasting facts: \n- The model has given high weight to 'total_owners' feature than 'average_weight' and 'total_wanters'. Even though the latter two are more correlated with average rating than total_owners.\n- Very less weight is given to 'users_rated' feature compare to other features like maxplaytime, minplaytime, etc.\n    \nThis indicates Linear Regression gives much importance to number of people who owns the game rather than the people who wants this game in trade for prediction of average rating.","463f2b6b":"There are almost 24000 records with zero average ratings.  According to BoardGameGeek, the minimum rating any game can receive is 1. So definately the records of games with zero average rating are not going to any useful for us.\n\nLets explore more about these games with zero ratings.","d4673626":"## Step 4: Select and Train a model","abc60dcb":"Above stats tells us that the game which receives zero average rating because there are no users who have given ratings to those games, there are hardly 1 to 2 users who owns or wants this game.\n\nLets get rid of those games with average rating equal to zero.","b97bc120":"This is even worse than Linear Regression. Lets see what we get from Random Forest.","24ca14d4":"We can see that for most of games, year of published is above 1500.","8064be7a":"- Average ratings for game will never be relied on id, type and name of game. So we will drop this column from dataset.\n- Bayes average rating is another form of rating which is used to ensure that highly rated minority interest games are ranked lower than highly rated mass interest games. We should not use this column for making predictions.","47b61121":"### Decision Tree Regression","be48de15":"- From above charts we can see that our model has failed to correctly predict the average ratings for games having\n    - average rating less than 4.\n    - average rating higher than 8.\n\n- In given dataset there were many games published after 1990 having average rating less than 3. Our model has failed predict to predict the correct rating for those games.\n\n- In orignal dataset, for some games having high average weight has low ratings. But looking at the predictions made model, there are very few games with high average weight has received low ratings.","fbe01e42":"We also observe that there are games having average weight equal to zero. We should get rid of those records also.","1364b78d":"- Following columns contain null values: yearpublished, minplayers, maxplayers, playingtime, minplaytime, maxplaytime, minage","2b86083a":"### Linear Regression","1d092b90":"Before saving the model, lets observe predictions made by model on overall dataset. This analysis will help us to know where actually model has underperformed.","f11a50d4":"Much better result. Now lets tune the Random Forest model to obtain best model which we can use as our final model.","96556d0a":"Have a look at both charts for rating above 7. There are many games in 2015 which has more than 8 ratings. There where hardly few games which receives average rating higher than 7. \n\nNote: The size of circle indicates the complexity of games. larger the size larger the complexity.","bcbcb9bc":"- For our problem, I will focus on the games which published after 1950. \n- Though dropping the records because they are very old in time might not be a good move. But I will prefer to have information about the time period in which board games were more popular and many peoples were playing them as the use of model we are going to build is has  to enable us to make better decisions about game new upcoming games.","f13acc31":"Moving on, One of the key observation we made earlier was that some of the columns contains outliers. Lets focus on some of those columns.","b53f36ee":"- It can be seen that the popularity of Board games started to rise in late 90's.\n- Popularity of board games affects the quality, quantity, longevity and competitions among games. In my opinion these factors can influence the average rating of games. \n\n- Lets do the comparision of average rating between Board games published prior to 1985 where games are moderate to less popular and for games which pubilshed after 2000.","86a1ce9f":"We also got much better RMSE on test dataset.","71361d09":"- If observe the train and test scores, There is not so much improvement in test score after parameter set {'max_depth': 12, 'n_estimators': 100}. On other side, RMSE is keep reducing as the max_depth is increasing. This indicates that if we continue to increase the max_depth model will start to overfit the training dataset.\n- So I have decided to use model with parameters {'max_depth': 12, 'n_estimators': 100}. "}}