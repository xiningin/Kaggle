{"cell_type":{"1741d8b7":"code","96d93141":"code","b3e0f86b":"code","89f25b1f":"code","6cec298e":"code","3f91a4c0":"code","0af3168a":"code","31dff6e9":"code","fa4fffb0":"code","23f7406a":"code","31bacd41":"code","ac1923bd":"code","6c3637be":"code","a76ffd6b":"code","30f27d51":"code","1a6e1888":"code","6e8a7fcc":"code","9e424fb9":"code","3c38e2e1":"code","40f3b275":"code","202c9759":"code","9430331f":"code","75a447e3":"code","b6e14b1a":"code","f0595ce7":"code","491e72d5":"code","5a6849a5":"code","dfc978f5":"code","3a7651d9":"code","5d06cc86":"code","ff473f86":"code","f9245d37":"code","69a5be36":"code","2b00bc1a":"code","ffea2a0f":"code","fe87d569":"code","8e15aff1":"code","bb374815":"code","468f3cd7":"code","716f867e":"code","a36eba42":"code","70b7b0aa":"code","dea10ad3":"code","8d0c5107":"code","fa4e6d71":"code","34b50f22":"code","b23cb4cd":"code","073e28fa":"code","10fb015e":"code","13f08ea3":"code","e0df5c5b":"code","6a119635":"code","b3a71136":"code","58b93991":"code","930743a0":"code","6155247f":"code","6e33154e":"code","23427eac":"code","53ca10eb":"code","5154465d":"code","f934dee2":"code","e5b30db0":"code","4e30d872":"code","c814025a":"code","ea85b2b9":"code","346d74a3":"code","e408f702":"code","42b98dfc":"code","239db8d0":"code","f802b3d1":"code","58a41e33":"code","ac69769c":"code","7ca51880":"code","b230114b":"code","66157556":"code","e9189831":"code","a3af6003":"code","5c424eca":"code","b9c100f1":"code","82e14f99":"code","927b386a":"code","0d36d258":"code","843c468e":"code","ada085b8":"code","7c054212":"code","d7133b5b":"code","f7e667b1":"code","0ce8183f":"code","1ccb0bbc":"code","9bc9442f":"code","193c3671":"code","96f34494":"code","c258192e":"code","b93737a4":"code","6c317a05":"code","fc248115":"code","82133509":"code","caa68d80":"code","8040b8d7":"code","94606de6":"markdown","089b00bd":"markdown","45d7c415":"markdown","b21bd06a":"markdown","a520e02c":"markdown","195a0d6d":"markdown","56eb5d18":"markdown","b70aef75":"markdown","08d6709a":"markdown","276d71e8":"markdown","ef52cc0b":"markdown","6a19820e":"markdown","c1427cad":"markdown","50353071":"markdown","5aae76c1":"markdown","736b06ea":"markdown","9edd7271":"markdown","e33e4211":"markdown","27730769":"markdown","aaeef54c":"markdown","4ff0deab":"markdown","96d892ba":"markdown","42fe5f89":"markdown","c587a89b":"markdown","a9bbdae1":"markdown","ad42a34a":"markdown","10affe4e":"markdown","e58a3f31":"markdown","4526e89c":"markdown","e3bc236a":"markdown","4557e42c":"markdown","8b0a92eb":"markdown","11acd048":"markdown","608afa58":"markdown","5f558b4f":"markdown","07ff1bee":"markdown","a7ad1c63":"markdown","1ce612f6":"markdown","25a12c7a":"markdown","71c475e7":"markdown","91551e90":"markdown","1723a26f":"markdown","081f8195":"markdown","6c8cc233":"markdown","2df445f4":"markdown","c6468025":"markdown","a7f00386":"markdown","a67ef075":"markdown","cbb39e83":"markdown","630c424c":"markdown","8bd988bf":"markdown","0d76f5a3":"markdown","00936f3e":"markdown","9b8a7637":"markdown","98040d24":"markdown","9bbc20fa":"markdown","9795a863":"markdown","240e0c52":"markdown","266842f9":"markdown","319b488d":"markdown","c143e29c":"markdown","aa1bb6df":"markdown","b1763b12":"markdown","1e19c645":"markdown","abf2cf1a":"markdown","35f9f02f":"markdown","e65bf2cb":"markdown","c432f760":"markdown","3e14fc3e":"markdown","3d6cc92f":"markdown","e4ff661f":"markdown","38235d67":"markdown","acb738d6":"markdown","a0eba1a9":"markdown","47c5b75e":"markdown","43c89298":"markdown","c2656525":"markdown","7ff121a0":"markdown","2adf47e0":"markdown","abd7cf6b":"markdown"},"source":{"1741d8b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt # for graphs\n\nfrom matplotlib import colors\nimport plotly.express as px # for cool graphs\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport re\n\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nplt.style.use('seaborn-whitegrid')\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96d93141":"train = pd.read_csv('\/kaggle\/input\/mldub-competition-2\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/mldub-competition-2\/test.csv')\nsample_sub = pd.read_csv('\/kaggle\/input\/mldub-competition-2\/sample_submission.csv')\ncharacters = pd.read_csv('\/kaggle\/input\/mldub-competition-2\/characters.csv')\nlocations = pd.read_csv('\/kaggle\/input\/mldub-competition-2\/locations.csv')\nscript_lines = pd.read_csv('\/kaggle\/input\/mldub-competition-2\/script_lines.csv')","b3e0f86b":" print('The train dataset contains {} episodes'.format(len(train)))\n print('The test dataset contains {} episodes'.format(len(test)))","89f25b1f":"train.head()","6cec298e":"characters.head()","3f91a4c0":"locations.head()","0af3168a":"script_lines.head()","31dff6e9":"print('Null values for the tables: \\n')\nprint(train.isnull().sum())\nprint(characters.isnull().sum())\nprint(locations.isnull().sum())\n","fa4fffb0":" train.rename(columns = {'id': 'episode_id'}, inplace = True)\n test.rename(columns = {'id': 'episode_id'}, inplace = True)\n characters.rename(columns = {'id': 'character_id'}, inplace = True)\n locations.rename(columns = {'id': 'location_id'}, inplace = True)","23f7406a":"tables = [train, characters, locations, script_lines]\n\nfor t in tables:\n    print(t.dtypes)\n    print('\\n')","31bacd41":"digit_check = script_lines['character_id'].str.isdigit().value_counts(dropna = False)\nprint(digit_check)","ac1923bd":"script_lines.loc[script_lines['character_id'].str.isdigit() == False]","6c3637be":"script_lines['character_id'].loc[script_lines['character_id'].str.isdigit() == False] = -1","a76ffd6b":"# Target variable\ntrain['imdb_score'].hist()\nplt.title('Discribution of IMDB scores.')\nplt.show()","30f27d51":"imdb_overtime = px.scatter(train, x=\"episode_id\", y=\"imdb_score\", color=\"season\",  template='ggplot2', color_continuous_scale=px.colors.qualitative.G10,  trendline=\"lowess\", title='IMDB Score Overtime')\nimdb_overtime.show()","1a6e1888":"train[['imdb_votes', 'us_viewers_in_millions', 'imdb_score']].corr()","6e8a7fcc":"# Highest rated episodes in the train data.\ntrain.nlargest(25,'imdb_score')","9e424fb9":" 'The median US viewership for the top rated episodes was {} millions'.format(train.nlargest(25,'imdb_score')['us_viewers_in_millions'].median())","3c38e2e1":"# Highest rated episodes in the train data.\ntrain.nsmallest(25,'imdb_score')","40f3b275":"'The median US viewership for the top rated episodes was {} millions'.format(train.nsmallest(25,'imdb_score')['us_viewers_in_millions'].median())","202c9759":"# Function returning dacades based on dates. Decade understood as running from ***0 to ***9.\ndef decade_converter(date):\n    date = str(date)\n    decade = 0\n    # Only the first episode aired in the 1980s, we'll add it to the 1990s.\n    if re.search(\"19(8|9).*\", date):\n        decade = '1990s'\n    elif re.search(\"200.*\", date):\n        decade = '2000s'\n    # Our data ends in 2016.\n    elif re.search(\"201.*\", date):\n        decade = '2010s'\n    return decade\n\ntrain['decade'] = train['original_air_date'].apply(decade_converter)\ntest['decade'] = test['original_air_date'].apply(decade_converter)","9430331f":"train['month'] = pd.to_datetime(train['original_air_date']).dt.month\ntest['month'] = pd.to_datetime(test['original_air_date']).dt.month ","75a447e3":"train['year'] = pd.to_datetime(train['original_air_date']).dt.year\ntest['year'] = pd.to_datetime(test['original_air_date']).dt.year ","b6e14b1a":"train['imdb_votes_by_mil_viewers'] = train['imdb_votes'] \/ train['us_viewers_in_millions']\ntest['imdb_votes_by_mil_viewers'] = test['imdb_votes'] \/ test['us_viewers_in_millions']","f0595ce7":"characters.head()","491e72d5":"# List with 4 main characters in lowercase.\nsimpsons_family = ['homer', 'marge', 'bart', 'lisa']\n\n# One-hot encoding for episodes with each of the main characters appearing in the titles.\nfor c in simpsons_family: \n    train[c + '_in_title'] = train['title'].apply(lambda x: 1 if c in x.lower() else 0)\n    test[c + '_in_title'] = test['title'].apply(lambda x: 1 if c in x.lower() else 0)","5a6849a5":"# Creating \"Treehouse of Horror\" feature for both train and test.\ntrain['is_halloween_special'] = train['title'].apply(lambda x: 1 if 'treehouse of horror' in x.lower() else 0)\ntest['is_halloween_special'] = test['title'].apply(lambda x: 1 if 'treehouse of horror' in x.lower() else 0)","dfc978f5":"sex_counts = characters['sex'].fillna('Not Specified').value_counts(dropna = False)\npx.bar(\n    sex_counts,\n    labels={ # replaces default labels by column name\n                \"index\": \"Character sex\",  \"value\": \"Count\"\n            },\n    title=\"Character sex counts in the characters table\")","3a7651d9":"locations_text_count = script_lines.raw_location_text.value_counts()","5d06cc86":"top_locations = list(locations_text_count[:19].index)\n\n# filter top locations\nlines_in_top_locations = script_lines[np.where(script_lines.raw_location_text.isin(top_locations), True, False)]\nlines_in_top_locations = lines_in_top_locations[['episode_id','location_id', 'raw_location_text']]","ff473f86":"top_locations_dummies = pd.get_dummies(lines_in_top_locations['raw_location_text'])\n# One hot encoding for wheter a line is in top location.\nlines_in_top_locations = lines_in_top_locations.join(top_locations_dummies)\n# Dropping the encoded variable (both in numeric and string form)\nlines_in_top_locations = lines_in_top_locations.drop(['location_id','raw_location_text'], axis = 1)\n# Summing number of lines in each location per episode\nlines_in_top_locations = lines_in_top_locations.groupby('episode_id').sum()\n# # Optional changing count to one-hot encoding\n# lines_in_top_locations = lines_in_top_locations.applymap(lambda x: 1 if x>0 else 0)","f9245d37":"train = train.merge(lines_in_top_locations, on='episode_id', how='left')\ntest = test.merge(lines_in_top_locations, on='episode_id', how='left')","69a5be36":"script_lines.head()","2b00bc1a":"# Calculating also total number of lines per episode.\nlines_per_episode = script_lines.groupby(['episode_id'])['spoken_words'].count()","ffea2a0f":"# get top characters by line count\ncharacter_lines_count = script_lines.raw_character_text.value_counts()\nmain_characters = list(character_lines_count[:19].index)\n\n# filter lines from main characters.\nmain_characters_filtered = script_lines[np.where(script_lines.raw_character_text.isin(main_characters), True, False)]\n\n# create new dataframe for dialogue lines.\ncharacters_dict = {\n    'From' : main_characters_filtered.raw_character_text[:-1].values,\n    'To' : main_characters_filtered.raw_character_text[1:].values}\ncharacters_mapped = pd.DataFrame.from_dict(characters_dict)\n\n# create character by character df.\nchar_by_char = pd.get_dummies(characters_mapped.To).groupby(characters_mapped.From).apply(sum)\nchar_by_char.columns = list(char_by_char.index)\nchar_by_char.index = list(char_by_char.index)\n\norder = main_characters\n\nchar_by_char_ordered = char_by_char.loc[order,order]\n\n# scale values to normalize the colours and make the differences more visible.\nchar_by_char_ordered_scaled = (char_by_char_ordered+1)**(1\/4096)\n\n# create a simpsons cmap.\nsimpsons_cmap = colors.ListedColormap(['white', 'red'])\n\n# create plot\nfig = plt.figure(figsize=(9,7))\n\nsns.heatmap(char_by_char_ordered_scaled, annot=False, cbar=False, cmap=\"PiYG\")\n","fe87d569":"# create new dataframe for dialogue lines.\ndialogue_dict = {\n    'from' : script_lines.character_id[:-1].values,\n    'to' : script_lines.character_id[1:].values}\ndialogue_mapped = pd.DataFrame.from_dict(dialogue_dict).fillna(999)\n# addint epsiode id from script_lines.\ndialogue_mapped = dialogue_mapped.merge(script_lines[['episode_id','raw_character_text']], left_index=True, right_index=True, how='left') \n\nprint('Example mapping of dialogue lines \\n')\nprint(dialogue_mapped[:20])","8e15aff1":"# Counting each from-to pair's occurences in each episode\ndialogue_pairs_counted = dialogue_mapped.groupby(['episode_id','from','to']).size().reset_index().rename(columns={0:'pair_count_per_episode'})\n\n# Simple hashing function for pairs  of characters. We take two character ids then multiply them and take a log of the result. \n# This is to prevent using characters names which would create a categorical variable that would later have to be one got-encoded. \ndialogue_pairs_counted['pair_id_hashed'] = np.log(dialogue_pairs_counted['from'].astype(int) * dialogue_pairs_counted['to'].astype(int))\n\n# Selecting pair with top count per episode\ntop_pair_per_episode = dialogue_pairs_counted[[ 'episode_id', 'pair_id_hashed','pair_count_per_episode']]\\\n    .loc[dialogue_pairs_counted\\\n    .groupby('episode_id')['pair_count_per_episode']\\\n    .idxmax()]\\\n    .set_index('episode_id')\\\n    .rename(columns={'pair_id_hashed':'top_pair_hashed_id'})\n\n# Adding top_pair_per_episode to train and test\ntrain = train.merge(top_pair_per_episode[['top_pair_hashed_id', 'pair_count_per_episode']], on='episode_id', how='left')\ntest = test.merge(top_pair_per_episode[['top_pair_hashed_id', 'pair_count_per_episode']], on='episode_id', how='left')\n\nprint('Example top pairs dialogue per episode \\n')\nprint(top_pair_per_episode[:20])","bb374815":"# Getting character with most lines per episode (among the top 20 character).\n\n# Counting lines per character and grouping by episode.\nlines_count_df = main_characters_filtered.groupby('episode_id')['character_id'].value_counts().reset_index(name='character_lines_count')\n\n# Getting the character_id with most lines per episode.\ntop_characters_per_episode = lines_count_df.loc[lines_count_df.groupby('episode_id')['character_lines_count'].idxmax()]\n\n# Changing character_id from object to numeric. \ntop_characters_per_episode['character_id'] = top_characters_per_episode['character_id'].astype('int64')\n\n# Joining with info from the characters tables.\ntop_characters_per_episode = top_characters_per_episode.merge(characters[['character_id', 'name', 'sex']], on='character_id', how='left')\n\n# Renaming columns to better highlight these are about top characters.\ntop_characters_per_episode = top_characters_per_episode.rename(columns={'character_id': 'top_character_in_episode', 'name' : 'top_character_name',  'sex' : 'top_character_sex'})\n\n# Adding top_character_in_episode to train and test\ntrain = train.merge(top_characters_per_episode[[ 'episode_id', 'top_character_in_episode', 'top_character_name', 'top_character_sex']], on='episode_id', how='left')\ntest = test.merge(top_characters_per_episode[[ 'episode_id', 'top_character_in_episode', 'top_character_name', 'top_character_sex']], on='episode_id', how='left')\n\n# Most popular characters. \nprint('Episodes in which given character had most lines: \\n')\nprint(top_characters_per_episode['top_character_name'].value_counts())","468f3cd7":"def characters_per_bucket_in_episode(dataframe, character_bucket, bucket_idx_start, bucket_idx_end):\n    # Selecting the characters in a given bucket based on indexes provided\n    characters_list = list(character_lines_count[bucket_idx_start:bucket_idx_end].index)\n    # Selecting lines of characters in the given bucket\n    characters_lines = script_lines[np.where(script_lines.raw_character_text.isin(characters_list), True, False)]\n    # Counting lines of characters in each episode\n    lines_count_df = characters_lines.groupby('episode_id')['character_id'].value_counts().reset_index(name='character_lines_count')\n    # Number of characters in a given bucket per episode\n    characters_per_episode_df = lines_count_df.groupby('episode_id').size().reset_index(name= character_bucket + 's_per_episode')\n    \n    # Adding characters_per_episode to train and test\n    dataframe = dataframe.merge(characters_per_episode_df[[ 'episode_id', character_bucket + 's_per_episode']], left_on='episode_id', right_on = 'episode_id', how='left')\n    return dataframe","716f867e":"train = characters_per_bucket_in_episode(train, 'main', 0 , 19)\ntest = characters_per_bucket_in_episode(test, 'main', 0 , 19)\n\ntrain = characters_per_bucket_in_episode(train, 'secondary', 20 , 99)\ntest = characters_per_bucket_in_episode(test, 'secondary', 20 , 99)\n\ntrain = characters_per_bucket_in_episode(train, 'guest', 100 , 500)\ntest = characters_per_bucket_in_episode(test, 'guest', 100 , 500)","a36eba42":"def characters_bucket_lines_ratio(dataframe, character_bucket, bucket_idx_start, bucket_idx_end):\n    # Selecting the characters in a given bucket based on indexes provided\n    characters_list = list(character_lines_count[bucket_idx_start:bucket_idx_end].index)\n    # Selecting lines of characters in the given bucket\n    characters_lines = script_lines[np.where(script_lines.raw_character_text.isin(characters_list), True, False)]\n    # Counting the lines of characters in a given bucket in each episode\n    bucket_lines_per_episode = characters_lines.groupby(['episode_id'])['spoken_words'].count()\n    bucket_lines_ratio = round(bucket_lines_per_episode \/ lines_per_episode, 2).reset_index(name=character_bucket + '_lines_ratio')\n\n    # Adding mains_lines_ratio to train and test\n    dataframe = dataframe.merge(bucket_lines_ratio[[ 'episode_id', character_bucket + '_lines_ratio']], on='episode_id', how='left')\n    return dataframe","70b7b0aa":"train = characters_bucket_lines_ratio(train, 'main', 0 , 19)\ntest = characters_bucket_lines_ratio(test, 'main', 0 , 19)\n\ntrain = characters_bucket_lines_ratio(train, 'secondary', 20 , 99)\ntest = characters_bucket_lines_ratio(test, 'secondary', 20 , 99)\n\ntrain = characters_bucket_lines_ratio(train, 'guest', 100 , 500)\ntest = characters_bucket_lines_ratio(test, 'guest', 100 , 500)","dea10ad3":"script_lines.loc[script_lines['raw_character_text'].isin(['Jack White', 'Meg White'])]","8d0c5107":"main_characters = list(character_lines_count[0:19].index)\n\n# Creating a list of variable names holding the line count of each of top 20 characters\ncharacter_n_lines = []\n\nfor c in main_characters:\n    character_n_lines.append(c.replace(\" \", \"_\") + '_lines')","fa4e6d71":"main_characters_dict = dict(zip(main_characters, character_n_lines)) ","34b50f22":"lines_counts = pd.DataFrame()\n\nfor key, value in main_characters_dict.items():\n    lines_counts[value] = script_lines.loc[script_lines['raw_character_text']==key,:].groupby(['episode_id'])['spoken_words'].count()\n    train[value] = train['episode_id'].map(lines_counts[value]).fillna(0)\n    test[value] = train['episode_id'].map(lines_counts[value]).fillna(0)","b23cb4cd":"train['n_lines'] = train['episode_id'].map(lines_per_episode).fillna(0)\ntest['n_lines'] = test['episode_id'].map(lines_per_episode).fillna(0)","073e28fa":"analyser = SentimentIntensityAnalyzer()\n\ndef sentiment_analyzer_scores_pos(sentence):\n    score = analyser.polarity_scores(sentence)\n    return score['pos']\n\n\ndef sentiment_analyzer_scores_neg(sentence):\n    score = analyser.polarity_scores(sentence)\n    # Representing negative sentiment by negative values\n    return score['neg'] * -1","10fb015e":"script_lines['normalized_text'] = script_lines['normalized_text'].astype('str') \n\nscript_lines['sentiment_positive'] = script_lines.normalized_text.apply(sentiment_analyzer_scores_pos)\nscript_lines['sentiment_negative'] = script_lines.normalized_text.apply(sentiment_analyzer_scores_neg)\n\n# Calculating the episode's sentiment\nscript_lines['sentiment'] = script_lines['sentiment_positive'] + script_lines['sentiment_negative']","13f08ea3":"episode_sentiments = script_lines.groupby('episode_id')[['sentiment', 'sentiment_positive', 'sentiment_negative']].mean()\n\ntrain = train.merge(episode_sentiments, on='episode_id', how='left')\ntest = test.merge(episode_sentiments, on='episode_id', how='left')","e0df5c5b":"train[['sentiment', 'sentiment_positive', 'sentiment_negative']].hist()","6a119635":"sentiment_per_score = px.scatter(train, x=\"sentiment\", y=\"imdb_score\",  template='ggplot2', color_continuous_scale=px.colors.qualitative.G10,  trendline=\"lowess\", title='IMDB Score per Sentiment').add_trace(go.Scatter(x=[0, 0], y=[4,10], mode=\"lines\", name='neutral sentiment'))\nsentiment_per_score.show()","b3a71136":"# Updating the standard set of stop words with some extra ones specific to normalized text in the script lines.\nSTOPWORDS.update(['youre', 'oh','im', 'one', 'now' ])\n\n# Selecting the lines with positive and negative sentiment respectively. \nlines_pos = script_lines[script_lines['sentiment'] > 0]\nlines_pos = lines_pos['normalized_text']\nlines_neg = script_lines[script_lines['sentiment'] < 0]\nlines_neg = lines_neg['normalized_text']\n\ndef wordcloud_draw(data, color = 'black'):\n    words = ' '.join(data)\n#     for word in words:  # iterating on a copy since removing will mess things up\n#         if word in most_common_english_words:\n#             words.remove(word)\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color=color,\n                      width=2500,\n                      height=2000\n                     ).generate(words)\n    plt.figure(1,figsize=(13, 13))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n    \nprint(\"Positive words\")\nwordcloud_draw(lines_pos,'white')\nprint(\"Negative words\")\nwordcloud_draw(lines_neg)","58b93991":"train.head()","930743a0":"train = train.drop(['top_character_name','title', 'original_air_date', 'number_in_series'], axis=1)\ntest = test.drop(['top_character_name','title', 'original_air_date', 'number_in_series'], axis=1)\n\ntrain_final = pd.get_dummies(train)\ntest_final = pd.get_dummies(test)","6155247f":"train_cols = list(train_final.drop('imdb_score', axis=1).columns)\ntest_cols = list(test_final.columns)\n\n# Aligning test and trani data because one-hot encoding of categorical variables could create conflicts.\n# train_final, test_final = train_final.align(test_final, join='left', axis=1)","6e33154e":"X_train = train_final[train_cols]\ny_train = train_final['imdb_score']\nX_test = test_final[train_cols]","23427eac":"# Replacing any empty values with 0\nX_train = X_train.fillna(-1)\nX_test = X_test.fillna(-1)\n\n# Double Checking if there are any empty values.\nprint(X_train.isnull().sum())","53ca10eb":"import h2o\nprint(h2o.__version__)\nfrom h2o.automl import H2OAutoML\n\nh2o.init(max_mem_size='16G')","5154465d":"# Converting pandas dataframe to h2o frame \n\nhf = h2o.H2OFrame(train_final)\nhf_test = h2o.H2OFrame(test_final)","f934dee2":"seed = 102\ntrainf, validf = hf.split_frame(ratios = [.8], seed = seed)","e5b30db0":"aml = H2OAutoML(max_models=30, seed=seed, max_runtime_secs=30000)\naml.train(x=train_cols, y='imdb_score', training_frame=hf)","4e30d872":"lb = aml.leaderboard\nlb.head()","c814025a":"m = h2o.get_model(lb[1,\"model_id\"])\nm.varimp(use_pandas=True)[:20]","ea85b2b9":"from h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom datetime import datetime\n\n# Define a model\nh2o_gbm  = H2OGradientBoostingEstimator(\n    model_id=str(datetime.now()) + '_gbm',\n    seed=seed,\n    learn_rate = 0.01,\n    max_depth = 9,\n    sample_rate = 1.,\n    col_sample_rate = 0.2,\n    ntrees = 500\n)\n\n# Train a model ==> MAJOR GOAL: identify important variables\nh2o_gbm.train(x=train_cols,\n              y='imdb_score',\n              training_frame=hf)\n\n","346d74a3":"# Copy of a model with the same initial params\nh2o_gbm_copy = H2OGradientBoostingEstimator(\n    model_id=str(datetime.now()) + '_gbm', #\n    seed=seed,\n    learn_rate = 0.01,\n    max_depth = 9,\n    sample_rate = 1.,\n    col_sample_rate = 0.2,\n    ntrees = 500\n)","e408f702":"# Storing validation and train sets errors\nvalid_array = []\ntrain_array = []\n\n# Some large number to serve as the inital error\nbest_valid_err=1e6\n\nfor num_vars in range(1,len(h2o_gbm.varimp())+1):\n  X_names_test=[]\n  for j in range(0,num_vars):\n    # print(num_vars,j, h2o_gbm.varimp()[j][0])\n    X_names_test.append(h2o_gbm.varimp()[j][0])\n\n  # Train the model using just top XX features \n  print('Using the first ',num_vars,' important features:', X_names_test)\n  h2o_gbm_copy.train(x=X_names_test,\n               y='imdb_score',\n               training_frame=hf,\n               validation_frame = validf)\n  \n  print('train',h2o_gbm_copy.rmse())\n  train_array.append(h2o_gbm_copy.rmse())\n\n  # retrieve the rmse value for the validation data:\n  print('valid',h2o_gbm_copy.rmse(valid=True))\n  valid_array.append(h2o_gbm_copy.rmse(valid=True))\n   \n  # storing the best prediction valid err and all predictions on trainset\n  if h2o_gbm_copy.rmse(valid=True)<best_valid_err:\n    prediction_train = h2o_gbm_copy.predict(hf_test)\n    best_valid_err = h2o_gbm_copy.rmse(valid=True)\n\n  perf = h2o_gbm_copy.model_performance(valid=True)\n  print(perf)  \n  print('=======================================')  \n","42b98dfc":"# top model valid err : it is a model with top-x features\nprint(best_valid_err)","239db8d0":"# Best model index \nmodel_index = valid_array.index(best_valid_err) + 1","f802b3d1":"# Limiting colums to top x columns in terms of relative importance\ntop_col_names = h2o_gbm.varimp(use_pandas=True)['variable'][:model_index].tolist()","58a41e33":"top_col_names = h2o_gbm.varimp(use_pandas=True)['variable'][:31].tolist()","ac69769c":"gbm_top_cols = h2o_gbm\ngbm_top_cols.train(x=top_col_names, y='imdb_score', training_frame=hf)","7ca51880":"aml_top_cols = H2OAutoML(max_models=30, seed=seed, max_runtime_secs=30000)\naml_top_cols.train(x=top_col_names, y='imdb_score', training_frame=hf)","b230114b":"lb_top_cols = aml_top_cols.leaderboard\nlb_top_cols.head()","66157556":"m = h2o.get_model(lb_top_cols[2,\"model_id\"])\nm.varimp(use_pandas=True)[:50]","e9189831":"m.varimp_plot()\n","a3af6003":"# h20 all models preds\n# preds = aml.predict(hf_test)\n\n# h20 all models top clumns \n# preds = aml_top_cols.predict(hf_test)\n\n# gmb preds\n# preds = h2o_gbm.predict(hf_test)\n\n# predict for top columns XGBoost\n# preds = xgb_top_cols.predict(hf_test)\n\n# For Best GBM from Grid Search\n# preds = best_gbm.predict(hf_test)\npreds = gbm_top_cols.predict(hf_test)\n\npreds_as_df = preds.as_data_frame()\n\npreds_as_df['predict'] = round(preds_as_df['predict'], 1)\n\npreds_as_df[:10]","5c424eca":"test_eval = test.join(preds_as_df)\n\npredict_overtime = px.scatter(test_eval, x=\"episode_id\", y=\"predict\", color_continuous_scale=px.colors.qualitative.G10,  trendline=\"lowess\", title='Predict Score Overtime')\n","b9c100f1":"test_eval = test.join(preds_as_df)\n\npredict_overtime = px.scatter(test_eval, x=\"episode_id\", y=\"predict\", color_continuous_scale=px.colors.qualitative.G10,  trendline=\"lowess\", title='Predict Score Overtime')\n\nfig = make_subplots(x_title = \"Predictions for test (Blue) vs IMDB Scores from train (Red)\")\nimdb_overtime = px.scatter(train, x=\"episode_id\", y=\"imdb_score\",  template='ggplot2',  trendline=\"lowess\", title='IMDB Score Overtime')\n\nfig.add_trace(imdb_overtime.data[0])\nfig.add_trace(predict_overtime.data[0])\nfig.update_layout(width=700, height=450, hovermode='x')\nfig.show()","82e14f99":"submission = sample_sub\nsubmission['imdb_score'] = preds_as_df\nsubmission","927b386a":"submission.to_csv('submission-h2o-30-gm-top41.csv', index=False)","0d36d258":"# from h2o.grid.grid_search import H2OGridSearch\n\n# grid_search_gbm = H2OGradientBoostingEstimator(\n#     stopping_rounds = 25,\n#     stopping_metric = \"RMSE\",\n#     col_sample_rate = 0.65,\n# #     sample_rate = 0.65,\n#     seed = seed\n# ) \n\n# hyper_params = {\n#     'learn_rate':[0.01, 0.02, 0.03],\n#     'max_depth':[4,6,8,10,16],\n#     'ntrees':[50, 150, 250, 500],\n#     'sample_rate': [.7, 1.]}\n# grid = H2OGridSearch(grid_search_gbm, hyper_params,\n#                          grid_id='depth_grid',\n#                          search_criteria={'strategy': \"Cartesian\"})\n# #Train grid search\n# grid.train(x=train_cols, \n#            y='imdb_score',\n#            training_frame=hf,\n#            validation_frame=validf)","843c468e":"# grid_sorted = grid.get_grid(sort_by='rmse',decreasing=False)","ada085b8":"# best_gbm = grid_sorted.models[0]\n# print(best_gbm)","7c054212":"# best_gbm.varimp_plot()\n","d7133b5b":"# # Copy of a model with the same initial params\n# best_gbm_copy = best_gbm(model_id=str(datetime.now()) + '_gbm')\n\n# best_gbm.get_params()\n\n\n# best_gbm_copy.Parameters\n\n# h2o_gbm_copy = H2OGradientBoostingEstimator(\n#     model_id=str(datetime.now()) + '_gbm', #\n#     seed=seed,\n#     learn_rate = 0.01,\n#     max_depth = 9,\n#     sample_rate = 1.,\n#     col_sample_rate = 0.2,\n#     ntrees = 500\n# )","f7e667b1":"# # Storing validation and train sets errors\n# valid_array = []\n# train_array = []\n\n# # Some large number to serve as the inital error\n# best_valid_err=1e6\n\n# for num_vars in range(1,len(best_gbm.varimp())+1):\n#   X_names_test=[]\n#   for j in range(0,num_vars):\n#     # print(num_vars,j, best_gbm_copy.varimp()[j][0])\n#     X_names_test.append(best_gbm.varimp()[j][0])\n\n#   # Train the model using just top XX features \n#   print('Using the first ',num_vars,' important features:', X_names_test)\n#   best_gbm_copy.train(x=X_names_test,\n#                y='imdb_score',\n#                training_frame=hf,\n#                validation_frame = validf)\n  \n#   print('train',best_gbm_copy.rmse())\n#   train_array.append(best_gbm_copy.rmse())\n\n#   # retrieve the rmse value for the validation data:\n#   print('valid',best_gbm_copy.rmse(valid=True))\n#   valid_array.append(best_gbm_copy.rmse(valid=True))\n   \n#   # storing the best prediction valid err and all predictions on trainset\n#   if best_gbm_copy.rmse(valid=True)<best_valid_err:\n#     prediction_train = best_gbm_copy.predict(hf_test)\n#     best_valid_err = best_gbm_copy.rmse(valid=True)\n\n#   perf = best_gbm_copy.model_performance(valid=True)\n#   print(perf)  \n#   print('=======================================')  \n","0ce8183f":"# # top model valid err : it is a model with top-x features\n# print(best_valid_err)","1ccb0bbc":"# # Limiting colums to top x columns in terms of relative importance\n# gbm_top_col_names = best_gbm.varimp(use_pandas=True)['variable'][:17].tolist()","9bc9442f":"# gbm_best_top_cols = H2OAutoML(max_models=30, seed=seed, max_runtime_secs=30000)\n# gbm_best_top_cols.train(x=gbm_top_col_names, y='imdb_score', training_frame=hf)","193c3671":"# xgb = XGBClassifier(\n#     random_state=102\n# )\n# xgb.fit(X_train, y_train)\n# preds = xgb.predict(X_test)","96f34494":"# class Optimizer:\n#     def __init__(self, metric, trials=30):\n#         self.metric = metric\n#         self.trials = trials\n#         self.sampler = TPESampler(seed=102)\n        \n#     def objective(self, trial):\n#         model = create_model(trial)\n#         model.fit(X_train, y_train)\n#         preds = model.predict(X_test)\n#         if self.metric == 'acc':\n#             return accuracy_score(y_val, preds)\n#         else:\n#             return f1_score(y_val, preds)\n            \n#     def optimize(self):\n#         study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n#         study.optimize(self.objective, n_trials=self.trials)\n#         return study.best_params","c258192e":"# from h2o.estimators import H2OXGBoostEstimator\n\n# # Build and train the model:\n# h2o_xgb = H2OXGBoostEstimator(model_id=str(datetime.now()) + '_xgb', #\n#     learn_rate = 0.01,\n#     max_depth = 9,\n#     sample_rate = 1.,\n#     col_sample_rate = 0.2,\n#     ntrees = 1000,\n#     booster='dart',\n#     normalize_type=\"tree\",\n#     seed=seed)\n\n# h2o_xgb.train(x=train_cols,\n#               y='imdb_score',\n#               training_frame=hf,\n#               validation_frame=validf)\n\n# # Eval performance:\n# perf = h2o_xgb.model_performance()\n\n# # Generate predictions on a test set (if necessary):\n# xgb_pred = h2o_xgb.predict(hf_test)","b93737a4":"# # Build and train the model:\n# h2o_xgb_copy = H2OXGBoostEstimator(model_id=str(datetime.now()) + '_xgb', #\n#     learn_rate = 0.01,\n#     max_depth = 9,\n#     sample_rate = 1.,\n#     col_sample_rate = 0.2,\n#     ntrees = 1000,\n#     booster='dart',\n#     normalize_type=\"tree\",\n#     seed=seed)","6c317a05":"# # Storing validation and train sets errors\n# valid_array = []\n# train_array = []\n\n# # Some large number to serve as the inital error\n# best_valid_err=1e6\n\n# for num_vars in range(1,len(h2o_xgb.varimp())+1):\n#   X_names_test=[]\n#   for j in range(0,num_vars):\n#     # print(num_vars,j, h2o_xgb.varimp()[j][0])\n#     X_names_test.append(h2o_xgb.varimp()[j][0])\n\n#   # Train the model using just top XX features \n#   print('Using the first ',num_vars,' important features:', X_names_test)\n#   h2o_xgb_copy.train(x=X_names_test,\n#                y='imdb_score',\n#                training_frame=hf,\n#                validation_frame = validf)\n  \n#   print('train',h2o_xgb_copy.rmse())\n#   train_array.append(h2o_xgb_copy.rmse())\n\n#   # retrieve the rmse value for the validation data:\n#   print('valid',h2o_xgb_copy.rmse(valid=True))\n#   valid_array.append(h2o_xgb_copy.rmse(valid=True))\n   \n#   # storing the best prediction valid err and all predictions on trainset\n#   if h2o_xgb_copy.rmse(valid=True)<best_valid_err:\n#     prediction_train = h2o_xgb_copy.predict(hf_test)\n#     best_valid_err = h2o_xgb_copy.rmse(valid=True)\n\n#   perf = h2o_xgb_copy.model_performance(valid=True)\n#   print(perf)  \n#   print('=======================================')  \n","fc248115":"# #top model valid err : it is a model with top-x features\n# print(best_valid_err)","82133509":"# # Best model index \n# model_index = valid_array.index(best_valid_err)","caa68d80":"# # Limiting colums to top x columns in terms of relative importance\n# top_col_names = h2o_gbm.varimp(use_pandas=True)['variable'][:model_index].tolist()","8040b8d7":"# xgb_top_cols = h2o_xgb\n# xgb_top_cols.train(x=top_col_names, y='imdb_score', training_frame=hf)","94606de6":"### Month of episode's airing","089b00bd":"### Top Characters per episode","45d7c415":"Let's also llok at the ratio of lines spoken by characters in each bucket.","b21bd06a":"## 4.3 Script Lines","a520e02c":"Let's inspect the words used in lines with positive and negative sentiment. This is primarly to  give us the feel of whether the evaluation of the script lines' sentiment matches our expectations of positive and negative words.","195a0d6d":"### GBM","56eb5d18":"---","b70aef75":"# 5. Train & validation <a name=\"#train&validation\"><\/a>","08d6709a":"#### GBM - Grid Seach","276d71e8":"The Simpsons rely heavily on guests appearances. \n\nOf course, what makes a guest is entireley subjective. This [cameo by the band 'White Stripes'](https:\/\/www.youtube.com\/watch?v=dUx3kDnT-p8)  is one of my favourite Simpsons clips but only has them saying a line each - and as such not making my selection. \n\nBut I think it's a fair approximation and I'll go ahead with it.\n\n**Bonus Fact**: Songs lyrics don't seem to be included in script_lines (unless written for the show specifically). See below and compare with the clip:","ef52cc0b":"### Main characters in episode titles","6a19820e":"What make's a high rated Simpsons episode then? Few parameters seem to stand out:\n\n* Metadata is carries way more information than any data about the episode's lines and characters.\n* The Simpsons 'Golden Era' in the 1990s seems noticable. After that point the show's rating are experiencing a steady decline. To what extent this is an objective phenomenon due to the show's reusing of ideas and to what it's driven by nostaliga for the old episodes is debatable.\n* Generally classic episodes set in Springfield (Simpson's Home, Moe's Tavern) with the core main characters eem to score better. This probably a characteristic of the early episodes too.\n* Positive sentiment mildly correlates with a higher IMDB rating\n","c1427cad":"### Character buckets","50353071":"Let's start by counting the number of lines in each episode.","5aae76c1":"## 6.Sumbission <a name=\"#submission\"><\/a>","736b06ea":"We are going to train our model using just a subset of columns presented in the train set.","9edd7271":"From looking at the top and lowest ranked episodes we can observe that Homer appears in the titels of 8 out of the top 25, and Bart in another 3.\n\nLisa appears in the titles of 5 out of the 25 **lowest** rated episodes, Homer in 1.\n\nInterestingly, Marge makes no appearances at all in top 25 and bottom 25 titles.\n\nLet's encode this information into 4 new columns.","e33e4211":"### XGBoost - Optimized\n","27730769":"### Special Episodes","aaeef54c":"# Table of contents\n1. [Intro and loading packages](#introduction)\n2. [Reading the data](#readingthedata)\n3. [Data cleaning](#datacleaning)\n4. [EDA & FE](#EDA&FE)\n    - 4.1 Characters\n    - 4.2 Locations\n    - 4.3 Script Lines\n5. [Training and Validation](#train&validation)\n6. [Submission](#submission)","4ff0deab":"Let's see how many of the characters in each bucket appear in a given episode.","96d892ba":"As we can see there's not that much data about the chracter's ex in the *characters* table, and the name and character_id we already have in *script_lines*. We can leave it aside for now.","42fe5f89":"Let's count the number of lines in top 20 most popular locations in each episode.","c587a89b":"# Intro and loading packages","a9bbdae1":"# 3. Cleaning the data <a name=\"#datacleaning\"><\/a>","ad42a34a":"Lets inspect the best model after grid search","10affe4e":"## XGBoost","e58a3f31":"All of these highest rated episodes are from the 1990s! And nearly all had viewership of over 10 M, many reaching well above 20 M.","4526e89c":"Best results using the first  17  important features: \n\n['imdb_votes', 'episode_id', 'season', 'imdb_votes_by_mil_viewers', 'us_viewers_in_millions', 'secondarys_lines_ratio', 'Marge_Simpson_Lines', 'Lisa_Simpson_Lines', 'decade_1990s', 'mains_lines_ratio', 'Homer_Simpson_Lines', 'month', 'number_in_season', 'guests_lines_ratio', 'year', 'Bart_Simpson_Lines', 'n_lines']","e3bc236a":"# 4. EDA & FE <a name=\"#EDA&FE\"><\/a>","4557e42c":"Furthermore we can see that \"Treehouse of Horror\" is a title for recurring special episodes, it makes sense to create feature using this info.","8b0a92eb":"The purpose of this kernal is to estimate the IMDB score of Simpsons episodes. The daata available includes emtat data about each episode, all script lines, locations and characters info.  \n\nThe data set is very small (358 rows for train, 239 for test) so it's key not to overfit.\n\nA lot of info available but unless we can create a model to understand what do people find funny based on the script lines (and hence the IMDB score), then most of it is just noise.\n\nMy impression is that the general sentiment is that the Simpsons used to be better (and as such rated more highly). Nostalgia is likely a big factor here - hence it might be that that all the info in the scripts might give us little in terms of finding out why are certain episodes are rated highly. The theory here is that even if all seasons were of equal 'quality', the old ones would still be rated higher because of this perceived notion that the Simpsons 'ain't what it used to be'.\n\nFor some non-quantified analysis regarding the Simpsons' decline overtime I'd recommend [this post from Grunge](https:\/\/www.grunge.com\/187992\/the-real-reason-the-simpsons-became-so-repetitive\/).  ","11acd048":"We can see that prediction have way lass variance than the actual scores from the train datasets.\n\nNotably there are no noticable outliers and the values tend to be clustered more closely around the trendline than is the case for the actual imdb scores.","608afa58":"### Characters with most lines per episode","5f558b4f":"## 4.1 Characters","07ff1bee":"### Who speaks to whom - entire series","a7ad1c63":"We can clearly see that most episodes have on average positive sentiment, although in general it's not very strong (seems that the sentiment gets aggregated across multiple lines in each episode).","1ce612f6":"### IMDB Votes by Millions of viewers","25a12c7a":"Looks ok, most columns have no null values, except for us_viewers_in_millions (which has 3) and character.sex (which has 6399 - we'll get to this later).","71c475e7":"### Top and lowest rated episodes","91551e90":"We're looking for any column that seems to store a number or an id but is not stored as int64 or float64. This could suggest that the data might be corrupt in some way.\n\nInterestingly, character_id should be an integer or float column but appears to be an object. Let's check if all values are indeed consisting only of digits.","1723a26f":"Let's check if there is correlation between number of viewers of the episode and number of votes with target variable.","081f8195":"### Who speaks to who - per episode","6c8cc233":"The distribution looks close to normal with very small amount of episodes with low ratings.","2df445f4":"### H20 AutoML\n\n","c6468025":"Seasons seem to be pretty much the strongest predictor of the IMDB score. In fact just basing on the average of each season one could make a pretty good predictor. \n\nThe key to this challenge seems to be in identyfing what distinguishes the outliers (both positive and negaitve), while avoiding overfitting to the tiny dataset.","a7f00386":"Worth noting that unless partial views are uncounted, then the viewers decide to watch the episode prior to knowing its quality. Hence, the viewership is not directly influenced by the quality of a given episode but more by the general viewership and quality of the preceeding episodes. In other words, bad episodes might produce a lagged drop in viewership for the subsequent ones. ","a67ef075":"Final approach:\n\n1. H2O for general check on what model to use\n2. Grid search for best hyperparameters on that model\n3. Eliminating the variables to see which combination brings the best results","cbb39e83":"Best result (RMSE 0.191424521732665) was achieved the first  35  important features: \n\n['imdb_votes', 'season', 'episode_id', 'year', 'us_viewers_in_millions', 'decade_1990s', 'imdb_votes_by_mil_viewers', 'decade_2010s', 'secondarys_lines_ratio', 'Marge_Simpson_Lines', 'Lisa_Simpson_Lines', 'Homer_Simpson_Lines', 'decade_2000s', 'mains_lines_ratio', 'month', 'secondarys_per_episode', 'mains_per_episode', 'guests_lines_ratio', 'number_in_season', 'Lenny_Leonard_Lines', 'Ned_Flanders_Lines', 'Bart_Simpson_Lines', 'n_lines', 'Waylon_Smithers_Lines', 'C._Montgomery_Burns_Lines', 'guests_per_episode', 'Seymour_Skinner_Lines', 'Chief_Wiggum_Lines', 'Milhouse_Van_Houten_Lines', 'Dr._Julius_Hibbert_Lines', 'top_character_sex_m', '*Grampa_Simpson_Lines*', 'lisa_in_title', 'Carl_Carlson_Lines', 'Kent_Brockman_Lines']","630c424c":"### Conclusions","8bd988bf":"Most Simpsons seasons run from September\/October to May. Perhaps early or late episodes in the season are better? This information could be to an extent captured by number_in_season but perhaps month of airing can better capture those in-season tendencies than specific episode_id?","0d76f5a3":"### Optimize for less variables","00936f3e":"### Explaining the top automl model","9b8a7637":"### Scoring","98040d24":"Let's start by importing the packages.","9bbc20fa":"Out of 1600+ characters who appeared in the show let's define thre arbitrary buckets in terms of all lines spoken on the show:\n\n* Main characters = top 20 in terms of lines across the entire show.\n* Secondary characters = betwen 21 and 100\n* Guest characters = between 101 and 500\n\nAny characters beyond 500th seems to have uttered only few lines.","9795a863":"## 4.2 Locations","240e0c52":"Let's also get the year of the original air date. The idea here is to remove some granularity of specific airdates and hopefully prevent overfitting.","266842f9":"### Evaluate preds ","319b488d":"Now let's check the contents of script lines and see if we can extract some meaningful features from it.","c143e29c":"### Character's sex","aa1bb6df":"Moreover there are two redundant mappings that offer the same as character_id and location_id respectively but in form of strings:\n\n- characters.name = script_lines.raw_character_text\n- locations.name = script_lines.raw_location_text","b1763b12":"We can see 3 clusters:\n\n1. Dialogue within the Simpsons family (Homer, Marge, Bart and Lisa) = Very Common\n2. Dialogue between one of the Simpsons and a side character = Occassional\n3. Dialogue between two side characters = Uncommon\n\n(Note that the diagonal line indicates characters talking to themselves, and some seem to be doing a bit more of that than others - Bart stands out among the main charcters, Krusty among the secondary).\n\nWe're obviously making assumptions here, e.g. about the direction of each dialouge line - a new line in a new scene would be seen as a continuation of the previous conversation. Similarly, there can be more than one recepient of a line. Still, I think major relations across the series hold in this data and it's enough for our purposes.\n\nOk, so admittedly this graph re-establishes that the main characters are the... main characters after all? Nonetheless, there are some interesting observations = for example Mr Burns talks to Mr Smithers more than them being two side characters would suggest - it makes sense, as Mr Smithers works for him in the series.","1e19c645":"### Lines per character","abf2cf1a":"This line seems to be pretty corrupt, fair amount of data missing and the dialogue line saved in the character_id column. Let's update the character_id to -1 and move on.","35f9f02f":"We can see the following mappings in the 4 tables:\n\n- train.id = script_lines.episode_id\n- characters.id = script_lines.character_id\n- locations.id = script_lines.location_id\n\nThe script_lines table therefore plays the central role in connecting all our data. I'm going to rename ids in other tables to conform to the naming in script_lines and avoid accidental joins on wrong 'ids'.\n\n","e65bf2cb":"We have correlations scores higher than 0.6. Let's try to create some features using this knowledge.","c432f760":"# 2. Reading the data <a name=\"readingthedata\"><\/a>","3e14fc3e":"### Final data cleaning","3d6cc92f":"# Appendix: Unused","e4ff661f":"Another interesting observation is that the location of the dialogue line seems to be used in a pretty relaxed nad general manner - the White Stripes dialogue doesn't really take place in the psychiatrists office but on a street corner. The lack of accuracy here is likelydue to the fact that there's [also an entire storyboard](https:\/\/www.animationconnection.com\/view\/4690-the-simpsons-storyboard-4880) used in making each episode - and as such there's no need for script's locations to be super specific.","38235d67":"Lets find our culprit line","acb738d6":"### Sentiment Analysis (~ 2 minutes to run)","a0eba1a9":"Nearly all of the lowest rated episodes are from the 21st century, with seasons 20+ dominating this list.\n\nThe viewership is also only slightly more than a quarted of what it used to be. This however, might be also explained by more general shifts in viewership between 1990 - 2020 (more fragmented pop culture, lesser role played by TV, more people watching online and remaining uncounted).\n\nThese changes could be perhaps in creating a new variable for the decade of airing (more general than the specific air date, but still capturing some of those trends).","47c5b75e":"Even though the vader SentimentIntensityAnalyzer is designed for social media, it seems to capture the sentiment of simpsons lines fairly well.\n\nWorth noting how bart feature more prominently in negative lines than positive ones (remember this is where the characters name gets mentioned, not lines by the character). Homer and Lisa seem to involved  pretty equally across positive and negative lines. Finally Marge stands out more in the positive one. ","43c89298":"![image.png](attachment:image.png)","c2656525":"GBM seems to perfom the best from multiple models. Lets try to train specifically this one","7ff121a0":"### Lines per episode","2adf47e0":"First, let's check a few distributions.","abd7cf6b":"### Decade of the episode's airing"}}