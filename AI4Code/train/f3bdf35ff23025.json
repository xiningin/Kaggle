{"cell_type":{"2215210c":"code","42bb214e":"code","a842a8c7":"code","70b48d91":"code","85b7ae6e":"code","35a395ae":"code","c8e24d74":"code","6e0e86c9":"code","7b459eae":"code","75a06fa9":"code","673d8000":"code","2e4fd289":"code","0834c6cd":"code","815b5882":"code","daf99877":"code","31becd84":"code","74567ef3":"code","cfd4becf":"code","91f31a88":"code","5d018590":"code","4238d2fd":"code","eaacb39e":"code","3d646579":"code","d5f3dae8":"code","07a30f77":"code","4bf76a3b":"code","03e0857d":"code","c3b06272":"code","971e0364":"code","e051e289":"code","0a28ac76":"code","22ba2257":"code","f00521d6":"code","98a45e53":"code","a46df453":"code","b83cb3c1":"code","caa95c0f":"code","445e7ec2":"code","96eae22c":"code","84af7956":"code","44f105d2":"code","90bc1357":"code","81df1431":"code","8fdd20ff":"code","98eb7e8a":"code","6f3177cf":"code","6725a0ee":"code","43d46eac":"code","6faee719":"code","fba63bae":"code","41c6f1aa":"code","3028cd2e":"code","6f95efa2":"code","d7f773b1":"code","c6bf7ffd":"code","648d63d8":"code","db926731":"code","5b2f14aa":"code","c5f5eb7f":"code","aa4a4080":"code","34f3d60f":"code","c70db677":"code","11cdb65e":"code","308512c6":"code","4e322738":"code","f6e90868":"code","2c34ea86":"code","0464e2dc":"code","24c66a63":"code","1b763f33":"code","8ada1079":"code","35c06080":"markdown","800d27cb":"markdown","8cb07f3f":"markdown","9121789f":"markdown","76d16926":"markdown","0cadc4b2":"markdown","55581e0b":"markdown","bc697edb":"markdown","a095c39d":"markdown","4f867643":"markdown","30a1a16d":"markdown","fd80af1a":"markdown","f65833df":"markdown","b1accd21":"markdown","4fc4836f":"markdown","36e9f7f6":"markdown","5ad7ed95":"markdown","51fb4d55":"markdown","e863e904":"markdown","2ae1563a":"markdown","410aa63b":"markdown","70f93bc6":"markdown","c0d3f7b2":"markdown","4b503c66":"markdown","ac878efb":"markdown","22a5fc34":"markdown","32cef521":"markdown","1333aac0":"markdown","cf858017":"markdown","db78af84":"markdown","91a0be70":"markdown","3ea91567":"markdown","54e53209":"markdown","3faa6fb3":"markdown","105ed635":"markdown","034447c3":"markdown","654bcbf0":"markdown","59662760":"markdown","ee9be3a3":"markdown","bffcbb3b":"markdown","00951558":"markdown","7ab5f9b6":"markdown","42808593":"markdown","345ab596":"markdown","6e322234":"markdown"},"source":{"2215210c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport pickle\nimport sklearn\nimport scipy\nfrom collections import Counter\n\nsns.set()","42bb214e":"#loading the dataset into the kernel and gaining an overview from the dataset\ndata = pd.read_csv(\"..\/input\/docspot\/datasets_228_482_diabetes.csv\")\ndata.head()","a842a8c7":"#let's understand the shape of our dataset by executing the code below\nprint(\"There are {} Rows and {} Columns in the dataset\".format(data.shape[0],data.shape[1]))","70b48d91":"#Let's gain some information about the data types of the dataset\ndata.info()","85b7ae6e":"#By executing the code below, we could gain some statistical infomration about our dataset\ndata.describe()","35a395ae":"data_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']","c8e24d74":"data[data_zeros] = np.where((data[data_zeros]==0), np.nan, data[data_zeros])","6e0e86c9":"#missing data\ntotal = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis=1, keys=[\"Total\", \"Percent\"])\nmissing_data.head(25)","7b459eae":"data[\"Glucose\"] = data[\"Glucose\"].fillna(data[\"Glucose\"].mean())\ndata.isnull().sum()","75a06fa9":"data[\"BloodPressure\"] = data[\"BloodPressure\"].fillna(data[\"BloodPressure\"].mean())\ndata.isnull().sum()","673d8000":"sns.boxplot(x = \"SkinThickness\", data = data )","2e4fd289":"data[\"SkinThickness\"].mean(), data[\"SkinThickness\"].median()","0834c6cd":"data[\"SkinThickness\"] = data[\"SkinThickness\"].fillna(data[\"SkinThickness\"].median())\ndata.isnull().sum()","815b5882":"sns.boxplot(x = \"Insulin\", data = data)","daf99877":"data['Insulin'].mean(), data['Insulin'].median()","31becd84":"data[\"Insulin\"] = data['Insulin'].fillna(data['Insulin'].median())\ndata.isnull().sum()","74567ef3":"sns.boxplot(x = \"BMI\", data = data)","cfd4becf":"data['BMI'].mean(), data['BMI'].median()","91f31a88":"data['BMI'] = data['BMI'].fillna(data['BMI'].median())\ndata.isnull().sum()","5d018590":"def outlier_detect(feature, data):\n    outlier_index = []\n\n    for each in feature:\n        Q1 = np.percentile(data[each], 25)\n        Q3 = np.percentile(data[each], 75)\n        IQR = Q3 - Q1\n        min_quartile = Q1 - 1.5*IQR\n        max_quartile = Q3 + 1.5*IQR\n        outlier_list = data[(data[each] < min_quartile) | (data[each] > max_quartile)].index\n        outlier_index.extend(outlier_list)\n        \n    outlier_index = Counter(outlier_index)\n    #If there are three or more outlier data features we must delete them. (n)\n    outlier_data = list(i for i, n in outlier_index.items() if n > 3)\n    return outlier_data","4238d2fd":"data.info()","eaacb39e":"outlier_data = outlier_detect(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n       'BMI', 'DiabetesPedigreeFunction','Insulin', 'Age'], data)\ndata.loc[outlier_data]","3d646579":"data = data.drop(outlier_data, axis=0).reset_index(drop=True)","d5f3dae8":"data_num = data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n       'BMI', 'DiabetesPedigreeFunction','Insulin', 'Age']]","07a30f77":"for i in data_num.columns:\n    plt.hist(data_num[i])\n    plt.title(i)\n    plt.show()","4bf76a3b":"data_feature = data.columns\n\nfor feature in data_feature:\n    p = sns.distplot(a = data[feature])\n    plt.show()","03e0857d":"sns.pairplot(data =data, hue = 'Outcome')\nplt.show()","c3b06272":"plt.figure(figsize=(12,8))\nsns.scatterplot(x =\"SkinThickness\", y= \"BMI\", hue = 'Outcome',data= data)\nplt.title(\"Distribution of the SkinThickness & BMI with the Target variable\")\nplt.show()","971e0364":"plt.figure(figsize=(12,8))\nsns.scatterplot(x =\"Glucose\", y= \"Insulin\", hue = 'Outcome',data= data)\nplt.title(\"Distribution of the Glucose & Insulin with the Target variable\")\nplt.show()","e051e289":"sns.countplot(x = 'Outcome', data= data)","0a28ac76":"data.info()","22ba2257":"g = sns.FacetGrid(data, row=\"Outcome\")\ng.map(sns.distplot, \"Age\", bins=25)\nplt.show()","f00521d6":"g = sns.FacetGrid(data, row=\"Outcome\")\ng.map(sns.distplot, \"Insulin\", bins=25)\nplt.show()","98a45e53":"g = sns.FacetGrid(data, row=\"Outcome\")\ng.map(sns.distplot, \"BloodPressure\", bins=25)\nplt.show()","a46df453":"corr_matrix = data.corr()\nplt.figure(figsize=(16,8))\nsns.heatmap(corr_matrix, cmap='Blues_r', annot=True)","b83cb3c1":"from scipy import stats\nfor feature in data.columns:\n    stats.probplot(data[feature], plot = plt)\n    plt.title(feature)\n    plt.show()","caa95c0f":"data[\"new\"] = data[\"SkinThickness\"] + data[\"BMI\"]","445e7ec2":"data.head()","96eae22c":"data.columns","84af7956":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\ndata[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age','new']] = scale.fit_transform(data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age','new']])\ndata.head()","44f105d2":"data.drop('SkinThickness', axis = 1, inplace = True)\ndata.drop('BMI', axis = 1, inplace =True)\ndata.head()","90bc1357":"X=data.drop('Outcome', axis = 1)\ny = data['Outcome']","81df1431":"X.head()","8fdd20ff":"y.head()","98eb7e8a":"from sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test = train_test_split(X,y, test_size =0.2, random_state = 0)","6f3177cf":"def svm_classifier(X_train, X_test, y_train, y_test):\n    \n    classifier_svm = SVC(kernel = 'rbf', random_state = 0)\n    classifier_svm.fit(X_train, y_train)\n\n    y_pred = classifier_svm.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    return print(f\"Train score : {classifier_svm.score(X_train, y_train)}\\nTest score : {classifier_svm.score(X_test, y_test)}\")\n#     print(\"-\"*100)\n#     print(cm)","6725a0ee":"def knn_classifier(X_train, X_test, y_train, y_test):\n    \n    classifier_knn = KNeighborsClassifier(metric = 'minkowski', p = 2)\n    classifier_knn.fit(X_train, y_train)\n\n    y_pred = classifier_knn.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    return print(f\"Train score : {classifier_knn.score(X_train, y_train)}\\nTest score : {classifier_knn.score(X_test, y_test)}\")\n#     print(\"-\"*100)\n#     print(cm)","43d46eac":"\ndef naive_classifier(X_train, X_test, y_train, y_test):\n    \n    classifier_naive = GaussianNB()\n    classifier_naive.fit(X_train, y_train)\n\n    y_pred = classifier_naive.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    return print(f\"Train score : {classifier_naive.score(X_train, y_train)}\\nTest score : {classifier_naive.score(X_test, y_test)}\")\n#     print(\"-\"*100)\n#     print(cm)","6faee719":"\ndef tree_classifier(X_train, X_test, y_train, y_test):\n    \n    classifier_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n    classifier_tree.fit(X_train, y_train)\n\n    y_pred = classifier_tree.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    return print(f\"Train score : {classifier_tree.score(X_train, y_train)}\\nTest score : {classifier_tree.score(X_test, y_test)}\")\n#     print(\"-\"*100)\n#     print(cm)","fba63bae":"\ndef forest_classifier(X_train, X_test, y_train, y_test):\n    classifier_forest = RandomForestClassifier(criterion = 'entropy', random_state = 0)\n    classifier_forest.fit(X_train, y_train)\n\n    y_pred = classifier_forest.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    return print(f\"Train score : {classifier_forest.score(X_train, y_train)}\\nTest score : {classifier_forest.score(X_test, y_test)}\")\n#     print(\"-\"*100)\n#     print(cm)","41c6f1aa":"def print_score(X_train, X_test, y_train, y_test):\n    print(\"SVM:\\n\")\n    svm_classifier(X_train, X_test, y_train, y_test)\n\n    print(\"-\"*100)\n    print()\n\n    print(\"KNN:\\n\")\n    knn_classifier(X_train, X_test, y_train, y_test)\n\n    print(\"-\"*100)\n    print()\n\n    print(\"Naive:\\n\")\n    naive_classifier(X_train, X_test, y_train, y_test)\n\n    print(\"-\"*100)\n    print()\n\n    print(\"Decision Tree:\\n\")\n    tree_classifier(X_train, X_test, y_train, y_test)\n\n    print(\"-\"*100)\n    print()\n\n    print(\"Random Forest:\\n\")\n    forest_classifier(X_train, X_test, y_train, y_test)\n","3028cd2e":"print_score(X_train, X_test, y_train, y_test)","6f95efa2":"classifier_svm = SVC(kernel = 'rbf', random_state = 0, probability=True)\nclassifier_svm.fit(X_train, y_train)\n\ny_pred = classifier_svm.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\ncm","d7f773b1":"pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","c6bf7ffd":"data['Outcome'].value_counts()","648d63d8":"from sklearn.metrics import roc_curve,roc_auc_score, classification_report\nprint(classification_report(y_test,y_pred))","db926731":"y_pred_prob = classifier_svm.predict_proba(X_test)[:,1]\ny_pred_prob","5b2f14aa":"fpr, tpr, threshold = roc_curve(y_test, y_pred_prob)\nprint(\"FPR:\\n\\n\", fpr)\n\n\nprint(\"-\"*100)\n\nprint(\"TPR:\\n\\n\", tpr)","c5f5eb7f":"plt.plot([0, 1], [0, 1], \"k--\", label = '50% AUC')\nplt.plot(fpr, tpr, label = \"Random Forest\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC Curve - Random Forest\")\nplt.show()","aa4a4080":"roc_auc_score(y_test,y_pred_prob)","34f3d60f":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier_svm, X = X_train, y = y_train, cv = 10)\nprint(accuracies.mean(), accuracies.std())","c70db677":"classifier_forest = RandomForestClassifier(criterion = 'entropy')\nclassifier_forest.fit(X_train, y_train)\ny_pred = classifier_forest.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\ncm","11cdb65e":"from sklearn.model_selection import GridSearchCV\nparameters = {\n    'n_estimators': [25, 50, 200, 300],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [14, 20, 25, 30]\n}","308512c6":"\ngrid_search = GridSearchCV(estimator = classifier_forest,\n                          param_grid = parameters,\n                          scoring = 'accuracy',\n                          cv = 10,\n                          n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nprint('best_accuracy = ',grid_search.best_score_)\nprint('best_parameters = ', grid_search.best_params_)","4e322738":"classifier_forest = RandomForestClassifier(criterion = 'gini', max_depth = 25, n_estimators = 200, random_state = 0)\nclassifier_forest.fit(X_train, y_train)\ny_pred = classifier_forest.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\ncm","f6e90868":"\n#param_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100], \n# 'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001], \n#'kernel': ['rbf', 'poly', 'linear']} ","2c34ea86":"#grid_search = GridSearchCV(estimator= classifier_svm, \n#param_grid = param_grid,\n#scoring='accuracy',\n#cv =10)\n#grid_search.fit(X_train,y_train)\n#print('best accuracy= ',grid_search.best_score_)\n#print('best parameters = ',grid_search.best_params_)","0464e2dc":"filename = 'diabetes_model.pkl'\npickle.dump(classifier_svm, open(filename, 'wb'))\n","24c66a63":"model = open('diabetes_model.pkl','rb')\nsvc = pickle.load(model)","1b763f33":"y_pred = svc.predict(X_test)","8ada1079":"confusion_matrix(y_test, y_pred)","35c06080":"* We have outlier here. Hence, it would impact on the mean of the column. It is highly better to fit the missing values with the median instead of mean of the feature","800d27cb":"* We have outlier here. Hence, it would impact on the mean of the column. It is highly better to fit the missing values with the median instead of mean of the feature","8cb07f3f":"# Welcome to my Notebook\n### in this notebook, we want to predict if the patient has the disease (Diabete) or not\n","9121789f":"# Correlation ","76d16926":"# Insulin & Outcome","0cadc4b2":"# Missing Values\n","55581e0b":"### Filling the missing values of the 'Insulin' column","bc697edb":"## Fitting data in various models","a095c39d":"### Classification Report (Accuracy, Precision, Recall, F1-score)","4f867643":"Great, we crated a new column which is the integration of the SkinThickness and BMI","30a1a16d":"* Interesting!!! Since there are many zeros in data and values of 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI' cannot be zero, Therefore, Converriting Zeros into NaN value","fd80af1a":"Filling the Missing Values of Glucose Column","f65833df":"# Data Visualization","b1accd21":"### Great! we do not have any missing values here\n### Let's remove the outlier from our dataset by executing the code below","4fc4836f":"* We have imbalaned data in our Outcome column\n* we will take care of that","36e9f7f6":"### we have an imbalanced data in Outcome column, therefore, we should look at the F1-score instead of Accuracy metrics","5ad7ed95":"* From the above table we can underestand that:\n* We have numeric values which contain of \"int64\" & \"float64\"","51fb4d55":"# Creating New features \n* We saw thatSkinThickness and BMI column have a postive correlation with the target","e863e904":"* * oops, we have encountered with overfittingbut, the best model is SVC.","2ae1563a":"### Improting the necessary libraries","410aa63b":"# AGE & Outcome","70f93bc6":"## Hyperparameter Tunning","c0d3f7b2":"#  Data Engineering\n* New Feature\n* Edit Feature\n* Drop Feature\n* Standardizing Data","4b503c66":"Filling the Missing Values of SkinThickness Column","ac878efb":"**ROC Score**","22a5fc34":"* Pair Plot to see Distribution of all data at a time and dependencies","32cef521":"Filling the Missing Values of BloodPressure Column","1333aac0":"### Filling the missing values of the \"BMI\" column","cf858017":"**Evaluating FPR, TPR, Threshold**","db78af84":"# observations:\n\n* SkinThickness and BMI column have a postive correlation with the target ","91a0be70":"# Outlier Detection","3ea91567":"# Drop the Features","54e53209":"# Exploratory Data Analysis EDA","3faa6fb3":"## Saving model using pickle","105ed635":"### Importing the dataset into the kernel","034447c3":"# Handling Missing Values\n\n* Filling NaN values with suitable mean and median values","654bcbf0":"**Plotting ROC Curve**","59662760":"![download.png](attachment:download.png)\n* Q1 = 1.Quartile 25%\n* Q2 = 2.Quartile 50% (median)\n* Q3 = 3.Quartile 75%\n* IQR = Q3 - Q1\n* Outlier data = (Q1 - 1.5 IQR ) U (Q3 + 1.5 IQR)","ee9be3a3":"**Getting probability instead of A\/B test**","bffcbb3b":"**Checking data is balanced or not**","00951558":"## Splitting data into train and test set","7ab5f9b6":"# Performance Metrics","42808593":"* We have outlier here. Hence, it would impact on the mean of the column. It is highly better to fit the missing values with the median instead of mean of the feature","345ab596":"### From the zero values, we can understand that they are Missing Values in our dataset\n### Hnece, We would convert the Zero values into the NAN values and then solving the missing values","6e322234":"## Standardizing Data"}}