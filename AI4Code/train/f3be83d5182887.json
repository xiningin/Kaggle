{"cell_type":{"4101644f":"code","8fdfed9e":"code","78198745":"code","45fd094b":"code","5384e2b9":"code","6cc42243":"code","87ebb658":"code","22d36dfa":"code","022b1ead":"code","606afcd0":"code","800ac6dd":"code","20ff0095":"code","3cb15ee6":"code","bab88636":"code","3614f61d":"code","90d8231c":"code","92001e20":"code","e54f962c":"code","9faa561f":"code","6476a66f":"code","d2a0172b":"code","ae39a622":"code","8055210e":"code","d582af39":"code","3339af30":"code","53d7b367":"code","5be60e33":"code","117aa3fe":"code","8f41dd8e":"code","86383333":"code","7fe06bfa":"code","904ffd78":"code","c2172b30":"code","4ca0271b":"code","80645715":"code","df747c1d":"code","0f0987d6":"code","b2fae2e5":"code","94464101":"code","f965e180":"code","aa223f41":"code","b0dca308":"code","23ea8976":"code","25b87a8c":"code","b9a4888d":"code","792af97b":"code","08090193":"code","5cf9d399":"code","fbb2a124":"code","1c2e54ff":"code","0b8c6014":"code","00dad9fc":"code","46074db8":"code","a939337a":"code","cdda1761":"code","6b5965d9":"code","e2664ba5":"markdown","71ed0466":"markdown","3ec1dc92":"markdown","be321782":"markdown","4ebca412":"markdown","1464a0a1":"markdown","5d75bc63":"markdown","a090e8c6":"markdown","44242223":"markdown","608cd05e":"markdown","101c0613":"markdown","65115845":"markdown","08d9492e":"markdown","29eb2a29":"markdown","a204284e":"markdown"},"source":{"4101644f":"# Loading the dataset\n\nimport numpy as np\nimport pandas as pd\nimport os\ntrain_data=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/titanic\/test.csv\")","8fdfed9e":"train_data.head()","78198745":"test_data.head()","45fd094b":"# see if null values are present\ntrain_data.info()","5384e2b9":"# see if null values are present\ntest_data.info()","6cc42243":"# import libraries to help visualize data\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")","87ebb658":"#gender frequrncy\ng = sns.FacetGrid(train_data, col=\"Sex\")\ng.map(sns.countplot, \"Survived\")\n\n# So, from below plots we get to know that even though men are more, \n#less male survive when compared to female","22d36dfa":"#Passenger Class\ngrid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.5, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();\n\n# Passengers in 1st class are more likely to survive","022b1ead":"#Age\nsns.FacetGrid(train_data, col=\"Survived\").map(sns.histplot, \"Age\", bins=25)","606afcd0":"# from plt below we can see, younger passenger are more likely to survive\nsns.scatterplot(x=train_data['PassengerId'],y=train_data['Age'],hue=train_data['Survived'])","800ac6dd":"# Fare\ng = sns.histplot(train_data['Fare'])\n\n## Fare is highly skewed towards left","20ff0095":"sns.scatterplot(x=train_data['Age'],y=train_data['Fare'],hue=train_data['Survived'])\n\n# Passengers with higher fare are more likely to survive","3cb15ee6":"# Siblings and spouse\nsns.barplot(x=\"SibSp\", y =\"Survived\", data=train_data)\nplt.show()","bab88636":"# parch\nsns.barplot(x=\"Parch\", y =\"Survived\", data=train_data)\nplt.show()","3614f61d":"# Family Size\n\nsns.scatterplot(x=train_data['PassengerId'],y=train_data['SibSp']+train_data['Parch'],hue=train_data['Survived'])\n\n# People with large family are less likely to survive","90d8231c":"#embarked\n\nsns.barplot(x=\"Embarked\", y =\"Survived\", data=train_data)\nplt.show()","92001e20":"sns.countplot(x=\"Embarked\", data=train_data)\nplt.show()","e54f962c":"# pair plot\n\ng = sns.pairplot(data=train_data, hue='Survived',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","9faa561f":"# first lets see how many null values are we dealing with\n\ntrain_data.isnull().sum()\n","6476a66f":"test_data.isnull().sum()","d2a0172b":"# Embarked : Only 2 null values in train data, we can replace the null values with 'S' as it is most common\n\ntrain_data['Embarked']=train_data['Embarked'].fillna('S')","ae39a622":"# lets combine train and test data before applying the transformtions\n\ndata = pd.concat([train_data.assign(ind=\"train\"), test_data.assign(ind=\"test\")], ignore_index=True)","8055210e":"# We can extract first letter of the cabin name, then group them accordingly\n\ndef extract_cabin_alpha(cabin):\n    cabin=str(cabin)\n    if(cabin=='nan'):\n        return \"M\"\n    else:\n        return cabin[0]\n\ndata['Deck']=data['Cabin'].apply(extract_cabin_alpha)","d582af39":"sns.barplot(x=data['Deck'],y=train_data['Survived'])","3339af30":"data['Deck'].value_counts()","53d7b367":"# Lets group values like : ABCT, DE, FG, M\ndef group_deck(deck):\n    if(deck in ['A','B','C','T']):\n        return \"ABC\"\n    elif(deck in ['D','E']):\n        return \"DE\"\n    elif (deck in ['F', 'G']):\n        return \"FG\"\n    else:\n        return \"M\"\n\ndata['Deck']=data['Deck'].apply(group_deck)","5be60e33":"sns.barplot(x=data['Deck'],y=train_data['Survived'])","117aa3fe":"data['Age'] = data.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","8f41dd8e":"data['Age'] = data['Age'].astype(int)\ndata.loc[ data['Age'] <= 15, 'Age'] = 0\ndata.loc[(data['Age'] > 15) & (data['Age'] <= 30), 'Age'] = 1\ndata.loc[(data['Age'] > 30) & (data['Age'] <= 45), 'Age'] = 2\ndata.loc[(data['Age'] > 45) & (data['Age'] <= 60), 'Age'] = 3\ndata.loc[ data['Age'] > 60, 'Age'] = 4","86383333":"data.head()","7fe06bfa":"data.describe()","904ffd78":"# Making bins according to distribution\ndata['Fare_bin'] = pd.cut(data['Fare'], bins=[0.0,7.9,14.45,31,512], labels=[0,1,\n                                                                                  2,3])\ndata['Fare_bin']=data['Fare_bin'].fillna(0)\n\ndata['Fare_bin']=data['Fare_bin'].astype(int)","c2172b30":"# all unique titles\ndef disp_title(name):\n    title=name.split(',')[1].split('.')[0].strip()\n    return title\ndata['Name'].apply(disp_title).unique()","4ca0271b":"# grouping titles\n\ndef get_title(name):\n    title=name.split(',')[1].split('.')[0].strip()\n    return title.strip()\ndata['Title']=data['Name'].apply(get_title)","80645715":"def group_titles(df):\n    title=df['Title']\n    if title in ['Don', 'Major', 'Jonkheer','Sir']:\n        return 'Mr'\n    elif title in ['the Countess', 'Mme', 'Lady', 'Dona']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title in ['Dr', 'Capt', 'Rev', 'Col']:\n        if df['Sex']=='male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n\ndata['Title']=data.apply(group_titles, axis=1)","df747c1d":"sns.countplot(x=data['Title'])","0f0987d6":"data['FamilySize']=data['SibSp']+data['Parch']","b2fae2e5":"data.head()","94464101":"final_data=data.drop(columns=['Fare','Name','Ticket','Cabin'])","f965e180":"test_data_pre, train_data_pre= final_data[final_data[\"ind\"].eq(\"test\")], final_data[final_data[\"ind\"].eq(\"train\")]","aa223f41":"# Removing Unnecessary columns\ntrain_data_pre=train_data_pre.drop(columns=['PassengerId','ind','SibSp','Parch'])\ntest_data_pre=test_data_pre.drop(columns=['Survived','PassengerId','ind','SibSp','Parch'])","b0dca308":"#spliting traindata into X and y\nX=train_data_pre.drop(columns=['Survived'])\ny=train_data_pre['Survived']","23ea8976":"#Correlation Matrix\nsns.heatmap(X.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':14})\nfig=plt.gcf()\nfig.set_size_inches(14,10)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","25b87a8c":"# Convering Categorical columns to numerical\n\ntraindf_X = pd.get_dummies(X, columns = [\"Sex\",\"Title\",\"Embarked\",\"Fare_bin\",'Deck'],\n                             prefix=[\"Sex\",\"Title\",\"Em_type\",\"Fare_type\",'Deck'])\ntestdf = pd.get_dummies(test_data_pre, columns = [\"Sex\",\"Title\",\"Embarked\",\"Fare_bin\",'Deck'],\n                             prefix=[\"Sex\",\"Title\",\"Em_type\",\"Fare_type\",'Deck'])","b9a4888d":"from sklearn import metrics\n\n# lets first define a function that'll help us know how good\/bad our model is doing\ndef get_scores(y_preds,y):\n    return {\n        'Accuracy':metrics.accuracy_score(y_preds,y),\n        'Precision':metrics.precision_score(y_preds,y),\n        'Recall':metrics.recall_score(y_preds,y),\n        'F1':metrics.f1_score(y_preds,y),\n        'ROC_AUC': metrics.roc_auc_score(y_preds,y)\n    }","792af97b":"# split data into train and val\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(traindf_X, y, test_size=0.3, random_state=42)","08090193":"from sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier","5cf9d399":"# lets define a function that trains model for us\n\ndef train_model(model):\n    model_=model\n    model_.fit(X_train,y_train)\n    y_preds=model_.predict(X_val)\n    return get_scores(y_preds,y_val)","fbb2a124":"model_list=[\n            DecisionTreeClassifier(random_state=42), \n            RandomForestClassifier(random_state=42),\n            XGBClassifier(random_state=42), \n            LGBMClassifier(random_state=42, is_unbalance=True), \n            LogisticRegression(random_state=42),\n            svm.SVC(random_state=42),\n            CatBoostClassifier(random_state=42,verbose=0),\n            AdaBoostClassifier(random_state=42)\n           ]\nmodel_names=['Decision Tree', 'Random Forest', 'XG Boost', 'Light GBM', 'Logistic Regression','SVM','CatBoost','AdaBoost']","1c2e54ff":"# Now lets train all the models and see how are they doing\n\nscores = pd.DataFrame(columns=['Name','Accuracy','Precision',\n                                'Recall',\n                                'F1',\n                                'ROC_AUC'])\nfor i in range(len(model_list)):\n    score=train_model(model_list[i])\n    scores.loc[i]=[model_names[i]]+list(score.values())\n\nfigure, axis = plt.subplots(2, 3)\nfigure.set_figheight(15)\nfigure.set_figwidth(20)\n\nfor i in range(2):\n    for j in range(3):\n        axis[i,j].set_xlim([.5,.9])\n        \naxis[0, 0].barh(scores['Name'],scores['Accuracy'],height=.5)\naxis[0, 0].set_title(\"Accuracy Score\")\n  \naxis[0, 1].barh(scores['Name'],scores['Precision'],height=.5)\naxis[0, 1].set_title(\"Precision\")\n\naxis[1, 0].barh(scores['Name'],scores['Recall'],height=.5)\naxis[1, 0].set_title(\"Recall\")\n\naxis[1, 2].barh(scores['Name'],scores['F1'],height=.5)\naxis[1, 2].set_title(\"F1\")\n\naxis[0, 2].barh(scores['Name'],scores['ROC_AUC'],height=.5)\naxis[0, 2].set_title('ROC_AUC')\n\naxis[1, 1].set_visible(False)\n\nplt.show()","0b8c6014":"# Let's start by defining what all parameters we want to tune for all 5 models\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils.fixes import loguniform\nparam_grids = [\n\n    {\n        'max_depth': [5,7,10,20,50],\n        'min_samples_leaf': [4, 5, 6, 7],\n        'min_samples_split': [8, 10,7],\n    },\n    {\n        'max_depth': [5,7, 20],\n        'min_samples_leaf': [4, 5, 6, 7],\n        'min_samples_split': [6,7,8],\n        'n_estimators': [100,500],\n        'oob_score' : [True],\n        'max_features' :['auto'],\n    },\n    {\n        'max_depth': [3, 5, 9], \n        'n_estimators': [5, 50, 100],\n        'learning_rate': [0.01]\n    },\n    {\n        'learning_rate': [0.1,.2],\n        'num_leaves': [100,200,150,300],\n        'n_estimators':[5, 20, 50, 100],\n        'boosting_type' : ['gbdt'],\n        'objective' : ['binary'],\n        'max_depth' : [10,25,50,100,150], \n        'colsample_bytree' : [0,3,0.5,0.7],\n        'subsample' : [0.3,0.5,0.7],\n        'min_split_gain' : [0.01],\n    },\n    {\n        'penalty' : ['l1', 'l2'],\n        'C' : np.logspace(-4, 4, 20),\n        'solver' : ['liblinear']\n    },\n    {\n        'C': [0.1,1, 10, 100], \n        'gamma': [1,0.1,0.01,0.001],\n        'kernel': ['rbf', 'poly', 'sigmoid']\n    },\n    {\n        'learning_rate': [0.1],\n        'depth': [4, 10],\n        'l2_leaf_reg': [ 5, 7, 9]\n    },\n    {\n        'n_estimators':[10,50,250,1000],\n        'learning_rate':[0.01,0.1]\n    }\n]","00dad9fc":"# Now that we have defined the parameters, we can now start the search\n\ntuned_scores=scores.drop(scores.index)\ntuned_models=[]\ndef grid_search_util(i):\n    grid_search = GridSearchCV(estimator = model_list[i], param_grid = param_grids[i], \n                          cv = 3, n_jobs = -1,verbose=1)\n    grid_search.fit(X_train, y_train)\n    return grid_search.best_estimator_\n\nfor i in range(len(model_list)):\n    model=grid_search_util(i)\n    tuned_models.append(model)\n    score=train_model(model)\n    tuned_scores.loc[i]=[model_names[i]]+list(score.values())\n    print(model_names[i],\" Done\")","46074db8":"for i in tuned_models:\n    print(i)","a939337a":"figure, axis = plt.subplots(2, 3)\nfigure.set_figheight(15)\nfigure.set_figwidth(20)\n\nfor i in range(2):\n    for j in range(3):\n        axis[i,j].set_xlim([.5,.9])\naxis[0, 0].barh(tuned_scores['Name'],tuned_scores['Accuracy'],height=.5)\naxis[0, 0].set_title(\"Accuracy Score\")\n\n\naxis[0, 1].barh(tuned_scores['Name'],tuned_scores['Precision'],height=.5)\naxis[0, 1].set_title(\"Precision\")\n\naxis[1, 0].barh(tuned_scores['Name'],tuned_scores['Recall'],height=.5)\naxis[1, 0].set_title(\"Recall\")\n\naxis[1, 2].barh(tuned_scores['Name'],tuned_scores['F1'],height=.5)\naxis[1, 2].set_title(\"F1\")\n\naxis[0, 2].barh(tuned_scores['Name'],tuned_scores['ROC_AUC'],height=.5)\naxis[0, 2].set_title('ROC_AUC')\n\naxis[1, 1].set_visible(False)\nplt.show()","cdda1761":"maj=np.zeros((len(testdf)))\n\nfor i in range(len(model_list)):\n    model=tuned_models[i].fit(X_train,y_train)\n    maj+=model.predict(testdf)\n\nfor i in range(len(maj)):\n    if(maj[i]==8):\n        maj[i]=1\n    else:\n        maj[i]=0","6b5965d9":"# saving the results for submission\npredictions = [int(x) for x in maj]\nsubmission = pd.DataFrame({'PassengerId':test_data['PassengerId'],'Survived':predictions})\nsubmission.to_csv('submission.csv',index = False)","e2664ba5":"### * Embarked","71ed0466":"#### In total we have 10 feature for training out models.\n#### Out of which Name [ Sex, Ticket, Cabin, Embarked ] are categorical and rest are numerical\n#### One more thing to notice is that we have many null values, but don't worry we'll deal with them later","3ec1dc92":"# 2) Feature Engineering\n\n#### Now that we have some knowledge of the data we're dealing with, we can get started with feature engineering","be321782":"### * SibSp and Parch : \n#### We can make a new feature FamilySize, which will be sum of the two","4ebca412":"#### * Cabin\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/0\/0d\/Olympic_%26_Titanic_cutaway_diagram.png\/330px-Olympic_%26_Titanic_cutaway_diagram.png)","1464a0a1":"# 1) Eploratory Data Analysis\n \n#### Now that we have the dataset loaded into pandas dataframe, lets explore it !","5d75bc63":"### * Fare","a090e8c6":"#### Time for majority voting. So, this is not exactly \"majority\" voting. Basically, I predict survival only if all models predict survival. Now the reason for that is debatable. On experimenting I found it works the best. Would love to hear you thoughts on this","44242223":"## Conclusion\n#### Though this approach works well, there is a lot of scope for improvement. As you can see I applied only basic feature engineering. Would love to know what all can we further do to perform better.\n\n### Thanks for reading..! Make sure to upvote if you liked the post. ","608cd05e":"# 3) Model Selection and Hyperparameter Tuning\n\n#### Phew, finally we can get into training machine learning models !","101c0613":"### * Name","65115845":"#### Now, regarding the model I chose. I chose total 8 model which you can see below. Now the reason I chose so many models was **Majority Voting**. \n#### YES Majority Voting\n#### Let's first train these models without any tuning and see how are they doing","08d9492e":"## Hyperparameter Tuning\n\n#### Alright, It's time for hyper parameter tuning!. We'll use GridSearchCV to find the best permutaion of hyper parameters for the model\n#### Note : I have reduced parameters at takes a lot of time","29eb2a29":"# Hey Everyone !\n\n#### This is my first notebook on Kaggle.! \n#### In this notebook, we'll go through the appoach I took for solving this problem.\n#### So, we can divide the task into 3 phases\n#### 1. EDA (Exploratory Data Analysis)\n#### 2. Feature Engineering \n#### 3. Model Selection and Hypeparameter Tuning\n\n#### And Finally we'll talk about the approach I took that got me into top 3% (79.186)\n\n### **Lets Get Started !**","a204284e":"### * Age\n\n#### The idea is, replace Age by median of correponding Sex and Pclass\n"}}