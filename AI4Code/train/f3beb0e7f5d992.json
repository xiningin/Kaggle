{"cell_type":{"af31bf07":"code","57e06640":"code","b7ccccc5":"code","1eb2938b":"code","6e4e3704":"code","1d470d88":"code","4c0e80ef":"code","36a73792":"code","84fc89f9":"code","594fd17f":"code","fa528cbd":"code","64ed9eb1":"code","fb9ae094":"code","36be203f":"code","efc74148":"code","4b551210":"code","7e795097":"code","99cdddc1":"code","d86d190f":"code","54f3a2e6":"markdown","3db3f202":"markdown","9512d0b5":"markdown","8e324a2a":"markdown","5fa486c1":"markdown","d0784905":"markdown","f7d8c2bd":"markdown","b2530b91":"markdown","2a4a88d2":"markdown","9a37fc7f":"markdown","63043b4c":"markdown","9f9a09be":"markdown","153c726e":"markdown","23191c26":"markdown","7da08b05":"markdown","87bae454":"markdown","899ca836":"markdown","857a50e6":"markdown","343a2e54":"markdown","bdc1ea92":"markdown","bf260d39":"markdown"},"source":{"af31bf07":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import preprocessing, model_selection, metrics\nimport lightgbm as lgb\n\ncolor = sns.color_palette()\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 9999","57e06640":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train rows and columns : \", train_df.shape)\nprint(\"Test rows and columns : \", test_df.shape)","b7ccccc5":"train_df.head()","1eb2938b":"test_df.head()","6e4e3704":"plt.figure(figsize=(12,8))\nsns.distplot(train_df[\"target\"].values, bins=50, kde=False)\nplt.xlabel('Target', fontsize=12)\nplt.title(\"Target Histogram\", fontsize=14)\nplt.show()","1d470d88":"plt.figure(figsize=(12,8))\nsns.distplot( np.log(train_df[\"target\"].values), bins=50, kde=False)\nplt.xlabel('Target', fontsize=12)\nplt.title(\"Log of Target Histogram\", fontsize=14)\nplt.show()","4c0e80ef":"    np.sum(train_df.isna().sum())","36a73792":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","84fc89f9":"unique_df = train_df.nunique().reset_index()\nunique_df.columns = [\"col_name\", \"unique_count\"]\nconstant_df = unique_df[unique_df[\"unique_count\"]==1]\nconstant_df.shape","594fd17f":"str(constant_df.col_name.tolist())","fa528cbd":"print(train_df[train_df.columns[~train_df.columns.isin(constant_df.col_name.tolist())]].shape)\ntrain_df_without_uniques = train_df[train_df.columns[~train_df.columns.isin(constant_df.col_name.tolist())]]","64ed9eb1":"from scipy.stats import spearmanr\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nlabels = []\nvalues = []\nfor col in train_df_without_uniques.columns:\n    if col not in [\"ID\", \"target\"]:\n        labels.append(col)\n        values.append(spearmanr(train_df_without_uniques[col].values, train_df_without_uniques[\"target\"].values)[0])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n \ncorr_df = corr_df[(corr_df['corr_values']>0.1) | (corr_df['corr_values']<-0.1)]\nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,30))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='b')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","fb9ae094":"cols_to_use = corr_df[(corr_df['corr_values']>0.11) | (corr_df['corr_values']<-0.11)].col_labels.tolist()\n\ntemp_df = train_df[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True, cmap=\"YlGnBu\", annot=True)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()","36be203f":"### Get the X and y variables for building model ###\ntrain_X = train_df.drop(constant_df.col_name.tolist() + [\"ID\", \"target\"], axis=1)\ntest_X = test_df.drop(constant_df.col_name.tolist() + [\"ID\"], axis=1)\ntrain_y = np.log1p(train_df[\"target\"].values)","efc74148":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)\n\n## plot the importances ##\nfeat_names = train_X.columns.values\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","4b551210":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 30,\n        \"learning_rate\" : 0.01,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.7,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=200, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result","7e795097":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\npred_test_full = 0\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n    pred_test_full += pred_test\npred_test_full \/= 5.\npred_test_full = np.expm1(pred_test_full)","99cdddc1":"# Making a submission file #\nsub_df = pd.DataFrame({\"ID\":test_df[\"ID\"].values})\nsub_df[\"target\"] = pred_test_full\nsub_df.to_csv(\"baseline_lgb.csv\", index=False)","d86d190f":"### Feature Importance ###\nfig, ax = plt.subplots(figsize=(12,18))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","54f3a2e6":"#### Dealing with missing values","3db3f202":"#### Correlation of features with target:\n\nNow let us find the correlation of the variables with target and plot them.\n\nThanks to @Heads or Tails kernel and Tariq's comment, it might be a good idea to use Spearman correlation inplace of pearson since spearman is computed on ranks and so depicts monotonic relationships while pearson is on true values and depicts linear relationships.\n\nThere are thousands of variables and so plotting all of them will give us a cluttered plot. So let us take only those variables whose absolute spearman correlation coefficient is more than 0.1 (just to reduce the number of variables) and plot them.","9512d0b5":"This model scored 1.47 RMSLE on the public LB. We did not do any feature selection (apart from removing the constant variables), feature engineering and parameter tuning. So doing that will further imporve the score. We can use this as our baseline model for any further modeling.\n\nNow let us look at the feature importance of this model.","8e324a2a":"There are quite a few variables with absolute correlation greater than 0.1","5fa486c1":"#### Feature Importance & Baseline - Light GBM:\n\nNow let us build a Light GBM model to get the feature importance.\n\nApart from feature importance, let us also get predictions on the test set using this model and keep them as baseline predictions.\n\nBelow code is a custom helper function for Light GBM.","d0784905":"Here again the top two important features are same as that of the Extra trees model.\n\nSo we could also do some form of feature selection using these feature importances and improve our models further.\n\nMay be in the next versions, let us look at the top variables from the non-linear models and do some more further analysis to understand tham.!","f7d8c2bd":"That is incredible, no missing variables at all! :D","b2530b91":"Seems like none of the selected variables have spearman correlation more than 0.7 with each other.\n\nThe above plots helped us in identifying the important individual variables which are correlated with target. However we generally build many non-linear models in Kaggle competitions. So let us build some non-linear models and get variable importance from them.\n\nIn this notebook, we will build two models to get the feature importances - Extra trees and Light GBM. It could also help us to see if the important features coming out from both of them are consistent. Let us first start with ET model.","2a4a88d2":"#### Lets view our train and test sets:","9a37fc7f":"#### Columns with constant values:\nGenerally we get problems with many columns, when there are a few columns with constant value in train set. So we can check that one as well.","63043b4c":"#### Correlation Heat Map:\n\nNow let us take these variables whose absolute value of correlation with the target is greater than 0.11 (just to reduce the number of features fuether) and do a correlation heat map.\n\nThis is just done to identify if there are any strong monotonic relationships between these important features. If the values are high, then probably we can choose to keep one of those variables in the model building process. Please note that we are doing this only for the very few features and feel free to add more features to explore more.","9f9a09be":"#### train_df head:","153c726e":"#### target variable distribution:","23191c26":"Let us do KFold cross validation and average the predictions of the test set.","7da08b05":"So the validation set RMSLE of the folds range from 1.40 to 1.46.\n\nLet us write the predictions of the model and write it to a file","87bae454":"#### test_df head:","899ca836":"#### Feature Importance - Extra trees model\n\nOur Evaluation metric for the competition is RMSLE. So let us use log of the target variable to build our models. Also please note that we are removing those variables with constant values (that we identified earlier).","857a50e6":"Hello World! The digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner\u2026 and often before they\u00b4ve even realized they need the service. In their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before.\n\nIn this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.\n\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.","343a2e54":"#### Data types:","bdc1ea92":"So we have 256 columns with constant values in the train set. Probably it is a good idea to remove them from the training. Just printing out the names below for ease.","bf260d39":"##### log transformation of the target variable:"}}