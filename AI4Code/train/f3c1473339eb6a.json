{"cell_type":{"70de0789":"code","33c3281e":"code","2cb28aa0":"code","efd61b89":"code","0fb5740e":"code","09d1f748":"code","0b5fc9bc":"code","802e38d3":"code","d4e61d1a":"code","b4e952e1":"code","d7a95692":"code","de5e4f1f":"code","bf19df16":"code","415a2df7":"code","b61fcdd7":"code","8a5fad22":"markdown","cf7557cd":"markdown","038ce758":"markdown"},"source":{"70de0789":"!conda remove -y greenlet\n!pip install pytorch-pretrained-bert\n!pip install allennlp\n!pip install https:\/\/github.com\/ceshine\/pytorch_helper_bot\/archive\/0.0.5.zip","33c3281e":"!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-development.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-test.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-validation.tsv -q","2cb28aa0":"import os\n\n# This variable is used by helperbot to make the training deterministic\nos.environ[\"SEED\"] = \"828\"\n\nimport logging\nimport gc\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\nfrom allennlp.modules.span_extractors import SelfAttentiveSpanExtractor, EndpointSpanExtractor\n\nfrom helperbot import (\n    TriangularLR, BaseBot, WeightDecayOptimizerWrapper,\n    GradualWarmupScheduler\n)","efd61b89":"BERT_MODEL = 'bert-large-uncased'\nCASED = False","0fb5740e":"class Head(nn.Module):\n    \"\"\"The MLP submodule\"\"\"\n    def __init__(self, bert_hidden_size: int):\n        super().__init__()\n        self.bert_hidden_size = bert_hidden_size\n        # self.span_extractor = SelfAttentiveSpanExtractor(bert_hidden_size)\n        self.span_extractor = EndpointSpanExtractor(\n            bert_hidden_size, \"x,y,x*y\"\n        )\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(bert_hidden_size * 7),\n            nn.Dropout(0.1),\n            nn.Linear(bert_hidden_size * 7, 64),           \n            nn.ReLU(),\n            nn.BatchNorm1d(64),      \n            nn.Dropout(0.5),\n            nn.Linear(64, 3)\n        )\n        for i, module in enumerate(self.fc):\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                nn.init.constant_(module.weight, 1)\n                nn.init.constant_(module.bias, 0)\n                print(\"Initing batchnorm\")\n            elif isinstance(module, nn.Linear):\n                if getattr(module, \"weight_v\", None) is not None:\n                    nn.init.uniform_(module.weight_g, 0, 1)\n                    nn.init.kaiming_normal_(module.weight_v)\n                    print(\"Initing linear with weight normalization\")\n                    assert model[i].weight_g is not None\n                else:\n                    nn.init.kaiming_normal_(module.weight)\n                    print(\"Initing linear\")\n                nn.init.constant_(module.bias, 0)\n                \n    def forward(self, bert_outputs, offsets):\n        assert bert_outputs.size(2) == self.bert_hidden_size\n        spans_contexts = self.span_extractor(\n            bert_outputs, \n            offsets[:, :4].reshape(-1, 2, 2)\n        ).reshape(offsets.size()[0], -1)\n        return self.fc(torch.cat([\n            spans_contexts,\n            torch.gather(\n                bert_outputs, 1,\n                offsets[:, [4]].unsqueeze(2).expand(-1, -1, self.bert_hidden_size)\n            ).squeeze(1)\n        ], dim=1))","09d1f748":"def tokenize(row, tokenizer):\n    break_points = sorted(\n        [\n            (\"A\", row[\"A-offset\"], row[\"A\"]),\n            (\"B\", row[\"B-offset\"], row[\"B\"]),\n            (\"P\", row[\"Pronoun-offset\"], row[\"Pronoun\"]),\n        ], key=lambda x: x[0]\n    )\n    tokens, spans, current_pos = [], {}, 0\n    for name, offset, text in break_points:\n        tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n        # Make sure we do not get it wrong\n        assert row[\"Text\"][offset:offset+len(text)] == text\n        # Tokenize the target\n        tmp_tokens = tokenizer.tokenize(row[\"Text\"][offset:offset+len(text)])\n        spans[name] = [len(tokens), len(tokens) + len(tmp_tokens) - 1] # inclusive\n        tokens.extend(tmp_tokens)\n        current_pos = offset + len(text)\n    tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n    assert spans[\"P\"][0] == spans[\"P\"][1]\n    return tokens, (spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]])\n\n\nclass GAPDataset(Dataset):\n    \"\"\"Custom GAP Dataset class\"\"\"\n    def __init__(self, df, tokenizer, labeled=True):\n        self.labeled = labeled\n        if labeled:\n            self.y = df.target.values.astype(\"uint8\")\n        \n        self.offsets, self.tokens = [], []\n        for _, row in df.iterrows():\n            tokens, offsets = tokenize(row, tokenizer)\n            self.offsets.append(offsets)\n            self.tokens.append(tokenizer.convert_tokens_to_ids(\n                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n        \n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        if self.labeled:\n            return self.tokens[idx], self.offsets[idx], self.y[idx]\n        return self.tokens[idx], self.offsets[idx], None\n\n    \ndef collate_examples(batch, truncate_len=490):\n    \"\"\"Batch preparation.\n    \n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"    \n    transposed = list(zip(*batch))\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])\n        tokens[i, :len(row)] = row\n    token_tensor = torch.from_numpy(tokens)\n    # Offsets\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    # Labels\n    if len(transposed) == 2:\n        return token_tensor, offsets, None\n    labels = torch.LongTensor(transposed[2])\n    return token_tensor, offsets, labels\n\n\nclass GAPModel(nn.Module):\n    \"\"\"The main model.\"\"\"\n    def __init__(self, bert_model: str, device: torch.device, use_layer: int = -2):\n        super().__init__()\n        self.device = device\n        self.use_layer = use_layer\n        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n            self.bert_hidden_size = 768\n        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n            self.bert_hidden_size = 1024\n        else:\n            raise ValueError(\"Unsupported BERT model.\")\n        self.bert = BertModel.from_pretrained(bert_model).to(device)\n        self.head = Head(self.bert_hidden_size).to(device)\n    \n    def forward(self, token_tensor, offsets):\n        token_tensor = token_tensor.to(self.device)\n        bert_outputs, _ =  self.bert(\n            token_tensor, attention_mask=(token_tensor > 0).long(), \n            token_type_ids=None, output_all_encoded_layers=True)\n        head_outputs = self.head(bert_outputs[self.use_layer], offsets.to(self.device))\n        return head_outputs            \n\n\n# Adapted from fast.ai library\ndef children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\n            \ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n    \n    \nclass GAPBot(BaseBot):\n    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n        avg_window=100, log_dir=\".\/cache\/logs\/\", log_level=logging.INFO,\n        checkpoint_dir=\".\/cache\/model_cache\/\", batch_idx=0, echo=False,\n        device=\"cuda:0\", use_tensorboard=False):\n        super().__init__(\n            model, train_loader, val_loader, \n            optimizer=optimizer, clip_grad=clip_grad,\n            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n            batch_idx=batch_idx, echo=echo,\n            device=device, use_tensorboard=use_tensorboard\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.loss_format = \"%.6f\"\n        \n    def extract_prediction(self, tensor):\n        return tensor\n    \n    def snapshot(self):\n        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n        loss = self.eval(self.val_loader)\n        loss_str = self.loss_format % loss\n        self.logger.info(\"Snapshot loss %s\", loss_str)\n        self.logger.tb_scalars(\n            \"losses\", {\"val\": loss},  self.step)\n        target_path = (\n            self.checkpoint_dir \/ \"best.pth\")        \n        if not self.best_performers or (self.best_performers[0][0] > loss):\n            torch.save(self.model.state_dict(), target_path)\n            self.best_performers = [(loss, target_path, self.step)]\n        self.logger.info(\"Saving checkpoint %s...\", target_path)\n        assert Path(target_path).exists()\n        return loss","0b5fc9bc":"def extract_target(df):\n    df[\"Neither\"] = 0\n    df.loc[~(df['A-coref'] | df['B-coref']), \"Neither\"] = 1\n    df[\"target\"] = 0\n    df.loc[df['B-coref'] == 1, \"target\"] = 1\n    df.loc[df[\"Neither\"] == 1, \"target\"] = 2\n    print(df.target.value_counts())\n    return df","802e38d3":"df_train = pd.concat([\n    pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\"),\n    pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\n], axis=0)\ndf_test = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\ndf_train = extract_target(df_train)\ndf_test = extract_target(df_test)\nsample_sub = pd.read_csv(\"..\/input\/sample_submission_stage_1.csv\")\nassert sample_sub.shape[0] == df_test.shape[0]","d4e61d1a":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=CASED,\n    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n)","b4e952e1":"test_ds = GAPDataset(df_test, tokenizer)\ntest_loader = DataLoader(\n    test_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","d7a95692":"skf = StratifiedKFold(n_splits=5, random_state=191)\n\nval_preds, test_preds, val_ys, val_losses = [], [], [], []\nfor train_index, valid_index in skf.split(df_train, df_train[\"target\"]):\n    print(\"=\" * 20)\n    print(f\"Fold {len(val_preds) + 1}\")\n    print(\"=\" * 20)\n    train_ds = GAPDataset(df_train.iloc[train_index], tokenizer)\n    val_ds = GAPDataset(df_train.iloc[valid_index], tokenizer)\n    train_loader = DataLoader(\n        train_ds,\n        collate_fn = collate_examples,\n        batch_size=32,\n        num_workers=2,\n        pin_memory=True,\n        shuffle=True,\n        drop_last=True\n    )\n    val_loader = DataLoader(\n        val_ds,\n        collate_fn = collate_examples,\n        batch_size=128,\n        num_workers=2,\n        pin_memory=True,\n        shuffle=False\n    )\n    model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n    # You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\n    set_trainable(model.bert, False)\n    set_trainable(model.head, True)\n    optimizer = WeightDecayOptimizerWrapper(\n        torch.optim.Adam(model.parameters(), lr=2e-3),\n        0.05\n    )\n    # optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n    bot = GAPBot(\n        model, train_loader, val_loader,\n        optimizer=optimizer, echo=True,\n        avg_window=40\n    )\n    gc.collect()\n    steps_per_epoch = len(train_loader) \n    n_steps = steps_per_epoch * 15\n    bot.train(\n        n_steps,\n        log_interval=steps_per_epoch \/\/ 2,\n        snapshot_interval=steps_per_epoch,\n#         scheduler=GradualWarmupScheduler(optimizer, 20, int(steps_per_epoch * 4),\n#             after_scheduler=CosineAnnealingLR(\n#                 optimizer, n_steps - int(steps_per_epoch * 4)\n#             )\n#         )\n        scheduler=TriangularLR(\n            optimizer, 20, ratio=3, steps_per_cycle=n_steps)\n    )\n    # Load the best checkpoint\n    bot.load_model(bot.best_performers[0][1])\n    bot.remove_checkpoints(keep=0)    \n    val_preds.append(torch.softmax(bot.predict(val_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())\n    val_ys.append(df_train.iloc[valid_index].target.astype(\"uint8\").values)\n    val_losses.append(log_loss(val_ys[-1], val_preds[-1]))\n    bot.logger.info(\"Confirm val loss: %.4f\", val_losses[-1])\n    test_preds.append(torch.softmax(bot.predict(test_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())","de5e4f1f":"val_losses","bf19df16":"final_test_preds = np.mean(test_preds, axis=0)\nfinal_test_preds.shape","415a2df7":"log_loss(df_test.target, final_test_preds)","b61fcdd7":"# Create submission file\ndf_sub = pd.DataFrame(final_test_preds, columns=[\"A\", \"B\", \"NEITHER\"])\ndf_sub[\"ID\"] = df_test.ID\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","8a5fad22":"\"pytorch_helper_bot\" is a thin abstraction of some common PyTorch training routines. It can easily be replaced, so you can mostly ignore it and focus on the preprocessing and model definition instead.","cf7557cd":"*2019\/03\/23 Update*:  Inspired by [hanxiao\/bert-as-service\n](https:\/\/github.com\/hanxiao\/bert-as-service), the hidden states (context vectors) of the second-to-last layer is used instead of the ones from the last layer. \n\n> Q: Why not the last hidden layer? Why second-to-last?\n\n> A: The last layer is too closed to the target functions (i.e. masked language model and next sentence prediction) during pre-training, therefore may be biased to those targets. If you question about this argument and want to use the last hidden layer anyway, please feel free to set pooling_layer=-1.","038ce758":"Changes made:\n\n1.  **KFold Validation and Bagging**. Currently only the MLP layers are trained, so a lot of repetitive feed-forwards are incurred between folds. But the current workflow allows you to unfreeze at least some of the layers of Bert encoder when you have the computing resources and big enough dataset.\n2. **[EndpointSpanExtractor](https:\/\/github.com\/allenai\/allennlp\/blob\/580dc8b0e2c6491d4d75b54c3b15b34b462e0c67\/allennlp\/modules\/span_extractors\/endpoint_span_extractor.py)** is used instead of [SelfAttentiveSpanExtractor](https:\/\/github.com\/allenai\/allennlp\/blob\/580dc8b0e2c6491d4d75b54c3b15b34b462e0c67\/allennlp\/modules\/span_extractors\/self_attentive_span_extractor.py). The self-attention layer did not work so well (probably because of the small size of the dataset). This extractor only use the start and the end of the span. \n3. Slight modification in Dataset Preparation Code. A small bug is fixed and the label transformation is moved out of the dataset class.\n4. Add a weight decay wrapper (to implement a variant of [AdamW](https:\/\/arxiv.org\/pdf\/1711.05101.pdf)). Not sure if it helps, but it's there for you to tune."}}