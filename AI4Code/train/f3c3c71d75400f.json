{"cell_type":{"286a2e53":"code","753f8a2a":"code","aa35369d":"code","b39bd94d":"code","4163e210":"code","c654438b":"code","5f3ef4a8":"code","d10e439c":"code","eb35eea9":"code","86de3390":"code","6ffedcb1":"code","7e7b0163":"code","6cd96d9b":"code","15b133d8":"code","2126b6b7":"code","bd16abc1":"code","71ab0ca1":"markdown","abce1f11":"markdown","ce22393f":"markdown","7d25ffd3":"markdown","e3fc6f22":"markdown","8e811c67":"markdown","38878630":"markdown","181e812c":"markdown","799a8dc7":"markdown","09596c14":"markdown","101ffb4e":"markdown","d07f744a":"markdown","b77cfa32":"markdown","1956fd65":"markdown","097e0b75":"markdown"},"source":{"286a2e53":"import numpy as np\nnp.random.seed(5)\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import np_utils","753f8a2a":"#from keras.datasets import mnist\n#(X_train, y_train), (X_test, y_test) = mnist.load_data()\ndef load_data(path):\n    with np.load(path) as f:\n        x_train, y_train = f['x_train'], f['y_train']\n        x_test, y_test = f['x_test'], f['y_test']\n        return (x_train, y_train), (x_test, y_test)\n\n(X_train, y_train), (X_test, y_test) = load_data('..\/input\/mnistnpz\/mnist.npz')\n\n#(X_train, y_train), (X_test, y_test) = mnist.load_data()","aa35369d":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","b39bd94d":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.cm as cm\n\nfig, ax = plt.subplots(ncols=10, nrows=1, figsize=(10, 5))\namostra = np.random.choice(60000, 10) #escolhe 10 imagens dentre as 60000\n\nfor i in range(len(amostra)):\n    imagem = np.array(X_train[amostra[i]])\n    ax[i].imshow(imagem, cmap = cm.Greys_r)\n    ax[i].get_xaxis().set_ticks([])\n    ax[i].get_yaxis().set_ticks([])\n    ax[i].set_title(y_train[amostra[i]]) # Coloca o label como t\u00edtulo da figura.\nplt.show()","4163e210":"X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train \/= 255\nX_test \/= 255","c654438b":"n_classes = 10 #s\u00e3o 10 classes: n\u00fameros de 0 a 9\nY_train = np_utils.to_categorical(y_train, n_classes)\nY_test = np_utils.to_categorical(y_test, n_classes)","5f3ef4a8":"X_train_flat = X_train.reshape(60000, 784)\nX_test_flat = X_test.reshape(10000, 784)","d10e439c":"#Par\u00e2metros\nnb_epoch = 15\nbatch_size = 128","eb35eea9":"model = Sequential()\nmodel.add(Dense(512, input_shape=(784,)))\nmodel.add(Activation('relu'))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(X_train_flat, Y_train,\n                    batch_size=batch_size, nb_epoch=nb_epoch,\n                    verbose=1, validation_data=(X_test_flat, Y_test))\n\nscore = model.evaluate(X_test_flat, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","86de3390":"model2 = Sequential()\nmodel2.add(Dense(512, input_shape=(784,)))\nmodel2.add(Activation('relu'))\n## Nova camada\nmodel2.add(Dense(512))\nmodel2.add(Activation('relu'))\n\nmodel2.add(Dense(10))\nmodel2.add(Activation('softmax'))\n\nmodel2.summary()\n\nmodel2.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\n\nhistory = model2.fit(X_train_flat, Y_train,\n                    batch_size=batch_size, nb_epoch=nb_epoch,\n                    verbose=1, validation_data=(X_test_flat, Y_test))\n\nscore = model2.evaluate(X_test_flat, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","6ffedcb1":"model3 = Sequential()\nmodel3.add(Dense(512, input_shape=(784,)))\nmodel3.add(Activation('relu'))\nmodel3.add(Dropout(0.3)) # percentual de neur\u00f4nios que ser\u00e3o zerados durante o aprendizado\nmodel3.add(Dense(512))\nmodel3.add(Activation('relu'))\nmodel3.add(Dropout(0.3)) # percentual de neur\u00f4nios que ser\u00e3o zerados durante o aprendizado\nmodel3.add(Dense(10))\nmodel3.add(Activation('softmax'))\n\nmodel3.summary()\n\nmodel3.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\n\nhistory = model3.fit(X_train_flat, Y_train,\n                    batch_size=batch_size, nb_epoch=nb_epoch,\n                    verbose=1, validation_data=(X_test_flat, Y_test))\n\nscore = model3.evaluate(X_test_flat, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","7e7b0163":"img_rows = 28\nimg_cols = 28\nfrom keras import backend as K\nif K.image_data_format() == 'channels_first':\n    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)","6cd96d9b":"## Par\u00e2metros\nn_filters = 32#n\u00famero de filtros\nn_pool = 2 #Tamanho da camada de pooling\nn_conv = 3 #Tamanho da kernel do filtro ","15b133d8":"X_train.shape","2126b6b7":"model4 = Sequential()\n\nmodel4.add(Conv2D(n_filters, kernel_size=(n_conv, n_conv),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel4.add(Conv2D(n_filters, kernel_size=(n_conv, n_conv),\n                 activation='relu'))\nmodel4.add(MaxPooling2D(pool_size=(n_pool, n_pool)))\nmodel4.add(Dropout(0.25))\n\nmodel4.add(Flatten())\nmodel4.add(Dense(128))\nmodel4.add(Activation('relu'))\nmodel4.add(Dropout(0.5))\nmodel4.add(Dense(n_classes))\nmodel4.add(Activation('softmax'))\n\nmodel4.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\n\nmodel4.summary()\n\nresult = model4.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=10, #reduzindo o n\u00famero de \u00e9pocas.\n          verbose=1, validation_data=(X_test, Y_test))\nscore = model4.evaluate(X_test, Y_test, verbose=0)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","bd16abc1":"import pandas as pd\npd.DataFrame(result.history)[['loss', 'val_loss']].plot()","71ab0ca1":"Primeiro formatamos os dados para em vez de ser uma matriz 28x28, ser um vetor de 784 valores.","abce1f11":"Com nosso primeiro modelo, conseguimos um resultado j\u00e1 muito bom, de 98,18% de acur\u00e1cia e um Categorical log-loss de 0.073. Vamos acrescentar mais uma camada na nossa rede neural e ver como fica.","ce22393f":"# Laborat\u00f3rio 10 - Classifica\u00e7\u00e3o de d\u00edgitos\n\nNesse laborat\u00f3rio vamos utilizar um conjunto de dados p\u00fablico de imagens de d\u00edgitos (de 0 a 9), o <a href=\"https:\/\/en.wikipedia.org\/wiki\/MNIST_database\">MNIST<\/a>.  N\u00f3s exploraremos algumas arquiteturas de redes neurais para identificar corretamente um d\u00edgito.","7d25ffd3":"## Parte 2 - Usando uma rede neural tradicional\n\nNa nossa primeira tentativa, vamos usar uma rede neural tradicional com apenas uma camada de neur\u00f4nios escondida.","e3fc6f22":"## Parte 1 - Carregando os dados\n\npara carregar os dados, vamos utilizar uma fun\u00e7\u00e3o auxiliar da API do Keras.","8e811c67":"Note que temos imagens de 28x28 pixels. S\u00e3o 60000 imagens de treinamento e 10000 para teste. Agora vamos visualizar uma imagem e seu r\u00f3tulo.","38878630":"O nosso resultado piorou. Isso pode ocorrer por alguns motivos. O primeiro \u00e9 que aumentando o n\u00famero de par\u00e2metros a serem aprendidos na nossa rede, precisar\u00edamos de mais \u00e9pocas para trein\u00e1-la. O segundo \u00e9 que a nossa rede pode estar se adaptando demais aos dados de treino e sendo incapaz de generalizar nos dados de teste (*Overfiting*). H\u00e1 outros motivos, relacionados aos par\u00e2metros, mas vamos nos ater a esses por enquanto.\n\nPara o segundo problema, podemos usar a t\u00e9cnica de <a href=\"http:\/\/jmlr.org\/papers\/volume15\/srivastava14a.old\/srivastava14a.pdf\" >Dropout<\/a>, que tem se mostrado muito efetiva na preven\u00e7\u00e3o de *overfitting*. ","181e812c":"Agora vamos explorar os dados","799a8dc7":"Agora montamos nossa rede convolucional","09596c14":"E convertemos o vetor com o n\u00famero representado em cada imagem num formato apropriado para o Keras","101ffb4e":"Agora montamos nossa rede neural","d07f744a":"## Parte 3 - Rede Neural Convolucional\n\nAgora vamos mudar de t\u00e9cnica, e tentar usar uma <a href=\"http:\/\/cs231n.github.io\/convolutional-networks\/\">rede neural convolucional<\/a>.\n\nPrimeiro temos de formatar os dados para um array com dimens\u00f5es (n_exemplos, n_cores, n_pixel_x, n_pixel_y)","b77cfa32":"Os dados est\u00e3o em valores entre 0 e 255. Vamos reescalon\u00e1-los para valores entre 0 e 1.","1956fd65":"Com a mudan\u00e7a de arquitetura, podemos ver que em 5 \u00e9pocas temos uma melhora de quase 50% no Categorical Log-loss (de 0,06 para 0,04) e a acur\u00e1cia chegou a 98,6%","097e0b75":"Com nosso terceiro modelo, obtivemos um resultado um pouco melhor de Log-loss e um pouco pior de acur\u00e1cia que o primeiro modelo.  Claro que \u00e9 uma compara\u00e7\u00e3o simplista, pois poder\u00edamos variar uma s\u00e9rie de par\u00e2metros para fazer essas compara\u00e7\u00f5es."}}