{"cell_type":{"fb0773d9":"code","c3d3fb20":"code","749ad9af":"code","12016d4b":"code","2f359498":"code","aadc64c3":"code","335a0667":"code","a6b425c0":"code","d07542af":"code","bcd54f0a":"code","dc9da6c9":"code","46d22b24":"code","cb38c801":"code","693d4ca9":"code","d97819f2":"code","817472ec":"markdown","ae892832":"markdown","206a912e":"markdown"},"source":{"fb0773d9":"import os\nimport copy\nimport pickle\nfrom tqdm.auto import tqdm\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#torch packages\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n#transformer packages\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom transformers import AdamW\nfrom transformers import get_scheduler\nfrom transformers import logging\nlogging.set_verbosity_error() #turn off bert warning\nlogging.set_verbosity_warning() #turn off bert warning","c3d3fb20":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \ndef generate_fold_num_for_dataset(data, num_fold):\n    skf = StratifiedKFold(n_splits=num_fold, shuffle=True)\n    for fold, ( _, val_) in enumerate(skf.split(X=data, y=data.worker)):\n        data.loc[val_ , \"kfold\"] = int(fold)\n    data[\"kfold\"] = data[\"kfold\"].astype(int)\n    return data\n\nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, use_tfidf=False, tfidf_matrix=None):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        self.use_tfidf = use_tfidf\n        if use_tfidf:\n            self.more_toxic_tfidf_idx = df['more_toxic_tfidf_idx'].values\n            self.less_toxic_tfidf_idx = df['less_toxic_tfidf_idx'].values\n            self.tfidf_matrix = tfidf_matrix\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        if self.use_tfidf:\n            more_toxic_tfidf_idx = self.more_toxic_tfidf_idx[index]\n            less_toxic_tfidf_idx = self.less_toxic_tfidf_idx[index]\n            more_toxic_tfidf = self.tfidf_matrix[more_toxic_tfidf_idx]\n            less_toxic_tfidf = self.tfidf_matrix[less_toxic_tfidf_idx]\n            return {\n                'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n                'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n                'more_toxic_tfidf': torch.tensor(more_toxic_tfidf, dtype=torch.long),\n                'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n                'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n                'less_toxic_tfidf': torch.tensor(less_toxic_tfidf, dtype=torch.long),\n                'target': torch.tensor(target, dtype=torch.long)\n            }\n        else:\n            return {\n                'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n                'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n                'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n                'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n                'target': torch.tensor(target, dtype=torch.long)\n            }","749ad9af":"class NN(nn.Module):\n    def __init__(self, bert_drop_out, HID_DIM=768, tfidf_len=0, use_tfidf=False):\n        super().__init__()\n        if use_tfidf:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                \n                nn.Linear(768+tfidf_len, 1),\n#                 nn.ReLU(),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.Dropout(0.3),\n                \n#                 nn.Linear(HID_DIM, HID_DIM),\n#                 nn.ReLU(),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.Dropout(0.4),       \n\n#                 nn.Linear(HID_DIM, 1) \n            )\n        else:\n            self.net = nn.Sequential(\n                nn.Dropout(p=bert_drop_out), #dropout for bert\n                nn.Linear(768, 1)\n#                 nn.Linear(768, HID_DIM),\n#                 nn.ReLU(),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.Dropout(0.3),\n                \n#                 nn.Linear(HID_DIM, HID_DIM),\n#                 nn.ReLU(),\n#                 nn.BatchNorm1d(HID_DIM),\n#                 nn.Dropout(0.4),       \n\n#                 nn.Linear(HID_DIM, 1) \n            )\n            \n    def forward(self, x):\n        score = self.net(x)\n        return score\n\nclass JigsawModel(nn.Module):\n    def __init__(self, BERT, NN):\n        super(JigsawModel, self).__init__()\n        self.bert = BERT\n        self.fc = NN\n        \n    def forward(self, ids, mask, tfidf_vec=None, use_tfidf=False):        \n        out = self.bert(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        if use_tfidf:\n            fc_in = torch.cat(\n                (out[\"pooler_output\"], tfidf_vec), dim=1\n            )\n        else:\n            fc_in = out[\"pooler_output\"]\n        outputs = self.fc(fc_in)\n        return outputs","12016d4b":"# def train_step(\n#     bert_model, trans_model, criterion, optimizer, \n#     train_loader, progress_bar, device, epoch, use_tfidf = False\n# ):\n#     y_preds = []\n#     epoch_loss = 0\n#     trans_model.train()\n#     bert_model.train()\n#     for i, data in enumerate(train_loader):\n#         more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n#         more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n#         less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n#         less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n#         targets = data['target'].to(device, dtype=torch.long)\n\n#         optimizer.zero_grad()\n        \n#         more_in = bert_model(\n#             input_ids = more_toxic_ids, attention_mask = more_toxic_mask,\n#             output_hidden_states=False\n#         )\n#         less_in = bert_model(\n#             input_ids = less_toxic_ids, attention_mask = less_toxic_mask,\n#             output_hidden_states=False\n#         )\n        \n#         if use_tfidf:\n#             more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n#             less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n#             more_in = torch.cat(\n#                 (more_in[\"pooler_output\"], more_toxic_tfidf), dim=1\n#             )\n#             less_in = torch.cat(\n#                 (less_in[\"pooler_output\"], less_toxic_tfidf), dim=1\n#             )\n            \n#         more_out = trans_model(more_in)\n#         less_out = trans_model(less_in)\n#         loss = criterion(less_out, more_out, targets)\n\n#         loss.backward()\n#         optimizer.step()\n#         epoch_loss += loss.item()\n        \n#         for j in range(len(data['more_toxic_ids'])):\n#             y_preds.append([less_out[j].item(), more_out[j].item()])\n        \n#         if progress_bar is not None:\n#             progress_bar.update(1)  \n            \n#         print('[ Epoch {}: {}\/{} ] loss:{:.3f}'.format(epoch, i+1, len(train_loader), loss.item()), end='\\r')\n    \n#     df_score = pd.DataFrame(y_preds,columns=['less','more'])\n#     train_accuracy = validate_accuracy(df_score)         \n    \n#     return df_score, train_accuracy, epoch_loss \/ len(train_loader) # return loss\n\n\n# def validate_all(\n#     bert_model, trans_model, criterion, \n#     valid_loader, device, use_tfidf=False\n# ):\n#     epoch_loss = 0\n#     y_preds = []\n    \n#     bert_model.eval()\n#     trans_model.eval()\n#     with torch.no_grad():\n#         for data in valid_loader:\n#             more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n#             more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n#             less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n#             less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n#             targets = data['target'].to(device, dtype=torch.long)\n            \n            \n#             more_in = bert_model(\n#                 input_ids = more_toxic_ids, attention_mask = more_toxic_mask,\n#                 output_hidden_states=False\n#             )\n#             less_in = bert_model(\n#                 input_ids = less_toxic_ids, attention_mask = less_toxic_mask,\n#                 output_hidden_states=False\n#             )\n\n#             if use_tfidf:\n#                 more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n#                 less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n#                 more_in = torch.cat(\n#                     (more_in[\"pooler_output\"], more_toxic_tfidf), dim=1\n#                 )\n#                 less_in = torch.cat(\n#                     (less_in[\"pooler_output\"], less_toxic_tfidf), dim=1\n#                 )\n\n#             more_out = trans_model(more_in)\n#             less_out = trans_model(less_in)\n#             loss = criterion(less_out, more_out, targets)\n\n#             epoch_loss += loss.item()\n#             for i in range(len(data['more_toxic_ids'])):\n#                 y_preds.append([less_out[i].item(), more_out[i].item()])\n#         df_score = pd.DataFrame(y_preds,columns=['less','more'])\n#         accuracy = validate_accuracy(df_score)\n#     return df_score, accuracy, (epoch_loss \/ len(valid_loader))","2f359498":"def train_step_combine(\n    model, criterion, optimizer, \n    train_loader, progress_bar, device, epoch, use_tfidf = False\n):\n    y_preds = []\n    epoch_loss = 0\n    model.train()\n    for i, data in enumerate(train_loader):\n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n\n        optimizer.zero_grad()\n        \n        if use_tfidf:\n            more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n            less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n            more_out = model(more_toxic_ids, more_toxic_mask, more_toxic_tfidf, use_tfidf=True)\n            less_out = model(less_toxic_ids, less_toxic_mask, less_toxic_tfidf, use_tfidf=True)\n        else:\n            more_out = model(more_toxic_ids, more_toxic_mask)\n            less_out = model(less_toxic_ids, less_toxic_mask)\n            \n        loss = criterion(more_out, less_out, targets)\n\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n        for j in range(len(data['more_toxic_ids'])):\n            y_preds.append([less_out[j].item(), more_out[j].item()])\n        \n        if progress_bar is not None:\n            progress_bar.update(1)  \n            \n        #print('[ Epoch {}: {}\/{} ] loss:{:.3f}'.format(epoch, i+1, len(train_loader), loss.item()), end='\\r')\n    \n    df_score = pd.DataFrame(y_preds,columns=['less','more'])\n    train_accuracy = validate_accuracy(df_score)         \n    \n    return df_score, train_accuracy, epoch_loss \/ len(train_loader) # return loss\n\n\ndef validate_all_combine(\n    model, criterion, \n    valid_loader, device, use_tfidf=False\n):\n    epoch_loss = 0\n    y_preds = []\n    \n    model.eval()\n    with torch.no_grad():\n        for data in valid_loader:\n            more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n            more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n            less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n            less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n            targets = data['target'].to(device, dtype=torch.long)\n            \n            if use_tfidf:\n                more_toxic_tfidf = data['more_toxic_tfidf'].to(device, dtype = torch.long)\n                less_toxic_tfidf = data['less_toxic_tfidf'].to(device, dtype = torch.long)\n                more_out = model(more_toxic_ids, more_toxic_mask, more_toxic_tfidf, use_tfidf=True)\n                less_out = model(less_toxic_ids, less_toxic_mask, less_toxic_tfidf, use_tfidf=True)\n            else:\n                more_out = model(more_toxic_ids, more_toxic_mask)\n                less_out = model(less_toxic_ids, less_toxic_mask)\n            \n            loss = criterion(more_out, less_out, targets)\n\n            epoch_loss += loss.item()\n            for i in range(len(data['more_toxic_ids'])):\n                y_preds.append([less_out[i].item(), more_out[i].item()])\n        df_score = pd.DataFrame(y_preds,columns=['less','more'])\n        accuracy = validate_accuracy(df_score)\n    return df_score, accuracy, (epoch_loss \/ len(valid_loader))\n\ndef validate_accuracy(df_score):\n    return len(df_score[df_score['less'] < df_score['more']]) \/ len(df_score)\n\ndef return_wrong_text(df_score, df_valid):\n    df_score_text = pd.concat((df_valid.reset_index().drop('index',axis=1),df_score),axis=1)\n    return df_score_text[df_score_text['less'] > df_score_text['more']]\n\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)","aadc64c3":"def plot_loss_plot(train_losses, valid_losses):\n    plt.plot(train_losses,label=\"Training\")\n    plt.plot(valid_losses,label=\"Validation\")\n    plt.title(\"Loss plot\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n    \ndef plot_acc_plot(train_accs, valid_accs):\n    plt.plot(train_accs,label=\"Training\")\n    plt.plot(valid_accs,label=\"Validation\")\n    plt.title(\"Accuracy plot\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","335a0667":"def remove_duplicates(df, used_col):\n    \"\"\"Combine `less_toxic` text and `more_toxic` text,\n    then remove duplicate pair of comments while keeping the last pair\n    \"\"\"\n    df[\"combine\"] = df[\"less_toxic\"] + df[\"more_toxic\"]\n    df = df.drop_duplicates(subset=used_col, keep=\"last\")\n    return df\n\ndef create_corpus(df_train):\n    all_corpus = df_train[\"more_toxic\"].to_list()\n    all_corpus += df_train[\"less_toxic\"].to_list()\n    #remove duplicates\n    all_corpus = list(\n        set(all_corpus)\n    )\n    return all_corpus\n\ndef create_mapping_dict(corpus):\n    idx = np.arange(len(corpus))\n    sentence2idx = dict(\n        zip(corpus, idx)\n    )\n    return sentence2idx\n\ndef tokenize_by_bert_tokenizer(corpus, tokenizer):\n    corpus_tokenized = [\n        tokenizer.tokenize(sentence) for sentence in corpus\n    ]\n    return corpus_tokenized\n\ndef identity_tokenizer(text):\n    return text\n\ndef corpus2tfidf(corpus_tokenized):\n    tfidf = TfidfVectorizer(preprocessor=' '.join, tokenizer=identity_tokenizer)    \n    tfidf_matrix_sparse = tfidf.fit_transform(corpus_tokenized)\n    tfidf_matrix = tfidf_matrix_sparse.toarray()\n    return tfidf, tfidf_matrix\n    \ndef construct_tfidf_matrix(df_train, tokenizer):\n    corpus = create_corpus(df_train)\n    sentence2idx = create_mapping_dict(corpus)\n    corpus_tokenized = tokenize_by_bert_tokenizer(corpus, tokenizer)\n    tfidf_obj, tfidf_matrix = corpus2tfidf(corpus_tokenized)\n    return sentence2idx, tfidf_obj, tfidf_matrix","a6b425c0":"set_seed(5080)\ndata_train = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ndata_test = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#load model and tokenizer\n# PRETRAINED_MODEL_NAME = \"GroNLP\/hateBERT\"\n# bert_tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\nPRETRAINED_MODEL_NAME = \"roberta-base\"\nbert_tokenizer = RobertaTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)","d07542af":"#text preprocessing\ndata_train_removed = remove_duplicates(data_train, \"combine\") #(30108, 4) -> (15410, 4)\n\n#construct tfidf\u4e00\u5b9a\u8981\u7528removed!\nsentence2idx, tfidf_obj, tfidf_matrix = construct_tfidf_matrix(data_train_removed, bert_tokenizer)\ndata_train[\"less_toxic_tfidf_idx\"] = data_train[\"less_toxic\"].apply(lambda x: sentence2idx[x])\ndata_train[\"more_toxic_tfidf_idx\"] = data_train[\"more_toxic\"].apply(lambda x: sentence2idx[x])","bcd54f0a":"with open('.\/tfidf_roberta_obj.pickle', 'wb') as f:\n    pickle.dump(tfidf_obj, f)","dc9da6c9":"tfidf_obj","46d22b24":"tfidf_matrix.shape","cb38c801":"#paramters settings\ntrain_valid_ratio = 0.25\nmax_token_length = 128 #\u4ee3\u8868\u6700\u591a\u653e\u5165BERT\u7684token\u9577\u5ea6\nbatch_size = 32\nuse_tfidf = False\n\ndf_train, df_valid = train_test_split(\n    data_train, test_size = train_valid_ratio\n)\n\n\nif use_tfidf:\n    tfidf_len = tfidf_matrix.shape[1]\n    train_dataset = JigsawDataset(\n        df_train, tokenizer=bert_tokenizer, \n        max_length=max_token_length , use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n     )\n    valid_dataset = JigsawDataset(\n        df_valid, tokenizer=bert_tokenizer, \n        max_length=max_token_length , use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n    )\nelse:\n    train_dataset = JigsawDataset(\n        df_train, tokenizer=bert_tokenizer, \n        max_length=max_token_length #, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n     )\n    valid_dataset = JigsawDataset(\n        df_valid, tokenizer=bert_tokenizer, \n        max_length=max_token_length #, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n    )\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True,\n    num_workers=2)\nvalid_loader = DataLoader(\n    valid_dataset, batch_size=batch_size, shuffle=False,\n    num_workers=2)","693d4ca9":"# LR = 1e-4\n# WD = 1e-6\n# bert_drop_out = 0.2\n# margin_list = [0.5]\n\nLR_list = [1e-5, 1e-4]\nWD_list = [0, 1e-6]\nbert_dropout_list = [0.2, 0.3]\nmax_patience = 2\n\nEPOCH = 10\nHID_DIM = 768\nMARGIN = 0.5\nDATE = \"0110\"\nmodel_name = \"roberta\"\n\n\nfor LR in LR_list:\n    for WD in WD_list:\n        for bert_drop_out in bert_dropout_list:\n            print(f\"LR = {LR}, WD = {WD}, bert drop out = {bert_drop_out}\")\n            bert = RobertaModel.from_pretrained(PRETRAINED_MODEL_NAME).to(device)\n            dnn = NN(bert_drop_out, HID_DIM\n                     #, tfidf_len, use_tfidf=True\n                    ).to(device)\n            dnn.apply(init_weights)\n            model = JigsawModel(bert, dnn)\n            trainable_params = list(model.parameters())\n            num_trainable_params = sum(p.numel() for p in trainable_params)\n\n            criterion = nn.MarginRankingLoss(margin=MARGIN)\n            optimizer = AdamW(\n                trainable_params\n                ,lr=LR,weight_decay=WD)\n\n            print(f\"Total trainable parameters {num_trainable_params}\")\n\n            num_training_steps = EPOCH * len(train_loader)\n\n            train_accs = []\n            valid_accs = []\n            train_losses = []\n            valid_losses = []\n            best_valid_loss = np.inf\n            best_valid_acc = 0\n            best_epoch = 0\n            best_model = None\n            no_update = 0\n            MODEL_DIR = f\".\/{DATE}_{model_name}_LR_{LR}_WD_{WD}_BDR_{bert_drop_out}.pth\"\n\n            # start training\n            #progress_bar = None\n            progress_bar = tqdm(range(num_training_steps))\n            for epoch in range(1, EPOCH+1):\n                _, train_acc, train_loss = train_step_combine(model, criterion, optimizer, train_loader, progress_bar, device, epoch, use_tfidf)\n                df_score, valid_acc, valid_loss = validate_all_combine(model, criterion, valid_loader, device, use_tfidf)\n                train_accs.append(train_acc)\n                valid_accs.append(valid_acc)\n                train_losses.append(train_loss)\n                valid_losses.append(valid_loss)\n\n                print(f\"Epoch {epoch}, Loss(Train\/Valid) = {round(train_loss, 4)}\/{round(valid_loss, 4)}, Accuracy(Train\/Valid) = {round(train_acc*100, 3)}%\/{round(valid_acc*100, 3)}%\")\n\n                if valid_acc > best_valid_acc:\n                    print(f\"Saving model...\")\n                    best_epoch = epoch\n                    best_valid_acc = valid_acc\n                    best_valid_loss = valid_loss\n                    best_model = model\n                    torch.save(\n                        {\"BERT\": best_model.bert.state_dict(),\"NN\": best_model.fc.state_dict()}\n                        ,MODEL_DIR\n                    )\n                #early stopping\n                else:\n                    no_update += 1 \n                \n                if no_update == max_patience:\n                    break\n                    \n            print(f\"Best epoch: {best_epoch}, valid loss: {round(best_valid_loss, 4)}, valid acc: {round(best_valid_acc*100, 4)}%\")\n\n            #compute acc for all dataset\n            all_dataset = JigsawDataset(\n                data_train, tokenizer=bert_tokenizer, \n                max_length=max_token_length, use_tfidf=use_tfidf, tfidf_matrix=tfidf_matrix\n            )\n\n            all_loader = DataLoader(\n                all_dataset, batch_size=batch_size, shuffle=True,\n                num_workers=2)\n\n            df_score, accuracy, valid_loss = validate_all_combine(model, criterion, all_loader, device, use_tfidf)\n            df_score_text = return_wrong_text(df_score, df_valid)\n            csv_out_path = f\".\/wrong_text_{DATE}_LR_{LR}_WD_{WD}_BDR_{bert_drop_out}.csv\"\n            df_score_text.to_csv(csv_out_path)\n            print(f\"Accuracy in all dataset set: {round(accuracy*100, 3)}%\")\n\n            #Evaluation\n            plot_loss_plot(train_losses, valid_losses)\n            plot_acc_plot(train_accs, valid_accs)","d97819f2":"# all_dataset = JigsawDataset(\n#     data_train, tokenizer=bert_tokenizer, \n#     max_length=max_token_length)\n\n# all_loader = DataLoader(\n#     train_dataset, batch_size=batch_size, shuffle=True,\n#     num_workers=2)\n\n# df_score, accuracy, valid_loss = validate_all(bert, dnn, criterion, all_loader, device)\n# print(f\"Accuracy in all dataset set: {round(accuracy*100, 3)}%\")\n\n# df_score_text = return_wrong_text(df_score, df_valid)\n# df_score_text.to_csv(\".\/wrong_text_1229_2.csv\")\n# print(df_score_text)\n\n# plot_loss_plot(train_losses, valid_losses)\n# plot_acc_plot(train_accs, valid_accs)","817472ec":"# Dataset and Dataloader","ae892832":"# Modeling","206a912e":"#### jack settings\n- PRETRAINED_MODEL_NAME = \"roberta-base\"\n- train_valid_ratio = 0.25\n- max_token_length = 128\n- batch_size = 32\n- LR = 1e-4\n- WD = 1e-6\n- EPOCH = 10"}}