{"cell_type":{"78f20483":"code","aa52a592":"code","accd223a":"code","1f32f465":"code","18a35ef6":"code","641efa51":"code","b5056fca":"code","8640fd30":"code","092ab200":"code","473f3c24":"code","4c4220cf":"code","17207924":"code","ac4e9790":"code","de15e496":"code","6608f551":"code","93f1cfc4":"code","72769e70":"code","f7b27af1":"code","8470b4ea":"code","0f8f213f":"code","0809a7ac":"code","15466c6c":"code","76a737cd":"code","cd42168c":"code","218ec53f":"code","a002a45a":"code","3282ab75":"code","9bb1c5d4":"code","ceb127c6":"code","81591cb7":"code","57147c99":"code","b9874b39":"code","32c8eedf":"code","4f59fbdd":"code","09a1561f":"code","eed5dbb7":"code","cd5d3b9d":"code","85f794b9":"code","53c689d6":"markdown","8d6ece90":"markdown","08207a30":"markdown","7cf5708d":"markdown","7dcbf3ec":"markdown","8e9e59f3":"markdown","4b2a1a20":"markdown","fcf1e709":"markdown","77e046c5":"markdown","2b0d93ca":"markdown","09f19f6c":"markdown","639dcf96":"markdown","0cb5a7bc":"markdown","09abfff7":"markdown","d07dabdf":"markdown"},"source":{"78f20483":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa52a592":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\n","accd223a":"df = pd.read_csv('..\/input\/car-price-prediction\/car_prediction_data.csv')\ndf.head()","1f32f465":"df.shape","18a35ef6":"import missingno as ms \nnull= df.isna().sum()\nper_null= df.isna().sum()\/df.isna().count()*100\nper_null= round(per_null, 1)\nmissing_data= pd.concat([null, per_null], axis=1,keys=['number of missing values', '% of missig values'])\nmissing_data","641efa51":"ms.matrix(df)","b5056fca":"# columns = df[cols for cols in df.columns if df[cols].nunique() < 10]\ncategorical_cols = [cols for cols in df.columns if df[cols].dtype == 'object' and df[cols].nunique()<10]\nprint(categorical_cols)\nnumerical_cols = [cols for cols in df.columns if df[cols].dtype in ['int64','float64']]\nprint(numerical_cols)\nall_columns = categorical_cols + numerical_cols\ndataset = df[all_columns]\n","8640fd30":"# We could create a columns named current_year and set it to 2021 and subtract it with year column\n# to find the car age and then remove the year and current_year columns but pandas know when you \n# subtract a number from a columns it will use the number for every single element in the column\n# and this is more simple and better way\ndataset['age'] = 2021 - dataset['Year']\ndataset=dataset.drop(['Year'],axis=1)\ndataset","092ab200":"final_dataset=pd.get_dummies(dataset,drop_first=True)\n# We drop one of the columns to avoid dummy variable trap. (avoid Multicollinearity)\nfinal_dataset.corr()","473f3c24":"final_dataset.corr()","4c4220cf":"import seaborn as sns\nsns.pairplot(final_dataset)","17207924":"corrmat = final_dataset.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(10,10))\n\ng=sns.heatmap(final_dataset[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","ac4e9790":"y = final_dataset.iloc[:,0]\nx = final_dataset.iloc[:,1:]\nx.head()\n\n# x.head()\n# y.head()","de15e496":"from sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(x,y)\nprint(model.feature_importances_)\n","6608f551":"feature_importance = pd.Series(model.feature_importances_,index=x.columns)\nfeature_importance.nlargest(5).plot(kind='barh')\nplt.show()","93f1cfc4":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.3, random_state=0)\n","72769e70":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nrfregressor = RandomForestRegressor()","f7b27af1":"\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10]\n","8470b4ea":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","0f8f213f":"rfgs=RandomizedSearchCV(estimator = rfregressor, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","0809a7ac":"print(x_train)\nrfgs.fit(x_train,y_train)","15466c6c":"rfgs.best_params_","76a737cd":"rfgs.best_score_","cd42168c":"predictions=rfgs.predict(x_valid)","218ec53f":"# we can see the normalized curve\nsns.distplot(y_valid-predictions)\n","a002a45a":"plt.scatter(y_valid,predictions)","3282ab75":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_valid, predictions))\nprint('MSE:', metrics.mean_squared_error(y_valid, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, predictions)))\n","9bb1c5d4":"import xgboost as xgb\nfrom scipy.stats import uniform, randint","ceb127c6":"xgb_model = xgb.XGBRegressor(objective=\"reg:linear\", random_state = 0)","81591cb7":"params = {\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 150), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}","57147c99":"xgb = RandomizedSearchCV(estimator = xgb_model, param_distributions = params,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","b9874b39":"xgb.fit(x_train,y_train)","32c8eedf":"xgb.best_params_","4f59fbdd":"xgb.best_score_","09a1561f":"predictions=xgb.predict(x_valid)","eed5dbb7":"sns.distplot(y_valid-predictions)","cd5d3b9d":"plt.scatter(y_valid,predictions)","85f794b9":"print('MAE:', metrics.mean_absolute_error(y_valid, predictions))\nprint('MSE:', metrics.mean_squared_error(y_valid, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, predictions)))","53c689d6":"In this notebook we will predict car price using regresson models and compare **XGBoost** with **Random Forest** to check which will perform better \nwhat is done in this notebook\n\n\n> * Data Preprocessing => checking for missing values \n> * Seprate Categorical variables from numericals \n> * Feature engineering and change column year to a useful column (car age)\n> * Dealing with categroical variables and encode them\n> * Scape from dummy variable trap \n> * Check and visualize correlation coefficient\n> * Find feature importance and top important features\n> * Split data into training set and test set\n> * Tuning hyper parameters with Randomized Search CV to find the best parameter\n> * Fit the models and evaluate\n\n","8d6ece90":"We can see that XGBoost perform better.\n\nMany thanks for your valuable time and checking my notebook.\n\nIf you have any **Question** feel free to ask in the **comments** I will be aswering them all ","08207a30":"As we can see there is **no missing values** in the dataset so we wont be worry about missing values.","7cf5708d":"# Finding The Car Age","7dcbf3ec":"# Dealing with categorical variables","8e9e59f3":"# Car Price Prediction","4b2a1a20":"# Checking for Missing values","fcf1e709":"# Finding Low Cardinality Categorical Variables And Numerical Variables","77e046c5":"# Random Forest Regressor","2b0d93ca":"# Checking the correlation coefficient","09f19f6c":"# XGBOOST Model","639dcf96":"# Split data into training set and test set","0cb5a7bc":"# Randomized Search CV","09abfff7":"# Feature Importance","d07dabdf":"# Really appreciate your time"}}