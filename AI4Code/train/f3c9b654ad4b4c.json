{"cell_type":{"4c8dbd7d":"code","cf94e620":"code","c0da6747":"code","d4a1b80c":"code","fc04905b":"code","e3b6553c":"code","c5166947":"code","7c9be629":"code","f635362b":"code","add495b3":"code","3b1fee53":"code","774c7d64":"code","610b1729":"code","e9c17d19":"code","8fb3c3bd":"code","149bdad1":"code","7ef25ef2":"code","afc899dd":"code","a8047804":"code","da38cc0f":"code","9873a08e":"code","b03e6a98":"code","fb9795b9":"code","98a9b798":"code","a7270ced":"code","23092834":"code","add7f2ab":"code","7aec5e55":"code","b5108804":"code","2819c766":"code","94258a6a":"code","15ab7ebc":"code","465ba805":"code","9f3000f6":"code","46f3e53c":"code","e9e30d3e":"code","cd77a0b9":"code","b1784272":"code","833f51b8":"code","84a4f692":"code","72287fa1":"code","5c793ec5":"code","39410a61":"code","733db54e":"code","47a734dd":"code","4ec80b11":"code","ac65ede7":"code","97a6ae51":"code","77d057ae":"code","fc8076d8":"code","c835dd00":"code","975965b4":"code","b753005c":"code","5ca30362":"code","a65c6b65":"code","1e33736a":"code","38de03b0":"code","b96c8399":"code","e5aad8dd":"code","b2ef8492":"code","1f2bddc8":"code","4817197f":"code","5f7e5ea7":"code","f562a399":"code","c5f808ab":"code","a1c2dc0f":"code","04227544":"markdown","d02f06c9":"markdown","910fab8e":"markdown","c52ec77f":"markdown","ffa06df0":"markdown","45be0655":"markdown","3dd87338":"markdown","3beedc11":"markdown","27922628":"markdown","6985ce3e":"markdown","eae8aba3":"markdown"},"source":{"4c8dbd7d":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport os \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set_style(\"whitegrid\")\n%matplotlib inline","cf94e620":"training = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntesting = pd.read_csv(\"..\/input\/titanic\/test.csv\")","c0da6747":"training.head()","d4a1b80c":"testing.head()","fc04905b":"print(training.columns)\nprint(testing.columns)","e3b6553c":"types_train = training.dtypes\nnum_values = types_train[(types_train == float)]\n\nprint(\"These are the numerical features:\")\nprint(num_values)","c5166947":"training.describe()","7c9be629":"# different ranges of value in each column\n# we need to scale the value in the range of 0-1","f635362b":"# null value inputation\ntraining.isnull().sum()","add495b3":"testing.isnull().sum()","3b1fee53":"# droping the ticket column as it doesnt correspond any signifance\ntraining.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\ntesting.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)","774c7d64":"# checking the distribution of age column\ncopy = training.copy()\ncopy.dropna(inplace = True)\nsns.distplot(copy[\"Age\"])","610b1729":"#counting the number of Flags\ntraining['Embarked'].value_counts()","e9c17d19":"#the median will be an acceptable value to place in the NaN cells\ntraining[\"Age\"].fillna(training[\"Age\"].median(), inplace = True)\ntesting[\"Age\"].fillna(testing[\"Age\"].median(), inplace = True) \ntraining[\"Embarked\"].fillna(\"S\", inplace = True)              # replacing the value by 'S' as it corresponds to maximum count\ntesting[\"Fare\"].fillna(testing[\"Fare\"].median(), inplace = True)","8fb3c3bd":"training.head()","149bdad1":"testing.head()","7ef25ef2":"sns.countplot('Sex', hue='Survived', data=training)","afc899dd":"#can ignore the testing set for now\nsns.barplot(x=\"Sex\", y=\"Survived\", data=training)\nplt.title(\"Distribution of Survival based on Gender\")\nplt.show()\n\ntotal_survived_females = training[training.Sex == \"female\"][\"Survived\"].sum()\ntotal_survived_males = training[training.Sex == \"male\"][\"Survived\"].sum()\n\nprint(\"Total people survived is: \" + str((total_survived_females + total_survived_males)))\nprint(\"Proportion of Females who survived:\") \nprint(total_survived_females\/(total_survived_females + total_survived_males))\nprint(\"Proportion of Males who survived:\")\nprint(total_survived_males\/(total_survived_females + total_survived_males))","a8047804":"# 68 percent of women survived in the accident compared to 32 % men","da38cc0f":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Distribution of Survival Based on Class\")\nplt.show()\n\ntotal_survived_one = training[training.Pclass == 1][\"Survived\"].sum()\ntotal_survived_two = training[training.Pclass == 2][\"Survived\"].sum()\ntotal_survived_three = training[training.Pclass == 3][\"Survived\"].sum()\ntotal_survived_class = total_survived_one + total_survived_two + total_survived_three\n\nprint(\"Total people survived is: \" + str(total_survived_class))\nprint(\"Proportion of Class 1 Passengers who survived:\") \nprint(total_survived_one\/total_survived_class)\nprint(\"Proportion of Class 2 Passengers who survived:\")\nprint(total_survived_two\/total_survived_class)\nprint(\"Proportion of Class 3 Passengers who survived:\")\nprint(total_survived_three\/total_survived_class)","9873a08e":"print('Distribution of survival of class 1: ', total_survived_one\/ len(training[training.Pclass == 1]))\nprint('Distribution of survival of class 2: ', total_survived_two\/ len(training[training.Pclass == 2]))\nprint('Distribution of survival of class 3: ', total_survived_three\/ len(training[training.Pclass == 3]))","b03e6a98":"sns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","fb9795b9":"sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","98a9b798":"survived_ages = training[training.Survived == 1][\"Age\"]\nnot_survived_ages = training[training.Survived == 0][\"Age\"]\nplt.subplot(1, 2, 1)\nsns.distplot(survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Survived\")\nplt.ylabel(\"Proportion\")\nplt.subplot(1, 2, 2)\nsns.distplot(not_survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Didn't Survive\")\nplt.subplots_adjust(right=1.7)\nplt.show()","a7270ced":"sns.stripplot(x=\"Survived\", y=\"Age\", data=training, jitter=True)","23092834":"# It appears as though passengers in the younger range of ages were more likely to survive than \n# those in the older range of ages, as seen by the clustering in the strip plot, \n# as well as the survival distributions of the histogram.","add7f2ab":"# Here is one final cumulative graph of a pair plot that shows the relations between all of the different features","7aec5e55":"sns.pairplot(training)","b5108804":"training[\"Embarked\"].unique()","2819c766":"from sklearn.preprocessing import LabelEncoder\n\nle_sex = LabelEncoder()\nle_sex.fit(training[\"Sex\"])\n\nencoded_sex_training = le_sex.transform(training[\"Sex\"])\ntraining[\"Sex\"] = encoded_sex_training\nencoded_sex_testing = le_sex.transform(testing[\"Sex\"])\ntesting[\"Sex\"] = encoded_sex_testing\n\nle_embarked = LabelEncoder()\nle_embarked.fit(training[\"Embarked\"])\n\nencoded_embarked_training = le_embarked.transform(training[\"Embarked\"])\ntraining[\"Embarked\"] = encoded_embarked_training\nencoded_embarked_testing = le_embarked.transform(testing[\"Embarked\"])\ntesting[\"Embarked\"] = encoded_embarked_testing","94258a6a":"training.sample(5)","15ab7ebc":"testing.sample(5)","465ba805":"# We can combine SibSp and Parch into one synthetic feature called family size, \n# which indicates the total number of family members on board for each member. ","9f3000f6":"training[\"FamSize\"] = training[\"SibSp\"] + training[\"Parch\"] + 1\ntesting[\"FamSize\"] = testing[\"SibSp\"] + testing[\"Parch\"] + 1","46f3e53c":"# This IsAlone feature also may work well with the data we're dealing with,\n# telling us whether the passenger was along or not on the ship.","e9e30d3e":"training[\"IsAlone\"] = training.FamSize.apply(lambda x: 1 if x == 1 else 0)\ntesting[\"IsAlone\"] = testing.FamSize.apply(lambda x: 1 if x == 1 else 0)","cd77a0b9":"# Although it may not seem like it, we can also extract some useful information from the name column. \n# Not the actual names themselves, but the title of their names like Ms. or Mr. \n# This may also provide a hint as to whether the passenger survived or not. \n# Therefore we can extract this title and then encode it like we did for Sex and Embarked.","b1784272":"for name in training[\"Name\"]:\n    training[\"Title\"] = training[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \nfor name in testing[\"Name\"]:\n    testing[\"Title\"] = testing[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)","833f51b8":"training.head() #Title column added","84a4f692":"titles = set(training[\"Title\"]) #making it a set gets rid of all duplicates\nprint(titles)","72287fa1":"title_list = list(training[\"Title\"])\nfrequency_titles = []\n\nfor i in titles:\n    frequency_titles.append(title_list.count(i))\n    \nprint(frequency_titles)","5c793ec5":"titles = list(titles)\n\ntitle_dataframe = pd.DataFrame({\n    \"Titles\" : titles,\n    \"Frequency\" : frequency_titles\n})\n\nprint(title_dataframe)","39410a61":"title_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Ms\": \"Other\", \"Dona\": \"Other\"}\n\ntraining.replace({\"Title\": title_replacements}, inplace=True)\ntesting.replace({\"Title\": title_replacements}, inplace=True)\n\nle_title = LabelEncoder()\nle_title.fit(training[\"Title\"])\n\nencoded_title_training = le_title.transform(training[\"Title\"])\ntraining[\"Title\"] = encoded_title_training\nencoded_title_testing = le_title.transform(testing[\"Title\"])\ntesting[\"Title\"] = encoded_title_testing","733db54e":"training.drop(\"Name\", axis = 1, inplace = True)\ntesting.drop(\"Name\", axis = 1, inplace = True)","47a734dd":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n#We need to reshape our data since the Scaler takes in arrays\nages_train = np.array(training[\"Age\"]).reshape(-1, 1)\nfares_train = np.array(training[\"Fare\"]).reshape(-1, 1)\nages_test = np.array(testing[\"Age\"]).reshape(-1, 1)\nfares_test = np.array(testing[\"Fare\"]).reshape(-1, 1)\n\ntraining[\"Age\"] = scaler.fit_transform(ages_train)\ntraining[\"Fare\"] = scaler.fit_transform(fares_train)\ntesting[\"Age\"] = scaler.fit_transform(ages_test)\ntesting[\"Fare\"] = scaler.fit_transform(fares_test)\n\n#You can try with MinMaxScaler as well to see how it performs in comparison, just replace StandardScaler with MinMaxScaler","4ec80b11":"training.head()","ac65ede7":"testing.head()","97a6ae51":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score \nfrom sklearn.model_selection import GridSearchCV","77d057ae":"X_train = training.drop(labels=[\"PassengerId\", \"Survived\"], axis=1) #define training features set\ny_train = training[\"Survived\"] #define training label set\nX_test = testing.drop(\"PassengerId\", axis=1) #define testing features set\n#we don't have y_test, that is what we're trying to predict with our model","fc8076d8":"X_train.head()","c835dd00":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","975965b4":"svc_clf = SVC() \n\nparameters_svc = {\"kernel\": [\"rbf\", \"linear\"], \"probability\": [True, False], \"verbose\": [True, False]}\n\ngrid_svc = GridSearchCV(svc_clf, parameters_svc, scoring=make_scorer(accuracy_score))\ngrid_svc.fit(X_training, y_training)\n\nsvc_clf = grid_svc.best_estimator_\n\nsvc_clf.fit(X_training, y_training)\npred_svc = svc_clf.predict(X_valid)\nacc_svc = accuracy_score(y_valid, pred_svc)","b753005c":"print(\"The Score for SVC is: \" + str(acc_svc))","5ca30362":"linsvc_clf = LinearSVC()\n\nparameters_linsvc = {\"multi_class\": [\"ovr\", \"crammer_singer\"], \"fit_intercept\": [True, False], \"max_iter\": [100, 500, 1000, 1500]}\n\ngrid_linsvc = GridSearchCV(linsvc_clf, parameters_linsvc, scoring=make_scorer(accuracy_score))\ngrid_linsvc.fit(X_training, y_training)\n\nlinsvc_clf = grid_linsvc.best_estimator_\n\nlinsvc_clf.fit(X_training, y_training)\npred_linsvc = linsvc_clf.predict(X_valid)\nacc_linsvc = accuracy_score(y_valid, pred_linsvc)\n\nprint(\"The Score for LinearSVC is: \" + str(acc_linsvc))","a65c6b65":"rf_clf = RandomForestClassifier()\n\nparameters_rf = {\"n_estimators\": [4, 5, 6, 7, 8, 9, 10, 15], \"criterion\": [\"gini\", \"entropy\"], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n                 \"max_depth\": [2, 3, 5, 10], \"min_samples_split\": [2, 3, 5, 10]}\n\ngrid_rf = GridSearchCV(rf_clf, parameters_rf, scoring=make_scorer(accuracy_score))\ngrid_rf.fit(X_training, y_training)\n\nrf_clf = grid_rf.best_estimator_\n\nrf_clf.fit(X_training, y_training)\npred_rf = rf_clf.predict(X_valid)\nacc_rf = accuracy_score(y_valid, pred_rf)\n\nprint(\"The Score for Random Forest is: \" + str(acc_rf))","1e33736a":"logreg_clf = LogisticRegression()\n\nparameters_logreg = {\"penalty\": [\"l2\"], \"fit_intercept\": [True, False], \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n                     \"max_iter\": [50, 100, 200], \"warm_start\": [True, False]}\n\ngrid_logreg = GridSearchCV(logreg_clf, parameters_logreg, scoring=make_scorer(accuracy_score))\ngrid_logreg.fit(X_training, y_training)\n\nlogreg_clf = grid_logreg.best_estimator_\n\nlogreg_clf.fit(X_training, y_training)\npred_logreg = logreg_clf.predict(X_valid)\nacc_logreg = accuracy_score(y_valid, pred_logreg)\n\nprint(\"The Score for Logistic Regression is: \" + str(acc_logreg))","38de03b0":"import xgboost as xgb","b96c8399":"dtrain_os= xgb.DMatrix(X_training, y_training)\ndtest_os = xgb.DMatrix(X_valid, y_valid)\n#What to monitor (in this case, *train*)\nwatchlist = [(dtrain_os, 'train')]\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 5\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nmodel = xgb.train(params,\n                dtrain_os,\n                1000,\n                watchlist,\n                early_stopping_rounds=50,\n                maximize=True,\n                verbose_eval=50)","e5aad8dd":"from sklearn.metrics import classification_report,confusion_matrix, accuracy_score,roc_auc_score, recall_score, roc_curve\n","b2ef8492":"confusion_matrix(y_training, np.where(model.predict(dtrain_os)<0.44, 0,1))","1f2bddc8":"model_performance = pd.DataFrame({\n    \"Model\": [\"SVC\", \"Linear SVC\", \"Random Forest\", \n              \"Logistic Regression\"],\n    \"Accuracy\": [acc_svc, acc_linsvc, acc_rf, \n              acc_logreg]\n})\n\nmodel_performance.sort_values(by=\"Accuracy\", ascending=False)","4817197f":"svc_clf.fit(X_train, y_train)","5f7e5ea7":"y_pred = svc_clf.predict(X_test)","f562a399":"y_pred = np.where(model.predict(xgb.DMatrix(X_test))<0.44,0,1)","c5f808ab":"df_pred = pd.DataFrame({\n        \"PassengerId\": testing[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\n\nprint(df_pred.shape)","a1c2dc0f":"df_pred.to_csv('.\/submittion.csv', index=False)","04227544":"#### null values in column\n- Age \n- Cabin","d02f06c9":"Checking the survival rate on the category **Gender**","910fab8e":"### Numerical Features\n\n- **Age** : indicates age of passenger\n- **Fare** : indicates fare paid by that passenger","c52ec77f":"**LogisiticRegression Model**","ffa06df0":"**RandomForest Model**","45be0655":"### Categorical Features\n\n- **Survived**: Indicates that if particular passenger survived(1) or not(0)\n- **Pclass**: Shows classes for the passenger, 1 for first, 2 for second and 3 for third.\n- **Sex**: Indicates gender of the passenger. Might be crucial indicator for our model since historical records show women were first to save in ship accidents.\n- **SibSp**: The number of siblings and spouses on the ship, might be useful for extracting family ties.\n- **Parch**: The number of parents and children on the ship, migt have similar use with SibSp.\n- **Embarked**: Flag for the where the passenger embarked from, C for Cherbourg, Q for Queenstown, S for Southampton","3dd87338":"**SVC Model**","3beedc11":"**sklearn Models to Test**","27922628":"## 1. Importing Libraries","6985ce3e":"## 2. Reading Data","eae8aba3":"**LinearSVC Model**"}}