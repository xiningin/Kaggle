{"cell_type":{"e898b319":"code","c70e41f1":"code","f5149887":"code","6fb160c7":"code","9163285f":"code","d2f1b769":"code","ff16eea1":"code","58b2ffc1":"code","3838b8d5":"code","1f889574":"code","843b1f20":"code","44d1f24e":"code","29ccc8b6":"code","8d212145":"code","28473cde":"code","d5cf57db":"code","581dccb5":"code","8a5baccb":"markdown","8ee66f61":"markdown","d0f530bd":"markdown","9bbe93b4":"markdown","c8643592":"markdown","ff9ed33f":"markdown","1d37bde0":"markdown","b69a2c05":"markdown","0ad02263":"markdown","2b08f551":"markdown","6a7f5b83":"markdown","3af633fa":"markdown","9c399565":"markdown","7549598f":"markdown","f907c5ee":"markdown","431a0efe":"markdown","c15b2d93":"markdown","17208ecb":"markdown","ce2d9389":"markdown","5493500f":"markdown","4b92da1e":"markdown","77aa029c":"markdown","9b26c6c1":"markdown","26b2d752":"markdown","e14197f1":"markdown","4aa51acd":"markdown","2bea9285":"markdown","5771679b":"markdown","5eec8442":"markdown","60ac189c":"markdown"},"source":{"e898b319":"# Import the dependencies\nimport numpy as np\nfrom scipy.linalg import toeplitz, cholesky, sqrtm, inv\n# import scipy.linalg as la\nfrom scipy import signal\nfrom scipy.integrate import odeint\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"white\")\nprint(\"Imports done\")","c70e41f1":"def g_gp(x,v):\n    \"\"\"\n    Generative process, equation of sensory mapping: g_pp(x) at point x    \n   \n    INPUTS:\n        x       - Hidden state, depth position in centimetres\n        v       - Hidden causal state, in this example not used\n        \n    OUTPUT:\n        y       - Temperature in degrees celsius\n        \n    \"\"\"\n    #t0= 20\n    #y=(t0-8)\/(0.2*x**2+1)+8\n    t0=25\n    return t0 -16 \/ (1 + np.exp(5-x\/5))\n\ndef dg_gp(x):\n    \"\"\"\n    Partial derivative of generative process towards x, equation of sensory mapping: g'_gp(x) at point x    \n   \n    INPUTS:\n        x       - Position in centimetres    \n        \n    OUTPUT:\n        y       - Temperature in degrees celsius\n        \n    \"\"\"\n    #t0= 20\n    #y=-2*0.2*x*(t0-8)\/(0.2*x**2+1)**2\n    \n    return -16\/5* np.exp(5-x\/5) \/ (np.exp(5-x\/5)+1)**2\n\n# Show the temperature curve\nx_show = np.arange (-0,50,0.01)\ny_show = g_gp(x_show,0)\ndy_show = dg_gp(x_show)\nplt.plot(y_show, x_show)\n#plt.plot(dy_show, x_show)\nplt.ylabel('Depth (centimeters)')\nplt.xlabel('Temperature (\u00b0 C)')\nplt.gca().invert_yaxis()\nplt.vlines(17, 50, 25, colors='r', linestyles='dashed')\nplt.hlines(25, 10,17, colors='r', linestyles='dashed')\nplt.text(17.3,27,\"Optimal temparature 17\u00b0 C\")\nplt.show;\n\nprint('Temperature at 25 centimetres is: ', g_gp(25,0), ' degrees celsius')","f5149887":"# a very simple example to showcase forward Euler method in action\n# The derivative of x^2 = 2x\n# Taking small steps along the gradient (= derivative) should show x^2\n\ndef f(x):\n    return x**2\n\ndef df(x):\n    return 2*x\n\n_dt=0.001 # The small timestep\n_T=2\n_x = np.arange (-_T,_T,_dt)\n_N=np.int(2*_T\/_dt) # amount of data points\n\n_I = np.zeros(_N)\n_I[0]=4\nfor _i in np.arange(1,_N):\n    _I[_i]= _I[_i-1] + _dt*df(_x[_i-1])\n\nplt.plot(_x,_I, \"-\");\nplt.plot(_x,f(_x), '--');","6fb160c7":"# Setting up the time data:\ndt = 0.005; # integration step, average neuron resets 200 times per second\nT = 5+dt; # maximum time considered\nt = np.arange(0,T,dt)\nN= t.size #Amount of data points\nprint ('Amount of data points: ', N)\nprint ('Starting with', t[0:5])\nprint ('Ending with', t[N-5:N])\nprint ('Data elements', np.size(t))","9163285f":"def makeNoise(C,s2,t):\n    \"\"\"\n    Generate coloured noise \n    Code by Sherin Grimbergen (V1 2019) and Charel van Hoof (V2 2020)\n    \n    INPUTS:\n        C       - variance of the required coloured noise expressed as desired covariance matrix\n        s2      - temporal smoothness of the required coloured noise, expressed as variance of the filter\n        t       - timeline \n        \n    OUTPUT:\n        ws      - coloured noise, noise sequence with temporal smoothness\n    \"\"\"\n    \n    if np.size(C)== 1:\n        n = 1\n    else:\n        n = C.shape[1]  # dimension of noise\n        \n    # Create the white noise with correct covariance\n    N = np.size(t)      # number of elements\n    L =cholesky(C, lower=True)  #Cholesky method\n    w = np.dot(L,np.random.randn(n,N))\n    \n    if s2 <= 1e-5: # return white noise\n        return w\n    else: \n        # Create the noise with temporal smoothness\n        P = toeplitz(np.exp(-t**2\/(2*s2)))\n        F = np.diag(1.\/np.sqrt(np.diag(np.dot(P.T,P))))\n        K = np.dot(P,F)\n        ws = np.dot(w,K)\n        return ws","d2f1b769":"class ai_capsule():\n    \"\"\"\n        Class that constructs a group of neurons that perform Active Inference for one hidden state, one sensory input, one prior\n        In neurology it could eg represent a (micro) column\n        \n        Version 0.1\n    \"\"\"\n    def __init__(self,dt, mu_v, Sigma_w, Sigma_z, a_mu):   \n        self.dt = dt    # integration step\n        self.mu_x = mu_v   # initializing the best guess of hidden state by the hierarchical prior\n        self.F = 0      # Free Energy\n        self.eps_x = 0  # epsilon_x, prediction error on hidden state\n        self.eps_y = 0  # epsilon_y, prediction error on sensory measurement\n        self.Sigma_w = Sigma_w #Estimated variance of the hidden state \n        self.Sigma_z = Sigma_z # Estimated variance of the sensory observation \n        self.alpha_mu = a_mu # Learning rate of the gradient descent mu (hidden state)\n    \n    def g(self,x,v):\n        \"\"\"\n            equation of sensory mapping of the generative model: g(x) at point x \n            Given as input for this example equal to the true generative process g_gp(x)\n        \"\"\"\n        return g_gp(x,v)\n    \n    def dg(self, x):\n        \"\"\"\n            Partial derivative of the equation of sensory mapping of the generative model towards x: g'(x) at point x \n            Given as input for this example equal to the true derivative of generative process dg_gp(x)\n        \"\"\"\n        return dg_gp(x)\n    \n    def f(self,x,v):\n        \"\"\"\n            equation of motion of the generative model: f(x) at point x \n            Given as input for this example equal to the prior belief v\n        \"\"\"\n        return v\n    \n    # def df(self,x): Derivative of the equation of motion of the generative model: f'(x) at point x\n    # not needed in this example \n\n        \n    def inference_step (self, i, mu_v, y):\n        \"\"\"\n        Perceptual inference    \n\n        INPUTS:\n            i       - tic, timestamp\n            mu_v    - Hierarchical prior input signal (mean) at timestamp\n            y       - sensory input signal at timestamp\n\n        INTERNAL:\n            mu      - Belief or hidden state estimation\n\n        \"\"\"\n\n        # Calculate prediction errors\n        self.eps_x = self.mu_x - self.f(self.mu_x, mu_v)  # prediction error hidden state\n        self.eps_y = y - self.g(self.mu_x, mu_v) #prediction error sensory observation\n        # Free energy gradient\n        dFdmu_x = self.eps_x\/self.Sigma_w - self.dg(self.mu_x) * self.eps_y\/self.Sigma_z\n        # Perception dynamics\n        dmu_x   = 0 - self.alpha_mu*dFdmu_x  # Note that this is an example without generalised coordinates of motion hence u'=0\n        # motion of mu_x \n        self.mu_x = self.mu_x + self.dt * dmu_x\n        \n        # Calculate Free Energy to report out\n        self.F = 0.5 * (self.eps_x**2 \/ self.Sigma_w + self.eps_y**2 \/ self.Sigma_z + np.log(self.Sigma_w * self.Sigma_z))\n        \n        return self.F, self.mu_x , self.g(self.mu_x,0)","ff16eea1":"def simulation (v, mu_v, Sigma_w, Sigma_z, noise, a_mu):\n    \"\"\"\n    Basic simplist example perceptual inference    \n   \n    INPUTS:\n        v        - Hydars actual depth, used in generative model, since it is a stationary example hidden state x = v + random fluctuation\n        mu_v     - Hydar prior belief\/hypotheses of the hidden state\n        Sigma_w  - Estimated variance of the hidden state \n        Sigma_z  - Estimated variance of the sensory observation  \n        noise    - white, smooth or none\n        a_mu     - Learning rate for mu\n        \n    \"\"\"\n\n\n    \n    # Init tracking\n    mu_x = np.zeros(N) # Belief or estimation of hidden state \n    F = np.zeros(N) # Free Energy of AI neuron\n    mu_y = np.zeros(N) # Belief or prediction of sensory signal\n    x = np.zeros(N) # True hidden state\n    y = np.zeros(N) # Sensory signal as input to AI neuron\n\n    # Create active inference neuron\n    capsule = ai_capsule(dt, mu_v, Sigma_w, Sigma_z, a_mu)  \n\n    # Construct noise signals with emporal smoothness:\n    np.random.seed(42)\n    sigma = 1\/2000 # smoothness of the noise parameter, variance of the filter\n    w = makeNoise(Sigma_w,sigma,t)\n    z = makeNoise(Sigma_z,sigma,t)\n\n    ssim = time.time() # start sim\n    \n    # Simulation\n    for i in np.arange(1,N):\n        # Generative process\n        if noise == 'white':\n            x[i] = v + np.random.randn(1)* Sigma_w\n            y[i] = g_gp(x[i],v) + np.random.randn(1)* Sigma_z\n        elif noise == 'smooth':\n            x[i]= v + w[0,i]\n            y[i] = g_gp(x[i],v) + z[0,i]\n        else: #no noise\n            x[i]= v \n            y[i] = g_gp(x[i],v)\n        #Active inference\n        F[i], mu_x[i], mu_y[i] = capsule.inference_step(i,mu_v,y[i])\n\n    # Print the results\n    tsim = time.time() - ssim\n    #print('Simulation time: ' + \"%.2f\" % tsim + ' sec' )\n\n    return F, mu_x, mu_y, x, y\n\n# Test case\n\nv = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'no noise',1) # prior and observation balanced, both variance of 1\n\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Basic Active Inference, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='Belief');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[0].grid(1);\naxes[1].plot(t[1:],mu_y1[1:],label='Belief');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='Belief');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","58b2ffc1":"v = 25 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'no noise',1) # prior and observation balanced\n\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Basic Active Inference, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='Belief');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[0].grid(1);\naxes[1].plot(t[1:],mu_y1[1:],label='Belief');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].plot(t[1:],F1[1:],label='Belief');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","3838b8d5":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'no noise',1) # prior and observation balanced\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,0.1,'no noise',1) # underctain about prior\nF3, mu_x3, mu_y3, x3, y3 = simulation(v,mu_v,0.1,1,'no noise',1) # Trust generative model, belief high variance in sensor\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Basic Active Inference, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='balanced');\naxes[0].plot(t[1:],mu_x2[1:],label='Trust sensor');\naxes[0].plot(t[1:],mu_x3[1:],label='Trust Genmodel');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='balanced');\naxes[1].plot(t[1:],mu_y2[1:],label='Trust sensor');\naxes[1].plot(t[1:],mu_y3[1:],label='Trust Genmodel');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\n#axes[1].legend(loc='upper right');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='balanced');\naxes[2].semilogy(t[1:],F2[1:],label='Trust sensor');\naxes[2].semilogy(t[1:],F3[1:],label='Trust Genmodel');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel;\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","1f889574":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,0.1,0.1,'white',1) # prior and observation balanced\n\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Basic Active Inference, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='Belief');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[0].grid(1);\naxes[1].plot(t[1:],mu_y1[1:],label='Belief');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='Belief');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel;","843b1f20":"# showcase the coloured noise of the experiment, use same random seed to reproduce exact same noise\nnp.random.seed(42) # same seed number so this noise depicted below is actually used\nz = makeNoise(1,1\/2000,t)\nplt.plot(t,z[0,:]);","44d1f24e":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'smooth',1) # prior and observation balanced\n\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Basic Active Inference, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='Belief');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[0].grid(1);\naxes[1].plot(t[1:],mu_y1[1:],label='Belief');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='Belief');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","29ccc8b6":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,10,10,'no noise',1) # high variance = high uncertainty =low precision\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'no noise',1) \nF3, mu_x3, mu_y3, x3, y3 = simulation(v,mu_v,0.1,0.1,'no noise',1) # low variance =  low uncertainty = high precision\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Basic Active Inference, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='$\\sigma^2$=10 low precision');\naxes[0].plot(t[1:],mu_x2[1:],label='$\\sigma^2$=1');\naxes[0].plot(t[1:],mu_x3[1:],label='$\\sigma^2$=0.1 high precision');\naxes[0].plot(t[1:],x1[1:],label='Generative process'); #note x1=x2=x3 because no noise\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='$\\sigma^2$=10 low precision');\naxes[1].plot(t[1:],mu_y2[1:],label='$\\sigma^2$=1');\naxes[1].plot(t[1:],mu_y3[1:],label='\\sigma^2$=0.1 high precision');\naxes[1].plot(t[1:],y1[1:],label='Generative process'); #note y1=y2=y3 because no noise\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='$\\sigma^2$=10 low precision');\naxes[2].semilogy(t[1:],F2[1:],label='$\\sigma^2$=1');\naxes[2].semilogy(t[1:],F3[1:],label='\\sigma^2$=0.1 high precision');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","8d212145":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,10,10,'smooth',1) # high variance = high uncertainty =low precision\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'smooth',1) \nF3, mu_x3, mu_y3, x3, y3 = simulation(v,mu_v,0.1,0.1,'smooth',1) # low variance =  low uncertainty = high precision\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Basic Active Inference, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='$\\sigma^2$=10 low precision');\naxes[0].plot(t[1:],mu_x2[1:],label='$\\sigma^2$=1');\naxes[0].plot(t[1:],mu_x3[1:],label='$\\sigma^2$=0.1 high precision');\naxes[0].plot(t[1:],x1[1:],label='Generative process (low precision)'); \naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='$\\sigma^2$=10 low precision');\naxes[1].plot(t[1:],mu_y2[1:],label='$\\sigma^2$=1');\naxes[1].plot(t[1:],mu_y3[1:],label='\\sigma^2$=0.1 high precision');\naxes[1].plot(t[1:],y1[1:],label='Generative process (low precision)'); \naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='$\\sigma^2$=10 low precision');\naxes[2].semilogy(t[1:],F2[1:],label='$\\sigma^2$=1');\naxes[2].semilogy(t[1:],F3[1:],label='\\sigma^2$=0.1 high precision');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","28473cde":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'no noise',1) # \nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'no noise',10) \nF3, mu_x3, mu_y3, x3, y3 = simulation(v,mu_v,1,1,'no noise',100) # \n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Basic Active Inference, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='LR=1');\naxes[0].plot(t[1:],mu_x2[1:],label='LR=10');\naxes[0].plot(t[1:],mu_x3[1:],label='LR=100');\naxes[0].plot(t[1:],x1[1:],label='Generative process'); \naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='LR=1');\naxes[1].plot(t[1:],mu_y2[1:],label='LR=10');\naxes[1].plot(t[1:],mu_y3[1:],label='LR=100');\naxes[1].plot(t[1:],y1[1:],label='Generative process'); \naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='LR=1');\naxes[2].semilogy(t[1:],F2[1:],label='LR=10');\naxes[2].semilogy(t[1:],F3[1:],label='LR=100');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","d5cf57db":"v = 30 # actual depth Hydar\nmu_v = 29.9 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'smooth',1) # variance 1\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,0.1,0.1,'smooth',1) # variance 0.1\nF3, mu_x3, mu_y3, x3, y3 = simulation(v,mu_v,0.01,0.01,'smooth',1) # variance 0.01\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Basic Active Inference, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='$\\sigma^2$=1');\naxes[0].plot(t[1:],mu_x2[1:],label='$\\sigma^2$=0.1');\naxes[0].plot(t[1:],mu_x3[1:],label='$\\sigma^2$=0.001');\naxes[0].plot(t[1:],x3[1:],label='Generative process ($\\sigma^2$=0.001)'); \naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].ticklabel_format(useOffset=False)\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='$\\sigma^2$=1');\naxes[1].plot(t[1:],mu_y2[1:],label='$\\sigma^2$=0.1');\naxes[1].plot(t[1:],mu_y3[1:],label='\\sigma^2$=0.001 ');\naxes[1].plot(t[1:],y3[1:],label='Generative process ($\\sigma^2$=0.001)'); \naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].ticklabel_format(useOffset=False)\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].plot(t[1:],F1[1:],label='$\\sigma^2$=1');\naxes[2].plot(t[1:],F2[1:],label='$\\sigma^2$=0.1');\naxes[2].plot(t[1:],F3[1:],label='\\sigma^2$=0.001');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].ticklabel_format(useOffset=False)\naxes[2].grid(1);","581dccb5":"class ai_capsule_linear():\n    \"\"\"\n        Class that constructs a group of neurons that perform Active Inference for one hidden state, one sensory input, one prior\n        In neurology it could eg represent a (micro) column\n        \n        Version 0.1\n    \"\"\"\n    def __init__(self,dt, mu_v, Sigma_w, Sigma_z, a_mu):   \n        self.dt = dt    # integration step\n        self.mu_x = mu_v   # initializing the best guess of hidden state by the hierarchical prior\n        self.F = 0      # Free Energy\n        self.eps_x = 0  # epsilon_x, prediction error on hidden state\n        self.eps_y = 0  # epsilon_y, prediction error on sensory measurement\n        self.Sigma_w = Sigma_w #Estimated variance of the hidden state \n        self.Sigma_z = Sigma_z # Estimated variance of the sensory observation \n        self.alpha_mu = a_mu # Learning rate of the gradient descent mu (hidden state)\n\n    \n    def g(self,x,v):\n        \"\"\"\n            equation of sensory mapping of the generative model: g(x) at point x \n            Given as input for this example equal to the true generative process g_gp(x)\n        \"\"\"\n        return -0.6*x+30\n        #return -x + 42\n        #return -x\n    \n    def dg(self, x):\n        \"\"\"\n            Partial derivative of the equation of sensory mapping of the generative model towards x: g'(x) at point x \n            Given as input for this example equal to the true derivative of generative process dg_gp(x)\n        \"\"\"\n        return -0.6\n        #return -1\n    \n    def f(self,x,v):\n        \"\"\"\n            equation of motion of the generative model: f(x) at point x \n            Given as input for this example equal to the prior belief v\n        \"\"\"\n        return v\n    \n    # def df(self,x): Derivative of the equation of motion of the generative model: f'(x) at point x\n    # not needed in this example \n\n        \n    def inference_step (self, i, mu_v, y):\n        \"\"\"\n        Perceptual inference    \n\n        INPUTS:\n            i       - tic, timestamp\n            mu_v    - Hierarchical prior input signal (mean) at timestamp\n            y       - sensory input signal at timestamp\n\n        INTERNAL:\n            mu      - Belief or hidden state estimation\n\n        \"\"\"\n\n        # Calculate prediction errors\n        self.eps_x = self.mu_x - self.f(self.mu_x, mu_v)  # prediction error hidden state\n        self.eps_y = y - self.g(self.mu_x, mu_v) #prediction error sensory observation\n        # Free energy gradient\n        dFdmu_x = self.eps_x\/self.Sigma_w - self.dg(self.mu_x) * self.eps_y\/self.Sigma_z\n        # Perception dynamics\n        dmu_x   = 0 - self.alpha_mu*dFdmu_x  # Note that this is an example without generalised coordinates of motion hence u'=0\n        # motion of mu_x \n        self.mu_x = self.mu_x + self.dt * dmu_x\n        \n        # Calculate Free Energy to report out\n        self.F = 0.5 * (self.eps_x**2 \/ self.Sigma_w + self.eps_y**2 \/ self.Sigma_z + np.log(self.Sigma_w * self.Sigma_z))\n        \n        return self.F, self.mu_x , self.g(self.mu_x,0)\n    \ndef simulation (v, mu_v, Sigma_w, Sigma_z, noise, a_mu, type):\n   \n    \"\"\"\n    Basic simplist example perceptual inference    \n   \n    INPUTS:\n        v        - Hydars actual depth, used in generative model, since it is a stationary example hidden state x = v + random fluctuation\n        mu_v     - Hydar prior belief\/hypotheses of the hidden state\n        Sigma_w  - Estimated variance of the hidden state \n        Sigma_z  - Estimated variance of the sensory observation  \n        noise    - white, smooth or none\n        a_mu     - Learning rate for mu\n        type     - 1= non-linear generative model; 2 - simple linear generative mode\n        \n    \"\"\"\n    \n    # Init tracking\n    mu_x = np.zeros(N) # Belief or estimation of hidden state \n    F = np.zeros(N) # Free Energy of AI neuron\n    mu_y = np.zeros(N) # Belief or prediction of sensory signal\n    x = np.zeros(N) # True hidden state\n    y = np.zeros(N) # Sensory signal as input to AI neuron\n\n    # Create active inference neuron\n    if type==1:\n        capsule = ai_capsule(dt, mu_v, Sigma_w, Sigma_z, a_mu)  \n    elif type==2:\n        capsule = ai_capsule_linear(dt, mu_v, Sigma_w, Sigma_z, a_mu)\n\n    # Construct noise signals with emporal smoothness:\n    np.random.seed(1234)\n    sigma = 1\/64 # smoothness of the noise parameter, variance of the filter\n    w = makeNoise(Sigma_w,sigma,t)\n    z = makeNoise(Sigma_z,sigma,t)\n\n    ssim = time.time() # start sim\n    \n    # Simulation\n    for i in np.arange(1,N):\n        # Generative process\n        if noise == 'white':\n            x[i] = v + np.random.randn(1)* Sigma_w\n            y[i] = g_gp(x[i],v) + np.random.randn(1)* Sigma_z\n        elif noise == 'smooth':\n            x[i]= v + w[0,i]\n            y[i] = g_gp(x[i],v) + z[0,i]\n        else: #no noise\n            x[i]= v \n            y[i] = g_gp(x[i],v)\n        #Active inference\n        F[i], mu_x[i], mu_y[i] = capsule.inference_step(i,mu_v,y[i])\n\n    # Print the results\n    tsim = time.time() - ssim\n    #print('Simulation time: ' + \"%.2f\" % tsim + ' sec' )\n\n    return F, mu_x, mu_y, x, y\n\n# Test case\n\nv = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'no noise',1,1) # prior and observation balanced, both variance of 1\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'no noise',2,1) # prior and observation balanced, both variance of 1\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Basic Active Inference, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='non-linear exact');\naxes[0].plot(t[1:],mu_x2[1:],label='linear approximation');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[0].grid(1);\naxes[1].plot(t[1:],mu_y1[1:],label='non-linear exact');\naxes[1].plot(t[1:],mu_y2[1:],label='linear approximation');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='non-linear exact');\naxes[2].semilogy(t[1:],F2[1:],label='linear approximation');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","8a5baccb":"## Experiment 1.08 - Tuning learning rates\nSame example as experiment 1.01 but with different learning rates with balanced variances: $\\sigma_z^2$ equals $\\sigma_w^2$   \nNote: for this example there is no noise in the generative process (although the generative model expects noise to be present) to best see how the belief develops.  ","8ee66f61":"## Notes\n* The belief of the hidden state and prediction of the sensory signal is quite stable despite noisy sensory signals. And quite simular as experiment 1.01","d0f530bd":"<a id='sec2'><\/a>\n# Active inference code version 0.1\nThe heart of the code for perceptual inference you will find in the inference_step function, where 1 iteration of perceptual inference is executed every time it is called.  \n1. First the prediction errors are calculated: $\\varepsilon_x = \\mu_x-f(\\mu_x,\\mu_v) = \\mu_x-\\mu_v $ and $\\varepsilon_y = y-g(\\mu_x,\\mu_v) $  \n1. Next the motion of the hidden state is calculated: $\\dot{\\mu}_x = - \\alpha_x \\frac{\\partial \\mathcal{F}( y,\\mu)}{\\partial \\mu_x} $ where $ \\frac{\\partial \\mathcal{F}( y,\\mu)}{\\partial \\mu_x})= \\frac{1}{\\sigma_w^2}\\varepsilon_x - g'(\\mu_x) \\frac{1}{\\sigma_z^2}\\varepsilon_y $. \n1. Updating the belief of $\\mu_x$ with the [forward Euler method](https:\/\/en.wikipedia.org\/wiki\/Euler_method): $\\mu_x(t+\\Delta t) = \\mu_x(t) + \\Delta t \\cdot \\dot{\\mu}_x$.  \nEuler Forward is a very basic (first order) implementation of a gradient descent, more advanced methods like [odeint](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.integrate.odeint.html) can be used but have chosen in this notebook to keep it simple for the most intuitive understanding. \n\nAnd to summarize it in a picture:\n<img src=\"https:\/\/i.imgur.com\/OvShzkF.jpg\" width=900>\n","9bbe93b4":"## Experiment 1.10 - Simplified generative model\nIn the above example we assumed Hydar learned the exact (non-linear) generative process (and it's derivatives). What would happen if Hydar has learned a simple linear generative model, basically learned that if you go deeper the temperature drops, for example $y = -0.6*x+30$\n","c8643592":"## Experiment 1.07 - Noise to signal ratio coloured noise\nSame example as experiment 1.06 but now with the actual coloured noise in the generative process  \n","ff9ed33f":"## Experiment 1.02 - Observe what is expected\nQuick check that what happens if the prior expectation and actual sensor readings do match. Hydar's prior and its actual depth is 25 centimeters, so it is expecting and receiving a temperature of 17 degrees.","1d37bde0":"## Notes\n* The red line shows the temperature and depth resulting from the low precision sensory input (a variance of 10 with a mean of 17). The corresponding active inference estimation is the blue (low precision) line. Interesting to see that despite the red line goes all over the place the green line is relative straight. Good for Hydar else it would be very dizzy. \n","b69a2c05":"## Experiment 1.06 - Noise to signal ratio\nSame example as experiment 1.01 but with different estimations of the noise (different variances 0.1 \/ 1 \/ 10) with balanced variances: $\\sigma_z^2$ equals $\\sigma_w^2$   \nNote: for this example there is no noise in the generative process (although the generative model expects noise to be present) to best see how the belief develops.  \n\n","0ad02263":"## Experiment 1.03 - Trust your sensors or your prior belief ?\nSame example as experiment 1.01 but now showcase the delta between\n1.  Balanced variances : $\\sigma_z^2$ equals $\\sigma_w^2$ \n1.  Confidence in own sensory observations (bottom-up): low variance $\\sigma_z^2$ compared $\\sigma_w^2$\n1.  Confidence in the internal model including prior (top-down): low variance $\\sigma_w^2$ compared $\\sigma_z^2$","2b08f551":"## Experiment 1.05 - Coloured noise\nSame example as experiment 1.01 but now with actual coloured noise in the generative process (couloured noise as explained in the [second notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-2))\n","6a7f5b83":"## Experiment 1.01 - The first\nSince this is the first experiment of the first example, let me position what we are looking at.  \nHydar's hypotheses or prior $\\mu_v$ is that it's depth is 25 centimeters, so it is expecting a temperature of 17 degrees. Its sensors give somehow a reading of approx 13 degrees, so around 30 centimeters depth. What to believe? Should it trust it's sensor or its internal hypothesis\/prior expectation?  \n\nRemember from the [third notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-3) that our perception is not only based on sensory observations but is also based on what we expect to perceive, the prior. But in this example Hydar beliefs it is in a certain place\/depth while its sensory observations give different readings, the brain needs to figure out $p(x \\mid y)$. It needs to weigh prior knowledge\/hypothesis with new sensory input in a Bayesian way. To recap the logic, all the way back to the [first notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-1), casting it as an Bayesian probability it need to figure out $p(x\\mid  y)=\\frac{p(y \\mid  x) * p(x)}{p(y)} $ by approximating it by a recognition density and we learned in the [second notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-1) that under the Laplace approximation it is equal to find the most likely mean $\\mu_x$ of the recognition density that maximizes $p(x \\mid  y)$ (estimating the mean of the hidden causes given sensory input) by minimizing the free energy.\n\nlet's see what happens if Hydar has equal trust in the sensor and the hypotheses\/prior, for this example there is no noise in the generative process yet (although the generative model expects noise to be present) to best see how the belief develops. \n\n","3af633fa":"### Notes\n\nYes, by selecting $\\mu_v$ close to $v$ (so also the prediction errors become low in the free energy equation) the resulting free energy becomes negative for $\\sigma^2<<1$ implying a negative surprise. Hmm, food for thought.","9c399565":"### Notes\n* as expected if Hydar trust the sensor more, it's belief about the depth $p(x \\mid y)$ is closer to the depth associated with the sensor input, so it also predicts a temperature closer to the sensor input.\n* if Hydar trust the prior expectation more, it's belief about the depth $p(x \\mid y)$ is far closer to the depth associated with the prior, so it also predicts a temperature much closer to the prior input.\n* Interesting to observe that the noise estimations start to have a different interpretation  \n\nNoise| Description related to generative process | new interpretation related to generative model |\n--- | --- | --- |\n$\\sigma^2_{z}$ | Covariance  of noise on sensory observation  |  Confidence in own sensory observations (bottom-up)\n$\\sigma^2_{w}$ | Covariance of noise on hidden state | Confidence in the internal model including prior (top-down)\n ","7549598f":"## Thank you\nPlease do copy this kernel and try out for yourself to better understand active inference and the free energy principle. My intent is to help catalyse knowledge and research on Active Inference in an engineering\/robotics\/data sciences\/machine learning context. Hope you liked my notebook (upvote top right), my way to contribute back to this fantastic Kaggle platform and community.","f907c5ee":"## Notes\n* I tuned the noise down to a variance of 0.1 to still see the curve of the Free energy coming down. If you run the same code with higher variances (eg >1) it still works nicely, see also one of the later experiments.\n* Question to myself on the to-do list: In artificial Neural Networks a common approach is to normalize the signals, didn't see yet such a thing in active inference?","431a0efe":"\n## Generative model\nThe generative model (model in the Hydra brain to encode a probabilistic model of the environment\/world in which it is immersed)\n* $x = f(x,v) + w $ \n* $y = g(x,v) + z $    \n\nwhere\n* The function of motion $f(x,v)$  is given as input and Hydar believes it's depth is simply equal to it's prior belief $f(\\mu_x,\\,u_v) = \\mu_v $. Note that the scope of this example is a static case (e.g. still water) so $x=f(x,v) + w$ instead of $\\dot x=f(x,v) + w$\n* The function of sensory mapping $g(x,v)$ is given as input and is equal to the true generative process. Same for and it's derivative $g'(x)$.\n\n\n","c15b2d93":"## Free energy\nThe Free Energy for one active inference capsule is (the simplest univariate case we used in the [second notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-2\/#sec5))\n\n$$\\mathcal{F}(y,\\mu) = \\frac{1}{2\\sigma_w^2}\\varepsilon_x^2 + \\frac{1}{2\\sigma_z^2}\\varepsilon_y^2  + \\frac{1}{2} ln\\: ({\\sigma_z^2\\sigma_w^2) } $$\nWhere\n* $\\varepsilon_x = \\mu_x-f(\\mu_x,\\mu_v) = \\mu_x-\\mu_v $ is the the motion prediction error.\n* $\\varepsilon_y = y-g(\\mu_x,\\mu_v) = y-g_{gp}(\\mu_x,\\mu_v) $ is the sensory prediction error. \n\n\n## Inference by gradient descent\nPerceptual inference by iterativly taking steps proportional to the negative of the partial derivative of the Free Energy with respect to $\\mu_x$. Following the steps of the [fourth notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-4), the gradient descent can be written as a differential equation:\n\n$$\\dot{\\mu}_x = \\mathcal{D}{\\mu}_x - \\alpha_x \\frac{\\partial \\mathcal{F}( y,\\mu)}{\\partial \\mu_x} $$ \n\nwher $\\mathcal{D}{\\mu}_x$ is 0 because this example is without generalised coordinates of motion.  \nThe derevative of the Free Energy can be calculated straightforward\n$$ \\frac{\\partial \\mathcal{F}(y,\\mu)}{\\partial {\\mu}_x}=  \\frac{\\partial \\varepsilon_x}{\\partial \\mu_x} \\frac{1}{\\sigma_w^2}\\varepsilon_x + \\frac{\\partial \\varepsilon_y}{\\partial \\mu_x} \\frac{1}{\\sigma_z^2}\\varepsilon_y = \\frac{1}{\\sigma_w^2}\\varepsilon_x - g'(\\mu_x) \\frac{1}{\\sigma_z^2}\\varepsilon_y $$ \n\n","17208ecb":"# Experiments\nLet's see the theory in action and showcase perceptual inference in our simulation environment. In this example, Hydar has to predict its body temperature and therefor needs to infer the hidden state (its depth).","ce2d9389":"## Long live Hydar!\n\nThe example is set in the simulation environment where Hydar needs to infer its depth based on temperature sensor readings, even when the sensor readings are not what is expected.  \n\nAs introduced in the 4th notebook, [Hydar](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-4\/#sec4) is an early evolutionary imaginary aquatic ancestor that must preserve its physical integrity to survive. For example, it needs to keep in a certain depth range.\n\nRemember from the [first notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-1) that in essence the brain is a prediction mechanism, one that attempts to minimize the error of its hypothesis about the world and the sensory input it receives:\n* The skull-bound brain needs to infer the world.\n* The brain builds an internal model of the world using Bayesian inference (the generative model).\n* Discrepancies between the internal model (the prediction) and the sensory observations result in prediction error.\n* The brain aims to minimize the Free Energy which is equivalent to minimizing prediction errors.\n* By either Improving perception, Acting on the environment, Learning the generative model, Optimizing expected precision of uncertainty. \n\nThe scope of this notebook is improving perception. In this example Hydar's brain is equipped with one active inference capsule to encode the generative model. With one single sensor (y), in this case to measure the temperature in degrees Celsius, one hidden state it tries to infer (x), in this case the depth, and one prior (v) representing its top-down hypotheses of the hidden state\/depth. \n\nIn this example the water is still \/ not moving, and also Hydar cannot move (yet).\n\n<img src=\"https:\/\/i.imgur.com\/wTiEZK9.jpg\" width=500>\n\n","5493500f":"# How the brain might function - code example 1\n### Free Energy Principle tutorial without a PhD\n  \n<img src=\"https:\/\/cdn.pixabay.com\/photo\/2018\/05\/08\/08\/44\/artificial-intelligence-3382507_960_720.jpg\" width=500>\n<center>Image by <a href=\"https:\/\/pixabay.com\/users\/geralt-9301\/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3382507\">Gerd Altmann<\/a> from <a href=\"https:\/\/pixabay.com\/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3382507\">Pixabay<\/a> <\/center>   \n<br>\n\nWelcome to my notebook on Kaggle. I did record my notes with examples so it might help others in their journey to understand **Active Inference** minimizing the underlying **Free Energy**. Neuroscience has produced a candidate which suggests that several global brain theories might be unified within a free-energy framework: [Free Energy Principle](https:\/\/en.wikipedia.org\/wiki\/Free_energy_principle) (FEP) by [Karl Fristion](https:\/\/en.wikipedia.org\/wiki\/Karl_J._Friston): The free-energy principle is an attempt to explain the structure and function of the brain, starting from the very fact that we exist.\n\nThis is the first code example notebook and it belongs to a series of notebooks on the Free Energy Principle tutorial without a PhD. If you are interested to understand this code please first read Inference in the brain: [part 1](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-1), [part 2](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-2), [part 3](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-3), [part 4](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-4). There is a lot to explain but I promise you an excellent journey into somthing that might hold the key to true AI.\n\n","4b92da1e":"## Generative process\n\n   \nThe generative process (the simulation environment simulating the external environment\/world generating the sensory states in this example)\n* $x = f_{gp}(x,v) + w $ \n* $y = g_{gp}(x,v) + z $    \n\nwhere\n* x is the position (depth in centimetres)\n* y is the temperature (degrees Celsius)\n* The function of motion  $f_{gp}(x,v)$ of this example is a static scope (e.g. still water) so $x= v + w$ (where $w$ is the random fluctuation) thus Hydar basically stays on it's initial position (plus\/minus some noise)\n* The function of sensory mapping  $g_{gp}(x,v)$  of this example is defined as $g_{gp}(x,v) = t_0 -\\frac{16}{1+e^{5-\\frac{x}{5}}}$ with $t_0=25$ ,  see picture above and code below.\n* The derivative of the function of sensory mapping with respect to $x$ is $g'_{gp}(x) = -\\frac{16e^{5-\\frac{x}{5}}}{5\\left(1+e^{5-\\frac{x}{5}}\\right)^2}$ (calculated upfront with some good old high-school math)\n\n\n","77aa029c":"## Experiment 1.09 - negative Free Energy?\nThe Free energy is defined as:\n$$\\mathcal{F}(y,\\mu) = \\frac{1}{2\\sigma_w^2}\\varepsilon_x^2 + \\frac{1}{2\\sigma_z^2}\\varepsilon_y^2  + \\frac{1}{2} ln\\: ({\\sigma_z^2\\sigma_w^2) } $$\nFocussing on the right side of the equation you can observe that: \n* if the variance becomes bigger the free energy becomes (proportionally) bigger. In other words, less able to lower the free energy in a noisy environment. Makes sense!\n* If the variance becomes smaller the free energy can be optimized lower, makes sense, however if the variance becomes much smaller than 1 the right side of the equation becomes negative and could the free energy become negative?","9b26c6c1":"### Notes\n* As expected, the sensor readings and prior confirm immediately, only 1 observation needed for Hydar to know it's depth. A good sign to showcase the strength of a model with top-down predictions and bottom-up sensory observations to perceive the world fast. \n* The Free Energy is zero, optimal","26b2d752":"## Notes\n+ Please do copy this notebook and experiment for yourself with different simplified generative models, conditions, etc\n+ The prediction\/approximation will be as good as the generative model is (for the range to be estimated).\n    + If you take for example a generative model y=-x, so it only knows the right direction, the free energy is minimized but far bigger ( FE > 100) compared to the minimization under the perfect non-linear generative model (FE < 5). The good news is that there is enough room to lower the free energy by optimizing the generative model, so the prediction error can be used to start optimizing the model as well (one of the next notebooks)\n    + If you take for example a generative model y=-x+42, so it knows the right direction and has a good approximation around the 17 centimeter depth zone. In that case Hydar is able to optimize the Free Energy and get good predictions like the non-linear exact model but if the depth has to be estimated in for example the 10 centimeter depth range the linear model starts to deviate more from the actual exact non-linear model and the Free Energy optimization is less.\n    + If you take for example a generative model y=-0.6x+30, a little more sophisticated but still linear, it gives Hydar a fair estimation around the 10-40 centimeter depth range it gives quite the same prediction\/estimations as the non-linear exact model (even able to lower the reach lower free energy levels around 25 centimeter depth). As long as Hydar stays in the 10-40 centimeter a simplified linear model serves Hydar well (same argument like you don't need to understand all the dynamics of gravity between the sun, earth, moon etc to have a model in your mind that the sun rises early in the morning and sets in the evening)\n    ","e14197f1":"## Notes\n\n* As expected higher learning rates deliver faster convergence\n* Same effect as higher noise precisions of experiment 1.06 which is understandable given the underlying math how the motion of the hidden state is calculated: $\\dot{\\mu}_x = - \\alpha_x \\frac{\\partial \\mathcal{F}( y,\\mu)}{\\partial \\mu_x} $ where $ \\frac{\\partial \\mathcal{F}( y,\\mu)}{\\partial \\mu_x})= \\frac{1}{\\sigma_w^2}\\varepsilon_x - g'(\\mu_x) \\frac{1}{\\sigma_z^2}\\varepsilon_y $.\n* A large learning rate of e.g. 100 does not make the convergence unstable","4aa51acd":"# Scope code example 1\nThis first example is a proof-of-concept that the very fundament works: \nPerceptual inference by minimizing the Free Energy. One active inference capsule (single observation, single hidden state, single prior) without generalised coordinates of motion inferring the hidden state based on the sensory observations.\n\nScope of the example: \n* One active inference capsule (single hidden state x, single sensory channel y and single hierarchical prior v, non-hierarchical generative model) \n* No generalised coordinates of motion (will be added in next example)\n* Static environment (no differential equation for equation of motion but f(x) expresses an a priori static reference point) \n* The generative model $\\mu_\\theta$ and expected precision of uncertainty $\\mu_\\lambda$ are invariant during the process and for this example given as input. Remember ${\\mu}_x$ is estimated by fast neuronal states while $\\mu_\\theta$ and $\\mu_\\lambda$ adapt on slower timescales (e.g. synaptic efficacy) so invariant during perceptual inference. We simply assume the generative model \/ noise estimation has already been learned and is known (so, function of motion and sensory mapping are given as input).   \n* The brain estimate for the hierarchical prior ($\\mu_v$) is given as input\n* Perceptual inference by minimizing the Free Energy for $\\mu_x$:   \n\n$$\\mu_x=\\underset{\\mu_x }{Argmin}\\:  \\mathcal{F}( y,\\mu)$$ \n* by iterativly taking steps proportional to the negative of the partial derivative of the Free Energy with respect to $\\mu_x$.\n ","2bea9285":"## notes\n* If there is a high precision (low variance), Hydar beliefs it's model\/sensors are more acturate so it can faster converge to the estimation.\n* If there is a low precision (high variance), Hydar takes longer to converge to the position or temperature, also is Free Energy is low, so not a lot to optimize\n* Makes sense, the higher (the internal belief of) the precision, the lower uncertainty, the \u00a8bigger steps\u00a8 to converge because Hydar can trust the signals\/beliefs.\n* Note that in active inference noise levels (variance $\\sigma^2$) are absolute numbers and not relative to the signal level. E.g. a variance of 1 does not tell how much noise it is, if the main signal is in a order of magnitude of 100 it is 1% but if the main signal was around 1 it is 100%. Just remember to chose sigma relative to the signal not too small and not big, such that the consequence of it is clearly visible in the graphs.\n* On my to-do list: I read somewhere for the Free Energy it is suggested that noise variance >1 but that would be strange for small signals?","5771679b":"## Summary code example 1\nTo recap all you need to perform active inference in the table below:\n\nWhat| Symbol | Description |\n--- | --- | --- |\nposition | x  | single external hidden environment state the Hydar brain tries to infer, in this case the position. <br> Hydar lives in a one-dimensional plane, so it is the depth in centimetres | \nposition | $\\mu_x$  | Hydars belief or estimation of the depth, the hidden state x it's tries to infer |\nbody temperature sensor | y  | single sensory observation, in this case the temperature.  | \nprior | v  | The initial depth of Hydar in the generative process | \nprior | $\\mu_v$  | Hydars prior belief of the depth |\nfunction of sensory mapping| $g(x,v)$  | Given as input and is equal to the true generative process $g(x,v) = g_{gp}(x,v) = t_0 -\\frac{16}{1+e^{5-\\frac{x}{5}}}$  | \nDerivative function of sensory mapping| $g'(x)$  | Given as input and is equal to the derivative of the true generative process towards $x$ $g'(x) = g_{gp}'(x) = -\\frac{16e^{5-\\frac{x}{5}}}{5\\left(1+e^{5-\\frac{x}{5}}\\right)^2}$  | \nfunction of motion | $f(x,v)$  | Given as input and Hydar believes it's depth is simply equal to it's prior belief $f(x) = \\mu_v $ with no motion |\nFree Energy | $\\mathcal{F}(y,\\mu)$  | $\\mathcal{F}(y,\\mu) = \\frac{1}{2\\sigma_w^2}\\varepsilon_x^2 + \\frac{1}{2\\sigma_z^2}\\varepsilon_y^2  + \\frac{1}{2} ln\\: ({\\sigma_z^2\\sigma_w^2) } $ |\nsensory prediction error | $\\varepsilon_y$  | $\\varepsilon_y = y-g(\\mu_x,\\mu_v) = y-g_{gp}(\\mu_x,\\mu_v) $ |\nmotion prediction error | $\\varepsilon_x$  | $\\varepsilon_x = \\mu_x-f(\\mu_x,\\mu_v) = \\mu_x-\\mu_v $ |\nDerevative Free Energy | $\\frac{\\partial \\mathcal{F}(y,\\mu)}{\\partial {\\mu}_x}$  | $ \\frac{\\partial \\mathcal{F}(y,\\mu)}{\\partial {\\mu}_x}=  \\frac{\\partial \\varepsilon_x}{\\partial \\mu_x} \\frac{1}{\\sigma_w^2}\\varepsilon_x + \\frac{\\partial \\varepsilon_y}{\\partial \\mu_x} \\frac{1}{\\sigma_z^2}\\varepsilon_y = \\frac{1}{\\sigma_w^2}\\varepsilon_x - g'(\\mu_x) \\frac{1}{\\sigma_z^2}\\varepsilon_y $ |\n\n\n\n","5eec8442":"## Experiment 1.04 - White noise\nSame example as experiment 1.01 but now with some actual white noise in the generative process","60ac189c":"### Notes\n* Hydars belief about the depth start with 25 centimeters as it's prior and the more the sensory observations keep giving a persistent signal that the temperature is lower is starts shifting it's belief about the depth $p(x \\mid y)$ to 27 centimeters.\n* The Free energy declines to a minim level but can't reduce further to 0 which makes sense because the gap between prior expectation and the sensor readings.\n* What in essence is happening \u00a8under the hood\u00a8 is that the mean of the hidden state $\\mu_x$ is in both error terms ($\\varepsilon_x = \\mu_x-f(\\mu_x) $ and $\\varepsilon_y = y-g(\\mu_x) $) and minimization can be done by moving $\\mu_x$ to the prior ($f(\\mu_x)=v$) minimizing $\\varepsilon_x$ or by moving $\\mu_x$ to the prior the sensor stimulus $g(\\mu_x) $ minimizing $\\varepsilon_y$. Both terms are weighted by the precision (inverse covariance) of the signals. As you can see in the motion of $\\dot{\\mu}_x = - \\alpha_x \\frac{\\partial \\mathcal{F}( y,\\mu)}{\\partial \\mu_x} = - \\alpha_x  ( \\frac{1}{\\sigma_w^2}\\varepsilon_x - g'(\\mu_x) \\frac{1}{\\sigma_z^2}\\varepsilon_y) $ \n"}}