{"cell_type":{"95d37706":"code","22d796ea":"code","e5846f3a":"code","417b6d4e":"code","30708d38":"code","f06481e9":"code","cfaf9a29":"code","5ad54297":"code","97a85ff8":"code","8dcb03d0":"code","bea17674":"code","e1f441fb":"code","d96a7c8d":"code","66ec68b1":"code","8f7ee8df":"code","3474831e":"code","bede9761":"code","21290471":"code","831394c3":"code","5af1d412":"code","3715149e":"code","0c510d30":"code","463084a8":"code","a558f6bc":"code","5239f7cc":"code","acb5171b":"code","e00ce151":"code","c68a4a5c":"code","39886ad5":"code","6e85db22":"code","da988d8e":"code","31270d99":"code","9a34749c":"code","85be74d8":"code","0750a0cc":"code","af91753f":"markdown","891446df":"markdown","1d89f6c8":"markdown","e8408a4f":"markdown","79fa5bf3":"markdown","131f92fb":"markdown","00a5403b":"markdown","c3ca9539":"markdown","d8a516dc":"markdown","221ee7b3":"markdown","f86a8d58":"markdown","af70c142":"markdown","d512baba":"markdown","b65e6d00":"markdown","d4a72e54":"markdown","ece12765":"markdown","ce68ee6d":"markdown","af6a5bc9":"markdown","867bc07e":"markdown","ff086bef":"markdown","7e434079":"markdown","f117f1dd":"markdown","6d03c750":"markdown","14682511":"markdown","761a4049":"markdown","2c4b17b7":"markdown","0b8c62c8":"markdown","7912c548":"markdown","4cf3fe50":"markdown","fc0ba89f":"markdown"},"source":{"95d37706":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","22d796ea":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport math\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import  GaussianNB\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder,MinMaxScaler,StandardScaler,LabelEncoder\nimport lightgbm as lgbm\nfrom sklearn.model_selection import train_test_split,StratifiedKFold,KFold,RepeatedStratifiedKFold\nimport matplotlib.pyplot as plt\n","e5846f3a":"monthly_exp=pd.read_csv(\"..\/input\/turkiye-is-bankas-machine-learning-challenge-3\/monthly_expenditures.csv\")\ntrain_set=pd.read_csv(\"..\/input\/turkiye-is-bankas-machine-learning-challenge-3\/train.csv\")\ntest_set=pd.read_csv(\"..\/input\/turkiye-is-bankas-machine-learning-challenge-3\/test.csv\")","417b6d4e":"train_set","30708d38":"monthly_exp","f06481e9":"train_set.describe()","cfaf9a29":"monthly_exp.describe()","5ad54297":"sns.countplot(x=\"target\", data=train_set)","97a85ff8":"sns.displot(train_set[\"yas\"],label=train_set[\"target\"])\n","8dcb03d0":"sns.displot(train_set[\"kidem_suresi\"],label=train_set[\"target\"])","bea17674":"merged=pd.merge(monthly_exp,train_set,on=\"musteri\",how=\"inner\")","e1f441fb":"merged","d96a7c8d":"df2 =pd.pivot_table(monthly_exp, \n                        values=['aylik_toplam_tutar','islem_adedi'],\n                        index=['musteri'],\n                        columns=['sektor'],\n                        aggfunc=np.sum,\n                        fill_value=0)\ndf2.columns =[\"toplam_\"+s1 +\"_\"+s2.lower() for (s1,s2) in df2.columns.tolist()]","66ec68b1":"df2.head()","8f7ee8df":"df3=monthly_exp.pivot_table(\n              values=['aylik_toplam_tutar','islem_adedi'],\n              index=['musteri'],\n              columns=['tarih'],\n              aggfunc=np.sum,\n              fill_value=0)\n\ndf3.columns=[s1+\"_\"+str(s2) for (s1,s2) in df3.columns.tolist()]\n","3474831e":"df4=pd.merge(df2,df3,on=\"musteri\",how=\"inner\")","bede9761":"merged_=pd.merge(df4,train_set,on=\"musteri\",how=\"inner\")\ntest_merged=pd.merge(df4,test_set,on=\"musteri\",how=\"inner\")","21290471":"merged_.describe()","831394c3":"outlier_cols=[\n 'toplam_aylik_toplam_tutar_bahce_cicekclk',\n 'toplam_aylik_toplam_tutar_benzin_yakit',\n 'toplam_aylik_toplam_tutar_dijital_urunlr',\n 'toplam_aylik_toplam_tutar_egl_spor_hobi',\n 'toplam_aylik_toplam_tutar_elkt_esya_bilg',\n 'toplam_aylik_toplam_tutar_giyim_aksesuar',\n 'toplam_aylik_toplam_tutar_ickili_yerler',\n 'toplam_aylik_toplam_tutar_klp_dernk_sosy',\n 'toplam_aylik_toplam_tutar_kuyumcu',\n 'toplam_aylik_toplam_tutar_restoran_cater',\n 'toplam_aylik_toplam_tutar_saglik_hizmtlr',\n 'toplam_aylik_toplam_tutar_tasimacilik',\n 'toplam_aylik_toplam_tutar_turizm_konaklm',\n ]\n\nz = np.abs(stats.zscore(merged_[outlier_cols]))\nmerged_x =merged_[outlier_cols][(z < 3).all(axis=1)]\nfor col in outlier_cols:\n  merged_[outlier_cols]=merged_x[outlier_cols]\nmerged_.dropna(subset = outlier_cols, inplace=True)\nmerged_","5af1d412":"full_df=pd.concat([merged_, test_merged], ignore_index=True)","3715149e":"full_df[\"kidem_suresi\"].describe()","0c510d30":"full_df.loc[full_df[\"kidem_suresi\"]<0,\"kidem_suresi\"]=0\nfull_df[\"kidem_suresi\"]=full_df[\"kidem_suresi\"]\/12","463084a8":"full_df[\"musteri_oldugu_yas\"]=full_df[\"yas\"]-full_df[\"kidem_suresi\"]\nfull_df.loc[full_df[\"musteri_oldugu_yas\"]<=0,\"musteri_oldugu_yas\"]=0\nfull_df[\"toplam_harcama\"]=full_df.iloc[:,1:14].sum(axis=1)\nfull_df[\"max_harcanan_ay\"]=full_df.iloc[:,27:33].idxmax(axis=1)\nfull_df[\"max_harcanan_ay\"]=full_df[\"max_harcanan_ay\"].str[19:]\nfull_df[\"max_harcanan_ay\"]=pd.to_datetime(full_df[\"max_harcanan_ay\"]).dt.month\nfull_df[\"toplam_islem\"]=full_df.iloc[:,14:27].sum(axis=1)\nfull_df[\"islembasi_harcama\"]=(full_df[\"toplam_harcama\"]\/full_df[\"toplam_islem\"]).astype(float)","a558f6bc":"full_df.isna().sum()","5239f7cc":"full_df.drop([\"tarih\",\"meslek_grubu\"],axis=1,inplace=True)\nmcv_isdurumu=merged_[\"is_durumu\"].mode()[0]\nmcv_egitim=merged_[\"egitim\"].mode()[0]\nfull_df[\"islembasi_harcama\"].fillna(0,inplace=True)\nfull_df[\"is_durumu\"].fillna(mcv_isdurumu,inplace=True)\nfull_df[\"egitim\"].fillna(mcv_egitim,inplace=True)\n","acb5171b":"for col in ['egitim','is_durumu']:\n  dummies = pd.get_dummies(full_df[col])\n  full_df[dummies.columns] = dummies\nfull_df.drop([\"egitim\",\"is_durumu\"],1,inplace=True)","e00ce151":"full_df","c68a4a5c":"full_df.drop(\"musteri\",1,inplace=True)","39886ad5":"train=full_df[:len(merged_)]\ntest=full_df[len(merged_):].drop(\"target\",axis=1).values\n\nX=train.drop(\"target\",axis=1).values\ny=train[\"target\"].values.astype(float)\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.4,stratify=y,random_state=42)","6e85db22":"skf =KFold(n_splits=10,shuffle=True,random_state=42) \npred_test=0\n\nfor train_index, test_index in skf.split(X_train,y_train): \n    x_train_fold, x_test_fold = X_train[train_index], X_train[test_index] \n    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n    lgtrain = lgbm.Dataset(x_train_fold, label=y_train_fold)\n    lgval = lgbm.Dataset(x_test_fold, label=y_test_fold)\n    params = {\n        \"objective\" : \"binary\",\n        \"metric\" : \"auc\",\n        'boosting_type' : 'goss',\n        'max_depth' : 4,#-1\n        \"num_leaves\" : 100,#20\n        \"learning_rate\" : 0.01,#0.01\n        \"feature_fraction\" : 0.6,#0.7 #0.5\n        \"bagging_freq\" : 10, #10 #20\n        \"bagging_seed\" : 42, #2018\n        \"verbosity\" : -1,\n        'lambda_l2' : 1.1,#0.1\n        'lambda_l1' : 0.000001,#0.00001,#0,\n        'max_bin' : 90 #default=250 #200 #170 #120 #90\n\n    }\n    model=lgbm.train(params,lgtrain,num_boost_round=1000,valid_sets=lgval,early_stopping_rounds=1000)\n    pred=model.predict(X_test)\n\n   \n\n    pred_test+=pred\/10\n","da988d8e":"fpr, tpr, thresholds = metrics.roc_curve(y_test,pred_test)\nauc_score=metrics.auc(fpr, tpr)\nauc_score","31270d99":"plt.plot(fpr,tpr,label=\"AUC=\"+str(auc_score))\nplt.legend(loc=4)\nplt.show()","9a34749c":"skf =KFold(n_splits=10,shuffle=True,random_state=42) \npred_sub =0\n\nfor train_index, test_index in skf.split(X,y): \n    x_train_fold, x_test_fold = X[train_index], X[test_index] \n    y_train_fold, y_test_fold = y[train_index], y[test_index]\n    lgtrain = lgbm.Dataset(x_train_fold, label=y_train_fold)\n    lgval = lgbm.Dataset(x_test_fold, label=y_test_fold)\n    params = {\n        \"objective\" : \"binary\",\n        \"metric\" : \"auc\",\n        'boosting_type' : 'goss',\n        'max_depth' : 4,#-1\n        \"num_leaves\" : 100,#20\n        \"learning_rate\" : 0.01,#0.01\n        \"feature_fraction\" : 0.6,#0.7 #0.5\n        \"bagging_freq\" : 10, #10 #20\n        \"bagging_seed\" : 42, #2018\n        \"verbosity\" : -1,\n        'lambda_l2' : 1.1,#0.1\n        'lambda_l1' : 0.000001,#0.00001,#0,\n        'max_bin' : 90 #default=250 #200 #170 #120 #90\n\n    }\n    model=lgbm.train(params,lgtrain,num_boost_round=1000,valid_sets=lgval,early_stopping_rounds=1000)\n    pred=model.predict(test)\n\n   \n\n    pred_sub+=pred\/10\n","85be74d8":"subfile=pd.DataFrame({\"musteri\":test_merged.musteri,\"target\":pred_sub})\nsubfile.to_csv(\"LGBMsub.csv\",index=False)","0750a0cc":"subfile","af91753f":"Merging the full featured monthly expenditures dataset which created at last step with train and test data.","891446df":"Splitting data to train and test to use test data on predicting at KFold cross validation step.Stratify parameter is used to make sklearn to split data with same distrubition according to \"target\" variable.","1d89f6c8":"# **Preprocessing**","e8408a4f":"This is my first notebook, forgive my mistakes :)","79fa5bf3":"1024 rows of train data is dropped after the removing outliers.","131f92fb":"# **Libraries Import**\nImporting necessary libraries","00a5403b":"Merging two pivot table to collect all expenditure features in one dataset. \n","c3ca9539":"Merging the whole train and test dataset to make preprocessing steps in same dataset.","d8a516dc":"### **Data Exploration**\n\nReading the train,test and monthly_expenditures datasets ","221ee7b3":"Getting dummy variables on categorical columns.","f86a8d58":"Removing outliers in \"toplam_aylik_tutar\" according to each \"sektor\". Std Dev is too much.","af70c142":"Extracting new features from dataset with feature engineering.","d512baba":"Plotting the ROC_AUC Curve.","b65e6d00":"Creating a pivot table to get sum of \"aylik_toplam_tutar\" and \"islem_adedi\"  according to \"tarih\" column. Neglecting \"sektor\" in this step.","d4a72e54":"Viewing the summary of monthly expenditures","ece12765":"Dropping \"musteri\" column which has no effect on predicting.","ce68ee6d":"Viewing the summary of train data","af6a5bc9":"### **Merging Datasets**\nFirst we are going to merge train dataset with monthly_exp inner join on musteri colum.","867bc07e":"\"monthly_exp\" dataset contains the expenditures of customers which in train and test dataset.","ff086bef":"Writing submissions to file.","7e434079":"Creating a pivot table to get sum of \"aylik_toplam_tutar\" and \"islem_adedi\" according to \"sektor\" column .Also neglecting the \"tarih\" column at this step.","f117f1dd":"# **\u0130\u015f Bankas\u0131 ML Challange KFold CV+LGBM**","6d03c750":"Checking the class distrubition of \"target\" column in train dataset.","14682511":"# **Creating Model**","761a4049":"Using KFold CV and predicting X_test at each fold.","2c4b17b7":"Predicting with same parameters on real test dataset with same parameters.","0b8c62c8":"Nearly 35% of column \"meslek_grubu\" is NaN so we are dropping that column also \"tarih\" is constant and we dont need while creating our model. And filling the NaN's of other columns with median.","7912c548":"Checking out train and monthly expenditures data","4cf3fe50":"We got meaningless data in \"kidem_suresi\" column such as negative values.Transforming that column to year type.","fc0ba89f":"Now we got all features in one dataset."}}