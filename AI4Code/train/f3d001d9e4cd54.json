{"cell_type":{"cc769078":"code","eedb06d4":"code","d4138212":"code","8373b9d4":"code","b9761ccc":"code","f7aa626d":"code","488df421":"code","02d44781":"code","c477af1b":"code","5fe10b1b":"code","637bbf8c":"code","5782a5f8":"code","bc6f8fd7":"code","63d385ad":"code","7a589398":"code","c5ccb47e":"markdown","41dd48ed":"markdown","4b6f98e1":"markdown","a2330271":"markdown","93ee86bf":"markdown","4982d04d":"markdown","d722b7aa":"markdown","3d1682ee":"markdown","f08e6508":"markdown","51a70f57":"markdown","4a1fe970":"markdown","aabb4308":"markdown","5a48baf2":"markdown","d07a24d1":"markdown","6e133982":"markdown","9750e144":"markdown","520556fa":"markdown","8e2f73e2":"markdown","7ba4903d":"markdown","41eb239c":"markdown","ebbd4570":"markdown","bd27d63f":"markdown"},"source":{"cc769078":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eedb06d4":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport category_encoders as ce","d4138212":"dataset = pd.read_csv('\/kaggle\/input\/vehicle-dataset-from-cardekho\/car data.csv')\ndataset.head(10)","8373b9d4":"y_data=dataset.iloc[:,[2]]\ny=y_data.values\n\n#removing cng cars\ny=np.delete(y,[18,36],0)\n\n#visual\ny_data.head(10)","b9761ccc":"x_data = dataset.iloc[:,1:]\n\n#encoding categorical variables\n\nohc_5 = ce.OneHotEncoder(cols=['Transmission','Seller_Type','Fuel_Type'])\nx_data=ohc_5.fit_transform(x_data);\nx_orig=x_data.values\n\n#removing records with 'Fuel_Type' == 'CNG'\nx_orig=np.delete(x_orig,[18,35],0)\n\n#avoiding dummy variable trap\nx_orig=np.delete(x_orig,[1,5,6,8,10],1)\n\n#visual\nx_data.head(10)","f7aa626d":"from sklearn.preprocessing import StandardScaler\nsc_x=StandardScaler()\nx=sc_x.fit_transform(x_orig)","488df421":"[x_row,x_col]=x.shape\nx=np.append(arr=np.ones((x_row,1)).astype(float),values=x,axis=1)","02d44781":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)","c477af1b":"def cost(x,y,theta):\n    [m,n]=x.shape\n    h=np.dot(x,theta);\n    J=(1\/(2*m))*np.sum((h-y)**2)\n    return J","5fe10b1b":"def gradient_descent(x,y,theta,iterations,alpha):\n    [m,n]=x.shape\n    theta_opt=theta\n    J_history=np.zeros((iterations,1))\n    grad_history=np.zeros((iterations,n))\n    grad_history=grad_history.reshape(iterations,n)\n    \n    for i in range(iterations):\n        h=np.dot(x,theta_opt);\n        grad = (alpha\/m)*(np.sum(((h-y)*x),axis=0))\n        grad=grad.reshape(-1,1)\n        theta_opt= theta_opt - grad\n        J_history[i]=cost(x,y,theta_opt)\n        grad_history[i]=theta_opt.transpose()\n        \n    return theta_opt,grad_history,J_history   ","637bbf8c":"theta=np.random.randn(x_col+1,1)\niterations=1000\n\ntheta_opt,grad_history,J_history = gradient_descent(x_train,y_train,theta,iterations,0.01)","5782a5f8":"iter_plt=np.array([range(iterations)]).transpose()\nplt.plot(iter_plt,J_history)\nplt.xlabel('iterations')\nplt.ylabel('cost')","bc6f8fd7":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()","63d385ad":"lr.fit(x_train,y_train)\ny_pred_sk=lr.predict(x_test)\ny_pred_sk=y_pred_sk.reshape(-1,1)","7a589398":"cost_SK=(1\/(2*len(y)))*np.sum((y_pred_sk-y_test)**2)\ncost_GD = cost(x_test,y_test,theta_opt)\nprint(\"the mean squared error of the sckikit learn model is \",cost_SK)\nprint(\"and the mean squared error of the cost of the model that was made manually is \",cost_GD)","c5ccb47e":"<p><h2>the scikit learn libraries are imported <br\/>an object of the linear regression class is made which acts as our multivariate linear regressor<\/h2><\/p>","41dd48ed":"# defining the gradient descent method (Batch gradient descent is used)\n<h2>parameters : <\/h2> <ul><li> 'x' is the feature matrix <\/li>  <li>'y' is the dependent variable <\/li> <li> 'theta' is the parameter vector of the hypothesis<\/li><li>'iterations' is the number of iterations the gradient descent will run<\/li>  <li> 'alpha' is the learning rate     <\/li><\/ul>\n<h2>return values : <\/h2><ul><li>'theta_opt' is the optimized theta<\/li>  <li>'grad_history' is the the array of gradients of each iteration<\/li>  <li>'J_history is the array of costs at each iteration<\/li><\/ul>","4b6f98e1":"# splitting the training and test sets\nhere cross validation set is not used , since training set is small and regularization is not done","a2330271":"# isolating the dependent variable ( selling price )\n\nthe 'Fuel_type' column is a categorical variable , where the category 'CNG' appears only twice\nfor simplicity , the records with 'Fuel_Type' == 'CNG are not considered  ( they are the 19th and 37th records)","93ee86bf":"<p><h2>behaviour of the plot is as expected , hence this confirms that gradient descent was implemented correctly <\/h2><\/p>\n<hr>","4982d04d":"# importing the dataset","d722b7aa":"<h2>with this the data preprocessing is complete<\/h2>\n<hr>\n","3d1682ee":"<p><h2>the regressor is fit to the training set (this means the  model will learn from the training set )<br\/>the predictions of the trained model on the test set are made<\/h2><\/p>","f08e6508":"# feature scaling \nhere **standardization** is used to scale the feature matrix","51a70f57":"# adding the intercept term (aka bias term )\nthe intercept term will act as the constant in the hypothesis equation","4a1fe970":"# here we are going to use 2 different models\n<h2> <ul> <li> 1. defining the cost function and performing gradient descent manually <\/li>\n    <li> 2. using a linear regression model from the scikit-learn library <\/li><\/ul> <\/h2>\n","aabb4308":"<hr>\n# now we compare the performances of each of the models using their mean squared error as the evaluation metric","5a48baf2":"<p><h2>if we had more data , feature creation can be done to increase the variance of the model <br\/>and this new  model can be trained on a larger dataset to balance the higher variance and this could lead to a more robust model.<br\/>if variance is too high , regularization can be applied.<br\/>that will be all :).<\/h2><\/p>   ","d07a24d1":"\n# defining the cost function  (Mean Squared Error)  \n<h3> parameters :<\/h3><ul><li> 'x' is the feature matrix <\/li>  <li>'y' is the dependent variable <\/li> <li> 'theta' is the parameter vector of the hypothesis<\/li><\/ul>\n    <h3> return values :<\/h3><ul><li>'J' is the cost aka 'error' is returned<\/li><\/ul>","6e133982":"# isolating the feature matrix\nand the categorical variables are one hot encoded","9750e144":"# 2. Cost optimization using scikit learn library","520556fa":"# 1. Manually optimizing the cost function","8e2f73e2":"<h1>Evaluating the gradient descent algorithm <\/h1>\n<p><h3> a graph is ploted with the x axis being the no of iterations and y axis being the cost error after that many iterations <\/h3><\/p> \n<p><h3> the cost is expected to decrease very rapidly in the beginning <br\/> this rate of decrease in the cost is expected to slow down and later flatten out<\/h3><\/p>\n","7ba4903d":" in the above table , columns : 6,7,9,11 are removed to avoid the dummy variable trap and  column : 2 removed as it is the dependent variable\n                                 ","41eb239c":"# importing needed libraries\n","ebbd4570":"<h1>performing gradient descent<\/h1>\n<ul><li>initially theta parameters are set randomly<\/li> <li> we use 1000 iterations <\/li><li> an alpha rate of 0.01<\/li><\/ul>","bd27d63f":"<hr>\n# data preprocessing"}}