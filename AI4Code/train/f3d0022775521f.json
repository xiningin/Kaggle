{"cell_type":{"48cd87d1":"code","282214d3":"code","b95d0e63":"code","f5705c68":"code","89f6addb":"code","44db1dfc":"code","8c12502a":"code","ba4ea733":"code","69cb16ba":"code","b14a7c1f":"code","dbcb59ce":"code","4dc52ee9":"code","ec3011d3":"code","75e31b88":"code","0e4ece73":"code","c425a1c8":"code","8229ba22":"code","6c255633":"code","050c78e7":"code","fbc263f8":"code","064941e8":"code","2544e1c4":"code","beb285cd":"code","89e40da1":"code","dc5aec25":"code","36926457":"code","97f78626":"code","cc82b185":"code","4ba0e67f":"code","46bf9850":"code","8b706856":"markdown","df0e89f4":"markdown","be4d5d8c":"markdown","45365be5":"markdown","484896ae":"markdown","ecd5aded":"markdown","d8a4b110":"markdown","27e4885b":"markdown","9a24bbd4":"markdown","e94e0107":"markdown","ae904b3d":"markdown","f2e79030":"markdown","2f36bca6":"markdown"},"source":{"48cd87d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# numpy and pandas for data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# File system manangement\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","282214d3":"# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b95d0e63":"# List files available\nprint(os.listdir(\"..\/input\/home-credit-default-risk\"))","f5705c68":"dir_name = \"..\/input\/home-credit-default-risk\"","89f6addb":"app_train = pd.read_csv(os.path.join(dir_name,'application_train.csv'))\nprint(\"Training data shape: \", app_train.shape)\napp_train.head()","44db1dfc":"print(\"Application train column names: \")\nprint(\"------------------------------\")\nfor _ in app_train.columns.values:\n    print(_, end=' , ')","8c12502a":"app_test = pd.read_csv(os.path.join(dir_name,'application_test.csv'))\nprint(\"Test data shape: \", app_test.shape)\napp_test.head()","ba4ea733":"print(\"Application test column names: \")\nprint(\"------------------------------\")\nfor _ in app_test.columns.values:\n    print(_, end=' , ')","69cb16ba":"app_train['TARGET'].value_counts()","b14a7c1f":"app_train['TARGET'].astype(int).plot.hist()","dbcb59ce":"app_train.isnull().sum()","4dc52ee9":"def missing_values_table(df):\n    # Total missing values\n    missing_values = df.isnull().sum()\n#     print(missing_values)\n    # Percentage of missing values\n    missing_values_percentage = 100 * missing_values \/ len(df)\n#     print(missing_values_percentage)\n    missing_values_table = pd.concat([missing_values,missing_values_percentage],axis=1)\n    print(\"Missing values and percentage shape: \",missing_values_table.shape)\n#     print(missing_values_table)\n    # Renaming the column names\n    missing_values_rename_columns = missing_values_table.rename(columns = {0: 'Missing Values', 1: 'Missing % of total values'})\n#     print(missing_values_rename_columns)\n    # Sorting the table by percentage of missing descendents\n    missing_values_rename_columns = missing_values_rename_columns[missing_values_rename_columns.iloc[:,1] != 0].sort_values('Missing % of total values', ascending = False).round(1)\n#     print(missing_values_rename_columns)\n    print(f'This dataframe has {df.shape[1]} columns. There are {missing_values_rename_columns.shape[0]} columns that have missing values.')\n    \n    return missing_values_rename_columns\n    ","ec3011d3":"app_train_missing_values = missing_values_table(app_train)\napp_train_missing_values.head(10)","75e31b88":"app_train.dtypes.value_counts()","0e4ece73":"app_train.select_dtypes('object').apply(pd.Series.nunique,axis = 0) # here axis = 1 means row in the dataframe and 0 means column","c425a1c8":"#Create a label encoder object\ndef label_encoder_train_test(train, test):\n\n    le = LabelEncoder()\n    le_count = 0\n\n    # Iterate through the columns\n    for col in train:\n        if train[col].dtype == 'object':\n            if len(list(train[col].unique())) <= 2:\n                le.fit(train[col])\n                train[col] = le.transform(train[col])\n                test[col] = le.transform(test[col])\n                le_count += 1\n\n    print(f'{le_count} columns were encoded.')","8229ba22":"label_encoder_train_test(app_train,app_test)","6c255633":"# One hot encoding for more than two categorical values\ndef one_hot_encoding_train_test(train, test):\n    train = pd.get_dummies(train)\n    test = pd.get_dummies(test)\n    \n    print(f\"Training feature shape: {train.shape}\")\n    print(f\"Testing feature shape: {test.shape}\")","050c78e7":"one_hot_encoding_train_test(app_train,app_test)","fbc263f8":"train_labels = app_train['TARGET']\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join='inner', axis=1) # axis = 1 for column based alignment\napp_train['TARGET'] = train_labels\n\nprint(f'Training feature shape: {app_train.shape}')\nprint(f'Testing feature shape: {app_test.shape}')\n","064941e8":"# The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application\nage = (app_train['DAYS_BIRTH']\/ -365).describe()\nage","2544e1c4":"app_train['DAYS_EMPLOYED'].describe()","beb285cd":"app_train.DAYS_EMPLOYED.plot.hist(title='Days employment histogram');\nplt.xlabel('Days employment');","89e40da1":"anomalous = app_train[app_train.DAYS_EMPLOYED == 365243]\nnon_anomalous = app_train[app_train.DAYS_EMPLOYED != 365243]\n\nprint(f'The non-anomalous default on {non_anomalous.TARGET.mean() * 100}% of loans')\nprint(f'The anomalous default on {anomalous.TARGET.mean() * 100}% of loans')\nprint(f'There are {len(anomalous)} anomalous days of employment')","dc5aec25":"app_train.DAYS_EMPLOYED_ANOM = app_train.DAYS_EMPLOYED == 365243\napp_train.DAYS_EMPLOYED.replace({365243: np.nan}, inplace = True)\napp_train.DAYS_EMPLOYED.plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","36926457":"app_test.DAYS_EMPLOYED_ANOM = app_test.DAYS_EMPLOYED == 365243\napp_test.DAYS_EMPLOYED.replace({365243: np.nan}, inplace = True)\nprint(f'There are {app_test.DAYS_EMPLOYED_ANOM.sum()} anomalies in the test data out of {len(app_test)} entries.')","97f78626":"correlations = app_train.corr()['TARGET'].sort_values()\n\nprint('Most positive correlations:\\n', correlations.tail(15))\nprint('\\nMost negative correlations:\\n', correlations.head(15))","cc82b185":"app_train.DAYS_BIRTH = abs(app_train.DAYS_BIRTH)\napp_train.DAYS_BIRTH.corr(app_train.TARGET)","4ba0e67f":"plt.style.use('fivethirtyeight')\nplt.hist(app_train['DAYS_BIRTH']\/365, edgecolor='k', bins=25);\nplt.title('Age of client');plt.xlabel('Age (years)');plt.ylabel('Count');","46bf9850":"## TODO: What KDE actually depicts here? Find out.\n\nplt.figure(figsize = (10,8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train.TARGET == 0, 'DAYS_BIRTH']\/365, label='target == 0')\n#KDE plot of loans that were not repaid om time\nsns.kdeplot()","8b706856":"**kernel density estimation plot (KDE)**","df0e89f4":"## **Exploratory Data Analysis**\n> Exploratory Data Analysis (EDA) is an open-ended process where we calculate\n* statistics\n* make figures to find trends\n* anomalies\n* patterns\n* relationships\nwithin the data. ","be4d5d8c":"## **Checking column types**","45365be5":"**Effect of age on repayment**","484896ae":"## **Missing values**","ecd5aded":"## Aligning Train and Test data","d8a4b110":"## **Distributions of repaid and non-repaid loan record**","27e4885b":"## **Encoding Categorical Variables**\n\n> * Label encoding\n> * One hot encoding","9a24bbd4":"## Anomalous value eradication  ","e94e0107":"## **The training data has 307511 observations (each one a separate loan) **","ae904b3d":"**Emining the distributions of the target columns**\n> Here we will determine the distributions of loan repayment or not related data","f2e79030":"## Correlations\n>  Pearson correlation coefficient\n\n* .00-.19 \u201cvery weak\u201d\n* .20-.39 \u201cweak\u201d\n* .40-.59 \u201cmoderate\u201d\n* .60-.79 \u201cstrong\u201d\n* .80-1.0 \u201cvery strong\u201d","2f36bca6":"## Back to Exploratory Data Analysis\n> **Anomalies**\n* One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method\n* These may be due to **mis-typed numbers**, **errors in measuring equipment**, or **they could be valid but extreme measurements**"}}