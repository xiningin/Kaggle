{"cell_type":{"01bc3f41":"code","215c4578":"code","dfeca018":"code","a90a0c3f":"code","1a8cd77e":"code","1085b113":"code","919f6c28":"code","016d95dd":"code","f8fd91ca":"code","aee248d3":"code","b7a633ef":"code","7c7ce99f":"code","288f69d4":"code","1a1e5363":"code","4ff9326a":"code","08314300":"code","ebaa04ba":"code","ecd8c340":"markdown","d3b1456b":"markdown","de22c1ff":"markdown","e34915b6":"markdown","ec223ada":"markdown","f38e9b4e":"markdown","24444702":"markdown","044b012b":"markdown","66754bb0":"markdown","7b7e54eb":"markdown"},"source":{"01bc3f41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","215c4578":"!pip install openpyxl","dfeca018":"# To start with importing libraries which are essential for the case\nimport pandas as pd\nfrom mlxtend.frequent_patterns import apriori, association_rules","a90a0c3f":"# Reading the excel format data set by using \"Year 2010-2011\" sheet named.\ndf_ = pd.read_excel(\"..\/input\/online-retail-ii-data-set-from-ml-repository\/online_retail_II.xlsx\", sheet_name=\"Year 2010-2011\")\ndf = df_.copy()","1a8cd77e":"# Information of dataset\ndf.info()","1085b113":"# Examining descriptive statistics\ndf.describe().T","919f6c28":"# outlier_tresholds function: This function captures the low and up limit of the dataset by using the quantile() method. \n# With this function, I aimed to determine the 1st and 3rd quartiles, subtract 1.5 times the difference between them \n# from the 1st quartile and find the low level, add the 3rd quartile and find the up level.\n# So why did I use this function?\n# I used it to define the boundaries of the dataset, to bring the dataset closer to normal.\ndef outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.01)\n    quartile3 = dataframe[variable].quantile(0.99)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n# replace_with_tresholds function: It allows us to replace the outlier values with limit values by using the low and \n# up limits that we extracted from the outlier_thresholds function.\n# So should we really need to remove outliers?\n# Outliers are values that are very rare in the data set and prevent us from generalizing the variable it is in. \n# Because they cause the mean to be biased. We, too, need to get rid of outliers to normalize the distributions of \n# the datasets.\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\n# I used this function to remove missing variables and aborted operations from dataset.\ndef retail_data_prep(dataframe):\n    dataframe.drop(dataframe[dataframe[\"StockCode\"] == \"POST\"].index, inplace=True)\n    dataframe.dropna(inplace=True)\n    dataframe = dataframe[~dataframe[\"Invoice\"].str.contains(\"C\", na=False)]\n    dataframe = dataframe[dataframe[\"Quantity\"] > 0]\n    dataframe = dataframe[dataframe[\"Price\"] > 0]\n    replace_with_thresholds(dataframe, \"Quantity\")\n    replace_with_thresholds(dataframe, \"Price\")\n    return dataframe","016d95dd":"# Calling the functions above\ndf = retail_data_prep(df)\ndf.head()","f8fd91ca":"# As I said at the beginning, let's choose Germany customers now.\ndf_gr = df[df['Country'] == \"Germany\"]","aee248d3":"# Let's group the data set according to Invoice and StockCode, add the Quantity values and fill the empty values with 0. \n# Let's write a function with the help of applymap and lambda, let the function we write go through all rows and columns \n# and assign 1 for each variable it sees as full, leaving the rest as 0.\ngr_invoice_product_df = df_gr.groupby(['Invoice', 'StockCode'])['Quantity'].sum().unstack().fillna(0). \\\n            applymap(lambda x: 1 if x > 0 else 0)\n\n# Check the df\ngr_invoice_product_df.head()","b7a633ef":"# Implementation of the Apriori algorithm\nfrequent_itemsets = apriori(gr_invoice_product_df, min_support=0.01, use_colnames=True)\n\n# Sort values with support\nfrequent_itemsets.sort_values(\"support\", ascending=False).head()","7c7ce99f":"# Association rules generation\nrules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.01)\n\n# Sort values with support\nrules.sort_values(\"support\", ascending=False).head()","288f69d4":"# check_id : This function helps us to find the name of products which id's are given.\ndef check_id(dataframe, stock_code):\n    product_name = dataframe[dataframe[\"StockCode\"] == stock_code][[\"Description\"]].values[0].tolist()\n    print(product_name)","1a1e5363":"# Find the products with given id\ncheck_id(df_gr, 21987)\ncheck_id(df_gr, 23235)\ncheck_id(df_gr, 22747)","4ff9326a":"# arl_recommender: With this function, 1 product recommendation is made for the products for which idsi and \n# recommend rules are entered. Rules are sorted according to lift values, an empty recommend list is created, \n# and a for loop is created over the descendant variables of the sorted rules. A cycle is created again for \n# the products assigned as product, this is done in order to avoid recommending the same product. The product \n# at the top of the sorted list for products with product_id equal to product is recommended.\ndef arl_recommender(rules_df, product_id, rec_count=1):\n    sorted_rules = rules_df.sort_values(\"lift\", ascending=False)\n    recommendation_list = []\n    for i, product in sorted_rules[\"antecedents\"].items():\n        for j in list(product):\n            if j == product_id:\n                recommendation_list.append(list(sorted_rules.iloc[i][\"consequents\"]))\n    recommendation_list = list({item for item_list in recommendation_list for item in item_list})\n    return recommendation_list[:rec_count]","08314300":"# Recommendation for users in cart with specified products\nprint(arl_recommender(rules, 21987, 1))\nprint(arl_recommender(rules, 23235, 1))\nprint(arl_recommender(rules, 22747, 1))","ebaa04ba":"# We can reach the names of the recommended products as follows.\ncheck_id(df_gr, 21244) \ncheck_id(df_gr, 23307) \ncheck_id(df_gr, 22551) ","ecd8c340":"In this study, I used the Online-Retail-II data set that I used before. Based on the three user information in the basket stage below, I aimed to suggest products to users. While doing the study, I extracted the decision rules from Germany customers.\n\n*  User 1 product id: 21987\n*  User 2 product id: 23235\n*  User 3 product id: 22747","d3b1456b":"> When we examine the output, we see that there are missing expressions in some variables.","de22c1ff":"# Case Study with Apriori","e34915b6":"![image.png](attachment:3e7db1d2-3a06-4653-8809-ca8c7cfe0570.png)","ec223ada":"> To implement the Apriori algorithm, we need to create a boolean table from the data we have. The Boolean-formatted table is the table where all the products sold are written as features and the presence\/non-existence of each feature is shown. We can access this format with the help of the following line of code.","f38e9b4e":"# Association Rule Learning","24444702":"The recommendation system is an application that supports the user in making the appropriate decision when purchasing a product they want and recommends products that may be suitable. Sequential execution of suggestions often predicts objects such as market basket suggestions.\n\nThis process finds relationships between products in customers' purchases and analyzes customers' purchasing habits. In line with this information, sales rates can be increased by determining shelf placements and effective sales strategies can be developed.\n\nTo apply the Apriori Algorithm and extract association rules, we need to explain the following terms.\n\n**Support(A,B)= P(A\u2229B)=** Number of transaction containing (A and B)\/ Total Transactions\n\n**Confidence = P(B|A) =** Number of transaction containing (A and B)x100\/ Total of transaction containing (A)\n\n**Lift=** Support(A,B)\/ Support(A)xSupport(B)","044b012b":"**Now that the data preprocessing phase is over, we can start to extract association rules.**","66754bb0":"> When the statistics are examined, it is seen that there are unexpected negative values in the quantity and price variables. These values represent returns in the data set. When the data set is examined, there is a \"C\" expression for returns in the \"Invoice\" variable.\n\n**I will continue with short functions for the data preprocessing phase.**","7b7e54eb":"**As a result, the Apriori algorithm allows us to observe situations that we cannot predict when we first look at the data, by establishing a relationship between the variables. It allows us to analyze future product recommendations.**"}}