{"cell_type":{"94636987":"code","d65e72dd":"code","5fde6bd5":"code","7d1cc6a7":"code","9ebfd381":"code","0262f19b":"code","58ca329a":"code","4d296c90":"code","e3d276f8":"code","f44c591e":"code","c3a5b4ea":"code","ab7ea1ee":"code","06ec9ce4":"code","07004334":"code","f0624ab8":"code","ab077f06":"code","eedb2c4f":"code","35e9769c":"code","cf23bf95":"code","217f1588":"code","a7a36685":"code","d9b7f935":"code","951e20fa":"code","2716242c":"code","fc8e3b66":"code","a94dc50e":"code","fccf79c3":"markdown","29f41aba":"markdown","a62fe657":"markdown","135b1fe5":"markdown","5bba9533":"markdown","6585152d":"markdown","6b80dba1":"markdown","7ba2546a":"markdown","31bb0681":"markdown","29127f02":"markdown","fd9edd80":"markdown","fc994df0":"markdown","10e1b2d6":"markdown","6df057d8":"markdown","21d02f23":"markdown","5d3d51b6":"markdown","48472821":"markdown","f9bf4b46":"markdown","9959420e":"markdown","6cc12473":"markdown","7cc4c13a":"markdown","d893a91c":"markdown","c581a2f5":"markdown","2fb5da74":"markdown","c7dd94a5":"markdown","baaf642f":"markdown","f545dea5":"markdown","19c1e7a3":"markdown","d5efec57":"markdown","83f4417a":"markdown","91ec7dab":"markdown","f91dc43f":"markdown"},"source":{"94636987":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport warnings\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d65e72dd":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv', index_col='row_id')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv', index_col='row_id')\nsample_sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/sample_submission.csv', index_col='row_id')","5fde6bd5":"color_c = px.colors.sequential.Teal\ncolor_d = px.colors.qualitative.Set2","7d1cc6a7":"for df in train, test:\n    df['date'] = pd.to_datetime(df['date'])","9ebfd381":"train.info()","0262f19b":"train.describe()","58ca329a":"fig = px.histogram(train, x='country', color='country', color_discrete_sequence=color_d)\nfig.update_layout(height=400, width=700, template='plotly_white', title='Count of country')","4d296c90":"fig = px.histogram(train, x='store', color='store', color_discrete_sequence=color_d)\nfig.update_layout(height=400, width=700, template='plotly_white', title='Count of store')","e3d276f8":"fig = px.histogram(train, x='product', color='product', color_discrete_sequence=color_d)\nfig.update_layout(height=400, width=700, template='plotly_white', title='Count of product')","f44c591e":"fig = px.histogram(data_frame=train, x='country', y='num_sold', color='product', barmode='group', color_discrete_sequence=color_d)\nfig.update_layout(height=400, width=700, template='plotly_white', title='Number of products sold by country')","c3a5b4ea":"temp = train.groupby(['country', 'product'])['num_sold'].sum()\ntemp = temp.groupby(level=0).apply(lambda x: np.round(x\/x.sum()*100)).reset_index()\ntemp.rename(columns={'num_sold': '% of total sales'})","ab7ea1ee":"temp = train.groupby(['date', 'country'])['num_sold'].sum().reset_index()\nfig = fig = px.line(data_frame=temp, x='date', y='num_sold', color='country', color_discrete_sequence=color_d)\nfig.update_layout(height=500, width=1000, template='plotly_white', title='Daily sales by country')","06ec9ce4":"pivot = train.pivot_table(index='date', values='num_sold', columns='country', aggfunc='sum')\n\nfig = px.imshow(pivot.corr(), color_continuous_scale=color_c)\nfig.update_layout(height=400, width=700, template='plotly_white')","07004334":"fig = px.histogram(data_frame=train, x='store', y='num_sold', color='product', barmode='group', color_discrete_sequence=color_d)\nfig.update_layout(height=400, width=700, template='plotly_white', title='Products sold by store')","f0624ab8":"temp = train.groupby(['store', 'product'])['num_sold'].sum()\ntemp = temp.groupby(level=0).apply(lambda x: np.round(x\/x.sum()*100)).reset_index()\ntemp.rename(columns={'num_sold': '% of total sales'})","ab077f06":"temp = train.groupby(['date', 'store'])['num_sold'].sum().reset_index()\n\nfig = px.line(data_frame=temp, x='date', y='num_sold', color='store', color_discrete_sequence=color_d)\nfig.update_layout(height=500, width=1000, template='plotly_white', title='Daily sales by store')","eedb2c4f":"pivot = train.pivot_table(index='date', values='num_sold', columns='store', aggfunc='sum')\n\nfig = px.imshow(pivot.corr(), color_continuous_scale=color_c)\nfig.update_layout(height=400, width=700, template='plotly_white')","35e9769c":"temp = train.groupby(['date', 'product'])['num_sold'].sum().reset_index()\n\nfig = px.line(data_frame=temp, x='date', y='num_sold', color='product', color_discrete_sequence=color_d)\nfig.update_layout(height=500, width=1000, template='plotly_white', title='Daily sales by product')","cf23bf95":"pivot = train.pivot_table(index='date', values='num_sold', columns='product', aggfunc='sum')\n\nfig = px.imshow(pivot.corr(), color_continuous_scale=color_c)\nfig.update_layout(height=400, width=700, template='plotly_white')","217f1588":"temp = train.groupby(['date', 'product'])['num_sold'].sum()\ntemp = temp.groupby(level=0).apply(lambda x: np.round(x\/x.sum()*100)).reset_index()\n\nfig = px.line(data_frame=temp, x='date', y='num_sold', color='product', color_discrete_sequence=color_d)\nfig.update_layout(height=500, width=1000, template='plotly_white', title='Daily relative proportion of products sold')","a7a36685":"hat = temp[temp['product'] == 'Kaggle Hat']\nhat.name = 'Hat'\n\nmug = temp[temp['product'] == 'Kaggle Mug']\nmug.name = 'Mug'\n\nsticker = temp[temp['product'] == 'Kaggle Sticker']\nsticker.name = 'Sticker'","d9b7f935":"for df in hat, mug, sticker:\n    \n    adf = adfuller(df['num_sold'], autolag='AIC')\n    \n    if adf[1] < 0.05:\n        print(f'p-value is below 0.05 at {np.round(adf[1], 4)} so we reject the null hypothesis: {df.name} data is stationary')\n    else:\n        print(f'p-value is above 0.05 at {np.round(adf[1], 4)} so we fail to reject the null hypothesis: {df.name} data is not stationary')","951e20fa":"temp = train.groupby('date')['num_sold'].sum().reset_index()","2716242c":"sd = seasonal_decompose(temp['num_sold'], model='additive', period=365)\nsd.plot();","fc8e3b66":"temp['30d MA'] = temp['num_sold'].rolling(window=30).mean()\ntemp['90d MA'] = temp['num_sold'].rolling(window=90).mean()","a94dc50e":"fig = px.line(temp, x='date', y='90d MA', title='Daily sales', color_discrete_sequence=color_d)\nfig.update_layout(height=400, width=1000, template='plotly_white', title='90 days moving average')","fccf79c3":"## Correlations","29f41aba":"# 5. Sales by product","a62fe657":"## Correlation","135b1fe5":"According to the 90 day moving average and to the seasonal decomposition, sales are slowly increasing over the years.","5bba9533":"## Time series","6585152d":"Correlation between sales in both stores seems pretty high. Let's verify that.","6b80dba1":"# 4. Sales by store","7ba2546a":"## Moving average","31bb0681":"## Time series","29127f02":"## Correlations","fd9edd80":"The seasonal component is quite obvious. Over the years, sales tend to increase.","fc994df0":"# 6. How does the sales proportions evolve?","10e1b2d6":"As observed on the graph and confirmed by the numbers, the proportions of total sales in all countries are the same.","6df057d8":"## Store","21d02f23":"# 2. Distributions","5d3d51b6":"As observed previously, mug and hat sales are the least correlated; though their correlation coefficient is quite high.","48472821":"Sales proportions seem to be stationary, so let's run an ADF test to be sure.","f9bf4b46":"## Product","9959420e":"## Time series","6cc12473":"Same observation as above : proportions of total sales by store are identical.","7cc4c13a":"# 1. Imports and setup","d893a91c":"## Stationarity check","c581a2f5":"## Total sales","2fb5da74":"## Country","c7dd94a5":"## Total sales","baaf642f":"Hello everyone! Here's a quick EDA on TPS January 22 dataset. Feel free to comment down below if you have any suggestions on how to improve this notebook. I'm eager to learn.  \nThank you and happy new year! \ud83c\udf8a","f545dea5":"# 7. Seasonal decomposition","19c1e7a3":" Again, let's run a correlation check.","d5efec57":"We couldn't ask for a better balanced dataset.","83f4417a":"# 3. Sales by country","91ec7dab":"Let's check correlations again.","f91dc43f":" All peaks happen at the beginning of the year. It's interesting to notice that as hat sales go down, mug sales increase. Indeed, why would you need a hat during winter, when you could just stay warm next to the fireplace, drinking a hot chocolate in your precious Kaggle mug? \u2615\ufe0f"}}