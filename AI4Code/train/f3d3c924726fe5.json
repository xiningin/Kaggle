{"cell_type":{"730a4b26":"code","77da0e3d":"code","bad3073b":"code","29accfb5":"code","24c52745":"code","883a7a85":"code","ffceaf52":"code","218fd7d5":"code","442a511f":"code","b6347897":"code","112ccb32":"code","77d5f7ee":"code","c3ede340":"code","09b5c87f":"code","3029b06e":"code","90fd9e16":"code","434d55db":"code","ed8829c8":"code","7977fe55":"code","aa478789":"code","ba65a225":"code","5d068192":"code","538e5bdf":"code","976488e7":"code","d6e4ce4b":"code","64c3220f":"code","e2907347":"code","7fc61116":"code","b03aa7d9":"code","f7259877":"code","8a080e02":"code","3d84949a":"code","47bfcd21":"code","386afd43":"code","fef89279":"code","20dadfaf":"code","3cb32a8e":"code","f38f70ad":"code","735962f6":"code","fdac8852":"code","08eedf81":"code","73454d00":"code","377ed227":"code","48e5a45b":"code","a01ff0a7":"code","5b767026":"code","d518ddf9":"code","7b782769":"code","46e32c31":"code","c663c733":"code","262b4851":"code","ef27c525":"code","d5efb02a":"code","5ff49588":"code","8bc5ff99":"code","ac81ea89":"code","94f8acd4":"code","e5917296":"code","657a7fe9":"code","35dc8289":"code","cbcf2f43":"code","b501f3b5":"markdown","0664d4dd":"markdown","0d4b9dd9":"markdown","82235139":"markdown","fe59a210":"markdown","700a911b":"markdown","1e35291b":"markdown","7c834b31":"markdown","29776809":"markdown","c5e09c18":"markdown","d88eef58":"markdown","9017e5c8":"markdown","1ee47c53":"markdown","e2bdee5d":"markdown"},"source":{"730a4b26":"# Creating an array using numpy library\nimport numpy as np\narr = np.array([1,2,3,4,5,6,7])","77da0e3d":"# data type of array\nprint(arr.dtype)\n\n# shape of array\nprint(arr.shape)\nprint(arr.size)","bad3073b":"# loading torch library \nimport torch \n\n# checking the version of torch library\ntorch.__version__","29accfb5":"# to set the device to cuda if available otherwise set it to cpu\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"","24c52745":"# converting the numpy array arr to tensor\ntensor = torch.from_numpy(arr)\ntensor","883a7a85":"# converting a tensor to array\narray_form = tensor.numpy()\nprint(array_form)\nprint(array_form.dtype)","ffceaf52":"# converting the numpy array arr to tensor with dtype float32 and device to cude\ntensor = torch.tensor(arr, dtype=torch.float32, device=device)\ntensor","218fd7d5":"# checking the shape or size of tensor\nprint(tensor.shape)\nprint(tensor.size())","442a511f":"# accessing tensor using indexing like arrays\nprint(tensor[4])\nprint(tensor[:4])\nprint(tensor[4:])","b6347897":"# changing the value of tensor[6] that is 7th element\ntensor[6] = 1000\nprint(tensor)","112ccb32":"# array arr has the same effect because they share the same memory location\nif(arr==tensor):\n    print(\"Yes! arr has been affected too!\")\nelse:\n    print(\"Nope! arr and tensor are different now!\")","77d5f7ee":"# make a copy of that array separately\ntensor = torch.tensor(arr)\nprint(tensor)\ntensor[0] = 101\n\n# let's check again if arr and tensor are still same?\nif(arr==tensor):\n    print(\"Yes! arr has been affected too!\")\nelse:\n    print(\"Nope! arr and tensor are different now!\")","c3ede340":"# creating a tensor using empty method (it will give uninitialized values)\ntensor = torch.empty(size=(4,4), device=device, dtype=torch.float32)\ntensor","09b5c87f":"# creating a tensor using zeros method\ntensor = torch.zeros(size=(4,3),device=device, dtype=torch.float32)\ntensor","3029b06e":"# creating a tensor using ones method\ntensor = torch.ones(size=(3,2),device=device, dtype=torch.float32)\ntensor","90fd9e16":"# creating a tensor using eye method\ntensor = torch.eye(n=5,device=device, dtype=torch.float32)\ntensor","434d55db":"# preserving the diagnol tensor of 5,5 ones tensor\ntensor = torch.diag(torch.ones(size=(5,5),device=device, dtype=torch.float32))\ntensor","ed8829c8":"# preserving the diagnol tensor of 5,5 random tensor\ntensor = torch.rand(size=(5,5),device=device, dtype=torch.float32)\nprint(tensor)\ntensor = torch.diag(tensor)\ntensor","7977fe55":"# creating a tensor using rand method\ntensor = torch.rand(size=(3,2),device=device, dtype=torch.float32)\ntensor","aa478789":"# creating a tensor of 6x2 of random values\ntensor = torch.rand(size=(6,2), device=device, dtype=torch.float32)\ntensor","ba65a225":"# creating a tensor of sequence 10 to 50 with skipping every 5 step\ntensor = torch.arange(start=10, end=60, step=5)\ntensor","5d068192":"# creating a tensor of sequence 10 to 50 with 7 equidistant values in between\ntensor = torch.linspace(start=10, end=60, steps=5)\ntensor","538e5bdf":"# creating a 3x4 tensor of sequence 10 to 120 with skipping every 10 step\ntensor = torch.tensor(np.arange(10, 121, 10).reshape(3,4), device=device, dtype=torch.float32)\ntensor","976488e7":"# creating a 3x4 tensor of unassigned values but normally distributed\ntensor = torch.empty(size=(3,4)).normal_(mean=0, std=1)\ntensor","d6e4ce4b":"# creating a 4x5 tensor of unassigned values but uniformly distributed\ntensor = torch.empty(size=(3,4)).uniform_(0, 2)\ntensor","64c3220f":"tensor = torch.arange(start=0, end=15, step=3)\ntensor.dtype","e2907347":"# converting the above tensor to int16\ntensor.short()","7fc61116":"# converting the above tensor back to int64\ntensor.long()","b03aa7d9":"# converting the above tensor to boolean\ntensor.bool()","f7259877":"# converting the above tensor to float16\ntensor.half()","8a080e02":"# converting the above tensor to float32\ntensor.float()","3d84949a":"# converting the above tensor to float64\ntensor.double()","47bfcd21":"# just get the 1st and 2nd columns of the tensor \n# tensor[:,0:2]","386afd43":"tensor = torch.tensor(np.arange(1,100,7).reshape(3,5), device=device, dtype=torch.float32)\ntensor","fef89279":"# add a scalar value to tensor made above using 2 different methods\n\n# method 1\nprint(tensor + 10)\n\n# method 2\nprint(torch.add(tensor,10))\n","20dadfaf":"# add a tensor to the tensor made above using 4 different methods \n\n# method 1\n# shape of the tesors should be same and both should be on same device, although dtype can differ\nprint(tensor.shape)\nprint(tensor.device)\nprint(tensor.dtype)\nc = tensor + torch.tensor(np.arange(10,81,5).reshape(3,5), dtype=torch.float64, device=device)\nprint(c)\n\n# method 2\nc = torch.add(tensor,torch.tensor(np.arange(10,81,5).reshape(3,5), dtype=torch.float64, device=device))\nprint(c)\n\n# method 3\n# the same operation above can be done using out argument of add method but initializing output variable is necessary\nd = torch.empty(size=(3,5), device=device, dtype=torch.float64)\ntorch.add(tensor,torch.tensor(np.arange(10,81,5).reshape(3,5), dtype=torch.float64, device=device), out=d)\nprint(d)\n\n# method 4\n# the same operation above can be done using inplace which much more better computationlly\ntensor.add_(torch.tensor(np.arange(10,81,5).reshape(3,5), dtype=torch.float64, device=device))\n","3cb32a8e":"# get a total of all the values in tensor c and d\nprint(c.sum())\nprint(d.sum())","f38f70ad":"# subtract a tensor from the tensor made above using 4 different methods\n\n# method 1\n# shape of the tesors should be same and both should be on same device, although dtype can differ\nprint(tensor.shape)\nprint(tensor.device)\nprint(tensor.dtype)\nc = tensor - torch.tensor(np.arange(10,81,5).reshape(3,5), dtype=torch.float64, device=device)\nprint(c)\n\n# method 2\nc = torch.subtract(tensor,torch.tensor(np.arange(10,81,5).reshape(3,5), dtype=torch.float64, device=device))\nprint(c)\n\n# method 3\n# the same operation above can be done using out argument of add method but initializing output variable is necessary\nd = torch.empty(size=(3,5), device=device, dtype=torch.float64)\ntorch.subtract(tensor,torch.tensor(np.arange(10,81,5).reshape(3,5), dtype=torch.float64, device=device), out=d)\nprint(d)\n\n# method 4\n# the same operation above can be done using inplace which much more better computationlly\ntensor.subtract_(torch.tensor(np.arange(10,81,5).reshape(3,5), dtype=torch.float64, device=device))\n","735962f6":"tensor = torch.tensor(np.arange(2,20,2).reshape(3,3), dtype=torch.int64, device=device)\ntensor","fdac8852":"# exponential values with pow method\nprint(tensor)\nprint(tensor.pow_(2)) # underscore will make it inplace\n\n# exponential values with asterick asterik\nprint(tensor)\nprint(tensor ** 3)","08eedf81":"# create 1D two tensors x and y \nx = torch.tensor(np.arange(1,5,1), dtype=torch.float64)\ny = torch.tensor(np.arange(5,9,1), dtype=torch.float64)\nprint(x)\nprint(y)","73454d00":"# using mul method to multiply x and y\nz = torch.ones(4,dtype=torch.float64)\ntorch.mul(x, y, out=z)","377ed227":"# using dot method to get the dot product of tensors x and y\n# (1*5) + (2*6)+ (3*7) + (4*8)\nanswer = torch.tensor(0, dtype=torch.float64)\ntorch.dot(x,y, out=answer)","48e5a45b":"# create 2D two tensors x and y \nx = torch.tensor(np.repeat([1,2,3],3).reshape(3,3), dtype=torch.float64)\ny = torch.tensor(np.arange(1,10,1), dtype=torch.float64)\nprint(x)\nprint(y)\n\n# Reshape tensor y to 3x3\ny = y.view(3,3)\nprint(y)","a01ff0a7":"# using mul method to multiply x and y\nz1 = torch.ones(3,3, dtype=torch.float64)\ntorch.mul(x, y, out = z1)\nprint(z1)\n\n# using matmul method to perform matrix multiplication on tensors x and y\nz2 = torch.ones(3,3, dtype=torch.float64)\ntorch.matmul(x, y, out = z2)\nprint(z2)\n\n# using x@y to perform matmul operation\nif torch.all(torch.eq(x@y, z2)):\n    print(\"Yes! matmul function works the same way as x@y.\")\nelse:\n    print(\"No! matmul function does not works the same way as x@y.\")","5b767026":"tensor_1 = torch.rand(size=(3,6), dtype=torch.float32, device=device)\ntensor_2 = torch.rand(size=(1,6), dtype=torch.float32, device=device)\n\nprint(tensor_1)\nprint(tensor_2)","d518ddf9":"tensor_1.add_(tensor_2)","7b782769":"tensor_1.subtract_(tensor_2)","46e32c31":"tensor = torch.tensor([[2,-1,5,0],[-1,3,3,-2]], dtype=torch.float32, device=device)\ntensor","c663c733":"# what is the maximum value of the tensor overall?\nvalue = torch.max(tensor)\nprint(value)","262b4851":"# what is the maximum value of the tensor above and at which index value for every column?\nindex, value = torch.max(tensor, dim=0) # dimension 0 means column wise\nprint(index)\nprint(value)","ef27c525":"# what is the maximum value of the tensor above and at which index value for every row?\nindex, value = torch.max(tensor, dim=1) # dimension 1 means row wise\nprint(index)\nprint(value)","d5efb02a":"# what is the minimum value of the tensor overall?\nvalue = torch.min(tensor)\nprint(value)","5ff49588":"# what is the minimum value of the tensor above and at which index value for every column?\nindex, value = torch.min(tensor, dim=0) # dimension 0 means column wise\nprint(index)\nprint(value)","8bc5ff99":"# what is the minimum value of the tensor above and at which index value for every row?\nindex, value = torch.min(tensor, dim=1) # dimension 1 means row wise\nprint(index)\nprint(value)","ac81ea89":"# convert tensor to absolute values\nvalue = torch.abs(tensor)\nprint(value)","94f8acd4":"value = torch.argmax(tensor)\nprint(value)","e5917296":"value = torch.argmin(tensor)\nprint(value)","657a7fe9":"# what is the mean value of the tensor overall?\nmean_tensor  = torch.mean(tensor)\nmean_tensor","35dc8289":"# sorting the tensor row wise\nprint(tensor)\nprint(torch.sort(tensor, descending=False, dim=1))","cbcf2f43":"# sorting the tensor column wise\nprint(tensor)\nprint(torch.sort(tensor, descending=False, dim=0))","b501f3b5":"## Ever heard of Broadcasting?","0664d4dd":"#### Do you know the difference between the **arange** method and **linspace** method?","0d4b9dd9":"## How about some mathematical operations?","82235139":"#### How do you get a tensor of normally disributed or uniformaly distributed values?","fe59a210":"## Basics of Tensors\n\nA tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array.It is a term and set of techniques known in machine learning in the training and operation of deep learning models can be described in terms of tensors. In many cases tensors are used as a replacement for NumPy to use the power of GPUs.\n\nTensors are a type of data structure used in linear algebra, and like vectors and matrices, you can calculate arithmetic operations with tensors.","700a911b":"## Can we convert our tensors to different types? Yeah!","1e35291b":"## Wait, there are useful mathematical methods still left...","7c834b31":"## Wanna play with some built-in methods?","29776809":"#### We have two exponentiation ways...","c5e09c18":"### Difference between mul and matmul methods\n**mul** method is used to perform scalar multiplication on tensors where each value of a matrix is multiplied by the corresponding value from another matrix yet, **matmul** or **mm** performs the proper matrix multiplication. ","d88eef58":"## Here comes... tensors!","9017e5c8":"## Are you afraid of multiplication and dot products of tensors? Don't be.","1ee47c53":"## You remember Numpy, right?","e2bdee5d":"#### Can you add or subtract both tensors of different shapes?\n\nYes, tensor_2 will duplicate its first row upto three rows to match the shape and will perform element wise add or subtract. Let's see..."}}