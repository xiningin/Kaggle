{"cell_type":{"8324536a":"code","dd277e54":"code","c9578645":"code","9e06d4b2":"code","0abb2989":"code","a6ac08c8":"code","0ebca280":"code","fdc0fc59":"code","ed0c6b20":"code","8fe42375":"code","886888f5":"code","fee5002a":"code","7356ccb8":"code","5aec1f4f":"code","f7769dcd":"code","95462d4a":"code","ad5a07ce":"code","2cfc849d":"code","7a1f4fd2":"code","d60e59f3":"code","ea054826":"code","61704723":"code","35efa07c":"code","8594b2cb":"code","013a2ab6":"code","4b42435f":"code","c53096b3":"code","1f3033d9":"code","634c3f65":"code","cf8eb8fc":"code","58d502d2":"code","cf6aab31":"code","2edbab85":"code","5b724f5b":"code","93a344b5":"code","574c6697":"code","f7badfde":"code","e77fa1d1":"code","fe06ed4b":"code","a8aa4fc3":"code","9a0548a4":"code","5f9b0503":"code","f6021342":"code","b9227f39":"code","9819fb5a":"code","51c8dcc1":"code","af90ebd6":"code","4405e117":"code","2a6dc05a":"code","6a54a264":"code","ec638720":"code","1558af43":"code","0a4e6deb":"code","4bbf208a":"markdown","5c2ed4a1":"markdown","bd61bcf7":"markdown","c5d06629":"markdown","a110ad66":"markdown","336f4e96":"markdown","f9b8b508":"markdown","d5b8d93b":"markdown","9953db91":"markdown","ea041883":"markdown"},"source":{"8324536a":"%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport regex as re\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dd277e54":"# These helper and data cleaning functions are from the old fast.ai course\n# The repository is here: https:\/\/github.com\/fastai\/fastai\/tree\/master\/old\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)\n        \ndef make_date(df, date_field:str):\n    \"Make sure `df[field_name]` is of the right date type.\"\n    field_dtype = df[date_field].dtype\n    if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        field_dtype = np.datetime64\n    if not np.issubdtype(field_dtype, np.datetime64):\n        df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)\n        \n\ndef add_datepart(df, fldnames, drop=True, time=False, errors=\"raise\"):\n    # add_datepart converts a column of df from a datetime64 to many columns containing the information from the date. \n    # This applies changes inplace.\n    if isinstance(fldnames,str): \n        fldnames = [fldnames]\n    for fldname in fldnames:\n        fld = df[fldname]\n        fld_dtype = fld.dtype\n        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n            fld_dtype = np.datetime64\n\n        if not np.issubdtype(fld_dtype, np.datetime64):\n            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n        targ_pre = re.sub('[Dd]ate$', '', fldname)\n        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n        if time: attr = attr + ['Hour', 'Minute', 'Second']\n        for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) \/\/ 10 ** 9\n        if drop: df.drop(fldname, axis=1, inplace=True)\n        \n        \ndef ifnone (a,b): #(a:Any,b:Any)->Any:\n    \"`a` if `a` is not None, otherwise `b`.\"\n    return b if a is None else a\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\n\ndef train_cats(df):    \n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n\ndef apply_cats(df, trn):\n    for n,c in df.items():\n        if (n in trn.columns) and (trn[n].dtype.name=='category'):\n            df[n] = c.astype('category').cat.as_ordered()\n            df[n].cat.set_categories(trn[n].cat.categories, ordered=True, inplace=True)\n\ndef numericalize(df, col, name, max_n_cat):\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = pd.Categorical(col).codes+1\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)      \n\n#\n# End fast.ai funcitons...\n#\n\n# This function I believe came from this guy: https:\/\/www.kaggle.com\/siavrez\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    #return df","c9578645":"df_cal = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv', parse_dates = ['date'])\ndf_sales_train = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ndf_prices = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ndf_submissions = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sample_submission.csv')\ndf_cal.shape, df_sales_train.shape, df_prices.shape, df_submissions.shape","9e06d4b2":"# The given format ain't no good for data science...\ndf_sales_train.head()","0abb2989":"# The format of the submission file is also not good for data science...\ndf_submissions.head()","a6ac08c8":"# Get all columns that aren't day columns\nlist_id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'] \ndf_d_cols = df_sales_train.drop(list_id_vars, axis=1)\ndf_d_cols.columns","0ebca280":"# This usually takes 16-19 sec to run\n%time df_melted_sales = df_sales_train.melt(id_vars = list_id_vars, value_vars = df_d_cols.columns, var_name = 'd', value_name = 'sales') ","fdc0fc59":"#the resulting melted sales data\ndf_melted_sales.head()","ed0c6b20":"# These columns are redundant to id so dropping them.\n# This notebook keeps runing into the 16 GB of RAM given in free Kaggle.\n# I do a bunch of stuff like this throughout trying to keep the DFs as small as possible.\n#df_melted_sales.drop(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], axis=1, inplace = True) \ndf_melted_sales.drop(['item_id', 'dept_id', 'cat_id', 'store_id'], axis=1, inplace = True) \ndf_melted_sales.head()","8fe42375":"sub_cols = df_submissions.drop(['id'], axis=1).columns\n\n# Like what we did to the sales data, we also need to melt the submission \ndf_melted_sub = df_submissions.melt(id_vars = ['id'], value_vars = sub_cols, \n                                    var_name = 'd', value_name = 'sales') \ndf_melted_sub.head()","886888f5":"df_melted_sub['d'] = df_melted_sub['d'].str.replace('F','')\ndf_melted_sub.head()","fee5002a":"df_melted_sub['d'] = pd.to_numeric(df_melted_sub['d'], errors='coerce') \ndf_melted_sub['d'] = df_melted_sub['d'] + 1913\ndf_melted_sub = df_melted_sub.applymap(str)\ndf_melted_sub['d'] = 'd_'+ df_melted_sub['d'].astype(str)\ndf_melted_sub.head()","7356ccb8":"# As per the contest notes, this should end at \"d_1941,\" so that's what we should see here...\ndf_melted_sub.tail() ","5aec1f4f":"%who DataFrame","f7769dcd":"del df_sales_train\ndel df_submissions\ndel df_d_cols\ngc.collect()","95462d4a":"reduce_mem_usage(df_cal)\nreduce_mem_usage(df_melted_sales)\nreduce_mem_usage(df_melted_sub) \nreduce_mem_usage(df_prices)","ad5a07ce":"# I've limited the sample size so we can keep it in RAM in a free Kaggle kernel.\n# Only looking at CA only.\n# Doing it as an 80\/20 split as is standard for train\/test.\ndf_ca_only =  df_melted_sales[df_melted_sales['state_id']=='CA']\ndf_ca_only.drop(['state_id'], axis=1, inplace = True) \ndf_test = df_ca_only.tail(20000) \ndf_train = df_ca_only.iloc[-120000:-20000] \ndf_submission = df_melted_sub.copy() # Copied just to keep naming conventions consistent\ndf_ca_only.shape, df_test.shape, df_train.shape, df_submission.shape","2cfc849d":"df_train.tail()","7a1f4fd2":"# If this was done right, the number in the id and d columns below should pick up where it left off above\ndf_test.head() ","d60e59f3":"# Merge the calendar data with the training, test and submission DFs\ndf_train = df_train.merge(df_cal, left_on='d', right_on='d', how='left')\ndf_test = df_test.merge(df_cal, left_on='d', right_on='d', how='left')\ndf_submission = df_submission.merge(df_cal, left_on='d', right_on='d', how='left')\ndf_train.shape, df_test.shape, df_submission.shape # should be same numb of cols","ea054826":"# Because we dropped columns from the training\/test\/submit DFs, we have to do this to be able to join to prices.\ndf_prices['id'] = df_prices['item_id'] +'_' + df_prices['store_id']\ndf_prices.drop(['item_id', 'store_id'], axis=1, inplace = True)\ndf_prices.head()","61704723":"# The training and submission data have a suffix that's not in the price data.\n# So here we're creating a column that matches the price id column.\ndf_train['id_for_price'] = df_train['id'].str.replace('_validation','')\ndf_test['id_for_price'] = df_test['id'].str.replace('_validation','')\ndf_submission['id_for_price'] = df_submission['id'].str.replace('_evaluation','')\ndf_submission['id_for_price'] = df_submission['id_for_price'].str.replace('_validation','')","35efa07c":"# Merge the price data with the training, test and submission DFs\ndf_train = pd.merge(df_train, df_prices,  how='left', left_on=['id_for_price', 'wm_yr_wk'], \n                    right_on = ['id', 'wm_yr_wk'])\ndf_test = pd.merge(df_test, df_prices,  how='left', left_on=['id_for_price', 'wm_yr_wk'], \n                   right_on = ['id', 'wm_yr_wk'])\ndf_submission = pd.merge(df_submission, df_prices,  how='left', left_on=['id_for_price', 'wm_yr_wk'], \n                         right_on = ['id', 'wm_yr_wk'])\n\n# Check to see if there are any missing prices\ndf_train['sell_price'].count(), df_train.shape, df_test['sell_price'].count(), df_test.shape, df_submission['sell_price'].count(), df_submission.shape","8594b2cb":"# Merging with price added a bunch of unneeded columns\ndf_train.drop(['id_for_price', 'id_y'], axis=1, inplace = True) \ndf_train.rename(columns = {\"id_x\":\"id\"}, inplace = True)\ndf_test.drop(['id_for_price', 'id_y'], axis=1, inplace = True) \ndf_test.rename(columns = {\"id_x\":\"id\"}, inplace = True)\ndf_submission.drop(['id_for_price', 'id_y'], axis=1, inplace = True) \ndf_submission.rename(columns = {\"id_x\":\"id\"}, inplace = True)","013a2ab6":"%who DataFrame","4b42435f":"# To save some RAM we will get rid of the dataframes that we merged from and are not in use now.\ndel df_cal\ndel df_melted_sales\ndel df_melted_sub\ndel df_prices\ngc.collect()","c53096b3":"reduce_mem_usage(df_train)\nreduce_mem_usage(df_test)\nreduce_mem_usage(df_submission)","1f3033d9":"# Saving the DFs for easy rollback and as a good starting point when reopening the notebook.\n# This takes a few minutes...\n\n# I am getting a \"halffloat\" error with the feather format...\n'''\ndf_train.to_feather('df_train_feather')\ndf_test.to_feather('df_test_feather')\ndf_submission.to_feather('df_submissions_feather')'''\n\n# Pickle seems ok though\ndf_train.to_pickle('df_train_pickle.pkl')\ndf_test.to_pickle('df_test_pickle.pkl')\ndf_submission.to_pickle('df_submissions_pickle.pkl')","634c3f65":"# Uncomment to load the pickle files...\n#df_train = pd.read_pickle('df_train_pickle.pkl')\n#df_test = pd.read_pickle('df_test_pickle.pkl')\n#df_submission = pd.read_pickle('df_submissions_pickle.pkl')","cf8eb8fc":"#cols_with_mmissing = df_merged.columns[df_merged.isnull().any()].tolist()\ndf_train.columns[df_train.isnull().any()]","58d502d2":"# Before removing nulls we add a boolean column to declare if the value was missing\ncols_with_missing = df_train.columns[df_train.isnull().any()].tolist()\n\nfor col in cols_with_missing:\n    df_train[col + '_was_missing'] = df_train[col].isnull()\n    df_test[col + '_was_missing'] = df_test[col].isnull()\n    df_submission[col + '_was_missing'] = df_submission[col].isnull()","cf6aab31":"imputer_values = {'event_name_1' :'None', 'event_type_1' :'None', \n                  'event_name_2' :'None', 'event_type_2' :'None'                  \n                 }\ndf_train.fillna(value = imputer_values, inplace = True)\ndf_test.fillna(value = imputer_values, inplace = True)\ndf_submission.fillna(value = imputer_values, inplace = True)\n\n# Checking to see if there are any missing values\n# The no of rows in each DF should be equal to the count\ndf_train.shape, df_train.count(), df_test.shape, df_test.count(), df_submission.shape, df_submission.count()","2edbab85":"#list of all non-numeric data\ndf_train.select_dtypes(include = 'object').columns ","5b724f5b":"#we're going to use pandas date exploding features, so the given dates will wind up being redundant\ndrop_fields = ['weekday', 'year', 'wday', 'month']\ndf_train.drop(drop_fields, axis = 1, inplace = True)\ndf_test.drop(drop_fields, axis = 1, inplace = True)\ndf_submission.drop(drop_fields, axis = 1, inplace = True)\ndf_train.shape, df_test.shape, df_submission.shape","93a344b5":"# The next cell will change the IDs into categorical values.\n# But the submission file needs them in the original format, so saving them to a series for use below\ndf_sub_ids = df_submission['id'].copy()\n\n# We'll need the d column for the submission file format.\n# But we can drop it from the train\/test dfs as we already have date information in the DF\ndf_train.drop(['d'], axis = 1, inplace = True)\ndf_test.drop(['d'], axis = 1, inplace = True)\ndf_sub_d = df_submission.pop('d')","574c6697":"# Label encode\n# For RAM considerations, we're not going to one hot encode anything.\n# These are functions from fast.ai that convert objects to categories and keeps those categories consistent across training and test\ntrain_cats(df_train)\napply_cats(df_test, df_train)\napply_cats(df_submission, df_train)\ndf_train.shape, df_test.shape, df_submission.shape\n\n# Change categories to numbers\ncat_cols = df_train.select_dtypes(include = 'category').columns\nfor i in cat_cols:\n    df_train['cat_'+i] = df_train[i].cat.codes\n    df_test['cat_'+i] = df_test[i].cat.codes\n    df_submission['cat_'+i] = df_submission[i].cat.codes\n\ndf_train.drop(cat_cols, axis = 1, inplace = True)\ndf_test.drop(cat_cols, axis = 1, inplace = True)\ndf_submission.drop(cat_cols, axis = 1, inplace = True)\n\n# Should return an empty list\ndf_train.select_dtypes(include = 'category').columns ","f7badfde":"# Should return an empty list\ndf_train.select_dtypes(include = 'object').columns","e77fa1d1":"# Explode out the date column into a ton of columns that a model can use\ndate_cols = df_train.select_dtypes(include = 'datetime64').columns\nfor i in date_cols:\n    add_datepart(df_train, i)\n    add_datepart(df_test, i)\n    add_datepart(df_submission, i)\n\n# Should see many more columns at the end of the list (year, month... is_year_end, etc) \n# Should also not see a \"date\" column\ndf_train.columns ","fe06ed4b":"# should only see number and boolean columns\ndf_train.dtypes ","a8aa4fc3":"reduce_mem_usage(df_train)\nreduce_mem_usage(df_test)\nreduce_mem_usage(df_submission)","9a0548a4":"gc.collect()","5f9b0503":"# pickle bookmark to restart from here\ndf_train.to_pickle('df_train_pickle.pkl')\ndf_test.to_pickle('df_test_pickle.pkl')\ndf_submission.to_pickle('df_submissions_pickle.pkl')","f6021342":"# train and test we'll use below, the submission file sales column is all 0's so, we can just get rid of it.\ny_train = df_train.pop('sales')\ny_test = df_test.pop('sales')\ndf_submission.drop(['sales'], axis =1, inplace = True)","b9227f39":"# As noted in the intro we're just using an out of the box RF for now.a\nm = RandomForestRegressor(n_jobs =-1) \n%time m.fit(df_train, y_train)","9819fb5a":"# Scoring in r-squared because it's the default and this is the quick start notebook\n# r-squared over 0 means that we're predicting better than if we just predicted the average value\n# r-squared of 1 means perfect\n# it's a good enough metric to optimize the model on\nm.score (df_test, y_test) ","51c8dcc1":"# For making predictions that we'll submit put train and test together and fit the model again for better predictions\ntrain_test_concat = pd.concat([df_test, df_train])\ny_concat = pd.concat([y_test, y_train])\nm.fit(train_test_concat, y_concat)\n\n# Scores much better on values it's seen! if only the world were so easy...\nm.score (train_test_concat, y_concat)","af90ebd6":"df_submission.dtypes","4405e117":"# And finally, let's make some predictions that we'll submit\npredictions = m.predict(df_submission)\npredictions","2a6dc05a":"# Add the predictions made above and the original ids back onto the dataframe\ndf_submission['sales'] = predictions\ndf_submission['id'] = df_sub_ids\ndf_submission['F'] = df_sub_d","6a54a264":"d = {'id': df_sub_ids,'F': df_sub_d, 'preds': predictions}\nsub_file = pd.DataFrame(data=d)\nsub_file.head()","ec638720":"# As per the competition requirements, we have to change the \"d\" values to F values\nsub_file['F'] = sub_file['F'].str.replace('d_','')\nsub_file['F'] = pd.to_numeric(sub_file['F'], errors='coerce') \nsub_file['F'] = sub_file['F'] - 1913\nsub_file['F'] = 'F'+ sub_file['F'].astype(str)\nsub_file.head()","1558af43":"# Pivot and reset the index to flatten the file\nsub_file = sub_file.pivot(index='id', columns='F', values='preds')\nsub_file = sub_file.reset_index()\nsub_file.head()","0a4e6deb":"# And finally... we have our submisison file...\ncsv_submit = sub_file.to_csv('submission.csv', index = False)","4bbf208a":"# Melt!<img src=\"https:\/\/www.wikihow.com\/images\/thumb\/4\/47\/Make-a-Crab-Melt-Sandwich-Final.jpg\/aid2170143-v4-728px-Make-a-Crab-Melt-Sandwich-Final.jpg.webp\" width=200 align = right valign = \"top\" hspace=\"20\">\nThe pandas melt method unpivots column values and puts it in rows... i.e. melting creates a row for every day for an item\/store combination. Doing this puts the data in a flat format that in a few cells will allow us us to merge the sales training data with the calendar and pricing data sets.","5c2ed4a1":"# Overview of this notebook! <img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/d\/d1\/HerdQuit.jpg\" align = \"right\" width = 300 alt=\"Wrangling\">\n\n### 1. Unpivot\nThe format of the data provided in this contest is not so great for data science! In order to get the historic data in the format that a model can use to make predictions requires a lot of wrangling. \n\nThe given layout of the data is that each product\/store combination is a row, and each date is a column (going out to 1900+ date columns...). It look like this:\n![image.png](attachment:image.png)\n\n###### (cows are to data as cowboys are to code? hmm...)","bd61bcf7":"# Transform! <img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/3d\/Fesoj_-_Papilio_machaon_%28by%29.jpg\" align = \"right\" width = 200>\nSo now we'll do some transforms to get all the data in the DFs numeric and remove nulls... i.e. we're putting the data in the format that you need to have the data in for any ML\/DL model.","c5d06629":"A model expects each row that only has variables for one prediction. So, for this particular contest, each row that's fed to a model needs to be an individual product\/store\/date\/etc combination. This requires unpivoting the original data (via pandas' \"melt\" function) and getting it to look like this:\n![image.png](attachment:image.png)\n\n### 2. Do some text manipulation\nMaybe just to be slightly more annoying, the column headings for the dates are given in the format of \"F#\" as opposed to the above historic pricing data of \"d_#\" (\"forecast\" instead of \"date\"), so in order to join the submission predictions to the calendar and pricing data, we'll need to change F# to d_#. Yes, this is just some basic text manipulation, but as I was hacking through this I found it really annoying that they couldn't just keep the date format consistent...\n\n### 3. Make predictions<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/c1\/George_%22Corpsegrinder%22_Fisher_of_Cannibal_Corpse.jpg\" align = \"right\" width = 200> \nAfter that, we're finally ready to make some predictions. In this notebook, we'll be using an out of the box Random Forest. \n\nThe point of this notebook isn't to build a highly accurate model or to do the other fun\/creative stuff of data science (like feature engineering, comparing differnt models, EDA, etc.); **the point of this notebook is to do the unglamorous grunt work so folks can fork it and do fun stuff yourself.**\n\n*(see the picture of a man who knows how to grunt...)*\n\n\n### 3. Re-pivot and do some text manipulation and make the submission file\nThe contest requires that the CSV submission file to be in the format as given (see the first picture)... so you have to re-pivot after you've made your predictions to make the final submission file. Yeah, this is more annoying string manipulaiton work...\n\n\nOk, enough of my rambling, let's get to the code...","a110ad66":"# Split!<img src=\"https:\/\/res.cloudinary.com\/sagacity\/image\/upload\/c_crop,h_1486,w_2046,x_0,y_0\/c_limit,dpr_auto,f_auto,fl_lossy,q_80,w_1080\/Screen_Shot_2017-01-18_at_10.04.37_AM_qaaqch.png\" align = \"right\" width = 200>\nI'm using a smaller subet of data because it'll be faster to read and predict and to be able to keep it all in the free Kaggle RAM. Note that because this is a time series problem I'm not using scikit-learn train test split. This is because we want to train with older data and test with new data to simulate actual conditions. You will have all historical data, and will try to predict the next few months... Train test split will randomly split the data.\n\n","336f4e96":"### Free up some RAM\nThe free Kaggle notebook only gives 16GB of RAM. So we'll delete the initial DFs that were merged into larger DFs and run the \"reduce_mem\" function on all DFs that we are going to be using in the next section of the notebook.\n\nI do a lot of stuff like this throughout. Obviously, feel free to skip these blocks if you're running this notebook in a more powerful environment.","f9b8b508":"# Leaderboard update - TOP 50% !!!\nFunny thing... this notebook that does a very simple random forest with no feature engineering, or hyperparamater tuning at all, shot way up the leaderboard from bottom 75% in the public version, to top 50% in the private version!!\n\nJust a pretty good lesson that doing a simple random forest should probably be the first thing that you do, and makes for a pretty good baseline for most structured data tasks.","d5b8d93b":"# Train and predict! <img src = \"https:\/\/cdn.ventrata.com\/image\/upload\/ar_1.5,c_fill,dpr_1.0,f_jpg,w_600\/v1543512847\/puu4dyjmualnnrtppag0.png\" align = \"right\" width = 200>\nOk, finally we're ready to train the model and get some predictions","9953db91":"# Make a file and submit it <img src =\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/19\/Letterbox_Bourdon.jpg\/800px-Letterbox_Bourdon.jpg\" align = \"right\" width= 200>\nSo, now that we have our predictions, we need to get the submission file in the format that is required by the contest.","ea041883":"# Merge! <img src=\"https:\/\/www.roadtrafficsigns.com\/img\/md\/X\/left-lane-merge-sign-x-w4-1l.png\" align = \"right\" width = 200>\nNow we'll combine all the reference data to the training and test data into the training and test data frames.\n\nIn addition to the standard train\/test dfs, we're also going to take the same steps with the submission data."}}