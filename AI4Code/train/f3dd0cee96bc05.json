{"cell_type":{"f606c711":"code","29ce7047":"code","f347ceda":"code","20a92da3":"code","17be319f":"code","ccf0c3bb":"code","cad87d90":"code","53a5040f":"code","49cfd3f5":"code","c66b79b1":"code","cbb2a1d3":"code","82a5cc24":"code","658ca067":"code","0db9b01b":"code","e583a505":"code","ad75e015":"code","b0da602c":"code","0e7db064":"code","7f7c703a":"code","524d89e9":"code","65a5019b":"markdown","1d5aaac9":"markdown","4edb43c0":"markdown","530d143b":"markdown"},"source":{"f606c711":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29ce7047":"from collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split","f347ceda":"train_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\n","20a92da3":"plt_value = [train_df.shape[0],test_df.shape[0]] \nplt_name = [\"Train\",\"Test\"]\nplt.figure()\nplt.bar(plt_name,plt_value,color=(0.2, 0.4, 0.6, 0.6))\nplt.title(\"Train and Test data amount\")\nplt.show()","17be319f":"train_labels_amount = Counter(train_df['label'])\ntrain_labels_amount","ccf0c3bb":"sns.countplot(train_df['label'])","cad87d90":"# Create input x and output label y\nx_train =  train_df.drop(labels = [\"label\"],axis = 1) \ny_train =  train_df['label']\n\nprint(\"X_train shape : \",x_train.shape)\nprint(\"y_train shape : \",y_train.shape)\n\n#We are going to use CNN so we need to reshape the x_train\n#Since they are black and white images color channel is 1\n\nx_train = x_train.values.reshape(-1,28,28,1)\nprint(\"X_train shape : \",x_train.shape)\n","53a5040f":"N = 0\n\nplt.imshow(x_train[N][:,:,0])","49cfd3f5":"# Encode label to one hot vectors (eg: \n# 0 -> [1,0,0,0,0,0,0,0,0,0]\n# 1 -> [0,1,0,0,0,0,0,0,0,0]\n# 2 -> [0,0,1,0,0,0,0,0,0,0]\n# 3 -> [0,0,0,1,0,0,0,0,0,0]\n# 4 -> [0,0,0,0,1,0,0,0,0,0]\n# 5 -> [0,0,0,0,0,1,0,0,0,0]\n# 6 -> [0,0,0,0,0,0,1,0,0,0]\n# 7 -> [0,0,0,0,0,0,0,1,0,0]\n# 8 -> [0,0,0,0,0,0,0,0,1,0]\n# 9 -> [0,0,0,0,0,0,0,0,0,1]\n\n\nprint(\"y_train shape : \",y_train.shape)\n\nlabel = y_train[N]\ny_train = to_categorical(y_train)\none_hot = y_train[N]\nprint(\"y_train shape : \",y_train.shape)\n\n\n\nprint(\"Label:{} -> encode:\".format(label),end = \" \")\nprint(one_hot)\n    \n\n","c66b79b1":"x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n                                                  test_size = 0.1, random_state=2)","cbb2a1d3":"model = Sequential()\n\nmodel.add(Conv2D(filters = 128, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 128, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 128, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\n\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\n\n\nmodel.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n#model.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation = \"relu\"))\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dense(10, activation = \"softmax\"))","82a5cc24":"optimizer = Adam(lr=0.0001)\nmodel.compile(optimizer = optimizer ,\n              loss = \"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()","658ca067":"datagenerator = ImageDataGenerator(\n    rotation_range=35,  # randomly rotate images in the range (degrees, 0 to 180)\n    zoom_range = 0.2, # Randomly zoom image \n    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n     horizontal_flip = True\n    \n)\ndatagenerator.fit(x_train)","0db9b01b":"num_epochs = 30\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.000001)","e583a505":"history = model.fit_generator(datagenerator.flow(x_train,y_train, batch_size=64),\n                              epochs = num_epochs,\n                              validation_data = (x_val,y_val),\n                              verbose = 1,\n                              steps_per_epoch=int(len(y_train)\/64),\n                              callbacks=[learning_rate_reduction])","ad75e015":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","b0da602c":"x_test = test_df.values.reshape(-1,28,28,1)\nprint(\"X_train shape : \",x_test.shape)","0e7db064":"# predict results\nresults = model.predict(x_test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","7f7c703a":"results","524d89e9":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"submission.csv\",index=False)","65a5019b":"### Create model\n","1d5aaac9":"### Check data amounts","4edb43c0":"### Prepare data for model input","530d143b":"### Data Augmentation\n"}}