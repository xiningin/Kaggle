{"cell_type":{"7b080d1b":"code","7c6608c0":"code","37f23d76":"code","c30b19b7":"code","cb54a19c":"code","9e7d090a":"code","01348034":"code","cd292f92":"markdown","087a4ae3":"markdown","dc7b1671":"markdown","2b33009d":"markdown","1e3e5ad9":"markdown","f239b177":"markdown","05acc09c":"markdown","6a48a6a0":"markdown","6a2c0fd2":"markdown","73a28622":"markdown"},"source":{"7b080d1b":"import pandas as pd\ndata=pd.read_csv('..\/input\/random-linear-regression\/train.csv')","7c6608c0":"data=data[0:100]","37f23d76":"data['x']","c30b19b7":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport matplotlib.animation as animation\n# delta thetha J thetha function diff og cost function wrt theta 1\ndef dtjt(X,Y,m,theta,theta_not):\n    ans=0\n    for i in range(m):\n        ans=ans+((theta*X[i]+theta_not-Y[i])*X[i])\n    return ans\n# delta thetha J thetha 0 function diff of cost function wrt theta 0\ndef dtjt_not(X,Y,m,theta,theta_not):\n    ans=0\n    for i in range(m):\n        ans=ans+((theta*X[i]+theta_not-Y[i]))\n    return ans\n\n#  predictions h(Q) \ndef h(x):\n    hi=[]\n    for i in range(0,len(X)):\n       hi.append(theta_not[-1]+theta[-1]*X[i])\n    return hi   \n    \narr_input=data['x']\n#print(df_input)\n\n# data set normalization values stored in an array \narr_input_nor=[]\n\narr_output=data['y']\n#print(df_output)\n\n\n# conforming our result     \nprint('array input')\nprint(arr_input)    \nprint('array output')\nprint(arr_output)\n\n# making an array input \narr_input_copy=arr_input\n\n# data set normalization \nfor i in range(0,len(arr_input)):\n    arr_input_nor.append((arr_input[i]-np.mean(arr_input_copy))\/(np.var(arr_input_copy)))\n \n#print('normalized array')    \n#print(arr_input_nor)  uncomment the cell to see the normalized array \n\nX=arr_input_nor\nY=arr_output\n\ntheta=[0,10]  #  initial theta one values \ntheta_not=[0,10] # initial theta0 values \neta_initial=0.1  # the learning rate \nm=len(X) # the length of input stored \n\ni=0 # initializaing the iteration \n                 ## GRADIENT DESCENT## \nwhile(abs(theta[-1]-theta[-2])>0.000001 and (len(theta)and len(theta_not))<10000):\n     # the condition of convergence is the diff b\/n value of the latest theta and previous theta \n    # and the iteration should not cross 1000 \n    \n    i=i+1 # the iteration indicator \n    eta=eta_initial\/np.sqrt(i) # the adaptive rate formula \n    temp=theta[-1]-eta*dtjt(X, Y, m, theta[-1],theta_not[-1]) # updating the theta \n    temp_not=theta_not[-1]-eta*dtjt_not(X, Y, m, theta[-1],theta_not[-1]) # updating theta 0\n    theta.append(temp) # storing  the theta values \n    theta_not.append(temp_not) # storing the theta 0 values \n    \n    \nprint('the slope  is ')    \nprint(theta[-1]) \nprint('the intercept  is ')        \nprint(theta_not[-1])    \n\n# graph plot \n\nfig=plt.figure()   # creating a figure object \n#left, bottom, width, height \nax=fig.add_axes([0,0,1,1]) # use for  tuning the positon of plot   \n#ax.plot(X,Y,label='Y')\nsns.scatterplot(X,Y,label='y') # scatterplot of Y values(ground truth) \nax.plot(X,h(X),label='h(x)') # the plot of predicted values \nplt.ylabel('h(x) VS Y') # Y label \nplt.xlabel('X') # the X axis label \nax.legend() # for describing the elements of graph","cb54a19c":"theta[-20:] # last 15 iteration results ","9e7d090a":"theta_not[-20:]","01348034":"len(theta)","cd292f92":"so the final hypothesis function is h(x)=theta*x+theta_not that is 421.42421820258477*x+41.766","087a4ae3":"# GD algorithm for univarient linear regression \n![image.png](attachment:image.png)","dc7b1671":"![image.png](attachment:image.png)","2b33009d":"# DATA","1e3e5ad9":"# number of iteration taken to converge ?\n","f239b177":"# about the note book \n\n# 1. what will i learn ?\n  \n  math behind linear regression using gradient descent and understand how the algorithm converge and the final hypothesis     is obtained \n  \n# 2. how is the result obtained ?\n\n   The prodecure is as follows:\n   \n   . take input with one feature and output and storing it in an array.\n   \n   . normalizing the input is needed so that gradient descent(GD) can converge in a better way and it brings all the\n     input into a similar scale between 0 to 1.\n     \n   . declaring functions for partial differentiation of cost function wrt the parameters so as to use in gradient descent\n   \n   . defining function for hypothesis h(x)=slopeX(input sample)+intercept.\n   \n   .initializing parameters theta0(intercept) and theta1(slope) for GD algo and simultaneously updating parameters and          deciding a convergence criteria for GD \n   \n   .ploting the scatter plot of data points and hypothesis trying  fit the data points \n   \n# resources to understand the concept better:\n\nhttps:\/\/www.youtube.com\/watch?v=kHwlB_j7Hkc","05acc09c":"# the slope how is it changing ?","6a48a6a0":"# how does the 2D contour look like ?\n![image.png](attachment:image.png)","6a2c0fd2":"# 3D plot of cost function ","73a28622":"# Notation used in program and our goal ?\n![image.png](attachment:image.png)"}}