{"cell_type":{"64e2c755":"code","2bfba7ab":"code","edd95f3b":"code","7215586b":"code","db4c086a":"code","2db8477c":"code","1bd8f321":"code","6137f6b6":"code","02032440":"code","b66db16c":"code","8a403ad3":"code","ab64e095":"code","7e34e257":"markdown","8a2262c9":"markdown"},"source":{"64e2c755":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ntrain=pd.read_csv('..\/input\/credit-default-prediction-ai-big-data\/train.csv')\ntest=pd.read_csv('..\/input\/credit-default-prediction-ai-big-data\/test.csv')\n","2bfba7ab":"import re\n[int(x) for x in str.split(' ') if x.isdigit()] #[0]\n\ndef getnumbers(string):\n    retstr=[]\n    for ci,xi in enumerate(string):\n        #print(xi)\n        if xi==np.nan:\n            print()\n        else:\n            retstr.append( [int(s) for s in re.findall(r'-?\\d+\\.?\\d*', xi)][0] )\n    return retstr\n\ntrain['Years in current job']=getnumbers( train['Years in current job'].fillna('10 Year').values )\ntest['Years in current job']=getnumbers( test['Years in current job'].fillna('10 Year').values )\n","edd95f3b":"train['Home Ownership'].unique()","7215586b":"def vulleeg(data):\n    data['NetVal']=0\n    for xi in range(len(data)):\n        tempv=data.loc[xi,'Current Credit Balance']\n        if data.loc[xi,'Current Loan Amount']==99999999.0:\n            data.loc[xi,'Current Loan Amount']=tempv\n        if data.loc[xi,'Term']=='Short Term':\n            data.loc[xi,'Term']=24\n        else:\n            data.loc[xi,'Term']=120\n        if data.loc[xi,'Home Ownership']=='Own Home':\n            data.loc[xi,'NetVal']=data.loc[xi,'Annual Income']*7 - data.loc[xi,'Current Credit Balance']\n        if data.loc[xi,'Home Ownership']=='Home Mortgage':\n            data.loc[xi,'NetVal']=data.loc[xi,'Annual Income']*7 - data.loc[xi,'Current Credit Balance']\n\n    data['Term2']=data['Current Loan Amount']\/(data['Monthly Debt']+1)\n    data['Term3']=data['Current Loan Amount']\/(data['Annual Income']+1)\n    data['ratio']=data['Maximum Open Credit']\/(data['Annual Income']+1)\n    data['Annual Income']=np.log(data['Annual Income']+1)\n    data['Maximum Open Credit']=np.log(data['Maximum Open Credit']+1)\n    data['Current Loan Amount']=np.log(data['Current Loan Amount']+1)\n    data['Current Credit Balance']=np.log(data['Current Credit Balance']+1)\n    data['Monthly Debt']=np.log(data['Monthly Debt']+1)\n    data['NetVal']=np.log(data['NetVal']+1)\n\n    data=data.replace(np.inf,999)\n    data=data.replace(-np.inf,-999)\n    \n    \n    return data\n        \n        \n           \ntrain=vulleeg(train)\ntest=vulleeg(test)\ntrain","db4c086a":"len(train['Credit Default'].dropna())","2db8477c":"def kluster2(data,grbvar,label,nummercl,level):\n    '''nummercl < ncol'''\n\n    from sklearn.cluster import KMeans\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import classification_report    \n    from scipy import spatial\n    import time\n    import matplotlib.pyplot as plt\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.decomposition import PCA,TruncatedSVD,NMF,FastICA\n    from umap import UMAP  # knn lookalike of tSNE but faster, so scales up\n    from sklearn.manifold import TSNE,Isomap,SpectralEmbedding,spectral_embedding,LocallyLinearEmbedding,MDS #limit number of records to 100000\n\n    from sklearn.gaussian_process import GaussianProcessClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.svm import SVC,NuSVC\n    import xgboost as xgb\n\n    simdata=data[data[label].isnull()==False].drop([label],axis=1)\n\n    #   Label encoding or remove string data\n    ytrain=data[label]\n    if True: \n        from category_encoders.cat_boost import CatBoostEncoder\n        CBE_encoder = CatBoostEncoder()\n        cols=[ci for ci in data.columns if ci not in ['index',label]]\n        coltype=data.dtypes\n        featured=[ci for ci in cols]\n        ytrain=data[label]\n        CBE_encoder.fit(data[:len(simdata)].drop(label,axis=1), ytrain[:len(simdata)])\n        data=CBE_encoder.transform(data.drop(label,axis=1))\n        data[label]=ytrain\n        \n    #add random columns\n    for xi in range(5):\n        labnm='rand'+str(xi)\n        data[labnm]=np.random.randint(0,10,size=(len(data), 1))\n    #find mean per label\n    train_me=data.drop([grbvar],axis=1).groupby(label).mean()        \n    #imput\n    kol=data.drop(label,axis=1).columns\n    from sklearn.experimental import enable_iterative_imputer  \n    from sklearn.impute import IterativeImputer\n    print( len(data.dropna()),len(data[data[label]!=np.nan]))\n    if len(data.dropna())<len(data[label].dropna()):\n        print('impute empty data')\n        data = IterativeImputer(random_state=0).fit_transform(data.drop(label,axis=1))\n    data = pd.DataFrame(data,columns=kol)   \n    data[label]=ytrain.values\n    #cosin similarity transform\n\n    print(train_me)\n    \n    simdata=data[data[label].isnull()==False].drop([grbvar,label],axis=1)\n    ytrain=data[data[label].isnull()==False][label]\n    simtest=data[data[label].isnull()==True].drop([grbvar,label],axis=1)\n    ytest=np.random.randint(0,1,size=(len(simtest), 1))  #fill data not used\n    iddata=data[grbvar].astype('int')\n    submit=data[data[label].isnull()==True][[grbvar,label]]\n    print(submit.columns,submit.describe())\n    if len(simtest)==0:\n        simtest=simdata[int(len(simdata)*0.9):]\n        ytest=ytrain[int(len(simdata)*0.9):]\n\n    print(simdata.shape,simtest.shape,data.shape,ytrain.shape)\n    #train_se=data.groupby('label').std()\n    train_cs2=cosine_similarity(simdata,train_me)\n    test_cs2=cosine_similarity(simtest,train_me)\n    dicto={ np.round(i,1) : ytrain.unique()[i] for i in range(0, len(ytrain.unique()))} #print(clf.classes_)\n    ypred=pd.Series(np.argmax(train_cs2,axis=1)).map(dicto)\n    \n    print('cosinesimilarity direction' ,classification_report(ytrain.values, ypred)  )\n    \n    trainmu=pd.DataFrame( simdata.values-simdata.values.mean(axis=1)[:,None])\n    testmu=pd.DataFrame( simtest.values-simtest.values.mean(axis=1)[:,None])\n    \n    trainmu[label]=ytrain\n    trainme2=trainmu.groupby(label).mean()    \n    #spatial 0.79\n\n    def adjcos_dist(size, matrix, matrixm):\n        distances = np.zeros((len(matrix),size))\n        M_u = matrix.mean(axis=1)\n        m_sub = matrix - M_u[:,None]\n        for first in range(0,len(matrix)):\n            for sec in range(0,size):\n                distance = spatial.distance.cosine(m_sub[first],matrixm[sec])\n                distances[first,sec] = distance\n        return distances\n\n    trainsp2=adjcos_dist(len(trainme2),trainmu.drop(label,axis=1).values,trainme2.values)\n    testsp2=adjcos_dist(len(trainme2),testmu.values,trainme2.values)\n    \n    print(trainsp2.shape,trainme2.shape,simdata.shape)\n    print('cosinesimilarity distance', classification_report(ytrain.values, pd.Series(np.argmin(trainsp2,axis=1)).map(dicto)  )  )\n    # blended with three classifiers random Forest\n    classifier=[\n                RandomForestClassifier(n_jobs=4),\n                LogisticRegression(n_jobs=4),\n                xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11,n_jobs=4),\n                SVC(probability=True),\n                KNeighborsClassifier(n_neighbors=3),\n                PCA(n_components=nummercl,random_state=0,whiten=True),\n                #TruncatedSVD(n_components=nummercl, n_iter=7, random_state=42),\n                #FastICA(n_components=nummercl,random_state=0),\n    ]\n    simdata2=np.hstack((train_cs2,trainsp2))\n    simtest2=np.hstack((test_cs2,testsp2))\n    kol2=['x'+str(xi) for xi in range(nummercl)]+['y'+str(xi) for xi in range(nummercl)]\n    for clf in classifier:\n        #clf = RandomForestClassifier(n_jobs=4) #GaussianProcessClassifier()#\n        print(simdata.shape,ytrain.shape,simtest.shape,data.shape,simdata2.shape,simtest2.shape)\n        print(simtest2)\n\n        try:\n            clf.fit(simdata, ytrain)\n            train_tr=clf.predict_proba(simdata)\n            test_tr=clf.predict_proba(simtest)\n        except:\n            clf.fit(simdata.append(simtest))\n            train_tr=clf.transform(simdata)\n            test_tr=clf.transform(simtest)\n            \n        #dicto={ i : clf.classes_[i] for i in range(0, len(clf.classes_) ) } #print(clf.classes_)\n        ypred=pd.Series(np.argmax(train_tr,axis=1)).map(dicto)\n        print(str(clf)[:10] ,classification_report(ytrain.values, ypred)  )\n        simdata2=np.hstack((simdata2,train_tr))\n        simtest2=np.hstack((simtest2,test_tr))\n        kol2=kol2+[str(clf)[:3]+str(xi) for xi in range(nummercl)]\n    #concat data\n    simdata=pd.DataFrame(simdata2,columns=kol2)\n    simtest=pd.DataFrame(simtest2,columns=kol2)\n\n    #plotimg2=pd.DataFrame(train_cs2,columns=['x'+str(xi) for xi in range(nummercl)])\n    clusters = [PCA(n_components=nummercl,random_state=0,whiten=True),\n                TruncatedSVD(n_components=nummercl, n_iter=7, random_state=42),\n                FastICA(n_components=nummercl,random_state=0),\n                Isomap(n_components=nummercl),\n                LocallyLinearEmbedding(n_components=nummercl),\n                SpectralEmbedding(n_components=nummercl),\n                #MDS(n_components=nummercl),\n                TSNE(n_components=3,random_state=0),\n                UMAP(n_neighbors=nummercl,n_components=10, min_dist=0.3,metric='minkowski'),\n                #NMF(n_components=nummercl,random_state=0),                \n                ] \n    clunaam=['PCA','tSVD','ICA','Iso','LLE','Spectr','tSNE','UMAP','NMF']\n    \n    clf = RandomForestClassifier()\n    clf.fit(simdata, ytrain)\n        \n    print('rFor pure',classification_report(ytrain,clf.predict(simdata))) \n    print(clf.predict(simtest))\n    submit[label]=clf.predict(simtest)\n    submit[label]=submit[label].astype('int')\n    submit[grbvar]=iddata#submit[grbvar].values.astype('int')\n    \n    submit[[grbvar,label]].to_csv('submission.csv',index=False)\n            \n    for cli in clusters:\n        print(cli)\n        clunm=clunaam[clusters.index(cli)] #find naam\n        \n        if str(cli)[:3]=='NMF':\n            maxmin=np.array([simdata.min(),simtest.min()])\n            simdata=simdata-maxmin.min()+1\n        svddata = cli.fit_transform(simdata.append(simtest))  #totale test\n        \n        #test what method is best\n        #ttrain=pd.DataFrame(svddata)\n        #ttrain[label]=ytrain\n        #clustertechniques2(ttrain.reset_index(),label,'index') #.append(test)\n        \n        \n        km = KMeans(n_clusters=nummercl, random_state=0)\n        km.fit_transform(svddata)\n        cluster_labels = km.labels_\n        cluster_labels = pd.DataFrame(cluster_labels, columns=[label])\n        #print(cluster_labels.shape) # train+test ok\n        pd.DataFrame(svddata[:len(simdata)]).plot.scatter(x=0,y=1,c=ytrain.values,colormap='viridis')\n        print(clunm,'kmeans_labelmean',cluster_labels.mean())   \n        submit[label]=cluster_labels[len(simdata):]\n        submit[label]=submit[label]#.astype('int')\n        submit[[grbvar,label]].to_csv('Clu'+str(cli)[:5]+'kmean.csv',index=False)\n        print('kmean'+str(cli)[:10],submit[[grbvar,label]].groupby(label).count() )\n        clf= xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11,n_jobs=4)\n        clf.fit(svddata[:len(simdata)], ytrain)\n        print(clunm+'+xgb',classification_report(ytrain,clf.predict(svddata[:len(simdata)])))        \n        #pd.DataFrame(svddata).plot.scatter(x=0,y=1,c=clf.predict(svddata),colormap='viridis')\n        submit[label]=clf.predict(svddata[len(simdata):])\n        #print(submit[submit[:,:].isnull()],submit[submit[label]==np.inf])\n        submit[label]=submit[label].astype('int')\n        submit[[grbvar,label]].to_csv('submitxgbkl_'+str(cli)[:5]+'.csv',index=False)\n        print('xgb'+str(cli)[:10],submit[[grbvar,label]].groupby(label).count() )\n    \n        plt.show()\n\n        #clusdata=pd.concat([pd.DataFrame(grbdata.reset_index()[grbvar]), cluster_labels], axis=1)\n        #if len(grbdata)<3: \n        #    data['Clu'+clunm+str(level)]=cluster_labels.values\n            \n        #else:\n        #    data=data.merge(clusdata,how='left',left_on=grbvar,right_on=grbvar)\n        confmat=confusion_matrix ( ytrain,cluster_labels[:len(simdata)])\n        dicti={}\n        for xi in range(len(confmat)):\n            #print(np.argmax(confmat[xi]),confmat[xi])\n            dicti[xi]=np.argmax(confmat[xi])\n        #print(dicti)\n        #print('Correlation\\n',confusion_matrix ( ytrain,cluster_labels[:len(ytrain)]))\n        #print(clunm+'+kmean clusterfit', classification_report(ytrain.map(dicti), cluster_labels[:len(simdata)])  )   \n        invdict = {np.round(value,1): key for key, value in dicti.items()}\n        #print(invdict)\n        submit[label]=cluster_labels[len(simdata):].values\n        #print(cluster_labels[len(simdata):])\n        #print(submit.describe().T)\n        ytest=submit[label].astype('int')\n        submit[label]=ytest.map(invdict)#.astype('int')\n        submit[[grbvar,label]].to_csv('submit'+str(cli)[:5]+'kmean.csv',index=False)\n        print('kmean'+str(cli)[:10],submit[[grbvar,label]].groupby(label).count() )        \n    return data\n\n\n\n#train2=kluster(plotimg[:10000].reset_index(),'index','label',10,1)\ntrain2=kluster2( train.append(test,ignore_index=True),'Id','Credit Default',len(train['Credit Default'].unique() ),1)","1bd8f321":"train.append(test)","6137f6b6":"train.shape,test.shape","02032440":"    \n    def data2sparse(data):\n        #print(data.info())\n        typekolom=data.dtypes\n        nummercol=[x for x in data.columns if typekolom[x]!='object']\n        objcol=[x for x in data.columns if typekolom[x]=='object']\n        print(nummercol)\n        from scipy.sparse import coo_matrix,vstack,hstack,csr_matrix\n        totalSPNr=coo_matrix(data[nummercol])\n        sparse=pd.DataFrame( totalSPNr.col,columns=['col'] )\n        sparse['row']=pd.DataFrame( totalSPNr.row )\n        sparse['data']=pd.DataFrame(totalSPNr.data)\n        sparse=sparse.dropna()\n        totalSPN=coo_matrix((sparse.data,(sparse.col,sparse.row)))\n        from sklearn.preprocessing import OneHotEncoder\n        enc=OneHotEncoder()\n        ohenc=enc.fit_transform(data[objcol])\n        totalSPN=vstack([totalSPN,ohenc.T] )\n        kolom=[x for x in nummercol]+[y for y in enc.get_feature_names(objcol)]\n        return pd.DataFrame( csr_matrix(totalSPN.transpose()).todense(),columns=kolom)\n\n    data2sparse(train)","b66db16c":"#train['total']=train['Home Ownership']+' '+train['Purpose']+' '+train['Term']\n#test['total']=test['Home Ownership']+' '+test['Purpose']+' '+test['Term']\nkolom=[x for x in train.columns if x not in ['Id','total','Home Ownership','Purpose','Term','total','Credit Default']]\nkolom","8a403ad3":"def ALSforecast(train,test,ytrain,ytest,textcolumn,valuecolumns,indexv,label):\n    print('input data',len(train),len(test),len(ytrain),len(ytest))\n    from scipy.sparse import coo_matrix,hstack,csr_matrix\n    def data2sparse(data):\n        #print(data.info())\n        typekolom=data.dtypes\n        nummercol=[x for x in data.columns if typekolom[x]!='object']\n        objcol=[x for x in data.columns if typekolom[x]=='object']\n        print(nummercol)\n        from scipy.sparse import coo_matrix,vstack,hstack,csr_matrix\n        totalSPNr=coo_matrix(data[nummercol])\n        sparse=pd.DataFrame( totalSPNr.col,columns=['col'] )\n        sparse['row']=pd.DataFrame( totalSPNr.row )\n        sparse['data']=pd.DataFrame(totalSPNr.data)\n        sparse=sparse.dropna()\n        totalSPN=coo_matrix((sparse.data,(sparse.col,sparse.row)))\n        from sklearn.preprocessing import OneHotEncoder\n        enc=OneHotEncoder()\n        ohenc=enc.fit_transform(data[objcol])\n        totalSPN=vstack([totalSPN,ohenc.T] )\n        kolom=[x for x in nummercol]+[y for y in enc.get_feature_names(objcol)]\n        return csr_matrix(totalSPN.transpose()),kolom\n    \n    print(textcolumn)    \n    #_________________________________________________________________\n    #prepare data\n    # random dataenricher\n    if False:\n            cols=[ci for ci in train.columns if ci not in [indexv,'index',label,textcolumn,'target']]\n            coltype=train.dtypes\n\n            for ci in cols:\n                if (coltype[ci]!=\"object\"  ):\n                    train[ci]=train[ci].astype('float32')\n                    print(ci)\n                    for di in cols[cols.index(ci)+1:]:\n                        if (coltype[di]!=\"object\"  ):\n                            s2=(train[ci]*train[di]).astype('float32')\n                            if np.abs(ytrain.corr(s2))>0.15:\n                                train[ci+'x'+di]=s2\n                                test[ci+'x'+di]=(test[ci]*test[di]).astype('float32')\n                                valuecolumns=valuecolumns+[ci+'x'+di]\n                            s2=(train[ci]\/(train[di]+1.1)).astype('float32')\n                            if np.abs(ytrain.corr(s2))>0.15:\n                                train[ci+'\/'+di]=s2\n                                test[ci+'\/'+di]=(test[ci]\/(test[di]+1)  ).astype('float32') \n                                valuecolumns=valuecolumns+[ci+'\/'+di]\n                            s2=(train[ci]*np.log(train[di]+1.1)).astype('float32')\n                            if np.abs(ytrain.corr(s2))>0.15:\n                                train[ci+'log'+di]=s2\n                                test[ci+'log'+di]=(test[ci]*np.log(test[di]+1)  ).astype('float32') \n                                valuecolumns=valuecolumns+[ci+'log'+di]\n\n    df=train.append(test)   \n    ratings,kolom=data2sparse(df.drop([label],axis=1))\n    #transform embed regression forecast\n    if False:\n        #dropcolumns=[x for x in df.columns if x not in valuecolumns]\n        X=ratings[:len(train)]\n        Z=ratings[len(train):]\n        Y=Z\n        #print(np.linalg.pinv( X.T.dot(X)) )\n        XtXi=np.linalg.pinv(np.dot(X.T,X).toarray()) \n        print(XtXi.shape,X.shape,Y.shape)\n        from scipy.sparse import coo_matrix,vstack,hstack,csr_matrix\n        for li in range(1,int(len(train)\/len(test)+1)):\n            Y = vstack((Y,Z))#np.concatenate((Y, Z))\n            print(Y.shape)\n        Y=Y[:len(train)]\n        XtXiXt=np.dot(csr_matrix(XtXi),X.T) \n        XtXiXtZ=np.dot(XtXiXt,Y)\n        print(XtXiXtZ.shape,X.shape,ratings.shape)\n        Yhat=pd.DataFrame( np.dot(ratings,XtXiXtZ).toarray(),columns=kolom ,index=df.index) \n        Yhat[indexv]=df[indexv]     #restore index\n        Yhat[label+'__Yh']=df[label]  #embed forecast\n        Yhat[label]=df[label]       #restore label\n        for ci in valuecolumns:\n            df[ci+'_rest']=df[ci]-Yhat[ci]\n            #dtrain[ci+'_ratio']=dtrain[ci]\/(Yhat[ci]+1)\n        print('transformed splitted basic\/rest',df.shape)\n\n    \n    #valuecolumns=[x for x in train.columns if x not in [textcolumn,indexv]]\n    print('expanded',df.shape,valuecolumns)\n    #data to sparse vectorizer\n    #ratings=data2sparse(df.drop(label,axis=1))\n    #try:\n    if False:\n        from sklearn.feature_extraction.text import TfidfVectorizer\n        vectorizer = TfidfVectorizer()\n        ratings=vectorizer.fit_transform(df[textcolumn].fillna(' ')  )\n        print(ratings.shape,'tfidf')\n        ratings=hstack([ratings,data2sparse(df[valuecolumns])] )\n        print(ratings.shape,'stacked')\n    #except:\n        #try:\n        #    ratings=data2sparse(df[valuecolumns])\n        #except:\n        #    ratings=vectorizer.fit_transform(df[textcolumn].fillna(' ')  )                    \n    print('rating matrix ready',ratings.shape)\n    \n    #_________________________________________________________________\n    #find hidden data\n    #ALS\n    import tqdm\n    import time\n    from implicit.als import AlternatingLeastSquares\n    from implicit.bpr import BayesianPersonalizedRanking \n    from implicit.nearest_neighbours import (BM25Recommender, CosineRecommender,TFIDFRecommender, bm25_weight)\n    #filter for NMF\n    #min_rating=0.0\n    #ratings.data[ratings.data < min_rating] = 0\n    #ratings.eliminate_zeros()\n    #\n    ratings.data = np.ones(len(ratings.data))\n    # generate a recommender model based off the input params\n    model = AlternatingLeastSquares()  #ALS model                --------------------------------------------- change possible\n    model = BayesianPersonalizedRanking()  #Baysian ranking model\n    # lets weight these models by bm25weight.\n    print(\"weighting matrix by bm25_weight\")\n    ratings = (bm25_weight(ratings, B=0.9) * 5).tocsr()\n    print(\"training model %s\", model)\n    start = time.time()\n    model.fit(ratings)\n    print(\"trained model '%s' in %s\", time.time() - start)\n    result=model.fit(ratings)\n    print('user',model.user_factors.shape,'item', model.item_factors.shape)\n    \n    #_________________________________________________________________\n    #print clustering graph on factors\n    #from umap import UMAP  # knn lookalike of tSNE but faster, so scales up\n    #from sklearn.manifold import TSNE #limit number of records to 100000\n    from matplotlib import pyplot as plt\n    #X_total_clu = UMAP(n_neighbors=2,n_components=10, min_dist=0.3,metric='minkowski').fit_transform(model.item_factors)\n    #X_total_clu = TSNE().fit_transform(model.item_factors)\n    #print(X_total_clu.shape)\n    #plt.scatter(X_total_clu[:len(train),0],X_total_clu[:len(train),1] ,c=ytrain.values,cmap='prism')\n    #plt.show()\n    \n    #_________________________________________________________________\n    reconstr=np.dot(model.item_factors,model.user_factors.T)\n    print(reconstr.shape)\n    \n    \n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier,ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n    from sklearn.linear_model import PassiveAggressiveClassifier,Perceptron,SGDClassifier,LogisticRegression\n    import xgboost as xgb\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.neural_network import MLPClassifier,MLPRegressor\n    from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_absolute_error\n    from sklearn.svm import SVC, LinearSVC,NuSVC\n\n    classifiers = [KNeighborsClassifier(n_neighbors=2,n_jobs=-1),\n                   xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11,n_jobs=4),\n                   SGDClassifier(average=True,max_iter=100),\n                   DecisionTreeClassifier(),\n                   LogisticRegression( solver=\"lbfgs\",multi_class ='auto'),#solver=\"lbfgs\",max_iter=500,n_jobs=-1,multi_class ='auto'),\n                   MLPClassifier(alpha=0.510,activation='logistic'),\n                   RandomForestClassifier(n_estimators=100, random_state=42,n_jobs=-1, oob_score=True),\n                   ExtraTreesClassifier(n_estimators=10, max_depth=50, min_samples_split=5, min_samples_leaf=1, random_state=None, min_impurity_decrease=1e-7),     \n                   PassiveAggressiveClassifier(max_iter=50, tol=1e-3,n_jobs=-1),\n                   MLPClassifier(),\n                   SVC(),\n                   \n                    ]\n\n    for classif in classifiers:\n        #classif=LogisticRegression( solver=\"lbfgs\",max_iter=500,n_jobs=-1,multi_class ='auto')\n\n        classif.fit(reconstr[:len(train)],ytrain ) #model.item_factors[:len(train)]\n        trainpredi=classif.predict(reconstr[:len(train)])\n        testpredi=classif.predict(reconstr[len(train):]) \n        plt.scatter(x=trainpredi, y=ytrain, marker='.', alpha=0.3)\n        print('classifier',str(classif))\n        print('compare train.algo ratio, train mean -test mean',ytrain.mean()\/trainpredi.mean(),trainpredi.mean(),testpredi.mean())\n        plt.show()   \n        \n        submit = pd.DataFrame({indexv: test[indexv],label: testpredi})\n        \n        submit[label]=submit[label].astype('int')\n        if str(classif)[:3]=='SVC':\n            filenaam='submission.csv' #str(classif)[:3]+\n        else:\n            filenaam=str(classif)[:3]+'submission.csv' #\n        submit.to_csv(path_or_buf =filenaam, index=False)\n\n        try:\n            cm = confusion_matrix(trainpredi, ytrain )\n            cmtest = confusion_matrix(testpredi, ytest)\n            print(\"Confusion Matrix: \\n\", cm)\n            print(\"Test Confusion Matrix: \\n\", cmtest)\n            print(\"Classification Report: \\n\", classification_report(trainpredi, ytrain ))\n            print(\"Test Classification Report: \\n\", classification_report(testpredi, ytest ))\n        \n        except:\n            cm = confusion_matrix(trainpredi, ytrain )\n            print('mean abs error train -test', mean_absolute_error(trainpredi,ytrain))\n            \n            print(\"Confusion Matrix: \\n\", cm)\n            print(\"Classification Report: \\n\", classification_report(trainpredi,ytrain ))\n    return submit\n\n\ncolom=[x for x in train.columns if x not in ['qid','question_text'] ]\n\ntemp=ALSforecast(train,test,train['Credit Default'],train['Credit Default'],'total',kolom,'Id','Credit Default')","ab64e095":"# imputer for handling missing values\nfor ci in train.columns:\n    coltype=train.dtypes\n    if (coltype[ci]!=\"object\"):\n        train[ci]=train[ci].fillna(train[ci].median())\n        if ci !='Credit Default':\n            test[ci]=test[ci].fillna(train[ci].median())\n","7e34e257":"def kluster2(data,grbvar,label,nummercl,level):\n    '''nummercl < ncol'''\n\n    from sklearn.cluster import KMeans\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import classification_report    \n    from scipy import spatial\n    \n    import matplotlib.pyplot as plt\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.neighbors import KNeighborsClassifier,NeighborhoodComponentsAnalysis\n    from sklearn.decomposition import PCA,TruncatedSVD,NMF,FastICA\n    from umap import UMAP  # knn lookalike of tSNE but faster, so scales up\n    from sklearn.manifold import TSNE,Isomap,SpectralEmbedding,spectral_embedding,LocallyLinearEmbedding,MDS #limit number of records to 100000\n\n    from sklearn.gaussian_process import GaussianProcessClassifier\n\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.svm import SVC,NuSVC\n\n    #   Label encoding or remove string data\n    ytrain=data[label]\n    if True: \n        from category_encoders.cat_boost import CatBoostEncoder\n        CBE_encoder = CatBoostEncoder()\n        cols=[ci for ci in data.columns if ci not in ['index',label]]\n        coltype=data.dtypes\n        featured=[ci for ci in cols]\n        ytrain=data[label]\n        data = CBE_encoder.fit_transform(data.drop(label,axis=1), ytrain)\n        data[label]=ytrain\n        \n    #add random columns\n    for xi in range(5):\n        labnm='rand'+str(xi)\n        data[labnm]=np.random.randint(0,10,size=(len(data), 1))\n    #find mean per label\n    train_me=data.groupby(label).mean()        \n    #imput\n    kol=data.drop(label,axis=1).columns\n    from sklearn.experimental import enable_iterative_imputer  \n    from sklearn.impute import IterativeImputer\n    data = IterativeImputer(random_state=0).fit_transform(data.drop(label,axis=1))\n    data = pd.DataFrame(data,columns=kol)   \n    data[label]=ytrain.values\n    #cosin similarity transform\n\n    print(train_me)\n    simdata=data[data[label].isnull()==False].drop([label],axis=1)\n    ytrain=data[data[label].isnull()==False][label]\n    simtest=data[data[label].isnull()==True].drop([label],axis=1)\n    ytest=ytrain.sample(len(simtest))  #fill data not used\n    iddata=data[grbvar]\n    submit=data[data[label].isnull()==True]\n    #print(submit.columns,submit.describe())\n\n    \n    print(simdata.shape,simtest.shape,data.shape,ytrain.shape)\n    #train_se=data.groupby('label').std()\n    train_cs2=cosine_similarity(simdata,train_me)\n    test_cs2=cosine_similarity(simtest,train_me)\n    print('cosinesimilarity direction' ,classification_report(ytrain.values, np.argmax(train_cs2,axis=1))  )\n    \n    trainmu=pd.DataFrame( simdata.values-simdata.values.mean(axis=1)[:,None])\n    testmu=pd.DataFrame( simtest.values-simtest.values.mean(axis=1)[:,None])\n    \n    trainmu[label]=ytrain\n    trainme2=trainmu.groupby(label).mean()    \n    #spatial 0.79\n\n    def adjcos_dist(size, matrix, matrixm):\n        distances = np.zeros((len(matrix),size))\n        M_u = matrix.mean(axis=1)\n        m_sub = matrix - M_u[:,None]\n        for first in range(0,len(matrix)):\n            for sec in range(0,size):\n                distance = spatial.distance.cosine(m_sub[first],matrixm[sec])\n                distances[first,sec] = distance\n        return distances\n\n    trainsp2=adjcos_dist(len(trainme2),trainmu.drop(label,axis=1).values,trainme2.values)\n    testsp2=adjcos_dist(len(trainme2),testmu.values,trainme2.values)\n    \n    print(trainsp2.shape,trainme2.shape,simdata.shape)\n    print('cosinesimilarity distance', classification_report(ytrain.values, np.argmin(trainsp2,axis=1))  )\n    # blended with three classifiers random Forest\n    \n    clf = GaussianProcessClassifier()#RandomForestClassifier()\n    clf.fit(simdata, ytrain)\n    train_tr=clf.predict_proba(simdata)\n    test_tr=clf.predict_proba(simtest)\n    print('Gauss' ,classification_report(ytrain.values, np.argmax(train_tr,axis=1))  )\n\n    clf = LogisticRegression()\n    clf.fit(simdata, ytrain)\n    train_lo=clf.predict_proba(simdata)\n    test_lo=clf.predict_proba(simtest)\n    \n    print('logist' ,classification_report(ytrain.values, np.argmax(train_lo,axis=1))  )\n\n    clf = NuSVC(probability=True)\n    clf.fit(simdata, ytrain)\n    train_sv=clf.predict_proba(simdata)\n    test_sv=clf.predict_proba(simtest)\n    \n    print('SVC' ,classification_report(ytrain.values, np.argmax(train_sv,axis=1))  )\n\n    #concat data\n    kol2=['x'+str(xi) for xi in range(nummercl)]+['y'+str(xi) for xi in range(nummercl)]+['w'+str(xi) for xi in range(nummercl)]+['v'+str(xi) for xi in range(nummercl)]+['u'+str(xi) for xi in range(nummercl)]\n    simdata=pd.DataFrame(np.hstack((train_cs2,trainsp2,train_tr,train_lo,train_sv)),columns=kol2)\n    simtest=pd.DataFrame(np.hstack((test_cs2,testsp2,test_tr,test_lo,test_sv)),columns=kol2)\n\n    #plotimg2=pd.DataFrame(train_cs2,columns=['x'+str(xi) for xi in range(nummercl)])\n    clusters = [PCA(n_components=nummercl,random_state=0,whiten=True),\n                TruncatedSVD(n_components=nummercl, n_iter=7, random_state=42),\n                FastICA(n_components=nummercl,random_state=0),\n                NMF(n_components=nummercl,random_state=0),\n                Isomap(n_components=nummercl),\n                LocallyLinearEmbedding(n_components=nummercl),\n                SpectralEmbedding(n_components=nummercl),\n                #MDS(n_components=nummercl),\n                TSNE(n_components=3,random_state=0),\n                UMAP(n_neighbors=nummercl,n_components=10, min_dist=0.3,metric='minkowski'),\n                ] \n    clunaam=['PCA','tSVD','ICA','NMF','Iso','LLE','Spectr','tSNE','UMAP']\n    \n    clf = RandomForestClassifier()\n    clf.fit(simdata, ytrain)\n    print('rFor pure',classification_report(ytrain,clf.predict(simdata))) \n    \n    submit[label]=clf.predict(simtest)\n    submit[label]=submit[label].astype('int')\n    submit[grbvar]=submit[grbvar].astype('int')\n    \n    submit[[grbvar,label]].to_csv('submit_rfor.csv',index=False)\n            \n    for cli in clusters:\n        print(cli)\n        clunm=clunaam[clusters.index(cli)] #find naam\n        if clunm=='NMF':\n            simdata=simdata-simdata.min()+1\n        svddata = cli.fit_transform(simdata.append(simtest))\n        \n        #test what method is best\n        #ttrain=pd.DataFrame(svddata)\n        #ttrain[label]=ytrain\n        #clustertechniques2(ttrain.reset_index(),label,'index') #.append(test)\n        \n        \n        km = KMeans(n_clusters=nummercl, random_state=0)\n        km.fit_transform(svddata)\n        cluster_labels = km.labels_\n        cluster_labels = pd.DataFrame(cluster_labels, columns=[label])\n        print(cluster_labels.shape)\n        pd.DataFrame(svddata[:len(simdata)]).plot.scatter(x=0,y=1,c=ytrain.values,colormap='viridis')\n        print(clunm,'kmeans_labelmean',cluster_labels.mean())   \n        submit[label]=cluster_labels[len(simdata):]\n        submit[label]=submit[label].astype('int')\n        submit[[grbvar,label]].to_csv('Clu'+str(cli)[:3]+'kmean.csv',index=False)\n        print('kmean'+str(cli)[:10],submit[[grbvar,label]].groupby(label).count() )\n        from sklearn.ensemble import RandomForestClassifier\n        from sklearn.svm import LinearSVC,SVC\n        import xgboost as xgb\n\n        clf = NuSVC()#RandomForestClassifier()\n        clf= xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11,n_jobs=4)\n        clf.fit(svddata[:len(simdata)], ytrain)\n        print(clunm+'+xgb',classification_report(ytrain,clf.predict(svddata[:len(simdata)])))        \n        #pd.DataFrame(svddata).plot.scatter(x=0,y=1,c=clf.predict(svddata),colormap='viridis')\n        submit[label]=clf.predict(svddata[len(simdata):])\n        submit[label]=submit[label].astype('int')\n    \n        submit[[grbvar,label]].to_csv('submitkl_'+str(cli)[:3]+'.csv',index=False)\n        print('xgb'+str(cli)[:10],submit[[grbvar,label]].groupby(label).count() )\n    \n        plt.show()\n\n        #clusdata=pd.concat([pd.DataFrame(grbdata.reset_index()[grbvar]), cluster_labels], axis=1)\n        #if len(grbdata)<3: \n        #    data['Clu'+clunm+str(level)]=cluster_labels.values\n            \n        #else:\n        #    data=data.merge(clusdata,how='left',left_on=grbvar,right_on=grbvar)\n        confmat=confusion_matrix ( ytrain,cluster_labels[:len(simdata)])\n        dicti={}\n        for xi in range(len(confmat)):\n            #print(np.argmax(confmat[xi]),confmat[xi])\n            dicti[xi]=np.argmax(confmat[xi])\n        #print('Correlation\\n',confusion_matrix ( ytrain,cluster_labels))\n        print(clunm+'+kmean clusterfit', classification_report(ytrain.map(dicti), cluster_labels[:len(simdata)])  )   \n        invdict = {value: key for key, value in dicti.items()}\n        submit[label]=cluster_labels[len(simdata):]\n        ytest=submit[label].astype('int')\n        submit[label]=ytest.map(invdict)#.astype('int')\n        submit[[grbvar,label]].to_csv('Clu'+str(cli)[:3]+'kmean.csv',index=False)\n        print('kmean'+str(cli)[:10],submit[[grbvar,label]].groupby(label).count() )        \n    return data\n\n\n\n#train2=kluster(plotimg[:10000].reset_index(),'index','label',10,1)\ntrain2=kluster2(train.append(test),'Id','Credit Default',len(train['Credit Default'].unique() ),1)\n","8a2262c9":"def kluster(data,grbvar,label,nummercl,level):\n    '''nummercl < ncol'''\n\n    from sklearn.cluster import KMeans\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import classification_report    \n    from scipy import spatial\n    \n    import matplotlib.pyplot as plt\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.neighbors import KNeighborsClassifier,NeighborhoodComponentsAnalysis\n    from sklearn.decomposition import PCA,TruncatedSVD,NMF,FastICA\n    from umap import UMAP  # knn lookalike of tSNE but faster, so scales up\n    from sklearn.manifold import TSNE,Isomap,SpectralEmbedding,spectral_embedding,LocallyLinearEmbedding,MDS #limit number of records to 100000\n\n    from sklearn.gaussian_process import GaussianProcessClassifier\n\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.svm import SVC\n\n    #   Label encoding or remove string data\n    ytrain=data[label]\n    if True: \n        from category_encoders.cat_boost import CatBoostEncoder\n        CBE_encoder = CatBoostEncoder()\n        cols=[ci for ci in data.columns if ci not in ['index',label]]\n        coltype=data.dtypes\n        featured=[ci for ci in cols]\n        ytrain=data[label]\n        data = CBE_encoder.fit_transform(data.drop(label,axis=1), ytrain)\n        data[label]=ytrain\n    #imput\n    from sklearn.experimental import enable_iterative_imputer  \n    from sklearn.impute import IterativeImputer\n    data = IterativeImputer(random_state=0).fit_transform(data.drop(label,axis=1))\n\n        \n    #cosin similarity transform\n\n    train_me=data.groupby(label).mean()\n    simdata=data.drop(label,axis=1)\n    #train_se=data.groupby('label').std()\n    train_cs2=cosine_similarity(simdata,train_me)\n    print('cosinesimilarity direction' ,classification_report(ytrain.values, np.argmax(train_cs2,axis=1))  )\n    \n    trainmu=pd.DataFrame( simdata.values-simdata.values.mean(axis=1)[:,None])\n    trainmu[label]=ytrain\n    trainme2=trainmu.groupby(label).mean()    \n    #spatial 0.79\n\n    def adjcos_dist(size, matrix, matrixm):\n        distances = np.zeros((len(matrix),size))\n        M_u = matrix.mean(axis=1)\n        m_sub = matrix - M_u[:,None]\n        for first in range(0,len(matrix)):\n            for sec in range(0,size):\n                distance = spatial.distance.cosine(m_sub[first],matrixm[sec])\n                distances[first,sec] = distance\n        return distances\n\n    trainsp2=adjcos_dist(len(trainme2),trainmu.drop(label,axis=1).values,trainme2.values)\n    print(trainsp2.shape,trainme2.shape,simdata.shape)\n    print('cosinesimilarity distance', classification_report(ytrain.values, np.argmin(trainsp2,axis=1))  )\n    # blended with random Forest\n    \n    clf = GaussianProcessClassifier()#RandomForestClassifier()\n    clf.fit(simdata, ytrain)\n    train_tr=clf.predict_proba(simdata)\n    print('rFor' ,classification_report(ytrain.values, np.argmax(train_tr,axis=1))  )\n\n    clf = LogisticRegression()\n    clf.fit(simdata, ytrain)\n    train_lo=clf.predict_proba(simdata)\n    print('logist' ,classification_report(ytrain.values, np.argmax(train_lo,axis=1))  )\n\n    clf = SVC(kernel='rbf', gamma=0.7,probability=True)\n    clf.fit(simdata, ytrain)\n    train_sv=clf.predict_proba(simdata)\n    print('SVC' ,classification_report(ytrain.values, np.argmax(train_sv,axis=1))  )\n\n    #concat data\n    plotimg=pd.DataFrame(1-trainsp2,columns=['y'+str(xi) for xi in range(nummercl)])\n    plotimg2=pd.DataFrame(train_cs2,columns=['x'+str(xi) for xi in range(nummercl)])\n    plotimg3=pd.DataFrame(train_tr,columns=['w'+str(xi) for xi in range(nummercl)])\n    plotimg4=pd.DataFrame(train_lo,columns=['v'+str(xi) for xi in range(nummercl)])\n    plotimg5=pd.DataFrame(train_sv,columns=['u'+str(xi) for xi in range(nummercl)])\n    for xi in plotimg2.columns:\n        plotimg[xi]=plotimg2[xi]\n    for xi in plotimg3.columns:\n        plotimg[xi]=plotimg3[xi]\n    for xi in plotimg4.columns:\n        plotimg[xi]=plotimg4[xi]\n    for xi in plotimg5.columns:\n        plotimg[xi]=plotimg5[xi]\n        \n    simdata=plotimg\n    #simdata = cosine_similarity(simdata)\n    clusters = [PCA(n_components=nummercl,random_state=0,whiten=True),\n                TruncatedSVD(n_components=nummercl, n_iter=7, random_state=42),\n                FastICA(n_components=nummercl,random_state=0),\n                NMF(n_components=nummercl,random_state=0),\n                Isomap(n_components=nummercl),\n                LocallyLinearEmbedding(n_components=nummercl),\n                SpectralEmbedding(n_components=nummercl),\n                #MDS(n_components=nummercl),\n                TSNE(n_components=3,random_state=0),\n                UMAP(n_neighbors=nummercl,n_components=10, min_dist=0.3,metric='minkowski'),\n                ] \n    clunaam=['PCA','tSVD','ICA','NMF','Iso','LLE','Spectr','tSNE','UMAP']\n    \n    #grbdata=data.groupby(grbvar).mean()\n    #simdata = cosine_similarity(grbdata.fillna(0))\n    #print(grbdata.shape,simdata.shape)\n    #if len(grbdata)<3:\n    #    simdata=data.drop(grbvar,axis=1)\n    #    simdata=simdata.dot(simdata.T)\n    #    from sklearn import preprocessing\n    #    simdata = preprocessing.MinMaxScaler().fit_transform(simdata)\n\n    clf = RandomForestClassifier()\n    clf.fit(simdata, ytrain)\n    print('rFor pure',classification_report(ytrain,clf.predict(simdata))) \n    \n    for cli in clusters:\n        print(cli)\n        clunm=clunaam[clusters.index(cli)] #find naam\n        if clunm=='NMF':\n            simdata=simdata-simdata.min()+1\n        svddata = cli.fit_transform(simdata)\n        \n        #test what method is best\n        #ttrain=pd.DataFrame(svddata)\n        #ttrain[label]=ytrain\n        #clustertechniques2(ttrain.reset_index(),label,'index') #.append(test)\n        \n        \n        km = KMeans(n_clusters=nummercl, random_state=0)\n        km.fit_transform(svddata)\n        cluster_labels = km.labels_\n        clulabel='Clu'+clunm+str(level)\n        cluster_labels = pd.DataFrame(cluster_labels, columns=[clulabel])\n        print(cluster_labels.shape)\n        pd.DataFrame(svddata).plot.scatter(x=0,y=1,c=ytrain.values,colormap='viridis')\n        print(clunm,'kmeans_labelmean',cluster_labels.mean())        \n        from sklearn.ensemble import RandomForestClassifier\n        from sklearn.svm import LinearSVC,SVC\n        import xgboost as xgb\n\n        clf = SVC()#RandomForestClassifier()\n        clf= xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11,n_jobs=4)\n        clf.fit(svddata, ytrain)\n        print(clunm+'+xgb',classification_report(ytrain,clf.predict(svddata)))        \n        #pd.DataFrame(svddata).plot.scatter(x=0,y=1,c=clf.predict(svddata),colormap='viridis')\n        \n\n        plt.show()\n\n        #clusdata=pd.concat([pd.DataFrame(grbdata.reset_index()[grbvar]), cluster_labels], axis=1)\n        #if len(grbdata)<3: \n        #    data['Clu'+clunm+str(level)]=cluster_labels.values\n            \n        #else:\n        #    data=data.merge(clusdata,how='left',left_on=grbvar,right_on=grbvar)\n        confmat=confusion_matrix ( ytrain,cluster_labels)\n        dicti={}\n        for xi in range(len(confmat)):\n            #print(np.argmax(confmat[xi]),confmat[xi])\n            dicti[xi]=np.argmax(confmat[xi])\n        #print('Correlation\\n',confusion_matrix ( ytrain,cluster_labels))\n        print(clunm+'+kmean clusterfit', classification_report(ytrain.map(dicti), cluster_labels)  )            \n    return data\n\n\n\n#train2=kluster(plotimg[:10000].reset_index(),'index','label',10,1)\n#train2=kluster(train,'Id','Credit Default',len(train['Credit Default'].unique() ),1)\n"}}