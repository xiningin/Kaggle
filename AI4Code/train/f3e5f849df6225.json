{"cell_type":{"1663ec14":"code","ce15f305":"code","376adce8":"code","6b2e6a35":"code","4c9903b7":"code","55642829":"code","5e20fd63":"code","18dbbfe4":"code","cfaac51f":"code","f9f7b3ff":"code","a1dabde9":"code","f70dcd3e":"code","67f7de7f":"code","f59966f4":"code","3b7b0617":"code","7a168e82":"code","4f8683d1":"code","1dd3d9a8":"code","01ce82e3":"code","be2134c0":"code","7d4b6ced":"code","015b9611":"code","f14da6af":"code","8fec6187":"code","dc7d67ee":"code","58540603":"code","dadedd43":"code","d1b49959":"code","45a90963":"code","94370481":"code","c4631527":"code","e0a2db00":"code","138795f2":"code","8e167075":"code","53ec3842":"code","6d46387b":"code","1e1cbd38":"code","e77f682f":"code","7b3abe03":"code","86b3678a":"code","50e78bba":"code","de626cda":"code","158ea4ce":"code","9c6e4d61":"code","bec5e386":"code","5b9b4439":"code","96648ee7":"markdown","c341d945":"markdown","0eac0e7f":"markdown","2fd3b8c0":"markdown","a1fcebc9":"markdown","22bb36b5":"markdown","4921c7d1":"markdown","be9ddea6":"markdown","4b823357":"markdown","d8dc4d82":"markdown","740a66f1":"markdown","f7776012":"markdown","9459122e":"markdown","5300620d":"markdown","8e3738c7":"markdown","a63a4566":"markdown"},"source":{"1663ec14":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom datetime import datetime\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nfrom utils import handle_data\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ce15f305":"data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nprint(data.shape)\ndata.head()","376adce8":"test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.shape","6b2e6a35":"num, cat, cont, disc, yr = handle_data(data).extract_var()\nprint(\"Numerical : \"+str(len(num))+\", Categorical \"+\n      str(len(cat)) + \", Continuous: \" + str(len(cont))+ \", Discrete: \" + str(len(disc)))","4c9903b7":"#Features with Year\nyr","55642829":"data.isnull().sum()","5e20fd63":"test.isnull().sum()","18dbbfe4":"data = data.drop(columns = ['PoolQC', 'MiscFeature','Alley','Fence','FireplaceQu'])\ntest = test.drop(columns = ['PoolQC', 'MiscFeature','Alley','Fence','FireplaceQu'])\ntrain_pre = handle_data(data = data) \ntest_pre = handle_data(data = test)","cfaac51f":"#Updating the columns in different types of variables\nnum, cat, cont, disc, yr = train_pre.extract_var()\nnum_test, cat_test, cont_test, disc_test, yr_test = test_pre.extract_var()","f9f7b3ff":"#For Categorical data\ntrain_pre.check_for_missing(cat)","a1dabde9":"#For numerical data\ntrain_pre.check_for_missing(cont)","f70dcd3e":"train_pre.Imputation(cont,stats=False)","67f7de7f":"train_pre.Imputation(cat,stats=False)","f59966f4":"test_pre.Imputation(cat_test,stats=False)","3b7b0617":"test_pre.Imputation(cont_test,stats=False)","7a168e82":"print(data.shape, test.shape)","4f8683d1":"multi_cat_cols = []\n\nfor col in data[cat].columns:\n        if data[col].nunique() > 10: \n            multi_cat_cols.append(col)  \n            print(col)\n            #print(data[cat].groupby(col)[col].count()\/ len(data[cat]))","1dd3d9a8":"train_pre.find_non_rare_labels('Neighborhood', 0.05)","01ce82e3":"for variable in ['Neighborhood', 'Exterior1st', 'Exterior2nd']:\n    data = train_pre.rare_encoding(variable, 0.05)\n    test = test_pre.rare_encoding(variable, 0.05)","be2134c0":"for col in ['Neighborhood', 'Exterior1st', 'Exterior2nd']:\n    temp_df = pd.Series(data[col].value_counts() \/ len(data) )\n\n    # make plot with the above percentages\n    fig = temp_df.sort_values(ascending=False).plot.bar()\n    fig.set_xlabel(col)\n\n    # add a line at 5 % to flag the threshold for rare categories\n    fig.axhline(y=0.05, color='red')\n    fig.set_ylabel('Percentage of houses')\n    plt.show()","7d4b6ced":"print(\"Train Data\")\ncols = ['SalePrice', 'BsmtFinSF1', 'GrLivArea', 'LotArea','1stFlrSF','TotalBsmtSF']\nsns_plot = sns.pairplot(data[cols], size = 2.5);\nsns_plot.savefig(\"output.png\")\nsns_plot","015b9611":"outliers = data[\n    (data['GrLivArea'] > 4000) &\n    (data['SalePrice'] < 300000)&\n    (data['LotArea'] > 100000) &\n    (data['TotalBsmtSF'] > 3000) &\n    (data['1stFlrSF'] > 2500)]\n\ndata.drop(outliers.index, inplace=True)\n","f14da6af":"outliers = test[\n    (test['GrLivArea'] > 4000) &\n    (test['LotArea'] > 100000) &\n    (test['TotalBsmtSF'] > 3000) &\n    (test['1stFlrSF'] > 2500)\n]\n\ntest.drop(outliers.index, inplace=True)\n    \n","8fec6187":"#Functions with Updated data !\ntrain_pre = handle_data(data = data) \ntest_pre = handle_data(data = test)","dc7d67ee":"print(data.shape, test.shape)","58540603":"corr_matrix = data.corr()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr_matrix, vmax=0.9, square=True)","dadedd43":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()","d1b49959":"data[cat] = ordinal_encoder.fit_transform(data[cat])\ntest[cat_test] = ordinal_encoder.fit_transform(test[cat_test])","45a90963":"num, cat, cont, disc, yr = train_pre.extract_var()\nprint(\"Numerical : \"+str(len(num))+\", Categorical \"+\n      str(len(cat)) + \", Continuous: \" + str(len(cont))+ \", Discrete: \" + str(len(disc)))","94370481":"num_test, cat_test, cont_test, disc_test, yr_test = test_pre.extract_var()\nprint(\"Numerical : \"+str(len(num_test))+\", Categorical \"+\n      str(len(cat_test)) + \", Continuous: \" + str(len(cont_test))+ \", Discrete: \" + str(len(disc_test)))","c4631527":"train_pre.check_for_missing(num)","e0a2db00":"test_pre.check_for_missing(num_test)","138795f2":"data.fillna(0, inplace = True)\ntest.fillna(0, inplace = True)","8e167075":"data.head()","53ec3842":"#Functions with Updated data !\ntrain_pre = handle_data(data = data) \ntest_pre = handle_data(data = test)","6d46387b":"print(data.shape, test.shape)","1e1cbd38":"from sklearn.model_selection import train_test_split , KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score","e77f682f":"X_train, X_test, y_train, y_test = train_test_split(data.drop('SalePrice', axis=1), data['SalePrice'], test_size=0.2, random_state=42)","7b3abe03":"print(X_train.shape, y_train.shape, data.shape, test.shape)","86b3678a":"from sklearn.linear_model import LinearRegression\nregr = LinearRegression()\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_test_pred = regr.predict(X_test)\ny_train_pred = regr.predict(X_train)","50e78bba":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))","de626cda":"from sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\ny_train_pred_1 = lasso.predict(X_train)\ny_test_pred_1= lasso.predict(X_test)\n#print(lasso.coef_)","158ea4ce":"print('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred_1),\n        mean_squared_error(y_test, y_test_pred_1)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred_1),\n        r2_score(y_test, y_test_pred_1)))","9c6e4d61":"import xgboost\n\nxgb = xgboost.XGBRegressor(colsample_bytree=0.4,\n                 gamma=0,                 \n                 learning_rate=0.07,\n                 max_depth=3,\n                 min_child_weight=1.5,\n                 n_estimators=10000,                                                                    \n                 reg_alpha=0.75,\n                 reg_lambda=0.45,\n                 subsample=0.6,\n                 seed=42)\nxgb.fit(X_train,y_train)\ny_test_pred_2 = xgb.predict(X_test)\ny_train_pred_2= xgb.predict(X_train)\n","bec5e386":"print('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred_2),\n        mean_squared_error(y_test, y_test_pred_2)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred_2),\n        r2_score(y_test, y_test_pred_2)))","5b9b4439":"y_pred = xgb.predict(test)\nfinal_predictions = y_pred\n\nsubmission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": final_predictions\n    })\n\nsubmission.to_csv(\"xgb.csv\", encoding='utf-8', index=False)\n\nprint(submission.head())","96648ee7":"## (A) Data Loading and Handling section\n---","c341d945":"### (E.3.) XGBoost","0eac0e7f":"### (B.2.) Handling High Cardinality and Rare Labels","2fd3b8c0":"## (C) Heatmap and Correlation","a1fcebc9":"### (B.2) Imputation","22bb36b5":"### (E.1.) Linear Regression","4921c7d1":"# Submission","be9ddea6":"### (E.2.) Lasso Regression ","4b823357":"# Thankyou!\n\nAuthor : [Pratik Kumar](https:\/\/pr2tik1.github.io)","d8dc4d82":"### (D) Encoding the categorical features","740a66f1":"## (B) Feature Engineering section\n---\nWe will be checking for missing values, high cardinality, rare labels and outliers. These are common problems that a dataset has in general.","f7776012":"### (B.1.) Handling Missing Data Columns","9459122e":"# House Price Prediction\n---\n\nWith the goal of predicting house prices this notebook uses sklearn, scipy and visualisation libraries such as matplotlib and plotly. I have used regression models and XG Boost to predict the prices. ","5300620d":"## (E) Machine learning section\n---\n\nWe will be using 3 models : Linear Regression, Lasso Regression and XG Boost to predict the sale price amount of the houses. The problem is classified as regression problem, hence we are using such models. ","8e3738c7":"### (B.3.) Handling Outliers","a63a4566":"We find out that out of the models the XG Boost performs better over our dataset. Hence I am using the model for final predication on test dataset.  "}}