{"cell_type":{"53109f7c":"code","658b87b6":"code","b33a42d4":"code","53289a9e":"code","f0a2503f":"code","f9b1d8d7":"code","dda06ca3":"code","57a5714a":"code","e9b072c3":"code","9fecfcd0":"code","7fff1745":"code","5369219e":"code","2d735aee":"code","6c26f6f1":"code","584bd223":"code","74763260":"code","014e19a2":"code","b0e71be8":"code","4cee54dd":"code","b8e6e166":"code","38e5535e":"code","70b425a8":"code","d8b37e16":"code","36bf368a":"code","2335a688":"markdown","d4093d20":"markdown","610569c1":"markdown","babdc71b":"markdown","ef1e7322":"markdown","a80e2be5":"markdown","8a420aaf":"markdown","167f2bed":"markdown"},"source":{"53109f7c":"import math\nimport pandas as pd\nimport numpy as np\n\nimport lightgbm as lgb\nimport time\nimport datetime\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\npd.set_option(\"display.precision\", 8)\n\nDIR_INPUT = '\/kaggle\/input\/liverpool-ion-switching'","658b87b6":"train_df = pd.read_csv(DIR_INPUT + '\/train.csv')\ntrain_df.shape","b33a42d4":"train_df.head()","53289a9e":"train_df.info()","f0a2503f":"train_df.describe()","f9b1d8d7":"train_df['time'].diff().min(), train_df['time'].diff().max()","dda06ca3":"train_df['open_channels'].value_counts()","57a5714a":"train_df.iloc[0:500000]['open_channels'].value_counts()","e9b072c3":"train_df['signal'].min(), train_df['signal'].max(), train_df['signal'].mean()","9fecfcd0":"fig = go.Figure(data=[\n    go.Scatter(x=train_df.iloc[100000:125000]['time'], y=train_df.iloc[100000:125000]['signal'], name='Signal'),\n])\n\nfig.update_layout(title='Signal (part of batch #0)')\nfig.show()","7fff1745":"batch = train_df.iloc[2200000:2202000]\nbatch.reset_index(drop=True, inplace=True)\n\ndata=[\n    go.Scatter(x=batch.index, y=batch['signal'], name='Signal'),\n]\n\nfor i in range(11):\n    ocx = batch[batch['open_channels'] == i]\n    data.append(go.Scatter(x=ocx.index, y=[i for _ in range(len(ocx))], name='OC: {}'.format(i), marker=dict(size=4), mode=\"markers\"))\n\nfig = go.Figure(data)\n\nfig.update_layout(title='Signal (part of batch #4)')\nfig.show()","5369219e":"fig = go.Figure(data=[\n    go.Bar(x=list(range(11)), y=train_df['open_channels'].value_counts().values)\n])\n\nfig.update_layout(title='Target (open_channels) distribution')\nfig.show()","2d735aee":"fig = make_subplots(rows=3, cols=4,  subplot_titles=[\"Batch #{}\".format(i) for i in range(10)])\ni = 0\nfor row in range(1, 4):\n    for col in range(1, 5):\n        data = train_df.iloc[(i * 500000):((i+1) * 500000 + 1)]['open_channels'].value_counts().values\n        fig.add_trace(go.Bar(x=list(range(11)), y=data), row=row, col=col)\n        \n        i += 1\n\n\nfig.update_layout(title_text=\"Target distribution in different batches\", showlegend=False)\nfig.show()","6c26f6f1":"window_sizes = [10, 50, 100, 1000]\n\nfor window in window_sizes:\n    train_df[\"rolling_mean_\" + str(window)] = train_df['signal'].rolling(window=window).mean()\n    train_df[\"rolling_std_\" + str(window)] = train_df['signal'].rolling(window=window).std()","584bd223":"fig, ax = plt.subplots(len(window_sizes),1,figsize=(20, 6 * len(window_sizes)))\n\nn = 0\nfor col in train_df.columns.values:\n    if \"rolling_\" in col:\n        if \"mean\" in col:\n            mean_df = train_df.iloc[2200000:2210000][col]\n            ax[n].plot(mean_df, label=col, color=\"mediumseagreen\")\n        if \"std\" in col:\n            std = train_df.iloc[2200000:2210000][col].values\n            ax[n].fill_between(mean_df.index.values,\n                               mean_df.values-std, mean_df.values+std,\n                               facecolor='lightgreen',\n                               alpha = 0.5, label=col)\n            ax[n].legend()\n            n+=1","74763260":"train_df = pd.read_csv(DIR_INPUT + '\/train.csv')\ntrain_df.shape","014e19a2":"window_sizes = [50, 100, 1000, 5000, 10000, 25000]\n\nfor window in window_sizes:\n    train_df[\"rolling_mean_\" + str(window)] = train_df['signal'].rolling(window=window).mean()\n    train_df[\"rolling_std_\" + str(window)] = train_df['signal'].rolling(window=window).std()\n    train_df[\"rolling_min_\" + str(window)] = train_df['signal'].rolling(window=window).min()\n    train_df[\"rolling_max_\" + str(window)] = train_df['signal'].rolling(window=window).max()\n    \n    \n    a = (train_df['signal'] - train_df['rolling_min_' + str(window)]) \/ (train_df['rolling_max_' + str(window)] - train_df['rolling_min_' + str(window)])\n    train_df[\"norm_\" + str(window)] = a * (np.floor(train_df['rolling_max_' + str(window)]) - np.ceil(train_df['rolling_min_' + str(window)]))\n    \ntrain_df.fillna(0, inplace=True)\n\ntrain_y = train_df['open_channels']\ntrain_x = train_df.drop(columns=['time', 'open_channels'])\n\ndel train_df","b0e71be8":"scaler = StandardScaler()\nscaler.fit(train_x)\ntrain_x_scaled = pd.DataFrame(scaler.transform(train_x), columns=train_x.columns)\n\ndel train_x","4cee54dd":"test_df = pd.read_csv(DIR_INPUT + '\/test.csv')\ntest_df.drop(columns=['time'], inplace=True)\ntest_df.shape","b8e6e166":"for window in window_sizes:\n    test_df[\"rolling_mean_\" + str(window)] = test_df['signal'].rolling(window=window).mean()\n    test_df[\"rolling_std_\" + str(window)] = test_df['signal'].rolling(window=window).std()\n    test_df[\"rolling_min_\" + str(window)] = test_df['signal'].rolling(window=window).min()\n    test_df[\"rolling_max_\" + str(window)] = test_df['signal'].rolling(window=window).max()\n    \n    \n    a = (test_df['signal'] - test_df['rolling_min_' + str(window)]) \/ (test_df['rolling_max_' + str(window)] - test_df['rolling_min_' + str(window)])\n    test_df[\"norm_\" + str(window)] = a * (np.floor(test_df['rolling_max_' + str(window)]) - np.ceil(test_df['rolling_min_' + str(window)]))\n    \ntest_df.fillna(0, inplace=True)\n","38e5535e":"test_x_scaled = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)\ndel test_df","70b425a8":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\nparams = {'num_leaves': 128,\n          'min_data_in_leaf': 64,\n          'objective': 'huber',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": 0.8,\n          \"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1,\n          'reg_lambda': 0.3\n         }\n","d8b37e16":"oof = np.zeros(len(train_x_scaled))\nprediction = np.zeros(len(test_x_scaled))\nscores = []\n\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(train_x_scaled)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = train_x_scaled.iloc[train_index], train_x_scaled.iloc[valid_index]\n    y_train, y_valid = train_y.iloc[train_index], train_y.iloc[valid_index]\n    \n    model = lgb.LGBMRegressor(**params, n_estimators = 5000, n_jobs = -1)\n    model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n            verbose=500, early_stopping_rounds=200)\n\n    y_pred_valid = model.predict(X_valid)\n    y_pred = model.predict(test_x_scaled, num_iteration=model.best_iteration_)\n\n    oof[valid_index] = y_pred_valid.reshape(-1,)\n    scores.append(mean_absolute_error(y_valid, y_pred_valid))\n\n    prediction += y_pred\n\nprediction \/= n_fold","36bf368a":"sample_df = pd.read_csv(DIR_INPUT + \"\/sample_submission.csv\", dtype={'time':str})\n\nsample_df['open_channels'] = np.round(prediction).astype(np.int)\nsample_df.to_csv(\"submission.csv\", index=False, float_format='%.4f')","2335a688":"## Rolling features","d4093d20":"# Ion Classification Switching\n\n> Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\n>\n> When ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.\n\n\n------\n\n****","610569c1":"# Target distribution","babdc71b":"# Model","ef1e7322":"## Target distribution in different batches","a80e2be5":"# Signal","8a420aaf":"# Train data\n\n> In this competition, you will be predicting the number of open_channels present, based on electrophysiological signal data.\n>\n> **IMPORTANT**: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.\n\n## `time`\nWe have 5M rows in the train dataset. According to the data description we have 50 seconds long 10kHz samples (500,000 rows per batch).\nSo we have 10 batches. The data in a batch is continuous, but discontinuous between batches.\n\n## `signal`\n\n\n## `open_channels`\nPredictions have 11 possible values of open_channels: 0-10.","167f2bed":"# Statistics"}}