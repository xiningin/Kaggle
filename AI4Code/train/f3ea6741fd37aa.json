{"cell_type":{"5ffd3b01":"code","be08ec46":"code","24996185":"code","e56bb5e4":"code","0ca5990b":"code","ae53c165":"code","3ded0c0d":"code","69f521ef":"code","4481e658":"code","1ba10590":"code","03377afd":"code","f8ff5762":"code","ecc4b33a":"code","40ad6260":"code","697bfd4a":"code","7e11f4c6":"code","bce32176":"code","c2dc7baa":"code","2b3d1f04":"code","880f4291":"code","bcef3861":"code","d5c9efe4":"code","eb7e8884":"code","daf409f1":"code","6d60ca58":"code","dbb0074f":"code","db790801":"code","256b57ee":"code","7f5227da":"code","47744827":"code","b3bd90ce":"markdown","9bd98c1e":"markdown","e5e57c0f":"markdown","d0b51641":"markdown","3e316e5c":"markdown","40b023f6":"markdown","5382e3d5":"markdown","21ad2bb5":"markdown","a7619db3":"markdown","a6af3eff":"markdown","ca0f4a62":"markdown","7eb782f1":"markdown","a9bc3ca2":"markdown","37a55dbe":"markdown","042304da":"markdown","f16e96a5":"markdown","e83ce4e6":"markdown","ad2d732a":"markdown","6d5b345a":"markdown","445985e1":"markdown","f39026b1":"markdown","d3687595":"markdown","7598014c":"markdown","6c19342c":"markdown","3c444285":"markdown","f6a73ef1":"markdown","fb027577":"markdown","70360a70":"markdown","7be491fb":"markdown","382bf9ae":"markdown","44c4306f":"markdown","81d7a78c":"markdown","48b0651b":"markdown"},"source":{"5ffd3b01":"from Bio import SeqIO\nfor sequence in SeqIO.parse('..\/input\/dna-sequence-dataset\/example_dna.fa', \"fasta\"):\n    print(sequence.id)\n    print(sequence.seq)\n    print(len(sequence))","be08ec46":"import numpy as np\nimport re\ndef string_to_array(seq_string):\n   seq_string = seq_string.lower()\n   seq_string = re.sub('[^acgt]', 'n', seq_string)\n   seq_string = np.array(list(seq_string))\n   return seq_string\n# create a label encoder with 'acgtn' alphabet\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(np.array(['a','c','g','t','z']))","24996185":"def ordinal_encoder(my_array):\n    integer_encoded = label_encoder.transform(my_array)\n    float_encoded = integer_encoded.astype(float)\n    float_encoded[float_encoded == 0] = 0.25 # A\n    float_encoded[float_encoded == 1] = 0.50 # C\n    float_encoded[float_encoded == 2] = 0.75 # G\n    float_encoded[float_encoded == 3] = 1.00 # T\n    float_encoded[float_encoded == 4] = 0.00 # anything else, lets say n\n    return float_encoded\n\n\n#Let\u2019s try it out a simple short sequence:\nseq_test = 'TTCAGCCAGTG'\nordinal_encoder(string_to_array(seq_test))","e56bb5e4":"from sklearn.preprocessing import OneHotEncoder\ndef one_hot_encoder(seq_string):\n    int_encoded = label_encoder.transform(seq_string)\n    onehot_encoder = OneHotEncoder(sparse=False, dtype=int)\n    int_encoded = int_encoded.reshape(len(int_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(int_encoded)\n    onehot_encoded = np.delete(onehot_encoded, -1, 1)\n    return onehot_encoded\n\n\n#So let\u2019s try it out with a simple short sequence:\nseq_test = 'GAATTCTCGAA'\none_hot_encoder(string_to_array(seq_test))","0ca5990b":"def Kmers_funct(seq, size):\n    return [seq[x:x+size].lower() for x in range(len(seq) - size + 1)]\n\n\n#So let\u2019s try it out with a simple sequence:\nmySeq = 'GTGCCCAGGTTCAGTGAGTGACACAGGCAG'\nKmers_funct(mySeq, size=7)","ae53c165":"words = Kmers_funct(mySeq, size=6)\njoined_sentence = ' '.join(words)\njoined_sentence","3ded0c0d":"mySeq1 = 'TCTCACACATGTGCCAATCACTGTCACCC'\nmySeq2 = 'GTGCCCAGGTTCAGTGAGTGACACAGGCAG'\nsentence1 = ' '.join(Kmers_funct(mySeq1, size=6))\nsentence2 = ' '.join(Kmers_funct(mySeq2, size=6))","69f521ef":"#Creating the Bag of Words model:\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform([joined_sentence, sentence1, sentence2]).toarray()\nX","4481e658":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1ba10590":"human_dna = pd.read_table('..\/input\/dna-sequence-dataset\/human.txt')\nhuman_dna.head()","03377afd":"human_dna['class'].value_counts().sort_index().plot.bar()\nplt.title(\"Class distribution of Human DNA\")","f8ff5762":"chimp_dna = pd.read_table('..\/input\/dna-sequence-dataset\/chimpanzee.txt')\nchimp_dna.head()","ecc4b33a":"chimp_dna['class'].value_counts().sort_index().plot.bar()\nplt.title(\"Class distribution of Chimpanzee DNA\")","40ad6260":"dog_dna = pd.read_table('..\/input\/dna-sequence-dataset\/dog.txt')\ndog_dna.head()","697bfd4a":"dog_dna['class'].value_counts().sort_index().plot.bar()\nplt.title(\"Class distribution of Dog DNA\")","7e11f4c6":"def Kmers_funct(seq, size=6):\n    return [seq[x:x+size].lower() for x in range(len(seq) - size + 1)]\n\n#convert our training data sequences into short overlapping k-mers of length 6. \n#Lets do that for each species of data we have using our Kmers_funct function.\n\nhuman_dna['words'] = human_dna.apply(lambda x: Kmers_funct(x['sequence']), axis=1)\nhuman_dna = human_dna.drop('sequence', axis=1)\n\nchimp_dna['words'] = chimp_dna.apply(lambda x: Kmers_funct(x['sequence']), axis=1)\nchimp_dna = chimp_dna.drop('sequence', axis=1)\n\ndog_dna['words'] = dog_dna.apply(lambda x: Kmers_funct(x['sequence']), axis=1)\ndog_dna = dog_dna.drop('sequence', axis=1)","bce32176":"human_dna.head()","c2dc7baa":"human_texts = list(human_dna['words'])\nfor item in range(len(human_texts)):\n    human_texts[item] = ' '.join(human_texts[item])\n#separate labels\ny_human = human_dna.iloc[:, 0].values # y_human for human_dna","2b3d1f04":"chimp_texts = list(chimp_dna['words'])\nfor item in range(len(chimp_texts)):\n    chimp_texts[item] = ' '.join(chimp_texts[item])\n#separate labels\ny_chim = chimp_dna.iloc[:, 0].values # y_chim for chimp_dna\n\ndog_texts = list(dog_dna['words'])\nfor item in range(len(dog_texts)):\n    dog_texts[item] = ' '.join(dog_texts[item])\n#separate labels\ny_dog = dog_dna.iloc[:, 0].values  # y_dog for dog_dna","880f4291":"y_human","bcef3861":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(ngram_range=(4,4)) #The n-gram size of 4 is previously determined by testing\nX = cv.fit_transform(human_texts)\nX_chimp = cv.transform(chimp_texts)\nX_dog = cv.transform(dog_texts)","d5c9efe4":"print(X.shape)\nprint(X_chimp.shape)\nprint(X_dog.shape)","eb7e8884":"# Splitting the human dataset into the training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y_human, \n                                                    test_size = 0.20, \n                                                    random_state=42)","daf409f1":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB(alpha=0.1)\nclassifier.fit(X_train, y_train)","6d60ca58":"y_pred = classifier.predict(X_test)","dbb0074f":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nprint(\"Confusion matrix for predictions on human test DNA sequence\\n\")\nprint(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))\ndef get_metrics(y_test, y_predicted):\n    accuracy = accuracy_score(y_test, y_predicted)\n    precision = precision_score(y_test, y_predicted, average='weighted')\n    recall = recall_score(y_test, y_predicted, average='weighted')\n    f1 = f1_score(y_test, y_predicted, average='weighted')\n    return accuracy, precision, recall, f1\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\" % (accuracy, precision, recall, f1))\n","db790801":"# Predicting the chimp, dog sequences\ny_pred_chimp = classifier.predict(X_chimp)","256b57ee":"# performance on chimpanzee genes\nprint(\"Confusion matrix for predictions on Chimpanzee test DNA sequence\\n\")\nprint(pd.crosstab(pd.Series(y_chim, name='Actual'), pd.Series(y_pred_chimp, name='Predicted')))\naccuracy, precision, recall, f1 = get_metrics(y_chim, y_pred_chimp)\nprint(\"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\" % (accuracy, precision, recall, f1))","7f5227da":"y_pred_dog = classifier.predict(X_dog)","47744827":"# performance on dog genes\nprint(\"Confusion matrix for predictions on Dog test DNA sequence\\n\")\nprint(pd.crosstab(pd.Series(y_dog, name='Actual'), pd.Series(y_pred_dog, name='Predicted')))\naccuracy, precision, recall, f1 = get_metrics(y_dog, y_pred_dog)\nprint(\"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\" % (accuracy, precision, recall, f1))","b3bd90ce":"Now for the real test. Let's see how our model perfoms on the DNA sequences from other species. First we'll try the Chimpanzee, which we would expect to be very similar to human. Then we will try man's (and woman's) best friend, the Dog DNA sequences.","9bd98c1e":"#Now let's make predictions on the human hold out test set and see how it performes on unseen data.","e5e57c0f":"## Now that we can load and manipulate biological sequence data easily, how can we use it for machine learning or deep learning?\n\nNow since machine learning or deep learning models require input to be feature matrices or numerical values and currently we still have our data in character or string format. So the next step is to encode these characters into matrices.\n\n\nThere are 3 general approaches to encode sequence data:\n\n1. Ordinal encoding DNA Sequence\n\n2. One-hot encoding DNA Sequence\n\n3. DNA sequence as a \u201clanguage\u201d, known as k-mer counting\n\nSo let us implement each of them and see which gives us the perfect input features.","d0b51641":" Here comes machine learning\u2026\n\n\nNow that we have learned how to extract feature matrix from the DNA sequence, let us apply our newly acquired knowledge to a real-life machine learning use case.","3e316e5c":"We will create a multinomial naive Bayes classifier. I previously did some parameter tuning and found the ngram size of 4 (reflected in the Countvectorizer() instance) and a model alpha of 0.1 did the best. Just to keep it simple I won't show that code here.","40b023f6":"Let us now do predictions on Dog test DNA sequence.","5382e3d5":"Here are the definitions for each of the 7 classes and how many there are in the human training data:\n![3eee0b_06abe8649116492ea56ad4a64017260c_mv2.webp](attachment:3eee0b_06abe8649116492ea56ad4a64017260c_mv2.webp)","21ad2bb5":"Creating the Bag of Words model using CountVectorizer(). This is equivalent to k-mer counting. The n-gram size of 4 was previously determined by testing.\n\nConvert our k-mer words into uniform length numerical vectors that represent counts for every k-mer in the vocabulary:","a7619db3":"Okay, so let's look at some model performce metrics like the confusion matrix, accuracy, precision, recall and f1 score. We are getting really good results on our unseen data, so it looks like our model did not overfit to the training data. In a real project I would go back and sample many more train test splits since we have a relatively small data set.","a6af3eff":"We need to now convert the lists of k-mers for each gene into string sentences of words that can be used to create the Bag of Words model. We will make a target variable y to hold the class labels.","ca0f4a62":"![3eee0b_61e8ff912b9d4daf8381ccc8101ec088_mv2.webp](attachment:3eee0b_61e8ff912b9d4daf8381ccc8101ec088_mv2.webp)\n\n\nThe order, or sequence, of these bases, determines what biological instructions are contained in a strand of DNA. For example, the sequence ATCGTT might instruct for blue eyes, while ATCGCT might instruct for brown.\n\n","7eb782f1":"## DNA data handling using Biopython\n\nHere is a brief example of how to work with a DNA sequence in fasta format using Biopython. The sequence object will contain attributes such as id and sequence and the length of the sequence that you can work with directly.\n\n\nWe will use Bio.SeqIO from Biopython for parsing DNA sequence data(fasta). It provides a simple uniform interface to input and output assorted sequence file formats.","a9bc3ca2":"Now we have all our data loaded, the next step is to convert a sequence of characters into k-mer words, default **size = 6 (hexamers)**. The function **Kmers_funct()** will collect all possible overlapping k-mers of a specified length from any sequence string.","37a55dbe":"### Load Chimpanzee DNA data","042304da":"So it produces the sequence ID, sequence and length of the sequence.\n\n","f16e96a5":"## Ordinal encoding DNA sequence data\n\nIn this approach, we need to encode each nitrogen bases as an ordinal value. For example \u201cATGC\u201d becomes [0.25, 0.5, 0.75, 1.0]. Any other base such as \u201cN\u201d can be a 0.\n\n\nSo let us create functions such as for creating a NumPy array object from a sequence string, and a label encoder with the DNA sequence alphabet \u201ca\u201d, \u201cc\u201d, \u201cg\u201d and \u201ct\u201d, but also a character for anything else, \u201cn\u201d.","e83ce4e6":"So the target variable contains an array of class values.","ad2d732a":"![3eee0b_a88b8c2ed9bd481a80e2695edcb3cb9a_mv2.webp](attachment:3eee0b_a88b8c2ed9bd481a80e2695edcb3cb9a_mv2.webp)","6d5b345a":"You can tune both the word length and the amount of overlap. This allows you to determine how the DNA sequence information and vocabulary size will be important in your application. For example, if you use words of length 6, and there are 4 letters, you have a vocabulary of size 4096 possible words. You can then go on and create a bag-of-words model like you would in NLP.\n\n\nLet\u2019s make a couple more \u201csentences\u201d to make it more interesting.","445985e1":"And here is a function to encode a DNA sequence string as an ordinal vector. It returns a NumPy array with A=0.25, C=0.50, G=0.75, T=1.00, n=0.00.","f39026b1":"It returns a list of k-mer \u201cwords.\u201d You can then join the \u201cwords\u201d into a \u201csentence\u201d, then apply your favorite natural language processing methods on the \u201csentences\u201d as you normally would.","d3687595":"## Introduction\n\nA genome is a complete collection of DNA in an organism. All living species possess a genome, but they differ considerably in size. The human genome, for instance, is arranged into 23 chromosomes, which is a little bit like an encyclopedia being organized into 23 volumes. And if you counted all the characters (individual DNA \u201cbase pairs\u201d), there would be more than 6 billion in each human genome. So it\u2019s a huge compilation.\n\n\nA human genome has about 6 billion characters or letters. If you think the genome(the complete DNA sequence) is like a book, it is a book about 6 billion letters of \u201cA\u201d, \u201cC\u201d, \u201cG\u201d and \u201cT\u201d. Now, everyone has a unique genome. Nevertheless, scientists find most parts of the human genomes are alike to each other.\n\n\nAs a data-driven science, genomics extensively utilizes machine learning to capture dependencies in data and infer new biological hypotheses. Nonetheless, the ability to extract new insights from the exponentially increasing volume of genomics data requires more powerful machine learning models. By efficiently leveraging large data sets, deep learning has reconstructed fields such as computer vision and natural language processing. It has become the method of preference for many genomics modeling tasks, including predicting the influence of genetic variation on gene regulatory mechanisms such as DNA receptiveness and splicing.\n\n\nSo in this article, we will understand how to interpret a DNA structure and how machine learning algorithms can be used to build a prediction model on DNA sequence data.\n\n\n## How is a DNA Sequence represented?\n\nThe diagram shows a tiny bit of a DNA double helix structure.\n![3eee0b_4f3f6d138e764ea8a9860381c4e52c01_mv2%20%281%29.gif](attachment:3eee0b_4f3f6d138e764ea8a9860381c4e52c01_mv2%20%281%29.gif)","7598014c":"## DNA sequence as a \u201clanguage\u201d, known as k-mer counting\n\nA hurdle that still remains is that none of these above methods results in vectors of uniform length, and that is a necessity for feeding data to a classification or regression algorithm. So with the above methods, you have to resort to things like truncating sequences or padding with \u201cn\u201d or \u201c0\u201d to get vectors of uniform length.\n\n\nDNA and protein sequences can be seen as the language of life. The language encodes instructions as well as functions for the molecules that are found in all life forms. The sequence language resemblance continues with the genome as the book, subsequences (genes and gene families) are sentences and chapters, k-mers and peptides are words, and nucleotide bases and amino acids are the alphabets. Since the relationship seems so likely, it stands to reason that the natural language processing(NLP) should also implement the natural language of DNA and protein sequences.\n\n\nThe method we use here is manageable and easy. We first take the long biological sequence and break it down into k-mer length overlapping \u201cwords\u201d. For example, if we use \u201cwords\u201d of length 6 (hexamers), \u201cATGCATGCA\u201d becomes: \u2018ATGCAT\u2019, \u2018TGCATG\u2019, \u2018GCATGC\u2019, \u2018CATGCA\u2019. Hence our example sequence is broken down into 4 hexamer words.\n\n\nIn genomics, we refer to these types of manipulations as \u201ck-mer counting\u201d, or counting the occurrences of each possible k-mer sequence and Python natural language processing tools make it super easy.","6c19342c":"So, for humans we have **4380** genes converted into uniform length feature vectors of 4-gram k-mer (length 6) counts. For chimp and dog, we have the same number of features with **1682** and **820** genes respectively.\n\n\nSo now that we know how to transform our DNA sequences into uniform length numerical vectors in the form of k-mer counts and ngrams, we can now go ahead and build a classification model that can predict the DNA sequence function based only on the sequence itself.\n\n\nHere I will use the human data to train the model, holding out 20% of the human data to test the model. Then we can challenge the model\u2019s generalizability by trying to predict sequence function in other species (the chimpanzee and dog).\n\n\nNext, train\/test split human dataset and build simple multinomial naive Bayes classifier.\n\n\nYou might want to do some parameter tuning and build a model with different ngram sizes, here I\u2019ll go ahead with an ngram size of 4 and a model alpha of 0.1.","3c444285":"### Load human DNA data","f6a73ef1":"The double-helix is the correct chemical representation of DNA. But DNA is special. It\u2019s a nucleotide made of four types of nitrogen bases: Adenine (A), Thymine (T), Guanine (G), and Cytosine. We always call them A, C, Gand T.\n\nThese four chemicals link together via hydrogen bonds in any possible order making a chain, and this gives one thread of the DNA double-helix. And the second thread of the double-helix balance the first. So if you have A on the first thread, you have to have T on the second. \n\nFurthermore, C and G always balance each other. So once you identify one thread of the helix, you can always spell the other.","fb027577":"## One-hot encoding DNA Sequence\n\nAnother approach is to use one-hot encoding to represent the DNA sequence. This is widely used in deep learning methods and lends itself well to algorithms like convolutional neural networks. In this example, \u201cATGC\u201d would become [0,0,0,1], [0,0,1,0], [0,1,0,0], [1,0,0,0]. And these one-hot encoded vectors can either be concatenated or turned into 2-dimensional arrays.","70360a70":"**Objective:** Build a classification model that is trained on the human DNA sequence and can predict a gene family based on the DNA sequence of the coding sequence. To test the model, we will use the DNA sequence of humans, dogs, and chimpanzees and compare the accuracies.\n\nGene families are groups of related genes that share a common ancestor. Members of gene families may be paralogs or orthologs. Gene paralogs are genes with similar sequences from within the same species while gene orthologs are genes with similar sequences in different species.\n\nThe dataset contains human DNA sequence, Dog DNA sequence, and Chimpanzee DNA sequence.","7be491fb":"The model seems to produce good results on human data. It also does on Chimpanzee which is because the Chimpanzee and humans share the same genetic hierarchy. The performance of the dog is not quite as good which is because the dog is more diverging from humans than the chimpanzee.","382bf9ae":"The DNA sequence is changed to lowercase, divided into all possible k-mer words of length 6, and ready for the next step.","44c4306f":"### Load Dog DNA data","81d7a78c":"Now let's do the same for chimp and dog.","48b0651b":"# Demystify DNA Sequencing with Machine Learning"}}