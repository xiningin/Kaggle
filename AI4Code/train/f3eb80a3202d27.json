{"cell_type":{"e190cc9a":"code","0bb3d6cc":"code","4a8a6888":"code","f10082c6":"code","0ed71256":"code","fcbb99e9":"code","18399eeb":"code","61e6ff7a":"markdown","7fb57d02":"markdown","2a5ce9a7":"markdown","626b41e5":"markdown","d627c0b2":"markdown","eec811d8":"markdown","a7e21e90":"markdown","501b6d32":"markdown","9c4ae2c4":"markdown","2d83af5e":"markdown","a6326ee4":"markdown","33bcb21e":"markdown","a40e73ae":"markdown","b77228dc":"markdown","1e9ef431":"markdown","8afb68f6":"markdown","e36c4a14":"markdown"},"source":{"e190cc9a":"from statistics import mean\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold","0bb3d6cc":"# training_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\n\n# train_y = training_data['label']\n# train_X = training_data.drop('label',axis=1)\n\n# data_amounts = [1000,5000,10000,15000,20000]\n\n# accs = list()\n\n# # data_amounts = [300,301]\n\n# plt.figure(figsize=(12,6))\n\n# for data_amt in data_amounts:\n\n#     # We use default parameters of SVM\n#     svc = SVC(gamma='scale',kernel='rbf',C=1)\n\n#     cv_results = cross_validate(svc, train_X[:data_amt], train_y[:data_amt], cv=2)\n\n# #     print(f\"Validation acc for each fold: {cv_results['test_score']}\")\n#     print(f\"Mean acc:  {mean(cv_results['test_score'])}\")\n\n#     accs.append(mean(cv_results['test_score']))\n\n\n# plt.legend()\n# plt.title('Accuracy by amount of data')\n# plt.xlabel('Amount of data')\n# plt.ylabel('Accuracy')\n\n# plt.plot(data_amounts, accs, '-o')\n\n# plt.tight_layout()\n# plt.show()\n\n# # plt.savefig(\"result.png\")\n\n","4a8a6888":"# from sklearn.model_selection import GridSearchCV\n# import warnings\n\n# warnings.filterwarnings(\"ignore\")\n\n\n# data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')[:14000]\n\n# y = data['label']\n\n# X = data.drop('label',axis=1)\n\n\n# params = {\n# \t\t  'C': [0.01,0.1,1,10,50,100],\n# \t\t  'kernel': ['linear','poly','rbf','sigmoid'],\n# \t\t  'gamma':['scale']\n# \t\t }\n\n# svc = SVC(**params, probability=True)\n\n# cv = KFold(n_splits=2)\n\n# clf = GridSearchCV(svc, param_grid=params, scoring='neg_log_loss', n_jobs=-1, cv=cv, verbose=10)\n# clf.fit(X, y)\n\n# print(clf.cv_results_['mean_test_score'])\n\n# C = [clf.cv_results_['params'][i]['C'] for i in range(len(clf.cv_results_['params']))]\n# kernel = [clf.cv_results_['params'][i]['kernel'] for i in range(len(clf.cv_results_['params']))]\n\n# score = clf.cv_results_['mean_test_score']\n\n# results = pd.DataFrame({'C':C,'kernel':kernel,'score':score})\n\n\n# results = pd.pivot_table(results, values='score', \n#                      index=['kernel'], \n#                      columns='C')\n\n\n# # Visualize results by heat map\n# import seaborn as sns\n\n# fig, ax = plt.subplots(figsize=(10,5))\n# ax = sns.heatmap(results, annot=True)\n\n# plt.savefig('svc_param_tuning.png')","f10082c6":"training_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\n\ntrain_y = training_data['label']\ntrain_X = training_data.drop('label',axis=1)\n\ntest_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\n\ntest_y = test_data['label']\ntest_X = test_data.drop('label',axis=1)\n\n\nsvc = SVC(gamma='scale',kernel='rbf',C=8)\n\ncv_results = cross_validate(svc, train_X, train_y, cv=3)\n\nprint(f\"Validation acc for each fold: {cv_results['test_score']}\")\nprint(f\"Mean acc:  {mean(cv_results['test_score'])}\")","0ed71256":"training_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\n\ntrain_y = training_data['label']\ntrain_X = training_data.drop('label',axis=1)\n\nsvc = SVC(gamma='scale',kernel='rbf',C=8)\n\ncv_results = cross_validate(svc, train_X\/255, train_y, cv=3)\n\nprint(f\"Validation acc for each fold: {cv_results['test_score']}\")\nprint(f\"Mean acc:  {mean(cv_results['test_score'])}\")\n\n\n","fcbb99e9":"from sklearn.decomposition import PCA\n\ntraining_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\n\ntrain_y = training_data['label']\ntrain_X = training_data.drop('label',axis=1)\n\npca = PCA().fit(train_X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\n","18399eeb":"from sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\n\ntraining_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\n\ntrain_y = training_data['label']\ntrain_X = training_data.drop('label',axis=1)\n\ntest_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\n\ntest_y = test_data['label']\ntest_X = test_data.drop('label',axis=1)\n\n\n# Cross validation ----------------------------------------------------------\nfolds = KFold(n_splits=3)\n\nacc = list()\n\n#run model\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X,train_y)):\n\tstrLog = \"fold {}\".format(fold_)\n\tprint(strLog)\n\n\tX_tr, X_val = train_X.iloc[trn_idx], train_X.iloc[val_idx]\n\ty_tr, y_val = train_y.iloc[trn_idx], train_y.iloc[val_idx]\n\n\tpca = PCA(n_components=400)\n\tpca.fit(X_tr)\n\tpca_tr_X = pca.transform(X_tr)\n\tpca_val_X = pca.transform(X_val)\n\n\tsvc = SVC(gamma='scale',kernel='rbf',C=8)\n\n\tsvc.fit(pca_tr_X,y_tr)\n\tpreds = svc.predict(pca_val_X)\n\n\tacc.append(accuracy_score(preds,y_val))\n\nprint(f\"Validation acc for each fold: {acc}\")\nprint(f\"Mean acc:  {mean(acc)}\")\n\npca = PCA(n_components=400)\npca.fit(train_X)\npca_train_X = pca.transform(train_X)\npca_test_X = pca.transform(test_X)\n\nsvc = SVC(gamma='scale',kernel='rbf',C=8)\n\nsvc.fit(train_X,train_y)\n\npreds = svc.predict(test_X)\n\nprint(f\"Test acc {accuracy_score(preds,test_y)}\")\n\n","61e6ff7a":"## Scaling SVM ##","7fb57d02":"![](https:\/\/imgur.com\/0iNNuMn.png)","2a5ce9a7":"1. ## Train SVM with found parameters#","626b41e5":"Since SVM is affected distance, we will scale our data.","d627c0b2":"## Parameter tuning ##","eec811d8":"## SVM with PCA ##","a7e21e90":"It seems that the accuracy start to saturate around 15000 when 2 splits, so we will only use 14000 data with 2 splits to search parameters. We also fix gamma='scale' because gamma='auto' consistently gave bad accuracies. In this way, we can speed up the process even more.\n\nThere is concern that combinations of parameters differ between the amount of data; however, SVM will less likley to fall into this given that its parameter range is narrower and kernel function and C is not affected by amount of data after some points unlike GBDT.","501b6d32":"![](https:\/\/imgur.com\/KeRkTrP.png)","9c4ae2c4":"How much score can SVM get? Let's find this out.","2d83af5e":"It seems that kernel='rbf' and C='10' produces the smallest log loss. I further narrowed down the value of C. We will use C=8 as the parameter to train our SVM.","a6326ee4":"We will use 400 components because this point holds about 96% of data variance while reducing dimensions by two times. We have to make sure to prevent data leakage when we do cross validation.","33bcb21e":"### Cumulative variance by number of components###","a40e73ae":"### Divide images by 255###","b77228dc":"# Fashion MNIST with Support Vector Machine #","1e9ef431":"Let's see if SVM can predict targets better with reduced dimensions. First, we will decide how many components we will use for PCA.","8afb68f6":"Given large data, we can't use all data to tune hyperparameters; thus, we will decide how many data to use by finding the amount of point which accuracy will start to saturate.","e36c4a14":"## Import libraries ##"}}