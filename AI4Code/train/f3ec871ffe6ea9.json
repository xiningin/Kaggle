{"cell_type":{"59c01ff4":"code","91935192":"code","53377249":"code","5e2df422":"code","a6ec27ad":"code","61661b4e":"code","a253839f":"code","b1973b24":"code","7f745895":"code","ce66ba9f":"code","1c880721":"markdown","73a5e20b":"markdown","d7ccb6da":"markdown","b84ed4d6":"markdown","f146103f":"markdown","a09cb2ab":"markdown","31e5fbc1":"markdown","1e46da24":"markdown","f8499aeb":"markdown","4665d511":"markdown","027ca4b3":"markdown"},"source":{"59c01ff4":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import RFE\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nimport category_encoders as ce\nimport warnings as wr\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import Isomap\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.manifold import LocallyLinearEmbedding\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import RFECV\nwr.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n# Get the dataset\nfile_name_test = \"test.csv\"\nfile_name_train = \"train.csv\"\ndatasetTest = pd.read_csv(file_name_test)\ndatasetTrain = pd.read_csv(file_name_train)\n\n# Prepare and split dataset\nidArray = datasetTest[\"Id\"]\ndel datasetTrain[\"Id\"]\ndel datasetTest[\"Id\"]","91935192":"# Remove useles features\n\ndel datasetTrain[\"GrLivArea\"]\ndel datasetTest[\"GrLivArea\"]\n\ndel datasetTrain[\"KitchenAbvGr\"]\ndel datasetTest[\"KitchenAbvGr\"]\n\ndel datasetTrain[\"EnclosedPorch\"]\ndel datasetTest[\"EnclosedPorch\"]\n\ndel datasetTrain[\"BsmtHalfBath\"]\ndel datasetTest[\"BsmtHalfBath\"]\n\ndel datasetTrain[\"LowQualFinSF\"]\ndel datasetTest[\"LowQualFinSF\"]\n\ndel datasetTrain[\"OverallCond\"]\ndel datasetTest[\"OverallCond\"]\n\ndel datasetTrain[\"MSSubClass\"]\ndel datasetTest[\"MSSubClass\"]\n\ndel datasetTrain[\"LotShape\"]\ndel datasetTest[\"LotShape\"]\n\ndel datasetTrain[\"LandContour\"]\ndel datasetTest[\"LandContour\"]\n\ndel datasetTrain[\"Fence\"]\ndel datasetTest[\"Fence\"]\n\ndel datasetTrain[\"MiscFeature\"]\ndel datasetTest[\"MiscFeature\"]\n\ndel datasetTrain[\"MiscVal\"]\ndel datasetTest[\"MiscVal\"]\n\ndel datasetTrain[\"RoofMatl\"]\ndel datasetTest[\"RoofMatl\"]\n\ndel datasetTrain[\"Exterior2nd\"]\ndel datasetTest[\"Exterior2nd\"]\n\ndel datasetTrain[\"MoSold\"]\ndel datasetTest[\"MoSold\"]\n\ndel datasetTest['LotFrontage']\ndel datasetTrain['LotFrontage']\n\ndel datasetTest['LotArea']\ndel datasetTrain['LotArea']\n\ndel datasetTest['Alley']\ndel datasetTrain['Alley']\n\nX, y = datasetTrain.iloc[:,:-1], datasetTrain.iloc[:,-1:]","53377249":"# Fill in missing values\n# Manual inputs\n\n# MasVnr\nX.loc[X['MasVnrType'].isnull(),'MasVnrType'] = 'None'\ndatasetTest.loc[datasetTest['MasVnrType'].isnull(),'MasVnrType'] = 'None'\n\nX.loc[X['MasVnrArea'].isnull(),'MasVnrArea'] = 0.0\ndatasetTest.loc[datasetTest['MasVnrArea'].isnull(),'MasVnrArea'] = 0.0\n\n# Basment\nbasement_cols=['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtFinSF1','BsmtFinSF2']\nfor cols in basement_cols:\n    if 'FinSF'not in cols:\n        datasetTest.loc[datasetTest[cols].isnull(),cols] = 'None'\n        X.loc[X[cols].isnull(),cols] = 'None'\n\n# Electrical\ndatasetTest.loc[datasetTest['Electrical'].isnull(),'Electrical'] = 'SBrkr'\nX.loc[X['Electrical'].isnull(),'Electrical'] = 'SBrkr'\n\n# Fireplaces\ndatasetTest.loc[datasetTest['FireplaceQu'].isnull(),'FireplaceQu'] = 'None'\nX.loc[X['FireplaceQu'].isnull(),'FireplaceQu'] = 'None'\n\n# Garage\ngarage_cols=['GarageType','GarageQual','GarageCond','GarageYrBlt','GarageFinish','GarageCars','GarageArea']\nfor cols in garage_cols:\n    if X[cols].dtype==np.object:\n        datasetTest.loc[datasetTest[cols].isnull(),cols] = 'None'\n        X.loc[X[cols].isnull(),cols] = 'None'\n    else:\n        datasetTest.loc[datasetTest[cols].isnull(),cols] = 0\n        X.loc[X[cols].isnull(),cols] = 0\n\n# Pool\ndatasetTest.loc[datasetTest['PoolQC'].isnull(),'PoolQC'] = 'None'\nX.loc[X['PoolQC'].isnull(),'PoolQC'] = 'None'\n\n# Automatic inputs\nprint('Missing: %d' % sum(pd.isnull(X.values).flatten()))\nprint('Missing: %d' % sum(pd.isnull(datasetTest.values).flatten()))\nnum_cols = X._get_numeric_data().columns\ncat_cols = list(set(X) - set(num_cols))\nimputerCat = SimpleImputer(strategy='most_frequent')\nimputerCat = imputerCat.fit(X[cat_cols])\nX[cat_cols] = imputerCat.transform(X[cat_cols])\ndatasetTest[cat_cols] = imputerCat.transform(datasetTest[cat_cols])\n\nimputerNum = SimpleImputer(strategy='mean')\nimputerNum = imputerNum.fit(X[num_cols])\nX[num_cols] = imputerNum.transform(X[num_cols])\ndatasetTest[num_cols] = imputerNum.transform(datasetTest[num_cols])\nprint('Missing: %d' % sum(pd.isnull(X.values).flatten()))\nprint('Missing: %d' % sum(pd.isnull(datasetTest.values).flatten()))\n","5e2df422":"# Convert categorical to numerical values\nnum_cols = X._get_numeric_data().columns\ncat_cols = list(set(X) - set(num_cols))\n\nencoder = ce.count.CountEncoder(verbose=False,drop_invariant=True,normalize=False)\nencoder = encoder.fit(X[cat_cols])\n\nX_cod = encoder.transform(X[cat_cols])\nX_cat = pd.DataFrame(X_cod)\nX_num = pd.DataFrame(X[num_cols])\nX = X_cat.join(X_num)\n\ndatasetTest_cod = encoder.transform(datasetTest[cat_cols])\ndatasetTest_cat = pd.DataFrame(datasetTest_cod)\ndatasetTest_num = pd.DataFrame(datasetTest[num_cols])\ndatasetTest = datasetTest_cat.join(datasetTest_num)","a6ec27ad":"# Normalize and standardize features\nX_col = X.columns\nnormalizer = preprocessing.Normalizer().fit(X)\n\nstandard = preprocessing.StandardScaler().fit(X)\n\nX_norm = normalizer.transform(X)\nX = pd.DataFrame(X_norm)\nX_norm = standard.transform(X)\nX = pd.DataFrame(X_norm, columns=X_col)\n\ndatasetTest_norm = normalizer.transform(datasetTest)\ndatasetTest = pd.DataFrame(datasetTest_norm)\ndatasetTest_norm = standard.transform(datasetTest)\ndatasetTest = pd.DataFrame(datasetTest_norm, columns=X_col)","61661b4e":"# Reduce only related data\ngarage_features = ['GarageCond','GarageType','GarageFinish','GarageQual','GarageYrBlt',\n    'GarageCars', 'GarageArea']\n\n# Garage\ngarage_som = PCA(n_components=1,whiten=True,svd_solver='auto')\ngarage_som = garage_som.fit(X[garage_features])\ngarage_X = pd.DataFrame(garage_som.transform(X[garage_features]))\n\ngarage_test = pd.DataFrame(garage_som.transform(datasetTest[garage_features]))\n\nX['garage'] = garage_X\nfor feat in garage_features:\n    del X[feat]\n\ndatasetTest['garage'] = garage_test\nfor feat in garage_features:\n    del datasetTest[feat]\n\n# General\nsvd = LinearDiscriminantAnalysis(n_components=25,solver='eigen',store_covariance=False)\n\nsvd = svd.fit(X,y)\nX_trans = svd.transform(X)\ndatasetTest_trans = svd.transform(datasetTest)\n\nX = pd.DataFrame(X_trans)\ndatasetTest = pd.DataFrame(datasetTest_trans)\n\n# Feature selection (elasticNet or Lasso)\nrfe = RFECV(estimator=Lasso(), min_features_to_select=10)\nrfe = rfe.fit(X,y)\n\nX_selected = rfe.transform(X)\nX = pd.DataFrame(X_selected)\n\ndatasetTest_selected = rfe.transform(datasetTest)\ndatasetTest = pd.DataFrame(datasetTest_selected)\nprint(datasetTest.shape)\nprint(X.shape)","a253839f":"# Save dataset\ndatasetTest[\"Id\"] = idArray\nX[\"SalePrice\"] = y\n\nX = X.drop_duplicates()\n\ndatasetTest.to_csv('preparedTest.csv', index = False)\nX.to_csv('procesedDataset.csv', index = False)","b1973b24":"# regression spot check script\nimport warnings\nfrom numpy import mean\nimport numpy as np\nimport pandas as pd\nfrom numpy import std\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.linear_model import Lars\nfrom sklearn.linear_model import LassoLars\nfrom sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import TheilSenRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import ExtraTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\n\ndef get_models(models=dict()):\n\t# linear models\n\tmodels['lr'] = LinearRegression()\n\talpha = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\tfor a in alpha:\n\t\tmodels['lasso-'+str(a)] = Lasso(alpha=a)\n\tfor a in alpha:\n\t\tmodels['ridge-'+str(a)] = Ridge(alpha=a)\n\tfor a1 in alpha:\n\t\tfor a2 in alpha:\n\t\t\tname = 'en-' + str(a1) + '-' + str(a2)\n\t\t\tmodels[name] = ElasticNet(a1, a2)\n\tmodels['huber'] = HuberRegressor()\n\tmodels['lars'] = Lars()\n\tmodels['llars'] = LassoLars()\n\tmodels['pa'] = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3)\n\tmodels['ranscac'] = RANSACRegressor()\n\tmodels['sgd'] = SGDRegressor(max_iter=1000, tol=1e-3)\n\tmodels['theil'] = TheilSenRegressor()\n\t# non-linear models\n\tn_neighbors = range(1, 21)\n\tfor k in n_neighbors:\n\t\tmodels['knn-'+str(k)] = KNeighborsRegressor(n_neighbors=k)\n\tmodels['cart'] = DecisionTreeRegressor()\n\tmodels['extra'] = ExtraTreeRegressor()\n\tmodels['svml'] = SVR(kernel='linear')\n\tmodels['svmp'] = SVR(kernel='poly')\n\tc_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\tfor c in c_values:\n\t\tmodels['svmr'+str(c)] = SVR(C=c)\n\t# ensemble models\n\tn_trees = 100\n\tmodels['ada'] = AdaBoostRegressor(n_estimators=n_trees)\n\tmodels['bag'] = BaggingRegressor(n_estimators=n_trees)\n\tmodels['rf'] = RandomForestRegressor(n_estimators=n_trees)\n\tmodels['et'] = ExtraTreesRegressor(n_estimators=n_trees)\n\tmodels['gbm'] = GradientBoostingRegressor(n_estimators=n_trees)\n\tprint('Defined %d models' % len(models))\n\treturn models\n\n# create a feature preparation pipeline for a model\ndef make_pipeline(model):\n\tsteps = list()\n\t# the model\n\tsteps.append(('model', model))\n\t# create pipeline\n\tpipeline = Pipeline(steps=steps)\n\treturn pipeline\n\n# evaluate a single model\ndef evaluate_model(X, y, model, folds, metric):\n\t# create the pipeline\n\tpipeline = make_pipeline(model)\n\t# evaluate model\n\tscores = cross_val_score(model, X, y, scoring=metric, cv=folds, n_jobs=-1)\n\treturn scores\n\n# evaluate a model and try to trap errors and and hide warnings\ndef robust_evaluate_model(X, y, model, folds, metric):\n\tscores = None\n\ttry:\n\t\twith warnings.catch_warnings():\n\t\t\twarnings.filterwarnings(\"ignore\")\n\t\t\tscores = evaluate_model(X, y, model, folds, metric)\n\texcept:\n\t\tscores = None\n\treturn scores\n\ndef evaluate_models(X, y, models, folds=10, metric='accuracy'):\n\tresults = dict()\n\tfor name, model in models.items():\n\t\t# evaluate the model\n\t\tscores = robust_evaluate_model(X, y, model, folds, metric)\n\t\t# show process\n\t\tif scores is not None:\n\t\t\t# store a result\n\t\t\tresults[name] = scores\n\t\t\tmean_score, std_score = mean(scores), std(scores)\n\t\t\tprint('>%s: %.3f (+\/-%.3f)' % (name, mean_score, std_score))\n\t\telse:\n\t\t\tprint('>%s: error' % name)\n\treturn results\n\n# print and plot the top n results\ndef summarize_results(results, maximize=True, top_n=10):\n\t# check for no results\n\tif len(results) == 0:\n\t\tprint('no results')\n\t\treturn\n\t# determine how many results to summarize\n\tn = min(top_n, len(results))\n\t# create a list of (name, mean(scores)) tuples\n\tmean_scores = [(k,mean(v)) for k,v in results.items()]\n\t# sort tuples by mean score\n\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n\t# reverse for descending order (e.g. for accuracy)\n\tif maximize:\n\t\tmean_scores = list(reversed(mean_scores))\n\t# retrieve the top n for summarization\n\tnames = [x[0] for x in mean_scores[:n]]\n\tscores = [results[x[0]] for x in mean_scores[:n]]\n\t# print the top n\n\tprint()\n\tfor i in range(n):\n\t\tname = names[i]\n\t\tmean_score, std_score = mean(results[name]), std(results[name])\n\t\tprint('Rank=%d, Name=%s, Score=%.3f (+\/- %.3f)' % (i+1, name, mean_score, std_score))\n\ndef getNamesAndScores(results):\n\t# check for no results\n\tif len(results) == 0:\n\t\tprint('no results')\n\t\treturn\n\t# determine how many results to summarize\n\tn = min(10, len(results))\n\t# create a list of (name, mean(scores)) tuples\n\tmean_scores = [(k,mean(v)) for k,v in results.items()]\n\t# sort tuples by mean score\n\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n\t# reverse for descending order (e.g. for accuracy)\n\tif True:\n\t\tmean_scores = list(reversed(mean_scores))\n\t# retrieve the top n for summarization\n\tnames = [x[0] for x in mean_scores[:n]]\n\tscores = [results[x[0]] for x in mean_scores[:n]]\n\treturn names, scores\n\n# load dataset\n# Import the dataset\nfile_name = \"procesedDataset.csv\"\ndataset = pd.read_csv(file_name)\nX, y = dataset.iloc[:,:-1], dataset.iloc[:,-1:]\n# get model list\nmodels = get_models()\n\n# evaluate models\nfig, axs = plt.subplots(2, 2)\nresults = evaluate_models(X, y, models, metric='r2')\n\nnames, scores = getNamesAndScores(results)\naxs[0, 0].boxplot(scores, labels=names)\naxs[0, 0].set_title('r2')\n\nresults = evaluate_models(X, y, models, metric='neg_mean_absolute_error')\nnames, scores = getNamesAndScores(results)\naxs[0, 1].boxplot(scores, labels=names)\naxs[0, 1].set_title('neg_mean_absolute_error')\n\nresults = evaluate_models(X, y, models, metric='neg_root_mean_squared_error')\nnames, scores = getNamesAndScores(results)\naxs[1, 0].boxplot(scores, labels=names)\naxs[1, 0].set_title('neg_root_mean_squared_error')\n\nresults = evaluate_models(X, y, models, metric='neg_root_mean_squared_error')\nnames, scores = getNamesAndScores(results)\naxs[1, 1].boxplot(scores, labels=names)\naxs[1, 1].set_title('neg_root_mean_squared_error')\n\nfor ax in axs.flat:\n    ax.set(xlabel='x-label', ylabel='y-label')\nplt.show(block=True)","7f745895":"from sklearn import linear_model\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport pickle\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std\n\n'''Here we will complete paramater tuninning for the four models\n    selected by spot-checking:\n    -- RandomForestRegressor (RF)\n    -- BaggingRegressor (Bag)\n    -- ExtraTreesRegressor (ET)\n    -- GradientBoostingRegressor (GBM)\n'''\n\n# Import the dataset\nfile_name = \"procesedDataset.csv\"\ndataset = pd.read_csv(file_name)\nX_train, X_test, y_train, y_test = train_test_split(dataset.drop(['SalePrice'],axis='columns'),dataset[\"SalePrice\"],test_size=0.2)\n\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=1)\n\ndef findParamModel1():\n    # RandomForestRegressor\n    print(\"Model 1 started\")\n    model = RandomForestRegressor()\n    n_estimators = range(50,200,20)\n    criterion = ['mse', 'mae']\n    min_samples_split = range(2,10,1)\n    min_samples_leaf = range(1,5,1)\n    max_features = ['auto', 'sqrt', 'log2']\n    grid = dict(n_estimators=n_estimators,criterion=criterion,\n    min_samples_split=min_samples_split,\n    min_samples_leaf=min_samples_leaf,max_features=max_features)\n    rsearch = RandomizedSearchCV(estimator=model, param_distributions=grid, n_iter=100, verbose=50)\n    rsearch.fit(X_train, y_train)\n    model1_params = rsearch.best_params_\n    print(\"Model 1 finished\")\n    return model1_params\n\ndef findParamModel2():\n    # BaggingRegressor\n    print(\"Model 2 started\")\n    model = BaggingRegressor()\n    n_estimators = range(50,200,20)\n    bootstrap = [True,False]\n    bootstrap_features = [True,False]\n    grid = dict(n_estimators=n_estimators,bootstrap=bootstrap,bootstrap_features=bootstrap_features)\n    rsearch = RandomizedSearchCV(estimator=model, param_distributions=grid, n_iter=100, verbose=50)\n    rsearch.fit(X_train, y_train)\n    model2_params = rsearch.best_params_\n    print(\"Model 2 finished\")\n    return model2_params\n\ndef findParamModel3():\n    # ExtraTreesRegressor\n    print(\"Model 3 started\")\n    model = ExtraTreesRegressor()\n    n_estimators = range(50,200,20)\n    criterion = ['mse', 'mae']\n    min_samples_split = range(2,10,1)\n    min_samples_leaf = range(1,5,1)\n    max_features = ['auto', 'sqrt', 'log2']\n    grid = dict(n_estimators=n_estimators,criterion=criterion,\n    min_samples_split=min_samples_split,\n    min_samples_leaf=min_samples_leaf,max_features=max_features)\n    rsearch = RandomizedSearchCV(estimator=model, param_distributions=grid, n_iter=100, verbose=50)\n    rsearch.fit(X_train, y_train)\n    model3_params = rsearch.best_params_\n    print(\"Model 3 finished\")\n    return model3_params\n\ndef findParamModel4():\n    # LinearRegression\n    print(\"Model 4 started\")\n    model = GradientBoostingRegressor()\n    loss = ['ls', 'lad', 'huber', 'quantile']\n    n_estimators = range(50,200,20)\n    criterion = ['mse', 'mae','friedman_mse']\n    min_samples_split = range(2,10,1)\n    min_samples_leaf = range(1,5,1)\n    max_depth = range(1,20,2)\n    max_features = ['auto', 'sqrt', 'log2']\n    grid = dict(loss=loss,n_estimators=n_estimators,criterion=criterion,min_samples_split=min_samples_split,\n    min_samples_leaf=min_samples_leaf,max_depth=max_depth,max_features=max_features)\n    rsearch = RandomizedSearchCV(estimator=model, param_distributions=grid, n_iter=100)\n    rsearch.fit(X_train, y_train)\n    model4_params = rsearch.best_params_\n    print(\"Model 4 finished\")\n    return model4_params\n\ndef testModel(model):\n    model = model.fit(X_train,y_train)\n    scores = cross_val_score(model, X_test, y_test, scoring=\"r2\", cv=15, n_jobs=-1)\n    return mean(scores), std(scores)\n\ndef makeCombinedModel():\n    model1 = RandomForestRegressor(n_estimators= 110, min_samples_split= 5,min_samples_leaf= 4, max_features= 'auto', criterion= 'mae')\n    model2 = BaggingRegressor(n_estimators= 70, bootstrap_features= False, bootstrap= True)\n    model3 = ExtraTreesRegressor(n_estimators= 90, min_samples_split= 6, min_samples_leaf= 3, max_features= 'auto', criterion= 'mae')\n    model4 = GradientBoostingRegressor(n_estimators= 130, min_samples_split= 6, min_samples_leaf= 4, max_features= 'auto', max_depth= 1, loss= 'lad', criterion= 'friedman_mse')\n\n    FINAL_MODEL = VotingRegressor(estimators=[('RF', model1), ('Bag', model2), (\"ET\", model3), ('gbm', model4)])\n    return FINAL_MODEL\n\n# Model 1 test\nprint(\"Model 1 (RandomForestRegressor)\")\nprint(testModel(RandomForestRegressor(n_estimators= 110, min_samples_split= 5,min_samples_leaf= 4, max_features= 'auto', criterion= 'mae')))\n\n# Model 2 test\nprint(\"Model 2 (BaggingRegressor)\")\nprint(testModel(BaggingRegressor(n_estimators= 70, bootstrap_features= False, bootstrap= True)))\n\n# Model 3 test\nprint(\"Model 3 (ExtraTreesRegressor)\")\nprint(testModel(ExtraTreesRegressor(n_estimators= 90, min_samples_split= 6, min_samples_leaf= 3, max_features= 'auto', criterion= 'mae')))\n\n# Model 4 test\nprint(\"Model 4 (GradientBoostingRegressor)\")\nprint(testModel(GradientBoostingRegressor(n_estimators= 130, min_samples_split= 6, min_samples_leaf= 4, max_features= 'auto', max_depth= 1, loss= 'lad', criterion= 'friedman_mse')))\n","ce66ba9f":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pickle\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import VotingRegressor\n\nfile_name = \"procesedDataset.csv\"\ndatasetTrain = pd.read_csv(file_name)\n\nX_train, X_test, y_train, y_test = train_test_split(datasetTrain.drop(['SalePrice'],axis='columns'),datasetTrain[\"SalePrice\"],test_size=0.2)\n\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=1)\n\nX, y = datasetTrain.iloc[:,:-1], datasetTrain.iloc[:,-1:]\n\ndef createCombinedModel():\n    model1 = RandomForestRegressor(n_estimators= 110, min_samples_split= 5,min_samples_leaf= 4, max_features= 'auto', criterion= 'mae')\n    model2 = BaggingRegressor(n_estimators= 70, bootstrap_features= False, bootstrap= True)\n    model3 = ExtraTreesRegressor(n_estimators= 90, min_samples_split= 6, min_samples_leaf= 3, max_features= 'auto', criterion= 'mae')\n    model4 = GradientBoostingRegressor(n_estimators= 130, min_samples_split= 6, min_samples_leaf= 4, max_features= 'auto', max_depth= 1, loss= 'lad', criterion= 'friedman_mse')\n\n    FINAL_MODEL = VotingRegressor(estimators=[('RF', model1), ('Bag', model2), (\"ET\", model3), ('gbm', model4)])\n    return FINAL_MODEL\n\ndef createSingleModel():\n    model3 = ExtraTreesRegressor(n_estimators= 90, min_samples_split= 6, min_samples_leaf= 3, max_features= 'auto', criterion= 'mae')\n    return model3\n\ndef testModel(model):\n    model = model.fit(X_train,y_train)\n    scores = cross_val_score(model, X_test, y_test, scoring=\"r2\", cv=15, n_jobs=-1)\n    return mean(scores), std(scores)\n\ndef model1():\n    save_file_name = \"array1.csv\"\n    mean_score = 0\n    while mean_score < score_aim:\n        model = createSingleModel()\n        mean_score, std_score = testModel(model)\n        print(\"Model 1\")\n        print(mean_score)\n    return model, save_file_name\n\ndef model2():\n    save_file_name = \"array2.csv\"\n    mean_score = 0\n    while mean_score < score_aim:\n        model = createCombinedModel()\n        mean_score, std_score = testModel(model)\n        print(\"Model 2\")\n        print(mean_score)\n    return model, save_file_name\n\nscore_aim = 0.85\n\n# Get model\nmodel, save_file_name = model1()\n#model, save_file_name = model2()\n\nmodel = model.fit(X,y)\n\n# Get test data\ntestFile = \"preparedTest.csv\"\ntestDataX = pd.read_csv(testFile)\n\n# Prepare test data\nresultX = testDataX[\"Id\"]\ntestDataX = testDataX.drop([\"Id\"], axis=1)\n\nresults = model.predict(testDataX)\n\n# Write the results to a file\nfinal_result = []\nfor i in range(0,resultX.count()):\n    final_result.append([resultX[i], results[i]])\nnp.savetxt(save_file_name, final_result, delimiter=',', fmt='%d')\n","1c880721":"# Model testing and predicting prices","73a5e20b":"**Convert categorical to numerical**","d7ccb6da":"**Import packages and the datasets**","b84ed4d6":"**Fill missing values**","f146103f":"# Parameter tunning","a09cb2ab":"**Feature engineering**","31e5fbc1":"# Data preperation","1e46da24":"**Normalize and standerdize**","f8499aeb":"**Save**","4665d511":"**Remove usles features and split dataset.**","027ca4b3":"# Spot checking"}}