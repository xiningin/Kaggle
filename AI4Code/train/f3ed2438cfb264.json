{"cell_type":{"80f75e77":"code","8d72c09b":"code","668d1f44":"code","110b538e":"code","47a2a327":"code","dc6dff14":"code","f3dd600f":"code","2667e145":"code","44f55b5a":"markdown","aa656dfa":"markdown","33af3048":"markdown","77b9627e":"markdown","e78b41a4":"markdown"},"source":{"80f75e77":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\")\ntrain","8d72c09b":"cat_features = [\n    \"cat0\", \"cat1\", \"cat2\", \"cat3\", \"cat4\", \"cat5\", \"cat6\", \"cat7\", \n    \"cat8\", \"cat9\"\n]\n\ncont_features = [\n    \"cont0\", \"cont1\", \"cont2\", \"cont3\", \"cont4\",\n    \"cont5\", \"cont6\", \"cont7\", \"cont8\", \"cont9\", \"cont10\", \n    \"cont11\", \"cont12\", \"cont13\"\n]","668d1f44":"!pip install category-encoders","110b538e":"from sklearn.preprocessing import LabelEncoder\nfrom category_encoders import LeaveOneOutEncoder\nfrom category_encoders import MEstimateEncoder\n\nlgb_cat_features = []\nxgb1_cat_features = []\nxgb2_cat_features = []\n\ndef label_encode(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column])\n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    return new_feature\n\ndef loo_encode(train_df, test_df, column):\n    loo = LeaveOneOutEncoder()\n    new_feature = \"{}_loo\".format(column)\n    loo.fit(train_df[column], train_df[\"target\"])\n    train_df[new_feature] = loo.transform(train_df[column])\n    test_df[new_feature] = loo.transform(test_df[column])\n    return new_feature\n\ndef mee_encode(train_df, test_df, column):\n    mee = MEstimateEncoder()\n    new_feature = \"{}_mee\".format(column)\n    mee.fit(train_df[column], train_df[\"target\"])\n    train_df[new_feature] = mee.transform(train_df[column])\n    test_df[new_feature] = mee.transform(test_df[column])\n    return new_feature\n\nfor feature in cat_features:\n    lgb_cat_features.append(label_encode(train, test, feature))\n\nfor feature in cat_features:\n    xgb1_cat_features.append(loo_encode(train, test, feature))\n    \nxgb2_cat_features.extend(xgb1_cat_features)\nfor feature in cat_features:\n    xgb2_cat_features.append(mee_encode(train, test, feature))\n\ntrain","47a2a327":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nn_folds = 10\n\nskf = KFold(n_splits=n_folds, random_state=2021, shuffle=True)\n\nlgb_train_oof = np.zeros((300000,))\nxgb1_train_oof = np.zeros((300000,))\nxgb2_train_oof = np.zeros((300000,))\n\nlgb_test_preds = 0\nxgb1_test_preds = 0\nxgb2_test_preds = 0\n\nfull_features = []\nfull_features.extend(lgb_cat_features)\nfull_features.extend(xgb2_cat_features)\nfull_features.extend(cont_features)\n\nlgb_features = []\nlgb_features.extend(lgb_cat_features)\nlgb_features.extend(cont_features)\n\nxgb1_features = []\nxgb1_features.extend(xgb1_cat_features)\nxgb1_features.extend(cont_features)\n\nxgb2_features = []\nxgb2_features.extend(xgb2_cat_features)\nxgb2_features.extend(cont_features)\n\nlgb_params = {\n    \"random_state\": 2021,\n    \"metric\": \"rmse\",\n    \"n_jobs\": -1,\n    \"early_stopping_round\": 200,\n    \"cat_features\": [x for x in range(len(lgb_cat_features))],\n    \"reg_alpha\": 9.03513073170552,\n    \"reg_lambda\": 0.024555737897445917,\n    \"colsample_bytree\": 0.2185112060137363,\n    \"learning_rate\": 0.003049106861273527,\n    \"max_depth\": 65,\n    \"num_leaves\": 51,\n    \"min_child_samples\": 177,\n    \"n_estimators\": 1600000,\n    \"cat_smooth\": 93.60968300634175,\n    \"max_bin\": 537,\n    \"min_data_per_group\": 117,\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.6709049555262285,\n    \"cat_l2\": 7.5586732660804445,\n}\n\nxgb1_params = {\n    \"seed\": 2021,\n    \"n_estimators\": 10000,\n    \"verbosity\": 1,\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"tree_method\": \"gpu_hist\", \n    \"gpu_id\": 0,\n    \"colsample_bytree\": 0.23116058185789234, \n    \"gamma\": 2.0737266506535375, \n    \"lambda\": 8.76288374058159, \n    \"learning_rate\": 0.01126802018395814, \n    \"max_depth\": 11, \n    \"min_child_weight\": 1.4477515824904934, \n    \"subsample\": 0.4898608703522127,\n    \"alpha\": 9.206528646529561,\n    \"max_bin\": 658,\n}\n\nxgb2_params = {\n    \"seed\": 2021,\n    \"n_estimators\": 10000,\n    \"verbosity\": 1,\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"tree_method\": \"gpu_hist\", \n    \"gpu_id\": 0,\n    \"colsample_bytree\": 0.21428906671248488, \n    \"gamma\": 2.116597615713485, \n    \"lambda\": 6.683336764226128, \n    \"learning_rate\": 0.008098678089211109, \n    \"max_depth\": 11, \n    \"min_child_weight\": 7.587302445130359, \n    \"subsample\": 0.6309499238527587,\n    \"alpha\": 8.078271293205107,\n    \"max_bin\": 570,\n}\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train, train[\"target\"])):\n    print(\"-------> Fold {} <--------\".format(fold + 1))\n    x_train, x_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[test_index])\n    y_train, y_valid = train[\"target\"].iloc[train_index], train[\"target\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[full_features])\n    x_valid_features = pd.DataFrame(x_valid[full_features])\n\n    print(\": Build LightGBM model\")\n    lgb_model = LGBMRegressor(\n        **lgb_params\n    )\n    lgb_model.fit(\n        x_train_features[lgb_features], \n        y_train,\n        eval_set=[(x_valid_features[lgb_features], y_valid)],\n        verbose=500,\n    )\n    \n    print(\"\")\n    print(\": Build XGBoost model 1\")\n    xgb1_model = XGBRegressor(\n        **xgb1_params\n    )\n    xgb1_model.fit(\n        x_train_features[xgb1_features], \n        y_train,\n        eval_set=[(x_valid_features[xgb1_features], y_valid)],\n        verbose=500,\n        early_stopping_rounds=200,\n    )\n\n    print(\"\")\n    print(\": Build XGBoost model 2\")\n    xgb2_model = XGBRegressor(\n        **xgb2_params\n    )\n    xgb2_model.fit(\n        x_train_features[xgb2_features], \n        y_train,\n        eval_set=[(x_valid_features[xgb2_features], y_valid)],\n        verbose=500,\n        early_stopping_rounds=200,\n    )\n    \n    lgb_oof_preds = lgb_model.predict(x_valid_features[lgb_features])\n    lgb_train_oof[test_index] = lgb_oof_preds\n\n    xgb1_oof_preds = xgb1_model.predict(x_valid_features[xgb1_features])\n    xgb1_train_oof[test_index] = xgb1_oof_preds\n\n    xgb2_oof_preds = xgb2_model.predict(x_valid_features[xgb2_features])\n    xgb2_train_oof[test_index] = xgb2_oof_preds\n\n    lgb_test_preds += lgb_model.predict(test[lgb_features]) \/ n_folds\n    xgb1_test_preds += xgb1_model.predict(test[xgb1_features]) \/ n_folds\n    xgb2_test_preds += xgb2_model.predict(test[xgb2_features]) \/ n_folds\n    print(\"\")\n    \nprint(\"--> Overall results for out of fold predictions\")\nprint(\": LGB RMSE = {}\".format(mean_squared_error(lgb_train_oof, train[\"target\"], squared=False)))\nprint(\": XGB 1 RMSE = {}\".format(mean_squared_error(xgb1_train_oof, train[\"target\"], squared=False)))\nprint(\": XGB 2 RMSE = {}\".format(mean_squared_error(xgb2_train_oof, train[\"target\"], squared=False)))","dc6dff14":"best_split_rmse = 9999999.9\nbest_split_combo = -1\n\nsplit_results = []\nsplit_percentages = []\nsplit_strings = []\nindex = 0\n\nprint(\"--> Calculating best combination\")\nsplits = []\nfor x in range(100, 0, -5):\n    if 100 - x == 0:\n        splits.append([x \/ 100., 0., 0.])\n    for y in range(100 - x, 0, -5):\n        if 100 - x - y == 0 and x + y == 100:\n            splits.append([x \/ 100., y \/ 100., 0.])\n            splits.append([x \/ 100., 0., y \/ 100.])\n        for z in range(100 - x - y, 0, -5):\n            if x + y + z == 100:\n                splits.append([x \/100., y \/ 100., z \/ 100.])\n            \nfor index, x in enumerate(splits):\n    combo_preds = (x[0] * lgb_train_oof) + (x[1] * xgb1_train_oof) + (x[2] * xgb2_train_oof)\n    rmse = mean_squared_error(combo_preds, train[\"target\"], squared=False)\n    split_results.append(rmse)\n    split_strings.append(\"LGB {:0.3} + XGB1 {:0.3} + XGB2 {:0.3}\".format(x[0], x[1], x[2]))\n    split_percentages.append(x)\n    if rmse < best_split_rmse:\n        best_split_rmse = rmse\n        best_split_combo = index\n    \nprint(\": Best combo is {}\".format(split_strings[best_split_combo]))\nprint(\": Best RMSE is {}\".format(best_split_rmse))","f3dd600f":"import matplotlib.pyplot as plt \n%matplotlib inline\n\nlabels = [x if index % 5 == 0 else \" \" for index, x in enumerate(split_strings)]\nplt.figure(figsize=(20, 10))\nplt.plot(split_strings, split_results)\nplt.xticks(labels, rotation=90)\nplt.xlabel('LGB \/ XGB1 \/ XGB2 Split Amounts')\nplt.ylabel('Root Mean Squared Error (RMSE)')\nplt.show()","2667e145":"preds = (lgb_test_preds * split_percentages[best_split_combo][0])\npreds += (xgb1_test_preds * split_percentages[best_split_combo][1])\npreds += (xgb2_test_preds * split_percentages[best_split_combo][2])\npreds = preds.tolist()\ntest_ids = test[\"id\"].tolist()\n\nsubmission = pd.DataFrame({\"id\": test_ids, \"target\": preds})\nsubmission.to_csv(\"submission.csv\", index=False)","44f55b5a":"# Feature Engineering","aa656dfa":"# Generate Submission\n\nGenerate the submission file using the best combination of values.","33af3048":"# Find Best Combo\n\nNow we'll search for the best split between the LightGBM model and the XGBoost model. We'll plot the results and make sure there aren't any additional useful combinations to check out besides the best looking one.","77b9627e":"# Build Models\n\nHere we will build 3 models - one LightGBM model and two XGBoost models. We'll save the results of each model separately so we can try different combinations of both later. We'll use out-of-fold predictions for the test set for better results.","e78b41a4":"# Introduction\n\nThis kernel combines XGBoost and LightGBM models together. It uses hand tuned XGBoost and LightGBM models to do so. It searches for the optimal combination of models to produce a blended combination of both. No extensive feature engineering is performed. In this instance we'll use 3 different models:\n\n* LightGBM with `LabelEncode`.\n* XGBoost with `LeaveOneOut` encoding.\n* XGBoost with `LeaveOneOut` encoding and `MEstimateEncoder`.\n\n## Credits\n\n* [LGBM Goes brrr!](https:\/\/www.kaggle.com\/maunish\/lgbm-goes-brrr) for tuned LGBM parameters."}}