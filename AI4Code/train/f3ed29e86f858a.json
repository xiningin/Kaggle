{"cell_type":{"1b75d4e2":"code","d67fb4f7":"code","af0acb52":"code","f7897463":"code","5b297d2d":"code","4da0f9b5":"code","b7bd9ad8":"code","f90fb687":"code","9daeca61":"code","9a391696":"code","fbba04f8":"code","530c82b1":"code","0cb641dd":"code","d93eda7b":"code","0726c671":"code","e2353893":"code","e9f14444":"code","2771c193":"code","d7214ef0":"code","5a401e37":"code","b43f7b6c":"code","e0b79684":"code","30336c62":"code","90d72b7b":"code","adf9866c":"code","8e7d63da":"code","93361616":"code","15235fc7":"code","5d16b181":"code","bf214e15":"code","6b282e52":"code","575fee94":"code","c44a2363":"code","29160604":"code","88ff3bf3":"code","45f4fa00":"code","dd43fa7b":"code","76d8dcf3":"code","80bc72a6":"code","b753f6ee":"code","244bfb21":"code","48492ef8":"code","7c98e081":"code","28d28191":"code","c7c061dc":"code","4d5a1a0d":"code","7af29f94":"code","0d4f0410":"code","7459655f":"code","a3badd49":"code","8be40620":"code","89f48a57":"code","e1bc2be0":"code","8a65829d":"code","700052ac":"code","55aa14e2":"code","cea82388":"code","91b8e7fa":"code","a1e7996b":"code","0f05fd39":"code","72c53b34":"code","8ac2828d":"code","af9b3a2f":"code","b32259a5":"code","07a8a4ed":"code","bba31d5e":"code","66467e48":"code","a8f2773e":"code","fc2989d7":"code","a75e19e3":"code","1fd4df7d":"code","f25448ee":"code","1cc16da5":"code","d64ececa":"code","47df5623":"code","57906cad":"code","c788ba1a":"code","4ab5b8d6":"code","b76cb01c":"code","ccef3d14":"code","17b07d1e":"code","28cef71f":"code","38d08d3c":"code","3a74ace1":"code","7d3ab2ca":"code","6422ad5d":"code","079e5cd7":"code","12174b3e":"code","243314cd":"code","40da6e84":"code","3b155497":"code","494dbe33":"code","27fb18f8":"code","82800274":"code","828a23cd":"code","b5a71f3b":"code","44d3f869":"code","befe4ef4":"code","261e9eb6":"code","1cb9ffaa":"code","80b7b4a9":"code","6efe4805":"code","f9146330":"code","fd7a3192":"code","d6ff167b":"code","ed68b630":"code","4b41ea41":"code","437fddcd":"code","dc6a1fe6":"code","95667e13":"code","491a30ca":"code","cd0f0918":"code","9f730a50":"code","4fffedca":"code","8e55c014":"code","9be968aa":"code","39dcceb4":"code","64a97eae":"code","872a1fee":"code","d0f0d8d3":"code","ed9d0a45":"code","8750526f":"code","fb909f1c":"code","9fe9504f":"code","e0f57f61":"markdown","df529140":"markdown","e376acde":"markdown","eea1c560":"markdown","e0cb9e82":"markdown","5c91acef":"markdown","2921d13a":"markdown","475d37ae":"markdown","9bc868bb":"markdown","fb20784d":"markdown","4ea13d14":"markdown","6aee5d2d":"markdown","7a3f9148":"markdown","9ef2f1d3":"markdown","79a6968a":"markdown","326db1d8":"markdown","87af1aa3":"markdown","67b312e5":"markdown","96403acd":"markdown","e37c15e8":"markdown","13824d9d":"markdown","f415589b":"markdown","d7ef65ab":"markdown","7e1a28f8":"markdown","f79e3b70":"markdown","09099672":"markdown","2e436063":"markdown","8c817c74":"markdown","e22ca966":"markdown","c1b39285":"markdown","1c276dda":"markdown","7aeb0f1f":"markdown","bf5dcaf7":"markdown","6fa6747b":"markdown","b84c0d55":"markdown","623f7fb5":"markdown","d290ca0b":"markdown","d95d2417":"markdown","f765578e":"markdown","2024dc94":"markdown","077ae5ff":"markdown","5cbc1e7d":"markdown","ef47bba9":"markdown","df4b0776":"markdown","de98829d":"markdown","fb3b7c9d":"markdown","cc7f3949":"markdown","4e645282":"markdown","0f19e706":"markdown","eae3a1a6":"markdown","80cd1568":"markdown","d954da88":"markdown","d05afd04":"markdown","7457aef4":"markdown","7336f4eb":"markdown","46e68f4a":"markdown","d0ac25f0":"markdown","247d7464":"markdown","776836fe":"markdown","cd233cb6":"markdown","e4e993b9":"markdown","6813f233":"markdown","fe70387d":"markdown","49c04008":"markdown","7ef25d5d":"markdown","70aee513":"markdown","f2b2bbd0":"markdown","8545fd1c":"markdown","755b7e97":"markdown"},"source":{"1b75d4e2":"# Libraries\n\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, KFold\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nimport json\nimport ast\nimport eli5\nimport shap\nfrom catboost import CatBoostRegressor\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model","d67fb4f7":"train = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')\n\n# from this kernel: https:\/\/www.kaggle.com\/gravix\/gradient-in-a-box\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ntrain = text_to_dict(train)\ntest = text_to_dict(test)","af0acb52":"train.head()","f7897463":"train.shape, test.shape","5b297d2d":"for i, e in enumerate(train['belongs_to_collection'][:5]):\n    print(i, e)","4da0f9b5":"train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0).value_counts()","b7bd9ad8":"train['collection_name'] = train['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\ntrain['has_collection'] = train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n\ntest['collection_name'] = test['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\ntest['has_collection'] = test['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n\ntrain = train.drop(['belongs_to_collection'], axis=1)\ntest = test.drop(['belongs_to_collection'], axis=1)","f90fb687":"for i, e in enumerate(train['genres'][:5]):\n    print(i, e)","9daeca61":"print('Number of genres in films')\ntrain['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts()","9a391696":"list_of_genres = list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)","fbba04f8":"plt.figure(figsize = (12, 8))\ntext = ' '.join([i for j in list_of_genres for i in j])\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top genres')\nplt.axis(\"off\")\nplt.show()","530c82b1":"Counter([i for j in list_of_genres for i in j]).most_common()","0cb641dd":"train['num_genres'] = train['genres'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_genres'] = train['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common(15)]\nfor g in top_genres:\n    train['genre_' + g] = train['all_genres'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_genres'] = test['genres'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_genres'] = test['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_genres:\n    test['genre_' + g] = test['all_genres'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['genres'], axis=1)\ntest = test.drop(['genres'], axis=1)","d93eda7b":"for i, e in enumerate(train['production_companies'][:5]):\n    print(i, e)","0726c671":"print('Number of production companies in films')\ntrain['production_companies'].apply(lambda x: len(x) if x != {} else 0).value_counts()","e2353893":"train[train['production_companies'].apply(lambda x: len(x) if x != {} else 0) > 11]","e9f14444":"# example of poster of such a film\n\nimg = Image.open(urlopen(\"https:\/\/image.tmdb.org\/t\/p\/w600_and_h900_bestv2\/5VKVaTJJsyDeOzY6fLcyTo1RA9g.jpg\"))\nimg","2771c193":"list_of_companies = list(train['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)","d7214ef0":"Counter([i for j in list_of_companies for i in j]).most_common(30)","5a401e37":"train['num_companies'] = train['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_production_companies'] = train['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_companies = [m[0] for m in Counter([i for j in list_of_companies for i in j]).most_common(30)]\nfor g in top_companies:\n    train['production_company_' + g] = train['all_production_companies'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_companies'] = test['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_production_companies'] = test['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_companies:\n    test['production_company_' + g] = test['all_production_companies'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['production_companies', 'all_production_companies'], axis=1)\ntest = test.drop(['production_companies', 'all_production_companies'], axis=1)","b43f7b6c":"for i, e in enumerate(train['production_countries'][:5]):\n    print(i, e)","e0b79684":"print('Number of production countries in films')\ntrain['production_countries'].apply(lambda x: len(x) if x != {} else 0).value_counts()","30336c62":"list_of_countries = list(train['production_countries'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_countries for i in j]).most_common(25)","90d72b7b":"train['num_countries'] = train['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_countries'] = train['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_countries = [m[0] for m in Counter([i for j in list_of_countries for i in j]).most_common(25)]\nfor g in top_countries:\n    train['production_country_' + g] = train['all_countries'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_countries'] = test['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_countries'] = test['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_countries:\n    test['production_country_' + g] = test['all_countries'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['production_countries', 'all_countries'], axis=1)\ntest = test.drop(['production_countries', 'all_countries'], axis=1)","adf9866c":"for i, e in enumerate(train['spoken_languages'][:5]):\n    print(i, e)","8e7d63da":"print('Number of spoken languages in films')\ntrain['spoken_languages'].apply(lambda x: len(x) if x != {} else 0).value_counts()","93361616":"list_of_languages = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_languages for i in j]).most_common(15)","15235fc7":"train['num_languages'] = train['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_languages'] = train['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_languages = [m[0] for m in Counter([i for j in list_of_languages for i in j]).most_common(30)]\nfor g in top_languages:\n    train['language_' + g] = train['all_languages'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_languages'] = test['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_languages'] = test['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_languages:\n    test['language_' + g] = test['all_languages'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['spoken_languages', 'all_languages'], axis=1)\ntest = test.drop(['spoken_languages', 'all_languages'], axis=1)","5d16b181":"for i, e in enumerate(train['Keywords'][:5]):\n    print(i, e)","bf214e15":"print('Number of Keywords in films')\ntrain['Keywords'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)","6b282e52":"list_of_keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nplt.figure(figsize = (16, 12))\ntext = ' '.join(['_'.join(i.split(' ')) for j in list_of_keywords for i in j])\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top keywords')\nplt.axis(\"off\")\nplt.show()","575fee94":"train['num_Keywords'] = train['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_Keywords'] = train['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\nfor g in top_keywords:\n    train['keyword_' + g] = train['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_Keywords'] = test['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_Keywords'] = test['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_keywords:\n    test['keyword_' + g] = test['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['Keywords', 'all_Keywords'], axis=1)\ntest = test.drop(['Keywords', 'all_Keywords'], axis=1)","c44a2363":"for i, e in enumerate(train['cast'][:1]):\n    print(i, e)","29160604":"print('Number of casted persons in films')\ntrain['cast'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)","88ff3bf3":"list_of_cast_names = list(train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_names for i in j]).most_common(15)","45f4fa00":"list_of_cast_names_url = list(train['cast'].apply(lambda x: [(i['name'], i['profile_path']) for i in x] if x != {} else []).values)\nd = Counter([i for j in list_of_cast_names_url for i in j]).most_common(16)\nfig = plt.figure(figsize=(20, 12))\nfor i, p in enumerate([j[0] for j in d]):\n    ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n    im = Image.open(urlopen(f\"https:\/\/image.tmdb.org\/t\/p\/w600_and_h900_bestv2{p[1]}\"))\n    plt.imshow(im)\n    ax.set_title(f'{p[0]}')","dd43fa7b":"list_of_cast_genders = list(train['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_genders for i in j]).most_common()","76d8dcf3":"list_of_cast_characters = list(train['cast'].apply(lambda x: [i['character'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_characters for i in j]).most_common(15)","80bc72a6":"train['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\ntop_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(15)]\nfor g in top_cast_names:\n    train['cast_name_' + g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\ntrain['genders_0_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['genders_1_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['genders_2_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    train['cast_character_' + g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\n    \ntest['num_cast'] = test['cast'].apply(lambda x: len(x) if x != {} else 0)\nfor g in top_cast_names:\n    test['cast_name_' + g] = test['cast'].apply(lambda x: 1 if g in str(x) else 0)\ntest['genders_0_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['genders_1_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['genders_2_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor g in top_cast_characters:\n    test['cast_character_' + g] = test['cast'].apply(lambda x: 1 if g in str(x) else 0)\n\ntrain = train.drop(['cast'], axis=1)\ntest = test.drop(['cast'], axis=1)","b753f6ee":"for i, e in enumerate(train['crew'][:1]):\n    print(i, e[:10])","244bfb21":"print('Number of casted persons in films')\ntrain['crew'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)","48492ef8":"list_of_crew_names = list(train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_names for i in j]).most_common(15)","7c98e081":"list_of_crew_names_url = list(train['crew'].apply(lambda x: [(i['name'], i['profile_path'], i['job']) for i in x] if x != {} else []).values)\nd = Counter([i for j in list_of_crew_names_url for i in j]).most_common(16)\nfig = plt.figure(figsize=(20, 16))\nfor i, p in enumerate([j[0] for j in d]):\n    ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n    if p[1]:\n        im = Image.open(urlopen(f\"https:\/\/image.tmdb.org\/t\/p\/w600_and_h900_bestv2{p[1]}\"))\n    else:\n        im = Image.new('RGB', (5, 5))\n    plt.imshow(im)\n    ax.set_title(f'Name: {p[0]} \\n Job: {p[2]}')","28d28191":"list_of_crew_jobs = list(train['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_jobs for i in j]).most_common(15)","c7c061dc":"list_of_crew_genders = list(train['crew'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_genders for i in j]).most_common(15)","4d5a1a0d":"list_of_crew_departments = list(train['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_departments for i in j]).most_common(14)","7af29f94":"list_of_crew_names = train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values\nCounter([i for j in list_of_crew_names for i in j]).most_common(15)","0d4f0410":"train['num_crew'] = train['crew'].apply(lambda x: len(x) if x != {} else 0)\ntop_crew_names = [m[0] for m in Counter([i for j in list_of_crew_names for i in j]).most_common(15)]\nfor g in top_crew_names:\n    train['crew_name_' + g] = train['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntrain['genders_0_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['genders_1_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['genders_2_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_crew_jobs = [m[0] for m in Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)]\nfor j in top_crew_jobs:\n    train['jobs_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\ntop_crew_departments = [m[0] for m in Counter([i for j in list_of_crew_departments for i in j]).most_common(15)]\nfor j in top_crew_departments:\n    train['departments_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n    \ntest['num_crew'] = test['crew'].apply(lambda x: len(x) if x != {} else 0)\nfor g in top_crew_names:\n    test['crew_name_' + g] = test['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntest['genders_0_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['genders_1_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['genders_2_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor j in top_crew_jobs:\n    test['jobs_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\nfor j in top_crew_departments:\n    test['departments_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n\ntrain = train.drop(['crew'], axis=1)\ntest = test.drop(['crew'], axis=1)","7459655f":"train.head()","a3badd49":"fig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train['revenue']);\nplt.title('Distribution of revenue');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(train['revenue']));\nplt.title('Distribution of log of revenue');","8be40620":"train['log_revenue'] = np.log1p(train['revenue'])","89f48a57":"fig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train['budget']);\nplt.title('Distribution of budget');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(train['budget']));\nplt.title('Distribution of log of budget');","e1bc2be0":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['budget'], train['revenue'])\nplt.title('Revenue vs budget');\nplt.subplot(1, 2, 2)\nplt.scatter(np.log1p(train['budget']), train['log_revenue'])\nplt.title('Log Revenue vs log budget');","8a65829d":"train['log_budget'] = np.log1p(train['budget'])\ntest['log_budget'] = np.log1p(test['budget'])","700052ac":"train['homepage'].value_counts().head()","55aa14e2":"train['has_homepage'] = 0\ntrain.loc[train['homepage'].isnull() == False, 'has_homepage'] = 1\ntest['has_homepage'] = 0\ntest.loc[test['homepage'].isnull() == False, 'has_homepage'] = 1","cea82388":"sns.catplot(x='has_homepage', y='revenue', data=train);\nplt.title('Revenue for film with and without homepage');","91b8e7fa":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='original_language', y='revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)]);\nplt.title('Mean revenue per language');\nplt.subplot(1, 2, 2)\nsns.boxplot(x='original_language', y='log_revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)]);\nplt.title('Mean log revenue per language');","a1e7996b":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train['original_title'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in titles')\nplt.axis(\"off\")\nplt.show()","0f05fd39":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train['overview'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in overview')\nplt.axis(\"off\")\nplt.show()","72c53b34":"vectorizer = TfidfVectorizer(\n            sublinear_tf=True,\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2),\n            min_df=5)\n\noverview_text = vectorizer.fit_transform(train['overview'].fillna(''))\nlinreg = LinearRegression()\nlinreg.fit(overview_text, train['log_revenue'])\neli5.show_weights(linreg, vec=vectorizer, top=20, feature_filter=lambda x: x != '<BIAS>')","8ac2828d":"print('Target value:', train['log_revenue'][1000])\neli5.show_prediction(linreg, doc=train['overview'].values[1000], vec=vectorizer)","af9b3a2f":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['popularity'], train['revenue'])\nplt.title('Revenue vs popularity');\nplt.subplot(1, 2, 2)\nplt.scatter(train['popularity'], train['log_revenue'])\nplt.title('Log Revenue vs popularity');","b32259a5":"test.loc[test['release_date'].isnull() == True, 'release_date'] = '01\/01\/98'","07a8a4ed":"def fix_date(x):\n    \"\"\"\n    Fixes dates which are in 20xx\n    \"\"\"\n    year = x.split('\/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year","bba31d5e":"train['release_date'] = train['release_date'].apply(lambda x: fix_date(x))\ntest['release_date'] = test['release_date'].apply(lambda x: fix_date(x))\ntrain['release_date'] = pd.to_datetime(train['release_date'])\ntest['release_date'] = pd.to_datetime(test['release_date'])","66467e48":"# creating features based on dates\ndef process_date(df):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n    \n    return df\n\ntrain = process_date(train)\ntest = process_date(test)","a8f2773e":"d1 = train['release_date_year'].value_counts().sort_index()\nd2 = test['release_date_year'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Number of films per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","fc2989d7":"d1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].sum()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='total revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and total revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Total revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","a75e19e3":"d1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].mean()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='mean revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and average revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Average revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","1fd4df7d":"sns.catplot(x='release_date_weekday', y='revenue', data=train);\nplt.title('Revenue on different days of week of release');","f25448ee":"plt.figure(figsize=(20, 6))\nplt.subplot(1, 3, 1)\nplt.hist(train['runtime'].fillna(0) \/ 60, bins=40);\nplt.title('Distribution of length of film in hours');\nplt.subplot(1, 3, 2)\nplt.scatter(train['runtime'].fillna(0), train['revenue'])\nplt.title('runtime vs revenue');\nplt.subplot(1, 3, 3)\nplt.scatter(train['runtime'].fillna(0), train['popularity'])\nplt.title('runtime vs popularity');","1cc16da5":"train['status'].value_counts()","d64ececa":"test['status'].value_counts()","47df5623":"plt.figure(figsize = (12, 12))\ntext = ' '.join(train['tagline'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in tagline')\nplt.axis(\"off\")\nplt.show()","57906cad":"sns.boxplot(x='has_collection', y='revenue', data=train);","c788ba1a":"sns.catplot(x='num_genres', y='revenue', data=train);\nplt.title('Revenue for different number of genres in the film');","4ab5b8d6":"sns.violinplot(x='genre_Drama', y='revenue', data=train[:100]);","b76cb01c":"f, axes = plt.subplots(3, 5, figsize=(24, 12))\nplt.suptitle('Violinplot of revenue vs genres')\nfor i, e in enumerate([col for col in train.columns if 'genre_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i \/\/ 5][i % 5]);","ccef3d14":"f, axes = plt.subplots(6, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs production company')\nfor i, e in enumerate([col for col in train.columns if 'production_company' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i \/\/ 5][i % 5]);","17b07d1e":"sns.catplot(x='num_countries', y='revenue', data=train);\nplt.title('Revenue for different number of countries producing the film');","28cef71f":"f, axes = plt.subplots(5, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs production country')\nfor i, e in enumerate([col for col in train.columns if 'production_country' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i \/\/ 5][i % 5]);","38d08d3c":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['num_cast'], train['revenue'])\nplt.title('Number of cast members vs revenue');\nplt.subplot(1, 2, 2)\nplt.scatter(train['num_cast'], train['log_revenue'])\nplt.title('Log Revenue vs number of cast members');","3a74ace1":"f, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs cast')\nfor i, e in enumerate([col for col in train.columns if 'cast_name' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i \/\/ 5][i % 5]);","7d3ab2ca":"f, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs cast')\nfor i, e in enumerate([col for col in train.columns if 'cast_character_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i \/\/ 5][i % 5]);","6422ad5d":"f, axes = plt.subplots(6, 5, figsize=(24, 32))\nplt.suptitle('Violinplot of revenue vs keyword')\nfor i, e in enumerate([col for col in train.columns if 'keyword_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i \/\/ 5][i % 5]);","079e5cd7":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['num_crew'], train['revenue'])\nplt.title('Number of crew members vs revenue');\nplt.subplot(1, 2, 2)\nplt.scatter(train['num_crew'], train['log_revenue'])\nplt.title('Log Revenue vs number of crew members');","12174b3e":"f, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs crew_character')\nfor i, e in enumerate([col for col in train.columns if 'crew_character_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i \/\/ 5][i % 5]);","243314cd":"f, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs jobs')\nfor i, e in enumerate([col for col in train.columns if 'jobs_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i \/\/ 5][i % 5]);","40da6e84":"train = train.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status', 'log_revenue'], axis=1)\ntest = test.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status'], axis=1)","3b155497":"for col in train.columns:\n    if train[col].nunique() == 1:\n        print(col)\n        train = train.drop([col], axis=1)\n        test = test.drop([col], axis=1)","494dbe33":"for col in ['original_language', 'collection_name', 'all_genres']:\n    le = LabelEncoder()\n    le.fit(list(train[col].fillna('')) + list(test[col].fillna('')))\n    train[col] = le.transform(train[col].fillna('').astype(str))\n    test[col] = le.transform(test[col].fillna('').astype(str))","27fb18f8":"train_texts = train[['title', 'tagline', 'overview', 'original_title']]\ntest_texts = test[['title', 'tagline', 'overview', 'original_title']]","82800274":"for col in ['title', 'tagline', 'overview', 'original_title']:\n    train['len_' + col] = train[col].fillna('').apply(lambda x: len(str(x)))\n    train['words_' + col] = train[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    train = train.drop(col, axis=1)\n    test['len_' + col] = test[col].fillna('').apply(lambda x: len(str(x)))\n    test['words_' + col] = test[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    test = test.drop(col, axis=1)","828a23cd":"# data fixes from https:\/\/www.kaggle.com\/somang1418\/happy-valentines-day-and-keep-kaggling-3\ntrain.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\ntrain.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \ntrain.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\ntrain.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\ntrain.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \ntrain.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\ntrain.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\ntrain.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\ntrain.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\ntrain.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\ntrain.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\ntrain.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\ntrain.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\ntrain.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \ntrain.loc[train['id'] == 1542,'budget'] = 1              # All at Once\ntrain.loc[train['id'] == 1570,'budget'] = 15800000       # Crocodile Dundee II\ntrain.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\ntrain.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\ntrain.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\ntrain.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\ntrain.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\ntrain.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\ntrain.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\ntrain.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\ntrain.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\ntest.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\ntest.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\ntest.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\ntest.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\ntest.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\ntest.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\ntest.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\ntest.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\ntest.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\ntest.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee\n\npower_six = train.id[train.budget > 1000][train.revenue < 100]\n\nfor k in power_six :\n    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000","b5a71f3b":"X = train.drop(['id', 'revenue'], axis=1)\ny = np.log1p(train['revenue'])\nX_test = test.drop(['id'], axis=1)","44d3f869":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)","befe4ef4":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\nmodel1 = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\nmodel1.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)","261e9eb6":"eli5.show_weights(model1, feature_filter=lambda x: x != '<BIAS>')","1cb9ffaa":"n_fold = 10\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)","80b7b4a9":"def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(X.shape[0])\n    prediction = np.zeros(X_test.shape[0])\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        if model_type == 'sklearn':\n            X_train, X_valid = X[train_index], X[valid_index]\n        else:\n            X_train, X_valid = X.values[train_index], X.values[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n                    verbose=1000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n\n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_squared_error(y_valid, y_pred_valid)\n            \n            y_pred = model.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n        \n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","6efe4805":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 10,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\noof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)","f9146330":"for col in train_texts.columns:\n    vectorizer = TfidfVectorizer(\n                sublinear_tf=True,\n                analyzer='word',\n                token_pattern=r'\\w{1,}',\n                ngram_range=(1, 2),\n                min_df=10\n    )\n    vectorizer.fit(list(train_texts[col].fillna('')) + list(test_texts[col].fillna('')))\n    train_col_text = vectorizer.transform(train_texts[col].fillna(''))\n    test_col_text = vectorizer.transform(test_texts[col].fillna(''))\n    model = linear_model.RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=folds)\n    oof_text, prediction_text = train_model(train_col_text, test_col_text, y, params=None, model_type='sklearn', model=model)\n    \n    X[col + '_oof'] = oof_text\n    X_test[col + '_oof'] = prediction_text","fd7a3192":"X.head()","d6ff167b":"def new_features(df):\n    df['budget_to_popularity'] = df['budget'] \/ df['popularity']\n    df['budget_to_runtime'] = df['budget'] \/ df['runtime']\n    \n    # some features from https:\/\/www.kaggle.com\/somang1418\/happy-valentines-day-and-keep-kaggling-3\n    df['_budget_year_ratio'] = df['budget'] \/ (df['release_date_year'] * df['release_date_year'])\n    df['_releaseYear_popularity_ratio'] = df['release_date_year'] \/ df['popularity']\n    df['_releaseYear_popularity_ratio2'] = df['popularity'] \/ df['release_date_year']\n    \n    df['runtime_to_mean_year'] = df['runtime'] \/ df.groupby(\"release_date_year\")[\"runtime\"].transform('mean')\n    df['popularity_to_mean_year'] = df['popularity'] \/ df.groupby(\"release_date_year\")[\"popularity\"].transform('mean')\n    df['budget_to_mean_year'] = df['budget'] \/ df.groupby(\"release_date_year\")[\"budget\"].transform('mean')\n        \n    return df","ed68b630":"X = new_features(X)\nX_test = new_features(X_test)","4b41ea41":"oof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)","437fddcd":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n\nparams = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 6,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\nmodel1 = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\nmodel1.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)\n\neli5.show_weights(model1, feature_filter=lambda x: x != '<BIAS>')","dc6a1fe6":"explainer = shap.TreeExplainer(model1, X_train)\nshap_values = explainer.shap_values(X_train)\n\nshap.summary_plot(shap_values, X_train)","95667e13":"top_cols = X_train.columns[np.argsort(shap_values.std(0))[::-1]][:10]\nfor col in top_cols:\n    shap.dependence_plot(col, shap_values, X_train)","491a30ca":"def top_cols_interaction(df):\n    df['budget_to_year'] = df['budget'] \/ df['release_date_year']\n    df['budget_to_mean_year_to_year'] = df['budget_to_mean_year'] \/ df['release_date_year']\n    df['popularity_to_mean_year_to_log_budget'] = df['popularity_to_mean_year'] \/ df['log_budget']\n    df['year_to_log_budget'] = df['release_date_year'] \/ df['log_budget']\n    df['budget_to_runtime_to_year'] = df['budget_to_runtime'] \/ df['release_date_year']\n    df['genders_1_cast_to_log_budget'] = df['genders_1_cast'] \/ df['log_budget']\n    df['all_genres_to_popularity_to_mean_year'] = df['all_genres'] \/ df['popularity_to_mean_year']\n    df['genders_2_crew_to_budget_to_mean_year'] = df['genders_2_crew'] \/ df['budget_to_mean_year']\n    df['overview_oof_to_genders_2_crew'] = df['overview_oof'] \/ df['genders_2_crew']\n    \n    return df","cd0f0918":"X = top_cols_interaction(X)\nX_test = top_cols_interaction(X_test)","9f730a50":"X = X.replace([np.inf, -np.inf], 0).fillna(0)\nX_test = X_test.replace([np.inf, -np.inf], 0).fillna(0)","4fffedca":"trainAdditionalFeatures = pd.read_csv('..\/input\/tmdb-competition-additional-features\/TrainAdditionalFeatures.csv')\ntestAdditionalFeatures = pd.read_csv('..\/input\/tmdb-competition-additional-features\/TestAdditionalFeatures.csv')\n\ntrain = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')\nX['imdb_id'] = train['imdb_id']\nX_test['imdb_id'] = test['imdb_id']\ndel train, test\n\nX = pd.merge(X, trainAdditionalFeatures, how='left', on=['imdb_id'])\nX_test = pd.merge(X_test, testAdditionalFeatures, how='left', on=['imdb_id'])\n\nX = X.drop(['imdb_id'], axis=1)\nX_test = X_test.drop(['imdb_id'], axis=1)","8e55c014":"X.head()","9be968aa":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 9,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\noof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)","39dcceb4":"xgb_params = {'eta': 0.01,\n              'objective': 'reg:linear',\n              'max_depth': 7,\n              'subsample': 0.8,\n              'colsample_bytree': 0.8,\n              'eval_metric': 'rmse',\n              'seed': 11,\n              'silent': True}\noof_xgb, prediction_xgb = train_model(X, X_test, y, params=xgb_params, model_type='xgb', plot_feature_importance=False)","64a97eae":"cat_params = {'learning_rate': 0.002,\n              'depth': 5,\n              'l2_leaf_reg': 10,\n              # 'bootstrap_type': 'Bernoulli',\n              'colsample_bylevel': 0.8,\n              'bagging_temperature': 0.2,\n              #'metric_period': 500,\n              'od_type': 'Iter',\n              'od_wait': 100,\n              'random_seed': 11,\n              'allow_writing_files': False}\noof_cat, prediction_cat = train_model(X, X_test, y, params=cat_params, model_type='cat')","872a1fee":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\noof_lgb_1, prediction_lgb_1 = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=False)","d0f0d8d3":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 7,\n         'learning_rate': 0.02,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.7,\n         \"bagging_freq\": 5,\n         \"bagging_fraction\": 0.7,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\noof_lgb_2, prediction_lgb_2 = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=False)","ed9d0a45":"train_stack = np.vstack([oof_lgb, oof_xgb, oof_cat, oof_lgb_1, oof_lgb_2]).transpose()\ntrain_stack = pd.DataFrame(train_stack, columns=['lgb', 'xgb', 'cat', 'lgb_1', 'lgb_2'])\ntest_stack = np.vstack([prediction_lgb, prediction_xgb, prediction_cat, prediction_lgb_1, prediction_lgb_2]).transpose()\ntest_stack = pd.DataFrame(test_stack, columns=['lgb', 'xgb', 'cat', 'lgb_1', 'lgb_2'])","8750526f":"params = {'num_leaves': 8,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 3,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\noof_lgb_stack, prediction_lgb_stack, _ = train_model(train_stack, test_stack, y, params=params, model_type='lgb', plot_feature_importance=True)","fb909f1c":"model = linear_model.RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=folds)\noof_rcv_stack, prediction_rcv_stack = train_model(train_stack.values, test_stack.values, y, params=None, model_type='sklearn', model=model)","9fe9504f":"sub = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\nsub['revenue'] = np.expm1(prediction_lgb)\nsub.to_csv(\"lgb.csv\", index=False)\nsub['revenue'] = np.expm1((prediction_lgb + prediction_xgb) \/ 2)\nsub.to_csv(\"blend.csv\", index=False)\nsub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat) \/ 3)\nsub.to_csv(\"blend1.csv\", index=False)\nsub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat + prediction_lgb_1) \/ 4)\nsub.to_csv(\"blend2.csv\", index=False)\nsub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat + prediction_lgb_1 + prediction_lgb_2) \/ 5)\nsub.to_csv(\"blend3.csv\", index=False)\n\nsub['revenue'] = prediction_lgb_stack\nsub.to_csv(\"stack_lgb.csv\", index=False)\nsub['revenue'] = prediction_rcv_stack\nsub.to_csv(\"stack_rcv.csv\", index=False)","e0f57f61":"<a id=\"crew\"><\/a>\n### crew","df529140":"Here we can see interactions between important features. There are some interesting things here. For example relationship between release_date_year and log_budget. Up to ~1990 low budget films brought higher revenues, but after 2000 year high budgets tended to be correlated with higher revenues. And in genereal the effect of budget diminished.\n\nLet's create new features as interactions between top important features. Some of them make little sense, but maybe they could improve the model.","e376acde":"<a id=\"or_title\"><\/a>\n### original_title\n\nIt can be interesting to see which words are common in titles.","eea1c560":"<a id=\"prod_count\"><\/a>\n### Production countries","e0cb9e82":"<a id=\"btoc\"><\/a>\n### belongs_to_collection","5c91acef":"## Content\n\n* [1 Data loading and overview](#data_loading)\n* [1.1 belongs_to_collection](#btoc)\n* [1.2 genres](#genres)\n* [1.3 Production companies](#production_companies)\n* [1.4 Production countries](#production_countries)\n* [1.5 Spoken languages](#lang)\n* [1.6 Keywords](#keywords)\n* [1.7 Cast](#cast)\n* [1.8 Crew](#crew)\n* [2 Data exploration](#de)\n* [2.1 Target](#target)\n* [2.2 Budget](#budget)\n* [2.3 Homepage](#homepage)\n* [2.4 Original language](#or_lang)\n* [2.5 Original title](#or_title)\n* [2.6 Overview](#overview)\n* [2.7 Popularity](#popularity)\n* [2.8 Release date](#release_date)\n* [2.9 Runtime](#runtime)\n* [2.10 Status](#status)\n* [2.11 Tagline](#tagline)\n* [2.12 Collections](#collections)\n* [2.13 Genres](#genres_)\n* [2.14 Production companies](#prod_comp)\n* [2.15 Production countries](#prod_count)\n* [2.16 Cast](#cast_viz)\n* [2.17 Keywords](#key_viz)\n* [2.18 Crew](#crew_viz)\n* [3 Modelling and feature generation](#basic_model)\n* [3.1 OOF features based on texts](#oof)\n* [3.2 Additional feature generation](#add_feat)\n* [3.3 Important features](#imp_feats)\n* [3.4 External features](#ext_feats)\n* [3.5 Blending](#blending)\n* [3.6 Stacking](#stacking)","2921d13a":"<a id=\"de\"><\/a>\n## Data exploration","475d37ae":"We can see that budget and revenue are somewhat correlated. Logarithm transformation makes budget distribution more managable.","9bc868bb":"2396 values in this column are empty, 604 contrain information about the collections. I suppose that only collection name can be useful. Another possibly useful feature is the fact of belonging to a collection.","fb20784d":"<a id=\"basic_model\"><\/a>\n## Modelling and feature generation","4ea13d14":"I think it is quite funny the most popular male role is playing himself. :)","6aee5d2d":"<a id=\"release_data\"><\/a>\n### release_date","7a3f9148":"In fact I think that number of production countries hardly matters. Most films are produced by 1-2 companies, so films with 1-2 companies have the highest revenue.","9ef2f1d3":"Some genres tend to have less revenue, some tend to have higher.","79a6968a":"Most of homepages are unique, so this feature may be useless.","326db1d8":"As we know there are much more english films and they have a higher range of values. Films with the highest revenue are usually in English, but there are also high revenue films in other languages.","87af1aa3":"As we can see revenue distribution has a high skewness! It is better to use `np.log1p` of revenue.","67b312e5":"<a id=\"key_viz\"><\/a>\n### Keywords","96403acd":"For now I'm not sure what to do with this data. I'll simply create binary columns for top-30 films. Maybe later I'll have a better idea.","e37c15e8":"<a id=\"cast_viz\"><\/a>\n### Cast","13824d9d":"<a id=\"keywords\"><\/a>\n### Keywords","f415589b":"<a id=\"collections\"><\/a>\n### Collections","d7ef65ab":"0 is unspecified, 1 is female, and 2 is male. (https:\/\/www.kaggle.com\/c\/tmdb-box-office-prediction\/discussion\/80983#475572)","7e1a28f8":"There are only 3000 samples in train data! Let's hope this is enough to train models.\n\nWe can see that some of columns contain lists with dictionaries. Some lists contain a single dictionary, some have several. Let's extract data from these columns!","f79e3b70":"AS we can see only 4 films in train data and 7 in test aren't released yet, so this feature is quite useless.","09099672":"<a id=\"data_loading\"><\/a>\n## Data loading and overview","2e436063":"Surprisingly films releases on Wednesdays and on Thursdays tend to have a higher revenue.","8c817c74":"<a id=\"tagline\"><\/a>\n### tagline","e22ca966":"Those who are casted heavily impact the quality of the film. We have not only the name of the actor, but also the gender and character name\/type.\n\nAt first let's have a look at the popular names.","c1b39285":"Normally films are produced by a single country, but there are cases when companies from several countries worked together.","1c276dda":"We can see that some words can be used to predict revenue, but we will need more that overview text to build a good model.","7aeb0f1f":"<a id=\"crew_viz\"><\/a>\n### Crew","bf5dcaf7":"<a id=\"popularity\"><\/a>\n### popularity\n\nI'm not exactly sure what does popularity represents. Maybe it is some king of weighted rating, maybe something else. It seems it has low correlation with the target.","6fa6747b":"<a id=\"blending\"><\/a>\n### Blending","b84c0d55":"Here we have some keywords describing films. Of course there can be a lot of them. Let's have a look at the most common ones.","623f7fb5":"<a id=\"target\"><\/a>\n### Target","d290ca0b":"The great crew is very important in creating the film. We have not only the names of the crew members, but also the genders, jobs and departments.\n\nAt first let's have a look at the popular names.","d95d2417":"<a id=\"cast\"><\/a>\n### cast","f765578e":"It seems that most of the films are 1.5-2 hour long and films with the highest revenue are also in this range","2024dc94":"<a id=\"homepage\"><\/a>\n### homepage","077ae5ff":"<a id=\"imp_feats\"><\/a>\n### Important features\n\nLet's have a look at important features using ELI5 and SHAP!","5cbc1e7d":"<a id=\"prod_comp\"><\/a>\n### Production companies","ef47bba9":"SHAP provides more detailed information even if it may be more difficult to understand.\n\nFor example low budget has negavite impact on revenue, while high values usually tend to have higher revenue.","df4b0776":"<a id=\"ext_feats\"><\/a>\n### External features\nI'm adding external features from this kernel: https:\/\/www.kaggle.com\/kamalchhirang\/eda-feature-engineering-lgb-xgb-cat by kamalchhirang. All credit for these features goes to him and his kernel.","de98829d":"I'll create separate columns for top-15 genres.","fb3b7c9d":"Films, which are part of a collection usually have higher revenues. I suppose such films have a bigger fan base thanks to previous films.","cc7f3949":"<a id=\"overview\"><\/a>\n### overview","4e645282":"<a id=\"genres\"><\/a>\n### genres","0f19e706":"<a id=\"add_feat\"><\/a>\n### Additional feature generation","eae3a1a6":"There are only a couple of countries, which have distinctly higher revenues compared to others.","80cd1568":"<a id=\"budget\"><\/a>\n### Budget","d954da88":"<a id=\"stacking\"><\/a>\n### Stacking","d05afd04":"<a id=\"runtime\"><\/a>\n### runtime\n\nThe length of the film in minutes","7457aef4":"There are only a couple of companies, which have distinctly higher revenues compared to others.","7336f4eb":"We can see that number of films and total revenue are growing, which is to be expected. But there were some years in the past with a high number of successful films, which brought high revenue.","46e68f4a":"<a id=\"lang\"><\/a>\n### Spoken languages","d0ac25f0":"<a id=\"genres_\"><\/a>\n### Genres","247d7464":"We can see that important features native to LGB and top features in ELI5 are mostly similar. This means that our model is quite good at working with these features.","776836fe":"<a id=\"production_companies\"><\/a>\n### production_companies","cd233cb6":"<a id=\"or_lang\"><\/a>\n### original_language","e4e993b9":"Let's try to see which words have high impact on the revenue. I'll build a simple model and use ELI5 for this.","6813f233":"Most of films have 1-2 production companies, cometimes 3-4. But there are films with 10+ companies! Let's have a look at some of them.","fe70387d":"<a id=\"production_countries\"><\/a>\n### production_countries","49c04008":"## General information\n\nIn this kernel I'm working with data from TMDB Box Office Prediction Challenge. Film industry is booming, the revunues are growing, so we have a lot of data about films. Can we build models, which will be able to accurately predict film revenues? Could this models be used to make some changes in movies to increase their revenues even further? I'll try answer this questions in my kernel!\n\n![](https:\/\/i.imgur.com\/KVb5zO7.jpg)\n*(Screenshot of the main page of https:\/\/www.themoviedb.org\/)*","7ef25d5d":"Genres column contains named and ids of genres to which films belong. Most of films have 2-3 genres and 5-6 genres are possible. 0 and 7 are outliers, I think. Let's extract genres! I'll create a column with all genres in the film and also separate columns for each genre.\n\nBut at first let's have a look at the genres themselves.","70aee513":"<a id=\"status\"><\/a>\n### Status","f2b2bbd0":"<a id=\"oof\"><\/a>\n### OOF features based on texts","8545fd1c":"Films with homepage tend to generate more revenue! I suppose people can know more about the film thanks to homepage.","755b7e97":"Drama, Comedy and Thriller are popular genres."}}