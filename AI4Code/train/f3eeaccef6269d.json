{"cell_type":{"92e2a531":"code","3178627d":"code","78bfce81":"code","e6d7ff0c":"code","b2b58256":"code","817a46b8":"code","3a50bbb8":"code","07abf3a6":"code","6ba7d148":"code","03e97bad":"code","83da38a5":"code","eeb9ef5f":"code","56f42711":"code","43d1d1e4":"code","9a645107":"code","27104715":"code","27f32e1b":"code","bf0e61ab":"code","d650b07a":"code","239649a8":"code","6eac2b5f":"code","168ed8ce":"code","1f077587":"code","8ead5243":"code","6f16c165":"code","6c9eb7b5":"code","ed99a8b6":"code","1bab6ac5":"code","06d7bdd0":"code","bddb4b5f":"code","5d3e90e1":"code","fae1165b":"code","6907be42":"code","6483faab":"code","03c55c6a":"code","bf12d343":"code","88867568":"code","a3069101":"markdown","6f5eea45":"markdown","06dbbbac":"markdown","811582e8":"markdown","f3389578":"markdown","ccebc168":"markdown","d2c338b0":"markdown","f689716b":"markdown","102c87bf":"markdown"},"source":{"92e2a531":"# used ideas from:\n# https:\/\/www.kaggle.com\/mmrosenb\/whales-an-exploration \n# https:\/\/www.kaggle.com\/stehai\/duplicate-images","3178627d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [14, 9]\n\nimport collections\nfrom PIL import Image\n\nDIR = \"..\/input\"\n\ntrain = pd.read_csv(os.path.join(DIR, \"train.csv\"))\ntest = pd.read_csv(os.path.join(DIR, \"sample_submission.csv\"))","78bfce81":"train.shape, test.shape","e6d7ff0c":"train.head()","b2b58256":"train['Id'].value_counts()[:4]","817a46b8":"counted = train.groupby(\"Id\").count().rename(columns={\"Image\":\"image_count\"})\ncounted.loc[counted[\"image_count\"] > 80,'image_count'] = 80\nplt.figure()\nsns.countplot(data=counted, x=\"image_count\")\nplt.show()\nsns.distplot(counted[\"image_count\"], norm_hist=True, kde=False, hist_kws={'cumulative': True})","3a50bbb8":"image_count_for_whale = train.groupby(\"Id\", as_index=False).count().rename(columns={\"Image\":\"image_count\"})\nwhale_count_for_image_count = image_count_for_whale.groupby(\"image_count\", as_index=False).count().rename(columns={\"Id\":\"whale_count\"})\nwhale_count_for_image_count['image_total_count'] = whale_count_for_image_count['image_count'] * whale_count_for_image_count['whale_count']\nwhale_count_for_image_count['image_total_count_cum'] = whale_count_for_image_count[\"image_total_count\"].cumsum() \/ len(train)\nsns.barplot(x='image_count',y='image_total_count_cum',data=whale_count_for_image_count)","07abf3a6":"whale_count_for_image_count[:5]","6ba7d148":"whale_count_for_image_count[-3:]","03e97bad":"fig = plt.figure(figsize = (20, 15))\nfor idx, img_name in enumerate(train[train['Id'] == 'new_whale']['Image'][:12]):\n    y = fig.add_subplot(3, 4, idx+1)\n    img = cv2.imread(os.path.join(DIR,\"train\",img_name))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    y.imshow(img)\n\nplt.show()","83da38a5":"single_whales = train['Id'].value_counts().index[-12:]\nfig = plt.figure(figsize = (20, 15))\n\nfor widx, whale in enumerate(single_whales):\n    for idx, img_name in enumerate(train[train['Id'] == whale]['Image'][:1]):\n        axes = widx + idx + 1\n        y = fig.add_subplot(3, 4, axes)\n        img = cv2.imread(os.path.join(DIR,\"train\",img_name))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        y.imshow(img)\n\nplt.show()","eeb9ef5f":"topN=5\ntop_whales = train['Id'].value_counts().index[1:1+topN]\nfig = plt.figure(figsize = (20, 5*topN))\n\nfor widx, whale in enumerate(top_whales):\n    for idx, img_name in enumerate(train[train['Id'] == whale]['Image'][:4]):\n        axes = widx*4 + idx+1\n        y = fig.add_subplot(topN, 4, axes)\n        img = cv2.imread(os.path.join(DIR,\"train\",img_name))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        y.imshow(img)\n\nplt.show()","56f42711":"imageSizes_train = collections.Counter([Image.open(f'{DIR}\/train\/{filename}').size\n                        for filename in os.listdir(f\"{DIR}\/train\")])\nimageSizes_test = collections.Counter([Image.open(f'{DIR}\/test\/{filename}').size\n                        for filename in os.listdir(f\"{DIR}\/test\")])","43d1d1e4":"def isdf(imageSizes):\n    imageSizeFrame = pd.DataFrame(list(imageSizes.most_common()),columns = [\"imageDim\",\"count\"])\n    imageSizeFrame['fraction'] = imageSizeFrame['count'] \/ sum(imageSizes.values())\n    imageSizeFrame['count_cum'] = imageSizeFrame['count'].cumsum()\n    imageSizeFrame['count_cum_fraction'] = imageSizeFrame['count_cum'] \/ sum(imageSizes.values())\n    return imageSizeFrame\n\ntrain_isdf = isdf(imageSizes_train)\ntrain_isdf['set'] = 'train'\ntest_isdf = isdf(imageSizes_test)\ntest_isdf['set'] = 'test'","9a645107":"isizes = train_isdf.merge(test_isdf, how=\"outer\", on=\"imageDim\")\nisizes['total_count'] = isizes['count_x'] + isizes['count_y']\ndims_order = isizes.sort_values('total_count', ascending=False)[['imageDim']]\nlen(dims_order)","27104715":"isizes = pd.concat([train_isdf, test_isdf])","27f32e1b":"isizes.shape","bf0e61ab":"isizes.head()","d650b07a":"popularSizes = isizes[isizes['fraction'] > 0.002]\npopularSizes.shape","239649a8":"popularSizes.groupby('set').max()['count_cum_fraction']","6eac2b5f":"sns.barplot(x='imageDim',y='fraction',data = popularSizes, hue=\"set\")\n_ = plt.xticks(rotation=45)","168ed8ce":"import imagehash\n\ndef getImageMetaData(file_path):\n    with Image.open(file_path) as img:\n        img_hash = imagehash.phash(img)\n        return img.size, img.mode, img_hash\n\ndef get_img_duplicates_info(df, dataset):\n    \n    m = df.Image.apply(lambda x: getImageMetaData(os.path.join(DIR, dataset, x)))\n    df[\"Hash\"] = [str(i[2]) for i in m]\n    df[\"Shape\"] = [i[0] for i in m]\n    df[\"Mode\"] = [str(i[1]) for i in m]\n    df[\"Length\"] = df[\"Shape\"].apply(lambda x: x[0]*x[1])\n    df[\"Ratio\"] = df[\"Shape\"].apply(lambda x: x[0]\/x[1])\n    df[\"New_Whale\"] = df.Id == \"new_whale\"\n    \n    \n    img_counts = df.Id.value_counts().to_dict()\n    df[\"Id_Count\"] = df.Id.apply(lambda x: img_counts[x])\n    return df","1f077587":"train_dups = get_img_duplicates_info(train, \"train\")","8ead5243":"train_dups.head()","6f16c165":"t = train_dups.Hash.value_counts()\nt = t.loc[t>1]","6c9eb7b5":"\"Duplicate hashes: {}\".format(len(t))","ed99a8b6":"t","1bab6ac5":"t.index[0]","06d7bdd0":"train_dups[train_dups['Hash'] == t.index[0]].head()","bddb4b5f":"fig = plt.figure(figsize = (20, 10))\nfor idx, img_name in enumerate(train_dups[train_dups['Hash'] == t.index[0]]['Image'][:2]):\n    y = fig.add_subplot(3, 4, idx+1)\n    img = cv2.imread(os.path.join(DIR,\"train\",img_name))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    y.imshow(img)\n\nplt.show()","5d3e90e1":"test_dups = get_img_duplicates_info(test, \"test\")","fae1165b":"test_d = test_dups.Hash.value_counts()\ntest_d = test_d.loc[test_d>1]\n\"Duplicate hashes in test: {}\".format(len(test_d))","6907be42":"common_hashes = test_dups.merge(train_dups, how=\"inner\", on=\"Hash\", suffixes=(\"_test\",\"_train\"))\ncommon_hashes.head()","6483faab":"\"Duplicate hashes between train and test: {}\".format(len(common_hashes))","03c55c6a":"fig = plt.figure(figsize = (10, 10))\n\nfor idx, images in enumerate(common_hashes[['Image_train','Image_test']].values):\n    y = fig.add_subplot(len(common_hashes),2, idx*2+1)\n    img = cv2.imread(os.path.join(DIR,\"train\",images[0]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    y.imshow(img)\n\n    y = fig.add_subplot(len(common_hashes),2, idx*2+2)\n    img = cv2.imread(os.path.join(DIR,\"test\",images[1]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    y.imshow(img)\n\n    \nplt.show()","bf12d343":"# train duplicates - to remove:\ntrain_to_remove = train_dups[train_dups['Hash'] == t.index[0]].drop_duplicates('Hash')[['Image']]\ntrain_to_remove.to_csv(\"train_remove.csv\",index=False)\ntrain_to_remove.head()","88867568":"# easy answers in test:\neasy_peasy = common_hashes[['Image_test','Id_train']]\neasy_peasy.to_csv(\"test_easy.csv\", index=False)\neasy_peasy.head()","a3069101":"# Resolutions\n\n#### over 7000 unique resolutions but 39 most popular cover ~45% images (both in train and in test)","6f5eea45":"# Basic data exploration:\n\n1. distribution of images per whale\n1. viewing some images (same whale, different whale, 'new_whale')\n1. distribution of image resolution between train & test\n1. duplicate image analysis by perceptual hash","06dbbbac":"#### Below: each row shows pictures of one whale. I think it's quite easy to at least see similiar appearence there","811582e8":"## Distribution of images per whale is highly skewed.\n\n1. 2000+ whales have just one image\n2. Single whale with most images have 73 of them\n3. Images dsitribution:\n  1. almost 30% comes from whales with 4 or less images\n  1. almost 40% comes from 'new_whale' group\n  1. the rest 30% comes from whales with 5-73 images\n","f3389578":"### below each row shows images with the same pHash, left column from train, right from test","ccebc168":"# Let's see some images\n\n#### Some images of 'new_whale'","d2c338b0":"# Duplicates\n\n1. 1 duplicate in train set\n1. 3 duplicates between train and test\n1. totally different than in playground dataset: \n  1. [playground duplicates](https:\/\/www.kaggle.com\/stehai\/duplicate-images)\n  1. [solution that used duplicate information](https:\/\/www.kaggle.com\/martinpiotte\/whale-recognition-model-with-score-0-78563)\n","f689716b":"#### The only duplicate found in train dataset comes from the same whale.\n","102c87bf":"    #### Now some pictures of whales that have just 1 image: quite a large variance in colors"}}