{"cell_type":{"c080e871":"code","23a21f42":"code","1435811a":"code","104e2e33":"code","fd47dbed":"code","b0c79c5b":"code","4562536b":"code","43325705":"code","9e77d32f":"code","cfe2683b":"code","79c329d7":"code","1f565236":"code","ebd7cab6":"code","b32b00ff":"code","d6d6b2ec":"code","fd318997":"code","8a91a145":"code","25a37fca":"code","21d663ed":"code","b959fdea":"code","ce8d3950":"code","a2bbaee6":"code","64824ec8":"code","fef71c07":"code","92d0fcb7":"code","4283490c":"code","05d6b60e":"code","800a3989":"code","679fbd5f":"code","33827075":"code","48b37691":"code","50b5ffaa":"code","781c5b35":"code","f46946d2":"code","e0126178":"code","8f0395f0":"code","86a5b0fb":"code","3db52075":"code","3cbd513d":"code","75347217":"code","65ffc1ff":"code","5a970332":"code","d6deebcb":"code","be50fe5e":"code","41e96b9a":"code","abdc49d1":"code","dc045608":"code","cd03d16f":"code","f2681700":"code","e43c4945":"code","d6f3e08f":"code","5deebf7e":"code","ee22b9f5":"code","84e5abe0":"code","f042f4da":"code","0e6df341":"code","cdda33db":"code","f138e845":"code","b0e153c5":"code","eb123c17":"code","e534548d":"code","9e9cf16f":"code","ee9e84de":"code","447c6364":"code","b175cb4d":"markdown","6ad0ec4c":"markdown","288cd06d":"markdown","bc835d9f":"markdown","5d7afa31":"markdown","0ba61038":"markdown","dffeab54":"markdown","0e0e8253":"markdown","41443a0f":"markdown","af34f1ce":"markdown","d19b322f":"markdown","05bf62d3":"markdown","fd5ef3bc":"markdown","955a16cd":"markdown","f1efc876":"markdown","5710cb3b":"markdown","d239514c":"markdown","9260d9c3":"markdown","af324482":"markdown","cb468769":"markdown","b7ce760a":"markdown"},"source":{"c080e871":"from datetime import datetime\nimport itertools\nimport ast\nimport re\nimport itertools\nfrom collections import Counter\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n# nltk.download('vader_lexicon')\n# nltk.download('punkt')\n# nltk.download('stopwords')\n# nltk.download('wordnet')\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nsns.set(color_codes=True)","23a21f42":"#folder = \"data\/kaggle\"\nfolder = \"..\/input\"","1435811a":"# Collect the data from imdb\nimdb_details = pd.read_csv(f\"{folder}\/imdb_details.csv\")","104e2e33":"imdb_details.head()","fd47dbed":"# Let's just draw the rating in function of the episode\nfig,ax=plt.subplots(figsize=[12,6])\n\nimdb_details.plot(ax = ax,kind = \"line\", y=\"rating\", legend=None)\n\nplt.xlabel(\"Episodes\")\nplt.ylabel(\"IMDB rating\")\n\nplt.show()","b0c79c5b":"# Let's draw the avergae rating per season\nfig,ax=plt.subplots(figsize=[12,6])\n\nimdb_details.groupby([\"season\"]).mean()[\"rating\"].plot(ax = ax,kind = \"line\", legend=None)\n\nplt.xlabel(\"Season\", fontsize = 15)\nplt.ylabel(\"Average IMDB rating\", fontsize = 15)\nplt.xticks(fontsize = 13)\nplt.yticks(fontsize = 13)\n\nplt.ylim([0,10])\n\nplt.show()\nfig.savefig('imdb_season_rating.png')","4562536b":"# Let's draw the number of review per season\nfig,ax=plt.subplots(figsize=[12,6])\n\nimdb_details.groupby([\"season\"]).mean()[\"nbr_votes\"].plot(ax = ax,kind = \"line\", legend=None)\n\nplt.xlabel(\"Season\", fontsize = 15)\nplt.ylabel(\"Average count of reviews\", fontsize = 15)\nplt.xticks(fontsize = 13)\nplt.yticks(fontsize = 13)\n\nplt.ylim([0,6000])\n\n\nplt.show()\n\nfig.savefig('imdb_season_ratingcount.png')","43325705":"dw_guide = pd.read_csv(f\"{folder}\/dwguide.csv\")","9e77d32f":"# Make some cleaning\ndw_guide[\"AI\"] = dw_guide.apply(lambda row: float(row[\"AI\"]),axis=1)\ndw_guide[\"views\"] = dw_guide.apply(lambda row: float(row[\"views\"].replace(\"m\",\"\")),axis=1)\ndw_guide[\"date\"] = dw_guide.apply(lambda row: row['broadcastdate'] + \" \" + row['broadcasthour'] ,axis=1)\ndw_guide[\"broadcastdate_datetime\"] = dw_guide.apply(lambda row: datetime.strptime(row['date'],\"%d %b %Y %I:%M%p\") ,axis=1)\ndw_guide[\"broadcastdate_hour\"] = dw_guide.apply(lambda row: row['broadcastdate_datetime'].hour,axis=1)\ndw_guide[\"broadcastdate_year\"] = dw_guide.apply(lambda row: row['broadcastdate_datetime'].year,axis=1)\n\n#Works on the title of the episode (for the classic area)\ndw_guide[\"title2\"] = dw_guide[\"title\"].apply(lambda x:x.split(\":\")[0])\n\n# Clean the dirty string list\ndw_guide[\"cast\"] = dw_guide[\"cast\"].apply(lambda x:ast.literal_eval(x))\ndw_guide[\"crew\"] = dw_guide[\"crew\"].apply(lambda x:ast.literal_eval(x))","cfe2683b":"# Estimate if the episode was in the classic era or the modern era\ndef is_classicperiod(x):\n    if x>=2005:\n        return False\n    return True\n\ndw_guide[\"is_classicperiod\"] = dw_guide[\"broadcastdate_year\"].apply(lambda x:is_classicperiod(x))","79c329d7":"dw_guide.sort_values([\"episodenbr\"], ascending = True, inplace = True)\ndw_guide.reset_index(inplace = True, drop = True)","1f565236":"dw_guide.head()","ebd7cab6":"# Collect the right columns for the analysis\ndw_guide_cast = dw_guide[[\"episodenbr\",\"title\",\"title2\",\"broadcastdate_datetime\",\"broadcastdate_year\",\"is_classicperiod\",\"cast\"]].reset_index()","b32b00ff":"# Rebuild the casting of the show\n#casting = dw_guide_cast[\"cast\"].explode().to_frame().reset_index() # Run with pandas 0.25\n\n# Use an old trick to do it\ncasting = pd.DataFrame({'index':dw_guide_cast[\"index\"].repeat(dw_guide_cast[\"cast\"].str.len()),'cast':np.concatenate(dw_guide_cast[\"cast\"].values)})","d6d6b2ec":"# Rebuild the casting of the show\ncasting[\"name\"] = casting[\"cast\"].apply(lambda x:x[\"name\"])\ncasting[\"role\"] = casting[\"cast\"].apply(lambda x:x[\"role\"])\n\n# Get an uncredited flag\ndef is_uncredited(x):\n    if \"uncredited\" in x:\n        return True\n    return False\ncasting[\"is_uncredited\"] = casting[\"name\"].apply(lambda x:is_uncredited(x))\n# Drop the uncredited tag in the name\ncasting[\"name\"] = casting[\"name\"].apply(lambda x:x.replace(\" (uncredited)\",\"\"))\n\n\ndel casting[\"cast\"]\ndel dw_guide_cast[\"cast\"]","fd318997":"casting.head()","8a91a145":"# Upgrade the general informations on the casting\ndw_guide_cast = dw_guide_cast.reset_index().merge(casting,on = [\"index\"])\ndel dw_guide_cast[\"index\"]","25a37fca":"dw_guide_cast.head()","21d663ed":"# Build some statistic on the actor who played in the show (if they are bask or not etc)\nagg_func = {\n    \"broadcastdate_year\":[\"min\",\"max\"],\n    \"episodenbr\":[pd.Series.nunique]\n}\nstats_cast = dw_guide_cast[dw_guide_cast[\"is_uncredited\"] == False].groupby([\"name\"]).agg(agg_func).reset_index()\nstats_cast[\"deltatime\"] = stats_cast[\"broadcastdate_year\",\"max\"] - stats_cast[\"broadcastdate_year\",\"min\"]\n\nstats_cast.sort_values([\"deltatime\"], ascending = False, inplace = True)\n\nold_columns = stats_cast.columns\nnew_columns = []\nfor column in old_columns:\n    new_columns.append(f\"{column[0]}_{column[1]}\")\nstats_cast.columns = new_columns","b959fdea":"# Determine if the actor start or end in the modern age of the show\ndef is_threshold(x, limit, is_superior = True):\n    if is_superior:\n        if x >= limit:\n            return True\n        return False\n    else:\n        if x <= limit:\n            return True\n        return False\n    \nstats_cast['firstappereance_ismodern'] = stats_cast[\"broadcastdate_year_min\"].apply(lambda x:is_threshold(x, 2005))\nstats_cast['lastappereance_ismodern'] = stats_cast[\"broadcastdate_year_max\"].apply(lambda x:is_threshold(x, 2005))\n\nstats_cast[\"appereance_modernage\"] = stats_cast.apply(lambda x:f\"FA:{x['firstappereance_ismodern']} \/ LA:{x['lastappereance_ismodern']}\",axis = 1)","ce8d3950":"stats_cast.head(5)","a2bbaee6":"# Number of actors in the show\nlen(stats_cast)","64824ec8":"# Determine the proportion of episode played in function of the profile of the actors\nstats = stats_cast[[\"episodenbr_nunique\",\"appereance_modernage\"]].groupby([\"appereance_modernage\"]).describe().unstack(1).to_frame().reset_index()\nstudy_episodes = pd.pivot_table(stats, columns = [\"level_1\"], index = [\"appereance_modernage\"], values = [0])","fef71c07":"# Renaming and selection of the right columns\nold_columns = study_episodes.columns\nnew_columns = []\nfor column in old_columns:\n    new_columns.append(f\"{column[1]}\")\nstudy_episodes.columns = new_columns\nstudy_episodes = study_episodes[[\"min\",\"25%\",\"50%\",\"75%\",\"max\",\"count\"]]","92d0fcb7":"study_episodes","4283490c":"# Let's see now the actor and their count of role\ncount_role = dw_guide_cast[dw_guide_cast[\"is_uncredited\"] == False].groupby([\"name\"]).nunique()[\"role\"].reset_index()\ncount_role.sort_values([\"role\"], ascending = False, inplace = True)","05d6b60e":"count_role.head()","800a3989":"dw_guide_cast[dw_guide_cast[\"name\"] == \"Nicholas Briggs\"][\"role\"].unique()","679fbd5f":"# Collect the scripts\nall_scripts = pd.read_csv(f\"{folder}\/all-scripts.csv\")\nall_scripts.sort_values([\"doctorid\",\"episodeid\"],ascending = True,inplace = True)","33827075":"all_scripts.head()","48b37691":"# Add details on the type of doctor\ndef clean_name(row):\n    clean_name = str(row[\"details\"])\n    for piece in [\" [OC]\",\"[OC]\"]:\n        if piece in clean_name:\n            clean_name = clean_name.replace(piece,\"\")\n    \n    if \"DOCTOR\" in clean_name:\n        return f\"DOCTOR_{row['doctorid']}\"\n    \n    return clean_name\n\nall_scripts[\"details\"] = all_scripts.apply(lambda row: clean_name(row) ,axis=1)","50b5ffaa":"# Focus on the talks of the episodes\nall_talks = all_scripts[all_scripts[\"type\"] == \"talk\"]\nall_talks.reset_index(inplace = True, drop = True)","781c5b35":"#Get max idx for each episode (to estimate the progression during a speech of the episode)\nmax_idx = all_scripts.groupby([\"episodeid\"]).max()[\"idx\"].to_frame()\nmax_idx.reset_index(inplace = True)\nmax_idx.columns= [\"episodeid\",\"max_idx\"]","f46946d2":"# upgrade the talks \nall_talks = all_talks.merge(max_idx,on = [\"episodeid\"])","e0126178":"# Estimate the progression and make some kind of buckets of progression\nall_talks[\"progression\"] = 100.0 * all_talks[\"idx\"] \/ all_talks[\"max_idx\"]\nall_talks[\"int_progression\"] = all_talks[\"progression\"].astype(int)\n\nfor block in [2,5,10]:\n    all_talks[f\"progression_{block}%\"] = all_talks[\"int_progression\"] \/ block\n    all_talks[f\"progression_{block}%\"] = all_talks[f\"progression_{block}%\"].astype(int)\n\ndel all_talks[\"max_idx\"]","8f0395f0":"# Sorting time\nall_talks.sort_values([\"episodeid\",\"idx\"], inplace = True)","86a5b0fb":"all_talks.head()","3db52075":"# Make some string cleaning\nall_talks[\"text\"] = all_talks[\"text\"].apply(lambda x:re.sub(\"[^a-zA-Z]\",\" \", str(x)))\nall_talks[\"text\"] = all_talks[\"text\"].apply(lambda row: row.lower())","3cbd513d":"# Make tokenisation of the scripts\nall_talks[\"tokenised_text\"] = all_talks[\"text\"].apply(lambda x:nltk.sent_tokenize(x) )","75347217":"# Quick check\nall_talks.sample(frac = 0.1).head()","65ffc1ff":"# Make word tokenisation\ndef get_wordtokenise(container):\n    new_container = []\n    for elt in container:\n        new_container.append(nltk.tokenize.word_tokenize(elt))\n    return new_container\n\nall_talks[\"word_tokenised_text\"] = all_talks[\"tokenised_text\"].apply(lambda x:get_wordtokenise(x))\nall_talks[\"merged_word_tokenised_text\"] = all_talks[\"word_tokenised_text\"].apply(lambda x:list(itertools.chain.from_iterable(x)))","5a970332":"all_talks.sample(frac = 0.1).head()","d6deebcb":"# Collect all the words used (after tokenisation)\nall_words = list(itertools.chain.from_iterable(all_talks[\"merged_word_tokenised_text\"].tolist()))","be50fe5e":"# Estimate the most common word used in all the script\nfdist = nltk.probability.FreqDist(all_words)\npd.DataFrame(fdist.most_common(10), columns = [\"word\",\"count\"])","41e96b9a":"# Get the stopwords\nstopwords_list = nltk.corpus.stopwords.words(\"english\")\n# But don't drop the who word please\ndel stopwords_list[stopwords_list.index(\"who\")]","abdc49d1":"def drop_stopwords(container,stop_words):\n    new_container = []\n    for subcontainer in container:\n        new_subcontainer = []\n        for elt in subcontainer:\n            if elt not in stopwords_list:\n                new_subcontainer.append(elt)\n        new_container.append(new_subcontainer)\n    return new_container\n\nall_talks[\"word_tokenised_text_nostopword\"] = all_talks[\"word_tokenised_text\"].apply(lambda x:drop_stopwords(x,stopwords_list))\nall_talks[\"merged_word_tokenised_text_nostopword\"] = all_talks[\"word_tokenised_text_nostopword\"].apply(lambda x:list(itertools.chain.from_iterable(x)))","dc045608":"# Collect all the words used (after tokenisation and drop of the stopwords)\nall_words = list(itertools.chain.from_iterable(all_talks[\"merged_word_tokenised_text_nostopword\"].tolist()))","cd03d16f":"# Estimate the most common word used in all the script\nfdist = nltk.probability.FreqDist(all_words)\npd.DataFrame(fdist.most_common(10), columns = [\"word\",\"count\"])","f2681700":"# Define the stemmer (to just get the root )\nps = nltk.stem.PorterStemmer()","e43c4945":"# It's time to find the root of the word\ndef get_stem(container,ps):\n    new_container = []\n    for subcontainer in container:\n        new_subcontainer = []\n        for elt in subcontainer:\n            new_subcontainer.append(ps.stem(elt))\n        new_container.append(new_subcontainer)\n    return new_container\n\nall_talks[\"word_tokenised_text_nostopword_stem\"] = all_talks[\"word_tokenised_text_nostopword\"].apply(lambda x:get_stem(x,ps))\nall_talks[\"merged_word_tokenised_text_nostopword_stem\"] = all_talks[\"word_tokenised_text_nostopword_stem\"].apply(lambda x:list(itertools.chain.from_iterable(x)))","d6f3e08f":"all_talks.sample(frac = 0.1).head()","5deebf7e":"# Collect all the words used (after tokenisation and drop of the stopwords)\nall_stems = list(itertools.chain.from_iterable(all_talks[\"merged_word_tokenised_text_nostopword_stem\"].tolist()))","ee22b9f5":"# Estimate the most common word used in all the script\nfdist = nltk.probability.FreqDist(all_stems)\npd.DataFrame(fdist.most_common(10), columns = [\"word\",\"count\"])","84e5abe0":"# Block to determine the word that are arriving before or after the word doctor\ncount_starter = 0\ncount_end = 0\ncount_only = 0\n\ndict_count = {\n    \"before\":[],\n    \"after\":[]\n}\n\nfor elt in all_talks[\"word_tokenised_text_nostopword\"]:\n    for quote in elt:\n        if \"doctor\" in quote:\n            quote_s = quote\n            concordance = nltk.ConcordanceIndex(quote_s)\n            detection = concordance.offsets('doctor')\n            if len(quote_s) != 1:\n                # Works on what's before doctor\n                for idx in detection:\n                    if idx == 0:\n                        count_starter += 1\n                        dict_count[\"after\"].append(quote_s[idx+1])\n                    elif idx == len(quote) - 1:\n                        count_end += 1\n                        dict_count[\"before\"].append(quote_s[idx-1])\n                    else:\n                        dict_count[\"after\"].append(quote_s[idx+1])\n                        dict_count[\"before\"].append(quote_s[idx-1])\n            else:\n                count_only += 1\n                                        \nprint(\"Count first word\", count_starter)\nprint(\"Count last word\", count_end)\nprint(\"Count only\", count_only)","f042f4da":"list(Counter(dict_count[\"before\"]).items())","0e6df341":"#Count_occurence in each list\ncount_before = pd.DataFrame(list(Counter(dict_count[\"before\"]).items()), columns = [\"word\",\"count_before\"])\ncount_after = pd.DataFrame(list(Counter(dict_count[\"after\"]).items()), columns = [\"word\",\"count_after\"])\n\n# Merge the count\ncount = pd.merge(count_before,count_after,on = \"word\",how='outer').fillna(0)\ncount[\"count_all\"] = count[\"count_before\"] + count[\"count_after\"]","cdda33db":"count.sort_values([\"count_all\"],ascending = False,inplace = True)\ncount.reset_index(inplace = True, drop = True)","f138e845":"count.head(20)","b0e153c5":"stopwords = set(STOPWORDS)\n#fig,ax=plt.subplots(figsize=[12,12])\ncnt = 1\nfor character in [\"ROSE\",\"MARTHA\",\"DONNA\",\"AMY\",\"CLARA\",\"BILL\"]:\n    print(character)\n    speech = all_talks[all_talks[\"details\"] == character]\n    text = \"\".join(sentence for sentence in speech[\"text\"].tolist())\n    wordcloud = WordCloud(stopwords=stopwords_list,background_color=\"white\").generate(text)\n\n    \n    all_words = list(itertools.chain.from_iterable(speech[\"merged_word_tokenised_text_nostopword\"].tolist()))\n    fdist = nltk.probability.FreqDist(all_words)\n    print(pd.DataFrame(fdist.most_common(10), columns = [\"word\",\"count\"]))\n    \n    fig,ax=plt.subplots(figsize=[12,12])\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    #plt.title(f\"Wordcloud : {character}\", fontsize = 15)\n    plt.show()\n    \n    fig.tight_layout()\n    fig.savefig(f\"wordcloud_{character}.png\")","eb123c17":"# Define the sentiment analyser\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()","e534548d":"# Let's do the analysis of all the scripts\ndata = []\nfor i,row in all_talks.iterrows():\n    sentence = row[\"text\"]\n    sa = sid.polarity_scores(sentence)\n    sa[\"index\"] = i\n    data.append(sa)\n\nsentiment_analysis = pd.DataFrame(data) \nsentiment_analysis.set_index([\"index\"],inplace = True)\nsentiment_analysis.columns = [f\"ss_{column}\"for column in sentiment_analysis.columns]","9e9cf16f":"all_talks_sa = pd.concat([all_talks,sentiment_analysis],axis = 1)\nall_talks_sa.sort_values([\"doctorid\",\"episodeid\",\"idx\"],inplace = True)","ee9e84de":"# Determine the text said by the doctor in general\ndef is_containingword(x,word):\n    if word in x:\n        return True\n    return False\n\nall_talks_sa[\"is_doctorspeech\"] = all_talks_sa[\"details\"].apply(lambda x:is_containingword(x,\"DOCTOR\"))","447c6364":"# Let's make a simple wordcloud to detect the negative word said by the doctor\n\n#Focus on the doctor talk\ndoctor_speech = all_talks_sa[all_talks_sa[\"is_doctorspeech\"]]\n# focus on the negative speech of the doctor\nspeech = doctor_speech[doctor_speech[\"ss_neg\"] >= 0.5]\n\nall_words = list(itertools.chain.from_iterable(speech[\"merged_word_tokenised_text_nostopword\"].tolist()))\nfdist = nltk.probability.FreqDist(all_words)\nprint(pd.DataFrame(fdist.most_common(10), columns = [\"word\",\"count\"]))\n\ntext = \"\".join(sentence for sentence in speech[\"text\"].tolist())\nwordcloud = WordCloud(stopwords=stopwords_list,background_color=\"white\").generate(text)\n\nfig,ax=plt.subplots(figsize=[12,12])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\n#plt.title(f\"Wordcloud : {character}\", fontsize = 15)\nplt.show()\n\nfig.tight_layout()\nfig.savefig(f\"wordcloud_negative_doctor.png\")","b175cb4d":"# Doctor Who ?\n\nIn this notebook there is a gentle introduction to the dataset associated to Doctor who that I just build. Nothing really fancy just an exploration of the data and some basic usage of the NLTK library to find some informations.","6ad0ec4c":"### Let's tokenise the scripts","288cd06d":"*The rating on IMDB is between 0 and 10, in the dataset this is the average rating display on the main page of the episode (maybe I should collect individual rating and comments)*\n\n**Notes**: As we can see the rating of the IMDB user seems very different in function of the episode but it never fall under 5. From this perspective it can be noticed that the last episode seems to be less appreciate than the others.\n","bc835d9f":"**Notes**: The most common words seems the small words , so we need to make some cleaning","5d7afa31":"**Notes**: The word doctor seems to be the one that is the more connect to doctor, the word who is just the 20th word the more related to doctor (on 4298 so that's not too bad)","0ba61038":"**Notes**: This study highlight the actor that are playing some characters in the costume like the daleks, cybermen . It's highlighting the voice actor behind the Dalek for exemple.","dffeab54":"### Nake some cleaning of the small words","0e0e8253":"**Notes**: This analyse is illustrating the fact that the actors of the classic era , where doing more than 1 episode it could be explained by the fact that the story was divide in multiple episodes (so you need to keep the same actors). For the modern age, most of the actors are doing only one episode.\n\nThe actors that can be find on the two eras have an important number of episodes, that could be explained by the fact that some actors that are playing the doctor in the classic era (so a high number of episodes).","41443a0f":"### Works on the stemmer","af34f1ce":"**Notes**: The word doctor","d19b322f":"## Analysis of the docto who guide data \n\nIn this part, I am going to see some details on the show in terms of broadcast and distribution.","05bf62d3":"### Let's make a small sentiment analysis on the doctor speech","fd5ef3bc":"**Notes**:In this case in avergae all the season seems to be appreciate with an average rating around 8 but the last season seems to have find his audience with an average rating around 6. This difference between the previous season can be explained by\n* the actor who was playing the doctor changed from Peter Capaldi to Jodie Whittaker, the new actress will be the first woman to interpret the Doctor (so that could have impacted the IMDB reviewers)\n* the showrunner is not Steven Moffat that relaunch the show with the modern area, so the tone and rythm could be different and the IMDB reviewers wil not like too\n\nI have no opinion on this last season because I still didn't finsih the last season with Peter Capaldi, but I was living in the UK and I saw all the hate on the newspapers about the choice of Jodie Whittaker to be the Doctor so maybe there is on the number of reviews a trace of this hate by seeing a very important number of reviews to do some bashing.\n","955a16cd":"### Focus on the casting of actors","f1efc876":"**Notes**: It doesn't seems that there is some review bombing on the last season of ther show, so it seems to be a dislike of this new season.","5710cb3b":"## Get details on the wordcloud of some companions","d239514c":"## IMDB (Modern age)\n\nIn this part, an analysis of the rating of the different seasons will be done to see if there is general trends in function of the new doctor etc.","9260d9c3":"### Dive in NLTK","af324482":"## Analyse of the scripts","cb468769":"## Get details on the doctor word concordance","b7ce760a":"**Notes**: Now we can start to have some interesting trends on the most used word, and guess what doctor is the most common !!!"}}