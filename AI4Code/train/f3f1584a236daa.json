{"cell_type":{"4e6a1c6e":"code","a529bb50":"code","5b34f2cb":"code","e4eb55b0":"code","ffca125e":"code","479d15ca":"code","b23613d8":"code","321e7856":"code","ff2f242d":"code","4669b3ea":"code","0e64662e":"code","02f68e76":"code","f38821c5":"code","5f985bc6":"code","f3f7cf37":"code","3819a097":"code","1eef9c47":"code","bc506f58":"code","ffcc7639":"code","cd2ddfa9":"code","8d3caaf4":"code","5bffe565":"code","3df8a04b":"code","8ada95f2":"code","c6e5b012":"code","56da76e4":"code","1bfe48fe":"code","5ec50083":"code","11d9aa72":"code","5dbdaad8":"code","3ad9edd0":"code","37ee4fb3":"code","12cc4cd4":"code","e07938dd":"code","d0dc61f2":"code","ac667f42":"code","8606d972":"code","f9456782":"markdown","2dcbadd8":"markdown","45961ceb":"markdown","3b3ddf46":"markdown","8ce24c58":"markdown","cc7259ce":"markdown","20befeb2":"markdown","1d1fb370":"markdown","34ff3b80":"markdown","03a5a4ab":"markdown","a52fa2e7":"markdown","9c6a17da":"markdown","b6e68f27":"markdown","bfced42b":"markdown","7f3bbe32":"markdown","f11916ca":"markdown","5f2fc962":"markdown","12092b94":"markdown","d427e555":"markdown"},"source":{"4e6a1c6e":"import warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n#supressing warnings for readability\nwarnings.filterwarnings(\"ignore\")\n\n# To plot pretty figures directly within Jupyter\n%matplotlib inline\n\n# choose your own style: https:\/\/matplotlib.org\/3.1.0\/gallery\/style_sheets\/style_sheets_reference.html\nplt.style.use('seaborn-whitegrid')\n\n# Go to town with https:\/\/matplotlib.org\/tutorials\/introductory\/customizing.html\n# plt.rcParams.keys()\nmpl.rc('axes', labelsize=14, titlesize=14)\nmpl.rc('figure', titlesize=20)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# contants for figsize\nS = (8,8)\nM = (12,12)\nL = (14,14)\n\n# pandas options\npd.set_option(\"display.max.columns\", None)\npd.set_option(\"display.max.rows\", None)\npd.set_option(\"display.precision\", 2)\n\n# read data\ndf = pd.read_parquet('..\/input\/nhs-proms-case-study\/data\/interim\/knee-ccg.parquet')","a529bb50":"# handy function to select oks columns\ndef oks_questions(t='t0'):\n  return [\n    col for col in df.columns if col.startswith(f\"oks_{t}\") and not col.endswith(\"_score\")\n]\n\n# replace sentinel values in oks columns\n# note we are doing imputation on original dataframe (rather than in pipeline later on)\n# so we can perform it prior to StratefiedShuffleSplit\noks_no9 = oks_questions('t0') + oks_questions('t1')\nimpute_oks = SimpleImputer(missing_values=9, strategy=\"most_frequent\")\ndf.loc[:, oks_no9] = impute_oks.fit_transform(df[oks_no9])\n\n# group columns t0\nage_band = [\"age_band\"]\ngender = [\"gender\"]\nage_band_categories = sorted([x for x in df.age_band.unique() if isinstance(x, str)])\ncomorb = [\n    \"heart_disease\",\n    \"high_bp\",\n    \"stroke\",\n    \"circulation\",\n    \"lung_disease\",\n    \"diabetes\",\n    \"kidney_disease\",\n    \"nervous_system\",\n    \"liver_disease\",\n    \"cancer\",\n    \"depression\",\n    \"arthritis\",\n]\nboolean = [\"t0_assisted\", \"t0_previous_surgery\", \"t0_disability\"]\neq5d = [\"t0_mobility\", \"t0_self_care\", \"t0_activity\", \"t0_discomfort\", \"t0_anxiety\"]\neq_vas = [\"t0_eq_vas\"]\ncategorical = [\"t0_symptom_period\", \"t0_previous_surgery\", \"t0_living_arrangements\"]\noks_score = [\"oks_t0_score\"]\n\n# add number of comorbidities as extra feature\nimpute_comorb = SimpleImputer(missing_values=9, strategy=\"constant\", fill_value=0)\ndf.loc[:, comorb] = impute_comorb.fit_transform(df[comorb])\ndf[\"n_comorb\"] = df.loc[:, comorb].sum()\n\n\n# define outcome Y\nCUT_OFF_PAIN = 4\nCUT_OFF_FUNCTIONING = 26\n\nfor t in (\"t0\", \"t1\"):\n    df[f\"oks_{t}_pain_total\"] = df[f\"oks_{t}_pain\"] + df[f\"oks_{t}_night_pain\"]\n    df[f\"oks_{t}_functioning_total\"] = (\n        df.loc[:, [col for col in oks_questions(t) if \"pain\" not in col]]\n        .sum(axis=1)\n    )\n    df[f\"y_{t}_pain_good\"] = df[f\"oks_{t}_pain_total\"].apply(\n        lambda s: True if s >= CUT_OFF_PAIN else False\n    )\n    df[f\"y_{t}_functioning_good\"] = df[f\"oks_{t}_functioning_total\"].apply(\n        lambda s: True if s >= CUT_OFF_FUNCTIONING else False\n    )\n\n# define binary outcome parameter\ndf[\"y_binary\"] = np.logical_and(df.y_t1_pain_good, df.y_t1_functioning_good)\n\n# Only using 1 split for stratefied sampling, more folds are used later on in cross-validation\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\nfor train_index, test_index in split.split(df, df[\"y_binary\"]):\n    df_train = df.loc[train_index]\n    df_test = df.loc[test_index]\n    \ny_train_pain_good = df_train.y_t1_pain_good\ny_train_pain_good = df_train.y_t1_functioning_good\ny_train_binary = df_train.y_binary\n\ny_test_pain_good = df_test.y_t1_pain_good\ny_test_pain_good = df_test.y_t1_functioning_good\ny_test_binary = df_test.y_binary","5b34f2cb":"y0 = pd.crosstab(df_train.y_t0_pain_good, df_train.y_t0_functioning_good, normalize=True)\ny0","e4eb55b0":"y1 = pd.crosstab(df_train.y_t1_pain_good, df_train.y_t1_functioning_good, normalize=True)\ny1","ffca125e":"# same pipeline as lecture 3\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\n# preprocessing pipelines for specific columns\nage_band_pipe = Pipeline(\n    steps=[\n        (\"impute\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n        (\"ordinal\", OrdinalEncoder(categories=[age_band_categories])),\n    ]\n)\ngender_pipe = Pipeline(\n    steps=[\n        (\"impute\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder()),\n    ]\n)\n\n# ColumnTransformer on all included columns.\n# Note columns that are not specified are dropped by default\ntransformers = {\n    \"age\": (\"age\", age_band_pipe, age_band),\n    \"gender\": (\"gender\", gender_pipe, gender),\n    \"comorb\": (\n        \"comorb\",\n        'passthrough',\n        comorb,\n    ),\n    \"categorical\": (\n        \"categorical\",\n        SimpleImputer(missing_values=9, strategy=\"most_frequent\"),\n        boolean + eq5d + categorical,\n    ),\n    \"oks\": (\n        \"oks\",\n        'passthrough',\n        oks_questions('t0'),\n    ),\n    \"eq_vas\": (\"eqvas\", SimpleImputer(missing_values=999, strategy=\"median\"), eq_vas),\n}\nprep = ColumnTransformer(\n    transformers=[v for _, v in transformers.items()])","479d15ca":"# list of columns for convenience\n# https:\/\/stackoverflow.com\/questions\/54646709\/sklearn-pipeline-get-feature-name-after-onehotencode-in-columntransformer\nX_train = prep.fit_transform(df_train)\nX_test = prep.fit_transform(df_test)\nX_columns = pd.Series(\n    age_band\n    + prep.named_transformers_[\"gender\"][\"onehot\"].get_feature_names().tolist()\n    + comorb\n    + boolean\n    + eq5d\n    + categorical\n    + oks_questions()\n    + oks_score\n    + eq_vas\n)","b23613d8":"X_columns","321e7856":"%%time\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\n\nsgd = Pipeline(\n    steps=[(\"prep\", prep), (\"sgd\", SGDClassifier(loss=\"hinge\", penalty=\"l2\", tol=1e-3))]\n)\n\n# https:\/\/scikit-learn.org\/stable\/tutorial\/statistical_inference\/putting_together.html\nsgd_parameters = {\"sgd__max_iter\": [10]} #, 100, 1000]}\n\nsgd_search = GridSearchCV(sgd, sgd_parameters, cv=5)\n_ = sgd_search.fit(df_train, y_train_binary)  # assign output of fit to dummy parameter to prevent printing in notebook","ff2f242d":"confusion_matrix(y_train_binary, sgd_search.best_estimator_.predict(df_train), normalize='all').round(3)","4669b3ea":"cross_val_score(sgd_search.best_estimator_, df_train, y_train_binary, cv=3, scoring=\"roc_auc\").round(3)","0e64662e":"%%time\nfrom sklearn.tree import DecisionTreeClassifier\n\ncart = Pipeline(\n    steps=[\n        (\"prep\", prep),\n        (\"cart\", DecisionTreeClassifier()),\n    ]\n)\n\ncart_parameters = {\n'cart__max_depth': [10, 30],\n'cart__min_samples_leaf': [0.02, 0.05, 0.1, 0.2],\n}\n\ncart_search = GridSearchCV(cart, cart_parameters, cv=5)\n_ = cart_search.fit(df_train, y_train_binary);","02f68e76":"confusion_matrix(y_train_binary, cart_search.best_estimator_.predict(df_train), normalize='all').round(3)","f38821c5":"cross_val_score(cart_search.best_estimator_, df_train, y_train_binary, cv=3, scoring=\"roc_auc\").round(3)","5f985bc6":"%%time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\nsvm = Pipeline(\n    steps=[\n        (\"prep\", prep),\n        (\"scaler\", StandardScaler()),\n        (\"clf\", LinearSVC(C=1, loss=\"hinge\")),\n    ]\n)\n_ = svm.fit(df_train, y_train_binary);","f3f7cf37":"confusion_matrix(y_train_binary, svm.predict(df_train), normalize='all').round(3)","3819a097":"cross_val_score(svm, df_train, y_train_binary, cv=3, scoring=\"roc_auc\").round(3)","1eef9c47":"from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve\n\nclfs = {\n    \"sgd\": sgd_search.best_estimator_,\n    \"cart\": cart_search.best_estimator_,\n    \"svm\": svm,\n}\n\nfig_roc, ax_roc = plt.subplots(figsize=S)\nplt.plot([0, 1], [0, 1], 'k--')\nfor k,v in clfs.items():\n    plot_roc_curve(v, df_test, y_test_binary, name=k, ax=ax_roc);","bc506f58":"fig_prc, ax_prc = plt.subplots(figsize=S)\nplt.plot([0,1], [ y1.iloc[1,1],  y1.iloc[1,1]], 'k--')\nfor k,v in clfs.items():\n    plot_precision_recall_curve(v, df_test, y_test_binary, name=k, ax=ax_prc)","ffcc7639":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrf = Pipeline(\n    steps=[(\"prep\", prep), (\"rf\", RandomForestClassifier(random_state=42, n_jobs=-1))]\n)\nrf_parameters = {\n    \"rf__max_samples\": [1000],  # don't use the whole dataset: takes too long!\n    \"rf__n_estimators\": [100],\n    \"rf__criterion\": [\"gini\"],\n    \"rf__max_features\": [\"auto\"],\n    \"rf__min_samples_leaf\": [1, 2, 5],\n    \"rf__max_depth\": [10, 15],\n    \"rf__oob_score\": [True, False],\n}\nrf_search = GridSearchCV(rf, rf_parameters, cv=5)\n_ = rf_search.fit(df_train, y_train_binary)","cd2ddfa9":"# take very long, so not run\n\n# %%time\n# from sklearn.ensemble import GradientBoostingClassifier\n\n# gb = Pipeline(\n#     steps=[(\"prep\", prep), (\"gb\", GradientBoostingClassifier())]\n# )\n# gb_parameters = {\n#     \"gb__learning_rate\": [1.0],\n#     \"gb__n_estimators\": [100, 200],\n#     \"gb__min_samples_leaf\": [1, 2, 5],\n#     \"gb__max_features\": [\"auto\"],\n#     \"gb__max_depth\": [2, 3, 5],\n# }\n# gb_search = GridSearchCV(gb, gb_parameters, cv=5)\n# _ = gb_search.fit(df_train, y_train_binary)","8d3caaf4":"%%time\nimport xgboost\n\nxgb = Pipeline(steps=[(\"prep\", prep), (\"xgb\", xgboost.XGBClassifier(random_state=42))])\n_ = xgb.fit(df_train, y_train_binary)","5bffe565":"confusion_matrix(y_train_binary, xgb.predict(df_train), normalize='all').round(3)","3df8a04b":"ensembles = {\n    \"rf\": rf_search.best_estimator_,\n#     \"gb\": gb_search.best_estimator_,\n    \"xgb\": xgb,\n}\n\nfig_roc2, ax_roc2 = plt.subplots(figsize=S)\nplt.plot([0, 1], [0, 1], 'k--')\nfor k,v in ensembles.items():\n    plot_roc_curve(v, df_train, y_train_binary, name=k, ax=ax_roc2)","8ada95f2":"fig_prc2, ax_prc2 = plt.subplots(figsize=S)\nplt.plot([0, 1], [y1.iloc[1,1], y1.iloc[1,1]], 'k--')\nfor k,v in ensembles.items():\n    plot_precision_recall_curve(v, df_train, y_train_binary, name=k, ax=ax_prc2)","c6e5b012":"fig_prc2a, ax_prc2a = plt.subplots(figsize=S)\nplt.plot([0, 1], [y1.iloc[1,1], y1.iloc[1,1]], 'k--')\nfor k,v in ensembles.items():\n    plot_precision_recall_curve(v, df_test, y_test_binary, name=k, ax=ax_prc2a)","56da76e4":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nbrf = Pipeline(\n    steps=[(\"prep\", prep), (\"rf\", RandomForestClassifier(random_state=42, n_jobs=-1))]\n)\nbrf_parameters = {\n    \"rf__max_samples\": [1000],  # don't use the whole dataset: takes too long!\n    \"rf__n_estimators\": [100],\n    \"rf__criterion\": [\"gini\"],\n    \"rf__max_features\": [\"auto\"],\n    \"rf__min_samples_leaf\": [1, 2, 5],\n    \"rf__max_depth\": [10, 15],\n    \"rf__oob_score\": [True, False],\n    \"rf__class_weight\": [\"balanced\"],\n}\nbrf_search = GridSearchCV(brf, brf_parameters, cv=5)\n_ = brf_search.fit(df_train, y_train_binary)","1bfe48fe":"%%time\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n\nibrf = Pipeline(\n    steps=[(\"prep\", prep), (\"rf\", BalancedRandomForestClassifier(random_state=42, n_jobs=-1))]\n)\nibrf_parameters = {\n    \"rf__max_samples\": [1000],  # don't use the whole dataset: takes too long!\n    \"rf__n_estimators\": [100],\n    \"rf__criterion\": [\"gini\"],\n    \"rf__max_features\": [\"auto\"],\n    \"rf__min_samples_leaf\": [1, 2, 5],\n    \"rf__max_depth\": [10, 15],\n    \"rf__oob_score\": [True, False],\n}\nibrf_search = GridSearchCV(ibrf, ibrf_parameters, cv=5)\n_ = ibrf_search.fit(df_train, y_train_binary)","5ec50083":"ensembles[\"brf\"] = brf_search.best_estimator_\nensembles[\"ibrf\"] = ibrf_search.best_estimator_\n\nfig_prc3, ax_prc3 = plt.subplots(figsize=S)\nplt.plot([0, 1], [y1.iloc[1,1], y1.iloc[1,1]], 'k--')\nfor k,v in ensembles.items():\n    plot_precision_recall_curve(v, df_train, y_train_binary, name=k, ax=ax_prc3);","11d9aa72":"%%time\n# see what performance is for outlier detection\ny_train_binary_inverted = np.invert(y_train_binary)\n\nxgb_inv = Pipeline(steps=[(\"prep\", prep), (\"xgb\", xgboost.XGBClassifier())])\n_ = xgb_inv.fit(df_train, y_train_binary_inverted)","5dbdaad8":"fig_prc4, ax_prc4 = plt.subplots(figsize=S)\nplt.plot([0, 1], [1 - y1.iloc[1,1], 1 - y1.iloc[1,1]], 'k--')\nplot_precision_recall_curve(xgb_inv, df_train, y_train_binary_inverted, name=\"xgb_inv\", ax=ax_prc4);","3ad9edd0":"from yellowbrick.classifier import ROCAUC, PrecisionRecallCurve\n\nroc = ROCAUC(rf_search,)\nroc.fit(df_train, y_train_binary)\nroc.score(df_train, y_train_binary)\nroc.show();","37ee4fb3":"prc = PrecisionRecallCurve(xgb_inv,)\nprc.fit(df_train, y_train_binary_inverted)\nprc.score(df_train, y_train_binary_inverted)\nprc.show();","12cc4cd4":"runs = [{\"random_state\": n} for n in range(2, 52, 10)]\nfor run in runs:\n    xgb = Pipeline(steps=[(\"prep\", prep), (\"xgb\", xgboost.XGBClassifier(random_state=run['random_state'], subsample=0.7))])\n    run[\"model\"] = xgb.fit(df_train, y_train_binary)","e07938dd":"fig_random, ax_random = plt.subplots(figsize=S)\nax_random.set_ylim(0.6, 1.0)\nplt.plot([0, 1], [y1.iloc[1,1], y1.iloc[1,1]], 'k--')\nfor run in runs:\n    plot_precision_recall_curve(run['model'], df_test, y_test_binary, name=k, ax=ax_random, alpha=0.5)","d0dc61f2":"fig_importance, ax_importance = plt.subplots(figsize=S)\nxgboost.plot_importance(xgb['xgb'], ax=ax_importance);","ac667f42":"# To DO: fix feature names in plot\nlist(zip(xgb['xgb'].get_booster().feature_names, X_columns))","8606d972":"#TODO: add prediction error vs. complexity plot","f9456782":"But hang on, don't get too excited yet: aren't we overfitting?","2dcbadd8":"### Ensemble methods: Random Forest & GradientBoosted Trees\n\nWe are going to do the same - train classifiers with grid search and cross-validation - but using an ensemble of learners this time.","45961ceb":"#### Try the inverse: predicting patients with not in good-good quadrant","3b3ddf46":"# Background to osteoarthritis case study\n\nThis is day 4 from the [5-day JADS NHS PROMs data science case study](https:\/\/github.com\/jads-nl\/execute-nhs-proms\/blob\/master\/README.md). To recap the previous lectures:\n\n- In **lecture 1** we focused on data understanding and explored various possible outcome parameters.\n- In **lecture 2** we constructed a combined outcome parameter using cut-off points for pain and physical functioning.\n- In **lecture 3** we performed regression and linear modeling on the outcome paramter `t1_eq_vas`.\n\nToday we are going to focus on classifcation.","8ce24c58":"### Bias-variance trade-off\n\nOne of the nice features of XGBoost is that it has [explicitly defined the complexity function](https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/model.html):\n\n$$\n\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2\n$$\n\nThat means that we can plot the results from the gridsearch into a prediction error vs. complexity graph to visualize the bias-variance trade-off.","cc7259ce":"#### LinearSVC  ","20befeb2":"#### Bonus: Yellowbrick\nTo further optimze your workflow, you can use [Yellowbrick](https:\/\/www.scikit-yb.org\/en\/latest\/index.html) for visualizing results.","1d1fb370":"#### PR curve\nThe results from the ROC curve are not very exciting. Let's look at the precision recall curve.","34ff3b80":"## Discussion: model assessment and evaluation\n\n### Greediness of algorithms\n\nEven if you have a good result, i.e. a good score with the test set, you should beware of the greediness of machine learning algorithms: the solution is _at best_ a local optimum, and sometimes not even that. Depending on the random state at initiazation, the results will vary. To illustrate this, let's do 5 runs of XBGBoost (being the best model we have looked at) with different random states.\n","03a5a4ab":"## Learning objectives\n### Modeling: classification\n\nRecall we defined different outcome Y suitable for classification in lecture 2:\n\n- `y_mcid`: outcome is good (True\/1) if `t1_oks_score` is above the threshold or `delta_oks_score` is larger than MCID\n- `y_t1_pain_good` and `y_t1_functioning_good`: combination of two binary outcomes, yielding a total of 4 classes.\n\nWe will use the second outcome since it is more challenging and versatile (see [this presentation (in Dutch)](https:\/\/kapitan.net\/wp-content\/uploads\/2018\/11\/181108_data_driven_healthcare_congres.pdf) for more details):\n- perform a binary classification, defining good outcome where both pain and functioning is good at `t1`\n- perform multiclass classification.\n\nWe will compare the performance of single estimators (SVM, Decision tree) with ensemble learning (Random Forest, Gradient Boosting). Grid search and cross validation is done for model assessment. Some examples are given to visualize scoring functions of models to aid in model selection.\n\n\n### Python: Hands-on Machine Learning (2nd edition)\n\n- [Classification (chapter 3)](https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/03_classification.ipynb)\n- [Support-vector machines (chapter 5)](https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/05_support_vector_machines.ipynb)\n- [Decision trees (chapter 6)](https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/06_decision_trees.ipynb)\n- [Ensemble learning and random forests (chapter 7)](https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/07_ensemble_learning_and_random_forests.ipynb)\n\n### Python: scikit-learn and matplotlib\n- [Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py)\n- [Visualizations](https:\/\/scikit-learn.org\/stable\/visualizations.html#visualizations)\n- [Real Python Matplotlib guide](https:\/\/realpython.com\/python-matplotlib-guide\/)\n- [Matplotlib anatomy](https:\/\/matplotlib.org\/3.2.2\/gallery\/showcase\/anatomy.html)\n- [Yellowbrick](https:\/\/www.slideshare.net\/RebeccaBilbro\/learning-machine-learning-with-yellowbrick)\n\nLet's start by re-running the relevant code from previous lectures, after which we can continue extending our pipelines to experiment with different classifiers.","a52fa2e7":"## Classification\n\n\n### Data mining goals\nRecall from our earlier discussions (phase 1: Business Understanding) that we want to predict the outcome of surgery. To make this more explicit, for now we choose to predict the outcome being in the bottom-right quadrant of `y1` i.e. with good functioning and less pain. The base-rate `P \/ (P + N) = 71%`. \n\n### Single estimators: Stochastic Gradient Decent, Decision Tree (CART) and SVM\n#### Stochastic Gradient Descent","9c6a17da":"Clearly more difficult to detect 'outliers' i.e. patients with less than optimal outcome.","b6e68f27":"#### Ensemble methods: balanced\nThe results are better than single estimators, but can we make it better still? We will try balanced Random Forest, using the `class_weight` option in scikit-learn and comparing that next to the `BalancedRandomForestClassifier` from the [imbalanced-learn](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/) package.","bfced42b":"### Feature importance\n\nMany tree-based methods can compute feature importance. Also here, note that this is a relative importance and that due to the greediness you can not draw very hard conclusions (features by swap places depending on the parameter settings). It is useful to get an intuitive feel which features contain the most information for your classification task.","7f3bbe32":"#### XGBoost\n\nThe GradientBoostingClassifier in sklearn is quite slow. [XGboost](https:\/\/xgboost.readthedocs.io\/en\/latest\/index.html) is a better and faster implementation. After installing it separately, you can use the sklearn API to use it in our existing pipeline.","f11916ca":"### Evalution of results\n\nSo let's evaluate the results and discuss whether we have reached our objective:\n\n- Is the obtained precision and recall useful?\n  - Would you use a prediction model on good outcome or not-good outcomes?\n  - Where would you but the decision threshold?\n- How could we improve the model?\n- What did we learn about the quality of the data? How much signal vs noise?","5f2fc962":"# Modeling: clustering & classfication","12092b94":"#### ROC curve\nIt is more intuitive to compare different estimators visually, i.e. visualizing the performance metrics. scikit-learn 0.22 comes with a [new plotting API](https:\/\/scikit-learn.org\/stable\/auto_examples\/release_highlights\/plot_release_highlights_0_22_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-0-22-0-py), so it's much easier than before to quickly produce charts. We start with the good ol' ROC curve (although later on I will stress not to use this!).","d427e555":"#### Decision Tree (CART)"}}