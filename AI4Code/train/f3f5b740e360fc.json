{"cell_type":{"cb919089":"code","8e201484":"code","9b8cadf0":"code","824f14d0":"code","cffc7fe0":"code","7856d37e":"code","47207b2c":"code","fbcd5638":"code","c7be35ae":"code","f795259d":"code","baa38725":"code","d5377d44":"code","51e495da":"code","72f4e680":"code","0807547a":"code","1ac01854":"code","dc152bbe":"code","f62f6256":"code","202f1f54":"code","ca05fba6":"code","d6e52fc0":"code","2a002194":"code","1cca3f3d":"code","b47fecac":"code","26902283":"code","811b2aab":"code","5d826e56":"markdown","7bf69c5a":"markdown","139f152d":"markdown","c7ecd7a4":"markdown","d8fe2004":"markdown","a5af7094":"markdown","90f5dbe0":"markdown","a1d18541":"markdown","8c66a7b4":"markdown","bd10c764":"markdown","38630e91":"markdown","4eeb19a4":"markdown","25baf5ed":"markdown","7b03658a":"markdown","7a26d297":"markdown","a202ab8e":"markdown","beb53e0b":"markdown","a2919d19":"markdown","23a14e99":"markdown","f4a7b7ec":"markdown","c9a4bf1a":"markdown","ab32a06d":"markdown","5c075a8b":"markdown","a282b887":"markdown","2bf89cc7":"markdown","03c1bbd2":"markdown","f304be16":"markdown","bbe1d2c8":"markdown","8b142dfe":"markdown","b17af161":"markdown","50b651bf":"markdown","feb48b44":"markdown","3f584dee":"markdown","1514dbfc":"markdown"},"source":{"cb919089":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\n%matplotlib inline","8e201484":"DF1 = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\n\nDF2 = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\n\nSAM = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","9b8cadf0":"X  = DF1.drop(columns = ['id','loss'])\n\nXX = DF2.drop(columns = ['id'])\n\ny  = DF1.loss","824f14d0":"display(y, y.min(), y.max())","cffc7fe0":"y.value_counts().plot(figsize=(16, 8), kind='bar')","7856d37e":"y.value_counts().plot(figsize=(10, 10), kind='pie')\n\ny.value_counts(normalize=True)","47207b2c":"hist_data = [ y ]  \n\ngroup_labels = ['y']\n    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \n\nfig.show()","fbcd5638":"path0 = '..\/input\/tps8-786780\/voting.csv' \n\nsub786780 = pd.read_csv(path0)","c7be35ae":"path1 = '..\/input\/tps8-786595\/submission43.csv' \n\nsub786595 = pd.read_csv(path1)","f795259d":"path2 = '..\/input\/tps8-786259\/In_LightAutoML_we_trust.csv' \n\nsub786259 = pd.read_csv(path2)","baa38725":"path3 = '..\/input\/tps8-786132\/submit.csv' \n\nsub786132 = pd.read_csv(path3)","d5377d44":"path4 = '..\/input\/tps8-785852\/submission.csv' \n\nsub785852 = pd.read_csv(path4)","51e495da":"path5 = '..\/input\/tps8-785308\/TPS8_785308.csv' \n\nsub785308 = pd.read_csv(path5)","72f4e680":"path6 = '..\/input\/tps8-785239\/submission.csv' \n\nsub785239 = pd.read_csv(path6)","0807547a":"hist_data = [sub786780.loss, sub786595.loss, sub786259.loss, sub786132.loss, sub785852.loss, sub785308.loss, sub785239.loss]  \n\ngroup_labels = ['7.86780', '7.86595', '7.86259', '7.86132', '7.85852', '7.85308', '7.85239']\n    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \n\nfig.show()","1ac01854":"hist_data = [sub786780.loss, sub786595.loss, sub786259.loss, sub786132.loss, sub785852.loss, sub785308.loss, sub785239.loss, y]  \n\ngroup_labels = ['7.86780', '7.86595', '7.86259', '7.86132', '7.85852', '7.85308', '7.85239', 'y']\n    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \n\nfig.show()","dc152bbe":"path11 = '..\/input\/tps-785318\/TPS_785318.csv' \n\nsub785318 = pd.read_csv(path11)","f62f6256":"path12 = '..\/input\/tps-785254\/TPS8_785254.csv' \n\nsub785254 = pd.read_csv(path12)","202f1f54":"path13 = '..\/input\/tps8-785237\/TPS8_785237.csv' \n\nsub785237 = pd.read_csv(path13)","ca05fba6":"hist_data = [sub785318.loss, sub785254.loss, sub785237.loss]  \n\ngroup_labels = ['Public Score: 7.85318', 'Public Score: 7.85254', 'Public Score: 7.85237']\n    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \n\nfig.show()","d6e52fc0":"path14 = '..\/input\/tps8-785159\/TPS8_785159.csv' \n\nsub785159 = pd.read_csv(path14)","2a002194":"hist_data = [sub785159.loss]  \n\ngroup_labels = ['Ensembling (7.85159)']\n    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \n\nfig.show()","1cca3f3d":"hist_data = [sub785318.loss, sub785254.loss, sub785237.loss, sub785159.loss]  \n\ngroup_labels = ['Public Score: 7.85318', 'Public Score: 7.85254', 'Public Score: 7.85237', 'Ensembling (7.85159)']\n    \nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False) \n\nfig.show()","b47fecac":"def coordinate1(main, min_lim, max_lim, constant1, constant2):   \n    \n    sub  = main.copy() \n    subv = sub.values    \n    suba = subv[:, 1]\n    \n    coor  = main.copy()    \n    coorv = coor.values\n    \n    for i in range (len(main)):\n        \n        if (suba[i] <= min_lim):\n            per = suba[i] - constant1\n            coorv[i, 1] = per\n            \n        if (suba[i] >= max_lim):\n            per = suba[i] + constant2\n            coorv[i, 1] = per\n          \n    coor.iloc[:, 1] = coorv[:, 1] \n    \n    ###############################   \n    X = suba\n    Y = coor.iloc[:, 1] \n    \n    plt.style.use('seaborn-whitegrid') \n    plt.figure(figsize=(9, 9), facecolor='lightgray')\n    plt.title(f'\\nC O O R D I N A T E\\n')   \n            \n    plt.scatter(X, X, s=2.0, label='Main(X=Y)')\n    plt.scatter(X, Y, s=2.0, label='Coordinated')\n    \n    plt.legend(fontsize=12, loc=2)\n    #plt.savefig('Coordinate_1.png')\n    plt.show()     \n    ###############################   \n    coor.iloc[:, 1] = coor.iloc[:, 1].astype(float)\n    hist_data = [suba, coor.iloc[:, 1]] \n    group_labels = ['Main', 'Coordinated']\n    \n    fig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False)\n    fig.show()   \n    ###############################       \n    print()\n    print(':::::::::::::::: Main Values ::::::::::::::::')\n    print(f'Min:{suba.min()}   Max:{suba.max()}\\n')\n    print(':::::::::::: Coordinated Values :::::::::::::')\n    print(f'Min:{coor.iloc[: ,1].min()}   Max:{coor.iloc[: ,1].max()}\\n')     \n    ###############################    \n    \n    return coor   ","26902283":"sub1 = coordinate1(sub785159, 5.5, 8.0, 0.2, 0.2)","811b2aab":"sub1.to_csv(\"submission1.csv\",index=False)\n!ls","5d826e56":"<div class=\"alert alert-success\">  \n<\/div>","7bf69c5a":"Thanks to: @alexryzhkov https:\/\/www.kaggle.com\/alexryzhkov\/aug21-lightautoml-starter","139f152d":"Thanks to: @oxzplvifi https:\/\/www.kaggle.com\/oxzplvifi\/tabular-denoising-residual-network\n\nThanks to: @pourchot https:\/\/www.kaggle.com\/pourchot\/in-python-tabular-denoising-residual-network","c7ecd7a4":"<div class=\"alert alert-success\">  \n<\/div>","d8fe2004":"### The first priority of all machine learning methods was only the \"RMSE\" constraint, and in practice we could not create a successful classification or regression. The predicted values do not have the desired scatter and are only gathered around the value of \"np.mean (y)\". Obviously, this range of numbers optimizes the \"RMSE\" equation, but as you can see, it will not necessarily be a good and realistic prediction.\n","a5af7094":"### At first glance, this may not seem like a difficult task. But unfortunately this is complicated. The order and ranking of the numbers predicted by the notebooks is not accurate. This means that with the slightest change, the \"RMSE\" equation may be out of the optimal state. And in practice we can not simply increase the scatter of numbers ... but we can still do something.\n","90f5dbe0":"## Output","a1d18541":"### As you can see, the results of different notebooks are very different. But they have one thing in common. The minimum and maximum values of all results are almost equal. But strangely enough, these values are completely different from the minimum and maximum \"y\" values. Please note the following diagram:","8c66a7b4":"Thanks to: @hiro5299834 https:\/\/www.kaggle.com\/hiro5299834\/tps-aug-2021-lgbm?scriptVersionId=71439698","bd10c764":"Thanks to: @hiro5299834 https:\/\/www.kaggle.com\/hiro5299834\/tps-aug-2021-lgbm-xgb-catboost","38630e91":"<div class=\"alert alert-success\">  \n<\/div>","4eeb19a4":"### Please note that these methods may increase the notebook score, but the final maximum and minimum values will still not change much. For comparison in the charts below, the inputs and outputs of our second notebook are provided.","25baf5ed":"<div class=\"alert alert-success\">\n    <h1 align=\"center\">If you find this work useful, please don't forget upvoting :)<\/h1>\n<\/div>","7b03658a":"## Method 1: Coordinate with constant values","7a26d297":"<div class=\"alert alert-success\">\n    <h1 align=\"center\">If you find this work useful, please don't forget upvoting :)<\/h1>\n<\/div>","a202ab8e":"## The methods of \"Ensembling\", \"Comparative Method\", etc. \n\n## Do not help to increase the scatter of numbers.\n","beb53e0b":"## Data Set of Challenge","a2919d19":"<div>\n    <h1 align=\"center\">Results & \"RMSE\" Evaluation<\/h1>    \n    <h1 align=\"center\">Tabular Playground Series - Aug 2021<\/h1> \n    <h4 align=\"center\">By: Somayyeh Gholami & Mehran Kazeminia<\/h4>\n<\/div>","23a14e99":"### As you can see, the score improved a lot with \"Ensembling\", but the minimum and maximum values did not change much. Perhaps it should be said that the \"RMSE\" equation has practically created a cage for our results and does not allow us to approach the real predictions :)","f4a7b7ec":"## Inputs & Output","c9a4bf1a":"Thanks to: @dmitryuarov https:\/\/www.kaggle.com\/dmitryuarov\/falling-below-7-87-voting-cb-xgb-lgbm","ab32a06d":"## Inputs","5c075a8b":"<div class=\"alert alert-success\">  \n<\/div>","a282b887":"Thanks to: @alexryzhkov https:\/\/www.kaggle.com\/alexryzhkov\/lightautoml-classifier-regressor-mix\/output?scriptVersionId=71481321","2bf89cc7":"<div class=\"alert alert-success\">  \n<\/div>   ","03c1bbd2":"<div class=\"alert alert-success\">  \n<\/div>","f304be16":"<div class=\"alert alert-success\">  \n<\/div>","bbe1d2c8":"Thanks to: @tensorchoko https:\/\/www.kaggle.com\/tensorchoko\/tabular-aug-2021-lightgbm","8b142dfe":"## How to get out of the \"RMSE\" cage?","b17af161":"<div class=\"alert alert-success\">  \n<\/div>","50b651bf":"### Of course, **we do not recommend this method**. We will publish much better methods in the next notebooks. But this method is a good example of conveying the concept we wanted to share.\n\n### We hope that the contents of this notebook will be useful for you to continue this challenge.\n\n## Good Luck.","feb48b44":"### We are not informed about \"train_data\" and \"test_data\" and their differences. For this reason, we expect our results to be approximately similar to \"y\". ***But did this happen?***\n\n### We will see that all the results of public notebooks have a significant distance from \"y\". While the score of these notebooks (error value) is approximately equal to \"7.8\". This is a big error and even bigger than \"np.mean (y)\".\n","3f584dee":"## Notebook Results","1514dbfc":"### The evaluation of this challenge is based on \"RMSE\". That means Submissions are scored on the root mean squared error. So obviously all the learning machine methods that users use are just trying to optimize the \"RMSE\" equation. But the important question is:\n\n## Can merely optimizing the \"RMSE\" equation guarantee the success of classification or regression?\n\n### Unfortunately, the answer is no. In this notebook, you will see that even the best results of machine learning methods are far from reality. For this study, we used the results of several public notebooks that have used different methods and are also the best public notebooks in terms of scores.\n"}}