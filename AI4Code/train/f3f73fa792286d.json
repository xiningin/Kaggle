{"cell_type":{"e3c45ee2":"code","d46bd999":"code","4584554b":"code","cde90205":"code","00a97b56":"code","b8ba8b8b":"code","4fce0fba":"code","1b6fe1d6":"code","eb4b0be6":"code","2de1409a":"code","181247a8":"code","9f096e67":"code","aa1322c9":"code","b8e662e3":"code","8ba658dd":"code","69dfb7ac":"code","d061d0e8":"code","66342072":"code","dc198d9b":"code","2149631a":"code","2e0326f7":"code","300f462c":"code","de7a3fbb":"code","a6b2f636":"code","90fea560":"code","41869f21":"code","2b9d9c4f":"code","ac46ae41":"code","dc105b6c":"code","16f83953":"code","dcb39f94":"code","ce6d8c7c":"code","9f3cb59c":"code","7fa498d9":"code","3c412419":"code","bb2cdff6":"markdown","47303851":"markdown","583dac59":"markdown","1e4b7a40":"markdown","5681b681":"markdown","18303ab8":"markdown","08278f8e":"markdown","5c6ea529":"markdown"},"source":{"e3c45ee2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#from sklearn import cross_validation\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score , cross_validate, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier","d46bd999":"#Now importing the dataset into python frame\n\ndf_train = pd.read_csv (\"..\/input\/train.csv\")\ndf_test = pd.read_csv (\"..\/input\/test.csv\")\n\nprint (df_test.shape)","4584554b":"print (df_train.isnull().sum())\nprint (df_train.info())","cde90205":"total = df_train.isnull().sum().sort_values(ascending = False)\npercentage = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat ([total,percentage], axis = 1, keys =  ['total', 'percentage'])\nprint (missing_data)","00a97b56":"#Checking the dataframe values\nprint (df_train.head())","b8ba8b8b":"# First checking the target feature (column)\nsns.countplot (x = \"Survived\", data = df_train)\nplt.legend (\"Survival count of the passengers\")\nplt.show()","4fce0fba":"# Relating Suvived feature to the \"Sex\" feature\nsns.countplot (x = \"Survived\" , hue = \"Sex\" , data = df_train)\nplt.show()","1b6fe1d6":"# Checking the realtion ship of \"Survived\" columns with \"Pclass\"\nsns.countplot (x = \"Survived\" , hue = \"Pclass\", data = df_train )\nplt.show()","eb4b0be6":"#Plotting the \"Age\" columns from the dataset\n\ndf_train [\"Age\"].plot.hist()\n# from this result we see that we have more of young passengers followed by children between 0-10","2de1409a":"# Plotting the \"Fare\" column\ndf_train [\"Fare\"].plot.hist(bins = 20)","181247a8":"# Checking out Sibsp column(family relations)\nsns.countplot ( x = \"SibSp\" , data = df_train)","9f096e67":"# After performing some intitial analysis, decided to drop few columns now\ndf_train.drop (['Ticket','PassengerId','Cabin'], axis = 1, inplace = True)\ndf_test.drop (['Ticket','Cabin'], axis = 1, inplace = True) # didn't drop passengerid column here, will be dropped in the end when doing submission fie\n","aa1322c9":"#Lets fill the Embarked feature nan value\n#To fill this lets see the most frequently occured value\n\nS = df_test[df_test['Embarked'] == 'S'].shape [0]\nprint (\"S in the data set is \" , S)\n\nQ = df_test [df_test ['Embarked'] == 'Q'].shape[0]\nprint (\"Q in the data set is \" , Q)\nC = df_test [df_test ['Embarked'] == 'C'].shape[0]\nprint (\"C in the data set is \" , C)\n\nS = df_train[df_train['Embarked'] == 'S'].shape [0]\nprint (\"S in the data set is \" , S)\n\nQ = df_train [df_train ['Embarked'] == 'Q'].shape[0]\nprint (\"Q in the data set is \" , Q)\nC = df_train [df_train ['Embarked'] == 'C'].shape[0]\nprint (\"C in the data set is \" , C)\n\n#So S is the most frequent feature so replacing NAN with S","b8e662e3":"# Replacing NAN of EMBARKED\n\ndf_train['Embarked'] = df_train['Embarked'].fillna ( \"S\")\ndf_test['Embarked'] = df_test['Embarked'].fillna ( \"S\")\n\n#checking the training and testing values to see nan in \"Embarked column\"\nprint (df_train.isnull().sum())\nprint (df_test.isnull().sum())","8ba658dd":"#Filing the missing values in the age feature, here trying to reate age feature with columns \"Name\" \n#To do that first extract the name field and relate it to age\n\ndf_train ['Title'] = df_train.Name.str.extract (' ([A-Za-z]+)\\.', expand=False)\ndf_test ['Title'] = df_test.Name.str.extract (' ([A-Za-z]+)\\.', expand=False)\ncrosstab_train = pd.crosstab (df_train['Title'], df_train['Sex'])\ncrosstab_test = pd.crosstab (df_test['Title'], df_test['Sex'])\n#print (crosstab_train)\n","69dfb7ac":"# replacing various titles with common names\ncombine = [df_train, df_test]\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ndf_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","d061d0e8":"# map each of the title groups to a numerical values\n\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n#print (df_train.head())\n    ","66342072":"#Now I would like to categorize \n\ndef simplify_ages (df_train , df_test):\n    df_train['Age'] = df_train.Age.fillna (-0.5)\n    df_test ['Age'] = df_test.Age.fillna (-0.5)\n    bins = (-1,0,5,12,18,25,35,60,120)\n    print (df_train.isnull().sum())\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut (df_train.Age , bins, labels = group_names)\n    categories_test = pd.cut (df_test.Age , bins, labels = group_names)\n    df_train.Age = categories\n    print (df_train.isnull().sum())\n    df_test.Age = categories_test\n    print (df_train.isnull().sum())\n    return df_train.Age , df_test.Age\n\ndef simplify_fares (df_train, df_test):\n    df_train ['Fare'] = df_train.Fare.fillna (-0.5)\n    df_test ['Fare'] = df_test.Fare.fillna (-0.5)\n    bins = ( -1,0, 8,15, 31, 1000)\n    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n    categories = pd.cut (df_train.Fare, bins, labels = group_names)\n    categories_test = pd.cut (df_test.Fare, bins, labels = group_names)\n    df_train.Fare = categories\n    df_test.Fare = categories_test\n    return df_train.Fare, df_test.Fare\ndf_train.Age , df_test.Age = simplify_ages (df_train , df_test)\ndf_train.Fare , df_test.Fare = simplify_fares (df_train , df_test)","dc198d9b":"print (df_train.isnull().sum())\nprint (df_train.Age[418])","2149631a":"#print (df_train[\"Title\"].head(20))\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\nfor x in range(len(df_train[\"Age\"])):\n    #print (x)\n    if df_train[\"Age\"][x] == \"Unknown\":\n        df_train[\"Age\"][x] = age_title_mapping[df_train[\"Title\"][x]]\n    \n    \nfor x in range(len(df_test[\"Age\"])):\n    #print (x)\n    if df_test[\"Age\"][x] == \"Unknown\":\n        df_test[\"Age\"][x] = age_title_mapping[df_test[\"Title\"][x]]\n        #print (df_train[\"Age\"][x])","2e0326f7":"#Now performing age mapping\n\n#map each Age value to a numerical value\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ndf_train['Age'] = df_train['Age'].map(age_mapping)\ndf_test['Age'] = df_test['Age'].map(age_mapping)\n","300f462c":"#Now the missing values of \"Age\" is filled now \"Name\" column needs to be dropped because it cannot have much importance\n#our target value\ndf_train.drop(['Name'], axis = 1, inplace = True)\ndf_test.drop(['Name'], axis = 1, inplace = True)\nprint (df_train.head())\nprint (df_test.head())\nprint (  \"The shape of dataframe train is :\" ,df_train.shape)\nprint (\"The shape of dataframe test is :\" ,df_test.shape)","de7a3fbb":"#One hot encoding\n\n#Converting categorical values to new columns\n\ndf_train_dummy = pd.get_dummies (df_train, drop_first = True)\n\ndf_test_dummy = pd.get_dummies (df_test, drop_first = True)\n#print (df_train_dummy.head())\nprint ( \"The shape of dataframe train after encoding is :\" ,df_train_dummy.shape)\nprint ( \"The shape of dataframe train after encoding is :\" ,df_test_dummy.shape)","a6b2f636":"#Dropping target feature and assigning it to target feature\ndf_train_target = df_train_dummy ['Survived'] \ndf_train_dummy.drop(['Survived'], axis = 1, inplace = True)\n","90fea560":"X_train,X_validation,y_train,y_validation = train_test_split (df_train_dummy,\n                                                              df_train_target, shuffle = False,\n                                                              test_size = 0.25, random_state = 42)\n## Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform (X_train)\nX_validation = sc.transform (X_validation)\n\n### print proportions\nprint('train: {}% | validation: {}% |'.format(round(len(y_train)\/len(df_train_target),2),\n                                                       round(len(y_validation)\/len(df_train_target),2)))\n","41869f21":"# create regularization hyperparameter space logistic regression\n#penalty = (['l1','l2'])\n#C_var = np.array ([10.0,1.0,0.1,0.01,0.001])\n#fit_interceptOptions = ([ 'False'])\n#solveroptions = ([  'liblinear', 'saga'])\nreg= LogisticRegression(random_state = 42)\n#reg = GridSearchCV(estimator = clf, param_grid = dict(C= C_var, solver = solveroptions))\nreg.fit (X_train, y_train)\n\n#y_predict_train = reg.predict (X_test)\ny_predict_val = reg.predict(X_validation)\nprint ( 'Model score calculated TRAIN DATA:' , reg.score (X_train, y_train))\n# print(reg.best_score_)\n# print(reg.best_estimator_.solver)\n\n","2b9d9c4f":"\nprint ('Accuracy score test mode:' ,  accuracy_score (y_validation, y_predict_val))\nprint (   'Confusion matrix'    , confusion_matrix (y_validation, y_predict_val))\nprint ('Precision score test mode:' ,  precision_score (y_validation, y_predict_val))\nprint ('Recall score test mode:' ,  recall_score (y_validation, y_predict_val))\nprint ('F1 score test mode:' ,  f1_score (y_validation, y_predict_val))","ac46ae41":"# With other classifier\nfrom sklearn.tree import DecisionTreeClassifier\nreg = DecisionTreeClassifier()\n\nreg.fit (X_train, y_train)\ny_predict_val = reg.predict (X_validation)\naccuracy_prediction = round (accuracy_score (y_predict_val, y_validation) *100,2)\nprint (accuracy_prediction)\nprint (   'Confusion matrix'    , confusion_matrix (y_validation, y_predict_val))\nprint ('Precision score test mode:' , round(precision_score (y_validation, y_predict_val)*100,2))\nprint ('Recall score test mode:' ,  round (recall_score (y_validation, y_predict_val)*100,2))\nprint ('F1 score test mode:' ,  round (f1_score (y_validation, y_predict_val)*100,2))","dc105b6c":"#Checking accuracy with Randomforestclassifier\nfrom sklearn.ensemble import RandomForestClassifier\nreg = RandomForestClassifier(n_estimators = 500 , max_depth = 7, criterion = 'gini')\n\nreg.fit (X_train, y_train)\ny_predict_val = reg.predict (X_validation)\naccuracy_prediction = round (accuracy_score (y_predict_val, y_validation) *100,2)\nprint (accuracy_prediction)\nprint (   'Confusion matrix'    , confusion_matrix (y_validation, y_predict_val))\nprint ('Precision score test mode:' , round(precision_score (y_validation, y_predict_val)*100,2))\nprint ('Recall score test mode:' ,  round (recall_score (y_validation, y_predict_val)*100,2))\nprint ('F1 score test mode:' ,  round (f1_score (y_validation, y_predict_val)*100,2))","16f83953":"from sklearn.neighbors import KNeighborsClassifier\nreg = KNeighborsClassifier(n_neighbors = 5, weights='uniform', algorithm = 'auto')\n\nreg.fit (X_train, y_train)\ny_predict_val = reg.predict (X_validation)\naccuracy_prediction = round (accuracy_score (y_predict_val, y_validation) *100,2)\nprint (accuracy_prediction)\nprint (   'Confusion matrix'    , confusion_matrix (y_validation, y_predict_val))\nprint ('Precision score test mode:' , round(precision_score (y_validation, y_predict_val)*100,2))\nprint ('Recall score test mode:' ,  round (recall_score (y_validation, y_predict_val)*100,2))\nprint ('F1 score test mode:' ,  round (f1_score (y_validation, y_predict_val)*100,2))","dcb39f94":"# By using gradient boosting classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nreg = GradientBoostingClassifier(n_estimators  = 500)\n\nreg.fit (X_train, y_train)\ny_predict_val = reg.predict (X_validation)\naccuracy_prediction = round (accuracy_score (y_predict_val, y_validation) *100,2)\nprint (accuracy_prediction)\nprint (   'Confusion matrix'    , confusion_matrix (y_validation, y_predict_val))\nprint ('Precision score test mode:' , round(precision_score (y_validation, y_predict_val)*100,2))\nprint ('Recall score test mode:' ,  round (recall_score (y_validation, y_predict_val)*100,2))\nprint ('F1 score test mode:' ,  round (f1_score (y_validation, y_predict_val)*100,2))","ce6d8c7c":"#print (df_train_target)\n## Feature Scaling again for Kfold\nsc = StandardScaler()\ndf_train_dummy1 = pd.DataFrame(sc.fit_transform (df_train_dummy))\n","9f3cb59c":"from sklearn.model_selection import KFold  # u should do STANDARD SCALER HERE feature scaling\nfrom sklearn.model_selection import StratifiedKFold\nreg = GradientBoostingClassifier(n_estimators  = 500)  # Choosing Gradient boosting estimator because of its good score \n# for F1 and accuracy\ndef run_kfold(reg):\n    kf = KFold (n_splits = 10, shuffle = True, random_state = 45)\n     \n    outcomes = []\n    fold = []\n    for train_index, test_index in kf.split (df_train_dummy1, df_train_target):\n        #fold +=1\n        X_train, X_test = df_train_dummy1.iloc[train_index] , df_train_dummy1.iloc [test_index]\n        y_train, y_test = df_train_target.iloc [train_index], df_train_target.iloc [test_index]\n        reg.fit (X_train,y_train)\n        y_pred = reg.predict (X_test)\n        accuracy = accuracy_score (y_test,y_pred)\n        outcomes.append (accuracy)\n        print(\"Fold  accuracy: {}\".format(accuracy))\n    mean_outcome = np.mean (outcomes)\n    print (\"Mean Accuracy {}\" . format (mean_outcome))\nrun_kfold (reg)","7fa498d9":"passenger_id = df_test_dummy ['PassengerId']\ndf_test_dummy.drop (['PassengerId'], axis = 1, inplace = True)","3c412419":"#Submission fil\n\npredict = reg.predict (df_test_dummy)\n\n#Now creating submission dataframe\n\nsubmission = pd.DataFrame ({'PassengerId' : passenger_id, 'Survived' : predict })\nsubmission.to_csv ('submission.csv', index = False)","bb2cdff6":"##### Submitting dataset to the competition ","47303851":"##### First Exploring the dataset","583dac59":"##### Applying Modelling","1e4b7a40":"Importing all the necessary libraries","5681b681":"Validate with KFold\nIs this model actually any good? It helps to verify the effectiveness of the algorithm using KFold. This will split our data into 10 buckets, then run the algorithm using a different bucket as the test set for each iteration.","18303ab8":"##### Analyzing the data","08278f8e":"##### Preprocessing the data","5c6ea529":"Splitting the training dataset between validation and training "}}