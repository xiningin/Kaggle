{"cell_type":{"8975936f":"code","1f07b3a0":"code","2217552d":"code","cc748cd8":"code","bdeb9fc6":"code","f8f9f134":"code","f61d27d3":"code","84cfc971":"code","4aac3e8c":"code","f25930b5":"code","b2c6c12e":"code","9e272013":"code","414fa632":"code","7a9f0887":"code","09cdf2e1":"code","80f3ebd0":"code","34a48e9e":"code","b9836950":"code","78e0faa3":"code","3f0ff109":"code","247f4641":"code","7b9cd859":"code","3af6b948":"code","ebf73774":"code","7a5a44fa":"code","61bf8456":"code","c7c73fb5":"code","01732cae":"code","3867907b":"code","f7f46c46":"code","d7d1b155":"code","01122fd1":"code","762290b9":"code","8a6edcbc":"code","cc4e04c7":"code","5bb9e746":"code","77742a76":"code","019c8fb7":"code","0f1e5fba":"code","a571993d":"code","bb65e7f4":"code","7d31a3c8":"code","d232ca9a":"markdown","932dd0c1":"markdown","ed6cc983":"markdown","d5e47fba":"markdown","5a31975f":"markdown","c341a7d2":"markdown","09ccbf74":"markdown","a249782b":"markdown","1620cab2":"markdown","51277438":"markdown","24055a77":"markdown","104ee89b":"markdown"},"source":{"8975936f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport missingno\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import boxcox\nimport warnings\nimport wandb\nimport math\nfrom imblearn.combine import SMOTETomek\n\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f07b3a0":"df = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ndf.head()","2217552d":"df.info()","cc748cd8":"df.shape","bdeb9fc6":"df.isna().sum()","f8f9f134":"fig = px.bar(y=df.isna().sum().values, x= df.columns, color = df.columns, \n       hover_name = (df.isna().sum().values \/ df.shape[0] * 100),\n      title= \"Count and % of null values in each column\")\nfig.update_traces(showlegend=False)\nfig.show()","f61d27d3":"cols = list(df.columns)[1:]\ncols.remove(\"audio_mode\")\ncols.remove(\"song_popularity\")","84cfc971":"f, axs = plt.subplots(4,3, figsize=(20,20))\nk = 0\nfor i in range(4):\n    for j in range(3):\n        sns.histplot(ax = axs[i,j],data = df, x= cols[k], hue = 'song_popularity', \n                    stat = \"density\", kde = True)\n        k+=1","4aac3e8c":"g = sns.FacetGrid(df, col=\"song_popularity\", hue = \"audio_mode\",palette = \"rocket\")\ng.map(sns.scatterplot, \"liveness\", \"danceability\", alpha=.7)\ng.add_legend()","f25930b5":"g = sns.PairGrid(df, vars=cols[:5], hue=\"song_popularity\", palette = \"Set2\")\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()","b2c6c12e":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot = True)","9e272013":"px.bar(y = df.skew(axis = 0).values, x = df.columns, title = \"Visualizing skewness for each column\")","414fa632":"missing_cols = df.columns[df.isna().sum()>0]\nmissing_cols","7a9f0887":"missingno.matrix(df)","09cdf2e1":"missingno.dendrogram(df)","80f3ebd0":"# Changing order of missing column list according to dependency predicted by dendrogram\nmissing_cols = [\"loudness\", \"energy\", \"instrumentalness\", \"acousticness\", \"danceability\",\n                \"key\", \"liveness\", \"song_duration_ms\"]","34a48e9e":"df_mean = df.copy(deep = True)\ndf_reg = df.copy(deep = True)","b9836950":"skew_cols = [\"speechiness\", \"instrumentalness\", \"acousticness\"]","78e0faa3":"for col in skew_cols:\n    try:\n        df_mean[col] = boxcox(df_mean[col])[0]\n        df_reg[col] = boxcox(df_reg[col])[0]\n    except:\n        df_mean[col] = np.log(df_mean[col])\n        df_reg[col] = np.log(df_reg[col])\ndf_mean[\"loudness\"] = np.cbrt(df_mean[\"loudness\"])\ndf_reg[\"loudness\"] = np.cbrt(df_reg[\"loudness\"])\ndf_mean[\"liveness\"] = np.log(df_mean[\"liveness\"])\ndf_reg[\"liveness\"] = np.log(df_reg[\"liveness\"])","3f0ff109":"px.bar(y = df_mean.skew(axis = 0).values, x = df_mean.columns, title = \"Visualizing skewness for each column\")","247f4641":"px.bar(y = df_reg.skew(axis = 0).values, x = df_reg.columns, title = \"Visualizing skewness for each column\")","7b9cd859":"for col in missing_cols:\n    df_mean[col].fillna(df_mean[col].mean(), inplace = True)\ndf_mean.isna().sum()","3af6b948":"df_regs_drop = df_reg.drop(missing_cols, axis = 1)\ndf_regs_drop = df_regs_drop.drop([\"id\"], axis = 1)\ndf_regs_drop.head()","ebf73774":"df_reg.head()","7a5a44fa":"for col in missing_cols:\n    df_regs_drop[col] = df_reg[col]\n    lr = LinearRegression()\n    testdf = df_regs_drop[df_regs_drop[col].isnull()==True]\n    traindf = df_regs_drop[df_regs_drop[col].isnull()==False]\n    y = traindf[col]\n    traindf.drop(col,axis=1,inplace=True)\n    lr.fit(traindf,y)\n    testdf.drop(col,axis=1,inplace=True)\n    pred = lr.predict(testdf)\n    testdf[col]= pred\n    traindf[col] = y\n    df_regs_drop = pd.concat([traindf, testdf])\ndf_regs_drop.isna().sum()\n","61bf8456":"plt.figure(figsize=(20,20))\nsns.heatmap(df_mean.corr(), annot = True)","c7c73fb5":"plt.figure(figsize=(20,20))\nsns.heatmap(df_regs_drop.corr(), annot = True)","01732cae":"Rf_mean = RandomForestClassifier(criterion = \"entropy\")\ny_mean = df_mean[\"song_popularity\"]\nx_mean = df_mean.drop([\"song_popularity\", \"id\"], axis = 1)\ntrain_x_mean, test_x_mean, train_y_mean, test_y_mean = train_test_split(x_mean, y_mean, \n                                             test_size = 0.3, random_state = 12)\nRf_mean.fit(train_x_mean, train_y_mean)\npreds = Rf_mean.predict(test_x_mean)\nprint(\"Accuracy: \", accuracy_score(test_y_mean, preds))\nprint(\"ROC AUC Score: \",roc_auc_score(test_y_mean, preds))\nprint(\"Confusion matrix: \", confusion_matrix(test_y_mean, preds))                        \n                        ","3867907b":"from sklearn.metrics import roc_curve\nfrom sklearn.preprocessing import StandardScaler","f7f46c46":"Rf_reg = RandomForestClassifier(n_estimators=500,criterion = \"gini\", max_features=\"auto\")\n\ny_reg = df_regs_drop[\"song_popularity\"]\nx_reg = df_regs_drop.drop(\"song_popularity\", axis = 1)\nx_reg_s = StandardScaler().fit_transform(x_reg)\ntrain_x_reg, test_x_reg, train_y_reg, test_y_reg = train_test_split(x_reg_s, y_reg, \n                                             test_size = 0.2, random_state = 12)\nRf_reg.fit(train_x_reg, train_y_reg)\npreds = Rf_reg.predict(test_x_reg)\nprint(\"Accuracy: \", accuracy_score(test_y_reg, preds))\nprint(\"ROC AUC Score: \",roc_auc_score(test_y_reg, preds))\nprint(\"Confusion matrix: \", confusion_matrix(test_y_reg, preds))     \n","d7d1b155":"wandb.login()","01122fd1":"import math\nsweep_config = {\n    \"method\": \"random\", # try grid or random\n    \"metric\": {\n      \"name\": \"roc_auc_score\",\n      \"goal\": \"maximize\"   \n    },\n    \"parameters\": {\n        \"booster\": {\n            \"values\": [\"gbtree\"]\n        },\n        \"max_depth\": {\n            \"values\": [6, 7, 8, 9,10,11,12]\n        },\n        \"learning_rate\": {\n            \"values\": [0.01, 0.05,0.1, 0.2, 0.3]\n        },\n        \"subsample\": {\n            \"values\": [1, 0.9, 0.8, 0.75, 0.4, 0.5, 0.3]\n        },\n        \"n_estimators\": {\n            \"values\": [200, 250, 500, 1000, 2000, 2500, 3000, 5000]\n        },\n        \"gamma\": {\n            \"distribution\": 'uniform', \n            \"min\": math.exp(-4), \n            \"max\": math.exp(4)\n        },\n        \"reg_lambda\": {\n            \"distribution\": 'uniform', \n            \"min\": math.exp(-4), \n            \"max\": math.exp(4)\n        },\n        \"reg_alpha\": {\n            \"distribution\": 'uniform', \n            \"min\": math.exp(-4), \n            \"max\": math.exp(4)\n        },\n        \"colsample_bytree\":{\n            \"distribution\": \"uniform\",\n            \"min\": 0.2,\n            \"max\": 1\n        },\n        \"colsample_bylevel\":{\n            \"distribution\": \"uniform\",\n            \"min\": 0.2,\n            \"max\": 1,\n        }\n        \n    }\n}\nsweep_id = wandb.sweep(sweep_config, project=\"SPP-XGBoost-sweeps\")","762290b9":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler","8a6edcbc":"ss = StandardScaler()\nimp = SimpleImputer(strategy = \"mean\")\nx = df.drop([\"song_popularity\", \"id\"], axis=1)\nx = imp.fit_transform(x)\nx = ss.fit_transform(x)\ny = df[\"song_popularity\"]\ny.shape","cc4e04c7":"ss = StandardScaler()\nx = df_regs_drop.drop(\"song_popularity\", axis = 1)\nx = ss.fit_transform(x)\ny = df[\"song_popularity\"]\ny.shape","5bb9e746":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","77742a76":"\n\n# load data\ndef train():\n    config_defaults = {\n    \"booster\": \"gbtree\",\n    \"max_depth\": 9,\n    \"learning_rate\": 0.1,\n    \"subsample\": 1,\n    \"seed\": 117,\n    \"test_size\": 0.2,\n    'objective': 'binary:logistic'\n    }\n\n    wandb.init(config=config_defaults)  # defaults are over-ridden during the sweep\n    config = wandb.config\n\n    # load data and split into predictors and targets\n    \n    # split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(x, y,\n                                                      test_size=config.test_size,\n                                                      random_state=config.seed)\n    os = SMOTETomek(1)\n    X_train,y_train = os.fit_resample(X_train,y_train)\n    # fit model on train\n    model = XGBClassifier(booster=config.booster, max_depth=config.max_depth,\n                        learning_rate=config.learning_rate, subsample=config.subsample, \n                        gpu_id = 0, eval_metric=\"auc\")\n    model.fit(X_train, y_train, eval_set=[(X_test,y_test)], verbose=False)\n\n    # make predictions on test\n    y_pred = model.predict(X_test)\n    \n    predictions = [round(value) for value in y_pred]\n\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Accuracy: {int(accuracy * 100.)}%\")\n    rocauc_score = roc_auc_score(y_test, predictions)\n    print(\"Roc AUC score: \", rocauc_score)\n    wandb.log({\"roc_auc_score\": rocauc_score})\n    wandb.log({\"Accuracy\": accuracy})","019c8fb7":"wandb.agent(sweep_id, train, count=50)","0f1e5fba":"test_df = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\nxt = test_df.drop([\"id\"], axis=1)\n\nxt = imp.transfrom(xt)\nxt = ss.transform(xt)\n","a571993d":"\nos = SMOTETomek(1)\nX_train,y_train = os.fit_resample(X_train,y_train)\n    # fit model on train\nparams = {'max_depth': 6,\n 'n_estimators': 250,\n 'learning_rate': 0.01,\n 'subsample': 0.3,\n 'colsample_bytree': 0.2427,\n 'colsample_bylevel': 0.5882,\n 'min_child_weight': 0.281257758038499,\n 'reg_lambda': 28.491,\n 'reg_alpha': 27.5,\n 'gamma': 8.137,\n \"objective\": 'binary:logistic'}\nmodel = XGBClassifier(**params, booster='gbtree',eval_metric=\"auc\",\n                      gpu_id=0)\nmodel.fit(X_train, y_train, eval_set=[(X_test,y_test)], early_stopping_rounds = 300, verbose=True)\n\n# make predictions on test\ny_pred = model.predict(X_test)\n\npredictions = [round(value) for value in y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {int(accuracy * 100.)}%\")\nrocauc_score = roc_auc_score(y_test, predictions)\nprint(\"Roc AUC score: \", rocauc_score)","bb65e7f4":"\nwandb.sklearn.plot_classifier(Rf_reg, train_x_reg, test_x_reg, train_y_reg, \n                              test_y_reg, preds, Rf_reg.predict_proba(test_x_reg), \n                              labels=[0,1], model_name=\"RF1\", feature_names=x_reg.columns)\n                        ","7d31a3c8":"x_reg.columns","d232ca9a":"## Check Null values","932dd0c1":"## Plotting column distributions and correlation matrix","ed6cc983":"Replacing Missing values with Mean","d5e47fba":"**Experimenting two approaches to replace missing values**\n- Replacing with mean of the column values.\n- Using linear regression to missing predict values.","5a31975f":"**Visualizing missing values**","c341a7d2":"- Can Use Boxcox or log transformations to handle skewness.\n- To handle missing values need to experiment with different approaches.","09ccbf74":"**The distribution plots shows that the data in these columns is highly skewed.**","a249782b":"## Testing both datasets on base Random Forest Models","1620cab2":"## Handling Skewness and Missing Values\n","51277438":"**Both scatter plots and correlation matrix shows there is little to no correlation between columns**  \nSkewness and missing values needs to be fixed to make this data usable for a model.","24055a77":"Replacing values using regression","104ee89b":"After handling skewness and missing values, correlation between columns where we handled replaced missing values using **Regression improves.**"}}