{"cell_type":{"540162e5":"code","5f22abcd":"code","4483e860":"code","afa419f2":"code","5af39e55":"code","36169648":"code","62d64045":"code","0b40fce8":"code","82dbf3d9":"code","0abd40cf":"code","9d1cd8bc":"code","135a028e":"code","a8e9911a":"code","198b2d0b":"code","85877938":"code","4089ccb5":"code","94cc4687":"code","dd6e2dd7":"markdown","c2540df9":"markdown","814493b4":"markdown","2b346e35":"markdown","d7dbbb83":"markdown","8c297947":"markdown"},"source":{"540162e5":"#location of the training data \ndata_location =  \"..\/input\/flickr8k\"\n#copy dataloader\n!cp ..\/input\/data-loader\/data_loader.py .\n\n#imports\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as T\n\n#custom imports \nfrom data_loader import FlickrDataset,get_data_loader","5f22abcd":"#show the tensor image\nimport matplotlib.pyplot as plt\ndef show_image(img, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    \n    #unnormalize \n    img[0] = img[0] * 0.229\n    img[1] = img[1] * 0.224 \n    img[2] = img[2] * 0.225 \n    img[0] += 0.485 \n    img[1] += 0.456 \n    img[2] += 0.406\n    \n    img = img.numpy().transpose((1, 2, 0))\n    \n    \n    plt.imshow(img)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated","4483e860":"#Initiate the Dataset and Dataloader\n\n#setting the constants\ndata_location =  \"..\/input\/flickr8k\"\nBATCH_SIZE = 256\n# BATCH_SIZE = 6\nNUM_WORKER = 4\n\n#defining the transform to be applied\ntransforms = T.Compose([\n    T.Resize(226),                     \n    T.RandomCrop(224),                 \n    T.ToTensor(),                               \n    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n])\n\n\n#testing the dataset class\ndataset =  FlickrDataset(\n    root_dir = data_location+\"\/Images\",\n    caption_file = data_location+\"\/captions.txt\",\n    transform=transforms\n)\n\n#writing the dataloader\ndata_loader = get_data_loader(\n    dataset=dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKER,\n    shuffle=True,\n    # batch_first=False\n)\n\n#vocab_size\nvocab_size = len(dataset.vocab)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","afa419f2":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as T","5af39e55":"class EncoderCNN(nn.Module):\n    def __init__(self):\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet50(pretrained=True)\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n        \n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n\n    def forward(self, images):\n        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n        return features\n","36169648":"#Bahdanau Attention\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n        super(Attention, self).__init__()\n        \n        self.attention_dim = attention_dim\n        \n        self.W = nn.Linear(decoder_dim,attention_dim)\n        self.U = nn.Linear(encoder_dim,attention_dim)\n        \n        self.A = nn.Linear(attention_dim,1)\n        \n        \n        \n        \n    def forward(self, features, hidden_state):\n        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n        \n        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n        \n        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n        \n        \n        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n        \n        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n        \n        return alpha,attention_weights\n        ","62d64045":"#Attention Decoder\nclass DecoderRNN(nn.Module):\n    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n        super().__init__()\n        \n        #save the model param\n        self.vocab_size = vocab_size\n        self.attention_dim = attention_dim\n        self.decoder_dim = decoder_dim\n        \n        self.embedding = nn.Embedding(vocab_size,embed_size)\n        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n        \n        \n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        \n        \n        self.fcn = nn.Linear(decoder_dim,vocab_size)\n        self.drop = nn.Dropout(drop_prob)\n        \n        \n    \n    def forward(self, features, captions):\n        \n        #vectorize the caption\n        embeds = self.embedding(captions)\n        \n        # Initialize LSTM state\n        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n        \n        #get the seq length to iterate\n        seq_length = len(captions[0])-1 #Exclude the last one\n        batch_size = captions.size(0)\n        num_features = features.size(1)\n        \n        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n                \n        for s in range(seq_length):\n            alpha,context = self.attention(features, h)\n            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n            h, c = self.lstm_cell(lstm_input, (h, c))\n                    \n            output = self.fcn(self.drop(h))\n            \n            preds[:,s] = output\n            alphas[:,s] = alpha  \n        \n        \n        return preds, alphas\n    \n    def generate_caption(self,features,max_len=20,vocab=None):\n        # Inference part\n        # Given the image features generate the captions\n        \n        batch_size = features.size(0)\n        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n        \n        alphas = []\n        \n        #starting input\n        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n        embeds = self.embedding(word)\n\n        \n        captions = []\n        \n        for i in range(max_len):\n            alpha,context = self.attention(features, h)\n            \n            \n            #store the apla score\n            alphas.append(alpha.cpu().detach().numpy())\n            \n            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n            h, c = self.lstm_cell(lstm_input, (h, c))\n            output = self.fcn(self.drop(h))\n            output = output.view(batch_size,-1)\n        \n            \n            #select the word with most val\n            predicted_word_idx = output.argmax(dim=1)\n            \n            #save the generated word\n            captions.append(predicted_word_idx.item())\n            \n            #end if <EOS detected>\n            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n                break\n            \n            #send generated word as the next caption\n            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n        \n        #covert the vocab idx to words and return sentence\n        return [vocab.itos[idx] for idx in captions],alphas\n    \n    \n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n","0b40fce8":"class EncoderDecoder(nn.Module):\n    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n        super().__init__()\n        self.encoder = EncoderCNN()\n        self.decoder = DecoderRNN(\n            embed_size=embed_size,\n            vocab_size = len(dataset.vocab),\n            attention_dim=attention_dim,\n            encoder_dim=encoder_dim,\n            decoder_dim=decoder_dim\n        )\n        \n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs\n","82dbf3d9":"#Hyperparams\nembed_size=300\nvocab_size = len(dataset.vocab)\nattention_dim=256\nencoder_dim=2048\ndecoder_dim=512\nlearning_rate = 3e-4\n","0abd40cf":"#init model\nmodel = EncoderDecoder(\n    embed_size=300,\n    vocab_size = len(dataset.vocab),\n    attention_dim=256,\n    encoder_dim=2048,\n    decoder_dim=512\n).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","9d1cd8bc":"#helper function to save the model\ndef save_model(model,num_epochs):\n    model_state = {\n        'num_epochs':num_epochs,\n        'embed_size':embed_size,\n        'vocab_size':len(dataset.vocab),\n        'attention_dim':attention_dim,\n        'encoder_dim':encoder_dim,\n        'decoder_dim':decoder_dim,\n        'state_dict':model.state_dict()\n    }\n\n    torch.save(model_state,'attention_model_state.pth')","135a028e":"num_epochs = 25\nprint_every = 100\n\nfor epoch in range(1,num_epochs+1):   \n    for idx, (image, captions) in enumerate(iter(data_loader)):\n        image,captions = image.to(device),captions.to(device)\n\n        # Zero the gradients.\n        optimizer.zero_grad()\n\n        # Feed forward\n        outputs,attentions = model(image, captions)\n\n        # Calculate the batch loss.\n        targets = captions[:,1:]\n        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n        \n        # Backward pass.\n        loss.backward()\n\n        # Update the parameters in the optimizer.\n        optimizer.step()\n\n        if (idx+1)%print_every == 0:\n            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n            \n            \n            #generate the caption\n            model.eval()\n            with torch.no_grad():\n                dataiter = iter(data_loader)\n                img,_ = next(dataiter)\n                features = model.encoder(img[0:1].to(device))\n                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n                caption = ' '.join(caps)\n                show_image(img[0],title=caption)\n                \n            model.train()\n        \n    #save the latest model\n    save_model(model,epoch)","a8e9911a":"#generate caption\ndef get_caps_from(features_tensors):\n    #generate the caption\n    model.eval()\n    with torch.no_grad():\n        features = model.encoder(features_tensors.to(device))\n        caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n        caption = ' '.join(caps)\n        show_image(features_tensors[0],title=caption)\n    \n    return caps,alphas\n\n#Show attention\ndef plot_attention(img, result, attention_plot):\n    #untransform\n    img[0] = img[0] * 0.229\n    img[1] = img[1] * 0.224 \n    img[2] = img[2] * 0.225 \n    img[0] += 0.485 \n    img[1] += 0.456 \n    img[2] += 0.406\n    \n    img = img.numpy().transpose((1, 2, 0))\n    temp_image = img\n\n    fig = plt.figure(figsize=(15, 15))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = attention_plot[l].reshape(7,7)\n        \n        ax = fig.add_subplot(len_result\/\/2,len_result\/\/2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent())\n        \n\n    plt.tight_layout()\n    plt.show()","198b2d0b":"#show any 1\ndataiter = iter(data_loader)\nimages,_ = next(dataiter)\n\nimg = images[0].detach().clone()\nimg1 = images[0].detach().clone()\ncaps,alphas = get_caps_from(img.unsqueeze(0))\n\nplot_attention(img1, caps, alphas)","85877938":"#show any 1\ndataiter = iter(data_loader)\nimages,_ = next(dataiter)\n\nimg = images[0].detach().clone()\nimg1 = images[0].detach().clone()\ncaps,alphas = get_caps_from(img.unsqueeze(0))\n\nplot_attention(img1, caps, alphas)","4089ccb5":"#show any 1\ndataiter = iter(data_loader)\nimages,_ = next(dataiter)\n\nimg = images[0].detach().clone()\nimg1 = images[0].detach().clone()\ncaps,alphas = get_caps_from(img.unsqueeze(0))\n\nplot_attention(img1, caps, alphas)","94cc4687":"#show any 1\ndataiter = iter(data_loader)\nimages,_ = next(dataiter)\n\nimg = images[0].detach().clone()\nimg1 = images[0].detach().clone()\ncaps,alphas = get_caps_from(img.unsqueeze(0))\n\nplot_attention(img1, caps, alphas)","dd6e2dd7":"## 5) Training Job from above configs","c2540df9":"## 6 Visualizing the attentions\nDefining helper functions\n<li>Given the image generate captions and attention scores<\/li>\n<li>Plot the attention scores in the image<\/li>","814493b4":"### 3) Defining the Model Architecture\n\nModel is seq2seq model. In the **encoder** pretrained ResNet model is used to extract the features. Decoder, is the implementation of the Bahdanau Attention Decoder. In the decoder model **LSTM cell**.","2b346e35":"### 2) **<b>Implementing the Helper function to plot the Tensor image**","d7dbbb83":"### 1) Initial Imports and loading the utils function. The dataset is used is <a href='https:\/\/www.kaggle.com\/adityajn105\/flickr8k'>Flickr 8k<\/a> from kaggle.<br>Custom dataset and dataloader is implemented in <a href=\"https:\/\/www.kaggle.com\/mdteach\/torch-data-loader-flicker-8k\">this<\/a> notebook.","8c297947":"### 4) Setting Hypperparameter and Init the model"}}