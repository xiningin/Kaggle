{"cell_type":{"ba87de1d":"code","9ea5ed30":"code","7a9b85ba":"code","d7d7e99b":"code","ad2eda51":"code","b0528717":"code","26637f8e":"code","460363ba":"code","308acbe7":"code","430ae34e":"code","2ca265f1":"code","38941df9":"code","642408f0":"code","1a014c60":"code","18ae61ce":"code","a3404938":"code","e0724f64":"code","2e8884b7":"code","0efaf8e8":"code","4d4207f6":"code","f720fcaa":"code","ae11ba12":"code","6a9b1719":"code","80ded6fb":"code","b6f46a77":"code","f50e44d5":"code","fa730d9a":"code","a4842432":"code","152859c9":"code","8b07522b":"code","bcd5f4c7":"code","0bce288b":"code","e23bbe0e":"code","4b868755":"code","d1be2c65":"code","9f17f8fa":"code","8e27d84d":"code","34dc84b4":"code","8d92db0a":"code","c08a0ed4":"code","83bb49c6":"code","b0558d4f":"code","45ca01c4":"code","f68047c6":"code","d5e7e9f9":"code","7ed99403":"code","3d1ec2a0":"code","607c35c1":"code","7bcb83ef":"code","fed36670":"code","b4f27863":"code","0fb1c1c1":"code","35c7cb80":"code","1a6a7f27":"code","39fab543":"code","d917d4c5":"code","c6c9e552":"code","d8330045":"code","f3e3daa9":"markdown","295d141a":"markdown","75344866":"markdown","0d531325":"markdown","ba3ae59a":"markdown","ab9ff24d":"markdown","942b3fec":"markdown","2d8c377b":"markdown","6debdb0d":"markdown","ff7e54d3":"markdown","943f494a":"markdown","4f1c999b":"markdown","f1309ffa":"markdown","b53256fd":"markdown","937d9f8a":"markdown","657e1dae":"markdown","2f1e9643":"markdown","10917686":"markdown","b62be7e2":"markdown","be60bf39":"markdown","aa2471f3":"markdown","3f3e6143":"markdown"},"source":{"ba87de1d":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","9ea5ed30":"train = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/train.csv\")\nlabel = train['income']\n\ndel train['income']\n\ntest = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/test.csv\")","7a9b85ba":"# \ub77c\ubca8 \uac12 \uc778\ucf54\ub529\nlabel = label.map(lambda x: 1 if x == '>50K' else 0)","d7d7e99b":"del train['id']\ndel test['id']","ad2eda51":"tmp_train = train.copy()\ntmp_test  = test.copy()","b0528717":"tmp_train.head()","26637f8e":"tmp_train.info()","460363ba":"tmp_train.describe()","308acbe7":"tmp_test.head()","430ae34e":"has_na_columns = ['workclass', 'occupation', 'native_country']","2ca265f1":"(tmp_train[has_na_columns] == '?').sum()","38941df9":"for c in has_na_columns:\n    tmp_train.loc[train[c] == '?', c] = train[c].mode()[0]\n    tmp_test.loc[test[c]   == '?', c] = test[c].mode()[0]","642408f0":"(tmp_train[has_na_columns] == '?').sum()","1a014c60":"tmp_train['capital_gain'].plot.hist()","18ae61ce":"tmp_train['log_capital_gain'] = train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_test['log_capital_gain']  = test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n\ntmp_train['log_capital_gain'].plot.hist()","a3404938":"train['capital_loss'].plot.hist()","e0724f64":"tmp_train['log_capital_loss'] = train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_test['log_capital_loss'] = test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n\ntmp_train['log_capital_loss'].plot.hist()","2e8884b7":"tmp_train = tmp_train.drop(columns=['capital_loss', 'capital_gain'])\ntmp_test  = tmp_test.drop(columns=['capital_loss', 'capital_gain'])","0efaf8e8":"tmp_train.head()","4d4207f6":"from sklearn.model_selection import train_test_split\n\ntmp_train, tmp_valid, y_train, y_valid = train_test_split(tmp_train, label, \n                                                          test_size=0.3,\n                                                          random_state=2020,\n                                                          shuffle=True,\n                                                          stratify=label)","f720fcaa":"tmp_train.head()","ae11ba12":"# \uc778\ub371\uc2a4 \ucd08\uae30\ud654\ntmp_train = tmp_train.reset_index(drop=True) # drop=True : \uae30\uc874 \uc778\ub371\uc2a4\ub85c \uc0c8\ub85c\uc6b4 \uceec\ub7fc \ub9cc\ub4e4\uc9c0 \ub9d0\uc544\ub77c\ntmp_valid = tmp_valid.reset_index(drop=True)\ntmp_test  = tmp_test.reset_index(drop=True)","6a9b1719":"tmp_train.head()","80ded6fb":"tmp_train.columns","b6f46a77":"cat_columns = [c for c, t in zip(tmp_train.dtypes.index, tmp_train.dtypes) if t == 'O'] \nnum_columns = [c for c in tmp_train.columns if c not in cat_columns]\n\nprint('\ubc94\uc8fc\ud615 \ubcc0\uc218: \\n{}\\n\\n \uc218\uce58\ud615 \ubcc0\uc218: \\n{}\\n'.format(cat_columns, num_columns))","f50e44d5":"from sklearn.preprocessing import StandardScaler\n# \uc2a4\ucf00\uc77c\ub9c1, \uc804\ucc98\ub9ac\uc758 \uae30\uc900 : train data set\nscaler = StandardScaler()\ntmp_train[num_columns] = scaler.fit_transform(tmp_train[num_columns])\ntmp_valid[num_columns] = scaler.transform(tmp_valid[num_columns])\ntmp_test[num_columns]  = scaler.transform(tmp_test[num_columns])","fa730d9a":"tmp_train.describe()","a4842432":"tmp_valid.describe()","152859c9":"tmp_test.describe()","8b07522b":"from sklearn.preprocessing import OneHotEncoder\n\ntmp_all = pd.concat([tmp_train, tmp_valid, tmp_test])\n\nohe = OneHotEncoder(sparse=False)\nohe.fit(tmp_all[cat_columns])","bcd5f4c7":"tmp_all[cat_columns]","0bce288b":"ohe_columns = list()\nfor lst in ohe.categories_:\n    ohe_columns += lst.tolist()","e23bbe0e":"new_train_cat = pd.DataFrame(ohe.transform(tmp_train[cat_columns]), columns=ohe_columns)\nnew_valid_cat = pd.DataFrame(ohe.transform(tmp_valid[cat_columns]), columns=ohe_columns)\nnew_test_cat  = pd.DataFrame(ohe.transform(tmp_test[cat_columns]), columns=ohe_columns)","4b868755":"new_train_cat.head()","d1be2c65":"cat_columns","9f17f8fa":"tmp_train = pd.concat([tmp_train, new_train_cat], axis=1) # \uc815\ud615\ub370\uc774\ud130\uc5d0\uc11c axis 0 : \ud589\uc73c\ub85c \ubd99\uc784\ntmp_valid = pd.concat([tmp_valid, new_valid_cat], axis=1) # axis 1 : \uc5f4\ub85c \ubd99\uc784\ntmp_test = pd.concat([tmp_test, new_test_cat], axis=1)\n\n# \uae30\uc874 \ubc94\uc8fc\ud615 \ubcc0\uc218 \uc81c\uac70\ntmp_train = tmp_train.drop(columns=cat_columns)\ntmp_valid = tmp_valid.drop(columns=cat_columns)\ntmp_test = tmp_test.drop(columns=cat_columns)","8e27d84d":"tmp_train.head()","34dc84b4":"tmp_y_train = y_train\ntmp_y_valid = y_valid","8d92db0a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import f1_score","c08a0ed4":"lr = LogisticRegression()\n\nlr.fit(tmp_train, tmp_y_train)\n\ny_pred = lr.predict(tmp_valid)\n\nprint(f\"Logistic Regression F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","83bb49c6":"svc = SVC()\n\nsvc.fit(tmp_train, tmp_y_train)\n\ny_pred = svc.predict(tmp_valid)\n\nprint(f\"Support Vector Machine F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","b0558d4f":"rf = RandomForestClassifier()\n\nrf.fit(tmp_train, tmp_y_train)\n\ny_pred = rf.predict(tmp_valid)\n\nprint(f\"RandomForest F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","45ca01c4":"xgb = XGBClassifier(tree_method='gpu_hist')\n\nxgb.fit(tmp_train, tmp_y_train)\n\ny_pred = xgb.predict(tmp_valid)\n\nprint(f\"XGBoost F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","f68047c6":"lgb = LGBMClassifier(tree_method='gpu_hist')\n\nlgb.fit(tmp_train, tmp_y_train)\n\ny_pred = lgb.predict(tmp_valid)\n\nprint(f\"LightGBM F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","d5e7e9f9":"def preprocess(x_train, x_valid, x_test):\n    tmp_x_train = x_train.copy()\n    tmp_x_valid = x_valid.copy()\n    tmp_x_test  = x_test.copy()\n    \n    tmp_x_train = tmp_x_train.reset_index(drop=True)\n    tmp_x_valid = tmp_x_valid.reset_index(drop=True)\n    tmp_x_test  = tmp_x_test.reset_index(drop=True)\n    \n    for c in has_na_columns:\n        tmp_x_train.loc[tmp_x_train[c] == '?', c] = tmp_x_train[c].mode()[0]\n        tmp_x_valid.loc[tmp_x_valid[c] == '?', c] = tmp_x_valid[c].mode()[0]\n        tmp_x_test.loc[tmp_x_test[c]   == '?', c] = tmp_x_test[c].mode()[0]\n    \n    tmp_x_train['log_capital_loss'] = tmp_x_train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_loss'] = tmp_x_valid['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_loss'] = tmp_x_test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train['log_capital_gain'] = tmp_x_train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_gain'] = tmp_x_valid['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_gain'] = tmp_x_test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train = tmp_x_train.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_valid = tmp_x_valid.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_test  = tmp_x_test.drop(columns=['capital_loss', 'capital_gain'])\n    \n    scaler = StandardScaler()\n    tmp_x_train[num_columns] = scaler.fit_transform(tmp_x_train[num_columns])\n    tmp_x_valid[num_columns] = scaler.transform(tmp_x_valid[num_columns])\n    tmp_x_test[num_columns]  = scaler.transform(tmp_x_test[num_columns])\n    \n    tmp_all = pd.concat([tmp_x_train, tmp_x_valid, tmp_x_test])\n\n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit(tmp_all[cat_columns])\n    \n    ohe_columns = list()\n    for lst in ohe.categories_:\n        ohe_columns += lst.tolist()\n    \n    tmp_train_cat = pd.DataFrame(ohe.transform(tmp_x_train[cat_columns]), columns=ohe_columns)\n    tmp_valid_cat = pd.DataFrame(ohe.transform(tmp_x_valid[cat_columns]), columns=ohe_columns)\n    tmp_test_cat  = pd.DataFrame(ohe.transform(tmp_x_test[cat_columns]), columns=ohe_columns)\n    \n    tmp_x_train = pd.concat([tmp_x_train, tmp_train_cat], axis=1)\n    tmp_x_valid = pd.concat([tmp_x_valid, tmp_valid_cat], axis=1)\n    tmp_x_test = pd.concat([tmp_x_test, tmp_test_cat], axis=1)\n\n    tmp_x_train = tmp_x_train.drop(columns=cat_columns)\n    tmp_x_valid = tmp_x_valid.drop(columns=cat_columns)\n    tmp_x_test = tmp_x_test.drop(columns=cat_columns)\n    \n    return tmp_x_train.values, tmp_x_valid.values, tmp_x_test.values","7ed99403":"def xgb_f1(y, t, threshold=0.5):\n    t = t.get_label()\n    y_bin = (y > threshold).astype(int) \n    return 'f1',f1_score(t, y_bin, average='micro')","3d1ec2a0":"from sklearn.model_selection import StratifiedKFold\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)","607c35c1":"val_scores = list()\noof_pred = np.zeros((test.shape[0],))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 Log Loss \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","7bcb83ef":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","fed36670":"val_scores = list()\n\nnew_x_train_list = [np.zeros((train.shape[0], 1)) for _ in range(4)]\nnew_x_test_list  = [np.zeros((test.shape[0], 1)) for _ in range(4)]\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    print(f\"Fold {i} Start\")\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clfs = [LogisticRegression(), \n            RandomForestClassifier(), \n            XGBClassifier(tree_method='gpu_hist'), \n            LGBMClassifier(tree_method='gpu_hist')]\n    \n    for model_idx, clf in enumerate(clfs):\n        clf.fit(x_train, y_train)\n        \n        new_x_train_list[model_idx][val_idx, :] = clf.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n        new_x_test_list[model_idx][:] += clf.predict_proba(x_test)[:, 1].reshape(-1, 1) \/ n_splits","b4f27863":"new_x_train_list","0fb1c1c1":"new_x_test_list","35c7cb80":"new_train = pd.DataFrame(np.concatenate(new_x_train_list, axis=1), columns=None)\nnew_label = label\nnew_test = pd.DataFrame(np.concatenate(new_x_test_list, axis=1), columns=None)\n\nnew_train.shape, new_label.shape, new_test.shape","1a6a7f27":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(new_train, new_label)):\n    x_train, y_train = new_train.iloc[trn_idx, :], new_label[trn_idx]\n    x_valid, y_valid = new_train.iloc[val_idx, :], new_label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test  = scaler.transform(new_test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","39fab543":"submit = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/sample_submission.csv\")","d917d4c5":"submit.loc[:, 'prediction'] = (oof_pred > 0.5).astype(int)","c6c9e552":"submit","d8330045":"submit.to_csv('submission.csv', index=False)","f3e3daa9":"#### 2) 2 Stage Meta Model \ud559\uc2b5\nnew_train, new_test\uc5d0 \ub4e4\uc5b4\uc788\ub294 \ubcc0\uc218\ub294 \ubaa8\ub450 \uc218\uce58\ud615 \ubcc0\uc218\uc774\ubbc0\ub85c Standard Scaling\ub9cc \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.<br>\n\uc0c8\ub85c \uc0dd\uc131\ud55c \ub370\uc774\ud130 new_train, new_test \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 2 Stage Meta Model\uc744 \ud559\uc2b5\ud558\uace0 \uacb0\uacfc\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.","295d141a":"#### 6) \uc778\ucf54\ub529\n\ubc94\uc8fc\ud615 \ubcc0\uc218\ub97c \uc218\uce58\ud615 \ubcc0\uc218\ub85c \uc778\ucf54\ub529 \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ubc94\uc8fc\ud615 \ubcc0\uc218\uc5d0\ub294 Onehot Encoding\uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4.","75344866":"### 5. Stacking \uc559\uc0c1\ube14\n2 stage \uc559\uc0c1\ube14\uc778 Stacking \uc559\uc0c1\ube14 \uc785\ub2c8\ub2e4. Stacking \uc559\uc0c1\ube14\uc740 \uc218\uc2ed\uac1c\uc758 1 stage \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubaa8\uc544 2 stage \ubaa8\ub378\ub85c \ud559\uc2b5 \ud6c4 \uacb0\uacfc\ub97c \ub0b4\ub294 \uc559\uc0c1\ube14 \ubc29\uc2dd\uc785\ub2c8\ub2e4.\n\n#### 1) 1 stage \uacb0\uacfc \ubaa8\uc73c\uae30\nStacking \uc559\uc0c1\ube14\uc744 \uc9c4\ud589\ud560 1 stage \ubaa8\ub378\uc758 \uacb0\uacfc(train, test)\ub97c \ubaa8\uc74d\ub2c8\ub2e4. ","0d531325":"### 1. \ub370\uc774\ud130 \uc804\ucc98\ub9ac","ba3ae59a":"#### 4) Log \ubcc0\ud658\ncapital_gain \ubcc0\uc218\uc640 capital_loss \ubcc0\uc218\uc758 \ubd84\ud3ec\uac00 \ud55c\ucabd\uc73c\ub85c \uce58\uc6b0\uce5c \ud615\ud0dc\uc774\ubbc0\ub85c Log \ubcc0\ud658\uc744 \ud1b5\ud574 \ubd84\ud3ec\uc758 \ud615\ud0dc\ub97c \uc870\uc815\ud574\uc8fc\uaca0\uc2b5\ub2c8\ub2e4.","ab9ff24d":"ID \uceec\ub7fc\uc740 \ud589\uc758 \uc2dd\ubcc4\uc790\ub85c \ud544\uc694 \uc5c6\ub294 \uceec\ub7fc\uc774\ubbc0\ub85c \uc0ad\uc81c\ud558\uaca0\uc2b5\ub2c8\ub2e4. ","942b3fec":"### 6. \uc2e4\uc2b5","2d8c377b":"#### 5) \ub370\uc774\ud130 \ucabc\uac1c\uae30\n##### 1. Train, Valid, Test Set\n* Train Data : \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294\ub370 \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \uc54c\uace0 \uc788\ub294 \ud559\uc2b5\ud560 \ub370\uc774\ud130, \uacfc\uac70 \ub370\uc774\ud130)\n* Valid Data : \ud559\uc2b5\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uac80\uc99d\ud558\ub294 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \ud559\uc2b5\ud558\uc9c0 \uc54a\uc744 \ub370\uc774\ud130, \ubaa8\ub378 \uac80\uc99d\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130, \uacfc\uac70 \ub370\uc774\ud130)\n* Test Data : \ud559\uc2b5\ud55c \ubaa8\ub378\ub85c \uc608\uce21\ud560 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \uc608\uce21\ud560 \ub370\uc774\ud130, \ubbf8\ub798 \ub370\uc774\ud130)","6debdb0d":"#### 2) \ub370\uc774\ud130 \ud655\uc778\n.head(), .describe(), .info() \ub4f1\uc758 \ud568\uc218\ub85c \ub370\uc774\ud130\ub97c \ud655\uc778\ud569\ub2c8\ub2e4. ","ff7e54d3":"#### 3) \uacb0\uce21\uce58 \ucc98\ub9ac\n\uc774\uc804 \ud0dc\uc9c4\ub2d8 \uac15\uc758\uc5d0\uc11c 'workclass', 'occupation', 'native_country' \uceec\ub7fc\uc5d0 \uacb0\uce21\uce58\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. <br>\n\uc77c\ubc18\uc801\uc778 \uacb0\uce21\uce58\uc640 \ub2e4\ub974\uac8c '?'\ub85c \ud45c\ud604\ub418\uc5b4\uc788\ub294 \uac12\ub4e4\uc740 \ud574\ub2f9 \uceec\ub7fc\uc758 \ucd5c\ube48\uac12\uc73c\ub85c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\n##### \ubc94\uc8fc\ud615 \ubcc0\uc218\uc758 \uacbd\uc6b0 \uac00\uc7a5 \uac04\ub2e8\ud558\uac8c \ucd5c\ube48\uac12\uc73c\ub85c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc9c0\ub9cc, \ub2e4\ub978 \uceec\ub7fc\uc744 \ud544\ud130\ub9c1\ud574\uc11c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. ex) education_num \ub4f1","943f494a":"#### 4) XGBoost","4f1c999b":"#### 3) \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8","f1309ffa":"#### 1) \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ubaa8\ub378","b53256fd":"#### 2) \uc11c\ud3ec\ud2b8 \ubca1\ud130 \uba38\uc2e0(rbf \ucee4\ub110)","937d9f8a":"#### 1) CSV \ud30c\uc77c \ubd88\ub7ec\uc624\uae30","657e1dae":"#### 5) LightGBM","2f1e9643":"#### 6) \uc2a4\ucf00\uc77c\ub9c1\nScikit-learn \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0 \uc788\ub294 Standard Scaler\ub97c \uc0ac\uc6a9\ud574\uc11c \uc218\uce58\ud615 \ubcc0\uc218\ub4e4\uc758 \ud45c\uc900\ud654\ub97c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.","10917686":"### 2. Scikit-Learn \ubd84\ub958 \ubaa8\ub378 \uc0ac\uc6a9\ud574\ubcf4\uae30\nScikit-Learn\uc758 \uae30\ubcf8 \ubd84\ub958 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\uac01 \ubaa8\ub378\uc758 \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc740 \ub300\ud68c \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc778 f1_score\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","b62be7e2":"##### \ub370\uc774\ud130 \ucabc\uac1c\uae30, Train -> (Train, Valid)\n- train_test_split \ud30c\ub77c\ubbf8\ud130 \n    - test_size  (float): Valid(test)\uc758 \ud06c\uae30\uc758 \ube44\uc728\uc744 \uc9c0\uc815\n    - random_state (int): \ub370\uc774\ud130\ub97c \ucabc\uac24 \ub54c \ub0b4\ubd80\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \ub09c\uc218 \uac12 (\ud574\ub2f9 \uac12\uc744 \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 \ub9e4\ubc88 \ub2ec\ub77c\uc9d1\ub2c8\ub2e4.)\n    - shuffle     (bool): \ub370\uc774\ud130\ub97c \ucabc\uac24 \ub54c \uc11e\uc744\uc9c0 \uc720\ubb34\n    - stratify   (array): Stratify\ub780, \ucabc\uac1c\uae30 \uc774\uc804\uc758 \ud074\ub798\uc2a4 \ube44\uc728\uc744 \ucabc\uac1c\uace0 \ub098\uc11c\ub3c4 \uc720\uc9c0\ud558\uae30 \uc704\ud574 \uc124\uc815\ud574\uc57c\ud558\ub294 \uac12\uc785\ub2c8\ub2e4. \ud074\ub798\uc2a4 \ub77c\ubca8\uc744 \ub123\uc5b4\uc8fc\uba74 \ub429\ub2c8\ub2e4.\n        - ex) \uc6d0\ubcf8 Train \ub370\uc774\ud130\uc758 \ud074\ub798\uc2a4 \ube44\uc728\uc774 (7:3) \uc774\uc5c8\ub2e4\uba74, \ucabc\uac1c\uc5b4\uc9c4 Train, Valid(test) \ub370\uc774\ud130\uc758 \ud074\ub798\uc2a4 \ube44\uc728\ub3c4 (7:3)\uc774 \ub429\ub2c8\ub2e4. \ub2f9\uc5f0\ud788 \ubd84\ub958 \ub370\uc774\ud130\uc5d0\uc11c\ub9cc \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","be60bf39":"### 4. OOF(Out-Of-Fold) \uc559\uc0c1\ube14\nk-Fold\ub97c \ud65c\uc6a9\ud574\uc11c \ubaa8\ub378 \uac80\uc99d \ubc0f \uac01 \ud3f4\ub4dc\uc758 \uacb0\uacfc\ub97c \uc559\uc0c1\ube14\ud558\ub294 OOF \uc559\uc0c1\ube14 \uc785\ub2c8\ub2e4.","aa2471f3":"### 3. k-Fold Cross Validation\n\uba3c\uc800 1. \uc5d0\uc11c \uc815\ub9ac\ud55c \uc804\ucc98\ub9ac \ud504\ub85c\uc138\uc2a4\ub97c \ud558\ub098\uc758 \ud568\uc218\ub85c \ub9cc\ub4ed\ub2c8\ub2e4.","3f3e6143":"### 7. \uacb0\uacfc \ub9cc\ub4e4\uae30"}}