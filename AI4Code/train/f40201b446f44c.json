{"cell_type":{"084aeb28":"code","4f09fb14":"code","cd7e360d":"code","373e2816":"code","cfac32b6":"code","4022628b":"code","d20d9e47":"code","825c06a8":"code","03ff0b75":"code","0c369b86":"code","c7d93dc0":"code","5b44d162":"code","9a62edfe":"code","7e5bf74e":"code","dee210e7":"code","0b8315de":"markdown","03d599c2":"markdown","71e04447":"markdown","90b4578c":"markdown","401a464c":"markdown","09edc37d":"markdown","dcff9e73":"markdown","2f750261":"markdown","26300c90":"markdown","709051c1":"markdown","20224f9d":"markdown","b78fa61a":"markdown","2eeb34da":"markdown","56a6beab":"markdown","342e8180":"markdown","6f4cb615":"markdown","c7961ad9":"markdown","284386e2":"markdown","9e0332b7":"markdown","564677b7":"markdown","b7217680":"markdown"},"source":{"084aeb28":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline","4f09fb14":"train = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\n\n# I'll remove the ID column of both train and test, cause they are of no use here\ntrain.drop(columns = 'id', inplace = True)\ntest.drop(columns = 'id', inplace =True)\n\n# Check\nprint('train columns', train.columns)\nprint('test columns', test.columns)","cd7e360d":"print('train shape', train.shape)\ntrain.describe()","373e2816":"box = train.boxplot(figsize = (30,8), rot = 90 )","cfac32b6":"# converting the describe table into a spread dataframe \nspread = pd.DataFrame(train.describe().T)\n\n# Calculating the IQR for each feature column\nIQR = spread['75%'] - spread['25%']\nspread['outliers'] = ( (spread['max'] > (spread['75%'] + 1.5 * IQR)) \n                         | (spread['min'] < (spread['25%'] - 1.5 * IQR)) )\n\n# Outliers count \nspread['outliers_count'] = ((train < (spread['25%'] - 1.5 * IQR)) \n                              | (train > (spread['75%'] + 1.5 * IQR))).sum()\n\nspread.head()","4022628b":"# Visualising the outliers\nplt.figure(figsize=(5,3))\nax = sns.countplot( x=\"outliers\", data=spread, palette=\"Set2\")\nax.set_title(\"Feature with Outliers\")","d20d9e47":"# Let's check for Null values \nprint('Total Null values:', train.isnull().sum().sum())","825c06a8":"fig, axes = plt.subplots(10,10,figsize=(15,12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(data=train, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    # sns.kdeplot(data=test, x=f'f{idx}', \n                # fill=True, \n                # ax=ax)\n    ax.set_xticks([])\n    # ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Average by class (by feature)', ha='center', fontweight='bold')\nfig.tight_layout()\nplt.show()","03ff0b75":"# Correlation with pearson corr \n\nplt.figure(figsize=(20,20))\nsns.heatmap(train.corr(), \n            annot=False, cmap=\"vlag\",\n            center = 0, linewidths=0.01,\n            cbar_kws={\"shrink\": .5}, square = True, vmin=-0.1, vmax=0.1) ","0c369b86":"# Correlation with spearman corr\nplt.figure(figsize=(20,20))\nsns.heatmap(train.corr(method = 'spearman'), \n            annot=False, cmap=\"vlag\",\n            center = 0, linewidths=0.01,\n            cbar_kws={\"shrink\": .5}, square = True, vmin=-0.1, vmax=0.1) ","c7d93dc0":"# Splitting\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train.iloc[: ,:-1], train.iloc[:,-1], \n                                                    test_size=0.4, random_state=2)\nprint(\"X-train shape\", x_train.shape, \"\\nY-test shape\" ,y_test.shape)","5b44d162":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor()\n# Train the model on training data\nrf.fit(x_train, y_train)\n# Use the forest's predict method on the test data\ny_pred = rf.predict(x_test)\n# Calculate the absolute errors\nerrors = abs(y_pred - y_test)\n# Print out the mean absolute error (mae)\nprint('Base line Mean Absolute Error', round(np.mean(errors), 4), 'degrees.')","9a62edfe":"# Splitting\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train.iloc[: ,:-1], train.iloc[:,-1], \n                                                    test_size=0.4, random_state=2)\nprint(\"X-train shape\", x_train.shape, \"\\nY-test shape\" ,y_test.shape)","7e5bf74e":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators=500, max_depth = 20, \n                           min_samples_split = 50, min_samples_leaf = 10,\n                           verbose = 1)\n\n# Train the model on training data\nrf.fit(x_train, y_train)\n\n# Use the forest's predict method on the test data\ny_pred = rf.predict(x_test)\n# Calculate the absolute errors\nerrors = abs(y_pred - y_test)\n# Print out the mean absolute error (mae)\nprint('Base line Mean Absolute Error', round(np.mean(errors), 4), 'degrees.')\n","dee210e7":"submission['loss'] = rf.predict(test)\nsubmission\nsubmission.to_csv(\"submission_rf2.2.csv\", index = False)","0b8315de":"# Importing","03d599c2":"**luckily no null values**","71e04447":"# Extracting Information\n","90b4578c":"# Model Creation","401a464c":"### Inference\nI am pretty sure this prediction can be improved with parametric tuning. \n* The following line of code is ideally derived using a CV + RF (multiple paramerters), but i couldn't do it, **ran out of RAM**. Possible reason for running out of RAM is too big of a data set and too many features to work with. \n* The model is under progress, i'll try redducing the dimensions to work around the data and then improve it as well. \n* If anyone has any suggestions, lets discuss","09edc37d":"Huge data set, thus a simple describe is not going to captute the full picture, so I'll rather do a Box plot, and see how the data looks. \n* P.S - Any other better method for suggestion","dcff9e73":"Obviously with such low correlations between the variables, an emsemble technique will come into the picture. Also since i am starting new, i'll do Random Forest regressor, one of the most used model algorithm. \n* P.s Also, i have tried multiple variation of random forest, but do to lack of a proper computing power, i'll show limited versions of the model runs. So if anyone has any sugestion as to how i can use such a huge data and run random forest, keep do comment and suggest.","2f750261":"## Correlation of Features","26300c90":"### Inference\nThanks to the huge feature numbers, visualising them together is a daunting task. Anyways, couple of thing:\n  * Aligning with the outliers calculation, the good number of data is out of normal, with hint of skewness\n  * I plotted the data wiht their y axis, to get a better understanding of the scaling, and yeah, scaling has to be done. Well as mentioned previously I'll perform std scalar and robust scalar to see how they effect the model. ","709051c1":"One thing is clear, the time needed for the code is too much and not worth the decrease in error. Need better method for this dataset.","20224f9d":"\n### Inference: \n* Performed correaltion with spearman since a lot of features are not normal in nature but all in vain, still i think spearman will be better form, since the data is not normally distributed. \n* Both the heatmaps dont have much of difference,in terms of correlation between the variables as i was expecting better correlation when considering the data nature. \n* P.s So for feature selection in case if i have to do, i'll try using using mutual info or PCA, (data size is a factor)","b78fa61a":"## RF - 2.0, 2.1\nWell i wanted to get the proper parameters for the model, but the runtime was insane for each. Must be a rookie mistake. \n* So the work around is, i'll show the output of each the runs their degree of error and run the final one again to show the improvement\n\n#### Rf -2.0\n* parameters : n_estimators=200, max_depth = 10, \n                           min_samples_split = 25, min_samples_leaf = 5,\n                           verbose = 1\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed: **47.1min finished**\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n **Base line Mean Absolute Error 6.18 degrees.**\n[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    2.2s finished\n\n#### Rf - 2.1\n* Parameters: n_estimators=200, max_depth = 20, \n                           min_samples_split = 50, min_samples_leaf = 10,\n                           verbose = 1\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed: **77.6min finished**\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n **Base line Mean Absolute Error 6.18 degrees.**\n[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    4.5s finished","2eeb34da":"## Need to check for outliers","56a6beab":"\n### Inference\n* Well the features are too less correlated, both positively and negatively with each other and the target, also the festures are lot less normally distributed, so I'll do a spearman corr heatmap, lets see if i see any changes.","342e8180":"Well there are multiple features that have outliers, so may be i'll have to tackle the outliers, maybe with min-max scaler or robust scaler. \n* P.s - Again it can change based on the dataset numbers and models that i would like to try, say a standard scalar or min-max scalar","6f4cb615":"This is my 2nd competition on kaggle and as a beginner in data science I am trying Random Forest regressor for the current TPS-AUG competiton. I have structured the notebook based on the results, it basically depicts my understanding of the results and the subsequent step. As a beginner, I think this approach will help me and other beginners like me in future. Also It's a work in progress...\n\n* P.S - I am all ears to Constructive criticism and suggestions to make my inferencing, data handling, and model generation better. ","c7961ad9":"## RF - 2.2","284386e2":"\nOOPs seems like box plot also gave a pretty weird outcome, any suggestion is welcome\n* Conclusion that can be drawn is that,   \n* feature f60 has pretty different scale compared to the rest.   \n* I need to perform some scaling to make the visualisation sensible, but for that i need to know the presence of outliers in the data. \n* Something for later but, i might want to see how data model is affected w and w\/o any scaling","9e0332b7":"## RF - baseline","564677b7":"Work in progress....","b7217680":"# Visualisation With Graphs"}}