{"cell_type":{"13bbebef":"code","d141faf9":"code","77c2a318":"code","d4a3f116":"code","c60093a1":"code","ca31e9ef":"code","1730a51c":"code","dfd7c4d4":"code","fc3a5309":"code","08ad89d0":"code","89c077f0":"code","93504461":"code","1fd2227f":"code","71afde7f":"code","f11682ad":"code","84998054":"code","d032cd5c":"code","cbc90bc9":"code","2814e9ce":"code","25130fba":"code","4bb7d2f0":"code","7b221cb5":"code","92755dab":"code","35002cb5":"code","c07ba69a":"code","6c6ebbac":"code","8d6f2ac2":"code","1ac4948b":"code","de64f411":"code","adb343f0":"code","81a38e07":"code","c4158938":"code","9e125df6":"code","fd7b07cb":"code","77485b76":"code","c1b7192b":"code","739f2d22":"code","ee9ac185":"code","e56a7cd7":"code","0be625ac":"code","4211a609":"code","4609d82a":"code","74cd0f27":"code","bc1f626b":"markdown","46e4a36a":"markdown","a5f0d5c0":"markdown","4a30b123":"markdown","ff0eac74":"markdown","82994d78":"markdown","01c630a7":"markdown","4001c05a":"markdown","bc2bf94e":"markdown","8911b187":"markdown","f20dbe40":"markdown","1ee31a48":"markdown","69702c4e":"markdown","0b2c86ce":"markdown","d1dac3e7":"markdown","9ab25e44":"markdown","ca08670c":"markdown","e0771a29":"markdown","1b9ea952":"markdown","9504b38b":"markdown","98dfe2aa":"markdown","72be564f":"markdown","c0c65d27":"markdown","74dbef4a":"markdown","539ed64f":"markdown","34de2b70":"markdown","d7fe586f":"markdown","7e0100cc":"markdown","d93b4359":"markdown","a6ffd2ab":"markdown","75421ce4":"markdown","3a5b8cc2":"markdown","12aa1f0e":"markdown","03ba16cf":"markdown","67b3b5e4":"markdown","c5df727a":"markdown","58c080e4":"markdown","07287212":"markdown","c5d19fd4":"markdown","4dc9b193":"markdown","d8c0a661":"markdown","35ab4c00":"markdown","f7969bd0":"markdown","23fbb085":"markdown"},"source":{"13bbebef":"import numpy as np \nimport pandas as pd \n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d141faf9":"training = pd.read_csv(\"..\/input\/train.csv\")\ntesting = pd.read_csv(\"..\/input\/test.csv\")","77c2a318":"training.head()","d4a3f116":"training.describe()","c60093a1":"print(training.keys())\nprint(testing.keys())","ca31e9ef":"def null_table(training, testing):\n    print(\"Training Data Frame\")\n    print(pd.isnull(training).sum()) \n    print(\" \")\n    print(\"Testing Data Frame\")\n    print(pd.isnull(testing).sum())\n\nnull_table(training, testing)","1730a51c":"training.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\ntesting.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\n\nnull_table(training, testing)","dfd7c4d4":"copy = training.copy()\ncopy.dropna(inplace = True)\nsns.distplot(copy[\"Age\"])","fc3a5309":"#the median will be an acceptable value to place in the NaN cells\ntraining[\"Age\"].fillna(training[\"Age\"].median(), inplace = True)\ntesting[\"Age\"].fillna(testing[\"Age\"].median(), inplace = True) \ntraining[\"Embarked\"].fillna(\"S\", inplace = True)\ntesting[\"Fare\"].fillna(testing[\"Fare\"].median(), inplace = True)\n\nnull_table(training, testing)\n","08ad89d0":"#can ignore the testing set for now\nsns.barplot(x=\"Sex\", y=\"Survived\", data=training)\nplt.title(\"Distribution of Survival based on Gender\")\nplt.show()\n\ntotal_survived_females = training[training.Sex == \"female\"][\"Survived\"].sum()\ntotal_survived_males = training[training.Sex == \"male\"][\"Survived\"].sum()\n\nprint(\"Total people survived is: \" + str((total_survived_females + total_survived_males)))\nprint(\"Proportion of Females who survived:\") \nprint(total_survived_females\/(total_survived_females + total_survived_males))\nprint(\"Proportion of Males who survived:\")\nprint(total_survived_males\/(total_survived_females + total_survived_males))","89c077f0":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Distribution of Survival Based on Class\")\nplt.show()\n\ntotal_survived_one = training[training.Pclass == 1][\"Survived\"].sum()\ntotal_survived_two = training[training.Pclass == 2][\"Survived\"].sum()\ntotal_survived_three = training[training.Pclass == 3][\"Survived\"].sum()\ntotal_survived_class = total_survived_one + total_survived_two + total_survived_three\n\nprint(\"Total people survived is: \" + str(total_survived_class))\nprint(\"Proportion of Class 1 Passengers who survived:\") \nprint(total_survived_one\/total_survived_class)\nprint(\"Proportion of Class 2 Passengers who survived:\")\nprint(total_survived_two\/total_survived_class)\nprint(\"Proportion of Class 3 Passengers who survived:\")\nprint(total_survived_three\/total_survived_class)","93504461":"sns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")\n#help(sns.barplot)","1fd2227f":"sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","71afde7f":"survived_ages = training[training.Survived == 1][\"Age\"]\nnot_survived_ages = training[training.Survived == 0][\"Age\"]\nplt.subplot(1, 2, 1)\nsns.distplot(survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Survived\")\nplt.ylabel(\"Proportion\")\nplt.subplot(1, 2, 2)\nsns.distplot(not_survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Didn't Survive\")\nplt.show()","f11682ad":"sns.stripplot(x=\"Survived\", y=\"Age\", data=training, jitter=True)","84998054":"sns.pairplot(training)","d032cd5c":"training.sample(5)","cbc90bc9":"testing.sample(5)","2814e9ce":"training.loc[training[\"Sex\"] == \"male\", \"Sex\"] = 0\ntraining.loc[training[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntraining.loc[training[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntraining.loc[training[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntraining.loc[training[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\ntesting.loc[testing[\"Sex\"] == \"male\", \"Sex\"] = 0\ntesting.loc[testing[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntesting.loc[testing[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntesting.loc[testing[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntesting.loc[testing[\"Embarked\"] == \"Q\", \"Embarked\"] = 2","25130fba":"testing.sample(10)","4bb7d2f0":"training[\"FamSize\"] = training[\"SibSp\"] + training[\"Parch\"] + 1\ntesting[\"FamSize\"] = testing[\"SibSp\"] + testing[\"Parch\"] + 1","7b221cb5":"training[\"IsAlone\"] = training.FamSize.apply(lambda x: 1 if x == 1 else 0)\ntesting[\"IsAlone\"] = testing.FamSize.apply(lambda x: 1 if x == 1 else 0)","92755dab":"for name in training[\"Name\"]:\n    training[\"Title\"] = training[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \nfor name in testing[\"Name\"]:\n    testing[\"Title\"] = testing[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \ntitle_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Ms\": \"Other\", \"Dona\": \"Other\", \"Rev\": \"Other\", \"Dr\": \"Other\"}\n\ntraining.replace({\"Title\": title_replacements}, inplace=True)\ntesting.replace({\"Title\": title_replacements}, inplace=True)\n\ntraining.loc[training[\"Title\"] == \"Miss\", \"Title\"] = 0\ntraining.loc[training[\"Title\"] == \"Mr\", \"Title\"] = 1\ntraining.loc[training[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntraining.loc[training[\"Title\"] == \"Master\", \"Title\"] = 3\ntraining.loc[training[\"Title\"] == \"Other\", \"Title\"] = 4\n\ntesting.loc[testing[\"Title\"] == \"Miss\", \"Title\"] = 0\ntesting.loc[testing[\"Title\"] == \"Mr\", \"Title\"] = 1\ntesting.loc[testing[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntesting.loc[testing[\"Title\"] == \"Master\", \"Title\"] = 3\ntesting.loc[testing[\"Title\"] == \"Other\", \"Title\"] = 4","35002cb5":"print(set(training[\"Title\"]))","c07ba69a":"training.sample(5)","6c6ebbac":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","8d6f2ac2":"from sklearn.metrics import make_scorer, accuracy_score ","1ac4948b":"from sklearn.model_selection import GridSearchCV","de64f411":"features = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Fare\", \"FamSize\", \"IsAlone\", \"Title\"]\nX_train = training[features] #define training features set\ny_train = training[\"Survived\"] #define training label set\nX_test = testing[features] #define testing features set\n#we don't have y_test, that is what we're trying to predict with our model","adb343f0":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","81a38e07":"svc_clf = SVC() \nsvc_clf.fit(X_training, y_training)\npred_svc = svc_clf.predict(X_valid)\nacc_svc = accuracy_score(y_valid, pred_svc)\n\nprint(acc_svc)","c4158938":"linsvc_clf = LinearSVC()\nlinsvc_clf.fit(X_training, y_training)\npred_linsvc = linsvc_clf.predict(X_valid)\nacc_linsvc = accuracy_score(y_valid, pred_linsvc)\n\nprint(acc_linsvc)","9e125df6":"rf_clf = RandomForestClassifier()\nrf_clf.fit(X_training, y_training)\npred_rf = rf_clf.predict(X_valid)\nacc_rf = accuracy_score(y_valid, pred_rf)\n\nprint(acc_rf)","fd7b07cb":"logreg_clf = LogisticRegression()\nlogreg_clf.fit(X_training, y_training)\npred_logreg = logreg_clf.predict(X_valid)\nacc_logreg = accuracy_score(y_valid, pred_logreg)\n\nprint(acc_logreg)","77485b76":"knn_clf = KNeighborsClassifier()\nknn_clf.fit(X_training, y_training)\npred_knn = knn_clf.predict(X_valid)\nacc_knn = accuracy_score(y_valid, pred_knn)\n\nprint(acc_knn)","c1b7192b":"gnb_clf = GaussianNB()\ngnb_clf.fit(X_training, y_training)\npred_gnb = gnb_clf.predict(X_valid)\nacc_gnb = accuracy_score(y_valid, pred_gnb)\n\nprint(acc_gnb)","739f2d22":"dt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_training, y_training)\npred_dt = dt_clf.predict(X_valid)\nacc_dt = accuracy_score(y_valid, pred_dt)\n\nprint(acc_dt)","ee9ac185":"from xgboost import XGBClassifier\n\nxg_clf = XGBClassifier(objective=\"binary:logistic\", n_estimators=10, seed=123)\nxg_clf.fit(X_training, y_training)\npred_xg = xg_clf.predict(X_valid)\nacc_xg = accuracy_score(y_valid, pred_xg)\n\nprint(acc_xg)","e56a7cd7":"model_performance = pd.DataFrame({\n    \"Model\": [\"SVC\", \"Linear SVC\", \"Random Forest\", \n              \"Logistic Regression\", \"K Nearest Neighbors\", \"Gaussian Naive Bayes\",  \n              \"Decision Tree\", \"XGBClassifier\"],\n    \"Accuracy\": [acc_svc, acc_linsvc, acc_rf, \n              acc_logreg, acc_knn, acc_gnb, acc_dt, acc_xg]\n})\n\nmodel_performance.sort_values(by=\"Accuracy\", ascending=False)","0be625ac":"rf_clf = RandomForestClassifier()\n\nparameters = {\"n_estimators\": [4, 5, 6, 7, 8, 9, 10, 15], \n              \"criterion\": [\"gini\", \"entropy\"],\n              \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n              \"max_depth\": [2, 3, 5, 10], \n              \"min_samples_split\": [2, 3, 5, 10],\n              \"min_samples_leaf\": [1, 5, 8, 10]\n             }\n\ngrid_cv = GridSearchCV(rf_clf, parameters, scoring = make_scorer(accuracy_score))\ngrid_cv = grid_cv.fit(X_train, y_train)\n\nprint(\"Our optimized Random Forest model is:\")\ngrid_cv.best_estimator_","4211a609":"rf_clf = grid_cv.best_estimator_\n\nrf_clf.fit(X_train, y_train)","4609d82a":"submission_predictions =rf_clf.predict(X_test)\n","74cd0f27":"submission = pd.DataFrame({\n        \"PassengerId\": testing[\"PassengerId\"],\n        \"Survived\": submission_predictions\n    })\n\nsubmission.to_csv(\"titanic.csv\", index=False)\nprint(submission.shape)","bc1f626b":"It appears that the Random Forest model works the best with our data so we will use it on the test set.","46e4a36a":"Let's create a dataframe to submit to the competition with our predictions of our model.","a5f0d5c0":"We can improve the accuracy of our model by turning the hyperparameters of our Random Forest model. We will run a GridSearchCV to find the best parameters for the model and use that model to train and test our data.","4a30b123":"<a id=\"p9\"><\/a>\n# 9. Submission","ff0eac74":"Great, now that we have the optimal parameters for our Random Forest model, we can build a new model with those parameters to fit and use on the test set.","82994d78":"**sklearn Models to Test**","01c630a7":"> Note that the numbers printed above are the proportion of male\/female survivors of all the surviviors ONLY. The graph shows the propotion of male\/females out of ALL the passengers including those that didn't survive.","4001c05a":"* Looks like the distribution of ages is slightly skewed right. Because of this, we can fill in the null values with the median for the most accuracy.","bc2bf94e":"<a id=\"p4\"><\/a>\n# 4. Plotting and Visualizing Data\nIt is very important to understand and visualize any data we are going to use in a machine learning model. By visualizing, we can see the trends and general associations of variables like Sex and Age with survival rate. We can make several different graphs for each feature we want to work with to see the entropy and information gain of the feature. ","8911b187":"**LogisiticRegression Model**","f20dbe40":"**GaussianNB Model**","1ee31a48":"It appears as though passengers in the younger range of ages were more likely to survive than those in the older range of ages, as seen by the clustering in the strip plot, as well as the survival distributions of the histogram.","69702c4e":"We take a look at the distribution of the Age column to see if it's skewed or symmetrical. This will help us determine what value to replace the NaN values.","0b2c86ce":"**XGBoost Model**","d1dac3e7":"To evaluate our model performance, we can use the make_scorere and accuracy_score function from sklearn metrics.","9ab25e44":"**KNeighbors Model**","ca08670c":"<a id=\"p5\"><\/a>\n# 5. Feature Engineering\nBecause values in the Sex and Embarked columns are categorical values, we have to represent these strings as numerical values in order to perform our classification with our model. We can also do this process through **One-Hot-Encoding**.","e0771a29":"**SVC Model**","1b9ea952":"## Update\nThank you all so much for the support and reading this kernel! I am very inspired to keep learning and I hope you are too. I am in the progress of making more kernels for more competitions as well as ones for data visualization and statistics. Please stay tuned for those, as I will be publishing them very soon! Again, thank you so much and please feel free to contact me or ask any questions! ","9504b38b":"# Contents\n1. [Importing Libraries and Packages](#p1)\n2. [Loading and Viewing Data Set](#p2)\n3. [Dealing with NaN Values (Imputation)](#p3)\n4. [Plotting and Visualizing Data](#p4)\n5. [Feature Engineering](#p5)\n6. [Modeling and Predicting with sklearn](#p6)\n7. [Evaluating Model Performances](#p7)\n8. [Tuning Parameters with GridSearchCV](#p8)\n9. [Submission](#p9)","98dfe2aa":"**RandomForest Model**","72be564f":"We can also use a GridSearch cross validation to find the optimal parameters for the model we choose to work with and use to predict on our testing set.","c0c65d27":"We change Sex to binary, as either 1 for female or 0 for male. We do the same for Embarked. We do this same process on both the training and testing set to prepare our data for Machine Learning.","74dbef4a":"<a id=\"p6\"><\/a>\n# 6. Model Fitting and Predicting\nNow that our data has been processed and formmated properly, and that we understand the general data we're working with as well as the trends and associations, we can start to build our model. We can import different classifiers from sklearn. We will try different types of models to see which one gives the best accuracy for its predictions.","539ed64f":"<a id=\"p2\"><\/a>\n# 2. Loading and Viewing Data Set\nWith Pandas, we can load both the training and testing set that we wil later use to train and test our model. Before we begin, we should take a look at our data table to see the values that we'll be working with. We can use the head and describe function to look at some sample data and statistics. We can also look at its keys and column names.","34de2b70":"**Defining Features in Training\/Test Set**","d7fe586f":"**LinearSVC Model**","7e0100cc":"Here is one final cumulative graph of a pair plot that shows the relations between all of the different features","d93b4359":"If you made it this far, congratulations!! You have gotten a glimpse at an introduction to data visualization, analysis and Machine Learning. You are well on your way to become a Data Science expert! Keep learning and trying out new things, as one of the most important things for Data Scientists is to be creative and perform analysis hands-on. Please upvote and share if this kernel helped you!","a6ffd2ab":"**Validation Data Set**\n\nAlthough we already have a test set, it is generally easy to overfit the data with these classifiers. It is therefore useful to have a third data set called the validation data set to ensure that our model doesn't overfit with the data. We can make this third data set with sklearn's train_test_split function. We can also use the validation data set to test the general accuracy of our model.","75421ce4":"**Age**","3a5b8cc2":"<a id=\"p8\"><\/a>\n# 8. Tuning Parameters with GridSearchCV","12aa1f0e":"**Gender**","03ba16cf":"# Machine Learning to Predict Titanic Survivors \nHi, I'm a current undergraduate student interested in the Data Science and Machine Learning field. In this Kernel, I will try to step by step build a ML model using sklearn to predict the outcomes of each passenger aboard the titanic. This guide is meant for people starting with data visualization, analysis and Machine Learning. If that sounds like you, then you're in the right place! It is not as difficult as you think to understand.\n\n*Please upvote and share if this helps you!! Also, feel free to fork this kernel to play around with the code and test it for yourself. If you plan to use any part of this code, please reference this kernel!* I will be glad to answer any questions you may have in the comments. Thank You! \n","67b3b5e4":"<a id=\"p3\"><\/a>\n# 3. Dealing with NaN Values (Imputation)\nThere are NaN values in our data set in the age column. Furthermore, the Cabin column has too many missing values and isn't useful to be used in predicting survival. We can just drop the column as well as the NaN values which will get in the way of training our model. We also need to fill in the NaN values with replacement values in order for the model to have a complete prediction for every row in the data set. This process is known as **imputation** and we will show how to replace the missing data.","c5df727a":"This IsAlone feature also may work well with the data we're dealing with, telling us whether the passenger was along or not on the ship.","58c080e4":"<a id=\"p7\"><\/a>\n# 7. Evaluating Model Performances\nAfter making so many models and predictions, we should evaluate and see which model performed the best and which model to use on our testing set.","07287212":"<a id=\"p1\"><\/a>\n# 1. Importing Libraries and Packages\nWe will use these packages to help us manipulate the data and visualize the features\/labels as well as measure how well our model performed. Numpy and Pandas are helpful for manipulating the dataframe and its columns and cells. We will use matplotlib along with Seaborn to visualize our data.","c5d19fd4":"Gender appears to be a very good feature to use to predict survival, as shown by the large difference in propotion survived. Let's take a look at how class plays a role in survival as well.","4dc9b193":"It appears that class also plays a role in survival, as shown by the bar graph. People in Pclass 1 were more likely to survive than people in the other 2 Pclasses.","d8c0a661":"**DecisionTree Model**","35ab4c00":"**Class**","f7969bd0":"Although it may not seem like it, we can also extract some useful information from the name column. Not the actual names themselves, but the title of their names like Ms. or Mr. This may also provide a hint as to whether the passenger survived or not. Therefore we can extract this title and then encode it like we did for Sex and Embarked.","23fbb085":"We can combine SibSp and Parch into one synthetic feature called family size, which indicates the total number of family members on board for each member. "}}