{"cell_type":{"8c8f2072":"code","ccd9c32f":"code","a5d99b8f":"code","3bb97442":"code","ac614b25":"code","4c5d40f5":"code","a42666a0":"code","aa350c10":"code","385a1f2e":"code","2eb60095":"code","e6daf339":"code","85b2a2c9":"code","6ad28dcb":"code","df12ffd2":"code","03fbc7ea":"code","07b3cce4":"code","298199cc":"code","89e362a3":"code","dbd1c31f":"code","f6ea2419":"code","b24b27c0":"code","3cde793c":"code","6ce64443":"code","2be66943":"code","44cc1d6d":"code","10383fc4":"code","ac259a0f":"code","63679407":"code","fbe8ebc6":"code","10c7943a":"code","1a694f76":"code","453dffc8":"code","3e19acac":"code","5dd94628":"code","8d094a4c":"code","e71fa5ad":"code","645287ed":"code","160fd13b":"code","23f3a822":"code","6523a7d0":"code","d39fe4e1":"code","137f1a16":"code","e649ff07":"code","8d377da6":"code","f9647e2d":"code","2f108677":"code","1eb6021d":"code","35877fa7":"code","07ca7e0a":"code","e03a0cc7":"code","84d66847":"code","f8552c40":"code","3ce6477f":"code","e74e9083":"code","bdc2c457":"code","3f732f97":"code","745a5a14":"code","0b6064e6":"code","8272e9ff":"code","15a3c609":"code","304f5be0":"code","b47668b4":"code","5635691a":"code","621fc7ca":"code","53078fdd":"code","91181cde":"markdown","de229bec":"markdown","4fbdabe9":"markdown","19718187":"markdown","4bfe5746":"markdown","4f6f0287":"markdown","a1cf8822":"markdown","f7941f34":"markdown","986a63e4":"markdown","dd62af55":"markdown","10c59a81":"markdown","d1c8dc13":"markdown","1d8a0b71":"markdown","9c9b5b21":"markdown","b2a4615f":"markdown"},"source":{"8c8f2072":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ccd9c32f":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nfrom matplotlib.cm import rainbow\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns","a5d99b8f":"df = pd.read_csv(dirname+'\/'+filename)","3bb97442":"# Check basic statistics from the dataset\ndf.describe()","ac614b25":"#  Check if there are missing values. No missing values.\ndf.isnull().sum()","4c5d40f5":"# Histograms from the numeric attributes. \nplt.figure(figsize = (20,20))\nplt.subplot(331);df['HeartDisease'].hist(bins = 100)\nplt.subplot(332);df['Age'].hist(bins = 100)\nplt.subplot(333);df['Cholesterol'].hist(bins = 100)\nplt.subplot(334);df['FastingBS'].hist(bins = 100)\nplt.subplot(335);df['MaxHR'].hist(bins = 100)\nplt.subplot(336);df['Oldpeak'].hist(bins = 100)\nplt.subplot(337);df['RestingBP'].hist(bins = 100);","a42666a0":"# Heatmap of the correlation matrix between the numerica attributes and the target, HeartDisease.\n# We can observe that the stronger correlations with HeartDisease are MaxHR(negatively) and Oldpeak(positively).\n\nimport seaborn as sns\nnum_cols = df.select_dtypes('number').columns\ncorr_mat = df[num_cols].corr()\ntop_corr_features = corr_mat.index\nplt.figure(figsize= (20,10))\ng = sns.heatmap(df[top_corr_features].corr(), annot = True, cmap = 'RdYlGn')","aa350c10":"# Create dummy variables from the categorical attributes.\ndf_dummies = pd.get_dummies(df, columns= ['Sex', 'ChestPainType', 'RestingECG','ExerciseAngina','ST_Slope'])\ndf_dummies.describe()","385a1f2e":"# Use the StandardScaler function to put all variables on the same scale. \n# As the categorical variables were dummy and are values 0 or 1, it is not necessary to change their scale. \n# The model will also not accept categorical variables as floats.\n\nfrom sklearn.preprocessing import StandardScaler\ndf_scaled = StandardScaler().fit_transform(df_dummies[['Age', 'RestingBP','Cholesterol', 'MaxHR', 'Oldpeak']])","2eb60095":"df_ = pd.DataFrame(df_scaled) # Converting to DataFrame\ndf_dummies[['Age', 'RestingBP','Cholesterol', 'MaxHR', 'Oldpeak']] = df_\ndf_dummies.describe()","e6daf339":"# Function to count the outliers using the boxplot method. It will also return the limits of the box plot,\n# the first quantile(Q1) ,dw, and the third quantile(Q3), up. \n\ndef outliers_count(data, col):    \n    d = data.loc[:,col]\n    d -= data.loc[:,col].mean()\n    \n    d \/= np.std(data.loc[:,col])\n    \n    qs = d.quantile([0.25,0.5,0.75])\n    up = qs.iloc[1] + 1.5*(qs.iloc[2] - qs.iloc[0])\n    dw = qs.iloc[1] - 1.5*(qs.iloc[2] - qs.iloc[0])\n    \n    print(\"Upper: \",up)\n    print(\"Down: \",dw)\n    \n    out = []\n    not_out = []\n    \n    for i in range(0,len(d)):\n        if d.iloc[i]>up:\n            out.append(d.iloc[i])\n        elif d.iloc[i]<dw:\n            out.append(d.iloc[i])\n        else:\n            not_out.append(d.iloc[i])\n    \n    return up, dw, out","85b2a2c9":"# Outlier detection for the Age variable.\nup_age,dw_age,out = outliers_count(df_dummies,'Age')\nprint(\"There are : {} outliers \".format(len(out)))\nout = []\n# We have 35 outliers in 919 observations, which represents 3.8% of the observations. ","6ad28dcb":"# Histogram that displays the outliers and the variable's distribution(center).\nplt.figure(figsize = (20,5))\nplt.subplot(131);df_dummies['Age'].loc[df_dummies['Age']<dw_age].hist(bins=100) # Observations < Q1\nplt.subplot(132);df_dummies['Age'].hist(bins=100)\nplt.subplot(133);df_dummies['Age'].loc[df_dummies['Age']>up_age].hist(bins=100); # Observations > Q3","df12ffd2":"# Outlier detection for the Resting Blood Pressure variable.\nup_BP,dw_BP,out = outliers_count(df_dummies,'RestingBP')\nprint(\"There are : {} outliers \".format(len(out)))\nout = []\n# There are 106 outliers in 919 observations, which represent 11.5% of the observations. ","03fbc7ea":"# Histogram that displays the outliers and the variable's distribution(center).\nplt.figure(figsize = (20,5))\nplt.subplot(131);df_dummies['RestingBP'].loc[df_dummies['RestingBP']<dw_BP].hist(bins=100) # Observations < Q1\nplt.subplot(132);df_dummies['RestingBP'].hist(bins=100)\nplt.subplot(133);df_dummies['RestingBP'].loc[df_dummies['RestingBP']>up_BP].hist(bins=100); # Observations > Q3","07b3cce4":"# Outlier detection for the Cholesterol variable.\nup_chol,dw_chol, out = outliers_count(df_dummies,'Cholesterol')\nprint(\"There are : {} outliers \".format(len(out)))\nout = []\n# There are 194 outliers in 919 observations, which represent 21.1% of the observations. ","298199cc":"# Histogram that displays the outliers and the variable's distribution(center).\nplt.figure(figsize = (20,5))\nplt.subplot(131);df_dummies['Cholesterol'].loc[df_dummies['Cholesterol']<dw_chol].hist(bins=100) # Observations < Q1\nplt.subplot(132);df_dummies['Cholesterol'].hist(bins=100)\nplt.subplot(133);df_dummies['Cholesterol'].loc[df_dummies['Cholesterol']>up_chol].hist(bins=100); # Observations > Q3\n","89e362a3":"# Outlier detection for the Max Heart Rate variable.\nup_HR,dw_HR,out =outliers_count(df_dummies,'MaxHR')\nprint(\"There are : {} outliers \".format(len(out)))\nout = []\n# There are 23 outliers in 919 observations, which represent 2.5% of the observations. ","dbd1c31f":"# Histogram that displays the outliers and the variable's distribution(center).\nplt.figure(figsize = (20,5))\nplt.subplot(131);df_dummies['MaxHR'].loc[df_dummies['MaxHR']<dw_HR].hist(bins=100) # Observations < Q1\nplt.subplot(132);df_dummies['MaxHR'].hist(bins=100)\nplt.subplot(133);df_dummies['MaxHR'].loc[df_dummies['MaxHR']>up_HR].hist(bins=100); # Observations > Q3","f6ea2419":"# Outlier detection for the Oldpeak variable.\nup_old,dw_old,out = outliers_count(df_dummies,'Oldpeak')\nprint(\"There are : {} outliers \".format(len(out)))\nout = []\n# There are 59 outliers in 919 observations, which represents 6.4% of the observations. ","b24b27c0":"# Histogram that displays the outliers and the variable's distribution(center).\nplt.figure(figsize = (20,5))\nplt.subplot(131);df_dummies['Oldpeak'].loc[df_dummies['Oldpeak']<dw_old].hist(bins=100) # Observations < Q1\nplt.subplot(132);df_dummies['Oldpeak'].hist(bins=100)\nplt.subplot(133);df_dummies['Oldpeak'].loc[df_dummies['Oldpeak']>up_old].hist(bins=100); # Observations > Q3","3cde793c":"# Mean of the variables(scaled) that contain more outliers, which we are going to treat. \ndf_dummies[['Cholesterol', 'RestingBP']].mean()","6ce64443":"# Median of the variables(scaled) that contain more outliers, which we are going to treat. \ndf_dummies[['Cholesterol', 'RestingBP']].median()","2be66943":"med_chol_dummy = df_dummies['Cholesterol'].median()\nchol_dummy = df_dummies['Cholesterol'].copy()\n\nfor i in range(1, len(chol_dummy)):\n    if chol_dummy[i] <= dw_chol:\n        chol_dummy[i] = med_chol_dummy\n    elif chol_dummy[i] >= up_chol:\n        chol_dummy[i] = med_chol_dummy","44cc1d6d":"# Histogram and boxplots pre and post-imputation by the median\nplt.figure(figsize = (20,10))\nplt.subplot(221);chol_dummy.hist(bins=100) # post-imputation\nplt.subplot(222);df_dummies['Cholesterol'].hist(bins=100) # pre-imputation\nplt.subplot(223);sns.boxplot(data=chol_dummy) # post-imputation\nplt.subplot(224);sns.boxplot(data=df_dummies['Cholesterol']); # pre-imputation","10383fc4":"med_BP_dummy = df_dummies['RestingBP'].median()\nBP_dummy = df_dummies['RestingBP'].copy()\n\nfor i in range(1, len(BP_dummy)):\n    if BP_dummy[i] <= dw_BP:\n        BP_dummy[i] = med_BP_dummy\n    elif BP_dummy[i] >= up_BP:\n        BP_dummy[i] = med_BP_dummy","ac259a0f":"# Histogram and boxplots pre and post-imputation by the median\nplt.figure(figsize = (20,10))\nplt.subplot(221);BP_dummy.hist(bins=100) # post-imputation\nplt.subplot(222);df_dummies['RestingBP'].hist(bins=100) # pre-imputation\nplt.subplot(223);sns.boxplot(data=BP_dummy) # post-imputation\nplt.subplot(224);sns.boxplot(data=df_dummies['RestingBP']); # pre-imputation","63679407":"# Vamos observar como as estisticas descriptivas mudaram ap\u00f3s imputa\u00e7\u00e3o. \n# Observe how the descriptive statistics changed after the outlier imputation.\n\ndf_dummies2 = df_dummies.copy()\ndf_dummies2['Cholesterol'] = chol_dummy\ndf_dummies2['RestingBP'] = BP_dummy\ndf_dummies2[['Cholesterol','RestingBP']].describe()","fbe8ebc6":"df_dummies[['Cholesterol','RestingBP']].describe()","10c7943a":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclass Model:\n    \n    def __init__(self, dataframe, target, testSize):\n        self.y = dataframe[target]\n        self.X = dataframe.drop([target], axis = 1)\n\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size= testSize, random_state=42)\n        \n    def knn_model(self, iterations):\n    \n        knn_scores = []\n        error_rate= []\n        \n        for k in range(1,iterations):\n            knn_classifier = KNeighborsClassifier(n_neighbors = k)\n            knn_classifier.fit(self.X_train,self.y_train)\n            pred_i = knn_classifier.predict(self.X_test)\n            error_rate.append(np.mean(pred_i != self.y_test))\n            score = cross_val_score(knn_classifier,self.X,self.y, cv=10)\n            knn_scores.append(score.mean())\n        \n        return knn_scores, error_rate\n    \n    def randomForest_model(self,iterations):\n        \n        rfc_score = []\n        rfc_error_rate= []\n        \n        for j in range(1,iterations):\n            rfClassifier = RandomForestClassifier(n_estimators = j, random_state = 42)\n            rfClassifier.fit(self.X_train,self.y_train)\n            y_pred = rfClassifier.predict(self.X_test)\n            rfc_error_rate.append(np.mean(y_pred != self.y_test))\n            score = cross_val_score(rfClassifier,self.X ,self.y, cv=10)\n            rfc_score.append(score.mean())\n            \n        return rfc_score, rfc_error_rate","1a694f76":"mod = Model(df_dummies, 'HeartDisease', 0.2)","453dffc8":"knn_scores, error_rate = mod.knn_model(40)","3e19acac":"plt.figure(figsize = (20,6))\nplt.plot([k for k in range(1,40)], knn_scores, color='red')\nplt.title('Score vs K')\nplt.xlabel('K')\nplt.ylabel('Score');","5dd94628":"plt.figure(figsize = (10,6))\nplt.plot(range(1,40),error_rate,color = 'blue',linestyle = '--',marker = 'o',markerfacecolor='red',markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","8d094a4c":"print(\"Best score using k-Nearest Neighbor: {}\".format(max(knn_scores)))\nprint(\"Best error rate using k-Nearest Neighbor: {}\".format(min(error_rate)))","e71fa5ad":"rfc_score, rfc_error_rate = mod.randomForest_model(40)","645287ed":"plt.figure(figsize = (20,10))\nplt.subplot(211);plt.plot([j for j in range(1,40)], rfc_score, color='red')\nplt.title('Score vs K')\nplt.xlabel('K')\nplt.ylabel('Score');\nplt.subplot(212);plt.plot(range(1,40),rfc_error_rate,color = 'blue',linestyle = '--',marker = 'o',markerfacecolor='red',markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","160fd13b":"plt.figure(figsize = (10,6))\nplt.plot(range(1,40),rfc_error_rate,color = 'blue',linestyle = '--',marker = 'o',markerfacecolor='red',markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","23f3a822":"print(\"Best score using Random Forest: {}\".format(max(rfc_score)))\nprint(\"Best error rate using Random Forest: {}\".format(min(rfc_error_rate)))","6523a7d0":"df_score = pd.DataFrame([[max(knn_scores),min(error_rate)],[max(rfc_score),min(rfc_error_rate)]],\n                        index = ['KNN','RandomForest'],\n                        columns = ['Score','Error Rate'])\ndf_score","d39fe4e1":"# Using the dataframe with outliers imputed by meadian\nmod = Model(df_dummies2, 'HeartDisease', 0.2)\nknn_scores, error_rate = mod.knn_model(40)\nrfc_score, rfc_error_rate = mod.randomForest_model(40)","137f1a16":"plt.figure(figsize = (20,10))\nplt.subplot(211);plt.plot([j for j in range(1,40)], rfc_score, color='red')\nplt.title('Score vs K')\nplt.xlabel('K')\nplt.ylabel('Score');\nplt.subplot(212);plt.plot(range(1,40),rfc_error_rate,color = 'blue',linestyle = '--',marker = 'o',markerfacecolor='red',markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","e649ff07":"print(\"Best score using k-Nearest Neighbor: {}\".format(max(knn_scores)))\nprint(\"Best error rate using k-Nearest Neighbor: {}\".format(min(error_rate)))\nprint(\"\\nBest score using Random Forest: {}\".format(max(rfc_score)))\nprint(\"Best error rate using Random Forest: {}\".format(min(rfc_error_rate)))","8d377da6":"df_score2 = pd.DataFrame([[max(knn_scores),min(error_rate)],[max(rfc_score),min(rfc_error_rate)]],\n                        index = ['KNN','RandomForest'],\n                        columns = ['Score','Error Rate'])\ndf_score2","f9647e2d":"df_score2-df_score","2f108677":"# Imputation via Monte Carlo, assuming a normal distribution. \n\nmed_Cholesterol_dummy = df_dummies['Cholesterol'].median()\nCholesterol_dummy = df_dummies['Cholesterol'].copy()\n\nfor i in range(1, len(Cholesterol_dummy)):\n    if Cholesterol_dummy[i] <= dw_chol:\n        dados = Cholesterol_dummy.iloc[0:i]\n        m = sum(Cholesterol_dummy.iloc[0:i])\/len(Cholesterol_dummy.iloc[0:i])\n        sd = np.std(Cholesterol_dummy.iloc[0:i])\n        Cholesterol_dummy.iloc[i] = np.random.normal(m,sd,1)[0]\n    elif Cholesterol_dummy[i] >= up_chol:\n        dados = Cholesterol_dummy.iloc[0:i]\n        m = sum(Cholesterol_dummy.iloc[0:i])\/len(Cholesterol_dummy.iloc[0:i])\n        sd = np.std(Cholesterol_dummy.iloc[0:i])\n        Cholesterol_dummy.iloc[i] = np.random.normal(m,sd,1)[0]","1eb6021d":"print('Imputation mean and median for the Cholesterol variable by mean\\n')\nprint(\"Mean: {}\".format(df_dummies2['Cholesterol'].mean()))\nprint(\"Median: {}\".format(df_dummies2['Cholesterol'].median()))\nprint('\\nImputation mean and median for the Cholesterol variable via Monte Carlo\\n')\nprint(\"Mean: {}\".format(Cholesterol_dummy.mean()))\nprint(\"Median: {}\".format(Cholesterol_dummy.median()))","35877fa7":"# Histograms of the Cholesterol variable, with imputation by Monte Carlo and mean.\nplt.figure(figsize = (20,5))\nplt.subplot(131);Cholesterol_dummy.hist(bins=100) #MC\nplt.subplot(132);df_dummies['Cholesterol'].hist(bins =100) #normal\nplt.subplot(133);df_dummies2['Cholesterol'].hist(bins =100); #mean","07ca7e0a":"med_RestingBP_dummy = df_dummies['RestingBP'].median()\nRestingBP_dummy = df_dummies['RestingBP'].copy()\n\nfor i in range(1, len(RestingBP_dummy)):\n    if RestingBP_dummy[i] <= dw_BP:\n        dados = RestingBP_dummy.iloc[0:i]\n        m = sum(RestingBP_dummy.iloc[0:i])\/len(RestingBP_dummy.iloc[0:i])\n        sd = np.std(RestingBP_dummy.iloc[0:i])\n        RestingBP_dummy.iloc[i] = np.random.normal(m,sd,1)[0]\n    elif RestingBP_dummy[i] >= up_BP:\n        dados = RestingBP_dummy.iloc[0:i]\n        m = sum(RestingBP_dummy.iloc[0:i])\/len(RestingBP_dummy.iloc[0:i])\n        sd = np.std(RestingBP_dummy.iloc[0:i])\n        RestingBP_dummy.iloc[i] = np.random.normal(m,sd,1)[0]","e03a0cc7":"print('Imputation mean and median for the RestingBP variable by mean\\n')\n\nprint(\"Mean: {}\".format(RestingBP_dummy.mean()))\nprint(\"Median: {}\".format(RestingBP_dummy.median()))\n\nprint('\\nImputation mean and median for the RestingBP variable via Monte Carlo\\n')\n\nprint(\"Mean: {}\".format(df_dummies2['RestingBP'].mean()))\nprint(\"Median: {}\".format(df_dummies2['RestingBP'].median()))","84d66847":"# Histograms of the Cholesterol variable, with imputation by Monte Carlo and mean.\nplt.figure(figsize = (20,5))\nplt.subplot(131);RestingBP_dummy.hist(bins=100) #MC\nplt.subplot(132);df_dummies['RestingBP'].hist(bins =100) #Normal\nplt.subplot(133);df_dummies2['RestingBP'].hist(bins =100); #mean","f8552c40":"df_dummies_monte = df_dummies.copy()\ndf_dummies_monte['Cholesterol'] = Cholesterol_dummy\ndf_dummies_monte['RestingBP'] = RestingBP_dummy","3ce6477f":"mod = Model(df_dummies_monte, 'HeartDisease', 0.2)","e74e9083":"knn_score_monte, error_rate_monte = mod.knn_model(40)","bdc2c457":"plt.figure(figsize = (20,6))\nplt.plot([k for k in range(1,40)], knn_score_monte, color='red')\nplt.title('Score vs K')\nplt.xlabel('K')\nplt.ylabel('Score');","3f732f97":"plt.figure(figsize = (10,6))\nplt.plot(range(1,40),error_rate_monte,color = 'blue',linestyle = '--',marker = 'o',markerfacecolor='red',markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","745a5a14":"print(\"Best score using k-Nearest Neighbor: {}\".format(max(knn_score_monte)))\nprint(\"Best error rate using k-Nearest Neighbor: {}\".format(min(error_rate_monte)))","0b6064e6":"rfc_score_monte, rfc_error_rate_monte = mod.randomForest_model(40)","8272e9ff":"plt.figure(figsize = (20,6))\nplt.plot([k for k in range(1,40)], rfc_score_monte, color='red')\nplt.title('Score vs K')\nplt.xlabel('K')\nplt.ylabel('Score');","15a3c609":"plt.figure(figsize = (10,6))\nplt.plot(range(1,40),rfc_error_rate_monte,color = 'blue',linestyle = '--',marker = 'o',markerfacecolor='red',markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","304f5be0":"print(\"Best score using Random Forest: {}\".format(max(rfc_score_monte)))\nprint(\"Best error rate using Random Forest: {}\".format(min(rfc_error_rate_monte)))","b47668b4":"df_score_monte = pd.DataFrame([[max(knn_score_monte),min(error_rate_monte)],[max(rfc_score_monte),min(rfc_error_rate_monte)]],\n                        index = ['KNN','RandomForest'],\n                        columns = ['Score','Error Rate'])","5635691a":"df_score2 -  df_score","621fc7ca":"df_score_monte-df_score","53078fdd":"df_score_monte - df_score2","91181cde":"Model run with imputation by median","de229bec":"For our model, we are going to work with two different models: k-nearest neighbor and random forest.\n\nThe classification of the k-nearest neigbor, in a simplistic way, works, with a majority 'vote' of the neighborhood of each observation. That is, each observation is assigned a class that is the class that represents the most k-neighbors of the point. This k can be specified, or obtained through experimentation as it was in our case.\n\nThe random forest classifier is a model that fits a number of decision tree sorting into several sub-samples, countered by the number of estimators, of the data set and uses the mean to improve prediction accuracy and to control over-fitting. . The number of estimators in our case is a sequence of numbers to see which gives the best result.","4fbdabe9":"Model run with no imputations","19718187":"Trying to the model with feature engineering. I imputed the values that were outliers in the Cholesterol and RestingBP columns via Monte Carlo.","4bfe5746":"### Description of the database used\n\nOur database is divided into 12 attributes, each containing 919 observations. The attributes are as follows:\n\n- Age: Patient's age, [Years].\n\n- Sex: Gender of the patient. [M: Male, F: Female]\n\n- ChestPainType: Type of chest pain. [TA: Typical angina, ATA: Atypical angina, NAP: Non-anginal pain, ASY: asymptomatic]\n\n- RestingBP: Resting blood pressure. [mm Hg]\n\n- Cholesterol: Serum cholesterol [mm\/dl]\n\n- FastingBS: Fasting blood sugar. [1: if blood sugar > 120 mg\/dl, 0: if lower]\n\n- RestingECG: Resting electrocardiogram results. [Normal: Normal, ST: ST-T wave abnormality (T wave inversions and\/or ST elevations or depressions with values \u200b\u200b> 0.05 mV), LVH: Showing probable or defined left ventricular hypertrophy by the Estes criterion]\n\n- MaxHR: Maximum heart rate reached. [Numeric value between 60 and 202]\n\n- ExerciseAngina: Exercise-induced angina.[Y: Yes, N: No]\n\n- Oldpeak: oldpeak = ST [Numerical value measured in depression]\n\n- ST_Slope: The slope of the peak ST segment of the exercise [Up: positive slope, Flat: straight, Down: negative slope]\n\n- HeartDisease: Resulting Class [1: Cardiovascular Disease, 0: Normal]","4f6f0287":"Let's change the cholesterol values because 21.1% of the observations were categorized as outliers. Let's replace the values with the median because the median is a statistic that is less affected by the presence of outliers that tend to distort the mean.","a1cf8822":"The result, above, when running the original DataFrame, with no imputation.","f7941f34":"Let's analyze the data from numerical variables, as some contain observations that can affect the performance of our model. An outliers function, using the boxplot method, was used to find the limits and thus be able to detect which observations are out of the normal range. Outliers are imputed by the median, as they themselves affect the mean and therefore it is not recommended to use the mean.","986a63e4":"## Model","dd62af55":"### Description of the problem to be solved\/analyzed\n\nCardiovascular diseases are a group of disorders of the heart and blood vessels, which are the leading causes of death worldwide. According to the WHO, 17.9 million people will die from cardiovascular disease in 2019, representing 32% of deaths globally. Of those 17.9 million deaths, 85% were from heart attacks and strokes. The majority, more than 75%, of deaths from cardiovascular diseases happen in low- and middle-income countries. Most cardiovascular diseases can be prevented by controlling behavioral risks such as tobacco use, eating habits that lead to obesity, physical inactivity, and alcohol abuse. It is also important to have routine tests so that you can detect and treat cardiovascular problems as soon as possible.\n\nIn this work, I intend to create a machine learning model, with techniques learned in class, that can predict whether a patient has a cardiovascular disease based on patient information. As it is a predictive model, we analyze the result in relation to the error. In other words, the error will be our metric to evaluate the model.\n\nPatient information is made available by the UCI Machine Learning Repository, and contains information about patients from different hospitals in different locations around the world.","10c59a81":"As it can be seen by the difference between the models, in which one had the outliers imputed by the media and the other didn't is very small, not even a 1% difference.","d1c8dc13":"Let's change the resting blood pressure values because 11.5% of the observations were categorized as outliers. Let's replace the values with the median because the median is a statistic that is less affected by the presence of outliers that tend to distort the mean.","1d8a0b71":"There were no noticeable improvements\/deteriorations on the score and error, when comparing the models run on the data that was imputed by monte carlo and the original data. ","9c9b5b21":"## Outliers","b2a4615f":"Again, there were no noticeable improvements\/deteriorations on the score and error, when comparing the models run on the data that was imputed by monte carlo and the data imputed by the mean. However, the model where the data was imputed by Monte Carlo is clearly better when run on the Random Forest classifier against both scenarios, where there is no imputation and imputation by median."}}