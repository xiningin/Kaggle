{"cell_type":{"9f9c1d18":"code","46e6dc01":"code","8afb9391":"code","08425ce3":"code","fa494ed4":"code","af07c864":"code","dd2b1fa4":"code","c8342c85":"code","837e5bcd":"code","c2cb68d1":"code","8f525632":"code","b991e6ba":"code","856cb3d1":"code","23f7efa9":"code","ba563eee":"code","82af8a40":"code","4a5cae49":"code","210055a7":"code","e14dd9fe":"code","2b40a4a5":"code","7da09a4d":"code","6b25a408":"code","25e2135e":"code","20b6b707":"code","f8bdb43f":"code","c8c029c8":"code","08ba2277":"code","275674af":"code","33151b39":"code","fe221ccc":"code","7506840c":"code","c17aecf9":"code","8363e1d9":"code","867c8e43":"code","5036093a":"code","0fc71a97":"code","00460bd4":"code","42a64c1e":"code","bae71215":"code","6821c141":"code","9cf207cd":"code","8709ca5b":"code","ec9d4b9d":"code","8e1e80b9":"code","c9fcb602":"code","1a66ee6e":"code","2930cea4":"code","5e8f0f28":"code","d3ec3fd4":"code","b723fb2e":"code","17b86721":"code","504b9754":"code","8c8cc899":"code","d94bf6a7":"markdown","fe6c61c3":"markdown","f6de2698":"markdown","0ef39b4c":"markdown","129ae8d9":"markdown","66a03c75":"markdown","e6efd324":"markdown","ca4a71d7":"markdown","d6fe23c3":"markdown","a0e6170f":"markdown","a8472081":"markdown","4a1db1c4":"markdown","fd3d90ce":"markdown"},"source":{"9f9c1d18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)","46e6dc01":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","8afb9391":"#reading test and train data\ndata = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv',index_col='Id')\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv',index_col='Id')","08425ce3":"#check the train data\ndata.head()","fa494ed4":"data.info()","af07c864":"#missing values\nmissing = data.isnull().sum()\nmissing = missing[missing>0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()\n\n","dd2b1fa4":"print(missing)\n","c8342c85":"#Let's going to start with numerical columns\nprint('Numerical colums:  \\n', data.select_dtypes(exclude=['object']).columns)\n\n","837e5bcd":"print('Number of Numerical Columns: \\n', len(data.select_dtypes(exclude=['object']).columns))","c2cb68d1":"\nprint('Statistic of Numerical Columns: \\n', data.select_dtypes(exclude=['object']).describe())","8f525632":"target = data.SalePrice\nplt.figure(figsize=(10,10))\n\nplt.subplot(2,2,1)\nplt.title('Distribution of SalePrice')\nsns.distplot(target)\n\nplt.subplot(2,2,2)\nplt.title('Distribution of Log-Transformed SalePrice')\nsns.distplot(np.log(target))","b991e6ba":"numerical_features = data.select_dtypes(exclude=['object']).drop(['SalePrice'], axis=1).copy()\n#numerical_features.head()","856cb3d1":"print(numerical_features .columns)","23f7efa9":"fig = plt.figure(figsize=(12,18))\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.distplot(numerical_features.iloc[:,i].dropna())\n    plt.xlabel(numerical_features.columns[i])\nplt.tight_layout()\nplt.show()","ba563eee":"fig = plt.figure(figsize=(12,18))\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.boxplot(y=numerical_features.iloc[:,i])\n\nplt.tight_layout()\nplt.show()","82af8a40":"fig = plt.figure(figsize=(12,18))\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.scatterplot(numerical_features.iloc[:,i],target)\nplt.tight_layout()\nplt.show()","4a5cae49":"\ncorrelation_num = data.select_dtypes(exclude='object').corr()\nplt.figure(figsize=(30,14))\nplt.title('Correlation between each feature related to size and SalePrice')\nsns.heatmap(data=correlation_num, annot=True)\n","210055a7":"#For better understanding I would like yp plot the corr> 0.80\ncorrelation_num = data.select_dtypes(exclude='object').corr()\nplt.figure(figsize=(30,14))\nplt.title('Correlation between each feature related to size and SalePrice')\nsns.heatmap(data=correlation_num>0.80, annot=True)","e14dd9fe":"print(data.select_dtypes(include='object').columns)","2b40a4a5":"#shows the count, unique value and frequent values and top values in categorical data\nprint(data.select_dtypes(include='object').describe())","7da09a4d":"categorical = data.select_dtypes(include=('object'))\nprint(data.select_dtypes(include=('object')).isnull().sum() )\n","6b25a408":"print(data.select_dtypes(include='object').nunique())","25e2135e":"fig = plt.figure(figsize=(18,8))\nsns.boxplot(x=data.Neighborhood, y=target)\nplt.xticks(rotation=90)\nplt.show()","20b6b707":"fig = plt.figure(figsize=(18,8))\nsns.boxplot(x=data.Street , y=target)\nplt.xticks(rotation=90)\nplt.show()","f8bdb43f":"fig = plt.figure(figsize=(12,6))\nsns.boxplot(x=data.KitchenQual, y=target)\n\nplt.show()","c8c029c8":"fig = plt.figure(figsize=(18,8))\nsns.boxplot(x=data.Exterior1st, y=target)\nplt.xticks(rotation=90)\nplt.show()","08ba2277":"#BEfore handling missing values let's copy our data\n\ndata_copy = data.copy()\ndata_copy[:50]","275674af":"print(data_copy.select_dtypes(include='object').isnull().sum())","33151b39":"data_copy['TotalSF'] = data_copy['TotalBsmtSF'] + data_copy['1stFlrSF'] +data_copy['2ndFlrSF']\ndata_copy['Total_Bathrooms'] = data_copy['FullBath'] + (0.5* data_copy['HalfBath']) + data_copy['BsmtFullBath'] + (0.5* data_copy['BsmtHalfBath'])\ndata_copy['Total_sqrt_footage'] = data_copy['BsmtFinSF1'] +data_copy['BsmtFinSF2'] + data_copy['1stFlrSF']+data_copy['2ndFlrSF']\ndata_copy['Total_porch_SF'] = data_copy['OpenPorchSF'] + data_copy['3SsnPorch'] +data_copy['EnclosedPorch'] +  data_copy['ScreenPorch'] + data_copy['WoodDeckSF']\n\n","fe221ccc":"#do the same process for test data\ntest['TotalSF'] = test['TotalBsmtSF'] + test['1stFlrSF'] +test['2ndFlrSF']\ntest['Total_Bathrooms'] = test['FullBath'] + (0.5* test['HalfBath']) + test['BsmtFullBath'] + (0.5* test['BsmtHalfBath'])\ntest['Total_sqrt_footage'] = test['BsmtFinSF1'] +test['BsmtFinSF2'] + test['1stFlrSF']+test['2ndFlrSF']\ntest['Total_porch_SF'] = test['OpenPorchSF'] + test['3SsnPorch'] +test['EnclosedPorch'] +  test['ScreenPorch'] + test['WoodDeckSF']","7506840c":"#new features \ndata_copy['haspool'] = data_copy['PoolArea'].apply(lambda x:1 if x>0 else 0)\ndata_copy['has2ndFloor'] = data_copy['2ndFlrSF'].apply(lambda x:1 if x>0 else 0)\ndata_copy['hasgarage'] = data_copy['GarageArea'].apply(lambda x:1 if x>0 else 0)\ndata_copy['hasbsmt'] = data_copy['TotalBsmtSF'].apply(lambda x:1 if x>0 else 0)\ndata_copy['hasfireplace'] = data_copy['Fireplaces'].apply(lambda x:1 if x>0 else 0)","c17aecf9":"#do same process for test data\ntest['haspool'] = test['PoolArea'].apply(lambda x:1 if x>0 else 0)\ntest['has2ndFloor'] = test['2ndFlrSF'].apply(lambda x:1 if x>0 else 0)\ntest['hasgarage'] = test['GarageArea'].apply(lambda x:1 if x>0 else 0)\ntest['hasbsmt'] = test['TotalBsmtSF'].apply(lambda x:1 if x>0 else 0)\ntest['hasfireplace'] = test['Fireplaces'].apply(lambda x:1 if x>0 else 0)","8363e1d9":"#add_count_colum_with more the 10 fetures\nimport category_encoders as ce\ncount_cat_colums = ['Neighborhood', 'Exterior1st','Exterior2nd']\n\ncount_enc = ce.TargetEncoder(cols=count_cat_colums)\ncount_enc.fit(data_copy[count_cat_colums], data_copy.SalePrice)\n\ndata_copy = data_copy.join(count_enc.transform(data_copy[count_cat_colums]).add_suffix('_count'))\ntest = test.join(count_enc.transform(test[count_cat_colums]).add_suffix('_count'))\n","867c8e43":"#add a buildg year as a new feture\nimport datetime\nnow = datetime.datetime.now()\nbuilding_age = now.year - data_copy['YearBuilt']\ndata_copy['building_age'] = now.year - data_copy['YearBuilt']\ntest['building_age'] = now.year - test['YearBuilt']\n","5036093a":"# Remove outliers based on observations on scatter plots against SalePrice:\ndata_copy = data_copy.drop(data_copy['LotFrontage']\n                                     [data_copy['LotFrontage']>200].index)\ndata_copy = data_copy.drop(data_copy['LotArea']\n                                     [data_copy['LotArea']>100000].index)\ndata_copy = data_copy.drop(data_copy['BsmtFinSF1']\n                                     [data_copy['BsmtFinSF1']>4000].index)\ndata_copy = data_copy.drop(data_copy['TotalBsmtSF']\n                                     [data_copy['TotalBsmtSF']>6000].index)\ndata_copy = data_copy.drop(data_copy['1stFlrSF']\n                                     [data_copy['1stFlrSF']>4000].index)\ndata_copy = data_copy.drop(data_copy.GrLivArea\n                                     [(data_copy['GrLivArea']>4000) & \n                                      (data_copy.SalePrice<300000)].index)\ndata_copy = data_copy.drop(data_copy.LowQualFinSF\n                                     [data_copy['LowQualFinSF']>550].index)\n","0fc71a97":"data_copy.head()","00460bd4":"#remove_columns = ['GarageArea','Neighborhood', 'Exterior1st','Exterior2nd','GarageYrBlt']\nremove_columns = ['MiscVal', 'MSSubClass', 'MoSold', 'YrSold','GarageArea', 'GarageYrBlt', 'TotRmsAbvGrd']","42a64c1e":"#before remove the columns copy the data frame\ndata_copy_second = data_copy.copy()\n","bae71215":"data_copy_second.head()\n","6821c141":"data_copy_second = data_copy_second.drop(remove_columns, axis=1)","9cf207cd":"data_copy_second.head()","8709ca5b":"y = data_copy_second.SalePrice\nX = data_copy_second.drop('SalePrice', axis=1)","ec9d4b9d":"#X.info()","8e1e80b9":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import GradientBoostingRegressor","c9fcb602":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1,test_size=0.2)","1a66ee6e":"#to find out columns has the more then 10 unique values\ncategorical_cols = [cname for cname in X.columns if \n                    X[cname].dtype == \"object\"]\n\nnumrical_cols = [cname for cname in X.columns if\n                 X[cname].dtype in ['int64','float64']]\nmy_cols = numrical_cols + categorical_cols\n\nX_train = train_X[my_cols].copy()\nX_valid = val_X[my_cols].copy()\nX_test = test[my_cols].copy()","2930cea4":"\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\n\n \nnumrical_transform = Pipeline(steps=[\n    ('num_imputer', SimpleImputer(strategy='constant')),\n    ('num_scaler', RobustScaler())\n    ])\n\ncategorical_transform = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numrical_transform, numrical_cols),       \n        ('cat',categorical_transform,categorical_cols),\n        ])\n","5e8f0f28":"#model = RandomForestRegressor(n_estimators=100, random_state=0)\n\nmodel =XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0,\n             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n             max_depth=3, min_child_weight=1, missing=None, n_estimators=900,\n             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=1)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model',model),\n                      ])\n\nclf.fit(X_train, train_y)\npreds = clf.predict(X_valid)\nscore = mean_absolute_error(val_y, preds)\nprint('MAE: ', score)","d3ec3fd4":"scores=[]\n#classifiers = ['Linear Svm', 'Radial Svm', 'Logistic Regression', 'KNN', 'Decision Tree', 'Random Forest']\n#models = [svm.SVC(kernel='linear'), svm.SVC(kernel='rbf'), LogisticRegression(solver='liblinear'), KNeighborsClassifier(n_neighbors=9), \n #         DecisionTreeClassifier(),  RandomForestClassifier(n_estimators=100)]\n\nclassifiers = ['XGBRegressor','Linear_Regression','Lasso','Ridge','Elastic','Gradient']\nmodels =[XGBRegressor(n_estimators=1000, learning_rate=0.05), LinearRegression(),Lasso(alpha=0.0005, random_state=5), \n         Ridge(alpha=0.002, random_state=5),ElasticNet(alpha=0.02, random_state=5, l1_ratio=0.7),\n         GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)]\n\nfor i in models:\n    model  = i\n    clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model',model),\n                      ])\n    clf.fit(X_train, train_y)\n    preds = clf.predict(X_valid)\n    score = mean_absolute_error(val_y, preds)\n    scores.append( score)\n    print(classifiers, scores)\nnew_models_data_frame = pd.DataFrame({'Score': scores}, index=classifiers)\nnew_models_data_frame\n    ","b723fb2e":"from sklearn.model_selection import GridSearchCV\nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nestimator = range(100, 1000, 100)\nlearning_r = [0.05, 0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nhyper = {'n_estimators':estimator, 'learning_rate':learning_r}\nmodel = GridSearchCV(estimator=XGBRegressor(), param_grid=hyper, verbose=True)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model',model),\n                      ])\nclf.fit(X_train, train_y)\nprint(model.best_score_)\nprint(model.best_estimator_)","17b86721":"#Trying Lasso algoritm\nnumrical_transform = Pipeline(steps=[\n    ('num_imputer', SimpleImputer(strategy='constant')),\n    ('num_scaler', RobustScaler())\n    ])\n\ncategorical_transform = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numrical_transform, numrical_cols),       \n        ('cat',categorical_transform,categorical_cols),\n        ])\n\nlasso_model = Lasso(alpha=0.0001, random_state=5)\n\n\nlasso_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model',lasso_model),\n                      \n                     ])\n\n\nlasso_clf.fit(X_train, train_y)\nlasso_val_predictions = lasso_clf.predict(X_valid)\nlasso_val_mae = mean_absolute_error(val_y, lasso_val_predictions)\nprint(lasso_val_mae)","504b9754":"preds_test = clf.predict(X_test)","8c8cc899":"output = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","d94bf6a7":"### Data cleaning & Preprocessing","fe6c61c3":"**I am going to explore the numerical column and categorical columns separetly...**","f6de2698":"## Exploring Categorical Columns\n","0ef39b4c":"### Target Column ","129ae8d9":"**Data Cleaning Process **\n* Remove the hight correlated data witch cause data lekeage.\n    * Excluding GarageArea - highly (0.88) correlated with GarageCars, which has a higher corr with Price\n    * Excluding GarageYrBlt - highly (0.83) correlated with YearBuilt\n* A few columns has low correlation with saleprice but I would like combine the a few of them with creating a few column\n    * TotalSF' = 'TotalBsmtSF' + '1stFlrSF' + '2ndFlrSF'\n    * 'Total_sqr_footage' = 'BsmtFinSF1' + features'BsmtFinSF2' +'1stFlrSF''2ndFlrSF'\n    * 'Total_Bathrooms' = 'FullBath' + (0.5 * 'HalfBath') +'BsmtFullBath' + (0.5 * 'BsmtHalfBath')\n    * 'Total_porch_sf' = 'OpenPorchSF' + '3SsnPorch' +'EnclosedPorch'+ 'ScreenPorch' +'WoodDeckSF'\n* Create new features: If the data in the columns more then zero it will go with True(1) otherwise false(0)\n    * new['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    * new['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    * new['hasgarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    * new['hasbsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    * new['hasfireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    \n * Remove the column already used and hight correlated \n * Deal with categorical data and remove athe column more than 10 categorical vaalues .\n     * ['Neighborhood', 'Exterior1st', 'Exterior2nd']\n    \n* We need to deal with  LotFrontage, LotFrontage with the mean value using Simple '']","66a03c75":"### How are numeric features related to SalePrice","e6efd324":"We have 81 columns, 5 of them have around %50 missing value.","ca4a71d7":"## READING DATA","d6fe23c3":"**This following columns highly correlated with each other**\n* 1stFlrSF - TotalBsmtSF\n* GarageYrBlt - YearBuilt\n* TotRmsAbvGrd - GrLivArea\n* YearRemodAdd - GarageYrBlt","a0e6170f":"## Numerical Columns","a8472081":"### Bivariate analysis - scatter plots for target versus numerical attributes","4a1db1c4":"Here's a brief version of what you'll find in the data description file.\n\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale","fd3d90ce":"**Data Cleaning Process **\n* Remove the hight correlated data witch cause data lekeage.\n    * Excluding GarageArea - highly (0.88) correlated with GarageCars, which has a higher corr with Price\n    * Excluding GarageYrBlt - highly (0.83) correlated with YearBuilt\n* A few columns has low correlation with saleprice but I would like combine the a few of them with creating a few column\n    * TotalSF' = 'TotalBsmtSF' + '1stFlrSF' + '2ndFlrSF'\n    * 'Total_sqr_footage' = 'BsmtFinSF1' + features'BsmtFinSF2' +'1stFlrSF''2ndFlrSF'\n    * 'Total_Bathrooms' = 'FullBath' + (0.5 * 'HalfBath') +'BsmtFullBath' + (0.5 * 'BsmtHalfBath')\n    * 'Total_porch_sf' = 'OpenPorchSF' + '3SsnPorch' +'EnclosedPorch'+ 'ScreenPorch' +'WoodDeckSF'\n* Create new features: If the data in the columns more then zero it will go with True(1) otherwise false(0)\n    * new['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    * new['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    * new['hasgarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    * new['hasbsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    * new['hasfireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    \n * Remove the column already used and hight correlated \n * Deal with categorical data and remove athe column more than 10 categorical vaalues .\n    \n* We need to deal with  LotFrontage, LotFrontage with the mean value using Simple "}}