{"cell_type":{"c70232ac":"code","e46d688e":"code","e4ac4608":"code","3fd8197f":"code","802f9edc":"code","bd8aaff4":"code","e812377c":"code","b2124831":"code","691ff7bf":"code","f486c612":"code","a1b5cb97":"code","30605f7e":"code","c58c3eb1":"code","1cdd8ead":"code","b45bee3b":"code","e3836ce4":"code","83fe0ace":"code","cc80aef2":"code","75a89af0":"code","1dd9e648":"code","19915254":"code","d69d9a9a":"code","63ac47c3":"code","886f0a85":"code","a4ee545b":"code","2ab5f9fd":"code","c500811c":"code","30f8502c":"code","ee050d8f":"code","6c2d21f0":"code","5d6898f3":"code","3a632a98":"code","54851a04":"code","25b593f5":"code","0a176d71":"code","35df64f3":"code","876907ff":"code","39448f70":"code","5466271b":"code","78add3df":"code","714ef705":"code","2af19933":"code","fdfe84fc":"code","b8440c74":"code","7231b0f3":"code","cd0dcf8a":"code","aae81420":"code","d91c4fe8":"code","0fe70b8b":"code","fc3163e0":"code","203082c7":"code","9f18581c":"code","d595815c":"code","43521a49":"code","b8775484":"code","e1f912c4":"code","951faaf4":"code","0bfa38c7":"code","c47b2071":"code","1657ef33":"code","6ddb98db":"code","9695c2e3":"code","d98287a3":"code","fe0523e6":"code","16529806":"code","89b9325d":"code","2c7e19a2":"code","1889ea60":"markdown","eb3d19e8":"markdown","17be76bc":"markdown","7559ef55":"markdown","49672073":"markdown","a135eeaa":"markdown","3f71379b":"markdown","5c8cd853":"markdown","51a87c6c":"markdown","c7cb91df":"markdown","6bb542b9":"markdown","bbeb207e":"markdown","e740b0c8":"markdown","c955ead5":"markdown","3a541b18":"markdown","7abc1c38":"markdown","6315f6a9":"markdown","0cd52ca1":"markdown","3422957a":"markdown","f60701fe":"markdown","ffb15f69":"markdown","a707dd6a":"markdown","06034d92":"markdown"},"source":{"c70232ac":"#import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# load the data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\n\n# get the shape of the train and test sets\ntrain_shape = train.shape\ntest_shape = test.shape\n\nprint('Train shape: {}'.format(train_shape))\nprint('Test shape: {}'.format(test_shape))","e46d688e":"sample_submission.head()","e4ac4608":"train.head()","3fd8197f":"test.head()","802f9edc":"structures = pd.read_csv('..\/input\/structures.csv')\ndipole_moments = pd.read_csv('..\/input\/dipole_moments.csv')\nmagnetic_shielding_tensors = pd.read_csv('..\/input\/magnetic_shielding_tensors.csv')\nmulliken_charges = pd.read_csv('..\/input\/mulliken_charges.csv')\npotential_energy = pd.read_csv('..\/input\/potential_energy.csv')\nscalar_coupling_contributions = pd.read_csv('..\/input\/scalar_coupling_contributions.csv')","bd8aaff4":"dipole_moments.head()","e812377c":"magnetic_shielding_tensors.head()","b2124831":"mulliken_charges.head()","691ff7bf":"potential_energy.head()","f486c612":"scalar_coupling_contributions.head()","a1b5cb97":"train.info()","30605f7e":"test.info()","c58c3eb1":"train.describe()","1cdd8ead":"test.describe()","b45bee3b":"train['type'].unique()","e3836ce4":"test.columns","83fe0ace":"# plot the categorical features - type\ntrain.groupby(\"type\").id.count().sort_values(ascending=False)[:10].plot.bar()","cc80aef2":"# plot the distribution of the median of target variable by type\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nmolecule_type = train.groupby('type',as_index=False)['scalar_coupling_constant'].median()\nfig = plt.figure(figsize=(7,5))\nplt.bar(molecule_type.type,molecule_type.scalar_coupling_constant,width=0.5,alpha=0.8)\n\nplt.xlabel('scalar_coupling_constant')\nplt.ylabel('Median Scalar Constant')\nplt.title('Median of scalar constant across type')\nplt.show()","75a89af0":"import seaborn as sns\nsns.distplot(train['scalar_coupling_constant'])","1dd9e648":"#skewness and kurtosis of the target variable\nprint(\"Skewness: \", train['scalar_coupling_constant'].skew())\nprint(\"Kurtosis: \", train['scalar_coupling_constant'].kurt())","19915254":"train.scalar_coupling_constant.unique()","d69d9a9a":"# re-load the train and test sets in their raw state\n\n# train = pd.read_csv('..\/input\/train.csv')\n# test = pd.read_csv('..\/input\/test.csv')","63ac47c3":"df_all = pd.concat([train,test],sort=False)","886f0a85":"# create new features using for joining main data to the structures data\n\ndf_all['jcoupling_type'] = df_all['type'].str.extract(\"(\\d)J\\w\") \ndf_all['jcoupling_atoms'] = df_all['type'].str.extract(\"\\dJ(\\w*)\") \ndf_all['jcoupling_atom_0'] = df_all.type.str[2]\ndf_all['jcoupling_atom_1'] = df_all.type.str[3]\ndf_all['coupling_atom_type'] = df_all['type'].str.extract(\"\\dJ(\\w*)\") \ndf_all['jcoupling_atom_0'] = df_all['atom_index_0'].astype(str) + '_' + df_all['jcoupling_atom_0']\ndf_all['jcoupling_atom_1'] = df_all['atom_index_1'].astype(str) + '_' + df_all['jcoupling_atom_1']\ndf_all['jcoupling_atoms'] = df_all['jcoupling_atom_0'].astype(str) + '_' + df_all['jcoupling_atom_1']\n\nstructures['jcoupling_atoms'] = structures['atom_index'].astype(str) + '_' + structures['atom']","a4ee545b":"df_all.head()","2ab5f9fd":"df_all.shape # 7,163,689","c500811c":"# join the atom structure xyz to the molecule data\n\ndf_new = pd.merge(df_all,structures[['molecule_name','jcoupling_atoms','x','y','z']],left_on=['molecule_name','jcoupling_atom_0'],right_on=['molecule_name','jcoupling_atoms'],how='left')\ndf_new.drop(columns='jcoupling_atoms_y',inplace=True)\ndf_new.rename(columns={\"x\": \"x_atom_0\", \"y\": \"y_atom_0\", \"z\": \"z_atom_0\",\"jcoupling_atoms_x\": \"jcoupling_atoms\"},inplace=True)\ndf_new.head()","30f8502c":"df_new = pd.merge(df_new,structures[['molecule_name','jcoupling_atoms','x','y','z']],left_on=['molecule_name','jcoupling_atom_1'],right_on=['molecule_name','jcoupling_atoms'],how='left')\ndf_new.drop(columns='jcoupling_atoms_y',inplace=True)\ndf_new.rename(columns={\"x\": \"x_atom_1\", \"y\": \"y_atom_1\", \"z\": \"z_atom_1\",\"jcoupling_atoms_x\": \"jcoupling_atoms\"},inplace=True)\ndf_new.head()","ee050d8f":"# calculate distance between jcoupling atoms in the molecule structure\n\ndf_new['dist'] = np.sqrt( (df_new.x_atom_0-df_new.x_atom_1)**2 + (df_new.y_atom_0-df_new.y_atom_1)**2 + (df_new.z_atom_0-df_new.z_atom_1)**2)\ndf_new.head()","6c2d21f0":"df_new.shape #7163689","5d6898f3":"final_features = ['id','molecule_name','atom_index_0','atom_index_1','type','jcoupling_type','coupling_atom_type','dist','scalar_coupling_constant']\nmolecule_atoms = df_new[final_features]","3a632a98":"molecule_atoms.head()","54851a04":"# encode categorical features\n\nfrom sklearn.preprocessing import LabelEncoder\n\nohe = pd.get_dummies(molecule_atoms['type'],prefix='ohe')\n# molecule_atoms.drop('type',axis=1,inplace=True)\nmolecule_atoms = pd.concat([molecule_atoms,ohe],axis=1)\nmolecule_atoms.head()","25b593f5":"train = molecule_atoms[molecule_atoms.id.isin(train.id)]\ntest = molecule_atoms[molecule_atoms.id.isin(test.id)]","0a176d71":"test.head()","35df64f3":"# drop the target variable from the test set\ntest = test.drop(columns='scalar_coupling_constant')\ntest.columns","876907ff":"train.columns","39448f70":"# left join the scalar coupling contribution dataframe to the train set\ntrain_new = pd.merge(train,scalar_coupling_contributions,left_on=['molecule_name','atom_index_0','atom_index_1','type'],right_on=['molecule_name','atom_index_0','atom_index_1','type'],how='left')\ntrain_new.head()","5466271b":"# left join the mulliken_charges and potential_energy dataframes to the train set\ntrain_new = pd.merge(train_new,potential_energy,left_on=['molecule_name'],right_on=['molecule_name'],how='left')\ntrain_new.head()","78add3df":"train_new.columns # 4,658,147","714ef705":"# numerical features except the scalar_coupling_constant,sd, pso, dso, potential_energy - one additional feature at a time to predict\nnum_features = ['atom_index_0', 'atom_index_1', 'jcoupling_type', 'dist',\n       'ohe_1JHC', 'ohe_1JHN', 'ohe_2JHC','ohe_2JHH', 'ohe_2JHN', 'ohe_3JHC', 'ohe_3JHH', 'ohe_3JHN']","2af19933":"# scale the numerical features\nfrom sklearn.preprocessing import StandardScaler\n\ntest_new = test.copy()\nsc = StandardScaler()\ntrain_new[num_features] = sc.fit_transform(train_new[num_features])\ntest_new[num_features] = sc.transform(test_new[num_features])","fdfe84fc":"test_new.head()","b8440c74":"# predict fc in the test\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n\n\n# kf = KFold(n_splits=5,shuffle=True,random_state=123)\n\n# fold = 0\n# fold_metrics = []\n# for train_index, test_index in kf.split(train_new):\n#     cv_train, cv_test = train_new.iloc[train_index], train_new.iloc[test_index]\n#     regressor = GradientBoostingRegressor()\n#     regressor.fit(X=cv_train[num_features],y=cv_train['fc'])\n#     predictions = regressor.predict(cv_test[num_features])\n#     metric = np.log(mean_absolute_error(cv_test['fc'],predictions))\n#     fold_metrics.append(metric)\n#     print('Fold:{}'.format(fold))\n#     print('CV train shape:{}'.format(cv_train.shape))\n#     print('Log mean squared error:{}'.format(metric))\n#     fold+=1","7231b0f3":"# Average and overall metric\n\n# mean_score = np.mean(fold_metrics)\n# overal_score_minimizing = np.mean(fold_metrics)+np.std(fold_metrics)\n# print(mean_score,overal_score_minimizing)\n\n# lr ---> 1.4350236728166157 1.4368374949231812\n# gb ---> 0.978808394321879 0.9800942928921375\n# rf ---> 1.0059670776324137 1.0071424059922574\n# dt ---> 1.1635835592319883 1.1646638382205583\n# xgb --> 0.9794824442750851 0.9803075149244272","cd0dcf8a":"# predict the fc feature\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nregressor = GradientBoostingRegressor()\nregressor.fit(X=train_new[num_features],y=train_new['fc'])\ntest_new['fc'] = regressor.predict(test_new[num_features])","aae81420":"# test_new.head()\ntrain = train_new.copy()\ntest = test_new.copy()","d91c4fe8":"# from sklearn.model_selection import GridSearchCV\n\n# param_grid = {\n#     'max_depth' : [3, 5, 7],\n#     'subsample' : [0.8, 0.9, 1.0]\n# }\n\n# regressor = GradientBoostingRegressor()\n# grid_search = GridSearchCV(estimator = regressor, param_grid = param_grid, \n#                           cv = 3,  verbose = 2)\n# grid_search.fit(train_new[num_features], train_new['fc'])","0fe70b8b":"features = ['atom_index_0', 'atom_index_1', 'dist','fc', 'ohe_1JHC', 'ohe_1JHN', 'ohe_2JHC',\n       'ohe_2JHH', 'ohe_2JHN', 'ohe_3JHC', 'ohe_3JHH', 'ohe_3JHN']\n# , 'jcoupling_type'","fc3163e0":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\ntrain[features] = sc.fit_transform(train[features])\ntest[features] = sc.transform(test[features])","203082c7":"train[features].head()","9f18581c":"# from sklearn.linear_model import LinearRegression\n\n# # fit the model on the train set\n# lr = LinearRegression()\n# lr.fit(X=train[features],y=train['scalar_coupling_constant'])","d595815c":"# kfold cross-validation for evaluating the model\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\nkf = KFold(n_splits=5,shuffle=True,random_state=123)\n\n# fold = 0\n# fold_metrics = []\n# for train_index, test_index in kf.split(train):\n#     cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n#     lr.fit(X=cv_train[features],y=cv_train['scalar_coupling_constant'])\n#     predictions = lr.predict(cv_test[features])\n#     metric = np.log(mean_absolute_error(cv_test['scalar_coupling_constant'],predictions))\n#     fold_metrics.append(metric)\n#     print('Fold:{}'.format(fold))\n#     print('CV train shape:{}'.format(cv_train.shape))\n#     print('Log mean squared error:{}'.format(metric))\n#     fold+=1","43521a49":"# Average and overall metric\n\n# mean_score = np.mean(fold_metrics)\n# overal_score_minimizing = np.mean(fold_metrics)+np.std(fold_metrics)\n# print(mean_score,overal_score_minimizing)","b8775484":"# test['scalar_coupling_constant'] = lr.predict(test[features])","e1f912c4":"# submission_2 = test[['id','scalar_coupling_constant']]","951faaf4":"# submission_2.head()","0bfa38c7":"# submission_2.shape","c47b2071":"# submission_2.to_csv('submission_v2.csv',index=False)","1657ef33":"# from sklearn.ensemble import RandomForestRegressor\n\n# rf = RandomForestRegressor()\n\n# fold = 0\n# fold_metrics = []\n# for train_index, test_index in kf.split(train):\n#     cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n#     rf.fit(X=cv_train[features],y=cv_train['scalar_coupling_constant'])\n#     predictions = rf.predict(cv_test[features])\n#     metric = np.log(mean_absolute_error(cv_test['scalar_coupling_constant'],predictions))\n#     fold_metrics.append(metric)\n#     print('Fold:{}'.format(fold))\n#     print('CV train shape:{}'.format(cv_train.shape))\n#     print('Log mean squared error:{}'.format(metric))\n#     fold+=1","6ddb98db":"# mean_score = np.mean(fold_metrics)\n# overal_score_minimizing = np.mean(fold_metrics)+np.std(fold_metrics)\n# print(mean_score,overal_score_minimizing)","9695c2e3":"from sklearn.ensemble import GradientBoostingRegressor\n\n# regressor = GradientBoostingRegressor()\n\n# fold = 0\n# fold_metrics = []\n# for train_index, test_index in kf.split(train):\n#     cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n#     regressor.fit(X=cv_train[features],y=cv_train['scalar_coupling_constant'])\n#     predictions = regressor.predict(cv_test[features])\n#     metric = np.log(mean_absolute_error(cv_test['scalar_coupling_constant'],predictions))\n#     fold_metrics.append(metric)\n#     print('Fold:{}'.format(fold))\n#     print('CV train shape:{}'.format(cv_train.shape))\n#     print('Log mean squared error:{}'.format(metric))\n#     fold+=1\n\"\"\"\nFold:0\nCV train shape:(3726517, 22)\nLog mean squared error:-1.651942379074779\nFold:1\nCV train shape:(3726517, 22)\nLog mean squared error:-1.6581039302273266\nFold:2\nCV train shape:(3726518, 22)\nLog mean squared error:-1.64427536075409\nFold:3\nCV train shape:(3726518, 22)\nLog mean squared error:-1.6546618867855556\nFold:4\nCV train shape:(3726518, 22)\nLog mean squared error:-1.6543852532228525\n\"\"\"","d98287a3":"mean_score = np.mean(fold_metrics)\noveral_score_minimizing = np.mean(fold_metrics) + np.std(fold_metrics)\nprint(mean_score,overal_score_minimizing) # -1.6526737620129208 -1.648038319556876","fe0523e6":"from sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor()\ngb.fit(X= train[features],y=train['scalar_coupling_constant'])\ntest['scalar_coupling_constant'] = gb.predict(test[features])","16529806":"submission_5 = test[['id','scalar_coupling_constant']]\nsubmission_5.to_csv('submission_v5.csv',index=False)","89b9325d":"submission_4.head()","2c7e19a2":"test.head()","1889ea60":"### 5.6. select the relevant features for the model","eb3d19e8":"### 5.9. use the features in the additional files and predicate the correspondance values in the test set","17be76bc":"## 5. Feature engineering","7559ef55":"### 5.8. split the dataframe into the train and test sets","49672073":"**features to be predicted in the test set:**\n\n* fc\n* sd\n* so\n* dso\n* potential_energy","a135eeaa":"## 1. Load the main data sets","3f71379b":"## 3. Investigate the train and test sets - EDA","5c8cd853":" Target is a continuous variable, therefore this is a regression problem. ","51a87c6c":"### 5.1. concatenate test and train sets","c7cb91df":"### 5.10. Build regression models to predict additional features in the test set","6bb542b9":"### 5.5. calculate the distance between the coupling atoms ","bbeb207e":"### 5.2. create new features using the type variable","e740b0c8":"### 5.4. Left join the concatenated data to the structures dataframe to get xyz values for the atom index_1","c955ead5":"### 6.1. Linear Regression","3a541b18":"### 6.2. Local validation","7abc1c38":"### 5.7. encode the type variable","6315f6a9":"### 5.11. Hyperparameter tunning","0cd52ca1":"### 5.3. Left join the concatenated data to the structures dataframe to get xyz values for the atom index_0 ","3422957a":"Additional Data\nNOTE: additional data is provided for the molecules in Train only!\n\n**dipole_moments.csv** - contains the molecular electric dipole moments. These are three dimensional vectors that indicate the charge distribution in the molecule. The first column (molecule_name) are the names of the molecule, the second to fourth column are the X, Y and Z components respectively of the dipole moment.\n\n**magnetic_shielding_tensors.csv** - contains the magnetic shielding tensors for all atoms in the molecules. The first column (molecule_name) contains the molecule name, the second column (atom_index) contains the index of the atom in the molecule, the third to eleventh columns contain the XX, YX, ZX, XY, YY, ZY, XZ, YZ and ZZ elements of the tensor\/matrix respectively.\n\n**mulliken_charges.csv** - contains the mulliken charges for all atoms in the molecules. The first column (molecule_name) contains the name of the molecule, the second column (atom_index) contains the index of the atom in the molecule, the third column (mulliken_charge) contains the mulliken charge of the atom.\n\n**potential_energy.csv** - contains the potential energy of the molecules. The first column (molecule_name) contains the name of the molecule, the second column (potential_energy) contains the potential energy of the molecule.\n\n**scalar_coupling_contributions.csv** - The scalar coupling constants in train.csv (or corresponding files) are a sum of four terms.\n\n**scalar_coupling_contributions.csv** contain all these terms. The first column (molecule_name) are the name of the molecule, the second (atom_index_0) and third column (atom_index_1) are the atom indices of the atom-pair, the fourth column indicates the type of coupling, the fifth column (fc) is the Fermi Contact contribution, the sixth column (sd) is the Spin-dipolar contribution, the seventh column (pso) is the Paramagnetic spin-orbit contribution and the eighth column (dso) is the Diamagnetic spin-orbit contribution.","f60701fe":"## 6. Build models","ffb15f69":"## 4. Identify the problem type","a707dd6a":"## 2. Load the additional sets","06034d92":"### 6.3. RandomForest Regressor"}}