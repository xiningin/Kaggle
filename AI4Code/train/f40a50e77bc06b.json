{"cell_type":{"c07f79af":"code","2bbbd030":"code","a0fab968":"code","2ed64aad":"code","e988b821":"code","5ceb1d8e":"code","f5abe052":"code","e58e5521":"code","debea405":"code","06f65aaf":"code","84be785a":"code","bb1fb75e":"code","89ac25a8":"code","aed18f19":"code","d7aa0688":"code","f00c8d18":"code","4ad94941":"code","105a1fde":"code","5d0dd484":"markdown","8a130a03":"markdown","f88664a1":"markdown","5dd4ab45":"markdown","ddbb478f":"markdown","00e1e19f":"markdown","c53028c7":"markdown","82fe304e":"markdown","4178f69b":"markdown","8ff2798e":"markdown","f05a5b6b":"markdown","e2237c74":"markdown"},"source":{"c07f79af":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport random\nfrom random import seed\nsns.set_theme(style=\"white\", palette=None)\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom sklearn.preprocessing import StandardScaler","2bbbd030":"dataset=pd.read_csv('..\/input\/mall-customers\/Mall_Customers.csv', index_col=False)\ndataset.describe()","a0fab968":"X = dataset.iloc[:, [3, 4]].values","2ed64aad":"def kmeans (K, n_iter, dataset, rn=170) :\n    output={}\n    Centroids=[]\n    X = dataset\n    m=X.shape[0] \n    n=X.shape[1] \n    seed(rn)\n    for i in range (K) : \n        rand=random.randint(0, m-1)\n        Centroids.append(X[rand])\n        \n    Centroids = np.array(Centroids).T\n    for i in range(n_iter):\n        EuclidianDistance=np.array([]).reshape(m,0)\n        for k in range(K):\n            tempDist=np.sum((X-Centroids[:,k])**2,axis=1)\n            EuclidianDistance=np.c_[EuclidianDistance,tempDist]\n        C=np.argmin(EuclidianDistance,axis=1)+1\n        Y={}\n        for k in range(K):\n            Y[k+1]=np.array([]).reshape(2,0)\n        for i in range(m):\n            Y[C[i]]=np.c_[Y[C[i]],X[i]]\n\n        for k in range(K):\n            Y[k+1]=Y[k+1].T\n\n        for k in range(K):\n            Centroids[:,k]=np.mean(Y[k+1],axis=0)\n        output=Y\n    return Centroids, output","e988b821":"centroids, output = kmeans(5, 100, X)","5ceb1d8e":"fig, ax = plt.subplots(1, 1, figsize=(10, 8))\nsns.scatterplot(x=X[:,0],y=X[:,1],color='black',label='unclustered data')\nplt.xlabel('Income')\nplt.ylabel('Number of transactions')\nplt.legend()\nplt.title('Plot of data points')\nplt.show()","f5abe052":"def plot(Centroids,Output, K) :\n    labels=[]\n    for i in range (K):\n        s = f'cluster{i+1}'\n        labels.append(s)\n    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n    color=sns.color_palette(None, K)\n    for k in range(K):\n        sns.scatterplot(x=Output[k+1][:,0], y=Output[k+1][:,1],color=color[k],label=labels[k])\n    sns.scatterplot(x=Centroids[0,:],y=Centroids[1,:],s=200,color='black',label='Centroids', marker='^')\n    plt.xlabel('Income')\n    plt.ylabel('Number of transactions')\n    plt.legend()\n    plt.show()","e58e5521":"plot(centroids, output, 5)","debea405":"centroids_for_different_k = []\nclusters_for_different_k = []\nfor i in range (1, 11) : \n    Centroids, Clusters = kmeans(i, 310, X)\n    centroids_for_different_k.append(Centroids)\n    clusters_for_different_k.append(Clusters)","06f65aaf":"fig, ax = plt.subplots(5, 2, figsize=(25, 40))\nk = 0;\nfor i in range (0, 5):\n    for j in range (0, 2):\n        labels=[]\n        for z in range (k+1):\n            s = f'cluster{z+1}'\n            labels.append(s)\n        color=sns.color_palette(None, k+1)\n        for l in range(0, k+1):\n            ax[i][j].scatter(x=clusters_for_different_k[k][l+1][:,0], y=clusters_for_different_k[k][l+1][:,1],color=color[l],label=labels[l])\n            \n        ax[i][j].scatter(x=centroids_for_different_k[k][0,:],y=centroids_for_different_k[k][1,:],s=200,color='black',label='Centroids', marker='^')\n        ax[i][j].set_title(f\"Number of Clusters K :{k+1}\", fontweight='bold')\n        ax[i][j].set_ylabel(\"Number of transactions\")\n        ax[i][j].set_xlabel( \"Income\")\n        ax[i][j].legend()\n        k=k+1","84be785a":"WCSS_array=np.array([])\nfor K in range(1,11):\n    Centroids, Output =kmeans(K, 100, X)\n    wcss=0\n    Centroids = Centroids.T\n    for k in range(K):\n        wcss+=np.sum((Output[k+1]-Centroids[k,:])**2)\n    WCSS_array=np.append(WCSS_array,wcss)\nK_array=np.arange(1,11,1)\nplt.plot(K_array,WCSS_array)\nplt.xlabel('Number of Clusters')\nplt.ylabel('within-cluster sums of squares (WCSS)')\nplt.title('Elbow method to determine optimum number of clusters')\nplt.show()","bb1fb75e":"centroids, output = kmeans(3, 100, X, 10)\nplot(centroids, output, 3)","89ac25a8":"centroids, output = kmeans(3, 50, X, 50)\nplot(centroids, output, 3)","aed18f19":"from sklearn import cluster, datasets\nn_samples = 1500\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)","d7aa0688":"blobs = datasets.make_blobs( n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5])\nsx, sy = blobs\ndf = pd.DataFrame({'x':sx[:,0], 'y':sx[:,1]}).values\ncen, clu = kmeans(3, 100, df)\nplot(cen, clu, 3)","f00c8d18":"sx, sy = noisy_circles\ndf = pd.DataFrame({'x':sx[:,0], 'y':sx[:,1]}).values\ncen, clu = kmeans(2, 100, df)\nplot(cen, clu, 2)","4ad94941":"sx, sy = noisy_moons\ndf = pd.DataFrame({'x':sx[:,0], 'y':sx[:,1]}).values\ncen, clu = kmeans(2, 100, df)\nplot(cen, clu, 2)","105a1fde":"random_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\nsx, sy = aniso\ndf = pd.DataFrame({'x':sx[:,0], 'y':sx[:,1]}).values\ncen, clu = kmeans(3, 100, df)\nplot(cen, clu, 3)","5d0dd484":"# Optimization Problem\n\nGiven a set X of n points in a d-dimensional space and an integer K , Which group the points into K clusters \n$C = \\{C_1, C_2,...,C_k \\}$ such that \n\n$\\large Cost(C) = \\large\\sum_{i = 1}^K \\sum_{x \\epsilon C_i} (x - c_i)^2$\n\nis minimized, where $\\large c_i$ is the centroid of the points in cluster $\\large C_i$","8a130a03":"thus we get very different cluster on two different runs if we initilalize the centroids randomly.\n\nThis Problem can be solved if we select original set of points by methods other than random. E.g., pick the most distant (from each other) points as cluster centers (K-means++ algorithm)","f88664a1":"# Limitations of K-means\n\n### 1. K-means changes with differential cluster initialization ","5dd4ab45":"# Procedure For K-Means","ddbb478f":"finding the solution to the aforementioned optimization problem in polynomial time is not possible, there we approximate it by using Lloyd\u2019s algorithm.","00e1e19f":"# K-Means\n\nIt is Partitional clustering approach where each cluster is associated with a centroid (center point), Each point is assigned to the cluster with the closest centroid.\nNumber of clusters, K, is a Hyperparameter which we must specified.\nThus The objective is to minimize the sum of distances of the points to their respective centroid","c53028c7":"### 2. K-means has problems when clustersare of different\n - Sizes\n - Densities\n - Non-globular shapes","82fe304e":"# Clustering\nIn general a grouping of objects such that the objects in a group (cluster) are similar (or related) to one another and different from (or unrelated to) the objects in other groups\n\n## Types of Clustering :\n1. Partional\n2. Heirarchical\n3. Density Based","4178f69b":"hierarchical clustering can be used to solve the aforementioned problems","8ff2798e":"# Determining Optimal Number of Clusters\n\nSum of squares of distances of every data point from its corresponding cluster centroid should be as minimum as possible.\nwe use a method called ELBOW method to find the appropriate number of clusters. The parameter which will be taken into consideration is Sum of squares of distances of every data point from its corresponding cluster centroid which is called WCSS ( Within-Cluster Sums of Squares).\nSteps involved in ELBOW method are:\n- Perform K-means clustering on different values of K ranging from 1 to any upper limit.\n- For each K, calculate WCSS (Within Cluster Sum of Squares)\n- Plot the value for WCSS with the number of clusters K.\n\nThe location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters. i.e the point after which WCSS doesn\u2019t decrease more rapidly is the appropriate value of K.","f05a5b6b":"__Step 1:__ random selection of centroids\n\n__Step 2:__ calculate distance to each point and assign each point to cluster\n\n__Step 3:__ calculate average of the assigned points and\n\n__Step 4:__ move centroids to the new position\n\n__Step 5:__ Steps 2-4 until cluster assignment is not changed","e2237c74":"# K-means with Different Values of K"}}