{"cell_type":{"bedd9ead":"code","29e34f95":"code","616c1112":"code","501e8941":"code","87865200":"code","d28629ad":"code","9235473b":"code","0aaf5d15":"code","df9bc901":"code","a42dd46f":"code","aa1def2f":"code","1afaa9f1":"code","fb5a10e9":"code","b16a2c81":"code","65588a57":"code","80031fc7":"code","01c9c5e9":"code","b57435f0":"code","c0367994":"code","0f04d211":"code","2e9f5b41":"code","2fd70457":"code","19921e85":"code","d70aa01d":"code","fdd8ae83":"code","922f18c4":"code","445dcf49":"code","ebd144da":"code","7ca89df7":"code","df96dabe":"code","d4ab8fe9":"code","b7388a68":"markdown","c915b071":"markdown","dff777ab":"markdown","6b0b880a":"markdown","eef954d6":"markdown","70601b81":"markdown","28c17603":"markdown","efeb8e4e":"markdown","4dc21ad8":"markdown","41eb746b":"markdown","8cdbff3d":"markdown","b8a47425":"markdown","7f0c3b16":"markdown","32443e7a":"markdown","3b3bfc33":"markdown","6daeb8da":"markdown","82d61df0":"markdown","27a1422e":"markdown","21d828ca":"markdown"},"source":{"bedd9ead":"!pip install '\/kaggle\/input\/torch-15\/yacs-0.1.7-py3-none-any.whl'\n!pip install '\/kaggle\/input\/torch-15\/fvcore-0.1.1.post200513-py3-none-any.whl'\n!pip install '\/kaggle\/input\/pycocotools\/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl'\n!pip install '\/kaggle\/input\/detectron2\/detectron2-0.1.3cu101-cp37-cp37m-linux_x86_64.whl'","29e34f95":"from detectron2.config import get_cfg\nfrom detectron2 import model_zoo","616c1112":"import numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\nimport seaborn as sns\n%matplotlib inline\n\nimport cv2\nimport itertools\nimport copy\n\n\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor, DefaultTrainer\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.structures import BoxMode\nimport detectron2.data.transforms as T\nfrom detectron2.data import detection_utils as utils","501e8941":"DATA_DIR  = '\/kaggle\/input\/global-wheat-detection\/train\/'\nTEST_DIR  = '\/kaggle\/input\/global-wheat-detection\/test\/'\nList_Data_dir = os.listdir(DATA_DIR)","87865200":"# for example model\n\nMODEL_USE = 3\n\nif MODEL_USE == 1:\n    MODEL = 'COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml'\n    WEIGHT_PATH = '..\/input\/global-wheat-detection-model\/Detectron_2\/faster_rcnn_R_50_FPN_3x.pth'\nelif MODEL_USE == 2:\n    MODEL = 'COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml'\n    WEIGHT_PATH = '..\/input\/global-wheat-detection-model\/Detectron_2_v2\/R-50_5k_augmen.pth'\nelif MODEL_USE == 3:\n    MODEL = 'COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml'\n    WEIGHT_PATH = '..\/input\/global-wheat-detection-model\/Detectron_2_v2\/R-50_10k_augmen.pth'\n    \nMAX_ITER = 5000 #10000","d28629ad":"raw = pd.read_csv('\/kaggle\/input\/global-wheat-detection\/train.csv')\nraw","9235473b":"print(f'Total number of train images: {raw.image_id.nunique()}')\nprint(f'Total number of test images: {len(os.listdir(TEST_DIR))}')","0aaf5d15":"plt.figure(figsize=(15,8))\nplt.title('Wheat Distribution', fontsize= 20)\nsns.countplot(x=\"source\", data=raw)\n\n# based on the chart, there are seven types of wheat from images data, with the most types 'ethz_1' and the least is 'inrae_1'","df9bc901":"# Extract bbox column to xmin, ymin, width, height, then create xmax, ymax, and area columns\n\nraw[['xmin','ymin','w','h']] = pd.DataFrame(raw.bbox.str.strip('[]').str.split(',').tolist()).astype(float)\nraw['xmax'], raw['ymax'], raw['area'] = raw['xmin'] + raw['w'], raw['ymin'] + raw['h'], raw['w'] * raw['h']\nraw","a42dd46f":"def show_image(image_id):\n    \n    fig, ax = plt.subplots(1, 2, figsize = (24, 24))\n    ax = ax.flatten()\n    \n    bbox = raw[raw['image_id'] == image_id ]\n    img_path = os.path.join(DATA_DIR, image_id + '.jpg')\n    \n    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n    image \/= 255.0\n    image2 = image\n    \n    ax[0].set_title('Original Image')\n    ax[0].imshow(image)\n    \n    for idx, row in bbox.iterrows():\n        x1 = row['xmin']\n        y1 = row['ymin']\n        x2 = row['xmax']\n        y2 = row['ymax']\n        label = row['source']\n        \n        cv2.rectangle(image2, (int(x1),int(y1)), (int(x2),int(y2)), (255,255,255), 2)\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        cv2.putText(image2, label, (int(x1),int(y1-10)), font, 1, (255,255,255), 2)\n    \n    ax[1].set_title('Image with Bondary Box')\n    ax[1].imshow(image2)\n\n    plt.show()","aa1def2f":"show_image(raw.image_id.unique()[1111])","1afaa9f1":"# split train, val\nunique_files = raw.image_id.unique()\n\ntrain_files = set(np.random.choice(unique_files, int(len(unique_files) * 0.90), replace = False))\ntrain_df = raw[raw.image_id.isin(train_files)]\ntest_df = raw[~raw.image_id.isin(train_files)]","fb5a10e9":"print(len(train_df.image_id.unique()), len(test_df.image_id.unique()))","b16a2c81":"def custom_dataset(df, dir_image):\n    \n    dataset_dicts = []\n    \n    for img_id, img_name in enumerate(df.image_id.unique()):\n        \n        record = {}\n        image_df = df[df['image_id'] == img_name]\n        img_path = dir_image + img_name + '.jpg'\n        \n        record['file_name'] = img_path\n        record['image_id'] = img_id\n        record['height'] = int(image_df['height'].values[0])\n        record['width'] = int(image_df['width'].values[0])\n                \n        objs = []\n        for _, row in image_df.iterrows():\n            \n            x_min = int(row.xmin)\n            y_min = int(row.ymin)\n            x_max = int(row.xmax)\n            y_max = int(row.ymax)\n            \n            poly = [(x_min, y_min), (x_max, y_min),\n                    (x_max, y_max), (x_min, y_max) ]\n            \n            poly = list(itertools.chain.from_iterable(poly))\n            \n            obj = {\n               \"bbox\": [x_min, y_min, x_max, y_max],\n               \"bbox_mode\": BoxMode.XYXY_ABS,\n               \"segmentation\": [poly],\n               \"category_id\": 0,\n               \"iscrowd\" : 0\n                \n                  }\n            \n            objs.append(obj)\n            \n        record['annotations'] = objs\n        dataset_dicts.append(record)\n        \n    return dataset_dicts","65588a57":"def custom_mapper(dataset_dict):\n    # Implement a mapper, similar to the default DatasetMapper, but with your own customizations\n    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n    transform_list = [T.Resize((800,800)),\n                      T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n                      T.RandomFlip(prob=0.5, horizontal=True, vertical=False), \n                      ]\n    image, transforms = T.apply_transform_gens(transform_list, image)\n    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n\n    annos = [\n        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n        for obj in dataset_dict.pop(\"annotations\")\n        if obj.get(\"iscrowd\", 0) == 0\n    ]\n    instances = utils.annotations_to_instances(annos, image.shape[:2])\n    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n    return dataset_dict\n\n\nclass WheatTrainer(DefaultTrainer):\n    \n    @classmethod\n    def build_train_loader(cls, cfg):\n        return build_detection_train_loader(cfg, mapper=custom_mapper)\n","80031fc7":"def register_dataset(df, dataset_label='wheat_train', image_dir = DATA_DIR):\n    \n    # Register dataset - if dataset is already registered, give it a new name    \n    try:\n        DatasetCatalog.register(dataset_label, lambda d=df: custom_dataset(df, image_dir))\n        MetadataCatalog.get(dataset_label).set(thing_classes = ['wheat'])\n    except:\n        # Add random int to dataset name to not run into 'Already registered' error\n        n = random.randint(1, 1000)\n        dataset_label = dataset_label + str(n)\n        DatasetCatalog.register(dataset_label, lambda d=df: custom_dataset(df, image_dir))\n        MetadataCatalog.get(dataset_label).set(thing_classes = ['wheat'])\n\n    return MetadataCatalog.get(dataset_label), dataset_label","01c9c5e9":"metadata, train_dataset = register_dataset(train_df)\nmetadata, val_dataset = register_dataset(test_df, dataset_label='wheat_test')","b57435f0":"MODEL","c0367994":"cfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(MODEL))\ncfg.DATASETS.TRAIN = (train_dataset,)\ncfg.DATASETS.TEST = ()\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(MODEL)  \ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR =  0.00025 \ncfg.SOLVER.MAX_ITER = MAX_ITER\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128     \ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = WheatTrainer(cfg)","0f04d211":"# create data loader\n# code from https:\/\/www.kaggle.com\/julienbeaulieu\/detectron2-wheat-detection-eda-training-eval\n\ntrain_data_loader = trainer.build_train_loader(cfg)\ndata_iter = iter(train_data_loader)\nbatch = next(data_iter)","2e9f5b41":"# visualization\n\nrows, cols = 1, 2\nplt.figure(figsize=(20,20))\n\nfor i, per_image in enumerate(batch[:4]):\n    \n    plt.subplot(rows, cols, i+1)\n    \n    # Pytorch tensor is in (C, H, W) format\n    img = per_image[\"image\"].permute(1, 2, 0).cpu().detach().numpy()\n    img = utils.convert_image_to_rgb(img, cfg.INPUT.FORMAT)\n\n    visualizer = Visualizer(img, metadata=metadata, scale=0.5)\n\n    target_fields = per_image[\"instances\"].get_fields()\n    labels = None\n    vis = visualizer.overlay_instances(\n        labels=labels,\n        boxes=target_fields.get(\"gt_boxes\", None),\n        masks=target_fields.get(\"gt_masks\", None),\n        keypoints=target_fields.get(\"gt_keypoints\", None),\n    )\n    plt.imshow(vis.get_image()[:, :, ::-1])","2fd70457":"trainer.resume_or_load(resume=False)\ntrainer.train()","19921e85":"# model + path\n\nprint(MODEL)\nprint(WEIGHT_PATH)","d70aa01d":"cfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(MODEL))\ncfg.MODEL.WEIGHTS = WEIGHT_PATH\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\ncfg.DATASETS.TEST = ('wheat_test', )\npredictor = DefaultPredictor(cfg)","fdd8ae83":"evaluator = COCOEvaluator(val_dataset, cfg, False, output_dir=\".\/output\/\")\nval_loader = build_detection_test_loader(cfg, val_dataset)\ninference_on_dataset(trainer.model, val_loader, evaluator)","922f18c4":"df_sub = pd.read_csv('..\/input\/global-wheat-detection\/sample_submission.csv')\ndf_sub","445dcf49":"# CONFIG\n\nfont = cv2.FONT_HERSHEY_SIMPLEX     \nfontScale = 1 \ncolor = (255, 255, 0)\nthickness = 2\nresults = []\n\ndef format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)\n\n\ndef result_show(df, color):\n    \n    for image_id in df_sub['image_id']:\n        im = cv2.imread('{}\/{}.jpg'.format(TEST_DIR, image_id))\n        boxes = []\n        scores = []\n        labels = []\n        outputs = predictor(im)\n        out = outputs[\"instances\"].to(\"cpu\")\n        scores = out.get_fields()['scores'].numpy()\n        boxes = out.get_fields()['pred_boxes'].tensor.numpy().astype(int)\n        labels= out.get_fields()['scores'].numpy()\n        boxes = boxes.astype(int)\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        result = {'image_id': image_id,'PredictionString': format_prediction_string(boxes, scores)}\n        results.append(result)\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB).astype(np.float32)\n        im \/= 255.0\n        \n        for b,s in zip(boxes,scores):\n            cv2.rectangle(im, (b[0],b[1]), (b[0]+b[2],b[1]+b[3]), color, thickness)\n            cv2.putText(im, '{:.2}'.format(s), (b[0],b[1]), font, 1, color, thickness)\n                \n        plt.figure(figsize=(12,12))\n        plt.imshow(im)","ebd144da":"result_show(df_sub['image_id'], color = (255, 255, 255))","7ca89df7":"result_show(df_sub['image_id'], color = (255, 255, 0))","df96dabe":"result_show(df_sub['image_id'], color = (255, 0, 0))","d4ab8fe9":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.to_csv('submission.csv', index=False)\ntest_df","b7388a68":"## References\n\n\n* https:\/\/github.com\/facebookresearch\/detectron2\n\n* https:\/\/www.kaggle.com\/dhiiyaur\/pytorch-fasterrcnn-eda-augmentation-training\n    \n* https:\/\/www.kaggle.com\/orkatz2\/inference-detectron2-resnest","c915b071":"## Without Augmentation\n\n> ![Untitled.png](attachment:Untitled.png)","dff777ab":"# Result","6b0b880a":"## Model 1\n\n> - faster_rcnn_R_50_FPN_3x\n> - Iter 5000","eef954d6":"<h3><center>Thank you for reading my notebook, upvote if you like this notebook    :)<h3><center>\n    \n****","70601b81":"![11Untitled.png](attachment:11Untitled.png)","28c17603":"## Import necessary libraries","efeb8e4e":"## Model 2\n\n> - faster_rcnn_R_50_FPN_3x\n> - Iter 5000\n> - RandomFlip Vertical (0.5)\n> - RandomFlip Horizontal (0.5)","4dc21ad8":"## Evaluation and Inference","41eb746b":"## Load Data and Simple EDA","8cdbff3d":"## With Augmentation\n\n> Augmentation\n> * RandomFlip Vertical (0.5)\n> * RandomFlip Horizontal (0.5)\n> * iter 5k & 10k\n\n>![Untitled.png](attachment:Untitled.png)","b8a47425":"## Data Preprocessing","7f0c3b16":"## Create a model and training","32443e7a":"<h1><center>DETECTRON 2 COMPARE MODELS<\/h1>\n    \n***","3b3bfc33":"## Augmentation Visualization","6daeb8da":"### Config","82d61df0":"## Changelog\n\n### V1\n\n> - add new model\n> - add augmentation","27a1422e":"## Model 3\n\n> - faster_rcnn_R_50_FPN_3x\n> - Iter 10000\n> - RandomFlip Vertical (0.5)\n> - RandomFlip Horizontal (0.5)","21d828ca":"<h1>Introduction<\/h1>\n\n<p style=\"text-align:justify;\">This competition wants us to predict bounding boxes around whaet heads from images of wheat plants, to solve this problom we have a sample of image, and csv file containing the image_id (the unique image ID), the width and height of the images, and bounding box, formatted as a Python-style list of [xmin, ymin, width, height]<\/p>\n\n***\n\nHello, this is my second notebook in this competition, if you want to see my other notebook, [Global Wheat Detection FASTER R-CNN [EDA - AUGMENTATION - COMPARE MODELS]](https:\/\/www.kaggle.com\/dhiiyaur\/fasterrcnn-eda-augmentation-compare-models). The main idea of this notebook its to compare models in detectron 2 with this dataset ( test with Nvidia T4).\n"}}