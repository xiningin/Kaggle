{"cell_type":{"a17395eb":"code","ed3a367a":"code","dc6d9779":"code","95c2be98":"code","925dd7a9":"code","0757f03f":"code","0a5a9f12":"code","4d4990dc":"code","19f9bb24":"code","d50a3bdd":"code","b8d17096":"code","6e8e89b5":"code","4e811da7":"code","5511ab8f":"code","d477d583":"code","6fc4a95d":"code","28cab5d2":"code","ffd0b067":"code","0218dbb0":"code","aa3ca2c4":"code","3aaae695":"code","ec97a8b2":"code","e789dec4":"code","8db359cf":"markdown","35809f76":"markdown","78e4f449":"markdown","744536c6":"markdown","b95e7d01":"markdown","a7c6738d":"markdown","42b99f2f":"markdown","45df734a":"markdown","bf8b8e52":"markdown","0ad4692c":"markdown"},"source":{"a17395eb":"!pip install -q sumeval==0.2.2\n!pip install -q nlpaug==1.1.3\n!pip install -q simpletransformers==0.60.9","ed3a367a":"!pip3 install cudf","dc6d9779":"import gc\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\n\nimport nlpaug.augmenter.word as naw\nfrom sumeval.metrics.rouge import RougeCalculator\n\nimport torch\nfrom simpletransformers.t5 import T5Model, T5Args\n\nprint('Pytorch version: %s'  % torch.__version__)","95c2be98":"warnings.simplefilter('ignore')\npd.set_option('display.max_colwidth', 10000)\ncuda =  torch.cuda.is_available()","925dd7a9":"import cudf\n","0757f03f":"df = pd.read_csv('..\/input\/news-summary\/news_summary.csv', encoding='ISO-8859-1').dropna().reset_index(drop=True)\nmore_df = pd.read_csv('..\/input\/news-summary\/news_summary_more.csv', encoding='ISO-8859-1')","0a5a9f12":"import gc\ngc.collect()","4d4990dc":"display(df.head(1))\ndisplay(more_df.head(1))","19f9bb24":"df['headlines_length'] = [len(df['headlines'][i]) for i in range(len(df))]\ndf['text_length'] = [len(df['text'][i]) for i in range(len(df))]\ndf['ctext_length'] = [len(df['ctext'][i]) for i in range(len(df))]\n\nmore_df['headlines_length'] = [len(more_df['headlines'][i]) for i in range(len(more_df))]\nmore_df['text_length'] = [len(more_df['text'][i]) for i in range(len(more_df))]","d50a3bdd":"print('df headlines length:\\n', df['headlines_length'].describe())\nprint()\nprint('more_df headlines length:\\n', more_df['headlines_length'].describe())","b8d17096":"print('df text length:\\n', df['text_length'].describe())\nprint()\nprint('df ctext length:\\n', df['ctext_length'].describe())\nprint()\nprint('more_df text length:\\n', more_df['text_length'].describe())","6e8e89b5":"df = df.drop(['author', 'date', 'read_more', 'ctext','headlines_length', 'text_length', 'ctext_length'], axis=1)\nmore_df = more_df.drop(['headlines_length', 'text_length'], axis=1)\ndf = pd.concat([df, more_df]).reset_index(drop=True)","4e811da7":"# https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial\n\nall_words = df['text'].str.split(expand=True).unstack().value_counts()\n \ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Jet',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 Word frequencies in the dataset'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\nfig.show()","5511ab8f":"wc = WordCloud(width=900, height=600)\n\nwc.generate(','.join(df['headlines']))\nplt.figure(figsize=(18,13))\nplt.imshow(wc)\nplt.axis('off')\nplt.title('headlines word cloud', fontdict={'fontsize': 20})\n\nwc.generate(','.join(df['text']))\nplt.figure(figsize=(18,13))\nplt.imshow(wc)\nplt.axis('off')\nplt.title('text word cloud', fontdict={'fontsize': 20})\n\nplt.show()","d477d583":"df = df.rename(columns={'text': 'input_text', 'headlines': 'target_text'}).reindex(columns=['input_text', 'target_text'])\ndf['prefix'] = ''\n\ntrain, test = train_test_split(df, test_size=0.2, random_state=42)\ntrain, valid = train_test_split(train, test_size=0.2, random_state=42)","6fc4a95d":"aug = naw.SynonymAug(aug_src='wordnet')\naugmented_text = aug.augment(list(train['input_text'].head(1)))\nprint(\"Original:\")\nprint(','.join(train.head(1)['input_text'].values))\nprint()\nprint(\"Augmented Text:\")\nprint(','.join(augmented_text))","28cab5d2":"train = pd.concat([\n    train,\n    pd.DataFrame({'input_text': naw.SynonymAug(aug_src='wordnet').augment(list(train['input_text'])),\n                  'target_text': list(train['target_text']),\n                  'prefix': ''}),\n                  ])","ffd0b067":"import gc\ngc.collect()","0218dbb0":"train_params = {\n    'max_seq_length': 512,\n    'max_length': 128,\n    'train_batch_size': 8,\n    'eval_batch_size': 8,\n    'num_train_epochs': 2,\n    'evaluate_during_training': True,\n    'evaluate_during_training_steps': 10000,\n    'use_multiprocessing': False,\n    'fp16': False,\n    'save_steps': -1,\n    'save_eval_checkpoints': False,\n    'save_model_every_epoch': False,\n    'no_cache': True,\n    'reprocess_input_data': True,\n    'overwrite_output_dir': True,\n    'preprocess_inputs': False,\n    'num_return_sequences': 1 \n}\n\nmodel = T5Model('t5', 't5-small', args=train_params, use_cuda=cuda)\nmodel.train_model(train, eval_data=valid)\ngc.collect()","aa3ca2c4":"pred_params = {\n        'max_seq_length': 512,\n        'use_multiprocessed_decoding': False\n        }\n\nmodel = T5Model('t5', 'outputs\/best_model', args=pred_params, use_cuda=cuda) \npred = model.predict(list(test['input_text']))","3aaae695":"random.sample(pred, 5)","ec97a8b2":"rouge = RougeCalculator(stopwords=True, lang=\"en\")\n\ndef rouge_calc(preds, targets):\n    rouge_1 = [rouge.rouge_n(summary=preds[i],references=targets[i],n=1) for i in range(len(preds))]\n    rouge_2 = [rouge.rouge_n(summary=preds[i],references=targets[i],n=2) for i in range(len(preds))]\n    rouge_l = [rouge.rouge_l(summary=preds[i],references=targets[i]) for i in range(len(preds))]\n\n    return {\"Rouge_1\": np.array(rouge_1).mean(),\n            \"Rouge_2\": np.array(rouge_2).mean(),\n            \"Rouge_L\": np.array(rouge_l).mean()}","e789dec4":"rouge_calc(pred, list(test['target_text']))","8db359cf":"## Import libraries <a class=\"anchor\" id=\"Import-libraries\"><\/a>","35809f76":"## EDA <a class=\"anchor\" id=\"EDA\"><\/a>","78e4f449":"# Text summarization with Simple Transformers T5\n\nIn this notebook, we implement a news article summarization task with T5, \n\nusing the news summary dataset published by [Kondalarao Vonteru].\n\n[Kondalarao Vonteru]: https:\/\/www.kaggle.com\/sunnysai12345","744536c6":"## Data Augmentation <a class=\"anchor\" id=\"Data-Augmentation\"><\/a>\n\n<img src=\"https:\/\/github.com\/makcedward\/nlpaug\/blob\/master\/res\/logo_small.png?raw=true\" style=\"height: 300px; width: 300px;  object-position: 0px;\"\/>\n\nUse nlpaug to apply data augmentation.\n\n[Document]\n\n[Document]: https:\/\/nlpaug.readthedocs.io\/en\/latest\/\n\n[Github]\n\n[Github]: https:\/\/github.com\/makcedward\/nlpaug","b95e7d01":"### Training","a7c6738d":"## Build the model <a class=\"anchor\" id=\"Build-the-model\"><\/a>\n\n<img src=\"https:\/\/repository-images.githubusercontent.com\/212747520\/6ef26800-0982-11ea-8476-80e5c7b4d3c4\" style=\"height: 250px; width: 500px;  object-position: 0px;\"\/>\n\nWe will use simple transformers to build the model.\n\n* [Document]\n\n[Document]: https:\/\/simpletransformers.ai\/\n\n* [Github]\n\n[Github]: https:\/\/github.com\/ThilinaRajapakse\/simpletransformers","42b99f2f":"After applying data augmentation, we will combine them with the original train data.","45df734a":"## Evaluation of the model <a class=\"anchor\" id=\"Evaluation-of-the-model\"><\/a>\n\n<img src=\"https:\/\/raw.githubusercontent.com\/chakki-works\/sumeval\/master\/doc\/top.png\" style=\"height: 150px; width: 600px;  object-position: 0px;\"\/>\n\nEvaluate the model performance with the [sumeval]'s Rouge score.\n\n[sumeval]: https:\/\/github.com\/chakki-works\/sumeval\n\nRouge1: Evaluate the generated text in units of bi-grams.\n\nRouge2: Evaluate the generated text in units of uni-grams.\n\nRougeL: Evaluate the match of the generated text sequence.","bf8b8e52":"## Content\n\n* [Import libraries](#Import-libraries)\n\n* [EDA](#EDA)\n\n* [Data Augmentation](#Data-Augmentation)\n\n* [Build the model](#Build-the-model)\n\n* [Evaluation of the model](#Evaluation-of-the-model)","0ad4692c":"### Predict"}}