{"cell_type":{"cfc292b7":"code","4efe2010":"code","d3188d9d":"code","b16a3119":"code","1fb5ff42":"code","dd7d2e93":"code","4ad580dd":"code","e729f9a7":"code","f92bedb3":"code","65060ee9":"code","2fde8efd":"code","baad7c30":"code","7557a8af":"code","47537dd3":"code","7fe82a3d":"code","79d7bded":"markdown","93f28538":"markdown","6d8e58f2":"markdown","39c09e0c":"markdown","051e0ffc":"markdown"},"source":{"cfc292b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4efe2010":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain.shape, test.shape","d3188d9d":"train.head()","b16a3119":"train.isna().sum().sort_values()","1fb5ff42":"for col in ['Alley','FireplaceQu','Fence','MiscFeature','PoolQC']:\n    train[col].fillna('NA', inplace=True)\n    test[col].fillna('NA', inplace=True)\n    \ntrain['LotFrontage'].fillna(train[\"LotFrontage\"].value_counts().to_frame().index[0], inplace=True)\ntest['LotFrontage'].fillna(test[\"LotFrontage\"].value_counts().to_frame().index[0], inplace=True)\n\ntrain[['GarageQual','GarageFinish','GarageYrBlt','GarageType','GarageCond']].isna().head(7)\nfor col in ['GarageQual','GarageFinish','GarageYrBlt','GarageType','GarageCond']:\n    train[col].fillna('NA',inplace=True)\n    test[col].fillna('NA',inplace=True)\n\nfor col in ['BsmtQual','BsmtCond','BsmtFinType1','BsmtFinType2','BsmtExposure']:\n    train[col].fillna('NA',inplace=True)\n    test[col].fillna('NA',inplace=True)\n\ntrain['Electrical'].fillna('SBrkr',inplace=True)\n\nmissings = ['GarageCars','GarageArea','KitchenQual','Exterior1st','SaleType','TotalBsmtSF','BsmtUnfSF','Exterior2nd',\n            'BsmtFinSF1','BsmtFinSF2','BsmtFullBath','Functional','Utilities','BsmtHalfBath','MSZoning']\n\nnumerical=['GarageCars','GarageArea','TotalBsmtSF','BsmtUnfSF','BsmtFinSF1','BsmtFinSF2','BsmtFullBath','BsmtHalfBath']\ncategorical = ['KitchenQual','Exterior1st','SaleType','Exterior2nd','Functional','Utilities','MSZoning']\n\n# using Imputer class of sklearn libs.\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy='median',axis=0)\nimputer.fit(test[numerical] + train[numerical])\ntest[numerical] = imputer.transform(test[numerical])\ntrain[numerical] = imputer.transform(train[numerical])\n\nfor i in categorical:\n    train[i].fillna(train[i].value_counts().to_frame().index[0], inplace=True)\n    test[i].fillna(test[i].value_counts().to_frame().index[0], inplace=True)    \n\ntrain[train['MasVnrType'].isna()][['SalePrice','MasVnrType','MasVnrArea']]\n\ntrain[train['MasVnrType']=='None']['SalePrice'].median()\ntrain[train['MasVnrType']=='BrkFace']['SalePrice'].median()\ntrain[train['MasVnrType']=='Stone']['SalePrice'].median()\ntrain[train['MasVnrType']=='BrkCmn']['SalePrice'].median()\n\ntrain['MasVnrArea'].fillna(181000,inplace=True)\ntest['MasVnrArea'].fillna(181000,inplace=True)\n\ntrain['MasVnrType'].fillna('NA',inplace=True)\ntest['MasVnrType'].fillna('NA',inplace=True)\n\nprint(train.isna().sum().sort_values()[-2:-1])\nprint(test.isna().sum().sort_values()[-2:-1])","dd7d2e93":"int64 =[]\nobjects = []\nfor col in train.columns.tolist():\n    if np.dtype(train[col]) == 'int64' or np.dtype(train[col]) == 'float64':\n        int64.append(col)\n    else:\n        objects.append(col)                      #here datatype is 'object'\n\ncontinues_int64_cols = ['LotArea', 'LotFrontage', 'MasVnrArea','BsmtFinSF2','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF',\n                  'GrLivArea','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']\ncategorical_int64_cols=[]\nfor i in int64:\n    if i not in continues_int64_cols:\n        categorical_int64_cols.append(i)\n\ndef barplot(X,Y):\n    plt.figure(figsize=(7,7))\n    sns.barplot(x=X, y=Y)\n    plt.show()\ndef scatter(X,Y):\n    plt.figure(figsize=(7,7))\n    sns.scatterplot(alpha=0.4,x=X, y=Y)\n    plt.show()\ndef hist(X):\n    plt.figure(figsize=(7,7))\n    sns.distplot(X, bins=40, kde=True)\n    plt.show()\ndef box(X):\n    plt.figure(figsize=(3,7))\n    sns.boxplot(y=X)\n    plt.show() \ndef line(X,Y):\n    plt.figure(figsize=(7,7))    \n    sns.lineplot(x=X, y=Y,color=\"coral\")\n    plt.show() \n\ntrain['MasVnrArea'] = train['MasVnrArea'].apply(lambda row: 1.0 if row>0.0 else 0.0)\ntrain['BsmtFinSF2'] = train['BsmtFinSF2'].apply(lambda row: 1.0 if row>0.0 else 0.0)\nbinary_cate_int64_cols = []\nbinary_cate_int64_cols.append('MasVnrArea')\nbinary_cate_int64_cols.append('BsmtFinSF2')\n\ntrain['LowQualFinSF'] = train['LowQualFinSF'].apply(lambda row: 1.0 if row>0.0 else 0.0)\nbinary_cate_int64_cols.append('LowQualFinSF')\n\nfor i in continues_int64_cols[14:]:\n    train[i] = train[i].apply(lambda row: 1.0 if row>0.0 else 0.0)\n    binary_cate_int64_cols.append(i)\n\nfor j in binary_cate_int64_cols:\n    if j in continues_int64_cols:\n        continues_int64_cols.remove(j)        #these special columns removing from the continues_int64_cols\n\n# we changed values of train only, here for test set\nfor i in binary_cate_int64_cols:\n    test[i] = test[i].apply(lambda row: 1.0 if row>0.0 else 0.0)\n# test[binary_cate_int64_cols].head(6)\n\nordinal_categorical_cols =[]\nordinal_categorical_cols.extend(['ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','HeatingQC','KitchenQual'])\nordinal_categorical_cols.extend(['FireplaceQu', 'GarageQual','GarageCond','PoolQC'])\n\nfor i in ordinal_categorical_cols:\n    if i in objects:\n        objects.remove(i)            # removing ordinal features from the objects\n\n# removinf 'Id' and 'SalePrice'\ncategorical_int64_cols.remove('Id')\ncategorical_int64_cols.remove('SalePrice')\n\ntrain['BsmtExposure'] = train['BsmtExposure'].map({'Gd':4, 'Av':3, 'Mn':2, 'No':1,'NA':0})\ntest['BsmtExposure'] = test['BsmtExposure'].map({'Gd':4, 'Av':3, 'Mn':2, 'No':1,'NA':0})\n\norder = {'Ex':5,\n        'Gd':4, \n        'TA':3, \n        'Fa':2, \n        'Po':1,\n        'NA':0 }\nfor i in ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']:\n    train[i] = train[i].map(order)\n    test[i] = test[i].map(order)\n# test[ordinal_categorical_cols].head()   \n\ntrain_objs_num = len(train)\ndataset=pd.concat(objs=[train[categorical_int64_cols+objects+ordinal_categorical_cols],test[categorical_int64_cols+objects+ordinal_categorical_cols]],axis=0)\ndataset_preprocessed = pd.get_dummies(dataset.astype(str), drop_first=True)\ntrain_nominal_onehot = dataset_preprocessed[:train_objs_num]\ntest_nominal_onehot= dataset_preprocessed[train_objs_num:]\n# train_nominal_onehot.shape, test_nominal_onehot.shap      \n\nX = pd.concat([train[continues_int64_cols], train[binary_cate_int64_cols], train_nominal_onehot], axis=1)\ny = train['SalePrice']\ntest_final = pd.concat([test[continues_int64_cols], test[binary_cate_int64_cols], test_nominal_onehot], axis=1)\n              \nX.shape, y.shape, test_final.shape","4ad580dd":"X.head()","e729f9a7":"from sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor","f92bedb3":"def xgb_model(X, y, test_final, param, kfolds = 5):\n    \n    folds = KFold(n_splits=kfolds, shuffle=True, random_state=4590)\n    \n    feature_importance = np.zeros((X.shape[1],kfolds))\n    val_matrix = np.zeros(len(X))\n    pred_matrix = np.zeros(len(test_final))\n\n    for fold_, (train_id, val_id) in enumerate(folds.split(X,X)):\n        print('--------------Fold-no---',fold_)\n        x0,y0 = X.iloc[train_id], y[train_id]\n        x1,y1 = X.iloc[val_id], y[val_id]\n        \n        train_data = xgb.DMatrix(x0, y0)\n        val_data = xgb.DMatrix(x1, y1)\n\n        num_round = 10000\n        watchlist = [(train_data, 'train'), (val_data, 'valid')]\n        \n        clf = xgb.train(param, train_data, num_round, watchlist, maximize=False, \n                                            verbose_eval=3000, early_stopping_rounds = 70)\n\n        val_matrix[val_id] = clf.predict(val_data, ntree_limit = clf.best_ntree_limit)\n#         feature_importance[:, fold_] = clf.feature_importance()\n#         feature_importance[:, fold_] = clf.best_estimator_.booster().get_fscore()\n        \n        testD = xgb.DMatrix(test_final)\n        pred_matrix += clf.predict(testD, ntree_limit = clf.best_ntree_limit) \/ folds.n_splits\n \n    print(' Validation error: ',np.sqrt(mean_squared_error(val_matrix, y)))\n    return clf, pred_matrix","65060ee9":"params = {\n      'objective': 'reg:squarederror', \n      'eval_metric': 'rmse',\n      'eta': 0.001,\n       'gamma':10,\n      'reg_alpha': 0.3,\n      'max_depth': 4, \n      'subsample': 0.8,\n       'scale_pos_weight': 1,\n       'max_bin':256,\n#       'best_score': 0.5,\n      'tweedie_variance_power': 1.5,\n      'colsample_bytree': 1,\n      'alpha':0.01,\n#        'lambda':90,\n       'min_child_weight': 5,\n      'random_state': 42, \n      'silent': False}\nmodel, predictions = xgb_model(X ,y , test_final,param = params, kfolds=5)\nprint(\"XGBM Training Completed...\")","2fde8efd":"feature_importance = model.get_score(importance_type='gain')\nlen(feature_importance), feature_importance","baad7c30":"features = list(feature_importance.keys())\nfeatures[:4]","7557a8af":"gridParams = {\n      'objective': ['reg:squarederror'], \n      'eval_metric': ['rmse'],\n      'eta': [0.001],\n       'gamma':[10,20],\n      'reg_alpha': [0.3],\n      'max_depth': [4,8], \n      'subsample': [0.8],\n       'scale_pos_weight': [1],\n       'max_bin':[256],\n      'tweedie_variance_power': [1.5],\n      'colsample_bytree': [0.6,0.8,1],\n       'num_round' : [8000],\n      'alpha':[0.03,0.01,0.005],\n       'min_child_weight': [5,7,9],\n      'random_state': [42], \n      'silent': [False]}\n\nclf = xgb.XGBRegressor()\n\n# Create the grid\ngrid = GridSearchCV(clf, gridParams,\n                    verbose=0,\n                    cv=3,\n                    n_jobs=-1)\n# Run the grid\ngrid.fit(X[features], y)\n\n# Print the best parameters found\nprint(grid.best_params_)\nprint(grid.best_score_)","47537dd3":"best_params = grid.best_params_\nmodel, predictions = xgb_model(X[features] ,y , test_final[features] ,param = best_params, kfolds=7)\nprint(\"XGBM Training Completed...\")","7fe82a3d":"submission = pd.DataFrame()\nsubmission['Id'] = test['Id']\nsubmission['SalePrice'] = predictions\nsubmission.to_csv('submission_xgb_gridsearch.csv',index=False)\nsubmission.head()","79d7bded":"# 1) Data Cleaning","93f28538":"I made a separate kernels for feature extraction, visualization of the features, you can see here : [house-price-feature-extraction-strategy](https:\/\/www.kaggle.com\/ashishbarvaliya\/house-price-feature-extraction-strategy), here i am not gonna repeat.","6d8e58f2":"# 2) Feature extraction","39c09e0c":"I made a separate kernels for understanding NaNs and choosing best filling NaNs strategy, you can see here : [house-price-fillna-strategy](http:\/\/www.kaggle.com\/ashishbarvaliya\/house-price-fillna-strategy), here i am not gonna repeat.","051e0ffc":"# 3) modeling"}}