{"cell_type":{"49cef387":"code","5355c805":"code","d3873f39":"code","aa93ad8b":"code","c5eb1ab9":"code","dad65620":"code","9e173d44":"code","077cbea6":"code","2a655205":"code","32ee4bda":"code","432cd68f":"code","47b85b59":"code","b1c00111":"code","e459d65c":"code","8656ec78":"code","2be12117":"code","3ea197db":"code","94c96507":"code","011bde58":"code","8d03db68":"code","a6b7448b":"code","d37ba86a":"code","bb4d360b":"code","3b4ac59c":"code","552e924d":"code","fd079b14":"code","76025ea4":"code","7378e98e":"code","64189dd8":"code","b880bdea":"code","799bfc03":"code","421be04c":"code","062cd430":"markdown","ac1d01fa":"markdown","6d66d370":"markdown","2c974d7b":"markdown","ce1c3bac":"markdown","5c5534b1":"markdown","6bcca213":"markdown","f829c5c5":"markdown","73191567":"markdown","82ece0c6":"markdown","19ef8be5":"markdown","28aaec37":"markdown","a91cf97a":"markdown","ebb83a66":"markdown","b2372d78":"markdown","b7e917ff":"markdown","96645127":"markdown","3d11deca":"markdown","6cdef5f7":"markdown","8f6b98bc":"markdown","5ed38ffc":"markdown","adfed817":"markdown","102318d1":"markdown"},"source":{"49cef387":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5355c805":"data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\nprint(data.head())\ntest_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')\nprint(test_data.head())\n","d3873f39":"import torch\nfrom torch import nn\nimport re\nimport nltk\n","aa93ad8b":"\ndef _preprocess(list_sentence):\n    \n    # remove URL\n    \n    URL = r'http\\S+'\n    for i in range(len(list_sentence)):\n        list_sentence[i] = re.sub(URL, ' ', list_sentence[i])\n        \n    # remove emoji, icon, symbol\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    for i in range(len(list_sentence)):\n        list_sentence[i] = emoji_pattern.sub(r' ', list_sentence[i])\n        \n    # remove punctuation and number\n    \n    punc = r'[0-9\\.\\,\\?\\!\\(\\)\\*\\^\\\"\\;\\:\\<\\>\\\/\\\\\\-\\+\\=]+'\n    for i in range(len(list_sentence)):\n        list_sentence[i] = re.sub(punc, ' ', list_sentence[i])\n    \n    # remove duplicate space\n    list_set = []\n    for sen in list_sentence:\n        list_set.append(sen.split(' '))\n\n    print('step 1 completed')\n    list_set1 = []\n    for i in range(len(list_set)):\n        _set = []\n        for j in range(len(list_set[i])):\n            if list_set[i][j] != '':\n                _set.append(list_set[i][j])\n        list_set1.append(_set)\n\n    list_set = list_set1.copy()\n    list_sentence = []\n\n    for _set in list_set:\n        list_sentence.append(' '.join(_set))\n    \n    # lower sentence\n    \n    for i in range(len(list_sentence)):\n        list_sentence[i] = list_sentence[i].lower()\n    # segment word\n    \n    list_seq = []\n    for sen in list_sentence:\n        list_seq.append(nltk.word_tokenize(sen))\n    \n    return list_seq","c5eb1ab9":"def count_word(list_seq):\n    list_vocab = []\n    for seq in list_seq:\n        list_vocab.extend(seq)\n    list_vocab = sorted(set(list_vocab))\n    fre_word = {}\n    for word in list_vocab:\n        fre_word[word] = 0\n    for seq in list_seq:\n        for word in seq:\n            fre_word[word] += 1\n    return fre_word\n\n\ndef create_vocab(fre_word, vocab_size=70000):\n    sort_word = sorted(fre_word, key=fre_word.get)\n    dict_word = {}\n    dict_word['[unk]'] = 0\n    index = 1\n    for i in range(len(sort_word) - vocab_size + 1, len(sort_word)):\n        dict_word[sort_word[i]] = index\n        index += 1\n    return dict_word\n\n\ndef index(word, dict_word):\n    if word in dict_word.keys():\n        return dict_word[word]\n    return dict_word['[unk]']\n\ndef word_to_index(list_seq, dict_word):\n    list_index = []\n    for seq in list_seq:\n        set_index = []\n        for word in seq:\n            set_index.append(index(word, dict_word))\n        list_index.append(set_index)\n    return list_index","dad65620":"def remove_empty(list_seq, list_label):\n    new_seqs = []\n    new_labels = []\n    for i in range(len(list_seq)):\n        if len(list_seq[i]) > 0:\n            new_seqs.append(list_seq[i])\n            new_labels.append(list_label[i])\n\n    return [new_seqs, new_labels]","9e173d44":"\n\n\nlist_sentence = []\nfor i in range(len(data)):\n    list_sentence.append(str(data['question_text'].loc[i]))\n\n\nlist_label = []\nfor i in range(len(data)):\n    list_label.append(float(data['target'].loc[i]))\n    \ntest_sentence = []\nfor i in range(len(test_data)):\n    test_sentence.append(str(test_data['question_text'].loc[i]))","077cbea6":"import matplotlib.pyplot as plt\ncount0 = 0\ncount1 = 0\nfor label in list_label:\n    if int(label) == 0:\n        count0 += 1\n    else: count1 += 1\nprint('So luong nhan 0 la ', count0)\nprint('So luong nhan 1 la ', count1)\n\nleft = [1, 2]\n\n# heights of bars\nheight = [count0, count1]\n \n# labels for bars\ntick_label = [0, 1]\n \n# plotting a bar chart\nplt.bar(left, height, tick_label = tick_label,\n        width = 0.8, color = ['red', 'green'])\n \n# naming the x-axis\nplt.xlabel('label')\n# naming the y-axis\nplt.ylabel('the quantities')\n# plot title\nplt.title('Number each label in the orgin dataset')\n \n# function to show the plot\nplt.show()","2a655205":"\nimport math\n#initialize transformer class\nclass self_transformer(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super(self_transformer, self).__init__()\n            self.linear1 = nn.Linear(input_dim, output_dim)\n            self.linear2 = nn.Linear(input_dim, output_dim)\n            self.linear3 = nn.Linear(input_dim, output_dim)\n            self.softmax = nn.Softmax(-1)\n        def forward(self, _input, output_dim):\n            question = self.linear1(_input) # 30x64\n            key = self.linear2(_input) # 30x64\n            value = self.linear3(_input) # 30x64\n            self_attention_ = torch.matmul(key, torch.transpose(question, - 2, -1))\/math.sqrt(output_dim)# 30x30\n            self_attention = self.softmax(self_attention_) # 30x30\n            # print(self_attention.shape)\n            self_output = torch.matmul(self_attention, value)\n            # print(self_output.shape)\n            return self_output\n\nclass multi_head_transformer(nn.Module):\n    def __init__(self, input_dim, output_dim, num_class):\n        super(multi_head_transformer, self).__init__()\n        self.linear = []\n        for i in range(num_class):\n            self.linear.append(nn.Linear(input_dim, output_dim))\n    def forward(self, multi,output_dim):\n        result = 0.0\n        for i in range(len(multi)):\n            result += self.linear[i](multi[i])\n        result = torch.flatten(result, start_dim=-2)\n        result = result.mean(-1)\n        result = torch.sigmoid(result)\n        return result\n\n\ndef transform(_input, model_1, model_2, output_dim_encode,output_dim_decode):\n    encoders = []\n    for i in range(0, 8):\n        encoders.append(model_1[i](_input, output_dim_encode))\n    decoder = model_2(encoders, output_dim_decode)\n    return decoder\n\n","32ee4bda":"\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch import nn\nimport random\nimport math\nfrom torch.utils.data import *\n\n\n#initialize some parameter for padding\n\ndef padding(list_seq, max_len):\n    return torch.from_numpy(pad_sequences(sequences=list_seq, maxlen=max_len))\n\n\n#processing pads into input before putting in transformer models\n\n\ndef create_pos(length, embed_dim=24):\n    position_encode = torch.zeros((length, embed_dim))\n    for i in range(length):\n        for j in range(embed_dim):\n            if j % 2 == 1:\n                position_encode[i][j] = math.sin(float(i+1)\/10000**(float(j+1)\/embed_dim))\n            else:\n                position_encode[i][j] = math.cos(float(i+1)\/10000**(float(j)\/embed_dim))\n    return position_encode\n\n\ndef process_data_to_input(data, embed_model, max_len):\n    return embed_model(data) + create_pos(max_len)\n \n \ndef create_transform_model(vocab_size, embed_dim):    \n    model_1 = []\n    for i in range(8):\n        model_1.append(self_transformer(input_dim=embed_dim, output_dim = 24))\n    model_2 = multi_head_transformer(input_dim = 24, output_dim=embed_dim, num_class=8)\n    embed_model = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n    \n    criterion = nn.L1Loss()\n    \n\n\n\n    optimizer = torch.optim.Adam([{'params':model_1[0].parameters()}, \n                                {'params':model_1[1].parameters()}, \n                                {'params':model_1[2].parameters()}, \n                                {'params':model_1[3].parameters()}, \n                                {'params':model_1[4].parameters()}, \n                                {'params':model_1[5].parameters()}, \n                                {'params':model_1[6].parameters()}, \n                                {'params':model_1[7].parameters()}, \n                                {'params':embed_model.parameters()},\n                                {'params':model_2.parameters()}], lr=0.00108)\n    \n    return [model_1, model_2, embed_model, criterion, optimizer]\n\n\n\n\ndef create_dataset(_input, label):\n    return [TensorDataset(_input, label), label.shape[0]]\n    # processed data\n\ndef _create_dataset(feature, label):\n    po_feature = []\n    ne_feature = []\n    po_label = []\n    ne_label = []\n    for i in range(feature.shape[0]):\n        if int(label[i]) == 0:\n            ne_feature.append(feature[i].unsqueeze(0))\n            ne_label.append(label[i].unsqueeze(0))\n        else:\n            po_feature.append(feature[i].unsqueeze(0))\n            po_label.append(label[i].unsqueeze(0))\n    min_count = min(len(ne_label), len(po_label))\n    print(min_count)\n\n    random_po = random.sample(range(0, len(po_label)), min_count)\n    random_ne = random.sample(range(0, len(ne_label)), min_count)\n\n    new_feature = []\n    new_label = []\n\n    for i in range(2*min_count):\n        if i % 2 == 0:\n            new_feature.append(po_feature[random_po[int(i\/2)]])\n            new_label.append(po_label[random_po[int(i\/2)]])\n        else:\n            new_feature.append(ne_feature[random_ne[int((i-1)\/2)]])\n            new_label.append(ne_label[random_ne[int((i-1)\/2)]])\n    new_feature = torch.cat(new_feature)\n    new_label = torch.cat(new_label)\n    print(new_feature.shape)\n    print(new_label.shape)\n\n    return [TensorDataset(new_feature, new_label), 2*min_count]\n\n\ndef split_dataset(data, data_size, percentage_attr):\n    train_size = int(data_size * percentage_attr)\n    test_size = data_size - train_size\n    input_train, input_test = random_split(data, [train_size, test_size])\n    train_loader = DataLoader(input_train.dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(input_test.dataset, batch_size=16, shuffle=False)\n    return [train_loader, test_loader]\n\ndef train_and_validation(train_loader, test_loader, model_1, model_2, optimizer, criterion, encode_dim=24, decode_dim=16, num_epochs = 10):\n    validation_acc = []\n    for epoch in range(num_epochs):\n        for i, mini_data in enumerate(train_loader):\n            inputs, labels = mini_data\n            embed_layer = process_data_to_input(inputs, embed_model, padding_size)\n            optimizer.zero_grad()\n            outputs = transform(embed_layer, model_1, model_2, output_dim_encode=encode_dim, output_dim_decode=decode_dim)\n            \n            loss = criterion(outputs, labels)\n            loss.backward(retain_graph=True)\n            optimizer.step()\n            # print('hello')\n\n            if i % 300 == 0:\n                print(loss.data)\n            \n        print('complete epoch ' + str(epoch) +' completed')\n            \n        count = 0\n        total_count = 0    \n        for i, mini_data in enumerate(test_loader):\n            \n            inputs, labels = mini_data\n            embed_layer = process_data_to_input(inputs, embed_model, padding_size)\n            outputs = transform(embed_layer, model_1, model_2, output_dim_encode=encode_dim, output_dim_decode=decode_dim)\n            for j in range(outputs.shape[0]):\n                total_count += 1\n                if abs(outputs[j].data - labels[j].data) < 0.5:\n                    count += 1\n        \n        validation_acc.append(count\/total_count)\n    predict_labels = []\n    real_labels = []\n    for i, mini_data in enumerate(test_loader):\n        \n        inputs, labels = mini_data\n        embed_layer = process_data_to_input(inputs, embed_model, padding_size)\n        outputs = transform(embed_layer, model_1, model_2, output_dim_encode=encode_dim, output_dim_decode=decode_dim)\n        for j in range(outputs.shape[0]):\n            predict_labels.append(outputs[j].data)\n            real_labels.append(labels[j].data)\n    for i in range(len(predict_labels)):\n        if predict_labels[i] > 0.5:\n            predict_labels[i] = 1\n        else:\n            predict_labels[i] = 0\n    return [validation_acc, predict_labels, real_labels]\n\n\n\n\n\n        \n        ","432cd68f":"nltk.download('punkt')","47b85b59":"list_seq = _preprocess(list_sentence)\ntest_seq = _preprocess(test_sentence)","b1c00111":"random_taken = random.sample(range(0, len(list_seq)), 5)\norgin_sentence = []\npos_preprocessed_sentence = []\nfor i in range(len(random_taken)):\n    orgin_sentence.append(list_sentence[random_taken[i]])\n    pos_preprocessed_sentence.append(list_seq[random_taken[i]])\nexample = pd.DataFrame({'Tr\u01b0\u1edbc khi x\u1eed l\u00fd': orgin_sentence})\nexample['Sau khi x\u1eed l\u00fd'] = pos_preprocessed_sentence\nexample\n\n    ","e459d65c":"all_seq = list_seq.copy()\nall_seq.extend(test_seq)","8656ec78":"list_seq, list_label = remove_empty(list_seq, list_label)\nprint('S\u1ed1 c\u00e2u c\u00f2n l\u1ea1i sau khi lo\u1ea1i b\u1ecf c\u00e1c c\u00e2u r\u1ed7ng ', len(list_seq))\nprint('S\u1ed1 nh\u00e3n c\u00f2n l\u1ea1i sau khi lo\u1ea1i b\u1ecf c\u00e1c nh\u00e3n \u1ee9ng v\u1edbi c\u00e2u r\u1ed7ng ', len(list_label))\n","2be12117":"\nfre_word = count_word(all_seq)\ndict_word = create_vocab(fre_word, vocab_size=70000)\nlist_index = word_to_index(list_seq, dict_word)\n","3ea197db":"vocab_size = 70000\nbatch_size = 16\nembed_size = 24\npadding_size = 16\npercentage_attr = 0.96","94c96507":"list_label = torch.tensor(list_label)","011bde58":"model_1, model_2, embed_model, criterion, optimizer = create_transform_model(vocab_size, embed_size)\n\n\n\n\n\n\n\n    \n\n","8d03db68":"padding_seq = padding(list_index, padding_size)","a6b7448b":"processed_data, data_size = _create_dataset(padding_seq, list_label)\n\n","d37ba86a":"\n\n\nleft = [1, 2]\n\n# heights of bars\nheight = [data_size\/2, data_size\/2]\n \n# labels for bars\ntick_label = [0, 1]\n \n# plotting a bar chart\nplt.bar(left, height, tick_label = tick_label,\n        width = 0.8, color = ['red', 'green'])\n \n# naming the x-axis\nplt.xlabel('label')\n# naming the y-axis\nplt.ylabel('the quantities')\n# plot title\nplt.title('Number each label in the updated dataset')\n \n# function to show the plot\nplt.show()","bb4d360b":"train_loader, test_loader = split_dataset(processed_data, data_size,percentage_attr)\n","3b4ac59c":"acc, predict_labels, real_labels = train_and_validation(train_loader, test_loader, model_1, model_2, optimizer, criterion)\nprint('Accuracy c\u1ee7a m\u00f4 h\u00ecnh tr\u00ean t\u1eadp validation l\u00e0 ', acc[len(acc) - 1], '\\n')\ntp = [0.0, 0.0]\npo = [0.0, 0.0]\nfn = [0.0, 0.0]\np_score = [0.0, 0.0]\nr_score = [0.0, 0.0]\nf1_score = [0.0, 0.0]\nfor label in range(2):\n    for i in range(len(predict_labels)):\n        if int(predict_labels[i]) == label:\n            po[label] += 1\n            if int(predict_labels[i]) == int(real_labels[i]):\n                tp[label] += 1\n        else:\n            if int(predict_labels[i]) != int(real_labels[i]):\n                fn[label] += 1\n    p_score[label] = tp[label] \/ (po[label] + 1e-4)\n    r_score[label] = tp[label] \/ (tp[label] + fn[label] + 1e-4)\n    f1_score[label] = 2 * p_score[label] * r_score[label] \/ (p_score[label] + r_score[label])\nprint('V\u1edbi nh\u00e3n 0:\\n\u0110i\u1ec3m precision l\u00e0 ', p_score[0], '\\n\u0110i\u1ec3m recall l\u00e0 ', r_score[0], '\\n\u0110i\u1ec3m f1 la ', f1_score[0], '\\n')\nprint('V\u1edbi nh\u00e3n 1:\\n\u0110i\u1ec3m precision l\u00e0 ', p_score[1], '\\n\u0110i\u1ec3m recall l\u00e0 ', r_score[1], '\\n\u0110i\u1ec3m f1 la ', f1_score[1], '\\n')","552e924d":"test_index = word_to_index(test_seq, dict_word)","fd079b14":"padding_test = padding(test_index, padding_size)","76025ea4":"padding_test = process_data_to_input(padding_test, embed_model, padding_size)","7378e98e":"def predict(features, model_1, model_2, optimizer, criterion, encode_dim=24, decode_dim=16):\n    list_label = []\n    for i in range(features.shape[0]):\n        list_label.append( transform(features[i], model_1, model_2, output_dim_encode=encode_dim, output_dim_decode=decode_dim).data)\n    result = []\n    for i in range(len(list_label)):\n        if list_label[i] < 0.5:\n            result.append(0)\n        else: result.append(1)\n    return result","64189dd8":"result = predict(padding_test, model_1, model_2, optimizer, criterion, encode_dim=24, decode_dim=16)","b880bdea":"print(result[0:100])","799bfc03":"submit_data = pd.DataFrame({'qid': test_data['qid'].values})","421be04c":"submit_data['prediction'] = result\nsubmit_data.to_csv('submission.csv', index=False)\nsubmit_data","062cd430":"### 3. Thi\u1ebft l\u1eadp c\u00e1c h\u00e0m:\n* X\u1eed l\u00fd d\u1eef li\u1ec7u\n* T\u1ea1o dictionary\n* Kh\u1edfi t\u1ea1o m\u00f4 h\u00ecnh\n* Chuy\u1ec3n \u0111\u1ed5i d\u1eef li\u1ec7u th\u00e0nh dataset\n* Hu\u1ea5n luy\u1ec7n v\u00e0 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh","ac1d01fa":"#### **H\u00e0m th\u1ef1c hi\u1ec7n ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u, bao g\u1ed3m:**\n* Lo\u1ea1i b\u1ecf s\u1ed1  \n* Lo\u1ea1i b\u1ecf c\u00e1c d\u1ea5u c\u00e2u\n* Lo\u1ea1i b\u1ecf URL  \n* Lo\u1ea1i b\u1ecf emoji, symbol, icon  \n* Chuy\u1ec3n t\u1ea5t c\u1ea3 c\u00e1c ch\u1eef vi\u1ebft hoa v\u1ec1 vi\u1ebft th\u01b0\u1eddng  \n* T\u00e1ch t\u1eeb, s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n nltk","6d66d370":"### 1. M\u00f4 t\u1ea3 b\u00e0i to\u00e1n v\u00e0 \u0111\u1ecdc d\u1eef li\u1ec7u","2c974d7b":"#### Kh\u1edfi t\u1ea1o c\u00e1c tham s\u1ed1 c\u1ea7n thi\u1ebft, v\u1edbi:  \n* vocab_size: K\u00edch th\u01b0\u1edbc t\u1eeb v\u1ef1ng\n* batch_size: K\u00edch th\u01b0\u1edbc mini batch  \n* embed_size: K\u00edch th\u01b0\u1edbc nh\u00fang t\u1eeb \n* padding_size: K\u00edch th\u01b0\u1edbc padding, c\u00e1c c\u00e2u s\u1ebd \u0111\u01b0\u1ee3c c\u0103n ch\u1ec9nh v\u1ec1 \u0111\u1ed9 d\u00e0i c\u1ed1 \u0111\u1ecbnh \u0111\u00fang b\u1eb1ng n\u00f3  \n* percentage_attr: T\u1ec9 l\u1ec7 l\u01b0\u1ee3ng d\u1eef li\u1ec7u l\u00e0m training v\u00e0 validating l\u1ea5y ra t\u1eeb t\u1eadp data g\u1ed1c\n","ce1c3bac":"#### Ti\u1ec1n h\u00e0nh \u0111\u1ecbnh l\u01b0\u1ee3ng ph\u00e2n b\u1ed1 s\u1ed1 l\u01b0\u1ee3ng c\u00e1c c\u00e2u theo c\u00e1c nh\u00e3n t\u01b0\u01a1ng \u1ee9ng trong t\u1eadp train.csv","5c5534b1":"#### Now turn to predicting label for test.csv, the result is print in submission.csv","6bcca213":"#### T\u1ea1o c\u00e1c h\u00e0m \u0111\u1ec3 ti\u1ebfn h\u00e0nh chuy\u1ec3n c\u00e1c feature, label th\u00e0nh c\u00e1c dataset \u0111\u1ec3 ti\u1ebfn h\u00e0nh \u0111\u01b0a v\u00e0o m\u00f4 h\u00ecnh, trong \u0111\u00f3:\n* H\u00e0m padding ti\u1ebfn h\u00e0nh \u0111\u01b0a c\u00e1c features v\u1ec1 c\u00f9ng k\u00edch c\u1ee1 chi\u1ec1u d\u00e0i  \n* H\u00e0m create_pos t\u1ea1o ma tr\u1eadn position encoding, gi\u00fap bi\u1ec3u th\u1ecb \u00fd ngh\u0129a v\u1ec1 v\u1ecb tr\u00ed c\u00e1c t\u1eeb trong c\u00e2u c\u1ee7a c\u00e1c features  \n* H\u00e0m process_data_to_input s\u1ebd nh\u00fang c\u00e1c feature, s\u1eed d\u1ee5ng h\u00e0m nh\u00fang word2vec k\u1ebft h\u1ee3p v\u1edbi position embedding v\u1eeba k\u1ec3 tr\u00ean  \n* H\u00e0m create_transformer_model s\u1ebd kh\u1edfi t\u1ea1o c\u00e1c tham s\u1ed1 c\u1ea7n thi\u1ebft c\u1ee7a m\u00f4 h\u00ecnh traing, g\u00f4m t\u1ea1o m\u1ea1ng neuron, c\u00e1c h\u00e0m m\u1ea5t m\u00e1t c\u0169ng nh\u01b0 h\u00e0m t\u00ednh to\u00e1n xu\u1ed1ng \u0111\u1ed3i  \n* H\u00e0m split_dataset s\u1ebd ti\u1ebfn h\u00e0nh chia dataset th\u00e0nh t\u1eadp train v\u00e0 t\u1eadp validation m\u1ed9t c\u00e1ch ng\u1eabu nhi\u00ean theo m\u1ed9t t\u1ec9 l\u1ec7 nh\u1ea5t \u0111\u1ecbnh  \n* H\u00e0m train_and_validation s\u1ebd ti\u1ebfn h\u00e0nh training v\u00e0 sau \u0111\u00f3 s\u1ebd t\u00ednh to\u00e1n accuarcy tr\u00ean t\u1eadp validation","f829c5c5":"#### Kh\u1edfi t\u1ea1o m\u00f4 h\u00ecnh m\u1ea1ng n\u01a1-ron theo c\u00e1ch ti\u1ebfp c\u1eadn c\u1ee7a l\u1edbp encoder c\u1ee7a transformer, v\u1edbi 8 \u0111\u1ea7u multi-head attentions.","73191567":"### 4. Th\u1ef1c thi c\u00e1c h\u00e0m x\u1eed l\u00fd d\u1eef li\u1ec7u v\u00e0 ch\u1ea1y m\u00f4 h\u00ecnh","82ece0c6":"#### C\u00e1c h\u00e0m x\u00e2y d\u1ef1ng b\u1ed9 t\u1eeb v\u1ef1ng d\u1ef1a tr\u00ean c\u00e1c t\u1eeb t\u00e1ch \u0111\u01b0\u1ee3c trong file train v\u00e0 test.csv, sau \u0111\u00f3 ti\u1ec1n h\u00e0nh \u0111\u00e1nh s\u1ed1 c\u00e1c t\u1eeb trong c\u00e1c c\u00e2u m\u1edbi \u0111\u01b0\u1ee3c t\u00e1ch d\u1ef1a tr\u00ean v\u1ecb tr\u00ed c\u00e1c t\u1eeb trong t\u1eadp t\u1eeb \u0111i\u1ec3n","19ef8be5":"Nh\u01b0 \u0111\u00e3 \u0111\u1ec1 c\u1eadp h\u00e0m _create_dataset s\u1ebd ti\u1ebfn h\u00e0nh undersampling d\u1eef li\u1ec7u sau \u0111\u00f3 chuy\u1ec3n th\u00e0nh dataset, ta c\u00f9ng th\u1ed1ng k\u00ea s\u1ed1 l\u01b0\u1ee3ng c\u1eb7p feature v\u00e0 label \u1ee9ng v\u1edbi nh\u00e3n 0 v\u00e0 1 c\u00f2n l\u1ea1i","28aaec37":"### TABLE OF CONTENTS\n#### 1. M\u00f4 t\u1ea3 b\u00e0i to\u00e1n v\u00e0 \u0111\u1ecdc d\u1eef li\u1ec7u\n#### 2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u\n#### 3. Kh\u1edfi t\u1ea1o c\u00e1c h\u00e0m x\u1eed l\u00fd v\u00e0 train, validate d\u1eef li\u1ec7u\n#### 4. Th\u1ef1c thi c\u00e1c h\u00e0m x\u1eed l\u00fd d\u1eef li\u1ec7u\n#### 5. D\u1ef1 \u0111o\u00e1n \u0111\u1ea7u ra tr\u00ean test.csv","a91cf97a":"### 2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u:\n#### File csv g\u1ed3m c\u00f3 3 c\u1ed9t:  \n**C\u1ed9t qid:** M\u00e3 c\u00e2u h\u1ecfi   \n**C\u1ed9t question_text:** N\u1ed9i dung c\u00e2u h\u1ecfi  \n**C\u1ed9t target:** Nh\u00e3n c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c ph\u00e2n lo\u1ea1i  \n","ebb83a66":"#### H\u00e0m th\u1ef1c hi\u1ec7n lo\u1ea1i b\u1ecf c\u00e1c c\u00e2u b\u1ecb r\u1ed7ng sau khi ti\u1ec1n x\u1eed l\u00fd","b2372d78":"Nh\u00ecn v\u00e0o \u0111\u1ed3 th\u1ecb c\u00f3 th\u1ec3 th\u1ea5y \u0111\u01b0\u1ee3c label 1 ch\u1ec9 chi\u1ebfm 6 % t\u1ed5ng s\u1ed1 nh\u00e3n trong c\u1ea3 t\u1eadp train data, \u0111i\u1ec1u n\u00e0y khi\u1ebfn m\u00f4 h\u00ecnh b\u1ecb imbalanced n\u1eb7ng, c\u00e1i m\u00e0 khi\u1ebfn m\u00f4 h\u00ecnh d\u1ec5 bias v\u1edbi c\u00e1c nh\u00e3n c\u00f2n l\u1ea1i. Do \u0111\u00f3, c\u1ea7n ph\u1ea3i t\u00ecm c\u00e1ch kh\u1eafc ph\u1ee5c v\u1ea5n \u0111\u1ec1 n\u00e0y.  \nC\u00f3 hai lo\u1ea1i ph\u01b0\u01a1ng ph\u00e1p c\u01a1 b\u1ea3n nh\u1eb1m gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\u00e0y, \u0111\u00f3 l\u00e0 oversampling v\u00e0 undersampling.Th\u1ef1c ch\u1ea5t hai ph\u01b0\u01a1ng ph\u00e1p v\u1eeba k\u1ec3 ch\u1ec9 \u0111\u01a1n gi\u1ea3n l\u00e0 b\u1ed3i th\u00eam d\u1eef li\u1ec7u \u1edf nh\u00e3n \u00edt h\u01a1n hay c\u1eaft b\u1edbt gi\u1eef lieuj \u1edf nh\u00e3n cao h\u01a1n sao cho l\u01b0\u01a1ng d\u1eef li\u1ec7u \u1edf c\u00e1c nh\u00e3n l\u00e0 nh\u01b0 nhau. \u1ede \u0111\u00e2y, t\u00f4i s\u1ebd ti\u1ebfn h\u00e0nh ph\u01b0\u01a1ng ph\u00e1p undersampling, \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n qua h\u00e0m _create_dataset.  \nT\u1ea1i h\u00e0m _create_dataset, t\u00f4i s\u1ebd c\u1eaft b\u1edbt d\u1eef li\u1ec7u \u1edf nh\u00e3n 0 m\u1ed9t c\u00e1ch ng\u1eabu nhi\u00ean, gi\u1eef l\u1ea1i m\u1ed9t l\u01b0\u1ee3ng d\u1eef li\u1ec7u ngang v\u1edbi l\u01b0\u01a1ng d\u1eef li\u1ec7u \u1edf label 1. Sau \u0111\u00f3 g\u1ed9p hai d\u1eef li\u1ec7u label 0 m\u1edbi v\u00e0 d\u1eef li\u1ec7u label 1 th\u00e0nh t\u1eadp data \u0111\u1ec3 ti\u1ebfn h\u00e0nh training v\u00e0 testing. ","b7e917ff":"#### Ti\u1ec1n h\u00e0nh chuy\u1ec3n d\u1eef li\u1ec7u \u0111\u1ecdc \u0111\u01b0\u1ee3c th\u00e0nh c\u00e1c list \u0111\u1ec3 ti\u1ec7n x\u1eed l\u00fd","96645127":"#### \u0110\u1ecdc d\u1eef li\u1ec7u t\u1eeb file train.csv v\u00e0 test.csv","3d11deca":"Sau khi kh\u1edfi t\u1ea1o c\u00e1c h\u00e0m th\u00edch h\u1ee3p, b\u00e2y gi\u1edd s\u1ebd \u00e1p d\u1ee5ng c\u00e1c h\u00e0m \u0111\u1ea5y \u0111\u1ec3 x\u1eed l\u00fd d\u1eef li\u1ec7u v\u00e0 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh c\u0169ng nh\u01b0 ki\u1ec3m th\u1eed","6cdef5f7":"### 5. D\u1ef1 \u0111o\u00e1n \u0111\u1ea7u ra tr\u00ean test.csv","8f6b98bc":"H\u1ecd v\u00e0 t\u00ean: Nguy\u1ec5n M\u1ea1nh H\u01b0ng  \nMSSV: 19021295  \nL\u1edbp: INT3405E 20\nGi\u00e1o vi\u00ean h\u01b0\u1edbng d\u1eabn: Tr\u1ea7n Qu\u1ed1c Long","5ed38ffc":"#### **M\u00f4 t\u1ea3 b\u00e0i to\u00e1n**: \u0110\u00e2y l\u00e0 b\u00e0i to\u00e1n thu\u1ed9c nh\u00f3m b\u00e0i to\u00e1n ph\u00e2n l\u1edbp, v\u1edbi \u0111\u1ea7u v\u00e0o l\u00e0 m\u1ed9t c\u00e2u h\u1ecfi, \u0111\u1ea7u ra l\u00e0 1 n\u1ebfu c\u00e2u h\u1ecfi \u0111\u00f3 l\u00e0 spam v\u00e0 1 l\u00e0 ng\u01b0\u1ee3c l\u1ea1i","adfed817":"D\u01b0\u1edbi \u0111\u00e2y l\u00e0 v\u00ed d\u1ee5 m\u1ed9t v\u00e0i c\u00e2u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd","102318d1":"#### In ra accuracy, p score, r score, f1 score cho c\u00e1c nh\u00e3n"}}