{"cell_type":{"5b7b0197":"code","67688f59":"code","587e8f40":"code","6836c768":"code","66b35b28":"code","ffc3c86f":"code","07592202":"code","7327f7ca":"code","c91d09a3":"code","4bc1b4f9":"code","4f18231d":"code","90002273":"code","423026b8":"markdown"},"source":{"5b7b0197":"import numpy as np \nimport pandas as pd \nimport os\nimport tensorflow as tf\nimport pandas as pd\nimport itertools\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix # Loading required libraries","67688f59":"data = pd.read_csv(\"..\/input\/diabetes.csv\") # Load the dataset","587e8f40":"data.head(5) # Quick look ","6836c768":"sns.countplot(data.Outcome)\nplt.title('Categories') # Class imbalance is there but not considerable","66b35b28":"y = data.Outcome\nX = data.drop(\"Outcome\", axis=1) ","ffc3c86f":"X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.3, \n                                                    random_state=42) # Spliting Train Test datasets","07592202":"clf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=90,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test) #Let's try to fit the Random Forest model on this Data","7327f7ca":"metrics.accuracy_score(y_test, y_pred) #  Not bad ","c91d09a3":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ny_pred = clf.predict(X_test)\nconfusion_mtx = confusion_matrix(y_test, y_pred) \nplot_confusion_matrix(confusion_mtx, classes = range(2)) \n# Confusion Matrix ","4bc1b4f9":"model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0,\n              learning_rate=0.1, max_delta_step=0, max_depth=7,\n              min_child_weight=1, missing=None, n_estimators=7, n_jobs=-1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=2)\nmodel.fit(X_train, y_train)\n#Let's try to fit the XGBosst classifier model on this Data","4f18231d":"y_pred = model.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred) # Better than Random Forest","90002273":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ny_pred = model.predict(X_test)\n#y_pred_classes = np.argmax(y_pred,axis = 1) \nconfusion_mtx = confusion_matrix(y_test, y_pred) \nplot_confusion_matrix(confusion_mtx, classes = range(2)) \n# Again Plotting Confusion Matrix","423026b8":"XGBoost is definately better than Random Forest. Accuracy attained on both are slightly different. "}}