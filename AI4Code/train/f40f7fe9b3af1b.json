{"cell_type":{"87ae0008":"code","604b7c64":"code","8b31717a":"code","29dc7026":"code","6e70dc22":"code","b3210764":"code","fa8c7d86":"code","11456d93":"code","1666cac0":"code","834411f4":"markdown","222845c2":"markdown","b6c66ac0":"markdown","05a0df64":"markdown","02a12d7d":"markdown","d448f816":"markdown","2969f089":"markdown","9685b3b0":"markdown","19d4bac7":"markdown","90b37f36":"markdown","6739993a":"markdown","23940dd2":"markdown","2c03d0d7":"markdown","97d23002":"markdown","d6ed9137":"markdown","d93e10cf":"markdown","b831e7b4":"markdown","7f1ec159":"markdown","4a64356b":"markdown","cdb4591b":"markdown","010b50d1":"markdown","0609e519":"markdown","216e13c5":"markdown","3595ca41":"markdown","4c7140ba":"markdown","e5b7fc3c":"markdown","f36a9dea":"markdown","5746399d":"markdown","29988a8a":"markdown"},"source":{"87ae0008":"# Base Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt  \nimport numpy as np\n# Transformation\nfrom sklearn.preprocessing import MinMaxScaler\n# Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import neighbors\n# Metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score","604b7c64":"pd.options.display.float_format = '{:.4f}'.format\n\nscaler = MinMaxScaler(feature_range=(0, 1))\n\ndf_power = pd.read_csv('..\/input\/hydropower-generation\/Hydropower_Consumption.csv', sep=',')\ndf_power = df_power.drop(columns = [\"Country\"])\ndf_power = pd.DataFrame(scaler.fit_transform(df_power), \n                        columns=['2000','2001','2002','2003','2004','2005',\n                                 '2006','2007','2008','2009','2010','2011',\n                                 '2012','2013','2014','2015','2016','2017',\n                                 '2018','2019'])\ndf_power.describe()","8b31717a":"X = df_power.drop(columns = [\"2019\"])\ny = df_power[\"2019\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","29dc7026":"rmsle_val = []\nbest_rmsle = 1.0\n\nfor k in range(20):\n    k = k+1\n    knn = neighbors.KNeighborsRegressor(n_neighbors = k)\n\n    knn.fit(X_train, y_train) \n    y_pred = knn.predict(X_test)\n    rmsle = np.sqrt(mean_squared_log_error(y_test,y_pred))\n    if (rmsle < best_rmsle):\n        best_rmsle = rmsle\n        best_k = k\n    rmsle_val.append(rmsle)\n    print('RMSLE value for k= ' , k , 'is:', rmsle)\n\nprint(f\"Best RMSLE: {best_rmsle}, Best k: {best_k}\")","6e70dc22":"curve = pd.DataFrame(rmsle_val) #elbow curve \ncurve.plot(figsize=(8,5))","b3210764":"params = {'n_neighbors':[2,3,4,5,6,7,8,9]}\n\nknn = neighbors.KNeighborsRegressor()\n\nmodel = GridSearchCV(knn, params, cv=5)\nmodel.fit(X_train,y_train)\nmodel.best_params_","fa8c7d86":"knn = neighbors.KNeighborsRegressor(n_neighbors = 4)\n\nknn.fit(X_train,y_train)\ny_pred = knn.predict(X_test)\nknn.score(X_test, y_test)","11456d93":"r2_valid = r2_score(y_test, y_pred)\nmae_valid = mean_absolute_error(y_test, y_pred)\nevs_valid = explained_variance_score(y_test, y_pred, multioutput='uniform_average')\nrmse_valid = np.sqrt(mean_squared_error(y_test, y_pred))\nrmsle_valid = np.sqrt(mean_squared_log_error(y_test, y_pred))\n\nprint('R2 Valid:',r2_valid)\nprint('EVS Valid:', evs_valid)\nprint('MAE Valid:', mae_valid)\nprint('RMSE Valid:',rmse_valid)\nprint('RMSLE Valid:', rmsle_valid)","1666cac0":"data_prediction = list(zip(y_test,y_pred))\ndata_prediction = pd.DataFrame(data_prediction, columns=['Test','Prediction'])\ndata_prediction.head(10)","834411f4":"1. [Introduction](#1.-Introduction)\n\n1. [Base Libraries](#2.-Base-Libraries)\n\n1. [Data Preprocessing](#3.-Data-PreProcessing)\n\n1. [Testing K Factors](#4.-Testing-K-Factors)\n\n1. [Tuning Hyperparameters](#5.-Tuning-Hyperparameters)\n\n1. [Validating the Models with Metrics](#6.-Validating-the-Models-with-Metrics)\n\n1. [Predicting Energy Generation](#7.-Predicting-Energy-Generation)\n\n1. [Conclusions](#8.-Conclusions)\n\n1. [References](#9.-References)","222845c2":"The results presented show a great proximity between the test and prediction values, showing that the model we created managed to fulfill its proposed objective of performing above 75% accuracy.","b6c66ac0":"### Table of Contents","05a0df64":"# 9. References","02a12d7d":"Initially, the data is imported from the dataset, and the categorical columns are excluded (in this case, only \"Country\"). To transform the dataset and still keep it as a Dataframe, the *scaler* library is used inside the *Dataframe* library, normalizing the data between 0 and 1, and keeping the dataframe properties. After this, the *describe* function is called, to show some important data about the dataframe, like mean, max, min, std and others.","d448f816":"The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set. For our dataset, GridSearch indicates 4 as the best values to our *k*, quite similar to our tests realized before.\n\nOnce you've found the best value for k, it's time to train the model and predict the results. We will also make use of the *score* function, which will allows us to see the accuracy rate for our model (80.16%).","2969f089":"Since we already have the training and test data selected, it is necessary to find the k factor that will generate the best results for the algorithm.","9685b3b0":"First, the basic libraries are imported: **pandas**, **matplotlib** and **numpy** to Dataframes, Graphs and numeric operations; **MinMaxScaler** to normalize our data between 0 and 1, **train_test_split** to help split the dataset (usually 70% training\/ 30% testing), **GridSearchCV** for hyperparameter tuning and **neighbors** to generate our models using *k*NN","19d4bac7":"The results show an R2 of 80.16% and an EVS of 81.31%, indicating that our model has a great fit to your sample (R2) and a strong association between the model and its current data (EVS). For the RMSLE, it only considers the relative error between and the predicted and the actual value and the scale of the error is not significant. On the other hand, RMSE value increases in magnitude if the scale of error increases. For our model, both presents very low values (4.99% and 3.90%)","90b37f36":"# 1. Introduction","6739993a":"# 5. Tuning Hyperparameters","23940dd2":"The graph shows a sharp drop in the RMSLE as *k* advances to 4, when it starts to increase indefinitely, leading us to the conclusion that 4 is the best result for *k*. There is also a way to find the best value for k in another way: hyperparametrization using GridSearchCV.","2c03d0d7":"For forecasting, historical data is used as input and future trends are predicted on the basis of this data, but accurate selection and extraction of meaningful features from data are challenging. However, the *k*nn algorithm, one of the most simple and useful Machine Learning techniques, can be easily used to generate powerful regression models. In this notebook, I focused on the use of kNN itself and its hyperparametrizations, to guarantee a better understanding of this algorithm and keep it as a plain tutorial to anyone who needs it. Another resources like feature engineering, dimensionality reduction or even another machine learning techniques are not used.\n\nThe dataset used is a energy generation compilation of several european countries, measured in THh between 2000 and 2019. Its contents were extracted from World in Data. ","97d23002":"\nOne of the ways to find this *k* factor is to perform a test with several values, and measure the percentage results. It can almost be considered a \"brute force test\", since it will be necessary to exhaust a large number of possibilities of *k*","d6ed9137":"Here the test results are compared with the prediction results within the metrics, so that we can see what the results are for each of them.","d93e10cf":"Once we have the model ready and the prediction made, we can apply the metrics and analyze the results.","b831e7b4":"To measure the efficiency of our generated models, we use a bunch of metrics (maybe more than we need):\n* **mean_absolute_error** (MAE): a measure of errors between paired observations expressing the same phenomenon;\n* **mean_squared_error**  (root - RMSE): the standard deviation of the residuals (prediction errors);\n* **mean_squared_log_error** (root - RMSLE): that measures the ratio between actual and predicted;\n* **r2_score** (R2): coefficient of determination, the proportion of the variance in the dependent variable that is predictable from the independent variable(s); and\n* **explained_variance_score** (EVS): measures the discrepancy between a model and actual data","7f1ec159":"# 3. Data PreProcessing","4a64356b":"# 6. Validating the Models with Metrics","cdb4591b":"Now, my goal was to check the possibility of creating a model that would predict power generation for 2019, based on the previous 18 years (2000 - 2018) with at least 75% accuracy. For this, the data set was separated into X and y, X being my prediction data, and y what I intended to predict. For this, the data set was separated into X and y, X being my prediction data, and y what I intended to predict. For this, they are divided into training (70%) and testing (30%).","010b50d1":"A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data.","0609e519":"# 7. Predicting Energy Generation","216e13c5":"# 4. Testing *k* Factors","3595ca41":"[1] Ali, M., Khan, Z. A., Mujeeb, S., Abbas, S., & Javaid, N. (2019). Short-Term Electricity Price and Load Forecasting using Enhanced Support Vector Machine and K-Nearest Neighbor. 2019 Sixth HCT Information Technology Trends (ITT). doi:10.1109\/itt48889.2019.9075063 \n\n[2] Ashfaq, T., & Javaid, N. (2019). Short-Term Electricity Load and Price Forecasting using Enhanced KNN. 2019 International Conference on Frontiers of Information Technology (FIT). doi:10.1109\/fit47737.2019.00057 \n\n[3] Chicco, D. (2017). Ten quick tips for machine learning in computational biology. BioData Mining, 10(1). doi:10.1186\/s13040-017-0155-3 \n\n[4] Claesen, Marc; Bart De Moor (2015). \"Hyperparameter Search in Machine Learning\". arXiv:1502.02127\n\n[5] Zaki, M., & Meira, W. (2014). Data Mining and Analysis: Fundamental Concepts and Algorithms. New York City, New York: Cambridge University Press (10.1017\/CBO9780511810114).","4c7140ba":"\nOnce we have the prediction ready and the model tested, we organize the results side by side to be able to make a comparison.","e5b7fc3c":"# 2. Base Libraries","f36a9dea":"Our metric indicates that the smallest error occurs when we have *k* = 4 (RMSLE of 0.0390), indicating a relative error between the predicted and current values of 3.90%.\n\nTherefore, we will present all the values in a graph, which will show us visually the results obtained. This function is known as the \"elbow function\", given the percentage variation that occurs between the values of *k*, first downwards and then upwards, when *k* finds its best value. We are plotting the RMSLE values against the *k* values","5746399d":"# 8. Conclusions","29988a8a":"Power generation data sets have a very close pattern, which can be identified even with a simple regression tool, even if the pre-processing of the data is simpler. Several other techniques and algorithms can also be applied to improve the performance of the model at higher levels."}}