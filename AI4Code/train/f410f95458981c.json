{"cell_type":{"84cba0b0":"code","3e749713":"code","9c7a711e":"code","e1249d48":"code","5e733785":"code","d81d7240":"code","40702560":"code","52b666e7":"code","52a172c3":"code","32e3db39":"code","23fa9808":"code","1ec8f632":"code","d42231e4":"code","2aceec3c":"code","948998cc":"code","d380f23f":"code","df92f93e":"code","00d4faaa":"code","98c3d009":"code","c7542b8d":"code","96b057de":"code","f76a5287":"code","582b0cc7":"code","09597f2e":"code","3278f23d":"code","38f5f174":"code","dd5a3311":"code","005764d8":"code","153dd5fe":"code","3b7bee2a":"code","9d7c9da1":"code","d0caaa09":"code","cc4e7836":"code","d31d843c":"markdown","f6583ac0":"markdown","5793c141":"markdown"},"source":{"84cba0b0":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport random\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom math import sqrt\nfrom statistics import mean, pstdev","3e749713":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M","9c7a711e":"def seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(42)","e1249d48":"ROOT = \"..\/input\/osic-pulmonary-fibrosis-progression\"\nBATCH_SIZE=128","5e733785":"tr = pd.read_csv(f\"{ROOT}\/train.csv\")\ntr.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\nchunk = pd.read_csv(f\"{ROOT}\/test.csv\")\n\nprint(\"add infos\")\nsub = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\nsub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\nsub = sub.merge(chunk.drop('Weeks', axis=1), on=\"Patient\")","d81d7240":"tr['WHERE'] = 'train'\nchunk['WHERE'] = 'val'\nsub['WHERE'] = 'test'\ndata = tr.append([chunk, sub])","40702560":"print(tr.shape, chunk.shape, sub.shape, data.shape)\nprint(tr.Patient.nunique(), chunk.Patient.nunique(), sub.Patient.nunique(), \n      data.Patient.nunique())\n#","52b666e7":"data['min_week'] = data['Weeks']\ndata.loc[data.WHERE=='test','min_week'] = np.nan\ndata['min_week'] = data.groupby('Patient')['min_week'].transform('min')","52a172c3":"base = data.loc[data.Weeks == data.min_week]\nbase = base[['Patient','FVC']].copy()\nbase.columns = ['Patient','min_FVC']\nbase['nb'] = 1\nbase['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\nbase = base[base.nb==1]\nbase.drop('nb', axis=1, inplace=True)","32e3db39":"data = data.merge(base, on='Patient', how='left')\ndata['base_week'] = data['Weeks'] - data['min_week']\ndel base","23fa9808":"COLS = ['Sex','SmokingStatus'] #,'Age'\nFE = []\nfor col in COLS:\n    for mod in data[col].unique():\n        FE.append(mod)\n        data[mod] = (data[col] == mod).astype(int)\n#=================","1ec8f632":"#\ndata['age'] = (data['Age'] - data['Age'].min() ) \/ ( data['Age'].max() - data['Age'].min() )\ndata['BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) \/ ( data['min_FVC'].max() - data['min_FVC'].min() )\ndata['week'] = (data['base_week'] - data['base_week'].min() ) \/ ( data['base_week'].max() - data['base_week'].min() )\ndata['percent'] = (data['Percent'] - data['Percent'].min() ) \/ ( data['Percent'].max() - data['Percent'].min() )\nFE += ['age','percent','week','BASE']","d42231e4":"tr = data.loc[data.WHERE=='train']\nchunk = data.loc[data.WHERE=='val']\nsub = data.loc[data.WHERE=='test']\ndel data","2aceec3c":"tr.shape, chunk.shape, sub.shape","948998cc":"C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n#=============================#\ndef score(y_true, y_pred):\n    #K.print_tensor(y_true)\n    #K.print_tensor(y_pred)\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 5] - y_pred[:, 1]\n    fvc_pred = y_pred[:, 3]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta \/ sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    #K.print_tensor(K.mean(metric))\n    return K.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    #print(K.int_shape(y_true))\n    #print(K.int_shape(y_pred))\n    # Pinball loss for multiple quantiles\n    #K.print_tensor(y_true[:1])\n    #K.print_tensor(y_pred[:1])\n    qs = [0.09, 0.159, 0.34, 0.50, 0.66, 0.841, 0.91]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    #K.print_tensor(e[:1])\n    #K.print_tensor((q*e)[:1])\n    #K.print_tensor(((q-1)*e)[:1])\n    v = tf.maximum(q*e, (q-1)*e)\n    #K.print_tensor(v[:1])\n    #K.print_tensor(K.mean(v))\n    return K.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n        #return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*tf.metrics.mean_absolute_error(y_true[:,0], y_pred[:,3])\n    return loss\n#=================\ndef make_model(nh):\n    z = L.Input((nh,), name=\"Patient\")\n    x = L.Dense(100, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n    #x = L.Dense(100, activation=\"relu\", name=\"d3\")(x)\n    p1 = L.Dense(7, activation=\"linear\", name=\"p1\")(x)\n    p2 = L.Dense(7, activation=\"relu\", name=\"p2\")(x)\n    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), name=\"preds\")([p1, p2])\n    #preds = L.Lambda(lambda x: x[0], name=\"preds\")([p1, p2])\n    #preds = L.Lambda(lambda x: tf.cumsum(x[1], axis=1), name=\"preds\")([p1, p2]) \n                     \n    \n    model = M.Model(z, preds, name=\"CNN\")\n    #model.compile(loss=qloss, optimizer=\"adam\", metrics=[score])\n    model.compile(loss=mloss(0.1), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model\n\n","d380f23f":"y = tr['FVC'].values\nz = tr[FE].values\nze = sub[FE].values\nnh = z.shape[1]\npe = np.zeros((ze.shape[0], 7))\npred = np.zeros((z.shape[0], 7))","df92f93e":"net = make_model(nh)\nprint(net.summary())\nprint(net.count_params())","00d4faaa":"NFOLD = 5\nkf = KFold(n_splits=NFOLD)","98c3d009":"%%time\ncnt = 0\nEPOCHS = 400\ntr_scores = []\nval_scores = []\nfor tr_idx, val_idx in kf.split(z):\n    cnt += 1\n    print(f\"FOLD {cnt}\")\n    net = make_model(nh)\n    net.fit(z[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS, \n            validation_data=(z[val_idx], y[val_idx]), verbose=0) #\n    print(\"train\", net.evaluate(z[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"val\", net.evaluate(z[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"predict val...\")\n    pred[val_idx] = net.predict(z[val_idx], batch_size=BATCH_SIZE, verbose=0)\n    print(\"predict test...\")\n    pe += net.predict(ze, batch_size=BATCH_SIZE, verbose=0) \/ NFOLD\n    tr_scores.append(net.evaluate(z[tr_idx], y[tr_idx],verbose=0, batch_size=BATCH_SIZE)[1])\n    val_scores.append(net.evaluate(z[val_idx], y[val_idx],verbose=0, batch_size=BATCH_SIZE)[1])\n#==============\nprint(\"\")\nprint(\"Mean training score: \", mean(tr_scores))\nprint(\"Mean validation score: \", mean(val_scores))\nprint(\"\")\nprint(\"STD training score: \", pstdev(tr_scores))\nprint(\"STD validation score: \", pstdev(val_scores))\nprint(\"\")\nprint('Overfitting metric - mean(val) - mean(tr):', mean(val_scores) - mean(tr_scores))\nprint('Overfitting metric - STD(val) - STD(tr):', pstdev(val_scores) - pstdev(tr_scores))\nprint(\"\")","c7542b8d":"pe[:10]","96b057de":"sigma_opt_mae = mean_absolute_error(y, pred[:, 3])\nsigma_opt_mse = sqrt(mean_squared_error(y, pred[:, 3]))\nunc = pred[:,5] - pred[:, 1]\nunc_real = abs(y - pred[:,3])\nsigma_mean = np.mean(unc)\nprint(\"MAE, SIGmean: \")\nprint(sigma_opt_mae, sigma_mean)\nprint(\"sqrt(MSE), SIGmean: \")\nprint(sigma_opt_mse, sigma_mean)","f76a5287":"idxs = np.random.randint(0, y.shape[0], 100)\nplt.plot(y[idxs], label=\"ground truth\")\n#plt.figure(figsize=(100,60))\nplt.plot(pred[idxs, 0], label=\"q09\")\nplt.plot(pred[idxs, 1], label=\"q18\")\nplt.plot(pred[idxs, 2], label=\"q34\")\nplt.plot(pred[idxs, 3], label=\"q50\")\nplt.plot(pred[idxs, 4], label=\"q66\")\nplt.plot(pred[idxs, 5], label=\"q82\")\nplt.plot(pred[idxs, 6], label=\"q91\")\nplt.legend(loc=\"best\")","582b0cc7":"print(unc.min(), unc.mean(), unc.max(), (unc>=0).mean())\nprint(unc_real.min(), unc_real.mean(), unc_real.max(), (unc_real>=0).mean())","09597f2e":"plt.hist(unc, bins=100, label='guess')\n#plt.hist(unc_real, bins=100, label='real')\nplt.title(\"uncertainty in prediction\")\nplt.show()","3278f23d":"sub.head()","38f5f174":"sub['FVC1'] = 0.996*pe[:, 3]\nsub['Confidence1'] = pe[:, 5] - pe[:, 1]","dd5a3311":"subm = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()","005764d8":"subm.loc[~subm.FVC1.isnull()].head(10)","153dd5fe":"subm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\nif sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']","3b7bee2a":"subm.head()","9d7c9da1":"subm.describe().T","d0caaa09":"otest = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\nfor i in range(len(otest)):\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1","cc4e7836":"subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)","d31d843c":"### PREDICTION","f6583ac0":"### BASELINE NN ","5793c141":"![](http:\/\/)Thank you @ulrich07 for this lovely notebook, don't forget upvode it -->\nhttps:\/\/www.kaggle.com\/ulrich07\/osic-multiple-quantile-regression-starter"}}