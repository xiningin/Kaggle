{"cell_type":{"e2a72f29":"code","1c0ff42e":"code","57f4af61":"code","a773787b":"code","948d8f02":"code","33064bd6":"code","47fe449c":"code","ce0ef634":"code","b5295a1e":"code","082c5f82":"code","5a2ad1ed":"code","37b711d7":"code","41792588":"code","db154d5a":"code","c16c4036":"code","4c904a99":"code","64586db0":"code","890b4228":"code","c6f034b3":"code","d6ff1f21":"code","899bfd33":"code","badc9d58":"code","a48e32b0":"code","0aea8007":"code","6e05d9f2":"code","1c42d379":"code","e2e4cafb":"code","2a81d569":"code","2434a727":"code","5211c1e2":"code","fad221ef":"markdown","cc06bd2c":"markdown","d7d79112":"markdown","acbcabe7":"markdown","4a938052":"markdown","c68645a9":"markdown","3116dddd":"markdown","8c200b24":"markdown","445c8aef":"markdown","964f7218":"markdown","df6c90c5":"markdown","887229fe":"markdown","86dd3bc0":"markdown","3727349f":"markdown","0f30785b":"markdown","e401b2b3":"markdown","11ca1315":"markdown","9e486628":"markdown"},"source":{"e2a72f29":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set()\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\nwarnings.filterwarnings('ignore')","1c0ff42e":"train = pd.read_csv('..\/input\/train_V2.csv')\ntest = pd.read_csv('..\/input\/test_V2.csv')","57f4af61":"\ntrain.head()","a773787b":"# Remove the row with the missing target value\ntrain = train[train['winPlacePerc'].isna() != True]\n","948d8f02":"# Add a feature containing the number of players that joined each match.\ntrain['playersJoined'] = train.groupby('matchId')['matchId'].transform('count')\ntest['playersJoined'] = test.groupby('matchId')['matchId'].transform('count')\n","33064bd6":"# Lets look at only those matches with more than 50 players.\ndata = train[train['playersJoined'] > 50]\n\nplt.figure(figsize=(15,15))\nsns.countplot(data['playersJoined'].sort_values())\nplt.title('Number of players joined',fontsize=15)\nplt.show()","47fe449c":"def normaliseFeatures(train):\n    train['killsNorm'] = train['kills']*((100-train['playersJoined'])\/100 + 1)\n    train['headshotKillsNorm'] = train['headshotKills']*((100-train['playersJoined'])\/100 + 1)\n    train['killPlaceNorm'] = train['killPlace']*((100-train['playersJoined'])\/100 + 1)\n    train['killPointsNorm'] = train['killPoints']*((100-train['playersJoined'])\/100 + 1)\n    train['killStreaksNorm'] = train['killStreaks']*((100-train['playersJoined'])\/100 + 1)\n    train['longestKillNorm'] = train['longestKill']*((100-train['playersJoined'])\/100 + 1)\n    train['roadKillsNorm'] = train['roadKills']*((100-train['playersJoined'])\/100 + 1)\n    train['teamKillsNorm'] = train['teamKills']*((100-train['playersJoined'])\/100 + 1)\n    train['damageDealtNorm'] = train['damageDealt']*((100-train['playersJoined'])\/100 + 1)\n    train['DBNOsNorm'] = train['DBNOs']*((100-train['playersJoined'])\/100 + 1)\n    train['revivesNorm'] = train['revives']*((100-train['playersJoined'])\/100 + 1)\n\n    # Remove the original features we normalised\n    train = train.drop(['kills', 'headshotKills', 'killPlace', 'killPoints', 'killStreaks', \n                        'longestKill', 'roadKills', 'teamKills', 'damageDealt', 'DBNOs', 'revives'],axis=1)\n\n    return train\n\ntrain = normaliseFeatures(train)\ntest = normaliseFeatures(test)","ce0ef634":"train.head()","b5295a1e":"# Total distance travelled\ntrain['totalDistance'] = train['walkDistance'] + train['rideDistance'] + train['swimDistance']\ntest['totalDistance'] = test['walkDistance'] + test['rideDistance'] + test['swimDistance']\n","082c5f82":"# Normalise the matchTypes to standard fromat\ndef standardize_matchType(data):\n    data['matchType'][data['matchType'] == 'normal-solo'] = 'Solo'\n    data['matchType'][data['matchType'] == 'solo'] = 'Solo'\n    data['matchType'][data['matchType'] == 'solo-fpp'] = 'Solo'\n    data['matchType'][data['matchType'] == 'normal-solo-fpp'] = 'Solo'\n    data['matchType'][data['matchType'] == 'normal-duo-fpp'] = 'Duo'\n    data['matchType'][data['matchType'] == 'duo'] = 'Duo'\n    data['matchType'][data['matchType'] == 'normal-duo'] = 'Duo'\n    data['matchType'][data['matchType'] == 'duo-fpp'] = 'Duo'\n    data['matchType'][data['matchType'] == 'squad'] = 'Squad'\n    data['matchType'][data['matchType'] == 'squad-fpp'] = 'Squad'\n    data['matchType'][data['matchType'] == 'normal-squad'] = 'Squad'\n    data['matchType'][data['matchType'] == 'normal-squad-fpp'] = 'Squad'\n    data['matchType'][data['matchType'] == 'flaretpp'] = 'Other'\n    data['matchType'][data['matchType'] == 'flarefpp'] = 'Other'\n    data['matchType'][data['matchType'] == 'crashtpp'] = 'Other'\n    data['matchType'][data['matchType'] == 'crashfpp'] = 'Other'\n\n    return data\n\n\ntrain = standardize_matchType(train)\ntest = standardize_matchType(test)","5a2ad1ed":"train = train.drop(['Id','groupId','matchId'], axis=1)\n# Save the Ids for the submission later on\ntest_ids = test['Id']\ntest = test.drop(['Id','groupId','matchId'], axis=1)","37b711d7":"# Transform the matchType into scalar values\nle = LabelEncoder()\ntrain['matchType']=le.fit_transform(train['matchType'])\ntest['matchType']=le.fit_transform(test['matchType'])","41792588":"# We can do a sanity check of the data, making sure we have the new \n# features created and the matchType feature is standardised.\ntrain.head()","db154d5a":"test.head()","c16c4036":"train.describe()","4c904a99":"scaler = MinMaxScaler()\ntrain_scaled = pd.DataFrame(scaler.fit_transform(train), columns=train.columns)\ntest_scaled = pd.DataFrame(scaler.fit_transform(test), columns=test.columns)\n\ntrain_scaled.head()","64586db0":"train_scaled.describe()","890b4228":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n","c6f034b3":"# Train Test Split\ny = train_scaled['winPlacePerc']\nX = train_scaled.drop(['winPlacePerc'],axis=1)\nsize = 0.30\nseed = 42\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=size, random_state=seed)\n","d6ff1f21":"# The function takes the training and validation data to fit and score the model\ndef runAllModels(X_train, X_validation, Y_train, Y_validation):\n    \n    \n    linear = LinearRegression(copy_X=True)\n    linear.fit(X_train,Y_train)\n    #print(\"Linear Model traininig: {0:.5}%\".format(linear.score(X_train,Y_train)*100))\n    print(\"Linear Model score: {0:.5}%\".format(linear.score(X_validation,Y_validation)*100))\n\n    ridge = Ridge(copy_X=True)\n    ridge.fit(X_train,Y_train)\n    #print(\"Ridge Model traininig: {0:.5}%\".format(ridge.score(X_train,Y_train)*100))\n    print(\"Ridge Model score: {0:.5}%\".format(ridge.score(X_validation,Y_validation)*100))\n\n    lasso = Lasso(copy_X=True)\n    lasso.fit(X_train,Y_train)\n    #print(\"Lasso Model traininig: {0:.5}%\".format(lasso.score(X_train,Y_train)*100))\n    print(\"Lasso Model score: {0:.5}%\".format(lasso.score(X_validation,Y_validation)*100))\n\n    elastic = ElasticNet(copy_X=True)\n    elastic.fit(X_train,Y_train)\n    #print(\"Elastic Model traininig: {0:.5}%\".format(elastic.score(X_train,Y_train)*100))\n    print(\"Elastic Model score: {0:.5}%\".format(elastic.score(X_validation,Y_validation)*100))\n\n    ada = AdaBoostRegressor(learning_rate=0.8)\n    ada.fit(X_train,Y_train)\n    #print(\"AdaBoost Model traininig: {0:.5}%\".format(ada.score(X_train,Y_train)*100))\n    print(\"AdaBoost Model score: {0:.5}%\".format(ada.score(X_validation,Y_validation)*100))\n\n    GBR = GradientBoostingRegressor(learning_rate=0.8)\n    GBR.fit(X_train,Y_train)\n    #print(\"GradientBoost Model traininig: {0:.5}%\".format(GBR.score(X_train,Y_train)*100))\n    print(\"GradientBoost Model score: {0:.5}%\".format(GBR.score(X_validation,Y_validation)*100))\n\n    forest = RandomForestRegressor(n_estimators=10)\n    forest.fit(X_train,Y_train)\n    #print(\"RandomForest Model traininig: {0:.5}%\".format(forest.score(X_train,Y_train)*100))\n    print(\"RandomForest Model score: {0:.5}%\".format(forest.score(X_validation,Y_validation)*100))\n\n    tree = DecisionTreeRegressor()\n    tree.fit(X_train,Y_train)\n    #print(\"DecisionTree Model traininig: {0:.5}%\".format(tree.score(X_train,Y_train)*100))\n    print(\"DecisionTree Model score: {0:.5}%\".format(tree.score(X_validation,Y_validation)*100))","899bfd33":"runAllModels(X_train, X_validation, Y_train, Y_validation)","badc9d58":"GBR = GradientBoostingRegressor(learning_rate=0.8)\nGBR.fit(X,y)\n#print(\"GradientBoost Model traininig: {0:.5}%\".format(GBR.score(X,y)*100))\n\npredictions = GBR.predict(test)","a48e32b0":"predictions[predictions > 1] = 1\npredictions[predictions < 0] = 0","0aea8007":"def create_submission(submission_Id, predictions, filename):\n    submission = pd.DataFrame({'Id': submission_Id, 'winPlacePerc': predictions})\n    submission.to_csv(filename+'.csv',index=False)","6e05d9f2":"create_submission(test_ids, predictions, 'submission_full')","1c42d379":"# Fit all the data to the model and return the predictions for the testing set.\ndef make_predictions(model, X, y, test):\n    model.fit(X,y)\n    #print(\"Model traininig: {0:.5}%\".format(model.score(X,y)*100))\n\n    return model.predict(test)\n    ","e2e4cafb":"# Create the models to use for our predictions\nlinear = LinearRegression(copy_X=True)\nridge = Ridge(copy_X=True)\nGBR = GradientBoostingRegressor(learning_rate=0.8)\nforest = RandomForestRegressor(n_estimators=10)\ntree = DecisionTreeRegressor()\n    ","2a81d569":"# Get some predictions from each model\npredictions_linear = make_predictions(linear, X, y, test_scaled)\npredictions_ridge = make_predictions(ridge, X, y, test_scaled)\npredictions_GBR = make_predictions(GBR, X, y, test_scaled)\npredictions_forest = make_predictions(forest, X, y, test_scaled)\npredictions_tree = make_predictions(tree, X, y, test_scaled)\n","2434a727":"# Adjust the predictions that are outside the bounds of the target variable.\npredictions_linear[predictions_linear > 1] = 1\npredictions_linear[predictions_linear < 0] = 0\n\npredictions_ridge[predictions_ridge > 1] = 1\npredictions_ridge[predictions_ridge < 0] = 0\n\npredictions_GBR[predictions_GBR > 1] = 1\npredictions_GBR[predictions_GBR < 0] = 0\n\npredictions_forest[predictions_forest > 1] = 1\npredictions_forest[predictions_forest < 0] = 0\n\npredictions_tree[predictions_tree > 1] = 1\npredictions_tree[predictions_tree < 0] = 0\n","5211c1e2":"# We create the submission files for each model\ncreate_submission(test_ids, predictions_linear, 'submission_linear')\ncreate_submission(test_ids, predictions_ridge, 'submission_ridge')\ncreate_submission(test_ids, predictions_GBR, 'submission_GBR')\ncreate_submission(test_ids, predictions_forest, 'submission_forest')\ncreate_submission(test_ids, predictions_tree, 'submission_tree')\n","fad221ef":"# Standardize the matchType feature\nHere I decided that many of the existing 16 seperate modes of game play were just different versions of four types of game.\n\n1. Solo: Hunger Games style, last man\/women standing.\n2. Duo: Teams of two against all other players.\n3. Squad: Teams of up to 4 players against All other players\n4. Other: These modes consist of custom and special events modes","cc06bd2c":"### TotalDistance\nAn additional feature we can create is the total distance the player travels. This is a combination of all the distance features in the original data set.","d7d79112":"We need to extract the target variable and split the data up into a training and validation set.","acbcabe7":"# Scale the features\nSome of the features have large variances, so in order to make sure they dont over influence the training or predictions. We can scale all our features so they provide the same influence over the model.","4a938052":"Lets take the best model, which looks like a GradientBoostRegressor with 92.86% accuracy on our validation data, but you could also just as easily take the RandomForest or DecisionTree.\n.\n## Gradient Boost\nWe'll train our Final Gradient boosting model on the full data set so we get the full predictive power of the data.","c68645a9":"If you liked this post, please upvote.","3116dddd":"Lets check out the data again.","8c200b24":"## Building the model\nWe'll try a number of models and use the best one for our predictions.","445c8aef":"Now we can transform the matchTypes into dummy values so we can use them in the model.","964f7218":"## Lets Engineer some features\nWe'll process the testing data the same way we do for the training data so the testing data has the same features and scaling as our training data.\n\n\n### PlayersJoined\nWe can determine the number of players that joined each match by grouping the data by matchID and counting the players.","df6c90c5":"Before we make the submission of our predictions, we need to make sure they are consistent with the boundaries of the target variable. The target variable \"winPlacePerc\" is a number between 0 and 1, so anything outside that will contribute to incorrect predictions.\n\nHere we'll force these values back down to the boundaries.","887229fe":"You can see most features range 0 to 100 or 1000's, but there are two features that doesn't really need scaling, VehicleDestroys and matchType, as they only range between 0 to 5, 6. Its not neccassary to scale these features, but we will any way, because it makes the code easier.","86dd3bc0":"# PUBG predictions\n\nI previously created a notebook going through some [exploratory anaylsys] of the PUBG data. I went through many of the different features avalailable and displayed an interesting plot describing the data and potential correlation with the target variable.\n\n* I found that there was one missing value for the target variable and decided that this row of data should be removed, as there was only one player for the match identified by the missing value.\n\n* I also made a few decisions about creating new features and one important way of breaking the data up to gain higher correllations with our features for seperate match types.\n\n\n[exploratory anaylsys]: https:\/\/www.kaggle.com\/beaubellamy\/pubg-eda#","3727349f":"You can see that there isn't always 100 players in each match, in fact its more likely to have between 90 and 100 players. It may be benficial to normalise those features that are affected by the number of players.\n\n### Normalised Features\nHere, I am making the assumption that it is easier to find an enemy when there are 100 players, than it is when there are 90 players.\n","0f30785b":"## Import libraries\nWe import the required libraries and import the data","e401b2b3":"# Model Development\n","11ca1315":"## Missing Data\nBased on our EDA, we found a row that had a NULL value for the target variable. We will remove the irrelevant row of data.","9e486628":"## More predictions\nWe'll try some of the better models we found and see how they perform on the predictions."}}