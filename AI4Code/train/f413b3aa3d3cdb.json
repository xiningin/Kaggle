{"cell_type":{"84824b8c":"code","d9bb3d6f":"code","67ce3181":"code","a71f43df":"code","7e265140":"code","498ffc54":"code","1f67cf02":"code","c848ba53":"code","7e63aa6b":"code","bae9deff":"code","96a69a71":"code","4d51fc40":"code","101da6e1":"code","e3e81632":"code","85fa5e0f":"code","1ac96064":"code","904b2532":"code","7d084754":"code","9b52fb9f":"code","5e1df31a":"code","5a9f2d66":"code","2c1bf5e7":"code","bbe13c16":"code","d0ef3dc2":"code","e0314724":"code","9e806bc9":"code","440ca671":"code","93f16470":"code","beec3b42":"code","2baf4915":"code","d757b2b9":"code","03fe936e":"code","e6d0a418":"code","454c7f65":"code","68b646dd":"code","dc105e1b":"code","bcc1d664":"markdown","804c5e97":"markdown","dcb8a75e":"markdown","f27990f6":"markdown","66b5313b":"markdown","8001dd03":"markdown","00e0dd6f":"markdown","b15b9b44":"markdown","19413520":"markdown","cf226e75":"markdown","5dc14f7d":"markdown","2f96bf03":"markdown","48628667":"markdown","d3d8396c":"markdown","2dcef72f":"markdown","89a9464e":"markdown","d71b6b13":"markdown","d7ad93d3":"markdown","63631836":"markdown","47ada068":"markdown","5243aa90":"markdown","49f93d77":"markdown","e9b19b1f":"markdown","4601489a":"markdown","e3527b9f":"markdown","9cc9cff5":"markdown","b6beeace":"markdown","adb40f3a":"markdown","fe9149f2":"markdown","1eff7009":"markdown","c93e503c":"markdown","3b3751fd":"markdown","3947ba6f":"markdown","02abafba":"markdown","2a8bc88f":"markdown","6ba75d1a":"markdown","1973b2c6":"markdown","8fd935db":"markdown","d37e6b6f":"markdown"},"source":{"84824b8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9bb3d6f":"# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","67ce3181":"\ndf=pd.read_csv('..\/input\/swarm-behaviour-classification\/Swarm_Behaviour.csv',nrows=1000)\n# df=pd.read_csv('..\/input\/swarm-behaviour-classification\/Swarm_Behaviour.csv')\ndf.head()","a71f43df":"# To estimate the execution time for a single line statement, we can use the magic command - %time\nimport seaborn as sns\n%time sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='coolwarm')\n\nX=df.iloc[:,:-1]\ny=df['Swarm_Behaviour']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","7e265140":"from sklearn.preprocessing import StandardScaler  # z = (x - u) \/ s\nscaler = StandardScaler()\nX_train_scale=scaler.fit_transform(X_train) # fit > z = (x - u) \/ s   # transform > apply to all train data\nX_test_scale=scaler.transform(X_test)  # already fitted just apply same equation in test\n# print(X_train_scale,X_test_scale)\n# print(scaler.mean_)  # it tell about the mean after data is fit ","498ffc54":"# build the lightgbm model\nimport lightgbm as lgb\nclf = lgb.LGBMClassifier()\nclf_fit=clf.fit(X_train, y_train)  \n\n# predict the results\ny_pred=clf_fit.predict(X_test)\n\n# view accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(y_pred, y_test)\nprint('LightGBM Model Test-set accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))\n\ny_pred_train = clf_fit.predict(X_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n\n# view confusion-matrix\n# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n\n# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n","1f67cf02":"import lightgbm\nMODEL_PARAMS = {'random_state': 1234,'learning_rate': 0.1,'n_estimators': 10}\nmodel = lightgbm.LGBMClassifier(**MODEL_PARAMS)\nprint(model.fit(X_train, y_train))\ny_pred=model.predict(X_test)","c848ba53":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n\n# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","7e63aa6b":"tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nfalse_positive_rate = fp \/ (fp + tn)\nprint('false_positive_rate',false_positive_rate)","bae9deff":"false_negative_rate = fn \/ (tp + fn)\nprint('false_negative_rate',false_negative_rate)","96a69a71":"true_negative_rate = tn \/ (tn + fp)\nprint('true_negative_rate',true_negative_rate)","4d51fc40":"negative_predictive_value = tn\/ (tn + fn)\nprint('negative_predictive_value',negative_predictive_value)","101da6e1":"false_discovery_rate = fp\/ (tp + fp)\nprint('false_discovery_rate',false_discovery_rate)","e3e81632":"from sklearn.metrics import recall_score\n\ntrue_positive_rate = tp \/ (tp + fn)\n\n# or simply\n\nprint('true_positive_rate',recall_score(y_test, y_pred))","85fa5e0f":"from sklearn.metrics import precision_score\npositive_predictive_value = tp\/ (tp + fp)\n\n# or simply\n\nprint('positive_predictive_value',precision_score(y_test, y_pred))","1ac96064":"from sklearn.metrics import accuracy_score\naccuracy = (tp + tn) \/ (tp + fp + fn + tn) \n#or \nprint('accuracy',accuracy_score(y_test, y_pred))","904b2532":"from sklearn.metrics import fbeta_score\n\nprint('fbeta_score beta<0',fbeta_score(y_test, y_pred, beta=0.5))","7d084754":"print('fbeta_score beta==1',fbeta_score(y_test, y_pred, beta=1))","9b52fb9f":"print('fbeta_score beta>1',fbeta_score(y_test, y_pred, beta=2))","5e1df31a":"from sklearn.metrics import cohen_kappa_score\n\nprint('cohen_kappa_score',cohen_kappa_score(y_test, y_pred))","5a9f2d66":"from sklearn.metrics import matthews_corrcoef\n\n\nprint('matthews_corrcoef',matthews_corrcoef(y_test, y_pred))","2c1bf5e7":"from scikitplot.metrics import plot_roc","bbe13c16":"from sklearn.metrics import roc_auc_score\n\nroc_auc = roc_auc_score(y_test, y_pred)\nprint('roc_auc',roc_auc)","d0ef3dc2":"from scikitplot.metrics import plot_precision_recall\n\n# fig, ax = plt.subplots()\n# plot_precision_recall(y_test, y_pred, ax=ax)","e0314724":"from sklearn.metrics import average_precision_score\n\nprint('average_precision_score',average_precision_score(y_test, y_pred))","9e806bc9":"from sklearn.metrics import log_loss\n\nprint('log_loss',log_loss(y_test, y_pred))","440ca671":"from sklearn.metrics import brier_score_loss\n\nprint('brier_score_loss',brier_score_loss(y_test, y_pred))","93f16470":"from scikitplot.metrics import plot_cumulative_gain\n\n# fig, ax = plt.subplots()\n# plot_cumulative_gain(y_test, y_pred, ax=ax)","beec3b42":"from scikitplot.metrics import plot_lift_curve \n\n# fig, ax = plt.subplots() \n# plot_lift_curve(y_true, y_pred, ax=ax)","2baf4915":"from scikitplot.metrics import plot_ks_statistic\n\n# fig, ax = plt.subplots()\n# plot_ks_statistic(y_true, y_pred, ax=ax)","d757b2b9":"from scikitplot.helpers import binary_ks_curve\n\nres = binary_ks_curve(y_test, y_pred)\nks_stat = res[3]\nprint('binary_ks_curve',ks_stat)","03fe936e":"from sklearn.tree import DecisionTreeClassifier\nX=df.iloc[:,:-1]\ny=df['Swarm_Behaviour']\nsns.countplot(x=\"Swarm_Behaviour\", data=df)\n\n# Splitting the dataset into training and test set.  \nfrom sklearn.model_selection import train_test_split  \nx_train, x_test, y_train, y_test= train_test_split(X, y, test_size= 0.25, random_state=0)\n\nmodel=DecisionTreeClassifier()\nmodel.fit(x_train,y_train)\nresult=model.score(x_test,y_test)\nprint('DecisionTreeClassifier score',result)","e6d0a418":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nmodel=DecisionTreeClassifier()\nk_fold_validation=KFold(n_splits=10)\n\nresult=cross_val_score(model,X,y,cv=k_fold_validation)\nprint(str(k_fold_validation))\nprint('\\nDecisionTreeClassifier cross_val_score',result)\nprint('\\nMin',result.min(),'Avearge',np.mean(result),'Max',result.max())","454c7f65":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\nmodel=DecisionTreeClassifier()\nsk_fold_validation=StratifiedKFold(n_splits=12)\n\nresult=cross_val_score(model,X,y,cv=sk_fold_validation)\nprint(str(sk_fold_validation))\nprint('\\nDecisionTreeClassifier cross_val_score',result)\nprint('\\nMin',result.min(),'Avearge',np.mean(result),'Max',result.max())","68b646dd":"from sklearn.model_selection import LeaveOneOut\n\nX1=X[:10]\ny1=y[:10]\nmodel=DecisionTreeClassifier()\nloo = LeaveOneOut()\n\nresult=cross_val_score(model,X1,y1,cv=loo)\nprint(str(loo))\nprint('\\nDecisionTreeClassifier cross_val_score',result)\nprint('\\nMin',result.min(),'Avearge',np.mean(result),'Max',result.max())","dc105e1b":"from sklearn.model_selection import ShuffleSplit\n\nmodel=DecisionTreeClassifier()\nrs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n\nresult=cross_val_score(model,X,y,cv=rs)\nprint(str(rs))\nprint('\\nDecisionTreeClassifier cross_val_score',result)\nprint('\\nMin',result.min(),'Avearge',np.mean(result),'Max',result.max())\n","bcc1d664":"# 6. False Discovery Rate\n\n![](https:\/\/i0.wp.com\/neptune.ai\/wp-content\/uploads\/fdr_fixed.png?fit=253%2C63&ssl=1)","804c5e97":"# K - Fold Cross validation\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-forum-message-attachments\/o\/inbox%2F4367831%2Fa7eaffa417f9905be8f0e22af7326ac0%2Fk-fold.jpg?generation=1609935772242624&alt=media)","dcb8a75e":"# 3. False Negative Rate | Type II error\n\n![](https:\/\/i0.wp.com\/neptune.ai\/wp-content\/uploads\/fnr_eq.png?fit=262%2C86&ssl=1)","f27990f6":"# 10. F beta score\n\nWhen choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, \n\n![](https:\/\/i1.wp.com\/neptune.ai\/wp-content\/uploads\/fbeta_eq.png?fit=604%2C88&ssl=1)\n\n\nWith 0<beta<1 we care more about precision and so the higher the threshold the higher the F beta score.\n\n\nWhen beta>1 our optimal threshold moves toward lower thresholds and with beta=1 it is somewhere in the middle.","66b5313b":"# 16. ROC AUC score\n\nhow good at ranking predictions your model is.","8001dd03":"**Following set of practices can be used to improve your model efficiency.**\n\n1 num_leaves : This is the main parameter to control the complexity of the tree model. Ideally, the value of num_leaves should be less than or equal to 2^(max_depth). Value more than this will result in overfitting.\n\n2 min_data_in_leaf : Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.\n\n3 max_depth : We also can use max_depth to limit the tree depth explicitly.","00e0dd6f":"# 1. Confusion Matrix\n\n![](https:\/\/glassboxmedicine.files.wordpress.com\/2019\/02\/confusion-matrix.png?w=816)","b15b9b44":"# 24. Kolmogorov-Smirnov statistic","19413520":"# 21. Cumulative gains chart\n\n","cf226e75":"**For Faster Speed**\n\n* Use bagging by setting bagging_fraction and bagging_freq.\n* Use feature sub-sampling by setting feature_fraction.\n* Use small max_bin.\n* Use save_binary to speed up data loading in future learning.\n","5dc14f7d":"# 7. True Positive Rate | Recall | Sensitivity\n\n![](https:\/\/i0.wp.com\/neptune.ai\/wp-content\/uploads\/tpr_Eq.png?fit=256%2C84&ssl=1)","2f96bf03":"# 20. Brier score\n\n![](https:\/\/i2.wp.com\/neptune.ai\/wp-content\/uploads\/brier_loss_eq.png?fit=436%2C51&ssl=1)","48628667":"**For better accuracy**\n\n* Use large max_bin (may be slower).\n* Use small learning_rate with large num_iterations\n* Use large num_leaves(may cause over-fitting)\n* Use bigger training data\n* Try dart\n* Try to use categorical feature directly.","d3d8396c":"# StratifiedKFold\n\nStratified K-Folds cross-validator.  for Dataset target (0\/1) Inbalance\n\nProvides train\/test indices to split data in train\/test sets.\n\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n\n**Male and Female Inbalance So we use StratifiedKFold than KFold**\n\n![](https:\/\/i0.wp.com\/dataaspirant.com\/wp-content\/uploads\/2020\/12\/8-Stratified-K-Fold-Cross-Validation.png?ssl=1)\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html\n\nhttps:\/\/youtu.be\/7062skdX05Y","2dcef72f":"# LeaveOneOut - Time More\n\n**Each sample** is used once as a test set (singleton) while the remaining samples form the training set.\n\n![](https:\/\/miro.medium.com\/max\/607\/1*SGNQoJtwG7YTcx7TRpUkGg.png)","89a9464e":"# ShuffleSplit\n\n","d71b6b13":"# Standardization fit() transform()\nwhere u is the mean of the training samples or zero if with_mean=False, and\n\ns is the standard deviation of the training samples or one if with_std=False.\n\n![](https:\/\/miro.medium.com\/max\/970\/0*3E-1O6yCamLFE3qE)\n","d7ad93d3":"# 19. Log loss\n\n![](https:\/\/i2.wp.com\/neptune.ai\/wp-content\/uploads\/logloss_eq-1.png?fit=947%2C44&ssl=1)","63631836":"# 22. Lift curve | lift chart\n\n","47ada068":"# 15. ROC Curve\n\nIt is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart.\n\nOf course, the higher TPR and the lower FPR is for each threshold the better and so classifiers that have curves that are more top-left side are better.","5243aa90":"# 2. False Positive Rate | Type I error\n\n![](https:\/\/i0.wp.com\/neptune.ai\/wp-content\/uploads\/fpr_eq.png?fit=257%2C86&ssl=1)","49f93d77":"# 12. F2 score (beta=2)\n\nIt\u2019s a metric that combines precision and recall, putting 2x emphasis on recall.","e9b19b1f":"# Intro about LightGBM - \n\nModel Buliding - fit(),predict()\n\nIt is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data.\n\nit focuses on accuracy of results\n\nLight GBM is prefixed as Light because of its high speed. Light GBM can handle the large size of data and takes lower memory to run.","4601489a":"# 13. Cohen Kappa Metric\n\n![](https:\/\/i1.wp.com\/neptune.ai\/wp-content\/uploads\/cohen_kappa_eq.png?fit=184%2C76&ssl=1)","e3527b9f":"# II - Cross Validation","9cc9cff5":"#  23. Kolmogorov-Smirnov plot","b6beeace":"# 11. F1 score (beta=1)\n\nIt\u2019s the harmonic mean between precision and recall.\n\nWe can adjust the threshold to optimize F1 score. Notice that for both precision and recall you could get perfect scores by increasing or decreasing the threshold. Good thing is, you can find a sweet spot for F1metric. As you can see, getting the threshold just right can actually improve your score by a bit 0.8077->0.8121.","adb40f3a":"# Reference\n\nhttps:\/\/neptune.ai\/blog\/evaluation-metrics-binary-classification","fe9149f2":"# 5. Negative Predictive Value\n\npredicted clean transactions in all non-fraudulent predictions.\n\n![](https:\/\/i2.wp.com\/neptune.ai\/wp-content\/uploads\/npv_eq.png?fit=267%2C84&ssl=1)","1eff7009":"# train_test_split","c93e503c":"# 17. Precision-Recall Curve\n\n","3b3751fd":"# Feature Engineering","3947ba6f":"# 4. True Negative Rate | Specificity\n\n In our fraud detection example, it tells us how many transactions, out of all non-fraudulent transactions,\n\n![](https:\/\/i2.wp.com\/neptune.ai\/wp-content\/uploads\/tnr_eq.png?fit=260%2C84&ssl=1)","02abafba":"# 9. Accuracy\n\nYou shouldn\u2019t use accuracy on imbalanced problems.\n\n\n![](https:\/\/i2.wp.com\/neptune.ai\/wp-content\/uploads\/acc_eq.png?fit=422%2C84&ssl=1)","2a8bc88f":"**To deal with over-fitting**\n\n* Use small max_bin\n* Use small num_leaves\n* Use min_data_in_leaf and min_sum_hessian_in_leaf\n* Use bagging by set bagging_fraction and bagging_freq\n* Use feature sub-sampling by set feature_fraction\n* Use bigger training data\n* Try lambda_l1, lambda_l2 and min_gain_to_split to regularization\n* Try max_depth to avoid growing deep tree\n\n\nhttps:\/\/www.kaggle.com\/prashant111\/lightgbm-classifier-in-python#1.-Introduction-to-LightGBM-","6ba75d1a":"# 18. PR AUC score | Average precision\n\n","1973b2c6":"# 8. Positive Predictive Value | Precision\n\n![](https:\/\/i1.wp.com\/neptune.ai\/wp-content\/uploads\/ppv_eq.png?fit=255%2C84&ssl=1)","8fd935db":"# 24 Evaluation Metrics for Binary Classification (And When to Use Them)\n\nhttps:\/\/www.kaggle.com\/prashant111\/lightgbm-classifier-in-python#1.-Introduction-to-LightGBM-\n\nhttps:\/\/neptune.ai\/blog\/evaluation-metrics-binary-classification","d37e6b6f":"# 14. Matthews Correlation Coefficient MCC\n\n![](https:\/\/i2.wp.com\/neptune.ai\/wp-content\/uploads\/mcc_eq.png?fit=738%2C89&ssl=1)"}}