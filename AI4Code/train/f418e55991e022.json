{"cell_type":{"6d28705e":"code","bc7dbb94":"code","6485dd7e":"code","4703d5dc":"code","85d9b53a":"code","ace2b23c":"code","6e761d7a":"code","83f1429e":"code","e6630203":"code","6d07df7d":"code","96c77a27":"code","aeb1d4ab":"code","df3de7ec":"code","fda9cd9a":"code","45bda718":"code","f7150db8":"markdown","e63e5c53":"markdown","b183e431":"markdown","42972b6b":"markdown","a3af05b0":"markdown","3e4be8fa":"markdown","325174f3":"markdown","0b5c15b5":"markdown","93b36d4d":"markdown","2dfce94a":"markdown","ffb3222f":"markdown","c5a900b7":"markdown","ab661993":"markdown","5c1d66ff":"markdown","a77b7fdf":"markdown","83fbd863":"markdown","cb200ba5":"markdown","f9d4ae94":"markdown","5b5b3b48":"markdown","1819755a":"markdown","e5b7dca6":"markdown","83014330":"markdown","1b412eb3":"markdown"},"source":{"6d28705e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc7dbb94":"# synthetic classification dataset\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom matplotlib import pyplot","6485dd7e":"# define dataset\nX, y = make_classification(n_samples = 1000, n_features = 2, n_informative = 2, n_redundant = 0,\n                          n_clusters_per_class = 1, random_state = 4)","4703d5dc":"print(X)","85d9b53a":"print(y)","ace2b23c":"# create scatter plot for samples from each class\nfor class_value in range(2):\n    # get row indexes for samples with this class\n    row_ix = where(y == class_value)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix,0],X[row_ix,1])\n# show the plot\npyplot.show()\n    ","6e761d7a":"from numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\nfrom matplotlib import pyplot\n#define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AffinityPropagation(damping=0.9, random_state = 4)\n#fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n    # create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()","83f1429e":"import scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\npyplot.show()","e6630203":"from sklearn.cluster import AgglomerativeClustering\n# define the model\nagglo_model = AgglomerativeClustering(n_clusters = 2)\n# fit model and predict clusters\ny_agglo = agglo_model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(y_agglo)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(y_agglo == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n","6d07df7d":"from sklearn.cluster import Birch\n# define the model\nbirch_model = Birch(threshold = 0.01, n_clusters = 2)\n# fit the model\nbirch_model.fit(X)\n# assign a cluster to each example\ny_birch = birch_model.predict(X)\n# retrieve unique clusters\nclusters = unique(y_birch)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(y_birch == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()","96c77a27":"from sklearn.cluster import DBSCAN\n# define the model\ndbscan_model = DBSCAN(eps=0.30, min_samples=9)\n# fit model and predict clusters\ny_dbscan = dbscan_model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(y_dbscan)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(y_dbscan == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()","aeb1d4ab":"from sklearn.cluster import KMeans\nk_model = KMeans(n_clusters=2)\n# fit the model\nk_model.fit(X)\n# assign a cluster to each example\ny_k= k_model.predict(X)\n# retrieve unique clusters\nclusters = unique(y_k)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(y_k == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()","df3de7ec":"from sklearn.cluster import MiniBatchKMeans\n# define the model\nmk_model = MiniBatchKMeans(n_clusters=2)\n# fit the model\nmk_model.fit(X)\n# assign a cluster to each example\ny_mk = mk_model.predict(X)\n# retrieve unique clusters\nclusters = unique(y_mk)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(y_mk == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()","fda9cd9a":"from sklearn.cluster import MeanShift\n# define the model\nms_model = MeanShift()\n# fit model and predict clusters\ny_ms = ms_model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(y_ms)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(y_ms == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()","45bda718":"from sklearn.mixture import GaussianMixture\n# define the model\ng_model = GaussianMixture(n_components=2)\n# fit the model\ng_model.fit(X)\n# assign a cluster to each example\ny_g = g_model.predict(X)\n# retrieve unique clusters\nclusters = unique(y_g)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(y_g == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()","f7150db8":"This is a scatter plot of all the 0s and 1s with respect to their X value. \n\nThe two groups that we can clearly see here are the groups of 0s and 1s.","e63e5c53":"# 3. BIRCH ","b183e431":"# Creating Synthetic Daataset","42972b6b":"Density-Based Spatial Clustering of Application with Noise.\n\nThis algorithm cannot analyse outliers properly.","a3af05b0":"There is unequal variance in each dimension. This method is less suited for this dataset.","3e4be8fa":"It predicted a cluster for each example in the dataset.\n\nIn this case, a good result is not achieved.","325174f3":"We have to take two clusters according to this Dendogram Graph.","0b5c15b5":"# 6. Mini-Batch K-Means\n","93b36d4d":"In this case, we can see that the clusters were identified perfectly. This is not surprising given that the dataset was generated as a mixture of Gaussians.","2dfce94a":"This result is equivalent to K-Means algo.","ffb3222f":"Due to presence of noise,grouping is not tuned properly.","c5a900b7":"This is a modified version of K-Means which uses mini-batches of sample data instead of using the entire dataset which can make it faster for large datasets, and perhaps more robust to statistical noise.","ab661993":"# 4. DBSCAN","5c1d66ff":"In this case a reasonable grouping is found.","a77b7fdf":"# 2. Agglomerative Clustering Algo","83fbd863":"# 5. K-Means","cb200ba5":"# 8. Gaussian Mixture Model","f9d4ae94":"Balanced Iterative Reduncing and Clustering using Heirarchies.\n\nHere also, dendogram will be used, to find the number of clusters needed.\n\nThis algo is really useful for large databases.","5b5b3b48":"A synthetic dataset is a computer generated dataset. ","1819755a":"It is a part of Hierarchical clustering. In Hierarchical clustering we use dendogram to find the optimum number of clusters.","e5b7dca6":"# 1. Affinity Propagation Clustering Algo","83014330":"# 7. Mean Shift","1b412eb3":"In this case an excellent grouping is found."}}