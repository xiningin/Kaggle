{"cell_type":{"fb7bc4db":"code","f76073ad":"code","ce62f006":"code","42159c48":"code","1da4c31d":"code","bf46da6d":"code","3fc3674d":"code","d4d16c57":"code","3a9653ee":"code","a0def6f6":"code","88ab74c3":"code","d276769b":"code","a9db5dd1":"code","f6c98af7":"code","2975a29d":"code","5845a6c2":"code","d91d21b2":"code","6fc6a3ed":"code","a6ceb023":"code","60f7336d":"code","aefd1cca":"code","34c538e3":"code","611922ec":"code","8900b127":"code","023c8cfb":"code","db07008f":"code","4a7eea15":"code","a4f221a3":"code","80e60a40":"code","59d36700":"code","66438ae1":"code","b8c13f07":"code","64b97b89":"code","1503c4a4":"code","045ad63a":"code","77960dfe":"code","443a9ac4":"code","c5c0d369":"code","7522a994":"code","e737cbea":"code","91fcb819":"code","2b1c9186":"code","ff587318":"code","4981bb87":"code","fa977e12":"code","a3241af8":"code","a0458a84":"code","ac9ef72f":"code","878bbb6f":"code","f80053fc":"code","40f642c5":"code","8d004e93":"code","a2d31a6f":"code","121d7c90":"code","086b493e":"code","88b0b6ad":"code","71bf7add":"code","d5e0a334":"code","be08c5c4":"code","09e90bfa":"code","53f27516":"code","b8698c7b":"code","fd2a00b6":"code","f6fb1fa1":"code","972a1bff":"code","23ece0c3":"code","391b68dd":"code","6b67b45d":"code","f15f258f":"code","5558ea2f":"code","9f6da6ab":"code","c9f94d26":"markdown","d44077d1":"markdown","b11d7b5e":"markdown","2a366f44":"markdown","92eb5460":"markdown","c9ec0739":"markdown","b9dcf59b":"markdown","07d6b6b3":"markdown","5f224531":"markdown","0bbc57c8":"markdown","05081b43":"markdown","74ebebff":"markdown","9ff3ffe0":"markdown","9288fbc7":"markdown","0dc0a91a":"markdown","e30414c5":"markdown","91e73f65":"markdown","d4b64bd8":"markdown","24d55049":"markdown","43e19f41":"markdown","57d5de49":"markdown","96f3b40d":"markdown","7ec401f5":"markdown","15fb6fe8":"markdown","b4a032d3":"markdown","7626918f":"markdown","a8bb9bce":"markdown","19ae16cd":"markdown","f7170565":"markdown","c4d2747c":"markdown","8829ba4e":"markdown","73050b8b":"markdown","413be85f":"markdown","7a35989a":"markdown","1dd2fd9c":"markdown","97127b86":"markdown","271c219b":"markdown","bd6c2c65":"markdown","5f2bfff6":"markdown","7f5eb742":"markdown","afdf3396":"markdown","a8824632":"markdown","280d19ea":"markdown","31b2dec8":"markdown","45189190":"markdown","842af67f":"markdown","db6d431e":"markdown","df17999f":"markdown","05362c16":"markdown","d628d229":"markdown"},"source":{"fb7bc4db":"# NumPy for numerical computing\nimport numpy as np\n\n# Pandas for DataFrames\nimport pandas as pd\n\n# Matplotlib for visualization\nfrom matplotlib import pyplot as plt\n# display plots in the notebook\n%matplotlib inline\n# import color maps\nfrom matplotlib.colors import ListedColormap\n\n# Seaborn for easier visualization\nimport seaborn as sns\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom math import sqrt\n\n# Function for splitting training and test set\nfrom sklearn.model_selection import train_test_split\n\n# Function to perform data standardization \nfrom sklearn.preprocessing import StandardScaler\n\n# Libraries to perform hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Import classes for ML Models\nfrom sklearn.linear_model import Ridge  ## Linear Regression + L2 regularization\nfrom sklearn.svm import SVR ## Support Vector Regressor\nfrom sklearn.ensemble import RandomForestRegressor ## Random Forest Regressor\nfrom sklearn.neighbors import KNeighborsRegressor ## KNN regressor\nfrom sklearn.tree import DecisionTreeRegressor ## Decision Tree Regressor\nfrom sklearn import linear_model ## Lasso Regressor\n\n# Evaluation Metrics\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error as mae\n\n# To save the final model on disk\nfrom sklearn.externals import joblib","f76073ad":"df = pd.read_csv('..\/input\/BlackFriday.csv')","ce62f006":"df.shape","42159c48":"df.head()","1da4c31d":"df.dtypes[df.dtypes=='object']","bf46da6d":"# Plot histogram grid\ndf.hist(figsize=(15,15), xrot=-45) ## Display the labels rotated by 45 degress\n\n# Clear the text \"residue\"\nplt.show()","3fc3674d":"df.describe()","d4d16c57":"df.describe(include=['object'])","3a9653ee":"plt.figure(figsize=(10,8))\nsns.countplot(y='Age', data=df)","a0def6f6":"plt.figure(figsize=(10,8))\nsns.countplot(y='Gender', data=df)","88ab74c3":"plt.figure(figsize=(10,8))\nsns.countplot(y='City_Category', data=df)","d276769b":"plt.figure(figsize=(10,8))\nsns.countplot(y='Stay_In_Current_City_Years', data=df)","a9db5dd1":"df.corr()","f6c98af7":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr())","2975a29d":"mask=np.zeros_like(df.corr())\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(15,10))\nwith sns.axes_style(\"white\"):\n    ax = sns.heatmap(df.corr()*100, mask=mask, fmt='.0f', annot=True, lw=1, cmap=ListedColormap(['green', 'yellow', 'red','blue']))","5845a6c2":"df = df.drop_duplicates()\nprint( df.shape )","d91d21b2":"df.Product_Category_2.unique()","6fc6a3ed":"df.Product_Category_2.fillna(0, inplace=True)","a6ceb023":"df.Product_Category_2.unique()","60f7336d":"df.Product_Category_3.unique()","aefd1cca":"df.Product_Category_3.fillna(0, inplace=True)","34c538e3":"# Display number of missing values by numeric feature\ndf.select_dtypes(exclude=['object']).isnull().sum()","611922ec":"# female: 0 and male: 1\ndef gender(x):\n    if x=='M':\n        return 1\n    return 0\n\ndf['Gender']=df['Gender'].map(gender)","8900b127":"# Defining different age groups\ndef agegroup(x):\n    if x=='0-17':\n        return 1\n    elif x=='18-25':\n        return 2\n    elif x ==  \"26-35\" :\n        return 3\n    elif x ==  \"36-45\" :\n        return 4\n    elif x ==  \"46-50\" :\n        return 5\n    elif x ==  \"51-55\" :\n        return 6\n    elif x ==  \"55+\" :\n        return 7\n    else:\n        return 0\n    \ndf['AgeGroup']=df['Age'].map(agegroup)","023c8cfb":"df.drop(['Age'],axis=1,inplace=True)","db07008f":"df['Bachelor']=((df.AgeGroup == 2) & (df.Marital_Status == 0) & (df.Gender == 1)).astype(int)","4a7eea15":"# Display percent of rows where Bachelor == 1\ndf[df['Bachelor']==1].shape[0]\/df.shape[0]","a4f221a3":"from sklearn.preprocessing import LabelEncoder","80e60a40":"P = LabelEncoder()\ndf['Product_ID'] = P.fit_transform(df['Product_ID'])\nU = LabelEncoder()\ndf['User_ID'] = P.fit_transform(df['User_ID'])","59d36700":"# Create a new dataframe with dummy variables for for our categorical features.\ndf = pd.get_dummies(df, columns=['City_Category', 'Stay_In_Current_City_Years'])","66438ae1":"df.head()","b8c13f07":"df.shape","64b97b89":"df.shape","1503c4a4":"sample_df = df.sample(n=50000,random_state=100)","045ad63a":"# Create separate object for target variable\ny = sample_df.Purchase\n# Create separate object for input features\nX = sample_df.drop('Purchase', axis=1)","77960dfe":"# Split X and y into train and test sets: 80-20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)","443a9ac4":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","c5c0d369":"train_mean = X_train.mean()\ntrain_std = X_train.std()","7522a994":"## Standardize the train data set\nX_train = (X_train - train_mean) \/ train_std","e737cbea":"## Check for mean and std dev.\nX_train.describe()","91fcb819":"## Note: We use train_mean and train_std_dev to standardize test data set\nX_test = (X_test - train_mean) \/ train_std","2b1c9186":"## Check for mean and std dev. - not exactly 0 and 1\nX_test.describe()","ff587318":"## Predict Train results\ny_train_pred = np.ones(y_train.shape[0])*y_train.mean()","4981bb87":"## Predict Test results\ny_pred = np.ones(y_test.shape[0])*y_train.mean()","fa977e12":"print(\"Train Results for Baseline Model:\")\nprint(\"*******************************\")\nprint(\"Root mean squared error: \", sqrt(mse(y_train.values, y_train_pred)))\nprint(\"R-squared: \", r2_score(y_train.values, y_train_pred))\nprint(\"Mean Absolute Error: \", mae(y_train.values, y_train_pred))","a3241af8":"print(\"Results for Baseline Model:\")\nprint(\"*******************************\")\nprint(\"Root mean squared error: \", sqrt(mse(y_test, y_pred)))\nprint(\"R-squared: \", r2_score(y_test, y_pred))\nprint(\"Mean Absolute Error: \", mae(y_test, y_pred))","a0458a84":"tuned_params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}\nmodel = GridSearchCV(Ridge(), tuned_params, scoring = 'neg_mean_absolute_error', cv=10, n_jobs=-1)\nmodel.fit(X_train, y_train)","ac9ef72f":"model.best_estimator_","878bbb6f":"## Predict Train results\ny_train_pred = model.predict(X_train)","f80053fc":"## Predict Test results\ny_pred = model.predict(X_test)","40f642c5":"print(\"Train Results for Ridge Regression:\")\nprint(\"*******************************\")\nprint(\"Root mean squared error: \", sqrt(mse(y_train.values, y_train_pred)))\nprint(\"R-squared: \", r2_score(y_train.values, y_train_pred))\nprint(\"Mean Absolute Error: \", mae(y_train.values, y_train_pred))","8d004e93":"print(\"Test Results for Ridge Regression:\")\nprint(\"*******************************\")\nprint(\"Root mean squared error: \", sqrt(mse(y_test, y_pred)))\nprint(\"R-squared: \", r2_score(y_test, y_pred))\nprint(\"Mean Absolute Error: \", mae(y_test, y_pred))","a2d31a6f":"## Building the model again with the best hyperparameters\nmodel = Ridge(alpha=0.0001)\nmodel.fit(X_train, y_train)","121d7c90":"indices = np.argsort(-abs(model.coef_))\nprint(\"The features in order of importance are:\")\nprint(50*'-')\nfor feature in X.columns[indices]:\n    print(feature)","086b493e":"tuned_params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}\nmodel = GridSearchCV(linear_model.Lasso(), tuned_params, scoring = 'neg_mean_absolute_error', cv=10, n_jobs=-1)\nmodel.fit(X_train, y_train)","88b0b6ad":"model.best_estimator_","71bf7add":"## Predict Train results\ny_train_pred = model.predict(X_train)","d5e0a334":"## Predict Test results\ny_pred=model.predict(X_test)","be08c5c4":"print(\"Train Results for Lasso Regression:\")\nprint(\"*******************************\")\nprint(\"Root mean squared error: \", sqrt(mse(y_train.values, y_train_pred)))\nprint(\"R-squared: \", r2_score(y_train.values, y_train_pred))\nprint(\"Mean Absolute Error: \", mae(y_train.values, y_train_pred))","09e90bfa":"print(\"Test Results for Lasso Regression:\")\nprint(\"*******************************\")\nprint(\"Root mean squared error: \", sqrt(mse(y_test, y_pred)))\nprint(\"R-squared: \", r2_score(y_test, y_pred))\nprint(\"Mean Absolute Error: \", mae(y_test, y_pred))","53f27516":"## Building the model again with the best hyperparameters\nmodel = linear_model.Lasso(alpha=0.0001)\nmodel.fit(X_train, y_train)","b8698c7b":"indices = np.argsort(-abs(model.coef_))\nprint(\"The features in order of importance are:\")\nprint(50*'-')\nfor feature in X.columns[indices]:\n    print(feature)","fd2a00b6":"tuned_params = {'n_estimators': [100, 200, 300, 400, 500], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\nmodel = RandomizedSearchCV(RandomForestRegressor(), tuned_params, n_iter=20, scoring = 'neg_mean_absolute_error', cv=5, n_jobs=-1)\nmodel.fit(X_train, y_train)","f6fb1fa1":"model.best_estimator_","972a1bff":"## Predict Train results\ny_train_pred = model.predict(X_train)","23ece0c3":"## Predict Test results\ny_pred = model.predict(X_test)","391b68dd":"print(\"Train Results for Random Forest Regression:\")\nprint(\"*******************************\")\nprint(\"Root mean squared error: \", sqrt(mse(y_train.values, y_train_pred)))\nprint(\"R-squared: \", r2_score(y_train.values, y_train_pred))\nprint(\"Mean Absolute Error: \", mae(y_train.values, y_train_pred))","6b67b45d":"print(\"Test Results for Random Forest Regression:\")\nprint(\"*******************************\")\nprint(\"Root mean squared error: \", sqrt(mse(y_test, y_pred)))\nprint(\"R-squared: \", r2_score(y_test, y_pred))\nprint(\"Mean Absolute Error: \", mae(y_test, y_pred))","f15f258f":"## Building the model again with the best hyperparameters\nmodel = RandomForestRegressor(n_estimators=200, min_samples_split=10, min_samples_leaf=4)\nmodel.fit(X_train, y_train)","5558ea2f":"indices = np.argsort(-model.feature_importances_)\nprint(\"The features in order of importance are:\")\nprint(50*'-')\nfor feature in X.columns[indices]:\n    print(feature)","9f6da6ab":"joblib.dump(model, 'rfr_BlackFriday.pkl') ","c9f94d26":"### Dropping the duplicates (De-duplication)","d44077d1":"Since this is an enormous dataset, my machine won't be capable enough to run machine learning models.Therefore lets take a sample of 50000 data points for evaluation.","b11d7b5e":"### Target Variable\nIn this approach:\n* Purchase- the purchase ammount","2a366f44":"#### Observations\n* The classes 26-35, 36-45, 18-25 are quite prevalent in the dataset.","92eb5460":"#### Filtering the data","c9ec0739":"Display the first 5 rows","b9dcf59b":"* In this model, for every test data point, we will simply predict the average of the train labels as the output.\n* We will use this simple model to perform hypothesis testing for other complex models.","07d6b6b3":"#### Test Train Split","5f224531":"* Finally, let's take a look at the relationships between numeric features and other numeric features.\n* ***Correlation*** is a value between -1 and 1 that represents how closely values for two separate features move in unison.\n* Positive correlation means that as one feature increases, the other increases; eg. a child's age and her height.\n* Negative correlation means that as one feature increases, the other decreases; eg. hours spent studying and number of parties attended.\n* Correlations near -1 or 1 indicate a strong relationship.\n* Those closer to 0 indicate a weak relationship.\n* 0 indicates no relationship.","0bbc57c8":"#### Feature Importance","05081b43":"## Handling the missing values","74ebebff":"**What to look for?**\n* The colorbar on the right explains the meaning of the heatmap - Dark colors indicate **strong negative correlations** and light colors indicate **strong positive correlations**.\n* Perhaps the most helpful way to interpret this correlation heatmap is to first find features that are correlated with our target variable by scanning the last column.\n* In this case, it doesn't look like many features are strongly correlated with the target variable.\n* Seems like there is negative correlation between the columns 'Purchase' and 'Product_Category_1'.","9ff3ffe0":"#### Feature Importance","9288fbc7":"## Feature Engineering","0dc0a91a":"## Acknowledgements\nThe dataset comes from a competition hosted by Analytics Vidhya.","e30414c5":"#### Encoding Dummy Variables","91e73f65":"### Saving the Winning model to disk","d4b64bd8":"## Machine Learning Models","24d55049":"## Correlations","43e19f41":"#### Indicator Variables","57d5de49":"## Distribution of numerical features","96f3b40d":"Looks like our data had no duplicates.","7ec401f5":"Some features are numerical and some are categorical","15fb6fe8":"## Distribution of Categorical features","b4a032d3":"Almost 11% of the data contains bachelors.This information will be helpful to suggest products for bachelors","7626918f":"#### Data Standardization\n* In data standardization we perform zero mean centering. i.e. we make the mean of all the features 0 and the standard deviation as 1.","a8bb9bce":"### Model 4: Random Forest Regression","19ae16cd":"#### Observations\n* Most customers are from city B","f7170565":"#### Observations\nFrom the hostagram of Product_category_1\n* There are most unmarriedd customers in the dataset.\n* The product 5 are most bought by the customers.\n* And so are product 1 and 8.\n\nThis information can be used to know about the products which have a high demand.","c4d2747c":"### Model 3: Lasso Regression","8829ba4e":"* Feature engineering is finding out new features from the existing ones.\n* This helps us isolating key information.","73050b8b":"* Before feeding the data to the machine learning algorithm, we need to convert categorical features into numerical features.\n* Therefore we need to create dummy variables for our categorical features.","413be85f":"### Model 1: Baseline Model","7a35989a":"#### Data Preparation","1dd2fd9c":"#### Observations \n* Most customers are males","97127b86":"* We know that bachelors are found mostly in the range 25 to 35.\n* Therefore we can make a feature combining the 'AgeGroup','Gender' and 'Marital_Status'","271c219b":"## Problem Description\nThe dataset here is a sample of the transactions made in a retail store. The store wants to know better the customer purchase behaviour against different products. Specifically, here the problem is a regression problem where we are trying to predict the dependent variable (the amount of purchase) with the help of the information contained in the other variables.\n\nClassification problem can also be settled in this dataset since several variables are categorical, and some other approaches could be \"Predicting the age of the consumer\" or even \"Predict the category of goods bought\". This dataset is also particularly convenient for clustering and maybe find different clusters of consumers within it.","bd6c2c65":"# Business Problem","5f2bfff6":"* There were missing values in the columns 'Product_Category_1' and 'Product_Category_2'.","7f5eb742":"Display the dimension of the dataset","afdf3396":"### Model 2: Ridge Regression","a8824632":"## Data Cleaning","280d19ea":"### Other Features\n* User_ID- unique id of the user\n* Product_ID- unique id of the product\n* Gender- male or female\n* Age- age category the customer belongs to\n* Occupation- Occupation of the customer\n* City_Category-city category the customer resides in\n* Stay_In_Current_City_Years- no. of years the customer has resided in the current city\n* Marital_Status- married or unmarried\n* Product_Category_1- products of category 1 \n* Product_Category_2- products of category 2\n* Product_Category_3- products of category 3\n* Purchase- Purchase amount in dollars","31b2dec8":"#### Importing the libraries","45189190":"#### Loading data from csv","842af67f":"## Exploratory Data Analysis","db6d431e":"#### Observations\n* Columns Product_Category_2 and Product_Category_3 have missing values.\n* Marital status has min and max values 0 and 1.Therefore, this might be a binary feature.","df17999f":"We don't have any numerical features with missing values","05362c16":"## Data Overview\nDataset of 550 000 observations about the black Friday in a retail store, it contains different kinds of variables either numerical or categorical. It contains missing values.","d628d229":"#### Observations\n* There are no missing values in categorical features.\n* There are 3623 unique products.\n* Most purchases have occured from the age group of 26 to 35 among the 7 unique age groups.\n\nThis information can be used to give better suggestions on products if the age of the customers are provided.\n* Most purchases have occured from the city category 'B'.\n\nThis information can be used to give better suggestions on products if the city category is provided."}}