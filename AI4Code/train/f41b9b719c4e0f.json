{"cell_type":{"0f4947f9":"code","847dc296":"code","c39a5c67":"code","b49d360f":"code","dae1482f":"code","32fb868c":"code","beb28ed2":"code","f3ebcc13":"code","26f95908":"code","a8d7f04b":"code","5e164e16":"code","c54fe813":"code","663d646f":"code","895560d3":"code","5ab77f17":"code","d3dff1b1":"code","d7563459":"code","fe8f467f":"code","f846831e":"code","8cf0c75f":"code","af45b08f":"code","44e47f13":"code","c2d69261":"code","9105034b":"code","48f28a96":"code","aca14f8a":"code","0f71fb7e":"code","a1fd0e56":"code","c8983779":"code","c2e6650d":"code","90364baa":"code","e38e661c":"code","4b65173d":"code","5a95f534":"code","f73dd1cd":"code","f9664ab4":"code","ae51958c":"code","55d05dbd":"code","b9e842c5":"code","a1412403":"code","f07634b9":"markdown","856eca31":"markdown","1ef1bd4c":"markdown","2d928ead":"markdown","887ba437":"markdown","8ed2c9e9":"markdown","99e852b0":"markdown","e644fd76":"markdown","068f52fd":"markdown","8200db67":"markdown","0f1e4cfa":"markdown","1d34f7b6":"markdown","ec720684":"markdown","71dbd329":"markdown","0a99f6a4":"markdown"},"source":{"0f4947f9":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\nimport glob\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adamax\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import LeakyReLU, Reshape\nfrom tensorflow.keras.layers import Dropout, Concatenate\nfrom tensorflow.keras.layers import Embedding, Dense, Flatten\nfrom tensorflow.keras.layers import Input, BatchNormalization","847dc296":"def rmspe(y_true, y_pred):\n    return (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","c39a5c67":"def rmspe_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, dtype=tf.float32)\n    y_pred = tf.cast(y_pred, dtype=tf.float32)\n    return (tf.math.sqrt(tf.reduce_mean(tf.math.square((y_true - y_pred) \/ y_true))))","b49d360f":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()","dae1482f":"def realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","32fb868c":"def count_unique(series):\n    return len(np.unique(series))","beb28ed2":"def get_stats_window(df, fe_dict, seconds_in_bucket, add_suffix = False):\n    df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    \n    if add_suffix:\n        df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n    \n    return df_feature","f3ebcc13":"def process_trade_data(trade_files):\n    \n    trade_df = pd.DataFrame()\n    \n    for file in tqdm(glob.glob(trade_files)):\n        \n        # Read source file\n        df_trade_data = pd.read_parquet(file)\n        \n        # Feature engineering\n        df_trade_data['log_return'] = df_trade_data.groupby('time_id')['price'].apply(log_return)\n        #df_trade_data.fillna(0, inplace=True)\n        \n        fet_engg_dict = {\n            'price': ['mean','std','sum'],\n            'size': ['mean','std','sum'],\n            'order_count': ['mean','std','sum'],\n            'seconds_in_bucket': [count_unique],\n            'log_return': [realized_volatility,'mean','std','sum']\n        }\n        \n        # Get the stats for different windows\n        df_feature = get_stats_window(df_trade_data, fet_engg_dict, seconds_in_bucket = 0, add_suffix = False)\n        df_feature_120 = get_stats_window(df_trade_data, fet_engg_dict, seconds_in_bucket = 120, add_suffix = True)\n        df_feature_240 = get_stats_window(df_trade_data, fet_engg_dict, seconds_in_bucket = 240, add_suffix = True)\n        df_feature_360 = get_stats_window(df_trade_data, fet_engg_dict, seconds_in_bucket = 360, add_suffix = True)\n        df_feature_480 = get_stats_window(df_trade_data, fet_engg_dict, seconds_in_bucket = 480, add_suffix = True)\n        \n        # Merge all\n        trade_agg_df = df_feature.merge(df_feature_120, how = 'left', left_on = 'time_id_', right_on = 'time_id__120')\n        trade_agg_df = trade_agg_df.merge(df_feature_240, how = 'left', left_on = 'time_id_', right_on = 'time_id__240')\n        trade_agg_df = trade_agg_df.merge(df_feature_360, how = 'left', left_on = 'time_id_', right_on = 'time_id__360')\n        trade_agg_df = trade_agg_df.merge(df_feature_480, how = 'left', left_on = 'time_id_', right_on = 'time_id__480')\n        trade_agg_df = trade_agg_df.add_prefix('trade_')\n        \n        # Generate row_id\n        stock_id = file.split('=')[1]\n        trade_agg_df['row_id'] = trade_agg_df['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n        trade_agg_df.drop(['trade_time_id_'], inplace=True, axis=1)\n        \n        # Merge with parent df\n        trade_df = pd.concat([trade_df, trade_agg_df])\n    \n    del df_trade_data, trade_agg_df, df_feature\n    del df_feature_120, df_feature_240\n    del df_feature_480, df_feature_360\n    gc.collect()\n    \n    return trade_df","26f95908":"def process_book_data(book_files):\n    \n    book_df = pd.DataFrame()\n    \n    for file in tqdm(glob.glob(book_files)):\n        \n        # Read source file\n        df_book_data = pd.read_parquet(file)\n        \n        # Feature engineering\n        df_book_data['wap1'] = (df_book_data['bid_price1'] *\n                                df_book_data['ask_size1'] +\n                                df_book_data['ask_price1'] *\n                                df_book_data['bid_size1'])  \/ (df_book_data['bid_size1'] +\n                                                               df_book_data['ask_size1'])\n\n        df_book_data['wap2'] = (df_book_data['bid_price2'] *\n                                df_book_data['ask_size2'] +\n                                df_book_data['ask_price2'] *\n                                df_book_data['bid_size2'])  \/ (df_book_data['bid_size2'] +\n                                                               df_book_data['ask_size2'])\n        \n        df_book_data['wap3'] = (df_book_data['bid_price1'] *\n                                df_book_data['bid_size1'] +\n                                df_book_data['ask_price1'] *\n                                df_book_data['ask_size1'])  \/ (df_book_data['bid_size1'] +\n                                                               df_book_data['ask_size1'])\n\n        df_book_data['wap4'] = (df_book_data['bid_price2'] *\n                                df_book_data['bid_size2'] +\n                                df_book_data['ask_price2'] *\n                                df_book_data['ask_size2'])  \/ (df_book_data['bid_size2'] +\n                                                               df_book_data['ask_size2'])\n\n        df_book_data['log_return1'] = df_book_data.groupby(['time_id'])['wap1'].apply(log_return)\n        df_book_data['log_return2'] = df_book_data.groupby(['time_id'])['wap2'].apply(log_return)\n        df_book_data['log_return3'] = df_book_data.groupby(['time_id'])['wap3'].apply(log_return)\n        df_book_data['log_return4'] = df_book_data.groupby(['time_id'])['wap4'].apply(log_return)\n        #df_book_data.fillna(0, inplace=True)\n        \n        df_book_data['wap_balance'] = abs(df_book_data['wap1'] - df_book_data['wap2'])\n        df_book_data['price_spread1'] = (df_book_data['ask_price1'] - df_book_data['bid_price1']) \/ ((df_book_data['ask_price1'] + df_book_data['bid_price1'])\/2)\n        df_book_data['price_spread2'] = (df_book_data['ask_price2'] - df_book_data['bid_price2']) \/ ((df_book_data['ask_price2'] + df_book_data['bid_price2'])\/2)\n        df_book_data['bid_spread'] = df_book_data['bid_price1'] - df_book_data['bid_price2']\n        df_book_data['ask_spread'] = df_book_data['ask_price1'] - df_book_data['ask_price2']\n        df_book_data['bid_ask_spread1'] = abs((df_book_data['bid_price1'] * df_book_data['bid_size1']) - (df_book_data['ask_price1'] * df_book_data['ask_size1']))\n        df_book_data['bid_ask_spread2'] = abs((df_book_data['bid_price2'] * df_book_data['bid_size2']) - (df_book_data['ask_price2'] * df_book_data['ask_size2']))\n        df_book_data['total_volume'] = (df_book_data['ask_size1'] + df_book_data['ask_size2']) + (df_book_data['bid_size1'] + df_book_data['bid_size2'])\n        df_book_data['volume_imbalance'] = abs((df_book_data['ask_size1'] + df_book_data['ask_size2']) - (df_book_data['bid_size1'] + df_book_data['bid_size2']))\n        \n        fet_engg_dict = {\n            'wap1': ['mean','std','sum'],\n            'wap2': ['mean','std','sum'],\n            'wap3': ['mean','std','sum'],\n            'wap4': ['mean','std','sum'],\n            'log_return1': [realized_volatility,'mean','std','sum'],\n            'log_return2': [realized_volatility,'mean','std','sum'],\n            'log_return3': [realized_volatility,'mean','std','sum'],\n            'log_return4': [realized_volatility,'mean','std','sum'],\n            'wap_balance': ['mean','std','sum'],\n            'price_spread1': ['mean','std','sum'],\n            'price_spread2': ['mean','std','sum'],\n            'bid_spread': ['mean','std','sum'],\n            'ask_spread': ['mean','std','sum'],\n            'bid_ask_spread1': ['mean','std','sum'],\n            'bid_ask_spread2': ['mean','std','sum'],\n            'total_volume': ['mean','std','sum'],\n            'volume_imbalance': ['mean','std','sum']\n        }\n        \n        # Get the stats for different windows\n        df_feature = get_stats_window(df_book_data, fet_engg_dict, seconds_in_bucket = 0, add_suffix = False)\n        df_feature_120 = get_stats_window(df_book_data, fet_engg_dict, seconds_in_bucket = 120, add_suffix = True)\n        df_feature_240 = get_stats_window(df_book_data, fet_engg_dict, seconds_in_bucket = 240, add_suffix = True)\n        df_feature_360 = get_stats_window(df_book_data, fet_engg_dict, seconds_in_bucket = 360, add_suffix = True)\n        df_feature_480 = get_stats_window(df_book_data, fet_engg_dict, seconds_in_bucket = 480, add_suffix = True)\n\n        # Merge all\n        book_agg_df = df_feature.merge(df_feature_120, how = 'left', left_on = 'time_id_', right_on = 'time_id__120')\n        book_agg_df = book_agg_df.merge(df_feature_240, how = 'left', left_on = 'time_id_', right_on = 'time_id__240')\n        book_agg_df = book_agg_df.merge(df_feature_360, how = 'left', left_on = 'time_id_', right_on = 'time_id__360')\n        book_agg_df = book_agg_df.merge(df_feature_480, how = 'left', left_on = 'time_id_', right_on = 'time_id__480')\n        book_agg_df = book_agg_df.add_prefix('book_')\n        \n        # Generate row_id\n        stock_id = file.split('=')[1]\n        book_agg_df['row_id'] = book_agg_df['book_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n        book_agg_df.drop(['book_time_id_'], inplace=True, axis=1)\n        \n        # Merge with parent df\n        book_df = pd.concat([book_df, book_agg_df])\n    \n    del df_book_data, book_agg_df, df_feature\n    del df_feature_120, df_feature_240\n    del df_feature_360, df_feature_480\n    gc.collect()\n    \n    return book_df","a8d7f04b":"with open(\"..\/input\/orvp-django-unchained\/ORVP_Ready_Meatballs.txt\", 'rb') as handle: \n    data = handle.read()\n\nprocessed_data = pickle.loads(data)\ntrain_df = processed_data['train_df']\n\ndel processed_data\ngc.collect()","5e164e16":"test_df = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\ntest_df['row_id'] = test_df['stock_id'].astype(str) + '-' + test_df['time_id'].astype(str)\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","c54fe813":"trade_test_df = process_trade_data('..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\/*')\nprint(f\"trade_test_df: {trade_test_df.shape}\")\ntrade_test_df.head()","663d646f":"test_df = pd.merge(test_df, trade_test_df, \n                   how='left', on='row_id', \n                   sort=False)\n\n#test_df.fillna(0, inplace=True)\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","895560d3":"book_test_df = process_book_data('..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*')\nprint(f\"book_test_df: {book_test_df.shape}\")\nbook_test_df.head()","5ab77f17":"test_df = pd.merge(test_df, book_test_df, \n                   how='left', on='row_id', \n                   sort=False)\n\n#test_df.fillna(0, inplace=True)\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","d3dff1b1":"vol_cols = []\nfor col in test_df.columns:\n    if 'realized_volatility' in col:\n        vol_cols.append(col)\n\nlen(vol_cols)","d7563459":"df_stock_id = test_df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min','sum']).reset_index()\ndf_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\ndf_stock_id = df_stock_id.add_suffix('_stock')\ndf_stock_id.head()","fe8f467f":"df_time_id = test_df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min','sum']).reset_index()\ndf_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\ndf_time_id = df_time_id.add_suffix('_time')\ndf_time_id.head()","f846831e":"test_df = test_df.merge(df_stock_id, how='left', \n                          left_on=['stock_id'], \n                          right_on=['stock_id__stock'])\n\ntest_df = test_df.merge(df_time_id, how='left', \n                          left_on=['time_id'], \n                          right_on=['time_id__time'])\n\ntest_df.drop(['stock_id__stock', 'time_id__time'], \n              axis = 1, inplace = True)\n\ndel df_stock_id, df_time_id\ngc.collect()\n\n#test_df.fillna(0, inplace=True)\nprint(f\"test_df: {test_df.shape}\")","8cf0c75f":"test_df['size_tau'] = np.sqrt(1\/test_df['trade_seconds_in_bucket_count_unique'])\ntest_df['size_tau_120'] = np.sqrt(1\/test_df['trade_seconds_in_bucket_count_unique_120'])\ntest_df['size_tau_240'] = np.sqrt(1\/test_df['trade_seconds_in_bucket_count_unique_240'])\ntest_df['size_tau_360'] = np.sqrt(1\/test_df['trade_seconds_in_bucket_count_unique_360'])\ntest_df['size_tau_480'] = np.sqrt(1\/test_df['trade_seconds_in_bucket_count_unique_480'])\n\ntest_df['size_tau2'] = np.sqrt(1\/test_df['trade_order_count_sum'])\ntest_df['size_tau2_120'] = np.sqrt(0.8\/test_df['trade_order_count_sum'])\ntest_df['size_tau2_240'] = np.sqrt(0.6\/test_df['trade_order_count_sum'])\ntest_df['size_tau2_360'] = np.sqrt(0.4\/test_df['trade_order_count_sum'])\ntest_df['size_tau2_480'] = np.sqrt(0.2\/test_df['trade_order_count_sum'])\n\ntest_df['size_tau3'] = np.sqrt(1\/test_df['trade_order_count_mean'])\ntest_df['size_tau3_120'] = np.sqrt(0.8\/test_df['trade_order_count_mean'])\ntest_df['size_tau3_240'] = np.sqrt(0.6\/test_df['trade_order_count_mean'])\ntest_df['size_tau3_360'] = np.sqrt(0.4\/test_df['trade_order_count_mean'])\ntest_df['size_tau3_480'] = np.sqrt(0.2\/test_df['trade_order_count_mean'])\n\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","af45b08f":"filter_cols = ['trade_time_id__120', 'trade_time_id__240', 'trade_time_id__360', \n               'trade_time_id__480', 'book_time_id__120', 'book_time_id__240', \n               'book_time_id__360', 'book_time_id__480']\n\ntest_df.drop(filter_cols, axis=1, inplace=True)\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","44e47f13":"train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n\nl = []\nfor n in range(7):\n    l.append([(x-1) for x in ((ids+1)*(kmeans.labels_ == n)) if x > 0])\n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in tqdm(l):\n    newDf = train_df.loc[train_df['stock_id'].isin(ind)]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append(newDf)\n    \n    newDf = test_df.loc[test_df['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append(newDf)\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\nmat2 = pd.concat(matTest).reset_index()\n\nmat2 = pd.concat([mat2, mat1.loc[mat1.time_id==5]])\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","c2d69261":"nnn = [\n    'time_id',\n    'book_log_return1_realized_volatility_0c1',\n    'book_log_return1_realized_volatility_1c1',\n    'book_log_return1_realized_volatility_2c1',\n    'book_log_return1_realized_volatility_3c1',\n    'book_log_return1_realized_volatility_4c1',\n    'book_log_return1_realized_volatility_5c1',\n    'book_log_return1_realized_volatility_6c1',\n    'book_total_volume_sum_0c1',\n    'book_total_volume_sum_1c1',\n    'book_total_volume_sum_2c1',\n    'book_total_volume_sum_3c1',\n    'book_total_volume_sum_4c1',\n    'book_total_volume_sum_5c1',\n    'book_total_volume_sum_6c1',\n    'trade_size_sum_0c1',\n    'trade_size_sum_1c1',\n    'trade_size_sum_2c1',\n    'trade_size_sum_3c1',\n    'trade_size_sum_4c1',\n    'trade_size_sum_5c1',\n    'trade_size_sum_6c1',\n    'trade_order_count_sum_0c1',\n    'trade_order_count_sum_1c1',\n    'trade_order_count_sum_2c1',\n    'trade_order_count_sum_3c1',\n    'trade_order_count_sum_4c1',\n    'trade_order_count_sum_5c1',\n    'trade_order_count_sum_6c1',\n    'book_price_spread2_sum_0c1',\n    'book_price_spread2_sum_1c1',\n    'book_price_spread2_sum_2c1',\n    'book_price_spread2_sum_3c1',\n    'book_price_spread2_sum_4c1',\n    'book_price_spread2_sum_5c1',\n    'book_price_spread2_sum_6c1',\n    'book_bid_spread_sum_0c1',\n    'book_bid_spread_sum_1c1',\n    'book_bid_spread_sum_2c1',\n    'book_bid_spread_sum_3c1',\n    'book_bid_spread_sum_4c1',\n    'book_bid_spread_sum_5c1',\n    'book_bid_spread_sum_6c1',\n    'book_ask_spread_sum_0c1',\n    'book_ask_spread_sum_1c1',\n    'book_ask_spread_sum_2c1',\n    'book_ask_spread_sum_3c1',\n    'book_ask_spread_sum_4c1',\n    'book_ask_spread_sum_5c1',\n    'book_ask_spread_sum_6c1',\n    'book_volume_imbalance_sum_0c1',\n    'book_volume_imbalance_sum_1c1',\n    'book_volume_imbalance_sum_2c1',\n    'book_volume_imbalance_sum_3c1',\n    'book_volume_imbalance_sum_4c1',\n    'book_volume_imbalance_sum_5c1',\n    'book_volume_imbalance_sum_6c1',\n    'book_bid_ask_spread2_sum_120_0c1',\n    'book_bid_ask_spread2_sum_120_1c1',\n    'book_bid_ask_spread2_sum_120_2c1',\n    'book_bid_ask_spread2_sum_120_3c1',\n    'book_bid_ask_spread2_sum_120_4c1',\n    'book_bid_ask_spread2_sum_120_5c1',\n    'book_bid_ask_spread2_sum_120_6c1',\n    'size_tau2_0c1',\n    'size_tau2_1c1',\n    'size_tau2_2c1',\n    'size_tau2_3c1',\n    'size_tau2_4c1',\n    'size_tau2_5c1',\n    'size_tau2_6c1'\n]\n\ntrain_df = pd.merge(train_df, mat1[nnn], how='left', on='time_id')\ntest_df = pd.merge(test_df, mat2[nnn], how='left', on='time_id')\n\ndel mat1, mat2\ngc.collect()\n\ntrain_df.shape, test_df.shape","9105034b":"# Get highly correlated columns to remove\n# df = train_df.loc[:, train_df.columns != 'target'].copy()\n# cor_matrix = df.corr().abs()\n# upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n# to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n# print(f\"Number of columns to drop: {len(to_drop)}\")","48f28a96":"select_cols = [\n    'stock_id', 'time_id', 'row_id', 'trade_price_mean', 'trade_price_std', 'trade_price_sum', 'trade_size_mean', 'trade_size_std', 'trade_size_sum', \n    'trade_order_count_mean', 'trade_order_count_std', 'trade_order_count_sum', 'trade_log_return_realized_volatility', 'trade_log_return_mean', \n    'trade_log_return_std', 'trade_log_return_sum', 'trade_order_count_std_120', 'trade_log_return_mean_120', 'trade_log_return_sum_120', 'trade_price_std_240', \n    'trade_size_mean_240', 'trade_order_count_std_240', 'trade_log_return_mean_240', 'trade_log_return_sum_240', 'trade_price_std_360', 'trade_size_mean_360', \n    'trade_size_std_360', 'trade_order_count_mean_360', 'trade_order_count_std_360', 'trade_log_return_mean_360', 'trade_log_return_sum_360', 'trade_price_std_480', \n    'trade_size_mean_480', 'trade_size_std_480', 'trade_size_sum_480', 'trade_order_count_mean_480', 'trade_order_count_std_480', \n    'trade_log_return_realized_volatility_480', 'trade_log_return_mean_480', 'trade_log_return_std_480', 'trade_log_return_sum_480', 'book_wap1_sum', \n    'book_log_return1_realized_volatility', 'book_log_return1_mean', 'book_log_return3_mean', 'book_log_return4_mean', 'book_wap_balance_mean', \n    'book_wap_balance_sum', 'book_price_spread1_std', 'book_price_spread2_sum', 'book_bid_spread_mean', 'book_bid_spread_std', 'book_bid_spread_sum', \n    'book_ask_spread_mean', 'book_ask_spread_std', 'book_ask_spread_sum', 'book_bid_ask_spread1_mean', 'book_bid_ask_spread1_std', 'book_bid_ask_spread2_mean', \n    'book_bid_ask_spread2_std', 'book_total_volume_mean', 'book_total_volume_std', 'book_volume_imbalance_mean', 'book_log_return1_mean_120', \n    'book_log_return2_mean_120', 'book_log_return3_mean_120', 'book_log_return4_mean_120', 'book_log_return1_mean_240', 'book_log_return2_mean_240', \n    'book_log_return3_mean_240', 'book_log_return4_mean_240', 'book_log_return1_mean_360', 'book_log_return2_mean_360', 'book_log_return3_mean_360', \n    'book_log_return4_mean_360', 'book_bid_ask_spread1_std_360', 'book_bid_ask_spread2_std_360', 'book_total_volume_std_360', 'book_volume_imbalance_std_360', \n    'book_wap1_std_480', 'book_log_return1_mean_480', 'book_log_return1_sum_480', 'book_log_return2_mean_480', 'book_log_return2_sum_480', \n    'book_log_return3_mean_480', 'book_log_return3_sum_480', 'book_log_return4_mean_480', 'book_log_return4_sum_480', 'book_price_spread1_std_480', \n    'book_price_spread2_std_480', 'book_bid_spread_std_480', 'book_ask_spread_std_480', 'book_bid_ask_spread1_std_480', 'book_bid_ask_spread2_std_480', \n    'book_total_volume_std_480', 'book_volume_imbalance_std_480', 'trade_log_return_realized_volatility_mean_stock', \n    'trade_log_return_realized_volatility_std_stock', 'trade_log_return_realized_volatility_max_stock', 'trade_log_return_realized_volatility_min_stock', \n    'trade_log_return_realized_volatility_120_max_stock', 'trade_log_return_realized_volatility_120_min_stock', \n    'trade_log_return_realized_volatility_240_max_stock', 'trade_log_return_realized_volatility_240_min_stock', \n    'trade_log_return_realized_volatility_360_max_stock', 'trade_log_return_realized_volatility_360_min_stock', \n    'trade_log_return_realized_volatility_480_max_stock', 'trade_log_return_realized_volatility_480_min_stock', \n    'book_log_return1_realized_volatility_mean_stock', 'book_log_return1_realized_volatility_std_stock', \n    'book_log_return1_realized_volatility_max_stock', 'book_log_return1_realized_volatility_min_stock', \n    'book_log_return2_realized_volatility_max_stock', 'book_log_return2_realized_volatility_min_stock', \n    'book_log_return3_realized_volatility_max_stock', 'book_log_return3_realized_volatility_min_stock', \n    'book_log_return4_realized_volatility_max_stock', 'book_log_return4_realized_volatility_min_stock', \n    'book_log_return1_realized_volatility_120_max_stock', 'book_log_return2_realized_volatility_120_min_stock', \n    'book_log_return1_realized_volatility_240_min_stock', 'book_log_return2_realized_volatility_240_min_stock', \n    'book_log_return3_realized_volatility_240_min_stock', 'book_log_return1_realized_volatility_360_min_stock', \n    'book_log_return2_realized_volatility_360_min_stock', 'book_log_return3_realized_volatility_360_min_stock', \n    'book_log_return4_realized_volatility_360_min_stock', 'book_log_return1_realized_volatility_480_max_stock', \n    'book_log_return1_realized_volatility_480_min_stock', 'book_log_return2_realized_volatility_480_max_stock', \n    'book_log_return2_realized_volatility_480_min_stock', 'book_log_return3_realized_volatility_480_max_stock', \n    'book_log_return3_realized_volatility_480_min_stock', 'book_log_return4_realized_volatility_480_max_stock', \n    'book_log_return4_realized_volatility_480_min_stock', 'trade_log_return_realized_volatility_mean_time', \n    'trade_log_return_realized_volatility_std_time', 'trade_log_return_realized_volatility_max_time', \n    'trade_log_return_realized_volatility_min_time', 'trade_log_return_realized_volatility_360_min_time', \n    'trade_log_return_realized_volatility_480_max_time', 'trade_log_return_realized_volatility_480_min_time', \n    'book_log_return1_realized_volatility_std_time', 'book_log_return1_realized_volatility_min_time', \n    'book_log_return2_realized_volatility_480_min_time', 'size_tau', 'size_tau_480', 'size_tau2', 'size_tau3',\n    'book_log_return1_realized_volatility_1c1', 'book_log_return1_realized_volatility_2c1', 'book_log_return1_realized_volatility_5c1',\n    'book_total_volume_sum_0c1', 'book_total_volume_sum_1c1', 'book_total_volume_sum_2c1', 'book_total_volume_sum_3c1', 'book_total_volume_sum_4c1', \n    'book_total_volume_sum_5c1', 'book_total_volume_sum_6c1', 'trade_size_sum_0c1', 'trade_size_sum_1c1', 'trade_size_sum_2c1', 'trade_size_sum_3c1', \n    'trade_size_sum_4c1', 'trade_size_sum_5c1', 'trade_size_sum_6c1', 'trade_order_count_sum_1c1', 'trade_order_count_sum_2c1', 'trade_order_count_sum_4c1', \n    'trade_order_count_sum_5c1', 'trade_order_count_sum_6c1', 'book_price_spread2_sum_2c1', 'book_price_spread2_sum_5c1', 'book_bid_spread_sum_1c1', \n    'book_bid_spread_sum_2c1', 'book_bid_spread_sum_5c1', 'book_ask_spread_sum_1c1', 'book_ask_spread_sum_2c1', 'book_ask_spread_sum_5c1', \n    'book_volume_imbalance_sum_0c1', 'book_volume_imbalance_sum_3c1', 'book_volume_imbalance_sum_4c1', 'book_volume_imbalance_sum_6c1', \n    'book_bid_ask_spread2_sum_120_0c1', 'book_bid_ask_spread2_sum_120_1c1', 'book_bid_ask_spread2_sum_120_2c1', 'book_bid_ask_spread2_sum_120_3c1', \n    'book_bid_ask_spread2_sum_120_4c1', 'book_bid_ask_spread2_sum_120_5c1', 'book_bid_ask_spread2_sum_120_6c1', 'size_tau2_0c1', 'size_tau2_1c1', \n    'size_tau2_2c1', 'size_tau2_3c1', 'size_tau2_4c1', 'size_tau2_5c1', 'size_tau2_6c1'\n]\n\nlen(select_cols)","aca14f8a":"Xtrain = train_df[select_cols].copy()\nYtrain = train_df['target'].copy()\nYtrain_strat = pd.qcut(train_df['target'].values, q=10, labels=range(0,10))\n\nXtrain.drop(['row_id'], axis=1, inplace=True)\nXtrain.replace([np.nan, np.inf, -np.inf], 0, inplace=True)\nprint(f\"Xtrain: {Xtrain.shape} \\nYtrain: {Ytrain.shape} \\nYtrain_strat: {Ytrain_strat.shape}\")\n\ndel train_df\ngc.collect()","0f71fb7e":"Xtest = test_df[select_cols].copy()\nXtest.drop(['row_id'], axis=1, inplace=True)\nXtest.replace([np.nan, np.inf, -np.inf], 0, inplace=True)\nprint(f\"Xtest: {Xtest.shape}\")","a1fd0e56":"cat_cols = ['stock_id','time_id']\n\nXtrain[cat_cols] = Xtrain[cat_cols].astype(int)\nXtest[cat_cols] = Xtest[cat_cols].astype(int)\ncat_cols_indices = [Xtrain.columns.get_loc(col) for col in cat_cols]\nprint(cat_cols_indices)","c8983779":"corr_list = []\ndf = Xtrain.copy()\ndf['target'] = Ytrain.ravel()\nfor col in df.columns:\n    corr = df[col].corr(df['target'])\n    corr_list.append([col, corr])\n\ndf = pd.DataFrame(corr_list, columns=['Column','Correlation'])\ndf['Correlation'] = np.round(df['Correlation'], 2)\ndf = df.sort_values(by='Correlation', ascending=False).head(20).copy()\n\nplt.figure(figsize=(12, 10))\nsns.barplot(x='Correlation', y='Column', data=df)\nplt.title(\"Top-20 features with high correlation with target\", pad=20);","c2e6650d":"del df\ngc.collect()","90364baa":"FOLD = 10\nSEEDS = [2018, 2020]\nCOUNTER = 0\n\noof_score_ridge = 0\noof_score_gbr = 0\noof_score_lgb = 0\noof_score_lr = 0\n\ny_pred_final_ridge = 0\ny_pred_final_gbr = 0\ny_pred_final_lgb = 0\ny_pred_final_lr = 0\n\ny_pred_meta_ridge = np.zeros((Xtrain.shape[0], 1))\ny_pred_meta_gbr = np.zeros((Xtrain.shape[0], 1))\ny_pred_meta_lgb = np.zeros((Xtrain.shape[0], 1))\ny_pred_meta_lr = np.zeros((Xtrain.shape[0], 1))","e38e661c":"print(\"Model Name  \\tSeed \\tFold \\tOOF Score \\tAggregate OOF Score\")\nprint(\"=\"*68)\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score_ridge = 0\n    seed_score_gbr = 0\n    seed_score_lgb = 0\n    seed_score_lr = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain, Ytrain_strat)):\n        COUNTER += 1\n\n        train_x, train_y = Xtrain.iloc[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n        weights = 1\/np.square(train_y)\n        \n        \n        #====================================================================\n        #                          Linear Regression\n        #====================================================================\n        \n        lr_model = LinearRegression()\n        lr_model.fit(train_x, train_y, sample_weight = weights)\n        \n        y_pred = lr_model.predict(val_x)\n        y_pred_meta_lr[val] += np.array([y_pred]).T\n        y_pred_final_lr += lr_model.predict(Xtest)\n        \n        score = rmspe(val_y, y_pred)\n        oof_score_lr += score\n        seed_score_lr += score\n        print(f\"LR        \\t{seed} \\t{idx+1} \\t{round(score,5)}\")\n        \n        \n        #====================================================================\n        #                           Bayesian Ridge\n        #====================================================================\n        \n        ridge_model = BayesianRidge(n_iter=1000)\n        ridge_model.fit(train_x, train_y, sample_weight = weights)\n        \n        y_pred = ridge_model.predict(val_x)\n        y_pred_meta_ridge[val] += np.array([y_pred]).T\n        y_pred_final_ridge += ridge_model.predict(Xtest)\n        \n        score = rmspe(val_y, y_pred)\n        oof_score_ridge += score\n        seed_score_ridge += score\n        print(f\"Bayesian Ridge \\t{seed} \\t{idx+1} \\t{round(score,5)}\")\n        \n        \n        #====================================================================\n        #                     HistGradientBoostingRegressor\n        #====================================================================\n        \n        gbr_model = HistGradientBoostingRegressor(\n            max_depth=7,\n            learning_rate=0.05,\n            max_iter=1000,\n            max_leaf_nodes=72, \n            early_stopping=True,\n            n_iter_no_change=100,\n            random_state=0\n        )\n        gbr_model.fit(train_x, train_y, sample_weight = weights)\n        \n        y_pred = gbr_model.predict(val_x)\n        y_pred_meta_gbr[val] += np.array([y_pred]).T\n        y_pred_final_gbr += gbr_model.predict(Xtest)\n        \n        score = rmspe(val_y, y_pred)\n        oof_score_gbr += score\n        seed_score_gbr += score\n        print(f\"GBR        \\t{seed} \\t{idx+1} \\t{round(score,5)}\")\n        \n        \n        #====================================================================\n        #                              LightGBM\n        #====================================================================\n        \n        lgb_model = LGBMRegressor(\n            boosting_type='gbdt', \n            num_leaves=72, \n            max_depth=7, \n            learning_rate=0.02, \n            n_estimators=1000, \n            objective='regression', \n            importance_type='gain',\n            min_child_samples=20, \n            subsample=0.65, \n            subsample_freq=10, \n            colsample_bytree=0.75, \n            reg_lambda=0.05, \n            random_state=0\n        )\n        \n        lgb_model.fit(train_x, train_y, eval_metric='rmse',\n                      eval_set=(val_x, val_y),\n                      early_stopping_rounds=100, \n                      categorical_feature=cat_cols_indices, \n                      sample_weight = weights, verbose=False)\n        \n        y_pred = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration_)\n        y_pred_meta_lgb[val] += np.array([y_pred]).T\n        y_pred_final_lgb += lgb_model.predict(Xtest, num_iteration=lgb_model.best_iteration_)\n        \n        score = rmspe(val_y, y_pred)\n        oof_score_lgb += score\n        seed_score_lgb += score\n        print(f\"LightGBM    \\t{seed} \\t{idx+1} \\t{round(score,5)}\\n\")\n        \n        \n    print(\"=\"*68)\n    print(f\"Bayesian Ridge \\t{seed} \\t\\t\\t\\t{round(seed_score_ridge \/ FOLD, 5)}\")\n    print(f\"GBR            \\t{seed} \\t\\t\\t\\t{round(seed_score_gbr \/ FOLD, 5)}\")\n    print(f\"LR             \\t{seed} \\t\\t\\t\\t{round(seed_score_lr \/ FOLD, 5)}\")\n    print(f\"LightGBM       \\t{seed} \\t\\t\\t\\t{round(seed_score_lgb \/ FOLD, 5)}\")\n    print(\"=\"*68)","4b65173d":"y_pred_final_ridge = y_pred_final_ridge \/ float(COUNTER)\ny_pred_final_gbr = y_pred_final_gbr \/ float(COUNTER)\ny_pred_final_lr = y_pred_final_lr \/ float(COUNTER)\ny_pred_final_lgb = y_pred_final_lgb \/ float(COUNTER)\n\ny_pred_meta_ridge = y_pred_meta_ridge \/ float(len(SEEDS))\ny_pred_meta_gbr = y_pred_meta_gbr \/ float(len(SEEDS))\ny_pred_meta_lr = y_pred_meta_lr \/ float(len(SEEDS))\ny_pred_meta_lgb = y_pred_meta_lgb \/ float(len(SEEDS))\n\noof_score_ridge \/= float(COUNTER)\noof_score_gbr \/= float(COUNTER)\noof_score_lr \/= float(COUNTER)\noof_score_lgb \/= float(COUNTER)\n\nprint(f\"Bayesian Ridge | Aggregate OOF Score: {round(oof_score_ridge,5)}\")\nprint(f\"GradientBoostingRegressor | Aggregate OOF Score: {round(oof_score_gbr,5)}\")\nprint(f\"Linear Regression | Aggregate OOF Score: {round(oof_score_lr,5)}\")\nprint(f\"LightGBM | Aggregate OOF Score: {round(oof_score_lgb,5)}\")","5a95f534":"num_cols = [col for col in Xtrain.columns if col not in cat_cols]\nlen(cat_cols), len(num_cols)","f73dd1cd":"for col in tqdm(num_cols):\n    transformer = QuantileTransformer(n_quantiles=5000, \n                                      random_state=2020, \n                                      output_distribution=\"normal\")\n    \n    vec_len = len(Xtrain[col].values)\n    vec_len_test = len(Xtest[col].values)\n\n    raw_vec = Xtrain[col].values.reshape(vec_len, 1)\n    test_vec = Xtest[col].values.reshape(vec_len_test, 1)\n    transformer.fit(raw_vec)\n    \n    Xtrain[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    Xtest[col] = transformer.transform(test_vec).reshape(1, vec_len_test)[0]\n\nprint(f\"Xtrain: {Xtrain.shape} \\nXtest: {Xtest.shape}\")","f9664ab4":"def dnn_model(data, catcols, numcols):\n    \n    num_inp = Input(shape=(numcols,))\n    \n    inputs = []\n    outputs = []\n    \n    for c in catcols:\n        num_unique_values = int(data[c].max())\n        embed_dim = int(min(np.ceil((num_unique_values)\/2), 50))\n        inp = Input(shape=(1,))\n        out = Embedding(input_dim=num_unique_values + 1, \n                        output_dim=embed_dim, \n                        embeddings_initializer='lecun_normal', \n                        name=c)(inp)\n        out = SpatialDropout1D(rate=0.2)(out)\n        out = Reshape(target_shape=(embed_dim, ))(out)\n        inputs.append(inp)\n        outputs.append(out)\n    \n    outputs.append(num_inp)\n    x = Concatenate()(outputs)\n    x = BatchNormalization()(x)\n    x = Dropout(rate=0.25)(x)\n    \n    x = Dense(units=128, kernel_initializer='lecun_normal', \n                kernel_regularizer=l2(0.0001))(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    x = Dropout(rate=0.35)(x)\n    \n    x = Dense(units=32, kernel_initializer='lecun_normal', \n                kernel_regularizer=l2(0.0001))(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    x = Dropout(rate=0.25)(x)\n\n    x_output = Dense(units=1, kernel_initializer='lecun_normal')(x)\n\n    model = Model(inputs=[inputs, num_inp], outputs=x_output, \n                  name='DNN_Model')\n    return model","ae51958c":"model = dnn_model(Xtrain, cat_cols, len(num_cols))\nmodel.summary()","55d05dbd":"FOLD = 10\nVERBOSE = 0\nBATCH_SIZE = 2048\nSEEDS = [2018, 2020]\n\noof_score = 0\ny_pred_meta_dnn = np.zeros((Xtrain.shape[0], 1))\ny_pred_final_dnn = 0\ncounter = 0\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain, Ytrain_strat)):\n        counter += 1\n\n        train_x, train_y = Xtrain.iloc[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n\n        tf.random.set_seed(seed)\n        model = dnn_model(Xtrain, cat_cols, len(num_cols))\n        model.compile(loss=rmspe_loss, optimizer=Adamax(lr=1e-2))\n\n        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=15, verbose=VERBOSE)\n\n        reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, \n                                      min_lr=1e-5, patience=4, \n                                      verbose=VERBOSE, mode='min')\n\n        history = model.fit(\n            [[train_x[col] for col in cat_cols], train_x[num_cols]], train_y, \n            batch_size=BATCH_SIZE,\n            epochs=100, \n            verbose=VERBOSE, \n            callbacks=[reduce_lr, early], \n            validation_data=([[val_x[col] for col in cat_cols], val_x[num_cols]], val_y)\n        )\n\n        y_pred = model.predict([[val_x[col] for col in cat_cols], val_x[num_cols]], batch_size=BATCH_SIZE)\n        y_pred_meta_dnn[val] += y_pred\n        y_pred_final_dnn += model.predict([[Xtest[col] for col in cat_cols], Xtest[num_cols]], batch_size=BATCH_SIZE)\n        \n        score = rmspe(val_y, y_pred.ravel())\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_dnn = y_pred_meta_dnn \/ float(len(SEEDS))\ny_pred_final_dnn = y_pred_final_dnn \/ float(counter)\noof_score \/= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","b9e842c5":"y_pred_final_gbr = np.array([y_pred_final_gbr]).T\ny_pred_final_lgb = np.array([y_pred_final_lgb]).T\ny_pred_final_dnn = y_pred_final_dnn.clip(0, 1e10)\n\ny_pred_final = (y_pred_final_lgb * 0.6) + (y_pred_final_gbr * 0.2) + (y_pred_final_dnn * 0.2)","a1412403":"submit_df = pd.DataFrame()\nsubmit_df['row_id'] = test_df['row_id']\nsubmit_df['target'] = y_pred_final.ravel()\nsubmit_df.to_csv(\".\/submission.csv\", index=False)\nsubmit_df.head()","f07634b9":"## Create submission file","856eca31":"## Correlation check","1ef1bd4c":"## Weighted Average Ensemble","2d928ead":"### Test data","887ba437":"### Group features","8ed2c9e9":"## Import libraries","99e852b0":"### Book data","e644fd76":"## Additional features","068f52fd":"## Load training data","8200db67":"### Trade data","0f1e4cfa":"## Deep Neural Model","1d34f7b6":"## Base models\n\n* **BayesianRidge**\n* **HistGradientBoostingRegressor**\n* **Linear Regression**\n* **LightGBM**","ec720684":"## Prepare testing data","71dbd329":"## Helper Functions","0a99f6a4":"### **Experiment Log:**\n\n|Version |Models Used |CV Score |LB Score| Changes Made\n| --- | --- | --- | --- | --- |\n|v1 |LightGBM | 0.2963 | 0.29675 | Baseline\n|v2 |LightGBM | 0.2945 | 0.29298 | Feature Tools <br> Quantile Transformation\n|v3 |LightGBM | 0.2938 | NA | Trade data included\n|v4 |LightGBM, XGBoost | NA | NA | Ensemble model used\n|v5 | LightGBM, XGBoost | 0.307 | 0.30579 | Ensemble model used\n|v6 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.2608 | 0.24344 | Ensemble models used <br> Poly features for log-return\n|v7 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | NA | NA | New features added\n|v8 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.2536 | 0.23701 | Error Correction\n|v9 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.2559 | 0.23454 | New features added\n|v10 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.2545 | 0.22974 | New features added using expanding mean\n|v11 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.24651 | 0.22841 | Quantile transformation for feature scaling\n|v12 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.2436 | 0.22758 | New features added\n|v13 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge <br> Voting Regressor | 0.24686 | 0.22779 | New lag features added\n|v14 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.24429 | 0.22805 | New lag features for realized volatility added\n|v15 | Linear Regression, GBR, XGBoost <br> LightGBM, Bayesian Ridge | 0.25574 | NA | New statistical features added\n|v16 | Linear Regression, GBR, XGBoost <br> LightGBM, Bayesian Ridge | 0.24618 | 0.22854 | Removed lag features\n|v17 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.24117 | 0.23223 | Quantile Transformation\n|v18 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.2405 | 0.22852 | New features added\n|v19 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.2405 | NA | Capturing meta features for models blend\n|v20 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.24028 | NA | Architecture revamp\n|v21 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.23989 | 0.22832 | Architecture revamp\n|v22 | LightGBM, CatBoost, GBR | 0.2439 | NA | Architecture revamp\n|v23 | LightGBM, CatBoost, GBR | 0.24025 | 0.22821 | Architecture revamp\n|v24 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.20982 | 0.21142 | Architecture revamp\n|v25 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.20525 | 0.21081 | Models blend section added\n|v26 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.20499 | 0.21051 | New features added\n|v27 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.20437 | 0.21035 | FeatureTools\n|v28 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.20325 | 0.21052 | DAE Features\n|v29 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.20545 | 0.20934 | New features added\n|v30 | Linear Regression, GBR, XGB <br> LightGBM, Bayesian Ridge | 0.20552 | NA | New base model added\n|v31 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.19908 | 0.20939 | LGB Model Tuning\n|v32 | Linear Regression, GBR, CatBoost <br> LightGBM, Bayesian Ridge | 0.20048 | 0.21009 | New models added for blending\n|v33 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.1999| 0.21123 | New models added for blending\n|v34 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.19993 | 0.21130 | New models added for blending \n|v35 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.20259 | 0.21156 | Tuned LGB Model\n|v36 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.19689 | 0.21041 | New models added for blending\n|v37 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.19840 | 0.21038 | Tuned LGB Model\n|v38 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.20545 | NA | Back to v29 Model design\n|v40 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.22461 | 0.21474 | New DAE Embeddings\n|v41 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.206095 | 11529167.26799 | FeatureTools removed <br> New features added\n|v42 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge | 0.195046 | 0.20733 | New K-Means features added <br> Quantile Transformation removed <br> Highly correlated features removed\n|v43 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge <br> Deep Neural Model | 0.194213 | 0.20736 | New Keras model added\n|v44 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge <br> Deep Neural Model | 0.193335 | 0.20736 | Modified LGB and Keras model\n|v45 | Linear Regression, GBR <br> LightGBM, Bayesian Ridge <br> Deep Neural Model | TBD | TBD | Removed meta model <br> Using weighted average ensemble"}}