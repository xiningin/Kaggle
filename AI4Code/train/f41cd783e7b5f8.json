{"cell_type":{"cc826b00":"code","23a2a86f":"code","b51711bc":"code","5f0e6407":"code","6cacf6bf":"code","41d0fd47":"code","670be2df":"code","d3e77af2":"code","bfcf2bb8":"code","efaddc47":"code","38c5d205":"code","1d1923db":"code","7534eb60":"code","e4cb64e3":"code","5e92990a":"code","5194a9b1":"code","dae21d0c":"code","5b632d13":"code","0a8abf42":"code","f86aa9b5":"code","154bf8b3":"code","5ad61fb1":"code","92888b32":"code","e1aecaa0":"code","c45d8389":"code","a078ac77":"code","9f5fb13a":"code","cf9a83ec":"code","8abf1a7f":"code","73c06c04":"code","f6ea00c0":"code","6556931a":"code","0bc71215":"code","531f270f":"code","cd491da3":"code","489fbf49":"code","6d7633b2":"code","9b058daf":"code","a94b0278":"code","3d653af5":"code","26f47a23":"code","ea4fec00":"code","e72b8c8f":"code","b16f44f5":"code","6bc73cf0":"code","6d50e7d1":"code","8ee82fe5":"code","903509c0":"code","5069f2ec":"code","55761e46":"code","9cb68042":"code","0e44c336":"code","7887d011":"code","f6df0ae9":"code","e75006b4":"code","68e9292d":"code","378e4d32":"code","61c8e90a":"code","b333c94c":"code","e3bd37e0":"code","c30cac77":"code","3de5bc74":"code","094ce5df":"code","ea1ca838":"code","9cb3cd22":"code","4d2c503b":"code","17ae2731":"code","5ccef57e":"code","f2c6cdc1":"markdown","39b3734e":"markdown","de529d65":"markdown","22b348b2":"markdown","be3eaabd":"markdown","792f089e":"markdown","7c82e1ac":"markdown"},"source":{"cc826b00":"! pip install tensorflow_datasets -q\n# ! pip install tensorflow-gpu -q\n! pip install tensorflow_addons -q","23a2a86f":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sbn\nimport tensorflow_addons as tfa","b51711bc":"tf.__version__","5f0e6407":"train_input_dataset = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_input_dataset = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","6cacf6bf":"train_input_dataset.head()","41d0fd47":"test_input_dataset.head()","670be2df":"train_target = train_input_dataset.pop('target')","d3e77af2":"train_dataset = tf.data.Dataset.from_tensor_slices((train_input_dataset.text.values, train_target.values))\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_input_dataset.text.values)","bfcf2bb8":"train_dataset","efaddc47":"test_dataset","38c5d205":"for feat, targ in train_dataset.take(3):\n    print(feat, targ)","1d1923db":"for feat in test_dataset.take(3):\n    print(feat)","7534eb60":"tokenizer = tfds.features.text.Tokenizer()\nvocabulary_set = set([''])","e4cb64e3":"for text, label in train_dataset:\n    tokens = tokenizer.tokenize(text.numpy())\n    vocabulary_set.update(tokens)\n\nfor text in test_dataset:\n    tokens = tokenizer.tokenize(text.numpy())\n    vocabulary_set.update(tokens)","5e92990a":"len(vocabulary_set)","5194a9b1":"encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)","dae21d0c":"def encode(text, label):\n    encoded_text = encoder.encode(text.numpy())\n    return encoded_text, label","5b632d13":"def encode_map_fn(text, label):\n    encoded_text, label = tf.py_function(encode,\n                                inp=[text, label],\n                                Tout=(tf.int64, tf.int64))\n    encoded_text.set_shape([None])\n    label.set_shape([])\n    return encoded_text, label\n\ndef encode_map_fn_test(text):\n        encoded_text, _ = tf.py_function(encode,\n                                inp=[text, tf.constant(0,dtype=tf.int64)],\n                                Tout=(tf.int64, tf.int64))\n        encoded_text.set_shape([None])\n        return encoded_text","0a8abf42":"encoded_train_dataset = train_dataset.map(encode_map_fn)\nencoded_test_dataset = test_dataset.map(encode_map_fn_test)","f86aa9b5":"for encoded_text, label in encoded_train_dataset.take(5):\n    print(encoded_text, label)","154bf8b3":"for encoded_text in encoded_test_dataset.take(5):\n    print(encoded_text)","5ad61fb1":"encoded_test_dataset, encoded_train_dataset","92888b32":"TAKE_SIZE = int(len(train_target)*0.7)\nBUFFER_SIZE = 1000\nBATCH_SIZE = 64\nTAKE_SIZE","e1aecaa0":"shuffled_encoded_train_dataset = encoded_train_dataset.shuffle(len(train_target))\ntrain_data = shuffled_encoded_train_dataset.take(TAKE_SIZE).shuffle(BUFFER_SIZE)\ntrain_data = train_data.padded_batch(BATCH_SIZE, ((None,),()))\n\nval_data = shuffled_encoded_train_dataset.skip(TAKE_SIZE)\nval_data = val_data.padded_batch(BATCH_SIZE, ((None,),()))","c45d8389":"for text, label in train_data.take(1):\n    print(text, label)","a078ac77":"embedding_dim = 16","9f5fb13a":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, 'relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])","cf9a83ec":"model.summary()","8abf1a7f":"model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=[tfa.metrics.F1Score(1), 'accuracy'])","73c06c04":"model.fit(train_data, validation_data=val_data, epochs=20)","f6ea00c0":"history = model.history.history","6556931a":"plt.plot(history['loss'], label=\"loss\")\nplt.plot(history['val_loss'], label=\"val_loss\")\nplt.legend()\nplt.show()","0bc71215":"plt.plot(history['accuracy'], label=\"accuracy\")\nplt.plot(history['val_accuracy'], label=\"val_accuracy\")\nplt.legend()\nplt.show()","531f270f":"plt.plot(history['f1_score'], label=\"f1_score\")\nplt.plot(history['val_f1_score'], label=\"val_f1_score\")\nplt.legend()\nplt.show()","cd491da3":"pred = model.predict(encoded_test_dataset.padded_batch(BATCH_SIZE, ((None,))))","489fbf49":"pred_col = (pred>0.5)*1","6d7633b2":"pred_col.shape","9b058daf":"test_dataset","a94b0278":"test_input_dataset.shape","3d653af5":"test_input_dataset['target'] = pred_col[:,0]","26f47a23":"test_input_dataset","ea4fec00":"result = test_input_dataset.drop([\"keyword\", \"location\", \"text\"], axis=1)","e72b8c8f":"result.to_csv(\"tensorflow_result.csv\",index=None)","b16f44f5":"!pip install spacy -q","6bc73cf0":"train_input_dataset = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_input_dataset = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n","6d50e7d1":"train_input_dataset.head()","8ee82fe5":"import spacy","903509c0":"nlp = spacy.load('en')","5069f2ec":"textcat = nlp.create_pipe(\"textcat\", config= {\n    \"exclusive_classes\": True,\n    \"architecture\": \"bow\"\n})","55761e46":"nlp.add_pipe(textcat)","9cb68042":"textcat.add_label(\"1\")\ntextcat.add_label(\"0\")","0e44c336":"train_texts = train_input_dataset.text","7887d011":"train_labels = [{'cats': {'1': label == 1,\n                          '0': label == 0}} \n                for label in train_input_dataset['target']]","f6df0ae9":"train_data = list(zip(train_texts, train_labels))\ntrain_data[:3]","e75006b4":"import random\nfrom spacy.util import minibatch\nfrom tqdm import tqdm\n\nrandom.seed(1)\nspacy.util.fix_random_seed(1)\noptimizer = nlp.begin_training()\n\nlosses = {}\nfor epoch in range(10):\n    random.shuffle(train_data)\n    batches = minibatch(train_data, size=64)\n    for batch in tqdm(batches):\n        texts, labels = zip(*batch)\n        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n    print(losses)","68e9292d":"test_texts = test_input_dataset.text","378e4d32":"test_docs = [nlp.tokenizer(text) for text in test_texts]","61c8e90a":"textcat = nlp.get_pipe('textcat')","b333c94c":"scores, _ = textcat.predict(test_docs)","e3bd37e0":"scores","c30cac77":"predicted_labels = scores.argmax(axis=1)","3de5bc74":"predicted_labels","094ce5df":"test_input_dataset['target'] = predicted_labels","ea1ca838":"test_input_dataset","9cb3cd22":"test_output = test_input_dataset[['id','target']]","4d2c503b":"test_output.set_index('id', inplace=True)","17ae2731":"test_output","5ccef57e":"test_output.to_csv(\"spacy_result.csv\")","f2c6cdc1":"# Using Tensorflow","39b3734e":"## Preprocessing","de529d65":"## Dataset","22b348b2":"### Prediction","be3eaabd":"# Using SpaCy","792f089e":"## Model","7c82e1ac":"## Prediction"}}