{"cell_type":{"ff348c2a":"code","423f16f9":"code","8b26c55b":"code","2d73c13a":"code","f5e0efee":"code","257d512d":"code","f44c52e4":"code","06b462ac":"code","e38f9b36":"code","398ce805":"code","9138f8fb":"code","3fcf2a1f":"code","a09ba590":"code","6d43747c":"code","935611be":"code","15fc782d":"code","eaa0fd82":"code","913ef120":"code","a0df3f98":"code","c16d66b6":"code","d29c8402":"markdown","e828cd51":"markdown","18347081":"markdown","51b683b5":"markdown","adc7bca4":"markdown","5d0e25ad":"markdown","0c9a0003":"markdown","61e0aa67":"markdown","80bedaef":"markdown","78648222":"markdown","5eeb8d91":"markdown","1c8ca8cb":"markdown"},"source":{"ff348c2a":"class Config:\n    name_v1 = \"Exp-102-ResBiLSTM-v2-CustomLoss-v1-AdamW-LogUin-30Fold-Seed2025-FineTune\"  \n    editor = \"masato8823\"\n    api_path = \"\/content\/drive\/masato8823\/kaggle.json\"\n    finetune = \"Exp-102-ResBiLSTM-v2-CustomLoss-v1-AdamW-LogUin-30Fold-Seed2025\"  # This code is after finetuning\n    lr = 1e-3\n    weight_decay = 2e-5\n    epochs = 512\n    scheduler = \"ReduceLROnPlateau\"  # : ReduceLROnPlateau: CosineDecayRestarts\n    early_stop = True\n    seq_len = 80\n    masking = False\n    steps_per_epochs = None\n    train_batch_size = 256\n    valid_batch_size = 1024\n    test_batch_size = 1024\n    n_fold = 30\n    trn_fold = list(range(30))\n    seed = 2025\n    target_col = \"pressure\"\n    debug = False\n\n    # Colab Env\n    submit_from_colab = False\n    upload_from_colab = False\n    \n    # Kaggle Env\n    kaggle_dataset_path = \"..\/input\/exp-102\"\n\nif Config.debug:\n    Config.epochs = 2\n    Config.trn_fold = [0]","423f16f9":"!pip install -U -q scikit-learn","8b26c55b":"import os\nimport sys\nimport joblib\nimport logging\nimport warnings\nimport datetime\nimport json\nimport shutil\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom requests import get\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Bidirectional, LSTM","2d73c13a":"# ========================================\n# Utils\n# ========================================\n\nclass Logger:\n    \"\"\"save log\"\"\"\n    def __init__(self, path):\n        self.general_logger = logging.getLogger(path)\n        stream_handler = logging.StreamHandler()\n        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n        if len(self.general_logger.handlers) == 0:\n            self.general_logger.addHandler(stream_handler)\n            self.general_logger.addHandler(file_general_handler)\n            self.general_logger.setLevel(logging.INFO)\n\n    def info(self, message):\n        # display time\n        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n\n    @staticmethod\n    def now_string():\n        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n    \n    \nclass Util:\n    \"\"\"save & load\"\"\"\n    @classmethod\n    def dump(cls, value, path):\n        joblib.dump(value, path, compress=True)\n\n    @classmethod\n    def load(cls, path):\n        return joblib.load(path)\n    \n    \nclass HorizontalDisplay:\n    \"\"\"display dataframe\"\"\"\n    def __init__(self, *args):\n        self.args = args\n\n    def _repr_html_(self):\n        template = '<div style=\"float: left; padding: 10px;\">{0}<\/div>'\n        return \"\\n\".join(template.format(arg._repr_html_())\n                         for arg in self.args)\n    \n    \ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2 \n    dfs = []\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    dfs.append(df[col].astype(np.int8))\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    dfs.append(df[col].astype(np.int16))\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    dfs.append(df[col].astype(np.int32))\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    dfs.append(df[col].astype(np.int64) ) \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    dfs.append(df[col].astype(np.float32))\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    dfs.append(df[col].astype(np.float32))\n                else:\n                    dfs.append(df[col].astype(np.float64))\n        else:\n            dfs.append(df[col])\n    \n    df_out = pd.concat(dfs, axis=1)\n    if verbose:\n        end_mem = df_out.memory_usage().sum() \/ 1024**2\n        num_reduction = str(100 * (start_mem - end_mem) \/ start_mem)\n        print(f'Mem. usage decreased to {str(end_mem)[:3]}Mb:  {num_reduction[:2]}% reduction')\n    return df_out","f5e0efee":"COLAB = \"google.colab\" in sys.modules\n\nif COLAB:\n    print(\"This environment is Google Colab\")\n\n    # install pack\n    ! pip install --quiet category_encoders\n    ! pip install --quiet tensorflow_addons\n\n    # mount\n    from google.colab import drive, files\n    drive.mount('\/content\/drive') \n\n    # use kaggle api (need kaggle token)\n    f = open(Config.api_path, 'r')\n    json_data = json.load(f) \n    os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n    os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n\n    DRIVE = f\"\/content\/drive\/{Config.editor}\"  # set directory path\n    EXP = Config.name_v1 if Config.name_v1 is not None else get(\"http:\/\/172.28.0.2:9000\/api\/sessions\").json()[0][\"name\"][:-6]  # get notebook name\n    INPUT = os.path.join(DRIVE, \"Input\")\n    OUTPUT = os.path.join(DRIVE, \"Output\")\n    SUBMISSION = os.path.join(DRIVE, \"Submission\")\n    OUTPUT_EXP = os.path.join(OUTPUT, EXP) \n    EXP_MODEL = os.path.join(OUTPUT_EXP, \"model\")\n    EXP_FIG = os.path.join(OUTPUT_EXP, \"fig\")\n    EXP_PREDS = os.path.join(OUTPUT_EXP, \"preds\")\n\n    # make dirs\n    for d in [INPUT, SUBMISSION, EXP_MODEL, EXP_FIG, EXP_PREDS]:\n        os.makedirs(d, exist_ok=True)\n\n    if not os.path.isfile(os.path.join(INPUT, \"train.csv.zip\")):\n        # load dataset\n        ! kaggle competitions download -c ventilator-pressure-prediction -p $INPUT \n    \n    # utils\n    logger = Logger(OUTPUT_EXP)\n\nelse:\n    print(\"This environment is Kaggle Kernel\")\n    INPUT = \"..\/input\/ventilator-pressure-prediction\"\n\n    EXP, OUTPUT, SUBMISSION = \".\/\", \".\/\", \".\/\"\n    EXP_MODEL = os.path.join(EXP, \"model\")\n    EXP_FIG = os.path.join(EXP, \"fig\")\n    EXP_PREDS = os.path.join(EXP, \"preds\")\n\n    if Config.kaggle_dataset_path is not None:\n        KD_MODEL = os.path.join(Config.kaggle_dataset_path, \"model\")\n        KD_EXP_PREDS = os.path.join(Config.kaggle_dataset_path, \"preds\")\n        shutil.copytree(KD_MODEL, EXP_MODEL)\n        shutil.copytree(KD_EXP_PREDS, EXP_PREDS)\n\n    # make dirs\n    for d in [EXP_MODEL, EXP_FIG, EXP_PREDS]:\n        os.makedirs(d, exist_ok=True)\n        \n    # utils\n    logger = Logger(EXP)\n\n# utils\nwarnings.filterwarnings(\"ignore\")\nsns.set(style='whitegrid')\n\n# tpu\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    TPU = None\n\n# 2nd import\nimport category_encoders as ce\nimport tensorflow_addons as tfa","257d512d":"train_name = \"train.csv.zip\" if COLAB else \"train.csv\"\ntest_name = \"test.csv.zip\" if COLAB else \"test.csv\"\nsub_name = \"sample_submission.csv.zip\" if COLAB else \"sample_submission.csv\"\n\ntrain = pd.read_csv(os.path.join(INPUT, train_name))\ntest = pd.read_csv(os.path.join(INPUT, test_name))\nsample_submission = pd.read_csv(os.path.join(INPUT, sub_name))\n\n# fisrt, add fold index\ntrain[\"fold\"], test[\"fold\"] = -1, -1\nfor i, lst in enumerate(\n    model_selection.StratifiedGroupKFold(\n        n_splits=Config.n_fold, \n        shuffle=True,\n        random_state=Config.seed).split(X=train, \n                                        y=train['R'].astype(str) + train['C'].astype(str), \n                                        groups=train[\"breath_id\"])):\n    if i in Config.trn_fold:\n        train.loc[lst[1].tolist(), \"fold\"] = i\n\nif Config.debug:\n    train = train[train[\"breath_id\"].isin(np.random.choice(train[\"breath_id\"].unique(), 1000))].reset_index(drop=True)\n    test = test[test[\"breath_id\"].isin(np.random.choice(test[\"breath_id\"].unique(), 1000))].reset_index(drop=True)\n    sample_submission = sample_submission[sample_submission[\"id\"].isin(test[\"id\"].tolist())].reset_index(drop=True)\n    \ntest[Config.target_col] = np.nan\n\n# cut seq\ntrain = train[train.groupby('breath_id')['time_step'].cumcount() < Config.seq_len].reset_index(drop=True)\ntest = test[test.groupby('breath_id')['time_step'].cumcount() < Config.seq_len].reset_index(drop=True)","f44c52e4":"def plot_pressure_line(train, breath_ids=None, additional_cols=None):\n    if breath_ids is None:\n        breath_ids = train[\"breath_id\"].unique()[:12].tolist()\n        \n    fig, axes = plt.subplots(figsize=(25, 18), ncols=4, nrows=3)\n    axes = np.ravel(axes)\n    plot_cols = [\"pressure\", \"u_in\", \"u_out\"]\n    \n    if additional_cols:\n        plot_cols += additional_cols\n        \n    for b, ax in zip(breath_ids, axes):\n        _df = train[train[\"breath_id\"]==b].copy()\n        (_df\n         .set_index(\"time_step\")[plot_cols]\n         .plot(colormap='Paired',\n               ax=ax,\n               title=f\"breath_id={b}, R={_df['R'].unique()}, C={_df['C'].unique()}\", \n               linewidth=2)\n        )\n\n    fig.subplots_adjust(hspace=0.3)\n    return fig\n\n\ndef plot_pressure_line_px(input_df, breath_id, additional_cols=[]):\n    cols = [\"pressure\",  \"u_in\", \"u_out\"] + additional_cols\n    breath_df = input_df[input_df[\"breath_id\"]==breath_id]\n    plot_df = pd.DataFrame()\n    for col in cols:\n        _df = breath_df[cols].rename(columns={col:\"y\"})\n        _df[\"time_step\"] = breath_df[\"time_step\"].values\n        _df[\"color\"] = col\n        plot_df = pd.concat([plot_df, _df])\n    fig = px.line(plot_df, x=\"time_step\", y=\"y\", color=\"color\",\n                  title=f\"breth_id={breath_id}, R={breath_df['R'].unique()}, C={breath_df['C'].unique()}\")\n    fig.update_traces(mode='markers+lines')\n    return fig","06b462ac":"breath_ids = list(train[\"breath_id\"].sample(12))\nfig = plot_pressure_line(train, breath_ids, additional_cols=[])","e38f9b36":"fig = plot_pressure_line_px(input_df=train, breath_id=train[\"breath_id\"].sample(1).values[0], additional_cols=[])\nfig.show()","398ce805":"def aggregation(input_df, group_key, group_values, agg_methods):\n    \"\"\"ref:https:\/\/github.com\/pfnet-research\/xfeat\/blob\/master\/xfeat\/helper.py\"\"\"\n    new_df = []\n    for agg_method in agg_methods:\n        for col in group_values:\n            if callable(agg_method):\n                agg_method_name = agg_method.__name__\n            else:\n                agg_method_name = agg_method\n            new_col = f\"agg_{agg_method_name}_{col}_grpby_{group_key}\"\n            df_agg = (input_df[[col] + [group_key]].groupby(group_key)[[col]].agg(agg_method))\n            df_agg.columns = [new_col]\n            new_df.append(df_agg)\n            \n    _df = pd.concat(new_df, axis=1).reset_index()\n    output_df = pd.merge(input_df[[group_key]], _df, on=group_key, how=\"left\")\n    return output_df.drop(group_key, axis=1)\n\n\ndef get_raw_features(input_df):\n    cols = [\n        \"time_step\",\n        \"u_in\",\n    ]\n    output_df = input_df[cols].copy()\n    return output_df\n\n\ndef get_ohe_features(input_df):\n    cols = [\"R\", \"C\"]\n    encoder = ce.OneHotEncoder()\n    output_df = encoder.fit_transform(input_df[cols].astype(str))\n    return output_df\n\n\ndef get_fold_value(input_df):\n    return input_df[[\"fold\"]]\n\n\ndef get_target_value(input_df):\n    return input_df[[Config.target_col]]\n\n\ndef get_cumlative_grpby_breath_id_features(input_df):\n    \n    input_df[\"area\"] = input_df[\"time_step\"] * input_df[\"u_in\"]\n    group_key = \"breath_id\"\n    group_values = [\"u_in\", \"area\"]\n    \n    output_df = pd.DataFrame()\n    for group_val in group_values:\n        col_name = f\"agg_cumsum_{group_val}_grpby_{group_key}\"\n        output_df[col_name] = input_df.groupby(group_key)[group_val].cumsum()\n    \n    # tubotubo feats\n    output_df[\"divede_cumsum_u_in_by_time_step\"] = np.log1p(output_df[\"agg_cumsum_u_in_grpby_breath_id\"] \/\n                                                    (input_df[\"time_step\"] + 1e-2))\n        \n    return output_df.fillna(0)\n\n\ndef get_mask_feature(input_df):\n    output_df = pd.DataFrame()\n    output_df[\"mask\"] = input_df[\"u_out\"] == 0\n    return output_df\n\n\ndef _get_agg_col_name(group_key, group_values, agg_methods):\n    out_cols = []\n    for group_val in group_values:\n        for agg_method in agg_methods:\n            out_cols.append(f\"agg_{agg_method}_{group_val}_grpby_{group_key}\")\n    return out_cols\n\n\ndef get_shift_grpby_breath_id_features(input_df):\n    shift_times = [1, 2, 3, 4]\n    group_key = \"breath_id\"\n    group_values = [\"u_in\"]\n    \n    output_df = pd.DataFrame()\n    for t in shift_times:\n        _df = input_df.groupby(group_key)[group_values].shift(t)\n        _df.columns = [f'shift={t}_{col}_grpby_{group_key}' for col in group_values]\n        output_df = pd.concat([output_df, _df], axis=1)\n    return output_df.fillna(0)\n\n\ndef get_diff_grpby_breath_id_features(input_df):\n    diff_times = [1, 2, 3, 4]\n    group_key = \"breath_id\"\n    group_values = [\"time_step\", \"u_in\"]\n    \n    output_df = pd.DataFrame()\n    output_df[\"breath_id\"] = input_df[\"breath_id\"].copy()\n\n    for t in diff_times:\n        _df = input_df.groupby(group_key)[group_values].diff(t)\n        _df.columns = [f'diff={t}_{col}_grpby_{group_key}' for col in group_values]\n        output_df = pd.concat([output_df, _df], axis=1)\n\n    # 1st derivative\n    for n in [1]:\n        col = f'slope={n}_time_step_u_in'\n        val = (output_df[f'diff={n}_u_in_grpby_breath_id'] \/\n               (output_df[f'diff={n}_time_step_grpby_breath_id'] + 1e-8))\n        output_df[col] = val\n               \n    return output_df.fillna(0).drop(\"breath_id\", axis=1)\n\n\ndef get_accel_grpby_breath_id_feature(input_df):\n    grby_u_in = input_df.groupby('breath_id')['u_in']\n    grby_time_step = input_df.groupby('breath_id')['time_step']\n    \n    # 2nd derivative\n    output_df = pd.DataFrame()\n    output_df[\"accel\"] = ((grby_u_in.shift(0) - 2 * grby_u_in.shift(1) + grby_u_in.shift(2)) \/\n                 (grby_time_step.diff(1) * grby_time_step.diff(1).shift(1)))\n\n    # clip accel\n    p001 = output_df[\"accel\"].quantile(0.01)\n    p099 = output_df[\"accel\"].quantile(0.99)\n    output_df[\"accel\"] = output_df[\"accel\"].clip(p001, p099)\n    \n    return output_df.fillna(0)\n\n\ndef get_features_df(train, test):\n    whole_df = pd.concat([train, test]).reset_index(drop=True)\n    whole_df[\"u_in\"] = np.log1p(whole_df[\"u_in\"])  \n    output_df = pd.DataFrame()\n    \n    funcs = [\n        get_target_value,\n        get_mask_feature,\n        get_fold_value,\n        get_raw_features,\n        get_ohe_features,\n        get_cumlative_grpby_breath_id_features,\n        get_shift_grpby_breath_id_features,\n        get_diff_grpby_breath_id_features,\n        get_accel_grpby_breath_id_feature,\n    ]\n\n    for func in funcs:\n        print(func.__name__)\n        _df = func(whole_df)\n        if func.__name__ not in [\n                                 \"get_mask_feature\",\n                                 \"get_target_value\",\n                                 \"get_fold_value\" \n                                 ]:\n\n            scaler = RobustScaler()\n            _df[whole_df[\"u_out\"]==0] = scaler.fit_transform(_df[whole_df[\"u_out\"]==0])\n            _df[whole_df[\"u_out\"]==1] = scaler.transform(_df[whole_df[\"u_out\"]==1])\n            _df = reduce_mem_usage(_df)\n            \n        output_df = pd.concat([output_df, _df], axis=1)\n    \n    output_df[\"breath_id\"] = whole_df[\"breath_id\"].copy()\n    train_feats_df = output_df.iloc[:len(train)]\n    test_feats_df = output_df.iloc[len(train):].reset_index(drop=True)\n    \n    return train_feats_df, test_feats_df","9138f8fb":"def masked_mae_loss(y_true, y_pred, loss_mask):\n    y_true_0 = tf.boolean_mask(y_true, tf.cast(loss_mask, dtype=tf.bool))\n    y_pred_0 = tf.boolean_mask(y_pred, tf.cast(loss_mask, dtype=tf.bool))\n    score = tf.keras.losses.mean_absolute_error(y_true_0, y_pred_0)\n    return score\n\n\ndef custom_mae_loss(y_true, y_pred, loss_mask):\n    y_true_0 = tf.boolean_mask(y_true, tf.cast(loss_mask, dtype=tf.bool))\n    y_pred_0 = tf.boolean_mask(y_pred, tf.cast(loss_mask, dtype=tf.bool))\n    score_0 = tf.keras.losses.mean_absolute_error(y_true_0, y_pred_0)\n\n    y_true_1 = tf.boolean_mask(y_true, tf.cast(1 - loss_mask, dtype=tf.bool))\n    y_pred_1 = tf.boolean_mask(y_pred, tf.cast(1 - loss_mask, dtype=tf.bool))\n    score_1 = tf.keras.losses.mean_absolute_error(y_true_1, y_pred_1)\n    score = score_0 * 2 + score_1 * 1\n    return score\n\n\ndef build_model(input_shape, only_inference=False):\n    \n    inputs_x = tf.keras.layers.Input(shape=input_shape, name=\"input_x\")\n    inputs_y = tf.keras.layers.Input(shape=(input_shape[0], 1), name=\"input_y\")\n    inputs_w = tf.keras.layers.Input(shape=(input_shape[0], 1), name=\"input_w\")\n\n    x0 = Bidirectional(LSTM(512, return_sequences=True))(inputs_x)\n    x = tf.keras.layers.Concatenate(axis=2)([inputs_x, x0])\n\n    x1 = Bidirectional(LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Concatenate(axis=2)([x0, x1])\n\n    x2 = Bidirectional(LSTM(128, return_sequences=True))(x)\n    x = tf.keras.layers.Concatenate(axis=2)([x1, x2])\n\n    x3 = Bidirectional(LSTM(256, return_sequences=True))(x)\n    x = tf.keras.layers.Concatenate(axis=2)([x2, x3])\n\n    x4 = Bidirectional(LSTM(512, return_sequences=True))(x)\n    \n    x = tf.keras.layers.Concatenate(axis=2)([x0, x1, x2, x3, x4])\n    x = tf.keras.layers.Dense(64, activation=\"selu\")(x)\n    \n    x = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n    model = tf.keras.Model(inputs=[inputs_x, inputs_y, inputs_w], outputs=x)\n    \n    if not only_inference:\n        model.add_loss(custom_mae_loss(inputs_y, x, inputs_w))\n        model.add_metric(masked_mae_loss(inputs_y, x, inputs_w), name=\"masked_mae\")\n\n    optimizer = tfa.optimizers.AdamW(lr=Config.lr, weight_decay=Config.weight_decay)\n    model.compile(optimizer=optimizer)\n    return model\n\n\ndef get_model(input_shape, only_inference=False):\n    # TPU setting\n    if TPU:\n        if COLAB:  # tpu in colab\n            tf.config.experimental_connect_to_cluster(TPU)\n            tf.tpu.experimental.initialize_tpu_system(TPU)\n            tpu_strategy = tf.distribute.experimental.TPUStrategy(TPU)\n\n        else:  # tpu in kaggle kernel\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n            tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n        with tpu_strategy.scope():\n            model = build_model(input_shape, only_inference)\n\n    else:\n        model = build_model(input_shape, only_inference)\n    \n    return model","3fcf2a1f":"NO_FEATURES = [\"breath_id\", \"mask\", \"fold\", Config.target_col]\n\n# ========================================\n# DataLoader\n# ========================================\ndef get_dataset(X, y=None, dataset=\"test\"):\n    \n    if dataset==\"train\":\n        train_dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((X, y))\n            .shuffle(10**8)\n            .batch(Config.train_batch_size)\n            .prefetch(tf.data.experimental.AUTOTUNE)\n            )\n        if Config.steps_per_epochs is not None:\n            train_dataset = train_dataset.repeat()\n        return train_dataset\n\n    elif dataset==\"valid\":\n        valid_dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((X, y))\n            .batch(Config.valid_batch_size)\n            .prefetch(tf.data.experimental.AUTOTUNE)\n        )\n        return valid_dataset\n    \n    elif dataset==\"test\":\n        test_dataset = (\n            tf.data.Dataset\n            .from_tensor_slices(X)\n            .batch(Config.test_batch_size)\n            .prefetch(tf.data.experimental.AUTOTUNE)\n        )\n        return test_dataset\n    \n    else:\n        raise NotImplementedError\n\n\ndef transform_features_for_rnn(input_df, feature_names='__all__'):\n    if feature_names == \"__all__\":\n        feature_names = input_df.columns.tolist()\n\n    features = []\n    _input_df = input_df.copy()\n    _input_df['time_index'] = _input_df.groupby('breath_id')['time_step'].cumcount()\n    _input_df = _input_df.query('time_index < @Config.seq_len' ).reset_index(drop=True)\n    use_cols = list(set(feature_names) - set(['breath_id', 'mask', 'fold']))\n    \n    if Config.masking:\n        # pad 0 in u_out==1\n        _input_df.loc[~input_df[\"mask\"], use_cols] = -1\n\n    pdf = pd.pivot_table(data=_input_df, index='breath_id', columns='time_index')\n    breath_id_count = len(pdf)\n\n    for feat_name in feature_names:\n        if feat_name in NO_FEATURES:\n            continue\n            \n        _feat = pdf[feat_name].values.reshape(breath_id_count, -1, 1)\n        features.append(_feat)\n\n    features = np.concatenate(features, axis=2)\n    print(features.shape)\n    return features\n    \n\ndef transform_target_for_rnn(train, target_col=Config.target_col):\n    _train = train.copy()\n    _train['time_index'] = _train.groupby('breath_id')['time_step'].cumcount()\n    _train = _train.query('time_index < @Config.seq_len').reset_index(drop=True)\n\n    if target_col != 'mask':\n        if Config.masking:\n            _train.loc[~train['mask'], Config.target_col] = -1\n\n    pdf = pd.pivot_table(data=_train, index='breath_id', columns='time_index', values=[target_col])\n    breath_id_count = len(pdf)\n    target = pdf[target_col].values.reshape(breath_id_count, -1, 1)\n    return target\n\n\n# ========================================\n# Scheduler\n# ========================================\ndef get_scheduler(monitor):\n    if Config.scheduler == \"custom-v1\":\n        def custom_scheduler(epoch):\n            x = Config.lr\n            if epoch >= 125: x = 0.0007\n            if epoch >= 185: x = 0.0004\n            if epoch >= 250: x = 0.0003\n            if epoch >= 275: x = 0.0002\n            if epoch >= 290: x = 0.00015\n            if epoch >= 305: x = 0.0001\n            if epoch >= 320: x = 0.000075\n            if epoch >= 325: x = 0.00006\n            if epoch >= 330: x = 0.00004\n            if epoch >= 330: x = 0.00003\n            if epoch >= 340: x = 0.00002\n            if epoch >= 345: x = 0.00001\n            return x\n        \n        # plot steps\n        plt.plot([custom_scheduler(i) for i in range(Config.epochs)])\n        plt.show()\n        scheduler = tf.keras.callbacks.LearningRateScheduler(custom_scheduler, verbose=1)\n    \n    elif Config.scheduler == \"ReduceLROnPlateau\":\n        scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor=monitor, \n            factor=0.7,\n            patience=16,\n            min_lr=1e-7, \n            verbose=1\n            )\n        \n    elif Config.scheduler == \"CosineDecayRestarts\":\n        cisine_decay_r = tf.keras.experimental.CosineDecayRestarts(\n            Config.lr,\n            first_decay_steps=Config.epochs \/\/ 2,\n            t_mul=1,\n            m_mul=1,\n            alpha=0.01\n            )\n        \n        # plot steps\n        plt.plot([cisine_decay_r(i) for i in range(Config.epochs)])\n        plt.show()\n\n        scheduler = tf.keras.callbacks.LearningRateScheduler(cisine_decay_r, verbose=1)\n    \n    else:\n        raise NotImplementedError\n    \n    return scheduler\n\n# ========================================\n# Training & Inference Func\n# ========================================\ndef training_rnn(train_df, valid_df, model, filepath):\n    \"\"\"Training func for RNN\"\"\"\n\n    train_x = transform_features_for_rnn(train_df)\n    valid_x = transform_features_for_rnn(valid_df)\n    train_y = transform_target_for_rnn(train_df)\n    valid_y = transform_target_for_rnn(valid_df)\n    \n    train_w = transform_target_for_rnn(train_df, \"mask\")\n    valid_w = transform_target_for_rnn(valid_df, \"mask\")\n    train_inputs = {\"input_x\":train_x, \"input_y\":train_y, \"input_w\":train_w}\n    valid_inputs = {\"input_x\":valid_x, \"input_y\":valid_y, \"input_w\":valid_w}\n    \n    tr_dataset = get_dataset(X=train_inputs, y=train_y, dataset=\"train\")\n    va_dataset = get_dataset(X=valid_inputs, y=valid_y, dataset=\"valid\")\n    \n    monitor = \"val_masked_mae\"\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath, \n        monitor=monitor, \n        verbose=1, \n        save_best_only=True, \n        save_weights_only=True,\n        mode=\"min\")\n    \n    callbacks = [checkpoint, get_scheduler(monitor)]\n    \n    if Config.early_stop:\n        early_stop = tf.keras.callbacks.EarlyStopping(\n            monitor=monitor,\n            min_delta=0.0,\n            patience=36, \n            mode=\"min\"\n        )\n        callbacks += [early_stop]\n\n    model.fit(\n        tr_dataset, \n        epochs=Config.epochs, \n        verbose=1, \n        callbacks=callbacks,\n        validation_data=va_dataset, \n        steps_per_epoch=Config.steps_per_epochs,\n    )\n\n    \ndef inference_rnn(test_df, model, filepath, is_test=False):\n    \"\"\"Inference func for RNN\"\"\"\n    model.load_weights(filepath)\n    \n    test_x = transform_features_for_rnn(test_df) \n    test_y_dummy = transform_target_for_rnn(test_df, \"mask\")\n    test_w = transform_target_for_rnn(test_df, \"mask\")\n    test_inputs = {\"input_x\":test_x, \"input_y\":test_y_dummy, \"input_w\":test_w}\n    \n    te_dataset = get_dataset(X=test_inputs, y=None, dataset=\"test\")\n    preds = model.predict(te_dataset)\n    print(preds.shape)\n    pad_width = 80 - Config.seq_len\n\n    if is_test:\n        preds = np.pad(preds, pad_width=[(0, 0), (0, pad_width), (0, 0)])\n\n    preds = np.concatenate(preds, 0)\n    return preds.reshape(-1)","a09ba590":"def metrics(y_true, y_pred, mask=None):\n    if mask is not None:\n        y_true, y_pred = y_true[mask], y_pred[mask]\n    score = mean_absolute_error(y_true, y_pred)\n    return score\n\n\ndef train_cv_rnn(train, metrics, name, directory):\n    \"\"\"cross validation training for RNN\"\"\"\n    input_shape = (Config.seq_len, train.shape[1] - len(NO_FEATURES)) \n    oof = np.zeros(len(train))\n    for i_fold in range(Config.n_fold):\n        \n        if i_fold in Config.trn_fold:\n        \n            K.clear_session()\n            filepath = os.path.join(directory, f\"{name}_fold{i_fold+1}.h5\")\n            tr_df, va_df = (train[train[\"fold\"] != i_fold].reset_index(drop=True),\n                            train[train[\"fold\"] == i_fold].reset_index(drop=True))\n\n            if not os.path.isfile(filepath):  # if trained model, no training\n                model = get_model(input_shape)\n                if Config.finetune is not None:  # fine tune (additinal training) \n\n                    if COLAB:\n                        pretrain_filepath = os.path.join(\n                            OUTPUT, Config.finetune, \"model\", Config.finetune + f\"-{Config.seed}_fold{i_fold+1}.h5\"\n                            )\n                    else:\n                        pretrain_filepath = os.path.join(\n                            \"..\/input\", Config.finetune, \"model\", Config.finetune + f\"-{Config.seed}_fold{i_fold+1}.h5\"\n                            )\n                                                         \n                    model.load_weights(pretrain_filepath)\n                \n                training_rnn(tr_df, va_df, model, filepath)\n  \n            K.clear_session()\n            model = get_model(input_shape, only_inference=True)\n            preds = inference_rnn(va_df, model, filepath)\n            score = metrics(np.array(va_df[Config.target_col]), np.array(preds), mask=np.array(va_df[\"mask\"], dtype=bool))\n            logger.info(f\"{name}_fold{i_fold+1} >>> val socre:{score:.4f}\")\n            oof[train[\"fold\"] == i_fold] = preds\n    \n    score = metrics(np.array(train[Config.target_col]), oof,  mask=np.array(train[\"mask\"], dtype=bool))\n    logger.info(f\"{name} >>> val score:{score:.4f}\")\n    return oof\n\n\ndef predict_cv_rnn(test, name, directory, is_test=True):\n    \"\"\"cross varidation prediction for RNN\"\"\"\n    input_shape = (Config.seq_len, test.shape[1] - len(NO_FEATURES))\n    model = get_model(input_shape, only_inference=True)\n    preds_fold = []\n    preds_fold_df = pd.DataFrame()\n    for i_fold in range(Config.n_fold):\n        if i_fold in Config.trn_fold:\n            \n            filepath = os.path.join(directory, f\"{name}_fold{i_fold+1}.h5\")\n            preds = inference_rnn(test, model, filepath, is_test=is_test)\n            preds_fold.append(preds)\n            preds_fold_df[f\"fold={i_fold:02}\"] = preds\n            logger.info(f\"{name}_fold{i_fold+1} inference\")\n    \n    preds = np.median(preds_fold, axis=0)\n    preds_fold_df.to_csv(os.path.join(EXP_PREDS, \"preds_fold.csv\"), index=False)\n    return preds\n\n\ndef post_processing(prediction, train_target):\n    \"\"\"round & clipping post process (by chirs)\"\"\"\n    \n    min_target, max_target = np.min(train_target), np.max(train_target)\n        \n    unique_target = np.array(sorted(np.unique(train_target)))\n    target_step = unique_target[1] - unique_target[0] \n\n    output_target = np.round((prediction - min_target) \/ target_step) * target_step + min_target\n    output_target = np.clip(output_target, min_target, max_target)\n\n    return output_target\n\n\ndef plot_regression_result(y, oof, directory):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    sns.distplot(y, label='y', color='cyan', ax=ax)\n    sns.distplot(oof, label='oof', color=\"magenta\", ax=ax)\n    \n    ax.legend()\n    ax.grid()\n    ax.set_title(\"regression_result\")\n    fig.tight_layout()\n    return fig","6d43747c":"# preprocess\nprint(\"# ============= # Preprocess # ============= #\")\ntrain_feats_df, test_feats_df = get_features_df(train, test)\ndisplay(train_feats_df)\nprint(train_feats_df.shape, test_feats_df.shape)\n\n# training\nprint(\"# ============= # Training # ============= #\")\noof_df = pd.DataFrame()\nname = f\"{Config.name_v1}-{Config.seed}\"\noof = train_cv_rnn(\n    train=train_feats_df,\n    metrics=metrics, \n    name=name, \n    directory=EXP_MODEL)\n\noof_df[name] = oof\noof_df.to_csv(os.path.join(EXP_PREDS, \"oof.csv\"), index=False)\n\n\n# get oof score \ny_true = train[Config.target_col]\ny_pred = oof_df.median(axis=1)\nmask = train[\"u_out\"] == 0\nused_mask = train[\"fold\"].isin(Config.trn_fold)\n\noof_score = metrics(y_true[used_mask], y_pred[used_mask], mask=mask)  # compe metrics \noof_score_all = metrics(y_true[used_mask], y_pred[used_mask])  # all record score\nlogger.info(f\"{Config.name_v1} compe score:{oof_score:.4f}, all score:{oof_score_all}\")\n\n\n# post processing for OOF\nprint(\"# ============= # PP for OOF # ============= #\")\ny_pred = post_processing(prediction=y_pred, train_target=y_true)\n\noof_score = metrics(y_true[used_mask], y_pred[used_mask], mask=mask)   # compe metrics \noof_score_all = metrics(y_true[used_mask], y_pred[used_mask])  # all record score\nlogger.info(f\"{Config.name_v1} compe score pp:{oof_score:.4f}, all score pp:{oof_score_all}\")\n\n\n# inference\nprint(\"# ============= # Inference # ============= #\")\npreds_df = pd.DataFrame()\nname = f\"{Config.name_v1}-{Config.seed}\"\npreds = predict_cv_rnn(\n    test=test_feats_df,\n    name=name, \n    directory=EXP_MODEL\n)\npreds_df[name] = preds\n\npreds_df.to_csv(os.path.join(EXP_PREDS, \"preds.csv\"), index=False)\ntest_pred = preds_df.median(axis=1)\n\n# post processing for OOF\nprint(\"# ============= # PP for PREDS # ============= #\")\ntest_pred = post_processing(prediction=test_pred, train_target=y_true)\n\nsample_submission['pressure'] = test_pred","935611be":"fig = plot_regression_result(y_true[(used_mask*mask).astype(bool)],\n                             y_pred[(used_mask*mask).astype(bool)], \n                             directory=EXP_FIG)\nfig.savefig(os.path.join(EXP_FIG, \"regression_result.png\"), dpi=300)\n\ntrain[\"oof\"] = y_pred\n\n# plot oof score (good & bad)\ntrain_ = train[(used_mask*mask).astype(bool)].reset_index(drop=True)\noof_score_by_breath =train_.groupby(\"breath_id\").apply(lambda x: metrics(x[\"pressure\"], x[\"oof\"], x[\"u_out\"]==0))\n\n# best12\nbreath_id_for_plot = oof_score_by_breath.sort_values().index[list(range(12))]\nfig = plot_pressure_line(train_, breath_ids=list(breath_id_for_plot), additional_cols=[\"oof\"])\nfig.savefig(os.path.join(EXP_FIG, \"best_oof.png\"), dpi=300)\n\n# bad12\nbreath_id_for_plot = oof_score_by_breath.sort_values(ascending=False).index[list(range(12))]\nfig = plot_pressure_line(train_, breath_ids=list(breath_id_for_plot), additional_cols=[\"oof\"])\nfig.savefig(os.path.join(EXP_FIG, \"bad_oof.png\"), dpi=300)","15fc782d":"def show_oof_score(train):\n    train_df = train.copy()\n\n    def _get_score_by_rc(input_df, fold):\n        info_lst = []\n        for rc in input_df[\"R-C\"].unique():\n            rc_df = input_df[input_df[\"R-C\"] == rc].reset_index(drop=True)\n            score = metrics(rc_df[\"pressure\"].values, rc_df[\"oof\"].values, mask=rc_df[\"u_out\"]==0)\n            info_lst.append([f\"fold={fold}\", rc, score])\n            \n        score = metrics(input_df[\"pressure\"].values, input_df[\"oof\"].values, mask=input_df[\"u_out\"]==0)\n        info_lst.append([f\"fold={fold}\", \"all\", score])\n        return info_lst\n\n    train_df[\"R-C\"] = train[\"R\"].astype(str) + \"-\" + train[\"C\"].astype(str)\n    plot_info = []\n    for f in train_df[\"fold\"].unique():\n        if f == -1:\n            continue\n        fold_df = train_df[train_df[\"fold\"] == f].reset_index(drop=True)\n        info_lst = _get_score_by_rc(fold_df, fold=f)\n        plot_info += info_lst\n    \n    info_lst =  _get_score_by_rc(train_df, fold=\"ALL\")\n    plot_info += info_lst\n        \n    plot_df = pd.DataFrame(plot_info, columns=[\"fold\", \"R-C\", \"MAE\"])\n    plot_df = plot_df.sort_values([\"fold\", \"R-C\"]).reset_index(drop=True)\n    fig = px.bar(plot_df, x=\"fold\", y=\"MAE\", color='R-C', barmode='group')\n\n    return plot_df, fig\n","eaa0fd82":"df, fig =  show_oof_score(train_)  \nfig.show()\nfig.write_html(os.path.join(EXP_FIG, \"oof_score.html\"))\ndisplay(df)","913ef120":"# plot bad predictions by R & C\ntrain_df_by_breath = (pd.merge(\n    pd.DataFrame(oof_score_by_breath, columns=[\"MAE\"]),\n    train_[[\"breath_id\", \"R\", \"C\"]].groupby(\"breath_id\").head(1),\n    on=\"breath_id\", how=\"left\"))\ntrain_df_by_breath[\"R-C\"] = train_df_by_breath[\"R\"].astype(str) + \"-\" + train_df_by_breath[\"C\"].astype(str)\n\nbad12_by_rc_dict = {}\nfor rc in train_df_by_breath[\"R-C\"].unique():\n    _df = train_df_by_breath[train_df_by_breath[\"R-C\"] == rc].reset_index(drop=True)\n    breath_ids = _df.sort_values(\"MAE\", ascending=False)[\"breath_id\"].tolist()[:12]\n    bad12_by_rc_dict[rc] = breath_ids\n\n    fig = plot_pressure_line(train_, breath_ids=bad12_by_rc_dict[rc], additional_cols=[\"oof\"])\n    plt.plot()\n    fig.savefig(os.path.join(EXP_FIG, f\"bad_oof_rc_{rc}.png\"), dpi=300)","a0df3f98":"filepath = os.path.join(SUBMISSION, f\"{Config.name_v1}.csv\")\nsample_submission.to_csv(filepath, index=False)\nmessage = f\"train-fold-num:{len(Config.trn_fold)},oof-score:{oof_score:.5f},oof-score-all:{oof_score_all:.5f}\"\nlogger.info(message)\n\nif Config.submit_from_colab:\n    ! kaggle competitions submit -c ventilator-pressure-prediction -f $filepath -m $message","c16d66b6":"# upload output folder to kaggle dataset\nif Config.upload_from_colab:\n    from kaggle.api.kaggle_api_extended import KaggleApi\n\n    def dataset_create_new(dataset_name, upload_dir):\n        dataset_metadata = {}\n        dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}\/{dataset_name}'\n        dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n        dataset_metadata['title'] = dataset_name\n        with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n            json.dump(dataset_metadata, f, indent=4)\n        api = KaggleApi()\n        api.authenticate()\n        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n\n    if len(EXP) >= 50:\n        dataset_name = EXP[:7]\n\n    dataset_create_new(dataset_name=dataset_name, upload_dir=OUTPUT_EXP)","d29c8402":"## Model","e828cd51":"## Simple EDA","18347081":"## Config","51b683b5":"## Funcs","adc7bca4":"## Plot Result","5d0e25ad":"## BiLSTM : 19th Plath Best Single Model [ CV:0.1302 LB:0.1147 ]\n\n\n### Abstract\n1. Feature Engineering\n    - Logarithmized u_in from the beginning\n    - First and second derivative for u_in\n    - RobustScaler fit features with u_out==0 only\n\n\n2. Model\n    - BiLSTM with ResNet-like structure\n    - Custom MAE Loss\n    - optimizer : AdamW\n    - scheduler : ReduceLrOnPlateum\n\n\n3. Post-Processing\n    - We used [Chris's PP](https:\/\/www.kaggle.com\/cdeotte\/ensemble-folds-with-median-0-153)\n\n\n### Common pipeline between Colab and kaggle\n\n1. The exact same code can be used between google colab & kaggle \n\n2. You can use the kaggle api to automatically load data and submit, and upload datasets\n\n3. Parallel fold training with multiple sessions is possible on colab\n\n4. Both TPU and GPU can be used","0c9a0003":"## Feature Engineering","61e0aa67":"## Main","80bedaef":"## Load Data","78648222":"## Library","5eeb8d91":"## SetUp","1c8ca8cb":"## Submission"}}