{"cell_type":{"f3acfd51":"code","5e568f02":"code","bd92143d":"code","8bf1116d":"code","d3739261":"code","e9a7bd55":"code","8f313531":"code","2fc666ab":"code","bc7a247a":"code","98368847":"code","ab32a962":"code","225e4131":"code","b004eb61":"code","de465c2e":"code","afd0cc68":"code","6ab93e90":"code","bce6b3f8":"code","dd83c094":"code","ef169f7d":"code","8d31c444":"markdown","76d52a2a":"markdown","02a2a627":"markdown","b5b4ae95":"markdown","147aa96e":"markdown","7e3b5044":"markdown","a070f4a1":"markdown","dab17c51":"markdown","f9377f4c":"markdown","a5e8f97d":"markdown","d0c08896":"markdown","dc82276b":"markdown","c2b21ec9":"markdown","78833c30":"markdown","7b37ad35":"markdown","d6f1e9ff":"markdown","06fd5d46":"markdown","eeca9694":"markdown","d6b512a3":"markdown","d6e0fc77":"markdown","4f76ebe2":"markdown","77e3d47c":"markdown","7764ad17":"markdown","90fd8071":"markdown","7686947a":"markdown","317fb5df":"markdown","6cff7bb9":"markdown","79fda49a":"markdown","76a1f86a":"markdown","5a222158":"markdown","baa42424":"markdown","ce2ab8a2":"markdown"},"source":{"f3acfd51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e568f02":"# Importing important Libraries\n!pip install keras-adabound\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU\nfrom keras.optimizers import RMSprop\nfrom keras_adabound import AdaBound\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","bd92143d":"train_data=pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain_label=train_data[\"label\"]\n\nX_train,X_test,y_train,y_test=train_test_split(train_data.drop([\"label\"],axis=1),\n                                             train_label,test_size=0.1,\n                                             stratify=train_label,\n                                             random_state=1)\n\nprint(\"Shape of  Total Data was : {} which we split in train-val(80-20)\".format(train_data.shape,\n                                                                                train_label.shape))\nprint(\"Shape of Train - Input:{} -Ouput:{} \\n Shape of Test Input:{} & Output :{} \".format(X_train.shape,\n                                                                                           y_train.shape,\n                                                                                           X_test.shape,\n                                                                                           y_test.shape))","8bf1116d":"#Checking Null value\nprint(\"How many values are mising in the train_csv :{} or test_csv :{}\".format(train_data.isnull().any().sum(),\n                                                                              test_data.isnull().any().sum(),))\n# No missing values so we are good to go.","d3739261":"# distribution in original dataset\n#print(train_data[\"label\"].value_counts())\n#\nfig=plt.figure(figsize=(15,5))\nax1=fig.add_subplot(131)\nsns.countplot(train_data[\"label\"],ax=ax1)\nax1.set_title(\"In Train_csv\",fontsize=20)\nax2=fig.add_subplot(132)\nsns.countplot(y_train,ax=ax2)\nax2.set_title(\"Training Set\",fontsize=20)\nax3=fig.add_subplot(133)\nsns.countplot(y_test,ax=ax3)\nax3.set_title(\"Test Set\",fontsize=20)","e9a7bd55":"fig,(ax1,ax2,ax3,ax4)=plt.subplots(1,4)\nplt.figure(figsize=(20,10))\nax1.imshow(np.array(X_train[2:3]).reshape(28,28),cmap='gray')\nax2.imshow(np.array(X_train[3:4]).reshape(28,28),cmap='gray')\nax3.imshow(np.array(X_train[4:5]).reshape(28,28))\nax4.imshow(np.array(X_train[5:6]).reshape(28,28))","8f313531":"print(\"The maximum :{}  & minimum  :{} value of pixel in our Image (Matrix)\".format(X_train.max(axis=1).max(),\n                                                                           X_train.min(axis=1).min()))\nblack_white_box=np.array([[255,0],[0,255]])\nplt.imshow(black_white_box.reshape(2,2),cmap='gray')","2fc666ab":"#Scaling - Moved  this step to the Modeling Part\n#X_train=X_train\/255\n#X_val=X_train\/255","bc7a247a":"# Preprocess the data (these are NumPy arrays)\n#  becuase we are scaling between 0 and 1 & we will get decimals thats why float\nX_train = X_train.values.reshape(-1, 784).astype(\"float32\") \/ 255\nX_test = X_test.values.reshape(-1, 784).astype(\"float32\") \/ 255\n\nprint(\"Shape of Training Input {} & of Test Input {}\".format(X_train.shape,X_test.shape))\n\n# # Hold Out last 10K for Validation \n# x_val = x_train[-10000:]\n# y_val = y_train[-10000:]\n# x_train = x_train[:-10000]\n# y_train = y_train[:-10000]","98368847":"print(\"Target Variable before transformation {}\".format(y_train.iloc[0]))\n      \n# Encoding \ny_train = to_categorical(y_train, num_classes = 10,dtype='float32')\ny_test = to_categorical(y_test, num_classes = 10,dtype='float32')\n\nprint(\"Target Variable After transformation {}\".format(y_train[0]))","ab32a962":"fig,ax=plt.subplots(2,5,sharex=True,sharey=True)\nax=ax.flatten()\n\nlabel_4=np.array(np.argmax(y_train,axis = 1) ==4).argsort()[-10:]\nfor i in range(10):\n    image=np.array(X_train[label_4[i]]).reshape(28,28)\n    ax[i].imshow(image,cmap='gray',interpolation='nearest')\nax[0].get_xaxis().set_ticklabels([])\nax[0].get_yaxis().set_ticklabels([])\n# ax[0].set_ylabel()\nplt.tight_layout()","225e4131":"# define the keras model - can also use add incrementally using model.add()\n# weights initialization -Xavier (also k\/a Glorot initialization)\n\n#Configuration Variables \nopt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\ninitializer = keras.initializers.GlorotNormal(seed=1)\nLeakyReLU_alpha=0.1\nbatch_size = 256\nno_epochs = 50\n\n# A common Debugging Workflow - add() + summary()\n# LeakyRelu has to be added seperataly (can't pass in Dense())\nmodel = Sequential()\nmodel.add(layers.Dense(64,input_dim=784,kernel_initializer=initializer,name=\"Dense_Layer1\"))\nprint(model.summary())\nmodel.add(layers.LeakyReLU(alpha=LeakyReLU_alpha))\nmodel.add(layers.Dense(64,kernel_initializer=initializer,name=\"Dense-Layer2\"))\nprint(model.summary())\nmodel.add(layers.LeakyReLU(alpha=LeakyReLU_alpha))\nmodel.add(layers.Dense(10,activation=\"softmax\",kernel_initializer=initializer,name=\"Predictor\"))\nprint(model.summary())","b004eb61":"# compile the keras model\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n# fit the keras model on the dataset\n# use verbose to see info of every epoch (verbose=0- no info)\n# history has 4 metrics for each epoch loss,accuracy,val_loss,val_accuracy\nhistory=model.fit(X_train,y_train,validation_split=0.1,verbose=0, epochs =no_epochs, batch_size=batch_size)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","de465c2e":"print(\"Final Training Accuracy :{:.2f}% & Final Validation Accuracy {:.2f}% :\".format(history.history['accuracy'][-1]*100,\n                                                                                      history.history['val_accuracy'][-1]*100))","afd0cc68":"## Test Accuracy:\n# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nresults = model.evaluate(X_test, y_test, batch_size=128)\nprint(\"Final Test Accuracy {:.2f}% :\".format(results[-1]*100))","6ab93e90":"from sklearn.metrics import confusion_matrix\nY_pred = model.predict(X_test) \nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(y_test,axis = 1) \nprint('Confusion Matrix')\nprint(confusion_matrix(Y_true, Y_pred_classes,labels=range(9)))","bce6b3f8":"np.array(Y_pred-y_test).sum(axis=1).argsort()[-10:]","dd83c094":"# get the top 10 misclassified examples\nmisclass_10=np.array(Y_pred_classes!=Y_true).argsort()[-10:]\nfig,ax=plt.subplots(2,5,sharex=True,sharey=True,figsize=(10,5))\nax=ax.flatten()\nfor i in range(10):\n    image=np.array(X_test[misclass_10[i]]).reshape(28,28)\n    ax[i].imshow(image,cmap='gray',interpolation='nearest')\n    ax[i].set_title(\"Predicted :{}\\n Actual :{}\".format(\n                    Y_pred_classes[misclass_10[i]],\n                    Y_true[misclass_10[i]]))\n","ef169f7d":"\ntest_data = test_data.values.reshape(-1, 784).astype(\"float32\") \/ 255\n\nprint(\"Shape of Training Input {} & of Test Input {}\".format(test_data.shape,test_data.shape))\n\nresults = model.predict(test_data)\n\n# select the index with max prob\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"Ann_digitmnist.csv\",index=False)","8d31c444":"<a id = \"Step6\"><\/a>\n# Evaluation of Model ","76d52a2a":"If want to play around with different Hyperparemeter & activation layers - [Go Play](https:\/\/playground.tensorflow.org\/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.44075&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)","02a2a627":"Transformation  of X_train - 3 things - to numpy array , reshape & to float\n\n1. **Why covert to numpy array**?X_train is a Dataframe (which is a tabular form (with column names)) but models accept numpy array(which only contains values with no column naming).\n2. **Why Reshape**? we want the input to have dimension (n_samplesXno_of_pixels) because every layer in ANN in Fully Connected Layer & we want to connect each pixel to each neuron on Layer 1.\n3. **Why Float**? Well we are dividing by 255 which returns Float.\n\nThese all 3 steps can be done in a single line.","b5b4ae95":"For no_of epoch =50\nwe got Training Accuray of 99.99% , but validation score- 97.29% good thing but not great until we get high score in Test set.","147aa96e":"Very **Important to define The input shape** becuase remember - shape of weight matrix depend on the shape of input , running model.summary() will give error.\n\nTo get the best out of Deep Learning we have tune its **Hyper-Parameters(HP)**.\n\n**What is Hyperparamter? & How are the different from parameters(P)**? - HP helps in learning and take care of all things related to learning such as OVerfitting,Underfitting,Vanishing\/Exploding Gradient & taking too long to reach the global minima. Parameters - are those that the models are learning (Weights & biases) , User specify- HP & Model Learn-P.\n\n**List of Hyperparameters-**\n* **Number of Layers**- higher the number - higher the number - better capacity to learn  complex decision boundary(too high means overfitting)\n* **Number of Neurons** - higher the number - higher the number -capacity to learn  complex decision boundary\n* **Activation Function** - Generally leaky relu (others- Sigmoid,tanh,elu) [Article](https:\/\/towardsdatascience.com\/hyper-parameters-in-action-a524bf5bf1c)\n* **Weight Initializer**- Xavier vs He initialization (others - randomnormal) [Article](https:\/\/towardsdatascience.com\/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404)\n* **Loss Function** -CategoricalCrossentropy (to use when class>2 : $-\\sum_{i=0}^mylog(p)$ - y is binary(0 or 1) which indicates true class & p- predicted probability , in our case 10 class. [Article](https:\/\/towardsdatascience.com\/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)\n* **Optimizer** - How to minimize the Loss Function (SGD,RMSProp,Adam) [Article](https:\/\/towardsdatascience.com\/how-do-we-train-neural-networks-edd985562b73)\n* **Learning rate** - it defines what is the step size (control the amount by which we update the parameters),generally low value because it converges smothly\n* **No of Epochs** - Epoch means the number of times we want to run through the complete dataset.\n* **Batch Size($2^x$)** -  32,64(Fit in CPU,GPU memory) .Mini Batch  (feed 1 batch -Feedforward & Backprop & then another batch & so on...we are able to iterate multiple times within 1 epoch)Batch size =1 will take noisy steps ,but batch size=32 will average the noise & will have very small noise but will lead in the right direction.\n* **Drop Out Value**- It is used to reduce Overfitting (also called Regularization), for each iteration we random select some % of neurons from each layer. Andrew NG 2 videos[Explanation ](https:\/\/www.youtube.com\/watch?v=ARq74QuavAo)[Implementation](https:\/\/www.youtube.com\/watch?v=D8PJAL-MZv8) , Dropout is not necessary until overfitting(high variance).\n\nNow Doing Search(Grid or Random) for best hyperparamter value is computationally expensive, so we will fix some and search some. [Article on hyperparameter tuning in DL](https:\/\/towardsdatascience.com\/hyperparameter-optimization-with-keras-b82e6364ca53)","7e3b5044":"## Model Compilation","a070f4a1":"## Traditional Artificial Neural Network(ANN)","dab17c51":"<a id = \"Step5\"><\/a>\n## Step 5: Build the Model","f9377f4c":"# Intuition behind Validation & Test set.\nBefore trying to find the sweet spot , first lets understand the difference between validation & test set.\nWhen we are building a model we want it to learn such that it able to  able to correctly classify similar new unseen data (not the same data).\n\nFirst thing is to get high training score( if not then Underfitting- also called high bias) then using new unseen dataset we want to control the learning such that it performs good enough on this data as well ( if not then high variance which means Training accuracy - validation accuracy is high (our model also has little variance)).\n\nThis unseen data which is used to control the learning of our model( when to stop) is called Validation set. (This generalization is done usually using Cross Validation Grid Search).[GridSearchCV](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_grid_search_digits.html)\n\nGenerally , Test set is where we check the actual accuracy of model where the model was trained on training set & generalized on validation.\n\n> Very Important point - If we get unsatisfied Test score & we change the parameters to improve the score using this test set ,it is not test set anymore -it has now become a validation set. When we are making multiple kaggle submission to increase the test set accuracy that does not give true Accuracy of model as we keep on adjusting the parameter to get the gold medal.\n\nNested Cross validation gives an estimate of the accuracy on test set.  [Video](https:\/\/www.youtube.com\/watch?v=DuDtXtKNpZs)","a5e8f97d":" We check the distribution of data of each class & we can see that its not equally distributed for each class label but the distributions within -5% to +15% of 4K(so no class is undersampled that much).\n And we can see that we follow the same distribution in train & validation set (using stratify parameter in train_test_split). ","d0c08896":"**But Why use Activation Function** ? Well Activation function helps in 2 things - it **help introduces non-linearity** (helps when the decision boundary is non-linear) & takes unbounded input($ -\\infty  to + \\infty$) to a predictable form (0,1) . \n\nAlso think about in this way when for next layers $a_2=f(w_2*a_1+b)$ if a_1 is range[0-1] & we initialize w & b properly (also close to 0)- (Xavier & He Initialization), we won't get high input value  & so we won't get stuck in the dead region.","dc82276b":"What is Model Compile? Compile defines the loss function, the optimizer and the metrics.Compile before Training becuse training tries to minimize **loss** using **optimizer** (algorithm to optimize) and return how well the model fit based on the **metric**.\n\nWe can also compile again after some training , if we need to redefine any of the 3 things .If you change the loss or optimizer we will lose the optimization it has already (not complete ) but the good thing is the weights remain the same.\n\nLet's see the below example:\n\nFind the shortest route between 1 and 7? \n\nCost function is to minimize distance between 1 & 7.\nOptimizer is the route we will take .\nmetric -gives how close we are from the final point\n\nLets say we compile the model with loss =l1 & optimizer=Opt1 & metric\n& based on the Opt1 we take 1-5-4-11-7 route , but lets say at point 4 we  recompile then model then basis new Opt2 it may take route 4-4-5-8-7 , we start from 4 not 1 (that is what i meant by saying \" weights remain same\"). (New metric cannot change the route it will just give different value based on how we define it).\n\nRecompile only when due to some reason we need to change loss,optimizer or metric.\n![Shortest route](https:\/\/i0.wp.com\/twominfun.com\/wp\/wp-content\/uploads\/2018\/04\/Shortest-route-puzzle-7-ans-1-2-5-7.png?fit=826%2C445)","c2b21ec9":"### No of parameters.\nTotal Parameters in Layer 1 : = (each  input dimensionX no of neurons in L1) + (bias in each neuron in L1)- (28X28X64) +64 =50240\n\nTotal Parameters in Layer 2 : = (no of neurons in L1 X no of neurons in L2) + (bias in each neuron in L2)- (64X64) +64 =4160\n\nsimilarly for Layer 3: (64X10)+10= 650.\n\nTotal Parameters = Parameters in L1 +Parameters in L2 + Parameters in L3 =(50240+4160+650)= 55,050","78833c30":"**Sequential**- gives a linear stack of layers which is what we want for ANN.\n\n**Dense** - means Fully Connected Layers where each input is connected to each output.\n\n**activatation function** - \"leaky relu\" which solves the problem of dead relu by introducing a small fraction,dead relu occurs when input(wx+b) <0 ,so learning is not possible as gradient is zero.\n\n**Different Activation Functions**\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*4ZEDRpFuCIpUjNgjDdT2Lg.png)","7b37ad35":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*gkXI7LYwyGPLU5dn6Jb6Bg.png) We divide each value by 255 to get each pixel value between 0 and 1 , we do this scaling because **with smaller inputs we will not endup in dead or saturated part of our activation function** , but what is the problem with these regions?- Because we calculate the gradient(slope) at that point & take step in the direction of local\/global minimum ,we will not be able to reach the local\/global minimum(also called vanishing Gradient). So we will analyse different type of Activation Functions how they are able to handle vanishing gradient.","d6f1e9ff":" lets also see how the pixel display the content(digit) of the image (few exmaples in original color & few in grayscale) - Can you guess the numbers ? ","06fd5d46":"![](https:\/\/miro.medium.com\/max\/2500\/1*ZB6H4HuF58VcMOWbdpcRxQ.png)**A Sample Neural Network ** -","eeca9694":"It is always important to define the Steps\n# Steps:\n1.  **Define Goal**- Given picture of Digits (0-9) ,Need to build a ANN which learns the imformation through Training & pedict when similar data is given.\n1. [Collect Data Set](#Step2) - All ready provided (train +test) which contains the values of 784 pixels  28X28 image (=784). Training set has 1st column which contains the  class of the samples. And if there is any missing value.\n1. [Transform section](#Step34)- Image set will be in matrix form with dimensions - nX784 where n is the no of samples , pixel value range from 0(black) to 255(white) , Transformations - divide by 255,data augmentation, why ?(in the section).\n1. [Visualization section](#Step34)- we can visualize pixel values to image ,Since we are operating on image & we won't be able to any insights here from the feature -label relation.\n1. [Build the Model](#Step5)- (ANN & its Hyperparameters) -We will build multiple layer neural network (no of layers & neurons in each layer is hyperparameter) . We will Train and Vaildate the Model using Cross validation.\n1. [Evaluate the Model](#Step6) - Plot Cost Function vs Iteration to analyse the performance of the model.","d6b512a3":"Before starting with CNN ,lets try ANN first ? ANN should work just fine(not great) because the number of weights won't be that much in this case, but it would be difficult to train image with high resolution (256X256X3) which just gives 12 million parameters in 1st layer(256X256X3X64 (assuming 64 neurons in 1st layer).\n\n> ANN Vs CNN?\nAnd the main reason because if we look at the different images of same class we can see that the number is always centered & with little rotation , so CNN detect features( such as edges) from group of pixels using filter(more detail in CNNpart) but ANN will look at each pixel individually ,which is not a great approach as it will be difficult for ANN to detect the image if we shift it just by 1 pixel. By looking at the examples & how different types are present it will be interesting to see of with enough layers(weights) if we will get good score","d6e0fc77":"Why  **Encode the output** ? such that in place of single value (5) we get vactor $[0 0 0 0 0 1 0 0 0 0]$ , But why ? Because the final output comes from softmax function which gives probability for each class ,let's say our initial weights were good enough so the softmax gave the output -    $[ 0.08,0.02,0,10,0.02,0.58,0.05,0.05,0.05,0.05]$ ,(softmax gives output such that sum=1) ,In our First Feedforward we got 0.58, now using Cost Function & Backpropogation(- means going back and updating the weights and biases) to increase 0.58 to 1(or close to 1).","4f76ebe2":"## Prediction & Submission","77e3d47c":"## Hyperparameters","7764ad17":"<a id = \"Step34\"><\/a>\n# Step 3,4- Transforming & Visulizing the Data","90fd8071":"let's see those 3% & try to understand why the model was not able to correctly identify.\n","7686947a":"### A sample ANN - Looking at the architecture we can see why its called Fully Connected.","317fb5df":"### Future Steps For Improving Accuracy:\n* Use Data Augmentation\n* Use Talos to do parameter Search - [Talos](https:\/\/towardsdatascience.com\/hyperparameter-optimization-with-keras-b82e6364ca53)\n* Use CNN ","6cff7bb9":"To understand the use of Hyperamater we need to look at this plot - Few sets of hyperparameter( control the training line - such that we achieve high accuracy with less no of iteration) & few sets of hyperparameters(Regularization- tries to stop the learning when we are not overfitting & we are getting highest accuracy(lowest loss) in validation set).\n\nRemember - The Aim is build a model that generalizes well , not to get high training accuracy. \n\nThe vertical line indicates the sweet spot.\n\n![Loss vs iterations](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTXFAnJKsUVfISgDz5wiMAXCbcNwZfF5-UKLQ&usqp=CAU)","79fda49a":" We know that pixel have value ranging 0(black) to 255 (white)- lets check the max & min value in our image & lets plot a matrix with value =[[255,0],[0,255]]","76a1f86a":" we can see how '2' changed to array with 0-1 encoding , But Why change data type to float? As discussed before the optimizer tries to minimize the cost function y=[0 0 1 0 0 0 0 0 0 0] & $y\\hat{}$ = [ 0.08,0.02,0,60,0.02,0.0.08,0.05,0.05,0.05,0.05] , now so that the cost function can compare these 2  we need to convert y to float(decimal points).","5a222158":"Tip for beginner - always remember to define just few libraries (like pandas,plt,numpy,keras(if required)) & others keep adding when required.","baa42424":"<a id = \"Step2\"><\/a>\nAs Goal is already defined , lets start with step 2:-\n# Step 2- Collect Data Set","ce2ab8a2":"##  Confusion Matrix "}}