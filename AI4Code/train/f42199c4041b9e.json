{"cell_type":{"16427d1b":"code","f5d27ec3":"code","10c8f9a0":"code","60dc72e1":"code","3945dd31":"code","1635161b":"code","bcf04e54":"code","ea6976b6":"code","a15ebe91":"code","37c60563":"code","69085c8d":"code","f6e977ca":"code","00b42ce2":"code","e818dab3":"code","992c4e4d":"code","ccf2ce20":"code","9d0366c9":"code","ddb81323":"code","c069bb83":"code","3ece09e3":"code","00646b46":"code","0f2d51f5":"code","9af05d82":"code","c18d54df":"code","7dd93587":"code","3a3c35da":"code","63ac4a58":"code","527ea06d":"code","5b34740c":"code","334849cc":"code","cf58ef04":"code","25faa670":"code","e41d0080":"code","5efa09c1":"code","4e2f4ee2":"code","709523ed":"code","bc4c7285":"code","6c08e295":"code","be5a47e8":"code","8ea094b8":"code","214e5f52":"code","e477d0c5":"code","c193e88b":"code","310bc9b0":"markdown","8b580b49":"markdown","236af3e2":"markdown","1ac6a987":"markdown","14378e67":"markdown","bba7b71b":"markdown","005e89d6":"markdown","759698fe":"markdown","bdab2d78":"markdown","93c3842f":"markdown","f07a91e4":"markdown","5185a196":"markdown","5f277c68":"markdown","0b4a043a":"markdown","5b3b3e4b":"markdown","d7a8a44c":"markdown","6b04aa4f":"markdown","09a2112a":"markdown","444301ac":"markdown","79bb31a6":"markdown","4ca7dd32":"markdown","4fb7dc73":"markdown","a80bb411":"markdown","fe305d02":"markdown","13f2f965":"markdown","fede6bf9":"markdown","df49b34a":"markdown","ef4d2b60":"markdown","19ef9e85":"markdown","d8144950":"markdown","06724a5e":"markdown","d41dd914":"markdown","5d99d890":"markdown","f9be029a":"markdown","a09e5d27":"markdown","6fc444d9":"markdown","bc8d792a":"markdown"},"source":{"16427d1b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f5d27ec3":"dataset = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ndataset.head()   #prints a nutshell of the dataset","10c8f9a0":"dataset.info()  #we get detailed info of the dataset","60dc72e1":"dataset.shape  #no of rows and columns","3945dd31":"dataset.describe()  #prints the numerical columns details","1635161b":"dataset.isnull().sum()","bcf04e54":"dataset['LoanAmount'] = dataset['LoanAmount'].fillna(dataset['LoanAmount'].mean())","ea6976b6":"dataset['Credit_History'] = dataset['Credit_History'].fillna(dataset['Credit_History'].median())","a15ebe91":"dataset.isnull().sum()","37c60563":"dataset.dropna(inplace=True)","69085c8d":"dataset.isnull().sum()","f6e977ca":"dataset.shape","00b42ce2":"print(pd.crosstab(dataset['Gender'],dataset['Loan_Status']))","e818dab3":"sns.countplot(dataset['Gender'],hue=dataset['Loan_Status'])","992c4e4d":"print(pd.crosstab(dataset['Married'],dataset['Loan_Status']))\nsns.countplot(dataset['Married'],hue=dataset['Loan_Status'])","ccf2ce20":"print(pd.crosstab(dataset['Self_Employed'],dataset['Loan_Status']))\nsns.countplot(dataset['Self_Employed'],hue=dataset['Loan_Status'])","9d0366c9":"print(pd.crosstab(dataset['Property_Area'],dataset['Loan_Status']))\nsns.countplot(dataset['Property_Area'],hue=dataset['Loan_Status'])","ddb81323":"dataset['Loan_Status'].replace('Y',1,inplace = True)\ndataset['Loan_Status'].replace('N',0,inplace = True)","c069bb83":"dataset['Loan_Status'].value_counts()","3ece09e3":"dataset.Gender=dataset.Gender.map({'Male':1,'Female':0})\ndataset['Gender'].value_counts()","00646b46":"dataset.Married=dataset.Married.map({'Yes':1,'No':0})\ndataset['Married'].value_counts()","0f2d51f5":"dataset.Dependents=dataset.Dependents.map({'0':0,'1':1,'2':2,'3+':3})\ndataset['Dependents'].value_counts()","9af05d82":"dataset.Education=dataset.Education.map({'Graduate':1,'Not Graduate':0})\ndataset['Education'].value_counts()","c18d54df":"dataset.Self_Employed=dataset.Self_Employed.map({'Yes':1,'No':0})\ndataset['Self_Employed'].value_counts()","7dd93587":"dataset.Property_Area=dataset.Property_Area.map({'Urban':2,'Rural':0,'Semiurban':1})\ndataset['Property_Area'].value_counts()","3a3c35da":"dataset['LoanAmount'].value_counts()","63ac4a58":"dataset['Loan_Amount_Term'].value_counts()","527ea06d":"dataset['Credit_History'].value_counts()","5b34740c":"plt.figure(figsize=(16,5))\nsns.heatmap(dataset.corr(),annot=True)\nplt.title('Correlation Matrix (for Loan Status)')","334849cc":"dataset.head()","cf58ef04":"X = dataset.iloc[:,1:-1].values\ny = dataset.iloc[:,-1].values","25faa670":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=0)","e41d0080":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","5efa09c1":"print(X_train)","4e2f4ee2":"import tensorflow as tf\ntf.__version__","709523ed":"ann = tf.keras.models.Sequential()","bc4c7285":"ann.add(tf.keras.layers.Dense(units=6, activation='relu'))","6c08e295":"ann.add(tf.keras.layers.Dense(units=6, activation='relu'))","be5a47e8":"ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))","8ea094b8":"ann.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])","214e5f52":"ann.fit(X_train, y_train, batch_size =32, epochs =100)","e477d0c5":"y_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1))","c193e88b":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","310bc9b0":"# Predicting the test set result","8b580b49":"# Creating the ANN Model","236af3e2":"> Comparison between Genders in getting the Loan:\n","1ac6a987":"# Our modified dataset","14378e67":"**Let's check the Missing values for the final time!**","bba7b71b":"> Lets check our dataset new shape","005e89d6":"# Checking the missing data","759698fe":"2.Crating a seceond hidden layer ","bdab2d78":" Here we can see married people has a greater chance to get the loan","93c3842f":"> Comparison between Married Status in getting the Loan:","f07a91e4":"> Comparison between Property Area for getting the Loan:","5185a196":"# Dataset Info","5f277c68":"**Importing the libraries**","0b4a043a":"Here, we can see that the Males have more chances to get the Loan.","5b3b3e4b":"# Feature scaling","d7a8a44c":"The tendency of loan varies semiurban > rural > urban ","6b04aa4f":"# Dataset Shape","09a2112a":"**Taking care of missing values in \"Loan Ammount\",\"credit history\" ","444301ac":"# Encoding of non-numerical values","79bb31a6":"# Deep dive into the dataset","4ca7dd32":"# Dataset Describtion","4fb7dc73":"3. Adding the output layer","a80bb411":"# Training the ANN model","fe305d02":"**Let's confirm if there are any missing values in 'LoanAmount' & 'Credit_History**","13f2f965":"#  Importing the libraries","fede6bf9":"1. Compiling the model","df49b34a":"2. Training the model","ef4d2b60":"# Making the confusion matrix","19ef9e85":"Here we can see not employed people has a greater chance to get the loan\n\n\n\n\n\n\n","d8144950":"# Importing and loading the dataset","06724a5e":"# Initialising the ANN","d41dd914":"# Spliting the dataset into train and test set","5d99d890":"**This method commonly used to handle the null values. Here, we either delete a particular row if it has a null value for a particular feature and a particular column if it has more than 70-75% of missing values. This method is advised only when there are enough samples in the data set. One has to make sure that after we have deleted the data, there is no addition of bias. Removing the data will lead to loss of information which will not give the expected results while predicting the output.**","f9be029a":"> Comparison between Self-Employed or Not in getting the Loan:\n","a09e5d27":"# Display the correlation matrix","6fc444d9":"**Now Let's drop all the missing value remaining **","bc8d792a":"1. Adding the first input layer and first hidden layer"}}