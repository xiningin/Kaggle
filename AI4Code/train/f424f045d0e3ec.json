{"cell_type":{"0808b3ef":"code","af13d00e":"code","a4a7809f":"code","326b5501":"code","487e4956":"code","34d635a3":"code","c3891517":"code","c81fadba":"code","2895b569":"markdown","5a04f12d":"markdown","731f78af":"markdown","25fb0604":"markdown","daf32f97":"markdown","44654c52":"markdown","2348ae10":"markdown","6691fffc":"markdown","88e6ae20":"markdown"},"source":{"0808b3ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.","af13d00e":"df = pd.read_csv(\"..\/input\/faults.csv\")\ndf.describe().T","a4a7809f":"from xlsxwriter.utility import xl_rowcol_to_cell\nconditions=[(df['Pastry'] == 1) & (df['Z_Scratch'] == 0)& (df['K_Scatch'] == 0)& (df['Stains'] == 0)& (df['Dirtiness'] == 0)& (df['Bumps'] == 0)& (df['Other_Faults'] == 0), (df['Pastry'] == 0) & (df['Z_Scratch'] == 1)& (df['K_Scatch'] == 0)& (df['Stains'] == 0)& (df['Dirtiness'] == 0)& (df['Bumps'] == 0)& (df['Other_Faults'] == 0),(df['Pastry'] == 0) & (df['Z_Scratch'] == 0)& (df['K_Scatch'] == 1)& (df['Stains'] == 0)& (df['Dirtiness'] == 0)& (df['Bumps'] == 0)& (df['Other_Faults'] == 0),(df['Pastry'] == 0) & (df['Z_Scratch'] == 0)& (df['K_Scatch'] == 0)& (df['Stains'] == 1)& (df['Dirtiness'] == 0)& (df['Bumps'] == 0)& (df['Other_Faults'] == 0),(df['Pastry'] == 0) & (df['Z_Scratch'] == 0)& (df['K_Scatch'] == 0)& (df['Stains'] == 0)& (df['Dirtiness'] == 1)& (df['Bumps'] == 0)& (df['Other_Faults'] == 0),(df['Pastry'] == 0) & (df['Z_Scratch'] == 0)& (df['K_Scatch'] == 0)& (df['Stains'] == 0)& (df['Dirtiness'] == 0)& (df['Bumps'] == 1)& (df['Other_Faults'] == 0),(df['Pastry'] == 0) & (df['Z_Scratch'] == 0)& (df['K_Scatch'] == 0)& (df['Stains'] == 0)& (df['Dirtiness'] == 0)& (df['Bumps'] == 0)& (df['Other_Faults'] == 1)]\nchoices = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\ndf['class'] = np.select(conditions, choices)\n#Dropping redundant column\n#Dropping Hot Encoding Classes\ndrp_cols=['TypeOfSteel_A400','Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\ndf.drop(choices, inplace=True,axis = 1)\ndf","326b5501":"color_code = {'Pastry':'Red', 'Z_Scratch':'Blue', 'K_Scatch':'Green', 'Stains':'Black', 'Dirtiness':'Pink', 'Bumps':'Brown', 'Other_Faults':'Gold'}\ncolor_list = [color_code.get(i) for i in df.loc[:,'class']]\npd.plotting.scatter_matrix(df.loc[:, df.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.3,\n                                       s = 50)\nplt.show()\nplt.savefig(\"figure_1.png\")","487e4956":"sns.countplot(x=\"class\", data=df)\ndf.loc[:,'class'].value_counts()","34d635a3":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nx,y = df.loc[:,df.columns != 'class'], df.loc[:,'class']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 2,shuffle=True)\n# Model complexity\nneig = np.arange(1, 27)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 27(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('k value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","c3891517":"x_train,x_test,y_train,y_test = train_test_split(x,y,stratify=y,test_size = 0.3,random_state = 2,shuffle=True)\n# Model complexity\nneig = np.arange(1, 27)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 27(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('k value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","c81fadba":"from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,stratify=y,test_size = 0.3,random_state = 8)\ntrees = np.arange(1, 50)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(trees):\n    # k from 1 to 27(exclude)\n    rf = RandomForestClassifier(random_state = 8, n_estimators=k, min_samples_split=2)\n    # Fit with rf\n    rf.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(rf.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(rf.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(trees, test_accuracy, label = 'Testing Accuracy')\nplt.plot(trees, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('No. of trees VS Accuracy')\nplt.xlabel('Number of Trees')\nplt.ylabel('Accuracy')\nplt.xticks(trees)\nplt.show()\n\nrf = RandomForestClassifier(random_state = 8, n_estimators=48, min_samples_split=2)\ny_pred = rf.fit(x_train,y_train).predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm,annot=True,fmt=\"d\") \nplt.title(\"Confusion Matrix\")\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\nprint('Classification report: \\n',classification_report(y_test,y_pred))\nprint(\"Best accuracy is {} with No. of trees = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))\n","2895b569":"# Pre-prepossing and Arranging Dataset Continued..\n3. Describing the color code of errors\n>         Error               Color\n>     1. 'Pastry'            'Red'\n>     2. 'Z_Scratch'         'Blue'\n>     3. 'K_Scatch'          'Green'\n>     4. 'Stains'            'Black'\n>     5. 'Dirtiness'         'Pink'\n>     6. 'Bumps'             'Brown'\n>     7. 'Other_Faults'      'Gold'     \n4. Plotting a scatter graph for general overview of errors in individual steel plates\n5. Displaying the graph for each attribute\n6. Save the figure","5a04f12d":"In random forests classifier, each tree in the ensemble is built from a sample drawn with replacement from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.\n\n>** Random Forest Classifier performs best with this complex dataset giving an approx accuracy of 80.1% with 48 trees.**\n","731f78af":"# Overview of total errors in the dataset","25fb0604":"# **Faulty Steel Plate Classification**\n\nFor the complex dataset below we have used KNN and Random Forest for classication of dataset. ","daf32f97":"# Model complexity:\n1. K is 12 which is small. Therefore, model is a complex model that can lead to overfitting. It means that model memozizes the trainning sets and cannot predict test set with good accuracy.\n2. If k is big, model that is less complex model can lead to underfit. (Not the case here)\n3. As you can see in plot, when K is 1 it memozize train sets and cannot give good accuracy on test set (overfit). Also if K is 12, model is leading to overfitting. Again, accuracy is not enough **as it looks like the model is just guessing** the test dataset even when K=12 is the best performance it gives. \n\n**Since the dataset is sparse and the error varies extremely i.e Other_Faults is at 673 and Dirtiness is at 55, it is better to try to get a better accuracy after stratifying the trainning dataset sample and checking the best performance.**","44654c52":"# Pre-processing and Arranging Dataset\n1. Dropping redundant column *'TypeOfSteel_A400'*\n2. Dropping hot encoding classes and creating one class column","2348ae10":"# Analyzing Dataset\n* Reading ***faults.csv*** file\n* Describing dataframe","6691fffc":"# Implementation of K-Nearest Neighbor Classifier\n1. Diving dataset into training and testing dataset\n2. Taking test size of 30% - Random shuffle\n3. Checking accuracy from k = 1 to 27\n4. Plot K-value vs Accuracy graph which shows the score of Training Accuracy and Testing Accuracy\n5. Extracting the K-value producing the best accurracy\n\n**K in general is called a hyperparameter and we need to choose the K that gives best performace.**\n\n**Below, K is ranged from 1 to 27(excluded) and accuracy is found for each K value.**\n","88e6ae20":"# **Again, we can see that K has improved just by little but still the model is overfitting **\nMoving on to Random Forest Classifier"}}