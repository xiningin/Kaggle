{"cell_type":{"49d5aa68":"code","66a77d26":"code","af1b9827":"code","51506522":"code","5cf48752":"code","7d4445a4":"code","9514e815":"code","4add5680":"code","7b22b5ed":"code","12489c7f":"code","f1731bc7":"code","1c6500bd":"code","ad24ce79":"code","7c5fe153":"code","02ecb860":"code","f5c609ab":"code","2c554aaf":"code","0b22d29e":"code","fd2ee9b2":"code","d23e0c7c":"code","1717779b":"markdown","9b999e52":"markdown","2a4eacc9":"markdown","24dafa9b":"markdown","5cb07946":"markdown","23d0bf1f":"markdown","e0d3bdfa":"markdown","b606d429":"markdown","e8b4af8a":"markdown","07915a75":"markdown","f914244c":"markdown","2e79f0ec":"markdown","5e2127aa":"markdown","b6b9a325":"markdown","1f80e52c":"markdown","3f01c393":"markdown","6da21b77":"markdown","6ace0704":"markdown","cb4406cb":"markdown","e7f8d075":"markdown","0874ed92":"markdown","d6bef20c":"markdown","19e159c3":"markdown","2ec9b6dc":"markdown","b7e65763":"markdown","19c24ff4":"markdown","29d11ee0":"markdown","ecd8487b":"markdown","1f41227d":"markdown","1318c18d":"markdown","cbbe482b":"markdown","3e28a350":"markdown","e630b891":"markdown","e234fac6":"markdown","e8f03aea":"markdown","89b7cb9e":"markdown"},"source":{"49d5aa68":"# Import libraries\nimport os, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras import backend as K\nfrom keras.models import load_model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Reproducability\n# https:\/\/keras.io\/getting_started\/faq\/#how-can-i-obtain-reproducible-results-using-keras-during-development.\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\n# Hide info, warnings and errors\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # filters out all TF messages","66a77d26":"x_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nx_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ny_train = x_train.pop('label')\nx_train = x_train.astype('float32') \/ 255.\nx_test = x_test.astype('float32') \/ 255.","af1b9827":"# MNIST images are 28x28X1 represented as a 784 dimensional vector\ninputs = keras.Input(shape=(784,)) \n\n# Create encoder, the latent space has 32-dimensions\nencoded = layers.Dense(32, activation='relu')(inputs) \nencoder = keras.Model(inputs, encoded)\n\n# Create decoder\nlatent_inputs = keras.Input(shape=(32,))\noutputs = layers.Dense(784, activation='relu')(latent_inputs)\ndecoder = keras.Model(latent_inputs, outputs)\n\n# Instantiate our autoencoder\noutputs = decoder(encoder(inputs))\nautoencoder = keras.Model(inputs, outputs, name='simplest_autoencoder')\n\n# Compile our autoencoder with Adam optimizer and a pixel-by-pixel loss (using binary crossentropy)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Train our autoencoder. We can use the images from the test set to evaluate the model loss on unseen data\nautoencoder.fit(x=x_train, y=x_train, epochs=100, batch_size=256, validation_data=(x_test, x_test))","51506522":"# Use the trained autoencoder to reconstruct the images from the test dataset\nx_test_reconstructed = autoencoder.predict(x_test)\n\n# Pick randomly and display 10 images and its recosntructed version \npos = 1\nfig=plt.figure(figsize=(20,5))\nfor i in (np.random.randint(0, x_test.shape[0], size=10)):\n    fig.add_subplot(2, 10, pos)\n    plt.imshow(np.array(x_test.iloc[i]).reshape(28,28))\n    plt.axis('off')\n    fig.add_subplot(2, 10, pos+10)\n    plt.imshow(x_test_reconstructed[i].reshape(28,28))\n    plt.axis('off')\n    pos = pos + 1\nplt.show()","5cf48752":"# Use our trained encoder to create the points in the latent space\nx_train_encoded = encoder.predict(x_train)\n\n# Scale the inputs before applying PCA\nx_train_enc_scaled = StandardScaler().fit_transform(x_train_encoded)\n\n# Create principal components\npca = PCA()\nx_train_enc_pca = pca.fit_transform(x_train_enc_scaled)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(x_train_enc_pca.shape[1])]\nx_train_enc_pca = pd.DataFrame(x_train_enc_pca, columns=component_names)\n\n# Add the labels\nx_train_enc_pca['label'] = y_train\n\n# Plot the points of the latent space, considering only PC1 and PC2\nplt.figure(figsize=(12, 10))\nsns.scatterplot(data=x_train_enc_pca, x='PC1', y='PC2', hue='label', palette=\"deep\")\nplt.show()","7d4445a4":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\nplot_variance(pca)\nplt.show()","9514e815":"def build_model(r=0.5):\n    \n    ##########################################################################################\n    # First, here's our encoder network, mapping inputs to our latent distribution parameters:\n    ##########################################################################################\n    original_dim = 28 * 28\n    intermediate_dim = 64\n    latent_dim = 2\n\n    inputs = keras.Input(shape=(original_dim,))\n    h = layers.Dense(intermediate_dim, activation='relu')(inputs)\n    z_mean = layers.Dense(latent_dim)(h)\n    z_log_sigma = layers.Dense(latent_dim)(h)\n\n    ##################################################################################\n    # We can use these parameters to sample new similar points from the latent space:\n    ##################################################################################\n\n    def sampling(args):\n        z_mean, z_log_sigma = args\n        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=0.1)\n        return z_mean + K.exp(z_log_sigma) * epsilon\n\n    z = layers.Lambda(sampling, name='encoder_sampling')([z_mean, z_log_sigma])\n\n    ################################################################################\n    # Finally, we can map these sampled latent points back to reconstructed inputs:\n    ################################################################################\n\n    # Create encoder\n    encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n\n    # Create decoder\n    latent_inputs = keras.Input(shape=(latent_dim,))\n    x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n    outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n    decoder = keras.Model(latent_inputs, outputs, name='decoder')\n\n    # instantiate VAE model\n    outputs = decoder(encoder(inputs)[2])\n    vae = keras.Model(inputs, outputs, name='vae')\n    \n    #We train the model using the end-to-end model, with a custom loss function: the sum of a reconstruction term, and the KL divergence regularization term.\n    reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n    reconstruction_loss *= original_dim\n    kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    vae_loss = r * reconstruction_loss + (1-r) * kl_loss\n    vae.add_loss(vae_loss)\n    vae.compile(optimizer='adam')\n    \n    return encoder, decoder, vae","4add5680":"encoder, decoder, vae = build_model(r=0.5)\n\nhistory = vae.fit(x_train, x_train,\n        epochs=100,\n        batch_size=32,\n        validation_data=(x_test, x_test))","7b22b5ed":"vae.save('vae.tf')\nencoder.save('vae_encoder.tf')\ndecoder.save('vae_decoder.tf')","12489c7f":"hist = pd.DataFrame(history.history)\nhist.plot()\nplt.show()\n\nfinal_loss = hist.iloc[99]['loss']\nfinal_val_loss = hist.iloc[99]['val_loss']\nprint(f'Final loss = {final_loss:.4f} and final val_loss = {final_val_loss:.4f}')","f1731bc7":"x_test_reconstructed = vae.predict(x_test)\n\npos = 1\nfig=plt.figure(figsize=(20,5))\nfor i in (np.random.randint(0, x_test.shape[0], size=10)):\n    fig.add_subplot(2, 10, pos)\n    plt.imshow(np.array(x_test.iloc[i]).reshape(28,28))\n    plt.title('img #' + str(i))\n    plt.axis('off')\n    fig.add_subplot(2, 10, pos+10)\n    plt.imshow(x_test_reconstructed[i].reshape(28,28))\n    plt.title('img #' + str(i))\n    plt.axis('off')\n    pos = pos + 1\nplt.show()","1c6500bd":"x_train_encoded = encoder.predict(x_train)\n\nz = pd.DataFrame([x_train_encoded[2][:,0],x_train_encoded[2][:,1],y_train]).T\nz.columns=['z_0', 'z_1', 'y_train']\n#z.head()\n\nplt.figure(figsize=(12, 10))\n#sns.scatterplot(x=x_train_encoded[2][:,0], y=x_train_encoded[2][:,1], hue=y_train, palette=\"deep\")\nsns.scatterplot(data=z, x='z_0', y='z_1', hue='y_train', palette=\"deep\")\nplt.show()","ad24ce79":"encoder_big_r, decoder_big_r, vae_big_r = build_model(r=0.999)\nhistory_big_r = vae_big_r.fit(x_train, x_train,\n        epochs=50,\n        batch_size=32,\n        validation_data=(x_test, x_test))\nvae_big_r.save('vae_big_r.tf')\n\nencoder_small_r, decoder_small_r, vae_small_r = build_model(r=0.001)\nhistory_small_r = vae_small_r.fit(x_train, x_train,\n        epochs=50,\n        batch_size=32,\n        validation_data=(x_test, x_test))\nvae_small_r.save('vae_small_r.tf')\n\nx_train_encoded_big_r = encoder_big_r.predict(x_train)\nz_big_r = pd.DataFrame([x_train_encoded_big_r[2][:,0],x_train_encoded_big_r[2][:,1],y_train]).T\nz_big_r.columns=['z_0', 'z_1', 'y_train']\nx_train_encoded_small_r = encoder_small_r.predict(x_train)\nz_small_r = pd.DataFrame([x_train_encoded_small_r[2][:,0],x_train_encoded_small_r[2][:,1],y_train]).T\nz_small_r.columns=['z_0', 'z_1', 'y_train']","7c5fe153":"fig=plt.figure(figsize=(20, 10))\nfig.add_subplot(1, 2, 1)\nsns.scatterplot(data=z_big_r, x='z_0', y='z_1', hue='y_train', palette=\"deep\")\nplt.title('r=0.999 -> reconstruction loss is predominant')\nfig.add_subplot(1, 2, 2)\nsns.scatterplot(data=z_small_r, x='z_0', y='z_1', hue='y_train', palette=\"deep\")\nplt.title('r=0.001 -> KL loss is predominant')\nplt.show()","02ecb860":"fig=plt.figure(figsize=(20, 4))\nfig.suptitle('When predominance is given to the reconstruction loss, the encoder is trained without constraint on z distribution')\nfig.add_subplot(1, 2, 1)\nsns.histplot(data=z_big_r['z_0'], kde=True)\nfig.add_subplot(1, 2, 2)\nsns.histplot(data=z_big_r['z_1'], kde=True)\nplt.show()\n\nfig=plt.figure(figsize=(20, 4))\nfig.suptitle('When predominance is given to the KL loss, the encoder is trained with a constraint so z is a unit Gaussian distribution')\nfig.add_subplot(1, 2, 1)\nsns.histplot(data=z_small_r['z_0'], kde=True)\nfig.add_subplot(1, 2, 2)\nsns.histplot(data=z_small_r['z_1'], kde=True)\nplt.show()","f5c609ab":"# Take 10 points randomly in this latent space\nrandom_points = np.random.normal(loc=0.0, scale=1.0, size=(10,2))\n\n# Create \"new\" images for these 10 points\ndecoder = load_model('vae_decoder.tf')\nnew_images = decoder.predict(random_points)\n\n# Display new images\npos = 1\nfig=plt.figure(figsize=(20,5))\nfor i in range(0,10):\n    fig.add_subplot(1, 10, pos)\n    plt.imshow(new_images[i].reshape(28,28))\n    plt.title(np.around(random_points[i],2))\n    plt.axis('off')\n    pos = pos + 1\nplt.show()","2c554aaf":"# create noisy inputs\nnoise_factor = 0.4\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n# format the inputs to the format expected by our convolutional autoencoder\nx_train = np.array(x_train).reshape(42000,28,28)\nx_test = np.array(x_test).reshape(28000,28,28)\nx_train_noisy = np.array(x_train_noisy).reshape(42000,28,28)\nx_test_noisy = np.array(x_test_noisy).reshape(28000,28,28)\n\n# display noisy inputs\npos = 1\nfig=plt.figure(figsize=(20,5))\nfor i in (np.random.randint(0, x_train_noisy.shape[0], size=10)):\n    fig.add_subplot(1, 10, pos)\n    plt.imshow(x_train_noisy[i])\n    plt.axis('off')\n    pos = pos + 1\nplt.show()","0b22d29e":"input = layers.Input(shape=(28, 28, 1))\n\n# Encoder\nx = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input)\nx = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\nx = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\nx = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n\n# Decoder\n# Note : UpSampling2D is just a simple scaling up of the image by using nearest neighbour or bilinear upsampling, \n# so nothing smart. Conv2DTranspose is a convolution operation whose kernel is learnt (just like normal conv2d operation) \n# while training your model. Using Conv2DTranspose will also upsample its input but the key difference is the model \n# should learn what is the best upsampling for the job.\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\nx = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n\n# Autoencoder\nautoencoder = keras.Model(input, x)\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")","fd2ee9b2":"autoencoder.fit(\n    x=x_train_noisy,\n    y=x_train,\n    epochs=50,\n    batch_size=128,\n    shuffle=True,\n    validation_data=(x_test_noisy, x_test),\n)","d23e0c7c":"x_test_denoised = autoencoder.predict(x_test_noisy)\n\npos = 1\nfig=plt.figure(figsize=(20,2))\nfig.suptitle('Original images')\nfor i in range(0,10):\n    fig.add_subplot(1, 10, pos)\n    plt.imshow(x_test[i].reshape(28,28))\n    plt.axis('off')\n    pos = pos + 1\nplt.show()\n\npos = 1\nfig=plt.figure(figsize=(20,2))\nfig.suptitle('Images denoised by the autoencoder')\nfor i in range(0,10):\n    fig.add_subplot(1, 10, pos)\n    plt.imshow(x_test_denoised[i].reshape(28,28))\n    plt.axis('off')\n    pos = pos + 1\nplt.show()","1717779b":"The answer is simple : it creates clusters in the latent space of inputs that are structurally similar. For instance, with the MNIST dataset, we can expect :\n* to find one cluster for the hand-written 1, another cluster for the hand-written 2, ... \n* it is likely that the 1's cluster is closer to the 7's cluster than the 5's cluster because a 1 look more like a 7 than a 5...  ","9b999e52":"We can use our model to remove the noise that was added to the test images in step 1. and display the output of the autoencoder. As you can see, there is little difference between the original image and the output of the autoencoder, quite impressive! ","2a4eacc9":"Autoencoders are lossy which means that the decompressed outputs will be degraded compared to the original inputs. The complexity of your model and the size of the latent reprensentation will allow you to manage the loss \/ complexity trade-off. Here with a very simple model and a 32-dimensional latent representation, this is not so bad at all!  ","24dafa9b":"![clusters of digits - small.png](attachment:d03637b1-b9b3-49b1-b097-db0c83c975a7.png)\n\nsource: [Hackernoon](https:\/\/hackernoon.com\/latent-space-visualization-deep-learning-bits-2-bd09a46920df)","5cb07946":"Below, we can check the influence of the importance given to the reconstruction loss versus the KL loss. Here, I took \"extreme\" values for r for the sake of the demonstration:\n* when the predominance is given to the reconstruction_loss (r=0.999), **the clusters are clear and pretty-well separated**. On the other hand,the distribution of the latent vector spreads over a big range (z_0 in [-20,50] and z_1 in [0, 60])\n* when the predominance is given to the kl_loss (r=0.001), the clusters overlap more but **the distribution of the latent vector is a unit Gaussian distribution**.","23d0bf1f":"# 2. Autoencoder","e0d3bdfa":"**Step 2 : create a convolutional autoencoder**","b606d429":"# 5. References","e8b4af8a":"Now we can try our VAE with the test images. The output is not that bad when you consider the original image is a 784-dimensional vector and the latent input is a 2-dimensional vector : some reconstructed images display the same digit as in the original image and others display a different digit! And if you train the VAE with a larger r, that is to say giving more predominance to the reconstruction_loss over the kl_loss, the reconstruction will be even better.","07915a75":"Variational Autoencoder is a type of autoencoder with added constraints on the encoded representations being learnt. Instead of letting the neural network learn an arbitrary function, it is learning the parameters of a probability distribution modeling your input data. The implementation below was proposed by F. Chollet [here](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html) with some minor adaptation. \n\nOur VAE is an end-to-end autoencoder mapping inputs to reconstructions with :\n* an encoder mapping inputs to the latent space (here a 2D latent space)\n* a decoder that can take points on the latent space and output the corresponding reconstructed images.\n\nAutoencoders use a pixel-by-pixel loss function. Here, our VAE loss combines 2 different loss functions:\n* A reconstruction loss (pixel-by-pixel) function\n* A KL (a.k.a. Kullback-Leibler) loss function  \n\nIn this implementation, it is a weighted average of these 2 losses : vae_loss = r * reconstruction_loss + (1-r) * kl_loss","f914244c":"This notebook was inspired by my latest readings on autoencoders and variational autoencoders (VAE). All the references can be found throughout this notebook and are aggregated in the last section.\n\nIn one sentence, an autoencoder is a type of neural networks that is trained to copy its input to its output. So if you are wondering \"why on earth a neural network that acts as an identity function could be of any interest?\", I hope that you will learn something interesting in this notebook. ","2e79f0ec":"The loss function in this case is \"a distance\" between input and output and is called a reconstruction loss. E.g. when the input is an image, a pixel-by-pixel comparison using RMSE or binary entropy can be used.  ","5e2127aa":"![vae.png](attachment:82abae00-6262-4b7b-98ed-743b13db7152.png)","b6b9a325":"# 3. Variational autoencoder (VAE)","1f80e52c":"![autoencoder_small.png](attachment:163d9f12-deec-4a2d-b9b6-e37a3a0a78f4.png)","3f01c393":"* [Tutorial on autoencoders and VAE (video in French)](https:\/\/www.youtube.com\/watch?v=lTy_AAjSTD8)\n* [Auto-Encoding Variational Bayes](https:\/\/arxiv.org\/abs\/1312.6114)\n* [How to Calculate the KL Divergence for Machine Learning](https:\/\/machinelearningmastery.com\/divergence-between-probability-distributions\/)\n* [Dog autoencoder](https:\/\/www.kaggle.com\/cdeotte\/dog-autoencoder)\n* [Building autoencoders in keras](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html)\n* [Understanding latent space in machine learning](https:\/\/towardsdatascience.com\/understanding-latent-space-in-machine-learning-de5a7c687d8d)","6da21b77":"We can see the clusters in the above plot but it is a bit messy: the clusters are not clearly separated but this plot allows to understand the general idea. \n\n*Note : why the clusters are not clearly separated? it's because 2 principal components do not represent well enough the points in the 32-dimensional latent space (only 20% of the variance is explained by the 2 first Principal Components). I guess the clusters are much well separated in the 32-dimensional latent space but it's pretty hard to visualize*\ud83e\udd14","6ace0704":"## 4.2 Denoising Models ","cb4406cb":"In this section, I'm going to show 2 applications of autoencoders. There are, of course, many more possible applications of autoencoders and if you know them, please add them in the comments section below. ","e7f8d075":"**Step 1 : create noisy inputs by adding a noise to x_train, x_test clean images**","0874ed92":"# 1. Introduction","d6bef20c":"# 4. Applications of autoencoders","19e159c3":"It's time to train our VAE:","2ec9b6dc":"You can use autoencoders to denoise inputs. To achieve this, the training process is slightly different : instead of training the autoencoder to copy its input to its output, the autoencoder is trained to map noisy inputs to clean outputs. E.g. train your autoencoder to map noisy digits images from the MNIST dataset to clean digits images. The implementation below is proposed by Santiago L. Valdarrama [here](https:\/\/keras.io\/examples\/vision\/autoencoder\/).","b7e65763":"An autoencoder is a special type of neural network that is trained to copy its input to its output. For example, given an image of a handwritten digit, an autoencoder first encodes the image into a lower dimensional latent representation, then decodes the latent representation back to an image. An autoencoder learns to compress the data while minimizing the reconstruction error.","19c24ff4":"**Step 3 : train the autoencoder**","29d11ee0":"**How does an autoencoder work?**","ecd8487b":"Below, the most simple possible autoencoder where the encoder and the decoder are composed of a single Dense layer. Let's train it and then use it to reconstruct the images of the test dataset:","1f41227d":"Autoencoders can be applied to any type of input data. In this notebook, I'll use an image dataset, the famous MNIST dataset. So let's start loading the data and normalizing all image pixels between 0 and 1:","1318c18d":"*Note : in this [notebook](https:\/\/www.kaggle.com\/cdeotte\/dog-autoencoder) created by GM Chris Deotte, you can find another example of generation of images of dogs using encoders* ","cbbe482b":"An autoencoder is a \"generative model\". Once your autoencoder is trained, just sample points from the latent space, you can generate new input data samples. Below, you can see 10 new \"MNIST-like\" images generated by the decoder that we created in the VAE section. We also display the coordinates of the points in the 2d latent space that were used to generate these images:","3e28a350":"*Note: there was a [competition on Kaggle](https:\/\/www.kaggle.com\/c\/denoising-dirty-documents) a few years ago where the participants were requested to remove noise from printed text. Many solutions implemented autoencoders.*","e630b891":"## 4.1 Generative Models ","e234fac6":"It's not straightforward to illustrate with the example above because the latent representation is 32-dimensional. Let's use a dimension reduction technique e.g. PCA to display the latent representation in a 2-d space. ","e8f03aea":"Because our latent space is 2-dimensional, we can look at the neighborhoods of different classes on the latent 2D plane. Each of these colored clusters is a type of digit. Close clusters are digits that are structurally similar.","89b7cb9e":"**Step 4 : denoise the images from the test set**"}}