{"cell_type":{"9a13f79a":"code","35adfe37":"code","c0fffaaa":"code","46e8fe0e":"code","a01d6673":"code","03b9f2b9":"code","5d924f0e":"code","775b4a3b":"code","828cd876":"code","adea5169":"code","640c3d0f":"code","de6e7d73":"code","7696ae5c":"code","5a272b27":"code","d49efec8":"code","39ec1b43":"code","e056c1bf":"code","6c9183ff":"code","11b6dcbe":"code","027a5e31":"code","71eb6654":"code","5261387d":"code","21ebf709":"code","9619c190":"code","e6750dd2":"code","43e145cf":"code","ffff096b":"code","6c50c85d":"code","021d6a18":"code","82e93b83":"code","3a4b42aa":"code","4f036980":"code","59f807b9":"code","9667cd49":"code","b6c9b164":"code","53190736":"code","b2322178":"code","5130e533":"code","ec7a2564":"code","daae2f02":"code","ae142206":"code","4c63903d":"markdown","abb319eb":"markdown","df960070":"markdown","faaf8fee":"markdown","9805ba13":"markdown","bfbfa1dc":"markdown","80e9b062":"markdown","2131134a":"markdown","1f89e50b":"markdown","bbd96a45":"markdown"},"source":{"9a13f79a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\ndataset=pd.read_csv('..\/input\/pcos-prediction\/ASIS.csv')","35adfe37":"dataset.sample(5)\n","c0fffaaa":"dataset[\"Age (yrs)\"]=((dataset[\"Age (yrs)\"]-dataset[\"Age (yrs)\"].min())\/(dataset[\"Age (yrs)\"].max()-dataset[\"Age (yrs)\"].min()))","46e8fe0e":"dataset.sample(5)\n","a01d6673":"dataset[\"BMI\"]=((dataset[\"BMI\"]-dataset[\"BMI\"].min())\/(dataset[\"BMI\"].max()-dataset[\"BMI\"].min()))","03b9f2b9":"dataset.sample(5)","5d924f0e":"print(dataset.shape)","775b4a3b":"X = pd.DataFrame(dataset.iloc[:,:-1])\ny =pd.DataFrame(dataset.iloc[:,-1])\n","828cd876":"dataset['PCOS'].value_counts()","adea5169":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.countplot(x='PCOS',data=dataset,palette='mako')\nplt.show","640c3d0f":"# Visualizing the data using heatmap\nsns.heatmap(dataset.corr(), cmap=\"BuPu\")\nplt.show()","de6e7d73":"dataset.sample(5)","7696ae5c":"import sklearn\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neural_network import MLPRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.metrics import r2_score\ndataset.drop(['Age (yrs)'], axis = 1)","5a272b27":"print(dataset.shape)\ndataset.describe().transpose()","d49efec8":"target_column = ['PCOS'] \npredictors = list(set(list(dataset.columns))-set(target_column))\ndataset[predictors] = dataset[predictors]\/dataset[predictors].max()\ndataset.describe().transpose()","39ec1b43":"X = dataset[predictors].values\ny = dataset[target_column].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=45)\nprint(X_train.shape); print(X_test.shape)","e056c1bf":"dataset.columns","6c9183ff":"feature_names = ['Age (yrs)', 'BMI', 'Cycle(R\/I)', 'Weight gain(Y\/N)',\n       'hairgrowth(Y\/N)', 'Skin darkening (Y\/N)', 'Hair loss(Y\/N)',\n       'Pimples(Y\/N)', 'Fast food (Y\/N)', 'Reg.Exercise(Y\/N)']\nprint(feature_names)","11b6dcbe":"from sklearn.model_selection import RepeatedKFold\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import SelectKBest\n\ndef select_features(X_train, y_train, X_test):\n\t# configure to select all features\n\tfs = SelectKBest(score_func=f_regression, k='all')\n\t# learn relationship from training data\n\tfs.fit(X_train, y_train)\n\t# transform train input data\n\tX_train_fs = fs.transform(X_train)\n\t# transform test input data\n\tX_test_fs = fs.transform(X_test)\n\treturn X_train_fs, X_test_fs, fs\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n# what are scores for the features\nfor i in range(len(fs.scores_)):\n\tprint('%s: %f' % (feature_names[i], fs.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.show()","027a5e31":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(hidden_layer_sizes=(5,8,5), activation='relu', solver='adam', max_iter=1000)\nmlp.fit(X_train,y_train.ravel())\n\npredict_train = mlp.predict(X_train)\npredict_test = mlp.predict(X_test)\ny3 = predict_test[:15]","71eb6654":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_train,predict_train))\nprint(classification_report(y_train,predict_train))","5261387d":"print(confusion_matrix(y_test,predict_test))\nprint(classification_report(y_test,predict_test))","21ebf709":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train,y_train.ravel())\npredict_train = clf.predict(X_train)\npredict_test = clf.predict(X_test)\ny1 = predict_test[:15]\nx= y_test[:15]","9619c190":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_train,predict_train))\nprint(classification_report(y_train,predict_train))","e6750dd2":"print(confusion_matrix(y_test,predict_test))\nprint(classification_report(y_test,predict_test))\n","43e145cf":"from sklearn import svm\n\nclf = svm.SVC()\nclf = clf.fit(X_train,y_train.ravel())\npredict_train = clf.predict(X_train)\npredict_test = clf.predict(X_test)","ffff096b":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n  \n\n  \n# Obtain scores from learning curve function\n# cv is the number of folds while performing Cross Validation\nsizes, training_scores, testing_scores = learning_curve(sklearn.svm.SVC(gamma= True), X, np.ravel(y,order='C'), cv=10, scoring='accuracy', train_sizes=np.linspace(0.01, 1.0, 50))\n  \n# Mean and Standard Deviation of training scores\nmean_training = np.mean(training_scores, axis=1)\nStandard_Deviation_training = np.std(training_scores, axis=1)\n  \n# Mean and Standard Deviation of .testing scores\nmean_testing = np.mean(testing_scores, axis=1)\nStandard_Deviation_testing = np.std(testing_scores, axis=1)\n  \n# dotted blue line is for training scores and green line is for cross-validation score\nplt.plot(sizes, mean_training, '--', color=\"b\",  label=\"Training score\")\nplt.plot(sizes, mean_testing, color=\"g\", label=\"Cross-validation score\")\nplt.legend(loc='lower right')\n  \n# Drawing plot\nplt.title(\"LEARNING CURVE FOR SVM Classifier\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","6c50c85d":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_train,predict_train))\nprint(classification_report(y_train,predict_train))","021d6a18":"print(confusion_matrix(y_test,predict_test))\nprint(classification_report(y_test,predict_test))","82e93b83":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf = clf.fit(X_train,y_train.ravel())\npredict_train = clf.predict(X_train)\npredict_test = clf.predict(X_test)\ny2= predict_test[:15]","3a4b42aa":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n  \n\n  \n# Obtain scores from learning curve function\n# cv is the number of folds while performing Cross Validation\nsizes, training_scores, testing_scores = learning_curve(GaussianNB(), X, np.ravel(y,order='C'), cv=10, scoring='accuracy', train_sizes=np.linspace(0.01, 1.0, 50))\n  \n# Mean and Standard Deviation of training scores\nmean_training = np.mean(training_scores, axis=1)\nStandard_Deviation_training = np.std(training_scores, axis=1)\n  \n# Mean and Standard Deviation of testing scores\nmean_testing = np.mean(testing_scores, axis=1)\nStandard_Deviation_testing = np.std(testing_scores, axis=1)\n  \n# dotted blue line is for training scores and green line is for cross-validation score\nplt.plot(sizes, mean_training, '--', color=\"b\",  label=\"Training score\")\nplt.plot(sizes, mean_testing, color=\"g\", label=\"Cross-validation score\")\nplt.legend(loc='lower right')\n  \n# Drawing plot\nplt.title(\"LEARNING CURVE FOR Naive's bayes Classifier\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","4f036980":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_train,predict_train))\nprint(classification_report(y_train,predict_train))","59f807b9":"print(confusion_matrix(y_test,predict_test))\nprint(classification_report(y_test,predict_test))","9667cd49":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0)\nclf = clf.fit(X_train,y_train.ravel())\npredict_train = clf.predict(X_train)\npredict_test = clf.predict(X_test)","b6c9b164":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_train,predict_train))\nprint(classification_report(y_train,predict_train))","53190736":"print(confusion_matrix(y_test,predict_test))\nprint(classification_report(y_test,predict_test))","b2322178":"from sklearn import neighbors\nclf = neighbors.KNeighborsClassifier(n_neighbors=10)\nclf = clf.fit(X_train,y_train.ravel())\npredict_train = clf.predict(X_train)\npredict_test = clf.predict(X_test)\nprint(confusion_matrix(y_test,predict_test))\nprint(classification_report(y_test,predict_test))","5130e533":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nclf = clf.fit(X_train,y_train.ravel())\npredict_train = clf.predict(X_train)\npredict_test = clf.predict(X_test)\n\n\n\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_train,predict_train))\nprint(classification_report(y_train,predict_train))\n\nprint(confusion_matrix(y_test,predict_test))\nprint(classification_report(y_test,predict_test))","ec7a2564":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.cluster import KMeans\n  \n\n  \n# Obtain scores from learning curve function\n# cv is the number of folds while performing Cross Validation\nsizes, training_scores, testing_scores = learning_curve(KMeans(n_clusters=2), X, np.ravel(y,order='C'), cv=10, scoring='accuracy', train_sizes=np.linspace(0.01, 1.0, 50))\n  \n# Mean and Standard Deviation of training scores\nmean_training = np.mean(training_scores, axis=1)\nStandard_Deviation_training = np.std(training_scores, axis=1)\n  \n# Mean and Standard Deviation of testing scores\nmean_testing = np.mean(testing_scores, axis=1)\nStandard_Deviation_testing = np.std(testing_scores, axis=1)\n  \n# dotted blue line is for training scores and green line is for cross-validation score\nplt.plot(sizes, mean_training, '--', color=\"b\",  label=\"Training score\")\nplt.plot(sizes, mean_testing, color=\"g\", label=\"Cross-validation score\")\nplt.legend(loc='lower right')\n  \n# Drawing plot\nplt.title(\"LEARNING CURVE FOR KMeans Classifier\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","daae2f02":"'''Comparing All Model '''\n\nmarker =['o','.','x','+','v','^','<','>','s','d']\nplt.figure(figsize=(8,2),dpi=100)\nplt.plot(x,y1,marker[3],color='red',markersize=4,linestyle=':',label= 'Decison Tree')\n\nplt.plot(x,y2,marker[2],color='black',markersize=4,linestyle=':',label= 'Naiyes bayes')\nplt.plot(x,y3,marker[8],color='yellow',markersize=4,linestyle=':',label= ' MLP Classifier')\nplt.xlabel(\"test\")\nplt.ylabel(\"Deviation in prediction\")\nplt.axis('equal')\nplt.legend(loc='lower left')\nplt.title(\"Y_test vs y_predict\")\nplt.show()","ae142206":"import matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')","4c63903d":"*** PCOS Portfolio ***","abb319eb":"*** NAIVE BAYES MODEL ***","df960070":"Important Feature","faaf8fee":"*** K-nearest neighbour ***","9805ba13":"*** SVM MODEL ***","bfbfa1dc":"*** random forest ***","80e9b062":"*** DECISION TREE MODEL ***","2131134a":"*** MLP MODEL ***","1f89e50b":"*** LOGISTIC REGRESSION MODEL ***","bbd96a45":"*** DATA VISULIZATION AND PRE-PROCESSING ***"}}