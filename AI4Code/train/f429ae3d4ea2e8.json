{"cell_type":{"4183ac6b":"code","da4e76b3":"code","12c26bd4":"code","a87efb15":"code","e6766959":"code","e485adcc":"code","30abb461":"code","4bf64bd9":"code","c9ce55bd":"code","31bc1a7f":"code","9c028967":"code","6fda2989":"code","2bac828a":"code","1aeb5ada":"code","01c8c6e6":"markdown","b6bb86c8":"markdown","743e77f1":"markdown","7331c937":"markdown","d62451eb":"markdown","ff9ecf69":"markdown","f3470300":"markdown","41890311":"markdown","b483e2dc":"markdown","ef4b6fae":"markdown"},"source":{"4183ac6b":"%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","da4e76b3":"data_path = '..\/input\/hourcsv\/hour.csv'\n\nrides = pd.read_csv(data_path)","12c26bd4":"rides.head()","a87efb15":"rides[:24*10].plot(x='dteday', y='cnt')","e6766959":"dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\nfor each in dummy_fields:\n    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n    rides = pd.concat([rides, dummies], axis=1)\n\nfields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\ndata = rides.drop(fields_to_drop, axis=1)\ndata.head()","e485adcc":"quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n# Store scalings in a dictionary so we can convert back later\nscaled_features = {}\nfor each in quant_features:\n    mean, std = data[each].mean(), data[each].std()\n    scaled_features[each] = [mean, std]\n    data.loc[:, each] = (data[each] - mean)\/std","30abb461":"# Save data for approximately the last 21 days \ntest_data = data[-21*24:]\n\n# Now remove the test data from the data set \ndata = data[:-21*24]\n\n# Separate the data into features and targets\ntarget_fields = ['cnt', 'casual', 'registered']\nfeatures, targets = data.drop(target_fields, axis=1), data[target_fields]\ntest_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]","4bf64bd9":"# Hold out the last 60 days or so of the remaining data as a validation set\ntrain_features, train_targets = features[:-60*24], targets[:-60*24]\nval_features, val_targets = features[-60*24:], targets[-60*24:]","c9ce55bd":"from helper_functions_bike_py import NeuralNetwork","31bc1a7f":"def MSE(y, Y):\n    return np.mean((y-Y)**2)","9c028967":"import unittest\n\ninputs = np.array([[0.5, -0.2, 0.1]])\ntargets = np.array([[0.4]])\ntest_w_i_h = np.array([[0.1, -0.2],\n                       [0.4, 0.5],\n                       [-0.3, 0.2]])\ntest_w_h_o = np.array([[0.3],\n                       [-0.1]])\n\nclass TestMethods(unittest.TestCase):\n    \n    ##########\n    # Unit tests for data loading\n    ##########\n    \n    def test_data_path(self):\n        # Test that file path to dataset has been unaltered\n        self.assertTrue(data_path.lower() == '..\/input\/hourcsv\/hour.csv')\n        \n    def test_data_loaded(self):\n        # Test that data frame loaded\n        self.assertTrue(isinstance(rides, pd.DataFrame))\n    \n    ##########\n    # Unit tests for network functionality\n    ##########\n\n    def test_activation(self):\n        network = NeuralNetwork(3, 2, 1, 0.5)\n        # Test that the activation function is a sigmoid\n        self.assertTrue(np.all(network.activation_function(0.5) == 1\/(1+np.exp(-0.5))))\n\n    def test_train(self):\n        # Test that weights are updated correctly on training\n        network = NeuralNetwork(3, 2, 1, 0.5)\n        network.weights_input_to_hidden = test_w_i_h.copy()\n        network.weights_hidden_to_output = test_w_h_o.copy()\n        \n        network.train(inputs, targets)\n        self.assertTrue(np.allclose(network.weights_hidden_to_output, \n                                    np.array([[ 0.37275328], \n                                              [-0.03172939]])))\n        self.assertTrue(np.allclose(network.weights_input_to_hidden,\n                                    np.array([[ 0.10562014, -0.20185996], \n                                              [0.39775194, 0.50074398], \n                                              [-0.29887597, 0.19962801]])))\n\n    def test_run(self):\n        # Test correctness of run method\n        network = NeuralNetwork(3, 2, 1, 0.5)\n        network.weights_input_to_hidden = test_w_i_h.copy()\n        network.weights_hidden_to_output = test_w_h_o.copy()\n\n        self.assertTrue(np.allclose(network.run(inputs), 0.09998924))\n\nsuite = unittest.TestLoader().loadTestsFromModule(TestMethods())\nunittest.TextTestRunner().run(suite)","6fda2989":"import sys\n\n####################\n### Set the hyperparameters in you myanswers.py file ###\n####################\n\nfrom helper_functions_bike_py import iterations, learning_rate, hidden_nodes, output_nodes\n\n\nN_i = train_features.shape[1]\nnetwork = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n\nlosses = {'train':[], 'validation':[]}\nfor ii in range(iterations):\n    # Go through a random batch of 128 records from the training data set\n    batch = np.random.choice(train_features.index, size=128)\n    X, y = train_features.iloc[batch].values, train_targets.iloc[batch]['cnt']\n                             \n    network.train(X, y)\n    \n    # Printing out the training progress\n    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)\n    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)\n    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii\/float(iterations)) \\\n                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n                     + \" ... Validation loss: \" + str(val_loss)[:5])\n    sys.stdout.flush()\n    \n    losses['train'].append(train_loss)\n    losses['validation'].append(val_loss)","2bac828a":"plt.plot(losses['train'], label='Training loss')\nplt.plot(losses['validation'], label='Validation loss')\nplt.legend()\n_ = plt.ylim()","1aeb5ada":"fig, ax = plt.subplots(figsize=(8,4))\n\nmean, std = scaled_features['cnt']\npredictions = network.run(test_features).T*std + mean\nax.plot(predictions[0], label='Prediction')\nax.plot((test_targets['cnt']*std + mean).values, label='Data')\nax.set_xlim(right=len(predictions))\nax.legend()\n\ndates = pd.to_datetime(rides.iloc[test_data.index]['dteday'])\ndates = dates.apply(lambda d: d.strftime('%b %d'))\nax.set_xticks(np.arange(len(dates))[12::24])\n_ = ax.set_xticklabels(dates[12::24], rotation=45)","01c8c6e6":"# Predicting Bike Sharing Patterns\n\nIn this project, I built neural network and use it to predict daily bike rental ridership.\n\n","b6bb86c8":"## Unit tests\n\nRun these unit tests to check the correctness of your network implementation. This will help you be sure your network was implemented correctly befor you starting trying to train it. These tests must all be successful to pass the project.","743e77f1":"## Check out the predictions\n\nHere, use the test data to view how well the network is modeling the data. If something is completely wrong here, make sure each step in the network is implemented correctly.","7331c937":"I'll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, I'll train on historical data, then try to predict on future data (the validation set).","d62451eb":"### Dummy variables\nHere we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables. This is simple to do with Pandas thanks to `get_dummies()`.","ff9ecf69":"## Load and prepare the data\n\nA critical step in working with neural networks is preparing the data correctly. Variables on different scales make it difficult for the network to efficiently learn the correct weights.","f3470300":"## Checking out the data\n\nThis dataset has the number of riders for each hour of each day from January 1 2011 to December 31 2012. The number of riders is split between casual and registered, summed up in the `cnt` column. You can see the first few rows of the data above.\n\nBelow is a plot showing the number of bike riders over the first 10 days or so in the data set. (Some days don't have exactly 24 entries in the data set, so it's not exactly 10 days.) You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders.","41890311":"### Scaling target variables\nTo make training the network easier, I'll standardize each of the continuous variables. That is, I'll shift and scale the variables such that they have zero mean and a standard deviation of 1.\n\nThe scaling factors are saved so we can go backwards when we use the network for predictions.","b483e2dc":"## Time to build the network\n\nBelow, follow these tasks:\n1. Implement the sigmoid function to use as the activation function. Set `self.activation_function` in `__init__` to sigmoid function.\n2. Implement the forward pass in the `train` method.\n3. Implement the backpropagation algorithm in the `train` method, including calculating the output error.\n4. Implement the forward pass in the `run` method.\n  ","ef4b6fae":"### Splitting the data into training, testing, and validation sets\n\nI'll save the data for the last approximately 21 days to use as a test set after we've trained the network. I'll use this set to make predictions and compare them with the actual number of riders."}}