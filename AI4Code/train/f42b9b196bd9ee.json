{"cell_type":{"6ec7f250":"code","e9d7aa9b":"code","72d8deb4":"code","d6bf5f24":"code","f40a9e14":"code","cf42415d":"code","31e3cb92":"code","168893f6":"code","d89e2c3e":"code","d7c0b040":"code","117f0525":"code","9e0fb73a":"code","9647691e":"code","a5fbe102":"code","63810db0":"code","13ff0944":"code","fef101cf":"code","0fb887a9":"code","ecd2f230":"code","1d0a1208":"code","f2f4ebc0":"code","3d1861d0":"code","63ad1131":"code","3ae860fe":"code","6740a1ff":"code","4fe47494":"code","71c43359":"code","3b568690":"code","7ea27fca":"code","7b8278da":"code","6648331c":"code","9001de20":"code","37f5002b":"code","87386855":"code","5145586e":"code","82a3bded":"code","5d5640f4":"code","72c0c159":"code","75ef03bb":"code","8e6721d5":"code","fc0e4c2c":"code","78a39951":"code","283e7564":"code","04dcafb5":"code","5ea1d56c":"code","a623761d":"code","0c7e4892":"code","9057980a":"code","206b89c0":"code","cb0fd77a":"code","57af1f19":"code","74818b98":"markdown","7f1c2d45":"markdown","ec752b25":"markdown","f44c27c3":"markdown","a5df40f1":"markdown","02a68f34":"markdown","2c6b92a2":"markdown","e24d83a4":"markdown","05265d08":"markdown","405b9e58":"markdown","3f6c8275":"markdown","e43a2edf":"markdown","8488c05a":"markdown","59253827":"markdown","89f9fdd2":"markdown","01bec5f8":"markdown","06737aea":"markdown","5e957f5f":"markdown","b2b55661":"markdown"},"source":{"6ec7f250":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport time\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nimport imageio\nfrom IPython.display import Image\nimport matplotlib.image as mpimg\nfrom skimage.transform import resize\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport zipfile\nfrom io import BytesIO\nfrom nibabel import FileHolder\nfrom nibabel.analyze import AnalyzeImage\nimport PIL\nfrom IPython import display\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\nLSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose, LeakyReLU,\\\nConvLSTM2D,Conv3D\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.datasets import mnist\nimport keras\n#SKLEARN CLASSIFIER\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","e9d7aa9b":"Sun_Yellow_Red = \"..\/input\/sun-video-for-deep-learning-process\/Sun Videos\/k42f2p.mp4\"\nSun_Blue = \"..\/input\/sun-video-for-deep-learning-process\/Sun Videos\/s0sdnw.mp4\"","72d8deb4":"Yellow_Red_List = []\n\nYellow_Red_Video = cv2.VideoCapture(Sun_Yellow_Red)\n\nwhile Yellow_Red_Video.isOpened():\n    \n    ret,frame = Yellow_Red_Video.read()\n    \n    if ret != True:\n        break\n        \n    if Yellow_Red_Video.isOpened():\n        Transformation_IMG = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        Resize_IMG = cv2.resize(Transformation_IMG,(180,180))\n        Yellow_Red_List.append(Resize_IMG)\n        \n        \nYellow_Red_Video.release()","d6bf5f24":"print(np.shape(np.asarray(Yellow_Red_List)))","f40a9e14":"Blue_List = []\n\nBlue_Video = cv2.VideoCapture(Sun_Blue)\n\nwhile Blue_Video.isOpened():\n    \n    ret,frame = Blue_Video.read()\n    \n    if ret != True:\n        break\n        \n    if Blue_Video.isOpened():\n        Transformation_IMG = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        Resize_IMG = cv2.resize(Transformation_IMG,(180,180))\n        Blue_List.append(Resize_IMG)\n        \nBlue_Video.release()","cf42415d":"print(np.shape(np.asarray(Blue_List)))","31e3cb92":"Main_IMG_List = []\n\nfor IMG in Blue_List:\n    Main_IMG_List.append(IMG)\n    \nfor IMG in Yellow_Red_List:\n    Main_IMG_List.append(IMG)","168893f6":"print(np.shape(np.asarray(Main_IMG_List)))","d89e2c3e":"Main_IMG_List = np.asarray(Main_IMG_List)\nYellow_Red_List = np.asarray(Yellow_Red_List)\nBlue_List = np.asarray(Blue_List)","d7c0b040":"print(Main_IMG_List.shape)\nprint(Yellow_Red_List.shape)\nprint(Blue_List.shape)","117f0525":"figure = plt.figure(figsize=(10,10))\n\nPicking_IMG = Main_IMG_List[4]\nplt.xlabel(Picking_IMG.shape)\nplt.ylabel(Picking_IMG.size)\nplt.imshow(Picking_IMG)","9e0fb73a":"figure = plt.figure(figsize=(10,10))\n\nPicking_IMG = Main_IMG_List[98]\nplt.xlabel(Picking_IMG.shape)\nplt.ylabel(Picking_IMG.size)\nplt.imshow(Picking_IMG)","9647691e":"figure = plt.figure(figsize=(10,10))\n\nPicking_IMG = Main_IMG_List[980]\nplt.xlabel(Picking_IMG.shape)\nplt.ylabel(Picking_IMG.size)\nplt.imshow(Picking_IMG)","a5fbe102":"figure = plt.figure(figsize=(10,10))\n\nPicking_IMG = Main_IMG_List[4075]\nplt.xlabel(Picking_IMG.shape)\nplt.ylabel(Picking_IMG.size)\nplt.imshow(Picking_IMG)","63810db0":"figure = plt.figure(figsize=(10,10))\n\nPicking_IMG = Main_IMG_List[3475]\nplt.xlabel(Picking_IMG.shape)\nplt.ylabel(Picking_IMG.size)\nplt.imshow(Picking_IMG)","13ff0944":"figure = plt.figure(figsize=(10,10))\n\nPicking_IMG = Main_IMG_List[3075]\nplt.xlabel(Picking_IMG.shape)\nplt.ylabel(Picking_IMG.size)\nplt.imshow(Picking_IMG)","fef101cf":"figure,axis = plt.subplots(5,5,figsize=(12,12))\n\nfor indexing,operation in enumerate(axis.flat):\n    \n    IMG_Pick = Main_IMG_List[indexing]\n    operation.set_xlabel(IMG_Pick.shape)\n    \n    operation.imshow(IMG_Pick)\n\nplt.tight_layout()\nplt.show()","0fb887a9":"iterations = 60\nvector_noise_shape = 180\ncount_example = 16\nbatch_size = 12\ncount_buffer = 60000","ecd2f230":"X_Train_YellowRed = (Yellow_Red_List - 127.5) \/ 127.5","1d0a1208":"print(X_Train_YellowRed.shape)","f2f4ebc0":"Train_Data_YellowRed = tf.data.Dataset.from_tensor_slices(X_Train_YellowRed).shuffle(count_buffer).batch(batch_size)","3d1861d0":"print(Train_Data_YellowRed)","63ad1131":"def Generator_Model():\n    \n    \n    Model = Sequential()\n    #\n    Model.add(Dense(90*90*128,use_bias=False,input_shape=(180,)))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Reshape((90,90,128)))\n    #\n    Model.add(Conv2DTranspose(128,(3,3),padding=\"same\",use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    \n    Model.add(Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Conv2DTranspose(3,(3,3),padding=\"same\",use_bias=False,activation=\"tanh\"))\n    \n    \n    return Model","3ae860fe":"Generator = Generator_Model()","6740a1ff":"print(Generator.summary())","4fe47494":"def Discriminator_Model():\n    \n    model = Sequential()\n    \n    model.add(Conv2D(64,(3,3),padding=\"same\",input_shape=[180,180,3]))\n    model.add(Dropout(0.3))\n    model.add(LeakyReLU())\n    \n    \n    model.add(Conv2D(128,(3,3),padding=\"same\"))\n    model.add(Dropout(0.3))\n    model.add(LeakyReLU())\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    \n    return model","71c43359":"Discriminator = Discriminator_Model()","3b568690":"print(Discriminator.summary())","7ea27fca":"Loss_Function = tf.keras.losses.BinaryCrossentropy(from_logits=True)","7b8278da":"Generator_Optimizer = tf.keras.optimizers.RMSprop(lr=0.0004,clipvalue=1.0,decay=1e-8)\nDiscriminator_Optimizer = tf.keras.optimizers.RMSprop(lr=0.0004,clipvalue=1.0,decay=1e-8)","6648331c":"seed = tf.random.normal([count_example,vector_noise_shape])","9001de20":"def Discriminator_Loss(real_out,fake_out):\n    \n    real_loss = Loss_Function(tf.ones_like(real_out),real_out)\n    fake_loss = Loss_Function(tf.zeros_like(fake_out),fake_out)\n    total_loss = real_loss + fake_loss\n    \n    return total_loss","37f5002b":"def Generator_Loss(fake_out):\n    \n    return Loss_Function(tf.ones_like(fake_out),fake_out)","87386855":"def display_and_save_images(model, epoch, test_input):\n    \n    predictions = model(test_input, training=False)\n    fig = plt.figure(figsize=(12, 12))\n    \n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i+1)\n        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5)\n        plt.axis('off')\n\n    plt.savefig('output_image{:04d}.png'.format(epoch))\n    plt.show()","5145586e":"def Train_Step(images):\n    \n    random_noise_vector = tf.random.normal([batch_size,vector_noise_shape])\n    \n    with tf.GradientTape() as Generator_Tape, tf.GradientTape() as Discriminator_Tape:\n        Generator_Fake_Images = Generator(random_noise_vector,training=False)\n        \n        real_out = Discriminator(images,training=True)\n        fake_out = Discriminator(Generator_Fake_Images,training=True)\n        \n        Generator_Loss_Out = Generator_Loss(fake_out)\n        Discriminator_Loss_Out = Discriminator_Loss(real_out,fake_out)\n        \n    Generator_Gradients = Generator_Tape.gradient(Generator_Loss_Out,Generator.trainable_variables)\n    Discriminator_Gradients = Discriminator_Tape.gradient(Discriminator_Loss_Out,Discriminator.trainable_variables)\n    \n    Generator_Optimizer.apply_gradients(zip(Generator_Gradients,Generator.trainable_variables))\n    Discriminator_Optimizer.apply_gradients(zip(Discriminator_Gradients,Discriminator.trainable_variables))","82a3bded":"def Train_Function(dataset,iterations):\n    \n    for epoch in range(iterations):\n        \n        start = time.time()\n        \n        for image_batch in dataset:\n            Train_Step(image_batch)\n            \n        display.clear_output(wait=True)\n        display_and_save_images(Generator,epoch+1,seed)\n    \n    display.clear_output(wait=True)\n    display_and_save_images(Generator,epoch,seed)","5d5640f4":"Train_Function(Train_Data_YellowRed, iterations)","72c0c159":"Predict_Random_Noise = tf.random.normal(shape=[30,vector_noise_shape])","75ef03bb":"Generator_Prediction = Generator(Predict_Random_Noise)","8e6721d5":"figure,axis = plt.subplots(3,3,figsize=(12,12))\n\nfor i,ax in enumerate(axis.flat):\n    Image_Picking = Generator_Prediction[i]\n    ax.imshow(Image_Picking,cmap=\"gray\")\n    ax.set_xlabel(Image_Picking.shape)\n    \nplt.tight_layout()\nplt.show()","fc0e4c2c":"figure = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(Generator_Prediction[25])\nplt.show()","78a39951":"figure = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(Generator_Prediction[5])\nplt.show()","283e7564":"figure = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(Generator_Prediction[2])\nplt.show()","04dcafb5":"figure = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(Generator_Prediction[29])\nplt.show()","5ea1d56c":"figure = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(Generator_Prediction[1])\nplt.show()","a623761d":"Generator.save(\"Generator.h5\")\nDiscriminator.save(\"Discriminator.h5\")","0c7e4892":"Future_Model = keras.Sequential(\n    [\n        keras.Input(\n            shape=(None, 40, 40, 1)\n        ),  # Variable-length sequence of 40x40x1 frames\n        layers.ConvLSTM2D(\n            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n        ),\n        layers.BatchNormalization(),\n        layers.ConvLSTM2D(\n            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n        ),\n        layers.BatchNormalization(),\n        layers.ConvLSTM2D(\n            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n        ),\n        layers.BatchNormalization(),\n        layers.ConvLSTM2D(\n            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n        ),\n        layers.BatchNormalization(),\n        layers.Conv3D(\n            filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n        ),\n    ]\n)","9057980a":"Future_Model.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\")","206b89c0":"def generate_movies(n_samples=1200, n_frames=15):\n    row = 80\n    col = 80\n    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n\n    for i in range(n_samples):\n        # Add 3 to 7 moving squares\n        n = np.random.randint(3, 8)\n\n        for j in range(n):\n            # Initial position\n            xstart = np.random.randint(20, 60)\n            ystart = np.random.randint(20, 60)\n            # Direction of motion\n            directionx = np.random.randint(0, 3) - 1\n            directiony = np.random.randint(0, 3) - 1\n\n            # Size of the square\n            w = np.random.randint(2, 4)\n\n            for t in range(n_frames):\n                x_shift = xstart + directionx * t\n                y_shift = ystart + directiony * t\n                noisy_movies[\n                    i, t, x_shift - w : x_shift + w, y_shift - w : y_shift + w, 0\n                ] += 1\n\n                # Make it more robust by adding noise.\n                # The idea is that if during inference,\n                # the value of the pixel is not exactly one,\n                # we need to train the model to be robust and still\n                # consider it as a pixel belonging to a square.\n                if np.random.randint(0, 2):\n                    noise_f = (-1) ** np.random.randint(0, 2)\n                    noisy_movies[\n                        i,\n                        t,\n                        x_shift - w - 1 : x_shift + w + 1,\n                        y_shift - w - 1 : y_shift + w + 1,\n                        0,\n                    ] += (noise_f * 0.1)\n\n                # Shift the ground truth by 1\n                x_shift = xstart + directionx * (t + 1)\n                y_shift = ystart + directiony * (t + 1)\n                shifted_movies[\n                    i, t, x_shift - w : x_shift + w, y_shift - w : y_shift + w, 0\n                ] += 1\n\n    # Cut to a 40x40 window\n    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n    noisy_movies[noisy_movies >= 1] = 1\n    shifted_movies[shifted_movies >= 1] = 1\n    return noisy_movies, shifted_movies","cb0fd77a":"epochs = 1\nnoisy_movies, shifted_movies = generate_movies(n_samples=1200)\n\n\nFuture_Model.fit(\n    noisy_movies[:1000],\n    shifted_movies[:1000],\n    batch_size=10,\n    epochs=epochs,\n    verbose=2,\n    validation_split=0.1,\n)","57af1f19":"movie_index = 1004\ntrack = noisy_movies[movie_index][:7, ::, ::, ::]\n\nfor j in range(16):\n    new_pos = Future_Model.predict(track[np.newaxis, ::, ::, ::, ::])\n    new = new_pos[::, -1, ::, ::, ::]\n    track = np.concatenate((track, new), axis=0)\n\n\n# And then compare the predictions\n# to the ground truth\ntrack2 = noisy_movies[movie_index][::, ::, ::, ::]\nfor i in range(15):\n    fig = plt.figure(figsize=(10, 5))\n\n    ax = fig.add_subplot(121)\n\n    if i >= 7:\n        ax.text(1, 3, \"Predictions\", fontsize=20, color=\"w\")\n    else:\n        ax.text(1, 3, \"Initial Trajectory\", fontsize=20)\n\n    toplot = track[i, ::, ::, 0]\n\n    plt.imshow(toplot)\n    ax = fig.add_subplot(122)\n    plt.text(1, 3, \"Ground Truth\", fontsize=20)\n\n    toplot = track2[i, ::, ::, 0]\n    if i >= 2:\n        toplot = shifted_movies[movie_index][i - 1, ::, ::, 0]\n\n    plt.imshow(toplot)\n    plt.savefig(\"%i_animate.png\" % (i + 1))","74818b98":"#### TRANSFORMATION YELLOW RED VIDEO","7f1c2d45":"#### GENERATOR","ec752b25":"#### TO MERGE","f44c27c3":"#### PATH","a5df40f1":"#### SAVING MODEL","02a68f34":"# DATA ENGINEERING BEFORE DC-GAN TRAINING","2c6b92a2":"# DCGAN PROCESS","e24d83a4":"#### DISCRIMINATOR","05265d08":"#### TRAINING","405b9e58":"#### TRANSFORMATION BLUE VIDEO","3f6c8275":"* You can also try other color spectrums if you want. I chose this as the general color spectrum is between red and yellow. This will give the best forecast for the future.","e43a2edf":"#### TO ARRAY","8488c05a":"#### TRAINING PARAMETERS","59253827":"# VISION","89f9fdd2":"# PACKAGES AND LIBRARIES","01bec5f8":"#### PREDICT","06737aea":"#### DISPLAYING PARAMETERS","5e957f5f":"# ADDITIONAL","b2b55661":"# DATA EXPORT AND TRANSFORMATION"}}