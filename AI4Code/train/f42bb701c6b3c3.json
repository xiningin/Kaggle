{"cell_type":{"a75daf46":"code","124202a3":"code","5f81c671":"code","267d9feb":"code","8b061a57":"code","0e1714b0":"code","0ad807df":"code","7721c611":"code","83d6f5d8":"code","19c3bae8":"code","505698d9":"code","8283f0f2":"code","ae981b1d":"code","bac247fc":"code","7af9340d":"markdown","8a9f3e14":"markdown","0301b920":"markdown","d617531b":"markdown","0b53f406":"markdown","23f85320":"markdown","e88520c6":"markdown","a9e46c23":"markdown"},"source":{"a75daf46":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","124202a3":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubm = pd.read_csv('..\/input\/sample_submission.csv')","5f81c671":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","267d9feb":"from sklearn.model_selection import GridSearchCV, train_test_split\n\ntrain_df, val_df = train_test_split(train, test_size=0.07, random_state=2018)","8b061a57":"from sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy import sparse\nclass NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1, solver='sag'):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n        self.solver = solver\n        \n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        # Check that X and y have correct shape\n        y = y.values\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) \/ ((y==y_i).sum()+1)\n\n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) \/ pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs, solver=self.solver).fit(x_nb, y)\n        return self","0e1714b0":"from sklearn.metrics import f1_score\n\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in [i * 0.01 for i in range(100)]:\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result","0ad807df":"train.head()","7721c611":"N = 50000\n\nvec = TfidfVectorizer(ngram_range=(1,3), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1, max_features=N)\n\ntrn_term_doc = vec.fit_transform(train_df['question_text'])\nval_term_doc = vec.transform(val_df['question_text'])\ntest_term_doc = vec.transform(test['question_text'])","83d6f5d8":"model = NbSvmClassifier(dual=True, solver='liblinear', C = 1e1)","19c3bae8":"model.fit(trn_term_doc, train_df['target'])","505698d9":"preds_val = model.predict_proba(val_term_doc)[:,1]\npreds_test = model.predict_proba(test_term_doc)[:,1]","8283f0f2":"best_threshold = threshold_search(y_true=val_df['target'], y_proba=preds_val)","ae981b1d":"best_threshold","bac247fc":"pred_test_y = (preds_test > best_threshold['threshold']).astype(int)\ntest_df = pd.read_csv(\"..\/input\/test.csv\", usecols=[\"qid\"])\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","7af9340d":"Naive Bayes SVM Classifier","8a9f3e14":"## Introduction\n\nThis is a replica of Jeremy Howard's kernel: https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline\nThe point of this kernel is to have a simple baseline with a linear model.\n\nThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a baseline for the Quora Insencere Quesrtions competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classi\ufb01cation](https:\/\/nlp.stanford.edu\/pubs\/sidaw12_simple_sentiment.pdf). In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).\n\nIf you're not familiar with naive bayes and bag of words matrices, there's a preview available of one of fast.ai's upcoming *Practical Machine Learning* course videos, which introduces this topic. Here is a link to the section of the video which discusses this: [Naive Bayes video](https:\/\/youtu.be\/37sFIak42Sc?t=3745).","0301b920":"This is to find optimal threshold","d617531b":"Use validation set to find optimal threshold","0b53f406":"Split in train and test","23f85320":"We use trigrams up to 50000 words","e88520c6":"F1 score results","a9e46c23":"## Building the model\n\nWe'll start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper."}}