{"cell_type":{"bd69c9a5":"code","efd635f4":"code","5b4eac77":"code","e70d8ed8":"code","a8424b97":"code","0f82264f":"code","7a0cf6ff":"code","851d1d0c":"code","19bef1dd":"code","504f3807":"code","7864f0c6":"code","fdd09343":"code","ba7c0a69":"code","98a6a9bd":"code","71d9f4f5":"code","0671ecac":"code","9c31a1cf":"code","962bfda9":"code","0afe8ba5":"code","fa64b7ea":"code","711d5f3b":"code","5ed7f9e3":"code","d150a985":"code","dea85db8":"markdown","92790cf8":"markdown","6555d56c":"markdown","ce8d3a7b":"markdown","00099777":"markdown","b6f7d196":"markdown","074998a8":"markdown","560606b8":"markdown","ce9e7376":"markdown","23b25ed3":"markdown","de3613ec":"markdown","165c4b00":"markdown","4428192a":"markdown","2b56a346":"markdown","83379d9c":"markdown","bbd0fd78":"markdown","6c69d885":"markdown","e8f8ed55":"markdown","967b1bf8":"markdown","ddeb91fe":"markdown","834a1286":"markdown","f40d2d19":"markdown","8ceae3c1":"markdown","9e72c5a0":"markdown","2df779a2":"markdown","8abf9baa":"markdown"},"source":{"bd69c9a5":"import tensorflow as tf \nimport numpy as np \nimport matplotlib.pyplot as plt \ntf.__version__","efd635f4":"img_generator = tf.keras.preprocessing.image.ImageDataGenerator(#rotation_range=90,\n                                                                brightness_range=(0.5,1), \n                                                                #shear_range=0.2, \n                                                                #zoom_range=0.2,\n                                                                channel_shift_range=0.2,\n                                                                horizontal_flip=True,\n                                                                vertical_flip=True,\n                                                                rescale=1.\/255,\n                                                                validation_split=0.3)","5b4eac77":"root_dir = '\/kaggle\/input\/caltech101\/101_ObjectCategories'\nimg_generator_flow_train = img_generator.flow_from_directory(\n    directory=root_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    shuffle=True,\n    subset=\"training\")\n\nimg_generator_flow_valid = img_generator.flow_from_directory(\n    directory=root_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    shuffle=True,\n    subset=\"validation\")","e70d8ed8":"imgs, labels = next(iter(img_generator_flow_train))\nfor img, label in zip(imgs, labels):\n    plt.imshow(img)\n    plt.show()","a8424b97":"base_model = tf.keras.applications.InceptionV3(input_shape=(224,224,3),\n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )","0f82264f":"base_model.trainable = False","7a0cf6ff":"model = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(102, activation=\"softmax\")\n])","851d1d0c":"model.summary()","19bef1dd":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n              loss = tf.keras.losses.CategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.CategoricalAccuracy()])","504f3807":"model.fit(img_generator_flow_train, validation_data=img_generator_flow_valid, steps_per_epoch=20, epochs=50) #20,50","7864f0c6":"# Visualise train \/ Valid Accuracy\nplt.plot(model.history.history[\"categorical_accuracy\"], c=\"r\", label=\"train_accuracy\")\nplt.plot(model.history.history[\"val_categorical_accuracy\"], c=\"b\", label=\"test_accuracy\")\nplt.legend(loc=\"upper left\")\nplt.show()","fdd09343":"# Visualise train \/ Valid Loss\nplt.plot(model.history.history[\"loss\"], c=\"r\", label=\"train_loss\")\nplt.plot(model.history.history[\"val_loss\"], c=\"b\", label=\"test_loss\")\nplt.legend(loc=\"upper left\")\nplt.show()","ba7c0a69":"imgs, labels = next(iter(img_generator_flow_valid))","98a6a9bd":"for layer in model.layers:\n    print(layer.name)","71d9f4f5":"base_model = model.layers[0]","0671ecac":"tf.keras.utils.plot_model(base_model, show_shapes=True, show_layer_names=True)","9c31a1cf":"for layer in base_model.layers:\n  print(layer.name)","962bfda9":"last_conv_layer_name = \"mixed10\"\nclassifier_layer_names = [layer.name for layer in model.layers][1:]","0afe8ba5":"# We start by setting up the dependencies we will use\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Display\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","fa64b7ea":"# The Grad-CAM algorithm\ndef get_img_array(img_path, size):\n    # `img` is a PIL image of size 299x299\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (299, 299, 3)\n    array = keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    array = np.expand_dims(array, axis=0)\n    return array\n\n\ndef make_gradcam_heatmap(\n    img_array, base_model, model, last_conv_layer_name, classifier_layer_names):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = base_model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(base_model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n    return heatmap","711d5f3b":"# Print what the top predicted class is\npreds = model.predict(imgs)\npred_labels = tf.argmax(preds, axis = -1)\n\nprint(\"Prediction output:\", preds)\n\nprint(\"Predicted label:\", pred_labels)","5ed7f9e3":"# Generate class activation heatmap\nheatmaps = []\n\nfor img in imgs:\n  heatmap = make_gradcam_heatmap(\n    tf.expand_dims(img,axis=0),base_model, model, last_conv_layer_name, classifier_layer_names\n  )\n  heatmaps.append(heatmap)\n\n\n# Display heatmap\nplt.matshow(heatmaps[0])\nplt.show()\n","d150a985":"from pathlib import Path\n\nfor img, pred_label, true_label, heatmap in zip(imgs, pred_labels, labels, heatmaps): \n    # We rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # We use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # We use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # We create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * 0.003 + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    save_path = \"saved_img.jpg\"\n    superimposed_img.save(save_path)\n\n    # Display Grad CAM\n    pred_file_path = np.argmax(img_generator_flow_valid.labels == pred_label)\n    pred_label_name = Path(img_generator_flow_valid.filepaths[pred_file_path]).parent.name\n\n    true_file_path = np.argmax(img_generator_flow_valid.labels == tf.argmax(true_label))\n    true_label_name = Path(img_generator_flow_valid.filepaths[true_file_path]).parent.name\n\n    print(\"Predicted label:\",pred_label_name)\n    print(\"True label:\", true_label_name)\n\n    display(Image(save_path))","dea85db8":"4. Compile our model with [`compile`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model#compile) using Adam with a learning rate of `0.001` and the appropriate loss and metrics. ","92790cf8":"## Conclusion  <a class=\"anchor\" id=\"chapter4\"><\/a>\n\nIn this notebook we explained what transfer learning is, and implemented it in an image classification project. \n\nIn this case, the InceptionV3 model applied as a base model to the Caltech101 dataset allows to have a very good accuracy on the classification using little computing power! \n\nTransfer learning is often used for image classification. **This notebook is a perfect example!**","6555d56c":"## Part 2 : Transfer Learning  <a class=\"anchor\" id=\"chapter2\"><\/a>\n\n1. Import a pretrained model from tensorflow, for example <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/InceptionV3\"> InceptionV3 <\/a>.","ce8d3a7b":"6. Loop over each image, predicted label and heatmap in order to display the images with the superimposed grad cam heatmap and the corresponding predicted label. ","00099777":"Examples of images in this dataset :\n![images.PNG](attachment:507227a5-dd1e-4fce-8c5b-c602e3504356.PNG)\n","b6f7d196":"# \u23ed\ufe0f Transfer Learning on images classification with Tensorflow","074998a8":"5. For all the images in the validation batch use the `make_gradcam_heatmap` to create a grad cam heatmap and store them all in a list. Then display the first heatmap.","560606b8":"5. Train the model and monitor the overfitting using the validation data.","ce9e7376":"## Part 1 : Preprocessing with ImageDataGenerator  <a class=\"anchor\" id=\"chapter1\"><\/a>\n\nThe easiest way to preprocess our data is using an `ImageDataGenerator`, that is a very straight forward way to load and preprocess image data for training deep learning models.\n\n1. Instanciate an <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\"> ImageDataGenerator <\/a> with the parameters of our choice.","23b25ed3":"6. Visualize our `accuracy` on the `train` and `test` set thanks to the attribute `history` of your model","de3613ec":"## What is Transfer Learning ?","165c4b00":"![transfer learning.jpg](attachment:4e69dd9c-6dcf-49b6-bb58-38cf9ffc16a1.jpg)\n\nImage of AI smart (https:\/\/www.aismartz.com\/blog\/an-introduction-to-transfer-learning\/)","4428192a":"## \ud83d\udca1 Projet Description ","2b56a346":"2. Get the name of the last convolution layer of the pretrained model and the predicition layers and store them in two variables called respectively `last_conv_layer_name`, and `classifier_layer_names`. ","83379d9c":"## Part 3 : Interpretation with Grad Cam  <a class=\"anchor\" id=\"chapter3\"><\/a>\n\n1. Create an object `imgs` and an object `labels` containing a batch of validation images and validation labels.","bbd0fd78":"7. Do the same for the `loss`","6c69d885":"## Table of Contents\n\n* [Part 1 : Preprocessing with ImageDataGenerator](#chapter1)\n    \n* [Part 2 : Transfer Learning](#chapter2)\n\n* [Part 3 : Interpretation with Grad Cam](#chapter3)\n\n* [Conclusion](#chapter4)","e8f8ed55":"3. Create now our complete model by adding the last layer adapted to our situation ","967b1bf8":"1. Import the classic librairies \n  * Tensorflow 2.0\n  * Numpy \n  * Matplotlib","ddeb91fe":"2. Prepare two objects `img_generator_flow_train` and `img_generator_flow_valid` thanks to the method `flow_from_directory`.","834a1286":"4. Create a `preds` object containing the predicitons from the model on the batch of validation images. Then create a `pred_labels` object containing the top class predicted for each image in that batch.","f40d2d19":"2. Set the weights of our imported model as non-trainable ","8ceae3c1":"The objective of this notebook is to train a model capable of classifying images according to **101 categories**. \n\nUsually the training of such a model is complicated and requires a lot of computing power. However, we will show here that it is possible to reuse already trained models that we will fine tune to fit our specific case. \n\nThis is what we call **Transfer Learning**.","9e72c5a0":"Transfer learning is a deep learning technique that makes it possible to re-use general models that have already been trained (to recognize photos, for example) in order to accelerate the development of models that meet specific needs.\n\nTransfer learning has been widely developed because it allows to reduce the volume of data needed to train models, but also to reduce computing time. It allows to save time and money (renting of the computing power).\n\nFor example to recognize crocodiles we can use transfer leanring base on alligators detection because they have a lot of features in common.","2df779a2":"3. Visualize a batch of images coming from the train set. ","8abf9baa":"## Dataset : CALTECH101\n\nCaltech-101 (http:\/\/www.vision.caltech.edu\/Image_Datasets\/Caltech101\/) consists of images of objects belonging to 101 classes, plus a background clutter class. \n\nEach image is labeled with a single object. \n\nEach class contains about 40-800 images, for a total of about 9,000 images. \n\nThe images vary in size, with typical edge lengths of 200 to 300 pixels. "}}