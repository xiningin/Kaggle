{"cell_type":{"9a2c57a4":"code","e3ba1b9d":"code","b63fd102":"code","7f502c0b":"code","68c9421d":"code","91935922":"code","d26fc071":"code","fdd255e0":"code","817bff1c":"code","88de896c":"code","77c7e306":"code","7ca74a63":"code","d3e09e97":"markdown","e4d6ba1e":"markdown","00cbf062":"markdown","4237babd":"markdown"},"source":{"9a2c57a4":"# This tutorial draws from several families of libraries:\n# the standard libraries os, json, and gc\n# numpy, matplotlib and pandas from the scipy ecosystem\n# the NLP libraries gensim and nltk\n# and scikit-learn (sklearn)\n\nimport gc      # ipython requires us to force garbage collection\nimport gensim\nfrom gensim import corpora, models\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nimport json as js\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.stem import WordNetLemmatizer,SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np # linear algebra\nfrom operator import itemgetter\nimport os\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import davies_bouldin_score,calinski_harabasz_score,silhouette_score\nfrom sklearn.svm import SVC","e3ba1b9d":"# the number of words in the vocabulary\nvocab_size = 1000\n\n# the dataset has gotten bigger; this ensures that everything fits into memory\n# set to -1 to load entire set.\ndata_set_size = 16000 \n\n# The following two functions are used to clean and normalize the text of the\n# papers\nstemmer = SnowballStemmer('english')\ndef lemmatize_stemming(text):\n    return stemmer.stem(text)\n    #return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess_stem_clean(text):\n    result = []\n    for token in simple_preprocess(text):\n        if token not in STOPWORDS:\n            result.append(lemmatize_stemming(token))\n    return result","b63fd102":"# Read and preprocess the data sets, creating an array where each row consists of ALL text from one \n# article\ni = 0\nk = 0\n\ncorona_all_text = []\nY=[]\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        i += 1\n        if data_set_size != -1 and k > data_set_size:\n            break\n        if i % 500 == 0:\n            print (\"Working (number %d)...\" % i)\n        \n        if filename.split(\".\")[-1] == \"json\":\n            \n            f = open(os.path.join(dirname, filename))\n            j = js.load(f)\n            f.close()\n            \n            try:\n                abstract_text = ' '.join([x['text'] for x in j['abstract']])\n            except:\n                abstract_text = \"\"\n            body_text = ' '.join(x['text'] for x in j['body_text'])\n            body_text += \" \" + abstract_text\n            if \"corona\" in body_text.lower() or \"covid\" in body_text.lower():\n                Y.append(1)\n            else:\n                Y.append(0)\n            k+=1\n            corona_all_text.append(preprocess_stem_clean(body_text))\n            \nprint (k)\nprint (i)","7f502c0b":"# create bag-of-words feature vector\ndictionary = corpora.Dictionary(corona_all_text)\ndictionary.filter_extremes(no_below=5, no_above=0.8, keep_n=vocab_size, keep_tokens=['corona', 'covid'])\n\n# returns a sparse bag-of-words vector (each dimension represents one word)\n# value is the number of times word appears in texts\ncorpus = [dictionary.doc2bow(text) for text in corona_all_text]\n\n# Space is tight, so force garbage collection on the input data\ncorona_all_text = [] \n\n# returns a matrix where the rows are the features and columnts are data items\ncorpus_matrix = gensim.matutils.corpus2dense(corpus, len(dictionary))\n\n# center the features on zero\ncorpus_mean = np.mean(corpus_matrix, axis=0)\ncorpus_matrix = corpus_matrix - corpus_mean","68c9421d":"# Perform PCA by computing the singular value decomposition of the \n# mean-adjusted features (i.e., bag-of-word wordcounts)\n#\n# u are the eigenvectors of the covariance matrix of the features \n#   (where each column is one eigenvector)\n#\n# s are the square roots of the eigenvectors\nu,s,w = np.linalg.svd(corpus_matrix)\n\n# Plot the eigenvalues. The convergence point represents a good place\n# to cut off the dimensions. \nplt.plot(s*s)\nplt.show()\n\n# inspect the top eigenvalue (dimension of highest variance)\ntop = u[:,0] \ntop_words = [(dictionary[i], top[i]) for i in range(vocab_size)]\ntop_words.sort(key=itemgetter(1))\n\ncorona_id = dictionary.token2id['corona']\n\n# note: sign here is arbitrary. Any words w\/ same valence as corona are important\nbest_ev_pos = np.argmax(u[corona_id,:])\npos_words = [(dictionary[i], u[i,best_ev_pos]) for i in range(vocab_size)]\npos_words.sort(key=itemgetter(1))\nprint (pos_words[-20:])\n\n# NEVER do this in your own code (repeat a pattern without making it a subroutine ;)\nbest_ev_neg = np.argmin(u[corona_id,:])\nneg_words = [(dictionary[i], u[i,best_ev_neg]) for i in range(vocab_size)]\nneg_words.sort(key=itemgetter(1))\nprint (neg_words[:20])","91935922":"# Run k-means\nkmeans = KMeans(n_clusters=2).fit(corpus_matrix.T)","d26fc071":"# Project into first 50 PCA dimensions\ncorpus_reduced = corpus_matrix.T.dot(u[:,:50])\n\n# Now lets run a few tests to try to estimate the best number of clusters\ndb_score = []\nch_score = []\ns_score = []\nmodels = {}\nlb = 4\nub = 20\nfor k in range(lb,ub):\n    kmeans = KMeans(n_clusters=k).fit(corpus_reduced)\n    db_score.append(davies_bouldin_score(corpus_reduced, kmeans.labels_)) # lower is better\n    ch_score.append(calinski_harabasz_score(corpus_reduced, kmeans.labels_)) # higher is better\n    s_score.append(silhouette_score(corpus_reduced, kmeans.labels_)) # higher is better \n    models[k] = kmeans\n    \nplt.clf()\nplt.plot(range(lb,ub), db_score, color=\"red\")\nplt.plot(range(lb,ub), s_score, color =\"blue\")\nplt.show()","fdd255e0":"plt.plot(range(lb,ub), ch_score)","817bff1c":"km8 = models[8]\nprint(km8.cluster_centers_.shape)\ncluster_center_bows = km8.cluster_centers_.dot(u[:,:50].T)\nprint (cluster_center_bows.shape)","88de896c":"def uncode_BOW(fv):\n    # zip might work here\n    x = [(dictionary[i], fv[i]) for i in range(vocab_size)]\n    return sorted(x,key=itemgetter(1))\n \n\ncluster_report = [print(uncode_BOW(x)[-20:]) for x in cluster_center_bows]\n#print (cluster_report)","77c7e306":"svm = SVC(kernel='linear').fit(corpus_matrix.T, Y)","7ca74a63":"print (svm.coef_)","d3e09e97":"Now we can join, sort, and rank the words in each centroid.","e4d6ba1e":"Scikit-learn's [documentation on clustering](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html) is **very** informative, including a very readable discussion of the most popular clustering methods, diagnostics and their tradoffs, and some pointers to visualizations. I recommend browsing it. \n\nIn the code below, I first demonstrate how to run clustering on the BOW feature set.","00cbf062":"I have not been able to get the code below to run in this notebook yet; it runs for over an hour and so far I haven't been able to keep my laptop open for much longer. I'm kind of surprised this is taking so long to run, but I'm leaving this up here for the sake of reference.","4237babd":"Diagnostics will return different results every time, and none of them are a replacement for good prior knowledge, but thus far k=8 seems be an interesting, critical value (i.e., where after running several times the diagnostics curves seem to pause, spike, or dip). Let's inspect model k=8. First, we find the cluster centers. Each of these represents an ideal document for that cluster. However, it is represented in eigenspace. To make sense out of it, we need back-project into the (mean-adjusted) BOW space. "}}