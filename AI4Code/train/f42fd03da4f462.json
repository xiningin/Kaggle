{"cell_type":{"e1c77c1b":"code","464c10df":"code","01355269":"code","14313c05":"code","812c1e6f":"code","16bc8f3e":"code","56889703":"code","ae17d79a":"code","ef9cb6c6":"code","21ca1fd3":"code","586c5977":"code","329b237a":"code","58b17b22":"code","34ddf624":"code","dbd082cd":"code","b86e0269":"code","b8b49d57":"code","6591f86a":"code","f1c9b5dd":"code","e04f9dc9":"code","4c0cbe12":"code","b9922a42":"code","62f82aa2":"code","223e3681":"code","b42c0ffa":"code","39adf592":"code","6fcf0026":"code","01214cc9":"code","6346eb37":"code","8bdf0ddd":"code","b2029d34":"code","93ffba7f":"markdown","fc6ae076":"markdown","0ffa0796":"markdown","93cb7676":"markdown","dfaddeb4":"markdown","4ff37d3b":"markdown","c81c0a7c":"markdown","68c40dfd":"markdown","8703e9fc":"markdown","99b6fa4a":"markdown","64b507a9":"markdown","08197631":"markdown","382e1c16":"markdown","eeec0d51":"markdown","1d47c70c":"markdown","c8d23d9e":"markdown","ccb0087b":"markdown","4c136638":"markdown"},"source":{"e1c77c1b":"# for basic mathematics operation \nimport numpy as np\nimport pandas as pd\nfrom pandas import plotting\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# for interactive visualizations\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nimport plotly.figure_factory as ff\n\n# for path\nimport os\nprint(os.listdir('..\/input\/'))","464c10df":"import numpy as np\nimport pandas as pd\nfrom pandas import plotting\n\n# importing the dataset\ndata = pd.read_csv('..\/input\/gazprom\/ildusnkr.csv')\n\ndat = ff.create_table(data.head())\n\npy.iplot(dat)","01355269":"# checking if there is any NULL data\n\ndata.isnull().any().any()","14313c05":"data","812c1e6f":"import warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (18, 8)\n\nplt.subplot(1, 2, 1)\nsns.set(style = 'whitegrid')\nsns.distplot(data['\u0427\u0438\u0441\u0442\u0430\u044f \u043f\u0440\u0438\u0431\u044b\u043b\u044c, \u0442\u044b\u0441. \u0440\u0443\u0431'])\nplt.title('\u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0447\u0438\u0441\u0442\u043e\u0439 \u043f\u0440\u0438\u0431\u044b\u043b\u0438', fontsize = 20)\nplt.xlabel('Range')\nplt.ylabel('Count')\n\n\nplt.subplot(1, 2, 2)\nsns.set(style = 'whitegrid')\nsns.distplot(data['\u0412\u044b\u0440\u0443\u0447\u043a\u0430, \u0442\u044b\u0441. \u0440\u0443\u0431.'], color = 'red')\nplt.title('\u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0440\u0443\u0447\u043a\u0438', fontsize = 20)\nplt.xlabel('Range of Age')\nplt.ylabel('Count')\nplt.show()","16bc8f3e":"sns.pairplot(data1)\nplt.title('Pairplot for the Data', fontsize = 20)\nplt.show()","56889703":"plt.rcParams['figure.figsize'] = (15, 8)\nsns.heatmap(data.corr(), cmap = 'Wistia', annot = True)\nplt.title('Heatmap for the Data', fontsize = 20)\nplt.show()","ae17d79a":"x = data.iloc[:, 1:25]##.values\n\n# let's check the shape of x\nprint(x.shape)","ef9cb6c6":"data=data.fillna(0)","21ca1fd3":"import scipy.cluster.hierarchy as sch\n\ndendrogram = sch.dendrogram(sch.linkage(x, method = 'ward'))\nplt.title('Dendrogam', fontsize = 20)\nplt.xlabel('Companies')\nplt.ylabel('Ecuclidean Distance')\nplt.show()","586c5977":"from sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(x)\n\nplt.scatter(x[y_hc == 0, 0], x[y_hc == 0, 1], s = 100, c = 'pink', label = 'miser')\nplt.scatter(x[y_hc == 1, 0], x[y_hc == 1, 1], s = 100, c = 'yellow', label = 'general')\nplt.scatter(x[y_hc == 2, 0], x[y_hc == 2, 1], s = 100, c = 'cyan', label = 'target')\nplt.scatter(x[y_hc == 3, 0], x[y_hc == 3, 1], s = 100, c = 'magenta', label = 'spendthrift')\nplt.scatter(x[y_hc == 4, 0], x[y_hc == 4, 1], s = 100, c = 'orange', label = 'careful')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('Hierarchial Clustering', fontsize = 20)\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","329b237a":"x = data.iloc[:, [2, 4]].values\nx.shape","58b17b22":"from sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\n\nplt.rcParams['figure.figsize'] = (15, 5)\nplt.plot(range(1, 11), wcss)\nplt.title('K-Means Clustering(The Elbow Method)', fontsize = 20)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.grid()\nplt.show()","34ddf624":"cat_cols = ['\u041a\u043e\u043c\u043f\u0430\u043d\u0438\u044f']\ncont_cols =x.columns.tolist()\n##cont_cols.remove(cat_cols[0])\n","dbd082cd":"x = data.iloc[:, 1:23]##.values\n\n# let's check the shape of x\nprint(x.shape)","b86e0269":"cont_cols","b8b49d57":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(x)\nscaled = scaler.transform(x)\nscaled_df = pd.DataFrame(scaled, columns=cont_cols)","6591f86a":"scaled_df","f1c9b5dd":"from sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport math\nfrom scipy import stats\nfrom sklearn.cluster import KMeans\nN_COMPONENTS = 17\npca = PCA(n_components=N_COMPONENTS)\npca.fit(scaled_df)\npca.explained_variance_ratio_[:4].sum()","e04f9dc9":"N_COMPONENTS = 15\npca = PCA(n_components=N_COMPONENTS)\npca.fit(scaled_df)\npca.explained_variance_ratio_[:4].sum()\npca_data = pca.transform(scaled_df)\npca_df = pd.DataFrame(pca_data).iloc[:,:4]\npca_df.columns = list(map(lambda x: f'pca_{x+1}', pca_df.columns))\n# pca_df['TENURE'] = df.TENURE","4c0cbe12":"fig = px.scatter_3d(pca_df,x='pca_1',y='pca_2',z='pca_3',opacity=0.3,color='pca_4')\nfig.show()","b9922a42":"cost = []\nks = []\nfor i in range(3,10):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(pca_df)\n    cost.append(kmeans.inertia_)\n    ks.append(i)\nsns.lineplot(x=np.array(ks), y=np.array(cost))\nplt.xticks(ks)\nplt.show()","62f82aa2":"kmeans = KMeans(n_clusters=5)\nkmeans.fit(pca_df)\nout = kmeans.predict(pca_df)","223e3681":"out = kmeans.predict(pca_df)","b42c0ffa":"pca_df","39adf592":"pca_df['out']=out","6fcf0026":"fig = px.scatter_3d(pca_df,x='pca_1',y='pca_2',z='pca_3',color='out',opacity=0.5,\n                    title='KMeans cluster with k=5')\nfig.show()","01214cc9":"def display_component(v, features_list, component_num,ax):\n    \n    row_idx = component_num\n    \n    v_1_row = v.iloc[:,row_idx]\n    v_1 = np.squeeze(v_1_row.values)\n    \n    comps = pd.DataFrame(list(zip(v_1, features_list)),\n                         columns=['weights', 'features'])\n    \n    comps['abs_weights']=comps['weights'].apply(lambda x: np.abs(x))\n    sorted_weight_data = comps.sort_values('abs_weights',ascending=False).head()\n    \n    sns.barplot(data=sorted_weight_data,\n                   x=\"weights\",\n                   y=\"features\",\n                   palette=\"Blues_d\",ax=ax)\n    ax.set_title(\"PCA Component Makeup, Component #\" + str(component_num), fontsize=20)\n","6346eb37":"features_list = np.array(cont_cols)\nv = pd.DataFrame(pca.components_)","8bdf0ddd":"fig, axes = plt.subplots(2,2,figsize=(20,8),constrained_layout=True)\naxes=axes.flatten()\nfor i,ax in enumerate(axes):\n    display_component(v, features_list, i,ax=ax)\nplt.show()","b2029d34":"pca_df","93ffba7f":">Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other","fc6ae076":"## Data Visualization","0ffa0796":"## Clustering Analysis","93cb7676":"**Installing the Libraries**","dfaddeb4":"## Kmeans Algorithm","4ff37d3b":"> This Clustering Analysis gives us a very clear insight about the different segments of the customers in the Mall. There are clearly Five segments of Customers namely Miser, General, Target, Spendthrift,  Careful based on their Annual Income and Spending Score which are reportedly the best factors\/attributes to determine the segments of a customer in a Mall.","c81c0a7c":"**The Elbow Method to find the No. of Optimal Clusters**","68c40dfd":"**Visualizing the Clusters of Hierarchial Clustering**","8703e9fc":"**K-means Algorithm**","99b6fa4a":"> The Above Graph for Showing the correlation between the different attributes of the Mall Customer Segementation Dataset, This Heat map reflects the most correlated features with Orange Color and least correlated features with yellow color.\n>> We can clearly see that these attributes do not have good correlation among them, that's why we will proceed with all of the features.","64b507a9":"**Visualizaing the Clusters**","08197631":"## Hierarchial Clustering","382e1c16":"**Reading the Dataset**","eeec0d51":">According to my own intuition by looking at the above clustering plot between the age of the customers and their corresponding spending scores, I have aggregated them into 4 different categories namely Usual Customers, Priority Customers, Senior Citizen Target Customers, Young Target Customers. Then after getting the results we can accordingly make different marketing strategies and policies to  optimize the spending scores of the customer in the Mall.","1d47c70c":"**Using Dendrograms to find the no. of Optimal Clusters**","c8d23d9e":"**Mall Customers Clustering Analysis**","ccb0087b":"* It has been shown the Andrews curves are able to preserve means, distance (up to a constant) and variances. Which means that Andrews curves that are represented by functions close together suggest that the corresponding data points will also be close together","4c136638":"**Clusters of Customers Based on their Ages**"}}