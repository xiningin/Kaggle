{"cell_type":{"cc3e4084":"code","b80e0c67":"code","45110520":"code","e540023a":"code","5909b883":"code","cddc8c40":"code","54dbc1fa":"code","93d87014":"code","6505db7c":"code","cce54b35":"code","6fb9cb9d":"markdown"},"source":{"cc3e4084":"%%time\n!cp -r ..\/input\/jigsaw-pytorch-pretrained-bert\/repository\/huggingface-pytorch-pretrained-BERT-3fc63f1\/ .\/\n!pip install .\/huggingface-pytorch-pretrained-BERT-3fc63f1\/.","b80e0c67":"%%time\n# Borrows a lot of code from https:\/\/www.kaggle.com\/bminixhofer\/simple-lstm-pytorch-version\nFOLD = 0\n\nimport os\nimport sys\nimport random\nimport glob\nimport gc\nimport requests\nimport pickle\nimport csv\n\nimport numpy as np\nimport pandas as pd\n\nimport mlcrate as mlc\n\nimport os\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils import data\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.utils.checkpoint as checkpoint\n\nfrom keras.preprocessing import text, sequence\n\n# from apex import amp\n\nimport spacy\nfrom spacy.lang.en import English\n\nimport matplotlib.pyplot as plt\n\nfrom pytorch_pretrained_bert import BertTokenizer, GPT2Tokenizer\n\n# disable progress bars when submitting\ndef is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\nSEED = 4242\n\ndef seed_everything(SEED=SEED):\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef get_n_params(model):\n    pp=0\n    for p in list(model.parameters()):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\n\n# from https:\/\/github.com\/floydhub\/save-and-resume\ndef save_checkpoint(state):\n    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n    print (\" Saving checkpoint\")\n\n    filename = f'.\/checkpoint-{state[\"fold\"]}.pt.tar'\n    torch.save(state, filename)\n\ndef initialize(model, fold):\n    path = f'.\/checkpoint-{fold}.pt.tar'\n    \n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint['model'])\n\n    print(f' Loaded checkpoint {path} | Trained for {checkpoint[\"epoch\"] + 1} epochs')\n    \n    return model","45110520":"WORKERS = 0\n\nSPLITS = 5\nMAX_LEN = 220\nNUM_WORDS = 100000\n\nBATCH_SIZE = 512","e540023a":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')","5909b883":"# train = train.loc[:1000]","cddc8c40":"x_train = train['comment_text'].values","54dbc1fa":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2')","93d87014":"x = []\nfor i in tqdm(range(len(x_train))):\n    x.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x_train[i])[:MAX_LEN]))\n    \nx_train = sequence.pad_sequences(x, maxlen=MAX_LEN, padding='post')\nx_train = torch.tensor(x_train, dtype=torch.int32)","6505db7c":"with open('x_train_gpt.pkl', 'wb') as handle:\n    pickle.dump(x_train, handle, protocol=pickle.HIGHEST_PROTOCOL)","cce54b35":"!rm -rf huggingface-pytorch-pretrained-BERT-3fc63f1","6fb9cb9d":"I found out why this happens.\n\nGPT2 was originally didn't have a dedicated padding token since it was trained on sequences of equal lengths. The maintainers of pytorch pretrained bert have gotten around this by letting you set special tokens with their own vocab indices.\n\nThis should fix the problem:\n\n    # Add the <pad> token to the vocabulary\n    SPECIAL_TOKENS = [\"<pad>\"]\n    tokenizer.set_special_tokens(SPECIAL_TOKENS)\n\n    # Set the number of special tokens in the model\n    model.set_num_special_tokens(len(SPECIAL_TOKENS))\n\n    # Get the <pad> token's index\n    pad_idx = tokenizer.convert_tokens_to_ids(['<pad>'])[0]\n    \n    # Use keras's tokenizer to pad sequences with pad_idx\n    x = []\n    for i in tqdm(range(len(x_train))):\n        x.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x_train[i])[:MAX_LEN]))\n    \n    x_train = sequence.pad_sequences(x, maxlen=MAX_LEN, padding='post', value=pad_idx)\n    x_train = torch.tensor(x_train, dtype=torch.int32)\n\nI also made a kernel where I preprocess the data and save it to disk [here](https:\/\/www.kaggle.com\/bkkaggle\/jigsaw-preprocessing-gpt2-1)\n\n#### Resources\n- https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/issues\/573\n- https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/issues\/577\n- https:\/\/medium.com\/huggingface\/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313"}}