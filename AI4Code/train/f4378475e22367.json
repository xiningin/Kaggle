{"cell_type":{"4102a76a":"code","4d10a4e8":"code","7967a08a":"code","bd3e8a83":"code","99d17e0b":"code","ca78d963":"code","848bcb76":"code","00a8e9b4":"code","c3d43bc9":"code","a779ddfb":"code","51221ecb":"code","38b576f4":"code","ac85c229":"code","505f8626":"code","eab3aca0":"code","758dd98c":"code","074fe81c":"code","3c11f3db":"code","02a34f2f":"code","c747dae8":"code","4c192d8b":"code","6fdbe03b":"code","67223bf3":"code","56a67673":"code","f0fc6e49":"code","d7e0a194":"code","b91214e4":"code","3ce54c5b":"code","dd13ae2d":"code","03659b81":"code","748932d1":"code","059a30c0":"code","a7ffd709":"code","aaf4c193":"code","255555b8":"code","528333d8":"code","e188cb25":"code","9e324535":"code","1e6c7d0d":"code","72b96652":"code","1dd7fa16":"code","73e4e509":"code","dd12d9da":"code","16a5022e":"code","aad13c2e":"markdown","c8343e48":"markdown","e7d86048":"markdown","bfeea15a":"markdown","b9044cfb":"markdown","bb39a505":"markdown","64c24e7d":"markdown","ee08cd04":"markdown","c2462ca6":"markdown","2d9302f2":"markdown","5b602830":"markdown","a9c613b7":"markdown","61195153":"markdown","0177eb3c":"markdown","33fadd85":"markdown","5c1bd8f1":"markdown","849872f5":"markdown","caa5ddea":"markdown","bb85fafa":"markdown","6f185394":"markdown","ce167865":"markdown","18f01e21":"markdown","8ab0c523":"markdown","84bf8785":"markdown","89c6e08b":"markdown","ff2496e8":"markdown","0e328ea0":"markdown","17a15142":"markdown","7b000d86":"markdown","0adad308":"markdown","bb989b8d":"markdown","bcbb663d":"markdown","9942f299":"markdown","271ba8a3":"markdown","a5751bbd":"markdown","dd5d344a":"markdown","e1793563":"markdown","5f0c5fac":"markdown","61c02001":"markdown","e1f4c56f":"markdown","40692423":"markdown","b2c10bd9":"markdown","b92b9980":"markdown","095300b6":"markdown","74c4b127":"markdown","7f2f6524":"markdown","0f6f9a94":"markdown","24cf1015":"markdown","718a32be":"markdown","2a284385":"markdown","73cb3df2":"markdown"},"source":{"4102a76a":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","4d10a4e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7967a08a":"#import some necessary librairies\n\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics","bd3e8a83":"# Read the datasets\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","99d17e0b":"##display the first five rows of the train dataset.\ntrain.shape\n\n##display the first five rows of the test dataset.\ntest.shape","ca78d963":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","848bcb76":"# Create a separate dateset with categorical variables\ncategorical_features = train.select_dtypes(include=[np.object])\n\nimport matplotlib.ticker as ticker\nformatter = ticker.FormatStrFormatter('$%1.0f')\n\n# Define a function to plot bar charts\ndef pivottabs(var):\n    b=train[[var, 'SalePrice']].groupby([var], as_index=False).mean().sort_values(by='SalePrice', ascending=False)\n    b['SalePrice'] = b['SalePrice'].astype(int)\n    c=b['SalePrice'].max() + 30000\n    d = dict(zip(b[var], b['SalePrice']))\n\n    colors = [plt.cm.Spectral(i\/float(len(d.keys()))) for i in range(len(d.keys()))]\n\n    fig = plt.figure(figsize = (12, 8))\n    ax = fig.add_subplot()\n\n    ax.bar(d.keys(), d.values(), color = colors)\n\n    for k, v in d.items():\n        ax.text(k, v+1, v, fontsize = 10, horizontalalignment='center', verticalalignment='center')\n\n    ax.tick_params(axis='x',  labelsize = 12, labelrotation = 90)\n    ax.set_ylim(0, c)\n    ax.yaxis.set_major_formatter(formatter)\n    ax.set_ylabel('Average Sale Price', fontsize = 13)\n    ax.set_xlabel(var, fontsize=13)\n    ax.set_title(\"Average Sale Price by \"+ var, fontsize = 14)\n    \n# call the function for each categorical variable \nfor columns in categorical_features:\n    pivottabs(columns)","00a8e9b4":"\nimport matplotlib.ticker as ticker\nformatter = ticker.FormatStrFormatter('$%.0f')\nnumeric_features = train.select_dtypes(include=[np.number])\n\ndef scatterplots(col):\n    fig, ax = plt.subplots()\n    ax.scatter(x = train[col], y = train['SalePrice'])\n    ax.grid(False)\n    ax.yaxis.set_major_formatter(formatter)\n    plt.ylabel('SalePrice', fontsize=13)\n    plt.xlabel([col], fontsize=13)\n    plt.show()    \n    \nfor columns in numeric_features:\n    scatterplots(columns)","c3d43bc9":"#Correlation map to see how features are correlated with SalePrice\ncorr_matrix = train.corr()\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)]= True\n\nf, ax = plt.subplots(figsize=(30, 45)) \nheatmap = sns.heatmap(corr_matrix,\n                      fmt='.1g' ,\n                      mask = mask,\n                      square = True,\n                      linewidths = .5,\n                      cmap = 'coolwarm',\n                      cbar_kws = {'shrink': .4, 'ticks' : [-1, -.5, 0, 0.5, 1]},\n                      vmin = -1, \n                      vmax = 1,\n                      annot = True,\n                      annot_kws = {'size': 12})\n#add the column names as labels\nax.set_yticklabels(corr_matrix.columns, rotation = 0)\nax.set_xticklabels(corr_matrix.columns)\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True})\n","a779ddfb":"# There are oultliers in GrLivArea which should be deleted since this variable shows a strong replationship with target variable\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","51221ecb":"sns.distplot(train['SalePrice'] , fit=norm);","38b576f4":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);","ac85c229":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","505f8626":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","eab3aca0":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percentage of missing values', fontsize=15)\nplt.title('Percentage missing data by feature', fontsize=15)","758dd98c":"for col in ('PoolQC','MiscFeature', 'Alley', 'Fence','FireplaceQu','GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n            'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass'):\n    all_data[col] = all_data[col].fillna('None')","074fe81c":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","3c11f3db":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    all_data[col] = all_data[col].fillna(0)","02a34f2f":"for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType'):\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])","c747dae8":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","4c192d8b":"all_data = all_data.drop(['Utilities'], axis = 1)","6fdbe03b":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","67223bf3":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","56a67673":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'] + all_data['GrLivArea'] + all_data['GarageArea']\nall_data['Bathrooms'] = all_data['FullBath'] + all_data['HalfBath']*0.5 \n","f0fc6e49":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","d7e0a194":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","b91214e4":"\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)","3ce54c5b":"train = all_data[:ntrain]\ntest = all_data[ntrain:]\n","dd13ae2d":"import xgboost as xgb\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Lars\nfrom sklearn.linear_model import LassoLars\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import AdaBoostRegressor, BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve","03659b81":"# Cross validate model with Kfold stratified cross val\nn_folds = 5\nkf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)","748932d1":"# Modeling differents algorithms. \n\nrandom_state = 2\nregressor = []\n\nregressor.append(LinearRegression())\nregressor.append(KernelRidge())\nregressor.append(ElasticNet(random_state=random_state))\nregressor.append(Lasso(random_state=random_state))\nregressor.append(GradientBoostingRegressor(random_state=random_state))\nregressor.append(XGBRegressor(random_state = random_state))\nregressor.append(LGBMRegressor(random_state=random_state))\nregressor.append(BaggingRegressor(random_state=random_state))\nregressor.append(RandomForestRegressor(random_state=random_state))\nregressor.append(DecisionTreeRegressor(random_state=random_state))\n\n\ncv_results = []\nfor regressor in regressor :\n    cv_results.append(np.sqrt(-cross_val_score(regressor, train.values, y = y_train, scoring = \"neg_mean_squared_error\", cv = kf, n_jobs=4)))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\n                       \"Algorithm\":[\"LinearRegression\",\n                                    \"KernelRidge\",\n                                    \"ElasticNet\",\n                                    \"Lasso\",\n                                    \"GradientBoostingRegressor\",\n                                    \"XGBRegressor\",\n                                    \"LGBMRegressor\",\n                                    \"DecisionTreeRegressor\",\n                                    \"BaggingRegressor\",\n                                    \"RandomForestRegressor\"\n                                    ]})\n\ncv_res = cv_res.sort_values(by = 'CrossValMeans', ascending=True)\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean RMSE\")\ng = g.set_title(\"Cross validation scores\")","059a30c0":"# KRR = KernelRidge()\n\n# KRR_grid = {\"alpha\" : [25,10,4,2,1.0,0.8,0.5,0.3,0.2,0.1,0.05,0.02,0.01],\n#                   \"kernel\" :   [\"polynomial\"],\n#                   \"degree\" : [1,2,3,4,5],\n#                   \"coef0\" :[1,1.5,2,2.5,3,3.5,4,4.5,5]\n#                   }\n\n# KRRModel = GridSearchCV(estimator = KRR, param_grid = KRR_grid, cv=kf, scoring=\"neg_mean_squared_error\", n_jobs= 4, verbose = 1)\n# KRRModel.fit(train,y_train)\n# KRR_best = KRRModel.best_estimator_\n# KRRModel.best_params_\n","a7ffd709":"# GBR = GradientBoostingRegressor()\n\n# GBR_grid = {\"n_estimators\" : [2000,3000],\n#             \"learning_rate\" :   [0.01,0.1],\n#             \"max_depth\" : [3,5],\n#             \"max_features\" :['sqrt'],\n#             \"min_samples_leaf\" :[10,15],\n#             \"min_samples_split\" :[2,5],\n#             \"loss\" :['huber']            \n#             }\n \n# GBRModel = GridSearchCV(estimator = GBR, param_grid = GBR_grid, cv=kf, scoring=\"neg_mean_squared_error\", n_jobs= 4, verbose = 1)\n# GBRModel.fit(train,y_train)\n# GBR_best = GBRModel.best_estimator_\n# GBRModel.best_params_","aaf4c193":"# LGBM = LGBMRegressor()\n\n# LGBM_grid = {\"n_estimators\" : [700, 720, 750],\n#             \"learning_rate\" :   [0.01,0.03, 0.05],\n#             \"max_bin\" :[55],\n#             \"bagging_fraction\" :[0.8, 0.7],\n#             \"bagging_freq\" :[5]\n#             }\n\n# LGBMModel = GridSearchCV(estimator = LGBM, param_grid = LGBM_grid, cv=kf, scoring=\"neg_mean_squared_error\", n_jobs= 4, verbose = 1)\n# LGBMModel.fit(train,y_train)\n# LGBM_best = LGBMModel.best_estimator_\n# LGBMModel.best_params_\n","255555b8":"# XGB = XGBRegressor()\n\n# XGB_grid =    {'nthread':[4], \n#               'objective':['reg:linear'],\n#               'learning_rate': [.03, 0.05, .07], #so called `eta` value\n#               'max_depth': [5, 6, 7],\n#               'min_child_weight': [4],\n#               'silent': [1],\n#               'subsample': [0.7],\n#               'colsample_bytree': [0.7],\n#               'n_estimators': [500]}\n \n# XGBModel = GridSearchCV(estimator = XGB, param_grid = XGB_grid, cv=kf, scoring=\"neg_mean_squared_error\", n_jobs= 4, verbose = 1)\n# XGBModel.fit(train,y_train)\n# XGB_best = XGBModel.best_estimator_\n# XGBModel.best_params_","528333d8":"# DTR = DecisionTreeRegressor(random_state=0)\n\n# DTR_grid =    {\"criterion\":['mse'],\n#                \"splitter\":['best'],\n#                \"max_depth\" : [2,3,5,10],\n#             \"max_features\" :['sqrt'],\n#             \"min_samples_leaf\" :[5,10,15],\n#             \"min_samples_split\" :[1,2,5]\n#               }\n \n# DTRModel = GridSearchCV(estimator = DTR, param_grid = DTR_grid, cv=kf, scoring=\"neg_mean_squared_error\", n_jobs= 4, verbose = 1)\n# DTRModel.fit(train,y_train)\n# DTR_best = DTRModel.best_estimator_\n# DTRModel.best_params_","e188cb25":"# LARS = Lasso()\n\n# LARS_grid = {\"alpha\" : [1,0.8,0.3,0.2,0.1,0.05,0.005,0.02,0.01],\n#              \"max_iter\" :[500,700,1000]          \n#                   }\n\n# LARSModel = GridSearchCV(estimator = LARS, param_grid = LARS_grid, cv=kf, scoring=\"neg_mean_squared_error\", n_jobs= 4, verbose = 1)\n# LARSModel.fit(train,y_train)\n# LARS_best = LARSModel.best_estimator_\n# LARSModel.best_params_","9e324535":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","1e6c7d0d":"#Build the base models based on GridSearch tuning\n\nKRR = KernelRidge(alpha=0.8, coef0=5, degree=2, gamma=None, kernel='polynomial', kernel_params=None)\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, max_iter = 500, random_state=1))\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01, max_depth=3, max_features='sqrt', min_samples_leaf=10, min_samples_split=5, loss='huber')\n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.7, learning_rate=0.03, max_depth=6, min_child_weight=4, n_estimators=500, subsample=0.7, silent=1, random_state =7)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,learning_rate=0.05, n_estimators=720, max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nmodel_DTR = DecisionTreeRegressor(criterion = 'mse', max_depth = 10, max_features = 'sqrt', min_samples_leaf = 5, min_samples_split = 2, splitter = 'best' )\n\nLRModel = LinearRegression()","72b96652":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_DTR)\nprint(\"DT Regression score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n\nscore = rmsle_cv(LRModel)\nprint(\"Linear Regression score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","1dd7fa16":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","73e4e509":"#Averaging the best models to optimize the prediction.\n\n#averaged_models = AveragingModels(models = (KRR, lasso, ENet, GBoost, model_xgb, model_lgb, model_DTR, LRModel))\naveraged_models = AveragingModels(models = (KRR, ENet, GBoost, lasso, model_xgb))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","dd12d9da":"averaged_models.fit(train.values, y_train)\naveraged_models_pred = np.expm1(averaged_models.predict(test.values))","16a5022e":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = averaged_models_pred\nsub.to_csv('finalSubmission.csv',index=False)","aad13c2e":"**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.","c8343e48":"**Feature that should be imputed with respective mode values**","e7d86048":"# Define a cross validation strategy","bfeea15a":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>","b9044cfb":"**Outliar Treatment**","bb39a505":"**Missing Data**","64c24e7d":"- **PoolQC** : data description says NA means \"No  Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. \n- **MiscFeature** : data description says NA means \"no misc feature\"\n- **Alley** : data description says NA means \"no alley access\"\n- **Fence** : data description says NA means \"no fence\"\n- **FireplaceQu** : data description says NA means \"no fireplace\"\n- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None\n- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no  basement.\n- **MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill None for the type. \n- **MSSubClass** : Na most likely means No building class. We can replace missing values with None","ee08cd04":"- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage)\n- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement\n- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. ","c2462ca6":"- **Functional** : data description says NA means typical","2d9302f2":"**Gradient Boosting Regressor**\n\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.","5b602830":"# What data we have\n![Data.jpg](attachment:Data.jpg)\n\nIn the absence of external factors, we would leverage the given factors. If the number of variables are not too high, I would recommend creating an excel table to summarize your variable analysis. This would help in analyzing variable information at single place and you can avoid scrolling the kernel up and down to review each variable individually.","a9c613b7":"- **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can **fill in missing values by the median LotFrontage of the neighborhood**.","61195153":"Welcome back!!!\n\nAs another effort to put down the basics together, I am writing this Kernel to guide young professionals on how machine learning projects should be framed and approached. As I always say, you should look at these challeneges from outsider's persepective. The whole idea is to understand how complex problems should be broken down into manageable chunks.","0177eb3c":"**Check relationship between numerical variables and Sale Price**","33fadd85":"let's first  concatenate the train and test data in the same dataframe","5c1bd8f1":"Is there any remaining missing value ? ","849872f5":"The skew seems now corrected and the data appears more normally distributed. ","caa5ddea":"**Lasso**\n\nlasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.","bb85fafa":"# Final submission","6f185394":"# Treating skewed features","ce167865":"![hprice.png](attachment:hprice.png)","18f01e21":"# Imputing missing values","8ab0c523":"# Observations - continous variables\n\n\nBased on scatterplot and correlation matrix, we can take certain decisions. \n\n**MSSub** - It's technically a categorical variable. The average sale price is high for certain categories so this variable might be useful \n\n**LotFrontage** - Doesn't look clearly associated with target variable\n\n**LotArea** - Lot size in sq feet seems to be not correlated with the target variable.\n\n**OverallQual** - Avg price increases with OverallQual and it's actually a categorical variable\n\n**OverallCond** - Overall condition rating seems to be highly correlated with Sale Price and also correlated with other variables such as YearBuilt, YearRemodAdd for obvious reasons.\n\n**YearBuilt** - Correlated with the target variable and other features and highly correlated with GarageYrBlt. We should use either one of them\n\n**YearRemodAdd** - Shows some association with the target variable\n\n**MasVnrArea** - 0.5 correlaion with target variable. We can park it for sometime and see how it affects ML algorithms\n\n**BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF** - Low correlation. TotalBsmtSF is highly correlated with 1stFlrSF but since the impact of 1stFlrSF on the target variable is more than TotalBsmtSF, we will keep it and drop the other.\n\n**1stFlrSF** - Correlated with GrLivArea. \n\n**2ndFlrSF** - Correlated with GrLivArea.\n\n**LowQualFinSF** - Low correlation. \n\n**GrLivArea** - Highly correlated with the target variable. There are oultliers in GrLivArea which should be deleted since this variable shows a strong replationship with target variable\n\n**BsmtFullBath and BsmtHalfBath** - Low correlation\n\n**FullBath and HalfBath** - Can be used to create new feature total bathrooms\n\n**BedroomAbvGr** - Low impact on target variable \n\n**KitchenAbvGr**\t - low correlation.\t\n\n**TotRmsAbvGrd** -  Total rooms above grade (does not include bathrooms) seems important\t \n\n**Fireplaces** - Seems important so we will keep it\t \n\n**GarageYrBlt** - It's correlated with YearBuilt \n\n**GarageCars** - Correlated with GarageArea and is of low importance \n\n**GarageArea** - Will keep this for further analysis\n\n**WoodDeckSF** - Low correlation. \n\n**OpenPorchSF** - Low correlation. \n\n**EnclosedPorch** - Low correlation. \n\n**3SsnPorch** - Low correlation.  \n\n**ScreenPorch** - Low correlation.\n\n**PoolArea** - Low correlation. \n\n**MiscVal** - Low correlation. \n\n**MoSold** - Low correlation.  \t \n\n**YrSold** - Low correlation.  \n\n","84bf8785":"**LGBMRegressor**\n\nLight GBM is a gradient boosting framework that uses tree based learning algorithm. Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.","89c6e08b":"**Adding one more important feature**","ff2496e8":"The target variable is right skewed.  As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","0e328ea0":"Getting the new train and test sets. ","17a15142":"It remains no missing value.\n","7b000d86":"# Averaging the base models","0adad308":"> **All codes commented post parameter tuning to save on run time.**","bb989b8d":"**KernelRidge**\n\nKernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.","bcbb663d":"# Observations - categorical variables\n\n**MSZoning** - This represents the general zoning classification. The average sale price for FV and RL zones is higher than in other zones.\n\n**Alley** - Alley represents the type of Alley access. Can we say that access to paved Alley would fetch you a better price for your property? We can't be so sure since more than 90% of values for this field are missing.\n\n**LotShape** - Lotshape is the general shape of the property. It seems 'IR2' is the most preferred one.\n\n**LandContour** - It represents flatness of the property. Though there is not a huge difference in average prices, 'HLS' seems to be preferred one. \n\n**Utilities** - It's evident that certain type (AllPub) of utilities is preferred over other.\n\n**LotConfig** - It represents lot configuration. We do see LotConfig affecting the average sale price. \n\n**LandSlope** - The slope of the property seems to be slightly related.\n\n**Neighborhood** - Highly related to Sale Price. We can clearly see that certain neighborhoods are preferred by buyers.\n\n**Condition1 & Condition2** - These variables represent proximity to the main road or railroad and we can observe sale price varying by conditions.\n\n**BldgType** - The average price for '1Fam' and 'twnhsE' type of dwelling is better than the other 3 dwelling types. \n\n**HouseStyle** - It represents the style of the dwelling and it appears that '2.5In' and '2Story' dwelling styles affect the sale price. \u00a0\n\n**RoofStyle and RoofMatl** - RoofStyle and RoofMatl represent the\u00a0type of roof and roof material respectively. It's evident that average prices for 'Shed' and 'Hip' roof-style and 'Wood Shingle' roof material impact sale price majorly.\n\n**Exterior1st and Exterior2nd** - These 2 variables reflect exterior covering on the house and we can clearly notice some pattern there.\n\n**MasVnrType** - 'Stone' is the preferred masonry veneer type.\n\n**ExterQual and ExterCond** - These variables are all about the quality of the exterior material and present condition of the same. As we can see in bar charts, the exterior quality and present condition of exterior material makes a lot of difference to the average sale price\nFoundation - Dwellings with 'PConc' foundation type fetch better average sale price. It seems 'Pconc' stands for concrete.\u00a0\n\n**BsmtQual,\u00a0BsmtCond,\u00a0BsmtExposure,\u00a0BsmtFinType1,\u00a0BsmtFinType2**\u00a0- These basement specific variables are about the quality and condition of the basement.\nHeating - Gas heating seems to be the most preferred one and heated floors are not that popular.\n\n**HeatingQC** - Excellent heating quality and condition show better average pricing.\n\n**CentralAir** - Availability of a central air conditioning system makes a property costlier.\n\n**Electrical** - 'SBrKr' type of electrical system seems to be important.\n\n**KitchenQual** - 'Excellent' Kitchen quality represents the overall condition of the property. A house with a good quality kitchen is expected to be costlier.\n\n**Functional** - 'Typ' home functionality rating is the preferred rating by home buyers.\u00a0\n\n**FireplaceQu** - It indicates quality of the fireplace. Almost 47% of the observations are missing for this variable.\n\n**GarageType, GarageFinish, GarageQual, and GarageCond** - These factors represent the garage location, finish, overall quality, and condition of the garage.\u00a0\nPavedDrive - Paved driveway impacts the average prices.\n\n**PoolQC** - It represents the condition of the pool if present. 99.5% properties do not have a pool \n\n**Fence** - Fence quality. 80% of the observations are missing.\u00a0\n\n**MiscFeature** -\u00a0\u00a0Miscellaneous feature not covered in other categories. 96% of the observations are missing.\n\n**SaleType** - 9 categories of sale type. Properties with category 'New' fetched the highest average price.\n\n**SaleCondition** - Represents the condition of the sale.\u00a0\n\n\n","9942f299":"# Building the base models","271ba8a3":"# GridSearch for parameter tuning","a5751bbd":"**Decision Tree Regressor**\n\nA decision tree regressor","dd5d344a":"- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'\n- **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.\n- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n- **SaleType** : Fill in again with most frequent which is \"WD\"","e1793563":"**Evaluating multiple models using GridSearch optimization method.**\n\nHyper-parameters are key parameters that are not directly learnt within the estimators. We have to pass these as arguments. Different hyper parameters can result in different model with varying performance\/accuracy. To find out what paparmeters are resulting in best score, we can use Grid Search method and use the optimum set of hyper parameters to build and select a good model.\n\nA search consists of:\n\n* an estimator (regressor or classifier)\n* a parameter space;\n* a method for searching or sampling candidates;\n* a cross-validation scheme; and\n* a score function.","5f0c5fac":"Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","61c02001":"**Feature that should be imputed with 0**","e1f4c56f":" **Log-transformation of the target variable**","40692423":"![CV.png](attachment:CV.png)\n\nCross Validation is one of the most powerful tool in Data Scientist's tool box. It helps you to understand the performance of your model and fight with overfitting. As we all know that Learning the model parameters and testing it on the same data is a big mistake. Such a model would have learned everything about the training data and would give result in a near perfect test score as it has already seen the data. The same model would fail terribly when tested on unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test.\n\nThe general approach is as follows:\n\n1. Split the dataset into k groups\n2. For each unique group:\n\n> * >  a. Keep one group as a hold out or test data set\n* >  b. Use the remaining groups as training data set\n* >  c. Build the model on the training set and evaluate it on the test set\n* >  d. Save the evaluation score \n\n3. Summarize the performance of the model using the sample of model evaluation scores\n\nYou can access following link and read about Cross Validation in detail.\n\nhttps:\/\/medium.com\/datadriveninvestor\/k-fold-cross-validation-6b8518070833 https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/improve-model-performance-cross-validation-in-python-r\/","b2c10bd9":"**Feature that should be imputed with median values**","b92b9980":"**XGBRegressor**\n\nThe XGBoost is a popular supervised machine learning model with characteristics like fast in computation, parallelization, and better performance","095300b6":"**Getting dummy categorical features**","74c4b127":"Let's impute the missing values - \n\n**Starting with variables which can be imputed with 'None'**","7f2f6524":"**Import libraries**","0f6f9a94":"**If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated -  That will keep me motivated** :-)\n\n![Good%20Bye.png](attachment:Good%20Bye.png)\n\n","24cf1015":"# The key objective\n![Real%20Estate.png](attachment:Real%20Estate.png)\n\n*Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.*\n\n*With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.*\n\nA dataset with 79 variables is given and the expectation is to build the model to predict the price of the house. Now there are different ways at looking at this challenge. \n1. Should we simply treat this as a data problem?\n2. Had it been a real problem and not a kaggle competition, how differently we would have treated this?\n3. Is the data enough (length & breadth) to predict house prices?\n4. Real world house prices are primarily driven by certain key factors such as availability of good schools, crime rate, availability of transport, infrastructure projects and so on\n\n","718a32be":"**Box Cox Transformation of (highly) skewed features**","2a284385":"**Transforming some numerical variables that are really categorical**","73cb3df2":"# Exploratory data analysis\n![analysis.png](attachment:analysis.png)\n\nThough we can analyze each variable one by one through visualization, I find it much easier to simply write a function to draw plots at one go. To keep it simple, we are just going to plot categorical variables and analyze average Sale Prices for each."}}