{"cell_type":{"c591e317":"code","dcdbdc47":"code","5d9077ad":"code","4b948746":"code","61e51371":"code","263e9f54":"code","5514efd6":"markdown","e23e1942":"markdown","2b675912":"markdown","02493768":"markdown","ba018f25":"markdown","8000d486":"markdown","5cd96cc1":"markdown","d74be290":"markdown","4d5d00b8":"markdown","04982f71":"markdown"},"source":{"c591e317":"import numpy\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\nimport warnings\nwarnings.filterwarnings('ignore')\n","dcdbdc47":"K.set_image_dim_ordering('th')\nseed = 7\nnumpy.random.seed(seed)\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\nX_train = X_train \/ 255\nX_test = X_test \/ 255\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\nnum_classes = y_test.shape[1]\n\n\n","5d9077ad":"image_gen = ImageDataGenerator(featurewise_center=False,\n    featurewise_std_normalization=False,\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,                                               \n    horizontal_flip=False, shear_range=0.2)\nimage_gen.fit(X_train, augment=True)\n","4b948746":"def cnn_model():\n    model = Sequential()\n    \n    model.add(Conv2D(30, (5, 5), padding='same', input_shape=(1, 28, 28), activation='relu'))\n    model.add(Conv2D(30, (5, 5), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n    \n    model.add(Conv2D(15, (3, 3), padding='same', activation='relu'))\n    model.add(Conv2D(15, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n    \n    \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n\n","61e51371":"model = cnn_model()\n\n\n#model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200)\n\nmodel.fit_generator(image_gen.flow(X_train, y_train, batch_size=32),steps_per_epoch=len(X_train) \/ 32,  \\\n                              epochs=32)","263e9f54":"scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\nprint(model.summary())","5514efd6":"Importing all required libraries","e23e1942":"**Fitting our model **\n* 1 Epoch = 1 Forward pass + 1 Backward pass for ALL training samples.I choose this to be 20 after hit and trial.Do remember to turn on gpu feature as it speeds up process.\n* Batch Size = Number of training samples in 1 Forward\/1 Backward pass. (With increase in Batch size, required memory space increases.)","2b675912":"**Model Accuracy - 99.48 % which is pretty accurate** ","02493768":"Importing our training data which is 28x28 size images of Hand written digits","ba018f25":"**Convolutional Neural Network with Keras**\n\nAfter working on different machine learning algorithms  have started analyzing  Neural Networks and how they works.\nIt is always better to have knowledge of both high level approach and then start learning mathematical derivations of algorithms we are applying.\nComing to Image recognition  tasks what I found after using neural network is that Neural network  are really much more powerful as compared to ML algos. In Ml we first need to identify contours then subparts of contour like wheels ,window if we want to identify a car which requires applying different logics and different algos.\nIn deep learning we only define layers of neurons in particular order and choose type of activation function and loss function\nI have compiled below mentioned information after studying from multiple sources on internet as reading just 1 article  we can not have clear picture about CNN.\n\nIn this kernal you will gain knowledge about - \n1.  Theoritical concept of Convolutional Neural Network.\n2.  Basic Perceptron Algorithm & BackPropogation Derivation.\n3.  Using High level library Keras for recognizing MNIST dataset.","8000d486":"**First why Convolutional Neural Networks ?**\nEach neuron in layer 1 and layer 2 of traditional neural networks are interconnected which makes it  computationaly expensive task to process images using neural networks as lot of weights needed to be learned.\nIn contrast CNN networks only few number of neurons are connected to neuron in 2nd layer depending on what feature we want to learn.Each combination identifying different set of features.\n\n**Theoritical concept of Convolutional Neural Network**\nPlease go through below mentioned link which gives very intuitive approach to learn - \nhttps:\/\/medium.com\/technologymadeeasy\/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8\nhttps:\/\/www.learnopencv.com\/image-classification-using-convolutional-neural-networks-in-keras\/","5cd96cc1":"**CNN Layers -**\n* **layer 1**\nConvolution Layer (30 features, same padding) --> relu --> Convolution Layer (30 features)  --> relu --> pooling layer \n* **layer 2**\nConvolution Layer (15 features, same padding) --> relu --> Convolution Layer (15 features) --> relu --> pooling layer --> Flatten --> DenseLayer --> Dense Layer  --> output\n\n**Explaining few terms -** \n1. Relu is a non linear activation function that maps output between 0 and infinity \n2. Same padding keeps the matrix size same by adding zeros in additional space\n3. Dropout helps in reducing overfitting in neural networks it turns off few neurons in layer randomly during each forward pass \n4. As we have multiple output classes we will use softmax function in output layer and categorical_crossentropy as loss function\n5. Optimization algo is used to update weights in network. Adam optimizer requires less memory and is Computationally efficient with good results ","d74be290":"Refrences ,Textbooks and Tutorials :\n*  https:\/\/medium.com\/technologymadeeasy\/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8\n*  https:\/\/www.learnopencv.com\/image-classification-using-convolutional-neural-networks-in-keras\/\n*  https:\/\/youtu.be\/0e0z28wAWfg\n \n\nThis Kernel is written by Chakshu Garg\n\n**I will keep on adding new findings to optimize CNN models in this notebook. Do share your valuable feedback in comment section below**. **:)**","4d5d00b8":"**3. Using High level library Keras for recognizing MNIST dataset.**\n   In starting I was confused due to different Deep learning libraries available in market each having own drawback and benefits. I choose Keras to start with as we can build powerful models on top of popular tensorflow and other libraries using this high level library in just few minutes giving us more spare time to invest in choosing number of layers and structure of neurons in network.","04982f71":"**2. Basic Perceptron Algorithm & BackPropogation Derivation -** \nAs explaining everything is beyond the scope of this notebook so just attaching screenshot from my notes if you have any doubt please feel free to comment below or you can email me at chakshu00garg@gmail.com -\n![image.png](attachment:image.png)\n \n"}}