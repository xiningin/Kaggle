{"cell_type":{"4b066a6c":"code","4562795c":"code","31066c86":"code","dc9b99c9":"code","e89ebd03":"code","3ca0a7fa":"code","b42c4e65":"code","979ddc39":"code","ced4c793":"code","39229f1c":"code","1e95b3e3":"code","e1d76709":"code","43e70d23":"code","a7e3a113":"code","2f0886b0":"code","f4216cef":"code","2f87912a":"code","5015b389":"code","d96dda60":"code","29045382":"code","8b0572c4":"code","43b04419":"code","f246d212":"code","a7cdf4eb":"code","2a1cdfa7":"code","30ef4d64":"code","0a6fcef4":"code","6c373bc9":"code","5a458d01":"markdown","7ec1c193":"markdown","7497dd3c":"markdown","c8677429":"markdown","d8eb5908":"markdown"},"source":{"4b066a6c":"# importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nfrom sklearn.model_selection import train_test_split\nimport re, string, nltk\nfrom wordcloud import WordCloud, STOPWORDS\nimport emoji\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Bidirectional, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4562795c":"df = pd.read_csv(\"..\/input\/amazon-reviews\/train.csv\",header=None)\ndf.columns=['sentiment','title','text']\ndf","31066c86":"# shape of data\nprint(f\"Data consists of {df.shape[0]} rows and {df.shape[1]} columns.\")","dc9b99c9":"# checking for null values\ndf.isna().sum()","e89ebd03":"# dropping null values\ndf = df.dropna()","3ca0a7fa":"# checking for any duplicate in the data\ndf.duplicated().sum()","b42c4e65":"df1 = df.sample(frac=0.05)\ndf1.shape","979ddc39":"df1.sentiment.value_counts()","ced4c793":"sns.countplot(df1.sentiment,palette=\"mako\")\nplt.title(\"Countplot for Sentiment Labels\")","39229f1c":"df1[\"title\"] = df1[\"title\"].astype(str)","1e95b3e3":"def clean_text(df, field):\n    df[field] = df[field].str.replace(r\"@\",\" at \")\n    df[field] = df[field].str.replace(\"#[^a-zA-Z0-9_]+\",\" \")\n    df[field] = df[field].str.replace(r\"[^a-zA-Z(),\\\"'\\n_]\",\" \")\n    df[field] = df[field].str.replace(r\"http\\S+\",\"\")\n    df[field] = df[field].str.lower()\n    return df\n\nclean_text(df1,\"title\")","e1d76709":"lemmatizer = WordNetLemmatizer()\n#stemmer = SnowballStemmer(\"english\")\nimport re\n#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\" \n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n#Lemmatize the corpus\ndef lemmatize_text(data):\n   # lemmatizer=WordNetLemmatizer()\n    out_data=\"\"\n    for words in data:\n        out_data+= lemmatizer.lemmatize(words)\n    return out_data\n\n#df1[\"title_clean\"] = df1[\"title\"].apply(preprocess_text)\ndf1[\"title_clean\"] = df1[\"title\"].apply(lambda x: remove_punctuations(x))\ndf1[\"title_clean\"] = df1[\"title_clean\"].apply(lambda x: remove_emoji(x))\ndf1[\"title_clean\"] = df1[\"title_clean\"].apply(lambda x: remove_html(x))\ndf1[\"title_clean\"] = df1[\"title_clean\"].apply(lambda x: remove_url(x))\ndf1[\"title_clean\"] = df1[\"title_clean\"].apply(lambda x: lemmatize_text(x))","43e70d23":"df1.head(15)","a7e3a113":"df1.sentiment.replace({1:0,2:1},inplace=True)\ndf1.head()","2f0886b0":"df = df1[[\"sentiment\",\"title_clean\"]]\ndf.head(10)","f4216cef":"df.sentiment.unique()","2f87912a":"from sklearn.model_selection import train_test_split\nX = df[\"title_clean\"]\ny = df.sentiment\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\ndisplay(X_train.shape)\ndisplay(X_test.shape)","5015b389":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)","d96dda60":"# using tokenizer to transform text messages into training and testing set\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)","29045382":"X_train_seq_padded = pad_sequences(X_train_seq, maxlen=64)\nX_test_seq_padded = pad_sequences(X_test_seq, maxlen=64)","8b0572c4":"X_train_seq_padded[0]","43b04419":"# construct model\nBATCH_SIZE = 32\n\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.index_word)+1,64))\nmodel.add(Bidirectional(LSTM(100, dropout=0,recurrent_dropout=0)))\nmodel.add(Dense(64, activation=\"relu\"))\nmodel.add(Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(\"adam\",\"binary_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()","f246d212":"from keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor=\"val_loss\",patience=5,verbose=True)","a7cdf4eb":"history = model.fit(X_train_seq_padded, y_train,batch_size=BATCH_SIZE,epochs=15,\n                    validation_data=(X_test_seq_padded, y_test),callbacks=[early_stop])","2a1cdfa7":"from sklearn.metrics import roc_auc_score\npred_train = model.predict(X_train_seq_padded)\npred_test = model.predict(X_test_seq_padded)\nprint('LSTM Recurrent Neural Network baseline: ' + str(roc_auc_score(y_train, pred_train)))\nprint('LSTM Recurrent Neural Network: ' + str(roc_auc_score(y_test, pred_test)))","30ef4d64":"model.evaluate(X_test_seq_padded, y_test)","0a6fcef4":"acc = history.history[\"accuracy\"]\nloss = history.history[\"loss\"]\n\nval_acc = history.history[\"val_accuracy\"]\nval_loss = history.history[\"val_loss\"]\n\nplt.figure(figsize=(9,6))\nplt.plot(acc,label=\"Training Accuracy\")\nplt.plot(val_acc,label=\"Validation Accuracy\")\nplt.legend()\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training and Validation Accuracy\")","6c373bc9":"plt.figure(figsize=(9,6))\nplt.plot(loss,label=\"Training Loss\")\nplt.plot(val_loss, label=\"Validation Loss\")\nplt.legend()\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss\")","5a458d01":"# Cleaning Data","7ec1c193":"# Train Test Split","7497dd3c":"# Amazon Title Reviews Sentiment - Bidirectional LSTM","c8677429":"# Bidirectional LSTM","d8eb5908":"* Classes are balanced. So, no need for oversampling or undersampling the target column."}}