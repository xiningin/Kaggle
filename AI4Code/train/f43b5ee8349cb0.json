{"cell_type":{"c12d739a":"code","57cfc5c0":"code","317f84e4":"code","2c97d4e5":"code","4e85e4ec":"code","200061a1":"code","65cf87f2":"code","9b5f79ac":"markdown","5ea6e927":"markdown","a4d811de":"markdown","7a325fbe":"markdown","b369f39f":"markdown","6f28b000":"markdown","ab82d5c2":"markdown","654728f5":"markdown","1bdd2697":"markdown","e2e38fe0":"markdown"},"source":{"c12d739a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nraw_data = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\nraw_data.head()","57cfc5c0":"corr_data = raw_data.copy()\n\n#It's not important what character is what number for calculate. Just turn it into digit value.\ndef to_digit(i):\n    if i in chars:\n        return chars[i]\n    else:\n        chars[i] = len(chars)+1\n        return chars[i]\n    \nfeatures = list(corr_data.columns)\n\nfor idx in features:\n    chars = {}\n    corr_data[idx] = corr_data[idx].map(to_digit)\n\nprint(corr_data)","317f84e4":"plot_corr = corr_data.corr()\n\nplot_corr = plot_corr.apply(lambda x : round(x, 2))\n\nclass_corr = plot_corr.iloc[0, :]\n\nclass_corr = class_corr.map(abs)\n\ncorr_x = class_corr.index[1:]\ncorr_y = class_corr.values[1:]\n\ncolors = sns.color_palette('pastel',len(corr_x))\n\nplt.figure(figsize=(25, 5))\nplt.bar(corr_x, corr_y, color = colors)\nplt.title(\"Correlation with class\", fontsize = 20, pad = 30)\nplt.tick_params(axis='x', labelsize=14, labelrotation=90)\n\nplt.show()","2c97d4e5":"# I'm gonna use these features which at least above 0.3\n#['bruises', 'odor', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring','ring-type', 'spore-print-color']\n\n#First, modify values to digit. I already make to_digit() above.\ntrain_data = raw_data.copy()\n\nfeatures = list(train_data.columns)\n\nfor idx in features:\n    chars = {}\n    train_data[idx] = train_data[idx].map(to_digit)\n\n\nX = train_data.drop('class', axis=1)\nX = X.loc[:, ['bruises', 'odor', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring','ring-type', 'spore-print-color']]\ny = train_data.loc[:, 'class']\n\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.33, random_state=42)","4e85e4ec":"from sklearn.naive_bayes import CategoricalNB\n\nnaive_bayes_model = CategoricalNB()\nnaive_bayes_model.fit(train_X, train_y)\n\nnaive_bayes_prediction = naive_bayes_model.predict(valid_X)\n\nprint(\"Accuracy:\", metrics.accuracy_score(valid_y, naive_bayes_prediction))","200061a1":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest_model = RandomForestClassifier(n_estimators = 36, criterion = 'gini', max_depth=2, random_state=42, verbose=1)#n_estimators = 35 -> Accuracy :  0.9488..\nrandom_forest_model.fit(train_X, train_y)\n\nrandom_forest_prediction = random_forest_model.predict(valid_X)\n\nprint(\"Accuracy:\", metrics.accuracy_score(valid_y, random_forest_prediction))","65cf87f2":"from sklearn.svm import SVC\n\nsupport_vector_machine_model = SVC()\nsupport_vector_machine_model.fit(train_X, train_y)\nsupport_vector_machine_prediction = support_vector_machine_model.predict(valid_X)\n\nprint(\"Accuracy:\", metrics.accuracy_score(valid_y, support_vector_machine_prediction))","9b5f79ac":"## 3. Correlation","5ea6e927":"## 5. Model 1 [Naive bayes]\n`p(\u0398|X) = p(X|\u0398)p(\u0398) \/ p(X)`","a4d811de":"## 2. Preprocess for calculate correlation\n\nThere are so many non-digit values in raw data. So, I'm gonna turn it into digit.","7a325fbe":"## 6. Model 2 [Random forest]","b369f39f":"## 7. Model 3 [Support vector machine]","6f28b000":"## 4. Preprocessing train data","ab82d5c2":"## 8. Conclusion","654728f5":"> Naive bayes(categoricalNB) ACC : 0.9433047370384186\n>\n> Random forest ACC : 0.9541215964192465 (n_estimators > 35)\n>\n> Support vector machine ACC : 0.9955240581872435\n\nSo, I would say SVM is better than other algorithms in this case.\ud83d\ude06","1bdd2697":"# Comparison between three model to predict poisonous mushroom \ud83e\udda0\n> 1. Naive Bayes\n> 2. Random Forest\n> 3. Support Vector Machine\n\n---","e2e38fe0":"## 1. Import modules & Read data"}}