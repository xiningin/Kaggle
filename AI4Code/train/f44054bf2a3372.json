{"cell_type":{"31c3eb4c":"code","35a0e578":"code","0a2afbb1":"code","f3d37d39":"code","24b4c900":"code","e4b96cd9":"code","50577b54":"code","444295ec":"code","9cc7f0e9":"code","c98a5b13":"code","839d6d2c":"code","2fec7a60":"code","58255da7":"code","a0d1a5ba":"code","f75e12a1":"code","d027b3d4":"code","88baea55":"code","5f1d0e5b":"code","8e598b80":"code","5033f7f5":"code","9018c362":"code","f33987a5":"code","14e6aff2":"code","f12efbfd":"code","91f0be23":"code","ea2e65ac":"code","794d1261":"code","90a8a037":"code","b482db89":"code","c2694748":"code","a8eba26f":"code","3d7117dd":"code","92a4ab99":"code","3e9a297d":"code","f4aca94b":"code","7284dfcd":"code","f32e9eec":"code","76b8f729":"code","ba50e657":"code","ebc782c6":"code","01aa064e":"code","0b869326":"code","9badf287":"code","81880973":"code","83d02afc":"code","6b6e6324":"code","b3e72539":"code","236f26a9":"code","3a397ec1":"code","5bb351e4":"code","6eaca40c":"code","ce5be341":"code","7044fdf7":"code","e8f16ed3":"code","129ab4e9":"code","2e8ad779":"code","f00b50d0":"code","15b5759a":"code","4415efdc":"code","39aca0d3":"code","eedabea9":"code","65f6d745":"code","88bef446":"code","ccc9f8a6":"code","4e089cf8":"code","b0d046de":"code","9d5a4995":"code","137c8382":"code","bf3ed2ca":"code","60441613":"code","d1515461":"code","6713805e":"code","701e73bd":"code","67047ec7":"code","ce510971":"code","c1317a46":"code","4a85d766":"code","18cb0af1":"code","cc093c8b":"code","90f29611":"code","ffc72269":"code","37bd2c3f":"code","dff92b04":"code","f4f36e54":"code","0f66460d":"code","0964596b":"code","dc695ca7":"code","532f82f9":"code","6a0b4cfd":"code","972ec02e":"code","020356b8":"code","85182fc1":"code","34dd8452":"code","8fb7f3c6":"markdown","c84cff45":"markdown","5a4085a9":"markdown","9a7858bb":"markdown","2974901b":"markdown","0be09329":"markdown","b14903bc":"markdown","8f7fb076":"markdown","18d5cf5c":"markdown","a09f0ec5":"markdown","20f28043":"markdown","b4c52ef0":"markdown","fecba882":"markdown","ba7281da":"markdown","e87c522b":"markdown","0a23d0b7":"markdown","207f76f0":"markdown","fe66c2d0":"markdown","75c2fbe1":"markdown","f2fb30e2":"markdown","325f07ae":"markdown","3fa49297":"markdown","9917d752":"markdown"},"source":{"31c3eb4c":"from sklearn import preprocessing, metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"ticks\", color_codes=True)\nimport statsmodels.api as sm\nfrom scipy import stats","35a0e578":"dataset = pd.read_csv(\"..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv\")","0a2afbb1":"dataset","f3d37d39":"### Check data types of all columns\ndataset.dtypes","24b4c900":"\ndataset.isnull().sum()","e4b96cd9":"\ndataset.fillna({'reviews_per_month':0}, inplace=True)\ndataset.fillna({'name':\"NoName\"}, inplace=True)\ndataset.fillna({'host_name':\"NoName\"}, inplace=True)\ndataset.fillna({'last_review':\"NotReviewed\"}, inplace=True)\n","50577b54":"\ndataset.isnull().sum()","444295ec":"\ndataset[\"price\"].describe()\n","9cc7f0e9":"### See the distribution of price\nhist_price=dataset[\"price\"].hist()\n### We observe that most listings have price less than $1000","c98a5b13":"### Lets plot histogram for prices less than $2000\nhist_price1=dataset[\"price\"][dataset[\"price\"]<1000].hist()\n### This give a clearer picture!","839d6d2c":"\ndataset[dataset[\"price\"]>1000]\n","2fec7a60":"dataset=dataset[dataset[\"price\"]<1000]","58255da7":"### We see a more Gaussian distribution here\nhist_price2=dataset[\"price\"][dataset[\"price\"]<250].hist()\n","a0d1a5ba":"### We use 250 as threshold price \ndataset=dataset[dataset[\"price\"]<250]","f75e12a1":"### Looking at the price column again\ndataset[\"price\"].describe()\n","d027b3d4":"###There are 221 unique neighbourhoods in NYC as per this data set. Most listings are in Williamsburg\ndataset['neighbourhood'].value_counts()","88baea55":"### Count how many neighbourhoods appear more than 200\ndfnh =dataset.groupby(\"neighbourhood\").filter(lambda x: x['neighbourhood'].count() > 200)\n","5f1d0e5b":"### Most data is covered. \nlen(dfnh[\"neighbourhood\"])","8e598b80":"### Count how many neighbourhoods appear only once\ndfnh =dataset.groupby(\"neighbourhood\").filter(lambda x: x['neighbourhood'].count() == 1)\nlen(dfnh[\"neighbourhood\"])","5033f7f5":"###Lets look at neighbourhood groups\ndataset['neighbourhood_group'].value_counts()\n","9018c362":"### Lets see the average listing price by neighbourhood group\nng_price=dataset.groupby(\"neighbourhood_group\")[\"price\"].mean()","f33987a5":"### Manhattan is most expensive and Bronx is the least expensive place to live\nng_price","14e6aff2":"### Lets see the distributuion of price and neighbourhood group. \nplott=sns.catplot(x=\"neighbourhood_group\",y=\"price\",hue=\"room_type\", kind=\"swarm\", data=dataset)\nplott\n","f12efbfd":"### Checking if there are duplicate host_ids and whats is the maximum number of listings per host_id\ndf = dataset.groupby([\"host_id\"])\nmax(df.size())\n","91f0be23":"## Here we can see that 32K host_ids are unique appearing only once whereas some host_ids appear as much as 238 times\ndf.size().value_counts().head()","ea2e65ac":"df.size().value_counts().tail()","794d1261":"### Finding the host_id with maximum listings\nhost_id_counts = dataset[\"host_id\"].value_counts()\nmax_host = host_id_counts.idxmax()\nmax_host","90a8a037":"###We see that Sonder(NYC) has the max number of listings\ndataset[dataset[\"host_id\"]==219517861]","b482db89":"\ndataset = dataset.drop(columns = [\"id\",\"host_name\"])","c2694748":"### Let's Analyse the listing name column\ndataset[\"name_length\"]=dataset['name'].map(str).apply(len)\n","a8eba26f":"###Max and Min name length\nprint(dataset[\"name_length\"].max())\nprint(dataset[\"name_length\"].min())\nprint(dataset[\"name_length\"].idxmax())\nprint(dataset[\"name_length\"].idxmin())\n","3d7117dd":"### Max name \ndataset.at[25832, 'name']","92a4ab99":"###Min name\ndataset.at[4033, 'name']","3e9a297d":"### Let's figure if name length has an impact on how much it is noticed. We can assume higher number of reviews mean more people lived there and hence more people \"noticed\" the listing\n#dataset[\"name_length\"].corr(dataset[\"number_of_reviews\"])\ndataset.plot.scatter(x=\"name_length\", y =\"number_of_reviews\" )","f4aca94b":"###There is hardly any relationship there. Lets try between price and name length \ndataset[dataset[\"name_length\"]<50].plot.scatter(x=\"price\", y =\"name_length\")\n#dataset[\"name_length\"].corr(dataset[\"price\"])","7284dfcd":"dataset.name_length.hist()","f32e9eec":"### Lets look at room_type variable\ndataset['room_type'].value_counts()\n### Most listings are either Entire home or Private room","76b8f729":"### Average price per room_type\nrt_price = dataset.groupby(\"room_type\")[\"price\"].mean()","ba50e657":"### Entire room has the highest price and shared room has lowest avg price which makes sense.\nrt_price","ebc782c6":"### Analysing minimum nights\n\ndataset[\"minimum_nights\"].describe()","01aa064e":"### Analysing minimum nights\n### We see most values are between 1 to 100\nhist_mn=dataset[\"minimum_nights\"].hist()\nhist_mn","0b869326":"### Closer look\nhist_mn1=dataset[\"minimum_nights\"][dataset[\"minimum_nights\"]<10].hist()\nhist_mn1","9badf287":"dataset[\"minimum_nights\"][dataset[\"minimum_nights\"]>30]","81880973":"### We replace all records with min nights > 30 by 30\ndataset.loc[(dataset.minimum_nights >30),\"minimum_nights\"]=30\n","83d02afc":"hist_mn2=dataset[\"minimum_nights\"][dataset[\"minimum_nights\"]<30].hist()\nhist_mn2","6b6e6324":"### Does minimum_nights have impact on price?\ndataset[\"minimum_nights\"].corr(dataset[\"price\"])","b3e72539":"###Finally lets analyse availability_365 column\ndataset[\"availability_365\"].describe()","236f26a9":"hist_av=dataset[\"availability_365\"].hist()\nhist_av","3a397ec1":"### After analysis, I have decided to drop these columns as they will not be useful in prediction\ndataset.drop([\"name\",'last_review',\"latitude\",'longitude'], axis=1, inplace=True)","5bb351e4":"### Dropping host_id\ndataset.drop([\"host_id\"], axis=1, inplace=True)","6eaca40c":"### Plotting correlation matrix \ncorr = dataset.corr(method='pearson')\nplt.figure(figsize=(15,8))\nsns.heatmap(corr, annot=True)\ndataset.columns","ce5be341":"### Lets check out data one more time before beginning prediction. \n###Looks good!\ndataset.dtypes","7044fdf7":"## lets try without neighbourhood column\n\ndataset_onehot1 = pd.get_dummies(dataset, columns=['neighbourhood_group',\"room_type\"], prefix = ['ng',\"rt\"],drop_first=True)\ndataset_onehot1.drop([\"neighbourhood\"], axis=1, inplace=True)","e8f16ed3":"### Checking dataframe shape\ndataset_onehot1.shape","129ab4e9":"X1= dataset_onehot1.loc[:, dataset_onehot1.columns != 'price']","2e8ad779":"Y1 = dataset_onehot1[\"price\"]","f00b50d0":"\nx_train1, x_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size=0.20, random_state=42)","15b5759a":"### Fitting Linear regression\nreg1 = LinearRegression().fit(x_train1, y_train1)","4415efdc":"### R squared value\nreg1.score(x_train1, y_train1)","39aca0d3":"### Coefficients\nreg1.coef_","eedabea9":"### Predicting \ny_pred1 = reg1.predict(x_test1)","65f6d745":"Coeff1 = pd.DataFrame(columns=[\"Variable\",\"Coefficient\"])\nCoeff1[\"Variable\"]=x_train1.columns\nCoeff1[\"Coefficient\"]=reg1.coef_\nCoeff1.sort_values(\"Coefficient\")","88bef446":"### Calculate RMSE\nrmse1 = np.sqrt(metrics.mean_squared_error(y_test1, y_pred1))\nrmse1","ccc9f8a6":"### Taking a closer look at the estimates\nX2 = sm.add_constant(x_train1)\nest = sm.OLS(y_train1, X2)\nest2 = est.fit()\nprint(est2.summary())\n","4e089cf8":"## No of reviews and ng_Staten Island is not significant and does not help our model much. Drop it\nx_train1.drop([\"number_of_reviews\",\"ng_Staten Island\"], axis=1,inplace=True)\nX2 = sm.add_constant(x_train1)\nest = sm.OLS(y_train1, X2)\nest2 = est.fit()\nprint(est2.summary())\n### Does not improve our model much","b0d046de":"\ndataset_onehot2 = pd.get_dummies(dataset, columns=['neighbourhood_group',\"neighbourhood\",\"room_type\"], prefix = ['ng',\"nh\",\"rt\"],drop_first=True)\n","9d5a4995":"dataset_onehot2.shape","137c8382":"XL1= dataset_onehot2.loc[:, dataset_onehot2.columns != 'price']\nYL1 = dataset_onehot2[\"price\"]\nx_trainL11, x_testL11, y_trainL11, y_testL11 = train_test_split(XL1, YL1, test_size=0.20, random_state=42)","bf3ed2ca":"\nregL1 = Lasso(alpha=0.01)\nregL1.fit(x_trainL11, y_trainL11) ","60441613":"### R squared\n### This regularised model did way better than normal linear regression\nregL1.score(x_trainL11, y_trainL11)","d1515461":"### RMSE\n### Smaller value than earlier\ny_predL1= regL1.predict(x_testL11)\nprint(np.sqrt(metrics.mean_squared_error(y_testL11,y_predL1)))","6713805e":"### We can see that some parameters have zero coefficients.\nregL1.coef_","701e73bd":"CoeffLS1 = pd.DataFrame(columns=[\"Variable\",\"Coefficients\"])\nCoeffLS1[\"Variable\"]=x_trainL11.columns\nCoeffLS1[\"Coefficients\"]=regL1.coef_\nCoeffLS1.sort_values(\"Coefficients\", ascending = False)","67047ec7":"### Finally, lets try Random forest regressor which I believe will give best results","ce510971":"### Initially, lets build a tree without any constraints.\nregrRM = RandomForestRegressor(n_estimators=300)\nregrRM.fit(x_trainL11, y_trainL11)","c1317a46":"### We get R squared value at 93.6%! There is obviously a problem of overfitting:(\n\nprint(regrRM.score(x_trainL11, y_trainL11))\ny_predL1= regrRM.predict(x_testL11)\nprint(np.sqrt(metrics.mean_squared_error(y_testL11,y_predL1)))","4a85d766":"### Using feature importance, we can see which feature had most weight\nregrRM.feature_importances_","18cb0af1":"CoeffRM1 = pd.DataFrame(columns=[\"Variable\",\"FeatureImportance\"])\nCoeffRM1[\"Variable\"]=x_trainL11.columns\nCoeffRM1[\"FeatureImportance\"]=regrRM.feature_importances_\nCoeffRM1.sort_values(\"FeatureImportance\", ascending = False)","cc093c8b":"regrRM.get_params()","90f29611":"\nregrRM2 = RandomForestRegressor(n_estimators=200, max_depth = 50, min_samples_split = 5,min_samples_leaf =4)\nregrRM2.fit(x_trainL11, y_trainL11)","ffc72269":"### We get a smaller value for R squared\nprint(regrRM2.score(x_trainL11, y_trainL11))\ny_predL1= regrRM2.predict(x_testL11)\nprint(np.sqrt(metrics.mean_squared_error(y_testL11,y_predL1)))","37bd2c3f":"CoeffRM2 = pd.DataFrame(columns=[\"Variable\",\"FeatureImportance\"])\nCoeffRM2[\"Variable\"]=x_trainL11.columns\nCoeffRM2[\"FeatureImportance\"]=regrRM2.feature_importances_\nCoeffRM2.sort_values(\"FeatureImportance\", ascending = False)","dff92b04":"### To find best values for the RF parameters, let us use cross validation\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 6)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n# Create the random grid\nrm_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","f4f36e54":"print(rm_grid)","0f66460d":"import time","0964596b":"# Use the random grid to search for best hyperparameters\nt1 = time.time()\nrf2 = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \nrf2_random = RandomizedSearchCV(estimator = rf2, param_distributions = rm_grid, n_iter = 180, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf2_random.fit(x_trainL11, y_trainL11)\nt2 =time.time()","dc695ca7":"### Time taken\n(t2-t1)\/60\n","532f82f9":"### Here we can see Best parameters for the best model\nrf2_random.best_params_","6a0b4cfd":"### Final R squared value\nrf2_random.score(x_trainL11, y_trainL11)","972ec02e":"### We finally have the least RMSE among all model!\ny_predL1= rf2_random.predict(x_testL11)\nprint(np.sqrt(metrics.mean_squared_error(y_testL11,y_predL1)))","020356b8":"### Finally lets compare all models\n### Including models from my previous project with pyspark\nrmsedt = {\"Model\":[\"RF1_Sprk\",\"RF2_Sprk\",\"RF3_Sprk\",\"LR\",\"L1\",\"RFR\"],\"RMSE\":[71.55745125705758,65.7207885074504\n,62.51297007998151,37.68939882420686,35.12428625156702,34.05098593042094]}\nrmsedf = pd.DataFrame(rmsedt)\nrsqdt = {\"Model\":[\"LR\",\"L1\",\"RFR\"],\"RSquared\":[50.3,56.7,77.8]}\nrsqdt = pd.DataFrame(rsqdt)","85182fc1":"sns.catplot(x=\"Model\", y=\"RMSE\", linestyles=[\"-\"],\n            kind=\"point\", data=rmsedf);","34dd8452":"sns.catplot(x=\"Model\", y=\"RSquared\", linestyles=[\"--\"], color =\"green\", kind=\"point\", data=rsqdt);","8fb7f3c6":"# Model Interpretation\nNow that we have the results of our model, lets try to interpret it in detail.\n1. We first look at the Adjusted R square value since this is a Multiple linear regression. It tell us that our independent variables can explain 50.3% of variations in our dependent variable, which is price.\n2. The constant or the y intercept has a value of 109.56. This means that putting all other x variables at 0, an Entire Apt\/Home in Bronx will have a predicted price of 109.56. Remember when we created dummy variables we dropped one dummy from each column which we use as reference.\n3. Let's now look at the coefficients. The coefficient of ng_Manhattan is 44.09. We interpret as: Everything else being constant, an Entire Apt\/ Home in Manhattan will cost 44.09 more that same in Bronx. We can similarly interpret coefficient of minimum night as: With everything else being constant, with every one unit increase in minimum number of nights, the predicted price decreases by 0.8075.\n4. The std error is nothing but sample standard deviation for each variable. the t column shows the value of t statistic which is the z score of the sample variable. Z score tells us how far a sample is from its mean. A Z score of 2 tells us that the sample is two standard deviation away from the mean.\n5. P values suggests how significant these estimates are. Considering alpha of 0.05, we can reject the Null hypothesis for all variables except number_of_reviews, and ng_Staten Island. Alpha is the degree of error we are willing to accept. we have chosen alpha as 5% which is also most commonly used.\n6. The confidence intervals show the upper bound and lower bound for the TRUE POPULATION coefficient with 95% confidence.\n\n","c84cff45":"## CrossValidation","5a4085a9":"## Let us start with basic Linear Regression to create a base line model ","9a7858bb":"There are five major neighbourhood groups in NYC with Manhattan and Brooklyn accounting for 85% of the listings","2974901b":"### Lets see what we can do to prevent overfitting\n1. We will set max depth to 50. This ensures that branching stops after 50th branching, otherwise each sample may have its branch and overfit.\n2. We will use min_samples_split as 5. The default value is 2. This means that each internal node will split as long as it has a minimum of two sample. We dont want that!\n3. We will use min_samples_leaf as 4. The default is 1. This means that a node is considered leaf node if it has just one sample. This can cause severe overfitting!","0be09329":"We see that the average price is 107. Price varies between 0 to 249","b14903bc":"Time to check results","8f7fb076":"239 listings have price per day > 1000. These are either super lavish listings or there was an error during input. Nonetheless, since this records are skewing our data a lot, we will treat them as outliers and drop them.","18d5cf5c":"Making dummies for neighbourhood group and room_type","a09f0ec5":"Looking at the price column","20f28043":"Let's replace reviews per month Nan by zero and null name and host_name by NoName. Also, replace last review with \"Not reviewed\"","b4c52ef0":"![](https:\/\/ichef.bbci.co.uk\/news\/1024\/cpsprodpb\/3E5D\/production\/_109556951_airbnb.png)","fecba882":"Again, range is between 1 night to 1250 nights. Quite odd, lets investigate","ba7281da":"How many listings have price more than 1000?","e87c522b":"Listing id and Host name are not useful for our analysis so I will drop them","0a23d0b7":"We see that the average price is 152. Price varies between 0 to 10K","207f76f0":"### Splitting into training and testing data","fe66c2d0":"We will use Lasso regression because it has the ability to nullify parameters that do not improve the model.\nAlso, the dataset isn't large enough and hence Lasso is a good choice as it would add a little bit bias but reduce variance greatly.\nStarting with alpha=0.1\nWe can use crossvalidation and check for many values for alpha to find best one, but I will save this process for the next model\nwhich I think will procduce better results.","75c2fbe1":"Here we can note that Brooklyn and Manhattan tend to have more listings with price>150.\nWe also note thar most listings above price>100 are entire home type followed by private room and shared room which is the cheapest.","f2fb30e2":"# Loading the data","325f07ae":"### Lets try to use the neighbourhood variable. It has more than 200 distinct values. \n### Hence, when we create dummies we will have large number of variables.","3fa49297":"*In this notebook, I have analyzed Airbnb data for NYC. I preprocessed data as and when required. For some columns, I created dummies so as to run regression models. I started out with OLS and improved it with Lasso regularization. I got the least RMSE with Random forest regressor. I have explained all the steps I did as well as interpretation of the model results. Please comment if you have any questions or feedback!*\nIf you found this notebook useful, do leave an UPVOTE!","9917d752":"Checking null values.\nSince last_review and reviews per month have same null records, lets assume that that listing never got ant review"}}