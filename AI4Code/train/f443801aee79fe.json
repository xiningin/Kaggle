{"cell_type":{"12921ab2":"code","b55be266":"code","1848ad1b":"code","6da3c495":"code","fef7d26a":"code","9a2ed4aa":"code","79c903bb":"code","e6bb3fc0":"code","c719aea1":"code","7021c482":"code","5aef3e48":"code","160fe6ab":"code","51548e77":"code","f45b4277":"code","8df831c0":"code","9b59428d":"code","0956bb99":"code","48a1dc68":"code","02eae17c":"code","9282dacd":"code","53533f49":"code","e84ee7d0":"code","5ecb42b5":"code","4d8bdcdf":"code","4b0a04ef":"code","227f6110":"code","3fb2e538":"code","82aa60d3":"code","4fa86361":"code","0e4bdc4a":"code","7ca1913e":"code","d72d0812":"code","f01e3374":"code","e60b2f44":"code","a2e985a0":"code","6d410379":"code","136c71cf":"code","257dfa91":"code","f217d2ab":"code","7c907d72":"code","bfe4ff64":"code","b322c1ca":"code","6a960bdc":"code","741f8152":"code","a6ae739c":"code","1cb3eaed":"code","5824a662":"code","56c1e6e2":"code","1147a5ee":"code","bb239228":"code","e27e4172":"code","8f0780d3":"code","677acc62":"code","fed9da98":"code","71ea1f28":"code","054f358e":"code","23af7246":"code","f8f62561":"code","c1df04b6":"code","98e15a52":"code","61313cce":"code","f3b5015e":"code","c1447c05":"code","747ba551":"code","ece891e5":"code","976d242e":"code","4a8532f3":"code","6fd56ede":"code","0658c0f8":"code","f2539658":"code","64abb833":"code","0421db7b":"code","156d4eea":"code","fe25737a":"code","236cb90b":"code","99eed3fa":"code","fb1f8ff2":"code","a5cf960b":"code","a2d58557":"code","d08b7e01":"code","030c3509":"code","61a020b0":"code","c026d616":"code","71994ffc":"code","50c44536":"code","ec01554a":"code","b97c2c59":"code","2cf60506":"code","5871ecd8":"code","bbaf2b82":"code","11840b71":"code","6dda7f3f":"code","9cc44925":"code","f02acb23":"code","f0bcfd0a":"code","b3cfaca6":"code","bfd4eeb9":"code","1ba3c3a8":"code","3eb1b18f":"code","66b8f969":"code","e624b3ea":"code","27bd7aa5":"code","e3b77e06":"markdown","015b7403":"markdown","d2c0f67d":"markdown","2397a162":"markdown","466121ce":"markdown","b3dc6917":"markdown","b9fd7cb6":"markdown","610487a2":"markdown","db0bc012":"markdown","94a1119d":"markdown","a5776ef4":"markdown","815e3cd5":"markdown","109c95a9":"markdown","913c9a2f":"markdown","67e2e47c":"markdown","9b38e65c":"markdown","d1ae7fb3":"markdown","45643157":"markdown","6aad6240":"markdown","92efa375":"markdown","7375f2b1":"markdown","7f4d2900":"markdown","7c675608":"markdown","e70bc534":"markdown","c0872190":"markdown","c5e96b70":"markdown","4f07da0e":"markdown","a53568e5":"markdown","959d4564":"markdown","326f2067":"markdown","48100e41":"markdown","fc5b2daf":"markdown","48bcdfb0":"markdown","d3616155":"markdown","4d2b9042":"markdown","2356555f":"markdown","ecce9bf7":"markdown","554ff48a":"markdown","b52de23c":"markdown","fbde40a4":"markdown","94561ef9":"markdown","3cd20547":"markdown","e906c77b":"markdown","862d765e":"markdown","28fc935b":"markdown","6c003f10":"markdown","51faeb74":"markdown","2e31fda9":"markdown","c23ec34a":"markdown","c9116330":"markdown","f78522bd":"markdown","346f5a13":"markdown","6fa8891a":"markdown","10d239d9":"markdown","95a0e34d":"markdown","c5b409d7":"markdown","95e50201":"markdown","41ffba81":"markdown"},"source":{"12921ab2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b55be266":"#IMPORTING RELEVANT LIBRARIES\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.colors = True\nimport datetime as dt\n#import chart_studio.plotly as py\nimport plotly.graph_objects as go\n#import scorecardpy as sc\nfrom sklearn import preprocessing","1848ad1b":"#READING DATASET\ndata = pd.read_csv('\/kaggle\/input\/adult-census-income\/adult.csv')","6da3c495":"data.shape","fef7d26a":"data.head()","9a2ed4aa":"data.tail()","79c903bb":"data.info()","e6bb3fc0":"data.describe(include='all')","c719aea1":"#Replacing '?' with NaN\ndata = data.replace({'?':np.NaN})","7021c482":"#Checking for nulls\ndata.isnull().sum()\/data.shape[0]","5aef3e48":"#Replacing nulls with most frequent value\ndata[\"workclass\"]=data[\"workclass\"].fillna(data[\"workclass\"].mode()[0])\ndata[\"occupation\"]=data[\"occupation\"].fillna(data[\"occupation\"].mode()[0])\ndata[\"native.country\"]=data[\"native.country\"].fillna(data[\"native.country\"].mode()[0])","160fe6ab":"data.isnull().sum()\/data.shape[0]","51548e77":"c = data.income.value_counts()\nc\/c.sum()","f45b4277":"c = data.workclass.value_counts()\nc\/c.sum()","8df831c0":"fig = plt.figure(figsize=(12,5))\nsns.countplot(data.workclass, hue=data.income)","9b59428d":"#Re-defining categories\n# data['workclass'] = data.workclass.replace({'Self-emp-not-inc': 'Self-emp', \n#                                             'Self-emp-inc': 'Self-emp', \n#                                             'Local-gov': 'Gov', \n#                                             'State-gov':'Gov', \n#                                             'Federal-gov':'Gov',\n#                                             'Without-pay':'Unemployed',\n#                                             'Never-worked':'Unemployed',\n#                                              np.NaN:'Unknown'})","0956bb99":"# c = data.workclass.value_counts()\n# c\/c.sum()","48a1dc68":"#Checking occupation variable\nfig = plt.figure(figsize=(20,6))\nsns.countplot(data.occupation, hue=data.income)","02eae17c":"c = data.occupation.value_counts()\nc\/c.sum()","9282dacd":"# data['occupation'] = data.occupation.replace({'Handlers-cleaners': 'Others', \n#                                               'Farming-fishing': 'Others', \n#                                               'Tech-support': 'Others',\n#                                               'Protective-serv':'Others', \n#                                               'Priv-house-serv':'Others',\n#                                               'Armed-Forces':'Others'})","53533f49":"c = data.occupation.value_counts()\nc\/c.sum()","e84ee7d0":"#Checking native.country variable\nc=data['native.country'].value_counts()\nc\/c.sum()","5ecb42b5":"sns.boxplot(data.income,data.age, hue=data.sex)","4d8bdcdf":"plt.hist(data['age'])","4b0a04ef":"#transforming using log\ndata['age'] = np.log(data.age)\nplt.hist(data['age'])","227f6110":"c = data.sex.value_counts()\nc\/c.sum()","3fb2e538":"#Checking marital.status variable\nfig = plt.figure(figsize=(15,6))\nsns.countplot(data['marital.status'], hue=data.income)","82aa60d3":"c = data['marital.status'].value_counts()\nc\/c.sum()","4fa86361":"# data['marital.status'] = data['marital.status'].replace({'Never-married': 'Single',\n#                                                          'Widowed':'Single',\n#                                                          'Divorced': 'Divorced\/Separated',\n#                                                          'Separated':'Divorced\/Separated',\n#                                                          'Married-spouse-absent':'Divorced\/Separated',\n#                                                          'Married-AF-spouse':'Married',\n#                                                          'Married-civ-spouse':'Married'});","0e4bdc4a":"# c = data['marital.status'].value_counts()\n# c\/c.sum()","7ca1913e":"#Checking education variable\nfig = plt.figure(figsize=(15,6))\nsns.countplot(data['education'], hue=data.income)","d72d0812":"c = data['education'].value_counts()\nc\/c.sum()","f01e3374":"#Checking education variable\nfig = plt.figure(figsize=(15,6))\nsns.countplot(data['education.num'], hue=data.income)","e60b2f44":"c = data[['education','education.num']].value_counts()\nc\/c.sum()","a2e985a0":"# data['education'] = data['education'].replace({'Preschool': 'Below Matric',\n#                                                '1st-4th': 'Below Matric',\n#                                                '5th-6th':'Below Matric',\n#                                                '7th-8th': 'Below Matric',\n#                                                '9th':'Below Matric',\n#                                                '10th':'Matric',\n#                                                '11th':'Matric',\n#                                                '12th':'Matric',\n#                                                'HS-grad':'UnderGrad',\n#                                                'Some-college':'UnderGrad',\n#                                                'Assoc-voc':'UnderGrad',\n#                                                'Assoc-acdm':'UnderGrad',\n#                                                'Bachelors':'UnderGrad',\n#                                                'Masters':'PostGrad',\n#                                                'Prof-school':'PostGrad',\n#                                                'Doctorate':'PostGrad'\n#                                                });","6d410379":"# c = data['education'].value_counts()\n# c\/c.sum()","136c71cf":"#Checking race variable\nfig = plt.figure(figsize=(10,6))\nsns.countplot(data['race'], hue=data.income)","257dfa91":"c = data['race'].value_counts()\nc\/c.sum()","f217d2ab":"#Checking hours per week by income variable\nfig = plt.figure(figsize=(20,6))\nsns.countplot(data['hours.per.week'], hue=data.income)","7c907d72":"sns.boxplot(data['income'],data['hours.per.week'])","bfe4ff64":"sns.distplot(data['hours.per.week'])","b322c1ca":"sns.boxplot(data['hours.per.week'])","6a960bdc":"upper_fence = plt.boxplot(data['hours.per.week'])['caps'][1].get_data()[1][1]\nprint('Upper fence is at',upper_fence)\nprint('Number of outliers:',len(data[data['hours.per.week']>upper_fence]))","741f8152":"data.drop(data[data['hours.per.week']>upper_fence].index, inplace = True)","a6ae739c":"sns.boxplot(data['hours.per.week'])","1cb3eaed":"sns.distplot(data['hours.per.week'])","5824a662":"from scipy import stats\ndata['hours.per.week']= np.where((data['hours.per.week']<=0),1,data['hours.per.week'])\nfitted_data, fitted_lambda = stats.boxcox(data['hours.per.week'])\nprint(fitted_lambda)","56c1e6e2":"plt.hist(fitted_data, color ='g', edgecolor = 'black', bins=10)","1147a5ee":"data['hours.per.week'] = stats.boxcox(data['hours.per.week'])[0]","bb239228":"#Relationship metric\nfig = plt.figure(figsize=(10,6))\nsns.countplot(data['relationship'], hue=data.income)","e27e4172":"c = data[['relationship']].value_counts()\nc\/c.sum()","8f0780d3":"fig = plt.figure(figsize=(10,6))\nsns.boxplot(data.income,data['fnlwgt'])","677acc62":"sns.boxplot(data['fnlwgt'])","fed9da98":"upper_fence = plt.boxplot(data['fnlwgt'])['caps'][1].get_data()[1][1]\nprint('Upper fence is at',upper_fence)\nprint('Number of outliers:',len(data[data['fnlwgt']>upper_fence]))","71ea1f28":"data.drop(data[data['fnlwgt']>upper_fence].index, inplace = True)","054f358e":"sns.boxplot(data['fnlwgt'])","23af7246":"sns.histplot(data['fnlwgt'])","f8f62561":"from scipy import stats\ndata['fnlwgt']= np.where((data['fnlwgt']<=0),1,data['fnlwgt'])\nfitted_data, fitted_lambda = stats.boxcox(data['fnlwgt'])\nprint(fitted_lambda)","c1df04b6":"plt.hist(fitted_data, color ='g', edgecolor = 'black', bins=10)","98e15a52":"data['fnlwgt'] = stats.boxcox(data['fnlwgt'])[0]","61313cce":"sns.histplot(data['fnlwgt'])","f3b5015e":"data.shape","c1447c05":"#capital gain\nfig = plt.figure(figsize=(10,6))\nsns.boxplot(data.income,data['capital.gain'])","747ba551":"from scipy import stats\ndata['capital.gain']= np.where((data['capital.gain']<=0),1,data['capital.gain'])\nfitted_data, fitted_lambda = stats.boxcox(data['capital.gain'])\nprint(fitted_lambda)","ece891e5":"plt.hist(fitted_data, color ='g', edgecolor = 'black', bins=10)","976d242e":"data['capital.gain'] = stats.boxcox(data['capital.gain'])[0]","4a8532f3":"#capital loss\nfig = plt.figure(figsize=(10,6))\nsns.boxplot(data.income,data['capital.loss'])","6fd56ede":"from scipy import stats\ndata['capital.loss']= np.where((data['capital.loss']<=0),1,data['capital.loss'])\nfitted_data, fitted_lambda = stats.boxcox(data['capital.loss'])\nprint(fitted_lambda)","0658c0f8":"plt.hist(fitted_data, color ='g', edgecolor = 'black', bins=10)","f2539658":"data['capital.loss'] = stats.boxcox(data['capital.loss'])[0]","64abb833":"data['target']=data['income'].apply(lambda x:1 if x =='<=50K' else 0)","0421db7b":"data=data.drop('income',axis=1)","156d4eea":"data.columns","fe25737a":"sns.heatmap(data.corr(), annot=True)","236cb90b":"# import packages\nimport pandas as pd\nimport numpy as np\nimport pandas.core.algorithms as algos\nfrom pandas import Series\nimport scipy.stats.stats as stats\nimport re\nimport traceback\nimport string\n\nmax_bin = 20\nforce_bin = 3\n\n# define a binning function\ndef mono_bin(Y, X, n = max_bin):\n    \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]\n    r = 0\n    while np.abs(r) < 1:\n        try:\n            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n            d2 = d1.groupby('Bucket', as_index=True)\n            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n            n = n - 1 \n        except Exception as e:\n            n = n - 1\n\n    if len(d2) == 1:\n        n = force_bin         \n        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n        if len(np.unique(bins)) == 2:\n            bins = np.insert(bins, 0, 1)\n            bins[1] = bins[1]-(bins[1]\/2)\n        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n        d2 = d1.groupby('Bucket', as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"MIN_VALUE\"] = d2.min().X\n    d3[\"MAX_VALUE\"] = d2.max().X\n    d3[\"COUNT\"] = d2.count().Y\n    d3[\"EVENT\"] = d2.sum().Y\n    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n    d3=d3.reset_index(drop=True)\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    \n    return(d3)\n\ndef char_bin(Y, X):\n        \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]    \n    df2 = notmiss.groupby('X',as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"COUNT\"] = df2.count().Y\n    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n    d3[\"EVENT\"] = df2.sum().Y\n    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    d3 = d3.reset_index(drop=True)\n    \n    return(d3)\n\ndef data_vars(df1, target):\n    \n    stack = traceback.extract_stack()\n    filename, lineno, function_name, code = stack[-2]\n    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n    \n    x = df1.dtypes.index\n    count = -1\n    \n    for i in x:\n        if i.upper() not in (final.upper()):\n            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n                conv = mono_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i\n                count = count + 1\n            else:\n                conv = char_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i            \n                count = count + 1\n                \n            if count == 0:\n                iv_df = conv\n            else:\n                iv_df = iv_df.append(conv,ignore_index=True)\n    \n    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n    iv = iv.reset_index()\n    return(iv_df,iv)","99eed3fa":"final_iv, IV = data_vars(data,data.target)","fb1f8ff2":"final_iv.head()","a5cf960b":"IV.sort_values('IV',ascending=False)","a2d58557":"transform_vars_list = data.columns.difference(['target'])\ntransform_prefix = 'new_' # leave this value blank if you need replace the original column values","d08b7e01":"transform_vars_list","030c3509":"for var in transform_vars_list:\n    small_df = final_iv[final_iv['VAR_NAME'] == var]\n    transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n    replace_cmd = ''\n    replace_cmd1 = ''\n    for i in sorted(transform_dict.items()):\n        replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n        replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n    replace_cmd = replace_cmd + '0'\n    replace_cmd1 = replace_cmd1 + '0'\n    if replace_cmd != '0':\n        try:\n            data[transform_prefix + var] = data[var].apply(lambda x: eval(replace_cmd))\n        except:\n            data[transform_prefix + var] = data[var].apply(lambda x: eval(replace_cmd1))","61a020b0":"data['new_age'].value_counts()","c026d616":"data[['age','new_age']].head()","71994ffc":"data[['occupation','new_occupation']].head()","50c44536":"data.columns","ec01554a":"new_var_list=['new_age', \n       'new_education', 'new_education.num',\n       'new_hours.per.week', 'new_marital.status', 'new_native.country',\n       'new_occupation', 'new_race', 'new_relationship', 'new_sex',\n       'new_workclass']","b97c2c59":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size=0.3, stratify = data['target'])","2cf60506":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(train[new_var_list],train['target'])\nprint(lr.coef_)\nprint(lr.intercept_)","5871ecd8":"train_pred = lr.predict_proba(train[new_var_list])[:,1]\ntest_pred = lr.predict_proba(test[new_var_list])[:,1]","bbaf2b82":"np.round(test_pred[:10],3)","11840b71":"model_score = lr.score(train[new_var_list], train['target'])\nprint(model_score)","6dda7f3f":"model_score = lr.score(test[new_var_list], test['target'])\nprint(model_score)","9cc44925":"from sklearn import metrics\ncm =metrics.confusion_matrix(test['target'], np.round(test_pred[:]), labels = [0,1])\ndf_cm = pd.DataFrame(cm, index = [i for i in [0,1]], \n                    columns = [i for i in [\"Pred0\",\"Pred1\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","f02acb23":"#Classification report for train data\nfrom sklearn.metrics import classification_report\nprint(classification_report(train['target'], np.round(train_pred[:])))","f0bcfd0a":"#Classification report for test data\nfrom sklearn.metrics import classification_report\nprint(classification_report(test['target'], np.round(test_pred[:])))","b3cfaca6":"from sklearn.metrics import recall_score\nrecall_nb = recall_score(train['target'], np.round(train_pred[:]))\nrecall_nb","bfd4eeb9":"from sklearn.metrics import recall_score\nrecall_nb = recall_score(test['target'], np.round(test_pred[:]))\nrecall_nb","1ba3c3a8":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(train['target'], np.round(train_pred[:]))\naverage_precision","3eb1b18f":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(test['target'], np.round(test_pred[:]))\naverage_precision","66b8f969":"from sklearn.metrics import plot_precision_recall_curve\ndisp = plot_precision_recall_curve(lr, test[new_var_list], test['target'])\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))","e624b3ea":"from sklearn.metrics import f1_score\nprint('F1 Score: %.3f' % f1_score(test['target'], np.round(test_pred[:])))","27bd7aa5":"from sklearn.metrics import roc_curve\nprint(roc_curve(test['target'], np.round(test_pred[:])))\nfrom sklearn.metrics import plot_roc_curve\ndisp = plot_roc_curve(lr, test[new_var_list], test['target'])\ndisp.ax_.set_title('2-class ROC curve: '\n                   'AP={0:0.2f}'.format(average_precision))","e3b77e06":"### Accuracy","015b7403":"1. Data was imported and its shape was found to be (32561,15)\n2. EDA:\n    - Observed symbol \u2018?\u2019 In the data. Was replaced with np.NaN\n    - Nulls were observed in workclass, occupation, native.country. These were treated by replacing with highest occurring level in the feature\n    - We explored each feature:\n        - Workclass, Occupation, Marital Status and Education has several levels with less than 5% data. Hence will have to use binning techniques\n        - Income by Age and Gender: Median age of females is slightly lesser than that of men. Age outliers are present for both genders. Majority of the adults are in the range 35-45 years of age. <=50K seems more right skewed than >50K\n        - Native country: Data is predominantly from USA\n        - Male to female ratio is 3:1\n        - Hours per week and fnlwgt : Both have too many outliers and were treated by dropping outliers greater than upper fence. Since the features did not follow normal distribution, we applied box-cox transformation\n        - Capital loss and capital gain both were transformed  using boxcox\n    - Correlation matrix: no major correlations could be seen between continuous variables\n\n3. Weight of Evidence and IV\n    - Insignificant vars are: fnlwgt, capital.gain, capital.loss since IV is less than 0.03 for these\n    - Mildly predictive vars are: native.country and race since 0.03 <= IV < 0.1\n    - Highly predictive vars are: workclass, hours.per.week, sex, education, occupation, education.num, age, marital.status, relationship\n    \n4. Feature elimination - we remove variables that do not have predictive power\n\n5. We split data into train-test datasets. Since data is is slightly imbalance (76:24 ratio), we ensure to stratify by dependent var\n\n6. We build Logistic Regression model using sklearn.linear_model\n    - We predict on train and test data to further evaluate\n    \n7. Model evaluation:\n\n    - Accuracy of the model on test data is 84%\n    - Confusion matrix and classification report were displayed\n    - we see that precision for class 1 is .87 and recall is .93\n    - We have plotted the precision recall curve as well as ROC\n    \n   \n        \n        \n        \n\n        ","d2c0f67d":"### Gender","2397a162":"Data is less than 5% for several levels","466121ce":"## Importing Libraries","b3dc6917":"## EDA, Univariate, Bivariate Analysis, Outlier detection, treatment and transformation","b9fd7cb6":"Observation:\n\n1. <=50K earners are highest across Never married and Married-civ-spouse\n2. 50K+ earners are highest across Married civ Spouse and low across others.","610487a2":"## WOE and IV","db0bc012":"outliers are too many with hours per week","94a1119d":"#### BoxCox transformation","a5776ef4":"### Education","815e3cd5":"Since age distribution is not normal, we transform using log function","109c95a9":"Observation:\n\nSince income buckets follow similar distribution across all levels and some marital status levels have less than 5% data, we may have to club some levels","913c9a2f":"### Hours per week","67e2e47c":"## Model Evaluation","9b38e65c":"### Logistic Regression model","d1ae7fb3":"### Native country","45643157":"### Capital loss","6aad6240":"### Relationship","92efa375":"Observe there are ? symbols in the dataset","7375f2b1":"### Precision-Recall curve","7f4d2900":"### Occupation by income","7c675608":"Converting dependant variable into numerical","e70bc534":"#### BoxCox transformation","c0872190":"distribution of income buckets is same across all races","c5e96b70":"## Reading dataset and basic checks","4f07da0e":"#### Outlier treatment","a53568e5":"Since the distribution is left skewed, we apply boxcox transformation","959d4564":"## Model development","326f2067":"### Feature importance","48100e41":"#### Outlier treatment","fc5b2daf":"### Feature elimination","48bcdfb0":"### fnlwgt","d3616155":"### Confusion Matrix","4d2b9042":"Observations:\n\n1. <=50K earners are highest in Prof-specialty, Adm-clerical and Craft repair\n2. 50K+ earners are highest in Prof specialty, Exec manager, Sales","2356555f":"### Income by age and gender","ecce9bf7":"## Correlation matrix","554ff48a":"Observations:\n1. There are levels with <5% data, hence may need to group some levels\n2. <=50K earners are highest within Private workclass\n3. 50K+ earners are higher than <=50K in only Self-emp-inc level. At all other levels, the trends are reverse.","b52de23c":"### F1-score","fbde40a4":"Observation:\n- Predominant race levels are White, Black","94561ef9":"No major correlations can be seen amongst continuous vars","3cd20547":"### Workclass by income","e906c77b":"### ROC ","862d765e":" Omitting the variables with no predictive power i.e. IV < 0.03","28fc935b":"Observation:\n- Insignificant vars are: fnlwgt, capital.gain, capital.loss since IV is less than 0.03 for these\n- Mildly predictive vars are: native.country and race since 0.03 <= IV < 0.1\n- Highly predictive vars are: workclass, hours.per.week, sex, education, occupation, education.num, age, marital.status, relationship","6c003f10":"Data is 91% of USA and not representative enough of other countries.","51faeb74":"### Splitting into train and test","2e31fda9":"### Precision","c23ec34a":"### Race","c9116330":"Observation: \n\n- Except for Masters, Prof school and Doctorate, other educations show that <=50K earners are more than 50K+ earners\n- Same pattern is followed by occupation.num","f78522bd":"Since distribution is not normal we apply box-cox","346f5a13":"Observation: \n\n- Male to female ratio is 3:1","6fa8891a":"## Summary","10d239d9":"Observation:\n- Levels have at least 5% data","95a0e34d":"### Capital gain","c5b409d7":"### Recall","95e50201":"Observations:\n- Median age of females is slightly lesser than that of men. Age outliers are present for both genders.\n- Majority of the adults are in the range 35-45 years of age\n- <=50K seems more right skewed than >50K","41ffba81":"### Marital Status"}}