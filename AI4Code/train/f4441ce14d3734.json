{"cell_type":{"f7c5c181":"code","de36252e":"code","972679b6":"code","df33548a":"code","7381cb74":"code","4c84da36":"code","f5a34543":"markdown","84371983":"markdown","dbc2c06a":"markdown","033bc22d":"markdown","cbf35f3f":"markdown","6475ffce":"markdown","fdeb999d":"markdown"},"source":{"f7c5c181":"import os\nimport pandas as pd\nimport matplotlib.pylab as plt\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nPATH = \"\/kaggle\/input\/song-popularity-prediction\/\"","de36252e":"df_train = pd.read_csv(os.path.join(PATH, \"train.csv\"))\ndf_test = pd.read_csv(os.path.join(PATH, \"test.csv\"))","972679b6":"df_train[\"isTest\"] = 0\ndf_test[\"isTest\"] = 1\n\ntt = pd.concat([df_train, df_test], axis=0)\ntt.drop([\"id\", \"song_popularity\"], axis=1, inplace=True)\ntt.reset_index(drop=True, inplace=True)","df33548a":"y_train = tt[\"isTest\"].copy()\nX_train = tt.drop([\"isTest\"], axis=1).copy()","7381cb74":"params = {\n    \"objective\": \"binary\",\n    \"random_state\": 42,\n    \"learning_rate\": 0.05,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"importance_type\": \"gain\"\n}","4c84da36":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (trn_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n    \n    X_train_cv, y_train_cv = X_train.loc[trn_idx], y_train[trn_idx]\n    X_val_cv, y_val_cv = X_train.loc[val_idx], y_train[val_idx]\n    \n    model = lgb.LGBMClassifier(**params)\n    model.fit(\n        X_train_cv, y_train_cv,\n        eval_set = (X_val_cv, y_val_cv),\n        eval_metric = \"auc\",\n        early_stopping_rounds = 25,\n        verbose=False\n    )\n    \n    preds_val_cv = model.predict_proba(X_val_cv)[:,1]\n    \n    print(f\"Fold {fold+1} AUC: {roc_auc_score(y_val_cv, preds_val_cv):.5f}\")","f5a34543":"We get AUC values close to 0.5 for all folds, which isn't better than randomness. This means that the model couldn't find a feature that helped him to separate the train class from the test class. \n\n**Conclusion**: Probably the distribution of the features is more or less the same in train and test set.\n\n**Note**: If we had got higher AUCs, we could have plotted the feature importances and suspect that the ones with more importance could be potential drifters.","84371983":"Now we create a new to label the train set as 0 and the test set as 1.","dbc2c06a":"## What is *covariate shift*?\n\nExtracted from this [blog post](https:\/\/www.analyticsvidhya.com\/blog\/2021\/10\/mlops-and-the-importance-of-data-drift-detection\/) of Analytics Vidhya:\n\n> Covariate shift is the change in the distribution of one or more of the independent variables or input variables of the dataset. This means that even though the relationship between feature and target variable remains unchanged, the distribution of the feature itself has changed. When statistical properties of this input data change, the same model which has been built before will not provide unbiased results. This leads to inaccurate predictions.\n\nIn Kaggle context, this applies to the train and test datasets. We train a model using certain features and make predictions for the test set. It's easy to realize that if the distribution of one or more features changes between these 2 datasets the quality of the predictions could worse.\n\nThere are examples of competitions where identifying drifting features were crucial. For example, in [this one](https:\/\/www.kaggle.com\/c\/LANL-Earthquake-Prediction\/discussion\/94390), the winner team carefully chose features making sure that they don't differ too much between train and test sets using the [Kolmogorov-Smirnov test](https:\/\/en.wikipedia.org\/wiki\/Kolmogorov%E2%80%93Smirnov_test).\n\nThe Analytics Vidhya post described in detail some methods used to recognize these kind of features. One that is used quite often in Kaggle is known as ***Adversarial validation***.\n\n### Adversarial Validation\nThe main idea of adversarial validation is this: if a binary classifier is capable to distinguish which rows of the data belong to the train set and which ones belong to the test set, then there is one or more features which exhibit different behaviour in train than in test. With that in mind, we can summarize the procedure as follows:\n\n* Drop the target column from the train set\n* Combine the train and test sets into one big dataset\n* Create a new target which is 0 for the train rows and 1 for the test rows\n* Perform binary classification with the new target\n* Look at the AUC score\n* Identify potential drifting features\n\n","033bc22d":"Perform crossvalidation with the new target.","cbf35f3f":"Let's apply this method for our Song Popularity Prediction dataset.","6475ffce":"![image](https:\/\/i.ibb.co\/Cmj5k3b\/adversarial.png)","fdeb999d":"First, read the data."}}