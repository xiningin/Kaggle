{"cell_type":{"55ec3967":"code","1982d44b":"code","0dd18a70":"code","c8e99773":"code","0996a097":"code","40e44d94":"code","e1f90061":"code","b695e995":"code","752f6811":"code","d62cc8b8":"code","8a7f48c7":"code","c1e27efc":"code","09d36458":"code","08aae4cb":"code","f3a8236b":"code","e69ea690":"code","58b81d9a":"code","d1d0df3f":"code","0212679c":"code","4c2a17f7":"code","4b956b38":"code","16011dd0":"code","d4770ca8":"code","b0dbf751":"code","43e95313":"code","ee2198d3":"markdown","bed08b68":"markdown","e3d95917":"markdown","0dbfadae":"markdown","31671b87":"markdown","16ec641b":"markdown","942ceb21":"markdown","ebe7388c":"markdown","16259ac2":"markdown","2549765f":"markdown","6766a24b":"markdown","b873e022":"markdown","00292faa":"markdown","c3e81d31":"markdown","3b949c86":"markdown","ca8f788b":"markdown","a47351a1":"markdown","b7ef16a0":"markdown","002480e8":"markdown","156770dc":"markdown","5f1b5de7":"markdown","f8b1949b":"markdown","62c7150b":"markdown","4b38e562":"markdown","fc0b9c87":"markdown","f867c14c":"markdown","7c40cb05":"markdown","4772f8d9":"markdown","3eed94ac":"markdown","cbf9b61c":"markdown","dfa4ad07":"markdown","1f76c60c":"markdown","8fd77f49":"markdown"},"source":{"55ec3967":"!pip install faker\nfrom keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\nfrom keras.layers import RepeatVector, Dense, Activation, Lambda\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras.models import load_model, Model\nimport keras.backend as K\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nfrom faker import Faker\nimport random\nfrom tqdm import tqdm\nfrom babel.dates import format_date\nimport matplotlib.pyplot as plt\n%matplotlib inline","1982d44b":"from faker import Faker\nfake = Faker()\n\n# We need to seed these guys. For some reason I always use 101\nFaker.seed(101)\nrandom.seed(101)","0dd18a70":"FORMATS = ['short', # d\/M\/YY\n           'medium', # MMM d, YYY\n           'medium',\n           'medium',\n           'long', # MMMM dd, YYY\n           'long',\n           'long',\n           'long',\n           'long',\n           'full', # EEEE, MMM dd, YYY\n           'full',\n           'full',\n           'd MMM YYY', \n           'd MMMM YYY',\n           'd MMMM YYY',\n           'd MMMM YYY',\n           'd MMMM YYY',\n           'd MMMM YYY',\n           'dd\/MM\/YYY',\n           'EE d, MMM YYY',\n           'EEEE d, MMMM YYY']","c8e99773":"for format in FORMATS:\n    print('%s => %s' %(format, format_date(fake.date_object(), format=format, locale='en')))","0996a097":"def random_date():\n    dt = fake.date_object()\n\n    try:\n        date = format_date(dt, format=random.choice(FORMATS), locale='en')\n        human_readable = date.lower().replace(',', '')\n        machine_readable = dt.isoformat()\n\n    except AttributeError as e:\n        return None, None, None\n\n    return human_readable, machine_readable, dt\n","40e44d94":"def create_dataset(m):\n    human_vocab = set()\n    machine_vocab = set()\n    dataset = []\n    \n    for i in tqdm(range(m)):\n        h, m, _ = random_date()\n        if h is not None:\n            dataset.append((h, m))\n            human_vocab.update(tuple(h))\n            machine_vocab.update(tuple(m))\n    \n    # We also add two special chars, <unk> for unknown characters, and <pad> to add padding at the end\n    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], list(range(len(human_vocab) + 2))))\n    inv_machine = dict(enumerate(sorted(machine_vocab)))\n    machine = {v: k for k, v in inv_machine.items()}\n \n    return dataset, human, machine, inv_machine","e1f90061":"m = 30000\ndataset, human_vocab, machine_vocab, inv_machine_vocab = create_dataset(m)","b695e995":"dataset[:10]","752f6811":"human_vocab","d62cc8b8":"machine_vocab","8a7f48c7":"inv_machine_vocab","c1e27efc":"def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n    X, Y = zip(*dataset)\n    \n    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n    \n    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n\n    return X, np.array(Y), Xoh, Yoh","09d36458":"def string_to_int(string, length, vocab):\n    string = string.lower()\n    string = string.replace(',','')\n    \n    if len(string) > length:\n        string = string[:length]\n        \n    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n    \n    if len(string) < length:\n        rep += [vocab['<pad>']] * (length - len(string))\n    \n    return rep","08aae4cb":"string_to_int('April 8th, 2000', 30, human_vocab)","f3a8236b":"Tx = 30\nTy = 10\nX, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n\nprint(\"X.shape:\", X.shape)\nprint(\"Y.shape:\", Y.shape)\nprint(\"Xoh.shape:\", Xoh.shape)\nprint(\"Yoh.shape:\", Yoh.shape)","e69ea690":"index = 0\nprint(\"Source date:\", dataset[index][0])\nprint(\"Target date:\", dataset[index][1])\nprint()\nprint(\"Source after preprocessing (indices):\", X[index])\nprint(\"Target after preprocessing (indices):\", Y[index])\nprint()\nprint(\"Source after preprocessing (one-hot):\", Xoh[index])\nprint(\"Target after preprocessing (one-hot):\", Yoh[index])","58b81d9a":"repeator = RepeatVector(Tx)\nconcatenator = Concatenate(axis=-1)\ndensor1 = Dense(10, activation = \"tanh\")\ndensor2 = Dense(1, activation = \"relu\")\nactivator = Activation('softmax', name='attention_weights')\ndotor = Dot(axes = 1)","d1d0df3f":"def one_step_attention(a, s_prev):\n    s_prev = repeator(s_prev)\n    concat = concatenator([a, s_prev])\n    e = densor1(concat)\n    energies = densor2(e)\n    alphas = activator(energies)\n    context = dotor([alphas, a])\n    \n    return context","0212679c":"n_a = 32\nn_s = 64\npost_activation_LSTM_cell = LSTM(n_s, return_state = True)\noutput_layer = Dense(len(machine_vocab), activation='softmax')","4c2a17f7":"def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n    X = Input(shape=(Tx, human_vocab_size))\n    s0 = Input(shape=(n_s,), name='s0')\n    c0 = Input(shape=(n_s,), name='c0')\n    s = s0\n    c = c0\n    \n    outputs = []\n    \n    a = Bidirectional(LSTM(n_a, return_sequences = True))(X)\n    \n    for t in range(Ty):\n        context = one_step_attention(a, s)\n        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n        out = output_layer(s)\n        outputs.append(out)\n    \n    model = Model([X, s0, c0], outputs)\n    return model","4b956b38":"mod = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))","16011dd0":"mod.summary()","d4770ca8":"opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\nmod.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","b0dbf751":"s0 = np.zeros((m, n_s))\nc0 = np.zeros((m, n_s))\noutputs = list(Yoh.swapaxes(0,1))\nmod.fit([Xoh, s0, c0], outputs, epochs=20, batch_size=100)","43e95313":"\nExample_dates = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\nfor example in Example_dates:\n    \n    source = string_to_int(example, Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n    source = source.reshape((1, ) + source.shape)\n    prediction = mod.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    \n    print(\"source:\", example)\n    print(\"output:\", ''.join(output)) ","ee2198d3":"**If you like this Notebook, please upvote and keep me motivated**","bed08b68":"# Importing Necessary Packages","e3d95917":"Let;s see how its working!","0dbfadae":"Let's generate a dataset with 30k samples. That's probably way too much, but it should do a good job","31671b87":"As referenced above, unlike traditional methods of machine translation that involve separately engineered components, NMT works cohesively to maximize its performance. Additionally, NMT employs the use of vector representations for words and internal state. This means that words are transcribed into a vector defined by a unique magnitude and direction. Compared to phrase-based models, this framework is much simpler. Rather than separate component like the language model and translation model, NMT uses a single sequence model that produces one word at a time.\n\n","16ec641b":"Neural Machine Translation is a machine translation approach that applies a large artificial neural network toward predicting the likelihood of a sequence of words, often in the form of whole sentences. Unlike statistical machine translation, which consumes more memory and time, neural machine translation, NMT, trains its parts end-to-end to maximize performance. NMT systems are quickly moving to the forefront of machine translation, recently outcompeting traditional forms of translation systems.","942ceb21":"![image.png](attachment:image.png)","ebe7388c":"Let's have a look at our machine readable vocabulary:","16259ac2":"# Train The Model","2549765f":"# Generating Dataset","6766a24b":"Machine translation is the task of automatically converting source text in one language to text in another language. Given a sequence of text in a source language, there is no one single best translation of that text to another language. This is because of the natural ambiguity and flexibility of human language. This makes the challenge of automatic machine translation difficult, perhaps one of the most difficult in artificial intelligence:","b873e022":"We have converted Human Readable dates to Machine Readable dates Successfuly!","00292faa":"create_dataset(m) will generate our dataset, taking m as the number of samples to create. It returns the dataset as a list, two dictionaries mapping index to character (these are our vocabularies), human and machine, and the inverse mapping, inv_machine, chars to index:","c3e81d31":"In this above image, chineese language is convereted to English language.","3b949c86":"The NMT uses a bidirectional recurrent neural network, also called an encoder, to process a source sentence into vectors for a second recurrent neural network, called the decoder, to predict words in the target language. This process, while differing from phrase-based models in method, prove to be comparable in speed and accuracy.","ca8f788b":"# Applications of Neural Machine Translation","a47351a1":"Lets have a look at these formats","b7ef16a0":"random_date() will generate a random date using a random format picked from our list FORMATS defined before. It'll return a tuple with the human and machine readable date plus the date object","002480e8":"# Neural Machine Translation","156770dc":"Lets have a look at inverse of the machine readable vocabulary","5f1b5de7":"**Any other Machine Translation Projects needed or suggetions please leave a comment**","f8b1949b":"# How does Neural Machine Translation work?","62c7150b":"# Preprocessing","4b38e562":"**Hurray!**","fc0b9c87":"Let's have a look at our human readable vocabulary:","f867c14c":"string_to_int(string, length, vocab) will return a list of indexes based on a string and vocabulary, vocab, cropping or padding it depending on the max length passed in:","7c40cb05":"# Testing the Model","4772f8d9":"# STAY HOME, STAY SAFE and DO KAGGLE","3eed94ac":"![image.png](attachment:image.png)","cbf9b61c":"# Define Model","dfa4ad07":"**Here in this notbook, Let's convert Human Readable dates like '8th of April, 2000 ' to Machine Readable format ' 08-04-2000'** ","1f76c60c":"One of the most popular translation machines in the world is Google Translate. The system uses Google Neural Machine Translation to increase its fluency and accuracy. The system not only applies a large data set for training its algorithms, its end-to-end design allows the system to learn over time and create better, more natural translations. Google Neural Machine Translation can even process what are called \"zero-shot translations.\" For example, the translation from French to Spanish is a zero-shot translation because it is a direct translation. Previously, Google Translate would translate the initial language into English, and then translate that English to the target language.\n","8fd77f49":"Inspecting the first 10 entries. Remember it contains a list of tuples => (human readable, machine readable):"}}