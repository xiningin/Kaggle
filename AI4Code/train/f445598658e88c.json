{"cell_type":{"2cbadfe4":"code","5f67b27e":"code","1fef4a1d":"code","476315bc":"code","19fe4eb5":"code","42e64c75":"code","7fa8dbc9":"code","c82bbde5":"code","86fd1329":"code","fe6b9d06":"code","45f5b1d4":"code","11f23387":"code","6dc37c45":"code","c2e26d18":"code","a9b18d02":"code","b162fd6d":"code","4c5953bd":"code","a131f416":"code","8d91a28d":"code","0e4ac862":"code","ab531e1e":"code","981d191b":"code","42f1f144":"code","331fccb0":"code","ee0567bd":"code","0e6f6cc9":"code","71dd4a6a":"code","e2f6847b":"code","15af21e2":"code","14cafea9":"code","f9c432c8":"markdown","da7d9022":"markdown","4dbb06db":"markdown","1493abf7":"markdown","f603266d":"markdown","14719dc8":"markdown","a46c8f99":"markdown","4dae4e8f":"markdown","6d54a42b":"markdown","f4e6d9ed":"markdown","15ad460d":"markdown","be4b8144":"markdown","4d2d27ff":"markdown","84309e34":"markdown","70f2779e":"markdown","e216196e":"markdown","08895658":"markdown","d2ed71df":"markdown","535303bb":"markdown","0235726c":"markdown","0850ca21":"markdown","bacc3ca6":"markdown","e9b9a185":"markdown","13b92f3f":"markdown","bf2a04e2":"markdown","e3c0b3d3":"markdown"},"source":{"2cbadfe4":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn import preprocessing                            \nfrom sklearn.decomposition import PCA, FastICA\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom keras.models import Sequential \nfrom keras import models, utils,backend\nimport keras.utils \nfrom keras.optimizers import Adam   \nfrom keras import layers \nfrom keras.layers import Activation, Dense ,Dropout, BatchNormalization, Input,LeakyReLU\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping ,ModelCheckpoint,ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.models import model_from_json  \n\n\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5f67b27e":"trainf = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntestf = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\nsub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\ntargets = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntargets1 = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntrain = trainf[trainf.cp_type != 'ctl_vehicle'].drop(columns=['cp_type'])\ntest = testf[testf.cp_type != 'ctl_vehicle'].drop(columns=['cp_type'])\nAtrain = train\nAtest = test","1fef4a1d":"train.head()","476315bc":"test.head()","19fe4eb5":"# Lets see size and check for Nulls\nprint('Train dataset',train.shape)\nprint('Test dataset',test.shape)\nmissing_train=(train.isnull().sum()).sum()\nmissing_test=(test.isnull().sum()).sum()\nprint('Missing values in train set:',missing_train,'Missing values in test set:',missing_test)","42e64c75":"categ_feat_train = train.select_dtypes(include=['object'])\nnum_feat_train = train.select_dtypes(exclude=['object'])\nprint('Numerical features',len(num_feat_train.columns))\nprint('Categorical features',len(categ_feat_train.columns))\nprint(categ_feat_train.columns)","7fa8dbc9":"matplotlib.rc('figure', figsize=(20, 12))\nfig, ax =plt.subplots(2,3)\nsns.countplot(x='cp_type', data=trainf,ax=ax[0,0])\nax[0,0].set_title('TRAIN- Compound \/ control treatment', fontsize=14, weight='bold')\nsns.countplot(x='cp_dose', data=trainf,ax=ax[0,1])\nax[0,1].set_title('TRAIN- Doses  Low \/ High', fontsize=14,weight='bold')\nsns.countplot(x='cp_time', data=trainf,ax=ax[0,2])\nax[0,2].set_title('TRAIN- Treatment duration (hours)', fontsize=14,weight='bold')\nsns.countplot(x='cp_type', data=testf,ax=ax[1,0])\nax[1,0].set_title('TEST- Compound \/ control treatment', fontsize=14, weight='bold')\nsns.countplot(x='cp_dose', data=testf,ax=ax[1,1])\nax[1,1].set_title('TEST- Doses  Low \/ High', fontsize=14,weight='bold')\nsns.countplot(x='cp_time', data=testf,ax=ax[1,2])\nax[1,2].set_title('TEST- Treatment duration (hours)', fontsize=14,weight='bold')\nplt.show()","c82bbde5":"matplotlib.rc('figure', figsize=(20, 4))\nfig, ax =plt.subplots(1,4)\nfig.suptitle('Genes distributions', fontsize=16)\nsel_genes = [7,16,33,66]\ni=0\nfor item in sel_genes:\n    train.hist(column=['g-'+ str(item)], ax=ax[i])\n    i+=1\nplt.show()","86fd1329":"matplotlib.rc('figure', figsize=(20, 4))\nfig.suptitle('Genes distributions G-0 to G-771', fontsize=16)\nsel_genes = list(range(0, 771))\nfor item in sel_genes:\n    sns.kdeplot(data=train['g-'+ str(item)], shade=False,legend=False)\nplt.show()","fe6b9d06":"matplotlib.rc('figure', figsize=(20, 4))\nfig, ax =plt.subplots(1,4)\nfig.suptitle('Cells distributions', fontsize=16)\nsel_cells = [7,16,30,45]\ni=0\nfor item in sel_cells:\n    train.hist(column=['c-'+ str(item)], ax=ax[i])\n    i+=1\nplt.show()","45f5b1d4":"matplotlib.rc('figure', figsize=(20, 4))\nfig.suptitle('Cells distributions c-0 to c-771', fontsize=16)\nsel_cells = list(range(0, 99))\nfor item in sel_cells:\n    sns.kdeplot(data=train['c-'+ str(item)], shade=False,legend=False)\nplt.show()","11f23387":"Cells = [c for c in train.columns if \"c-\" in c]\nplt.figure(figsize=(10,6))\nsns.heatmap(train[Cells].corr(), cmap='viridis')\nplt.title('Cell viability correlations (Train set)', fontsize=14, weight='bold')\nplt.show()","6dc37c45":"targets.head()","c2e26d18":"targets.columns","a9b18d02":"target_classes1 = targets.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\ntarget_classes1.columns = ['Sig_Ids', 'activations']\ntarget_classes1 = target_classes1.groupby(['activations'])['Sig_Ids'].count().reset_index()\ntarget_classes2 = targets1.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\ntarget_classes2.columns = ['Sig_Ids', 'activations']\ntarget_classes2 = target_classes2.groupby(['activations'])['Sig_Ids'].count().reset_index()\nmatplotlib.rc('figure', figsize=(8, 5))\nfig, ax =plt.subplots(1,2)\nsns.barplot(x=\"activations\", y=\"Sig_Ids\", data=target_classes1, ax=ax[0]).set_title('Scored targets')\nsns.barplot(x=\"activations\", y=\"Sig_Ids\", data=target_classes2, ax=ax[1]).set_title('NonScored targets')\nplt.show()","b162fd6d":"#For Model1\n\ncombo = pd.concat([train, test], axis=0)\ncols=combo.columns.tolist()\ncatego = combo.iloc[:, 0:3]\ngenes_df =combo[cols[3:775]]\ncells_df = combo[cols[775:]]","4c5953bd":"from sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nthr = VarianceThreshold(0.99)  \n\n# Model1\nVT_ALL = thr.fit_transform(combo[cols[3:]])\n# Model2\ncells_df=thr.fit_transform(cells_df)\ngenes_df=thr.fit_transform(genes_df)\n","a131f416":"from sklearn.preprocessing import QuantileTransformer\n\nQUA = QuantileTransformer(n_quantiles=250, output_distribution='normal')\n\n#Model1\nVT_ALL = QUA.fit_transform(VT_ALL)\n#Model2\ncells_df = QUA.fit_transform(cells_df)\ngenes_df = QUA.fit_transform(genes_df)","8d91a28d":"#### Model1\nall_ica = FastICA(n_components=500,max_iter=500)\nVT_ALL_ica=all_ica.fit_transform(VT_ALL)\n\nVT_ALL_ica_df = pd.DataFrame(VT_ALL_ica , columns=[\"VTICA\" + str(i) for i in range(500)], index=combo.index)\nfull= combo.iloc[:, 1:]\n\nnew_combo= pd.concat([full,VT_ALL_ica_df], axis=1 )\ntrain = new_combo[ : train.shape[0]]\ntest = new_combo[-test.shape[0] : ]","0e4ac862":"##########################  model 2 ##########################\n\nc_ica = FastICA(n_components=50,max_iter=500)\ncells_ica=c_ica.fit_transform(cells_df)\ng_ica = FastICA(n_components=300,max_iter=500)\ngenes_ica=g_ica.fit_transform(genes_df)\n\ncells_ica_df = pd.DataFrame(cells_ica , columns=[\"C-ICA\" + str(i) for i in range(50)], index=combo.index)\ngenes_ica_df = pd.DataFrame(genes_ica , columns=[\"G-ICA\" + str(i) for i in range(300)], index=combo.index)\n\nfull= combo.iloc[:, 1:]\nnew_combo2= pd.concat([full,cells_ica_df,genes_ica_df], axis=1 )\ntrain2 = new_combo2[ : train.shape[0]]\ntest2 = new_combo2[-test.shape[0] : ]","ab531e1e":"train['cp_time'] = train['cp_time'].map( {24: 1, 48: 2, 72: 3} ).astype(int)\n#train = pd.get_dummies(train, columns = [\"cp_type\"], prefix=\"CPTP\",drop_first=True)\ntrain = pd.get_dummies(train, columns = [\"cp_dose\"], prefix=\"CPD\", drop_first=True)\n\ntest['cp_time'] = test['cp_time'].map( {24: 1, 48: 2, 72: 3} ).astype(int)\n#test = pd.get_dummies(test, columns = [\"cp_type\"], prefix=\"CPTP\",drop_first=True)\ntest = pd.get_dummies(test, columns = [\"cp_dose\"], prefix=\"CPD\",drop_first=True)","981d191b":"train2['cp_time'] = train2['cp_time'].map( {24: 1, 48: 2, 72: 3} ).astype(int)\n#train = pd.get_dummies(train, columns = [\"cp_type\"], prefix=\"CPTP\",drop_first=True)\ntrain2 = pd.get_dummies(train2, columns = [\"cp_dose\"], prefix=\"CPD\", drop_first=True)\n\ntest2['cp_time'] = test2['cp_time'].map( {24: 1, 48: 2, 72: 3} ).astype(int)\n#test = pd.get_dummies(test, columns = [\"cp_type\"], prefix=\"CPTP\",drop_first=True)\ntest2 = pd.get_dummies(test2, columns = [\"cp_dose\"], prefix=\"CPD\",drop_first=True)","42f1f144":"targets0 = targets.loc[Atrain.index]\ny = targets0.drop(['sig_id'], axis=1)\n\nX = train.values\nXtest = test.values\n\n#Model2\nX2 = train2.values\nXtest2 = test2.values","331fccb0":"def build_model():\n    keras.backend.clear_session()\n    model = models.Sequential()\n    n_cols = X.shape[1]\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(2048,activation='relu', input_shape=(n_cols,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(2048,activation='relu' ))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(206, activation='sigmoid'))\n    return model   ","ee0567bd":"## Model1  Smoothing 0.0001\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=1E-5)\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True)\n\nn_labels = y.shape[1]\nn_features = X.shape[1]\nn_train = X.shape[0]\nn_test = Xtest.shape[0]\n\n\n# Label smoothing\n\n# Since our score is heavily punished by very confident (very close to 0\/1) incorrect answers\n#as @Rahul suggests, labels will be smoothed to a small extent, and predictions are clipped. There\n# is a build in option for label smoothing in BinaryCrossentropy set to a small value\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,0.0001,0.9999)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n\nn_seeds = 6\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\nn_folds = 5\ny_pred1 = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nfor seed in seeds:\n    fold = 0\n    mltsplit = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in mltsplit.split(X,y.values):\n        X_train = X[train]\n        X_test  = X[test]\n        y_train = y.values[train]\n        y_test  = y.values[test]\n\n        model=build_model()\n        model.compile(optimizer=keras.optimizers.Adam(lr=1e-3, decay=1e-3 \/ 200), loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.0001), metrics=logloss)\n\n        hist = model.fit(X_train,y_train, batch_size=128, epochs=50,verbose=0,validation_data = (X_test,y_test),callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        model.save('LabelSmoothed_seed_'+str(seed)+'_fold_'+str(fold))\n        y_val = model.predict(X_test)\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))\/(n_folds*n_seeds)\n        y_pred1 += model.predict(Xtest)\/(n_folds*n_seeds)\n        print('seed=',seed,'fold=',fold)\n        fold += 1\nprint('OOF score is ',oof)\n\nplt.figure(figsize=(12,8))\nhist_train = np.zeros(40)\nhist_val = np.zeros(40)\nfor i in range(n_folds):\n    hist_train += np.array(hists[i].history['loss'][:40])\/n_folds\n    hist_val += np.array(hists[i].history['val_loss'][:40])\/n_folds\n\nplt.plot(hist_train)\nplt.plot(hist_val)\nplt.yscale('log')\nplt.yticks(ticks=[1,1E-1,1E-2])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","0e6f6cc9":"## Model2  No smoothing\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=10, mode='min', min_lr=1E-5)\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True)\n\nn_labels = y.shape[1]\nn_features = X.shape[1]\nn_train = X.shape[0]\nn_test = Xtest.shape[0]\n\nn_seeds = 6\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\nn_folds = 5\ny_pred2 = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nfor seed in seeds:\n    fold = 0\n    mltsplit = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in mltsplit.split(X,y.values):\n        X_train = X[train]\n        X_test  = X[test]\n        y_train = y.values[train]\n        y_test  = y.values[test]\n\n        model=build_model()\n        model.compile(optimizer=keras.optimizers.Adam(lr=1e-3, decay=1e-3 \/ 200), loss=tf.keras.losses.BinaryCrossentropy())\n\n        hist = model.fit(X_train,y_train, batch_size=128, epochs=50,verbose=0,validation_data = (X_test,y_test),callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        model.save('LabelSmoothed_seed_'+str(seed)+'_fold_'+str(fold))\n        y_val = model.predict(X_test)\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))\/(n_folds*n_seeds)\n        y_pred2 += model.predict(Xtest)\/(n_folds*n_seeds)\n        print('seed=',seed,'fold=',fold)\n        fold += 1\nprint('OOF score is ',oof)\n\nplt.figure(figsize=(12,8))\nhist_train = np.zeros(40)\nhist_val = np.zeros(40)\nfor i in range(n_folds):\n    hist_train += np.array(hists[i].history['loss'][:40])\/n_folds\n    hist_val += np.array(hists[i].history['val_loss'][:40])\/n_folds\n\nplt.plot(hist_train)\nplt.plot(hist_val)\nplt.yscale('log')\nplt.yticks(ticks=[1,1E-1,1E-2])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","71dd4a6a":"# model TYPE2 ICA 50 ICA 300\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=1E-5)\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True)\n\nn_labels = y.shape[1]\nn_features = X2.shape[1]\nn_train = X2.shape[0]\nn_test = Xtest2.shape[0]\n\nn_seeds = 6\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\nn_folds = 5\ny_pred3 = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nfor seed in seeds:\n    fold = 0\n    mltsplit = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in mltsplit.split(X2,y.values):\n        X_train = X2[train]\n        X_test  = X2[test]\n        y_train = y.values[train]\n        y_test  = y.values[test]\n\n        model=build_model()\n        model.compile(optimizer=keras.optimizers.Adam(lr=1e-3, decay=1e-3 \/ 200), loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=logloss)\n\n        hist = model.fit(X_train,y_train, batch_size=128, epochs=50,verbose=0,validation_data = (X_test,y_test),callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        model.save('LabelSmoothed_seed_'+str(seed)+'_fold_'+str(fold))\n        y_val = model.predict(X_test)\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))\/(n_folds*n_seeds)\n        y_pred3 += model.predict(Xtest2)\/(n_folds*n_seeds)\n        print('seed=',seed,'fold=',fold)\n        fold += 1\nprint('OOF score is ',oof)\n\nplt.figure(figsize=(12,8))\nhist_train = np.zeros(40)\nhist_val = np.zeros(40)\nfor i in range(n_folds):\n    hist_train += np.array(hists[i].history['loss'][:40])\/n_folds\n    hist_val += np.array(hists[i].history['val_loss'][:40])\/n_folds\n\nplt.plot(hist_train)\nplt.plot(hist_val)\nplt.yscale('log')\nplt.yticks(ticks=[1,1E-1,1E-2])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","e2f6847b":"## Model4  TYPE1 No smoothing\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=10, mode='min', min_lr=1E-5)\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=15, mode='min',restore_best_weights=True)\n\nn_labels = y.shape[1]\nn_features = X.shape[1]\nn_train = X.shape[0]\nn_test = Xtest.shape[0]\n\nn_seeds = 6\nnp.random.seed(1)\nseeds = np.random.randint(0,100,size=n_seeds)\n\nn_folds = 5\ny_pred4 = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nfor seed in seeds:\n    fold = 0\n    mltsplit = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in mltsplit.split(X,y.values):\n        X_train = X[train]\n        X_test  = X[test]\n        y_train = y.values[train]\n        y_test  = y.values[test]\n\n        model=build_model()\n        model.compile(optimizer=keras.optimizers.Adam(lr=1e-3, decay=1e-3 \/ 200), loss=tf.keras.losses.BinaryCrossentropy())\n\n        hist = model.fit(X_train,y_train, batch_size=128, epochs=50,verbose=0,validation_data = (X_test,y_test),callbacks=[reduce_lr, early_stopping])\n        hists.append(hist)\n        model.save('LabelSmoothed_seed_'+str(seed)+'_fold_'+str(fold))\n        y_val = model.predict(X_test)\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))\/(n_folds*n_seeds)\n        y_pred4 += model.predict(Xtest)\/(n_folds*n_seeds)\n        print('seed=',seed,'fold=',fold)\n        fold += 1\nprint('OOF score is ',oof)\n\nplt.figure(figsize=(12,8))\nhist_train = np.zeros(40)\nhist_val = np.zeros(40)\nfor i in range(n_folds):\n    hist_train += np.array(hists[i].history['loss'][:40])\/n_folds\n    hist_val += np.array(hists[i].history['val_loss'][:40])\/n_folds\n\nplt.plot(hist_train)\nplt.plot(hist_val)\nplt.yscale('log')\nplt.yticks(ticks=[1,1E-1,1E-2])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","15af21e2":"pred_df = pd.DataFrame(y_pred, index =[Atest.index]) \npred_df2 = pd.DataFrame(y_pred2, index =[Atest.index]) \npred_df3 = pd.DataFrame(y_pred3, index =[Atest.index]) \npred_df4 = pd.DataFrame(y_pred4, index =[Atest.index]) \nfinal_df1=pred_df.add(pred_df2)\nfinal_df2=final_df1.add(pred_df3)\nfinal_df3=final_df2.add(pred_df4)\nfinal=final_df3\/4","14cafea9":"sub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\ntarget_cols= targets.columns[1:207]\nsub[target_cols] = 0\nsub.loc[Atest.index,1:] = final.values\nsub.loc[Atest.index,1:] =pred_df.clip(0.001,0.999)\nsub.to_csv('submission.csv', index=False)","f9c432c8":"# <font color='blue'> Dimensionality reduction <\/font>\n\n## Using ICA to reduce dimensionality\n\nI examined both PCA and ICA to perform an initial reduction in the dimensionality of the input dataset while still preserving most of the important data structure. There is work presenting the use of ICA in dimensionality reduction, deconvolution, data pre-processing, meta-analysis, and others applied to different data types (transcriptome, methylome, proteome, single-cell data) like [this one](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6771121\/) and also references that ICA can be selected over PCA depending on the case. My best results were with ICA so I will use it in this kernel. Determining the optimal number of independent components is a difficult task and is selected by trials, [this article ](https:\/\/bmcgenomics.biomedcentral.com\/articles\/10.1186\/s12864-017-4112-9) can be very informative.\n<img src=\"https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6771121\/bin\/ijms-20-04414-g001.jpg\" width=\"600\">\n","da7d9022":"<a id=\"s4\"><\/a>\n## <font color='blue'> Targets <\/font>","4dbb06db":"### <font color='blue'> Loading datasets <\/font>","1493abf7":"The targets we have to predict are basically probabilities of activation for each of various proteins-targets. This is a multi-label problem since one sample can be classified to multiple targets or none. The main target types as we can see are activators, inhibitors, receptors agonists and antagonists, agents, stimulants. \n\n* Receptors are chemical structures, composed of protein, that receive and transduce signals that may be integrated into biological systems.[1] These signals are typically chemical messengers which bind to a receptor and cause some form of cellular\/tissue response, e.g. a change in the electrical activity of a cell.\n * Agonists are chemicals that bind to a receptors and activate them to produce a biological response.  \n * An antagonist blocks the action of the agonist, while an inverse agonist causes an action opposite to that of the agonist.\n* Activators : They are proteins that increase transcription of a gene or set of genes. Activators are considered to have positive control over gene expression, as they function to promote gene transcription and, in some cases, are required for the transcription of genes to occur. Most activators are DNA-binding proteins that bind to enhancers  \n* Inhibitors  : An enzyme inhibitor is a molecule that binds to an enzyme and decreases its activity\n   \n  \n\n","f603266d":"We will load two data sets, train and test. Train data set is used for model training and test data set will be the unseen data used to make predictions, lets see now how they look like.","14719dc8":"As we can see doses and treatment duration times are distributed equally but only 8% of the samples are treated with a control perturbation. Distributions are similar both in training and test sets ,probably indicating that they follow same experiment setup (since each combo of drug-timing-dose can be viewed as one independent experiment, so there are 6 independent realizations per drug, although some drugs have been profiled more than once). ","a46c8f99":"## <font color='blue'> Gene features  <\/font>\n \nThe role of genes is to encode proteins who dictate how a cell functions. So, genes expressed in a particular cell determine what that cell can do. We can see that genes expression values in our data sets show normal like distribution, with zero mean as random following plots show.","4dae4e8f":"There are two target files containing MOAs of interest, the first one (train_targets_scored.csv) contains features that are scored and an auxiliary one (train_targets_nonscored.csv) having elements that are not scored . Let's see what type of features they have","6d54a42b":"## <font color='blue'> REFERENCES <\/font>  \nThere are really many inspiring notebooks and discussions published for this contest covering various aspects, that helped me a lot in to build this kernel to mention a few:\n\n* https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/184005\n* https:\/\/www.kaggle.com\/isaienkov\/mechanisms-of-action-moa-prediction-eda\n* https:\/\/www.kaggle.com\/rahulsd91\/moa-label-smoothing\n* https:\/\/www.kaggle.com\/nayuts\/moa-pytorch-nn-pca-rankgauss\n\nThank you very much for your time reading this kernel. Please feel free to leave your comments and suggestions about how I can improve this work. And don't forget , if you found something that you liked or gave you an idea, do UPVOTE!\n        \n    ","f4e6d9ed":"There is a clear high correlation between cell viabilities that has to be examined. ","15ad460d":"# <font color='blue'>  Categorical Data <\/font>\n In the following part we shall convert all categorical features into dummy\/indicator variables","be4b8144":"<a id=\"s1\"><\/a>\n### <font color='blue'> Importing libraries <\/font>","4d2d27ff":"## <font color='blue'>  Quantile Transformation <\/font>\n\nIt might be helpful for our models to change distributions using QuantileTransformer, that provides non-linear transformations in which distances between marginal outliers and inliers are shrunk. StandardScaler and MinMaxScaler were also tested but since they are very sensitive to outliers, I didn't use them.","84309e34":"### <font color='blue'>  Cp_type, cp_time and cp_dose features <\/font>\nThere are three categorical features cp_type,cp_time and cp_dose. Let us see how their values are distributed in the training dataset. ","70f2779e":"## <font color='blue'> Multi-label Models <\/font>","e216196e":"## <font color='blue'>Genes correlation analysis <\/font>\n\nIt would probably be useful to examine if there is any pairwise correlation (suggesting  a biological relationship) between genes, such that changes in the expression levels of one gene correspond to changes in the expression level of another gene.\nThere are references though like [this ](https:\/\/www.frontiersin.org\/articles\/10.3389\/fmicb.2015.00650\/full) , suggesting that \u201c \u2026. computation of pairwise gene associations (correlation; mutual information) produces unexpectedly large variation in estimates of pairwise gene association\u2014regardless of the metric used, the organism under study, or the number and source of the samples probably due to sampling bias.\u201d And also \u201c\u2026.many individual genes show small differences in absolute gene expression levels across the set of samples. These small differences are due mainly to \u201cnoise\u201d instead of \u201csignal\u201d attributable to environmental or genetic perturbations. \nSo we will not examine corellations but we can keep that ICA as we shall see later could be used as a helpful denoising step even though it might be sightly biased towards highly expressed genes.","08895658":"So, there are 876 columns in train and test sets, three of them categorical, and there are no missing values. \n\n* sig_id is the unique primary key of the sample \n* Features with g- : are gene expression levels and there are 772 of them (from g-0 to g-771) \n* Features with c- : are cell viability measurements for each cell line, there are 100 of them (from c-0 to g-99) \n* Features with cp_:  \n  cp_type: samples are treated with a compound(trt_cp) or with a control perturbation (ctl_vehicle) that has no MoAs.   \n  cp_time: duration of the treatment (24, 48 or 72 hours)  \n  cp_dose: dosage of the treatment low\/high (D1\/D2)\n","d2ed71df":"### <font color='blue'> Cell correlation analysis <\/font>\nLet's explore how cells are correlated.","535303bb":"<img src=\"https:\/\/cdn.pixabay.com\/photo\/2016\/11\/30\/12\/17\/cells-1872666_960_720.jpg\" width=\"600\">\n<i> Image by <a href=\"https:\/\/pixabay.com\/users\/qimono-1962238\/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1872666\">Arek Socha<\/a> from <a href=\"https:\/\/pixabay.com\/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1872666\">Pixabay<\/a> <\/i>","0235726c":"## <font color='blue'> Cell features analysis <\/font>\n\nThese features are related to cell viability, high negative cell viability values reflect a high numbers of cell deaths and low values high survival rates. Their distributions are skewed with a normal-like look but with heavy negative tails and peaks at -10 probably due to experiment data normalization procedure. ","0850ca21":"We can see that numbers of activations differ for scored and non_scored datasets, but in any case more than 90% of the drugs activate zero or one of the target columns.","bacc3ca6":"## <font color='blue'> Experimenting with model parameters and pre\/post processing data <\/font>\n\nI tested various techniques and parameters during this work, some of them improved my LB significally.  \n\n1) Using cp_type column : predictions after removing this feature were less accurate.  \n2) Using Standard \/Minmax scaling but Normal Quantile Transform before ICA lead to better predictions  \n3) Label smoothing is definitely leading to improved scores due to the way of LB scoring system. Trying to use values larger than 0.001\/0.999 for min\/max lowered scores.  \n4) Adding weight normalization was tested too but without any success  \n5) Testing AdamW, LazyAdam optimizers, didn't improve results  \n6) Testing Leaky_relu ,elu no obvious improvement  \n7) Changing batch size from 128 up to 512 didn't change LB score (as expected)  \n8) Using larger models : more hidden layers and\/or nodes per layer up to 8192\/2048 showed small improvement (possibly due to some extra overfitting)  \n9) Tried batch normalization before the activation function, not suitable for our activations  \n10) Tested different values for FastICA independent components","e9b9a185":"<a id=\"s3\"><\/a>\n## <font color='blue'> Exploring the data<\/font>","13b92f3f":"## <font color='blue'> Removing uninformative features <\/font>\n\nThe fewer and more useful features we have, the better for our models. An interesting idea could be to try to find and remove any \"uninformative\" genes in our data sets like those ones with very low expression values in most samples or those whose expression values shows small variation throughout samples. Removing those feature could help our models perform better and faster. We have to remember though that what we see comes after z-score and data quantile normalization so we have to do some \"reverse enginineering\", but there is no way to get back to values before transformations since statistics like mean and std are lost. So we will use data as is and try VarianceThreshold to detect and remove low variance features (we have already scaled our data). Target variability was set to 90% with trial and error. ","bf2a04e2":"# <font color='blue'> Introduction  <\/font>\n\nThere has been a change in drug discovery procedure in the last years,to a more targeted model based on the understanding of the underlying biological mechanism of a disease. Scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. Mechanism-of-action (MoA) is a label that describes the biological activity of a given molecule. One approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs. For this competition, we will be predicting the probability that the sample(sig_id) had a positive response for each MoA target , given various inputs such as gene expression data and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cells\u2019 responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). Drugs can have multiple MoA annotations, so the task is formally a multi-label classification problem. This kernel has been influenced by great kernels posted by other fellow kagglers and is my first try in a research contest. Thank you all for sharing all this great content!","e3c0b3d3":"### <font color='blue'>     Missing Values<\/font>"}}