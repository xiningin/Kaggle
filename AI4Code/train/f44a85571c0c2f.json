{"cell_type":{"8d38d788":"code","782d3241":"code","a9e57ca8":"code","4cf31307":"code","b98bded4":"code","d1e0815c":"code","3e18610c":"code","1a9d00d3":"code","135f07ae":"code","f5355b0d":"code","007eb137":"code","969e06c1":"code","3df906d0":"code","0a4f46b0":"code","395ae3fd":"code","a267e707":"code","587f36be":"code","c3aba9cd":"code","f4be3023":"code","b6ecdbab":"code","6ce26379":"code","50441ebc":"code","874be971":"code","4905464b":"code","311af7a4":"code","5b14bd6c":"code","91dd1d13":"code","f526ad75":"code","21538f1f":"code","fcaef8ab":"code","3c9e240f":"code","2b019856":"code","cbd0bfff":"code","6051de4d":"code","0ce124ea":"code","cae2fd56":"code","1ec75267":"code","bd040805":"code","cd80c4b7":"code","f41cfc97":"code","0a596c81":"code","6c813873":"code","fde92415":"code","fd521024":"code","b1b09668":"code","be0b39f1":"code","9f83e8c4":"markdown","da3b4f97":"markdown","b64e8f80":"markdown","c9f53c78":"markdown","06c893f3":"markdown","e9e62415":"markdown"},"source":{"8d38d788":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","782d3241":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf","a9e57ca8":"df.info()","4cf31307":"df.isnull().sum()","b98bded4":"#Low variance filter\ndf['Age'].fillna(df['Age'].median(),inplace=True)\ndf['Cabin'].fillna(df['Cabin'].mode()[0], inplace=True)\ndf['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n","d1e0815c":"df.isnull().sum()","3e18610c":"df.corr()","1a9d00d3":"from sklearn.preprocessing import LabelEncoder\nlb=LabelEncoder()\ndf['Sex']=lb.fit_transform(df['Sex'])\ndf['Embarked']=lb.fit_transform(df['Embarked'])\ndf['Cabin']=lb.fit_transform(df['Cabin'])\n\n","135f07ae":"df=df.drop(['Name','Ticket','PassengerId'], axis=1)","f5355b0d":"df","007eb137":"X=df.drop(['Survived'],axis=1)\ny=df['Survived']","969e06c1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 0)","3df906d0":"X_train.shape,y_train.shape","0a4f46b0":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\nX_train","395ae3fd":"from sklearn.metrics import accuracy_score\ndef train_and_eval(model, trainX, trainY, testX, testY):\n\n    # training\n    _ = model.fit(trainX, trainY)\n\n    # predictions\n    y_preds_train = model.predict(trainX)\n    y_preds_test = model.predict(testX)\n\n    # evaluation\n    print()\n    print(model)\n    print(f\"Train accuracy score : {accuracy_score(y_train, y_preds_train)}\")\n    print(f\"Test accuracy score : {accuracy_score(y_test, y_preds_test)}\")\n    print('\\n',40*'-')","a267e707":"from sklearn.linear_model import LogisticRegression\n\nC = [0.001, 0.01, 0.1, 1, 5, 10]\n\nfor c in C: \n    # Define model\n    log_model = LogisticRegression(C=c, max_iter=500, random_state=1, n_jobs=-1, penalty='l2')\n    \n    # Train and evaluate model\n    train_and_eval(model=log_model,\n                   trainX=X_train,\n                   trainY=y_train,\n                   testX=X_test,\n                   testY=y_test)\n    ","587f36be":"# Retrieve the model parameters.\nb = log_model.intercept_[0]\nw = log_model.coef_.T\n# Calculate the intercept and gradient of the decision boundary.\nc = -b\/w[1]\nm = -w[0]\/w[1]\n\nprint(\"Slope: \" + str(m))\nprint(\"Intercept: \"+ str(c))","c3aba9cd":"from sklearn.svm import SVC\n\nsvc = SVC(kernel = 'linear')\ntrain_and_eval(model=svc,\n                   trainX=X_train,\n                   trainY=y_train,\n                   testX=X_test,\n                   testY=y_test)","f4be3023":"svc.coef_","b6ecdbab":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU, PReLU, ELU\nfrom keras.layers import Dropout\n\n#initialize the ANN\nclassifier = Sequential()\n\n# Add the input layer and the first hidden layer\nclassifier.add(Dense(units = 8, kernel_initializer = 'he_uniform',activation='relu',input_dim =8))\n\n# Add the second hidden layer\nclassifier.add(Dense(units = 256, kernel_initializer = 'he_uniform',activation='relu'))\n\n# Add the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n\n# Compile the ANN\nclassifier.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nmodel_history=classifier.fit(X_train, y_train, batch_size = 64, epochs = 100)\n\n# list all data in history\nclassifier.summary()\n#total params are total number of weights and biases ","6ce26379":"from keras.utils.vis_utils import plot_model\n\nplot_model(\n    classifier,\n    show_shapes=True, show_layer_names=True\n)","50441ebc":"# Predict the Test set results\nY_pred = classifier.predict(X_test)\nY_pred","874be971":"Y_pred = (Y_pred > 0.5)","4905464b":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, Y_pred)\nprint(cm)","311af7a4":"#Calculate the Accuracy\nfrom sklearn.metrics import accuracy_score\nscore=accuracy_score(Y_pred,y_test)\nprint((score)*100,\"%\")","5b14bd6c":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, Y_pred))","91dd1d13":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=200, random_state=0)\nrf.fit(X_train, y_train)\nY_pred_rf=rf.predict(X_test)\nprint(\"Random Forest accuracy\", accuracy_score(Y_pred_rf, y_test)*100,\"%\")","f526ad75":"from sklearn.tree import plot_tree\n\nfig = plt.figure(figsize=(15, 10))\nplot_tree(rf.estimators_[0],\n          filled=True, impurity=True, \n          rounded=True)\n\nplt.show()","21538f1f":"import matplotlib.pyplot as plt\nfeatures = df.columns\nimportances = rf.feature_importances_\nindices = np.argsort(importances)[-9:]\nplt.title('Feature importance')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.show()","fcaef8ab":"from sklearn.metrics import confusion_matrix\ncm_rf = confusion_matrix(y_test,Y_pred_rf)","3c9e240f":"import seaborn as sns\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})","2b019856":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(max_depth=3)\ndtc.fit(X_train, y_train)","cbd0bfff":"from sklearn import tree\n\nfig, ax = plt.subplots(figsize=(80, 80))\n\ntree.plot_tree(dtc, filled=True)\nplt.show()","6051de4d":"train_and_eval(model=dtc,\n                   trainX=X_train,\n                   trainY=y_train,\n                   testX=X_test,\n                   testY=y_test)","0ce124ea":"test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest.head()","cae2fd56":"test.isnull().sum()","1ec75267":"#Low variance filter\ntest['Age'].fillna(test['Age'].median(),inplace=True)\ntest['Cabin'].fillna(test['Cabin'].mode()[0], inplace=True)\ntest['Fare'].fillna(test['Fare'].median(),inplace=True)","bd040805":"test.isnull().sum()","cd80c4b7":"test","f41cfc97":"test=test.drop(['Name','Ticket'], axis=1)","0a596c81":"testmodel=test.drop(['PassengerId'],axis=1)\n\n","6c813873":"testmodel=test.drop(['PassengerId'],axis=1)\n\n","fde92415":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntestmodel = sc.fit_transform(testmodel)","fd521024":"y_pred = classifier.predict(testmodel)","b1b09668":"y_pred = (y_pred > 0.5).astype(int).reshape(testmodel.shape[0])\ny_pred[:], len(y_pred)\n","be0b39f1":"import csv\nSubmission = pd.DataFrame({ 'PassengerId': test['PassengerId'], 'Survived': y_pred})\nSubmission.to_csv(\"Titanic_Submission.csv\", index=False)\nSubmission.head()","9f83e8c4":"**Feature Importance**\n\nFeature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.\n\nThe scores are useful and can be used in a range of situations in a predictive modeling problem, such as:\n* Better understanding the data.\n* Better understanding a model.\n* Reducing the number of input features.","da3b4f97":"**Model Building**","b64e8f80":"**Label Encoding**\n\nIt refers to converting the labels into a numeric form so as to convert them into the machine-readable form.","c9f53c78":"**Importing the Random Forest Model**","06c893f3":"**Test Data**","e9e62415":"**Feature Selection**"}}