{"cell_type":{"7ab9002d":"code","25b183b4":"code","b6130224":"code","e8fef79a":"code","95465b95":"code","e6625af8":"code","2c631644":"code","6c2009ec":"code","0b4b3337":"code","c5f6d3e2":"code","420857f2":"code","396730f9":"code","c94f3fe9":"code","8397aced":"code","1aa9920e":"code","7f247b63":"code","898d19d9":"markdown","22a62539":"markdown","dabb511e":"markdown","3c16e349":"markdown","d2e81392":"markdown","1007e20b":"markdown","41e24ac3":"markdown","303d5255":"markdown","a4becbef":"markdown","5028fa69":"markdown","f08ddd01":"markdown","96f4c8ee":"markdown","924dccb4":"markdown","161d1ab6":"markdown"},"source":{"7ab9002d":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_colwidth', -1)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras.layers import Dense, Embedding, LSTM, Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nimport sqlite3\nfrom tqdm import tqdm\nimport re\nfrom bs4 import BeautifulSoup\n\nfrom sklearn.model_selection import train_test_split","25b183b4":"conn = sqlite3.connect('..\/input\/database.sqlite')\nfiltered_data = pd.read_sql_query(''' SELECT * FROM REVIEWS LIMIT 100000''', conn)\n\n# Give reviews with Score>3 a positive rating(1), and reviews with a score<3 a negative rating(0).\ndef partition(x):\n    if x < 3:\n        return 0\n    return 1\n\ndef findMinorClassPoints(df):\n    posCount = int(df[df['Score']==1].shape[0]);\n    negCount = int(df[df['Score']==0].shape[0]);\n    if negCount < posCount:\n        return negCount\n    return posCount\n\n#changing reviews with score less than 3 to be positive and vice-versa\nactualScore = filtered_data['Score']\npositiveNegative = actualScore.map(partition) \nfiltered_data['Score'] = positiveNegative\n\n#Performing Downsampling\n# samplingCount = findMinorClassPoints(filtered_data)\n# postive_df = filtered_data[filtered_data['Score'] == 1].sample(n=5000)\n# negative_df = filtered_data[filtered_data['Score'] == 0].sample(n=5000)\n\n# filtered_data = pd.concat([postive_df, negative_df])\n\nprint(\"Number of data points in our data\", filtered_data.shape)\nfiltered_data.head(3)","b6130224":"#Sorting data according to ProductId in ascending order\nsorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n\n#Deduplication of entries\nfinal=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal.shape\n\n#Removing the anamolies\nfinal=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n\n#Preprocessing\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\npreprocessed_reviews = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(final['Text'].values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    # sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n    preprocessed_reviews.append(sentance.strip())\n    \n## Similartly you can do preprocessing for review summary also.\ndef concatenateSummaryWithText(str1, str2):\n    return str1 + ' ' + str2\n\npreprocessed_summary = []\n# tqdm is for printing the status bar\nfor sentence in tqdm(final['Summary'].values):\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n    #sentence = BeautifulSoup(sentence, 'lxml').get_text()\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    # sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n    preprocessed_summary.append(sentence.strip())\n    \npreprocessed_reviews = list(map(concatenateSummaryWithText, preprocessed_reviews, preprocessed_summary))\nfinal['CleanedText'] = preprocessed_reviews\nfinal['CleanedText'] = final['CleanedText'].astype('str')","e8fef79a":"X = final['CleanedText']\ny = final['Score']","95465b95":"del final\ndel preprocessed_reviews\ndel preprocessed_summary\ndel sorted_data\ndel filtered_data","e6625af8":"X_t, X_test, y_t, y_test = train_test_split(X, y, test_size=0.20, stratify=y, shuffle=True)\nX_train, X_cv, y_train, y_cv = train_test_split(X_t, y_t, test_size=0.20, stratify=y_t, shuffle=True)\nprint(\"Shape of Input  - Train:\", X_train.shape)\nprint(\"Shape of Output - Train:\", y_train.shape)\nprint(\"Shape of Input  - CV   :\", X_cv.shape)\nprint(\"Shape of Output - CV   :\", y_cv.shape)\nprint(\"Shape of Input  - Test :\", X_test.shape)\nprint(\"Shape of Output - Test :\", y_test.shape)","2c631644":"tokenize = Tokenizer(num_words=5000)\ntokenize.fit_on_texts(X_train)\n\nX_train_new = tokenize.texts_to_sequences(X_train)\nX_cv_new = tokenize.texts_to_sequences(X_cv)\nX_test_new = tokenize.texts_to_sequences(X_test)\n\nprint(X_train_new[1])\nprint(len(X_train_new))","6c2009ec":"# truncate and\/or pad input sequences\nmax_review_length = 1000\nX_train_new = sequence.pad_sequences(X_train_new, maxlen=max_review_length)\nX_cv_new = sequence.pad_sequences(X_cv_new, maxlen=max_review_length)\nX_test_new = sequence.pad_sequences(X_test_new, maxlen=max_review_length)\n\nprint(X_train_new.shape)\nprint(X_train_new[1])","0b4b3337":"# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()\n    \nn_epochs = 5\nbatchsize = 512\n\nfinal_output = pd.DataFrame(columns=[\"Model\", \"Architecture\",\n                                     \"TRAIN_LOSS\", \"TEST_LOSS\", \"TRAIN_ACC\", \"TEST_ACC\"]);","c5f6d3e2":"# create the model\nembed_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(5000, embed_vector_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"***********************************************\")\nprint(\"Printing the Model Summary\")\nprint(model.summary())\nprint(\"***********************************************\")","420857f2":"m_hist = model.fit(X_train_new, y_train, epochs=n_epochs, \n                   batch_size=batchsize, verbose=1, validation_data=(X_cv_new, y_cv))\n\nscore = model.evaluate(X_test_new, y_test, batch_size=batchsize)\nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfinal_output = final_output.append({\"Model\": 1,\n                                    \"Architecture\": 'Embedding-LSTM-Sigmoid', \n                                    \"TRAIN_LOSS\": '{:.5f}'.format(m_hist.history[\"loss\"][n_epochs-1]),\n                                    \"TEST_LOSS\": '{:.5f}'.format(score[0]),\n                                    \"TRAIN_ACC\": '{:.5f}'.format(m_hist.history[\"acc\"][n_epochs-1]),\n                                    \"TEST_ACC\": '{:.5f}'.format(score[1])}, ignore_index=True)\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch')\nax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,n_epochs+1))\n\nvy = m_hist.history['val_loss']\nty = m_hist.history['loss']\nplt_dynamic(x, vy, ty, ax)","396730f9":"# create the model\nembed_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(5000, embed_vector_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dropout(rate=0.5))\nmodel.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\nmodel.add(Dropout(rate=0.5))\nmodel.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\nmodel.add(Dropout(rate=0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"***********************************************\")\nprint(\"Printing the Model Summary\")\nprint(model.summary())\nprint(\"***********************************************\")","c94f3fe9":"m_hist = model.fit(X_train_new, y_train, epochs=n_epochs, \n                   batch_size=batchsize, verbose=1, validation_data=(X_cv_new, y_cv))\n\nscore = model.evaluate(X_test_new, y_test, batch_size=batchsize)\nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfinal_output = final_output.append({\"Model\": 2,\n                                    \"Architecture\": 'Embedding-LSTM-Dropout-Dense(128-Relu)-Dropout-Dense(64-Relu)-Dropout-Sigmoid', \n                                    \"TRAIN_LOSS\": '{:.5f}'.format(m_hist.history[\"loss\"][n_epochs-1]),\n                                    \"TEST_LOSS\": '{:.5f}'.format(score[0]),\n                                    \"TRAIN_ACC\": '{:.5f}'.format(m_hist.history[\"acc\"][n_epochs-1]),\n                                    \"TEST_ACC\": '{:.5f}'.format(score[1])}, ignore_index=True)\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch')\nax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,n_epochs+1))\n\nvy = m_hist.history['val_loss']\nty = m_hist.history['loss']\nplt_dynamic(x, vy, ty, ax)","8397aced":"# create the model\nembed_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(5000, embed_vector_length, input_length=max_review_length))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"***********************************************\")\nprint(\"Printing the Model Summary\")\nprint(model.summary())\nprint(\"***********************************************\")","1aa9920e":"m_hist = model.fit(X_train_new, y_train, epochs=n_epochs, \n                   batch_size=batchsize, verbose=1, validation_data=(X_cv_new, y_cv))\n\nscore = model.evaluate(X_test_new, y_test, batch_size=batchsize)\nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfinal_output = final_output.append({\"Model\": 3,\n                                    \"Architecture\": 'Embedding-LSTM-LSTM-Sigmoid', \n                                    \"TRAIN_LOSS\": '{:.5f}'.format(m_hist.history[\"loss\"][n_epochs-1]),\n                                    \"TEST_LOSS\": '{:.5f}'.format(score[0]),\n                                    \"TRAIN_ACC\": '{:.5f}'.format(m_hist.history[\"acc\"][n_epochs-1]),\n                                    \"TEST_ACC\": '{:.5f}'.format(score[1])}, ignore_index=True)\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch')\nax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,n_epochs+1))\n\nvy = m_hist.history['val_loss']\nty = m_hist.history['loss']\nplt_dynamic(x, vy, ty, ax)","7f247b63":"final_output","898d19d9":"### Tokenizing the dataset","22a62539":"This is just to give batch input to the RNN","dabb511e":"### Splitting the data","3c16e349":"## Data Preprocessing","d2e81392":"## Loading the data","1007e20b":"### Model M1 ( Embedding -> LSTM -> Output(Sigmoid) ) ","41e24ac3":"### Model M3 ( Embedding -> LSTM -> LSTM -> Output(Sigmoid) ) ","303d5255":"Here,\n\nThe dataset which we used is Amazon fine food reviews dataset.\nThere are a couple different models that we tried -\n* Model 1 was having architecture with one LSTM layer.\n* Model 2 was having architecture with one LSTM layer, intermediate dropouts set to 0.5 and 2 dense hidden layers with ReLU activation\n* Model 3 was having architecture with 2 LSTM layers.\n\nConclusion that can be drawn from the above models is that\nAll the models are performing great in terms of execution. All of them are converging very faster.\n\nThough Model 3 with 2 LSTM layers converges a little bit faster but it is requiring more training time.","a4becbef":"### Padding the dataset","5028fa69":"The dataset is available in two forms\n1. .csv file\n2. SQLite Database\n\nIn order to load the data, We have used the SQLITE dataset as it is easier to query the data and visualise the data efficiently.\n<br> \n\nHere as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score is above 3, then the recommendation wil be set to \"positive\". Otherwise, it will be set to \"negative\".","f08ddd01":"Segregating the input and output data from the dataset.\n\nWe will be using the Cleaned Text i.e preprocessed data from the dataset and score for that text","96f4c8ee":"### Model M2 ( Embedding -> LSTM -> Dropout -> Dense(128-Relu) -> Dropout -> Dense (64-Relu) -> Dropout -> Output(Sigmoid) ) ","924dccb4":"# Conclusion","161d1ab6":"# Amazon Fine Food Reviews Analysis\n\n\nData Source: https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews <br>\n\nEDA: https:\/\/nycdatascience.com\/blog\/student-works\/amazon-fine-foods-visualization\/\n\n\nThe Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.<br>\n\nNumber of reviews: 568,454<br>\nNumber of users: 256,059<br>\nNumber of products: 74,258<br>\nTimespan: Oct 1999 - Oct 2012<br>\nNumber of Attributes\/Columns in data: 10 \n\nAttribute Information:\n\n1. Id\n2. ProductId - unique identifier for the product\n3. UserId - unqiue identifier for the user\n4. ProfileName\n5. HelpfulnessNumerator - number of users who found the review helpful\n6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n7. Score - rating between 1 and 5\n8. Time - timestamp for the review\n9. Summary - brief summary of the review\n10. Text - text of the review\n\n\n#### Objective:\nGiven a review, determine whether the review is positive (rating of 4 or 5) or negative (rating of 1 or 2).\n\n<br>\n[Q] How to determine if a review is positive or negative?<br>\n<br> \n[Ans] We could use Score\/Rating. A rating of 4 or 5 can be cosnidered as a positive review. A rating of 1 or 2 can be considered as negative one. A review of rating 3 is considered nuetral and such reviews are ignored from our analysis. This is an approximate and proxy way of determining the polarity (positivity\/negativity) of a review."}}