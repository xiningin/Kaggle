{"cell_type":{"37e88298":"code","884121ac":"code","aaf9b88a":"code","31a2c787":"code","1d79ed90":"code","1db2f383":"code","f83cd6a7":"code","a96e0591":"code","88afa04d":"code","858274c6":"code","067b675e":"code","fb78e30c":"code","2b9e7c71":"code","97e0f0f1":"code","561a0963":"code","228c9c56":"code","f54a1870":"code","af680854":"code","9d1a2e63":"code","97c30938":"markdown","9f2df69a":"markdown","a6584cd1":"markdown","ba5ff5e1":"markdown","819158b9":"markdown","3e58d061":"markdown","69b898fa":"markdown"},"source":{"37e88298":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","884121ac":"import pickle\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime as dt\nfrom sklearn.linear_model import LinearRegression,Ridge,ElasticNet,SGDRegressor,Lasso\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport scipy as sp\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVR,LinearSVR\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_validate","aaf9b88a":"data = pd.read_pickle(\"\/kaggle\/input\/btc-data\/btc_data.pkl\")\ncols_to_del = ['limit_up_price','limit_down_price','source','volume','instrument_id',\"instrument_digital_id\",'turnover','type',\"open_interst\",\"last_price\"]\ndata.drop(columns=cols_to_del,axis=1,inplace=True)\n#Calculating Spread and other features\ndata['spread'] = data['ask_prices1'] - data['bid_prices1']\n#find mid\ndata['mid'] = (data['bid_prices1'] + data['ask_prices1'])\/2\ndata['mid_change'] = data.mid.diff()\ndata['return'] = data['spread']\/data['mid']\ndata['Volume_Order_Imbalance'] = data['bid_size1']\/(data['bid_size1'] + data['ask_size1'])\ndata['BA_size_spread'] = data['ask_size1'] - data['bid_size1']\n#converting unix timestamp to pandas timestamp\ndata.machine_time_stamp = pd.to_datetime(data.machine_time_stamp, unit='ns')\ndata.exchange_time_stamp = pd.to_datetime(data.exchange_time_stamp, unit='ns')\n#resampling to 1 s\ndf = data.resample(\"1s\",on=\"machine_time_stamp\")[['bid_prices1','ask_prices1','bid_size1','ask_size1','spread','mid','return','Volume_Order_Imbalance','BA_size_spread']].mean()\ndf.fillna(method='ffill',inplace=True)\ndf['return_10s'] = df['return'].shift(-10)\nmid_change_df = data.resample(\"1s\",on=\"machine_time_stamp\")['mid_change'].sum()\ndf = df.merge(mid_change_df,left_index=True,right_index=True)\ndf.dropna(inplace=True)\ntarget = 'return_10s'\nX = df.drop('return_10s',axis=1)\ny = df[target]\n#scaling features with min max scaler\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled,columns=X.columns)\nX_scaled.index = X.index\ndf_scaled = X_scaled.merge(y,left_index=True,right_index=True)","31a2c787":"df['return_10s'].equals(df_scaled.return_10s)","1d79ed90":"f, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1, 5,figsize=(25,5))\nn, bins, patches=ax1.hist(df[\"spread\"],bins=100)\nn, bins, patches=ax2.hist(df[\"Volume_Order_Imbalance\"],bins=1000)\nn, bins, patches=ax3.hist(df[\"mid_change\"],bins=500)\nn, bins, patches=ax4.hist(df[\"mid\"],bins=500)\nn, bins, patches=ax5.hist(df[\"BA_size_spread\"],bins=500)\nax1.set_ylim(0,2000)\nax1.set_title('Spread Distribution')\nax1.set_ylabel('Count')\nax2.set_title('Volume Order Imbalance Distribution')\nax3.set_ylim(0,5000)\nax3.set_title('mid_change Distribution')\nax4.set_title('mid price Distribution')\nax5.set_title('BA size spread Distribution')\nplt.show()","1db2f383":"#split train test data\n#train_size = int(len(df)*0.8)\n#test_size = len(df) - train_size\n#train_df = df[0:train_size]\n#test_df = df[train_size:]\ntrain,dev_test = train_test_split(df_scaled,test_size=0.3,random_state=20)\ndev,test = train_test_split(dev_test,test_size=0.5,random_state=2)","f83cd6a7":"train.return_10s.plot()","a96e0591":"dev.return_10s.plot()","88afa04d":"test.return_10s.plot()","858274c6":"f, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1, 5,figsize=(25,5))\nax1.scatter(x=train.spread,y=train.return_10s,c='r',label=\"spread\")\nax2.scatter(x=train['Volume_Order_Imbalance'],y=train.return_10s,label=\"Volume_Order_Imbalance\")\nax3.scatter(x=train['mid_change'],y=train.return_10s,label=\"Midprice_change\",c=\"green\")\nax4.scatter(x=train['mid'],y=train.return_10s,label=\"Mid\",c=\"c\")\nax5.scatter(x=train['BA_size_spread'],y=train.return_10s,label=\"Mid\",c=\"c\")\nax1.set_ylabel('Return in 10s')\nax1.set_xlabel('Spread')\nax2.set_xlabel('Volume_Order_Imbalance')\nax3.set_xlabel('Midprice Change')\nax4.set_xlabel('Mid')\nax5.set_xlabel(\"BA_size_spread\")\nax1.legend()\nax2.legend()\nax3.legend()\nax4.legend()\nf.suptitle(\"Return in 10s vs features\")\nplt.show()","067b675e":"#Feature Selection with correlation\ndf_corr_health = df.corr()['return_10s'].sort_values(ascending=False)[1:]\ntop_corr_features = df_corr_health.index\n# plot top 20 most correlated features to our target (HEALTH)\ndf_corr_health.plot(kind='barh')\nplt.show()","fb78e30c":"df.corr().return_10s.sort_values()","2b9e7c71":"# bid_size1, ask_size1, BA_size_spread are highly correlated with Volume_Order_Imbalance\n# Thus only pick Volume_Order_Imbalance to prevent overfitting\nfeature_cols = ['Volume_Order_Imbalance','mid_change','mid','spread']\ntarget_col = \"return_10s\"","97e0f0f1":"train_y = train[target_col].to_numpy()\ndev_y = dev[target_col].to_numpy()\ntest_y = test[target_col].to_numpy()\ntrain_X = train[feature_cols]\ndev_X = dev[feature_cols]\ntest_X = test[feature_cols]","561a0963":"total_train = pd.concat([train,dev])\ntotal_train_y = total_train.return_10s.to_numpy()\ntotal_train_X = total_train[feature_cols]","228c9c56":"total_train_X","f54a1870":"baseline = LinearRegression().fit(total_train_X,total_train_y)\nbaseline_train_result = baseline.predict(total_train_X)","af680854":"print(\"--------------------Train Result----------------------\")\nprint(\"R2 Score : \",r2_score(total_train_y,baseline_train_result))\nf, (ax1) = plt.subplots(1,figsize=(7,5))\nax1.plot(baseline_train_result,label='train_result',alpha=1,c='orange')\nax1.plot(total_train_y,label=\"ground truth\",alpha=0.5)\nplt.title('Baseline Result and Ground Truth TRAIN')","9d1a2e63":"baseline_test_result = baseline.predict(test_X)\nprint(\"--------------------Test Result----------------------\")\nprint(\"R2 Score : \",r2_score(test_y,baseline_test_result))\nplt.plot(baseline_test_result,label='baseline_result',alpha=1,c='orange')\nplt.plot(test_y,label=\"ground truth\",alpha=0.5)\nplt.legend()\nplt.title('Baseline Result and Ground Truth DEV')","97c30938":"R2 = r2 in linear regression so no need for correlation coefficient to be a matrix","9f2df69a":"Split train dev test","a6584cd1":"Building Baseline Model with Linear Regression\n\nUsing Pearson correlation coefficient, KS test p value, MSE as a performance metric, R2","ba5ff5e1":"Modelling","819158b9":"After using different models, I think it would be best to model return_10s using basic Linear Regression due to the fact that, there are useful missing information in that 10s that I cant gain access to. Plus Linear Regression is the simplest model that could capture good value of correlation and a good resemblence of the distribution of returns","3e58d061":"Plotting Distribution of Target and Features","69b898fa":"Feature Selection"}}