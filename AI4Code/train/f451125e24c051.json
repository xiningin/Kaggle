{"cell_type":{"dfb084fe":"code","32e2c6a5":"code","e263803c":"code","20cf8b9d":"code","2c7731c6":"code","bd11e8ed":"code","624e5ccb":"code","82db6df5":"code","0982492b":"code","ab2e4f24":"code","cd0a52db":"markdown","1a0d1217":"markdown","44b0352e":"markdown","b682be06":"markdown","ad3758ef":"markdown","91173d58":"markdown"},"source":{"dfb084fe":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nfrom joblib import Parallel, delayed\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\nimport os\nfrom sklearn.linear_model import LinearRegression\nimport warnings\nfrom sklearn.model_selection import train_test_split, KFold\nimport lightgbm as lgb\nimport time\nimport joblib\nimport datatable as dt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\nwarnings.filterwarnings(action='ignore', category=UserWarning)\nfrom tqdm import tqdm\nimport gc","32e2c6a5":"def my_metrics(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\ndef rmspe(y_true, y_pred):  # f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool\n    output = my_metrics(y_true, y_pred)\n    return 'rmspe', output, False\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):   \n    book_train_subset = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/book_{dataType}.parquet\/stock_id={stock_id}\/')\n    book_train_subset.sort_values(by=['time_id', 'seconds_in_bucket'])\n\n    book_train_subset['bas'] = (book_train_subset[['ask_price1', 'ask_price2']].min(axis = 1)\n                                \/ book_train_subset[['bid_price1', 'bid_price2']].max(axis = 1)\n                                - 1)                               \n    book_train_subset['wap'] = (book_train_subset['bid_price1'] * book_train_subset['ask_size1'] +\n                            book_train_subset['ask_price1'] * book_train_subset['bid_size1']) \/ (\n                            book_train_subset['bid_size1']+ book_train_subset['ask_size1'])\n    book_train_subset['log_return'] = (book_train_subset.groupby(by = ['time_id'])['wap'].\n                                       apply(log_return).\n                                       reset_index(drop = True).\n                                       fillna(0)\n                                      )\n    stock_stat = pd.merge(\n        book_train_subset.groupby(by = ['time_id'])['log_return'].agg(realized_volatility).reset_index(),\n        book_train_subset.groupby(by = ['time_id'], as_index = False)['bas'].mean(),\n        on = ['time_id'],\n        how = 'left'\n    )\n    stock_stat['stock_id'] = stock_id\n    return stock_stat\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n    return stock_stat_df","e263803c":"keep_stock_id = 1\n#keep_stock_id = 1\n\nfolds = 7\nseed_list = [i for i in range(12, 13)]\nearly_stopping = 200","20cf8b9d":"# train -------------------------\nif keep_stock_id:\n    td = dt.fread('..\/input\/mytrain\/X_131_features.csv')\n    X = td.to_pandas()\n    del td\nelse: \n    X = pd.read_csv(\"..\/input\/mytrain\/X.csv\")\ny = pd.read_csv(\"..\/input\/mytrain\/y.csv\")\n# to_test ----------------------------------------------------\ntest = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest_dataSet = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntest_dataSet = test_dataSet\nfinal_pred1 = test_dataSet[['row_id']]\nto_test = test_dataSet.drop(['row_id'], axis = 1).fillna(0)\nif keep_stock_id:\n    train = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/train.csv\")\n    cols = [f'stock_id_{c}' for c in list(set(train.stock_id))]\n    to_test[cols] = pd.DataFrame(np.stack([(to_test.stock_id == c).astype('int') for c in list(set(train.stock_id))]).T, columns = cols)\nelse:\n    to_test = to_test.drop(\"stock_id\", axis = 1)\n    X = X.drop(\"stock_id\", axis = 1)","2c7731c6":"config = {'input_path': \"..\/input\/optiver-realized-volatility-prediction\/trade_\",\n          'train_path': '..\/input\/optiver-realized-volatility-prediction\/train.csv',\n          'test_path' : '..\/input\/optiver-realized-volatility-prediction\/test.csv'}\ntest_df = pd.read_csv(config['test_path'])\ndef read_data(stock_id, data_type):\n    file = glob.glob(config['input_path']+f'{data_type}.parquet\/stock_id={stock_id}\/*')[0]\n    df = pd.read_parquet(file)\n    return df\ndef get_final_df(df, data_type):\n    final_df = pd.DataFrame()\n    unique_id = df['stock_id'].unique().tolist()\n    for stock_id in tqdm(unique_id):\n        temp_stock_df = read_data(stock_id=stock_id, data_type=data_type)\n        temp_stock_df['stock_id'] = stock_id\n        final_df = pd.concat([final_df, temp_stock_df])\n    final_df.reset_index(drop=True)\n    return final_df\ndef get_agg_info(df):\n    agg_df = df.groupby(['stock_id', 'time_id']).agg(mean_sec_in_bucket = ('seconds_in_bucket', 'mean'), \n                                                     mean_price = ('price', 'mean'),\n                                                     mean_size = ('size', 'mean'),\n                                                     mean_order = ('order_count', 'mean'),\n                                                     max_sec_in_bucket = ('seconds_in_bucket', 'max'), \n                                                     max_price = ('price', 'max'),\n                                                     max_size = ('size', 'max'),\n                                                     max_order = ('order_count', 'max'),\n                                                     min_sec_in_bucket = ('seconds_in_bucket', 'min'), \n                                                     min_price = ('price', 'min'),\n                                                     min_size = ('size', 'min'),\n                                                     min_order = ('order_count', 'min'),\n                                                     median_sec_in_bucket = ('seconds_in_bucket', 'median'), \n                                                     median_price = ('price', 'median'),\n                                                     median_size = ('size', 'median'),\n                                                     median_order = ('order_count', 'median')\n                                                    ).reset_index()\n    \n    return agg_df\ntest_final_df = get_final_df(df=test_df, data_type='test')\ntest_agg = get_agg_info(df=test_final_df)\ntest_final_df = pd.merge(test_df, test_agg, on=['stock_id', 'time_id'], how='left')\ntest_final_df.fillna(-999, inplace=True)\ntest_final_df = test_final_df.drop(\"row_id\", axis = 1)\nto_test = to_test.merge(test_final_df, on=['stock_id', 'time_id'], how='left')\nto_test.fillna(-999, inplace=True)\nto_test = to_test.drop(\"stock_id\", axis = 1)","bd11e8ed":"X","624e5ccb":"to_test","82db6df5":"def objective(trial , X = X , y = y):\n    if keep_stock_id: \n        params = {\n            'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 1e-5 , 1),\n            'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 3 , 9),\n            'num_leaves' : trial.suggest_int('num_leaves' , 20 , 60),\n            'learning_rate' : trial.suggest_uniform('learning_rate' , 0.02 , 0.08),\n            'max_depth' : trial.suggest_int('max_depth', 20 , 60),\n            'n_estimators' : trial.suggest_int('n_estimators', 3000 , 3600),\n            'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.05 , 0.15),\n            'subsample' : trial.suggest_uniform('subsample' , 0.7 , 1.0),\n            'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.5 , 1),\n            'min_child_samples' : trial.suggest_int('min_child_samples', 10, 40),\n            'metric' : 'rmse', #'rmse'\n            'device_type' : 'gpu',\n        }\n    else:\n        params = {\n            'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 1e-5 , 1),\n            'reg_lambda' : trial.suggest_loguniform('reg_lambda', 1e-5 , 1),\n            'num_leaves' : trial.suggest_int('num_leaves' , 10 , 60),\n            'learning_rate' : trial.suggest_uniform('learning_rate' , 0.02 , 0.08),\n            'max_depth' : trial.suggest_int('max_depth', 10 , 30),\n            'n_estimators' : trial.suggest_int('n_estimators', 60 , 1000),\n            'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.05 , 0.15),\n            'subsample' : trial.suggest_uniform('subsample' , 0.4 , 1.0),\n            'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.4 , 1),\n            'min_child_samples' : trial.suggest_int('min_child_samples', 10, 40),\n            'metric' : 'rmse', #'rmse'\n            'device_type' : 'gpu',\n        }\n    #pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'rmspe', valid_name = 'valid_0')  \n    score = 0\n    for seed in seed_list: \n        kf = KFold(n_splits = folds ,random_state= seed, shuffle=True)\n        for idx_train,idx_test in kf.split(X, y):\n            X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n            y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n            model = lgb.LGBMRegressor(**params, random_state = seed, n_jobs = -1)\n            model.fit(X_train, y_train.values.ravel(), eval_set = [(X_test , y_test.values.ravel())] ,eval_metric = rmspe, early_stopping_rounds = early_stopping, \\\n             verbose = 1500\n                    #   ,callbacks = [pruning_callback]\n                     ) \n            y_pred = model.predict(X_test)  \n            score += (my_metrics(y_test.values.ravel(), y_pred) \/ folds) \/ len(seed_list)                 \n    del model\n    return score\nimport optuna\nstudy = optuna.create_study(direction = 'minimize' , study_name = 'lgbm'\n                           # , pruner = optuna.pruners.HyperbandPruner()\n                           )\nstudy.optimize(objective , n_trials = 4)\nprint('numbers of the finished trials:' , len(study.trials))\nprint('the best params:' , study.best_trial.params)\nprint('the best value:' , study.best_value)\nprint(\"done\")\ntime.sleep(60)","0982492b":"params = {'reg_alpha': 0.028887686028843496, \n          'reg_lambda': 5.609420891429227, \n          'num_leaves': 56, 'learning_rate': 0.07716013360199103, \n          'max_depth': 23, 'n_estimators': 3398,\n          'min_child_weight': 0.11236058184476623, \n          'subsample': 0.8933699572438508, \n          'colsample_bytree': 0.9836129968372216, \n          'min_child_samples': 28} # Best is trial 2 with value: 0.28758503964033905.","ab2e4f24":"score = 0\nfor seed in seed_list: \n    kf = KFold(n_splits = folds ,random_state= seed, shuffle=True)\n    count = 1\n    for idx_train,idx_test in kf.split(X, y):\n        print(\"=\" * 40)\n        print(\"seed\", seed)\n        print(\"fold\", count)\n        print(\"=\" * 30)\n        start_time = time.time()\n        X_train, X_test = X.iloc[idx_train], X.iloc[idx_test]\n        y_train, y_test = y.iloc[idx_train], y.iloc[idx_test]\n        model = lgb.LGBMRegressor(**params, random_state = seed, n_jobs = -1, metric = 'rmse', device_type = 'gpu')\n        model.fit(X_train, y_train, eval_set = [(X_test , y_test.values.ravel())], eval_metric = rmspe,\\\n                  early_stopping_rounds = early_stopping, verbose = False)\n        cv_score = my_metrics(y_test.values.ravel(), model.predict(X_test))\n        score += (cv_score \/ folds) \/ len(seed_list)\n        joblib.dump(model, f'LGBM seed_{seed}_fold_{count}_cv_score_{round(cv_score, 3)}.pkl') # save model\n        end_time = time.time()\n        run_time = round(end_time - start_time)\n        print (\"fold\", count, \"took\", run_time , \"seconds to run\")\n        count += 1\n        print (\"The estimated remaining training time in the current seed\", seed, \"are\",\\\n               round(((folds - count) * run_time) \/ 60, 3), \"minuets\")\n        print(\"Validation score\", cv_score)\nprint(\"Mean RMSPE validation score of\", folds, \"folds\", score)\n","cd0a52db":"# Infer: https:\/\/www.kaggle.com\/yus002\/realized-volatility-prediction-lgbm-infer","1a0d1217":"# Training Data Set comes from \n* # https:\/\/www.kaggle.com\/mayunnan\/realized-volatility-prediction-code-template\n* # https:\/\/www.kaggle.com\/thanish\/randomforest-starter-submission","44b0352e":"# Data","b682be06":"# Save models","ad3758ef":"# Config","91173d58":"# Helper methods"}}