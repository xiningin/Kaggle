{"cell_type":{"02b22552":"code","85e7bc29":"code","79828d35":"code","ccd1af4e":"code","3d19fc4c":"code","1fa3db35":"code","bd462b8a":"code","d75420b1":"code","8827eaf7":"code","7bd54b3c":"code","90c222cd":"code","52fe6b06":"code","3208598d":"code","1c510aa3":"code","91f93de5":"code","ae2c76de":"code","0a2dc8fd":"code","2ce49b83":"code","8f90f524":"code","54f40c07":"code","95487c17":"code","91ebe15d":"code","bba9f0c3":"code","59735992":"code","99671d05":"code","d4024ced":"code","5e050fcd":"code","762c6998":"code","3a522ef1":"code","fbe5e7f9":"code","3a6c989c":"code","63125634":"code","7440e4ac":"code","5205a276":"code","3a4ff6be":"markdown","01e5728a":"markdown","1973dfa4":"markdown","ca9cdf5a":"markdown","c4aad6db":"markdown","0d3b33ae":"markdown","b007f571":"markdown","bbe624b3":"markdown","c6f44576":"markdown","2687a71f":"markdown","f7160187":"markdown"},"source":{"02b22552":"# import of packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.io as pio\npio.templates","85e7bc29":"import plotly.express as px","79828d35":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter","ccd1af4e":"df =pd.read_json('..\/input\/simplyhired-job-data-listing-2020\/marketing_sample_for_simplyhired_com-simplyhired_com_jobs__20200101_20200331.ldjson',lines=True)","3d19fc4c":"df.head()","1fa3db35":"df = df[['job_title', 'category',\n       'company_name', 'city', 'state', 'country', 'post_date',\n       'job_description', 'job_type',\n       'inferred_city', 'inferred_state',\n       'inferred_salary_from','inferred_salary_to', 'inferred_salary_currency', 'salary_offered']]","bd462b8a":"df.columns","d75420b1":"# missing values\n\nmissing_value = 100 * df.isnull().sum()\/len(df)\n\nmissing_value = missing_value.reset_index()\n\nmissing_value.columns = ['variables','missing values in percentage']\n\n# barplot\nplt.figure(figsize=(10,6))\nsns.set_context(context='notebook',font_scale=1.5)\nsns.barplot(x='missing values in percentage',y='variables',data=missing_value,palette='Set2');\nplt.title('Missing values in each column');\nplt.tight_layout();\n\n# heatmap\nplt.figure(figsize=(14,6));\nplt.title('Missing values in each columns');\nsns.set_context(context='notebook',font_scale=1.5);\nsns.heatmap(df.isnull(),cmap='Set3',cbar=False,yticklabels=False);","8827eaf7":"#salary cleaning\n\ndf['salary_offered'] = df['salary_offered'].str.replace('a','')\ndf['salary_offered'] = df['salary_offered'].str.strip('year')\ndf['salary_offered'] = df['salary_offered'].str.replace('$','')\ndf['salary_offered'] = df['salary_offered'].str.replace(',','')\n","7bd54b3c":"# split the Salary Estimate into two parts Min and Max\n\nSalary_min_max = df['salary_offered'].str.split('-',expand=True)","90c222cd":"# join the columns\ndf['Salary_Min'] = Salary_min_max[0]\ndf['Salary_Max'] = Salary_min_max[1]\n\n# convert the column into float64\ndf['Salary_Min'] = pd.to_numeric(df['Salary_Min'])\ndf['Salary_Max'] = pd.to_numeric(df['Salary_Max'])","52fe6b06":"import re\n\ndef  clean_text(text):\n    \"\"\"\n    Fuction to clean the text data\n    * symbols\n    * change to lower_case\n    \"\"\"\n    text = text.str.lower()\n    text = text.apply(lambda T: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", T))  \n        \n    return text","3208598d":"df['job_description']= clean_text(df['job_description'])","1c510aa3":"from nltk.corpus import stopwords\n\nFre_word = set(stopwords.words('english'))\n\ndef del_stopwords(text):\n    \"\"\"\n    function to remove the stopwords\n    \"\"\"\n    return \" \".join([word for word in str(text).split() if word not in Fre_word])\n\ndf['job_description'] = df['job_description'].apply(lambda T: del_stopwords(T))","91f93de5":"# most frequent words\n\nfrom collections import Counter\n\ncount = Counter()\n\nfor text in df['job_description'].values:\n    for word in text.split():\n        count[word] = count[word] + 1\n\ncount.most_common(10)","ae2c76de":"# Removing most Frequent words\n\nFreq_word = set([i for (i, ic) in count.most_common(10)])\n\n\ndef del_freq_word(text):\n    \"\"\"\n    This function will remove the frequent words\n    \n    \"\"\"\n    return \" \".join([word for word in str(text).split() if word not in Freq_word])\n\ndf['job_description'] = df['job_description'].apply(lambda text: del_freq_word(text))\n","0a2dc8fd":"\nFreq_word = set([i for (i, ic) in count.most_common(10)])\n\n\ndef del_freq_word(text):\n    \"\"\"\n    This function will remove the frequent words\n    \n    \"\"\"\n    return \" \".join([word for word in str(text).split() if word not in Freq_word])\n\ndf['job_description'] = df['job_description'].apply(lambda text: del_freq_word(text))\n","2ce49b83":"# job description\n\ndf['job_description'] = df['job_description']","8f90f524":"#job title\n\ndf['job_title'] = df['job_title'].str.replace(r'\\W',' ')# remove Non-alphanumeric (+ - *)\ndf['job_title'] = df['job_title'].str.replace(r'\\d+',' ')# remove digits\ndf['job_title'] = df['job_title'].str.replace('  ','')# remove space","54f40c07":"df['category'] = pd.Categorical(df['category'])","95487c17":"df['post_date'] = pd.to_datetime(df['post_date'])","91ebe15d":"# extracting month from the date\n\ndf['Job posted month'] =  df['post_date'].dt.month","bba9f0c3":"# data for plot\njob_title_plot = df['job_title'].value_counts()[0:20].reset_index()\njob_title_plot.columns = ['Job_title','Count']\n\n# plot\nfig = px.bar(job_title_plot,y='Job_title',x='Count',color='Count',title='Top 20 job titles')\nfig.show()","59735992":"job_title_pie = df['job_title'].value_counts()[0:45].reset_index()\njob_title_pie.columns = ['Job_title','Count']\n\nfig = px.pie(job_title_pie,values='Count',names='Job_title',title='Top 45 job title')\nfig.show()","99671d05":"job_cate = df['category'].value_counts().reset_index()[0:10]\njob_cate.columns = ['Job_type','Count']\n\nfig  = px.pie(job_cate, names='Job_type',values='Count',title='Job types',template='ggplot2')\nfig.show()","d4024ced":"# company name\ncomp_name = df['company_name'].value_counts().reset_index()[0:20]\ncomp_name.columns = ['company_name','count']\n\n\nfig = px.bar(comp_name,y='company_name',x='count',title='Top 20 companies name with job post',template='ggplot2',color='count')\n                           \nfig.show()","5e050fcd":"city_job = df['city'].value_counts().reset_index()[0:20]\ncity_job.columns = ['City','count']\n\nfig = px.bar(city_job,y='City',x='count',title='Top 20 city where job posted',template='gridon',color='count')\nfig.show()","762c6998":"state_count = df['state'].value_counts().reset_index()[0:20]\nstate_count.columns = ['state','count']\n\nfig  = px.bar(state_count, x='state',y='count',title='Top 20 states',template='ggplot2')\nfig.show()","3a522ef1":"# salary plot\n\nx = df['Salary_Min']\ny = df['Salary_Max']\n\n\nfig, ax = plt.subplots(1,2,figsize=(18, 6))\n\nplt.figure(figsize=(10,6))\n\nsns.set_style('dark')\nsns.set_context(context = 'notebook',font_scale=1.2);\nsns.boxplot(x, ax = ax[0],palette='rainbow');\nsns.boxplot(y, ax = ax[1],palette='spring');\n\nax[0].title.set_text('Minimum Salary')\nax[1].title.set_text('Maximum Salary')\n\nplt.tight_layout()","fbe5e7f9":"from wordcloud import WordCloud, STOPWORDS\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in df['job_description']: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for text in range(len(tokens)): \n        tokens[text] = tokens[text].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 1200, height = 1200, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 20).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = '#6897BB') \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title('Job Description')  \nplt.show() ","3a6c989c":"freq_word = count.most_common(20)\n\nfreq_word_plot1 = pd.Series( (v[0] for v in freq_word) )\nfreq_word_plot2 = pd.Series( (v[1] for v in freq_word) )\n\n\ndata_freq = pd.DataFrame({'Words':freq_word_plot1,'Count':freq_word_plot2})\ndata_freq = data_freq.sort_values('Count')\n\n\n#plot\nplt.figure(figsize = (12,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.barplot(data=data_freq,y='Words',x='Count');\nplt.title('Top 20 Frequent words in the Job Description');\nplt.tight_layout();","63125634":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","7440e4ac":"# Trigram analysis for Job Description\n\nplt.figure(figsize = (10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nUnigram_word = text_ngrams(df['job_description'],10,3)\nUnigram_word = dict(Unigram_word)\nsns.barplot(x=list(Unigram_word.values()),y=list(Unigram_word.keys()),palette = 'rainbow');\nplt.title('Trigram analysis for job description');","5205a276":"# Thanks!\n\n## upvote are highly appreciated!","3a4ff6be":"# What are cities which have most of the jobs?","01e5728a":"# Simply hired job site EDA","1973dfa4":"# what employers prefer to offer full time or part-time?","ca9cdf5a":"# Missing values","c4aad6db":"# What are the top company that offer jobs?","0d3b33ae":"# What are the minimum and maximum salary offered per annum?","b007f571":"# what job titles are most preferred?","bbe624b3":"# what qualification and skill required?","c6f44576":"# which are states has most of the jobs?","2687a71f":"# Data cleaning","f7160187":"# Data visualization"}}