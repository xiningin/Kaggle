{"cell_type":{"87f6a41e":"code","117e4fa1":"code","87380240":"code","b9094cbc":"code","c9f2b20f":"code","8c54daa5":"code","9c5e05ca":"code","440f3e4e":"code","8befb942":"code","b06cc7b5":"code","b2120d90":"code","18a52eda":"code","60cbd12f":"code","8b3c4cb9":"code","dab83ba9":"code","a7e070f5":"code","4dce4bdd":"code","d1427a34":"code","55a17c58":"code","af7f92f1":"code","948fcdea":"code","71d074b9":"code","62432397":"code","8798afd9":"code","921fa365":"code","bb50191f":"code","2d84bcd9":"code","297f7de4":"code","a9d7f986":"code","a4ff7e95":"code","27b61cb5":"code","e25988e8":"code","f8eb6168":"code","8ba3232d":"code","a085a72c":"code","5564d4c3":"code","6df0c0ef":"code","d60e7c1e":"code","b30cb02d":"code","bc869ff0":"code","ad0a464c":"code","9e3aab9b":"code","54797883":"code","a84a9a1c":"code","4af59178":"code","809a4bfc":"code","cb6acd16":"code","f1faf8c4":"code","1783fc5e":"code","09981ecc":"code","fae300f6":"code","a6feb64a":"code","fa1b1966":"code","e4e40abc":"code","d10899b7":"code","20395f5c":"code","0fc1d685":"code","52f24806":"code","3d8ce1b2":"code","8c6d58ba":"code","6c6b89a2":"code","cd1fece7":"code","9afb3d97":"code","8452d6d0":"code","e94a6cd2":"code","f0308f44":"code","d07bda1b":"code","23070875":"code","4ee326ac":"code","6fb9e1de":"code","2478b40d":"code","bacce699":"code","c199f0e2":"code","2e32635d":"code","810c69a9":"code","3050dc82":"code","e44be611":"code","8e77d0d3":"code","47ab1945":"code","43356abc":"code","a011bb33":"code","bb78c481":"code","c8b89da2":"code","dc753760":"code","13c8dc04":"code","602c08d2":"code","6f8ac4c0":"code","671b7d60":"code","9da2ebfb":"code","7c0e4926":"code","754ad47d":"code","c1c80f18":"code","bf6bd958":"code","fa2e88f4":"code","eeac2b58":"code","01355a6e":"code","8c5e3a00":"code","357f60d1":"code","5f1bfff3":"code","cd796159":"code","9ca56ff5":"code","a50e3d2a":"code","8609b40b":"code","a414e17b":"code","de51789e":"code","dd21b5ef":"code","1d7ac41e":"code","6c01e07a":"code","8406af2d":"code","5d4c257f":"code","ea16756b":"code","b85c5a04":"code","66a3fccc":"code","067a0e7c":"code","532930c8":"code","d95d2b47":"code","f9c02341":"code","94876a56":"code","099a6b01":"code","c53f8a88":"code","37826be9":"code","aca73255":"code","1dc1497c":"code","c25490d2":"code","aef2c1aa":"code","d566c07b":"code","d923a04b":"code","371047a4":"code","08d2aebc":"code","41bbb411":"code","8afeb758":"code","79a10b74":"code","f9bc888f":"code","d1ac4127":"code","ad05d5e2":"code","71c0c93a":"code","e021536c":"code","a5d2c4fb":"code","92d4ef6d":"code","62e8b50e":"code","55674b39":"code","c48ff772":"code","1889c4c4":"code","84ec659c":"code","65a1198f":"code","34e34e9b":"code","051b7d07":"code","aaca53c7":"code","37e9268b":"code","a54736d8":"code","b694b1aa":"code","3b5ee574":"code","c0e30012":"code","f6ee9088":"code","bddb17f1":"code","f5de7473":"code","3eadf88c":"markdown","2c92edff":"markdown"},"source":{"87f6a41e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","117e4fa1":"!pip uninstall --y typing\n!pip install nvtx dask_cuda\n!pip install git+https:\/\/github.com\/NVIDIA\/NVTabular.git@4c92dffac4354d816178264bcfcdec722db2ec1c","87380240":"import os\nfrom time import time\nimport re\nimport glob\nimport warnings\n# import cudf\nimport gc\n\n# tools for data preproc\/loading\nimport torch\n# import rmm\n# import nvtabular as nvt\n# from nvtabular.ops import Normalize,  Categorify,  LogOp, FillMissing, Clip, get_embedding_sizes\n# from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n# from nvtabular.utils import device_mem_size, get_rmm_size\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# tools for training\nfrom fastai.basics import Learner\n\nfrom fastai.tabular.all import *\nfrom fastai.tabular.model import TabularModel\nfrom fastai.tabular.data import TabularDataLoaders\nfrom fastai.metrics import accuracy\nfrom fastai.callback.progress import ProgressCallback\nfrom collections import defaultdict\nfrom numba import njit\nfrom scipy import stats\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom kmodes import kmodes\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression\n\n# import dask\n# import dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\ngc.collect()\n\n  ################################################################################################################\n\n# class CFG:\n#     TRAIN_SIZE = 0.2\n#     MIN_WEIGHT = 0\n#     NROWS = None\n#     FILL_NA = -999\n    \n# # -75% of time space took by train.csv so we can train faster models\n# # and avoid RAM errors\n\n# features_columns = [\"feature_%d\" % i for i in range(130)]\n# columns_dtypes = {}\n# for column in features_columns:\n#     columns_dtypes[column] = \"float16\"\n# columns_dtypes[\"resp_1\"] = \"float16\"\n# columns_dtypes[\"resp_2\"] = \"float16\"\n# columns_dtypes[\"resp_3\"] = \"float16\"\n# columns_dtypes[\"resp_4\"] = \"float16\"\n# columns_dtypes[\"resp\"] = \"float16\"\n\n# print(\"Loading dataset...\")\n# dataset = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/train.csv\", delimiter=\",\", nrows=CFG.NROWS, dtype=columns_dtypes)\n# dataset = dataset[dataset.weight > CFG.MIN_WEIGHT]\n# print(\"Done!\")\n\n# print(\"Splitting train\/test dataset...\")\n# train_number_items = int(dataset.shape[0] * CFG.TRAIN_SIZE)\n# train = dataset[:train_number_items]\n# test = dataset[train_number_items + 1:]\n# print(\"Done!\")\n\n# print(\"Filling NaN values...\")\n# train = train.fillna(CFG.FILL_NA)\n# test = test.fillna(CFG.FILL_NA)\n# print(\"Done.\")\n\n# print(\"Preparing X and y...\")\n# X_train = train[features_columns].to_numpy()\n# X_test = test[features_columns].to_numpy()\n# y_train = np.where(train[\"resp\"] > 0, 1, 0)\n# y_test = np.where(test[\"resp\"] > 0, 1, 0)\n# d_train = train[\"date\"].to_numpy()\n# d_test = test[\"date\"].to_numpy()\n# w_train = train[\"weight\"].to_numpy()\n# w_test = test[\"weight\"].to_numpy()\n# r_train = train[\"resp\"].to_numpy()\n# r_test = test[\"resp\"].to_numpy()\n# resp_train = train[[\"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]].to_numpy()\n# resp_test = test[[\"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]].to_numpy()\n\n# resp_label_train = np.where(resp_train > 0, 1, 0)\n# resp_label_test = np.where(resp_test > 0, 1, 0)\n\n# print(\"Done!\")\n\n# print(\"Deleting unused variables...\")\n# del dataset\n# del train\n# del test\n# print(\"Done!\")\n\n# print(\"Train\/test sizes: %d\/%d\" % (len(X_train), len(X_test)))","b9094cbc":"js=pd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\")","c9f2b20f":"js.columns","8c54daa5":"drop_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4','ts_id']","9c5e05ca":"js=js[js.weight>0]\njs=js[js.date>85]\njs['action'] = (  (js['resp_1'] > 0 ) & (js['resp_2'] > 0 ) & (js['resp_3'] > 0 ) & (js['resp_4'] > 0 ) &  (js['resp'] > 0 )   ).astype('int')\njs.drop(drop_cols, inplace=True, axis=1)\njs.reset_index(inplace=True)\njs.to_feather('js')","440f3e4e":"js.drop(['index'], inplace=True, axis=1)\njs.to_feather('js')","8befb942":"import pathlib\npath = pathlib.Path(\"\/kaggle\/input\/jane-street-market-prediction\")","b06cc7b5":"js.head()","b2120d90":"dic1 = dic.set_index('Unnamed: 0').to_dict()\ndic.drop(dic.index,inplace=True)\ndic1=dic1['0']\ntrain = pd.read_csv(path\/\"train.csv\",dtype=dic1)","18a52eda":"OUTPUT_DATA_DIR='\/kaggle\/working\/' ","60cbd12f":"train_paths = ['..\/input\/train-and-valid\/train']\nvalid_paths = ['..\/input\/train-and-valid\/valid']","8b3c4cb9":"train = pd.read_parquet('..\/input\/train-and-valid\/train')\nvalid = pd.read_parquet('..\/input\/train-and-valid\/valid')","dab83ba9":"gc.collect()","a7e070f5":"resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\nfeats=[f'feature_{i}' for i in range(130)]\ncon_names = ['weight'] + resp_cols + feats \ncat_names = [x for x in train.columns if '_na' in x]\ncat_names= ['date'] + cat_names","4dce4bdd":"train=train[train.weight>0]\ntrain=train[train.date>85]\nvalid=valid[valid.weight>0]","d1427a34":"len(train)","55a17c58":"len(valid)","af7f92f1":"data=pd.concat([train,valid])","948fcdea":"data.head()","71d074b9":"data.drop('index',axis=1,inplace=True)","62432397":"js.reset_index(inplace=True)","8798afd9":"js.to_feather('js') ","921fa365":"for i in range(1,4):\n    print(i)","bb50191f":"t.head()","2d84bcd9":"len(t)","297f7de4":"len(t)","a9d7f986":"gc.collect()","a4ff7e95":"tnacol = train.columns[train.isnull().any()]\ntna = train[tnacol].isnull()\nnat={}\nfor c in tnacol:\n    nat[c]= c+\"_na\"\ntna.rename(mapper=nat,inplace=True)","27b61cb5":"vnacol = valid.columns[valid.isnull().any()]\nvna = valid[vnacol].isnull()\nnav={}\nfor c in vnacol:\n    nav[c]= c+\"_na\"\nvna.rename(mapper=nav,inplace=True)    ","e25988e8":"#Fn to reduce memory usage significantly\ndef reduce_memory_usage(df):\n    \n    start_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    end_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) \/ start_memory} % \")\n    ","f8eb6168":"# valid = valid.astype({c: np.float32 for c in valid.select_dtypes(include='float64').columns}) #limit memory use\n# valid['action'] =  (  (valid['resp_1'] > 0 ) & (valid['resp_2'] > 0 ) & (valid['resp_3'] > 0 ) & (valid['resp_4'] > 0 ) &  (valid['resp'] > 0 )   ) & (valid['weight'] > 0).astype('int')","8ba3232d":"# train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n# train['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   )& (train['weight'] > 0).astype('int')\n","a085a72c":"def eval_metric(dataset):\n    p = []\n    for d in dataset.date.unique():\n        p.append(np.sum(dataset[dataset.date==d].weight*dataset[dataset.date==d].resp*dataset[dataset.date==d].action))\n    t = (np.sum(p)*np.sqrt(250))\/(np.sqrt(np.sum(p**2))*len(dataset.date.unique()))\n    u = min(max(t,0),6)*np.sum(p)\n    return u\n","5564d4c3":"\ndef fillna_npwhere_njit(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","6df0c0ef":"t=t.to_numpy()","d60e7c1e":"gc.collect()","b30cb02d":"l = [i for i in range(130)]\nl","bc869ff0":"dic","ad0a464c":"di","9e3aab9b":"di = {}\nfor i in range(130):\n    di[i]=f'feature_{i}'","54797883":"\ndata.rename(columns = di, inplace = True)\n# valid.rename(columns = di, inplace = True)","a84a9a1c":"data.feature_77","4af59178":"\ndef KNNimp(dataset):\n    imputer = KNNImputer()\n    dataset = imputer.fit_transform(dataset)","809a4bfc":"fillm(train)","cb6acd16":"\n\n\n#Fill Na using numpy numba : fillna_npwhere_njit\n@njit(parallel=True, fastmath=True)\ndef cluster_missing(dataset):\n    array = dataset.to_numpy()\n#     dataset.interpolate(method ='quadratic', axis=0, inplace=True)\n    d=dataset.isna().sum().sort_values(ascending=False).to_dict()\n    g=defaultdict(list)\n    for k in d.keys():\n        if (d[k]!=0):\n            g[d[k]].append(k)\n            p=d[k]\n        else:\n            break;\n    return g\n#     for k in g.keys():\n#         imp = IterativeImputer(sample_posterior=True)\n#         dataset[g[k]]=imp.fit_transform(dataset[g[k]])\n#         gc.collect()\n#         gc.collect()\n#     print(dataset.isnull().sum().sum())\n# def cluster_tags(dataset):\n    \n\ndef spearman(df):\n    m = df.corr()\n    return m;\n#     sx,sy=np.where(s>0.5)\n#     for  i in range(len(sx)):\n#         if(sx[i]>sy[i]):\n\n# Using kmeans to cluster the features based on their correlation\ncorr_feat_mtx = spearman(t).to_numpy()\nkmeans = KMeans(n_clusters = 20, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ncorr_feat_labels = kmeans.fit_predict(corr_feat_mtx)\n\n# Preparing a dataframe to collect some cluster stats\n# corr_feat_clust_df = pd.DataFrame(np.c_[feat_names, corr_feat_labels])\n# corr_feat_clust_df.columns = [\"feature\", \"cluster\"]\n# corr_feat_clust_df['feat_list'] = corr_feat_clust_df.groupby([\"cluster\"]).transform(lambda x: ', '.join(x))\n# corr_feat_clust_df = corr_feat_clust_df.groupby([\"cluster\", \"feat_list\"]).size().reset_index(name = 'feat_count')\n# corr_feat_clust_df\n    \n\n\n\n","f1faf8c4":"gc.collect()","1783fc5e":"tag1.isna().sum().sum()","09981ecc":"tag1=tag1.astype('float')","fae300f6":"x2=corr_feat_labels","a6feb64a":"pip install missingno","fa1b1966":"corrclus = defaultdict(list)\nfor i in range(130):\n    f='feature_{}'.format(i)\n    corrclus[corr_feat_labels[i]].append(f)","e4e40abc":"corrclus","d10899b7":"# Clustering Using Tags, Missing, Distribution, Pearson Correlation \ng = defaultdict(list)\ng['a'] = [1,2,9,10,15,16,42,43,45,46,48,49]\ng['b'] = [3,4,5,6,7,8,11,12,14]\ng['c'] = [17,18,19,20,21,22,23,24,25,26]\ng['d'] = [27,28,29,30,31,32,33,34,35,36,37,39,40]\ng['e'] = [41,44,51,52,53,55,56,57,58,59,60,61]\ng['f'] = [42,43,45]\ng['g'] = [46,48,49,50,86,87,88]\ng['h'] = [62,63,65,66]\ng['i'] = [64,67,68]\ng['j'] = [65,66]\ng['k'] = [69,70,71]\ng['l'] = [72,78,84,90,93,96,99,102,105,108,114]\ng['m'] = [73,74,75,76,78,79,80,81,82]\ng['n'] = [120,122,124,126,128]\ng['o'] = [121,123,125,127,129]\ng['p'] = [89,92,98,101,104,109,113,116]\ng['q'] = [95,107,119]\ng['r'] = [108,110,111,112,115,117,118]\ng['s'] = [92,93,94,99,100,105]\ng['t'] = [85,91,97,100,103,106]\noutf = [13,38,47,54,77,83]","20395f5c":"train.drop(outf,axis=1,inplace=True)\nvalid.drop(outf,axis=1,inplace=True)","0fc1d685":"lm = [None]*130\nfor x in g.keys():\n    for i in g[x]:\n        lm[i]=x\n        \n        ","52f24806":"pip install --no-cache-dir --upgrade autoimpute","3d8ce1b2":"pip install autoimpute","8c6d58ba":"from autoimpute.analysis import MiLinearRegression","6c6b89a2":"# @njit(parallel=True, fastmath=True)\ndef wt(dataset):\n    return dataset.corr().to_numpy()\ndef fillm(dataset):\n    for k in g.keys():\n        lr = LinearRegression()\n        imp = IterativeImputer(estimator=lr, verbose=2, max_iter=50, tol=1e-10, imputation_order='ascending')\n        if (dataset=='valid') : imp.initial_startegy=train[g[k]].mean()\n        dataset[g[k]]=imp.fit_transform(dataset[g[k]])\n        gc.collect()\n        gc.collect()\n    print(dataset.isnull().sum().sum())","cd1fece7":"gc.collect()","9afb3d97":"fillm(valid)","8452d6d0":"len(valid)","e94a6cd2":"len(train)","f0308f44":"gc.collect()","d07bda1b":"train[g['a']]","23070875":"# GMM Clustering\ndef gm(t):\n    from sklearn.mixture import GaussianMixture\n    gmm = GaussianMixture(n_components=4)\n    gmm.fit(t)\n\n    #predictions from gmm\n    labels = gmm.predict(t)\n    frame = pd.DataFrame(t)\n    frame['cluster'] = labels\n    frame.columns = ['Weight', 'Height', 'cluster']\n\n    color=['blue','green','cyan', 'black']\n    for k in range(0,4):\n        t = frame[frame[\"cluster\"]==k]\n        plt.scatter(t[\"Weight\"],t[\"Height\"],c=color[k])\n    plt.show()\n\n\n            \n    \n\n","4ee326ac":"train_w0 = train[train.weight==0]\ntrain = train[train.weight>0]","6fb9e1de":"# fillm(train)\nreduce_memory_usage(train)","2478b40d":"valid_w0 = valid[valid.weight==0]\nvalid = valid[valid.weight>0]","bacce699":"gc.collect()","c199f0e2":"# fillm(valid)\nreduce_memory_usage(valid)\ngc.collect()","2e32635d":"len(train)","810c69a9":"fillm(train)\nfillm(valid)","3050dc82":"procs = [Categorify, FillMissing, Normalize]\ny_names = 'salary'\ny_block = CategoryBlock()","e44be611":"to = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=con_names,\n                   y_names=y_names, y_block=y_block, splits=splits)","8e77d0d3":"train[train.date==360]","47ab1945":"valid = train[1683632:]","43356abc":"train=train[:1683632]","a011bb33":"\nlen(valid)","bb78c481":"gc.collect()","c8b89da2":"train.interpolate(method ='quadratic', axis=0, inplace=True)","dc753760":"av","13c8dc04":"g479=['feature_28','feature_27','feature_18','feature_17']\ng476=['feature_102','feature_7','feature_78','feature_72','feature_84','feature_108','feature_96','feature_90','feature_114','feature_8'] \ng99=['feature_121','feature_120','feature_55']\ng74=['feature_21','feature_32','feature_31','feature_22']\ng72=['feature_92','feature_104','feature_80','feature_98','feature_116','feature_86','feature_74','feature_110','feature_11','feature_12']\ng71=['feature_121','feature_120']\ng48 = ['feature_55']","602c08d2":"gc.collect()","6f8ac4c0":"from sklearn.mixture import GaussianMixture\n","671b7d60":"train_y = (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\nvalid_y =  (  (valid['resp_1'] > 0 ) & (valid['resp_2'] > 0 ) & (valid['resp_3'] > 0 ) & (valid['resp_4'] > 0 ) &  (valid['resp'] > 0 )   ).astype('int')","9da2ebfb":"gmm = GaussianMixture(n_components=2,max_iter=200).fit(train)\ngc.collect()","7c0e4926":"gm_preds=gmm.predict(valid)\n","754ad47d":"gm_preds","c1c80f18":"len(gm_preds[gm_preds==valid_y])\/len(gm_preds)","bf6bd958":"arr = [1]*len(gm_preds)","fa2e88f4":"gm_preds = abs(gm_preds - arr) ","eeac2b58":"gm_preds","01355a6e":"traindf = pd.concat([train,valid])","8c5e3a00":"len(traindf)","357f60d1":"valid.interpolate(method ='quadratic', axis=0, inplace=True)","5f1bfff3":"av=valid.isna().sum().sort_values(ascending=False).to_dict()","cd796159":"av","9ca56ff5":"cat_features = cat_names >> Categorify()\ncon_features = con_names >> Normalize()","a50e3d2a":"features = cat_features+con_features+['action']","8609b40b":"train[con_names]=train[con_names].astype('float32')\nvalid[con_names]=valid[con_names].astype('float32')","a414e17b":"workflow = nvt.Workflow(features)","de51789e":"train_dataset = nvt.Dataset(train)\nvalid_dataset = nvt.Dataset(valid)","dd21b5ef":"gc.collect()","1d7ac41e":"\n\nworkflow.fit(train_dataset)\n\n","6c01e07a":"OUTPUT_DATA_DIR='.\/'","8406af2d":"output_train_dir = os.path.join(OUTPUT_DATA_DIR, 'train\/')\noutput_valid_dir = os.path.join(OUTPUT_DATA_DIR, 'valid\/')\n! mkdir -p $output_train_dir\n! mkdir -p $output_valid_dir","5d4c257f":"\n\nworkflow.transform(train_dataset).to_parquet(output_path=output_train_dir,\n                                             shuffle=nvt.io.Shuffle.PER_PARTITION, \n                                             out_files_per_proc=5)\n","ea16756b":"gc.collect()","b85c5a04":"\n\nworkflow.transform(valid_dataset).to_parquet(output_path=output_valid_dir, out_files_per_proc=5)\n\n","66a3fccc":"len(train)+len(valid)","067a0e7c":"gc.collect()","532930c8":"BATCH_SIZE = int(os.environ.get('BATCH_SIZE', 1024*16))           # Batch-size for training neural networks\nPARTS_PER_CHUNK = int(os.environ.get('PARTS_PER_CHUNK', 1))","d95d2b47":"!nvidia-smi","f9c02341":"gc.collect()\n","94876a56":"train_data_itrs = TorchAsyncItr(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    cats=cat_names,\n    conts=con_names,\n    labels=['action'],\n    parts_per_chunk=PARTS_PER_CHUNK\n)\nvalid_data_itrs = TorchAsyncItr(\n    valid_dataset,\n    batch_size=BATCH_SIZE,\n    cats=cat_names,\n    conts=con_names,\n    labels=['action'],\n    parts_per_chunk=PARTS_PER_CHUNK\n)","099a6b01":"def gen_col(batch):\n    return (batch[0], batch[1], batch[2].long())","c53f8a88":"train_dataloader = DLDataLoader(train_data_itrs, collate_fn=gen_col, batch_size=None, pin_memory=False, num_workers=0)\nvalid_dataloader = DLDataLoader(valid_data_itrs, collate_fn=gen_col, batch_size=None, pin_memory=False, num_workers=0)\ndatabunch = TabularDataLoaders(train_dataloader, valid_dataloader)","37826be9":"embeddings = list(get_embedding_sizes(workflow).values())\nembeddings","aca73255":"model = TabularModel(emb_szs=embeddings, n_cont=len(con_names), out_sz=2, layers=[512, 256]).cuda()\nlearn =  Learner(databunch, model, loss_func = torch.nn.CrossEntropyLoss(), metrics=[accuracy], cbs=ProgressCallback())","1dc1497c":"from fastai.callback.schedule import fit_one_cycle","c25490d2":"train.head()","aef2c1aa":"from fastai.tabular.all import *","d566c07b":"learn.lr_find()","d923a04b":"learning_rate = 1.32e-2\nepochs = 5\nlearn.fit(epochs, learning_rate)\nfit_one_cycle(learn, n_epoch=epochs, lr_max=learning_rate)\n","371047a4":"gc.collect()","08d2aebc":"learn.lr_find()\n","41bbb411":"corr_mat = train[features].corr('spearman')","8afeb758":"select_f = corr_mat[abs(corr_mat)>=0.5]","79a10b74":"miss_X = train[miss_idx]\nmiss_mat = miss_X.corr()\nmiss_f = miss_mat[abs(miss_mat)>=0.5]","f9bc888f":"miss_f = pd.read_csv(\"..\/input\/janestreet\/miss_f.csv\")","d1ac4127":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\n\n\ndic.to_csv('dic.csv')\n\ndef create_download_link(title = \"Download CSV file\", filename = \"dic.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='dic.csv')","ad05d5e2":"import torch\nimport torch.nn as nn","71c0c93a":"gc.collect()","e021536c":"train = ","a5d2c4fb":"imputer = KNNImputer(weights=wt)","92d4ef6d":"train[features].isna().sum().sum()","62e8b50e":"gc.collect()","55674b39":"class missnet():\n    def __init__(self,x,y):\n        self.y = y\n        self.x = x\n        self.data = x.merge(y,left_index=True,right_index=True)\n        \n        \n    def forward(self):\n        knn_imputer = impute.KNNImputer()\n        knn_imputer.fit_transform(self.x)\n        train_y = self.y[self.y!=np.nan]\n        predict_y = self.\n        \n        \n        \n    ","c48ff772":"type(train.index)","1889c4c4":"train.ts_id","84ec659c":"train.set_index(train.ts_id,inplace=True)\n","65a1198f":"data = pd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\")","34e34e9b":"data.to_hdf(path_or_buf=\".\/data_hdf5_format\",key=\"data_hdf5_format\")","051b7d07":"\ngc.collect()","aaca53c7":"a=train.isna().sum().sort_values(ascending=False).to_dict()","37e9268b":"tag.shape","a54736d8":"a","b694b1aa":"gc.collect()","3b5ee574":"train.feature_12[:72]","c0e30012":"for y in miss_X:\n    \"X_{}\".format(y) = miss_f[y]","f6ee9088":"procs = [Categorify, FillMissing, Normalize]\nsplits = (L(np.arange(1000000), use_list=True),\n          L(np.arange(1000000, 1981287), use_list=True))","bddb17f1":"splits","f5de7473":"to = TabularPandas(train, procs, cat_col, cont_col, y_names=\"action\", splits=splits)\ndls = to.dataloaders(bs=16384)","3eadf88c":"# Below we define a neural net with 2 hidden layers to predict missing values","2c92edff":"# Implementing GMM"}}