{"cell_type":{"a9095d58":"code","f8ca62c7":"code","fec85ea1":"code","feef8b4a":"code","24cc8f7b":"code","2eed6900":"code","6af003b4":"code","44a47fb0":"code","0550000b":"code","e50f1156":"code","d80c21a4":"code","32f742cd":"code","cfe7aa0e":"code","55dc265c":"code","9e9bbb95":"markdown","217438b0":"markdown","227e09da":"markdown","0fb9c9e5":"markdown","50589ecb":"markdown","ce39c504":"markdown","c9b89d2a":"markdown","6e6fa534":"markdown","29b4c531":"markdown","c5ed7aa8":"markdown"},"source":{"a9095d58":"corpus = open('..\/input\/book-of-proverbs\/prove_en.txt',encoding='latin-1').read()\nprint('----- Here a small example of the book Chapter 1 ----')\nprint(corpus[1:258])\nprint(\"----------------------------------------------------\")","f8ca62c7":"import os \nimport tensorflow as tf\nimport numpy as np\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nprint(tf.__version__)\nprint(tf.config.experimental.list_physical_devices('GPU'))\nprint(tf.test.gpu_device_name())","fec85ea1":"tokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(corpus)\n[encoded] = np.array(tokenizer.texts_to_sequences([corpus]))","feef8b4a":"print(f\" --- Unique Numbers per letter : {list(tokenizer.index_word.items())[15:25]} ------\\n\")\n\nExample = 'for receiving instruction in prudent behavior'\nsequences = [6, 10, 5, 4, 7,14, 19, 4, 6, 3, 10, 1, 6, 10, 1, 20, 7, 14, 13, 2, 10, 4, 1, 22, 2,9, 8, 24, 6, 3,7]\nprint(f\"This is the example : -- {Example}--  \\nand this is text_into_sequences {tokenizer.texts_to_sequences([Example])}\\n\")\nprint(f\"This is the example : -- {sequences}--  \\nand this is sequences_into_texts -- {''.join(tokenizer.sequences_to_texts([sequences]))} --\")","24cc8f7b":"#this is just an example --< however you can take this length as the length of of your sequences\nsentence = '''for gaining wisdom and instruction;\n    for understanding words of insight;'''\nprint(f\"this is the length of each paragraph ------ {len(sentence)}\")","2eed6900":"os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n\ndef seq_target (seq):\n    X_ = seq[:-1]\n    y_ = seq[1:]\n    return X_,y_\n\n#Sequences Parameters\nseq_len = 80\nbatch_size = 128\nwindows_length = seq_len + 1\n\n#Transforming array into Tensor \ndataset = tf.data.Dataset.from_tensor_slices(encoded)\n\n#Creating sequences\ndataset = dataset.batch(windows_length, drop_remainder=True)\ndataset = dataset.map(seq_target)","6af003b4":"#Showing batches\nbatch_ = 0\nfor X_,y_ in dataset.take(2):\n    print(f\" X_train, batch #{batch_}\\n Sequences : {X_}\\n\\n X_train\\n text: {''.join(tokenizer.sequences_to_texts([X_.numpy()]))}\")\n    print(f\" y_train, batch #{batch_}\\n Sequences : {y_}\\n\\n y_train\\n text: {''.join(tokenizer.sequences_to_texts([y_.numpy()]))}\")\n    batch_ += 1","44a47fb0":"dataset = dataset.shuffle(1000).batch(batch_size, drop_remainder=True)","0550000b":"def customize_loss (y_true, y_pred):\n    return keras.losses.sparse_categorical_crossentropy(y_true,y_pred, from_logits=True)\n\ndef model(Input_dimen,out_dimen,batch_size):\n    model = keras.Sequential()\n    model.add(keras.layers.Embedding(input_dim = Input_dimen , output_dim = out_dimen, \n                                     batch_input_shape = [batch_size, None]))\n    model.add(keras.layers.LSTM(1024,return_sequences=True,stateful =True,  \n                                recurrent_initializer='glorot_uniform'))\n    model.add(keras.layers.GRU(500, return_sequences = True,\n                                stateful=True , recurrent_initializer='glorot_uniform'))\n    model.add(keras.layers.GRU(300, return_sequences = True,\n                                stateful=True , recurrent_initializer='glorot_uniform'))\n    model.add(keras.layers.TimeDistributed(keras.layers.Dense(Input_dimen)))\n    \n    model.compile(optimizer = 'adam' , loss = customize_loss)\n    return model","e50f1156":"## Parameters\nunique_words = sorted(set(corpus))\nlength_voc = len(unique_words)\n\nInput_dimen = length_voc #Number of unique Words \nout_dimen = 45 #this is another hyperparameter as rule of thomb out_dim must be lesser than input_dim and lesser than number of variables\nbatch_size = 128 \n\nModel = model(Input_dimen,out_dimen,batch_size)\nhistory = Model.fit(dataset, epochs = 500, verbose = 0)\nprint(\"----------------------Training----------------------\")\n\n#Saving weigths\nweights = Model.get_weights()","d80c21a4":"import plotly.graph_objects as go\nimport numpy as np\n\nx = np.arange(10)\n\nfig = go.Figure(data=go.Scatter(y=history.history['loss'], x=list(np.arange(1,501))))\nfig.update_layout(title = 'Loss Per Epochs')\nfig.show()","32f742cd":"generator = model(Input_dimen,out_dimen,batch_size=1)\ngenerator.set_weights(weights)\ngenerator.build(tf.TensorShape([1,None])) #changing Input","cfe7aa0e":"def generate_text(model_,start_seed, gen_size = 500 , temp = 1.0):\n    num_generate = gen_size\n    input_ = np.array(tokenizer.texts_to_sequences(start_seed))\n    input_ = tf.reshape(input_,[1,3]) #changing the tensor shae\n    input_ = tf.cast(input_,  tf.float32)\n\n    text_generated = []\n    temperature = temp\n    model_.reset_states()\n\n    for i in range(num_generate):\n        predictions = model_(input_)\n        predictions = tf.squeeze(predictions,0)\n        predictions = predictions\/temperature\n\n        predicted_id = tf.random.categorical(predictions,num_samples = 1)[-1,0].numpy()\n        input_ = tf.expand_dims([predicted_id],0)\n        text_generated.extend(tokenizer.sequences_to_texts([[predicted_id]])) #predicting and joining\n    return (start_seed+\"\".join(text_generated))","55dc265c":"print(generate_text(generator ,'Son', gen_size=1500))","9e9bbb95":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#a6a000 ;\n       font-size:100%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 15px;\n          color:white;\">\n        <b> 2.1 |<\/b>  -  Tokenization\n    <\/p>    \n<\/div>","217438b0":"3. If you are familiar with Machine Learning \/ Deep Learning you know that Computer ***cannot process text*** so we have to find a way to transoform that text into numbers, but no worries someone else already found the solution, actually they are 3, ***these are really simplified*** example because I am not talking about the space dimmensionality:\n- [OneHotEconding](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) which consist on giving bunch of 0 and 1 to a letter for example A = [0,0,0,1] ,B = [0,0,0,1] where each letter has unique a combination of 0,1.\n- [Text tokenization](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer) each word has a unique number for example Dog = 1 , House = 1528 etc\n- [Word2Vec](https:\/\/www.tensorflow.org\/tutorials\/text\/word2vec) which consist on giving a unqie vector to each word, for example Dog = [0.1,0.5,0.3] , Car = [0.89,0.25.0.36] \n\n### How Do we do that? \n#### We use Tensorflow\nAs showed below we will use Tensorflow Tokenizer, notice that the option ***char_level*** must be True, the reason [instead of splitting a text into words, the splitting is done into characters, for example, smarter becomes s-m-a-r-t-e-r](https:\/\/towardsdatascience.com\/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9) which is what I (explanation later).\n- we call the **library**, then **char_level** = ***True*** and finally **fit_on_texts**\n- next we ahve to encode or transform text into sequences:\n        -text_to_sequences: consist on assign unique numbers to each letter for example a=1,b=2....z=25 and then transform the text = dog -> [4 15 7]\n        -sequences_to_text: is the opposite, transform number into letter like this [ 8 5 12 12 15 ] = H e l l o\n- ***Solo Will show the words that are in the training set (corpus)  only***\n        \n#### Some Examples Here","227e09da":"Now we will take that huge array of 85256 elements and create n-sequences of aprox 80 or 75 --up to you -- ***I chose 80 elements by sequence***\n\nNow lets create some sequences and also create what we call ***X_train*** and ***y_train***\n\nDo you remember our sequences -- ***The proverbs of Solomon son of David*** = *** [1,2,3,4,5,6,7,8,3,9,10,4,10,6,11,4] *** , here is when we use shift = 1  \n   -  Basically X_train will be from position 0 to 80 and y_train will be from position 2 to 81 and so on \n    - now for simplicty i will use this shift = 1  , and window = 4 and batch = 4.. \n        - sequences = X_train = [1,2,3,4] (The), y_train = [2,3,4,5] = 'he ' (notice that we have the space there) and here the target would consist on predictin = the space\n        - sequences = X_train = [2,3,4,5] ( pr), y_train = [3,4,5,6] = 'prov'and here the target would consist on predicting 'v'\n        - sequences = X_train = [3,4,5,6] (prov), y_train = [4,5,6,7] = 'rove'and here the target would consist on predicting 'e'\n    - Now lets create batches:\n        - after creating sequences we will have [[1,2,3,4],[2,3,4,5]],[[2,3,4,5],[3,4,5,6]]... in total 13 sequences for X_trian and y_train if you dont believe then count 4 element with one setp forward other wise (Create your own for loop and find the number of sequences), ok I have ***26 Sequences*** and want 4 batches then 26 \/ 4 = **6.5** ~ 6 then we will have 4 batches and each batch will have 12 sequences , 6 for X_train and 6 for y_train\n   \n### Let's do it in tensorflow\n\n- We call Tensorflow to create batch\n- then we create sequences \n- Finally we print the sequences and notice that batch 2  has 1 more letter than batch 1.\n         ","0fb9c9e5":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#a6a000 ;\n       font-size:100%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 15px;\n          color:white;\">\n        <b> 3 |<\/b>  -  Conclusion \n    <\/p>    \n<\/div>\n\nIt was really nice to learn how to predict text using RNN, but also challenging because dealing with tensor is something new to me:\n***insights***\n- Remeber that this model will only generate words that are in the input text\n- It is amazing to notice some patterns that the model learned such as verse number, commas , point, and spaces.\n- The larger the corpus is, the better result\n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1619038\/2661735\/Woman%20Running%20Photo%20Twitter%20Header%20%281%29.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211011%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211011T044916Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=4bba3d30eedfea172242c493e54061b42791e1aa0285fe60fc3bf46842961f2299265297deac197ca0ac6aead5b33f018354375ef9ce5e5a83ff4d84e3274c8085b2169a473ce33d870f31c3fdf42b76b018951b0517cee653ccd3dcff4504a9630bd1b4b686c2f74f9d514b8d5bbd1e5e2c51918268407b604f1647f394db9d473abeb12097eac7004c8aaedf85bdb65ef72c82c471af6e893bd3bea2ba59e3f3e3afa0f34a088c3ff4539b1af956b3456c6368405a13a4447a3f70b683ab973e3f514e2600fe5e2d8655bca306f42c8ea466a6d750db300c21eb58026ab456f0416c787f602a04aa777464da16931a262bc28c215d62e955720f219437576e\" style=\"float:right;\" alt=\"Drawing\"\/>\n","50589ecb":"<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1619038\/2661735\/Woman%20Running%20Photo%20Twitter%20Header.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211011%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211011T044921Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=8561c6c8dbcb73bdf8661986c079d74649a1e0aeca4f432e07ca82ee0d146561384d1d3857ffd33643979ca07cd7850c55dd1c30be56dfcb7b7c1f994f2eb2c12718c1edad39df99a76e5b02f1e43c6aef5235809d600a66d81ef5817f16140de86d7755916e7fdedf505f79c4b2eb21defeec7e95568e139544e36b59a3512f9f421ff50b4f28d86f7cf43b36e516bb67e3e9037b720d2a5077249a39bb467c59dac44b656ba9c2c28e2103f75ba60f304d52f093f743a02e6dacc1381407559bd5c79bc5a2e15c5dcd724b72befd9e2e959ac45fcf983f2446785912399225d117e5b6dfa7d2a69380b219292eca0380a0fb3f99324847bb8b1d9b744dfa03\" style=\"float:right;\" alt=\"Drawing\"\/>\n\n","ce39c504":"### How does Sequences work?\nWe must define what is sequence: [In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed and order matters. Like a set, it contains members. The number of elements is called the length of the sequences](https:\/\/en.wikipedia.org\/wiki\/Sequence) reprhasing this i would say sequences are like lines or a bunch of number together in groups that has the same number of items \n- take these sentences as example \n  - sentence = ***for gaining wisdom and instruction;\n    for understanding words of insight***\n    - this sentences is a sequence with 68 letter (length)\n    - from the previous sentence we can take these 2 sequences. ***[for gaining wisdom and instruction]*** with 34 letters ,and  ***[for understanding words of insight]*** with 33 letters\n    - and then even 4 sequences : ***[for gaining wis] ,[dom and instruction],[for understading], [words of insigth]***\n    \nAnother important characteristics of sequences are **shift**,and  windows (length) , we already mentioned length but now we must define shift\n- Shift is jut how many step in the future you want to use. Step in the future.. whatt ??\n    - Same example as before : ***for gaining wisdom and instruction*** if we want to use this sentence to train Solo we must create a sequence and another sequence + shift, let say I define shfit as 1 (some people call it n_step honestly it doesnt matter as long as you understand the concept)\n        - Ok , let's create sequences with len\/window = 3 and shift = 1 (one step in the future) from ***for gaining wisdom and instruction***\n            - sequence - > **[for]** ; sequence + shift - > ***[for ]*** ,  sequence - > **[or ]** ; sequence + shift - > ***[or g]***, sequence - > **[r g]** ; sequence + shift - > ***[ gai]***\n            - sequence - > **[gai]** ; sequence + shift - > ***[gain]*** ,  sequence - > **[ain]** ; sequence + shift - > ***[aini]***, sequence - > **[ini]** ; sequence + shift - > ***[inin]***\n            - another example is ***hello how are you **** , ***hello how*** --> ***hello how a*** ----> ***hello how ar*** ----> ***hello how are*** etc etc etc .\n            \n### Now letters into numbers\n\nAs mentioned before we use ***Tokenizaition*** to transformed letters into numbers , now the ***2nd step*** would be **Create batches and Sequences** so let's do it","c9b89d2a":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#a6a000 ;\n       font-size:100%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 15px;\n          color:white;\">\n        <b> 2.2 |<\/b>  -  Batches\n    <\/p>    \n<\/div>\n\n***Batches*** here [you have a formal explanation\/definition](https:\/\/machinelearningmastery.com\/difference-between-a-batch-and-an-epoch\/) but basically is to divide our number of samples and create group of n-elements, lets take an example \n\n**For simpliclity** I will use this sentence -- The proverbs of Solomon son of David -- and i will asume the following sequences is this ***= [1,2,3,4,5,6,7,8,3,9,10,4,10,6,11,4,and so on ....]***\n\nSo here we have a sample composed of 16-elements, now let's create batches.\n- Frist, we have to define how many batches -- if I want 2 then there will be two list of ***8 elements each*** if I want ***3*** I would have 3 batches of ***5 elements each*** and the one that is let will be deleted\n    - 2 batches of 8 elements [1,2,3,4,5,6,7,8,] , [3,9,10,4,10,6,11,4]\n    - 3 batches of 5 elements [1,2,3,4,5] , [6,7,8,3,9] , [10,4,10,6,11] 4 will be dropped\n    \n### But.....\n\nwe had a huge array of 85256 letters -- you can use [tokenizer.document_count](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer) to find the total number of letters --- thefore we must create sequences first , which I explained before we have to define the window = length of sequences and shift:\n\n#### How can I define the window and shift?\n\nI would like to say that ***that depends on*** you text: as you notice in the sample that we take from Chapter one we noticed that basically the author wrote two sentences per paragraph:\n- So let's take the average letter which is TOTAL NUMBER OF WORDS \/ NUMBER OF PAIR OF SENTENCES BY CHAPER (taking chapter one as example).. if you do it you will realize that the average length is 79 ~ 80 so i will use 80 (chec the code below)","6e6fa534":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#a6a000 ;\n       font-size:200%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b> 1 |<\/b> Introduction - A wise AI - \n    <\/p>    \n<\/div>\n\n### **<span style='color:#a6a000'>1.1<\/span> | Interesting Facts**\n\nLet's star this notebook with some definitions about **writting** from wikipedia [\"Writing is a medium of human communication that involves the representation of a language with written symbols\"](https:\/\/en.wikipedia.org\/wiki\/Writing), but why writting?, because in this notebook We will build a GRU-LSTM deep Neural Networl that will ***learn, find pattern and generate text***.\n\nYou might hear about ***GPT-Neo*** the amazing AI model which is capable of writing strikingly coherent articles in English when given a text prompt, if it's not check this [article](https:\/\/www.wired.com\/story\/ai-generate-convincing-text-anyone-use-it\/), as mentioned previously, in this notebook we will go step by step and learn ***how generate text using AI*** , We will use LSTM, GRU both are variation of recurrent neural network (RNN) but,\nbefore moving foward it is important to **highlight and define some expectations** we wont build a super-hyper-mega-AI able to write your Master's thesis or PhD dissertation. \n   - From this point I will name this AI as **\"Salo\"**.\n   - Salo will find some pattern (**it will be explained later**), learn grammar, and then show some text.\n   - Salo's Vocabulary is not limited to the words including in the corpus (training text) so do not expect words like AI, Keras, verbatim, etc. \n   \n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#a6a000 ;\n       font-size:200%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:white;\">\n        <b> 2 |<\/b> Working with Sequences - step by step \n    <\/p>    \n<\/div>\n   \n### **<span style='color:#a6a000'>2.1<\/span> | Salo. What or Who and How will it learn?**\n\n## What? \nSalo is a Deep learning neural network (LSTM, GRU) which learn how to recognize some patterns, but how can it learn those patterns?, take this two sentences as an example: -- **\"Let's star this notebook with some definitions\"** and **\"before moving foward it is important to highlight and define some expectations\"** -- , are you able to find some patterns? if it's not, dont worry , We can help you:\n  - As you can notice there are spaces between words -- Let's**<-Space->**star**<-Space->**this**<-Space->**notebook --\n  - Now let's count each word in the first sentence 'l','e',''','s','<-space->',.....,'t','i','o',n' if we sum up we get 45, this means that usually the senteces are composed of 45 letters, and right there we have another pattern:  4 letters (aprox) <-Space-> 4 letter : -- L e t ' s <-space-> s t a r <-space-> t h i s <-space-> ... --\n  - Finally  **some** + word must have **s** for example ***some*** expectation***s*** , ***some*** definition***s***\n  - We can keep thinking of all those possible patterns such as consonant + vowel , points, number etc. but in reality that is not pratical but Salo has kaggle TPU, GPU and many more computational power than our brain so it doesnt mind.\n\n## How?\n\nWell if ***you are a deep learning practitioner you can skip this part***, other wise , let's get into the topic.\n1. **First**, We need to find a large text - corpus (the bigger the better) in my case I am using [Book_of_Proverbs](https:\/\/en.wikipedia.org\/wiki\/Book_of_Proverbs) by ***Solomon, Son of David*** feel free to use any that you like.\n2. Can you find some patterns in the section (take a look of the text below) ?\n    - notice that after **';'** a new sentence starts.\n    - each sentence has 34 letters aprox.","29b4c531":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#a6a000 ;\n       font-size:100%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 15px;\n          color:white;\">\n        <b> 2.4 |<\/b>  -  Predicting \n    <\/p>    \n<\/div>\n\n- One of the disadvantages of RNN with *** stateful = True *** is the fact that they learn and train with fix input dimensions, to generate text we had to copy the previos model weights , create a new model and ***change the number of batch from 128 to 1***","c5ed7aa8":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#a6a000 ;\n       font-size:100%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 15px;\n          color:white;\">\n        <b> 2.3 |<\/b>  -  Model \n    <\/p>    \n<\/div>\n<div\n<body>\n<img src=\"https:\/\/s167.daydaynews.cc\/?url=http%3A%2F%2Fp9.pstatp.com%2Flarge%2Fpgc-image%2F23ed45a175ee45f795a77f3706a22563\" style=\"float:right;\" alt=\"Drawing\"\/>\n<h3 style=\"float:left;\">Model and Training<\/h3>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<\/p>\n    <p>The following is a representaiton of LSTM cell during training, of course the math behind is a serious thing but check how the information goes through all the gates,\n    In this model we will use LSTM, GRU and DENSE layer the reason why? because everyone is free to make their own model, in Deep learning the tunning part has to do with how many neurons you use, no one can tell you why , or how many it just a try and error thing.<\/p>\n    <p>My architecture will be Embedding layer then LSTM followed by GRU and finally Timedistributed Dense layers<\/p>\n    <p>Some importan thing to considered is : this will be a model with fix input dimensions,I will train this model with stateful gate = True, and use Sparse categotical crossentropy,sadly I wont explain those concepts but here you have amazing references about <a href = 'https:\/\/stats.stackexchange.com\/questions\/326065\/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other'>Cross Entropy vs. Sparse Cross Entropy: When to use one over the other<\/a> , and <a href = 'https:\/\/datascience.stackexchange.com\/questions\/26663\/when-to-use-stateful-lstm'>When to use Stateful LSTM?<\/a>, <a href = 'https:\/\/datascience.stackexchange.com\/questions\/26663\/when-to-use-stateful-lstm'>What is an embedding layer in a neural network?.<\/a> <\/p>\n    <p><a href=\"https:\/\/stats.stackexchange.com\/questions\/182775\/what-is-an-embedding-layer-in-a-neural-network\">The animation was created and Published at 2019-12-22 18:50:06, by DayDayNews<\/a><\/p>\n<\/body>\n<\/div>\n\n# How does it work ?\n\nFor a more formal explanation you can check [LSTM Networks - EXPLAINED!](https:\/\/www.youtube.com\/watch?v=QciIcRxJvsM) which is an amazing video explaining this topic in full and showing an example. if not let me give you a simple example :\n\n- If I show you -- **[1 -> 2 -> 3 -> 4 -> 5 ->]** -- , --**[-> 1 -> 2 -> 3 -> 4 -> 5 -> 6]**--, -- **[->1 -> 2 -> 3 -> 4 -> 5 -> 6 -> 7 -> what is the next number? --]** of course we can guess that the next number is 8 because it has a pattern of n+1.\n- How about this one -- **[1 -> 2 -> 5]** --, -- **[2 -> 3 -> 6]** -- , --**[3 -> 4 -> 7]--**, **[-> 4 -> 5 -> 8**]-- can you see the patter (n+1,n+2, m+3 ,where m =2)\n- And finally **whirlpool preschool estradio albuterol decontrol** what is the next word if I star with ***r*** ? well int his case we have to check the letter that follow R in this case --whi**rl**pool--,--p**re**school--, --est**ra**dio-- , --albute**ro**l -- , --decont**ro**l-- a quick answer will be the **\"o\"** has higher probabilities based on those words then:\n    - the our word is **ro** and what is the next letter? well we repeat the same process , and this is a ***quick example*** and very very simplified version about how RNN- LSTM works\n\n\n\n\n\n"}}