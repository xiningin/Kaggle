{"cell_type":{"10dbb839":"code","40a020ac":"code","0cef8991":"code","e3515298":"code","d1c89197":"code","20c51548":"code","adb771d1":"code","a03f9ed5":"code","10b9aff9":"code","0473f7c3":"code","a926d02c":"code","a034670d":"code","e9010335":"code","f6aa4582":"code","78b5a309":"code","c622d19c":"code","1c8c5edd":"code","701658a4":"code","63939303":"code","88108d3a":"code","1f783513":"code","764ac339":"code","56cd7ad8":"code","f8463182":"code","22d24ff8":"code","0a5d282f":"code","9e0bf4d0":"code","5890f557":"code","1b1074ce":"code","5bcc1a7a":"code","add4c7ba":"code","4b18c5c5":"code","59aacd41":"code","341e3e50":"code","ff8cc377":"code","4389edd0":"code","31ef68e3":"code","3bf6f95c":"code","e71810ee":"code","1d052c1a":"code","b496a95e":"code","264d79bb":"code","0fcb2699":"code","1f924d0b":"code","cbc6c055":"code","8cf328a5":"code","28c89dcc":"code","3b45a73c":"code","23ae9f74":"code","6bbef003":"code","5da01db3":"code","87eeba4e":"code","611660a5":"code","7a796803":"code","4a874ee6":"markdown","6779c3dd":"markdown","17968439":"markdown","97dea68d":"markdown","727f0225":"markdown","cc5f2cba":"markdown","061149d0":"markdown","b5fec018":"markdown","2728269d":"markdown","25c5bb08":"markdown","419bc77c":"markdown","34319776":"markdown","1b9ab2d2":"markdown","ef9125d4":"markdown","f702ea2e":"markdown","709e46e8":"markdown","9386830a":"markdown","de970af2":"markdown","71342400":"markdown","44ba81f8":"markdown","6555a16f":"markdown","16f2d5af":"markdown","bc7800f2":"markdown","d0208390":"markdown","d321f18c":"markdown","8e6a758a":"markdown"},"source":{"10dbb839":"# importing libraries\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","40a020ac":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n","0cef8991":"#show the top five columns in train data\ntrain.head()","e3515298":"#illustrate the information of train and test data\ntrain.info()\nprint('*'*60)\ntest.info()","d1c89197":"train.describe()","20c51548":"train.describe(include='O')","adb771d1":"def missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","a03f9ed5":"#missing values in train data\nmissing_percentage(train)","10b9aff9":"#missing values in test data\nmissing_percentage(test)","0473f7c3":"train=train.drop(['Cabin'],axis=1)\ntest=test.drop(['Cabin'],axis=1)","a926d02c":"train=train.drop(['PassengerId', 'Ticket'],axis=1)\n#for PassengerId in test we can not drop it because we use it in submission\ntest=test.drop(['Ticket'],axis=1)","a034670d":"train['Title'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest['Title']  = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(train['Title'], train['Sex'])\ntest.head()","e9010335":"for i in [train,test]:\n  i['Title'] =i['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n  'Don', 'Major', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n  i['Title'] = i['Title'].replace('Mlle', 'Miss')\n  i['Title'] = i['Title'].replace('Ms', 'Miss')\n  i['Title'] = i['Title'].replace('Mme', 'Mrs')\n  i['Title'] = i['Title'].replace('Dr', 'Mrs')\n  i['Title'] = i['Title'].replace('Rev', 'Mrs')\n\n\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","f6aa4582":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in [train,test]:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","78b5a309":"train.head()","c622d19c":"train = train.drop(['Name'], axis=1)\ntest = test.drop(['Name'], axis=1)\n","1c8c5edd":"for dataset in [train,test]:\n    dataset['Sex'] = dataset['Sex'].map( {'male': 0 , 'female': 1} ).astype(int)\n","701658a4":"train.head()","63939303":"train['Embarked'].dropna().unique()","88108d3a":"Emb_mode = train.Embarked.dropna().mode()[0]\nEmb_mode","1f783513":"train['Embarked']=train['Embarked'].fillna(Emb_mode)\ntest['Embarked']=test['Embarked'].fillna(Emb_mode)","764ac339":"train['Embarked'] = train['Embarked'].map( {'S': 1, 'C': 2, 'Q': 3} ).astype(int)\ntest['Embarked'] = test['Embarked'].map( {'S': 1, 'C': 2, 'Q': 3} ).astype(int)\n","56cd7ad8":"train.head()","f8463182":"test[test.Fare.isnull()]","22d24ff8":"missing_value = test[(test.Pclass == 3) & \n                     (test.Embarked == 1) & \n                     (test.Sex == 0)].Fare.mean()\nprint(missing_value)\n## replace the test.fare null values with test.fare mean\ntest['Fare']=test['Fare'].fillna(missing_value)","0a5d282f":"train['FareBand'] = pd.qcut(train['Fare'], 5)\ntrain[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","9e0bf4d0":"for data in [train,test]:\n  data.loc[ data['Fare'] <= 7.854\t, 'Fare'] = 0\n  data.loc[(data['Fare'] > 7.854) & (data['Fare'] <= 10.5), 'Fare'] = 1\n  data.loc[(data['Fare'] > 10.5) & (data['Fare'] <= 21.679), 'Fare']   = 2\n  data.loc[(data['Fare'] > 21.679) & (data['Fare']<=39.688), 'Fare'] = 3\n  data.loc[data['Fare']>39.688,'Fare']=4\n  data['Fare']=data['Fare'].astype(int)   \ntrain.head(20)","5890f557":"#dropping FareBand feature\ntrain=train.drop(['FareBand'],axis=1)","1b1074ce":"train.head()\n","5bcc1a7a":"train.info()","add4c7ba":"# Let's look at the his\nplt.subplots(figsize = (22,10))\nsns.distplot(train.Age, bins = 100, kde = True, rug = False, norm_hist=False);\n","4b18c5c5":"train['Age']=train['Age'].fillna(30)\ntest['Age']=test['Age'].fillna(30)","59aacd41":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='g',shade=True, label='survived')\nplt.title('Surviver and Non Survivors with different Ages', fontsize = 25, pad = 40)\nplt.xlabel(\"Age\", fontsize = 15, labelpad = 20)\nplt.ylabel('Frequency', fontsize = 15, labelpad= 20);","341e3e50":"for data in [train,test]:    \n    data.loc[ data['Age'] <= 10, 'Age'] = 0\n    data.loc[(data['Age'] > 10) & (data['Age'] <= 25), 'Age'] = 1\n    data.loc[(data['Age'] > 25) & (data['Age'] <= 35), 'Age'] = 2\n    data.loc[(data['Age'] > 35) & (data['Age'] <= 60), 'Age'] = 3\n    data.loc[ data['Age'] > 60, 'Age']\n","ff8cc377":"train.head()","4389edd0":"for data in [train,test]:\n    data['FamilySize'] = data['SibSp'] + dataset['Parch'] + 1\n\ntrain[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","31ef68e3":"for data in [train,test]:\n    data['IsAlone'] = 0\n    data.loc[data['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","3bf6f95c":"train = train.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest = test.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)","e71810ee":"train.head()","1d052c1a":"test.head()","b496a95e":"X = train.drop(\"Survived\", axis=1)\nY = train[\"Survived\"]","264d79bb":"X.shape, Y.shape, test.shape","0fcb2699":"test.head()","1f924d0b":"# Import Libraries\nfrom sklearn.model_selection import train_test_split\n#----------------------------------------------------\n\n#----------------------------------------------------\n#Splitting train data into train and test\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=44, shuffle =True)\n\n#Splitted Data\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","cbc6c055":"#Import Libraries\nfrom sklearn.neural_network import MLPClassifier\n#----------------------------------------------------\n\n#----------------------------------------------------\n#Applying MLPClassifier Model \n\n\nMLPClassifierModel = MLPClassifier(activation='tanh', # can be also identity , logistic , relu\n                                   solver='adam',  # can be also sgd , adam ,lbfgs\n                                   learning_rate='constant', # can be also invscaling , adaptive\n                                   early_stopping= False,\n                                   alpha=0.0001,random_state=33)\nMLPClassifierModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('MLPClassifierModel Train Score is : ' , MLPClassifierModel.score(X_train, y_train))\nprint('MLPClassifierModel Test Score is : ' , MLPClassifierModel.score(X_test, y_test))\n#print('MLPClassifierModel loss is : ' , MLPClassifierModel.loss_)\n#print('MLPClassifierModel No. of iterations is : ' , MLPClassifierModel.n_iter_)\n#print('MLPClassifierModel No. of layers is : ' , MLPClassifierModel.n_layers_)\n#print('MLPClassifierModel last activation is : ' , MLPClassifierModel.out_activation_)\n#print('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = MLPClassifierModel.predict(X_test)\ny_pred_prob = MLPClassifierModel.predict_proba(X_test)\n#print('Predicted Value for MLPClassifierModel is : ' , y_pred[:10])\n#print('Prediction Probabilities Value for MLPClassifierModel is : ' , y_pred_prob[:10])","8cf328a5":"#Import Libraries\nfrom sklearn.ensemble import RandomForestClassifier\n#----------------------------------------------------\n\n#Applying RandomForestClassifier Model \n\n\nRandomForestClassifierModel = RandomForestClassifier(criterion = 'gini',n_estimators=100,max_depth=4,random_state=33) #criterion can be also : entropy ,gini\nRandomForestClassifierModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('RandomForestClassifierModel Train Score is : ' , RandomForestClassifierModel.score(X_train, y_train))\nprint('RandomForestClassifierModel Test Score is : ' , RandomForestClassifierModel.score(X_test, y_test))\n#print('RandomForestClassifierModel features importances are : ' , RandomForestClassifierModel.feature_importances_)\n#print('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = RandomForestClassifierModel.predict(X_test)\ny_pred_prob = RandomForestClassifierModel.predict_proba(X_test)\n#print('Predicted Value for RandomForestClassifierModel is : ' , y_pred[:10])\n#print('Prediction Probabilities Value for RandomForestClassifierModel is : ' , y_pred_prob[:10])","28c89dcc":"#Import Libraries\nfrom sklearn.svm import SVC\n#----------------------------------------------------\n\n#----------------------------------------------------\n#Applying SVC Model \n\nSVCModel = SVC(kernel= 'rbf',# it can be also linear,poly,sigmoid,precomputed ,rbf\n               max_iter=200,C=1.0,gamma='auto')  \nSVCModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('SVCModel Train Score is : ' , SVCModel.score(X_train, y_train))\nprint('SVCModel Test Score is : ' , SVCModel.score(X_test, y_test))\n#print('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = SVCModel.predict(X_test)\n#print('Predicted Value for SVCModel is : ' , y_pred[:10])","3b45a73c":"# Import Libraries\nfrom sklearn.neighbors import KNeighborsClassifier\n#----------------------------------------------------\n\n#----------------------------------------------------\n#Applying KNeighborsClassifier Model \n\nKNNClassifierModel = KNeighborsClassifier(n_neighbors= 4,weights ='uniform', # it can be distance,uniform\n                                          algorithm='kd_tree') # it can be ball_tree, kd_tree,brute,auto\nKNNClassifierModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('KNNClassifierModel Train Score is : ' , KNNClassifierModel.score(X_train, y_train))\nprint('KNNClassifierModel Test Score is : ' , KNNClassifierModel.score(X_test, y_test))\n#print('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = KNNClassifierModel.predict(X_test)\ny_pred_prob = KNNClassifierModel.predict_proba(X_test)\n#print('Predicted Value for KNNClassifierModel is : ' , y_pred[:10])\n#print('Prediction Probabilities Value for KNNClassifierModel is : ' , y_pred_prob[:10])","23ae9f74":"#Import Libraries\nfrom sklearn.tree import DecisionTreeClassifier\n#----------------------------------------------------\n\n#----------------------------------------------------\n#Applying DecisionTreeClassifier Model \n\nDecisionTreeClassifierModel = DecisionTreeClassifier(criterion='entropy',max_depth=3,random_state=44) #criterion can be entropy\nDecisionTreeClassifierModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('DecisionTreeClassifierModel Train Score is : ' , DecisionTreeClassifierModel.score(X_train, y_train))\nprint('DecisionTreeClassifierModel Test Score is : ' , DecisionTreeClassifierModel.score(X_test, y_test))\n#print('DecisionTreeClassifierModel Classes are : ' , DecisionTreeClassifierModel.classes_)\n#print('DecisionTreeClassifierModel feature importances are : ' , DecisionTreeClassifierModel.feature_importances_)\n#print('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = DecisionTreeClassifierModel.predict(X_test)\ny_pred_prob = DecisionTreeClassifierModel.predict_proba(X_test)\n#print('Predicted Value for DecisionTreeClassifierModel is : ' , y_pred[:10])\n#print('Prediction Probabilities Value for DecisionTreeClassifierModel is : ' , y_pred_prob[:10])","6bbef003":"models = pd.DataFrame({\n    'model' : ['MLPClassifier','RandomForestClassifier','SVC','KNeighborsClassifier',' DecisionTreeClassifier'],\n    'train_score' : [MLPClassifierModel.score(X_train, y_train) , RandomForestClassifierModel.score(X_train, y_train) ,\n                   SVCModel.score(X_train, y_train) , KNNClassifierModel.score(X_train, y_train) , \n                   DecisionTreeClassifierModel.score(X_train, y_train)\n                   ],\n    'test_score' : [MLPClassifierModel.score(X_test, y_test) , RandomForestClassifierModel.score(X_test, y_test) ,\n                   SVCModel.score(X_test, y_test) , KNNClassifierModel.score(X_test, y_test) , \n                   DecisionTreeClassifierModel.score(X_test, y_test)]\n})\nmodels.sort_values(by='test_score',ascending=False)","5da01db3":"test_Id=test['PassengerId']\ntest=test.drop('PassengerId', axis=1)","87eeba4e":"submission = pd.DataFrame({\n        \"PassengerId\": test_Id,\n        \"Survived\":  RandomForestClassifierModel.predict(test)\n    })","611660a5":"submission.head()","7a796803":"submission.to_csv('submission.csv', index=False)","4a874ee6":"Frist we will import the important Libraries and algorthim that helps us in our nootbook\n","6779c3dd":"# loading data\n\nThe Python Pandas packages helps us work with our datasets.","17968439":"we will show some statistical feature of our numerical train data","97dea68d":"we will replace the nan values to 's'","727f0225":"# feature engineering\nNow Name feature in this case is unuseful,so we want to make some feature engineering to extract title from it, we can make this by regular expression","cc5f2cba":"# importing libraries","061149d0":"For fare feature there is one null value in test data,we will fill it","b5fec018":"We can convert all categorical features to numerical values. ","2728269d":"if we want to show statistical feature of categorical train data","25c5bb08":"From the this graph we will fit the Age into five ranges","419bc77c":"We can convert all categorical features to numerical values in the Embarked feature","34319776":"we will split the train data into train and test to see if there is an overfitting in our training model","1b9ab2d2":"# submission","ef9125d4":"# Dealing with missing values","f702ea2e":"As shown above there are many types of data:\n\ncategorical:  Sex,Survived, and Embarked. Ordinal: Pclass.\nWhich features are numerical?\n\nnumerical\n\nContinous: Age, Fare. Discrete: SibSp, Parch.","709e46e8":"\nAfter we replaced all titles to few titles we can convert these categorical titles to ordinal(numeric)","9386830a":"We will make a dataframe to show the algorithms and the score of train and test data","de970af2":"Now the SibSp and Parch columns are considered the size of family and it is sured that if passenger is alone it is affected that if he survived or not","71342400":"As we saw Cabin feature have 78% from it is null and very difficult to fill it.\n\n So it is the best to git rid of from it","44ba81f8":"After we finished Name feature and print the head of train data,we want to deal with the Embarked feature\n\n","6555a16f":"Let us deal with Age feature","16f2d5af":"Now we take what we want from Name feature and we can drop it","bc7800f2":"As we see from this graph the most passengers age is in 20 to 40 years so we will fill the null values by (20+40)\/2 =30 years","d0208390":"# Modeling the Data\n","d321f18c":"**I hope you have benefited from this code**","8e6a758a":"There is another feature should be droped because it has no effect like PassengerId and Ticket.\n\n It helps us to avoide overfitting"}}