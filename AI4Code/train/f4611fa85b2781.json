{"cell_type":{"99701519":"code","fc335be7":"code","b1385b90":"code","eedbb5db":"code","b161dc5d":"code","4bdaf833":"code","43a6c734":"code","8f306002":"code","63d7bc63":"code","fcbe18eb":"code","d813ec5e":"code","3a4a8573":"code","fb983824":"code","cacbd9a3":"code","d452443c":"code","0218ed07":"code","1499a5ed":"code","5775bc63":"code","9651229d":"code","bf79c354":"code","92d80071":"code","03f4d9fd":"code","89367ad3":"code","fd3aceca":"code","73cfbf15":"code","132e6e67":"code","09fd3159":"code","32f90c02":"markdown","1eaac98f":"markdown","c73e6a83":"markdown"},"source":{"99701519":"import pandas as pd\nimport numpy as np","fc335be7":"test=pd.read_csv('..\/input\/summeranalytics2020\/test.csv')\ntrain=pd.read_csv('..\/input\/summeranalytics2020\/train.csv')","b1385b90":"train.describe().T","eedbb5db":"train.info()","b161dc5d":"train.isnull().sum()","4bdaf833":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()","43a6c734":"plt.figure(figsize=(15,5))\nsns.boxplot(y='Attrition',data=train,x='MonthlyIncome',orient='h')","8f306002":"train[train.MonthlyIncome>12500].Attrition.value_counts()","63d7bc63":"plt.figure(figsize=(15,5))\nsns.countplot(hue='Attrition',data=train,x='Age')","fcbe18eb":"plt.figure(figsize=(15,5))\nsns.countplot(hue='Attrition',data=train,x='TotalWorkingYears')","d813ec5e":"plt.hist(np.sqrt(train['TotalWorkingYears']))","3a4a8573":"sns.boxplot(y='Attrition',data=train,x='YearsInCurrentRole',orient='h')","fb983824":"# Normalising Data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()","cacbd9a3":"train[['Age','DistanceFromHome','EmployeeNumber','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager','MonthlyIncome','TotalWorkingYears','YearsAtCompany']]=scaler.fit_transform(train[['Age','DistanceFromHome','EmployeeNumber','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager','MonthlyIncome','TotalWorkingYears','YearsAtCompany']])\ntest[['Age','DistanceFromHome','EmployeeNumber','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager','MonthlyIncome','TotalWorkingYears','YearsAtCompany']]=scaler.fit_transform(test[['Age','DistanceFromHome','EmployeeNumber','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager','MonthlyIncome','TotalWorkingYears','YearsAtCompany']])","d452443c":"## Dropping Unnecessary Features based on EDA\ntrain2=train.drop(['Id','DistanceFromHome','EmployeeNumber','Gender','NumCompaniesWorked','PercentSalaryHike','PerformanceRating','TrainingTimesLastYear','YearsWithCurrManager','Behaviour','CommunicationSkill'],axis=1)\ntest2=test.drop(['Id','DistanceFromHome','EmployeeNumber','Gender','NumCompaniesWorked','PercentSalaryHike','PerformanceRating','TrainingTimesLastYear','YearsWithCurrManager','Behaviour','CommunicationSkill'],axis=1)","0218ed07":"train3=pd.get_dummies(train, drop_first=True)\nX_test=pd.get_dummies(test, drop_first=True)","1499a5ed":"y_train3=pd.Series(train.Attrition)\ntrain3.drop('Attrition', axis=1, inplace=True)","5775bc63":"# grid searching key hyperparametres for logistic regression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n# define models and parameters\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(train3, y_train3)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","9651229d":"C_range=[0.01,0.1,1,10,100]\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\nmax_itera=[10,100,1000,1500]\n\n#for i in max_itera:\nlr=LogisticRegression(C=0.1,random_state=0,solver='newton-cg',penalty='l2',max_iter=100)\nlr.fit(train3,y_train3)\nprint('RUC_AUC score for Logistic Regression Trainig for  is'  +str(roc_auc_score(y_train3,lr.predict(train3))))\n#print('RUC_AUC score for Logistic Regression Validation for is'   +str(roc_auc_score(y_val,lr.predict(X_val))))\nprint('\\n')","bf79c354":"# grid searching key hyperparameters for RandomForestClassifier\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# define models and parameters\nmodel = RandomForestClassifier()\nn_estimators = [10, 100, 1000]\nmax_features = ['sqrt', 'log2']\n# define grid search\ngrid = dict(n_estimators=n_estimators,max_features=max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(train3, y_train3)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","92d80071":"rf=RandomForestClassifier(random_state=0,max_features='log2',n_estimators=1000)\nrf.fit(train3,y_train3)\nprint('RUC_AUC score for Random Forest Training  is'+str(roc_auc_score(y_train3,rf.predict(train3))))","03f4d9fd":"# grid searching key hyperparametres for SVC\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n# define dataset\n\nmodel = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 1.0, 0.1, 0.01]\ngamma = ['scale']\n# define grid search\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(train3, y_train3)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","89367ad3":"from sklearn.svm import SVC\nC_range=[0.01,0.1,1,10,100,1000,10000]\ngammas = [0.1, 1, 10, 100]\ndegrees = [0, 1, 2, 3, 4, 5, 6]\n\n#for i in degrees:\nsvc=SVC(random_state=0,probability=True,C=50,gamma='scale',kernel='rbf')\nsvc.fit(train3,y_train3)\nprint('RUC_AUC score for SVM Training  is' +str(roc_auc_score(y_train3,svc.predict(train3))))\nprint('\\n')","fd3aceca":"# make a prediction with a stacking ensemble\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# define the base models\nlevel0 = list()\nlevel0.append(('svm', SVC(random_state=0,probability=True,C=50,gamma='scale',kernel='rbf')))\nlevel0.append(('rf', RandomForestClassifier(random_state=0,max_features='log2',n_estimators=1000)))\n# define meta learner model\nlevel1 = LogisticRegression(C=10,random_state=0,solver='liblinear',penalty='l2',max_iter=1000)\n# define the stacking ensemble\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5, stack_method='predict_proba')\n# fit the model on all available data\nmodel.fit(train3, y_train3)","73cfbf15":"stack_pred=model.predict_proba(X_test)\nstack=[]\nfor i in stack_pred:\n        stack.append(i[1])","132e6e67":"stack_sub= pd.DataFrame({'ID':np.arange(1,471), 'Attrition':stack}, index=None, columns=['ID','Attrition'])","09fd3159":"stack_sub.to_csv('stack_7.csv', index=False)","32f90c02":"# EDA & Data Cleaning","1eaac98f":"# Stacking","c73e6a83":"# Data Preprocessing (after analysing each feature like above)"}}