{"cell_type":{"42e817bd":"code","e2a0eb00":"code","c9f56823":"code","2053577e":"code","de3392c5":"code","e19fc27c":"code","fec39861":"code","4d4a5167":"code","b47907bb":"code","35670f50":"code","1aefc24b":"code","deb6a1a2":"code","0e2f9cb2":"code","4c1b5beb":"code","edea6fd1":"code","db8b860d":"code","47b45f5a":"code","b23f702d":"code","c58f00ec":"code","a4968347":"code","605b370e":"code","53136916":"code","ce8ac468":"code","1f76ed20":"code","43917b95":"code","26b50937":"markdown","18b98f37":"markdown","0bd32002":"markdown","44e95eb2":"markdown","fefb71b1":"markdown","19691ccc":"markdown","27c86e2b":"markdown","2d6b5fa7":"markdown","13e3775c":"markdown"},"source":{"42e817bd":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom string import punctuation\nimport re","e2a0eb00":"faq =pd.read_csv('..\/input\/mental-health-faq-for-chatbot\/Mental_Health_FAQ.csv')\nfaq","c9f56823":"faq_quest = faq[['Question_ID', 'Questions']]\nfaq_answ = faq[['Question_ID', 'Answers']]","2053577e":"def to_lower(text):\n    return text.lower()","de3392c5":"contractions_dict = {     \n\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n\"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n\"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I had\", \"I'd've\": \"I would have\",\n\"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it had\",\n\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"iit will have\", \"it's\": \"it is\", \"let's\": \"let us\",\n\"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n\"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she had\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n\"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that had\", \"that'd've\": \"that would have\",\n\"that's\": \"that is\", \"there'd\": \"there had\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they had\",\n\"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n\"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we had\", \"we'd've\": \"we would have\",\n\"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n\"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n\"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n\"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n\"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n\"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you had\",\n\"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n}\n\ndef expand_contraction(text, contraction_dict):\n    contraction_pattern= re.compile('({})'.format('|'.join(contraction_dict.keys())), flags= re.IGNORECASE | re.DOTALL)\n    \n    def expand_match(contraction):\n        match= contraction.group(0)\n        first_char= match[0]\n        expanded_contraction= contraction_dict.get(match) \\\n            if contraction_dict.get(match) \\\n            else contraction_dict.get(match.lower())\n        expanded_contraction= expanded_contraction\n        return expanded_contraction\n        \n    expanded_text= contraction_pattern.sub(expand_match, text)\n    expanded_text= re.sub(\"'\",\"\", expanded_text)\n    return expanded_text\n\ndef main_contraction(text):\n    text = expand_contraction(text, contractions_dict)\n    return text","e19fc27c":"def remove_number(text):\n    output = ''.join(c for c in text if not c.isdigit())\n    return output","fec39861":"def remove_punct(text):\n    return \"\".join(c for c in text if c not in punctuation)","4d4a5167":"def to_strip(text):\n    return \" \".join([c for c in text.split() if len(c)>2])","b47907bb":"def remove_char(text):\n    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n    return text","35670f50":"def remove_duplicate(text):\n    text = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", text)\n    return text","1aefc24b":"import nltk\nfrom nltk.corpus import stopwords\nstopwords.words('english')\n\ndef remove_stopwords(text):\n    stop_words= stopwords.words('english')\n    \n    return ' '.join(c for c in nltk.word_tokenize(text) if c not in stop_words)","deb6a1a2":"from nltk.stem import WordNetLemmatizer\n\nwordnet_lemma = WordNetLemmatizer()\n\ndef lemma(text):\n    lemmatize_words = [wordnet_lemma.lemmatize(word) for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    return ' '.join(lemmatize_words)","0e2f9cb2":"faq_quest['prep1']= faq_quest['Questions'].apply(to_lower)\nfaq_quest['prep2']= faq_quest['prep1'].apply(main_contraction)\nfaq_quest['prep3']= faq_quest['prep2'].apply(remove_number)\nfaq_quest['prep4']= faq_quest['prep3'].apply(remove_punct)\nfaq_quest['prep5']= faq_quest['prep4'].apply(to_strip)\nfaq_quest['prep6']= faq_quest['prep5'].apply(remove_char)\nfaq_quest['prep7']= faq_quest['prep6'].apply(remove_duplicate)\nfaq_quest['prep8']= faq_quest['prep7'].apply(remove_stopwords)\nfaq_quest['lemma']= faq_quest['prep8'].apply(lemma)\nfaq_quest.head(10)","4c1b5beb":"faq_answ['prep1']= faq_answ['Answers'].apply(to_lower)\nfaq_answ['prep2']= faq_answ['prep1'].apply(main_contraction)\nfaq_answ['prep3']= faq_answ['prep2'].apply(remove_number)\nfaq_answ['prep4']= faq_answ['prep3'].apply(remove_punct)\nfaq_answ['prep5']= faq_answ['prep4'].apply(to_strip)\nfaq_answ['prep6']= faq_answ['prep5'].apply(remove_char)\nfaq_answ['prep7']= faq_answ['prep6'].apply(remove_duplicate)\nfaq_answ['prep8']= faq_answ['prep7'].apply(remove_stopwords)\nfaq_answ['lemma']= faq_answ['prep8'].apply(lemma)\nfaq_answ.head(10)","edea6fd1":"def dictionary(check):\n    check = check.str.extractall('([a-zA_Z]+)')\n    check.columns = ['check']\n    b = check.reset_index(drop=True)\n    check = b['check'].value_counts()\n    \n    dictionary = pd.DataFrame({'word': check.index, 'freq': check.values})\n    dictionary.index = dictionary['word']\n    dictionary.drop('word', axis = 1, inplace=True)\n    dictionary.sort_values('freq', inplace= True, ascending= False)\n    \n    return dictionary\n\ndictionary_clean = dictionary(faq_quest['lemma'])\ndictionary_clean[:30].plot(kind = 'barh',figsize = (10,10))","db8b860d":"def dictionary(check):\n    check = check.str.extractall('([a-zA_Z]+)')\n    check.columns = ['check']\n    b = check.reset_index(drop=True)\n    check = b['check'].value_counts()\n    \n    dictionary = pd.DataFrame({'word': check.index, 'freq': check.values})\n    dictionary.index = dictionary['word']\n    dictionary.drop('word', axis = 1, inplace=True)\n    dictionary.sort_values('freq', inplace= True, ascending= False)\n    \n    return dictionary\n\ndictionary_clean = dictionary(faq_answ['lemma'])\ndictionary_clean[:30].plot(kind = 'barh',figsize = (10,10))","47b45f5a":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef plot_cloud(wordcloud):\n    plt.figure(figsize= (20,10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off')","b23f702d":"word_cloud= WordCloud(max_font_size=30, max_words=15, background_color=\"white\", colormap ='Set2', collocations = False, stopwords = stopwords.words('english')).generate(str(faq_quest['lemma']))\nplot_cloud(word_cloud)","c58f00ec":"word_cloud= WordCloud(max_font_size=30, max_words=15, background_color=\"white\", colormap='Set2', collocations= False, stopwords= stopwords.words('english')).generate(str(faq_answ['lemma']))\nplot_cloud(word_cloud)","a4968347":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\nfaq['AnswersEncode'] = label.fit_transform(faq['Answers'])\nfaq","605b370e":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC","53136916":"text = faq['Questions']\ny= faq['AnswersEncode'].values","ce8ac468":"tfidf = TfidfVectorizer(use_idf=True, analyzer='word', stop_words='english', token_pattern=r'\\b[^\\d\\W]+\\b', ngram_range=(1,2))\nX_train = tfidf.fit_transform(text)\nprint(X_train)","1f76ed20":"lsvc = LinearSVC(random_state = 2021)\nlsvc.fit(X_train, y)","43917b95":"search_test = [\n    \"How can I recover?\",\n    \"Is cannabis dangerous?\",\n    \"What is the side effect of drinking?\"\n]\n\nsearch_engine = tfidf.transform(search_test)\nresult = lsvc.predict(search_engine)\n\nfor question in result:\n    faq_data = faq.loc[faq.isin([question]).any(axis=1)]\n    print(\"Answer: \", faq_data['Answers'].values)","26b50937":"I try to learn how to explore NLP to build a small search engine based on the chatbot conversation. If you have any advice to improve it, please comment below. Thank you and have a nice reading!","18b98f37":"## Modeling","0bd32002":"## Testing","44e95eb2":"## Text Pre-processing","fefb71b1":"## Wordcloud","19691ccc":"This section shows the most frequent of words that pop up on chatbot.","27c86e2b":"**Test the data using 3 questions that I make my own.**","2d6b5fa7":"## Labelling","13e3775c":"## Text Exploration"}}