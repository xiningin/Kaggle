{"cell_type":{"0c7797e1":"code","dd98799e":"code","58cbf48e":"code","504999df":"code","6b86c53c":"code","ac3de010":"code","7d46df5a":"code","f2f99705":"code","0d75bb70":"code","dcaceec4":"code","ad2208dd":"code","be9b072d":"code","9ed63624":"code","87f39721":"code","9afd61b8":"code","54effc6f":"code","a672980f":"code","5ca1c44a":"code","96c824ae":"code","1c64c3b6":"code","a0e4298e":"code","062a99ff":"code","7765a92b":"markdown","f1281b6c":"markdown","90386fb3":"markdown","0ff6cd06":"markdown","e04be52f":"markdown","6bac31d4":"markdown","48bae205":"markdown","e9a8380d":"markdown","21c5e6c8":"markdown","2948b165":"markdown","9df3989a":"markdown","bd80cb14":"markdown","37129129":"markdown","4b129eae":"markdown","dbbeb1cb":"markdown","bc2604ce":"markdown","505352e5":"markdown","ed6d0ced":"markdown","3b22f960":"markdown","9523bfdc":"markdown","06b8ebb7":"markdown","4e8fb83e":"markdown","82e6970d":"markdown","c55471fb":"markdown","95969a08":"markdown","8da5d038":"markdown","2e24baae":"markdown","b05f13b1":"markdown","31416916":"markdown","ca9a725d":"markdown","5e5090d6":"markdown","822dc0db":"markdown","d99e3a90":"markdown"},"source":{"0c7797e1":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","dd98799e":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores\/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","58cbf48e":"from kaggle_environments import make, evaluate\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 1000})\n\nenv.run([\"markov_agent.py\", \"nash_equilibrium.py\"])\n\nenv.render(mode=\"ipython\", width=800, height=800)","504999df":"seasons = 100\nepisodes = 1000","6b86c53c":"import numpy as np\nimport pandas as pd\nimport json\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import make\n\nfrom IPython.display import Markdown as md\n\naction_board = pd.DataFrame(columns = [\"season\",\n                                      \"episode\",\n                                      \"Markov Action\",\n                                      \"Nash Action\",\n                                      \"Markov Reward\",\n                                      \"Nash Reward\"])\nleaderboard = pd.DataFrame(columns = [\"season\",\n                                      \"Markov Reward\",\n                                      \"Nash Reward\"])\n\n\nindex = 0\nenv = make(\"rps\", configuration={\"episodeSteps\": episodes})\n\nfor season in range(seasons):\n    env.reset()\n    results = env.run([\"markov_agent.py\", \"nash_equilibrium.py\"])\n    for result in results:\n        if (result[0].observation.step == 0):\n            continue\n        action_board = action_board.append({\"season\": season,\n                              \"episode\": result[0].observation.step,\n                              \"Markov Action\": result[0].action,\n                              \"Nash Action\": result[1].action,\n                              \"Markov Reward\": result[0].reward,\n                              \"Nash Reward\": result[1].reward},\n                                        ignore_index=True)\n        if result[0].status == \"DONE\":\n            leaderboard = leaderboard.append({\"season\": season,\n                              \"Markov Reward\": result[0].reward,\n                              \"Nash Reward\": result[1].reward},\n                                        ignore_index=True)","ac3de010":"md('# \\(Not so\\) Markov - Nash Equilibrium : {} - {}'.format(len(leaderboard[leaderboard[\"Markov Reward\"] > 0]), len(leaderboard[leaderboard[\"Nash Reward\"] > 0])))","7d46df5a":"md('# Tie : {}'.format(len(leaderboard[leaderboard[\"Markov Reward\"] == 0])))","f2f99705":"if (len(leaderboard[leaderboard[\"Markov Reward\"] > 0]) == len(leaderboard[leaderboard[\"Nash Reward\"] > 0])):\n    winner = \"Tie!\"\nelif (len(leaderboard[leaderboard[\"Markov Reward\"] > 0]) > len(leaderboard[leaderboard[\"Nash Reward\"] > 0])):\n    winner = \"Winner is Markov!\"\nelse:\n    winner = \"Winner is Nash!\"\nmd('<a id=\"11\"><\/a><h1 style=\\'background:#FBE338; border:0; color:black\\'><center>{}<center><h2>'.format(winner))","0d75bb70":"leaderboard.plot(subplots=True, figsize=(15,10))","dcaceec4":"leaderboard[['Markov Reward', 'Nash Reward']].plot.hist(bins=10,  alpha=0.5, figsize=(15,10))","ad2208dd":"action_board[['Markov Action', 'Nash Action']].plot.hist(bins=3, alpha=0.5, xticks=[0,1,2], figsize=(15,10))","be9b072d":"fig, ax = plt.subplots(figsize=(20,10))\nfor i, g in action_board.groupby('season'):\n    g.plot(x='episode', y='Markov Reward', ax=ax, legend=False )","9ed63624":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[(action_board['episode']<episodes\/2)].groupby('season'):\n    g.plot(x='episode', y='Markov Reward', ax=ax, legend=False )","87f39721":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[((action_board['episode']>episodes\/3) & (action_board['episode']<2*episodes\/3))].groupby('season'):\n    g.plot(x='episode', y='Markov Reward', ax=ax, legend=False )","9afd61b8":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[action_board['episode']>episodes\/2].groupby('season'):\n    g.plot(x='episode', y='Markov Reward', ax=ax, legend=False )","54effc6f":"leaderboard.head()","a672980f":"leaderboard.tail()","5ca1c44a":"leaderboard.describe()","96c824ae":"action_board.head()","1c64c3b6":"action_board.tail()","a0e4298e":"action_board.drop(columns='season').describe()","062a99ff":"# Report boards\nleaderboard_csv = 'Not_so_Markov_leaderboard_S' + str(seasons) + 'E' + str(episodes) + '.csv'\naction_board_csv = 'Not_so_Markov_action_board_S'+ str(seasons) + 'E' + str(episodes) + '.csv'\nleaderboard.to_csv(leaderboard_csv)\naction_board.to_csv(action_board_csv)\nprint(leaderboard_csv)\nprint(action_board_csv)","7765a92b":"# Data export","f1281b6c":"<h1 style='background:#FBE338; border:0; color:black'><center>Dataset<center><h1>","90386fb3":"### First 5 seasons rewards","0ff6cd06":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: (Not so) Markov<center><h2>","e04be52f":"# Season's reward histogram","6bac31d4":"<h1 style='background:#FBE338; border:0; color:black'><center>Analysis<center><h1>","48bae205":"Dataset is exported, collected and publicly shared in [Rock Paper Scissors Agents Battles](https:\/\/www.kaggle.com\/jumaru\/rock-paper-scissors-agents-battles) dataset.","e9a8380d":"<a id=\"11\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Validate<center><h2>\n\n\n","21c5e6c8":"## Last half rewards","2948b165":"## First half rewards","9df3989a":"* Agent `(Not so) Markov` has a clear tendency to shoot `paper`.\n* Agent's `(Not so) Markov` change of strategy at `500` & `900` episode is not always good strategy.\n* When reward is around `-20` looks like a good point to change strategy.","bd80cb14":"## All episodes reward","37129129":"# (Not so) Markov vs Nash Equilibrium: Rock Paper Scissors\n\n\n### 100 seasons of Markov vs Nash on Rock Paper Scissors\n### 1000 episodes per season\n\n### Bonus: Dataset generation","4b129eae":"*...breaking chains excites me!*","dbbeb1cb":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/a8\/Andrei_Markov.jpg\/220px-Andrei_Markov.jpg) ","bc2604ce":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Nash Equilibrium<center><h2>","505352e5":"<a id=\"11\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Action<center><h2>\n\n","ed6d0ced":"<h1 style='background:#FBE338; border:0; color:black'><center>Result<center><h1>\n","3b22f960":"## First 5 actions","9523bfdc":"## Rewards Statistics ","06b8ebb7":"### Last 5 seasons rewards","4e8fb83e":"## Mid-episodes reward","82e6970d":"## Last 5 actions","c55471fb":"* [Rock Paper Scissors - Nash Equilibrium Strategy](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-nash-equilibrium-strategy) & [Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison) by [Yaroslav Isaienkov](https:\/\/www.kaggle.com\/ihelon)\n* [(Not so) Markov](https:\/\/www.kaggle.com\/alexandersamarin\/not-so-markov) by [Alexander Samarin](https:\/\/www.kaggle.com\/alexandersamarin)\n* [LB simulation](https:\/\/www.kaggle.com\/superant\/lb-simulation) by [Ant \ud83d\udc1c](https:\/\/www.kaggle.com\/superant)","95969a08":"*...if we all go for the blonde we are blocking each other.*","8da5d038":"## Actions Statistics","2e24baae":"# Season's results","b05f13b1":"# Actions histogram","31416916":"<h1 style='background:#FBE338; border:0; color:black'><center>Conclusion<center><h1>","ca9a725d":"# References","5e5090d6":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/a9\/John_Forbes_Nash%2C_Jr._by_Peter_Badge.jpg\/220px-John_Forbes_Nash%2C_Jr._by_Peter_Badge.jpg)","822dc0db":"## Leaderboard","d99e3a90":"# Action board"}}