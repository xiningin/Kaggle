{"cell_type":{"699e535d":"code","1e93d82b":"code","01b2d0bd":"code","ef8f3c7c":"code","e38df67f":"code","a206bd20":"code","c186f436":"code","a89366c8":"code","fb27b252":"code","e0434acd":"code","7f954ffd":"code","32919eea":"code","1336d4d1":"code","ff9ee925":"code","ac932a71":"code","f94abc76":"code","82cb454d":"code","14281129":"code","6de56ab2":"code","b57bd91d":"code","2227ff5c":"code","8279bf74":"code","fc362676":"code","9721bf4d":"code","70e34e80":"code","f8ebb02e":"code","5cb315ac":"code","fe9421fd":"code","f384f915":"code","a013f673":"code","7105072c":"code","67e0f1a7":"markdown","f5572ddf":"markdown","c13cca8c":"markdown","7226b82f":"markdown","50cdf6a2":"markdown","ed0f7c3a":"markdown","897af8fc":"markdown","a6457f6a":"markdown","d66e2783":"markdown","6eb58dc5":"markdown","69b31b3c":"markdown","900b9b19":"markdown","55fd063d":"markdown"},"source":{"699e535d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1e93d82b":"# importing libraries and magic functions\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n%config InlineBackend.figure_format ='retina'\n%matplotlib inline\n","01b2d0bd":"# read data\ndf = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf.head()","ef8f3c7c":"df.describe()\nprint(\"The size of the dataframe is:\",df.shape)","e38df67f":"# check for null values\ndf_missing = df.isnull().sum()\ndf_missing\n\n# calculate the % of missing values\nperc_missing = round(100*(df_missing\/len(df)),2)\nperc_missing","a206bd20":"# dropping columns with large % of missing values - also dropping RISK_MM based on instructions in the description\n\ndf_dropped = df.drop(['Evaporation','Sunshine','Cloud9am','Cloud3pm','RISK_MM'], axis=1)","c186f436":"df_dropped.isna().sum()","a89366c8":"# Now we are dropping the remaining rows with nan values\n\ndf_dropped = df_dropped.dropna()","fb27b252":"df_dropped.head()\nprint(\"The new size of the dataframe is:\", df_dropped.shape)\nprint(\"We deleted\",df.shape[0]-df_dropped.shape[0],\"rows and\", df.shape[1]-df_dropped.shape[1],\"columns.\")\ndf_dropped.dtypes","e0434acd":"# change date type to datetime\n\ndf_dropped['Date'] = pd.to_datetime(df_dropped['Date'])","7f954ffd":"# Adding columns Year and Month\n\ndf_dropped['Year'] = pd.to_datetime(df_dropped['Date']).dt.year\ndf_dropped['Month'] = pd.to_datetime(df_dropped['Date']).dt.month","32919eea":"# set Date as index\n\ndf_dropped.set_index('Date', inplace=True)\ndf_dropped.head()","1336d4d1":"# Plotting rainfall during time\nplt.figure(figsize=(20,5))\ndf_dropped['Rainfall'].plot()\nplt.box(False)\nplt.title ('Rainfall throughout the Years',fontweight=\"bold\", fontsize=15)","ff9ee925":"# plotting Rainfall per Month\nplt.figure(figsize=(8,5))\nsns.barplot(x = 'Month', y='Rainfall', data=df_dropped, color = 'skyblue')\nplt.box(False)\nplt.title ('Rainfall throughout Months', fontweight=\"bold\",fontsize=15)","ac932a71":"# plotting average Rainfall by Location\ndf_loc = df_dropped.groupby('Location').agg({'Rainfall':'mean'}).sort_values(by='Rainfall', ascending=False) \n\ndf_loc.plot(kind='bar',figsize=(20,5))\nplt.box(False)\nplt.title ('Average Rainfall by Location', fontsize=15, fontweight=\"bold\")\nplt.show()","f94abc76":"# Plotting Temperature and Rainfall\n\nfig, (ax1,ax2) = plt.subplots(1, 2, figsize=(15, 5))\nsns.despine(left=True)\nsns.scatterplot(x='MinTemp', y='Rainfall', data=df_dropped, ax=ax1)\nax1.set_title(\"Lowest Temperature and Amount of Rainfall\",fontweight=\"bold\")\nsns.scatterplot(x='MaxTemp', y='Rainfall', data=df_dropped, color=\"tomato\", ax=ax2)\nax2.set_title(\"Highest Temperature and Amount of Rainfall\",fontweight=\"bold\")\n","82cb454d":"# Renaming Dataframe for the Machine Learning Part\ndf_ML = df_dropped","14281129":"# Dropping columns that we do not need for the model building part\ndf_ML = df_ML.drop(['Location','Year'], axis=1)","6de56ab2":"# Adjusting the Target Variables' values: Yes\/No with 1\/0\ndf_ML = df_ML.replace({'RainTomorrow':'Yes','RainToday':'Yes'},1)\ndf_ML = df_ML.replace({'RainTomorrow':'No','RainToday':'No'},0)","b57bd91d":"# Create Dummies for categorical variables\ndf_ML = pd.get_dummies(df_ML, prefix = ['WindDir3pm','WindDir9am','WindDir3pm'])\n\ndf_ML.head()","2227ff5c":"# Correlation\n# Create Correlation mask >0.5:\ndf_ML_corr = df_ML.corr()\ncondition = abs(df_ML.corr()) > 0.5\n#df_ML_corr[condition]","8279bf74":"# heatmap\n# correlation plot\nplt.figure(figsize=(20,20))\nsns.heatmap(df_ML.corr(), cmap = 'Wistia')\n","fc362676":"# Dropping highly correlated columns\n\ndf_ML = df_ML.drop(['WindGustSpeed','Humidity9am',], axis=1)","9721bf4d":"# Standardize our Data - Feature Scaling 0-1 scale \n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0,1)) \n\n#assign scaler to column:\ndf_scaled = pd.DataFrame(scaler.fit_transform(df_ML), columns=df_ML.columns)\n\ndf_scaled.head()\n","70e34e80":"# Selection of the most important features using SelectKBest\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nX = df_scaled.loc[:,df_scaled.columns!='RainTomorrow']\ny = df_scaled[['RainTomorrow']]\n\nselector = SelectKBest(chi2, k=5)\nselector.fit(X, y)\n\nX_new = selector.transform(X)\nprint(\"The 5 most important features are:\", X.columns[selector.get_support(indices=True)]) ","f8ebb02e":"# Creating a new dataframe with the most important features\n\ndf_new = df_scaled[['Rainfall', 'Humidity3pm','WindDir9am_E', 'WindDir9am_N','RainToday','RainTomorrow']]","5cb315ac":"df_new['RainTomorrow'].value_counts()[0]","fe9421fd":"Percentage_No = df_new['RainTomorrow'].value_counts()[0]\/len(df_new['RainTomorrow'])*100\nPercentage_Yes = df_new['RainTomorrow'].value_counts()[1]\/len(df_new['RainTomorrow'])*100","f384f915":"# checking the distribution of our target variable \nprint(df_new['RainTomorrow'].value_counts())\n\nprint(\"Percentage Occurences of No Rain on the following day:\", round(Percentage_No,2),\"%\")\nprint(\"Percentage Occurences of Rain on the following day:\", round(Percentage_Yes,2),\"%\")\n\nsns.countplot(df_new['RainTomorrow'])\nplt.title('Balance target',fontsize=15, fontweight='bold')\nplt.box(False)","a013f673":"from sklearn.model_selection import train_test_split\n\n# clarify what is y and what is X\ny = df_new['RainTomorrow']\nX = df_new.drop(['RainTomorrow'], axis = 1)\n\n# Train-Test Split 80-20\n# Note: We use stratify = y here because we have an unbalanced Dataset and we want to sample equal occurences of the target variable outcomes\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,stratify = y)\n","7105072c":"### Model Pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport time\n\nclassifiers = [LogisticRegression(),DecisionTreeClassifier(),KNeighborsClassifier(2)]\n\nfor classifier in classifiers:\n    t0=time.time()\n    pipe = Pipeline(steps=[('classifier', classifier)])\n    pipe.fit(X_train, y_train)   \n    score = pipe.score(X_test, y_test)\n    print(f\"The accuracy score is: {round(score,2)*100}%\")\n    print('Time taken to execute:' , time.time()-t0)","67e0f1a7":"### Choosing the best Model","f5572ddf":"**Data Cleaning & Manipulation**","c13cca8c":"### Variable Descriptions:\n\n* ** RainTomorrow: The target variable. Did it rain the following day? YES\/NO**\n\n* Date: The date of observation\n* Location: The common name of the location of the weather station\n* MinTemp: The minimum temperature in degrees celsius\n* MaxTemp: The maximum temperature in degrees celsius\n* Rainfall: The amount of rainfall recorded for the day in mm\n* Evaporation: The so-called Class A pan evaporation (mm) in the 24 hours to 9am\n* Sunshine: The number of hours of bright sunshine in the day.\n* WindGustDir: The direction of the strongest wind gust in the 24 hours to midnight\n* WindGustSpeed: The speed (km\/h) of the strongest wind gust in the 24 hours to midnight\n* WindDir9am: Direction of the wind at 9am\n* WindDir3p: Direction of the wind at 3pm\n* WindSpeed9am: Wind speed (km\/hr) averaged over 10 minutes prior to 9am\n* WindSpeed3pm: Wind speed (km\/hr) averaged over 10 minutes prior to 3pm\n* Humidity9a: Humidity (percent) at 9am\n* Humidity3pm: Humidity (percent) at 3pm\n* Pressure9am: Atmospheric pressure (hpa) reduced to mean sea level at 9am\n* Pressure3pm: Atmospheric pressure (hpa) reduced to mean sea level at 3pm\n* Cloud9am: Fraction of sky obscured by cloud at 9am. This is measured in \"oktas\", which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.\n* Cloud3pm: Fraction of sky obscured by cloud (in \"oktas\": eighths) at 3pm. See Cload9am for a description of the values\n* Temp9am: Temperature (degrees C) at 9am\n* Temp3pm: Temperature (degrees C) at 3pm\n* RainToday: Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0\n* RISK_MM: The amount of next day rain in mm. Used to create response variable RainTomorrow. A kind of measure of the \"risk\". Will be left out in the model.\n","7226b82f":"## Data Preparation for ML","50cdf6a2":"### Train-Test Split","ed0f7c3a":"**We get the best accuracy score with the Logistic Regression Model with 84%. The Decision Tree Algorithm is slightly faster but only scores 83%.KNN performs worse and is comparatable slow.**","897af8fc":"### Feature Scaling","a6457f6a":"### Exploratory Data Analysis","d66e2783":"## Exploratory Data Analysis","6eb58dc5":"### Feature Selection","69b31b3c":"**We can see that the distribution between the two outcomes Rain on the following day and No Rain on the following day is unbalanced. This can lead to a biased result. Therefore we will balance the outcome variable in the next step while splitting the training ans testing data using the stratify argument.**\n","900b9b19":"### Checking the Target variables' distribution","55fd063d":"# <span style=\"color:lightblue\">**Will it rain tomorrow?**<\/span>\n\n<iframe src=\"https:\/\/giphy.com\/embed\/KatjlSAMx0K9zdHMR4\" width=\"480\" height=\"347\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen><\/iframe><p><a href=\"https:\/\/giphy.com\/gifs\/tcm-vintage-turner-classic-movies-gene-kelly-KatjlSAMx0K9zdHMR4\">via GIPHY<\/a><\/p>\n\n#### Context\nPredict whether or not it will rain tomorrow by training a binary classification model \n\n#### Content\nThis dataset contains daily weather observations from numerous Australian weather stations such as Rainfall, Wind and Humidity."}}