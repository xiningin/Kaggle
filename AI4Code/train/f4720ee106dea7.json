{"cell_type":{"b206a286":"code","7ab79124":"code","c2bc4abe":"code","d2d41280":"code","fc55a5fd":"code","0b99a35a":"code","d351b6f3":"code","9a24bd2d":"code","ebb86459":"code","e7665257":"code","49b4459e":"code","3795b062":"code","dff1c26c":"code","7a70d0a7":"code","97a10284":"code","aa659e65":"code","f2b0b3d4":"code","4377c106":"code","ca79ccf7":"code","caeaa345":"code","e1b1fb98":"code","59719cd9":"code","7d2dec7e":"code","93a54867":"code","eafed9bf":"code","540a2360":"code","4ec43440":"code","813c8f9f":"code","78e92d57":"markdown","3b3978e7":"markdown","57c42ca1":"markdown","e6117499":"markdown","f38bbc45":"markdown","8063eed3":"markdown","658574de":"markdown","82e599e7":"markdown","99a67b62":"markdown","849ac5a9":"markdown","68e98a67":"markdown","c9c9f58f":"markdown"},"source":{"b206a286":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfrom contextlib import redirect_stdout\nimport copy\n\n# \u8a55\u4fa1\u6307\u6a19\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import log_loss\n\n# hyperparameter tuning\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials","7ab79124":"class XgBoostFeatures:\n    def __init__(self):\n        pass\n    # create features for the model\n    def data_process(self, data, params):\n        self.ticket_data(data)\n        self.name_data(data)\n        self.embarked_data(data)\n        self.cabin_data(data)\n        self.fare_data(data)\n        self.parch_data(data)\n        self.sibsp_data(data)\n        self.pclass_data(data)\n        self.age_data(data)\n        self.sex_data(data)\n        \n        # [\"Ticket_processed\", \"Name_processed\", \"Embarked_processed\", \"Cabin_processed\", \"Fare_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Age_processed\", \"Sex_processed\"]\n        return data[params]\n\n    def ticket_data(self, data):\n        data_ticket = data[\"Ticket\"].to_numpy()\n\n        ret_val = []\n        for ticket in data_ticket:\n            split_data = ticket.split(\" \")\n            if split_data[0].isdigit():\n                ret_val.append(0)\n            elif split_data[0]==\"PC\":\n                ret_val.append(1)\n            elif split_data[0]==\"C.A.\" or split_data[0]==\"CA\"or split_data[0]==\"CA.\":\n                ret_val.append(2)\n            elif split_data[0]==\"A\/5.\" or split_data[0]==\"A\/5\"or split_data[0]==\"A.\/5.\"or split_data[0]==\"A.5.\":\n                ret_val.append(3)\n            else:\n                ret_val.append(4)\n\n        data[\"Ticket_processed\"] = np.asarray(ret_val).astype(np.int8)\n        \n    def name_data(self, data):\n        data_name = data[\"Name\"].to_numpy()\n\n        ret_val = []\n        for name in data_name:\n            name_split = name.split(\" \")\n            target_name = \"\"\n            for target in name_split:\n                try:\n                    if target[-1] == \".\":\n                        target_name = target\n                        break\n                except Exception as e:\n                    print(str(e)+\":\"+name+\":\"+target)\n\n            if target_name == \"Mr.\":\n                ret_val.append(0)\n            elif target_name == \"Mrs.\":\n                ret_val.append(1)\n            elif target_name == \"Miss.\":\n                ret_val.append(2)\n            else:\n                ret_val.append(3)            \n\n        data[\"Name_processed\"] = np.asarray(ret_val).astype(np.int8)\n\n    def embarked_data(self, data):\n        data_embarked = data[\"Embarked\"].to_numpy()\n        tmp = []\n        for d in data_embarked:\n            try:\n                if np.isnan(d):\n                    tmp.append(np.nan)\n            except:\n                if d[0] == \"S\":\n                    tmp.append(3)\n                elif d[0] == \"Q\":\n                    tmp.append(2)\n                elif d[0] == \"C\":\n                    tmp.append(1)\n                else:\n                    tmp.append(-1)\n        data[\"Embarked_processed\"] = np.asarray(tmp)\n        \n    def cabin_data(self, data):\n        data_cabin = data[\"Cabin\"].to_numpy()\n        tmp = []\n        for d in data_cabin:\n            try:\n                if np.isnan(d):\n                    tmp.append(np.nan)\n            except:\n                if d[0] == \"A\":\n                    tmp.append(7)\n                elif d[0] == \"B\":\n                    tmp.append(6)\n                elif d[0] == \"C\":\n                    tmp.append(5)\n                elif d[0] == \"D\":\n                    tmp.append(4)\n                elif d[0] == \"E\":\n                    tmp.append(3)\n                elif d[0] == \"F\":\n                    tmp.append(2)\n                elif d[0] == \"G\":\n                    tmp.append(1)\n                else:\n                    tmp.append(-1)\n        data[\"Cabin_processed\"] = np.asarray(tmp).astype(np.float32)\n\n    def fare_data(self, data):\n        data[\"Fare_processed\"] = data[\"Fare\"]\n        \n    def parch_data(self, data):\n        data[\"Parch_processed\"] = data[\"Parch\"]\n        \n    def sibsp_data(self, data):\n        data[\"SibSp_processed\"] = data[\"SibSp\"]\n        \n    def pclass_data(self, data):\n        data[\"Pclass_processed\"] = data[\"Pclass\"]\n        \n    def age_data(self, data):\n        data[\"Age_processed\"] = data[\"Age\"]\n        \n    def sex_data(self, data):\n        data_sex = data[\"Sex\"].to_numpy()\n        data_sex[data_sex==\"male\"] = 0.0\n        data_sex[data_sex==\"female\"] = 1.0\n        data[\"Sex_processed\"] = data_sex.astype(np.float32)","c2bc4abe":"class XgBoost:\n    def __init__(self):\n        self.create_features = XgBoostFeatures()\n        self.train_data_raw = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n        self.test_data_raw = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n        \n        self.use_data = [\"Ticket_processed\", \"Name_processed\", \"Embarked_processed\", \"Cabin_processed\", \"Fare_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Age_processed\", \"Sex_processed\"]\n\n        self.train_data_x = self.create_features.data_process(self.train_data_raw, self.use_data)\n        self.train_data_y = self.train_data_raw[\"Survived\"]\n        self.test_data = self.create_features.data_process(self.test_data_raw, self.use_data)\n        \n        self.history = [] # to avoid the error on train()\n        \n\n    ###########################################################################################################\n    #\n    #    training & test\n    #\n    #########################################################################################################\n    def calculate_loss(self, ground_truth, pred):\n        loss = pd.DataFrame({'LogLoss': log_loss(ground_truth, pred)},\n                               index = ['scores'])\n        return loss\n    \n    def calculate_scores(self, ground_truth, pred):\n        scores = pd.DataFrame({'Accuracy': accuracy_score(ground_truth, pred),\n                              'F1': f1_score(ground_truth, pred),\n                              'Precision': precision_score(ground_truth, pred),\n                              'Recall': recall_score(ground_truth, pred)},\n                               index = ['scores'])\n        return scores\n    \n    def test(self):\n        xgb_test = xgb.DMatrix(self.test_data)\n        # XGboost\u306e\u7d50\u679c\n        result = self.best_model.predict(xgb_test, iteration_range=(0, self.model.best_iteration) )\n        \n        final_data = {'PassengerId': pd.read_csv(\"..\/input\/titanic\/test.csv\")[\"PassengerId\"], 'Survived': np.round(result).astype(np.int8)}\n        submission = pd.DataFrame(data=final_data)\n        submission.to_csv('submission_lgbm.csv',index =False)\n        \n    def train(self, params, print_result_flag=False):\n        X_train, X_val, y_train, y_val = train_test_split(self.train_data_x, self.train_data_y, random_state=40)\n        xgb_train = xgb.DMatrix(X_train, label=y_train)\n        xgb_val = xgb.DMatrix(X_val, label=y_val)\n        \n        # params\n        params[\"booster\"] = 'gbtree'\n        params[\"objective\"] = 'binary:logistic'\n        params[\"eval_metric\"] = 'auc'\n        params[\"eta\"] = 0.01\n\n        # \u5b66\u7fd2\u306e\u7d4c\u904e\u3092\u4fdd\u5b58\n        evaluation_results = {}\n        evals = [(xgb_train, 'train'), (xgb_val, 'val')]\n        \n        with redirect_stdout(open(os.devnull, 'w')):\n\n            self.model = xgb.train(params,\n                              xgb_train,\n                              num_boost_round=1500,\n                              evals=evals,\n                              evals_result=evaluation_results,\n                              early_stopping_rounds=800,\n                             )\n        \n        # XGBoost\u63a8\u8ad6\n        self.y_pred_train = self.model.predict(xgb_train, iteration_range=(0, self.model.best_iteration) )\n        self.y_pred_val = self.model.predict(xgb_val, iteration_range=(0, self.model.best_iteration) )\n        # self.y_pred_val = self.model.predict(xgb_val, ntree_limit = self.model.best_ntree_limit )\n\n        train_scores = self.calculate_scores(y_train, np.round(self.y_pred_train))\n        train_loss = self.calculate_loss(y_train, self.y_pred_train)\n        train_scores = train_scores.join(train_loss)\n        if print_result_flag:\n            print(\"training eval\")\n            display(train_scores)\n        \n        val_scores = self.calculate_scores(y_val, np.round(self.y_pred_val))\n        val_loss = self.calculate_loss(y_val, self.y_pred_val)\n        val_scores = val_scores.join(val_loss)\n\n        if print_result_flag:\n            print(\"validation eval\")\n            display(val_scores)\n        \n        self.history.append((params, val_scores[\"LogLoss\"].to_numpy(), val_scores, copy.deepcopy(self.model)))\n        \n        return {\"loss\": val_scores[\"LogLoss\"].to_numpy(), \"status\": STATUS_OK}\n    \n    def train_with_hyper_opt(self):\n        training_params_space = {\n            'max_depth': hp.randint('max_depth', 3, 9),\n            'min_child_weight': hp.loguniform('min_child_weight', np.log(0.01), np.log(10.0)),\n            'gamma': hp.quniform('gamma', 0, 0.5, 0.05),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 0.95, 0.05),\n            'subsample': hp.quniform('subsample', 0.5, 0.95, 0.05),\n            #'alpha': hp.loguniform('alpha', np.log(1e-8), np.log(1.0)),\n            #'lambda': hp.loguniform('lambda', np.log(1e-8), np.log(10.0)),\n        }\n        \n        max_evals = 20\n        trials = Trials()\n        self.history = []\n        \n        fmin(self.train, training_params_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n        \n        self.history = sorted(self.history, key=lambda tpl: tpl[1])\n        best=self.history[0]\n        print(f'best params:{best[0]}, score:{best[1]}')\n        display(best[2])\n        self.best_model = best[3]\n        ","d2d41280":"my_xgboost = XgBoost()\nmy_xgboost.train_data_x","fc55a5fd":"training_params = {\n    'eta': 0.01, \n    'num_round': 1000,\n    'max_depth': 5,\n    'min_child_weight': 1.0,\n    'gamma': 0.0,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'alpha': 0.0,\n    'lambda': 1.0,\n    'nthread': 4, \n}\nmy_xgboost.train(training_params, print_result_flag=True)","0b99a35a":"my_xgboost.train_with_hyper_opt()","d351b6f3":"my_xgboost.test()","9a24bd2d":"xgb.to_graphviz(my_xgboost.best_model)","ebb86459":"xgb.plot_importance(my_xgboost.best_model)","e7665257":"import pandas as pd\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\n\nimport os\nfrom contextlib import redirect_stdout\nimport copy\n\n# \u8a55\u4fa1\u6307\u6a19\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import log_loss","49b4459e":"class SVMFeatures:\n    def __init__(self):\n        pass\n    # create features for the model\n    def data_process(self, data, params):\n        new_params = copy.deepcopy(params)\n        self.ticket_data(data)\n        if \"Ticket_processed_one_hot\" in new_params:\n            new_params.remove(\"Ticket_processed_one_hot\")\n            tmp = self.one_hot_ticket_data(data)\n            data = pd.concat([data, tmp], axis=1)\n            new_params.extend(tmp.columns)\n        self.name_data(data)\n        self.embarked_data(data)\n        if \"Embarked_processed_one_hot\" in new_params:\n            new_params.remove(\"Embarked_processed_one_hot\")\n            tmp = self.one_hot_embarked_data(data)\n            data = pd.concat([data, tmp], axis=1)\n            new_params.extend(tmp.columns)\n            \n        self.cabin_data(data)\n        self.fare_data(data)\n        self.parch_data(data)\n        self.sibsp_data(data)\n        self.pclass_data(data)\n        self.age_data(data)\n        self.sex_data(data)\n        # [\"Embarked_processed_one_hot\", Ticket_processed\", \"Name_processed\", \"Embarked_processed\", \"Cabin_processed\", \"Fare_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Age_processed\", \"Sex_processed\"]\n        return data[new_params]\n\n    def ticket_data(self, data):\n        data_ticket = data[\"Ticket\"].to_numpy()\n\n        ret_val = []\n        for ticket in data_ticket:\n            split_data = ticket.split(\" \")\n            if split_data[0].isdigit():\n                ret_val.append(0)\n            elif split_data[0]==\"PC\":\n                ret_val.append(1)\n            elif split_data[0]==\"C.A.\" or split_data[0]==\"CA\"or split_data[0]==\"CA.\":\n                ret_val.append(2)\n            elif split_data[0]==\"A\/5.\" or split_data[0]==\"A\/5\"or split_data[0]==\"A.\/5.\"or split_data[0]==\"A.5.\":\n                ret_val.append(3)\n            else:\n                ret_val.append(4)\n        ret_val = ret_val \/ max(abs( np.asarray(ret_val)))\n        data[\"Ticket_processed\"] = np.asarray(ret_val).astype(np.float32)\n\n    def one_hot_ticket_data(self, data):\n        data_ticket = data[\"Ticket\"].to_numpy()\n\n        ret_val = []\n        for ticket in data_ticket:\n            split_data = ticket.split(\" \")\n            if split_data[0].isdigit():\n                ret_val.append(0)\n            elif split_data[0]==\"PC\":\n                ret_val.append(1)\n            elif split_data[0]==\"C.A.\" or split_data[0]==\"CA\"or split_data[0]==\"CA.\":\n                ret_val.append(2)\n            elif split_data[0]==\"A\/5.\" or split_data[0]==\"A\/5\"or split_data[0]==\"A.\/5.\"or split_data[0]==\"A.5.\":\n                ret_val.append(3)\n            else:\n                ret_val.append(4)\n                \n        ret_val = pd.get_dummies(ret_val, columns=\"Ticket\" ,prefix='Ticket', prefix_sep='-')\n        return ret_val\n                        \n    def name_data(self, data):\n        data_name = data[\"Name\"].to_numpy()\n\n        ret_val = []\n        for name in data_name:\n            name_split = name.split(\" \")\n            target_name = \"\"\n            for target in name_split:\n                try:\n                    if target[-1] == \".\":\n                        target_name = target\n                        break\n                except Exception as e:\n                    print(str(e)+\":\"+name+\":\"+target)\n\n            if target_name == \"Mr.\":\n                ret_val.append(0)\n            elif target_name == \"Mrs.\":\n                ret_val.append(1)\n            elif target_name == \"Miss.\":\n                ret_val.append(2)\n            else:\n                ret_val.append(3)            \n                \n        ret_val = ret_val \/ max(abs( np.asarray(ret_val)))\n        data[\"Name_processed\"] = np.asarray(ret_val).astype(np.float32)\n\n    def embarked_data(self, data):\n        data_embarked = data[\"Embarked\"].to_numpy()\n        tmp = []\n        for d in data_embarked:\n            try:\n                if np.isnan(d):\n                    tmp.append(-1)\n            except:\n                if d[0] == \"S\":\n                    tmp.append(3)\n                elif d[0] == \"Q\":\n                    tmp.append(2)\n                elif d[0] == \"C\":\n                    tmp.append(1)\n                else:\n                    tmp.append(-1)\n                    \n        tmp = tmp \/ max(abs(np.asarray(tmp)))\n        data[\"Embarked_processed\"] = np.asarray(tmp).astype(np.float32)\n    \n    def one_hot_embarked_data(self, data):\n        data_embarked = data[\"Embarked\"].to_numpy()\n        tmp = []\n        for d in data_embarked:\n            try:\n                if np.isnan(d):\n                    tmp.append(-1)\n            except:\n                if d[0] == \"S\":\n                    tmp.append(3)\n                elif d[0] == \"Q\":\n                    tmp.append(2)\n                elif d[0] == \"C\":\n                    tmp.append(1)\n                else:\n                    tmp.append(-1)\n                    \n        tmp = pd.get_dummies(tmp, columns=\"Embarked\" ,prefix='Embarked', prefix_sep='-')\n        if \"Embarked--1\" in tmp.columns: # not contain \"-1\" in test data\n            tmp = tmp.drop(\"Embarked--1\", axis=1)\n        #data[\"Embarked_processed\"] = np.asarray(tmp).astype(np.float32)\n        return tmp\n    \n    def cabin_data(self, data):\n        data_cabin = data[\"Cabin\"].to_numpy()\n        tmp = []\n        for d in data_cabin:\n            try:\n                if np.isnan(d):\n                    tmp.append(-2)\n            except:\n                if d[0] == \"A\":\n                    tmp.append(7)\n                elif d[0] == \"B\":\n                    tmp.append(6)\n                elif d[0] == \"C\":\n                    tmp.append(5)\n                elif d[0] == \"D\":\n                    tmp.append(4)\n                elif d[0] == \"E\":\n                    tmp.append(3)\n                elif d[0] == \"F\":\n                    tmp.append(2)\n                elif d[0] == \"G\":\n                    tmp.append(1)\n                else:\n                    tmp.append(-1)\n        tmp = tmp \/ max(abs(np.asarray(tmp)))\n        data[\"Cabin_processed\"] = np.asarray(tmp).astype(np.float32)\n\n    def fare_data(self, data):\n        data_fare = data[\"Fare\"].to_numpy()\n        data_fare[np.isnan(data_fare)] = -1.0\n        data_fare = data_fare \/ max(abs(data_fare))\n        data[\"Fare_processed\"] = data_fare\n        \n    def parch_data(self, data):\n        data_parch = data[\"Parch\"]\n        #data_parch = data_parch \/ max(abs(data_parch))\n        data[\"Parch_processed\"] = data_parch\n        \n    def sibsp_data(self, data):\n        data_sibsp = data[\"SibSp\"]\n        #data_sibsp = data_sibsp \/ max(abs(data_sibsp))\n        data[\"SibSp_processed\"] = data_sibsp\n        \n    def pclass_data(self, data):\n        data_pclass = data[\"Pclass\"]\n        #data_pclass = data_pclass \/ max(abs(data_pclass))\n        data[\"Pclass_processed\"] = data_pclass\n        \n    def age_data(self, data):\n        data_age = data[\"Age\"]\n        data_age[np.isnan(data_age)] = -1.0       \n        data_age = data_age \/ max(abs(data_age))\n        data[\"Age_processed\"] = data_age\n        \n    def sex_data(self, data):\n        data_sex = data[\"Sex\"].to_numpy()\n        data_sex[data_sex==\"male\"] = 0.0\n        data_sex[data_sex==\"female\"] = 1.0\n        #data_sex = data_sex \/ max(abs(data_sex))\n        data[\"Sex_processed\"] = data_sex.astype(np.float32)","3795b062":"class SVM:\n    def __init__(self):\n        self.create_features = SVMFeatures()\n        self.train_data_raw = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n        self.test_data_raw = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n        \n        # don't use Embarked, Fare, Age beaed on Permutaion Importance\n        # self.data_params = [\"Ticket_processed_one_hot\", \"Embarked_processed_one_hot\", \"Ticket_processed\", \"Name_processed\", \"Embarked_processed\", \"Cabin_processed\", \"Fare_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Age_processed\", \"Sex_processed\"]\n        self.data_params = [\"Name_processed\",  \"Cabin_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Sex_processed\"]\n\n        self.train_data_x = self.create_features.data_process(self.train_data_raw, self.data_params)\n        self.train_data_y = self.train_data_raw[\"Survived\"]\n        self.test_data = self.create_features.data_process(self.test_data_raw, self.data_params)\n        \n        self.data_params = self.train_data_x.columns\n                \n\n    ###########################################################################################################\n    #\n    #    training & test\n    #\n    #########################################################################################################\n    def calculate_loss(self, ground_truth, pred):\n        loss = pd.DataFrame({'LogLoss': log_loss(ground_truth, pred)},\n                               index = ['scores'])\n        return loss\n    \n    def calculate_scores(self, ground_truth, pred):\n        scores = pd.DataFrame({'Accuracy': accuracy_score(ground_truth, pred),\n                              'F1': f1_score(ground_truth, pred),\n                              'Precision': precision_score(ground_truth, pred),\n                              'Recall': recall_score(ground_truth, pred)},\n                               index = ['scores'])\n        return scores\n    \n    def test(self):\n        result = self.model.predict(self.test_data)\n        \n        final_data = {'PassengerId': pd.read_csv(\"..\/input\/titanic\/test.csv\")[\"PassengerId\"], 'Survived': np.round(result).astype(np.int8)}\n        submission = pd.DataFrame(data=final_data)\n        submission.to_csv('submission_svm.csv',index =False)\n        \n    def train(self, params, print_result_flag=False):\n        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.train_data_x, self.train_data_y, random_state=40)   \n        \n        with redirect_stdout(open(os.devnull, 'w')):\n            self.model = svm.SVC(kernel=params[\"kernel\"],C=params[\"C\"])\n            self.model.fit(self.X_train, self.y_train)\n        # XGBoost\u63a8\u8ad6\n        self.y_pred_train = self.model.predict(self.X_train)\n        self.y_pred_val = self.model.predict(self.X_val)\n\n        train_scores = self.calculate_scores(self.y_train, np.round(self.y_pred_train))\n        train_loss = self.calculate_loss(self.y_train, self.y_pred_train)\n        train_scores = train_scores.join(train_loss)\n        if print_result_flag:\n            print(\"training eval\")\n            display(train_scores)\n        \n        val_scores = self.calculate_scores(self.y_val, np.round(self.y_pred_val))\n        val_loss = self.calculate_loss(self.y_val, self.y_pred_val)\n        val_scores = val_scores.join(val_loss)\n\n        if print_result_flag:\n            print(\"validation eval\")\n            display(val_scores)\n        ","dff1c26c":"my_svm = SVM()\ndisplay(my_svm.create_features.one_hot_ticket_data(my_svm.train_data_raw))\ndisplay(my_svm.create_features.one_hot_ticket_data(my_svm.test_data_raw))","7a70d0a7":"display(my_svm.train_data_x)\ndisplay(my_svm.test_data)","97a10284":"my_svm = SVM()\nmy_svm.train_data_x # show training data","aa659e65":"training_params = {\n    'kernel': 'rbf', \n    'C': 0.1,\n}\nmy_svm.train(training_params, print_result_flag=True)","f2b0b3d4":"my_svm.test()","4377c106":"from sklearn.inspection import permutation_importance\n\nr = permutation_importance(my_svm.model, my_svm.X_val, my_svm.y_val, n_repeats=30, random_state=0)\n\nfor i in r.importances_mean.argsort()[::-1]:\n     print(f\"{my_svm.data_params[i]:<20}\"\n          f\"{r.importances_mean[i]:.3f}\"\n          f\" +\/- {r.importances_std[i]:.3f}\")\n        \n# [\"Ticket_processed\", \"Name_processed\", \"Embarked_processed\", \"Cabin_processed\", \"Fare_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Age_processed\", \"Sex_processed\"]\n# [\"Name_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Sex_processed\"]\n","ca79ccf7":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\n\nimport os\nfrom contextlib import redirect_stdout\n\n# \u8a55\u4fa1\u6307\u6a19\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import log_loss\n\n# keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers.core import Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","caeaa345":"class NNFeatures:\n    def __init__(self):\n        self.max_age = -1\n        pass\n    \n    def data_process(self, data, params):\n        self.ticket_data(data)\n        self.name_data(data)\n        self.embarked_data(data)\n        self.cabin_data(data)\n        self.fare_data(data)\n        self.parch_data(data)\n        self.sibsp_data(data)\n        self.pclass_data(data)\n        self.age_data(data)\n        self.sex_data(data)\n        \n        return data[params]\n\n    def ticket_data(self, data):\n        data_ticket = data[\"Ticket\"].to_numpy()\n\n        ret_val = []\n        for ticket in data_ticket:\n            split_data = ticket.split(\" \")\n            if split_data[0].isdigit():\n                ret_val.append(0)\n            elif split_data[0]==\"PC\":\n                ret_val.append(1)\n            elif split_data[0]==\"C.A.\" or split_data[0]==\"CA\"or split_data[0]==\"CA.\":\n                ret_val.append(2)\n            elif split_data[0]==\"A\/5.\" or split_data[0]==\"A\/5\"or split_data[0]==\"A.\/5.\"or split_data[0]==\"A.5.\":\n                ret_val.append(3)\n            else:\n                ret_val.append(4)\n        ret_val = ret_val \/ max(abs( np.asarray(ret_val)))\n        data[\"Ticket_processed\"] = np.asarray(ret_val).astype(np.float32)\n        \n    def name_data(self, data):\n        data_name = data[\"Name\"].to_numpy()\n\n        ret_val = []\n        for name in data_name:\n            name_split = name.split(\" \")\n            target_name = \"\"\n            for target in name_split:\n                try:\n                    if target[-1] == \".\":\n                        target_name = target\n                        break\n                except Exception as e:\n                    print(str(e)+\":\"+name+\":\"+target)\n\n            if target_name == \"Mr.\":\n                ret_val.append(0)\n            elif target_name == \"Mrs.\":\n                ret_val.append(1)\n            elif target_name == \"Miss.\":\n                ret_val.append(2)\n            else:\n                ret_val.append(3)            \n                \n        ret_val = ret_val \/ max(abs( np.asarray(ret_val)))\n        data[\"Name_processed\"] = np.asarray(ret_val).astype(np.float32)\n\n    def embarked_data(self, data):\n        data_embarked = data[\"Embarked\"].to_numpy()\n        tmp = []\n        for d in data_embarked:\n            try:\n                if np.isnan(d):\n                    tmp.append(-1)\n            except:\n                if d[0] == \"S\":\n                    tmp.append(3)\n                elif d[0] == \"Q\":\n                    tmp.append(2)\n                elif d[0] == \"C\":\n                    tmp.append(1)\n                else:\n                    tmp.append(-1)\n                    \n        tmp = tmp \/ max(abs(np.asarray(tmp)))\n        data[\"Embarked_processed\"] = np.asarray(tmp).astype(np.float32)\n        \n    def cabin_data(self, data):\n        data_cabin = data[\"Cabin\"].to_numpy()\n        tmp = []\n        for d in data_cabin:\n            try:\n                if np.isnan(d):\n                    tmp.append(-2)\n            except:\n                if d[0] == \"A\":\n                    tmp.append(7)\n                elif d[0] == \"B\":\n                    tmp.append(6)\n                elif d[0] == \"C\":\n                    tmp.append(5)\n                elif d[0] == \"D\":\n                    tmp.append(4)\n                elif d[0] == \"E\":\n                    tmp.append(3)\n                elif d[0] == \"F\":\n                    tmp.append(2)\n                elif d[0] == \"G\":\n                    tmp.append(1)\n                else:\n                    tmp.append(-1)\n        tmp = tmp \/ max(abs(np.asarray(tmp)))\n        data[\"Cabin_processed\"] = np.asarray(tmp).astype(np.float32)\n\n    def fare_data(self, data):\n        data_fare = data[\"Fare\"].to_numpy()\n        data_fare[np.isnan(data_fare)] = -1.0\n        data_fare = data_fare \/ max(abs(data_fare))\n        data[\"Fare_processed\"] = data_fare\n        \n    def parch_data(self, data):\n        data_parch = data[\"Parch\"]\n        #data_parch = data_parch \/ max(abs(data_parch))\n        data[\"Parch_processed\"] = data_parch\n        \n    def sibsp_data(self, data):\n        data_sibsp = data[\"SibSp\"]\n        #data_sibsp = data_sibsp \/ max(abs(data_sibsp))\n        data[\"SibSp_processed\"] = data_sibsp\n        \n    def pclass_data(self, data):\n        data_pclass = data[\"Pclass\"]\n        #data_pclass = data_pclass \/ max(abs(data_pclass))\n        data[\"Pclass_processed\"] = data_pclass\n        \n    def age_data(self, data):\n        data_age = data[\"Age\"]\n        data_age[np.isnan(data_age)] = -1.0  \n        if self.max_age == -1:\n            self.max_age = max(abs(data_age))\n        data_age = data_age \/ max(abs(data_age))\n        data[\"Age_processed\"] = data_age\n        \n    def sex_data(self, data):\n        data_sex = data[\"Sex\"].to_numpy()\n        data_sex[data_sex==\"male\"] = 0.0\n        data_sex[data_sex==\"female\"] = 1.0\n        data_sex = data_sex \/ max(abs(data_sex))\n        data[\"Sex_processed\"] = data_sex.astype(np.float32)\n","e1b1fb98":"from keras.wrappers.scikit_learn import KerasClassifier\n\n# don't use Embarked and Cabin based on the result of Permutation Importance\nnn_data_params = [\"Ticket_processed\", \"Name_processed\", \"Embarked_processed\",\"Cabin_processed\", \"Fare_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Age_processed\", \"Sex_processed\"]\n\nclass NeuralNet:\n    #[\"Ticket_processed\", \"Name_processed\", \"Embarked_processed\", \"Cabin_processed\", \"Fare_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Age_processed\", \"Sex_processed\"]\n    \n\n    def __init__(self):\n        self.create_features = NNFeatures()\n        self.train_data_raw = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n        self.test_data_raw = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n        \n\n        self.train_data_x = self.create_features.data_process(self.train_data_raw, nn_data_params)\n        #self.train_data_y = pd.get_dummies(self.train_data_raw[\"Survived\"])\n        self.train_data_y = self.train_data_raw[\"Survived\"]\n\n        self.test_data = self.create_features.data_process(self.test_data_raw, nn_data_params)\n    \n    ###########################################################################################################\n    #\n    #    model \n    #\n    #########################################################################################################\n    def create_model(self):\n        inputs = keras.Input(shape=(len(nn_data_params),))\n        x = layers.Dense(256, activation=\"relu\")(inputs)\n        x = layers.Dense(512, activation=\"relu\")(x)\n        x = layers.Dense(256, activation=\"relu\")(x)\n        outputs = layers.Dense(2, activation=\"sigmoid\")(x)\n        return keras.Model(inputs=inputs, outputs=outputs)\n    \n    def create_model_self_attention(self):\n        activation_fun = \"swish\"\n        \n        inputs = keras.Input(shape=(len(nn_data_params),))\n        x = inputs\n        \n        query_encoding = x\n        query_value_attention = tf.keras.layers.Attention()([x, x])\n        x = tf.keras.layers.Concatenate()([query_encoding, query_value_attention])\n        x = layers.Dense(256, activation=activation_fun)(x)\n        x = BatchNormalization()(x)\n        \n        x_ = x\n        \n        query_encoding = x\n        query_value_attention = tf.keras.layers.Attention()([x, x])\n        x = tf.keras.layers.Concatenate()([query_encoding, query_value_attention])\n        x = layers.Dense(512, activation=activation_fun)(x)\n        x = BatchNormalization()(x)\n        \n        x = layers.Dense(512, activation=activation_fun)(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.5)(x)\n        \n        #x = keras.layers.Concatenate()([x, x_])\n        x = layers.Dense(256, activation=activation_fun)(x)\n        x = BatchNormalization()(x)\n        \n        x = layers.Dense(256, activation=activation_fun)(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.5)(x)\n        \n        outputs = layers.Dense(2, activation=\"softmax\")(x)\n        return keras.Model(inputs, outputs)\n    \n    def create_model_5dim_layer_perceptron(self, input_dim=len(nn_data_params), \\\n                                           activation=\"relu\", \\\n                                           optimizer=\"adam\", \\\n                                           out_dim=512, \\\n                                           dropout=0.5):\n\n        model = Sequential()\n\n        # \u5165\u529b\u5c64 - \u96a0\u308c\u5c641\n        model.add(Dense(input_dim=input_dim, units=out_dim))\n        model.add(BatchNormalization())\n        model.add(Activation(activation))\n        #model.add(Dropout(dropout))\n\n        # \u96a0\u308c\u5c641 - \u96a0\u308c\u5c642\n        model.add(Dense(units=out_dim))\n        model.add(BatchNormalization())\n        model.add(Activation(activation))\n        #model.add(Dropout(dropout))\n\n        # \u96a0\u308c\u5c641 - \u96a0\u308c\u5c642\n        model.add(Dense(units=out_dim))\n        model.add(BatchNormalization())\n        model.add(Activation(activation))\n        #model.add(Dropout(dropout))\n        \n        # \u96a0\u308c\u5c642 - \u96a0\u308c\u5c643\n        model.add(Dense(units=out_dim))\n        model.add(BatchNormalization())\n        model.add(Activation(activation))\n        model.add(Dropout(dropout))\n\n        # \u96a0\u308c\u5c642 - \u96a0\u308c\u5c644\n        model.add(Dense(units=out_dim))\n        model.add(BatchNormalization())\n        model.add(Activation(activation))\n        model.add(Dropout(dropout))\n        \n        # \u96a0\u308c\u5c643 - \u51fa\u529b\u5c64\n        #model.add(Dense(units=1))\n        #model.add(Activation(\"sigmoid\"))\n        model.add(Dense(units=2))\n        model.add(Activation(\"softmax\"))\n        \n        #model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n        return model\n    \n     ###########################################################################################################\n    #\n    #    training & test\n    #\n    #########################################################################################################   \n    \n    def calculate_loss(self, ground_truth, pred):\n        loss = pd.DataFrame({'LogLoss': log_loss(ground_truth, pred)},\n                               index = ['scores'])\n        return loss\n    \n    def calculate_scores(self, ground_truth, pred):\n        scores = pd.DataFrame({'Accuracy': accuracy_score(ground_truth, pred),\n                              'F1': f1_score(ground_truth, pred),\n                              'Precision': precision_score(ground_truth, pred),\n                              'Recall': recall_score(ground_truth, pred)},\n                               index = ['scores'])\n        return scores\n    \n    def test(self):        \n        result = self.model.predict(self.test_data)\n        result = np.argmax(result, axis=1)\n        result = pd.DataFrame(result, columns=['Survived'])\n        submission = pd.concat([pd.read_csv(\"..\/input\/titanic\/test.csv\")[\"PassengerId\"], result], axis=1)\n        submission.to_csv('submission_nn.csv',index =False)\n        \n    def train(self, print_result_flag=False):\n        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.train_data_x, self.train_data_y, random_state=40)        \n        \n        #with redirect_stdout(open(os.devnull, 'w')):\n        #self.model = KerasClassifier(build_fn=self.create_model_5dim_layer_perceptron, epochs=75, batch_size=128, verbose=2)    \n        self.model = self.create_model_self_attention()\n        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n\n        # self.model = KerasClassifier(build_fn=self.model, epochs=75, batch_size=128, verbose=2)    \n\n        self.model.fit(self.x_train, self.y_train, epochs=200, batch_size=512, verbose=2)\n       #  self.model.fit(self.x_train, self.y_train)\n\n        # prediction\n        self.y_pred_train = self.model.predict(self.x_train)\n        self.y_pred_val = self.model.predict(self.x_val)\n\n        #print(np.argmax(self.y_pred_train))\n        #print(self.y_train)\n        \n        train_scores = self.calculate_scores(self.y_train, np.argmax(self.y_pred_train, axis=1))\n        train_loss = self.calculate_loss(self.y_train, self.y_pred_train)\n        train_scores = train_scores.join(train_loss)\n        if print_result_flag:\n            print(\"training eval\")\n            display(train_scores)\n        \n        val_scores = self.calculate_scores(self.y_val, np.argmax(self.y_pred_val, axis=1))\n        val_loss = self.calculate_loss(self.y_val, self.y_pred_val)\n        val_scores = val_scores.join(val_loss)\n\n        if print_result_flag:\n            print(\"validation eval\")\n            display(val_scores)\n        ","59719cd9":"my_nn = NeuralNet()\nmy_nn.train_data_x","7d2dec7e":"my_nn.train(print_result_flag=True)","93a54867":"my_nn.test()","eafed9bf":"# not working now\n'''\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_nn.model, random_state=1).fit(my_nn.x_val,my_nn.y_val)\nX_cols = np.array(my_nn.x_val.columns.tolist(), dtype='str')\neli5.show_weights(perm, top=2500, feature_names = X_cols)\n\n        \n# [\"Ticket_processed\", \"Name_processed\", \"Embarked_processed\", \"Cabin_processed\", \"Fare_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Age_processed\", \"Sex_processed\"]\n# [\"Name_processed\", \"Parch_processed\", \"SibSp_processed\", \"Pclass_processed\", \"Sex_processed\"]\n'''","540a2360":"import os\nimport glob\nimport pandas as pd\n\nclass Ensemble:\n    def __init__(self):\n        self.result_files_dir = \"\/kaggle\/working\/\"\n        self.ensemble_final_result = \"ensemble_final_result.csv\"\n        self.raw_data = self.read_files()\n        display(self.raw_data)\n        self.ensemble_result = self.ensemble()\n        \n    def read_files(self):\n        files = glob.glob(self.result_files_dir+\"*csv\")\n        data = pd.read_csv(files[0])\n        print(files[0])\n        for filename in files:\n            if filename == files[0]:\n                continue\n            if filename == self.result_files_dir+self.ensemble_final_result:\n                continue\n            print(filename)\n            tmp = pd.read_csv(filename)\n            index_name = filename.split(\".\")[0]\n            data[index_name] = tmp[\"Survived\"]\n        return data\n    \n    def ensemble(self):\n        return self.raw_data.mode(axis=1)[0].tolist()\n\n    def out_submission(self):\n        submission = pd.concat([pd.read_csv(\"..\/input\/titanic\/test.csv\")[\"PassengerId\"], pd.DataFrame(self.ensemble_result, columns=[\"Survived\"])], axis=1)\n        display(submission)\n        submission.to_csv(self.ensemble_final_result,index =False)","4ec43440":"my_ensemble = Ensemble()","813c8f9f":"my_ensemble.out_submission()","78e92d57":"## Feature Engineering","3b3978e7":"## show the learned xgboost model","57c42ca1":"## Feature Engineering","e6117499":"## Permutation Importance - Show the importance of features for the training ","f38bbc45":"# ensemble","8063eed3":"# Neural Network","658574de":"# SVM","82e599e7":"## Train & Validation & Test","99a67b62":"## Traing & Validation & Test","849ac5a9":"# My Advanced Titanic\n* This is my advenced titanic project. This code aims to get high score based on my first kaggle submission with titanic.\n  * XgBoost\n  * SVM\n  * Neural Net\n  * Ensemble the model above\n* Post Process and Tuning\n  * Permutation Importance","68e98a67":"# XgBoost","c9c9f58f":"## Feature Engineering"}}