{"cell_type":{"1df76b4b":"code","82a48e35":"code","25a14ec8":"code","31b12646":"code","7a712459":"code","7f735421":"code","9d080a6b":"code","7a23e181":"code","5aeb4ffa":"code","7daea4da":"code","ecfdb1b3":"code","51fdee07":"code","b248d2bf":"code","a9f6cb57":"code","a63c6da9":"code","2def3724":"code","929bb9ff":"code","f343e782":"code","3e2b6cf4":"code","6d4af6ad":"code","f27b8b9a":"code","c46231eb":"code","895f945d":"code","039e132e":"code","1f75e449":"code","2f7d1570":"code","de3ad62c":"code","c3dda47f":"code","9882ac03":"code","9ea58837":"code","fcc8e323":"code","877bf3d3":"code","36b91386":"code","5c07a5e0":"code","77c7f27b":"code","6778036c":"code","f0c1b120":"code","629e123f":"code","4f7c094b":"code","909d0f86":"code","6e6e2d1e":"code","168aeab4":"code","61cf5b79":"code","87df782d":"code","0f890c8b":"code","5608d1bc":"code","ce251f78":"code","98f37643":"code","3ddc31ac":"code","8774dbb8":"code","3779ef80":"code","4baba8b3":"code","ae6e3825":"code","5d23aac7":"code","176c4c92":"code","c02703c8":"code","97f472c2":"code","fc5b4a11":"code","45684b2b":"code","abe644a1":"code","7b7c43ee":"code","672a6e10":"code","be439433":"code","2c66c4d9":"code","cac4d34e":"code","7c74d65b":"code","ef5b747c":"code","ae0c0dd9":"code","32db1929":"code","0e57886d":"code","bb5d1534":"code","492e0866":"code","be20ccb8":"code","aa2b30a9":"code","c8a23e06":"code","32e95b95":"code","e77077a5":"code","7df630a4":"code","8c1d5bd9":"code","223ee09b":"code","ff0df8ea":"code","fd82dbd5":"code","262c8944":"code","126d5de7":"code","dde12a26":"code","fc99dd39":"code","5f3ee402":"code","38ccc5d6":"code","42d85ea9":"code","2cbc724a":"code","93917fb0":"code","c433c0a0":"code","394a86a2":"code","ca4270bf":"code","73525c23":"code","36f59137":"code","066980b3":"code","1d6f7fe1":"code","94df580d":"code","caa01b34":"code","5bb135b6":"code","af070bf9":"code","fdcf0f7b":"code","9d307261":"code","63db8d36":"code","00c12c19":"code","c7756987":"code","4c30d473":"code","df249afa":"code","645c37a1":"code","85f36e17":"code","54819d1e":"code","0b035ea3":"code","a2e57fc8":"code","5edb8a89":"code","915db6f7":"code","55401900":"code","644c4491":"code","c2ef70b4":"markdown","8e1d1b7b":"markdown","4c9d4e48":"markdown","6ac1b848":"markdown","60cd68d9":"markdown","e10e0002":"markdown","664cb8bc":"markdown","df79c369":"markdown","fe0d60f5":"markdown","88cb9e6e":"markdown","8a1cc072":"markdown","d9bc94a9":"markdown","c809fd73":"markdown","9b6b5857":"markdown","e21f9667":"markdown","d3c9c4c5":"markdown","5bb32b2b":"markdown","968d88a4":"markdown","3f59ebf4":"markdown","a7afae70":"markdown","5dd1d157":"markdown","1a179ef0":"markdown","19f0e29b":"markdown","5342dc7f":"markdown","bc746fd2":"markdown","aa82944d":"markdown"},"source":{"1df76b4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n#Visualization \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport missingno\nimport cufflinks as cf\nimport plotly.offline as po\nimport plotly.express as px\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,VotingClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(action='once')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82a48e35":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv') # Loading training Data\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv') # Loading Testing Data","25a14ec8":"## First 5 rows of training dataset\ntrain_data.head(5)","31b12646":"## First 5 rows of testing dataset\ntest_data.head(5)","7a712459":"## last 5 rows of training dataset\ntrain_data.tail(5)","7f735421":"## Last 5 rows of test dataset\ntest_data.tail(5)","9d080a6b":"## Data types of Each Attributes\ntrain_data.dtypes","7a23e181":"## Data Describe which gives mean max min 25% 50% and 75% answers of Each Feature\ntrain_data.describe()","5aeb4ffa":"train_data['PassengerId'].isnull().sum() ## Getting total null values in PassengerID feature.","7daea4da":"test_data['PassengerId'].isnull().sum() ## Getting total null values in PassengerID feature. Testing Data","ecfdb1b3":"train_data['Survived'].isnull().sum() ## Getting total null values in Survived feature.","51fdee07":"train_data['Pclass'].isnull().sum() ## Getting total null values in Pclass feature.","b248d2bf":"test_data['Pclass'].isnull().sum() ## Getting total null values in Pclass feature.","a9f6cb57":"train_data['Sex'].isnull().sum() ## Getting total null values in Sex feature.","a63c6da9":"test_data['Sex'].isnull().sum() ## Getting total null values in Sex feature.","2def3724":"train_data['Age'].isnull().sum() ## Getting total null values in Age feature.","929bb9ff":"test_data['Age'].isnull().sum() ## Getting total null values in Age feature.","f343e782":"mean = train_data['Age'].mean()\ntrain_data['Age'] = train_data['Age'].replace(np.NAN,mean) ### Here Missing Values are been replaced with Mean value","3e2b6cf4":"mean = test_data['Age'].mean()\ntest_data['Age'] = test_data['Age'].replace(np.NAN,mean) ### Here Missing Values are been replaced with Mean value","6d4af6ad":"train_data['SibSp'].isnull().sum() ## Getting total null values in SibSp feature.","f27b8b9a":"test_data['SibSp'].isnull().sum() ## Getting total null values in SibSp feature of Testing Data.","c46231eb":"train_data['Parch'].isnull().sum() ## Getting total null values in Parch feature.","895f945d":"test_data['Parch'].isnull().sum() ## Getting total null values in Parch feature testing Data.","039e132e":"train_data['Cabin'].isnull().sum() ## Getting total null values in Cabin feature.","1f75e449":"test_data['Cabin'].isnull().sum() ## Getting total null values in Cabin feature of testing Data.","2f7d1570":"train_data['Embarked'].isnull().sum() ## Getting total null values in Embarked feature.","de3ad62c":"test_data['Embarked'].isnull().sum() ## Getting total null values in Embarked feature of Testing Data.","c3dda47f":"train_data['Cabin'].value_counts().to_frame()","9882ac03":"test_data['Cabin'].value_counts().to_frame()","9ea58837":"train_data['Embarked'].value_counts().to_frame()","fcc8e323":"new_train_data = train_data.copy()","877bf3d3":"new_train_data.head()","36b91386":"new_train_data.drop('Name',axis=1,inplace=True)","5c07a5e0":"new_train_data['Embarked'] = new_train_data['Embarked'].replace(np.NAN,\"S\")\nnew_train_data['Embarked'].isnull().sum()","77c7f27b":"new_train_data['Sex'] = pd.get_dummies(new_train_data['Sex']) #### One Hot Encoder for Sex male =0 female = 1","6778036c":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlb = LabelEncoder()\nnew_train_data['Embarked'] = lb.fit_transform(new_train_data['Embarked'])\nonehotencode = OneHotEncoder(handle_unknown='ignore')\nencf = pd.DataFrame(onehotencode.fit_transform(new_train_data[['Embarked']]).toarray())\nnew_train_data = new_train_data.join(encf)\nnew_train_data.head()","f0c1b120":"## Rename the Columns 0,1,2 to C,Q,S\nnew_train_data.rename(columns={0:\"C\",1:\"Q\",2:\"S\"},inplace=True)","629e123f":"new_train_data.head()","4f7c094b":"fig=px.box(new_train_data,x='Fare')\nfig.show()","909d0f86":"fig = px.box(test_data,x='Fare')\nfig.show()","6e6e2d1e":"fig = px.box(new_train_data,x='Age')\nfig.show()","168aeab4":"fig = px.box(test_data,x='Age')\nfig.show()","61cf5b79":"new_train_data = new_train_data[new_train_data.Fare > 0]\nnew_train_data = new_train_data[new_train_data.Fare <= 300]","87df782d":"fig = px.box(new_train_data,x='Fare')\nfig.show()","0f890c8b":"missingno.matrix(new_train_data)","5608d1bc":"missingno.matrix(test_data)","ce251f78":"new_train_data['Cabin'].value_counts()","98f37643":"test_data['Cabin'].value_counts()","3ddc31ac":"new_train_data_cabin = new_train_data.copy()","8774dbb8":"new_test_data = test_data.copy()","3779ef80":"fare_mean = new_test_data['Fare'].mean()\nnew_test_data['Fare'] = new_test_data['Fare'].replace(np.NAN,fare_mean)\nmissingno.matrix(new_test_data)","4baba8b3":"new_train_data_cabin['Cabin_type'] = new_train_data_cabin['Cabin'].str.split(r'[0-9]').str[0].tolist()\nnew_test_data['Cabin_type'] = new_test_data['Cabin'].str.split(r'[0-9]').str[0].tolist()","ae6e3825":"new_train_data_cabin","5d23aac7":"new_test_data","176c4c92":"new_train_data_cabin['Cabin_type'].value_counts()","c02703c8":"new_test_data['Cabin_type'].value_counts()","97f472c2":"lb1 = LabelEncoder()\nnew_train_data_cabin['Cabin_type'] = lb1.fit_transform(new_train_data_cabin['Cabin_type'].astype(str))\nnew_test_data['Cabin_type'] = lb1.fit_transform(new_test_data['Cabin_type'].astype(str))","fc5b4a11":"new_train_data_cabin['Cabin_type'].value_counts()","45684b2b":"new_train_data_cabin['Cabin_type'].value_counts()","abe644a1":"new_train_data_cabin['Cabin_type'] = new_train_data_cabin['Cabin_type'].replace(0,11)\nnew_train_data_cabin['Cabin_type'] = new_train_data_cabin['Cabin_type'].replace(10,0)\nnew_train_data_cabin['Cabin_type'] = new_train_data_cabin['Cabin_type'].replace(11,10)","7b7c43ee":"new_test_data['Cabin_type'] = new_test_data['Cabin_type'].replace(0,11)\nnew_test_data['Cabin_type'] = new_test_data['Cabin_type'].replace(10,0)\nnew_test_data['Cabin_type'] = new_test_data['Cabin_type'].replace(11,10)","672a6e10":"new_train_data_cabin['Cabin_type'].value_counts()","be439433":"new_test_data['Cabin_type'].value_counts()","2c66c4d9":"new_train_data_cabin.head()","cac4d34e":"new_test_data.head()","7c74d65b":"new_train_data_cabin['Family'] = new_train_data_cabin['SibSp'] + new_train_data_cabin['Parch'] + 1\nnew_train_data_cabin","ef5b747c":"new_test_data['Family'] = new_test_data['SibSp'] + new_test_data['Parch'] + 1\nnew_test_data","ae0c0dd9":"new_test_data.drop('Name',axis=1,inplace=True)\nnew_test_data['Sex'] = pd.get_dummies(new_test_data['Sex'])\nnew_test_data","32db1929":"len(set(test_data.Ticket) - set(test_data.Ticket).intersection(set(new_train_data_cabin.Ticket))),len(set(test_data.Ticket)),len(test_data)","0e57886d":"len(set(new_test_data.Cabin_type) - set(new_test_data.Cabin_type).intersection(set(new_train_data_cabin.Cabin_type))),len(set(new_test_data.Cabin_type)),len(new_test_data)","bb5d1534":"group = new_train_data_cabin.groupby(['Sex']).agg({'Fare':['mean']})\ngroup.columns = ['mean_fare_sex']\ngroup.reset_index(inplace=True)\nnew_train_data_cabin = pd.merge(new_train_data_cabin,group,on=['Sex'], how='left')\nnew_train_data_cabin","492e0866":"group = new_test_data.groupby(['Sex']).agg({'Fare':['mean']})\ngroup.columns = ['mean_fare_sex']\ngroup.reset_index(inplace=True)\nnew_test_data = pd.merge(new_test_data,group,on=['Sex'], how='left')\nnew_test_data","be20ccb8":"new_train_data_cabin.drop('Ticket',axis=1, inplace=True)\nnew_train_data_cabin","aa2b30a9":"new_test_data.drop('Ticket',axis=1, inplace=True)\nnew_test_data","c8a23e06":"new_train_data_cabin.drop('PassengerId',axis=1,inplace=True)","32e95b95":"new_test_data.drop('PassengerId',axis=1,inplace=True)","e77077a5":"new_train_data_cabin['Pclass'].value_counts()","7df630a4":"new_train_data_cabin.drop('Cabin',axis=1,inplace=True)\nnew_train_data_cabin","8c1d5bd9":"new_test_data.drop('Cabin',axis=1,inplace=True)\nnew_test_data","223ee09b":"bins = np.linspace(min(new_train_data_cabin['Age']),max(new_train_data_cabin['Age']),4)\ngroup_names = [1,2,3]\nnew_train_data_cabin['Age_binned'] = pd.cut(new_train_data_cabin['Age'],bins,labels=group_names,include_lowest=True)\nnew_train_data_cabin","ff0df8ea":"bins = np.linspace(min(new_test_data['Age']),max(new_test_data['Age']),4)\ngroup_names = [1,2,3]\nnew_test_data['Age_binned'] = pd.cut(new_test_data['Age'],bins,labels=group_names,include_lowest=True)\nnew_test_data","fd82dbd5":"bins = np.linspace(min(new_train_data_cabin['Fare']),max(new_train_data_cabin['Fare']),4)\ngroup_names = [1,2,3]\nnew_train_data_cabin['Fare_binned'] = pd.cut(new_train_data_cabin['Fare'],bins,labels=group_names,include_lowest=True)\nnew_train_data_cabin","262c8944":"bins = np.linspace(min(new_test_data['Fare']),max(new_test_data['Fare']),4)\ngroup_names = [1,2,3]\nnew_test_data['Fare_binned'] = pd.cut(new_test_data['Fare'],bins,labels=group_names,include_lowest=True)\nnew_test_data","126d5de7":"new_train_data_cabin.duplicated().sum()","dde12a26":"new_train_data_cabin.drop_duplicates(inplace=True)\nnew_train_data_cabin.duplicated().sum()","fc99dd39":"new_train_data_cabin","5f3ee402":"training = new_train_data_cabin.copy()\ntraining","38ccc5d6":"training","42d85ea9":"training.drop('C',axis=1,inplace=True)\ntraining.drop('Q',axis=1,inplace=True)\ntraining.drop('S',axis=1,inplace=True)\ntraining","2cbc724a":"x_data = training.drop('Survived',axis=1,inplace=False) #Independent Feature\ny_data = training[['Survived']] #Dependent Feature","93917fb0":"x_data.head()","c433c0a0":"y_data.head()","394a86a2":"x_data.drop('mean_fare_sex',axis=1,inplace = True)","ca4270bf":"x_data['Age_binned']= x_data['Age_binned'].astype('int16')\nx_data['Fare_binned']= x_data['Fare_binned'].astype('int16')\nx_data.dtypes","73525c23":"new_test_data.drop('mean_fare_sex',axis=1,inplace = True)","36f59137":"new_test_data['Age_binned']= new_test_data['Age_binned'].astype('int16')\nnew_test_data['Fare_binned']= new_test_data['Fare_binned'].astype('int16')\nnew_test_data.dtypes","066980b3":"x_data.describe()","1d6f7fe1":"############################## Decision Tree Classifier ##################################\nDt = DecisionTreeClassifier()\nDt.fit(x_data,y_data)\ny_pred = Dt.predict(x_data)\naccuracy_score(y_data,y_pred)*100","94df580d":"## Feature Importance\nfeature = []\nimportance = []\nfor i, column in enumerate(x_data):\n    print(\"The Feature importance for {} is : {}\".format(column,Dt.feature_importances_[i]))\n    feature.append(column)\n    importance.append(Dt.feature_importances_[i])","caa01b34":"plt.figure(figsize=(8,8))\nsns.barplot(x=importance,y=feature)\nplt.show()","5bb135b6":"rto =  RandomForestClassifier(n_estimators=28)\nrto.fit(x_data,y_data.values.ravel())\ny_pred = rto.predict(x_data)\naccuracy_score(y_data,y_pred)*100","af070bf9":"## Feature Importance\nfeature = []\nimportance = []\nfor i, column in enumerate(x_data):\n    print(\"The Feature importance for {} is : {}\".format(column,rto.feature_importances_[i]))\n    feature.append(column)\n    importance.append(rto.feature_importances_[i])","fdcf0f7b":"plt.figure(figsize=(8,8))\nsns.barplot(x=importance,y=feature)\nplt.show()","9d307261":"new_test_data","63db8d36":"new_test_data['Embarked'] = lb1.fit_transform(new_test_data['Embarked'])\nnew_test_data","00c12c19":"x_data.dtypes","c7756987":"x_data1 = x_data.drop('Fare_binned',axis= 1,inplace= False)\nx_data1","4c30d473":"x_data1.drop('Parch',axis=1,inplace=True)\nx_data1","df249afa":"new_test_data1 = new_test_data.drop('Fare_binned',axis= 1,inplace= False)\nnew_test_data1","645c37a1":"new_test_data1.drop('Parch',axis=1,inplace=True)\nnew_test_data1","85f36e17":"model_names = {\n    'svm' : {\n     'model': SVC(gamma='auto'),\n     'params': {\n         'C': [1,5,10,26,28,29,30],\n         'kernel' : ['rbf','linear']\n     }\n    },\n    'logistic_regression': {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params' : {\n            'C': [1,5,10,26,28,29,30],\n        }\n    },\n    'random_forest' : {\n        'model' : RandomForestClassifier(),\n        'params' : {\n            'n_estimators' : [1,5,10,26,28,29,30]\n        }\n    },\n    'xgboost' : {\n        'model' : xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5),\n        'params' : {\n            'alpha' : [0.1,1,10,100,1.1],\n            'n_estimators' : [1,5,10,26,28,29,30]\n        }\n        \n    },\n    'bagging_classifier' : {\n        'model' : BaggingClassifier(DecisionTreeClassifier(), max_samples= 0.5,max_features=1.0),\n        'params' : {\n            'n_estimators' : [1,5,10,26,28,29,30]\n        }\n    },\n    'adbboost_classifier' : {\n        'model' :  AdaBoostClassifier(DecisionTreeClassifier(),learning_rate=1),\n        'params' : {\n            'n_estimators' : [1,5,10,26,28,29,30]\n        }\n    }\n}","54819d1e":"from sklearn.model_selection import GridSearchCV","0b035ea3":"scores = []\nfor model_name, mp in model_names.items():\n    clf = GridSearchCV(mp['model'],mp['params'],cv=5,return_train_score=False)\n    clf.fit(x_data1,y_data.values.ravel())\n    scores.append({\n        'model' : model_name,\n        'best_score' : clf.best_score_,\n        'best_params' : clf.best_params_\n    })","a2e57fc8":"d=pd.DataFrame(scores,columns=['model','best_score','best_params'])","5edb8a89":"d","915db6f7":"px.bar(data_frame=d,x='model',y='best_score')","55401900":"estimator3 = [] \nestimator3.append(('svm', SVC(gamma='auto',kernel='linear',C=1)))\nestimator3.append(('lg', LogisticRegression(solver='liblinear',multi_class='auto',C=1)))\nestimator3.append(('rfc', RandomForestClassifier(n_estimators=28))) \nestimator3.append(('xgb', xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha=1.1,n_estimators=28))) \nestimator3.append(('bg', BaggingClassifier(DecisionTreeClassifier(), max_samples= 0.5,max_features=1.0,n_estimators=28))) \nestimator3.append(('ada2', AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=5,learning_rate=1))) \n\nvotin = VotingClassifier(estimators=estimator3,voting='hard')\nvotin.fit(x_data1,y_data.values.ravel())","644c4491":"predic = votin.predict(new_test_data1)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predic})\noutput.to_csv('my_submission22.csv', index=False)\nprint(\"Your submission was successfully saved!\")","c2ef70b4":"##### FEATURE Pclass","8e1d1b7b":"Handling Missing Values of Embarked Feature by substituting the frequent occurring value \"S\".","4c9d4e48":"##### FEATURE SibSp","6ac1b848":"##### FEATURE Embarked","60cd68d9":"### 4. LOADING DATASET","e10e0002":"##### FEATURE SEX","664cb8bc":"#### 5.1 IDENTIFY AND HANDLE MISSING VALUES","df79c369":"   ##### Dummies values for Embarked\n   Truning the Categorical Variable into Quantitative Variable in python","fe0d60f5":"### 3. IMPORTING lIBRARIES \n\nThis are the libraries used for preprocessing of data, for analysis of data and building models for prediction.","88cb9e6e":"As you can see the above \"Age\" Feature where there are 177 null values. So we need to handle the missing values.","8a1cc072":"##### FEATURE PARCH ","d9bc94a9":"**Binning on Age Feature and Fare feature**","c809fd73":"### 6 EXPLORATORY DATA ANALYSIS\n   EDA tells us the characteristics that influence the target variable","9b6b5857":"Now We know that Cabin and Embarked have missing values. we will now deal with it.","e21f9667":"Mean Encoding feature","d3c9c4c5":"### 2. DATA UNDERSTANDING \n\n\nThe dataset is divided into training data and testing data. The source of Data is Kaggle <a herf=\"https:\/\/www.kaggle.com\/c\/titanic\/data\">Titanic Machine Learning From Disaster<\/a>\n\n\nHere are Features of Titanic Machine Learning from Disaster dataset\n1. PassengerId - Unique Id assign to each passenger.\n2. Survived - Tell us whether the passenger survived or not 0 = no and 1 = yes survived.\n3. Pclass - Tells us to which class of ticket, the passenger belongs eg 1 = 1st class, 2 = 2nd class, 3 = 3rd class.\n4. Name - Tells us the name of passengers.\n5. Sex - Tells us the sex of passsengers.\n6. Age - Tells us the Age og passengers.\n7. SibSp - of siblings \/ spouses aboard the Titanic.\n8. Parch - of parents \/ children aboard the Titanic.\n9. Ticket - Ticket number.\n10. Fare - Passenger Fare.\n11. Cabin - Cabin no.\n12. Embarked - Port of Embarkation where C = Cherbourg, Q = Queenstwon, S = Soythamptom.","5bb32b2b":"##### FEATURE CABIN ","968d88a4":"##### FEATURE AGE ","3f59ebf4":"##### FEATURE Passenger","a7afae70":"##### 5.2 Data Formating \n","5dd1d157":"Since there are lot of Missing Values in Cabin Attributes so we are dropping it since it has lot of unique values and there is no numerical value on the basis of which we can fill the missing values of Cabin Attribute","1a179ef0":"##### FEATURE Survived \n","19f0e29b":"### 5. PREPROCESSING OF DATA \n\nThe process of converting or mapping of data from the initail raw from into another format, in order to prepare the data for further analysis. This is also known as Data cleaning or Data Wrangling.\n1. Identify and handle Missing Values.\n2. Data Formating .\n3. Data Normalization(Scaling of Data).\n4. Truning Categorical Variables into Quantitative variable.","5342dc7f":"### 1 . UNDERSTANDING THE PROBLEM\nThe dataset is about titanic ship. it is a well know passenger ship that sink into water. In this problem we have to predict the survival of passengers. This is the basic understanding of the problem.","bc746fd2":"### 7. MODELING ","aa82944d":"<img src='https:\/\/media.nationalgeographic.org\/assets\/photos\/000\/273\/27302.jpg' width=500px height=200px\/>"}}