{"cell_type":{"a3d6df42":"code","2f5bf76b":"code","f8f82e3e":"code","f7855a53":"code","9d1cb0c0":"code","720410db":"code","0d928227":"code","9099a750":"code","4d606970":"code","6ebdcc27":"code","689e8bc7":"code","ae9bb5ae":"code","b6e79bed":"code","6bed984e":"code","7f8c1e2c":"code","f09bb2b7":"code","979acd00":"code","45de0385":"code","41b606c3":"code","a8ea0ede":"code","0bd00f00":"code","169b2b01":"code","9177d1f5":"code","b4b6a489":"code","4a6715a9":"code","485f9362":"code","863e7b0b":"code","57512fdf":"code","3d926472":"code","466775da":"code","7d0d3d09":"code","334eaded":"code","de08b55f":"code","bd0a3f64":"code","0629484c":"code","18810e82":"code","eea2408b":"code","e45a31a2":"code","89989b6d":"markdown","40b5f6cb":"markdown","175cfe04":"markdown","2e1ab583":"markdown","42b65040":"markdown","323dcd9b":"markdown","14ed757f":"markdown","6fd0c9ea":"markdown","fe55d546":"markdown","6bf4b72e":"markdown","c5fd03e8":"markdown","a0ca30e8":"markdown","719bb8d2":"markdown","eef87d16":"markdown","a0b4065c":"markdown","15aeb368":"markdown","c3611303":"markdown","4b30196b":"markdown","43ece941":"markdown","0f247a36":"markdown","a715671d":"markdown","1ca3378d":"markdown","4a00d5ef":"markdown","63f42979":"markdown","1ab9ef56":"markdown","3188f2ed":"markdown","6cff58d0":"markdown","e2193d43":"markdown","99178487":"markdown","c12e91ea":"markdown"},"source":{"a3d6df42":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import RepeatedKFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import BayesianRidge, Lasso, LinearRegression, Ridge, RidgeCV\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR","2f5bf76b":"df = pd.read_csv(\"..\/input\/flight-take-off-data-jfk-airport\/M1_final.csv\")\ndf.head()","f8f82e3e":"df.info()","f7855a53":"df.isna().sum()","9d1cb0c0":"df.select_dtypes([\"object_\"]).head()","720410db":"df[\"Dew Point\"] = df[\"Dew Point\"].astype(\"int64\")\n\n# Let's check if this fixed it.\ndf.select_dtypes([\"object_\"]).head()","0d928227":"df[df[\"Wind\"].isna()]","9099a750":"df.dropna(inplace=True)","4d606970":"df.isna().sum()[\"Wind\"]","6ebdcc27":"df_le = df.copy() # will be used for label encoding\ndf_ohe = df.copy() # will be used for one hot encoding","689e8bc7":"le = LabelEncoder()\n\nfor x in df_le.select_dtypes([\"object_\"]).columns:\n    df_le[x] = le.fit_transform(df_le[x]).astype(\"str\")","ae9bb5ae":"df_le.select_dtypes([\"object_\"]).head()","b6e79bed":"X = df_le.drop(\"TAXI_OUT\", axis=1)\ny = df_le[\"TAXI_OUT\"]","6bed984e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","7f8c1e2c":"def rmse(y_true, y_pred):\n    \"\"\"Helper function for calculating root mean squared error.\"\"\"\n    return round(mean_squared_error(y_true, y_pred) ** 0.5, 2)","f09bb2b7":"# Creating ndarray for storing rmse values for models with Label Encoding\nrmse_le = np.zeros(7, dtype=\"float64\")","979acd00":"reg1 = LinearRegression().fit(X_train, y_train)\ny_pred1 = reg1.predict(X_test)\nrmse_le[0] = rmse(y_test, y_pred1)\nrmse_le[0]","45de0385":"cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\nreg = RidgeCV(alphas=np.arange(0, 1, 0.01), cv=cv, scoring='neg_mean_absolute_error')\nreg.fit(X_train, y_train)\nprint(f\"alpha: {reg.alpha_}\")","41b606c3":"reg2 = Ridge(alpha=0.0, normalize=True).fit(X_train, y_train)\ny_pred2 = reg2.predict(X_test)\nrmse_le[1] = rmse(y_test, y_pred2)\nrmse_le[1]","a8ea0ede":"reg3 = Lasso().fit(X_train, y_train)\ny_pred3 = reg3.predict(X_test)\nrmse_le[2] = rmse(y_test, y_pred3)\nrmse_le[2]","0bd00f00":"err_rate = []\nfor i in range(1, 50):\n    knn = KNeighborsClassifier(n_neighbors=i).fit(X_train,y_train)\n    y_pred = knn.predict(X_test)\n    err_rate.append(np.mean(y_pred != y_test))\nk_index = err_rate.index(min(err_rate))\nmin_err = min(err_rate)\nprint(f\"Minimum error of {min_err} at K = {k_index}.\")","169b2b01":"reg4 = KNeighborsRegressor(n_neighbors=31).fit(X_train, y_train)\ny_pred4 = reg4.predict(X_test)\nrmse_le[3] = rmse(y_test, y_pred4)\nrmse_le[3]","9177d1f5":"reg5 = SVR().fit(X_train, y_train)\ny_pred5 = reg5.predict(X_test)\nrmse_le[4] = rmse(y_test, y_pred5)\nrmse_le[4]","b4b6a489":"reg6 = BayesianRidge().fit(X_train, y_train)\ny_pred6 = reg6.predict(X_test)\nrmse_le[5] = rmse(y_test, y_pred6)\nrmse_le[5]","4a6715a9":"reg7 = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_train, y_train)\ny_pred7 = reg7.predict(X_test)\nrmse_le[6] = rmse(y_test, y_pred4)\nrmse_le[6]","485f9362":"# Label Encoding some of the columns because they mess up the model\nle = LabelEncoder()\n\ndf_ohe[\"TAIL_NUM\"] = le.fit_transform(df_ohe[\"TAIL_NUM\"]).astype(str)\ndf_ohe[\"Wind\"] = le.fit_transform(df_ohe[\"Wind\"]).astype(str)\ndf_ohe[\"Condition\"] = le.fit_transform(df_ohe[\"Condition\"]).astype(str)\n\ndf_ohe = pd.get_dummies(df_ohe, columns=[\"MONTH\", \"DAY_OF_WEEK\", \"OP_UNIQUE_CARRIER\", \"DEST\"])","863e7b0b":"X = df_ohe.drop(\"TAXI_OUT\", axis=1)\ny = df_ohe[\"TAXI_OUT\"]","57512fdf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","3d926472":"# Creating ndarray for storing rmse values for models with One Hot Encoding\nrmse_ohe = np.zeros(7, dtype=\"float64\")","466775da":"reg1 = LinearRegression().fit(X_train, y_train)\ny_pred1 = reg1.predict(X_test)\nrmse_ohe[0] = rmse(y_test, y_pred1)\nrmse_ohe[0]","7d0d3d09":"reg2 = Ridge(alpha=0.0, normalize=True).fit(X_train, y_train)\ny_pred2 = reg2.predict(X_test)\nrmse_ohe[1] = rmse(y_test, y_pred2)\nrmse_ohe[1]","334eaded":"reg3 = Lasso().fit(X_train, y_train)\ny_pred3 = reg3.predict(X_test)\nrmse_ohe[2] = rmse(y_test, y_pred3)\nrmse_ohe[2]","de08b55f":"err_rate = []\nfor i in range(1, 50):\n    knn = KNeighborsClassifier(n_neighbors=i).fit(X_train,y_train)\n    y_pred = knn.predict(X_test)\n    err_rate.append(np.mean(y_pred != y_test))\nk_index = err_rate.index(min(err_rate))\nmin_err = min(err_rate)\nprint(f\"Minimum error of {min_err} at K = {k_index}.\")","bd0a3f64":"reg4 = KNeighborsRegressor(n_neighbors=36).fit(X_train, y_train)\ny_pred4 = reg4.predict(X_test)\nrmse_ohe[3] = rmse(y_test, y_pred4)\nrmse_ohe[3]","0629484c":"reg5 = SVR().fit(X_train, y_train)\ny_pred5 = reg5.predict(X_test)\nrmse_ohe[4] = rmse(y_test, y_pred5)\nrmse_ohe[4]","18810e82":"reg6 = BayesianRidge().fit(X_train, y_train)\ny_pred6 = reg6.predict(X_test)\nrmse_ohe[5] = rmse(y_test, y_pred6)\nrmse_ohe[5]","eea2408b":"reg7 = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_train, y_train)\ny_pred7 = reg7.predict(X_test)\nrmse_ohe[6] = rmse(y_test, y_pred4)\nrmse_ohe[6]","e45a31a2":"models = [\n          \"Linear Regression\", \"Ridge Regression\", \"Lasso Regression\",\n          \"KNN Model\", \"Support Vector Regression\", \"Naive Bayes\",\n          \"Randrom Forest Regressor\"\n         ]\n\nplt.plot(models, rmse_le, label=\"Label Encoding\")\nplt.plot(models, rmse_ohe, label=\"One Hot Encoding\")\n\nplt.xlabel(\"Model Name\")\nplt.xticks(rotation = -90)\nplt.ylabel(\"RMSE\")\n\nplt.legend()\nplt.show()","89989b6d":"## Linear Regression","40b5f6cb":"## Splitting the data in training and testing set\n\n\n- Training data set is used for fitting our model to learn the patterns.\n- Testing data set is used for prediction and unbiased evaluation of our final model\n\nWe can do this by using [sklearn.model_selection.train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html).\n\nTraining data set - 90% of the total data\n\nTesting data set - 10% of the total data","175cfe04":"## Lasso Regression","2e1ab583":"# Loading the data","42b65040":"# Exploring the data\n\nKnowledge about the data you are working on is very important for data analysis.\n\nWhat I have used:\n\n- [pandas.DataFrame.info](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.info.html)\n\n- [pandas.DataFrame.isna](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.isna.html)\n\n- [pandas.DataFrame.select_dtypes](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.select_dtypes.html)","323dcd9b":"## Linear Regression","14ed757f":"## KNN Model","6fd0c9ea":"# Building Models for Label Encoded Data","fe55d546":"## KNN Model","6bf4b72e":"## Creating features (X) and label (y)\n\nFeatures are often referred to as \"independent variables\" and Label is often referred to as \"dependent variable\".\n\nHere `TAXI_OUT` is our label because it depends on other features.","c5fd03e8":"## Splitting the data in training and testing set","a0ca30e8":"## Naive Bayes","719bb8d2":"## Ridge Regression","eef87d16":"# Building Models for One Hot Encoded Data","a0b4065c":"## Naive Bayes","15aeb368":"## Lasso Regression","c3611303":"## Creating features (X) and label (y)","4b30196b":"## Random Forest Regressor","43ece941":"# Label Encoding the categorical columns\n\nMost of the machine learning models don't like text. What do they like? Numbers. **Label Encoding** is an important part of **feature engineering** which is used to convert categorical data into numerical form so that they can be provided to our machine learning model.\n\nExample: If we have two colors red and green\n\n```\n+-------+\n| color |\n+-------+\n|  red  |\n+-------+\n| green |\n+-------+\n```\n\nAfter applying Label Encoding this would be converted into something like:\n\n```\n+-------+\n| color |\n+-------+\n|   0   |\n+-------+\n|   1   |\n+-------+\n```\n\nWe can do this by using [sklearn.preprocessing.LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html)\n\nExtra Resource: https:\/\/www.geeksforgeeks.org\/ml-label-encoding-of-datasets-in-python\/","0f247a36":"# Importing the library","a715671d":"# Creating copies of our DataFrame","1ca3378d":"## Ridge Regression","4a00d5ef":"## Support Vector Regressor (SVR)","63f42979":"I couldn't get LightGBM to work","1ab9ef56":"## Random Forest Regressor","3188f2ed":"# Dropping null values\n\nPreviously when we were exploring the data, we saw that `Wind` column had some missing values. As our data set is large and the number of missing values are very very small as compared to that of our data set, we can safely drop the rows!","6cff58d0":"# Conclusion","e2193d43":"# One Hot Encoding the categorical columns\n\n**One Hot Encoding** is an important part of __feature engineering__ which is used to convert categorial data into numerical form so that they can be provided to our machine learning model.\n\nExample: If we have two colors red and green and we want to represent red we could do something like\n\n```\n+---+-----+\n|red|green|\n+---------+\n| 1 |  0  |\n+---+-----+\n```\n\nThese are often referred to as \"dummy variables\".\n\nWe can do this by using [pandas.get_dummies](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.get_dummies.html)\n\nAlternate: [sklearn.preprocessing.OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html)\n\nExtra Resource: https:\/\/www.educative.io\/blog\/one-hot-encoding","99178487":"Hmm... do you notice something weird? `Dew Point` should have been a dtype of numeric but it has a dtype of object! Let's fix this. ","c12e91ea":"## Support Vector Regressor (SVR)"}}