{"cell_type":{"2ba370d7":"code","2637ea86":"code","ac679d30":"code","f6a091cd":"code","b552f0dd":"code","ea041db9":"code","bb7af60d":"code","8be3954e":"code","4aeebd55":"code","8dab638e":"code","d7f56783":"code","182d7306":"code","bfc7f540":"code","1ceaaff7":"markdown","7d8ba855":"markdown","2ed1ec5d":"markdown","811b5af2":"markdown","70148c12":"markdown","f2891efd":"markdown","3c7015e6":"markdown","9840a068":"markdown","117d1087":"markdown"},"source":{"2ba370d7":"import math\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport re\nimport unidecode\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom imblearn.under_sampling import RandomUnderSampler\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, GRU, Embedding, LSTM, Bidirectional\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau","2637ea86":"voc_size = 50000   # number of words in vocabulary to make embedding\nmax_sequence_length = 300 # take maximum 300 characters in the input sequence\nembedding_dim = 128 # create the word embeddings in this dimension. Any text input willl be mapped into vector of lendth 128\n\nEPOCHS = 70 \nBATCH_SIZE = 2048 \nLEARNING_RATE = 0.001\nFOLDS = 4 \nVERBOSE = 0","ac679d30":"# Cleaning the text  from unnecessary characters\ndef clean_data(data):\n    final = []\n    for sent in data:\n        sent = sent.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n        soup = BeautifulSoup(sent, \"html.parser\")\n        sent = soup.get_text(separator=\" \")\n        remove_https = re.sub(r'http\\S+', '', sent)\n        sent = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n        sent = unidecode.unidecode(sent)\n        sent = sent.lower()\n        sent = re.sub(r\"[^a-zA-Z0-9:$-,()%.?!]+\", ' ', sent) \n        sent = re.sub(r\"[:$-,()%.?!]+\", ' ',sent)\n        stoplist = stopwords.words(\"english\")\n        sent = [word for word in word_tokenize(sent) if word not in stoplist]\n        sent = \" \".join(sent)\n        final.append(sent)\n    return final\n\n# Return the probability from output number\ndef sigmoid(x):\n    return 1 \/ (1 + math.exp(-x))\n\n# Set seed to get the same output everytime\ndef seed_everything(SEED=13):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    tf.random.set_seed(SEED)\n    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n\nseed_everything()","f6a091cd":"train_prev_comp = \"..\/input\/toxic-comment\/jigsaw-toxic-comment-train.csv\"\ntest_cur_comp = \"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\"","b552f0dd":"# Reading train file from previous competition\ndf = pd.read_csv(train_prev_comp)\n\ndf[\"y\"] = (df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].sum(axis=1) > 0).astype(int)\ndf.drop([\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"], axis=1, inplace = True)\ndf.head()","ea041db9":"df[\"y\"].value_counts()","bb7af60d":"X = np.array(df[\"comment_text\"].values)\nX = X.reshape(-1,1)\ny = np.array(df[\"y\"].values)\nrus = RandomUnderSampler(random_state=0)\ntrain, target = rus.fit_resample(X, y)\n\ntrain = train.flatten()\ndf = pd.DataFrame()\ndf[\"text\"] = train\ndf[\"target\"] = target\n\n# Now its balanced\ndf[\"target\"].value_counts()","8be3954e":"# Cleaning the data\ndf[\"text\"] = clean_data(df[\"text\"])","4aeebd55":"from tensorflow.keras import backend as K\ndef ALReLU(x):\n   alpha = 0.01\n   return K.maximum(K.abs(alpha*x), x)\n\nfrom tensorflow.keras.utils import get_custom_objects\nget_custom_objects().update({'ALReLU':  tf.keras.layers.Activation(ALReLU)})","8dab638e":"# Defining sequential model with LSTM units. \ndef lstm_model():\n    model = Sequential([\n        Embedding(voc_size, embedding_dim, input_length = max_sequence_length),\n        LSTM(256, return_sequences=True),\n        Dropout(0.1),\n        LSTM(128, return_sequences=True),\n        Dropout(0.1),    \n        LSTM(64, return_sequences = True),\n        Dropout(0.1),\n        LSTM(32, return_sequences = False),\n        Dropout(0.2),\n        Dense(16, activation=ALReLU),\n        Dropout(0.3),\n        Dense(1, activation=\"sigmoid\")\n    ])\n    return model","d7f56783":"preds_valid_f = {}\npreds_test = []\ntotal_auc = []\nf_scores = []\ncounter = 0\n\nkf = StratifiedKFold(n_splits=FOLDS,random_state=0,shuffle=True)\n\ntrain = df[\"text\"]\ntarget = df[\"target\"]\n\nfor fold,(train_index, valid_index) in enumerate(kf.split(train, target)):\n\n    counter +=1\n    \n    X_train,X_valid = train.loc[train_index], train.loc[valid_index]\n    y_train,y_valid = target.loc[train_index], target.loc[valid_index]\n  \n    # Preprocessing\n    tokenizer = Tokenizer(num_words = voc_size)\n    tokenizer.fit_on_texts(X_train.values)\n    X_train = tokenizer.texts_to_sequences(X_train.values)\n    X_train = pad_sequences(X_train, maxlen = max_sequence_length)\n \n    X_valid = tokenizer.texts_to_sequences(X_valid.values)\n    X_valid = pad_sequences(X_valid, maxlen = max_sequence_length)\n    \n    model = lstm_model()\n    model.compile(\n        optimizer= Adam(learning_rate=LEARNING_RATE),\n        loss='binary_crossentropy',\n        metrics=['AUC'],\n    )\n    \n    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, \n                           patience=5, verbose=VERBOSE, mode='min')\n    \n    # checkpoint save the model witg best validation accuracy\n    chk_point = ModelCheckpoint(f'.\/Model_{counter}C.h5', \n                                monitor='val_loss', verbose=VERBOSE, \n                                save_best_only=True, mode='min')\n    es = EarlyStopping(\n        patience=10,\n        min_delta=0,\n        monitor='val_loss',\n        restore_best_weights=True,\n        verbose=0,\n        mode='min', \n        baseline=None,\n    )\n\n    history = model.fit(  X_train, y_train,\n                validation_data = (X_valid, y_valid),\n                batch_size = BATCH_SIZE, \n                epochs = EPOCHS,\n                callbacks = [es, lr, chk_point],\n                shuffle = True,\n                verbose = 1\n              )\n    \n    model = load_model(f'.\/Model_{counter}C.h5')\n\n    #  model predicts wheither test dataset is toxic or not.\n    # Probability is then multiplied by 100 and we get score\n    test = pd.read_csv(test_cur_comp)\n    test[\"text\"] = clean_data(test[\"text\"])\n    x_test = tokenizer.texts_to_sequences(test[\"text\"].values)\n    x_test = pad_sequences(x_test, maxlen = max_sequence_length)\n    pred = model.predict(x_test)\n    pred = [sigmoid(x) * 100 for x in pred]\n    \n    preds_test.append(pred)","182d7306":"final_pred = np.mean(preds_test, axis = 0)","bfc7f540":"# Making submission file\nfinal = pd.DataFrame()\nfinal[\"comment_id\"] = test[\"comment_id\"]\nfinal[\"score\"] = pred\nfinal.to_csv(\"submission.csv\", index=False)","1ceaaff7":"### Hyperparameters and Helper Functions\nAll hyperparameters need to be tuned in order to achieve the best result from the mode.","7d8ba855":"### Submission","2ed1ec5d":"\nThis notebook is just a fork of https:\/\/www.kaggle.com\/sabinaabdurakhmanova\/jigsaw-comment-toxicity-lstm-starter by using custom activation function ALReLU ( https:\/\/arxiv.org\/abs\/2012.07564 ) instead of ReLU\n\n**score 0.785 (LB)**\n\n\n### Introduction\nHere is simple approach to rate severity of toxic comments. Find probability of the comment to be toxic one and give the score based on this probability. \n\nNotebook is build upon this public kernel: https:\/\/www.kaggle.com\/devkhant24\/jigsaw-comment-toxicity-bidirectional-gru. I only changed the model and added some comments","811b5af2":"Dataset is imbalanced as there are more 0 values than 1. Training model on such data will give biased result. To solve it, undersampling is used. It randomly drops some data with zero values and makes values to be equal","70148c12":"Model is trained for several folds that helps to use data efficiently. Every time 3\/4th dataset will be used as training data and remaining as validation. 4 models will be trained and their predictions averaged","f2891efd":"Then combine all 6 categories into one. So now dataset just shows if the comment is toxic or not. Therefore we simply have binary classification problem","3c7015e6":"### Preprocessing\nTake dataset from toxic comment classification challenge that classifies comments into 1 of 7 categories (toxic, severe_toxic, obscene, threat, insult, identity_hate). Seventh category is when all of them are zeros, means the comment is not toxic","9840a068":"### Model and Training\nWe define LSTM network that has feedback connections and therefore can find relationship of samples in the sequential input. It is better than GRU as it remembers more information, however takes more time to be trained","117d1087":"Note that such approach is very naive as it treats any toxic class equally while identity_hate for example should get larger score than obscene and severe_toxic comment is worse than toxic. It is better to classify the comment and assign different score for each class."}}