{"cell_type":{"b14ec4c0":"code","3e9072d7":"code","7cc1d861":"code","ea51d9ed":"code","91cffc15":"code","e9d6362a":"code","4ddd4d08":"code","d1c06bed":"code","bb33257b":"code","c3a8a019":"code","9e937a65":"code","bcf3c36e":"code","7919a553":"code","b0353875":"code","98dc8be5":"code","6bd0ada5":"code","64bfd0a0":"code","ce2b1534":"code","f645fe2e":"code","2595f21c":"code","a28d9db3":"code","bafd8c0f":"code","ad0977ce":"code","81e0bc9a":"code","092130df":"code","37471b8f":"code","f2407378":"markdown","04fac949":"markdown","1ad020b8":"markdown","ca196dbd":"markdown","9da80cf3":"markdown","6746a71a":"markdown","a6028fb3":"markdown","a7bd58f1":"markdown","12656053":"markdown","90da65db":"markdown","327d7f75":"markdown","ffbbb562":"markdown","7cf27764":"markdown","6d0ed898":"markdown","d753519b":"markdown","d0afe94b":"markdown"},"source":{"b14ec4c0":"import numpy as np\nimport pandas as pd\n\n# for data viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\n\n# for the regression analysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        data = os.path.join(dirname, filename)\n\nplt.style.use('ggplot')\n%matplotlib inline","3e9072d7":"# filtering warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# set to display all rows\npd.set_option('display.max_rows', None)\n\n# size configuration of the plots\nplt.rcParams['figure.figsize'] = (12, 8)","7cc1d861":"df = pd.read_csv(data)\ndf.head()","ea51d9ed":"df.info()","91cffc15":"# checking what percentage of the dataset is represented by nulls\n\nfor col in df.columns:\n    pct_missing = np.mean(df[col].isnull())\n    print('{} - {}%'.format(col, pct_missing))","e9d6362a":"# drop NAs, considering that they represent a very small percentage of our dataset\n\ndf.dropna(inplace=True)","4ddd4d08":"# a piece of the dataframe, to illustrate:\n\ndf[df.name.duplicated(keep=False)].sort_values(by='name',ascending=True).head(10)","d1c06bed":"# adjusting data types and transforming columns\n\ndf.budget = df.budget.astype(int)\ndf.gross = df.gross.astype(int)\ndf.votes = df.votes.astype(int)\n\ndf['released_date'] = df.released.str.split('(', expand=True)[0]\ndf['released_date'] = pd.to_datetime(df.released_date)\ndf['year'] = df.released_date.dt.year\ndf['released_country'] = df.released.str.split('(', expand=True)[1].str.replace(r'[^a-zA-Z\\d\\s:]', '')\ndf = df.drop('released', axis=1)\n\ndf['rating'] = df.rating.str.replace('Not Rated', 'Unrated')","bb33257b":"#1 \n\ntop5 = df.sort_values(by='gross', ascending=False).head(5)\nsns.barplot(x='name', y='gross', data=top5, palette='viridis');\nplt.xticks(rotation=45);\nplt.xlabel('');\nplt.ylabel('Revenue');\nplt.title('Top 5 Movies by Revenue', loc='left', fontsize=18, pad=20);","c3a8a019":"#2 \n\nstar = df.star.value_counts().head(10)\ndirec = df.director.value_counts().head(5)\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1)\nstar.plot(kind='bar', ax=ax1, color='b', title='Top 10 Actors by Number of Movies');\ndirec.plot(kind='bar', ax=ax2, color='darkcyan', title='Top 5 Directors by Number of Movies');","9e937a65":"#3\n\ndirec_rev = df.groupby('director').sum().sort_values(by='budget', ascending=False).head(5)\nsns.barplot(x=direc_rev.index, y='gross', data=direc_rev, palette='viridis_r');\nplt.xticks(rotation=45);\nplt.xlabel('');\nplt.ylabel('Revenue');\nplt.title('Top 5 Directors by Revenue', loc='left', fontsize=18, pad=20);","bcf3c36e":"#4 \n\nrelease_year = df.groupby('year').count()\nsns.lineplot(x=release_year.index, y='name', data=release_year, linewidth=4);\nplt.xticks(rotation=45);\nplt.xlabel('');\nplt.ylabel('Number of Movies');\nplt.title('Numbers of Movies Released by Year (1980-2020)', loc='left', fontsize=18, pad=20);","7919a553":"year_rev = df.groupby('year').sum()['gross']\nyear_rev_pct = year_rev.pct_change()\n\nrev_df = pd.DataFrame({\n    'Yearly Revenue': year_rev, \n    'Yearly Revenue (%)': year_rev_pct * 100\n    })\n\nrev_df.iloc[-10:]","b0353875":"#5\nbest_movies = df.sort_values(by='score', ascending=False).head(5)\ng = sns.barplot(x='name', y='score', data=best_movies, palette='icefire');\nax = g\nplt.xticks(rotation=45);\nplt.xlabel('');\nplt.title('Top 5 Best Movies by Score', loc='left', fontsize=18, pad=20);\n\n# displaying the scores:\n\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 20),\n                textcoords='offset points')\n_ = g.set_ylim(0,10) ","98dc8be5":"plt.scatter(x=df.budget, y=df.gross, color='b', alpha=.7);\nplt.title('Budget vs. Gross Revenue', loc='left', fontsize=18, pad=20);\nplt.xlabel('Budget');\nplt.ylabel('Gross Revenue');","6bd0ada5":"# regplot for a linear reg line\n\nsns.regplot(data=df, x='budget', y='gross', color='b', line_kws={'color':'r'});\nplt.title('Budget vs. Gross Revenue Correlation', loc='left', fontsize=18, pad=20);\nplt.xlabel('Budget');\nplt.ylabel('Gross Revenue');","64bfd0a0":"sns.heatmap(df.corr(method='pearson'), annot=True, cmap='Blues');\nplt.title('Correlation Matrix (numeric features)', loc='left', fontsize=18, pad=20);","ce2b1534":"df_num = df.copy()\n\nfor col in df_num.columns:\n    if (df_num[col].dtype == 'object'):\n        df_num[col] = df_num[col].astype('category')\n        df_num[col] = df_num[col].cat.codes\n\ndf_num.head()","f645fe2e":"sns.heatmap(df_num.corr(method='pearson'), annot=True, cmap='YlOrBr');\nplt.title('Correlation Matrix (all features)', loc='left', fontsize=18, pad=20);","2595f21c":"corr_mat = df_num.corr().unstack().sort_values(ascending=False)\ncorr_mat[((corr_mat) > .3) & ((corr_mat) != 1)]","a28d9db3":"# 1) The features I am going to use are those that have shown some correlation to the gross revenue, either negative or positive\n\nfeat = pd.DataFrame(df.corr().gross[(abs(df_num.corr().gross)) > .18].sort_values(ascending=False))\nfeat.iloc[1:] # excludes the 'gross', which is corr to itself","bafd8c0f":"# 2) Split the dataset\n\nX = df[['budget', 'votes', 'runtime', 'year', 'score']]\ny = df.gross\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)","ad0977ce":"# 3) Scale the independent variables\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","81e0bc9a":"# 4) Build the model and draw predictions\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\npredictions = linreg.predict(X_test)\n\nmae = mean_absolute_error(y_test, predictions)\nmse = mean_squared_error(y_test, predictions)\nrmse = np.sqrt(mse)\nrsqrt = r2_score(y_test, predictions) * 100\n\nprint('MAE: {} \\nMSE: {} \\nRMSE: {} \\nR-squared: {}'.format(mae, mse, rmse, rsqrt))","092130df":"ax = sns.regplot(x=y_test, y=linreg.predict(X_test),\n                scatter_kws={'color': 'blue'}, line_kws={'color':'red'});\nax.set(xlabel='y test', ylabel='predicted y');\nax.set(title='Predicted vs. Actual Gross Revenue');","37471b8f":"coeff = pd.DataFrame(index=X.columns, columns=['Coefficients'], data=linreg.coef_)\ncoeff","f2407378":"## Thank you!\n#### I truly appreciate you reading this far. Feel free to check out some of my Kaggle projects [here](https:\/\/www.kaggle.com\/yanscosta\/code).","04fac949":"##### A brief description of what the following transformations aim to do:\n* adjust the data types of `budget`, `gross` and `votes` to integer\n* split the `released` column into `released_date` and `released_country`. Then change the `released_date` data type to a time series.\n* drop the `released` column. Insert the `released_date` year as the `year` column.\n* renamed the 'not rated' values in the `rating` column to simply 'unranked' (which is also a label), to reduce redundancy\n","1ad020b8":"##### Dealing with the categorical variables\nThe following simply transforms each variable of the 'object' type to 'category', so they can be used for correlation analysis.","ca196dbd":"# Import Libraries and Read in the Data","9da80cf3":"##### Checking for high correlation","6746a71a":"# Insights\n\nThrough this analysis, we can clearly see that variables like `company`, `director`, `star`, and `country` where a movie is released have little to no correlation with the actual revenue. Whereas `budget` and the `votes` a movie gets seems to have greater impact in its earnings.\n\nAnother interesting point is that the `year` of release tends to influentiate its `budget`. It is obvious that the movie industry has only gotten bigger year after year, so it is only natural that the average budget increases and, with that, the revenue. `Runtime` also shows some correlation with the `budget`, as longer films tend to cost higher to be produced.\n\nThat does not necessarily reflect the movie industry in its entirely, but surely allows for a deeper look into what tends to drive up revenues in the industry - and what does not.\n<br>\n---","a6028fb3":"#### With only these features, the model can already explain roughly **67%** of the variability of the data.","a7bd58f1":"##### Handling nulls values (an aesthetically pleasing approach)\nI found this slightly more presentable than the common .isna().sum(), even though it's a bit more verbose.","12656053":"It is clear that throughout the last decade, the revenue, in average, was notably stable, even with a few peaks (e.g. 2015, year in which we had **Star Wars VII, Jurassic World, Avengers: Age of Ultron, etc**). But in 2020, which cinemas and studios shut down, the industry's revenue fell by ~**89%**.","90da65db":"# Correlation Analysis\n\n##### The idea is to identify and investigate the features that are most highly correlated to **gross revenue**. ","327d7f75":"##### Dealing with duplicates\nIn this particular case, the most reasonable approach to check for duplicates would be through the `name` column (since duplicates would be expected for the rest of the columns). Nevertheless, after little investigation, we can see that the duplicates in this column are actually either **remakes** or simply **homonyms**. \nTherefore, *we will not exclude any*.","ffbbb562":"# Movie Correlation and Regression Analysis\n**August 15, 2021**\n\n---","7cf27764":"# Regression Analysis\n\nThe goal of this section is to build a model that, given certain features (independent variables), could **predict the gross revenue** of a movie (dependent variable). For that, I will:\n##### 1) Select the features to be used in the model\n##### 2) Split between training and test sets \n##### 3) Scale numerical variables\n##### 4) Evaluate the model and predict on test and sample data","6d0ed898":"# EDA\n##### Before moving further into the actual analysis, let's explore the dataset through some visualizations.\n* 1. What are the top 5 movies by gross revenue? \n* 2. What are the stars that made the most movies in this period of time? And the directors? \n* 3. What are the directors that have generated the most revenue? \n* 4. What is the volume of movies coming out per year?\n* 5. What are the best movies by score? ","d753519b":"##### A quick side note:\nIn 2020, we obviously had the surge of the Covid-19 pandemic, which clearly impacted the movie industry, as the previous graph denotes. Let's briefly see its impact on revenue:","d0afe94b":"# Data Cleaning and Transformations"}}