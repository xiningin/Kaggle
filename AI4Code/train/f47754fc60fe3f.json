{"cell_type":{"8d23bc3c":"code","812d70d5":"code","fc4ec922":"code","5a3f360d":"code","15990017":"code","4f59db9e":"code","67d059a4":"code","01137c0f":"code","ab5629ae":"code","e593bbfd":"code","5818118e":"code","8e220f94":"code","9a1431b6":"code","3388bb11":"code","b8a9f7c6":"code","aaf60365":"code","c82f6fa3":"code","d8bc7961":"code","9faa61ed":"code","8b4234f7":"code","c19546ff":"code","9551c565":"code","488ac01f":"code","d1551a91":"code","ae69b39d":"code","43d11c55":"code","8d04ea94":"code","2a460dc3":"code","2e0c8174":"code","8a9a62fd":"code","e71fe3a5":"code","61b2f08c":"code","5f115758":"code","c5020f7d":"code","72163176":"code","119f0e88":"code","c649ac41":"code","47f1c724":"code","c209f347":"code","7de311c2":"code","0678892f":"code","87fed48b":"code","a4adfe36":"markdown","3304b62e":"markdown","56948275":"markdown","15b7fca3":"markdown","b287c301":"markdown","69a4e672":"markdown","d5d9e8a1":"markdown","4b15cbd8":"markdown","433485ad":"markdown","a7067d85":"markdown","49c62f8e":"markdown","c40e6277":"markdown"},"source":{"8d23bc3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.linear_model import ElasticNet, Lasso, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\npd.set_option(\"display.max_columns\", None)","812d70d5":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","fc4ec922":"train.head(10)","5a3f360d":"for col in train.columns:\n    print(col + \"   ** Unique value: \" + str(len(pd.unique(train[col]))))","15990017":"train.describe()","4f59db9e":"train.info()","67d059a4":"num_features = ['LotFrontage','LotArea','OverallQual','YearBuilt','YearRemodAdd','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','MoSold','YrSold']\ncat_features = ['MSSubClass','MSZoning','Street','Alley','LotShape', 'OverallCond','LandContour','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition']","01137c0f":"trainID = train['Id']\ntestID = test['Id']\n\ntrain.drop(\"Id\", axis = 1 , inplace = True)\ntest.drop(\"Id\", axis = 1 , inplace = True)\n\nprint(f\"Train and test data size: {train.shape}, {test.shape}\")","ab5629ae":"fig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(num_features, 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(num_features), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', palette='Blues', data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","e593bbfd":"sns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Plotting sales price distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt) ##QQ Plot\nplt.show()","5818118e":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution\nsns.distplot(train['SalePrice'], fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt) ##QQ plot\nplt.show()","8e220f94":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(f\"all_data size is : {all_data.shape}\")","9a1431b6":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(30)","3388bb11":"corr_matrix = train.corr()\n#plt.subplots(figsize=(12,9))\n#sns.heatmap(corrmat, vmax = 0.9, square = True)\ncorr_matrix['SalePrice'].sort_values(ascending = False)","b8a9f7c6":"### These features have a large % of missing values\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'MSSubClass', 'GarageCond', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'):\n    all_data[col] = all_data[col].fillna(0)\n\nall_data = all_data.drop(['Utilities'], axis=1) ### contains only 2 unique values\n\n# MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\n# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n","aaf60365":"skewed_features = all_data[num_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_features})\nskewness = skewness[abs(skewness) > 0.5]\n\nskewed_list = skewness.index\nlamb = 0.15\nfor feat in skewed_list:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lamb)","c82f6fa3":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)\n\n# Alternatively we can use Label Encoder - but One Hot encoding gives better result\n#for c in cat_features:\n#    lbl = LabelEncoder() \n#    lbl.fit(list(all_data[c].values)) \n#    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \n#print('Shape all_data: {}'.format(all_data.shape))","d8bc7961":"all_data.head(10)","9faa61ed":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","8b4234f7":"transformer = RobustScaler().fit(train)\n\ntrain_scaled = pd.DataFrame(transformer.transform(train), columns = train.columns)\n\ntest_scaled = pd.DataFrame(transformer.transform(test), columns = train.columns) ","c19546ff":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=10).get_n_splits(train_scaled)\n    rmse= np.sqrt(-cross_val_score(model, train_scaled, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","9551c565":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","488ac01f":"#lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nlasso = Lasso(alpha =0.005, random_state=10)\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso cross val score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d1551a91":"ENet = ElasticNet(alpha=0.0005, l1_ratio=0.01, random_state=10)\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet cross val score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ae69b39d":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge cross val score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","43d11c55":"GBoost = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.05,\n                                   max_depth=5, max_features='sqrt',\n                                   min_samples_leaf=20, min_samples_split=20, \n                                   loss='huber', random_state =10)\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting cross val score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8d04ea94":"rf = RandomForestRegressor(n_estimators=1000,\n                          max_depth=5,\n                          min_samples_split=20,\n                          min_samples_leaf=20,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=10)\nscore = rmsle_cv(rf)\nprint(\"RFR cross val score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","2a460dc3":"# Support Vector Regressor\nsvr = SVR(C= 10, epsilon= 0.008, gamma=0.0003)\nscore = rmsle_cv(svr)\nprint(\"SVR cross val score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","2e0c8174":"from sklearn.ensemble import StackingRegressor\nRf_base = RandomForestRegressor(n_estimators=1000,\n                          max_depth=5,\n                          min_samples_split=20,\n                          min_samples_leaf=20,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\nGBoost_base = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.05,\n                                   max_depth=5, max_features='sqrt',\n                                   min_samples_leaf=20, min_samples_split=20, \n                                   loss='huber', random_state =10)\n\nestimators = [\n     ('rf', Rf_base),\n     ('gb', GBoost_base)]\n\n\nStkRegressor = StackingRegressor(estimators= estimators, \n                           final_estimator=LinearRegression())\n\nscore = rmsle_cv(StkRegressor)\nprint(\"Stck score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","8a9a62fd":"StkRegressor.fit(train_scaled, y_train)\nstacked_train_pred = np.expm1(StkRegressor.predict(train_scaled))\nprint(rmsle(np.expm1(y_train), stacked_train_pred))","e71fe3a5":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.6, gamma=0.02, \n                             learning_rate=0.05, max_depth=7, \n                             min_child_weight=1, n_estimators=3000,\n                             reg_alpha=0.5, reg_lambda=0.5,subsample = 0.5,\n                             nthread = -1, seed = 10)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=14, max_depth = 7,\n                              learning_rate=0.05, n_estimators=3000,\n                              max_bin = 50, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.5,\n                              seed = 10, subsample=0.5,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 10)","61b2f08c":"model_xgb.fit(train_scaled, y_train)\nxgb_train_pred = np.expm1(model_xgb.predict(train_scaled))\n#xgb_pred = np.expm1(model_xgb.predict(test_scaled))\nprint(rmsle(np.expm1(y_train), xgb_train_pred))","5f115758":"model_lgb.fit(train_scaled, y_train)\nlgb_train_pred = np.expm1(model_lgb.predict(train_scaled))\n#lgb_pred = np.expm1(model_lgb.predict(test_scaled))\nprint(rmsle(np.expm1(y_train), lgb_train_pred))","c5020f7d":"X_train, X_val, Y_train, Y_val = train_test_split(train_scaled, y_train, test_size=0.25, random_state = 10)\n\nStkRegressor.fit(X_train, Y_train)\nstacked_val_pred = np.expm1(StkRegressor.predict(X_val))\nprint(\"Stck score: {:.4f} \\n\" .format(rmsle(np.expm1(Y_val), stacked_val_pred)))\n\n","72163176":"model_xgb.fit(X_train, Y_train)\nxgb_val_pred = np.expm1(model_xgb.predict(X_val))\nprint(rmsle(np.expm1(Y_val), xgb_val_pred))","119f0e88":"model_lgb.fit(X_train, Y_train)\nlgb_val_pred = np.expm1(model_lgb.predict(X_val))\nprint(rmsle(np.expm1(Y_val), lgb_val_pred))","c649ac41":"print(rmsle(np.expm1(Y_val), (xgb_val_pred*0+lgb_val_pred*0.2+stacked_val_pred*0.8)))","47f1c724":"feature_important = model_xgb.get_booster().get_score(importance_type='weight')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\ndata.nlargest(30, columns=\"score\").plot(kind='bar', figsize = (20,5))","c209f347":"feature_plot = lgb.plot_importance(model_lgb, max_num_features = 20, importance_type = \"gain\", figsize = (15,10))\n","7de311c2":"#print(rmsle(y_train,stacked_train_pred*0.33 +\n #              xgb_train_pred*0.33 + lgb_train_pred*0.35))","0678892f":"#ensemble = stacked_pred*0.33 + xgb_pred*0.33 + lgb_pred*0.35","87fed48b":"#sub = pd.DataFrame()\n#sub['Id'] = test_ID\n#sub['SalePrice'] = ensemble\n#sub.to_csv('submission.csv',index=False)","a4adfe36":"We apply log function to transform skewed SalesPrice distribution to approximately conform to normality. We will need to apply exponential function on outputs of regressors to get the actual predicted SalesPrice.\n\n**Further reading** - https:\/\/www.analyticsvidhya.com\/blog\/2016\/07\/deeper-regression-analysis-assumptions-plots-solutions\/?utm_source=blog&utm_medium=one-hot-encoding-vs-label-encoding-using-scikit-learn\n\n**Useful link on target transformation** - https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_transformed_target.html","3304b62e":"Feature scaling is performed during the data pre-processing to handle features with highly varying magnitudes or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.\n\n**Further reading:** \nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html\n\nhttps:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35","56948275":"Overview of machine learning steps using various regression techniques:\n- Data preparation: Missing value handling, Encoding, Feature scaling\n- Exploratory analysis\n- Regression type - Lasso, Ridge, Elastic Net, SVR, Random Forest, Gradient Boost, XGB, LGBM, Stacking & Blending\n- Cross-validation\n- Feature importance\n","15b7fca3":"**SalesPrice distriburtion shows a positive skewness**","b287c301":"**Data cleaning and Handling Missing Values**","69a4e672":"Several machine learning algorithms such as Regression\/ Naive Bayes prefer or perform better when numerical variables have a Gaussian probability distribution. Power transforms such as Box-Cox are a technique for transforming numerical input variables to have a Gaussian or more-Gaussian-like probability distribution. \n\n**Further reading** - https:\/\/machinelearningmastery.com\/power-transforms-with-scikit-learn\/","d5d9e8a1":"\nApply One-Hot Encoding when:\n\n - The categorical feature is not ordinal\n- The number of categorical features is less\n\nApply Label Encoding when:\n\n - The categorical feature is ordinal (Good, Neutral, Poor etc.)\n - The number of categories is quite large\nFurther reading: ****\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/one-hot-encoding-vs-label-encoding-using-scikit-learn\/","4b15cbd8":"Useful guide on Boosting parameter tuning - \n\nhttps:\/\/www.kaggle.com\/prashant111\/a-guide-on-xgboost-hyperparameters-tuning\n\nhttps:\/\/neptune.ai\/blog\/lightgbm-parameters-guide","433485ad":"**Let's plot target variable SalesPrice's v\/s numeric features**","a7067d85":"Combining LGBM and Stacking Regressor outputs (blending) slightly reduces the prediction error","49c62f8e":"**Correlation matrix to check which features have high correlation with SalePrice**","c40e6277":"**Combining train and test sets for data cleaning and handling missing values.**"}}