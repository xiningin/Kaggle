{"cell_type":{"e6c026bb":"code","5ef1f494":"code","f991bd72":"code","e0c9321d":"code","5eb81bda":"code","a24f7796":"code","8d2bc75a":"code","d6ef7062":"code","ba7ab76b":"code","1da4e191":"code","ad1d0ef2":"code","a0a816b0":"code","5ac717bc":"code","09ed460e":"code","c7a44019":"code","1f1b7781":"code","1e8b329e":"code","374d9940":"code","a51af4cc":"code","a2012db7":"code","efba521f":"code","cf8d6160":"code","13c7341e":"code","c48dbeec":"code","0f70ae76":"code","eff35655":"markdown","71238b20":"markdown","1401305e":"markdown","c8567428":"markdown","14841b1d":"markdown","12d6375e":"markdown","cd6543a2":"markdown","e41908e0":"markdown"},"source":{"e6c026bb":"import numpy as np\nimport pandas as pd\nimport gensim\nimport string\n\nfrom keras.callbacks import LambdaCallback\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Dense, Activation\nfrom keras.models import Sequential\nfrom keras.utils.data_utils import get_file\n\nimport nltk","5ef1f494":"data = pd.read_csv(\"..\/input\/Womens Clothing E-Commerce Reviews.csv\")\ndata[:2]","f991bd72":"print(\"Preparing text\")\nreviews = data[\"Review Text\"].dropna()\ntokens = [[word for word in nltk.word_tokenize(doc.lower()) if word] for doc in reviews]","e0c9321d":"# from this gist: https:\/\/gist.github.com\/maxim5\/c35ef2238ae708ccb0e55624e9e0252b\nprint('Training word2vec')\nword_model = gensim.models.Word2Vec(tokens, size=25, min_count=1, window=5, iter=100)\npretrained_weights = word_model.wv.syn0\nvocab_size, embedding_size = pretrained_weights.shape\nprint('Result embedding shape:', pretrained_weights.shape)","5eb81bda":"print('Checking similar words:')\nfor word in ['dress', 'tall', 'return', 'sweater']:\n    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.wv.most_similar(word)[:8])\n    print('  %s -> %s' % (word, most_similar))","a24f7796":"model.save(\"word2vec-clothes-25d.model\")","8d2bc75a":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nN_TOPICS = 10\n\nvec = TfidfVectorizer(stop_words = nltk.corpus.stopwords.words('english'), ngram_range=(1,3), min_df=5, max_df = 0.9)\nlda = LatentDirichletAllocation(n_components = N_TOPICS, )\n\npipeline = Pipeline([(\"vec\", vec), (\"lda\", lda)])\npipeline.fit(reviews)","d6ef7062":"vec_model = pipeline.steps[0][1]\nlda_model = pipeline.steps[1][1]","ba7ab76b":"from sklearn.metrics import homogeneity_score\n\ncluster_labels = np.argmax(pipeline.transform(reviews), axis=1)\nhomogeneity_score(data.loc[reviews.index, \"Rating\"], cluster_labels)","1da4e191":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (topic_idx))\n        print(\"Mean: {:.2f}\".format(data.loc[np.where(cluster_labels == topic_idx)[0], \"Rating\"].mean()))\n        print(\"Std: {:.2f}\".format(data.loc[np.where(cluster_labels == topic_idx)[0], \"Rating\"].std()))\n        print(\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\nno_top_words = 10\ndisplay_topics(lda_model, vec_model.get_feature_names(), no_top_words)","ad1d0ef2":"# some utility code\ndef word2idx(word):\n    return word_model.wv.vocab[word].index\ndef idx2word(idx):\n    return word_model.wv.index2word[idx]","a0a816b0":"# get mean word2vec of each sentence\nX_averaged_word2vec = [np.mean([word_model.wv.word_vec(word) for word in sentence], axis=0) for sentence in tokens]\nX_averaged_word2vec = np.array(X_averaged_word2vec)","5ac717bc":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters= N_TOPICS)\nkmeans.fit(X_averaged_word2vec)","09ed460e":"from sklearn.metrics import homogeneity_score\n\ncluster_labels = kmeans.predict(X_averaged_word2vec)\nhomogeneity_score(data.loc[reviews.index, \"Rating\"], cluster_labels)","c7a44019":"stopwords = nltk.corpus.stopwords.words('english')\n\ndef get_freq_per_topic(topic_idx):\n    topic_sentences = reviews.reindex(np.where(cluster_labels == topic_idx)[0])\n    topic_words = [nltk.word_tokenize(v) for v in topic_sentences.dropna()]\n    flattened_topic_words = [word.lower() for sublist in topic_words for word \n                             in sublist if len(word) >= 3 and word not in stopwords]\n    return pd.Series(nltk.FreqDist(flattened_topic_words))\n    ","1f1b7781":"# frequency of all words across all reviews\nall_words = [nltk.word_tokenize(v) for v in reviews.dropna()]\nflattened_all_words = [word.lower() for sublist in all_words \n                       for word in sublist if len(word) >= 3 and word not in stopwords]\nall_words_frequency = pd.Series(nltk.FreqDist(flattened_all_words))","1e8b329e":"list_series_frequencies = [get_freq_per_topic(v) for v in range(N_TOPICS)]","374d9940":"for cluster in range(N_TOPICS):\n    top_words_cluster = list_series_frequencies[cluster].sort_values(ascending=False)[:10000]\n    uniqueness_index = top_words_cluster.div(all_words_frequency, ).dropna().sort_values(ascending=False)\n    uniqueness_index = uniqueness_index[(uniqueness_index > 0.2)]\n    top_20 = list_series_frequencies[cluster].reindex(uniqueness_index.index)\\\n                                            .sort_values(ascending=False).index.tolist()[:20]\n    print(\"--- Cluster\", cluster, \"---\")\n    print(\"Mean: {:.2f}\".format(data.loc[np.where(cluster_labels == cluster)[0], \"Rating\"].mean()))\n    print(\"Std: {:.2f}\".format(data.loc[np.where(cluster_labels == cluster)[0], \"Rating\"].std()))\n    print(top_20)","a51af4cc":"%%time\nprint('\\nPreparing the data for LSTM...')\n\nmaxlen = max([len(v) for v in tokens])\nTIMESTEPS = 100\n\n# train_x = np.zeros([len(tokens), maxlen], dtype=np.int32)\n# train_y = np.zeros([len(tokens)], dtype=np.int32)\n\nlist_x = []\nlist_y = []\n\nfor _, sentence in enumerate(tokens):\n    for i in range(len(sentence)):\n        if len(sentence) <= i + TIMESTEPS:\n            # the last word is the target\n            input_words = sentence[:-1]\n            target_word = sentence[-1]\n            \n            list_input_words = []\n            for t, word in enumerate(input_words):\n                list_input_words.append(word2idx(word))\n            list_x.append(list_input_words)\n            list_y.append(word2idx(target_word))\n            break\n        else:\n            input_words = sentence[i:i+TIMESTEPS]\n            target_word = sentence[i+TIMESTEPS]\n            \n            list_input_words = []\n            for t, word in enumerate(input_words):\n                list_input_words.append(word2idx(word))\n            list_x.append(list_input_words)\n            list_y.append(word2idx(target_word))","a2012db7":"from keras.preprocessing.sequence import pad_sequences\n\nx_indexes = pad_sequences(list_x, maxlen=TIMESTEPS, padding='post')\ny_indexes = np.array(list_y)","efba521f":"from sklearn.model_selection import train_test_split\n\n# Train\/Test splits\nX_train, X_test, y_train, y_test = train_test_split(x_indexes, y_indexes, test_size=0.4)","cf8d6160":"from keras.layers import Input, LSTM, RepeatVector, TimeDistributed\nfrom keras.models import Model\n\nLSTM_SIZE = 50\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, \n                    weights=[pretrained_weights]), )\nmodel.add(LSTM(units=LSTM_SIZE))\nmodel.add(Dense(units=vocab_size))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n\n# # autoencoder model\n# model = Sequential()\n# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights], input_length=TIMESTEPS))\n# model.add(LSTM(units=LSTM_SIZE))\n# model.add(RepeatVector(TIMESTEPS))\n# model.add(LSTM(embedding_size, return_sequences=True))\n# model.add(LSTM(embedding_size, ))\n# model.add(TimeDistributed(Dense(1)))\n\n# model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae', 'mse'])\nmodel.summary()","13c7341e":"def sample(preds, temperature=1.0):\n    if temperature <= 0:\n        return np.argmax(preds)\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\ndef generate_next(text, num_generated=10):\n    word_idxs = [word2idx(word) for word in text.lower().split()]\n    for i in range(num_generated):\n        prediction = model.predict(x=np.array(word_idxs))\n        idx = sample(prediction[-1], temperature=0.7)\n        word_idxs.append(idx)\n    return ' '.join(idx2word(idx) for idx in word_idxs)\n\ndef on_epoch_end(epoch, _):\n    print('\\nGenerating text after epoch: %d' % epoch)\n    texts = [\n    'it\\'s a very',\n    'i',\n    'this is',\n    'i wanted',\n    'again'\n    ]\n    for text in texts:\n        sample = generate_next(text)\n        print('%s... -> %s' % (text, sample))\n","c48dbeec":"from keras.callbacks import LambdaCallback\n\nhistory = model.fit(X_train, y_train, validation_split= 0.2,\n                  batch_size=2048,\n                  epochs=100,\n                  callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])","0f70ae76":"# import keras.backend as K\n\n# K.clear_session()","eff35655":"# Preparation","71238b20":"# LSTM Architecture","1401305e":"## Getting the usual words per topic\n- Get frequency of all words\n- Get frequency of words inside each cluster\n- Divide the frequencies.\n- Intuition: if a word appeared only in 1 topic and it's frequent in that topic then it may be a unique word","c8567428":"# Word2Vec using this corpus","14841b1d":"## Bonus: Pretrained Word2Vec\nI'll concatenate my trained word2vec to a pretrained one. Let's see the similar words after.\n(todo)","12d6375e":"# Clustering\nWe'll use homogeneity score against the rating to come up with topics with uniform ratings.\n## Baseline: Vanilla LDA on 10 topics using count vectorizer ","cd6543a2":"# Preparing Input for Text Generation","e41908e0":"## K-Means on the word2vec\n- Average each word in the sentence\n- Do clustering"}}