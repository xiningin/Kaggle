{"cell_type":{"25921b19":"code","fbd654e3":"code","001fdcb0":"code","88d261da":"code","c182e10b":"code","8dbef59c":"code","a4aa9656":"code","818f026c":"code","e7972753":"code","77403b2e":"code","8d0ef144":"code","96db7111":"code","4d45da32":"code","66f3a9ce":"code","5968220f":"code","9254f502":"code","93fed319":"code","bdfe47f1":"code","c15024d3":"code","58290d8e":"code","881aeba0":"code","dffc5a3f":"code","8e20c092":"code","d24d7adc":"code","02fe89f7":"code","177015f1":"code","48b947a1":"code","bb7521b7":"code","839af645":"code","f3fc5572":"code","610e450d":"code","6b1a9046":"code","077cf228":"code","6e8da30d":"code","5a6c9350":"code","4e6b894d":"code","28b96863":"code","3e70f517":"code","8f26e14b":"code","c2994d85":"code","6a0ffa6f":"code","cad69856":"code","823d92d8":"code","2aff2eea":"markdown","16a19c6d":"markdown","7d4186ff":"markdown","0285ef36":"markdown","55fd5921":"markdown","475931cb":"markdown","fd1d5fc7":"markdown","0dc85cbf":"markdown","b1a3cc92":"markdown","b9ebf431":"markdown","834113ba":"markdown","4718f53b":"markdown","db24834c":"markdown","eb7a8275":"markdown","ee36f7f2":"markdown","88892628":"markdown","a14d6186":"markdown","5a6a3add":"markdown","dc2ca369":"markdown","d6c7adab":"markdown","5e170bf2":"markdown","8d777576":"markdown"},"source":{"25921b19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import scale\nfrom tensorflow.keras import backend as K, callbacks\nfrom sklearn.metrics import accuracy_score as accuracy, f1_score, mean_absolute_error as mae\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\nfrom keras.utils.vis_utils import plot_model\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fbd654e3":"dataset_NASDAQ = pd.read_csv(\"\/kaggle\/input\/cnnpred-stock-market-prediction\/Processed_NASDAQ.csv\", parse_dates=['Date'])\ndataset_NYSE = pd.read_csv(\"\/kaggle\/input\/cnnpred-stock-market-prediction\/Processed_NYSE.csv\", parse_dates=['Date'])\ndataset_SP = pd.read_csv(\"\/kaggle\/input\/cnnpred-stock-market-prediction\/Processed_SP.csv\", parse_dates=['Date'])\ndataset_DJI = pd.read_csv(\"\/kaggle\/input\/cnnpred-stock-market-prediction\/Processed_DJI.csv\", parse_dates=['Date'])\ndataset_RUSSELL = pd.read_csv(\"\/kaggle\/input\/cnnpred-stock-market-prediction\/Processed_RUSSELL.csv\", parse_dates=['Date'])","001fdcb0":"dataset_NASDAQ.head(5)","88d261da":"dataset_NASDAQ['Date']","c182e10b":"dataset_NASDAQ.index = dataset_NASDAQ['Date']\ndataset_NYSE.index = dataset_NYSE['Date']\ndataset_SP.index = dataset_SP['Date']\ndataset_DJI.index = dataset_DJI['Date']\ndataset_RUSSELL.index = dataset_RUSSELL['Date']","8dbef59c":"dataset_NASDAQ.index","a4aa9656":"dataset_NASDAQ['Date']","818f026c":"dataset_NASDAQ.columns","e7972753":"# del dataset_NASDAQ['Name'] # Deletes \"Name\" column","77403b2e":"dataset_NASDAQ.columns","8d0ef144":"dataset_NASDAQ","96db7111":"print(dataset_NASDAQ.shape)\nprint(dataset_NYSE.shape)\nprint(dataset_SP.shape)\nprint(dataset_DJI.shape)\nprint(dataset_RUSSELL.shape)","4d45da32":"whole_data = dataset_NASDAQ.append(dataset_NYSE, ignore_index=True)","66f3a9ce":"whole_data","5968220f":"whole_data = whole_data.append(dataset_SP, ignore_index=True)\nwhole_data = whole_data.append(dataset_DJI, ignore_index=True)\nwhole_data = whole_data.append(dataset_RUSSELL, ignore_index=True)\nwhole_data","9254f502":"whole_data[\"Close\"]","93fed319":"print(len(dataset_NASDAQ ))\nprint(len(dataset_NYSE))\nprint(len(dataset_SP))\nprint(len(dataset_DJI ))\nprint(len(dataset_RUSSELL))","bdfe47f1":"whole_data.isnull().sum()","c15024d3":"whole_data.info()","58290d8e":"import matplotlib.pyplot as plt\nwhole_data.hist(figsize=(30,30))\nplt.show()","881aeba0":"def cnn_data_sequence(data, target, seque_len):\n    print ('sequencing data ...')\n    new_train = []\n    new_target = []\n\n    for index in range(data.shape[1] - seque_len + 1):\n        new_train.append(data[:, index: index + seque_len])\n        new_target.append(target[index + seque_len - 1])\n\n    new_train = np.array(new_train)\n    new_target = np.array(new_target)\n\n    return new_train, new_target","dffc5a3f":"# Remember: we defined our indexes as our dates.\nprint(dataset_NASDAQ.Close[0])\nprint(\"--------\")\ndataset_NASDAQ.Close[0:5]","8e20c092":"# (data['Close'][predict_day:] \/ data['Close'][:-predict_day].values).astype(int)\n(dataset_NASDAQ['Close'][1:] \/ dataset_NASDAQ['Close'][:-1].values).astype(int) # -> Bu k\u0131sma bir daha bak","d24d7adc":"predict_index = \"RUT\"  #  -> Change this part to see the results. # RUT, S&P, NYA, NASDAQ, DJI\nnumber_of_stocks = 0\norder_stocks = []\npredict_day = 1\n\n\ndef prepare_for_CNN():\n    global number_of_stocks\n    global samples_in_each_stock\n    global number_feature\n    #global predict_index\n    global order_stocksw\n    tottal_train_data = np.empty((0,82))  # column number is 82\n    tottal_train_target = np.empty((0))\n    tottal_test_data = np.empty((0,82))\n    tottal_test_target = np.empty((0))\n    \n    for data in [dataset_DJI, dataset_NASDAQ, dataset_NYSE, dataset_RUSSELL, dataset_SP]:\n        \n        number_of_stocks += 1\n        \n        df_name = data['Name'][0]\n        order_stocks.append(df_name)\n        del data['Name']                # deletes \"Name\" column\n\n        target = (data['Close'][predict_day:] \/ data['Close'][:-predict_day].values).astype(int)\n        print(target)   # (0's or 1's.)\n        print(\"*****\")\n        data = data[:-predict_day]\n        target.index = data.index\n        # Becasue of using 200 days Moving Average as one of the features\n        data = data[200:]                         # Bu k\u0131sma bir daha bak\n        data = data.fillna(0)\n        data['target'] = target\n        target = data['target']\n        del data['target']\n        del data['Date']\n        # data['Date'] = data['Date'].apply(lambda x: x.weekday())\n\n        number_feature = data.shape[1]\n        samples_in_each_stock = data.shape[0]\n\n        train_data = data[data.index < '2016-04-21']\n        train_data = scale(train_data)\n        \n        print(\"train data:\", train_data)\n        \n        if df_name == predict_index:\n            tottal_train_target = target[target.index < '2016-04-21']\n            tottal_test_target = target[target.index >= '2016-04-21']\n\n        data = pd.DataFrame(scale(data.values), columns=data.columns)\n        data.index = target.index\n        test_data = data[data.index >= '2016-04-21']\n\n        tottal_train_data = np.concatenate((tottal_train_data, train_data))\n        print(tottal_train_data.shape)\n        tottal_test_data = np.concatenate((tottal_test_data, test_data))\n        print(tottal_test_data.shape)\n        \n    print(\"order_stocks:\", order_stocks)\n        \n    train_size = int(tottal_train_data.shape[0]\/number_of_stocks)\n    print(\"Train size:\", train_size)\n    test_size = int(tottal_test_data.shape[0] \/ number_of_stocks)\n    print(\"Test size:\", test_size)\n    \n    tottal_train_data = tottal_train_data.reshape(number_of_stocks, train_size, number_feature)\n    print(\"Total train data shape:\", tottal_train_data.shape)\n    tottal_test_data = tottal_test_data.reshape(number_of_stocks, test_size, number_feature) \n    print(\"Total test data shape:\", tottal_test_data.shape)\n    \n    print(\"tottal_train_target:\", tottal_train_target, \"shape :\", tottal_train_target.shape)\n    print(\"tottal_test_target: \", tottal_test_target, \"shape:\", tottal_test_target.shape)\n\n    return tottal_train_data, tottal_test_data, tottal_train_target, tottal_test_target","02fe89f7":"def f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision_pos = precision(y_true, y_pred)\n    recall_pos = recall(y_true, y_pred)\n    precision_neg = precision((K.ones_like(y_true)-y_true), (K.ones_like(y_pred)-K.clip(y_pred, 0, 1)))\n    recall_neg = recall((K.ones_like(y_true)-y_true), (K.ones_like(y_pred)-K.clip(y_pred, 0, 1)))\n    f_posit = 2*((precision_pos*recall_pos)\/(precision_pos+recall_pos+K.epsilon()))\n    f_neg = 2 * ((precision_neg * recall_neg) \/ (precision_neg + recall_neg + K.epsilon()))\n\n    return (f_posit + f_neg) \/ 2\n\ndef sklearn_acc(model, test_data, test_target):\n    overall_results = model.predict(test_data)\n    test_pred = (overall_results > 0.5).astype(int)\n    acc_results = [mae(overall_results, test_target), accuracy(test_pred, test_target),\n                   f1_score(test_pred, test_target, average='macro')]\n\n    return acc_results","177015f1":"number_filter = [8,8,8]\n\ndef CNN(train_data, test_data, train_target, test_target):\n    # hisory of data in each sample\n    seq_len = 60\n    epoc = 100\n    drop = 0.1\n    \n    cnn_train_data, cnn_train_target = cnn_data_sequence(train_data, train_target, seq_len)\n    print(\"cnn_train_data:\", cnn_train_data.shape)\n    print(\"cnn_train_target:\", cnn_train_target.shape)\n    \n    cnn_test_data, cnn_test_target = cnn_data_sequence(test_data, test_target, seq_len)\n    print(\"cnn_test_data:\", cnn_test_data.shape)\n    print(\"cnn_test_target:\", cnn_test_target.shape)\n    \n    result = []\n    \n    for i in range(1,40):   \n        K.clear_session()\n        print ('i: ', i)\n        \n        print('fitting model')\n        \n        model = Sequential()\n                                            # M0 -> inputs are given (60,5,82)\n        #layer 1                            # M1 -> (60,5,8) (# past days, # markets, # features)\n        model.add(Conv2D(number_filter[0], (1, 1), activation='relu', input_shape=(number_of_stocks,seq_len, number_feature), data_format='channels_last'))\n        model.add(Dropout(0.1)) # added     # +++ Dropout \n        \n        #layer 2\n        model.add(BatchNormalization())     # added   +++ Batch Normalization\n        model.add(Conv2D(number_filter[1], (number_of_stocks, 3), activation='relu'))\n        model.add(MaxPool2D(pool_size=(1, 2)))\n        model.add(Dropout(0.2)) # added\n\n        \n        #layer 3\n        model.add(Conv2D(number_filter[2], (1, 3), activation='relu'))\n        model.add(MaxPool2D(pool_size=(1, 2)))\n        \n        # Flattening Layer:\n        model.add(Flatten())\n        model.add(Dropout(0.01))  # added\n        \n        # Last Layer:\n        model.add(Dense(1, activation='sigmoid'))\n\n        model.compile(optimizer='Adam', loss='mae', metrics=['acc',f1])\n\n        #best_model = callbacks.ModelCheckpoint(filepath, monitor='val_f1', verbose=0, save_best_only=True,\n                                               #save_weights_only=False, mode='max', period=1)\n\n        model.fit(cnn_train_data, cnn_train_target, epochs=epoc, batch_size=128, verbose=0, validation_split=0.25) #callbacks=[best_model],\n\n    #   model = load_model(filepath, custom_objects={'f1': f1})\n        test_pred = sklearn_acc(model,cnn_test_data, cnn_test_target)\n        print (test_pred)\n        result.append(test_pred)\n                \n        model.summary() # added \n        \n        plot_model(model)    \n    \n    print('saving results')\n    results = pd.DataFrame(result , columns=['MAE', 'Accuracy', 'F-score'])\n    results = results.append([results.mean(), results.max(), results.std()], ignore_index=True)\n    #results.to_csv(join(Base_dir, '3D-models\/{}\/new results.csv'.format(predict_index)), index=False)\n    return results\n    ","48b947a1":"train_data, test_data, train_target, test_target = prepare_for_CNN()\n\nCNN(train_data, test_data, train_target, test_target)","bb7521b7":"def cnn_data_sequence_for_intermediate(data, target, seque_len):\n    print ('sequencing data ...')\n    new_train = []\n    new_target = []\n\n    for index in range(data.shape[1] - seque_len + 1):\n        new_train.append(data[:, index: index + seque_len])\n        new_target.append(target[index + seque_len - 1])\n\n    new_train = np.array(new_train)\n    new_target = np.array(new_target)\n\n    return new_train, new_target","839af645":"from tensorflow.python.keras import Model\n\ndef intermediate_CNN(train_data, test_data, train_target, test_target):\n    # hisory of data in each sample\n    seq_len = 60\n    epoc = 100\n    drop = 0.1\n    \n    cnn_train_data, cnn_train_target = cnn_data_sequence(train_data, train_target, seq_len)\n    print(\"cnn_train_data:\", cnn_train_data.shape)\n    print(\"cnn_train_target:\", cnn_train_target.shape)\n    \n    cnn_test_data, cnn_test_target = cnn_data_sequence(test_data, test_target, seq_len)\n    print(\"cnn_test_data:\", cnn_test_data.shape)\n    print(\"cnn_test_target:\", cnn_test_target.shape)\n    \n    result = []\n    \n           \n    print('fitting model')\n    \n    model = Sequential()\n                                        # M0 -> inputs are given (60,5,82)\n    #layer 1                            # M1 -> (60,5,8) (# past days, # markets, # features)\n    model.add(Conv2D(number_filter[0], (1, 1), activation='relu', input_shape=(number_of_stocks,seq_len, number_feature), data_format='channels_last'))\n    model.add(Dropout(0.1)) # added     # +++ Dropout \n    \n    #layer 2\n    model.add(BatchNormalization())     # added   +++ Batch Normalization\n    model.add(Conv2D(number_filter[1], (number_of_stocks, 3), activation='relu'))\n    model.add(MaxPool2D(pool_size=(1, 2)))\n    model.add(Dropout(0.2)) # added\n\n    \n    #layer 3\n    model.add(Conv2D(number_filter[2], (1, 3), activation='relu'))\n    model.add(MaxPool2D(pool_size=(1, 2)))\n    \n    # Flattening Layer:\n    model.add(Flatten())\n    outputlayer = Dropout(0.01)\n    model.add(outputlayer)  # added\n        \n    return model,outputlayer","f3fc5572":"new_train, new_target = cnn_data_sequence_for_intermediate(train_data, train_target, 60)\nmodel, outputlayer = intermediate_CNN(train_data, test_data, train_target, test_target)\nintermediate_model = Model(model.input, outputlayer.output)\ncnn_features = intermediate_model.predict(new_train)","610e450d":"new_test, new_target_test = cnn_data_sequence_for_intermediate(test_data, test_target, 60)\ntest_cnn_features = intermediate_model.predict(new_test)","6b1a9046":"from xgboost import XGBClassifier","077cf228":"xgbmodel = XGBClassifier()","6e8da30d":"xgbmodel.fit(cnn_features, new_target)","5a6c9350":"pred = xgbmodel.predict(test_cnn_features)","4e6b894d":"from sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(new_target_test, pred)\nprint(\"Accuracy:\", accuracy)","28b96863":"print(\"f1 score:\", f1_score(new_target_test, pred, average=\"macro\"))","3e70f517":"print(\"MAE:\", mae(new_target_test, pred))","8f26e14b":"best_SP_results = []\nSP_CNN_XGBoost = [0.5532544378698225, 0.5518382901750042, 0.4467455621301775] # Accuracy, F1, MAE","c2994d85":"best_DJI_results = []\nDJI_CNN_XGBoost = [0.4970414201183432, 0.48484848484848486, 0.5029585798816568]","6a0ffa6f":"best_NASDAQ_results = []\nNASDAQ_CNN_XGBoost = [0.5355029585798816, 0.5219063921798279, 0.46449704142011833]","cad69856":"best_NYSE_results = []\nNYSE_CNN_XGBoost = [0.5325443786982249, 0.45984545049965614, 0.46745562130177515]","823d92d8":"best_RUSSELL_results = []\nRUSSELL_CNN_XGBoost = [0.5266272189349113, 0.5255640944660842, 0.47337278106508873]","2aff2eea":"# NASDAQ Results:","16a19c6d":"![image.png](attachment:4b8fb5e9-d3db-419f-a586-9cfbad869b00.png)","7d4186ff":"![image.png](attachment:9f2b007a-5b4b-44c5-b1c4-eb7dce423c21.png)\n![image.png](attachment:a141bf18-89e5-40b5-a42d-2361925d6f9b.png)\n![image.png](attachment:75306e14-0418-4bfa-9617-c0bf03d4d46d.png)","0285ef36":"![image.png](attachment:df7b0e18-a1be-4e0e-a39c-bad3e9704eb8.png)","55fd5921":"![image.png](attachment:eb92e190-7ae1-45df-a694-79a72ca9a29d.png)","475931cb":"![image.png](attachment:e05688a1-603b-4f87-925a-b11c2a140977.png)","fd1d5fc7":"## Data Preparation","0dc85cbf":"![image.png](attachment:87cce2fb-0699-48d2-bfef-3d348912cbf1.png)\n![image.png](attachment:b1d4aa43-6eb0-489f-b744-435f8c82000d.png)\n![image.png](attachment:993fcfde-b2b8-4f80-ad05-813207bf6f3f.png)","b1a3cc92":"![image.png](attachment:cc7e78f6-bc70-4e94-876b-953d25b663c8.png)\n![image.png](attachment:4025b31c-59c0-4483-81ac-8ebf83e0d520.png)\n![image.png](attachment:6541f186-bf7c-46eb-bf11-a5e0c2f290d3.png)","b9ebf431":"![image.png](attachment:dc5b0891-05e0-43da-a732-8b1540e20795.png)\n![image.png](attachment:a8e4995d-4317-4522-a860-da8c21d9913d.png)\n![image.png](attachment:d205bc03-1c75-42a6-bb03-28debd3ee7fe.png)","834113ba":"![image.png](attachment:40ef2ea6-6c1b-45f2-a1fe-e94a457499eb.png)","4718f53b":"![image.png](attachment:36357dba-a589-4cfa-97d1-372796ee33b8.png)","db24834c":"# DJI Results:","eb7a8275":"![image.png](attachment:742c6c67-dac8-428e-920c-21748aff14c6.png)","ee36f7f2":"# S&P 500 Results:","88892628":"![image.png](attachment:fba87088-d3ec-44de-91b7-c9c47d0d2a38.png)\n![image.png](attachment:7f67fbf9-88bd-49f0-a333-5db70fcabebe.png)\n![image.png](attachment:199dc984-7f6b-4997-89c6-a55a77cc199a.png)","a14d6186":"# RUSSELL Results:","5a6a3add":"![image.png](attachment:c4e62991-62ae-4770-8ce9-55045ddd5a90.png)","dc2ca369":"![image.png](attachment:13c42fb9-ab65-4b0f-bbd7-10c4d0b90973.png)","d6c7adab":"# XGBoost","5e170bf2":"![image.png](attachment:66fbdff2-e2be-4283-8069-50801b883d48.png)","8d777576":"# NYSE Results:"}}