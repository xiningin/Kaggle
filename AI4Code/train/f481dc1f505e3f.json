{"cell_type":{"29a8bbba":"code","1eb3284b":"code","70aacf55":"code","60e19720":"code","bcdc5c4f":"code","a47f74db":"code","8c2f1d0b":"code","87e79be2":"code","f3a16c49":"code","f90ebd34":"code","fb20d18d":"markdown","bc99c812":"markdown","38d201d7":"markdown","07843e35":"markdown","e1780253":"markdown","5a6e90f9":"markdown","67333a7d":"markdown","28c26636":"markdown","791f88fe":"markdown","cb3b5f61":"markdown","05806252":"markdown","20c4c518":"markdown","330e3c9f":"markdown"},"source":{"29a8bbba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom time import time\nfrom typing import List, Tuple, Union\nfrom functools import partial\n\nfrom scipy.optimize import linprog\nimport scipy.stats as stats\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score, f1_score\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport holoviews as hv\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.hv.extension('bokeh')\nhv.extension('bokeh')","1eb3284b":"def _unit_shadow_prices(\n    model_metrics: pd.Series, peer_metrics: pd.DataFrame, greater_is_better: List[bool], compute_primal: bool = False\n) -> np.ndarray:\n    peer_metrics = (peer_metrics.where(lambda x: x.ne(model_metrics, 0), np.nan)\n    .dropna())\n\n    greater_is_better_weight = np.where(greater_is_better, 1, -1)\n    inputs_outputs = greater_is_better_weight * np.ones_like(peer_metrics)\n\n    # outputs - inputs\n    A_ub = inputs_outputs * peer_metrics\n    b_ub = np.zeros(A_ub.shape[0])\n\n    # \\sum chosen model inputs = 1\n    A_eq = np.where(greater_is_better_weight < 0.0, model_metrics, 0).reshape(1, -1)\n    b_eq = np.array(1.0).reshape(1, -1)\n\n    # max outputs == min -outputs\n    c = np.where(greater_is_better_weight >= 0.0, model_metrics, 0.0).reshape(1, -1)\n\n    # compute dual\n    dual_A_ub = np.vstack((A_ub, A_eq)).T\n    dual_c = np.hstack((b_ub, b_eq.reshape(-1,))).T\n    dual_b_ub = c.T\n\n    dual_result = linprog(\n        dual_c,\n        A_ub=-dual_A_ub,\n        b_ub=-dual_b_ub,\n        bounds=[(0, None) for _ in range(dual_A_ub.shape[1] -1 )] + [(None, None)],\n    )\n\n    return dual_result.fun\n\n\ndef data_envelopment_analysis(\n    validation_metrics: Union[pd.DataFrame, np.ndarray], greater_is_better: List = []\n) -> pd.DataFrame:\n    \"\"\"\n    :param validation_metrics: Metrics produced by __SearchCV\n    :param greater_is_better: Whether that metric are to be considered inputs to decrease or outputs to increase\n    :return: Shadow prices for comparing a model to is peers & Hypothetical Comparison Units to compare units\n    \"\"\"\n    partialed_unit_shadow_scores = partial(\n        _unit_shadow_prices,\n        peer_metrics=validation_metrics,\n        greater_is_better=greater_is_better,\n    )\n    efficiency_scores = pd.DataFrame(validation_metrics).apply(\n        partialed_unit_shadow_scores, axis=1\n    )\n\n    return efficiency_scores\n","70aacf55":"# get some data\nX, y = load_breast_cancer(return_X_y=True)\n# build a classifier\nclf = MLPClassifier()\n# specify parameters and distributions to sample from\nparam_dist = {\n    \"hidden_layer_sizes\": [(10, 5, ), (10, ), (10, 5, 3, )],\n    \"learning_rate_init\": stats.uniform(0, 1),\n    \"alpha\": stats.uniform(1e-4, 1e0),\n}\n# run randomized search\nn_iter_search = 100\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score),  'F1': make_scorer(f1_score)}\nrandom_search = RandomizedSearchCV(\n    clf, param_distributions=param_dist, \n    scoring = scoring,\n    n_iter=n_iter_search, \n    n_jobs=-1,\n    refit = 'AUC',\n    return_train_score=True\n)\nrandom_search.fit(X, y)\ncv_results_df = pd.DataFrame(random_search.cv_results_)\n\n\n# %%\nmetrics = [\n    \"mean_fit_time\",\n    \"mean_score_time\",\n    \"mean_test_Accuracy\",\n    \"mean_test_AUC\",\n    \"mean_test_F1\",\n]\nmetrics_greater_is_better = [False, False, True, True, True]\nefficiency_scores = data_envelopment_analysis(\n    validation_metrics=cv_results_df[metrics],\n    greater_is_better=metrics_greater_is_better,\n)","60e19720":"table = hv.Dataset(cv_results_df.loc[:, metrics])\nmatrix = hv.operation.gridmatrix(table)\n\ntop = hv.Dataset(cv_results_df.loc[:, metrics]\n                   .assign(efficiency_scores=efficiency_scores)\n                   .where(lambda df: df.efficiency_scores >= df.efficiency_scores.quantile(0.75))\n                   .drop(columns=['efficiency_scores']))\nbest = hv.operation.gridmatrix(top)\n(matrix * best).opts(title='Top 25% in Red', width=800, height=600)","bcdc5c4f":"(cv_results_df.loc[:, ['param_alpha', 'param_hidden_layer_sizes', 'param_learning_rate_init'] + metrics]\n .assign(efficiency_scores=efficiency_scores)\n .nlargest(3, 'efficiency_scores'))","a47f74db":"(cv_results_df.loc[:, ['param_alpha', 'param_hidden_layer_sizes', 'param_learning_rate_init'] + metrics]\n .assign(efficiency_scores=efficiency_scores)\n .nsmallest(3, 'efficiency_scores'))","8c2f1d0b":"columns = ['RANK', 'METHOD', 'TOP 1 ACCURACY', 'TOP 5 ACCURACY', 'NUMBER OF PARAMS', 'PAPER TITLE', 'YEAR']\nimagenet = pd.read_csv('\/kaggle\/input\/papers-with-code-imagenet-rankings\/efficiency_results (1).csv', usecols=columns)\nimagenet.loc[:, 'NUMBER OF PARAMS'] = imagenet.loc[:, 'NUMBER OF PARAMS'].apply(lambda s: float(s[:-1]))\nimagenet.loc[:, 'TOP 5 ACCURACY'] = imagenet.loc[:, 'TOP 5 ACCURACY'].apply(lambda s: float(s[:-1])\/100).astype(float)\nimagenet.loc[:, 'TOP 1 ACCURACY'] = imagenet.loc[:, 'TOP 1 ACCURACY'].apply(lambda s: float('.'.join(s[:-1].split(',')))\/100).astype(float)\n\nimagenet","87e79be2":"imagenet_metrics = ['NUMBER OF PARAMS', 'TOP 1 ACCURACY','TOP 5 ACCURACY']\nimagenet_metrics_greater_is_better = [False, True, True]\n                    \nimagenet_efficiency_scores = data_envelopment_analysis(\n    validation_metrics=imagenet.loc[:, imagenet_metrics],\n    greater_is_better=imagenet_metrics_greater_is_better,\n)\n\ntable = hv.Dataset(imagenet.loc[:, imagenet_metrics])\nmatrix = hv.operation.gridmatrix(table)\n\ntop = hv.Dataset(imagenet.loc[:, imagenet_metrics]\n                   .assign(efficiency_scores=imagenet_efficiency_scores)\n                   .where(lambda df: df.efficiency_scores >= df.efficiency_scores.quantile(0.75))\n                   .dropna()\n                   .drop(columns=['efficiency_scores']))\nbest = hv.operation.gridmatrix(top)\n(matrix * best).opts(title='Top 25% in Red', width=800, height=600)","f3a16c49":"(imagenet\n .assign(efficiency_scores=imagenet_efficiency_scores)\n .nlargest(3, 'efficiency_scores'))","f90ebd34":"(imagenet\n .assign(efficiency_scores=imagenet_efficiency_scores)\n .nsmallest(3, 'efficiency_scores'))","fb20d18d":"\nApplications of Machine Learning at the Edge face many design constraints around the size, speed and accuracy of Machine Learning pipelines.  In order to handle such constraints, practitioners have looked to two primary areas of study: model compression and acceleration, which aims to shrink existing models to fit on device, and multi-objective optimization, which aim to optimally search for new efficient models.  \n\nWhile these two approaches can and have been used in conjunction with one-another, neither method addresses the problem of scoring and selecting from within the large space of models which lie of the Pareto-optimal front. As such, these methods rely on domain expertise in evaluating these trade-off and provide limited signal to their underlying optimization procedures on the efficiency of particular models compared to their peers.  \n\nData Envelopment Analysis (DEA) is a non-parametric method in Operations Research for estimating production possibility frontiers. Inside of finance and quantitative management, DEA is typically used as a method by which to access the productive efficiency of fund managers or business units. Using DEA, one aims to assess a decision making unit (DMU) against their peers based an their optimal weighting of inputs, such as their allocated funds and number of staff, and outputs, such as their performance against a benchmark and fund value-at-risk (VAR).  In the equations below, we show the dual formulation of DEA which provides a means by which to compute an efficiency\nmeasure, $E$, to score DMU's.  This score represents the largest ratio of inputs used by a weighted sum of non-reference DMU's to the inputs of a reference DMU at some minimum level of output.  \n\n$$\n\\begin{array}{ll@{}ll}\n\\text{Minimise}  & \\displaystyle E  &\\\\\n\\text{subject to}& \\displaystyle E x_{i, 0} -& \\sum\\limits_{k=1}^{n}   \\lambda_{k}x_{i, k} \\geq 0  & \\forall i \\in 1 ,..., m \\\\\n\\text{}  &\n\\displaystyle &\\sum\\limits_{i=1}^{m}   \\lambda_{k}y_{r, k}  \\geq y_{r, 0}  &\\forall r \\in 1 ,..., s\\\\\n                 &                                                & \\lambda_{k} \\geq 0\n\\end{array}\n$$\n\nTo date, Data  Envelopment  Analysis  (DEA) has seen little study in it's application to model scoring, selection and optimization.  By considering model size, memory usage, training and inference time as inputs and measures like Accuracy, Precision and Recall as outputs, one can devise a approach using DEA by which to score models' efficiency based on their performance across criteria and resource utilization. ","bc99c812":"Unsurprisingly our most efficient models were many of our models designed for the edge.  Here, these models demonstrate high levels of Accurancy for their given number of parameters. ","38d201d7":"The worst performers were our larger models with some having over 800 million parameters. ","07843e35":"While simulated data is valuable in any study, I thought it interesting to investigate the use of real-world data for our analysis. Here I used a dateset from the platform Papers with Code, which details the scores for state-of-the-art models on the ImageNet image classification problem through time. Here we will be comparing the efficiency of models based on their inpus of NUMBER OF PARAMS (in millions) to their predictive accuracy metrics, TOP 1 ACCURACY and TOP 5 ACCURACY. ","e1780253":"Our worst models tend to be larger models with higher learning rates and lower levels of regularization. ","5a6e90f9":"In the plot below, we can easily see the pareto front which emergences for all paired metrics. In red, I have flagged the models which DEA has scores as beng in the top 25% of models with the greatest efficiency. Using DEA, have have attempted to find models which for a given budget of compute provide the best AUC,F1 and Acurracy Scores. ","67333a7d":"# Experiment\nIn this notebook we have taken two approaches to investigating model performance using the Efficiency Scores of Data Envelopment Analysis.  The first approach uses the results from a randomized search of hyper-parameters for a shallow neural network classifier trained using Stochastic Gradient Descent (SGD) on the Breast Cancer Wisconsin (Diagnostic) Data Set, and the second  approach investigates a public dataset of ImageNet model metrics provided by the [Papers with Code](https:\/\/paperswithcode.com\/sota\/image-classification-on-imagenet). ","28c26636":"As we can see from our results, our best models appear to be our shallow models with high leels of regularization and low learning rates. This is to be expected for such a dataset and provides confidence in our approach taken. ","791f88fe":"## Neural Network Classifier\nFor our neural network model, we have opted to perform DEA on the results of random search accross 100 models. Accross these models we have explored neural networks of varying depth, regularization and learning rate.  ","cb3b5f61":"# Papers with Code","05806252":"Industry applications for Machine Learning aim to balance objectives in model compression, inference speed, model bias and variance and model stability across random initialization and updates as part of their design considerations.  While traditional methods in hyperparamter optimization focus on optimally sampling from the universe of model architectures and hyperparameters to improve only a single metric for model performance, multi-objective methods in hyper-parameter optimization have been used select models which are optimal across one or more criteria.  In this notebook I aim to investigate the applications of Data Envelopment Analysis (DEA) as method for multi-objective hyper-parameter search and model selection in constrained environments. ","20c4c518":"# Conclusion\nThis is an idea I have been playing around with with a while now. I initially wrote a blog post on this idea about two years ago after taking an advanced course in Operations Research. I explored the idea again about 9 months ago when I did some benchmarking for my [super_spirals github project](https:\/\/github.com\/marcusinthesky\/super-spirals) which I collaberated with one of my classmates on and worked on an early proposal around this concept. I am still a little skeptical about the approach myself, but really like the idea and believe despite some reaching assumptions it may make a great heuristic for model selection in some settings. ","330e3c9f":"I again opted to mark out top 25% best models, based on efficiency scores, in red. What is interesting to note is now much model architecture plays into efficiency. This is definately an artefact of how DEA contracts input and outputs, but it worth noting. "}}