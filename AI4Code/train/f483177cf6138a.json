{"cell_type":{"cd73fe87":"code","e254ad92":"code","c1cc6be3":"code","9d9165b5":"code","537ace4a":"code","22d168be":"code","e6dd2ec6":"code","dd48bae5":"code","5a89328e":"code","95a1cebf":"code","90b2876b":"code","31d938cf":"code","51d77f99":"code","bc3cfce5":"code","6b2287c1":"code","01f8102b":"code","3f96a857":"code","09f55f8a":"code","64f841fb":"code","900c19b9":"code","1aa90bc6":"code","053aa32d":"code","94ab4f81":"code","0101a988":"code","6423d0d4":"code","1c911711":"code","37134070":"code","31938900":"code","b2559891":"markdown","c09a1a29":"markdown","a3b8ecf6":"markdown","36df7a6a":"markdown","dee72fcf":"markdown","d237ba74":"markdown","7e39bd5c":"markdown","ed33c282":"markdown","561d1da9":"markdown","818fc849":"markdown","259ae1ea":"markdown"},"source":{"cd73fe87":"# Import des librairies n\u00e9cessaires\nimport matplotlib.pyplot as plt       # Plotting\nimport numpy as np                    # Tableau Multidimensionnel\nimport pandas as pd                   # Manipulation des Dataframe\nimport seaborn as sns                 # Librairie de visualisation\n\n# Librairies Scikit Learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# Dataset des chiffres du MNIST\nfrom keras.datasets import mnist\n\n# Librairies Keras pour la construction du r\u00e9seau CNN\nimport keras\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Dense, Flatten\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\nfrom keras.models import load_model\nimport matplotlib.pyplot as plt","e254ad92":"# Configurations principales de nos mod\u00e8les\nIMG_SIZE          = 28               # taille cot\u00e9 final d'une image en pixel (ici 28x28)\nNB_EPOCHS_DENOISE = 10               # nombre epoch alogithme debruiter\nNB_EPOCHS_CLASSIF = 10               # nombre epoch alogithme classification des digits\nBATCH_SIZE        = 64               # taille batch de traitement\nNOISE_FACTOR      = 0.75             # facteur de bruitage gaussian\nPLOT_SIZE         = (20,2)           # visualisation matplotlib\nDISPLAY_IMG       = 10               # visualisation matplotlib\nSAV_MODEL_DENOISE = \"denoiser.h5\"    # sauvegarde du modele de debruitage\nSAV_MODEL_PREDICT = \"classifier.h5\"  # sauvegarde du modele de classification\nNUM_CAT_DIGIT     = 10               ","c1cc6be3":"# Fonction pour afficher les donn\u00e9es matricielles sous forme d'images\ndef display_image(X, y, n, label=False):\n    plt.figure(figsize=(20,2))\n    for i in range(n):\n        ax = plt.subplot(1, n, i+1)\n        plt.imshow(X[i].reshape(IMG_SIZE, IMG_SIZE))\n        if label:\n            plt.title(\"Digit: {}\".format(y[i]))\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    plt.show()","9d9165b5":"# Path vers les jeux de donn\u00e9es\ntrain_dir = \"..\/input\/train.csv\"\ntest_dir = \"..\/input\/test.csv\"\n\n# Lecture du jeu d'entrainement via pandas \u00e0 partir d'un fichier csv\ndf_train = pd.read_csv(train_dir)","537ace4a":"# Chargement des donn\u00e9es X et y (pour la classification)\ny_train = df_train['label']\nX_train = df_train.drop(columns=['label'])  # suppression du label\n\n# Nombre de classe de digits\nNUM_CAT_DIGIT = y_train.nunique()\nprint(\"Il y a {} classes de Digits dans le Dataset\".format(NUM_CAT_DIGIT))","22d168be":"# On affiche les images connues avec leur labels\n# Pour convertir un dataframe vers un numpy array on utilise .values\ndisplay_image(X_train.values, y_train, n=10, label=True)","e6dd2ec6":"# 1. Split entre jeu d'entrainement et jeu de validation avec un ratio de 90\/10\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=False)","dd48bae5":"# 2. Reshape des data pour les formatter en 28x28x1 (3Dimensions n\u00e9cessaires)\nX_train = X_train.values.reshape(-1, 28,28,1)\nX_val = X_val.values.reshape(-1, 28,28,1)","5a89328e":"# 3. Normalisation des donn\u00e9es pour avoir des valeurs de pixels entre 0 et 255\nX_train = X_train \/ 255.0\nX_val = X_val \/ 255.0","95a1cebf":"# 4. Remplacement des valeurs des labels par des valeurs cat\u00e9goriques\nY_train  = pd.get_dummies(y_train).values\nY_val  = pd.get_dummies(y_val).values","90b2876b":"# Lecture jeu de test\nX_test = pd.read_csv(test_dir)\n\n# Traitement des donn\u00e9es de la m\u00eame fa\u00e7on que pour l'entrainement\n# Reshape\nX_test = X_test.values.reshape(-1, 28,28,1)\n# Normalisation\nX_test = X_test \/ 255.0","31d938cf":"# Processing des images (pas de traitement particulier pour ne pas alourdir le mod\u00e8le) :\nfrom keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator( featurewise_center=False )","51d77f99":"# Initialisation \nclassifier = Sequential()\n# ------------------------------------\n# Couche de Convolution et MaxPooling\n# ------------------------------------\n# Conv2D : https:\/\/keras.io\/layers\/convolutional\/\n#     filters : nombres de filtres de convolutions\n#     kernel_size : taille des filtres de la fen\u00eatre de convolution \n#     input_shape : taille de l'image en entr\u00e9e (\u00e0 pr\u00e9ciser seulement pour la premi\u00e8re couche)\n#     activation  : choix de la fonction d'activation\n# BatchNormalisation : permet de normaliser les coefficients d'activation afin de les maintenirs proche de 0 pour simplifier les calculs num\u00e9riques\n# MaxPooling : Op\u00e9ration de maxPooling sur des donn\u00e9es spatiales (2D) : voir illustration ci-dessus\n# Dropout : permet de d\u00e9sactiver al\u00e9atoirement une proportion de neurones (afin d'\u00e9viter le surentrainement sur le jeu d'entrainement)\n\nclassifier.add(Conv2D(filters = 32, kernel_size = (5, 5), activation='relu', padding='Same', input_shape = (IMG_SIZE, IMG_SIZE, 1)))\nclassifier.add(Conv2D(filters = 64, kernel_size = (3, 3), activation='relu', padding='Same'))\nclassifier.add(MaxPooling2D(strides=(2,2)))\nclassifier.add(Dropout(0.25))","bc3cfce5":"# -----------------------------------------\n# Classifier (couche enti\u00e8rement Connect\u00e9e)\n# -----------------------------------------\n\n# Flatten : conversion d'une matrice en un vecteur plat\n# Dense   : neurones\nclassifier.add(Flatten())     # Applatissement de la sortie du r\u00e9seau de convolution\nclassifier.add(Dense(units=256, activation='relu'))\nclassifier.add(Dropout(0.25))\n\n# Couche de sortie : nombre de neurones = nombre de classe \u00e0 pr\u00e9dire\nclassifier.add(Dense(units=NUM_CAT_DIGIT, activation='softmax'))","6b2287c1":"# R\u00e9capitulatif de l'architecture mod\u00e8le de classification\nclassifier.summary()","01f8102b":"# S\u00e9lection de l'optimiser pour la decente de gradient\nclassifier.compile(loss='categorical_crossentropy', optimizer = Adam(lr=0.0001), metrics=[\"accuracy\"])","3f96a857":"# D\u00e9marrage de l'entrainement du r\u00e9seau\nhist = classifier.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n                                steps_per_epoch=200,              # nombre d'\u00e9tape car nous utilisons des g\u00e9n\u00e9rateurs\n                                epochs=NB_EPOCHS_CLASSIF,         # nombre de boucle \u00e0 r\u00e9aliser sur le jeu de donn\u00e9es complet\n                                verbose=1,                        # verbosit\u00e9\n                                validation_data=(X_val, Y_val))   # donn\u00e9es de validation (X(donn\u00e9es) et y(labels))","09f55f8a":"# Evaluation de la performance du classifier\nfinal_loss, final_acc = classifier.evaluate(X_val, Y_val, verbose=0)\nprint(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))","64f841fb":"# Affichage des courbes d'apprentissage\n# Loss\nplt.subplots(figsize=(12,6))\nplt.title(\"Loss\")\nplt.plot(hist.history['loss'], color='b', label='train_loss')\nplt.plot(hist.history['val_loss'], color='r', label='validation_loss')\nplt.legend(loc='upper right')\nplt.show()\n\n# Accuracy\nplt.subplots(figsize=(12,6))\nplt.title(\"Accuracy\")\nplt.plot(hist.history['acc'], color='b', label='train_accuracy')\nplt.plot(hist.history['val_acc'], color='r', label='validation_accuracy')\nplt.legend(loc='upper left')\nplt.show()","900c19b9":"# Pr\u00e9dictions et vecteur de probabilit\u00e9\nY_hat = classifier.predict(X_test)","1aa90bc6":"# G\u00e9n\u00e9ration des vecteurs de verit\u00e9 (Y_true) et de pr\u00e9diction (Y_pred)\nY_pred = np.argmax(Y_hat, axis=1)\nY_true = np.argmax(Y_val, axis=1)","053aa32d":"# Affichage des pr\u00e9dictions\ndisplay_image(X_test.reshape(-1, 784), Y_pred, n=10, label=True)","94ab4f81":"# ajout d'un bruit Gaussien\nX_train_noisy = X_train + NOISE_FACTOR * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\nX_val_noisy   = X_val + NOISE_FACTOR * np.random.normal(loc=0.0, scale=1.0, size=X_val.shape) \nX_test_noisy  = X_test + NOISE_FACTOR * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape) \n\n# limitation des valeurs entre 0 et 1 (l'ajout de bruit peut provoquer des d\u00e9passements)\nX_train_noisy = np.clip(X_train_noisy, 0., 1.)\nX_val_noisy   = np.clip(X_val_noisy, 0., 1.)\nX_test_noisy  = np.clip(X_test_noisy, 0., 1.)","0101a988":"# Visualisation Images bruit\u00e9es\ndisplay_image(X_val_noisy, None, n=10, label=False)","6423d0d4":"# Cr\u00e9ation de l'autoencoder\n\n# Cr\u00e9ation du format d'entr\u00e9e (Input Tensor)\ninput_img = Input(shape=(IMG_SIZE, IMG_SIZE, 1)) \n\n# 1 Encode\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\nencoded.shape\n\n\"\"\"\nA ce point les dimensions de l'image sont (7, 7, 32) => 1568 dimensions\nPour ce travail et pour am\u00e9liorer la reconstruction des images bruyantes nous gardons une \nhaute dimensionalit\u00e9\n\"\"\"\n\n# 2 Decode\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\n\n# Un autoencoder utilise la fonction sigmoid comme fonction d'activation\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\ndecoded.shape\n\n# Assemblage du modele\nautoencoder = Model(input_img, decoded)\n\n# Pour les autoencoders la fonction de cout peut \u00eatre soit binary_crossentropy soit rmse\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\nautoencoder.summary()","1c911711":"# Visualisation des images encod\u00e9es\nencoder = Model(input_img, encoded)\nencoded_imgs = encoder.predict(X_train)\nn = DISPLAY_IMG\nplt.figure(figsize=(20, 8))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(encoded_imgs[i].reshape(16, 2 * 49).T)\n    plt.title(\"Encoded: {}\".format(y_train[i]))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","37134070":"# Entrainement de l'autoencoder\nautoencoder.fit(X_train_noisy, X_train,\n                epochs=NB_EPOCHS_DENOISE,\n                batch_size=BATCH_SIZE,\n                shuffle=True,\n                validation_data=(X_val_noisy, X_val),            \n                )","31938900":"\"\"\" decodage et prediction\"\"\"\n# --- decodage des images bruit\u00e9es via le r\u00e9seau de neurones de classification\n# --- pr\u00e9diction de cat\u00e9gorie des images d\u00e9bruit\u00e9es\ndecoded_imgs = autoencoder.predict(X_test_noisy)\npredicted_decoded_digit = classifier.predict_classes(decoded_imgs)\n\n# -- affichage image originale\n# -- et de la reconstruction (debruitage)\nn = DISPLAY_IMG * 2\nplt.figure(figsize=(PLOT_SIZE))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(2, n, i+1)\n    plt.title(\"Pred: {}\".format(predicted_decoded_digit[i]))\n    plt.imshow(X_test_noisy[i].reshape(IMG_SIZE, IMG_SIZE))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    # display reconstruction\n    ax = plt.subplot(2, n, i+1 + n)\n    plt.imshow(decoded_imgs[i].reshape(IMG_SIZE, IMG_SIZE))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","b2559891":"### Entrainement de l'autoencodeur\n\nAvant de pouvoir utiliser ce r\u00e9seau il est n\u00e9cessaire d'effectuer son entrainement.\n\nDe m\u00eame qu'un r\u00e9seau classique il faut alimenter les donn\u00e9es en entr\u00e9e du r\u00e9seau et la sortie attendue.\nIl est important de noter que l'entr\u00e9e sont les images bruit\u00e9es et les sorties les images non bruit\u00e9es. \n\nCette combinaison doit permettre \u00e0 notre mod\u00e8le d'apprendre une fa\u00e7on de d\u00e9bruiter les images.","c09a1a29":"## Entrainement du Classifier","a3b8ecf6":"## D\u00e9finition et utilisation des autoencoders\n\n![image.png](attachment:image.png)\n\n**Qu'est-ce qu'un autoencoder ?**\n\nUn autoencoder est un mod\u00e8le de compression et de d\u00e9compression des donn\u00e9es. \n\nIl est **sp\u00e9cifique** aux donn\u00e9es sur lequels nous l'entrainons et il permet de r\u00e9aliser un apprentissage **non supervis\u00e9** des features de ces donn\u00e9es.\n\nIl est principalement utilis\u00e9 dans les domaines de **d\u00e9bruitage**, de **d\u00e9tection d'anomalie** ou de **r\u00e9duction de dimensionalit\u00e9**.\n\nComme ils ne sont pas supervis\u00e9s les autoencoders n'ont **pas besoin de donn\u00e9es \"labellis\u00e9es\"** pour fonctionner : ils utilisent les donn\u00e9es qui leur sont fournies en entr\u00e9e.\n\nLe principe de fonctionnement est simple : il faut trouver une fonction d'encodage puis de d\u00e9codage permettant de retrouver l'apparence de l'image originale (ou plus g\u00e9n\u00e9ralement de minimiser la fonction de cout entre les donn\u00e9es en entr\u00e9e et la reconstruction en sortie).\n\nLes autoencodeurs cherchent donc \u00e0 trouver une **repr\u00e9sentation pertinente et compress\u00e9e des donn\u00e9es** fournies en entr\u00e9e.\n\nUne fois la donn\u00e9es compress\u00e9es l'autoencodeur doit \u00eatre capable de reg\u00e9n\u00e9rer int\u00e9gralement les donn\u00e9es (pour cel\u00e0 il est n\u00e9cessaire que la compression permette de trouver les **caract\u00e9ristiques discriminantes** des donn\u00e9es d'entr\u00e9es)\n\nA noter que les autoencodeurs peuvent aussi s'appliquer \u00e0 des s\u00e9quences.\n\nCf image ci-dessus","36df7a6a":"## Ajout de Bruit Gaussien aux images\n\nPour permettre le d\u00e9bruitage des images et ainsi mettre en oeuvre notre autoencoder il faut d'abord **construire des images bruit\u00e9es**.\n\nNous allons donc **ajouter un bruit Gaussien** al\u00e9atoire sur les images.","dee72fcf":"# Cr\u00e9ation d'un Autoencodeur pour le D\u00e9bruitage d'images\n\nDans ce Tutorial nous allons voir comment cr\u00e9er des **autoencoders** et dans quel contexte nous pouvons les utiliser.\n\nNous irons \u00e9galement un peu plus loin que dans les chapitres pr\u00e9c\u00e9dents car nous allons cette fois-ci **combiner deux mod\u00e8les** de r\u00e9seaux neuronaux que nous allons entrainer s\u00e9parement: un classifier et un autoencoder. \n\nNous avons d\u00e9j\u00e0 abord\u00e9 le classifier dans les d\u00e9tails dans les chapitres pr\u00e9c\u00e9dents sur les r\u00e9seaux de convolutions, c'est donc plus particuli\u00e8rement sur l'**autoencoder** que nous allons axer ce tutorial.\n\nEncore une fois nous n'allons pas nous encombrer de tous les d\u00e9tails et calcul math\u00e9matiques de fonctions de co\u00fbts du mod\u00e8le et\/ou du calcul des descente de gradient :-).\n\nLe jeu de donn\u00e9es utilis\u00e9 sera celui que nous connaissons maintenant c'est \u00e0 dire celui du **MNIST sur les chiffres** .\n\nR\u00e9f\u00e9rences :\n\nhttps:\/\/arxiv.org\/pdf\/1708.08487.pdf\n\nhttps:\/\/blog.keras.io\/building-autoencoders-in-keras.html\n\nhttps:\/\/fr.wikipedia.org\/wiki\/Auto-encodeur","d237ba74":"Cel\u00e0 semble efficace ;-)\nMerci","7e39bd5c":"## Lecture des donn\u00e9es\n\nComme vu pr\u00e9c\u00e9dement nous allons utiliser les **librairies Pandas** pour la lecture de notre jeu de donn\u00e9es afin de cr\u00e9er les structures matricielles permettant leur traitement","ed33c282":"# Construction du Classifier\n\nComme vu dans les chapitres pr\u00e9c\u00e9dents nous allons construire un Classifier gr\u00e2ce \u00e0 un r\u00e9seau de Convolution (CNN).\n\nVolontairement nous allons construire un *r\u00e9seau tr\u00e9s simple* comme ce r\u00e9seau n'est pas l'objectif de notre tutorial.","561d1da9":"### Construction du mod\u00e8le\n\nNous allons maitenant passer \u00e0 l'impl\u00e9mentation du mod\u00e8le via les librairies **Keras.**","818fc849":"# Combinaison des mod\u00e8les\n\nNous allons maintenant regarder si la combinaison de nos mod\u00e8les fonctionne.\n\nDans un premier temps nous d\u00e9bruitons les images gr\u00e2ce \u00e0 notre autoencodeur et dans un second temps nous faisons une pr\u00e9diction de la classe de l'image.\n\nLes r\u00e9sultats ci-dessous:","259ae1ea":"# Autoencoder (denoiser)\n\nMaintenant que nous disposons d'images bruit\u00e9es nous allons cr\u00e9er notre autoencoder. Ce dernier est compos\u00e9 de deux fonctions : **l'encoder et le decodeur**.\n\nLa premi\u00e8re fonction encode les donn\u00e9es d'origine en une repr\u00e9sentation (g\u00e9n\u00e9ralement) de plus petite dimension, tandis que le r\u00e9seau de d\u00e9codeurs reconvertit cette repr\u00e9sentation dans l'espace de dimensions d'origine. \n\nL'id\u00e9e est que la repr\u00e9sentation en basse dimension pr\u00e9sente une r\u00e9duction de dimensionnalit\u00e9 non lin\u00e9aire des donn\u00e9es d'origine.\nDans notre cas pour am\u00e9liorer la reconstruction des images bruit\u00e9es nous n'allons pas faire une baisse de la dimensionalit\u00e9. \n\nNotre but est comme expliqu\u00e9 plus haut de trouver une fonction de compression permettant **l'extraction des caracteristiques discriminantes** de l'image en entr\u00e9e et ceci afin de permettre la reg\u00e9n\u00e9ration des donn\u00e9es \u00e0 partir de cette forme compress\u00e9e.\n\nComme nos entr\u00e9es sont des images nous allons utiliser des **r\u00e9seaux de convolutions** comme encodeurs et d\u00e9codeurs.\n\nCf repr\u00e9sentation ci-dessous\n\n\n![image.png](attachment:image.png)"}}