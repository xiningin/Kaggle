{"cell_type":{"a80c0f9a":"code","88afcd56":"code","6e42ac7d":"code","9a0123cb":"code","5282e08b":"code","c81cb5dd":"code","f4d45545":"code","5ca34866":"code","25ff69a6":"code","fb78f959":"code","81a886e0":"code","256d7849":"code","cfdf5c65":"code","081f69f3":"code","8a4d5d31":"code","aaeb24ad":"code","0aaa5e83":"code","999f21dd":"code","0a484c94":"code","9bcc70b5":"code","32bf0dca":"code","61ba3104":"code","981efff2":"code","8f0c8e5d":"code","6e99f045":"code","b49cd09e":"code","0b0c9bd2":"markdown","696934da":"markdown","08665634":"markdown","ce9d8b6b":"markdown","4b441a59":"markdown","d59c6b90":"markdown","ec138fc5":"markdown","8146ecec":"markdown","c42c8007":"markdown","2d414e3b":"markdown","d176743b":"markdown","78456ead":"markdown","e4ed4c04":"markdown","7874f608":"markdown","050c7440":"markdown","de41c384":"markdown","6571eeca":"markdown","ebf1bbee":"markdown","82fe8d92":"markdown","6ffdc096":"markdown","bb66ba0e":"markdown","4ee54056":"markdown","93354a7f":"markdown","1a1dd2fa":"markdown","21600378":"markdown","79c6590e":"markdown","27bd4146":"markdown","4a5b753f":"markdown","ec29df33":"markdown","458eda99":"markdown","493557fd":"markdown","3315d08a":"markdown"},"source":{"a80c0f9a":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix , accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nimport pandas.core.algorithms as algos\nfrom pandas import Series\nimport scipy.stats.stats as stats\nimport re\nimport traceback\nimport string\n\nimport warnings\n\nimport gc; gc.enable()\n\n%matplotlib inline\nsns.set(style=\"darkgrid\", color_codes=True, font_scale=1.3)\nwarnings.filterwarnings(\"ignore\")","88afcd56":"#read in data\ndf = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\", index_col = 0)\ndf.info()","6e42ac7d":"df.reset_index(inplace=True)\ndf.head()","9a0123cb":"df.shape","5282e08b":"df.isna().sum()","c81cb5dd":"#drop last column with no values\ndf.drop(\"Unnamed: 32\", axis = 1, inplace = True)","f4d45545":"#Setting our target and converting values to numeric\ndf['target'] = df['diagnosis'].apply(lambda x : 1 if x == 'M' else 0)\ndf.drop('diagnosis', axis = 1, inplace = True)\ndf.head()","5ca34866":"# visualize distribution of classes \nplt.figure(figsize=(10, 8))\nsns.countplot(df['target'], palette='RdBu')\n\n# count number of obvs in each class\nbenign, malignant = df['target'].value_counts()\nprint('Number of cells labeled Benign: ', benign)\nprint('Number of cells labeled Malignant : ', malignant)\nprint('')\nprint('% of cells labeled Benign', round(benign \/ len(df) * 100, 2), '%')\nprint('% of cells labeled Malignant', round(malignant \/ len(df) * 100, 2), '%')","25ff69a6":"target = 'target'\n_id = \"id\"\nused_cols = [col for col in df.columns.tolist() if col not in [target, _id]]\nX = df[used_cols].copy()\ny = df[target].copy()\ndel df\ngc.collect()","fb78f959":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)","81a886e0":"max_bin = 20\nforce_bin = 3\n\n# define a binning function\ndef mono_bin(Y, X, n = max_bin):\n    \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]\n    r = 0\n    while np.abs(r) < 1:\n        try:\n            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n            d2 = d1.groupby('Bucket', as_index=True)\n            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n            n = n - 1 \n        except Exception as e:\n            n = n - 1\n\n    if len(d2) == 1:\n        n = force_bin         \n        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n        if len(np.unique(bins)) == 2:\n            bins = np.insert(bins, 0, 1)\n            bins[1] = bins[1]-(bins[1]\/2)\n        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n        d2 = d1.groupby('Bucket', as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"MIN_VALUE\"] = d2.min().X\n    d3[\"MAX_VALUE\"] = d2.max().X\n    d3[\"COUNT\"] = d2.count().Y\n    d3[\"EVENT\"] = d2.sum().Y\n    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n    d3=d3.reset_index(drop=True)\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    \n    return(d3)\n\ndef char_bin(Y, X):\n        \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]    \n    df2 = notmiss.groupby('X',as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"COUNT\"] = df2.count().Y\n    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n    d3[\"EVENT\"] = df2.sum().Y\n    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    d3 = d3.reset_index(drop=True)\n    \n    return(d3)\n\ndef data_vars(df1, target):\n    \n    stack = traceback.extract_stack()\n    filename, lineno, function_name, code = stack[-2]\n    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n    \n    x = df1.dtypes.index\n    count = -1\n    \n    for i in x:\n        if i.upper() not in (final.upper()):\n            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n                conv = mono_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i\n                count = count + 1\n            else:\n                conv = char_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i            \n                count = count + 1\n                \n            if count == 0:\n                iv_df = conv\n            else:\n                iv_df = iv_df.append(conv,ignore_index=True)\n    \n    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n    iv = iv.reset_index()\n    return(iv_df,iv)","256d7849":"final_iv, df_IV = data_vars(X_train,y_train)","cfdf5c65":"final_iv.sort_values(\"IV\").head()","081f69f3":"df_IV.loc[df_IV['IV'] < .02].sort_values(\"IV\")\n","8a4d5d31":"useless_ = df_IV.loc[df_IV['IV'] < .02]['VAR_NAME'].tolist()","aaeb24ad":"df_IV.loc[(df_IV['IV']>.02) & (df_IV['IV']<.1)].sort_values(\"IV\")","0aaa5e83":"df_IV.loc[(df_IV['IV']>.1) & (df_IV['IV']<.3)].sort_values(\"IV\")","999f21dd":"df_IV.loc[(df_IV['IV']>.3) & (df_IV['IV']<.5)].sort_values(\"IV\")","0a484c94":"df_IV.loc[df_IV['IV']>.5].sort_values(\"IV\")","9bcc70b5":"#function that uses our information determined form WOE and applies it to our data\ndef transform_(df, transform_vars_list, final_iv):\n    for var in transform_vars_list:\n        small_df = final_iv[final_iv['VAR_NAME'] == var]\n        transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n        replace_cmd = ''\n        replace_cmd1 = ''\n        for i in sorted(transform_dict.items()):\n            replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n            replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n        replace_cmd = replace_cmd + '0'\n        replace_cmd1 = replace_cmd1 + '0'\n        if replace_cmd != '0':\n            try:\n                df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd))\n            except:\n                df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd1))\n    return df","32bf0dca":"#dropping useless predictors\nkeep_cols = [col for col in used_cols if col not in useless_]","61ba3104":"transform_prefix = ''\nX_train = transform_(X_train, keep_cols, final_iv)","981efff2":"logreg = LogisticRegression(fit_intercept = False, class_weight = 'balanced', C = 1e15)\nmodel_log = logreg.fit(X_train, y_train)","8f0c8e5d":"X_test = transform_(X_test, keep_cols, final_iv)","6e99f045":"y_pred = model_log.predict(X_test)","b49cd09e":"print(confusion_matrix(y_test,y_pred))  \nprint(classification_report(y_test,y_pred)) \nprint(\"The accuracy score is\" + \" \"+ str(accuracy_score(y_test, y_pred)))","0b0c9bd2":"## Training Model","696934da":"<a href=\"#top\">Back to Top<\/a>","08665634":"<a href=\"#top\">Back to Top<\/a>","ce9d8b6b":"## Load in Data","4b441a59":"## Import Packages","d59c6b90":"<a href=\"#top\">Back to Top<\/a>","ec138fc5":"## EDA","8146ecec":"The purpose of this notebook is to show how to use Weight of Evidence (WOE) and Information Value (IV) to predict whether breast tissues are benign or malignant using the Breast Cancer Wisconsin (Diagnostic) Data Set.  \n\nSince this is a binary classification problem, Logisitic Regression will be used as the classifier. \n\nAll information included in this notebook on WOE and IV can be found here: [Weight of evidence and Information Value using Python](https:\/\/medium.com\/@sundarstyles89\/weight-of-evidence-and-information-value-using-python-6f05072e83eb)\n\n<a href=\"#top\">Back to Top<\/a>","c42c8007":"<a href=\"#top\">Back to Top<\/a>","2d414e3b":"We can see from these metrics that using WOE and IV with Logistic Regression can be a strong technique to use in solving binary classification problems.","d176743b":"## Conclusion","78456ead":"From the reference material an IV less than 0.02 is characterized as a useless predictor","e4ed4c04":"An IV between 0.3 to 0.5 is characterized as a strong predictor","7874f608":"## WOE and IV\nref : [Weight of evidence and Information Value using Python](https:\/\/medium.com\/@sundarstyles89\/weight-of-evidence-and-information-value-using-python-6f05072e83eb)","050c7440":"### Weight of Evidence","de41c384":"# Identifying Breast Cancer using Logistic Regression with Weight of Evidence","6571eeca":"<a href=\"#top\">Back to Top<\/a>","ebf1bbee":"![Weight Of Evidence](https:\/\/miro.medium.com\/max\/768\/1*6Aw782wiyiFtzvK7EOY8CA.png)\n![Information Value](https:\/\/miro.medium.com\/max\/1400\/1*xWA7a2KsTQOhaQ9MZFJJeQ.png)","82fe8d92":"An IV is greater than 0.5 is characterized as suspicious or too good to be true.","6ffdc096":"<a href=\"#top\">Back to Top<\/a>","bb66ba0e":"<a href=\"#top\">Back to Top<\/a>","4ee54056":"An IV between 0.02 to 0.1 is characterized as a weak predictor","93354a7f":"<a href=\"#top\">Back to Top<\/a>","1a1dd2fa":"Let me know if you have any feedback. ","21600378":"More information on Weight of Evidence and Information Value used for this can be found in the following Notebooks:\n* [Weight of Evidence(WOE) & Information Value(IV)](https:\/\/www.kaggle.com\/pavansanagapati\/weight-of-evidence-woe-information-value-iv)\n* [IV + WoE Starter for Python](https:\/\/www.kaggle.com\/puremath86\/iv-woe-starter-for-python)","79c6590e":"<a href=\"#top\">Back to Top<\/a>","27bd4146":"## Encoding Data","4a5b753f":"## Introduction","ec29df33":"* [Introduction](#Introduction)\n* [Import Packages](#Import-Packages)\n* [Load in Data](#Load-in-Data)\n* [EDA](#EDA)\n* [WOE and IV](#WOE-and-IV)\n    * [Weight of Evidence](#Weight-of-Evidence)\n    * [Information Value](#Information-Value)\n* [Encoding Data](#Encoding-Data)\n* [Training Model](#Training-Model)\n* [Testing and Checking Metrics](#Testing-and-Checking-Metrics)\n* [Conclusion](#Conclusion)","458eda99":"### Information Value","493557fd":"An IV between 0.1 to 0.3 is characterized as a medium predictor","3315d08a":"<a href=\"#top\">Back to Top<\/a>"}}