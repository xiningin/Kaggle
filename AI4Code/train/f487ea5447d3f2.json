{"cell_type":{"4876c94a":"code","c1d7104a":"code","68cab179":"code","25b83b67":"code","ae6a67e7":"code","176246ac":"code","29ee856a":"code","4c186ae4":"code","27209c8b":"code","c54fc56b":"code","9822f711":"code","4e51a8de":"code","ab0dfaff":"code","5ed3eab1":"code","4839d2fc":"code","1d12e8d5":"code","455dcdd1":"code","ebf52e9a":"code","9848cda2":"code","041a9560":"code","5e04efdd":"code","75f45bd5":"code","c5185d14":"code","95d6a04a":"code","b99f3d46":"code","5b4a3fab":"code","57825e54":"code","347adc3f":"code","02794590":"code","fd698bae":"code","13c14d13":"code","3032a6e9":"code","6c552418":"code","402fd35a":"code","800402cb":"code","9073a32f":"code","f21c51ab":"code","8acc0b35":"code","9cd99207":"code","ae7211d4":"code","397ca601":"code","5fbc8f59":"code","a2f7df50":"code","9608c0ce":"code","809e29fd":"code","10f90ea3":"code","d1e088ce":"code","9221b1ad":"code","0ed4f0c4":"code","3415d861":"code","41898af5":"code","06246cbb":"code","93fd1916":"code","e9196d42":"code","89d05a65":"code","33239418":"code","70b47fea":"code","f3818905":"code","beede251":"code","2a8eba4d":"markdown","e9a5e448":"markdown","968e5ae0":"markdown","c11fbf77":"markdown","acf63f4a":"markdown","47b3afd2":"markdown"},"source":{"4876c94a":"# importing our modules into google colab\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n%pylab inline\n","c1d7104a":"# importing our dataset from gdrive\nhouseData=pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")\nhouseData.head()\ninitial_zip=houseData['zipcode']  # initial data series for zipcode as in csv","68cab179":"# checking the houseData\nhouseData.describe().head()","25b83b67":"#checking data for null values\nhouseData.isnull().sum()   #no null values ","ae6a67e7":"\n#checking\/deleting for duplicated values in data\nhouseData.drop_duplicates()","176246ac":"\n#reaffirming the cleaned duplicates\nhouseData.duplicated().sum() #0 nil","29ee856a":"#data information\nhouseData.info()","4c186ae4":"\nhouseData['bedrooms'].describe()","27209c8b":"#changing the date format to its required format\nhouseData[\"date\"]=pd.to_datetime(houseData[\"date\"]) # changes the data type to datetime format\nhouseData['date']","c54fc56b":"houseData.head(3)","9822f711":"\n# getting to know the location by their zip code \n#checking the distribution of the zipcode\/date data either even or uneven\nhouseData['date'].unique()\nplt.figure(figsize=[7,7],)\nsns.histplot(x=houseData['date'].unique())\n\n\nhouseData['zipcode'].unique()\nplt.figure(figsize=[10,7])\nsns.displot(x=houseData['zipcode'].unique())","4e51a8de":"# on column zip code \nhouseData['zipcode'].unique().max() # 98199\nhouseData['zipcode'].unique().min() # 98001\nhouseData['zipcode'].unique()","ab0dfaff":"# installing library on google colab\n!pip install pgeocode   #---installed already\n","5ed3eab1":"# to change zip data to locations\nimport pgeocode \nzip_code=[]\nzipcode=houseData['zipcode'].unique()\nfor i in range(len(zipcode)): # converting the zipcode to str data\n  code=str(zipcode[i])\n  zip_code.append(code)\nzip_code\n#for more info\n#http:\/\/www.geonames.org\/postalcode-search.html?q=98199&country=US\nnomi=pgeocode.Nominatim('us')\nzipcode_=nomi.query_postal_code(zip_code)\nzipcode_  # dataframed column zip code\n\n","4839d2fc":"#scraping zipcode_\nzipcode_.head()","1d12e8d5":"#scraping of zipcode_\nzipcode_.duplicated().sum()#0\nzipcode_.isna().sum() #community_name 69 community_code    69\n","455dcdd1":"zipcode_.drop(['community_name','community_code'],axis=1,inplace=True)\nzipcode_.head()","ebf52e9a":"#rechecking zip code if all are Washington ,visit the link below for verification\n#http:\/\/www.geonames.org\/postalcode-search.html?q=98199&country=US\nzip_code=[]\nall(zipcode_['state_name'])=='Washington' #True ","9848cda2":"houseData.head(2) #rechecking house dataset","041a9560":"# to replace data in houseData(zipcode) with zipcode_[place_name\tstate_name]\nhouseData['zipcode'].unique()  # houseData(zipcode) zipcode data\nzipcode_['place_name']   # zipcode_[place_name] all(state_name)='Washington'\nhouseData.replace(houseData['zipcode'].unique(),zipcode_['place_name'],inplace=True)\n","5e04efdd":"#review dataset\nhouseData.head()","75f45bd5":"houseData.tail(2)","c5185d14":"initial_zip.head(2)  # from csv","95d6a04a":"final_zip=houseData['zipcode']\nfinal_zip.head(2)","b99f3d46":"houseData.head(2)","5b4a3fab":"\n#changing the column name(zipcode=Location)\nnew_columns=['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'Location',\n       'lat', 'long', 'sqft_living15', 'sqft_lot15']\nnew_columns1=['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'Location','State Name','zip code',\n       'lat', 'long', 'sqft_living15', 'sqft_lot15']\nhouseData=pd.DataFrame(houseData,columns=new_columns)\nhouseData['Location']=houseData['Location'].fillna(final_zip) #filling up Nan with its code name \nhouseData['zip code']=initial_zip  # adding another column for zipcode\nhouseData['State Name']='Washington' # adding another column for the state_name\nhouseData['date']=pd.to_datetime(houseData['date'])\nhouseData[new_columns1]\nhouseData.head()","57825e54":"houseData.isna().sum() #0\nhouseData.info()","347adc3f":"#Distribution of house prices\nfig=plt.figure(figsize=[5,5])\nsns.distplot(houseData['price'])  # outliers present\nprint('       Distribution of Prices')\n","02794590":"# No of bedrooms and Bathrooms\nfig=plt.figure(figsize=[7,7])\nsns.countplot(x=houseData['bedrooms'],data=houseData) # outliers present","fd698bae":"plt.figure(figsize=[10,10])\nsns.countplot(x=houseData['bathrooms'],data=houseData)  # outliers present\n","13c14d13":"# Price as related to bedrooms\/bathrooms\nplt.figure(figsize=[7,7]).add_subplot(211).set_title('No Of Bedrooms')\nsns.lineplot(x=houseData['bedrooms'],y=houseData['price'],data=houseData,legend='full')\nplt.figure(figsize=[7,7]).add_subplot(212).set_title('No Of Bathrooms')\nsns.lineplot(x=houseData['bathrooms'],y=houseData['price'],data=houseData,legend='full',color='r')\n","3032a6e9":"# Price as related to locations per year\nfig=plt.figure(figsize=[10,10])\nfig.add_subplot(121).set_title(\"Concentration of Houses per Location\",fontsize=15)\nhouseData['Location'].value_counts().plot()\nfig.add_subplot(122).set_title('Distribution of the house prices in Washington',fontsize=15,loc='center')\nsns.distplot(x=houseData['price'])","6c552418":"#visualizing column and row correlation\nsns.set()\nplt.figure(figsize=(20,10))\nsns.heatmap(houseData.corr(),annot=True,fmt='.2f',cmap='OrRd')\nplt.show()","402fd35a":"cor=houseData.corr()\ncor['price']\n# price will be compared with grade&sqft_above with over 60% correlation","800402cb":"print('Price as dependent on the Grade and Sqft Above'.upper())\nsns.set()\nlabels={'title': 'Price Vs Grade',\n        'title2': 'Price Versus Sqt_Above'\n    \n}\nax1=plt.figure(figsize=[20,7]).add_subplot(211)\nsns.pointplot(x=houseData['grade'],y=houseData['price'],hue=houseData['Location'],data=houseData)\nax1.set_title(labels['title'],fontsize=12,color='m',loc='center')\nax=plt.figure(figsize=[20,7]).add_subplot(212)\nsns.pointplot(x=houseData['sqft_above'],y=houseData['price'],hue=houseData['Location'],data=houseData)\nax.set_title(labels['title2'],fontsize=12,color='m',loc='center')\n\nplt.show()\n  ","9073a32f":"houseData['Location'].value_counts()","f21c51ab":"houseData['year']=houseData['date'].apply(lambda date:date.year)\nhouseData['month']=houseData['date'].apply(lambda date:date.month)","8acc0b35":"#picking the location with the highest data counts\nsns.set()\nseattle_cp=houseData[houseData['Location']=='Seattle']['price']\nrenton_cp=houseData[houseData['Location']=='Renton']['price']\nbellevue_cp=houseData[houseData['Location']=='Bellevue']['price']\nkent_cp=houseData[houseData['Location']=='Kent']['price']\ncol=[seattle_cp,renton_cp,bellevue_cp,kent_cp]\nnam=['Seattle','Renton','Bellevue','Kent']\nplt.figure(figsize=[10,10])\nfor i,dat in enumerate(col):\n  ax=plt.subplot(2,2,i+1)\n  sns.barplot(x=houseData['year'],y=col[i],data=houseData,color='r')\n  ax.set_title(f'Price of Houses in {nam[i].upper()} Per Year',fontsize=15,color='m',loc='center')\n  plt.tight_layout\n\nplt.show()\n","9cd99207":"# for the price per year and location in view\nsns.set()\nplt.figure(figsize=[20,10]).add_subplot(121).set_title('House Price Per Year'.upper())\nsns.boxplot(x=houseData['year'],y=houseData['price'],data=houseData)\nplt.figure(figsize=[20,10]).add_subplot(122)\nsns.barplot(x=houseData['year'],y=houseData['price'],hue=houseData['Location'],data=houseData,saturation=0.5)\nplt.tight_layout\nplt.show()\n\n#calculating the difference in price change btw 2014 and 2015\n\n\n\n","ae7211d4":"p_2014=houseData[houseData['year']==2014]\np_2014=p_2014['price'].sum()\np_2015=houseData[houseData['year']==2015]\np_2015=p_2015['price'].sum()\nprint('Increment in house prices from 2014 to 2015 is :', ((p_2014-p_2015)\/p_2014)*100)\n","397ca601":"#lat and #longitude relationship\nplt.figure(figsize=[5,5],frameon=False)\nsns.scatterplot(x=houseData['lat'],y=houseData['long'],hue=houseData['price'],data=houseData,color='m')\n# insights : a particular location within a city is more considered","5fbc8f59":"houseData.columns","a2f7df50":"# relationship between the square fit and price\nsns.regplot(x=houseData['sqft_living'],y=houseData['price'],data=houseData,color='g')","9608c0ce":"houseData.head()","809e29fd":"y=houseData.price\nX=houseData.drop(columns=['date','id','price','sqft_lot15','Location','State Name'])","10f90ea3":"X.head()","d1e088ce":"X.head(2)","9221b1ad":"y","0ed4f0c4":"# Splitting and scaling the data into training and testing dataset\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)","3415d861":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","41898af5":"[X_train.shape ,X_test.shape]","06246cbb":"#TENSOR FLOW\nfrom tensorflow.keras import Sequential #basic feed forward model\nfrom tensorflow.keras.layers import Dense\nmodel=Sequential()\nmodel.add(Dense(19,activation='relu'))\nmodel.add(Dense(19,activation='relu'))\nmodel.add(Dense(19,activation='relu'))\nmodel.add(Dense(19,activation='relu'))\n\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam',loss='mse')  #builds the ,model for the first time","93fd1916":"from tensorflow.keras.callbacks import EarlyStopping\nearlystop=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=25)","e9196d42":"start_time=time.time()\nmodel.fit(x=X_train,y=y_train,validation_data=(X_test,y_test),callbacks=earlystop,batch_size=128,epochs=1000) #fitting the models to our variables with 1000 iterations ## train the model\nend_time=time.time()\nprint('Cell\/Model Runtime: ',end_time-start_time,end=' ')","89d05a65":"losses=pd.DataFrame(model.history.history)\nlosses.plot()","33239418":"prediction=model.predict(X_test)  #make predictions with zero losses\nprediction","70b47fea":"# Evaluating the model by scoring\n\nfrom sklearn.metrics import explained_variance_score,mean_absolute_error,confusion_matrix,accuracy_score\n#explained_variance_score\nprint('explained_variance_score : '.upper(),end='\\033[94m{}\\033[00m'.format(explained_variance_score(y_test,prediction)))\nprint('\\r')\n#mean_absolute_error\nprint('mean_absolute_error :'.upper(),end=' \\033[96m{}\\033[00m'.format(round(mean_absolute_error(y_test,prediction),2)))","f3818905":"prediction","beede251":"#the accuracy of the dataset tested and good","2a8eba4d":"INSIGHTS\n\n*   The price of house in Washington DC,USA generally decreased  by 50% in 2015\n\n","e9a5e448":"FEATURE ENGINEERING","968e5ae0":"DATA CLEANING","c11fbf77":"INSIGHTS\n1.   The grade has a high correlation with house price as price increases with the grade\n2.   Also people tend to buy houses in Seattle more than other locations\n\n\n\n","acf63f4a":"### houseData VISUALIZATION\n\n*  Distribution of house prices\n*  No of bedrooms and Bathrooms\n*  Price as related to bedrooms\/bathrooms\n*  Price as related to locations per year \n\n\n\n","47b3afd2":"From the above we can deduce that houses are more sold in seattle as compared to other locations within Washington,USA between 2014 and 2015"}}