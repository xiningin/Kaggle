{"cell_type":{"873121cf":"code","c0fc3580":"code","20125f6c":"code","7d7edd4b":"code","fec1d9bd":"code","7fbcb4b0":"code","1b893dcd":"code","cd3a87c2":"code","a9d86ceb":"code","4db40f37":"code","00eeac0c":"code","3e16d172":"code","2c564baf":"code","4e49083c":"code","1b931b46":"code","40105258":"code","29d05805":"code","f22181b8":"code","3b02ccce":"code","2510be3a":"code","85c4622f":"code","d4b893c5":"code","80ec8880":"code","84db6647":"code","9e8f60da":"code","c49d9b5a":"code","4a3efb1d":"code","0c5ae6dd":"code","8f7fd1f1":"code","191049b4":"code","cc7d06cb":"code","5cb1c0b8":"code","b0bf363d":"code","b3298a90":"code","148315ea":"code","85267c17":"code","8aacb66e":"code","4f230a5c":"code","a8063a31":"code","2b6f8972":"code","c5454699":"code","bea0c908":"code","7ffbb033":"code","fe8a54d1":"code","0492baca":"code","8a7ab9f9":"code","642fef58":"code","fb46099b":"code","5cc5bf5e":"code","58b168d8":"code","b5b2f303":"code","686b22a3":"code","82a5a7c8":"code","9ebe1ce6":"code","2859115c":"code","3ca2ed82":"code","063fca98":"code","b0a04cf5":"code","42e57d9a":"code","18abf3da":"code","c0b7dc4e":"code","cd065849":"code","186236ad":"code","400c6214":"code","14d94342":"code","bf80af16":"code","df6ced45":"code","338859c5":"code","890c527b":"code","a745cd91":"code","5bcd9fa2":"code","82a40f74":"markdown","23876ceb":"markdown","69271fa6":"markdown","578be289":"markdown","92199a07":"markdown","7d6dafa7":"markdown","992ed238":"markdown","be5caa46":"markdown","2d199702":"markdown"},"source":{"873121cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c0fc3580":"df = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.head()","20125f6c":"df['Churn'].value_counts(normalize = True)","7d7edd4b":"#df.describe\n#df.sum\ndf.isna().sum()","fec1d9bd":"df.describe()","7fbcb4b0":"df.shape","1b893dcd":"sns.countplot(df['gender'])","cd3a87c2":"sns.catplot(y=\"Churn\", kind=\"count\", data=df, height=2.6, aspect=2.5, orient='h')","a9d86ceb":"sns.countplot(df['Churn'])","4db40f37":"#MonthlyCarges and Contract\navg=(\n    df.\n     groupby(['Contract', 'Churn'])['MonthlyCharges'].\n     sum().\n    reset_index().\n     sort_values(by = 'MonthlyCharges',\n                ascending = False))\ntable = avg.pivot(index='Contract',\n                 columns='Churn',\n                 values = 'MonthlyCharges')\ntable","00eeac0c":"x = table.index\nwidth = 0.35\nfor col in table.columns:\n    plt.bar(x,table[col], width, label=col)\n    plt.title('Churn per contract type')\n    #plt.xticks(rotation=90)\nplt.legend()\nplt.show()","3e16d172":"#MonthlyCarges and Gender\n#, 'Churn'\navgg=(\n    df.\n     groupby(['gender', 'Churn'])['MonthlyCharges'].\n    sum().#.agg({'MonthlyCharges':np.mean}).\n    reset_index().\n     sort_values(by = 'MonthlyCharges',\n                ascending = False))\navgg\n","2c564baf":"tableg = avgg.pivot(index='gender',\n                 columns='Churn',\n                 values = 'MonthlyCharges')\ntableg","4e49083c":"xg = tableg.index\nfor col in tableg.columns:\n    plt.bar(xg,tableg[col], label=col)\n    plt.title('Churn per gender')\n    #plt.xticks(rotation=90)\nplt.legend()\nplt.show()","1b931b46":"sns.distplot(df['tenure'])\n","40105258":"sns.distplot(df['MonthlyCharges'])","29d05805":"for col in df.columns:\n    print('{} unique element: {}'.format(col,df[col].nunique()))","f22181b8":"def barplot_percentages(feature, orient='v', axis_name=\"percentage of customers\"):\n    ratios = pd.DataFrame()\n    g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n    g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n    g[axis_name] = g[axis_name]\/len(df)\n    if orient == 'v':\n        ax = sns.barplot(x=feature, y= axis_name, hue='Churn', data=g, orient=orient)\n        ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])\n    else:\n        ax = sns.barplot(x= axis_name, y=feature, hue='Churn', data=g, orient=orient)\n        ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n    ax.plot()\nbarplot_percentages(\"SeniorCitizen\")","3b02ccce":"df['churn_rate'] = df['Churn'].replace(\"No\", 0).replace(\"Yes\", 1)\ng = sns.FacetGrid(df, col=\"SeniorCitizen\", height=4, aspect=.9)\nax = g.map(sns.barplot, \"gender\", \"churn_rate\", palette = \"Blues_d\", order= ['Female', 'Male'])","2510be3a":"df2 = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf2.head()","85c4622f":"df.isna().any()","d4b893c5":"df['churn_rate'].value_counts()","80ec8880":"categorylist = ['gender',\n'Partner',\n'Dependents',\n'PhoneService',\n'MultipleLines',\n'InternetService',\n'OnlineSecurity',\n'OnlineBackup',\n'DeviceProtection',\n'TechSupport',\n'StreamingTV',\n'StreamingMovies',\n'Contract',\n'PaperlessBilling',\n'PaymentMethod']\ndata = pd.get_dummies(df,columns=categorylist)\ndata.shape","84db6647":"data.head()","9e8f60da":"data.sample(frac=.1).plot('tenure','MonthlyCharges', subplots=True, kind='scatter')","c49d9b5a":"# Installing more packages\ndata = data._get_numeric_data()","4a3efb1d":"data.head()","0c5ae6dd":"corr = data.corr()\ncorr.style.background_gradient()","8f7fd1f1":"from sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import train_test_split","191049b4":"train, test = train_test_split(data, test_size = 0.1)\ntrain.shape, test.shape","cc7d06cb":"test.head()","5cb1c0b8":"train['churn_rate'].value_counts(normalize = True)","b0bf363d":"test['churn_rate'].value_counts(normalize = True)","b3298a90":"x_test = test.drop(['churn_rate'], axis=1)\ny_test = test['churn_rate']","148315ea":"x_test.head()","85267c17":"y_test.head()","8aacb66e":"x = train.drop(['churn_rate'], axis=1)\ny = train['churn_rate']","4f230a5c":"forest = ExtraTreesClassifier(n_estimators=100,\n                              random_state=0)\n\nforest.fit(x, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(figsize=(8, 6))\n\nplt.title(\"Feature importances\")\nplt.bar(range(x.shape[1]), importances[indices],\n       color=\"b\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x.shape[1]), indices)\nplt.xlim([-1, x.shape[1]])","a8063a31":"imp_features = []\nfor i in indices:\n    imp_features.append(\"var_\"+str(i))","2b6f8972":"imp_features[:20]","c5454699":"# Plot the TOP 20 feature importances of the forest\n\nplt.figure(figsize=(8, 6))\n\nplt.title(\"Feature importances\")\nplt.bar(range(x.shape[1]), importances[indices],\n       color=\"b\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x.shape[1]), indices)\nplt.xlim([-1, 19])","bea0c908":"std","7ffbb033":"indices","fe8a54d1":"feature_rank = list()\nfor i in range(1,len(indices)):\n    feature_rank.append(x.columns[i])\n\nprint(feature_rank)","0492baca":"x.columns","8a7ab9f9":"drop = ['gender_Male', 'Partner_No', 'Dependents_No', 'PhoneService_No', 'PaperlessBilling_No'] \nx = x.drop(drop,axis=1)","642fef58":"x.head()","fb46099b":"forest = ExtraTreesClassifier(n_estimators=100,\n                              random_state=0)\n\nforest.fit(x, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(figsize=(8, 6))\n\nplt.title(\"Feature importances\")\nplt.bar(range(x.shape[1]), importances[indices],\n       color=\"b\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x.shape[1]), indices)\nplt.xlim([-1, x.shape[1]])","5cc5bf5e":"feature_rank = list()\nfor i in range(1,len(indices)):\n    feature_rank.append(x.columns[i])\n\nprint(feature_rank)","58b168d8":"# Plot the TOP 20 feature importances of the forest\n\nplt.figure(figsize=(8, 6))\n\nplt.title(\"Feature importances\")\nplt.bar(range(x.shape[1]), importances[indices], \n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x.shape[1]), feature_rank, rotation=90)\nplt.xlim([-1, 19])","b5b2f303":"# Plot the TOP 20 feature importances of the forest\n\nplt.figure(figsize=(8, 6))\n\nplt.title(\"Feature importances\")\nplt.bar(range(x.shape[1]), importances[indices], \n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x.shape[1]), feature_rank, rotation=90)\n#plt.xlim([-1, 19])","686b22a3":"train, test = train_test_split(data, test_size = 0.1)\ntrain.shape, test.shape","82a5a7c8":"x_train = train.drop(['churn_rate'], axis=1)\ny_train = train['churn_rate']","9ebe1ce6":"x_train.shape, y_train.shape, x_test.shape, y_test.shape","2859115c":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(x_train, y_train)","3ca2ed82":"# Predicting the Test set results\ny_pred = classifier.predict(x_test)","063fca98":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","b0a04cf5":"#Confusion Matrix Graph\nfrom mlxtend.plotting import plot_confusion_matrix\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()","42e57d9a":"#Making the Confusion Matrix\n# confusion_matrix is a function\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import cohen_kappa_score\nfrom sklearn.metrics import precision_recall_fscore_support\ncm = confusion_matrix(y_test,y_pred)\nk_stat = cohen_kappa_score(y_test,y_pred)\nscore = precision_recall_fscore_support(y_test,y_pred)","18abf3da":"k_stat","c0b7dc4e":"score","cd065849":"roc_auc_score(y_test, y_pred)","186236ad":"#Applying k-fold crossvalidation\nfrom sklearn.model_selection import cross_val_score, cross_validate\naccuracies = cross_val_score(estimator = classifier,X = x_train, y = y_train, cv = 10)\nstats = cross_validate(estimator = classifier,X = x_train, y = y_train, cv = 10)\nmean_ac= accuracies.mean()\nstd_ac= accuracies.std()","400c6214":"print(stats)\nprint(accuracies)\nprint(mean_ac)\nprint(std_ac)","14d94342":"sns.countplot(y_pred)\n","bf80af16":"sns.countplot(y_train)","df6ced45":"import collections\ncollections.Counter(y_pred),collections.Counter(y_train)","338859c5":"#Performance\n#Applying Grid Search to find the best model and best parameters\nfrom sklearn.model_selection import GridSearchCV\n# Include in the dictionaries the parameters we want to optimize\nparameters = [ {'criterion':['entropy'],'min_impurity_decrease':[0.01,0.001,0.005,0.05],'min_impurity_split':[0.05,0.01,0.001,0.0001,0.0000001]}]\ngrid_search = GridSearchCV(estimator = classifier, param_grid = parameters,scoring='accuracy',cv=10,n_jobs=-1)\ngrid_search = grid_search.fit(x_train,y_train)\nbest_accuracy = grid_search.best_score_\nbest_parmeters = grid_search.best_params_","890c527b":"grid_search","a745cd91":"best_accuracy","5bcd9fa2":"best_parmeters","82a40f74":"# 2. Feature Importance\n","23876ceb":"Repeat the tree by dropping redundant columns","69271fa6":"## 1.2 Encoding\nThe encoding can be used to normalise labels, as we have many categorical data this is used to convert the data\n* Label encoding: Coverts each value in a column to a number.\u00a0disadvantage that the numeric values can be \u201cmisinterpreted\u201d\u00a0.\n* One hot encoding: Despite the different names, the basic strategy is to convert each category value into a new column and assigns a 1 or 0 (True\/False) value to the column. This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data\u00a0set. \n* Custom Binary Encoding: Use some combination of label encoding and one hot encoding to create a binary column that meets your needs for further\u00a0analysis.\n","578be289":"# Excersice on Churn prediction\n#### Data base\nThe Dataset contains costumer data including attributes such as:\n1. Customers who left within the last month \u2013 the column is called Churn\n2. Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n3. Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n4. Demographic info about customers \u2013 gender, age range, and if they have partners and dependents\n\nThe following excercise is divided in 3 parts, exploratory analysis, metodology analysis and results.","92199a07":"**1.2 Summarising statistics**","7d6dafa7":"**1.3. Descriptive variables**","992ed238":"# 1. Exploratory analysis\n","be5caa46":"**1.1 Finding missing values**","2d199702":"# 3. Analysis\n## 3.1 Random Forest\n"}}