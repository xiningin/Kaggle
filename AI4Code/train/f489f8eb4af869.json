{"cell_type":{"2a02fd0d":"code","0478b3ef":"code","eb9e0abf":"code","8a9f580f":"code","607d542c":"code","ead7dae7":"code","ef4c59f3":"code","bc85f953":"code","5019469a":"code","c956919d":"code","1378c9fd":"code","bf085d47":"code","86a27197":"code","d61b2841":"code","0323748e":"code","d41417c8":"code","ac4e1e83":"code","88747cd6":"code","d4db54cd":"code","bab1b638":"code","a4994504":"code","15264708":"code","e29a52ef":"code","c6c14338":"code","cd322c62":"code","220d7d19":"code","1f2e0e36":"code","787b2755":"code","9010f51d":"code","b98432a3":"code","7e44d79d":"code","a10dd30f":"code","79566dbe":"code","b98abd45":"code","a951a7b1":"code","5d9e32fa":"code","31987488":"code","2d6b0366":"code","610a4cd5":"code","be5f7ba6":"markdown","39ebe57c":"markdown","8a5093dc":"markdown","94eae1a6":"markdown","19631af1":"markdown","37b5637c":"markdown","7616ab18":"markdown","316b9a97":"markdown","5f31d509":"markdown","669d1a72":"markdown","4af31d2a":"markdown","03845075":"markdown","e2126eb2":"markdown","96b9abe4":"markdown","7d7e1bec":"markdown","d0fc4259":"markdown","ac909c64":"markdown","a9e7b575":"markdown","afb18655":"markdown","dbf71aca":"markdown","5090f9b6":"markdown"},"source":{"2a02fd0d":"# All modules and packages are imported at the start of the notebook\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost\nfrom xgboost import XGBRegressor\nimport lightgbm\nfrom lightgbm import LGBMRegressor\n\n# define constants we'll use in the modelling and validation stages.\nRANDOM_SEED = 42 #ensure consistency of tests\nCV_FOLDS = 12","0478b3ef":"# The accompanying data_description.txt file indicates the type of each of the test and training data features.\n# \n# While setting the type of categorical features to 'category' can save memory and help with filtering activities such as selecting features based on dtype, it often causes problems.\n# during data wrangling. For example, replacing missing categorical data with a category 'NA' that's not in the original category set will fail.\n# We also run into problems with onehotencoding after removing minority categories. For this reason, I will limit setting of dtypes to numerical values only.\n\ndata_types = {'MSZoning': 'category',\n              'Street': 'category',\n              'Alley': 'category',\n              'LotShape': 'category',\n              'LandContour': 'category',\n              'Utilities': 'category',\n              'LotConfig': 'category',\n              'LandSlope': 'category',\n              'Neighborhood': 'category',\n              'Condition1': 'category',\n              'Condition2': 'category',\n              'BldgType': 'category',\n              'HouseStyle': 'category',\n              'RoofStyle': 'category',\n              'RoofMatl': 'category',\n              'Exterior1st': 'category',\n              'Exterior2nd': 'category',\n              'MasVnrType': 'category',\n              'ExterQual': 'category',\n              'ExterCond': 'category',\n              'Foundation': 'category',\n              'BsmtQual': 'category',\n              'BsmtCond': 'category',\n              'BsmtExposure': 'category',\n              'BsmtFinType1': 'category',\n              'BsmtFinType2': 'category',\n              'ExterQual': 'category',\n              'Heating': 'category',\n              'HeatingQC': 'category',\n              'CentralAir': 'category',\n              'Electrical': 'category',\n              'KitchenQual': 'category',\n              'Functional': 'category',\n              'FireplaceQu': 'category',\n              'GarageType': 'category',\n              'GarageFinish': 'category',\n              'GarageQual': 'category',\n              'GarageCond': 'category',\n              'PavedDrive': 'category',\n              'PoolQC': 'category',\n              'Fence': 'category',\n              'MiscFeature': 'category',\n              'SaleType': 'category',\n              'SaleCondition': 'category',\n              'MoSold': 'int64',\n              'YrSold': 'int64',\n              'YearBuilt': 'int64'}\n\ndata_types = {'MoSold': 'int64',\n              'YrSold': 'int64',\n              'YearBuilt': 'int64'}\n              \n# load the training an test data into seperate pandas DataFrames\n#df_train = pd.read_csv('train.csv', index_col='Id', dtype=data_types)\n#df_test = pd.read_csv('test.csv', index_col='Id', dtype=data_types)\n\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id', dtype=data_types)\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id', dtype=data_types)","eb9e0abf":"df_train.info()","8a9f580f":"# Helper function to display missing data by feature\n# https:\/\/stackoverflow.com\/questions\/26266362\/how-to-count-the-nan-values-in-a-column-in-pandas-dataframe#26266451 \n#\ndef missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'No. Missing Values', 1 : '%'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '%', ascending=False).round(1)\n        print (\"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n               \" features with missing values.\")\n        \n        return mis_val_table_ren_columns\n\nmissing_values_table(df_train)","607d542c":"df_train.describe()","ead7dae7":"df_train.head()","ef4c59f3":"def num_summary(df, feature):\n    \"\"\" Creates a distplot of the specified numerical column of a DataFrame.\n\n    Args:\n        df: DataFrame\n        num_label: The column to be plotted\n\n    Returns:\n        None:\n    \"\"\"\n    print(\"Feature: \", feature, '\\n__________________________\\n')\n    print(df[feature].describe())\n    print('skew: ', df[feature].skew())\n    print('kurtosis: ', df[feature].kurtosis())\n    \n    minimum = df[feature].min()\n    maximum = df[feature].max()\n    sns.distplot(df[feature])\n    plt.xlim((minimum-0.5, maximum+0.5))\n    plt.title(feature)\n    plt.show()\n    \n    sns.boxplot(data=df, x=feature)\n    plt.show()\n    \ndef cat_summary(df, feature, normalise=True):\n    \"\"\" Creates a distplot of the specified nominal column of a DataFrame.\n\n    Args:\n        df: DataFrame\n        num_label: The column to be plotted\n\n    Returns:\n        None:\n    \"\"\"\n    print(\"Feature: \", feature, '\\n__________________________\\n')\n    print('value_count: ', df[feature].value_counts(normalize=normalise))\n    print('skew: ', df[feature].value_counts(normalize=normalise).skew())\n    print('kurtosis: ', df[feature].value_counts(normalize=normalise).kurtosis())\n    \n    df[feature].value_counts().sort_values(ascending=False).plot(kind='bar')\n    plt.title(feature)\n    plt.show()","bc85f953":"numerical_features = df_train.select_dtypes(include='number').columns.tolist()\n\n# A number of features have zero KDE. This is causing distplot to throw an error. Removing features from visual exploration until later.\nnumerical_features.remove('BsmtFinSF2')\nnumerical_features.remove('LowQualFinSF')\nnumerical_features.remove('BsmtHalfBath')\nnumerical_features.remove('KitchenAbvGr')\nnumerical_features.remove('EnclosedPorch')\nnumerical_features.remove('3SsnPorch')\nnumerical_features.remove('ScreenPorch')\nnumerical_features.remove('PoolArea')\nnumerical_features.remove('MiscVal')\n\nfor feature in numerical_features:\n    num_summary(df_train, feature)","5019469a":"#histogram and normal probability plot of SalePrice\nsns.distplot(df_train['SalePrice'])\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","c956919d":"#histogram and normal probability plot of the natural log of SalePrice\nsns.distplot(np.log(df_train['SalePrice']))\nfig = plt.figure()\nres = stats.probplot(np.log(df_train['SalePrice']), plot=plt)","1378c9fd":"#histogram and normal probability plot of the natural log of GrLivArea\nsns.distplot(np.log(df_train['GrLivArea']))\nfig = plt.figure()\nres = stats.probplot(np.log(df_train['GrLivArea']), plot=plt)","bf085d47":"categorical_features = df_train.select_dtypes(include='object').columns.tolist()\n\nfor feature in categorical_features:\n    cat_summary(df_train, feature)","86a27197":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap((df_train.select_dtypes(include='number')).astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=False)","d61b2841":"def get_redundant_pairs(df):\n    \"\"\" Determine the diagonal and lower triangular pairs of correlation matrix\n\n    Args:\n        df: DataFrame\n\n    Returns:\n        df: A list of tuples (row, column) of cells to drop in correlation\n        matrix\n    \"\"\"\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    \"\"\" Determine the correlation coefficients of features in a DataFrame\n    and return the top n in order of importance.\n\n    Args:\n        df: DataFrame\n        n:  The number of top coefficients to return\n\n    Returns:\n        df: The top n correlation coefficients\n    \"\"\"\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(\"=========================\\n\")\n\nprint(get_top_abs_correlations(df_train[df_train.select_dtypes(include='number').columns.tolist()], 15))","0323748e":"# let's start from a fresh copy of the data.\ndata_types = {'MoSold': 'int64',\n              'YrSold': 'int64',\n              'YearBuilt': 'int64'}\n\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id', dtype=data_types)\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id', dtype=data_types)","d41417c8":"# let's perform additional feature engineering\nfor data in [df_train, df_test]:\n    data['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    data['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasgarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasbsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasfireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    \n    # These features do not improve performance.\n    #\n    #data['YrBltAndRemod'] = data['YearBuilt']+data['YearRemodAdd']\n    #data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n    #data['Total_sqr_footage'] = (data['BsmtFinSF1'] + data['BsmtFinSF2'] +\n    #                             data['1stFlrSF'] + data['2ndFlrSF'])\n    #data['Total_Bathrooms'] = (data['FullBath'] + (0.5 * data['HalfBath']) +\n    #                           data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath']))\n    #data['Total_porch_sf'] = (data['OpenPorchSF'] + data['3SsnPorch'] +\n    #                          data['EnclosedPorch'] + data['ScreenPorch'] +\n    #                          data['WoodDeckSF'])","ac4e1e83":"# create a list of numerical features without the target SalePrice\nnumerical_features = df_train.select_dtypes(include='number').columns.tolist()\nnumerical_features.remove('SalePrice')\n\n# create a list of categorical features\ncategorical_features = df_train.select_dtypes(include='object').columns.tolist()","88747cd6":"# numerical features to drop. More than 20% missing data OR correlation highly (>0.7) with other features\nnumerical_features_to_drop = ['FireplaceQu'] + ['GarageCars', 'GarageYrBlt', 'TotRmsAbvGrd', '1stFlrSF']\n\nfor data in [df_train, df_test]:\n    data.drop(numerical_features_to_drop,inplace=True, axis=1)     ","d4db54cd":"# categorical features with more than 20% missing data. Replace missing data with new category 'missing_data'\ncategorical_features_na = ['PoolQC', 'MiscFeature', 'Alley', 'Fence']\n\nfor data in [df_train, df_test]:\n    for feature in categorical_features_na:\n        data[feature].fillna(\"missing_value\", inplace = True)","bab1b638":"missing_values_table(df_train)","a4994504":"class TransLog(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_index):\n        \n        self.feature_index = feature_index\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X_trans = np.log(X[:, self.feature_index])\n        return np.c_[X[:, 0:self.feature_index - 1], X_trans, X[:, self.feature_index:]]","15264708":"# Separate train data into X (input features) and y (output target) \n\n# Log transform y_train. Remember, we'll need to raise e to the power y_test before submission!\ny_train = np.log(df_train['SalePrice'])\n\nX_train = df_train\nX_train.drop(columns=['SalePrice'], inplace=True)\n\n# We only have input features for test, hence no need to drop 'SalePrice'\nX_test = df_test\n\n# Recompute the list of numerical and categorical features\nnumerical_features = df_test.select_dtypes(include='number').columns.tolist()\ncategorical_features = df_test.select_dtypes(include='object').columns.tolist()","e29a52ef":"# As the Pipeline works with Series not dataframes, we need the index of feature GrLivArea\nlog_feature_index = numerical_features.index('GrLivArea')","c6c14338":"# Define the numerical transformation pipeline\nnum_pipeline = Pipeline([\n                        ('simple_imp', SimpleImputer(strategy='median')),\n                        ('log_trans', TransLog(feature_index=log_feature_index)),\n                        ('std_scaler', StandardScaler()) #minmaxscaler is not an improvement.\n                        ])\n\n\ncat_pipeline = Pipeline([\n                        ('simple_imp', SimpleImputer(strategy='most_frequent')),\n                        ('one_hot_enc', OneHotEncoder(handle_unknown='ignore'))\n                        ])\n\n# Define a composite transformation pipeline.\ncomp_pipeline = ColumnTransformer([('num_pipeline', num_pipeline, numerical_features),\n                                   ('cat_pipeline', cat_pipeline, categorical_features)\n                                  ])\n\n# fit AND transform X_train using the pipeline.\nX_train_prepared = comp_pipeline.fit_transform(X_train)\n\n# NOTE: we have already fit the pipeline using X_train. We therefore only call transform on X_test. We don't want to fit the test set. Not only is it wrong, but because there are minority categories in some features, X_train_prepared and X_test_prepared will be of different dimensions!\nX_test_prepared = comp_pipeline.transform(X_test)","cd322c62":"print(X_train_prepared.shape)\nprint(X_test_prepared.shape)","220d7d19":"# Let's keep track of each model's performance\nresults = []","1f2e0e36":"# Linear Regression classifier\nlr_clf = LinearRegression()\n\n%time lr_clf.fit(X_train_prepared, y_train)\n\n# Apply the model to the train dataset.\ny_train_predicted = lr_clf.predict(X_train_prepared)\n\nresults.append({'alg': 'lr_clf', 'model': lr_clf, 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted)})\nresult=results[-1]\nprint('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])","787b2755":"# random forest classifier\nrf_clf = RandomForestRegressor(random_state=RANDOM_SEED)\n\n%time rf_clf.fit(X_train_prepared, y_train)\n\n# Apply the model to the train dataset.\ny_train_predicted = rf_clf.predict(X_train_prepared)\n\nresults.append({'alg': 'rf_clf', 'model': rf_clf, 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted)})\nresult=results[-1]\nprint('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])","9010f51d":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\nada_clf=AdaBoostRegressor(random_state=RANDOM_SEED, n_estimators=100)\n\n%time ada_clf.fit(X_train_prepared,y_train)\n\n# Apply the model to the train dataset.\ny_train_predicted = ada_clf.predict(X_train_prepared)\n\nprint('Feature Importance Values\\n\\n',ada_clf.feature_importances_)\n\nresults.append({'alg': 'ada_clf', 'model': ada_clf, 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted)})\nresult=results[-1]\nprint('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])","b98432a3":"# XGBoost Classifier\nxgb_clf = XGBRegressor(objective=\"reg:squarederror\", seed=RANDOM_SEED)\n\n%time xgb_clf.fit(X_train_prepared, y_train) \n\n# Apply the model to the train dataset.\ny_train_predicted = xgb_clf.predict(X_train_prepared)\n\nresults.append({'alg': 'xgb_clf', 'model': xgb_clf, 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted)})\nresult=results[-1]\nprint('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])","7e44d79d":"# Lasso Classifier\nlso_clf = Lasso(random_state=RANDOM_SEED)\n\n%time lso_clf.fit(X_train_prepared, y_train) \n\n# Apply the model to the train dataset.\ny_train_predicted = lso_clf.predict(X_train_prepared)\n\nresults.append({'alg': 'lso_clf', 'model': lso_clf, 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted)})\nresult=results[-1]\nprint('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])","a10dd30f":"# ElasticNet Classifier\neln_clf = ElasticNet(random_state=RANDOM_SEED)\n\n%time eln_clf.fit(X_train_prepared, y_train) \n\n# Apply the model to the train dataset.\ny_train_predicted = eln_clf.predict(X_train_prepared)\n\nresults.append({'alg': 'eln_clf', 'model': eln_clf, 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted)})\nresult=results[-1]\nprint('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])","79566dbe":"# LightGBM Classifier\nlgbm_clf = LGBMRegressor(random_state=RANDOM_SEED)\n\n%time lgbm_clf.fit(X_train_prepared, y_train) \n\n# Apply the model to the train dataset.\ny_train_predicted = lgbm_clf.predict(X_train_prepared)\n\nresults.append({'alg': 'lgbm_clf', 'model': lgbm_clf, 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted)})\nresult=results[-1]\nprint('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])","b98abd45":"# Summarise Results - Sorted by MAE\nprint('\\n-------------------------Results-----------------------')\nfor result in sorted(results, key=lambda k: k['mae']) :\n    print('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])","a951a7b1":"# limit scope of variables\nxgb_param_grid = {\n        'min_child_weight': [1],\n        'gamma': [0],\n        'subsample': [1.0],\n        'colsample_bytree': [0.2],\n        'max_depth': [3],\n        'n_estimators': [2500]\n        }\n\n\n#xgb_param_grid = {\n#        'min_child_weight': [1, 2, 3, 4],\n#        'gamma': [0, 0.1, 0.2],\n#        'subsample': [0.8, 0.9, 1.0],\n#        'colsample_bytree': [0.2, 0.25, 0.3, 0.35, 0.4],\n#        'max_depth': [3, 4, 5, 6],\n#        'n_estimators': [2000]\n#        }\n\n# Perform grid search with cross validation\nxgb_grid_search = GridSearchCV(xgb_clf, xgb_param_grid, cv=CV_FOLDS, n_jobs=4, return_train_score=True, verbose=1)\n\n%time  xgb_grid_search.fit(X_train_prepared, y_train)\n\n# Get the best model parameters\nprint(xgb_grid_search.best_params_)\nprint(xgb_grid_search.best_score_)\n\ny_train_predicted = xgb_grid_search.best_estimator_.predict(X_train_prepared)\n\nresults.append({'alg': 'xgb_clf_tuned', 'model': xgb_grid_search.best_estimator_, 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted), 'best_params': xgb_grid_search.best_params_})","5d9e32fa":"# limit scope of variables to test\nrf_param_grid = [{'ccp_alpha': [0.0],\n                  'criterion': ['mse'],  # mae is causing a problem. Live lock? \n                  'max_depth': [None],\n                  'max_features': [None],\n                  'max_leaf_nodes': [None],\n                  'min_impurity_decrease': [0.0],\n                  'oob_score': [True],\n                  'min_impurity_split': [None],\n                  'min_samples_leaf': [2],\n                  'min_samples_split': [4],\n                  'min_weight_fraction_leaf': [0.0],\n                  'random_state': [42],\n                 }\n                ]\n\n#rf_param_grid = [{'ccp_alpha': [0.0],\n#                  'criterion': ['mse'],  # mae is causing a problem. Live lock? \n#                  'max_depth': [None, 2, 5],\n#                  'max_features': [None, 'auto'],\n#                  'max_leaf_nodes': [None],\n#                  'min_impurity_decrease': [0.0],\n#                  'oob_score': [True],\n#                  'min_impurity_split': [None],\n#                  'min_samples_leaf': [1, 2],\n#                  'min_samples_split': [4, 5, 6],\n#                  'min_weight_fraction_leaf': [0.0],\n#                  'random_state': [42],\n#                 }\n#                ]\n\n# Perform grid search.\nrf_grid_search = GridSearchCV(rf_clf, rf_param_grid, n_jobs=4, cv=CV_FOLDS, return_train_score=True, verbose=1)\n\n%time rf_grid_search.fit(X_train_prepared, y_train)\n\n# Get the best model parameters\nprint(rf_grid_search.best_params_)\nprint(rf_grid_search.best_score_)\n\ny_train_predicted = rf_grid_search.best_estimator_.predict(X_train_prepared)\n\nresults.append({'alg': 'rf_clf_tuned', 'model': rf_grid_search.best_estimator_, 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted), 'best_params': rf_grid_search.best_params_})","31987488":"# limit scope of variables to test\nada_param_grid = [{'n_estimators': [2000],\n                   'loss': ['square'],\n                   'random_state': [RANDOM_SEED]\n                 }\n                ]\n\n#ada_param_grid = [{'n_estimators': [50, 100, 300, 2000],\n#                   'loss': ['linear', 'square', 'exponential'],\n#                   'random_state': [RANDOM_SEED]\n#                 }\n#                ]\n\n# Perform grid search.\nada_grid_search = GridSearchCV(ada_clf, ada_param_grid, n_jobs=4, cv=CV_FOLDS, return_train_score=True, verbose=1)\n\n%time ada_grid_search.fit(X_train_prepared, y_train)\n\n# Get the best model parameters\nprint(ada_grid_search.best_params_)\nprint(ada_grid_search.best_score_)\n\ny_train_predicted = ada_grid_search.best_estimator_.predict(X_train_prepared)\n\n\nresults.append({'alg': 'ada_clf_tuned', 'model': ada_grid_search.best_estimator_, 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted)})\nresult=results[-1]\nprint('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])","2d6b0366":"#from sklearn.linear_model import Lasso\n\n# Stacking\n#estimators = [\n#              ('ada_clf', ada_grid_search.best_estimator_),\n#              ('xgb_clf', xgb_grid_search.best_estimator_),\n#              ('rf_clf', rf_grid_search.best_estimator_)             ]\n             \n#stack_clf = StackingRegressor(estimators=estimators, final_estimator=Lasso())\n\n#%time  stack_clf.fit(X_train_prepared, y_train)\n\n#y_train_predicted = stack_clf.predict(X_train_prepared)\n\n#results.append({'alg': 'stack_clf', 'mse': mean_squared_error(y_train, y_train_predicted), 'mae': mean_absolute_error(y_train, y_train_predicted)})\n#result=results[-1]\n#print('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])","610a4cd5":"# Summarise Results - Sorted by MAE\nprint('\\n-------------------------Results-----------------------')\nfor result in sorted(results, key=lambda k: k['mae']) :\n    print('Algorithm:', result['alg'], 'MSE:', result['mse'], 'MAE:', result['mae'])\n    \nbest_model = sorted(results, key=lambda k: k['mae'])[0]['model']\nprint(best_model)\n\ny_test_predicted = np.exp(best_model.predict(X_test_prepared))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': y_test_predicted})\n#output.to_csv('submission.csv', index=False)\noutput.to_csv('\/kaggle\/working\/submission.csv', index=False)","be5f7ba6":"# Model Testing & Submission","39ebe57c":"# Data Exploration","8a5093dc":"The log transform of SalePrice has made it significally more 'Normal'. A similar approach with 'GrLivArea' achieves a similar result.","94eae1a6":"Confirm the feature dimension of train and test are the same. If not, go back to the transformation pipeline and fix it.","19631af1":"There are:\n\n* 1460 rows of data\n* a mixture of category(43), float64(3), int64(34) data types.\n* 80 features and an index Id\n* the entire DataFrame only takes up 0.5 MB of memory. If we hadn't set the Dtype of objects upon reading from csv file, it would have been double this.\n\nLet's now calculate missing (null) as percentage of the total number of records.","37b5637c":"Let's start with a brief summary of the DataFrame.","7616ab18":"XGBoost is currently leading the way with a significant reduction in MAE over other algorithms.\n\nEventually we'll have a full compliment of ensemble models and incoporate stacking. Right now, we don't have enought models to take advantage of it, but let's plumb it in ready for later versions of the notebook.","316b9a97":"Of the 19 features that have missing data, 6 (5 of type object, 1 of type float) have > 20% of missing data. \n\n* PoolQC (ojb)\n* MiscFeature (ojb)\n* Alley (ojb)\n* Fence (ojb)\n* FireplaceQu (num)\n\nDuring the data preparation section we will:\n\n**Features with 20% or more missing data**\n* Drop numerical features with more than 20% missing data.\n* Create a new category 'Missing_Data' for object\/category features. This avoids losing potentially valuable categorical data while avoiding using mode.\n\n**Features with less than 20% missing data**\n* Use either mean or median for missing numerical data. The degree of skewness will guide us (refer to EDA)\n* Use mode for missing object\/category features.","5f31d509":"Let's review how the transformations have altered df_train","669d1a72":"Good. All features now have less than 20% missing data. Imputation will address the remaing missing data. The rest of data preparation we'll do inside of the main transformation pipeline operation.","4af31d2a":"# Data Preparation\n\nIn this version we'll be including categorical data (by performing one hot encoding) after dropping or imputing features as detailed earlier. This will be extensively developed in future versions.\n\nI'll be using sklearn.pipeline.Pipeline to manage most (all in a later iteration) of the transformation of training and testing data and developing custom transform functions as required.","03845075":"# Feature Selection\nMinor feature selection performed earlier based on correlation matrix. More feature selection will be performed in future versions of the notebook.\n\n\n# Model Tuning\nLet's start with grid search and cross-validation for each model in preparation for stacking.","e2126eb2":"Computation time has gone up dramatically, with no real improvemnt in MAE compared with Linear Regression. We can however see that many of the features coefficients are zero. We may be able to use this information to reduce features in later versions of the notebook.","96b9abe4":"Let's now look at how featues correlate and then pairplot of interesting feature pairs.","7d7e1bec":"# Model Building\n\nIn this version of the notebook, let's limit modelling to only a handful of models:\n\n* Linear Regression - maninly serving as a baseline.\n* Random Forest\n* AdaBoost\n* XGBoost\n* Lasso\n* ElasticNet\n* LightGBM\n* Stacking - mainly to plumb in meta-modelling for use in later versions of the notebook.","d0fc4259":"There's a high degree (> 0.7) of linear correlation between the following features:\n\n* GarageCars <-> GarageArea\n* YearBuilt <-> GarageYrBlt\n* GrLivArea <-> TotRmsAbvGrd\n* TotalBsmtSF <-> 1stFlrS\n* OverallQual <-> SalePrice\n* GrLivArea <-> SalePrice\n\nWe want our model to consist of independent features, so let us drop the following features:\n\n* GarageCars\n* GarageYrBlt\n* TotRmsAbvGrd\n* 1stFlrS","ac909c64":"In this version of the notebook, let's keep observations to high-level highlights\n\n* Cardinality is low to medium across all the features\n* A number of features are dominated by 1~2 categories. We may be able to use this to quickly reduce dimensionality when categorical data is included into the modelling process. If one category dominates (>80%) we can probably drop the feature. If there are lot's of minority features, we can probably drop them during the pipeline transformation.\n\n","a9e7b575":"The actual ordered values deviate from the theoretical (normally distributed values). Let's log transform SalesPrice and see if that improves actual vs theoretical.","afb18655":"In this version of the notebook, let's keep observations to a high-level highlights\n\n* A number of the distributions are skewed (mostly right skewed).\n* Most features have outliers to be considered and potentially removed.\n\n\nLet's look at how closely SalePrice follows a normal distrubtion. We saw from the earlier plot that it is rigt skewed (1.88), but let's take an even closer look.","dbf71aca":"# Introduction\n\nWelcome. This tutorial focuses on developing a model to predict California House Prices. It is intended for a beginner-level audience.\n\n## Purpose\n\nThis notebook serves two purposes:\n\n* it acts as a beginner-level worked example of a machine learning regression problem\n* it consolidates my own understanding of the theory and practice of following the data science framework contained within this notebook.\n\n\n## Competition Description\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n## Goal\n\nIt is your job to predict the sales price for each house. For each __Id__ in the test set, you must predict the value of the __SalePrice__ variable. \n\n## Metrics\n\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n\n## Approach\n\nWe will use the following approach to predict which passengers survived:\n\n* Problem Statement\n* Data Gathering\n* Data Exploration\n* Data Preparation\n* Model Building\n* Model Tuning\n* Feature Selection\n* Model Validation\n* Model Testing and Submission\n\n__NOTE__ Early versions of this notebook will focus on obtaining baseline results for common regression algorithms and meta-learning algorithms. Early versions will purposely contain minimal Exploratory Data Analysis (EDA) and data preperation. Instead, the intial focus will be to use features that require minimal pre-processing and be restricted to numerical types. Once a working pipeline is in place, subsequent revisions will iteratively develop and refine each of the steps outlined in the approach above.  \n\n# Data Gathering\n\nThe three data sources of interest are:\n\n* test.csv\n* train.csv\n* data_description.txt\n\nIn this version of the notebook, the data is not enriched with external sources.","5090f9b6":"As expected, there's a large variation in scale between each of the features. Prescaling of data will be important for almost all of the modelling algorithms we use. "}}