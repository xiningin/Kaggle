{"cell_type":{"4b998716":"code","ff56ab1b":"code","f52557bf":"code","2e4f248f":"code","28ffb080":"code","4e379b4a":"code","a7e32a98":"code","d44c8a5e":"code","51f9c66b":"code","812dea4f":"code","4a3de2a9":"code","46740981":"code","f31948bb":"code","320f74de":"code","7f3957e1":"code","3d688711":"code","f0df9e88":"code","463eacd4":"code","56e457ec":"code","5b99dca4":"code","950e5e66":"code","2c65d794":"code","bc4bce25":"code","c72f5867":"markdown","4e3c88b3":"markdown","5c2baf91":"markdown","44b6ed1d":"markdown","af01927d":"markdown","1649c0e5":"markdown","fb4d4fef":"markdown","1e5a4a58":"markdown","fa8d1ef2":"markdown","ff09fd17":"markdown","8ccd1421":"markdown","e1b4a788":"markdown","b167e094":"markdown","17b8a3b0":"markdown","1e136eec":"markdown"},"source":{"4b998716":"from transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\")","ff56ab1b":"txt = \"This week at NiHub is really interesting and I am so glad I could join.\"\n\nclassifier(txt)","f52557bf":"txt = \"I missed my train, I am so frustrated.\"\n\nclassifier(txt)","2e4f248f":"txt = \"I am totally lost with these AI stuff. Ways too technical for me this late in the afternoon.\"\n\nclassifier(txt)","28ffb080":"txt = \"Our order book is full, our factory is running to maximum capacity. This quarter looks promising.\"\n\nclassifier(txt)","4e379b4a":"from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline\nmodel = AutoModelForSequenceClassification.from_pretrained('uer\/roberta-base-finetuned-chinanews-chinese')\ntokenizer = AutoTokenizer.from_pretrained('uer\/roberta-base-finetuned-chinanews-chinese')\ncn_classification = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)","a7e32a98":"txt = \"\u5317\u4eac\u4e0a\u4e2a\u6708\u53ec\u5f00\u4e86\u4e24\u4f1a\"\n\ncn_classification(txt)","d44c8a5e":"txt = \"\u8fd1\u5e74\u6765\uff0c\u90e8\u5206\u98df\u54c1\u548c\u5316\u5986\u54c1\u4f01\u4e1a\u4e3a\u8ffd\u6c42\u9ad8\u989d\u5229\u6da6\uff0c\u8bbe\u8ba1\u548c\u4f7f\u7528\u5c42\u6570\u8fc7\u591a\u3001\u7a7a\u9699\u7387\u8fc7\u5927\u3001\u6210\u672c\u8fc7\u9ad8\u7684\u5305\u88c5\uff0c\u5c06\u5305\u88c5\u6210\u672c\u9644\u52a0\u5230\u6d88\u8d39\u8005\u8eab\u4e0a\uff0c\u65e2\u9020\u6210\u8d44\u6e90\u6d6a\u8d39\u548c\u73af\u5883\u6c61\u67d3\uff0c\u53c8\u635f\u5bb3\u4e86\u6d88\u8d39\u8005\u7684\u5408\u6cd5\u6743\u76ca\u3002\"\n\ncn_classification(txt)","51f9c66b":"txt = \"\u5728\u7b2c78\u5c4a\u5a01\u5c3c\u65af\u56fd\u9645\u7535\u5f71\u8282\u8fdb\u884c\u5168\u7403\u9996\u6620\uff0c\u5f71\u7247\u65a9\u83b7\u5168\u573a\u597d\u8bc4\uff0c\u6620\u540e\u638c\u58f0\u957f\u8fbe\u516b\u5206\u949f\u3002\"\n\ncn_classification(txt)","812dea4f":"classifier = pipeline(\"zero-shot-classification\")","4a3de2a9":"txt = \"This is a course about the Transformers library\"\nlabels = [\"education\", \"politics\", \"business\"]\n\nclassifier(txt, candidate_labels=labels)","46740981":"txt_coding = 'C++ Operators are used to perform operations on variables and values. In the example below, we use the + operator to add together two values. Although the + operator is often used to add together two values, like in the example above, it can also be used to add together a variable and a value, or a variable and another variable.'\ntxt_woodwork = 'The most common type of biscuit joints is edge-to-edge joints. This is often used for gluing up table tops of various width boards of the same thickness, where biscuits are used along the planed long edges of the boards. To glue up a tabletop of various boards, lay out the boards side-by-side with each board\\'s end grain turned in the opposite direction of that of the previous board.'\ntxt_cooking = 'In a large bowl, combine the beef, egg, onion, milk and bread OR cracker crumbs. Season with salt and pepper to taste and place in a lightly greased 9x5-inch loaf pan, or form into a loaf and place in a lightly greased 9x13-inch baking dish.'\nlabels=['coding', 'cooking', 'woodworking']","f31948bb":"classifier(txt_coding, candidate_labels=labels)","320f74de":"classifier(txt_cooking, candidate_labels=labels)","7f3957e1":"classifier(txt_woodwork, candidate_labels=labels)","3d688711":"classifier_fr = pipeline(\"zero-shot-classification\", \n                         model=\"BaptisteDoyen\/camembert-base-xnli\")","f0df9e88":"txt = \"L'\u00e9quipe de France joue aujourd'hui au Parc des Princes\"\nlabels = [\"sport\",\"politique\",\"science\"]\n\nclassifier_fr(txt, candidate_labels=labels)","463eacd4":"txt = \"Write your text here\"\nlabels = [\"education\", \"politics\", \"business\"]\n\nclassifier(txt, candidate_labels=labels)","56e457ec":"generator = pipeline(\"text-generation\")","5b99dca4":"results = generator(\"In this worshop about AI, we will teach you how to\")\n\nprint(results[0]['generated_text'])","950e5e66":"txt = \"I really want to get somewhere with my studies but\"\nlength = 50\nnbr_answers = 3\n\nresults = generator(txt, total_length=length, num_return_sequences=nbr_answers);\nfor i, r in enumerate(results):\n    print(i+1, '>>>')\n    print(r['generated_text'])\n    print('-------------')","2c65d794":"gpt2 = pipeline(\"text-generation\", model=\"distilgpt2\")","bc4bce25":"txt = \"In this worshop, we will explain you how to\"\nlength = 50\nnbr_answers = 3\nresults = gpt2(txt, max_length=length, num_return_sequences=nbr_answers)\nfor i, r in enumerate(results):\n    print(i+1, '>>>')\n    print(r['generated_text'])\n    print('-------------')","c72f5867":"## Sentiment Analysis\nThis is an example of how the NLP application can extract semantic-like meaning from the text. In this case, we will classify a text as:\n- positive sentiment\n- negative sentiment\n\nThe system uses what is called a pretrained Language Model and a pretrained classier. \n\nFirst we create a specific classifer for sentiment analysis (first time, a model will be downloaded). It is pretrained for language and for sentiment classification. We can imediately make prediction without training ourselves.","4e3c88b3":"Natural Language Processing is a vast topic on which people work for decades using various methodolgies.\n\nOver the last 10 years, Deep Learning has made vast progress in NLP, especially these last 5 years with the arrival of **transformers**.\n\nIt is out of the scope of this notebook to explain what these powerfull tools are. But thanks to Hugging Face, it is possible to experiment  their power.\n\n## Hugging Face\n<div align=\"left\" width=100%><img src=\"https:\/\/huggingface.co\/front\/assets\/course-logo.svg\" width=10%><\/div>\n\nThe Hugging Face consits of about 50 people and describes its ambition as: \"Build, train and deploy state of the art models powered by the reference open source in natural language processing.\"\n\nIt offers tools making available pre-trained language models you may have hear of such as: BERT, ALBERT, RoBERTa, DistilBERT, GPT, GPT-2, Transformer XL, BART, mBART, T5 ...","5c2baf91":"### Try your own !","44b6ed1d":"# Conclusion\n\nThis is just a simple notebook to experiment with NLP. There are obviously a wide space of possibilities with this, especially when you use some of these tools and integrate it into finetuned or customized sytems.\n\nHopefully, this will give you a taste for machine learning and deep learning.","af01927d":"# Natural Language Processing - Experiments","1649c0e5":"Here three labels: **coding, cooking and woodworking**.\n\nThe sentences are cut and pasted from related web pages.\n\nLet's see what it gives.","fb4d4fef":"The model evaluate that the sentence is most probably about \"education\" with a condidence of 84%.","1e5a4a58":"Sounds like the generator has gotten good marketing lessons !","fa8d1ef2":"Write your own sentence and see how the classifer evaluates the sentiment. You will get a class as well as a score (how sure te model is)","ff09fd17":"You may have heard of GPT-n from OpenAI. Here is a version of GPT-2, that is pretty good at text generation.","8ccd1421":"Seems our classifer is less positive with wood work \ud83d\ude00 But it still gets it right.","e1b4a788":"## Text generation\n\nText generation is a basic function usefull for chatbots, advanced question and answer systems and other application. It is not easy to generate a realistic sentence.\n\nHere we use it in a simple way, for fun.\n\nFirst we create our generator, then we start a sentence and have the generator continue it","b167e094":"There are versions of this zero shot classification in several languages, but not yet in Chinese. The one below is for French.","17b8a3b0":"## Zero-shot classification\nPretrained language model are really powerfull, and in some cases it even allows to do what is called zero-shot classification, that is to classify texts that haven\u2019t been labelled. This is a useful scenario because labelling text is rather time-consuming and most often requires specific domain expertise. \n\nLet's try what it gives.\n\nFirst we create a classifier for zero shot classification. First time you create it, it will download the pretrained models. You can see that some of the files are several gigabytes.","1e136eec":"### Chinese model\nThere is also one model pretrained in Chinese, and classifying news text into the type of news it may be."}}