{"cell_type":{"a6628430":"code","d5f5631d":"code","83da5492":"code","4484d734":"code","e77c813a":"code","6dbbfccb":"code","69358f3d":"code","8705947d":"code","75fe4956":"code","642ab04d":"code","81b158c6":"code","11a6c160":"markdown","ab2a9649":"markdown","13ee0afe":"markdown","cba507b5":"markdown","e075ba53":"markdown","41123193":"markdown","129ad0e0":"markdown","99a4e990":"markdown","458aead2":"markdown","0efd072a":"markdown","fdd42da6":"markdown","e1a42b91":"markdown","baac2bf2":"markdown","1e06963a":"markdown","3af55388":"markdown","d814e243":"markdown"},"source":{"a6628430":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5f5631d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold # For creating folds\nfrom sklearn.metrics import log_loss # Evaluation metrics","83da5492":"df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\")\nss = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")","4484d734":"print(f\"Shape of train : {df.shape}\")\nprint(f\"Shape of test : {test.shape}\")\nprint(f\"Shape of sample submission : {ss.shape}\")","e77c813a":"df.head()","6dbbfccb":"df.info()","69358f3d":"test.info()","8705947d":"sns.countplot(x= df.target)","75fe4956":"df[\"kfold\"] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.target\nkf = StratifiedKFold(n_splits=5)\nfor f, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n  df.loc[v_,\"kfold\"] = f","642ab04d":"lgbm = LGBMClassifier(random_state=42)\nlogloss = []\nlgbm_pred = 0\nfor f in range(5): # Looping around 5 folds\n    \n    #Splitting the data into train and validation set\n    train = df[df.kfold!= f].reset_index(drop=True) \n    valid = df[df.kfold== f].reset_index(drop=True)\n    \n    #Creating X_train and y_train\n    X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_train = train.target\n    X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_valid = valid.target\n    X_test = test.drop([\"id\"], axis=1)\n    \n    \n    #Fitting the model\n    lgbm.fit(X_train,y_train)\n    \n    #Predicting for valid and test datasets\n    valid_preds = lgbm.predict_proba(X_valid)\n    lgbm_pred += lgbm.predict_proba(X_test)\/5\n    \n    #Calculating log loss\n    logloss.append(log_loss(y_valid,valid_preds))\n    \nprint(logloss)\nprint(sum(logloss)\/len(logloss))","81b158c6":"ss[\"Class_1\"] = lgbm_pred[:,0]\nss[\"Class_2\"] = lgbm_pred[:,1]\nss[\"Class_3\"] = lgbm_pred[:,2]\nss[\"Class_4\"] = lgbm_pred[:,3]\nss[\"Class_5\"] = lgbm_pred[:,4]\nss[\"Class_6\"] = lgbm_pred[:,5]\nss[\"Class_7\"] = lgbm_pred[:,6]\nss[\"Class_8\"] = lgbm_pred[:,7]\nss[\"Class_9\"] = lgbm_pred[:,8]\nss.to_csv(\"\/kaggle\/working\/sub.csv\", index=False)","11a6c160":"## 6. Creating submission file","ab2a9649":"**The average log loss is 1.7560632816516921**","13ee0afe":"### Hi everyone, this is a very basic starter notebook for this competition with LightGBM.","cba507b5":"## 3. Basic data check","e075ba53":"Creating folds for the train dataset, so that we can train the model for the n folds, to avoid overfitting.","41123193":"There are no missing values in the both train and test datasets and all are integers, so the categories might be encoded already.","129ad0e0":"## 1. Import libraries","99a4e990":"Since it is a baseline\/starter model, I am not doing EDA and directly moving onto model building part.","458aead2":"## Approach","0efd072a":"1. Import libraries\n2. Read the data\n3. Check for missing values and target distribution\n4. Create folds for Cross Validation\n5. Fit with base LGBMClassifier\n6. Create submission files","fdd42da6":"## 2. Reading the train, test and sample submission file","e1a42b91":"### Thank you! ","baac2bf2":"## 4. Checking target distribution","1e06963a":"## 5. Basline model","3af55388":"If you like the notebook kindly upvote it. It will motivate me to write more notebooks. :)","d814e243":"Target column is imbalanced, so I will use StratifiedKFold for cross validation."}}