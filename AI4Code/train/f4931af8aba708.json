{"cell_type":{"c6c3249d":"code","3726cfe0":"code","8320ca52":"code","fc0c6dc4":"code","99dc7824":"code","bd6df472":"code","c5f03668":"code","5a72d1c4":"code","0ffbaacd":"code","db0568c2":"code","df67699f":"code","4ff9701a":"code","562aa553":"code","63c9c449":"markdown","e0fcb136":"markdown"},"source":{"c6c3249d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport os \nimport time \nimport json \nimport requests \nfrom tqdm import tqdm \nimport wandb \nfrom wandb.keras import WandbCallback \nfrom kaggle_secrets import UserSecretsClient \nimport random \nfrom typing import Tuple \nimport gc \n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler \n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import LSTM, Input, Bidirectional, Dense \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.callbacks import Callback\nimport tensorflow.keras.backend as K\n\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything()\npd.set_option(\"display.max_columns\", None)","3726cfe0":"config = dict(\n    competition = \"ventilator\", \n    infra = \"kaggle\", \n    train = True, \n    type = \"train\", \n    debug = False, \n    inference = True, \n    \n    model_name = \"lstm\", \n    frame_word = \"tensorflow\", \n    device = \"tpu\", \n    n_fold = 5, \n    early_stopping_rounds = 30, \n    batch_size = 1024, \n    epoch = 299, \n    verbose = 100, \n    seed = 42 \n)\n","8320ca52":"user_secrets = UserSecretsClient()\nurl = user_secrets.get_secret(\"WEB_HOOK_URL\") \n\nuser_secrets = UserSecretsClient()\napi = user_secrets.get_secret(\"wandb_api\")\n\nwandb.login(key=api)\n\nrun = wandb.init(\n    project = config[\"competition\"], \n    name = config[\"model_name\"], \n    config = config, \n    group = config[\"model_name\"], \n    job_type = config[\"type\"]\n)\n\ndef slack(txt):\n    requests.post(url, data=json.dumps({\n        \"username\": \"kaggle\", \n        \"text\": txt \n    }))","fc0c6dc4":"if config[\"debug\"]:\n    train = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\", nrows=80*100)\n    test = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/test.csv\", nrows=80*100)\nelse:\n    train = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\")\n    test = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/test.csv\")\n\nsort = np.sort(train.pressure.unique())\nPRESSURE_MIN = sort[0]\nPRESSURE_MAX = sort[-1]\nPRESSURE_STEP = sort[1] - sort[0]","99dc7824":"def reduce_mem_usage(train_data):\n    start_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n\n    end_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return train_data","bd6df472":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","c5f03668":"def lag_feature(df) -> pd.DataFrame:\n    df[\"area\"] = df.time_step * df.u_in \n    df[\"area\"] = df.groupby(\"breath_id\")[\"area\"].cumsum()\n    \n    df[\"u_in_cumsum\"] = df.groupby(\"breath_id\")[\"u_in\"].cumsum()\n    \n    for i in range(4):\n        df[\"u_in_\"+f\"lag{i+1}\"] = df.groupby(\"breath_id\")[\"u_in\"].shift(i+1).fillna(0)\n        df[\"u_out_\"+f\"lag{i+1}\"] = df.groupby(\"breath_id\")[\"u_out\"].shift(i+1).fillna(0)\n\n        df[\"u_in_\"+f\"back{i+1}\"] = df.groupby(\"breath_id\")[\"u_in\"].shift((-1)*(i+1)).fillna(0)\n        df[\"u_out_\"+f\"back{i+1}\"] = df.groupby(\"breath_id\")[\"u_out\"].shift((-1)*(i+1)).fillna(0)\n\n    df[\"u_out_rolling_10\"] = df.groupby(\"breath_id\")[\"u_out\"].rolling(window=10).mean().reset_index(drop=True).fillna(0)\n    df[\"u_in_rolling_10\"] = df.groupby(\"breath_id\")[\"u_in\"].rolling(window=10).mean().reset_index(drop=True).fillna(0)\n    \n    df[\"u_in_max\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"max\")\n    df[\"u_in_min\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"min\")\n    df[\"u_in_mean\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"mean\")\n    df[\"u_out_max\"] = df.groupby(\"breath_id\")[\"u_out\"].transform(\"max\")\n    df[\"u_out_min\"] = df.groupby(\"breath_id\")[\"u_out\"].transform(\"min\")\n    df[\"u_out_mean\"] = df.groupby(\"breath_id\")[\"u_out\"].transform(\"mean\")\n    \n    for i in range(4):\n        df[\"u_in\"+f\"_diff{i+1}\"] = df[\"u_in\"] - df[f\"u_in_lag{i+1}\"]\n        df[\"u_in\"+f\"_diff_back{i+1}\"] = df[\"u_in\"] - df[f\"u_in_back{i+1}\"]\n\n        df[\"u_out\"+f\"_diff{i+1}\"] = df[\"u_out\"] - df[f\"u_out_lag{i+1}\"]\n        df[\"u_out\"+f\"_diff_back{i+1}\"] = df[\"u_out\"] - df[f\"u_out_back{i+1}\"]\n\n    df[\"u_in_diff_max\"] = df[\"u_in_max\"] - df[\"u_in\"]\n    df[\"u_in_diff_min\"] = df[\"u_in_min\"] - df[\"u_in\"]\n    df[\"u_in_diff_mean\"] = df[\"u_in_mean\"] - df[\"u_in\"]\n    \n    df[\"cross\"] = df[\"u_in\"] * df[\"u_out\"]\n    df[\"cross2\"] = df[\"time_step\"] * df[\"u_out\"]\n    \n    df[\"time_class\"] = df.groupby(\"breath_id\").cumcount()\n    df[\"R\"] = df.R.astype(str)\n    df[\"C\"] = df.C.astype(str)\n    df[\"R_C\"] = df.R + \"_\" + df.C \n    gc.collect()\n    return df\n\ndef group_feature(train, test) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    # time_class x u_in\n    time_grp = train.groupby(\"time_class\").mean().loc[:, [\"u_in\"]]\n    time_grp = time_grp.rename(columns={\"u_in\": \"u_in_time_class\"})\n    train = pd.merge(train, time_grp, how=\"left\", left_on=\"time_class\", right_index=True)\n    test = pd.merge(test, time_grp, how=\"left\", left_on=\"time_class\", right_index=True)\n    del time_grp \n    gc.collect()\n    \n    print(1)\n    \n    # R x u_in \n    r = train.groupby(\"R\").mean().loc[:, [\"u_in\"]]\n    r = r.rename(columns={\"u_in\": \"u_in_r_mean\"})\n    train = pd.merge(train, r, how=\"left\", left_on=\"R\", right_index=True)\n    test = pd.merge(test, r, how=\"left\", left_on=\"R\", right_index=True)\n    del r \n    gc.collect()\n\n    \n    # c x u_in \n    c = train.groupby(\"C\").mean().loc[:, [\"u_in\"]]\n    c = c.rename(columns={\"u_in\": \"u_in_c_mean\"})\n    train = pd.merge(train, c, how=\"left\", left_on=\"C\", right_index=True)\n    test = pd.merge(test, c, how=\"left\", left_on=\"C\", right_index=True)\n    del c \n    gc.collect()\n    \n    print(2)\n\n    # r_c x u_in \n    rc = train.groupby(\"R_C\").mean().loc[:, [\"u_in\"]]\n    rc = rc.rename(columns={\"u_in\": \"u_in_rc_mean\"})\n    train = pd.merge(train, rc, how=\"left\", left_on=\"R_C\", right_index=True)\n    test = pd.merge(test, rc, how=\"left\", left_on=\"R_C\", right_index=True)\n    del rc \n    gc.collect()\n    \n    print(3)\n\n    # r_c, time_class x u_in \n    rc = train.groupby([\"R_C\", \"time_class\"]).mean().loc[:, [\"u_in\"]]\n    rc = rc.rename(columns={\"u_in\": \"u_in_rc_time_mean\"})\n    train = pd.merge(train, rc, how=\"left\", left_on=[\"R_C\", \"time_class\"], right_index=True)\n    test = pd.merge(test, rc, how=\"left\", left_on=[\"R_C\", \"time_class\"], right_index=True)\n    del rc \n    gc.collect()\n    \n    print(4)\n\n    \n    # get dummmies object\n    last_train_shape = train.shape[0]\n    y = train.pressure.values.ravel()\n    df = pd.concat([train.drop(\"pressure\", axis=1), test])\n    df = pd.get_dummies(data=df, columns=[\"R\", \"C\", \"R_C\"])\n    train, test = df.iloc[:last_train_shape, :], df.iloc[last_train_shape:, :]\n    del df \n    train[\"pressure\"] = y \n    del y \n    gc.collect()\n    return train, test ","5a72d1c4":"%%time \n\ntrain = lag_feature(train)\ntest = lag_feature(test)","0ffbaacd":"%%time \n\ntrain, test = group_feature(train, test)","db0568c2":"@tf.custom_gradient\ndef round_with_gradients(x):\n    def grad(dy):\n        return dy\n    return tf.round(x), grad\n\nclass ScaleLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super(ScaleLayer, self).__init__()\n        self.min = tf.constant(PRESSURE_MIN, dtype=np.float32)\n        self.max = tf.constant(PRESSURE_MAX, dtype=np.float32)\n        self.step = tf.constant(PRESSURE_STEP, dtype=np.float32)\n\n    def call(self, inputs):\n        steps = tf.math.divide(tf.math.add(inputs, -self.min), self.step)\n        int_steps = round_with_gradients(steps)\n        rescaled_steps = tf.math.add(tf.math.multiply(int_steps, self.step), self.min)\n        clipped = tf.clip_by_value(rescaled_steps, self.min, self.max)\n        return clipped\n    \ndef build_model(input_shape):\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n    for hidden in [1024, 512, 256, 128]:\n        model.add(Bidirectional(LSTM(hidden ,return_sequences=True)))\n    model.add(Dense(128, activation=\"selu\"))\n    model.add(Dense(1))\n    model.add(ScaleLayer())\n    model.compile(optimizer=\"adam\", loss=\"mae\")\n    return model \n\nmodel = build_model((80, train.shape[-1]))\nmodel.summary()","df67699f":"if config[\"debug\"] is not True and config[\"device\"] == \"tpu\":\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n    # instantiate a distribution strategy\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n\ndef scaler(tr, va, te):\n    RS = RobustScaler()\n    return RS.fit_transform(tr), RS.transform(va), RS.transform(te)\n\n\ndef mae(pred, corr):\n    return np.mean(np.abs(pred - corr))\n\n\ndef submit(pred, name):\n    sub = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/sample_submission.csv\")\n    sub[\"pressure\"] = pred \n    sub.to_csv(f\"submission_lstm_{name}.csv\", index=False)\n    del sub \n    \n    \ndef callbacks_tools(fold) -> Tuple[object, object, object]:\n    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n    es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, \n                           mode=\"min\", restore_best_weights=True)\n    os.makedirs(\"models\", exist_ok=True)\n    checkpoint_filepath = f\"models\/{fold}.hdf5\"\n    sv = keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n            save_weights_only=False, mode='auto', save_freq='epoch',\n            options=None\n    )\n    wb = WandbCallback(log_weights=True)\n    return lr, es, sv, wb \n\n\ndef viz_predict(corr, pred):\n    plt.figure(figsize=(15, 6))\n    \n    plt.subplot(121)\n    sns.histplot(corr)\n    plt.title(\"Label\")\n    \n    plt.subplot(122)\n    sns.histplot(pred)\n    plt.title(\"Predict\")\n    \n    plt.show()\n    \ndef train_nn(train, test):\n    with tpu_strategy.scope():\n        k = keras.backend\n        \n        predict_val, val_idx, predict_test = [], [], []\n        kf = GroupKFold(config[\"n_fold\"])\n\n        for fold, (tr, va) in enumerate(kf.split(train, train.pressure, train.breath_id)):\n            print(f\"=====================fold: {fold+1}=========================\")\n            x_train, x_val = train.iloc[tr].drop([\"id\", \"breath_id\", \"pressure\"], axis=1), train.iloc[va].drop([\"id\", \"breath_id\", \"pressure\"], axis=1)\n            y_train, y_val = train.iloc[tr][\"pressure\"], train.iloc[va][\"pressure\"]\n            use_col = x_train.columns \n            x_test = test[use_col]\n\n            # transform shape \n            x_train, x_val, x_test = scaler(x_train, x_val, x_test)\n            x_train = x_train.reshape(-1, 80, len(use_col))\n            x_val = x_val.reshape(-1, 80, len(use_col))\n            x_test = x_test.reshape(-1, 80, len(use_col))\n            y_train = y_train.values.reshape(-1, 80, 1)\n            y_val = y_val.values.reshape(-1, 80, 1)\n\n            # setup models\n            model = build_model((80, len(use_col)))\n            lr, es, sv, wb = callbacks_tools(fold+1)\n\n            # training step \n            model.fit(x_train, \n                     y_train, \n                     validation_data=(x_val, y_val), \n                      callbacks=[lr, es, sv, wb], \n                      epochs= 1 if config[\"debug\"] else config[\"epoch\"],\n                      batch_size=config[\"batch_size\"])\n\n            # prediction val test \n            pred_v = model.predict(x_val, batch_size=config[\"batch_size\"], verbose=config[\"verbose\"]).squeeze().reshape(-1, 1).squeeze()\n            pred_t = model.predict(x_test, batch_size=config[\"batch_size\"], verbose=config[\"verbose\"]).squeeze().reshape(-1, 1).squeeze()\n            predict_val.append(pred_v)\n            predict_test.append(pred_t)\n            val_idx.append(va)\n\n            print(f\"fold: {fold+1} | MAE: {mae(pred_v, y_val.squeeze().reshape(-1, 1).squeeze())}\")\n\n            del x_train, x_val, x_test, model\n\n        # concat prediction \n        predict_val = np.concatenate(predict_val)\n        val_idx = np.concatenate(val_idx)\n        val_idx = np.argsort(val_idx)\n        predict_val = predict_val[val_idx]\n        del val_idx\n\n        # finally cv score \n        print(\"#############################################################\")\n        print(f\"LSTM CV: {mae(predict_val, train.pressure.values.ravel())}\")\n        print(\"#############################################################\")\n\n        # predict test transform\n        predict_mean = np.mean(predict_test, 0)\n        predict_median = np.median(predict_test, 0)\n        #### \n        predict_mean_clip = (np.round(predict_mean - PRESSURE_MIN)\/PRESSURE_STEP) * PRESSURE_STEP + PRESSURE_MIN\n        predict_mean_clip = np.clip(predict_mean_clip, PRESSURE_MIN, PRESSURE_MAX)\n\n        predict_median_clip = (np.round(predict_median - PRESSURE_MIN)\/PRESSURE_STEP) * PRESSURE_STEP + PRESSURE_MIN\n        predict_median_clip = np.clip(predict_median_clip, PRESSURE_MIN, PRESSURE_MAX)\n        ### \n\n        # submit \n        if config[\"debug\"] is not True:\n            submit(predict_mean, \"mean\")\n            submit(predict_median, \"median\")\n            submit(predict_mean_clip, \"mean_clip\")\n            submit(predict_median_clip, \"median_clip\")\n\n        gc.collect()\n        slack(\"lstm done.\")\n        return predict_val \n","4ff9701a":"pred_v = train_nn(train, test)","562aa553":"viz_predict(train.pressure.values.ravel(), pred_v)","63c9c449":"# Train ","e0fcb136":"# Model "}}