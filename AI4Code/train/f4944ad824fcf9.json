{"cell_type":{"56013668":"code","af544063":"code","9795f882":"code","813a0e92":"code","c1e81a3b":"code","f6b6b795":"code","3d6a8d05":"code","604b93c0":"code","a0b0ff41":"code","4fba1cf0":"code","03f7a33c":"code","df865f4b":"code","7f7fa935":"code","eec8e8cb":"code","492e34a4":"code","a90bcbbf":"code","4325eb0f":"code","0d284f19":"code","3ddf56fa":"code","5b4faa59":"code","0869915c":"code","85d8b033":"code","98754446":"code","9bb1d20c":"code","63d4c74f":"code","c30b4280":"code","5be3a4b5":"code","33946218":"code","81dcfa75":"code","2f0b6c2f":"code","1601b3a1":"code","64bb36ea":"code","b50bf9ce":"code","afc3e320":"code","4f947fed":"code","11d8dd69":"code","e2e30cb2":"code","77fd655c":"code","b6604cc7":"code","7eeb3ad9":"markdown","810bd235":"markdown","003e4dc4":"markdown","bee4e327":"markdown","c46c6322":"markdown","9e3b807b":"markdown","40831132":"markdown","5a7c4785":"markdown","2b02d18e":"markdown","c0ce1114":"markdown","24e5cd6f":"markdown","88a62e42":"markdown","ce1cc0b4":"markdown","dc03483a":"markdown","84bf1159":"markdown","d01942d6":"markdown","28671eb7":"markdown","e72b9f03":"markdown","04674533":"markdown","d199f227":"markdown","c5dac21c":"markdown","37a57d1c":"markdown","dcd50cdc":"markdown","e1105c29":"markdown"},"source":{"56013668":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.utils import resample\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,recall_score,roc_auc_score,roc_curve,precision_score,f1_score,auc,accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n","af544063":"customer_data = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest_data = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')","9795f882":"print(customer_data.shape)\nprint(test_data.shape)","813a0e92":"customer_data.head()","c1e81a3b":"test_data.head()","f6b6b795":"test_data.describe()","3d6a8d05":"customer_data['target'].value_counts().plot.bar()\nplt.show()","604b93c0":"print(customer_data.isnull().sum().any())\nprint(test_data.isnull().sum().any())","a0b0ff41":"\ncolumns = list(customer_data.columns)\ncolumns.remove('target')\ncolumns.remove('ID_code')\n\ntarget0_data = customer_data[customer_data['target']==0]\ntarget1_data = customer_data[customer_data['target']==1]\nplt.figure(figsize=(14,8))\nplt.title(\"Distribution of mean row data based on target \")\nsns.distplot(target0_data[columns].mean(axis=1),color='blue',kde=True,bins=100,label='target_0')\nsns.distplot(target1_data[columns].mean(axis=1),color='red',kde=True,bins=100,label='target_1')\nplt.legend()\nplt.show()","4fba1cf0":"corr_matrix = customer_data[columns].corr()\nsns.heatmap(corr_matrix)","03f7a33c":"corr = customer_data.corr()\nhigh_corr = np.where(corr>0.5)\nhigh_corr = [(corr(x),corr(y)) for x,y in zip(*high_corr) if x!=y and x<y]\nif len(high_corr)==0:\n    print(\"There are no correlated variables\")","df865f4b":"from sklearn.decomposition import PCA\nratio={}\nfor i in range(190,200):\n    pca=PCA(n_components=i).fit(customer_data[columns])\n    ratio[i]=sum(pca.explained_variance_ratio_)\n    \npd.Series(ratio).plot()\nplt.show()","7f7fa935":"def boxplot_func(data_frame,col):\n    sns.set(style=\"whitegrid\")\n    plt.title(\"Outliers\")\n    fig, ax = plt.subplots(10,10,figsize=(18,24))\n    counter=0\n    for c in col:\n        counter+=1\n        plt.subplot(10,10,counter)\n        sns.boxplot(data_frame[c])\n        plt.xlabel(c)\n        plt.tick_params(axis='x', labelsize=7, pad= -7)\n    plt.show()","eec8e8cb":"col = customer_data.columns.values[2:102]\nboxplot_func(customer_data,col)","492e34a4":"col=customer_data.columns.values[102:]\nboxplot_func(customer_data,col)","a90bcbbf":"copy_train_data = customer_data.copy()\ncopy_test_data = test_data.copy()","4325eb0f":"def outlier_removal(df):\n    for i in columns:\n        q75, q25 =np.percentile(df.loc[:,i],[75,25])\n        iqr  = q75-q25\n        min  = q25 - (iqr*1.5)\n        max  = q75 + (iqr*1.5)\n        df = df.drop(df[df.loc[:,i]<min].index) \n        df = df.drop(df[df.loc[:,i]>max].index)\n        return df","0d284f19":"customer_data = outlier_removal(customer_data)\n#test_data = outlier_removal(test_data) ## We don't need to remove outliers from test data\nprint(\"Total number of observations dropped in train set:\",copy_train_data.shape[0]-customer_data.shape[0])\ncustomer_data.shape","3ddf56fa":"customer_data['target'].value_counts()","5b4faa59":"class_0,class_1 = customer_data.target.value_counts()\n\ndf_class_0 = customer_data[customer_data['target']==0]\ndf_class_1 = customer_data[customer_data['target']==1]\n\nunder_df_0 = df_class_0.sample(class_1)\ndf_train_under = pd.concat([under_df_0,df_class_1],axis=0)\n\nprint(df_train_under.target.value_counts())\ndf_train_under.describe()","0869915c":"over_df = resample(df_class_1, replace=True, n_samples=179813,random_state=123)\n\ndf_train_over = pd.concat([over_df,df_class_0],axis=0)\n\nlen(df_train_over)\nprint(df_train_over.target.value_counts())\ndf_train_over.describe()","85d8b033":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train_under = df_train_under[columns]\ny_train_under = df_train_under['target']\n\nX_train_over = df_train_over[columns]\ny_train_over = df_train_over['target']\nprint(X_train_under.shape)\ntrain_x,test_x,train_y,test_y = train_test_split(X_train_under,y_train_under,train_size=0.8,random_state=42,stratify=y_train_under)\ntrain_y.shape","98754446":"print(train_y.value_counts())","9bb1d20c":"def metrics(y_true,y_pred):\n    print(\"Confusion Matrix\")\n    print(confusion_matrix(y_true,y_pred))\n    \n    print(\"Accuracy:\", accuracy_score(y_true,y_pred))\n    print(\"Precision:\", precision_score(y_true,y_pred))\n    print(\"F1 Score:\", f1_score(y_true,y_pred))\n    print(\"Recall:\", recall_score(y_true,y_pred))\n    \n    false_positive_rate,recall,thresholds = roc_curve(y_true,y_pred)\n    roc_auc = auc(false_positive_rate,recall)\n    \n    print(\"ROC:\",roc_auc)\n    \n    plt.plot(false_positive_rate,recall,'b')\n    plt.plot([0,1],[0,1],'r--')\n    plt.title(\"AUC=%0.2f\"%roc_auc)\n    plt.show()","63d4c74f":"logistic_model = LogisticRegression().fit(train_x,train_y)\nlogistic_predict = logistic_model.predict(train_x)\n\nprint(\"Metrics:\")\nmetrics(train_y,logistic_predict)","c30b4280":"logistic_predict_test = logistic_model.predict(test_x)\nprint(\"Metrics for test:\")\nmetrics(test_y,logistic_predict_test)","5be3a4b5":"tree = RandomForestClassifier(n_estimators=10,max_depth=7,random_state=1).fit(train_x,train_y)\ntree_train_predict = tree.predict(train_x)\n    \nprint(\"Metrics:\")\nmetrics(train_y,tree_train_predict)","33946218":"tree_test_predict = tree.predict(test_x)\nprint(\"Metrics:\")\nmetrics(test_y,tree_test_predict)","81dcfa75":"naive = GaussianNB().fit(train_x,train_y)\nnaive_train_predict = naive.predict(train_x)\nprint(\"Metrics:\")\nmetrics(train_y,naive_train_predict)","2f0b6c2f":"naive_test_predict = naive.predict(test_x)\nprint(\"Metrics:\")\nmetrics(test_y,naive_test_predict)","1601b3a1":"models = []\nmodels.append((\"LogisticRegression\",LogisticRegression()))\nmodels.append((\"Random Forest\",RandomForestClassifier(n_estimators=10,max_depth=7,random_state=1)))\nmodels.append((\"NaiveBayes\",GaussianNB()))\n\n","64bb36ea":"def model_test(x_data,y_data):\n    for name,model in models:\n        train_x,test_x,train_y,test_y = train_test_split(x_data,y_data,train_size=0.75,random_state=42,stratify=y_data)\n        print(\"#\"*10,\"Validation for %s \"%name,\"#\"*10)\n        model.fit(train_x,train_y)\n        metrics(train_y,model.predict(train_x))\n        pred = model.predict(test_x)\n        print(\"Testing Metrics of %s\"%name)\n        metrics(test_y,pred)\n","b50bf9ce":"model_test(X_train_over,y_train_over)","afc3e320":"from sklearn.inspection import permutation_importance\n\nimps = permutation_importance(naive, test_x, test_y)\nimportances = imps.importances_mean\nstd = imps.importances_std\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\nfor f in range(test_x.shape[1]):\n    print(\"%d. (%f)\" % (f + 1, importances[indices[f]]))","4f947fed":"feature = pd.DataFrame({\"imp\":importances,\"col\":columns})\nfeature = feature.sort_values(['imp','col'],ascending=[True,False]).iloc[-30:]\nfeature.plot(kind='barh',x='col',y='imp',figsize=(10,7),legend=None)\nplt.title(\"Feature Importances\")\nplt.ylabel(\"Features\")\nplt.xlabel(\"Importances\")\nplt.show()","11d8dd69":"test_data.drop(['ID_code'],axis=1,inplace=True)\n\npredict = naive.predict(test_data)\n","e2e30cb2":"pd.Series(predict).value_counts().plot(kind='bar')","77fd655c":"sample_submission = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/sample_submission.csv')\nsample_submission['target'] = predict\n","b6604cc7":"print(sample_submission.head())\nsample_submission.to_csv('submission_1.csv',index=False)","7eeb3ad9":"####  We will look for correlation variables to decrease high dimensionality. We have tried to numerically show it as visually the plot would be too large. ","810bd235":"## 2. Oversampling","003e4dc4":"### 3) Naive Bayes","bee4e327":"### Data Exploration","c46c6322":"## Resampling\nNote we are dealing with a data set very unbalanced, where there is only 10% of records categorized with target 1, so those customers who have made a financial transaction. So we will try sampling the data","9e3b807b":"As we can see that there is a small variation in the mean of all feature that could explain the target variable.","40831132":"There is no missing value present","5a7c4785":"### Missing Value Analysis","2b02d18e":"This is an anonymised dataset with 199 discrete numeric variables, with a dependent variable labeled as a binary variable and a column in string format with an identifier label. Two training datasets are provided, a training dataset and evaluation dataset, but no target variable so that for our purpose we won't use it to train the models. The task that is requested in this challenge is to predict the value of the target column in the test set.","c0ce1114":"### Data Extraction","24e5cd6f":"### Loading the libraries","88a62e42":"## PCA","ce1cc0b4":"#### It is observed that we require whole 200 features to explain the variance. So we will not be applying PCA here.","dc03483a":"### 2) Random Forest","84bf1159":"## Checking the distribution\nGet an idea of this data distribution, we review in the training dataset that we will work with, we review the histogram of the mean values of each record based on the binary target variable","d01942d6":"## Outlier","28671eb7":"## Feature Importance","e72b9f03":"## Content\n#### 1)Libraries\n#### 2)Data extraction\n#### 3)Data exploration\n#### 4)Unbalanced Data and Resampling\n#### 5)Feature selection\n#### 6) Classification models\n#### 7)Detection of the most influential variables","04674533":"### We have imbalance data.","d199f227":"We try to detect potential correlated variables to decrease high dimensionality. As the correlation matrix is too large visually as seen above, we tried to numerically detect the existence of correlations above 0.5 and below -0.5.","c5dac21c":"## 1. Under Sampling","37a57d1c":"## Testing with Undersampling Data\n\n### 1) Logistic Regression","dcd50cdc":"## Oversampling Data","e1105c29":"## Classification Models"}}