{"cell_type":{"65c833a8":"code","d1d76d2b":"code","4f1c646e":"code","a2f2b7bf":"code","8c59e3ac":"code","5568e500":"code","53e8aeb3":"code","e09ef1b9":"code","74d1097b":"code","c1b8aad4":"code","581b07dd":"code","5f13bc71":"code","0c01b53f":"code","01a69e38":"code","0fdfca14":"code","8b0b2a15":"code","ffc7b7d2":"code","1f287590":"code","71432b15":"code","2b77f297":"code","2fa197bb":"code","b245c10d":"code","341f4948":"code","64aabd85":"code","3c4bd782":"code","85353f7a":"code","955ebb1e":"code","dc58fcfc":"code","89a8ab07":"code","d0e2311b":"code","8b9fac9d":"code","a1923bf0":"code","ed1ec333":"code","fa79d845":"code","31e92397":"code","7bed2ede":"code","e256ff1c":"code","3a30f892":"code","1fe2144b":"code","5efeb123":"code","ec715e5d":"code","5a6996f9":"code","c6dec75b":"code","e732c828":"code","491f1be4":"code","7d718675":"code","52c870fd":"code","d1b94238":"code","f272348d":"code","7ac21f64":"code","71325b0f":"code","6dc559b9":"code","f76e0ad5":"code","9ebe6a9a":"code","6ffc3cf5":"code","322080d2":"code","b4fb53eb":"code","10ad5f36":"code","6048ff5b":"code","25b84505":"code","50d01bc9":"code","1f36b473":"code","23889b2c":"code","ac15735f":"code","6b30345d":"code","5e241433":"code","69d93824":"code","f0dc0171":"code","e43e0485":"code","05659f2f":"code","a6fd7c2b":"code","b68f2f54":"code","0ec0835e":"code","894576f7":"code","e865e76e":"code","69e1d152":"code","cc56a9be":"code","b7cb4321":"code","1b899570":"code","843acc5e":"code","23006242":"code","34b36785":"code","91acb0b2":"code","879b55e2":"code","cdb2a1b5":"markdown","d5a6b1c2":"markdown","549c604e":"markdown","3fe365a7":"markdown","76e4cc93":"markdown","bb194e96":"markdown","0288a1b9":"markdown","9bb2cba8":"markdown","90e115ea":"markdown","9f600d3e":"markdown","e93ddfec":"markdown","7d3cff3d":"markdown","5e34684d":"markdown","daff3485":"markdown","feae5091":"markdown","2166c1c5":"markdown","1e35091a":"markdown","a893a4d0":"markdown","bfabf209":"markdown","863898fd":"markdown","466062a7":"markdown","78d9c7d2":"markdown","b2dbb297":"markdown","60ccf6eb":"markdown","dd375018":"markdown","748bd77f":"markdown","586791f4":"markdown","0c6fe6c8":"markdown","4ee1875e":"markdown","bcaeabb7":"markdown","dc157ba9":"markdown","7d9f80e7":"markdown","ffced8cc":"markdown","6a304c49":"markdown","69cf767b":"markdown","5d881ed5":"markdown","15c7ce86":"markdown","80f692e1":"markdown","47b211ac":"markdown","9fbae958":"markdown","68458e66":"markdown","3d490e4e":"markdown","744e47ed":"markdown","705e5a5b":"markdown","f051a550":"markdown","4db2df4b":"markdown","62ab423d":"markdown","901bce55":"markdown","c8aa50cd":"markdown","a320acca":"markdown","1ea46811":"markdown","68f4e278":"markdown","baf5c776":"markdown","a92de4a6":"markdown","5dc6bed4":"markdown","c649ee8f":"markdown","d6b1025f":"markdown"},"source":{"65c833a8":"# General tools\nimport pandas as pd\nimport numpy as np\nimport os, math\nfrom collections import Counter\n\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nmorancolor=sns.color_palette(['#6a2202', '#bc7201', '#e5ab09', '#22180d', '#0f1a26','#241c24', '#745656', '#c7b44f', '#977f48', '#392c23'])\n# sns.set_theme(palette=morancolor)\nplt.style.use(\"fivethirtyeight\")\nplt.style.use('grayscale') \n\nplt.rcParams['font.family']='serif'\nplt.rcParams['figure.dpi'] =100 # high resolution\n\n# Manage warnings\nimport warnings\nwarnings.filterwarnings('ignore')","d1d76d2b":"df=pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndf.dropna(how='all',inplace=True)\ndf.drop_duplicates(inplace=True)\ndf.shape","4f1c646e":"df.sample(4)","a2f2b7bf":"df.info()","8c59e3ac":"df.nunique().sort_values(ascending=False)","5568e500":"catcol=[\"sex\", \"fbs\", \"exng\", \"restecg\", \"slp\", \"cp\", \"thall\", \"caa\"]\nnumcol=[\"chol\", \"thalachh\", \"trtbps\", \"age\", \"oldpeak\"]","53e8aeb3":"df.describe()","e09ef1b9":"def despine():\n    sns.despine(top=1,bottom=1,right=1,left=1)\n    \ndef title(title,fontsize=13):\n    plt.title(title,fontweight='bold',fontsize=fontsize)","74d1097b":"def countall(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,annotsize=10,color=None):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        sns.countplot(df[col],color=color)\n        for p in ax.patches:\n            ax.annotate(f\"{p.get_height()\/df[col].shape[0]*100:.2f}%\",xy=[p.get_x(),p.get_height()],fontsize=annotsize)\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold')","c1b8aad4":"from scipy.stats import skew\ndef kdeall(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,meanskew=True,kdecut=0,legendsize=10,xlabelsize=13,loc='best'):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if meanskew==True:\n            sns.kdeplot(df.dropna(subset=[col])[col],cut=kdecut,label=f'Skewness: {skew(df.dropna(subset=[col])[col]):.2f}',lw=3)\n            plt.axvline(df.dropna(subset=[col])[col].mean(),label='mean',color='#22180d',lw=1.5)\n            plt.axvline(df.dropna(subset=[col])[col].median(),label='median',ls='--',color='#22180d',lw=1.5)\n            plt.legend(fontsize=legendsize,loc=loc)\n        else: sns.kdeplot(df.dropna(subset=[col])[col],cut=kdecut,lw=3)\n        sns.rugplot(df.dropna(subset=[col])[col])\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold',fontsize=xlabelsize)","581b07dd":"def boxall(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,target=None,choose=1,xlabelsize=12,showmeans=True):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if target==None: sns.boxplot(df[col],showmeans=showmeans)\n        else: \n            if choose==1: sns.boxplot(df[target],df[col],showmeans=showmeans)\n            else: sns.boxplot(df[col],df[target],showmeans=showmeans)\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold',fontsize=xlabelsize)","5f13bc71":"def pointall(df,lst,target,h=4,w=10,cut=3,hspace=.5,wspace=.25,annotsize=10,choose=1):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if choose==1: sns.pointplot(x=df[target],y=df[col],lw=3)\n        else: sns.pointplot(x=df[col],y=df[target],lw=3)\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold')","0c01b53f":"def kde2(df,lst,target,h=4,w=10,cut=3,hspace=.5,wspace=.25,annotsize=10):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        sns.kdeplot(df[col],hue=df[target],cut=0,lw=3)\n        sns.rugplot(df[col],hue=df[target])\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold')","01a69e38":"def barall(df,lst,target,h=4,w=10,cut=3,hspace=.5,wspace=.25,color=None):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        sns.barplot(df[col],df[target],color=color)        \n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold')","0fdfca14":"df.corr().output.abs().sort_values(ascending=False)[1:][::-1].plot(kind='barh',figsize=(7,5))","8b0b2a15":"print(df.output.value_counts(1))\nsns.catplot(data=df,y=\"output\",kind='count',height=4,aspect=2)\ntitle('Distribution of Output',fontsize=17)","ffc7b7d2":"countall(df,catcol,h=10,annotsize=8,color=\"#221f1a\")\nplt.suptitle('Distribution of our categorical features',fontweight='bold',fontsize=17)\ndespine()","1f287590":"kdeall(df,numcol,h=10,cut=2)\nplt.suptitle('Distribution of our continuous features',fontweight='bold',fontsize=17)\ndespine()","71432b15":"boxall(df,numcol)","2b77f297":"pointall(df,numcol,\"output\")","2fa197bb":"pointall(df,catcol,'output',choose=0,h=7)","b245c10d":"from scipy.stats import ttest_ind\ndef checking(x):\n    stat,p=ttest_ind(df[df.output==0][x],df[df.output==1][x])\n    return p","341f4948":"col=df.corr().output.abs().sort_values(ascending=False)[1:].index.tolist()\npd.Series({i:checking(i) for i in col}).sort_values(ascending=False).plot(kind='barh',figsize=(7,5))\nplt.axvline(x=.05)\nplt.show()\n\npd.Series({i:checking(i) for i in col}).sort_values(ascending=False)","64aabd85":"kde2(df,numcol,\"output\",h=7,hspace=.3)","3c4bd782":"boxall(df,numcol,target='output',h=9,hspace=.25)","85353f7a":"barall(df,catcol,'output',h=10,hspace=.3,color='#221f1a')","955ebb1e":"df.corr().output.abs().sort_values(ascending=False)[1:][::-1].plot(kind='barh',figsize=(7,5))","dc58fcfc":"f=plt.figure(figsize=(10,5))\nmask = np.triu(np.ones_like(df.corr()))\nsns.heatmap(df.corr(),cmap=sns.color_palette(['#231f1e', '#47423e', '#7e736f', '#b3a59c', '#ffffff'][::-1]),\n                                             vmax=1,vmin=-1,mask=mask,annot=True,fmt='.2f',annot_kws={\"size\":8})\nplt.yticks(fontsize=9)\nplt.xticks(fontsize=9)\nplt.show()","89a8ab07":"sns.pairplot(df,hue='output',corner=True)","d0e2311b":"f,ax=plt.subplots(1,2,figsize=(10,4))\nsns.countplot(df.sex,hue=df.output,ax=ax[0])\nsns.violinplot(df.sex,df.age,hue=df.output,split=True,scale='count')\nplt.suptitle('Age and Sex',fontweight='bold',fontsize=17)\nplt.show()","8b9fac9d":"sns.pointplot(data=df,x='sex',y='age',hue='output')","a1923bf0":"f=plt.figure(figsize=(9,2.5))\nsns.lineplot(df.age,df.trtbps,hue=df.output,lw=3)\ntitle('Age and Resting blood pressure')","ed1ec333":"f=plt.figure(figsize=(9,2.5))\nsns.lineplot(df.age,df.thalachh,hue=df.output,lw=3)\ntitle('Age and Maximum heart rate')","fa79d845":"f,ax=plt.subplots(1,2,figsize=(10,4))\nsns.pointplot(df.slp,df.oldpeak,hue=df.output,ax=ax[0])\nsns.boxplot(df.slp,df.oldpeak,hue=df.output)\nplt.suptitle('Oldpeak and Slp',fontweight='bold',fontsize=17)\ndespine()","31e92397":"f,ax=plt.subplots(1,2,figsize=(10,4))\nsns.pointplot(df.exng,df.thalachh,hue=df.output,ax=ax[0])\nsns.boxplot(df.exng,df.thalachh,hue=df.output)\nplt.suptitle('Thalachh (Maximum heart rate achieved) and Exng',fontweight='bold',fontsize=17)\ndespine()","7bed2ede":"f,ax=plt.subplots(1,2,figsize=(10,4))\nsns.pointplot(df.cp,df.thalachh,hue=df.output,ax=ax[0])\nsns.boxplot(df.cp,df.thalachh,hue=df.output)\nplt.suptitle('Thalachh and Cp',fontweight='bold',fontsize=17)\ndespine()","e256ff1c":"f,ax=plt.subplots(1,2,figsize=(10,4))\nsns.pointplot(df.slp,df.thalachh,hue=df.output,ax=ax[0])\nsns.boxplot(df.slp,df.thalachh,hue=df.output)\nplt.suptitle('Thalachh and Slp',fontweight='bold',fontsize=17)\ndespine()","3a30f892":"f,ax=plt.subplots(1,2,figsize=(10,4))\nsns.pointplot(df.cp,df.output,hue=df.exng,ax=ax[0])\nsns.scatterplot(df.thalachh,df.oldpeak,hue=df.output)\nax[0].set_title('Exng and Cp',fontweight='bold')\nax[1].set_title('Thalachh and Oldpeack',fontweight='bold')\ndespine()","1fe2144b":"# save our orginal features set\norgset=df.corr().output.abs().sort_values(ascending=False)[1:].index.tolist()\norgset","5efeb123":"f=plt.figure(figsize=(10,5))\nmask = np.triu(np.ones_like(df.corr()))\nsns.heatmap(df.corr(),cmap=sns.color_palette(['#231f1e', '#47423e', '#7e736f', '#b3a59c', '#ffffff'][::-1]),\n                                             vmax=1,vmin=-1,mask=mask,annot=True,fmt='.2f',annot_kws={\"size\":8})\nplt.yticks(fontsize=9)\nplt.xticks(fontsize=9)\nplt.show()","ec715e5d":"def outliers(x,fence=1.5):\n    q1,q3=np.percentile(df[x],25), np.percentile(df[x],75)\n    iqr=q3-q1\n    print(f\"{x}: {len(df[(df[x]<q1-fence*iqr)|(df[x]>q3+1.5*iqr)])} outliers\")\n    return df[(df[x]<q1-fence*iqr)|(df[x]>q3+1.5*iqr)].index.tolist()","5a6996f9":"sub=df.nunique().sort_values(ascending=False)\nsub=sub[sub>5].index.tolist()","c6dec75b":"sub=df.nunique().sort_values(ascending=False)\nsub=sub[sub>5].index.tolist()\nboxall(df,sub,cut=3,h=5)","e732c828":"ind=[outliers(i) for i in [\"chol\",\"thalachh\",\"trtbps\",\"oldpeak\"]]\nprint(f\"\\nWe are going to drop {len(list(set([j for i in ind for j in i])))} outliers\")\ndf.drop(index=list(set([j for i in ind for j in i])),inplace=True)","491f1be4":"boxall(df,sub,cut=3,h=5)","7d718675":"sub=df.nunique().sort_values(ascending=False)\nsub=sub[sub>5].index.tolist()\nkdeall(df,sub,h=7)","52c870fd":"# let's make a copy of our dataset before scaling\ndf2=df.copy()","d1b94238":"from sklearn.preprocessing import StandardScaler\nstdfeature=[\"chol\",\"thalachh\",\"trtbps\",\"age\"]\nscl=StandardScaler()","f272348d":"from sklearn.preprocessing import Normalizer\ndf.oldpeak=Normalizer().fit_transform(df.oldpeak.values.reshape(-1,1))","7ac21f64":"kdeall(df,sub,h=7)","71325b0f":"import statsmodels.api as sm\ndef select_by_pvalue(target,df,fence=.05,w=7,h=5,textw=.5,texth=.2):\n    mod=sm.OLS(df[target],df.drop(target,axis=1))\n    fii=mod.fit()\n    sub=fii.summary2().tables[1][\"P>|t|\"].sort_values()\n    sub.plot(kind='barh',figsize=(w,h))\n    pvalue_set=sub[sub<=fence].index.tolist()\n    plt.axvline(x=fence,lw=3,ls='--')\n    plt.figtext(textw, texth, 'dropped',fontsize=15, fontweight='bold')\n    return pvalue_set","6dc559b9":"pvalue_set=select_by_pvalue('output',df,fence=.1)\npvalue_set","f76e0ad5":"from statsmodels.tools.tools import add_constant\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef vif(x):\n    X=add_constant(x)\n    return pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])],index=X.columns).sort_values(ascending=False)","9ebe6a9a":"vif(df.drop(columns='output'))","6ffc3cf5":"from sklearn.decomposition import PCA\ndef reduct_pca(X,w=8,h=3):\n    pca=PCA(n_components=X.shape[1],random_state=0)\n    Xpca=pca.fit_transform(X)\n    f=plt.figure(figsize=(w,h))\n    plt.bar(range(1,X.shape[1]+1),pca.explained_variance_ratio_,label='individual explained variance')\n    plt.step(range(1,X.shape[1]+1), np.cumsum(pca.explained_variance_ratio_),where='mid',label='cumulative explained variance')\n    plt.xticks(range(1,X.shape[1]+1))\n    plt.xlabel('# of principal components')\n    plt.ylabel('explained variance ratio',fontsize=13)\n    plt.yticks(fontsize=12)\n    plt.legend(fontsize=12,loc='best')\n    despine()\n    return Xpca","322080d2":"Xpca=pd.DataFrame(reduct_pca(X=df.drop('output',axis=1))[:,:8])\nXpca.shape","b4fb53eb":"X=df.drop('output',axis=1)\ny=df.output\nX.shape,y.shape","10ad5f36":"from sklearn.model_selection import StratifiedKFold\ndef check(clf,X,y=y,n_splits=10):\n    kf=StratifiedKFold(n_splits=n_splits,random_state=0,shuffle=True)\n    k_tracc,k_teacc=[],[]\n    for (tr,te) in kf.split(X,y):\n        clf.fit(X.iloc[tr],y.iloc[tr])\n        k_tracc.append(clf.score(X.iloc[tr],y.iloc[tr]))\n        k_teacc.append(clf.score(X.iloc[te],y.iloc[te]))\n    print(f\"Train score: {np.mean(k_tracc)}\")\n    print(f\"Test score: {np.mean(k_teacc)}\")\n    return clf","6048ff5b":"def selectmoran(modellst,X,y=y,n_splits=10,random_state=0):\n    kf=StratifiedKFold(n_splits=n_splits,random_state=random_state,shuffle=True)\n    namelst,imp,tr_acc,te_acc=[],[],[],[]\n    for clf in modellst:\n        namelst.append(type(clf).__name__)\n        k_tracc, k_teacc, k_f1, k_imp=[],[],[],[]\n        for (tr,te) in kf.split(X,y): # train, test index\n            clf.fit(X.iloc[tr],y.iloc[tr])\n            k_tracc.append(clf.score(X.iloc[tr],y.iloc[tr]))\n            k_teacc.append(clf.score(X.iloc[te],y.iloc[te]))\n            if hasattr(clf,\"feature_importances_\"): k_imp.append(clf.feature_importances_)\n        tr_acc.append(np.mean(k_tracc))\n        te_acc.append(np.mean(k_teacc))\n        if len(k_imp)==0: imp.append(False)\n        else: imp.append(np.mean(k_imp,axis=0))\n    score=pd.DataFrame({'Model':namelst,'Train_accuracy':tr_acc,'Test_accuracy':te_acc}).sort_values('Test_accuracy',ascending=False)\n    return score,imp","25b84505":"def plotscoring(score,title,w=7,h=5,alpha=.97,axvline=.8,yticksize=12):\n    f,ax=plt.subplots(figsize=(w,h))\n    print(f\"Mean accuracy for all models: {np.mean(score.Test_accuracy)}\\n\")\n    print(score)\n    sns.barplot(x=score.Test_accuracy,y=score.Model,alpha=alpha,color='#47423e')\n    sns.barplot(x=-score.Train_accuracy,y=score.Model,alpha=alpha,color='#231f1e')\n    ax.set_xlim(-1,1)\n    plt.axvline(x=0,color='black')\n    plt.xlabel('Train\/ Test accuracy')\n    plt.ylabel('')\n    plt.title(f\"Model score for {title}\",fontweight='bold')\n    plt.axvline(x=axvline,ls=':')\n    plt.yticks(fontsize=yticksize)\n    despine()","50d01bc9":"from sklearn.model_selection import GridSearchCV\ndef grid(clf,params,X,y=y,cv=10):\n    grid=GridSearchCV(clf,params,cv=cv)\n    grid.fit(X,y)\n    print(f\"Best score: {grid.best_score_}\")\n    print(f\"Best params: {grid.best_params_}\")","1f36b473":"def plotting_importances(score,imp,X=X,w=8,h=3,rotation=90,xsize=10):\n    for (a,b) in zip(score.Model,imp):\n        if b is not False:\n            ind=np.argsort(b)[::-1]\n            cols=X.columns\n            plt.figure(figsize=(w,h))\n            plt.title(f\"Feature importances via {a}\",fontweight='bold',fontsize=13)\n            plt.bar(range(X.shape[1]),b[ind])\n            plt.xticks(range(X.shape[1]),cols[ind],rotation=rotation,fontsize=xsize)\n            plt.xlim([-1,X.shape[1]])\n            plt.tight_layout()","23889b2c":"from sklearn.linear_model import LogisticRegression,Perceptron,SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier,BaggingClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier,XGBRFClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC,LinearSVC\n\nmodellst=[LogisticRegression(solver='liblinear'), Perceptron(), SGDClassifier(), \n          RandomForestClassifier(), GradientBoostingClassifier(),\n          AdaBoostClassifier(), BaggingClassifier(), CatBoostClassifier(), XGBClassifier(), XGBRFClassifier(),\n          GaussianNB(), LGBMClassifier(), SVC()]","ac15735f":"score_orgset,imp_orgset=selectmoran(modellst,X=X)","6b30345d":"plotscoring(score_orgset,'Initial features set')","5e241433":"plotting_importances(score_orgset,imp_orgset)","69d93824":"pvalue_set","f0dc0171":"score_pvalueset,imp_pvalueset=selectmoran(modellst,X[pvalue_set],y)","e43e0485":"plotscoring(score_pvalueset,'P-value subset')","05659f2f":"score_pca,imp_pca=selectmoran(modellst,Xpca,y)","a6fd7c2b":"plotscoring(score_pca,'PCA subset')","b68f2f54":"def plot_meanscore(meanscore_set):\n    print(meanscore_set)\n    f=plt.figure(figsize=(8,3))\n    meanscore_set.Test_accuracy[::-1].plot(kind='barh',color='#221f1a',alpha=.95)\n    (-1*meanscore_set.Train_accuracy[::-1]).plot(kind='barh',alpha=.95)\n    plt.axvline(x=0)\n    despine()","0ec0835e":"meanscore=pd.DataFrame({'Test_accuracy':[np.mean(i.Test_accuracy) for i in [score_orgset,score_pca,score_pvalueset]],\n             'Train_accuracy':[np.mean(i.Train_accuracy) for i in [score_orgset,score_pca,score_pvalueset]]},\n             index=['Initial_features','PCA_subset','Pvalue_subset']).sort_values('Test_accuracy',ascending=False)\nmaxscore=pd.DataFrame({'Test_accuracy':[np.max(i.Test_accuracy) for i in [score_orgset,score_pca,score_pvalueset]],\n             'Train_accuracy':[np.max(i.Train_accuracy) for i in [score_orgset,score_pca,score_pvalueset]]},\n             index=['Initial_features','PCA_subset','Pvalue_subset']).sort_values('Test_accuracy',ascending=False)","894576f7":"plot_meanscore(meanscore)","e865e76e":"plot_meanscore(maxscore)","69e1d152":"plotscoring(score_orgset,'Initial features')","cc56a9be":"tuning_org=[GaussianNB(**{'var_smoothing': 5e-10}),\n           LogisticRegression(**{'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'},random_state=0),\n           CatBoostClassifier(**{'depth': 4, 'learning_rate': 0.03, 'n_estimators': 100}),\n           AdaBoostClassifier(**{'learning_rate': 0.05, 'n_estimators': 50}),\n           RandomForestClassifier(**{'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 3, 'n_estimators': 50},random_state=1),\n           GradientBoostingClassifier(** {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_leaf': 3, 'n_estimators': 80}),\n           XGBClassifier(**{'booster': 'dart', 'learning_rate': 0.03, 'max_depth': 2, 'n_estimators': 100, 'subsample': 0.8}),\n           SGDClassifier(**{'alpha': 0.0002, 'learning_rate': 'optimal', 'loss': 'perceptron', 'penalty': 'l1', 'validation_fraction': 0.1},random_state=1),\n           LGBMClassifier(),\n           BaggingClassifier(base_estimator=SVC(),n_estimators=100),\n           XGBRFClassifier(learning_rate=.03,n_estimators=300,subsample=.8)]\nscore_tuorg,imp_tuorg=selectmoran(tuning_org,X)","b7cb4321":"plotscoring(score_tuorg,'Hypertuning - Initial features')","1b899570":"plotscoring(score_pca,'PCA subset')","843acc5e":"tuning_pca=[XGBClassifier(**{'booster': 'gblinear', 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1}),\n           BaggingClassifier(base_estimator=SVC(),n_estimators=100),\n           SVC(**{'C': 1, 'degree': 3, 'gamma': 'scale', 'kernel': 'linear'}),\n           LogisticRegression(**{'C': 1, 'max_iter': 300, 'penalty': 'l2', 'solver': 'saga'}),\n           GaussianNB(**{'var_smoothing': 5e-10}),\n           RandomForestClassifier(** {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 3, 'n_estimators': 80}),\n           CatBoostClassifier(**{'depth': 4, 'learning_rate': 0.05, 'n_estimators': 100, 'subsample': 0.8}),\n           AdaBoostClassifier(learning_rate=0.3, n_estimators=150),\n           GradientBoostingClassifier(**{'learning_rate': 0.05, 'max_depth': 3, 'min_samples_leaf': 1, 'n_estimators': 150}),\n           LGBMClassifier(**{'boosting_type': 'goss', 'learning_rate': 0.03, 'max_depth': 5, 'n_estimators': 80, 'subsample': 1}),\n           Perceptron()]\nscore_tupca,imp_tupca=selectmoran(tuning_pca,Xpca,y)","23006242":"plotscoring(score_tupca,'Hypertuning - PCA subset')","34b36785":"meanscoretu=pd.DataFrame({'Test_accuracy':[np.mean(i.Test_accuracy) for i in [score_tuorg,score_tupca]],\n             'Train_accuracy':[np.mean(i.Train_accuracy) for i in [score_tuorg,score_tupca]]},\n             index=['Initial_features','PCA_subset']).sort_values('Test_accuracy',ascending=False)\nmaxscoretu=pd.DataFrame({'Test_accuracy':[np.max(i.Test_accuracy) for i in [score_tuorg,score_tupca]],\n             'Train_accuracy':[np.max(i.Train_accuracy) for i in [score_tuorg,score_tupca]]},\n             index=['Initial_features','PCA_subset']).sort_values('Test_accuracy',ascending=False)","91acb0b2":"plot_meanscore(meanscoretu)","879b55e2":"plot_meanscore(maxscoretu)","cdb2a1b5":"After hypertuning parameters, XGBClassifier show better performances in both initial and PCA features set with a test accuracy of about 84%","d5a6b1c2":"# 6. Hypertuning parameters","549c604e":"#### Continuous features","3fe365a7":"#### Oldpeak and Slp","76e4cc93":"## 3.1 Outliers detection","bb194e96":"- fbs seems not to be a good predictor","0288a1b9":"#### Maximum heart rate achieved and Exng","9bb2cba8":"In general, people with **more chance of heart attack** are more likely to have:\n- **Lower cholestoral** in mg\/dl fetched via BMI sensor (around 241 mg\/dl)\n- **Higher maximum heart rate achieved** (around 160, while healthy people have maximum heart rate achieved of about 140)\n- **Lower resting blood pressure** (around 129 mm Hg)\n- **Younger**\n- **Lower oldpeak** (around 0.6)\n- Sex as 0\n- Exercise induced angina (no)\n- Chest pain type different from typical angina (we should consider create a new feature to replace cp, which indicates whether the person has typical angina\n- We also notice that people having left ventricular hypertrophy tend to have lower chance of getting heart attack","90e115ea":"- All features are numeric\n- Seems like we don't have missing data","9f600d3e":"#### Thalachh and Slp","e93ddfec":"- **thalachh, oldpeak, and age show clear difference** between 2 groups of output. We see that people with thalachh values over 157 and (or) olpeak under 0.8 have higher chance of getting heart attack\n- trbps doesn't show a significant difference between 2 output groups while chol show a slightly difference","7d3cff3d":"- Our target variable distribution is quite balanced","5e34684d":"Source: https:\/\/www.kaggle.com\/rashikrahmanpritom\/heart-attack-analysis-prediction-dataset?select=heart.csv\n\n# About this dataset\n    Age : Age of the patient\n    Sex : Sex of the patient\n    exang: exercise induced angina (1 = yes; 0 = no)\n    ca: number of major vessels (0-3)\n    cp : Chest Pain type chest pain type\n        Value 1: typical angina\n        Value 2: atypical angina\n        Value 3: non-anginal pain\n        Value 4: asymptomatic\n    trtbps : resting blood pressure (in mm Hg)\n    chol : cholestoral in mg\/dl fetched via BMI sensor\n    fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    rest_ecg : resting electrocardiographic results\n        Value 0: normal\n        Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n        Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n    thalach : maximum heart rate achieved\n    target : 0= less chance of heart attack 1= more chance of heart attack","daff3485":"# 5. Modeling","feae5091":"## 5.4 Conclusion","2166c1c5":"- Not really much outliers\n- Outliers appear in chol, trtbps, and oldpeak features","1e35091a":"#### Our target variable: Output","a893a4d0":"# 6.2 Performing on PCA subset","bfabf209":"## 1.2 Data distribution","863898fd":"# 6.1 Performing on our initial features","466062a7":"# 4. Feature selection","78d9c7d2":"## 2.1 Univariate analysis","b2dbb297":"- All outliers have been removed","60ccf6eb":"    H0: mean values for both groups are the same\n    H1: mean values for both groups are significantly different\n\n    Alpha (significance level): 5%","dd375018":"- There is also a fairly linear trend between maximum heart rate achieved, exercise induced angina and output","748bd77f":"#### Categorical features","586791f4":"- There is no linear trend between thalachh and Slp","0c6fe6c8":"# 2. Exploratory data analysis","4ee1875e":"## 2.2 Multivariate analysis","bcaeabb7":"#### Age and Maximum heart rate","dc157ba9":"- Now, we see some linear trend between age and sex regardless of our output","7d9f80e7":"## 4.2 Dimensionality reduction via PCA","ffced8cc":"## 3.3 Feature scaling","6a304c49":"#### Cp and Thalachh","69cf767b":"#### Age and Resting blood pressure","5d881ed5":"- Binary features: output, sex, fbs, exng\n- Features with 3-5 unique values: restecg, slp, cp, thall, caa\n- Continuous features: chol, thalachh, trtbps, age, oldpeak","15c7ce86":"- chol and fbs features do not show significant differences between 2 groups of output","80f692e1":"# Content\n1. Importing dataset and python libraries\n    - 1.1 Overview\n    - 1.2 Data distribution\n2. Exploratory data analysis\n    - 2.1 Univariate analysis\n    - 2.2 Multivariate analysis\n3. Data preprocessing\n    - 3.1 Outliers detection\n    - 3.2 Feature scaling\n4. Feature selection\n    - 4.1 Using P-value\n    - 4.2 Dimensionality reduction via PCA\n    - 4.3 Feature importances\n5. Modeling\n    - 5.1 Performing on our initial features\n    - 5.2 Performing on P-value subset\n    - 5.3 Performing on PCA subset\n    - 5.4 Conclusion\n6. Hypertuning parameters\n    - 6.1 Initial features\n    - 6.2 PCA subset","47b211ac":"#### Sex and Age","9fbae958":"- There is a fairly linear trend between oldpeak, slp and output\n- Oldpeak of people having slp as 0 has larger variance than those with slp as 1 or 2","68458e66":"### 2.1.2 Our target variable and other independent features","3d490e4e":"# 3. Data preprocessing","744e47ed":"## 4.1 Using P-values","705e5a5b":"## 5.2 Performing on P-value subset","f051a550":"- The average accuracy for test data among our subsets is around 78%, the initial features set has the highest mean test score, in the next step, we will try to improve our models' accuracy","4db2df4b":"## 5.2 Performing on PCA subset","62ab423d":"- Earlier, we've known that people with maximum heart rate achieved have higher probability of having a heart attack\n- From the lineplot, we find that younger people tend to have a slightly higher maximum heart rate","901bce55":"- Our continuous features have small skewness\n- Oldpeak feature is right-skewed","c8aa50cd":"# 1. Importing dataset and python libraries","a320acca":"- Age: The negative correlation between output and age suggests that **younger people have a higher change of getting heart attack**\n- Resting bllod pressure: There is a negative correlation between trtbps (resting blood pressure and output) indicating that people with **lower resting blood pressure have a higher probability of having a heart attack**\n- Maximum heart rate achieved: The positive correlation between output and thalack (maximum heart rate achieved) denotes that people with **higher maximum heart rate achieved are more likely to have a heart attack**\n- Both chol (cholestoral in mg\/dl fetched via BMI sensor) and fbs (fasting blood sugar) show weak correlations with our target feature\n\nSome features show significant correlation, such as slp and oldpeak, exng and cp, exng and thalachh, etc","1ea46811":"## 1.1 Overview","68f4e278":"### 2.1.1 Distribution","baf5c776":"#### Other combinations","a92de4a6":"- We need 8 features to explained about 80% of the variance, so we will create our pca subset with 8 principal components","5dc6bed4":"- When isolate output, we see that the trend is not really linear","c649ee8f":"## 5.1 Performing on our initial features","d6b1025f":"**About distribution of our categorical features:**\n- Sex: People having sex as 1 are more than twice people having sex as 0\n- fbs (fasting blood sugar >120 mg\/dl): most people in our sample don't have fasting blood sugar\n- exang: exercise induced angina: people are more likely to have exng as 0\n- restecg (resting electrocardiographic results): the proportions of normal and having ST-T wave abnormality are quite balanced while there is a small percentage of people with obvious left ventricular hypertrophy\n- cp (chest pain type): the proportion of people having chest pain type 1 (typical angina) is highest (approximately 50%) while the proportion of people having chest pain type 4 (asymptomatic) is the lowest with only around 7.62%"}}