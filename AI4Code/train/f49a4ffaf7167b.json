{"cell_type":{"205fb1e2":"code","e7bb4361":"code","818d1b3e":"code","49012adf":"code","3a987787":"code","4caf1f37":"code","2d0b40cc":"code","b081c9f4":"code","e2d66655":"code","d843d202":"code","47ff2bf9":"code","f79c7cb0":"markdown","9628d194":"markdown","358c49a5":"markdown","4ee4a737":"markdown","39767ccf":"markdown","636eb6a0":"markdown","45b8e7f7":"markdown"},"source":{"205fb1e2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport string\nfrom tqdm import tqdm\nfrom wordcloud import WordCloud, STOPWORDS","e7bb4361":"train_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample_sub=pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","818d1b3e":"print('Train data Shape: ',train_df.shape)\ntrain_df.head(10)","49012adf":"imp_cols=['excerpt','target']\ntrain_df=train_df[imp_cols]\ntrain_df.head()","3a987787":"sns.displot(train_df['target'])","4caf1f37":"train_df['target'].describe()","2d0b40cc":"#Lets take a look at length of sentences\nsent_len=[len(i.split()) for i in train_df['excerpt']]\nsns.displot(sent_len)","b081c9f4":"#Now let's clean the paragraphs for better understanding of words and length\nsp=stopwords.words('english')\n#Remove punctuation\nprint('Cleaning Punctuations')\ncleaned_text=[txt.translate(str.maketrans('','',string.punctuation)) for txt in train_df['excerpt']]\n\nprint('Cleaning numbers')\ncleaned_text=[' '.join([i for i in txt.lower().split() if i.isalpha()]) for txt in cleaned_text]\n\nprint('Cleaning Stopwords')\ncleaned_text=[' '.join(i for i in txt.split() if i not in sp) for txt in cleaned_text]\n","e2d66655":"#Let's take a look at how cleaning affected each sentence\nsent_len=[len(i.split(),) for i in cleaned_text]\nsns.displot(sent_len)","d843d202":"#Counter rare words\ncorpus=' '.join([i for i in cleaned_text])\nword_freq=Counter(corpus.split())\n\n#Words with freq less than 10\nfrequent_words=[]\nfor word in tqdm(word_freq.keys()):\n    if word_freq[word]<=10:\n        frequent_words.append(word)\n\nfrequent_words=' '.join([i for i in frequent_words])","47ff2bf9":"wordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                min_font_size = 10).generate(frequent_words)\n\nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\nplt.show()","f79c7cb0":"**Length distribution of sentences after cleaning**","9628d194":"**Get frequency of each word in corpus**\n","358c49a5":"**Target Distribution Plot**","4ee4a737":"**Word Cloud of most rare words**","39767ccf":"**Import Dataset**","636eb6a0":"**Clean sentences**\n* Remove puntuations\n* Remove stop words\n* Remove numbers and words containing numbers","45b8e7f7":"**Sentences length Distribution**"}}