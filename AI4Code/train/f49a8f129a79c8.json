{"cell_type":{"a049861e":"code","d2b25e8e":"code","cb36b1a6":"code","a4f79a7d":"code","c8280e3a":"code","9f41104b":"code","dab02041":"code","d9f72462":"code","99aa6011":"code","8ad7fb98":"code","930a4d30":"code","aab9b19d":"code","0235ae9d":"code","f1312950":"code","5a8a6cc0":"code","7f942ae3":"code","c9cc4b06":"code","2c192315":"code","bc83d4a3":"code","d84dfec8":"code","62d49127":"code","03c6eb1d":"code","ce287ead":"code","9ee484e6":"code","a76f938d":"code","b762473a":"code","f171aba6":"code","ad8e79b2":"code","e3f1cf75":"markdown","604cbeae":"markdown","9a0323a1":"markdown","aa1c7b0e":"markdown","84b29450":"markdown","d1df47bd":"markdown","f532ae69":"markdown","5170c705":"markdown","8063ac15":"markdown","c275cefb":"markdown","5adc4d50":"markdown","60ba960b":"markdown","b28a8576":"markdown","66266df9":"markdown","d816db0f":"markdown","d9c426bb":"markdown","4c650def":"markdown","7c09037c":"markdown","60f204e5":"markdown","127902a5":"markdown","bacb49d1":"markdown","ea09022c":"markdown"},"source":{"a049861e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport numpy as np","d2b25e8e":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n","cb36b1a6":"def evaluate(model, x_val, y_val):\n    y_pred = model.predict(x_val)\n    r2 = metrics.r2_score(y_val, y_pred)\n    mse = metrics.mean_squared_error(y_val, y_pred)\n    mae = metrics.mean_absolute_error(y_val, y_pred)\n    msle = metrics.mean_squared_log_error(y_val, y_pred)\n    mape = np.mean(tf.keras.metrics.mean_absolute_percentage_error(y_val, y_pred).numpy())\n    rmse = np.sqrt(mse)\n    rmlse_score = rmlse(y_val, y_pred).numpy()\n    print(\"R2 Score:\", r2)\n    print(\"MSE:\", mse)\n    print(\"MAE:\", mae)\n    print(\"MSLE:\", msle)\n    print(\"MAPE\", mape)\n    print(\"RMSE:\", rmse)\n    print(\"RMLSE\", rmlse_score)\n    return {\"r2\": r2, \"mse\": mse, \"mae\": mae, \"msle\": msle, \"mape\": mape, \"rmse\": rmse, \"rmlse\": rmlse_score}","a4f79a7d":"def rmlse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean(tf.square(tf.math.log(y_pred + 1) - tf.math.log(y_true + 1))))","c8280e3a":"def submit(model, X, ids, file_path):\n    SalePrice = model.predict(X)\n    submission = pd.DataFrame({\"Id\": ids, \"SalePrice\": SalePrice.reshape(-1)})\n    submission.to_csv(file_path, index=False)","9f41104b":"train.head()","dab02041":"train.shape","d9f72462":"train.info()","99aa6011":"train.describe()","8ad7fb98":"correlation_scores = train.corr()\ncorrelation_scores","930a4d30":"train.corr()[\"SalePrice\"].sort_values(key = lambda x: abs(x), ascending=False)","aab9b19d":"for data in [train, test]:\n    null_counts = data.isnull().sum()\n    null_counts[null_counts > 0]\n    null_columns = list(pd.DataFrame(null_counts[null_counts > 0]).index)\n    for column in null_columns:\n        if data[column].dtype == object:\n            data[column] = data[[column]].replace(np.NAN, \"Unknown\")\n        else:\n            data[column] = data[column].replace(np.NAN, data[column].mean())","0235ae9d":"train_test = pd.get_dummies(pd.concat([train, test]))","f1312950":"train_test.head()","5a8a6cc0":"mean_value = train_test.mean()\nstd_value = train_test.std()\nmean_value.pop(\"SalePrice\")\nstd_value.pop(\"SalePrice\")\nprint(mean_value)\nprint(std_value)","7f942ae3":"train_features = train_test.iloc[0: len(train)]\ntest_features = train_test.iloc[len(train):]\n_ = train_features.pop(\"Id\")\n_ = test_features.pop(\"SalePrice\")\ntest_ids = test_features.pop(\"Id\")","c9cc4b06":"train_features, val_features = train_test_split(train_features, test_size=0.2, random_state=np.random.randint(1000))","2c192315":"train_features.corr()","bc83d4a3":"thresold = 0.05\ncorrelated_scores = train_features.corr()[\"SalePrice\"]\ncorrelated_scores = correlated_scores[correlated_scores.abs() >= thresold]\ncorrelated_columns = list(correlated_scores.index)\ncorrelated_columns.remove(\"SalePrice\")\nprint(correlated_columns)","d84dfec8":"train_targets = train_features.pop(\"SalePrice\")\nval_targets = val_features.pop(\"SalePrice\")","62d49127":"categorical_columns = set(train.dtypes[train.dtypes==object].index)","03c6eb1d":"scale_strategies = [\"none\", \"standard_scale\", \"standard_scale_exclude_categorcial_features\"]\nscale_strategy = scale_strategies[2]\nif scale_strategy == scale_strategies[1]:\n    train_features = (train_features - mean_value) \/ std_value\n    val_features = (val_features - mean_value) \/ std_value\n    test_features = (test_features - mean_value) \/ std_value\nif scale_strategy == scale_strategies[2]:\n    for column in train_features.columns:\n        is_categorical_feature = False\n        components = column.split(\"_\")\n        if len(components) == 2 and components[0] in categorical_columns:\n            is_categorical_feature = True\n        if is_categorical_feature == False:\n            for features in [train_features, val_features, test_features]:\n                features.loc[:, column] = (features.loc[:, column] - mean_value[column]) \/ std_value[column]","ce287ead":"train_features.head()","9ee484e6":"use_correlated_columns = True\nif use_correlated_columns:\n    train_features = train_features[correlated_columns]\n    val_features = val_features[correlated_columns]\n    test_features = test_features[correlated_columns]","a76f938d":"import catboost\nimport time\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nbegin = time.time()\nparameters = {\n    \"depth\": [4, 5, 6, 7, 8, 9],\n    \"learning_rate\": [0.01, 0.05, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15],\n    \"iterations\": [500, 10000], \n}\ndef train_with_catboost(hyperparameters, X_train, X_val, y_train, y_val):\n    keys = hyperparameters.keys()\n    best_index = {key:0 for key in keys}\n    best_cat = None\n    best_score = 10e8\n    for (index, key) in enumerate(keys):\n        print(\"Find best parameter for %s\" %(key))\n        items = hyperparameters[key]\n        best_parameter = None\n        temp_best = 10e8\n        for (key_index, item) in enumerate(items):\n            iterations = hyperparameters[\"iterations\"][best_index[\"iterations\"]] if key != \"iterations\" else item\n            learning_rate = hyperparameters[\"learning_rate\"][best_index[\"learning_rate\"]] if key != \"learning_rate\" else item\n            depth = hyperparameters[\"depth\"][best_index[\"depth\"]] if key != \"depth\" else item\n            print(\"Training with iterations: %d learning_rate: %.2f depth:%d\"%(iterations, learning_rate, depth))\n            cat = catboost.CatBoostRegressor(\n                iterations = iterations, \n                learning_rate = learning_rate,\n                depth = depth,\n                verbose=500\n            )\n            cat.fit(X_train, y_train, verbose=False)\n            result = evaluate(cat, X_val, y_val)\n            score = result[\"rmlse\"]\n            if score < temp_best:\n                temp_best = score\n                best_index[key] = key_index\n                best_parameter = item\n            if score < best_score:\n                best_score = score\n                best_cat = cat\n        print(\"Best Parameter for %s: \"%(key), best_parameter)\n    best_parameters = {\n        \"iterations\": hyperparameters[\"iterations\"][best_index[\"iterations\"]],\n        \"learning_rate\": hyperparameters[\"learning_rate\"][best_index[\"learning_rate\"]],\n        \"depth\": hyperparameters[\"depth\"][best_index[\"depth\"]]\n    }\n    return best_cat, best_score, best_parameters\nbest_cat, best_score, best_parameters = train_with_catboost(parameters, train_features, val_features, train_targets, val_targets)\nprint(\"Best RMLSE: \", best_score)\nprint(\"Best Parameters: \", best_parameters)\nelapsed = time.time() - begin \nprint(\"Elapsed time: \", elapsed)\nsubmit(best_cat, test_features, test_ids, \"submission_cat.csv\")","b762473a":"from sklearn.model_selection import KFold\nX = pd.concat([train_features, val_features])\ny = pd.concat([train_targets, val_targets])\nfold = 1\nmodels = []\nfor train_indices, valid_indices in KFold(n_splits=5, shuffle=True).split(X):\n    print(\"Training with Fold %d\" % (fold))\n    X_train = X.iloc[train_indices]\n    X_val = X.iloc[valid_indices]\n    y_train = y.iloc[train_indices]\n    y_val = y.iloc[valid_indices]\n    cat = catboost.CatBoostRegressor(\n        iterations = best_parameters[\"iterations\"], \n        learning_rate = best_parameters[\"learning_rate\"],\n        depth = best_parameters[\"depth\"]\n    )\n    cat.fit(X_train, y_train, verbose=False)\n    models.append(cat)\n    evaluate(cat, X_val, y_val)\n    submit(cat, test_features, test_ids, \"submission_cat_fold%d.csv\"%(fold))\n    fold += 1","f171aba6":"SalePrice = np.mean([model.predict(test_features) for model in models], axis=0)\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": SalePrice})\nsubmission.to_csv(\"submission.csv\", index=False)","ad8e79b2":"cat = catboost.CatBoostRegressor(\n    iterations = best_parameters[\"iterations\"], \n    learning_rate = best_parameters[\"learning_rate\"],\n    depth = best_parameters[\"depth\"]\n)\ncat.fit(X, y, verbose=False)\nevaluate(cat, X, y)\nsubmit(cat, test_features, test_ids, \"submission_cat_all_dataset.csv\")","e3f1cf75":"## Conclusion\nCatboost is a good Regression tool to solve House Price Prediction Problem. In validation dataset, it can achieve MAE about 14400 and MAPE about 8.4. What it mean is that when this Model predict house prices, it has 14400 dollars error and 8.4% error in average. When it comes to RMLSE score, this model can get 0.10 in validation set and 0.13 in test set (top 25% rank in Kaggle LeaderBoard), looks overfits a little bit. Since this is a small dataset,when training the Model using K-Fold algorithm, results can also be different sinificantly. If we take the mean value of the KFold results, it can often get a better result.","604cbeae":"\n## If you found my work useful, please give me an upvote, thanks.","9a0323a1":"## Import Datasets","aa1c7b0e":"**Submission**","84b29450":"**Evaluation Function**","d1df47bd":"### Convert Categorical Features to Numerical Features","f532ae69":"**Factors that impact house price most**","5170c705":"## Model Development and Evaluation","8063ac15":"### Calculate Correlated Features","c275cefb":"## Common Functions","5adc4d50":"### Missing Value Imputation\n\nI will use following strategies to apply imputation to missing values. \n- For numerical columns, I will replace missing value with their mean value.\n- For categorical columns, I will replace missing value with unknown category.","60ba960b":"**Statistic infos**","b28a8576":"## Model Training with all data\nI would like to train the Model with all data to see what's happening becuase it seems a waste not to use all data.","66266df9":"**Correlation scores**","d816db0f":"### Hyperparameter Tuning","d9c426bb":"**Root Mean Squared Logarithmic Error**","4c650def":"## Import Packages","7c09037c":"### Model Training with K-Fold Algorithm","60f204e5":"## Exploratory Data Analysis & Data Preprocessing","127902a5":"### Feature Scaling","bacb49d1":"### Train Validation Split","ea09022c":"## House Price Regression with CatBoost\n## Table of Contents\n- Summary\n- Import Packages\n- Import Datasets\n- Common Functions\n- Exploratory Data Analysis & Data Preprocessing\n    - Statistic infos\n    - Missing Value Imputation\n    - Convert Categorical Features to Numerical Features\n    - Train Validation Split\n    - Calculate Correlated Features\n    - Feature Scaling\n- Model Development and Evaluation\n    - Hyperparameter Tuning\n    - Model Training with K-Fold Algorithm\n    - Model Training with all data\n- Conclusion\n\n\n## Summary\nIn this notebook, I will use CatBoost to create House Price Predictor and use hyperparameter searching techniques to find best results."}}