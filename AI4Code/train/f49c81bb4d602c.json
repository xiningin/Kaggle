{"cell_type":{"b388c6e3":"code","e7488097":"code","dfb9dc6f":"code","804123b3":"code","28c00de2":"code","210ffa8c":"code","db8ad767":"code","4f082d19":"code","791ea47e":"code","b1cda9c0":"code","68a21148":"code","e7608c7d":"code","28e0ea1b":"code","60a3ada4":"code","bef46537":"code","dc824b7f":"code","20011c1a":"code","fdeeb873":"code","5b035d44":"code","bc176cef":"code","9978c428":"code","14ce8321":"code","4fb4e582":"code","e811d711":"code","938e7e36":"code","76ee448a":"code","1aed0ed4":"code","ef3b4980":"code","a043d109":"code","0113fdc5":"code","4da80c95":"code","af5c85ae":"markdown","ccacc912":"markdown","bf993921":"markdown","da638518":"markdown","71210181":"markdown","e65f6c15":"markdown","1d3aa235":"markdown","99d84f63":"markdown"},"source":{"b388c6e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e7488097":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import confusion_matrix ,classification_report,precision_score, recall_score ,f1_score \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC \nimport warnings\nwarnings.filterwarnings('ignore')","dfb9dc6f":"data = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","804123b3":"data.head()","28c00de2":"data.info()","210ffa8c":"data.describe()","db8ad767":"data.describe().columns","4f082d19":"# numeric and categorical value separately\ndata_num = data[['age','trtbps','chol','thalachh','oldpeak']]\ndata_cat =data[['sex','cp','fbs','restecg','exng']]","791ea47e":"for i in data_num.columns:\n    plt.hist(data_num[i])\n    plt.title(i)\n    plt.show()","b1cda9c0":"print(data_num.corr())\nsns.heatmap(data_num.corr())","68a21148":"pd.pivot_table(data, index='output', values=['age','trtbps','chol','thalachh','oldpeak'])","e7608c7d":"for i in data_cat.columns:\n    sns.barplot(data_cat[i].value_counts().index,data_cat[i].value_counts()).set_title(i)\n    plt.show()","28e0ea1b":"print(pd.pivot_table(data,index='output',columns='sex', values='age'))\nprint(\"=\"*100)\nprint(pd.pivot_table(data,index='output',columns='cp', values='age'))\nprint(\"=\"*100)\nprint(pd.pivot_table(data,index='output',columns='fbs', values='age'))\nprint(\"=\"*100)\nprint(pd.pivot_table(data,index='output',columns='restecg', values='age'))\nprint(\"=\"*100)\nprint(pd.pivot_table(data,index='output',columns='exng', values='age'))","60a3ada4":"print(pd.pivot_table(data,index='output',columns='sex', values='chol'))\nprint(\"=\"*100)\nprint(pd.pivot_table(data,index='output',columns='cp', values='chol'))\nprint(\"=\"*100)\nprint(pd.pivot_table(data,index='output',columns='fbs', values='chol'))\nprint(\"=\"*100)\nprint(pd.pivot_table(data,index='output',columns='restecg', values='chol'))\nprint(\"=\"*100)\nprint(pd.pivot_table(data,index='output',columns='exng', values='chol'))","bef46537":"for i in data_num.columns:\n    sns.boxplot(data_num[i])\n    plt.title(i)\n    plt.show()","dc824b7f":"def outlinefree(dataCol):\n    # sorting column\n    sorted(dataCol)\n    \n    # getting percentile 25 and 27 that will help us for getting IQR (interquartile range)\n    Q1,Q3 = np.percentile(dataCol,[25,75])\n    \n    # getting IQR (interquartile range)\n    IQR = Q3-Q1\n    \n    # getting Lower range error\n    LowerRange = Q1-(1.5 * IQR)\n    \n    # getting upper range error\n    UpperRange = Q3+(1.5 * IQR)\n    \n    # return Lower range and upper range.\n    return LowerRange,UpperRange","20011c1a":"lwtrtbps,uptrtbps = outlinefree(data['trtbps'])\nlwchol,upchol = outlinefree(data['chol'])\nlwoldpeak,upoldpeak = outlinefree(data['oldpeak'])","fdeeb873":"data['trtbps'].replace(list(data[data['trtbps'] > uptrtbps].trtbps) ,uptrtbps,inplace=True)\ndata['chol'].replace(list(data[data['chol'] > upchol].chol) ,upchol,inplace=True)\ndata['oldpeak'].replace(list(data[data['oldpeak'] > upoldpeak].oldpeak) ,upoldpeak,inplace=True)","5b035d44":"features = data.iloc[:,:-1].values\nlabel = data.iloc[:,-1].values","bc176cef":"#------------------------LogisticRegression-----------------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=102)\n\nclassimodel= LogisticRegression()  \nclassimodel.fit(X_train, y_train)\ntrainscore =  classimodel.score(X_train,y_train)\ntestscore =  classimodel.score(X_test,y_test)  \n\nprint(\"test score: {} train score: {}\".format(testscore,trainscore),'\\n')\n\ny_pred =  classimodel.predict(X_test)\n\n#from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)","9978c428":"print(' f1 score: ',f1_score(y_test, y_pred),'\\n')\nprint(' precision score: ',precision_score(y_test, y_pred),'\\n')\nprint(' recall score: ',recall_score(y_test, y_pred),'\\n')\nprint(classification_report(y_test, y_pred))","14ce8321":"#--------------------------------------K-Nearest Neighbor(KNN)-----------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=193) \n\n\nclassifier= KNeighborsClassifier()  \nknnmodel =  classifier.fit(X_train, y_train) \n\ntrainscore =  knnmodel.score(X_train,y_train)\ntestscore =  knnmodel.score(X_test,y_test)  \n\nprint(\"test score: {} train score: {}\".format(testscore,trainscore),'\\n')\n\ny_predknn =  knnmodel.predict(X_test)\n\nprint(confusion_matrix(y_test, y_predknn))","4fb4e582":"print(\"f1_score: \",f1_score(y_test, y_predknn),'\\n')\nprint(\"precision_score: \",precision_score(y_test, y_predknn),'\\n')\nprint(\"recall_score: \",recall_score(y_test, y_predknn),'\\n')\nprint(classification_report(y_test, y_predknn))\n","e811d711":"#------------------------------naive bayes---------------------------\nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=34) \n\nNBmodel = GaussianNB()  \nNBmodel.fit(X_train, y_train) \n\ntrainscore =  NBmodel.score(X_train,y_train)\ntestscore =  NBmodel.score(X_test,y_test)  \n\nprint(\"test score: {} train score: {}\".format(testscore,trainscore),'\\n')\ny_predNB =  NBmodel.predict(X_test)\nprint(confusion_matrix(y_test, y_predNB))","938e7e36":"print(\"f1_score: \",f1_score(y_test, y_predNB),'\\n')\nprint(\"precision_score: \",precision_score(y_test, y_predNB),'\\n')\nprint(\"recall_score: \",recall_score(y_test, y_predNB),'\\n')\nprint(classification_report(y_test, y_predNB))","76ee448a":"#-------------------------------- support vector classification -------------------------------------  \nX_train, X_test, y_train, y_test= train_test_split(features,label, test_size= 0.25, random_state=8) \n\nsvcmodel = SVC(probability=True)  \nsvcmodel.fit(X_train, y_train) \n\ntrainscore =  svcmodel.score(X_train,y_train)\ntestscore =  svcmodel.score(X_test,y_test)  \n\nprint(\"test score: {} train score: {}\".format(testscore,trainscore),'\\n')\n\ny_predsvc =  svcmodel.predict(X_test)\n\nprint(confusion_matrix(y_test, y_predsvc))","1aed0ed4":"print(\"f1_score: \",f1_score(y_test, y_predsvc),'\\n')\nprint(\"precision_score: \",precision_score(y_test, y_predsvc),'\\n')\nprint(\"recall_score: \",recall_score(y_test, y_predsvc),'\\n')\nprint(classification_report(y_test, y_predsvc),'\\n')","ef3b4980":"#-------------------------------------- LogisticRegression -------------------------------------\nprobabilityValues = classimodel.predict_proba(features)[:,1]\n#Calculate AUC\nauc = roc_auc_score(label,probabilityValues)\nprint(auc)\n#Calculate roc_curve\nfpr,tpr, threshold =  roc_curve(label,probabilityValues)\nplt.plot([0,1],[0,1], linestyle = '--')\nplt.plot(fpr,tpr)","a043d109":"#-------------------------------------- KNeighborsClassifier -------------------------------------\nprobabilityValues = knnmodel.predict_proba(features)[:,1]\n#Calculate AUC\nauc = roc_auc_score(label,probabilityValues)\nprint(auc)\n#Calculate roc_curve\nfpr,tpr, threshold =  roc_curve(label,probabilityValues)\nplt.plot([0,1],[0,1], linestyle = '--')\nplt.plot(fpr,tpr)","0113fdc5":"#-------------------------------------- naive bayes -------------------------------------\nprobabilityValues = NBmodel.predict_proba(features)[:,1]\n#Calculate AUC\nauc = roc_auc_score(label,probabilityValues)\nprint(auc)\n#Calculate roc_curve\nfpr,tpr, threshold =  roc_curve(label,probabilityValues)\nplt.plot([0,1],[0,1], linestyle = '--')\nplt.plot(fpr,tpr)","4da80c95":"#-------------------------------------- SVC -------------------------------------\nprobabilityValues = svcmodel.predict_proba(features)[:,1]\n#Calculate AUC\nauc = roc_auc_score(label,probabilityValues)\nprint(auc)\n#Calculate roc_curve\nfpr,tpr, threshold =  roc_curve(label,probabilityValues)\nplt.plot([0,1],[0,1], linestyle = '--')\nplt.plot(fpr,tpr)","af5c85ae":"## Feature Engineering","ccacc912":"## ROC Curve\nI like to see how the ROC Curve performs on various different models to get the best Model.\n\n1. **LogisticRegression 0.9214756258234519**\n2. K-Nearest Neighbor(KNN) 0.8049407114624506\n3. support vector classification 0.7574879227053138\n4. naive bayes 0.9104084321475626","bf993921":"# Model Building\nI like to see how various different models perform with default parameters on dataset and checking F1 score.\n\n1. **LogisticRegression  0.9268292682926829**\n2. K-Nearest Neighbor(KNN) 0.7865168539325843\n3. support vector classification 0.7999999999999999\n4. naive bayes 0.9090909090909092","da638518":"## Index\n1. Dataset description\n2. Exploratory data analysis (EDA)\n3. Data Preprocessing and Normailzation\n4. Feature Engineering\n5. Model Building\n6. ROC Curve\n7. conclusion","71210181":"## Dataset description\n\n1. Age : Age of the patient\n2. Sex : Sex of the patient\n3. exang: exercise induced angina (1 = yes; 0 = no)\n4. ca: number of major vessels (0-3)\n5. cp : Chest Pain type chest pain type\n        Value 1: typical angina\n        Value 2: atypical angina\n        Value 3: non-anginal pain\n        Value 4: asymptomatic\n6. trtbps : resting blood pressure (in mm Hg)\n7. chol : cholestoral in mg\/dl fetched via BMI sensor\n8. fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n9. rest_ecg : resting electrocardiographic results\n        Value 0: normal\n        Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n        Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n10. thalach : maximum heart rate achieved\n11. target : 0= less chance of heart attack 1= more chance of heart attack","e65f6c15":"## Exploratory data analysis (EDA)\n### 1) For numeric data \n\n* Made histograms to understand distributions \n* Corrplot \n* Pivot table comparing survival rate across numeric variables \n\n### 2) For Categorical Data \n* Made bar charts to understand balance of classes \n* Made pivot tables to understand relationship with survival ","1d3aa235":"## Conclusion\naccording to the model and ROC curve, we found the best model for the dataset. **LogisticRegression algorithm** giving us the best result.\n\n\n    f1 score:  0.9268292682926829\n    ROC curve: 0.9214756258234519","99d84f63":"## Data Preprocessing and Normailzation\nUse the percentile technique to normalize the columns."}}