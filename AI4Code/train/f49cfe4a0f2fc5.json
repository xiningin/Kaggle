{"cell_type":{"cb1e49fe":"code","529b8f67":"code","596cd546":"code","d267f2f2":"code","8fa1743e":"code","01d3cc32":"code","c5569bef":"code","d0451941":"code","3ad30bc8":"code","50420e37":"code","e77a7fd6":"code","2915edfa":"code","2f010f1c":"code","5039e0ff":"code","e9405787":"code","21641e2e":"code","83d0883b":"code","4557228f":"code","518d4c44":"code","e62092cc":"code","880134fd":"code","3f1bcfa4":"code","f4692d8c":"code","d6cc84ec":"code","d6935362":"code","557fe09c":"code","06f4c0b0":"code","d4362b87":"code","1dc25686":"code","d296f4f8":"code","87714260":"code","45268e51":"code","39cad4a4":"code","79c6313b":"code","6a7966de":"code","dd59e268":"code","44372e2c":"code","675dbe78":"code","56d4f8e4":"code","3296bb9c":"code","fcdab9c9":"code","b9ded90d":"code","6f9f16d6":"code","d3532ceb":"code","95649061":"code","53c10393":"code","799f284f":"code","52ea1c63":"code","7bd09526":"code","aa04da78":"code","8bc6dbe3":"code","9c5d2500":"code","29c78ce7":"code","45d55683":"code","9929a075":"code","08ea2dd0":"code","783759ff":"code","0b8556fb":"code","55427020":"code","684c586b":"code","df73a2ba":"code","78a8fe38":"code","b5dd0177":"code","c350f2a1":"code","057fce18":"code","bc92c79d":"code","11c2b74b":"code","9b6e2b81":"code","10782369":"code","81f56bfd":"code","4029a46c":"code","11fdd3c3":"code","0566d30d":"code","290bda91":"code","58bb6031":"code","80a3486b":"code","a82dc5f4":"code","c8f685d0":"code","bba02c54":"code","fa620184":"code","139a3a97":"code","ea4067c3":"code","9b56af66":"code","9bd6c4b5":"code","1be32858":"code","615fbe63":"code","fd512026":"code","173499c0":"code","347ce50a":"code","ddae6c1b":"code","efb20af7":"code","17cff41d":"code","c377a128":"code","16658a5c":"code","1a7e408d":"code","91381c7d":"code","72acd939":"code","4e5800a2":"code","e390345a":"code","e8952d3a":"code","51d67f45":"code","efac82b1":"code","273516be":"code","7f061ce3":"code","c7aafc1e":"code","0f220418":"code","ab1ce5c7":"code","0e7d5355":"markdown","d27385bf":"markdown","1f5b135c":"markdown","634857e0":"markdown","0fac6a05":"markdown","5d7c0dbe":"markdown","4a3743a2":"markdown","d3adcc4e":"markdown","68f7cd55":"markdown","4b9ae65a":"markdown","54cf7437":"markdown","e42f2ad6":"markdown","39ef2f84":"markdown","0a98a58d":"markdown","3d9641a0":"markdown","83c0b6c2":"markdown","d59fd56d":"markdown","7168ca91":"markdown","e0aea413":"markdown","0729e688":"markdown","59b369ea":"markdown","b6aab707":"markdown","afc6c3d7":"markdown","c722eb8c":"markdown","32ab9cca":"markdown","ea10d604":"markdown","fe267918":"markdown","e6e552b7":"markdown","4aca357a":"markdown","3727acaf":"markdown","8b6c2a47":"markdown","3d19ee64":"markdown","56e9d581":"markdown","aa35ccf5":"markdown","057318ed":"markdown","28ef4efa":"markdown","c987010f":"markdown","28821cf8":"markdown","b87111d5":"markdown","db4ee073":"markdown","7648f3ae":"markdown","49a48490":"markdown","4e624f33":"markdown","f9c37c95":"markdown","1fe21866":"markdown","e19dcba9":"markdown","73db0f95":"markdown","25365155":"markdown","46095ba4":"markdown","b9d60f5f":"markdown","f0bc3c48":"markdown","77b8a741":"markdown","6ed885a6":"markdown","70ea2a5c":"markdown","5877bbbc":"markdown","5ecd3b09":"markdown","388b2a46":"markdown","cbca5b85":"markdown","6686b6f0":"markdown","620b0938":"markdown","0f09d59d":"markdown","8724b4d9":"markdown","57d1b261":"markdown","d6787b55":"markdown","ed53aba2":"markdown","0a31e2e2":"markdown","8a20a86f":"markdown","42d8bc19":"markdown","6b0e4c8d":"markdown","712859b8":"markdown","d2038e3a":"markdown","391aba29":"markdown","0a3921a3":"markdown","b36d93a3":"markdown","17fd7e03":"markdown","aab6c765":"markdown","9565bd6c":"markdown","4e6f05f4":"markdown","3cc82ece":"markdown","a6b51f85":"markdown","11b61475":"markdown","4e36ca92":"markdown","ba50e419":"markdown","b8b1d655":"markdown","7aa5dde8":"markdown","c5df6807":"markdown","79cc9b8b":"markdown","77fe1284":"markdown","603c88a9":"markdown","47c76020":"markdown","9d209d76":"markdown","1f41a3ee":"markdown","66d412eb":"markdown","cf0446e3":"markdown","f26d9d42":"markdown","6716035a":"markdown","9c6c7186":"markdown","47e92fb8":"markdown"},"source":{"cb1e49fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","529b8f67":"try:\n    import pycaret\nexcept:\n    !pip install pycaret\n\ntry:\n    import missingno\nexcept:\n    !pip install missingno","596cd546":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import classification_report, log_loss\nfrom sklearn import preprocessing\nimport umap\nimport umap.plot\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nimport scipy.stats as stats\n\nimport warnings\nwarnings.filterwarnings('ignore')","d267f2f2":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_data_org = train_data.copy()\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission_data = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ntitanic_df = pd.concat([train_data, test_data], ignore_index = True, sort = False)\ntr_idx = titanic_df['Survived'].notnull()\ntitanic_dl_df = titanic_df.copy()","8fa1743e":"titanic_df.head(5).T.style.set_properties(**{'background-color': 'black',\n                           'color': 'white',\n                           'border-color': 'white'})","01d3cc32":"titanic_df.drop(['PassengerId'],axis=1,inplace=True)","c5569bef":"from pycaret.anomaly import *","d0451941":"pycaret.anomaly.setup(\n    data=train_data_org,\n    silent=True)","3ad30bc8":"pca = pycaret.anomaly.create_model('pca')","50420e37":"pca_df = pycaret.anomaly.assign_model(pca)","e77a7fd6":"abnormal_data = pca_df[pca_df.Anomaly == 1].sort_values(by='Anomaly_Score', ascending=False)\nprint(\"the size of anomaly = \",len(abnormal_data))\nabnormal_data.head(10).style.set_properties(**{'background-color': 'black',\n                           'color': 'white',\n                           'border-color': 'white'})","2915edfa":"tuned_pca = tune_model(model = 'pca', supervised_target = 'Survived')","2f010f1c":"plt.style.use(\"dark_background\")\nplot_model(tuned_pca,plot='umap')","5039e0ff":"plot_model(tuned_pca,plot='tsne')","e9405787":"titanic_df.info()","21641e2e":"plt.figure(figsize = (10,8))\nwith plt.rc_context({'figure.facecolor':'black'}):\n    sns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\n    plt.style.use(\"dark_background\")\n    ax = titanic_df.dtypes.value_counts().plot(kind='bar',fontsize=20,color='purple')\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+ p.get_width() \/ 2., height + 0.1, height, ha = 'center', size = 25)\n    sns.despine()","83d0883b":"colors = ['gold', 'mediumturquoise']\nlabels = ['Non-Suvivor','Suvivor']\nvalues = titanic_df['Survived'].value_counts()\/titanic_df['Survived'].shape[0]\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_traces(hoverinfo='label+percent', textinfo='percent', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='white', width=0.1)))\nfig.update_layout(\n    title_text=\"Titanic Survivor\",\n    title_font_color=\"white\",\n    legend_title_font_color=\"yellow\",\n    paper_bgcolor=\"black\",\n    plot_bgcolor='black',\n    font_color=\"white\",\n)\nfig.show()","4557228f":"import missingno as msno\nplt.style.use(\"dark_background\")\nmsno.matrix(titanic_df.drop(['Survived'],axis=1),color=(138\/255,43\/255,226\/255),fontsize=30)","518d4c44":"isnull_series = titanic_df.loc[:,:'Cabin'].isnull().sum()\nisnull_series[isnull_series > 0].sort_values(ascending=False)\n\nplt.figure(figsize = (20,10))\n\nax = isnull_series[isnull_series > 0].sort_values(ascending=False).plot(kind='bar',\n                                                                        grid = False,\n                                                                        fontsize=20,\n                                                                        color='purple')\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+ p.get_width() \/ 2., height + 5, height, ha = 'center', size = 30)\nsns.despine()","e62092cc":"titanic_df['Has_Cabin'] = titanic_df['Cabin'].isnull().astype(int)","880134fd":"total_cnt = titanic_df['Survived'].count()\nplt.figure(figsize=(12,8))\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\nplt.style.use(\"dark_background\")\nax = sns.countplot(x=\"Has_Cabin\",\n                   hue=\"Survived\", \n                   data=titanic_df,\n                   palette = 'Purples_r')\nax.set_title('Survived Count\/Rate')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 10, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=20)\nsns.despine()","3f1bcfa4":"titanic_df['Cabin'] = titanic_df['Cabin'].fillna('N')\ntitanic_df['Cabin_label'] = titanic_df['Cabin'].str.get(0)\n\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 2)\nplt.style.use(\"dark_background\")\n\nplt.figure(figsize=(25,10))\nax = sns.countplot(x=\"Cabin_label\", hue=\"Survived\", data=titanic_df,palette = 'Purples_r')\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 10, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=25)\nsns.despine()","f4692d8c":"import re\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\ntitanic_df['Title'] = titanic_df['Name'].apply(get_title)","d6cc84ec":"titanic_df['Title'] = titanic_df['Title'].replace(\n       ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], \n       'Rare')\n\ntitanic_df['Title'] = titanic_df['Title'].replace('Mlle', 'Miss')\ntitanic_df['Title'] = titanic_df['Title'].replace('Ms', 'Miss')\ntitanic_df['Title'] = titanic_df['Title'].replace('Mme', 'Mrs')\ntitanic_df['Title'].unique()","d6935362":"rcParams['figure.figsize'] = 20,10\nax = sns.countplot(x='Title',hue ='Survived',data=titanic_df,palette=\"Purples_r\")\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 10, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=25)\nsns.despine()","557fe09c":"rcParams['figure.figsize'] = 20,15\ntitles = titanic_df['Title'].unique()\nplt.subplots_adjust(hspace=1.5)\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\nplt.style.use(\"dark_background\")\nidx = 1\nfor title in titles:\n    plt.subplot(3,2,idx)\n    ax = sns.histplot(x='Age',data=titanic_df[titanic_df['Title']== title],hue ='Survived',palette=\"Purples_r\",kde=True)\n    ax.set_title(title)\n    sns.despine()\n    idx = idx + 1","06f4c0b0":"rcParams['figure.figsize'] = 12,7\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\nplt.style.use(\"dark_background\")\nax = sns.countplot(x='Embarked',hue = 'Survived',data=titanic_df,palette=\"Purples_r\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width() \/ 2., height + 3, f'{height \/ total_cnt * 100:2.1f}%', ha = 'center', size = 25)\nsns.despine()","d4362b87":"rcParams['figure.figsize'] = 12,7\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\nplt.style.use(\"dark_background\")\nax = sns.countplot(x='Embarked',hue = 'Sex',data=titanic_df,palette=\"Purples_r\")\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width() \/ 2., height + 3, f'{height \/ total_cnt * 100:2.1f}%', ha = 'center', size = 25)\nsns.despine()","1dc25686":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ntitanic_df[['Embarked']] = imp.fit_transform(titanic_df[['Embarked']])","d296f4f8":"rcParams['figure.figsize'] = 20,10\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 2)\nplt.style.use(\"dark_background\")\nax = sns.countplot(x='SibSp',hue ='Survived',data=titanic_df,palette=\"Purples_r\")\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 7, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=20)\nsns.despine()","87714260":"rcParams['figure.figsize'] = 20,10\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 2)\nplt.style.use(\"dark_background\")\nax = sns.countplot(x='Parch',hue ='Survived',data=titanic_df,palette=\"Purples_r\")\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 7, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=20)\nsns.despine()","45268e51":"titanic_df['FamilySize'] = titanic_df['SibSp'] + titanic_df['Parch'] + 1\nrcParams['figure.figsize'] = 25,10\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 2)\nplt.style.use(\"dark_background\")\nax = sns.countplot(x='FamilySize',hue ='Survived',data=titanic_df,palette=\"Purples_r\")\nax.set_title('Survived Rate')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 7, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=20)\nsns.despine()","39cad4a4":"titanic_df['IsAlone'] = 0\ntitanic_df.loc[titanic_df['FamilySize'] == 1, 'IsAlone'] = 1\n\nrcParams['figure.figsize'] = 15,10\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1.5)\nplt.style.use(\"dark_background\")\nax = sns.countplot(x='IsAlone',hue ='Survived',data=titanic_df,palette=\"Purples_r\")\nax.set_title('Survived Rate')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 10, f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=25)\nsns.despine()","79c6313b":"plt.figure(figsize = (10,13))\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1.5)\nplt.style.use(\"dark_background\")\nplt.subplots_adjust(hspace=0.3)\nax1 = plt.subplot(2,1,1)\nsns.histplot(x=\"Fare\", hue=\"Survived\", data=titanic_df,palette = 'Purples_r',kde=True)\nax1.axvline(x=titanic_df['Fare'].mean(), color='g', linestyle='--', linewidth=3)\nax1.text(titanic_df['Fare'].mean(), 90, \"Mean\", horizontalalignment='left', size='small', color='yellow', weight='semibold')\nax1.set_title('Fare Histogram',fontsize=20)\nax2 = plt.subplot(2,1,2)\nstats.probplot(titanic_df['Fare'],dist = stats.norm, plot = ax2)\nax2.set_title('Fare Q-Q plot',fontsize=20)\nsns.despine()\n\nmean = titanic_df['Fare'].mean()\nstd = titanic_df['Fare'].std()\nskew = titanic_df['Fare'].skew()\nprint('Fare : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","6a7966de":"from sklearn.preprocessing import QuantileTransformer\ntransformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\ntitanic_df['Fare'] = transformer.fit_transform(titanic_df[['Fare']])","dd59e268":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntitanic_df[['Fare']] = imp.fit_transform(titanic_df[['Fare']])","44372e2c":"plt.figure(figsize = (10,13))\nplt.subplots_adjust(hspace=0.3)\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1.5)\nplt.style.use(\"dark_background\")\nax1 = plt.subplot(2,1,1)\nsns.histplot(x=\"Fare\", hue=\"Survived\", data=titanic_df,palette = 'Purples_r',kde=True)\nax1.axvline(x=titanic_df['Fare'].mean(), color='g', linestyle='--', linewidth=3)\nax1.text(titanic_df['Fare'].mean(), 90, \"Mean\", horizontalalignment='left', size='small', color='yellow', weight='semibold')\nax1.set_title('Fare Histogram',fontsize=20)\nax2 = plt.subplot(2,1,2)\nstats.probplot(titanic_df['Fare'],dist = stats.norm, plot = ax2)\nax2.set_title('Fare Q-Q plot',fontsize=20)\nsns.despine()\n\nmean = titanic_df['Fare'].mean()\nstd = titanic_df['Fare'].std()\nskew = titanic_df['Fare'].skew()\nprint('Fare : mean: {0:.4f}, std: {1:.4f}, skew: {2:.4f}'.format(mean, std, skew))","675dbe78":"titanic_df['Fare_class'] = pd.qcut(titanic_df['Fare'], 5, labels=['F1', 'F2', 'F3','F4','F5' ])","56d4f8e4":"rcParams['figure.figsize'] = 15,10\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1.5)\nplt.style.use(\"dark_background\")\nax = sns.histplot(x=\"Fare_class\", hue=\"Survived\", data=titanic_df,palette = 'Purples_r',kde=True)\nsns.despine()","3296bb9c":"titanic_df['Fare_class'] = titanic_df['Fare_class'].replace({'F1':1,'F2':2,'F3':3,'F4':4,'F5':5})","fcdab9c9":"titanic_df['Has_Age'] = titanic_df['Age'].isnull().astype(int)","b9ded90d":"rcParams['figure.figsize'] = 10,6\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1.5)\nplt.style.use(\"dark_background\")\nax = sns.countplot(x='Has_Age',hue ='Survived',data=titanic_df,palette=\"Purples_r\")\nplt.legend(loc = 'upper right')\nax.set_title('Survived Rate')\nfor p in ax.patches:\n    x, height, width = p.get_x(), p.get_height(), p.get_width()\n    ax.text(x + width \/ 2, height + 12,f'{height \/ total_cnt * 100:2.1f}%',va='center', ha='center', size=25)\nsns.despine()","6f9f16d6":"rcParams['figure.figsize'] = 12,7\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1.5)\nplt.style.use(\"dark_background\")\nax = sns.histplot(x=\"Age\", hue=\"Survived\", data=titanic_df,palette = 'Purples_r',kde=True)\nplt.axvline(x=titanic_df['Age'].mean(), color='g', linestyle='--', linewidth=3)\nplt.text(titanic_df['Age'].mean(), 60, \"Mean\", horizontalalignment='left', size='small', color='yellow', weight='semibold')\nsns.despine()","d3532ceb":"imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\ntitanic_df[['Age']] = imputer.fit_transform(titanic_df[['Age']])","95649061":"robuster = RobustScaler()\ntitanic_df['Age'] = robuster.fit_transform(titanic_df[['Age']])","53c10393":"titanic_df = pd.get_dummies(titanic_df, columns = ['Title','Sex', 'Embarked','Cabin_label'], drop_first=True)","799f284f":"corr=titanic_df.corr().round(1)\n\nplt.figure(figsize=(25, 20))\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\nplt.style.use(\"dark_background\")\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,annot=True,cmap='Purples',mask=mask,cbar=True)\nplt.title('Correlation Plot')","52ea1c63":"plt.figure(figsize=(15, 10))\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\nplt.style.use(\"dark_background\")\nabs(corr['Survived']).sort_values()[:-1].plot.barh(color='Purple')\n","7bd09526":"def drop_features(df):\n    df.drop(['Name','Ticket','SibSp','Parch','Fare_class',\n             'Cabin','Cabin_label_G','Cabin_label_T',\n             'Cabin_label_F','FamilySize','Embarked_Q','Title_Rare'],\n            axis=1,\n            inplace=True)\n    return df\n\ntitanic_df = drop_features(titanic_df)","aa04da78":"corr=titanic_df.corr().round(1)\n\nplt.figure(figsize=(20, 15))\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\nplt.style.use(\"dark_background\")\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,annot=True,cmap='Purples',mask=mask,cbar=True)\nplt.title('Correlation Plot')","8bc6dbe3":"plt.figure(figsize=(15, 10))\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\nplt.style.use(\"dark_background\")\nabs(corr['Survived']).sort_values()[:-1].plot.barh(color='Purple')","9c5d2500":"tr_idx = titanic_df['Survived'].notnull()\ny_titanic_df = titanic_df[tr_idx]['Survived']\nX_titanic_df= titanic_df[tr_idx].drop('Survived',axis=1)\nX_test_df = titanic_df[~tr_idx].drop('Survived',axis=1)","29c78ce7":"X_train, X_val, y_train, y_val=train_test_split(X_titanic_df, y_titanic_df, \\\n                                                  test_size=0.2, random_state=11)","45d55683":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import plot_partial_dependence\n\nclf = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\nfig,ax = plt.subplots(figsize=(18,35))\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1.2)\nplt.style.use(\"dark_background\")\nfig.tight_layout()\nplot_partial_dependence(clf, X_train, X_train.columns,ax=ax)","9929a075":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(n_estimators=100).fit(X_train, y_train)\nfig,ax = plt.subplots(figsize=(18,35))\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1.2)\nplt.style.use(\"dark_background\")\nfig.tight_layout()\nplot_partial_dependence(clf, X_train, X_train.columns,ax=ax)","08ea2dd0":"all_cols = [cname for cname in X_titanic_df.columns]\n\nsns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\nplt.style.use(\"dark_background\")","783759ff":"from sklearn.inspection import permutation_importance\n\nresult = permutation_importance(\n    clf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nforest_importances = pd.Series(result.importances_mean, index=all_cols)","0b8556fb":"sorted_idx = result.importances_mean.argsort()\nplt.rcParams.update({'font.size': 15})\nfig, ax = plt.subplots(figsize=(10,8))\nax.boxplot(\n    result.importances[sorted_idx].T, vert=False, labels=X_train.columns[sorted_idx]\n)\nax.set_title(\"Permutation Importances\")\nfig.tight_layout()\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nplt.show()","55427020":"X_train.shape","684c586b":"mapper = umap.UMAP().fit(X_train)\numap.plot.points(mapper, labels=y_train, theme='fire')","df73a2ba":"umap.plot.connectivity(mapper, show_points=True)","78a8fe38":"def drop_features(df):\n    df.drop(['Ticket','PassengerId','Cabin'],\n            axis=1,\n            inplace=True,\n            errors='ignore')\n    return df\n\ntitanic_dl_df = drop_features(titanic_dl_df)\ntitanic_dl_df['Pclass'] = titanic_dl_df['Pclass'].astype(object)","b5dd0177":"def replace_name(name):\n    if \"Mr.\" in name: return \"Mr\"\n    elif \"Mrs.\" in name: return \"Mrs\"\n    elif \"Miss.\" in name: return \"Miss\"\n    elif \"Master.\" in name: return \"Master\"\n    elif \"Ms.\": return \"Ms\"\n    else: return \"No\"\ntitanic_dl_df['Name'] = titanic_dl_df['Name'].apply(replace_name)","c350f2a1":"imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\ntitanic_dl_df[['Age']] = imputer.fit_transform(titanic_dl_df[['Age']])\n\ntransformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\ntitanic_dl_df['Fare'] = transformer.fit_transform(titanic_dl_df[['Fare']])\n\nmean_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntitanic_dl_df[['Fare']] = mean_imp.fit_transform(titanic_dl_df[['Fare']])\n\nfreq_imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ntitanic_dl_df[['Embarked']] = freq_imp.fit_transform(titanic_dl_df[['Embarked']])","057fce18":"from fastai import *\nfrom fastai.tabular.all import * ","bc92c79d":"cat_cols = titanic_dl_df.select_dtypes(include = ['object', 'bool']).columns.tolist()\nnum_cols = titanic_dl_df.select_dtypes(exclude = ['object', 'bool']).columns.tolist()\nnum_cols.remove('Survived')\nprocs = [Categorify, FillMissing, Normalize]\ny_names = 'Survived'\ny_block = CategoryBlock()\nsplits = RandomSplitter(valid_pct=0.2)(range_of(train_data))","11c2b74b":"dls = TabularDataLoaders.from_df(titanic_dl_df[tr_idx],\n                                 procs=procs, \n                                 cat_names=cat_cols, \n                                 cont_names=num_cols,\n                                 splits = splits,\n                                 y_block = y_block,\n                                 y_names=y_names                                 \n                                )","9b6e2b81":"learn = tabular_learner(dls,layers=[16,32,32,16,8],\n                        metrics=accuracy,\n                        cbs = [EarlyStoppingCallback(monitor='accuracy', patience=5), \n                               ActivationStats(with_hist=True)])\nlearn.model","10782369":"sr = learn.lr_find()\nsr.valley","81f56bfd":"learn.fit_one_cycle(100,sr.valley)","4029a46c":"learn.recorder.plot_loss()","11fdd3c3":"learn.recorder.plot_sched()\nplt.subplots_adjust(wspace=0.5)","0566d30d":"learn.recorder.fine_tune(epochs=10)","290bda91":"def plot_layer_stats(self, idx):\n    plt,axs = subplots(1, 3, figsize=(15,3))\n    plt.subplots_adjust(wspace=0.5)\n    for o,ax,title in zip(self.layer_stats(idx),axs,('mean','std','% near zero')):\n        ax.plot(o)\n        ax.set_title(title)","58bb6031":"plt.style.use(\"dark_background\")\nplt.subplots_adjust(wspace=1)\nplot_layer_stats(learn.activation_stats,-1)","80a3486b":"plt.style.use(\"dark_background\")\nplt.subplots_adjust(wspace=1)\nplot_layer_stats(learn.activation_stats,-2)","a82dc5f4":"learn.activation_stats.color_dim(-2)","c8f685d0":"titanic_dl_test = titanic_dl_df[~tr_idx]\ntitanic_dl_test = titanic_dl_test.drop('Survived',axis=1)","bba02c54":"dl = learn.dls.test_dl(titanic_dl_test)\npreds = learn.get_preds(dl=dl)\nresults = preds[0].argmax(axis=1)\nresults = results.tolist()","fa620184":"submission_data['Survived'] = results\n#submission_data.to_csv('dl_submission.csv', index = False)","139a3a97":"from pycaret.classification import *\nclf1 = setup(data = titanic_df[tr_idx], \n             target = 'Survived',\n             preprocess = False,\n             numeric_features = all_cols,\n             silent=True)","ea4067c3":"top5 = compare_models(sort='Accuracy',n_select = 5,\n                      exclude = ['knn', 'svm','ridge','nb','dummy','qda','xgboost']\n                     )","9b56af66":"catboost = create_model('catboost')\nrf = create_model('rf')\nlightgbm = create_model('lightgbm')\ngbc = create_model('gbc')\nmlp = create_model('mlp')\nlr = create_model('lr')\ndt = create_model('dt')","9bd6c4b5":"with plt.rc_context({'figure.facecolor':'grey'}):\n    interpret_model(catboost)","1be32858":"with plt.rc_context({'figure.facecolor':'grey'}):\n    interpret_model(rf)","615fbe63":"with plt.rc_context({'figure.facecolor':'grey'}):\n    interpret_model(lightgbm)","fd512026":"with plt.rc_context({'figure.facecolor':'grey'}):\n    interpret_model(dt)","173499c0":"tuned_rf = tune_model(rf, optimize = 'Accuracy',early_stopping = True,search_library='optuna')\ntuned_lightgbm = tune_model(lightgbm, optimize = 'Accuracy',early_stopping = True,search_library='optuna')\ntuned_catboost = tune_model(catboost, optimize = 'Accuracy',early_stopping = True,search_library='optuna')\ntuned_gbc = tune_model(gbc, optimize = 'Accuracy',early_stopping = True,search_library='optuna')\ntuned_lr = tune_model(lr, optimize = 'Accuracy',early_stopping = True,search_library='optuna')\ntuned_mlp = tune_model(mlp, optimize = 'Accuracy',early_stopping = True)\ntuned_dt = tune_model(dt, optimize = 'Accuracy',early_stopping = True)","347ce50a":"plt.figure(figsize=(10, 8))\nwith plt.rc_context({'figure.facecolor':'black','text.color':'white'}):\n    plot_model(tuned_mlp, plot='learning')","ddae6c1b":"plt.figure(figsize=(8, 8))\nwith plt.rc_context({'figure.facecolor':'black','text.color':'black'}):\n    plot_model(tuned_dt, plot='tree')","efb20af7":"plt.figure(figsize=(10, 8))\nplot_model(tuned_dt, plot='learning')","17cff41d":"stack_model = stack_models(estimator_list = [mlp,rf,lightgbm,catboost,gbc,lr], meta_model = catboost ,optimize = 'Accuracy')","c377a128":"plt.figure(figsize=(8, 8))\nplot_model(stack_model, plot='boundary')","16658a5c":"plt.figure(figsize=(8, 8))\nplot_model(stack_model, plot = 'auc')","1a7e408d":"plt.figure(figsize=(8, 8))\nplot_model(stack_model, plot='confusion_matrix')","91381c7d":"blend_soft = blend_models(estimator_list = [mlp,rf,lightgbm,catboost,gbc,lr], optimize = 'Accuracy',method = 'soft')","72acd939":"plt.figure(figsize=(8, 8))\nplot_model(blend_soft, plot='boundary')","4e5800a2":"plt.figure(figsize=(8, 8))\nplot_model(blend_soft, plot = 'auc')","e390345a":"plt.figure(figsize=(8, 8))\nplot_model(blend_soft, plot='confusion_matrix')","e8952d3a":"blend_hard = blend_models(estimator_list = [mlp,rf,lightgbm,catboost,gbc,lr], optimize = 'Accuracy',method = 'hard')","51d67f45":"plt.figure(figsize=(8, 8))\nplot_model(blend_hard, plot='boundary')","efac82b1":"plt.figure(figsize=(8, 8))\nplot_model(blend_hard, plot='confusion_matrix')","273516be":"cali_model = calibrate_model(blend_soft)","7f061ce3":"final_model = finalize_model(cali_model)","c7aafc1e":"plt.figure(figsize=(8, 8))\nplot_model(final_model, plot='boundary')","0f220418":"plt.figure(figsize=(8, 8))\nplot_model(final_model, plot='confusion_matrix')","ab1ce5c7":"last_prediction = final_model.predict(X_test_df)\nsubmission_data['Survived'] = last_prediction.astype(int)\nsubmission_data.to_csv('submission.csv', index = False)","0e7d5355":"**Multilayer perceptron (MLP)**\n\n> A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). The term MLP is used ambiguously, sometimes loosely to mean any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation). Multilayer perceptrons are sometimes colloquially referred to as \"vanilla\" neural networks, especially when they have a single hidden layer.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Multilayer_perceptron","d27385bf":"<span style=\"color:Blue\"> Observation\n* Cabin_label_B, Cabin_label_C, Has_Age, and IsAlone are not significant in the RandomForestClassifier model.","1f5b135c":"Let's check the correlation between the target value (Suvived) and other features.","634857e0":"----------------------------------------\n## Preprecessing for DL\nCompared to Classic ML, the process of Feature Engineering is simple.\n\nIn most cases, the following steps are required.\n\nHandling missing values\nEncoding for categorical features (one-hot encoding is mainly used).\nStandard or Min-Max Scaling","0fac6a05":"## Choosing top models\n\n> This function trains and evaluates the performance of a given estimator using cross validation. The output of this function is a score grid with CV scores by fold. \n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","5d7c0dbe":"---------------------\n## Training","4a3743a2":"<span style=\"color:Blue\"> Observation\n* IsAlone, Title_Miss, Title_Mrs, Cabin_label_B, Cabin_label_C, Has_Age, and IsAlone are not significant in the GradientBoostingClassifier model.","d3adcc4e":"---------------------------------------\n## Plotting Model\n\n> This function analyzes the performance of a trained model.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/anomaly.html\n\n\nBoth u-MAP and t-SNE enable low-dimensional visual confirmation through dimensionality reduction.\nIf you are interested in dimensional reduction, you can take a look at the notebook below.\n\n[dimensionality-dimension-reduction](https:\/\/www.kaggle.com\/ohseokkim\/the-curse-of-dimensionality-dimension-reduction)","68f7cd55":"---------------------\n## Finding the proper learning rate\n\n![](https:\/\/miro.medium.com\/max\/1200\/1*Q-2Wh0Xcy6fsGkbPFJvMhQ.gif)\n\nPicture Credit: https:\/\/miro.medium.com\n\nAs shown in the figure above, if the learning rate is selected too large, divergence occurs, and if the learning rate is selected too small, convergence is delayed. Learning rate is one of the important hyperparameters used in deep learning and has a great influence on performance. In general, there is a tendency to determine the learning rate through various experiments, but fast.ai provides a method to find the learning rate.\n\nWe decided to leave the troublesome task of determining the learning rate to the machine and focus on more creative work.","4b9ae65a":"-------------------------------------------------------------\n# Selecting Features\n\nFeatures that are not helpful in judging the above heatmap and survivors, or that have other derived variables, will be removed.","54cf7437":"<span style=\"color:Blue\"> Observation:\n   \n* The greater the feature importance, the earlier the separation.","e42f2ad6":"-------------------------------------------------\n## FamilySize ( Derived variable )","39ef2f84":"Although the survivor is small, the imbalance is not large enough for over\/under sampling.\nIf you want to know more about over\/under sampling, please refer to the notebook below.\n\n[Over\/Under sampling](https:\/\/www.kaggle.com\/ohseokkim\/preprocessing-resolving-imbalance-by-sampling)","0a98a58d":"**Question: Does the number of accompanying family members affect the survival rate??**","3d9641a0":"PassengerId has nothing to do with survior. It can be removed immediately.","83c0b6c2":"Looking at the two figures above, it can be seen that the rate of change of the target value for each feature value is different. In particular, it can be seen that the partial dependency on age and fare is very different. Finally, a hint from the above figure is that each model can predict biased results, and it is necessary to harmonize these results with the power of collective intelligence. In other words, you can indirectly feel the need for ensemble.","d59fd56d":"The training dataset has 16 dimensions. To show the approximate distribution of the training dataset preprocessed above, let's reduce the dimension to two dimensions and draw it.","7168ca91":"<span style=\"color:Blue\"> Observation:\n* The number of data determined as anomaly is 45.\n* Among the outliers, there is a lot of information about passengers with Pclass 1. \n* In many cases, the information of passengers with Embarked S and C is determined to be an outlier.","e0aea413":"<span style=\"color:Blue\"> **Cognitive Bias**\n\n> A cognitive bias is a systematic pattern of deviation from norm or rationality in judgment. Individuals create their own \"subjective reality\" from their perception of the input. An individual's construction of reality, not the objective input, may dictate their behavior in the world. Thus, cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, or what is broadly called irrationality.\n>  1. some are due to ignoring relevant information (e.g., neglect of probability),\n>  2. some involve a decision or judgment being affected by irrelevant information (for example the framing effect where the same problem receives different responses depending on how it is described; or the distinction bias where choices presented together have different outcomes than those presented separately), and\n>  3. others give excessive weight to an unimportant but salient feature of the problem (e.g., anchoring).\n    \nRef: https:\/\/en.wikipedia.org\/wiki\/Cognitive_bias\n    \nWe may have cognitive biases when we first encounter datasets. For example, when looking at the features of the titanic dataset, you might think that gender and age are important without detailed analysis. Of course, such a decision could be right. However, we can receive feedback through the model to see if our thinking is correct and modify our thinking based on this. \n\nAt this point, we once again strongly say: **Machine! Tell Me Everything!**\n\nIn this notebook, we analyze the dataset in as many ways as possible and try to understand the behavior of the models and the dataset through this.\nTo do this, we would like to organize the notebook in the following order.\n1. First, check what outliers there are and what characteristics the outliers have.\n2. Perform detailed EDA and pre-process.\n3. Check which features are important in various ways.\n4. Model using deep learning (fast.ai) and understand deep learning.\n5. Using ML (ensemble), do three modeling and understand the ensemble operation.","0729e688":"# Loading Dataset","59b369ea":"------------------------------------------------------------------------\n## Has_Cabin ( Derived variable )\n\n**Question: Is there a difference in the survival rate between passengers with and without cabin?**","b6aab707":"---------------------------------------------------\n## Soft Voting\n\n![](https:\/\/miro.medium.com\/max\/806\/1*bliKQZGPccS7ho9Zo6uC7A.jpeg)\n\nPicture Credit: https:\/\/miro.medium.com\n\n> This function trains a Soft Voting classifier for select models passed in the estimator_list param. The output of this function is a score grid with CV scores by fold.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","afc6c3d7":"---------------------------------------------------------------------------------------------------------------------------------------\n# Hard Voting\n\n![](https:\/\/miro.medium.com\/max\/428\/1*XnZwlg7Th3nga25sSlanJQ.jpeg)\n\nPicture Credit: https:\/\/vitalflux.com\n\n\n> This function trains a **Majority Rule classifier** for select models passed in the estimator_list param. The output of this function is a score grid with CV scores by fold.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","c722eb8c":"-------------------------------------------\n## Evaluating Model","32ab9cca":"<span style=\"color:Blue\"> Observation:\n\nWhen FamilySize is 1, the survival rate is significantly lower than in other cases. I think it will be helpful when the model is learning.","ea10d604":"### An Extension To Imputation\n\n> Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing.\n\n![](https:\/\/i.imgur.com\/UWOyg4a.png)\n\n> In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries.\n> \n> In some cases, this will meaningfully improve results. In other cases, it doesn't help at all.\n\nRef: https:\/\/www.kaggle.com\/alexisbcook\/missing-values","fe267918":"## Creating Models\n\n> This function trains and evaluates the performance of a given estimator using cross validation. The output of this function is a score grid with CV scores by fold. \n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","e6e552b7":"------------------------------------------------\n## Parch ( Number of parents\/children )","4aca357a":"Compared to the soft blending model, the boundary does not look clean.","3727acaf":"<span style=\"color:Blue\"> Observation:\n* Among the features, if you look at Fare and Age, the features are spread in a wide distribution of importance, and the colors are also spread from blue to red. \n* Each model is learning with the importance of different features. The diversity of these models seems to increase the performance of the ensemble model.\n* Title_Mr and Sex_male play an important role in how the model learns.","8b6c2a47":"<hr style=\"border: solid 3px blue;\">\n\n# Deep learning using fast.ai\n\n![](https:\/\/thumbs.gfycat.com\/FlawedImpracticalGuillemot-size_restricted.gif)\n\nPicture Credit: https:\/\/thumbs.gfycat.com\n\n<span style=\"color:Blue\"> **What is fast.ai?**\n    \n> fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library\n\nRef: https:\/\/github.com\/fastai\/fastai\n    \nModeling DL and setting of hyperparameters are more important compared to classical ML. However, these processes are not easy. Here, we are going to perform these processes using fast.ai. Also, check whether training is done properly using the activation map.","3d19ee64":"**It is skewed to one side. Consider nonlinear scaling. In this case, we will use QuantileTransformer.**\n\n> The quantile function ranks or smooths out the relationship between observations and can be mapped onto other distributions, such as the uniform or normal distribution.\n\nIf you want to know more about Scaling, please refer to the notebook below.\n\n[NotebooK](https:\/\/www.kaggle.com\/ohseokkim\/preprocessing-linear-nonlinear-scaling)","56e9d581":"If you look at the above figure, you can see that the distribution of activation values \u200b\u200bis evenly distributed. I think you can decide that the learning has been done properly.","aa35ccf5":"---------------------------------------\n# Checking Last Results","057318ed":"In machine learning, it is important to determine the boundary. In particular, in tree-based models, it is more important to determine the boundary, because the process of creating a new leaf in the tree is also the process of determining the boundary.\nLooking at the above picture again, there are many overlapping points with the green dot indicating the survior and the blue dot indicating the non-survior. Determining the boundary in this situation would be a very difficult task.\nIf the feature engineer work was done well, the distribution of the two points to determine the boundary would have been well divided. However, the titanic dataset is difficult to do with some missing values and a small dataset.","28ef4efa":"UMAP (Uniform Manifold Approximation and Projection), which is faster than t-SNE and separates the data space well, has been proposed for nonlinear dimensionality reduction. In other words, it can process very large datasets quickly and is suitable for sparse matrix data. Furthermore, compared to t-SNE, it has the advantage of being able to embed immediately when new data comes in from other machine learning models.","c987010f":"-----------------------------------\n## Fare      ","28821cf8":"----------------------------------------------------------------\n## Checking training results\n![](https:\/\/miro.medium.com\/max\/474\/1*wZg_RQHPRtn62dDp2Ez86A.jpeg)\n\nPicture Credit:https:\/\/miro.medium.com","b87111d5":"---------------------------------------------\n## Cabin_Label ( Derived variable )","db4ee073":"<span style=\"color:Blue\"> Observation\n    \nOur model observed the loss values \u200b\u200bof the train\/valid dataset and found points that were not overfitting using early stopping.","7648f3ae":"---------------------------------------------------------------\n# Setting up","49a48490":"--------------------------------------------------\n# Finalizing the last model\n> This function trains a given estimator on the entire dataset including the holdout set.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html\n\nThe blend_soft model is selected based on the above result. Finally, the model is tuned with the entire dataset.\n","4e624f33":"## Setting up models\n\n> This function trains and evaluates performance of all estimators available in the model library using cross validation. The output of this function is a score grid with average cross validated scores.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","f9c37c95":"-------------------------------------------------------------------------------------------------\n## Calibrating the final model\n\n> This function calibrates the probability of a given estimator using isotonic or logistic regression. \n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","1fe21866":"t-SNE is often used for visualization purposes by compressing data on a two-dimensional plane. Points that are close to the original feature space are also expressed in a two-dimensional plane after compression. Since the nonlinear relationship can be identified, the model performance can be improved by adding the compression results expressed by these t-SNEs to the original features. However, since the computation cost is high, it is not suitable for compression exceeding two or three dimensions.","e19dcba9":"--------------------------------------------------------------------------------------------\n## Has_Age ( Derived variable )\n\n**Question: Does the survival rate make a difference with and without age records?**","73db0f95":"----------------------------------------------------------------------\n## SibSp ( Number of siblings\/spouses )","25365155":"-------------------------------------------------------\n## Explaining features with partial dependence\n\n> Partial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features (the \u2018complement\u2019 features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest\n> \nRef: https:\/\/scikit-learn.org\/stable\/modules\/partial_dependence.html#partial-dependence","46095ba4":"<span style=\"color:Blue\"> Observation\n    \nOur fast.ai found an appropriate learning rate as shown in the figure above and gave us a guide.","b9d60f5f":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction\n\n![](https:\/\/c.tenor.com\/poOBxHtWHFcAAAAC\/tell-me-everything-inform-me.gif)\n\nPicture Credit: https:\/\/c.tenor.com","f0bc3c48":"-------------------\n## Defining TabularDataLoaders","77b8a741":"<span style=\"color:Blue\"> Observation:\n\n* Cases with cabins have more survivors compared to cases without cabins. It is likely that the new derived variable will be helpful in the classification of survivors.","6ed885a6":"**Let's impute missing value for Embarked feature. The strategy for Embarked's missing values is to choose 'most_frequent'.**","70ea2a5c":"-------------------------------------------------------------\n# Categorical Features\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*wYbTRM0dgnRzutwZq63xCg.png)\n\nPicture Credit: https:\/\/miro.medium.com\n\nCategorical data can be classified into ordinal data and nominal data. In the case of an ordinal type, there is a difference in importance for each level. This value plays an important role in the case of regression, so encode it with care.\n\nIt is difficult to encode categorical features compared to numeric features. For ordinal data, it is more difficult.","5877bbbc":"<span style=\"color:Blue\"> Observation:\n\nOK! Skewness decreased. Even looking at the Q-Q plot, the normality improved.","5ecd3b09":"<hr style=\"border: solid 3px blue;\">\n\n# EDA","388b2a46":"-----------------------------------------------\n## Creating Model\n\n> This function trains a given model from the model library. All available models can be accessed using the models function.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/anomaly.html","cbca5b85":"It is noteworthy that passengers with Pclass 1 are especially often judged as outliers. Will we be able to solve this mystery through EDA?","6686b6f0":"-----------------------------------------------------------\n## Checking learning rate and momemtum scheduling\n![](https:\/\/www.andreaperlato.com\/img\/momentum.png)\n\nPicture Credit: https:\/\/www.andreaperlato.com\n\nMomentum combines the direction from the gradient descent optimization algorithm obtained from the previous otimization procedure with the direction obtained from the current procedure to overcome the noisy gradient well.\n\nIf you look at the figure below, fast.ai finds an appropriate convergence point after increasing the learning rate gradually, lowering the learning rate again to find an appropriate learning rate. As opposed to learning, modem started with a large value and changed to a low value.\n","620b0938":"----------------------------------------------------------------------------\n# Visualizing Training Dataset after Dimension Reduction\n\n\nBefore training, we can check our processed datasets at a low level. If we can easily determine the boundary even with our eyes in a low dimension, it can be considered that the preprocessing has been done very well.\n\nDon't be discouraged if you don't see a clear boundary. This is because our dataset is high-dimensional, and only because we may not be able to see it with the naked eye.","0f09d59d":"--------------------------------\n## Stacking\n\n![](https:\/\/mlfromscratch.com\/content\/images\/2020\/01\/model_stacking_overview-4.png)\n\nPicture Credit: https:\/\/mlfromscratch.com","8724b4d9":"----------------------------------------------------\n# Numerical Features\n\n![](https:\/\/static-assets.codecademy.com\/Courses\/Hypothesis-Testing\/Intro_to_variable_types_4.png)\n\nPicture Credit: https:\/\/t3.ftcdn.net","57d1b261":"----------------------------------------------------\n## Predicting using test dataset","d6787b55":"<hr style=\"border: solid 3px blue;\">\n\n# Classic Machine Learning using Ensemble\n\n![](https:\/\/miro.medium.com\/max\/637\/1*3GIDYOn2GNcv9bq4bQk5YA.jpeg)\n\nPicture Credit: https:\/\/miro.medium.com\n\n> Supervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem.Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. The term ensemble is usually reserved for methods that generate multiple hypotheses using the same base learner. The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning\n\nJust as humans can have biased thinking, each model can also make biased predictions. Among the methods that do not make biased predictions while maintaining generality, ensemble is one of the best methods. Weak models will be able to obtain stable and better results while complementing each other's ideas.","ed53aba2":"----------------------------------------------------------\n## Picking outliers\n**Let's pick the top 10 outliers**","0a31e2e2":"# Tuning Hyperparameters\n\n> This function tunes the hyperparameters of a given estimator. The output of this function is a score grid with CV scores by fold of the best selected model based on optimize parameter. \n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","8a20a86f":"------------------------------\n## Tuning Model\n\n> This function tunes the fraction parameter of a given model.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/anomaly.html","42d8bc19":"-------------------------------------------------\n## Activations Histogram\n\n![](https:\/\/forums.fast.ai\/uploads\/default\/original\/3X\/5\/7\/57a02a03d86a56561484aee9e88222ecbb7c1cf5.jpeg)\n\n\n<span style=\"color:Blue\"> **Colorful Dimension**\n> The idea of the colorful dimension is to express with colors the mean and standard deviation of activations for each batch during training. Vertical axis represents a group (bin) of activation values. Each column in the horizontal axis is a batch. The colours represent how many activations for that batch have a value in that bin.\n\nRef: https:\/\/forums.fast.ai\/","6b0e4c8d":"<span style=\"color:Blue\"> Observation:\n* There are 5 object features. It is expected with categorical features.\n* There are 3 float-type features.\n* There are 3 integer-type features.","712859b8":"**Let's check the correlation of each feature.**","d2038e3a":"---------------------------------------\n# Checking Correlation","391aba29":"# Checking Target Value Imbalace","0a3921a3":"<span style=\"color:Blue\"> Observation:\n    \n* Many passengers on board at S port died.\n* For passengers boarding at port C, the survival rate is higher than the mortality rate.","b36d93a3":"------------------------------------------------------\n## Doing the fine tuning","17fd7e03":"---------------------------------------\n## Embarked","aab6c765":"## Checking the final model","9565bd6c":"It seems that the Boundary is set properly.","4e6f05f4":"<hr style=\"border: solid 3px blue;\">\n\n# Detecting Anomaly \n\n![](https:\/\/miro.medium.com\/max\/697\/1*O3lOgPwuHP7Vfc1T6NDRrQ.png)\n\nPicture Credit:https:\/\/miro.medium.com\n\n<span style=\"color:Blue\"> **Anomaly Detection**\n> In data analysis, anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Anomaly_detection\n    \nBefore EDA we can get a guide to anomaly (outlier) from the model. In fact, it is almost impossible to perform the task of finding outliers in a high-dimensional dataset without the help of a model.\n    \nWe can look for anomaly before proceeding with EDA and gain insight into future EDA.","3cc82ece":"--------------------------\n## Modeling\n\n* Metric is determined by accuracy.\n* Set Patience to 5 and early stopping.\n* Sets a callback for stating the activation values.","a6b51f85":"<span style=\"color:Blue\"> Observation:\n\nThe mortality rate is higher in the case of Mr. I think it will help with learning.","11b61475":"<span style=\"color:Blue\"> Observation:\n\nThose who were alone died more than those who were not alone. The derived feature seems to be helpful for model training.","4e36ca92":"-------------------------------\n## Assigning Model\n> This function assigns anomaly labels to the dataset for a given model. (1 = outlier, 0 = inlier).\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/anomaly.html","ba50e419":"# Interpreting Models\n\nThis function analyzes the predictions generated from a trained model. Most plots in this function are implemented based on the SHAP (SHapley Additive exPlanations).\n\n> SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions\n\nRef: https:\/\/shap.readthedocs.io\/en\/latest\/\n\n**If you want to know more about feature importance and SHAP, please refer to the notebook below.**\n\n[Notebook](https:\/\/www.kaggle.com\/ohseokkim\/explaning-machine-by-feature-importnace)","b8b1d655":"## Checking Data Type","7aa5dde8":"<span style=\"color:Blue\"> Observation:\n    \n* More than the case where Age is not missing.\n* Cases in which age is not missed have a higher survival rate than cases in which age is omitted.","c5df6807":"------------------------------------------------------\n# Encoding\n\nLet's perform encoding on categorical features.\n\nWhen only tree-based models are used, label encoding is sufficient. However, we will use one-hot encoding for model extension in the future.\n\nIf you want to know more about the encoding of categorical features, please refer to the notebook below.\n\n[Notebook](https:\/\/www.kaggle.com\/ohseokkim\/preprocessing-encoding-categorical-data)","79cc9b8b":"<span style=\"color:Blue\"> Observation\n    \n* There is a large correlation between FamilySize and SibSp and Parch. Since the derived variable FamilySize is made of SibSp and Parch, SibSp and Parch are removed.\n* The relationship between Cabin and Has_Cabin is high. Therefore, the derived variable Has_Cabin is left and Cabin is removed.\n* The relationship between Fare and Fare_class is high. Fare is selected because skewness is removed by nonlinear transform of the Fare feature.\n* There are many features that are not related to the survived value.","77fe1284":"________________________________________________________\n## Name\n\nIt seems difficult to find the feature directly related to the survivor.","603c88a9":"______________________________________________\n## Alone ( Derived variable )","47c76020":"<span style=\"color:Blue\"> Observation:\n    \nThere are missing values for Age, Cabin, Fare, and Embarked features. In particular, there are many missing values for Age and Cabin features. Let's think about how to handle these missing values.","9d209d76":"<span style=\"color:Blue\"> Observation\n    \nThe above two figures confirm the distribution of activation values for each layer. What you need to pay attention to here is to check whether the weight value is widely distributed close to zero. If many values are distributed at zero, it means that the learning is not done properly. When such a situation occurs, it seems that modeling and learning are necessary again.","1f41a3ee":"Considering above results, the soft blending model seems appropriate among ensemble models. Therefore, we use this model to make the final prediction with the test dataset.","66d412eb":"<span style=\"color:Blue\"> Observation:\n* In the case of Mr, the number of survivors is small.\n* In the case of Mrs and Miss, there are many survivors.\n\nI think it will be helpful in judging survivors using this.\nHowever, it seems difficult to find the relationship between age and title from the above distributions. Therefore, it seems difficult to use this to fill in the missing values of age.","cf0446e3":"----------------------------------------------------------\n## Feature importance based on feature permutation\n> The estimator is required to be a fitted estimator. X can be the data set used to train the estimator or a hold-out set. The permutation importance of a feature is calculated as follows. First, a baseline metric, defined by scoring, is evaluated on a (potentially different) dataset defined by the X. Next, a feature column from the validation set is permuted and the metric is evaluated again. The permutation importance is defined to be the difference between the baseline metric and metric from permutating the feature column.\n> \nRef: https:\/\/scikit-learn.org\/stable","f26d9d42":"-------------------------------------------------------------------------\n## Age","6716035a":"## Checking and Handling Missing Values","9c6c7186":"--------------------------------------------\n## Setting Up\n\n> This function initializes the training environment and creates the transformation pipeline. Setup function must be called before executing any other function. It takes one mandatory parameter: data. All the other parameters are optional.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/anomaly.html","47e92fb8":"<span style=\"color:Blue\"> Observation\n   \nAs shown in the figure above, when viewed in two dimensions, there are quite a few areas where the survivors and the dead overlap."}}