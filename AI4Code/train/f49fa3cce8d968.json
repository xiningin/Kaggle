{"cell_type":{"33edba49":"code","9e430a0c":"code","5f552e21":"code","50baa6a4":"code","a710348a":"code","0c24c10f":"code","eeaf3e17":"code","5dce8b67":"code","3b787909":"code","8d671221":"markdown","a5bacea8":"markdown","31497be4":"markdown","d937f62e":"markdown","18009c24":"markdown"},"source":{"33edba49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e430a0c":"heart_data  = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")\n## Exploring the data set\nheart_data.describe()\nheart_data.head()\n\n\n## Noting that lots of columns are either continuous numbers (0...*) or Categorical values \n## NO NAN values in dataset\n\n\n## Splitting data into train and test data \nfrom sklearn.model_selection import train_test_split\n\n# Separate target from predictors\ny = heart_data['HeartDisease']\nX = heart_data.drop(['HeartDisease'], axis=1)\n\n# Divide data into training and validation subsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)","5f552e21":"# Get list of categorical variables\ns = (heart_data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","50baa6a4":"## Attemping to use ordinal encoder to encode categorical values into columns with 0,1,2,3,4 etc\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = X_train.copy()\nlabel_X_test = X_test.copy()\n\n# Apply ordinal encoder to each column with categorical data\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\nlabel_X_test[object_cols] = ordinal_encoder.transform(label_X_test[object_cols])\n\n\n#print(label_X_train.shape)\n#print(label_X_test.shape)\n#print(X_train.shape)\n#print(X_test.shape)\nprint(label_X_train.head(5))\n","a710348a":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","0c24c10f":"## Printing MAE from the above ordinal_encoder\nprint(\"MAE from Approach 2 (Ordinal Encoding):\") \nprint(score_dataset(label_X_train, label_X_test, y_train, y_test))","eeaf3e17":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[object_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_test.index = X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_test = X_test.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n\nprint(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_test, y_train, y_test))","5dce8b67":"## Setting up a function to output the MAE given input parameters\ndef optimise_forest_param(X_train, X_valid, y_train, y_valid,n_estimators=100, max_depth=None, max_features=\"auto\" ):\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=0, max_depth=max_depth, max_features=max_features, bootstrap=False)\n    model.fit(X_train, y_train)\n    #print(\"N Estimator = \", n_estimators)\n    #print(\"max_depth = \", max_depth)\n    #print(\"max_featues = \", max_features)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\n## Finding optimal n_estimator\nn_estimator_value = []\nn_estimator_mae = []\n## ran this a few times to get a good idea of what range to run the numbers\nfor i in range(1, 50, 1):\n    n_estimator_mae.append(optimise_forest_param(label_X_train, label_X_test, y_train, y_test, i))\n    n_estimator_value.append(i)\nn_estimator_dict = dict(zip(n_estimator_value,n_estimator_mae ))\nprint(\"The minmum MAE value corresponded to n_estimator = \", min(n_estimator_dict, key=n_estimator_dict.get))\noptimal_nestimator= min(n_estimator_dict, key=n_estimator_dict.get)\n\n\n\n## Finding optimal max_depth\nmax_depth_value = []\nmax_depth_mae = []\n## ran this a few times to get a good idea of what range to run the numbers\nfor i in range(1, 50, 1):\n    max_depth_mae.append(optimise_forest_param(label_X_train, label_X_test, y_train, y_test, optimal_nestimator, i))\n    max_depth_value.append(i)\nmax_depth_dict = dict(zip(max_depth_value,max_depth_mae ))\nprint(\"The minmum MAE value corresponded to max_depth = \", min(max_depth_dict, key=max_depth_dict.get))\noptimal_max_depth= min(max_depth_dict, key=max_depth_dict.get)\n\n\n\n\n","3b787909":"def run_final_model(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=optimal_nestimator, random_state=0, max_depth = optimal_max_depth)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\nrun_final_model(label_X_train, label_X_test, y_train, y_test)\nprint(\"Model Accuracy = \", 1-(run_final_model(label_X_train, label_X_test, y_train, y_test)))\n\n\n","8d671221":"# Will Now attempt to do some data pre-proccessing","a5bacea8":"# Comparing Ordinal Encoding to One-Hot-Encoding","31497be4":"# Running Final Model","d937f62e":"# We will now try to optimise random forest parameters\n\nOrdinal Encoding had a lower Mean Absoloute Error (MAE) thus we will use that going forwards","18009c24":"# Setting up the model and scoring it"}}