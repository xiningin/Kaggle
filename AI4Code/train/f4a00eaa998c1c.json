{"cell_type":{"5cb5b60e":"code","da990aee":"code","500ff059":"code","e05ed8df":"code","8c7693f9":"code","ebd8aa6b":"code","2036b86d":"code","ebe65bbc":"code","608cdff7":"code","1c92ac8c":"code","0020c24a":"code","2db32de5":"code","bbe64cb4":"code","21da231e":"code","765d4512":"code","91ae00d5":"code","61dba27c":"code","ab77fca9":"code","aa52cd5e":"code","44fd189e":"code","8a284e22":"markdown","b5d176cc":"markdown","c4d38c13":"markdown","2c012edc":"markdown","be15d287":"markdown","3ba166bd":"markdown","8f548676":"markdown","301c2e9e":"markdown","5add202d":"markdown","30814d13":"markdown","03805bb0":"markdown","ebecc049":"markdown"},"source":{"5cb5b60e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns # for plotting\nimport matplotlib.pyplot as plt # for plotting\nimport folium\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","da990aee":"for f in os.listdir(\"..\/input\/cpe-data\"):\n    print(f)","500ff059":"root = \"..\/input\/cpe-data\"\nfor obj in os.listdir(root):\n    if os.path.isdir(\"{}\/{}\".format(root, obj)):\n        print(\"Folder {} has next data files:\".format(obj))\n        for file in os.listdir(\"{}\/{}\".format(root, obj)):\n            if os.path.isfile(\"{}\/{}\/{}\".format(root, obj,file)):\n                print(\"    -- {} dimension of {}\".format(pd.read_csv(\"{}\/{}\/{}\".format(root, obj, file), skiprows=1).shape, file))                ","e05ed8df":"file_path = '..\/input\/cpe-data\/Dept_24-00098\/24-00098_Vehicle-Stops-data.csv'\npd.read_csv(file_path).head()","8c7693f9":"df = pd.read_csv('..\/input\/cpe-data\/Dept_24-00098\/24-00098_Vehicle-Stops-data.csv', skiprows=(1,2),  parse_dates=['INCIDENT_DATE'])\ndf.head()","ebd8aa6b":"df.shape","2036b86d":"df.columns","ebe65bbc":"df.info()","608cdff7":"df[df.duplicated(keep='first')].head(6)","1c92ac8c":"df[df.duplicated(keep='first')].shape","0020c24a":"df.drop_duplicates(keep='first', inplace=True)\ndf = df.reset_index()","2db32de5":"df.shape","bbe64cb4":"df_pivot = df.pivot_table('INCIDENT_DATE', \n                          index = df['INCIDENT_DATE'].dt.year.rename('YEAR'),\n                          columns = df['INCIDENT_DATE'].dt.month.rename('MONTH'), \n                          aggfunc='count')","21da231e":"plt.figure(figsize=(15,15))\nsns.heatmap(df_pivot, cmap='RdYlGn', fmt=\"d\", annot=True, linewidths=.5)","765d4512":"print(df['LOCATION_LATITUDE'].min(), df['LOCATION_LATITUDE'].max())\nprint(df['LOCATION_LONGITUDE'].min(), df['LOCATION_LONGITUDE'].max())","91ae00d5":"df[df['LOCATION_LATITUDE'].isna()].shape","61dba27c":"df_2015 = df[(df['INCIDENT_DATE'].dt.year==2015)&(df['INCIDENT_DATE'].dt.month==1)&(df['INCIDENT_DATE'].dt.day==1)]\ndf_2015.shape","ab77fca9":"kmap = folium.Map([44.89, -93.00], height=800, zoom_start=10, tiles='CartoDB dark_matter')\nfor j, rown in df_2015.iterrows():\n    if str(rown[\"LOCATION_LATITUDE\"]) != \"nan\":\n        lon = float(rown[\"LOCATION_LATITUDE\"])\n        lat = float(rown[\"LOCATION_LONGITUDE\"])\n        folium.CircleMarker([lon, lat], radius=5, color='red', fill=True).add_to(kmap)\nkmap","aa52cd5e":"race_dict = {'Native Am':'Native American'}\ndf['SUBJECT_RACE'] = df['SUBJECT_RACE'].replace(race_dict)","44fd189e":"plt.figure(figsize=(16,12))\nsns.boxplot(y=\"SUBJECT_AGE\", x=\"SUBJECT_RACE\", hue=\"SUBJECT_GENDER\", data=df)","8a284e22":"At first, I most interested in the biggest dataset. The dataset represents information about Vehicle Stops by policemen. As you can see below we needed to skip one of the the first two rows with `skiprows` and also we can parse date field wit `parse_dates` arguments of `read_csv`.  I skipped the second row. The first seems more human.\nAs result, we have a table with 710471 records and 12 different columns.","b5d176cc":"We cannot see any data, go on deeply!","c4d38c13":"## Imports and Data\n\nA list of using libraries for analysis are below.","2c012edc":"## Part 1. Vehicle Stops","be15d287":"Each folder contains other folders and files. But the main data files (police reports) lie on a level below. Let us check it together. We can see that directories and files name give for us first insight about data. Also, I printed the dimension of data.","3ba166bd":"### Conclusions\nIt is not the end. To be continued... ","8f548676":"### How is distributed of Vehicle Stops by Policemen across years and months?","301c2e9e":"### Data","5add202d":"### How age of drivers is distributed by race and gender?","30814d13":"### Where Policemen were stopped of Vehicles at 1 January 2015?","03805bb0":"### Find Duplicates","ebecc049":"I checked dataset and find some amount duplicates. Not a lot, only 1512."}}