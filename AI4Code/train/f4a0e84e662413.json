{"cell_type":{"771c184f":"code","bb9bb052":"code","b14ba44e":"code","0f9240bb":"code","0c639f3b":"code","b8fd73c3":"code","151b72b2":"code","50f5daa3":"code","ad99431b":"code","e26477b6":"code","7f73532f":"code","a83a0460":"code","2c83fef6":"code","4d7f9812":"code","1f6d89fa":"code","5c3f1a08":"code","85c4ced5":"code","8c490d8a":"code","dadf19ed":"code","dcb27208":"code","f14c80a8":"code","2961c1bc":"code","391b37c8":"code","121aed09":"code","1e3ac6d5":"code","2e1a14ea":"code","f79337b3":"code","38d45273":"code","dfce6f7b":"code","b59d25c2":"code","0b752599":"code","cc973967":"code","c68767b3":"code","340fcd58":"code","8054e85f":"code","461616ea":"code","7698bf2a":"code","b251f867":"code","a476f140":"code","48741753":"code","516d02c7":"code","dba6e62a":"code","bf4d39f9":"code","972ca67a":"code","d5418d13":"code","3a060024":"code","f83d6a22":"code","f5db7c11":"code","6b230bdf":"code","6b50a27e":"code","12907dac":"code","7211b7d4":"code","0ab9a745":"code","b9333686":"code","f8f8b4e9":"code","42f470ff":"code","1017d800":"markdown","5810d489":"markdown","00440998":"markdown","9d515fb1":"markdown","a836cf91":"markdown","d11ab401":"markdown","e8b52819":"markdown","05e3d8cb":"markdown","8b341cc8":"markdown","de737e18":"markdown","54a4a7ea":"markdown","dac768b2":"markdown","3c237ecb":"markdown","b5e7cb55":"markdown","4e68d899":"markdown","c86e04c1":"markdown","ce7b60d4":"markdown","3f9a9d50":"markdown","efe6b7b1":"markdown","9b1eb5af":"markdown","53d5c36d":"markdown","5bf43b87":"markdown","ee5ea6da":"markdown","a2ac3057":"markdown","a4d797c4":"markdown","9014cadf":"markdown","29c211dd":"markdown","c6f46fad":"markdown","cd784616":"markdown","34d5e757":"markdown","36a13bc0":"markdown","e990547e":"markdown","4c9480f0":"markdown","3a67765f":"markdown","ce4066d2":"markdown","805520ed":"markdown","ca1c9c17":"markdown","3c5535a6":"markdown","601517ba":"markdown","81bea165":"markdown","adc61cb2":"markdown","a01f270f":"markdown","3178c890":"markdown","789a495a":"markdown","d7711328":"markdown","cf3d6a8e":"markdown","5faa1327":"markdown"},"source":{"771c184f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","bb9bb052":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 69","b14ba44e":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_all = pd.concat([df_train, df_test], sort = True).reset_index(drop = True)\n\ndf_train.name = 'Train DF'\ndf_test.name = 'Test DF'\ndf_all.name = 'All DF'\n\ndfs = [df_train, df_test]\n\ndef print_df_info(train, test, resp_col):\n    \"\"\"\n    train: train dataframe\n    test: test dataframe\n    resp_col: name of the column of the response\n    \"\"\"\n    print('Number of Training Examples = {}'.format(train.shape[0]))\n    print('Number of Test Examples = {}\\n'.format(test.shape[0]))\n    print('Training X Shape = {}'.format(train.shape))\n    print('Training y Shape = {}\\n'.format(train[resp_col].shape[0]))\n    print('Test X Shape = {}'.format(test.shape))\n    print('Test y Shape = {}\\n'.format(test.shape[0]))\n    print(train.columns)\n    print(test.columns)\n\nprint_df_info(df_train, df_test, 'Survived')","0f9240bb":"df_train.info()","0c639f3b":"df_train.sample(5)","b8fd73c3":"def print_missing_val_count(dfs):\n    \"\"\"\n    dfs is a list of our dataframes (train\/test)\n    \"\"\"\n    for df in dfs:\n        print('{}'.format(df.name))\n        for col in df.columns:\n            print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n        print('\\n')\nprint_missing_val_count(dfs)","151b72b2":"df_corr = df_all.corr().abs().unstack().sort_values(kind = 'quicksort', ascending = False).reset_index()\ndf_corr.rename(columns = {'level_0':'Feature 1', 'level_1':'Feature 2', 0:'Correlation Coefficient'}, inplace = True)\ndf_corr","50f5daa3":"df_corr_age = df_corr[df_corr['Feature 1'] == 'Age']\ndf_corr_age","ad99431b":"pclass_sex_med_group = df_all.groupby(['Sex', 'Pclass']).median().Age\nprint(pclass_sex_med_group)\npclass_sex_mean_group = df_all.groupby(['Sex', 'Pclass']).mean().Age\nprint(pclass_sex_mean_group)","e26477b6":"df_all.Age = df_all.groupby(['Sex', 'Pclass']).Age.apply(lambda x: x.fillna(x.median()))","7f73532f":"df_all[df_all.Embarked.isnull()]","a83a0460":"df_all.groupby(['Sex', 'Pclass']).agg(lambda x:x.value_counts().index[0])","2c83fef6":"df_all.Embarked = df_all.Embarked.fillna(\"C\")","4d7f9812":"df_all[df_all.Fare.isnull()]","1f6d89fa":"fare_price = df_all.groupby(['Embarked', 'Pclass', 'Sex']).Fare.median()\nprint(fare_price)\nfare_price = fare_price['S'][3]['male']\nfare_price","5c3f1a08":"df_all.Fare = df_all.Fare.fillna(fare_price)","85c4ced5":"df_all['Deck'] = df_all['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'M')\n# We want to group the class with the Deck the people were and display the count for each pair \ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\ndf_all_decks","8c490d8a":"def get_pclass_dist(df):\n    \n    # Creating a dictionary for every passenger class count in every deck\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    \n    # Multi-Index column so grab the first one (A,B,C,...)\n    decks = df.columns.levels[0]    \n    for deck in decks:\n        # 3 different classes \n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n\n    # Creating a dictionary for every passenger class percentage in every deck\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count \/ df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages\n\nall_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\n\nall_deck_per","dadf19ed":"def display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    print(df_percentages)\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.75\n    \n    # If we look at the print below we see that by transposing our percentage that first columns is class 1 and so on \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, pclass1, color='#936bf2', edgecolor='white', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f2bd52', edgecolor='white', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#52f2b5', edgecolor='white', width=bar_width, label='Passenger Class 3')\n\n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)   \n    \n    plt.show() \n    \ndisplay_pclass_dist(all_deck_per)","dcb27208":"# Passenger in the T deck is changed to A\nidx = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[idx, 'Deck'] = 'A'","f14c80a8":"df_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n                                                                                   'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'}).transpose()\nprint(df_all_decks_survived)\ndef get_survived_dist(df):\n    \n    # Creating a dictionary for every survival count in every deck\n    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    decks = df.columns.levels[0]    \n\n    for deck in decks:\n        for survive in range(0, 2):\n            surv_counts[deck][survive] = df[deck][survive][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count \/ df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    return surv_counts, surv_percentages\n","2961c1bc":"def display_surv_dist(percentages):\n    \n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85    \n\n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, survived, color='#936bf2', edgecolor='white', width=bar_width, label=\"Survived\")\n    plt.bar(bar_count, not_survived, bottom=survived, color='#f2bd52', edgecolor='white', width=bar_width, label=\"Not Survived\")\n \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage in Decks', size=18, y=1.05)\n    \n    plt.show()\n\nall_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\ndisplay_surv_dist(all_surv_per)","391b37c8":"df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n\ndf_all['Deck'].value_counts()","121aed09":"df_all.drop(['Cabin'], inplace=True, axis=1)","1e3ac6d5":"df_train.index[-1]\ndef divide_df(all_data,last_idx_train, first_idx_test, resp_col):\n    return all_data.loc[:last_idx_train], all_data.loc[last_idx_train:].drop([resp_col], axis = 1)\n\ndf_train, df_test = divide_df(df_all, df_train.index[-1], df_test.index[0], 'Survived')\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Testing Set'\n\ndfs = [df_train, df_test]\nprint_missing_val_count(dfs)","2e1a14ea":"survived = df_train.Survived.value_counts()[1]\ndead = df_train.Survived.value_counts()[0]\nsurvived_per = survived\/(survived + dead) * 100\ndead_per = 100 - survived_per\n\nprint('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))\nprint('{} of {} passengers didnt survive and it is the {:.2f}% of the training set.'.format(dead, df_train.shape[0], dead_per))\n\nplt.figure(figsize=(12,10))\nsns.countplot(df_train.Survived)\n\nplt.xlabel('Survival', size = 15, labelpad = 15)\nplt.ylabel('Passenger Count', size = 15, labelpad = 15)\nplt.xticks((0, 1), ['Not Survived ({0:.2f}%)'.format(dead_per), 'Survived ({0:.2f}%)'.format(survived_per)])\nplt.tick_params(axis='x', labelsize=13)\nplt.tick_params(axis='y', labelsize=13)\n\nplt.title('Training Set Survival Distribution', size=15, y=1.05)\n\nplt.show()","f79337b3":"df_train_corr = df_train.drop(['PassengerId'], axis = 1).corr().abs().unstack().sort_values(kind = 'quicksort', ascending = False).reset_index()\ndf_train_corr.rename(columns = {'level_0':'Feature 1', 'level_1':'Feature 2', 0:'Correlation Coefficient'}, inplace = True)\nprint(df_train_corr)\n# Here we notice that each pairwise correlations repeat itself so we'll drop them\ndf_train_corr.drop(df_train_corr[1::2].index, inplace = True)\n# We'll also drop the correlation to itself \ndf_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index, inplace = True)\ndf_train_corr","38d45273":"df_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n# Here we notice that each pairwise correlations repeat itself so we'll drop them\ndf_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\n# We'll also drop the correlation to itself \ndf_test_corr = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)\ndf_test_corr","dfce6f7b":"# Training set high correlations\ncorr = df_train_corr['Correlation Coefficient'] > 0.1\ndf_train_corr[corr]","b59d25c2":"# Test set high correlations\ncorr = df_test_corr['Correlation Coefficient'] > 0.1\ndf_test_corr[corr]","0b752599":"fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=14)\n    axs[i].tick_params(axis='y', labelsize=14)\n    \naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)\n\nplt.show()","cc973967":"cont_features = ['Age', 'Fare']\nsurv = df_train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(cont_features):    \n    # Distribution of survival in feature\n    sns.distplot(df_train[~surv][feature], label='Not Survived', hist=True, color='#e74c3c', ax=axs[0][i])\n    sns.distplot(df_train[surv][feature], label='Survived', hist=True, color='#2ecc71', ax=axs[0][i])\n    \n    # Distribution of feature in dataset\n    sns.distplot(df_train[feature], label='Training Set', hist=False, color='#e74c3c', ax=axs[1][i])\n    sns.distplot(df_test[feature], label='Test Set', hist=False, color='#2ecc71', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    for j in range(2):        \n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n    \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n        \nplt.show()","c68767b3":"cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=df_train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()","340fcd58":"df_all.head()","8054e85f":"df_all['Fare'] = pd.qcut(df_all['Fare'], 12)","461616ea":"fig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_all)\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","7698bf2a":"df_all['Age'] = pd.qcut(df_all['Age'], 10)","b251f867":"fig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=df_all)\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","a476f140":"df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()","48741753":"df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')","516d02c7":"fig, axs = plt.subplots(figsize=(12, 9))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all)\n\nplt.xlabel('Ticket Frequency', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()","dba6e62a":"df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1","bf4d39f9":"fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()","972ca67a":"def extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families\n\ndf_all['Family'] = extract_surname(df_all['Name'])\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]","d5418d13":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]","3a060024":"mean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA","f83d6a22":"for df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) \/ 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) \/ 2    ","f5db7c11":"non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n\nfor df in dfs:\n    for feature in non_numeric_features:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])","6b230bdf":"cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)","6b50a27e":"df_all = pd.concat([df_train, df_test], sort = True).reset_index(drop = True)\ndrop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n\ndf_all.drop(columns=drop_cols, inplace=True)\n\ndf_all.head()","12907dac":"X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\ny_train = df_train['Survived'].values\nX_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))","7211b7d4":"single_best_model = RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1)\n\nleaderboard_model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1) ","0ab9a745":"N = 5\noob = 0\nprobs = pd.DataFrame(np.zeros((len(X_test), N * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N + 1) for j in range(2)])\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=df_all.columns)\nfprs, tprs, scores = [], [], []\n\nskf = StratifiedKFold(n_splits=N, random_state=N, shuffle=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n    print('Fold {}\\n'.format(fold))\n    \n    # Fitting the model\n    leaderboard_model.fit(X_train[trn_idx], y_train[trn_idx])\n    \n    # Computing Train AUC score\n    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_train[trn_idx], leaderboard_model.predict_proba(X_train[trn_idx])[:, 1])\n    trn_auc_score = auc(trn_fpr, trn_tpr)\n    # Computing Validation AUC score\n    val_fpr, val_tpr, val_thresholds = roc_curve(y_train[val_idx], leaderboard_model.predict_proba(X_train[val_idx])[:, 1])\n    val_auc_score = auc(val_fpr, val_tpr)  \n      \n    scores.append((trn_auc_score, val_auc_score))\n    fprs.append(val_fpr)\n    tprs.append(val_tpr)\n    \n    # X_test probabilities\n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 1]\n    importances.iloc[:, fold - 1] = leaderboard_model.feature_importances_\n        \n    oob += leaderboard_model.oob_score_ \/ N\n    print('Fold {} OOB Score: {}\\n'.format(fold, leaderboard_model.oob_score_))   \n    \nprint('Average OOB Score: {}'.format(oob))","b9333686":"importances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 20))\nsns.barplot(x='Mean_Importance', y=importances.index, data=importances)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Random Forest Classifier Mean Feature Importance Between Folds', size=15)\n\nplt.show()","f8f8b4e9":"def plot_roc_curve(fprs, tprs):\n    \n    tprs_interp = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize=(15, 15))\n    \n    # Plotting ROC for each fold and computing AUC scores\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs), 1):\n        tprs_interp.append(np.interp(mean_fpr, fpr, tpr))\n        tprs_interp[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        ax.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC Fold {} (AUC = {:.3f})'.format(i, roc_auc))\n        \n    # Plotting ROC for random guessing\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8, label='Random Guessing')\n    \n    mean_tpr = np.mean(tprs_interp, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    \n    # Plotting the mean ROC\n    ax.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC (AUC = {:.3f} $\\pm$ {:.3f})'.format(mean_auc, std_auc), lw=2, alpha=0.8)\n    \n    # Plotting the standard deviation around the mean ROC Curve\n    std_tpr = np.std(tprs_interp, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='$\\pm$ 1 std. dev.')\n    \n    ax.set_xlabel('False Positive Rate', size=15, labelpad=20)\n    ax.set_ylabel('True Positive Rate', size=15, labelpad=20)\n    ax.tick_params(axis='x', labelsize=15)\n    ax.tick_params(axis='y', labelsize=15)\n    ax.set_xlim([-0.05, 1.05])\n    ax.set_ylim([-0.05, 1.05])\n\n    ax.set_title('ROC Curves of Folds', size=20, y=1.02)\n    ax.legend(loc='lower right', prop={'size': 13})\n    \n    plt.show()\n\nplot_roc_curve(fprs, tprs)","42f470ff":"class_survived = [col for col in probs.columns if col.endswith('Prob_1')]\nprobs['1'] = probs[class_survived].sum(axis=1) \/ N\nprobs['0'] = probs.drop(columns=class_survived).sum(axis=1) \/ N\nprobs['pred'] = 0\npos = probs[probs['1'] >= 0.5].index\nprobs.loc[pos, 'pred'] = 1\n\ny_pred = probs['pred'].astype(int)\n\nsubmission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test['PassengerId']\nsubmission_df['Survived'] = y_pred.values\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)","1017d800":"We can now drop the feature `Cabin` because we have the information within `Deck`. ","5810d489":"#### **2.2.2 Embarked**\n`Embarked` is a categorical feature and there are only **2** missing values in whole data set.","00440998":"More visualization because why not.","9d515fb1":"### 2.3 Target Distribution","a836cf91":"We're gonna write a function that gives a dictionary that holds information about how many passengers of certain class is in each cabins.","d11ab401":"Interesting enough, we observe that different decks have different survival rates and the ones near the middle are the one with the highest survival rates so it is an important feature. Moreover, we observe that even though the cabin **A** was composed only of passengers of first class, it's survival rate seems below average. We also run in the problem that the Missing deck **M** has the worst survival rate. \n\nWe're going to do a bit of feature engineering and reduce the cardinality of `Deck` by grouping some decks together. \n* **A**, **B** and **C** decks are labeled as **ABC** because all of them have only 1st class passengers\n* **D** and **E** decks are labeled as **DE** because both of them have similar passenger class distribution and same survival rate\n* **F** and **G** decks are labeled as **FG** because of the same reason above\n* **M** deck doesn't need to be grouped with other decks because it is very different from others and has the lowest survival rate.\n","e8b52819":"#### **2.2.4 Cabin**\n`Cabin` feature is non trivial. The large portion of the `Cabin` feature is missing and the feature itself can't be ignored completely because some the cabins might have higher survival rates. \n![alt text](https:\/\/vignette.wikia.nocookie.net\/titanic\/images\/f\/f9\/Titanic_side_plan.png\/revision\/latest?cb=20180322183733)","05e3d8cb":"Let's now check survival rates on each Deck","8b341cc8":"There are too many unique `Ticket` values to analyze, so grouping them up by their frequencies makes things easier.\n\n**How is this feature different than `Family_Size`?** Many passengers travelled along with groups. Those groups consist of friends, nannies, maids and etc. They weren't counted as family, but they used the same ticket.","de737e18":"### **2.6 Conclusion**\nMost of the features are correlated with each other. This relationship can be used to create new features with feature transformation and feature interaction. Target encoding could be very useful as well because of the high correlations with `Survived` feature.\n\nSplit points and spikes are visible in continuous features. They can be captured easily with a decision tree model, but linear models may not be able to spot them.","54a4a7ea":"#### **3.5.2 One-Hot Encoding the Categorical Features**\nThe categorical features (`Pclass`, `Sex`, `Deck`, `Embarked`, `Title`) are converted to one-hot encoded features with `OneHotEncoder`. `Age` and `Fare` features are not converted because they are ordinal unlike the previous ones.","dac768b2":"Since they are both somewhat similar we'll use the median since it's output is integers. ","3c237ecb":"Let's add some visualization by displaying a bar chart with the pourcentage of passenger class for each cabin. ","b5e7cb55":"### **4.2 Feature Importance**","4e68d899":"### **3.4 Target Encoding**\n`extract_surname` function is used for extracting surnames of passengers from the `Name` feature. `Family` feature is created with the extracted surname. This is necessary for grouping passengers in the same family.","c86e04c1":"### **3.3 Title & Is Married**\n`Title` is created by extracting the prefix before `Name` feature. According to graph below, there are many titles that are occuring very few times. Some of those titles doesn't seem correct and they need to be replaced. **Miss**, **Mrs**, **Ms**, **Mlle**, **Lady**, **Mme**, **the Countess**, **Dona** titles are replaced with **Miss\/Mrs\/Ms** because all of them are female. Values like **Mlle**, **Mme** and **Dona** are actually the name of the passengers, but they are classified as titles because `Name` feature is split by comma. **Dr**, **Col**, **Major**, **Jonkheer**, **Capt**, **Sir**, **Don** and **Rev** titles are replaced with **Dr\/Military\/Noble\/Clergy** because those passengers have similar characteristics. **Master** is a unique title. It is given to male passengers below age **26**. They have the highest survival rate among all males.","ce7b60d4":"#### **2.2.1 Age**\nMissing values in `Age` might be important to fill, let's explore how we're going to do that. We're going to print the pairwise correlation between each features. ","3f9a9d50":"### **4.1 Random Forest**\nCreated 2 `RandomForestClassifier`'s. One of them is a single model and the other is for k-fold cross validation.","efe6b7b1":"`StratifiedKFold` is used for stratifying the target variable. The folds are made by preserving the percentage of samples for each class in target variable (`Survived`).","9b1eb5af":"#### **3.1.2 Age**\n`Age` feature has a normal distribution with some spikes and bumps and **10** quantile based bins are used for `Age`. The first bin has the highest survival rate and 4th bin has the lowest survival rate. Those were the biggest spikes in the distribution. There is also an unusual group **(34.0, 40.0]** with high survival rate that is captured in this process.","53d5c36d":"### **2.5.2 Categorical Features**\n\nEvery categorical feature has at least one class with high mortality rate. Those classes are helpful to predict whether the passenger is a survivor or victim.","5bf43b87":"Let's do the same for test","ee5ea6da":"Both are female and of upper class so let's see what's the most frequent embarkement from that group. We'll look at the mode.","a2ac3057":"# 4. Model","a4d797c4":"We're going to split our entire dataset cleaned into training and testing. ","9014cadf":"### 4.4 Submission","29c211dd":"### **3.1.1 Fare**\n`Fare` feature is positively skewed and survival rate is extremely high on the right end. **12** quantile based bins are used for `Fare` feature. Even though the bins are too much, they provide decent amount of information gain. The groups at the left side of the graph has the lowest survival rate and the groups at the right side of the graph has the highest survival rate. This high survival rate was not visible in the distribution graph. ","c6f46fad":"Let's grab the median from that group","cd784616":"### **3.2 Frequency Encoding**\n`Family_Size` is created by adding `SibSp`, `Parch` and **1**. `SibSp` is the count of siblings and spouse, and `Parch` is the count of parents and children. Those columns are added in order to find the total size of families. Adding **1** at the end, is the current passenger. Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates.\n* Family Size with **1** are labeled as **Alone**\n* Family Size with **2**, **3** and **4** are labeled as **Small**\n* Family Size with **5** and **6** are labeled as **Medium**\n* Family Size with **7**, **8** and **11** are labeled as **Large**","34d5e757":"We get that the most common is **C** for that specific group. ","36a13bc0":"We're going to create a new feature using cabins feature. `M` will stand for Missing.","e990547e":"### **3.5.1 Label Encoding Non-Numerical Features**\n`Embarked`, `Sex`, `Deck` , `Title` and `Family_Size_Grouped` are object type, and `Age` and `Fare` features are category type. They are converted to numerical type with `LabelEncoder`. `LabelEncoder` basically labels the classes from **0** to **n**. This process is necessary for models to learn from those features.","4c9480f0":"We're going to also group using `Sex` feature in order to increase accuracy. Should we fill the values using the mean or median for example? Let's first see if the two differ by much. ","3a67765f":"\n\nfig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=14)\n    axs[i].tick_params(axis='y', labelsize=14)\n    \naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)\n\nplt.show()","ce4066d2":"### **3.6 Conclusion**\n`Age` and `Fare` features are binned. Binning helped dealing with outliers and it revealed some homogeneous groups in those features. `Family_Size` is created by adding `Parch` and `SibSp` features and **1**. `Ticket_Frequency` is created by counting the occurence of `Ticket` values.\n\n`Name` feature is very useful. First, `Title` and `Is_Married` features are created from the title prefix in the names. Second, `Family_Survival_Rate` and `Family_Survival_Rate_NA`  features are created by target encoding the surname of the passengers. `Ticket_Survival_Rate` is created by target encoding the `Ticket` feature. `Survival_Rate` feature is created by averaging the `Family_Survival_Rate` and `Ticket_Survival_Rate` features.\n\nFinally, the non-numeric type features are label encoded and categorical features are one-hot encoded. Created **5** new features (`Family_Size`, `Title`, `Is_Married`, `Survival_Rate` and `Survival_Rate_NA`) and dropped the useless features after encoding.","805520ed":"\n\nfig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=14)\n    axs[i].tick_params(axis='y', labelsize=14)\n    \naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)\n\nplt.show()","ca1c9c17":"### **2.2 Missing Values**\nAs seen from the info of the training data, some columns have missing values. Let's figure out exactly how many are missing. ","3c5535a6":"# 2. Exploratory Data Analysis\n\n### **2.1 Overview**\n* `PassengerId` is the unique id of the row and it doesn't have any effect on target\n* `Survived` is the target variable we are trying to predict (**0** or **1**):\n    - **1 = Survived**\n    - **0 = Not Survived**\n* `Pclass` (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has **3** unique values (**1**,  **2** or **3**):\n    - **1 = Upper Class**\n    - **2 = Middle Class**\n    - **3 = Lower Class**\n* `Name`, `Sex` and `Age` are self-explanatory\n* `SibSp` is the total number of the passengers' siblings and spouse\n* `Parch` is the total number of the passengers' parents and children\n* `Ticket` is the ticket number of the passenger\n* `Fare` is the passenger fare\n* `Cabin` is the cabin number of the passenger\n* `Embarked` is port of embarkation and it is a categorical feature which has **3** unique values (**C**, **Q** or **S**):\n    - **C = Cherbourg**\n    - **Q = Queenstown**\n    - **S = Southampton**","601517ba":"# 1. Start by creating our datasets","81bea165":"### 3.5 Feature Transformation","adc61cb2":"### 4.3 ROC Curve","a01f270f":"* **100%** of **A**, **B** and **C** decks are 1st class passengers\n* Deck **D** has **87%** 1st class and **13%** 2nd class passengers\n* Deck **E** has **83%** 1st class, **10%** 2nd class and **7%** 3rd class passengers\n* Deck **F** has **62%** 2nd class and **38%** 3rd class passengers\n* **100%** of **G** deck are 3rd class passengers\n* There is one person on the boat deck in **T** cabin and he is a 1st class passenger. **T** cabin passenger has the closest resemblance to **A** deck passengers so he is grouped with **A** deck\n* Passengers labeled as **M** are the missing values in `Cabin` feature. I don't think it is possible to find those passengers' real `Deck` so I decided to use **M** like a deck","3178c890":"### **2.5.1 Continuous Features**\nBoth of the continuous features (`Age` and `Fare`) have good split points and spikes for a decision tree to learn. One potential problem for both features is, the distribution has more spikes and bumps in training set, but it is smoother in test set. Model may not be able to generalize to test set because of this reason.","789a495a":"### 2.4 Correlations\n\nLet's go through the pairwise coefficients again, we can drop `PassengerId` since we don't expect any causation due to the random `ID` attached to passengers. ","d7711328":"Our data is cleaned! No more missing values. ","cf3d6a8e":"# 3. Feature Engineering","5faa1327":"#### **2.2.3 Fare**\n`Fare` is a value feature but there is only one passenger with missnig value."}}