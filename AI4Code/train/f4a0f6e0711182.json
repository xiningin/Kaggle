{"cell_type":{"d7a034db":"code","2e47b44e":"code","649c9d3e":"code","9bc0c0e2":"code","f7ead5b5":"code","52943fc2":"code","6cfceda3":"code","691b218d":"code","ee043daa":"code","070787a2":"code","f0e3f930":"code","8b0e1921":"code","86c43ec8":"code","b54d0c05":"code","e98fa160":"code","5e5f1fe2":"markdown","54c56fee":"markdown","17237b63":"markdown","7b0f1efb":"markdown","8f3df7c1":"markdown","800b91b7":"markdown","da875e12":"markdown"},"source":{"d7a034db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\n# Any results you write to the current directory are saved as output.\n# Others\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem.snowball import SnowballStemmer","2e47b44e":"train_df=pd.read_csv('..\/input\/train.csv')","649c9d3e":"train_df.shape","9bc0c0e2":"def clean_text(text):\n    \n    ## Remove puncuation\n    text = text.translate(string.punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    ## Remove stop words\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops and len(w) >= 3]\n    \n    text = \" \".join(text)\n\n    ## Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    ## Stemming\n    text = text.split()\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n\n    return text","f7ead5b5":"train_df['question_text'] = train_df['question_text'].map(lambda x: clean_text(x))","52943fc2":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_df['question_text'])\n\ntrain_sequences = tokenizer.texts_to_sequences(train_df['question_text'])\ntrain_data = pad_sequences(train_sequences, maxlen=100)","6cfceda3":"\nEMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))","691b218d":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((50000, 300))\nfor word, index in tokenizer.word_index.items():\n    if index > 50000 - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector","ee043daa":"model_glove = Sequential()\nmodel_glove.add(Embedding(50000, 300, input_length=100, weights=[embedding_matrix], trainable=False))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(Conv1D(64, 5, activation='relu'))\nmodel_glove.add(MaxPooling1D(pool_size=4))\nmodel_glove.add(LSTM(300))\nmodel_glove.add(Dense(1, activation='sigmoid'))\nmodel_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","070787a2":"model_glove.fit(train_data, train_df['target'], validation_split=0.2, epochs = 2)","f0e3f930":"test_df=pd.read_csv('..\/input\/test.csv')","8b0e1921":"test_df['question_text'] = test_df['question_text'].map(lambda x: clean_text(x))","86c43ec8":"test_sequences = tokenizer.texts_to_sequences(test_df['question_text'])\ntest_data = pad_sequences(test_sequences, maxlen=100)","b54d0c05":"predictions= model_glove.predict_classes(test_data)","e98fa160":"submission_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nsubmission_df['prediction'] = predictions\nsubmission_df.to_csv(\"submission.csv\", index=False)","5e5f1fe2":"#### Text Preprocessing","54c56fee":"#### It takes around 1 hour to fit the model on the data for 2 epochs. Please note that I haven't split the training data into train and valid sets for simplicity.","17237b63":"This kernel is based on the tutorial series written by Sabber Ahamed @msahamed on the Medium.\nhttps:\/\/medium.com\/@sabber\/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa","7b0f1efb":"#### Embedding vector forms the first layer followed by Convolutional network and finally wrapped by LSTM.","8f3df7c1":"#### Pre-processing on the text data.","800b91b7":"#### Maximum length of the question text is set to be 100. If the text is less 100 characters, zeroes will be padded else the text will be truncated","da875e12":"#### Using Glove pre-trained model"}}