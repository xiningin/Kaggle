{"cell_type":{"0bb77556":"code","5ea427d8":"code","921a8df1":"code","2901f066":"code","6af58aa7":"code","bbd9b945":"code","33abd8cb":"code","2caf43e1":"code","7e857215":"code","9220ac82":"code","c7e72b1a":"markdown","d89ae5a9":"markdown","2581b1a9":"markdown","5990050a":"markdown","42d6bdb9":"markdown","3636f23c":"markdown","ef5018e4":"markdown","d91c88e8":"markdown","b7a9bf5a":"markdown","7f131230":"markdown"},"source":{"0bb77556":"MAX_LEN = 192  #Reduced for quicker execution\nLR = 1e-5\nBATCH_SIZE = 16 # per TPU core\nTOTAL_STEPS_STAGE1 = 300\nVALIDATE_EVERY_STAGE1 = 100\nTOTAL_STEPS_STAGE2 = 200\nVALIDATE_EVERY_STAGE2 = 100\n\nPRETRAINED_MODEL = 'jplu\/tf-xlm-roberta-large'\nD = '\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/'\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nimport logging\n# no extensive logging \nlogging.getLogger().setLevel(logging.NOTSET)\n\nAUTO = tf.data.experimental.AUTOTUNE","5ea427d8":"def connect_to_TPU():\n    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n\n    return tpu, strategy, global_batch_size\n\n\ntpu, strategy, global_batch_size = connect_to_TPU()\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","921a8df1":"train_df = pd.read_csv(D+'jigsaw-toxic-comment-train.csv')\nval_df = pd.read_csv(D+'validation.csv')\ntest_df = pd.read_csv(D+'test.csv')\nsub_df = pd.read_csv(D+'sample_submission.csv')\n\n# subsample the train dataframe to 50%-50%\ntrain_df = pd.concat([\n    train_df.query('toxic==1'),\n    train_df.query('toxic==0').sample(sum(train_df.toxic),random_state=42)\n])\n# shufle it just to make sure\ntrain_df = train_df.sample(frac=1, random_state = 42)","2901f066":"%%time\n\ndef regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])\n    \n\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\nX_train = regular_encode(train_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\nX_val = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\nX_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)\n\ny_train = train_df.toxic.values.reshape(-1,1)\ny_val = val_df.toxic.values.reshape(-1,1)","6af58aa7":"def create_dist_dataset(X, y=None, training=False):\n    dataset = tf.data.Dataset.from_tensor_slices(X)\n\n    ### Add y if present ###\n    if y is not None:\n        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n        \n    ### Repeat if training ###\n    if training:\n        dataset = dataset.shuffle(len(X)).repeat()\n\n    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n\n    ### make it distributed  ###\n    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n    return dist_dataset\n    \n    \ntrain_dist_dataset = create_dist_dataset(X_train, y_train, True)\nval_dist_dataset   = create_dist_dataset(X_val)\ntest_dist_dataset  = create_dist_dataset(X_test)","bbd9b945":"%%time\n\ndef create_model_and_optimizer():\n    with strategy.scope():\n        transformer_layer = TFAutoModel.from_pretrained(PRETRAINED_MODEL)                \n        model = build_model(transformer_layer)\n        optimizer = tf.keras.optimizers.Adam(learning_rate=LR, epsilon=1e-08)\n    return model, optimizer\n\n\ndef build_model(transformer):\n    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n    # Huggingface transformers have multiple outputs, embeddings are the first one\n    # let's slice out the first position, the paper says its not worse than pooling\n    x = transformer(inp)[0][:, 0, :]  \n    out = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=[inp], outputs=[out])\n    \n    return model\n\n\nmodel, optimizer = create_model_and_optimizer()\nmodel.summary()","33abd8cb":"def define_losses_and_metrics():\n    with strategy.scope():\n        loss_object = tf.keras.losses.BinaryCrossentropy(\n            reduction=tf.keras.losses.Reduction.NONE, from_logits=False)\n\n        def compute_loss(labels, predictions):\n            per_example_loss = loss_object(labels, predictions)\n            loss = tf.nn.compute_average_loss(\n                per_example_loss, global_batch_size = global_batch_size)\n            return loss\n\n        train_accuracy_metric = tf.keras.metrics.AUC(name='training_AUC')\n\n    return compute_loss, train_accuracy_metric\n\n\n\ndef train(train_dist_dataset, val_dist_dataset=None, y_val=None,\n          total_steps=5000, validate_every=500):\n    step = 0\n    ### Training lopp ###\n    for tensor in train_dist_dataset:\n        distributed_train_step(tensor) \n        step+=1\n\n        if (step % validate_every == 0):   \n            ### Print train metrics ###  \n            train_metric = train_accuracy_metric.result().numpy()\n            print(\"Step %d, train AUC: %.5f\" % (step, train_metric))   \n            \n            ### Test loop with exact AUC ###\n            if val_dist_dataset:\n                val_metric = roc_auc_score(y_val, predict(val_dist_dataset))\n                print(\"     validation AUC: %.5f\" %  val_metric)   \n\n            ### Reset (train) metrics ###\n            train_accuracy_metric.reset_states()\n            \n        if step  == total_steps:\n            break\n\n\n\n@tf.function\ndef distributed_train_step(data):\n    strategy.experimental_run_v2(train_step, args=(data,))\n\ndef train_step(inputs):\n    features, labels = inputs\n\n    with tf.GradientTape() as tape:\n        predictions = model(features, training=True)\n        loss = compute_loss(labels, predictions)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    train_accuracy_metric.update_state(labels, predictions)\n\n\n\n\ndef predict(dataset):  \n    predictions = []\n    for tensor in dataset:\n        predictions.append(distributed_prediction_step(tensor))\n    ### stack replicas and batches\n    predictions = np.vstack(list(map(np.vstack,predictions)))\n    return predictions\n\n@tf.function\ndef distributed_prediction_step(data):\n    predictions = strategy.experimental_run_v2(prediction_step, args=(data,))\n    return strategy.experimental_local_results(predictions)\n\ndef prediction_step(inputs):\n    features = inputs  # note datasets used in prediction do not have labels\n    predictions = model(features, training=False)\n    return predictions\n\n\ncompute_loss, train_accuracy_metric = define_losses_and_metrics()","2caf43e1":"%%time\ntrain(train_dist_dataset, val_dist_dataset, y_val,\n      TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1)","7e857215":"%%time\n# make a new dataset for training with the validation data \n# with targets, shuffling and repeating\nval_dist_dataset_4_training = create_dist_dataset(X_val, y_val, training=True)\n\n# train again\ntrain(val_dist_dataset_4_training,\n      total_steps = TOTAL_STEPS_STAGE2, \n      validate_every = VALIDATE_EVERY_STAGE2)  # not validating but printing now","9220ac82":"%%time\nsub_df['toxic'] = predict(test_dist_dataset)[:,0]\nsub_df.to_csv('submission.csv', index=False)","c7e72b1a":"## Build model from pretrained transformer\n\n- Note: Downloading the model takes some time!","d89ae5a9":"## Tokenize  it with the models own tokenizer\n\n- Note it takes some time!\n- Note, we need to reshape the targets","2581b1a9":"## Create distributed tensorflow datasets\n\n- Note, validation dataset does not contain labels, we keep track of it ourselves","5990050a":"## Finetune it on the validation data","42d6bdb9":"## Make predictions and submission","3636f23c":"### Define stuff for the custom training loop\n\nWe will need:\n- 1, losses, and  optionally a training AUC metric here: these need to be defined in the scope of th distributed strategy. \n- 2, A full training loop\n- 3, A distributed train step called in the training loop, which uses a single replica train step\n- 4, A prediction loop with dstibute \n\n\n\n- Note, we are using exact AUC, for the valdationdata, and approximate AUC for the training data","ef5018e4":"## Finally train it on english comments\n\n\n- Note it takes some time\n- Don't mind the warning: \"Converting sparse IndexedSlices to a dense Tensor\"","d91c88e8":" ## Load text data into memory","b7a9bf5a":"## About this notebook\n\n\nI have seen many great notebooks using Pytorch and the Keras built in training loop, but I wanted to share one which uses a custom training loop in TensorFlow 2.\n\nI hope this starter will allow more people to start experimenting with their unique ideas for tweaking.\n\nAs an example, a custom loop allows us to use the exact AUC for validation instead of the  (very convenient) [approximate value used in Keras](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/metrics\/AUC). The two values may differ if predictions are close to each other, and not uniformly distributed (both are happening here).\n\n** Note: This notebook tries to be simple, and only uses a small amount of data, and it does not use translated datasets or other tricks. You need to add those yourself to squeeze out a good score.**\n\nSuggestions\/improvements are appreciated!\n\n---\n\n### References:\n\n\n- This notebook heavily relies on the great [notebook]((https:\/\/www.kaggle.com\/xhlulu\/\/jigsaw-tpu-xlm-roberta) by, Xhulu: [@xhulu](https:\/\/www.kaggle.com\/xhulu\/) \n- The tensorflow distrubuted training tutorial: [Link](https:\/\/www.tensorflow.org\/tutorials\/distribute\/custom_training)","7f131230":"## Connect to TPU"}}