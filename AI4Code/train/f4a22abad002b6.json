{"cell_type":{"b40ffc33":"code","47f46506":"code","2ad1d116":"code","26fe862b":"code","c2e86897":"code","45436053":"code","ce2a8c69":"code","50772212":"code","178307fa":"code","6d7a0d21":"code","c9f2ac5a":"code","56ac53db":"code","e079eca7":"code","9bd75d1a":"code","fdef9664":"code","7c42ca88":"code","e43725a8":"code","9892e16b":"code","6ec65e7c":"code","3d2b6fad":"code","340b89e6":"code","66d5e62f":"code","ae944486":"code","f5c8e8ef":"code","50aa3848":"code","eb683a9d":"code","a456f147":"code","bec36c2d":"code","09184ec9":"code","3f733a06":"code","063dcc99":"code","9806fdc9":"code","0db1b7f6":"code","1a506bc0":"code","165f03d8":"code","3a6b28f5":"code","316c0139":"code","57bb4d8e":"code","184a69b1":"code","475c8de8":"code","1e0c034e":"code","bf1cf1e3":"code","34e6eb24":"code","1c5b64a7":"code","58264ce7":"code","e9afefc1":"code","486b5ec5":"code","659f2842":"code","b1f9ec8c":"code","729d02ed":"code","4ae9d056":"code","4079ce41":"code","99bc748b":"code","b9f8e5ca":"code","6c55b33d":"code","98d4e218":"code","9147f49b":"code","a5c7cfaa":"code","ce87cec7":"code","a39f5b7e":"code","edfeff51":"code","10578b30":"code","c44dafcc":"code","5b706369":"code","b4ddcde1":"code","b65c8fbe":"code","82fa7f76":"code","b5171da6":"code","cdff9f6e":"code","b800887a":"code","eac63f3c":"code","609008c4":"code","656afe29":"code","f3f39056":"code","f3049df8":"code","e2ff3b22":"code","3669b8b2":"code","fccf96aa":"code","ffc0e719":"code","fc5fd960":"code","cf6084a0":"code","0e7dfd5e":"code","c8669d54":"code","99c0ead8":"code","2f61676e":"code","ce21a7a9":"code","c4fc4dfd":"code","4c620a1a":"code","fe1cb512":"code","2e0c8214":"code","8fb019dc":"code","3bf11595":"code","54e7bc4f":"code","d74242a8":"code","11ebbe12":"code","92bf49a0":"code","255f6d87":"code","0905aab4":"code","0d57dfa5":"code","5117ea78":"code","9947f928":"code","ce69af8f":"code","e8ebd19d":"code","daaac332":"code","78feb063":"code","11071179":"code","7dc62346":"code","9cd02b5a":"code","1ba27cb4":"code","831dbb92":"code","5e4368ad":"code","00887c3b":"code","fe8ccb02":"code","8c6131d5":"code","4317aaf8":"code","dcb7e9fe":"code","fd8dcecb":"code","b290fa14":"code","5117950e":"code","e218d975":"code","1487ff1f":"code","f1ed261f":"markdown","cda90ae6":"markdown","b698b1cf":"markdown","36104f99":"markdown","8f58f431":"markdown","75a4a72a":"markdown","64506205":"markdown","8d03a77d":"markdown","35cb14fe":"markdown","f1bdd431":"markdown","00d6ca0c":"markdown","7e1c20b4":"markdown","a53e005d":"markdown","a800b27d":"markdown","fc2861e1":"markdown","59842132":"markdown","1d2727e3":"markdown","34f7e8fc":"markdown","6de94081":"markdown","43a7f516":"markdown","f34dfbd4":"markdown","c61e9f82":"markdown","51212001":"markdown","e16f85d8":"markdown","374c6588":"markdown","4b18ffc6":"markdown","1c44e0ee":"markdown","fb4f3938":"markdown","9c16bd89":"markdown","6b5459e8":"markdown","e1f1f21d":"markdown","f0f42fcb":"markdown","9a1d83f1":"markdown","63d8fc58":"markdown","54fb779c":"markdown","503d59d1":"markdown","cf98e212":"markdown","9cd74ac8":"markdown","cf21ef12":"markdown","b08c363b":"markdown","9ed978ed":"markdown","df43201e":"markdown","920f2c70":"markdown","13358eef":"markdown","f0ec069b":"markdown","5484361a":"markdown","d5db1c99":"markdown","6de09ad2":"markdown","b906c1d5":"markdown","5905411f":"markdown","e0ded9af":"markdown","9baa9063":"markdown","d2b543fa":"markdown","e6ccd7b8":"markdown","eb440c88":"markdown","ed65fc3e":"markdown","3ee3a4f6":"markdown","cb562158":"markdown","2ed3044b":"markdown","6be30578":"markdown","19e69d66":"markdown","8e7de8df":"markdown","dfc4192b":"markdown","ec2f19f4":"markdown","dd9b1e9b":"markdown","dd11490a":"markdown","7d3a7522":"markdown","901873cc":"markdown","ea74a7cd":"markdown","b190ad35":"markdown","0e748344":"markdown","1f5fa50e":"markdown","3e7938ac":"markdown","2ceba618":"markdown","08d900a3":"markdown"},"source":{"b40ffc33":"import requests\nimport os","47f46506":"# This function will be used to acquire the data from the UCI website\ndef aquire_data(path_to_data, data_urls):\n    if not os.path.exists(path_to_data):\n        os.mkdir(path_to_data)\n        \n    for url in data_urls:\n        data = requests.get(url).content\n        filename = os.path.join(path_to_data, os.path.basename(url))\n        with open(filename, 'wb') as file: \n            file.write(data)","2ad1d116":"data_urls = [\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/adult\/adult.data\",\n             \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/adult\/adult.names\",\n             \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/adult\/adult.test\"]\n\naquire_data('data', data_urls)","26fe862b":"# Check the success of accessing the data\nprint('Output n\u00b0 {}\\n'.format(1))\n! find data","c2e86897":"column_names = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \n                \"Martial Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \n                \"Capital-Gain\", \"Capital-Loss\", \"Hours-per-week\", \"Country\", \"Income\"] \n","45436053":"import pandas as pd\nimport numpy as np","ce2a8c69":"train = pd.read_csv('data\/adult.data', names=column_names, sep=' *, *', na_values='?', \n                   engine='python')\ntest = pd.read_csv('data\/adult.test', names=column_names, sep=' *, *', skiprows=1, \n                   engine='python', na_values='?')","50772212":"test.Income.unique() ","178307fa":"train.Income.unique()","6d7a0d21":"test.Income = np.where(test.Income == '<=50K.', '<=50K', '>50K')","c9f2ac5a":"# Concatenate train and test. We will split it before the training phase \ndf = pd.concat((train, test), axis=0)","56ac53db":"df.Income.unique()","e079eca7":"print('Output n\u00b0 {}\\n'.format(2))\n\n'''\nFirst 5 observations\n'''\ndf.head()","9bd75d1a":"print('Output n\u00b0 {}\\n'.format(3))\n\n'''\nLast 5 observations\n'''\ndf.tail()","fdef9664":"print('Output n\u00b0 {}\\n'.format(4))\n\nprint('Our data contains {} observations and {} columns.'.format(df.shape[0],\n                                                                df.shape[1]))","7c42ca88":"print('Output n\u00b0 {}\\n'.format(5))\nprint(df.isnull().sum())","e43725a8":"print('Output n\u00b0 {}\\n'.format(6))\nprint(df.dtypes)","9892e16b":"# Workclass  \nprint('Output n\u00b0 {}\\n'.format(7))\nprint('Number of missing values: {}'.format(len(df['Workclass'].unique())))\nprint(df['Workclass'].unique())","6ec65e7c":"# Occupation  \nprint(print('Output n\u00b0 {}\\n'.format(8)))\nprint('Number of missing values: {}'.format(len(df['Occupation'].unique())))\nprint(df['Occupation'].unique())","3d2b6fad":"# Country  \nprint('Output n\u00b0 {}\\n'.format(9))\nprint('Number of missing values: {}'.format(len(df['Country'].unique())))\nprint(df['Country'].unique())\n","340b89e6":"import statistics as stat","66d5e62f":"def fill_categorical_missing(data, column):\n    data.loc[data[column].isnull(), column] = stat.mode(data[column])","ae944486":"cols_to_fill = ['Workclass', 'Occupation', 'Country']\n\nfor col in cols_to_fill:\n    fill_categorical_missing(df, col)\n\nprint('Output n\u00b0 {}\\n'.format(10))\n\n# Check the final data if there is any missing values \nprint(df.isnull().sum())","f5c8e8ef":"df_cp = df.copy()","50aa3848":"df_cp.head()","eb683a9d":"df_cp.describe()","a456f147":"import seaborn as sns \nimport numpy as np\nimport matplotlib.pyplot as plt","bec36c2d":"# Age \nsns.boxplot(y='Age', data=df_cp)\nplt.show()","09184ec9":"def ten_to_ten_percentiles(data, column):\n    for i in range(0,100,10):\n        var = data[column].values\n        var = np.sort(var, axis=None)\n        print('{} percentile value is {}'.format(i, var[int(len(var) * (float(i)\/100))]))\n    print('100 percentile value is {}'.format(var[-1]))","3f733a06":"ten_to_ten_percentiles(df_cp, 'Age')","063dcc99":"#calculating column values at each percntile 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\ndef percentiles_from_90(data, column):\n    for i in range(90,100):\n        var = data[column].values\n        var = np.sort(var, axis=None)\n        print('{} percentile value is {}'.format(i, var[int(len(var) * (float(i)\/100))]))\n    print('100 percentile value is {}'.format(var[-1]))","9806fdc9":"#calculating colunm values at each percntile 99.0,99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100\ndef percentiles_from_99(data, column):\n    for i in np.arange(0.0, 1.0, 0.1):\n        var =data[column].values\n        var = np.sort(var,axis = None)\n        print(\"{} percentile value is {}\".format(99+i,var[int(len(var)*(float(99+i)\/100))]))\n    print(\"100 percentile value is \",var[-1])","0db1b7f6":"# Education-Num\nsns.boxplot(y='Education-Num', data=df_cp)\nplt.show()","1a506bc0":"ten_to_ten_percentiles(df_cp, 'Education-Num')","165f03d8":"# Capital-Gain\nsns.boxplot(y='Capital-Gain', data=df_cp)\nplt.show()","3a6b28f5":"ten_to_ten_percentiles(df_cp, 'Capital-Gain')","316c0139":"percentiles_from_90(df_cp, 'Capital-Gain')","57bb4d8e":"percentiles_from_99(df_cp, 'Capital-Gain')","184a69b1":"# Removing the outliers based on 99.5th percentile of Capital-Gain\ndf_cp = df_cp[df_cp['Capital-Gain']<=34095]","475c8de8":"# Capital-Gain\nsns.boxplot(y='Capital-Gain', data=df_cp)\nplt.show()","1e0c034e":"# Capital-Loss\nsns.boxplot(y='Capital-Loss', data=df_cp)\nplt.show()","bf1cf1e3":"ten_to_ten_percentiles(df_cp, 'Capital-Loss')","34e6eb24":"percentiles_from_90(df_cp, 'Capital-Loss')","1c5b64a7":"percentiles_from_99(df_cp, 'Capital-Loss')","58264ce7":"# Hours-per-week\nsns.boxplot(y='Hours-per-week', data=df_cp)\nplt.show()","e9afefc1":"ten_to_ten_percentiles(df_cp, 'Hours-per-week')","486b5ec5":"def remove_outliers(data):\n    a = data.shape[0]\n    print(\"Number of salary records = {}\".format(a))\n        \n    temp_data = data[data['Capital-Gain']<=34095]\n    b = temp_data.shape[0]\n    \n    print('Number of outliers from the Capital-Gain column= {}'.format(a - b))\n        \n    data = data[(data['Capital-Gain']<=34095)]\n    \n    print('Total outlies removed = {}'.format(a-b))\n    print('-----'*10)\n    return data","659f2842":"print('Removing all the outliers from the data')\nprint('-----'*10)\ndf_no_outliers = remove_outliers(df)\n\nproportion_remaing_data = float(len(df_no_outliers)) \/ len(df)\nprint('Proportion of observation that remain after removing outliers = {}'.format(proportion_remaing_data))","b1f9ec8c":"df_no_outliers.Income.unique()","729d02ed":"palette = {\"<=50K\":\"r\", \">50K\":\"g\"}\nsns.countplot(x=\"Income\", data=df_no_outliers, hue=\"Income\", palette=palette)","4ae9d056":"df_no_outliers.describe()","4079ce41":"# Age  \ndf_no_outliers.Age.plot(kind='kde', title='Density plot for Age', color='c')","99bc748b":"# Capital-Gain  \ndf_no_outliers['Capital-Gain'].plot(kind='kde', title='Density plot for Capital-Gain', color='c')","b9f8e5ca":"# Capital-Loss  \ndf_no_outliers['Capital-Loss'].plot(kind='kde', title='Density plot for Capital-Loss', color='c')","6c55b33d":"# Capital-Loss  \ndf_no_outliers['Hours-per-week'].plot(kind='kde', title='Density plot for Hours-per-week', color='c')","98d4e218":"# Capital-Gain and Education-Num \n# use scatter plot for bi-variate distribution\ndf_no_outliers.plot.scatter(x='Education-Num', y='Capital-Gain', color='c', title='scatter plot : Education-Num vs Capital-Gain');","9147f49b":"# Hours-per-week and Education-Num \n# use scatter plot for bi-variate distribution\ndf_no_outliers.plot.scatter(x='Education-Num', y='Hours-per-week', color='c', title='scatter plot : Education-Num vs Hours-per-week');","a5c7cfaa":"# Capital-Gain and Hours-per-week\n# use scatter plot for bi-variate distribution\ndf_no_outliers.plot.scatter(x='Hours-per-week', y='Capital-Gain', color='c', title='scatter plot : Hours-per-week vs Capital-Gain');","ce87cec7":"# Capital-Gain and Capital-Loss\n# use scatter plot for bi-variate distribution\ndf_no_outliers.plot.scatter(x='Capital-Gain', y='Capital-Loss', color='c', title='scatter plot : Capital-Loss vs Capital-Gain');","a39f5b7e":"numerical_cols = ['int64']  \nplt.figure(figsize=(10, 10))\nsns.heatmap( \n            df_no_outliers.select_dtypes(include=numerical_cols).corr(),\n            cmap=plt.cm.RdBu, \n            vmax=1.0,\n            linewidths=0.1,\n            linecolor='white', \n            square=True,\n            annot=True\n)","edfeff51":"df_no_outliers.head()","10578b30":"df_no_outliers['Country'].unique()","c44dafcc":"south_df = df_no_outliers[df_no_outliers['Country']=='South']\na = south_df.shape[0]\nb = df_no_outliers.shape[0]\n\nprint('{} rows corresponds to South, which represents {}% of the data'.format(a, (1.0*a\/b)*100))","5b706369":"south_index = south_df.index \ndf_no_outliers.drop(south_index, inplace=True)","b4ddcde1":"# Changing the corresponding values.\ndf_no_outliers.loc[df_no_outliers['Country']=='Outlying-US(Guam-USVI-etc)', 'Country'] = 'Outlying-US'\ndf_no_outliers.loc[df_no_outliers['Country']=='Trinadad&Tobago', 'Country'] = 'Trinadad-Tobago'\ndf_no_outliers.loc[df_no_outliers['Country']=='Hong', 'Country'] = 'Hong-Kong'","b65c8fbe":"# Check if the process worked\ndf_no_outliers['Country'].unique()","82fa7f76":"asia = ['India', 'Iran', 'Philippines', 'Cambodia', 'Thailand', 'Laos', 'Taiwan', \n       'China', 'Japan', 'Vietnam', 'Hong-Kong']  \n\namerica = ['United-States', 'Cuba', 'Jamaica', 'Mexico', 'Puerto-Rico', 'Honduras', \n           'Canada', 'Columbia', 'Ecuador', 'Haiti', 'Dominican-Republic', \n           'El-Salvador', 'Guatemala', 'Peru', 'Outlying-US', 'Trinadad-Tobago', \n           'Nicaragua', '']  \n\neurope = ['England', 'Germany', 'Italy', 'Poland', 'Portugal', 'France', 'Yugoslavia', \n          'Scotland', 'Greece', 'Ireland', 'Hungary', 'Holand-Netherlands'] ","b5171da6":"# Now, create a dictionary to map each country to a Corresponding continent. \ncontinents = {country: 'Asia' for country in asia}\ncontinents.update({country: 'America' for country in america})\ncontinents.update({country: 'Europe' for country in europe})","cdff9f6e":"# Then use Pandas map function to map continents to countries  \ndf_no_outliers['Continent'] = df_no_outliers['Country'].map(continents)","b800887a":"df_no_outliers['Continent'].unique()","eac63f3c":"def Occupation_VS_Income(continent):\n    choice = df_no_outliers[df_no_outliers['Continent']==continent] \n    countries = list(choice['Country'].unique())\n\n    for country in countries:\n        pd.crosstab(choice[choice['Country']==country].Occupation, choice[choice['Country']==country].Income).plot(kind='bar', \n                                                                                                                       title='Income VS Occupation in {}'.format(country))","609008c4":"Occupation_VS_Income('Asia')","656afe29":"Occupation_VS_Income('America')","f3f39056":"Occupation_VS_Income('Europe')","f3049df8":"def Workclass_VS_Income(continent):\n    choice = df_no_outliers[df_no_outliers['Continent']==continent] \n    countries = list(choice['Country'].unique())\n\n    for country in countries:\n        pd.crosstab(choice[choice['Country']==country].Workclass, choice[choice['Country']==country].Income).plot(kind='bar', \n                                                                                                                       title='Income VS Workclass in {}'.format(country))","e2ff3b22":"Workclass_VS_Income('Asia')","3669b8b2":"Workclass_VS_Income('America')","fccf96aa":"Workclass_VS_Income('Europe')","ffc0e719":"def MaritalStatus_VS_Income(continent):\n    choice = df_no_outliers[df_no_outliers['Continent']==continent] \n    countries = list(choice['Country'].unique())\n\n    for country in countries:\n        pd.crosstab(choice[choice['Country']==country]['Martial Status'], choice[choice['Country']==country].Income).plot(kind='bar', \n                                                                                                                       title='Income VS Workclass in {}'.format(country))","fc5fd960":"MaritalStatus_VS_Income('Asia')","cf6084a0":"# reset_index(): to convert to aggregation result to a pandas dataframe.\nagg_df = df_no_outliers.groupby(['Continent','Country', 'Martial Status'])['Capital-Gain'].mean().reset_index()","0e7dfd5e":"agg_df['Mean_Capital_Gain'] = agg_df['Capital-Gain']\nagg_df.drop('Capital-Gain', axis=1, inplace=True)","c8669d54":"agg_df.head()","99c0ead8":"import seaborn as sns","2f61676e":"def Mean_TotCapital_VS_Marital_Status(continent):\n    choice = agg_df[agg_df['Continent']==continent] \n    countries = list(choice['Country'].unique())\n\n    for country in countries:\n        df_c = choice[choice['Country']==country]\n        ax = sns.catplot(x='Martial Status', y='Mean_Capital_Gain', \n                         kind='bar', data=df_c)\n\n        ax.fig.suptitle('Country: {}'.format(country))\n        ax.fig.autofmt_xdate()","ce21a7a9":"Mean_TotCapital_VS_Marital_Status('Asia')","c4fc4dfd":"Mean_TotCapital_VS_Marital_Status('America')","4c620a1a":"Mean_TotCapital_VS_Marital_Status('Europe')","fe1cb512":"edu = df_no_outliers.Education.unique()\neduNum = df_no_outliers['Education-Num'].unique()\nprint('Education: \\nTotal category:{}\\nValues: {}\\n'.format(len(edu),list(edu)))\nprint('Education Num: \\nTotal Education-Num:{}\\nValues: {}'.format(len(eduNum),\n                                                                  list(eduNum)))","2e0c8214":"ax = sns.catplot(x='Education', y='Education-Num', kind='bar', data=df_no_outliers)\nax.fig.suptitle('Numerical Representation of Educations')\nax.fig.autofmt_xdate()","8fb019dc":"# Finally remove the Education column  \ndf_no_outliers.drop('Education', axis=1, inplace=True)","3bf11595":"df_no_outliers['Capital-State'] = df_no_outliers['Capital-Gain'] - df_no_outliers['Capital-Loss']","54e7bc4f":"# Then remove Capital-Gain and Capital-Loss. \ndf_no_outliers.drop(['Capital-Gain', 'Capital-Loss'], axis=1, inplace=True)","d74242a8":"'''\nLet not forget to drop the 'Continent' column we added for \nvisualization purpose. \n'''\ndf_no_outliers.drop('Continent', axis=1, inplace=True)","11ebbe12":"df_no_outliers.head(3)","92bf49a0":"# AgeState based on Age\ndf_no_outliers['AgeState'] = np.where(df_no_outliers['Age'] >= 18, 'Adult', 'Child')","255f6d87":"# AgeState Counts  \ndf_no_outliers['AgeState'].value_counts()","0905aab4":"sns.countplot(x='AgeState', data=df_no_outliers)","0d57dfa5":"df_no_outliers.drop('fnlwgt', axis=1, inplace=True)","5117ea78":"df_no_outliers.head()","9947f928":"# Information about our data\ndf_no_outliers.info()","ce69af8f":"# Columns: Workclass, Martial Status Occupation, Relationship, Race, Sex, Country, AgeState\ndf_no_outliers = pd.get_dummies(df_no_outliers, columns=['Workclass', 'Martial Status', 'Occupation', \n                                 'Relationship', 'Race', 'Sex', 'Country', 'AgeState'])","e8ebd19d":"df_no_outliers['Income'].unique()","daaac332":"'''\n1: For those who make more than 50K \n0: For those who don't\n'''\ndf_no_outliers['Income'] = np.where(df_no_outliers['Income'] =='>50K', 1, 0)","78feb063":"# Reorder columns : In order to have 'Income' as last feature.\ncolumns = [column for column in df_no_outliers.columns if column != 'Income']\ncolumns = columns + ['Income'] \ndf = df_no_outliers[columns]","11071179":"# Information about our data\ndf.info()","7dc62346":"y = df.Income.ravel()\nX = df.drop('Income', axis=1).as_matrix().astype('float')","9cd02b5a":"print('X shape: {} | y shape: {}'.format(X.shape, y.shape))","1ba27cb4":"from sklearn.model_selection import train_test_split","831dbb92":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","5e4368ad":"print('X train shape: {} | y shape: {}'.format(X_train.shape, y_train.shape))\nprint('X test shape: {} | y shape: {}'.format(X_test.shape, y_test.shape))","00887c3b":"from sklearn.dummy import DummyClassifier","fe8ccb02":"dummy_clf = DummyClassifier(strategy='most_frequent', random_state=0)","8c6131d5":"# Train the model \ndummy_clf.fit(X_train, y_train)","4317aaf8":"print('Score of baseline model : {0:.2f}'.format(dummy_clf.score(X_test, y_test)))","dcb7e9fe":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV","fd8dcecb":"lr_clf = LogisticRegression(random_state=0)\nparameters = {'C':[1.0, 10.0, 50.0, 100.0, 1000.0], 'penalty' : ['l1','l2']}\nlr_clf = GridSearchCV(lr_clf, param_grid=parameters, cv=3)","b290fa14":"lr_clf.fit(X_train, y_train)","5117950e":"lr_clf.best_params_","e218d975":"print('Best score : {0:.2f}'.format(lr_clf.best_score_))","1487ff1f":"print('Score for logistic regression - on test : {0:.2f}'.format(lr_clf.score(X_test, y_test)))","f1ed261f":"We need to deal with the problem of distribution for all our numerical data values in the feature engineering part. ","cda90ae6":"### B.4.3- For Europe","b698b1cf":"##### A.1- Education and Education-Num  ","36104f99":"## B.4- Mean Capital Gain VS Martial Status for each continent","8f58f431":"We have 6 numerical columns (Age to Hours-per-week). To the left, we have many statistical information such as :  \n* **count**: for the total number of observation for each column.   \n* mean: the mean value of each column   \n* std: the standard deviation    \n* 25%, 50% and 75% are quantiles. \n\nWith the quantiles, min and max, the dataset can be splitted into 4 buckets:  \n* Bucket 1: below 25% (e.g) for **Age** column, 25% of people are under **28 years old**.\n* Bucket 2: between 25% and 50% (e.g), 25% of them (50%-25%) are between **28 and 37 years old**.  \n* Bucket 3: between 50% and 75% (e.g), 25% of them are between **37 and 48 years old** .  \n* Bucket 4: between above 75% (e.g), 25% of them are over **48 years old**.  \n\n**Then all the values beyond 1.5xIQR are considered as outliers. ** \nIQR = Inter Quartile Range = 75th - 25th.   \n\nThis images gives a better understanding of a boxplot.   \n![](https:\/\/www.researchgate.net\/publication\/318986284\/figure\/fig1\/AS:525404105646080@1502277508250\/Boxplot-with-outliers-The-upper-and-lower-fences-represent-values-more-and-less-than.png)\n\nThen we will create a helper function that will remove all the outliers from our dataset. But, before that, let have a look at the boxplot.   ","75a4a72a":"\n#### B- Categorical Data","64506205":"There is no special extreme value here. ","8d03a77d":"We can notice that all our data have been acquired from the UCI website. Here we have :  \n* **adult.names**: which corresponds to the different column names   \n* **adult.data**: corresponds to all the observations in the training data.  \n* **data.test**: corresponds to all the observation in the test data  \n","35cb14fe":"### B.3.1- For Asia","f1bdd431":"#### A- Numerical Data   \nFor this part, we will be performing centrality measure (mean, median) and dispersion measures (range, percentiles, variance, standard deviation).  \nAll those information can be found with pandas **describe()** function.  ","00d6ca0c":"We know all the columns with missing values, and their type. We also have an idea of the unique values of each of those columns, now, we can perform the missing values replacement process.   \n\nTo do so, we will create a helper function that will perform this task for all the columns using python **statistics** built-in function.","7e1c20b4":"We have many countries from different continent. For better visualization, it might be interesting to create a new column **Continent** in order to easily group information per continent and the corresponding countries. ","a53e005d":"### B.1.1- For Asia","a800b27d":"Here we are going to acquire the training and the test datasets. \nThe corresponding column names have been specified in the previous **column_names** variable. Then, we use the regular expression **' \\*, \\*'** to trim all the whitespaces we can encounter in our datasets. As all the missing values have been specificied by **?**, so, **na_values** is used to take them into consideration during the data loading. Finally we specify **engine='python'** to avoid the warning that comes after using regular expression syntax.  ","fc2861e1":"##### A.2- Bivariate analysis  \nWe will try to determine the correlation between some numerical data.","59842132":"##### A.1- Univariate Analysis ","1d2727e3":"Here, we have the continents corresponding to all the existing contries in our dataset.","34f7e8fc":"## 2- Data Acquisition  \nWe are going to acquire our dataset into **text** format, after downloading it from the **[UCI Machine Learning](https:\/\/archive.ics.uci.edu\/ml\/datasets\/adult)** website. Here are the following libraries that we will be using to acquire the dataset and perform all the preprocessing and analysis.  ","6de94081":"To accomplish this task; I will create a new dataframe containing the grouping result of Continent, Contient, Marital Status and the **mean value of Capital Gain**","43a7f516":"We can notice that we have 24720 adults who make less than 50K dollars and only 7841 of them make more than 50K dollars. So,only 24% of adult make more than 50K dollars.","f34dfbd4":"The Occupation column has 15 unique values, including **nan** ","c61e9f82":"Now, we are going to create a helper function in order to remove all the outliers, based in our previous univariate analysis.  ","51212001":"From the correlation matrix, we can see that the level of relationship is very low between the numerical features.  ","e16f85d8":"From the previous plot, we can see that \n* Bachelor <==> 13  \n* HS-grad <==> 9\n* 7th-8th <==> 4   \n* 9th <==> 5    \n* Preschool <==> 1 \n* etc.  \nBased on those information, we will need only one column to represent the **level of education**, and in our case,   \nwe will choose **Education-Num** (remove **Education** column) which corresponds to the numerical representation.  ","374c6588":"## 7- Next Step  \n* Feature Normalization  and Standardization  \n* Feature selection\n* Use different models: Ensemble Technics \n\n**If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated -  That will keep me motivated to update it on a regular basis** :-)","4b18ffc6":"## 3- Data Munging \nIn this step, we will perform two main tasks.  \n* **Dealing with missing values**    \nDuring data collection, it is very common to face missing data problem, that can occur for many reasons (confidentiality, error,etc.). So, it is very important to understand those problems, in order to fill them using appropriate techniques before applying any Machine Learning algorithm.    \n\n\n* **Dealing with outliers**     *\nOutliers are those values that are far away from the normal values that can be observed in the whole data. They can introduce high bias in our final model performance, and can even lead us to taking wrong conclusion during the analysis step.  \n\n#### A- Treating missing values   \nWe will use pandas **isnull()** function to look at all the missing values for each column.  ","1c44e0ee":"First thing first! \nLet's take a look at the number of people who make more that 50K and those who don't","fb4f3938":"There is no interesting pattern. ","9c16bd89":"##### Logistic Regression ","6b5459e8":"We have a positive relationship between the number of year of education and the Capital Gain. The more educated you are, your are likely to have more capital. ","e1f1f21d":"The Country column has 42 unique values, including **nan** ","f0f42fcb":"**fnlwgt** column is not an important feature. ","9a1d83f1":"### B- Dealing with outliers  \nTo be able to identify outliers in our dataset, we will use **seaborn** **boxplot** to all our numerical columns, and show the final result with **matplotlib**'s **show()** function.    \nWe the help of the **Output n\u00b06 (i.e print(df.dtypes))**, we can see all our numrical columns; But a better way to look at them is to apply pandas **describe** function, which gives more statistical information about all the numerical columns.  \n\nIn this part, we are going to use the copy of our training dataset for outliers analysis, then create a helper function that will finally be applied to the original training data for outliers removal.","63d8fc58":"##### Baseline Model","54fb779c":"#### 6.2- Models & Evaluation   \nBefore building any machine learning model. It is important to build a baseline model first, in order judge the performance of the upcoming models.  ","503d59d1":"##### A.3- Age State (Adult or Child)   \nA person older than 18 is an adult. Otherwise he\/she is a child.  ","cf98e212":"Let calculate 0-100th percentile to find a correct percentile value for removal of outliers","9cd74ac8":"We can see that all the values to the right are equal to zero, which means that we have no missing values in our dataset.    ","cf21ef12":"# Adult Income : Exploratory Analysis And Precition   \n\nThis notebook has been created to help you go through the steps of a Machine Learning project Life-Cicle, from Business Understanding to presenting the final result to the Business.  \n\n## 1. Business Understanding \n## 2. Data aquisition  \n          Automatique Data aquisition  \n          Convert data into a Pandas Data Frame\n          \n## 3- Data Munging  \n          Treating missing values\n          Working with outliers\n          \n## 4- Exploratory Data Analysis \n          Univariate Analysis      \n          Bivariate analysis           \n          \n## 5- Feature Engineering \n          Derived Features\n          Categorical Feature encoding\n          \n## 6- Preparation, Models and Evaluation    \n          Preparation\n          Models and Evaluation  \n          \n## 7- Next Step  \n\n","b08c363b":"### B.2.2- For America","9ed978ed":"After removing the outliers from out data, still 99.49% of the dataset remain present. ","df43201e":"We could see from the boxplot of Age that there is no extreme value. Then after checking with percentile values, we have a confirmation of our remark. ","920f2c70":"We can remove all the corresponding rows for **Country == South** because, it corresponds to only 0.244% of the original dataset. ","13358eef":"## 4- Exploratory Data Analysis   ","f0ec069b":"We are going to perform the following preprocessing:  \n* Outlying-US(Guam-USVI-etc) ==> Outlying-US   \n* Trinadad&Tobago ==> Trinadad-Tobago  \n* Hong ==> Hong-Kong","5484361a":"## 5- Feature Engineering   \nThis is one of the most crucial aspect for a Data Science project. It is a process of transforming the raw data to better representative \nfeatures in order to create better predictive models. \n\n#### A- Derived Features   \nSometimes, it is important to perform some transformations on the features\/columns in order to reduce the number of original data columns. \nLet's start looking at our columns.","d5db1c99":"We need to transform the **Income** column value for test data, in order to remove the **\".\"** at the end  ","6de09ad2":"## 1- Business Understanding  \nOur data contains an individual's annual income results based on various factors (Education level, Occupation,Gender, Age, etc.). \nGiven a new individual, our goal is to predict if that person makes more or less than 50K. ","b906c1d5":"### B.2.3- For Europe","5905411f":"People without any capital Gain lose a lot of money, which is obvious, because without any capital Gain, you would need to borrow with interest, and then keep **\"surviving\".** ","e0ded9af":"### B.1.3- For Europe","9baa9063":"We can not identify any interesting pattern from this visualization. ","d2b543fa":"### B.1.2- For America","e6ccd7b8":"Workclass has 9 unique values including **nan** (missing value)","eb440c88":"We can clearly see that the changes have been made. ","ed65fc3e":"## 6- Preparation, Models and Evaluation    \n#### 6.1- Data Preparation   \nWe need to split our dataset for training and testing data.  \n80% of the data will be used for training and 20% for testing.","3ee3a4f6":"##### A.2- Capital-Loss and Capital-Gain  \nFrom those two features, we can create a new column called **Capital-State** that will be the difference between Capital-Gain and Capital-Loss.  \nThen we will remove those two features.  ","cb562158":"To the left, we have the name of the features and the number of missing values to the right. We can see that:   \n* **Workclass** has 1836 missing values   \n* **Occupation** has 1843 missing values  \n* **Country** has 583 missing values   \n\nTo deal with all the missing data, we couuld think of removing all the records (rows\/observations) with those missing values. But, this technique could not be a better choice for our case, because we could lose much more data. To do so, we will use the following technique :  \n* Replace missing data of categorical columns data with the mode value (most occuring category) of that column.   \n* Replace missing numerical columns data with the median value of that column. Here we could use the mean instead of median, but the mean is very prompt to outliers (extreme values).     \n\nTo be able to identify which columns has which type, we can use pandas dtype() function.   \n\n","2ed3044b":"## B.3- Income VS Marital Status for countries in each continent  ","6be30578":"We can see that The **Education-Num** seems to be the numerical representation of **Education**, and also the same Total number (16). To do so, we will need only one of them, not both columns.  \nLet's check some observations (rows) to verify our hypothesis if there is a corrrespondance between **Education-Num** and **Education**.   \nThen we can simply visualize the two columns in order to check the correspondance between them.  ","19e69d66":"No special extreme value here as we could notice for Capital-Gain. ","8e7de8df":"### B.4.1- For Asia","dfc4192b":"Here, we have a positive skewed distribution for Age feature. ","ec2f19f4":"#### B- Categorical Feature encoding    \nA machine learning model only works with numerical features. To do so, we need to encode all our categorical features. Those features are represented by **object**  with the help of the previous **info** command.    \nWe are going to perform the **One Hot Ending** method on all the categorical features by using Pandas **get_dummies()** function.  \nWe are not going to take in consideration **Income** column, because it is the column we try to predict.  ","dd9b1e9b":"## B.2- Income VS Workclass for countries in each continent  ","dd11490a":"### B.2.1- For Asia","7d3a7522":"Going deeper with the percentile values, we can have more information. So, here is a function that will give us the percentile values for each values from 99 to 100 percentile. ","901873cc":"### B.4.2- For America","ea74a7cd":"## B.1- Income VS Occupation for countries in each continent  \nI created a helper fonction in order to preprocess for each country in one shot. ","b190ad35":"There is country name called **South** which is definitly an error. It could be considered as **continent**, then we could associate in with the corresponding continent. But, here is the problem: we have both **South-America**, **South-Asia** that could be possible values. In order to avoid including more errors in our data, it might be better to remove the corresponding observations in case that action does not lead to loosing too much data.  ","0e748344":"To the left, we have the columns name, and their corresponding types to the right. So, we can see that the columns with missing values (discussed previously) are all categorical data (object).    \nThen, we can have a look at all the distincs (unique) values in each columns with pandas **unique()** function.  ","1f5fa50e":"There is no anomalies with Education number. ","3e7938ac":"There are many explorations we can do in order to have a better understanding of the data.   \nHere are some possibilities we could have:  \n* B.1- Income VS Occupation for countries in each continent\n* B.2- Income VS Workclass for countries in each continent\n* B.3- Income VS Marital Status for countries in each continent\n* B.4- Mean Capital Gain VS Martial Status for each continent\n","2ceba618":"From this result, we can see that our features are in different scales, so that information will be useful for feature engineering step. For simple visualization purpose, we can plot the probability density of all those features. ","08d900a3":"### Convert Data into a Pandas Data Frame  "}}