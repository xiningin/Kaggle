{"cell_type":{"990eddf3":"code","5b3b6648":"code","2e5ae348":"code","0de5b185":"code","a63d0dfb":"code","3073e407":"code","34221522":"code","e897031e":"code","e570a715":"code","687aa6bf":"code","22fabeda":"code","ab7e2b8c":"code","f8e01261":"code","1ac5eddc":"code","d8537a91":"code","5b26b6ce":"code","be04dbaa":"code","ccc60bb5":"code","6321a353":"code","9eda6d37":"code","82d9c297":"code","837c4ca8":"code","0642a0b5":"code","3515ab27":"code","144ab575":"markdown","fe5f3040":"markdown","6cc19a08":"markdown","c92c9d00":"markdown","7a3fcaa8":"markdown","734b8e8d":"markdown","929968f9":"markdown","1e2579b5":"markdown","0ddd0389":"markdown","4c017829":"markdown","790e6ed7":"markdown","41b648f3":"markdown","05bbc3d4":"markdown","91f0c1df":"markdown","00414879":"markdown","1bae807d":"markdown","015e4f02":"markdown","1dc521a5":"markdown","8a742873":"markdown","8feb31dc":"markdown","3d310936":"markdown"},"source":{"990eddf3":"import pandas as pd\nimport numpy as np\nimport matplotlib \nfrom matplotlib import pyplot as plt\nimport sklearn\nimport seaborn as sns\nimport scipy as sp\nfrom scipy import stats\n\nimport os\nprint(os.listdir(\"..\/input\"))","5b3b6648":"\n\ntrain = pd.read_csv('..\/input\/launchds-classification\/bank-train.csv')\ntest = pd.read_csv('..\/input\/launchds-classification\/bank-test.csv')\n\ntrain.head() # lots of categorical variables\ntrain.describe() # pdays, previous, y is very skewed","2e5ae348":"fig, axes = plt.subplots(1, 2, figsize=(10, 2), sharey=False, dpi=100)\nsns.distplot(train['pdays'] , color=\"dodgerblue\", ax=axes[0], axlabel='Pdays')\nsns.distplot(train['previous'] , color=\"deeppink\", ax=axes[1], axlabel='Previous')\n","0de5b185":"print(train.isnull().apply(sum), '\\n') # no null values\nprint(train.groupby('y').count()['id'], '\\n') # 29245=0, 3705=1\nprint('There are',len(test),'testing observations') # 8238 testing observations","a63d0dfb":"train.head()\n'''categorical: \njob: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management',\n       'retired', 'self-employed', 'services', 'student', 'technician',\n       'unemployed', 'unknown'\nmarital: 'divorced', 'married', 'single', 'unknown'\neducation: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate',\n       'professional.course', 'university.degree', 'unknown'\ndefault: 'no', 'unknown', 'yes'\nhousing: 'no', 'unknown', 'yes'\nloan: 'no', 'unknown', 'yes'\ncontact: 'cellular', 'telephone'\nmonth: 'apr', 'aug', 'dec', 'jul', 'jun', 'mar', 'may', 'nov', 'oct',\n       'sep'\nday_of_week: 'fri', 'mon', 'thu', 'tue', 'wed'\npoutcome: 'failure', 'nonexistent', 'success'\n''' \n\ndef yes_no_uk(x):\n  '''used to transform default, housing, and loan'''\n  if x=='yes':\n    return(1)\n  elif x=='no':\n    return(0)\n  elif x=='unknown':\n    return(2)\n  \ndef poutcome(x):\n  '''used to transform poutcome'''\n  if x=='success':\n    return(1)\n  elif x=='failure':\n    return(0)\n  elif x=='nonexistent':\n    return(2)\n\ndef day_of_week(x):\n  '''used to transform day_of_week'''\n  if x=='mon':\n    return(1)\n  elif x=='tue':\n    return(2)\n  elif x=='wed':\n    return(3)\n  elif x=='thu':\n    return(4)\n  elif x=='fri':\n    return(5)\n  \ndef month(x):\n  '''used to transform month'''\n  if x=='jan':\n    return(1)\n  elif x=='feb':\n    return(2)\n  elif x=='mar':\n    return(3)\n  elif x=='apr':\n    return(4)\n  elif x=='may':\n    return(5)\n  elif x=='jun':\n    return(6)\n  elif x=='jul':\n    return(7)\n  elif x=='aug':\n    return(8)\n  elif x=='sep':\n    return(9)\n  elif x=='oct':\n    return(10)\n  elif x=='nov':\n    return(11)\n  elif x=='dec':\n    return(12)\n","3073e407":"# transforming training ordinal variables\ndefault_labels = train['default'].apply(yes_no_uk)\nhousing_labels = train['housing'].apply(yes_no_uk)\nloan_labels = train['loan'].apply(yes_no_uk)\nmonth_labels = train['month'].apply(month)\nday_labels = train['day_of_week'].apply(day_of_week)\npoutcome_labels = train['poutcome'].apply(poutcome)\n\n# transforming test data rdinal variables\ndefault_labels2 = test['default'].apply(yes_no_uk)\nhousing_labels2 = test['housing'].apply(yes_no_uk)\nloan_labels2 = test['loan'].apply(yes_no_uk)\nmonth_labels2 = test['month'].apply(month)\nday_labels2 = test['day_of_week'].apply(day_of_week)\npoutcome_labels2 = test['poutcome'].apply(poutcome)","34221522":"# transforming categorical variables\nmarital_labels = pd.get_dummies(train['marital'])\njob_labels = pd.get_dummies(train['job'])\neducation_labels = pd.get_dummies(train['education'])\ncontact_labels = pd.get_dummies(train['contact'])\n\n# making sure the unknowns have a specific label\nmarital_labels.columns = ['divorced', 'married', 'single', 'unknown.marital']\njob_labels.columns = ['admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management',\n       'retired', 'self-employed', 'services', 'student', 'technician',\n       'unemployed', 'unknown.job']\neducation_labels.columns = ['basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate',\n       'professional.course', 'university.degree', 'unknown.education']\n\n\n\n# transforming test data\nmarital_labels2 = pd.get_dummies(test['marital'])\njob_labels2 = pd.get_dummies(test['job'])\neducation_labels2 = pd.get_dummies(test['education'])\ncontact_labels2 = pd.get_dummies(test['contact'])\n\n# making sure the unknowns have a specific label\nmarital_labels2.columns = ['divorced', 'married', 'single', 'unknown.marital']\njob_labels2.columns = ['admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management',\n       'retired', 'self-employed', 'services', 'student', 'technician',\n       'unemployed', 'unknown.job']\neducation_labels2.columns = ['basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate',\n       'professional.course', 'university.degree', 'unknown.education']","e897031e":"train_y = train['y']\ntrain2 = train[['id', 'age', 'duration', 'campaign','pdays', 'previous', 'emp.var.rate', 'cons.price.idx',\n       'cons.conf.idx', 'euribor3m', 'nr.employed']]\n\ntrain2['default'] = default_labels\ntrain2['housing'] = housing_labels\ntrain2['loan'] = loan_labels\ntrain2['month'] = month_labels\ntrain2['day'] = day_labels\ntrain2['poutcome'] = poutcome_labels\n\ntrain2 = pd.concat([train2, marital_labels], axis=1)\ntrain2 = pd.concat([train2, job_labels], axis=1)\ntrain2 = pd.concat([train2, education_labels], axis=1)\ntrain2 = pd.concat([train2, contact_labels], axis=1)\n\ntrain2['y'] = train_y","e570a715":"test2 = test[['id', 'age', 'duration', 'campaign','pdays', 'previous', 'emp.var.rate', 'cons.price.idx',\n       'cons.conf.idx', 'euribor3m', 'nr.employed']]\n\ntest2['default'] = default_labels2\ntest2['housing'] = housing_labels2\ntest2['loan'] = loan_labels2\ntest2['month'] = month_labels2\ntest2['day'] = day_labels2\ntest2['poutcome'] = poutcome_labels2\n\ntest2 = pd.concat([test2, marital_labels2], axis=1)\ntest2 = pd.concat([test2, job_labels2], axis=1)\ntest2 = pd.concat([test2, education_labels2], axis=1)\ntest2 = pd.concat([test2, contact_labels2], axis=1)","687aa6bf":"corr = pd.DataFrame()\nfor a in list('y'):\n    for b in list(train2.columns.values):\n        corr.loc[b, a] = train2.corr().loc[a, b]\n        \nsns.heatmap(corr)\nprint(corr['y'].sort_values())\n\n# variables with abs(corr)<0.01\n'''\nbasic.4y              -0.009658\nhigh.school           -0.009604\nhousemaid             -0.008696\nself-employed         -0.008180\nunknown.job           -0.003743\ntechnician            -0.001249\nmanagement            -0.000280\nloan                   0.000409\nprofessional.course    0.000415\nunknown.marital        0.002550\nilliterate             0.007441\nday                    0.008814\n'''\ntrain3 = train2[['age', 'duration', 'campaign', 'pdays', 'previous',\n       'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m',\n       'nr.employed', 'default', 'housing', 'month', 'poutcome',\n       'divorced', 'married', 'single', 'admin.',\n       'blue-collar', 'entrepreneur', 'retired',\n       'services', 'student', 'unemployed',\n       'basic.6y', 'basic.9y', 'university.degree',\n       'unknown.education', 'cellular', 'telephone']]\ntest3 = test2[['age', 'duration', 'campaign', 'pdays', 'previous',\n       'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m',\n       'nr.employed', 'default', 'housing', 'month', 'poutcome',\n       'divorced', 'married', 'single', 'admin.',\n       'blue-collar', 'entrepreneur', 'retired',\n       'services', 'student', 'unemployed',\n       'basic.6y', 'basic.9y', 'university.degree',\n       'unknown.education', 'cellular', 'telephone']]\n\n# variables with abs(corr)<0.05\n\"\"\"\nbasic.9y              -0.043711\nmarried               -0.042574\nservices              -0.031471\nbasic.6y              -0.024711\nentrepreneur          -0.016653\ndivorced              -0.010230\nbasic.4y              -0.009658\nhigh.school           -0.009604\nhousemaid             -0.008696\nself-employed         -0.008180\nunknown.job           -0.003743\ntechnician            -0.001249\nmanagement            -0.000280\nloan                   0.000409\nprofessional.course    0.000415\nunknown.marital        0.002550\nilliterate             0.007441\nday                    0.008814\nhousing                0.011729\nunemployed             0.014542\nunknown.education      0.016053\nage                    0.027631\nadmin.                 0.030412\nmonth                  0.036602\n\"\"\"\ntrain4 = train2[['duration', 'campaign', 'pdays', 'previous',\n       'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m',\n       'nr.employed', 'default', 'poutcome', 'single', 'blue-collar','retired',\n       'student', 'university.degree', 'cellular', 'telephone']]\n\ntest4 = test2[['duration', 'campaign', 'pdays', 'previous',\n       'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m',\n       'nr.employed', 'default', 'poutcome', 'single', 'blue-collar','retired',\n       'student', 'university.degree', 'cellular', 'telephone']]","22fabeda":"train2_lite = train2.iloc[:-2000, :-1]\ntrainy_lite = train2.iloc[:-2000, -1]\ntrain2_test = train2.iloc[-2000:, :-1]\ntrainy_test = train2.iloc[-2000:, -1]\n\ntrain3_lite = train3.iloc[:-2000, :-1]\ntrain3_test = train3.iloc[-2000:, :-1]\n\ntrain4_lite = train4.iloc[:-2000, :-1]\ntrain4_test = train4.iloc[-2000:, :-1]","ab7e2b8c":"from sklearn.linear_model import LogisticRegression\nlogis = LogisticRegression(solver='liblinear',fit_intercept=True)\n\nlogis_test = logis.fit(train2_lite, trainy_lite)\npreds = logis_test.predict(train2_test)\nprint(sklearn.metrics.f1_score(trainy_test, preds))\nprint(sklearn.metrics.accuracy_score(trainy_test, preds))\n\nlog_test2 = logis.fit(train3_lite, trainy_lite)\npreds2 = log_test2.predict(train3_test)\nprint(sklearn.metrics.f1_score(trainy_test, preds2))\nprint(sklearn.metrics.accuracy_score(trainy_test, preds2))\n\nlog_test3 = logis.fit(train4_lite, trainy_lite)\npreds3 = log_test3.predict(train4_test)\nprint(sklearn.metrics.f1_score(trainy_test, preds3))\nprint(sklearn.metrics.accuracy_score(trainy_test, preds3))\n\n","f8e01261":"from sklearn.feature_selection import f_regression\n(F_vals, p_vals) = f_regression(train2_lite, trainy_lite)\n\ncols = list(train2_lite.columns[p_vals<0.01])\ntrainF = train2[cols]\n\ntrainF_lite = trainF.iloc[:-2000, :-1]\ntrainF_test = trainF.iloc[-2000:, :-1]","1ac5eddc":"log_testF = logis.fit(trainF_lite, trainy_lite)\npredsF = log_testF.predict(trainF_test)\nprint(sklearn.metrics.f1_score(trainy_test, predsF))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsF))\n# no better than original","d8537a91":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\n\nlr = LinearRegression()\nlinreg = lr.fit(train2_lite, trainy_lite)\npredsLR = linreg.predict(train2_test)>0.32\nprint(sklearn.metrics.f1_score(trainy_test, predsLR))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsLR))\n\nrr = Ridge(alpha=0.000001, normalize=True)\nridge = rr.fit(train2_lite, trainy_lite)\npredsR = ridge.predict(train2_test)>0.32\nprint(sklearn.metrics.f1_score(trainy_test, predsR))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsR))\n\nlasso = Lasso(alpha=0.00000000001, normalize=True)\nlass = lasso.fit(train2_lite, trainy_lite)\npredsLass = lass.predict(train2_test)>0.32\nprint(sklearn.metrics.f1_score(trainy_test, predsLass))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsLass))","5b26b6ce":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\ntree = DecisionTreeClassifier()\ntreeD = tree.fit(train2_lite, trainy_lite)\npredsTD = treeD.predict(train2_test)\nprint(sklearn.metrics.f1_score(trainy_test, predsTD))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsTD))\n\nforest = RandomForestClassifier(criterion = 'entropy')\nforestR = forest.fit(train2_lite, trainy_lite)\npredsF = forestR.predict(train2_test)\nprint(sklearn.metrics.f1_score(trainy_test, predsF))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsF))","be04dbaa":"print(pd.DataFrame({'Gain': treeD.feature_importances_}, index = train2_lite.columns).sort_values('Gain', ascending = False))\nprint(pd.DataFrame({'Importance': forestR.feature_importances_}, index = train2_lite.columns).sort_values('Importance', ascending = False))","ccc60bb5":"trainT = train2[['duration', 'id', 'age', 'euribor3m', 'nr.employed', \n                 'pdays', 'campaign', 'day', 'cons.conf.idx', 'housing',\n                'emp.var.rate']]\ntrainT_lite = trainT.iloc[:-2000,:]\ntrainT_test = trainT.iloc[-2000:, :]\n\ntestT = test2[['duration', 'id', 'age', 'euribor3m', 'nr.employed', \n                 'pdays', 'campaign', 'day', 'cons.conf.idx', 'housing',\n                'emp.var.rate']]","6321a353":"treeD2 = tree.fit(trainT_lite, trainy_lite)\npredsTD2 = treeD2.predict(trainT_test)\nprint(sklearn.metrics.f1_score(trainy_test, predsTD2))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsTD2))\n\n\nforest2 = RandomForestClassifier(criterion = 'gini')\nforestR2 = forest2.fit(trainT_lite, trainy_lite)\npredsF2 = forestR2.predict(trainT_test)\nprint(sklearn.metrics.f1_score(trainy_test, predsF2))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsF2))","9eda6d37":"linregT = lr.fit(trainT_lite, trainy_lite)\npredsLRT = linregT.predict(trainT_test)>0.3\nprint(sklearn.metrics.f1_score(trainy_test, predsLRT))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsLRT))\n\n\nrr = Ridge(alpha=0.000001, normalize=True)\nridgeT = rr.fit(trainT_lite, trainy_lite)\npredsRT = ridgeT.predict(trainT_test)>0.3\nprint(sklearn.metrics.f1_score(trainy_test, predsRT))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsRT))\n\n\nlassoT = Lasso(alpha=0.000001, normalize=True)\nlassT = lassoT.fit(trainT_lite, trainy_lite)\npredsLassT = lassT.predict(trainT_test)>0.3\nprint(sklearn.metrics.f1_score(trainy_test, predsLassT))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsLassT))\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\nmodel = gnb.fit(trainT_lite, trainy_lite)\npredsG = model.predict(trainT_test)\nprint(sklearn.metrics.f1_score(trainy_test, predsG))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsG))\n\n","82d9c297":"logis_test2 = logis.fit(train2_lite, trainy_lite)\npredsL = logis_test2.predict(train2_test)\nprint(sklearn.metrics.f1_score(trainy_test, predsL))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsL))","837c4ca8":"from sklearn.svm import SVC\nfrom sklearn.linear_model import  LogisticRegression\nclassifier = SVC(kernel=\"linear\")\n\nsvm = classifier.fit(trainT_lite, trainy_lite)\npredsSVM = svm.predict(trainT_test)\nprint(sklearn.metrics.f1_score(trainy_test, predsSVM))\nprint(sklearn.metrics.accuracy_score(trainy_test, predsSVM))","0642a0b5":"from sklearn.neighbors import KNeighborsClassifier \nknn = KNeighborsClassifier()\nknn.fit(trainT_lite, trainy_lite)\npredKNN = knn.predict(trainT_test)\nprint(sklearn.metrics.f1_score(trainy_test, predKNN))\nprint(sklearn.metrics.accuracy_score(trainy_test, predKNN))\n","3515ab27":"# 1. Linear Regression\nlr = LinearRegression()\nlinregT = lr.fit(trainT, train.iloc[:, -1]) # best so far\nprediction1 = linregT.predict(testT)>0.3\n\ndef TF(x):\n  if x==True:\n    return(1)\n  elif x==False:\n    return(0)\n\nsubmission = pd.concat([test.id, pd.Series(prediction1)], axis = 1)\nsubmission.columns = ['id', 'Predicted']\nsubmission['Predicted'] = submission['Predicted'].apply(TF)\nsubmission.to_csv('submission.csv', index=False)\n\n# 2. SVM\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import  LogisticRegression\n#classifier = SVC(kernel=\"linear\")\n#svm = classifier.fit(trainT, train.iloc[:, -1])\npredictions3 = svm.predict(testT)\nsubmission = pd.concat([test.id, pd.Series(predictions3)], axis = 1)\nsubmission.columns = ['id', 'Predicted']\nsubmission.to_csv('submission.csv', index=False)\n\n\n# 3. Random Forest\nforest2 = RandomForestClassifier(criterion = 'gini')\nforestR2 = forest2.fit(trainT, train.iloc[:, -1])\npredsF2 = forestR2.predict(testT)\nsubmission = pd.concat([test.id, pd.Series(predsF2)], axis = 1)\nsubmission.columns = ['id', 'Predicted']\nsubmission.to_csv('submission.csv', index=False)","144ab575":"I then ran a Linear\/Ridge\/Lasso\/Bayes Regression on this new set of features selected by the trees, and I reached my best model so far. Again, I tried playing around with values of lambda, but I couldn't get a good compromise without reducing the model performance. I did adjust the cutoff value to 0.3 (instead of 0.5) for the Linear\/Ridge\/Lasso becuase that gave the bet accuracy and f1 score. My first submission was of this linear regression model.","fe5f3040":"Next came the task of transforming the categorical variables. I wanted to leave the features default, housing, loan, and poutcome as a [0,1,2] for [No,Yes,Unknown]. That could be revised later as a potential way to cut down on information. I also wanted day of the week and month to preserve the order as they fall in a calendar, so I modified them to 1-5 for Mon-Fri and 1-12 for jan-dec. Two things to note here: (1) there were no jan or feb data in the month column and no sat or sun in the day column (2) I went back later to see if coding each month\/day in it's own column would be useful and it significantly hurt my model, so I switched back to keeping them within the same variable.","6cc19a08":"It was at this point that I was convinced that I needed to go back and parse down my feature subset even more, so I took only the features that were included in the the top 10 of both Decision Tree and Random Forest. No different selection of features improved Linear\/Ridge\/Lasso\/Bayes\/Logistic, so I went back to the drawing board. Something way back when I switched categorical variables to numeric probably want optimized.","c92c9d00":"The features Pdays and Previous were heavily skewed in their distribution of values, so I thought it would be interesting to plot them and see what's up.","7a3fcaa8":"Next I had to transform the categorical features that did not have a pre-defined order through one-hot encoding. These were the marital, job, education, and contact columns. I made sure to specify which 'unknown' corresponded to which variable for easy interpretation later.","734b8e8d":"Then I went back to the two tree models and input the new set of features to see if the model performance improved. they actually performed slightly better with the removal of variables, so I think I reduced a bit of overfitting that was occuring. I'd be curious to go back and try parsing down the variables even more later.","929968f9":"Next, I wanted to see if there were any null values, what the balance of 1s versus 0s were in the response variable, and see how many observations there are.","1e2579b5":"To start I performed a basic logistic regression on all 3 feature sets, and printed the results below. I used f1 score (which wants to be maximized) because I couldn't find the f mean score metric. The accuracy score is also a good measure of how well the model does at prediction. Further analysis could try different types of logistic regression other than 'liblinear'.","0ddd0389":"When we joined as a group, we attmpted imputing the unknown values, which was very difficult and did not get us anywhere. We also tried different methods of feature selection (stepwise, further reduction based on tree importance) and those did not yield good results either. We also tried oversampling to help balance out the number of successes and failures in the response column, but that actually hurt our model performance. \n\n\n\nBased on all of the attempts above, we submitted our top three models:\n1. Linear regression with a cutoff of 0.3 on the top 13 features most important to decision tree and random forest\n2. SVM on the top 13 features most important to decision tree and random forest\n3. Random Forest on the top 13 features most important to decision tree and random forest\n\nThe code for producing the final predictions is shown below. The accuracy and f1score of the same models but with the training data can be found in the previous sections.","4c017829":"Installing packages used to clean and visualize the data. There are other packages\/functions used later down for each type of regression.","790e6ed7":"I decided to run another logistic regression. It performed fairly well, and although the accuracy score was one of the highest to date, the f1 score was still fairly low, so I believe there still may be overfitting happening. I will go back later and try an even more reduced subset of variables.","41b648f3":"Next, I uploaded and looked at a rough summary of the datasets.","05bbc3d4":"So since that turned out any better than the original logistic regressions, I tried a new approach with Linear, Ridge, and Lasso regressions. (Spoiler this didn't really work out either) I messed around with different values of lambda, but the higher values caused the performance of the model to decrease, and the lower values were the same as the linear regression. This is something I could come back to and spend time refining, but I decided to see if there was a better group of features that I hadn't uncovered yet.","91f0c1df":"Then it came time ot piece the training and testing data back together, with the numerical features from the original data and the modified ordinal\/categoricla features just created.","00414879":"I found that there was significant overlap between the two methods of determining feature importance. The top 10 features from decision tree and random forest are shown below with dashes next to features that were included in both.\n\nDecision Tree\nduration               0.302235-\nid                     0.153253-\nage                    0.070679-\neuribor3m              0.053408-\nnr.employed            0.037945-\npdays                  0.034321-\ncampaign               0.032640-\nday                    0.030208-\nemp.var.rate           0.024689\nmonth                  0.021207\n\nRandom Forest\nduration             0.327945-\nnr.employed          0.154874-\nid                   0.116405-\nage                  0.074104-\neuribor3m            0.037914-\ncampaign             0.032394-\ncons.conf.idx        0.023777\nday                  0.023379-\npdays                0.022481-\nhousing              0.013133\n\nUsing this imformation, I created a new set of the training data with all of these features shown above. the ones not included in both top 10 were within the top 15 of the other feature list, so I felt comfortable including them. I didn't include month however, because I felt that variable had too much going on with it.","1bae807d":"Then I began a journey of trying different feature selection methods to see what woudl yield the best result. I started by comparing the correlation of each feature with the y variable. Through that I created two new feature sets, one that had features with abs(corr)>0.01 and one wiht abs(corr)>0.05. Then I used all three feature sets to go throgh some basic models that I will touch on later.","015e4f02":"Then I made my own split on the training set to have a train_training and train_testing set to use for checking the validity of the model. There are definitley better ways that taking the last 2000 rows as the testing set, but that was the easiest way to replicate across all datasets. The y split wil also be the same for all feature sets, so i only had to define that once.","1dc521a5":"Removing the features I did barely improved the model, so I tried selecting a different subset of features using an ANOVA F-test. I used all variables that were significant at an alpha=0.01 level and ran another logistic regression. ","8a742873":"I also tried K-Nearest Neighbors which was fairly successful, but not as good as SVM or Linear Regression. ","8feb31dc":"Welcome to Decision trees! I did a Decision Tree Classification and Random Forest Classification to see what features they decided were important. My thought was that I could take the top 10 or so features that were of note and go back and try some of the simpler regressions with that new subset of features.","3d310936":"Next, I moved onto Support Vector Machines (SVM), which gave me my second best entry to date. This took a minute to run, but was really a"}}