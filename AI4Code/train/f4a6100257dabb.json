{"cell_type":{"cdd9eed5":"code","22ee79a2":"code","466ae031":"code","d03101ca":"code","a991d122":"code","d0baff68":"code","7ceeedc0":"code","f7283d0d":"code","7c2ada1e":"code","5d4a89bd":"code","20002030":"code","e6583a0c":"code","b7c6354c":"code","ee29acc3":"code","2760bcec":"code","ac5b390c":"code","e4f19122":"code","31a6fca1":"code","cfe515fe":"code","20ba2655":"code","0d2a0fe8":"code","77cce4c6":"markdown","55507cea":"markdown"},"source":{"cdd9eed5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n%matplotlib inline\n\n\n\nimport gensim\nimport string\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\nimport nltk\nimport re","22ee79a2":"tweet_train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntweet_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\ntweet_train.head()","466ae031":"tweet_train['text'][5]","d03101ca":"tweet = pd.concat([tweet_train,tweet_test],axis=0)","a991d122":"tweet.shape,tweet_train.shape,tweet_test.shape","d0baff68":"sentances = [word for word in tweet_train['text']]\n\nsen = []\nfor word in sentances:\n    text = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE)\n    text = re.sub(r'http?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'[#,!,;,*,@,\\n,),?,(,.]', '', text)\n    text = text.lower()\n    emoji=re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji.sub('',text)\n    sen .append(text)\n    \n    ","7ceeedc0":"glove_dict = {}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt') as f:\n    for line in f:\n        line = line.split()\n        word = line[0]\n        values=np.asarray(line[1:],'float32')\n        glove_dict[word]=values\nlen(glove_dict)","f7283d0d":"glove_dict['the']","7c2ada1e":"corpus = []\nfor word in sen:\n    word = word.split()\n    corpus.append(word)\n    \n    ","5d4a89bd":"corpus[1]","20002030":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","e6583a0c":"tweet_pad","b7c6354c":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","ee29acc3":"from tqdm import tqdm\n","2760bcec":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=glove_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","ac5b390c":"embedding_matrix","e4f19122":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom tensorflow.keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\n\n","31a6fca1":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n\n","cfe515fe":"model.summary()","20ba2655":"X_train,X_test,y_train,y_test = train_test_split(tweet_pad,tweet_train['target'], random_state=0, test_size=0.2)\n","0d2a0fe8":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","77cce4c6":"## Gloval Vector for word representation","55507cea":"### Model building"}}