{"cell_type":{"9799026c":"code","1fddf593":"code","f1c14bc8":"code","7ad8327b":"code","ac0d48cb":"code","82d4485a":"code","23038b97":"code","09157135":"code","a052b1a8":"code","5d209234":"code","f2662b8d":"code","32bb0223":"code","e8d88c16":"code","c6b0a93a":"code","923d1357":"code","4e0fb6d2":"code","21bcb9e6":"code","56b36b6c":"code","5fd4f9d3":"code","a8ee303a":"code","365a02e8":"code","04ea678f":"code","98d19ad7":"code","5be32708":"code","1efc80fa":"code","ff7d1751":"code","80cf3a9e":"code","4fac9512":"code","a28ba6fa":"code","6300820f":"code","19857549":"markdown","13390927":"markdown","85bf45c1":"markdown","cf5c9de2":"markdown","0238b83c":"markdown","b113e5a5":"markdown","d95d549f":"markdown","db3eaeed":"markdown","fd13fe58":"markdown","997159cd":"markdown","f5260cae":"markdown","c00d2311":"markdown","336c89c5":"markdown","9cba313a":"markdown","f2b3e2f5":"markdown","2236516e":"markdown","c23b1305":"markdown","2d38132b":"markdown","c409ddb9":"markdown","a0003ad0":"markdown","c2fcd545":"markdown","e3afe4e3":"markdown","5cc8cf35":"markdown","2f792bd3":"markdown","f072d368":"markdown","7959207a":"markdown","68d339b3":"markdown"},"source":{"9799026c":"!pip install pycaret==2.0","1fddf593":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport math\nimport random\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\nsns.set(style=\"ticks\", context=\"talk\")\nplt.style.use(\"dark_background\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f1c14bc8":"data=pd.read_csv('\/kaggle\/input\/classifieddata\/Classified Data')\ndataforpycaret=pd.read_csv('\/kaggle\/input\/classifieddata\/Classified Data')\ndata.head()","7ad8327b":"data.drop(['Unnamed: 0'],axis = 1,inplace = True)\ndataforpycaret.drop(['Unnamed: 0'],axis = 1,inplace = True)\ndata.head()","ac0d48cb":"msno.matrix(data)","82d4485a":"report=pp.ProfileReport(data)","23038b97":"report","09157135":"plt.figure(figsize = (10,10)) #getting a visual of our data\nsns.heatmap(data.corr())\nplt.show()","a052b1a8":"scaler = StandardScaler()\nscaler.fit(data.drop('TARGET CLASS',axis = 1)) #standard sccaling our data for better results\nscaled_features = scaler.transform(data.drop('TARGET CLASS',axis = 1))\ndata_features = pd.DataFrame(scaled_features,columns = data.columns[:-1])\ndata_features.head()","5d209234":"def acc_summary(pipeline, X_train, y_train, X_val, y_val):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    y_pred = sentiment_fit.predict(X_val)\n    acc = metrics.accuracy_score(y_val, y_pred)\n    print(\"accuracy : {0:.2f}\".format(acc))\n    print(\"-\"*80)\n    return acc","f2662b8d":"names = [ \n        'Logistic Regression',\n        \"K Nearest Neighbour Classifier\",\n         \"Random Forest Classifier\"\n         ]\nclassifiers = [\n    LogisticRegression(),\n    KNeighborsClassifier(),\n    RandomForestClassifier(),\n        ]\n\nzipped_clf = zip(names,classifiers)","32bb0223":"def classifier_comparator(X_train,y_train,X_val,y_val,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([\n            ('classifier', c)\n        ])\n        print(\"Validation result for {}\".format(n))\n        clf_acc= acc_summary(checker_pipeline,X_train, y_train, X_val, y_val)\n        result.append((n,clf_acc))\n    return result","e8d88c16":"X_train,X_val,y_train,y_val=train_test_split(data_features,data['TARGET CLASS'],test_size=0.3,random_state=2)","c6b0a93a":"classifier_comparator(X_train,y_train,X_val,y_val)","923d1357":"from pycaret.classification import *","4e0fb6d2":"dataforpycaret.head()","21bcb9e6":"clf1 = setup(data = dataforpycaret, \n             target = 'TARGET CLASS',\n             normalize=True,\n             pca=True,\n             feature_selection=True,\n             silent = True)\n\n#quite intuitive isn't it ?","56b36b6c":"compare_models()","5fd4f9d3":"lrmodel=create_model('lr')","a8ee303a":"tuned_lr=tune_model(lrmodel)","365a02e8":"gbcmodel=create_model('gbc')","04ea678f":"tuned_gbc=tune_model(gbcmodel)","98d19ad7":"xgbmodel=create_model('xgboost')","5be32708":"tuned_xgb=tune_model(xgbmodel)","1efc80fa":"plot_model(lrmodel, plot = 'boundary')","ff7d1751":"plot_model(gbcmodel, plot = 'boundary')","80cf3a9e":"plot_model(xgbmodel, plot = 'boundary')","4fac9512":"knnmodel=create_model('knn')","a28ba6fa":"tuned_knn=tune_model(knnmodel)","6300820f":"plot_model(tuned_knn, plot = 'boundary')","19857549":"# Predicting whether you'll ace an interview or not..","13390927":"As we can see the most basic model performs better than the complex ones,But we haven't done fine tuning of model,nor we have done any feature selction,just plain basic implementation.Now let me show you the power of pycaret and with what efficiency we can perform model selection and feature selection as well.","85bf45c1":"   ### 1.Logistic Regression","cf5c9de2":"## 3.Extreme Gradient Boosting","0238b83c":"<font color='red'> **  Do Upvote the Kernel and Comment your views in the comment section below . **  <\/font> ","b113e5a5":"Data is ready and scaled lets start fitting our models.","d95d549f":"Now we haven't tuned the models yet,I am going to take the first two models and tune it and check whether the accuracy increases or not.","db3eaeed":"We can see that the unnamed column is unecessary and we can work well without it. Hence, we will drop it.","fd13fe58":"# Importing Pycaret Classification Module","997159cd":"for plotting Decision Booundary,I wanted to see how __KNN__ bouundary turns out.","f5260cae":"# Generating Pandas Profile report.","c00d2311":"## 2.Gradient Boosting Classifier ","336c89c5":"## Plotting the decision Boundary.","9cba313a":"## Splitting The Data.","f2b3e2f5":"![image.png](attachment:image.png)","2236516e":"### Pretty cool than the previous two...","c23b1305":"The reason we are making two copies-one on which we'll work hard by writing multiple lines of code and on the other,I'll pycaret which does the entire work within a few lines.","2d38132b":"# Lets see the data again.","c409ddb9":"# we can even easily visualize model predictions.","a0003ad0":"# Do Comment and share your opinions!!!","c2fcd545":"## Lets build our model.","e3afe4e3":"### Importing The libraries.","5cc8cf35":"# Comparing all models-That's it!!!","2f792bd3":"Lets check how many null values does the dataset contains.","f072d368":"- From the Report , it is clear that all the variables vary in the same manner,i.e the all vary with same gaussian distribution.\n- From the Correlation,we see that the variables having really small correaltion values with the target variable.","7959207a":"Luckily the dataset has no null values,which has reduced half of our efforts.","68d339b3":"# Tuning models."}}