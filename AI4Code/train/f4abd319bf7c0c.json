{"cell_type":{"5d6ff7a1":"code","e83447a8":"code","49da0830":"code","e49b35e7":"code","85131ab2":"code","7c529ebd":"code","a88f91ff":"code","1b353f86":"code","3848a177":"code","bd808b75":"code","3ca4758c":"code","9c6283a1":"code","6341416e":"code","c046537f":"code","d055632c":"code","8f7fb4b0":"code","da8dbcdb":"code","6a291091":"code","0500f440":"code","2f524254":"code","42c35908":"code","4da88bc0":"code","16f3a259":"code","8a05520f":"code","931be08c":"code","ffdafef7":"code","57a0bb71":"code","8256e756":"code","bcd7c879":"code","395b3147":"code","ada4ec5a":"code","a23f90a5":"code","a4bc13e7":"code","123958a8":"code","ff11458f":"code","afbba799":"code","ec084869":"code","37dedf25":"code","e5de4c32":"code","7c961a5a":"code","31322ea4":"code","88780a3b":"code","4b94f902":"code","2eab4d8e":"code","1d657cdd":"code","00da2ef2":"code","24dcedba":"code","36b62d0f":"code","f5bf1b67":"code","c9a367db":"code","d3f562da":"code","56b85697":"code","cd766838":"code","fd373215":"code","9603cadd":"code","2279f17e":"code","89a5ae82":"code","ee2806d9":"code","0941651b":"markdown","817efdc6":"markdown","e6f0c082":"markdown","0e19413b":"markdown","40b343fd":"markdown","118205fd":"markdown","57de5fc9":"markdown","dd1e641f":"markdown","2ced78f6":"markdown","015d2282":"markdown","123c010a":"markdown","cb5ab560":"markdown","d5937629":"markdown","d2a9ae84":"markdown","6baaff0a":"markdown","59b865e9":"markdown","53ed62cb":"markdown","92bfcb86":"markdown","ea1f1470":"markdown","48c21f38":"markdown","49406350":"markdown","58bc7ec8":"markdown","79487e8f":"markdown","6d81dd25":"markdown","296f8d17":"markdown","42cbaace":"markdown","17988a1c":"markdown","78cf7bd2":"markdown","96867192":"markdown"},"source":{"5d6ff7a1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\n\n# reading the dataset\ncars = pd.read_csv(\"..\/input\/car-data\/CarPrice_Assignment.csv\")","e83447a8":"# summary of the dataset: 205 rows, 26 columns, no null values\nprint(cars.info())","49da0830":"# head\ncars.head()","e49b35e7":"# symboling: -2 (least risky) to +3 most risky\n# Most cars are 0,1,2\ncars['symboling'].astype('category').value_counts()\n\n","85131ab2":"# aspiration: An (internal combustion) engine property showing \n# whether the oxygen intake is through standard (atmospheric pressure)\n# or through turbocharging (pressurised oxygen intake)\n\ncars['aspiration'].astype('category').value_counts()","7c529ebd":"# drivewheel: frontwheel, rarewheel or four-wheel drive \ncars['drivewheel'].astype('category').value_counts()","a88f91ff":"# wheelbase: distance between centre of front and rarewheels\nsns.distplot(cars['wheelbase'])\nplt.show()","1b353f86":"# curbweight: weight of car without occupants or baggage\nsns.distplot(cars['curbweight'])\nplt.show()","3848a177":"# stroke: volume of the engine (the distance traveled by the \n# piston in each cycle)\nsns.distplot(cars['stroke'])\nplt.show()","bd808b75":"# compression ration: ration of volume of compression chamber \n# at largest capacity to least capacity\nsns.distplot(cars['compressionratio'])\nplt.show()","3ca4758c":"# target variable: price of car\nsns.distplot(cars['price'])\nplt.show()","9c6283a1":"# all numeric (float and int) variables in the dataset\ncars_numeric = cars.select_dtypes(include=['float64', 'int'])\ncars_numeric.head()","6341416e":"# dropping symboling and car_ID \ncars_numeric = cars_numeric.drop(['symboling', 'car_ID'], axis=1)\ncars_numeric.head()","c046537f":"# paiwise scatter plot\n\nplt.figure(figsize=(20, 10))\nsns.pairplot(cars_numeric)\nplt.show()","d055632c":"# correlation matrix\ncor = cars_numeric.corr()\ncor","8f7fb4b0":"# plotting correlations on a heatmap\n\n# figure size\nplt.figure(figsize=(16,8))\n\n# heatmap\nsns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\nplt.show()\n","da8dbcdb":"# variable formats\ncars.info()","6a291091":"# converting symboling to categorical\ncars['symboling'] = cars['symboling'].astype('object')\ncars.info()","0500f440":"# CarName: first few entries\ncars['CarName'][:30]","2f524254":"# Extracting carname\n\n# Method 1: str.split() by space\ncarnames = cars['CarName'].apply(lambda x: x.split(\" \")[0])\ncarnames[:30]","42c35908":"# Method 2: Use regular expressions\nimport re\n\n# regex: any alphanumeric sequence before a space, may contain a hyphen\np = re.compile(r'\\w+-?\\w+')\ncarnames = cars['CarName'].apply(lambda x: re.findall(p, x)[0])\nprint(carnames)","4da88bc0":"# New column car_company\ncars['car_company'] = cars['CarName'].apply(lambda x: re.findall(p, x)[0])","16f3a259":"# look at all values \ncars['car_company'].astype('category').value_counts()","8a05520f":"# replacing misspelled car_company names\n\n# volkswagen\ncars.loc[(cars['car_company'] == \"vw\") | \n         (cars['car_company'] == \"vokswagen\")\n         , 'car_company'] = 'volkswagen'\n\n# porsche\ncars.loc[cars['car_company'] == \"porcshce\", 'car_company'] = 'porsche'\n\n# toyota\ncars.loc[cars['car_company'] == \"toyouta\", 'car_company'] = 'toyota'\n\n# nissan\ncars.loc[cars['car_company'] == \"Nissan\", 'car_company'] = 'nissan'\n\n# mazda\ncars.loc[cars['car_company'] == \"maxda\", 'car_company'] = 'mazda'","931be08c":"cars['car_company'].astype('category').value_counts()","ffdafef7":"# drop carname variable\ncars = cars.drop('CarName', axis=1)","57a0bb71":"cars.info()","8256e756":"# outliers\ncars.describe()","bcd7c879":"cars.info()","395b3147":"# split into X and y\nX = cars.loc[:, ['symboling', 'fueltype', 'aspiration', 'doornumber',\n       'carbody', 'drivewheel', 'enginelocation', 'wheelbase', 'carlength',\n       'carwidth', 'carheight', 'curbweight', 'enginetype', 'cylindernumber',\n       'enginesize', 'fuelsystem', 'boreratio', 'stroke', 'compressionratio',\n       'horsepower', 'peakrpm', 'citympg', 'highwaympg',\n       'car_company']]\n\ny = cars['price']\n","ada4ec5a":"# creating dummy variables for categorical variables\n\n# subset all categorical variables\ncars_categorical = X.select_dtypes(include=['object'])\ncars_categorical.head()\n","a23f90a5":"# convert into dummies\ncars_dummies = pd.get_dummies(cars_categorical, drop_first=True)\ncars_dummies.head()","a4bc13e7":"# drop categorical variables \nX = X.drop(list(cars_categorical.columns), axis=1)","123958a8":"# concat dummy variables with X\nX = pd.concat([X, cars_dummies], axis=1)","ff11458f":"# scaling the features\nfrom sklearn.preprocessing import scale\n\n# storing column names in cols, since column names are (annoyingly) lost after \n# scaling (the df is converted to a numpy array)\ncols = X.columns\nX = pd.DataFrame(scale(X))\nX.columns = cols\nX.columns","afbba799":"# split into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","ec084869":"# Building the first model with all the features\n\n# instantiate\nlm = LinearRegression()\n\n# fit\nlm.fit(X_train, y_train)","37dedf25":"# print coefficients and intercept\nprint(lm.coef_)\nprint(lm.intercept_)","e5de4c32":"# predict \ny_pred = lm.predict(X_test)\n\n# metrics\nfrom sklearn.metrics import r2_score\n\nprint(r2_score(y_true=y_test, y_pred=y_pred))","7c961a5a":"# RFE with 15 features\nfrom sklearn.feature_selection import RFE\n\n# RFE with 15 features\nlm = LinearRegression()\nrfe_15 = RFE(lm, 15)\n\n# fit with 15 features\nrfe_15.fit(X_train, y_train)\n\n# Printing the boolean results\nprint(rfe_15.support_)           \nprint(rfe_15.ranking_)  ","31322ea4":"# making predictions using rfe model\ny_pred = rfe_15.predict(X_test)\n\n# r-squared\nprint(r2_score(y_test, y_pred))","88780a3b":"# RFE with 6 features\nfrom sklearn.feature_selection import RFE\n\n# RFE with 6 features\nlm = LinearRegression()\nrfe_6 = RFE(lm, 6)\n\n# fit with 6 features\nrfe_6.fit(X_train, y_train)\n\n# predict\ny_pred = rfe_6.predict(X_test)\n\n# r-squared\nprint(r2_score(y_test, y_pred))","4b94f902":"# import statsmodels\nimport statsmodels.api as sm  \n\n# subset the features selected by rfe_15\ncol_15 = X_train.columns[rfe_15.support_]\n\n# subsetting training data for 15 selected columns\nX_train_rfe_15 = X_train[col_15]\n\n# add a constant to the model\nX_train_rfe_15 = sm.add_constant(X_train_rfe_15)\nX_train_rfe_15.head()","2eab4d8e":"# fitting the model with 15 variables\nlm_15 = sm.OLS(y_train, X_train_rfe_15).fit()   \nprint(lm_15.summary())","1d657cdd":"# making predictions using rfe_15 sm model\nX_test_rfe_15 = X_test[col_15]\n\n\n# # Adding a constant variable \nX_test_rfe_15 = sm.add_constant(X_test_rfe_15, has_constant='add')\nX_test_rfe_15.info()\n\n\n# # Making predictions\ny_pred = lm_15.predict(X_test_rfe_15)\n","00da2ef2":"# r-squared\nr2_score(y_test, y_pred)","24dcedba":"# subset the features selected by rfe_6\ncol_6 = X_train.columns[rfe_6.support_]\n\n# subsetting training data for 6 selected columns\nX_train_rfe_6 = X_train[col_6]\n\n# add a constant to the model\nX_train_rfe_6 = sm.add_constant(X_train_rfe_6)\n\n\n# fitting the model with 6 variables\nlm_6 = sm.OLS(y_train, X_train_rfe_6).fit()   \nprint(lm_6.summary())\n\n\n# making predictions using rfe_6 sm model\nX_test_rfe_6 = X_test[col_6]\n\n\n# Adding a constant  \nX_test_rfe_6 = sm.add_constant(X_test_rfe_6, has_constant='add')\nX_test_rfe_6.info()\n\n\n# # Making predictions\ny_pred = lm_6.predict(X_test_rfe_6)","36b62d0f":"# r2_score for 6 variables\nr2_score(y_test, y_pred)","f5bf1b67":"n_features_list = list(range(4, 20))\nadjusted_r2 = []\nr2 = []\ntest_r2 = []\n\nfor n_features in range(4, 20):\n\n    # RFE with n features\n    lm = LinearRegression()\n\n    # specify number of features\n    rfe_n = RFE(lm, n_features)\n\n    # fit with n features\n    rfe_n.fit(X_train, y_train)\n\n    # subset the features selected by rfe_6\n    col_n = X_train.columns[rfe_n.support_]\n\n    # subsetting training data for 6 selected columns\n    X_train_rfe_n = X_train[col_n]\n\n    # add a constant to the model\n    X_train_rfe_n = sm.add_constant(X_train_rfe_n)\n\n\n    # fitting the model with 6 variables\n    lm_n = sm.OLS(y_train, X_train_rfe_n).fit()\n    adjusted_r2.append(lm_n.rsquared_adj)\n    r2.append(lm_n.rsquared)\n    \n    \n    # making predictions using rfe_15 sm model\n    X_test_rfe_n = X_test[col_n]\n\n\n    # # Adding a constant variable \n    X_test_rfe_n = sm.add_constant(X_test_rfe_n, has_constant='add')\n\n\n\n    # # Making predictions\n    y_pred = lm_n.predict(X_test_rfe_n)\n    \n    test_r2.append(r2_score(y_test, y_pred))\n","c9a367db":"# plotting adjusted_r2 against n_features\nplt.figure(figsize=(10, 8))\nplt.plot(n_features_list, adjusted_r2, label=\"adjusted_r2\")\nplt.plot(n_features_list, r2, label=\"train_r2\")\nplt.plot(n_features_list, test_r2, label=\"test_r2\")\nplt.legend(loc='upper left')\nplt.show()","d3f562da":"# RFE with n features\nlm = LinearRegression()\n\nn_features = 6\n\n# specify number of features\nrfe_n = RFE(lm, n_features)\n\n# fit with n features\nrfe_n.fit(X_train, y_train)\n\n# subset the features selected by rfe_6\ncol_n = X_train.columns[rfe_n.support_]\n\n# subsetting training data for 6 selected columns\nX_train_rfe_n = X_train[col_n]\n\n# add a constant to the model\nX_train_rfe_n = sm.add_constant(X_train_rfe_n)\n\n\n# fitting the model with 6 variables\nlm_n = sm.OLS(y_train, X_train_rfe_n).fit()\nadjusted_r2.append(lm_n.rsquared_adj)\nr2.append(lm_n.rsquared)\n\n\n# making predictions using rfe_15 sm model\nX_test_rfe_n = X_test[col_n]\n\n\n# # Adding a constant variable \nX_test_rfe_n = sm.add_constant(X_test_rfe_n, has_constant='add')\n\n\n\n# # Making predictions\ny_pred = lm_n.predict(X_test_rfe_n)\n\ntest_r2.append(r2_score(y_test, y_pred))","56b85697":"# summary\nlm_n.summary()","cd766838":"# results \nr2_score(y_test, y_pred)","fd373215":"# Error terms\nc = [i for i in range(len(y_pred))]\nfig = plt.figure()\nplt.plot(c,y_test-y_pred, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nfig.suptitle('Error Terms', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                      # X-label\nplt.ylabel('ytest-ypred', fontsize=16)                # Y-label\nplt.show()","9603cadd":"# Plotting the error terms to understand the distribution.\nfig = plt.figure()\nsns.distplot((y_test-y_pred),bins=50)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test-y_pred', fontsize=18)                  # X-label\nplt.ylabel('Index', fontsize=16)                          # Y-label\nplt.show()","2279f17e":"# mean\nnp.mean(y_test-y_pred)\n","89a5ae82":"sns.distplot(cars['price'],bins=50)\nplt.show()","ee2806d9":"# multicollinearity\npredictors = ['carwidth', 'curbweight', 'enginesize', \n             'enginelocation_rear', 'car_company_bmw', 'car_company_porsche']\n\ncors = X.loc[:, list(predictors)].corr()\nsns.heatmap(cors, annot=True)\nplt.show()","0941651b":"Note that the model with 15 variables gives about 93.9% r-squared, though that is on training data. The adjusted r-squared is 93.3.","817efdc6":"Let's create a new column to store the compnay name and check whether it looks okay.","e6f0c082":"## 3. Data Preparation \n\n\n#### Data Preparation\n\nLet's now prepare the data and build the model.","0e19413b":"#### Data Exploration\n\nTo perform linear regression, the (numeric) target variable should be linearly related to *at least one another numeric variable*. Let's see whether that's true in this case.\n\n\nWe'll first subset the list of all (independent) numeric variables, and then make a **pairwise plot**.","40b343fd":"#### Model Building Using RFE\n\nLet's now build a model using recursive feature elimination to select features. We'll first start off with an arbitrary number of features, and then use the ```statsmodels``` library to build models using the shortlisted features (this is also because sklearn doesn't have adjusted r-squared, statsmodels has).","118205fd":"### 1. Data Understanding and Exploration\n\nLet's first have a look at the dataset and understand the size, attribute names etc.","57de5fc9":"## 3. Model Building and Evaluation","dd1e641f":"Here, although the variable ```symboling``` is numeric (int), we'd rather treat it as categorical since it has only 6 discrete values. Also, we do not want 'car_ID'.","2ced78f6":"### Final Model\n\nLet's now build the final model with 6 features.","015d2282":"Let's now make a pairwise scatter plot and observe linear relationships.","123c010a":"This is quite hard to read, and we can rather plot correlations between variables. Also, a heatmap is pretty useful to visualise multiple correlations in one plot.","cb5ab560":"#### Understanding the Data Dictionary\n\nThe data dictionary contains the meaning of various attributes; some non-obvious ones are:","d5937629":"Based on the plot, we can choose the number of features considering the r2_score we are looking for. Note that there are a few caveats in this approach, and there are more sopisticated techniques to choose the optimal number of features:\n\n- Cross-validation: In this case, we have considered only one train-test split of the dataset; the values of r-squared and adjusted r-squared will vary with train-test split. Thus, cross-validation is a more commonly used technique (you divide the data into multiple train-test splits into 'folds', and then compute average metrics such as r-squared across the 'folds'\n\n- The values of r-squared and adjusted r-squared are computed based on the training set, though we must *always look at metrics computed on the test set*. For e.g. in this case, the test r2 actually goes down with increasing n - this phenomenon is called 'overfitting', where the performance on training set is good because the model has in some way 'memorised' the dataset, and thus the performance on test set is worse.\n\nThus, we can choose anything between 4 and 12 features, since beyond 12, the test r2 goes down; and at lesser than 4, the r2_score is too less.\n\nIn fact, the test_r2 score doesn't increase much anyway from n=6 to n=12. It is thus wiser to choose a simpler model, and so let's choose n=6.\n","d2a9ae84":"The heatmap shows some useful insights:\n\nCorrelation of price with independent variables:\n- Price is highly (positively) correlated with wheelbase, carlength, carwidth, curbweight, enginesize, horsepower (notice how all of these variables represent the size\/weight\/engine power of the car)\n\n- Price is negatively correlated to ```citympg``` and ```highwaympg``` (-0.70 approximately). This suggest that cars having high mileage may fall in the 'economy' cars category, and are priced lower (think Maruti Alto\/Swift type of cars, which are designed to be affordable by the middle class, who value mileage more than horsepower\/size of car etc.)\n\nCorrelation among independent variables:\n- Many independent variables are highly correlated (look at the top-left part of matrix): wheelbase, carlength, curbweight, enginesize etc. are all measures of 'size\/weight', and are positively correlated \n\n\nThus, while building the model, we'll have to pay attention to multicollinearity (especially linear models, such as linear and logistic regression, suffer more from multicollinearity).","6baaff0a":"## 2. Data Cleaning\n\nLet's now conduct some data cleaning steps. \n\nWe've seen that there are no missing values in the dataset. We've also seen that variables are in the correct format, except ```symboling```, which should rather be a categorical variable (so that dummy variable are created for the categories).\n\nNote that it *can* be used in the model as a numeric variable also. \n\n","59b865e9":"Now it may look like that the mean is not 0, though compared to the scale of 'price', -380 is not such a big number (see distribution below).","53ed62cb":"Thus, for the model with 6 variables, the r-squared on training and test data is about 89% and 88.5% respectively. The adjusted r-squared is about 88.6%.","92bfcb86":"Not bad, we are getting approx. 83% r-squared with all the variables. Let's see how much we can get with lesser features.","ea1f1470":"### Final Model Evaluation\n\nLet's now evaluate the model in terms of its assumptions. We should test that:\n- The error terms are normally distributed with mean approximately 0\n- There is little correlation between the predictors\n- Homoscedasticity, i.e. the 'spread' or 'variance' of the error term (y_true-y_pred) is constant","48c21f38":"## Car Price Prediction (CPP)\n\nThe solution is divided into the following sections: \n- Data understanding and exploration\n- Data cleaning\n- Data preparation\n- Model building and evaluation\n","49406350":"Thus, the test r-squared of model with 15 features is about 89.4%, while training is about 93%. Let's compare the same for the model with 6 features.","58bc7ec8":"Notice that the carname is what occurs before a space, e.g. alfa-romero, audi, chevrolet, dodge, bmx etc.\n\nThus, we need to simply extract the string before a space. There are multiple ways to do that.\n\n\n","79487e8f":"Notice that **some car-company names are misspelled** - vw and vokswagen should be volkswagen, porcshce should be porsche, toyouta should be toyota, Nissan should be nissan, maxda should be mazda etc.\n\nThis is a data quality issue, let's solve it.","6d81dd25":"The ```car_company``` variable looks okay now. Let's now drop the car name variable.","296f8d17":"Note that RFE with 6 features is giving about 88% r-squared, compared to 89% with 15 features. \nShould we then choose more features for slightly better performance?\n\nA better metric to look at is adjusted r-squared, which penalises a model for having more features, and thus weighs both the goodness of fit and model complexity. Let's use statsmodels library for this.\n","42cbaace":"#### Model Building and Evaluation ","17988a1c":"Netx, we need to extract the company name from the column ```CarName```. ","78cf7bd2":"### Choosing the optimal number of features\n\nNow, we have seen that the adjusted r-squared varies from about 93.3 to 88 as we go from 15 to 6 features, one way to choose the optimal number of features is to make a plot between n_features and adjusted r-squared, and then choose the value of n_features.","96867192":"Though this is the most simple model we've built till now, the final predictors still seem to have high correlations. One can go ahead and remove some of these features, though that will affect the adjusted-r2 score significantly (you should try doing that). \n\n\nThus, for now, the final model consists of the 6 variables mentioned above."}}