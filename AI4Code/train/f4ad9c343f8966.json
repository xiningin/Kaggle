{"cell_type":{"5f5fd369":"code","1e747303":"code","b871f0e3":"code","0f6fb118":"code","2971aef9":"code","33852002":"code","3431eff9":"code","73d06d89":"code","a15efa8b":"code","03b65df9":"code","aac4c386":"code","c2e19925":"code","0c1bf92f":"code","a70875a3":"code","eaddbd95":"code","5701a99a":"code","1b46deaa":"code","fde9eb11":"code","f03825a4":"code","f99f0ae5":"code","75b9bfb7":"code","b133b025":"code","0cdf5924":"markdown","617467a9":"markdown","a407933d":"markdown"},"source":{"5f5fd369":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import words, stopwords\nimport re\nfrom urllib.parse import urlparse\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\nimport time\nimport datetime\nimport random as rand\n\nfrom catboost import CatBoostRegressor\n\nfrom matplotlib import pyplot as plt\n\npd.options.display.max_columns = None\npd.options.mode.chained_assignment = None","1e747303":"# Read and concatenate datasets for 2020 and 2021\ndata_2020 = pd.read_csv('..\/input\/medium-data-science-articles-dataset\/medium-data-science-articles-2020.csv')\ndata_2020","b871f0e3":"data_2021 = pd.read_csv('..\/input\/medium-2021-data-science-articles-dataset\/medium-data-science-articles-2021.csv')\ndata_2021","0f6fb118":"data = pd.concat([data_2020, data_2021]).reset_index(drop=True)\ndata","2971aef9":"# First, we'll extract the name of the site the article is published on from article's URL\n# and replace it with a numerical ID\ndata['article_domain'] = data.url.apply(lambda x: urlparse(x).netloc)\ndata['article_domain'].value_counts()","33852002":"# Constructing dictionary of site names and corresponding integer IDs\nsite_id_dict = {}\nsite_id = 0\n\nfor site in data['article_domain'].unique():\n    site_id_dict[site] = site_id\n    site_id += 1","3431eff9":"# There are only 7 possible tags, so we can convert the tag name into numerical ID\ndata['tag'].value_counts()","73d06d89":"# Constructing dictionary of tag names and corresponding integer IDs\ntag_id_dict = {}\ntag_id = 0\n\nfor tag in data['tag'].unique():\n    tag_id_dict[tag] = tag_id\n    tag_id += 1","a15efa8b":"# 'author_page' is dropped as its semantics is similar to 'author' and contains redundant HTTP address elements\ndata = data.drop(columns=['author_page'])\ndata","03b65df9":"# In the third line from the end, there is a HTML tag which is not removed\n# I guess there are some more similar parsing errors in the data\n# Let's strip HTML tags from dataset and keep the title\/subtitle text only\nfor col in ['title', 'author', 'subtitle']:\n    data[col] = data[col].apply(lambda x: re.sub(r'<[^>]+>', '', str(x)))\n\ndata","aac4c386":"# There are 'nan' values in subtitle; Pandas treats them as strings,\n# but let's replace them with 'none' as more appropriate word to\n# denote a missing value\ndata.isna().any()","c2e19925":"# Replacing 'nan' with 'none' as more natural term for missing values :)\ndata = data.replace('nan', 'none')\ndata","0c1bf92f":"# Text preprocessing: removing stopwords, punctuation signs,\n# words that are 1 and 2 letters long\n# remaining words are lemmatized and made lowercase\nstop_words = stopwords.words('english')\nlemmatizer = WordNetLemmatizer()\n\nfor col in ['title', 'author', 'subtitle']:\n    data[col] = data[col].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in re.sub('[^a-zA-Z0-9]', ' ', x).lower().split()\n                                                   if w not in stop_words and len(w) > 2]))\n\n\n# Replacing tags and sites with their numerical IDs created above\ndata['tag'] = data['tag'].apply(lambda x: tag_id_dict[x])\ndata['site_id'] = data['article_domain'].apply(lambda x: site_id_dict[x])\n\ndata","a70875a3":"# Let's add one date-based feature: for each month (except January) we'll count\n# an estimate of tag (topic) popularity as ratio of number of articles\n# with this tag published in previous month and number of all articles published in previous month\n# For each article, we'll add the popularity of its tag during previous month\ndata_w_tag_rates = pd.DataFrame()\n\nfor y in [0, 1]:\n    for i in range(1, 13):     \n        cur_mon_data = data[data.date.str.contains(f'202{y}-{i}-' if i >= 10 else f'202{y}-0{i}-')]\n\n        # For Jan 2020, there's no previous month so\n        # additional feature is set to zero\n        if y == 0 and i == 1:\n            cur_mon_data['prev_mon_topic_rate'] = 0\n        else:\n            if i > 1:\n                prev_mon_data = data[data.date.str.contains(f'202{y}-{i-1}-' if i-1 >= 10 else f'202{y}-0{i-1}-')]\n            # For 2021-01, the previous month is 2020-12, we should keep it in mind\n            else:\n                prev_mon_data = data[data.date.str.contains('2020-12-')]\n            prev_mon_tag_rates = {}\n            for tag in tag_id_dict.values():\n                prev_mon_cur_tag = prev_mon_data[prev_mon_data.tag == tag]\n                prev_mon_tag_rates[tag] = len(prev_mon_cur_tag) \/ len(prev_mon_data)\n\n            cur_mon_data['prev_mon_topic_rate'] = cur_mon_data.tag.apply(lambda x: prev_mon_tag_rates[x])\n\n        data_w_tag_rates = data_w_tag_rates.append(cur_mon_data)\n    \ndata = data_w_tag_rates\n\n# We also need to make train and test sets\n# We keep date column to simplify the split,\n# but we need to convert the date to milliseconds\n# to use it as numerical training parameter\ndata['date_ms'] = data['date'].apply(lambda x: float(time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%d\").timetuple())))\nmax_ts = data['date_ms'].max()\nmin_ts = data['date_ms'].min()\ndata['date_ms'] = data['date_ms'].apply(lambda x: (x - min_ts) \/ (max_ts - min_ts))\n\ndata","eaddbd95":"# Final dataframe before feature engineering block\ndata","5701a99a":"'''clap_sums = []\nresp_sums = []\nread_time_sums = []\nclap_tag_sums = []\nresp_tag_sums = []\nread_time_tag_sums = []\n\nsite_clap_sums = []\nsite_resp_sums = []\nsite_read_time_sums = []\nsite_clap_tag_sums = []\nsite_resp_tag_sums = []\nsite_read_time_tag_sums = []\n\ntitle_lens = []\nsubtitle_lens = []\nnickname_lens = []\n\ntitle_num_amounts = []\nsubtitle_num_amounts = []\nauthor_num_amounts = []\n\njarg_title_amounts = []\njarg_subtitle_amounts = []\n\nen_word_corpus = words.words()\n\n# faster dataframe slicing? (append instead of slice?)\nart_slice = pd.DataFrame(columns=data.columns)\n\nfor i in range(len(data)):\n    if i % 1000 == 0:\n        print(f'Processing {i}\/{len(data)}')\n    \n    cur_art = data.iloc[i]\n    art_slice = art_slice.append(cur_art)\n    \n    author_articles = art_slice[art_slice.author == cur_art.author]\n    if len(author_articles):\n        clap_sums.append(author_articles.claps.sum())\n        resp_sums.append(author_articles.responses.sum())\n        read_time_sums.append(author_articles.reading_time.sum())\n    else:\n        clap_sums.append(0)\n        resp_sums.append(0)\n        read_time_sums.append(0)\n        \n    author_tag_articles = author_articles[author_articles.tag == cur_art.tag]\n    if len(author_tag_articles):\n        clap_tag_sums.append(author_tag_articles.claps.sum())\n        resp_tag_sums.append(author_tag_articles.responses.sum())\n        read_time_tag_sums.append(author_tag_articles.reading_time.sum())\n    else:\n        clap_tag_sums.append(0)\n        resp_tag_sums.append(0)\n        read_time_tag_sums.append(0)\n        \n    site_articles = art_slice[art_slice.site_id == cur_art.site_id]\n    if len(site_articles):\n        site_clap_sums.append(site_articles.claps.sum())\n        site_resp_sums.append(site_articles.responses.sum())\n        site_read_time_sums.append(site_articles.reading_time.sum())\n    else:\n        site_clap_sums.append(0)\n        site_resp_sums.append(0)\n        site_read_time_sums.append(0)\n        \n    site_tag_articles = site_articles[site_articles.tag == cur_art.tag]\n    if len(site_tag_articles):\n        site_clap_tag_sums.append(site_tag_articles.claps.sum())\n        site_resp_tag_sums.append(site_tag_articles.responses.sum())\n        site_read_time_tag_sums.append(site_tag_articles.reading_time.sum())\n    else:\n        site_clap_tag_sums.append(0)\n        site_resp_tag_sums.append(0)\n        site_read_time_tag_sums.append(0)\n        \n    title_lens.append(len(cur_art.title.split(' ')))\n    subtitle_lens.append(len(cur_art.subtitle.split(' ')) if cur_art.subtitle != 'none' else 0)\n    nickname_lens.append(len(cur_art.author.split(' ')))\n    \n    title_num_amounts.append(len([w for w in cur_art.title.split(' ') if w.isdigit()]))\n    subtitle_num_amounts.append(len([w for w in cur_art.subtitle.split(' ') if w.isdigit()]))\n    author_num_amounts.append(len([w for w in cur_art.author.split(' ') if w.isdigit()]))\n    \n    jarg_title_amounts.append(len([w for w in cur_art.title.split(' ') if w not in en_word_corpus]))\n    jarg_subtitle_amounts.append(len([w for w in cur_art.subtitle.split(' ') if w not in en_word_corpus]))\n    \ndata['author_clap_sum'] = clap_sums\ndata['author_resp_sum'] = resp_sums\ndata['author_read_time_sum'] = read_time_sums\ndata['author_clap_tag_sum'] = clap_tag_sums\ndata['author_resp_tag_sum'] = resp_tag_sums\ndata['author_read_time_tag_sum'] = read_time_tag_sums\n\ndata['site_clap_sum'] = site_clap_sums\ndata['site_resp_sum'] = site_resp_sums\ndata['site_read_time_sum'] = site_read_time_sums\ndata['site_clap_tag_sum'] = site_clap_tag_sums\ndata['site_resp_tag_sum'] = site_resp_tag_sums\ndata['site_read_time_tag_sum'] = site_read_time_tag_sums\n\ndata['title_len'] = title_lens\ndata['subtitle_len'] = subtitle_lens\ndata['nickname_len'] = nickname_lens\n\ndata['title_num_amount'] = title_num_amounts\ndata['subtitle_num_amount'] = subtitle_num_amounts\ndata['nickname_num_amount'] = author_num_amounts\n\ndata['jarg_title_amount'] = jarg_title_amounts\ndata['jarg_subtitle_amount'] = jarg_subtitle_amounts\n\ndata'''","1b46deaa":"# This input file contains the result of numerical features extraction;\n# as it was mentioned above, its generation takes about 7 hours (at the moment I don't know the way\n# how to speed the process up)\ndata = pd.read_csv('..\/input\/medium-202021-mlds-articles-numerical-stats\/medium-2021-articles-and-nums.csv').drop(columns=['Unnamed: 0'])\ndata","fde9eb11":"# At the moment we have 25 numerical features, except 'claps', for each article\ndata_num = data[['tag', 'date', 'site_id',\n               'prev_mon_topic_rate', 'date_ms', 'author_clap_sum', 'author_resp_sum',\n               'author_read_time_sum', 'author_clap_tag_sum', 'author_resp_tag_sum',\n               'author_read_time_tag_sum', 'site_clap_sum', 'site_resp_sum',\n               'site_read_time_sum', 'site_clap_tag_sum', 'site_resp_tag_sum',\n               'site_read_time_tag_sum', 'title_len', 'subtitle_len', 'nickname_len',\n               'title_num_amount', 'subtitle_num_amount', 'nickname_num_amount',\n               'jarg_title_amount', 'jarg_subtitle_amount', 'claps']]\ndata_num","f03825a4":"# Train\/test loop: each time we train on all previous months ant test on current one;\n# calculating R2 score, MAE and 100+\/1000+\/10000+ error rates for each 'fold'\n# and showing them on line chart\ntrain_data_w_date = pd.DataFrame(columns=data_num.columns)\n\nr2s = []\nmaes = []\nerrors_100 = []\nerrors_1000 = []\nerrors_10000 = []\n\nfor y in [0, 1]:\n    for i in range(1, 13):\n        test_data_w_date = pd.DataFrame(columns=data_num.columns)\n        if len(train_data_w_date) == 0:\n            train_data_w_date = train_data_w_date.append(data_num[data_num.date.str.contains('2020-01-')]).reset_index(drop=True)\n            test_data_w_date = test_data_w_date.append(data_num[data_num.date.str.contains('2020-02-')]).reset_index(drop=True)\n            i += 1\n        elif i == 12:\n            if y == 0:\n                train_data_w_date = train_data_w_date.append(data_num[data_num.date.str.contains('2020-12-')]).reset_index(drop=True)\n                test_data_w_date = test_data_w_date.append(data_num[data_num.date.str.contains('2021-01-')]).reset_index(drop=True)\n            else:\n                # when we have 2021-12 in train, there's no next month, so we stop the loop\n                break\n        else:\n            train_data_w_date = train_data_w_date.append(data_num[data_num.date.str.contains(f'202{y}-{i}-' if i >= 10 else f'202{y}-0{i}-')]).reset_index(drop=True)\n            test_data_w_date = test_data_w_date.append(data_num[data_num.date.str.contains(f'202{y}-{i+1}-' if i+1 >= 10 else f'202{y}-0{i+1}-')]).reset_index(drop=True)\n        \n        print(f'Min train data {train_data_w_date.date.iloc[0]}, max {train_data_w_date.date.iloc[-1]}')\n        print(f'Min test data {test_data_w_date.date.iloc[0]}, max {test_data_w_date.date.iloc[-1]}')\n        print(f\"Min claps in test: {test_data_w_date.claps.min()}, max: {test_data_w_date.claps.max()}\")\n        \n        train_data = train_data_w_date.drop(columns=['date'])\n        test_data = test_data_w_date.drop(columns=['date'])\n\n        cbr = CatBoostRegressor(loss_function='MAE', eval_metric='MAE', random_state=42,\n                            early_stopping_rounds=50, verbose=0)\n        cbr.fit(train_data.drop(columns=['claps']), train_data.claps)\n        Y_pred = cbr.predict(test_data.drop(columns=['claps']))\n        \n        cur_r2 = r2_score(test_data.claps, Y_pred)\n        cur_mae = mean_absolute_error(test_data.claps, Y_pred)\n        \n        print(f'Current CatBoost performance:')\n        print(f'R2={round(cur_r2, 4)}')\n        print(f'MAE={round(cur_mae, 4)}')\n        \n        r2s.append(cur_r2)\n        maes.append(cur_mae)\n\n        print('************')\n        Y_diff = abs(test_data.claps - Y_pred)\n        print(f'Rate of regression errors > 100: {len(Y_diff[Y_diff > 100])} \/ {len(test_data)} '\n              f'({round(len(Y_diff[Y_diff > 100]) \/ len(test_data) * 100, 2)}%)')\n        errors_100.append(len(Y_diff[Y_diff > 100]) \/ len(test_data))\n        \n        print(f'Rate of regression errors > 1000: {len(Y_diff[Y_diff > 1000])} \/ {len(test_data)} '\n              f'({round(len(Y_diff[Y_diff > 1000]) \/ len(test_data) * 100, 2)}%)')\n        errors_1000.append(len(Y_diff[Y_diff > 1000]) \/ len(test_data))\n        \n        print(f'Rate of regression errors > 10000: {len(Y_diff[Y_diff > 10000])} \/ {len(test_data)} '\n              f'({round(len(Y_diff[Y_diff > 10000]) \/ len(test_data) * 100, 2)}%)')\n        errors_10000.append(len(Y_diff[Y_diff > 10000]) \/ len(test_data))\n        \n        print('***')","f99f0ae5":"plt.figure(figsize=(10, 5))\nplt.ylabel('R2 score')\nplt.xlabel('Number of months in train set')\nplt.plot([i+1 for i in range(len(r2s))], r2s)\nplt.show()","75b9bfb7":"plt.figure(figsize=(10, 5))\nplt.ylabel('MAE')\nplt.xlabel('Number of months in train set')\nplt.plot([i+1 for i in range(len(maes))], maes)\nplt.show()","b133b025":"plt.figure(figsize=(10, 5))\nplt.ylabel('Error rates')\nplt.xlabel('Number of months in train set')\nplt.plot([i+1 for i in range(len(errors_100))], errors_100)\nplt.plot([i+1 for i in range(len(errors_1000))], errors_1000)\nplt.plot([i+1 for i in range(len(errors_10000))], errors_10000)\nplt.legend(['Rate of errors > 100', 'Rate of errors > 1000', 'Rate of errors > 10000'])\nplt.show()","0cdf5924":"The evaluation results appear to be not very stable from month to month, but the error trend is slowly decreasing, so I believe the higher performance is reachable with larger amount of available data.\n\nIn addition, the set of features suggested by me is not exhaustive (I already have a couple of thoughts about its widening). Maybe we can use raw textual data as additional information source, but LSTM-based models that I tried in a  private notebook gave me R2 below zero, so I decided to return to more \"numerical-based\" approach :)\n\n**Thanks for your attention! Hopefully it was interesting to check out my solution.**","617467a9":"**Greetings!**\n\nToday we'll predict the number of claps for data science articles by extracting multiple numerical features from the articles description and passing these features to CatBoost regressor. The models will be trained and evaluated for each month of 24 in the dataset.","a407933d":"The block that generates numerical features for articles is commented out as it took ~7 hours for Kaggle CPU to process it; I guess there's a way to optimize it, but for now I just pre-made the final dataframe and added it as data source for this notebook.\n\nThe list of added features is as follows:\n- sum of claps, responses and reading time received by the author before posting a new article\n- sum of claps, responses and reading time received by the author for previous articles with the same tag\n- length of preprocessed title, subtitle and author in words\n- number of numericals in preprocessed title, subtitle and author\n- number of jargon and technical terms (words that are not present in NLTK English dictionary) in title and subtitle text\n\nIn future this list may be extended."}}