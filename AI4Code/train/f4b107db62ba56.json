{"cell_type":{"0089ca38":"code","80ea2c61":"code","b20bb932":"code","b9320b77":"code","38edab2e":"code","ba17f036":"code","fcc134f0":"code","92f06e9f":"code","f5ff164e":"code","6ea79aaf":"code","b7387bab":"code","5778b481":"code","debe2581":"code","279a6f2d":"code","1632a9b8":"code","9f5b5107":"code","01d3de11":"code","01e9a2a8":"code","a7c19a2b":"code","3f586215":"code","7c876dc4":"code","877347ea":"code","5460c48f":"code","a9f49ef7":"markdown","152ff684":"markdown","07cd1baf":"markdown","2ae37915":"markdown","d027f05d":"markdown","c46b8a67":"markdown","b320ef6c":"markdown","23cc04e0":"markdown","bd96c457":"markdown","347a7ebb":"markdown","5494ec84":"markdown","5fe5f7dc":"markdown","1d4ce156":"markdown","2e51670a":"markdown","a377ee9e":"markdown","88289d41":"markdown","6a93586a":"markdown","4da83522":"markdown","ae03c755":"markdown","1a34e589":"markdown","56be7445":"markdown","95d9ffcf":"markdown","c2477d12":"markdown","283d6ab6":"markdown","a1506c7a":"markdown","af182af3":"markdown","c60094c5":"markdown","77cf9ad6":"markdown","67431333":"markdown","72bf01db":"markdown","76fc6404":"markdown","9e079e88":"markdown","cd499f61":"markdown","002dd2c7":"markdown","ee0682a6":"markdown","d5394387":"markdown","2fd304c7":"markdown","c3353349":"markdown","264ea532":"markdown","d5787371":"markdown"},"source":{"0089ca38":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport glob\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\npd.set_option('max_columns', 1000)\nfrom tqdm import tqdm\nfrom sklearn.neighbors import BallTree\nimport math\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nfrom datetime import datetime\nimport pytz\nfrom IPython.display import HTML\nimport scipy.stats as stats\nimport matplotlib as mpl\nfrom matplotlib import animation, rc, use\nfrom matplotlib.patches import Rectangle, Arrow\nimport tensorflow as tf\nfrom matplotlib.patches import Polygon\nimport matplotlib.patheffects as pe\nimport gc\n\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    \n    return df\n\n\ndef get_dx_dy(radian_angle, dist):\n    dx = dist * math.cos(radian_angle)\n    dy = dist * math.sin(radian_angle)\n    return dx, dy\n\n\ndef create_football_field(linenumbers=True,\n                          endzones=True,\n                          highlight_line=False,\n                          highlight_line_number=50,\n                          highlighted_name='Line of Scrimmage',\n                          fifty_is_los=False,\n                          figsize=(12*2, 6.33*2)):\n    \"\"\"\n    Function that plots the football field for viewing plays.\n    Allows for showing or hiding endzones.\n    \"\"\"\n    rect = patches.Rectangle((0, 0), 120, 53.3, linewidth=0.1,\n                             edgecolor='r', facecolor='slategrey', zorder=0)\n\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.add_patch(rect)\n\n    plt.plot([10, 10, 10, 20, 20, 30, 30, 40, 40, 50, 50, 60, 60, 70, 70, 80,\n              80, 90, 90, 100, 100, 110, 110, 120, 0, 0, 120, 120],\n             [0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3,\n              53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 53.3, 0, 0, 53.3],\n             color='white')\n    if fifty_is_los:\n        plt.plot([60, 60], [0, 53.3], color='gold')\n        plt.text(62, 50, '<- Player Yardline at Snap', color='gold')\n    # Endzones\n    if endzones:\n        ez1 = patches.Rectangle((0, 0), 10, 53.3,\n                                linewidth=0.3,\n                                edgecolor='k',\n                                facecolor='royalblue',\n                                alpha=0.4,\n                                zorder=1)\n        ez2 = patches.Rectangle((110, 0), 120, 53.3,\n                                linewidth=0.3,\n                                edgecolor='k',\n                                facecolor='royalblue',\n                                alpha=0.4,\n                                zorder=1)\n        ax.add_patch(ez1)\n        ax.add_patch(ez2)\n    plt.xlim(0, 120)\n    plt.ylim(0, 53.3)\n    plt.axis('off')\n    if linenumbers:\n        for x in range(20, 110, 10):\n            numb = x\n            if x > 50:\n                numb = 120 - x\n            plt.text(x, 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white')\n            plt.text(x - 0.95, 53.3 - 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white', rotation=180)\n    if endzones:\n        hash_range = range(11, 110)\n    else:\n        hash_range = range(1, 120)\n\n    for x in hash_range:\n        ax.plot([x, x], [0.4, 0.7], color='white')\n        ax.plot([x, x], [53.0, 52.5], color='white')\n        ax.plot([x, x], [22.91, 23.57], color='white')\n        ax.plot([x, x], [29.73, 30.39], color='white')\n\n    if highlight_line:\n        hl = highlight_line_number + 10\n        plt.plot([hl, hl], [0, 53.3], color='yellow')\n        plt.text(hl + 2, 50, '<- {}'.format(highlighted_name),\n                 color='yellow')\n    return fig, ax\n\n\n\nclass CreateNFLData:\n\n    def __init__(self):\n        pass\n\n    def LoadData(self, Normal=True):\n        if Normal == True:\n            print(\"Loading Original Data\")\n            globbed_files = glob.glob(\"week*.csv\") #creates a list of all csv files\n            data = []\n            for csv in tqdm(globbed_files):\n                frame = pd.read_csv(csv, index_col=0)\n                data.append(frame)\n\n            WeekData = pd.concat(data).reset_index()\n            WeekData\n        \n        else:\n            print(\"Loading Modified Data\")\n            globbed_files = glob.glob(\"Revised Data\/*.csv\") #creates a list of all csv files\n            data = []\n            for csv in tqdm(globbed_files):\n                frame = pd.read_csv(csv, index_col=0)\n                data.append(frame)\n\n            WeekData = pd.concat(data).reset_index()\n            WeekData\n        return WeekData\n\n\n\n    def Standardize(self,W):\n        print(\"Standardizing Data..\")\n        W['Dir_rad'] = np.mod(90 - W.dir, 360) * math.pi\/180.0\n        W['ToLeft'] = W.playDirection == \"left\"\n        W['TeamOnOffense'] = \"home\"\n        W.loc[W.possessionTeam != W.PlayerTeam, 'TeamOnOffense'] = \"away\"\n        W['IsOnOffense'] = W.PlayerTeam == W.TeamOnOffense # Is player on offense?\n        W['YardLine_std'] = 100 - W.yardlineNumber\n        W.loc[W.yardlineSide.fillna('') == W.possessionTeam,  \n                'YardLine_std'\n                ] = W.loc[W.yardlineSide.fillna('') == W.possessionTeam,  \n                'yardlineNumber']\n        W['X_std'] = W.x\n        W.loc[W.ToLeft, 'X_std'] = 120 - W.loc[W.ToLeft, 'x'] \n        W['Y_std'] = W.y\n        W.loc[W.ToLeft, 'Y_std'] = 160\/3 - W.loc[W.ToLeft, 'y'] \n        #W['Orientation_std'] = -90 + W.Orientation\n        #W.loc[W.ToLeft, 'Orientation_std'] = np.mod(180 + W.loc[W.ToLeft, 'Orientation_std'], 360)\n        W['Dir_std'] = W.Dir_rad\n        W.loc[W.ToLeft, 'Dir_std'] = np.mod(np.pi + W.loc[W.ToLeft, 'Dir_rad'], 2*np.pi)\n        W['dx'] = round(W['s']*np.cos(W['Dir_std']),2)\n        W['dy'] = round(W['s']*np.sin(W['Dir_std']),2)\n        W['X_std'] = round(W['X_std'],2)\n        W['Y_std'] = round(W['Y_std'],2)\n        #W['Orientation_rad'] = np.mod(W.o, 360) * math.pi\/180.0\n        W['Orientation_rad'] = np.mod(-W.o + 90, 360) * math.pi\/180.0\n        W['Orientation_std'] = W.Orientation_rad\n        W.loc[W.ToLeft, 'Orientation_std'] = np.mod(np.pi + W.loc[W.ToLeft, 'Orientation_rad'], 2*np.pi)\n        W['MPH'] = W['s'] \/ 0.488889\n        return W\n\n    \n    def FrameData(self,WeekData1):\n        NotNone = WeekData1.query('event != \"None\"')\n        NotNone = NotNone.groupby(['gameId','playId','event'])['frameId'].max().reset_index()\n        NotNone = NotNone.set_index(['gameId','playId','event'], drop= True).unstack('event').reset_index()\n        NotNone.columns = [' '.join(col).strip() for col in NotNone.columns.values]\n        NotNone.columns = NotNone.columns.str.replace('frameId' , '')\n        NotNone.columns = NotNone.columns.str.replace(' ' , '')\n        NotNone['Code'] = NotNone['gameId'].astype(str) + \"-\" + NotNone['playId'].astype(str)\n        NotNone = NotNone.set_index('Code')\n        NotNone = NotNone.loc[~NotNone.index.duplicated(keep='first')]\n\n        for col in tqdm(NotNone.columns):\n            NotNone['Contains_' + str(col)] = np.where(NotNone[col] > 0, True, False)\n\n        Cols = ['ball_snap', 'man_in_motion', 'pass_arrived', 'pass_forward','pass_outcome_caught', 'play_action', 'run_pass_option', 'Contains_man_in_motion', 'Contains_pass_arrived', 'Contains_pass_forward', 'Contains_pass_outcome_caught', 'Contains_play_action','Contains_run_pass_option']\n    #   WeekData1 = pd.merge(df, NotNone, how=\"left\", left_on=['gameId','playId'], right_on=['gameId','playId'] )\n        for col in Cols:\n            WeekData1[col] = WeekData1.Code.map(NotNone[col])\n\n        del NotNone\n        gc.collect()\n\n        WeekData1['After_snap'] = np.where(WeekData1['frameId'] > WeekData1['ball_snap'],1,0)\n        WeekData1['After_Throw'] = np.where(WeekData1['frameId'] > WeekData1['pass_forward'],1,0)\n        WeekData1['After_PassArrived'] = np.where(WeekData1['frameId'] > WeekData1['pass_arrived'],1,0)\n        WeekData1['After_PlayAction'] = np.where(WeekData1['frameId'] > WeekData1['play_action'],1,0)\n    #   WeekData1['After_run_pass_option'] = np.where(WeekData1['frameId'] > WeekData1['run_pass_option'],1,0)\n        WeekData1['After_Catch'] = np.where(WeekData1['frameId'] > WeekData1['pass_outcome_caught'],1,0)\n\n        \n        LOS = WeekData1.query('displayName == \"Football\" & After_snap == 0')\n        LOS = LOS.groupby(['gameId','playId'])['X_std','Y_std'].agg('median').reset_index()\n        LOS.columns = ['gameId','playId','LOSX','LOSY']\n        LOS['Code'] = LOS['gameId'].astype(str) + \"-\" + LOS['playId'].astype(str)\n        LOS = LOS.set_index('Code')\n        LOS = LOS.loc[~LOS.index.duplicated(keep='first')]\n        WeekData1[\"LOSX\"] = WeekData1.Code.map(LOS['LOSX'])\n        WeekData1[\"LOSY\"] = WeekData1.Code.map(LOS['LOSY'])\n        WeekData1['Distfrom_LOSX'] = WeekData1['X_std'] - WeekData1['LOSX']\n        WeekData1['Distfrom_LOSY'] = WeekData1['Y_std'] - WeekData1['LOSY']\n        WeekData1['AbsDistfrom_LOSX'] = np.abs(WeekData1['X_std'] - WeekData1['LOSX'])\n        WeekData1['AbsDistfrom_LOSY'] = np.abs(WeekData1['Y_std'] - WeekData1['LOSY'])\n        del LOS\n        gc.collect()\n        return WeekData1\n    \n    def import_data(self,file,columns=False,cols=\"\"):\n        \"\"\"create a dataframe and optimize its memory usage\"\"\"\n        if columns == False:\n            df = pd.read_csv(file, low_memory=False)\n            df = reduce_mem_usage(df)\n        else:\n            df = pd.read_csv(file, low_memory=False, usecols=cols)\n            df = reduce_mem_usage(df)\n        return df\n    \n    \n\n\n\n\n\n\nclass AnimatePlay:\n    def __init__(self, play_df,player_id=[], Tri = False, MPH = False,Text=\"\",Show='jerseyNumber',method='all' ) -> None:\n        self._MAX_FIELD_Y = 53.3\n        self._MAX_FIELD_X = 120\n        self._MAX_FIELD_PLAYERS = 22\n        \n\n        self.Tri = Tri\n        self.MPH = MPH\n        self.player_id = player_id\n        self.Show = Show\n        self.method = method\n        self.Text = Text\n\n        self._CPLT = sns.color_palette(\"husl\", 2)\n        self._frame_data = play_df\n        self._times = sorted(play_df.time.unique())\n        self._stream = self.data_stream()\n        \n        self._date_format = \"%Y-%m-%dT%H:%M:%S.%fZ\" \n        self._mean_interval_ms = np.mean([delta.microseconds\/1000 for delta in np.diff(np.array([pytz.timezone('US\/Eastern').localize(datetime.strptime(date_string, self._date_format)) for date_string in self._times]))])\n        \n        self._fig, self._ax_field = create_football_field()\n\n        self._fig.set_figheight(10)\n        self._fig.set_figwidth(15)\n        \n        self._fig.tight_layout()\n        \n        self._ax_field = plt.gca()\n        \n        self._ax_home = self._ax_field.twinx()\n        self._ax_away = self._ax_field.twinx()\n        self._ax_jersey = self._ax_field.twinx()\n\n        self.ani = animation.FuncAnimation(self._fig, self.update, frames=len(self._times), interval = self._mean_interval_ms, \n                                          init_func=self.setup_plot, blit=False)\n        \n        plt.close()\n       \n    @staticmethod\n    def set_axis_plots(ax, max_x, max_y) -> None:\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n\n        ax.set_xlim([0, max_x])\n        ax.set_ylim([0, max_y])\n        \n    @staticmethod\n    def convert_orientation(x):\n        return (-x + 90)%360\n    \n    @staticmethod\n    def polar_to_z(r, theta):\n        return r * np.exp( 1j * theta)\n    \n    @staticmethod\n    def deg_to_rad(deg):\n        return deg*np.pi\/180\n        \n    def data_stream(self):\n        for time in self._times:\n            yield self._frame_data[self._frame_data.time == time]\n    \n    def setup_plot(self): \n        self.set_axis_plots(self._ax_field, self._MAX_FIELD_X, self._MAX_FIELD_Y)\n        \n        ball_snap_df = self._frame_data[(self._frame_data.event == 'ball_snap') & (self._frame_data.team == 'football')]\n        self._ax_field.axvline(ball_snap_df.X_std.to_numpy()[0], color = 'yellow', linestyle = '--')\n        \n        self.set_axis_plots(self._ax_home, self._MAX_FIELD_X, self._MAX_FIELD_Y)\n        self.set_axis_plots(self._ax_away, self._MAX_FIELD_X, self._MAX_FIELD_Y)\n        self.set_axis_plots(self._ax_jersey, self._MAX_FIELD_X, self._MAX_FIELD_Y)\n        \n        for idx in range(10,120,10):\n            self._ax_field.axvline(idx, color = 'k', linestyle = '-', alpha = 0.05)\n            \n        self._scat_field = self._ax_field.scatter([], [], s = 200, color = 'red')\n        self._scat_home = self._ax_home.scatter([], [], s = 900, color = self._CPLT[0], edgecolors = 'k')\n        self._scat_away = self._ax_away.scatter([], [], s = 900, color = self._CPLT[1], edgecolors = 'k')\n        \n        self._scat_jersey_list = []\n        self._scat_number_list = []\n        self._scat_name_list = []\n        self._scat_mph_list = []\n        self._a_dir_list = []\n        self._a_or_list = []\n        self._a_tri_list = []\n        for _ in range(self._MAX_FIELD_PLAYERS):\n            self._scat_jersey_list.append(self._ax_jersey.text(0, 0, '', horizontalalignment = 'center', verticalalignment = 'center', c = 'black',fontweight='bold',fontsize='large',path_effects=[pe.withStroke(linewidth=3, foreground=\"white\")]))\n            self._scat_number_list.append(self._ax_jersey.text(0, 0, '', horizontalalignment = 'center', verticalalignment = 'center', c = 'white',fontweight='bold',fontsize=14,path_effects=[pe.withStroke(linewidth=5, foreground=\"dodgerblue\")]))\n            self._scat_name_list.append(self._ax_jersey.text(0, 0, '', horizontalalignment = 'center', verticalalignment = 'center', c = 'black',fontweight='bold',fontsize='larger',path_effects=[pe.withStroke(linewidth=5, foreground=\"gold\")]))\n            self._scat_mph_list.append(self._ax_jersey.text(0, 0, '', horizontalalignment = 'center', verticalalignment = 'center', c = 'lime',fontweight='bold',fontsize='larger'))\n\n            self._a_dir_list.append(self._ax_field.add_patch(Arrow(0, 0, 0, 0, color = 'k')))\n            self._a_or_list.append(self._ax_field.add_patch(Arrow(0, 0, 0, 0, color = 'k')))\n            self._a_tri_list.append(self._ax_field.add_patch(Arrow(0, 0, 0, 0, color = 'k')))\n            \n        return (self._scat_field, self._scat_home, self._scat_away,*self._scat_mph_list, *self._scat_jersey_list, *self._scat_number_list, *self._scat_name_list)\n        \n    def update(self, anim_frame):\n        pos_df = next(self._stream)\n        \n        for label in pos_df.team.unique():\n            label_data = pos_df[pos_df.team == label]\n\n            if label == 'football':\n                self._scat_field.set_offsets(np.hstack([label_data.X_std, label_data.Y_std]))\n            elif label == 'home':\n                self._scat_home.set_offsets(np.vstack([label_data.X_std, label_data.Y_std]).T)\n            elif label == 'away':\n                self._scat_away.set_offsets(np.vstack([label_data.X_std, label_data.Y_std]).T)\n\n        jersey_df = pos_df[pos_df.jerseyNumber.notnull()]\n        \n        for (index, row) in pos_df[pos_df.jerseyNumber.notnull()].reset_index().iterrows():\n            self._scat_jersey_list[index].set_position((row.X_std, row.Y_std))\n            self._scat_jersey_list[index].set_text(row.position)\n            if self.method == 'single':\n                try:\n                    self._scat_number_list[index].set_text(np.where(np.isin(row.nflId,self.player_id) == True,str(self.Text) +\" \"+ str(round(row[self.Show],2)),\"\"))\n                    self._scat_number_list[index].set_position((row.X_std, row.Y_std+2.4))\n                except:\n                    self._scat_number_list[index].set_text(np.where(np.isin(row.nflId,self.player_id) == True,str(self.Text) +\" \"+ str(row[self.Show]),\"\"))\n                    self._scat_number_list[index].set_position((row.X_std, row.Y_std+2.4))\n                    pass\n            else:\n                try:\n                    self._scat_number_list[index].set_text(str(round(row[self.Show],2)))\n                    self._scat_number_list[index].set_position((row.X_std, row.Y_std+2.4))\n                except:\n                    self._scat_number_list[index].set_text(row[self.Show])\n                    self._scat_number_list[index].set_position((row.X_std, row.Y_std+2.4))\n                    pass               \n\n            self._scat_name_list[index].set_text(np.where(row.frameId <= 10,row.displayName.split()[-1],\"\"))\n            self._scat_name_list[index].set_position((row.X_std, row.Y_std-1.9))\n            if self.MPH == True:\n                self._scat_mph_list[index].set_text(np.where((row.s \/ 0.488889) > 17,str(round(float(row.s \/ 0.488889),2)) + \" MPH\",\"\"))\n                self._scat_mph_list[index].set_position((row.X_std, row.Y_std+1.9))\n            else:\n                pass\n\n            player_vel = np.array([row.dx, row.dy])\n            player_orient = np.array([np.real(self.polar_to_z(3, row.Orientation_std)), np.imag(self.polar_to_z(3, row.Orientation_std))])\n            \n            self._a_dir_list[index].remove()\n            self._a_dir_list[index] = self._ax_field.add_patch(Arrow(row.X_std, row.Y_std, player_vel[0], player_vel[1], color = 'black'))\n            \n            self._a_or_list[index].remove()\n            self._a_or_list[index] = self._ax_field.add_patch(Arrow(row.X_std, row.Y_std, player_orient[0], player_orient[1], color = 'blue', width = 2))\n\n            if self.Tri == True:\n                if (self.method == 'single') & (np.isin(row.nflId,self.player_id) == True):\n                    self._a_tri_list[index].remove()\n                    self._a_tri_list[index] = self._ax_field.add_patch(Polygon([[row.X_std, row.Y_std], [row.X_std_COpp,row.Y_std_COpp],[row.X_std_QB,row.Y_std_QB]], closed=True, fill=False, hatch='\/',color='lime'))\n                else:\n      #              self._a_tri_list[index].remove()\n      #              self._a_tri_list[index] = self._ax_field.add_patch(Polygon([[row.X_std, row.Y_std], [row.X_std_COpp,row.Y_std_COpp],[row.X_std_QB,row.Y_std_QB]], closed=True, fill=False, hatch='\/',color='lime'))\n                    pass\n            else:\n                pass\n        \n        return (self._scat_field, self._scat_home, self._scat_away, *self._scat_jersey_list, *self._scat_number_list, *self._scat_name_list)","80ea2c61":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport glob\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\npd.set_option('max_columns', 1000)\n\nfrom sklearn.neighbors import BallTree\n\n#from BDBUtils.Utilities import CreateNFLData\nfrom IPython.core.display import HTML\nimport time\nimport math\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nfrom datetime import datetime\nimport pytz\nfrom IPython.display import HTML\nimport scipy.stats as stats\nimport matplotlib as mpl\nfrom matplotlib import animation, rc\nfrom matplotlib.patches import Rectangle, Arrow\nimport tensorflow as tf\n#from BDBUtils.Utilities import CreateNFLData\n\nimport glob\nimport os\n\nnp.set_printoptions(suppress=True)\n\nimport gc\n\ndef convert_orientation(x):\n    return (x)%360\n\ndef deg_to_rad(deg):\n        return deg*np.pi\/180\n\n\n\nCreate = CreateNFLData()\n\nstart = time.process_time()\n\nWeeks = range(1,18)\n\n#globbed_files = glob.glob(\"..\/input\/revised-data\/*.csv\") #creates a list of all csv files\ndata = []\nfor n in tqdm(Weeks):\n    filename = '..\/input\/revised-data\/week' + str(n) + '.csv'\n    frame = Create.import_data(filename,columns=True, cols=['week', 'gameId', 'playId', 'frameId', 'time', 'nflId', 'displayName', 'jerseyNumber', 'position', 'team', 'X_std', 'Y_std', 'Dir_std', 'dx', 'dy', 'Orientation_std', 's', 'MPH', 'a', 'dis', 'event','route', 'PlayerTeam','yardlineNumber', 'YardLine_std', 'OnOffense',  'closestOpp_Id', 'Opp_Dist_COpp','route_COpp', 'X_std_COpp', 'Y_std_COpp', 'Dir_std_COpp', 'dx_COpp', 'dy_COpp', 'Orientation_std_COpp', 'MPH_COpp', 's_COpp', 'a_COpp', 'dis_COpp', 'position_COpp', 'Pos_Rank_COpp','closestTeam_Id', 'Team_Dist_CTm', 'nflId_y_CTm', 'route_CTm', 'X_std_CTm', 'Y_std_CTm','QB_Dist_QB', 'X_std_QB', 'Y_std_QB', 'Orientation_std_QB', 'FootDist', 'Targeted'])\n    frame['Code'] = frame['gameId'].astype(str) + \"-\" + frame['playId'].astype(str)\n    plays = pd.read_csv('..\/input\/nfl-big-data-bowl-2021\/plays.csv', usecols=['gameId', 'playId','down', 'yardsToGo','penaltyCodes', 'penaltyJerseyNumbers', 'passResult', 'offensePlayResult', 'playResult', 'epa', 'isDefensivePI','offenseFormation',\t'personnelO',\t'defendersInTheBox',\t'personnelD',\t'typeDropback','playType'])\n    plays['Code'] = plays['gameId'].astype(str) + \"-\" + plays['playId'].astype(str)\n    plays = plays.set_index('Code')\n    plays = plays.loc[~plays.index.duplicated(keep='first')]\n    Cols = ['playId','down', 'yardsToGo','penaltyCodes', 'penaltyJerseyNumbers', 'passResult', 'offensePlayResult', 'playResult', 'epa', 'isDefensivePI','offenseFormation',\t'personnelO',\t'defendersInTheBox',\t'personnelD',\t'typeDropback','playType']\n    for col in Cols:\n        frame[col] = frame.Code.map(plays[col])\n    Obs = frame.select_dtypes(include=['object']).columns.to_list()\n    frame[Obs] = frame[Obs].astype('category')\n    data.append(frame)\n    del plays\n    gc.collect()\n    del frame\n    del Cols\n    del Obs\n    gc.collect()\n\nprint(\"finish\")\nWeekData = pd.concat(data).reset_index()\ndel data\ngc.collect()\n\n\nFinaldf1 = Create.FrameData(WeekData)\ndel WeekData\ngc.collect()\nFinaldf1.drop(['index','Code', 'ball_snap', 'man_in_motion', 'pass_arrived', 'pass_forward', 'pass_outcome_caught', 'play_action', 'run_pass_option'], axis=1, inplace=True)\nFinaldf1.memory_usage().sum() \/ (1024**2)\n\ngc.collect()\nFinaldf1.memory_usage().sum() \/ (1024**2)\n\nFinaldf1['QBslope'] = deg_to_rad(convert_orientation(np.rad2deg(np.arctan2(Finaldf1['Y_std_QB'] - Finaldf1['Y_std'], Finaldf1['X_std_QB'] - Finaldf1['X_std']))))\nFinaldf1['WRslope'] = deg_to_rad(convert_orientation(np.rad2deg(np.arctan2(Finaldf1['Y_std_COpp'] - Finaldf1['Y_std'],Finaldf1['X_std_COpp'] - Finaldf1['X_std']))))\n\nFinaldf1['QBslope1'] = convert_orientation(np.rad2deg(np.arctan2(Finaldf1['Y_std_QB'] - Finaldf1['Y_std'], Finaldf1['X_std_QB'] - Finaldf1['X_std'])))\nFinaldf1['WRslope1'] = convert_orientation(np.rad2deg(np.arctan2(Finaldf1['Y_std_COpp'] - Finaldf1['Y_std'],Finaldf1['X_std_COpp'] - Finaldf1['X_std'])))\nFinaldf1['Def_Or'] = convert_orientation(np.rad2deg(Finaldf1['Orientation_std']))\nFinaldf1['Diff_QB'] = Finaldf1['QBslope1'] - Finaldf1['Def_Or']\nFinaldf1['Diff_WR'] = Finaldf1['WRslope1'] - Finaldf1['Def_Or']\n\nFinaldf1['Diff_QB'] = abs(np.where(Finaldf1['Diff_QB'] < -180,Finaldf1['Diff_QB'] + 360,Finaldf1['Diff_QB'] ))\nFinaldf1['Diff_QB'] = abs(np.where(Finaldf1['Diff_QB'] > 180,Finaldf1['Diff_QB'] - 360,Finaldf1['Diff_QB'] ))\n\nFinaldf1['Diff_WR'] = abs(np.where(Finaldf1['Diff_WR'] < -180,Finaldf1['Diff_WR'] + 360,Finaldf1['Diff_WR'] ))\nFinaldf1['Diff_WR'] = abs(np.where(Finaldf1['Diff_WR'] > 180,Finaldf1['Diff_WR'] - 360,Finaldf1['Diff_WR'] ))\n\nFinaldf1['Player_POV'] = np.where(Finaldf1['Diff_QB'] < Finaldf1['Diff_WR'],\"QB\",Finaldf1['position_COpp'])\nFinaldf1['Looking_AtQB'] = np.where(Finaldf1['Diff_QB'] < Finaldf1['Diff_WR'],1,0)\n\nFinaldf1['diffDir'] = np.absolute(Finaldf1['Dir_std'] - Finaldf1['Dir_std_COpp'])\n\nFinaldf1['disRatio'] = Finaldf1['Opp_Dist_COpp'] \/ np.sqrt((Finaldf1['X_std_COpp'] - Finaldf1['X_std_CTm'])**2 + (Finaldf1['Y_std_COpp'] - Finaldf1['Y_std_CTm'])**2)\n\nFinaldf1['Event2'] = np.where(Finaldf1['After_snap'] == 0,\"Before Snap\",\"After Snap - Before Throw\")\nFinaldf1['Event2'] = np.where((Finaldf1['After_Throw'] == 1 & (Finaldf1['After_PassArrived'] == 0)),\"Ball in the Air\", Finaldf1['Event2'])\n\nFinaldf1['EventCount'] = Finaldf1.groupby(['gameId','playId','Event2'])['Event2'].transform('count') \/ Finaldf1.groupby(['gameId','playId'])['nflId'].transform('nunique')\nFinaldf1['EventOrder'] = Finaldf1.groupby(['gameId','playId','Event2'])['frameId'].rank(ascending=True, method='dense').astype(int)\nFinaldf1['EventPct'] = Finaldf1['EventOrder'] \/ Finaldf1['EventCount']\n\nFinaldf1['Partition'] = np.where(Finaldf1['EventPct'] > (1\/2), \"2nd Phase\", \"1st Phase\")\n\nFinaldf1['Group'] = Finaldf1['Event2'] + \"-\" + Finaldf1['Partition']\n\nFinaldf1['Player_POV'] = Finaldf1['Player_POV'].astype('category')\nFinaldf1['Event2'] = Finaldf1['Event2'].astype('category')\nFinaldf1['Partition'] = Finaldf1['Partition'].astype('category')\nFinaldf1['Group'] = Finaldf1['Group'].astype('category')","b20bb932":"Example = Finaldf1.query('displayName == \"Allen Hurns\" & gameId == 2018110500 & playId == 1918 & After_snap == 1').filter(['gameId','playId','frameId','nflId','displayName','a'], axis=1).head(30)\nExample['a'] = Example['a'].astype(np.float64)\n\ndef highlight_max(s):\n    is_max = s.isin(s.nlargest(n=2, keep='first'))\n    return ['background-color: yellow' if v else '' for v in is_max]","b9320b77":"Example['a_pct_change'] = Example.groupby(['gameId','playId','nflId']).apply(lambda x: x['a'].shift(-1).pct_change()).reset_index(0,drop=True).fillna(0).T\n\nExample.style.apply(highlight_max, subset=['a_pct_change'])","38edab2e":"%%time\nimport sys\nimport warnings\nfrom pandas.core.common import SettingWithCopyWarning\n\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Find outside receivers\n\nOutsideWR = Finaldf1.query('OnOffense == True & After_PassArrived == 0 & After_snap == 1 & After_Catch == 0 & EventOrder == 1 & Group == \"After Snap - Before Throw-1st Phase\"')#.groupby(['gameId','playId'])['Y_std'].agg(['min','max']).reset_index()\nOutsideWR['Min'] = OutsideWR.groupby(['gameId','playId'])['Y_std'].transform('min')\nOutsideWR['Max'] = OutsideWR.groupby(['gameId','playId'])['Y_std'].transform('max')\n\nOutsideWR = OutsideWR.groupby(['gameId','playId'])['Min','Max'].agg('first').reset_index()\n\nOutsideWR['Code'] = OutsideWR['gameId'].astype(str) + \"-\" + OutsideWR['playId'].astype(str)\nOutsideWR = OutsideWR.set_index('Code')\nOutsideWR = OutsideWR.loc[~OutsideWR.index.duplicated(keep='first')]\n\n#Find defender orientation - are they looking at the receiver?\n\n\nFinaldf1['WRslope1_Opp'] = convert_orientation(np.rad2deg(np.arctan2(Finaldf1['Y_std'] - Finaldf1['Y_std_COpp'],Finaldf1['X_std'] - Finaldf1['X_std_COpp'])))\nFinaldf1['Def_Or_Opp'] = convert_orientation(np.rad2deg(Finaldf1['Orientation_std_COpp']))\nFinaldf1['Diff_WR_Opp'] = Finaldf1['WRslope1_Opp'] - Finaldf1['Def_Or_Opp']\n\n\nFinaldf1['Diff_WR_Opp'] = abs(np.where(Finaldf1['Diff_WR_Opp'] < -180,Finaldf1['Diff_WR_Opp'] + 360,Finaldf1['Diff_WR_Opp'] ))\nFinaldf1['Diff_WR_Opp'] = abs(np.where(Finaldf1['Diff_WR_Opp'] > 180,Finaldf1['Diff_WR_Opp'] - 360,Finaldf1['Diff_WR_Opp'] ))\n\nfrom scipy.signal import argrelextrema\n\ndef get_maxima(x):\n    return x.iloc[argrelextrema(x['a_pct_change'].values,np.greater)]\n\ndef f(x):    \n    return x['a_pct_change'] + x['playId'] + x['nflId'] + x['gameId']\n#2018100706\n\ndf = Finaldf1.query('Targeted == 1 & position == \"WR\" & After_PassArrived == 0 & After_snap == 1 & After_Catch == 0').filter(['week','gameId','playId','frameId','time','event','nflId','displayName','route','EventOrder','Event2','closestOpp_Id','position_COpp','passResult','down','yardsToGo','offensePlayResult','isDefensivePI','epa','After_Throw','Distfrom_LOSY','a','a_COpp','Opp_Dist_COpp','X_std','Y_std','dx','dx_COpp','dy','dy_COpp','Dir_std','Dir_std_COpp','Diff_WR_Opp'], axis=1)\n\n\n\n\ndf['Code'] = df['gameId'].astype(str) + \"-\" + df['playId'].astype(str)\n\n# Find outside receivers\n\ndf['minY'] = df.Code.map(OutsideWR['Min'])\n\ndf['maxY'] = df.Code.map(OutsideWR['Max'])\n\n\n\n\ndel OutsideWR\ngc.collect()\n\n\n\n\ndf['a'] = df['a'].astype(np.float64)\ndf['a_COpp'] = df['a_COpp'].astype(np.float64)\ndf['X_std'] = df['X_std'].astype(np.float64)\ndf['Opp_Dist_COpp'] = df['Opp_Dist_COpp'].astype(np.float64)\n\n# Apply Acceleration pct change function\n\ndf['a_pct_change'] = df['a'].shift(-1).pct_change()\ndf['a_pct_change'] = df.groupby(['gameId','playId','nflId']).apply(lambda x: x['a'].shift(-1).pct_change()).reset_index(0,drop=True).reset_index(0,drop=True).reset_index(0,drop=True).fillna(0).T\n\ndf['a_COpp_pct_change'] = df.groupby(['gameId','playId','nflId']).apply(lambda x: x['a_COpp'].shift(-1).pct_change()).reset_index(0,drop=True).reset_index(0,drop=True).reset_index(0,drop=True).fillna(0).T\n\ndf['Opp_Dist_COpp_change'] = df.groupby(['gameId','playId','nflId']).apply(lambda x: x['Opp_Dist_COpp'].shift(-1).pct_change()).reset_index(0,drop=True).reset_index(0,drop=True).reset_index(0,drop=True).fillna(0).T\n\n# Cumulative sum of yards from line of scrimmage, will help us determine specific routes. i.e cut at 10 yds = 10 yd Hitch\n\ndf['XYards'] = df.groupby(['gameId','playId','nflId']).apply(lambda x: x['X_std'].diff(1).cumsum()).reset_index(0,drop=True).reset_index(0,drop=True).reset_index(0,drop=True).fillna(0).T\n\ndf['YYards'] = df.groupby(['gameId','playId','nflId']).apply(lambda x: x['Y_std'].diff(1).cumsum()).reset_index(0,drop=True).reset_index(0,drop=True).reset_index(0,drop=True).fillna(0).T\n\ndf['OutsideWR'] = np.where((df.groupby(['gameId','playId','nflId'])['Y_std'].transform('first') == df['maxY']) | (df.groupby(['gameId','playId','nflId'])['Y_std'].transform('first') == df['minY']), 1,0)\n\n# Is the receiver on the left or right side of the formation\n\ndf['Ballsnap_pos'] = np.where(df.groupby(['gameId','playId','nflId'])['Distfrom_LOSY'].transform('first') > 0,0,1) \n\n\n# Press or no press coverage\n\ndf['DefenderInitial'] = df.groupby(['gameId','playId','nflId'])['Opp_Dist_COpp'].transform('first')\n\ndf['PressCoverage'] = np.where(df['DefenderInitial'] <= 3, 1,0)\n\ndf['a_pct_change_max'] = df.groupby(['gameId','playId','nflId'])['a_pct_change'].transform('max')\ndf['a_pct_change_max'] = df.where((df['After_Throw'] == 0) & (df['EventOrder'] >= 5)).groupby(['gameId','playId','nflId'])['a_pct_change'].transform('max')\n\ndf = df.replace([np.inf, -np.inf], 0)\n\ndf['Code'] = df.apply(f, axis=1)\n\n# May seem confusing but we are trying to find the 1-2 maxima pct changes for each play. We have all of the pct changes in a list, however some plays share the same pct change #, so we can add pct change + playid + gameid + nflid to create unique values in the list\n# instead of using iterrows function, this method actually saves us a lot of time even though it is unorthodox\n\n\ndf['a_pct_change_max'] = df[df['Code'].where((df['After_Throw'] == 0) & (df['EventOrder'] >= 5) ).isin(df.where((df['After_Throw'] == 0) & (df['EventOrder'] >= 5) ).groupby(['gameId','playId','nflId']).apply(get_maxima).reset_index(0,drop=True).reset_index(0,drop=True).reset_index(0,drop=True).groupby(['gameId','playId','nflId'])['a_pct_change'].nlargest(2, keep='all').to_frame().reset_index().query('a_pct_change > .35').apply(f, axis=1).to_list())]\n\n# Find cut frames\n\ndf['after_receiver_cut'] = np.where(df['a_pct_change_max'].notnull(),1,0)\ndf['after_receiver_cut'] = df.groupby(['gameId','playId','nflId'])['after_receiver_cut'].cumsum()\n\ndf['a_pct_change_max'] = df.where((df['After_Throw'] == 0) & (df['EventOrder'] >= 5)).groupby(['gameId','playId','nflId','after_receiver_cut'])['a_pct_change'].transform('max')\ndf['a_pct_change_max'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['a_pct_change_max'].transform('max')\n\ndf['receiver_cutframe'] = np.where(df['a_pct_change'] == df['a_pct_change_max'] ,df['frameId'],0)\ndf['receiver_cutframe'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['receiver_cutframe'].transform('max')\n\ndf['a_COpp_pct_change_max'] = df.where((df['EventOrder'] >= df['receiver_cutframe'])).groupby(['gameId','playId','nflId','after_receiver_cut'])['a_COpp_pct_change'].transform('max')\ndf['a_COpp_pct_change_max'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['a_COpp_pct_change_max'].transform('max')\n\n\ndf['Def_cutframe'] = np.where(df['a_COpp_pct_change'] == df['a_COpp_pct_change_max'] ,df['frameId'],0)\ndf['Def_cutframe'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['Def_cutframe'].transform('max')\n\n\n\n\n\ndf = df.replace([np.inf, -np.inf], 0)\n\n# Only keep data on\/after receiver is made their first cut\n\ndf = df.query('after_receiver_cut >= 1')\n\n\ndf['a_pct_change_max'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['a_pct_change'].transform('max')\ndf['a_COpp_pct_change_max'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['a_COpp_pct_change'].transform('max')\n\ndf['receiver_cutframe'] = np.where(df['a_pct_change'] == df['a_pct_change_max'] ,df['frameId'],0)\ndf['receiver_cutframe'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['receiver_cutframe'].transform('max')\n\ndf['Def_cutframe'] = np.where(df['a_COpp_pct_change'] == df['a_COpp_pct_change_max'] ,df['frameId'],0)\ndf['Def_cutframe'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['Def_cutframe'].transform('max')\n\ndf = df.replace([np.inf, -np.inf], 0)\n\n\ndf['EventOrder'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['frameId'].rank(ascending=True, method='dense').astype(int)\n\ndf = df.query('EventOrder <= 25')\n\ndf['a_diff'] =  df['a'] - df['a_COpp']\ndf['dx_diff'] =  df['dx'] - df['dx_COpp']\ndf['a_pct_change_diff'] =  df['a_pct_change'] - df['a_COpp_pct_change']\ndf['CutFrame_diff'] =  df['receiver_cutframe'] - df['Def_cutframe']\n\ndf['Route2'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['YYards'].transform('last')\ndf['Route3'] = np.where((df['Ballsnap_pos'] == 0) & (df['Route2'] < 0), 1,2)\ndf['Route3'] = np.where((df['Ballsnap_pos'] == 1) & (df['Route2'] < 0), 2, df['Route3'])\n\ndf['Route3'] = np.where((df['after_receiver_cut'] == 2), np.nan, df['Route3'])\ndf['Route3'] = df.groupby(['gameId','playId','nflId'])['Route3'].transform('max')\n\ndf['Route3'] = np.where(df['Route3'] == 1, \"IN\", \"OUT\")\n\ndf['Route2'] = df['Route3'] + \"+\" + df['route']\n\n\nCorr = df.groupby(['gameId','playId','nflId','after_receiver_cut'])[['a','a_COpp']].corr().reset_index()\nCorr = Corr.filter(['gameId','playId','nflId','after_receiver_cut','a'], axis=1).query('a < 1')\n\nCorr['Code'] = Corr['gameId'].astype(str) + \"-\" + Corr['playId'].astype(str) + \"-\" + Corr['nflId'].astype(str) + \"-\" + Corr['after_receiver_cut'].astype(str)\nCorr = Corr.set_index('Code')\nCorr = Corr.loc[~Corr.index.duplicated(keep='first')]\n\ndf['Code'] = df['gameId'].astype(str) + \"-\" + df['playId'].astype(str) + \"-\" + df['nflId'].astype(str) + \"-\" + df['after_receiver_cut'].astype(str)\ndf['a_corr'] = df.Code.map(Corr['a'])\n\nCorr = df.groupby(['gameId','playId','nflId','after_receiver_cut'])[['dx','dx_COpp']].corr().reset_index()\nCorr = Corr.filter(['gameId','playId','nflId','after_receiver_cut','dx'], axis=1).query('dx < 1')\n\nCorr['Code'] = Corr['gameId'].astype(str) + \"-\" + Corr['playId'].astype(str) + \"-\" + Corr['nflId'].astype(str) + \"-\" + Corr['after_receiver_cut'].astype(str)\nCorr = Corr.set_index('Code')\nCorr = Corr.loc[~Corr.index.duplicated(keep='first')]\n\ndf['dx_corr'] = df.Code.map(Corr['dx'])\n\nCorr = df.groupby(['gameId','playId','nflId','after_receiver_cut'])[['dy','dy_COpp']].corr().reset_index()\nCorr = Corr.filter(['gameId','playId','nflId','after_receiver_cut','dy'], axis=1).query('dy < 1')\n\nCorr['Code'] = Corr['gameId'].astype(str) + \"-\" + Corr['playId'].astype(str) + \"-\" + Corr['nflId'].astype(str) + \"-\" + Corr['after_receiver_cut'].astype(str)\nCorr = Corr.set_index('Code')\nCorr = Corr.loc[~Corr.index.duplicated(keep='first')]\n\ndf['dy_corr'] = df.Code.map(Corr['dy'])\n\ndf['a_diff'] =  df['a'] - df['a_COpp']\ndf['dx_diff'] =  df['dx'] - df['dx_COpp']\ndf['dy_diff'] =  df['dy'] - df['dy_COpp']\ndf['a_pct_change_diff'] =  df['a_pct_change'] - df['a_COpp_pct_change']\ndf['CutFrame_diff'] =  df['receiver_cutframe'] - df['Def_cutframe']\n\n\ndf['Opp_Dist_COpp_max'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['Opp_Dist_COpp'].transform('max')\n\ndf['Opp_Dist_COpp_maxframe'] = np.where(df['Opp_Dist_COpp'] == df['Opp_Dist_COpp_max'] ,df['EventOrder'],0)\ndf['Opp_Dist_COpp_maxframe'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['Opp_Dist_COpp_maxframe'].transform('max')\n\ndf['Diff_Ori_Opp_max_1sec'] = df.where((df['EventOrder'] <= 10)).groupby(['gameId','playId','nflId','after_receiver_cut'])['Diff_WR_Opp'].transform('max')\ndf['Diff_Ori_Opp_mean_1sec'] = df.where((df['EventOrder'] <= 10)).groupby(['gameId','playId','nflId','after_receiver_cut'])['Diff_WR_Opp'].transform('mean')\ndf['Diff_Ori_Opp_var_1sec'] = df.where((df['EventOrder'] <= 10)).groupby(['gameId','playId','nflId','after_receiver_cut'])['Diff_WR_Opp'].transform('var')\n\ndf['Diff_Ori_Opp_max_1sec'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['Diff_Ori_Opp_max_1sec'].transform('max')\ndf['Diff_Ori_Opp_mean_1sec'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['Diff_Ori_Opp_mean_1sec'].transform('max')\ndf['Diff_Ori_Opp_var_1sec'] = df.groupby(['gameId','playId','nflId','after_receiver_cut'])['Diff_Ori_Opp_var_1sec'].transform('max')\n\n\n\ndf['Diff_WR_Opp_max_1sec'] = (df['Diff_Ori_Opp_max_1sec']**df.groupby(['gameId','playId','nflId','after_receiver_cut'])['Opp_Dist_COpp'].transform('mean')) \/ 1000\ndf['Diff_WR_Opp_mean_1sec'] = (df['Diff_Ori_Opp_mean_1sec']**df.groupby(['gameId','playId','nflId','after_receiver_cut'])['Opp_Dist_COpp'].transform('mean'))  \/ 1000\ndf['Diff_WR_Opp_var_1sec'] = (df['Diff_Ori_Opp_mean_1sec']**df.groupby(['gameId','playId','nflId','after_receiver_cut'])['Opp_Dist_COpp'].transform('mean')) \/ 1000\n\n\n\ndf = df.replace([np.inf, -np.inf], 0)","ba17f036":"df['Type'] = np.where(df.groupby(['week','gameId','playId','nflId'])['after_receiver_cut'].transform(\"max\") > 1,2,1)\nTry = df.query('Type == 2 & after_receiver_cut == 2 | Type == 1 & after_receiver_cut == 1 ')\n\nMAAE = Try.groupby(['week','gameId','playId','nflId','displayName','route','Route2','Ballsnap_pos','closestOpp_Id','position_COpp','passResult','down','yardsToGo','offensePlayResult','isDefensivePI','epa','after_receiver_cut', 'Type','OutsideWR','PressCoverage']).agg({'EventOrder':[('max','max')],\n                                                'XYards':[('mean', 'mean')],\n                                                'Opp_Dist_COpp':[('first', 'first'),('last', 'last'), ('mean', 'mean'), ('count', 'count'),('max','max')],\n                                                'Opp_Dist_COpp_change':[('var', 'var'), ('mean', 'mean'), ('min', 'min'),('max','max')],\n                                                'a_pct_change':[('max', 'max'),('var', 'var'),('mean', 'mean'),('first', 'first'),('min', 'min'),('last', 'last')],\n                                                'dx_COpp':[('max', 'max'),('var', 'var'),('mean', 'mean'),('first', 'first'),('min', 'min'),('last', 'last')],\n                                                'a_COpp':[('max', 'max'),('var', 'var'),('mean', 'mean'),('first', 'first'),('min', 'min'),('last', 'last')],\n                                                'Diff_WR_Opp':[('max', 'max'),('var', 'var'),('mean', 'mean'),('first', 'first'),('min', 'min'),('last', 'last')],\n                                                'a_diff':[('first', 'first'),('mean', 'mean'),('max','max'),('var','var'),('min', 'min'),('last', 'last')],\n                                                'dx_diff':[('first', 'first'),('mean', 'mean'),('max','max'),('var','var'),('min', 'min'),('last', 'last')],\n                                                'dy_diff':[('first', 'first'),('mean', 'mean'),('max','max'),('var','var'),('min', 'min'),('last', 'last')],\n                                                'a_pct_change_diff':[('first', 'first'),('mean', 'mean'),('max','max'),('var','var'),('min', 'min'),('last', 'last')],\n                                                'Opp_Dist_COpp_maxframe':[('mean', 'mean')],\n                                                'CutFrame_diff':[('mean', 'mean')],\n                                                'Diff_WR_Opp_max_1sec':[('max', 'max')],\n                                                'Diff_WR_Opp_mean_1sec':[('max', 'max')],\n                                                'Diff_WR_Opp_var_1sec':[('max', 'max')],\n                                                }).reset_index(drop=False)\nMAAE.columns = MAAE.columns.map('_'.join)\n\nMAAE['Total'] = MAAE.groupby(['playId_','nflId_','route_'])['EventOrder_max'].transform('sum')\nMAAE['Cov_pct'] = MAAE.groupby(['playId_','nflId_','route_'])['Opp_Dist_COpp_count'].transform('sum') \/ MAAE['Total']\nMAAE['DefenderSuccess'] = np.where(MAAE['passResult_'] == \"C\",0,1)\n\nRoutes = ['SLANT','OUT','HITCH','GO','IN']\n\nMAAE = MAAE.query('Opp_Dist_COpp_first < 10 & Cov_pct > .70 & EventOrder_max > 10 & isDefensivePI_ == False')\nMAAE = MAAE.query('position_COpp_ == \"CB\" | position_COpp_ == \"DB\"  ')\nMAAE = MAAE.query('route_.isin(@Routes)', engine='python')\n\n\nMAAE['Opp_Dist_COpp_diff'] = MAAE['Opp_Dist_COpp_last'] - MAAE['Opp_Dist_COpp_first']\n\nMAAE['XYards_mean'] = 5 * round(MAAE['XYards_mean']\/5).astype(int)\n\n\nMAAE['Route2_'] = np.where((MAAE['Route2_'] == \"IN+GO\") | (MAAE['Route2_'] == \"OUT+GO\"),MAAE['Route2_'], MAAE['route_'] )\n\n\nMAAE = MAAE.query('XYards_mean != 0 &  XYards_mean <= 20')\n\nMAAE['XYards_mean'] = np.where(MAAE['XYards_mean'] > 15, 15,MAAE['XYards_mean'])\n\n\nMAAE['Opp_Dist_COpp_maxframe_mean'] = MAAE['Opp_Dist_COpp_maxframe_mean'] \/ MAAE['Opp_Dist_COpp_count']\n\nMAAE['Route2_'] = np.where((MAAE['Route2_'] == \"HITCH\") | (MAAE['Route2_'] == \"IN\") | (MAAE['Route2_'] == \"OUT\"),MAAE['XYards_mean'].astype(str) + \"yd+\" + MAAE['Route2_'], MAAE['Route2_'] )\n\n\nMAAE['route_'] = np.where((MAAE['Route2_'] == \"IN+GO\") | (MAAE['Route2_'] == \"OUT+GO\"),MAAE['Route2_'], MAAE['route_'] )","fcc134f0":"from sklearn.metrics import accuracy_score,f1_score\nfrom IPython.core.display import HTML\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom catboost import CatBoostClassifier\nfrom scipy.stats import randint, uniform\nfrom sklearn.model_selection import StratifiedKFold,GridSearchCV, RandomizedSearchCV,cross_val_score\nfrom sklearn.metrics import average_precision_score,recall_score,make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport sklearn.model_selection as GridSearchCV\n\nfrom sklearn.model_selection import cross_val_predict,cross_val_score\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import svm\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomTreesEmbedding\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\n\nfrom tqdm import tqdm\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomTreesEmbedding\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom lightgbm import LGBMClassifier\n\n\ndef Predictions(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n    clfs1 = []\n\n    clfs1.append((\"DecisionTree\",\n                Pipeline([(\"DecisionTree\", DecisionTreeClassifier())])))  \n\n    clfs1.append((\"RandomForestClassifier\",\n                Pipeline([(\"RandomForestClassifier\", RandomForestClassifier(n_estimators = 1000,random_state = 123,max_depth = 9,criterion = \"gini\"))]))) \n\n \n    clfs1.append((\"ExtraTreesClassifier\",\n                Pipeline([(\"ExtraTreesClassifier\", ExtraTreesClassifier())])))      \n\n    clfs1.append((\"AdaBoostClassifier\",\n                Pipeline([(\"AdaBoostClassifier\", AdaBoostClassifier())])))                             \n\n    clfs1.append((\"GradientBoostingClassifier\",\n                Pipeline([ (\"GradientBoostingClassifier\", GradientBoostingClassifier())]))) \n\n    clfs1.append((\"RidgeClassifier\",\n                        Pipeline([(\"RidgeClassifier\", RidgeClassifier())]))) \n\n    clfs1.append((\"BaggingClassifier\",\n                        Pipeline([(\"RidgeClassifier\", BaggingClassifier())])))\n\n    clfs1.append((\"CatboostClassifier\",\n                        Pipeline([(\"CatboostClassifier\", CatBoostClassifier(logging_level='Silent'))]))) \n\n    clfs1.append((\"LGBMClassifier\",\n                        Pipeline([(\"LGBMClassifier\", LGBMClassifier())])))\n\n\n    n_folds = 5\n    seed = 42\n    ModelName = []\n    TrainAcc = []\n    TestAcc = []\n    TrainF1 = []\n    TestF1 = []\n    features = []\n    for name, model  in clfs1:\n        try:\n                kfold = KFold(n_splits=n_folds, random_state=seed)\n           #     print(name)\n                ModelName.append(name)\n                OG = cross_val_predict(model, X_train, y_train, cv=kfold, n_jobs=-1)\n                TrainAcc.append(accuracy_score(y_train, OG))     \n                model = model\n                model.fit(X_train,y_train)\n                YOne = model.predict(X_test)\n                TestAcc.append(accuracy_score(y_test, YOne))   \n                TrainF1.append(f1_score(y_train, OG))\n                TestF1.append(f1_score(y_test, YOne))\n                try:\n                    fea_imp = pd.DataFrame({'imp': model.steps[0][1].feature_importances_, 'col': X.columns})\n                    fea_imp[['imp']] = MinMaxScaler().fit_transform(fea_imp[['imp']])\n                    fea_imp = fea_imp.sort_values(['imp','col'], ascending=[False,False]).reset_index()\n             #       display(HTML(fea_imp.iloc[:5].to_html()))\n                    features.append(fea_imp)\n                except:\n                        continue\n        except:\n                continue\n\n\n\n\n\n    df = pd.DataFrame(list(zip(ModelName,TrainAcc,TestAcc,TrainF1,TestF1)), columns=['ModelName','TrainAcc','TestAcc','TrainF1','TestF1'])\n    feats = pd.concat(features)\n  #  display(HTML(df.sort_values(by=['TestF1'], ascending=False).head(20).to_html()))\n    return df, feats","92f06e9f":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nimport warnings\n\n\n\n\nROUTES = ['5yd+OUT', '15yd+HITCH', '5yd+IN', '10yd+HITCH', '15yd+OUT', '10yd+OUT', '5yd+HITCH', '15yd+IN', 'SLANT', 'OUT+GO', 'IN+GO', '10yd+IN']\n\ndfs = []\n\ndata=[]\nfeat_imp = []\n\n\nfor route in ROUTES:\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        Group = MAAE.loc[(MAAE['Route2_'] == route)]\n\n        Cols = [ 'OutsideWR_', 'PressCoverage_', 'Opp_Dist_COpp_last', 'Opp_Dist_COpp_mean', 'Opp_Dist_COpp_max', 'Opp_Dist_COpp_change_var', 'Opp_Dist_COpp_change_mean', 'Opp_Dist_COpp_change_min', 'Opp_Dist_COpp_change_max','dx_COpp_max', 'dx_COpp_var', 'dx_COpp_mean', 'dx_COpp_min', 'dx_COpp_last', 'a_COpp_max', 'a_COpp_var', 'a_COpp_mean', 'a_COpp_min', 'a_COpp_last', 'Diff_WR_Opp_max', 'Diff_WR_Opp_var', 'Diff_WR_Opp_mean', 'Diff_WR_Opp_min', 'Diff_WR_Opp_last', 'a_diff_mean', 'a_diff_max', 'a_diff_var', 'a_diff_min', 'a_diff_last', 'dx_diff_mean', 'dx_diff_max', 'dx_diff_var', 'dx_diff_min', 'dx_diff_last',  'dy_diff_mean', 'dy_diff_max', 'dy_diff_var', 'dy_diff_min', 'dy_diff_last', 'a_pct_change_diff_mean', 'a_pct_change_diff_max', 'a_pct_change_diff_var', 'a_pct_change_diff_min', 'a_pct_change_diff_last', 'Opp_Dist_COpp_maxframe_mean', 'CutFrame_diff_mean', 'Opp_Dist_COpp_diff','Diff_WR_Opp_max_1sec_max', 'Diff_WR_Opp_mean_1sec_max', 'Diff_WR_Opp_var_1sec_max']    \n        X = Group[Cols]\n        y = Group['DefenderSuccess']\n\n        X[Cols] = SimpleImputer().fit_transform(X[Cols])\n        X[Cols] = StandardScaler().fit_transform(X[Cols])\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42, stratify=y)\n\n        df, feats = Predictions(X,y)\n\n        model = LogisticRegression()\n        solvers = ['newton-cg', 'lbfgs', 'liblinear']\n        penalty = ['l2']\n        c_values = [100, 10, 1.0, 0.1, 0.01]\n        # define grid search\n        grid = dict(solver=solvers,penalty=penalty,C=c_values)\n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=65)\n        grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n        grid_clf_acc = grid_search.fit(X, y)\n\n\n        #Predict values based on new parameters\n        y_pred_acc = grid_clf_acc.predict(X_test)\n        y_pred_acctrain = grid_clf_acc.predict(X_train)\n        \n        feats5 = feats.groupby(['col'])['imp'].mean().reset_index().sort_values(by=['imp'], ascending=False).head(5)['col'].to_list()\n        \n        print('-----------------------------------------------')\n        print('Route: ', route)\n        print('Model Parameters: ', grid_clf_acc.best_params_)\n        print('Test Accuracy Score : ' + str(accuracy_score(y_test,y_pred_acc)))\n        print('Test F1 Score : ' + str(f1_score(y_test,y_pred_acc)))\n        print('Top 5 Feature Importance : ', feats5)\n\n\n        Group['Grade'] = grid_clf_acc.predict_proba(X)[:,1]\n        Group[['Grade']] = MinMaxScaler().fit_transform(Group[['Grade']])\n        Group['Grade'] = Group['Grade']*100\n\n        feats['route'] = route\n        data.append(df)\n        feat_imp.append(feats)\n\n\n        dfs.append(Group)\n\n    \nFinal = pd.concat(dfs)","f5ff164e":"ex = Final.query('displayName_ == \"Stefon Diggs\" & gameId_ == 2018100709 & playId_ == 3046 ').sort_values(by=['Grade'], ascending=True).filter(['gameId_','playId_','displayName_','route_','Route2_','epa_','Opp_Dist_COpp_first', 'Opp_Dist_COpp_last', 'dy_diff_max','Diff_WR_Opp_max', 'Grade'])\nex.columns = ['gameId','playId','Name','route','route2','epa','Initial_Separation', 'SeparationAfter', 'Velocitydy_diff_max','DefenderOrientation_Max', 'Grade']\nex","6ea79aaf":"CBs = Finaldf1.filter(['displayName','nflId','position','Targeted','playId','epa'], axis=1).query('position == \"CB\" & Targeted == 1 | position == \"DB\" & Targeted == 1| position == \"FS\" & Targeted == 1')\nCBs = CBs.groupby(['displayName','nflId','position','playId'])['epa'].mean().reset_index()\nCBs = CBs.groupby(['displayName','nflId','position'])['epa'].mean().reset_index()\nCBs = CBs.set_index('nflId')\nCBs = CBs.loc[~CBs.index.duplicated(keep='first')]\n","b7387bab":"import seaborn as sns\n# for basic mathematics operation \nimport numpy as np\nimport pandas as pd\nfrom pandas import plotting\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# for interactive visualizations\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nimport plotly.figure_factory as ff\n\nfrom sklearn.cluster import KMeans\n\n#var1 = 'Press_Success'\nvar1 = 'Grade'\nvar2 = 'epa_'\n\nMAAE1 = Final.groupby(['closestOpp_Id_']).agg({var1:[('mean', 'mean'),('count','count')],\n                                                         var2:[('mean', 'mean')],}).reset_index(drop=False)\nMAAE1.columns = MAAE1.columns.map('_'.join)\n\nMAAE1.columns = MAAE1.columns.str.replace(var2 +'_mean' , var2)\nMAAE1.columns = MAAE1.columns.str.replace(var1 + '_mean' , 'mean')\nMAAE1.columns = MAAE1.columns.str.replace(var1 + '_count' , 'count')\nMAAE1.columns = MAAE1.columns.str.replace('closestOpp_Id__' , 'closestOpp_Id_')\n\n#MAAE1[['mean']] = MinMaxScaler().fit_transform(MAAE1[['mean']])\nMAAE1[['mean']] = MinMaxScaler().fit_transform(MAAE1[['mean']])*100\nMAAE1['displayName'] = MAAE1.closestOpp_Id_.map(CBs['displayName'])\n#MAAE1['epa'] = MAAE1.closestOpp_Id_.map(CBs['epa'])\nMAAE1 = MAAE1.sort_values(by=['mean'], ascending=False).query('count > 20')\n\nprint(\"Total players with more than 20 targeted snaps on breaking routes: \", len(MAAE1))\nprint(\"-------------------------------------------------------------------------------\")\nprint(\"Top 10 Players\")\n\n\nCols = ['displayName', 'mean', 'count', 'epa_']\n\nMAAE = MAAE1[Cols]\n\nMAAE.columns = ['Name', 'Avg. Grade', 'Cut Routes', 'Avg. EPA']\n\n\ndisplay(HTML(MAAE.head(10).to_html()))\n\nfig, ax = plt.subplots(figsize=(15,10))\nslope, intercept, r_value, pv, se = stats.linregress(MAAE1['mean'], MAAE1[var2])\n\nprint(\"Correlation:\", r_value)\n\nsns.regplot(MAAE1['mean'], MAAE1[var2], line_kws={'label':'$y=%3.7s*x+%3.7s$'%(slope, intercept)})\nplt.xticks(fontsize=15)\nplt.xlabel(\"Average \" + var1)\nplt.legend()\nplt.yticks(fontsize=15)\nplt.ylabel(\"Average \" + \"EPA\")","5778b481":"import seaborn as sns\n\n#var1 = 'Press_Success'\nvar1 = 'Grade'\nvar2 = 'epa_'\n\nFinal['RouteType'] = np.where((Final['route_'] != \"IN+GO\") & (Final['route_'] != \"OUT+GO\"), \"One_Cut\", \"Double_Move\")\n\nMAAE1 = Final.groupby(['closestOpp_Id_','RouteType']).agg({var1:[('mean', 'mean'),('count','count')],\n                                                         var2:[('mean', 'mean')],}).reset_index(drop=False)\nMAAE1.columns = MAAE1.columns.map('_'.join)\n\nMAAE1.columns = MAAE1.columns.str.replace(var2 +'_mean' , var2)\nMAAE1.columns = MAAE1.columns.str.replace(var1 + '_mean' , 'mean')\nMAAE1.columns = MAAE1.columns.str.replace(var1 + '_count' , 'count')\nMAAE1.columns = MAAE1.columns.str.replace('closestOpp_Id__' , 'closestOpp_Id_')\n\n#MAAE1[['mean']] = MinMaxScaler().fit_transform(MAAE1[['mean']])\n#MAAE1[['mean']] = MinMaxScaler().fit_transform(MAAE1[['mean']])*100\nMAAE1['displayName'] = MAAE1.closestOpp_Id_.map(CBs['displayName'])\n#MAAE1['epa'] = MAAE1.closestOpp_Id_.map(CBs['epa'])\n#MAAE1 = MAAE1.sort_values(by=['mean'], ascending=False).query('count >= 20')\n\n\nMAAE1 = MAAE1.pivot_table(index=['displayName'], columns='RouteType_', values=['mean','count'],aggfunc=[np.mean], fill_value=0, margins=True).reset_index()#.sort_values(by=['All'], ascending=False)\nMAAE1.columns = MAAE1.columns.map('_'.join)\nMAAE1.columns = MAAE1.columns.str.replace('mean_mean_' , 'mean_')\nMAAE1.columns = MAAE1.columns.str.replace('mean_count_' , 'count_')\n\n#MAAE1['count'] = MAAE1.iloc[:,-3:-1].apply(lambda s: (s > 0).sum(), axis=1)\n#MAAE1 = MAAE1.sort_values(by=['All'], ascending=False).query('count > 6')\n\nMAAE1 = MAAE1.sort_values(by=['mean_All'], ascending=False).query('count_Double_Move >= 4 & count_One_Cut >= 15')\nMAAE1['One_Cut_rank'] = MAAE1['mean_One_Cut'].rank(ascending=False)\nMAAE1['DoubleMove_rank'] = MAAE1['mean_Double_Move'].rank(ascending=False)\nMAAE1['Avg_Rank'] = MAAE1.iloc[:,-2:].mean(axis = 1, skipna = True)\nMAAE1['WeightedMean'] = MAAE1['mean_Double_Move']*(MAAE1['count_Double_Move']\/(MAAE1['count_Double_Move']+MAAE1['count_One_Cut'])) + MAAE1['mean_One_Cut']*(MAAE1['count_One_Cut']\/(MAAE1['count_Double_Move']+MAAE1['count_One_Cut']))\nMAAE1['RankDiffMean'] = np.abs(MAAE1['One_Cut_rank'] - MAAE1['DoubleMove_rank'])\nMAAE1.drop(['count_Double_Move','count_One_Cut','count_All'], axis=1, inplace=True)\n\nprint(\"Total players with more than 15 targeted snaps on breaking routes & 4 or more double move routes: \", len(MAAE1))\n\nCols = ['displayName__','mean_One_Cut', 'One_Cut_rank',  'mean_Double_Move','DoubleMove_rank',  'WeightedMean']\n\n\nMAAE = MAAE1[Cols]\n\n\nMAAE.columns = ['Name','Avg. One-Cut Grade', 'One-Cut Rank',  'Avg. Double-Move Grade','Double-Move Rank','Weighted Grade']\n\ndisplay(HTML(MAAE.head(10).sort_values(by=['Weighted Grade'], ascending=False).to_html()))\n\nfig, ax = plt.subplots(figsize=(15,10))\nslope, intercept, r_value, pv, se = stats.linregress(MAAE1['One_Cut_rank'], MAAE1['DoubleMove_rank'])\n\nprint(\"Correlation:\", r_value)\n\nsns.regplot(MAAE1['One_Cut_rank'], MAAE1['DoubleMove_rank'], line_kws={'label':'$y=%3.7s*x+%3.7s$'%(slope, intercept)})\nplt.xticks(fontsize=15)\nplt.xlabel('One-Cut Rank')\nplt.legend()\nplt.yticks(fontsize=15)\nplt.ylabel('Double-Move Rank')\n","debe2581":"#x = CBalign.drop(['Defender'], axis=1).values\n\nx = MAAE1[['One_Cut_rank', 'DoubleMove_rank','RankDiffMean']].values\nkm = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 1000, n_init = 10, random_state = 0)\nkm.fit(x)\nlabels = km.labels_\ncentroids = km.cluster_centers_\n\nMAAE1['labels'] =  labels\ntrace1 = go.Scatter3d(surfacecolor='darkgrey',\n    x= MAAE1['One_Cut_rank'],\n    y= MAAE1['DoubleMove_rank'],\n    z= MAAE1['WeightedMean'],\n    mode='markers',text=MAAE1['displayName__'],\n     marker=dict(\n        color = MAAE1['labels'], \n        colorscale='Plasma',\n        size= 5,\n        line=dict(\n            color= MAAE1['labels'],\n            width= 10\n        ),\n        opacity=0.8\n     )\n)\ndf = [trace1]\n\nlayout = go.Layout(\n    title = 'Corner Types',\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0  \n    ),\n    scene = dict(\n            xaxis = dict(title  = 'Elite on One Cuts -->'),\n            yaxis = dict(title  = '<-- Elite on Double Moves'),\n            zaxis = dict(title  = 'Overall Score')\n        )\n)\n\nfig = go.Figure(data = df, layout = layout)\n\nfig.update_layout(\n    autosize=False,\n    width=800,\n    height=500)\npy.iplot(fig)","279a6f2d":"MAAE1.query('labels == 0').filter(['displayName__','mean_One_Cut','One_Cut_rank','mean_Double_Move','DoubleMove_rank','WeightedMean']).sort_values(by=['WeightedMean'], ascending=False)","1632a9b8":"MAAE1.query('labels == 2').filter(['displayName__','mean_One_Cut','One_Cut_rank','mean_Double_Move','DoubleMove_rank','WeightedMean']).sort_values(by=['WeightedMean'], ascending=False)","9f5b5107":"MAAE1.query('labels == 4').filter(['displayName__','mean_One_Cut','One_Cut_rank','mean_Double_Move','DoubleMove_rank','WeightedMean']).sort_values(by=['WeightedMean'], ascending=False)","01d3de11":"MAAE1.query('labels == 1').filter(['displayName__','mean_One_Cut','One_Cut_rank','mean_Double_Move','DoubleMove_rank','WeightedMean']).sort_values(by=['WeightedMean'], ascending=False)","01e9a2a8":"MAAE1.query('labels == 3').filter(['displayName__','mean_One_Cut','One_Cut_rank','mean_Double_Move','DoubleMove_rank','WeightedMean']).sort_values(by=['WeightedMean'], ascending=False)","a7c19a2b":"ex = Final.query('displayName_ == \"Allen Hurns\" & gameId_ == 2018110500 & playId_ == 1918 ').sort_values(by=['Grade'], ascending=True).filter(['gameId_','playId_','route_','Route2_','epa_','Grade'])\nex.columns = ['gameId','playId','route','route2','epa', 'Grade']\nex","3f586215":"ex = Final.query('displayName_ == \"Odell Beckham\" & gameId_ == 2018100701 & playId_ == 2460 ').sort_values(by=['Grade'], ascending=True).filter(['gameId_','playId_','route_','Route2_','epa_','Grade'])\nex.columns = ['gameId','playId','route','route2','epa', 'Grade']\nex","7c876dc4":"ex = Final.query('displayName_ == \"Keenan Allen\" & gameId_ == 2018110408 & playId_ == 2245 ').sort_values(by=['Grade'], ascending=True).filter(['gameId_','playId_','route_','Route2_','epa_','Grade'])\nex.columns = ['gameId','playId','route','route2','epa', 'Grade']\nex","877347ea":"ex = Final.query('displayName_ == \"Albert Wilson\" & gameId_ == 2018101404 & playId_ == 2046 ').sort_values(by=['Grade'], ascending=True).filter(['gameId_','playId_','route_','Route2_','epa_','Grade'])\nex.columns = ['gameId','playId','route','route2','epa', 'Grade']\nex","5460c48f":"ex = Final.query('displayName_ == \"Amari Cooper\" & gameId_ == 2018093009 & playId_ == 5050 ').sort_values(by=['Grade'], ascending=True).filter(['gameId_','playId_','route_','Route2_','epa_','Grade'])\nex.columns = ['gameId','playId','route','route2','epa', 'Grade']\nex","a9f49ef7":"There is a strong **negative correlation of -.53** between this grading system and EPA, thus confirming that there is potentially a relationship between a defender who can stay with a receiver after the cut and fewer expected points added.","152ff684":"## Cluster     <span style=\"color:#e377c2;font-size:36px;\">          PINK       <\/span> - Gamblers\n\nThese players will perform well on any breaking route thrown to them, however, their knack for jumping routes will pay the price when a talented receiver runs a crisp double move on them for a big play.","07cd1baf":"# Improving Route Detail For Analysis\n\nNow that we know when the receivers make their cut, we can create a more in-depth view of the route by taking the cumulative sum difference of the receiver's X position from the line of scrimmage and rounding the cut yardage in increments of 5. Here are the new routes for our analysis:\n\n\n**Single Cuts:**\n- Slant\n- 5yd + OUT\n- 10yd + OUT\n- 15yd + OUT\n- 5yd + IN\n- 10yd + IN\n- 15yd + IN\n- 5yd + HITCH\n- 10yd + HITCH\n- 15yd + HITCH\n\nOn plays where the receiver makes 2 cuts and heads up field, we can use alignment info and the cumulative sum of the Y coordinate difference from the line of scrimmage to find out which direction the receiver moved to make their first cut either IN or OUT:\n\n**Double Moves:**\n- IN + GO - This will contain Slant n' Go's (Sluggos)\n- OUT + GO - This will contain Out n' up's\/ Hitch n' Go's","2ae37915":"Some may argue that the most critical aspect of man-to-man coverage relies on the cornerback\u2019s ability to stay with the receiver after he makes his cut. The modern-day NFL route tree has evolved to a point where nearly every route involves some sort of cut, or multiple cuts.\n\nWe want to quantify how well the defender can stay with the receiver after he makes his initial cut on one-cut routes, and second cut on double-moves.\n\n**Single-Cut Routes:** Hitches, Slants, Outs, Digs (Ins), and Comebacks\n\n**Double-Move Routes:** Out n\u2019 up's, Slant n' Go's (Sluggos), Hitch n\u2019 Go's\n","d027f05d":"We can rank the defender among their peers for One-Cuts and Double Moves, and we will check the correlation.","c46b8a67":"# Model Output\n- Route\n- Logistic Regression grid search parameters\n- Model accuracy predicting Defender Success\n- F1 score of the model to ensure model validity\n- Avg. Top 5 Feature importance from Tree\/Boosting Models","b320ef6c":"![](https:\/\/media.giphy.com\/media\/JtCRRORLG0zzUeA1lq\/giphy.gif)","23cc04e0":"## Cluster     <span style=\"color:#9467bd;font-size:36px;\">          PURPLE       <\/span> - Consistently Average\n\nThese players should not be feared by opponents, they are middle of the pack in both categories.","bd96c457":"# One-Cut Grade vs. Double-Move Grade","347a7ebb":"## Cluster     <span style=\"color:#ff7f0e;font-size:36px;\">          ORANGE       <\/span> - Liabilities\n\nThis group has difficulty staying with a receiver on any single-cut route, as well as any double move. Teams should be actively targeting these players","5494ec84":"# Model Approach\n\n## Features:\n\n- Outside WR - 1 = Yes \/ 0 = No\n- Press Coverage  - 1 = Yes \/ 0 = No\n- Separation - Mean, Max, Var, Last Frame\n- Separation Percent Change- Mean, Max, Min, Var\n- Defender Velocity(dx) relative to X-axis - Mean, Max, Min, Var, Last Frame\n- Defender Acceleration - Mean, Max, Min, Var, Last Frame\n- Defender Orientation Angle Diff from receiver location - Mean, Max, Min, Var, Last Frame\n- Defender \/ Receiver Acceleration Difference - Mean, Max, Min, Var, Last Frame\n- Defender \/ Receiver Velocity(dx) relative to X-axis Difference - Mean, Max, Min, Var, Last Frame\n- Defender \/ Receiver Velocity(dy) relative to Y-axis Difference - Mean, Max, Min, Var, Last Frame\n- Defender \/ Receiver Acceleration Percent Change Difference - Mean, Max, Min, Var, Last Frame\n- Max Separation Frame\n- Cut Frame Difference - Difference between Defender and receiver cut frames\n- Difference between separation at start of cut and 2.5 seconds after\n\n\n\n## Model\n\nI will use a similar approach to my other [submission](https:\/\/www.kaggle.com\/jdruzzi\/quantifying-press-coverage-ability). Except this time we will use data for only 2.5 seconds after the receiver makes their first cut on single cut routes, and use their 2nd cut on double move routes. \n\nInstead of throwing all routes into the same model, we will loop through and generate a model for each route, append the score predictions to the corresponding play and then concatenate the dataset back together.\n\n**Logic:** Not all routes can be evaluated the same way. For example, difference in velocity relative to the X-axis may be a non-factor when evaluating coverage on an out route, but it may be a crucial feature when evaluating a hitch\/ comeback. \nAnother reason we train each route individually is because this scoring metric is an inverse of completion probability model, so air yards and other factors may affect the scoring system. By modeling each route individually, this allows us to isolate just the receiver and cornerback interaction.\n\n\n\n\nUsing logistic regression, we will predict Defender Success where completion = 0, and incomplete\/interception = 1\n\nWe can use the predicted probability of defender success, normalize it, and then multiply by 100 to create a scoring function.\n\nDefender Grade = MinMaxScalar(P(Defender Success)) x 100\n\n1-100 with 100 being the most ideal grade.","5fe5f7dc":"----------------------------------------------------","1d4ce156":"## Cluster     <span style=\"color:blue;font-size:36px;\">          BLUE       <\/span> - Best All-Around\n\nThese players are very rare, they are disciplined enough to stay with the receiver on double moves, as well as, perform the best in the league when breaking on single cut routes.","2e51670a":"# Identify the Cut\n\nAbove you will see Allen Hurns run a double move called a \"Sluggo\" route, in which his initial cut led Malcolm Butler to believe he's running a slant, only to make a second cut up field blowing right by him for an easy touchdown.\n\nThis is a perfect example to show how we can pin-point the exact frames Allen Hurns made both of his cuts. We can do this by analyzing Hurns's acceleration percent change frame-by-frame.\n\nBelow you will see Hurns's acceleration (\"a\") frame-by-frame, using a pct_change function and lagging by -1, the 2 largest percent increases in the acceleration help us identify when Hurns made each cut.","a377ee9e":"--------------------------------------------------------","88289d41":"![](https:\/\/media.giphy.com\/media\/t2o10PsQUtAUCmFeLq\/giphy.gif)","6a93586a":"We actually see a slight **negative correlation of -.13** between One-Cut Rank and Double-Move Rank!\n\n### How is this possible?\n\nSome defenders could be more lax in coverage to prevent the double move from occurring, thus giving them a bad one-cut rank and a good double-move ranking. Another group of defenders will be overly aggressive and try to jump on every one-cut route, leading them to be burned on double-moves. We can identify each group with the use of clustering.","4da83522":"![](https:\/\/media.giphy.com\/media\/wdIRaIIikhq7LHRV9a\/giphy.gif)","ae03c755":"------------------------------------------","1a34e589":"------------------------------------------------","56be7445":"-------------------------------------------------------------","95d9ffcf":"# Bonus Content","c2477d12":"-------------------------------------------------------------------------------------","283d6ab6":"Notice how the defender maintains close distance with the receiver up until he makes his cut. We were able to show:\n- The magnitude of the separation change\n    - The difference from when the receiver makes his cut, to 2.5  seconds after the cut\n- The max difference in sideline Velocity(dy)\n    - You can see the massive 5 pt difference in velocity moving from sideline to sideline. Diggs pushes towards the sideline while Darby is still moving in the opposite direction.\n- Max Defender Orientation\n    - Darby gets completely turned around on the route, we can capture this by taking the mean, max, and variance of the defender's orientation relative to the receiver's position on the field.\n- A final grade of 12.43 for Ronald Darby, indicating bad coverage after the receiver cut\n\n\n## Where Most Metrics Fail\n\nMetrics like **average EPA** and **average Separation** are popular when evaluating cornerback talent, but they fail to tell the full story. \n\n- EPA cannot account for bad coverages on errant throws, drops, and non-targeted receivers\n\n- EPA cannot account for good coverages on plays where there's good coverage but a miraculous catch, and non-targeted receivers\n\n- Average Separation is too vague and could potentially be skewed. The cornerback could be inches away from the receiver for the entirety of the route, only to be left in the dust in the final seconds when the receiver makes his break.\n\nSo what makes this grading system so important, is that we evaluate only the most crucial aspect of the route, and that is the cut.","a1506c7a":"![](https:\/\/media.giphy.com\/media\/2UtOjJh635SBmLMou3\/giphy.gif)","af182af3":"----------------------------------------------------------------------------------","c60094c5":"# Clustering Analysis","77cf9ad6":"-----------------------------------------------------------","67431333":"------------------------------------------------------------","72bf01db":"![](https:\/\/media.giphy.com\/media\/f4naKrzrdy6cNNV6SE\/giphy.gif)","76fc6404":"## Cluster     <span style=\"color:#bcbd22;font-size:36px;\">          YELLOW       <\/span> - Conservatives\n\nThese players will refuse to let up big plays against them by maintaining strict discipline on double moves, however, their conservative play allows teams to dink and dunk down the field on easy one-cut routes.","9e079e88":"![](https:\/\/media.giphy.com\/media\/wdIRaIIikhq7LHRV9a\/giphy.gif)","cd499f61":"# Defender Evaluation: One-Cut Routes + Double Moves","002dd2c7":"# Let's Visualize\n","ee0682a6":"![](https:\/\/media.giphy.com\/media\/r48xl3h1AzQufggLMP\/giphy.gif)","d5394387":"# Final Thoughts:\n\nUsing this grading system, we can evaluate who is best defending one-cut routes, as well as double moves. What is also unique about this grading system, is that it can also be used to evaluate cornerbacks that were not targeted on the play because we are only observing the receiver \/ cornerback interaction, with no knowledge of the pass outcome. The clustering analysis shows that some defenders have certain tendencies when defending these types of routes, NFL coaches can exploit these tendencies when creating their game plan for the week. This submission also demonstrated a way to create detailed routes by analyzing the receiver's acceleration change frame-by-frame, and the grading model can be flipped to quantify receiver route running ability for scouts and offensive coaches.\n\n\n## Other Work:\n\n1. [Shadow Cornerback + Coverage Analysis](https:\/\/www.kaggle.com\/jdruzzi\/shadow-cornerback-coverage-analysis)\n\n2. [Defender Bite Velocity on Play-Action](https:\/\/www.kaggle.com\/jdruzzi\/defender-bite-velocity-on-play-action)\n\n3. [Pass Coverage Classification](https:\/\/www.kaggle.com\/jdruzzi\/pass-coverage-classification-80-recall)\n\n4. [Quantifying Press Coverage](https:\/\/www.kaggle.com\/jdruzzi\/quantifying-press-coverage-ability)\n\n5. [Defender Tendencies: One-Cut Routes + Double Moves](https:\/\/www.kaggle.com\/jdruzzi\/defender-tendencies-one-cut-routes-double-moves)\n\n## Data:\n\n[Revised BDB Data](https:\/\/www.kaggle.com\/jdruzzi\/revised-bdb-data)","2fd304c7":"-------------------------","c3353349":"Let's look at Stefon Diggs working Ronald Darby on this 10 yard out. I will highlight a few features to better help visualize this grading system.","264ea532":"# Targeted Coverage Score After Cut vs. Targeted EPA","d5787371":"-----------------------------------------------------------"}}