{"cell_type":{"c1d9a18b":"code","0e6e5b2e":"code","20a7762b":"code","9be217f1":"code","67c743e1":"code","15cdbbf6":"code","0d852f0d":"code","7953c020":"code","8ed089d2":"code","e2dfa09f":"code","1b2bc53b":"code","29d91df2":"code","9fdd2517":"code","678d44a1":"code","a94db04d":"code","906ffe8e":"code","823fb6a8":"code","dc96ecb3":"code","b47d9bc0":"code","852a3aa1":"code","ff5d3047":"code","053e4640":"code","346e0787":"code","45393614":"code","b11d192a":"code","e2d7fea4":"code","8c1c9601":"code","f7fb3581":"code","c8ec1934":"code","f5c62725":"code","da1ad83b":"code","284b3298":"code","023d66a4":"markdown","f34a128e":"markdown","91687f49":"markdown","7cbe5bd4":"markdown","11858ff9":"markdown","f7537b7d":"markdown","26c213f3":"markdown","39b578a5":"markdown","afddea3c":"markdown","a825a1b5":"markdown","dd648d4b":"markdown","6ea46e9c":"markdown","52bc9eeb":"markdown","cf265cf5":"markdown","cb8565f1":"markdown","fa56647a":"markdown","43568642":"markdown","f5e108f7":"markdown","bb3de8ee":"markdown","fca11ee9":"markdown","95d1b48a":"markdown"},"source":{"c1d9a18b":"!pip install -q torchsummary\n\n# from google.colab import files\n# import os\n\n# print(\"Upload your kaggle.json file containing your API keys\")\n# if not os.path.exists(r\"\/content\/fer2018.zip\"):\n#     uploaded = files.upload()\n#     for fn in uploaded.keys():\n#         print('User uploaded file \"{name}\" with length {length} bytes'.format(\n#             name=fn, length=len(uploaded[fn])))\n\n#     !mkdir ~\/.kaggle\n#     !cp kaggle.json ~\/.kaggle\/\n#     !chmod 600 \/root\/.kaggle\/kaggle.json\n#     !kaggle datasets download -d ashishpatel26\/fer2018\n#     !unzip -qq  fer2018.zip -d datasets\/","0e6e5b2e":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nfrom torchsummary import summary\n\nfrom PIL import Image\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision.utils import make_grid\n\nsns.set_style('whitegrid')\nplt.style.use(\"fivethirtyeight\")\npd.set_option('display.max_columns', 20)\n\n\n%matplotlib inline","20a7762b":"dataset = pd.read_csv('..\/input\/fer2018\/fer20131.csv')\ndataset.info()","9be217f1":"# dataset.head()","67c743e1":"dataset.Usage.value_counts()","15cdbbf6":"# extracting pixel data from pixel column\n# convert it to integer\n# drop original pixel column\n# add all pixels as individual column\n\npixels = []\n\nfor pix in dataset.pixels:\n    values = [int(i) for i in pix.split()]\n    pixels.append(values)\n\npixels = np.array(pixels)\n\n# rescaling pixel values\npixels = pixels\/255.0\n\n\ndataset.drop(columns=['pixels'], axis=1, inplace=True)\n\npix_cols = [] # for keeping track of column names\n\n# add each pixel value as a column\nfor i in range(pixels.shape[1]):\n    name = f'pixel_{i}'\n    pix_cols.append(name)\n    dataset[name] = pixels[:, i]","0d852f0d":"dataset.head()","7953c020":"emotions = {\n    0: 'Angry', \n    1: 'Disgust', \n    2: 'Fear', \n    3: 'Happy', \n    4: 'Sad', \n    5: 'Surprise', \n    6: 'Neutral'\n}","8ed089d2":"class FERDataset(Dataset):\n    '''\n        Parse raw data to form a Dataset of (X, y).\n    '''\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n        self.tensor_transform = T.ToTensor()\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_id = int(row['emotion'])\n        img = np.copy(row[pix_cols].values.reshape(48, 48))\n        img.setflags(write=True)\n\n        if self.transform:\n            img = Image.fromarray(img)\n            img = self.transform(img)\n        else:\n            img = self.tensor_transform(img)\n\n        return img, img_id","e2dfa09f":"plt.figure(figsize=(9, 8))\nsns.countplot(x=dataset.emotion)\n_ = plt.title('Emotion Distribution')\n_ = plt.xticks(ticks=range(0, 7), labels=[emotions[i] for i in range(0, 7)], )","1b2bc53b":"def image_transformations() -> (object, object):\n    '''\n        Return transformations to be applied.\n        Input:\n            None\n        Output:\n            train_tfms: transformations to be applied on the training set\n            valid_tfms: transformations to be applied on the validation or test set\n    '''\n\n    train_trans = [      \n        T.RandomCrop(48, padding=4, padding_mode='reflect'),     \n        T.RandomRotation(15),\n        T.RandomAffine(\n            degrees=0,\n            translate=(0.01, 0.12),\n            shear=(0.01, 0.03),\n        ),\n        T.RandomHorizontalFlip(),\n        T.ToTensor(),\n    ]\n\n    val_trans = [\n        T.ToTensor(), \n    ]\n\n    train_transformations = T.Compose(train_trans)\n    valid_tfms = T.Compose(val_trans)\n\n    return train_transformations, valid_tfms","29d91df2":"def get_train_dataset(dataframe: object, transformation: bool=True) -> (object, object):\n    '''\n        Returns an object on FERDataset class\n        Input:\n            dataframe: object -> DataFrame object containing the whole data\n            transformation: bool [optional] ->  Apply transformations\n    '''\n\n    # extracts rows specific to Training, PublicTest\n    dataframe = dataframe.loc[dataframe.Usage.isin(['Training', 'PublicTest'])]\n    # drop Usage column as it's no longer needed    \n    dataframe = dataframe.drop('Usage', axis=1)\n\n    # split dataset into training and validation set\n    np.random.seed(42)  \n    msk = np.random.rand(len(dataframe)) < 0.8\n\n    train_df = dataframe[msk].reset_index()\n    val_df = dataframe[~msk].reset_index()\n\n    # get transformations\n    if transformation:\n        train_tfms, valid_tfms = image_transformations()\n    else:\n        train_tfms, valid_tfms = None, None\n\n    # fetch dataset\n    train_ds = FERDataset(dataframe, transform=train_tfms)\n    val_ds = FERDataset(dataframe, transform=valid_tfms)\n    return train_ds, val_ds","9fdd2517":"def get_train_dataloader(dataframe: object, transformation=True, batch_size: int=64) -> (object, object):\n    '''\n        Returns train and test dataloaders.\n        Input:\n            dataframe: dataset DataFrame object\n            batch_size: [optional] int\n        Output:\n            train_dl: train dataloader object\n            valid_dl: validation dataloader object\n    '''\n    # fetech train and validation dataset\n    train_ds, valid_ds = get_train_dataset(dataframe, transformation=transformation)\n    \n    train_dl = DataLoader(train_ds, batch_size, shuffle=True, \n                     num_workers=3, pin_memory=True)\n    valid_dl = DataLoader(valid_ds, batch_size*2, \n                    num_workers=2, pin_memory=True)\n    \n    return train_dl, valid_dl","678d44a1":"def get_test_dataloader(dataframe: object, batch_size: int=128) -> object:\n    '''\n        Returns test set dataloaders.\n        Input:\n            dataframe: dataset DataFrame object\n            batch_size: [optional] int\n        Output:\n            test_dl: test dataloader object\n    '''\n    # extracts rows specific to PrivateTest\n    test_df = dataframe.loc[dataset.Usage.isin(['PrivateTest'])]\n\n    # drop Usage column as it's no longer needed\n    test_df = test_df.drop('Usage', axis=1)\n\n    # get transformations same as validation set\n    _, valid_tfms = image_transformations()\n    \n    test_dataset = FERDataset(test_df, transform=valid_tfms)\n    test_dl = DataLoader(test_dataset, batch_size, num_workers=3 , pin_memory=True)\n\n    # move loader to GPU (class defined ahead)\n    test_dl = DeviceDataLoader(test_dl, device)\n    return test_dl","a94db04d":"train_dl_un, _ = get_train_dataloader(dataset, transformation=False)\ntrain_dl, _ = get_train_dataloader(dataset)","906ffe8e":"for images, _ in train_dl_un:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16, 8))\n    plt.axis(\"off\")\n    plt.imshow(make_grid(images, nrow=8).permute((1, 2, 0))) # move the channel dimension\n    break\n\n_ = plt.suptitle(\"Images\", y=0.92, fontsize=16)","823fb6a8":"for images, _ in train_dl:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16, 8))\n    plt.axis(\"off\")\n    plt.imshow(make_grid(images, nrow=8).permute((1, 2, 0))) # move the channel dimension\n    break\n\n_ = plt.suptitle(\"Transformed Images\", y=0.92, fontsize=16)","dc96ecb3":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","b47d9bc0":"device = get_default_device()\ndevice","852a3aa1":"# Can be used for any Image Classification task\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        inputs, labels = batch\n        outputs = self(inputs)\n        loss = F.cross_entropy(outputs, labels)\n        acc = accuracy(outputs, labels)\n        return {'loss': loss, 'acc': acc.detach()}\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc.detach()}\n        \n    def get_metrics_epoch_end(self, outputs, validation=True):\n        if validation:\n            loss_ = 'val_loss'\n            acc_ = 'val_acc'\n        else:\n            loss_ = 'loss'\n            acc_ = 'acc'\n\n        batch_losses = [x[f'{loss_}'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   \n        batch_accs = [x[f'{acc_}'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      \n        return {f'{loss_}': epoch_loss.detach().item(), f'{acc_}': epoch_acc.detach().item()}\n    \n    def epoch_end(self, epoch, result, num_epochs):\n        print(f\"Epoch: {epoch+1}\/{num_epochs} -> lr: {result['lrs'][-1]:.5f} loss: {result['loss']:.4f}, acc: {result['acc']:.4f}, val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f}\\n\")","ff5d3047":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","053e4640":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [\n              nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n              nn.BatchNorm2d(out_channels),\n              nn.ReLU(inplace=True)\n    ]\n\n    if pool:\n        layers.append(nn.MaxPool2d(kernel_size=2))\n    \n    return nn.Sequential(*layers)","346e0787":"# updated channels for the use case\n# added and additional layer in classifier\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.conv1 = conv_block(in_channels, 16, pool=False) # 16 x 48 x 48\n        self.conv2 = conv_block(16, 32, pool=True) # 32 x 24 x 24\n        self.res1 = nn.Sequential( #  32 x 24 x 24\n            conv_block(32, 32, pool=False), \n            conv_block(32, 32, pool=False)\n        )\n\n        self.conv3 = conv_block(32, 64, pool=True) # 64 x 12 x 12\n        self.conv4 = conv_block(64, 128, pool=True) # 128 x 6 x 6\n\n        self.res2 = nn.Sequential( # 128 x 6 x 6\n             conv_block(128, 128), \n             conv_block(128, 128)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2), # 128 x 3 x 3\n            nn.Flatten(),\n            nn.Linear(128*3*3, 512), #512\n            nn.Linear(512, num_classes) # 7\n        )\n        self.network = nn.Sequential(\n            self.conv1,\n            self.conv2,\n            self.res1,\n            self.conv3,\n            self.conv4,\n            self.res2,\n            self.classifier,\n        )\n\n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out       \n\n    def __repr__(self):\n        return f\"{self.network}\"\n    \n    def __str__(self):\n        summary(self.network, (1, 48, 48)) ","45393614":"class EmotionRecognition(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2, 2), # output: 16 x 24 x 24\n\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2, 2), # output: 64 x 12 x 12\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2), # output: 128 x 6 x 6\n\n            nn.Flatten(), \n            nn.Linear(128*6*6, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 7))\n        \n    def forward(self, xb):\n        return self.network(xb)\n\n    def __repr__(self):\n        return f\"{self.network}\"\n    \n    def __str__(self):\n        summary(self.network, (1, 48, 48))","b11d192a":"@torch.no_grad()\ndef evaluate(model: object, val_loader: object) -> dict:\n    '''\n        Evaluate model on the validation set\n        Input:\n            model: training model object\n            val_loder: validation data loader object\n        Output:\n            validation metrics\n    '''\n\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.get_metrics_epoch_end(outputs=outputs, validation=True)\n\n\ndef get_lr(optimizer: object) -> float:\n    ''' Returns current learning rate'''\n\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\n\ndef fit_model(model_name: str,\n              model: object, \n              epochs: int, \n              max_lr: float, \n              train_loader: object, \n              val_loader: object,\n              weight_decay: float=0, \n              grad_clip: float=None, \n              opt_func: object=torch.optim.SGD):\n    '''\n        This function is responsible for training our model.\n        We use a One Cycle learning rate policy to update our learning rate \n        with each epoch.\n        The best model is saved during each epoch.\n        Input:\n            model_name: str \n            model: object\n            epochs: int -> Max epochs\n            max_lr: float -> Maximum allowed learning rate during learning\n            train_loader: training set data loader\n            val_loader: validation set data loader\n            weight_decay: float -> value to decrease weights during training of each batch\n            grad_clip: float -> maximum allowed gradient value\n            opt_func: optimzer object\n        Output:\n            history: list of metrics\n    '''\n\n    torch.cuda.empty_cache()\n    BEST_VAL_SCORE = 0.0 # for keeping track of best model score\n    history = []\n\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=max_lr,\n                                                    epochs=epochs, \n                                                    steps_per_epoch=len(train_loader))\n\n    for epoch in range(epochs):\n        train_history = []\n        lrs = []\n\n        # Training Phase \n        model.train()\n        for batch in tqdm(train_loader, ascii=True, desc=f'Epoch: {epoch+1}\/{epochs}'):\n            info = model.training_step(batch)\n            loss = info['loss']\n            # contains batch loss and acc for training phase\n            train_history.append(info)\n            loss.backward()\n\n            # Gradient clipping\n            if grad_clip:\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            lrs.append(get_lr(optimizer))\n            scheduler.step()\n\n\n        train_result = model.get_metrics_epoch_end(train_history, validation=False)\n        val_result = evaluate(model, val_loader)\n        result = {**train_result, **val_result}\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result, epochs)\n\n        # Save the best model\n        if result['val_acc'] > BEST_VAL_SCORE:\n            BEST_VAL_SCORE = result['val_acc']\n            save_name = f\"{model_name}_epoch-{epoch+1}_score-{round(result['val_acc'], 4)}.pth\"\n            !rm -f '{model_name}'_*\n            torch.save(model.state_dict(), save_name)\n\n        history.append(result)\n    return history","e2d7fea4":"# functions to fetch test dataset and generate submission file for best model\n\ndef load_best(model_name: str) -> object:\n    '''Returns the best model'''\n\n    # get model defintion\n    best_model = models[model_name]\n\n    # load trained weights\n    path = r\".\/\"\n    file_path = ''\n    \n    for i in os.listdir(path):\n        if os.path.isfile(os.path.join(path,i)) and i.startswith(f'{model_name}'):\n            file_path = os.path.join(path, i)\n            \n    print(f\"Loaded model: {file_path[2:]} weights.\")\n    best_model.load_state_dict(torch.load(file_path))\n\n    # move model to gpu\n    best_model = to_device(best_model, device)\n    return best_model   \n\n\n@torch.no_grad()\ndef generate_prediction(model_name: str) -> None:\n    '''Generate prediction on the test set'''\n\n    # load test dataset\n    test_dl = get_test_dataloader(dataset)\n    \n    # load model\n    model = load_best(model_name)\n\n    # clear cuda cache\n    torch.cuda.empty_cache()\n\n    # generate prediction using the validation step method defined in Base class\n    with torch.no_grad():\n        model.eval()\n        outputs = [model.validation_step(batch) for batch in test_dl]\n        metrics = model.get_metrics_epoch_end(outputs=outputs, validation=True)\n\n    print(f\"Test Scores:\\n Loss: {round(metrics['val_loss'], 3)}, Accuracy: {round(metrics['val_acc'], 3)}\")","8c1c9601":"def end_to_end(model_name: str, parameters: dict=None) -> dict:\n    '''\n        A simple function end-to-end training and testing on the selected model.\n        Inputs:\n            model_name: str -> chosen model name\n            parameters: dict -> dictionary of hyperparameters for the model\n        Outputs:\n            history: dict -> dictionary containing model metrics(loss, score, lr)\n\n    '''\n    torch.cuda.empty_cache()\n\n    # hyperparameters\n    BATCH_SIZE = 512 # batch_sizes[model_name]\n    epochs = parameters[\"epochs\"]\n    max_lr = parameters[\"max_lr\"]\n    weight_decay = parameters[\"weight_decay\"]\n    grad_clip = parameters[\"grad_clip\"]\n    opt_func = parameters[\"opt_func\"]\n\n    # get transformed dataset\n    train_dl, valid_dl = get_train_dataloader(dataset, batch_size=BATCH_SIZE)\n    # move dataset to use GPU\n    train_dl = DeviceDataLoader(train_dl, device)\n    valid_dl = DeviceDataLoader(valid_dl, device)\n\n    # get model\n    model = models[model_name]\n\n    # move model to GPU\n    model = to_device(model, device)\n    \n    # train model\n    history = fit_model(\n                model_name,\n                model, \n                epochs, \n                max_lr, \n                train_dl, \n                valid_dl,\n                weight_decay, \n                grad_clip, \n                opt_func\n            )\n\n    # cleaning\n    torch.cuda.empty_cache()\n\n    # generate predictions\n    print(\"Genearating predictions on the Test set\")\n    generate_prediction(model_name)\n    return history","f7fb3581":"# plotting metrics\n\ndef plot_accuracies(history):\n    train_acc = [r['acc'] for r in history]\n    val_acc = [r['val_acc'] for r in history]\n    plt.plot(train_acc, '-kx', label=\"train_acc\")\n    plt.plot(val_acc, '-rx', label=\"val_acc\")\n    plt.legend()\n    _ = plt.xticks(ticks=range(len(train_acc)), \n                   labels=[str(i) for i in range(1, len(train_acc)+1)])\n    plt.xlabel('epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy vs. epochs')\n\ndef plot_losses(history):\n    train_losses = [r['loss'] for r in history]\n    val_losses = [r['val_loss'] for r in history]\n    plt.plot(train_losses, '-kx', label=\"train_loss\")\n    plt.plot(val_losses, '-rx', label=\"val_loss\")\n    plt.legend()\n    _ = plt.xticks(ticks=range(len(train_losses)), \n                   labels=[str(i) for i in range(1, len(train_losses)+1)])\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss vs. epochs')\n\ndef plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","c8ec1934":"models = {\n    'ResNet9': ResNet9(in_channels=1, num_classes=7),\n    'EmotionRecognition': EmotionRecognition(),\n}","f5c62725":"# TRAINING CONSTANTS\n\ntraining_parameters = {\n    \"epochs\": 2,\n    \"max_lr\": 0.01,\n    \"weight_decay\": 0.1,\n    \"grad_clip\": 1e-4,\n    \"opt_func\": torch.optim.Adam,\n}","da1ad83b":"model_name = \"ResNet9\"\n# model_name = \"EmotionRecognition\"\n\nhistory = end_to_end(model_name, training_parameters)","284b3298":"# plotting score and loss\n\nplt.figure(figsize=(18, 6))\nplt.subplot(1, 3, 1)\nplot_accuracies(history)\nplt.subplot(1, 3, 2)\nplot_losses(history)\n\nplt.subplot(1, 3, 3)\nplot_lrs(history)","023d66a4":"# Setting up GPU usage","f34a128e":"# Download Dataset\n\n* For running on Google Colab","91687f49":"## Helper Functions","7cbe5bd4":"## Models","11858ff9":"* We're going to use the `Training` and `PublicTest` rows combined together for training and validation set split into  80-20 proportion\n* `PrivateTest` will be our final test dataset.","f7537b7d":"# Dataset Preparation","26c213f3":"# Setup Training","39b578a5":"# Visualization","afddea3c":"## Model: From scratch","a825a1b5":"# Train Model","dd648d4b":"# Data Augmentations\n\n* We're going to apply various augmentation techniques.\n* All available transformations are listed in : [pytorch transforms](https:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html)\n","6ea46e9c":"# Dataset Class","52bc9eeb":"## Metric","cf265cf5":"# Training plots","cb8565f1":"# Facial Expression Recognition\n\n* The data consists of 48x48 pixel grayscale images of faces. \n* The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. \n* The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories:\n0. `Angry`\n1. `Disgust` \n2. `Fear` \n3. `Happy`\n4. `Sad`\n5. `Surprise`,\n6. `Neutral`","fa56647a":"# Dataset and Dataloader","43568642":"## Model: ResNet-9 ","f5e108f7":"## Base Image Classification Class","bb3de8ee":"# Data Imbalance\n\n* To deal with class Imbalance we can try different image transformations","fca11ee9":"# Imports","95d1b48a":"# Model Building"}}