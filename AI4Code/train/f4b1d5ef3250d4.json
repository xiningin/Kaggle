{"cell_type":{"71981f36":"code","fbc98537":"code","020508b7":"code","cd2c1c80":"code","0880101b":"code","bb7f09d1":"code","dd024518":"code","ece86e1a":"code","b98cfa98":"code","d728387d":"code","e22ff1fc":"code","06b2a1a9":"code","4d47d68e":"code","f04fbefa":"code","48e70d6b":"code","0a6c827d":"code","b4415ddf":"code","66bc0dac":"code","08ed5a90":"code","62498714":"code","030f008f":"code","21191f8d":"markdown","ef1dee54":"markdown","074012c7":"markdown","ff2cb815":"markdown","524179f4":"markdown","ad55733f":"markdown","ad448b3b":"markdown","9f5e4be6":"markdown","99531958":"markdown"},"source":{"71981f36":"kernel_mode = True\n\nimport sys\nif kernel_mode:\n    sys.path.insert(0, \"..\/input\/iterative-stratification\")\n    sys.path.insert(0, \"..\/input\/pytorch-lightning\")\n    sys.path.insert(0, \"..\/input\/gen-efficientnet-pytorch\")\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\nimport math\nimport pickle\nfrom pickle import dump, load\nimport glob\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib.cm import get_cmap\nfrom matplotlib import rcParams\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder, MinMaxScaler, RobustScaler\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.manifold import TSNE\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nimport torch.optim as optim\n\nfrom torch.nn import Linear, BatchNorm1d, ReLU\nfrom torchvision import transforms\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.metrics.functional import classification\n\nimport geffnet\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport cv2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.options.display.max_columns = None\n\nimport gc\ngc.enable()\n\nrand_seed = 1120\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\nset_seed(rand_seed)\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"PyTorch Lightning Version: {pl.__version__}\")","fbc98537":"!mkdir -p \/root\/.cache\/torch\/hub\/checkpoints\/\n!cp ..\/input\/gen-efficientnet-pretrained\/tf_efficientnet_*.pth \/root\/.cache\/torch\/hub\/checkpoints\/\n!ls -la \/root\/.cache\/torch\/hub\/checkpoints\/","020508b7":"model_type = \"b3\"\npretrained_model = f\"tf_efficientnet_{model_type[:2]}_ns\"\nexperiment_name = f\"deepinsight_efficientnet_v4_{model_type}\"\n\ndataset_folder = \"..\/input\/lish-moa\" if kernel_mode else \"\/workspace\/Kaggle\/MoA\/\"\n\nmodel_info = {\n    \"model_path\":\n    f\"..\/input\/effnet-swish\/{model_type}\"\n    if kernel_mode else\n    f\"\/workspace\/Kaggle\/MoA\/completed\/deepinsight_efficientnet_v4_b3\/{experiment_name}\"\n}\n\nnum_workers = 2 if kernel_mode else 6\ngpus = [0]\n\nif model_type == \"b0\":\n    batch_size = 128\n    infer_batch_size = 256\n    image_size = 300  # B0\n    drop_rate = 0.2  # B0\n    resolution = 300\nelif model_type == \"b3\":\n    batch_size = 64\n    infer_batch_size = 128\n    image_size = 300  # B3\n    drop_rate = 0.3  # B3\n    resolution = 300\nelif model_type == \"b5\":\n    batch_size = 8\n    infer_batch_size = 16\n    image_size = 456  # B5\n    drop_rate = 0.4  # B5\n    resolution = 456\nelif model_type == \"b7\":\n    batch_size = 2\n    infer_batch_size = 4\n    # image_size = 800  # B7\n    image_size = 772  # B7\n    drop_rate = 0.5  # B7\n    resolution = 772\nelse:\n    batch_size = 48\n    infer_batch_size = 128\n    image_size = 300  # B3\n    drop_rate = 0.3  # B3\n    resolution = 300\n\n# DeepInsight Transform\nperplexity = 5\n\ndrop_connect_rate = 0.2\nfc_size = 512\n\n# Swap Noise\nswap_prob = 0.2\nswap_portion = 0.15","cd2c1c80":"## training_params\nmodel_output_folder = \"\/kaggle\/working\/\"\nepochs = 30\nweight_decay = 0\nT_max = 30","0880101b":"train_features = pd.read_csv(f\"{dataset_folder}\/train_features.csv\",\n                             engine='c')\ntrain_labels = pd.read_csv(f\"{dataset_folder}\/train_targets_scored.csv\",\n                           engine='c')\n\ntrain_extra_labels = pd.read_csv(\n    f\"{dataset_folder}\/train_targets_nonscored.csv\", engine='c')\n\ntest_features = pd.read_csv(f\"{dataset_folder}\/test_features.csv\", engine='c')\n\nsample_submission = pd.read_csv(f\"{dataset_folder}\/sample_submission.csv\",\n                                engine='c')","bb7f09d1":"# Sort by sig_id to ensure that all row orders match\ntrain_features = train_features.sort_values(\n    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\ntrain_labels = train_labels.sort_values(by=[\"sig_id\"], axis=0,\n                                        inplace=False).reset_index(drop=True)\ntrain_extra_labels = train_extra_labels.sort_values(\n    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)\n\nsample_submission = sample_submission.sort_values(\n    by=[\"sig_id\"], axis=0, inplace=False).reset_index(drop=True)","dd024518":"category_features = [\"cp_type\", \"cp_dose\"]\nnumeric_features = [\n    c for c in train_features.columns\n    if c != \"sig_id\" and c not in category_features\n]\nall_features = category_features + numeric_features\ngene_experssion_features = [c for c in numeric_features if c.startswith(\"g-\")]\ncell_viability_features = [c for c in numeric_features if c.startswith(\"c-\")]\nlen(numeric_features), len(gene_experssion_features), len(\n    cell_viability_features)","ece86e1a":"train_classes = [c for c in train_labels.columns if c != \"sig_id\"]\ntrain_extra_classes = [c for c in train_extra_labels.columns if c != \"sig_id\"]\nlen(train_classes), len(train_extra_classes)","b98cfa98":"for df in [train_features, test_features]:\n    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})","d728387d":"# Modified from DeepInsight Transform\n# https:\/\/github.com\/alok-ai-lab\/DeepInsight\/blob\/master\/pyDeepInsight\/image_transformer.py\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.manifold import TSNE\nfrom scipy.spatial import ConvexHull\nfrom matplotlib import pyplot as plt\nimport inspect\n\n\nclass DeepInsightTransformer:\n    \"\"\"Transform features to an image matrix using dimensionality reduction\n\n    This class takes in data normalized between 0 and 1 and converts it to a\n    CNN compatible 'image' matrix\n\n    \"\"\"\n    def __init__(self,\n                 feature_extractor='tsne',\n                 perplexity=30,\n                 pixels=100,\n                 random_state=None,\n                 n_jobs=None):\n        \"\"\"Generate an ImageTransformer instance\n\n        Args:\n            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n                class instance with method `fit_transform` that returns a\n                2-dimensional array of extracted features.\n            pixels: int (square matrix) or tuple of ints (height, width) that\n                defines the size of the image matrix.\n            random_state: int or RandomState. Determines the random number\n                generator, if present, of a string defined feature_extractor.\n            n_jobs: The number of parallel jobs to run for a string defined\n                feature_extractor.\n        \"\"\"\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n\n        if isinstance(feature_extractor, str):\n            fe = feature_extractor.casefold()\n            if fe == 'tsne_exact'.casefold():\n                fe = TSNE(n_components=2,\n                          metric='cosine',\n                          perplexity=perplexity,\n                          n_iter=1000,\n                          method='exact',\n                          random_state=self.random_state,\n                          n_jobs=self.n_jobs)\n            elif fe == 'tsne'.casefold():\n                fe = TSNE(n_components=2,\n                          metric='cosine',\n                          perplexity=perplexity,\n                          n_iter=1000,\n                          method='barnes_hut',\n                          random_state=self.random_state,\n                          n_jobs=self.n_jobs)\n            elif fe == 'pca'.casefold():\n                fe = PCA(n_components=2, random_state=self.random_state)\n            elif fe == 'kpca'.casefold():\n                fe = KernelPCA(n_components=2,\n                               kernel='rbf',\n                               random_state=self.random_state,\n                               n_jobs=self.n_jobs)\n            else:\n                raise ValueError((\"Feature extraction method '{}' not accepted\"\n                                  ).format(feature_extractor))\n            self._fe = fe\n        elif hasattr(feature_extractor, 'fit_transform') and \\\n                inspect.ismethod(feature_extractor.fit_transform):\n            self._fe = feature_extractor\n        else:\n            raise TypeError('Parameter feature_extractor is not a '\n                            'string nor has method \"fit_transform\"')\n\n        if isinstance(pixels, int):\n            pixels = (pixels, pixels)\n\n        # The resolution of transformed image\n        self._pixels = pixels\n        self._xrot = None\n\n    def fit(self, X, y=None, plot=False):\n        \"\"\"Train the image transformer from the training set (X)\n\n        Args:\n            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n            y: Ignored. Present for continuity with scikit-learn\n            plot: boolean of whether to produce a scatter plot showing the\n                feature reduction, hull points, and minimum bounding rectangle\n\n        Returns:\n            self: object\n        \"\"\"\n        # Transpose to get (n_features, n_samples)\n        X = X.T\n\n        # Perform dimensionality reduction\n        x_new = self._fe.fit_transform(X)\n\n        # Get the convex hull for the points\n        chvertices = ConvexHull(x_new).vertices\n        hull_points = x_new[chvertices]\n\n        # Determine the minimum bounding rectangle\n        mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n\n        # Rotate the matrix\n        # Save the rotated matrix in case user wants to change the pixel size\n        self._xrot = np.dot(mbr_rot, x_new.T).T\n\n        # Determine feature coordinates based on pixel dimension\n        self._calculate_coords()\n\n        # plot rotation diagram if requested\n        if plot is True:\n            # Create subplots\n            fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n            ax[0, 0].scatter(x_new[:, 0],\n                             x_new[:, 1],\n                             cmap=plt.cm.get_cmap(\"jet\", 10),\n                             marker=\"x\",\n                             alpha=1.0)\n            ax[0, 0].fill(x_new[chvertices, 0],\n                          x_new[chvertices, 1],\n                          edgecolor='r',\n                          fill=False)\n            ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n            plt.gca().set_aspect('equal', adjustable='box')\n            plt.show()\n        return self\n\n    @property\n    def pixels(self):\n        \"\"\"The image matrix dimensions\n\n        Returns:\n            tuple: the image matrix dimensions (height, width)\n\n        \"\"\"\n        return self._pixels\n\n    @pixels.setter\n    def pixels(self, pixels):\n        \"\"\"Set the image matrix dimension\n\n        Args:\n            pixels: int or tuple with the dimensions (height, width)\n            of the image matrix\n\n        \"\"\"\n        if isinstance(pixels, int):\n            pixels = (pixels, pixels)\n        self._pixels = pixels\n        # recalculate coordinates if already fit\n        if hasattr(self, '_coords'):\n            self._calculate_coords()\n\n    def _calculate_coords(self):\n        \"\"\"Calculate the matrix coordinates of each feature based on the\n        pixel dimensions.\n        \"\"\"\n        ax0_coord = np.digitize(self._xrot[:, 0],\n                                bins=np.linspace(min(self._xrot[:, 0]),\n                                                 max(self._xrot[:, 0]),\n                                                 self._pixels[0])) - 1\n        ax1_coord = np.digitize(self._xrot[:, 1],\n                                bins=np.linspace(min(self._xrot[:, 1]),\n                                                 max(self._xrot[:, 1]),\n                                                 self._pixels[1])) - 1\n        self._coords = np.stack((ax0_coord, ax1_coord))\n\n    def transform(self, X, empty_value=0):\n        \"\"\"Transform the input matrix into image matrices\n\n        Args:\n            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                where n_features matches the training set.\n            empty_value: numeric value to fill elements where no features are\n                mapped. Default = 0 (although it was 1 in the paper).\n\n        Returns:\n            A list of n_samples numpy matrices of dimensions set by\n            the pixel parameter\n        \"\"\"\n\n        # Group by location (x1, y1) of each feature\n        # Tranpose to get (n_features, n_samples)\n        img_coords = pd.DataFrame(np.vstack(\n            (self._coords, X.clip(0, 1))).T).groupby(\n                [0, 1],  # (x1, y1)\n                as_index=False).mean()\n\n        img_matrices = []\n        blank_mat = np.zeros(self._pixels)\n        if empty_value != 0:\n            blank_mat[:] = empty_value\n        for z in range(2, img_coords.shape[1]):\n            img_matrix = blank_mat.copy()\n            img_matrix[img_coords[0].astype(int),\n                       img_coords[1].astype(int)] = img_coords[z]\n            img_matrices.append(img_matrix)\n\n        return img_matrices\n\n    def fit_transform(self, X, empty_value=0):\n        \"\"\"Train the image transformer from the training set (X) and return\n        the transformed data.\n\n        Args:\n            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n            empty_value: numeric value to fill elements where no features are\n                mapped. Default = 0 (although it was 1 in the paper).\n\n        Returns:\n            A list of n_samples numpy matrices of dimensions set by\n            the pixel parameter\n        \"\"\"\n        self.fit(X)\n        return self.transform(X, empty_value=empty_value)\n\n    def feature_density_matrix(self):\n        \"\"\"Generate image matrix with feature counts per pixel\n\n        Returns:\n            img_matrix (ndarray): matrix with feature counts per pixel\n        \"\"\"\n        fdmat = np.zeros(self._pixels)\n        # Group by location (x1, y1) of each feature\n        # Tranpose to get (n_features, n_samples)\n        coord_cnt = (\n            pd.DataFrame(self._coords.T).assign(count=1).groupby(\n                [0, 1],  # (x1, y1)\n                as_index=False).count())\n        fdmat[coord_cnt[0].astype(int),\n              coord_cnt[1].astype(int)] = coord_cnt['count']\n        return fdmat\n\n    @staticmethod\n    def _minimum_bounding_rectangle(hull_points):\n        \"\"\"Find the smallest bounding rectangle for a set of points.\n\n        Modified from JesseBuesking at https:\/\/stackoverflow.com\/a\/33619018\n        Returns a set of points representing the corners of the bounding box.\n\n        Args:\n            hull_points : an nx2 matrix of hull coordinates\n\n        Returns:\n            (tuple): tuple containing\n                coords (ndarray): coordinates of the corners of the rectangle\n                rotmat (ndarray): rotation matrix to align edges of rectangle\n                    to x and y\n        \"\"\"\n\n        pi2 = np.pi \/ 2.\n\n        # Calculate edge angles\n        edges = hull_points[1:] - hull_points[:-1]\n        angles = np.arctan2(edges[:, 1], edges[:, 0])\n        angles = np.abs(np.mod(angles, pi2))\n        angles = np.unique(angles)\n\n        # Find rotation matrices\n        rotations = np.vstack([\n            np.cos(angles),\n            np.cos(angles - pi2),\n            np.cos(angles + pi2),\n            np.cos(angles)\n        ]).T\n        rotations = rotations.reshape((-1, 2, 2))\n\n        # Apply rotations to the hull\n        rot_points = np.dot(rotations, hull_points.T)\n\n        # Find the bounding points\n        min_x = np.nanmin(rot_points[:, 0], axis=1)\n        max_x = np.nanmax(rot_points[:, 0], axis=1)\n        min_y = np.nanmin(rot_points[:, 1], axis=1)\n        max_y = np.nanmax(rot_points[:, 1], axis=1)\n\n        # Find the box with the best area\n        areas = (max_x - min_x) * (max_y - min_y)\n        best_idx = np.argmin(areas)\n\n        # Return the best box\n        x1 = max_x[best_idx]\n        x2 = min_x[best_idx]\n        y1 = max_y[best_idx]\n        y2 = min_y[best_idx]\n        rotmat = rotations[best_idx]\n\n        # Generate coordinates\n        coords = np.zeros((4, 2))\n        coords[0] = np.dot([x1, y2], rotmat)\n        coords[1] = np.dot([x2, y2], rotmat)\n        coords[2] = np.dot([x2, y1], rotmat)\n        coords[3] = np.dot([x1, y1], rotmat)\n\n        return coords, rotmat","e22ff1fc":"class LogScaler:\n    \"\"\"Log normalize and scale data\n\n    Log normalization and scaling procedure as described as norm-2 in the\n    DeepInsight paper supplementary information.\n    \n    Note: The dimensions of input matrix is (N samples, d features)\n    \"\"\"\n    def __init__(self):\n        self._min0 = None\n        self._max = None\n\n    \"\"\"\n    Use this as a preprocessing step in inference mode.\n    \"\"\"\n    def fit(self, X, y=None):\n        # Min. of training set per feature\n        self._min0 = X.min(axis=0)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Global max. of training set from X_norm\n        self._max = X_norm.max()\n\n    \"\"\"\n    For training set only.\n    \"\"\"\n    def fit_transform(self, X, y=None):\n        # Min. of training set per feature\n        self._min0 = X.min(axis=0)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Global max. of training set from X_norm\n        self._max = X_norm.max()\n\n        # Normalized again by global max. of training set\n        return (X_norm \/ self._max).clip(0, 1)\n\n    \"\"\"\n    For validation and test set only.\n    \"\"\"\n    def transform(self, X, y=None):\n        # Adjust min. of each feature of X by _min0\n        for i in range(X.shape[1]):\n            X[:, i] = X[:, i].clip(min=self._min0[i], max=None)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Normalized again by global max. of training set\n        return (X_norm \/ self._max).clip(0, 1)","06b2a1a9":"train_transform = A.Compose(\n    [\n        A.Cutout(num_holes=50, max_h_size=40, max_w_size=40, fill_value=0, p=0.3),\n    ]\n)","4d47d68e":"class MoAImageSwapDataset(torch.utils.data.Dataset):\n    def __init__(self,\n                 features,\n                 labels,\n                 transformer,\n                 image_transform,\n                 swap_prob=0.15,\n                 swap_portion=0.1):\n        self.features = features\n        self.labels = labels\n        self.transformer = transformer\n        self.image_transform = image_transform\n        self.swap_prob = swap_prob\n        self.swap_portion = swap_portion\n\n    def __getitem__(self, index):\n        normalized = self.features[index, :]\n\n        # Swap row features randomly\n        normalized = self.add_swap_noise(index, normalized)\n\n        normalized = np.expand_dims(normalized, axis=0)\n\n        # Note: we are setting empty_value=1 to follow the setup in the paper\n        image = self.transformer.transform(normalized, empty_value=1)[0]\n\n        # Resize to target size\n        gene_cht = cv2.resize(image, (image_size, image_size),\n                              interpolation=cv2.INTER_CUBIC)\n\n        # Convert to 3 channels\n        image = np.repeat(gene_cht[np.newaxis, :, :], 3, axis=0)\n        if self.image_transform is not None:\n            image = self.image_transform(image=image)[\"image\"]\n\n        return {\"x\": image, \"y\": self.labels[index, :]}\n\n    def add_swap_noise(self, index, X):\n        if np.random.rand() < self.swap_prob:\n            swap_index = np.random.randint(self.features.shape[0], size=1)[0]\n            # Select only gene expression and cell viability features\n            swap_features = np.random.choice(\n                np.array(range(3, self.features.shape[1])),\n                size=int(self.features.shape[1] * self.swap_portion),\n                replace=False)\n            X[swap_features] = self.features[swap_index, swap_features]\n\n        return X\n\n    def __len__(self):\n        return self.features.shape[0]","f04fbefa":"class MoAImageDataset(torch.utils.data.Dataset):\n    def __init__(self, features, labels, transformer):\n        self.features = features\n        self.labels = labels\n        self.transformer = transformer\n\n    def __getitem__(self, index):\n        normalized = self.features[index, :]\n        normalized = np.expand_dims(normalized, axis=0)\n\n        # Note: we are setting empty_value=1 to follow the setup in the paper\n        image = self.transformer.transform(normalized, empty_value=1)[0]\n\n        # Resize to target size\n        gene_cht = cv2.resize(image, (image_size, image_size),\n                              interpolation=cv2.INTER_CUBIC)\n\n        # Convert to 3 channels\n        image = np.repeat(gene_cht[np.newaxis, :, :], 3, axis=0)\n\n        return {\"x\": image, \"y\": self.labels[index, :]}\n\n    def __len__(self):\n        return self.features.shape[0]\n\n\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, features, labels, transformer):\n        self.features = features\n        self.labels = labels\n        self.transformer = transformer\n\n    def __getitem__(self, index):\n        normalized = self.features[index, :]\n        normalized = np.expand_dims(normalized, axis=0)\n\n        # Note: we are setting empty_value=1 to follow the setup in the paper\n        image = self.transformer.transform(normalized, empty_value=1)[0]\n\n        # Resize to target size\n        gene_cht = cv2.resize(image, (image_size, image_size),\n                              interpolation=cv2.INTER_CUBIC)\n\n        # Convert to 3 channels\n        image = np.repeat(gene_cht[np.newaxis, :, :], 3, axis=0)\n\n        return {\"x\": image, \"y\": -1}\n\n    def __len__(self):\n        return self.features.shape[0]","48e70d6b":"class LogScaler:\n    \"\"\"Log normalize and scale data\n\n    Log normalization and scaling procedure as described as norm-2 in the\n    DeepInsight paper supplementary information.\n    \n    Note: The dimensions of input matrix is (N samples, d features)\n    \"\"\"\n    def __init__(self):\n        self._min0 = None\n        self._max = None\n\n    \"\"\"\n    Use this as a preprocessing step in inference mode.\n    \"\"\"\n    def fit(self, X, y=None):\n        # Min. of training set per feature\n        self._min0 = X.min(axis=0)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Global max. of training set from X_norm\n        self._max = X_norm.max()\n\n    \"\"\"\n    For training set only.\n    \"\"\"\n    def fit_transform(self, X, y=None):\n        # Min. of training set per feature\n        self._min0 = X.min(axis=0)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Global max. of training set from X_norm\n        self._max = X_norm.max()\n\n        # Normalized again by global max. of training set\n        return (X_norm \/ self._max).clip(0, 1)\n\n    \"\"\"\n    For validation and test set only.\n    \"\"\"\n    def transform(self, X, y=None):\n        # Adjust min. of each feature of X by _min0\n        for i in range(X.shape[1]):\n            X[:, i] = X[:, i].clip(min=self._min0[i], max=None)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Normalized again by global max. of training set\n        return (X_norm \/ self._max).clip(0, 1)","0a6c827d":"# Reference: https:\/\/github.com\/rwightman\/gen-efficientnet-pytorch\/blob\/master\/geffnet\/efficientnet_builder.py#L672\ndef initialize_weight_goog(m, n='', fix_group_fanout=True):\n    # weight init as per Tensorflow Official impl\n    # https:\/\/github.com\/tensorflow\/tpu\/blob\/master\/models\/official\/mnasnet\/mnasnet_model.py\n    if isinstance(m, nn.Conv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out \/\/= m.groups\n        m.weight.data.normal_(0, math.sqrt(2.0 \/ fan_out))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        fan_out = m.weight.size(0)  # fan-out\n        fan_in = 0\n        if 'routing_fn' in n:\n            fan_in = m.weight.size(1)\n        init_range = 1.0 \/ math.sqrt(fan_in + fan_out)\n        m.weight.data.uniform_(-init_range, init_range)\n        m.bias.data.zero_()\n\n\ndef initialize_weight_default(m, n=''):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        nn.init.kaiming_uniform_(m.weight,\n                                 mode='fan_in',\n                                 nonlinearity='linear')\n        \nfrom torch.nn.modules.loss import _WeightedLoss\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.001):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n        return loss\n\nclass Swish(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.nn.Sigmoid()(i)\n        ctx.save_for_backward(i)\n        return result\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = torch.nn.Sigmoid()(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\nswish = Swish.apply\n\nclass Swish_module(nn.Module):\n    def forward(self, x):\n        return swish(x)\nswish_layer = Swish_module()","b4415ddf":"class MoAEfficientNet(pl.LightningModule):\n    def __init__(\n            self,\n            pretrained_model_name,\n            training_set=(None, None),  # tuple\n            valid_set=(None, None),  # tuple\n            test_set=None,\n            transformer=None,\n            image_transform=None,\n            num_classes=206,\n            in_chans=3,\n            drop_rate=0.2,\n            drop_connect_rate=0.,\n            fc_size=512,\n            learning_rate=1e-3,\n            weight_init='goog'):\n        super(MoAEfficientNet, self).__init__()\n\n        self.train_data, self.train_labels = training_set\n        self.valid_data, self.valid_labels = valid_set\n        self.test_data = test_set\n        self.transformer = transformer\n        self.image_transform = image_transform\n\n        self.backbone = getattr(geffnet, pretrained_model)(\n            pretrained=True,\n            in_chans=in_chans,\n            drop_rate=drop_rate,\n            drop_connect_rate=drop_connect_rate,\n            weight_init=weight_init)\n\n        self.backbone.classifier = nn.Sequential(\n            nn.Linear(self.backbone.classifier.in_features, fc_size,\n                      bias=True), Swish_module(), nn.Dropout(p=drop_rate),\n            nn.Linear(fc_size, num_classes, bias=True))\n\n        if self.training:\n            for m in self.backbone.classifier.modules():\n                initialize_weight_goog(m)\n\n        # Save passed hyperparameters\n        self.save_hyperparameters(\"pretrained_model_name\", \"num_classes\",\n                                  \"in_chans\", \"drop_rate\", \"drop_connect_rate\",\n                                  \"weight_init\", \"fc_size\", \"learning_rate\")\n\n    def forward(self, x):\n        return self.backbone(x)\n\n    def training_step(self, batch, batch_idx):\n        x = batch[\"x\"]\n        y = batch[\"y\"]\n        x = x.float()\n        y = y.type_as(x)\n        logits = self(x)\n\n        # loss = F.binary_cross_entropy_with_logits(logits, y, reduction=\"mean\")\n        loss = SmoothBCEwLogits(smoothing = 0.001)(logits, y)\n\n        self.log('train_loss',\n                 loss,\n                 on_step=True,\n                 on_epoch=True,\n                 prog_bar=True,\n                 logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x = batch[\"x\"]\n        y = batch[\"y\"]\n        x = x.float()\n        y = y.type_as(x)\n        logits = self(x)\n\n        val_loss = F.binary_cross_entropy_with_logits(logits,\n                                                      y,\n                                                      reduction=\"mean\")\n\n        self.log('val_loss',\n                 val_loss,\n                 on_step=True,\n                 on_epoch=True,\n                 prog_bar=True,\n                 logger=True)\n\n        return val_loss\n\n    def test_step(self, batch, batch_idx):\n        x = batch[\"x\"]\n        y = batch[\"y\"]\n        x = x.float()\n        y = y.type_as(x)\n        logits = self(x)\n        return {\"pred_logits\": logits}\n\n    def test_epoch_end(self, output_results):\n        all_outputs = torch.cat([out[\"pred_logits\"] for out in output_results],\n                                dim=0)\n        print(\"Logits:\", all_outputs)\n        pred_probs = F.sigmoid(all_outputs).detach().cpu().numpy()\n        print(\"Predictions: \", pred_probs)\n        return {\"pred_probs\": pred_probs}\n\n    def setup(self, stage=None):\n \n        self.train_dataset = MoAImageSwapDataset(self.train_data,\n                                                 self.train_labels,\n                                                 self.transformer,\n                                                 self.image_transform,\n                                                 swap_prob=swap_prob,\n                                                 swap_portion=swap_portion)\n\n        self.val_dataset = MoAImageDataset(self.valid_data, self.valid_labels,\n                                           self.transformer)\n\n        self.test_dataset = TestDataset(self.test_data, None, self.transformer)\n\n    def train_dataloader(self):\n        train_dataloader = DataLoader(self.train_dataset,\n                                      batch_size=batch_size,\n                                      shuffle=True,\n                                      num_workers=4,\n                                      pin_memory=True,\n                                      drop_last=False)\n        print(f\"Train iterations: {len(train_dataloader)}\")\n        return train_dataloader\n\n    def val_dataloader(self):\n        val_dataloader = DataLoader(self.val_dataset,\n                                    batch_size=infer_batch_size,\n                                    shuffle=False,\n                                    num_workers=0,\n                                    pin_memory=True,\n                                    drop_last=False)\n        print(f\"Validate iterations: {len(val_dataloader)}\")\n        return val_dataloader\n\n    def test_dataloader(self):\n        test_dataloader = DataLoader(self.test_dataset,\n                                     batch_size=infer_batch_size,\n                                     shuffle=False,\n                                     num_workers=num_workers,\n                                     pin_memory=True,\n                                     drop_last=False)\n        print(f\"Test iterations: {len(test_dataloader)}\")\n        return test_dataloader\n\n    def configure_optimizers(self):\n        print(f\"Initial Learning Rate: {self.hparams.learning_rate:.6f}\")\n        optimizer = optim.Adam(self.parameters(),\n                               lr=self.hparams.learning_rate,\n                               weight_decay=weight_decay)\n\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                         T_max=T_max,\n                                                         eta_min=0,\n                                                         last_epoch=-1)\n\n        return [optimizer], [scheduler]","66bc0dac":"def get_model_train(training_set, valid_set, transformer, image_transform):\n    model = MoAEfficientNet(\n        pretrained_model_name=pretrained_model,\n        training_set=training_set,  # tuple\n        valid_set=valid_set,  # tuple\n        test_set=(None, None),\n        transformer=transformer,\n        image_transform=image_transform,\n        drop_rate=drop_rate,\n        drop_connect_rate=drop_connect_rate,\n        fc_size=fc_size,\n        weight_init='goog')\n\n    return model\n\ndef extract_feature_map(train, resolution, perplexity):\n    transformer= DeepInsightTransformer(pixels=resolution,\n                                        perplexity=perplexity)\n    transformer=transformer.fit(train)\n    return transformer\n\ndef norm2_normalization(train, valid, test):\n    scaler=LogScaler()\n    train_set=scaler.fit_transform(train)\n    valid_set=scaler.transform(valid)\n    test_set=scaler.transform(test)\n    return train_set, valid_set, test_set, scaler\n\ndef save_pickle(obj, model_output_folder, fold_i, name):\n    dump(obj, open(f\"{model_output_folder}\/fold{fold_i}_{name}.pkl\", 'wb'),\n         pickle.HIGHEST_PROTOCOL)\n\ndef load_pickle(model_output_folder, fold_i, name):\n    return load(open(f\"{model_output_folder}\/fold{fold_i}_{name}.pkl\", 'rb'))","08ed5a90":"kfolds = 10\nskf = MultilabelStratifiedKFold(n_splits=kfolds,\n                                shuffle=True,\n                                random_state=rand_seed)\n\nlabel_counts = np.sum(train_labels.drop(\"sig_id\", axis=1), axis=0)\ny_labels = label_counts.index.tolist()","62498714":"# for i, (train_index, val_index) in enumerate(fold_split):\nfor i, (train_index, val_index) in enumerate(skf.split(train_features, train_labels[y_labels])):\n\n    if i >= 0:\n\n        seed_everything(i)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n        logger = TensorBoardLogger(model_output_folder,\n                                  name=f\"fold{i}\/logs\",\n                                  default_hp_metric=False)\n\n        train = train_features.loc[train_index, all_features].copy().values\n        fold_train_labels = train_labels.loc[train_index, train_classes].copy().values\n        valid = train_features.loc[val_index, all_features].copy().values\n        fold_valid_labels = train_labels.loc[val_index, train_classes].copy().values\n        test = test_features[all_features].copy().values\n\n        # LogScaler (Norm-2 Normalization)\n        print(\"Running norm-2 normalization ......\")\n        train, valid, test, scaler = norm2_normalization(train, valid, test)\n        save_pickle(scaler, model_output_folder, i, \"log-scaler\")\n\n        # Extract DeepInsight Feature Map\n        print(\"Extracting feature map ......\")\n        transformer = extract_feature_map(train,\n                                          resolution=resolution,\n                                          perplexity=perplexity)\n        save_pickle(transformer, model_output_folder, i, \"deepinsight-transform\")\n\n        model = get_model_train(training_set=(train, fold_train_labels),\n                          valid_set=(valid, fold_valid_labels),\n                          transformer=transformer,\n                          image_transform=train_transform)\n\n        callbacks = [\n            EarlyStopping(monitor='val_loss_epoch',\n                          min_delta=1e-6,\n                          patience=5,\n                          verbose=True,\n                          mode='min',\n                          strict=True),\n            LearningRateMonitor(logging_interval='step')\n        ]\n        # https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/generated\/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint\n        checkpoint_callback = ModelCheckpoint(\n            filepath=f\"{model_output_folder}\/fold{i}\" + \"\/{val_loss_epoch:.6f}\" + f\"_fold{i}\",\n            save_top_k=1,\n            save_weights_only=False,\n            save_last=False,\n            verbose=True,\n            monitor='val_loss_epoch',\n            mode='min',\n            prefix='')\n\n        trainer = Trainer(\n            gpus=gpus,\n            distributed_backend=\"dp\",  # multiple-gpus, 1 machine\n            max_epochs=epochs,\n            benchmark=False,\n            deterministic=True,\n            checkpoint_callback=checkpoint_callback,\n            callbacks=callbacks,\n            accumulate_grad_batches=1,\n            gradient_clip_val=0,\n            precision=16,\n            logger=logger)\n        trainer.fit(model)","030f008f":"torch.cuda.empty_cache()\ngc.collect()","21191f8d":"**Weight initialization, Loss and Swish**","ef1dee54":"# DeepInsight Transform - t-SNE 2D Embeddings\nBased on https:\/\/github.com\/alok-ai-lab\/DeepInsight, but with some corrections to the norm-2 normalization.\n\nMost of the credits should be given to the original authors!","074012c7":"# Dataset","ff2cb815":"# Training","524179f4":"# Load MoA Data","ad55733f":"# EfficientNet Pytorch Lightning Training kernel\n\nCredit goes to @markpeng for this wonderful idea transforming tabular data into tsne space.","ad448b3b":"# Model Definition","9f5e4be6":"## Implementation\n\nCheckout <a href=\"https:\/\/static-content.springer.com\/esm\/art%3A10.1038%2Fs41598-019-47765-6\/MediaObjects\/41598_2019_47765_MOESM1_ESM.pdf\" target=\"_blank\">DeepInsight paper supplementary information<\/a> for more details.","99531958":"# Feature Encoding\nAs we only have three metadata features, a quick manual encoding process is done. All features are normalized into the value range of [0, 1]."}}