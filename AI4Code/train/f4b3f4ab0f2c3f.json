{"cell_type":{"4e37ac10":"code","363e7609":"code","c0f0e2a7":"code","35acf431":"code","0ec9f75f":"code","c882c345":"code","b7e52088":"code","0a02e441":"code","a0c58d1d":"code","da16e3cb":"code","7ef3d19d":"code","5cf4bbf4":"code","f82eea5c":"code","212a5300":"code","665ddffd":"code","3f681f8c":"code","a30d7bc5":"code","35e0661e":"code","476a5329":"code","51543aa2":"code","f2a52a11":"code","61e844a4":"code","7d1280fb":"code","68021e31":"code","a4d246ba":"code","b1c4ecca":"markdown","e3ca35c5":"markdown","652c9c41":"markdown","391c13ff":"markdown","5f487408":"markdown","adee43cc":"markdown","1c6e0f22":"markdown","31d755db":"markdown","08f1a87b":"markdown","756cdd08":"markdown"},"source":{"4e37ac10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","363e7609":"from collections import OrderedDict","c0f0e2a7":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nprint(train.shape)\ntype(train)","35acf431":"test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(test.shape)\ntype(test)","0ec9f75f":"t_train = train['label']\nx_train = train.drop(labels = ['label'], axis = 1)","c882c345":"t_train","b7e52088":"x_train2 = x_train.to_numpy()\nt_train2 = t_train.to_numpy()\nx_test2 = test.to_numpy()\nx_train2, x_train2.shape, t_train2, t_train2.shape","0a02e441":"t_train2 # label \ud615\uc2dd (1D)\n# one hot encoding \ud615\ud0dc\ub85c \ubcc0\uacbd. (2D)\nt_train2_1hot = np.zeros((t_train2.size, t_train2.max() + 1)) # (m, n) array\nt_train2_1hot[np.arange(t_train2.size), t_train2] = 1 ## **** index\ub0b4 \uc778\uc790\ub85c array\uac00 \ub4e4\uc5b4\uac00????? *****\nt_train2, t_train2_1hot, t_train2_1hot.shape","a0c58d1d":"def softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)\n        y = np.exp(x) \/ np.sum(np.exp(x), axis=0)\n        return y.T \n\n    x = x - np.max(x) # \uc624\ubc84\ud50c\ub85c \ub300\ucc45\n    return np.exp(x) \/ np.sum(np.exp(x))\n    \ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n        \n    # \ud6c8\ub828 \ub370\uc774\ud130\uac00 \uc6d0-\ud56b \ubca1\ud130\ub77c\uba74 \uc815\ub2f5 \ub808\uc774\ube14\uc758 \uc778\ub371\uc2a4\ub85c \ubc18\ud658\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n             \n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) \/ batch_size\n\nclass Affine:\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n        self.x = None\n        self.dW = None\n        self.db = None\n        \n    def forward(self, x):\n        self.x = x\n        out = np.dot(x, self.W) + self.b\n        return out\n        \n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis = 0)\n        return dx\n    # X.shape == grad(X).shape !!!\n    \nclass ReLU:\n    def __init__(self):\n        self.mask = None\n        \n    def forward(self, x):\n        self.mask = (x <= 0) # T\/F \ub85c \ucd9c\ub825.\n        out = x.copy()\n        out[self.mask] = 0 # True\uc778 \uc6d0\uc18c\uc5d0\ub9cc 0\uc73c\ub85c \ub300\uccb4.\n        return out\n    \n    def backward(self, dout):\n        dout[self.mask] = 0\n        dx = dout\n        return dx\n    \nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss = None\n        self.y = None\n        self.t = None\n    \n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        return self.loss\n    \n    def backward(self, dout = 1):\n        batch_size = self.t.shape[0]\n        dx = (self.y- self.t) \/ batch_size\n        return dx\n\n# ------------------------------\nclass TwoLayerNet:\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n        self.params = {}\n        self.params['W1'] = np.random.randn(input_size, hidden_size) * weight_init_std\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = np.random.randn(hidden_size, output_size) * weight_init_std\n        self.params['b2'] = np.zeros(output_size)\n        \n        self.layers = OrderedDict()\n        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n        self.layers['Relu1'] = ReLU()\n        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n        \n        self.lastLayer = SoftmaxWithLoss()\n        \n    def predict(self, x):\n        for layer in self.layers.values():\n            x = layer.forward(x)\n            \n        return(x)\n    \n    def loss(self, x, t):\n        y = self.predict(x)\n        return self.lastLayer.forward(y, t)\n    \n    def accuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis = 1)\n        if t.ndim != 1 : t = np.argmax(t, axis = 1)\n        \n        accuracy = np.sum(y == t) \/ float(x.shape[0])\n        return accuracy\n\n    def gradient(self, x, t):\n        \n        # forward\n        self.loss(x, t)\n        \n        # backward\n        dout = 1\n        dout = self.lastLayer.backward(dout)         # \"t\" label > one hot encoding!!\n        \n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n            \n        grads = {}\n        grads['W1'] = self.layers['Affine1'].dW\n        grads['b1'] = self.layers['Affine1'].db\n        grads['W2'] = self.layers['Affine2'].dW\n        grads['b2'] = self.layers['Affine2'].db\n        \n        return grads","da16e3cb":"net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\nnet.params.keys(), net.layers","7ef3d19d":"p1 = net.predict(x_train2) # output : ... > Softmax! (so, the p1's dimension be : (n,10), n = nrow(trainset))\np1.shape","5cf4bbf4":"l1 = net.loss(x_train2, t_train2_1hot)\nl1 # cross entropy error of the first iteration ","f82eea5c":"a1 = net.accuracy(x_train2, t_train2_1hot)\na1 # about 10% accuracy (depend on seed)","212a5300":"g1 = net.gradient(x_train2, t_train2_1hot)\ng1.keys()","665ddffd":"learning_rate = 0.01\n\nnew_W1 = net.params[\"W1\"] - learning_rate * g1[\"W1\"]\nnew_b1 = net.params[\"b1\"] - learning_rate * g1[\"b1\"]\nnew_W2 = net.params[\"W2\"] - learning_rate * g1[\"W2\"]\nnew_b2 = net.params[\"b2\"] - learning_rate * g1[\"b2\"]\n\nnew_W1.shape, new_b1.shape, new_W2.shape, new_b2.shape","3f681f8c":"net2 = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n\niter_max = 10000\ntrain_size = x_train2.shape[0]\nbatch_size = 100\nlearning_rate = 0.01\n\ntrain_loss_list = []\ntrain_acc_list = []\n\niter_per_epoch = max(train_size \/ batch_size, 1)","a30d7bc5":"for i in range(iter_max):\n    # mini batch\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train2[batch_mask]\n    t_batch = t_train2_1hot[batch_mask]\n    \n    # gradient\n    grad = net2.gradient(x_batch, t_batch)\n    \n    # update weights\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        net2.params[key] -= learning_rate * grad[key]\n    \n    loss = net2.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n    \n    if i % iter_per_epoch == 0:\n        train_acc = net2.accuracy(x_train2, t_train2)\n        train_acc_list.append(train_acc)","35e0661e":" train_acc_list","476a5329":"train_acc # in my result, approx. 15~45% accuracy, and no improve accuracy along with epoches (depend on each random seeds)","51543aa2":"t_test = net2.predict(x_test2)\nt_pred = np.argmax(t_test, axis = 1)\nt_pred, t_pred.shape","f2a52a11":"sub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","61e844a4":"sub","7d1280fb":"sub['Label'] = t_pred","68021e31":"sub","a4d246ba":"sub.to_csv('.\/sub1.csv', index = False)","b1c4ecca":"ch 5.7.4 \ucc38\uace0","e3ca35c5":"# **4. submission =====**","652c9c41":"# **0. define functions**","391c13ff":"## 1-2. calculate gradient","5f487408":"# **2. 20000 iteration =====**","adee43cc":"## 1-1. predict, loss and accuracy functions in the class","1c6e0f22":"# **1. first iteration =====**","31d755db":"\uc5ec\uae30\uc11c\ub294 TwoLayerNet \ud074\ub798\uc2a4 \ub0b4 \ud568\uc218\ub4e4\uc744 \ud558\ub098\uc529 \uc791\ub3d9\ud558\uba74\uc11c, input\uc744 \ub123\uc5c8\uc744 \ub54c output\uc774 \uc5b4\ub5bb\uac8c \ub098\uc624\ub294\uc9c0 \ud655\uc778\ud55c\ub2e4. (\ubb3c\ub860 \uac01 \ud568\uc218 \ub0b4 \uc791\ub3d9\uc6d0\ub9ac\ub294 \ubcf8\uc778\uc774 \uacf5\ubd80\ud560 \uac83.)","08f1a87b":"# FOR NOVICE 2 (including me) - Deep Learning from Scratch 1 ch1 ~ ch5 (my exercise)\n\n! After \"MNIST for novice 1 (Deep Learning from Scratch 1)\" notebook. (I'm a beginner too.. haha)\n\nIn \"MNIST for novice 1 (Deep Learning from Scratch 1)\", a Two Layer Network was implemented using minimal functions, including forward, backward, and weight updates. The goal was to understand the basics of how neural networks work while implementing the code.\n\n**In this notebook, we fit(train), predict, and submit actual MNIST data using the TwoLayerNet function defined in the book.**\n\n> 1. Define the TwoLayerNet class, and repeatedly train forward, backward, and weight update in the class using the for statement. (mini-batch)\n> 2. Apply the learned model to the test data to derive the results and submit.\n\nAs a result, the score is about 0.409... It's true with a very simple model, but I've found that the results are much better than when only one iteration was run.\n\nNext, I plan to submit the results by fitting and predicting the model using CNN in chapters 6 to 7. Then I think you have roughly learned the content of Deep Learning from Scratch 1!\n\n!Please understand that my native language is not English.\n\n**!if you fine something wrong in my code, please feel free to let me know!!**\n\n----------------------------------------------------------------------------------------\n**\uc774 \ub178\ud2b8\ubd81\uc740 \uc81c\uac00 \uacf5\ubd80\ud55c \ucf54\ub4dc\ub97c \uacf5\uc720\ud55c \uac83\uc785\ub2c8\ub2e4**\n\n! \uc55e\uc5d0 \uc791\uc131\ud55c \"MNIST for novice 1 (Deep Learning from Scratch 1)\" notebook\uc5d0 \uc774\uc5b4\uc9c4 \ub0b4\uc6a9\uc785\ub2c8\ub2e4. (\uc800\ub3c4 \ucd08\uc2ec\uc790\uc785\ub2c8\ub2e4..\u314e\u314e)\n\n\"MNIST for novice 1 (Deep Learning from Scratch 1)\"\uc5d0\uc11c\ub294 Two Layer Network\ub97c forward, backward \uadf8\ub9ac\uace0 \uac00\uc911\uce58 \uc5c5\ub370\uc774\ud2b8\uae4c\uc9c0 \ucd5c\uc18c\ud55c\uc758 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uad6c\ud604\ud588\uc2b5\ub2c8\ub2e4. \uadf8 \ubaa9\uc801\uc740 \ucf54\ub4dc\ub97c \uad6c\ud604\ud558\uba74\uc11c \uae30\ucd08\uc801\uc778 \uc2e0\uacbd\ub9dd \uc791\ub3d9 \uc6d0\ub9ac\ub97c \uc774\ud574\ud558\ub294 \uac83\uc774\uc5c8\uc2b5\ub2c8\ub2e4.\n\n**\uc774 \ub178\ud2b8\ubd81\uc5d0\uc11c\ub294 \ucc45\uc5d0\uc11c \uc815\uc758\ud55c TwoLayerNet \ud568\uc218\ub97c \ud65c\uc6a9\ud574\uc11c \uc2e4\uc81c MNIST \ub370\uc774\ud130\ub97c \ud559\uc2b5\ud558\uace0, \uc608\uce21\ud574\uc11c \uc81c\ucd9c\ud558\ub294 \uac83\uc774 \ubaa9\uc801\uc785\ub2c8\ub2e4.**\n\n> 1. TwoLayerNet \ud074\ub798\uc2a4\ub97c \uc815\uc758\ud558\uace0, \ud074\ub798\uc2a4 \ub0b4 forward, backward, weight update\ub97c for\ubb38\uc744 \ud65c\uc6a9\ud574 \ubc18\ubcf5 \ud559\uc2b5\ud569\ub2c8\ub2e4. (mini-batch) \n> 2. \ud559\uc2b5\ud55c \ubaa8\ub378\uc744 test data\uc5d0 \uc801\uc6a9\ud558\uc5ec \uacb0\uacfc\ub97c \ub3c4\ucd9c\ud574\uc11c, submit\ud569\ub2c8\ub2e4. \n\n\uacb0\uacfc\uc801\uc73c\ub85c, score\uc740 0.409 \uc815\ub3c4 \uc785\ub2c8\ub2e4... \uc544\uc8fc \uac04\ub2e8\ud55c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\uc11c \uadf8\ub807\uc2b5\ub2c8\ub2e4\ub9cc, \ud558\ub098\uc758 iteration\ub9cc \uc2e4\ud589\ud588\uc744 \ub54c\ubcf4\ub2e4 \ud6e8\uc52c \uc88b\uc740 \uacb0\uacfc\uc784\uc744 \ud655\uc778\ud588\uc2b5\ub2c8\ub2e4.\n\n\ub2e4\uc74c\uc5d0\ub294 chapter6 ~ 7\uc5d0 \uc788\ub294 CNN\uc744 \ud65c\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 fitting\ud558\uace0 \uc608\uce21\ud558\uc5ec \uacb0\uacfc\ub97c \uc81c\ucd9c\ud574 \ubcfc \uc0dd\uac01\uc785\ub2c8\ub2e4. \uadf8\ub7ec\uba74 Deep Learning from Scratch 1\uc758 \ub0b4\uc6a9\uc744 \ub300\ub7b5\uc801\uc73c\ub85c \ud559\uc2b5\ud55c \uac83\uc774\ub77c\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4!\n\n\uc870\uae08\uc529 \uacf5\ubd80\ud574\uac00\uba74\uc11c \ucd94\uac00\ud560 \uc608\uc815\uc785\ub2c8\ub2e4.\n\n**\ud2c0\ub9b0 \ubd80\ubd84 \uc788\uc73c\uba74 \ud3b8\ud558\uac8c \ub9d0\uc500\ud574\uc8fc\uc138\uc694!**","756cdd08":"# **3. predict test set label =====**"}}