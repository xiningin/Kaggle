{"cell_type":{"bd12f988":"code","fa719fc1":"code","19470fdd":"code","0a084cc2":"code","fadc98dc":"code","684688fe":"code","2f620b55":"code","fbbdb887":"code","0979bafb":"code","c2f37240":"code","52b1371e":"code","f47a4a8d":"code","96dbdc3a":"code","98aaa6ce":"code","66baa471":"code","a6e6941e":"code","eb3dc0c1":"code","88efe8ba":"code","725f8839":"code","d99cf57a":"code","18dba3a2":"code","fac7e039":"code","9ceebef7":"code","52c6741e":"markdown"},"source":{"bd12f988":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fa719fc1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom datetime import timedelta\nimport math\n%matplotlib inline","19470fdd":"path = \"\/kaggle\/input\/covid19-global-forecasting-week-3\/\"","0a084cc2":"train_data = pd.read_csv(path+\"train.csv\")\n#train_data = train_data[(train_data.Country_Region==\"US\") ]\ntrain_df = train_data\n\ntrain_df['area'] = [str(i)+str(' - ')+str(j) for i,j in zip(train_data['Country_Region'], train_data['Province_State'])]\ntrain_df['Date'] = pd.to_datetime(train_df['Date'])\nfull_data = train_df","fadc98dc":"#train_df.tail(50)","684688fe":"today = full_data['Date'].max()+timedelta(days=1) \nprint (today)\nprint ('Last update of this dataset was ' + str(train_df.loc[len(train_df)-1]['Date']))\n","2f620b55":"def get_country_data(train_df, area, metric):\n    country_data = train_df[train_df['area']==area]\n    country_data = country_data.drop(['Id','Province_State', 'Country_Region'], axis=1)\n    country_data = pd.pivot_table(country_data, values=['ConfirmedCases','Fatalities'], index=['Date'], aggfunc=np.sum) \n    country_data = country_data[country_data[metric]!=0]\n    tmp = country_data.shift(periods=1,fill_value=0)\n    country_data['prior_confirmed'] = tmp.ConfirmedCases\n    country_data['prior_deaths'] = tmp.Fatalities\n    country_data['DailyConfirmed'] = country_data.ConfirmedCases - country_data.prior_confirmed\n    country_data['DailyFatalities'] = country_data.Fatalities - country_data.prior_deaths\n    return country_data        ","fbbdb887":"area_info = pd.DataFrame(columns=['area', 'cases_start_date', 'deaths_start_date', 'init_ConfirmedCases', 'init_Fatalities','init_DailyConfirmed','init_DailyFatalities'])\nfor i in range(len(train_df['area'].unique())):\n    area = train_df['area'].unique()[i]\n    area_cases_data = get_country_data(train_df, area, 'ConfirmedCases')\n    #print (area_cases_data)\n   \n    area_deaths_data = get_country_data(train_df, area, 'Fatalities')\n    cases_start_date = area_cases_data.index.min()\n    deaths_start_date = area_deaths_data.index.min()\n    if len(area_cases_data) > 0:\n        confirmed_cases = max(area_cases_data['ConfirmedCases'])\n        last = area_cases_data.tail(1)\n        last_daily = np.float(last['DailyConfirmed'])\n\n    else:\n        confirmed_cases = 0\n        last_daily = 0\n    if len(area_deaths_data) > 0:\n        fatalities = max(area_deaths_data['Fatalities'])\n        last = area_deaths_data.tail(1)\n        last_death = np.float(last['DailyFatalities'])\n    else:\n        fatalities = 0\n        last_death = 0\n    #print (last_daily)\n    #print (last_death)\n    area_info.loc[i] = [area, cases_start_date, deaths_start_date, confirmed_cases, fatalities,last_daily,last_death]\narea_info = area_info.fillna(pd.to_datetime(today))\narea_info['init_cases_day_no'] = pd.to_datetime(today)-area_info['cases_start_date']\narea_info['init_cases_day_no'] = area_info['init_cases_day_no'].dt.days.fillna(0).astype(int)\narea_info['init_deaths_day_no'] = pd.to_datetime(today)-area_info['deaths_start_date']\narea_info['init_deaths_day_no'] = area_info['init_deaths_day_no'].dt.days.fillna(0).astype(int)\n#area_info['init_DailyConfirmed'] = last_daily.astype(float)\n#area_info['init_DailyFatalities'] = last_death.astype(float)\narea_info.head()","0979bafb":"def make_cdf (y):\n    cdf = []\n    for i in range(1,len(y)+1): \n        total = np.sum(y[:i])\n        #print (total)\n        cdf.append(total)\n        #print (cdf)\n    return cdf","c2f37240":"from scipy.special import factorial\ndef gamma_pdf(x, k, lam, ymax):\n\n    k = np.float(k)\n    #print ('k is ' + str(k))\n    \n    num = ymax * (np.power(lam,k) * np.power(x,(k-1)) * np.exp(-lam*x))\n    if k < 0.5:\n        k = 1\n    else:\n         k = np.round(k)    \n    den = (factorial (k-1))\n    return num\/den\n\n    \ndef gamma_fit(train_df, area, metric,to_fit, est_count):\n    area_data = get_country_data(train_df, area, metric)\n    x_data = range(len(area_data.index))\n    y_data = area_data[to_fit]\n    x_data = np.array(x_data,dtype='float64')\n    y_data = np.array(y_data,dtype='float64')\n    #x_data = x_data.ravel()\n    #y_data = y_data.ravel()\n    #_data = np.asarray(x_data).ravel()\n    #y_data = np.asarray(y_data).ravel()\n    #print (y_data)\n    if len(y_data) < 5:\n        estimated_k = 6  \n        estimated_lam = 0.1 \n        ymax = np.float(est_count)\n    elif max(y_data) == 0:\n        estimated_k = 6  \n        estimated_lam = 0.1 \n        ymax = np.float(est_count)\n    else:\n        \n        p0_est=[6.0 ,0.1,est_count]\n        try:\n            popt, pcov = curve_fit(gamma_pdf, x_data, y_data,bounds=([0,0,0],100000000),p0=p0_est, maxfev=1000000)\n                                   #bounds=([0,0,0],100000000), p0=p0_est, maxfev=1000000)\n            estimated_k, estimated_lam, ymax = popt\n        except RuntimeError:\n            print(area)\n            print(\"Runtime Error - curve_fit failed\") \n            estimated_k = 6  \n            estimated_lam = 0.1 \n            ymax = est_count\n        #else:\n        #    print(area)\n        #    print(\"Catch all Error - curve_fit failed\") \n        #    estimated_k = 5  \n        #    estimated_lam = 0.1 \n        #    ymax = est_count\n\n    estimated_parameters = pd.DataFrame(np.array([[area, estimated_k, estimated_lam, ymax]]), columns=['area', 'k', 'lam', 'ymax'])\n    return estimated_parameters","52b1371e":"def get_parameters(metric, to_fit):\n    parameters = pd.DataFrame(columns=['area', 'k', 'lam', 'ymax'], dtype=np.float)\n    for area in train_df['area'].unique():\n        #print ('Area fitting is ' + area)\n        if metric == 'ConfirmedCases':\n            init = area_info[area_info.area == area]['init_ConfirmedCases']\n        else:\n            init = area_info[area_info.area == area]['init_Fatalities']\n        init = init.astype(float)\n        #print (init)\n        # establish an initial guess for maxy\n        est_count = init * 4.0\n        #print (est_count)\n        estimated_parameters = gamma_fit(train_df, area, metric, to_fit, est_count)\n        parameters = parameters.append(estimated_parameters)\n    if True:\n        try:\n            parameters['k'] = pd.to_numeric(parameters['k'], downcast=\"float\")\n            parameters['lam'] = pd.to_numeric(parameters['lam'], downcast=\"float\")\n            parameters['ymax'] = pd.to_numeric(parameters['ymax'], downcast=\"float\")\n        except RuntimeError: \n            print ('run time error')\n        except TypeError:\n            print ('type error')\n        #else:\n        #    print (\"error on parameter conversion\")\n        #parameters = parameters.replace({'k': {-1: parameters[parameters['ymax']>0].median()[0]}, \n        #                                 'lam': {-1: parameters[parameters['ymax']>0].median()[1]}, \n        #                                 'ymax': {-1: parameters[parameters['ymax']>0].median()[2]}})\n    return parameters","f47a4a8d":"cases_parameters = get_parameters('ConfirmedCases','DailyConfirmed')\ncases_parameters.tail(20)","96dbdc3a":"deaths_parameters = get_parameters('Fatalities','DailyFatalities')\ndeaths_parameters.tail(20)","98aaa6ce":"fit_df = area_info.merge(cases_parameters, on='area', how='left')\nfit_df = fit_df.rename(columns={\"k\": \"cases_k\", \"lam\": \"cases_lam\", \"ymax\": \"cases_ymax\"})\nfit_df = fit_df.merge(deaths_parameters, on='area', how='left')\nfit_df = fit_df.rename(columns={\"k\": \"deaths_k\", \"lam\": \"deaths_lam\", \"ymax\": \"deaths_ymax\"})\n\nfit_df.head()\n","66baa471":"test_data = pd.read_csv(path+\"test.csv\")\ntest_df = test_data.copy()\n#test_df = test_data[(test_data.Country_Region==\"US\") & (test_data.Province_State != 'x')].copy()\ntest_df['area'] = [str(i)+str(' - ')+str(j) for i,j in zip(test_df['Country_Region'], test_df['Province_State'])]\n\ntest_df = test_df.merge(fit_df, on='area', how='left')\ntest_df = test_df.merge(cases_parameters, on='area', how='left')\n#print (len(test_df))\n\n#test_df = test_df.rename(columns={\"k\": \"cases_k\", \"lam\": \"cases_lam\", \"ymax\": \"cases_ymax\"})\n#test_df = test_df.merge(deaths_parameters, on='area', how='left')\n#test_df = test_df.rename(columns={\"k\": \"deaths_k\", \"lam\": \"deaths_lam\", \"ymax\": \"deaths_ymax\"})\n#test_df.cases_k = test_df.cases_k.astype(float)\n#test_df.deaths_k = test_df.deaths_k.astype(float)\n#for i,t in test_df.iterrows():\n#    print (i)\n#    print (t.area)\n#    print (t.cases_k)\ntest_df['Date'] = pd.to_datetime(test_df['Date'])\ntest_df['cases_start_date'] = pd.to_datetime(test_df['cases_start_date'])\ntest_df['deaths_start_date'] = pd.to_datetime(test_df['deaths_start_date'])\n\ntest_df['cases_day_no'] = test_df['Date']-test_df['cases_start_date']\ntest_df['cases_day_no'] = test_df['cases_day_no'].dt.days.fillna(0).astype(int)\ntest_df['deaths_day_no'] = test_df['Date']-test_df['deaths_start_date']\ntest_df['deaths_day_no'] = test_df['deaths_day_no'].dt.days.fillna(0).astype(int)\ntest_df['DailyFatalities_fit'] = 0\n\n","a6e6941e":"fit_df[(fit_df.area>'US') & (fit_df.area < 'UT')]","eb3dc0c1":"#y = gamma_pdf(d, ['cases_k']), t['cases_lam'], t['cases_ymax'])\npred_yd = []\npred_yc = []\nfor (idx, df) in test_df.iterrows():\n    #print('for death day ' + str(df['deaths_day_no']))\n    y  = gamma_pdf(df['deaths_day_no'], df['deaths_k'], df['deaths_lam'], df['deaths_ymax'])\n    #print (y)\n    pred_yd.append([df.area,y])\n    #print ('for confirmed day ' + str(df['cases_day_no']))\n    yc = gamma_pdf(df['cases_day_no'], df['cases_k'], df['cases_lam'], df['cases_ymax'])\n    pred_yc.append([df.area,yc])\n    #test_df['DailyCases_pred'] = round(test_df['DailyConfirmed_fit']+test_df['DailyConfirmed_error'])\n\n    #test_df['DailyFatalities_pred'] = round(test_df['DailyFatalities_fit']+test_df['DailyFatalities_error'])\n","88efe8ba":"yd_df = pd.DataFrame( pred_yd)\nyc_df = pd.DataFrame( pred_yc)","725f8839":"yc_df.columns = ['Area','Predicted']\nyd_df.columns = ['Area','Predicted']","d99cf57a":"def make_pred(df):\n    cdf_all = pd.DataFrame()\n    for a in df['Area'].unique():\n        tmp = df[df.Area==a]\n        cdf = make_cdf (tmp.Predicted)\n        cdf = pd.DataFrame(cdf)\n        cdf_all = pd.concat([cdf_all, cdf])\n    return cdf_all\n\ncdfc = make_pred(yc_df)\ncdfd = make_pred(yd_df)\ncdfc.columns =['Pred']\ncdfd.columns =['Pred']\n\n\n    ","18dba3a2":"test_df['DailyFatalities_fit'] = cdfd.Pred.values\ntest_df['DailyCases_fit'] = cdfc.Pred.values","fac7e039":"# generate submission\nsubmission = pd.DataFrame(data={'ForecastId': test_df['ForecastId'], 'ConfirmedCases': test_df['DailyCases_fit'], 'Fatalities': test_df['DailyFatalities_fit']}).fillna(0.5)\nsubmission.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)","9ceebef7":"submission.head()","52c6741e":"## Gamma Curve Fitting - Overview of This Approach\n\nThis notebook is a followup to some previous effort of mine and Daner Ferhadi, https:\/\/www.kaggle.com\/dferhadi.\nSee:\n* https:\/\/www.kaggle.com\/dferhadi\/logistic-curve-fit-parameter-tuning\n* https:\/\/www.kaggle.com\/wjholst\/covid-19-growth-patterns-in-critical-countries\n* https:\/\/www.youtube.com\/watch?v=Kas0tIxDvrg&t=35s \n\n\nBoth of us used a logistic model to both predict and to identify critial inflection points in the growth model. You can observer from early track of the virus growth, that the initial rate is exponential. Eventually the curve tends to flatten and turn down. That is when the curve begins to take on the sigmoid properties.\n\nHowever, when you observe these events over time, the probability distribution does not look normal but rather skewed with a long right tail. That is why this model uses a gamma pdf, which can be tuned to more realistically fit the actual distributions. \n\nThis approach is not a machine learning effort, but rather employs the Python curve_fit library to find the closest fit for each population group. I use Daner's code base and use the gamma function to formulate the predictions.\n\n\n\n"}}