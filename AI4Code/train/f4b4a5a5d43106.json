{"cell_type":{"479072b9":"code","222cc519":"code","3fcad3cf":"code","2ea54f2c":"code","2b2f69c6":"code","a4287a69":"code","bbc3f2be":"code","4048de31":"code","82ce5b67":"code","488ac6e4":"code","d0ff8f87":"code","6c348602":"code","b8cce1ed":"code","61219328":"code","5fd61223":"code","b3d29263":"code","58640777":"code","1fe202c2":"code","d5c19739":"code","0b929f6c":"code","070bfda6":"code","1a5645ee":"code","1d8316ed":"code","1620fe81":"code","5c3d74fa":"code","aed86f52":"code","034234c5":"code","2b0ef7c4":"code","cb13e479":"code","cf5f0a2a":"code","d7458bee":"code","a6146cbb":"code","474a7df8":"code","eacabed6":"code","77e68bfe":"code","962a4b77":"code","85ed8238":"code","bd2f6e7c":"code","3f534352":"code","6899621a":"markdown","9f99fc61":"markdown","f80df691":"markdown","7a342d83":"markdown","7ee9537b":"markdown","1782bb7c":"markdown","e7c2440d":"markdown","9ff72f0d":"markdown"},"source":{"479072b9":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\n\nimport gc\ngc.enable()","222cc519":"BATCH_SIZE = 32\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.5, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1, 1)]\nROBERTA_PATH = \"..\/input\/roberta-transformers-pytorch\/roberta-base\"\nTOKENIZER_PATH = \"..\/input\/roberta-transformers-pytorch\/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nDEVICE","3fcad3cf":"test_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")","2ea54f2c":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","2b2f69c6":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","a4287a69":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.25,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","bbc3f2be":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","4048de31":"NUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\n\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in tqdm(range(NUM_MODELS)):            \n    model_path = f\"..\/input\/commonlit-roberta-0467\/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    all_predictions[model_index] = predict(model, test_loader)\n            \n    del model\n    gc.collect()","82ce5b67":"model1_predictions = all_predictions.mean(axis=0)","488ac6e4":"ROBERTA_PATH = \"..\/input\/pre-trained-roberta-solution-in-pytorch\"\nTOKENIZER_PATH = \"..\/input\/pre-trained-roberta-solution-in-pytorch\"\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\nNUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\n\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in tqdm(range(NUM_MODELS)):            \n    model_path = f\"..\/input\/pre-trained-roberta-solution-in-pytorch\/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    all_predictions[model_index] = predict(model, test_loader)\n            \n    del model\n    gc.collect()","d0ff8f87":"mdp = all_predictions.mean(axis=0)","6c348602":"test = test_df\n\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import RobertaConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaModel\nfrom IPython.display import clear_output","b8cce1ed":"def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '')\n    tok = tokenizer.encode_plus(\n        data, \n        max_length=max_len, \n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids'])\n    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length)\n    return curr_sent\n\nclass DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","61219328":"class CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        \n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        \n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        \n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n            \n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        \n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n                \n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n                \n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits \/= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        \n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output","5fd61223":"def make_model(model_name, num_labels=1):\n    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    config = RobertaConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n):\n    \n    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n    test_sampler = SequentialSampler(test_dataset)\n    \n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size \/\/ 2, \n        sampler=test_sampler, \n        pin_memory=False, \n        drop_last=False, \n        num_workers=0\n    )\n\n    return test_loader","b3d29263":"class Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n\n    def evaluate(self, data_loader, tokenizer):\n        preds = []\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids']\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids\n                    )\n                \n                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n                preds += logits\n        return preds\n\ndef config(fold, model_name, load_model_path):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    \n    max_len = 250\n    batch_size = 8\n\n    model, tokenizer = make_model(\n        model_name=model_name, \n        num_labels=1\n    )\n    \n    model.load_state_dict(\n        torch.load(f'{load_model_path}\/model{fold}.bin')\n    )\n    \n    test_loader = make_loader(\n        test, tokenizer, max_len=max_len,\n        batch_size=batch_size\n    )\n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n    return (\n        model, tokenizer, \n        test_loader, scaler\n    )","58640777":"def run(fold=0, model_name=None, load_model_path=None):\n    model, tokenizer, \\\n        test_loader, scaler = config(fold, model_name, load_model_path)\n    \n    import time\n    wtf = \"Variations of the delayed-write policy differ in when modified data blocks\\\n    are flushed to the server. One alternative is to flush a block when it is about to\\\n    be ejected from the client\u2019s cache. This option can result in good performance,\\\n    but some blocks can reside in the client\u2019s cache a long time before they are\\\n    written back to the server. A compromise between this alternative and the\\\n    write-through policy is to scan the cache at regular intervals and to flush\\\n    blocks that have been modified since the most recent scan, just as UNIX scans\\\n    its local cache. Sprite uses this policy with a 30-second interval. NFS uses the\\\n    policy for file data, but once a write is issued to the server during a cache\\\n    flush, the write must reach the server \u2019s disk before it is considered complete.\\\n    NFS treats metadata (directory data and file-attribute data) differently. Any\\\n    metadata changes are issued synchronously to the server. Thus, file-structure\\\n    loss and directory-structure corruption are avoided when a client or the server\\\n    crashes.\"\n\n    evaluator = Evaluator(model, scaler)\n\n    test_time_list = []\n\n    torch.cuda.synchronize()\n    tic1 = time.time()\n\n    preds = evaluator.evaluate(test_loader, tokenizer)\n\n    torch.cuda.synchronize()\n    tic2 = time.time() \n    test_time_list.append(tic2 - tic1)\n    \n    del model, tokenizer, test_loader, scaler\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return preds","1fe202c2":"pred_df1 = pd.DataFrame()\n#pred_df2 = pd.DataFrame()\npred_df3 = pd.DataFrame()\n\nfor fold in tqdm(range(5)):\n    pred_df1[f'fold{fold}'] = run(fold%5, '..\/input\/roberta-transformers-pytorch\/roberta-base\/', '..\/input\/commonlit-roberta-base-i\/')\n    #pred_df2[f'fold{fold+5}'] = run(fold%5, '..\/input\/roberta-transformers-pytorch\/roberta-large', '..\/input\/roberta-large-itptfit\/')\n    pred_df3[f'fold{fold+10}'] = run(fold%5, '..\/input\/roberta-transformers-pytorch\/roberta-large', '..\/input\/commonlit-roberta-large-ii\/')","d5c19739":"pred_df1 = np.array(pred_df1)\n#pred_df2 = np.array(pred_df2)\npred_df3 = np.array(pred_df3)\n\nmodel2_predictions = (pred_df1.mean(axis=1) * 0.6) + (pred_df3.mean(axis=1) * 0.4)","0b929f6c":"import os\nimport numpy as np\nimport pandas as pd\nimport random\n\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, TensorDataset, SequentialSampler, RandomSampler, DataLoader\n\nfrom tqdm.notebook import tqdm\n\nimport gc; gc.enable()\nfrom IPython.display import clear_output\n\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nlogging.set_verbosity_error()","070bfda6":"INPUT_DIR = '..\/input\/commonlitreadabilityprize'\nMODEL_DIR = '..\/input\/roberta-transformers-pytorch\/roberta-large'\nCHECKPOINT_DIR = '..\/input\/clrp-mean-pooling\/'\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nMAX_LENGTH = 248\nTEST_BATCH_SIZE = 1\nHIDDEN_SIZE = 1024\n\nNUM_FOLDS = 5\nSEEDS = [113]\n\ntest = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))","1a5645ee":"class MeanPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings \/ sum_mask\n        logits = self.linear(mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds","1d8316ed":"def get_test_loader(data):\n\n    x_test = data.excerpt.tolist()\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\n    encoded_test = tokenizer.batch_encode_plus(\n        x_test, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n\n    dataset_test = TensorDataset(\n        encoded_test['input_ids'],\n        encoded_test['attention_mask']\n    )\n\n    dataloader_test = DataLoader(\n        dataset_test,\n        sampler = SequentialSampler(dataset_test),\n        batch_size=TEST_BATCH_SIZE\n    )\n    \n    return dataloader_test\n\ntest_dataloader = get_test_loader(test)","1620fe81":"all_predictions = []\nfor seed in SEEDS:\n    \n    fold_predictions = []\n    \n    for fold in tqdm(range(NUM_FOLDS)):\n        model_path = f\"model_{seed + 1}_{fold + 1}.pth\"\n        \n        print(f\"\\nUsing {model_path}\")\n        \n        model_path = CHECKPOINT_DIR + f\"model_{seed + 1}_{fold + 1}.pth\"\n        model = MeanPoolingModel(MODEL_DIR)\n        model.load_state_dict(torch.load(model_path)) \n        model.to(DEVICE)\n        model.eval()\n\n        predictions = []\n        for batch in test_dataloader:\n\n            batch = tuple(b.to(DEVICE) for b in batch)\n\n            inputs = {'input_ids':      batch[0],\n                      'attention_mask': batch[1],\n                      'labels':         None,\n                     }\n\n     \n            preds = model(**inputs).item()\n            predictions.append(preds)\n            \n        del model \n        gc.collect()\n            \n        fold_predictions.append(predictions)\n    all_predictions.append(np.mean(fold_predictions, axis=0).tolist())\n    \nmodel3_predictions = np.mean(all_predictions,axis=0)","5c3d74fa":"import os\nfrom pathlib import Path\nfrom torch.utils.data import RandomSampler, SequentialSampler, Sampler\nfrom torch.utils.data import Dataset, DataLoader\n\ndef convert_examples_to_features(text, tokenizer, max_len):\n\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\nclass CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.tolist()\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt = self.excerpts[item]\n            label = self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\n\nclass AttentionHead(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size*4)\n        self.linear = nn.Linear(self.h_size*2, 1)\n        self.linear_out = nn.Linear(self.h_size*8, 1)\n\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n       \n        all_hidden_states = torch.stack(transformer_out.hidden_states)\n        cat_over_last_layers = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n        )\n        \n        cls_pooling = cat_over_last_layers[:, 0]   \n        head_logits = self.head(cat_over_last_layers)\n        y_hat = self.linear_out(torch.cat([head_logits, cls_pooling], -1))\n        \n        return y_hat\n\ndef predic(data, in_folder_path):\n    in_folder_path = Path(in_folder_path)\n    mp = \"Protocol stacks, as specified by network layering models, add information\\\n    to a message to ensure that it reaches its destination. A naming system (such\\\n    as DNS) must be used to translate from a host name to a network address, and\\\n    another protocol (such as ARP) may be needed to translate the network number\\\n    to a network device address (an Ethernet address, for instance). If systems are\\\n    located on separate networks, routers are needed to pass packets from source\\\n    network to destination network.\\\n    There are many challenges to overcome for a distributed system to work\\\n    correctly. Issues include naming of nodes and processes in the system, fault\\\n    tolerance, error recovery, and scalability.\\\n    A DFS is a file-service system whose clients, servers, and storage devices\\\n    are dispersed among the sites of a distributed system. Accordingly, service\\\n    activity has to be carried out across the network; instead of a single centralized\\\n    data repository, there are multiple independent storage devices.\\\n    Ideally, a DFS should look to its clients like a conventional, centralized\\\n    file system. The multiplicity and dispersion of its servers and storage devices\\\n    should be transparent. A transparent DFS facilitates client mobility by bringing\\\n    the client\u2019s environment to the site where the client logs in.\\\n    There are several approaches to naming schemes in a DFS. In the simplest\\\n    approach, files are named by some combination of their host name and local\\\n    name, which guarantees a unique system-wide name. Another approach,\\\n    popularized by NFS, provides a means to attach remote directories to local\\\n    directories, thus giving the appearance of a coherent directory tree.\\\n    Requests to access a remote file are usually handled by two complementary\\\n    methods. With remote service, requests for accesses are delivered to the server.\\\n    The server machine performs the accesses, and the results are forwarded back\\\n    to the client. With caching, if the data needed to satisfy the access request are\\\n    not already cached, then a copy of the data is brought from the server to the\\\n    client. Accesses are performed on the cached copy. The problem of keeping the\\\n    cached copies consistent with the master file is the cache-consistency problem.\\\n    Practice Exercises\\\n    17.1 Why would it be a bad idea for gateways to pass broadcast packets\\\n    between networks? What would be the advantages of doing so?\\\n    17.2 Discuss the advantages and disadvantages of caching name transla-\\\n        tions for computers located in remote domains.\\\n    17.3 What are the advantages and disadvantages of using circuit switching?\\\n    For what kinds of applications is circuit switching a viable strategy?\\\n    17.4 What are two formidable problems that designers must solve to\\\n    implement a network system that has the quality of transparency?\"\n\n    \n    models_folder_path = Path(in_folder_path \/ 'models')\n    models_preds = []\n    \n    for model_num in range(5):\n        \n        tokenizer = AutoTokenizer.from_pretrained(in_folder_path)\n            \n        print(f'Inference#{model_num+1}\/5')\n        test_ds = CLRPDataset(data=data, tokenizer=tokenizer, max_len=256, is_test=True)\n        test_sampler = SequentialSampler(test_ds)\n        test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=4)\n        model = torch.load(models_folder_path \/ f'best_model_{model_num}.pt').to('cuda')\n        print(f'Model {models_folder_path} \/ best_model_{model_num}.pt is pushed to Device')\n\n        all_preds = []\n        model.eval()\n\n        for step,batch in enumerate(test_dataloader):\n            sent_id, mask = batch['input_ids'].to('cuda'), batch['attention_mask'].to('cuda')\n            with torch.no_grad():\n                preds = model(sent_id, mask)\n                all_preds += preds.flatten().cpu().tolist()\n\n        models_preds.append(all_preds)\n        del model, tokenizer, test_dataloader, test_sampler, test_ds\n        gc.collect()\n        torch.cuda.empty_cache()\n    return np.array(models_preds).mean(axis = 0)","aed86f52":"pd1 = predic(test_df, '..\/input\/comdbli')","034234c5":"pd1","2b0ef7c4":"class CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size*4)\n        self.linear = nn.Linear(self.h_size*8, self.h_size \/\/ 2)\n        self.linear_out = nn.Linear(self.h_size \/\/ 2, 1)\n        self.tanh = nn.Tanh()\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n       \n        all_hidden_states = torch.stack(transformer_out.hidden_states)\n        cat_over_last_layers = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n        )\n        \n        cls_pooling = cat_over_last_layers[:, 0]   \n        head_logits = self.head(cat_over_last_layers)\n        logits = self.tanh(self.linear(torch.cat([head_logits, cls_pooling], -1)))\n        y_hat = self.linear_out(logits)\n        \n        return y_hat\n","cb13e479":"pd3 = predic(test_df, '..\/input\/dbx777')","cf5f0a2a":"pd3","d7458bee":"models_folder_path = Path('..\/input\/notebook6921fb919e\/models')\nmodels_preds = []\n    \nfor model_num in range(5):\n        \n    tokenizer = AutoTokenizer.from_pretrained('..\/input\/comdbli')\n    data = test_df\n    print(f'Inference#{model_num+1}\/5')\n    test_ds = CLRPDataset(data=data, tokenizer=tokenizer, max_len=256, is_test=True)\n    test_sampler = SequentialSampler(test_ds)\n    test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=4)\n    model = torch.load(models_folder_path \/ f'best_model_{model_num}.pt').to('cuda')\n    print(f'Model {models_folder_path} \/ best_model_{model_num}.pt is pushed to Device')\n\n    all_preds = []\n    model.eval()\n\n    for step,batch in enumerate(test_dataloader):\n        sent_id, mask = batch['input_ids'].to('cuda'), batch['attention_mask'].to('cuda')\n        with torch.no_grad():\n            preds = model(sent_id, mask)\n            all_preds += preds.flatten().cpu().tolist()\n\n    models_preds.append(all_preds)\n    del model, tokenizer, test_dataloader, test_sampler, test_ds\n    gc.collect()\n    torch.cuda.empty_cache()\npd4 = np.array(models_preds).mean(axis = 0)","a6146cbb":"pd4","474a7df8":"class CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size*4)\n        self.linear = nn.Linear(self.h_size*2, 1)\n        self.linear_out = nn.Linear(self.h_size*8, 1)\n\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n       \n        all_hidden_states = torch.stack(transformer_out.hidden_states)\n        cat_over_last_layers = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n        )\n        \n        cls_pooling = cat_over_last_layers[:, 0]   \n        head_logits = self.head(cat_over_last_layers)\n        y_hat = self.linear_out(torch.cat([head_logits, cls_pooling], -1))\n        \n        return y_hat\ndef predic2(data, in_folder_path):\n    in_folder_path = Path(in_folder_path)\n\n    tp = \"In the final part of the book, we integrate the concepts described earlier\\\n    by examining real operating systems. We cover two such systems in\\\n    detail \u2014 Linux and Windows 7. We chose Linux for several reasons: it is\\\n    popular, it is freely available, and it represents a full-featured UNIX system.\\\n    This gives a student of operating systems an opportunity to read \u2014 and\\\n    modify \u2014 real operating-system source code.\\\n    We also cover Windows 7 in detail. This recent operating system from\\\n    Microsoft is gaining popularity not only in the standalone-machine market\\\n    but also in the workgroup \u2013 server market. We chose Windows 7 because\\\n    it provides an opportunity to study a modern operating system that has\\\n    a design and implementation drastically different from those of UNIX.\\\n    In addition, we briefly discuss other highly influential operating sys-\\\n    tems. Finally, we provide on-line coverage of two more systems: FreeBSD\\\n    and Mach. The FreeBSD system is another UNIX system. However,\\\n    whereas Linux combines features from several UNIX systems, FreeBSD\\\n    is based on the BSD model. FreeBSD source code, like Linux source\\\n    code, is freely available. Mach is a modern operating system that provides\\\n    compatibility with BSD UNIX.\"\n    \n    models_folder_path = Path(in_folder_path \/ 'models')\n    models_preds = []\n    \n    for model_num in range(5):\n        \n        tokenizer = torch.load('..\/input\/tokenizers\/roberta-tokenizer.pt')\n            \n        print(f'Inference#{model_num+1}\/5')\n        test_ds = CLRPDataset(data=data, tokenizer=tokenizer, max_len=256, is_test=True)\n        test_sampler = SequentialSampler(test_ds)\n        test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=4)\n        model = torch.load(models_folder_path \/ f'best_model_{model_num}.pt').to('cuda')\n        print(f'Model {models_folder_path} \/ best_model_{model_num}.pt is pushed to Device')\n\n        all_preds = []\n        model.eval()\n\n        for step,batch in enumerate(test_dataloader):\n            sent_id, mask = batch['input_ids'].to('cuda'), batch['attention_mask'].to('cuda')\n            with torch.no_grad():\n                preds = model(sent_id, mask)\n                all_preds += preds.flatten().cpu().tolist()\n\n        models_preds.append(all_preds)\n        del model, tokenizer, test_dataloader, test_sampler, test_ds\n        gc.collect()\n        torch.cuda.empty_cache()\n    return np.array(models_preds).mean(axis = 0)","eacabed6":"pd2 = predic2(test_df, '..\/input\/0463-robertalarge')","77e68bfe":"pd2","962a4b77":"predictions = ((model1_predictions + mdp + pred_df1.mean(axis=1) + pred_df3.mean(axis=1) + model3_predictions)\/5 + ((pd1 + pd4 + pd3)\/3 + pd2)\/2)\/2\npredictions","85ed8238":"results = pd.DataFrame(np.vstack((model1_predictions, model2_predictions, model3_predictions, predictions)).transpose(), \n                       columns=['model1','model2','model3','ensemble'])\n\nresults","bd2f6e7c":"submission_df.target = predictions\nsubmission_df","3f534352":"submission_df.to_csv(\"submission.csv\", index=False)","6899621a":"# Inference","9f99fc61":"# Model 2\nInspired from: [https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3](https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3)","f80df691":"# Dataset","7a342d83":"# Model 1\nInspired from: https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm","7ee9537b":"# Overview\n\nThis notebook combines three models.","1782bb7c":"## Model 3 \n\nInspired from: https:\/\/www.kaggle.com\/jcesquiveld\/best-transformer-representations","e7c2440d":"# Inference","9ff72f0d":"# Dataset"}}