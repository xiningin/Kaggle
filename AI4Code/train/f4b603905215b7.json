{"cell_type":{"20d7544c":"code","20965138":"code","89f931ec":"code","d8134a47":"code","3b52b7a3":"code","0fd8ec8f":"code","b169ee14":"code","8d42e5a9":"code","69d4d285":"code","2114c2ef":"code","91717bc1":"code","626afaed":"code","2fa31f6a":"code","0cad327f":"code","cddc6db4":"code","91d0760e":"code","e798278a":"code","8810c79a":"code","1b37ea96":"code","6179e9ff":"code","51dac4e7":"code","a69e4bf0":"markdown","bcd31635":"markdown","a26826bc":"markdown","153555ff":"markdown","cb54fbb4":"markdown","1a22cc0e":"markdown","7927317a":"markdown","9b41774c":"markdown","378eb5d7":"markdown","e10a8a55":"markdown","438cbcff":"markdown","1b7d4f68":"markdown","a853fb6c":"markdown","20b9d277":"markdown"},"source":{"20d7544c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom ipywidgets import interact,  FloatSlider\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\n\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import SMOTE","20965138":"pd.options.display.max_columns = 100\npd.options.display.max_rows = 100\npd.options.display.width=100\nplt.style.use('ggplot')","89f931ec":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","d8134a47":"df.shape","3b52b7a3":"df.isnull().sum().sum()","0fd8ec8f":"df.describe()","b169ee14":"df.dtypes","8d42e5a9":"diff_class = df['Class'].value_counts()\ndiff_class.plot(kind='bar', color=['m', 'k'], figsize=(5, 5))\nplt.xticks(range(2), ['Normal  [0]', 'Fraud  [1]'], rotation=0)\nfor i, v in enumerate(diff_class):\n    plt.text(i-0.1, v+3000, str(v))\nplt.title('Class Count')\nplt.show()","69d4d285":"ss = StandardScaler()\ndf['Amount'] = ss.fit_transform(df[['Amount']])\ndf['Time'] = ss.fit_transform(df[['Time']])","2114c2ef":"for var in df.columns[:-1]:\n    \n    sns.boxplot(df[var], hue=df['Class'], palette='Set3')\n    mean = df[var].mean()\n    std = df[var].std()\n    plt.axvline(mean - 3 * std, 0, 1)\n    plt.text(mean - 3 * std, -0.55, 'mean - 3* std', rotation=60)\n    plt.axvline(mean + 3 * std, 0, 1)\n    plt.text(mean + 3 * std, -0.55, 'mean + 3* std', rotation=60)\n    \n\n    plt.show()\n    ","91717bc1":"X_train, X_test, y_train, y_test = train_test_split(df.drop('Class', axis=1), df[['Class']].values, test_size=0.3,random_state=1997)","626afaed":"lr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)","2fa31f6a":"def plot_confusion_matrix(y_test, y_pred):\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    labels_name = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n    labels_count = [value for value in cf_matrix.flatten()]\n    labels_percentage = [ \"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/ np.sum(cf_matrix)]\n\n    labels = [f'{x}\\n {y} \\n{z}' for x, y, z in zip(labels_name, labels_count, labels_percentage)]\n    labels = np.array(labels, dtype=str).reshape(2,2)\n\n    recall = cf_matrix[1,1]\/(cf_matrix[1,0] + cf_matrix[1,1])\n    precision = cf_matrix[1,1]\/(cf_matrix[0,1] + cf_matrix[1,1])\n    accuracy = (cf_matrix[0, 0] + cf_matrix[1,1])\/ np.sum(cf_matrix)\n    f1_score = (2*precision*recall)\/(precision + recall)\n\n    stats = '\\n\\n Recall:   {0:.03}\\n Precision:   {1:.03}\\n Accuracy:  {2:.03}\\nF1-Score:  {3:.03}'.format(recall, precision, accuracy, f1_score)\n\n    sns.heatmap(cf_matrix, annot=labels, fmt='', center=3, linewidth=3, linecolor='k', cbar=False)\n    plt.title('Confusion matrix\\n', fontsize=20)\n    plt.xlabel('Predicted Label'+stats, fontsize=14)\n    plt.ylabel('True Label', fontsize=14)\n\n    plt.show()\nplot_confusion_matrix(y_test, y_pred)","0cad327f":"y_prob = lr.predict_proba(X_test)\ny_prob = y_prob[:, 1] # Probability of getting the output 1 (Fraud)","cddc6db4":"fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n# gmeans = np.sqrt(tpr*(1-fpr))\n# ix = np.argmax(gmeans)\n# print(\"Best thresholds=%f, G-Mean=%.3f\" %(thresholds[ix], gmeans[ix]))\n\nplt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\nplt.plot(fpr, tpr, marker='.', label='Logistic')\n# plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='best', sizes=(200, 100))\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.title('TPR vs FPR', fontsize=20)\nplt.show()","91d0760e":"lr_precision, lr_recall, lr_thresholds = precision_recall_curve(y_test, y_prob)\nno_skill = len(y_test[y_test==1])\/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\nplt.plot(lr_precision, lr_recall, marker='.', label='Logistic')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\nplt.title('Precision vs Recall', fontsize=20)\nplt.show()","e798278a":"print(\"Slide, Range -> (0.001, 0.04)\")\ndef update(var=0.004):\n    print(\"y_prob should be greater than >\", var)\n    predict_mine = np.where(y_prob > var, 1, 0)\n    plot_confusion_matrix(y_test, predict_mine)\n\ninteract(update, var=FloatSlider(min=0.001, max=0.04, step=0.001))","8810c79a":"lr_b = LogisticRegression(max_iter=1000, class_weight='balanced')\nlr_b.fit(X_train, y_train)\ny_pred_b = lr_b.predict(X_test)\nplot_confusion_matrix(y_test, y_pred_b)","1b37ea96":"tl = TomekLinks(sampling_strategy='majority')\nX_train_tl, y_train_tl = tl.fit_sample(X_train, y_train)\nlr_tl = LogisticRegression(max_iter=1000, class_weight='balanced')\nlr_tl.fit(X_train_tl, y_train_tl)\ny_pred_tl = lr_tl.predict(X_test)\nplot_confusion_matrix(y_test, y_pred_tl)","6179e9ff":"smote  = SMOTE(sampling_strategy='minority')\nX_train_sm, y_train_sm = smote.fit_sample(X_train, y_train)","51dac4e7":"lr_sm = LogisticRegression(max_iter=1000)\nlr_sm.fit(X_train_sm, y_train_sm)\ny_pred_sm = lr_sm.predict(X_test)\nplot_confusion_matrix(y_test, y_pred_sm)","a69e4bf0":"predict_proba gives you the probabilities for the target (0 and 1 in this case) in array form. The number of probabilities for each row is equal to the number of categories in target variable. ","bcd31635":"# 6. Synthetic Minority Oversampling Technique( SMOTE)","a26826bc":"\n- __Upper Left Square__: The amount of correctly classified by model of no fraud transactions.\n- __Upper Right Square__: The amount of incorrectly classified transactions as fraud cases, but the actual label is no fraud .\n- __Lower Left Square__: The amount of incorrectly classified transactions as no fraud cases, but the actual label is fraud .\n- __Lower Right Square__: The amount of correctly classified by our model of fraud transactions.\n\n\n- *Recall*: Out of all the positive classes, how much we predicted correctly.  (TP\/TP+FN)\n- *Precision*: Out of all the positive classes we have predicted, how many are actually positive. (TP\/TP+FP)\n- *Accuracy*: Out of all the classes, how much we predicted correctly \n- *F-measure*: 2(Recall) (Precision)\/(Recall + Precision)","153555ff":" # 1. Logistic Regression","cb54fbb4":"# 5. Tomek Link","1a22cc0e":"# 4. Logistic Regression with balanced class weight","7927317a":"In this Kernel, I tried different methods for Credit Card Fraud Detection. \n![image.png](attachment:image.png)\nCheck the last Cell for knowledge sources.","9b41774c":"![image.png](attachment:image.png)","378eb5d7":"# 3. Change the Threshold","e10a8a55":"![image.png](attachment:image.png)","438cbcff":"# 2. Predict Proba","1b7d4f68":"1. https:\/\/www.svds.com\/learning-imbalanced-classes\/\n2. https:\/\/stackoverflow.com\/questions\/30972029\/how-does-the-class-weight-parameter-in-scikit-learn-work\n3. https:\/\/stackoverflow.com\/questions\/34831676\/how-to-perform-undersampling-the-right-way-with-python-scikit-learn\n4. https:\/\/www.xenonstack.com\/wp-content\/uploads\/xenonstack-credit-card-fraud-detection.png","a853fb6c":"Split the data in training and test set. ","20b9d277":"Distribution of different columns."}}