{"cell_type":{"1098b158":"code","9261817d":"code","0ae11ac1":"code","29c1634e":"code","427491b8":"code","2f074e8d":"code","0c754d1c":"code","21b4c2c8":"code","b0fbe0ac":"code","6cdc6716":"code","7363dcad":"code","77a1942f":"code","3346bd77":"code","ab45eb2c":"code","61763c73":"code","f8b8bd59":"code","b4cc7e61":"code","c995d30d":"code","d73d0115":"code","7703cc0f":"code","9d9a0592":"markdown","05285cd9":"markdown","a535119c":"markdown","2ee792aa":"markdown","18b544b5":"markdown","7cc69fb9":"markdown","65669355":"markdown","69df6a82":"markdown","8a2921d9":"markdown","ff732d55":"markdown","0f215c64":"markdown"},"source":{"1098b158":"import numpy as np\nimport pandas as pd\n\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, roc_auc_score\nfrom pandas.plotting import scatter_matrix\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nsns.set_style(\"dark\")\nimport warnings\nwarnings.filterwarnings('ignore')","9261817d":"dataset = pd.read_csv(\"..\/input\/iris-dataset\/iris.csv\")","0ae11ac1":"dataset.info()","29c1634e":"print(dataset.describe())\nprint(\"\\n\")\nprint(dataset.groupby('class').size())","427491b8":"#scatter_matrix\ndataset.columns\nsns.pairplot(dataset, hue='class', size=3);\nplt.suptitle('Visualizing Multidimensional Relationships')","2f074e8d":"dataset.columns\nsns.pairplot(dataset, size=3)\nplt.suptitle('Relationship between continuous variables')","0c754d1c":"fig, axs = plt.subplots(1, 4, figsize=(16, 4))\ncontinuous_features = list(dataset.columns)\ncontinuous_features.remove('class')\naxs = axs.ravel()\n\nfor ax, colname in zip(axs, continuous_features):\n    sns.distplot(dataset[colname], ax=ax)\n    \nplt.suptitle('Univariate analysis on continuous variables')","21b4c2c8":"g = sns.PairGrid(dataset, vars= ['sepal-length', 'sepal-width', 'petal-length','petal-width'], hue='class', palette='RdBu_r')\ng.map(plt.scatter, alpha=0.8)\ng.add_legend();\nplt.suptitle('Correlation between Variables and Class')","b0fbe0ac":"#create the correlation matrix heat map\nplt.figure(figsize=(10,6))\nsns.heatmap(dataset.corr(),linewidths=.1,cmap=\"YlGnBu\", annot=True)\nplt.yticks(rotation=0);\nplt.suptitle('Correlation Matrix')","6cdc6716":"sns.jointplot(dataset['sepal-length'], dataset['sepal-width'])\nplt.suptitle('Joint distribution between sepal-length & sepal-width')\nplt.show()\n\nsns.jointplot(dataset['petal-length'], dataset['petal-width'])\nplt.suptitle('Joint distribution between petal-length & petal-width')\nplt.show()","7363dcad":"sns.swarmplot(dataset['sepal-length'], dataset['sepal-width'], dataset['class'])\nplt.suptitle('Distribution between sepal-length & sepal-width with Class')\nplt.show()\n\nsns.swarmplot(dataset['petal-length'], dataset['petal-width'], dataset['class'])\nplt.suptitle('Distribution between petal-length & petal-width with Class')\nplt.show()","77a1942f":"sns.pointplot(dataset['sepal-length'], dataset['sepal-width'], dataset['class'])\nplt.suptitle('Central tendency between sepal-length & sepal-width with Class')\nplt.show()\n\nsns.pointplot(dataset['petal-length'], dataset['petal-width'], dataset['class'])\nplt.suptitle('Central tendency between petal-length & petal-width with Class')\nplt.show()","3346bd77":"#Split dataset\n\narray = dataset.values\nX = array[:, 0:4]\nY = array[:, 4]","ab45eb2c":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=1)\n\nmodel.fit(X, Y)\ny_pred = model.predict(X)\naccuracy_score(Y, y_pred)","61763c73":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\ncross_val_score(model, X, Y, cv=5)","f8b8bd59":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\ndef PolynomialRegression(degree=2, **kwargs):\n    return make_pipeline(PolynomialFeatures(degree),\n                         LinearRegression(**kwargs))","b4cc7e61":"def make_data(N, err=1.0, rseed=1):\n    # randomly sample the data\n    rng = np.random.RandomState(rseed)\n    X = rng.rand(N, 1) ** 2\n    Y = 10 - 1. \/ (X.ravel() + 0.1)\n    if err > 0:\n        Y += err * rng.randn(N)\n    return X, Y\n\nX, Y = make_data(40)","c995d30d":"X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n\nplt.scatter(X.ravel(), Y, color='black')\naxis = plt.axis()\nfor degree in [1, 3, 5]:\n    y_test = PolynomialRegression(degree).fit(X, Y).predict(X_test)\n    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\nplt.xlim(-0.1, 1.0)\nplt.ylim(-2, 12)\nplt.legend(loc='best');","d73d0115":"from sklearn.model_selection import validation_curve\ndegree = np.arange(0, 21)\ntrain_score, val_score = validation_curve(PolynomialRegression(), X, Y, 'polynomialfeatures__degree', degree, cv=5)\n\nplt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('degree')\nplt.ylabel('score');","7703cc0f":"#Learning Rate\nplt.scatter(X.ravel(), Y)\nlim = plt.axis()\ny_test = PolynomialRegression(3).fit(X, Y).predict(X_test)\nplt.plot(X_test.ravel(), y_test);\nplt.axis(lim);","9d9a0592":"# 2. Import dataset","05285cd9":"**A categorical scatterplot ith non-overlapping point**","a535119c":"# About Dataset\n[IRIS](https:\/\/images.app.goo.gl\/aedQ4WAimLPNZvPj6)","2ee792aa":"# 3. Exploratory data analysis of IRIS dataset","18b544b5":"Wait what!! Accuracy score of 1.0, which indicates that 100%!!!\nLet's do some Model validation via cross-validation.","7cc69fb9":"**Hope you liked this notebook, perhaps in future more updates will be added as per suggestions. Any suggestion will be highly appreciated.**","65669355":"**Context**\n\nThe Iris flower data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n\nThis dataset became a typical test case for many statistical classification techniques in machine learning such as support vector machines\n\n**Content**\n\nThe dataset contains a set of 150 records under 5 attributes - Petal Length, Petal Width, Sepal Length, Sepal width and Class(Species).","69df6a82":"# 1. Import Libraries","8a2921d9":"**The joint distribution between different datasets, along with the associated marginal distributions**","ff732d55":"**Showing point estimation (using error bars) and confidence intervals using scatter plot**","0f215c64":"Seems Overfitted??? Let's perform cross-validation to compute the validation curve for a class of model.\nRef. https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/"}}