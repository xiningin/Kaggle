{"cell_type":{"9cce8c39":"code","3ae01219":"code","e46e343b":"code","a815e6fa":"code","75685a3d":"code","3bed5851":"code","6521f917":"code","c7094d18":"code","39668330":"code","76608148":"code","a43e1579":"code","cfce41da":"code","1f597699":"code","7e3d2096":"code","1718ab92":"code","660bc0d4":"code","4a60dce9":"code","84c233fc":"code","4033c5dc":"code","2d9f8f28":"code","71275ef8":"code","f5eb9a9b":"code","6ab7aa54":"code","4eb78b33":"code","91587c4b":"code","88a5ae3e":"code","700efc31":"code","65baa93a":"code","a9b44160":"code","f88bc457":"code","89e2cedf":"code","0168634c":"code","d3822808":"code","b57800ab":"code","aaed4289":"code","00e6ebe3":"code","13982dc6":"code","6aaf8f99":"code","d9862a9b":"code","c5321ca3":"code","5709c23d":"code","7a9714af":"code","057bc42b":"code","7bf390da":"code","46c35ce1":"code","3a9a0b5f":"code","5aa40911":"code","cbaa599d":"markdown","a833054d":"markdown","95f0066b":"markdown","0d66a6d0":"markdown","fd5ae558":"markdown"},"source":{"9cce8c39":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC  \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport missingno\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","3ae01219":"traindata = pd.read_csv(\"..\/input\/mymusicalprefrences\/train.csv\")\ntestdata = pd.read_csv(\"..\/input\/mymusicalprefrences\/test.csv\")\n\nmusic_dataset = pd.concat([traindata, testdata]).reset_index(drop=True)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\npalette = ['#2596be',\"#e28743\",\"#80391e\"]\nsns.palplot(palette)","e46e343b":"music_dataset.head()","a815e6fa":"music_dataset.size","75685a3d":"missingno.bar(music_dataset, color=palette, figsize=(30,6))","3bed5851":"testdata.columns = [i.strip() for i in testdata.columns]\ntraindata.columns = [i.strip() for i in traindata.columns]","6521f917":"print(testdata.columns)","c7094d18":"print(traindata.columns)","39668330":"raw1 = traindata.drop(['Id'],axis=1)","76608148":"raw1.info()","a43e1579":"testdata.info()","cfce41da":"raw1.describe()","1f597699":"raw2 = raw1.drop(['Album_type','Version'],axis=1)\ntestdata=testdata.drop(['Album_type','Version'],axis=1)\ntestdata['Vocal'].fillna('N',inplace=True)\nraw2 = raw2.reset_index(drop=True)","7e3d2096":"raw3 = raw2.dropna()\nraw3","1718ab92":"print(raw3.columns.values)","660bc0d4":"resort_columns=['Category','Duration','Release_year', 'BPM','Energy','Dancebility', 'Happiness','Artists', 'Track',  'Artists_Genres', 'Album',\n       'Labels', 'Key', 'Vocal', 'Country']\nresort_columns_t=['Duration','Release_year', 'BPM','Energy','Dancebility', 'Happiness','Artists', 'Track',  'Artists_Genres', 'Album',\n       'Labels', 'Key', 'Vocal', 'Country']\nraw4=raw3[resort_columns]\ntestdata=testdata[resort_columns_t]","4a60dce9":"y_train=raw4['Category']\nraw4=pd.concat([raw4.drop(['Category'],axis=1),testdata],axis=0)","84c233fc":"print('Artists:',len(raw4['Artists'].unique()))\nprint('Track:',len(raw4['Track'].unique()))\nprint('Key:',len(raw4['Key'].unique()))\nprint('Artists_Genres:',len(raw4['Artists_Genres'].unique()))\nprint('Vocal:',raw4['Vocal'].unique())\nprint('Country:',len(raw4['Country'].unique()))","4033c5dc":"print(raw4['Track'].value_counts())\nprint(raw4['Artists'].value_counts())\nprint(raw4['Vocal'].value_counts())","2d9f8f28":"raw4 = raw4.drop(['Track'],axis=1)","71275ef8":"raw4[\"isMajor\"], raw4[\"Key\"] = raw4[\"Key\"].apply(lambda x: x.split(\" \")[1]), raw4[\"Key\"].apply(lambda x: x.split(\" \")[0])\nraw4.loc[:,\"Key\"] = raw4[\"Key\"].replace({\"D\u266d\": \"C#\", \"E\u266d\": \"D#\", \"G\u266d\": \"F#\", \"A\u266d\": \"G#\",\"B\u266d\":\"A#\"})\n\nraw4.loc[:,\"isMajor\"] = (raw4[\"isMajor\"]==\"Major\").astype(int)","f5eb9a9b":"keydum = pd.get_dummies(raw4[\"Key\"])\nkeydum = keydum.reset_index(drop=True)","6ab7aa54":"raw4 = raw4.reset_index(drop=True)\ndef voc2dum(key):\n    list_F=[]\n    list_M=[]\n    list_N=[]\n    for i in range(len(key)):\n        if 'F' in key[i]:\n            list_F.extend([1])\n        else:\n            list_F.extend([0])\n        # see major or not\n        if 'M' in key[i]:\n            list_M.extend([1])\n        else:\n            list_M.extend([0])\n        if 'N' in key[i]:\n            list_N.extend([1])\n        else:\n            list_N.extend([0])\n    dummydummy=pd.DataFrame({'VF':list_F,'VM':list_M,'VN':list_N})   \n    return dummydummy","4eb78b33":"vocdum=voc2dum(raw4['Vocal'])\nvocdum","91587c4b":"unique = []\nfor i in raw4.index:\n    unique.extend(raw4.loc[i,'Artists_Genres'].split(\"|\"))\n\nSuni = pd.Series(unique)\nprint(len(Suni.unique()))\nSuni = Suni.unique()","88a5ae3e":"def style2dum(form,col):\n    data=np.zeros((len(col),len(form)))\n    for i in range(len(form)):\n        for j in range(len(col)):\n            if form[i] in col[j]:\n                data[j][i]=1\n    quasidum = pd.DataFrame(data )\n    return quasidum","700efc31":"stydum = style2dum(Suni,raw4['Artists_Genres'])\nstydum.columns = Suni\nstydum.columns","65baa93a":"stydum","a9b44160":"den = stydum.sum(axis=1)\nimport math\n\nfor i in range(946):\n    for j in Suni:\n        stydum.loc[i,j]\/=math.sqrt(den[i])","f88bc457":"embedding = TSNE(n_components = 2, init = \"pca\")\nesd = embedding.fit_transform(stydum)\nesd = pd.DataFrame(esd, columns = [\"sty_tsne1\",\"sty_tsne2\"])","89e2cedf":"extdum = pd.get_dummies(raw4['Country'])\nembedding1 = TSNE(n_components=2, init=\"pca\")\nesd1 = embedding1.fit_transform(extdum)\nesd1 = pd.DataFrame(esd1,columns=[\"con_tsne1\",\"con_tsne2\"])\n","0168634c":"namedum = pd.get_dummies(raw4['Artists'])\nembedding2 = TSNE(n_components=3, init=\"pca\")\nesd2 = embedding1.fit_transform(namedum)\nesd2 = pd.DataFrame(esd1,columns=[\"name_tsne1\",\"name_tsne2\"])","d3822808":"#  esd2 can't be put into raw5, because after that it will be too large.","b57800ab":"raw5 = pd.concat([raw4,esd,esd1,vocdum,keydum],axis=1)\nraw5 = raw5.drop(['Artists_Genres','Key','Vocal','Country','Artists','Album','Labels'],axis = 1)\nraw5","aaed4289":"scaler = MinMaxScaler()\nscaler.fit(raw5)\n\nraw5 = pd.DataFrame(data=scaler.transform(raw5),columns = raw5.columns,index=raw5.index)\n\nraw5.to_csv('clean_data.csv',index=0)\n\nX_train=raw5[raw5.index<len(y_train)]\nX_test =raw5[raw5.index>=len(y_train)]\n\nXt_train, Xt_test, yt_train, yt_test = train_test_split(X_train, y_train, test_size = 0.20,random_state=5467)","00e6ebe3":"svclassifier = SVC(kernel='poly')\nfitted = svclassifier.fit(Xt_train, yt_train)\ny_pred = svclassifier.predict(Xt_test)\n\nsvclassifierl = SVC(kernel='linear')\nfittedl = svclassifierl.fit(Xt_train, yt_train)\ny_predl = svclassifierl.predict(Xt_test)","13982dc6":"KN = KNeighborsClassifier()\nKN.fit(Xt_train,yt_train)\nKN_pred = KN.predict(Xt_test)","6aaf8f99":"rf = RandomForestClassifier()\nrf.fit(Xt_train,yt_train)\nrf_pred = rf.predict(Xt_test)","d9862a9b":"gd = GradientBoostingClassifier()\ngd.fit(Xt_train,yt_train)\ngd_pred = gd.predict(Xt_test)","c5321ca3":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(Xt_train,yt_train)\ntree_pred = decision_tree.predict(Xt_test)","5709c23d":"regr = LogisticRegression() \nregr.fit(Xt_train, yt_train)\nLog_pred = regr.predict(Xt_test)","7a9714af":"print('Logistic_Regression:',accuracy_score(yt_test, Log_pred)*100,'%')\nprint('Decision Tree:', accuracy_score(yt_test, tree_pred)*100,'%') # to summarize all\nprint('Random Forest:', accuracy_score(yt_test, rf_pred)*100,'%')\nprint('Gradient Boost:', accuracy_score(yt_test, gd_pred)*100,'%')\nprint('KNeighbors:',accuracy_score(yt_test, KN_pred)*100,'%')\nprint('SVM_p:',accuracy_score(yt_test, y_pred)*100,'%')\nprint('SVM_l:',accuracy_score(yt_test, y_predl)*100,'%')","057bc42b":"gd_scored = cross_val_score(gd,X_train,y_train,cv=5)\nprint(gd_scored)\nprint(confusion_matrix(yt_test,gd_pred))\nprint(classification_report(yt_test,gd_pred))","7bf390da":"rf_scored=cross_val_score(rf,X_train,y_train,cv=5)\nprint(rf_scored)\nprint(confusion_matrix(yt_test,rf_pred))\nprint(classification_report(yt_test,rf_pred))","46c35ce1":"final_pred = gd.predict(X_test)\nfinal_pred","3a9a0b5f":"final_pred=pd.DataFrame({'Id':np.linspace(665,964,300,dtype=np.int16),'Category':final_pred})\nfinal_pred.to_csv('submission.csv',index=0)","5aa40911":"final_pred.head()","cbaa599d":"We can see that Random Forest and Gradient Boost. Let's go to see how good they are more precisely.\n","a833054d":"The same song with different key or other things is a problem \\\nWe assume that the name is less important than the other features, so we'll just drop it.","95f0066b":"We decide to drop irrelated columns, after that we will delete all line with null-value ","0d66a6d0":"Then first I will make a linear model for testing, that is, assume that the effects are linearly added.","fd5ae558":"Listing all unique Artists_Genres"}}