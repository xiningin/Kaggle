{"cell_type":{"5b992851":"code","16f83803":"code","6cd1a2e7":"code","536259ac":"code","c61b6960":"code","61f9bd30":"code","3dc8f9d9":"code","d004c574":"code","9aa153f2":"code","21eea9fb":"code","d0eaa7c2":"code","fb900bad":"code","e0695010":"code","bacb5d44":"code","2b7195ce":"code","a354aabe":"code","79299edf":"code","bb808eb9":"code","c697d7aa":"code","f7185a29":"code","ecb75360":"code","90216436":"markdown","126da265":"markdown","61fb9bd5":"markdown","d41a9ae0":"markdown","17759fef":"markdown","619be310":"markdown","65e2c764":"markdown","e170782d":"markdown","9768fc78":"markdown","a0282234":"markdown","91f28725":"markdown","c24f0ac3":"markdown","89b72000":"markdown","5fbfdeb5":"markdown","70e3e226":"markdown","c11b0cc2":"markdown","4019eb83":"markdown","8cab4aab":"markdown","9f6202b0":"markdown","29ed535f":"markdown","de27a9d4":"markdown"},"source":{"5b992851":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, cross_val_score, cross_validate\nfrom xgboost import XGBRegressor, plot_importance\nfrom statistics import mean\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","16f83803":"data = pd.read_csv('..\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip')\ntest = pd.read_csv('..\/input\/mercedes-benz-greener-manufacturing\/test.csv.zip')\nprint(data.shape, test.shape)\ndata.head()","6cd1a2e7":"print(data.dtypes.value_counts())\ndata.isnull().sum().sort_values(ascending=False) # no missing value!\ntest.isnull().sum().sort_values(ascending=False) # no missing value for test data as well!\n\n# cardinality=1 columns: 12 columns in data and 5 columns in test\ndata_one_cardinality_columns = [column for column in data.columns if data[column].nunique()==1]\ntest_one_cardinality_columns = [column for column in test.columns if test[column].nunique()==1]\n\none_cardinality_columns = data_one_cardinality_columns + test_one_cardinality_columns","536259ac":"sns.boxplot(data.y)","c61b6960":"data_o = data[(data['y'] <= 200)]\nsns.boxplot(data_o.y)","61f9bd30":"!pip install facets-overview","3dc8f9d9":"### Create the feature stats for the datasets and stringify it.\nimport base64\nfrom facets_overview.generic_feature_statistics_generator import GenericFeatureStatisticsGenerator\n\ngfsg = GenericFeatureStatisticsGenerator()\nproto = gfsg.ProtoFromDataFrames([{'name': 'train', 'table': data}])\nprotostr = base64.b64encode(proto.SerializeToString()).decode(\"utf-8\")\n\n### Display the facets overview visualization for this data\nfrom IPython.core.display import display, HTML\n\nHTML_TEMPLATE = \"\"\"\n        <script src=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/webcomponentsjs\/1.3.3\/webcomponents-lite.js\"><\/script>\n        <link rel=\"import\" href=\"https:\/\/raw.githubusercontent.com\/PAIR-code\/facets\/1.0.0\/facets-dist\/facets-jupyter.html\" >\n        <facets-overview id=\"elem\"><\/facets-overview>\n        <script>\n          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n        <\/script>\"\"\"\nhtml = HTML_TEMPLATE.format(protostr=protostr)\ndisplay(HTML(html))","d004c574":"y = data_o.y\nX = data_o.drop(['ID','y'], axis=1)\nX_submission = test.drop('ID', axis=1)\n\n# Concatenate X and X_submission before applying auto OneHotEncoder\nX_con = pd.concat([X, X_submission], axis=0)\n\n# Apply auto OneHotEncoder \nX_con_ohe = pd.get_dummies(X_con)\n\n# Now deviding back to train and test data \nX_ohe = X_con_ohe[:len(X)]\nX_submission_ohe = X_con_ohe[len(X):]","9aa153f2":"xgb= XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n\nfit_params = {\"early_stopping_rounds\": 5, \"eval_set\": [(X_ohe, y)]}\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nxgb_scores = cross_validate(xgb, X_ohe, y, scoring=['neg_mean_absolute_error','r2'], cv=cv, n_jobs=-1, \n                            verbose=1, fit_params=fit_params, return_estimator=True)\n\nprint('mae:',abs(xgb_scores['test_neg_mean_absolute_error'].mean()))\nprint('r2:',xgb_scores['test_r2'].mean())","21eea9fb":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(max_depth=4, n_estimators=5, random_state=42)\n# evaluate model\ncv = KFold(n_splits=10, shuffle=True, random_state=42)\nrf_scores = cross_validate(rf, X_ohe, y, scoring=['neg_mean_absolute_error','r2'], cv=cv, n_jobs=-1, return_estimator=True)\n\nrf_base_mae = abs(rf_scores['test_neg_mean_absolute_error'].mean())\nrf_base_r2 = rf_scores['test_r2'].mean()\n\nprint('mae:',rf_base_mae)\nprint('r2:',rf_base_r2)","d0eaa7c2":"from sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingRandomSearchCV\nfrom scipy.stats import uniform, randint\n\nxgb_model = XGBRegressor(n_estimators=200)   \n\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"subsample\": uniform(0.6, 0.4)}\n\nfit_params = {\n    \"early_stopping_rounds\": 5,\n    \"eval_set\": [(X_ohe, y)]}\n\nxgb_halvsearch = HalvingRandomSearchCV(xgb_model,                   \n                param_distributions=params, resource='n_estimators', \n                max_resources=40, random_state=42, cv=5, scoring='r2',\n                verbose=1, n_jobs=-1, return_train_score=True)\n\nxgb_halvsearch.fit(X_ohe, y, **fit_params)\nprint('best r2 score:',xgb_halvsearch.best_score_)","fb900bad":"import shap\n\nestimater = rf_scores['estimator'][1]   # I use the one generating the score close to the average....\n\nexplainer = shap.explainers.Tree(estimater)\nshap_values = explainer(X_ohe)\n\nshap.summary_plot(shap_values, X_ohe)","e0695010":"# Check mean shap value for each column\nshap_mean = np.abs(shap_values.values).mean(axis=0)\nshap_mean_columns = pd.Series(shap_mean, index=X_ohe.columns)\n#shap_mean_columns.value_counts().sort_index()\nshap_mean_columns.sort_values(ascending=False)","bacb5d44":"# creat column list that has mean shap is higher 0.01 =>about 31 columns\nshap_incl_columns = shap_mean_columns[shap_mean_columns.values>0.01].index.to_list()\nprint(shap_incl_columns)\nX_ohe_fs = X_ohe[shap_incl_columns]\nX_ohe_fs.shape","2b7195ce":"# I borrowed the code from Dmitriy K. thanks!\nfrom sklearn.feature_selection import RFECV\n\nselector = RFECV(estimater, step = 1, cv=5, n_jobs=-1,verbose=1, scoring='r2')\nselector.fit(X_ohe_fs, y)\n\nprint(selector.grid_scores_)\n\nrfecv_features = [f for f, s in zip(X_ohe_fs, selector.support_) if s]\nprint('selected features:', rfecv_features)\n\nX_ohe_rfecv = X_ohe[rfecv_features]","a354aabe":"rf = RandomForestRegressor(max_depth=4, n_estimators=5, random_state=42)\ncv = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# Score with all features\nprint('mae_all_data:',rf_base_mae)\nprint('r2_all_data:',rf_base_r2)\n\n# Score with features with mean shap value >0.01 (31 columns)\nrf_scores1 = cross_validate(rf, X_ohe_fs, y, scoring=['neg_mean_absolute_error','r2'], cv=cv, n_jobs=-1, return_estimator=True)\nprint('mae_shap_features:', abs(rf_scores1['test_neg_mean_absolute_error'].mean()))\nprint('r2_shap_features:', rf_scores1['test_r2'].mean())\n\n# Score with RFECV features (5 columns)\nrf_scores2 = cross_validate(rf, X_ohe_rfecv, y, scoring=['neg_mean_absolute_error','r2'], cv=cv, n_jobs=-1, return_estimator=True)\nprint('mae_rfecv_features:', abs(rf_scores2['test_neg_mean_absolute_error'].mean()))\nprint('r2_rfecv_features:', rf_scores2['test_r2'].mean())","79299edf":"from sklearn.model_selection import train_test_split\n\nfinal_model = RandomForestRegressor(max_depth=4, n_estimators=5, random_state=42)\nfinal_model.fit(X_ohe_fs,y)\n\n# make prediction and calcurate the difference betweeen y and prediction on each dataset\nX_train, X_test, y_train, y_test = train_test_split(X_ohe_fs, y,shuffle=False, test_size=0.25)\npredicted_y = pd.Series(final_model.predict(X_test))\npredicted_y.index = X_test.index\ndif = abs(predicted_y-y_test)","bb808eb9":"# DataFrame with columns y, predicted_y, difference\ncompare = y_test.to_frame().join(predicted_y.to_frame(name='predicted_y'))\ncompare = compare.join(dif.to_frame(name='abs_dif'))\n\n# Not necessarily lineplot but I can see the difference easily.\nsns.set_theme(context='notebook', style='darkgrid')\nplt.figure(figsize=(24, 6.5))\nsns.lineplot(data=compare.iloc[250:350,0:2])","c697d7aa":"# plot the difference\nplt.figure(figsize=(24, 6.5))\nsns.lineplot(data=compare.iloc[250:350,2]) ","f7185a29":"sns.displot(data=compare, x=y_test)\nsns.displot(data=compare, x=predicted_y)","ecb75360":"X_submission_ohe_fs = X_submission_ohe[shap_incl_columns]\n\npredict = final_model.predict(X_submission_ohe_fs)\nsubmission = pd.DataFrame({'ID': test.ID, 'y': predict})\nsubmission.to_csv('submission.csv', index=False)","90216436":"#### 6.2 Reflection\n * The final model generally follows well to the target but does not predict well for the longer test cycle. But the purpose is to reduce the test cycle time. So this discrepancy is not so important either.\n * Distribution plot of the predicted values shows that the model traces the characteristic of the target distribution. ","126da265":"## Introduction\nI am not a data scientist but an engineer\/researcher in the manufacturing domain. I work with the data here and reflect myself from a production engineering perspective. Any comments or advice are welcome!","61fb9bd5":"Takt time for car assembly lines are often around 60 sec but not sure for the premium cars like Mercedes. If this y values reflect the reality, the tak time may be longer than 60 sec and there may be two or more test stations at the end of the line. Anyhow, one data point over 250 sec seems very strange. I can remove this data point.  ","d41a9ae0":"### 5.3 Compare results after the feature selection","17759fef":"### 3.3 XGboost HavlingRandamizedSeaerchCV\nThis is not necessary but I just wanted to experiement how this turns out.","619be310":"## 4. Explore with SHAP","65e2c764":"## 7. submission","e170782d":"## 1. Exploring the dataset \n### 1.1 Basic checkup on & Cleaning","9768fc78":"I could drop these one cardinality columns but I do not know the data well yet. So I will consider it later.","a0282234":"### 5.2 Recursive Feature Elimination and Cross-Validated selection (RFECV)","91f28725":"### 3.4 Reflection on running some estimators\n* Random Forest is quick and generates ok result. So I will use this further.\n* 5 sec deviation from the target on average is probably ok, condering the current mean and variation of the target.\n* A few 0.01 points increase in r2 only affects a few 0.1 seconds improvement for the accuracy (mae). This is very little considering that the average test cycles is around 100 sec, and such small time reduction is easily diminished by other factors in production.\n* So it makes less sense in spending hours to improve the scores...","c24f0ac3":"## 2. Data preprocessing","89b72000":"### 3.2 Random Forest","5fbfdeb5":"## 3. Run some estimaters as baselines\n* Most of the manufcaturing related relational data work very well with ensemble trees. I have not seen so far other models such as liner regression or neural network have beaten them... \n* R2 is used for socoring but mae is more informative in this test bench case. So I use mae as well.\n\n### 3.1 XGboost","70e3e226":"## 5. Feature selection\n### 5.1 Feature selection based on mean SHAP values","c11b0cc2":"### 1.2 Facets overview from Google. I like this one for inital data exploration.","4019eb83":"### 1.3 Some reflection on the data\n* I understand they are list of car features. Eight categorical and the rest is binary. \n* No particular outliears and such things due to this. Clean and simple dataset.\n* Since the input data is car features, the quality of the data should be always good. Otherwise they can not build cars! \n* Many features have low variance e.g. 99.8% is 0. Not sure they will contribute to the model training.\n* The description of the dataset says \"dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing\". This is a bit difficult for me to understand. Does the permutation matter for the test time or is it combination of features instead? The latter makes more sense to me but maybe I misunderstand something....","8cab4aab":"So there are quite many features having very low shap values. It may be a good idea to remove them.","9f6202b0":"### 5.4 Reflection on feature selection\n* Reduced features show a slightly better results.\n* Accuracy wise, Shap_features or Rfecv_features does not make difference.\n* I use shap features here. If the features are those requiring maintenace for instance sensor values, then I would use fewer features. But this case the data should be very stable...\n* Inference time of the trained model is short enough, since the car features should be decided before the actual manufacturing and takt time is much longer than that time.","29ed535f":"So several features, especialy when they are true, contributes to the inference.","de27a9d4":"## 6. Use RF with shap features for the final model\n### 6.1 Map the difference between the target and predicted\nI learned from experience that just looking at aggregated statistical data such as means can be risky especially in manufacturing. It is good the check how the inference looks like against each target."}}