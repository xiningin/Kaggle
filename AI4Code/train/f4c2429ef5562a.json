{"cell_type":{"1d5c07da":"code","07c40528":"code","7d7ea263":"code","00a8407d":"code","c1d01902":"code","339a15c7":"code","8879ce01":"code","7a6ea7e0":"code","b44f3d0c":"code","2d235f40":"code","3837e07e":"code","dd824b35":"code","78df0d6b":"code","bcb3e44b":"code","f61bb398":"code","0dc7d43a":"code","61302e00":"code","ce975678":"code","8c1b5781":"code","9d4864f8":"code","ffcb5bc8":"code","eae333d7":"code","b85a4255":"code","dcbbdb35":"code","51fff8cc":"code","86876942":"code","8e874a8c":"code","513ce890":"code","343c05b9":"code","d302087e":"code","58d9eb04":"code","dfcba71f":"code","1e5c7c81":"code","3d268694":"code","51f7b7a4":"code","6477908d":"code","aef8dfa0":"code","9647c592":"code","1d3e41b9":"code","e8bebb46":"code","1f4157a9":"markdown","d1510569":"markdown","d827e687":"markdown","33c233e2":"markdown","28d1886d":"markdown","4d747b91":"markdown","874ec3ff":"markdown","1ef8b3b5":"markdown","6025077d":"markdown","e162081d":"markdown","15153485":"markdown","e0936a19":"markdown","8766482b":"markdown","9261824a":"markdown","f53c488f":"markdown","862758b1":"markdown","e74ae987":"markdown","7c3932e1":"markdown","bac5fa17":"markdown","0e5dc903":"markdown","3171e5ae":"markdown","c21ecad2":"markdown","f10efcba":"markdown","029de6a7":"markdown","cd20e8dc":"markdown","54399350":"markdown","4acdbad5":"markdown","70310165":"markdown","08db9bd2":"markdown","8e9345ac":"markdown"},"source":{"1d5c07da":"import pandas as pd \nimport numpy as np\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('dark')\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nprint('Setup complete')","07c40528":"# Load and display train data\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_data.head()","7d7ea263":"# Load and display test data\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_data.head()","00a8407d":"train_data.info()","c1d01902":"test_data.info()","339a15c7":"train_data['Survived'].value_counts(normalize=True)","8879ce01":"g = sns.countplot(y=train_data['Survived']).set_title('Survivors and deads count')","7a6ea7e0":"fig, axarr = plt.subplots(1, 2, figsize=(12,6))\na = sns.countplot(train_data['Sex'], ax=axarr[0]).set_title('Passengers count by sex')\naxarr[1].set_title('Survival rate by sex')\nb = sns.barplot(x='Sex', y='Survived', data=train_data, ax=axarr[1]).set_ylabel('Survival rate')","b44f3d0c":"train_data.groupby('Pclass').Survived.mean()","2d235f40":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\na = sns.countplot(x='Pclass', hue='Survived', data=train_data, ax=axarr[0]).set_title('Survivors and deads count by class')\naxarr[1].set_title('Survival rate by class')\nb = sns.barplot(x='Pclass', y='Survived', data=train_data, ax=axarr[1]).set_ylabel('Survival rate')","3837e07e":"train_data.groupby(['Pclass', 'Sex']).Survived.mean()","dd824b35":"plt.title('Survival rate by sex and class')\ng = sns.barplot(x='Pclass', y='Survived', hue='Sex', data=train_data).set_ylabel('Survival rate')","78df0d6b":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\naxarr[0].set_title('Age distribution')\nf = sns.distplot(train_data['Age'], color='g', bins=40, ax=axarr[0])\naxarr[1].set_title('Age distribution for the two subpopulations')\ng = sns.kdeplot(train_data['Age'].loc[train_data['Survived'] == 1], \n                shade= True, ax=axarr[1], label='Survived').set_xlabel('Age')\ng = sns.kdeplot(train_data['Age'].loc[train_data['Survived'] == 0], \n                shade=True, ax=axarr[1], label='Not Survived')","bcb3e44b":"plt.figure(figsize=(8,5))\ng = sns.swarmplot(y='Sex', x='Age', hue='Survived', data=train_data).set_title('Survived by age and sex')","f61bb398":"plt.figure(figsize=(8,5))\nh = sns.swarmplot(x='Pclass', y='Age', hue='Survived', data=train_data).set_title('Survived by age and class')","0dc7d43a":"train_data.Fare.describe()","61302e00":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\nf = sns.distplot(train_data.Fare, color='g', ax=axarr[0]).set_title('Fare distribution')\nfare_ranges = pd.qcut(train_data.Fare, 4, labels = ['Low', 'Mid', 'High', 'Very high'])\naxarr[1].set_title('Survival rate by fare category')\ng = sns.barplot(x=fare_ranges, y=train_data.Survived, ax=axarr[1]).set_ylabel('Survival rate')","ce975678":"plt.figure(figsize=(8,5))\n# I excluded the three outliers with fare > 500 from this plot\na = sns.swarmplot(x='Sex', y='Fare', hue='Survived', data=train_data.loc[train_data.Fare<500]).set_title('Survived by fare and sex')","8c1b5781":"train_data.loc[train_data.Fare==0]","9d4864f8":"def remove_zero_fares(row):\n    if row.Fare == 0:\n        row.Fare = np.NaN\n    return row\n# Apply the function\ntrain_data = train_data.apply(remove_zero_fares, axis=1)\ntest_data = test_data.apply(remove_zero_fares, axis=1)\n# Check if it did the job\nprint('Number of zero-Fares: {:d}'.format(train_data.loc[train_data.Fare==0].shape[0]))","ffcb5bc8":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\nsns.countplot(train_data['Embarked'], ax=axarr[0]).set_title('Passengers count by boarding point')\np = sns.countplot(x = 'Embarked', hue = 'Survived', data = train_data, \n                  ax=axarr[1]).set_title('Survivors and deads count by boarding point')","eae333d7":"g = sns.countplot(data=train_data, x='Embarked', hue='Pclass').set_title('Pclass count by embarking point')","b85a4255":"train_data['Title'] = train_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntest_data['Title'] = test_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())","dcbbdb35":"train_data['Title'].value_counts()","51fff8cc":"test_data['Title'].value_counts()","86876942":"# Substitute rare female titles\ntrain_data['Title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\ntest_data['Title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\n# Substitute rare male titles\ntrain_data['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Mr', inplace=True)\ntest_data['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Mr', inplace=True)","8e874a8c":"train_data.groupby('Title').Survived.mean()","513ce890":"plt.title('Survival rate by Title')\ng = sns.barplot(x='Title', y='Survived', data=train_data).set_ylabel('Survival rate')","343c05b9":"# Extract the first two letters\ntrain_data['Ticket_lett'] = train_data.Ticket.apply(lambda x: x[:2])\ntest_data['Ticket_lett'] = test_data.Ticket.apply(lambda x: x[:2])\n# Calculate ticket length\ntrain_data['Ticket_len'] = train_data.Ticket.apply(lambda x: len(x))\ntest_data['Ticket_len'] = test_data.Ticket.apply(lambda x: len(x))","d302087e":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\na = sns.countplot(train_data['SibSp'], ax=axarr[0]).set_title('Passengers count by SibSp')\naxarr[1].set_title('Survival rate by SibSp')\nb = sns.barplot(x='SibSp', y='Survived', data=train_data, ax=axarr[1]).set_ylabel('Survival rate')","58d9eb04":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\na = sns.countplot(train_data['Parch'], ax=axarr[0]).set_title('Passengers count by Parch')\naxarr[1].set_title('Survival rate by Parch')\nb = sns.barplot(x='Parch', y='Survived', data=train_data, ax=axarr[1]).set_ylabel('Survival rate')","dfcba71f":"# Creation of a new Fam_size column\ntrain_data['Fam_size'] = train_data['SibSp'] + train_data['Parch'] + 1\ntest_data['Fam_size'] = test_data['SibSp'] + test_data['Parch'] + 1","1e5c7c81":"plt.title('Survival rate by family size')\ng = sns.barplot(x='Fam_size', y='Survived', data=train_data).set_ylabel('Survival rate')","3d268694":"# Creation of four groups\ntrain_data['Fam_type'] = pd.cut(train_data.Fam_size, [0,1,4,7,11], labels=['Solo', 'Small', 'Big', 'Very big'])\ntest_data['Fam_type'] = pd.cut(test_data.Fam_size, [0,1,4,7,11], labels=['Solo', 'Small', 'Big', 'Very big'])","51f7b7a4":"plt.title('Survival rate by family type')\ng = sns.barplot(x=train_data.Fam_type, y=train_data.Survived).set_ylabel('Survival rate')","6477908d":"y = train_data['Survived']\nfeatures = ['Pclass', 'Fare', 'Title', 'Embarked', 'Fam_type', 'Ticket_len', 'Ticket_lett']\nX = train_data[features]\nX.head()","aef8dfa0":"numerical_cols = ['Fare']\ncategorical_cols = ['Pclass', 'Title', 'Embarked', 'Fam_type', 'Ticket_len', 'Ticket_lett']\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Bundle preprocessing and modeling code \ntitanic_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', RandomForestClassifier(random_state=0, n_estimators=500, max_depth=5))\n])\n\n# Preprocessing of training data, fit model \ntitanic_pipeline.fit(X,y)\n\nprint('Cross validation score: {:.3f}'.format(cross_val_score(titanic_pipeline, X, y, cv=10).mean()))","9647c592":"X_test = test_data[features]\nX_test.head()","1d3e41b9":"# Preprocessing of test data, get predictions\npredictions = titanic_pipeline.predict(X_test)","e8bebb46":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint('Your submission was successfully saved!')","1f4157a9":"Here is the final result. I have relatively high hopes for this new feature since the survival rate in most cases appears to be either significantly above or below the average survival rate, which should help our model.","d1510569":"Since we don't expect that a passenger's boarding point could change the chance of surviving, we guess this is probably due to the higher proportion of first and second class passengers for those who came from Cherbourg rather than Queenstown and Southampton.  \nTo check this, we see the class distribution for the different embarking points.","d827e687":"Values in `Age`, `Cabin` and `Embarked` are missing in the train data, while values in `Age`, `Fare` and `Cabin` are missing in the test data.  \nIf needed, we will take care of those later.  \nFinally, just to have an idea of what we are going to predict, we focus on the target: let's see how many passengers survived.","33c233e2":"# Conclusion\nThis was my very first Kaggle competition and climbing up the leaderboard one step at a time was definitely a really nice journey.  \nBelow you can find a histogram of Titanic LB scores in September 2020: right now, this notebook places in top 4%.\n![](attachment:Titanic_LB.png)\nThis is a very good score, but we can still improve our classifier with a clever grouping approach and ensembling models.  \nIf you are interested in how to do this, [here](https:\/\/www.kaggle.com\/mviola\/titanic-wcg-knns-ensemble-0-82775-top-1) you can find my other notebook that reaches 0.82775 (you might want to check [this](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/179147) out as well to have an idea of some of the top scores now).  \nFor the moment, let me know if you found this notebook useful or you just liked it: I would really appreciate it!  \nGood luck with this competition and see you in the next one.","28d1886d":"Another interesting thing to look at is the relation between `Age`, `Pclass` and `Survived`.  \nWe see the influence of `Pclass` is the important one as there are no super clear horizontal patterns.  \nAlso, we note that there were not many children in the first class.","4d747b91":"# Introduction\nThis notebook covers my first approach to the Titanic: Machine Learning from Disaster competition (check it out [here](http:\/\/www.kaggle.com\/c\/titanic) for more details).  \nAs a beginner, my goal was to score over 80% with basic feature engineering and a simple model and I eventually made it with great satisfaction and hard work.  \nFor those of you who want to do the same without overcomplicating things, I will guide you through this process and hopefully help you in some way.  \nLet's get started!  \n![](https:\/\/www.ilmessaggero.it\/photos\/MED_HIGH\/34\/78\/2383478_1527_titanic.jpg)\n# Importing packages and data\nWe begin by loading our standard modules and taking a look at the data.","874ec3ff":"However, when it came down to modeling, these fare categories did not help at all as they underfit quite substantially.  \nLooking at the more detailed plot below, we also see for example that all males with fare between 200 and 300 died: that is not what we would expect.  \nFor this reason, I left the `Fare` feature as it is in order to prevent losing too much information: at deeper levels of a tree, a more discriminant relationship might open up and it could become a good group detector.","1ef8b3b5":"Looking at the distribution of the titles, it might be convenient to move the really low-frequency ones into bigger groups.  \nAfter analyzing them, I substituted all rare female titles with Miss and all rare male titles with Mr.","6025077d":"We see that in the training data only around 38.4% of the passengers managed to survive the disaster: this is an important value that we have to keep in mind.\n# Feature analysis and creation\nThe goal of this section is to gain a general understanding of our data to perform a more precise feature selection in the modeling part.  \nWe will thus explore one feature at a time in order to determine its importance in predicting if a passenger survived or not.\n## Sex\nWe see that around 65% of the passengers were male while the remaining 35% were female.  \nThe important thing to notice here is that the survival rate for women was four times the survival rate for men and this makes `Sex` one of the most informative features.  \nIt is not a case that the gender submission on its own scores 0.76555!","e162081d":"## Family type\nSince we have two seemingly weak predictors, one thing we can do is combine them to get a stronger one.  \nIn the case of `SibSp` and `Parch`, we can join the two variables to get a family size feature, which is the sum of `SibSp`, `Parch` and 1 (who is the passenger himself). ","15153485":"After all these considerations it is finally time to put everything together in a simple and quite efficient model.\n# Modeling\nWe start by selecting the features we will use and isolating the target.  \nAs I said, I will not consider `Cabin` and in the end, I also excluded `Age` as the relevant information which is being a young man is encoded in the Master title.  \nI also did not use `Sex` as it is not useful given the `Title` column: adult males and young children have the same sex but are really different categories as we saw before, so we don't want to confuse our algorithm.  \nIf you don't extract the `Title` column, remember to put `Sex` in your models as it is pretty important!","e0936a19":"## Age\nDespite this column contains a lot of missing values, we see that in the training data the average age was just under 30 years.  \nHere is the plot of the age distribution in general compared to the one for the survivors and the deads.","8766482b":"This should help our model a little, so I think we are fine here.\n## SibSp\n`SibSp` is the number of siblings or spouses of a person aboard the Titanic.  \nWe see that more than 90% of people traveled alone or with one sibling or spouse.  \nThe survival rate between the different categories is a bit confusing but we see that the chances of surviving are lower for those who traveled alone or with more than 2 siblings.  \nFurthermore, we notice that no one from a big family with 5 or 8 siblings was able to survive.","9261824a":"All we have to do now is convert them into the submission file!","f53c488f":"We can also see the survival rate by `Sex` and `Pclass`, which is quite impressive: first class and second class women who were rescued were respectively 97% and 92%, while the percentage drops to 50% for third-class women.  \nDespite that, this is still more than the 37% survival rate for first-class men. ","862758b1":"## Parch\nSimilar to the `SibSp` column, this feature contains the number of parents or children each passenger was traveling with.  \nHere we draw the same conclusions as `SibSp`: we see again that small families had more chances to survive than bigger ones and passengers who traveled alone.","e74ae987":"Plotting the survival rate by family size it is clear that people who were alone had a lower chance of surviving than families up to 4 components, while the survival rate drops for bigger families and ultimately becomes zero for very large ones.","7c3932e1":"After all these plots I am not sure about the importance of `Age` in a model: I guess we will see later, even though I am thinking of not using it.\n## Fare\nFrom the description, we see that the `Fare` distribution is positively skewed, with 75% of data under 31 and a maximum of 512.  \nJust to understand better this feature, the simplest idea here could be creating fare ranges using quartiles.  \nAt a first look, we notice that the higher the fare, the higher the possibility of surviving.","bac5fa17":"To further summarize the previous trend, as my final feature I created four groups for family size.","0e5dc903":"The claim is correct and hopefully justifies why that survival rate is so high.  \nAgain this feature might be useful in detecting groups at a deeper level of a tree and this is the only reason why I keep it.\n## Name\nThe `Name` column contains useful information as for example we could identify family groups using surnames.  \nIn this notebook, however, I extracted only the passengers' title from it, creating a new feature for both train and test data.","3171e5ae":"At a first look, the relationship between `Age` and `Survived` appears not to be very clear: we notice for sure that there is a peak corresponding to young passengers for those who survived, but apart from that the rest is not very informative.  \nWe can appreciate this feature more if we consider `Sex` too: now it is clearer that a good number of male survivors had less than 12 years, while the female group has no particular properties.","c21ecad2":"## Embarked \n`Embarked` tells us where a passenger boarded from.  \nThere are three possible values for it: Southampton, Cherbourg and Queenstown.  \nIn the training data, more than 70% of the people boarded from Southampton, slightly under 20% from Cherbourg and the rest from Queenstown.  \nCounting survivors by boarding point, we see that more people who embarked from Cherbourg survived than those who died.","f10efcba":"Since some of them are first or second class passengers, I decided to remove zero-Fares that might confuse my model.  \nWith the help of this function, we are going to set null values every time we encounter a zero value for `Fare`.  \nThose will be imputed later when we train our model.","029de6a7":"After seeing Erik's kernel [here](https:\/\/www.kaggle.com\/erikbruin\/titanic-2nd-degree-families-and-majority-voting), it reminded me I had not analyzed this feature deep enough.  \nWhen I printed the description, I should have also noticed that the minimum value for `Fare` is zero and that is a bit strange.  \nIs this information correct? Let's see who these passengers are.","cd20e8dc":"We are now ready to make our predictions by simply calling the predict method on the test data.","54399350":"## Cabin and Ticket\nThe `Cabin` feature is somewhat problematic as there are many missing values.  \nI don't expect it to help our model too much so I don't even analyze it.  \nOn the other side, a correctly engineered `Ticket` column is the best way to find family groups but it is not the approach I chose for this notebook (I will maybe try it in another one).  \nSince it is a pity to delete it knowing its full potential, I decided to create two new columns: one for the ticket first two letters and the second one for the ticket length.","4acdbad5":"Here is the final result: I think we discovered a nice pattern.","70310165":"Since from the EDA I remember that we have missing values in both train and test data and multiple categorical variables to deal with, I decided to use pipelines to simplify all the work.","08db9bd2":"## Pclass\nThere were three classes on the ship and from the plot we see that the number of passengers in the third class was higher than the number of passengers in the first and second classes combined.  \nHowever, the survival rate by class is not the same: more than 60% of first-class passengers and around half of the second class passengers were rescued, whereas 75% of third class passengers were not able to survive the disaster.  \nFor this reason, this is definitely an important aspect to consider.","8e9345ac":"We then check for missing values in both training and test data."}}