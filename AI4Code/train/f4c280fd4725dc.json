{"cell_type":{"52f9f39e":"code","429269af":"code","43cacdd6":"code","b94a680b":"code","833d91c4":"code","b9129ae0":"code","f39cc554":"code","dbc05952":"code","5e6de15e":"code","a951d809":"code","4046550c":"code","320a23e0":"code","66e32e2a":"code","c149d395":"code","5fea1ec7":"code","19415380":"code","c7714923":"code","fffb18cf":"code","6851bad9":"code","3f8c4ab8":"code","4d8bc84e":"code","0c7870b9":"code","3cea61c6":"code","2e2b8e54":"code","e9314bcf":"code","1a2fc2d4":"code","7c0a269f":"code","240cc7e0":"code","a845df5e":"code","d279c759":"code","6eec9439":"code","cd14df56":"code","683929d5":"code","06f303c5":"code","d09eeb02":"code","01535c58":"code","17c159db":"code","0f8c158c":"code","1fbae09c":"code","80157cd1":"code","3843ec35":"code","944fd2e2":"markdown","e21160c3":"markdown","9de49c10":"markdown","0d5b1d39":"markdown","19e2b8c8":"markdown","ede27963":"markdown","4013f911":"markdown","78479c5d":"markdown","9d2947fe":"markdown","d370d372":"markdown","edf306f2":"markdown","1055b105":"markdown","cf9fef60":"markdown","0ea6ac72":"markdown","4532b928":"markdown","5fa18ff3":"markdown","d11c524a":"markdown","69700507":"markdown","e22f68e4":"markdown"},"source":{"52f9f39e":"# importing the necessary Libraries\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nsns.set()","429269af":"cars_train = pd.read_csv('..\/input\/used-cars-price-prediction\/train-data.csv')\ncars_train = cars_train.drop('Unnamed: 0', axis = 1)\n\ncars_test = pd.read_csv('..\/input\/used-cars-price-prediction\/test-data.csv')\ncars_test = cars_test.drop('Unnamed: 0', axis = 1)","43cacdd6":"# Finding the no.of nulls in each column\ncars_train.isnull().sum()","b94a680b":"#Table Information\ncars_train.info()","833d91c4":"cars_train.describe()","b9129ae0":"cars_train.head(1)","f39cc554":"# As per the given data, Mielage, Engine& Power fields contains the units which makes the entire column as string. To convert it to integer, spliting those column into two (value - int dtype & Units - String dtype)\ncars_train[['Mileag','M_Units']] = cars_train.Mileage.str.split(expand=True)\ncars_train[['Engin','E_Units']] = cars_train.Engine.str.split(expand=True)\ncars_train[['Powe','P_Units']] = cars_train.Power.str.split(expand=True)\n\n# Creating a new columns with the company name from the Name(Contains car into includes company name, carname, model) column\ncars_train['Company'] = cars_train.Name.str.partition(' ')[0]","dbc05952":"# Replacing all the junk values with null\ncars_train['Powe'] = np.where(cars_train.Powe == 'null', None , cars_train.Powe)\ncars_train['Mileag'] = np.where(cars_train.Mileag == '0.0', None , cars_train.Mileag)\ncars_train['Seats'] = np.where(cars_train.Seats == 0, None , cars_train.Seats)","5e6de15e":"# Dropping un-necessary columns\ncars_train_ADS = cars_train.drop(['New_Price', 'Mileage','Engine', 'Power', 'M_Units', 'E_Units', 'P_Units'], axis = 1)","a951d809":"# Convert the Mileage Engine, Power data-types from object to integer\/float\ncars_train_ADS.Mileag = cars_train_ADS.Mileag.astype(float)\ncars_train_ADS.Engin = cars_train_ADS.Engin.astype(float)\ncars_train_ADS.Powe = cars_train_ADS.Powe.astype(float)\ncars_train_ADS.Seats = cars_train_ADS.Seats.astype(float)","4046550c":"cars_train_ADS.isnull().sum()","320a23e0":"# Replacing the missing values with the group mode values by its Name\ndef fast_mode(cars_train_ADS, key_cols, value_col):\n    return(cars_train_ADS.groupby(key_cols + [value_col]).size()\n          .to_frame('counts').reset_index().sort_values('counts', ascending = False).drop_duplicates(subset = key_cols)).drop(columns = 'counts')\n\ncars_train_ADS.loc[cars_train_ADS.Powe.isnull(), 'Powe'] = cars_train_ADS.Name.map(fast_mode(cars_train_ADS, ['Name'], 'Powe').set_index('Name').Powe)\ncars_train_ADS.loc[cars_train_ADS.Engin.isnull(), 'Engin'] = cars_train_ADS.Name.map(fast_mode(cars_train_ADS, ['Name'], 'Engin').set_index('Name').Engin)\ncars_train_ADS.loc[cars_train_ADS.Seats.isnull(), 'Seats'] = cars_train_ADS.Name.map(fast_mode(cars_train_ADS, ['Name'], 'Seats').set_index('Name').Seats)\ncars_train_ADS.loc[cars_train_ADS.Seats.isnull(), 'Mileag'] = cars_train_ADS.Name.map(fast_mode(cars_train_ADS, ['Name'], 'Mileag').set_index('Name').Mileag)","66e32e2a":"cars_train_ADS.isnull().sum()","c149d395":"cars_train_ADS['Seats'] = cars_train_ADS['Seats'].fillna(cars_train_ADS.groupby(['Company'])['Seats'].transform('mean'))\ncars_train_ADS['Engin'] = cars_train_ADS['Engin'].fillna(cars_train_ADS.groupby(['Company','Seats'])['Engin'].transform('mean'))\ncars_train_ADS['Mileag'] = cars_train_ADS['Mileag'].fillna(cars_train_ADS.groupby(['Company','Seats','Engin','Fuel_Type','Transmission'])['Mileag'].transform('mean'))\ncars_train_ADS['Powe'] = cars_train_ADS['Powe'].fillna(cars_train_ADS.groupby(['Company','Engin'])['Powe'].transform('mean'))","5fea1ec7":"cars_train_ADS.isnull().sum()","19415380":"# Removing the untreatable null rows - We are removing 0.007% of data which are not treatble\ncars_train_ADS = cars_train_ADS[(cars_train_ADS.Powe.isnull() == False) & (cars_train_ADS.Mileag.isnull() == False)]\ncars_train_ADS.head(3)","c7714923":"cars_train_ADS.isnull().sum()","fffb18cf":"cars_train_ADS.describe()","6851bad9":"l = cars_train_ADS.groupby('Location').agg({'Company':'count'}).reset_index().rename(columns={'Company':'No_Cars'})\nl['Percentage'] = (l['No_Cars']\/sum(l['No_Cars']) *100).round(2).astype(str) + '%'\nfig = px.bar(l, x='No_Cars',y = 'Location',\n             hover_data=['No_Cars', 'Percentage'],\n            text = 'Percentage', title = 'Most number of used cars selling are in Mumbai: 783 followed by Hyderabad: 735')\nfig.show()","3f8c4ab8":"l = cars_train_ADS.groupby('Fuel_Type').agg({'Company':'count'}).reset_index().sort_values(by = 'Company', ascending=False).rename(columns={'Company':'No_Cars'})\nl['Percentage'] = (l['No_Cars']\/sum(l['No_Cars']) *100).round(2).astype(str) + '%'\nl['chat_dummy'] = l.No_Cars.astype(str) +'; '+ l.Fuel_Type\n\nfig = px.pie(l, values='No_Cars', names='Fuel_Type',\n             title='Diesel type cars are selling more (53.42%) followed by petrol car1s (45.47%) ',\n             hover_data=['Percentage'])\nfig.update_traces(textposition='outside', textinfo='percent+label')\nfig.show()\n\n","4d8bc84e":"l = cars_train_ADS.groupby('Company').agg({'Name':'count'}).reset_index().sort_values(by = 'Name', ascending=True).rename(columns={'Name':'No_Cars'})\nl['Percentage'] = (l['No_Cars']\/sum(l['No_Cars']) *100).round(2).astype(str) + '%'\nfig = px.bar(l, y='No_Cars',x = 'Company',\n             hover_data=['No_Cars', 'Percentage'], text = 'Percentage',\n             title = 'Maruthi cars are selling more: 20.24% followed by hyundai 18.29%')\nfig.show()","0c7870b9":"l = cars_train_ADS.groupby(['Owner_Type','Transmission']).agg({'Name':'count'}).reset_index().sort_values(by = 'Name', ascending=True).rename(columns={'Name':'No_Cars'})\nl['Percentage'] = (l['No_Cars']\/sum(l['No_Cars']) *100).round(2).astype(str) + '%'\n\nfig = px.bar(l, x=\"Owner_Type\", y=\"No_Cars\",\n             color='Transmission', barmode='group',\n             text='Percentage')\nfig.show()","3cea61c6":"colors = ['#00CC96','#636EFA','#AB63FA','#EF553B']\nl = cars_train_ADS.groupby('Owner_Type').agg({'Company':'count'}).reset_index().sort_values(by = 'Company', ascending=False).rename(columns={'Company':'No_Cars'})\nl['Percentage'] = (l['No_Cars']\/sum(l['No_Cars']) *100).round(2).astype(str) + '%'\nfig = go.Figure(data=[go.Bar(x=l.Owner_Type, y=l.No_Cars, \n                             marker_color=colors , text=l.Percentage)])\nfig.update_layout(title_text='No.of different types of Owners for used cars')","2e2b8e54":"l = cars_train_ADS.groupby('Year').agg({'Price':'mean'}).reset_index()\nfig = go.Figure(data=go.Scatter(x=l.Year, y=l.Price,mode='lines+markers',line_color='#00CC96'))\nfig.show()","e9314bcf":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(20,8))\nax1 = fig.add_subplot(1,2,1)\nsns.boxplot(x='Owner_Type', y='Price', data=cars_train_ADS)\nax1.set_title('Owner vs Price')\n\nax2 = fig.add_subplot(1,2,2)\nsns.boxplot(x='Company', y='Price', data=cars_train_ADS)\nloc,labels = plt.xticks()\nax2.set_xticklabels(labels, rotation=90)\nax2.set_title('Brand vs Price')\nplt.show()","1a2fc2d4":"fig = plt.figure(figsize=(20,8))\nax1 = fig.add_subplot(1,3,1)\nsns.boxplot(x='Fuel_Type', y='Price', data=cars_train_ADS)\nax1.set_title('Fuel_Type vs Price')\n\nax2 = fig.add_subplot(1,3,2)\nsns.boxplot(x='Location', y='Price', data=cars_train_ADS)\nloc,labels = plt.xticks()\nax2.set_xticklabels(labels, rotation=90)\nax2.set_title('Location vs Price')\n\nax3 = fig.add_subplot(1,3,3)\nsns.boxplot(x='Transmission', y='Price', data=cars_train_ADS)\nloc,labels = plt.xticks()\nax3.set_xticklabels(labels, rotation=90)\nax3.set_title('Transmission vs Price')\nplt.show()","7c0a269f":"var1 = 'Location'\nLocation = cars_train_ADS[[var1]]\nLocation = pd.get_dummies(Location,drop_first=True)\n\nvar2 = 'Fuel_Type'\nFuel_Type = cars_train_ADS[[var2]]\nFuel_Type = pd.get_dummies(Fuel_Type,drop_first=True)\n\nvar3 = 'Transmission'\nTransmission = cars_train_ADS[[var3]]\nTransmission = pd.get_dummies(Transmission,drop_first=True)\n\ncars_train_ADS.replace({\"First\":1,\"Second\":2,\"Third\": 3,\"Fourth & Above\":4},inplace=True)\n\ncars_train_ADS= pd.concat([cars_train_ADS,Location,Fuel_Type,Transmission],axis=1)\ncars_train_ADS.head()\n","240cc7e0":"cars_train_ADS = cars_train_ADS.drop(['Name', 'Location', 'Fuel_Type', 'Transmission','Company'], axis = 1)","a845df5e":"# Function to calculate VIF\ndef calculate_vif(data):\n    vif_df = pd.DataFrame(columns = ['Column Names', 'Vif'])\n    Rsquare_df = pd.DataFrame(columns = ['Column Names', 'R_Sq'])\n    x_var_names = data.columns\n    for i in range(0, x_var_names.shape[0]):\n        y = data[x_var_names[i]]\n        x = data[x_var_names.drop([x_var_names[i]])]\n        R_Sq = sm.OLS(y,x).fit().rsquared\n        vif = round(1\/(1-R_Sq),2)\n        vif_df.loc[i] = [x_var_names[i], vif]\n        Rsquare_df.loc[i] = [x_var_names[i], R_Sq]\n    return pd.merge(vif_df, Rsquare_df, on = 'Column Names', how = 'inner').sort_values(by = 'Vif', axis = 0, ascending=False, inplace=False)\n\nX=cars_train_ADS.drop(['Price'],axis=1)\nprint('there is a co-linearity between mutiple columns')\ncalculate_vif(X)","d279c759":"cars_train_ADS_X = cars_train_ADS.drop('Price', axis = 1)\ncars_train_ADS_y = cars_train_ADS['Price']","6eec9439":"Standardscaler = StandardScaler()\ncars_train_ADS_X_col = cars_train_ADS_X.columns\ncars_train_ADS_X = pd.DataFrame(Standardscaler.fit_transform(cars_train_ADS_X),columns = cars_train_ADS_X_col )\ncars_train_ADS_X.head(2)","cd14df56":"#PCA\npca = PCA().fit(cars_train_ADS_X)\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, 22, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 20, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance - In this case, to get atlease 95% of variance we required 16 components')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()\n","683929d5":"pca = PCA(n_components = 0.95)\npca.fit(cars_train_ADS_X)\ncars_train_ADS_X = pca.transform(cars_train_ADS_X)","06f303c5":"cars_train_ADS_train = pd.DataFrame(cars_train_ADS_X,cars_train_ADS_y).reset_index()\ncars_train_ADS_train.head()","d09eeb02":"# Null value treatment for test dataset\n# As per the given data, Mielage, Engine& Power fields contains the units which makes the entire column as string. To convert it to integer, spliting those column into two (value - int dtype & Units - String dtype)\ncars_test[['Mileag','M_Units']] = cars_test.Mileage.str.split(expand=True)\ncars_test[['Engin','E_Units']] = cars_test.Engine.str.split(expand=True)\ncars_test[['Powe','P_Units']] = cars_test.Power.str.split(expand=True)\n\n# Creating a new columns with the company name from the Name(Contains car into includes company name, carname, model) column\ncars_test['Company'] = cars_test.Name.str.partition(' ')[0]\n\n# Replacing all the junk values with null\ncars_test['Powe'] = np.where(cars_test.Powe == 'null', None , cars_test.Powe)\ncars_test['Mileag'] = np.where(cars_test.Mileag == '0.0', None , cars_test.Mileag)\ncars_test['Seats'] = np.where(cars_test.Seats == 0, None , cars_test.Seats)\n\n# Dropping un-necessary columns\ncars_test_ADS = cars_test.drop(['New_Price', 'Mileage','Engine', 'Power', 'M_Units', 'E_Units', 'P_Units'], axis = 1)\n\n# Convert the Mileage Engine, Power data-types from object to integer\/float\ncars_test_ADS.Mileag = cars_test_ADS.Mileag.astype(float)\ncars_test_ADS.Engin = cars_test_ADS.Engin.astype(float)\ncars_test_ADS.Powe = cars_test_ADS.Powe.astype(float)\ncars_test_ADS.Seats = cars_test_ADS.Seats.astype(float)\n\n\n# Replacing the missing values with the group mode \/ mean values of its Name\ndef fast_mode(cars_test_ADS, key_cols, value_col):\n    return(cars_test_ADS.groupby(key_cols + [value_col]).size()\n          .to_frame('counts').reset_index().sort_values('counts', ascending = False).drop_duplicates(subset = key_cols)).drop(columns = 'counts')\n\ncars_test_ADS.loc[cars_test_ADS.Powe.isnull(), 'Powe'] = cars_test_ADS.Name.map(fast_mode(cars_test_ADS, ['Name'], 'Powe').set_index('Name').Powe)\ncars_test_ADS.loc[cars_test_ADS.Engin.isnull(), 'Engin'] = cars_test_ADS.Name.map(fast_mode(cars_test_ADS, ['Name'], 'Engin').set_index('Name').Engin)\ncars_test_ADS.loc[cars_test_ADS.Seats.isnull(), 'Seats'] = cars_test_ADS.Name.map(fast_mode(cars_test_ADS, ['Name'], 'Seats').set_index('Name').Seats)\ncars_test_ADS.loc[cars_test_ADS.Seats.isnull(), 'Mileag'] = cars_test_ADS.Name.map(fast_mode(cars_test_ADS, ['Name'], 'Mileag').set_index('Name').Mileag)\n\ncars_test_ADS['Seats'] = cars_test_ADS['Seats'].fillna(cars_test_ADS.groupby(['Company'])['Seats'].transform('mean'))\ncars_test_ADS['Engin'] = cars_test_ADS['Engin'].fillna(cars_test_ADS.groupby(['Company','Seats'])['Engin'].transform('mean'))\ncars_test_ADS['Mileag'] = cars_test_ADS['Mileag'].fillna(cars_test_ADS.groupby(['Company','Seats','Engin','Fuel_Type','Transmission'])['Mileag'].transform('mean'))\ncars_test_ADS['Powe'] = cars_test_ADS['Powe'].fillna(cars_test_ADS.groupby(['Company','Engin'])['Powe'].transform('mean'))\n\n# Removing the untreatable null rows - We are removing 0.007% of data which are not treatble\ncars_test_ADS = cars_test_ADS[(cars_test_ADS.Powe.isnull() == False) & (cars_test_ADS.Mileag.isnull() == False)]\ncars_test_ADS.head(3)\ncars_test_ADS.isnull().sum()\n\nvar1 = 'Location'\nLocation = cars_test_ADS[[var1]]\nLocation = pd.get_dummies(Location,drop_first=True)\n\nvar2 = 'Fuel_Type'\nFuel_Type = cars_test_ADS[[var2]]\nFuel_Type = pd.get_dummies(Fuel_Type,drop_first=True)\n\nvar3 = 'Transmission'\nTransmission = cars_test_ADS[[var3]]\nTransmission = pd.get_dummies(Transmission,drop_first=True)\n\ncars_test_ADS.replace({\"First\":1,\"Second\":2,\"Third\": 3,\"Fourth & Above\":4},inplace=True)\n\ncars_test_ADS= pd.concat([cars_test_ADS,Location,Fuel_Type,Transmission],axis=1)\ncars_test_ADS.head()\n\ncars_test_ADS = cars_test_ADS.drop(['Name', 'Location', 'Fuel_Type', 'Transmission','Company'], axis = 1)\n\n#SCALING\nStandardscaler = StandardScaler()\ncars_test_ADS_col = cars_test_ADS.columns\ncars_test_ADS = pd.DataFrame(Standardscaler.fit_transform(cars_test_ADS),columns = cars_test_ADS_col )\ncars_test_ADS.head(2)\n\n\n#PCA\npca = PCA().fit(cars_test_ADS)\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, 22, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 20, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance - In this case, to get atlease 95% of variance we required 16 components')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()\n\n\npca = PCA(n_components = 0.95)\npca.fit(cars_test_ADS)\ncars_test_ADS = pca.transform(cars_test_ADS)","01535c58":"y=cars_train_ADS_train['Price']\nX=cars_train_ADS_train.drop('Price',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 25)","17c159db":"linear_reg = LinearRegression()\nlinear_reg.fit(X_train, y_train)\ny_pred= linear_reg.predict(X_test)\nscore_1=r2_score(y_test,y_pred)\nprint(\"Accuracy on Traing set: \",linear_reg.score(X_train,y_train))\nprint(\"Accuracy on Testing set: \",linear_reg.score(X_test,y_test))\nprint(\"R2 score\", score_1)","0f8c158c":"reg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)\ny_pred= reg_rf.predict(X_test)\nscore_1=r2_score(y_test,y_pred)\nprint(\"Accuracy on Traing set: \",reg_rf.score(X_train,y_train))\nprint(\"Accuracy on Testing set: \",reg_rf.score(X_test,y_test))\nprint(\"R2 score\", score_1)","1fbae09c":"#Randomized Search CV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n\nrf_random.fit(X,y)","80157cd1":"rf_random.best_params_","3843ec35":"reg_rf = RandomForestRegressor(n_estimators=1000,min_samples_split=2,min_samples_leaf=1,max_features='sqrt',max_depth=25)\nreg_rf.fit(X_train, y_train)\ny_pred= reg_rf.predict(X_test)\nscore_1=r2_score(y_test,y_pred)\nprint(\"Accuracy on Traing set: \",reg_rf.score(X_train,y_train))\nprint(\"Accuracy on Testing set: \",reg_rf.score(X_test,y_test))\nprint(\"R2 score\", score_1)","944fd2e2":"###### There are no-null values in above created ADS. We are good to go\n\n## Exploratory Data Analysis","e21160c3":"## Principal component analysis(PCA)\n- There is a co-linearity between independent columns. To convert the set of co-related variables to set of un-corelated variables by reduce the dimensions of dataset.\n- To perform PCM, scaling should be performed on data to remove the bias","9de49c10":"From hyperprameter tunning, better accuracy is obtaining at\n- max_depth: 25\n- max_features: sqrt\n- min_samples_leaf: 1\n- min_samples_split: 2\n- n_estimators: 1000","0d5b1d39":"- 82% of cars for selling are first hand cars\n- 15.9% of cars for selling are second hand cars\n- 1.8% of cars for selling are third hand cars\n- 0.14% of cars for selling are fourth& Above hand cars","19e2b8c8":"From above summary: Columns with missed\/Null values are Seats, Mileage, Power, Engine","ede27963":"I've tried with two algorithms and hyperparameter tuning for Random forest regressor. If you like the notebook the Please Upvote add up your comments to this nootebook \nHappy coding","4013f911":"In RandomForest Regressor, accuracy is comparitively better than linear regression. To improve the accuracy, hyperparameter tuing is performed","78479c5d":"## Linear regression","9d2947fe":"## Scaling the data by using MINMAX Scaler\n- Scaled the data for each metrics by using feature scaling techniques to reduce the bias, to normalize the data within a range and speeding up the calculation while trainning the model\n- After applying the MinMax Scaler, data range is in between 0 to 1","d370d372":"### Summary of Missing valued columns\n- Mileage: (Total : 70)\n   - Missing values: 2\n   - Inappropriate data (Mielage = 0) : 68\n- Engine: (Total : 36)\n   - Missing values: 36\n- Power: (Total : 143)\n   - Missing values: 36\n   - Inappropriate data (Power = null bhp) : 107\n- Seats: (Total : 43)\n   - Missing values: 42\n   - Inappropriate data (seats = 0) : 1\n- New Price: (Total : 5195)\n   - This column contains approx 87% of nll values. So, we can drop this column","edf306f2":"### Data Cleaning\/ Feature engineerng for test dataset","1055b105":"1. ### Spliting data for training the model","cf9fef60":"## Bivariant Analysis ","0ea6ac72":"## Uni-Variant Analysis","4532b928":"1. 1.  ## Data Cleaning and creating an ADS for Analysis","5fa18ff3":"In linear reegression we didn't obtain good acuracy. So, Lets try with another alogorithm: RandomForest Regressor\n\n## RandomForest Regressor ","d11c524a":"## Data Information\n- Null value finding\n- Table Information\n- 5pt Summary","69700507":"## Feature engineering\nWe have left with only 5 categorical features:\n- Location\n- Fuel_Type\n- Transmission\n- Owner_Type\n- Company\n\nFor hadeling categorical data. We can use these 2 methods:\n- Nominal Encoding\n    - One hot Encoding\n    - One hot Encoding with many categorical\n    - Mean encoding\n- Ordinal Encoding\n    - Label Encoding\n    - Target guided ordinal Encoding\n\nWhere OneHotEncoder is used where data are not in any order and LabelEncoder when data is in order.\n\nSo, for each Features we will use plots to find out what to be used there.\n","e22f68e4":"### Treating with NULL vaues"}}