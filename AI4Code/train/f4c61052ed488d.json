{"cell_type":{"1678e02c":"code","c0e2031b":"markdown"},"source":{"1678e02c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n###############################################################################\n################################## Imports\n###############################################################################\nimport numpy as np\nimport pandas as pd\nimport os\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport gc\nfrom sklearn import neighbors\nfrom sklearn import metrics, preprocessing\n\n###############################################################################\n################################## Data\n###############################################################################\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nX = train.iloc[:,1:257]\nX_test = test.iloc[:,1:257]\nY = train.iloc[:,257]\n\ncols = [c for c in train.columns if c not in ['id', 'target']]\n\ncols.remove('wheezy-copper-turtle-magic')\n\nprediction = np.zeros(len(test))\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(pd.concat([X, X_test]))\nscaler.fit(X)\nX = scaler.transform(X)\nX_test = scaler.transform(X_test)\n\n###############################################################################\n################################## Model\n###############################################################################\nskf = StratifiedKFold(n_splits=5, random_state=42)\n\noof = np.zeros(len(train))\nst = time.time()\nfor i in range(512):\n    if i%5==0: print('Model : ',i, 'Time : ', time.time()-st)\n\n    x = train[train['wheezy-copper-turtle-magic']==i]\n    x_test = test[test['wheezy-copper-turtle-magic']==i]\n    y = Y[train['wheezy-copper-turtle-magic']==i]\n    idx = x.index\n    idx_test = x_test.index\n    x.reset_index(drop=True,inplace=True)\n    x_test.reset_index(drop=True,inplace=True)\n    y.reset_index(drop=True,inplace=True)\n    \n    clf = lgb.LGBMRegressor()\n    clf.fit(x[cols],y)\n    important_features = [i for i in range(len(cols)) if clf.feature_importances_[i] > 0] \n    cols_important = [cols[i] for i in important_features]\n    \n    skf = StratifiedKFold(n_splits=10, random_state=42)\n    for train_index, valid_index in skf.split(x.iloc[:,1:-1], y):\n        # KNN\n        clf = neighbors.KNeighborsClassifier(5)\n        clf.fit(x.loc[train_index][cols_important], y[train_index])\n        oof[idx[valid_index]] = clf.predict_proba(x.loc[valid_index][cols_important])[:,1]\n        prediction[idx_test] += clf.predict_proba(x_test[cols_important])[:,1] \/ 25.0\n    print(i, 'oof auc : ', roc_auc_score(Y[idx], oof[idx]))\n        \nprint('total auc : ',roc_auc_score(train['target'],oof))\n\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = prediction\nsub.to_csv('submission.csv',index=False)","c0e2031b":"## Overview\nSo, since other people in this competition realised that the dataset was most probably built using 512 different datasets, I used 512 models like others already did. Also, since it seems some feature are really just noise, I use LGB for feature selection to remove feature that never appear in the model. The KNN model is simple and yet is performing very well in this dataset."}}