{"cell_type":{"fea3bf0e":"code","e5dbf6d9":"code","292803bc":"code","aa99cd42":"code","2ced9912":"code","f7107241":"code","25a96d36":"code","ab68ba55":"code","f365dd35":"code","962bd01a":"code","84fc8ffe":"code","8d9b128e":"code","da070b15":"code","9c0f01d0":"markdown","22f20047":"markdown","503d5d2c":"markdown","0adc5f58":"markdown","06b65a70":"markdown","4ff63625":"markdown","0a8c39f8":"markdown","4168ece2":"markdown","7bef7fb2":"markdown","09743d48":"markdown","5b71417f":"markdown","bd456188":"markdown"},"source":{"fea3bf0e":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model\nimport numpy as np\nimport time\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nprint ('modules loaded')","e5dbf6d9":"sdir=r'..\/input\/ceramic-tiles-defects-crackspotspinhole\/dataset'\nflist=os.listdir(sdir)\nlabels=[]\nfilepaths=[]\nfor f in flist:\n    fpath=os.path.join(sdir,f)\n    filepaths.append(fpath)\n    klass =f.split('.')[0]\n    labels.append(klass)\nFseries=pd.Series(filepaths, name='filepaths')\nLseries = pd.Series(labels, name='labels')\ndf=pd.concat([Fseries, Lseries], axis=1)\nprint (df['labels'].value_counts())\nprint(df.isna().sum())\nclasses=list(df['labels'].unique())\nclass_count=len(classes)","292803bc":"train_df, dummy_df =train_test_split(df, train_size=.9, shuffle=True, random_state=123, stratify= df['labels'])\nvalid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify =dummy_df['labels'])\nprint('train_df length: ', len(train_df), '  test_df length: ',len(test_df), '  valid_df length: ', len(valid_df))\nprint (train_df['labels'].value_counts())","aa99cd42":"img_path=r'..\/input\/ceramic-tiles-defects-crackspotspinhole\/dataset\/crack.1.jpg'\nimg=plt.imread(img_path)\nprint ('Image shape is: ', img.shape)\nplt.axis('off')\nplt.imshow(img)","2ced9912":"img_size=(150,150)\nbatch_size= 40\n# calculate test_batch_size and test_step so we go through test files exactly once\nlength=len(test_df)\ntest_batch_size=sorted([int(length\/n) for n in range(1,length+1) if length % n ==0 and length\/n<=80],reverse=True)[0]  \ntest_steps=int(length\/test_batch_size)\nprint ('test batch size= ', test_batch_size, '  test steps= ', test_steps)\ntrgen=ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\ntvgen=ImageDataGenerator()\ntrain_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\nvalid_gen=tvgen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\ntest_gen=tvgen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)","f7107241":"img_shape=(img_size[0], img_size[1], 3)\nmodel_name='EfficientNetB3'\nbase_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \nx=base_model.output\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=.45, seed=123)(x)        \noutput=Dense(class_count, activation='softmax')(x)\nmodel=Model(inputs=base_model.input, outputs=output)\nmodel.compile(Adamax(learning_rate=.001), loss='categorical_crossentropy', metrics=['accuracy']) ","25a96d36":"class ASK(keras.callbacks.Callback):\n    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n        super(ASK, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True # if True query the user on a specified epoch\n        \n    def on_train_begin(self, logs=None): # this runs on the beginning of training\n        if self.ask_epoch == 0: \n            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n            self.ask=False # do not query the user\n        if self.epochs == 1:\n            self.ask=False # running only for 1 epoch so do not query user\n        else:\n            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')  \n        self.start_time= time.time() # set the time at which training started\n        \n    def on_train_end(self, logs=None):   # runs at the end of training     \n        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n        hours = tr_duration \/\/ 3600\n        minutes = (tr_duration - (hours * 3600)) \/\/ 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) # print out training duration time\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask: # are the conditions right to query the user?\n            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n                ans=input()\n                \n                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True # halt training\n                else: # user wants to continue training\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush=True)\n                              \n                    else:\n                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)","ab68ba55":"rlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5,   patience=1,  verbose=1)\nestop=tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\",   patience=4,  verbose=1,   restore_best_weights=True)\nepochs=40\nask_epoch=2 # at end of 5th epoch ask to enter H to halt training or an integer for how many more epochs to run then ask again\ncallbacks=[rlronp, estop, ASK( model,epochs, ask_epoch)]   \n","f365dd35":"history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)","962bd01a":" acc= model.evaluate(test_gen, verbose= 1,  steps=test_steps)   \n print('Model accuracy on test set is ', acc[1]*100)","84fc8ffe":"working_dir=r'.\/'\nsave_path=os.path.join(working_dir, 'EfficientNetB3.h5')\nmodel.save(save_path, overwrite=True, include_optimizer=True, save_format='h5')","8d9b128e":"from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nsns.set_style('darkgrid')\nclasses=list(train_gen.class_indices.keys())\nclass_count=len(classes)\nlabels=test_gen.labels\nfiles=test_gen.filenames\nerror_file_list=[]\nindexes=[]\nerrors=0\npreds=model.predict(test_gen, steps=test_steps, verbose=1)\ntests=len(preds)\nfor i, p in enumerate (preds):\n    index=np.argmax(p) \n    indexes.append(index)\n    if index != labels[i]:\n        errors +=1\n        error_file_list.append(files[i])\nacc=( tests-errors)\/tests * 100\nprint(f'There were {errors}, errors in {tests} tests for an accuracy of {acc:6.2f}' )\nif errors > 0:\n    print ('A list of files that were incorrectly predicted is shown below')\n    for i in range (len(error_file_list)):\n        print (error_file_list[i])\n\nclr = classification_report(labels, indexes, target_names=classes, digits= 4)\nprint(\"Classification Report:\\n----------------------\\n\", clr)\ncm = confusion_matrix(labels, indexes )        \nlength=len(classes)\nif length<8:\n    fig_width=8\n    fig_height=8\nelse:\n    fig_width= int(length * .5)\n    fig_height= int(length * .5)\nplt.figure(figsize=(fig_width, fig_height))\nsns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \nplt.xticks(np.arange(length)+.5, classes, rotation= 90)\nplt.yticks(np.arange(length)+.5, classes, rotation=0)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","da070b15":"import cv2\nimg=plt.imread(img_path) # read in the image shown earlier\nprint ('Input image shape is ', img.shape)\n# resize the image so it is the same size as the images the model was trained on\nimg=cv2.resize(img, img_size) # in earlier code img_size=(150,150) was used for training the model\nprint ('the resized image has shape ', img.shape)\n### show the resized image\nplt.axis('off')\nplt.imshow(img)\n# Normally the next line of code rescales the images. However the EfficientNet model expects images in the range 0 to 255\n# img= img\/255\n# plt.imread returns a numpy array so it is not necessary to convert the image to a numpy array\n# since we have only one image we have to expand the dimensions of img so it is off the form (1,150,150,3)\n# where the first dimension 1 is the batch size used by model.predict\nimg=np.expand_dims(img, axis=0)\nprint ('image shape after expanding dimensions is ',img.shape)\n# now predict the image\npred=model.predict(img)\nprint ('the shape of prediction is ', pred.shape)\n# this dataset has 15 classes so model.predict will return a list of 15 probability values\n# we want to find the index of the column that has the highest probability\nindex=np.argmax(pred[0])\n# to get the actual Name of the class earlier Imade a list of the class names called classes\nklass=classes[index]\n# lets get the value of the highest probability\nprobability=pred[0][index]*100\n# print out the class, and the probability \nprint(f'the image is predicted as being {klass} with a probability of {probability:6.2f} %')","9c0f01d0":"## Define an early stop callback and a reduce learning rate callback and instantiate the ASK callback","22f20047":"## train_df is not balanced but should be OK","503d5d2c":"## evaluate model on the test set","0adc5f58":"## save the model","06b65a70":"# split df into a train_df, a test_df and a valid df","4ff63625":"## Use transfer learning with EfficientNetB3 model","0a8c39f8":"## define a custom callback that after ask_epoch asks if you want to continue training or halt.\n## You can enter an integer for how many more\n## epochs to run then be asked again, or enter H to halt training","4168ece2":"## do predictions on the test set and generate classification report","7bef7fb2":"## input an image and get the shape","09743d48":"## How to do predictions on a single image file","5b71417f":"## make train, test and valid generators","bd456188":"## train the model "}}