{"cell_type":{"6a2d4f17":"code","6434f6b2":"code","93f5d2d2":"code","1e2c5244":"code","7945e6af":"code","50d37baa":"code","6d553d47":"code","57495ba3":"code","178bb785":"code","0f6e7701":"code","9d2efb6a":"code","ec62f542":"code","46407f4c":"code","3329dbde":"code","708b32bc":"code","5bfa6215":"code","b46f78fd":"code","c0295b53":"code","dd1301d4":"code","f4614e95":"code","f582cf93":"code","3bf084ae":"code","34732bb0":"code","91565f26":"code","66a9aaf6":"code","fda8834f":"code","5eccb948":"code","339f54ca":"code","db5247a6":"markdown","5cb143e7":"markdown","19ea6c49":"markdown","44b9a565":"markdown","2cc910a5":"markdown","1178aa7f":"markdown","c570eac7":"markdown","fac11332":"markdown","3e89ef9a":"markdown","952ba253":"markdown","f39bfcaa":"markdown","e4ce2646":"markdown","7d2442bb":"markdown","98566c34":"markdown","ccff3124":"markdown","ef720f0e":"markdown","bb00e388":"markdown","43f95094":"markdown"},"source":{"6a2d4f17":"import gc\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import LayerNormalization\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.layers import Conv1D, Flatten, Dense\nfrom tensorflow.keras.layers import Input, Dropout, Activation\n\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\nfrom transformers import AlbertTokenizer, TFAlbertModel, AlbertConfig\nfrom transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig","6434f6b2":"! mkdir \".\/Bert-Base-Uncased\"\n! mkdir \".\/Albert-Base-V2\"\n! mkdir \".\/DistilBert-Base-Uncased\"","93f5d2d2":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_df[\"excerpt_wordlen\"] = train_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\ntrain_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\ntrain_df.set_index(\"id\", inplace=True)\nprint(f\"train_df: {train_df.shape}\\n\")\ntrain_df.head()","1e2c5244":"test_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_df[\"excerpt_wordlen\"] = test_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\ntest_df.drop(['url_legal','license'], inplace=True, axis=1)\ntest_df.set_index(\"id\", inplace=True)\nprint(f\"test_df: {test_df.shape}\\n\")\ntest_df.head()","7945e6af":"Ytrain = train_df['target'].values\nYtrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\ntrain_df.drop(['target'], inplace=True, axis=1)\nprint(f\"Ytrain: {Ytrain.shape}\")","50d37baa":"FOLD = 5\nNUM_SEED = 1\nVERBOSE = 1\nMINI_BATCH_SIZE = 16\nNUM_EPOCH = 20\nMAX_LEN = max(train_df['excerpt_wordlen'].max(), \n              test_df['excerpt_wordlen'].max()) + 11\n\nBERT_BASE_UNCASED = \"..\/input\/huggingface-bert-variants\/bert-base-uncased\/bert-base-uncased\"\nALBERT_BASE_V2 = \"..\/input\/albert-base-v2-tf2\"\nDISTILBERT_BASE_UNCASED = \"..\/input\/huggingface-bert-variants\/distilbert-base-uncased\/distilbert-base-uncased\"","6d553d47":"def sent_encode(texts, tokenizer):\n    input_ids = []\n    attention_mask = []\n    token_type_ids = []\n\n    for text in tqdm(texts):\n        tokens = tokenizer.encode_plus(text, max_length=MAX_LEN, truncation=True, \n                                       padding='max_length', add_special_tokens=True, \n                                       return_attention_mask=True, return_token_type_ids=True, \n                                       return_tensors='tf')\n        \n        input_ids.append(tokens['input_ids'])\n        attention_mask.append(tokens['attention_mask'])\n        token_type_ids.append(tokens['token_type_ids'])\n\n    return np.array(input_ids), np.array(attention_mask), np.array(token_type_ids)","57495ba3":"def rmse_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, dtype=tf.float32)\n    y_pred = tf.cast(y_pred, dtype=tf.float32)\n    return tf.math.sqrt(tf.math.reduce_mean((y_true - y_pred)**2))","178bb785":"def commonlit_model(transformer_model, use_tokens_type_ids=True):\n    \n    input_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n    token_type_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"token_type_ids\")\n\n    if use_tokens_type_ids:\n        embed = transformer_model(input_id, token_type_ids=token_type_id, attention_mask=attention_mask)[0]\n    \n    else:\n        embed = transformer_model(input_id, attention_mask=attention_mask)[0]\n    \n    #x = embed[:, 0, :]\n    embed = LayerNormalization()(embed)\n    \n    x = WeightNormalization(\n            Conv1D(filters=384, kernel_size=5, \n                   strides=2, padding='same', \n                   kernel_regularizer=l2(0.0001),\n                   kernel_initializer='he_uniform'))(embed)\n    x = LayerNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout1D(rate=0.25)(x)\n    \n    x = WeightNormalization(\n            Conv1D(filters=192, kernel_size=5, \n                   strides=2, padding='same', \n                   kernel_regularizer=l2(0.0001),\n                   kernel_initializer='he_uniform'))(x)\n    x = LayerNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout1D(rate=0.25)(x)\n    \n    x = Flatten()(x)\n    x = Dropout(rate=0.5)(x)\n    \n    x = Dense(units=1, kernel_initializer='lecun_normal')(x)\n\n    model = Model(inputs=[input_id, attention_mask, token_type_id], outputs=x, \n                  name='CommonLit_Readability_Model')\n    return model","0f6e7701":"tokenizer = BertTokenizer.from_pretrained(BERT_BASE_UNCASED)","9d2efb6a":"Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\nXtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")","ec62f542":"Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\nXtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")","46407f4c":"'''\nconfig = BertConfig.from_pretrained(\n    BERT_BASE_UNCASED,\n    hidden_dropout_prob=0.1, \n    attention_probs_dropout_prob=0.1\n)\n'''\nconfig = BertConfig()\nconfig.output_hidden_states = False\n\ntransformer_model = TFBertModel.from_pretrained(BERT_BASE_UNCASED, config=config)","3329dbde":"model = commonlit_model(transformer_model)\nmodel.summary()","708b32bc":"np.random.seed(23)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\ncounter = 0\noof_score = 0\ny_pred_final1 = 0\n\n\nfor sidx, seed in enumerate(seeds):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        tf.random.set_seed(seed)\n\n        model = commonlit_model(transformer_model)\n        \n        model.compile(loss=rmse_loss,\n                      metrics=[RootMeanSquaredError(name='rmse')],\n                      optimizer=Adam(lr=8e-5))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=5, verbose=VERBOSE)\n        \n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n                                      min_lr=1e-7, patience=2, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'.\/Bert-Base-Uncased\/CLRP_Bert_Base_Uncased_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model.fit(\n            [train_x_id, train_x_mask, train_x_token], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=NUM_EPOCH, \n            verbose=VERBOSE, \n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n        )\n        \n        model.load_weights(f'.\/Bert-Base-Uncased\/CLRP_Bert_Base_Uncased_{counter}C.h5')\n        \n        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n        y_pred_final1 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_final1 = y_pred_final1 \/ float(counter)\noof_score \/= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","5bfa6215":"tokenizer = AlbertTokenizer.from_pretrained(ALBERT_BASE_V2)","b46f78fd":"Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\nXtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")","c0295b53":"Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\nXtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")","dd1301d4":"'''\nconfig = AlbertConfig.from_pretrained(\n    ALBERT_BASE_V2,\n    hidden_dropout_prob=0.1, \n    attention_probs_dropout_prob=0.1\n)\n'''\nconfig = AlbertConfig(\n    hidden_size=768,\n    num_attention_heads=12,\n    intermediate_size=3072\n)\nconfig.output_hidden_states = False\n\ntransformer_model = TFAlbertModel.from_pretrained(ALBERT_BASE_V2, config=config)","f4614e95":"model = commonlit_model(transformer_model)\nmodel.summary()","f582cf93":"np.random.seed(29)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\ncounter = 0\noof_score = 0\ny_pred_final2 = 0\n\n\nfor sidx, seed in enumerate(seeds):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        tf.random.set_seed(seed)\n\n        model = commonlit_model(transformer_model)\n        \n        model.compile(loss=rmse_loss,\n                      metrics=[RootMeanSquaredError(name='rmse')],\n                      optimizer=Adam(lr=4e-5))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=5, verbose=VERBOSE)\n        \n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n                                      min_lr=1e-7, patience=2, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'.\/Albert-Base-V2\/CLRP_Albert_Base_V2_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model.fit(\n            [train_x_id, train_x_mask, train_x_token], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=NUM_EPOCH, \n            verbose=VERBOSE, \n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n        )\n        \n        model.load_weights(f'.\/Albert-Base-V2\/CLRP_Albert_Base_V2_{counter}C.h5')\n        \n        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n        y_pred_final2 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_final2 = y_pred_final2 \/ float(counter)\noof_score \/= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","3bf084ae":"tokenizer = DistilBertTokenizer.from_pretrained(DISTILBERT_BASE_UNCASED)","34732bb0":"Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\nXtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")","91565f26":"Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\nXtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")","66a9aaf6":"'''\nconfig = DistilBertConfig.from_pretrained(\n    DISTILBERT_BASE_UNCASED,\n    hidden_dropout_prob=0.1, \n    attention_probs_dropout_prob=0.1\n)\n'''\nconfig = DistilBertConfig()\nconfig.output_hidden_states = False\n\ntransformer_model = TFDistilBertModel.from_pretrained(DISTILBERT_BASE_UNCASED, config=config)","fda8834f":"model = commonlit_model(transformer_model, use_tokens_type_ids=False)\nmodel.summary()","5eccb948":"np.random.seed(31)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\ncounter = 0\noof_score = 0\ny_pred_final3 = 0\n\n\nfor sidx, seed in enumerate(seeds):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        tf.random.set_seed(seed)\n\n        model = commonlit_model(transformer_model, use_tokens_type_ids=False)\n        \n        model.compile(loss=rmse_loss,\n                      metrics=[RootMeanSquaredError(name='rmse')],\n                      optimizer=Adam(lr=8e-5))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=5, verbose=VERBOSE)\n        \n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n                                      min_lr=1e-7, patience=2, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'.\/DistilBert-Base-Uncased\/CLRP_DistilBert_Base_Uncased_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model.fit(\n            [train_x_id, train_x_mask, train_x_token], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=NUM_EPOCH, \n            verbose=VERBOSE, \n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n        )\n        \n        model.load_weights(f'.\/DistilBert-Base-Uncased\/CLRP_DistilBert_Base_Uncased_{counter}C.h5')\n        \n        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n        y_pred_final3 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_final3 = y_pred_final3 \/ float(counter)\noof_score \/= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","339f54ca":"y_pred_final = (y_pred_final1 + y_pred_final2 + y_pred_final3) \/ 3.0\n\nsubmit_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmit_df['target'] = y_pred_final\nsubmit_df.to_csv(\".\/submission.csv\", index=False)\nsubmit_df.head()","db5247a6":"### Generate word tokens and attention masks","5cb143e7":"## Bert-Base-Uncased Model","19ea6c49":"### Fit the model with K-Fold validation","44b9a565":"## Extract target label","2cc910a5":"## Albert-Base-V2 Model","1178aa7f":"## Import libraries","c570eac7":"## Model Hyperparameters","fac11332":"### Fit the model with K-Fold validation","3e89ef9a":"### Generate word tokens and attention masks","952ba253":"## DistilBert-Base-Uncased Model","f39bfcaa":"### Initialize the DistilBert-Base model","e4ce2646":"### Initialize the Bert-Base model","7d2442bb":"## Create submission file","98566c34":"## Helper Functions","ccff3124":"### Generate word tokens and attention masks","ef720f0e":"### Fit the model with K-Fold validation","bb00e388":"### Initialize the Albert-V2 model","43f95094":"## Load source datasets"}}