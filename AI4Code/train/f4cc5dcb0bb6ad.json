{"cell_type":{"c8c341c4":"code","763a4d0e":"code","bcb8f490":"code","021974a1":"code","d8edb168":"code","4bc7612a":"code","39d586d8":"code","5e3bd94a":"code","a48df020":"code","1dedb1ac":"code","ebdd1e0a":"code","beb7e3b5":"code","01ba7006":"code","f52b82fb":"code","3ce7b63e":"code","790d21c7":"code","3c6b3b2c":"code","ffcb5efd":"code","b00506fe":"code","2f9d618b":"code","d19dc0a8":"code","6a405a02":"code","eb54ba4d":"code","76fbca67":"code","d81326d8":"code","ab758c62":"code","47e87752":"code","10d855d9":"code","b7eb915a":"code","418c2340":"code","5c1b324c":"code","2e5b5994":"code","975a6177":"code","bfab9b05":"code","c053e02d":"markdown","740159fd":"markdown","17209dfb":"markdown","2ac729ce":"markdown","a0024a8b":"markdown","0ee70ac6":"markdown","ba787ae3":"markdown","ef8ffd75":"markdown","58b1e4c9":"markdown","b38c9327":"markdown"},"source":{"c8c341c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","763a4d0e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Embedding, Input,  Activation\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import initializers, optimizers, layers\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.simplefilter(action=\"ignore\")\n\n","bcb8f490":"train = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')","021974a1":"# Lets go through the data\n\ntrain.head()","d8edb168":"# Shape of the train set\n\ntrain.shape","4bc7612a":"test.head()","39d586d8":"# Shape of test set\ntest.shape","5e3bd94a":"# Describing the  train data\ntrain.describe()","a48df020":"# Getting the info about the train set\ntrain.info()","1dedb1ac":"train.isnull().sum()","ebdd1e0a":"# download the stopwords from NLTK\nimport nltk                                # Python library for NLP\nnltk.download('stopwords')","beb7e3b5":"import re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import regexp_tokenize   # module for tokenizing strings\nfrom nltk.tokenize import TreebankWordTokenizer","01ba7006":"# def process(comment):\n#     \"\"\"Process  function.\n#     Input:\n#         comment: a string containing a comment\n#     Output:\n#         comments_clean: a list of words containing the processed comment\n#     \"\"\"\n#     stemmer = PorterStemmer()\n#     stopwords_english = stopwords.words('english')\n#     # remove stock market tickers like $GE\n#     comment = re.sub(r'\\$\\w*', '', comment)\n#     # remove old style text \"RT\"\n#     comment = re.sub(r'^RT[\\s]+', '', comment)\n#     # remove hyperlinks\n#     comment = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', comment)\n#     # remove hashtags\n#     # only removing the hash # sign from the word\n#     comment = re.sub(r'#', '', comment)\n#     # tokenize comments\n#     tokenizer = TreebankWordTokenizer()\n#     comment_tokens = tokenizer.tokenize(comment)\n\n#     comments_clean = []\n#     for word in comment_tokens:\n#         if (word not in stopwords_english and  # remove stopwords\n#                 word not in string.punctuation):  # remove punctuation\n#             # tweets_clean.append(word)\n#             stem_word = stemmer.stem(word)  # stemming word\n#             comments_clean.append(stem_word)\n\n#     return comments_clean","f52b82fb":"# train['comment_text'] = train['comment_text'].apply(lambda x: process(x))","3ce7b63e":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","790d21c7":"# Applying the clean_text on train set\n\ntrain['comment_text'] = train['comment_text'].apply(lambda x: clean_text(x))","3c6b3b2c":"train['comment_text'].head()","ffcb5efd":"# Before Applying the clean_text on test set\n\ntest['comment_text'].head()","b00506fe":"# Applying the clean_text on test set\n\ntest['comment_text'] = test['comment_text'].apply(lambda x: clean_text(x))","2f9d618b":"# After Applying the clean_text on test set\n\ntest['comment_text'].head()","d19dc0a8":"# Definig a function to remove the stopwords\n\ndef remove_stopwords(text):\n    \n    words = [word for word in text if word not in stopwords.words('english')]\n    return words","6a405a02":"# # Applying the remove_stopwords on train set\n\n# train['comment_text'] = train['comment_text'].apply(lambda x: remove_stopwords(x))\n# train.head()","eb54ba4d":"# # Applying the remove_stopwords on test set\n\n# test['comment_text'] = test['comment_text'].apply(lambda x: remove_stopwords(x))\n# test.head()","76fbca67":"# Checking the count of the various types of words\n\ncols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntargets = train[cols].values\n\ntrain_df = train['comment_text']\ntest_df = test['comment_text']","d81326d8":"val_counts = train[cols].sum()\n\nplt.figure(figsize=(8,5))\nax = sns.barplot(val_counts.index, val_counts.values, alpha=0.8)\n\nplt.title(\"Comments per Classes\")\nplt.xlabel(\"Various Comments Type\")\nplt.ylabel(\"Counts of the Comments\")\n\nrects = ax.patches\nlabels = val_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height+5, label, ha=\"center\", va=\"bottom\")\n\n\nplt.show()","ab758c62":"# Word Cloud for train set\n\nfrom wordcloud import WordCloud\nwords = ' '.join([text for text in train['comment_text'] ])\n\n\nword_cloud = WordCloud(\n                       width=1600,\n                       height=800,\n                       #colormap='PuRd', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\").generate(words)\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.title('Comments and their Nature', fontsize = 40)\nplt.axis(\"off\")\nplt.show()\n\n","47e87752":"# Word Cloud for test set\n\nwords = ' '.join([text for text in test['comment_text'] ])\n\n\nword_cloud = WordCloud(\n                       width=1600,\n                       height=800,\n                       #colormap='PuRd', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\").generate(words)\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"bilinear\")\nplt.title('Comments and their Nature', fontsize = 40)\nplt.axis(\"off\")\nplt.show()","10d855d9":"max_features = 22000\ntokenizer = Tokenizer(num_words = max_features)\ntokenizer.fit_on_texts(list(train_df))\n\ntokenized_train = tokenizer.texts_to_sequences(train_df)\ntokenized_test = tokenizer.texts_to_sequences(test_df)","b7eb915a":"maxlen = 200\nX_train = pad_sequences(tokenized_train, maxlen = maxlen)\nX_test = pad_sequences(tokenized_test, maxlen = maxlen)","418c2340":"embed_size = 128\nmaxlen = 200\nmax_features = 22000\n\ninp = Input(shape = (maxlen, ))\nx = Embedding(max_features, embed_size)(inp)\nx = LSTM(60, return_sequences=True, name='lstm_layer')(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)","5c1b324c":"model = Model(inputs=inp, outputs=x)\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)","2e5b5994":"model.summary()","975a6177":"batch_size = 64\nepochs = 2\nmodel.fit(X_train, targets, batch_size=batch_size, epochs=epochs, validation_split=0.1)","bfab9b05":"prediction = model.predict(X_test)\nprediction","c053e02d":"# Preprocessing the Comments of the Train Set","740159fd":"##### Plotting the word Cloud","17209dfb":"From above we can see that most of the comments has been labelled as toxic in nature followed by obscence comments and insult comments","2ac729ce":"Guys if you feel that this notebook is useful please upvote\n\nFeel free to Comment for any suggestions or queries","a0024a8b":"Yay! there is no missing values","0ee70ac6":"Considering all the above mentioned method in single function process","ba787ae3":"The next step is to remove stop words. Stop words are words that don't add significant meaning to the text.","ef8ffd75":"From above we can see that there is no any missing values \nIn fact we can check this too by using the commands of the next line\n\n","58b1e4c9":"Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\n* Tokenizing the string\n* Lowercasing\n* Removing stop words and punctuation\n* Stemming\n\n\n### Tokenize the String\nTo tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The tokenize module from NLTK allows us to do these easily:\n\n\n### Remove stop words and punctuations\nThe next step is to remove stop words and punctuation. Stop words are words that don't add significant meaning to the text. You'll see the list provided by NLTK when you run the cells below.\n\n### Stemming\nStemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n\nConsider the words:\n\n* learn\n* learning\n* learned\n* learnt\nAll these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That's because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n\n* happy\n* happiness\n* happier\nWe can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen","b38c9327":"# Importing the necessary Libraries"}}