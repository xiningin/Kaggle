{"cell_type":{"04c86af5":"code","999a4a83":"code","241c09df":"code","47fbab28":"code","275b5650":"code","ad72a00d":"code","865c0dab":"code","c1bc3980":"code","acac25c4":"code","8db5eff5":"code","33abe5a9":"code","43d09b11":"code","b1275c0a":"code","7a37a215":"code","99977047":"code","7d1d4d17":"code","a573f948":"code","e808d226":"code","dd777e6d":"code","582cdbda":"code","b077ddde":"code","f2957bb4":"code","457c0644":"code","1ff18a46":"code","6e07d574":"code","2754b7b0":"code","65c1eb25":"code","d27e1347":"code","9bd9bc08":"code","7bed3eb6":"code","cca81779":"code","ffa003f9":"code","f8791266":"code","d657aa55":"code","0476db7e":"code","aa68fa3a":"code","f3956602":"code","582867cf":"code","0ac3f763":"code","268c30dc":"code","514794d5":"code","3f3fe9a6":"code","f91e5a9d":"code","08114f9b":"markdown","b5e86f42":"markdown","7caf5bd5":"markdown","5deda250":"markdown","4bf7952e":"markdown","8178f221":"markdown","3478ce99":"markdown","da18ca96":"markdown","88662ada":"markdown","e3c09bfb":"markdown","03ccab96":"markdown","6703aaaf":"markdown","b2beaf80":"markdown","b2048b5b":"markdown","a58238cd":"markdown","93481924":"markdown","0a09ea94":"markdown","93ce0b48":"markdown","ac9bd1f5":"markdown","48ec0eee":"markdown","35dba9db":"markdown","7ccafb02":"markdown","173416bf":"markdown","6ceac569":"markdown","97deeb55":"markdown","ead680f0":"markdown","76b05d63":"markdown","58a3866a":"markdown","b3c3b497":"markdown","51718f61":"markdown"},"source":{"04c86af5":"import numpy as np\nimport pandas as pd\n\nnp.random.seed(0) \nimport random\n\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","999a4a83":"# (X_train, y_train), (X_test, y_test) = mnist.load_data()\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\n\nX_train = train.drop(labels = [\"label\"], axis = 1)\ny_train = train['label']\n\nX_test = test\n\nprint(X_train.shape, X_test.shape)","241c09df":"X_train_plot = X_train.values.reshape(-1, 28, 28)","47fbab28":"def Show_example_digits(mono = 'gray'):\n    fig = plt.figure(figsize = (16, 16))\n    for idx in range(15):\n        plt.subplot(5, 5,idx+1)\n        plt.imshow(X_train_plot[idx], cmap = mono)\n        plt.title(\"Digit {}\".format(y_train[idx]))\n        \n    plt.tight_layout()\n    \nShow_example_digits()","275b5650":"# Function return digit in grayscale\ndef plot_digit(digit, dem = 28, font_size = 12):\n    max_ax = font_size * dem\n    \n    fig = plt.figure(figsize=(13, 13))\n    plt.xlim([0, max_ax])\n    plt.ylim([0, max_ax])\n    plt.axis('off')\n    black = '#000000'\n    \n    for idx in range(dem):\n        for jdx in range(dem):\n\n            t = plt.text(idx * font_size, max_ax - jdx*font_size, digit[jdx][idx], fontsize = font_size, color = black)\n            c = digit[jdx][idx] \/ 255.\n            t.set_bbox(dict(facecolor=(c, c, c), alpha = 0.5, edgecolor = 'black'))\n            \n    plt.show()","ad72a00d":"rand_number = random.randint(0, len(y_train))\nprint(y_train[rand_number])\nplot_digit(X_train_plot[rand_number])","865c0dab":"digit_range = np.arange(10)\n\nval = y_train.value_counts().index\ncnt = y_train.value_counts().values\nmycolors = ['red', 'blue', 'green', 'orange', 'brown', 'grey', 'pink', 'olive', 'deeppink', 'steelblue']\n\nplt.figure(figsize = (15, 7))\nplt.title(\"The number of digits in the data\", fontsize = 20)\nplt.xticks(range(10))\nplt.bar(val, cnt, color = mycolors);","c1bc3980":"img_rows, img_cols = 28, 28\n\nnum_pixels = X_train.shape[1] \n\ninput_shape = (img_rows, img_cols)","acac25c4":"# Data Normalization [0, 1]\nX_train \/= 255\nX_test \/= 255\n\n# one-hot encoding for target column\ny_train = to_categorical(y_train)\n\n# | [0, 1, 2, ... , 9] | = 10\nnum_classes = y_train.shape[1]\n\n# Number of objects, vector size (28 * 28)\nprint(X_train.shape, X_test.shape)","8db5eff5":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 2)","33abe5a9":"def draw_learning_curve(history, key='accuracy', ylim=(0.8, 1.01)):\n    plt.figure(figsize=(12,6))\n    plt.plot(history.history[key])\n    plt.plot(history.history['val_' + key])\n    plt.title('Learning Curve')\n    plt.ylabel(key.title())\n    plt.xlabel('Epoch')\n    plt.ylim(ylim)\n    plt.legend(['train', 'test'], loc='best')\n    plt.show()","43d09b11":"def get_mlp():\n    \n    return Sequential([\n        #input layer is automatic generation by keras\n        \n        #hidden layer\n        Dense(512, input_dim = num_pixels, activation='relu'),\n        \n        #output layer\n        Dense(num_classes, activation='softmax')\n    ])","b1275c0a":"model = get_mlp()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","7a37a215":"X_train.shape","99977047":"learning_history = model.fit(X_train, y_train,\n          batch_size = 1024, epochs = 40, verbose = 2,\n          validation_data=(X_val, y_val));","7d1d4d17":"score = model.evaluate(X_val, y_val, verbose = 0)\nprint('Test loss: {}%'.format(score[0] * 100))\nprint('Test accuracy: {}%'.format(score[1] * 100))\n\nprint(\"MLP Error: %.2f%%\" % (100 - score[1] * 100))","a573f948":"draw_learning_curve(learning_history, 'accuracy', ylim = (0.95, 1.001))","e808d226":"def get_mlpv2():\n    \n    return Sequential([\n        Dense(512, input_dim=num_pixels, activation='relu'),\n        Dropout(0.3),\n        Dense(256, activation='relu'),\n        Dropout(0.2),\n        Dense(128, activation='relu'),\n        Dense(num_classes, kernel_initializer='normal', activation='softmax')\n    ])","dd777e6d":"model = get_mlpv2()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","582cdbda":"learning_history = model.fit(X_train, y_train,\n          batch_size = 1024, epochs = 40, verbose = 2,\n          validation_data=(X_val, y_val));","b077ddde":"draw_learning_curve(learning_history, 'accuracy', ylim = (0.97,1.))","f2957bb4":"score = model.evaluate(X_val, y_val, verbose = 0)\nprint('Test loss: {}%'.format(score[0] * 100))\nprint('Test accuracy: {}%'.format(score[1] * 100))\n\nprint(\"MLP Error: %.2f%%\" % (100 - score[1] * 100))","457c0644":"X_train.shape","1ff18a46":"X_train = X_train.values.reshape(-1, 28, 28, 1)\nX_val = X_val.values.reshape(-1, 28, 28, 1)\nX_test = X_test.values.reshape(-1, 28, 28, 1)\ninput_shape = (28, 28, 1)","6e07d574":"def get_triplecnn():\n    return Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape = input_shape),\n        Conv2D(32, kernel_size=(3, 3), activation='relu' ),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n        Conv2D(64, kernel_size=(3, 3), activation='relu' ),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        Conv2D(128, kernel_size=(3, 3), activation='relu' ),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        \n        Flatten(),\n        \n        Dense(256, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation = \"softmax\")\n        \n    ])","2754b7b0":"model = get_triplecnn()\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel.summary()","65c1eb25":"learning_history = model.fit(X_train, y_train,\n          batch_size = 128,\n          epochs = 50,\n          verbose = 1,\n          validation_data = (X_val, y_val))","d27e1347":"score = model.evaluate(X_val, y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nprint(\"CNN Error: %.2f%%\" % (100-score[1]*100))","9bd9bc08":"draw_learning_curve(learning_history, 'accuracy', ylim = (0.98,1.))","7bed3eb6":"y_pred = model.predict(X_val)","cca81779":"def draw_output(idx_nums):\n    plt.figure(figsize = (20, 20))\n    plt.xticks( range(10) )\n    x = np.ceil(np.sqrt(len(idx_nums)))\n    cnt = 1\n    for ph in idx_nums:\n        plt.subplot(x, x, cnt)\n        curr_photo = y_val[ph]\n        \n        plt.xlim(0, 10)\n        plt.title(\"Digit: {0}\\n idx: {1} \".format(np.argmax(y_val[ph]), ph), fontsize = 10) \n        plt.bar(range(10), y_pred[ph])\n        \n        cnt += 1","ffa003f9":"cnt_error = []\nfor idx, (a, b) in enumerate(zip(y_val, y_pred)):\n    if np.argmax(a) == np.argmax(b): continue\n    cnt_error.append( (np.argmax(a)) )\n\ncnt_error = np.unique(cnt_error, return_counts = True)\nsns.set_style(\"darkgrid\")\nplt.figure(figsize = (15, 7))\nbar_plot = sns.barplot(cnt_error[0], cnt_error[1], palette=\"muted\")\nplt.show()","f8791266":"cnt_ind = 1\nlist_idx = []\nX_val_plot = X_val.reshape( X_val.shape[:-1] )\nfig = plt.figure(figsize=(14, 14))\n\nfor idx, (a, b) in enumerate(zip(y_val, y_pred)):\n    if np.argmax(a) == np.argmax(b): continue\n    if (np.argmax(a) == 2 or np.argmax(a) == 9):    \n        plt.subplot(5, 5, cnt_ind)\n        plt.imshow(X_val_plot[idx], cmap='gray', interpolation='none')\n        plt.title('y_true={0}\\ny_pred={1}\\n ind = {2}'.format(np.argmax(a), np.argmax(b), idx))\n        plt.tight_layout()\n        list_idx.append(idx)\n        cnt_ind += 1","d657aa55":"draw_output(list_idx)","0476db7e":"train_aug = ImageDataGenerator(\n        featurewise_center = False,\n        samplewise_center = False,\n        featurewise_std_normalization = False, \n        samplewise_std_normalization = False,\n        zca_whitening = False,\n        horizontal_flip = False,\n        vertical_flip = False,\n        fill_mode = 'nearest',\n        rotation_range = 10,  \n        zoom_range = 0.1, \n        width_shift_range = 0.1, \n        height_shift_range = 0.1)\n        \n\ntrain_aug.fit(X_train)\ntrain_gen = train_aug.flow(X_train, y_train, batch_size=64)","aa68fa3a":"def get_newtriplecnn():\n    return Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape = input_shape),\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        \n        Flatten(),\n          \n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        \n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.4),\n        \n        Dense(64, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.3),\n        \n        Dense(num_classes, activation = \"softmax\")\n        \n    ])","f3956602":"model = get_newtriplecnn()\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel.summary()","582867cf":"callbacks1 = [ \n    EarlyStopping(monitor = 'loss', patience = 6), \n    ReduceLROnPlateau(monitor = 'loss', patience = 3), \n    ModelCheckpoint('..\/working\/model.best.hdf5', save_best_only=True) # saving the best model\n]","0ac3f763":"history = model.fit_generator((train_gen), epochs = 100, \n                               steps_per_epoch = X_train.shape[0] \/\/ 64,\n                               validation_data = (X_val, y_val),\n                               callbacks = callbacks1,\n                             )","268c30dc":"model = load_model('..\/working\/model.best.hdf5')","514794d5":"score = model.evaluate(X_val, y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nprint(\"CNN Error: %.2f%%\" % (100-score[1]*100))","3f3fe9a6":"draw_learning_curve(history, 'accuracy', ylim = (0.985,1.))","f91e5a9d":"output = model.predict(X_test)\n\noutput = np.argmax(output, axis = 1)\n\noutput = pd.Series(output, name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001), name = \"ImageId\"), output], axis = 1)\n\nsubmission.to_csv(\"submission.csv\", index=False)","08114f9b":"# Adding Callbacks\n- EarlyStopping (Stop training when a monitored metric has stopped improving)\n- ReduceLROnPlateau (Reduce learning rate when a metric has stopped improving)\n- ModelCheckpoint (Callback to save the Keras model or model weights at some frequency)","b5e86f42":"#### Let's see these photos (2, 9)","7caf5bd5":"#### Let's assign the values provided by the model","5deda250":"## Data Augmentation","4bf7952e":"## MLP ([Multilayer perceptron](https:\/\/en.wikipedia.org\/wiki\/Multilayer_perceptron))","8178f221":"## Data Preparing","3478ce99":"# Loading the best model","da18ca96":"#### Chart of the number of digits in the data","88662ada":"# Adding new layer and Dropout to avoid overfitting","e3c09bfb":"#### The number of errors for the each digit","03ccab96":"# Generate output","6703aaaf":"# Split data","b2beaf80":"# Importing","b2048b5b":"#### Run the `draw_output` function to see the probability of each value occurring","a58238cd":"### About MNIST Dataset","93481924":"We achieved a great result of 99.6% accuracy","0a09ea94":"#### We need to reshape data.","93ce0b48":"#### Building new model and using batch normalization","ac9bd1f5":"### Let's see in which cases the model is invalid.","48ec0eee":"It will use 3 convolutional layers: (Conv2D, Conv2D, pool)","35dba9db":"## Exploratory Data Analysis","7ccafb02":"#### As you can see, the model is wrong in cases where the common person would also have trouble finding the correct answer.","173416bf":"# ~98% accuracy with easy model MLP ","6ceac569":"# Run new model ","97deeb55":"## CNN ([Convolutional_neural_network](https:\/\/en.wikipedia.org\/wiki\/Convolutional_neural_network))","ead680f0":" The **MNIST** database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The **MNIST** database contains 60,000 training images and 10,000 testing images.  Photo size: **28x28 p**.","76b05d63":"# Digit Recognizer (Experimenting on MNIST)","58a3866a":"**You can use GPU to accelerate model training**","b3c3b497":"#### I made function to visual output","51718f61":"##  Reading data"}}