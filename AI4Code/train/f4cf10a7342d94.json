{"cell_type":{"b8f5945d":"code","8b902917":"code","67fd4fd6":"code","deadeeb6":"code","05021083":"code","1d8a773a":"code","9ad0f81a":"code","38bd5889":"code","1ef20b44":"code","0b8c3c3b":"code","ffd2a1b8":"code","6c2716b6":"code","b0c7dc82":"code","c4d6b3bb":"code","0a74d453":"code","6977559f":"code","63d29df1":"code","d61dc3a8":"code","c7a7d52d":"code","5c2a278a":"code","3c07718f":"code","86d063ec":"code","9fa06529":"code","52500c60":"code","202d5d75":"code","5095c009":"code","6529caf0":"code","88fba1fa":"code","6900f5ca":"code","2c296863":"code","8d182485":"code","1b91fdea":"code","b4d3ac7c":"code","91863478":"code","fb38ba63":"code","797fdf86":"code","5ca54145":"code","d86dd4ac":"code","65f1042c":"markdown","85fc2ac9":"markdown","f7a52e93":"markdown","dc605cf2":"markdown","0985722e":"markdown"},"source":{"b8f5945d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import norm, rankdata\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn import preprocessing\n\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8b902917":"data = pd.read_csv('..\/input\/train.csv')","67fd4fd6":"test_data = pd.read_csv('..\/input\/test.csv')","deadeeb6":"t_d = test_data.copy()","05021083":"test_data.drop(columns=['ID_code'],axis=1,inplace=True)","1d8a773a":"#data.describe().transpose()","9ad0f81a":"data.info()","38bd5889":"#X.columns","1ef20b44":"#Y.head()","0b8c3c3b":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components=110)\n# X_dat_pca = pca.fit_transform(X)","ffd2a1b8":"#X_dat_pca[0]","6c2716b6":"#test_data_pca=pca.transform(test_data)","b0c7dc82":"#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X_dat_pca,Y.values,test_size=0.10,random_state=42)\n","c4d6b3bb":"one_indices= np.array(data[data.target==1].index)\nzero_indices = np.array(data[data.target==0].index)\n#now let us a define a function for make undersample data with different proportion\n#different proportion means with different proportion of normal classes of data\ndef undersample(zero_indices,one_indices,times):#times denote the normal data = times*fraud data\n    Normal_indices_undersample = np.array(np.random.choice(zero_indices,(times*Count_insincere_transacation),replace=False))\n    print(len(Normal_indices_undersample))\n    undersample_data= np.concatenate([one_indices,Normal_indices_undersample])\n\n    undersample_data = data.iloc[undersample_data,:]\n    #print(undersample_data)\n    print(len(undersample_data))\n\n#     print(\"the normal transacation proportion is :\",len(undersample_data[undersample_data.target==0])\/len(undersample_data))\n#     print(\"the fraud transacation proportion is :\",len(undersample_data[undersample_data.target==1])\/len(undersample_data))\n    print(\"total number of record in resampled data is:\",len(undersample_data))\n    return(undersample_data)\n","0a74d453":"Count_insincere_transacation = len(data[data[\"target\"]==1]) # fraud by 1","6977559f":"Undersample_data = undersample(zero_indices,one_indices,3)","63d29df1":"df = Undersample_data.copy()\nY = df['target']\ndf.drop(columns=['target','ID_code'],axis=1,inplace=True)\nX = df.loc[:,:]","d61dc3a8":"from sklearn.decomposition import PCA\npca = PCA(n_components=110)\nX = preprocessing.normalize(X)\nX = preprocessing.scale(X)\n\nX_dat_pca = pca.fit_transform(X)\nX_dat_pca = preprocessing.normalize(X_dat_pca)\nX_dat_pca = preprocessing.scale(X_dat_pca)","c7a7d52d":"Y=Y.values","5c2a278a":"np.isnan(np.min(X_dat_pca))","3c07718f":"test_data = preprocessing.normalize(test_data)\ntest_data = preprocessing.scale(test_data)\n\ntest_data_pca=pca.transform(test_data)\ntest_data_pca = preprocessing.normalize(test_data_pca)\ntest_data_pca = preprocessing.scale(test_data_pca)\n","86d063ec":"X_dat_pca=X_dat_pca.astype(float)","9fa06529":"test_data_pca=test_data_pca.astype(float)","52500c60":"NFOLDS = 30\nRANDOM_STATE = 120\n\n\nfolds = StratifiedKFold(n_splits=NFOLDS, shuffle=True,random_state=RANDOM_STATE)\noof_preds = np.zeros((len(X_dat_pca), 1))\ntest_preds = np.zeros((len(test_data_pca), 1))\nroc_cv =[]","202d5d75":"type(Y)","5095c009":"for fold_, (trn_, val_) in enumerate(folds.split(Y, Y)):\n    print(\"Current Fold: {}\".format(fold_))\n    trn_x, trn_y = X_dat_pca[trn_, :], Y[trn_]\n    val_x, val_y = X_dat_pca[val_, :], Y[val_]\n    #print(trn_)\n    #print(trn_y)\n    \n    clf = Pipeline([\n        ('lr_clf', LogisticRegression(solver='lbfgs', max_iter=10000))\n    ])\n\n    clf.fit(trn_x, trn_y)\n\n    val_pred = clf.predict_proba(val_x)[:,1]\n    test_fold_pred = clf.predict_proba(test_data_pca)[:,1]\n    \n    roc_cv.append(roc_auc_score(val_y, val_pred))\n    \n    print(\"AUC = {}\".format(roc_auc_score(val_y, val_pred)))\n    oof_preds[val_, :] = val_pred.reshape((-1, 1))\n    test_preds += test_fold_pred.reshape((-1, 1))","6529caf0":"# param = {\n#     'max_depth': 20,  # the maximum depth of each tree\n#     'eta': 0.001,  # the training step for each iteration\n#     'silent': 1,\n#     'objective': 'binary:logistic'\n#      } \n# param['nthread'] = 4\n# # the number of classes that exist in this datset\n# num_round = 1000  # the number of training iterations\n# for fold_, (trn_, val_) in enumerate(folds.split(Y, Y)):\n#     print(\"Current Fold: {}\".format(fold_))\n#     trn_x, trn_y = X_dat_pca[trn_, :], Y[trn_]\n#     val_x, val_y = X_dat_pca[val_, :], Y[val_]\n#     #print(trn_)\n#     #print(trn_y)\n#     dtrain = xgb.DMatrix(trn_x, label=trn_y)\n#     dtest = xgb.DMatrix(val_x, label=val_y)\n#     dtest1=xgb.DMatrix(test_data_pca)\n    \n#     bst = xgb.train(param, dtrain, num_round)\n\n#     preds = bst.predict(dtest)\n#     #preds=int(preds)\n#     #test_fold_pred = bst.predict(dtest1)\n#     #best_preds = np.asarray([np.argmax(line) for line in preds])\n    \n#     roc_cv.append(roc_auc_score(val_y, preds))\n    \n#     print(\"AUC = {}\".format(roc_auc_score(val_y, preds)))\n#     #oof_preds[val_, :] = preds.reshape((-1, 1))\n#     #test_preds += test_fold_pred.reshape((-1, 1))","88fba1fa":"test_preds=test_preds\/NFOLDS","6900f5ca":"# from sklearn.linear_model import LogisticRegression\n# lgr = LogisticRegression(solver='lbfgs',max_iter=1000)","2c296863":"# lgr.fit(X_train,y_train)\n# lgr_training_predictions = lgr.predict(X_train)\n# print(lgr_training_predictions.shape)\n# print(y_train.shape)\n# from sklearn.metrics import accuracy_score\n# print(accuracy_score(y_train,lgr_training_predictions))","8d182485":"# lgr_test_pred=lgr.predict(X_test)\n# print(accuracy_score(y_test,lgr_test_pred))","1b91fdea":"# test_data_predictions = lgr.predict(test_data_pca)","b4d3ac7c":"# test_data_predictions[:20]","91863478":"print(\"Saving submission file\")\nsample = pd.read_csv('..\/input\/sample_submission.csv')\nsample.target = test_preds.astype(float)\nsample.ID_code = t_d[\"ID_code\"]\nsample.to_csv('submission.csv', index=False)","fb38ba63":"# submission_lgr = pd.DataFrame({\n#         \"ID_code\": t_d[\"ID_code\"],\n#         \"target\": test_preds\n#     })\n# submission_lgr.to_csv('submission_ens.csv', index=False)","797fdf86":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components=None)\n# pca.fit(X)","5ca54145":"# a = np.array(pca.explained_variance_ratio_)","d86dd4ac":"# su=0.0\n# for i,b in enumerate(a):\n#     su = su + b\n#     if(su<=0.95):\n#         print(i)","65f1042c":"## We read the data from the give dataset.","85fc2ac9":"## We understood what exactly is there in the data.\n1. ### Number of independent variables (X) are 200\n2. ### There's ID column avaliable\n3. ### Dependent variable is 1(Y) i.e target\n4. ### Total Number of rows are 200000","f7a52e93":"### Applying PCA to check the variance of each variable and deciding the number of components for the data.","dc605cf2":"### Checking the columns of independent variable it includes Id as well","0985722e":"### Here We divided the data into X and Y"}}