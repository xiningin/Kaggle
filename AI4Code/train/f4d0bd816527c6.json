{"cell_type":{"1b4b97d8":"code","805c07ee":"code","609ca684":"code","7611848d":"code","2d9af7f0":"code","4f2e9123":"code","d3f05721":"code","f3fe0cf0":"code","18f134c6":"code","c18eacb0":"code","55ffc245":"code","02d20011":"code","d76a1dc5":"code","e0ca9d71":"code","0287a5e5":"code","154c4776":"code","82638285":"code","52822f81":"code","07a31fac":"code","92c8b1d4":"code","00db00a2":"code","dde02b97":"code","ddb4bdd9":"code","28f140a8":"code","5dafed5a":"code","db0f6caf":"code","a785dff0":"code","cf9e2193":"code","52241d41":"code","b92525e7":"code","fb73ac75":"code","19bfe0de":"code","7f6ed747":"code","5fbc288b":"code","e3720e48":"code","327477e3":"code","5a5b9596":"code","e0ad00c7":"code","20c86881":"code","9c7e0894":"code","82143635":"markdown","57507fe4":"markdown","342339bf":"markdown","0a70d5db":"markdown","0a5e42c4":"markdown","88c4006a":"markdown","e2fc7aed":"markdown","115128ca":"markdown","47a456ea":"markdown","320258b3":"markdown","639b7d32":"markdown","1a53de78":"markdown","ac4b6fa4":"markdown","9fb074cd":"markdown","eb361f47":"markdown","ed89fc6c":"markdown","d0d17614":"markdown","954b3417":"markdown","c9c98154":"markdown","ccfeb681":"markdown","08d3e22d":"markdown","bcc5c800":"markdown","bd9b7595":"markdown","a0b93cd3":"markdown","ba4928ec":"markdown","efacd8af":"markdown","fcb8f786":"markdown","5c9abdb3":"markdown","3881b8aa":"markdown","932364de":"markdown","a76cb03c":"markdown","2c6034e0":"markdown","f64780ad":"markdown","5f375ade":"markdown","945e535e":"markdown","a66c784d":"markdown","d138e9be":"markdown","6ea93a75":"markdown","d2a2f069":"markdown","e2f3781c":"markdown","ef8bfa84":"markdown","e8755c2e":"markdown","c5ad34da":"markdown","4936ca7e":"markdown","fddc430a":"markdown","11f9a9a7":"markdown","1d711814":"markdown"},"source":{"1b4b97d8":"from matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torch.autograd import Variable\nimport torch\nfrom torchvision import transforms","805c07ee":"def to_np(v):\n    \"\"\"\n    Converts Variable or Tensor to numpy array.\n    -------------------------------------------------------\n    Parameters:\n        v: A pyTorch Variable or Tensor\n    Output:\n        Return a numpy array\n    \"\"\"\n    if isinstance(v, (np.ndarray, np.generic)): return v\n    if isinstance(v, (list,tuple)): return [to_np(o) for o in v]\n    if isinstance(v, Variable): v=v.data\n    if torch.cuda.is_available():\n        if isinstance(v, torch.cuda.HalfTensor): v=v.float()\n    if isinstance(v, torch.FloatTensor): v=v.float()\n    return v.cpu().numpy()\n\ndef score(model, x, y=None, ret=0):\n    \"\"\"\n    Get either predicted probabilities or predictions or total right predictions \n    for r = 1, 2, 0 respectively.\n    ---------------------------------------------------------------------------------\n    Parameters:\n        model: A Neural Network Model\n        x: A Variable to be sent to model() function to get predictions\n        y: (default=None) Actual Labels. If y not given ret can only be 1 or 2.\n        ret: A parameter to tell what to return. '1' for predicted probanilities,\n            '2' for predictioins and '0' for total right predicions and predicted\n            probabilities.\n    \"\"\"\n    y_pred = model(x)\n    if ret == 1:\n        return to_np(y_pred)  # Numpy array of probabilities (size : batch_size x 10)\n    elif ret == 2:\n        return to_np(y_pred).argmax(1) # Numpy array of predictions\n    else:\n        ypred_argmax = to_np(y_pred).argmax(axis=1)\n        return np.sum(ypred_argmax == to_np(y)), y_pred # Total correct predctions and pytorch Variable of probabilities\n    \ndef accuracy(preds, targs):\n    \"\"\"\n    To calculate accuracy of predicted values.\n    -----------------------------------------------\n    Parameters:\n        preds: A Variable with predicted probabilities\n        targs: Variable of Actual predictions\n    Output:\n        Return accuracy of model.\n    \"\"\"\n    preds = torch.max(preds, dim=1)[1]\n    return (preds==targs).float().mean()\n\ndef show(img, title=None):\n    \"\"\"\n    Function to plot a single image.\n    -------------------------------------------------\n    Parameters:\n        img: A numpy array of image\n        title: Title for plot\n    Output:\n        Plots image using matplotlib.\n    \"\"\"\n    plt.imshow(img, cmap=\"gray\")\n    if title is not None: plt.title(title)\n\ndef plots(ims, figsize=(12, 6), rows=2, titles=None):\n    \"\"\"\n    Plot multiple images in diff. subplots, configured by parameter \"rows\".\n    ----------------------------------------------------------------------------\n    Parameters:\n        ims: An numpy array of arrays of diff images\n        figsize: parameter to be passed to matplotlib \"plt\" function\n        rows: number of rows in plot for subplots\n        titles: Array of titles for all images\n    Output:\n        Plot a matplotlib plot with (r*c) subplots\n    \"\"\"\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)\/\/rows\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None: sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], cmap='gray')","609ca684":"PATH = \"..\/input\/\"","7611848d":"train = pd.read_csv(f'{PATH}train.csv')\ntest = pd.read_csv(f'{PATH}test.csv')","2d9af7f0":"test.shape","4f2e9123":"idx = np.random.permutation(train.shape[0])\nvalid_idx, train_idx = idx[:10000], idx[10000:]\nxtrain, xvalid = train.iloc[train_idx,:].values, train.iloc[valid_idx,:].values\n\nytrain, yvalid = xtrain[:, 0], xvalid[:,0]\nxtrain, xvalid = xtrain[:, 1:], xvalid[:, 1:]","d3f05721":"plt.hist(train.iloc[:,0])","f3fe0cf0":"mean = xtrain.mean()\nstd = xtrain.std()","18f134c6":"# Normalization\nxtrain = (xtrain-mean)\/std\nxvalid = (xvalid-mean)\/std","c18eacb0":"# Reshape to image dimensions.\n\nxtrain_imgs = np.reshape(xtrain, (-1, 1, 28, 28))\nxtrain_imgs.shape","55ffc245":"import cv2\n# cv2.setUseOptimized(True); cv2.useOptimized()\n\ndef rotate_cv(im_arr):\n    \"\"\"\n    Randomly rotate array of images using opencv.\n    ---------------------------------------------------\n    Parameters:\n        im_arr: array of numpy array of images.\n    Output:\n        Return array of numpy arrays of rotated images.\n    \"\"\"\n    output = []\n    degs = np.random.randint(-20, 20, len(im_arr) )\n    for i in range(len(im_arr)):\n        img = np.reshape(im_arr[i], (28, 28) )\n        r,c,*_ = img.shape\n        M = cv2.getRotationMatrix2D((c\/\/2,r\/\/2),degs[i],1)  # Rotating max of 20 deg in clockwise or anti-clockwise\n        out = cv2.warpAffine(img,M,(c,r))\n        output.append(out)\n    return np.array(output)\n\ndef translate_cv(im_arr):\n    \"\"\"\n    Randomly translate array of images using opencv.\n    ---------------------------------------------------\n    Parameters:\n        im_arr: array of numpy array of images.\n    Output:\n        Return array of numpy arrays of translated images.\n    \"\"\"\n    output = []\n    pos = np.random.randint(-7, 7, len(im_arr))\n    for i in range(len(im_arr)):\n        img = np.reshape(im_arr[i], (28, 28) )\n        r, c, *_ = img.shape\n        M = np.float32([[1, 0, pos[i]],[0, 1, 0]])        # Moving max 7 pixels left or right \n        dst = cv2.warpAffine(img, M, (c,r) )\n        output.append(dst)\n    return np.array(output)\n\ndef gaussBlur_cv(im_arr):\n    \"\"\"\n    Add Gaussian Blur randomly to array of images using opencv.\n    ---------------------------------------------------\n    Parameters:\n        im_arr: array of numpy array of images.\n    Output:\n        Return array of numpy arrays of Blurred images.\n    \"\"\"\n    output = []\n    for i in range(len(im_arr)):\n        img = np.reshape(im_arr[i], (28, 28) )\n        y = int(np.random.randint(3)*0.5)\n        if y: dst = cv2.GaussianBlur(img, (0, 0), 2)\n        else: dst = img\n        output.append(dst)\n    return np.array(output)\n\ndef sobel_cv(im_arr):\n    \"\"\"\n    Filter with Sobel randomly to array of images using opencv.\n    ---------------------------------------------------\n    Parameters:\n        im_arr: array of numpy array of images.\n    Output:\n        Return array of numpy arrays of Filtered images.\n    \"\"\"\n    output = []\n    for i in range(len(im_arr)):\n        img = np.reshape(im_arr[i], (28, 28) )\n        y = int(np.random.randint(3)*0.5)\n        if y: dst = cv2.Sobel(img, cv2.CV_8U, 1, 0)\n        else: dst = img\n        output.append(dst)\n    return np.array(output)\n\ndef dilate_cv(im_arr):\n    \"\"\"\n    Morphological dilation randomly to array of images using opencv.\n    ---------------------------------------------------\n    Parameters:\n        im_arr: array of numpy array of images.\n    Output:\n        Return array of numpy arrays of Morphed images.\n    \"\"\"\n    output = []\n    for i in range(len(im_arr)):\n        img = np.reshape(im_arr[i], (28, 28) )\n        y = int(np.random.randint(3)*0.5)\n        kernel = np.ones((3,3), np.uint8)\n        if y: dst = cv2.dilate(img, kernel, iterations=1)\n        else: dst = img\n        output.append(dst)\n    return np.array(output)","02d20011":"%time xtrain_rotated = rotate_cv(xtrain_imgs[25:35]); plots(xtrain_rotated)","d76a1dc5":"%time xtrain_trans = translate_cv(xtrain_imgs[0:10]); plots(xtrain_trans)","e0ca9d71":"#%time xtrain_blur = gaussBlur_cv(xtrain_imgs[0:10]); plots(xtrain_blur) # Not using it","0287a5e5":"#%time xtrain_sobel = sobel_cv(xtrain_imgs[0:10]); plots(xtrain_sobel) # Not using it","154c4776":"%time xtrain_dilate = dilate_cv(xtrain_imgs[0:10]); plots(xtrain_dilate)","82638285":"from sklearn.manifold import TSNE\nfrom time import time\n\nX = xtrain[0:5000]\nY = ytrain[0:5000]\n\nimport plotly\nplotly.offline.init_notebook_mode(connected=True)\n\nimport plotly.offline as py\nfrom plotly import tools\nimport plotly.graph_objs as go\n\ndef plot_tSNE(pt, Y, title):\n    \"\"\"\n    To plot a 3D plot using plotly. (For MNIST data only)\n    ------------------------------------------------------------------------------\n    Parameters:\n        pt: points in 3 dimensions with corresponding labels\/names in \"Y\"\n        Y: labels\/names for each point in \"pt\"\n        title: For title of plot\n    Output:\n        plots a 3D interactive graph of points in \"pt\" having labels in \"Y\"\n    \"\"\"\n    data = []\n    for i in range(10):\n        index = (Y == i)\n        trace = go.Scatter3d(x=pt[index, 0], y=pt[index, 1], z=pt[index, 2], mode='markers',\n                          marker=dict(size=6, line=dict(color=plt.cm.Set1(i \/ 10.)), opacity=0.97),\n                          text = [f'{i}'], name=f'{i}', hoverinfo='name')\n        data.append(trace)\n    layout = go.Layout(margin=dict(l=0,r=0,b=0,t=0), title=title)\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename=title)","52822f81":"t0 = time()\n%time pt = TSNE(n_components=3, n_iter=5000, init='pca', random_state=0).fit_transform(X)\nplot_tSNE(pt, Y,\"t-SNE plot(time %.2fs)\" %(time() - t0))","07a31fac":"class DataLoader():\n    def __init__(self, xt, yt, batch_size=32):\n        \"\"\"\n        Data Loader for training set. Outputs Variable of batch_size images and \n        Variable of batch_size categories on each next() call on generator for\n        len(xt)\/batch_size calls.\n        -------------------------------------------------------------------------------\n        Parameters:\n            xt: numpy array of images in training set\n            yt: numpy array of categories in training set\n            batch_size: (default=32) number of images and categories to output per call\n        Output:\n            Return Variable of batch_size images and a Variable of batch_size categories per call\n            for len(xt)\/batch_size calls.\n        \"\"\"\n        self.xt, self.yt = xt, yt\n        # Apply select augmentation to about 1\/3rd of images\n        #self.xt = dilate_cv(self.xt)\n        self.bs = batch_size\n    def __iter__(self):\n        xt_trans = translate_cv(dilate_cv(self.xt))\n        xt_rot = rotate_cv(xt_trans)\n        xout, yout = [], []\n        for i in range(len(self.xt)):\n            xout.append(xt_rot[i])\n            yout.append(self.yt[i])\n            if len(xout) == self.bs:\n                yield ( Variable(torch.cuda.DoubleTensor(np.array(xout, dtype=\"float32\"))),\n                                    Variable(torch.cuda.LongTensor(np.array(yout, dtype=\"float32\"))) )\n                xout, yout = [], []\n        if len(xout) > 0: yield ( Variable(torch.cuda.DoubleTensor(np.array(xout, dtype=\"float32\"))),\n                                    Variable(torch.cuda.LongTensor(np.array(yout, dtype=\"float32\"))) )\n    def __len__(self):\n        if len(self.xt)%self.bs==0 : return len(self.xt)\/\/self.bs\n        else : return len(self.xt)\/\/self.bs+1","92c8b1d4":"%time dl = DataLoader(xtrain_imgs, ytrain)","00db00a2":"class valGenerator():\n    def __init__(self, xv, yv, batch_size=500):\n        \"\"\"\n        Randomly output Variable of batch_size images and Variable of batch_size categories per \n        next() call on generator.\n        ----------------------------------------------------------------------------------------\n        Parameters:\n            xv: Array containing numpy arrays of diff. images\n            yv: Array containing categories of images in xv\n            batch_size: (default=500) parameter to control number of images and categories to output\n        Output:\n            Return Variable of batch_size images and Variable of batch_size categories per \n            next() call on generator.\n        \"\"\"\n        self.data = np.zeros((len(xv), 28*28+1))\n        self.bs = batch_size\n        self.data[:, 0] = yv\n        self.data[:, 1:785] = xv\n    def __iter__(self):\n        while True:\n            idxs = np.random.permutation(len(self.data))[0:self.bs]\n            yield (Variable(torch.cuda.DoubleTensor(self.data[idxs, 1:785].reshape((-1, 1, 28, 28)))), \n                 Variable(torch.cuda.LongTensor(self.data[idxs, 0])) )","dde02b97":"vgen = valGenerator(xvalid, yvalid)","ddb4bdd9":"class ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n    \n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, padding=1),\n            nn.Dropout(0.20),\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(32*8*8, 500),\n            nn.ReLU(),\n            nn.BatchNorm1d(500),\n            nn.Dropout(p=0.5),\n            nn.Linear(500, 100),\n            nn.ReLU(),\n            nn.Linear(100, 10),\n            nn.ReLU(),\n            nn.Softmax(dim=1)\n        )\n    def forward(self, x):\n        x = self.conv(x)\n        #print(x.size())       # Checking the size of output given out, for updating the input size of first layer in fc.\n        x = x.view(-1, x.size(1)*x.size(2)*x.size(3))\n        return self.fc(x)","28f140a8":"#loss = nn.CrossEntropyLoss()\nmetrics = [accuracy]\n#opt = optim.Adam(net.parameters(), weight_decay=1e-4)\n#opt = optim.SGD(net.parameters(), 1e-3, momentum=0.999, weight_decay=1e-3, nesterov=True)","5dafed5a":"def set_lrs(opt, lr):\n    \"\"\"\n    Set learning rate in each layer equal to lr.\n    ----------------------------------------------------\n    Parameters:\n        opt: Optimizer for which lr is to be set\n        lr: new learning rate for each layer\n    \"\"\"\n    for pg in opt.param_groups: pg['lr'] = lr","db0f6caf":"class lrAnnealing():\n    def __init__(self, ini_lr, epochs, itr_per_epoch, mult_dec=False):\n        \"\"\"\n        Class to Anneal learning rate with warm restarts with time. It decreases \n        learning rate as multiple cosine waves with dec. amplitudes.1e-5 is taken \n        as zero. (The lower point for cosine)\n        ---------------------------------------------------------------------------\n        Parameters:\n            ini_lr: Initial learning rate\n            epochs: Number of epochs\n            itr_per_epoch: iterations per epoch\n            mult_dec: T\/F, If to use Annealing with warm restarts or hard\n        \"\"\"\n        self.epochs = epochs\n        self.ipe = itr_per_epoch\n        self.m_dec = mult_dec\n        self.ppw = (self.ipe * self.epochs) \/\/ 4    # Points per wave of cosine (For 4 waves per fit method)\n        self.count = 0\n        self.lr = ini_lr\n        self.values = np.cos(np.linspace(np.arccos(self.lr), np.arccos(1e-5), self.ppw))\n        self.mult = 1\n    def __call__(self, opt):\n        self.count += 1\n        set_lrs(opt, self.values[self.count-1]*self.mult)\n        if self.count == len(self.values):\n            self.count = 0\n            if self.m_dec: self.mult \/= 2","a785dff0":" y = np.concatenate([np.cos(np.arange(0,1,.01)),\n                    np.cos(np.arange(.3, 1, .01)),\n                    np.cos(np.arange(.5, 1, .01))])","cf9e2193":"x = np.array(np.arange(0, 220, 1))","52241d41":"plt.plot(x, y)","b92525e7":"def fit(model, lr,  train_dl, valGen, n_epochs, crit, opt, metrics, annln=True, mult_dec=False):\n    \"\"\"\n    Function to fit the model to training set and print F1 scores for both training set\n    and validation set.\n    -------------------------------------------------------------------------------------\n    Parameters:\n        model: Model (Neural Network) to which Training set will fit\n        lr: Learning rate (initil learning rate if annln=True)\n        train_dl: Train DataLoader which loads training data in batches (should give Tensors as output)\n        valGen: Validation DataLoader which loads validation data in batches (\")\n        n_epochs: number of epochs\n        loss: Loss function to calculate and backpropagate loss (eg: CrossEntropy)\n        opt: Optimizer, to update weights (eg: RMSprop)\n        metrics: Function to calculate score of model (eg: accuracy, F1 score)\n        annln: (default=True) If to use LRAnnealing or not\n        mult_dec: (default=True) If to dec. max Learning rate on every cosine cycle\n    \"\"\"\n    if annln: annl = lrAnnealing(lr, 40, 500, mult_dec=mult_dec)  # itr_per_epoch = len(xtrain) \/\/ batch_size\n    for epoch in range(n_epochs):\n        losses = []\n        vl = iter(valGen)\n        dl = iter(train_dl)\n        length = len(train_dl)\n        for t in range(length):\n            # Annealing\n            if annln: annl(opt)\n            \n            xt, yt = next(dl)\n            xt = xt.view(-1, 1, 28, 28)\n            xt, yt = xt, yt\n\n            # Forward pass: compute predicted y and loss by passing x to model.\n            y_pred = model(xt)\n            l = crit(y_pred, yt)\n            losses.append(l)\n\n            # Before backward pass, use the optimizer object to zero all of the gradients for the variable it will update\n            # (which are Learnable weights of the model)\n            opt.zero_grad()\n\n            # Backward pass: compute gradient of the loss with respect to the model params\n            l.backward()\n\n            # Calling the step function on an Optimizer makes an update to its parameters\n            opt.step()\n        \n        xv, yv = next(vl)\n        \n        yright =[]\n        valLosses = []\n        mult = len(xv)\/\/100\n        for i in range(0, len(xv), 100):\n            if i == mult*100:\n                a, b = score(model, xv[i:len(xv)], yv[i:len(xv)])\n                yright.append(a)\n                valLosses.append(crit(b, yv[i:len(xv)]).data.cpu().numpy())\n            else:\n                a, b = score(model, xv[i:i+100], yv[i:i+100])\n                yright.append(a)\n                valLosses.append(crit(b, yv[i:i+100]).data.cpu().numpy())\n        val_scores = np.sum(yright)\/len(xv)\n        \n        meanValLoss = 0\n        for vliter in range(len(valLosses)):\n            meanValLoss += valLosses[vliter]\n        meanValLoss\/=len(valLosses)\n        \n        mean_loss = 0\n        for liter in range(len(losses)):\n            mean_loss += losses[liter].data.cpu().numpy()\n        mean_loss\/=len(losses)\n        #print(val_scores, mean_loss, meanValLoss)\n        if epoch == 69:\n            print(\"Epoch \" + str(epoch) + \"::\"\n                + \"  loss: \" + str(mean_loss)\n                + \", valLoss: \" + str(meanValLoss)\n                +\", valAcc: \" + str(val_scores*100))","fb73ac75":"# Before Ensemble Highest Score was 0.994, and now _____.\nnets = [0]*10\n\nfor i in range(10):\n    nets[i] = ConvNet().cuda().double()\n    loss = nn.CrossEntropyLoss()\n    \n    #opts = [\n    #        optim.Adam(nets[i].parameters(), weight_decay=1e-4),\n    #        optim.SGD(nets[i].parameters(), 1e-3, momentum=0.999, weight_decay=1e-3, nesterov=True),\n    #        optim.RMSprop(nets[i].parameters(), weight_decay=1e-4, momentum=0.9)\n    #    ]\n    opt =  optim.SGD(nets[i].parameters(), 1e-3, momentum=0.999, weight_decay=1e-3, nesterov=True)  \n    # opt = opts[np.random.randint(3)]\n    \n    print(f'{i}th ConvNet:')\n    %time fit(nets[i], 1e-2, dl, vgen, 70, loss, opt, metrics, mult_dec=True)","19bfe0de":"ximgs, _ = next(iter(vgen))\nximgs = ximgs[0:8]\ny_pred = to_np(nets[0](ximgs))\ny_pred[0:2].argmax(1)\n\nplots(to_np(ximgs).reshape(-1, 28, 28), titles=score(nets[0], ximgs, ret=2))","7f6ed747":"# Lets print 4 kernels from every convolutional layer\nk = []\nfor m in nets[0].modules():\n    if isinstance(m, nn.Conv2d):\n        if m.weight.data[0].size(0) > 3:\n            k.append(m.weight.data[0][0:5].contiguous().cpu().numpy().reshape(-1, 3, 3))\n            plots(m.weight.data[0][0:4].contiguous().cpu().numpy().reshape(-1, 3, 3), rows=1)\n        else:\n            k.append(m.weight.data[0:5].contiguous().cpu().numpy().reshape(-1, 3, 3))\n            plots(m.weight.data[0:4].contiguous().cpu().numpy().reshape(-1, 3, 3), rows=1)","5fbc288b":"# Lets take one input images and multiply it with some of our kernels and see what we get:\nfinal = []\nimg = np.zeros((5, 28, 28))\nfor i in range(5): img[i] = to_np(ximgs[0]).reshape((28, 28))\nfor m in img: final.append(m)\n\nkernel = k[0]\nconv = np.zeros((5, 26, 26))\n\nfor m in range(5):\n    output_img = np.zeros((26, 26))\n    for i in range(26):\n        for j in range(26):\n            output_img[i, j] = max(sum(sum(img[m, i:i+3, j:j+3] * kernel[m])), 0)  # with ReLU \n    final.append(output_img)\n\nplots(final)","e3720e48":"# Now lets take them through one of the kernels in second convolutional layer:\nimg = final\nfinal = []\nfor i in range(5, 10): final.append(img[i])\nimg = np.zeros((5, 26, 26))\nfor i in range(5): img[i] = final[i]\n\nkernel = k[1]\nconv = np.zeros((5, 24, 24))\n\nfor m in range(5):\n    output_img = np.zeros((24, 24))\n    for i in range(24):\n        for j in range(24):\n            output_img[i, j] = max(sum(sum(img[m, i:i+3, j:j+3] * kernel[m])), 0)\n    final.append(output_img)\n\nplots(final)","327477e3":"# Now lets take them through one of the kernels in first maxpooling layer:\nimg = final\nfinal = []\nfor i in range(5, 10): final.append(img[i])\nimg = np.zeros((5, 24, 24))\nfor i in range(5): img[i] = final[i]\n\nconv = np.zeros((5, 23, 23))\n\nfor m in range(5):\n    output_img = np.zeros((23, 23))\n    for i in range(23):\n        for j in range(23):\n            output_img[i, j] = np.max(img[m, i:i+2, j:j+2])\n    final.append(output_img)\n\nplots(final)","5a5b9596":"# Now lets take them through one of the kernels in next convolutional layer\nimg = final\nfinal = []\nfor i in range(5, 10): final.append(img[i])\nimg = np.zeros((5, 23, 23))\nfor i in range(5): img[i] = final[i]\n\nkernel = k[2]\nconv = np.zeros((5, 21, 21))\n\nfor m in range(5):\n    output_img = np.zeros((21, 21))\n    for i in range(21):\n        for j in range(21):\n            output_img[i, j] = max(sum(sum(img[m, i:i+3, j:j+3] * kernel[m])), 0) # with ReLU\n    final.append(output_img)\n\nplots(final)","e0ad00c7":"# Now lets take them through one of the kernels in next convolutional layer\nimg = final\nfinal = []\nfor i in range(5, 10): final.append(img[i])\nimg = np.zeros((5, 21, 21))\nfor i in range(5): img[i] = final[i]\n\nkernel = k[3]\nconv = np.zeros((5, 19, 19))\n\nfor m in range(5):\n    output_img = np.zeros((19, 19))\n    for i in range(19):\n        for j in range(19):\n            output_img[i, j] = max(sum(sum(img[m, i:i+3, j:j+3] * kernel[m])), 0) # with ReLU\n    final.append(output_img)\n\nplots(final)","20c86881":"# Now lets take them through one of the kernels in first maxpooling layer\nimg = final\nfinal = []\nfor i in range(5, 10): final.append(img[i])\nimg = np.zeros((5, 19, 19))\nfor i in range(5): img[i] = final[i]\n\nconv = np.zeros((5, 18, 18))\n\nfor m in range(5):\n    output_img = np.zeros((18, 18))\n    for i in range(18):\n        for j in range(18):\n            output_img[i, j] = np.max(img[m, i:i+2, j:j+2])\n    final.append(output_img)\n\nplots(final)","9c7e0894":"test = (test-mean)\/std\ntest = np.resize(test, (28000, 1, 28, 28) )\ntest = Variable(torch.DoubleTensor(test)).cuda()\nyp = np.zeros((28000 ,2), dtype='int'); yp.shape\n\n\n# Because we only have about 12GB (?) GPU here, we will get output predictions sequentially: (Was getting cuda mem. error)\nfor i in range(0, 28000, 100):\n    prob_sum = np.zeros((100, 10), dtype='float64')\n    for j in range(10):\n        prob_sum += to_np(nets[j](test[i:i+100]))\n    yp[i:i+100] = np.array(np.vstack([np.arange(i+1, i+101), prob_sum.argmax(1)]).T)\n    \nres = pd.DataFrame(yp, index = np.arange(len(yp)), columns=[\"ImageId\", \"Label\"])\nres.to_csv(\"submission.csv\", index=False)\nres.head()","82143635":"# Making Convolutional Neural Network: <a id=\"makingCNN\"><\/a>\n---","57507fe4":"### Defining Fit method: <a id=\"defFitMethod\"><\/a>","342339bf":"Shuffling data and splitting into training and validation set:","0a70d5db":"When it comes to DataLoaders, one important thing to check is the batch size being used. A trade off between batch size and accuracy can be seen because as we increase batch size we will get more standardized error and we might not be able to learn as effectively. \n\nFor example we might not learn some features which is present in many but still that number is quite less than sample size. Through a larger batch size we might not be able to learn those features. \n\n\n![Imgur](https:\/\/i.imgur.com\/VL4PbHQ.png)\nSource: [https:\/\/www.researchgate.net\/figure\/Performance-comparison-CNN-for-different-batch-size-in-CI-FAR10-Testing-accuracy-for_fig24_312593963](https:\/\/www.researchgate.net\/figure\/Performance-comparison-CNN-for-different-batch-size-in-CI-FAR10-Testing-accuracy-for_fig24_312593963)\n\n\nAs we decrese batch size we learn more and more features and faster. And we get a better accuracy in the end.\n","0a5e42c4":"<a id=\"lrAnnealing\"><\/a>\nThere can be many types of annealing functions, here I have used learning rate annealing with restarts :\n\n**Learning Rate Annealing : ** Learning rate annealing basically reduces learning rate of model when it has stopped learning. Here I have implemented a simple one. It reduces learning rate as time passes.\n\nHow is this helpful? As the dimentionality increases, our Network will have exponentially more local minimas, and they are actually nearly at same level and some worse than the others. \n\nWhy? Because some can be steep and others a plateau, still at same level. So, if we change our weights even by small amounts in a steep minima, we might get really bad results. But if we are in a plateau at same level, a more general solution, we will get nearlly the same answer even if we get a little bit here or there. So, in LR Annealing with restarts, learning rate is increased from time to time so that we get out of steep minimas and hopefully converge to more general ans.\n\nSource: [I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with restarts.\narXiv preprint arXiv:1608.03983, 2016](https:\/\/arxiv.org\/pdf\/1608.03983)\n","88c4006a":"Though BatchNormalization incurs some time penalty, still it fares well than Non-normalized data:\n\n\n![Imgur](https:\/\/i.imgur.com\/aCXovq6.png)\nSource: [https:\/\/towardsdatascience.com\/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faster-60ba4d054b73](https:\/\/towardsdatascience.com\/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faster-60ba4d054b73)","e2fc7aed":"Configurations I used:\n1.  Adam : with default lr( = 1 ) and different weight decays **: :** \nSeems to coverge at suboptimal solution (about 98%), although learning was fast. Actually Adam automatically reduces learning rate for the features which are most occuring.\n1.  SGD : with lr(= 1) and momentum and Learning Rate Annealing **: :** \nReally bad choice; was not learning much. Actually SGD with learning rate annealing fares well in many cases, than all other optimizers, if a decent learning rate is chosen.\n1. SGD : with  lr(=1e-5) and momentum and Learning Rate Annealing **: :** \nReally slow on learning.\nLearning rate of 1e-3 and 1e-4 seemed to give good results.\n1. SGD : with lr(=1e-3) and momentum and Learnig Rate Annealing and nesterov = True **: :** \nWhat nesterov = True means is that it is using NAG algorithm, which doen't just move into the slope gaining speed (this is what momentum does), it looks forward and corrects its position too. \n\nSomething to read : [http:\/\/ruder.io\/optimizing-gradient-descent\/](http:\/\/ruder.io\/optimizing-gradient-descent)\n\nSGD with momentum, NAG and learning rate annealing seemed to give the best results for this CNN. (atleast from the optimizers and parameters I tried.)","115128ca":"# Lets see convolution in action: <a id=\"convInAction\"><\/a>","47a456ea":"Now lets take them through one of the kernels in next convolutional layer:","320258b3":"# Making a DataLoader: <a id=\"makingDataLoader\"><\/a>\n___","639b7d32":"Lets take one input images and multiply it with some of our kernels and see what we get:","1a53de78":"# Getting Predictions <a id=\"gettingPreds\"><\/a>\n---","ac4b6fa4":"# Imports <a id=\"imports\"><\/a>\n---","9fb074cd":"Using Ensemble of 15 CNNs: (method used [here](https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist))","eb361f47":"**MaxPool : ** This and some other similar layers actually makes our matrix smaller and smaller and less complex, and saving only the important features with its location.","ed89fc6c":"# Introduction\nIn this kernel we will go stepwise, through each process, understanding intuition behind each step.\n\n## Index:\n+ [Imports](#imports)\n+ [Some Useful Functions](#someUsefulFunctions)\n+ [Data Preprocessing](#dataPrep)\n    - [Data Normalisation](#dataNorm)\n    - [Data Augmentation](#dataAugmentation)\n+ [Visualizing Classes](#visClasses) : using t-SNE to visualize dataset in 3D.\n+ [Making Data Loader](#makingDataLoader)\n+ [Making CNN](#makingCNN)\n+ [Training Network](#trainNet)\n    - [LR Annealing](#lrAnnealing)\n    - [Defining Fit Method](#defFitMethod)\n    - [Fitting to model](#fitting)\n+ [Final Checking](#finalCheck)\n+ [Convolution in action](#convInAction) : Taking one image through a few kernels from each layer of Convolution Network and look at output.\n+ [Getting predictions and making submission](#gettingPreds)\n","d0d17614":"But there is one more trade off we have to look at. Actually two:\n1.  If we decrease batch size too much (say, to 1) we will have to calculate loss for every single data point and we will have to backpropagate too for every one of them. That will be too time consuming. So, there is a time vs batch size trade off.\n2. Secondly, if we decrease batch size too much (say, to 1) [SGD] we don't get a smooth learning curve. i.e. the network tries to learn every data point and moves in its direction and moves in next data points direction, then next and then next ... (And we might end at a sub optimal ans). So, mini-Batch Gradient Descent fares well most of the times and gives better result than both, as we learn those features in this case too and that too smoothly moving towards the global minimum. With mini batch gradient descent we can also get more speed because then we will be able to send data in matrices to GPU.\n\nSGD on every data point:\n![Imgur](https:\/\/i.imgur.com\/MiiMhFq.png)\nSource: [http:\/\/ruder.io\/optimizing-gradient-descent\/](http:\/\/ruder.io\/optimizing-gradient-descent\/)","954b3417":"Some of the functions are taken from Fast.ai library ([here](https:\/\/github.com\/fastai\/fastai)).","c9c98154":"Now lets take them through one of the kernels in first maxpooling layer:","ccfeb681":"Data Augmentation is a very important step in our training if we don't have enough data. It helps us create more data from existing data. It helps in **reducing over-fitting**.","08d3e22d":"### Fitting to model: <a id=\"fitting\"><\/a>","bcc5c800":"### Why  Batch Normalization:\n**Issues With Training Deep Neural Networks : **\n1. Internal Covariate shift:\n\n    Covariate shift refers to the change in the input distribution to a learning system. In the case of deep networks, the input to each layer is affected by parameters in all the input layers. So even small changes to the network get amplified down the network. This leads to change in the input distribution to internal layers of the deep network and is known as internal covariate shift.\nIt is well established that networks converge faster if the inputs have been whitened (ie zero mean, unit variances) and are uncorrelated and internal covariate shift leads to just the opposite.\n\n1. Vanishing Gradient:\n\n      Saturating nonlinearities (like tanh or sigmoid) can not be used for deep networks as they tend to get stuck in the saturation region as the network grows deeper. Some ways around this are to use:\n\n    * Nonlinearities like ReLU which do not saturate\n    * Smaller learning rates\n    * Careful initializations\n    \nSource:  [https:\/\/gist.github.com\/shagunsodhani\/4441216a298df0fe6ab0](https:\/\/gist.github.com\/shagunsodhani\/4441216a298df0fe6ab0), [Sergey Ioffe, Christian Szegedy arXiv:1502.03167](https:\/\/arxiv.org\/pdf\/1502.03167.pdf)\n\nSome graphs on performance of Networks with and without Batch Normalization: \n![Imgur](https:\/\/i.imgur.com\/D9QMn1x.png)","bd9b7595":"4 kernels from every convolutional layer:","a0b93cd3":"**Fully Connected Layers:**\nWe want to categorize our inputs and thats why we have to move to fully connected layers, to eventually have only 10 output nodes, which will tell us the probability of every category by the end of training. \nOne might think how is our network able to tell probability? Basically, it is because of the presence of softmax layer in the end which does two things:\n1.  As it uses exponential function inside, it makes bigger numberes more bigger. By which we get contrasted values in output. Good for categorization.\n1.  We divide by sum of all outputs, which gives a feel of probability. But, as the machine becomes more accurate, it goes closer and closer to actual probabilities.","ba4928ec":"![Imgur](https:\/\/i.imgur.com\/NdPuSu0.png)","efacd8af":"In a convolution neural network we try to learn some matricies, which are called kernels, convolutional matrix or mask. These kernels are features present in the images. It can be some border, some shape, or even some complex parts like nose, eyes etc.\n![Imgur](https:\/\/i.imgur.com\/KRiTBaX.png)\nSource: [Isma Hadji, Richard P. Wildes arXiv:1803.08834](https:\/\/arxiv.org\/pdf\/1803.08834.pdf)\n","fcb8f786":"For training a network three things are important to set,\n1.  **Loss Function** : which calculates the difference between current output and actual output. Here, CrossEntropyLoss is used, which is commonly used for Multi-class Classification problems.\n1.  **Metrics** : A fuction to calculate viability of network (How robust is our network). This function can be Accuracy, F1 score etc.\n1.  **Optimizer** : It is a function which updates our parameters, different optimizers use different techniques to update parameters in a way, so that our network learns faster and gives optimal result.","5c9abdb3":"### Reshape images:","3881b8aa":"Data normalization in case of CNN and images helps because it makes convergence faster. \n\nAlso we will use BatchNormalization inside our network. As training progresses and we update parameters to different extents, we lose this normalization, which slows down training and these changes amplifies as the network becomes deeper. We will see BatchNorm later. \n\n","932364de":"# Training Network: <a id=\"trainNet\"><\/a>\n---","a76cb03c":"# Visualizing Classes (t-SNE): <a id=\"visClasses\"><\/a>\n\nt-SNE is a very popular technique for dimentionality reduction. It is mostly used for visualization of data in 2D or 3D.  \n( [L.J.P. van der Maaten and G.E. Hinton; 9(Nov):2579--2605, 2008](http:\/\/www.jmlr.org\/papers\/volume9\/vandermaaten08a\/vandermaaten08a.pdf) )\n\nWe will use sklearn's implementation of t-SNE.\nFor that we will take 5000 images and labels and reduce their dimentionality from 28x28 to just 3 (!). We will see if there is some difference between the classes in higher dimension, i.e. are points of same category clustered together or not and are they away from other categories or not. That is what t-SNE does. It effectively tries to project a higher dimensinal data to lower dimensions.\n\nAnd to plot the points we will use plotly to plot all points in an interactive 3D space.","2c6034e0":"# Final Checking <a id=\"finalCheck\"><\/a>\n---","f64780ad":"Now lets take them through one of the kernels in first maxpooling layer:","5f375ade":"## Data Augmentation: <a id=\"dataAugmentation\"><\/a>","945e535e":"Here these are the steps essential for learning:\n1.  **Predict** (Put input values throgh model, and get output);\n1. ** Calculate Loss** (by loss function, gives out current loss);\n1.  **Calculate gradients of loss with respect to model parameters** (loss.backward);\n1.  **Update Parameters** (optimizer.step());","a66c784d":"We can clearly see the distinction between the classes even in 3 dimensions. As you can see most of the points belonging to same class are nearby.","d138e9be":"### Data Normalization: <a id=\"dataNorm\"><\/a>","6ea93a75":"These kernels are matched with image on every loaction to check if that feature is present there or not. Only when that feature is present, matrix multiplication with that kernel and matrix of some location in image gives a high number as ouput, which is saved in a location corresponding to location of matrix in input image in output of that convolutional layer (a matrix).  We wil see some kernels in action in the end.\n![Imgur](https:\/\/i.imgur.com\/McTaE78.jpg)\nSource: [http:\/\/machinelearninguru.com\/computer_vision\/basics\/convolution\/image_convolution_1.html](http:\/\/machinelearninguru.com\/computer_vision\/basics\/convolution\/image_convolution_1.html)","d2a2f069":"# Some Useful Functions <a id=\"someUsefulFunctions\"><\/a>\n---","e2f3781c":"### LRs trend with number of iterations will look something like this:","ef8bfa84":"Now lets take them through one of the kernels in next convolutional layer:","e8755c2e":"We have nearly same quantity of all categories. Thats good.\n\nBut overall data is still less. We will use **Data Augmentation** to get more images from the existing ones.","c5ad34da":"As we can see some matices have retained some information and in others information retained is decreasing. i.e. those particular kernels are not able to find find what they were looking for.\n\nHere we are using a few kernels only, from each layer. Every layer has many many kerels.\nAnd as we go deeper and deeper, this becomes more abstract, which thankfully our network will understand and give us most accurate predictions.","4936ca7e":"**ReLU : **\nReLU is a key part in contribution of immense power of a deep neural networks. Without it or without some other similar activation functions, our NN will just be a Linear Regression problem with lots of parameters. These activation functions are the parts Deep Neural Networks which makes them fly and also gives them non linearity, and a sufficiently deep neural network can fit any function, any function at all.\n\nSomething to read: [http:\/\/neuralnetworksanddeeplearning.com\/chap4.html](http:\/\/neuralnetworksanddeeplearning.com\/chap4.html)","fddc430a":"Now lets take them through one of the kernels in second convolutional layer:","11f9a9a7":"**Dropout : ** This is basically to prevent overfitting. When we have less data and we don't want to overfit, dropout can come in handy.\nIt is a technique where randomly selected neurons are dropped during training. This means that their contribution to the activation of downstream neurons is temporally set to zero on the forward propagation and any weight updates are not applied to the neuron on the back propagation.\n\nSource: [https:\/\/machinelearningmastery.com\/dropout-regularization-deep-learning-models-keras\/](https:\/\/machinelearningmastery.com\/dropout-regularization-deep-learning-models-keras\/)","1d711814":"# Data Preprocessing <a id=\"dataPrep\"><\/a>\n---"}}