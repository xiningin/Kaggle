{"cell_type":{"78b5c8bc":"code","1e758e9d":"code","a337a336":"code","32afbcf5":"code","d5ca6fb7":"code","1c5b761f":"code","bff15435":"code","46b3e7f1":"code","45301529":"code","5b3b18f9":"code","a3731357":"code","a694f9bd":"code","ee231f9c":"code","de2f8c52":"code","2f8a1b6c":"code","a9395eb9":"code","fabe320f":"code","b7f2a54b":"code","39020100":"code","ed13c091":"code","37a0a83f":"markdown","c13041dc":"markdown","6a20780b":"markdown","437ff2d4":"markdown","bb101411":"markdown","32018245":"markdown"},"source":{"78b5c8bc":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nimport glob\nfrom joblib import Parallel, delayed\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom bayes_opt import BayesianOptimization\n\nfrom tqdm import tqdm\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","1e758e9d":"train = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/test.csv\")\nsub = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv\")","a337a336":"book_train_filepath = \"\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\"\ntrade_train_filepath = \"\/kaggle\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\"\nbook_test_filepath = \"\/kaggle\/input\/optiver-realized-volatility-prediction\/book_test.parquet\"\ntrade_test_filepath = \"\/kaggle\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\"\n\n# get filenames in book and trade files\nbook_train_filenames = os.listdir(book_train_filepath)\ntrade_train_filenames = os.listdir(trade_train_filepath)\nbook_test_filenames = os.listdir(book_test_filepath)\ntrade_test_filenames = os.listdir(trade_test_filepath)","32afbcf5":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef RMSPE(true, pred):\n    rmspe = np.sqrt(np.mean(np.square((true-pred)\/true)))\n    return rmspe\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(RMSPE(true = labels, pred = preds),5), False","d5ca6fb7":"def pre_data(stock_id, train = True):\n    \n    if train == True:\n        book_filepath = book_train_filepath\n        trade_filepath = trade_train_filepath\n    else:\n        book_filepath = book_test_filepath\n        trade_filepath = trade_test_filepath\n    \n    path = os.path.join(book_filepath, f\"stock_id={stock_id}\".format(stock_id))\n    data = pd.read_parquet(path)\n    \n    data['stock_id'] = stock_id\n    \n    data['wap1'] = (data['bid_price1'] * data['ask_size1'] + data['ask_price1'] * data['bid_size1']) \/ (data['bid_size1']+ data['ask_size1'])\n    data['wap2'] = (data['bid_price2'] * data['ask_size2'] + data['ask_price2'] * data['bid_size2']) \/ (data['bid_size2']+ data['ask_size2'])\n    \n    data['wap_balance'] = data['wap1'] - data['wap2']\n    \n    data['log_return1'] = data.groupby(['time_id'])['wap1'].apply(log_return)\n    data['log_return2'] = data.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    data['spread1'] = (data['ask_price1'] - data['bid_price1'])\/data['bid_price1']\n    data['spread2'] = data['ask_price2'] - data['bid_price2']\/data['bid_price2']\n    \n    data['net_size1'] = data['bid_size1'] - data['ask_size1']\n    data['net_size2'] = data['bid_size2'] - data['ask_size2']\n    \n    \n    data = data.groupby(['stock_id', 'time_id']).agg(wap_balance_mean = ('wap_balance', 'mean'),\n                                                     volatility1 = ('log_return1', realized_volatility),\n                                                     volatility2 = ('log_return2', realized_volatility),\n                                                     spread1_mean = ('spread1', 'mean'),\n                                                     spread2_mean = ('spread2', 'mean'),\n                                                     net_size1_mean = ('net_size1', 'mean'),\n                                                     net_size2_mean = ('net_size2', 'mean'))\n    \n    \n    \n    # trade \n\n    trade_path = os.path.join(trade_filepath, f\"stock_id={stock_id}\".format(stock_id))\n    trade_data = pd.read_parquet(trade_path)\n    trade_data['stock_id'] = stock_id\n    \n    trade_data = trade_data.groupby(['stock_id', 'time_id']).agg(trade_price_mean = ('price', 'mean'),\n                                                                 trade_size_mean = ('size', 'mean'),\n                                                                 trade_order_mean = ('order_count', 'mean')).reset_index()\n    \n    \n\n    final = data.merge(trade_data, how = 'left', on = ['stock_id', 'time_id'])\n    #final = final.merge(volatility, how = 'left', on = ['stock_id', 'time_id'])\n    \n    #final['row_id'] = final['time_id'].apply(lambda x: f'{stock_id}-{x}')\n        \n    return final\n\n\ndef get_dataset(id_list, train = True):\n\n    stock = Parallel(n_jobs=-1)(\n        delayed(pre_data)(stock_id, train = True) \n        for stock_id in id_list\n    )\n    \n    stock_df = pd.concat(stock, ignore_index = True)\n\n    return stock_df","1c5b761f":"stock_train_df = get_dataset(train['stock_id'].unique(), train = True)\ntrain_df = train.merge(stock_train_df, how = 'left', on = ['stock_id', 'time_id'])\n\nstock_test_df = pre_data(0, train = False)\ntest_df = test.merge(stock_test_df, how = 'left', on = ['stock_id', 'time_id']).fillna(-999)","bff15435":"plt.figure(figsize = (8, 8))\ncorr = train_df.corr()\nmask = np.zeros_like(corr, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, cmap = 'coolwarm', mask = mask, linewidth = 0.5, vmin = -1, vmax = 1,\n           cbar_kws = {'shrink': .5})\nplt.title('Correlation Heatmap of train features', fontsize = 20, fontweight = 'bold')\nplt.show()","46b3e7f1":"kfold = KFold(n_splits = 5, shuffle = True, random_state = 0)\n\nfeatures = train_df.drop(['target'], axis = 1)\ntarget = train_df['target']\n\n# define LGBM model\nlgbm = LGBMRegressor(random_state = 0)\nX_train, X_val, y_train, y_val = train_test_split(features, target, test_size = 0.2, random_state = 0)","45301529":"X_train, X_val, y_train, y_val = train_test_split(features, target, test_size = 0.2, random_state = 0)\n\n\ndef lgbm_cv(learning_rate, n_estimators, max_depth, num_leaves, subsample, min_data_in_leaf, silent = True):\n    params = {'learning_rate': learning_rate,\n          'n_estimators': int(n_estimators),\n          'max_depth': int(max_depth),\n          'num_leaves': int(num_leaves),\n          'subsample': subsample,\n          'min_data_in_leaf': int(min_data_in_leaf),\n          'verbose': -1,\n          'force_col_wise': True\n             }\n    \n    lgbm_train = lgb.Dataset(X_train, label = y_train, categorical_feature = ['stock_id'], weight = 1\/np.square(y_train))\n    lgbm_val = lgb.Dataset(X_val, label = y_val, categorical_feature = ['stock_id'], weight = 1\/np.square(y_val))\n    \n    model = lgb.train(params = params,\n                      train_set = lgbm_train,\n                      valid_sets = [lgbm_train, lgbm_val],\n                      feval = feval_RMSPE,\n                      verbose_eval = 1,\n                      early_stopping_rounds = 100,\n                      num_boost_round = 1000)\n    \n    pred = model.predict(X_val)\n    rmspe_score = RMSPE(y_val, pred)\n    \n    return rmspe_score","5b3b18f9":"pbounds = {'learning_rate': (0.005, 0.1),\n          'n_estimators': (1000, 2000),\n          'max_depth': (10, 20),\n          'num_leaves': (7, 14),\n          'subsample': (0.5, 0.9),\n          'min_data_in_leaf': (5, 20)\n          }\n\nlgbm_bo = BayesianOptimization(f = lgbm_cv, pbounds = pbounds, verbose = 2, random_state = 0)\n\n# init_points :  initial number of Random Search points \n# n_iter : iteration number\n# acq : Acquisition Function - we use EI\n# xi : exploration (default: 0)\nlgbm_bo.maximize(init_points = 2, n_iter = 10, acq = 'ei', xi = 0.01)","a3731357":"# optimal parameter\nprint(lgbm_bo.max)\n\nopt_params = lgbm_bo.max['params']","a694f9bd":"# GridSearchCV\n\n# scoring function\n#rmspe = make_scorer(RMSPE, greater_is_better = False)\n\n#X_train, X_val, y_train, y_val = train_test_split(new_features, target, test_size = 0.2, random_state = 0)\n\n#grid_model = GridSearchCV(lgbm, param_grid = params, cv = 5, scoring = rmspe)\n#grid_model.fit(X_train, y_train)\n\n#print('Best parameter: ', grid_model.best_params_)\n#print('Best score: ', grid_model.best_score_)","ee231f9c":"params = {'learning_rate': opt_params['learning_rate'],\n          'objective': 'regression',\n          'n_estimators': int(np.round(opt_params['n_estimators'], 0)), \n          'max_depth': int(np.round(opt_params['max_depth'], 0)),\n          'num_leaves': int(np.round(opt_params['num_leaves'], 0)),\n          'min_data_in_leaf': int(np.round(opt_params['min_data_in_leaf'], 0)),\n          'subsample': opt_params['subsample'],\n          'force_col_wise': True,\n          'verbose': -1\n          }\n\n    \n# KFold\nrmspe_list = []\nmodel_list = []\nfor i, (train_idx, val_idx) in enumerate(kfold.split(features)):\n    print(f'################# {i+1}th Fold #################')\n    X_train, X_val = features.iloc[train_idx, :], features.iloc[val_idx, :]\n    y_train, y_val = target[train_idx], target[val_idx]\n        \n    train_set = lgb.Dataset(X_train, label = y_train, categorical_feature = ['stock_id'], weight = 1\/np.square(y_train))\n    val_set = lgb.Dataset(X_val, label = y_val, categorical_feature = ['stock_id'], weight = 1\/np.square(y_val))\n\n    model = lgb.train(params = params,\n                      train_set = train_set,\n                      valid_sets = [train_set, val_set],\n                      feval = feval_RMSPE,\n                      early_stopping_rounds = 100,\n                      num_boost_round = 1000,\n                      verbose_eval = 100)\n    \n    model_list.append(model)\n    \n    pred = model.predict(X_val)\n    rmspe_score = RMSPE(y_val, pred)\n    \n    print(f'RMSPE: {np.round(rmspe_score, 4)}')\n    rmspe_list.append(rmspe_score)","de2f8c52":"feat_imp = pd.DataFrame()\nfeat_imp['feature'] = features.columns.tolist()\nfeat_imp['importance'] = model.feature_importance(importance_type = 'gain')\n\nfeat_imp = feat_imp.sort_values(by = ['importance'], ascending = False).reset_index(drop = True)\nfeat_imp","2f8a1b6c":"plt.figure(figsize = (8, 6))\nax = sns.barplot(data = feat_imp, x = 'importance', y = 'feature', color = '#006699', edgecolor = 'black')\n\nfor i in ['right', 'top']:\n        ax.spines[i].set_visible(False)\n        \nplt.title('Feature Importance', fontsize = 20, fontweight = 'bold')\nplt.xlabel('Importance', fontsize = 10, fontweight = 'bold')\nplt.ylabel('Feature', fontsize = 10, fontweight = 'bold')\nplt.show()","a9395eb9":"ax = plt.subplot(1, 1, 1)\n#ax.plot(range(1, 6), rmspe_list, color ='#006699', marker = 'o')\nax.fill_between(range(1, 6), 0, rmspe_list, alpha = 0.4, color = '#d9e6f2')\nax.scatter(range(1, 6), rmspe_list, color = '#006699')\nplt.axhline(y = np.mean(rmspe_list), color = '#cc0000', linestyle = ':', linewidth = 2)\nplt.text(1 ,np.mean(rmspe_list)+0.0015, 'mean', \n         bbox = dict(facecolor ='#cc0000', edgecolor='#cc0000', boxstyle='round', alpha = 0.2))\n\n\nax.set_ylim([0.23, 0.26])\nfor i in ['left', 'right', 'top']:\n        ax.spines[i].set_visible(False)\n\nax.set_xlabel('Fold', fontweight = 'bold')\nax.set_ylabel('RMSPE', fontweight = 'bold')\nax.set_xticks(range(1,6))\nplt.title('RMSPE in each fold', fontsize = 15, fontweight = 'bold', pad = 20)\nplt.show()","fabe320f":"test_df_new = test_df.drop(['row_id'], axis = 1)","b7f2a54b":"test_pred = np.zeros(len(test_df_new))\nfor model in model_list:\n    pred = model.predict(test_df_new)\n    test_pred += pred\/len(model_list)","39020100":"test_df['target'] = test_pred\nsub = test_df[['row_id', 'target']]\nsub","ed13c091":"sub.to_csv('submission.csv', index = False)","37a0a83f":"# Optimal Realized Volatility Prediction\n\nThis notebook contains some techniques of modeling process. If you are interested in EDA of this data, please check [here](https:\/\/www.kaggle.com\/hyewon328\/understand-and-visualize-volatility-data)!\n\n## Process\n1. [Preprocessing](#pre)\n2. [Feature Selection](#fs)\n3. [Bayesian Optimization](#opt)\n4. [Modeling](#model)\n5. [Prediction & Submission](#pred)","c13041dc":"## Hyperparameter Tuning: Bayesian Optimization <a class=\"anchor\" id=\"opt\"><\/a>\nTo select optimal parameter, we need to conduct hyperparameter tuning.\n\n**GridSearchCV** take too long since this tests all possible combinations of parameters. **RandomSearchCV** take less time but it chooses set of parameters randomly(does not test all combinations of parameters), selected parameter may not be an optimal parameter. Both algorithms do not contains prior knowledge information\n\n**Bayesian Optimization** keep track of past evaluation results which they use to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function. Also it's fatster than GridSearchCV, and more precise than RandomSearchCV.\n(Ref: <https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f>)\n","6a20780b":"## LGBM Modeling <a class=\"anchor\" id=\"model\"><\/a>","437ff2d4":"## Prediction & Submission <a class=\"anchor\" id=\"pred\"><\/a>","bb101411":"## Data Preprocessing <a class=\"anchor\" id=\"pre\"><\/a>\n\nIn **preprocessing part**, we generate some additional variables for LGBM Modeling.\n\nReference:\n<https:\/\/www.kaggle.com\/manels\/lgb-starter>","32018245":"# Load package & data"}}