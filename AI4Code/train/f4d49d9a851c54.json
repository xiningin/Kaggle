{"cell_type":{"5b96d98f":"code","330b2333":"code","13f774a7":"code","b7eebc86":"code","3eab6caf":"code","fc9dd618":"code","a561db58":"code","84bb3268":"code","7ad32858":"code","c849f7a0":"code","37338990":"code","66e38ffe":"code","92733fab":"code","3b71433d":"code","f84680f0":"code","97d13add":"code","6a79d191":"code","380b6d44":"code","c163e69d":"code","5c126ca2":"code","ec9779ef":"code","1f615484":"markdown","bcf133f5":"markdown","9718e4b9":"markdown","65fe5fbe":"markdown","608ca86a":"markdown","fbdf0aa0":"markdown","89f82fcc":"markdown","615f7139":"markdown","e1cc0658":"markdown","6b9a32eb":"markdown","6ca6c41e":"markdown","f05170c5":"markdown","d97b8361":"markdown","dce0a18b":"markdown","1a0f28eb":"markdown","d3ec9a95":"markdown","c9f46875":"markdown","7d0e9a9c":"markdown","24fd70ac":"markdown","85f0ea2a":"markdown","cc6c9e90":"markdown","a2d30ff6":"markdown","1ef255d1":"markdown","1c1158a1":"markdown","87d00451":"markdown","f4498034":"markdown","44e37566":"markdown","26443a70":"markdown","eb09bdf5":"markdown","f49ec40a":"markdown","1ca1810e":"markdown","3b9befc2":"markdown","66c2e890":"markdown","339a99ed":"markdown","090d6bd3":"markdown","1182c430":"markdown","05a20f68":"markdown","82080716":"markdown"},"source":{"5b96d98f":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","330b2333":"dataset = pd.read_csv('..\/input\/best-regression-model\/Data.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","13f774a7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","b7eebc86":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","3eab6caf":"y_pred = regressor.predict(X_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","fc9dd618":"from sklearn.metrics import r2_score\nprint('The score of Multiple linear regression is:')\nr2_score(y_test, y_pred)","a561db58":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\npoly_reg = PolynomialFeatures(degree = 4)\nX_poly = poly_reg.fit_transform(X_train)\nregressor = LinearRegression()\nregressor.fit(X_poly, y_train)","84bb3268":"y_pred = regressor.predict(poly_reg.transform(X_test))\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","7ad32858":"from sklearn.metrics import r2_score\nprint('The score of Polynomial Regression is:')\nr2_score(y_test, y_pred)","c849f7a0":"from sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(X_train, y_train)","37338990":"y_pred = regressor.predict(X_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","66e38ffe":"from sklearn.metrics import r2_score\nprint('The score of Decision Tree Regression is:')\nr2_score(y_test, y_pred)","92733fab":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(X_train, y_train)","3b71433d":"y_pred = regressor.predict(X_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","f84680f0":"from sklearn.metrics import r2_score\nprint('The score of Random Forest regression is:')\nr2_score(y_test, y_pred)","97d13add":"y = y.reshape(len(y),1)","6a79d191":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","380b6d44":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\ny_train = sc_y.fit_transform(y_train)","c163e69d":"from sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X_train, y_train)","5c126ca2":"y_pred = sc_y.inverse_transform(regressor.predict(sc_X.transform(X_test)))\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","ec9779ef":"from sklearn.metrics import r2_score\nprint('The score of Support Vector Regression is:')\nr2_score(y_test, y_pred)","1f615484":"# Visit the link for the  [Regression Metrics](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#regression-metrics)  which we use to calculate the accuracy of our model and select the best model based on our accuracy.\n\n* In here we are going to use the R-square metrics for evaluating the accuracy and determining the best model.\n\n* There is no direct method to detect the best model applicable for our data we need to test each model on our data and take the model which has more accuracy for the starters, in the upcoming notebooks we will see different techniques which will help to finalize a particular model for our data.\n\n* For now we go by trial and error method by using all models.\n\n\n","bcf133f5":"## Splitting the dataset into the Training set and Test set","9718e4b9":"## Training the Decision Tree Regression model on the Training set","65fe5fbe":"# Ps: This notebook is not very friendly for the people who started in machine learning without any knowledge, if u want to understand everything then go through the previous notebooks from starting which have been explaiined in dumb and detail then u can understand this better.","608ca86a":"# Multiple Linear Regression","fbdf0aa0":"## Predicting the Test set results","89f82fcc":"## Evaluating the Model Performance","615f7139":"![Screenshot%20%282%29.png](attachment:Screenshot%20%282%29.png)","e1cc0658":"## Training the **Polynomial Regression model** on the Training set","6b9a32eb":"## Splitting the dataset into the Training set and Test set","6ca6c41e":"# Like this notebook then upvote it.\n\n\n# Need to improve it then comment below.\n\n\n# Enjoy Machine Learning","f05170c5":"## Training the Multiple Linear Regression model on the Training set","d97b8361":"## Evaluating the Model Performance","dce0a18b":"![Screenshot%20%283%29.png](attachment:Screenshot%20%283%29.png)","1a0f28eb":"## Predicting the Test set results","d3ec9a95":"![Screenshot%20%281%29.png](attachment:Screenshot%20%281%29.png)","c9f46875":"## Evaluating the Model Performance","7d0e9a9c":"## Predicting the Test set results","24fd70ac":"## Evaluating the Model Performance","85f0ea2a":"## Evaluating the Model Performance","cc6c9e90":"## Importing the libraries","a2d30ff6":"**In the above pic the dark line on which Yi^ is plotted is the predicted result and the red points Yi are the real values.\nWe need to get the difference square of these two points as minimum as possible so that the predicted result will be closer to the real**","1ef255d1":"For better understanding of current notebook for beginners go through the links:\n\n [1.1 Data Preprocessing](http:\/\/www.kaggle.com\/saikrishna20\/data-preprocessing-tools)\n\n\n[1.2 Simple linear Regression](https:\/\/www.kaggle.com\/saikrishna20\/1-2-simple-linear-regression) \n\n\n[1.3 Multiple linear Regression with Backward Elimination](http:\/\/www.kaggle.com\/saikrishna20\/1-3-multiple-linear-regression-backward-eliminat)\n\n[1.4 Polynomial Linear Regression](https:\/\/www.kaggle.com\/saikrishna20\/1-4-polynomial-linear-regression)\n\n[1.5 Support Vector Regression (SVR)](https:\/\/www.kaggle.com\/saikrishna20\/1-5-support-vector-regression-svr\/edit\/run\/37240657)\n\n[1.6 Decision Tree Regressor](https:\/\/www.kaggle.com\/saikrishna20\/1-6-decision-tree-regression)\n\n[1.7 Random Forest Regression](https:\/\/www.kaggle.com\/saikrishna20\/1-7-random-forest-regression)\n\nIt basically tells u about the preprocessing & different models of Regression which will help u in understanding this notebook better","1c1158a1":"We will be using adjusted R-square if there are more features i.e more columns in X because by using more features the SStot value will increase and make the value of R-Square to not decrease, to compensate this we use adjusted R- Square","87d00451":"## Predicting the Test set results","f4498034":"# Support Vector Regression (SVR)","44e37566":"## Feature Scaling","26443a70":"In the formula of adjusted R- Square the p is the number of regressors or features so if it increases due to negative sign it decreases the value of denomenator and inturn increase the value which is to be subtracted from 1. so we get a better results.","eb09bdf5":"## Predicting the Test set results","f49ec40a":"# Credits: Machine Learning A-Z, Udemy.","1ca1810e":"![Regression.JPG](attachment:Regression.JPG)","3b9befc2":"## Importing the dataset","66c2e890":"The score of Multiple linear regression is: \n0.9325315554761302\n\nThe score of Polynomial Regression is:\n0.9458192606428147\n\nThe score of Decision Tree Regression is:\n0.9226091050550043\n\nThe score of Random Forest regression is:\n0.9615980699813017\n\nThe score of Support Vector Regression is:\n0.9480784049986258\n\n* All the numbers 0.94.. mean that the predicted values are very nearer to the original values and the predicted are less deviating from the original values.\n\n* If the score is 1.0 which means that predicted outcomes are a perfect match to the real values.\n\n* If the score is less, the model is not predicting a good outcome which we can rely on.\n\n* R-square score generally tells us how near is the predicted values to the real values.\n\n\n* The maximum score is obtained from Random Forest model, so we will choose this model and even it's an ensemble model we prefer this model.\n\n* There are much things to learn about model selection in the upcoming notebooks but u can get the basic idea of which model to select & how to select.\n","339a99ed":"![Screenshot%20%284%29.png](attachment:Screenshot%20%284%29.png)","090d6bd3":"## Training the SVR model on the Training set","1182c430":"SSres which is sum of squares of residuals(i.e. the difference btwn Yi & Yi^)\nSStot which is the sum of squares of total(i.e. the diff between Yi and Y average)\n\nWe can see the expression of R- square in the pic and the SStot will be constant the only thing we can decrease in a particular dataset is the SSres, to reduce it the predicted should be close to the actual\/ real values.\n\nR- square value in general lies between 0 & 1, there can Negative values also, which means the model is failed\/ it's the worst.\nWe try to bring the value of R- Square as nearest to 1 for better accuracy.","05a20f68":"## Training the Random Forest Regression model on the whole dataset","82080716":"# 1.8 Regression Model Selection"}}