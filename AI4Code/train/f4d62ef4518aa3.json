{"cell_type":{"df91ed79":"code","33e0925f":"code","8c1f674a":"code","fdc0b207":"code","a78cac20":"code","68f8a581":"code","ae7b0e1f":"code","dca8e054":"code","de06b702":"code","b78adaa3":"code","0df008ab":"code","b884acf2":"code","a135fddb":"code","43206330":"code","088b6973":"code","485db443":"code","52bcff2f":"code","1ed46d8f":"code","edb6da71":"code","451fac68":"code","4c3f6f19":"code","03fb5671":"code","b1e5b384":"code","9d72e9cc":"code","57f89345":"code","b282b07f":"code","cc02df75":"code","6a80fb85":"code","dde35a76":"code","b853bb38":"code","e3cd517b":"code","1803834b":"markdown","9b6d737f":"markdown","f7117be2":"markdown","ffbdc01e":"markdown","9c24b7ae":"markdown","ba395837":"markdown","e4fb94df":"markdown","477ed8ab":"markdown","e6648c43":"markdown","fc438b70":"markdown","f3d1ccc8":"markdown","2ea1f425":"markdown","d86833eb":"markdown","bb681192":"markdown","c3dc66ea":"markdown","c2aa836f":"markdown","66f75553":"markdown","6791cfc4":"markdown","0cacb1c0":"markdown","90bf3a75":"markdown","dc9a9c0d":"markdown","0e5069ed":"markdown","e4dc2d5c":"markdown"},"source":{"df91ed79":"import gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm","33e0925f":"import matplotlib.pyplot as plt\nimport seaborn as sns","8c1f674a":"from numpy.random import MT19937\nfrom numpy.random import RandomState, SeedSequence\nrs = RandomState(MT19937(SeedSequence(7879)))","fdc0b207":"base_path = \"\/kaggle\/input\/optiver-realized-volatility-prediction\/\"\n\ndf_train = pd.read_csv(base_path+\"train.csv\")\ndf_test = pd.read_csv(base_path+\"test.csv\")\ndf_submission = pd.read_csv(base_path+\"sample_submission.csv\")","a78cac20":"%%time\nmin_id_select = 20\nmax_id_select = 45\n\ndf_book = pd.read_parquet(base_path+'book_train.parquet', engine='pyarrow', filters=[[('stock_id', '>=', min_id_select), ('stock_id', '<', max_id_select)]])\n#df_book = pd.read_parquet(base_path+'book_train.parquet')\ndf_trade = pd.read_parquet(base_path+'trade_train.parquet')\ndf_book_test = pd.read_parquet(base_path+'book_test.parquet')\ndf_trade_test = pd.read_parquet(base_path+'trade_test.parquet')","68f8a581":"size_trade_train = df_trade.memory_usage().sum() \/ 1024**2\nsize_book_train = df_book.memory_usage().sum() \/ 1024**2\nprint(\"Memory usage for book_train.parquet: %.2f MB\" % size_book_train)\nprint(\"Memory usage for trade_train.parquet: %.2f MB\" % size_trade_train)","ae7b0e1f":"df_train.info()","dca8e054":"df_book.info()","de06b702":"df_trade.info()","b78adaa3":"df_test.info()","0df008ab":"df_trade.stock_id = df_trade.stock_id.astype(\"int8\")\ndf_book.stock_id = df_book.stock_id.astype(\"int8\")","b884acf2":"print(df_trade.stock_id.unique().shape)\nprint(df_book.stock_id.unique().shape)\nprint(df_train.stock_id.unique().shape)\nprint(df_test.stock_id.unique().shape)","a135fddb":"df_train.head(10)","43206330":"df_test.head(10)","088b6973":"df_submission.info()","485db443":"df_submission.head(20)","52bcff2f":"df_trade.head(10)","1ed46d8f":"df_trade.describe()","edb6da71":"grouped_order_count = df_trade.groupby(by=\"stock_id\").agg({\"order_count\": 'sum'}).reset_index()\nfig, ax = plt.subplots(figsize=(30, 10))\nsns.barplot(x=\"stock_id\", y=\"order_count\", data=grouped_order_count, ax=ax)\nsorted_order_count = grouped_order_count.sort_values(by=\"order_count\")\nprint(sorted_order_count.head(10))\nprint(sorted_order_count.tail(10))\nplt.show()\nplt.clf()","451fac68":"grouped_order_count = df_trade.groupby(by=\"stock_id\").agg({\"size\": 'sum'}).reset_index()\nfig, ax = plt.subplots(figsize=(30, 10))\nsns.barplot(x=\"stock_id\", y=\"size\", data=grouped_order_count, ax=ax)\nsorted_order_count = grouped_order_count.sort_values(by=\"size\")\nprint(sorted_order_count.head(10))\nprint(sorted_order_count.tail(10))\nplt.show()\nplt.clf()","4c3f6f19":"df_book.head(10)","03fb5671":"df_book_test.head(10)","b1e5b384":"df_book.describe()","9d72e9cc":"# %%time\n# fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n# sns.histplot(df_book.ask_price1, kde=True, bins=100, ax=axs[0][0])\n# sns.histplot(df_book.ask_price2, kde=True, bins=100, ax=axs[0][1])\n# sns.histplot(df_book.bid_price2, kde=True, bins=100, ax=axs[1][0])\n# sns.histplot(df_book.bid_price2, kde=True, bins=100, ax=axs[1][1])\n# plt.show()\n# plt.clf()","57f89345":"max_time = df_book.time_id.max()\nprint(max_time)\nbook_sample = df_book[df_book.time_id == max_time]\nprint(book_sample.shape)\nbook_sample.head()","b282b07f":"grouped_stocks = book_sample.groupby(by=\"stock_id\").agg([\"mean\", \"median\"]).reset_index()\ngrouped_stocks","cc02df75":"record_number = max_id_select - min_id_select\nx = 4\ny = int(np.ceil(record_number \/ x))\nfig, axs = plt.subplots(y, x, sharex=True, sharey=True, figsize=(40, 40))\n\nfor index in range(record_number):\n    sns.histplot(book_sample.loc[book_sample.stock_id == (index+min_id_select)].seconds_in_bucket, kde=True, bins=60, ax=axs[int(np.floor(index\/x))][index%x])\n\nplt.tight_layout()\nplt.show()\nplt.clf()","6a80fb85":"rand_time = rs.choice(df_book.time_id.unique())\nprint(rand_time)\nbook_sample = df_book[df_book.time_id == rand_time]\n\nrecord_number = max_id_select - min_id_select\nx = 4\ny = int(np.ceil(record_number \/ x))\nfig, axs = plt.subplots(y, x, sharex=True, sharey=True, figsize=(40, 40))\n\nfor index in range(record_number):\n    sns.histplot(book_sample.loc[book_sample.stock_id == (index+min_id_select)].seconds_in_bucket, kde=True, bins=60, ax=axs[int(np.floor(index\/x))][index%x])\n\nplt.tight_layout()\nplt.show()\nplt.clf()","dde35a76":"rand_stock = rs.choice(df_book.stock_id.unique())\nprint(rand_stock)\nbook_sample = df_book[df_book.stock_id == rand_stock][-50:]\n\nfig, ax = plt.subplots(figsize=(20,20))\n\nplt.plot(book_sample.ask_price1, color=\"firebrick\")\nplt.plot(book_sample.ask_price2, color=\"goldenrod\")\nplt.plot(book_sample.bid_price1, color=\"forestgreen\")\nplt.plot(book_sample.bid_price2, color=\"deepskyblue\")\n\nplt.show()\nplt.clf()","b853bb38":"df_book.time_id.value_counts().sort_index()","e3cd517b":"np.sort(df_trade.stock_id.unique())","1803834b":"**Not much to say except that `book_test.parquet` is composed of only 3 rows that match the first row from `test.csv`.**","9b6d737f":"**Looking at `bid_price[1|2]` and `ask_price[1|2]`, one can deduce that prices has been normalized.**","f7117be2":"**There are only three rows there for the sake of automation and to have an idea of what the hidden data will look like. This file tells us which `row_id` to predict.**","ffbdc01e":"**Prediction for the rows from `test.csv`.**","9c24b7ae":"* **Here, it is possible to note that the size of each order is not available. They are all blended under `size`.**\n* **The rest of the fields are relatively explanatory by themselves.**","ba395837":"## trade_[train|test].parquet","e4fb94df":"**Some of the indexes are non sequential (i.e. `time_id`, `stock_id`). For now, I do not know if it may be a problem but it may cause trouble in order to apply time series machine learning algorithm (e.g. LSTM).** \n\n* **Should one consider that the price did not move between gaps ?**\n* **Should one assume there was no trade during gaps period ?**","477ed8ab":"* **Max `seconds_in_bucket` is 5.99e+02 which match the 10 min timeframe for the forecast.**\n* **Mean and max of `bid_price1` > `bid_price2` which is coherent with the two levels of order book.**\n* **All the columns description are available in the [\"Data\"](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/data) tab of the competition so I will not go further on this.**","e6648c43":"## test.csv","fc438b70":"# Other Comments","f3d1ccc8":"* **`stock_id = 24` and `stock_id = 25` do not exist hence the empty plots**\n* **Some stocks seem steadily traded over time while for some others the activity is more sparse.**\n* **Picking random `time_id` seems to converge to the conclusion that each stock have roughly the same trade pattern over time (i.e. the one that are very active are always very active and the ones that are not very active stay that way).**\n* **High `order_count` and high `size` are correlated with the activity from `seconds_in_bucket` (see `stock_id` equal 29, 43, 69, 124).**","2ea1f425":"## book_[train|test].parquet","d86833eb":"# Explanatory Data Analysis","bb681192":"# Data Discovery","c3dc66ea":"**Casting back `stock_id` to int8 instead of categorical. For the other columns, types look coherent.**","c2aa836f":"**There is a total of 112 different stocks in the dataset.**","66f75553":"## Loading Data","6791cfc4":"## train.csv","0cacb1c0":"`\n Memory usage for book_train.parquet once loaded: 5901.70 MB\n Memory usage for trade_train.parquet once loaded: 549.07 MB`\n\n**Ok, this smells bad. It reminds me an other kaggle competition where having huge dataset were a huge pain in the foot. Everything will be slow and tedious : unexpected notebook restart, plotting data, feature enginnering, ...**\n\n**Some of the following will be a generalization of a subset of book data. It is probably possible to build something or circumvent the issue but I am not sure it is worthy.**","90bf3a75":"**This file contains the realized volatility (called `target`) for several couple of `(stock_id, time_id)`, couples that will constitute the unique identifier called `row_id` for predictions submission. They also allow to identify trade and book orders from parquet files.**","dc9a9c0d":"**I am a bit late to the party and I suppose there are already good EDA based notebooks in the Code section of the competition. Nonetheless, I think it is an exercise almost required before trying to address the main objectives. For me it is one of the greatest way to get your hands on an unknown dataset so let's go !**","0e5069ed":"# **Optiver Realized Volatility Prediction**","e4dc2d5c":"## sample_submission.csv "}}