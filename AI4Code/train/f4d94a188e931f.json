{"cell_type":{"e407d6ee":"code","e42cc099":"code","a8a58e60":"markdown","6c96a7ba":"markdown","2d249223":"markdown","c5e5bb6f":"markdown","6c5e324b":"markdown","2ae7b4f6":"markdown","554e14ce":"markdown","7fd30779":"markdown","87d9d9bd":"markdown","aae23120":"markdown","936cbe87":"markdown","c07d5d60":"markdown","903c1e29":"markdown","b8ec7fee":"markdown","fc4949d8":"markdown","00b3a7f6":"markdown","2bb74f34":"markdown","5145f506":"markdown","7b93738a":"markdown","9542aff9":"markdown","3a47fef6":"markdown","31607986":"markdown","b5ee4f0f":"markdown","a6fed695":"markdown","e0e6ce96":"markdown","a6cad5db":"markdown","3fae8651":"markdown","e745480a":"markdown","db082c16":"markdown","2f26a56d":"markdown","8cace0c8":"markdown","6366b64b":"markdown","db91115b":"markdown","a8f16bf6":"markdown"},"source":{"e407d6ee":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.applications.vgg16 import decode_predictions\nfrom keras.applications.vgg16 import VGG16\n# load the model\nmodel = VGG16()\n# load an image from file\nimage = load_img('..\/input\/Mug.png', target_size=(224, 224))\n# convert the image pixels to a numpy array\nimage = img_to_array(image)\n# reshape data for the model\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n# prepare the image for the VGG model\nimage = preprocess_input(image)\n# predict the probability across all output classes\nyhat = model.predict(image)\n# convert the probabilities to class labels\nlabel = decode_predictions(yhat)\n# retrieve the most likely result, e.g. highest probability\nlabel = label[0][0]\n# print the classification\nprint('%s (%.2f%%)' % (label[1], label[2]*100))","e42cc099":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.applications.inception_v3 import decode_predictions\nfrom keras.applications.inception_v3 import InceptionV3\n# load the model\nmodel = InceptionV3()\n# load an image from file\nimage = load_img('..\/input\/Mug.png', target_size=(299, 299))\n# convert the image pixels to a numpy array\nimage = img_to_array(image)\n# reshape data for the model\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n# prepare the image for the VGG model\nimage = preprocess_input(image)\n# predict the probability across all output classes\nyhat = model.predict(image)\n# convert the probabilities to class labels\nlabel = decode_predictions(yhat)\n# retrieve the most likely result, e.g. highest probability\nlabel = label[0][0]\n# print the classification\nprint('%s (%.2f%%)' % (label[1], label[2]*100))","a8a58e60":"![Screen%20Shot%202019-08-19%20at%2012.23.34.png](attachment:Screen%20Shot%202019-08-19%20at%2012.23.34.png)","6c96a7ba":"![Screen%20Shot%202019-08-19%20at%2012.22.24.png](attachment:Screen%20Shot%202019-08-19%20at%2012.22.24.png)","2d249223":"Number of operations = (14\u00d714\u00d748)\u00d7(5\u00d75\u00d7480) = 112.9M\n\nWith the use of 1\u00d71 convolution:","c5e5bb6f":"The below image is the \u201cnaive\u201d inception module(without 1 X 1 convolution). It performs convolution on an input, with 3 different sizes of filters (1x1, 3x3, 5x5). Additionally, max pooling is also performed. The outputs are concatenated and sent to the next inception module.","6c5e324b":"We have already discussed about convolution layers (denoted by <b> CONV <\/b>) and pooling layers (denoted by <b> POOL <\/b>).\n\n<b> RELU <\/b> is just a non linearity which is applied similar to neural networks.\n\nThe <b> FC <\/b> is the fully connected layer of neurons at the end of CNN. Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks and work in a similar way.","2ae7b4f6":"Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently.\n\nThe most common approach used in pooling is max pooling.","554e14ce":"Global Average Pooling","7fd30779":"![Screen%20Shot%202019-08-19%20at%2010.47.24.png](attachment:Screen%20Shot%202019-08-19%20at%2010.47.24.png)","87d9d9bd":"> ### GoogLeNet \/ InceptionNet","aae23120":"<b>Convolution on input with different sizes of filters:<\/b>\n- Because of this huge variation in the location of the information(A mug in an image can cover the image completely\/ part of it\/ very little space), choosing the right kernel size for the convolution operation becomes tough. A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally.\n\nVery deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network.","936cbe87":"GoogleNet have filters with multiple sizes operating on the same level. The network essentially is a bit \u201cwider\u201d rather than \u201cdeeper\u201d.","c07d5d60":"![Screen%20Shot%202019-08-19%20at%2013.07.55.png](attachment:Screen%20Shot%202019-08-19%20at%2013.07.55.png)","903c1e29":"![Screen%20Shot%202019-08-19%20at%2010.46.20.png](attachment:Screen%20Shot%202019-08-19%20at%2010.46.20.png)","b8ec7fee":"![Screen%20Shot%202019-08-18%20at%2017.33.22.png](attachment:Screen%20Shot%202019-08-18%20at%2017.33.22.png)","fc4949d8":"# CNN\nCNNs have wide applications in image and video recognition, recommender systems and natural language processing. In this article, the example that we will take is related to Computer Vision. However, the basic concept remains the same and can be applied to any other use-case!","00b3a7f6":"![](http:\/\/)![Screen%20Shot%202019-08-18%20at%2019.05.59.png](attachment:Screen%20Shot%202019-08-18%20at%2019.05.59.png)","2bb74f34":"What we\u2019ll cover:\n\nThe 1\u00d71 Convolution\n\nInception Module\n\nGlobal Average Pooling\n\nOverall Architecture\n\nAuxiliary Classifiers for Training\n\nTesting Details","5145f506":"<b>Typical Architecture of a CNN<\/b>","7b93738a":"## CNN Architectures","9542aff9":"Number of operations for 1\u00d71 = (14\u00d714\u00d716)\u00d7(1\u00d71\u00d7480) = 1.5M\n\nNumber of operations for 5\u00d75 = (14\u00d714\u00d748)\u00d7(5\u00d75\u00d716) = 3.8M\n\nTotal number of operations = 1.5M + 3.8M = 5.3M\n\nwhich is much much smaller than 112.9M !!!!!!!!!!!!!!!\n\nIndeed, the above example is the calculation of 5\u00d75 conv at inception (4a).\n\n(We may think that, when dimension is reduced, actually we are working on the mapping from high dimension to low dimension in a non-linearity way. In contrast, for PCA, it performs linear dimension reduction.)\n\nThus, inception module can be built without increasing the number of operations largely compared the one without 1\u00d71 convolution!","3a47fef6":"### CNN vs. Traditional Computer Vision\n\nImage classification is the task of classifying a given image into one of the pre-defined categories. Traditional pipeline for image classification involves two modules: viz. <b>feature extraction<\/b> and <b>classification<\/b>. \n\n<b> Feature extraction <\/b> involves extracting a higher level of information from raw pixel values that can capture the distinction among the categories involved. This feature extraction is done in an unsupervised manner wherein the classes of the image have nothing to do with information extracted from pixels. Some of the traditional and widely used features are GIST, HOG, SIFT, LBP etc. After the feature is extracted, a classification module is trained with the images and their associated labels. A few examples of these modules are SVM, Logistic Regression, Random Forest, decision trees etc.\n\n<b> The problem with this pipeline <\/b> is that feature extraction cannot be tweaked according to the classes and images. So if the chosen feature lacks the representation required to distinguish the categories, the accuracy of the classification model suffers a lot, irrespective of the type of classification strategy employed. A common theme among the state of the art following the traditional pipeline has been, to pick multiple feature extractors and club them inventively to get a better feature. But this involves <b> too many heuristics as well as manual labor to tweak parameters according to the domain to reach a human level accuracy. <\/b>. That\u2019s why it took years to build a good computer vision system(like OCR, face verification, image classifiers, object detectors etc), that can work with a wide variety of data encountered during practical application.","31607986":"The architecture is shown below:","b5ee4f0f":"### CNN Explanation\nCNNs, like neural networks, are made up of neurons with learnable weights and biases. Each neuron receives several inputs, takes a weighted sum over them, pass it through an activation function and responds with an output. The whole network has a loss function and all the tips and tricks that we developed for neural networks still apply on CNNs. Pretty straightforward, right?\n\nSo, how are Convolutional Neural Networks is different than Neural Networks?\n\n<b> CNNs operate over Volumes ! <\/b>\n\nWhat do we mean by this?","a6fed695":"1\u00d71 convolution can help to reduce model size which can also somehow help to reduce the overfitting problem!!","e0e6ce96":"1. The 1\u00d71 Convolution\n1\u00d71 convolution is used with ReLU. In GoogLeNet, 1\u00d71 convolution is used as a dimension reduction module to reduce the computation. By reducing the computation bottleneck, depth and width can be increased.\nI pick a simple example to illustrate this. Suppose we need to perform 5\u00d75 convolution without the use of 1\u00d71 convolution as below:","a6cad5db":"As stated before, deep neural networks are computationally expensive. To make it cheaper, the authors limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions. Though adding an extra operation may seem counterintuitive, 1x1 convolutions are far more cheaper than 5x5 convolutions, and the reduced number of input channels also help. Do note that however, the 1x1 convolution is introduced after the max pooling layer, rather than before.","3fae8651":"![Screen%20Shot%202019-08-19%20at%2011.15.44.png](attachment:Screen%20Shot%202019-08-19%20at%2011.15.44.png)","e745480a":"Unlike neural networks, where the input is a vector, here the input is a multi-channeled image (3 channeled in this case).\n\nThere are other differences that we will talk about in a while.\n\nBefore we go any deeper, let us first understand what convolution means.\n\n#### Convolution\nWe take the 5 X 5 X 3 filter and slide it over the complete image and along the way take the dot product between the filter and chunks of the input image.\n\nFor every dot product taken, the result is a scalar. The complete image is convolved with the filter.\n\nThe convolution layer comprises of a set of independent filters (10 in the example shown). Each filter is independently convolved with the image and we end up with 6 feature maps of shape 28 X 28 X 10.\n\nFor a number of convolution layers in sequence, all these filters are initialized randomly and become our parameters which will be learned by the network subsequently.\n\nFor a particular feature map (the output received on convolving the image with a particular filter is called a feature map), <b> each neuron is connected only to a small chunk of the input image and all the neurons have the same connection weights. <\/b> \n\n<b>CNNs have a couple of concepts called parameter sharing and local connectivity<\/b>\n\n<b> Parameter sharing <\/b> is sharing of weights by all neurons in a particular feature map.\n\n<b> Local connectivity <\/b> is the concept of each neural connected only to a subset of the input image (unlike a neural network where all the neurons are fully connected)\n\nThis helps to reduce the number of parameters in the whole system and makes the computation more efficient.\n\n#### Pooling Layers ","db082c16":"The VGG() class takes a few arguments that may only interest you if you are looking to use the model in your own project, e.g. for transfer learning.\n\nFor example:\n\n- include_top (True): Whether or not to include the output layers for the model. You don\u2019t need these if you are fitting the model on your own problem.\n- weights (\u2018imagenet\u2018): What weights to load. You can specify None to not load pre-trained weights if you are interested in training the model yourself from scratch.\n- input_tensor (None): A new input layer if you intend to fit the model on new data of a different size.\n- input_shape (None): The size of images that the model is expected to take if you change the input layer.\n- pooling (None): The type of pooling to use when you are training a new set of output layers.\n- classes (1000): The number of classes (e.g. size of output vector) for the model.","2f26a56d":"![Screen%20Shot%202019-08-18%20at%2017.52.54.png](attachment:Screen%20Shot%202019-08-18%20at%2017.52.54.png)","8cace0c8":"Previously, fully connected (FC) layers are used at the end of network, such as in AlexNet. All inputs are connected to each output.\nNumber of weights (connections) above = 7\u00d77\u00d71024\u00d71024 = 51.3M\nIn GoogLeNet, global average pooling is used nearly at the end of network by averaging each feature map from 7\u00d77 to 1\u00d71, as in the figure above.\nNumber of weights = 0\nAnd authors found that a move from FC layers to average pooling improved the top-1 accuracy by about 0.6%.\n","6366b64b":"Needless to say, it is a pretty deep classifier. As with any very deep network, it is subject to the vanishing gradient problem.\nTo prevent the middle part of the network from \u201cdying out\u201d, the authors introduced two auxiliary classifiers (The purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss. Weight value used in the paper was 0.3 for each auxiliary loss.\n# The total loss used by the inception net during training.\ntotal_loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2","db91115b":"### VGG16\nVGG16 was publised in 2014 and is one of the simplest (among the other cnn architectures used in Imagenet competition). It's Key Characteristics are:\n\n- This network contains total 16 layers in which weights and bias parameters are learnt.\n- A total of 13 convolutional layers are stacked one after the other and 3 dense layers for classification.\n- The number of filters in the convolution layers follow an increasing pattern (similar to decoder architecture of autoencoder).\n- The informative features are obtained by max pooling layers applied at different steps in the architecture.\n- The dense layers comprises of 4096, 4096, and 1000 nodes each.\n- The cons of this architecture are that it is slow to train and produces the model with very large size.\n\nThe VGG16 architecture is given below:","a8f16bf6":"![Screen%20Shot%202019-08-18%20at%2017.54.02.png](attachment:Screen%20Shot%202019-08-18%20at%2017.54.02.png)"}}