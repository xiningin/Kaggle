{"cell_type":{"c3383f1e":"code","09c171e6":"code","b43b9432":"code","3267a80e":"markdown"},"source":{"c3383f1e":"%%writefile submission.py\n\nimport random\nfrom sklearn.naive_bayes import MultinomialNB\nimport numpy as np\n\nhistory = [0]*2*1001\n\nmy_last_move = -1\n\nclf = MultinomialNB()\n\nnum_of_strategies = 4\nlast_move_real = None\nlast_moves = [0]*num_of_strategies\nscore_fict = [0]*num_of_strategies\nscore_real = 0\n\ndef have_i_won(my_move, his_move):\n    if my_move == his_move:       return 0\n    if my_move == (his_move+1)%3: return 1\n    \n    return -1\n\ndef bayes_prediction(history, step):\n    global clf\n    \n    if step <= 7: return random.randint(0,2)\n    \n    X = np.array(history[(2*(step-5)):(2*step)])\n            \n    X=X.reshape(1, -1)\n    pred = clf.predict(X)\n        \n    pred=int(pred[0])\n    return  (pred+1)%3\n\ndef bayes_learn(history, step):\n    global clf\n    if step > 7:\n        X   = np.array(history[(2*(step-6)):(2*(step-1))])\n                \n        X=X.reshape(1, -1)\n        y=[history[2*(step-1)+1]]\n            \n        clf.partial_fit(X, y, classes=[0,1,2])\n    \n    \ndef strategy_selection(observation, configuration):\n    global history, last_move_real, last_moves\n    global score_real, score_fict\n    \n    best_strategy = 0 # Default Strategy: Random\n    \n    if observation.step > 0:\n        # First: record the history\n        history[2*(observation.step-1)]   = last_move_real\n        history[2*(observation.step-1)+1] = observation.lastOpponentAction\n                \n        # Incremental learning step (update frequencies, whatever)\n        bayes_learn(history, observation.step)\n        \n        \n        # Next: Calculate the score\n        score_real += have_i_won(last_move_real, observation.lastOpponentAction)\n        \n        # Next: Calculate the score, had I played differently\n        # and choose the strategy that would have done the best given all games\n        score_max = -10000\n        for i in range(len(last_moves)):\n            score_fict[i] += have_i_won(last_moves[i], observation.lastOpponentAction)\n\n            if score_fict[i] > score_max:\n                score_max = score_fict[i]\n                best_strategy = i\n        \n    \n    last_moves[0] = random.randint(0,2)\n    last_moves[1] = bayes_prediction(history, observation.step)\n    last_moves[2] = (last_moves[1]+1)%3\n    last_moves[3] = (last_moves[1]+2)%3\n    \n    last_move_real = last_moves[best_strategy]\n        \n    return last_move_real","09c171e6":"%run -i submission.py","b43b9432":"# Simple test suite:\nclass Test:\n    step = 0\n    lastOpponentAction = 0\n    \ndef have_i_won_env(my_move, his_move):\n    if my_move == his_move:       return 0\n    if my_move == (his_move+1)%3: return 1\n    \n    return -1\n\n\nc=Test()\n\nscore = 0\nfor i in range(1000):\n    c.step = i\n    move = strategy_selection(c, c)\n    #print(str(i) +': ' + str(move))\n    c.lastOpponentAction = random.randint(0,2)\n    score += have_i_won_env(move, c.lastOpponentAction)\n    print(\"Round \", i, \" with score: \", score)","3267a80e":"# General Idea\n\nThe Nash Equilibrium in this game is of course playing randomly.\n\nHowever, this will not lead to a good submission, as there are people uploading submissions with some \"structure\" in them, and if you are better at finding structure, then, you are exploitable.\n\nThis means, when you are losing, you should switch strategies.\n\nIn here, I introduce a high level way of doing this: I have a simple Prediction Algorithm based on https:\/\/www.kaggle.com\/daniello20000\/a-simple-machine-learing-approach, but it could be any algorithm: frequency counting, pattern matching, ... and I evaluate different pattern in parallel and count their ficticious scores. The different patterns are:\n\nFirst: The opponent outguesses me every time and he wins. So, I change my strategy by adding one.\nSecond: Same as above, but now, he predicts, I do that. So, I change my strategy by adding two.\nLast: If nothing works, I switch to a random strategy.\n\nAfterward, I select the one best one of the four strategies."}}