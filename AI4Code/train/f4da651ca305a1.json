{"cell_type":{"8e77fd5b":"code","01592541":"code","a302acc2":"code","46ef2ffc":"code","d3270d6d":"code","70df0046":"code","fc676f16":"code","7c858c0a":"code","b50a04ab":"code","e5e0b9de":"code","edc784d9":"code","52ca4e86":"code","3bc4833f":"code","2f5632e1":"code","94eeadc0":"code","d4da3b7e":"code","78380497":"code","81e7c1a3":"code","dd3c8317":"code","484901b4":"code","8ba9a9ad":"code","9dec8e7a":"code","2ac247b2":"code","b7efaded":"code","de4b104f":"markdown","1acdbf20":"markdown","8b7355c2":"markdown","d518bbbf":"markdown","ee3a8532":"markdown","ea60191a":"markdown","741bd609":"markdown","35c8f4a7":"markdown","e93fddd0":"markdown","a4ed2729":"markdown","d720f401":"markdown","394032f8":"markdown"},"source":{"8e77fd5b":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np \nimport pandas as pd\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.cluster import KMeans\nimport lightgbm as lgb\n\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\npath_data = '..\/input\/optiver-realized-volatility-prediction\/'\n\nimport warnings\nwarnings.filterwarnings('ignore')","01592541":"%%time\nStockID = True\nif StockID:\n    df_training = pd.read_pickle('..\/input\/optiver-dataset-version-2\/training.pkl')\nelse:\n    df_training = pd.read_pickle('..\/input\/optiver-dataset-version-2\/training_no_stock_id.pkl')","a302acc2":"df_training = df_training.drop(['wap2_mean', 'wap2_std'], axis = 1)","46ef2ffc":"%%time\ncols_kmeans = ['log_return1_realized_volatility', 'log_return1_realized_absvar',\n    'log_return1_realized_quarticity','log_return1_realized_tripower_quarticity',\n    'log_return1_realized_quadpower_quarticity', 'log_return1_realized_1',\n    'log_return1_realized_2']\nclustering = False\nif clustering:\n    SSE = []\n    for cluster in range(1,11):\n        print(f'Done Cluster: {cluster}')\n        kmeans = KMeans(n_init = 8, n_clusters = cluster, init='k-means++')\n        kmeans.fit(df_training[cols])\n        SSE.append(kmeans.inertia_)\n    # Elbow Method\n    frame = pd.DataFrame({'Cluster':range(1,11), 'SSE':SSE})\n    plt.figure(figsize=(12,6))\n    plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n    plt.xlabel('Number of clusters')\n    plt.ylabel('Inertia')","d3270d6d":"%%time \n\nKMEAN = True\nif KMEAN:\n    kmeans = KMeans(n_init = 20, n_clusters = 8, init='k-means++')\n\n    kmeans.fit(df_training[cols_kmeans].fillna(0))\n        \n    df_training['kmeans'] = kmeans.labels_","70df0046":"df_training['kmeans']","fc676f16":"#%%time\n#df_testing = pd.read_pickle('..\/input\/optiver-dataset-version-1\/testing.pkl')","7c858c0a":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef realized_absvar(series):\n    return np.sqrt(np.pi\/(2*series.count()))*np.sum(np.abs(series))\n\n# Calculate integrated quarticity\ndef realized_quarticity(series):\n    return (series.count()\/3)*np.sum(series**4)\n\ndef calc_depth(df):\n    depth = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'] + df['bid_price2'] * df[\n               'bid_size2'] + df['ask_price2'] * df['ask_size2']\n    return depth\n\ndef realized_3(series):\n    from scipy.special import gamma\n    numerator = (gamma(1\/2)**3)*np.sum((abs(series)**(4\/3)).rolling(window=3).apply(np.prod))\n    denominator = 8 * (gamma(7\/6)**3)*np.sum(series**2)\n    return np.sqrt(numerator\/denominator)\n\ndef realized_2(series):\n    return np.sqrt(((np.pi**2)*np.sum(abs(series.rolling(window=4).apply(np.product, raw=True))))\/(8*np.sum(series**2)))\n\ndef realized_1(series):\n    return np.sqrt(np.sum(series**4)\/(6*np.sum(series**2)))\n\ndef realized_tripower_quarticity(series):\n    from scipy.special import gamma\n    series = series ** (4\/3)\n    series = abs(series).rolling(window=3).apply(np.prod, raw=True)\n    return series.shape[0]*0.25*((gamma(1\/2)**3)\/(gamma(7\/6)**3))*np.sum(series)\n\ndef realized_quadpower_quarticity(series):\n    series = abs(series.rolling(window=4).apply(np.product, raw=True))\n    return (np.sum(series) * series.shape[0] * (np.pi**2))\/4\n\ndef processing_book_data(file_path):\n    \n    df = pd.read_parquet(file_path)\n    # Wap 1\n    df['wap1'] = calc_wap1(df) \n    df['log_return1'] = df.groupby('time_id')['wap1'].apply(log_return)\n    # Wap 2 \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    df['depth'] = calc_depth(df)\n    \n    create_feature_dict = {\n        'log_return1':[realized_volatility,realized_absvar,realized_quarticity,realized_tripower_quarticity,realized_quadpower_quarticity,realized_1,realized_2],\n        'log_return2':[realized_volatility,realized_absvar,realized_quarticity,realized_tripower_quarticity,realized_quadpower_quarticity,realized_1,realized_2],\n        'wap_balance':[np.mean],\n        #'price_spread':[np.mean],\n        'bid_spread':[np.mean,np.sum],\n        'ask_spread':[np.mean, np.max],\n        'volume_imbalance':[np.mean],\n        #'total_volume':[np.mean],\n        'wap1':[np.mean,np.std],\n        #'wap2':[np.mean,np.std]\n    }\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility]\n    }\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    # 300 and 400 \n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    # 600\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    # merge\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature.drop(['time_id__400', 'time_id__300'], axis = 1, inplace = True)\n    \n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature\n\ndef processing_trade_data(file_path):\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n    }\n    \n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n    }\n    \n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    \n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    \n    df_feature.drop(['time_id__400', 'time_id__300'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n\n    return df_feature\n\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility',  \n                'log_return1_realized_volatility_400',\n                'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300',\n                'log_return2_realized_volatility_300',\n                'trade_log_return_realized_volatility', \n                'trade_log_return_realized_volatility_400',\n                'trade_log_return_realized_volatility_300']\n\n    valo_cols1 = ['log_return1_realized_volatility','trade_log_return_realized_volatility_400']\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'max', 'min' ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    feature_column_needed = [ 'stock_id__stock' ,'log_return1_realized_volatility_mean_stock', \n       'log_return1_realized_volatility_max_stock',\n       'log_return1_realized_volatility_min_stock',\n       'log_return1_realized_volatility_400_max_stock',\n       'log_return1_realized_volatility_400_min_stock',\n       'log_return2_realized_volatility_400_min_stock',\n       'log_return1_realized_volatility_300_min_stock',\n       'log_return2_realized_volatility_300_min_stock',\n       'trade_log_return_realized_volatility_mean_stock',\n       'trade_log_return_realized_volatility_max_stock',\n       'trade_log_return_realized_volatility_400_max_stock',\n       'trade_log_return_realized_volatility_400_min_stock',\n       'trade_log_return_realized_volatility_300_min_stock',]\n    \n    df_stock_id_column = pd.DataFrame(df_stock_id, columns = feature_column_needed)\n    \n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[valo_cols1].agg(['mean', 'min' ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    feature_column_needed = ['time_id__time',\n        'log_return1_realized_volatility_mean_time',\n       'trade_log_return_realized_volatility_400_min_time']\n    \n    df_time_id_column = pd.DataFrame(df_time_id, columns= feature_column_needed)\n    \n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id_column, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id_column, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n\ndef read_train_test():\n    #train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    #train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    #print(f'Our training set has {train.shape[0]} rows')\n    return test\n\ndef preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = path_data + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = path_data + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = path_data + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = path_data + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(processing_book_data(file_path_book),processing_trade_data(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df","b50a04ab":"%%time\ntest = read_train_test()\n\ntest_ids = test.stock_id.unique()\n\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)\ndf_testing = test.merge(df_test, on = ['row_id'], how = 'left')\ndf_testing['kmeans'] = kmeans.predict(df_testing[cols_kmeans].fillna(0))\nif StockID:\n    df_testing = get_time_stock(df_testing)\n    df_testing.drop(['trade_log_return_realized_volatility_400', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_300', 'trade_log_return_realized_volatility_300', 'log_return2_realized_volatility_400','log_return1_realized_volatility_400'], axis = 1, inplace = True)\nelse:\n    pass","e5e0b9de":"df_testing.shape","edc784d9":"#import seaborn as sns\n#plt.figure(figsize=(30,30))\n#corr = df_training.corr()\n#sns.heatmap(corr,cmap='Blues',linewidth=0.5,annot=True)","52ca4e86":"import optuna \nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\ndef rmspe(y_true, y_pred):\n    return (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef rmspe_xgb(pred, dtrain):\n    y = dtrain.get_label()\n    return 'rmspe', rmspe(y, pred)\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","3bc4833f":"num_folds = 5\ngroup_fold = GroupKFold( n_splits = num_folds )\n\nfeatures = [col for col in df_training.columns if col not in { \"target\", \"row_id\"}]\nX = df_training[features]\ny = df_training['target']\n\ndef objective(trial):\n\n    # Optuna suggest params\n    seed = 1111\n    params = {\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.2),        \n        'lambda_l1': trial.suggest_float('lambda_l1', 0.1, 1),\n        'lambda_l2': trial.suggest_float('lambda_l2', 0.1, 1),\n        'num_leaves': trial.suggest_int('num_leaves', 400, 800),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.50, 1),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.50, 1),\n        'bagging_freq': trial.suggest_int('bagging_freq',1, 2),\n        'min_data_in_leaf':trial.suggest_int('min_data_in_leaf',400,700),\n        'max_depth': trial.suggest_int('max_depth', 6 , 13),\n        'seed': seed,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs': -1,\n        #'device': 'gpu','gpu_platform_id': 0,\n        #'gpu_device_id': 0\n    }  \n\n    \n    rmspe_list = []\n\n    for fold, (trn_ind, val_ind) in enumerate(group_fold.split(X, y, groups = X['time_id'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=5000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=20,\n                          feval = feval_rmspe)\n        \n        #preds = model.predict(d_val)\n        preds = model.predict(x_val[features])\n        score = rmspe(y_val, preds)\n        rmspe_list.append(score)\n    \n    print(f'Trial done: rmspe values on folds: {np.mean(rmspe_list)}')\n    return np.mean(rmspe_list)","2f5632e1":"n_trials = 10\n\nFIT_LGB = False\n\nif FIT_LGB:\n    study = optuna.create_study(direction=\"minimize\",study_name = 'LGB')\n    study.optimize(objective)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","94eeadc0":"from catboost import CatBoostRegressor, Pool\nnum_folds = 5\ngroup_fold = GroupKFold( n_splits = num_folds )\n\nfeatures = [col for col in df_training.columns if col not in { \"target\", \"row_id\"}]\nX = df_training[features]\ny = df_training['target']\ndef objective(trial):\n\n    params = {\n        #\"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1),\n        \"depth\": trial.suggest_int(\"depth\", 6, 11),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\",0.01,0.2),\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\",0.5,1),\n        \"random_seed\": 42,\n        \"iterations\": 5000,\n        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Lossguide\", \"Depthwise\"]),\n        \"bootstrap_type\":\"Bayesian\", \n        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0, 5),\n        \"loss_function\" : 'RMSE',\n        \"eval_metric\":\"RMSE\",\n        \"has_time\": True\n        #\"task_type\":'GPU'\n    }\n    \n    if params[\"grow_policy\"] == \"Depthwise\" or params[\"grow_policy\"] == \"Lossguide\":\n        params[\"min_data_in_leaf\"] = trial.suggest_int(\"min_data_in_leaf\", 400, 700)\n    if params[\"grow_policy\"] == \"Lossguide\":\n        params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 10, 20)\n    \n    #\n    rmspe_list = []\n\n    for fold, (trn_ind, val_ind) in enumerate(group_fold.split(X, y, groups = X['time_id'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        \n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        \n        train_pool = Pool(x_train, y_train,weight=train_weights)\n        val_pool = Pool(x_val, y_val,weight=val_weights)\n        \n        model = CatBoostRegressor(**params)\n        \n        model.fit(train_pool,\n                  use_best_model = True,\n                  eval_set = val_pool, \n                  verbose = 250,\n                  early_stopping_rounds=10,\n                  plot =True)\n\n        preds = model.predict(x_val[features])\n        score = rmspe(y_val, preds)\n        rmspe_list.append(score)\n    \n    print(f'Trial done: rmspe values on folds: {np.mean(rmspe_list)}')\n    return np.mean(rmspe_list)","d4da3b7e":"n_trials = 10\n\nFIT_CAT = False\n\nif FIT_CAT:\n    study = optuna.create_study(direction=\"minimize\",study_name = 'CAT')\n    study.optimize(objective, n_trials=n_trials)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","78380497":"cat_params = {'depth': 6, 'learning_rate': 0.08814388269284729, 'l2_leaf_reg': 0.863393460851281, \n              'grow_policy': 'Lossguide', 'bagging_temperature': 0.9196224418501292, 'min_data_in_leaf': 443, \n              'max_leaves': 20,\"random_seed\": 42,\"iterations\": 5000,\n              \"bootstrap_type\":\"Bayesian\", \"loss_function\" : 'RMSE',\"eval_metric\":\"RMSE\",\"has_time\": True}\n\ncat_params_1 = {'depth': 6, 'learning_rate': 0.02562650627754423, 'l2_leaf_reg': 0.6658610403321601,\n                'grow_policy': 'SymmetricTree', 'bagging_temperature': 4.585387000263623,\"random_seed\": 42,\"iterations\": 5000,\n                \"bootstrap_type\":\"Bayesian\", \"loss_function\" : 'RMSE',\"eval_metric\":\"RMSE\",\"has_time\": True}","81e7c1a3":"from catboost import CatBoostRegressor\n\nimport joblib\ndef train_and_evaluate_cat(train, test, params, boost=5000):\n    test = test.drop('time_id',axis=1)\n    features = [col for col in train.columns if col not in { \"target\", \"row_id\", \"time_id\" }]\n    #features = [col for col in train.columns if col not in {\"target\", \"row_id\"}]\n    X = train[features] \n    y = train['target']\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    \n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    \n    # Create a KFold object\n    num_folds = 5\n    # Group Fold\n    group_fold = GroupKFold( n_splits = num_folds )\n    \n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(group_fold.split(X, y, groups = train['time_id'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_pool = Pool(x_train, y_train,weight=train_weights)\n        val_pool = Pool(x_val, y_val,weight=val_weights)\n        model = CatBoostRegressor(**params)\n        \n        model.fit(train_pool,use_best_model =True, eval_set = val_pool, \n                  verbose = 250,early_stopping_rounds=20,plot =True)\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        \n        # Predict the test set\n        test_predictions += model.predict(test[features]) \/ num_folds\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    print(30*'-')\n    # Return test predictions\n    return test_predictions","dd3c8317":"CAT_TRAIN = False\nif CAT_TRAIN:\n    predictions_cat_1 = train_and_evaluate_cat( df_training, df_testing, cat_params_1 )","484901b4":"new_params = {'learning_rate': 0.04617790260722144, 'lambda_l1': 0.7812638405539516, \n              'lambda_l2': 0.10048137762832532, 'num_leaves': 305, 'feature_fraction': 0.7620458604190743,\n              'bagging_fraction': 0.8666298803788923, 'bagging_freq': 2, 'min_data_in_leaf': 574, 'max_depth': 11,'seed': 1111,\n              'objective': 'rmse','boosting': 'gbdt','verbosity': -1,'n_jobs': -1,} # 0.2288\n\nnew_params_1 = {'learning_rate': 0.12100470091570181, 'lambda_l1': 0.9484882120442775, \n                'lambda_l2': 0.18592387603709012, 'num_leaves': 747, 'feature_fraction': 0.5464453484366312, \n                'bagging_fraction': 0.8852154214540782, 'bagging_freq': 2, 'min_data_in_leaf': 533, 'max_depth': 9,\n               'seed': 1111,'objective': 'rmse','boosting': 'gbdt','verbosity': -1,'n_jobs': -1} # 0.22913\n\nnew_params_2 = {'learning_rate': 0.0798333323013617, 'lambda_l1': 0.931510376585135, \n                'lambda_l2': 0.9836497055219714, 'num_leaves': 750, 'feature_fraction': 0.5975156297650424, \n                'bagging_fraction': 0.8211686012186546, 'bagging_freq': 1, 'min_data_in_leaf': 494, 'max_depth': 12,\n                 'seed': 1111,'objective': 'rmse','boosting': 'gbdt','verbosity': -1,'n_jobs': -1} # 0.2290\nnew_params_3 = {'learning_rate': 0.035210179568149945, 'lambda_l1': 0.2785515724790116, 'lambda_l2': 0.1344375389299167, \n                'num_leaves': 420, 'feature_fraction': 0.7424685714197569, \n                'bagging_fraction': 0.7501140603166803, 'bagging_freq': 1, 'min_data_in_leaf': 639, 'max_depth': 11,\n               'seed': 1111,'objective': 'rmse','boosting': 'gbdt','verbosity': -1,'n_jobs': -1} # 0.2287","8ba9a9ad":"import joblib\ndef train_and_evaluate_lgb(train, test, params, boost=5000):\n    test['stock_id'] = test['stock_id'].astype('category')\n    test = test.drop('time_id',axis=1)\n    features = [col for col in train.columns if col not in { \"target\", \"row_id\", \"time_id\" }]\n    X = train[features]\n    X['stock_id'] = X['stock_id'].astype('category')\n    y = train['target']\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    \n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    \n    # Create a KFold object\n    num_folds = 5\n    # Group Fold\n    group_fold = GroupKFold( n_splits = num_folds )\n    \n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(group_fold.split(X, y, groups = train['time_id'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=boost,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=20,\n                          feval = feval_rmspe)\n        \n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        \n        # Predict the test set\n        test_predictions += model.predict(test[features]) \/ num_folds\n        \n        # Save model\n        joblib.dump(model, f'model_fold{fold}.pkl')\n        print(f'Model {fold + 1} Saved...')\n        \n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    print(30*'-')\n    lgb.plot_importance(model,max_num_features=20)\n    \n    # Return test predictions\n    return test_predictions","9dec8e7a":"LGB_TRAIN = True\nif LGB_TRAIN:\n    predictions_lgb_1 = train_and_evaluate_lgb( df_training, df_testing, new_params )\n    predictions_lgb_2 = train_and_evaluate_lgb( df_training, df_testing, new_params_1 )\n    predictions_lgb_3 = train_and_evaluate_lgb( df_training, df_testing, new_params_2 )\n    predictions_lgb_4 = train_and_evaluate_lgb( df_training, df_testing, new_params_3 )","2ac247b2":"# LGBM and CatBoost Ensemble\ndf_testing['target'] = ((predictions_lgb_1 * 0.35) + (predictions_lgb_2 * 0.15) + (predictions_lgb_3 * 0.15) + (predictions_lgb_4 * 0.35))# * 0.95 + (predictions_cat_1) * 0.05\ndf_testing[['row_id', 'target']].to_csv('submission.csv',index = False)","b7efaded":"pd.read_csv(\"submission.csv\")","de4b104f":"# LGBM Tuning","1acdbf20":"# Kmeans","8b7355c2":"# CAT Boost Training","d518bbbf":"## Hey,It was really great to be part of this competition. Learnt a lot of new stuff.\n\n1. I have reffred a lot from existing public notebooks. Lovely Thanks for that.**\n\n2. Used CatBoost and 2 LGBM models, which is trained through 5 GroupKFold by time_id. \n\n3. I have used very few feature, only about 30-40.\n\n4. Optuna was my library for Hyperparameter tuning. \n\n5. Also used Kmeans Algo on columns which cor-related highly with target column.\n\n7. Had other stuff going on, I was able to put very little effort on this competition(Some where about 4-5 days). I wish I had more time to be spent on this competition, I could have learnt more\n\n8. **With this Notebook I was promoted to notebook expert. Thank You for your support.**\n\n**All the Best**","ee3a8532":"# Imports","ea60191a":"# LGB Training ","741bd609":"# Preprocessing && Test Data ","35c8f4a7":"# Correlation ","e93fddd0":"# CatBoost Tuning ","a4ed2729":"# Model && Tuning ","d720f401":"# Submission CSV","394032f8":"# Dataset Pre-load"}}