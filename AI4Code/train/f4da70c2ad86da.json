{"cell_type":{"da9ef935":"code","7a2c24cc":"code","d52a961f":"code","66fef4b6":"code","45f35ec7":"code","9100a093":"code","b9a95896":"code","120164d7":"code","4128cbae":"code","27c701c0":"code","40c1e595":"code","509a9367":"code","3d2bd964":"code","2cffb7bc":"code","fa354fec":"code","e94d20bb":"code","2ef97985":"code","5f0d1e10":"code","c4b6b396":"code","bb5cee70":"code","a6360275":"code","6d1986ca":"code","fff9b989":"code","e2d3a672":"code","e91aec7a":"code","f3ca7c1c":"code","1f2a4164":"code","a6b3f59d":"code","6674d867":"code","9c293a98":"code","6a636084":"code","b60f017a":"code","74d49438":"code","43f2ed9f":"code","e8eaddae":"code","6ddb6361":"markdown","15940d1c":"markdown","44f7c866":"markdown","97977cd8":"markdown","faaf25d7":"markdown","6a6f29cc":"markdown","36d7a3b7":"markdown","b0a5ff31":"markdown","de92b091":"markdown","a7769d00":"markdown","f3a85189":"markdown"},"source":{"da9ef935":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, precision_recall_curve\n\nimport re\nimport string\nimport tqdm\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nlemma = WordNetLemmatizer()\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization,SpatialDropout1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","7a2c24cc":"train= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","d52a961f":"print('There are {} rows and {} columns in train set'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in test et'.format(test.shape[0],test.shape[1]))","66fef4b6":"train.head(10)","45f35ec7":"missing_val = pd.DataFrame(train.isnull().sum())\nmissing_val = missing_val.reset_index()\nmissing_val","9100a093":"train.info()","b9a95896":"train.describe().T","120164d7":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions\n\ntrain['clean_text'] = train['text'].apply(lambda x: remove_URL(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_emoji(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_html(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_punct(x))","4128cbae":"# Tokenizing the cleaned texts.\n\ntrain['tokenized'] = train['clean_text'].apply(word_tokenize)\n","27c701c0":"train.head()","40c1e595":"train['lower'] = train['tokenized'].apply(\n    lambda x: [word.lower() for word in x])\n\ntrain['no_stopwords'] = train['lower'].apply(\n    lambda x: [word for word in x if word not in set(nltk.corpus.stopwords.words('english'))])\n\n","509a9367":"train['no_stopwords'] = [' '.join(map(str, l)) for l in train['no_stopwords']]","3d2bd964":"train.head()","2cffb7bc":"test['clean_text'] = test['text'].apply(lambda x: remove_URL(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_emoji(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_html(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_punct(x))\n\ntest['tokenized'] = test['clean_text'].apply(word_tokenize)\n\ntest['lower'] = test['tokenized'].apply(\n    lambda x: [word.lower() for word in x])\n\ntest['no_stopwords'] = test['lower'].apply(\n    lambda x: [word for word in x if word not in set(nltk.corpus.stopwords.words('english'))])\n\ntest['no_stopwords'] = [' '.join(map(str, l)) for l in test['no_stopwords']]","fa354fec":"combined = train['no_stopwords'].tolist() + test['no_stopwords'].tolist()","e94d20bb":"len(combined)","2ef97985":"# Target distribution.\n\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\nsns.countplot(train['target'], ax=axes[0])\naxes[1].pie(train['target'].value_counts(),\n            labels=['Not Disaster', 'Disaster'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Distribution of the Tweets', fontsize=24)\nplt.show()","5f0d1e10":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')","c4b6b396":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.no_stopwords[train['target']==1])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","bb5cee70":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.no_stopwords[train['target']==0])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","a6360275":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='yellow')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Words in a processed tweet')\nplt.show()","6d1986ca":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='purple')\nax1.set_title('disaster tweets')\nword=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='orange')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Average word length in each processed tweet')","fff9b989":"# Load pretrained GloVe embeddings\n\nembeddings_index = dict()\nf = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","e2d3a672":"#Creating Embeddings for the tweets\n\nmax_len_tweet = 50\n\ntok = Tokenizer()\ntok.fit_on_texts(combined)\nvocab_size = len(tok.word_index) + 1\nencoded_tweet = tok.texts_to_sequences(combined)\npadded_tweet = pad_sequences(encoded_tweet, maxlen=max_len_tweet, padding='post')\n\nvocab_size = len(tok.word_index) + 1\n\ntweet_embedding_matrix = np.zeros((vocab_size, 100))\nfor word, i in tok.word_index.items():\n    t_embedding_vector = embeddings_index.get(word)\n    if t_embedding_vector is not None:\n        tweet_embedding_matrix[i] = t_embedding_vector","e91aec7a":"\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 100, input_length=max_len_tweet, embeddings_initializer=Constant(tweet_embedding_matrix), trainable=False))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(128, dropout=0.3, recurrent_dropout=0.2))\nmodel.add(BatchNormalization())\n# model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n# model.add(BatchNormalization())\n# model.add(LSTM(20,dropout=0.2, recurrent_dropout=0.2))\n# model.add(Dropout(0.3))\n#model.add(BatchNormalization())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\noptimzer=Adam(learning_rate=1e-4)\nmodel.compile(loss='binary_crossentropy', optimizer=optimzer, metrics=['accuracy', 'mae'])","f3ca7c1c":"model.fit(padded_tweet[:7613], train['target'].values, epochs = 11)","1f2a4164":"test.head()","a6b3f59d":"test.head()","6674d867":"preds = model.predict(padded_tweet[7613:])","9c293a98":"preds[:20]","6a636084":"# pred=[1 if i>0.5 else 0 for i in preds]\npred = np.round(preds).astype(int).reshape(3263)","b60f017a":"pred","74d49438":"submission=pd.DataFrame()\nsubmission['id']=test['id'].to_list()\nsubmission['target']=pred","43f2ed9f":"submission.head(10)","e8eaddae":"submission.to_csv('submission4.csv',index=False)","6ddb6361":"### Words in a processed tweet","15940d1c":"## Exploratory Data Analysis","44f7c866":"### Import necessary libraries","97977cd8":"### Average number of words in a processed tweet","faaf25d7":"### Word cloud for all disaster tweets","6a6f29cc":"### Load data","36d7a3b7":"### Creating embedding of our tweets using GloVe embeddings","b0a5ff31":"### Data cleaning and preprocessing\n\nBefore doing the data analysis, it would be better if we could clean-up our data to remove html tags, emojis and other punctuations","de92b091":"### Define LSTM network","a7769d00":"### Word cloud for all disaster tweets","f3a85189":"### Test set"}}