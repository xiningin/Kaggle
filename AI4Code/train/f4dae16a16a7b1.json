{"cell_type":{"d17dd313":"code","a17425b9":"code","47573925":"code","878bd0bb":"code","3cab395c":"code","9542b10f":"code","40158532":"code","de175c05":"code","18e70b8f":"code","b86ebbf2":"code","bc8d42de":"code","fea7d28a":"code","96649e84":"code","becf06bc":"code","1acc4c83":"code","521775cd":"markdown","b366a9af":"markdown","9f052c46":"markdown","43395efb":"markdown","2daa8228":"markdown","9bb0dd33":"markdown","361381ed":"markdown","2ee35641":"markdown"},"source":{"d17dd313":"import os\n\nimport numpy as np\nfrom sklearn import metrics\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","a17425b9":"#Define encoder.\ndef regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","47573925":"#'''\ndef build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_lang_tags = Input(shape=(4,), dtype=tf.float32, name=\"input_lang_tags\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    x = Concatenate()([cls_token, input_lang_tags])\n    out = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=[input_word_ids, input_lang_tags], outputs=out)\n    \n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n    \n    return model\n\n'''\n#Build a pure text model where language information is not considered.\ndef build_model_PT(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n    \n    return model\n#'''","878bd0bb":"AUTO = tf.data.experimental.AUTOTUNE\n# Configuration\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync#16\nMAX_LEN = 192","3cab395c":"# First load the real tokenizer\nMODEL = 'jplu\/tf-xlm-roberta-large'\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","9542b10f":"def lang_embed(lang, tran):\n    lang_codes = {'en':'000', 'es':'100', 'fr':'010',\n                  'it':'001', 'pt':'110', 'ru':'101',\n                  'tr':'011'}\n    tran_codes = {'orig':'0', 'tran':'1'}\n    vec = lang_codes[lang]+tran_codes[tran]\n    vec = [int(v) for v in vec]\n    return vec","40158532":"def text_process(text):\n    ws = text.split(' ')\n    if(len(ws)>160):\n        text = ' '.join(ws[:160]) + ' ' + ' '.join(ws[-32:])\n    return text\n\n#Build the original validation corpus.\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\nvalid['comment_text'] = valid['comment_text'].apply(lambda x: text_process(x))\n\nx_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\nlang_tag_valid = np.array([lang_embed(row['lang'], 'orig') for _, row in valid.iterrows()])\ny_valid = valid.toxic.values\n\ngap = 128#valid.shape[0]%BATCH_SIZE\nx_valid = np.concatenate((x_valid, x_valid[-gap:]))\nlang_tag_valid = np.concatenate((lang_tag_valid, lang_tag_valid[-gap:]))\ny_valid = np.concatenate((y_valid, y_valid[-gap:]))\nprint(y_valid.shape)\n'''\n#Build the translated validation corpus.\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_valid_translated.csv')\nvalid['comment_text'] = valid['comment_text'].apply(lambda x: text_process(x))\n\nx_tran_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\nlang_tag_tran_valid = np.array([lang_embed('en', 'tran') for _, row in valid.iterrows()])\ny_tran_valid = valid.toxic.values\n\ngap = 128#valid.shape[0]%BATCH_SIZE\nx_tran_valid = np.concatenate((x_tran_valid, x_tran_valid[-gap:]))\nlang_tag_tran_valid = np.concatenate((lang_tag_tran_valid, lang_tag_tran_valid[-gap:]))\ny_tran_valid = np.concatenate((y_tran_valid, y_tran_valid[-gap:]))\nprint(y_tran_valid.shape)\n'''\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","de175c05":"#Build the original and translated test corpora.\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\ntest['content'] = test['content'].apply(lambda x: text_process(x))\n\n#tran_test = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_test_translated.csv')\n#tran_test['translated'] = tran_test['translated'].apply(lambda x: text_process(x))\n                       \nx_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\nlang_tag_test = np.array([lang_embed(row['lang'], 'orig') for _, row in test.iterrows()])\n\n#x_tran_test = regular_encode(tran_test.translated.values, tokenizer, maxlen=MAX_LEN)\n#lang_tag_tran_test = np.array([lang_embed('en', 'tran') for _, row in tran_test.iterrows()])","18e70b8f":"mybest = pd.read_csv('\/kaggle\/input\/mybest\/sub9521.csv')\nmybest['orig'] = test['content']\nmybest['lang'] = test['lang']\nmybest['tran'] = ''#tran_test['translated']\n\nout = []\nfor _, row in mybest.iterrows():\n    #if row['lang'] not in ('fr', 'ru', 'pt'):#Only gather those not in validation?\n        #continue\n    if(row['toxic']>=0.5):\n        out.append([row['orig'], row['tran'], row['lang'], 1])\n    elif(row['toxic']<0.5):\n        out.append([row['orig'], row['tran'], row['lang'], 0])\n\ntrain = pd.DataFrame(out, columns=['orig', 'tran', 'lang', 'toxic'])\ntrain['orig'] = train['orig'].apply(lambda x: text_process(x))\n#train['tran'] = train['tran'].apply(lambda x: text_process(x))\nprint(train.shape)","b86ebbf2":"%%time \nx_orig_train = regular_encode(train.orig.values, tokenizer, maxlen=MAX_LEN)\nlang_tag_orig_train = np.array([lang_embed(row['lang'], 'orig') for _, row in train.iterrows()])\n\n#x_tran_train = regular_encode(train.tran.values, tokenizer, maxlen=MAX_LEN)\n#lang_tag_tran_train = np.array([lang_embed('en', 'tran') for _, row in train.iterrows()])\n\ny_train = train.toxic.values\n\ngap = 128#train.shape[0]%BATCH_SIZE\nx_orig_train = np.concatenate((x_orig_train, x_orig_train[-gap:]))\nlang_tag_orig_train = np.concatenate((lang_tag_orig_train, lang_tag_orig_train[-gap:]))\n\n#x_tran_train = np.concatenate((x_tran_train, x_tran_train[-gap:]))\n#lang_tag_tran_train = np.concatenate((lang_tag_tran_train, lang_tag_tran_train[-gap:]))\n\ny_train = np.concatenate((y_train, y_train[-gap:]))\nprint(y_train.shape)","bc8d42de":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    #model = build_model_PT(transformer_layer, max_len=MAX_LEN)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\n#model.summary()\n#model.load_weights('\/kaggle\/input\/basemodels\/mg2m.h5')\nmodel.load_weights('\/kaggle\/input\/basemodels\/mixmoriggen1.h5')\n#model.load_weights('\/kaggle\/input\/en2m1211\/en2m1211.h5')\n#model.load_weights('\/kaggle\/input\/mixmodel0\/mixm0.h5')","fea7d28a":"#'''#First train on best orignal data.\nhistory = model.fit([x_orig_train[:63872],lang_tag_orig_train[:63872]], y_train[:63872],\n                    validation_data=([x_valid[:8064],lang_tag_valid[:8064]], y_valid[:8064]),\n                    batch_size=BATCH_SIZE, epochs=1, verbose=1)\n'''\n#First train on best orignal data.\nhistory = model.fit(x_orig_train[:63872], y_train[:63872],\n                    validation_data=(x_valid[:8064], y_valid[:8064]),\n                    batch_size=BATCH_SIZE, epochs=1, verbose=1)\n#'''","96649e84":"#'''#Fine train on validation data.#\nhistory = model.fit([x_valid[:8064],lang_tag_valid[:8064]], y_valid[:8064],\n                    validation_data=([x_valid[:8064],lang_tag_valid[:8064]], y_valid[:8064]),\n                    batch_size=BATCH_SIZE, epochs=1, verbose=1)\n'''\nhistory = model.fit(x_valid[:8064], y_valid[:8064],\n                    validation_data=(x_valid[:8064], y_valid[:8064]),\n                    batch_size=BATCH_SIZE, epochs=2, verbose=1)\n#'''","becf06bc":"model.save_weights(\"\/kaggle\/working\/mixgn2mp4.h5\")","1acc4c83":"from sklearn.metrics import roc_auc_score\nsub['toxic'] = model.predict([x_test, lang_tag_test], verbose=1)\n#sub['toxic'] = model.predict(x_test, verbose=1)\nsub.to_csv('submission.csv', index=False)\nscore1 = roc_auc_score(mybest.toxic.round().astype(int), sub.toxic.values)\nscore2 = roc_auc_score(sub.toxic.round().astype(int), mybest.toxic.values)\nprint('p: %2.4f %2.4f'%(100*score1, 100*score2))","521775cd":"***3.1 Train on original data.***","b366a9af":"**Load text data into memory**","9f052c46":"# **This notebook is used for training.**\n\nI adopted a lot from xhlulu's notebook at Link. Many thanks!\nXLM-Roberta was used.","43395efb":"1. **1. Gather pseudal labelled corpus for fine train.**","2daa8228":"**3. Fine train on pseudo labelled corpus.**","9bb0dd33":"**2.Load model into the TPU**","361381ed":"***3.3 Save model.***","2ee35641":"***3.4 Predict.***"}}