{"cell_type":{"66ca7359":"code","73d2cd6b":"code","9ca36d10":"code","18d38238":"code","71870d41":"code","813a9200":"code","ca191280":"code","658f68b8":"code","a19cf6d7":"code","eaa83354":"code","2af89950":"code","aa53be3d":"code","b88deb19":"code","f83533b0":"code","8cab037f":"code","197adc73":"code","f9257399":"code","5f3e31f7":"code","04ddb55d":"code","ace8e211":"code","0a66e64d":"code","9f47970c":"code","fe8ea46c":"code","d87f5ade":"code","d0dbe91d":"code","e7fde499":"code","81a3f17b":"code","f4f4d127":"code","d9810e95":"code","6d9dc148":"code","59bd4c88":"code","dfc14d25":"markdown","467229ec":"markdown","bf12c871":"markdown","176c46da":"markdown","39753c47":"markdown","7d1dd2fc":"markdown","1d153068":"markdown","36e1a2eb":"markdown","b13512c5":"markdown"},"source":{"66ca7359":"!pip install Unidecode\n!pip install wandb\n!pip install --upgrade torchtext","73d2cd6b":"import re\nimport random\nimport string\nimport multiprocessing\nimport wandb\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.vocab import build_vocab_from_iterator\n\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nfrom unidecode import unidecode","9ca36d10":"from kaggle_secrets import UserSecretsClient\nwandb_key = UserSecretsClient().get_secret(\"wandb_key\")\n\nwandb.login(key=wandb_key)\nwandb.init(project=\"Generate-News\")","18d38238":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")","71870d41":"def load_data(file_path):\n    df = pd.read_json(file_path, lines = True)\n    df['category'] = pd.Categorical(df['category'])\n    return df[[\"short_description\", \"category\"]]","813a9200":"df = load_data('..\/input\/news-category-dataset\/News_Category_Dataset_v2.json')\ndf.head()","ca191280":"df = df[df[\"short_description\"] != \"\"]","658f68b8":"df.describe()","a19cf6d7":"def combine_categories(category):\n    # Convert to NEWS\n    if category == \"THE WORLDPOST\" or category == \"WORLDPOST\" or category == \"WORLD NEWS\":\n        return \"WORLD\"\n    \n    # CONVERT TO ARTS\n    if category == \"ARTS & CULTURE\" or category == \"CULTURE & ARTS\":\n        return \"ARTS\"\n\n    # Conver to ENTERTAINMENT\n    if category == \"COMEDY\" or category == \"SPORTS\":\n        return \"ENTERTAINMENT\"\n\n    # Conver to EMPOWERMENT\n    if category == \"BLACK VOICES\" or category == \"WOMEN\" or category == \"QUEER VOICES\" or category == \"LATINO VOICES\":\n        return \"EMPOWERMENT\"\n\n    # Convert to FOOD\n    if category == \"TASTE\" or category == \"FOOD & DRINK\":\n        return \"FOOD\"\n\n    # Convert to LIFESTYLE\n    if category == \"TRAVEL\" or category == \"MEDIA\" or category == \"TECH\" or category == \"RELIGION\" or category == \"SCIENCE\" or category == \"STYLE\" or category == \"HOME & LIVING\" or category == \"STYLE & BEAUTY\":\n        return \"LIFESTYLE\"\n    \n    # Convert to HEALTH\n    if category == \"HEALTHY LIVING\" or category == \"WELLNESS\":\n        return \"HEALTH\"\n\n    # Convert to MARRIAGE\n    if category == \"DIVORCE\" or category == \"WEDDINGS\":\n        return \"MARRIAGE\"\n\n    # Convert to BUSINESS\n    if category == \"MONEY\":\n        return \"BUSINESS\"\n\n    # Convert to ENVIRONMENT\n    if category == \"GREEN\":\n        return \"ENVIRONMENT\"\n\n    # Convert to EDUCATION\n    if category == \"COLLEGE\":\n        return \"EDUCATION\"\n\n    # Convert to PARENTING\n    if category == \"PARENTS\":\n        return \"PARENTING\"\n\n    # Convert to MISCELLANEOUS\n    if category == \"CRIME\" or category == \"WEIRD NEWS\" or category == \"IMPACT\" or category == \"GOOD NEWS\" or category == \"FIFTY\":\n        return \"MISCELLANEOUS\"\n    \n    return category","eaa83354":"df[\"category\"] = df[\"category\"].map(combine_categories)\ndf[\"category\"] = pd.Categorical(df[\"category\"])","2af89950":"def get_special_tokens(categories):\n    special_tokens = [\"<pad>\", \"<unk>\", \"<eos>\"]\n    for c in categories:\n        cat = \"<\" + c.lower() + \">\"\n        special_tokens.append(cat)\n\n    return special_tokens\nspecial_tokens = get_special_tokens(list(df[\"category\"].unique()))","aa53be3d":"print(special_tokens)","b88deb19":"def preprocess(text):\n    text = text.strip()\n    text = unidecode(text)\n    return text","f83533b0":"df[\"short_description\"] = df[\"short_description\"].apply(preprocess)","8cab037f":"for sent in df.iloc[:, 0]:\n    max_len = 0\n    curr_len = len(sent)\n    if curr_len > max_len:\n        max_len = curr_len\n\nprint(max_len)","197adc73":"def build_vocab(df):\n    text = df['short_description'].str.cat(sep=' ')\n    yield list(text)\n\nvocab = build_vocab_from_iterator(build_vocab(df), specials=special_tokens)\nvocab.set_default_index(vocab[\"<unk>\"])","f9257399":"vocab_size = len(vocab)\nprint(vocab_size)","5f3e31f7":"class TextDataset(torch.utils.data.Dataset):\n    def __init__(self, df, vocab, seq_len):\n         self.seq_len = seq_len\n         self.vocab = vocab\n         self.df = df\n    \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        sent = [\"<\" + self.df.iloc[idx, 1].lower() + \">\"]\n        sent += list(self.df.iloc[idx, 0])\n        sent.append(\"<eos>\")\n        sent = self.vocab(sent)\n        return (\n            torch.tensor(sent[:-1]), \n            torch.tensor(sent[1:])\n        )","04ddb55d":"dataset = TextDataset(df, vocab, 14)","ace8e211":"input_tokens, target_tokens = next(iter(dataset))","0a66e64d":"print(input_tokens)\nprint(vocab.lookup_tokens(input_tokens.cpu().numpy()))","9f47970c":"print(target_tokens)\nprint(vocab.lookup_tokens(target_tokens.cpu().numpy()))","fe8ea46c":"def collate_fn_pad(batch):\n    '''\n    Padds batch of variable length\n\n    note: it converts things ToTensor manually here since the ToTensor transform\n    assume it takes in images rather than arbitrary tensors.\n    '''\n\n    ## Get sequence lengths\n    lengths = torch.tensor([ t[0].shape[0] for t in batch ])\n    \n    ## Pad Inputs\n    inputs = [ t[0] for t in batch ]\n    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n\n    ## Pad Targets\n    targets = [ t[1] for t in batch ]\n    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n   \n    return inputs[torch.argsort(lengths, descending=True)], targets[torch.argsort(lengths, descending=True)], lengths.sort(descending=True)[0]","d87f5ade":"lookup_dict = {value: key for key, value in enumerate(np.unique(df[\"category\"]))}\nclass_sample_count = np.array([len(np.where(df[\"category\"] == t)[0]) for t in lookup_dict.keys()])\nweight = 1. \/ class_sample_count\nsamples_weight = np.array([weight[lookup_dict[t]] for t in df.loc[:, \"category\"]])\n\nsamples_weight = torch.from_numpy(samples_weight)\nsamples_weight = samples_weight.double()\nsampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight))","d0dbe91d":"class NewsRNN(nn.Module):\n    def __init__(self, vocab_size, hidden_size, num_layers, padding_idx=0, device=\"cpu\"):\n        super(NewsRNN, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.device = device\n\n        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=padding_idx)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, inputs, hidden, lengths):\n        batch_size, seq_len = inputs.size()\n\n        # Embed Output\n        out = self.embedding(inputs)\n\n        # Run through RNN\n        out = torch.nn.utils.rnn.pack_padded_sequence(out, lengths, batch_first=True)\n        out, hidden = self.lstm(out, hidden)\n\n        # Undo packing\n        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        \n        out = self.fc(out)\n        return out, hidden\n\n    def init_hidden(self, seq_len): \n        return(\n            torch.zeros(self.num_layers, seq_len, self.hidden_size).to(self.device),\n            torch.zeros(self.num_layers, seq_len, self.hidden_size).to(self.device)\n        )","e7fde499":"rnn_model = NewsRNN(vocab_size, 256, 3, device=DEVICE)\nrnn_model = rnn_model.to(DEVICE)","81a3f17b":"EPOCHS = 50\nBATCH_SIZE = 16","f4f4d127":"def predict(model, vocab, text, device, max_characters = 100):\n    model.eval()\n\n    with torch.no_grad():\n        characters = [text]\n        hidden, cell = model.init_hidden(len(characters))\n\n        while True and len(characters) <= max_characters:\n            x = torch.tensor(vocab(characters)).unsqueeze(0).to(device)\n            y_pred, (hidden, cell) = model(x, (hidden, cell), [len(characters)])\n\n            last_characters_logits = y_pred[0][-1]\n            prob = torch.nn.functional.softmax(last_characters_logits, dim=0).detach().cpu().numpy()\n            characters_index = np.random.choice(len(last_characters_logits), p=prob)\n            character = vocab.lookup_token(characters_index)\n            characters.append(character)\n            if character == \"<eos>\":\n                break\n        \n    return characters","d9810e95":"def train(dataset, model, sampler, device):\n\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_pad, \n                                                num_workers=(multiprocessing.cpu_count() - 1), \n                                                pin_memory=True, drop_last=True, sampler=sampler\n                                             )\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    pbar = tqdm()\n\n    text_table = wandb.Table(columns=[\"epoch\", \"text\"])\n\n    for epoch in range(EPOCHS):\n        model.train()\n\n        print(f\"Epoch: {epoch + 1} \/ {EPOCHS}\")\n        print(f\"=\" * 10)\n        pbar.reset(total=len(dataloader))\n\n\n        hidden, cell = model.init_hidden(BATCH_SIZE)\n        losses = []\n\n        for i, (inputs, targets, lengths) in enumerate(dataloader):\n            optimizer.zero_grad()\n\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            y_pred, (hidden, cell) = model(inputs, (hidden, cell), lengths)\n            loss = criterion(y_pred.transpose(1, 2), targets)\n\n            hidden = hidden.detach()\n            cell = cell.detach()\n\n            loss.backward()\n            optimizer.step()\n\n            losses.append(loss.item())\n\n            # Update tqdm\n            pbar.update()\n\n        losses = np.mean(losses)\n        tokens = predict(model, vocab, random.choice(special_tokens[3:]), device)\n        pred_sent = \"\".join(tokens)\n        text_table.add_data(epoch, pred_sent)\n        wandb.log({\"average_loss\": losses}, step=epoch)\n        print(f\"Loss: {losses} | Prediction: {pred_sent}\")\n\n    wandb.log({\"training_logs\": text_table})","6d9dc148":"train(dataset, rnn_model, sampler, DEVICE)","59bd4c88":"for cat in special_tokens[3:]:\n    tokens = predict(rnn_model, vocab, cat, DEVICE)\n    sentence = \"\".join(tokens)\n    print(sentence)","dfc14d25":"# Import dependencies","467229ec":"# Build a Weighted Data Sampler for the Dataloader","bf12c871":"# Download data from Kaggle","176c46da":"# Build vocabulary and dataset","39753c47":"# Training","7d1dd2fc":"# Create Model","1d153068":"# Preprocess text","36e1a2eb":"# Encode labels","b13512c5":"# Load data"}}