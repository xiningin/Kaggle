{"cell_type":{"d532fe6e":"code","6f84f72b":"code","d6629ec8":"code","4fb634bb":"code","73b26b13":"code","9acf3f03":"code","879e0746":"code","a79e224d":"code","d322673d":"code","4aff3c1e":"code","f49030f9":"code","1c172dae":"code","927059df":"code","c6ae21d3":"code","8710e1d2":"code","e8136da0":"code","a65152ba":"code","26d6f6ec":"code","cbd9005d":"code","323b8633":"code","336a36fe":"code","6437841d":"code","2af0b418":"code","242a6dfc":"code","846beaaf":"code","4f7ab156":"code","5ed341ef":"code","e95344f8":"code","776ed382":"code","f4bd2f63":"code","74077d25":"code","6e0f3583":"code","f7c0d2ba":"code","57080965":"code","71f15429":"code","456845be":"code","7146502d":"markdown","58bc3501":"markdown","625054e5":"markdown","1a188050":"markdown","ff2695a6":"markdown","fb418827":"markdown","ad9e87f8":"markdown","b7edb95b":"markdown","130428a4":"markdown","755f12db":"markdown","b7c7b5ac":"markdown","2e9d3a5f":"markdown","b9a4d089":"markdown","0957ed51":"markdown","5d9e1608":"markdown","6ee64517":"markdown","c3c73d30":"markdown","d44164bc":"markdown","822d4551":"markdown","01a2d124":"markdown","d4122ddd":"markdown","8b01f003":"markdown","447ba1d5":"markdown"},"source":{"d532fe6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.simplefilter('ignore')\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f84f72b":"from IPython.core.display import display, HTML, Javascript\n\n# ----- Notebook Theme -----\n\nnotebook_theme = 'carrot'\ncolor_maps = {'turquoise': ['#1abc9c', '#e8f8f5', '#d1f2eb', '#a3e4d7', '#76d7c4', '#48c9b0', '#1abc9c', '#17a589', '#148f77', '#117864', '#0e6251'], 'green': ['#16a085', '#e8f6f3', '#d0ece7', '#a2d9ce', '#73c6b6', '#45b39d', '#16a085', '#138d75', '#117a65', '#0e6655', '#0b5345'], 'emerald': ['#2ecc71', '#eafaf1', '#d5f5e3', '#abebc6', '#82e0aa', '#58d68d', '#2ecc71', '#28b463', '#239b56', '#1d8348', '#186a3b'], 'nephritis': ['#27ae60', '#e9f7ef', '#d4efdf', '#a9dfbf', '#7dcea0', '#52be80', '#27ae60', '#229954', '#1e8449', '#196f3d', '#145a32'], 'peter': ['#3498db', '#ebf5fb', '#d6eaf8', '#aed6f1', '#85c1e9', '#5dade2', '#3498db', '#2e86c1', '#2874a6', '#21618c', '#1b4f72'], 'belize': ['#2980b9', '#eaf2f8', '#d4e6f1', '#a9cce3', '#7fb3d5', '#5499c7', '#2980b9', '#2471a3', '#1f618d', '#1a5276', '#154360'], 'amethyst': ['#9b59b6', '#f5eef8', '#ebdef0', '#d7bde2', '#c39bd3', '#af7ac5', '#9b59b6', '#884ea0', '#76448a', '#633974', '#512e5f'], 'wisteria': ['#8e44ad', '#f4ecf7', '#e8daef', '#d2b4de', '#bb8fce', '#a569bd', '#8e44ad', '#7d3c98', '#6c3483', '#5b2c6f', '#4a235a'], 'wet': ['#34495e', '#ebedef', '#d6dbdf', '#aeb6bf', '#85929e', '#5d6d7e', '#34495e', '#2e4053', '#283747', '#212f3c', '#1b2631'], 'midnight': ['#2c3e50', '#eaecee', '#d5d8dc', '#abb2b9', '#808b96', '#566573', '#2c3e50', '#273746', '#212f3d', '#1c2833', '#17202a'], 'sunflower': ['#f1c40f', '#fef9e7', '#fcf3cf', '#f9e79f', '#f7dc6f', '#f4d03f', '#f1c40f', '#d4ac0d', '#b7950b', '#9a7d0a', '#7d6608'], 'orange': ['#f39c12', '#fef5e7', '#fdebd0', '#fad7a0', '#f8c471', '#f5b041', '#f39c12', '#d68910', '#b9770e', '#9c640c', '#7e5109'], 'carrot': ['#e67e22', '#fdf2e9', '#fae5d3', '#f5cba7', '#f0b27a', '#eb984e', '#e67e22', '#ca6f1e', '#af601a', '#935116', '#784212'], 'pumpkin': ['#d35400', '#fbeee6', '#f6ddcc', '#edbb99', '#e59866', '#dc7633', '#d35400', '#ba4a00', '#a04000', '#873600', '#6e2c00'], 'alizarin': ['#e74c3c', '#fdedec', '#fadbd8', '#f5b7b1', '#f1948a', '#ec7063', '#e74c3c', '#cb4335', '#b03a2e', '#943126', '#78281f'], 'pomegranate': ['#c0392b', '#f9ebea', '#f2d7d5', '#e6b0aa', '#d98880', '#cd6155', '#c0392b', '#a93226', '#922b21', '#7b241c', '#641e16'], 'clouds': ['#ecf0f1', '#fdfefe', '#fbfcfc', '#f7f9f9', '#f4f6f7', '#f0f3f4', '#ecf0f1', '#d0d3d4', '#b3b6b7', '#979a9a', '#7b7d7d'], 'silver': ['#bdc3c7', '#f8f9f9', '#f2f3f4', '#e5e7e9', '#d7dbdd', '#cacfd2', '#bdc3c7', '#a6acaf', '#909497', '#797d7f', '#626567'], 'concrete': ['#95a5a6', '#f4f6f6', '#eaeded', '#d5dbdb', '#bfc9ca', '#aab7b8', '#95a5a6', '#839192', '#717d7e', '#5f6a6a', '#4d5656'], 'asbestos': ['#7f8c8d', '#f2f4f4', '#e5e8e8', '#ccd1d1', '#b2babb', '#99a3a4', '#7f8c8d', '#707b7c', '#616a6b', '#515a5a', '#424949']}\n# color_maps = {'red': ['#f44336', '#ffebee', '#ffcdd2', '#ef9a9a', '#e57373', '#ef5350', '#f44336', '#e53935', '#d32f2f', '#c62828', '#b71c1c', '#ff8a80', '#ff5252', '#ff1744', '#d50000'], 'pink': ['#e91e63', '#fce4ec', '#f8bbd0', '#f48fb1', '#f06292', '#ec407a', '#e91e63', '#d81b60', '#c2185b', '#ad1457', '#880e4f', '#ff80ab', '#ff4081', '#f50057', '#c51162'], 'purple': ['#9c27b0', '#f3e5f5', '#e1bee7', '#ce93d8', '#ba68c8', '#ab47bc', '#9c27b0', '#8e24aa', '#7b1fa2', '#6a1b9a', '#4a148c', '#ea80fc', '#e040fb', '#d500f9', '#aa00ff'], 'deep': ['#673ab7', '#ede7f6', '#d1c4e9', '#b39ddb', '#9575cd', '#7e57c2', '#673ab7', '#5e35b1', '#512da8', '#4527a0', '#311b92', '#b388ff', '#7c4dff', '#651fff', '#6200ea', '#ff5722', '#fbe9e7', '#ffccbc', '#ffab91', '#ff8a65', '#ff7043', '#ff5722', '#f4511e', '#e64a19', '#d84315', '#bf360c', '#ff9e80', '#ff6e40', '#ff3d00', '#dd2c00'], 'indigo': ['#3f51b5', '#e8eaf6', '#c5cae9', '#9fa8da', '#7986cb', '#5c6bc0', '#3f51b5', '#3949ab', '#303f9f', '#283593', '#1a237e', '#8c9eff', '#536dfe', '#3d5afe', '#304ffe'], 'blue': ['#2196f3', '#e3f2fd', '#bbdefb', '#90caf9', '#64b5f6', '#42a5f5', '#2196f3', '#1e88e5', '#1976d2', '#1565c0', '#0d47a1', '#82b1ff', '#448aff', '#2979ff', '#2962ff', '#607d8b', '#eceff1', '#cfd8dc', '#b0bec5', '#90a4ae', '#78909c', '#607d8b', '#546e7a', '#455a64', '#37474f', '#263238'], 'light': ['#03a9f4', '#e1f5fe', '#b3e5fc', '#81d4fa', '#4fc3f7', '#29b6f6', '#03a9f4', '#039be5', '#0288d1', '#0277bd', '#01579b', '#80d8ff', '#40c4ff', '#00b0ff', '#0091ea', '#8bc34a', '#f1f8e9', '#dcedc8', '#c5e1a5', '#aed581', '#9ccc65', '#8bc34a', '#7cb342', '#689f38', '#558b2f', '#33691e', '#ccff90', '#b2ff59', '#76ff03', '#64dd17'], 'cyan': ['#00bcd4', '#e0f7fa', '#b2ebf2', '#80deea', '#4dd0e1', '#26c6da', '#00bcd4', '#00acc1', '#0097a7', '#00838f', '#006064', '#84ffff', '#18ffff', '#00e5ff', '#00b8d4'], 'teal': ['#009688', '#e0f2f1', '#b2dfdb', '#80cbc4', '#4db6ac', '#26a69a', '#009688', '#00897b', '#00796b', '#00695c', '#004d40', '#a7ffeb', '#64ffda', '#1de9b6', '#00bfa5'], 'green': ['#4caf50', '#e8f5e9', '#c8e6c9', '#a5d6a7', '#81c784', '#66bb6a', '#4caf50', '#43a047', '#388e3c', '#2e7d32', '#1b5e20', '#b9f6ca', '#69f0ae', '#00e676', '#00c853'], 'lime': ['#cddc39', '#f9fbe7', '#f0f4c3', '#e6ee9c', '#dce775', '#d4e157', '#cddc39', '#c0ca33', '#afb42b', '#9e9d24', '#827717', '#f4ff81', '#eeff41', '#c6ff00', '#aeea00'], 'yellow': ['#ffeb3b', '#fffde7', '#fff9c4', '#fff59d', '#fff176', '#ffee58', '#ffeb3b', '#fdd835', '#fbc02d', '#f9a825', '#f57f17', '#ffff8d', '#ffff00', '#ffea00', '#ffd600'], 'amber': ['#ffc107', '#fff8e1', '#ffecb3', '#ffe082', '#ffd54f', '#ffca28', '#ffc107', '#ffb300', '#ffa000', '#ff8f00', '#ff6f00', '#ffe57f', '#ffd740', '#ffc400', '#ffab00'], 'orange': ['#ff9800', '#fff3e0', '#ffe0b2', '#ffcc80', '#ffb74d', '#ffa726', '#ff9800', '#fb8c00', '#f57c00', '#ef6c00', '#e65100', '#ffd180', '#ffab40', '#ff9100', '#ff6d00'], 'brown': ['#795548', '#efebe9', '#d7ccc8', '#bcaaa4', '#a1887f', '#8d6e63', '#795548', '#6d4c41', '#5d4037', '#4e342e', '#3e2723'], 'grey': ['#9e9e9e', '#fafafa', '#f5f5f5', '#eeeeee', '#e0e0e0', '#bdbdbd', '#9e9e9e', '#757575', '#616161', '#424242', '#212121'], 'white': ['#ffffff'], 'black': ['#000000']}\n\ncolor_maps = {i: color_maps[i] for i in color_maps if i not in ['clouds', 'silver', 'concrete', 'asbestos', 'wet asphalt', 'midnight blue', 'wet']}\n\nCMAP = 'Oranges'\nprompt = '#1DBCCD'\nmain_color = '#E58F65' # color_maps[notebook_theme]\nstrong_main_color = '#EB9514' # = color_maps[notebook_theme] \ncustom_colors = [strong_main_color, main_color]\n\n# ----- Notebook Theme -----\n\nhtml_contents =\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <link rel=\"stylesheet\" href=\"https:\/\/www.w3schools.com\/w3css\/4\/w3.css\">\n        <link rel=\"stylesheet\" href=\"https:\/\/fonts.googleapis.com\/css?family=Raleway\">\n        <link rel=\"stylesheet\" href=\"https:\/\/fonts.googleapis.com\/css?family=Oswald\">\n        <link rel=\"stylesheet\" href=\"https:\/\/fonts.googleapis.com\/css?family=Open Sans\">\n        <link rel=\"stylesheet\" href=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/font-awesome\/4.7.0\/css\/font-awesome.min.css\">\n        <style>\n        .title-section{\n            font-family: \"Oswald\", Arial, sans-serif;\n            font-weight: bold;\n            color: \"#6A8CAF\";\n            letter-spacing: 6px;\n        }\n        hr { border: 1px solid #E58F65 !important;\n             color: #E58F65 !important;\n             background: #E58F65 !important;\n           }\n        body {\n            font-family: \"Open Sans\", sans-serif;\n            }        \n        <\/style>\n    <\/head>    \n<\/html>\n\"\"\"\n","d6629ec8":"from sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier\nimport optuna","4fb634bb":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import colors\nsns.set(rc={\"axes.facecolor\":\"#FFF9ED\",\"figure.facecolor\":\"#FFF9ED\"})\npallet = [\"#682F2F\", \"#9E726F\", \"#D6B2B1\", \"#B9C0C9\", \"#9F8A78\", \"#F3AB60\"]\ncmap = colors.ListedColormap([\"#682F2F\", \"#9E726F\", \"#D6B2B1\", \"#B9C0C9\", \"#9F8A78\", \"#F3AB60\"])\ncm = sns.light_palette('green', as_cmap=True)","73b26b13":"train = pd.read_csv(\"..\/input\/song-popularity-5-folds\/train_5_folds (3).csv\")\ntest = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\nss = pd.read_csv(r\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\n\n# train['isTrain'] = True\n# test['isTrain'] = False\n\n\ntt = pd.concat([train, test])\n","9acf3f03":"# Dataset Link - https:\/\/www.kaggle.com\/sahilrajpal121\/song-popularity-5-folds\n\n# from sklearn.model_selection import StratifiedKFold\n# train['kfold'] = -1\nFOLDS = 5\n# skf = StratifiedKFold(n_splits = FOLD)\n\n# for f, (train_idx, val_idx) in enumerate(skf.split(train, train['song_popularity'])):\n#     train.loc[val_idx, 'kfold'] = f\n# train.to_csv('train_5_folds.csv', index = False)","879e0746":"train.head().style.background_gradient(cmap=cm)","a79e224d":"#categorising the cols \n\nnum_cols = ['song_duration_ms',\n            'acousticness',\n            'danceability',\n            'energy',\n           'instrumentalness',\n             'liveness',\n            'loudness',\n           'speechiness',\n            'tempo',\n             'audio_valence'\n           ]\ncat_cols = ['key',\n           'audio_mode',\n            'time_signature',\n           ]\n\nFEATURES = num_cols + cat_cols","d322673d":"(train.song_popularity.value_counts() \/ len(train)) * 100","4aff3c1e":"nans = pd.DataFrame([train.isna().mean(), test.isna().mean()]).T\nnans = nans.rename(columns = {0:'train_missing', 1:'test_missing'})\n\nnans.query('train_missing > 0').plot(kind='barh', figsize=(10,8),cmap=cmap)\nplt.show()","f49030f9":"corr = train[FEATURES].corr()\n\nplt.figure(figsize=(12,10))\nsns.heatmap(corr, \n            mask = np.triu(corr), \n            annot=True,\n           cmap = cmap)\nplt.show()","1c172dae":"sns.histplot(train,x='key', hue='song_popularity');plt.show()\nsns.histplot(train,x='audio_mode', hue='song_popularity');plt.show()\nsns.histplot(train,x='time_signature', hue='song_popularity');plt.show()","927059df":"sns.pairplot(train, vars=num_cols, hue='song_popularity')\nplt.show()","c6ae21d3":"from sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler,PowerTransformer, FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder","8710e1d2":"def compare_transformation(data, col, box_cox=False):\n    fig, ax = plt.subplots(2,2,figsize=(12,10))\n    data_scaled = data.copy()\n    \n    sns.kdeplot(data=data, x=col, ax=ax[0][0],fill=True, color=pallet[0])\n    ax[0][0].legend(['original'])\n    \n    #for log-transformation\n    add = 0\n    if data[col].min() < 0:\n        add = -1 * data[col].min() + 1\n    elif data[col].min() == 0:\n        add = 1\n    data_scaled_inter = (data[col] + add).values.reshape(-1,1)\n    \n    data_scaled[col] = FunctionTransformer(np.log1p).fit_transform(data_scaled_inter)\n    sns.kdeplot(data=data_scaled, x=col, ax = ax[0][1],fill=True, color=pallet[5])\n    ax[0][1].legend(['log1p'])\n\n    \n    data_scaled = RobustScaler().fit_transform(data)\n    data_scaled = pd.DataFrame(data_scaled, columns = data.columns)\n    sns.kdeplot(data=data_scaled, x=col, ax=ax[1][0],fill=True, color=pallet[2])\n    ax[1][0].legend(['RobustScaler'])\n    \n    data_scaled = PowerTransformer('yeo-johnson').fit_transform(data)\n    data_scaled = pd.DataFrame(data_scaled, columns = data.columns)\n    sns.kdeplot(data=data_scaled, x=col, ax=ax[1][1],fill=True, color=pallet[3])\n    ax[1][1].legend(['yeo-johnson'])\n    \n\n#     looks awfully lot like yeo-johnson. This needs more work to transform so i am putting it out    \n    if box_cox:\n        ax = plt.figure(figsize=(3,3))\n        data_scaled_inter = PowerTransformer('box-cox').fit_transform(data_scaled_inter)\n        data_scaled[col] = data_scaled_inter\n    #     data_scaled = pd.DataFrame(data_scaled, columns = data.columns)\n        sns.kdeplot(data=data_scaled,x=col, ax=ax,fill=True, color=pallet[4], legend=False)\n        ax.set_xlabel(col)\n        ax.legend(['box-cox'])\n\n    \n#     looks like it provides same kind of normal distribution for every kind of columns \n#     so not gonna use this\n\n#     data_scaled = QuantileTransformer(output_distribution='normal').fit_transform(data)\n#     data_scaled = pd.DataFrame(data_scaled, columns = data.columns)\n#     sns.kdeplot(data=data_scaled, x=col, ax=ax[1][2],fill=True, color=pallet[1])\n#     ax[1][2].legend(['normal'])\n\n\n    plt.tight_layout(pad=1)","e8136da0":"compare_transformation(data=train, col='instrumentalness')\ncompare_transformation(data=train, col='audio_valence')\n","a65152ba":"subset = train.query('instrumentalness > 0.2')\nprint('instrumentalness > 0.2 value counts : \\n', subset.song_popularity.value_counts())\nsns.histplot(data=subset,x='instrumentalness',  hue='song_popularity')\nplt.show()","26d6f6ec":"\ndef preprocessing(\n                    data = None,\n                    test = None,\n                    num_pipe = [\n                    IterativeImputer(),\n                    PowerTransformer()\n                    ],\n                    cat_pipe = [\n                    KNNImputer(n_neighbors=1)\n                    ],\n                    num_cols = num_cols,\n                    cat_cols = cat_cols\n                 ):\n    '''\n    provide pipeline along with the columns you want to trasnform\n    \n    >>> X = preprocessing(\n                    data = train,\n                    test = None,\n                    num_pipe = [\n                    IterativeImputer(),\n                    PowerTransformer()\n                    ],\n                    cat_pipe = [\n                    KNNImputer(n_neighbors=1)\n                    ],\n                    num_cols = num_cols,\n                    cat_cols = cat_cols\n                 )\n    '''\n                    \n    \n    FEATURES = num_cols + cat_cols\n    \n    data_eng = data.copy()\n    num_pipeline = make_pipeline(*num_pipe)\n    cat_pipeline = make_pipeline(*cat_pipe)\n    if cat_cols:\n        data_eng[cat_cols] = data_eng[cat_cols].astype('object')\n\n    preprocessor = ColumnTransformer(\n            transformers=[\n                (\"num\", num_pipeline,num_cols),\n                (\"cat\", cat_pipeline, cat_cols)])\n    data_eng[FEATURES] = preprocessor.fit_transform(data_eng[FEATURES])\n    \n    # return test data too if passed\n    if not test.empty: \n        test_eng = test.copy()\n        test_eng[FEATURES] = preprocessor.transform(test_eng[FEATURES])\n        return data_eng,  test_eng\n    \n    return data_eng\n\ndef one_hot(data = None, test = None, cat_cols = cat_cols, remove_original = False):\n    '''\n    Pass the columns to one hot encode\n    \n    >>> data_ohe = ohe(data=data, cat_cols = ['col1', 'col2'])\n    \n    '''\n    X = data.copy()\n    X = data[cat_cols].astype('object')\n    X = pd.get_dummies(X)\n    \n    if remove_original == True:\n        data = data.drop(cat_cols, axis = 1)\n    X = pd.concat([data, X],axis=1)\n    \n    # return test data too if passed\n    if not test.empty: \n        X_test = test.copy()\n        X_test = test[cat_cols].astype('object')\n        X_test = pd.get_dummies(X_test)\n        if remove_original:\n            test = test.drop(cat_cols, axis=1)\n        X_test = pd.concat([test, X_test], axis = 1)\n        return X, X_test\n    return X","cbd9005d":"X_train, X_test = preprocessing(data=train,\n                                test = test,\n                                num_cols = num_cols,\n                                cat_cols = cat_cols,\n                                num_pipe=[\n                                    IterativeImputer(max_iter=50, tol=0.0001, random_state=42),\n                                    RobustScaler()\n                                    ],\n                                cat_pipe = [\n                                    IterativeImputer(max_iter=50, tol=0.0001, random_state=42)\n                                ]\n                               )\n# X_train, X_test = one_hot(data = X_train, cat_cols = cat_cols, remove_original=True, test = X_test)\n\n","323b8633":"\nFEATURES = [ c for c in X_train.columns if c not in ['id', 'kfold', 'song_popularity']]\nX_train.shape, X_test.shape # test is without kfold and target variable ","336a36fe":"y_train = pd.Series(X_train.song_popularity,name='song_popularity')\nX_train = X_train.drop('song_popularity', axis=1)","6437841d":"from imblearn.under_sampling import EditedNearestNeighbours\n\nprint(f'original dataset shape \\n{y_train.value_counts()}')\nX, y = EditedNearestNeighbours().fit_resample(X_train, y_train)\nprint(f'undersampled dataset shape \\n{y.value_counts()}')\n\n\n# upsampling and downsampling mix\n# from imblearn.combine import SMOTETomek \n# X, y = SMOTETomek(1).fit_resample(X_train, y_train)\n\n\n# from imblearn.under_sampling import TomekLinks\n# print(f'original dataset shape {y_train.value_counts()}')\n# tl = TomekLinks(sampling_strategy='majority')\n# X, y = tl.fit_resample(X_train,y_train)\n# print(f\" after undersampling {y.value_counts()}\")\n# # print('Removed indexes:', id_tl)\n","2af0b418":"from sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport optuna \nfrom optuna import Trial","242a6dfc":"\n# def objective(trial, X = X, y = y):\n#     X_train, X_val, y_train, y_val = train_test_split(X,y, stratify=y, test_size=0.3)\n    \n#     param_grid = {\n#         'n_estimators': trial.suggest_int('n_estimators',400,4000,400),\n#         'max_depth': trial.suggest_int('max_depth',6,10),\n#         'eta': trial.suggest_float('eta', 0.007,0.01),\n#         'subsample': trial.suggest_discrete_uniform('subsample', 0.2,0.9,0.1),\n#         'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n#         'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n# #         'tweedie_variance_power': trial.suggest_discrete_uniform('tweedie_variance_power', 1.0,2.0,0.1),\n#         'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n#         'gamma': trial.suggest_loguniform('gamma', 1e-4,1e4),\n#         'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4,1e4),\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4,1e4),\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4,1e4),\n#     }\n\n\n    \n#     reg = XGBClassifier(\n#         objective = 'binary:logistic',\n#         eval_metric = 'auc',\n#         # Split job for 4 CPUs\n#         n_jobs=-1,\n#         # Use set of params generated by param grid\n#         **param_grid\n#     )\n#     reg.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n#             eval_metric='auc',\n#             early_stopping_rounds=1000,\n#             verbose=100)\n    \n#     preds = reg.predict_proba(X_val)[:, 1]\n#     score = roc_auc_score(y_val, preds)\n\n#     return score","846beaaf":"\n# #study = optuna.create_study(direction='maximize',\n#        #                     sampler=optuna.samplers.TPESampler(),\n#                   #         study_name = 'XGB_Regressor')\n# #study.optimize(objective, timeout = 60 * 60 * 8)\n\n# trial = study.best_trial\n# print(f'best auc found : {trial.value}')\n\n# for key, value in trial.params.items():\n#     print(f'{key} : {value} ')\n# trial = study.best_trial\n# trial.params\n# params = trial.params","4f7ab156":"# params = {'n_estimators': 2400,\n#           'max_depth': 6,\n#           'eta': 0.008406239898559783,\n#           'subsample': 0.4,\n#           'colsample_bylevel': 0.6,\n#           'colsample_bytree': 0.6, 'booster': 'gbtree',\n#           'gamma': 0.02587615796708838,\n#           'min_child_weight': 36.101477481773,\n#           'reg_alpha': 16.05366218089325, 'reg_lambda': 0.003219637931050364\n#           }\n\n# previous xgboost params\n# params = {'n_estimators': 3200,\n#  'max_depth': 7,\n#  'eta': 0.009181726796297935,\n#  'subsample': 0.2,\n#  'colsample_bylevel': 0.7,\n#  'colsample_bytree': 0.7,\n#  'booster': 'dart',\n#  'gamma': 0.03613516166386254,\n#  'min_child_weight': 164.49101683474453,\n#  'reg_alpha': 0.20225046979146954,\n#  'reg_lambda': 2.427143979319499}\n\n\n# params = {'n_estimators': 4000,\n#  'max_depth': 8,\n#  'eta': 0.00768725844685428,\n#  'subsample': 0.8,\n#  'colsample_bylevel': 0.8,\n#  'colsample_bytree': 0.30000000000000004,\n#  'booster': 'dart',\n#  'gamma': 0.0014088419463299886,\n#  'min_child_weight': 0.07518866386055255,\n#  'reg_alpha': 0.3064609276192991,\n#  'reg_lambda': 0.0012029185701084482}\n# {'n_estimators': 3200,\n#  'max_depth': 7,\n#  'eta': 0.009181726796297935,\n#  'subsample': 0.2,\n#  'colsample_bylevel': 0.7,\n#  'colsample_bytree': 0.7,\n#  'booster': 'dart',\n#  'gamma': 0.03613516166386254,\n#  'min_child_weight': 164.49101683474453,\n#  'reg_alpha': 0.20225046979146954,\n#  'reg_lambda': 2.427143979319499}","5ed341ef":"params = {'n_estimators': 2800,\n          'max_depth': 7,\n          'eta': 0.008899648182521336,\n          'subsample': 0.9,\n          'colsample_bylevel': 0.8,\n          'colsample_bytree': 0.4,\n          'booster': 'dart',\n          'gamma': 0.002366945516947468,\n          'min_child_weight': 0.001112745672967798,\n          'reg_alpha': 0.01845422627018639,\n          'reg_lambda': 1.7347252057124811}","e95344f8":"# Adding back target column so we can split with \"kfold\" column\nX_train = pd.concat([X, y], axis=1)\nX_train.shape","776ed382":"X_train.head()","f4bd2f63":"final_test_preds = []\nfinal_valid_preds = {}\nscores = []\n\nfor fold in range(FOLDS):\n    xtrain = X_train[X_train['kfold'] != fold].reset_index(drop=True)\n    xvalid = X_train[X_train['kfold'] == fold].reset_index(drop=True)\n    xtest = X_test.copy()\n    \n    valid_id = xvalid.id.values.tolist()\n    \n    ytrain = xtrain['song_popularity'].values\n    yvalid = xvalid['song_popularity'].values\n    \n    model = XGBClassifier(**params, \n                        objective = 'binary:logistic',\n                        eval_metric = 'auc',\n                        n_jobs=-1\n#                         tree_method = 'gpu_hist',\n#                         predictor = 'gpu_predictor',\n                        \n                        )\n    model.fit(xtrain[FEATURES], ytrain,\n              eval_set = [(xvalid[FEATURES], yvalid)],\n              verbose=100, early_stopping_rounds=300)\n    preds_valid = model.predict_proba(xvalid[FEATURES])[:, 1]\n    preds_test = model.predict_proba(xtest[FEATURES])[:, 1]\n    \n    final_test_preds.append(preds_test)\n    final_valid_preds.update(dict(zip(valid_id, preds_valid)))\n    \n\n    score1 = roc_auc_score(yvalid, preds_valid)\n    scores.append(score1)\n    print(f\"fold {fold} auc : {score1}\")\n                             \n\n    \n                             \nprint(np.mean(scores))\nfinal_valid_pred1=pd.DataFrame.from_dict(final_valid_preds,orient='index').reset_index()\nfinal_valid_pred1.columns=['id',\"preds_1\"]\nfinal_valid_pred1.to_csv('train_pred_1.csv',index=False)\n\ny_test=np.mean(np.column_stack(final_test_preds),axis=1)\ntest_preds1=pd.DataFrame(y_test,columns=['test_preds1'])\ntest_preds1['id']=test_preds1.index\ntest_preds1.columns=['preds_1','id']\ntest_preds1.to_csv(\"test_preds1.csv\",index=False)                             \n\n","74077d25":"train_preds = pd.read_csv(r\".\/train_pred_1.csv\")\ntest_preds = pd.read_csv(r\".\/test_preds1.csv\")","6e0f3583":"test_preds","f7c0d2ba":"model = XGBClassifier(**params, \n        objective = 'binary:logistic',\n        eval_metric = 'auc',\n        n_jobs=-1)\n\nmodel.fit(X_train[FEATURES],y)\n","57080965":"ss['song_popularity'] = model.predict_proba(X_test[FEATURES])[:, 1]\nss['song_popularity'].value_counts()","71f15429":"ss.to_csv('submission.csv', index=False)","456845be":"import joblib\nxgb_joblib = joblib.dump(model, 'xgb_first.pkl')","7146502d":"From the figures above, there is **no obvious** relation of any feature with the target. Both of the classes are present at both the times","58bc3501":"`energy` and `loudness` are the most correlated feature","625054e5":"# Target Imbalance Percentage","1a188050":"**I have made a 5-fold dataset using `StratifiedKFold` code below.**\n\n**Additionally, I have saved the dataset with `kfold` into Kaggle Datasets for reproducibility.**\n\n**You can also import the dataset from here -> [Song Popularity 5 folds](https:\/\/www.kaggle.com\/sahilrajpal121\/song-popularity-5-folds)**","ff2695a6":"64 % of target belongs to negative class while 36 % belongs to positive class","fb418827":"Keeping the imbalanced target in mind, the distribution doesn't show any relation. Tough luck!","ad9e87f8":"### Tail of `instrumentalness` :-\n","b7edb95b":"If one hot encoding is used, we need to include those column names again","130428a4":"`instrumentalness` is an interesting take. It has very long tail and is concentrated in a very small range","755f12db":"# Distribution between the features ","b7c7b5ac":"# Imputation & Scaling","2e9d3a5f":"# Handling Imbalanced Data\n\nThere are two ways you can approach Imbalanced data :\n\n* Upsampling  : This method tries to create synthetic data for the minority class ( which is positive class in this dataset) just enough so that the number of instances for the target classes become fairly equal. This method tries to make copies of minority class which results in duplicate instances.\n\n\n* Downsampling : This method is the opposite of Upsampling. In Downsampling, we try to reduce the instances of majority class to match with the number of instances of minory class.\n\n\n**There are other methods which includes mixes the perks of both Upsampling and Downsampling. They usally perform better than individual techniques.** \n\nI have tried the individual techniques in different ways but they underperform compared to the ensemble sampling techniques. \n\n`imblearn` library is developed by `sklearn` core-developers for the specific use of dealing with imbalanced data.\n\nAs we saw earlier above, the target classes are nearly inseparable for all the features and there doesn't seem to be any obvious relation with the target. \n\nIt would possibly help if we could remove such instances indistinguishable features for which targets are different.\n\nFollowing are the some of the different resampling strategies from `imblearn` :\n![](https:\/\/imbalanced-learn.org\/stable\/_images\/sphx_glr_plot_comparison_under_sampling_004.png)","b9a4d089":"# KFold Cross-Validation","0957ed51":"## Plotting Different Scaling Techniques\n\nWe will be plotting different types of scaling techniques to see what kind of scaling technique will preserve the information and be robust to outliers","5d9e1608":"Both the test and train data has same columns from which values are missing and surprisingly enough, all of the columns are missing almost the same amount of value i.e **10%**","6ee64517":"# Reading and Peaking \ud83d\udc40","c3c73d30":"### Train it back on the full data before submitting","d44164bc":"# Check Missing Values","822d4551":"# Correlation","01a2d124":"# Finding Hyperparameters using Optuna","d4122ddd":"# Preprocessing Functions\n\nMaking Functions to pass list of methods to be performed.\n\nThese methods are to be performed on numerical features and categorical features separately.\n\nThe list of methods will act as a pipeline and thus, they will be executed sequentially.","8b01f003":"# Importing libraries","447ba1d5":"# Save the Model and submit "}}