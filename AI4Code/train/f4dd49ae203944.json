{"cell_type":{"f7c0db18":"code","eed05b74":"code","9152c5f6":"code","82a6d62b":"code","df320f13":"code","d1eab3f0":"code","47f9ab34":"code","c0ef9a9c":"code","ea066d1f":"code","fe6e5019":"code","fe99fa6d":"code","81e6a058":"code","f56e0141":"code","6d155ddf":"code","8e438d45":"code","560aa195":"code","93b68fa7":"code","2410b151":"code","5790fe26":"code","6beef919":"code","de2c38d0":"code","b3f19e31":"code","6a991cdc":"code","a06fb8c6":"code","a7b39c9c":"code","45766396":"code","aec3b096":"code","99d0b808":"code","a001f54c":"code","d564f719":"code","09c56c2f":"code","39692f53":"code","117323b6":"code","c25df665":"code","7d83c680":"code","78f261b8":"code","c8fb348d":"code","a0a2dd0f":"code","58e973f3":"code","47128d69":"code","3ec02666":"code","3f663dfa":"code","df601c1f":"code","ae512acb":"code","6fc3f7da":"code","109dfaba":"code","ca15521e":"code","0ae83eec":"code","fcfa144b":"code","d4d7ac5a":"code","5455502b":"code","0ab14b1f":"code","cdc7cfd3":"code","0e045a84":"code","a8c68f27":"code","048772bd":"code","95c1609d":"code","7a178017":"code","b38f7aaa":"code","854624cd":"code","c1962888":"code","85d4bdd7":"code","33ed7f27":"code","f4ce9f52":"code","ff17a46e":"code","ebad2e2c":"code","27460593":"code","44984021":"code","6fce145d":"code","b693f5f0":"markdown","7060845f":"markdown","0b0ce3f8":"markdown","3e02f13b":"markdown","ac55a5d0":"markdown","faff786a":"markdown","5c190163":"markdown","4162afbf":"markdown","3e32e91a":"markdown","aac37789":"markdown","394d5f8b":"markdown","23e49e33":"markdown","84e27a8c":"markdown","cc9a912e":"markdown","602d13ad":"markdown","1264d8c8":"markdown"},"source":{"f7c0db18":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","eed05b74":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","9152c5f6":"train_data=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ncombine=[train_data, test_data]","82a6d62b":"train_data.shape","df320f13":"train_data.head()","d1eab3f0":"train_data.info()","47f9ab34":"train_data.describe()","c0ef9a9c":"print(test_data.shape)\ntest_data.head()","ea066d1f":"test_data.info()","fe6e5019":"## check the missing value \ntrain_data.isnull().sum().sort_values(ascending=False)","fe99fa6d":"# for the test data missing value\ntest_data.isnull().sum().sort_values(ascending=False)","81e6a058":"plt.figure(figsize=(20, 10))\nsns.heatmap(train_data.corr(), cmap='viridis', annot=True)","f56e0141":"cat_feature=[feature for feature in train_data.columns if train_data[feature].dtypes=='O']\nprint(len(cat_feature))\ncat_feature","6d155ddf":"num_feature=[feature for feature in train_data.columns if train_data[feature].dtypes!='O']\nprint(len(num_feature))\nnum_feature","8e438d45":"features=['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']\nfig, ax=plt.subplots(1, 5, figsize=(20, 4))\nfor i in range(len(features)):\n    sns.countplot(data=train_data, x=features[i],hue='Survived', ax=ax[i] )","560aa195":"## for the numerical feature \ntrain_data.hist(column=num_feature, layout=(1,len(num_feature)), figsize=(20,4))","93b68fa7":"sns.pairplot(data=train_data , hue='Survived')","2410b151":"## check the table for plcass who have survied or not\ntrain_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","5790fe26":"## for the sex , sibsp, parch and embarked with survied \ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","6beef919":"train_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","de2c38d0":"train_data[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)\n","b3f19e31":"train_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)\n","6a991cdc":"## check the age feature with survived 0 or 1\nsns.FacetGrid(train_data, col='Survived').map(plt.hist, 'Age', bins=20)","a06fb8c6":"sns.FacetGrid(train_data, col='Survived', row='Pclass').map(plt.hist, 'Age', bins=20)","a7b39c9c":"sns.FacetGrid(train_data, col='Survived', row='Embarked').map(sns.barplot, 'Sex','Fare')","45766396":"## remove the ticket fetaure from the dataset becoz there is no impace of the ticket on the Survived feature \ntrain_data.drop('Ticket', axis=1, inplace=True)\ntest_data.drop('Ticket', axis=1, inplace=True)\n## not combine both train and test data\ncombine=[train_data, test_data]","aec3b096":"train_data.head()","99d0b808":"## now split the name by regular expression \nfor dataset in combine:\n    dataset['Title']=dataset.Name.str.extract('([A-Za-z]+)\\.', expand=False)\n    ","a001f54c":"train_data['Title'].value_counts()","d564f719":"## check for the sex feature\npd.crosstab(train_data['Title'], train_data['Sex'])","09c56c2f":"## in the above table some feature of the title feature have less number  of male and female condidate\nfor dataset in combine:\n    dataset['Title']=dataset['Title'].replace(['Capt','Col','Countess','Don','Dr','Jonkheer','Lady','Major','Rev','Sir', 'Done'], 'Rare')\n    dataset['Title']=dataset['Title'].replace(['Mlle','Ms'],'Miss')\n    dataset['Title']=dataset['Title'].replace('Mme', 'Mrs')\n    ","39692f53":"## now check the title with survived fetaure\ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","117323b6":"sns.countplot('Title', hue='Survived', data=train_data)","c25df665":"train_data.head()","7d83c680":"## nor drop the name and passengerid from the train data and name from the test data\ntrain_data.drop(['Name', 'PassengerId'], axis=1, inplace=True)\ntest_data.drop('Name', axis=1, inplace=True)\ncombine=[train_data, test_data]","78f261b8":"## check the null value\ntrain_data.Age.isnull().sum()","c8fb348d":"for dataset in combine:\n    dataset['Sex']=dataset['Sex'].map({'male': 0, 'female':1})\nsns.FacetGrid(train_data, col='Sex' , row='Pclass').map(plt.hist, 'Age', bins=20)    ","a0a2dd0f":"ages=np.zeros([2,3])\nages","58e973f3":"for dataset in combine:\n    for i in range(0,2):\n        for j in range(0,3):\n            guess_df=dataset[(dataset['Sex']==i) & (dataset['Pclass']==j+1)]['Age'].dropna()\n            age_guess=guess_df.median()\n            ages[i,j]=int(age_guess\/0.5+0.5)*0.5\n    for i in range(0,2):\n        for j in range(0,3):\n            dataset.loc[(dataset.Age.isnull()) & (dataset.Sex==i) & (dataset.Pclass==j+1), 'Age']=ages[i,j]\n    \n    dataset['Age']=dataset['Age'].astype(int)\n    \nprint(train_data.isnull().sum())    \ntrain_data.head()    \n    ","47128d69":"## a lot of null value in the cabin feature but we does not drop it from the data we take the first char of the string and nan value replace by N \nfor dataset in combine:\n    dataset['Cabin']=dataset['Cabin'].str[0]\n","3ec02666":"for dataset in combine:\n    dataset['Cabin']=dataset['Cabin'].replace(np.nan, 'N')\ntrain_data.head() ","3f663dfa":"mapping_cabin={'A':0,\"B\":1, 'C':2, \"D\":3, 'E':4, 'F':5,'G':6,'N':7,'T':8}\nfor dataset in combine:\n    dataset['Cabin']=dataset['Cabin'].map(mapping_cabin)\n","df601c1f":"sns.countplot(data=train_data, x='Cabin', hue='Survived')","ae512acb":"## now convert the age into range\ntrain_data['AgeRange']=pd.cut(train_data['Age'], 5)\ntrain_data[['AgeRange', 'Survived']].groupby(['AgeRange'], as_index=False).mean().sort_values(by='Survived', ascending=False)","6fc3f7da":"for dataset in combine:\n    dataset.loc[dataset['Age']<=16, 'Age']=0\n    dataset.loc[(dataset['Age']>16) & (dataset['Age']<=32), 'Age']=1\n    dataset.loc[(dataset['Age']>32) & (dataset['Age']<=48), 'Age']=2   \n    dataset.loc[(dataset['Age']>48) & (dataset['Age']<=64), 'Age']=3\n    dataset.loc[dataset['Age']>64, 'Age']=4\ntrain_data.Age.value_counts() ","109dfaba":"train_data.drop('AgeRange', axis=1, inplace=True)\ncombine=[train_data, test_data]","ca15521e":"## now combine the sibsp and parch  feature into a familySize feature\nfor dataset in combine:\n    dataset['Family_size']=dataset['Parch']+dataset['SibSp']+1\ntrain_data[['Family_size', 'Survived']].groupby(['Family_size'], as_index=False).mean().sort_values(by='Survived', ascending=False)   ","0ae83eec":"for dataset in combine:\n    dataset['IsAlone']=0\n    dataset.loc[dataset['Family_size']==1, 'IsAlone']=1\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()    ","fcfa144b":"## drop the parch , sibsp and family size from the traiin and test data\ntrain_data.drop(['SibSp', 'Parch', 'Family_size'], axis=1, inplace=True)\ntest_data.drop(['SibSp', 'Parch', 'Family_size'], axis=1, inplace=True)\ncombine=[train_data, test_data]","d4d7ac5a":"freq=train_data['Embarked'].mode()[0]\nfor dataset in combine:\n    dataset['Embarked']=dataset['Embarked'].fillna(freq)","5455502b":"for dataset in combine:\n    dataset['Embarked']=dataset['Embarked'].map({'S':0,'C':1, \"Q\":2})","0ab14b1f":"test_data['Fare'].fillna(test_data['Fare'].dropna().median(), inplace=True)\ntrain_data['FareBand']=pd.cut(train_data['Age'], 4)\ntrain_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='Survived', ascending=False)","cdc7cfd3":"for dataset in combine:\n    dataset.loc[dataset['Fare']<=1, 'Age']=0\n    dataset.loc[(dataset['Fare']>1) & (dataset['Fare']<=2), 'Age']=1\n    dataset.loc[(dataset['Fare']>2) & (dataset['Fare']<=3), 'Age']=2\n    dataset.loc[(dataset['Fare']>3) & (dataset['Fare']<=4), 'Age']=3\n    dataset['Fare']=dataset['Fare'].astype(int)\n","0e045a84":"train_data.drop('FareBand', axis=1, inplace=True)\ncombine=[train_data, test_data]","a8c68f27":"train_data.dtypes","048772bd":"mapping_title={'Master':1, 'Miss':2, 'Mr':3, 'Mrs':4, 'Rare':5}\nfor dataset in combine:\n    dataset['Title']=dataset['Title'].map(mapping_title)\n    dataset['Title']=dataset['Title'].fillna(0)\n    dataset['Title']=dataset['Title'].astype(int)\n    ","95c1609d":"train_data.head()","7a178017":"test_data.head()","b38f7aaa":"X_train=train_data.drop('Survived', axis=1)\ny_train=train_data['Survived']\nX_test=test_data.drop('PassengerId', axis=1).copy()\nX_train.shape, y_train.shape, X_test.shape","854624cd":"from sklearn.linear_model import LogisticRegression\nmodel1=LogisticRegression(max_iter=1000)\nmodel1.fit(X_train ,y_train)\ny_pred=model1.predict(X_test)\nacc_log=round(model1.score(X_train , y_train)*100, 2)\nacc_log","c1962888":"svm=SVC()\nsvm.fit(X_train ,y_train)\ny1_pred=svm.predict(X_test)\nacc_svc=round(svm.score(X_train,y_train)*100, 2)\nacc_svc","85d4bdd7":"knn=KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train ,y_train)\ny2_pred=knn.predict(X_test)\nacc_knn=round(knn.score(X_train,y_train)*100, 2)\nacc_knn","33ed7f27":"tree=DecisionTreeClassifier(max_depth=1000)\ntree.fit(X_train,y_train)\ny5_pred=tree.predict(X_test)\nacc_tree=round(tree.score(X_train ,y_train)*100, 2)\nacc_tree","f4ce9f52":"sgd=SGDClassifier()\nsgd.fit(X_train,y_train)\ny6_pred=sgd.predict(X_test)\nacc_sgd=round(sgd.score(X_train ,y_train)*100, 2)\nacc_sgd","ff17a46e":"random=RandomForestClassifier()\nrandom.fit(X_train,y_train)\ny7_pred=random.predict(X_test)\nacc_random=round(random.score(X_train ,y_train)*100, 2)\nacc_random","ebad2e2c":"boost=GradientBoostingClassifier(n_estimators=1000, learning_rate=0.12, max_depth=4,random_state=2)\nboost.fit(X_train,y_train)\ny8_pred=boost.predict(X_test)\nacc_boost=round(boost.score(X_train ,y_train)*100, 2)\nacc_boost","27460593":"import xgboost\nboost1=xgboost.XGBClassifier(n_estimators=1000,max_depth=5,learning_rate=0.2)\nboost1.fit(X_train ,y_train)\npred=boost1.predict(X_test)\nacc_boost1=round(boost1.score(X_train ,y_train)*100, 2)\nacc_boost1","44984021":"model=pd.DataFrame({\n    'Model':['Support vector machine','KNN','Decision tree classifier','SGDClassifier','XGBoost', \n             'Random Forest Classifier','GradientBoostingClassifier',], \n    'Score':[acc_svc, acc_knn, acc_tree, acc_sgd, acc_boost1,acc_random, acc_boost ]\n})\nmodel.sort_values(by='Score', ascending=False)","6fce145d":"result1=pd.DataFrame({'PassengerId':test_data['PassengerId'],'Survived':y7_pred })\nresult1.to_csv('Submission101.csv', index=False)","b693f5f0":"## Random Forest Classifier","7060845f":"## SGD Classifier","0b0ce3f8":"## Gradient boosting classifier","3e02f13b":"## Build the model","ac55a5d0":"### Decision tree , random forest and gradient boosting classifer have same score so choose one ","faff786a":"## Decision Tree Classifier","5c190163":"## Data analysis","4162afbf":"## Support vector classifier","3e32e91a":"## load the train and test data","aac37789":"## find the cat and num feature ","394d5f8b":"## KNN","23e49e33":"## test data","84e27a8c":"## train data","cc9a912e":"## Logistic regression","602d13ad":"## check the correlation\n","1264d8c8":"## XGBoost"}}