{"cell_type":{"bd606682":"code","c7fb63ef":"code","c827c56b":"code","7feaadc1":"code","8e87313a":"code","f2585f88":"code","1b71e129":"code","74242806":"code","c191d3cb":"code","b9df8b1c":"code","f00be2cc":"code","648d9f0c":"code","f0ced226":"code","ffad90a3":"code","21040cce":"code","6a72d194":"code","bbe04f54":"code","76686336":"code","7974ecee":"code","8d917cd1":"code","b5fb76f1":"code","3dc517b6":"code","d0ff9170":"code","6aeddbd4":"code","3693a557":"code","9f7755a5":"code","e27d2250":"code","c4dfa6d5":"code","4b460385":"markdown","49d21ca2":"markdown","2d7ab5a0":"markdown","6714251e":"markdown","82def7cc":"markdown","6660e6fd":"markdown","fb382bb3":"markdown","0a0255c5":"markdown","3b450e86":"markdown","8c174c34":"markdown","ef2379d7":"markdown"},"source":{"bd606682":"import tensorflow as tf","c7fb63ef":"tf.version","c827c56b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7feaadc1":"path = \"\/kaggle\/input\/digit-recognizer\/\"\ntrain = pd.read_csv(path + \"train.csv\")\nprint(train.shape)","8e87313a":"X_train = train.drop([\"label\"], axis =1).values.astype('float32')\nY_train = train[\"label\"].values.astype('int32')\ntest = pd.read_csv(path + \"test.csv\")\nX_test = test.values.astype('float32')\nprint(\"X_train_dim: \", X_train.shape,\"Y_train_dim: \", Y_train.shape, \"X_test_dim: \", X_test.shape)","f2585f88":"X_train.shape","1b71e129":"X_train = X_train.reshape((X_train.shape[0], 28, 28)) \nX_test = X_test.reshape((X_test.shape[0], 28, 28)) \nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0","74242806":"## Add one more dimension for color \nX_train = X_train.reshape((X_train.shape[0],28,28,1))\nX_test = X_test.reshape((X_test.shape[0],28, 28,1))\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")","c191d3cb":"import tensorflow as tf\nclass mcb(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs = {}):\n        if (logs.get('acc') > 0.999):\n            print(\"\\nReached a good accuracy\\n\")\n            self.model.stop_training = True\ncallbacks = mcb()","b9df8b1c":"from tensorflow.keras.utils import to_categorical\nY_train = to_categorical(Y_train)","f00be2cc":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dropout, Dense\nfrom tensorflow.keras.models import Sequential","648d9f0c":"model2 = Sequential([\n    Conv2D(64, (3,3), activation = 'relu', input_shape = (28,28,1), padding = 'same'),\n    BatchNormalization(),\n    \n    Conv2D(64, (3,3), activation= 'relu', padding = 'same'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size = (2,2)),\n    \n    Dropout(0.35),\n    \n    Conv2D(64, (3,3), activation = 'relu'),\n    BatchNormalization(),\n    \n    Conv2D(64 , (3,3), activation = 'relu', padding = 'same'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size = (2,2), strides = (2,2)),\n    \n    Dropout(0.35),\n    \n    Flatten(),\n    Dense(256, activation = 'relu'),\n    BatchNormalization(),\n    \n    Dropout(0.25),\n    \n    Dense(10, activation = 'sigmoid')\n])","f0ced226":"model2.compile(optimizer= 'adam', loss = 'categorical_crossentropy', metrics=['acc'])\nmodel2.summary()","ffad90a3":"from tensorflow.keras.utils import plot_model\nplot_model(model2, to_file='model2.png', show_shapes=True, show_layer_names=True)\nfrom IPython.display import Image\nImage(\"model2.png\")","21040cce":"modelt = model2.fit(X_train, Y_train,epochs =30, callbacks = [callbacks], validation_split = 0.1)","6a72d194":"model3 = Sequential([\n    Conv2D(64, (3,3), activation = 'relu', input_shape = (28,28,1), padding = 'same'),\n    BatchNormalization(),\n    \n    Conv2D(64, (3,3), activation= 'relu', padding = 'same'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size = (2,2)),\n    \n    Dropout(0.35),\n    \n    Conv2D(64, (3,3), activation = 'relu'),\n    BatchNormalization(),\n    \n    Conv2D(64 , (3,3), activation = 'relu', padding = 'same'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size = (2,2), strides = (2,2)),\n    \n    Dropout(0.35),\n    \n    Flatten(),\n    Dense(256, activation = 'relu'),\n    BatchNormalization(),\n    \n    Dropout(0.25),\n    \n    Dense(10, activation = 'sigmoid')\n])","bbe04f54":"model3.compile(loss = 'categorical_crossentropy', metrics = ['acc'], optimizer = 'adam')\nbatch_size = 128\nsteps_per_epoch = X_train.shape[0]\/\/batch_size","76686336":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n    rotation_range = 10,\n    zoom_range = 0.1,\n    width_shift_range = 0.1,\n    height_shift_range = 0.1\n)\ndatagen.fit(X_train)","7974ecee":"modelt2 = model3.fit(datagen.flow(X_train, Y_train, batch_size = 128), steps_per_epoch = steps_per_epoch, callbacks = [callbacks], epochs = 30)","8d917cd1":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(2,1)\nax[0].plot(modelt.history['acc'], color = 'b', label = \"Training_accuracy\")\nax[0].plot(modelt.history['val_acc'], color = 'r', label = \"Dev_accuracy\")\nlegend = ax[0].legend(loc = 'best', shadow = True)\n\nax[1].plot(modelt.history['loss'], color = 'b', label = \"Training_loss\")\nax[1].plot(modelt.history['val_loss'], color =  'r', label = \"Dev_loss\")\nlegend = ax[1].legend(loc = 'best', shadow = True)\n\nfig, ax = plt.subplots(2,1)\nax[0].plot(modelt2.history['acc'], color = 'b', label = \"Training accuracy\")\nax[0].legend(loc = 'best', shadow = True)\nax[1].plot(modelt2.history['loss'], color = 'r', label = \"Training loss\")\nax[1].legend(loc = 'best', shadow = True)","b5fb76f1":"y_pred = model3.predict(X_test, verbose = 1)","3dc517b6":"print(y_pred[0])","d0ff9170":"predictions=[]\nfor i in range(len(X_test)):\n    a=np.where(y_pred[i] == max(y_pred[i]))\n    predictions.append(a[0][0])","6aeddbd4":"len(X_test)","3693a557":"import matplotlib.pyplot as plt\nimport random\ni = random.randint(0,28000)\nplt.imshow(X_test[i].reshape(28,28), cmap = plt.get_cmap('gray'))\nplt.title(predictions[i])","9f7755a5":"# model.save(_________)\n# model.save_weights('model')","e27d2250":"import pandas as pd\ncounter = range(1, len(predictions) + 1)\nsolution = pd.DataFrame({\"ImageId\": counter, \"label\": list(predictions)})","c4dfa6d5":"solution.to_csv(\"digit_recognizer8.csv\", index = False)","4b460385":"We predict test set using our trained model","49d21ca2":"**For reshaping the input elements**","2d7ab5a0":"**For generating a csv file for submission**","6714251e":"Model is inspired from [here](https:\/\/www.kaggle.com\/sajaldeb25\/kaggle-digit-recognizer-my-best-model-cnn-999) with minor changes\n\nModel2 is for hypertuning the parameters( {train+dev}sets ), whereas model3 is for training the model with all training set while augmenting the data","82def7cc":"predictions is a list of our labels test images of size 28000","6660e6fd":"Here, we define our neural network","fb382bb3":"We implement callbacks here to get the minimum accuracy possible as there is every danger, the model may overfit if trained over and over too many times.","0a0255c5":"We can see overall model in the below picture","3b450e86":"We train upto 20 epochs but hope to be ended up soon","8c174c34":"*Kindly note that in above cell index doesnt work as it is a numpy array ,so **np.where()** is used*","ef2379d7":"**For saving Model**"}}