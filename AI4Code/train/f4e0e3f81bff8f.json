{"cell_type":{"a2a47d24":"code","2e20dd14":"code","088e07ee":"code","429df1bd":"code","6983cf6a":"code","bc25e416":"code","32e1afb8":"code","75adb7a3":"code","afc3502d":"code","1cb34cf8":"code","23d4aabd":"code","4928fc53":"code","5d5743e0":"code","e8606bbe":"code","31f4233d":"code","632675cd":"code","3b9b32a5":"code","03e25679":"code","49efae39":"code","a1f69930":"code","3ca8fe4b":"code","d5348a55":"code","fc32fdc1":"code","ee5d8661":"code","2f1a7e2b":"code","4db2cafb":"code","96f6b9ba":"code","4f4b863e":"code","b0d8060a":"code","cdc8911b":"code","dc94f9b3":"code","c8c54760":"code","72c19f48":"code","d10c15e2":"code","4940491c":"markdown","3e84059f":"markdown","4909ab2c":"markdown","06b7f592":"markdown","3fa2a208":"markdown","0901a4e2":"markdown","462ef24c":"markdown","44f84d80":"markdown","20550eaa":"markdown","b7c622db":"markdown","4fbf397c":"markdown","0b69dcc5":"markdown","3c50b69c":"markdown","9797e46a":"markdown"},"source":{"a2a47d24":"# !pip install 'kaggle-environments>=0.1.6' > \/dev\/null 2>&1\n","2e20dd14":"import numpy as np\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make","088e07ee":"class ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5, pair=[None, 'random']):\n        self.env = make('connectx', debug=False)\n        self.pair = [None, 'random']\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n\n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        if np.random.random() < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n    \n    \nclass CNN_model(nn.Module):\n\n    def __init__(self, h, w, outputs):\n\n        super(CNN_model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=2, stride=1)\n#         self.bn1 = nn.BatchNorm2d(16)\n#         self.conv2 = nn.Conv2d(16, 32, kernel_size=2, stride=1)\n#         self.bn2 = nn.BatchNorm2d(32)\n#         self.conv3 = nn.Conv2d(32, 32, kernel_size=2, stride=1)\n#         self.bn3 = nn.BatchNorm2d(32)\n\n        # Number of Linear input connections depends on output of conv2d layers\n        # and therefore the input image size, so compute it.\n        def conv2d_size_out(size, kernel_size = 2, stride = 1):\n            return (size - (kernel_size - 1) - 1) \/\/ stride  + 1\n        convw = conv2d_size_out(w)\n        convh = conv2d_size_out(h)\n        linear_input_size = convw * convh * 16\n        self.head = nn.Linear(linear_input_size, outputs)\n\n    # Called with either one element to determine next action, or a batch\n    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n    def forward(self, x):\n        # import pdb; pdb.set_trace()\n        x = F.relu(self.conv1(x))\n#         x = F.relu(self.bn2(self.conv2(x)))\n#         x = F.relu(self.bn3(self.conv3(x)))\n        return self.head(x.view(x.size(0), -1))\n\n\nclass DQN:\n    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.gamma = gamma\n        self.model = CNN_model(6,7, num_actions)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []} # The buffer\n        self.max_experiences = max_experiences\n        self.min_experiences = min_experiences\n\n    def predict(self, inputs):\n        return self.model(torch.from_numpy(inputs).float().view(-1, 1, 6, 7))\n\n    def train(self, TargetNet):\n        if len(self.experience['s']) < self.min_experiences:\n            # Only start the training process when we have enough experiences in the buffer\n            return 0\n\n        # Randomly select n experience in the buffer, n is batch-size\n        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n        states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n        actions = np.asarray([self.experience['a'][i] for i in ids])\n        rewards = np.asarray([self.experience['r'][i] for i in ids])\n\n        # Prepare labels for training process\n        states_next = np.asarray([self.preprocess(self.experience['s2'][i]) for i in ids])\n        dones = np.asarray([self.experience['done'][i] for i in ids])\n        value_next = np.max(TargetNet.predict(states_next).detach().numpy(), axis=1)\n        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n\n        actions = np.expand_dims(actions, axis=1)\n        actions_one_hot = torch.FloatTensor(self.batch_size, self.num_actions).zero_()\n        actions_one_hot = actions_one_hot.scatter_(1, torch.LongTensor(actions), 1)\n        selected_action_values = torch.sum(self.predict(states) * actions_one_hot, dim=1)\n        actual_values = torch.FloatTensor(actual_values)\n\n        self.optimizer.zero_grad()\n        loss = self.criterion(selected_action_values, actual_values)\n        loss.backward()\n        self.optimizer.step()\n\n    # Get an action by using epsilon-greedy\n    def get_action(self, state, epsilon):\n        if np.random.random() < epsilon:\n            return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0]))\n        else:\n            prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].detach().numpy()\n            for i in range(self.num_actions):\n                # \u3082\u3046\u3059\u3067\u306b\u57cb\u307e\u3063\u3066\u3044\u308bcell\u306f\u5bfe\u8c61\u5916\n                if state.board[i] != 0:\n                    prediction[i] = -1e7\n            return int(np.argmax(prediction))\n\n    # Method used to manage the buffer\n    def add_experience(self, exp):\n        if len(self.experience['s']) >= self.max_experiences:\n            for key in self.experience.keys():\n                self.experience[key].pop(0)\n        for key, value in exp.items():\n            self.experience[key].append(value)\n\n    def copy_weights(self, TrainNet):\n        self.model.load_state_dict(TrainNet.state_dict())\n\n    def save_weights(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def load_weights(self, path):\n        self.model.load_state_dict(torch.load(path))\n    \n    # Each state will consist of the board and the mark\n    # in the observations\n    def preprocess(self, state):\n        result = state.board[:]\n        \n        # I omitted state.mark from input. I'm guessing this feat is redundant\n#         result.append(state.mark)\n\n        return result","429df1bd":"\ndef set_direction(d, direction):\n    if direction == \"row\":\n        dx, dy = d, 0\n    elif direction == \"col\":\n        dx, dy = 0, d\n    elif direction == \"diag\":\n        dx, dy = d, d\n    elif direction == \"anti-diag\":\n        dx, dy = d, -1*d\n    return dx, dy \n\ndef count_seq(new_stone_loc, state,mark):\n    \"\"\"change state for each direction\"\"\"\n    ans = 0\n    i, j = new_stone_loc\n    for direction in [\"row\", \"col\", \"diag\", \"anti-diag\"]:\n        count_sequences = 0\n        for dir_ in [1, -1]:\n            for d in range(4):\n                try:\n                    dx, dy = set_direction(dir_*d,direction)\n                    if dx == 0 and dy == 0:\n                        continue\n                    elif state[i + dx, j + dy] == mark:\n                        count_sequences += 1\n                    else:\n                        break\n                except IndexError:\n                    break\n        ans = max(count_sequences, ans)\n    return ans","6983cf6a":"# np.zeros((6,7))","bc25e416":"def reward_coordination(obs, prev_obs):\n    # prev_observation\u3068observation\u3092\u6bd4\u8f03\u3057\u3066\n    # \u81ea\u5206\u306estone\u304c\u9023\u7d50\u3057\u3066\u3044\u308b\u304b\u3044\u306a\u304b\u3067reward\u3092\u5909\u66f4\u3059\u308b\u3002\n    # \u9023\u7d50\u78ba\u8a8d\u30e1\u30bd\u30c3\u30c9\n    # import pdb; pdb.set_trace()\n\n    obs_mat = np.array(obs.board).reshape(-1,7)\n    prev_obs_mat = np.array(prev_obs.board).reshape(-1,7)\n    new_stone_loc = np.where(obs_mat - prev_obs_mat == obs.mark)\n    out = count_seq(new_stone_loc, obs_mat, obs.mark)\n\n    return out","32e1afb8":"def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n    rewards = 0\n    iter_ = 0\n    done = False\n    observations = env.reset()\n    while not done:\n        # Using epsilon-greedy to get an action\n        action = TrainNet.get_action(observations, epsilon)\n\n        # Caching the information of current state\n        prev_observations = observations\n\n        # Take action\n        observations, reward, done, _ = env.step(action)\n\n        # Apply new rules\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n#             reward = -0.05 # Try to prevent the agent from taking a long move\n\n            # Try to promote the agent to \"struggle\" when playing against negamax agent\n            # as Magolor's (@magolor) idea\n            reward = 0.5\n#             reward = reward_coordination(observations, prev_observations) * 0.5\n\n        rewards += reward\n\n        # Adding experience into buffer\n        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n        TrainNet.add_experience(exp)\n\n        # Train the training model by using experiences in buffer and the target model\n        TrainNet.train(TargetNet)\n        iter_ += 1\n        if iter_ % copy_step == 0:\n            # Update the weights of the target model when reaching enough \"copy step\"\n            TargetNet.copy_weights(TrainNet)\n    return rewards","75adb7a3":"def rule_based(obs, conf):\n    def get_results(x, y, mark, multiplier):\n        \"\"\" get list of points, lowest cells and \"in air\" cells of a board[x][y] cell considering mark \"\"\"\n        # set board[x][y] as mark\n        board[x][y] = mark\n        results = []\n        # if some points in axis already found - axis blocked\n        blocked = [False, False, False, False]\n        # i is amount of marks required to add points\n        for i in range(conf.inarow, 2, -1):\n            # points\n            p = 0\n            # lowest cell\n            lc = 0\n            # \"in air\" points\n            ap = 0\n            # axis S -> N, only if one mark required for victory\n            if i == conf.inarow and blocked[0] is False:\n                (p, lc, ap, blocked[0]) = process_results(p, lc, ap,\n                              check_axis(mark, i, x, lambda z : z, y + inarow_m1, lambda z : z - 1))\n            # axis SW -> NE\n            if blocked[1] is False:\n                (p, lc, ap, blocked[1]) = process_results(p, lc, ap,\n                    check_axis(mark, i, x - inarow_m1, lambda z : z + 1, y + inarow_m1, lambda z : z - 1))\n            # axis E -> W\n            if blocked[2] is False:\n                (p, lc, ap, blocked[2]) = process_results(p, lc, ap,\n                    check_axis(mark, i, x + inarow_m1, lambda z : z - 1, y, lambda z : z))\n            # axis SE -> NW\n            if blocked[3] is False:\n                (p, lc, ap, blocked[3]) = process_results(p, lc, ap, \n                    check_axis(mark, i, x + inarow_m1, lambda z : z - 1, y + inarow_m1, lambda z : z - 1))\n            results.append((p * multiplier, lc, ap))\n        # restore board[x][y] original value\n        board[x][y] = 0\n        return results\n    \n    def check_axis(mark, inarow, x, x_fun, y, y_fun):\n        \"\"\" check axis (NE -> SW etc.) for lowest cell and amounts of points and \"in air\" cells \"\"\"\n        (x, y, axis_max_range) = get_x_y_and_axis_max_range(x, x_fun, y, y_fun)\n        zeros_allowed = conf.inarow - inarow\n        #lowest_cell = y\n        # lowest_cell calculation turned off\n        lowest_cell = 0\n        for i in range(axis_max_range):\n            x_temp = x\n            y_temp = y\n            zeros_remained = zeros_allowed\n            marks = 0\n            # amount of empty cells that are \"in air\" (don't have board bottom or mark under them)\n            in_air = 0\n            for j in range(conf.inarow):\n                if board[x_temp][y_temp] != mark and board[x_temp][y_temp] != 0:\n                    break\n                elif board[x_temp][y_temp] == mark:\n                    marks += 1\n                # board[x_temp][y_temp] is 0\n                else:\n                    zeros_remained -= 1\n                    if (y_temp + 1) < conf.rows and board[x_temp][y_temp + 1] == 0:\n                        in_air -= 1\n#                 if y_temp > lowest_cell:\n#                     lowest_cell = y_temp\n                if marks == inarow and zeros_remained == 0:\n                    return (sp, lowest_cell, in_air, True)\n                x_temp = x_fun(x_temp)\n                y_temp = y_fun(y_temp)\n                if y_temp < 0 or y_temp >= conf.rows or x_temp < 0 or x_temp >= conf.columns:\n                    return (0, 0, 0, False)\n            x = x_fun(x)\n            y = y_fun(y)\n        return (0, 0, 0, False)\n        \n    def get_x_y_and_axis_max_range(x, x_fun, y, y_fun):\n        \"\"\" set x and y inside board boundaries and get max range of axis \"\"\"\n        axis_max_range = conf.inarow\n        while y < 0 or y >= conf.rows or x < 0 or x >= conf.columns:\n            x = x_fun(x)\n            y = y_fun(y)\n            axis_max_range -= 1\n        return (x, y, axis_max_range)\n    \n    def process_results(p, lc, ap, axis_check_results):\n        \"\"\" process results of check_axis function, return lowest cell and sums of points and \"in air\" cells \"\"\"\n        (points, lowest_cell, in_air, blocked) = axis_check_results\n        if points > 0:\n            if lc < lowest_cell:\n                lc = lowest_cell\n            ap += in_air\n            p += points\n        return (p, lc, ap, blocked)\n    \n    def get_best_cell(best_cell, current_cell):\n        \"\"\" get best cell by comparing factors of cells \"\"\"\n        for i in range(len(current_cell[\"factors\"])):\n            # index 0 = points, 1 = lowest cell, 2 = \"in air\" cells\n            for j in range(3):\n                # if value of best cell factor is smaller than value of\n                # the same factor in the current cell\n                # best cell = current cell and break the loop,\n                # don't compare lower priority factors\n                if best_cell[\"factors\"][i][j] < current_cell[\"factors\"][i][j]:\n                    return current_cell\n                # if value of best cell factor is bigger than value of\n                # the same factor in the current cell\n                # break loop and don't compare lower priority factors\n                if best_cell[\"factors\"][i][j] > current_cell[\"factors\"][i][j]:\n                    return best_cell\n        return best_cell\n    \n    def get_factors(results):\n        \"\"\" get list of factors represented by results and ordered by priority from highest to lowest \"\"\"\n        factors = []\n        for i in range(conf.inarow - 2):\n            if i == 1:\n                # my checker in this cell means my victory two times\n                factors.append(results[0][0][i] if results[0][0][i][0] > st else (0, 0, 0))\n                # opponent's checker in this cell means my defeat two times\n                factors.append(results[0][1][i] if results[0][1][i][0] > st else (0, 0, 0))\n                # if there are results of a cell one row above current\n                if len(results) > 1:\n                    # opponent's checker in cell one row above current means my defeat two times\n                    factors.append(results[1][1][i] if -results[1][1][i][0] > st else (0, 0, 0))\n                    # my checker in cell one row above current means my victory two times\n                    factors.append(results[1][0][i] if -results[1][0][i][0] > st else (0, 0, 0))\n                else:\n                    for j in range(2):\n                        factors.append((0, 0, 0))\n            else:\n                for j in range(2):\n                    factors.append((0, 0, 0))\n                for j in range(2):\n                    factors.append((0, 0, 0))\n            # consider only if there is no \"in air\" cells\n            if results[0][1][i][2] == 0:\n                # placing opponent's checker in this cell means opponent's victory\n                factors.append(results[0][1][i])\n            else:\n                factors.append((0, 0, 0))\n            # placing my checker in this cell means my victory\n            factors.append(results[0][0][i])\n            # central column priority\n            factors.append((1 if i == 1 and shift == 0 else 0, 0, 0))\n            # if there are results of a cell one row above current\n            if len(results) > 1:\n                # opponent's checker in cell one row above current means my defeat\n                factors.append(results[1][1][i])\n                # my checker in cell one row above current means my victory\n                factors.append(results[1][0][i])\n            else:\n                for j in range(2):\n                    factors.append((0, 0, 0))\n        # if there are results of a cell two rows above current\n        if len(results) > 2:\n            for i in range(conf.inarow - 2):\n                # my checker in cell two rows above current means my victory\n                factors.append(results[2][0][i])\n                # opponent's checker in cell two rows above current means my defeat\n                factors.append(results[2][1][i])\n        else:\n            for i in range(conf.inarow - 2):\n                for j in range(2):\n                    factors.append((0, 0, 0))\n        return factors\n\n\n    # define my mark and opponent's mark\n    my_mark = obs.mark\n    opp_mark = 2 if my_mark == 1 else 1\n    \n    # define board as two dimensional array\n    board = []\n    for column in range(conf.columns):\n        board.append([])\n        for row in range(conf.rows):\n            board[column].append(obs.board[conf.columns * row + column])\n    \n    best_cell = None\n    board_center = conf.columns \/\/ 2\n    inarow_m1 = conf.inarow - 1\n    \n    # standard amount of points\n    sp = 1\n    # \"seven\" pattern threshold points\n    st = 1\n    \n    # start searching for best_cell from board center\n    x = board_center\n    \n    # shift to right or left from board center\n    shift = 0\n    \n    # searching for best_cell\n    while x >= 0 and x < conf.columns:\n        # find first empty cell starting from bottom of the column\n        y = conf.rows - 1\n        while y >= 0 and board[x][y] != 0:\n            y -= 1\n        # if column is not full\n        if y >= 0:\n            # results of current cell and cells above it\n            results = []\n            results.append((get_results(x, y, my_mark, 1), get_results(x, y, opp_mark, 1)))\n            # if possible, get results of a cell one row above current\n            if (y - 1) >= 0:\n                results.append((get_results(x, y - 1, my_mark, -1), get_results(x, y - 1, opp_mark, -1)))\n            # if possible, get results of a cell two rows above current\n            if (y - 2) >= 0:\n                results.append((get_results(x, y - 2, my_mark, 1), get_results(x, y - 2, opp_mark, 1)))\n            \n            # list of factors represented by results\n            # ordered by priority from highest to lowest\n            factors = get_factors(results)\n\n            # if best_cell is not yet found\n            if best_cell is None:\n                best_cell = {\n                    \"column\": x,\n                    \"factors\": factors\n                }\n            # compare values of factors in best cell and current cell\n            else:\n                current_cell = {\n                    \"column\": x,\n                    \"factors\": factors\n                }\n                best_cell = get_best_cell(best_cell, current_cell)\n                        \n        # shift x to right or left from board center\n        if shift >= 0: shift += 1\n        shift *= -1\n        x = board_center + shift\n\n    # return index of the best cell column\n    return best_cell[\"column\"]","afc3502d":"env = ConnectX(pair=[rule_based,\"negamax\"])","1cb34cf8":"gamma = 0.99\ncopy_step = 25\nhidden_units = [128, 128, 128, 128, 128]\nmax_experiences = 50000\nmin_experiences = 100\nbatch_size = 32\nlr = 1e-2\nepsilon = 0.95\ndecay = 0.999\nmin_epsilon = 0.05\nepisodes = 60000\n\nprecision = 7","23d4aabd":"num_states = env.observation_space.n + 1\nnum_actions = env.action_space.n\n\nall_total_rewards = np.empty(episodes)\nall_avg_rewards = np.empty(episodes) # Last 100 steps\nall_epsilons = np.empty(episodes)\n\n# Initialize models\nTrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\nTargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)","4928fc53":"import copy\n\n\npbar = tqdm(range(episodes))\nfor n in pbar:\n    epsilon = max(min_epsilon, epsilon * decay)\n    total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n    all_total_rewards[n] = total_reward\n    avg_reward = all_total_rewards[max(0, n - 100):(n + 1)].mean()\n    all_avg_rewards[n] = avg_reward\n    all_epsilons[n] = epsilon\n\n    pbar.set_postfix({\n        'episode reward': total_reward,\n        'avg (100 last) reward': avg_reward,\n        'epsilon': epsilon\n    })\n\n    if n % 10000 == 0:\n        epsilon = 0.999\n        TrainNet_adversarial = copy.deepcopy(TrainNet)\n        env = ConnectX(switch_prob=0.5, pair=[\"negamax\", TrainNet_adversarial])\n        range_st = n\/\/5000\n        range_ed = range_st + 5000\n        plt.plot(all_avg_rewards[range_st:range_ed])\n        plt.xlabel('Episode')\n        plt.ylabel('Avg rewards (100)')\n        plt.show()\n    if n % 10000 == 0:\n        epsilon = 0.999\n        TrainNet_adversarial = copy.deepcopy(TrainNet)\n        env = ConnectX(switch_prob=0.5, pair=[\"negamax\", rule_based])\n        range_st = n\/\/5000\n        range_ed = range_st + 5000\n        plt.plot(all_avg_rewards[range_st:range_ed])\n        plt.xlabel('Episode')\n        plt.ylabel('Avg rewards (100)')\n        plt.show()","5d5743e0":"#\u3000\u4e0a\u8a18\u7e70\u308a\u8fd4\u3059\u3002","e8606bbe":"all_avg_rewards","31f4233d":"plt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","632675cd":"TrainNet","3b9b32a5":"plt.plot(all_epsilons)\nplt.xlabel('Episode')\nplt.ylabel('Epsilon')\nplt.show()","03e25679":"TrainNet.save_weights('.\/weights.pth')","49efae39":"def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n    \"\"\"\n    Parameters\n    ----------\n    input_data : (\u30c7\u30fc\u30bf\u6570, \u30c1\u30e3\u30f3\u30cd\u30eb, \u9ad8\u3055, \u5e45)\u306e4\u6b21\u5143\u914d\u5217\u304b\u3089\u306a\u308b\u5165\u529b\u30c7\u30fc\u30bf\n    filter_h : \u30d5\u30a3\u30eb\u30bf\u30fc\u306e\u9ad8\u3055\n    filter_w : \u30d5\u30a3\u30eb\u30bf\u30fc\u306e\u5e45\n    stride : \u30b9\u30c8\u30e9\u30a4\u30c9\n    pad : \u30d1\u30c7\u30a3\u30f3\u30b0\n    Returns\n    -------\n    col : 2\u6b21\u5143\u914d\u5217\n    \"\"\"\n    N, C, H, W = input_data.shape\n    out_h = (H + 2*pad - filter_h)\/\/stride + 1\n    out_w = (W + 2*pad - filter_w)\/\/stride + 1\n\n    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n    for y in range(filter_h):\n        y_max = y + stride*out_h\n        for x in range(filter_w):\n            x_max = x + stride*out_w\n            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n    return col\ndef relu(tensor):\n    return np.where(tensor>0, tensor,0 )","a1f69930":"# TrainNet.model.conv1.weight.shape","3ca8fe4b":"in_h = 6; in_w = 7\n\nstride = 1;pad = 0; filter_h = 2;filter_w = 2\nboard = np.arange(42).reshape((1, 1, in_h, in_w))\nout_C = 16\nN, C, H, W = board.shape\nout_h = (H + 2*pad - filter_h)\/\/stride + 1\nout_w = (W + 2*pad - filter_w)\/\/stride + 1\noutput_cnn = np.zeros((N, out_C, out_h, out_w), dtype=np.float)","d5348a55":"# vectorized_board = im2col(board.reshape((1,1, in_h, in_w)),\n#           filter_h=filter_h, filter_w=filter_w, stride=stride, pad=pad) \n\n# conv_weight = TrainNet.model.conv1.weight[i].flatten().detach().numpy()\n# conv_bias = TrainNet.model.conv1.bias.flatten().detach().numpy()\n\n# for i in range(out_C):\n#     filter_ = conv_weight\n#     # 'filter_{} = np.array({}, dtype=np.float32)\\n'.format(i, filter_)\n#     \"out_elems_n = reshaped_board@filter_0.T\"\n#     # print(vectorized_board.shape, \":\", filter_0.detach().numpy().shape)\n#     vectorized_out = vectorized_board@filter_.reshape((-1, 1))\n#     vectorized_out = vectorized_out.reshape((out_h, out_w))\n#     output_cnn[:, i, :, :] = vectorized_out\n# output_cnn = output_cnn + conv_bias.reshape(1, out_C, 1, 1)","fc32fdc1":"# tensor_board = torch.FloatTensor(board.reshape((1, 1, 6, -1)))\n\n# out = TrainNet.model.head.weight.detach().numpy()@output_cnn.reshape(-1, 1)","ee5d8661":"# out.argmax()\n# out # myagent\u95a2\u6570\u306e\u6700\u7d42\u7684\u306a\u30ea\u30bf\u30fc\u30f3","2f1a7e2b":"# TrainNet.model.conv1(tensor_board).shape","4db2cafb":"# \u518d\u73fe\n# out = TrainNet.model.head.weight.detach().numpy()@output_cnn.reshape((-1, 1)) #+ out_bias.reshape(-1, 1)\n# out","96f6b9ba":"# # Write hidden layers\n# for i, (w, b) in enumerate(fc_layers[:-1]):\n#     my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n#     my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n# # Write output layer\n# my_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\n# my_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n","4f4b863e":"# print(my_agent)","b0d8060a":"# Create the agent\nmy_agent = '''def my_agent(observation, configuration):\n    import numpy as np\n\n'''\nmy_agent += f'''    in_h = {in_h}; in_w = {in_w}; stride = {stride};pad = {pad}; filter_h = {filter_h};filter_w = {filter_w}\n    board = np.array(observation.board[:])\n    board = board.reshape((1, 1, in_h, in_w))\n    out_C = {out_C}\n    N, C, H, W = board.shape\n    out_h = (H + 2*pad - filter_h)\/\/stride + 1\n    out_w = (W + 2*pad - filter_w)\/\/stride + 1\n    output_cnn = np.zeros((N, out_C, out_h, out_w), dtype=np.float)\\n'''\n\nimport inspect \ninner_method = \"\"\n# print('    ' + inspect.getsource(im2col))\nmethod_list = [im2col, relu]\nfor method in method_list:\n    for line in inspect.getsource(method).split('\\n'):\n        inner_method += \"    \" + line + \"\\n\"\nmy_agent += inner_method\n\n#f\"np.array({TrainNet.model.conv1.weight[i].flatten().detach().tolist()}, dtype=np.float32)\\n\"\n\nmy_agent += '''    vectorized_board = im2col(board.reshape((1,1, in_h, in_w)),\n          filter_h=filter_h, filter_w=filter_w, stride=stride, pad=pad)\\n'''\n\n\n\n# Ongoing development 6\/13\n#\u3053\u3053\u3092 numpy.array\u3068\u3057\u3066\u5909\u6570conv_weight\u306b\u6e21\u3057\u3066\u3042\u3052\u308b\n#\u3000\u8907\u6570CNN\u5c64\u5bfe\u5fdc\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b...TBD\nmy_agent += f\"    conv_weight = np.array({TrainNet.model.conv1.weight.detach().tolist()}, dtype=np.float32)\\n\"\nmy_agent += f\"    conv_bias = np.array({TrainNet.model.conv1.bias.flatten().detach().tolist()}, dtype=np.float32)\\n\"\n\n\nmy_agent += \"\"\"    for i in range(out_C):\n        filter_ = conv_weight[i].flatten()\n        vectorized_out = vectorized_board@filter_.reshape((-1, 1))\n        vectorized_out = vectorized_out.reshape((out_h, out_w))\n        output_cnn[:, i, :, :] = vectorized_out\n\n    output_cnn = output_cnn + conv_bias.reshape((1, out_C, 1, 1))\n    output_cnn = relu(output_cnn)\n\"\"\"\n\n\n\nmy_agent += f\"    out_weight = np.array({TrainNet.model.head.weight.detach().tolist()}, dtype=np.float32)\\n\"\nmy_agent += f\"    out_bias = np.array({TrainNet.model.head.bias.detach().tolist()}, dtype=np.float32)\\n\"\n\nmy_agent += \"\"\"    out = out_weight@output_cnn.reshape((-1, 1)) + out_bias.reshape((-1, 1))\\n\"\"\"\n# \u5404\u5217\u4e00\u756a\u4e0a\u306e\u30bb\u30eb\u304c\u7a7a\u3044\u3066\u3044\u306a\u304b\u3063\u305f\u5834\u5408\u3001\u305d\u3053\u306f\u9078\u3070\u306a\u3044\u3088\u3046\u306b\u3059\u308b\u3002\nmy_agent += '''\n    for i in range(configuration.columns):\n        if observation.board[i] != 0:\n            out[i] = -1e7\n    return int(np.argmax(out))\n    '''\n","cdc8911b":"with open('submission.py', 'w') as f:\n    f.write(my_agent)","dc94f9b3":"from submission import my_agent","c8c54760":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate agent's performance.\nprint(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\nprint(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=10)))\nprint(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))","72c19f48":"# evaluate(\"connectx\", [\"negamax\", \"random\"], num_episodes=10)\n# print(inspect.getsource(evaluate))","d10c15e2":"# env.run([\"random\", \"negamax\"])","4940491c":"## This kernel was extended from https:\/\/www.kaggle.com\/phunghieu\/connectx-with-deep-q-learning.\nThanks for your tutorial of DQN using pytorch and connectX!\n\nI modified it and introduced CNN as DQN model.\n\n- 4\/6\n    - introduced CNN and change rewards function.\n    \n- 4\/8 \n    - <font color=\"red\">I struggled with how to make \"my_agent\" method using CNN module. It is an ongoing task<\/font>\n- 6\/14 \n    - completed the implementation with 1 CNN layer!!!\n    - There might be some problems about the ability to win...\n\n- 6\/16\n    - I will modify this kernel as I can change multiple layers of CNN.\n    - Also I have to set reward functions when the last piece of the sequence is located in the middle of the sequence.","3e84059f":"<a class=\"anchor\" id=\"create_an_agent\"><\/a>\n# Create an agent\n[Back to Table of Contents](#ToC)\n\n- Let me remind the model as follows\n\n```py\nclass CNN_model(nn.Module):\n\n    def __init__(self, h, w, outputs):\n\n        super(CNN_model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=2, stride=1)\n\n        def conv2d_size_out(size, kernel_size = 2, stride = 1):\n            return (size - (kernel_size - 1) - 1) \/\/ stride  + 1\n        convw = conv2d_size_out(w)\n        convh = conv2d_size_out(h)\n        linear_input_size = convw * convh * 16\n        self.head = nn.Linear(linear_input_size, outputs)\n\n    # Called with either one element to determine next action, or a batch\n    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return self.head(x.view(x.size(0), -1))\n```","4909ab2c":"<a class=\"anchor\" id=\"save_weights\"><\/a>\n# Save weights\n[Back to Table of Contents](#ToC)","06b7f592":"<a class=\"anchor\" id=\"create_connectx_environment\"><\/a>\n# Create ConnectX environment\n[Back to Table of Contents](#ToC)","3fa2a208":"<a class=\"anchor\" id=\"configure_hyper_parameters\"><\/a>\n# Configure hyper-parameters\n[Back to Table of Contents](#ToC)","0901a4e2":"<a class=\"anchor\" id=\"install_libraries\"><\/a>\n# Install libraries\n[Back to Table of Contents](#ToC)","462ef24c":"# About Reinforcement Learning and Deep Q-Learning\n> \"Reinforcement learning is an area of machine learning that is focused on training agents to take certain actions at certain states from within an environment to maximize rewards. DQN (Deep Q-Net) is a reinforcement learning algorithm where a deep learning model is built to find the actions an agent can take at each state.\" \n\n","44f84d80":"<a class=\"anchor\" id=\"define_useful_classes\"><\/a>\n# Define useful classes\nNOTE: All classes here were copied from my previous [*kernel*](https:\/\/www.kaggle.com\/phunghieu\/connectx-with-deep-q-learning) and switched from using TF2.0 to PyTorch. If you prefer TF2.0, let check [ConnectX with Deep Q-Learning](https:\/\/www.kaggle.com\/phunghieu\/connectx-with-deep-q-learning) kernel.\n\n---\n[Back to Table of Contents](#ToC)","20550eaa":"<!--\n- Using pretrained models, I plan to make the two models battle and transfer learning-->\n<!--\n- why do we need TargetNet ?\n    - might be Critic module.\n- Can we implement RL adversarially ?\n     could be easily\n- DQN can be modified to CNN(20' 4\/7)\n-->\n","b7c622db":"<a class=\"anchor\" id=\"import_libraries\"><\/a>\n# Import libraries\n[Back to Table of Contents](#ToC)","4fbf397c":"<a class=\"anchor\" id=\"ToC\"><\/a>\n# Table of Contents\n1. [Install libraries](#install_libraries)\n1. [Import libraries](#import_libraries)\n1. [Define useful classes](#define_useful_classes)\n1. [Define helper-functions](#define_helper_functions)\n1. [Create ConnectX environment](#create_connectx_environment)\n1. [Configure hyper-parameters](#configure_hyper_parameters)\n1. [Train the agent](#train_the_agent)\n1. [Save weights](#save_weights)\n1. [Create an agent](#create_an_agent)\n1. [Evaluate the agent](#evaluate_the_agent)","0b69dcc5":"<a class=\"anchor\" id=\"evaluate_the_agent\"><\/a>\n# Evaluate the agent\n[Back to Table of Contents](#ToC)","3c50b69c":"<a class=\"anchor\" id=\"train_the_agent\"><\/a>\n# Train the agent\n[Back to Table of Contents](#ToC)","9797e46a":"<a class=\"anchor\" id=\"define_helper_functions\"><\/a>\n# Define helper-functions\n[Back to Table of Contents](#ToC)"}}