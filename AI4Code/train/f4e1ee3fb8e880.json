{"cell_type":{"f7d03900":"code","12d00286":"code","9a04c2d4":"code","c778fad9":"code","04bb5635":"code","a938a09c":"code","dbb2f724":"code","36ad3025":"code","82d5a402":"code","93652b82":"code","410aa79d":"code","cef95672":"code","6f6250ab":"code","b3875bce":"code","c01dc9bf":"code","e3b9c4e4":"code","46c6ea43":"code","f38f9573":"code","e4eaf720":"code","fccf8ce6":"code","9449481a":"code","08d37221":"code","b212dbb6":"code","b5dfe6e0":"code","c2b97835":"code","bc88bc3a":"code","82ffa756":"code","123be64c":"code","3e2c78a2":"code","7740fd23":"code","1934fab3":"code","3f1edfcd":"code","81d213e4":"code","0c851621":"code","8840259e":"code","34728bc4":"code","872f1e7c":"code","26aa9673":"code","6b742d5f":"code","fa693291":"code","b7e8e8ef":"code","2aad9219":"code","c91fdfd1":"code","b2f528fe":"code","88b85225":"markdown","b425d04c":"markdown","a37d41cf":"markdown","951dacd5":"markdown","9700c9b4":"markdown","a9f0a14d":"markdown","47a3ebf0":"markdown","438a91e4":"markdown","86c251da":"markdown","62ce3c04":"markdown","39ef1db2":"markdown","c9223b3c":"markdown","f6d7cc56":"markdown","85068f3a":"markdown","329d3273":"markdown","43372645":"markdown","543c2447":"markdown","2e84cc61":"markdown"},"source":{"f7d03900":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nimport lightgbm as lgb\nimport warnings\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns","12d00286":"full_train_orig = pd.read_csv(\"..\/Data\/application_train.csv\")\ntest =  pd.read_csv(\"..\/Data\/application_test.csv\")","9a04c2d4":"### Load bureau data\nbureau = pd.read_csv(\"..\/Data\/bureau.csv\")\nbureau_bal = pd.read_csv(\"..\/Data\/bureau_balance.csv\")\n\nbureau_bal_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n# for col in bb_cat:\n#     bb_aggregations[col] = ['mean']\nbb_agg = bureau_bal.groupby('SK_ID_BUREAU').agg(bureau_bal_aggregations)\nbb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n#bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n#bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n\nbureau_gb = bureau.groupby(\"SK_ID_CURR\").agg({'DAYS_CREDIT':['mean', 'max', 'min'], 'CREDIT_DAY_OVERDUE':['mean', 'max', 'min'],\n                                             'CNT_CREDIT_PROLONG': ['sum', 'max'], 'AMT_CREDIT_SUM':'mean',\n                                             'DAYS_CREDIT_UPDATE':['max', 'min'], 'AMT_CREDIT_SUM_OVERDUE':['mean'],\n                                             'SK_ID_BUREAU':'count'})\n# bureau_gb = bureau.groupby(\"SK_ID_CURR\")[[\"DAYS_CREDIT\", \"CREDIT_DAY_OVERDUE\", \"CNT_CREDIT_PROLONG\", \"DAYS_CREDIT_UPDATE\"\n#                              , \"AMT_CREDIT_SUM_OVERDUE\", \"AMT_CREDIT_SUM\"]].mean().add_suffix(\"_bur\").reset_index()\nbureau_gb.columns = ['_bureau_'.join(col) for col in bureau_gb.columns]\nbureau_gb = bureau_gb.reset_index()\n\n### OHE categorical features and combine\nbureau_cats = pd.get_dummies(bureau.select_dtypes('object').drop(\"CREDIT_CURRENCY\", axis=1))\nbureau_cats['SK_ID_CURR'] = bureau['SK_ID_CURR']\nbureau_cats_grouped = bureau_cats.groupby('SK_ID_CURR').agg('sum').reset_index()\nbureau_gb = pd.merge(bureau_gb, bureau_cats_grouped, on = 'SK_ID_CURR', how = 'left')","c778fad9":"### Load bureau data\n\n\n# bureau_gb = bureau.groupby(\"SK_ID_CURR\").agg({'DAYS_CREDIT':['mean', 'max', 'min'], 'CREDIT_DAY_OVERDUE':['mean', 'max', 'min'],\n#                                              'CNT_CREDIT_PROLONG': ['sum', 'max'], 'AMT_CREDIT_SUM':'mean',\n#                                              'DAYS_CREDIT_UPDATE':['max', 'min'], 'AMT_CREDIT_SUM_OVERDUE':['mean']})\n# # bureau_gb = bureau.groupby(\"SK_ID_CURR\")[[\"DAYS_CREDIT\", \"CREDIT_DAY_OVERDUE\", \"CNT_CREDIT_PROLONG\", \"DAYS_CREDIT_UPDATE\"\n# #                              , \"AMT_CREDIT_SUM_OVERDUE\", \"AMT_CREDIT_SUM\"]].mean().add_suffix(\"_bur\").reset_index()\n# bureau_gb.columns = ['_'.join(col) for col in bureau_gb.columns]\n# bureau_gb = bureau_gb.reset_index()\n\n# ### OHE categorical features and combine\n# bureau_cats = pd.get_dummies(bureau.select_dtypes('object'))\n# bureau_cats['SK_ID_CURR'] = bureau['SK_ID_CURR']\n# bureau_cats_grouped = bureau_cats.groupby('SK_ID_CURR').agg('sum').reset_index()\n# bureau_gb = pd.merge(bureau_gb, bureau_cats_grouped, on = 'SK_ID_CURR', how = 'left')","04bb5635":"### Load previous application data\nprv = pd.read_csv(\"..\/Data\/previous_application.csv\")\nprv['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\nprv['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\nprv['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\nprv['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\nprv['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\nprv['APP_CREDIT_PERC'] = prv['AMT_APPLICATION'] \/ prv['AMT_CREDIT']\nprv_gb = prv.groupby(\"SK_ID_CURR\").agg({\"AMT_ANNUITY\":['mean'], \"AMT_APPLICATION\":['mean'], \"AMT_CREDIT\":['mean'],\n                                        \"AMT_DOWN_PAYMENT\":['mean'], \"AMT_GOODS_PRICE\":['mean'], \"SELLERPLACE_AREA\":['mean'],\n                                        \"DAYS_DECISION\":['min', 'max'], \"DAYS_TERMINATION\":['min', 'max'], \"DAYS_LAST_DUE\":['min', 'max'],\n                                        \"DAYS_FIRST_DUE\":['min', 'max'], \"DAYS_LAST_DUE_1ST_VERSION\":['min', 'max'],\n                                        \"SK_ID_PREV\":['count']})\nprv_gb.columns = ['_prev_'.join(col) for col in prv_gb.columns]\nprv_gb = prv_gb.reset_index()\n\nprv_cats = pd.get_dummies(prv.select_dtypes('object').drop([\"NAME_TYPE_SUITE\", \"WEEKDAY_APPR_PROCESS_START\", \"NAME_CONTRACT_TYPE\"], axis=1))\nprv_cats['SK_ID_CURR'] = prv['SK_ID_CURR']\nprv_cats_grouped = prv_cats.groupby('SK_ID_CURR').agg('sum').reset_index()\nprv_gb = pd.merge(prv_gb, prv_cats_grouped, on = 'SK_ID_CURR', how = 'left')","a938a09c":"prv_gb.head(2)","dbb2f724":"### installment data \ninst = pd.read_csv(\"..\/Data\/installments_payments.csv\")\ninst[\"days_diff\"] = inst[\"DAYS_INSTALMENT\"] - inst[\"DAYS_ENTRY_PAYMENT\"]\ninst[\"amt_diff\"] = inst[\"AMT_INSTALMENT\"] - inst[\"AMT_PAYMENT\"]\ninst['PAYMENT_PERC'] = inst['AMT_PAYMENT'] \/ inst['AMT_INSTALMENT']\n# Days past due and days before due (no negative values)\ninst['DPD'] = inst['DAYS_ENTRY_PAYMENT'] - inst['DAYS_INSTALMENT']\ninst['DBD'] = inst['DAYS_INSTALMENT'] - inst['DAYS_ENTRY_PAYMENT']\ninst['DPD'] = inst['DPD'].apply(lambda x: x if x > 0 else 0)\ninst['DBD'] = inst['DBD'].apply(lambda x: x if x > 0 else 0)\n    \ninst_gb = inst.groupby(\"SK_ID_CURR\").agg({\"days_diff\":['mean', 'max', 'min'], \"amt_diff\":['mean', 'max', 'min'],\n                                        \"NUM_INSTALMENT_VERSION\":[ 'max', 'count'], \"NUM_INSTALMENT_NUMBER\":['max', 'count'], \n                                          \"SK_ID_PREV\":['count']})\ninst_gb.columns = ['_inst_'.join(col) for col in inst_gb.columns]\ninst_gb = inst_gb.reset_index()\n# inst_gb = inst.groupby(\"SK_ID_CURR\")[[\"AMT_INSTALMENT\",\"AMT_PAYMENT\"]].sum().add_suffix(\"_inst\").reset_index()\n# inst_gb[\"diff\"] = inst_gb[\"AMT_INSTALMENT_inst\"] - inst_gb[\"AMT_PAYMENT_inst\"]\n# inst_gb = inst_gb.drop([\"AMT_INSTALMENT_inst\", \"AMT_PAYMENT_inst\"], axis=1)","36ad3025":"inst_gb.head(2)","82d5a402":"### POS_Cash data\ncash = pd.read_csv(\"..\/Data\/POS_CASH_balance.csv\")\ncash_gb = cash.groupby(\"SK_ID_CURR\").agg({\"SK_ID_PREV\":['count']})\ncash_gb.columns = ['_cash_'.join(col) for col in cash_gb.columns]\ncash_gb = cash_gb.reset_index()\n\n# cash_cats = pd.get_dummies(cash.select_dtypes('object'))\n# cash_cats['SK_ID_CURR'] = cash['SK_ID_CURR']\n# cash_cats_grouped = cash_cats.groupby('SK_ID_CURR').agg('sum').reset_index()\n# cash_gb = pd.merge(cash_gb, cash_cats_grouped, on = 'SK_ID_CURR', how = 'left')\n\n# cash_cats = pd.get_dummies(cash.select_dtypes('object'))\n# cash_cats['SK_ID_CURR'] = cash['SK_ID_CURR']\n# cash_gb = cash_cats.groupby('SK_ID_CURR').agg('sum').reset_index()","93652b82":"cash_gb.head(2)","410aa79d":"### credit card balance data \nccb = pd.read_csv(\"..\/Data\/credit_card_balance.csv\")\nccb_gb = ccb.groupby(\"SK_ID_CURR\").agg(['min', 'max', 'mean', 'sum', 'var']).add_suffix(\"_ccb\")\nccb_gb.columns = ['_inst_'.join(col) for col in ccb_gb.columns]\nccb_gb = ccb_gb.reset_index()","cef95672":"### Additional hand crafted features\nfull_train_orig['LOAN_INCOME_RATIO'] = full_train_orig['AMT_CREDIT'] \/ full_train_orig['AMT_INCOME_TOTAL']\nfull_train_orig['ANNUITY_INCOME_RATIO'] = full_train_orig['AMT_ANNUITY'] \/ full_train_orig['AMT_INCOME_TOTAL']\nfull_train_orig['ANNUITY LENGTH'] = full_train_orig['AMT_CREDIT'] \/ full_train_orig['AMT_ANNUITY']\nfull_train_orig['WORKING_LIFE_RATIO'] = full_train_orig['DAYS_EMPLOYED'] \/ full_train_orig['DAYS_BIRTH']\nfull_train_orig['INCOME_PER_FAM'] = full_train_orig['AMT_INCOME_TOTAL'] \/ full_train_orig['CNT_FAM_MEMBERS']\nfull_train_orig['CHILDREN_RATIO'] = full_train_orig['CNT_CHILDREN'] \/ full_train_orig['CNT_FAM_MEMBERS']\n\nfull_train_orig['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\nfull_train_orig['DAYS_EMPLOYED_PERC'] = full_train_orig['DAYS_EMPLOYED'] \/ full_train_orig['DAYS_BIRTH']\nfull_train_orig['CNT_CHILDREN_outlier'] = (full_train_orig['CNT_CHILDREN'] > 6).astype(int)\nfor i in full_train_orig['CNT_CHILDREN']:\n    if i > 6:\n        full_train_orig['CNT_CHILDREN'].replace({i: np.nan}, inplace = True)\n        \n# full_train_orig['OWN_CAR_AGE_outlier'] = (full_train_orig['OWN_CAR_AGE'] > 60).astype(int)\n# for i in full_train_orig['OWN_CAR_AGE']:\n#     if i > 60:\n#         full_train_orig['OWN_CAR_AGE'].replace({i: np.nan}, inplace = True)\n\n# full_train_orig['CNT_FAM_MEMBERS_outlier'] = (full_train_orig['CNT_FAM_MEMBERS'] > 5).astype(int)\n# for i in full_train_orig['CNT_FAM_MEMBERS']:\n#     if i > 5:\n#         full_train_orig['CNT_FAM_MEMBERS'].replace({i: np.nan}, inplace = True)\n\nfull_train_orig['REGION_RATING_CLIENT_W_CITY'].map(lambda s: 1 if s == -1 else 0).sum()\nfor i in full_train_orig['REGION_RATING_CLIENT_W_CITY']:\n    if i == -1:\n        full_train_orig['REGION_RATING_CLIENT_W_CITY'].replace({i: 1}, inplace = True)\n\n# full_train_orig['OBS_30_CNT_SOCIAL_CIRCLE_outlier'] = (full_train_orig['OBS_30_CNT_SOCIAL_CIRCLE'] > 17).astype(int)\n# for i in full_train_orig['OBS_30_CNT_SOCIAL_CIRCLE']:\n#     if i > 17:\n#         full_train_orig['OBS_30_CNT_SOCIAL_CIRCLE'].replace({i: np.nan}, inplace = True)\n#full_train_orig['NEW_CREDIT_TO_GOODS_RATIO'] = full_train_orig['AMT_CREDIT'] \/ full_train_orig['AMT_GOODS_PRICE']\n#full_train_orig = full_train_orig.drop([\"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\"], axis=1)\n# full_train_orig['NEW_INC_PER_CHLD'] = full_train_orig['AMT_INCOME_TOTAL'] \/ (1 + full_train_orig['CNT_CHILDREN'])\n# full_train_orig['NEW_SOURCES_PROD'] = full_train_orig['EXT_SOURCE_1'] * full_train_orig['EXT_SOURCE_2'] * full_train_orig['EXT_SOURCE_3']\n# full_train_orig['NEW_EXT_SOURCES_MEAN'] = full_train_orig[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n# full_train_orig['NEW_SCORES_STD'] = full_train_orig[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n# full_train_orig['NEW_CAR_TO_BIRTH_RATIO'] = full_train_orig['OWN_CAR_AGE'] \/ full_train_orig['DAYS_BIRTH']\n# full_train_orig['NEW_CAR_TO_EMPLOY_RATIO'] = full_train_orig['OWN_CAR_AGE'] \/ full_train_orig['DAYS_EMPLOYED']\n# full_train_orig['NEW_PHONE_TO_BIRTH_RATIO'] = full_train_orig['DAYS_LAST_PHONE_CHANGE'] \/ full_train_orig['DAYS_BIRTH']\n# full_train_orig['NEW_PHONE_TO_EMPLOY_RATIO'] = full_train_orig['DAYS_LAST_PHONE_CHANGE'] \/ full_train_orig['DAYS_EMPLOYED']\n\n\ntest['LOAN_INCOME_RATIO'] = test['AMT_CREDIT'] \/ test['AMT_INCOME_TOTAL']\ntest['ANNUITY_INCOME_RATIO'] = test['AMT_ANNUITY'] \/ test['AMT_INCOME_TOTAL']\ntest['ANNUITY LENGTH'] = test['AMT_CREDIT'] \/ test['AMT_ANNUITY']\ntest['WORKING_LIFE_RATIO'] = test['DAYS_EMPLOYED'] \/ test['DAYS_BIRTH']\ntest['INCOME_PER_FAM'] = test['AMT_INCOME_TOTAL'] \/ test['CNT_FAM_MEMBERS']\ntest['CHILDREN_RATIO'] = test['CNT_CHILDREN'] \/ test['CNT_FAM_MEMBERS']\n\ntest['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\ntest['DAYS_EMPLOYED_PERC'] = test['DAYS_EMPLOYED'] \/ test['DAYS_BIRTH']\ntest['CNT_CHILDREN_outlier'] = (test['CNT_CHILDREN'] > 6).astype(int)\nfor i in test['CNT_CHILDREN']:\n    if i > 6:\n        test['CNT_CHILDREN'].replace({i: np.nan}, inplace = True)\n\n# test['OWN_CAR_AGE_outlier'] = (test['OWN_CAR_AGE'] > 60).astype(int)\n# for i in test['OWN_CAR_AGE']:\n#     if i > 60:\n#         test['OWN_CAR_AGE'].replace({i: np.nan}, inplace = True)\n\n# test['CNT_FAM_MEMBERS_outlier'] = (test['CNT_FAM_MEMBERS'] > 5).astype(int)\n# for i in test['CNT_FAM_MEMBERS']:\n#     if i > 5:\n#         test['CNT_FAM_MEMBERS'].replace({i: np.nan}, inplace = True)\n        \ntest['REGION_RATING_CLIENT_W_CITY'].map(lambda s: 1 if s == -1 else 0).sum()\nfor i in test['REGION_RATING_CLIENT_W_CITY']:\n    if i == -1:\n        test['REGION_RATING_CLIENT_W_CITY'].replace({i: 1}, inplace = True)\n        \n        \n# test['OBS_30_CNT_SOCIAL_CIRCLE_outlier'] = (test['OBS_30_CNT_SOCIAL_CIRCLE'] > 17).astype(int)\n# for i in test['OBS_30_CNT_SOCIAL_CIRCLE']:\n#     if i > 17:\n#         test['OBS_30_CNT_SOCIAL_CIRCLE'].replace({i: np.nan}, inplace = True)\n#test['NEW_CREDIT_TO_GOODS_RATIO'] = test['AMT_CREDIT'] \/ test['AMT_GOODS_PRICE']\n#test = test.drop([\"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\"], axis=1)\n# test['NEW_INC_PER_CHLD'] = test['AMT_INCOME_TOTAL'] \/ (1 + test['CNT_CHILDREN'])\n# test['NEW_SOURCES_PROD'] = test['EXT_SOURCE_1'] * test['EXT_SOURCE_2'] * test['EXT_SOURCE_3']\n# test['NEW_EXT_SOURCES_MEAN'] = test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n# test['NEW_SCORES_STD'] = test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n# test['NEW_CAR_TO_BIRTH_RATIO'] = test['OWN_CAR_AGE'] \/ test['DAYS_BIRTH']\n# test['NEW_CAR_TO_EMPLOY_RATIO'] = test['OWN_CAR_AGE'] \/ test['DAYS_EMPLOYED']\n# test['NEW_PHONE_TO_BIRTH_RATIO'] = test['DAYS_LAST_PHONE_CHANGE'] \/ test['DAYS_BIRTH']\n# test['NEW_PHONE_TO_EMPLOY_RATIO'] = test['DAYS_LAST_PHONE_CHANGE'] \/ test['DAYS_EMPLOYED']","6f6250ab":"full_train_orig = pd.merge(full_train_orig, bureau_gb, how=\"inner\", on=\"SK_ID_CURR\")\ntest = pd.merge(test, bureau_gb, how=\"left\", on=\"SK_ID_CURR\")","b3875bce":"full_train_orig.shape","c01dc9bf":"full_train_orig = pd.merge(full_train_orig, prv_gb, how=\"inner\", on=\"SK_ID_CURR\")\ntest = pd.merge(test, prv_gb, how=\"left\", on=\"SK_ID_CURR\")","e3b9c4e4":"full_train_orig = pd.merge(full_train_orig, inst_gb, how=\"inner\", on=\"SK_ID_CURR\")\ntest = pd.merge(test, inst_gb, how=\"left\", on=\"SK_ID_CURR\")","46c6ea43":"full_train_orig.shape","f38f9573":"full_train_orig = pd.merge(full_train_orig, ccb_gb, how=\"left\", on=\"SK_ID_CURR\")\ntest = pd.merge(test, ccb_gb, how=\"left\", on=\"SK_ID_CURR\")","e4eaf720":"full_train_orig.shape","fccf8ce6":"# full_train_orig = pd.merge(full_train_orig, cash_gb, how=\"inner\", on=\"SK_ID_CURR\")\n# test = pd.merge(test, cash_gb, how=\"left\", on=\"SK_ID_CURR\")","9449481a":"# '''\n# Removing columns with more than 100 null values (filters out 62 columns out of 122 columns)\n# And not useful (based on EDA) which removes reduces feature size to 43)\n# '''\n\n# null_columns = full_train_orig.columns[full_train_orig.isnull().sum().values > 100000].values.tolist()\n# correlated_columns = ['AMT_ANNUITY', 'AMT_GOODS_PRICE', 'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT_W_CITY',\n#                      'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE']\n# useless_columns = [ \"FLAG_DOCUMENT_2\", \"FLAG_DOCUMENT_4\", \"FLAG_DOCUMENT_5\", \"FLAG_DOCUMENT_7\"\n#                   ,'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',\n#        'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15',\n#        'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',\n#        'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']","08d37221":"### Manually removing columns which doesn't make sense (based on EDA)\n#full_train = full_train_orig.drop(null_columns+useless_columns, axis = 1)","b212dbb6":"full_train = full_train_orig","b5dfe6e0":"full_train_y = full_train.TARGET.values\nfull_train = full_train.drop([\"TARGET\"], axis = 1)\nfull_train = full_train.set_index(\"SK_ID_CURR\")\nnum_feats = full_train._get_numeric_data().columns.values.tolist()\ncat_feats = list(set(full_train.columns.values) - set(num_feats))","c2b97835":"## Categorical Features - Train\ntrain_cat= full_train[cat_feats]\ntrain_cat = pd.get_dummies(train_cat)\n\n## Numerical Features - Train\ntrain_num = full_train[num_feats]\n## Categorical Features - Test\ntest_cat = test[cat_feats]\ntest_cat = pd.get_dummies(test_cat)\n\n## Numerical Features - Test\ntest_num = test[num_feats]","bc88bc3a":"full_train_feats = pd.concat([train_num, train_cat], axis=1)\ntest_feats = pd.concat([test_num, test_cat], axis=1)","82ffa756":"full_train_feats = full_train_feats.fillna((full_train_feats.median()))\ntest_feats = test_feats.fillna(test_feats.median())","123be64c":"# full_train_feats = full_train_feats.apply(lambda x: x.fillna(x.mean()),axis=0)\n# test_feats = test_feats.apply(lambda x: x.fillna(x.mean()),axis=0)","3e2c78a2":"train_X, valid_X, train_y, valid_y = train_test_split(full_train_feats, full_train_y, train_size = 0.8, stratify=full_train_y, random_state=42)","7740fd23":"# ### RF classifier\n# params_rf={\n#     'max_depth': [20, 40, 60], #[3,4,5,6,7,8,9], # 5 is good but takes too long in kaggle env\n#     'n_estimators': [100, 300, 500], #[1000,2000,3000]\n# }\n\n# rf_clf = RandomForestClassifier()\n# rf = GridSearchCV(rf_clf,\n#                   params_rf,\n#                   cv=3,\n#                   scoring=\"roc_auc\",\n#                   n_jobs=1,\n#                   verbose=2)\n# rf.fit(train_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1), train_y)\n# best_est_rf = rf.best_estimator_\n# print(best_est)","1934fab3":"valid_probs_rf = rf.predict_proba(valid_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))[:,1]\nvalid_preds_rf = rf.predict(valid_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))","3f1edfcd":"print(accuracy_score(valid_y, valid_preds_rf))\nprint(roc_auc_score(valid_y, valid_probs_rf))","81d213e4":"list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist()))","0c851621":"# params={\n#     'max_depth': [3, 5], #[3,4,5,6,7,8,9], # 5 is good but takes too long in kaggle env\n#     'subsample': [0.6, 0.8], #[0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n#     'colsample_bytree': [0.5, 0.7], #[0.5,0.6,0.7,0.8],\n#     'n_estimators': [500, 700], #[1000,2000,3000]\n#     'reg_alpha': [0.1, 0.05],  #[0.01, 0.02, 0.03, 0.04]\n#     'scale_pos_weight':[3, 5]\n# }\n\n# xgb_clf = xgb.XGBClassifier(missing=9999999999)\n# rs = GridSearchCV(xgb_clf,\n#                   params,\n#                   cv=3,\n#                   scoring=\"roc_auc\",\n#                   n_jobs=1,\n#                   verbose=2)\n# rs.fit(train_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1), train_y)\n# best_est = rs.best_estimator_\n# print(best_est)","8840259e":"valid_probs_rs = rs.predict_proba(valid_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))[:,1]\nvalid_preds_rs= rs.predict(valid_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))[:,1]\nprint(accuracy_score(valid_y, valid_preds_rs))\nprint(roc_auc_score(valid_y, valid_probs_rs))","34728bc4":"xgb_single = xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n       min_child_weight=1, missing=9999999999, n_estimators=500,\n       nthread=-1, objective='binary:logistic', reg_alpha=0.05,\n       reg_lambda=1, scale_pos_weight=3, seed=0, silent=True,\n       subsample=0.8)\n\nxgb_single.fit(train_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1), train_y)\nvalid_probs_xgb_single = xgb_single.predict_proba(valid_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))[:,1]\nvalid_preds_xgb_single = xgb_single.predict(valid_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))\nprint(accuracy_score(valid_y, valid_preds_xgb_single))\nprint(roc_auc_score(valid_y, valid_probs_xgb_single))","872f1e7c":"### Train AUC\ntrain_probs_xgb_single = xgb_single.predict_proba(train_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))[:,1]\nprint(roc_auc_score(train_y, train_probs_xgb_single))","26aa9673":"xgb_single.fit(full_train_feats.drop(list(set(full_train_feats.columns.tolist()) - set(test_feats.columns.tolist())), axis=1), full_train_y)","6b742d5f":"params={\n    'max_depth': [3, 4, 5], #[3,4,5,6,7,8,9], # 5 is good but takes too long in kaggle env\n    'subsample': [0.4, 0.6], #[0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n    'colsample_bytree': [0.5, 0.7], #[0.5,0.6,0.7,0.8],\n    'n_estimators': [500, 700], #[1000,2000,3000]\n    'reg_alpha': [0.01, 0.05], #[0.01, 0.02, 0.03, 0.04]\n    'scale_pos_weight':[3, 5], \n    'num_leaves':[30, 50]\n    \n}\n\nlgb_clf = lgb.LGBMClassifier()\nrs = GridSearchCV(lgb_clf,\n                  params,\n                  cv=3,\n                  scoring=\"roc_auc\",\n                  n_jobs=1,\n                  verbose=1)\nrs.fit(train_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1), train_y)\nbest_est = rs.best_estimator_\nprint(best_est)","fa693291":"valid_probs_rs = rs.predict_proba(valid_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))[:,1]\nvalid_preds_rs= rs.predict(valid_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))\nprint(accuracy_score(valid_y, valid_preds_rs))\nprint(roc_auc_score(valid_y, valid_probs_rs))","b7e8e8ef":"rs.best_estimator_","2aad9219":"best_model = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.5,\n        learning_rate=0.1, max_depth=3, min_child_samples=20,\n        min_child_weight=0.001, min_split_gain=0.0, n_estimators=500,\n        n_jobs=-1, num_leaves=30, objective=None, random_state=None,\n        reg_alpha=0.05, reg_lambda=0.0, scale_pos_weight=3, silent=True,\n        subsample=0.4, subsample_for_bin=200000, subsample_freq=0)\n\nbest_model.fit(train_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1), train_y)\nvalid_probs_best = best_model.predict_proba(valid_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))[:,1]\nvalid_preds_best = best_model.predict(valid_X.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1))\nprint(accuracy_score(valid_y, valid_preds_best))\nprint(roc_auc_score(valid_y, valid_probs_best))","c91fdfd1":"best_model.fit(full_train_feats.drop(list(set(train_X.columns.tolist()) - set(test_feats.columns.tolist())), axis=1), full_train_y)","b2f528fe":"### Prepare submission file and save to disk\nresult_df = pd.DataFrame({'SK_ID_CURR':test.SK_ID_CURR.values, \"TARGET\":xgb_single.predict_proba(test_feats.drop(list(set(test_feats.columns.tolist()) - set(train_X.columns.tolist())), axis=1))[:,1]})\nresult_df.to_csv(\"test_submission.csv\", index=False)","88b85225":"# Prepare Submission file ","b425d04c":"# XGboost with Grid Search","a37d41cf":"## Load bureau dataset and aggregate per customer","951dacd5":"## Add some handcrafted features ","9700c9b4":"## Combine installment data","a9f0a14d":"## Load credit card balance data and aggregate per customer","47a3ebf0":"## Load installment dataset and aggregate per customer ","438a91e4":"## Combine credit card balance data ","86c251da":"## Combine bureau data","62ce3c04":"# Single XGBoost model with best parameters","39ef1db2":"## Load previous dataset and aggregate per customer","c9223b3c":"# Random Forest Classifier","f6d7cc56":"# LightGBM model ","85068f3a":"## combine pos_cash data","329d3273":"## Load main datasets ","43372645":"## Combine previous application data","543c2447":"## Load cash data and aggregate per customer","2e84cc61":"## Import Required Libraries "}}