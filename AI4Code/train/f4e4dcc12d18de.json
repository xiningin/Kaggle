{"cell_type":{"0e41a821":"code","59153f7a":"code","60c16d5a":"code","19175a00":"code","d9eb2c08":"code","09a43a0a":"code","5c75a4a3":"code","41e5e099":"code","ef57d5c9":"code","84306ac8":"code","d08efffc":"code","5f2429ef":"code","f913551d":"code","d9be4c95":"code","ea478f3e":"code","4f97d052":"code","7f6c966f":"code","f30694dc":"code","558c28c6":"code","db3bcf9f":"code","d0451446":"code","a8fecee3":"code","214e840e":"code","e777dd7a":"code","498d8a76":"code","72aa5505":"code","b50b31c0":"code","f8d92b1d":"code","84818536":"code","2d83fe63":"code","7910c5a5":"code","576df216":"code","53cbf173":"code","65ac2ded":"code","20ca0453":"code","6d8c6638":"code","b33faf26":"code","79ce86ae":"code","41226eda":"code","6b1e1774":"code","fb0c9b14":"code","8262848a":"code","fad4855f":"code","290d935b":"code","eae325ad":"code","e4b3b07b":"code","956996e6":"code","8a106d00":"code","45d13470":"code","732b7190":"code","4f65d6a8":"code","b2dd87fd":"code","6fabcc90":"code","648b0209":"code","8414e943":"code","1d5a3979":"code","d42f4f65":"code","05a96a09":"code","155e01c5":"code","336a5ebe":"code","d84633ed":"code","8e7a2027":"code","c8ca9d53":"code","94eaf5d2":"code","e08ddc13":"code","84448f44":"code","5c0bd072":"code","3f990ad3":"code","8af42a1c":"code","c3d68aa9":"code","1f94501b":"code","1176b860":"code","05aef884":"code","b9625c33":"code","a13fd066":"code","54f19d13":"code","0df7c195":"code","1d1e58dd":"code","d29d479a":"code","eeccf638":"code","10f54d4c":"code","9364ea5e":"code","40bc6de3":"code","27abe21d":"code","8bad4b1c":"code","8d87c9f0":"code","69726f06":"markdown","c7d108c9":"markdown","3d50ad09":"markdown","d91c65d6":"markdown","64ac00bd":"markdown","bf8e0ed4":"markdown","02901190":"markdown","18e853fa":"markdown","da141286":"markdown","6cf3f435":"markdown","97a31b0d":"markdown","ab026903":"markdown","2ed164e5":"markdown","6fa4bf06":"markdown","a36ffdbd":"markdown","c7156097":"markdown","446b85b3":"markdown","6669b1c1":"markdown","df270431":"markdown","2fadc519":"markdown","8ecd9e39":"markdown","579cd225":"markdown","fd6f5812":"markdown","85f4517b":"markdown","3ef6fddb":"markdown","5abf38ea":"markdown","ef231f8b":"markdown","1f968a99":"markdown","efb5ebc5":"markdown","4d934571":"markdown","621363a5":"markdown","e7fe0c75":"markdown","de0fae84":"markdown","4a8481e4":"markdown","c3b9056a":"markdown","fe77f1fb":"markdown","7c4bea75":"markdown","257d84de":"markdown","d56a4d82":"markdown","5ed9a531":"markdown","543c5d11":"markdown","4aa9ac83":"markdown","249228eb":"markdown","89c9da78":"markdown","2fc2f0a9":"markdown","65305bae":"markdown","9f8bfb9b":"markdown","b61ab7c7":"markdown","8bc6d945":"markdown","683acd88":"markdown","93f1141e":"markdown","78cf91d7":"markdown","33335fc9":"markdown","c25efd29":"markdown","19879bf1":"markdown","0236825d":"markdown","8148b6b6":"markdown","009b3163":"markdown","66194ef5":"markdown","af25fe53":"markdown","edfe8b0f":"markdown","3c0ce7be":"markdown","e81ffa16":"markdown","2dd54c3c":"markdown","9131850e":"markdown","babd6f34":"markdown","3d0e0851":"markdown","e643793e":"markdown","07b812ed":"markdown","71d43e2d":"markdown","b27d1a73":"markdown","0478d557":"markdown","9ede2a31":"markdown","d61a8188":"markdown","8ec3f1d9":"markdown","1e9ff431":"markdown","5eaaa1d4":"markdown","4c39f334":"markdown","bff1f443":"markdown","3889f98e":"markdown","1669b5b2":"markdown","be683913":"markdown","13936f4f":"markdown","874c4f78":"markdown","93d965e4":"markdown","2cca047a":"markdown","ea59a400":"markdown"},"source":{"0e41a821":"import numpy as np\nimport pandas as pd","59153f7a":"data = pd.read_csv(\".\/..\/input\/shootings\/shootings.csv\")\ndata.head()","60c16d5a":"data = data.drop(['Unnamed: 0', 'Incident.Number', 'Role'], axis=1)","19175a00":"import operator\n\ndef maxChar(x, l):\n    dic = {}\n    if x is np.nan:\n        x = \"None\"\n    for c in x.split(', '):\n        if c in dic:\n            dic[c] += 1\n        else:\n            dic[c] = 1\n    d = dict(sorted(dic.items(), key=lambda item: item[1]))\n    for i in d.keys():\n        if i in l:\n            return i\n    return l[0]\n\ndef maxNum(x):\n    try:\n        return int(x)\n    except:\n        return np.NaN","d9eb2c08":"data.Sex.unique()","09a43a0a":"import matplotlib.pyplot as plt\nplt.figure(figsize=(14,4))\nplt.xticks(rotation='vertical')\nplt.hist(data.Sex)\nplt.show()","5c75a4a3":"l = ['F','M','U']\ndata.Sex = data.Sex.apply(lambda x: maxChar(x, l))\ndata.Sex = data.Sex.astype('category')\ndata.Sex = data.Sex.cat.codes","41e5e099":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,4))\nplt.title(\"Sex Dist\")\nplt.xticks(rotation='vertical')\nplt.xlabel(\"Categor\u00eda\")\nplt.hist(data.Sex)\nplt.show()","ef57d5c9":"data.Race.unique()","84306ac8":"l = ['B','W','H','A','U']\ndata.Race = data.Race.apply(lambda x: maxChar(x, l))\ndata.Race = data.Race.astype('category')\ndata.Race = data.Race.cat.codes\ndata.Race.unique()","d08efffc":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,4))\nplt.title(\"Race Dist\")\nplt.xticks(rotation='vertical')\nplt.xlabel(\"Categor\u00eda\")\nplt.hist(data.Race)\nplt.show()","5f2429ef":"data.Injury.unique()","f913551d":"l = ['Wounded','Killed','None','Unknown']\ndata.Injury = data.Injury.apply(lambda x: maxChar(x, l))\ndata.Injury = data.Injury.astype('category')\ndata.Injury = data.Injury.cat.codes","d9be4c95":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,4))\nplt.title(\"Injury Dist\")\nplt.xticks(rotation='vertical')\nplt.xlabel(\"Categor\u00eda\")\nplt.hist(data.Injury)\nplt.show()","ea478f3e":"data['Suspect.Weapon'].unique()","4f97d052":"l = [\"Physical Force\",\"Firearm\", \"Hammer\", \"Realistic BB gun\",\"None\", \"Vehicle\", \"Unknown\",\"knife\",\"Metal Pole\",\"BB Gun\",\"Glass Shard\",\"Scissors\",\"Pen\",\"Toy Gun\"]\ndata['Suspect.Weapon'] = data['Suspect.Weapon'].apply(lambda x: maxChar(x, l))\ndata['Suspect.Weapon'] = data['Suspect.Weapon'].astype('category')\ndata['Suspect.Weapon'] = data['Suspect.Weapon'].cat.codes","7f6c966f":"import matplotlib.pyplot as plt\nplt.figure(figsize=(15,4))\nplt.title(\"Suspect Weapon Dist\")\nplt.xticks(rotation='vertical')\nplt.xlabel(\"Categor\u00eda\")\nplt.hist(data['Suspect.Weapon'])\nplt.show()","f30694dc":"from sklearn.impute import SimpleImputer\ndata.Age = data.Age.apply(lambda x: maxNum(x))\nimputerMean = SimpleImputer(missing_values=np.NaN, strategy='most_frequent')\ndata.Age = imputerMean.fit_transform(data.Age.values.reshape(-1,1))","558c28c6":"import datetime as dt\ndata['Incident.Date'] = pd.to_datetime(data['Incident.Date'])\ndata['Incident.Date.Ordinal']=data['Incident.Date'].map(dt.datetime.toordinal)","db3bcf9f":"data.isna().sum()\/len(data) * 100","d0451446":"data = data.dropna()","a8fecee3":"X = data[['Incident.Date.Ordinal', 'Sex', 'Race','Injury','lat','lon','Suspect.Weapon']] # Regresores\ny = data['Age'] # Variable a predecir","214e840e":"#Escalamos los datos\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\nX=X.copy()\nscaler = RobustScaler()\nX[[\"Incident.Date.Ordinal\",\"lat\",\"lon\"]] = scaler.fit_transform(X[[\"Incident.Date.Ordinal\",\"lat\",\"lon\"]])","e777dd7a":"X = pd.get_dummies(X, drop_first=True, columns=['Suspect.Weapon', 'Injury', 'Race', 'Sex'])","498d8a76":"X","72aa5505":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n\nsplits = 5\ncv = KFold(n_splits = splits)\n\nmean_r2 = 0\nmean_mae = 0\nmean_mse = 0\nitera = 0\nfor train_index, test_index in cv.split(X):\n    itera +=1\n    x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n    x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n    \n    #Declaramos modelo\n    model = LinearRegression()\n    model.fit(x_train, y_train)\n    mean_r2 = mean_r2 + model.score(x_test, y_test)\n    print(\"Iteraci\u00f3n:\", itera, \"\\n-----------------------\")\n    print(\"R2:\", model.score(x_test, y_test))\n    y_pred = model.predict(x_test)\n    mean_mae += mean_absolute_error(y_test,y_pred)\n    mean_mse += mean_squared_error(y_test, y_pred)\n    print(\"MSE:\", mean_squared_error(y_test, y_pred))\n    print(\"MAE:\", mean_absolute_error(y_test,y_pred))\n    print(\"-----------------------\")\n    \n#Obtenemos la media de los resultados basados en la media\nprint(\"\\n***** Resultados ********\")\nprint(\"R2:\", np.round(mean_r2 \/ splits, 2))\nprint(\"MAE:\", np.round(mean_mae \/ splits, 2))\nprint(\"MSE:\", np.round(mean_mse \/ splits, 2))","b50b31c0":"pd.DataFrame(model.coef_,X.columns, columns=['Coeff']).transpose()","f8d92b1d":"from sklearn.svm import LinearSVR\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\n\nsplits = 5\ncv = KFold(n_splits = splits)\nshape = (7, 7)\nmae = np.empty(shape)\nmse = np.empty(shape)\nr2 = np.empty(shape)\n\nfor i,c in enumerate([1,10,20,50,70,100,1000]):\n    for j,t in enumerate([10,1,1e-1,1e-2,1e-5,1e-10,1e-20]):\n        mean_r2 = 0\n        mean_mae = 0\n        mean_mse = 0\n        for train_index, test_index in cv.split(X):\n            x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n            x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n            #Declaramos modelo\n            model = LinearSVR(random_state = 0 ,tol = t, C = c, max_iter = 1000) # random_state para hacerlo reproducible\n            model.fit(x_train, y_train)\n            mean_r2 = mean_r2 + model.score(x_test,y_test)\n            y_pred = model.predict(x_test)\n            mean_mae += mean_absolute_error(y_test,y_pred)\n            mean_mse += mean_squared_error(y_test, y_pred)\n        \n        mae[i][j] = np.round(mean_mae \/ splits, 2)\n        r2[i][j] = np.round(mean_r2 \/ splits, 2)\n        mse[i][j] = np.round(mean_mse \/ splits, 2)\nmae = pd.DataFrame(mae, columns=['1','10','20','50','70','100','1000'])\nr2 = pd.DataFrame(r2, columns=['1','10','20','50','70','100','1000'])\nmse = pd.DataFrame(mse, columns=['1','10','20','50','70','100','1000'])\n\nmae = mae.set_index([pd.Index([10,1,1e-1,1e-2,1e-5,1e-10,1e-20])])\nmse = mse.set_index([pd.Index([10,1,1e-1,1e-2,1e-5,1e-10,1e-20])])\nr2 = r2.set_index([pd.Index([10,1,1e-1,1e-2,1e-5,1e-10,1e-20])])","84818536":"r2","2d83fe63":"mae","7910c5a5":"mse","576df216":"from sklearn.svm import SVR\n\nsplits = 5\ncv = KFold(n_splits = splits, shuffle = True) \n\n#Variable acumuladora\nsplits = 5\ncv = KFold(n_splits = splits)\nshape = (3, 4)\nres = np.empty(shape)\n\nfor j,k in enumerate(['linear','poly','rbf','sigmoid']):\n    mean_r2 = 0\n    mean_mae = 0\n    mean_mse = 0\n    for train_index, test_index in cv.split(X):\n        x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n        x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n        #Declaramos modelo\n        model = SVR(max_iter=1000, kernel=k)\n        model.fit(x_train, y_train)\n        \n        mean_r2 = mean_r2 + model.score(x_test,y_test)\n        y_pred = model.predict(x_test)\n        mean_mae += mean_absolute_error(y_test,y_pred)\n        mean_mse += mean_squared_error(y_test, y_pred)\n        model.fit(x_train, y_train)\n\n    res[0][j] = np.round(mean_r2\/splits, 2)\n    res[1][j] = np.round(mean_mae\/splits, 2)\n    res[2][j] = np.round(mean_mse\/splits, 2)\n\nres = pd.DataFrame(res, columns=['linear','poly','rbf','sigmoid'])\nres = res.set_index([pd.Index(['R2','MAE','MSE'])])\nres","53cbf173":"from sklearn.ensemble import RandomForestRegressor\n\nsplits = 5\ncv = KFold(n_splits = splits)\nshape = (3, 6)\nres = np.empty(shape)\n\nfor j,k in enumerate([1,2,5,10,20,25]):\n    mean_r2 = 0\n    mean_mae = 0\n    mean_mse = 0\n    for train_index, test_index in cv.split(X):\n        x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n        x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n        #Declaramos modelo\n        model = RandomForestRegressor(criterion=\"mse\" ,max_depth = k, bootstrap=True, random_state=0)\n        model.fit(x_train, y_train)\n        \n        mean_r2 = mean_r2 + model.score(x_test,y_test)\n        y_pred = model.predict(x_test)\n        mean_mae += mean_absolute_error(y_test,y_pred)\n        mean_mse += mean_squared_error(y_test, y_pred)\n        model.fit(x_train, y_train)\n\n    res[0][j] = np.round(mean_r2\/splits, 2)\n    res[1][j] = np.round(mean_mae\/splits, 2)\n    res[2][j] = np.round(mean_mse\/splits, 2)\n\nres = pd.DataFrame(res, columns=['1','2','5','10','20','25'])\nres = res.set_index([pd.Index(['R2','MAE','MSE'])])\nres","65ac2ded":"from sklearn.linear_model import Ridge\n\nsplits = 5\ncv = KFold(n_splits = splits)\nshape = (3, 8)\nres = np.empty(shape)\n\nfor j,a in enumerate([1,2,5,10,20,50,100,1000]):\n    mean_r2 = 0\n    mean_mae = 0\n    mean_mse = 0\n    for train_index, test_index in cv.split(X):\n        x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n        x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n        model = Ridge(alpha=a, random_state=0)\n        model.fit(x_train, y_train)\n        \n        mean_r2 = mean_r2 + model.score(x_test,y_test)\n        y_pred = model.predict(x_test)\n        mean_mae += mean_absolute_error(y_test,y_pred)\n        mean_mse += mean_squared_error(y_test, y_pred)\n        model.fit(x_train, y_train)\n\n    res[0][j] = np.round(mean_r2\/splits, 2)\n    res[1][j] = np.round(mean_mae\/splits, 2)\n    res[2][j] = np.round(mean_mse\/splits, 2)\n\nres = pd.DataFrame(res, columns=['1','2','5','10','20','50','100','1000'])\nres = res.set_index([pd.Index(['R2','MAE','MSE'])])\nres","20ca0453":"X = data[['Incident.Date.Ordinal', 'Injury', 'Age','Sex','lat','lon','Suspect.Weapon']]\ny = data['Race']\n\nX = X.copy()\nscaler = RobustScaler()\nX[[\"Incident.Date.Ordinal\",\"lat\",\"lon\"]] = scaler.fit_transform(X[[\"Incident.Date.Ordinal\",\"lat\",\"lon\"]])","6d8c6638":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix, f1_score, precision_score\n\nsplits = 5\ncv = KFold(n_splits= splits, shuffle=True, random_state=1) \nshape = (3, 8)\nres = np.empty(shape)\n\nfor j,c in enumerate([0.1,1,10,20,50,70,100,1000]):\n    mean_ac = 0\n    mean_prec = 0\n    mean_f1 = 0\n    for train_index, test_index in cv.split(X):\n\n        x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n        x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n        model = LogisticRegression(C= c, max_iter=100000)\n        model.fit(x_train, y_train)\n        \n        y_predicted = model.predict(x_test)\n        mean_prec += precision_score(y_test, y_predicted, average= 'macro', labels=np.unique(y_predicted))\n        mean_ac += model.score(x_test, y_test)\n        mean_f1 += f1_score(y_test, y_predicted, average='macro', labels=np.unique(y_predicted))\n\n    res[0][j] = np.round(mean_ac\/splits, 2)\n    res[1][j] = np.round(mean_prec\/splits, 2)\n    res[2][j] = np.round(mean_f1\/splits, 2)\n\nres = pd.DataFrame(res, columns=['0.1','1','10','20','50','70','100','1000'])\nres = res.set_index([pd.Index(['Accuracy','Precision','F1-Score'])])\nres","b33faf26":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = LogisticRegression(C= c, max_iter=100000)\nmodel.fit(X_train, y_train)\nplot_confusion_matrix(model, X_test, y_test)\nplt.show()","79ce86ae":"from sklearn.naive_bayes import GaussianNB\n\nsplits = 5\ncv = KFold(n_splits = splits, shuffle=True, random_state=1)\n\nmean_ac = 0\nmean_prec = 0\nmean_f1 = 0\n\nfor train_index, test_index in cv.split(X):\n    x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n    x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n    model = GaussianNB()\n    model.fit(x_train, y_train)\n    \n    y_predicted = model.predict(x_test)\n    mean_prec += precision_score(y_test, y_predicted, average= 'macro', labels=np.unique(y_predicted))\n    mean_ac += model.score(x_test, y_test)\n    mean_f1 += f1_score(y_test, y_predicted, average='macro', labels=np.unique(y_predicted))\n\nprint(\"Accuracy:\", np.round(mean_ac\/splits, 2))\nprint(\"Precision:\", np.round(mean_prec\/splits, 2))\nprint(\"F1-Score:\", np.round(mean_f1\/splits, 2))","41226eda":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\nplot_confusion_matrix(model, X_test, y_test)\nplt.show()","6b1e1774":"from sklearn.svm import LinearSVC\nfrom sklearn.model_selection import StratifiedKFold \n\nsplits = 3\ncv = StratifiedKFold(n_splits = splits, shuffle=True, random_state=1) \n\nshape = (10, 10)\nacc = np.empty(shape)\nprec = np.empty(shape)\nf1 = np.empty(shape)\n\nfor i,c in enumerate([1e-5,1e-4,1e-3,1e-2,1e-1,1,10,100,1000,10000]):\n    for j,t in enumerate([1000,100,10,1,1e-1,1e-2,1e-5,1e-10,1e-15,1e-20]):\n        mean_prec = 0\n        mean_ac = 0\n        mean_f1 = 0\n        for train_index, test_index in cv.split(X,y):\n            x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n            x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n            model = LinearSVC(random_state=2, tol = t, C = c)\n            model.fit(x_train, y_train)\n            \n            y_predicted = model.predict(x_test)\n            mean_prec += precision_score(y_test, y_predicted, average= 'macro', labels=np.unique(y_predicted))\n            mean_ac += model.score(x_test, y_test)\n            mean_f1 += f1_score(y_test, y_predicted, average='macro', labels=np.unique(y_predicted))\n\n             \n        acc[i][j] = np.round(mean_ac \/ splits, 2)\n        prec[i][j] = np.round(mean_prec \/ splits, 2)\n        f1[i][j] = np.round(mean_f1 \/ splits, 2)\n\ncols = ['1e-5','1e-4','1e-3','1e-2','1e-1','1','10','100','1000','10000']\nacc = pd.DataFrame(acc, columns = cols)\nprec = pd.DataFrame(prec, columns = cols)\nf1 = pd.DataFrame(f1, columns = cols)\n\nindices = [1000,100,10,1,1e-1,1e-2,1e-5,1e-10,1e-15,1e-20]\nacc = acc.set_index([pd.Index(indices)])\nprec = prec.set_index([pd.Index(indices)])\nf1 = f1.set_index([pd.Index(indices)])","fb0c9b14":"acc","8262848a":"prec","fad4855f":"f1","290d935b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = LinearSVC(random_state=2, tol = 1e3, C = 1e2)\nmodel.fit(X_train, y_train)\nplot_confusion_matrix(model, X_test, y_test)\nplt.show()","eae325ad":"from sklearn.svm import SVC\n\nsplits = 3\ncv = StratifiedKFold(n_splits = splits, shuffle=True, random_state=1) \n\nshape = (6, 6)\nacc = np.empty(shape)\nprec = np.empty(shape)\nf1 = np.empty(shape)\n\nfor i,c in enumerate([1e-3,1e-2,1e-1,1,10,100]):\n    for j,t in enumerate([100,10,1,1e-1,1e-2,1e-5]):\n        mean_prec = 0\n        mean_ac = 0\n        mean_f1 = 0\n        for train_index, test_index in cv.split(X,y):\n            x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n            x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n            model = SVC(gamma=\"auto\", C=c, tol=t)\n            model.fit(x_train, y_train)\n            \n            y_predicted = model.predict(x_test)\n            mean_prec += precision_score(y_test, y_predicted, average= 'macro', labels=np.unique(y_predicted))\n            mean_ac += model.score(x_test, y_test)\n            mean_f1 += f1_score(y_test, y_predicted, average='macro', labels=np.unique(y_predicted))\n\n             \n        acc[i][j] = np.round(mean_ac \/ splits, 2)\n        prec[i][j] = np.round(mean_prec \/ splits, 2)\n        f1[i][j] = np.round(mean_f1 \/ splits, 2)\n\ncols = ['1e-3','1e-2','1e-1','1','10','100']\nacc = pd.DataFrame(acc, columns = cols)\nprec = pd.DataFrame(prec, columns = cols)\nf1 = pd.DataFrame(f1, columns = cols)\n\nindices = [100,10,1,1e-1,1e-2,1e-5]\nacc = acc.set_index([pd.Index(indices)])\nprec = prec.set_index([pd.Index(indices)])\nf1 = f1.set_index([pd.Index(indices)])","e4b3b07b":"acc","956996e6":"prec","8a106d00":"f1","45d13470":"from sklearn import tree\n\nsplits = 5\ncv = KFold(n_splits=splits) \n\nmean_ac = 0\nmean_prec = 0\nmean_f1 = 0\n\nfor train_index, test_index in cv.split(X):\n    x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n    x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n    model = tree.DecisionTreeClassifier()\n    model.fit(x_train, y_train)\n    \n    y_predicted = model.predict(x_test)\n    mean_prec += precision_score(y_test, y_predicted, average= 'macro', labels=np.unique(y_predicted))\n    mean_ac += model.score(x_test, y_test)\n    mean_f1 += f1_score(y_test, y_predicted, average='macro', labels=np.unique(y_predicted))\n\nprint(\"Accuracy:\", np.round(mean_ac\/splits, 2))\nprint(\"Precision:\", np.round(mean_prec\/splits, 2))\nprint(\"F1-Score:\", np.round(mean_f1\/splits, 2))","732b7190":"from sklearn.neighbors import KNeighborsClassifier\nsplits = 5\ncv = KFold(n_splits= splits) \nshape = (11, 3)\nres = np.empty(shape)\nvecinos = [1,2,5,7,10,12,15,20,30,40,50]\nfor i, n in enumerate(vecinos):\n    mean_acc = 0\n    mean_prec = 0\n    mean_f1 = 0\n    for train_index, test_index in cv.split(X):\n        x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n        x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n        model = KNeighborsClassifier(n_neighbors=n)\n        model.fit(x_train, y_train)\n        \n        y_predicted = model.predict(x_test)\n        mean_prec += precision_score(y_test, y_predicted, average= 'macro', labels=np.unique(y_predicted))\n        mean_acc += model.score(x_test, y_test)\n        mean_f1 += f1_score(y_test, y_predicted, average='macro', labels=np.unique(y_predicted))\n    res[i][0] = np.round(mean_acc\/splits,2)\n    res[i][1] = np.round(mean_prec\/splits,2)\n    res[i][2] = np.round(mean_f1\/splits,2)\n    \nres= pd.DataFrame(res, columns = ['Accuracy','Precision','F1-score'])\n\nindices = [1,2,5,7,10,12,15,20,30,40,50]\nres.set_index([pd.Index(indices)])","4f65d6a8":"from sklearn.ensemble import RandomForestClassifier\n\nsplits = 5\ncv = KFold(n_splits=splits) \n\nmean_acc = 0\nmean_prec = 0\nmean_f1 = 0\n\nfor train_index, test_index in cv.split(X):\n    x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n    x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n\n    model = RandomForestClassifier(random_state=0)\n    model.fit(x_train, y_train)\n    \n    \n    mean_prec += precision_score(y_test, y_predicted, average= 'macro', labels=np.unique(y_predicted))\n    mean_acc += model.score(x_test, y_test)\n    mean_f1 += f1_score(y_test, y_predicted, average='macro', labels=np.unique(y_predicted))\n\nprint(\"Accuraccy\" , np.round(mean_acc\/splits, 2))\nprint(\"Precision\" , np.round(mean_prec\/splits, 2))\nprint(\"F1-Score\" , np.round(mean_f1\/splits, 2))","b2dd87fd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(X_train, y_train)\nplot_confusion_matrix(model, X_test, y_test)\nplt.show()","6fabcc90":"X = data[['Incident.Date.Ordinal', 'Age', 'Sex','Race','lat','lon','Suspect.Weapon']]\ny = data['Injury']\nX = X.copy()\nscaler = RobustScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)","648b0209":"plt.hist(y.apply(lambda x:int(x)), range=[0,3])\nplt.title('Injury Histogram')\nplt.xlabel('Type')\nplt.ylabel('Cases')\nplt.show()","8414e943":"def getMetrics(X,y, model):\n    \n    X_train = X[:int(len(X)*0.75)]\n    X_test = X[int(len(X)*0.75):]\n\n    y_train = y[:int(len(y)*0.75)]\n    y_test = y[int(len(y)*0.75):]\n    \n    clf = model.fit(X_train, y_train)\n    y_pred =  model.predict(X_test)\n    print('Accuracy:',round(model.score(X_test, y_test),2))\n    prec = precision_score(y_test, y_pred, average= 'macro', labels=np.unique(y_pred))\n    print('Precision:', round(prec,2))\n    f1 = f1_score(y_test, y_pred, average='macro', labels=np.unique(y_pred))\n    print('F1-Score:', round(f1,2))\n    \n    return f1\n    ","1d5a3979":"from sklearn.ensemble import RandomForestClassifier\n\nfor n in [2,10,100,1000]:\n    print(\"Resultados para\",n,\"estimadores:\")\n    print(\"-----------------------------------------------\")\n    randomForest = RandomForestClassifier(n_estimators=n, max_depth=3, max_leaf_nodes=4, random_state=42)\n    getMetrics(X,y, randomForest)\n    print(\"\")","d42f4f65":"shape = (4,4)\nres = np.empty(shape)\nfor i,n in enumerate([2,10,100,1000]):\n    for j,l in enumerate([2,4,10,30]):\n        print(\"Resultados con max_depth\",n,\"y max_leaft_nodes\", l)\n        print(\"-----------------------------------------------\")\n        randomForest = RandomForestClassifier(n_estimators=1000, max_depth=n, max_leaf_nodes=l, random_state=42)\n        f1 = getMetrics(X, y, randomForest)\n        res[i][j] = round(f1, 2)\n        print(\"\")\n\nres= pd.DataFrame(res, columns = [2,4,10,30])\nindices = [2,10,100,1000]\nres = res.set_index([pd.Index(indices)])","05a96a09":"res.head()","155e01c5":"from sklearn.ensemble import BaggingClassifier\nfor n in [2,10,100,1000,10000]:\n    print(\"Resultados para\",n,\"estimadores:\")\n    print(\"-----------------------------------------------\")\n    baggingSVC = BaggingClassifier(SVC(),n_estimators=n, random_state=42)\n    getMetrics(X,y, baggingSVC)\n    print(\"\")","336a5ebe":"for k in ['linear','poly','rbf','sigmoid']:\n    print(\"Resultados kernel\",k,\":\")\n    print(\"-----------------------------------------------\")\n    baggingSVC = BaggingClassifier(SVC(kernel=k),n_estimators=100, random_state=42)\n    getMetrics(X,y, baggingSVC)\n    print(\"\")","d84633ed":"for i,c in enumerate([1e-15,1e-10,1e-5,1e-2,1e-1,1]):\n    print(\"Resultados C:\", c)\n    print(\"-----------------------------------------------\")\n    baggingLog = BaggingClassifier(LogisticRegression(C = c,max_iter=100000),n_estimators=100, random_state=42)\n    f1 = getMetrics(X,y, baggingLog)\n    print(\"\")","8e7a2027":"for k in [1,2,5,10,25,50,100]:\n    print(\"Resultados n\u00ba vecinos\",k,\":\")\n    print(\"-----------------------------------------------\")\n    baggingknn = BaggingClassifier(KNeighborsClassifier(n_neighbors=k),n_estimators=1000 ,random_state=42)\n    getMetrics(X, y, baggingknn)\n    print(\"\")","c8ca9d53":"from sklearn.ensemble import AdaBoostClassifier\n\nfor l in [1e-20,1e-10,1e-5,1e-2,1e-1,1,10]:\n    print(\"Resultado learning_rate\",l,\":\")\n    print(\"-----------------------------------------------\")\n    adaboost = AdaBoostClassifier(n_estimators=100, learning_rate=l, random_state=42)\n    getMetrics(X, y, adaboost)","94eaf5d2":"from sklearn.ensemble import AdaBoostClassifier\n\nfor c in [1e-20,1e-10,1e-5,1e-2,1e-1,1,10]:\n    print(\"Resultado learning_rate\",c,\":\")\n    print(\"-----------------------------------------------\")\n    adaboost = AdaBoostClassifier(base_estimator = LogisticRegression(C = c,max_iter=100000),n_estimators=100, learning_rate=l, random_state=42)\n    getMetrics(X, y, adaboost)\n    print(\"\")","e08ddc13":"estimator = BaggingClassifier(LogisticRegression(C = 1e-5,max_iter=100000),n_estimators=100, random_state=42)\nadaboost = AdaBoostClassifier(base_estimator = estimator,n_estimators=100, learning_rate=0.01, random_state=42)\ngetMetrics(X, y, adaboost)","84448f44":"estimator = BaggingClassifier(SVC(),n_estimators=100, random_state=42)\nadaboost = AdaBoostClassifier(base_estimator = estimator,n_estimators=100, learning_rate=0.01, random_state=42)\nres = getMetrics(X, y, adaboost)","5c0bd072":"from sklearn.ensemble import GradientBoostingClassifier\nfor l in [1e-10,1e-5,1e-2,0.1,1,10,100]:\n    print(\"Resultado learning_rate\",l,\":\")\n    print(\"-----------------------------------------------\")\n    gradientboost =GradientBoostingClassifier(n_estimators=100, learning_rate=l)\n    getMetrics(X,y, gradientboost)\n    print(\"\")","3f990ad3":"from sklearn.ensemble import StackingClassifier\n\n# define the base models\nbase_models = list()\nbase_models.append(('knn', KNeighborsClassifier()))\nbase_models.append(('cart', RandomForestClassifier(random_state=42)))\nbase_models.append(('svm', SVC(random_state=42)))\nbase_models.append(('lr', LogisticRegression(random_state=42)))\n\n# define meta learner model\nmeta_learner = SVC(random_state=42)\n\n# define the stacking ensemble\nstacking = StackingClassifier(estimators=base_models, final_estimator=meta_learner, cv=5)\n\nres = getMetrics(X,y, stacking)","8af42a1c":"from sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler","c3d68aa9":"X = data[['Sex', 'Race','Injury','lat','lon','Suspect.Weapon', 'Age']]\nXs = pd.DataFrame(StandardScaler().fit_transform(X))","1f94501b":"def elbow(dataframe):\n  plt.figure(figsize=(8, 8))\n  wcss = []\n  for i in range(1, 20):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++')\n    kmeans.fit(dataframe)\n    wcss.append(kmeans.inertia_)\n  plt.plot(range(1, 20), wcss)\n  plt.title('M\u00e9todo Elbow')\n  plt.xlabel('N\u00ba clusters')\n  plt.ylabel('WCSS')\n  plt.show()","1176b860":"elbow(Xs)","05aef884":"def getClusters(dataframe, c1, n):\n  dic = {}\n  for i in range(n):\n    cluster = dataframe.iloc[np.where(c1==i)].index\n    dic[\"c\"+str(i)] = cluster.to_list()\n  return dic","b9625c33":"cluster = KMeans(n_clusters=10).fit(Xs)\npred =  cluster.predict(Xs)","a13fd066":"x = getClusters(Xs, pred, 10)","54f19d13":"X.iloc[x['c8']]","0df7c195":"from scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix\nfrom sklearn.cluster import AgglomerativeClustering \n\ncluster = AgglomerativeClustering(n_clusters=10, linkage='complete')  \ncluster.fit_predict(Xs)\ny = getClusters(Xs,cluster.labels_,10)\n","1d1e58dd":"X.iloc[y['c4']]","d29d479a":"plt.figure(figsize=(20,4)) \nplt.subplot(1, 2, 1)\nplt.title(\"Cluster mediante k-means\")\nplt.bar(x.keys(), [len(x[i]) for i in x.keys()])\n\nplt.subplot(1, 2, 2)\nplt.title(\"Cluster jerarquico\")\nplt.bar(y.keys(), [len(y[i]) for i in y.keys()])\nplt.show()","eeccf638":"print(\"Elementos clusters k-means\")\nprint(x)\nprint(\"-----------------------------\")\nprint(\"Elementos clusters jerarquico\")\nprint(y)","10f54d4c":"def getClusters(dataframe, c1, n):\n  dic = {}\n  for i in n:\n    cluster = dataframe.iloc[np.where(c1==i)].index\n    dic[\"c\"+str(i)] = cluster.to_list()\n  return dic","9364ea5e":"from sklearn.cluster import OPTICS\ncluster = OPTICS().fit(Xs)\n\nprint(np.unique(cluster.labels_))\noptics_result = getClusters(Xs, cluster.labels_, np.unique(cluster.labels_))","40bc6de3":"print(optics_result)","27abe21d":"o = optics_result\nplt.title(\"Cluster OPTICS\")\nplt.bar(o.keys(), [len(o[i]) for i in  o.keys()])\nplt.show()","8bad4b1c":"X.iloc[optics_result['c4']]","8d87c9f0":"X.iloc[optics_result['c5']]","69726f06":"Aplicamos OPTICS con los par\u00e1metros por defecto y se obtienen 10 clusters diferentes. Se muestras a continuaci\u00f3n:","c7d108c9":"# Tiroteos en Houston\nEl departamento de policia de Houston tiene un reporte de los tiroteos sufridos en esa ciudad en el periodo de 2010 a 2016. El objetivo de este notebook, ser\u00e1 el an\u00e1lisis de esos datos mediante t\u00e9cnicas de miner\u00eda de datos, para poder obtener predictores y clasificadores. As\u00ed como intentar obtener conclusiones robustas sobre los datos.\n\nFuente de datos: https:\/\/data.world\/fileunderjeff\/hpd-shootings-2010-2016\/workspace\/file?filename=shootings.csv","3d50ad09":"# SVM lineal\nVamos a comprobar si los datos son linealmente separable mediante m\u00e1quinas de soporte de vectores. Para ello vamos a ir variando los par\u00e1metros de este modelo, en concreto la toleracia y el par\u00e1metro de regularizaci\u00f3n C. ","d91c65d6":"### Preprocesado y limpieza de datos","64ac00bd":"### Conclusi\u00f3n ensembles\nSe ha tratado de utilizar diferentes ensembles, algunos de ellos se ha experimentado con los par\u00e1metros para tratar de encontrar la mejor combinaci\u00f3n y otros se han probado con los par\u00e1metros con defecto. Tambi\u00e9n se ha probado a utilizar diferente cantidad de estimadores para ver como afectan.\n\nLos resultados obtenidos sin embargo no han sido buenos, esto no significa que no se haya hecho correctamente, sino que simplemente la clase que se pretende predecir no sea tan predecible como se esperaba, tal vez si hubiera existido una mayor cantidad de datos se hubieran obtenido mejores resultados.","bf8e0ed4":"Eliminamos columnas que no son relevantes y que no aportan ning\u00fan tipo de valor. Como es por ejemplo el n\u00famero de incidente o la del rol, que siempre es el mismo.","02901190":"Si comparamos los resultados de los cl\u00fasters vemos como los obtenidos por k-means estan algo m\u00e1s balanceados, mientras que el obtenido por el cl\u00faster jer\u00e1rquico tiene el cluster c0 m\u00e1s poblado que los dem\u00e1s.","18e853fa":"En regresi\u00f3n log\u00edstica con C peque\u00f1os se consiguen mejores resultados. en esta ocasi\u00f3n hemos conseguido un F1-Score de 0.63 superando a SVC. Va mejorando pero seguimos sin tener un buen predictor. Por ello vamos a seguir probando con otros modelos.\n\n### Bagging KNN\n\nKNN es otro de los modelos que podriamos probar e ir variando su n\u00famero de vecinos para ver que resultados se obtienen. ","da141286":"## Conclusi\u00f3n Clustering\nExisten diferentes m\u00e9todos para crear clusters, hemos probado m\u00e9todos basados en distancia como K-means, cluster jerarquicos aglomerativos y clusters basados en densidad.\n\nParece ser que los basados en densidad nos han aportado mejores resultados, ofreciendo clusters con observaciones m\u00e1s parecidas y alejando el ruido.\n\nUn elemento que nos puede haber jugado una mala pasada para la detecci\u00f3n de los cluster es el desbalanceo entre las clases de todas la variables, ya que seguramente con un dataset m\u00e1s 'rico' y variado hubieramos obtenido mejores resultados.","6cf3f435":"Se muestra una tabla con los resultados obtenidos, C=1 nos da los mejores resultados con un F1-score de 0.29.\n\nSi mostramos la matriz de confusi\u00f3n de la \u00faltima iteraci\u00f3n, comprobamos como no ha dado buenos resultados. Por ejemplo, la clase 1, la predice bien 21 veces de 24 que aparece esa clase, que est\u00e1 bastante bien, pero con las dem\u00e1s clases no acierta ni una. Con esto quiero decir que no estamos ante un buen clasificador, prueba de ellos siendo las m\u00e9tricas tan bajitas que saca.","97a31b0d":"### Bagging Logistic Regression\nPara la regresi\u00f3n log\u00edstica como hemos visto que a mayor n\u00famero de estimadores parece obtener mejor resultado, seguiremos utilizando ese n\u00famero de estimadores(100), adem\u00e1s de que si usamos m\u00e1s repercute bastante en el tiempo de ejecuci\u00f3n y la mejora que pueda tener no compensa respecto a la cantidad de tiempo que hay que esperar.\n\nNos vamos a centrar en variar el par\u00e1metro de la regularizaci\u00f3n C.","ab026903":"Finalmente, vamos a comprobar la cantidad de nulos que existen en el dataset, y vemos que existen algunas filas con datos de lat y lon ausentes. Para acabar con esto, lo m\u00e1s sencillo es eliminar esas filas, ya que en este caso, aplicar t\u00e9cnicas de imputaci\u00f3n creo que no ser\u00eda la mejor opci\u00f3n.","2ed164e5":"Seg\u00fan el m\u00e9todo de Elbow para la elecci\u00f3n del n\u00famero \u00f3ptimo de clusters, un total de 10 clusters ser\u00eda aproximadamente la mejor opci\u00f3n.","6fa4bf06":"Como vemos en esta columna, existe una gran cantidad de valores \u00fanicos. Pero porque realmente son una combiancion con otros. Por ello, vamos a aplciar la funci\u00f3n definida arriba _maxChar()_, que obtendr\u00e1 el valor m\u00e1s repetido en el caso de que existan varios. Una vez pasada la columna por la funci\u00f3n, vamos a categorizar los valores, de forma que pasen de Masculino, Femenino o Desconocido a 0,1,2.","a36ffdbd":"Los resultado arrojados no son especialmente buenos, pero se puede observar como que al aumentar el n\u00famero de estimadores se obtienen mejores m\u00e9tricas en conjunto global.\nLa que mejore resultado a dado es la que utiliza 1000 estimadores. Por ello, vamos ahora a probar con 1000 estimadores pero probando a cambiar entre los diferentes par\u00e1metros de los \u00e1rboles de decisi\u00f3n. En este caso nos vamos a quedar solo con una m\u00e9trica, para facilitar la lectura de resultados  F1-score que refleja bastante bien el funcionamiento del modelo.","c7156097":"Estos son los resultados obtenidos con cada una de las m\u00e9tricas, y para cada tama\u00f1o de vecinos. Parece ser que funciona mejor con muchos vecinos que con pocos vecinos. Con 40 vecinos parece obtener el mejor resultado en conjunto para las 3 m\u00e9tricas. Pero que raro! si con pocos vecinos tendr\u00eda que hacer overfitting? Pero no, se obtiene mejor resultado con muchos vecinos porque al estar los datos desbalanceados, se obtiene una especie de media general que predice la clase dominante.","446b85b3":"# Gaussian Navie Bayes\nEn este caso no tenemos ning\u00fan par\u00e1metro para tunear, as\u00ed que simplemente comprobaremos los resultados.","6669b1c1":"# Regresi\u00f3n log\u00edstica\nVamos a clasificar mediante regresi\u00f3n log\u00edstica e ir variando el par\u00e1metro C para ver como afecta a los resultados.","df270431":"### Clusters de densidad\n#### OPTICS\n\nLa siguiente prueba a realizar va a ser con clusters de densidad, que va a tratar detectar regiones densas de datos en el espacio. Con esta t\u00e9cnica quiz\u00e1 obtengamos mejores resultados para nuestro objetivo de detectar bandas criminales, ya que no es necesario definir un n\u00famero de clusters. Otra cosa buena que tiene es que es robusto al ruido y a los outliers, lo cual supone tambi\u00e9n una ventaja.","2fadc519":"Con un learning rate de 0.01 se consiguen unos mejores resultados exactamente igual que con lo \u00e1rboles de decisi\u00f3n, por ello vamos a intentar tambi\u00e9n utilizarlo con otro estimador,pero en este caso ser\u00e1 uno fuerte, como bagging de Logistic regression o con SVC.","8ecd9e39":"Aqu\u00ed la matriz de confusi\u00f3n con una diagonal bastante pobre, y que nos indica adem\u00e1s de que la cantidad de datos que tenemos es muy baja. Esta falta de datos hacen que nuestras predicciones sean pobres, adem\u00e1s de que tengamos pocos datos para probar en test.","579cd225":"Se muestra una tabla con las m\u00e9tricas y con las diferentes m\u00e1ximas profundidades de \u00e1rbol.\nLos resultados se mantienen similares a los modelos anteriores, ninguna profundidad de \u00e1rbol parece afectar significativamente al modelo.","fd6f5812":"Tras probar 4 de los kernels que nos permiter configurar sklearn y con el n\u00famero de estimadores que mejor resultados nos han dado con el kernel por defecto. Si observamos los resultados, el kernel que mejor ha funcionado ha sido el kernel 'sigmoido' con un F1-Score de 0.44, mejorando as\u00ed la obtenida por el random forest. A\u00fan as\u00ed no es ninguna maravilla.","85f4517b":"Con la fecha, para que el algortimo puedra procesarla vamos a convertirla a ordinal.","3ef6fddb":"## SVR\nPara aplicar el algoritmo de SVR vamos a aplicar varios Kernels y a comparar los resultados para las diferentes m\u00e9tricas.","5abf38ea":"Probamos a relizar un cl\u00faster jer\u00e1rquico aglomerativo, indic\u00e1ndole que el n\u00famero de cluster que queremos que genere sea 10 y utilizando de m\u00e9todo de linkage \"complete\".\nComprobamos tambi\u00e9n el contenido de uno de los clusters para ver como est\u00e1 conformado:","ef231f8b":"Creamos los clusters y luego llamamos a la funci\u00f3n para observar que elementos pertencen a cada cluster.","1f968a99":"#### One-HotEncoding\nUna vez normalizados los valores continuos, vamos a codificar con one-HotEncoding las variables categ\u00f3ricas. Pero... si ya estaban codificadas!!! S\u00ed , pero con one-HotEncoding, facilitaremos al algoritmo su trabajo y obtendremos mejores resultados.","efb5ebc5":"Vemos que las observaciones que hay en este cluster son algo variadas y parece que no se adapta mucho a nuestro objetivo. Por ello vamos a probar con otros m\u00e9todos de clustering para comprobar si se llega a la misma soluci\u00f3n.\n\n### Clustering jer\u00e1rquico\n","4d934571":"# SVM RADIAL\nAl igual que con el kernel lineal , jugaremos con el par\u00e1metro C y la toleracia.","621363a5":"En un principio se veia como que con un learning rate bajo ofrecia mejores resultados, hasta que hemos llegado al learning rate de 10 que nos da los mismos resultados que con knn. Quiz\u00e1 sea una buena idea probar AdaBoost con otro estimador, as\u00ed que vamos a repetir el proceso, pero ahora por ejemplo con logistic regression como estimador.","e7fe0c75":"Una colunma que tenemos que preprocesar y adem\u00e1s aplicar imputaci\u00f3n es la de 'Age'. Tras pasar la columna  de texto a valores enteros, aplicamos una imputaci\u00f3n por la estrateg\u00eda del valor m\u00e1s frecuente, ya que creo que es la que mejor se adapta a este caso.","de0fae84":"# Random forest","4a8481e4":"Para R2, obtenemos los siguientes resultados, cuya mejor combinaci\u00f3n de par\u00e1metros es C=20 y tol=1e-5, obteniendo -0.16, que sigue siendo un resultado malo, similar al obtenido con el modelo anterior.\n\n#### MAE resultados","c3b9056a":"En primer lugar vamos a crear una funci\u00f3n a la cual le pasaremos el ensemble y los datos. Los dividir\u00e1 en entrenamiento y en test, se entrena el modelo y finalmemte se c\u00e1lcula el error de entrenamiento.","fe77f1fb":"### Cargamos los datos","7c4bea75":"Comprobamos las observaciones contenidas en uno de los clusters creados.","257d84de":"Estos coeficientes nos dicen por ejemplo, que si la persona que comete un delito pertenece al sexo 2 ser\u00e1 unos 7 a\u00f1os mayor que si es del sexo 1.","d56a4d82":"Con learning rates peque\u00f1os nos da mejores resultados, pero seguimos sin conseguir mejoras respecto a otra pruebas que hemos hecho.","5ed9a531":"Aqu\u00ed vemos el histograma de la variable que queremos predecir 'Injury', existen 4 posibles tipos: muerto, en herido, si no ha pasado nada o si es desconocido.","543c5d11":"Con Ridge como era de esperar tampoco hemos obtenido mejores resultados que con los otros modelos. A\u00fan consiguiendola m\u00e9trica R2 m\u00e1s elevada sigue siendo malo.","4aa9ac83":"## Ridge Regresion\nPara este modelo vamos a ver como afecta la variaci\u00f3n del par\u00e1metro alpha.","249228eb":"No conseguimos ninguna mejora respecto a otros intentos anteriores y el tiempo de computaci\u00f3n que requiere es demasiado alto.","89c9da78":"Para predecir el tipo de herida provocada a la persona\/s que se han visto implicadas en el tiroteo, vamos a probar a usar primero el cl\u00e1sico Random Forest, luego ,vamos a hacer uso de ensembles utilizando en primero lugar la t\u00e9cnicas de bagging, probando con diferentes clasificadores como \u00e1rboles de decisi\u00f3n, Vectores de soporte radiales, regresi\u00f3n log\u00edsticas, knn, radiales...\n","2fc2f0a9":"#### Normalizaci\u00f3n\/Escalado","65305bae":"#### Selecci\u00f3n de caracter\u00edsticas para regresi\u00f3n\n\nSin ser un experto en la materia, todas la caracter\u00edsticas de este dataset parecen ser interesantes para realizar predicciones. No obstante, dejaremos fuera la columna 'Synopsis', que a\u00fan pudiendo ser una columna con valor muy interesante, requerir\u00eda de t\u00e9cnicas de miner\u00eda de texto, que se sale del objetivo de este notebook.\n\nEn la parte de regresi\u00f3n, el objetivo va a ser intentar predecir la edad del sospechoso. Para eso vamos a dividir los datos de la siguiente forma:","9f8bfb9b":"Creamos un m\u00e9todo que nos devuelva los \u00edndices de los elementos que hay en cada cluster","b61ab7c7":"## Clustering\nMediante el clustering vamos a tratar de agrupar observaciones de tiroteos que contengan caracter\u00edsticas similares. Esto nos podr\u00eda servir por ejemplo, para detectar bandas callejeras organizadas, ya que los miembros de estos grupos suelen ser de la misma edad, actuar el lugares parecidos y utilizar las mismas armas. Como primer procedimiento para obtener estos grupos vamos a probar a utilizar k-means.\n### k-means","8bc6d945":"Para la m\u00e9trica MAE, tambi\u00e9n se consigue el mejor resultado con los par\u00e1metros anteriores,obteniendo un resultado ligeramente mejor que la regresi\u00f3n lineal.\n\n#### MSE resultados","683acd88":"Aqu\u00ed vemos como los resultados obtenidos siguen siendo bastante malos. Ninguna de las m\u00e9tricas nos ofrece buenos resultados.","93f1141e":"## SVR Lineal\nOtro modelo que se puede probar es los vectores de soporte para regresiones lineales.\nEn este caso, como el modelo tiene diferentes par\u00e1metros con los que jugar, vamos a probarlo con diferentes par\u00e1metros C y jugando tambi\u00e9n con la tolerancia.","78cf91d7":"# Parte 1\n## REGRESI\u00d3N ","33335fc9":"# \u00c1rbol de decisi\u00f3n","c25efd29":"En este caso, parece que con 100 estimadores nos da la mejor puntuaci\u00f3n de f1-score, por ello vamos a utilizar esa cantidad de estimadores para probar los diferentes kernels.","19879bf1":"La gr\u00e1fica muestra la distribuci\u00f3n de los diferentes clusters. Se ve que existe bastante ruido(c-1), pero a su vez tenemos clusters de entre 10 y 20 observaciones. Vamos a fijarnos en uno, en el C4 por ejemplo y vamos a observar sus componentes:","0236825d":"### Bagging SVCs\nAhora vamos a cambiar el modelo que usamos en el Bagging, vamos a probar por ejemplo SVC, con difetentes kernels. Pero primero vamos a ver, al igual que antes si afecta el n\u00famero de estimadores.","8148b6b6":"Vamos preprocesar primero la variable 'Sex', veamos que valores \u00fanicos tiene.","009b3163":"Aqu\u00ed se muestra una tabla resumen de los resultados de arriba. Las columnas ser\u00eda el valor de la cantidad de hojas m\u00e1ximas para los \u00e1rboles y como \u00edndices de fila, la profundidad m\u00e1xima. Es interesante ver como el mejor resultado de f1-score nos salen cuando el n\u00famero m\u00e1ximo de hojas coincide con el n\u00famero de clases de la clase que queremos predecir, lo c\u00faal tiene sentido. Por otro lado, vemos como en cuanto a la profundidad del \u00e1rbol, funciona mejor con \u00e1rboles poco profundos.","66194ef5":"#### Gradient Boost\nEl gradientBoostingClassifier de sklearn tiene bastantes par\u00e1metros que se podr\u00edan probar, sin embargo, solo probaremos distintos learning rates, usando 100 estimadores.","af25fe53":"Creamos los datasets a utilizar y escalamos los datos","edfe8b0f":"\n# Regresi\u00f3n Lineal\nLa primera regresi\u00f3n a utilizar es una de las m\u00e1s b\u00e1sicas, se trata de la regresi\u00f3n lineal. Para tener una mejor visi\u00f3n de como funciona el modelo se va a aplicar validaci\u00f3n cruzada. Tambi\u00e9n se usar\u00e1n metricas como MAE MSE Y R2 para evaluar el modelo.","3c0ce7be":"Si nos fijamos en el cluster C5 vemos otro claro ejemplo de observaciones bastante parecidas y que podr\u00eda significar la existencia de una banda criminal.","e81ffa16":"Este algoritmo nos da la mejor puntuaci\u00f3n en la m\u00e9trica F1-Score. No obstante la precisi\u00f3n y el accuracy no son muy altos. La matriz de confusi\u00f3n muestra resultados bastante similares a los otros algoritmos, con una clara tendencia hacia la clase 1 que es la clase dominante.","2dd54c3c":"Una vez m\u00e1s los resultados no son para nada prometedores.","9131850e":"Obtenemos un resultado similar al obtenido con k-means, siguen apareciendo observaciones algo parecidas, pero no es lo que estamos buscando.","babd6f34":"# Clasificaci\u00f3n\nHa llegado el momento de pasar a la clasificaci\u00f3n. Para realizarla intentaremos predecir la raza de la persona que comete el delito. El precisi\u00f3n del modelo estar\u00e1 definido por m\u00e9tricas usualmemte utilizadas en clasificaci\u00f3n como: Accuracy, precision y F1-score. Adem\u00e1s de usar matrices de confusi\u00f3n para mostrar visualmente los resultados.","3d0e0851":"Aqu\u00ed escalamos los datos. Para ello usamos Robust Escaler, \u00bfporque? Porque como su nombre indica es robusto a outliers y ya que nuestras regresores no siguen una distribuci\u00f3n normal y tienen bastantes outliers, este escalador puede ser una buena opci\u00f3n frente a otros, que son m\u00e1s sensibles a los outliers o requeiren una distribuci\u00f3n normal.\n\nEste escalado, lo realizamos sobre las variables no categ\u00f3ricas.","e643793e":"## Ramdom Forest\nPara este algoritmo existen par\u00e1metros interesantes con los que podemos trabajar como por ejemplo, la profundidad m\u00e1xima de los \u00e1rboles.","07b812ed":"Como no tenemos una regresi\u00f3n lineal , los resultados son pobres. Si nos fijamos en la metricas obtenidas, R2 nos da un resultado bastante malo, siendo negativo. Por lo que este modelo, poca varianza captura. La m\u00e9trica MAE nos indica que en media se equivoca unos 7 a\u00f1os a la hora de predecir la edad de los sospechosos, lo cual es bastante. Y el MSE, vemos tambi\u00e9n que es bastante alto para la variable que queremos predecir. As\u00ed que se puede afirmar que este modelo no es para nada bueno.\n\nPodemos echar una vista a los coeficientes que se han obtenido:","71d43e2d":"En las tablas anteriores se representan los valores obtenidos para accuracy, precision y f1-score. En las columnas tenemos el par\u00e1metro C, y en los \u00edndices la tolerancia.\nSi nos fijamos en la tabla de f1-score, para unata tolerancia:1e3 y C=1e-2, se obtiene un 0,6 de f-score, que es hasta el momento la puntuaci\u00f3n m\u00e1s alta obtenida en los algortimos de clasificaci\u00f3n, pero sigue siendo una puntuaci\u00f3n no muy buena..\n\nAhora bien, si ploteamos la matriz de confusi\u00f3n y le pasamos el dataset entero. Se puede observar como solo con la clase 1 predice algo bien, las dem\u00e1s no obtiene buenos resultados, suele tender a decir que pertenecen a la clase 1. Algo muy parecido nos ha paso con regresi\u00f3n log\u00edstica.","b27d1a73":"Como era de esperar, no obtenemos buenos resultados tampoco con este modelo. Y parece que no exista forma alguna de obtener buen resultado con ning\u00fan modelo. A priori parece ser, como se ha comentado anteriormente, que sea por el desbalance de datos y que tampoco existe una gran cantidad. Quiz\u00e1 tambi\u00e9n puede que existan regresores que sobren en el modelo.","0478d557":"Tras todos los cambios realizados los datos que utilizar\u00e1 el algoritmo son algo asi:","9ede2a31":"\nEn algunas columnas encontramos, no un \u00fanico valor sino varios, debido a que el incidente, puede ser que se haya producido con varios sospechosos. Como soluci\u00f3n a esto vamos a quedarnos con el valor que m\u00e1s se repita. Por ejemplo, si en una fila, en la columna de armas sospechosas, encontramos ['Machete','Pistola','Pistola'], nos quedaremos con pistola \u00fanicamente, al ser el arma m\u00e1s utilizada. Para poder hacer esto, vamos a crear un par de funciones tanto para realizarlo con texto como con valores num\u00e9ricos.","d61a8188":"Veamos como queda ahora la distribuci\u00f3n:","8ec3f1d9":"## Stacking\nPara la parte de stacking, vamos a definir una serie de estimadores y un estimador final. Para esta tarea se podri\u00e1n combiarn muchos modelos, utilizar weak learners o bagging, boosting o combinaci\u00f3n entre ellos. Por razones de tiempo se probar\u00e1 simplemente con una combinaci\u00f3n de weak learners con sus par\u00e1metros por defecto y utilizar\u00e9 como estimador final un SCV.","1e9ff431":"Adem\u00e1s tiene la siguiente distribuci\u00f3n","5eaaa1d4":"Las mejores puntuaciones se obtienen con un gran n\u00famero de vecinos, un F1-Score de 0.63 al igual que con Logistic Regression. ","4c39f334":"Podemos afirmar lo mismo que con el MSE, poca mejora se encuentra.","bff1f443":"# KNN\nProbamos el algoritmo KNN con difertente n\u00famero de vecinos y observamos comom var\u00eda el resultado.","3889f98e":"El \"mejor\" resultado se obtiene con el kernel rfb, este kernel ofrece unos resultados bastante similares a los anteriores por lo que con este modelo tampoco se obtiene una mejora significativa respecto a los anteriores.","1669b5b2":"Si observamos los componentes del cluster C4, obtenemos una serie de elementos con caracter\u00edsticas muy similares, mismo sexo, misma raza, mismo tipo de herida, misma arma sospechosa, unas edades bastante parecidas entre los 24-33. Tambi\u00e9n se observa que los tiroteos han sido cometidos en lugares bastante cercanos entre s\u00ed. Quiz\u00e1 este cluster est\u00e9 indicando una serie tiroteos cometidos por una misma banda.","be683913":"#### Random Forest\nLa librer\u00eda sklearn proporciona herramientas para poder realizar un clasificador con RandomForest de manera sencilla. Vamos a a\u00f1adir una semilla para que puedan ser\nreproducibles los resultados e iremos probando a variar el n\u00famero de estimadores y algunos par\u00e1metros propios de los \u00e1rboles de decisi\u00f3n.","13936f4f":"### Boosting\n#### AdaBoost\nAdaboost utiliza como estimador por defecto Arboles de decisi\u00f3n. Por tanto vamos a probar que resultados nos arroja con el estimador por defecto, adem\u00e1s vamos a probar \na variar el learning rate, para comprobar como afecta.","874c4f78":"Con los vectores de soporte radiales hemos mejorado respecto a las m\u00e9tricas obtenidas con el lineal, sin embargo, siguen siendo malas. Esta vez no mostraremos la matriz de confusi\u00f3n.","93d965e4":"# PARTE 2: ENSEMBLES Y CLUSTERING\n\n## ENSEMBLES\n### BAGGING\n","2cca047a":"Bien, mucho mejor. Ahora vamos a repetir el proceso con las columnas categ\u00f3ricas restantes.","ea59a400":"Hemos guardado los resultados de cada una de las m\u00e9tricas obtenidas jugando con los par\u00e1metros en un dataframe. En las columnas tendremos el valor de C y en los indices la tolerancia.\n#### R2 resultados"}}