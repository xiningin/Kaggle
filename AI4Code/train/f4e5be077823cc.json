{"cell_type":{"46e016d7":"code","b86355d0":"code","d191a23b":"code","5883eb37":"code","23635f4c":"code","87e12054":"code","0afc9191":"code","05003a9b":"code","179c1a79":"code","130ef604":"code","73d43c3a":"code","91921ba3":"code","b25e7c1b":"code","b8890ae1":"code","60b3dc04":"code","5177bb36":"code","c4107771":"code","4c35cdc0":"code","79ad68e1":"code","bd8c446c":"code","5daac22f":"code","31d6a63b":"code","1acf142c":"code","44a37c04":"code","d5b1d8d5":"code","215e369d":"code","a573aa91":"code","256a704a":"code","91756544":"code","8d8269d1":"code","522ad2e5":"code","c0159ca8":"code","eb311803":"code","eb69ccbf":"code","22ba35fb":"code","471d9e68":"code","1df9bcc0":"code","e44cf6ad":"code","6947ad84":"code","d3f8d75f":"code","12418027":"code","9f6097d0":"code","01e1c53e":"code","58740a13":"code","4280c75f":"code","86cb11e7":"markdown","8978977a":"markdown","741130e6":"markdown","e5e14611":"markdown","062ffa20":"markdown","6c69866c":"markdown","e8f7e5e1":"markdown","7034c1d5":"markdown","a7cc9046":"markdown","10139b66":"markdown","b334b290":"markdown","b48ec6eb":"markdown","1a80a252":"markdown","39a9f786":"markdown","a315691a":"markdown","b98550d9":"markdown","bbc06daf":"markdown","1601a2c0":"markdown","4ca164cc":"markdown","71bf80f0":"markdown","aed30222":"markdown","3344dac2":"markdown"},"source":{"46e016d7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nadult = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\",\n        engine='python',\n        sep=r'\\s*,\\s*',\n        na_values=\"?\")\n","b86355d0":"adult.shape","d191a23b":"adult.head()","5883eb37":"adult.info()","23635f4c":"adult.isnull().sum()","87e12054":"print(\"workclass\")\nprint(adult['workclass'].describe())\nprint()\n\nprint(\"occupation\")\nprint(adult['occupation'].describe())\nprint()\n\nprint(\"native.country\")\nprint(adult['native.country'].describe())\nprint()","0afc9191":"print(adult['workclass'].value_counts())\nprint()\n\nprint(adult['native.country'].value_counts())\nprint()\n      \nprint(adult['occupation'].value_counts())\nprint()","05003a9b":"value = adult['workclass'].describe().top\nadult['workclass'] = adult['workclass'].fillna(value)\n\nvalue = adult['occupation'].describe().top\nadult['occupation'] = adult['occupation'].fillna(value)","179c1a79":"#Para facilitar a visualiza\u00e7\u00e3o dos gr\u00e1ficos, mudaremos momentaneamente a vari\u00e1vel income para uma num\u00e9rica\nadult['income']=adult['income'].map({'<=50K':0, '>50K':1})","130ef604":"#Contagem de quantos ganham <=50K e >50K\nsns.countplot(adult['income'])\nplt.ylabel('Quantidade')","73d43c3a":"#Rela\u00e7\u00e3o entre o quanto ganha e o sexo\ng = sns.barplot(x='sex',y='income',data=adult)\nplt.ylabel('Probabilidade de ganhar >50k')\nplt.xlabel('Sexo')\nplt.show()","91921ba3":"#Matriz de correla\u00e7\u00e3o entre as vari\u00e1veis num\u00e9ricas\nvariaveis_numericas = ['age','fnlwgt','education.num','capital.gain','capital.loss','hours.per.week','income']\ng = sns.heatmap(adult[variaveis_numericas].corr(),annot=True, fmt='.2f', cmap='coolwarm')\nplt.show(g)","b25e7c1b":"#Rela\u00e7\u00e3o entre o quanto se ganha e o n\u00edvel de educa\u00e7\u00e3o\ng = sns.catplot(x='education.num',y='income',data=adult,kind='bar',height=6,palette='muted')\ng.despine(left=True)\nplt.ylabel('Probabilidade de ganhar >50K')\nplt.show(g)","b8890ae1":"#Idade das pessoas avaliadas pelo censo \nplt.figure(figsize=(13,7))\nsns.distplot(adult['age'], bins=70)\nplt.ylabel('Quantidade')\nplt.xlabel('Idade')","60b3dc04":"#Quantidade de horas trabalhadas\nplt.figure(figsize=(13,7))\nadult['hours.per.week'].hist()\nplt.xlabel('Horas por semana')\nplt.ylabel('Quantidade')","5177bb36":"#Rela\u00e7\u00e3o entre o quanto se ganha e a etnia\nplt.figure(figsize=(17,10))\nb=sns.countplot(x='race',hue='income',data=adult)","c4107771":"#Juntando as vari\u00e1veis capital gain e capital loss de modo a evitar sobre ajuste\nadult['capital']=adult['capital.gain']-adult['capital.loss']\nadult['capital'].describe()","4c35cdc0":"#Rela\u00e7\u00e3o entre o esatado civil e quanto se ganha \nplt.figure(figsize=(17,10))\nb=sns.countplot(x='marital.status',hue='income',data=adult)","79ad68e1":"#Rela\u00e7\u00e3o entre o rela\u00e7\u00e3o familiar e quanto se ganha \nplt.figure(figsize=(17,10))\nb=sns.countplot(x='relationship',hue='income',data=adult)","bd8c446c":"#Revertendo o processo usado pra analisar a base\nadult['income']=adult['income'].map({0:'<=50K', 1:'>50K'})","5daac22f":"#Removendo as vari\u00e1veis que n\u00e3o ser\u00e3o utilizadas e tratando das que ser\u00e3o\ncopy=adult['income']\nadult=adult.drop(['native.country','capital.gain','capital.loss','education','fnlwgt','Id','income'],axis=1)\nadult['income']=copy\n\ncolums_to_encode=['workclass','marital.status','sex','race','relationship','occupation']\nfor feature in colums_to_encode:\n    le=preprocessing.LabelEncoder()\n    adult[feature]=le.fit_transform(adult[feature])","31d6a63b":"x_adult=adult[['age','workclass','education.num','marital.status','sex','race','relationship','occupation','hours.per.week','capital']]\ny_adult=adult.income","1acf142c":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_adult,y_adult, test_size=0.3 , random_state = 0)","44a37c04":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\npca=PCA()\nx_train = pca.fit_transform(x_train)","d5b1d8d5":"pca.explained_variance_ratio_","215e369d":"logreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\naccuracy_score(y_test,y_pred)","a573aa91":"from sklearn.tree import DecisionTreeClassifier\nimport sklearn.ensemble\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold","256a704a":"#Tentando uma \u00c1rvore qualquer e uma floresta qualquer\ncv_res=cross_val_score(DecisionTreeClassifier(),x_train,y_train,cv=10)\nprint('\u00c1rvore',cv_res.mean())\ncv_res=cross_val_score(sklearn.ensemble.RandomForestClassifier(n_estimators=100),x_train,y_train,cv=10)\nprint('Floresta ale\u00e1t\u00f3ria', cv_res.mean())","91756544":"#Busca gridge demorando mais que o normal\n\n\n#clf=sklearn.ensemble.RandomForestClassifier()\n#kf=KFold(n_splits=3)\n#max_features=np.array([2,3,4,5])\n#n_estimators=np.array([25,50,100,150])\n#min_samples_leaf=np.array([50,75,100])\n#param_grid=dict(n_estimators=n_estimators,max_features=max_features,min_samples_leaf=min_samples_leaf)\n#grid=GridSearchCV(estimator=clf,param_grid=param_grid,cv=kf)\n#gres=grid.fit(x_train,y_train)\n#print(\"Melhor\",gres.best_score_)\n#print(\"Par\u00e2metros\",gres.best_params_)","8d8269d1":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(solver=\"adam\", alpha=0.0001, hidden_layer_sizes=(5,),\n                   random_state=1, learning_rate='constant', learning_rate_init=0.01,\n                   max_iter=50, activation='logistic', momentum=0.9,verbose=True,\n                   tol=0.0001)\ncv_res=cross_val_score(mlp,x_train,y_train,cv=10)\ncv_res.mean()","522ad2e5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nC = pd.read_csv(\"\/kaggle\/input\/atividade3\/train.csv\",\n        engine='python',\n        sep=r'\\s*,\\s*',\n        na_values=\"?\")","c0159ca8":"C.head()","eb311803":"#Criando variavel para guardar o target e retirando Id\ny_C=C.median_house_value\nC=C.drop(['Id','median_house_value'],axis=1)","eb69ccbf":"C.info()","22ba35fb":"C[\"persons\/bedrooms\"] = C[\"population\"]\/C[\"total_bedrooms\"]\nC[\"rooms\/households\"] = C[\"total_rooms\"]\/C[\"households\"]\nC=C.drop(['population','total_bedrooms','total_rooms','households'],axis=1)","471d9e68":"C.hist(bins=200, figsize=(25,20))","1df9bcc0":"plt.figure(figsize=(17,10))\nplt.xlabel('Pessoas por quarto')\nC[\"persons\/bedrooms\"].hist(bins=200, range=(0,6))","e44cf6ad":"plt.figure(figsize=(17,10))\nplt.xlabel('C\u00f4modos por casa')\nC[\"rooms\/households\"].hist(bins=200, range=(0,10))","6947ad84":"plt.figure(figsize=(10,10))\nplt.title(\"Matriz de correla\u00e7\u00e3o\")\nsns.heatmap(C.corr(), annot=True, linewidths=0.2)","d3f8d75f":"C['median_age'].describe()","12418027":"plt.figure(figsize=(17,10))\nplt.xlabel('Idade m\u00e9dia')\nC[\"median_age\"].hist(bins=200, range=(0,53))","9f6097d0":"from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostRegressor\nfrom sklearn import neural_network\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold","01e1c53e":"LR = Ridge()\ncv_res=cross_val_score(LR,C,y_C,cv=10)\ncv_res.mean()","58740a13":"LR = Lasso()\ncv_res=cross_val_score(LR,C,y_C,cv=10)\ncv_res.mean()","4280c75f":"neural_net = neural_network.MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam',learning_rate='adaptive', max_iter=800,learning_rate_init=0.01, warm_start = True, alpha=0.01)\ncv_res=cross_val_score(neural_net,C,y_C,cv=10)\ncv_res.mean()","86cb11e7":"## 6 - Terceiro Classificador - Rede Neural","8978977a":"### Em nosso Dataset, temos os seguintes features:\u00b6\n\n   - Id - Identifica\u00e7\u00e3o dos locais\n   - latitude: Latitude do local (em graus)\n   - longitude: Longitude do local\n   - median_age: Mediana das idades dos im\u00f3veis no local\n   - total_rooms: Contagem do n\u00famero de c\u00f4modos das casas na regi\u00e3o\n   - total_bedrooms: Contagem do total de quartos das casas na regi\u00e3o\n   - population: Popula\u00e7\u00e3o na regi\u00e3o\n   - households: N\u00famero total de casas na regi\u00e3o\n   - median_income: Mediana da renda das pessoas na regi\u00e3o\n   - median_house_value: Vari\u00e1vel Target.\n\nComo \u00e9 pass\u00edvel a ser reparado a vari\u00e1vel Id n\u00e3o impacta muito em nossa an\u00e1lise, por isso iremos retir\u00e1-la","741130e6":"## 5 - Segundo Classificador - Random Forest","e5e14611":"# Compara\u00e7\u00e3o de Classificadores na base Adult\n### Atividade 2 da disciplina PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Pad\u00f5es ","062ffa20":"## 4 - Conclus\u00e3o\nTodas as tr\u00eas regress\u00f5es obtiveram resultados bem semelhantes, sendo igualmente f\u00e1ceis de serem implementadas, no entanto apenas a rede neural tem sua interpretabilidade mais complicada de ser feita. Provavelmente, uma das poss\u00edveis maneiras de melhorar esses resultados seria implementando outras t\u00e9cnicas.","6c69866c":"Agora que terminou-se de modificar as vari\u00e1veis, a base ser\u00e1 estudada","e8f7e5e1":"### 3.2 Regress\u00e3o Lasso","7034c1d5":"### 3.1 Regress\u00e3o de Ridge","a7cc9046":"### 3.3 Rede Neural","10139b66":"# Exerc\u00edcio Programa extra\n## An\u00e1lise e Regress\u00e3o na base de dados California Pricing\n\n## 1 - Importando a base de dados e algumas bibliotecas","b334b290":"## 3 - Tratando as vari\u00e1veis da base","b48ec6eb":"## 4 - Primeiro classificador - Regrass\u00e3o Log\u00edstica com PCA","1a80a252":"Todas as vari\u00e1veis s\u00e3o num\u00e9ricas e tamb\u00e9m n\u00e3o h\u00e1 a prensen\u00e7a de valores faltantes, facilitando a prepara\u00e7\u00e3o de nossa base de dados\n\nDe modo a reduzir o sobre ajuste e reduzir as vari\u00e1veis iremos criar  rela\u00e7\u00f5es da popula\u00e7\u00e3o\/quarto e quartos\/casas\n","39a9f786":"Sendo bem percept\u00edvel no caso da classe 'native.country' existem vari\u00e1veis que possuem pequena vari\u00e2ncia, por isso continuaremos a estudar nossa base para poder remover essas colunas que podem criar sobre ajuste na base.","a315691a":"\u00c9 interessante notar que as novas duas vari\u00e1veis criadas tem seus valores centrados no meio, enquanto a idade m\u00e9dia das casas est\u00e1 centrada no meio do gr\u00e1fico, por\u00e9m h\u00e1 um pico em 52 anos.","b98550d9":"## 3 - Treinando os regressores e avaliando-os","bbc06daf":"\u00c9 interessante observar que quanto maior o n\u00edvel de educa\u00e7\u00e3o, maior \u00e9 a chance de ganhar acima de 50K","1601a2c0":"Como pode-se perceber, a base apresenta valores faltantes nas vari\u00e1veis 'workclass', 'occupation' e 'native country', por isso iremos estudar como elas se comportam.\nAl\u00e9m disso podemos separ\u00e1-las em vari\u00e1veis num\u00e9ricas e n\u00e3o num\u00e9ricas.","4ca164cc":"## 2 - Analisando a base e Engenharia de Features","71bf80f0":"## 2 - Analisando a base Adult","aed30222":"    Ap\u00f3s tratar os dados e tentar otimizar a base, foram utilizados os seguintes classificadores: Regress\u00e3o log\u00edstica, Floresta Aleat\u00f3ria e Rede Neural. \n    \n    De modo geral a que obteve o melhor desempenho dentre todas foi a Floresta Aleat\u00f3ria al\u00e9m de ter sido a mais f\u00e1cil a ser implementada, no entanto seu problema se localiza no tempo gasto para ser executada assim como o processamento necess\u00e1rio para procurar pelos melhores par\u00e2metros.\n    \n    J\u00e1 tanto a regress\u00e3o log\u00edstica quanto a rede neural obtiveram resultados semelhantes e desempenhos semelhantes ao se tratar do tempo gasto pra execu\u00e7\u00e3o dos respectivos algoritmos, por\u00e9m a regress\u00e3o \u00e9 mais facilmente interpretada.\n    \n    Creio que o desempenho da regress\u00e3o se deu em boa parte devido ao tratamento da base, que, ao lidar de forma diferente, seria poss\u00edvel um melhor resultado.","3344dac2":"## 1 - Importando biliotecas"}}