{"cell_type":{"ed1183cb":"code","a254e948":"code","3e37ec35":"code","80a4120c":"code","ad6b2a57":"code","ee8e5bcd":"code","ce05d214":"code","f98a1199":"code","84e1d8af":"code","e847ab85":"code","2149e31a":"code","74161ccc":"code","d910e08f":"code","38bbd5a5":"code","bb6c9364":"code","a69f5d41":"code","6c2ea1b1":"code","e2d7859c":"code","b6fff443":"code","c1130ac9":"code","e2d9c80a":"code","18ff5c29":"code","c3788dcf":"code","09668e32":"code","8fd749d9":"code","7e2e9246":"code","bc7dd10d":"code","55ee8e46":"code","02f3760a":"code","7685a5ab":"code","27325c91":"code","8508175f":"code","47f29049":"code","a69d81c7":"code","61b4dc27":"code","d1cb9af3":"code","40f3989d":"code","9c7edc60":"code","516a3df5":"code","198ea1c4":"code","62c7c1ac":"code","623d2591":"code","37edd357":"code","8f5d8c87":"code","9bc27d97":"code","259bab4a":"code","c768c8f7":"code","36c32884":"code","9584d6a1":"code","ea795b5e":"code","8cada0a8":"code","9967073c":"code","0ef988c7":"code","d6038e39":"code","9690b543":"code","e570b53c":"code","ddc3adb4":"code","15137d6c":"code","2ed1ab6d":"code","492c3118":"code","4efc1a40":"code","342eb320":"code","84ad32b0":"code","4307ff4b":"code","2f4d8038":"code","03af757f":"code","bece10f5":"code","0a8cf720":"code","bc96db2d":"code","d6970132":"code","b1dd6d0c":"code","65144ba5":"code","7d0454b6":"code","fbfd1816":"code","b4aab054":"code","296f12e5":"code","caccc27e":"code","4be9fdeb":"code","be0636d9":"code","b58f2dc1":"code","79cf0bdc":"code","b6c66809":"code","50a17105":"code","d260adb9":"code","240b0ecd":"code","aa4446e1":"code","654f29d6":"code","8f54cb80":"code","07a4505b":"code","f2d853e8":"code","2042aacf":"code","820cc622":"code","7542c329":"code","33a44c5a":"code","d00c0319":"code","14caf159":"code","4dc5af34":"code","f379be12":"code","64747de5":"code","fa4bf332":"code","0597a0e3":"code","19cb9b8e":"code","8f5ac513":"code","09b2dcd7":"code","49d54164":"code","789c1ef5":"code","54cc6c58":"code","2830faf3":"code","6de55f8d":"code","b430efed":"code","bb701fe1":"code","f56f9484":"code","4e0cde19":"code","4af75576":"code","1ec33787":"code","d7c1c8d1":"code","a9e8494d":"code","24bb638d":"code","f953c3d1":"code","8898a1c1":"code","4b7f7cef":"code","4b891905":"code","8442759c":"code","1cb79c1f":"code","95adc9a0":"code","a57e61f9":"code","6011d9f8":"code","bb65235b":"code","ff28e219":"code","8bc68d88":"code","29a7f2e8":"code","b269077d":"code","12356178":"code","e5679340":"code","1044dde7":"code","497edd79":"code","c3ffc958":"code","39ca1c7c":"code","09277dd2":"code","c0ac9e34":"code","f2aa9821":"code","5b0c8fa6":"code","57e86301":"code","85177ef0":"code","e251fcd7":"code","85fcc123":"code","058f637f":"code","fb063626":"code","3c640507":"code","a07a6a0d":"code","3ba61f13":"code","e9ce74f8":"code","5f3149a2":"code","1d55bdca":"code","790978f9":"code","9a4c8d10":"code","53950242":"code","a0a5b7dc":"code","0f0da1dc":"code","9b2fbeac":"markdown","8be175de":"markdown","e5c61c41":"markdown","2f4d9bdf":"markdown","fd8038a3":"markdown","71d1d929":"markdown","dc99c2b5":"markdown","8adbc5bb":"markdown","27dee64f":"markdown","71ed180b":"markdown","a513f512":"markdown","93c31551":"markdown","4adbef70":"markdown","6e23ec27":"markdown","c8401c31":"markdown","1cdca910":"markdown","7db51498":"markdown","2441b04b":"markdown","dd4536c8":"markdown","39bad88e":"markdown","c23efe2a":"markdown","812d63fb":"markdown","db912ec1":"markdown","dd424268":"markdown","72d35ed3":"markdown","1841c22f":"markdown","6bb53b01":"markdown","0d405358":"markdown","1f934e6f":"markdown","2088dc2b":"markdown","fecda958":"markdown","4944c83f":"markdown","aa359f98":"markdown","c8a12fbb":"markdown","66f095e6":"markdown","2a95e3f2":"markdown","5571c0c0":"markdown","deedc6de":"markdown","a0d4049e":"markdown","62097f6a":"markdown","487b885d":"markdown","bee48743":"markdown","aec98379":"markdown","0b16b97e":"markdown","6a131aee":"markdown","9883aebe":"markdown","3dc587ce":"markdown","c193d360":"markdown","264f7833":"markdown","8be4f42f":"markdown","483571bc":"markdown","05a48b1e":"markdown","aa3a3775":"markdown","7fcefd46":"markdown","afec025a":"markdown","17ea887c":"markdown","c6c37c82":"markdown","39646334":"markdown","80e0ce45":"markdown","caba20ce":"markdown","45a4d83a":"markdown","a5a1f94c":"markdown","c5ea85c2":"markdown","96ac53cf":"markdown"},"source":{"ed1183cb":"#Ignore warnings \nimport warnings\nwarnings.filterwarnings('ignore')","a254e948":"#Import the necessary libraries\n#Basic Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# setting restriction on the number of rows and coulmns displayed in output\npd.set_option('display.max_columns',999)\npd.set_option('display.max_rows',200)\n\n# Libraries for data modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve","3e37ec35":"# Creating a class color for setting print formatting\nclass color:\n   BLUE = '\\033[94m'\n   BOLD = '\\033[1m'\n   END = '\\033[0m'","80a4120c":"#Read and Understand the data\n\ndf = pd.read_csv(\"..\/input\/leadsscore\/Leads.csv\")\nprint(color.BOLD+color.BLUE+'Shape of the dataframe df : {}'.format(df.shape) +color.END)\n","ad6b2a57":"df.head()","ee8e5bcd":"df.describe()","ce05d214":"df.info()","f98a1199":"#Replacing the SELECT value with null value\ndf = df.replace('Select',np.nan)","84e1d8af":"#Checking the number of missing values and its percentage\nTotal_missing = df.isnull().sum().sort_values(ascending = False)\nTotal_missing_Perc = (100*df.isnull().sum()\/df.shape[0]).sort_values(ascending = False)\ndf_missing_values = pd.concat([Total_missing,Total_missing_Perc], axis=1, keys=['Total_missing_values', 'Percent_missing_values'])\ndf_missing_values.head(30)","e847ab85":"# Checking how many columns have more than 45% of missing data\n\nprint(color.BOLD+ color.BLUE+'Total no of columns with missing values more than 45% : {}'.format(df_missing_values[df_missing_values['Percent_missing_values'] >= 45].shape[0])+ color.END)","2149e31a":"#Create a new dataframe named df_cleaned with all columns with data misisng < 45% for our  further analysis\n\ndf_cleaned = df.loc[:,(100*df.isnull().sum()\/df.shape[0]).sort_values(ascending = False) < 45]\ndf_cleaned.head()","74161ccc":"#Checking the columns present in dataframe\ndf_cleaned.columns","d910e08f":"#Dropping Prospect ID and Lead Number and Tags columns as these columns are scroe variables created by the sales team and may not be available.\ndf_cleaned = df_cleaned.drop(['Prospect ID','Lead Number','Tags'],axis=1)","38bbd5a5":"#Checking the no of unique values in each column\ndf_cleaned.nunique()","bb6c9364":"#Dropping all columns with one unique value\n\ndf_cleaned.drop(['Magazine','Receive More Updates About Our Courses','Update me on Supply Chain Content','Get updates on DM Content','I agree to pay the amount through cheque'], axis=1,inplace = True)\n","a69f5d41":"#Verifying the unique values for each columns again to confirm\ndf_cleaned.nunique()","6c2ea1b1":"# Get the value counts of all the columns\n\nfor column in df_cleaned:\n    \n    print(df_cleaned[column].astype('category').value_counts())\n    print('___________________________________________________')","e2d7859c":"# Defining the map function\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\nvarlist =  ['Do Not Email', 'Do Not Call','Search','X Education Forums','Newspaper Article','Newspaper','Digital Advertisement','Through Recommendations','A free copy of Mastering The Interview']\n\n# applying the map function\ndf_cleaned[varlist] = df_cleaned[varlist].apply(binary_map)","b6fff443":"# Checking the missing value columns\nTotal_missing = df_cleaned.isnull().sum().sort_values(ascending = False)\nTotal_missing_Perc = (100*df.isnull().sum()\/df.shape[0]).sort_values(ascending = False)\ndf_missing_values = pd.concat([Total_missing,Total_missing_Perc], axis=1, keys=['Total_missing_values', 'Percent_missing_values'])\ndf_missing_values.head(10)","c1130ac9":"# Data imputation for column 'City'\n\nplt.figure(figsize=(8,12))\nsns.countplot(df_cleaned.City)\nplt.title(\"Checking the mode of column city- UniVariate Analysis\")\nplt.show()","e2d9c80a":"df_cleaned.City.value_counts()","18ff5c29":"s = df_cleaned.City.value_counts()\nax=s.plot.bar(width=.8) \n\nfor i, v in s.reset_index().iterrows():\n    ax.text(i, v.City , v.City, color='green')","c3788dcf":"# Most of the leads are from Mumbai , so we can map the NAN values of 'City' to Mumbai\ndf_cleaned['City'] = df_cleaned['City'].replace(np.nan,'Mumbai')","09668e32":"# Data imputation for column 'Specialization'\n\nplt.figure(figsize=(25,10))\nsns.countplot(df_cleaned.Specialization)\nplt.xticks(rotation=90)\nplt.title(\"Checking the mode of column Specialization-UniVariate Analysis\")\nplt.show()\n","8fd749d9":"df_cleaned.Specialization.value_counts()","7e2e9246":"# As the specialization is distributed across multiple values ,we can impute it with mode and therefore we can put the Nan values as 'Unknown'\n\ndf_cleaned['Specialization']=df_cleaned['Specialization'].replace(np.nan,'Unknown')","bc7dd10d":"# Data imputation for column 'What matters most to you in choosing a course'\n\nplt.figure(figsize=(12,10))\nsns.countplot(df_cleaned['What matters most to you in choosing a course'])\nplt.xticks(rotation=90)\nplt.title(\"Checking the mode of column 'What matters most to you in choosing a course'-UniVariate Analysis\")\nplt.show()\n","55ee8e46":"df_cleaned['What matters most to you in choosing a course'].value_counts()","02f3760a":"# Most of the values here is related to a single value 'Better Career Prospects' so we can map the NAN values to Better Career Prospects\n\ndf_cleaned['What matters most to you in choosing a course']=df_cleaned['What matters most to you in choosing a course'].replace(np.nan,'Better Career Prospects')","7685a5ab":"\n# Data imputation for column 'What is your current occupation'\n\nplt.figure(figsize=(12,10))\nsns.countplot(df_cleaned['What is your current occupation'])\nplt.xticks(rotation=90)\nplt.title(\"Checking the mode of column 'What is your current occupation'-UniVariate Analysis\")\nplt.show()\n","27325c91":"df_cleaned['What is your current occupation'].value_counts()","8508175f":"# Replace all the Nan values with 'Unemployed' as it is the majority\n\ndf_cleaned['What is your current occupation']=df_cleaned['What is your current occupation'].replace(np.nan,'Unemployed')","47f29049":"# Data imputation for column 'Country'\n\nplt.figure(figsize=(25,10))\nsns.countplot(df_cleaned['Country'])\nplt.xticks(rotation=90)\nplt.title(\"Checking the mode of column 'Country'-UniVariate Analysis\")\nplt.show()\n","a69d81c7":"df_cleaned['Country'].value_counts()","61b4dc27":"#Replacing all the Nan values with India as it is the majority\n\ndf_cleaned['Country']=df_cleaned['Country'].replace(np.nan,'India')","d1cb9af3":"# As the columns TotalVisits, Page Views Per Visit , Last Activity and Lead Source have less than 2% of Nan Values we chose to drop those rows .\n\ndf_cleaned.dropna(inplace=True)","40f3989d":"#Final check of missing values \nTotal_missing = df_cleaned.isnull().sum().sort_values(ascending = False)\nTotal_missing_Perc = (100*df_cleaned.isnull().sum()\/df_cleaned.shape[0]).sort_values(ascending = False)\ndf_missing_values = pd.concat([Total_missing,Total_missing_Perc], axis=1, keys=['Total_missing_values', 'Percent_missing_values'])\ndf_missing_values","9c7edc60":"# Number of rows retained after data cleaning\n\nprint(round(100*(df_cleaned.shape[0] \/ df.shape[0]),2))\n            ","516a3df5":"#Check the shape of cleaned dataframe\ndf_cleaned.shape","198ea1c4":"# Get the value counts of all the columns\n\nfor column in df_cleaned:\n    \n    print(df_cleaned[column].astype('category').value_counts())\n    print('___________________________________________________')","62c7c1ac":"df_cleaned=df_cleaned.replace(to_replace=['bing','google','Click2call','Press_Release','Social Media','Live Chat','Pay per Click Ads','welearnblog_Home','NC_EDM','WeLearn','blog','testone','youtubechannel'],\n           value= 'Others')","623d2591":"df_cleaned['Lead Source'].value_counts()","37edd357":"df_cleaned=df_cleaned.replace(to_replace=['Email Marked Spam','View in browser link Clicked','Resubscribed to emails','Form Submitted on Website','Email Received','Approached upfront'],\n           value= 'Others')","8f5d8c87":"df_cleaned['Last Notable Activity'].value_counts()","9bc27d97":"#First we will check the numerical variables against converted.\nsns.set(font_scale=1)\nplt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'Converted', y = 'TotalVisits', data = df_cleaned)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'Converted', y = 'Total Time Spent on Website', data = df_cleaned)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'Converted', y = 'Page Views Per Visit', data = df_cleaned)\nplt.show()","259bab4a":"# outlier treatment for TotalVisits\n#PLots for outlier analysis\nsns.boxplot(df_cleaned.TotalVisits)\nplt.show()\n\n# Defining outlier treatment function  \ndef outlier_treatment(datacolumn):\n sorted(datacolumn)\n Q1,Q3 = np.percentile(datacolumn , [25,75])\n IQR = Q3 - Q1\n lower_range = Q1 - (1.5 * IQR)\n upper_range = Q3 + (1.5 * IQR)\n return lower_range,upper_range\n\n#Calculating IQR\nlowerbound,upperbound = outlier_treatment(df_cleaned.TotalVisits)\nprint(lowerbound,upperbound)\n\n#check outliers for the TotalVisits\nclmn = 'TotalVisits'\nTotal_no_of_outliers = df_cleaned[(df_cleaned['TotalVisits'] > upperbound) | (df_cleaned['TotalVisits'] < lowerbound)] .shape[0]\n\nprint(color.BOLD + color.BLUE + 'Total no of outliers for column {0} : {1}'.format(clmn,Total_no_of_outliers ) + color.END)\n\n#calculate % of outliers in the data\n100*Total_no_of_outliers\/df_cleaned.shape[0]","c768c8f7":"#Delete the TotalVisits outliers from the dataset as it is very less\ndf_cleaned = df_cleaned[(df_cleaned['TotalVisits'] <= upperbound) & (df_cleaned['TotalVisits'] >= lowerbound )]","36c32884":"\n#PLots for outlier analysis - Total Time Spent on Website\nsns.boxplot(df_cleaned['Total Time Spent on Website'])\nplt.show()\n\n\n# Calculating IQR \nlowerbound,upperbound = outlier_treatment(df_cleaned['Total Time Spent on Website'])\n\nprint(lowerbound,upperbound)\n\n#check outliers for the Total_Time_Spent_on_Website\nclmn = 'Total Time Spent on Website'\nTotal_no_of_outliers = df_cleaned[(df_cleaned['Total Time Spent on Website'] > upperbound) | (df_cleaned['Total Time Spent on Website'] < lowerbound)] .shape[0]\n\nprint(color.BOLD + color.BLUE + 'Total no of outliers for column {0} : {1}'.format(clmn,Total_no_of_outliers ) + color.END)\n\n#calculate % of outliers in the data\n100*Total_no_of_outliers\/df_cleaned.shape[0]","9584d6a1":"#Delete the 'Total Time Spent on Website' outliers from the dataset as it is very less \ndf_cleaned = df_cleaned[(df_cleaned['Total Time Spent on Website']<= upperbound) & (df_cleaned['Total Time Spent on Website'] >= lowerbound )]","ea795b5e":"#PLots for outlier analysis - Page Views Per Visit\nsns.boxplot(df_cleaned['Page Views Per Visit'])\nplt.show()\n\n\n# Calculating IQR \nlowerbound,upperbound = outlier_treatment(df_cleaned['Page Views Per Visit'])\n\nprint(lowerbound,upperbound)\n\n#check outliers for the Page Views Per Visit\nclmn = 'Page Views Per Visit'\nTotal_no_of_outliers = df_cleaned[(df_cleaned['Page Views Per Visit'] > upperbound) | (df_cleaned['Page Views Per Visit'] < lowerbound)] .shape[0]\n\nprint(color.BOLD + color.BLUE + 'Total no of outliers for column {0} : {1}'.format(clmn,Total_no_of_outliers ) + color.END)\n\n#calculate % of outliers in the data\n100*Total_no_of_outliers\/df_cleaned.shape[0]","8cada0a8":"#Delete the 'Total Time Spent on Website' outliers from the dataset as it is very less\ndf_cleaned = df_cleaned[(df_cleaned['Page Views Per Visit']<= upperbound) & (df_cleaned['Page Views Per Visit'] >= lowerbound )]","9967073c":"\nplt.figure(figsize=(20, 7))\nplt.subplot(1,3,1)\nsns.boxplot(x = 'Converted', y = 'TotalVisits', data = df_cleaned)\nplt.title(\"TotalVisits vs Converted\")\nplt.subplot(1,3,2)\nsns.boxplot(x = 'Converted', y = 'Total Time Spent on Website', data = df_cleaned)\nplt.title(\"Total Time spent on Website vs Converted\")\nplt.subplot(1,3,3)\nsns.boxplot(x = 'Converted', y = 'Page Views Per Visit', data = df_cleaned)\nplt.title(\"Page views per visit vs Converted\")\nplt.show()","0ef988c7":"\nsns.catplot(x = \"TotalVisits\", hue = \"Converted\", data = df_cleaned, kind = \"count\") #, aspect =3\nplt.show()\ng = sns.catplot(x = \"Page Views Per Visit\", hue = \"Converted\", data = df_cleaned, kind = \"count\", aspect =2) \ng.set_xticklabels(rotation=90)\nplt.show()\n","d6038e39":"plt.figure(figsize = (60,60)) \nsns.catplot(x = \"Lead Origin\", hue = \"Converted\", data = df_cleaned, kind = \"count\", aspect =1.5)\nplt.show()\ng = sns.catplot(x = \"Lead Source\", hue = \"Converted\", data = df_cleaned, kind = \"count\", aspect =3)\ng.set_xticklabels(rotation=90)\nplt.show()\nsns.catplot(x = \"Do Not Email\", hue = \"Converted\", data = df_cleaned, kind = \"count\")#, aspect =3.5)\nplt.show()\nsns.catplot(x = \"Do Not Call\", hue = \"Converted\", data = df_cleaned, kind = \"count\")#, aspect =3.5)\nplt.show()\ng = sns.catplot(x = \"Last Activity\", hue = \"Converted\", data = df_cleaned, kind = \"count\", aspect =3)\ng.set_xticklabels(rotation=90)\nplt.show()\ng = sns.catplot(x = \"Country\", hue = \"Converted\", data = df_cleaned, kind = \"count\", aspect =3)\ng.set_xticklabels(rotation=90)\nplt.show()\ng = sns.catplot(x = \"Specialization\", hue = \"Converted\", data = df_cleaned, kind = \"count\", aspect =2)\ng.set_xticklabels(rotation=90)\nplt.show()\ng = sns.catplot(x = \"What is your current occupation\", hue = \"Converted\", data = df_cleaned, kind = \"count\")\ng.set_xticklabels(rotation=90)\nplt.show()\ng = sns.catplot(x = \"What matters most to you in choosing a course\", hue = \"Converted\", data = df_cleaned, kind = \"count\")\ng.set_xticklabels(rotation=90)\nplt.show()\ng = sns.catplot(x = \"Search\", hue = \"Converted\", data = df_cleaned, kind = \"count\")#, aspect = 3)\nplt.show()\ng = sns.catplot(x = \"Newspaper Article\", hue = \"Converted\", data = df_cleaned, kind = \"count\")#, aspect = 3)\nplt.show()\ng = sns.catplot(x = \"X Education Forums\", hue = \"Converted\", data = df_cleaned, kind = \"count\")#, aspect = 3)\nplt.show()\ng = sns.catplot(x = \"Newspaper\", hue = \"Converted\", data = df_cleaned, kind = \"count\")#, aspect = 3)\nplt.show()\ng = sns.catplot(x = \"Digital Advertisement\", hue = \"Converted\", data = df_cleaned, kind = \"count\")#, aspect = 3)\nplt.show()\ng = sns.catplot(x = \"Through Recommendations\", hue = \"Converted\", data = df_cleaned, kind = \"count\")#, aspect = 3)\nplt.show()\ng = sns.catplot(x = \"City\", hue = \"Converted\", data = df_cleaned, kind = \"count\", aspect = 3)\nplt.show()\ng = sns.catplot(x = \"A free copy of Mastering The Interview\", hue = \"Converted\", data = df_cleaned, kind = \"count\")#, aspect = 3)\nplt.show()\ng = sns.catplot(x = \"Last Notable Activity\", hue = \"Converted\", data = df_cleaned, kind = \"count\", aspect = 3)\ng.set_xticklabels(rotation=90)\nplt.show()","9690b543":"#Dropping the columns which doesnt give much inferences\ndf_cleaned = df_cleaned.drop(['What matters most to you in choosing a course','Search','Newspaper Article','X Education Forums','Newspaper',\n           'Digital Advertisement','Through Recommendations','Page Views Per Visit','TotalVisits','Last Activity','A free copy of Mastering The Interview','Country'],1)","e570b53c":"# Final columns present in a cleaned data frame\ndf_cleaned.columns","ddc3adb4":"# Creating dummies\ndummy = pd.get_dummies(df_cleaned[['Lead Origin','Lead Source','Specialization','What is your current occupation','City',\n                                   'Last Notable Activity']], drop_first=True)\ndummy.head()","15137d6c":"# Adding the results back to the cleaned dataframe\ndf_cleaned = pd.concat([df_cleaned, dummy], axis=1)\ndf_cleaned.head()","2ed1ab6d":"# We have created dummies for the below variables, so we can drop them\n# Dropping the columns whose dummies have been created\ndf_cleaned=df_cleaned.drop(['Lead Origin','Lead Source','Specialization','What is your current occupation','City','Last Notable Activity'], axis = 1)","492c3118":"df_cleaned.head()","4efc1a40":"df_cleaned.shape","342eb320":"# Putting feature variable to X\nX = df_cleaned.drop(['Converted'], axis=1)\nX.head()","84ad32b0":"# Putting response variable to y\ny = df_cleaned['Converted']\n\ny.head()","4307ff4b":"# Splitting the data into train and test:\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","2f4d8038":"scaler = StandardScaler()\n\nX_train[['Total Time Spent on Website']] = scaler.fit_transform(X_train[['Total Time Spent on Website']])\n\nX_train.head()","03af757f":"# Plotting the the correlation matrix \nplt.figure(figsize = (60,60))        # Size of the figure\nsns.heatmap(df_cleaned.corr(),annot = True)\nplt.show()","bece10f5":"#Checking the present lead conversion rate\nconvert = (sum(df_cleaned['Converted'])\/len(df_cleaned['Converted'].index))*100\nconvert","0a8cf720":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","bc96db2d":"logreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 20)             # running RFE with 20 variables as output\nrfe = rfe.fit(X_train, y_train)","d6970132":"rfe.support_","b1dd6d0c":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","65144ba5":"col = X_train.columns[rfe.support_]","7d0454b6":"X_train.columns[~rfe.support_]","fbfd1816":"#Model 2\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","b4aab054":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","296f12e5":"col = col.drop('What is your current occupation_Housewife', 1)","caccc27e":"# Let's re-run the model using the selected variables\n#Model 3\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","4be9fdeb":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","be0636d9":"col = col.drop('Last Notable Activity_Had a Phone Conversation', 1)","b58f2dc1":"# Let's re-run the model using the selected variables\n#Model 4\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","79cf0bdc":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b6c66809":"col = col.drop('Lead Origin_Lead Import', 1)","50a17105":"# Let's re-run the model using the selected variables\n#Model 5\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","d260adb9":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","240b0ecd":"col = col.drop('Lead Origin_Lead Add Form', 1)","aa4446e1":"# Let's re-run the model using the selected variables\n#Model 6\nX_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","654f29d6":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8f54cb80":"col = col.drop('What is your current occupation_Unemployed', 1)","07a4505b":"# Let's re-run the model using the selected variables\n#Model 7\nX_train_sm = sm.add_constant(X_train[col])\nlogm7 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm7.fit()\nres.summary()","f2d853e8":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","2042aacf":"col = col.drop('What is your current occupation_Student', 1)","820cc622":"# Let's re-run the model using the selected variables\n#Model 8\nX_train_sm = sm.add_constant(X_train[col])\nlogm8 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm8.fit()\nres.summary()","7542c329":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","33a44c5a":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","d00c0319":"y_train_pred[:10]","14caf159":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_Prob':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","4dc5af34":"y_train_pred_final['predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\n# Let's see the head\ny_train_pred_final.head()","f379be12":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","64747de5":"print(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","fa4bf332":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","0597a0e3":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","19cb9b8e":"# Let us calculate specificity\nTN \/ float(TN+FP)","8f5ac513":"# Calculate false postive rate - predicting lead when it is not a lead\nprint(FP\/ float(TN+FP))","09b2dcd7":"# positive predictive value \nprint (TP \/ float(TP+FP))","49d54164":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","789c1ef5":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","54cc6c58":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_Prob, drop_intermediate = False )","2830faf3":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","6de55f8d":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","b430efed":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","bb701fe1":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","f56f9484":"y_train_pred_final['final_predicted'] = y_train_pred_final.Converted_Prob.map( lambda x: 1 if x > 0.35 else 0)\n\ny_train_pred_final.head()","4e0cde19":"y_train_pred_final['Lead_Score'] = y_train_pred_final.Converted_Prob.map( lambda x: round(x*100))\n\ny_train_pred_final.head()","4af75576":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","1ec33787":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","d7c1c8d1":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","a9e8494d":"# sensitivity of our logistic regression model\nTP \/ float(TP+FN)","24bb638d":"#specificity\nTN \/ float(TN+FP)","f953c3d1":"# Calculate false postive rate - predicting lead converted when lead does not have converted\nprint(FP\/ float(TN+FP))","8898a1c1":"# Positive predictive value \nprint (TP \/ float(TP+FP))","4b7f7cef":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","4b891905":"#confusion matrix\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion","8442759c":"confusion[1,1]\/(confusion[0,1]+confusion[1,1])","1cb79c1f":"confusion[1,1]\/(confusion[1,0]+confusion[1,1])","95adc9a0":"precision_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)\n","a57e61f9":"recall_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","6011d9f8":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","bb65235b":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","ff28e219":"X_test[['Total Time Spent on Website']] = scaler.transform(X_test[['Total Time Spent on Website']])","8bc68d88":"X_test = X_test[col]\nX_test.head()","29a7f2e8":"X_test_sm = sm.add_constant(X_test)","b269077d":"y_test_pred = res.predict(X_test_sm)","12356178":"y_test_pred[:10]","e5679340":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","1044dde7":"# Let's see the head\ny_pred_1.head()","497edd79":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","c3ffc958":"# Putting Prospect ID to index\ny_test_df['Prospect ID'] = y_test_df.index","39ca1c7c":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","09277dd2":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","c0ac9e34":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_Prob'})","f2aa9821":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex(['Prospect ID','Converted','Converted_Prob'], axis=1)","5b0c8fa6":"# Let's see the head of y_pred_final\ny_pred_final.head()","57e86301":"y_pred_final['final_predicted'] = y_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.35 else 0)","85177ef0":"y_pred_final.head()","e251fcd7":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)","85fcc123":"#Lead Score on Test data\ny_pred_final['Lead_Score'] = y_pred_final.Converted_Prob.map( lambda x: round(x*100))\n\ny_pred_final.head()","058f637f":"#confusion matrix\nconfusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","fb063626":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","3c640507":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","a07a6a0d":"# Let us calculate specificity\nTN \/ float(TN+FP)","3ba61f13":"# Calculate false postive rate - predicting lead converted when lead does not have converted\nprint(FP\/ float(TN+FP))","e9ce74f8":"# Positive predictive value \nprint (TP \/ float(TP+FP))","5f3149a2":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","1d55bdca":"confusion[1,1]\/(confusion[0,1]+confusion[1,1])","790978f9":"confusion[1,1]\/(confusion[1,0]+confusion[1,1])","9a4c8d10":"precision_score(y_pred_final.Converted, y_pred_final.final_predicted)","53950242":"recall_score(y_pred_final.Converted, y_pred_final.final_predicted)","a0a5b7dc":"y_train_pred_final","0f0da1dc":"y_pred_final","9b2fbeac":"## Categorical variables against converted","8be175de":"The column 'Last Notable Activity' 16 categories and about 6 cateorgories are having single digit values. So we chose to combine the categories into a single category named others as this Others category may give better insights than so many categories.\n","e5c61c41":"Using sklearn utilities for the same","2f4d9bdf":"## Precision and Recall","fd8038a3":"## Precision and recall tradeoff","71d1d929":"## BiVariate Analysis ","dc99c2b5":"\n   # <b><center><font size=\"15\"><span style='font-family:\"Times New Roman\"'> LEAD SCORING CASE STUDY <\/span><\/font><\/center><\/b>\n\n***\n ","8adbc5bb":"An ROC curve demonstrates several things:\n\nIt shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\nThe closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\nThe closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","27dee64f":"## Importing Leads dataset","71ed180b":"Converting the values Yes and No to 1 and 0.- Binary mapping","a513f512":"## Test-Train Split","93c31551":"## Accuracy might always not be the right metric. So calculating metrics beyond metric.\n- Sensitivity\n- Specificity\n- false postive rate\n- positive predictive value \n- Negative predictive value","4adbef70":"Inference:\n\nOut of the 3 numerical variables TotalVisits and Page_Views_Per_visit doesnt show much effect on conversion rate, but Total_Time_Spent_on_Website seems to have an impact on coversion rate with more time spend leading to more likelier conversion.","6e23ec27":"### Final Train and Test Sets","c8401c31":"The column 'Lead Source' 20 categories and about 13 cateorgories are having single digit values. So we chose to combine the categories into a single category named others as this Others category may give better insights than so many categories.\n","1cdca910":"## Data Imputation - For the columns with less percentage of missing values (<45) ","7db51498":"## Checking VIFs","2441b04b":"## Precision on Test data\nTP \/ TP + FP","dd4536c8":"## Precision\nTP \/ TP + FP","39bad88e":"### Inferences :","c23efe2a":"We find that these columns have outliers","812d63fb":"The p value of column 'Lead Origin_Lead Import\" is very high. So dropping it","db912ec1":"1. Lead Origin : API and Landing page submission has high non converted count where as lead ad form has high converted value\n2. Lead Source : All the Categories have high non converted count except references and welingak website have high converted count\n3. Donot Email : Do not email is an sn indicator variable selected by the customer wherein they select whether or not they want to be emailed about the course or not.\nThe conversion rate is low in both 'yes' and 'no' cases.\n4. Do not call : Do not call column is an indicator variable selected by the customer wherein they select whether of not they want to be called about the course or not. The conversion rate is low.\n5. Last Activity : All the Categories have high non converted count except sms sent have high converted count\n6. Country : Data values are highly skewed towards category India\n7. Specialization : Seems many courses like 'buisiness adminitration','banking investment and insurance','finance management' and few \nothers have high convertion\n8. What is your current occupation : The conversion rate is very high for working professionals but count is less. So this course should be advertised more to working professionals to increase the count.\n9. What matters most to you in choosing a course : Almost all the people are opting the course for better career prospects.But the conversion rate is low. So more efforts should be put in designing the structure of the course.\n10. Search,Newspaper Article,X Education forums,Newspaper,Digital Advertisement,Through Recommendations : All the above graphs indicating whether the customer had seen the through any of the above columns.(How did the lead get to know about the course).\nAll the leads found out about the course on their own and not through any of the above columns\/items.\n11. City : The conversion rates is low for all the cities but 'Mumbai' has the highest count. So more focus should be put in this city to improve the conversion rate.\n12. A free copy of mastering the interview : Nothing can be inferred from this as the data is evenly distributed between yes and no\n13. Last Notable Activity : The conversion rate is high for 'SMS Sent'.    ","dd424268":"## Plotting the ROC Curve","72d35ed3":"Making predictions on the test set","1841c22f":"The p value of column 'Last Notable Activity_Had a Phone Conversation\" is very high. So dropping it","6bb53b01":"## Data Quality Check and Missing values","0d405358":"Identifying the categorical columns with many categories and merging two or more categories of a column into a category named \"Others\"","1f934e6f":"We are chosing to drop these 7 columns which have more than 45% of the missing data.","2088dc2b":"Few columns have \"Select\" as there values. This gives us the information that either the user havent selected any option or chose not to select. So these values gives us the same infomation as the \"null\" values. Hence we are converting Select into NULL values","fecda958":"The above logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. Adding of a lead score column to the final model which is populated by multiplying the conversion probability value with 100 so that we get the score between 0-100.\n\n1. The business can calculate the Lead score of each candidate using the optimal cut off and if the lead score is greater than 35 (as the cut off probability is 0.35) then that candidate can be considered as a Hot Lead and the sales team can concentrate more on that person to convert him into a potential paying customer.\n2. To increase the lead score of the candidate the business can target the Lead source feature and try to increase the count of categories Welingak Website and Reference as it has more potential leads. Moreover, the business can target on working professionals as they can turn into possible prospect lead. The business needs to target less on leads who have selected the \u2018Do not email\u2019 option as there is a negative coefficient and this indicates that it doesn\u2019t help the business.\n3. If there is business decides to the increase the budget for sales team then it can reduce the probability cut off from 0.35 i.e., can target customers with lead score less than 35 as this change would lead to an increase in the targeted customers, and it might result in high lead conversion rate in turn improving the profits of the business.\n4. And if the business wants to concentrate on any other vertical and intends to reduce the sales team work (thereby diverting the employees towards a new vertical, if any) then it can choose to increase the cut off from 0.35 to a higher one. This strategy can be applied when the business has reached its target and is willing to accept less lead conversion rate. During the application of this strategy, make sure that you target at least those with lead source of welingak and reference categories and working professionals as these have very high prospects of getting hot leads and potential paying customers. \n\nTo conclude the conversion rate of the leads is 80% when the lead with lead score 38 and above is targeted.\n","4944c83f":"## Visualising the Data\n\nWe would need to analyse the distribution of various values of all the above considered fields against the \"Converted\" value .\n\nIntent is to check how  the convertion column is really dependent on the variable","aa359f98":"Optimal cutoff probability is that probability where we get balanced sensitivity and specificity","c8a12fbb":"Assessing the model with StatsModels","66f095e6":"<u><font size=\"6\"><span style='font-family:\"Times New Roman\"'>Problem Statement<\/span><\/font><\/u>\n\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses and fill the form if interested with their details. The company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. We need to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers.\n\n<u><font size=\"6\"><span style='font-family:\"Times New Roman\"'>Solution Approach<\/span><\/font><\/u>\n\nBuild a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. \n\n<u><font size=\"6\"><span style='font-family:\"Times New Roman\"'>Model Objective<\/span><\/font><\/u>\n\nBuild a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.\n\n","2a95e3f2":"### Mergings categorical values ","5571c0c0":"We are not mergings the values of column 'Country' as it has many single digit categories and by mergins them the count of others category may exceed drastically and may have a wrong impact on our analysis","deedc6de":"## Feature Scaling","a0d4049e":"   # <u><b><font size=\"6\"><span style='font-family:\"Times New Roman\"'> Summary Report <\/span><\/font><b><u>\n\n","62097f6a":"<u><font size=\"6\"><span style='font-family:\"Times New Roman\"'>Detailed Steps Followed while building the model<\/span><\/font><\/u>\n\n1.\tFirstly, we have imported the data and the libraries needed to build this model\n2.\tData Cleaning:\n    - Firstly, converted the columns have \"Select\" as their values to null as this gives us the information that either the user hasn\u2019t selected any option or chose not to sele ct.\n    - Have dropped the columns with greater than 45% missing values and even with one unique value\n    - Performed binary mapping\n    - Used the appropriate data imputation technique for other columns with missing values.\n3.\tData Analysis:\n    - Identified the outlier data and handled it.\n    - Performed the EDA on all the columns and have plotted them against \u2018Converted\u2019 and inferred which columns might be suitable for further analysis\n4.\tData Preparation:\n    - Have created a dummy variable for the categorical columns\n    - Performed the test train split\n    - Feature scaling have been done\n5.\tHave started building the logistic regression model and using RFE technique chose the top 20 variables which will give more accurate results.\n6.\tDropping the columns p values greater than 0.05 and VIF greater than 2.5\n7.\tCalculated the model evaluation parameters like accuracy, sensitivity, specificity, Precision and recall\n8.\tThe optimal cut off was 0.35 so we have considered it as a cut off probability\n9.\tMake predictions on test data and using the same threshold value of 0.35 predicted if the lead is converted or not.\n10.\tAdding of a lead score column to the final model which is populated by multiplying the conversion probability value with 100 so that we get the score between 0-100.\n\n","487b885d":"## Creating dummy variables for categorical variables:\n","bee48743":"### From the curve above, 0.35 is the optimum point to take it as a cutoff probability.","aec98379":"The VIF value of 'What is your current occupation_Unemployed' is very high. So dropping it","0b16b97e":"### All variables are now significant and VIF is below 2.5 for all variables.","6a131aee":"## Based on the analysis above some columns are providing very less information so dropping them would be ideal for our data model.","9883aebe":"Missing value imputation for each column \n\nAs these are categorical column the best imputation method would be Mode","3dc587ce":"The p value of column \"What is your current occupation_Housewife\" is very high. So dropping it","c193d360":"## Import all the necessary libraries","264f7833":"## Feature Selection Using RFE","8be4f42f":"The VIF value of 'Lead Origin_Lead Add Form' is very high. So dropping it","483571bc":"In general we assume that lead is converted(1) if conv_prob is greater than 0.5\n\nCreating new column 'predicted' with 1 if Conv_Prob > 0.5 else 0","05a48b1e":"Getting the predicted valueson the train set","aa3a3775":"## Looking at correlations","7fcefd46":"The p value of 'What is your current occupation_Student' is very high. So dropping it.","afec025a":"# Model Building\n\n## Running First Training Model","17ea887c":"## Understanding the structure of Leads dataset","c6c37c82":"We have retained a good number of rows(98%). No much data is lost","39646334":"## Finding Optimal Cutoff Point","80e0ce45":"## Recall\nTP \/ TP + FN","caba20ce":"## We want to generate a Lead score between 0 to 100. So multiplying the probability with 100","45a4d83a":"### There might be few columns which have only one unique values. These columns does not give much of the insights. So chosing to drop them","a5a1f94c":"### Again we get same cutoff around 0.4 which is nearer\n\n## Making predictions on the test set","c5ea85c2":"## Bivariate for the three numerical columns","96ac53cf":"## Recall on Test data\nTP \/ TP + FN"}}