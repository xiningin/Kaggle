{"cell_type":{"be42b6ae":"code","0b9e2ece":"code","7a8afa74":"code","199e4a36":"code","69bcdfab":"code","ba76821e":"code","c61de02a":"code","838ac385":"code","19597ec7":"code","a5037760":"code","e61df46a":"code","b5e5eac9":"code","2f38aa6f":"code","49b5f855":"code","c9515000":"code","e1b7306e":"code","ec5440b9":"code","8fc4c801":"code","57bb112a":"code","e4e8023a":"code","ee8f8af0":"code","9d7fc970":"code","4914f962":"code","72a9baae":"code","ea609493":"code","1345b0c9":"code","d0753fae":"code","c06d6e6d":"code","7322a78e":"code","23be07a7":"code","6c660938":"code","57c886e5":"code","da118af9":"code","73e416e2":"code","a740c78a":"code","e7cd5b6d":"code","cbbc9656":"code","c9b139c7":"markdown","53d10476":"markdown","bf940033":"markdown","8043c632":"markdown","1da28bb6":"markdown"},"source":{"be42b6ae":"import pandas as pd\nimport re\nimport string\nimport os\nimport torch\nimport numpy as np\nimport tqdm","0b9e2ece":"!pip install transformers","7a8afa74":"from transformers import BertModel, BertTokenizer","199e4a36":"path_to_dataset = '\/kaggle\/input\/nlp-getting-started\/'","69bcdfab":"test = pd.read_csv(os.path.join(path_to_dataset, 'test.csv'))\ntrain = pd.read_csv(os.path.join(path_to_dataset, 'train.csv'))","ba76821e":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","c61de02a":"class Model(torch.nn.Module):\n    \n    def __init__(self, ):\n        \n        super(Model, self).__init__()\n        self.base_model = BertModel.from_pretrained('bert-base-uncased') # use pre-trained BERT model by HuggingFace\n        self.fc1 = torch.nn.Linear(768, 1) # simple logistic regression above the bert model\n        \n    def forward(self, ids, masks):\n        \n        x = self.base_model(ids, attention_mask=masks)[1]\n        x = self.fc1(x)\n        return x\n        ","838ac385":"model = Model()","19597ec7":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'","a5037760":"model = model.to(device)","e61df46a":"def bert_encode(text, max_len=512):\n    \n    text = tokenizer.tokenize(text)\n    text = text[:max_len-2]\n    input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n    tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n    tokens += [0] * (max_len - len(input_sequence))\n    pad_masks = [1] * len(input_sequence) + [0] * (max_len - len(input_sequence))\n\n    return tokens, pad_masks","b5e5eac9":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z0-9] \\n', '', text)\n    return text","2f38aa6f":"train_text = train.text[:6000]\nval_text = train.text[6000:]","49b5f855":"train_text = train_text.apply(clean_text)\nval_text = val_text.apply(clean_text)","c9515000":"train_tokens = []\ntrain_pad_masks = []\nfor text in train_text:\n    tokens, masks = bert_encode(text)\n    train_tokens.append(tokens)\n    train_pad_masks.append(masks)\n    \ntrain_tokens = np.array(train_tokens)\ntrain_pad_masks = np.array(train_pad_masks)","e1b7306e":"val_tokens = []\nval_pad_masks = []\nfor text in val_text:\n    tokens, masks = bert_encode(text)\n    val_tokens.append(tokens)\n    val_pad_masks.append(masks)\n    \nval_tokens = np.array(val_tokens)\nval_pad_masks = np.array(val_pad_masks)","ec5440b9":"\nclass Dataset(torch.utils.data.Dataset):\n    \n    def __init__(self, train_tokens, train_pad_masks, targets):\n        \n        super(Dataset, self).__init__()\n        self.train_tokens = train_tokens\n        self.train_pad_masks = train_pad_masks\n        self.targets = targets\n        \n    def __getitem__(self, index):\n        \n        tokens = self.train_tokens[index]\n        masks = self.train_pad_masks[index]\n        target = self.targets[index]\n        \n        return (tokens, masks), target\n    \n    def __len__(self,):\n        \n        return len(self.train_tokens)","8fc4c801":"train_dataset = Dataset(\n                    train_tokens=train_tokens,\n                    train_pad_masks=train_pad_masks,\n                    targets=train.target[:6000]\n)","57bb112a":"batch_size = 6\nEPOCHS = 2","e4e8023a":"train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)","ee8f8af0":"criterion = torch.nn.BCEWithLogitsLoss()","9d7fc970":"opt = torch.optim.Adam(model.parameters(), lr=0.00001)","4914f962":"model.train()\ny_preds = []\n\nfor epoch in range(EPOCHS):\n        for i, ((tokens, masks), target) in enumerate(train_dataloader):\n\n            y_pred = model(\n                        tokens.long().to(device), \n                        masks.long().to(device)\n                    )\n            loss = criterion(y_pred, target[:, None].float().to(device))\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            print('\\rEpoch: %d\/%d, %f%% loss: %0.2f'% (epoch+1, EPOCHS, i\/len(train_dataloader)*100, loss.item()), end='')\n        print()","72a9baae":"val_dataset = Dataset(\n                    train_tokens=val_tokens,\n                    train_pad_masks=val_pad_masks,\n                    targets=train.target[6000:].reset_index(drop=True)\n)","ea609493":"val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=3, shuffle=False)","1345b0c9":"def accuracy(y_actual, y_pred):\n    y_ = y_pred > 0\n    return np.sum(y_actual == y_).astype('int') \/ y_actual.shape[0]","d0753fae":"model.eval()\navg_acc = 0\nfor i, ((tokens, masks), target) in enumerate(val_dataloader):\n\n    y_pred = model(\n                tokens.long().to(device), \n                masks.long().to(device), \n            )\n    loss = criterion(y_pred,  target[:, None].float().to(device))\n    acc = accuracy(target.cpu().numpy(), y_pred.detach().cpu().numpy().squeeze())\n    avg_acc += acc\n    print('\\r%0.2f%% loss: %0.2f, accuracy %0.2f'% (i\/len(val_dataloader)*100, loss.item(), acc), end='')\nprint('\\nAverage accuracy: ', avg_acc \/ len(val_dataloader))","c06d6e6d":"class TestDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, test_tokens, test_pad_masks):\n        \n        super(TestDataset, self).__init__()\n        self.test_tokens = test_tokens\n        self.test_pad_masks = test_pad_masks\n        \n    def __getitem__(self, index):\n        \n        tokens = self.test_tokens[index]\n        masks = self.test_pad_masks[index]\n        \n        return (tokens, masks)\n    \n    def __len__(self,):\n        \n        return len(self.test_tokens)","7322a78e":"test_tokens = []\ntest_pad_masks = []\nfor text in test.text:\n    tokens, masks = bert_encode(text)\n    test_tokens.append(tokens)\n    test_pad_masks.append(masks)\n    \ntest_tokens = np.array(test_tokens)\ntest_pad_masks = np.array(test_pad_masks)","23be07a7":"test_dataset = TestDataset(\n    test_tokens=test_tokens,\n    test_pad_masks=test_pad_masks\n)","6c660938":"test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=3, shuffle=False)","57c886e5":"model.eval()\ny_preds = []\nfor (tokens, masks) in test_dataloader:\n\n    y_pred = model(\n                tokens.long().to(device), \n                masks.long().to(device), \n            )\n    y_preds += y_pred.detach().cpu().numpy().squeeze().tolist()","da118af9":"submission_df = pd.read_csv(os.path.join(path_to_dataset, 'sample_submission.csv'))","73e416e2":"submission_df['target'] = (np.array(y_preds) > 0).astype('int')","a740c78a":"submission_df.target.value_counts()","e7cd5b6d":"submission_df","cbbc9656":"submission_df.to_csv('submission.csv', index=False)","c9b139c7":"Test the model on the validation dataset","53d10476":"Define accuracy metric","bf940033":"Use Adam Optimizer with learning rate of 0.00001","8043c632":"Train for 2 epochs.","1da28bb6":"Use first 6000 for training, rest for validation"}}