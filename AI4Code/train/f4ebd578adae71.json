{"cell_type":{"27709f18":"code","f5680ac5":"code","6942600c":"code","157a6320":"code","d1aee108":"code","be686b12":"code","690ea915":"code","5850e269":"code","04578730":"code","8c7bf1c5":"code","1a9bc7a1":"code","0d34e9fc":"code","dc311891":"code","a44fb971":"code","47c8d1e7":"code","8b4ec9a8":"code","b9a6021b":"code","a745127b":"code","c9308fbe":"code","61a5c3f6":"code","1b9493ba":"markdown","b299b576":"markdown","b1949296":"markdown","3fb0ed98":"markdown","fb059063":"markdown","16abb9f9":"markdown","77c60e86":"markdown","ace00058":"markdown","ade0c520":"markdown"},"source":{"27709f18":"import numpy as np\nimport pandas as pd\n\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\n\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfrom optuna.integration import LightGBMPruningCallback\n\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.under_sampling import OneSidedSelection, NeighbourhoodCleaningRule, TomekLinks\n\n\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 20)","f5680ac5":"submission = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')\nX_train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv', index_col=0)\nX_test = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv', index_col=0)","6942600c":"for col in X_train.columns[X_train.dtypes == \"object\"].tolist():\n    X_train[col] = X_train[col].astype('category')\n    \nfor col in X_test.columns[X_test.dtypes == \"object\"].tolist():\n    X_test[col] = X_test[col].astype('category')","157a6320":"X = X_train.drop('target', axis=1)\ny = X_train['target']\n\nK = 10 # cross validation\n\nfixedparams = {'random_state': 42,\n               'n_estimators': 10000, \n               'learning_rate': 0.03, \n               'metric': 'auc', \n               'verbose':-1   \n}","d1aee108":"def model_instance(hyperparams, fixedparams):\n\n    clf = LGBMClassifier(**hyperparams['clf'], **fixedparams) \n    \n    if hyperparams['resample'] == 'random':\n        resample = RandomUnderSampler(sampling_strategy='majority')\n    else:\n        resample = None\n        \n    if hyperparams['power'] == True:\n        cont = [col for col in X_train.columns if 'cont' in col]\n        numeric_transformer = PowerTransformer(method='yeo-johnson',\n                                               standardize=True)\n\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('num', numeric_transformer, cont)])\n    else:\n        preprocessor = None\n    \n    pipe = Pipeline([('preprocessor', preprocessor),\n                     ('resample', resample),\n                     ('clf', clf) ])\n    return pipe","be686b12":"def fit_with_stop(pipe, X, y, X_val, y_val, trial, hyperparams, early_stopping_rounds = 50):\n    \n    if(trial != 0):\n        pruning_callback = [LightGBMPruningCallback(trial, 'auc')]\n    else: \n        pruning_callback = None\n    \n    if hyperparams['power'] == True:\n        pipe_interim = pipe.named_steps.preprocessor.fit(X)\n        X_val = pipe_interim.transform(X_val)\n    \n    pipe.fit(X, y,\n              clf__eval_set=(X_val, y_val),\n              clf__early_stopping_rounds=early_stopping_rounds,\n              clf__verbose=0,\n              clf__eval_metric=\"auc\",\n              clf__callbacks=pruning_callback)\n    return pipe","690ea915":"def evaluate(model, X, y):\n\n    yp = model.predict_proba(X)[:, 1]\n    auc_score = roc_auc_score(y, yp)\n    return auc_score","5850e269":"def kfold_prediction(X, y, X_test, k, hyperparams, fixedparams, early_stopping_rounds = 50):\n\n    yp = np.zeros(len(X_test))\n    \n    kf = StratifiedKFold(n_splits=k,random_state=42,shuffle=True)\n    model = model_instance(hyperparams, fixedparams)\n    \n    for i, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n        print(f\"\\n FOLD {i} ...\")\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        model_fit = fit_with_stop(model, X_train, y_train,\n                                  X_val, y_val, 0, hyperparams, \n                                  early_stopping_rounds)\n        yp += model_fit.predict_proba(X_test)[:, 1] \/ k\n    \n    return yp","04578730":"def objective(trial):\n    \n    global X, y, K, fixedparams\n\n    hyperparams = {\n        'resample': trial.suggest_categorical(\"resample\", [None]),\n        'power': trial.suggest_categorical(\"power\", [False]),\n        'clf':{\n            'boosting_type': trial.suggest_categorical(\"boosting_type\", ['gbdt']),\n            'num_leaves': trial.suggest_int('num_leaves', 2, 1024),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n            'max_depth': trial.suggest_int('max_depth', 1, 64),\n               \n            'max_delta_step': trial.suggest_int('max_delta_step', 1, 15),\n            ##'max_bin': trial.suggest_int('max_bin', 32, 255),\n            ##'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 256),\n            ##'min_data_in_bin': trial.suggest_int('min_data_in_bin', 1, 256),\n            'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10),\n            'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10),\n            #'min_split_gain' : trial.suggest_discrete_uniform('min_split_gain', 0, 5, 0.01),\n            \n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n            #'subsample': trial.suggest_float('subsample ', 0.1, 1.0),\n            \n            'cat_smooth': trial.suggest_float('cat_smooth', 10, 100.0),\n            'cat_l2': trial.suggest_int('cat_l2', 1, 20)\n        }\n\n    }\n    \n    kf = StratifiedKFold(n_splits=K,random_state=42,shuffle=True)\n    scores = []\n    model = model_instance(hyperparams, fixedparams)\n    \n    for i, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n        \n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        model_fit = fit_with_stop(model, X_train, y_train, X_val, y_val,\n                                  trial, hyperparams)\n        val_score = evaluate(model_fit, X_val, y_val)\n        scores.append(val_score)\n    \n    return np.nanmean(scores)\n","8c7bf1c5":"study = optuna.create_study(direction='maximize',\n                            pruner=optuna.pruners.HyperbandPruner())","1a9bc7a1":"%%time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nstudy.optimize(objective, timeout=60*5, n_jobs=-1,\n               n_trials=None, gc_after_trial=False)","0d34e9fc":"study.trials_dataframe()","dc311891":"study.best_value","a44fb971":"plot_optimization_history(study)","47c8d1e7":"optuna.visualization.plot_parallel_coordinate(study)","8b4ec9a8":"plot_param_importances(study)","b9a6021b":"study.best_params","a745127b":"best_params = {'resample': None,\n 'power': False,\n 'boosting_type': 'gbdt',\n 'num_leaves': 250,\n 'min_child_samples': 75,\n 'max_depth': 63,\n 'max_delta_step': 5,\n 'reg_alpha': 3.4218738754608045,\n 'reg_lambda': 3.0962347920614643,\n 'colsample_bytree': 0.3189138428868663,\n 'cat_smooth': 45.74149068289875,\n 'cat_l2': 19}","c9308fbe":"final_params = dict()\nfinal_params['clf']=dict(best_params)\n\nfinal_params['resample']=final_params['clf']['resample']\ndel final_params['clf']['resample']\n\nfinal_params['power']=final_params['clf']['power']\ndel final_params['clf']['power']\n\nfixedparams['learning_rate'] = 0.005","61a5c3f6":"%%time\n\nsubmission.loc[:, 'target'] = kfold_prediction(X, y, X_test, 10, \n                                               final_params, fixedparams,\n                                               500)\nsubmission.to_csv('submission.csv', index = False)","1b9493ba":"# Evaluate optimization","b299b576":"# References:\n\n- <https:\/\/optuna.readthedocs.io\/>\n- <https:\/\/www.kaggle.com\/rmiperrier\/tps-mar-lgbm-optuna>\n- <https:\/\/towardsdatascience.com\/how-to-make-your-model-awesome-with-optuna-b56d490368af>\n- <https:\/\/optuna.readthedocs.io\/en\/v1.0.0\/tutorial\/pruning.html>\n- <https:\/\/www.kaggle.com\/kst6690\/dsb2019-tuning-lightgbm-parameter-using-optuna>","b1949296":"# Import dependencies","3fb0ed98":"# Prepare to submit","fb059063":"# Prepare data","16abb9f9":"# Model Tuning","77c60e86":"# Problem definition\n\n\n-  Details:\n\nAccording to the description: \"The dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\"\n\n-  Solution:\n\nA LightGBM model will be adjusted using Bayesian optimization with lib [optuna](https:\/\/optuna.readthedocs.io\/en\/stable\/) (optimize hyperparameters and pre processing). The goal will be to maximize area under the ROC curve\n\n<p align=\"right\"><span style=\"color:firebrick\">Dont forget to upvote if the notebook was useful! <i class=\"fas fa-hand-peace\"><\/i><\/span> <\/p>","ace00058":"# Custom Functions","ade0c520":"Update the best parameters after a long training"}}