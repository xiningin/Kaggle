{"cell_type":{"6f586fe1":"code","7db30012":"code","92f915e6":"code","b09df458":"code","3ada4f63":"code","8a0c83c9":"code","8e98998c":"code","dd46c6ca":"code","89ecc982":"code","b9b0aa04":"code","23f02db8":"code","1504ac61":"code","9795311c":"code","788c880a":"code","7fa8dc9a":"code","f274479d":"code","e735054e":"code","4172e996":"code","24cc387c":"code","a908f641":"code","a081c7b7":"code","f745542b":"code","a9551755":"code","c569e89b":"code","4e437ab8":"code","1a49db20":"code","a5c6b70b":"code","cf693a35":"code","f37a767d":"code","cce7ae21":"code","2b273d60":"code","038b4e38":"markdown","2b2e6785":"markdown","eec77891":"markdown","787fe57a":"markdown","96022ff0":"markdown","7bc06c73":"markdown","4b0481bc":"markdown","13ac2db6":"markdown","4ccc6a68":"markdown","88e1eb76":"markdown","a9154cfc":"markdown","e1313e30":"markdown","78bc0f35":"markdown","2ab994e0":"markdown","10b064ae":"markdown","d5e38289":"markdown","5c76de30":"markdown","7f77ae11":"markdown","f0fe417e":"markdown","c6139f0e":"markdown"},"source":{"6f586fe1":"#!conda env list","7db30012":"#import sys\n#!conda install --yes --prefix {sys.prefix} scikit-learn","92f915e6":"# PACKAGE\nimport numpy as np","b09df458":"import matplotlib.pyplot as plt\nfrom matplotlib import cbook\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n#import seaborn as sns","3ada4f63":"from sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler","8a0c83c9":"# input methods\nimport os\nprint(os.listdir(\"..\/input\"))\n","8e98998c":"training_data = pd.read_csv(\"..\/input\/train.csv\")\ntraining_data = training_data.values","dd46c6ca":"length = training_data.shape[0]\nlength\ntraining_length = round(0.7*(length))\n#dev_length = round(0.8*(length))\nx_train = training_data[:training_length,1:].T\ny_train = training_data[:training_length,0]\n\n#x_dev = training_data[training_length+1:dev_length,1:].T\n#y_dev = training_data[training_length+1:dev_length,0]\n\nx_test = training_data[training_length+1:,1:].T\ny_test = training_data[training_length+1:,0]\n\n\n\ny_train = y_train.reshape(1,len(y_train))\n#y_dev = y_dev.reshape(1,len(y_dev))\ny_test = y_test.reshape(1,len(y_test))","89ecc982":"scaler = MinMaxScaler(feature_range=(0, 1))\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test) ","b9b0aa04":"#scaler_train = StandardScaler().fit(x_train)\n#scaler_test = StandardScaler().fit(x_test)\n#x_train = scaler_train.transform(x_train)\n#x_test = scaler_test.transform(x_test)","23f02db8":"\nimg = x_train[:,3]\nimg = img.reshape(28,28)\nplt.imshow(img,cmap = 'gray')\nplt.show()\n#y_train[:,3]","1504ac61":"count_in_labels = []\nfor i in range(10):\n    idx = y_train == i\n    count_in_labels.append(len(y_train[idx]))\n    print(\"Count in label %d is %d\"%(i,count_in_labels[i]))","9795311c":"logisticregsr = LogisticRegression(multi_class='multinomial', solver = 'lbfgs')\ny_train_l = y_train.ravel()\nlogisticregsr.fit(x_train.T,y_train_l)","788c880a":"pred = logisticregsr.predict(x_test.T)\ny_test_check = y_test.ravel()\ny_train_check = y_train.ravel()\n#y_test_check.shape\nscore_train = logisticregsr.score(x_train.T,y_train_check)*100\nprint(\"The accuracy on train data is\",score_train,\"%\")\nscore_test = logisticregsr.score(x_test.T,y_test_check)*100\nprint(\"The accuracy on test data is\",score_test,\"%\")","7fa8dc9a":"c_matrix = confusion_matrix(y_test_check,pred)","f274479d":"print(c_matrix)","e735054e":"def transform_output(y):\n    l = y.shape[1]\n    ty = np.zeros((10,l))\n    for i in range(l):\n        s = y[0,i].astype(int)\n        # print(type(s))\n        ty[s,i] = 1\n    return ty","4172e996":"def function(z):\n    return np.maximum(0.01*z,z)","24cc387c":"def function_prime(z):\n    a = np.greater(z,0.01*z)\n    a = a.astype(int)\n    a[a==0] = 0.01\n    return a","a908f641":"def cost_function(y,a,m):\n    b = y-a\n    s = (1\/(2*m))*(np.dot(b.T,b))\n    l = s.shape[1]\n    dsum = 0\n    for i in range(l):\n        dsum = dsum + s[i,i]\n    return dsum","a081c7b7":"def sigmoid(z):\n    return 1\/(1 + np.exp(-z))","f745542b":"def sigmoid_prime(z):\n    return sigmoid(z)*(1-sigmoid(z))","a9551755":"def grad_a_cost_function(y,a,m):\n    return -(1\/m)*(y-a)\n","c569e89b":"class Network:\n    # atype is the activation type; whether it is a RELU or sigmoid\n    def __init__(self,sizes,atype):\n        self.num_layers = len(sizes)\n        self.weights = []\n        if atype==\"sigmoid\":\n            self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n            #self.biases = [np.zeros((y,1)) for y in sizes[1:]]\n            #self.weights = [np.zeros((sizes[1],sizes[0]))]\n            #self.weights = [np.zeros((sizes[1],sizes[0]))\n            for x in range(1,len(sizes)):\n                (self.weights).append(np.random.randn(sizes[x],sizes[x-1]))\n        else:\n            self.biases = [np.zeros((y,1)) for y in sizes[1:]]\n            #self.weights = [np.zeros((sizes[1],sizes[0]))]\n            for x in range(1,len(sizes)):\n                (self.weights).append(np.random.randn(sizes[x],sizes[x-1])*np.sqrt(2\/sizes[x-1]))\n                \n            \n    def feedforward(self, batch_x):\n        length = len(self.weights)\n        a_cache=[]\n        a = batch_x\n        a_cache.append(a)\n        z_cache=[] \n        for i in range(length-1):\n            z = np.dot(self.weights[i],a) + self.biases[i]\n            z_cache.append(z)\n            a = function(z)\n            a_cache.append(a)\n        i=i+1\n        z = np.dot(self.weights[i],a) + self.biases[i]\n        z_cache.append(z)\n        a = sigmoid(z)\n        a_cache.append(a)\n        return a_cache,z_cache\n    \n    def feedforward_output(self, x_test):\n        length = len(self.weights)\n        a = x_test\n        for i in range(length-1):\n            z = np.dot(self.weights[i],a) + self.biases[i]\n            a = function(z)\n            #print(\"z[\",i,\"] = \",z)\n        i = i + 1\n        z = np.dot(self.weights[i],a) + self.biases[i]\n        #print(\"z[\",i,\"] = \",z)\n        a = sigmoid(z)\n        return a\n    \n    def compute_dZ(self,batch_x,batch_y,l,Z_cache,dZ):\n        m = batch_x.shape[1]\n        n = batch_x.shape[0]\n        #y = batch[n-1,:]\n        #y = y.reshape(1,len(y))\n        # print(type(y))\n        #print(y.shape)\n        #y = transform_output(y)\n        if l== self.num_layers - 2:\n            #a = function(Z_cache[l])\n            a = sigmoid(Z_cache[l])\n            #return grad_a_cost_function(batch_y,a,m)*function_prime(Z_cache[l]) \n            return grad_a_cost_function(batch_y,a,m)*sigmoid_prime(Z_cache[l])\n        else:\n            return np.dot(self.weights[l+1].T,dZ)*function_prime(Z_cache[l])\n    \n    def backpropagation(self,batch_x,batch_y):\n        dw = []\n        db = []\n        m = batch_x.shape[1]\n        A_cache,Z_cache = self.feedforward(batch_x)\n        L = self.num_layers\n        dZ = []\n        for l in range(L-2,-1,-1):\n            dZ = self.compute_dZ(batch_x,batch_y,l,Z_cache,dZ)\n            db.append((1\/m)*np.sum(dZ,axis = 1,keepdims = True))\n            dw.append((1\/m)*np.dot(dZ,A_cache[l].T))\n        db.reverse()\n        dw.reverse()\n        return dw,db \n        \n    def update(self,batch_x,batch_y,eta,vdw,vdb):\n        beta = 0.9\n        dw,db = self.backpropagation(batch_x,batch_y)\n        \n        vdw = [beta*x +(1-beta)*y for (x,y) in zip(vdw,dw)]\n        vdb = [beta*x +(1-beta)*y for (x,y) in zip(vdb,db)]\n        for i in range(self.num_layers-1):\n            #check(dw[i])\n            self.weights[i] = self.weights[i] - eta*vdw[i]\n            self.biases[i] = self.biases[i] - eta*vdb[i]\n        \n        return vdw,vdb\n\n            \n    # we assume that in the training data, rows represent the features of the training set and columns denote the \n    # instances of the training set.\n    \n    def SGD(self,x_train,y_train,batch_size,no_of_epoch,eta,x_test=None,y_test=None):\n        training_data = x_train\n        print(training_data.shape)\n        print(y_train.shape)\n        training_data = np.append(training_data,y_train,axis = 0)\n        print(training_data.shape)\n        # In the previous two lines, we have put x_train and y_train into one matrix: training data;\n        #training_data = [[x_train],\n        #                 [ytrain]]\n        n = training_data.shape[0]\n        training_size = training_data.shape[1]\n        vdw = [np.zeros(y.shape) for y in self.weights]\n        vdb = [np.zeros(y.shape) for y in self.biases]\n        for i in range(no_of_epoch):\n            np.random.shuffle(training_data.T)\n            x_train_shuffled = training_data[:-1,:]\n            y_train_shuffled = training_data[n-1,:]\n            y_train_shuffled = y_train_shuffled.reshape(1,len(y_train_shuffled ))\n            # print(type(y_train_shuffled))\n            y_train_shuffled_transformed = transform_output(y_train_shuffled)\n            batches_x = [x_train_shuffled[:,k:k + batch_size] for k in range(0,training_size,batch_size)]\n            batches_y = [y_train_shuffled_transformed[:,k:k + batch_size] for k in range(0,training_size,batch_size)]\n            for (batch_x,batch_y) in zip(batches_x,batches_y):\n                vdw,vdb = self.update(batch_x,batch_y,eta,vdw,vdb)\n            a = self.feedforward_output(x_train[:,1:10])\n            m = a.shape[1]\n            y = transform_output(y_train[:,1:10])\n            cost = cost_function(y,a,m)\n            print(\"The cost after epoch no.\",i,\" is \",cost)\n            #a = np.argmax(a,axis = 0)\n            #s = score(y_train,a)\n            #print(s)\n        \n        ","4e437ab8":"n = Network([784,30,30,10],\"RELU\")\nn.SGD(x_train,y_train,10,50,0.5)","1a49db20":"img = x_test[:,7]","a5c6b70b":"img = img.reshape(len(img),1)\nA_test= n.feedforward_output(img)\nA_test = np.argmax(A_test,axis=0)\nprint(img.shape)\nimg = img.reshape(28,28)\nplt.imshow(img,cmap = 'gray')\nplt.show()\nprint(\"The predicted output is\", A_test)","cf693a35":"A_train = n.feedforward_output(x_train)\nA_train = np.argmax(A_train,axis = 0)\n#A.shape\nA_train = A_train.reshape(1,len(A_train))","f37a767d":"A_test = n.feedforward_output(x_test)\nA_test = np.argmax(A_test,axis = 0)\nA_test.shape\nA_test = A_test.reshape(1,len(A_test))","cce7ae21":"def score(y,a):\n    l = y.shape[1]\n    return (1 - (np.count_nonzero(y-a)\/l))*100","2b273d60":"score_train = score(y_train,A_train)\nscore_test = score(y_test,A_test)\nprint(\"Accuracy on train data is \",score_train, \"% and on test data is\",score_test,\"%\")","038b4e38":"# Dividing dataset into training, dev and test dataset.","2b2e6785":"# Score","eec77891":"# Aim of the Project\n\nIn this project, we  experiment with two models to classify MNIST data. First, we train a logistic regression model which shows classification accuracy of around $90\\%$ in the test dataset and $85\\%$ in the submission dataset. Second, we train a four layer neural network model on MNIST training dataset. As of now, the classification accuracy on the training dataset is around $98\\%$ and on the test dataset is around $95\\%$.\n\nThis main purpose of the project is to understand the backpropagation algorithm. The neural network model has been coded from scratch and may be extremely inefficient in practice. I plan to do further experiments on hyperparameter tuning, regularization, effect of other activitation functions, cost functions and performance evaluation on other network architectures.\n\n\n","787fe57a":"## Standardization\n\nData preprocessing such that the mean of the data is $0$ and standard deviation is $1$.","96022ff0":"# Kaggle dataset","7bc06c73":"# Experimenting with Neural Network\n\n## Transforming the output in the dataset\n\nEach entry in <code>y_train<\/code> (or <code>y_test<\/code>) is a number in the range of 0 to 9 which is the label of the corresponding instance in x_train. The function <code>transform_output<\/code> takes such a label $y$ as input and outputs a binary column vector with $1$ in the $y^{th}$ position and $0$ in rest of the entries. \nFor example, if an instance has label $5$, then the corresponding transformed label vector is ${(0,0,0,0,0,1,0,0,0,0)}^T$.\n\n## Building the neural network\n\nWe define a network class which has the information of size of the layers and corresponding initializing weights and biases.\n\nTake a proper look at the indices. It can get confusing. if there are three layers, then there will be two weight matrices and biases.\n\n### Cost function\n\nLet $y$ be the transformed label vector of an arbitrary instance and $a$ be the output of the model. Then the cost function $C$ is defined as : \n\n$$ C(y,a) =  \\frac{{||y-a||}^{2}}{2} $$\n\nOver the whole training set, the (vectorized) cost function is an average of cost functions of all the instances,\n\n$$ C(Y,A) = \\frac{1}{2m}\\sum\\limits_{i=1}^{m}C(y^{(i)},a^{(i)})$$\n\nwhere $Y =[y^{(1)}\\ ...\\ y^{(m)}]$, $A = [a^{(1)}\\ ...\\ a^{(m)}]$ and $y^{(i)}$ and $a^{(i)}$ are the transformed label vector and output of the model respectively. Observe that both $y^{(i)}$ and $a^{(i)}$ are column vectors.\n\nOne important thing to note that $Y$ is given and $A$ is computed by the model. Therefore, the actual variables in the cost function are actually the weights and biases of the neural networks. \n\n### He Initialization\n\n### Activation function\n\nWe use RELU as the activation function for hidden layers and sigmoid for probability estimation for output layer.\n\n### Feedforward \n\nGiven a neural network with its set of weights, biases and input vector $x$, feedforward computes the prediction vector $a$ (where value at each entry i is the probability that x belongs to the class i). The computation at layer $l$ is \n$$ z^{[l]} = w^{[l]}.a^{[l-1]} +b^{[l]} $$\n$$ a^{[l]} = \\sigma(z^{[l]})$$\n\nI have defined two functions <code>feedforward<\/code> and <code>feedforward_output<\/code>. <code>feedforward<\/code>, along with computing $a$, also caches $z$ and $a$ so that they can be used in backpropagation. The other function just computes $a$.\n\n### Backpropagation\n\nBackpropagation is used to compute $\\frac{\\partial C}{\\partial w^{[l]}}$ and $\\frac{\\partial C}{\\partial b^{[l]}}$ for all layers $l$ in the neural network. <code>backpropagation<\/code> returns two lists of the partial derivatives of all the weights and biases($dw$ and $db$).\n\nComputation of the partial derivatives make use of the following formulae:\n\n$$ \\frac{\\partial C}{\\partial w^{[l]}} =  \\frac{\\partial C}{\\partial z^{[l]}}. \\frac{\\partial z^{[l]}}{\\partial w^{[l]}} $$\n\n$$  \\frac{\\partial C}{\\partial b^{[l]}} =  \\frac{\\partial C}{\\partial z^{[l]}}$$\n\nIn the code, we represent $\\frac{\\partial C}{\\partial w^{[l]}}$ as $dw$, $\\frac{\\partial C}{\\partial b^{[l]}}$\nas db and  $\\frac{\\partial C}{\\partial z^{[l]}}$ as $dz$.\n\nIn the vectorized form (i.e over all inputs of the batch), dw and db are computed as:\n\n$$ dw^{[l]} =  \\frac{1}{m}dz.{A^{[t]}}^T$$\n$$ db^{[l]} = \\frac{1}{m}.\\sum\\limits_{i=1}^{m} dz^{[l]}[:,i]$$\n\n### compute_dZ function\n\n$dz^{[l]}$ is computed using the compute_dZ function. \nIn the vectorized form, it is computed using the following equation: \n\n$$dz^{[L]} = \\nabla_a C*\\sigma'(z^{[L]}) $$\n$$dz^{[l]} = {w^{[l+1]}}^T.dz^{[l+1]}*\\sigma'(z^{[l]})$$\n\nHere $*$ is a broadcast operation.\n\nTherefore, <code>compute_dZ<\/code> takes as arguments, training data (<code>batch_x,batch_y<\/code>), $l$ (current layer), $z$ values (stored in <code>Z_cache<\/code>), and $dz^{l+1}$ (as <code>dZ<\/code>).\n\n### Exponentially Weighted average\n\nGiven a set of points $\\lbrace a_1,...,a_n\\rbrace$, moving average $\\lbrace v_1,...,v_n\\rbrace$ is computed as follows:\n\n<ul>\n<li> Initialize $v_1 = 0$.<\/li>\n<li> \nFor $i = 2$ to $n$:\n<ul>\n   <li>$ v_i = \\beta v_{i-1} + (1 - \\beta)a_i $<\/li>\n<\/ul>        \n<\/li>\n<\/ul> \n\nwhere $\\beta\\in (0,1)$.\n\nThe set $\\lbrace v_1,v_2,...,v_n\\rbrace$ is tolerant to noise in the original dataset.\n\n### Gradient Descent with Moment\n\nWe use the idea of exponentially weighted average to gradient descent. For every iteration, we use backpropagation to compute <code>dw<\/code> and <code>db<\/code> and then compute the exponentially weighted average using the computed gradients.\n<ul>\n<li> Compute <code>dw<\/code> and <code>db<\/code> using <code>backpropagation<\/code>.<\/li>\n<li> <code>vdw<\/code> $=\\beta$<code>vdw<\/code> $+(1-\\beta)$<code>dw<\/code><\/li>\n<li> <code>vdb<\/code> $=\\beta$<code>vdb<\/code> $+(1-\\beta)$<code>db<\/code><\/li>\n<\/ul>\n\nSetting $\\beta = 0.9$ works as a pretty robust value. Using Gradient Descent with moment leads to faster convergence.\n\n### Update function\n\n\n<code>update<\/code> function calls the backpropagation function to compute the gradients $dw$ and $db$, and updates the weights and biases of the neural network by taking exponentially weighted average.\n\n\n### Minibatch Gradient Descent \n\nMinibatch Gradient Descent function is implemented as SGD in the code which takes in as arguments training data, batch_size, no_of_epoch and eta as parameters and perform the procedure outlined in the process outline. \n\nProcess outline:\n\nFor no_of_epoch times, do the following :  \n<ul>\n<li>Randomly shuffle the training data. <\/li>\n<li> Seperate the training_data into <code>x_train_shuffled<\/code> and <code>y_train_shuffled<\/code>.<\/li>\n<li>Divide the shuffled training data (<code>x_train_shuffled<\/code> and <code>y_train_shuffled<\/code>) into batches of specified <code>batch_size<\/code> and store the batches in the lists <code>batches_x<\/code> and <code>batches_y<\/code>.<\/li>\n\n<li>For each (<code>batch_x<\/code>,<code>batch_y<\/code>) in <code>zip(batches_x,batches_y)<\/code>, use the tuple to update the parameters (i.e call the update function which in turn performs the backpropagation and computes gradient descent with moment).<\/li>\n\n<li>Completion of the whole dataset marks the end of one epoch.<\/li>\n<\/ul>\n\n<code>batch_size, no_of_epoch<\/code> and <code>eta<\/code> are the hyperparameters of the algorithm.","4b0481bc":"# Exploratory Data Analysis","13ac2db6":"# Experimenting with Logostic Regression","4ccc6a68":"# Accuracy\n\nThe accuracy on the train data is around $94\\%$ and on the test data is about $92\\%$.\n\n# Confusion Matrix\n","88e1eb76":"All the labels have approximately same amount of instances.","a9154cfc":"The submission set resulted in an accuracy of around 85.2%","e1313e30":"The standardization has been commented out since this resulted in deterioration of the performance of the logistic regression.","78bc0f35":"# References\n<ul>\n <li>neuralnetworksanddeeplearning.com <\/li>\n  <li>deeplearning.ai Coursera Specialization<\/li>  \n<\/ul>","2ab994e0":"# Packages needed","10b064ae":"# Explanation of the dataset dimensions\n\n<code>training_data<\/code> is a numpy array with columns representing features and rows as the instances of the training dataset. The first column of the <code>training_data<\/code> is the label column.","d5e38289":"# Initialization of hyperparameters","5c76de30":"# Defining the Neural Network","7f77ae11":"## Training the dataset","f0fe417e":"# Data Rescaling and Standardization\n## Rescaling","c6139f0e":"Here is an example of a training set image."}}