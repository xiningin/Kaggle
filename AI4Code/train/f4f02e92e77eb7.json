{"cell_type":{"05520645":"code","c125a141":"code","2aa95fca":"code","795b28a9":"code","d028ca86":"code","2657f71c":"code","2ea30dbd":"code","aa6721ce":"code","1a1155a1":"code","09d21b76":"code","7fc626ad":"code","5202c6d5":"code","c240d76b":"code","94141b2a":"code","6b895f53":"code","cec63d26":"code","16719a52":"code","1dae8b01":"code","2f5a8c2a":"code","4cee1987":"code","d1924748":"code","7b2b1a89":"markdown","4cd59adc":"markdown","bc0ac4a4":"markdown","b194388d":"markdown","d78c44c9":"markdown","afb858c2":"markdown","f68f9f78":"markdown","de8d398e":"markdown","1ddbaa04":"markdown","36f892e8":"markdown","d88f2876":"markdown","85261881":"markdown","278154b3":"markdown","293139f0":"markdown","99b25b8b":"markdown","61206699":"markdown","32fc8fc9":"markdown","2ec85d8b":"markdown","4eeaff7f":"markdown","e19444ae":"markdown","62a19e0c":"markdown","5efa4c95":"markdown","f8b44616":"markdown","5355641d":"markdown","c075632c":"markdown","fdc2d632":"markdown","89ad2f7a":"markdown","29527706":"markdown","1c913acf":"markdown","a228be06":"markdown","c1409bb5":"markdown","58090f47":"markdown","4bb63492":"markdown","9cef6e4b":"markdown","0d95190a":"markdown","04a78b99":"markdown","527cb9c1":"markdown","e2212ebb":"markdown","9ba3e69f":"markdown"},"source":{"05520645":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c125a141":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dateutil import parser\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nimport pickle\nfrom lightgbm import LGBMClassifier\nimport warnings\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')","2aa95fca":"data=pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndata.head(2)","795b28a9":"data.isnull().sum()","d028ca86":"cols = data.columns\nprint(\"# Rows in the dataset {0}\".format(len(data)))\nprint(\"---------------------------------------------------\")\nfor col in cols:\n    print(\"# Rows in {1} with ZERO value: {0}\".format(len(data.loc[data[col] ==0]),col))","2657f71c":"\ncorrmat = data.corr()\nfig = plt.figure(figsize = (12,12))\nsns.heatmap(corrmat,vmax = 1,square = True,annot = True,vmin = -1)\nplt.show()","2ea30dbd":"final_cols = cols\nfinal_cols = list(final_cols)\nfinal_cols.remove('ca')\nfinal_cols.remove('cp')\nfinal_cols.remove('exang')\nfinal_cols.remove('fbs')\nfinal_cols.remove('restecg')\nfinal_cols.remove('sex')\nfinal_cols.remove('slope')\nfinal_cols.remove('target')\nfinal_cols.remove('thal')\nfinal_cols","aa6721ce":"X = data.drop('target',axis=1) #predictor feature columns\ny = data.target\ny.value_counts()","1a1155a1":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=42)\nX_res_OS , Y_res_OS = sm.fit_resample(X,y)\npd.Series(Y_res_OS).value_counts()","09d21b76":"X_res_OS = pd.DataFrame(X_res_OS,columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal'])\nY_res_OS = pd.DataFrame(Y_res_OS,columns=['target'])","7fc626ad":"X_train,X_test,y_train,y_test = train_test_split(X_res_OS,Y_res_OS,test_size = 0.1,random_state=10)\nprint('Training Set :',len(X_train))\nprint('Test Set :',len(X_test))\nprint('Training labels :',len(y_train))\nprint('Test labels :',len(y_test))","5202c6d5":"from sklearn.impute import SimpleImputer \nfill = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nX_train = fill.fit_transform(X_train[final_cols])\nX_test = fill.fit_transform(X_test[final_cols])","c240d76b":"from sklearn.datasets import make_classification\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB","94141b2a":"rf = RandomForestClassifier(max_features=5, n_estimators=500)\nrf.fit(X_train, y_train)","6b895f53":"nb = GaussianNB()\nnb.fit(X_train, y_train)","cec63d26":"r_probs = [0 for _ in range(len(y_test))]\nrf_probs = rf.predict_proba(X_test)\nnb_probs = nb.predict_proba(X_test)","16719a52":"rf_probs = rf_probs[:, 1]\nnb_probs = nb_probs[:, 1]","1dae8b01":"from sklearn.metrics import roc_curve, roc_auc_score\nr_auc = roc_auc_score(y_test, r_probs)\nrf_auc = roc_auc_score(y_test, rf_probs)\nnb_auc = roc_auc_score(y_test, nb_probs)","2f5a8c2a":"print('Random (chance) Prediction: AUROC = %.3f' % (r_auc))\nprint('Random Forest: AUROC = %.3f' % (rf_auc))\nprint('Naive Bayes: AUROC = %.3f' % (nb_auc))","4cee1987":"r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\nrf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\nnb_fpr, nb_tpr, _ = roc_curve(y_test, nb_probs)","d1924748":"import matplotlib.pyplot as plt\nplt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)\nplt.plot(rf_fpr, rf_tpr, marker='.', label='Random Forest (AUROC = %0.3f)' % rf_auc)\nplt.plot(nb_fpr, nb_tpr, marker='.', label='Naive Bayes (AUROC = %0.3f)' % nb_auc)\n\n# Title\nplt.title('ROC Plot')\n# Axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# Show legend\nplt.legend() # \n# Show plot\nplt.show()","7b2b1a89":"### Findout Zero Values in DataSet","4cd59adc":"We can see that the dataset is unbalanced.First we will balance the dataset.Balancing data set is a good approach to improve the accuracy of the machine learning model.","bc0ac4a4":"### Terminology","b194388d":"We can see that there is not much correlation between the features in the dataset.If corelation was high we can face issue of multicollinearity.In that case we would need to use feature engineering to avoid multi colinearity.","d78c44c9":"### Heat Map ","afb858c2":"### Naive Bayes","f68f9f78":"We ca see that the Random Forest Model Has Shightly Higher Value of AUC.So we can ay Random Fores Model Would perform better than the Naive Bayes Model for this particular Data Set.","de8d398e":"### Random Forest","1ddbaa04":"In this Kerne we will try to Predict who will have Heart Disease.But our main focus will be on how to compare two models using ROC and AUC Curves.In this notebook we will be covering following topic\n\n1.Data import and preprocessing\n\n2.Exploratory Data Analysis \n\n3.Feature Engineering\n\n4.Model Built\n\n5.Model Comparsion using ROC curve\n\n6.Conclusion","36f892e8":"### Predicting Probabilities","d88f2876":"### Test Train Split","85261881":"### What is ROC Curve?\n\nROC curve is a plot of False Positive Rate with the True Positive Rate \n\n$TPR(Sensitivity) = \\frac{TP}{TP + FN}$\n\n$FPR (1 - Specificity) = \\frac{FP}{TN + FP}$  ","278154b3":"### 4.Machine Learning Model Built","293139f0":"### Converting Numpy Arrays into Dataframe","99b25b8b":"## 3.Feature Engineering ","61206699":"### Computing AUROC and ROC curve values","32fc8fc9":"### Missing Values","2ec85d8b":"### Plotting ROC Curve","4eeaff7f":"### Replacing all the ZEROS with Mean of the Column","e19444ae":"# 2.Exploraory Data Analysis ","62a19e0c":"### Importing Modules","5efa4c95":"### Creating Feature of Matrix","f8b44616":"### Importing Modules","5355641d":"- age: The person's age in years\n- sex: The person's sex (1 = male, 0 = female)\n- cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n- trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n- chol: The person's cholesterol measurement in mg\/dl\n- fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n- restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n- thalach: The person's maximum heart rate achieved\n- exang: Exercise induced angina (1 = yes; 0 = no)\n- oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n- slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n- ca: The number of major vessels (0-3)\n- thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n- target: Heart disease (0 = no, 1 = yes)","c075632c":"### Calculate ROC curve","fdc2d632":"We have dropped the columns with categorical columns.This is because when we do feature engineering we will be repacing the numerical column Zero values with the mean values.This is not needed for the columns with categorical variables.","89ad2f7a":"# 5.Model Comparsion with ROC Curve","29527706":"We are very lucky here there are no Null Values in the dataset.But in this data the missing values are present in the form of value ZERO. Our next task would be to find out numbers of ZEROS in each column.","1c913acf":"### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","a228be06":"# 1.Data Import And Preprocessing","c1409bb5":"We have kept Probabilities for the Positive Outcome.","58090f47":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https:\/\/lnkd.in\/gj7bMQA\n\n### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","4bb63492":"### Printing AUC Scores","9cef6e4b":"The columns which have categorical values can have ZERO values.But columns like cp,trestbps,chol,fbs,exang,oldpean and Slope should not have value ZERO.The presence of ZERO in this columns indicate the presence of null values.","0d95190a":"### Importing Data","04a78b99":"The number of rows of data is very low in the dataset.This may not be sufficient to build a good model.Let us see how our model works out.","527cb9c1":"# 6.Conclusion\n\n1.We have imported and preprocessed the data set.We have handled the missing values which were in the form of Zeros.\n\n2.We have dropped some of the features as they wont have much influence on our model prediction.\n\n3.As the dataset was unbalanced we balanced the dataset by doing oversampling.This helps to improve the model accuracy. mode 4.We have done the Heart Disease prediction using Random Forest and Naive Bayes algorithm.\n\n5.We plotted a ROC curve for our models and used the AUC value to arrive at he best model","e2212ebb":"# TO BE CONTINUED ","9ba3e69f":"### Balancing Dataset"}}