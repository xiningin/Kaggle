{"cell_type":{"37131773":"code","ff2523d1":"code","cf7bb43a":"code","88e25cba":"code","4f085613":"code","af393ec2":"code","d6c0432b":"code","ff04b1f3":"code","90862204":"code","f1229ca1":"code","0de75614":"code","85fcc3b8":"code","03c32c34":"code","9876ff98":"code","322524b8":"code","e6de50c0":"code","3f290ee6":"code","fe3b76e3":"code","90f2ac3b":"code","4f3102bf":"code","a2d7215a":"code","e0ffe603":"code","3b8e8df9":"code","2d0c91b8":"code","064d2eee":"code","e78dea14":"code","98a47bd5":"code","e9db748a":"code","7fd06751":"code","b3f83da9":"code","b3b62966":"code","e126a959":"code","37be6324":"markdown","abf1c5f1":"markdown","c7005c06":"markdown","009852b5":"markdown","67fb25d0":"markdown","07d2828a":"markdown"},"source":{"37131773":"from google.colab import drive\n\ndrive.mount('\/content\/gdrive')","ff2523d1":"import os\nos.environ['KAGGLE_CONFIG_DIR'] = '\/content\/gdrive\/MyDrive\/kaggleflowersdata'","cf7bb43a":"%cd \/content\/gdrive\/MyDrive\/kaggleflowersdata","88e25cba":"!ls","4f085613":"!kaggle datasets download -d alxmamaev\/flowers-recognition","af393ec2":"!mkdir flowers-recognition\n!mv flowers-recognition.zip flowers-recognition","d6c0432b":"%cd flowers-recognition\/","ff04b1f3":"# !unzip flowers-recognition.zip","90862204":"# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport cv2       # open CV\nimport os\nimport tensorflow as tf\nimport keras\n# Data manipulation & visulation\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# data augmentation\nfrom keras.preprocessing.image import ImageDataGenerator\n\n#model creation and evaluation \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom keras.layers import Dense\nfrom keras.layers import Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.applications.vgg16 import VGG16\nfrom keras.callbacks import ReduceLROnPlateau\n\n\nimport random as rn\nfrom tqdm import tqdm","f1229ca1":"print(os.listdir('\/content\/gdrive\/MyDrive\/kaggleflowersdata\/flowers-recognition\/flowers'))","0de75614":"Daisy_flower_dir = '\/content\/gdrive\/MyDrive\/kaggleflowersdata\/flowers-recognition\/flowers\/daisy'\nSunflower_flower_dir = '\/content\/gdrive\/MyDrive\/kaggleflowersdata\/flowers-recognition\/flowers\/sunflower'\nTulip_flower_dir = '\/content\/gdrive\/MyDrive\/kaggleflowersdata\/flowers-recognition\/flowers\/tulip'\nDandelion_flower_dir = '\/content\/gdrive\/MyDrive\/kaggleflowersdata\/flowers-recognition\/flowers\/dandelion'\nRose_flower_dir = '\/content\/gdrive\/MyDrive\/kaggleflowersdata\/flowers-recognition\/flowers\/rose'","85fcc3b8":"images = []\nlabels = []\nimg_size = 150\n\ndef image_data(flower_name, DIR):\n    for i in tqdm(os.listdir(DIR)):\n        try:\n            \n            path = os.path.join(DIR,i)\n            img = cv2.imread(path)\n            img = cv2.resize(img, (img_size, img_size))\n            img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        \n            images.append(np.array(img))\n            labels.append(str(flower_name))\n            \n        except:\n            print(path)\n            ","03c32c34":"image_data('Daisy', Daisy_flower_dir)\nlen(images)","9876ff98":"image_data('Sunflower', Sunflower_flower_dir)\nlen(images)","322524b8":"image_data('Tulip', Tulip_flower_dir)\nlen(images)","e6de50c0":"image_data('Dandelion', Dandelion_flower_dir)\nlen(images)","3f290ee6":"image_data('Rose', Rose_flower_dir)\nlen(images)","fe3b76e3":"data = np.array(images)\nlabels = np.array(labels)\nprint('Input(Feature) Data shape :', data.shape)\nprint('Output(Labels) Data shape :', labels.shape)","90f2ac3b":"fig, ax = plt.subplots(4, 2, figsize = (15, 20))\nfor i in range(4):\n    for j in range(2):\n        l = rn.randint(0, data.shape[0])\n        ax[i,j].imshow(data[l])\n        ax[i,j].set_title(labels[l])\n        ax[i,j].axis('off')","4f3102bf":"sns.countplot(labels)\nplt.title(\"Class Distribution\")\nplt.xlabel(\"Class Number\")","a2d7215a":"le = LabelEncoder()\ny = le.fit_transform(labels)\ny = to_categorical(y, 5)\ny[0]","e0ffe603":"# Normalize the input data in range [0 1]\nX = data\/255","3b8e8df9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 5)\n\nprint(\"X_train shape :\",X_train.shape)\nprint(\"y_train shape :\",y_train.shape)\nprint(\"X_test shape :\" ,X_test.shape)\nprint(\"y_test shape :\",y_test.shape)","2d0c91b8":"np.random.seed(40)\nrn.seed(40)","064d2eee":"Base_model = VGG16(include_top= False, weights='imagenet',input_shape=(150,150,3), pooling='avg')\nBase_model.summary()","e78dea14":"model = Sequential()\n\nmodel.add(Base_model)\nmodel.add(Dense(256,activation='relu'))\n# adding prediction(softmax) layer\nmodel.add(Dense(5,activation=\"softmax\"))","98a47bd5":"# freeze layers that are already pre-trained(Base Model)\nBase_model.trainable = False","e9db748a":"reduc_lr=ReduceLROnPlateau(monitor='val_acc', factor=0.1, min_delta=0.0001, patience=2, verbose=1)","7fd06751":"datagen = ImageDataGenerator(featurewise_center= False,\n                              samplewise_center= False,\n                              featurewise_std_normalization= False,\n                              samplewise_std_normalization=False,\n                              rotation_range= 10,        # 0- 180\n                              zca_whitening=False,\n                              zoom_range=0.1,            # Randomly zoom image\n                              width_shift_range=0.2,     # randomly shift images horizontally (fraction of total width)\n                              height_shift_range=0.2,    # randomly shift images vertically (fraction of total height)\n                              horizontal_flip=True,      # randomly flip images\n                              vertical_flip=False)       # randomly flip images\n                             \ndatagen.fit(X_train)","b3f83da9":"model.summary()","b3b62966":"model.compile(optimizer=Adam(lr = 1e-4), loss= 'categorical_crossentropy', metrics=['accuracy'])","e126a959":"batch_size=64\nmodel_history = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = 50, validation_data = (X_test,y_test),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] \/\/ batch_size)","37be6324":"### Compiling and Training the Model","abf1c5f1":"## Call the BEAST","c7005c06":"## Add Fully Connected Layers","009852b5":"* We are not using fully connected layers of VGG16 model(include_top= False). We will add it from our side because in VGG16 softmax layer has 1000 classes and We have only 5 classes.\n* We are using pretrained weights of Imagenet\n* This is a sumary of my base model not a actual model","67fb25d0":"### Data augmentation","07d2828a":"### Setting a learning rate annealer-\nTo make the optimizer converge faster and closest to the global minimum of the loss function, an annealing method of the learning rate (LR) needs to be used."}}