{"cell_type":{"aebaa871":"code","66fa87d2":"code","ef6a3e70":"code","faae73f7":"code","0ea9b6f8":"code","1fc22bd5":"code","0cd9db04":"code","315d506d":"code","af26dd53":"code","41601b97":"code","7b0b9aff":"code","93b26857":"code","92233b63":"code","72e22e26":"code","85b020dd":"code","eb906b61":"code","64eaaa53":"code","6afaa895":"code","15fd5f65":"code","2f09c733":"code","6a7195ff":"code","0eecad26":"code","a1951ed6":"code","9f3bbe17":"markdown","c2893eaa":"markdown","4bba44e1":"markdown","35db2e53":"markdown","511c1e2f":"markdown","6cd2d5a4":"markdown","18a84283":"markdown","d490e18e":"markdown","93504cef":"markdown","efde79d0":"markdown","5cc33982":"markdown","472c9113":"markdown","8021ce7e":"markdown","72480d04":"markdown","f56dc96b":"markdown","d69e954c":"markdown","7f32bc08":"markdown","790bdec8":"markdown","63d19f9e":"markdown"},"source":{"aebaa871":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","66fa87d2":"data = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndata.head()","ef6a3e70":"data.isnull().any()","faae73f7":"data.describe().T","0ea9b6f8":"import plotly.express as px\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Box(y=data['ejection_fraction'],name='Ejection Fraction',boxpoints='suspectedoutliers',jitter=0.3,marker_color='rgb(52, 152, 219)',line_color='rgb(52, 152, 219)'))","1fc22bd5":"fig = go.Figure()\nfig.add_trace(go.Box(y=data['platelets'],name='Platelets',boxpoints='suspectedoutliers',jitter=0.3,marker_color='rgb(20, 143, 119)',line_color='rgb(20, 143, 119)'))","0cd9db04":"fig = go.Figure()\nfig.add_trace(go.Box(y=data['creatinine_phosphokinase'],name='Creatinine Phosphokinase',boxpoints='suspectedoutliers',jitter=0.3,marker_color='rgb(40, 55, 71)',line_color='rgb(40, 55, 71)'))","315d506d":"fig = go.Figure()\nfig.add_trace(go.Box(y=data['serum_creatinine'],name='Serum Creatinine',boxpoints='suspectedoutliers',jitter=0.3,marker_color='rgb(128, 139, 150)',line_color='rgb(128, 139, 150)'))","af26dd53":"fig = go.Figure()\nfig.add_trace(go.Box(y=data['serum_sodium'],name='Serum Sodium',boxpoints='suspectedoutliers',jitter=0.3,marker_color='rgb(236, 112, 99)',line_color='rgb(236, 112, 99)'))","41601b97":"import seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize=(15,10))\nax = sns.heatmap(data.corr(),linewidths=.5,annot=True);","7b0b9aff":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\ny = data['DEATH_EVENT']\nx = data.drop(['DEATH_EVENT'],axis=1)\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=42)","93b26857":"from sklearn.linear_model import LogisticRegression\nloj_model = LogisticRegression(solver='lbfgs').fit(x_train,y_train)\ny_pred = loj_model.predict(x_test)\nprint('Accuracy Score For Logistic Regression Model:',accuracy_score(y_test,y_pred))","92233b63":"from xgboost import XGBClassifier\nxgb_model = XGBClassifier().fit(x_train,y_train)\ny_pred = xgb_model.predict(x_test)\nprint('Simple Accuracy Score For XGBoost Model:',accuracy_score(y_test,y_pred))","72e22e26":"from sklearn.model_selection import GridSearchCV\nxgb_params = {\n    'n_estimators':[100,500,1000,2000],\n    'subsample':[0.6,0.8,1.0],\n    'max_depth':[3,4,5,6],\n    'learning_rate':[0.1,0.01,0.02,0.05],\n    \n}\nxgb = XGBClassifier()\nxgb_cv_model = GridSearchCV(xgb,xgb_params,cv=10,n_jobs=-1,verbose=2)\nxgb_cv_model.fit(x_train,y_train)","85b020dd":"print('Best Parameters:' + str(xgb_cv_model.best_params_))","eb906b61":"xgb_tuned = XGBClassifier(learning_rate=xgb_cv_model.best_params_['learning_rate'],\n                         subsample=xgb_cv_model.best_params_['subsample'],\n                         max_depth=xgb_cv_model.best_params_['max_depth'],\n                         n_estimators=xgb_cv_model.best_params_['n_estimators']).fit(x_train,y_train)","64eaaa53":"y_pred = xgb_tuned.predict(x_test)\nprint('Verified Accuracy Score For XGBoost Model:',accuracy_score(y_test,y_pred))","6afaa895":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf_model = rf.fit(x_train,y_train)\ny_pred = rf_model.predict(x_test)\nprint('Simple Accuracy Score For RF Model:',accuracy_score(y_test,y_pred))","15fd5f65":"rf_params = {\n    'max_depth':[2,3,5,8,10],\n    'max_features':[2,5,8],\n    'n_estimators':[10,500,1000,2000],\n    'min_samples_split':[2,5,10]\n}\nrf = RandomForestClassifier()\nrf_cv_model = GridSearchCV(rf,rf_params,cv=10,n_jobs=-1,verbose=2)\nrf_cv_model.fit(x_train,y_train)","2f09c733":"print('Best Parameters:' + str(rf_cv_model.best_params_))","6a7195ff":"rf = RandomForestClassifier(max_depth=rf_cv_model.best_params_['max_depth'],\n                           max_features=rf_cv_model.best_params_['max_features'],\n                           n_estimators=rf_cv_model.best_params_['n_estimators'],\n                           min_samples_split=rf_cv_model.best_params_['min_samples_split']).fit(x_train,y_train)","0eecad26":"y_pred = rf.predict(x_test)\nprint('Verified Accuracy Score For RF Model:',accuracy_score(y_test,y_pred))","a1951ed6":"# Representation of important values\nimport matplotlib.pyplot as plt\nImportance = pd.DataFrame({'Importance':rf.feature_importances_*100},\n                         index=x_train.columns)\nImportance.sort_values(by='Importance',\n                      axis=0,\n                      ascending=True).plot(kind='barh',color='DarkOrange')\nplt.xlabel('Importance for variable');","9f3bbe17":"<center><h4 style=\"color:#2F4F4F;\">We have contradictory observations within this variable.<\/h4><\/center>","c2893eaa":"<a id=\"5\"><\/a> <br>\n\n<center><h1 style=\"color:#DC143C;\">Algorithms<\/h1><\/center>\n<p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n\n<center><h4 style=\"color:#2F4F4F;\">We will verify multiple models for the data we have.<\/h4><\/center>\n<p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n\n<center><h4 style=\"color:#2F4F4F;\">The models we will use<\/h4><\/center>\n\n\n[<center>Logistic Regression<\/center>](#30)\n\n[<center>Extreme Gradient Boosting (XGBoost)<\/center>](#31)\n\n[<center>Random Forests<\/center>](#32)\n\n\n    \n    \n<center><h3>I will work on these algorithms.<\/h3><\/center>","4bba44e1":"<a id=\"31\"><\/a> <br>\n<a id=\"7\"><\/a> <br>\n\n<center><h1 style=\"color:#DC143C;\">Extreme Gradient Boosting (XGBoost)<\/h1><\/center>","35db2e53":"<center><h4 style=\"color:#2F4F4F;\">This variable exists inside but that of this variable doesn't affect that much.From here I understand that we won't do anything to these outliers, but I think we need to use a robust algorithm.<\/h4><center>","511c1e2f":"<center><h4 style=\"color:#2F4F4F;\">I will fit all our models according to this separation<\/h4><\/center>","6cd2d5a4":"<center><h4 style=\"color:#2F4F4F;\">Looking at it, there is a very, very little correlation between some variables.There is a link between age, smoking and deaths, which is normal as we can guess.This map did not explain the situation to us much, we cannot make an analysis because they are dying because of this, it is completely machine learning.<\/h4><\/center>","18a84283":"<a id=\"9\"><\/a> <br>\n\n<center><h1 style=\"color:#228B22;\">Results<\/h1><\/center>\n<p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n\n<center><h3 style=\"color:#2F4F4F;\">Logistic Regression : %80 Accuracy Score<\/h3><\/center>\n    <p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n\n<center><h3 style=\"color:#2F4F4F;\">Random Forests : %72 Accuracy Score<\/h3><\/center>\n        <p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n                                                       \n\n<center><h3 style=\"color:#2F4F4F;\">eXtreme Gradient Boosting ( XGBoost ) : %75 Accuracy Score<\/h3><\/center>\n            <p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n\n<center><h3 style=\"color:#228B22;\">Thank you all for viewing, stay healthy!<\/h3><\/center>","d490e18e":"<center><h4 style=\"color:#2F4F4F;\">Yes, good, there is no missing observation when you look at it.Of course, there may be cases of contradiction, we need to take a look at this, but before that, let's take a look at the descriptive statistics.<\/h4><\/center>","93504cef":"<center><h4 style=\"color:#2F4F4F;\">Yes, there are outlier observations for this variable, as I said before, we don't know why they are inconsistent, maybe this outlier could lead to death, and now I noticed that the numbers of this variable require a very large normalization process.<\/h4><\/center>","efde79d0":"<center><h4 style=\"color:#2F4F4F;\">There doesn't seem to be much of an outlier for the ejection fraction variable so this variable wouldn't be a problem for us.<\/h4><\/center>","5cc33982":"<a id=\"2\"><\/a> <br>\n\n<center><h1 style=\"color:#DC143C;\">Descriptive Statistics<\/h1><\/center>\n    \n<center><h4 style=\"color:#2F4F4F;\">In this section, we will take a look at the data we have and try to make average estimates.<\/h4><\/center>\n\n","472c9113":"<h1 style=\"color:#FF4500;\"><center>\ud83e\udde1Heart Failure Prediction<\/center><\/h1>\n<h2 style=\"color:#228B22;\"><center>Visualization And Algorithms<\/center><\/h2>\n<img src=\"https:\/\/www.pikpng.com\/pngl\/b\/306-3065627_instagram-heart-png-clipart-background-discord-heart-emoji.png\" width=300>\n\n\n\n<p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n    <p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n        <p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n            <p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n<center><p style=\"color:#2F4F4F;\">Hello, first of all, I wish you all healthy and happy days.Our topic today is to build a high-level forecast model based on the data we have.I think we'll be successful.First of all, I want to start with visual analysis, let's save the prediction model for the end.<\/p><\/center>\n                <p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n                    \n                  \n\n\n<center><h2 style=\"color:#2F4F4F;\">Content<\/h2><\/center>\n                   \n\n\n[<center>Cleaning Data<\/center>](#1)\n\n                        \n[<center>Descriptive Statistics<\/center>](#2)\n\n                        \n[<center>Outliers<\/center>](#3)\n\n                        \n[<center>Correlation Map<\/center>](#4)\n\n                        \n[<center>Algorithms<\/center>](#5)\n\n                        \n[<center>Logistic Regression<\/center>](#6)\n\n                        \n[<center>Extreme Gradient Boosting (XGBoost)<\/center>](#7)\n\n                        \n[<center>Random Forests<\/center>](#8)\n\n                        \n[<center>Results<\/center>](#9)\n","8021ce7e":"<a id=\"1\"><\/a> <br>\n<center><h1 style=\"color:#DC143C;\">Cleaning Data<\/h1><\/center>\n<center><h4 style=\"color:#2F4F4F;\">First, let's take a look at it and clean up the data set we have.<\/h4><\/center>","72480d04":"<a id=\"3\"><\/a> <br>\n<center><h1 style=\"color:#DC143C;\">Outliers<\/h1><\/center>\n<center><h4 style=\"color:#2F4F4F;\">There is a situation like this, we do not know how the data set is collected. It may be wrong to decide on what the exceptions are against, but as a data scientist we have to look at them.\nWe will make outliers by considering certain variables, these will be numerical variables. I aim to use an interactive library for visualization.<\/h4><\/center>\n<p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n    <p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n<center><h3>Note: Our library is plotly library, those who want to see how it is done can open the cells I have hidden.<\/h3><\/center>","f56dc96b":"<a id=\"32\"><\/a> <br>\n<a id=\"8\"><\/a> <br>\n\n<center><h1 style=\"color:#DC143C;\">Random Forests<\/h1><\/center>","d69e954c":"<center><h4 style=\"color:#2F4F4F;\">In the same way, there are contradictory observations for this variable.<\/h4><\/center>","7f32bc08":"<a id=\"4\"><\/a> <br>\n<center><h1 style=\"color:#DC143C;\">Correlation Map<\/h1><\/center>\n<center><h4 style=\"color:#2F4F4F;\">I think this method can help you make a little more sense.<\/h4><\/center>\n<p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n    <p>&#160;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<p>\n        \n\n<center><h3>Note: I'll use the seaborn library for this,If you don't know how to do it, you can open the hidden cell and look.<\/h3><\/center>","790bdec8":"<center><h4 style=\"color:#2F4F4F;\">Looking at it, we can say that the data set we have appeals to middle ages.The minimum age is 40 and the maximum is 95.Other than that, I could not comment more here.Now we can take a look at the outliers.<\/h4><\/center>","63d19f9e":"<a id=\"30\"><\/a> <br>\n<a id=\"6\"><\/a> <br>\n\n<center><h1 style=\"color:#DC143C;\">Logistic Regression<\/h1><\/center>\n<center><h4 style=\"color:#2F4F4F;\">First I will do a common separation for all algorithms.<\/h4><\/center>"}}