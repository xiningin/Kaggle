{"cell_type":{"fbc1731f":"code","30fbe2ad":"code","5056e25d":"code","c53a67ce":"code","58e6703d":"code","17ec2a34":"code","9e8d7218":"code","8c43a8d0":"code","c8268309":"code","94591434":"code","943b6a86":"code","4fd3dde3":"code","1ff129d0":"code","6aaf1ac7":"code","61bdfc55":"code","29e38929":"code","ad4109d9":"code","1c60c032":"code","b311192b":"code","4e4fbfee":"code","4755702b":"code","00f5a998":"code","fd9c48cf":"code","726b3485":"code","333eb6b6":"code","decf205b":"markdown","92b6b027":"markdown","3a6180b7":"markdown","5da67285":"markdown","c0f95a23":"markdown","0f71ffe8":"markdown","9ac70ccd":"markdown","bf85653b":"markdown","0bce70e6":"markdown","d910dc4b":"markdown","0dd3fafa":"markdown","072a4a63":"markdown","09028749":"markdown","26c4291b":"markdown","6967d03c":"markdown","f11b0415":"markdown","158b5971":"markdown"},"source":{"fbc1731f":"#1. kutuphaneler\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.datasets import load_iris\n","30fbe2ad":"#2. Veri Onisleme\n#2.1. Veri Yukleme\nveriler = pd.read_csv('..\/input\/iris\/Iris.csv')\nprint(veriler.head())\n\n\nx = veriler.iloc[:,1:5].values #ba\u011f\u0131ms\u0131z de\u011fi\u015fkenler\n#y = veriler.iloc[:,5:].values #ba\u011f\u0131ml\u0131 de\u011fi\u015fken\ny = veriler.Species.values\nprint(x[:5])\nprint(y[:5])\n\n#verilerin egitim ve test icin bolunmesi\nfrom sklearn.cross_validation import train_test_split\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.33, random_state=0)\n\n#verilerin olceklenmesi\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.transform(x_test)","5056e25d":"# 1. Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression(random_state=0)\nlogr.fit(X_train,y_train) #egitim\n\ny_pred = logr.predict(X_test) #tahmin\n#print(y_pred)\n#print(y_test)\n\n#karmasiklik matrisi\ncm = confusion_matrix(y_test,y_pred)\n#print('LR')\n# %% cm visualization\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","c53a67ce":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=1, metric='minkowski')\nknn.fit(X_train,y_train)\n\ny_pred = knn.predict(X_test)\n\ncm = confusion_matrix(y_test,y_pred)\nprint('KNN')\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","58e6703d":"# 3. SVC (SVM classifier)\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='poly')\nsvc.fit(X_train,y_train)\n\ny_pred = svc.predict(X_test)\n\ncm = confusion_matrix(y_test,y_pred)\nprint('SVC')\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","17ec2a34":"# 4. Naive Bayes Classification\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\ny_pred = gnb.predict(X_test)\n\ncm = confusion_matrix(y_test,y_pred)\nprint('GNB')\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","9e8d7218":"# 5. Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion = 'entropy')\n\ndtc.fit(X_train,y_train)\ny_pred = dtc.predict(X_test)\n\ncm = confusion_matrix(y_test,y_pred)\nprint('DTC')\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","8c43a8d0":"# 6. Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=10, criterion = 'entropy')\nrfc.fit(X_train,y_train)\n\ny_pred = rfc.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('RFC')\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","c8268309":"# 7. ROC , TPR, FPR de\u011ferleri \ny_proba = rfc.predict_proba(X_test)\n#print(y_test)\n#print(y_proba[:,0])\n\nfrom sklearn import metrics\nfpr , tpr , thold = metrics.roc_curve(y_test,y_proba[:,0],pos_label='e')\nprint(fpr)\nprint(tpr)","94591434":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nveriler = pd.read_csv('..\/input\/clustering\/musteriler.csv')\nprint(veriler.head())\nX = veriler.iloc[:,3:].values\n#print(X)\nx1 = X[:,0]\ny1 = X[:,1]\nplt.scatter(x1,y1)\nplt.show()","943b6a86":"from sklearn.cluster import KMeans\nsonuclar = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters = i, init='k-means++', random_state= 123)\n    kmeans.fit(X)\n    sonuclar.append(kmeans.inertia_)\n\nplt.plot(range(1,11),sonuclar)","4fd3dde3":"kmeans = KMeans( n_clusters = 3, init = 'k-means++')\nkmeans.fit(X)\nprint(kmeans.cluster_centers_)\n\nclusters = kmeans.fit_predict(X)\nveriler[\"label\"] = clusters\n#print(clusters)\n\nplt.scatter(veriler.Hacim[veriler.label == 0 ],veriler.Maas[veriler.label == 0],color = \"red\")\nplt.scatter(veriler.Hacim[veriler.label == 1 ],veriler.Maas[veriler.label == 1],color = \"green\")\nplt.scatter(veriler.Hacim[veriler.label == 2 ], veriler.Maas[veriler.label == 2],color = \"blue\")\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],color = \"yellow\")\nplt.show()","1ff129d0":"from scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(X,method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","6aaf1ac7":"from sklearn.cluster import AgglomerativeClustering\nac = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\nY_tahmin = ac.fit_predict(X)\n#print(Y_tahmin)\n\nplt.scatter(X[Y_tahmin==0,0],X[Y_tahmin==0,1],s=100, c='red')\nplt.scatter(X[Y_tahmin==1,0],X[Y_tahmin==1,1],s=100, c='blue')\nplt.scatter(X[Y_tahmin==2,0],X[Y_tahmin==2,1],s=100, c='green')\nplt.scatter(X[Y_tahmin==3,0],X[Y_tahmin==3,1],s=100, c='yellow')\nplt.title('HC')\nplt.show()","61bdfc55":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import r2_score\nimport statsmodels.api as sm\n\nveriler = pd.read_csv('..\/input\/comparing-regression2\/maaslar2.csv')\nprint(veriler.head())\nx = veriler.iloc[:,1:2]\ny = veriler.iloc[:,2:]\nX = x.values\nY = y.values\nveriler.head()","29e38929":"veriler.corr()","ad4109d9":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X,Y)\nmodel = sm.OLS(lin_reg.predict(X),X)\nprint(model.fit().summary())\nprint(\"Linear R2 degeri:\")\nprint(r2_score(Y, lin_reg.predict((X))))\n\nplt.scatter(X,Y,color='red')\nplt.plot(x,lin_reg.predict(X), color = 'blue')\nplt.show()","1c60c032":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 4)\nx_poly = poly_reg.fit_transform(X)\n#print(x_poly)\nlin_reg2 = LinearRegression()\nlin_reg2.fit(x_poly,y)\nmodel2 = sm.OLS(lin_reg2.predict(poly_reg.fit_transform(X)),X)\n\nprint(\"Polynomial R2 degeri:\")\nprint(r2_score(Y, lin_reg2.predict(poly_reg.fit_transform(X)) ))\n\nplt.scatter(X,Y,color = 'red')\nplt.plot(X,lin_reg2.predict(poly_reg.fit_transform(X)), color = 'blue')\nplt.show()","b311192b":"from sklearn.preprocessing import StandardScaler\nsc1 = StandardScaler()\nx_olcekli = sc1.fit_transform(X)\nsc2 = StandardScaler()\ny_olcekli = sc2.fit_transform(Y)\n\nfrom sklearn.svm import SVR\n\nsvr_reg = SVR(kernel = 'rbf')\nsvr_reg.fit(x_olcekli,y_olcekli)\n\nmodel3 = sm.OLS(svr_reg.predict(x_olcekli),x_olcekli)\nprint(model3.fit().summary())\nprint(\"SVR R2 degeri:\")\nprint(r2_score(y_olcekli, svr_reg.predict(x_olcekli)) )","4e4fbfee":"plt.scatter(x_olcekli,y_olcekli,color='red')\nplt.plot(x_olcekli,svr_reg.predict(x_olcekli),color='blue')\nplt.show()","4755702b":"from sklearn.tree import DecisionTreeRegressor\nr_dt = DecisionTreeRegressor(random_state=0)\nr_dt.fit(X,Y)\nprint('dt ols')\nmodel4 = sm.OLS(r_dt.predict(X),X)\nprint(model4.fit().summary())\nprint(\"Decision Tree R2 degeri:\")\nprint(r2_score(Y, r_dt.predict(X)) )","00f5a998":"Z = X + 0.5\nK = X - 0.4\n\nplt.scatter(X,Y, color='red')\nplt.plot(x,r_dt.predict(X), color='blue')\nplt.plot(x,r_dt.predict(Z),color='green')\nplt.plot(x,r_dt.predict(K), color = 'yellow')\nplt.show()","fd9c48cf":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(n_estimators = 10, random_state=0)\nrf_reg.fit(X,Y)\nprint('dt ols')\nmodel5 = sm.OLS(rf_reg.predict(X),X)\nprint(model5.fit().summary())\nprint(\"Random Forest R2 degeri:\")\nprint(r2_score(Y, rf_reg.predict(X)) )","726b3485":"plt.scatter(X,Y, color='red')\nplt.plot(x,rf_reg.predict(X), color = 'blue')\nplt.plot(x,rf_reg.predict(Z), color = 'green')\nplt.show()","333eb6b6":"print('----------------')\nprint(\"Linear R2 value:\")\nprint(r2_score(Y, lin_reg.predict((X))))\n\nprint(\"Polynomial R2 value:\")\nprint(r2_score(Y, lin_reg2.predict(poly_reg.fit_transform(X)) ))\n\nprint(\"SVR R2 value:\")\nprint(r2_score(y_olcekli, svr_reg.predict(x_olcekli)) )\n\nprint(\"Decision Tree R2 value:\")\nprint(r2_score(Y, r_dt.predict(X)) )\n\nprint(\"Random Forest R2 value:\")\nprint(r2_score(Y, rf_reg.predict(X)) )","decf205b":"# Comparing Regression Methods (MLR-PR-SVR-DT-RF)\n","92b6b027":"\n<a id=\"2.\"><\/a> \n# K-Nearest Neighbour (KNN) Classification","3a6180b7":"\n<a id=\"4.\"><\/a> \n# Naive Bayes Classification","5da67285":"<a id=\"8.\"><\/a> \n# Hierarchical Clustering\n\n## Dendogram\n\n","c0f95a23":"\n<a id=\"10.\"><\/a> \n# Polynomial Regression\n","0f71ffe8":"It is the kernel that I have tried and compiled from the courses of [Dr. \u015eadi Evren \u015eeker](https:\/\/www.udemy.com\/user\/sadievrenseker\/) (Language of the courses is Turkish: [Python ile Makine \u00d6\u011frenmesi](https:\/\/www.udemy.com\/makine-ogrenmesi\/)), which is has more than 5 courses on Udemy.\n\n\n\n# Content\n\n## Comparing Classification Methods LR-KNN-SVM-NB-RF\n\n* [Logistic Regression Classification](#1.)\n* [K-Nearest Neighbour (KNN) Classification](#2.)\n* [SVM Classification](#3.)\n* [Naive Bayes Classification](#4.)\n* [Decision Tree Classification](#5.)\n* [Random Forest Classification](#6.)\n\n![](https:\/\/iili.io\/JGe6Ss.png)\n\n\n## Comparing Clustering Methods K-Means-Hierarchical\n\n* [K-Means Clustering](#7.)\n* [Hierarchical Clustering](#8.)\n\n![](https:\/\/iili.io\/JGebi7.png)\n\n\n## Comparing Regression Methods (MLR-PR-SVR-DT-RF)\n\n* [Linear Regression](#9.)\n* [Polynomial Regression](#10.)\n* [Support Vector Regression , Scaling](#11.)\n* [Decision Tree](#12.)\n* [Random Forest](#13.)\n\n\n![](https:\/\/iili.io\/JGkfWB.png)\n","9ac70ccd":"\n<a id=\"13.\"><\/a> \n# Random Forest\n","bf85653b":"## HC","0bce70e6":"<a id=\"11.\"><\/a> \n# Support Vector Regression , Scaling\n","d910dc4b":"# Comparing Classification Methods LR-KNN-SVM-NB-RF\n\n\n<a id=\"1.\"><\/a> \n# Logistic Regression Classification","0dd3fafa":"\n<a id=\"7.\"><\/a> \n# K-Means Clustering","072a4a63":"\n<a id=\"5.\"><\/a> \n# Decision Tree Classification","09028749":"<a id=\"12.\"><\/a> \n# Decision Tree\n\n* [Random Forest](#13.)\n","26c4291b":"\n<a id=\"3.\"><\/a> \n# SVM Classification","6967d03c":"<a id=\"9.\"><\/a> \n# Linear Regression\n","f11b0415":"# Comparing Clustering Methods K-Means-Hierarchical","158b5971":"\n<a id=\"6.\"><\/a> \n# Random Forest Classification"}}