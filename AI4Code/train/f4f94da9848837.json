{"cell_type":{"5f076350":"code","80bdbd53":"code","4c0d11ee":"code","8b4109ee":"code","c90c803c":"code","47ac3cb2":"code","fe0c8c72":"code","ddbb0465":"code","c0b3af8e":"code","3b34106a":"code","4999657c":"code","7f67789a":"code","fa223508":"code","bd699707":"code","98a46c11":"code","3a151898":"code","07a67115":"markdown","2049247e":"markdown","1129dd14":"markdown","7ba3a49f":"markdown","b4551252":"markdown","04492340":"markdown","1e164310":"markdown","1d6fe5a6":"markdown","743fcf08":"markdown","c4a36e13":"markdown","3b8a161c":"markdown","a0e6c45d":"markdown","1c075d16":"markdown"},"source":{"5f076350":"import requests\nimport numpy as np # linear algebra\nimport pandas as pd # dataframes\nfrom bs4 import BeautifulSoup # work with html\nimport nltk # natural language packages\nimport string # to do some work with strings\nimport matplotlib.pyplot as plt # data visualization\n%matplotlib inline  \nimport seaborn as sns # data visualization\nsns.set(color_codes=True) # data visualization\nfrom textblob import TextBlob # sentiment analysis ","80bdbd53":"# Getting the Alice's HTML\ntry:\n    r = requests.get('https:\/\/www.gutenberg.org\/files\/11\/11-h\/11-h.htm', verify=False)\nexcept:\n    print(\"Ops, not here\")\n    \n# P.S.: Unhappy, we cannot run this code here in Kaggle, but I'll continue the process, and import the already extracted DataSet","4c0d11ee":"tokens = []\nwith open(\"..\/input\/tokens.txt\", \"r\") as f:\n    for line in f:\n        tokens.append(str(line.strip()))\nprint(tokens[0:8])","8b4109ee":"# Looping through the tokens and make them lower case\nwords = []\nfor word in tokens:\n    words.append(word.lower())\nwords[0:5]\n","c90c803c":"# Removing the stopwords\nfrom nltk.corpus import stopwords\n#Here you may need download the stopwords: nltk.download('stopwords')\nsw = stopwords.words('english')\nwords_ns = []\nfor word in words:\n    if word not in sw:\n        words_ns.append(word)\nwords_ns[0:5]\n","47ac3cb2":"# Creating the word frequency distribution\nfreqdist = nltk.FreqDist(words_ns)\nfreqdist.most_common(30)","fe0c8c72":"# What are doing gutenberg and project doing there? I also dislike some of these words:\nnew_stopwords = ['gutenberg', 'project', 'would', 'went', '1', 'e', 'tm', 'could', 'must']\nword_final = []\nfor word in words_ns:\n    if word not in new_stopwords:\n        word_final.append(word)","ddbb0465":"# Plotting the word frequency distribution\nfreqdist = nltk.FreqDist(word_final)\nplt.figure(figsize=(18,9))\nplt.xlabel('Words', fontsize=18)\nplt.ylabel('Freq', fontsize=16)\nplt.xticks(size = 15)\nplt.yticks(size = 15)\nfreqdist.plot(20)","c0b3af8e":"# what about bigrams?\nbigrams = list(nltk.bigrams(word_final))\nfreqdist = nltk.FreqDist(bigrams)\nplt.figure(figsize=(18,9))\nplt.xlabel('Words', fontsize=18)\nplt.ylabel('Freq', fontsize=16)\nplt.xticks(size = 15)\nplt.yticks(size = 15)\nfreqdist.plot(25)\n","3b34106a":"# what about trigrams?\ntrigrams = list(nltk.trigrams(word_final))\nfreqdist = nltk.FreqDist(trigrams)\nfreqdist.most_common(6)","4999657c":"#returning the text to a string format\nall_text = \"\".join([\" \" + i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()\n#here I'm at the start of the text, because the first charachters are descriptions of the site\nall_text[2000:2500]\nsize = len(all_text)\nall_text = all_text[2165:size]\nsize = len(all_text)\n#Dividing in 30 parts\npart_size = size\/30\ndf = pd.DataFrame()\nparts = []\nfor i in range(30):\n    parts.append(all_text[int((i*part_size)):int((part_size*(i+1)))])","7f67789a":"#turning into a dataframe (to easily manipulate\nd = {'sentence':parts}\ndf = pd.DataFrame(d)","fa223508":"#The last five parts also are texts from the site, so we're droping it\ndf = df.drop(df.index[29])\ndf = df.drop(df.index[28])\ndf = df.drop(df.index[27])\ndf = df.drop(df.index[26])\ndf = df.drop(df.index[25])","bd699707":"#defining the diferent sentiment analysis methods:\n\n#TextBlob:\ndef analize_sentiment_textblop(sentence):\n    analysis = TextBlob(sentence)\n    return analysis.sentiment.polarity\n\n#Vader:\n# analyser = SentimentIntensityAnalyzer()\ndef sentiment_analyzer_positive(sentence):\n    score = analyser.polarity_scores(sentence)\n    retorno = score.get('pos')\n    return retorno\n\ndef sentiment_analyzer_negative(sentence):\n    score = analyser.polarity_scores(sentence)\n    retorno = score.get('neg')\n    return retorno\n\ndef sentiment_analyzer_vader(sentence):\n    score = analyser.polarity_scores(sentence)\n    #print(\"{:-<40} {}\".format(sentence, str(score)))\n    retorno = score.get('compound')\n    return retorno","98a46c11":"df['textblop'] = df['sentence'].apply(lambda x: analize_sentiment_textblop(x))","3a151898":"df[['sentence','textblop']].plot(kind='bar', title='Sentiment \/ part of the book', figsize=(19,9), fontsize=10, colormap='viridis')\nplt.show()","07a67115":"# This would be the next steps:\n\n    #Setting the correct text encoding of the HTML page\n        r.encoding = 'utf-8'\n\n    #Extracting the HTML from the request object\n        html = r.text","2049247e":"If we print part of the HTML here, our return is:\n\n**print(html[0:500])**\n\n ?xml version=\"1.0\" encoding=\"utf-8\"?>\n !DOCTYPE html\n   PUBLIC \"-\/\/W3C\/\/DTD XHTML 1.0 Strict\/\/EN\"\n   \"http:\/\/www.w3.org\/TR\/xhtml1\/DTD\/xhtml1-strict.dtd\" >\n html xmlns=\"http:\/\/www.w3.org\/1999\/xhtml\" lang=\"en\">\n   head>\n    meta content=\"pg2html (binary v0.17)\" name=\"linkgenerator\" \/>\n     title>\n       Alice's Adventures in Wonderland, by Lewis Carroll\n    \/title>\n     style type=\"text\/css\">\n    <!--\n    body { margin:5%; background:#faebd0; text-align:justify}\n    P { text-in\n\n# Of course we don't want all this, so...","1129dd14":"# Now, we need our **data**...\n Fortunately, there's this link: [](http:\/\/www.gutenberg.org\/).\n Acording to the site:\n*\"**Project Gutenberg** offers over 58,000 free eBooks. Choose among free epub and Kindle eBooks, download them or read them online. You will find the world's great literature here, with focus on older works for which U.S. copyright has expired. Thousands of volunteers digitized and diligently proofread the eBooks, for enjoyment and education.\"*\n\nAnd they have our book! So, we'll use request and BeautifulSoup to acess the URL, colect the HTML and extract just the text that we need!!\n","7ba3a49f":"# Great! Now we can proceed direct coding!","b4551252":"# Hm... Looks like we have a happy ending... But, of course, we need to investigate it...\n\nFor now this is what I have...\n\n# Thanks for reading!","04492340":"    #We would not use vader here, but you may try...\n         df['vader'] = df['sentence'].apply(lambda x: sentiment_analyzer_vader(x))\n","1e164310":"# Ok,\nNow I'll import the tokens that we created with the commands we had written\nThis isn't a part of the analysis, but we need here in Kaggle:\n\nIf you wish to know how I saved, here's the code:\n\n\n    with open(\"tokens.txt\", \"w\") as f:\n        for s in tokens:\n                f.write(str(s) +\"\\n\")\n\n\n","1d6fe5a6":"    #Creating a BeautifulSoup object from the HTML\n        soup = BeautifulSoup(html)\n\n    #Getting the text out of the soup\n        text = BeautifulSoup.get_text(soup)\n\n# Printing a part of our beloved text\nprint(text[3670:4480])\n\n***There was nothing so very remarkable in that; nor did Alice think it so\n      very much out of the way to hear the Rabbit say to itself, \u2018Oh dear! Oh\n      dear! I shall be late!\u2019 (when she thought it over afterwards, it occurred\n      to her that she ought to have wondered at this, but at the time it all\n      seemed quite natural); but when the Rabbit actually took a watch out of\n      its waistcoat-pocket, and looked at it, and then hurried on, Alice started\n      to her feet, for it flashed across her mind that she had never before seen\n      a rabbit with either a waistcoat-pocket, or a watch to take out of it, and\n      burning with curiosity, she ran across the field after it, and fortunately\n      was just in time to see it pop down a large rabbit-hole under the hedge.***","743fcf08":"# oK, now that we saw how \"said alice\" is frequent...\n\nLet's do some **Sentiment Analysis**...\n\nFirts, I'm separating the text in 30 parts (from the beginning to the end)","c4a36e13":"But we need to do some filtering in this text... \nFor this, we tokenize our data:\n\n    #Creating a tokenizer that finds only words\n        tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n\n    #Tokenizing the text\n        tokens = tokenizer.tokenize(text)\n\nPrinting out the first 8 words \/ tokens\n**print(tokens[0:8])**\n\n***['Alice', 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll']***","3b8a161c":"\n\n# First, we'll start importing the packages:","a0e6c45d":"PS: Another option of sentiment analysis that I like to use, especialy for social media,is **vaderSentiment**, I'll not be using in this Kernel, but bellow is what we would need\n* from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer*","1c075d16":"# Hii!\n\n*In this Kernel, we'll be doing some simple exploratory text analysis under one of my favorite classics of all time, **Lewis Carroll's** masterpiece, **Alice in Wonderland***\n\n![](https:\/\/s8296.pcdn.co\/wp-content\/uploads\/2015\/02\/20150225_alice.jpg)\n\nIt's just a simple work, feel free to ask anything and suggest improvements\n\nMy email: rbiasuz@ucs.br\n\n**Hope you enjoy it!!\n**\n:D\n"}}