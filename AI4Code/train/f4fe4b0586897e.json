{"cell_type":{"3b43399e":"code","3afd7795":"code","be1b3eb4":"code","2f8526fd":"code","72dbd16b":"code","7ac1c12d":"code","1a5fd163":"code","382c7ccf":"code","6173b0dd":"code","765d6284":"code","7b8ca707":"code","eac93af0":"code","6aacc91c":"code","ad04f431":"markdown","c6669bf9":"markdown","230d471e":"markdown","325a2399":"markdown","c643e997":"markdown","98a2074d":"markdown","796f6c53":"markdown","0d60891f":"markdown","dedce9ac":"markdown","4f0bdbef":"markdown","507e9249":"markdown"},"source":{"3b43399e":"# inspired by [0]\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom datetime import datetime\nimport os\nprint(os.listdir(\"..\/input\"))","3afd7795":"# inspired by [...]\ns1=pd.read_csv('..\/input\/s1v8-nanashi-90-lines-solution-0901-fast\/s1-v8.csv')['target']#[1]\ns2=pd.read_csv('..\/input\/santander-magic-lgb-0-901\/submission.csv')['target']#[2]\ns3=pd.read_csv('..\/input\/s3v32-ashish-gupta-eda-pca-scaler-lgbm\/s3-v32.csv')['target']#[3]\ns4=pd.read_csv('..\/input\/eda-pca-simple-lgbm-on-kfold-technique\/submission26.csv')['target']#[4]\ns5=pd.read_csv('..\/input\/lgb-2-leaves-augment\/lgb_submission.csv')['target']#[5]\ns6=pd.read_csv('..\/input\/s6v1-ole-morten-light-gbm-with-data-augment\/s6-v1.csv')['target']#[6]\ns7=pd.read_csv('..\/input\/s7v19-subham-sharma-what-is-next-in-santander\/s7-v19.csv')['target']#[7]\ns8=pd.read_csv('..\/input\/s8v5-joshua-reed-santander-customer-transaction\/s8-v5.csv')['target']#[8]\ns9=pd.read_csv('..\/input\/s9v16-gauravtambi-lgbm-augmentation\/s9-v16.csv')['target']#[9]\ns10=pd.read_csv('..\/input\/best-parameters-lb-0-900\/submission.csv')['target']#[10]\n\nsubmission = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/sample_submission.csv')\n\nsolutions_set = pd.DataFrame({'s1': s1, 's2': s2, 's3': s3, 's4': s4, 's5': s5, 's6': s6,\n                              's7': s7, 's8': s8, 's9': s9, 's10': s10})","be1b3eb4":"# since we use AUC, and distribution of the probability is not Normal, Kendal correlation is more appropriate\nkendall = solutions_set.corr(method = 'kendall')\nplt.figure(figsize=(10, 6))\nplt.title('Kendall correlations between the 0.901-solutions')\nsns.heatmap(kendall, annot = True, fmt = \".3f\")","2f8526fd":"# Pearson correlations between the submissions\npearson = solutions_set.corr(method = 'pearson')\nplt.figure(figsize=(10, 6))\nplt.title('Pearson correlations between the 0.901-solutions')\nsns.heatmap(pearson, annot=True, fmt=\".3f\")","72dbd16b":"# Density of the solutions\nplt.figure(figsize=(10, 6))\nplt.title('Density of the 0.901-solutions')\nsns.kdeplot(s1, label = 's1', shade = True)\nsns.kdeplot(s2, label = 's2', shade = True)\nsns.kdeplot(s3, label = 's3', shade = True)\nsns.kdeplot(s4, label = 's4', shade = True)\nsns.kdeplot(s5, label = 's5', shade = True)\nsns.kdeplot(s6, label = 's6', shade = True)\nsns.kdeplot(s7, label = 's7', shade = True)\nsns.kdeplot(s8, label = 's8', shade = True)\nsns.kdeplot(s9, label = 's9', shade = True)\nsns.kdeplot(s10, label = 's10', shade = True)","7ac1c12d":"# Preprocessing: scaling (scale all the submissions to vars with mean = 0 and std = 1 since PCA doesn't like inputs with different scales)\nscaler = StandardScaler()\nsolutions_set_scaled = pd.DataFrame(scaler.fit_transform(solutions_set),\n                                    columns = ['s1', 's2', 's3', 's4', 's5', 's6', \n                                               's7', 's8', 's9', 's10'])","1a5fd163":"solutions_set_scaled.describe().applymap('{:,.2f}'.format)","382c7ccf":"# increase the weight for the s5\nsolutions_set_scaled['s5'] = solutions_set_scaled['s5'] * 3\n\n# table style (red color)\ndef red_color(val):\n    color = 'red'\n    return 'color: %s' % color\n\n# table: implement styles\nsolutions_set_scaled_style1 = solutions_set_scaled.describe().applymap('{:,.2f}'.format)\nsolutions_set_scaled_style1.style.applymap(red_color, subset = ['s5'])","6173b0dd":"# PCA\npca = PCA(n_components = 1)\nfactor = pca.fit_transform(solutions_set_scaled)\nprint(pca.explained_variance_ratio_)","765d6284":"# Loadings. These loadings can be interpreted as weights of each submission\npca.components_","7b8ca707":"# Pearson correlation of the submissions with the extracted factor\nplt.figure(figsize=(10, 6))\nplt.title('Pearson correlation of the extracted factor with the 0.901-solutions')\nsolutions_set_scaled['factor'] = factor\npearson_pca = solutions_set_scaled.corr(method = 'pearson')\nsns.heatmap(pearson_pca, annot = True, fmt = \".3f\")","eac93af0":"# Kendall correlation of the submissions with the extracted factor\nplt.figure(figsize=(10, 6))\nplt.title('Kendall correlation of the extracted factor with the 0.901-solutions')\nkendall_pca = solutions_set_scaled.corr(method = 'kendall')\nsns.heatmap(kendall_pca, annot = True, fmt = \".3f\")","6aacc91c":"# PCA blender\n# Since we use AUC the values for the target don't really matter\n# What matters is the order of the target values\nsubmission['target'] = solutions_set_scaled['factor']\n\nfilename=\"blended_submission_{:%Y-%m-%d_%H_%M}.csv\".format(datetime.now())\nsubmission.to_csv(filename, index=False)","ad04f431":"## *Stay tuned, I'll continue to add 0.901 solutions to the blender","c6669bf9":"**The main purpose:** \n\nUsing Principal Component Analysis (PCA) extract a single factor which indirectly influence all the 0.901-submissions, this factor will be the final submission. You can use this method to increase the score of your own solutions.\n\n**What's new:** \n\n- **Weighted** PCA approach is tested: more weight is added for the solution #5 [5] to make it more important for the PCA.\n- 3 new 0.901-solution are added (see [8], [9], [10]).\n- all the 0.901-solutions are updated to the latest versions (if the current version doesn't have 0.901 score than previous version is used (for example, s6-v1.csv dataset is used instead of the original kernel))\n\nPrevious version of this kernel is [here](https:\/\/www.kaggle.com\/darbin\/pca-blender-of-0-901-solutions?scriptVersionId=12188974)","230d471e":"# Submission","325a2399":"# Inspired by:\n\n[0] [Nanashi (#242), \"Simple blend (my best score)\"](https:\/\/www.kaggle.com\/jesucristo\/simple-blend-my-best-score)\n\n[1] [Nanashi (#242), \"90 lines solution 0.901 #Fast\": V8](https:\/\/www.kaggle.com\/jesucristo\/90-lines-solution-0-901-fast?scriptVersionId=11837989). Dataset: s1-v8\n\n[2] [Nanashi (#242), \"Santander Magic LGB 0.901\": V21](https:\/\/www.kaggle.com\/jesucristo\/santander-magic-lgb-0-901?scriptVersionId=11960403). Dataset: original (current version of kernel)\n\n[3] [Ashish Gupta (#237), \"EDA, PCA + LGBM : Santander Transactions\": V32](https:\/\/www.kaggle.com\/roydatascience\/eda-pca-lgbm-santander-transactions?scriptVersionId=11798375). Dataset: s3-v32\n\n[4] [Ashish Gupta (#237), \"EDA, PCA + Simple LGBM on KFold Technique\"](https:\/\/www.kaggle.com\/roydatascience\/eda-pca-simple-lgbm-on-kfold-technique). Dataset: original (current version of kernel)\n\n[5] [Jiwei Liu (#5), \"LGB 2 leaves + augment\"](https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment). Dataset: original (current version of kernel)\n\n[6] [Ole Morten Grod\u00e5s (#649), \"Lightgbm with data augmentation\"](https:\/\/www.kaggle.com\/omgrodas\/lightgbm-with-data-augmentation). Dataset: original (current version of kernel)\n\n[7] [Subham Sharma (#1977), \"Santander (-: Augment to the rescue\": V19](https:\/\/www.kaggle.com\/subhamsharma96\/santander-augment-to-the-rescue?scriptVersionId=11821134). Dataset: s7-v19\n\n[8] [Joshua Reed (#1821), \"Santander Customer Transaction Prediction\": V5](https:\/\/www.kaggle.com\/josh24990\/santander-customer-transaction-prediction). Dataset: s8-v5\n\n[9] [GauravTambi (#1497), \"LGBM + Augmentation\": V16](https:\/\/www.kaggle.com\/gtambi\/lgbm-augmentation?scriptVersionId=12043767). Dataset: s9-v16\n\n[10] [Deepak N (#2039), \"Best Parameters LB 0.900\"](https:\/\/www.kaggle.com\/deepak525\/best-parameters-lb-0-900). Dataset: original (current version of kernel)\n\n# PCA components:\n- [sklearn.decomposition.pca](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)\n- [sklearn.preprocessing.StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html)","c643e997":"# PCA. Extract one single factor, which explains all the 7 submissions variations","98a2074d":"Note that we've just increased the importance of the s5 (std = 3 instead of 1) for the PCA","796f6c53":"Check the importance of the s5 in the final PCA solution: 0.71 comparing to 0.23 for other solutions","0d60891f":"By the way: increasing the weight for s5 has increased the explained_variance_ratio from 0.99942 to 0.99957","dedce9ac":"# Solutions analysis","4f0bdbef":"Kendall correlations between the extracted factor and the submissions are **higher** (statistically significant?) than any other correlation between the submissions. High correlation of the factor with s5 [5] solution shows the higher importance of that solution for the final PCA model.","507e9249":"Submission s7 remains the less correlated with the other solutions.\n\nSolution s8 has a good correlation with s5 and s1."}}