{"cell_type":{"1ea65449":"code","d7beef2a":"code","884cb05b":"code","fd962c2d":"code","6d3d9a78":"code","21dd656e":"code","7c42c19f":"code","416bfadc":"code","a7000056":"code","682231d1":"code","540fee4f":"code","2b8c7745":"code","5de8c525":"code","d6380e13":"code","b1774e19":"code","d632afec":"code","06a5c8a4":"code","04688b5c":"code","2a0554be":"code","510fcc9b":"code","cab577bf":"code","4f374742":"code","11a6d48b":"code","9c531687":"code","ae688d13":"code","faec3252":"code","35db28dc":"code","139ca506":"markdown","59e197a5":"markdown","bc1ba08d":"markdown","b3451c35":"markdown","7ae9ba66":"markdown","0a4fa285":"markdown","54835461":"markdown","d5cb4675":"markdown"},"source":{"1ea65449":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport torch","d7beef2a":"corpus = [\n     'This is a book.',\n     'That is a apple.',\n     'And this is a pencil.',\n     'And that is a notebook.' ]\nprint(type(corpus))","884cb05b":"tfidfVectorizer = TfidfVectorizer()\ntfidfVectorizer.fit(corpus) ","fd962c2d":"tfidfVectorizer.get_params()","6d3d9a78":"tfidfVectorizer.vocabulary_ \n","21dd656e":"sorted(tfidfVectorizer.vocabulary_.items()) ","7c42c19f":"result = tfidfVectorizer.transform(corpus)\n\n# print(result)\nprint(\"array = \",result.toarray())\nresultArray = result.toarray()\nprint(\"shape =\",resultArray.shape)","416bfadc":"tfidfVectorizer = TfidfVectorizer(min_df=2) # document frequency >= 2\ntfidfVectorizer.fit(corpus) ","a7000056":"tfidfVectorizer.get_params()","682231d1":"tfidfVectorizer.vocabulary_ ","540fee4f":"sorted(tfidfVectorizer.vocabulary_.items()) ","2b8c7745":"result = tfidfVectorizer.transform(corpus)\n\n# print(result)\nprint(\"array = \",result.toarray())\nresultArray = result.toarray()\nprint(\"shape =\",resultArray.shape)","5de8c525":"tfidfVectorizer = TfidfVectorizer(analyzer='char')\ntfidfVectorizer.fit(corpus) ","d6380e13":"tfidfVectorizer.get_params()","b1774e19":"tfidfVectorizer.vocabulary_ ","d632afec":"sorted(tfidfVectorizer.vocabulary_.items()) ","06a5c8a4":"result = tfidfVectorizer.transform(corpus)\n\n# print(result)\nprint(\"array = \",result.toarray())\nresultArray = result.toarray()\nprint(\"shape =\",resultArray.shape)","04688b5c":"tfidfVectorizer = TfidfVectorizer(ngram_range=(1,2)) # one word ~ two word \ntfidfVectorizer.fit(corpus) ","2a0554be":"tfidfVectorizer.get_params()","510fcc9b":"tfidfVectorizer.vocabulary_ ","cab577bf":"sorted(tfidfVectorizer.vocabulary_.items()) ","4f374742":"result = tfidfVectorizer.transform(corpus)\n\n# print(result)\nprint(\"array = \",result.toarray())\nresultArray = result.toarray()\nprint(\"shape =\",resultArray.shape)","11a6d48b":"tfidfVectorizer = TfidfVectorizer(max_features=10)\ntfidfVectorizer.fit(corpus) ","9c531687":"tfidfVectorizer.get_params()","ae688d13":"tfidfVectorizer.vocabulary_ ","faec3252":"sorted(tfidfVectorizer.vocabulary_.items()) ","35db28dc":"result = tfidfVectorizer.transform(corpus)\n\n# print(result)\nprint(\"array = \",result.toarray())\nresultArray = result.toarray()\nprint(\"shape =\",resultArray.shape)","139ca506":"# 4.ngram_range parameter","59e197a5":"# 1.default parameter","bc1ba08d":"max_featuresint, default=None\n\nIf not None, build a vocabulary that only consider the top max_features \nordered by term frequency across the corpus.\n\nThis parameter is ignored if vocabulary is not None.","b3451c35":"# make data","7ae9ba66":"# 3.analyzer parameter","0a4fa285":"'word' or 'char'","54835461":"# 2.min_df parameter\n\ndf => document-frequency","d5cb4675":"# 5.max_features parameter"}}