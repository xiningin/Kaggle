{"cell_type":{"cbd0eb4e":"code","e3f9b47a":"code","897d8974":"code","4b88dc11":"code","3bdf2bba":"code","d81d0e49":"code","de9c2117":"code","a176a2dd":"code","e3d24b47":"code","992c188c":"code","b8a7cfe3":"code","3bea0e31":"code","8f592c99":"code","f5956727":"code","8048a327":"code","fd756f6d":"code","81088ffe":"code","d5d5f6e6":"code","17111f27":"code","5e2e6ccc":"code","72f67dc7":"code","77071861":"code","1cf446e4":"code","a5f60e9f":"code","9e5e99ab":"code","c5704f64":"code","7ea04537":"code","768e0142":"code","217c5f4f":"code","c570d7cc":"code","e5fd01a6":"markdown","b7498057":"markdown","e3b271bc":"markdown","8b665afb":"markdown","f63f69a2":"markdown","e4fbd8de":"markdown","fb8c9c0d":"markdown","a9c5bef3":"markdown","46d61200":"markdown","32b55f63":"markdown","2fd1a083":"markdown","e84e8fd0":"markdown","e4984402":"markdown","7987230a":"markdown","d95b766c":"markdown","05224c77":"markdown","8b13d5da":"markdown","b0e5d4ce":"markdown","a43dce35":"markdown","1b5b8f74":"markdown","993e8c0d":"markdown","3f6f31e0":"markdown"},"source":{"cbd0eb4e":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Dense, BatchNormalization\nfrom tensorflow.keras.initializers import RandomNormal, Constant","e3f9b47a":"df = pd.read_csv('\/kaggle\/input\/iba-ml1-final-project\/train.csv')","897d8974":"df['Product_Category'].value_counts()","4b88dc11":"df.drop(df[df['Product_Category'] == 'Chemises'].index, inplace = True)\ndf.drop(df[df['Product_Category'] == 'Casual bottoms'].index, inplace = True)","3bdf2bba":"import nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer() \n\ndef preprocess(sentence):\n    \n    sentence=str(sentence)\n    sentence = sentence.lower()\n    sentence=sentence.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', sentence)\n    rem_url=re.sub(r'http\\S+', '',cleantext)\n    rem_num = re.sub('[0-9]+', '', rem_url)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(rem_num)  \n    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n    stem_words=[stemmer.stem(w) for w in filtered_words]\n    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n    return \" \".join(filtered_words)","d81d0e49":"df['Review']=df['Review'].map(lambda s:preprocess(s)) \ndf[\"Review_Title\"]=df[\"Review_Title\"].map(lambda s:preprocess(s)) ","de9c2117":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nimr = SimpleImputer(strategy='most_frequent')\ndf[['Division', 'Product_Category','Department']]=imr.fit_transform(df[['Division', \n                                                                            'Product_Category',\n                                                                            'Department']])\nOhe = OneHotEncoder().fit_transform(df[['Division', 'Product_Category','Department']]).toarray()","a176a2dd":"from numpy import asarray\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnumeric_data =  df[['Age','Pos_Feedback_Cnt']]\nscaled_num = scaler.fit_transform(numeric_data)","e3d24b47":"num_tags = 27    # Number of the columns obtained after applying OHE to the 3 categorical dataset\nnumeric_val=2  # Number of features for numerical values, which were 2 as it is described in the code written above\nnum_words = 10000  # Size of vocabulary obtained when preprocessing text data\nnum_rating = 5  # Number of predictions for rating values\n\ntitle_input = keras.Input(\n    shape=(None,), name=\"title\")  # Variable-length sequence of ints\nbody_input = keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\ntags_input = keras.Input(                         # Input for the categorical values\n    shape=(num_tags,), name=\"tags\") \nnum_inputs = keras.Input(         # Input for the numerical values\n    shape=(numeric_val,), name=\"numeric\")\n\ntitle_features = layers.Embedding(num_words, 64)(title_input)  # Initializing the embedding layer for the title features\nBatchNormalization()\nbody_features = layers.Embedding(num_words, 128)(body_input)  # Initializing the embedding layer for the body features\nBatchNormalization()\ntags_features = layers.Dense(16, activation='relu')(tags_input)  # Initializing the dense layer for categorical values\nBatchNormalization()\nnum_features = layers.Dense(16, activation='relu')(num_inputs)  # Initializing the dense layer for numerical values\nBatchNormalization()\n\n\nbody_features=layers.Dropout(.3)(body_features)\ntitle_features = layers.Dropout(.3)(title_features)\n\ntitle_features =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,recurrent_dropout=0.2))(title_features)\nBatchNormalization()\n\nbody_features =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,recurrent_dropout=0.2))(body_features)\nBatchNormalization()\n\nbody_features = layers.Dropout(.3)(body_features)\nbody_features = layers.Dense(16, activation='relu')(body_features)\nBatchNormalization()\n\ntitle_features=layers.Dropout(.3)(title_features)\ntitle_features=layers.Dense(16, activation='relu')(title_features)\nBatchNormalization()\n\nBatchNormalization(momentum=0.95, epsilon=0.005,beta_initializer=RandomNormal(mean=0.0, stddev=0.05),\n                   gamma_initializer=Constant(value=0.9))\n\nx = layers.concatenate([body_features,title_features,tags_features,num_features])\n\nrecommended_pred = layers.Dense(1,activation='sigmoid', name=\"recommended\")(x)\nrating_pred = layers.Dense(num_rating,activation='softmax', name=\"rating\")(x)\n\nmodel = keras.Model(\n    inputs=[body_input,title_input,tags_input,num_inputs],\n    outputs=[recommended_pred, rating_pred]\n)","992c188c":"keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)","b8a7cfe3":"model.compile(\n    optimizer='Adam',\n    loss={\n        \"recommended\": keras.losses.BinaryCrossentropy(),\n        \"rating\": keras.losses.CategoricalCrossentropy(),\n    },\n    metrics = ['accuracy'],\n)","3bea0e31":"from sklearn.model_selection import train_test_split\ntext=df[['Review','Review_Title']]\ntext=np.array(text)\nX=np.column_stack((text,Ohe,scaled_num))\ny = df[['Rating','Recommended']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","8f592c99":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7)\nbody_transformer = vectorizer.fit_transform(df['Review']).toarray()\ntitle_transformer=vectorizer.fit_transform(df['Review_Title']).toarray()\nbody_transformer.shape","f5956727":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7)\nbody_tfidf_transf = tfidfconverter.fit_transform(df['Review']).toarray()\ntitle_tfidf_transf = tfidfconverter.fit_transform(df['Review_Title']).toarray()\nbody_tfidf_transf.shape","8048a327":"VOCAB_SIZE = 10000\nbody_data=[]\ntitle_data=[]\nfor i in range (len(X_train)):\n    body_data.append(X_train[i][0])\nfor i in range (len(X_train)):\n    title_data.append(X_train[i][1])\n    \nencoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE,ngrams=1)\nencoder.adapt(body_data)\n\nbody_data=encoder(body_data).numpy()\ntitle_data=encoder(title_data).numpy()\n\nrecommended = y_train['Recommended'].values\nrating = OneHotEncoder().fit_transform(np.array(y_train['Rating']).reshape(-1, 1)).toarray()","fd756f6d":"categorical=X_train[:,2:-2]\nnumerical=X_train[:,-2:]\ncategorical=np.asarray(categorical).astype('int')\nnumerical= np.asarray(numerical).astype('float')","81088ffe":"model.fit(    \n     [body_data, title_data, categorical,numerical], \n      [recommended, rating],\n    validation_split=0.2,\n    epochs=5,\n    batch_size=32,\n)","d5d5f6e6":"recom_predict = np.where(model.predict([body_data, title_data, categorical,numerical])[0] > 0.5, 1, 0)\nrat_predict=[np. argmax(i)+1 for i in model.predict([body_data, title_data, categorical,numerical])[1] ]","17111f27":"body_test=[]\ntitle_test=[]\nfor i in range (len(X_test)):\n    body_test.append(X_test[i][0])\nfor i in range (len(X_test)):\n    title_test.append(X_test[i][1])\n\nbody_test=encoder(body_test).numpy()\ntitle_test=encoder(title_test).numpy()","5e2e6ccc":"categorical_test=X_test[:,2:-2]\nnumerical_test=X_test[:,-2:]\ncategorical_test=np.asarray(categorical_test).astype('int')\nnumerical_test= np.asarray(numerical_test).astype('float')","72f67dc7":"recom_test_pred = np.where(model.predict([body_test, title_test, categorical_test,numerical_test])[0] > 0.5, 1, 0)\nrat_test_pred=[np. argmax(i)+1 for i in model.predict([body_test, title_test, categorical_test,numerical_test])[1] ]","77071861":"from sklearn.metrics import precision_recall_curve\nprint('Training data rating prediction report')\nprint(classification_report(y_train['Rating'].values, rat_predict))\nprint()\nprint('Test data rating prediction report')\nprint(classification_report(y_test['Rating'].values, rat_test_pred))\nprint()\n\nprint('Precision recall curve for training data')\nprecisions, recalls, thresholds = precision_recall_curve(y_train['Recommended'].values,recom_predict)\nplt.plot(precisions, recalls)\nplt.xlabel('Precision')\nplt.ylabel('Recall')\nplt.show()\n\nprint('Precision recall curve for test data')\nprecisions, recalls, thresholds = precision_recall_curve(y_test['Recommended'].values,recom_test_pred)\nplt.plot(precisions, recalls)\nplt.xlabel('Precision')\nplt.ylabel('Recall')\nplt.show()","1cf446e4":"test_df =  pd.read_csv('\/kaggle\/input\/iba-ml1-final-project\/test.csv')","a5f60e9f":"test_df['Review']=test_df['Review'].map(lambda s:preprocess(s)) \ntest_df['Review_Title']=test_df['Review_Title'].map(lambda s:preprocess(s))","9e5e99ab":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nimr = SimpleImputer(strategy='most_frequent')\ntest_df[['Division', 'Product_Category','Department']]=imr.fit_transform(test_df[['Division', \n                                                                            'Product_Category',\n                                                                            'Department']])\nOhe_fin = OneHotEncoder().fit_transform(test_df[['Division', 'Product_Category','Department']]).toarray()","c5704f64":"from numpy import asarray\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnum_data_fin =  test_df[['Age','Pos_Feedback_Cnt']]\nscaled_num_fin = scaler.fit_transform(num_data_fin)","7ea04537":"body_fin=encoder(test_df['Review']).numpy()\ntitle_fin=encoder(test_df['Review_Title']).numpy()","768e0142":"recom_fin_pred = np.where(model.predict([body_fin,title_fin,Ohe_fin,scaled_num_fin])[0] > 0.5, 1, 0).ravel()\nrat_fin_pred=[np. argmax(i)+1 for i in model.predict([body_fin,title_fin,Ohe_fin,scaled_num_fin])[1]]","217c5f4f":"submission = pd.DataFrame({'Id':test_df['Id'],'Rating':rat_fin_pred,'Recommended':recom_fin_pred})","c570d7cc":"filename = 'Test1.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","e5fd01a6":"# Count Vectorizer","b7498057":"# Model fitting","e3b271bc":"As it is advised by most of the users, I have used 'Adam' optimizer for the model. As a loss function of prediction of the 'Recommended' value, I have used BinaryCrossentropy because it is either 1 or 0. For the prediction of the 'Rating' categorical value, I have used CategoricalCrossentropy loss function, however to fit the 'Rating' value into this, I have applied OHE to the rating value which will be described later. Used metrics is 'accuracy' which is suitable to describe the model performance","8b665afb":"# Bag-Of-Words approach for the dataset","f63f69a2":"I have chosen the categorical and numerical values from the train test split to use it in the evaluation process.","e4fbd8de":"Applying the cleaning function to the review and review title columns","fb8c9c0d":"This is the model designing part of the code. I have used functional API to predict 2 target values which are \"Recommended\" binary value and \"Rating\" categorical value. I have decided to use Bidirectional LSTM for review and review_title features which is seperated as body_input and title_input respectively in the following code. I have used BatchNormalization which is used to normalize the inputs and after applying this score is increased significantly. I have used several droputs to prevent model from overfitting, however model actually performed better for train_test split data and submission while overfitted to the training model.","a9c5bef3":"For the project, I have used TextVectorization from the keras which is numerating the words with unique value. I have adapted the 'Review' texts to the enconder, because I have assumed that most of the words used in the title is most probably also in some of the texts and used \"encoder\" to fit it to the text features.","46d61200":"# Model Performance evaluation","32b55f63":"In the following code, I am stacking the text  ('Review' and 'Review_Title'), categorical (Ohe) and numerical (scaled_num) into one variable to seperate it using the train_test_split to evaluate later.","2fd1a083":"# Model Designing","e84e8fd0":"In the next line, I have created the function to clean the text from all kind of extra characters which is not directly useful for our models, such as numbers, html tags, stopwords such as, \"and\", \"is\", \"that\" which doesn't have any meaning in the context of predicting the reviewers opinion. We have also lowered case all the words, to not differentiate the same words for this reason. And as the classical NLP techniques, I have used PorterStemmer and WordNetLemmatizer which is extremely useful while categorizing the words, because some of the words are missing final characther or have some extra suffix which would be considered as different words while tokenizing so it would decrease the model performance. This two methods were enough for this project according to my opinion. ","e4984402":"As I have described in the EDA, we have 3 categorical features in this dataset, so I have imputed them first to remove NaN values from the dataset and used OneHotEncoder to encode them into numerical values and finally created Ohe variable to combine them as one value, to use later in the functional API.","7987230a":"Here I am dropping the two categories which doesn't exist in the test dataframe and appear only once or twice in this dataframe. Therefore I decided that it is not useful, and while dropping I can have the fixed size of categorical values for all data frames(test data for submission and train_test split)","d95b766c":"Following lines of codes are for creating the array with the predicted values for recommendation and rating values. Because of having the probability for the values, I have transformed them into integers. For recommendation value, if prediction is more then 50 percent I accepted it as 1. For rating values, because I have used OHE and indexing start from 0, I found the maximum probability value and find the rating value as index+1.","05224c77":"After giving the input and output values to the model, I have defined the validation_split to check in every iteration. Batch size is recommended to be 32 by most of the people. Number of epochs differs based on the parameters of the model, but for the purpose of demonstration, I will iterate it only for 10 times.","8b13d5da":"# TextVectorization approach","b0e5d4ce":"# Data preprocessing","a43dce35":"We have 2 numerical values in the dataset, so I have used StandardScaler to scale them and used them also as one value which is called \"scaled_num\"","1b5b8f74":"In the next lines, I have done the same process of the text processing, data processing, etc.","993e8c0d":"# TF-IDF Vectorizer","3f6f31e0":"Following two lines of the codes, demonstrate how to use Bag-Of-Words approach for the dataset. I haven't preferred to use it for this project, but demonstrated how to use them. Parameters are adjustable, so we can change the number of max features. min_df and max_df are used to remove very rare and very frequent appeared words in the dataset."}}