{"cell_type":{"71dd7fa9":"code","e929ffed":"code","8c8a4ed2":"code","5e078430":"code","c4798209":"code","4ed24b55":"code","00acf371":"code","7406cfc2":"code","065f4af4":"code","088bf3ce":"code","b56160c3":"code","0548aff0":"markdown","b1540477":"markdown","7f53665a":"markdown","0e363e4c":"markdown","b085a716":"markdown","89d64395":"markdown","35d85c8e":"markdown","b7ac2150":"markdown","c74afaef":"markdown","64ec49a5":"markdown","5dc8b06d":"markdown","7a0c6b46":"markdown"},"source":{"71dd7fa9":"import numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize","e929ffed":"def mae(ytrue, ypred, uout=None):\n    if isinstance(uout, pd.Series):\n        # print(f'MAE (Inspiration Phase):')\n        return np.mean(np.abs((ytrue - ypred)[uout == 0]))\n    else:\n        # print('MAE (All Phases):')\n        return np.mean(np.abs((ytrue - ypred)))","8c8a4ed2":"data = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv', usecols=['pressure', 'u_out'])\nytrue = data.pressure\nuout = data.u_out","5e078430":"pressure_sorted = np.sort(data['pressure'].unique())\nPRESSURE_MIN = pressure_sorted[0]\nPRESSURE_MAX = pressure_sorted[-1]\nPRESSURE_STEP = pressure_sorted[1] - pressure_sorted[0]\n\ndef post_process(pressure):\n    pressure = np.round((pressure - PRESSURE_MIN) \/ PRESSURE_STEP) * PRESSURE_STEP + PRESSURE_MIN\n    pressure = np.clip(pressure, PRESSURE_MIN, PRESSURE_MAX)\n    return pressure","c4798209":"oof1 = np.load('..\/input\/vpp-156-oof\/oof_preds.npy')\nsub1 = pd.read_csv('..\/input\/tfbidirectional156\/submission_median_round_tfbidirec.csv')\nsub1.pressure = post_process(sub1.pressure)\nprint(mae(ytrue, oof1, uout))\n\noof2 = np.load('..\/input\/gb-vpp-why-so-serious\/oof_preds.npy')\nsub2 = pd.read_csv('..\/input\/gb-vpp-why-so-serious\/submission_median_round.csv')\nprint(mae(ytrue, oof2, uout))\n\noof3 = np.load('..\/input\/gb-vpp-to-infinity-and-beyond-td\/oof_preds.npy')\nsub3 = pd.read_csv('..\/input\/gb-vpp-to-infinity-and-beyond-td\/submission_median_round.csv')\nprint(mae(ytrue, oof3, uout))\n\n\noof4 = pd.read_csv('..\/input\/ventilator-train-classification\/exp080_conti_rc\/oof.csv', usecols=['oof'])\noof4 = oof4.to_numpy().ravel()\nsub4 = pd.read_csv('..\/input\/ventilator-train-classification\/exp080_conti_rc\/submission_median_pp.csv')\nprint(mae(ytrue, oof4, uout))\n","4ed24b55":"X = np.stack([oof1, oof2, oof3, oof4], 1)[uout == 0]\ny = ytrue[uout == 0]\n\nprint(f'X shape: {X.shape}')\nprint(f'y shape: {y.shape}')","00acf371":"# Optimization method requires us to give some initial guess values for the weights.\n# The most straightforward way is to pick equal weights.\nx0 = np.array([0.25, 0.25, 0.25, 0.25])\n\n# The cost function is just the ensemble's mean absolute error.\ndef cost(x0):\n    return mae(y, np.sum(X*x0, -1), uout)\n\nbnds = ((0, 1), (0, 1), (0, 1), (0, 1)) # Weights have to be between 0 and 1.\ncons = ({'type': 'eq', 'fun': lambda x:  1 - sum(x)}) # Sum of the weights will be 1.\nres = minimize(cost, x0, bounds=bnds)","7406cfc2":"print(res)","065f4af4":"submission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\nfor sub in zip([sub1, sub2, sub3, sub4], res.x):\n    submission.pressure += sub[0].pressure * sub[1]","088bf3ce":"submission.to_csv('submission.csv', index=False)","b56160c3":"submission.pressure = post_process(submission.pressure)\nsubmission.to_csv('submission_pp.csv', index=False)","0548aff0":"## Optimization with Scipy","b1540477":"## Load Packages","7f53665a":"Since we ensemble four models, we have only four features for each timestep as can be seen above.","0e363e4c":"# A Basic Ensembling Technique with Scipy Optimize\n\nIn [VPP: A Basic Ensembling Technique](https:\/\/www.kaggle.com\/tolgadincer\/vpp-a-basic-ensembling-technique) notebook, I demonstrated how to stack several weak models to improve the CV score, and hence the LB score. As a metalearner I had used RidgeRegressorCV, which minimizes the mean square error.\n\nIn this notebook, I'll estimate the ensemble weights with linear programming using scipy optimize. For this we will use \n\n## Problem Formulation\nThe ensemble prediction (EP) can be written as:\n\n<center>\n    EP = $\\sum_{i=1}^{4}$ w$_i$ $\\times$ OOF$_i$\n<\/center>\n\nThen, the cost function will be just the mean absolute error calculated by the ensemble predictions, which can be written as follow:\n\n<center>\n    cost = $\\frac{1}{N}$ $\\sum$ $|y_i - EP_i|$\n<\/center>\n\nThe task is to minimize the cost function while keeping the sum of the weights at 1 and the weights themselves in the [0, 1] range. Finding the minimum of the cost function will give us the optimum weights.\n\n<center>\n    minimize(cost)\n<\/center>\n\n<center>\n    s.t\n<\/center>\n\n<center>\n    $\\sum_{i=1}^{4}$ w$_i$ = 1\n<\/center>\n\n<center>\n    0 $\\leq$ w$_i$ $\\leq$ 1\n<\/center>","b085a716":"## Post Processing","89d64395":"The MAE score printed above is the score of the stacked models with the ensemble weights found from the optimization procedure. It's 0.015 better than the best scoring model!\n\nNote that the MAE score we found is same as the MAE score in the [VPP: A Basic Ensembling Technique](https:\/\/www.kaggle.com\/tolgadincer\/vpp-a-basic-ensembling-technique) notebook up to the 4th decimal; however the weights are slightly different. The difference in the weights do not change the LB score (see Version 1).","35d85c8e":"## Competition Metric\n\nCompetition metric is the mean absolute error, which is calculated only over the inspiration phase data (`u_out==0`). ","b7ac2150":"## Load Data\n\nFrom the training data, we will need the training targets (pressure values). Since the competition metric, mean absolute error, is evaluated only on the inspiration phase (`u_out==0`), we also need the u_out values.","c74afaef":"## Ensembling the Predictions","64ec49a5":"## Input & Target Values","5dc8b06d":"## Postprocessing Tools","7a0c6b46":"## Load the OOF and the Submission Predictions"}}