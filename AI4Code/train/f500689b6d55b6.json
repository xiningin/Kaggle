{"cell_type":{"dea31f93":"code","c383786c":"code","e064aa1b":"code","1fc4e28d":"code","19ea107c":"code","f485e161":"code","5ffe4b3e":"code","1c00df9a":"code","2171f5ce":"code","725d3f1a":"code","ebcbcaed":"code","5739d8a0":"code","86a22ab3":"code","db1cbbd0":"code","45a7abab":"code","e998c079":"code","5c326fe3":"markdown","00c4b33e":"markdown","256c34e0":"markdown","3399fb01":"markdown","ce691ebf":"markdown","b2ed8ea2":"markdown","c83f8a5b":"markdown","dab0824a":"markdown","42568fb9":"markdown"},"source":{"dea31f93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c383786c":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ncolors = ['royalblue','red','deeppink', 'maroon', 'mediumorchid', 'tan', 'forestgreen', 'olive', 'goldenrod', 'lightcyan', 'navy']\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])","e064aa1b":"from sklearn.datasets import make_circles\nX, y = make_circles(n_samples=200, noise=0.01)\nplt.scatter(X[:,0], X[:,1],c=vectorizer(y))","1fc4e28d":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2,perplexity=40, random_state=42)\nX_reduced_tsne = tsne.fit_transform(X)\nplt.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1],c=vectorizer(y))","19ea107c":"tsne = TSNE(n_components=2,perplexity=5, random_state=42)\nX_5 = tsne.fit_transform(X)\ntsne = TSNE(n_components=2,perplexity=30, random_state=42)\nX_30 = tsne.fit_transform(X)\ntsne = TSNE(n_components=2,perplexity=40, random_state=42)\nX_40 = tsne.fit_transform(X)\ntsne = TSNE(n_components=2,perplexity=100, random_state=42)\nX_100 = tsne.fit_transform(X)","f485e161":"plt.figure(figsize=(16, 4))\nplt.subplot(141)\nplt.gca().set_title('Perplexity 5')\nplt.scatter(X_5[:,0], X_5[:,1],c=vectorizer(y))\nplt.subplot(142)\nplt.gca().set_title('Perplexity 30')\nplt.scatter(X_30[:,0], X_30[:,1],c=vectorizer(y))\nplt.subplot(143)\nplt.gca().set_title('Perplexity 40')\nplt.scatter(X_40[:,0], X_40[:,1],c=vectorizer(y))\nplt.subplot(144)\nplt.gca().set_title('Perplexity 1000')\nplt.scatter(X_100[:,0], X_100[:,1],c=vectorizer(y))","5ffe4b3e":"from sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1)\nmnist.target = mnist.target.astype(np.uint8)\nX = mnist[\"data\"]\ny = mnist[\"target\"]","1c00df9a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.95)","2171f5ce":"pca = PCA(n_components=2)\nX_PCA=pca.fit_transform(X_train)","725d3f1a":"tsne = TSNE(n_components=2,perplexity=100, random_state=42)\nX_100 = tsne.fit_transform(X_train)","ebcbcaed":"y_train","5739d8a0":"plt.figure(figsize=(16, 5))\nax1=plt.subplot(121)\nplt.gca().set_title('PCA')\nscatter=plt.scatter(X_PCA[:,0], X_PCA[:,1],c=vectorizer(y_train),label=color)\nax2=plt.subplot(122)\nplt.gca().set_title('tSNE')\nplt.scatter(X_100[:,0], X_100[:,1],c=vectorizer(y_train),label=y_train)","86a22ab3":"from sklearn.datasets import make_swiss_roll\nX, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)","db1cbbd0":"axes = [-11.5, 14, -2, 23, -12, 15]\n\nfig = plt.figure(figsize=(6, 5))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot)\nax.view_init(10, -70)\nax.set_xlabel(\"$x_1$\", fontsize=18)\nax.set_ylabel(\"$x_2$\", fontsize=18)\nax.set_zlabel(\"$x_3$\", fontsize=18)\nax.set_xlim(axes[0:2])\nax.set_ylim(axes[2:4])\nax.set_zlim(axes[4:6])\n\nplt.show()","45a7abab":"pca = PCA(n_components=2)\nX_PCA=pca.fit_transform(X)\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X)","e998c079":"plt.figure(figsize=(16, 4))\nplt.subplot(121)\nplt.gca().set_title('PCA')\nplt.scatter(X_PCA[:,0], X_PCA[:,1],c=t, cmap=plt.cm.hot)\nplt.subplot(122)\nplt.gca().set_title('tSNE')\nplt.scatter(X_tsne[:,0], X_tsne[:,1],c=t, cmap=plt.cm.hot)","5c326fe3":"## tSNE on makecircle","00c4b33e":"# Making Swissroll","256c34e0":"## PCA and tSNE in MNIST","3399fb01":"# tSNE Introducion:\n* tSNE tries to reduce dimension by preserving the neighborhood.\n* This simply means if we are taking a point a, in the original dimension (50 Features) if my closest neighbors were b,c,d,e,f. In the lower dimension also my closest neghbors should be b,c,d,e,f (2 Featurs). \n* This simmilarity between the neighbors are measured by KL Divergence.","ce691ebf":"* n_components: What is the dimension size of the lower dimension space?\n* perplexity: Decides kind of how many neighbors to be considered.  Value should be within 5 and 50. Larger datsets have realtively large perplexity value. ","b2ed8ea2":"![image.png](attachment:image.png)","c83f8a5b":"# Perplexity aspect with tSNE ","dab0824a":"# tSNE syntax","42568fb9":"## Importing Libraries"}}