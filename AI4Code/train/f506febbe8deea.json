{"cell_type":{"1bcf6bd7":"code","d22c3c54":"code","ef5b6170":"code","3ff50851":"code","7b2839c5":"code","101ede79":"code","85d08e02":"code","642bbbb9":"code","0a719a9e":"code","8644591d":"code","3e542776":"code","4c7e0dfb":"code","29aa160d":"code","e9a2c20b":"code","5d7d4804":"code","a2303b41":"code","a3ab5e46":"code","a6a6f877":"code","e7d441bd":"code","a6b1ff1f":"code","5480ef43":"code","8f8eb803":"code","7616ab92":"code","497dc6dd":"code","55a2cbbd":"code","13495495":"code","3843e0e3":"code","76dce387":"code","e18e6e21":"code","1aa0a7a9":"code","26ca3cbb":"code","af314aa7":"code","6223953a":"code","d244f567":"code","3806c176":"code","eef2cd80":"code","df021e27":"code","0fad8883":"code","639936c4":"code","f290ec74":"code","9e5c4cd3":"code","dcd22b55":"code","b3ff2cc2":"code","3de6fc7c":"code","01e80fbc":"code","b8db0ac9":"code","c3cb5bdd":"code","80789f03":"code","1fb487fc":"code","1ddbb395":"code","d0a6b175":"code","15b4f971":"code","1da8c18c":"code","3147fd37":"code","eb983fc7":"code","d60268cf":"code","29b5ea93":"code","a1c50ea2":"code","7e275f8b":"code","d2e62183":"code","51eb851e":"code","5cbfd203":"code","7d65175d":"code","12290466":"code","786232ab":"code","b590fe98":"code","7edbc723":"code","a760213a":"code","02989cd0":"code","1cf5550f":"code","d5cce893":"code","b467d171":"code","d9f95904":"code","f2cc2334":"markdown","99be5051":"markdown","80b1512f":"markdown","6a3f4a79":"markdown","dc75c3f0":"markdown","a2c6d7c0":"markdown","4441a384":"markdown","db4eeffc":"markdown","b8efb7b7":"markdown","3f956d38":"markdown","0d1669a4":"markdown","978dd005":"markdown","93e9ec94":"markdown","43aba9b4":"markdown","9212cddb":"markdown","2ddd8ff5":"markdown","e05cf556":"markdown","85253738":"markdown","19594420":"markdown","d29f3e1e":"markdown"},"source":{"1bcf6bd7":"!pip install \"..\/input\/textstat\/Pyphen-0.10.0-py3-none-any.whl\"\n!pip install \"..\/input\/textstat\/textstat-0.7.0-py3-none-any.whl\"","d22c3c54":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport textstat\nimport matplotlib.pyplot as plt\n\nfrom spacy.lang.en import English\nfrom collections import defaultdict","ef5b6170":"nlp=English()\nstop_words=nlp.Defaults.stop_words\n\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\nprint(nlp.pipe_names)","3ff50851":"train=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntrain=train[['id', 'excerpt', 'target']]\ntrain.head()","7b2839c5":"class Vocab:\n    def __init__(self, passages):\n        self.passages=passages\n        self.word2id={}\n        self.id2word={}\n        self.vocab_freq={}\n        self.vocab_doc_freq={}\n        self.vocab_=[]\n    def build(self):\n        for passage in self.passages:\n            for word in nlp(passage):\n                word=word.text\n                if word not in self.vocab_freq:\n                    self.vocab_freq[word]=0\n                self.vocab_freq[word]+=1\n        for idx, word in enumerate(self.vocab_freq.keys()):\n            self.word2id[word]=idx\n            self.id2word[idx]=word\n            self.vocab_.append(word)\n    def filter_vocab_by_count(self, min_freq=5, max_freq=500000):\n        temp_vocab={}\n        for word, freq in self.vocab_freq.items():\n            if freq <= min_freq or freq>=max_freq:\n                continue\n            temp_vocab[word]=freq\n        return temp_vocab","101ede79":"%%time\nvocab=Vocab(train.excerpt.values)\nvocab.build()","85d08e02":"for token in nlp('Hello 123dfasl _hel'):\n    print(token.is_punct)","642bbbb9":"def get_sentence_count(excerpt):\n    return len( list(nlp(excerpt).sents) )\n\ndef get_word_count(excerpt):\n    cnt=0\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct:\n            continue\n        cnt+=1\n    return cnt\n    \ndef get_unique_word_count(excerpt):\n    word_set=set()\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct:\n            continue\n        word_set.add(token.lower_)\n    return len(word_set)\n\ndef get_word_count_without_stopword(excerpt):\n    cnt=0\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct or token.is_stop:\n            continue\n        cnt+=1\n    return cnt\n\ndef get_distinct_word_count_without_stopword(excerpt):\n    words=set()\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct or token.is_stop:\n            continue\n        words.add(token.lower_)\n    return len(words)\n\n\ndef get_stopword_count(excerpt):\n    cnt=0\n    for word in nlp(excerpt):\n        if word.is_stop:\n            cnt+=1\n    return cnt\n\ndef get_unique_stopword_count(excerpt):\n    word_set=set()\n    for word in nlp(excerpt):\n        if word.is_stop:\n            word_set.add(word)\n    return len(word_set)\n    \ndef get_punctuation_count(excerpt):\n    cnt=0\n    for word in nlp(excerpt):\n        if word.is_punct:\n            cnt+=1\n    return cnt\n\ndef get_title_word_count(excerpt):\n    cnt=0\n    for word in nlp(excerpt):\n        if word.is_title:\n            cnt+=1\n    return cnt\n\ndef get_unique_title_word_count(excerpt):\n    words=set()\n    for word in nlp(excerpt):\n        if word.is_title:\n            words.add(word.text)\n    return len(words)\n\ndef get_capital_word_count(excerpt):\n    cnt=0\n    for word in nlp(excerpt):\n        if word.is_upper:\n            cnt+=1\n    return cnt\n\ndef get_unique_capital_word_count(excerpt):\n    words=set()\n    for word in nlp(excerpt):\n        if word.is_upper:\n            words.add(word)\n    return len(words)\n\ndef get_syllable_counts(excerpt):\n    syllabel_freq=defaultdict(int)\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct:\n            continue\n        syllabel_freq[ textstat.syllable_count(token.text) ]+=1\n    return syllabel_freq\n\ndef get_syllable_count_without_stop(excerpt):\n    syllabel_freq=defaultdict(int)\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct or token.is_stop:\n            continue\n        syllabel_freq[ textstat.syllable_count(token.text) ]+=1\n    return syllabel_freq","0a719a9e":"%%time\n\ntrain['sentence_count']=train.excerpt.apply(get_sentence_count)\n\ntrain['word_count']=train.excerpt.apply(get_word_count)\ntrain['distinct_word_count']=train.excerpt.apply(get_unique_word_count)\n\ntrain['stopword_count']=train.excerpt.apply(get_stopword_count)\ntrain['distinct_stopword_count']=train.excerpt.apply(get_unique_stopword_count)\n\n\ntrain['word_count_without_stopword']=train.excerpt.apply(get_word_count_without_stopword)\ntrain['distinct_word_count_without_stopword']=train.excerpt.apply(get_distinct_word_count_without_stopword)\n\n\ntrain['punctuation_counts']=train.excerpt.apply(get_punctuation_count)\n\ntrain['title_words_count']=train.excerpt.apply(get_title_word_count)\ntrain['distinct_title_words_count']=train.excerpt.apply(get_unique_title_word_count)\n\n\ntrain['capital_word_count']=train.excerpt.apply(get_capital_word_count)\ntrain['distinct_capital_word_count']=train.excerpt.apply(get_unique_capital_word_count)","8644591d":"%%time\ntrain['word_redundancy']=1-(train['distinct_word_count'].div(train['word_count']))\ntrain['stopword_redundancy']=1-(train['stopword_count'].div(train['word_count']))\ntrain['word_redundancy_witout_stopwords']=1-(train['distinct_word_count_without_stopword'].div(train['word_count_without_stopword']))\n\n\ntrain['title_word_proportion']=train['title_words_count'].div(train['word_count'])\ntrain['title_word_per_sentence']=train['title_words_count'].div(train['sentence_count'])\n\n\ntrain['capital_word_proportaion']=train['capital_word_count'].div(train['word_count'])\n\n\ntrain['words_per_punctuation']=train['word_count'].div(train.punctuation_counts)\ntrain['words_per_sentence']=train['word_count'].div(train['sentence_count'])","3e542776":"%%time\ntrain['syllable_freq']=train.excerpt.apply(get_syllable_counts)\ntrain['syllable_without_stop_freq']=train.excerpt.apply(get_syllable_count_without_stop)\n\n\ntrain['0syllable']=train.syllable_freq.apply(lambda x: x[0])\ntrain['0syllable_proportion']=train['0syllable'].div(train['word_count'])\n\ntrain['1syllable']=train.syllable_freq.apply(lambda x: x[1])\ntrain['1syllable_proportion']=train['1syllable'].div(train['word_count'])\n\ntrain['2syllable']=train.syllable_freq.apply(lambda x: x[2])\ntrain['2syllable_proportion']=train['2syllable'].div(train['word_count'])\n\n\ntrain['3syllable']=train.syllable_freq.apply(lambda x: x[3])\ntrain['3syllable_proportion']=train['3syllable'].div(train['word_count'])\n\n\ntrain['4syllable']=train.syllable_freq.apply(lambda x: x[4])\ntrain['4syllable_proportion']=train['4syllable'].div(train['word_count'])\n\n\ntrain['>=5syllable']=train.syllable_freq.apply(lambda x: sum(x.values()) - x[0]- x[1]- x[2]- x[3]- x[4] )\ntrain['>=5syllable_proportion']=train['>=5syllable'].div(train['word_count'])\n\n\n\n\ntrain['0syllable_no_stop']=train.syllable_without_stop_freq.apply(lambda x: x[0])\ntrain['0syllable_no_stop_proportion']=train['0syllable_no_stop'].div(train['word_count_without_stopword'])\n\n\ntrain['1syllable_no_stop']=train.syllable_without_stop_freq.apply(lambda x: x[1])\ntrain['1syllable_no_stop_proportion']=train['1syllable_no_stop'].div(train['word_count_without_stopword'])\n\ntrain['2syllable_no_stop']=train.syllable_without_stop_freq.apply(lambda x: x[2])\ntrain['2syllable_no_stop_proportion']=train['2syllable_no_stop'].div(train['word_count_without_stopword'])\n\ntrain['3syllable_no_stop']=train.syllable_without_stop_freq.apply(lambda x: x[3])\ntrain['3syllable_no_stop_proportion']=train['3syllable_no_stop'].div(train['word_count_without_stopword'])\n\ntrain['4syllable_no_stop']=train.syllable_without_stop_freq.apply(lambda x: x[4])\ntrain['4syllable_no_stop_proportion']=train['4syllable_no_stop'].div(train['word_count_without_stopword'])\n\ntrain['>=5syllable_no_stop']=train.syllable_without_stop_freq.apply(lambda x: sum(x.values()) - x[0]- x[1]- x[2]- x[3]- x[4] )\ntrain['>=5syllable_no_stop_proportion']=train['>=5syllable_no_stop'].div(train['word_count_without_stopword'])\n\ntrain.head()","4c7e0dfb":"train.columns","29aa160d":"train.word_redundancy.describe()","e9a2c20b":"train.word_redundancy_witout_stopwords.describe()","5d7d4804":"plt.figure(figsize=(12, 5))\nsns.histplot(data=train, x='word_redundancy', stat='probability', color='red',  label=\"Word Redundancy\")\nsns.histplot(data=train, x='word_redundancy_witout_stopwords', stat='probability', color='blue', \n             label=\"Word Redundance Without StopWords\")\nplt.legend(loc='best')\nplt.show()","a2303b41":"fig, ax=plt.subplots(2, 1, sharex=True)\n\nsns.boxplot(data=train, x='word_redundancy', color='red', ax=ax[0])\nsns.boxplot(data=train, x='word_redundancy_witout_stopwords', color='blue', ax=ax[1])\n\nfig.show()","a3ab5e46":"plt.figure(figsize=(12, 5))\n\nsns.scatterplot(data=train, x='word_redundancy', y='target', color='red', label='Word Redundancy')\nsns.scatterplot(data=train, x='word_redundancy_witout_stopwords', y='target', \n                color='blue', label='Word Redundancy Without Stopwords')\n\nplt.legend(loc='best')\nplt.show()","a6a6f877":"sns.scatterplot(data=train[train['word_redundancy_witout_stopwords']>0.45],\n                x='word_redundancy_witout_stopwords', y='target', \n                color='blue', label='Word Redundancy Without Stopwords')\n","e7d441bd":"train[(train.word_redundancy_witout_stopwords>0.45) & (train.target>0)].excerpt.values[0]","a6b1ff1f":"train[(train.word_redundancy_witout_stopwords>0.45) & (train.target < -1.0)].excerpt.values[0]","5480ef43":"\nplt.figure(figsize=(12, 5))\nsns.histplot(data=train, x='word_count_without_stopword', stat='probability', \n             color='red',  label=\"Word Count Without StopWord\")\n\nsns.histplot(data=train, x='distinct_word_count_without_stopword', stat='probability', color='blue', \n             label=\"Distinct Word Count Without StopWord\")\nplt.legend(loc='best')\nplt.show()\n","8f8eb803":"fig, ax=plt.subplots(2, 1, sharex=True)\n\nsns.boxplot(data=train, x='word_count_without_stopword', color='red', ax=ax[0])\nsns.boxplot(data=train, x='distinct_word_count_without_stopword', color='blue', ax=ax[1])\n\nfig.show()","7616ab92":"plt.figure(figsize=(12, 5))\n\nplt.hlines(y=-1, xmin=0, xmax=150, color='green')\nsns.scatterplot(data=train, x='word_count_without_stopword', y='target', color='red')\nsns.scatterplot(data=train, x='distinct_word_count_without_stopword', y='target', color='blue')\n\nplt.legend(loc='best')\nplt.show()","497dc6dd":"sns.lineplot(data=train, x='distinct_word_count_without_stopword', y='target')","55a2cbbd":"plt.figure(figsize=(12, 5))\nsns.histplot(data=train, x='capital_word_count', stat='probability', color='red',  label=\"Capital Word Count\")\nplt.legend(loc='best')\nplt.show()\n\n\nplt.figure(figsize=(12, 5))\nsns.histplot(data=train, x='capital_word_proportaion', stat='probability', color='blue', label=\"Capital Word Proportions\")\nplt.legend(loc='best')\nplt.show()\n","13495495":"train['capital_word_count'].describe()","3843e0e3":"train['capital_word_proportaion'].describe()","76dce387":"plt.figure(figsize=(12, 5))\nsns.scatterplot(data=train, x='capital_word_count', y='target', color='red')\nplt.legend(loc='best')\nplt.show()\n\n\nplt.figure(figsize=(12, 5))\nsns.scatterplot(data=train, x='capital_word_proportaion', y='target', color='blue')\nplt.legend(loc='best')\nplt.show()","e18e6e21":"sns.lineplot(data=train, x='capital_word_proportaion', y='target')","1aa0a7a9":"plt.figure(figsize=(12, 5))\nsns.histplot(data=train, x='sentence_count', stat='probability', \n             color='red',  label=\"Sentence Count\")\nplt.legend(loc='best')\nplt.show()","26ca3cbb":"sns.scatterplot(data=train, x='sentence_count', y='target')","af314aa7":"sns.lineplot(data=train, x='sentence_count', y='target')","6223953a":"_, ax=plt.subplots(1, 4, figsize=(15, 5))\nsns.heatmap(train[['sentence_count', 'target']].corr(), annot=True, cbar=False, ax=ax[0])\n\nsns.heatmap(train[(train.sentence_count>5) & \n                  (train.sentence_count<=20)\n                 ][['sentence_count', 'target']].corr(), annot=True, cbar=False, ax=ax[1])\n\nsns.heatmap(train[(train.sentence_count>20) & \n                  (train.sentence_count<30) \n                 ][['sentence_count', 'target']].corr(), annot=True, cbar=False, ax=ax[2])\n\nsns.heatmap(train[train.sentence_count>=30][['sentence_count', 'target']].corr(), annot=True, cbar=False, ax=ax[3])\n\nplt.legend(loc='best')\nplt.show()","d244f567":"plt.figure(figsize=(12, 5))\nsns.histplot(data=train, x='title_words_count', stat='probability', \n             color='red',  label=\"Title Word Count\")\nplt.legend(loc='best')\nplt.show()\n\n\nplt.figure(figsize=(12, 5))\nsns.histplot(data=train, x='title_word_per_sentence', stat='probability', \n             color='blue', label=\"Title Word Per Sentence\")\nplt.legend(loc='best')\nplt.show()","3806c176":"sns.lineplot(data=train, x='title_words_count', y='target')","eef2cd80":"_, ax=plt.subplots(1, 4, figsize=(15, 5))\nsns.heatmap(train[['title_words_count', 'target']].corr(), annot=True, cbar=False, ax=ax[0])\n\nsns.heatmap(train[(train.title_words_count>5) & \n                  (train.title_words_count<=20)\n                 ][['title_words_count', 'target']].corr(), annot=True, cbar=False, ax=ax[1])\n\nsns.heatmap(train[(train.title_words_count>20) & \n                  (train.title_words_count<30) \n                 ][['title_words_count', 'target']].corr(), annot=True, cbar=False, ax=ax[2])\n\nsns.heatmap(train[train.title_words_count>=30][['title_words_count', 'target']].corr(), annot=True, cbar=False, ax=ax[3])\n\nplt.legend(loc='best')\nplt.show()","df021e27":"sns.lineplot(data=train, x='title_word_per_sentence', y='target')","0fad8883":"plt.figure(figsize=(12, 5))\nsns.histplot(data=train, x='word_count', stat='probability', label=\"Word Count\")\nplt.legend(loc='best')\nplt.show()","639936c4":"sns.scatterplot(data=train, x='word_count', y='target')","f290ec74":"sns.lineplot(data=train, x='word_count', y='target')","9e5c4cd3":"train[['word_count', 'target']].corr()","dcd22b55":"[colname for colname in train.columns if 'syllable' in colname]","b3ff2cc2":"train[['0syllable_no_stop','1syllable_no_stop','2syllable_no_stop',\n       '3syllable_no_stop','4syllable_no_stop','>=5syllable_no_stop','target'\n]].corr()","3de6fc7c":"train[['0syllable_no_stop','1syllable_no_stop','2syllable_no_stop',\n       '3syllable_no_stop','4syllable_no_stop','>=5syllable_no_stop','target'\n]].describe()","01e80fbc":"plt.figure(figsize=(14, 5))\nsns.boxplot(data=train[['0syllable_no_stop','1syllable_no_stop','2syllable_no_stop',\n                        '3syllable_no_stop','4syllable_no_stop','>=5syllable_no_stop']])\n\nplt.show()","b8db0ac9":"sns.scatterplot(data=train[train['>=5syllable_no_stop'] >0 ], \n                x='>=5syllable_no_stop',\n                y='target')\n","c3cb5bdd":"train[['0syllable_no_stop_proportion', '1syllable_no_stop_proportion',\n       '2syllable_no_stop_proportion','3syllable_no_stop_proportion',\n       '4syllable_no_stop_proportion','>=5syllable_no_stop_proportion',\n       'target'\n      ]].corr()\n","80789f03":"train[['0syllable_no_stop_proportion', '1syllable_no_stop_proportion',\n       '2syllable_no_stop_proportion','3syllable_no_stop_proportion',\n       '4syllable_no_stop_proportion','>=5syllable_no_stop_proportion']].describe()","1fb487fc":"print(\"Number Of Documents in which syllable count==0\")\n(train[['0syllable_no_stop_proportion', '1syllable_no_stop_proportion',\n       '2syllable_no_stop_proportion','3syllable_no_stop_proportion',\n       '4syllable_no_stop_proportion','>=5syllable_no_stop_proportion']]==0).sum()","1ddbb395":"plt.figure(figsize=(14, 5))\nplt.xticks(rotation=30)\nsns.boxplot(data=train[['0syllable_no_stop_proportion', '1syllable_no_stop_proportion',\n       '2syllable_no_stop_proportion','3syllable_no_stop_proportion',\n       '4syllable_no_stop_proportion','>=5syllable_no_stop_proportion']])\n\nplt.show()","d0a6b175":"_, ax=plt.subplots(2, 3, figsize=(17, 5))\n\nfor i, colname in enumerate(['0syllable_no_stop_proportion', '1syllable_no_stop_proportion',\n                             '2syllable_no_stop_proportion','3syllable_no_stop_proportion',\n                             '4syllable_no_stop_proportion','>=5syllable_no_stop_proportion']):\n    \n    sns.heatmap(train[train[colname] >0][[colname, 'target']].corr(), annot=True, ax=ax[i\/\/3, i%3],\n                cbar=False\n               )\nplt.show()","15b4f971":"sns.histplot(data=train, x='punctuation_counts', bins=100)","1da8c18c":"sns.scatterplot(data=train, x='punctuation_counts', y='target')","3147fd37":"train[['punctuation_counts', 'target']].corr()","eb983fc7":"sns.histplot(train['words_per_sentence'], bins=100)","d60268cf":"sns.scatterplot(data=train, x='words_per_sentence', y='target')\n","29b5ea93":"sns.heatmap(train[['words_per_sentence','target']].corr(), annot=True)","a1c50ea2":"from sklearn.preprocessing import StandardScaler\n\nimport torch\nimport torch.nn as nn","7e275f8b":"train_columns=['sentence_count', 'word_count', 'distinct_word_count', 'words_per_sentence',\n       'stopword_count', 'distinct_stopword_count', 'word_count_without_stopword', \n       'distinct_word_count_without_stopword', \n       'punctuation_counts', 'title_words_count', 'distinct_title_words_count',\n       'word_redundancy', 'stopword_redundancy', 'word_redundancy_witout_stopwords',\n       'title_word_proportion', 'title_word_per_sentence', 'words_per_punctuation',  \n       '0syllable_no_stop', '0syllable_no_stop_proportion', \n       '1syllable_no_stop', '1syllable_no_stop_proportion', \n       '2syllable_no_stop', '2syllable_no_stop_proportion', \n       '3syllable_no_stop','3syllable_no_stop_proportion', \n       '4syllable_no_stop','4syllable_no_stop_proportion', \n       '>=5syllable_no_stop', '>=5syllable_no_stop_proportion'\n]\n\nprint('Train Columns:')\nprint(train_columns)","d2e62183":"train_df=train[['id']+train_columns+['target']].copy()\ntrain_df.head()","51eb851e":"def get_cv_ids():\n    global train_df\n    df=train_df.copy()\n    cv_ids=[]\n    ranges=[(-4, -3.0), (-3.0, -2.0),(-2.0, -1.0),(-1.0, 0.0), (0.0, 1.0), (1.0, 2)]\n    for r in ranges:\n        l=r[0]\n        h=r[1]\n        \n        cur_cvids=list(df[(df.target>=l) & (df.target<h)].id.values)\n        np.random.choice(cur_cvids)\n        cv_ids+=cur_cvids[:int(len(cur_cvids)*0.1)]\n    return cv_ids\ncv_ids=get_cv_ids()\n\n\nval_df=train_df[train_df.id.isin(cv_ids)].copy()\ntrain_df=train_df[train_df.id.isin(cv_ids)==False].copy()\n\nprint('Number Of Validation Records:', len(val_df))\nprint('Number Of Train Records:', len(train_df))\n\nval_df.target.hist(bins=100)\nplt.show()","5cbfd203":"%%time\ntest_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\ntest_df['sentence_count']=test_df.excerpt.apply(get_sentence_count)\n\ntest_df['word_count']=test_df.excerpt.apply(get_word_count)\ntest_df['distinct_word_count']=test_df.excerpt.apply(get_unique_word_count)\n\ntest_df['stopword_count']=test_df.excerpt.apply(get_stopword_count)\ntest_df['distinct_stopword_count']=test_df.excerpt.apply(get_unique_stopword_count)\n\n\ntest_df['word_count_without_stopword']=test_df.excerpt.apply(get_word_count_without_stopword)\ntest_df['distinct_word_count_without_stopword']=test_df.excerpt.apply(get_distinct_word_count_without_stopword)\n\n\ntest_df['punctuation_counts']=test_df.excerpt.apply(get_punctuation_count)\n\ntest_df['title_words_count']=test_df.excerpt.apply(get_title_word_count)\ntest_df['distinct_title_words_count']=test_df.excerpt.apply(get_unique_title_word_count)\n\n\ntest_df['capital_word_count']=test_df.excerpt.apply(get_capital_word_count)\ntest_df['distinct_capital_word_count']=test_df.excerpt.apply(get_unique_capital_word_count)\n\n\ntest_df['word_redundancy']=1-(test_df['distinct_word_count'].div(test_df['word_count']))\ntest_df['stopword_redundancy']=1-(test_df['stopword_count'].div(test_df['word_count']))\ntest_df['word_redundancy_witout_stopwords']=1-(test_df['distinct_word_count_without_stopword'].div(test_df['word_count_without_stopword']))\n\n\ntest_df['title_word_proportion']=test_df['title_words_count'].div(test_df['word_count'])\ntest_df['title_word_per_sentence']=test_df['title_words_count'].div(test_df['sentence_count'])\n\ntest_df['capital_word_proportaion']=test_df['capital_word_count'].div(test_df['word_count'])\n\n\ntest_df['words_per_punctuation']=test_df['word_count'].div(test_df.punctuation_counts)\ntest_df['words_per_sentence']=test_df['word_count'].div(test_df['sentence_count'])\n\ntest_df['syllable_freq']=test_df.excerpt.apply(get_syllable_counts)\ntest_df['syllable_without_stop_freq']=test_df.excerpt.apply(get_syllable_count_without_stop)\n\n\ntest_df['0syllable']=test_df.syllable_freq.apply(lambda x: x[0])\ntest_df['0syllable_proportion']=test_df['0syllable'].div(test_df['word_count'])\n\ntest_df['1syllable']=test_df.syllable_freq.apply(lambda x: x[1])\ntest_df['1syllable_proportion']=test_df['1syllable'].div(test_df['word_count'])\n\ntest_df['2syllable']=test_df.syllable_freq.apply(lambda x: x[2])\ntest_df['2syllable_proportion']=test_df['2syllable'].div(test_df['word_count'])\n\n\ntest_df['3syllable']=test_df.syllable_freq.apply(lambda x: x[3])\ntest_df['3syllable_proportion']=test_df['3syllable'].div(test_df['word_count'])\n\n\ntest_df['4syllable']=test_df.syllable_freq.apply(lambda x: x[4])\ntest_df['4syllable_proportion']=test_df['4syllable'].div(test_df['word_count'])\n\n\ntest_df['>=5syllable']=test_df.syllable_freq.apply(lambda x: sum(x.values()) - x[0]- x[1]- x[2]- x[3]- x[4] )\ntest_df['>=5syllable_proportion']=test_df['>=5syllable'].div(test_df['word_count'])\n\n\ntest_df['0syllable_no_stop']=test_df.syllable_without_stop_freq.apply(lambda x: x[0])\ntest_df['0syllable_no_stop_proportion']=test_df['0syllable_no_stop'].div(test_df['word_count_without_stopword'])\n\n\ntest_df['1syllable_no_stop']=test_df.syllable_without_stop_freq.apply(lambda x: x[1])\ntest_df['1syllable_no_stop_proportion']=test_df['1syllable_no_stop'].div(test_df['word_count_without_stopword'])\n\ntest_df['2syllable_no_stop']=test_df.syllable_without_stop_freq.apply(lambda x: x[2])\ntest_df['2syllable_no_stop_proportion']=test_df['2syllable_no_stop'].div(test_df['word_count_without_stopword'])\n\ntest_df['3syllable_no_stop']=test_df.syllable_without_stop_freq.apply(lambda x: x[3])\ntest_df['3syllable_no_stop_proportion']=test_df['3syllable_no_stop'].div(test_df['word_count_without_stopword'])\n\ntest_df['4syllable_no_stop']=test_df.syllable_without_stop_freq.apply(lambda x: x[4])\ntest_df['4syllable_no_stop_proportion']=test_df['4syllable_no_stop'].div(test_df['word_count_without_stopword'])\n\ntest_df['>=5syllable_no_stop']=test_df.syllable_without_stop_freq.apply(lambda x: sum(x.values()) - x[0]- x[1]- x[2]- x[3]- x[4] )\ntest_df['>=5syllable_no_stop_proportion']=test_df['>=5syllable_no_stop'].div(test_df['word_count_without_stopword'])\n\ntest_df=test_df[['id']+train_columns].copy()\ntest_df.head()","7d65175d":"print(train_df.shape)\nprint(val_df.shape)\nprint(test_df.shape)","12290466":"scaler=StandardScaler()\nX_train=scaler.fit_transform(train_df[train_columns])\nprint(X_train.shape)","786232ab":"target_mean=train_df.target.mean()\ntarget_std=train_df.target.std()\n\nprint('Target Mean:', target_mean)\nprint(\"Target Std:\", target_std)","b590fe98":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, phase):\n        self.df=df\n        self.phase=phase\n    def __getitem__(self, idx):\n        row=self.df.iloc[idx]\n        X=scaler.transform(row[train_columns].values.reshape(1, -1))\n        X=torch.tensor(X,dtype=torch.float32)\n        X=X.view(-1)\n        if (self.phase=='train') or (self.phase=='val'):\n            y=(row.target - target_mean)\/target_std\n            y=torch.tensor(y, dtype=torch.float32)\n            return (X, y)\n        return X\n    def __len__(self):\n        return len(self.df)","7edbc723":"train_dataset=Dataset(train_df, 'train')\nval_dataset=Dataset(val_df, 'val')\ntest_dataset=Dataset(test_df, 'test')\n\n\ntrain_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\nval_dataloader=torch.utils.data.DataLoader(val_dataset, batch_size=512, shuffle=False)\ntest_dataloader=torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False)","a760213a":"class Model(nn.Module):\n    def __init__(self, in_feat, out_feat):\n        super().__init__()\n        self.linear1=nn.Linear(in_feat, 128)\n        self.bn1=nn.BatchNorm1d(128)\n        self.dropout1=nn.Dropout(0.4)\n        self.relu1=nn.ReLU()\n        \n        self.linear2=nn.Linear(128, 64)\n        self.bn2=nn.BatchNorm1d(64)\n        self.dropout2=nn.Dropout(0.5)\n        self.relu2=nn.ReLU()\n        \n        self.out=nn.Linear(64, out_feat)\n    def forward(self, x):\n        x=self.linear1(x)\n        x=self.bn1(x)\n        x=self.dropout1(x)\n        x=self.relu1(x)\n        \n        x=self.linear2(x)\n        x=self.bn2(x)\n        x=self.dropout2(x)\n        x=self.relu2(x)\n        \n        y=self.out(x)\n        return y","02989cd0":"def train_epoch(model, criterion, optimizer, train_dataloader):\n    epoch_loss=0.0\n    model.train()\n    for (X, y) in train_dataloader:\n        y_hat=model(X).view(-1)\n        loss=criterion(y_hat, y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss+=loss.item()\n    epoch_loss\/=len(train_dataloader)\n    return epoch_loss\n\ndef infer(models, dataloader):\n    preds=[]\n    for X in dataloader:\n        y=torch.zeros(X.shape[0])\n        for model in models:\n            model.eval()\n            with torch.no_grad():\n                y+=((model(X).view(-1) * target_std) + target_mean)\n        y\/=len(models)\n        preds+=list(y.numpy())\n    return preds\n\ndef evaluate(model, criterion, dataloader):\n    epoch_loss=0.0\n    model.eval()\n    for (X,ytrue) in dataloader:\n        y=torch.zeros(X.shape[0])\n        with torch.no_grad():\n            y+=model(X).view(-1)\n        loss=criterion(y, ytrue)\n        epoch_loss+=loss.item()\n    epoch_loss\/=len(dataloader)\n    return epoch_loss","1cf5550f":"models=[]\nepochs=20\nfor i in range(5):\n    model=Model(29, 1)\n    optimizer=torch.optim.AdamW(model.parameters(), weight_decay=1e-4)\n    criterion=nn.MSELoss(reduction='mean')\n    best_loss=None\n    \n    for j in range(epochs):\n        epoch_loss=train_epoch(model, criterion, optimizer, train_dataloader)\n        val_loss=evaluate(model,criterion, val_dataloader)\n        \n        if (best_loss is None) or (val_loss < best_loss):\n            best_loss=val_loss\n            torch.save(model, 'model_{}.pt'.format(i+1))\n        if (j+1)%5==0:\n            print('Epoch:{} | Train Loss:{:.4f} | Val Loss:{:.4f}'.format(j+1, epoch_loss, val_loss))\n    \n    print(\"Best Loss\")\n    print(best_loss)\n    print(\"===\"*10)\n    model=torch.load( 'model_{}.pt'.format(i+1))\n    models.append(model)","d5cce893":"test_dataset=Dataset(test_df, 'test')\ntest_dataloader=torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=512)\ntest_preds=infer(models, test_dataloader)\ntest_df['target']=test_preds","b467d171":"submission_df=test_df[['id', 'target']].copy()\nsubmission_df.to_csv('submission.csv', index=False)","d9f95904":"submission_df","f2cc2334":"# Punctuations and words per sentence","99be5051":"1. Stopwords take much higher redundancy in excerpts.\n2. There are few excerpts which take higher redudandancy (>0.6) even without stopwords\n3. More redundant excerpts without stopword in most cases may result in ease of read for explaining to the lower grade students.","80b1512f":"Syllables 3, 4, 5 have negative correlation with the target (ease of read)","6a3f4a79":"# lets check for word counts","dc75c3f0":"# lets check for the title words and sentence statistics","a2c6d7c0":"1. Does n't look word counts have direct correlation with the target","4441a384":"# Model ","db4eeffc":"# lets check the capital words","b8efb7b7":"# Dataset","3f956d38":"Generate Vocabulary","0d1669a4":"# Model Training","978dd005":"Distinct Word Count without stopwords have downward trend with target variable","93e9ec94":"> Redundant words are there in both the child stories and the physics passages.\n\n> Some words makes the passages differ in their subjects.\n\n> In the children text there are some words (their, one of them, our etc, which are collective) may help in understanding text","43aba9b4":"As can be seen for the excerpts with >0.45 redundancy witout stopwords had higher chances of being ease of read.\n\nlet us look at some of the samples","9212cddb":"# lets see the syllable proporations","2ddd8ff5":"Syllables","e05cf556":"> Line plots are from mean estimators.\n\n> Sentence Counts have positive trend with the target variable  and the confidence is decreasing after 15.\n\n> lower sentence counts looks to be hard to read from line plot. I think that could be because of explainability \n\n> Sentence Lengths of >=20 had negative correlation witht the target","85253738":"> Looks Capital words in the passages are not contributing more for the target variable\n\n> Observing the scatter & line plot , they look very random with target variable.","19594420":"# ","d29f3e1e":"# lets check the syllables"}}