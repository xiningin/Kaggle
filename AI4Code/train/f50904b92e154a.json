{"cell_type":{"719e8f8b":"code","b0d68964":"code","8a40dd35":"code","644e4c14":"code","1374126e":"code","4e47e0a6":"code","6346cd42":"code","a96e51bd":"code","505a582a":"code","4f340430":"code","21380535":"code","2b9a9ae1":"code","0cc824e8":"code","6f78b453":"code","642ebbe5":"code","0bec333e":"code","68d858b6":"code","878ee372":"code","2e14ecac":"code","db3b65c1":"code","85f344fc":"code","52d718dc":"code","1d8b37cb":"code","89638e40":"code","1533afce":"code","0da95e60":"code","c9c7b571":"code","44cec34f":"code","f3163bda":"code","c19cfd0a":"code","78d3499b":"code","947711db":"code","34ae3747":"code","2571d669":"markdown","18c60c34":"markdown","6653a9a5":"markdown","dd74a9ea":"markdown","d6e73a57":"markdown","fc74b3fd":"markdown","93e6f7d1":"markdown","1155cee6":"markdown","162c1e44":"markdown","b7b4237e":"markdown","8e9e7c17":"markdown","4bfba455":"markdown","3710f1a6":"markdown","73663532":"markdown","a0396d99":"markdown","20a2640f":"markdown","2cc02511":"markdown","6577e5b1":"markdown","e32c07bd":"markdown","f95bd6c5":"markdown","ba4c08ec":"markdown","3b72b758":"markdown","0f0aa68f":"markdown","b6b9244e":"markdown","b96afc1f":"markdown","a192eca8":"markdown","f96d802a":"markdown","0664608a":"markdown","6ddb8398":"markdown","4566b917":"markdown"},"source":{"719e8f8b":"#################### LOAD MODULES ####################\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport re\nimport json\nimport requests\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport random as rd\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.naive_bayes import BernoulliNB\nimport numpy as np\nimport matplotlib.pyplot as plt","b0d68964":"### Load the Data\n\nAirCrash = pd.read_csv('..\/input\/large-passenger-plane-crashes-19332009\/Large_Passenger_Plane_Crashes_1933_to_2009.csv')","8a40dd35":"##### Exploratory Data Analysis #########\n\n# Plot Data by Cluster ID\ny = [\"High Fatalities\", \"Low Fatalities\"]\nx = AirCrash['ClustID'].value_counts()\n\nplt.barh(y, x, color=('darkred', 'darkblue'))\nplt.xlabel('Number of Crashes')\nplt.title('Large Airplane Crashes (1933-2009)')\nplt.show()\n","644e4c14":"# plot histogram of Passengers and Crew Aboard Plane Crash\nplt.hist(AirCrash['Aboard'], bins=20, edgecolor='black', color='purple')\nplt.xlabel('Number Aboard Plane Crash')\nplt.ylabel('Number of Crashes')\nplt.title(\"Histogram of Passengers and Crew Aboard Air Crashes\")\nplt.show()\n","1374126e":"# plot histogram of Air Crash Survivors\nplt.hist(AirCrash['Survivors'], bins=20, edgecolor='black', color='darkblue')\nplt.xlabel('Survivors by Crash')\nplt.ylabel('Number of Crashes')\nplt.title(\"Histogram of Survivors in Large Airplane Air Crashes\")\n\nplt.show()","4e47e0a6":"#%% Split the Data frame in two: High Fatalities and Low Fatalities\n\nHighFatal = AirCrash.copy()\ndroplow = HighFatal.loc[HighFatal['ClustID']\n                        == 'Low Fatality'].index\nHighFatal = HighFatal.drop(droplow)\n\n\nLowFatal = AirCrash.copy()\ndrophigh = LowFatal.loc[LowFatal['ClustID']\n                        == 'High Fatality'].index\nLowFatal = LowFatal.drop(drophigh)","6346cd42":"# plot histogram of Low Fatality Air Crash Survivors\nplt.hist(LowFatal['Survivors'], bins=20, edgecolor='black', color='darkgreen')\nplt.xlabel('Survivors by Crash')\nplt.ylabel('Number of Crashes')\nplt.title(\"Histogram of Survivors in Low Fatality Air Crashes\")\n\nplt.show()\n\n","a96e51bd":"# plot histogram of High Fatality Air Crash Survivors\nplt.hist(HighFatal['Survivors'], bins=20, edgecolor='black', color='orange')\nplt.xlabel('Survivors by Crash')\nplt.ylabel('Number of Crashes')\nplt.title(\"Histogram of Survivors in High Fatality Air Crashes\")\n\nplt.show() ","505a582a":"#%% Function to clean the txt files \n\n\ndef CleanText(DF_Text, empty_list):\n\n    for row in DF_Text:\n        # iteratively save each row as a list of tokens separated by spaces\n        Tokens = row.split(\" \")\n\n        Wordlist = []  # empty list to temporarily store clean Tokens\n\n        for item in Tokens:  # cleaning\n            item = item.lower() # change to lowercase\n            item = item.lstrip()  # remove spaces to the left\n            item = re.sub(\"\\n\", \"\", item)  # remove new line\n            item = re.sub(\"\\'\", \"\", item)  # remove single quotes\n            item = re.sub(\"\\.\", \"\", item)  # remove periods\n            item = re.sub(\"\\,\", \"\", item)  # remove commas\n            item = re.sub(r\"\\d\", \"\", item)  # remove digits\n            Wordlist.append(item)\n\n        Text = \" \".join(Wordlist) # stores wordlist as collection of terms or document\n\n        # Append Text to empty_list\n        empty_list.append(Text)\n\n    return print(empty_list[:5]) # returns first 5 documents for review\n","4f340430":"#Place cleaned Summary text in LowFatal and HighFatal DF into lists\n\n# Create empty lists\nLowFatalContent = []\n\n# Used CleanText Function to append lists and review first 5 documents\nCleanText(LowFatal['Summary'], LowFatalContent)\n","21380535":"#Place cleaned Summary text in LowFatal and HighFatal DF into lists\n\n# Create empty lists\nHighFatalContent = []\n\n# Used CleanText Function to append lists\nCleanText(HighFatal['Summary'], HighFatalContent)","2b9a9ae1":"# Get stats on length of words in each Summary\nLoFatalWordCounts = []\n\n# Use LowFatalConent to calculate Word Counts\nfor w in LowFatalContent:\n    Term = w.split(\" \")\n    Terms = []\n    Terms.append(Term)\n    LoFatalWordCounts.append(len(Terms[0]))\n\n# plot histogram of word counts\nplt.hist(LoFatalWordCounts, bins=5, edgecolor='black', color='green')\nplt.xlabel('Words Per Summary')\nplt.ylabel('Number of Crashes')\nplt.title(\"Histogram of Word Counts in Low Fatality Summary\")\nplt.show() \n\n","0cc824e8":"# Get stats on length of words in each Summary\nHiFatalWordCounts = []\n\n# Use HighFatalConent to calculate Word Counts\nfor w in HighFatalContent:\n    Term = w.split(\" \")\n    Terms = []\n    Terms.append(Term)\n    HiFatalWordCounts.append(len(Terms[0]))\n\n# plot histogram of word counts\nplt.hist(HiFatalWordCounts, bins=5, edgecolor='black', color='darkred')\nplt.xlabel('Words Per Summary')\nplt.ylabel('Number of Crashes')\nplt.title(\"Histogram of Word Counts in High Fatality Summary\")\nplt.show() ","6f78b453":"print(stopwords.words('english'))","642ebbe5":"######## DEFINE THE VECTORIZERS ###############\n\n\"\"\"\nFour Vectorizers were defined. 2 for LowFatalContent and \n2 for HighFatalContent.  Although the Vectorizers are duplicated\nfor each set of data, customized names were used to assure results\nare distinguished correctly\n\"\"\"\n\n# Define parameters for LowFatalContent Vectorizers\nLTfidl1 = TfidfVectorizer(input=\"content\",\n                          stop_words='english',  # NLTK defined\n                          ngram_range=(1, 1),  # unigram\n                          norm='l1',  # Document length normalization\n                          min_df=2, # term must exist in at least two documents\n                          max_df=0.95, # exclude terms existing in >95% of documents\n                          analyzer='word')  # feature as words\n\nLTfidl2 = TfidfVectorizer(input=\"content\",\n                          stop_words='english',  \n                          ngram_range=(1, 1),  \n                          norm='l2',  # Euclidean length normalization\n                          min_df=2,\n                          max_df=0.95,\n                          analyzer='word')  \n\n###################\n\n# Define parameters for HighFatalContent Vectorizers\n\nHTfidl1 = TfidfVectorizer(input=\"content\",\n                          stop_words='english',  \n                          ngram_range=(1, 1),  \n                          norm='l1',  # Document length normalization\n                          min_df=2,\n                          max_df=0.95,\n                          analyzer='word')  \n\n# Define parameters for custom TFidfVectorizer\nHTfidl2 = TfidfVectorizer(input=\"content\",\n                          stop_words='english',  \n                          ngram_range=(1, 1),  \n                          norm='l2',  # Euclidean length normalization\n                          min_df=2,\n                          max_df=0.95,\n                          analyzer='word')  \n","0bec333e":"### VECTORIZE THE LowFatalContent\n\n# vectorize Document length normalization\nLoFatalTVl1 = LTfidl1.fit_transform(LowFatalContent)\n\n# vectorize Euclidean length normalization\nLoFatalTVl2 = LTfidl2.fit_transform(LowFatalContent)\n\n\"\"\" Vocabulary for both vectorizers should be identical \"\"\"\n\n# Check vocabulary lengths\nprint(len(LTfidl1.vocabulary_), len(LTfidl2.vocabulary_))\n# Output: 464 464","68d858b6":"# Create function to print first n items in list or dictionary\ndef HeadCount(vocabulary, n):\n    count = 0\n    result = []\n\n    for item in vocabulary:\n        if count < n:\n            result.append(item)\n        count += 1\n    return result","878ee372":"# Use HeadCount function to spot check vocabulary of LTfidl1\nHeadCount(LTfidl1.vocabulary_, 10)\n","2e14ecac":"# Use HeadCount function to spot check vocabulary of LTfidl2\nHeadCount(LTfidl2.vocabulary_, 10)\n","db3b65c1":"# VECTORIZE THE HighFatalContent\n\n# vectorize Document length normalization\nHiFatalTVl1 = HTfidl1.fit_transform(HighFatalContent)\n\n# vectorize Euclidean length normalization\nHiFatalTVl2 = HTfidl2.fit_transform(HighFatalContent)\n\n\"\"\" Vocabulary for all three vectorizers should be identical \"\"\"\n\n# Check vocabulary lengths\nprint(len(HTfidl1.vocabulary_), len(HTfidl2.vocabulary_))\n# Output:  1176 1176\n","85f344fc":"# Use HeadCount function to spot check vocabulary of HTfidl1\nHeadCount(HTfidl1.vocabulary_, 10)","52d718dc":"# Use HeadCount function to spot check vocabulary of HTfidl2\nHeadCount(HTfidl2.vocabulary_, 10)","1d8b37cb":"# Place the document matrices into data frames\n\n\"\"\" Note: since LTfidl1 & LTfidl2 share identical features \nand since HTfidl1 & HTfidl2 also share identical features,\nonly one of each doc matrices need to be used to define the data frame columns.\"\"\"\n\n# Get the features (words) from the document matrices.\nLoFeatures = LTfidl1.get_feature_names() \nHiFeatures = HTfidl1.get_feature_names()\n\n\n# Create data frame for each combination of data and vectorizer\nLoFatalT1DF = pd.DataFrame(LoFatalTVl1.toarray(), columns=LoFeatures)\nprint(LoFatalT1DF.shape)  # (108 464) correct dimensions\n\nLoFatalT2DF = pd.DataFrame(LoFatalTVl2.toarray(), columns=LoFeatures)\nprint(LoFatalT2DF.shape)  # ((108 464) correct dimensions\n\nHiFatalT1DF = pd.DataFrame(HiFatalTVl1.toarray(), columns=HiFeatures)\nprint(HiFatalT1DF.shape)  # (348, 1176) correct dimensions\n\nHiFatalT2DF = pd.DataFrame(HiFatalTVl2.toarray(), columns=HiFeatures)\nprint(HiFatalT2DF.shape)  # (348, 1176) correct dimensions","89638e40":"# Build the Topic Models using Latent Dirichlet Allocation\n\nn_topics = 3\niterations = 1000\n\n# Define the models - 4 models were defined for distinct results\nLDAL1 = LatentDirichletAllocation(n_components=n_topics,  # number of topics\n                                  max_iter=iterations,  # number of iterations\n                                  random_state=23,  # allow replication of results\n                                  learning_method='online') \n\nLDAL2 = LatentDirichletAllocation(n_components=n_topics, max_iter=iterations,\n                                  random_state=23, learning_method='online')\n\nLDAH1 = LatentDirichletAllocation(n_components=n_topics, max_iter=iterations,\n                                  random_state=23, learning_method='online')\n\nLDAH2 = LatentDirichletAllocation(n_components=n_topics, max_iter=iterations,\n                                  random_state=23, learning_method='online')","1533afce":"# Fit the data to the model\n\nLoTVl1 = LDAL1.fit(LoFatalT1DF)\nLoTVl2 = LDAL2.fit(LoFatalT2DF)\n\nHiTVl1 = LDAH1.fit(HiFatalT1DF)\nHiTVl2 = LDAH2.fit(HiFatalT2DF)\n","0da95e60":"# LDAWords wordplot function\n\ndef LDAWords(model, feature_names, n_words, num_topics):\n    word_topic = np.array(model.components_).transpose()\n\n    num_top_words = n_words\n    vocab_array = np.asarray(feature_names)\n\n    for t in range(num_topics):\n        plt.subplot(1, num_topics, t + 1)  # \n        plt.ylim(0, num_top_words + 0.5)  # stretch y-axis to space-out words\n        # remove axis tick marks.\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(f'Topic {t + 1}', fontdict={'fontsize': 20})\n\n        top_words_idx = np.argsort(word_topic[:, t])[::-1]  # sort desc order\n        top_words_idx = top_words_idx[:num_top_words]\n        top_words = vocab_array[top_words_idx]\n        top_words_shares = word_topic[top_words_idx, t]\n        for i, (word, share) in enumerate(zip(top_words, top_words_shares)):\n            plt.text(0.3, num_top_words - i - 0.25, word, fontsize=14)\n\n    plt.tight_layout(rect=(0, 0, 1.5, 1.5))\n    plt.show \n\n    pass\n","c9c7b571":"# Plot the top 10 words for each topic in the LDAL1 model\nLDAWords(LDAL1, LoFeatures, 10, 3)","44cec34f":"# Plot the top 10 words for each topic in the LDAH1 model\nLDAWords(LDAH1, HiFeatures, 10, 3)","f3163bda":"# Plot the top 10 words for each topic in the LDAL2 model\nLDAWords(LDAL2, LoFeatures, 10, 3)","c19cfd0a":"# Plot the top 10 words for each topic in the LDAH2 model\nLDAWords(LDAH2, HiFeatures, 10, 3)","78d3499b":"# Create PlotTopicWeights function\n\ndef PlotTopicWeights(model, feature_names, n_top_words, n_topics):\n    # define figure single row with n_topics per row share x-axis\n    fig, axes = plt.subplots(1, n_topics, figsize=(30, 15), sharex=True)\n\n    # Input title for plot\n    # Title = input(\"Give plot a title: \")  # this will not work on Kaggle\n    \n    # Work-around solution to Title input field\n    Title = title #title object will need to be defined before running function\n    \n    # sort features with weights and extract values\n    for topic_idx, topic in enumerate(model.components_):\n        top_features = topic.argsort()[:-n_top_words - 1:-1]\n        features = [feature_names[i] for i in top_features]\n        weights = topic[top_features]\n\n        # plot the horizontal bar charts\n        ax = axes[topic_idx]\n        ax.barh(features, weights, height=0.7)\n        ax.set_title(f'Topic {topic_idx +1}', fontdict={'fontsize': 30})\n        ax.invert_yaxis()\n        ax.tick_params(axis='both', which='major', labelsize=20)\n        fig.suptitle(Title, fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()","947711db":"# Plot the Top 10 Features with their weights using PlotTopicWeights\ntitle = \"Low Fatality Air Crashes TFIDF L2 Norm LDA\"\nPlotTopicWeights(LDAL2, LoFeatures, 10, 3)","34ae3747":"# Plot the Top 10 Features with their weights using PlotTopicWeights\ntitle = \"High Fatality Air Crashes TFIDF L2 Norm LDA\"\nPlotTopicWeights(LDAH2, HiFeatures, 10, 3)\n","2571d669":"Now let's apply the LDAWord() function to find the top 10 features for each topic in the LDAH1 model. Let's describe the output as:\n\n## High Fatality Air Crashes TFIDF L1 Norm\n(Document Length Normalization)","18c60c34":"Use HeadCount() to spotcheck the first ten vocabulary items in LTfidl1 and LTfidl2 to make sure the low fatality documents are identical.","6653a9a5":"A comparison of survivors in low fatality and high fatality crashes are shown as histograms.  \n\nAll low fatality crashes had more than 100 survivors per crash. ","dd74a9ea":"## Extracting and Cleaning Text Data\nA function to clean text data will be defined below in order to extract and clean the summary text from the LowFatal and HighFatal data sets. ","d6e73a57":"Let's apply the PlotTopicWeights() function to both the LDAL2 and LDAH2 models and compare the results.","fc74b3fd":"Now let's apply the LDAWord() function to find the top 10 features for each topic in the LDAH2 model and call the results: \n\n## High Fatality Air Crashes TFIDF L2 Norm \n(Euclidean Length Normalization)","93e6f7d1":"Apply the models to the data sets:\n* LoFatalT1DF = Low Fatality data with Document Length Normalization\n* LoFatalT2DF = Low Fatality data with Euclidean Length Normalization\n* HiFatalT1DF = High Fatality data with Document Length Normalization\n* HiFatalT2DF = High Fatality data with Euclidean Length Normalization\n","1155cee6":"None of the documents HighFatalContent contain more than 400 words with a majority of the crashes described in less than 100 words.","162c1e44":"All high fatality crashes had less than a hundred survivors per crash where over 250 crashes had 5 or less survivors. ","b7b4237e":"Use the CleanText function to extract and clean LowFatal summary description.  Store the cleaned descriptions in list named LocalFatalContent.  Review first 5 documents in the list.","8e9e7c17":"With the text for low fatality crashes and high fatality crashes cleaned and separated, a histogram of the word counts for each type of crash can be visualized.  \n\nEach document in the LowFatalContent list have fewer than 300 words to describe each crash with the majority of crashes described in less than 100 words. ","4bfba455":"## About the Data\nThe data contains 456 records with 16 variables.\n\nEach record represents a large passenger plane crash that occurred between 1933 and 2009.  The records are categorized into two groups or ClustID's defined as follows:\n1. Low Fatality - crashes involving 94 to 517 passengers aboard (average of 178 aboard) with a high mean survival rate of 93%\n2.  High Fatality - crashes involving 69 to 644 passengers aboard (average of 125 aboard) with a low mean survival rate of 6%\n\nDefinition of Variables:\n\n* Date: Date of accident\n* Time: Local time, in 24 hr. in the format hh:mm\n* Location: Location of the accident\n* Operator: Airline or operator of the aircraft\n* Flight: Flight number assigned by the aircraft operator\n* Route: Complete or partial route flown prior to the accident\n* Type: Aircraft type\n* Registration: ICAO registration of the aircraft\n* cn\/In: Construction or serial number \/ Line or fuselage number\n* Aboard: Total people aboard\n* Fatalities: Total fatalities aboard\n* Ground: Total killed on the ground\n* Survivors: Total survivors aboard\n* Survival Rate: Survivors divided by Aboard represented as a float (% of survivors)\n* Summary: Brief description of the accident and cause if known\n* ClustID: Label describing the fatalities aboard - Boolean (\"High Fatality\", \"Low Fatality\")","3710f1a6":"It appears the data vectorized using the L2 or Euclidean Length normalization provided three distinct topics for both the Low Fatality and High Fatality summaries.  The first ten features in each topic also appear to provide words that could be subjectively categorized. \n\nFor example Low Fatality Topic 1 seems to point to defective parts, with Topic 2 refering to landing and takeoff issues, and lastly hijackings for Topic 3.\n\nThe topics differ slightly for High Fatality with Topic 1 refering to hijackings, Topic 2 suggesting explosives and Topic 3 addressing issues with landing and in-flight problems.\n\nBefore finalizing the topic results, lets re-visualize the L2 Norm results with the weights included for each feature.  The weights offer a measure of influence or association the feature has to the topic.  The greater the weight, the more influence it may have in defining the topic.  To perform this task a new function named PlotTopicWeights() will be used to visualize the wordplots.  This function is simply the LDAWord() function modified to include the weights.","73663532":"Use HeadCount() to spotcheck the first ten vocabulary items in HTfidl1 and HTfidl2 to make sure the high fatality documents are identical.","a0396d99":"A histogram of the number of passengers and crew aboard each crash shows a larger share of crashes among planes with 100 to 200 people aboard vs. airliners with more than 200 aboard ","20a2640f":"Use the CleanText function to extract and clean HightFatal summary description.  Store the cleaned descriptions in list named HighFatalContent.  Review first 5 documents in the list.","2cc02511":"The model parameters were set to categorize the documents into 3 topics.  The number of topics were subjectively chosen. Typically iteratively trying different number of topics is recommended until the words in each topic appear to be coherent. Three topics appears to be as good a number to start with as any other.  However, in general the larger the number, the less coherent the topics become.\n\nTo view the results a function named LDAWords(). The function takes the top n features of each topic and visually plots them in ranking order.  The features if coherent can be subjectively given a topic name based on how the words or features may be related.","6577e5b1":"Great the number of words in the document matrices match. The first 10 words in each of the high fatality document matrices also match.  \n\nNext place each of the document matrices into data frames","e32c07bd":"Great the four dataframes were created with the expected dimensions. \n\nNext build the topic models.\n\n## Model Analysis \nTo determine if coherent topics can be distinguished from the summary reports of the airplane crash data, the Latent Dirichlet Allocation (LDA) algorithm in Python will be used to perform topic modeling.  Latent Dirichlet Allocation performs topic modeling through a process of defining documents and words to a given number of topics in a Dirichlet distribution and iteratively associates word topic assignments using a Bayesian multinomial distribution that takes into account the term and document frequencies in text data.\nFor more about LDA I recommend Luis Serrano's video [Latent Drichlet Allocation](https:\/\/www.youtube.com\/watch?v=T05t-SqKArY&t=1450s) that provides a nice illustrative intro on Topic Modeling with LDA. \n\n![image.png](attachment:c6db91ea-d676-472b-aae3-0e2ccc2d9441.png)\n\n### Defining Parameters to LDA Models:\n* n components = 3  - the number of possible topics assumed\n* max iteration = 1000 \u2013 maximum iterations LDA improves word topic association \n* random state = 23 \u2013 to keep process consistent and allow for results to be replicated \n","f95bd6c5":"An inspection of the number of survivors per plane crash uncovers that the majority of the crashes (over 300) had less than 25 survivors ","ba4c08ec":"The topic results for Low Fatality appears to have created three distinct topics based on the summary descriptions.  Interestingly, the topic results for High Fatality had duplicated features in two of the three topics, suggesting only two distinct topics were truly created. \n\nLet's now visualize the topics using the vectorized data with L2 or Euclidean Length normalization to see if different topic features are found.\n\nLet's first apply the LDAWord() function to find the top 10 features for each topic in the LDAL2 model and call the results: \n\n## Low Fatality Air Crashes TFIDF L2 Norm \n(Euclidean Length Normalization)","3b72b758":"To examine the results a simple HeadCount() function will be used to review the first n items in a list or dictionary.","0f0aa68f":"## Exploratory Data Analysis\nLet's explore the data through some basic visualizations. \n\nAn inspection of the value counts by cluster type (ClustID) revealed that approximately 76% (348) of the airplane crashes had high fatalities, and 24% (108) had low fatalities.","b6b9244e":"![image.png](attachment:1ca8dad0-9721-41c6-a6cd-f1bfdfeb97e0.png)\n\n## Main Objective\nThis analysis will examine the differences between low fatality plane crashes and high fatality plane crashes by comparing the summary descriptions of each large passenger plane crash.  In a previous analysis (see [Surviving Air Disasters: Cluster Analysis](https:\/\/bit.ly\/2WSR0Ux)), word clouds were generated to compare the summary descriptions between high and low fatality crashes. \n\n![image.png](attachment:23db3554-beae-487e-8c08-59e4173347db.png)\n\nA preliminary examination of the words in the cloud suggest some themes may arise from the descriptions. To further investigate this possibility, topic modeling using Latent Dirichlet Allocation (LDA) will be applied to discover if distinct causes can be categorized to shed light on the differences between high and low fatality crashes.\n\n### Analysis Highlights\nThis analysis will explore and provide the following:\n\n* Brief preliminary EDA on airplane crash counts in the data.\n* Brief description of the Data Transformation process to prepare the data for analysis\n* Illustrative description of Vectorizing text using Term Frequency and TFIDF L1 & L2 Norm\n* Latent Drichlet Allocation Model Analysis for Topic Modeling\n* Visualizing the results using wordplots.\n* Final Results and Conclusions","b96afc1f":"Great the number of words in the document matrices match. The first 10 words in each of the low fatality document matrices also match.  \n\nNow repeat the process for HighFatalContent.\n\nVectorize HighFatalContent using defined HTfidl1 and HTfidl2 vectorizers above.  The document matrix outputs will be named HiFatalTVl1 and HiFatalTVl2 respectively.","a192eca8":"To simplify further exploration, the data will be split into two data frames: HighFatal & LowFatal ","f96d802a":"Vectorize LowFatalContent using defined LTfidl1 and LTfidl2 vectorizers above.  The document matrix outputs will be named LoFatalTVl1 and LoFatalTVl2 respectively.","0664608a":"## Data Transformation: \nThe summary text of each cleaned data from .csv format will be converted into data frames in order to perform the Latent Dirichlet Allocation algorithm on the data for topic modeling.  The transformation will follow the steps illustrated below. \n\n![image.png](attachment:46cdfe10-4d59-471e-a8ad-76d8a7cb5b7f.png)\n\nThe first two steps above have already been completed in the code above. The text data from the .csv file was loaded Python and cleaned.  The next two steps will be to 1) vectorize the cleaned texts into document matrices (sparse matrices), and 2) transfer the document matrices into data frames.  Stemming or lemmatization will not be performed on any of the text data. \n\n## Vectorization:\nTwo methods of vectorization will be employed using the TfidfVectorizer module in Python.  \nTFIDF is an acronym for Term Frequency Inverse Document Frequency.  Terms are the words in a document. A document is a collection of words. In this case each summary description of a crash is a document. \n\nA Term Frequency Inverse Document Frequency (TFIDF) calculation first requires calculating the Term Frequency (TF).  TF refers to the number of times a word (a.k.a. term or feature) appears in a document.  This can be expressed as a sparse matrix as shown below. \n\n![image.png](attachment:4ae81115-4e4a-4d1c-85de-ccec1e3de9c6.png)\n\nA Document frequency is defined as the number of documents that contain a term or feature.  The inverse would be the document frequency represented as a fraction with the numerator and denominator exchanging positions.  The calculation then takes the term frequency multiplied by the log of the inversed document frequency  or TF*log(IDF). This calculation process is illustrated below.\n\n![image.png](attachment:3f60d19d-6911-46a0-ab46-584e2685e3fc.png)\n\nThe difference in the two methods of TFIDF vectorization used are as follows:\n\n1)\tDocument Length Normalization (L1) \u2013   terms are given a value that reflects the number of times a term appears in a document relative to the size or total number of terms in a document. (i.e., term counts in document\/total number of terms in document)\n\n2)\tEuclidean Length Normalization (L2) \u2013 terms are given values that reflect the number of times a term appears in a document relative to the Euclidean length of a document's vector. \n\nIn some cases using an L1 or L2 normalization can make a difference in the topic modeling results. Both methods will be applied to explore the best output.\n\n### Defining Parameters for all vectorizers :\n* Terms were set to unigrams - only one word per term.\n* Terms must exist in at least two documents to be included\n* Terms existing in greater than 95% of documents were excluded\n* English Stopwords as defined by NLTK were removed from the documents (stopwords provided below).","6ddb8398":"Apply the LDAWord() function to find the top 10 features for each topic in the LDAL1 model. Let's describe the output as:\n\n## Low Fatality Air Crashes TFIDF L1 Norm \n(Document Length Normalization)","4566b917":"## Results:\n\nThe results provide some interesting similarities and differences between the topics generated for low fatality crashes vs. high fatality crashes. \n\nThere are two topics in each data set that have interesting similarities.  Topic 2 from low fatality and topic 3 from high fatality appear to have similar vocabulary with some identical terms.  These two topics could both be interpreted to be associated with landings, takeoffs and inflight challenges.  Both of these topics also carry the highest weights suggesting they are the most prominent topics for both types of crashes.  Intuitively this makes sense. These terms seem to fit the types of causes that would occur most often.  \"Landing,\" \"takeoff,\" \"runway,\" \"approach,\" \"engine,\" \"airport,\" \"crashed\", and \"failure\" are terms that seem to speak to common crash themes.  \n\nLess similar, but still comparable are topic 3 from low fatality and topic 1 from high fatality.  Here the terms \"hijacking,\" \"hijacker,\" \"rebel\", \"stormed,\" \"hostages,\" \"shot,\" and \"killed\" suggest an airplane crash due to some form of attack.  However the low fatality terms appear to focus more on hijacking situations while the high fatality terms seem to lean more towards an attack either from within or outside the aircraft.  The terms \"misidentification\" and \"acquitted\" add some mystery to high fatality topic 1. It is unclear how those words may be associated with the others, but one can surmise a misidentified plane being shot down by mistake or by some military intervention to still fall within an attack theme.  \n\nThe last two topics: topic 1 for low fatality and topic 2 for high fatality seem to be the two most dissimilar topics among the two data sets.  The terms \"tank,\" \"route,\" \"debris,\" \"belt,\" \"old,\" and \"wiring,\" in topic 1 of low fatality seem almost harmless compared to the terms \"detonation,\" \"bomb,\" \"device,\" \"forces,\" and \"explosive\" seen in topic 2 of high fatality.   These topics seem almost complete opposites in the level of danger each one defines.  The discovery of this difference between the two data sets provides one of the most interesting insights that might be taken from the results.\n\nGiven the features to each topic and the comparisons discussed above the 3 topic results for low and high fatality large passenger plane crashes can be categorized as follows:\n\n**Low Fatality Air Crash Topics:**\n1. \tDamages to Interior\/Exterior of Aircraft\n2.\tLanding, Takeoff and Inflight Challenges\n3.\tHijacking Occurrences\n\n**High Fatality Air Crash Topics:**\n1.\tAttacks to Aircraft\n2.\tExplosives\n3.\tLanding, Takeoff and Inflight Challenges\n\nIt is no surprise that landing, takeoff and inflight challenges were prominent topics for both types of crashes as these are common causes to all types of crashes. Historically hijacking aircrafts have also been low fatality situations, but more recent hijackings such as 9\/11 may contradict those facts.  Hijacking was a term also used in high fatality crashes under the category of \"Attacks to Aircraft.\"  Although not necessarily a fair comparison, it was interesting to see the weights of \"Hijacking Occurences\" to have higher weights than \"Attacks to Aircraft\" giving some insight to how common the situations occur for each type of crash.  It was also interesting to find that LDA topic modeling separated what seems to clearly indicate an \"Explosives\" topic for high fatality.  Such occurrences may have been assumed to be rare, but it was nevertheless prominent enough to receive a highly coherent grouping of words.  Lastly \"Damages to Interior\/Exterior of Aircraft\" had very low weights for low fatality crashes which may indicate low occurrence of air crash causes.\n\n## Conclusion:\n\nThis analysis set out to discover if topic modeling could be used to find differences between two distinct types of airplane crashes.  The findings provided promising results to support topic modeling as a useful tool at distinguishing differences within similar categories. In this instance topic modeling was used as a tool to further explore an analysis that successfully identified two particular groups of crashes and contributed insights to key causes to similar yet different types of occurrences.\n\nDespite the success of this analysis the findings were quite mixed. Half of the data sets generated incomprehensible topics.  If the analysis did not explore different options of vectorizing the data, these findings may not have been discovered.  It must then be observed that luck and persistence play a role in the success of discovering topics using LDA.  Although the initial efforts of assuming 3 topics worked out well in this instance, topic modeling usually requires multiple attempts of trial and error before \u201cgood\u201d results come to fruition.  One should also be aware that the results are open to interpretation and may lead to presumptive solutions.  \n\nFor instance in the low fatality air crashes, Topic 1 was labeled as Damages to Interior\/Exterior of Aircraft.  This could be a presumptive title based on the fact that the data is about airplane crashes.  Given the low and nearly equal weights of each word in the category it is plausible these words have no association to a crash event.  After all none of the words in Topic 1 necessarily suggest a \u2018damage\u2019 has occurred.  It may also be argued that Topic 1 was a \u2018forced\u2019 result since the model had to produce three assigned topics when in reality only Topic 2 and Topic 3 could be found.  In this case the weights might give some guidance on whether the topic is relevant at all. \n\nThese are some considerations to keep in mind when interpreting the results of topic modeling. In some or maybe most cases the results to be found by LDA may be difficult to decode. Other times one may have expectations and find topics to meet those expectations that may not exist.  Although the findings discovered in this analysis seemed to have provided some interesting insights to the difference in causes to low and high fatality airplane crashes, the results should be viewed with some skepticism and may need some validation from how others interpret the same results. \n\n\n### If you liked this post or learned something new, consider giving me an UPVOTE.\n#### Thank you!"}}