{"cell_type":{"44395fc7":"code","494780fa":"code","5b2876a0":"code","fac3cc36":"code","8fff596a":"code","1550b21f":"code","6b0dbe4c":"code","fd0a75b6":"code","df931e01":"code","f065117b":"code","a738d5e0":"code","ec0c38e8":"code","83921406":"code","63ff31f9":"code","f53770f4":"code","0dd1b3ed":"code","4921f726":"code","39c4026f":"code","3e9528b2":"code","9e18a431":"code","3f3ce23c":"code","467847ba":"code","0bfaa2e0":"code","57ab9ddf":"code","97716d59":"code","3172ab40":"code","6db4a44d":"code","48a422ae":"markdown","39a8248e":"markdown","a02c80a4":"markdown","e2d4d328":"markdown","fb738e09":"markdown","ae8b05c0":"markdown","77b06a1b":"markdown","ccfedcc6":"markdown","964bea8b":"markdown","97d1d2ef":"markdown","e081d3aa":"markdown","7c9aefb9":"markdown","f3b790c5":"markdown","a9580363":"markdown","c0f9794c":"markdown","4bfd4d9a":"markdown","be58df81":"markdown","7ae5323d":"markdown","ca76d51a":"markdown","de8ffb96":"markdown"},"source":{"44395fc7":"!pip install datatable\n!pip install MLXtend","494780fa":"import numpy as np\nimport pandas as pd\nimport tqdm\nimport itertools\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\npd.options.display.max_columns = 200\nimport os\nimport gc\nimport re\nimport datatable as dt\ndef chunks(l, n):\n    \"\"\" Yield n successive chunks from l.\n    \"\"\"\n    newn = int(len(l) \/ n)\n    for i in range(0, n-1):\n        yield l[i*newn:i*newn+newn]\n    yield l[n*newn-newn:]\n    \ninput_path = '\/kaggle\/input\/'\nroot_path = os.path.join(input_path, 'jane-street-market-prediction')\ntrain = dt.fread(os.path.join(root_path, \"train.csv\")).to_pandas()\nfloat64_cols = train.select_dtypes('float64').columns\ntrain[float64_cols] = train[float64_cols].astype('float32')\nresp_cols = [i for i in train.columns if 'resp' in i]\n\nfeatures_names = list(set(train.columns) - set(resp_cols) - set(['weight', 'ts_id', 'date']))\nfeatures_index = list(map(lambda x: int(re.sub(\"feature_\", \"\", x)), features_names))\nfeatures = sorted(list(zip(features_names, features_index)), key = lambda x: x[1])\nfeatures = [i[0] for i in features] + resp_cols","5b2876a0":"display(train.info(), train.dtypes.value_counts().to_frame().rename(columns = {0: 'dtype'}))","fac3cc36":"#count\nnan_values_train = (train\n .apply(lambda x: x.isna().sum(axis = 0)\/len(train))\n .to_frame()\n .rename(columns = {0: 'percentage_nan_values'})\n.sort_values('percentage_nan_values', ascending = False)\n)\n\ndisplay((train\n .apply(lambda x: x.isna().sum(axis = 0))\n .to_frame()\n .rename(columns = {0: 'count_nan_values'})\n.sort_values('count_nan_values', ascending = False)\n.transpose()), nan_values_train.transpose())","8fff596a":"fig, ax = plt.subplots(figsize = (20, 12))\n\nsns.set_palette(\"RdBu\", 10)\n#RdBu, YlGn\nax = sns.barplot(x='percentage_nan_values', \n            y='feature', \n            palette = 'GnBu_r',\n            data=nan_values_train.reset_index().rename(columns = {'index': 'feature'}).head(40))\n\nfor p in ax.patches:\n    width = p.get_width() \n    if width < 0.01:# get bar length\n        ax.text(width,       # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center')  # vertical alignment\n    else:\n        if width < 0.03:\n            color_text = 'black'\n        else:\n            color_text = 'white'\n        ax.text(width \/2, \n                # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center',\n            color = 'white',\n            fontsize = 11)  # vertical alignment\n\nax.set_title('Top 40 Features for percentage of NaN Values')\n\n","1550b21f":"top_nan_features = (nan_values_train.head(30).index.tolist())\ndel nan_values_train\nfig, axes = plt.subplots(10, 3, figsize = (40, 30))\nax = axes.ravel()\n\nts_id = train.ts_id\nmini_df = pd.concat([(train[top_nan_features].isna().astype(int)),train[['date']]], 1)\nmini_df = mini_df.groupby('date').sum().reset_index()\n\nfor i in range(len(top_nan_features)):\n    \n    feature_name = top_nan_features[i]\n    \n    mini_df[[feature_name, \"date\"]].plot(y = feature_name , kind = 'line',\n                                         xlabel = 'date', \n                                         ylabel = feature_name+ \"_nans\", linewidth=0.3,\n                                         legend = False,\n                                         ax = ax[i])","6b0dbe4c":"train['daily_ts_id'] = (train.groupby('date').cumcount())","fd0a75b6":"fig, axes = plt.subplots(10, 3, figsize = (40, 30))\nax = axes.ravel()\n\nts_id = train.ts_id\nmini_df = pd.concat([(train[top_nan_features].isna().astype(int)),train[['ts_id']]], 1).iloc[:50000, :]\nnew_day = (train.iloc[:50000, :].query(\"daily_ts_id == 0\").ts_id.tolist())\n\nfor i in range(len(top_nan_features)):\n    \n    feature_name = top_nan_features[i]\n    \n    mini_df[[feature_name, \"ts_id\"]].plot(y = feature_name , kind = 'line',\n                                         xlabel = 'ts_id', \n                                         ylabel = feature_name+ \"_nans\", linewidth=0.3,\n                                         legend = False,\n                                         ax = ax[i])\n    for m in range(len(new_day)):\n        ax[i].axvline(new_day[m], alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = 'blue')\n        if i == 0:\n            if m == 2:\n                ax[i].text(new_day[m]-1500, 1.1, \"day {}\".format(m), size = 7, alpha = 0.8)\n            else:\n                ax[i].text(new_day[m]+200, 1.1, \"day {}\".format(m), size = 7, alpha = 0.8)","df931e01":"#percentage\ntrain_unique_values = (train\n .apply(lambda x: x.nunique())\n .to_frame()\n .rename(columns = {0: 'number_unique_values'})\n.sort_values('number_unique_values', ascending = True)\n)\n\ndisplay(train_unique_values.transpose())\ndel train_unique_values","f065117b":"feature_chunks = list(chunks(range(len(features)), 15))\n\nfor j in feature_chunks:\n\n    fig, axes = plt.subplots(3, 3, figsize = (20, 12))\n    ax = axes.ravel()\n\n    for i in j:\n\n        feature = train[features[i]]\n    \n        sns.distplot(feature, hist=True, kde=True, \n         #bins=35, \n         color = 'blue', \n         hist_kws={'edgecolor':'black'},\n         kde_kws={'linewidth': 2}, ax = ax[i%9])\n        ax[i%9].grid(True)","a738d5e0":"for j in feature_chunks:\n\n    fig, axes = plt.subplots(3, 3, figsize = (20, 12))\n    ax = axes.ravel()\n\n    for i in j:\n\n        feature = train[features[i]]\n    \n        sns.boxplot(feature, \n         color = 'blue', ax = ax[i%9])\n        ax[i%9].grid(True)","ec0c38e8":"for j in feature_chunks:\n\n    fig, axes = plt.subplots(3, 3, figsize = (20, 12))\n    ax = axes.ravel()\n\n    for i in j:\n\n        mini_df = (train.groupby('date').agg({features[i]: ['mean', 'std']}))\n        mini_df.iloc[:, 0].plot(kind = 'line',xlabel = 'date', \n                                ylabel = features[i], linewidth=1,\n                               ax = ax[i%9])","83921406":"train['daily_ts_id'] = (train.groupby('date').cumcount())","63ff31f9":"gc.collect() #Let's free some RAM up\ncreated_cols = [] \ncorrelation_matrix = train.drop(created_cols, axis = 1, errors = 'ignore').corr()","f53770f4":"plt.figure(figsize = (30, 12))\n\nax = sns.heatmap(\n    correlation_matrix, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(220, 20, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    {'fontsize': 5},\n    rotation=90,\n    horizontalalignment='right'\n);","0dd1b3ed":"corrMatrix=correlation_matrix.loc[features, features].copy()\n\ncorrMatrix.loc[:,:] =  np.tril(corrMatrix, k=-1) # borrowed from Karl D's answer\nCORRELATION_THRESHOLD = 0.65\nalready_in = set()\npositive_correlated_features = []\nfor col in corrMatrix:\n    perfect_corr = corrMatrix[col][corrMatrix[col] > CORRELATION_THRESHOLD].index.tolist()\n    if perfect_corr and col not in already_in:\n        already_in.update(set(perfect_corr))\n        perfect_corr.append(col)\n        positive_correlated_features.append(perfect_corr)\n\n\nte = TransactionEncoder()\n\nte_ary = te.fit(positive_correlated_features).transform(positive_correlated_features)\n\ndf = pd.DataFrame(te_ary, columns=te.columns_)\n\nfrequent_itemsets = (apriori(df, min_support=0.03, use_colnames=True))\nfrequent_itemsets = frequent_itemsets.loc[frequent_itemsets.itemsets.apply(lambda x: len(x) > 2)].reset_index(drop = True)\npd.options.display.max_colwidth = 300","4921f726":"plt.figure(figsize = (30, 10))\n#all_but_time = list(set(train.columns) - set(['date', 'ts_id', 'daily_ts_id']))\nax = sns.heatmap(\n    correlation_matrix.loc[['date', 'ts_id', 'daily_ts_id'], features], \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(220, 20, n=200),\n    square=False\n)","39c4026f":"abs_daily = (correlation_matrix.loc[features, 'daily_ts_id'].to_frame()\n.rename(columns = {'daily_ts_id' : 'correlation'}).assign(abs_corr=lambda x: abs(x)))\nfeatures_daily = sorted(abs_daily.sort_values('abs_corr', ascending = False).head(5).index.tolist())\n\nfig, axes = plt.subplots(5, 1, figsize = (15, 10), sharex = True)\nnew_day = (train.iloc[:50000, :].query(\"daily_ts_id == 0\").ts_id.tolist())\nax = axes.ravel()\nfor j in range(len(features_daily)):\n    feature_name = features_daily[j]\n    feature = train[feature_name]\n    feature[:50000].plot(linewidth = 0.5, ax = ax[j], xlabel='ts_id', ylabel = feature_name)\n    for m in range(len(new_day)):\n        ax[j].axvline(new_day[m], alpha = 0.5, ymin = -5, ymax = 5, linestyle = \":\", color = 'blue')\n        if j == 0:\n            if m == 2:\n                ax[j].text(new_day[m]-1500, 5.1, \"day {}\".format(m), size = 7, alpha = 0.8)\n            else:\n                ax[j].text(new_day[m]+200, 5.1, \"day {}\".format(m), size = 7, alpha = 0.8)","3e9528b2":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_correlations(df, percentage=0.1):\n    au_corr = df.corr().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    n = int(len(au_corr)*percentage)\n    return au_corr[0:n]\n\nfeatures_without_resp = list(set(features) - set(resp_cols))\ndf_correlation_plot = correlation_matrix.loc[features_without_resp, features_without_resp]\n\ndf_correlation_plot = (get_top_correlations(df_correlation_plot, percentage = 1)\n                      .reset_index(drop = False)\n                      .rename(columns = {0: 'correlation'}))\n\ndf_correlation_plot['index_1'] = df_correlation_plot.level_0.str.replace(\"feature_\", \"\").astype(int)\ndf_correlation_plot['index_2'] = df_correlation_plot.level_1.str.replace(\"feature_\", \"\").astype(int)\ndf_correlation_plot['absolute_correlation'] = df_correlation_plot['correlation'].abs()\ndf_correlation_plot['sign_correlation'] = (df_correlation_plot['correlation'] > 0).astype(int).replace({1: 'positive', 0:'negative'})\n\ndf_correlation_plot['pair_of_features'] = df_correlation_plot['index_1'].astype(str) + \"-\" + df_correlation_plot['index_2'].astype(str) \ngc.collect()","9e18a431":"fig, axes = plt.subplots(1, 2, figsize = (20, 12))\nax = axes.ravel()\nsns.set_palette(\"RdBu\", 10)\n#RdBu, YlGn\npositive_dict = {0: 'positive', 1: 'negative'}\nfor j in range(2):\n    if j == 0:\n        sns.barplot(x='correlation', \n            y='pair_of_features',\n            ax = ax[j],\n            data=df_correlation_plot.head(40))\n    else:\n        sns.barplot(x='correlation', \n            y='pair_of_features',\n            ax = ax[j],\n            data=df_correlation_plot.tail(40))\n\n    for p in ax[j].patches:\n        width = p.get_width() \n        ax[j].text(width \/2, \n                # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center',\n            color = 'black',\n            fontsize = 11)  # vertical alignment\n\n    ax[j].set_title('Top 40 pair of features for {} correlation'.format(positive_dict[j]))","3f3ce23c":"fig, ax = plt.subplots(1, 1, figsize = (12, 8))\nquantiles = np.quantile(df_correlation_plot.correlation, [0.05, 0.25, 0.5, 0.75, 0.95])\nsns.distplot(df_correlation_plot.correlation, hist=False, kde=True, \n             color = 'blue', hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1}, ax = ax)\nfor j in range(len(quantiles)):\n    ax.axvline(quantiles[j], alpha = 0.3, ymax = 2, linestyle = \":\")\nax.grid(True)\nax.text(quantiles[0]-.1, 0.37, \"5th\", size = 10, alpha = 0.8)\nax.text(quantiles[1]-.1, 0.37, \"25th\", size = 10, alpha = 0.85)\nax.text(quantiles[2]-.1, 0.37, \"50th\", size = 10, alpha = 1)\nax.text(quantiles[3]-.1, 0.37, \"75th\", size = 10, alpha = 0.85)\nax.text(quantiles[4]-.1, 0.37, \"95th\", size = 10, alpha =.8)\nax.set_title('Correlation distribution')","467847ba":"gc.collect()\nautocorr_dataframe = (pd.DataFrame(train.apply(lambda x: x.autocorr(), 0))\n                     .reset_index().rename(columns = {'index': 'feature', 0: 'autocorrelation'})\n                     .sort_values('autocorrelation', ascending = False))\ngc.collect()","0bfaa2e0":"fig, ax = plt.subplots(figsize = (20, 12))\n\nsns.set_palette(\"RdBu\", 10)\n#RdBu, YlGn\nax = sns.barplot(x='autocorrelation', \n            y='feature', \n           # palette = 'GnBu_r',\n            data=(autocorr_dataframe\n                  .head(40)))\n\nfor p in ax.patches:\n    width = p.get_width() \n    if width < 0.01:# get bar length\n        ax.text(width,       # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center')  # vertical alignment\n    else:\n        ax.text(width \/2, \n                # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center',\n            color = 'black',\n            fontsize = 11)  # vertical alignment\n\nax.set_title('Top 40 Features for autocorrelation')","57ab9ddf":"fig, ax = plt.subplots(1, 1, figsize = (12, 8))\nquantiles = np.quantile(autocorr_dataframe.autocorrelation, [0.05, 0.25, 0.5, 0.75, 0.95])\nsns.distplot(autocorr_dataframe.autocorrelation, hist=False, kde=True, \n             color = 'blue', hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1}, ax = ax)\nfor j in range(len(quantiles)):\n    ax.axvline(quantiles[j], alpha = 0.3, ymax = 2, linestyle = \":\")\nax.grid(True)\nax.text(quantiles[0]-.05, 0.27, \"5th\", size = 10, alpha = 0.8)\nax.text(quantiles[1]-.05, 0.37, \"25th\", size = 10, alpha = 0.85)\nax.text(quantiles[2]-.05, 0.37, \"50th\", size = 10, alpha = 1)\nax.text(quantiles[3]-.05, 0.37, \"75th\", size = 10, alpha = 0.85)\nax.text(quantiles[4]-.05, 0.37, \"95th\", size = 10, alpha =.8)\nax.set_title('Autocorrelation Lag 1 distribution')","97716d59":"def crosscorr(datax, datay, lag=0):\n    \"\"\" Lag-N cross correlation. \n    Parameters\n    ----------\n    lag : int, default 0\n    datax, datay : pandas.Series objects of equal length\n\n    Returns\n    ----------\n    crosscorr : float\n    \"\"\"\n    return datax.corr(datay.shift(lag))","3172ab40":"RECALCULATE = False\nif RECALCULATE:\n\n    total_lags = range(1, 3)\n\n    cross_corr = {}\n\n    combinations = list(itertools.product(features, features))\n\n    for j in total_lags:\n        cross_corr[j] = []\n        for k in tqdm.tqdm(combinations):\n            cross_corr[j].append(crosscorr(train[k[0]], train[k[1]], lag = j))\n        #cross_corr[j] = list(map(lambda x: crosscorr(train[x[0]], train[x[1]], j), combinations))\n\n    cross_correlations = (pd.DataFrame(combinations)\n                          .rename(columns = {0: 'first_feature', 1: 'second_feature'}))\n\n    cross_correlations_melt = (pd.melt(cross_correlations, id_vars=['first_feature', 'second_feature'], \n                               value_vars=['cross_correlation_lag_1', 'cross_correlation_lag_2'],\n                               var_name = 'lag',\n                               value_name = 'cross_correlation')\n                              .assign(lag=lambda x: x.lag.str.replace('cross_correlation_lag_', \"\")))\n\n    cross_correlations_melt['pair_of_features'] = (cross_correlations_melt['first_feature'].str.replace(\"feature_\", \"\") + \n                                              \"_\"  + cross_correlations_melt['second_feature'].str.replace(\"feature_\", \"\") + \"_lag\" +\n                                                   cross_correlations_melt['lag']\n                                                  ).astype(str)\n\nelse:\n    cross_correlations_melt = pd.read_pickle(os.path.join(input_path, 'crosscorrelation\/lag1and2crosscorrelations_melted.pickle'))\n    cross_correlations_melt['pair_of_features'] = (cross_correlations_melt['first_feature'].str.replace(\"feature_\", \"\") + \n                                              \"_\"  + cross_correlations_melt['second_feature'].str.replace(\"feature_\", \"\") + \"_lag\" +\n                                                   cross_correlations_melt['lag']\n                                                  ).astype(str)","6db4a44d":"fig, axes = plt.subplots(1, 2, figsize = (20, 12))\nax = axes.ravel()\nsns.set_palette(\"RdBu\", 10)\n#RdBu, YlGn\npositive_dict = {0: 'negative', 1: 'positive'}\nfor j in range(2):\n    if j == 0:\n        sns.barplot(x='cross_correlation', \n                y='pair_of_features', \n               # palette = 'GnBu_r',\n                data=(cross_correlations_melt\n                      .sort_values('cross_correlation')\n                      .head(40)),\n                   ax = ax[j])\n    else:\n        sns.barplot(x='cross_correlation', \n                y='pair_of_features', \n               # palette = 'GnBu_r',\n                data=(cross_correlations_melt\n                      .sort_values('cross_correlation', ascending = False)\n                      .head(40)),\n                   ax = ax[j])\n\n    for p in ax[j].patches:\n        width = p.get_width() \n        ax[j].text(width \/2, \n                # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center',\n            color = 'black',\n            fontsize = 11)  # vertical alignment\n\n    ax[j].set_title('Top 40 Features for {} crosscorrelation'.format(positive_dict[j]))","48a422ae":"Unique values per column in Train","39a8248e":"<img src=\"https:\/\/images.emojiterra.com\/google\/android-10\/512px\/1f914.png\" width=\"20\" height=\"20\" style=\"top:03%; left:80%\"> \n\n*Vertical blue lines indicate a new day. These features are supersimilar and definitely seem to have a positive correlation with daily_ts_id. I don't have a clue about their actual meaning*","a02c80a4":"Boxplots to further understand each feature distribution","e2d4d328":"<img src=\"https:\/\/images.emojiterra.com\/google\/android-10\/512px\/1f914.png\" width=\"20\" height=\"20\" style=\"top:03%; left:80%\"> \n\n*It seems like there are group of features very correlated, most of them are also near each other as we can see around the diagonal.\nDate and ts_id are very much correlated, but I don't see any other feature particularly correlated with them.*","fb738e09":"# Imports and Data Loading\n\nHere we just install necessary packages (datatable), import them, define functions for later usage and load train.csv data. \n\n**Train float64 columns will be downcasted to float32 to ease RAM burden**\n","ae8b05c0":"<img src=\"https:\/\/images.emojiterra.com\/google\/android-10\/512px\/1f914.png\" width=\"20\" height=\"20\" style=\"left\"> \n\n*Most of the features have a different behaviour before and after date 200, being more spiky before and almost stationary after.*\n","77b06a1b":"<img src=\"https:\/\/images.emojiterra.com\/google\/android-10\/512px\/1f914.png\" width=\"50\" height=\"50\" style=\"top:03%; left:80%\"> \n\nIt seems like for most of the features nan happen in the first part of the day (each blue line represents the beginning of a new day).\n","ccfedcc6":"<img src=\"https:\/\/images.emojiterra.com\/google\/android-10\/512px\/1f914.png\" width=\"20\" height=\"20\" style=\"top:03%; left:80%\"> \n\n*There are definitely some features very correlated with daily_ts_id. Let's plot a few of them*","964bea8b":"Cross Correlation","97d1d2ef":"Distribution seems pretty symmetric. ","e081d3aa":"# Exploratory Data Analysis Jane Street\n\n<img src=\"https:\/\/www.europol.europa.eu\/sites\/default\/files\/images\/finance_budget.jpg\">\n\n\n## Hi all! I have decided to try my best at exploring the data. I'm eager to share with you what I've found and to receive your feedback even more! ","7c9aefb9":"<img src=\"https:\/\/images.emojiterra.com\/google\/android-10\/512px\/1f914.png\" width=\"50\" height=\"50\" style=\"top:03%; left:80%\"> \n\nSome features have almost the same number of nan values. Let's plot their distribution over time.\n","f3b790c5":"# Basic Train Exploration\n\nHere I start looking at how train.csv looks like: \n\n- dtypes and memory usage\n- nan values distribution\n- unique values distribution\n\n- feature density\/boxplot distribution\n- feature distribution through time\n\n","a9580363":"Let's create a new columns which gives the daily_tsid, starting each date from 0 and squashing it between 0-1 ","c0f9794c":"Autocorrelation for different lags","4bfd4d9a":"<img src=\"https:\/\/images.emojiterra.com\/google\/android-10\/512px\/1f914.png\" width=\"20\" height=\"20\" style=\"top:03%; left:80%\"> \n\n*So many features are highly correlated, let's see the correlation distribution*","be58df81":"# Features relationships\n\nHere I try to go a little bit deeper in inspecting the relationships between features and between feature and time. \n\n- Features Correlations\n- Features AutoCorrelation\n- Features CrossCorrelation","7ae5323d":"<h4> Features Distributions <\/h4>","ca76d51a":"Let's plot all features and then deepdive on the relationship between them","de8ffb96":"<img src=\"https:\/\/images.emojiterra.com\/google\/android-10\/512px\/1f914.png\" width=\"20\" height=\"20\" style=\"left\"> \n\n*Most of the features have long-tailed distributions, some of them in both directions, while others just on the positive axis. \nThere are also some bimodal distributions, it would be nice to inspect their relationship with time*\n"}}