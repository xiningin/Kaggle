{"cell_type":{"8321e3d5":"code","663b621b":"code","3fd79dd5":"code","d3040462":"code","da4fb5a9":"code","1ad11222":"code","c74512ab":"code","f7319f7d":"code","a9189db6":"code","fbb9d118":"code","27f48fcf":"code","458cf320":"code","01722bc8":"code","a253fcd2":"code","0ab4350f":"code","e61c8fda":"code","50b58bd9":"code","29673972":"code","c9aa5e05":"code","a0c7a88b":"code","f5b80b71":"code","c59a58a4":"code","df0e4095":"code","f7a93069":"markdown","c724dd52":"markdown","ee808571":"markdown","d9235aaa":"markdown","92af0ed9":"markdown","dc5ac1f8":"markdown","e102dce7":"markdown","f7e13ac8":"markdown","e7b7b1f2":"markdown","43406491":"markdown","9308aa24":"markdown","0b293a7a":"markdown","29894249":"markdown","e3f315d0":"markdown","7aa6820f":"markdown","20d993d1":"markdown","82773f7c":"markdown","58ea82e5":"markdown","e24a8454":"markdown"},"source":{"8321e3d5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_colwidth', -1)\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom matplotlib_venn import venn2, venn2_circles\n\nimport seaborn as sns\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport string\nimport spacy # Leading library for NLP\nnlp = spacy.load('en')\n\nfrom wordcloud import WordCloud\nfrom sklearn.linear_model import LinearRegression\n\n# Load survey data from 2017\nsurvey_schema_2017 = pd.read_csv(\"..\/input\/kaggle-survey-2017\/schema.csv\",encoding='ISO-8859-1')\nmultiple_choice_responses_2017 = pd.read_csv(\"..\/input\/kaggle-survey-2017\/multipleChoiceResponses.csv\",encoding='ISO-8859-1')\nfreeform_responses_2017 = pd.read_csv(\"..\/input\/kaggle-survey-2017\/freeformResponses.csv\",encoding='ISO-8859-1')\n\n# Load survey data from 2018\nsurvey_schema_2018 = pd.read_csv(\"..\/input\/kaggle-survey-2018\/SurveySchema.csv\")\nmultiple_choice_responses_2018 = pd.read_csv(\"..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv\")\nfreeform_responses_2018 = pd.read_csv(\"..\/input\/kaggle-survey-2018\/freeFormResponses.csv\")\n\n# Load survey data from 2019\nsurvey_schema_2019 = pd.read_csv(\"..\/input\/kaggle-survey-2019\/survey_schema.csv\")\nmultiple_choice_responses_2019 = pd.read_csv(\"..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\")\nfreeform_responses_2019 = pd.read_csv(\"..\/input\/kaggle-survey-2019\/other_text_responses.csv\")\n\n# Load survey data from 2020\nmultiple_choice_responses_2020 = pd.read_csv(\"..\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv\")\n\n# Load Kaggle Meta Data\nkagglers_achievements_df = pd.read_csv(\"..\/input\/meta-kaggle\/UserAchievements.csv\")\nkagglers_df = pd.read_csv(\"..\/input\/meta-kaggle\/Users.csv\")\n\nctds_df = pd.read_csv(\"..\/input\/chai-time-data-science\/Episodes.csv\")\nctds_kaggle_df = ctds_df[ctds_df.category == 'Kaggle']\n\nall_data = []\nfor episode in ctds_kaggle_df.index:\n    try:\n        transcript = pd.read_csv(f\"\/kaggle\/input\/chai-time-data-science\/Cleaned Subtitles\/E{episode}.csv\")\n        transcript['episode'] = episode\n        transcript['heroes'] = ctds_kaggle_df.loc[episode].heroes\n        if episode == 1:\n            all_data = transcript\n        else:\n            all_data = all_data.append(transcript)#[1:-1])\n    except:\n        print(f\"Episode {episode} does not have a transcript.\")\n\nall_data = all_data.reset_index()\nall_data.columns = ['line', 'time', 'speaker', 'text', 'episode', 'heroes']\n\nall_data_red = all_data[['episode', 'heroes','speaker', 'text']][(all_data.speaker != 'Unknown Speaker') & (all_data.episode !=34)] # Sorry, I have yet to learn how to handle the data cleaning for three different speakers\nall_data_red['speaker'] = all_data_red['speaker'].apply(lambda x: re.sub('Dr. ', '', x))\nall_data_red['key'] = (all_data_red['speaker'] != all_data_red['speaker'].shift(1)).astype(int).cumsum()\nall_data_grouped = all_data_red.groupby(['episode', 'heroes', 'key', 'speaker'])['text'].apply(' '.join).to_frame().reset_index(drop=False)\nall_data_grouped['speaker_generic'] = all_data_grouped['speaker'].apply(lambda x: 0 if x=='Sanyam Bhutani' else 1)\nall_data_grouped = all_data_grouped.groupby(['episode', 'heroes'], as_index=False).apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\nall_data_grouped['new_question'] = (all_data_grouped['speaker'] == 'Sanyam Bhutani').astype(int).cumsum()\nall_data_grouped = all_data_grouped.pivot(index=['episode', 'heroes', 'new_question'], columns='speaker_generic')['text']\nall_data_grouped.columns = ['interviewer', 'guest']\nall_data_grouped = all_data_grouped.fillna('')\nquote = all_data_grouped.reset_index(drop=False)","663b621b":"kaggle_motivation = freeform_responses_2017[~freeform_responses_2017.KaggleMotivationFreeForm.isna()].KaggleMotivationFreeForm\n\n# Convert text to lowercase\nkaggle_motivation = kaggle_motivation.apply(lambda x: x.lower())\n\n# Remove punctuation\nkaggle_motivation = kaggle_motivation.apply(lambda x: re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", x))\n\n# Remove non-Roman characters\nkaggle_motivation = kaggle_motivation.apply(lambda x: re.sub(\"([^\\x00-\\x7F])+\", \" \", x))\n\n# Tokenize\ndef tokenize(x):\n    x = nlp(x)\n    x_clean = \"\"\n    for token in x:\n        # Remove stop words and remove words with fewer than 3 chars\n        if (not token.is_stop) and (len(token) > 3):\n            # Lemmatize and tokenize\n            x_clean += str(token) + \" \" \n    return x_clean\n\nkaggle_motivation = kaggle_motivation.apply(lambda x: tokenize(x))\n\n# Start with one review:\ntext = ''.join(kaggle_motivation)\n\ndef one_color_func(word=None, font_size=None, \n                   position=None, orientation=None, \n                   font_path=None, random_state=None):\n    h = 200 # 0 - 360\n    s = 100 # 0 - 100\n    l = random_state.randint(30, 70) # 0 - 100\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color ='white', random_state=2020, color_func=one_color_func).generate(text)\n\n# Display the generated image:\nplt.figure(figsize=[14,6])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n","3fd79dd5":"### Help finctions ###\ndef unify_age_2017(x):\n    if x < 22:\n        return '18-21'\n    elif x < 25:\n        return '22-24'\n    elif x < 30:\n        return '25-29' \n    elif x < 35:\n        return '30-34' \n    elif x < 40:\n        return '35-39'\n    elif x < 45:\n        return '40-44'\n    elif x < 50:\n        return '45-49'\n    elif x < 55:\n        return '50-54'\n    elif x < 60:\n        return '55-59'\n    elif x < 70:\n        return '60-69'\n    else:\n        return '70+'\n\n\ndef unify_compensation_2017(x):\n    try:\n        x = re.sub(',','', x)\n        x = int(x)\n        if x < 10000:\n            return '0-9,999'\n        elif x <20000:\n            return '10,000-19,999'\n        elif x < 30000:\n            return '20,000-29,999'\n        elif x < 40000:\n            return '30,000-39,999'\n        elif x < 50000:\n            return '40,000-49,999'\n        elif x < 60000:\n            return '50,000-59,999'\n        elif x < 70000:\n            return '60,000-69,999'\n        elif x  < 80000:\n            return '70,000-79,999'\n        elif x < 90000:\n            return '80,000-89,999'\n        elif x < 100000:\n            return '90,000-99,999'\n        elif x < 125000:\n            return '100,000-124,999'\n        elif x < 150000:\n            return '125,000-149,999'\n        elif x < 200000:\n            return '150,000-199,999'\n        elif x < 250000:\n            return '200,000-249,999'\n        elif x < 300000:\n            return '250,000-299,999'\n        elif x < 500000:\n            return '300,000-500,000'\n        else: \n            return '> $500,000'\n    except:\n        return x\n    \ndef unify_compensation_2018(x):\n    if x == '0-10,000':\n        return '0-9,999'\n    elif x == '0-10,000':\n        return '2,000-2,999'\n    elif x == '0-10,000':\n        return '5,000-7,499'\n    elif x == '0-10,000':\n        return '7,500-9,999'\n    elif x == '10-20,000':\n        return '10,000-19,999'\n    elif x == '20-30,000':\n        return '20,000-29,999'\n    elif x == '30-40,000':\n        return '30,000-39,999'\n    elif x == '40-50,000':\n        return '40,000-49,999'\n    elif x == '50-60,000':\n        return '50,000-59,999'\n    elif x == '60-70,000':\n        return '60,000-69,999'\n    elif x == '70-80,000':\n        return '70,000-79,999'\n    elif x == '80-90,000':\n        return '80,000-89,999'\n    elif x == '90-100,000':\n        return '90,000-99,999'\n    elif x == '100-125,000':\n        return '100,000-124,999'\n    elif x == '125-150,000':\n        return '125,000-149,999'\n    elif x == '150-200,000':\n        return '150,000-199,999'\n    elif x == '200-250,000':\n        return '200,000-249,999'\n    elif x == '250-300,000':\n        return '250,000-299,999'\n    elif ((x == '300-400,000') | (x == '400-500,000')):\n        return '300,000-500,000'\n    elif x == '500,000+':\n        return '> $500,000'\n    else: \n        return 'I do not wish to disclose my approximate yearly compensation'\n\ndef unify_compensation_2019(x):\n    if ((x == '$0-999') | (x== '1,000-1,999') | (x == '2,000-2,999') | (x == '3,000-3,999') | (x == '4,000-4,999')  | (x == '5,000-7,499') | (x == '7,500-9,999')):\n        return '0-9,999'\n    elif ((x == '10,000-14,999') | (x == '15,000-19,999')):\n        return '10,000-19,999'\n    elif ((x == '20,000-24,999') | (x == '25,000-29,999')):\n        return '20,000-29,999'\n    else: \n        return x","d3040462":"# Merge all survey data from all years into one dataframe and unify responses as much as possible\ndf =  pd.DataFrame()\n\nrelevant_2017 =  pd.DataFrame()\nrelevant_2018 =  pd.DataFrame()\nrelevant_2019 =  pd.DataFrame()\nrelevant_2020 =  pd.DataFrame()\n\n\n### 2017 ###\nrelevant_2017['Age'] = multiple_choice_responses_2017.Age\nrelevant_2017['Age'] = relevant_2017['Age'].apply(lambda x: unify_age_2017(x))\nrelevant_2017['Compensation_Amout'] = multiple_choice_responses_2017.CompensationAmount\nrelevant_2017['Compensation_Currency'] = multiple_choice_responses_2017.CompensationCurrency\nrelevant_2017['Compensation'] = relevant_2017['Compensation_Amout'].apply(lambda x: unify_compensation_2017(x))\n\nrelevant_2017['Country'] = multiple_choice_responses_2017.Country\nrelevant_2017['Occupation'] = multiple_choice_responses_2017.CurrentJobTitleSelect\nrelevant_2017['Occupation_Freeform'] = freeform_responses_2017.CurrentJobTitleFreeForm\nrelevant_2017['Student'] = multiple_choice_responses_2017.StudentStatus\nrelevant_2017['Occupation'] = relevant_2017.apply(lambda x: 'Student' if x.Student == 'Yes' else x.Occupation, axis=1)\nrelevant_2017 = relevant_2017.drop('Student', axis=1)\nrelevant_2017['Education'] = multiple_choice_responses_2017.FormalEducation\nrelevant_2017['Language_Recommendation'] = multiple_choice_responses_2017.LanguageRecommendationSelect\nrelevant_2017['Language_Recommendation_Freeform'] = freeform_responses_2017.LanguageRecommendationFreeForm\nrelevant_2017['Programming_Experience'] = multiple_choice_responses_2017.Tenure\nrelevant_2017['Year'] = 2017\n\n### 2018 ###\nrelevant_2018['Age'] = multiple_choice_responses_2018.Q2\nrelevant_2018['Age'] = relevant_2018['Age'].replace({'70-79' : '70+', '80+' : '70+'}) \nrelevant_2018['Compensation_Amout'] = multiple_choice_responses_2018.Q9\nrelevant_2018['Compensation_Currency'] = 'USD'\nrelevant_2018['Compensation'] = relevant_2018['Compensation_Amout'].apply(lambda x: unify_compensation_2018(x))\nrelevant_2018['Country'] = multiple_choice_responses_2018.Q3\nrelevant_2018['Occupation'] = multiple_choice_responses_2018.Q6\nrelevant_2018['Occupation_Freeform'] = freeform_responses_2018.Q6_OTHER_TEXT\nrelevant_2018['Education'] = multiple_choice_responses_2018.Q4\nrelevant_2018['Language_Recommendation'] = multiple_choice_responses_2018.Q18\nrelevant_2018['Language_Recommendation_Freeform'] = freeform_responses_2018.Q18_OTHER_TEXT\nrelevant_2018['Programming_Experience'] = multiple_choice_responses_2018.Q24\n\nrelevant_2018['Hosted_Notebook_Products__Kaggle_Notebooks'] = multiple_choice_responses_2018.Q14_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Hosted_Notebook_Products__Google_Colab'] = multiple_choice_responses_2018.Q14_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Hosted_Notebook_Products__Azure_Notebooks'] = multiple_choice_responses_2018.Q14_Part_3.apply(lambda x: 0 if x != x else 1) | freeform_responses_2018.Q14_OTHER_TEXT.apply(lambda x: 1 if ('databricks' in str(x).lower()) else 0) \nrelevant_2018['Hosted_Notebook_Products__Google_Cloud_Datalab'] = multiple_choice_responses_2018.Q14_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Hosted_Notebook_Products__Paperspace_Gradient'] = multiple_choice_responses_2018.Q14_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Hosted_Notebook_Products__FloydHub'] = multiple_choice_responses_2018.Q14_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Hosted_Notebook_Products__Binder_JupyterHub'] = multiple_choice_responses_2018.Q14_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Hosted_Notebook_Products__IBM_Watson_Studio'] = freeform_responses_2018.Q14_OTHER_TEXT.apply(lambda x: 1 if (('ibm' in str(x).lower()) |('watson' in str(x).lower())) else 0) \nrelevant_2018['Hosted_Notebook_Products__Code_Ocean'] = freeform_responses_2018.Q14_OTHER_TEXT.apply(lambda x: 1 if ('ocean' in str(x).lower()) else 0) \nrelevant_2018['Hosted_Notebook_Products__AWS_Notebook'] = freeform_responses_2018.Q14_OTHER_TEXT.apply(lambda x: 1 if ('aws' in str(x).lower()) else 0) | freeform_responses_2018.Q14_OTHER_TEXT.apply(lambda x: 1 if ('sagemaker' in str(x).lower()) else 0) \nrelevant_2018['Hosted_Notebook_Products__Domino_Datalab'] = multiple_choice_responses_2018.Q14_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Hosted_Notebook_Products__Crestle'] = multiple_choice_responses_2018.Q14_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Hosted_Notebook_Products__Cocalc'] = freeform_responses_2018.Q14_OTHER_TEXT.apply(lambda x: 1 if ('cocalc' in str(x).lower()) else 0) \nrelevant_2018['Hosted_Notebook_Products__Datalore'] = freeform_responses_2018.Q14_OTHER_TEXT.apply(lambda x: 1 if ('datalore' in str(x).lower()) else 0) \nrelevant_2018['Hosted_Notebook_Products__Databricks'] = freeform_responses_2018.Q14_OTHER_TEXT.apply(lambda x: 1 if ('brick' in str(x).lower()) else 0) \nrelevant_2018['Hosted_Notebook_Products__None'] = multiple_choice_responses_2018.Q14_Part_10.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Hosted_Notebook_Products__Other'] = freeform_responses_2018.Q14_OTHER_TEXT\n\nrelevant_2018['Activities__Analyze_and_understand_data'] = multiple_choice_responses_2018.Q11_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Activities__Build_ML_service'] = multiple_choice_responses_2018.Q11_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Activities__Build_data_infrastructure'] = multiple_choice_responses_2018.Q11_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Activities__Build_Experimentation'] = 0\nrelevant_2018['Activities__Build_prototypes'] = multiple_choice_responses_2018.Q11_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Activities__Research'] = multiple_choice_responses_2018.Q11_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Activities__None'] = multiple_choice_responses_2018.Q11_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Activities__Other'] = freeform_responses_2018.Q11_OTHER_TEXT\n\nrelevant_2018['Framework__TensorFlow'] = multiple_choice_responses_2018.Q19_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Framework__Keras'] = multiple_choice_responses_2018.Q19_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Framework__PyTorch'] = multiple_choice_responses_2018.Q19_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Framework__TensorFlow_Keras'] = relevant_2018['Framework__TensorFlow'] & relevant_2018['Framework__Keras']\nrelevant_2018['Framework__TensorFlow_Keras_PyTorch'] = relevant_2018['Framework__TensorFlow'] | relevant_2018['Framework__Keras'] | relevant_2018['Framework__PyTorch']\n\nrelevant_2018['Cloud_Platform__GCP'] = multiple_choice_responses_2018.Q15_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Cloud_Platform__AWS'] = multiple_choice_responses_2018.Q15_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Cloud_Platform__Azure'] = multiple_choice_responses_2018.Q15_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Cloud_Platform__IBM'] = multiple_choice_responses_2018.Q15_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Cloud_Platform__Alibaba'] = multiple_choice_responses_2018.Q15_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Cloud_Platform__Salesforce'] = freeform_responses_2018.Q15_OTHER_TEXT.apply(lambda x: 1 if ('force' in str(x).lower()) else 0) \nrelevant_2018['Cloud_Platform__Oracle'] = freeform_responses_2018.Q15_OTHER_TEXT.apply(lambda x: 1 if ('oracle' in str(x).lower()) else 0) \nrelevant_2018['Cloud_Platform__SAP'] = freeform_responses_2018.Q15_OTHER_TEXT.apply(lambda x: 1 if ('sap ' in str(x).lower()) else 0) \nrelevant_2018['Cloud_Platform__VMWare'] = freeform_responses_2018.Q15_OTHER_TEXT.apply(lambda x: 1 if ('vm' in str(x).lower()) else 0) \nrelevant_2018['Cloud_Platform__Red_Hat'] = freeform_responses_2018.Q15_OTHER_TEXT.apply(lambda x: 1 if ('red' in str(x).lower()) else 0) \nrelevant_2018['Cloud_Platform__None'] = multiple_choice_responses_2018.Q15_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Cloud_Platform__Other'] = freeform_responses_2018.Q15_OTHER_TEXT.apply(lambda x: 0 if x != x else 1)\n\nrelevant_2018['Media__Twitter'] = multiple_choice_responses_2018.Q38_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Media__Newsletters'] = multiple_choice_responses_2018.Q38_Part_2.apply(lambda x: 0 if x != x else 1) | multiple_choice_responses_2018.Q38_Part_7.apply(lambda x: 0 if x != x else 1) | multiple_choice_responses_2018.Q38_Part_15.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Media__Reddit'] = multiple_choice_responses_2018.Q38_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Media__Kaggle'] = multiple_choice_responses_2018.Q38_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Media__Course_Forums'] = multiple_choice_responses_2018.Q38_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Media__YouTube'] = multiple_choice_responses_2018.Q38_Part_6.apply(lambda x: 0 if x != x else 1) | multiple_choice_responses_2018.Q38_Part_9.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Media__Podcast'] = multiple_choice_responses_2018.Q38_Part_8.apply(lambda x: 0 if x != x else 1) | multiple_choice_responses_2018.Q38_Part_16.apply(lambda x: 0 if x != x else 1) | multiple_choice_responses_2018.Q38_Part_17.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Media__Blogs'] = multiple_choice_responses_2018.Q38_Part_10.apply(lambda x: 0 if x != x else 1) | multiple_choice_responses_2018.Q38_Part_13.apply(lambda x: 0 if x != x else 1) | multiple_choice_responses_2018.Q38_Part_14.apply(lambda x: 0 if x != x else 1)  | multiple_choice_responses_2018.Q38_Part_18.apply(lambda x: 0 if x != x else 1)  | multiple_choice_responses_2018.Q38_Part_19.apply(lambda x: 0 if x != x else 1)  | multiple_choice_responses_2018.Q38_Part_20.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Media__Journal_Publications'] = multiple_choice_responses_2018.Q38_Part_11.apply(lambda x: 0 if x != x else 1) | multiple_choice_responses_2018.Q38_Part_12.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Media__Slack'] = multiple_choice_responses_2018.Q38_OTHER_TEXT.apply(lambda x: 1 if ('slack' in str(x).lower()) else 0) \nrelevant_2018['Media__None'] = multiple_choice_responses_2018.Q38_Part_21.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Media__Other'] = multiple_choice_responses_2018.Q38_Part_22.apply(lambda x: 0 if x != x else 1)\n\nrelevant_2018['Visual__ggplot2'] = multiple_choice_responses_2018.Q21_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Visual__Matplotlib'] = multiple_choice_responses_2018.Q21_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Visual__Altair'] = multiple_choice_responses_2018.Q21_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Visual__Shiny'] = multiple_choice_responses_2018.Q21_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Visual__D3.js'] = multiple_choice_responses_2018.Q21_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Visual__Plotly'] = multiple_choice_responses_2018.Q21_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Visual__Bokeh'] = multiple_choice_responses_2018.Q21_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Visual__Seaborn'] = multiple_choice_responses_2018.Q21_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Visual__Geoplotlib'] = multiple_choice_responses_2018.Q21_Part_9.apply(lambda x: 0 if x != x else 1)\nrelevant_2018['Visual__Leaflet_Folium'] = multiple_choice_responses_2018.Q21_Part_10.apply(lambda x: 0 if x != x else 1)\n\nrelevant_2018['Year'] = 2018\n\n### 2019 ###\nrelevant_2019['Age'] = multiple_choice_responses_2019.Q1\nrelevant_2019['Compensation_Amout'] = multiple_choice_responses_2019.Q10\nrelevant_2019['Compensation_Currency'] = 'USD'\nrelevant_2019['Compensation'] = relevant_2019['Compensation_Amout'].apply(lambda x: unify_compensation_2019(x))\nrelevant_2019['Country'] = multiple_choice_responses_2019.Q3\nrelevant_2019['Occupation'] = multiple_choice_responses_2019.Q5\nrelevant_2019['Occupation_Freeform'] = freeform_responses_2019.Q5_OTHER_TEXT\nrelevant_2019['Education'] = multiple_choice_responses_2019.Q4\nrelevant_2019['Language_Recommendation'] = multiple_choice_responses_2019.Q19\nrelevant_2019['Language_Recommendation_Freeform'] = freeform_responses_2019.Q19_OTHER_TEXT\nrelevant_2019['Programming_Experience'] = multiple_choice_responses_2019.Q15\n\nrelevant_2019['Hosted_Notebook_Products__Kaggle_Notebooks'] = multiple_choice_responses_2019.Q17_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hosted_Notebook_Products__Google_Colab'] = multiple_choice_responses_2019.Q17_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hosted_Notebook_Products__Azure_Notebooks'] = multiple_choice_responses_2019.Q17_Part_3.apply(lambda x: 0 if x != x else 1) | freeform_responses_2019.Q17_OTHER_TEXT.apply(lambda x: 1 if ('databricks' in str(x).lower()) else 0) \nrelevant_2019['Hosted_Notebook_Products__Google_Cloud_Datalab'] = multiple_choice_responses_2019.Q17_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hosted_Notebook_Products__Paperspace_Gradient'] = multiple_choice_responses_2019.Q17_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hosted_Notebook_Products__FloydHub'] = multiple_choice_responses_2019.Q17_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hosted_Notebook_Products__Binder_JupyterHub'] = multiple_choice_responses_2019.Q17_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hosted_Notebook_Products__IBM_Watson_Studio'] = multiple_choice_responses_2019.Q17_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hosted_Notebook_Products__Code_Ocean'] = multiple_choice_responses_2019.Q17_Part_9.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hosted_Notebook_Products__AWS_Notebook'] = multiple_choice_responses_2019.Q17_Part_10.apply(lambda x: 0 if x != x else 1) | freeform_responses_2019.Q17_OTHER_TEXT.apply(lambda x: 1 if ('sagemaker' in str(x).lower()) else 0) \nrelevant_2019['Hosted_Notebook_Products__Domino_Datalab'] = freeform_responses_2019.Q17_OTHER_TEXT.apply(lambda x: 1 if ('domino' in str(x).lower()) else 0) \nrelevant_2019['Hosted_Notebook_Products__Crestle'] = freeform_responses_2019.Q17_OTHER_TEXT.apply(lambda x: 1 if ('crestle' in str(x).lower()) else 0) \nrelevant_2019['Hosted_Notebook_Products__Cocalc'] = freeform_responses_2019.Q17_OTHER_TEXT.apply(lambda x: 1 if ('cocalc' in str(x).lower()) else 0) \nrelevant_2019['Hosted_Notebook_Products__Datalore'] = freeform_responses_2019.Q17_OTHER_TEXT.apply(lambda x: 1 if ('datalore' in str(x).lower()) else 0) \nrelevant_2019['Hosted_Notebook_Products__Databricks'] = freeform_responses_2019.Q17_OTHER_TEXT.apply(lambda x: 1 if ('brick' in str(x).lower()) else 0) \nrelevant_2019['Hosted_Notebook_Products__None'] = multiple_choice_responses_2019.Q17_Part_11.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hosted_Notebook_Products__Other'] = freeform_responses_2019.Q17_OTHER_TEXT\n\nrelevant_2019['Activities__Analyze_and_understand_data'] = multiple_choice_responses_2019.Q9_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Activities__Build_ML_service'] = multiple_choice_responses_2019.Q9_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Activities__Build_data_infrastructure'] = multiple_choice_responses_2019.Q9_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Activities__Build_prototypes'] = multiple_choice_responses_2019.Q9_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Activities__Build_Experimentation'] = multiple_choice_responses_2019.Q9_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Activities__Research'] = multiple_choice_responses_2019.Q9_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Activities__None'] = multiple_choice_responses_2019.Q9_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Activities__Other'] = freeform_responses_2019.Q9_OTHER_TEXT\n\nrelevant_2019['Algorithms__Linear_or_Logistic_Regression'] = multiple_choice_responses_2019.Q24_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__Decision_Trees_or_Random_Forests'] = multiple_choice_responses_2019.Q24_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__Gradient_Boosting_Machines'] = multiple_choice_responses_2019.Q24_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__Bayesian_Approaches'] = multiple_choice_responses_2019.Q24_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__Evolutionary_Approaches'] = multiple_choice_responses_2019.Q24_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__Dense_Neural_Networks'] = multiple_choice_responses_2019.Q24_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__CNN'] = multiple_choice_responses_2019.Q24_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__GAN'] = multiple_choice_responses_2019.Q24_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__RNN'] = multiple_choice_responses_2019.Q24_Part_9.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__Transformer_Networks'] = multiple_choice_responses_2019.Q24_Part_10.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__None'] = multiple_choice_responses_2019.Q24_Part_11.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithms__Other'] = freeform_responses_2019.Q24_OTHER_TEXT\n\nrelevant_2019['Algorithm_Cluster_Traditional_ML'] = relevant_2019.Algorithms__Linear_or_Logistic_Regression | relevant_2019.Algorithms__Decision_Trees_or_Random_Forests | relevant_2019.Algorithms__Gradient_Boosting_Machines | relevant_2019.Algorithms__Bayesian_Approaches | relevant_2019.Algorithms__Evolutionary_Approaches | relevant_2019.Algorithms__Dense_Neural_Networks\nrelevant_2019['Algorithm_Cluster_Deep_Learning_Vision'] = relevant_2019.Algorithms__CNN | relevant_2019.Algorithms__GAN\nrelevant_2019['Algorithm_Cluster_Deep_Learning_NLP'] = relevant_2019.Algorithms__RNN | relevant_2019.Algorithms__Transformer_Networks\nrelevant_2019['Algorithm_Cluster_Deep_Learning'] = relevant_2019['Algorithm_Cluster_Deep_Learning_Vision'] | relevant_2019['Algorithm_Cluster_Deep_Learning_NLP']\nrelevant_2019['Algorithm_Cluster_Other'] = relevant_2019['Algorithms__Other'].apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Algorithm_Cluster_None'] = ((relevant_2019.Algorithm_Cluster_Traditional_ML == 0) & (relevant_2019.Algorithm_Cluster_Deep_Learning_Vision == 0 ) & (relevant_2019.Algorithm_Cluster_Deep_Learning_NLP == 0) & (relevant_2019.Algorithm_Cluster_Other == 0)).astype(int)\n\n\nrelevant_2019['Hardware__CPU'] = multiple_choice_responses_2019.Q21_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hardware__GPU'] = multiple_choice_responses_2019.Q21_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hardware__TPU'] = multiple_choice_responses_2019.Q21_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hardware__None'] = multiple_choice_responses_2019.Q21_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Hardware__Other'] = freeform_responses_2019.Q21_OTHER_TEXT\n\nrelevant_2019['Framework__TensorFlow'] = multiple_choice_responses_2019.Q28_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Framework__Keras'] = multiple_choice_responses_2019.Q28_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Framework__PyTorch'] = multiple_choice_responses_2019.Q28_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Framework__TensorFlow_Keras'] = relevant_2019['Framework__TensorFlow'] & relevant_2019['Framework__Keras']\nrelevant_2019['Framework__TensorFlow_Keras_PyTorch'] = relevant_2019['Framework__TensorFlow'] | relevant_2019['Framework__Keras'] | relevant_2019['Framework__PyTorch']\n\nrelevant_2019['Cloud_Platform__GCP'] = multiple_choice_responses_2019.Q29_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Cloud_Platform__AWS'] = multiple_choice_responses_2019.Q29_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Cloud_Platform__Azure'] = multiple_choice_responses_2019.Q29_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Cloud_Platform__IBM'] = multiple_choice_responses_2019.Q29_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Cloud_Platform__Alibaba'] = multiple_choice_responses_2019.Q29_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Cloud_Platform__Salesforce'] = multiple_choice_responses_2019.Q29_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Cloud_Platform__Oracle'] = multiple_choice_responses_2019.Q29_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Cloud_Platform__SAP'] = multiple_choice_responses_2019.Q29_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Cloud_Platform__VMWare'] = multiple_choice_responses_2019.Q29_Part_9.apply(lambda x: 0 if x != x else 1) \nrelevant_2019['Cloud_Platform__Red_Hat'] = multiple_choice_responses_2019.Q29_Part_10.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Cloud_Platform__None'] = multiple_choice_responses_2019.Q29_Part_11.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Cloud_Platform__Other'] = freeform_responses_2019.Q29_OTHER_TEXT.apply(lambda x: 0 if x != x else 1)\n\nrelevant_2019['Media__Twitter'] = multiple_choice_responses_2019.Q12_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Media__Newsletters'] = multiple_choice_responses_2019.Q12_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Media__Reddit'] = multiple_choice_responses_2019.Q12_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Media__Kaggle'] = multiple_choice_responses_2019.Q12_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Media__Course_Forums'] = multiple_choice_responses_2019.Q12_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Media__YouTube'] = multiple_choice_responses_2019.Q12_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Media__Podcast'] = multiple_choice_responses_2019.Q12_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Media__Blogs'] = multiple_choice_responses_2019.Q12_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Media__Journal_Publications'] = multiple_choice_responses_2019.Q12_Part_9.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Media__Slack'] = multiple_choice_responses_2019.Q12_Part_10.apply(lambda x: 0 if x != x else 1) \nrelevant_2019['Media__None'] = multiple_choice_responses_2019.Q12_Part_11.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Media__Other'] = multiple_choice_responses_2019.Q12_OTHER_TEXT.apply(lambda x: 0 if x != x else 1)\n\nrelevant_2019['Visual__ggplot2'] = multiple_choice_responses_2019.Q20_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Visual__Matplotlib'] = multiple_choice_responses_2019.Q20_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Visual__Altair'] = multiple_choice_responses_2019.Q20_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Visual__Shiny'] = multiple_choice_responses_2019.Q20_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Visual__D3.js'] = multiple_choice_responses_2019.Q20_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Visual__Plotly'] = multiple_choice_responses_2019.Q20_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Visual__Bokeh'] = multiple_choice_responses_2019.Q20_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Visual__Seaborn'] = multiple_choice_responses_2019.Q20_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Visual__Geoplotlib'] = multiple_choice_responses_2019.Q20_Part_9.apply(lambda x: 0 if x != x else 1)\nrelevant_2019['Visual__Leaflet_Folium'] = multiple_choice_responses_2019.Q20_Part_10.apply(lambda x: 0 if x != x else 1)\n\nrelevant_2019['Year'] = 2019\n\n\n### 2020 ###\nrelevant_2020['Age'] = multiple_choice_responses_2020.Q1\nrelevant_2020['Compensation_Amout'] = multiple_choice_responses_2020.Q24\nrelevant_2020['Compensation_Currency'] = 'USD'\nrelevant_2020['Compensation'] = relevant_2020['Compensation_Amout'].apply(lambda x: unify_compensation_2019(x))\nrelevant_2020['Country'] = multiple_choice_responses_2020.Q3\nrelevant_2020['Occupation'] = multiple_choice_responses_2020.Q5\n#relevant_2020['Occupation_Freeform'] = multiple_choice_responses_2020.Q5_OTHER\nrelevant_2020['Education'] = multiple_choice_responses_2020.Q4\nrelevant_2020['Language_Recommendation'] = multiple_choice_responses_2020.Q8\n#relevant_2020['Language_Recommendation_Freeform'] = multiple_choice_responses_2020.Q8_OTHER\nrelevant_2020['Programming_Experience'] = multiple_choice_responses_2020.Q6\nrelevant_2020['ML_Experience'] = multiple_choice_responses_2020.Q15\n\nrelevant_2020['Hosted_Notebook_Products__Kaggle_Notebooks'] = multiple_choice_responses_2020.Q10_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__Google_Colab'] = multiple_choice_responses_2020.Q10_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__Azure_Notebooks'] = multiple_choice_responses_2020.Q10_Part_3.apply(lambda x: 0 if x != x else 1) | multiple_choice_responses_2020.Q17_OTHER.apply(lambda x: 1 if ('databricks' in str(x).lower()) else 0) \nrelevant_2020['Hosted_Notebook_Products__Google_Cloud_Datalab'] = multiple_choice_responses_2020.Q10_Part_11.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__Paperspace_Gradient'] = multiple_choice_responses_2020.Q10_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__FloydHub'] = multiple_choice_responses_2020.Q10_OTHER.apply(lambda x: 1 if ('floyd' in str(x).lower()) else 0)\nrelevant_2020['Hosted_Notebook_Products__Binder_JupyterHub'] = multiple_choice_responses_2020.Q10_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__IBM_Watson_Studio'] = multiple_choice_responses_2020.Q10_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__Code_Ocean'] = multiple_choice_responses_2020.Q10_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__AWS_Notebook'] = multiple_choice_responses_2020.Q10_Part_9.apply(lambda x: 0 if x != x else 1) | multiple_choice_responses_2020.Q10_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__Domino_Datalab'] = multiple_choice_responses_2020.Q10_OTHER.apply(lambda x: 1 if ('domino' in str(x).lower()) else 0) \nrelevant_2020['Hosted_Notebook_Products__Crestle'] = multiple_choice_responses_2020.Q10_OTHER.apply(lambda x: 1 if ('crestle' in str(x).lower()) else 0) \nrelevant_2020['Hosted_Notebook_Products__Cocalc'] = multiple_choice_responses_2020.Q10_OTHER.apply(lambda x: 1 if ('cocalc' in str(x).lower()) else 0) \nrelevant_2020['Hosted_Notebook_Products__Datalore'] = multiple_choice_responses_2020.Q10_OTHER.apply(lambda x: 1 if ('datalore' in str(x).lower()) else 0)\nrelevant_2020['Hosted_Notebook_Products__Google_Cloud_AI'] = multiple_choice_responses_2020.Q10_Part_10.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__Databricks'] = multiple_choice_responses_2020.Q10_Part_12.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__None'] = multiple_choice_responses_2020.Q10_Part_13.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hosted_Notebook_Products__Other'] = multiple_choice_responses_2020.Q17_OTHER\n\nrelevant_2020['Activities__Analyze_and_understand_data'] = multiple_choice_responses_2020.Q23_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Activities__Build_ML_service'] = multiple_choice_responses_2020.Q23_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Activities__Build_data_infrastructure'] = multiple_choice_responses_2020.Q23_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Activities__Build_prototypes'] = multiple_choice_responses_2020.Q23_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Activities__Build_Experimentation'] = multiple_choice_responses_2020.Q23_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Activities__Research'] = multiple_choice_responses_2020.Q23_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Activities__None'] = multiple_choice_responses_2020.Q23_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Activities__Other'] = multiple_choice_responses_2020.Q23_OTHER\n\nrelevant_2020['Algorithms__Linear_or_Logistic_Regression'] = multiple_choice_responses_2020.Q17_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__Decision_Trees_or_Random_Forests'] = multiple_choice_responses_2020.Q17_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__Gradient_Boosting_Machines'] = multiple_choice_responses_2020.Q17_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__Bayesian_Approaches'] = multiple_choice_responses_2020.Q17_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__Evolutionary_Approaches'] = multiple_choice_responses_2020.Q17_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__Dense_Neural_Networks'] = multiple_choice_responses_2020.Q17_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__CNN'] = multiple_choice_responses_2020.Q17_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__GAN'] = multiple_choice_responses_2020.Q17_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__RNN'] = multiple_choice_responses_2020.Q17_Part_9.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__Transformer_Networks'] = multiple_choice_responses_2020.Q17_Part_10.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__None'] = multiple_choice_responses_2020.Q17_Part_11.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithms__Other'] = multiple_choice_responses_2020.Q17_OTHER\n\nrelevant_2020['Algorithm_Cluster_Traditional_ML'] = relevant_2020.Algorithms__Linear_or_Logistic_Regression | relevant_2020.Algorithms__Decision_Trees_or_Random_Forests | relevant_2020.Algorithms__Gradient_Boosting_Machines | relevant_2020.Algorithms__Bayesian_Approaches | relevant_2020.Algorithms__Evolutionary_Approaches | relevant_2020.Algorithms__Dense_Neural_Networks\nrelevant_2020['Algorithm_Cluster_Deep_Learning_Vision'] = relevant_2020.Algorithms__CNN | relevant_2020.Algorithms__GAN\nrelevant_2020['Algorithm_Cluster_Deep_Learning_NLP'] = relevant_2020.Algorithms__RNN | relevant_2020.Algorithms__Transformer_Networks\nrelevant_2020['Algorithm_Cluster_Deep_Learning'] = relevant_2020['Algorithm_Cluster_Deep_Learning_Vision'] | relevant_2020['Algorithm_Cluster_Deep_Learning_NLP']\nrelevant_2020['Algorithm_Cluster_Other'] = relevant_2020['Algorithms__Other'].apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Algorithm_Cluster_None'] = ((relevant_2020.Algorithm_Cluster_Traditional_ML == 0) & (relevant_2020.Algorithm_Cluster_Deep_Learning_Vision == 0 ) & (relevant_2020.Algorithm_Cluster_Deep_Learning_NLP == 0) & (relevant_2020.Algorithm_Cluster_Other == 0)).astype(int)\n\n\nrelevant_2020['Hardware__CPU'] = multiple_choice_responses_2020.Q12_OTHER.apply(lambda x: 1 if ('cpu' in str(x).lower()) else 0)\nrelevant_2020['Hardware__GPU'] = multiple_choice_responses_2020.Q12_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hardware__TPU'] = multiple_choice_responses_2020.Q12_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hardware__None'] = multiple_choice_responses_2020.Q12_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Hardware__Other'] = multiple_choice_responses_2020.Q12_OTHER\n\nrelevant_2020['Framework__TensorFlow'] = multiple_choice_responses_2020.Q16_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Framework__Keras'] = multiple_choice_responses_2020.Q16_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Framework__PyTorch'] = multiple_choice_responses_2020.Q16_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Framework__TensorFlow_Keras'] = relevant_2020['Framework__TensorFlow'] & relevant_2020['Framework__Keras']\nrelevant_2020['Framework__TensorFlow_Keras_PyTorch'] = relevant_2020['Framework__TensorFlow'] | relevant_2020['Framework__Keras'] | relevant_2020['Framework__PyTorch']\n\nrelevant_2020['Cloud_Platform__GCP'] = multiple_choice_responses_2020.Q26_A_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__AWS'] = multiple_choice_responses_2020.Q26_A_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__Azure'] = multiple_choice_responses_2020.Q26_A_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__IBM'] = multiple_choice_responses_2020.Q26_A_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__Alibaba'] = multiple_choice_responses_2020.Q26_A_Part_9.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__Tencent'] = multiple_choice_responses_2020.Q26_A_Part_10.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__Salesforce'] = multiple_choice_responses_2020.Q26_A_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__Oracle'] = multiple_choice_responses_2020.Q26_A_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__SAP'] = multiple_choice_responses_2020.Q26_A_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__VMWare'] = multiple_choice_responses_2020.Q26_A_Part_8.apply(lambda x: 0 if x != x else 1) \nrelevant_2020['Cloud_Platform__Red_Hat'] = multiple_choice_responses_2020.Q26_A_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__None'] = multiple_choice_responses_2020.Q26_A_Part_11.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Cloud_Platform__Other'] = multiple_choice_responses_2020.Q26_A_OTHER.apply(lambda x: 0 if x != x else 1)\n\n\nrelevant_2020['Media__Twitter'] = multiple_choice_responses_2020.Q39_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Media__Newsletters'] = multiple_choice_responses_2020.Q39_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Media__Reddit'] = multiple_choice_responses_2020.Q39_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Media__Kaggle'] = multiple_choice_responses_2020.Q39_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Media__Course_Forums'] = multiple_choice_responses_2020.Q39_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Media__YouTube'] = multiple_choice_responses_2020.Q39_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Media__Podcast'] = multiple_choice_responses_2020.Q39_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Media__Blogs'] = multiple_choice_responses_2020.Q39_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Media__Journal_Publications'] = multiple_choice_responses_2020.Q39_Part_9.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Media__Slack'] = multiple_choice_responses_2020.Q39_Part_10.apply(lambda x: 0 if x != x else 1) \nrelevant_2020['Media__None'] = multiple_choice_responses_2020.Q39_Part_11.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Media__Other'] = multiple_choice_responses_2020.Q39_OTHER.apply(lambda x: 0 if x != x else 1)\n\nrelevant_2020['Visual__Matplotlib'] = multiple_choice_responses_2020.Q14_Part_1.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Visual__Seaborn'] = multiple_choice_responses_2020.Q14_Part_2.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Visual__Plotly'] = multiple_choice_responses_2020.Q14_Part_3.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Visual__ggplot2'] = multiple_choice_responses_2020.Q14_Part_4.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Visual__Shiny'] = multiple_choice_responses_2020.Q14_Part_5.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Visual__D3.js'] = multiple_choice_responses_2020.Q14_Part_6.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Visual__Altair'] = multiple_choice_responses_2020.Q14_Part_7.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Visual__Bokeh'] = multiple_choice_responses_2020.Q14_Part_8.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Visual__Geoplotlib'] = multiple_choice_responses_2020.Q14_Part_9.apply(lambda x: 0 if x != x else 1)\nrelevant_2020['Visual__Leaflet_Folium'] = multiple_choice_responses_2020.Q14_Part_10.apply(lambda x: 0 if x != x else 1)\n\nrelevant_2020['Year'] = 2020\n\n\nrelevant_2018 = relevant_2018.loc[1:].reset_index(drop=True)\nrelevant_2019 = relevant_2019.loc[1:].reset_index(drop=True)\nrelevant_2020 = relevant_2020.loc[1:].reset_index(drop=True)\n\ndf = relevant_2017\ndf = df.append(relevant_2018)\ndf = df.append(relevant_2019)\ndf = df.append(relevant_2020)\n\ndf = df.reset_index(drop=True)","da4fb5a9":"def categorize_occupations(x):\n    if 'data scien' in x:\n        return 'Data Scientist'\n    elif 'software developer' in x:\n        return 'Software Engineer'\n    elif any(s in x for s in ['student', 'intern']):\n        return 'Student'\n    elif 'research' in x:\n        return 'Research Scientist'\n    elif 'data analy' in x:\n        return 'Data Analyst'\n    elif 'ness analy' in x:\n        return 'Business Analyst'\n    elif any(s in x for s in ['prof', 'teach', 'lect', 'educat', 'faculty', 'academi']):\n        return 'Teacher\/Professor'\n    elif any(s in x for s in ['ngineer', 'enginner']):\n        return 'Other Engineer'\n    elif 'project' in x:\n        return 'Product\/Project Manager'\n    elif any(s in x for s in ['lead', 'manag', 'head', 'direct', 'dircetor']):\n        return 'Manager'\n    elif any(s in x for s in ['chief', 'cto', 'ceo', 'coo','cfo', 'cio', 'cdo']):\n        return 'Chief Officer'\n    elif 'test' in x:\n        return 'Software Tester'\n    elif 'tired' in x:\n        'Not employed'\n    elif any(s in x for s in ['writer', 'journal']):\n        'Data Journalist'\n    else:\n        return 'Other'\n\ndict_occupation = {'Software Developer\/Software Engineer' : 'Software Engineer',\n               'Researcher' : 'Research Scientist',\n               'Scientist\/Researcher' : 'Research Scientist',\n               'Research Assistant' : 'Research Scientist',\n                   \n               'Marketing Analyst' : 'Other',\n                   'Salesperson' : 'Other',\n                   \n                   'Consultant' : 'Other',\n              }\ndf['Occupation'] = df['Occupation'].replace(dict_occupation)\n\ndf['Occupation_Freeform'] = df['Occupation_Freeform'].apply(lambda x: str(x).lower())\ndf['Occupation_Freeform'] = df['Occupation_Freeform'].apply(lambda x: re.sub('\\W+',' ', x))\ndf['Occupation_Freeform'] = df['Occupation_Freeform'].apply(lambda x: re.sub('senior','', x))\ndf['Occupation_Freeform'] = df['Occupation_Freeform'].apply(lambda x: categorize_occupations(x))\ndf['Occupation'] = df.apply(lambda x: x.Occupation_Freeform if x.Occupation == 'Other' else x.Occupation, axis= 1)#.value_counts().to_frame().head(50)\n\ndf.Activities__Other = df.Activities__Other.fillna('NaN')\ndf.Occupation = df.apply(lambda x: 'Student' if (('student' in x.Activities__Other.lower()) & ~('teach' in x.Activities__Other.lower()) & ~('educate' in x.Activities__Other.lower()) & (x.Occupation != 'Student')) else x.Occupation, axis = 1)\n\n#df.Occupation.value_counts()","1ad11222":"temp = df.groupby('Year').agg(Survey_Participants = ('Year', 'count' ),\n                       Algorithm_Cluster_Traditional_ML = ('Algorithm_Cluster_Traditional_ML', 'sum'),\n                       Algorithm_Cluster_Deep_Learning_Vision = ('Algorithm_Cluster_Deep_Learning_Vision', 'sum'),\n                       Algorithm_Cluster_Deep_Learning_NLP = ('Algorithm_Cluster_Deep_Learning_NLP', 'sum'),\n                       Algorithm_Cluster_Deep_Learning = ('Algorithm_Cluster_Deep_Learning', 'sum'),\n                       Algorithms__None = ('Algorithms__None', 'sum'),\n                              Algorithms_Cluster_Other = ('Algorithm_Cluster_Other', 'sum'),\n                      )\n\nfor c in temp.columns:\n    if c != 'Survey_Participants':\n        temp[c] = temp[c] \/ temp['Survey_Participants'] *100\n\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\nax[0].set_title(\"Percentage of Kagglers' \\nUsage of Machine Learning Algorithms\", fontsize=16)\n\ngroup1 = int((df[(df.Year == 2020) & (df.Algorithm_Cluster_Traditional_ML == 0) & (df.Algorithm_Cluster_Deep_Learning != 0)].shape[0] \/ df[(df.Year == 2020)].shape[0])*100)\ngroup2 = int((df[(df.Year == 2020) & (df.Algorithm_Cluster_Traditional_ML != 0) & (df.Algorithm_Cluster_Deep_Learning == 0)].shape[0] \/ df[(df.Year == 2020)].shape[0])*100) \nintersection = int((df[(df.Year == 2020) & (df.Algorithm_Cluster_Traditional_ML != 0) & (df.Algorithm_Cluster_Deep_Learning != 0)].shape[0] \/ df[(df.Year == 2020)].shape[0])*100)\n\nv1 = venn2(subsets = (group1, group2, intersection),\n          set_labels = ( '', '', ''),\n          set_colors=( 'deepskyblue', 'lightgrey'),\n           alpha=1,\n           ax=ax[0])\n\nv1.get_patch_by_id('11').set_color('skyblue')\nc1 = venn2_circles(subsets = (group1, group2, intersection), color='skyblue', ax=ax[0])\nc1[0].set_lw(3.0)\nc1[1].set_lw(0.0)\nax[0].annotate('Deep Learning', xy=v1.get_label_by_id('10').get_position() - np.array([0, -0.05]), xytext=(-70,70),\nha='center', textcoords='offset points', \narrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.5',color='gray'))\n\nax[0].annotate('Traditional Machine Learning\\n (Regression, Decision Trees, \\nGBM, etc.)', xy=v1.get_label_by_id('01').get_position() - np.array([0, -0.05]), xytext=(70,70),\nha='center', textcoords='offset points', \narrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=-0.5',color='gray'))\n\nax[0].annotate('Both', xy=v1.get_label_by_id('11').get_position() - np.array([0, -0.05]), xytext=(0,70),\nha='center', textcoords='offset points',\narrowprops=dict(arrowstyle='->', color='gray'))\n\n\n\nax[1].set_title(\"Percentage of Kagglers' \\nUsage of Deep Learning Algorithms\", fontsize=16)\ngroup1 = int((df[(df.Year == 2020) & (df.Algorithm_Cluster_Deep_Learning_NLP == 0) & (df.Algorithm_Cluster_Deep_Learning_Vision != 0)].shape[0] \/ df[(df.Year == 2020)].shape[0])*100)\ngroup2 = int((df[(df.Year == 2020) & (df.Algorithm_Cluster_Deep_Learning_NLP != 0) & (df.Algorithm_Cluster_Deep_Learning_Vision == 0)].shape[0] \/ df[(df.Year == 2020)].shape[0])*100) \nintersection = int((df[(df.Year == 2020) & (df.Algorithm_Cluster_Deep_Learning_NLP != 0) & (df.Algorithm_Cluster_Deep_Learning_Vision != 0)].shape[0] \/ df[(df.Year == 2020)].shape[0])*100)\n\nv2 = venn2(subsets = (group1, group2, intersection),\n          set_labels = ( '', '', ''),\n          set_colors=( 'lightskyblue', 'skyblue'),ax=ax[1])\nv2.get_patch_by_id('11').set_color('deepskyblue')\nv2.get_patch_by_id('11').set_alpha(1)\n#c2 = venn2_circles(subsets = (group1, group2, intersection),linewidth=3, color='skyblue', ax=ax[1])\n\nax[1].annotate('Computer Vision', xy=v2.get_label_by_id('10').get_position() - np.array([0, -0.05]), xytext=(-70,60),\nha='center', textcoords='offset points', \narrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.5',color='gray'))\n\nax[1].annotate('NLP', xy=v2.get_label_by_id('01').get_position() - np.array([0, -0.05]), xytext=(70,60),\nha='center', textcoords='offset points', \narrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=-0.5',color='gray'))\n\nax[1].annotate('Both', xy=v2.get_label_by_id('11').get_position() - np.array([0, -0.05]), xytext=(0,60),\nha='center', textcoords='offset points',\narrowprops=dict(arrowstyle='->', color='gray'))\n\n\nplt.show()\n\ntemp.columns = ['Participants', 'Traditional ML',\n       'Deep Learning (Vision)',\n       'DeepLearning (NLP)',\n       'Algorithm_Cluster_Deep_Learning', 'Algorithms__None',\n       'Algorithms_Cluster_Other']     \ntemp[['Participants', 'Traditional ML',\n       'Deep Learning (Vision)',\n       'DeepLearning (NLP)']].iloc[2:].style.set_caption('Percentage of Respondents Using Machine Learning Algorithms')\\\n    .format({\"Traditional ML\": \"{:20,.1f}%\", \"Deep Learning (Vision)\": \"{:20,.1f}%\",  \"DeepLearning (NLP)\": \"{:20,.1f}%\"})\\\n    .background_gradient(subset=['Traditional ML', 'Deep Learning (Vision)', 'DeepLearning (NLP)'], cmap='Blues', vmin=0, vmax=62.0)","c74512ab":"kagglers_achievements_df = pd.read_csv(\"..\/input\/meta-kaggle\/UserAchievements.csv\")\nkagglers_achievements_df = kagglers_achievements_df[kagglers_achievements_df.TierAchievementDate.notna()]\nkagglers_achievements_df['TierAchievementYear'] = kagglers_achievements_df['TierAchievementDate'].apply(lambda x: str(x).split('\/')[2])\n\nlabels = ['Expert', 'Master', 'Grandmaster']\ntiers = kagglers_achievements_df[(kagglers_achievements_df.TierAchievementYear == '2020') & (kagglers_achievements_df.Tier > 1)].Tier.value_counts().values\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(6, 4))\nrects = ax.bar(x, tiers, width)\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Number of Kagglers', fontsize=14)\nax.set_ylim([0, 2200])\nax.set_title('Tier Achievements in 2020', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(labels, fontsize=14)\nrects[0].set_color('plum')\nrects[1].set_color('orangered')\nrects[2].set_color('gold')\n\n\n\"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\nfor rect in rects:\n    height = rect.get_height()\n    ax.annotate('{}'.format(height),\n                xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                xytext=(0, 3),  # 3 points vertical offset\n                textcoords=\"offset points\",\n                ha='center', va='bottom')\n\n\n\nfig.tight_layout()\n\nplt.show()","f7319f7d":"hardware_2019_df = df[(df.Year ==2019)][['Algorithm_Cluster_Traditional_ML',\n       'Algorithm_Cluster_Deep_Learning_Vision',\n       'Algorithm_Cluster_Deep_Learning_NLP','Hardware__CPU', 'Hardware__GPU', 'Hardware__TPU',\n       ]]\n\nhardware_2019_df.columns = ['Traditional ML', 'Deep Learning (Vision)', 'Deep Learning (NLP)','CPU', 'GPU', 'TPU']\n\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(14,6))\n\nsns.countplot(multiple_choice_responses_2020.iloc[1:].Q13, order=['Never',  'Once', '2-5 times','6-25 times', 'More than 25 times'], palette='Blues', ax=ax[0])\n\nax[0].set_title('TPU Experience')\nax[0].set_ylabel(ylabel = 'Number of Respondents', fontsize = 12)\nax[0].set_xlabel(xlabel = 'Number of Used Times', fontsize = 12)\n\nhardware_2020_df = df[(df.Year ==2020)][['Algorithm_Cluster_Traditional_ML',\n       'Algorithm_Cluster_Deep_Learning_Vision',\n       'Algorithm_Cluster_Deep_Learning_NLP','Hardware__CPU', 'Hardware__GPU', 'Hardware__TPU',\n       ]]\nhardware_2020_df.columns = ['Traditional ML', 'Deep Learning (Vision)', 'Deep Learning (NLP)','CPU', 'GPU', 'TPU',]\n\nax[1].set_title('TPU Usage in 2020', fontsize=14)\nax[1].bar([2019, 2020], \n          [round(hardware_2019_df[['TPU']].sum(axis=0).values[0] \/ len(hardware_2019_df) * 100, 2), \n           round(hardware_2020_df[['TPU']].sum(axis=0).values[0]\/ len(hardware_2020_df) * 100, 2)])\n\nax[1].set_ylim([0, 5])\nax[1].set_ylabel(ylabel = 'Percentage of Respondents [%]', fontsize = 14)\nax[1].set_xticks([2019, 2020])\nax[1].set_xticklabels(['2019', '2020'])\n\nplt.show()","a9189db6":"advice = quote[quote.new_question == 562]\nprint(\"Sanyam: '\" + advice.interviewer.values[0] + \"'\\n\")\nprint(advice.heroes.values[0] +  \": '\" + advice.guest.values[0])","fbb9d118":"parul_interview_df = pd.read_csv(\"..\/input\/chai-time-data-science\/Cleaned Subtitles\/E49.csv\")\nparul_interview_df[parul_interview_df.Time == '53:40'].Text.values[0]","27f48fcf":"language_recommendation_df = df[df.Language_Recommendation.isin(['Python', 'R', 'SQL', 'Julia'])].groupby('Year').Language_Recommendation.value_counts().to_frame()\nlanguage_recommendation_df.columns = ['dummy']\nlanguage_recommendation_df = language_recommendation_df.reset_index(drop=False)\nlanguage_recommendation_df = language_recommendation_df.pivot(index='Year', columns='Language_Recommendation')['dummy']\nlanguage_recommendation_df['n_responses'] = df.groupby('Year').Year.count()\n\nfor c in language_recommendation_df.columns:\n    if c != 'n_responses':\n        language_recommendation_df[c] = language_recommendation_df[c]\/language_recommendation_df['n_responses'] * 100\nlanguage_recommendation_df = language_recommendation_df.drop('n_responses', axis=1) \nlanguage_recommendation_df = language_recommendation_df.fillna(0)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n\n\nlanguage_recommendation_df[['Python', 'R', 'SQL']].plot(ax=ax, marker='x')\n\nax.set_title('Recommendation for Beginners')\nax.set_ylabel(ylabel = 'Percentage of Respondents [%]', fontsize = 12)\nax.set_xticks([2017, 2018, 2019, 2020])\nax.set_xticklabels(['2017', '2018', '2019', '2020'])\nplt.show()","458cf320":"print(f\"{round(language_recommendation_df.iloc[3].Julia,2)}% of respondents recommend learning Julia for beginners.\")","01722bc8":"advice = quote[quote.new_question == 1115]\nprint(\"Sanyam: '\" + advice.interviewer.values[0] + \"'\\n\")\nprint(advice.heroes.values[0] +  \": '\" + advice.guest.values[0])\n","a253fcd2":"temp = df[(df.Occupation.isin(['Data Scientist', 'Research Scientist']) & (df.Year != 2017))]\ntemp = temp.groupby(['Year', 'Occupation']).agg(Framework__TensorFlow_Keras_PyTorch = ('Framework__TensorFlow_Keras_PyTorch', 'sum' ),\n                                                                   Framework__TensorFlow = ('Framework__TensorFlow', 'sum'),\n                                                                    Framework__Keras = ('Framework__Keras', 'sum'),\n                                                                    Framework__PyTorch = ('Framework__PyTorch', 'sum'),\n                                                                    Framework__TensorFlow_Keras = ('Framework__TensorFlow_Keras', 'sum'))\ntemp['Framework__TensorFlow'] = temp['Framework__TensorFlow'] \/ temp['Framework__TensorFlow_Keras_PyTorch']\ntemp['Framework__Keras'] = temp['Framework__Keras'] \/ temp['Framework__TensorFlow_Keras_PyTorch']\ntemp['Framework__PyTorch'] = temp['Framework__PyTorch'] \/ temp['Framework__TensorFlow_Keras_PyTorch']\ntemp['Framework__TensorFlow_Keras'] = temp['Framework__TensorFlow_Keras'] \/ temp['Framework__TensorFlow_Keras_PyTorch']\n\ntemp = temp.reset_index(drop=False)\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n\n#ax = sns.heatmap(temp, vmin=0, vmax=100,  annot=True,)\nfor i, occ in enumerate(['Data Scientist', 'Research Scientist']):#, 'Student']):\n\n    framework_by_occupation = temp[temp.Occupation==occ][['Year', 'Framework__TensorFlow', 'Framework__Keras', 'Framework__PyTorch']]\n    reg_tensorflow = LinearRegression().fit(np.array(framework_by_occupation.Year).reshape(-1, 1), np.array(framework_by_occupation.Framework__TensorFlow).reshape(-1, 1))\n    reg_keras = LinearRegression().fit(np.array(framework_by_occupation.Year).reshape(-1, 1), np.array(framework_by_occupation.Framework__Keras).reshape(-1, 1))\n    reg_pytorch = LinearRegression().fit(np.array(framework_by_occupation.Year).reshape(-1, 1), np.array(framework_by_occupation.Framework__PyTorch).reshape(-1, 1))\n\n    framework_by_occupation = framework_by_occupation.append({'Year':2021, \n                  'Framework__TensorFlow': reg_tensorflow.predict(np.array([[2021]]))[0,0], \n                  'Framework__Keras': reg_keras.predict(np.array([[2021]]))[0,0], \n                  'Framework__PyTorch': reg_pytorch.predict(np.array([[2021]]))[0,0], }, \n                 ignore_index=True)\n    \n    framework_by_occupation = framework_by_occupation.append({'Year':2022, \n                  'Framework__TensorFlow': reg_tensorflow.predict(np.array([[2022]]))[0,0], \n                  'Framework__Keras': reg_keras.predict(np.array([[2022]]))[0,0], \n                  'Framework__PyTorch': reg_pytorch.predict(np.array([[2022]]))[0,0], }, \n                 ignore_index=True)\n    '''\n    framework_by_occupation = framework_by_occupation.append({'Year':2023, \n                  'Framework__TensorFlow': reg_tensorflow.predict(np.array([[2023]]))[0,0], \n                  'Framework__Keras': reg_keras.predict(np.array([[2023]]))[0,0], \n                  'Framework__PyTorch': reg_pytorch.predict(np.array([[2023]]))[0,0], }, \n                 ignore_index=True)\n    \n    framework_by_occupation = framework_by_occupation.append({'Year':2024, \n                  'Framework__TensorFlow': reg_tensorflow.predict(np.array([[2024]]))[0,0], \n                  'Framework__Keras': reg_keras.predict(np.array([[2024]]))[0,0], \n                  'Framework__PyTorch': reg_pytorch.predict(np.array([[2024]]))[0,0], }, \n                 ignore_index=True)'''\n    #* Among **students** TensorFlow\/Keras is still more popular than PyTorch. However, it looks like Keras is gaining popularity due to its low threshold to getting started. However, PyTorch might replace TensorFlow in teaching students concepts.\n    \n    framework_by_occupation.set_index('Year').plot(ax=ax[i], marker='x')\n\n    \n    ax[i].set_ylim([0,1])\n    ax[i].set_ylabel(ylabel = 'Percentage of Respondents Used by [0.01%]', fontsize = 14)\n    ax[i].set_xlabel(xlabel = 'Year', fontsize = 14)\n    ax[i].set_xticks([2018, 2019, 2020, 2021, 2022,])# 2023, 2024])\n    ax[i].add_patch(Rectangle((2020.5, 0), 4, 1, fill=True, alpha=0.2, color='Grey', lw=0))\n    ax[i].annotate('Prediction', xy=(2020.8, 0.9), fontsize=14, color='black')\n    ax[i].set_title(occ, fontsize = 16)","0ab4350f":"advice = quote[quote.new_question == 78]\n\nprint(\"Sanyam: '\" + advice.interviewer.values[0] + \"'\\n\")\nprint(advice.heroes.values[0] +  \": '\" + advice.guest.values[0] + \"'\")","e61c8fda":"temp = df[(df.Occupation.isin(['Data Scientist', 'Research Scientist']) & (df.Algorithm_Cluster_Deep_Learning == 1) & (df.Year != 2017))]\n\nnlp = temp[temp.Algorithm_Cluster_Deep_Learning_NLP == 1]\nnlp = nlp[['Framework__PyTorch', 'Framework__TensorFlow_Keras']].sum()# \/ len(nlp) *100\n\nvision = temp[temp.Algorithm_Cluster_Deep_Learning_Vision == 1]\nvision = vision[['Framework__PyTorch', 'Framework__TensorFlow_Keras']].sum()# \/ len(vision) * 100\n\ndeep_learning_framework = pd.concat([nlp, vision], axis=1)\ndeep_learning_framework.columns = ['NLP', 'Vision']\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n\ndeep_learning_framework.plot(kind='bar', ax=ax, color=['dodgerblue', 'skyblue'])\n\nax.set_title('Framework Popularity among Deep Learning Practitioners ', fontsize = 16)\nax.set_ylabel(ylabel = 'Respondents', fontsize = 14)\nax.set_xticklabels(labels = ['PyTorch', 'TensorFlow\/Keras'], rotation=0, fontsize = 14)\n\nplt.show()","50b58bd9":"visualization_cols = df.columns[df.columns.str.contains('Visu')]\n\nall_df = df[(df.Year == 2020) ].groupby('Year')[visualization_cols].sum()\n\nall_df['respondents'] = df[(df.Year == 2020)].Year.value_counts()\n\nfor c in all_df.columns:\n    all_df[c] = all_df[c] \/ all_df['respondents'] * 100\n\nall_df = all_df.drop('respondents', axis=1)\nall_df.columns = [re.sub('_', ' ', c.split('__')[1]) for c in all_df.columns]\nall_df['label'] = 'All Respondents'\nexperienced_df = df[(df.Year == 2020) & df.ML_Experience.isin(['5-10 years',  '10-20 years','20 or more years'])].groupby('Year')[visualization_cols].sum()\n\nexperienced_df['respondents'] = df[(df.Year == 2020) & df.ML_Experience.isin(['5-10 years',  '10-20 years','20 or more years'])].Year.value_counts()\n\nfor c in experienced_df.columns:\n    experienced_df[c] = experienced_df[c] \/ experienced_df['respondents'] * 100\n\nexperienced_df = experienced_df.drop('respondents', axis=1)\nexperienced_df.columns = [re.sub('_', ' ', c.split('__')[1]) for c in experienced_df.columns]\nexperienced_df['label'] = 'Experienced Respondents'\ntemp = all_df.append(experienced_df)\ntemp =temp.set_index('label')\n#temp = temp.reset_index(drop=False)\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n\ntemp[['Matplotlib', 'Seaborn','Plotly', 'Bokeh', 'Geoplotlib', 'Leaflet Folium', 'Altair',]].T.plot(kind='bar', ax=ax[0], color=['lightblue', 'deepskyblue', 'steelblue'])\nax[0].set_title('Usage of Visualisation Libraries for Python', fontsize = 14)\nax[0].set_ylabel(ylabel = 'Percentage of Respondents [%]', fontsize = 12)\nax[0].set_ylim([0,80])\n\ntemp[['ggplot2', 'Shiny']].T.plot(kind='bar', ax=ax[1], color=['lightblue', 'deepskyblue', 'steelblue'])\nax[1].set_title('Usage of Visualisation Libraries for R', fontsize = 14)\nax[1].set_ylabel(ylabel = 'Percentage of Respondents [%]', fontsize = 12)\nax[1].set_ylim([0,80])\n#plt.xticks(rotation=0, fontsize=12)\n\nplt.show()","29673972":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 4))\n\nsns.heatmap(hardware_2019_df.corr()[['CPU', 'GPU', 'TPU']].iloc[:3], vmin=0, vmax=1, annot=True, cmap='Blues', ax= ax)\nax.set_title('Correlation of Algorithm and Hardware Usage in 2019', fontsize=14)\n\n\nplt.show()\n","c9aa5e05":"advice = quote[quote.new_question == 385]\n\nprint(\"Sanyam: '\" + advice.interviewer.values[0] + \"'\\n\")\nprint(advice.heroes.values[0] +  \": '\" + advice.guest.values[0] + \"'\")","a0c7a88b":"notebook_cols = df.columns[df.columns.str.startswith('Hosted_Notebook')]\ncloud_cols = df.columns[df.columns.str.startswith('Cloud_Platform')]\nalternatives = df[(df.Hardware__GPU == 1) & (df.Year == 2020)][['Occupation',  'Hosted_Notebook_Products__Google_Colab', 'Hosted_Notebook_Products__Kaggle_Notebooks','Hosted_Notebook_Products__Binder_JupyterHub', 'Hosted_Notebook_Products__None']]\nalternatives['Student_status'] = alternatives.Occupation.apply(lambda x: 1 if x == 'Student' else 0)\ntemp = alternatives.groupby(['Student_status']).sum()\ntemp['respondents'] = alternatives.Student_status.value_counts()\n\nfor c in temp.columns:\n    temp[c] = temp[c] \/ temp['respondents'] * 100\n\ntemp = temp.drop('respondents', axis=1)\ntemp.index = ['Not Student', 'Student']\ntemp.columns = [re.sub('_', ' ', c.split('__')[1]) for c in temp.columns]\n    \nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6))\n\ntemp.T.plot(kind='bar', ax=ax, color=['deepskyblue', 'steelblue'])\nax.set_title('Usage of Hosted Notebook Products', fontsize = 14)\nax.set_ylabel(ylabel = 'Percentage of Respondents [%]', fontsize = 12)\nplt.xticks(rotation=0, fontsize=12)\nplt.show()\n","f5b80b71":"advice = quote[quote.new_question == 93]\nprint(\"Sanyam: '\" + advice.interviewer.values[0] + \"'\\n\")\nprint(advice.heroes.values[0] +  \": '\" + advice.guest.values[0])\n\nadvice = quote[quote.new_question == 516]\nprint(\"Sanyam: '\" + advice.interviewer.values[0] + \"'\\n\")\nprint(advice.heroes.values[0] +  \": '\" + advice.guest.values[0])","c59a58a4":"freeform_responses_2019.iloc[16716]","df0e4095":"\"\"\"\n## Any Media Recommendations?\n\nWhere do Kagglers get the latest news on data science topic? Let's rank the three most popular media sources for each year this question was asked.\n\"\"\"\n\nmedia_df = df[['Year','Media__Twitter', 'Media__Newsletters', 'Media__Reddit',\n       'Media__Kaggle', 'Media__Course_Forums', 'Media__YouTube',\n       'Media__Podcast', 'Media__Blogs', 'Media__Journal_Publications',\n       'Media__Slack', 'Media__None']].groupby('Year').sum()\n\nmedia_df['n_respondents'] = df.Year.value_counts()\n\nfor c in media_df.columns:\n    if c != 'n_respondents':\n        media_df[c] = media_df[c]\/media_df['n_respondents'] * 100\nmedia_df = media_df.drop('n_respondents', axis=1) \n\nmedia_df = media_df.T.reset_index(drop=False)\nmedia_df.columns=['Media', 2017, 2018, 2019, 2020]\n\nmedia_dict = {}\nfor m in ['Media__Twitter', 'Media__Newsletters', 'Media__Reddit',\n       'Media__Kaggle', 'Media__Course_Forums', 'Media__YouTube',\n       'Media__Podcast', 'Media__Blogs', 'Media__Journal_Publications',\n       'Media__Slack', 'Media__None']:\n    media_dict[m] = m.split('__')[1]\nmedia_df.Media = media_df.Media.replace(media_dict)\n\nmedia_ranking_df = pd.DataFrame(columns=[2018, 2019, 2020])\nfor year in [2018, 2019, 2020]:\n    media_ranking_df[year] = media_df.sort_values(by=year, ascending=False).Media[:3].reset_index(drop=True)\n\n    \ndef color_df(val):\n    if val == 'Kaggle':\n        return 'background-color: deepskyblue'\n    elif val == 'YouTube':\n        return 'background-color: tomato'\n    elif val == 'Blogs':\n        return 'background-color: steelblue'\n\nmedia_ranking_df.style.applymap(color_df).set_caption('Most Popular Media Sources')\n\n\"\"\"\n**Findings**:\n* Kagglers get their news on Kaggle - duh!\n* Video content is becoming more popular than text content\n\nThis year, there has been some great content on YouTube by a few Kagglers. Kaggle's first 4x Grandmaster [Abhishek](https:\/\/www.kaggle.com\/abhishek) has been posting great tutorials and interviews on [his YouTube channel](https:\/\/www.youtube.com\/user\/abhisheksvnit) he **started this year**. \n\nAlso, [Sanyam](https:\/\/www.kaggle.com\/init27) has been uploading the [Chai Time Data Science Show](https:\/\/www.youtube.com\/c\/ChaiTimeDataScience), which is a podcast where he interviews researchers, Kagglers, and data science practitioners, **twice a week** this year. There even was a fun **analytics challenge** based on the [dataset](https:\/\/www.kaggle.com\/rohanrao\/chai-time-data-science\/notebooks) from the interviews.\n\nAnother cool podcast by fellow Kagglers [currypurin](https:\/\/www.kaggle.com\/currypurin) and [regonn](https:\/\/www.kaggle.com\/regonn) I enjoy is [regonn&curry.fm](https:\/\/open.spotify.com\/show\/4qRP8siOYH92k3oMSxUG9U?si=yUHWVrk-Sp2f9lV_vnJmOA) , where they **discuss the latest Kaggle challenges**. Unfortunately, it is in Japanese only but if you speak Japanese, I definetely recommend it.\n\n**Thank you guys for sharing your high quality content with us! I have learned a lot!**\n\nSince we are on the topic of podcast, I was surprised that **only 7.5% Kagglers listen to data science related podcasts**.\n\n\"\"\"\n\nprint(f\"Only {round(media_df[media_df.Media == 'Podcast'][2020].values[0],1)}% of Kagglers listen to data science related Podcasts.\")","f7a93069":"## Understand the Importance of EDA and Visualization\nExploratory Data Analysis (EDA) and Visualization are an integral part of the data science workflow. On a side note: If you don't want to start building models directly, you could also start by digging into the data sets and creating EDAs. Let's see which tools there are to help you along:\nIf you want to visualize your data, then you might want to pick one of the below depending on the programming language you chose earlier.\n* R: ggplot, Shiny, \n* Python: Matplotlib, Altair, Plotly, Python, Seaborn, Geoplotlib, Leaflet \/ Folium\n\nLet's see if we can see anything that highly experienced Kagglers are doing differently.","c724dd52":"**Python has been the most recommended programming language** for data science newbies since the first survey was launched in 2017. This year about 70% of respondents recommend Python while in 2017 less than half of respondents were recommending it. The popularity of R has been decreasing slowly over the past four years. Although it is not a classical programming language, **more and more Kagglers are now recommending that data science newbies should also know SQL**.\n\nTo conclude, **pick Python or R. Both are good choices.** However, Python is definitely more popular. Additionally, if you want to get a job as a data scientist, then you probably should know how to write SQL queries.\n\nOne a side note: Some of you may have noticed that Kaggle kernels also now give you the option of **Julia scripts**. Although [Julia](https:\/\/julialang.org\/) has been around since 2012, this year I have seen Julia being titled the [new kid on the block for data science](https:\/\/towardsdatascience.com\/introducing-julia-an-alternative-to-python-and-r-for-data-science-dcbf98346253). However, none of the respondents were recommending it for beginners in the past two years. This year, only **0.6% of respondents recommended Julia** for data science newbies.","ee808571":"We can see that 62% of Kagglers regularly use traditional Machine Learning algorithms such as linear regression, decision trees, etc. 34% of Kagglers regularly use Deep Learning algorithms out of which **29% use it in the vision** domain and **19% use it in the language** domain. Furthermore,  31% of Kagglers regularly use both traditional Machine Learning and Deep Learning algorithms. Most Kagglers using Deep Learning algorithms also use traditional machine learning algorithms\n\nAdditionally, the **percentage of Kagglers using Deep Learning algorithms has increased by 1.6% from 2019 to 2020.** I believe this is because Kaggle is constantly pushing us with a lot of interesing challenges. About one year ago, I learned how to use Deep Learning algorithms for the first time on Kaggle. Therefore, I want to say **thank you to all the Kagglers sharing their work and teaching each other and thank you to Kaggle for providing us with new challenges!**\n\n## Kagglers' Tier Achievements in 2020\nWhile we are on the topic of **Kagglers levelling up**, let's have quick look at their achievements in 2020.\n54 Kagglers reached the Grandmaster tier this year. This is a lot of hard and consistent work. Good job, guys!","d9235aaa":"# Characteristics of a Kaggler\nWhile Kaggle - the platform - is a platform for data science competitions, Kaggle - the community - is a community about learning and sharing knowledge about data science. On a very shallow analysis level of what type of person a typical Kaggler is, we would probably get some shallow information like: male, mid twenties, college educated. However, if we look deeper, we can see that **Kagglers are curious and hardworking individuals, ready for a challenge, and open to learn new things**.\n\nLet me show you why I think the survey data indicates this:\n\n## Kagglers are Levelling Up in Deep Learning\nThis year, we have seen a large selection of different competitions on Kaggle. \nMy feeling is that **competitions with tabular data might be more popular** among Kagglers. I think this is because **we are all here to learn** and competitions with tabular data are especially beginner friendly because you can apply traditional machine learning algorithms, which might be easier to apply for beginners.\n\nLet's explore what the skill set of Kagglers contains in regards to machine learning algorithms:","92af0ed9":"## Pick A Programming Language\nThe data science Venn diagram cosists of programming, math, and domain knowledge. All three are important to practice data science. Therefore, [Parul Pandey](https:\/\/www.kaggle.com\/parulpandey) stresses that you should get a good foundation in coding. But what is the best programming language to get started?\n\n> \"[...] [M]ake sure you have a solid background and foundation. I especially say stats and coding - **don't run away from coding**. And coding is such an essential tool kit even  if you're not a data scientist [...] And I think you should really, really focus on coding and you should because if you're good at coding, data science also becomes easier. [...] \" -  [Parul Pandey](https:\/\/www.kaggle.com\/parulpandey)$^*$","dc5ac1f8":"In below figure we can see that Python is the dominant choice of programming language among experienced data scientists.","e102dce7":"# Trends in 2020 with Advice from Top Kagglers\n\nIn this notebook I want to explore **what the current trends in Data Science are** and **what we can learn from experienced Machine Learning practitioners**. For this, I will also incorporate the [Meta-Kaggle](https:\/\/www.kaggle.com\/kaggle\/meta-kaggle) dataset and the [Chai Time Data Science](https:\/\/www.kaggle.com\/rohanrao\/chai-time-data-science) dataset, which contains the transcripts of the [CTDS Podcast](https:\/\/chaitimedatascience.com\/).\n\nThis year has been eventful to say the least. Let's explore the Kaggle survey data and find out what the current trends in Data Science are:\n* Did **PyTorch** exceed **TensorFlow\/Keras** in popularity?\n* **TPUs** are now available in Kaggle kernels but how popular are they?","f7e13ac8":"# Tips for Contributors\nSo, now you that you have entered your first competition. Where do you go from here? Once you feel comfortable with traditional machine learning techniques, you can start expanding your knowledge in another field such as Deep Learning or time series.  \n\n## PyTorch or TensorFlow\/Keras?\nOver the past year, I have seen this questions a few times in the discussion sections of competitions: \n> \"I am new to Deep Learning. Should I start with PyTorch or TensorFlow\/Keras?\"\n\nThis question might even be more frequently asked than whether you should learn Python or R. \nAlthough both have their advantages and disadvantages, in my opinion both frameworks strongly benefit from this healthy competition and are even moving in the direction of strong cooperation by [enabling conversion](https:\/\/www.youtube.com\/watch?v=nngMcB7LKzE) between the two.\nHowever, from what I have seen in Kaggle kernels the past year, **my gut feeling says that PyTorch is about to pass TensorFlow\/Keras in popularity**. Let's see if that is the case.","e7b7b1f2":"# Sorry for the long post\nhere is somebody that has answered all freeform answers with 'potato'.","43406491":"## Learning New Things\n\nThis year Kaggle has enabled [TPUs on Kaggle kernels](https:\/\/www.youtube.com\/watch?v=1pdwRQ1DQfY) and launched a few challenges for us to get familiar with TPUs. But how many of us have played around with them?\n\nAlthough Kaggle kernels have TPUs available and there were a few challenges to learn about TPUs on Kaggle, the majority of Kagglers have never used a TPU. Even Kagglers that have used TPUs before have only used them a few times. **However, the percentages of Kagglers that have used a TPU before has doubled**. That is why I think Kagglers are open to new challenges and learning new things. ","9308aa24":"## Learn Fundamental Machine Learning Theory\nFrom above quotes from [CPMP](https:\/\/www.kaggle.com\/cpmpml) and [Parul Pandey](https:\/\/www.kaggle.com\/parulpandey), we already see that getting some fundamental Machine Learning and Statistics knowledge is important.\n\n**(TODO: What online courses? What level of education? )**","0b293a7a":"From the above plots, we can see that my gut feeling was wrong but also not completely off. In the **industry**, TensorFlow\/Keras is still leading. However, this could change in two years. In the **academia**, PyTorch is about to pass TensorFlow\/Keras in its popularity. Looking forward to see if that is the case next year.  Overall it looks like PyTorch is going to become more popular than TensorFlow\/Keras in the next few years with academia leading the way and industry following close behind.\n\n> \"I've worked quite a lot on **TensorFlow and Keras** in the last two years. So I'm very comfortable with it [...] but **only for natural language processing** problems. **When it comes to images, I like PyTorch more** becase, first of all, when you start with an image competition  you can build a small simple convolutional neural net and see how it's performing. Then what's the next thing you are going to do? You're going to take some model from some pre-train model, right? Say ResNet or something like that and try to find the ImageNet weights weren't. Yeah, and then fine-tune. But it's difficult in Tensorflow or Keras, because they don't have a proper a model zoo where you can find these weights for different kinds of models. And with PyTorch people seem to be quite fast, and doing these kind of things. So you have a very extensive zoo of models that you can choose from. And you also have the weights available. So you don't have to train it from scratch on ImageNet.' - [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek)$^*$\n","29894249":"## Enter Your First Competition\n\nNow it is time to get some practice. Kaggle competitions are a great way to get started. Pick a competition that interests you. If you feel intimidated by live competitions, then you can always pick a Playground or Getting Started competition. But to be honest, don't be intimidated. Start with a simply dummy submission and then build up your solution from there. Read through other peoples notebooks and try to include hints from the discussions into your solution. \n>\"[...] But, you know, I would really say get an account go sign in and you know, go go put a submission in you know, make it all zeros make it the average, you know, and just just improve. It's a it's an iterative process to almost all of us. [...] And most people will have better results if they can chip away at that problem in bite sized chunks. So the first thing is get the data in, **go submit something really basic and then just iterate from there** and see where it takes you.\" - [Dieter](kaggle.com\/christofhenkel)$^*$","e3f315d0":"**CPUs** are sufficient for the usage of **traditional Machine Learning** algorithms. The hardware of choice for **Deep Learning** algorithms seems to be **GPUs**. Although only 4% of respondents use TPUs, **respondents using TPUs have doubled from 2019 to 2020**.\n\nWe can see that if you want to train a Deep Learning algorithm, GPUs are the most popular choice right now. However, GPUs are quite costly. An alternative is using hosted notebook products. For example, Kaggle kernels enable you to use [more than 30 hours of GPU time per week](https:\/\/news.developer.nvidia.com\/how-kaggle-makes-gpus-accessible-to-5-million-data-scientists\/).","7aa6820f":"Since we are all here to learn, let's explore what we can learn from Kagglers that are more experienced in machine learning, professional data scientists and researchers in the field. In the following, we will explore tips from top Kagglers for people ranging from absolute beginners to experienced data science practitioners. For fun reasons, I have used the Kaggler tiers to denote the experience level. \n\n# Tips for Novices\n\nIn my opinion, [CPMP](https:\/\/www.kaggle.com\/cpmpml) gives one of the most comprehensive advice for newbies on the [CTDS Podcast](https:\/\/chaitimedatascience.com\/). The TL;DR version is: learn to code, learn some Machine Learning basics, and then get your hands dirty in a Kaggle competition. In the following section, we will explore this a little bit more.\n\n> \"So first practice Python. It's not the only language you can use. You can use R. [...] Take some courses. So I like Andrew Ng's Machine Learning course or the Stanford ML course [...] [D]on't start with deep learning unless all you care about is images or natural language processing [...] and then enter a Kaggle 'Playground' or 'Getting Started' competition [...]\" - [CPMP](https:\/\/www.kaggle.com\/cpmpml)$^*$\n\n$^*$ Please unhide output for full quote.\n","20d993d1":"## How Much Computing Power Do I Need?\n\nTraining Machine Learning algorithms can be compute expensive. Large neural networks take ages to train on CPUs alone. This can be much faster if you use GPUs instead. \nAdditionally, there are so-called Tensor Processing Units (TPUs) which are especially desgined to lift heavy computes for Deep Learning algorithms. \n\nLet's see how the usage of certain algorithms correlates to the usage of certain hardware.","82773f7c":"For Python the most popular libraries are matplotlib, seaborn, and plotly. For R ggplot2 is more popular than Shiny. **Overall Kagglers with more than 5 years of experience in Machine Learning have a higher response percentage** than the average Kaggler. From the last point I think we can gain some great insight: **Visualizing your data is an integral part of the Data Science workflow**. So, pick a few libraries and start visualizing some data! ","58ea82e5":"# Tips for Experts\nSo, now that you have levelled up and gained quite a bit of knowledge, how do you become truly good at this? The key is to be persistent and continuously keep learning. But what if you have all that basics down? A good way to keep learning is to read up on how competition winners approached the problem. Usually, it will be posted in the discussion sections after a competition.\n>\"[...] So I think you should be **persistent**. So even if you didn't perform well, **read how the winner approached the problem.**\" - [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek)$^*$\n\n> \"[...] I think the other thing still is that we live in an information age and there is so much information out there and not making use of it is stupid. So you should also always try to **read on the solutions of previous competitions**. You should also always try  to figure out when a hint on the discussion forums is important because I think that's a very crucial part that a lot of top characters can figure out quite quickly, like someone posting something about, oh, this feature I have found that this feature is important that maybe people should get into it and most kind of ignore that because and top guy just kind of can make the connection between that and what what they should do. [...]\" - [Psi](https:\/\/www.kaggle.com\/philippsinger)$^*$","e24a8454":"The most popular hosted notebook products among Kagglers are Google Colab, Kaggle Notebooks and Binder JupyterHub. **Students tend to use hosted notebook products more** than not students."}}