{"cell_type":{"bd8a46a9":"code","fd961e89":"code","b34c03eb":"code","ee28a1a9":"code","d8590001":"code","98f6c77f":"code","2fb9ef65":"code","b9a16dc0":"code","b1f5853c":"code","c850e72c":"code","51e79bfd":"code","353a6ced":"code","11bff0a5":"code","8de10296":"code","d1b4a422":"code","018612f0":"code","5d2bd047":"code","91add6a6":"code","6a30dbb3":"code","0bf6d1f2":"markdown","1c1a79f8":"markdown","39205074":"markdown","3092fe7b":"markdown","61602844":"markdown","b0c2400c":"markdown","6fc1526d":"markdown","b76b4745":"markdown","4c3b7a42":"markdown"},"source":{"bd8a46a9":"# import relevant modules\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn\nimport imblearn\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Settings\npd.set_option('display.max_columns', None)\nnp.set_printoptions(threshold=np.nan)\nnp.set_printoptions(precision=3)\nsns.set(style=\"darkgrid\")\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","fd961e89":"train = pd.read_csv(\"..\/input\/Train_data.csv\")\ntest = pd.read_csv(\"..\/input\/Test_data.csv\")","b34c03eb":"print(train.head(4))\n\nprint(\"Training data has {} rows & {} columns\".format(train.shape[0],train.shape[1]))","ee28a1a9":"print(test.head(4))\n\nprint(\"Testing data has {} rows & {} columns\".format(test.shape[0],test.shape[1]))","d8590001":"# Descriptive statistics\ntrain.describe()","98f6c77f":"print(train['num_outbound_cmds'].value_counts())\nprint(test['num_outbound_cmds'].value_counts())","2fb9ef65":"#'num_outbound_cmds' is a redundant column so remove it from both train & test datasets\ntrain.drop(['num_outbound_cmds'], axis=1, inplace=True)\ntest.drop(['num_outbound_cmds'], axis=1, inplace=True)","b9a16dc0":"# Attack Class Distribution\ntrain['class'].value_counts()","b1f5853c":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# extract numerical attributes and scale it to have zero mean and unit variance  \ncols = train.select_dtypes(include=['float64','int64']).columns\nsc_train = scaler.fit_transform(train.select_dtypes(include=['float64','int64']))\nsc_test = scaler.fit_transform(test.select_dtypes(include=['float64','int64']))\n\n# turn the result back to a dataframe\nsc_traindf = pd.DataFrame(sc_train, columns = cols)\nsc_testdf = pd.DataFrame(sc_test, columns = cols)","c850e72c":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n\n# extract categorical attributes from both training and test sets \ncattrain = train.select_dtypes(include=['object']).copy()\ncattest = test.select_dtypes(include=['object']).copy()\n\n# encode the categorical attributes\ntraincat = cattrain.apply(encoder.fit_transform)\ntestcat = cattest.apply(encoder.fit_transform)\n\n# separate target column from encoded data \nenctrain = traincat.drop(['class'], axis=1)\ncat_Ytrain = traincat[['class']].copy()\n","51e79bfd":"train_x = pd.concat([sc_traindf,enctrain],axis=1)\ntrain_y = train['class']\ntrain_x.shape","353a6ced":"test_df = pd.concat([sc_testdf,testcat],axis=1)\ntest_df.shape","11bff0a5":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier();\n\n# fit random forest classifier on the training set\nrfc.fit(train_x, train_y);\n# extract important features\nscore = np.round(rfc.feature_importances_,3)\nimportances = pd.DataFrame({'feature':train_x.columns,'importance':score})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\n# plot importances\nplt.rcParams['figure.figsize'] = (11, 4)\nimportances.plot.bar();","8de10296":"from sklearn.feature_selection import RFE\nimport itertools\nrfc = RandomForestClassifier()\n\n# create the RFE model and select 10 attributes\nrfe = RFE(rfc, n_features_to_select=15)\nrfe = rfe.fit(train_x, train_y)\n\n# summarize the selection of the attributes\nfeature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), train_x.columns)]\nselected_features = [v for i, v in feature_map if i==True]\n\nselected_features","d1b4a422":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(train_x,train_y,train_size=0.70, random_state=2)","018612f0":"from sklearn.svm import SVC \nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Train KNeighborsClassifier Model\nKNN_Classifier = KNeighborsClassifier(n_jobs=-1)\nKNN_Classifier.fit(X_train, Y_train); \n\n# Train LogisticRegression Model\nLGR_Classifier = LogisticRegression(n_jobs=-1, random_state=0)\nLGR_Classifier.fit(X_train, Y_train);\n\n# Train Gaussian Naive Baye Model\nBNB_Classifier = BernoulliNB()\nBNB_Classifier.fit(X_train, Y_train)\n            \n# Train Decision Tree Model\nDTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\nDTC_Classifier.fit(X_train, Y_train)","5d2bd047":"from sklearn import metrics\n\nmodels = []\nmodels.append(('Naive Baye Classifier', BNB_Classifier))\nmodels.append(('Decision Tree Classifier', DTC_Classifier))\nmodels.append(('KNeighborsClassifier', KNN_Classifier))\nmodels.append(('LogisticRegression', LGR_Classifier))\n\nfor i, v in models:\n    scores = cross_val_score(v, X_train, Y_train, cv=10)\n    accuracy = metrics.accuracy_score(Y_train, v.predict(X_train))\n    confusion_matrix = metrics.confusion_matrix(Y_train, v.predict(X_train))\n    classification = metrics.classification_report(Y_train, v.predict(X_train))\n    print()\n    print('============================== {} Model Evaluation =============================='.format(i))\n    print()\n    print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()","91add6a6":"for i, v in models:\n    accuracy = metrics.accuracy_score(Y_test, v.predict(X_test))\n    confusion_matrix = metrics.confusion_matrix(Y_test, v.predict(X_test))\n    classification = metrics.classification_report(Y_test, v.predict(X_test))\n    print()\n    print('============================== {} Model Test Results =============================='.format(i))\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()        \n","6a30dbb3":"# PREDICTING FOR TEST DATA using KNN\npred_knn = KNN_Classifier.predict(test_df)\npred_NB = BNB_Classifier.predict(test_df)\npred_log = LGR_Classifier.predict(test_df)\npred_dt = DTC_Classifier.predict(test_df)","0bf6d1f2":"# EVALUATE MODELS","1c1a79f8":"# SCALING NUMERICAL ATTRIBUTES","39205074":"# VALIDATING MODELS","3092fe7b":"# FITTING MODELS","61602844":"# ENCODING CATEGORICAL ATTRIBUTES","b0c2400c":"# EXPLORATORY ANALYSIS","6fc1526d":"# DATASET PARTITION","b76b4745":"# FEATURE SELECTION","4c3b7a42":"# LOAD DATA"}}