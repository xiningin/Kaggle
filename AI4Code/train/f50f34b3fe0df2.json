{"cell_type":{"50f30ac4":"code","2646c7ea":"code","3675d56b":"code","5ea7c93e":"code","d31a8649":"code","17da524f":"code","98f5401a":"code","710b5594":"code","c9814c2e":"code","8b55e245":"code","66a4051a":"code","7aa97e2c":"code","366a5366":"code","8d7cbbf6":"code","792fa529":"code","e1d636b7":"code","83f45c16":"code","f7edd1ca":"code","102acf13":"code","1cfed9f8":"code","da3ba7cc":"code","c3aa4560":"code","ab76fccf":"code","8807bd29":"code","fc6d8040":"code","c81ef087":"code","dc6f5241":"code","56d93e9f":"code","97b63fe3":"code","4736f7a0":"code","96ed354b":"code","6dde930e":"code","817ed85d":"code","35586e5a":"code","894867fe":"code","e236424c":"code","669a9f1e":"code","638ed1c4":"code","f28a799d":"code","e50e12cb":"code","6f4c78b6":"code","0496b98a":"code","0e0168e2":"code","0da753ae":"code","28f95a1f":"code","022a3ff3":"code","cf8dd8b2":"code","bf9ce301":"code","dc819396":"code","f2853754":"code","4dc4e227":"code","1ab31be2":"code","d9f768fa":"code","c49744d9":"code","2d8a8275":"code","8740d45b":"code","53e87075":"code","5b93e1f6":"code","84363683":"code","06df4cfb":"code","72a3eb3d":"code","dfcf8f7c":"code","95a28b70":"code","6e9d9546":"code","391f1de0":"code","6e3389e2":"code","5f349d5e":"code","bf72aa22":"code","20a4c775":"code","6b5321c9":"code","da2e0c6a":"code","2094ba30":"code","def127c9":"code","69510034":"code","9199c559":"code","8d6c7274":"code","e86106f7":"code","6e047357":"code","ad43d83c":"markdown","7da62b00":"markdown","b9d1d3b0":"markdown","fc863459":"markdown","20ef9cca":"markdown","996d5e96":"markdown","9eb1932e":"markdown","a64f4b4d":"markdown","59b673c9":"markdown","93d247d0":"markdown","36ceebfc":"markdown","e44c5eb5":"markdown","d0798fec":"markdown","4608d37f":"markdown","4b2de56f":"markdown","4d60b11c":"markdown","96007bcc":"markdown","75386349":"markdown","58beb0e9":"markdown","6dd6ab71":"markdown","1db80a10":"markdown","1c76a045":"markdown","b1c79352":"markdown","f01f33af":"markdown","ad8efc17":"markdown","0a835511":"markdown","0eefd5f5":"markdown","1944fa31":"markdown","4e946df3":"markdown","340144fa":"markdown","54655904":"markdown","8998d4b0":"markdown","5d9cc74f":"markdown","b57aeeb1":"markdown","ccb13272":"markdown","a5f3a986":"markdown","ed489538":"markdown","163ca5b9":"markdown","7980a53b":"markdown","807d1439":"markdown","3a45dd5b":"markdown","957cd05a":"markdown","8cdd5290":"markdown","e3a363e7":"markdown","41739761":"markdown","3adf20f1":"markdown","9cbddc28":"markdown","fda83fb0":"markdown","e2d929ac":"markdown","4c7a2762":"markdown","b1bd3ddb":"markdown","d8250b28":"markdown","6b1d2c0b":"markdown","8b836fb2":"markdown","72bb496a":"markdown"},"source":{"50f30ac4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","2646c7ea":"print(os.listdir(\"..\/input\")) #to check the name of the directory inside which we have our files","3675d56b":"from glob import glob\nData = glob('..\/input\/IDC_regular_ps50_idx5\/**\/*.png', recursive=True)  #we extract only png files","5ea7c93e":"Data","d31a8649":"from PIL import Image\nImage.open(Data[0]).size","17da524f":"\nfrom PIL import Image #adds support for opening, manipulating, and saving many different image file formats\nfrom tqdm import tqdm #adds progress bar for the loops\ndimentions=list()\nx=1\nfor images in tqdm(Data):\n    dim = Image.open(images)\n    size= dim.size\n    if size not in dimentions:\n        dimentions.append(size)\n        x+=1\n    if(x>3): #going through all the images will take up lot of memory, so therefore we will check until we get three different dimentions.\n        break\nprint(dimentions)\n","98f5401a":"import cv2 #used for computer vision tasks such as reading image from file, changing color channels etc\nimport matplotlib.pyplot as plt #for plotting various graph, images etc.\ndef view_images(image): #function to view an image\n    image_cv = cv2.imread(image) #reads an image\n    plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)); #displays an image\nview_images(Data[18])","710b5594":"def hist_plot(image): #to plot histogram of pixel values present in an image VS intensities\n    img = cv2.imread(image)\n    plt.subplot(2, 2,1)\n    view_images(image)\n    plt.subplot(2, 2,2)\n    plt.hist(img.ravel()) \n    plt.xlabel('Pixel Values')\n    plt.ylabel('Intensity')\nhist_plot(Data[169])\n    ","c9814c2e":"from tqdm import tqdm\nimport csv #to open and write csv files\nData_output=list()\nData_output.append([\"Classes\"])\nfor file_name in tqdm(Data):\n    Data_output.append([file_name[-10:-4]])\nwith open(\"output.csv\", \"w\") as f:\n    writer = csv.writer(f)\n    for val in Data_output:\n        writer.writerows([val])","8b55e245":"from IPython.display import display # Allows the use of display() for DataFrames\ndata_output = pd.read_csv(\"output.csv\")\ndisplay(data_output.head(5))\nprint(data_output.shape)","66a4051a":"class1 = data_output[(data_output[\"Classes\"]==\"class1\" )].shape[0]\nclass0 = data_output[(data_output[\"Classes\"]==\"class0\" )].shape[0]\nobjects=[\"class1\",\"class0\"]\ny_pos = np.arange(len(objects))\ncount=[class1,class0]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Number of images')\nplt.title('Class distribution')\n \nplt.show()","7aa97e2c":"percent_class1=class1\/len(Data)\npercent_class0=class0\/len(Data)\nprint(\"Total Class1 images :\",class1)\nprint(\"Total Class0 images :\",class0)\nprint(\"Percent of class 0 images : \", percent_class0*100)\nprint(\"Percent of class 1 images : \", percent_class1*100)","366a5366":"from sklearn.utils import shuffle #to shuffle the data\nData,data_output= shuffle(Data,data_output)","8d7cbbf6":"\nfrom tqdm import tqdm\ndata=list()\nfor img in tqdm(Data):\n    image_ar = cv2.imread(img)\n    data.append(cv2.resize(image_ar,(50,50),interpolation=cv2.INTER_CUBIC))","792fa529":"data_output=data_output.replace(to_replace=\"class0\",value=0)\ndata_output=data_output.replace(to_replace=\"class1\",value=1)","e1d636b7":"from keras.utils import to_categorical #to hot encode the output labels\ndata_output_encoded =to_categorical(data_output, num_classes=2)\nprint(data_output_encoded.shape)","83f45c16":"from sklearn.model_selection import train_test_split\ndata=np.array(data)\nX_train, X_test, Y_train, Y_test = train_test_split(data, data_output_encoded, test_size=0.3)\nprint(\"Number of train files\",len(X_train))\nprint(\"Number of test files\",len(X_test))\nprint(\"Number of train_target files\",len(Y_train))\nprint(\"Number of  test_target  files\",len(Y_test))","f7edd1ca":"X_train=X_train[0:70000]\nY_train=Y_train[0:70000]\nX_test=X_test[0:30000]\nY_test=Y_test[0:30000]","102acf13":"from keras.utils import to_categorical #to hot encode the data\nfrom imblearn.under_sampling import RandomUnderSampler #For performing undersampling\n\nX_train_shape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\nX_test_shape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\nX_train_Flat = X_train.reshape(X_train.shape[0], X_train_shape)\nX_test_Flat = X_test.reshape(X_test.shape[0], X_test_shape)\n\nrandom_US = RandomUnderSampler(ratio='auto') #Constructor of the class to perform undersampling\nX_train_RUS, Y_train_RUS = random_US.fit_sample(X_train_Flat, Y_train) #resamples the dataset\nX_test_RUS, Y_test_RUS = random_US.fit_sample(X_test_Flat, Y_test) #resamples the dataset\ndel(X_train_Flat,X_test_Flat)\n\nclass1=1\nclass0=0\n\nfor i in range(0,len(Y_train_RUS)): \n    if(Y_train_RUS[i]==1):\n        class1+=1\nfor i in range(0,len(Y_train_RUS)): \n    if(Y_train_RUS[i]==0):\n        class0+=1\n#For Plotting the distribution of classes\nclasses=[\"class1\",\"class0\"]\ny_pos = np.arange(len(classes))\ncount=[class1,class0]\nplt.bar(y_pos, count, color = 'green', align='center', alpha=0.5)\n# plt.xticks(y_pos, objects)\nplt.ylabel('Number of images')\nplt.title('Class distribution')\n \nplt.show()\n\n\n#hot encoding them\nY_train_encoded = to_categorical(Y_train_RUS, num_classes = 2)\nY_test_encoded = to_categorical(Y_test_RUS, num_classes = 2)\n\ndel(Y_train_RUS,Y_test_RUS)\n\nfor i in range(len(X_train_RUS)):\n    X_train_RUS_Reshaped = X_train_RUS.reshape(len(X_train_RUS),50,50,3)\ndel(X_train_RUS)\n\nfor i in range(len(X_test_RUS)):\n    X_test_RUS_Reshaped = X_test_RUS.reshape(len(X_test_RUS),50,50,3)\ndel(X_test_RUS)\n","1cfed9f8":"X_test, X_valid, Y_test, Y_valid = train_test_split(X_test_RUS_Reshaped, Y_test_encoded, test_size=0.2,shuffle=True)","da3ba7cc":"print(\"Number of train files\",len(X_train_RUS_Reshaped))\nprint(\"Number of valid files\",len(X_valid))\nprint(\"Number of train_target files\",len(Y_train_encoded))\nprint(\"Number of  valid_target  files\",len(Y_valid))\nprint(\"Number of test files\",len(X_test))\nprint(\"Number of  test_target  files\",len(Y_test))","c3aa4560":"from sklearn.utils import shuffle\nX_train,Y_train= shuffle(X_train_RUS_Reshaped,Y_train_encoded)","ab76fccf":"display(Y_train_encoded.shape)\ndisplay(Y_test.shape)\ndisplay(Y_valid.shape)","8807bd29":"print(\"Training Data Shape:\", X_train.shape)\nprint(\"Validation Data Shape:\", X_valid.shape)\nprint(\"Testing Data Shape:\", X_test.shape)\nprint(\"Training Label Data Shape:\", Y_train.shape)\nprint(\"Validation Label Data Shape:\", Y_valid.shape)\nprint(\"Testing Label Data Shape:\", Y_test.shape)","fc6d8040":"import itertools #create iterators for effective looping\n#Plotting the confusion matrix for checking the accuracy of the model\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","c81ef087":"from keras.preprocessing.image import ImageDataGenerator  #For Image argumentaton\ndatagen = ImageDataGenerator(\n        shear_range=0.2,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        zoom_range=0.2,\n        rescale=1\/255.0,\n        horizontal_flip=True,\n        vertical_flip=True)","dc6f5241":"predictions_arg = [np.argmax(argum_model.predict(np.expand_dims(feature, axis=0))) for feature in tqdm(X_test_e)]","56d93e9f":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_Arg=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_arg))\nplot_confusion_matrix(cnf_matrix_Arg, classes=class_names)","97b63fe3":"import tensorflow.keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.keras import initializers, layers\n\n\nclass Length(layers.Layer):\n    def call(self, inputs, **kwargs):\n        return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:-1]\n\n    def get_config(self):\n        config = super(Length, self).get_config()\n        return config\n\n\nclass Mask(layers.Layer):\n    def call(self, inputs, **kwargs):\n        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n            assert len(inputs) == 2\n            inputs, mask = inputs\n        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n            # compute lengths of capsules\n            x = K.sqrt(K.sum(K.square(inputs), -1))\n            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n\n        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n        return masked\n\n    def compute_output_shape(self, input_shape):\n        if type(input_shape[0]) is tuple:  # true label provided\n            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n        else:  # no true label provided\n            return tuple([None, input_shape[1] * input_shape[2]])\n\n    def get_config(self):\n        config = super(Mask, self).get_config()\n        return config\n\n\ndef squash(vectors, axis=-1):\n    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n    scale = s_squared_norm \/ (1 + s_squared_norm) \/ K.sqrt(s_squared_norm + K.epsilon())\n    return scale * vectors\n\n\nclass CapsuleLayer(layers.Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3,\n                 kernel_initializer='glorot_uniform',\n                 **kwargs):\n        super(CapsuleLayer, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_initializer = initializers.get(kernel_initializer)\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n        self.input_num_capsule = input_shape[1]\n        self.input_dim_capsule = input_shape[2]\n\n        # Transform matrix\n        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n                                        self.dim_capsule, self.input_dim_capsule],\n                                 initializer=self.kernel_initializer,\n                                 name='W')\n\n        self.built = True\n\n    def call(self, inputs, training=None):\n        inputs_expand = K.expand_dims(inputs, 1)\n\n        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n\n        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n\n        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n\n        assert self.routings > 0, 'The routings should be > 0.'\n        for i in range(self.routings):\n            c = tf.nn.softmax(b, dim=1)\n\n            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n\n            if i < self.routings - 1:\n                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return tuple([None, self.num_capsule, self.dim_capsule])\n\n    def get_config(self):\n        config = {\n            'num_capsule': self.num_capsule,\n            'dim_capsule': self.dim_capsule,\n            'routings': self.routings\n        }\n        base_config = super(CapsuleLayer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\ndef PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n  \n    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n                           name='primarycap_conv2d')(inputs)\n    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n\ndef margin_loss(y_true, y_pred):\n    \"\"\"\n    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n    :param y_true: [None, n_classes]\n    :param y_pred: [None, num_capsule]\n    :return: a scalar loss value.\n    \"\"\"\n    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n\n    return K.mean(K.sum(L, 1))\n\n","4736f7a0":"import numpy as np\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n# from utils import combine_images\nfrom PIL import Image\n# from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n\nK.set_image_data_format('channels_last')\n\n\nx = layers.Input(shape=(256,256,3))\nconv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\nprimarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\ndigitcaps = CapsuleLayer(num_capsule=2, dim_capsule=16, routings=3, name='digitcaps')(primarycaps)\nout_caps = Length(name='capsnet')(digitcaps)\n\neval_model = models.Model(x, out_caps)","96ed354b":"eval_model.summary()","6dde930e":"import numpy as np\nfrom keras import layers, models, optimizers\nfrom keras import backend as K\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom utils import combine_images\nfrom PIL import Image\nfrom capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n\nK.set_image_data_format('channels_last')\n\n\n# def CapsNet(input_shape, n_class, routings):\n#     \"\"\"\n#     A Capsule Network on MNIST.\n#     :param input_shape: data shape, 3d, [width, height, channels]\n#     :param n_class: number of classes\n#     :param routings: number of routing iterations\n#     :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n#             `eval_model` can also be used for training.\n#     \"\"\"\n#     x = layers.Input(shape=input_shape)\n\n#     # Layer 1: Just a conventional Conv2D layer\n#     conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n\n#     # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n#     primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n\n#     # Layer 3: Capsule layer. Routing algorithm works here.\n#     digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n#                              name='digitcaps')(primarycaps)\n\n#     # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n#     # If using tensorflow, this will not be necessary. :)\n#     out_caps = Length(name='capsnet')(digitcaps)\n\n#     # Decoder network.\n#     y = layers.Input(shape=(n_class,))\n#     masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n#     masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n\n#     # Shared Decoder model \n#     # Models for training and evaluation (prediction)\n#     train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n#     eval_model = models.Model(x, [out_caps, decoder(masked)])\n\n#     # manipulate model\n#     noise = layers.Input(shape=(n_class, 16))\n#     noised_digitcaps = layers.Add()([digitcaps, noise])\n#     masked_noised_y = Mask()([noised_digitcaps, y])\n#     manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n#     return train_model, eval_model, manipulate_model\n\n\n# def margin_loss(y_true, y_pred):\n#     \"\"\"\n#     Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n#     :param y_true: [None, n_classes]\n#     :param y_pred: [None, num_capsule]\n#     :return: a scalar loss value.\n#     \"\"\"\n#     L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n#         0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n\n#     return K.mean(K.sum(L, 1))\n\n\n","817ed85d":"def train(model, data, args):\n    \"\"\"\n    Training a CapsuleNet\n    :param model: the CapsuleNet model\n    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n    :param args: arguments\n    :return: The trained model\n    \"\"\"\n    # unpacking the data\n    (x_train, y_train), (x_test, y_test) = data\n\n    # compile the model\n    model.compile(optimizer=optimizers.Adam(lr=args.lr),\n                  loss=[margin_loss, 'mse'],\n                  loss_weights=[1., args.lam_recon],\n                  metrics={'capsnet': 'accuracy'})\n\n    \"\"\"\n    # Training without data augmentation:\n    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n    \"\"\"\n\n    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n    def train_generator(x, y, batch_size, shift_fraction=0.):\n        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n                                           height_shift_range=shift_fraction)  # shift up to 2 pixel for MNIST\n        generator = train_datagen.flow(x, y, batch_size=batch_size)\n        while 1:\n            x_batch, y_batch = generator.next()\n            yield ([x_batch, y_batch], [y_batch, x_batch])\n\n    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n    model.fit_generator(generator=train_generator(x_train, y_train, args.batch_size, args.shift_fraction),\n                        steps_per_epoch=int(y_train.shape[0] \/ args.batch_size),\n                        epochs=args.epochs,\n                        validation_data=[[x_test, y_test], [y_test, x_test]],\n                        callbacks=[log, tb, checkpoint, lr_decay])\n    # End: Training with data augmentation -----------------------------------------------------------------------#\n\n    model.save_weights(args.save_dir + '\/trained_model.h5')\n    print('Trained model saved to \\'%s\/trained_model.h5\\'' % args.save_dir)\n\n    from utils import plot_log\n    plot_log(args.save_dir + '\/log.csv', show=True)\n\n    return model\n\n\ndef test(model, data, args):\n    x_test, y_test = data\n    y_pred, x_recon = model.predict(x_test, batch_size=100)\n    print('-'*30 + 'Begin: test' + '-'*30)\n    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))\/y_test.shape[0])\n\n    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n    image = img * 255\n    Image.fromarray(image.astype(np.uint8)).save(args.save_dir + \"\/real_and_recon.png\")\n    print()\n    print('Reconstructed images are saved to %s\/real_and_recon.png' % args.save_dir)\n    print('-' * 30 + 'End: test' + '-' * 30)\n    plt.imshow(plt.imread(args.save_dir + \"\/real_and_recon.png\"))\n    plt.show()\n\n\ndef manipulate_latent(model, data, args):\n    print('-'*30 + 'Begin: manipulate' + '-'*30)\n    x_test, y_test = data\n    index = np.argmax(y_test, 1) == args.digit\n    number = np.random.randint(low=0, high=sum(index) - 1)\n    x, y = x_test[index][number], y_test[index][number]\n    x, y = np.expand_dims(x, 0), np.expand_dims(y, 0)\n    noise = np.zeros([1, 10, 16])\n    x_recons = []\n    for dim in range(16):\n        for r in [-0.25, -0.2, -0.15, -0.1, -0.05, 0, 0.05, 0.1, 0.15, 0.2, 0.25]:\n            tmp = np.copy(noise)\n            tmp[:,:,dim] = r\n            x_recon = model.predict([x, y, tmp])\n            x_recons.append(x_recon)\n\n    x_recons = np.concatenate(x_recons)\n\n    img = combine_images(x_recons, height=16)\n    image = img*255\n    Image.fromarray(image.astype(np.uint8)).save(args.save_dir + '\/manipulate-%d.png' % args.digit)\n    print('manipulated result saved to %s\/manipulate-%d.png' % (args.save_dir, args.digit))\n    print('-' * 30 + 'End: manipulate' + '-' * 30)\n\n\n\n\n\n    import os\n    import argparse\n    from keras.preprocessing.image import ImageDataGenerator\n    from keras import callbacks\n    model, eval_model, manipulate_model = CapsNet(input_shape=x_train.shape[1:],\n                                                  n_class=len(np.unique(np.argmax(y_train, 1))),\n                                                  routings=args.routings)\n    model.summary()\n\n    # train or test\n    if args.weights is not None:  # init the model weights with provided one\n        model.load_weights(args.weights)\n    if not args.testing:\n        train(model=model, data=((x_train, y_train), (x_test, y_test)), args=args)\n    else:  # as long as weights are given, will run testing\n        if args.weights is None:\n            print('No weights are provided. Will test using random initialized weights.')\n        manipulate_latent(manipulate_model, (x_test, y_test), args)\n        test(model=eval_model, data=(x_test, y_test), args=args)","35586e5a":"from keras.preprocessing.image import ImageDataGenerator  #For Image argumentaton\ndatagen_caps = ImageDataGenerator(\n        shear_range=0.2,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        zoom_range=0.2,\n        rescale=1\/255.0,\n        horizontal_flip=True,\n        vertical_flip=True)","894867fe":"from keras import backend as K\nfrom keras.layers import Layer\nfrom keras import activations\nfrom keras import utils\n\nfrom keras.models import Model\n\n\n\n# the squashing function.\n# we use 0.5 in stead of 1 in hinton's paper.\n# if 1, the norm of vector will be zoomed out.\n# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n# and be zoomed out while original norm is greater than 0.5.\ndef squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    scale = K.sqrt(s_squared_norm) \/ (0.5 + s_squared_norm)\n    return scale * x\n\n\n\n# define our own softmax function instead of K.softmax\n# because K.softmax can not specify axis.\ndef softmax(x, axis=-1):\n    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n    return ex \/ K.sum(ex, axis=axis, keepdims=True)\n\n\n\ndef margin_loss(y_true, y_pred):\n    lamb, margin = 0.5, 0.1\n    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n\n\n","e236424c":"class Capsule(layers.Layer):\n    def __init__(self,\n                 num_capsule,\n                 dim_capsule,\n                 routings=3,\n                 share_weights=True,\n                 activation='squash',\n                 **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.share_weights = share_weights\n        if activation == 'squash':\n            self.activation = squash\n        else:\n            self.activation = activations.get(activation)\n\n    def build(self, input_shape):\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(1, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(input_num_capsule, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n\n    def call(self, inputs):\n        if self.share_weights:\n            hat_inputs = K.conv1d(inputs, self.kernel)\n        else:\n            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n\n        batch_size = K.shape(inputs)[0]\n        input_num_capsule = K.shape(inputs)[1]\n        hat_inputs = K.reshape(hat_inputs,\n                               (batch_size, input_num_capsule,\n                                self.num_capsule, self.dim_capsule))\n        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n\n        b = K.zeros_like(hat_inputs[:, :, :, 0])\n        for i in range(self.routings):\n            c = softmax(b, 1)\n            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(o, hat_inputs, [2, 3])\n                if K.backend() == 'theano':\n                    o = K.sum(o, axis=1)\n\n        return o\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","669a9f1e":"def squash(vectors, axis=-1):\n    \"\"\"\n    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n    :param vectors: some vectors to be squashed, N-dim tensor\n    :param axis: the axis to squash\n    :return: a Tensor with same shape as input vectors\n    \"\"\"\n    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n    scale = s_squared_norm \/ (1 + s_squared_norm) \/ K.sqrt(s_squared_norm + K.epsilon())\n    return scale * vectors","638ed1c4":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense, Input, Reshape, Lambda\nfrom keras.models import Sequential, Model\n\n\ninput_image = Input(shape=(50, 50, 3))\nx = Conv2D(64, (3, 3), activation='relu')(input_image)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = AveragePooling2D((2, 2))(x)\nx = Conv2D(128, (3, 3), activation='relu')(x)\nx = Conv2D(128, (3, 3), activation='relu')(x)\n\n\n\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\nthen connect a Capsule layer.\n\nthe output of final model is the lengths of 10 Capsule, whose dim=16.\n\nthe length of Capsule is the proba,\nso the problem becomes a 10 two-classification problem.\n\"\"\"\n\n# x = Reshape((-1, 128))(x)\nprimarycaps = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\ncapsule = Capsule(2, 16, 3, True)(primarycaps)\noutput = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\nmodel1 = Model(inputs=input_image, outputs=output)\n\n# we use a margin loss\nmodel1.compile(loss=margin_loss, optimizer='AdaDelta', metrics=['accuracy'])\nmodel1.summary()\n","f28a799d":"term = keras.callbacks.TerminateOnNaN()","e50e12cb":"# batch_size=64\nbatch_size=32 #comment this line and uncomment the above line if you want the batch size of 64\nepochs=22\nmodel1.fit_generator(datagen_caps.flow(X_train, Y_train, batch_size), \n          validation_data=(X_valid_e, Y_valid), steps_per_epoch=len(X_train) \/ batch_size, callbacks = [term],\n          epochs=epochs, verbose=1)","6f4c78b6":"from keras.utils import plot_model\nplot_model(model1, to_file='model1.png', show_shapes = True, show_layer_names = True)","0496b98a":"from IPython.display import SVG\nfrom keras.utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","0e0168e2":"train_acc1 = model1.history.history['acc']\nval_acc1 = model1.history.history['val_acc']\nplt.plot(train_acc1)\nplt.plot(val_acc1, color = 'green')\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","0da753ae":"train_loss1 = model1.history.history['loss']\nval_loss1 = model1.history.history['val_loss']\nplt.plot(train_loss1)\nplt.plot(val_loss1, color = 'green')\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","28f95a1f":"import tensorflow as tf\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.models import Model\n\n# the squashing function.\n# we use 0.5 in stead of 1 in hinton's paper.\n# if 1, the norm of vector will be zoomed out.\n# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n# and be zoomed out while original norm is greater than 0.5.\ndef squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    scale = K.sqrt(s_squared_norm) \/ (0.5 + s_squared_norm)\n    return scale * x\n\n\n\n# define our own softmax function instead of K.softmax\n# because K.softmax can not specify axis.\ndef softmax(x, axis=-1):\n    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n    return ex \/ K.sum(ex, axis=axis, keepdims=True)\n\n\n\ndef margin_loss(y_true, y_pred):\n    lamb, margin = 0.5, 0.1\n    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n\n\n\n\nclass Capsule(Layer):\n    def __init__(self,\n                 num_capsule,\n                 dim_capsule,\n                 routings=3,\n                 share_weights=True,\n                 activation='squash',\n                 **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.share_weights = share_weights\n        if activation == 'squash':\n            self.activation = squash\n        else:\n            self.activation = activations.get(activation)\n\n    def build(self, input_shape):\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(1, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(input_num_capsule, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n\n    def call(self, inputs):\n        if self.share_weights:\n            hat_inputs = K.conv1d(inputs, self.kernel)\n        else:\n            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n\n        batch_size = K.shape(inputs)[0]\n        input_num_capsule = K.shape(inputs)[1]\n        hat_inputs = K.reshape(hat_inputs,\n                               (batch_size, input_num_capsule,\n                                self.num_capsule, self.dim_capsule))\n        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n\n        b = K.zeros_like(hat_inputs[:, :, :, 0])\n        for i in range(self.routings):\n            c = softmax(b, 1)\n            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(o, hat_inputs, [2, 3])\n                if K.backend() == 'theano':\n                    o = K.sum(o, axis=1)\n\n        return o\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","022a3ff3":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.layers import Dropout, Flatten, Dense, Input, Reshape, Lambda, Activation, BatchNormalization\nfrom tensorflow.keras.models import Sequential, Model\n\ncustom_model = Sequential()\n# custom_model.add(256, (5, 5), input_shape=(50, 50, 3))\ncustom_model.add(Conv2D(256, (5, 5), input_shape=(50, 50, 3)))\ncustom_model.add(BatchNormalization())\ncustom_model.add(Conv2D(128, (3, 3), activation='relu'))\n# custom_model.add(BatchNormalization())\ncustom_model.add(Reshape((-1, 128)))\ncustom_model.add(Capsule(64, 8, 3, True))\ncustom_model.add(Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2))))\ncustom_model.add(Dense(8, activation='softmax'))\n","cf8dd8b2":"# import tensorflow as tf\n\nimport keras.backend as K\nfrom keras.layers import Layer\nfrom keras import activations\nfrom keras import utils\nfrom keras.models import Model\n\n# the squashing function.\n# we use 0.5 in stead of 1 in hinton's paper.\n# if 1, the norm of vector will be zoomed out.\n# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n# and be zoomed out while original norm is greater than 0.5.\ndef squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    scale = K.sqrt(s_squared_norm) \/ (0.5 + s_squared_norm)\n    return scale * x\n\n\n\n# define our own softmax function instead of K.softmax\n# because K.softmax can not specify axis.\ndef softmax(x, axis=-1):\n    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n    return ex \/ K.sum(ex, axis=axis, keepdims=True)\n\n\n\ndef margin_loss(y_true, y_pred):\n    lamb, margin = 0.5, 0.1\n    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n\n\n\n\nclass Capsule(Layer):\n    \"\"\"A Capsule Implement with Pure Keras\n    There are two vesions of Capsule.\n    One is like dense layer (for the fixed-shape input),\n    and the other is like timedistributed dense (for various length input).\n\n    The input shape of Capsule must be (batch_size,\n                                        input_num_capsule,\n                                        input_dim_capsule\n                                       )\n    and the output shape is (batch_size,\n                             num_capsule,\n                             dim_capsule\n                            )\n\n    Capsule Implement is from https:\/\/github.com\/bojone\/Capsule\/\n    Capsule Paper: https:\/\/arxiv.org\/abs\/1710.09829\n    \"\"\"\n\n    def __init__(self,\n                 num_capsule,\n                 dim_capsule,\n                 routings=3,\n                 share_weights=True,\n                 activation='squash',\n                 **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.share_weights = share_weights\n        if activation == 'squash':\n            self.activation = squash\n        else:\n            self.activation = activations.get(activation)\n\n    def build(self, input_shape):\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(1, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.kernel = self.add_weight(\n                name='capsule_kernel',\n                shape=(input_num_capsule, input_dim_capsule,\n                       self.num_capsule * self.dim_capsule),\n                initializer='glorot_uniform',\n                trainable=True)\n\n    def call(self, inputs):\n        \"\"\"Following the routing algorithm from Hinton's paper,\n        but replace b = b + <u,v> with b = <u,v>.\n\n        This change can improve the feature representation of Capsule.\n\n        However, you can replace\n            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n        with\n            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n        to realize a standard routing.\n        \"\"\"\n\n        if self.share_weights:\n            hat_inputs = K.conv1d(inputs, self.kernel)\n        else:\n            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n\n        batch_size = K.shape(inputs)[0]\n        input_num_capsule = K.shape(inputs)[1]\n        hat_inputs = K.reshape(hat_inputs,\n                               (batch_size, input_num_capsule,\n                                self.num_capsule, self.dim_capsule))\n        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n\n        b = K.zeros_like(hat_inputs[:, :, :, 0])\n        for i in range(self.routings):\n            c = softmax(b, 1)\n            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(o, hat_inputs, [2, 3])\n                if K.backend() == 'theano':\n                    o = K.sum(o, axis=1)\n\n        return o\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","bf9ce301":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense, Input, Reshape, Lambda, Activation, BatchNormalization\nfrom keras.models import Sequential, Model\n\n\ninput_image = Input(shape=(50, 50, 3))\nx = Conv2D(256, (5, 5), activation='relu')(input_image)\nx = BatchNormalization()(x)\nx = Conv2D(128, (3, 3), activation='relu')(x)\nx = BatchNormalization()(x)\n# x = Conv2D(64, (3, 3), activation='relu')(x)\n# x = AveragePooling2D((2, 2))(x)\n# x = Conv2D(128, (3, 3), activation='relu')(x)\n# x = Conv2D(128, (3, 3), activation='relu')(x)\n# x = Reshape((-1, 128))(x)\nprimarycaps = PrimaryCap(x, dim_capsule=8, n_channels=64, kernel_size=9, strides=2, padding='valid')\n# capsule = Capsule(64, 8, 3, True)(primarycaps)\ncapsule = Capsule(48, 16, 3, True)(primarycaps)\ncapsule = Capsule(32, 24, 3, False)(capsule)# This is more than one capsule layer\ncapsule = Capsule(16, 32, 3, False)(capsule)# This is more than one capsule layer\ncapsule = Capsule(2, 48, 3, False)(capsule)# This is more than one capsule layer\n# output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\nout_caps = Length(name='capsnet')(capsule)\nmodel2 = Model(inputs=input_image, outputs=out_caps)\n\n\n\n\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\nthen connect a Capsule layer.\n\nthe output of final model is the lengths of 10 Capsule, whose dim=16.\n\nthe length of Capsule is the proba,\nso the problem becomes a 10 two-classification problem.\n\"\"\"\n\n\n# we use a margin loss\nmodel2.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy'])\nmodel2.summary()","dc819396":"import keras\nterm = keras.callbacks.TerminateOnNaN()\n\nbatch_size=32\nepochs=22\n# epochs=50 #In cases if you try and are not getting the satisfactory accuracy compared to other models then it s advisable to train\n#it for longer period of time by increasing the number of epochs.\nmodel2.fit_generator(datagen.flow(X_train, Y_train, batch_size), \n          validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train) \/ batch_size,callbacks = [term],\n          epochs=epochs, verbose=1)\n","f2853754":"from keras.utils import plot_model\nplot_model(model2, to_file='model2.png', show_shapes = True, show_layer_names = True)","4dc4e227":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense, Input, Reshape, Lambda, Activation, BatchNormalization\nfrom keras.models import Sequential, Model\n\ncustom_model = Sequential()\ncustom_model.add(Conv2D(64, (7,7), input_shape=(50, 50, 3)))\ncustom_model.add(Activation('elu'))\ncustom_model.add(BatchNormalization())\ncustom_model.add(Conv2D(64, (7,7)))\ncustom_model.add(Activation('elu'))\ncustom_model.add(BatchNormalization())\ncustom_model.add(MaxPooling2D(pool_size=(2,2)))\ncustom_model.add(Dropout(0.2))\n\ncustom_model.add(Conv2D(128, (5,5)))\ncustom_model.add(Activation('elu'))\ncustom_model.add(BatchNormalization())\ncustom_model.add(Conv2D(128, (5,5)))\ncustom_model.add(Activation('elu'))\ncustom_model.add(BatchNormalization())\ncustom_model.add(MaxPooling2D(pool_size=(2,2)))\ncustom_model.add(Dropout(0.3))\n \ncustom_model.add(Conv2D(256, (3,3)))\ncustom_model.add(Activation('elu'))\ncustom_model.add(BatchNormalization())\ncustom_model.add(Conv2D(256, (3,3)))\ncustom_model.add(Activation('elu'))\ncustom_model.add(BatchNormalization())\n# custom_model.add(MaxPooling2D(pool_size=(2,2)))\ncustom_model.add(Dropout(0.4))\n \ncustom_model.add(Flatten())\ncustom_model.add(Dense(328, activation='relu'))\ncustom_model.add(Dense(128, activation='relu'))\ncustom_model.add(Dense(8, activation='softmax'))\n\ncustom_model.summary()","1ab31be2":"import keras\nterm = keras.callbacks.TerminateOnNaN()\n\nbatch_size=32\nepochs=22\n# epochs=50 #In cases if you try and are not getting the satisfactory accuracy compared to other models then it s advisable to train\n#it for longer period of time by increasing the number of epochs.\ncustom_model.fit_generator(datagen.flow(X_train, Y_train, batch_size), \n          validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train) \/ batch_size,callbacks = [term],\n          epochs=epochs, verbose=1)","d9f768fa":"from tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nweight_decay = 1e-4\n_EPSILON = 10e-8\n\na = 0.2625\nepochs = 175\niterations = 1\nlearning_rate = 0.0002\n\n\nacc_list = list()\nloss_list = list()\nval_acc_list = list()\nval_loss_list = list()\n\n\nfor i in range(iterations):\n  \n\n  def _to_tensor(x, dtype):\n    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n    # Arguments\n        x: An object to be converted (numpy array, list, tensors).\n        dtype: The destination type.\n    # Returns\n        A tensor.\n    \"\"\"\n    x = tf.convert_to_tensor(x)\n    if x.dtype != dtype:\n        x = tf.cast(x, dtype)\n    return x\n\n  _EPSILON = 10e-8\n  def customentropy(target, output, from_logits=False):\n\n    \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n    # Arguments\n        output: A tensor resulting from a softmax\n            (unless `from_logits` is True, in which\n            case `output` is expected to be the logits).\n        target: A tensor of the same shape as `output`.\n        from_logits: Boolean, whether `output` is the\n            result of a softmax, or is a tensor of logits.\n    # Returns\n        Output tensor.\n    \"\"\"\n    # Note: tf.nn.softmax_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    # scale preds so that the class probas of each sample sum to 1\n\n\n\n\n\n    output \/= tf.reduce_sum(output,\n                            axis=len(output.get_shape()) - 1,\n                            keepdims=True)\n        # manual computation of crossentropy\n    epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n    output = tf.clip_by_value(output, epsilon, 1. - epsilon)\n    return tf.reduce_sum(target * (tf.math.exp(1.0)-tf.math.exp(tf.math.pow(output, a))),\n                               axis=len(output.get_shape()) - 1)\n    \n  datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    )\n  datagen.fit(x_train)\n \n  #training\n  batch_size = 64\n \n  opt_rms = tf.keras.optimizers.Adam(lr=learning_rate,decay=1e-6)\n  custom_model.compile(loss=customentropy, optimizer=opt_rms, metrics=['accuracy'])\n\n  custom_model.fit_generator(datagen.flow(x_train, y_train,\n                                 batch_size=batch_size),\n                                 steps_per_epoch=x_train.shape[0] \/\/ batch_size, \n                                 epochs=epochs,\n                                 #verbose=1, \n                                 validation_data=(x_test, y_test),\n                                 workers=1)  \n  \n  acc_list.append(custom_model.history.history['acc'])\n  val_acc_list.append(custom_model.history.history['val_acc'])\n  loss_list.append(custom_model.history.history['loss'])\n  val_loss_list.append(custom_model.history.history['val_loss'])","c49744d9":"\nimport keras\nterm = keras.callbacks.TerminateOnNaN()\n\nbatch_size=32\nepochs=22\n# epochs=50 #In cases if you try and are not getting the satisfactory accuracy compared to other models then it s advisable to train\n#it for longer period of time by increasing the number of epochs.\nmodel2.fit_generator(datagen_caps.flow(X_train, Y_train, batch_size), \n          validation_data=(X_valid_e, Y_valid), steps_per_epoch=len(X_train) \/ batch_size,callbacks = [term],\n          epochs=epochs, verbose=1)\n","2d8a8275":"train_acc = model2.history.history['acc']\nval_acc = model2.history.history['val_acc']\n\nplt.plot(train_acc)\nplt.plot(val_acc, color='green')\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","8740d45b":"train_loss = model2.history.history['loss']\nval_loss = model2.history.history['val_loss']\n\nplt.plot(train_loss)\nplt.plot(val_loss, color='green')\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","53e87075":"import numpy\nprint('The highest accuracy it achieved during the validation was ' , numpy.amax(numpy.array(model2.history.history['val_acc'])))","5b93e1f6":"predictions_caps2 = [np.argmax(model2.predict(np.expand_dims(feature, axis=0))) for feature in tqdm(X_test_e)]","84363683":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_caps2=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_caps2))\nplot_confusion_matrix(cnf_matrix_caps2, classes=class_names)","06df4cfb":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\nnf_matrix_caps2=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_transfer))\nplot_confusion_matrix(cnf_matrix_transfer, classes=class_names)","72a3eb3d":"train_acc_conv = argum_model.history.history['acc'][:20]\ntrain_acc_caps = model2.history.history['acc']\ntrain_acc_trans = model_transfer.history.history['acc']\n# [1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \nplt.plot(train_acc_conv)\nplt.plot(train_acc_caps, color='green')\nplt.plot(train_acc_trans, color='red')\n\n\n# plt.plot(train_acc1, color='red')\n# plt.plot(val_acc1, color = 'orange')\nplt.title('Training Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['ConvNet', 'CapsNet', 'VGG 19'], loc='upper left')\nplt.show()","dfcf8f7c":"train_loss_conv = argum_model.history.history['loss'][:20]\ntrain_loss_caps = model2.history.history['loss']\ntrain_loss_trans = model_transfer.history.history['loss']\n# [1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \nplt.plot(train_loss_conv)\nplt.plot(train_loss_caps, color='green')\nplt.plot(train_loss_trans, color='red')\n\n\n# plt.plot(train_acc1, color='red')\n# plt.plot(val_acc1, color = 'orange')\nplt.title('Training Loss')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['ConvNet', 'CapsNet', 'VGG 19'], loc='upper left')\nplt.show()","95a28b70":"val_acc_conv = argum_model.history.history['val_acc']\nval_acc_caps = model2.history.history['val_acc']\nval_acc_trans = model_transfer.history.history['val_acc']\n\nplt.plot(val_acc_conv)\nplt.plot(val_acc_caps, color='green')\nplt.plot(val_acc_trans, color='red')\n\n\n# plt.plot(train_acc1, color='red')\n# plt.plot(val_acc1, color = 'orange')\nplt.title('Validation Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['ConvNet', 'CapsNet', 'VGG 19'], loc='upper left')\nplt.show()\n","6e9d9546":"val_acc_simp = argum_model.history.history['val_loss']\nval_acc_augm = model2.history.history['val_loss']\nval_acc_caps = model_transfer.history.history['val_loss']\n\nplt.plot(val_acc_simp)\nplt.plot(val_acc_augm, color='green')\nplt.plot(val_acc_caps, color='red')\n\n\n# plt.plot(train_acc1, color='red')\n# plt.plot(val_acc1, color = 'orange')\nplt.title('Validation Loss')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['ConvNet', 'CapsNet', 'VGG 19'], loc='upper left')\nplt.show()\n","391f1de0":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_caps2=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_caps2))\nplot_confusion_matrix(cnf_matrix_caps2, classes=class_names)","6e3389e2":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_transfer=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_transfer))\nplot_confusion_matrix(cnf_matrix_transfer, classes=class_names)","5f349d5e":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_Arg=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_arg))\nplot_confusion_matrix(cnf_matrix_Arg, classes=class_names)","bf72aa22":"# tp=0\n# for i in range(0,len(Y_test)): #Number of positive cases\n#     if(np.argmax(Y_test[i])==1):\n#         tp+=1\n#Senstivity of models\n# confusion_bench_s=cnf_matrix_bench[1][1]\/tp *100 \nconfusion_Arg_sens=cnf_matrix_Arg[0][0]\/(cnf_matrix_Arg[0][0]+cnf_matrix_Arg[0][1])\ncnf_matrix_caps2_sens=cnf_matrix_caps2[0][0]\/(cnf_matrix_caps2[0][0]+cnf_matrix_caps2[0][1])\ncnf_matrix_transfer_sens=cnf_matrix_transfer[0][0]\/(cnf_matrix_transfer[0][0]+cnf_matrix_transfer[0][1])\n\nclasses=[\"ConvNet\",\"CapsNet\", \"VGG19\"]\nobjects=[\"ConvNet\",\"CapsNet\", \"VGG19\"]\ny_pos = np.arange(len(classes))\ncount=[confusion_Arg_sens, cnf_matrix_caps2_sens, cnf_matrix_transfer_sens]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Percentage')\nplt.title('Sensitivity')\n\nplt.show()","20a4c775":"cnf_matrix_Arg","6b5321c9":"cnf_matrix_Arg[0][0]+cnf_matrix_Arg[0][1]","da2e0c6a":"cnf_matrix_caps2","2094ba30":"[confusion_Arg_sens, cnf_matrix_caps2_sens, cnf_matrix_transfer_sens]","def127c9":"# tp=0\n# tn=0\n# for i in range(0,len(Y_test)):  #Number of postive cases\n#     if(np.argmax(Y_test[i])==1): \n#         tp+=1\n# for i in range(0,len(Y_test)): #number of negative cases\n#     if(np.argmax(Y_test[i])==0):\n#         tn+=1\nconfusion_Arg_spec=cnf_matrix_Arg[1][1]\/(cnf_matrix_Arg[1][0]+cnf_matrix_Arg[1][1])\ncnf_matrix_caps2_spec=cnf_matrix_caps2[1][1]\/(cnf_matrix_caps2[1][0]+cnf_matrix_caps2[1][1])\ncnf_matrix_transfer_spec=cnf_matrix_transfer[1][1]\/(cnf_matrix_transfer[1][1]+cnf_matrix_transfer[1][1])\n\nclasses=[\"ConvNet\",\"CapsNet\", \"VGG19\"]\nobjects=[\"ConvNet\",\"CapsNet\", \"VGG19\"]\ny_pos = np.arange(len(classes))\ncount=[confusion_Arg_spec, cnf_matrix_caps2_spec, cnf_matrix_transfer_spec]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Percentage')\nplt.title('Specificity')\n\nplt.show()","69510034":"[confusion_Arg_spec, cnf_matrix_caps2_spec, cnf_matrix_transfer_spec]","9199c559":"confusion_Arg_prec=cnf_matrix_Arg[0][0]\/(cnf_matrix_Arg[0][0]+cnf_matrix_Arg[1][0])\ncnf_matrix_caps2_prec=cnf_matrix_caps2[0][0]\/(cnf_matrix_caps2[0][0]+cnf_matrix_caps2[1][0])\ncnf_matrix_transfer_prec=cnf_matrix_transfer[0][0]\/(cnf_matrix_transfer[0][0]+cnf_matrix_transfer[1][0])\n\nclasses=[\"ConvNet\",\"CapsNet\", \"VGG19\"]\nobjects=[\"ConvNet\",\"CapsNet\", \"VGG19\"]\ny_pos = np.arange(len(classes))\ncount=[confusion_Arg_prec, cnf_matrix_caps2_prec, cnf_matrix_transfer_prec]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Percentage')\nplt.title('Precision')\n\nplt.show()","8d6c7274":"[confusion_Arg_prec, cnf_matrix_caps2_prec, cnf_matrix_transfer_prec]","e86106f7":"confusion_Arg_fmea=(2*confusion_Arg_prec*confusion_Arg_sens)\/(confusion_Arg_prec+confusion_Arg_sens)\ncnf_matrix_caps2_fmea=(2*cnf_matrix_caps2_prec*cnf_matrix_caps2_sens)\/(cnf_matrix_caps2_prec+cnf_matrix_caps2_sens)\ncnf_matrix_transfer_fmea=(2*cnf_matrix_transfer_prec*cnf_matrix_transfer_prec)\/(cnf_matrix_transfer_prec+cnf_matrix_transfer_prec)\n\nclasses=[\"ConvNet\",\"CapsNet\", \"VGG19\"]\nobjects=[\"ConvNet\",\"CapsNet\", \"VGG19\"]\ny_pos = np.arange(len(classes))\ncount=[confusion_Arg_fmea, cnf_matrix_caps2_fmea, cnf_matrix_transfer_fmea]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Percentage')\nplt.title('F-measure')\n\nplt.show()","6e047357":"[confusion_Arg_fmea, cnf_matrix_caps2_fmea, cnf_matrix_transfer_fmea]","ad43d83c":"The following are the functions and parameters that are further required for construction of `Capsule Network`.","7da62b00":"> ***Now we will plot the confusion matrix :***","b9d1d3b0":"## The entire capsule network","fc863459":"## Comparision between the models \n\n** Graph of all four models' validation accuracy and loss.**","20ef9cca":"> **Code Conclusion **: We have total of 277524 image files","996d5e96":"> ***Now we will plot the confusion matrix :***","9eb1932e":"## More sophisticated model\n\nWith 2 capsule layers","a64f4b4d":"**Breast Cancer Detection**\n![](https:\/\/blogs.nvidia.com\/wp-content\/uploads\/2018\/01\/AI_Mammographie.jpg)","59b673c9":"## Specificity between the three models.","93d247d0":"I have by default added the data augmentation. The augmentor for `CapsNet` is called `datagen_caps`.","36ceebfc":"***Domain Background*** : \n\tBreast Cancer is the most common type of cancer in woman worldwide accounting for 20% of all cases.\n    \n>     In 2012 it resulted in 1.68 million new cases and 522,000 deaths.\n    \nOne of the major problems is that women often neglect the symptoms, which could cause more adverse effects on them thus lowering the survival chances. In developed countries, the survival rate is although high, but it is an area of concern in the developing countries where the 5-year survival rates are poor. In India, there are about one million cases every year and the five-year survival of stage IV breast cancer is about 10%. Therefore it is very important to detect the signs as early as possible. \n    \n>     Invasive ductal carcinoma (IDC) is the most common form of breast cancer.\n   \n   About 80% of all breast cancers are invasive ductal carcinomas. Doctors often do the biopsy or a scan if they detect signs of IDC. The cost of testing for breast cancer sets one back with $5000, which is a very big amount for poor families and also manual identification of presence and extent of breast cancer by a pathologist is critical. Therefore automation of detection of breast cancer using Histopathology images could reduce cost and time as well as improve the accuracy of the test. This is an active research field lot of research papers and articles are present online one that I like is -(https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5453426\/) as they used deep learning approach to study on histology images and achieved the sensitivity of 95 which is greater than many pathologists (~90). This shows the power of automation and how it could help in the detection of breast cancer.\n\n","e44c5eb5":"> **Code Conclusion :**  There are only png extentions which are present in alphabets therefore it means that we have only one image extention files with *.png* extentions. Therefore we will load only that.","d0798fec":"****<img src='model1.png'\/>","4608d37f":"# XifengGuo","4b2de56f":"In the next step we will OneHot encode our data to better work with neural networks.","4d60b11c":"# End Remarks\n\nIt was observed that the CapsNet performed better than all of the models. \n\nIt had performed well on the train data, validation data and the test data as well as seen by the confusion matrix.\n\nAlso, it trained in less no. of epochs and the remained stable for the rest of the epochs. While, somewhat unstable nature was shown by augmented model. \n\nAnd the transfer learning model's accuracy saturated at around 80%.\n\nFurther, I am willing to make this a more complex, hence better model that may consists of more than just two capsule layers.\n","96007bcc":"> ***Code Conclusion :*** From the above image we can conclude that brighter region is more than the darken region in our image.  ","75386349":"**Data Exploration**","58beb0e9":"## F-measure","6dd6ab71":"Next step is we need to extract the class names in which each files belong from its file names. We will save it in output.csv file.","1db80a10":"We will also rescale our image pixels, from range of 0-255.0 to 0-1.","1c76a045":"We will now add image argumentation to our data, so that it may be set for wider range of domain","b1c79352":"We will go for spliting testing set into validation set.","f01f33af":"Now lets look at the color ranges that our images have","ad8efc17":"***Data Extraction and Visualization***","0a835511":"We have a large dataset and we will work with neural networks, therefore for better debugging we will use only a part of data, considering limited RAM and non GPU processor, this will not cost us much as we would also be using under sampling methods and image argumentation to deal with class imbalances and moderate data.","0eefd5f5":"Next Step is that we will check whether the dimentions of all the images are same or different","1944fa31":"Through below 2 cells you can print the accuracy and loss of the model.","4e946df3":"Now we will split our data into training set and testing set.","340144fa":"## Precision","54655904":"\nIn data exploration we will first check the name of the files.","8998d4b0":"> ***Code Conclusion : *** We can see that the dimentions of images are not equal therefore we would make it all equal  to work bettter with our network.","5d9cc74f":"> ***Data Processing  *** ","b57aeeb1":"It was observed that the model, after certain epochs starts attaining the nan loss. Hence we applied the below call back in order to terminate as soon as it starts attaining nan loss.","ccb13272":"We will first shuffle are images to remove any patterns if present and then load them.","a5f3a986":"\n***Image Argumentation***","ed489538":"Below two cells can plot the confusion matrix for the `model2` capsule network.","163ca5b9":"> We need to now preprocess our image file. We change pixels range from 0-255 to 0-1.","7980a53b":"> ***Code Conclusion :*** We can see that we have an unbalanced class and which is a common problem when we have medical data, therefore this is one another problem that we have to deal with later.","807d1439":"## Capsule Layer\n\n** This is the custom layer of capsule network. **","3a45dd5b":"**IMPORT FILES**","957cd05a":"# The Capsule Network starts from here on.","8cdd5290":"## Sensitivity between the three models.","e3a363e7":"### However, I am willing to add some more analysis and visualization in a sooner time.","41739761":"> ***BENCHMARK MODEL: *** A simple CNN model","3adf20f1":"Below code reads the data from output.csv and displays it","9cbddc28":"We also need a validation set inorder to check overfitting. We can do two things either split test set further into valid set or split train se into valid set.","fda83fb0":"We would encode our output data which is present as Class1 and Class0 to 1 and 0.","e2d929ac":"> *Class1* represents** IDC(+)** and* Class0* represents** IDC(-)**","4c7a2762":"We will now do undersampling, to treat our data for class imbalances. The Code inspiration for undersampling is taken from a notebook - https:\/\/www.kaggle.com\/paultimothymooney\/predict-idc-in-breast-cancer-histology-images","b1bd3ddb":"# The training part starts from here on.","d8250b28":"> ***Code Conclusion :*** We can see that images are very small, though they are cropped images, its hard for human eye to understand them without using some high costly machines. ","6b1d2c0b":"## Below is the model trained using data augmentation.","8b836fb2":"Now we have our three sets of train, valid and test. We will now create our benchmark model.","72bb496a":"<img src='model2.png' \/>"}}