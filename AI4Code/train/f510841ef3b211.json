{"cell_type":{"fed7f71d":"code","5fc2f36b":"code","fdc25024":"code","3918dcd6":"code","982deec8":"code","37702005":"code","1b9fe955":"code","08e418fd":"markdown","63fd2ab2":"markdown","12362f5c":"markdown","edd34a3c":"markdown","7f548467":"markdown","50306223":"markdown","f6fe1a21":"markdown"},"source":{"fed7f71d":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.keras.callbacks import ReduceLROnPlateau","5fc2f36b":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n                                            patience=2,\n                                            verbose=1,\n                                            factor=0.5,\n                                            min_lr=0.00001)\ncallbacks = [learning_rate_reduction]","fdc25024":"mnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train \/ 255.0, x_test \/ 255.0","3918dcd6":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n","982deec8":"hist = model.fit(x_train, y_train,\n                    epochs=30,\n                    validation_data=(x_test, y_test),\n                    callbacks=[callbacks], verbose=2)","37702005":"epochs = range(len(hist.history['accuracy']))\nplt.plot(epochs, hist.history['accuracy'], color = 'blue', label = 'Training')\nplt.plot(epochs, hist.history['val_accuracy'], color = 'red', label = 'Validation')\nplt.legend(loc='best', shadow=True)\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.show()","1b9fe955":"test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntest = test \/ 255.0\ntest = test.values.reshape(-1, 28, 28, 1)\nresults = model.predict(test)\nresults = np.argmax(results, axis=1)\nresults = pd.Series(results, name='Label')\nsubmission = pd.concat([pd.Series(range(1, 28001), name='ImageId'), results], axis=1)\nsubmission.to_csv('mnist_kaggle_submisison.csv', index=False)\nprint('Done')","08e418fd":"### Training the Network for 30 Epochs","63fd2ab2":"### Loading Data and Normalization","12362f5c":"### Setup","edd34a3c":"### Visualizing Learning Cureves","7f548467":"### Constructing and Compiling the Network","50306223":"## Yaser Marey\nOctober 12th, 2020\n\n#### MNIST is considered a solved problem by many researchers, and in this Notebook I am demonstrating that.\n\n### The Accuracy achieved and Training Time and, Setup\n99.782% with NN of one hidden layer of 256 nodes, no convolution, trained on my local Core i7 CPU 2.8 GHZ, No GPU machine for only ~120 seconds!\n### Things I did differently\nA couple of things, at least compared to the notebooks I have seen tackling this dataset:\n#### 1. I am using the data sets loaded to Keras instead of preprocessing the dataset myself.\n#### 2. I am monitoring validation accuracy to slow down the learning rate when the change in validation accuracy is less than a threshold of 0.00001","f6fe1a21":"### Finally, Generating Submssion"}}