{"cell_type":{"ae824d89":"code","7aec1470":"code","3ae1339b":"code","892cb5eb":"code","192658b4":"code","b320979d":"code","69fa96e3":"code","bf5ac3ed":"code","b0035692":"code","6ebfd328":"code","10e2734f":"markdown","a9e4f2a3":"markdown","8d62b6e1":"markdown","0c559d39":"markdown","2a3bb95a":"markdown","b916c11a":"markdown","a5ab921b":"markdown","6866c58c":"markdown","c0bb3c49":"markdown"},"source":{"ae824d89":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7aec1470":"df=pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf.head(10)","3ae1339b":"df.describe()","892cb5eb":"feature_df=df[['age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','high_blood_pressure','platelets','serum_creatinine','serum_sodium','sex','smoking','time']]\nX=np.asarray(feature_df)\nY=y=np.asarray(df['DEATH_EVENT'])","192658b4":"#Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n","b320979d":"from sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB","69fa96e3":"e_knn=np.zeros(100);\nfor i in range(0,len(e_knn)):\n    knn_model = KNeighborsClassifier(n_neighbors=i+1)\n    knn_model.fit(X_train, y_train)\n    yh_knn=knn_model.predict(X_test)\n    e_knn[i]=accuracy_score(y_test, yh_knn)\n    \nprint(\"KNN Prediction Accuracy Score: \",np.round(e_knn.max(),3),' with N = ',e_knn.argmax()+1)\nplt.plot(np.arange(1,101),e_knn)\nplt.plot(e_knn.argmax()+1,e_knn.max(),'or')\nplt.title('KNN Accuracy Score')\nplt.xlabel('N')\nplt.ylabel('Accuracy')\nplt.show()","bf5ac3ed":"rf_model = RandomForestClassifier(criterion='gini')\nrf_model.fit(X_train, y_train)\nyh_rf=rf_model.predict(X_test)\ne_rf=accuracy_score(y_test, yh_rf)\n    \nprint(\"Random Forest Prediction Accuracy Score: \",np.round(e_rf.max(),3))","b0035692":"e_gnb=np.zeros(100)\nparams_NB = np.logspace(0,-9, num=100)\nfor i in range(0,len(params_NB)):\n    gnb_model = GaussianNB(var_smoothing=params_NB[i])\n    gnb_model.fit(X_train, y_train)\n    yh_gnb=gnb_model.predict(X_test)\n    e_gnb[i]=accuracy_score(y_test, yh_gnb)\n    \nprint(\"Naive Bayes Prediction Accuracy Score: \",np.round(e_gnb.max(),3),' with Smoothing = ',params_NB[e_gnb.argmax()])\nplt.plot(params_NB,e_gnb,'.-')\nplt.plot(params_NB[e_gnb.argmax()],e_gnb.max(),'or')\nplt.title('Naive Bayes Accuracy Score')\nplt.xscale('log')\nplt.xlabel('Var Smoothing')\nplt.ylabel('Accuracy')\nplt.show()","6ebfd328":"plt.bar(['K-Nearest Neighbors','Random Forest','Naive Bayes'],[e_knn.max(),e_rf.max(),e_gnb.max()])","10e2734f":"### K Nearest Neighbors\n\nWe will attempt to obtain the most optimum number for nearest neighbors, and obtain the highest accuracy score.","a9e4f2a3":"We can clearly see above that Random Forest algorithm provides the highest accuracy in predicting heart failure cases (>90% accuracy).","8d62b6e1":"Then we load the CSV file and inspect the dataframe.","0c559d39":"Now we assign the feature columns as variable X, and label column as Y. This establishes the ground truth for evaluating the predicted values later on.","2a3bb95a":"### Naive Bayes\n\nAnd lastly we model our prediction with Naive Bayes algorithm. The parameter to adjust will be the smoothing variable. This usually is done within a logarithmic space.","b916c11a":"We split the dataframe into training dataset and testing dataset. We will allocate 30% of the dataframe as test set.","a5ab921b":"## Conclusion","6866c58c":"# Heart Failure Prediction\n## K-Nearest Neighbors vs Random Forest vs Naive Bayes\n\nHello, in this notebook we will compare the accuracy between 3 classifier algorithms in predicting heart failure based on clinical data.\nFirst, we import the relevant libraries.","c0bb3c49":"### Random Forest\n\nOne of the most widely used algorithms for classifying labels. We will loop through 10-110 to obtain the most optimum number of estimators parameter."}}