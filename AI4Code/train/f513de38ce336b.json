{"cell_type":{"238f2dd7":"code","42c2e68a":"code","c4d3f8f9":"code","9ddeb1dd":"code","1324cb84":"code","171558a5":"code","c0963013":"code","2d7a21be":"code","f8d69e74":"code","5006db05":"code","5386ba7f":"markdown"},"source":{"238f2dd7":"# ========================================\n# Config\n# ========================================\nclass Config:\n    name = \"TF-Pairwise-Toxic-Model\"  \n    only_inference = True\n    model_name = \"..\/input\/roberta-base\"\n    margin = 0.5\n\n    head = 60\n    tail = 48\n    max_length = head + tail\n    lr = 1e-5\n    weight_decay = 1e-5\n    steps_per_epochs = 32\n\n    scheduler = dict(\n        scheduler=\"get_schedule_with_warmup\", \n        num_warmup_steps=0, \n        num_train_steps=None,\n        min_lr_ratio=0.1, \n        power=1.)\n    # scheduler = None\n    \n    early_stop = dict(\n        monitor = \"val_loss\",\n        min_delta = 0.,\n        patience = 4,\n        mode = \"min\"\n    )\n    # early_stop = None\n\n    epochs = 16\n    train_batch_size = 64\n    valid_batch_size = 256\n    test_batch_size = 256\n    \n    n_fold = 10\n    trn_fold = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    seed = 2022\n    target_col = [\"target\"]\n    debug = False\n\n    # Colab Env\n    upload_from_colab = False\n    api_path = \"\/content\/drive\/Shareddrives\/Jigsaw-Rate-Severity-of-Toxic-Comments\/mst8823\/kaggle.json\"\n    drive_path = \"\/content\/drive\/Shareddrives\/Jigsaw-Rate-Severity-of-Toxic-Comments\/mst8823\"\n    \n    # Kaggle Env\n    kaggle_dataset_path = \"..\/input\/tf-keras-pairwise-toxic-model-tpu-train\"\n\nif Config.debug:\n    Config.epochs = 2\n    Config.n_fold = 2\n    Config.trn_fold = [0]\n    Config.train_batch_size = 1\n    Config.steps_per_epochs = None\n\nprint(len(Config.name))","42c2e68a":"# ========================================\n# Library\n# ========================================\nimport os\nimport json\nimport warnings\nimport shutil\nimport logging\nimport joblib\nimport random\nimport datetime\nimport sys\nimport math\nimport time\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K","c4d3f8f9":"# ========================================\n# Utils\n# ========================================\nclass Logger:\n    \"\"\"save log\"\"\"\n    def __init__(self, path):\n        self.general_logger = logging.getLogger(path)\n        stream_handler = logging.StreamHandler()\n        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n        if len(self.general_logger.handlers) == 0:\n            self.general_logger.addHandler(stream_handler)\n            self.general_logger.addHandler(file_general_handler)\n            self.general_logger.setLevel(logging.INFO)\n\n    def info(self, message):\n        # display time\n        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n\n    @staticmethod\n    def now_string():\n        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n    \n    \ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    tf.random.set_seed(seed)\n\n\ndef read_csv(filepath, **kwargs):\n    \n    if os.path.isdir(filepath):\n        filename = filepath.split(\"\/\")[-1]\n        filepath = os.path.join(filepath, filename)\n        \n    try:\n        csv_data = pd.read_csv(filepath,  **kwargs)\n    except:\n        csv_data = pd.read_csv(filepath + \".zip\",  **kwargs)\n\n    return csv_data\n\n\ndef change_dict_key(d, old_key, new_key, default_value=None):\n    d[new_key] = d.pop(old_key, default_value)\n\n\ndef auto_select_accelerator(tpu, is_colab=True):\n\n    if is_colab:  # tpu in colab\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n\n    else:  # tpu in kaggle kernel\n        tpu_ = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n        tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu_)\n\n    print(f\"Running on {tpu_strategy.num_replicas_in_sync} replicas\")\n    return tpu_strategy\n\n\nclass GroupKFold:\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure (by katsu1110)\n    \"\"\"\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X=None, y=None, groups=None):\n        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = groups.unique()\n        for tr_group_idx, va_group_idx in kf.split(unique_ids):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(groups.isin(tr_group))[0]\n            val_idx = np.where(groups.isin(va_group))[0]\n            yield train_idx, val_idx\n\n\ndef get_leak_free_valid_idx(train):\n    \"\"\"tito notebook\"\"\"\n    df = train.copy()\n    texts = set(df.less_toxic.to_list() + df.more_toxic.to_list())\n    text2id = {t:id for id,t in enumerate(texts)}\n    df['less_id'] = df['less_toxic'].map(text2id)\n    df['more_id'] = df['more_toxic'].map(text2id)\n\n    # Set array to store pair information\n    len_ids = len(text2id)\n    idarr = np.zeros((len_ids,len_ids), dtype=bool)\n\n    for lid, mid in df[['less_id', 'more_id']].values:\n        min_id = min(lid, mid)\n        max_id = max(lid, mid)\n        idarr[max_id, min_id] = True\n\n    # Recursively retrieve the text that is paired with the text whose id is i,\n    # and store it's id in this_list.\n    # then set idarr[i, j] to False\n    def add_ids(i, this_list):\n        for j in range(len_ids):\n            if idarr[i, j]:\n                idarr[i, j] = False\n                this_list.append(j)\n                this_list = add_ids(j,this_list)\n                #print(j,i)\n        for j in range(i+1,len_ids):\n            if idarr[j, i]:\n                idarr[j, i] = False\n                this_list.append(j)\n                this_list = add_ids(j,this_list)\n                #print(j,i)\n        return this_list\n\n    group_list = []\n    for i in tqdm(range(len_ids)):\n        for j in range(i+1,len_ids):\n            if idarr[j, i]:\n                this_list = add_ids(i,[i])\n                #print(this_list)\n                group_list.append(this_list)\n\n    id2groupid = {}\n    for gid,ids in enumerate(group_list):\n        for id in ids:\n            id2groupid[id] = gid\n\n    df['less_gid'] = df['less_id'].map(id2groupid)\n    df['more_gid'] = df['more_id'].map(id2groupid)\n    return df[\"less_gid\"]","9ddeb1dd":"# ========================================\n# SetUp\n# ========================================\nCOLAB = \"google.colab\" in sys.modules\n\nif COLAB:\n    print(\"This environment is Google Colab\")\n    # import library\n    ! pip install --quiet transformers\n    ! pip install --quiet iterative-stratification\n    ! pip install --quiet tensorflow-addons\n\n    # mount\n    from google.colab import drive\n    if not os.path.isdir(\"\/content\/drive\"):\n        drive.mount('\/content\/drive') \n\n    # use kaggle api (need kaggle token)\n    f = open(Config.api_path, 'r')\n    json_data = json.load(f) \n    os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n    os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n\n    DRIVE = Config.drive_path\n    EXP = (Config.name if Config.name is not None \n           else get(\"http:\/\/172.28.0.2:9000\/api\/sessions\").json()[0][\"name\"][:-6])  # get notebook name\n    INPUT = os.path.join(DRIVE, \"Input\")\n    OUTPUT = os.path.join(DRIVE, \"Output\")\n    SUBMISSION = os.path.join(DRIVE, \"Submission\")\n    OUTPUT_EXP = os.path.join(OUTPUT, EXP) \n    EXP_MODEL = os.path.join(OUTPUT_EXP, \"model\")\n    EXP_FIG = os.path.join(OUTPUT_EXP, \"fig\")\n    EXP_PREDS = os.path.join(OUTPUT_EXP, \"preds\")\n\n    # all jigsaw input data\n    INPUT_JIGSAW_01 = os.path.join(INPUT, \"jigsaw-toxic-comment-classification-challenge\")\n    INPUT_JIGSAW_02 = os.path.join(INPUT, \"jigsaw-unintended-bias-in-toxicity-classification\")\n    INPUT_JIGSAW_03 = os.path.join(INPUT, \"jigsaw-multilingual-toxic-comment-classification\")\n    INPUT_JIGSAW_04 = os.path.join(INPUT, \"jigsaw-toxic-severity-rating\")\n    jigsaw_inputs = [INPUT_JIGSAW_01, INPUT_JIGSAW_02, INPUT_JIGSAW_03, INPUT_JIGSAW_04]\n\n    # make dirs\n    for d in [INPUT, SUBMISSION, EXP_MODEL, EXP_FIG, EXP_PREDS] + jigsaw_inputs:\n        os.makedirs(d, exist_ok=True)\n\n    if not os.path.isfile(os.path.join(INPUT_JIGSAW_04, \"comments_to_score.csv.zip\")):\n        # load dataset\n        ! kaggle competitions download -c jigsaw-toxic-comment-classification-challenge -p $INPUT_JIGSAW_01 \n        ! kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification -p $INPUT_JIGSAW_02 \n        ! kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification -p $INPUT_JIGSAW_03 \n        ! kaggle competitions download -c jigsaw-toxic-severity-rating -p $INPUT_JIGSAW_04 \n    \n    # utils\n    logger = Logger(OUTPUT_EXP)\n\nelse:\n    print(\"This environment is Kaggle Kernel\")\n    ! pip install --quiet ..\/input\/iterative-stratification\/iterative-stratification-master\n    INPUT = \"..\/input\"\n    INPUT_JIGSAW_01 = os.path.join(INPUT, \"jigsaw-toxic-comment-classification-challenge\")\n    INPUT_JIGSAW_02 = os.path.join(INPUT, \"jigsaw-unintended-bias-in-toxicity-classification\")\n    INPUT_JIGSAW_03 = os.path.join(INPUT, \"jigsaw-multilingual-toxic-comment-classification\")\n    INPUT_JIGSAW_04 = os.path.join(INPUT, \"jigsaw-toxic-severity-rating\")\n\n    EXP, OUTPUT, SUBMISSION = \".\/\", \".\/\", \".\/\"\n    EXP_MODEL = os.path.join(EXP, \"model\")\n    EXP_FIG = os.path.join(EXP, \"fig\")\n    EXP_PREDS = os.path.join(EXP, \"preds\")\n\n    if Config.kaggle_dataset_path is not None:\n        KD_MODEL = os.path.join(Config.kaggle_dataset_path, \"model\")\n        KD_EXP_PREDS = os.path.join(Config.kaggle_dataset_path, \"preds\")\n        shutil.copytree(KD_MODEL, EXP_MODEL)\n        shutil.copytree(KD_EXP_PREDS, EXP_PREDS)\n\n    # make dirs\n    for d in [EXP_MODEL, EXP_FIG, EXP_PREDS]:\n        os.makedirs(d, exist_ok=True)\n        \n    # utils\n    logger = Logger(EXP)\n\n# utils\nwarnings.filterwarnings(\"ignore\")\nsns.set(style='whitegrid')\nseed_everything(seed=Config.seed)\n\n# 2nd import\nfrom transformers import AutoTokenizer, TFAutoModel, WarmUp\nimport tensorflow_addons as tfa\n\n# set device\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# tpu\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()\n    REPLICAS = auto_select_accelerator(TPU, COLAB).num_replicas_in_sync\n    \nexcept ValueError:\n    TPU = None\n    REPLICAS = 1","1324cb84":"# ========================================\n# Load Data\n# ========================================\ntest = read_csv(os.path.join(INPUT_JIGSAW_04 , \"comments_to_score.csv\"))\nsample_submission = read_csv(os.path.join(INPUT_JIGSAW_04 , \"sample_submission.csv\"))\n\nif not Config.only_inference:\n    train = read_csv(os.path.join(INPUT_JIGSAW_04 , \"validation_data.csv\"))\n    if Config.debug:\n        train = train.sample(100).reset_index(drop=True)\n    groups = get_leak_free_valid_idx(train)\n\n    # fisrt, add fold index\n    train[\"fold\"] = -1\n    for i, lst in enumerate(\n        GroupKFold(\n            n_splits=Config.n_fold,\n            shuffle=True, \n            random_state=Config.seed\n            )\n        .split(X=train, y=train, groups=groups)):\n\n        if i in Config.trn_fold:\n            train.loc[lst[1].tolist(), \"fold\"] = i\n        \n    # add target\n    train[Config.target_col] = 1\n    display(train)","171558a5":"# ========================================\n# Data Set\n# ========================================\ndef prepare_input(text, tokenizer):\n\n    if Config.tail == 0:\n        inputs = tokenizer.batch_encode_plus(\n            text, \n            return_tensors=None, \n            add_special_tokens=True, \n            max_length=Config.max_length,\n            pad_to_max_length=True,\n            truncation=True)\n        inputs = dict(inputs)\n\n    else:\n        inputs = tokenizer.batch_encode_plus(\n            text,\n            return_tensors=None, \n            add_special_tokens=True, \n            truncation=True)\n        inputs = dict(inputs)\n        \n        for k, v_lst in inputs.items():\n\n            new_v_lst = []\n            for i in range(len(v_lst)):\n                v = v_lst[i]\n\n                v_length = len(v)\n                if v_length > Config.max_length:\n                    v = np.hstack([v[:Config.head], v[-Config.tail:]])\n\n                if k == 'input_ids':\n                    new_v = np.ones(Config.max_length) * tokenizer.pad_token_id\n\n                else:\n                    new_v = np.zeros(Config.max_length)\n\n                new_v[:v_length] = v \n                new_v_lst.append(np.array(new_v, dtype=np.int))\n\n            inputs[k] = new_v_lst\n\n    return inputs\n\n\ndef get_dataset(X, y=None, dataset=\"test\"):\n    if dataset==\"train\":\n\n        train_dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((X, y))\n            .shuffle(2048)\n            .batch(Config.train_batch_size * REPLICAS)\n            .prefetch(tf.data.experimental.AUTOTUNE)\n            )\n        if Config.steps_per_epochs is not None:\n            train_dataset = train_dataset.repeat()\n\n        return train_dataset\n\n    elif dataset==\"valid\":\n\n        valid_dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((X, y))\n            .batch(Config.valid_batch_size * REPLICAS)\n            .prefetch(tf.data.experimental.AUTOTUNE)\n        )\n        return valid_dataset\n    \n    elif dataset==\"test\":\n        test_dataset = (\n            tf.data.Dataset\n            .from_tensor_slices(X)\n            .batch(Config.test_batch_size * REPLICAS)\n            .prefetch(tf.data.experimental.AUTOTUNE)\n        )\n        return test_dataset","c0963013":"# ========================================\n# Model\n# ========================================\ndef build_toxic_model():\n    \"\"\"TFAutoModel\"\"\"\n    transformer = TFAutoModel.from_pretrained(Config.model_name)\n    input_word_ids = tf.keras.layers.Input(shape=(Config.max_length, ), dtype=tf.int32, name='input_ids')\n    attention_mask = tf.keras.layers.Input(shape=(Config.max_length, ), dtype=tf.int32, name='attention_mask')\n\n    x = transformer(input_word_ids, attention_mask=attention_mask)\n    x = x[1]\n    x = tf.keras.layers.Dropout(0.2)(x)\n\n    output = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[input_word_ids, attention_mask],\n                                  outputs=[output])\n    \n    return model\n\n\ndef build_pairwise_toxic_model(toxic_model, optimizer=None):\n    less_toxic_input_ids = tf.keras.layers.Input(shape=(Config.max_length, ), dtype=tf.int32, name='less_toxic_input_ids')\n    less_toxic_attention_mask = tf.keras.layers.Input(shape=(Config.max_length, ), dtype=tf.int32, name='less_toxic_attention_mask')\n    less_toxic_inputs = {\"input_ids\":less_toxic_input_ids, \"attention_mask\":less_toxic_attention_mask}\n\n    more_toxic_input_ids = tf.keras.layers.Input(shape=(Config.max_length, ), dtype=tf.int32, name='more_toxic_input_ids')\n    more_toxic_attention_mask = tf.keras.layers.Input(shape=(Config.max_length, ), dtype=tf.int32, name='more_toxic_attention_mask')\n    more_toxic_inputs = {\"input_ids\":more_toxic_input_ids, \"attention_mask\":more_toxic_attention_mask}\n\n    less_toxic_preds = toxic_model(less_toxic_inputs)\n    more_toxic_preds = toxic_model(more_toxic_inputs)\n    rank_diff = more_toxic_preds - less_toxic_preds\n\n    model = tf.keras.Model(\n        inputs=[less_toxic_input_ids, less_toxic_attention_mask, more_toxic_input_ids, more_toxic_attention_mask], \n        outputs=[rank_diff]\n        )\n\n    if optimizer is not None:\n        model.compile(optimizer=optimizer, loss=margin_ranking_loss, metrics=custom_accuracy)\n\n    return model\n\n\ndef margin_ranking_loss(true_rank_diff, preds_rank_diff):\n    return tf.math.maximum(Config.margin + (-tf.cast(true_rank_diff, tf.float32) * preds_rank_diff), 0)\n\n\ndef custom_accuracy(t, rank_diff): \n    num_correct = tf.math.reduce_sum(tf.where(rank_diff > 0, 1, 0))\n    num_error = tf.math.reduce_sum(tf.where(rank_diff > 0, 0, 1))\n    score = num_correct \/ (num_correct + num_error)\n    return score\n\n\ndef get_tokenizer():\n    return AutoTokenizer.from_pretrained(Config.model_name)\n","2d7a21be":"def get_score_v2(df):\n    score = len(df[df[\"less_toxic_pred\"] < df[\"more_toxic_pred\"]]) \/ len(df)\n    return score\n\n\ndef get_scheduler():\n\n    # set warmup steps if it is None\n    if \"num_train_steps\" in list(Config.scheduler.keys()):\n        if Config.scheduler[\"num_train_steps\"] is None:\n            Config.scheduler[\"num_train_steps\"] = ((Config.steps_per_epochs * Config.epochs) -\n                                                      Config.scheduler[\"num_warmup_steps\"])\n    \n    if Config.scheduler[\"scheduler\"] == \"ReduceLROnPlateau\":\n        scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_loss\", \n            factor=Config.scheduler[\"factor\"],\n            patience=Config.scheduler[\"patience\"],\n            min_lr=Config.scheduler[\"min_lr\"], \n            verbose=1\n            )\n        return scheduler\n        \n    elif Config.scheduler == \"CosineDecayRestarts\":\n        cisine_decay_r = tf.keras.experimental.CosineDecayRestarts(\n            Config.lr,\n            first_decay_steps=Config.scheduler[\"first_decay_step\"],\n            t_mul=Config.scheduler[\"t_mul\"],\n            m_mul=Config.scheduler[\"m_mul\"],\n            alpha=Config.scheduler[\"alpha\"]\n            )\n        scheduler = tf.keras.callbacks.LearningRateScheduler(cisine_decay_r, verbose=1)\n        return scheduler\n    \n            \n    elif Config.scheduler[\"scheduler\"] == \"get_schedule_with_warmup\":\n\n        scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n            initial_learning_rate=Config.lr,\n            decay_steps=Config.scheduler[\"num_train_steps\"] - Config.scheduler[\"num_warmup_steps\"],\n            end_learning_rate=Config.lr * Config.scheduler[\"min_lr_ratio\"],\n            power=Config.scheduler[\"power\"],\n        )\n\n        if Config.scheduler[\"num_warmup_steps\"] > 0:\n            scheduler = WarmUp(\n                initial_learning_rate=Config.lr,\n                decay_schedule_fn=scheduler,\n                warmup_steps=Config.scheduler[\"num_warmup_steps\"],\n            )\n        optimizer = tfa.optimizers.AdamW(learning_rate=scheduler, weight_decay=Config.weight_decay)\n        return scheduler, optimizer\n\n    else:\n        raise NotImplementedError\n    \n    return scheduler\n\n\ndef get_pairwise_tocix_inputs(df, tokenizer):\n\n    less_toxic_text = prepare_input(df[\"less_toxic\"].fillna(\"none\").tolist(), tokenizer=tokenizer)\n    more_toxic_text = prepare_input(df[\"more_toxic\"].fillna(\"none\").tolist(), tokenizer=tokenizer)\n\n    change_dict_key(less_toxic_text, \"input_ids\", \"less_toxic_input_ids\")\n    change_dict_key(less_toxic_text, \"attention_mask\", \"less_toxic_attention_mask\")\n    change_dict_key(more_toxic_text, \"input_ids\", \"more_toxic_input_ids\")\n    change_dict_key(more_toxic_text, \"attention_mask\", \"more_toxic_attention_mask\")\n\n    out_text = {**less_toxic_text, **more_toxic_text}\n    return out_text\n\n\ndef training_v2(train_df, valid_df, filepath):\n\n    if Config.steps_per_epochs is None:\n        Config.steps_per_epochs = len(train_df) \/\/ Config.train_batch_size\n    \n    # model setting\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath, \n        monitor=\"val_loss\", \n        verbose=1, \n        save_best_only=True, \n        save_weights_only=True,\n        mode=\"min\")\n    \n    callbacks = [checkpoint]\n    optimizer = tfa.optimizers.AdamW(learning_rate=Config.lr, weight_decay=Config.weight_decay)\n    if Config.scheduler is not None:\n        if Config.scheduler[\"scheduler\"] == \"get_schedule_with_warmup\":\n            _,  optimizer = get_scheduler()  # new optimizer\n        else:\n            scheduler = get_scheduler()\n            callbacks += [scheduler]\n\n    if Config.early_stop is not None:\n        early_stop = tf.keras.callbacks.EarlyStopping(\n            monitor=Config.early_stop[\"monitor\"],\n            min_delta=Config.early_stop[\"min_delta\"],\n            patience=Config.early_stop[\"patience\"], \n            mode=Config.early_stop[\"mode\"],\n            verbose=1\n        )\n        callbacks += [early_stop]\n\n    # get model :TPU setting\n    if TPU:\n        tpu_strategy = auto_select_accelerator(TPU, COLAB)\n        with tpu_strategy.scope():\n            toxic_model = build_toxic_model()\n            pairwise_toxic_model = build_pairwise_toxic_model(toxic_model, optimizer)\n\n    else:\n        toxic_model = build_toxic_model()\n        pairwise_toxic_model = build_pairwise_toxic_model(toxic_model, optimizer)\n\n    # tf dataset\n    tokenizer = get_tokenizer()\n    tr_x_inputs = get_pairwise_tocix_inputs(train_df, tokenizer)\n    va_x_inputs = get_pairwise_tocix_inputs(valid_df, tokenizer)\n    tr_y = train_df[Config.target_col].values\n    va_y = valid_df[Config.target_col].values\n\n    tr_dataset = get_dataset(X=tr_x_inputs, y=tr_y, dataset=\"train\")\n    va_dataset = get_dataset(X=va_x_inputs, y=va_y, dataset=\"valid\")\n\n    # training pairwise toxic model\n    pairwise_toxic_model.fit(\n        tr_dataset, \n        epochs=Config.epochs, \n        verbose=1, \n        callbacks=callbacks,\n        validation_data=va_dataset, \n        steps_per_epoch=Config.steps_per_epochs)\n\n    # save toxic model weights\n    toxic_model = build_toxic_model()\n    pairwise_toxic_model = build_pairwise_toxic_model(toxic_model)\n    pairwise_toxic_model.load_weights(filepath)  # use check point weights\n    toxic_model.save_weights(filepath)\n    \n\ndef inference_v2(text, filepath):\n    tokenizer = get_tokenizer()\n\n    # get model :TPU setting\n    if TPU:\n        tpu_strategy = auto_select_accelerator(TPU, COLAB)\n        with tpu_strategy.scope():\n            toxic_model = build_toxic_model()\n    else:\n        toxic_model = build_toxic_model()\n        \n    toxic_model.load_weights(filepath)\n    toxic_inputs = prepare_input(text, tokenizer)\n    te_dataset = get_dataset(toxic_inputs, dataset=\"test\")\n\n    preds = toxic_model.predict(te_dataset)\n    return preds\n\n\ndef train_cv_v2(train):\n\n    oof_cols = [\"worker\", \"less_toxic\", \"more_toxic\", \"fold\", \n                \"less_toxic_pred\", \"more_toxic_pred\"]\n    oof_df = pd.DataFrame(np.zeros((len(train), len(oof_cols))), columns=oof_cols)\n\n    for i_fold in range(Config.n_fold):\n        if i_fold in Config.trn_fold:\n            K.clear_session()\n            filepath = os.path.join(\n                        EXP_MODEL,\n                        f\"{Config.name}-seed{Config.seed}-fold{i_fold}.h5\")\n            \n            tr_df, va_df = (train[train[\"fold\"] != i_fold].reset_index(drop=True),\n                            train[train[\"fold\"] == i_fold].reset_index(drop=True))\n                        \n            if not os.path.isfile(filepath):  # if trained model, no training\n                training_v2(tr_df, va_df, filepath)\n            \n            va_text = list(sorted(set(va_df[\"less_toxic\"].unique()) | set(va_df[\"more_toxic\"].unique())))\n            va_preds = inference_v2(va_text, filepath)\n            _df = pd.DataFrame({\"text\":va_text, \"pred\":np.concatenate(va_preds)})\n \n            preds_df = pd.merge(\n                va_df, \n                _df.rename(columns={\"text\": \"less_toxic\", \"pred\": \"less_toxic_pred\"}),\n                on=\"less_toxic\", how=\"left\")\n            \n            preds_df = pd.merge(\n                preds_df, \n                _df.rename(columns={\"text\": \"more_toxic\", \"pred\": \"more_toxic_pred\"}),\n                on=\"more_toxic\", how=\"left\")\n            oof_df.loc[train[\"fold\"] == i_fold, oof_cols] = preds_df[oof_cols].values\n\n            # fold score\n            score = get_score_v2(preds_df)\n            logger.info(f\"{Config.name}-seed{Config.seed}-fold{i_fold} >>>>> Score={score:.4f}\")\n\n    # overall score\n    score = get_score_v2(oof_df[[\"less_toxic_pred\", \"more_toxic_pred\"]])\n    logger.info(f\"{Config.name}-seed{Config.seed}-OOF-Score >>>>> Score={score:.4f}\")\n\n    return oof_df.reset_index(drop=True)\n\n\ndef predict_cv_v2(text):\n    K.clear_session()\n\n    preds_fold = []\n    preds_fold_df = pd.DataFrame()\n\n    for i_fold in range(Config.n_fold):\n        if i_fold in Config.trn_fold:\n            filepath = os.path.join(\n                EXP_MODEL,\n                f\"{Config.name}-seed{Config.seed}-fold{i_fold}.h5\")\n            \n            preds = inference_v2(text, filepath)\n            preds_fold.append(preds)\n            preds_fold_df[f\"FOLD={i_fold:02}\"] = np.concatenate(preds)    \n\n    return preds_fold, preds_fold_df","f8d69e74":"# ========================================\n# Main\n# ========================================\nif not Config.only_inference:\n    # training\n    print(\"# ---------- # Start Training # ---------- #\")\n    oof_df = train_cv_v2(train)\n    oof_df.to_csv(os.path.join(EXP_PREDS, \"oof.csv\"), index=False)\n\n    fold_mask = train[\"fold\"].isin(Config.trn_fold)\n    score = get_score_v2(oof_df.loc[fold_mask, [\"less_toxic_pred\", \"more_toxic_pred\"]])\n    logger.info(f\"Jigsaw04-Jigsaw-Rate-Severity={score:.4f}\")\n\n# prediction\nprint(\"# ---------- # Start Inference # ---------- #\")\npreds_fold, preds_fold_df = predict_cv_v2(test[\"text\"].fillna(\"none\").tolist())\npreds_fold_df.to_csv(os.path.join(EXP_PREDS, f\"comments_to_score_preds_fold_df.csv\"), index=False)\n\n# make submission\nprint(\"# ---------- # Make Submission # ---------- #\")\nsample_submission[\"score\"] = np.mean(preds_fold, axis=0)\nsample_submission[\"score\"] = sample_submission[\"score\"].rank(method='first') # to rank\ndisplay(sample_submission)\nfilename = Config.name + \".csv\" if COLAB else \"submission.csv\"\nsample_submission.to_csv(os.path.join(SUBMISSION, filename), index=False)\nprint(\"# ---------- # Finish Experiment!! # ---------- #\")","5006db05":"# upload output folder to kaggle dataset\nif Config.upload_from_colab:\n    from kaggle.api.kaggle_api_extended import KaggleApi\n\n    def dataset_create_new(dataset_name, upload_dir):\n        dataset_metadata = {}\n        dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}\/{dataset_name}'\n        dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n        dataset_metadata['title'] = dataset_name\n        with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n            json.dump(dataset_metadata, f, indent=4)\n        api = KaggleApi()\n        api.authenticate()\n        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n\n    if len(EXP) >= 50:\n        dataset_name = EXP[:7]\n    else:\n        dataset_name = EXP\n\n    dataset_create_new(dataset_name=dataset_name, upload_dir=OUTPUT_EXP)","5386ba7f":"1. This notebook is an implementation of the Margin Ranking Loss model using TF \/ Keras.\n2. It uses TPU for training. (GPU for inference)\n3. roberta base.\n\n- reference\n    - https:\/\/www.kaggle.com\/yasufuminakama\/jigsaw4-luke-base-starter-train\/notebook\n    - https:\/\/www.kaggle.com\/quincyqiang\/download-huggingface-pretrain-for-kaggle\n    - https:\/\/www.kaggle.com\/its7171\/jigsaw-cv-strategy\n\ntraining notebook is [here](https:\/\/www.kaggle.com\/mst8823\/tf-keras-pairwise-toxic-model-tpu-train)"}}