{"cell_type":{"776a3da4":"code","39b5004b":"code","70bba8a3":"code","8511d28c":"code","9d6f49d8":"code","aada883a":"code","c72b2a96":"code","0fec9050":"code","d1cf7889":"code","51dd0ce5":"code","6eadbae3":"code","59bf5914":"code","ae6aca8d":"code","b9a91f03":"code","debf4ebd":"code","0d4165f3":"code","ea7b30da":"code","70e2f62c":"code","087c9101":"code","c4c1eeb7":"code","852570af":"code","402bdcb5":"code","5197b47f":"code","3b378331":"code","2cf76eb8":"code","5923fadd":"code","7242ff90":"code","7c163b27":"code","17a1708b":"code","23b19bbe":"code","a3eb8868":"code","51569a71":"code","422a9053":"code","b3597109":"code","62ccacb9":"code","6f5d0d51":"code","d7730cbb":"code","8f3ef894":"code","281e1388":"code","91d04382":"code","e2810867":"code","3fb49b79":"code","db190710":"code","bbd87c42":"code","501c747b":"code","a34fb4a8":"code","7230ec50":"code","c4ea813f":"code","cc69254b":"code","4c98f336":"code","ad0a9f58":"code","356132c3":"code","81e9c642":"code","debd0c60":"code","9f3672d7":"code","8d0793a7":"code","68939279":"code","cce789bd":"code","fd929ebd":"code","d471dbeb":"code","498c15f1":"code","1ffd5798":"code","80f7ff31":"code","4e15dec2":"code","2f0217cb":"markdown","aead42eb":"markdown","44a3b518":"markdown","05be2e2d":"markdown","ad55952e":"markdown","abb0f706":"markdown","b438529b":"markdown","676d390d":"markdown","b7c05f19":"markdown","ecb0581c":"markdown","c5e7a504":"markdown","ef0fb856":"markdown","18947648":"markdown","f9373d2b":"markdown","a7943fe9":"markdown","f4c91ea3":"markdown","087c33de":"markdown","4ce4f0b7":"markdown","508c1b82":"markdown","fdc1f3f6":"markdown","b04e7a73":"markdown","f3198d4f":"markdown","5b1886b1":"markdown","14ebfdab":"markdown","c8e23211":"markdown","5e6bd317":"markdown","56ce409b":"markdown","91de6f98":"markdown","611558a5":"markdown","a5c75fe5":"markdown","52321f69":"markdown"},"source":{"776a3da4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39b5004b":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","70bba8a3":"df=pd.read_csv('\/kaggle\/input\/summer-products-and-sales-in-ecommerce-wish\/summer-products-with-rating-and-performance_2020-08.csv')\ndf.head()","8511d28c":"test_list = [\"Men\", \"Men's\", \"Man\",\"Man's\"]\ntest = [\"F\" if \"Women\" in item else \"M\" for item in df['title_orig']]\ntest2 = [\"F\" if \"Women's\" in item else \"M\" for item in df['title_orig']]\n\n\ndf['Gender'] = ['F' if 'F' in (test[i] or test2[i]) else 'M' for i,x in enumerate(test)]","9d6f49d8":"df.drop(columns=['title','currency_buyer','merchant_id','merchant_has_profile_picture','merchant_profile_picture','product_url','product_picture','product_id'], inplace=True)","aada883a":"df.drop(columns='crawl_month',inplace=True)","c72b2a96":"print(df['urgency_text'].value_counts())\nprint(df['urgency_text'].isna().sum())","0fec9050":"df['urgency_text'].replace(np.nan,'N',inplace=True)\ndf['urgency_text'].replace('Quantit\u00e9 limit\u00e9e !', 'Y',inplace=True)\ndf['urgency_text'].replace('R\u00e9duction sur les achats en gros', 'Y', inplace=True)\ndf.drop(columns='has_urgency_banner',inplace=True)","d1cf7889":"df['origin_country'].value_counts()","51dd0ce5":"df.replace(['US','VE','SG','GB','AT'],'Other',inplace=True)\ndf['origin_country'].value_counts()","6eadbae3":"df['inventory_total'] = [\"Full\" if ele == 50 else \"Not Full\" for ele in df['inventory_total']]","59bf5914":"df.inventory_total.value_counts()","ae6aca8d":"df[['merchant_title','merchant_name']][:15]","b9a91f03":"counts = pd.DataFrame(df['merchant_title'].value_counts())\ndf['repeat'] = ['Y' if counts.loc[ele][0] > 1 else \"N\" for ele in df['merchant_title']]\ndf.drop(columns=['merchant_title','merchant_name','merchant_info_subtitle'],inplace=True)\ndf.drop(columns='theme',inplace=True)\ndf.drop(columns='product_color',inplace=True)\n","debf4ebd":"items = ['Shirt','Dress','Shorts','Pants','Skirt','Sweater']","0d4165f3":"clothes_test = [np.nan]*1573\nfor item in items:\n    for ind,ele in enumerate(df['title_orig']):\n        if clothes_test[ind] is np.nan and item in ele:\n            clothes_test[ind]=item","ea7b30da":"clothes = pd.DataFrame(clothes_test)","70e2f62c":"print(clothes.value_counts())\nprint(clothes.value_counts().sum())","087c9101":"df['Clothing'] = clothes","c4c1eeb7":"df[df['Clothing'].isna()][['title_orig','tags']]","852570af":"items_round2 = ['beachwear', 'beach wear','swimsuit','romper','jumpsuit','t-shirts','blouse']\nfor item in items_round2:\n    for ind,ele in enumerate(df['tags']):\n        if clothes_test[ind] is np.nan and item in ele:\n            clothes_test[ind]=item","402bdcb5":"clothes = pd.DataFrame(clothes_test)\ndf['Clothing'] = clothes","5197b47f":"df['Clothing'].value_counts(dropna=False)","3b378331":"df[df['Clothing'].isna()][['title_orig','tags']]","2cf76eb8":"items_round3 = ['bikini', 'Bikini','T-shirt','Shorts','Vest','Tank','tank']\nfor item in items_round3:\n    for ind,ele in enumerate(df['tags']):\n        if clothes_test[ind] is np.nan and item in ele:\n            clothes_test[ind]=item","5923fadd":"clothes = pd.DataFrame(clothes_test)\ndf['Clothing'] = clothes\ndf['Clothing'].value_counts(dropna=False)","7242ff90":"df['Clothing'].replace(['T-shirt','t-shirts'], value='Shirt', inplace=True)\ndf['Clothing'].replace(['Vest','sweater','Sweater','blouse'], value='Blouse', inplace=True)\ndf['Clothing'].replace(['beachwear','Bikini', 'bikini', 'beach wear','swimsuit'], value='Swimsuit', inplace=True)\ndf['Clothing'].replace(['romper','jumpsuit'], value='Romper', inplace=True)\ndf['Clothing'].replace(['tank'], value='Tank', inplace=True)\ndf['Clothing'].replace(np.nan, value='Other', inplace=True)\n\ndf['Clothing'].value_counts(dropna=False)","7c163b27":"df.drop(columns=['title_orig','tags'],inplace=True)","17a1708b":"drops= ['merchant_rating','merchant_rating_count','shipping_is_express','shipping_option_price','product_variation_inventory',\n       'badge_fast_shipping','badge_product_quality','badge_local_product','badges_count','shipping_option_name']\ndf.drop(columns=drops,inplace=True)","23b19bbe":"discount = ((df['price']-df['retail_price'])\/df['retail_price'])*-100","a3eb8868":"df['Discount']=discount\ndf[['price','retail_price','Discount','units_sold']]","51569a71":"corr = df.corr()\nfig, ax = plt.subplots(figsize=(15, 10))\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\ndropSelf = np.zeros_like(corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=colormap, linewidths=.5, annot=True, fmt=\".2f\", mask=dropSelf)\nplt.title('Correlation Analysis');","422a9053":"df.isna().sum()","b3597109":"df['rating_five_count'].fillna(df['rating_five_count'].mean(), inplace=True)\ndf['rating_four_count'].fillna(df['rating_four_count'].mean(), inplace=True)\ndf['rating_three_count'].fillna(df['rating_three_count'].mean(), inplace=True)\ndf['rating_two_count'].fillna(df['rating_two_count'].mean(), inplace=True)\ndf['rating_one_count'].fillna(df['rating_one_count'].mean(), inplace=True)\ndf['origin_country'].fillna(df['origin_country'].mode()[0],inplace=True)\ndf.drop(columns='product_variation_size_id',inplace=True)","62ccacb9":"plt.figure(figsize=(18,10))\nsns.distplot(df.price, label=\"Sale Price\")\nsns.distplot(df.retail_price, label = \"Retail Price\")\nplt.legend()\nplt.xlabel(\"EUR\")\nplt.title(\"Retail and Sale Price Distributions\")\nplt.show()","6f5d0d51":"plt.figure(figsize=(15,5))\nsns.boxplot(x=df.price)\nplt.xlabel(\"EUR\")\nplt.title(\"Price Distribution\")\nplt.show()\nplt.figure(figsize=(15,5))\nsns.boxplot(x=df.retail_price)\nplt.title(\"Retail Price Distribution\")\nplt.xlabel(\"EUR\")\nplt.show()","d7730cbb":"result_sold = df.groupby(\"Clothing\")['units_sold'].sum().reset_index().sort_values(by='units_sold')\nresult_discount = df.groupby(\"Clothing\")['Discount'].mean().reset_index().sort_values(by='Discount')","8f3ef894":"f,(ax1,ax2) = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x='Clothing',y='units_sold',data=result_sold, order=result_sold['Clothing'], ax=ax1)\nax1.set_xlabel(\"Clothing Category\")\nax1.set_ylabel(\"Amount Sold\")\nax1.set_title(\"Cummulative Sales per Clothing Category\")\nsns.barplot(x='Clothing',y='Discount',data=df, order=result_discount['Clothing'], ax=ax2)\nax2.set_xlabel(\"Clothing Category\")\nax2.set_ylabel(\"Discount %\")\nax2.set_title(\"Discount % per Clothing Category\")\n\n","281e1388":"df['rating_bins'] = pd.cut(df['rating'],bins=[0,1,2,3,4,5], labels=['1*','2*','3*','4*','5*'])","91d04382":"ratings_sold = df.groupby(\"rating_bins\")['units_sold'].sum().reset_index().sort_values(by='units_sold')\nratings_discount = df.groupby(\"rating_bins\")['Discount'].mean().reset_index().sort_values(by='Discount')","e2810867":"f,(ax1,ax2) = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x='rating_bins',y='units_sold',data=ratings_sold, order=ratings_sold['rating_bins'], ax = ax1)\nax1.set_xlabel(\"Ratings\")\nax1.set_ylabel(\"Amount Sold\")\nax1.set_title(\"Cummulative Sales by Ratings\")\nsns.barplot(x='rating_bins', y='Discount', data=ratings_discount, order=ratings_discount['rating_bins'], ax=ax2)\nax2.set_xlabel(\"Ratings\")\nax2.set_ylabel(\"Discount %\")\nax2.set_title(\"Discount % by Ratings\")\n","3fb49b79":"price_bins = df.groupby('rating_bins')['price'].mean().reset_index().sort_values('price')\ndiscount_bins = df.groupby('rating_bins')['retail_price'].mean().reset_index().sort_values('retail_price')","db190710":"f,(ax1,ax2) = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x='rating_bins', y='price', data=price_bins, order=price_bins['rating_bins'], ax=ax1)\nax1.set_xlabel('Ratings')\nax1.set_ylabel('Sale Price')\nax1.set_title(\"Sale Price vs Ratings\")\nsns.barplot(x='rating_bins', y='retail_price', data=discount_bins, order=discount_bins['rating_bins'],ax=ax2)\nax2.set_xlabel(\"Ratings\")\nax2.set_ylabel(\"Retail Price\")\nax2.set_title(\"Retail Price vs Ratings\")","bbd87c42":"sns.distplot(df['Discount'])","501c747b":"df[['Discount']].describe()","a34fb4a8":"df['Sale'] = df['Discount']>df['Discount'].mean()\ndf['Sale'].replace({False:0, True:1}, inplace=True)","7230ec50":"ratings_sold2 = df.groupby([\"rating_bins\", \"uses_ad_boosts\"])['units_sold'].sum().reset_index().sort_values(by='units_sold')\nratings_discount2 = df.groupby([\"rating_bins\",\"uses_ad_boosts\"])['Discount'].mean().reset_index().sort_values(by='Discount')","c4ea813f":"f,(ax1,ax2) = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x='rating_bins',y='units_sold', hue='uses_ad_boosts', data=ratings_sold2, ax = ax1)\nax1.set_xlabel(\"Ratings\")\nax1.set_ylabel(\"Amount Sold\")\nax1.set_title(\"Cummulative Sales by Ratings\")\nsns.barplot(x='rating_bins', y='Discount', hue='uses_ad_boosts', data=ratings_discount2, ax=ax2)\nax2.set_xlabel(\"Ratings\")\nax2.set_ylabel(\"Discount %\")\nax2.set_title(\"Discount % by Ratings\")","cc69254b":"price_bins2 = df.groupby(['rating_bins','uses_ad_boosts'])['price'].mean().reset_index().sort_values('price')\ndiscount_bins2 = df.groupby(['rating_bins','uses_ad_boosts'])['retail_price'].mean().reset_index().sort_values('retail_price')","4c98f336":"f,(ax1,ax2) = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x='rating_bins', y='price', hue='uses_ad_boosts', data=price_bins2, ax=ax1)\nax1.set_xlabel('Ratings')\nax1.set_ylabel('Sale Price')\nax1.set_title(\"Sale Price vs Ratings\")\nsns.barplot(x='rating_bins', y='retail_price', hue='uses_ad_boosts', data=discount_bins2,ax=ax2)\nax2.set_xlabel(\"Ratings\")\nax2.set_ylabel(\"Retail Price\")\nax2.set_title(\"Retail Price vs Ratings\")","ad0a9f58":"ratings_sold3 = df.groupby([\"rating_bins\", \"Sale\"])['units_sold'].sum().reset_index().sort_values(by='units_sold')\nratings_discount3 = df.groupby([\"rating_bins\",\"Sale\"])['Discount'].mean().reset_index().sort_values(by='Discount')","356132c3":"f,(ax1,ax2) = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x='rating_bins',y='units_sold', hue='Sale', data=ratings_sold3, ax = ax1)\nax1.set_xlabel(\"Ratings\")\nax1.set_ylabel(\"Amount Sold\")\nax1.set_title(\"Cummulative Sales by Ratings\")\nsns.barplot(x='rating_bins', y='Discount', hue='Sale', data=ratings_discount3, ax=ax2)\nax2.set_xlabel(\"Ratings\")\nax2.set_ylabel(\"Discount %\")\nax2.set_title(\"Discount % by Ratings\")","81e9c642":"price_bins3 = df.groupby(['rating_bins','Sale'])['price'].mean().reset_index().sort_values('price')\ndiscount_bins3 = df.groupby(['rating_bins','Sale'])['retail_price'].mean().reset_index().sort_values('retail_price')","debd0c60":"f,(ax1,ax2) = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x='rating_bins', y='price', hue='Sale', data=price_bins3, ax=ax1)\nax1.set_xlabel('Ratings')\nax1.set_ylabel('Sale Price')\nax1.set_title(\"Sale Price vs Ratings\")\nsns.barplot(x='rating_bins', y='retail_price', hue='Sale', data=discount_bins3,ax=ax2)\nax2.set_xlabel(\"Ratings\")\nax2.set_ylabel(\"Retail Price\")\nax2.set_title(\"Retail Price vs Ratings\")","9f3672d7":"num_cols=['price','retail_price','uses_ad_boosts','rating','rating_count','countries_shipped_to','Discount','Sale']\ncat_cols=['inventory_total','urgency_text','origin_country','Gender','repeat','Clothing','rating_bins']","8d0793a7":"X = df[cat_cols+num_cols]","68939279":"data_map = {'inventory_total':{'Full':1, 'Not Full': 0},\n            'urgency_text' : {'Y': 1, 'N':0},\n            'origin_country': {'CN': 1, 'Other':0},\n            'Gender' : {'F':1, 'M':0},\n            'repeat' : {'Y' : 1, 'N' : 0},\n            'Clothing' : {'Shirt' : 1, 'Dress':2,'Swimsuit':3,'Shorts':4,'Romper':5,'Blouse':6,'Pants':7,'Tank':8,'Other':9,'Skirt':10},\n            'rating_bins' : {'1*':1,'2*':2,'3*':3,'4*':4,'5*':5}\n}","cce789bd":"num_feats = X.select_dtypes(include=[\"int64\",\"float64\"]).columns","fd929ebd":"num_feats","d471dbeb":"X.replace(data_map, inplace=True)\ny = df[['units_sold']]","498c15f1":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","1ffd5798":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])","80f7ff31":"from sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, num_feats)])","4e15dec2":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier()\n    ]\nfor classifier in classifiers:\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', classifier)])\n    pipe.fit(X_train, y_train)   \n    print(classifier)\n    print(\"model score: %.3f\" % pipe.score(X_test, y_test))","2f0217cb":"We're definitely getting closer. There are a few more similar words or phrasings that we should be able to encapsilate before we re-bin some of the categories.","aead42eb":"<h1>Data Visualization<\/h1>","44a3b518":"Interestingly, the actual sale price largely follows the expected relationship of paying more for a product with higher customer reviews. However, this doesn't seem to be the case with the original retail price. Instead, lower rated products retail significantly more than higher or medium ranked products.\n\nThis largely follows the previous chart of discount percent by rating. These originally low rated, high retail products are sold at some of the highest discount percentages. Ultimately this suggests a correlation between initial retail, ratings, and the end need to discount products to induce sales.","05be2e2d":"Again we can see a largely inverse relationship between ratings and discount percentages as a function of sales. In other words, if a product is successfully selling, then it is unlikely to be discounted. Or at the very least, it will be discounted at a much lower percent.\n\nInterestingly, middle to upper middle products appear to sell the most items by cummulative counts. This may be a function of percentage breakdown of product ratings, and won't be analyized this time, but would certainly be a factor to consider.","ad55952e":"Overall, it appears with the exception of dresses, there is an overall inverse relationship between cummulative sales by clothing category to the discount percent for said category. In other words, the higher the percentage of discount (more negative per the above graph), then the larger overall volume of clothes sales!","abb0f706":"Here's an interesting observation: The retail price for non-sale items are all around $8. In contrast, there is a wide range of retail prices corresponding to sale items. This intuitively makes sense though. If a product is initially listed at high retail and unable to sell, then ultimately it would be listed at a discounted price.","b438529b":"The issue with our discount distribution is is is essentially two set of data merged into one. Let's create another feature that tracks if there is or is not a discount. We will use our mean of the data to establish our boundary. At ~25% we will say anything above is on sale, and anything below is selling at normal retail price.","676d390d":"Interestingly, it would appear that ratings hold far stronger correlation to unit sales than price. Although this might be construed as leaked data, we will consider it a feature since people often look at stars and ratings when assessing whether or not to buy a product.\n\nAdditionally, the percent discount doesn't appear to drive sales as much as originally anticipated. Let's transition into some more visualizations to delve deeper into these hypothesis.","b7c05f19":"Definitely confirms that there is a wide range of retail prices, but they are fairly well normalized once they are actually sold.","ecb0581c":"Next, let's take a look at the relationship between ratings and cost. Do better rated products inherently cost more?","c5e7a504":"We will wrap up our model construction here, without getting too into the details of hyperparameter tuning at this point. As we can see from our initial runs, Random Forest Classification and Gradient Boosting Classification yielded similar accuracies of ~75%. Not too bad for an initial model and no further tuning.","ef0fb856":"We can see that some merchants have repeat occurances of several key products. Not sure if this is useful at the moment, but we will create a feature to track repeat merchants and drop the title and name columns.","18947648":"Let's continue to scrape through the title_orig and tags features to extract the main type of item any given product is. Narrowing down the products to a finite grouping should prove more significant than sentence or list structures of words.","f9373d2b":"Interesting to see that units sold has a higher correlation with the count of individual star reviews vice the overall rating. Let's go ahead and trim away a few more of the features that are likely leaks (such as shipping price to overall price).","a7943fe9":"We've started creating a converted array that extracted the type of clothing from the original title. We still have ~500 NaN values remaining. Let's explore them and see if a common clothing article was missed while feature engineering.","f4c91ea3":"Obviously, a product that is not on sale will have a negative discount percent (price markup). So this graph isn't necessarily a great insight.","087c33de":"Overall, there doesn't appear to be a major change in retail or sale price based upon the use of ads. Now let's try using the binary sale feature for evaluation.","4ce4f0b7":"Create a new column that extracts out the gender from the title. We will futher feature engineer this to a number later.","508c1b82":"The crawl month is just the month that the web crawler supplied the data. This isn't going to be useful for analysis since all months are 08\/2020","fdc1f3f6":"<h1>Background<\/h1>\n<p> We are provided sales information for a Chinese online retailer, Wish. For those unfamiliar with Wish, think of it as a discounted price version of Amazon. For our analysis, we will be focusing on clothing sales during the month of August. Specifically we will look to address the following quesitons:\n    <ul>\n        <li>Are sales sensitive to price drops?<\/li>\n        <li>Do bad products sell?<\/li>\n        <li>What is the relationship between product quality (as determined by ratings), price, and sales?<\/li>\n    <\/ul>  \n<\/p>","b04e7a73":"<h1>Model Construction<\/h1>\n<p>Finally, let's try to predict unit sales based upon our feature engineering.<\/p>","f3198d4f":"<h1>Data Exploration and Feature Engineering<\/h1>","5b1886b1":"<h1>Conclusion<\/h1>\n<p>Let's recap on what we set out to accomplish:\n    <ul>\n        <li> Evaluate if humans are sensitive to discount percents: \u2714<\/li>\n        <li> Evaluate the success of bad product sales: \u2714<\/li>\n        <li> Evaluate the relationship between product quality (based on user rating), sales, and price: \u2714<\/li>\n    <\/ul>\nFurther areas of imporovement include: \n<ul>\n    <li>Further feature engineering<\/li>\n    <li>Additional pipelines for hyperparameter tuning<\/li>\n    <li>Further statistical analysis on the success of model selection<\/li>\n<\/ul>\n\nAs a newer practitioner, please provide feedback and input below! Thanks!\n    \n<\/p>","14ebfdab":"Now let's break down our previous graphs into categories based on using ads to boost sales and the binary assessment of product being on sale.","c8e23211":"The use of ads does not appear to correlate to cummulative sales or discount percentages with regard to user rating reviews. Let's see how it correlates to sale price and retail price.","5e6bd317":"Most items sold have a long tail of values for their retail price. However, notice that most of sale prices seem rather normally distributed. Let's take a look at the relative amounts of outliers for each batch compared to their own means and standard deviations.","56ce409b":"Lets bin all the other countries together. Either from CN or from elsewhere.","91de6f98":"Feature engineer the urgency_text column to reflect whether or not the product had an urgent text or sale pitch associated with the sale.","611558a5":"Down to 86! Not too bad. Let's stop there for now, re-group some of the categories and see what we've got.","a5c75fe5":"I have clearly overlooked a few key categories. Lets re-create our items key list to include things like beachwear, swimsuit, romper, jumpsuit, t-shirts, and blouse. Let's also switch gears and try to iterate over the tags feature rather than the title this time.","52321f69":"Let's run a correlation matrix to see where we stand with some of the inner-relation between some of the quantitative features."}}