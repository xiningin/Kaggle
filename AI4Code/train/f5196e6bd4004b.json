{"cell_type":{"d55a3e69":"code","e71a7231":"code","47be2102":"code","db0e35e9":"code","b077b775":"code","a4e8e98d":"code","f4765695":"code","bd4c1e2b":"code","6cca9583":"markdown","75ec57b9":"markdown","22ed0b6a":"markdown"},"source":{"d55a3e69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e71a7231":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.model_selection import cross_validate","47be2102":"import pandas as pd\ntrain = pd.read_csv('..\/input\/learn-together\/train.csv', index_col = 'Id')\ntest = pd.read_csv('..\/input\/learn-together\/test.csv', index_col = 'Id')\ntrain.columns\ntrain.info()\ntrain.head()","db0e35e9":"# Make a copy of train df for ML experiments\ntrain_2 = train.copy()\ntrain_2.columns","b077b775":"# Separate feature and target arrays as X and y\nX = train_2.drop('Cover_Type', axis = 1)\ny=train_2.Cover_Type\nprint(X.columns)\ny[:5]","a4e8e98d":"# Parameters for Random Forest Hyperparameter tuning\nparam_grid_rf = {'n_estimators': np.logspace(2,3.5,8).astype(int),\n                 'max_features': [0.1,0.3,0.5,0.7,0.9],\n                 'max_depth': np.logspace(0,3,10).astype(int),\n                 'min_samples_split': [2, 5, 10],\n                 'min_samples_leaf': [1, 2, 4],\n                 'bootstrap':[True, False]}\n\n# Instantiate RandomForestClassifier\nrf = RandomForestClassifier(random_state = 42)\n\n# Create a Random Search Parameter Grid\ngrid_rf = RandomizedSearchCV(estimator=rf, \n                          param_distributions=param_grid_rf, \n                          n_iter=100,\n                          cv=3, \n                          verbose=3, \n                          n_jobs=1,\n                          scoring = {'NLL':'neg_log_loss', 'Accuracy':'accuracy'}, \n                          refit='NLL')\n# Fit Random Forest Models - 100 models each with 3 rounds of cross validation - 300 fitted models\ngrid_rf.fit(X,y)\nprint('The Best Random Forest Estimator is: ', grid_rf.best_estimator_)\nprint('The Best Random Forest Parameters are: ', grid_rf.best_params_)\nprint('The Best Random Forest score is: ', grid_rf.best_score_)","f4765695":"# Parameters for Extremely Randomized Trees Hyperparameter tuning\nparam_grid_extra = {'n_estimators': np.logspace(2,3.5,8).astype(int),\n                    'max_features': [0.1,0.3,0.5,0.7,0.9],\n                    'max_depth': np.logspace(0,3,10).astype(int),\n                    'min_samples_split': [2, 5, 10],\n                    'min_samples_leaf': [1, 2, 4],\n                    'bootstrap':[True, False]}\n\n# Instantiate ExtraTreesClassifier\nextra_trees = ExtraTreesClassifier(random_state = 42)\n\n# Create a Random Search Parameter Grid\ngrid_extra_trees = RandomizedSearchCV(estimator=extra_trees, \n                          param_distributions=param_grid_extra, \n                          n_iter=100,\n                          cv=3, \n                          verbose=3, \n                          n_jobs=1,\n                          scoring = {'NLL':'neg_log_loss', 'Accuracy':'accuracy'}, \n                          refit='NLL')\n# Fit Extra Trees Models - 100 models each with 3 rounds of cross validation - 300 fitted models\ngrid_extra_trees.fit(X,y)\nprint('The Best Extra Trees Estimator is: ', grid_extra_trees.best_estimator_)\nprint('The Best Extra Trees Parameters are: ', grid_extra_trees.best_params_)\nprint('The Best Extra Trees score is: ', grid_extra_trees.best_score_)","bd4c1e2b":"# Parameters for Light Gradient Boosting Machines Hyperparameter tuning\nparam_grid_lgbm = {'n_estimators': np.logspace(2,3.5,8).astype(int),\n                   'feature_fraction': [0.1,0.3,0.5,0.7,0.9],\n                   'bagging_fraction': [0.5,0.6,0.7,0.8,0.9],\n                   'max_depth': np.logspace(0,3,10).astype(int),\n                   'min_samples_split': [2, 5, 10],\n                   'min_data_in_leaf': [1, 2, 4],\n                   'learning_rate':[0.005,0.01,0.05,0.1,0.5]}\n\n# Instantiate ExtraTreesClassifier\nlgbm = LGBMClassifier(random_state = 42, is_provide_training_metric = True)\n\n# Create a Random Search Parameter Grid\ngrid_lgbm = RandomizedSearchCV(estimator=lgbm, \n                          param_distributions=param_grid_lgbm, \n                          n_iter=100,\n                          cv=3, \n                          verbose=3, \n                          n_jobs=1,\n                          scoring = {'NLL':'neg_log_loss', 'Accuracy':'accuracy'}, \n                          refit='NLL')\n# Fit LightGBM Models - 100 models each with 3 rounds of cross validation - 300 fitted models\ngrid_lgbm.fit(X,y)\nprint('The Best LightGBM Estimator is: ', grid_lgbm.best_estimator_)\nprint('The Best LightGBM Parameters are: ', grid_lgbm.best_params_)\nprint('The Best LightGBM score is: ', grid_lgbm.best_score_)","6cca9583":"## **Up Next:**\n\nIn a separate kernel, Plug in the best parameters for each of the above 3 classifiers as well as construct a stacked classifier on top of these classifiers.","75ec57b9":"Create Stacked Classifier using Random Forest, Extra Trees and LGBM. Use Extra Trees as meta classifier\nstack = StackingCVClassifier(classifiers=[rf,\n                                         extra_trees,\n                                         lgbm],\n                            use_probas=True,\n                            meta_classifier=extra_trees, random_state = 42)\ncross_val_score(stack,X,y,cv = 5, scoring = 'accuracy', verbose = 3)","22ed0b6a":"# Baseline Prediction accuracy with Zero Feature Selection\n## Hyper parameter optimization with Tree based algorithms\nIn this Kernel, we will focus on finding the best classification model for each of **Random Forest, Extremely Randomized Trees and LightGBM classifiers**. We will use **all features as is**, to get a feel for the ability of these models, given an optimal set of hyperparameters, to produce a reasonably predicting model with **absolutely no feature selection performed**. \n\nThe reason i chose Tree based algorithms to baseline prediction accuracy is these algorithms need no pre processing, are resistant to outliers and have historically shown best performance in prediction competitions. \n\nWhile Random Forests are valued for their ability to decorrelate the individual decision trees by randomizing the split variables that are chosen, Extra Trees take this randomization one step further by choosing even the split points at random thereby decorrelating the trees to a even greater extent. This gives very diverse trees which can potentially perform even better with unseen test data as they further reduce overfitting (hopefully not underfit as much). \n\nI used LGBM to apply boosting which basically sequentially improves predictions by focussing on the error generated at each tree. I used it for its ability to run faster on large datasets. Our train set may be small with only 15000 rows but the test set has 0.5  million rows. LGBM is different from other boosting algorithms in the way the trees are grown, but i am not getting into details here. This [link](http:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc) is kind of a primer to knowing LGBM before checking out official documentation\n\nOnce we have determined the best choice of parameters for each of these algorithms,we will apply these parameters to predict and baseline accuracy levels.\n\nFinally we will also try to see if a stacked classifier with these 3 models as inputs is able to come up with predictions as good or better than these individual models.\n\nThe final best model for these three tree algorithms and the stacked classifier will be executed in the next kernel so that we do not end up running hyper parameter optimization again and consuming computing resources unnessarily. All the heavy lifting will be done in this Kernel. I want to publish even the random grid models logs to record how the model fitting process took place and analyze the metrics as against each set of hyperparameters.\n\nWe will use **RandomizedSearchCV instead of GridSearchCV** as the latter will take up too much computing resources if taking a wider range of values for each hyperparameter. I will set number of iteration as 100 with 3 folds of Cross Validation so that we are actually fitting 300 models, which can be considered a decent number of experiments to find an optimal set of hyper parameters.\n\nFinally, We will show 2 metrics for each model built- **Accuracy as well as Negative Log loss** and use Negative log loss as the basis for choosing the best model. This is because, log loss accords severe penalty for generating a higher probability of class prediction and then getting the class prediction wrong during the model training."}}