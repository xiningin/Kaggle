{"cell_type":{"1eafc6de":"code","a523bd92":"code","e6c5eb0b":"code","7c6b33df":"code","c877bf44":"code","049b98bf":"code","c64b9a57":"code","7ed38678":"code","74599e5b":"code","b5559194":"code","97915f72":"code","3595b271":"code","e6c2b932":"code","2d1bdcd1":"code","71d6d630":"code","46ecbafe":"code","ffa81c7b":"code","b3050d72":"code","f3be253c":"code","358d6417":"code","c022997d":"code","2d82ab54":"code","20711e78":"code","79dfafaf":"code","cd589cf8":"code","3566a154":"code","f52ab0db":"code","49b21ecf":"code","52fdec43":"code","ad127071":"code","84b8bbf2":"code","8b457d47":"code","524c8ba9":"code","a749721d":"code","042c40df":"code","012f9e3b":"code","2aa0935d":"code","5ec13655":"code","db060c94":"code","ca632585":"code","0738fd1a":"code","bec7aef2":"code","86fe29a1":"code","03646472":"code","4c0714b7":"code","b19da7ef":"code","602b84c4":"code","baf3a395":"code","faf720f4":"code","1aaf1762":"code","4563e02e":"code","1e1755d6":"code","12ecfc39":"code","e21f55b4":"code","f29fa8a7":"markdown","554954b6":"markdown","8e25eda6":"markdown","61d42613":"markdown","dd79a23c":"markdown","5860b7a9":"markdown","c6cdaac1":"markdown","1cff3270":"markdown","4cd60101":"markdown","c6005614":"markdown","d1359d09":"markdown","79e44546":"markdown","e31f82c5":"markdown"},"source":{"1eafc6de":"import tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport pandas as pd","a523bd92":"data = '..\/input\/poetry\/alicia-keys.txt'","e6c5eb0b":"dataset_text = open(data, 'rb').read().decode(encoding = 'utf-8')","7c6b33df":"print(dataset_text)","c877bf44":"len(dataset_text)","049b98bf":"vocab = sorted(set(dataset_text))","c64b9a57":"print('{} unique characters'.format(len(vocab)))","7ed38678":"vocab","74599e5b":"char2idx = {char: index for index, char in enumerate(vocab)}","b5559194":"char2idx","97915f72":"idx2char = np.array(vocab)","3595b271":"idx2char","e6c2b932":"idx2char[10]","2d1bdcd1":"char2idx[':']","71d6d630":"text_as_int = np.array([char2idx[char] for char in dataset_text])","46ecbafe":"text_as_int","ffa81c7b":"text_as_int.shape","b3050d72":"print('{} characters mapped to int ---> {}'.format(repr(dataset_text[:13]), text_as_int[:13]))","f3be253c":"len(dataset_text)","358d6417":"seq_length = 100\nexamples_per_epoch = len(dataset_text) \/\/ seq_length\nexamples_per_epoch","c022997d":"char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)","2d82ab54":"char_dataset","20711e78":"sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)","79dfafaf":"sequences","cd589cf8":"for item in sequences.take(50):\n  print(repr(''.join(idx2char[item.numpy()])))","3566a154":"def split_input_target(chunk):\n  input_text = chunk[:-1]\n  target_text = chunk[1:]\n  return input_text, target_text","f52ab0db":"dataset = sequences.map(split_input_target)","49b21ecf":"for input_example, target_example in dataset.take(10):\n  print('Input data:', repr(''.join(idx2char[input_example.numpy()])))\n  print('Target data:', repr(''.join(idx2char[target_example.numpy()])))","52fdec43":"batch_size = 64\nbuffer_size = 10000","ad127071":"dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)","84b8bbf2":"dataset","8b457d47":"len(vocab)","524c8ba9":"vocab_size = len(vocab)","a749721d":"embedding_dim = 256","042c40df":"rnn_units = 1024","012f9e3b":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n                               tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n                               tf.keras.layers.Dense(vocab_size)])\n  return model","2aa0935d":"model = build_model(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)","5ec13655":"for input_example_batch, target_example_batch in dataset.take(10):\n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape)","db060c94":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)","ca632585":"sampled_indices","0738fd1a":"sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()","bec7aef2":"sampled_indices","86fe29a1":"print('Input: \\n', repr(''.join(idx2char[input_example_batch[0]])))\nprint()\nprint('Next char predictions: \\n', repr(''.join(idx2char[sampled_indices])))","03646472":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","4c0714b7":"example_batch_loss = loss(target_example_batch, example_batch_predictions)","b19da7ef":"example_batch_loss.numpy().mean()","602b84c4":"model.compile(optimizer='Adam', loss=loss)","baf3a395":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)","faf720f4":"epochs = 40\nhistory = model.fit(dataset, epochs = epochs, callbacks=[checkpoint_callback])","1aaf1762":"tf.train.latest_checkpoint(checkpoint_dir)","4563e02e":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))","1e1755d6":"model.summary()","12ecfc39":"def generate_text(model, start_string):\n  \n  num_generate = 1000\n\n  \n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n  \n  text_generated = []\n   \n  temperature = 1.0\n\n  \n  for i in range(num_generate):\n    \n    predictions = model(input_eval)\n\n    \n    predictions = tf.squeeze(predictions, 0)\n    predictions = predictions \/ temperature\n    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n    \n    input_eval = tf.expand_dims([predicted_id], 0)\n\n    text_generated.append(idx2char[predicted_id])\n  \n  return (start_string + ''.join(text_generated))","e21f55b4":"print(generate_text(model, start_string='New : '))","f29fa8a7":"# Model training","554954b6":"Restore last checkpoint","8e25eda6":"**Optimizer and loss function**","61d42613":"**Checkpoints**","dd79a23c":"# Creation of training examples and batches","5860b7a9":"# Database loading and exploration","c6cdaac1":"# Mapping text to numbers","1cff3270":"# Execution of training","4cd60101":"# Context\n\n**The file we chose to use is the music by Alicia keys new york.**\n\n**We will use Recurring Neural Networks to generate texts.**\n\n**The Neural Network will learn from the texts so that it can write its own texts.**\n\n**The goal is to train the LSTM to predict the next character in a text string.**\n","c6005614":"# Prediction Loop","d1359d09":"# **If you find this notebook useful, support with an upvote** \ud83d\udc4d","79e44546":"# Model building","e31f82c5":"# Text Generation"}}