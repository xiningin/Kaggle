{"cell_type":{"fb107714":"code","dcc5d17b":"code","792fe2f2":"code","9ce87711":"code","3640515f":"code","127a72fc":"code","870300f5":"code","4b2e7c84":"code","b9cecb5c":"code","93cf9ce4":"code","d8be698b":"code","dbd56329":"code","43bf160a":"code","8cc070e5":"code","03641a9f":"code","a226bf61":"code","d6ef1cc1":"code","9d98295f":"code","145c068d":"code","8fc89d3d":"code","f5d4c214":"code","ed698e29":"code","c57e6bb5":"code","7071363e":"code","22262929":"code","d5d2fbc5":"code","2a4f52ac":"code","0229d41f":"code","d5a51682":"code","f96e5d5d":"code","0a6ebd1c":"code","96a118ec":"code","37bdef79":"markdown","91f05a9a":"markdown","d6b0cca0":"markdown","99a38af3":"markdown","31c44fd0":"markdown","822ea3ed":"markdown","a12665c8":"markdown","82db058f":"markdown","0c146e53":"markdown","72963d2d":"markdown","94f757c2":"markdown","07f7ecaf":"markdown","135b6d34":"markdown","71d35044":"markdown","180ad0fd":"markdown","75c237d6":"markdown","623be29b":"markdown","b84b8c37":"markdown","b60cacdd":"markdown","e2e90791":"markdown","1d2e6d09":"markdown","40236232":"markdown","c93f0f9e":"markdown","93021f62":"markdown","69c3eb96":"markdown","72e91c36":"markdown","a3a8faa2":"markdown","fcfee7ec":"markdown","80a699d9":"markdown","af312b9a":"markdown","500fa6fb":"markdown","0bf8b107":"markdown","fbe58d8e":"markdown","79e97c19":"markdown"},"source":{"fb107714":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn","dcc5d17b":"%matplotlib inline\npd.set_option('display.max_columns', None)","792fe2f2":"data= pd.read_csv('..\/input\/world-happiness\/2019.csv')\ndf = pd.DataFrame(data)","9ce87711":"df['rank']=0\ndf['rank']= pd.cut(df['Overall rank'], bins=[0, 40, 78, 119, 157], labels = ['A', 'B', 'C', 'D'] )","3640515f":"df.head()","127a72fc":"df['rank'].describe()","870300f5":"from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)","4b2e7c84":"train_df.shape","b9cecb5c":"train_df.columns","93cf9ce4":"train_df.head()","d8be698b":"train_df.dtypes","dbd56329":"train_df.describe()","43bf160a":"train_df.isnull().sum() \/ train_df.shape[0]","8cc070e5":"plt.figure(figsize=(11,7))\nplt.plot('Overall rank',  # x\n         'GDP per capita',  # y\n         data=train_df, \n         linestyle='none', \n         marker='*', \n         markersize=20,\n         color='green', \n         alpha=0.9)\n\nplt.title('Overall rank of GDP per capita', fontsize=30)\nplt.xlabel('Overall rank of GDP per capita', fontsize=25)\nplt.ylabel('GDP per capita', fontsize=25)\nplt.show()","03641a9f":"plt.figure(figsize=(11,7))\nplt.plot('Overall rank',  # x\n         'Freedom to make life choices',  # y\n         data=train_df, \n         linestyle='none', \n         marker='*', \n         markersize=20,\n         color='green', \n         alpha=0.9)\n\nplt.title('Overall rank of Freedom to make life choices', fontsize=30)\nplt.xlabel('Overall rank of Freedom to make life choices', fontsize=25)\nplt.ylabel('Freedom to make life choices', fontsize=25)\nplt.show()","a226bf61":"plt.figure(figsize=(11,7))\nplt.plot('Overall rank',  # x\n         'Healthy life expectancy',  # y\n         data=train_df, \n         linestyle='none', \n         marker='*', \n         markersize=20,\n         color='green', \n         alpha=0.9)\n\nplt.title('Overall rank of Healthy life expectancy', fontsize=30)\nplt.xlabel('Overall rank of Healthy life expectancy', fontsize=25)\nplt.ylabel('Healthy life expectancy', fontsize=25)\nplt.show()","d6ef1cc1":"plt.figure(figsize=(11,7))\nplt.plot('Overall rank',  # x\n         'Perceptions of corruption',  # y\n         data=train_df, \n         linestyle='none', \n         marker='*', \n         markersize=20,\n         color='green', \n         alpha=0.9)\n\nplt.title('Overall rank of Perceptions of corruption', fontsize=30)\nplt.xlabel('Overall rank of Perceptions of corruption', fontsize=25)\nplt.ylabel('Perceptions of corruption', fontsize=25)\nplt.show()","9d98295f":"plt.figure(figsize=(11,7))\nplt.plot('Overall rank',  # x\n         'Social support',  # y\n         data=train_df, \n         linestyle='none', \n         marker='*', \n         markersize=20,\n         color='green', \n         alpha=0.9)\n\nplt.title('Overall rank of Social support', fontsize=30)\nplt.xlabel('Overall rank of Social support', fontsize=25)\nplt.ylabel('Social support', fontsize=25)\nplt.show()","145c068d":"plt.figure(figsize=(11,7))\nplt.plot('Overall rank',  # x\n         'Generosity',  # y\n         data=train_df, \n         linestyle='none', \n         marker='*', \n         markersize=20,\n         color='green', \n         alpha=0.9)\n\nplt.title('Overall rank of Generosity', fontsize=30)\nplt.xlabel('Overall rank of Generosity', fontsize=25)\nplt.ylabel('Generosity', fontsize=25)\nplt.show()","8fc89d3d":"import seaborn as sns\n\nplt.figure(figsize=(11,7))\nsns.boxplot(x=\"rank\", \n            y=\"GDP per capita\", \n            data=train_df)\nplt.title('GDP per capita', fontsize=30)\nplt.show()","f5d4c214":"import seaborn as sns\n\nplt.figure(figsize=(11,7))\nsns.boxplot(x=\"rank\", \n            y=\"Freedom to make life choices\", \n            data=train_df)\nplt.title('Freedom to make life choices', fontsize=30)\nplt.show()","ed698e29":"import seaborn as sns\n\nplt.figure(figsize=(11,7))\nsns.boxplot(x=\"rank\", \n            y=\"Healthy life expectancy\", \n            data=train_df)\nplt.title('Healthy life expectany', fontsize=30)\nplt.show()","c57e6bb5":"import seaborn as sns\n\nplt.figure(figsize=(11,7))\nsns.boxplot(x=\"rank\", \n            y=\"Perceptions of corruption\", \n            data=train_df)\nplt.title('Perceptions of corruption', fontsize=30)\nplt.show()","7071363e":"import seaborn as sns\n\nplt.figure(figsize=(11,7))\nsns.boxplot(x=\"rank\", \n            y=\"Social support\", \n            data=train_df)\nplt.title('Social support', fontsize=30)\nplt.show()","22262929":"import seaborn as sns\n\nplt.figure(figsize=(11,7))\nsns.boxplot(x=\"rank\", \n            y=\"Generosity\", \n            data=train_df)\nplt.title('Generosity', fontsize=30)\nplt.show()","d5d2fbc5":"x = ['Score', 'GDP per capita',\n       'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption']\n\nx2 = [ 'GDP per capita',\n       'Social support', 'Healthy life expectancy',\n       'Freedom to make life choices', 'Generosity',\n       'Perceptions of corruption']\n\nx_data = train_df.drop(['Score', 'Country or region', 'Overall rank', 'rank'], axis=1) \nx_data2 = train_df.drop(['Country or region', 'Overall rank', 'rank', 'Generosity', 'Score'], axis=1)\nx_data3 = train_df.drop(['Country or region', 'Overall rank', 'rank', 'Generosity', 'Score', 'Perceptions of corruption'], axis=1)\n\ny_data = train_df['rank']","2a4f52ac":"sns.pairplot(df, hue= \"rank\")","0229d41f":"x_test = test_df.drop(['Score', 'Country or region', 'Overall rank', 'rank'], axis=1) \nx_test2 = test_df.drop(['Score', 'Country or region', 'Overall rank', 'rank', 'Generosity'], axis=1)\nx_test3 = test_df.drop(['Score', 'Country or region', 'Overall rank', 'rank', 'Generosity', 'Perceptions of corruption'], axis=1)\n\ny_testdata = test_df['rank']","d5a51682":"colormap = plt.cm.PuBu \nplt.figure(figsize=(10, 8)) \nplt.title(\"Heatmap\", y = 1.05, size = 15) \nsns.heatmap(x_data.astype(float).corr(), linewidths = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = \"white\", annot = True, annot_kws = {\"size\" : 16})","f96e5d5d":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier().fit(x_data,y_data)\nprint(\"\ud6c8\ub828 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(dt.score(x_data, y_data)))\nprint(\"\ud14c\uc2a4\ud2b8 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(dt.score(x_test, y_testdata)))\nprint(\"\ud2b9\uc131 \uc911\uc694\ub3c4:\\n\", dt.feature_importances_)\nx_test\n##depth \uc870\uc808\ndt = DecisionTreeClassifier(max_depth=3, random_state=0).fit(x_data,y_data)\nprint(\"\ud6c8\ub828 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(dt.score(x_data, y_data)))\nprint(\"\ud14c\uc2a4\ud2b8 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(dt.score(x_test, y_testdata)))","0a6ebd1c":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier().fit(x_data,y_data)\n\nprint(\"\ud6c8\ub828 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(forest.score(x_data, y_data)))\nprint(\"\ud14c\uc2a4\ud2b8 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(forest.score(x_test, y_testdata)))\nprint(\"\ud2b9\uc131 \uc911\uc694\ub3c4:\\n\", forest.feature_importances_)\n\nforest2 = RandomForestClassifier().fit(x_data3,y_data)\n\nprint(\"\ud6c8\ub828 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(forest2.score(x_data3, y_data)))\nprint(\"\ud14c\uc2a4\ud2b8 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(forest2.score(x_test3, y_testdata)))\nprint(\"\ud2b9\uc131 \uc911\uc694\ub3c4:\\n\", forest2.feature_importances_)","96a118ec":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3).fit(x_data,y_data)\n\nprint(\"\ud6c8\ub828 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(knn.score(x_data, y_data)))\nprint(\"\ud14c\uc2a4\ud2b8 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(knn.score(x_test, y_testdata)))\n\nknn2 = KNeighborsClassifier(n_neighbors=3).fit(x_data2,y_data)\n\nprint(\"\ud6c8\ub828 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(knn2.score(x_data2, y_data)))\nprint(\"\ud14c\uc2a4\ud2b8 \uc138\ud2b8 \uc815\ud655\ub3c4:  {:.3f}.\".format(knn2.score(x_test2, y_testdata)))","37bdef79":"From Figure 3-3, the high relationship between healthy life expectancy and rank can be confirmed. The countries in the top 30% have at least a 0.7 healthy life expectancy score, while the countries in the top 60% have at least a 0.3 healthy life expectancy score. \n","91f05a9a":"The relationship between GDP per capita and rank is shown in Figure 3-5. As the group goes down from A to D, the GDP score also goes down.","d6b0cca0":"## 3.3. Heat Map\n\nA heat map is a method to visualize different data in heat spread figures using colors. Similar colors are used as the correlation is strong. It is shown in Figure 3-11 to find out the correlation. \n","99a38af3":"## 2.2 Basic statistics\n\nThe following are the basic statistics for the data. \nAverage, top 25, 50, 70%, smallest, and the largest amount are arranged in the following chart 2.2. \n\n\n\n![Screen Shot 2021-11-06 at 3.07.23 PM.png](attachment:8bf84e93-fcb1-4690-87e3-c2b88941df0d.png)\n\n\n","31c44fd0":"## 3. Data visualization\n\nData visualization is the process that visually expresses data analysis. I created scatter plots and box plots to find out the relationship between each variable and happiness level rankings.\nI first divided the countries into four groups (A, B, C, D) and then created the general happiness level box plot for each of the six variables and the overall rank. \n","822ea3ed":"In Figure 3-1, as both the ranking and GDP scores are high, the relationship is strong. Countries in the top 30% have an approximate GDP of at least 0.8 and countries in the top 60% have at least 0.5. ","a12665c8":"## 4.3. KNN (K-Nearest-Neighbor)\n \nThe KNN algorithm is a method for predicting new data by using the closest k amount of existing data. The method is to find new data by determining the value of k that makes the test error small. The advantage is that it is easy to understand and is not significantly affected by outliers, so the larger the amount of data, the more effective the result. On the other hand, the disadvantage is that because there are more data features, it takes more time and doesn\u2019t work when the specific value is 0. Also, prediction takes a lot of time even though the model is built quickly.\n","82db058f":"The relationship between social support and rank is shown in Figure 3-9. As the group goes down from A to D group, the figure shows a trend of social support score also going down.  ","0c146e53":"## 3.1 Scatter Plot\n\nA scatter plot is a graph method that shows the relationship between two variables by displaying points on the coordinates using a Cartesian coordinate system. I used the scatter plot to compare happiness level ranking and each variable.\n","72963d2d":"The initial training set accuracy was 100%, but the test set accuracy dropped to 40%. As the data is overfitted, accuracy dropped significantly. Overfitting is a phenomenon in which an error occurs in the actual data due to over-training only on the training data. In this case, the training data used for learning was over-learned, and the classification accuracy was also lowered. Therefore, the accuracy was 100% at first, but the actual data accuracy was 40%. \nSecondly, by adjusting the depth of the model and trying decision making again, the training test accuracy was about 75% and the actual test set accuracy was about 46%.\nIn the first training test, 100% and 75% were correctly classified in the training data, but overfitting resulted in an accuracy of 40.6% in the test data. Therefore, when looking at the two results, it can be seen that both models using the decision tree are not good models due to the low accuracy of the actual (test) data.\n\n","94f757c2":"## 2.1 Variables explanation\n\nThe following chart 2.1 includes a brief explanation for each variable and GWP questions. ","07f7ecaf":"The initial training set accuracy was 100%, but the actual test set accuracy dropped to 50%. Although overfitting also occurred like the decision tree above, the accuracy of the Random Forest is high. \n\nThe variables \u201cPerceptions of Corruption\u201d and \u201cGenerosity\u201d, two variables that had low relationships with the happiness ranking, are excluded for the second time. As a result, the training set accuracy is the same as 100%, while the actual test set accuracy increased to 62.5%. By excluding two variables that have lower relationships with overall rank, the accuracy increased. Therefore, it appears that the actual data set accuracy is highest in the Random Forest method. \n\n","135b6d34":"In the case of KNN, the accuracy is 77% in the first test, which is lower than other algorithms. The accuracy of the actual set is also 46%, which is very low. Because the KNN method takes a long time, when there are many variables or train data, it consequently takes a long time and the accuracy is low.\n\nNext, the KNN method was used by including generosity to variables. As a result, the accuracy of the initial training set dropped to 73% and the actual data accuracy was also reduced to 44%. It seems that the overall accuracy was lower than that of other algorithms and there was more misclassification. \n","71d35044":"The relationship between healthy life expectancy and rank is shown in Figure 3-7. As the rank goes down, there is a tendency for healthy life expectancy to also go down. ","180ad0fd":"## 3.2 Box Plot\n\n\nOften referred to as a \u201cbox-and-whisker plot\u201d or \u201cboxplot\u201d, a box plot is a graph that visualizes numerical data. This graph expresses five summarized figures from the data. \nThe data is divided into four groups to analyze differences between countries with high happiness levels and countries with low happiness levels. The countries ranked from 1-39 are in the A group, from 40-74 are in the B group, from 75-118 are in the C group, and from 118-155 countries are in the D group. \n\n","75c237d6":"## 5. Conclusion\nOverall, the variables most related to the ranking of the happiness index are GDP per capita, freedom to make life choices, healthy life expectancy, and social support. On the other hand, the corruption scale and generosity did not appear to have a high correlation with the happiness level rank. In addition, the happiness level rank of each country is usually determined by several variables because even though the one variable is significantly high, that doesn\u2019t guarantee its overall rank is also high.  \n","623be29b":"## 4.2. Random Forest\n\n\nThe random forest algorithm is one of the ensemble learning methods used for classification and regression analysis. It works resulting in a class (classification) or average prediction (regression analysis) from multiple decision trees constructed during the training process. The advantage of Random Forest is that it is very easy and simple as it is similar to the method of Decision Tree. Also, the Random Forest method doesn\u2019t often result in overfitting, which means that new data is generalized easily in a short amount of time. The consumption amount is big because the Random Forest is a method of making many Decision Trees and adding all of them. Also, rapid performance improvement does not occur even if the amount of training data increases.\n","b84b8c37":"As fig 3-4 shows, even though the corruption score is low and the countries\u2019 happiness level is low, the countries below the top 30% all have low corruption rates. This means that the relationship between the corruption rate and the happiness level ranking is weak. Exceptionally, Rwanda\u2019s corruption rate is very low but the happiness level is also low. The main reason for this is after the 1994 massacre, GDP per capita and healthy life expectancy in Rwanda remain low. ","b60cacdd":"Figure 3-8 shows the relationship between perceptions of corruption and rank. While the corruption rate significantly decreases between the A to B group, there is little or no difference from the B to D group. Some exceptional data can be noticed in the figure. \n","e2e90791":"Lastly, through figure 3-6, there is little or no correlation between generosity and overall rank as there are no big differences between different groups of countries. Myanmar is an exceptional case with the lowest rank. As many people believe in the Buddhist concept of the next life, people donate more as they hope they will live a better life in the future. Not only do people donate for social and religious reasons, but they also donate a lot voluntarily. This has resulted in an overall high happiness score in Myanmar. \n\n\n","1d2e6d09":"## 4.1.  Decision Tree\n \nA decision tree is an algorithm that classifies and predicts data by dividing the data. As the method is appropriate for the data, it was used to predict the accurate classification. The method is to first measure the impurity\/uncertainty by using the cost function (entropy) and after performing pre\/post pruning. The advantage of decision trees is that they are intuitive and fast to visualize both continuous and categorical data. On the other hand, it can be easily over-fitted, and if the characteristics of the data are not straight, the result will be inaccurate. In the case of regression, the value may drop significantly, and if it is a characteristic value, the function might be poor.\n","40236232":"Figure 3-6 indicates the relationship between freedom to make life choices and rank. As the happiness level goes down from A to D, the freedom score also goes down. ","c93f0f9e":"## 6. Thoughts\n \nAnalyzing the \u201cWorld happiness level ranking\u201d data using Python was unfamiliar at first, but I gradually got used to it and was amazed. It was impressive to be able to establish hypotheses by displaying graphs in various visualization methods, starting with importing data. By adding analysis and detailed explanations to the visualized data, I was able to understand the data better. It was interesting to know the relationship between the happiness level rank and each variable. There were some confusing parts in the process of writing the report and in the visualization part. For instance, I did not understand the concept of fitting a model such as the decision tree and KNN at first. Eventually, I was able to understand the concepts by referring to other websites. Overall, I was impressed that I was able to answer all of the questions I had when I started writing this report. There were some difficulties, but it was fun and memorable.\n","93021f62":"## Reference\nhttps:\/\/worldhappiness.report\/ed\/2019\/changing-world-happiness\/ (World Happiness Report)\nhttps:\/\/namu.wiki\/w\/%EA%B1%B4%EA%B0%95%20%EC%88%98%EB%AA%85\nhttps:\/\/ko.wikipedia.org\/wiki\/%EB%B6%80%EC%A0%95%EB%B6%80%ED%8C%A8\nhttps:\/\/ko.wikipedia.org\/wiki\/%EC%82%AC%ED%9A%8C%EC%A0%81_%EC%A7%80%EC%A7%80#:~:text=%EC%82%AC%ED%9A%8C%EC%A0%81%20%EC%A7%80%EC%A7%80\nhttps:\/\/ko.wikipedia.org\/wiki\/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%8B%9C%EA%B0%81%ED%99%94\nhttps:\/\/ko.wikipedia.org\/wiki\/%EC%82%B0%EC%A0%90%EB%8F%84\nhttps:\/\/doreen-vallog.tistory.com\/entry\/%EC%84%B8%EA%B3%84-%EB%AC%B8%ED%99%94-2020-%EC%84%B8%EC%83%81%EC%97%90%EC%84%9C-%EA%B0%80%EC%9E%A5-%EB%B6%88%ED%96%89%ED%95%9C-%EB%82%98%EB%9D%BC-Top5\nhttp:\/\/news.kmib.co.kr\/article\/view.asp?arcid=0921361522\nhttp:\/\/www.hyunbulnews.com\/news\/articleView.html?idxno=278037\nhttps:\/\/ko.wikipedia.org\/wiki\/%EC%83%81%EC%9E%90_%EC%88%98%EC%97%BC_%EA%B7%B8%EB%A6%BC\nhttps:\/\/ko.wikipedia.org\/wiki\/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8#%EC%A0%95%EC%9D%98\nhttps:\/\/process-mining.tistory.com\/102\nhttps:\/\/smecsm.tistory.com\/53\nhttps:\/\/blog.naver.com\/rlacksdid93\/221482013474\nhttps:\/\/leedakyeong.tistory.com\/entry\/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4Decision-Tree-%EA%B3%BC%EC%A0%81%ED%95%A9overfitting-%ED%95%B4%EA%B2%B0%EB%B0%A9%EB%B2%95-%EA%B0%80%EC%A7%80%EC%B9%98%EA%B8%B0-%EC%95%99%EC%83%81%EB%B8%94Random-Forest\nhttps:\/\/hong-yp-ml-records.tistory.com\/33 [HONG YP's Data Science BLOG]\n","69c3eb96":"![Screen Shot 2021-11-06 at 2.15.51 PM.png](attachment:cd03ead4-ce77-4158-9adb-60722bbbbe81.png)","72e91c36":"## Summary\n\nThis is a report based on data from the \u201cWorld Happiness Report\u201d compiled by The Gallup World Poll (GWP). The ultimate goal is to analyze and visualize the relationship between happiness level and variables, as well as understand the differences between countries with high and low happiness levels. ","a3a8faa2":"Lastly, Figure 3-10 shows the relationship between generosity and rank. Compared to other box plots, Figure 3-10 noticeably has little change from A to D group. Group A is numerically highest in generosity rate, but there is little correlation in terms of each group\u2019s average. \n","fcfee7ec":"Countries with low happiness level rankings generally have freedom scores that are high, so the relationship is low in Figure 3-2. Countries in the top 30% have at least a 0.24 score of freedom, while countries in the top 60% have at least a 0.1 score of freedom. \n","80a699d9":"## 4. Model fitting\nTo check whether the groups (A, B, C, D) made with the variables in this report can be classified well, the classification model was selected as a decision tree, random forest, and k nearest neighbor algorithm.\n","af312b9a":"# Relationship between Variables and Happiness level","500fa6fb":"## Background explanation\n\n\nI chose \u201cWorld happiness level\u201d as a dataset among many others because I was interested in which variables determine happiness levels. Especially, I wanted to investigate by which criteria each countries\u2019 happiness levels are determined and what theories can be made from that. I also was interested in the differences between the countries in first place and the countries that ranked lower in terms of their happiness levels. I further wondered if all six variables are related to determining happiness level rankings. Learning about how variables and happiness levels are related is the final goal of analyzing and visualizing this data. \n","0bf8b107":"## Data explanation\n\nThis data comes from research on the happiness levels of 155 countries. It is based on answers to one to two questions for each variable. The data has estimates that represent samples collected from 2013 to 2016. The happiness level is made up of six variables (GDP per capita, healthy life expectancy, generosity, corruption, freedom, social support) and scored higher than the dystopia score, which serves to make sure every country\u2019s lowest score is at least higher than the dystopia score. On the World Happiness Report, dystopia score is the lowest score observed among the six key variables. Also, residuals, the difference between average score and data, are calculated between 0 and 10. \n","fbe58d8e":"Through Fig. 3-11, the relationship between the happiness level rank and the variables can be visualized. The relationship between rank and GDP has a negative relationship, as the higher the GDP, the lower the happiness rank. The relationship between rank and social support has a strong negative relationship, as the higher the social support, the lower the rank. There is a negative relationship because higher health life expectancy results in a lower ranking. The higher the freedom, the higher the happiness ranking, and the more positive the relationship. The higher the corruption scale, the lower the overall ranking, which is somewhere between a negative and a positive relationship. Looking at the relationship with each variable, the relationship between GDP and healthy life expectancy and the relationship between GDP and social support seem to have the strongest positive relationship. In other words, the higher the GDP, the higher the healthy life expectancy and social support. On the other hand, the correlation coefficient between healthy life expectancy and generosity was -0.027, showing little correlation.\n","79e97c19":"In Figure 3-5, the happiness level ranking is high because the social support rate is high. The following graph also visually shows that the correlation is strong. The countries in the top 30% have at least a 1.9 social support score and the countries in the top 60% have at least a 0.9 social support score. \n"}}