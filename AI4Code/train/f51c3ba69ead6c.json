{"cell_type":{"678104ff":"code","5a46593a":"code","0ebb97d2":"code","180d5fde":"code","0ce881d4":"code","a027b65e":"code","33287518":"code","f395eceb":"code","4802235a":"code","9c23bb96":"code","0ef68276":"code","0b319d86":"code","4ed3cf54":"code","b0c34b6d":"code","c79f6296":"code","14ee9b68":"code","5dfa927a":"code","2921a240":"code","fce308b2":"code","e4a87b1a":"code","0da9434c":"code","1b496292":"code","8f29ba2d":"code","8d96949c":"code","aa60d779":"code","1ea36ed9":"code","e3e35958":"code","63918e92":"code","f48035af":"code","80bb3dd9":"code","8ed39269":"code","07f2854d":"code","00e5e4cc":"code","eb9a4b03":"code","acc3bbb6":"code","995f15b1":"code","0f2fe725":"code","7923406e":"code","438ff4f6":"code","e5f4376b":"code","cb577c94":"code","7f65c4fe":"code","0005abf0":"code","50c1f9a3":"markdown","eddf71b7":"markdown","ec2e4f59":"markdown","1074c3b3":"markdown","c6e5d0da":"markdown","33c2c109":"markdown","931c4b31":"markdown","644f7e07":"markdown","7b35fe87":"markdown","17c29fe5":"markdown","18f4fa1f":"markdown","271998f6":"markdown","f7c41782":"markdown","9608c053":"markdown","02b1c2b0":"markdown","04e8091b":"markdown","dd16f5c2":"markdown","c7974327":"markdown","1f84bc06":"markdown","41cfc620":"markdown","d06caf5a":"markdown","1de548a3":"markdown","d554c2ab":"markdown","f7f8907f":"markdown","a90178a0":"markdown","5c3fb72d":"markdown","c4af7563":"markdown","cff91975":"markdown","977f03f2":"markdown","ae932dda":"markdown","ce102f6d":"markdown","faf98aad":"markdown","c1e8fe5b":"markdown","5f52284c":"markdown","381d4626":"markdown","18a6f164":"markdown","f4b751eb":"markdown"},"source":{"678104ff":"# Import the necessary packages used in this notebook\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\nimport warnings\n# Remove any warning messages\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.","5a46593a":"datafr = pd.read_csv(\"..\/input\/heart.csv\", error_bad_lines=False)","0ebb97d2":"display(datafr.head(10))","180d5fde":"# Dimension of the datatset\nprint(\"Dimension of the dataset is: \",datafr.shape)\n# Check if any column has missing value\ndatafr.isnull().sum()\n","0ce881d4":"len(datafr.target[datafr.target==0])","a027b65e":"len(datafr.target[datafr.target==1])","33287518":"cleanup_nums = {\"cp\":     {0: \"CPType 0\", 1: \"CPType 1\", 2:\"CPType 2\", 3: \"CPType 3\"},\n                \"thal\": {1: \"Normal\", 2: \"Fixed Defect\", 3: \"Reversable Defect\" }}\ndatafr.replace(cleanup_nums, inplace=True)\ndatafr.head()","f395eceb":"from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)","4802235a":"male =len(datafr[datafr['sex'] == 1])\nfemale = len(datafr[datafr['sex']== 0])\n\n# Data to plot\nlabels = 'Male','Female'\nsizes = [male,female]\n\n# Plot\nplt.figure(figsize=(6,6))\nplt.pie(sizes, explode=(0, 0.1), labels=labels, colors=sns.color_palette(\"Purples\"),\nautopct='%1.1f%%', shadow=True, startangle=90)\nplt.title('Pie Chart Ratio for Sex Distribution\\n', fontsize=16)\nsns.set_context(\"paper\", font_scale=1.2)","9c23bb96":"f, axes = plt.subplots(1, 3, sharey=True, figsize=(15, 8))\nsns.boxplot(x=\"sex\", y=\"trestbps\", hue=\"target\", data=datafr, palette='Purples', ax=axes[0])\naxes[0].set_title('BoxPlot for {}'.format(\"trestbps\"))\nsns.boxplot(x=\"sex\", y=\"thalach\", hue=\"target\", data=datafr, palette='Purples', ax=axes[1])\naxes[1].set_title('BoxPlot for {}'.format(\"thalach\"))\nsns.boxplot(x=\"sex\", y=\"chol\", hue=\"target\", data=datafr, palette='Purples', ax=axes[2])\naxes[2].set_title('BoxPlot for {}'.format(\"chol\"))","0ef68276":"plt.figure(figsize=(6,6))\nsns.catplot(x=\"sex\", y=\"age\", hue=\"target\", kind=\"swarm\", data=datafr, palette='Purples')\nplt.title('Swarm Plot of Sex vs Age\\n', fontsize=16)\nsns.set_context(\"paper\", font_scale=1.2)","0b319d86":"# Data to plot\nlabels = 'Chest Pain Type:0','Chest Pain Type:1','Chest Pain Type:2','Chest Pain Type:3'\nsizes = [len(datafr[datafr['cp'] == \"CPType 0\"]),len(datafr[datafr['cp'] == \"CPType 1\"]),\n         len(datafr[datafr['cp'] == \"CPType 2\"]),\n         len(datafr[datafr['cp'] == \"CPType 3\"])]\n\nplt.figure(figsize=(6,6))\n\n# Plot\nplt.pie(sizes, explode=(0.05, 0.05, 0.05, 0.05), labels=labels, colors=sns.color_palette(\"Purples\"),\nautopct='%1.1f%%', shadow=True, startangle=90)\nplt.title('Pie Chart Ratio for type of Chest Pain\\n', fontsize=16)\nsns.set_context(\"paper\", font_scale=1.2)\n\nplt.axis('equal')\nplt.show()","4ed3cf54":"plt.figure(figsize=(6,6))\nsns.catplot(x=\"cp\", y=\"age\", hue=\"target\", kind=\"swarm\", data=datafr, palette='Purples')\nplt.title('Swarm Plot of CP vs Age\\n', fontsize=16)\nsns.set_context(\"paper\", font_scale=1.2)","b0c34b6d":"\"\"\"\nNormalizing the values and then making it as a DataFrame and then plotting using sns.barplot.\n\"\"\"\nplt.figure(figsize=(6,6))\ntemp = (datafr.groupby(['target']))['cp'].value_counts(normalize=True)\\\n.mul(100).reset_index(name = \"percentage\")\nsns.barplot(x = \"cp\", y = \"percentage\", hue = \"target\", data = temp, palette='Purples')\\\n.set_title(\"Chest Pain vs Heart Disease\\n\", fontsize=16)\nsns.set_context(\"paper\", font_scale=1.2)","c79f6296":"# Data to plot\nlabels = 'fasting blood sugar < 120 mg\/dl','fasting blood sugar > 120 mg\/dl'\nsizes = [len(datafr[datafr['fbs'] == 0]),len(datafr[datafr['cp'] == \"CPType 1\"])]\n\nplt.figure(figsize=(6,6))\n \n# Plot\nplt.pie(sizes, explode=(0.05, 0.05), labels=labels, colors=sns.color_palette(\"Purples\"),\nautopct='%1.1f%%', shadow=True, startangle=90)\nplt.title('Pie Chart Ratio for Fasting Blood Sugar\\n', fontsize=16)\nsns.set_context(\"paper\", font_scale=1.2)\n \nplt.axis('equal')\nplt.show()   ","14ee9b68":"sns.set(style=\"darkgrid\")\nplt.figure(figsize=(6,6))\nsns.distplot(datafr['thalach'],kde=False,bins=30,color='steelblue')\nplt.title('Distribution of Thalach\\n', fontsize=16)\nsns.set_context(\"paper\", font_scale=1.4)","5dfa927a":"sns.set(style=\"darkgrid\")\nplt.figure(figsize=(6,6))\nsns.distplot(datafr['chol'],kde=False,bins=30,color=\"purple\")\nplt.title('Distribution of Chol\\n', fontsize=16)\nsns.set_context(\"paper\", font_scale=1.4)","2921a240":"plt.figure(figsize=(15,6))\nsns.countplot(x='age',data = datafr, hue = 'target',palette='GnBu')\nsns.set_context(\"paper\", font_scale=1.4)\nplt.show()","fce308b2":"datafr = pd.get_dummies(datafr, columns=[\"cp\", \"thal\"])\ndatafr.head()","e4a87b1a":"# Predictor variables\nX= datafr.drop('target',axis=1)\n# Target or Class variable\nY=datafr['target']","0da9434c":"# Let's using scikit learn to split our dataset\nfrom sklearn.model_selection import train_test_split\n# Using 70:30 ratio for train:test\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=.3,random_state=400)","1b496292":"X_train.shape","8f29ba2d":"X_test.shape","8d96949c":"# Using StandardScaler to scale features\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# In the given dataset categorical features are already one-hot encoded, thus we do not reuqire to one-hot encode them\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.preprocessing import OneHotEncoder\n\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(X_train_scaled)\n\nX_test_scaled = scaler.transform(X_test)\nX_test = pd.DataFrame(X_test_scaled)","aa60d779":"from sklearn.model_selection import cross_val_score\n# Using 10 folds cross-validation\ndef CrossVal(trainX,trainY,model):\n    accuracy=cross_val_score(model,trainX , trainY, cv=10, scoring='accuracy')\n    return(accuracy)","1ea36ed9":"# Start with Support Vector Machine for Binary Classification\nfrom sklearn import svm\nclf = svm.SVC(gamma='scale', probability=True)\n# Creare a model with X_train and Y_train data\nclf.fit(X_train,Y_train)\n# predict probabilities\nprobs = clf.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n","e3e35958":"# Run the model on X_test to predict the target labels. Use cross-validation accuracy to check if model overfits or underfits\npredict1 = clf.predict(X_test)\nclf=svm.SVC(C=0.2,probability=True,kernel='rbf',gamma='scale')\nscore_clf=CrossVal(X_train,Y_train,clf)\nprint(\"Cross-Validation accuracy is {:.2f}%\".format(score_clf.mean()*100))","63918e92":"# Compare the predicted target labels with Y_test\nfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score\nprint(\"Test Accuracy using SVM Model: {:.2f}%\".format(accuracy_score(Y_test,predict1)*100))\n# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,predict1)\n\n# calculate AUC\nauc_svm = roc_auc_score(Y_test, probs)\n#print('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(Y_test, probs)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title(\"ROC Curve for SVM with AUC Score: {:.3f}\".format(auc_svm))\n# show the plot\nplt.show()\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Purples', fmt = 'd')\nsvm_f1=f1_score(Y_test,predict1)\nplt.title('F1 Score for SVM model is {:.2f}'.format(svm_f1))","f48035af":"# Next we take Random Forest Model (Ensemble) for Binary Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 13,random_state = 40)\n# Creare a model with X_train and Y_train data\nrf.fit(X_train,Y_train)\n# predict probabilities\nprobs = rf.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]","80bb3dd9":"# Run the model on X_test to predict the target labels. Use cross-validation accuracy to check if model overfits or underfits\npredict2 = rf.predict(X_test)\nrf=RandomForestClassifier(n_estimators=13, n_jobs=-1, random_state=40)\nscore_rf= CrossVal(X_train,Y_train,rf)\nprint('Cross-Validation accuracy is {:.2f}%'.format(score_rf.mean()*100))","8ed39269":"# Compare the predicted target labels with Y_test\nfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score\nprint(\"Accuracy using Random Forest Model: {:.2f}%\".format(accuracy_score(Y_test,predict2)*100))\n# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,predict2)\n\n# calculate AUC\nauc_rf = roc_auc_score(Y_test, probs)\n#print('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(Y_test, probs)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title(\"ROC Curve for Random Forest with AUC Score: {:.3f}\".format(auc_rf))\n# show the plot\nplt.show()\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Purples', fmt = 'd')\nrf_f1=f1_score(Y_test,predict2)\nplt.title('F1 Score for Random Forest model is {:.2f}'.format(rf_f1))","07f2854d":"# Next we take Logistic Regression for Binary Classification\nfrom sklearn.linear_model import LogisticRegression\nlogit=LogisticRegression(class_weight='balanced', tol=1e-10)\n\n# Creare a model with X_train and Y_train data\nlogit.fit(X_train,Y_train)\n# predict probabilities\nprobs = logit.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]","00e5e4cc":"# Run the model on X_test to predict the target labels. Use cross-validation accuracy to check if model overfits or underfits\npredict3 = logit.predict(X_test)\nscore_logit= CrossVal(X_train,Y_train,logit)\nprint('Cross-Validation accuracy is {:.2f}%'.format(score_logit.mean()*100))","eb9a4b03":"# Compare the predicted target labels with Y_test\nfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score\nprint(\"Accuracy using Logistic Regression Model: {:.2f}%\".format(accuracy_score(Y_test,predict3)*100))\n# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,predict3)\n\n# calculate AUC\nauc_lr = roc_auc_score(Y_test, probs)\n#print('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(Y_test, probs)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title(\"ROC Curve for Logistic Regression with AUC Score: {:.3f}\".format(auc_lr))\n# show the plot\nplt.show()\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Purples', fmt = 'd')\nlogit_f1=f1_score(Y_test,predict3)\nplt.title('F1 Score for Logistic Regression model is {:.2f}'.format(logit_f1))","acc3bbb6":"# Next we take KNN Classifier for Binary Classification\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create an empty list to hold error values\nerror = []\n\n# Calculating error for K values between 1 and 30\nfor i in range(1, 30):  \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, Y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != Y_test))","995f15b1":"plt.figure(figsize=(12, 6))  \nplt.plot(range(1, 30), error, color='red', linestyle='dashed', marker='o',  \n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')  \nplt.xlabel('K Value')  \nplt.ylabel('Mean Error')  ","0f2fe725":"knn=KNeighborsClassifier(algorithm='auto',n_neighbors= 9)\n# Creare a model with X_train and Y_train data\nknn.fit(X_train,Y_train)\n# predict probabilities\nprobs = knn.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]","7923406e":"# Run the model on X_test to predict the target labels. Use cross-validation accuracy to check if model overfits or underfits\npredict4 = knn.predict(X_test)\nscore_knn= CrossVal(X_train,Y_train,knn)\nprint('Cross-Validation accuracy is {:.2f}%'.format(score_knn.mean()*100))","438ff4f6":"# Compare the predicted target labels with Y_test\nfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score\nprint(\"Accuracy using K Nearest Neighbours Model: {:.2f}%\".format(accuracy_score(Y_test,predict4)*100))\n# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,predict4)\n\n# calculate AUC\nauc_knn = roc_auc_score(Y_test, probs)\n#print('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(Y_test, probs)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title(\"ROC Curve for KNN with AUC Score: {:.3f}\".format(auc_knn))\n# show the plot\nplt.show()\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Purples', fmt = 'd')\nknn_f1=f1_score(Y_test,predict4)\nplt.title('F1 Score for K Nearest Neighbour model is {:.2f}'.format(knn_f1))","e5f4376b":"# Next we take AdaBoost for Binary Classification\nfrom sklearn.ensemble import AdaBoostClassifier\n# Using Random Forest model 'rf' for boosting\nada=AdaBoostClassifier(rf,n_estimators=100, random_state=40, learning_rate=0.1)\n# Creare a model with X_train and Y_train data\nada.fit(X_train,Y_train)\n# predict probabilities\nprobs = ada.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]","cb577c94":"# Run the model on X_test to predict the target labels. Use cross-validation accuracy to check if model overfits or underfits\npredict5 = ada.predict(X_test)\nscore_ada= CrossVal(X_train,Y_train,ada)\nprint('Cross-Validation accuracy is {:.2f}%'.format(score_ada.mean()*100))","7f65c4fe":"# Compare the predicted target labels with Y_test\nfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score\nprint(\"Accuracy using AdaBoost Model: {:.2f}%\".format(accuracy_score(Y_test,predict5)*100))\n# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,predict5)\n\n# calculate AUC\nauc_ada = roc_auc_score(Y_test, probs)\n#print('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(Y_test, probs)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title(\"ROC Curve for AdaBoost with AUC score: {:.3f}\".format(auc_ada))\n# show the plot\nplt.show()\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Purples', fmt = 'd')\nada_f1=f1_score(Y_test,predict5)\nplt.title('F1 Score for AdaBoost model is {:.2f}'.format(ada_f1))","0005abf0":"cv_svm = score_clf.mean()*100\ncv_rf = score_rf.mean()*100\ncv_lr = score_logit.mean()*100\ncv_knn = score_knn.mean()*100\ncv_ada = score_ada.mean()*100\n# Cross Validation Accuracy list for all models\ncv = [cv_svm, cv_rf, cv_lr, cv_knn, cv_ada]\n# F1 Score list for all models\nf1 = [svm_f1, rf_f1, logit_f1, knn_f1, ada_f1]\n# AUC Score list for all models\nauc = [auc_svm, auc_rf, auc_lr, auc_knn, auc_ada]\n# Name List of ML Models used\nmodels = ['SVM', 'Random Forest', 'Logistic Regression', 'KNN', 'AdaBoost']\ny_pos = np.arange(len(models)) #Position = 0,1,2,3,4\n\n# Plot Cross Validation Accuracy\nplt.figure(figsize=(10, 6))  \nplt.bar(y_pos, cv, align='center', alpha=0.8, color=sns.color_palette(\"PuRd\"))\nplt.xticks(y_pos, models)\nplt.ylabel('Cross Validated Accuracy')\nplt.title('Performance based on CV Accuracy')\n\n# Plot F1 Score\nplt.figure(figsize=(10, 6))  \nplt.bar(y_pos, f1, align='center', alpha=0.8, color=sns.color_palette(\"RdPu\"))\nplt.xticks(y_pos, models)\nplt.ylabel('F1 Score')\nplt.title('Performance based on F1 Score')\n\n# Plot AUC Score\nplt.figure(figsize=(10, 6))  \nplt.bar(y_pos, auc, align='center', alpha=0.8, color=sns.color_palette(\"BuPu\"))\nplt.xticks(y_pos, models)\nplt.ylabel('AUC Score')\nplt.title('Performance based on AUC Score')\n","50c1f9a3":"### Preprocessing and Cleaning","eddf71b7":"### 5) Using our fifth Machine Learning Approach: AdaBoost","ec2e4f59":"### 4) Using our forth Machine Learning Approach: KNN\nIn order to identify the ideal number of neighbours for KNN, we first calculate the error percentage each neighbour might result in. The lower the error the better our model fits on that current set of neighbours.","1074c3b3":"# Conclusion","c6e5d0da":"### How to take care of your heart: <br>\nThese simple steps can greatly reduce your risk of developing heart disease:\n\n1. Eating a heart-healthy diet that includes fruits, vegetables, whole grains and fish\n1. Engaging in at least 30 minutes of moderate aerobic activity per day\n1. Maintaining a healthy weight\n1. Quitting smoking\n1. Taking necessary medications for hypertension, diabetes or high cholesterol if directed by your physician\n\n![Visit Keck Medicine of USC](https:\/\/www.keckmedicine.org\/wp-content\/uploads\/2018\/02\/USC-17365_CVTIinfographic_170914.jpg)","33c2c109":"### Visualizing the ratio of dataset based on (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)","931c4b31":"### Checking the distribution of feature 'chol: serum cholestoral in mg\/dl'","644f7e07":"### One-Hot Encoding categorical variables","7b35fe87":"### **The Dataset used for this challenge is [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)**","17c29fe5":"### Plot displaying potential of heart disease based on various levels of chest pain types by age","18f4fa1f":"### Plot displaying male and female with potential heart disease based on age","271998f6":"The distribution is normal indicating most population fall between 140-180 with some left tail indicating few outliers which we can validate once we create a scatterplot  with residuals, leverage and cook's distance.","f7c41782":"### Categorical attributes are :-\n\"cp\",\"fbs\", \"restecg\", \"ca\", \"thal\"\n\n### Numeric attributes are :-\n\"age\", \"sex\", \"trestbps\", \"chol\", \"thalach\", \"exang\", \"oldpeak\", \"slope\"","9608c053":"**Based on the above two comparison plots: Cross-Validated Accuracy and F1 Score, K Nearest Neighbours (KNN) seems to have an ideal balance between underfitting and overfitting along with a high F1 Score. KNN also outperforms the competition with the highest test accuracy of 86.81% and AUC Score of 0.908. <br> Thus out of the 5 models used above KNN will be our choice of selection.**","02b1c2b0":"# Ingestion\n### Import Libraries","04e8091b":"### Splitting the dataset into test and train","dd16f5c2":"### Description of each column\n* age: age in years\n* sex: female=0; male=1\n* cp: chest pain type (4 levels)\n* trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n* chol: serum cholestoral in mg\/dl\n* fbs: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* restecg: resting electrocardiographic results (3 levels)\n* thalach: maximum heart rate achieved\n* exang: exercise induced angina (1 = yes; 0 = no)\n* oldpeak: ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n* ca: number of major vessels (0-3) colored by flourosopy (3 levels)\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n* target: (class variable) with 1 or 0","c7974327":"# Modeling\n### Define a cross-validation function\nWe are using cross-validation to predict our training accuracy in order to avoid underfitting and overfitting of our model. The closer the results of cross-validation accracy and test accuracy will determine the ideal fit of our model.","1f84bc06":"The distribution is normal indicating most population falls between 200-300 which some right tail indicating few outliers which we can validate once we create a scatterplot with residuals, leverage and cook's distance.","41cfc620":"### 3) Using our third Machine Learning Approach: Logistic Regression","d06caf5a":"### Find and Replace Labels","1de548a3":"### 1) Using our first Machine Learning Approach: SVM","d554c2ab":"As the heart is one of the body\u2019s most essential organs, it is important to understand how to keep it healthy, especially because heart disease is the leading cause of death among both men and women in the U.S.\n\nYou can reduce your risk for developing heart disease by learning about the various types of heart disease, understanding the symptoms associated with heart disease and adopting heart-healthy lifestyle strategies.\n\n[For more info](https:\/\/www.keckmedicine.org\/4-types-of-heart-disease-and-how-to-help-prevent-them\/)","f7f8907f":"# Visualization\n### Visualizing the ratio of dataset based on attribute sex","a90178a0":"### Visualizing the ratio of different chest pain types in the dataset","5c3fb72d":"### Displaying the structure of dataset","c4af7563":"### Identifying the shape of the dataset","cff91975":"### Checking the distribution of feature 'thalach: maximum heart rate achieved'","977f03f2":"Since there are no NaNs here thus we do not require to impute any missing values","ae932dda":"### Visualizing the distribution of people having heart disease based on age","ce102f6d":"# Data Preparation","faf98aad":"**They are roughly the same so we can use ROC curve as it summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds. <br>\nPrecision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds. <br>\nROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n<br> [More info](https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/)**","c1e8fe5b":"### Read the dataset and save to variable 'datafr'","5f52284c":"![](https:\/\/cdn-img.health.com\/sites\/default\/files\/styles\/medium_16_9\/public\/styles\/main\/public\/166421106.jpg?itok=m7byxM2V)","381d4626":"### 2) Using our second Machine Learning Approach: Random Forest","18a6f164":"From the above figure we can see that we get a minimum error of around 0.14 when k is either 9 or 12.","f4b751eb":"### Identify the number of observation for target variable for each class"}}