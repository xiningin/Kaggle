{"cell_type":{"867a349b":"code","38690d43":"code","13fff9e9":"code","5cae39bd":"code","3cf4610e":"code","8cf3b604":"code","c770ca6f":"code","40a99808":"code","d2cd8238":"code","2f82eb46":"code","6f8a2a1b":"code","f1078b81":"code","3745d59d":"code","e6f1ffa8":"code","3c073bbb":"code","94b30ab2":"code","173c6e74":"code","df9960c2":"code","7c0ae0f0":"code","5a4bc285":"code","77e71de6":"code","5a8ca85b":"code","24f8fd24":"code","539b0486":"code","75e5ebc5":"code","b926e7e2":"code","1412016b":"code","001f9137":"code","d1311aba":"code","ba7155b7":"code","77b5869a":"code","642ac37d":"code","54c0cce2":"markdown","e2bcef8d":"markdown","9c0e886d":"markdown","14f7c855":"markdown","8c1d864b":"markdown","8dbc3bed":"markdown","6cac0158":"markdown","015334d5":"markdown","9e6bca83":"markdown","18b25066":"markdown","a9011be6":"markdown","bdd06d0d":"markdown","61fbdf32":"markdown","9cc54df6":"markdown","74213435":"markdown","22f6cc0e":"markdown","407ee1c3":"markdown","85ce312f":"markdown","f11cfe2f":"markdown","297759f2":"markdown","198fdb3b":"markdown","c4c5a22c":"markdown","44277736":"markdown","8d925d39":"markdown","e0f85850":"markdown","ca732d57":"markdown","33dfd581":"markdown","7bb19e8b":"markdown","e6a06293":"markdown","cd7bb4ed":"markdown","701eff25":"markdown","9e1d0c0a":"markdown","daa0fb11":"markdown","90016d14":"markdown","2ddeac5e":"markdown","9bb8b6e2":"markdown","d087b315":"markdown","ca0b3c25":"markdown","a6e6fa0c":"markdown","90315f97":"markdown","9cdd5f04":"markdown","00c38e3d":"markdown","d9181800":"markdown","3c53e29b":"markdown","6e1e3559":"markdown","7f040658":"markdown","012a50b1":"markdown"},"source":{"867a349b":"#import data packages\nimport math as m\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport seaborn as sns #(sam seaborn!!)\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","38690d43":"#Input csv filed into two data fram object: train_df and test_df\n#train = 891 entries of passengers, features, with labels if survived or died \n#test = 418 entries of titanic passengers, features, no labels\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\ncombine = [train_df, test_df]","13fff9e9":"#Examine first 10 rows to take a look at the data \n#note: passengerID randomly assigned by Kaggle (correlates with index)\ntrain_df[:10]","5cae39bd":"#print info\ntrain_df.info()","3cf4610e":"#lookin at test set\nprint(test_df.info())","8cf3b604":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c770ca6f":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False) ","40a99808":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d2cd8238":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2f82eb46":"train_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False) ","6f8a2a1b":"#sns.barplot(x = \"Family\", y = \"Survived\", data = train_df)","f1078b81":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n#grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n#grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n#grid.add_legend();","3745d59d":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","e6f1ffa8":"# changing males to 0 and females to 1 , changing the type to an integer \nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","3c073bbb":"#combining Sibling and spouses with parents and children to create a new family column \ntrain_df[\"Family\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]\ntest_df[\"Family\"] = test_df[\"SibSp\"] + test_df[\"Parch\"] \n\ntrain_df.head()","94b30ab2":"#drop columns that Family replaces \ntrain_df.drop('Parch', axis = 1, inplace = True)\ntrain_df.drop('SibSp', axis = 1, inplace = True)\ntest_df.drop('Parch', axis = 1, inplace = True)\ntest_df.drop('SibSp', axis = 1, inplace = True)\n\ntrain_df.head()","173c6e74":"test_df[\"IsAlone\"] = 0\ntrain_df[\"IsAlone\"] = 0\ntrain_df.loc[train_df['Family'] == 1, 'IsAlone'] = 1\ntest_df.loc[test_df['Family'] == 1, 'IsAlone'] = 1\n\n#This was Manav's code (which I modified)\n#for dataset in combine:\n    #dataset['IsAlone'] = 0\n    #dataset.loc[dataset['Family'] == 1, 'IsAlone'] = 1\n\ntrain_df.info()","df9960c2":"train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","7c0ae0f0":"for val in train_df.columns:\n    if val != 'IsAlone' and val != 'PassengerId' and val != 'Pclass' and val != 'Sex' and val != 'Survived':\n        train_df = train_df.drop([val], axis=1)\nfor val in test_df.columns:\n    if val != 'IsAlone' and val != 'PassengerId' and val != 'Pclass' and val != 'Sex':\n        test_df = test_df.drop([val], axis=1)","5a4bc285":"train_df.head()","77e71de6":"test_df.head() ","5a8ca85b":"#split training data into the feature list w\/o passenger id and labels (survived or not)\n# features - x\n# labels - y\nX_train = train_df.drop([\"Survived\", \"PassengerId\"], axis=1)\nY_train = train_df[\"Survived\"]\n\nX_test = test_df.drop('PassengerId', axis=1)\n\nX_train.head()\nY_train.head()","24f8fd24":"#from sklearn.cross_validation import train_test_split\n#train_test_split splits our data for us into training and testing! (in a cross validation)\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import train_test_split\n\n#this means a 70-30 split, pulling out 30% of data for test size (what percent of the data goes into the test)\n#4 variables are returned from the method call\n\nvalid_X_train, valid_X_test, valid_Y_train, valid_Y_test = train_test_split(X_train, Y_train, train_size = .7, test_size = .3)\n\nvalid_X_train.shape, valid_X_test.shape, valid_Y_train.shape, valid_Y_test.shape \n ","539b0486":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(valid_X_train, valid_Y_train)\nY_pred = logreg.predict(valid_X_test)\n\nprint(accuracy_score(valid_Y_test, Y_pred)* 100)\n\n#a check to see if data overfitted- if these numbers are close together, then the data is not overfit \n#this uses the method of the used classifier\n# I got this specific code from Ms. Sconyers!\nprint((logreg.score(X_train, Y_train)*100), logreg.score(valid_X_train, valid_Y_train)*100)","75e5ebc5":"svc = SVC()\nsvc.fit(valid_X_train, valid_Y_train)\nY_pred = svc.predict(valid_X_test)\n#acc_svc = round(svc.score(valid_X_train, valid_Y_train) * 100, 2)\n#acc_svc\n\nprint(accuracy_score(valid_Y_test, Y_pred)* 100)\n\n#a check to see if data overfitted- if these numbers are close together, then the data is not overfit \nprint((svc.score(X_train, Y_train)*100), svc.score(valid_X_train, valid_Y_train)*100)","b926e7e2":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(valid_X_train, valid_Y_train)\nY_pred = knn.predict(valid_X_test)\n#acc_knn = round(knn.score(valid_X_train, valid_Y_train) * 100, 2)\n#acc_knn\n\nprint(accuracy_score(valid_Y_test, Y_pred)* 100)\n\n#a check to see if data overfitted- if these numbers are close together, then the data is not overfit \nprint((knn.score(X_train, Y_train)*100), knn.score(valid_X_train, valid_Y_train)*100)","1412016b":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(valid_X_train, valid_Y_train)\nY_pred = gaussian.predict(valid_X_test)\n#acc_gaussian = round(gaussian.score(valid_X_train, valid_Y_train) * 100, 2)\n#acc_gaussian\n\nprint(accuracy_score(valid_Y_test, Y_pred)* 100)\n\n#a check to see if data overfitted- if these numbers are close together, then the data is not overfit \nprint((gaussian.score(X_train, Y_train)*100), gaussian.score(valid_X_train, valid_Y_train)*100)","001f9137":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(valid_X_train, valid_Y_train)\nY_pred = decision_tree.predict(valid_X_test)\n#acc_decision_tree = round(decision_tree.score(valid_X_train, valid_Y_train) * 100, 2)\n#acc_decision_tree\n\nprint(accuracy_score(valid_Y_test, Y_pred)* 100)\n\n#a check to see if data overfitted- if these numbers are close together, then the data is not overfit \nprint((decision_tree.score(X_train, Y_train)*100), decision_tree.score(valid_X_train, valid_Y_train)*100)","d1311aba":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(valid_X_train, valid_Y_train)\nY_pred = random_forest.predict(valid_X_test)\nrandom_forest.score(valid_X_train, valid_Y_train)\n#acc_random_forest = round(random_forest.score(valid_X_train, valid_Y_train) * 100, 2)\n#acc_random_forest\n\nprint(accuracy_score(valid_Y_test, Y_pred)* 100)\n\n#a check to see if data overfitted- if these numbers are close together, then the data is not overfit \nprint((random_forest.score(X_train, Y_train)*100), random_forest.score(valid_X_train, valid_Y_train)*100)","ba7155b7":"tree_pred = decision_tree.predict(X_test)","77b5869a":"final_pred = tree_pred\nsubmission = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': final_pred\n})\nsubmission.to_csv('submission.csv', index=False)","642ac37d":"submission.info()","54c0cce2":"## Input Data Sets","e2bcef8d":"First, I wanted to take a clean look at the first 10 lines of data from the train set (which I have printed below) to get a look at what trends I notice off the bat, what the headers are, and what I may need to clean. ","9c0e886d":"These are what my two data sets now look like:","14f7c855":"Then, I combined columns for parents\/children and sibling\/spouses to just family members- because there is a consistant linear regression: those who had more family members were less likely to survive. ","8c1d864b":"Citation for correlation tests: https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions lines 9-12 from Manav Sehgal. Throughout my lab, I referred to his notebook, as it was a very helpful way to get started. I only used a few pieces of his code, but referring to his code on data analysis, and changing female and male to integers was really helpful. ","8dbc3bed":"Lillian Ellis \/\/ Completed 12\/9\/18","6cac0158":"Links to Sources:\n\nhttps:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8 \n\nhttps:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","015334d5":"(note: I commented out my first graph, although it worked, because it was using too much memory and it kept erroring out) ","9e6bca83":"As you can see below, being alone was a clear advantage, as 34% of those not alone survived, while 55% of those who were alone survived!","18b25066":"Still lots of missing ages, cabins (lots of missing data!), one fare missing. There are integer, object, and float values. \n\nOverall:\n- both sets have all features, there is nothing off here\n- most features are pretty complete exept for age, cabin\n- some missing features surrounding embarking location and fare","a9011be6":"Next, I imported and defined my data sets (both train and test) that I will use throughout the rest of my lab. Throughout my lab, I made sure that everything I did for test, I also did for train, to ensure both data sets contained the same values. ","bdd06d0d":"## Check\/Analyze Data","61fbdf32":"**Questions I still Have:**","9cc54df6":"**Printing Train Info**","74213435":"Most of my data looks pretty clean, but I do notice that there are a few missing values. I next printed the info of train to get an overview of the entire data set. ","22f6cc0e":"From looking at this, we have lots of data for everything execpt for age (limited knowledge) and cabin (cabin numbers for 1st class). Data is also missing data for 2 passengers on where they embarked. We have fairly solid and consistent data with all of the other columns, which have 891 values each. ","407ee1c3":"**Printing Test Info**","85ce312f":"## Import Packages","f11cfe2f":"First, I imported all of my dataset packages, along with machine learning packages to use for the lab. ","297759f2":"Here I printed some graphs to make the correlation tests above easier to read. ","198fdb3b":"- Why didn't combine work for my data set? (still a mystery)\n- Julia ended up getting a higher accuracy score than mine: using sex, passenger Id, and fare\n    - But when I tried just using the same features she used, my accuracy score greatly decreased\n    - Why did that happen? ","c4c5a22c":"**Additional Notes:**","44277736":"List of Data Cleaning Tasks:\n- print basic correlations between passenger, class, sex, SipSp, Embarked, etc. (as these are not missing much data, pretty telling)\n- print graphs to see other correlations \n- change female and male to 1 and 0\n- combine SibSp and Parch into family \n- Create other values based on finds from correlations","8d925d39":"- I would have really loved to have achieved a higher accuracy score, and I think I could have done this by changing the weight for the various columns I ended up training my classifiers with\n    - for example, because sex is the greatest determinate of survival, it would be great if I could find a way to have that column carry more weight overall. I think this could be done with neural networks, and I hope to explore this in the future! \n    ","e0f85850":"## Feature Analysis and Clean Up of Data","ca732d57":"- I tried to combine different columns (other than just SibSp and Parch) into a combined data to maybe give more weight to one column over the other, as I thought that overlapping would boost my accuracy score\n    - I tried a few different combinations, and ultimatley decided that it acctually greatly decreased my accuracy score\n    - In the future, however, I hope to find out different ways to overlap data ","33dfd581":"**Acknowledgements:**","7bb19e8b":"## Titanic Lab Competition Entry 2018","e6a06293":"## Training Classifiers","cd7bb4ed":"The last step I took before training my machine was deleting all of the columns other than the ones I found were the most relavant, telling, full, and now clean! (IsAlone, Class, and Sex). I am also keeping passenger Id in both, which I will delete later, but I need it for identifying each passenger for my final submission. ","701eff25":"In this lab, I participated in the Kaggle Titanic competition where we worked to train a machine to predict who lived and who died on the Titanic. We were given both a training data set and a test data set to train and test our machines! As I found throughout the lab, there were some reoccuring trends for who survived and who did not. Throughout my lab, I learned a lot about how to take analysis from a data set and acctually get to make predictions on it!","9e1d0c0a":"**Main Takeaways from Correlations:**","daa0fb11":"- sex is the biggest indicator of survival!\n    - 74% of females survive , 19% of males survive\n- class is also a pretty big indicator \n    - 63% of those in 1st class survived, 47% in 2nd, and 24% in 3rd\n- whether you were alone or not is also pretty big!\n    - the more family members you had on board, the less likely you were to survive ","90016d14":"- Manav Sehgal greatly helped me with my code, and I used a lot of strategy and code from him. His notebook allowed me to analyze effectively and I learned many new classifiers from his notebook. \n- Ms. Sconyers greatly helped me navigate through errors in my code, and helped me strategize on which data I ended up using for training my classifiers. I couldn't have completed this lab without all of her help!","2ddeac5e":"Next, I deleted  sibsp and parch in both data sets - as I will now instead using new column of family ","9bb8b6e2":"I conducted some analysis and clean up on the features to determine a good set to work with when training my classifier- based on the features that will tell us the labels (these must be numerical)\n\nDetermining:\n- which features to get rid of \n- which features to clean\n- **which features correlate to labels (and how accurate each one is)**\n- how to handle missing data\n- maybe we will need to combine data (gender and class? gender and embarked? etc.)\n\nNeed to clean up features for both training and test at the same time (so these align and chance at accurate feature is good)","d087b315":"Now that I am done with cleaning, and selected the data I want to keep in both, I am ready to train my classifier. I first created the machine, fed in the feature set to train the machine with my labels, and then started to make predictions with 6 different classifiers and printed the accuracy method! ","ca0b3c25":"**Areas of Future Exploration:**","a6e6fa0c":"Now that I have finished printing my accuracy, I have decided that I will use logreg as my submission and winning classifier, as it consistently prints an accuracy score around 80. I will now predict with the X_test (not valid_X_test), and create a submission csv with that classifier. ","90315f97":"Note: Throughout my lab, I greatly referred to Manav Sehgal's notebook on the Kaggle competition. In Manav's notebook, he includes the combine data set defined above (test and train combined). I had some troubles with combine throughout my lab, but it was helpful when converting male and female passengers to 0 and 1. Here is the link to his notebook: https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions ","9cdd5f04":"TO DO:\n- create machine\n- feed feature set to train machine with the labels\n- start to make predictions and get accuracy rate up! ","00c38e3d":"## Pick the Best Classifier and Create Submission ","d9181800":"Both decision tree and random forest consistently print an accuracy score around 80, which is highest out of all of my classifiers. ","3c53e29b":"My first step in cleaning the sex column was to convert females to 1 and males to 0. I got this code from Manav's notebook. ","6e1e3559":"So, I decided to especially focus on sex, class, and family members. ","7f040658":"**Abstract:**","012a50b1":"However, I then decided to create an \"IsAlone\" column- which indicates if you have family members or are all alone (because if you are alone- higher chance of survival). This new column simplified data, and eliminated potential outliers. I also got this idea from Manav, but I modified it as I was having trouble with the \"combine\" data set. "}}