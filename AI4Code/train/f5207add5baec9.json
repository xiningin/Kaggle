{"cell_type":{"1d9ec834":"code","6f1fdf39":"code","6347377b":"code","04665de0":"code","35255a7a":"code","a304cbe2":"code","3d5a55f0":"code","f8a0d4fe":"code","53993612":"code","12c1546c":"code","a0d32a00":"code","caf236af":"code","f7d67303":"code","ad872e63":"code","392c7c8e":"code","a31d318b":"code","c2dac126":"code","825a630c":"code","5e83f32d":"code","dcfb03fe":"code","f84c527d":"code","2a4c287f":"code","6c90021f":"code","54b714b4":"markdown","7776b76d":"markdown","5945b25f":"markdown"},"source":{"1d9ec834":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom sklearn.metrics import confusion_matrix, classification_report","6f1fdf39":"image_dir = Path('..\/input\/food41\/images')","6347377b":"filepaths = list(image_dir.glob(r'**\/*.jpg'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\nimage_df = pd.concat([filepaths, labels], axis=1).sample(frac=1.0, random_state=1).reset_index(drop=True)","04665de0":"# To check all the classes have same number of elements\nimage_df['Label'].value_counts()","35255a7a":"len(image_df['Label'].value_counts())","a304cbe2":"# To know what is inside a dir\n!ls '..\/input\/food41\/images\/'","3d5a55f0":"# Displaying Image and Label from the Path\nfrom PIL import Image\npath = image_df['Filepath'][3]\nlabel = image_df['Label'][3]\nimg_arr = np.array(Image.open(path))\nprint(label)\nplt.imshow(img_arr)\nplt.show()","f8a0d4fe":"train_df, test_df = train_test_split(image_df, train_size=0.75, shuffle=True, random_state=1)","53993612":"# Preprocessing the input image using MobileNetV2\ntrain_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)\n\n# train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n#         rescale=1.\/255,\n#         rotation_range=30,\n#         shear_range=0.3,\n#         horizontal_flip=True,\n#         width_shift_range=0.1,\n#         height_shift_range=0.1,\n#         zoom_range=0.25,\n#         validation_split=0.2\n# )\n# test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n#         rescale = 1.\/255\n# )\n\n# data_augmentation = tf.keras.Sequential([\n#      layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n#                                                  input_shape=(224, \n#                                                               224,\n#                                                               3)),\n#     layers.experimental.preprocessing.RandomRotation(0.1),\n#     layers.experimental.preprocessing.RandomZoom(0.1),\n# ])","12c1546c":"batch_size = 128\n\n# Image Augmentation\ntrain_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=batch_size,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=batch_size,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=batch_size,\n    shuffle=False\n)","a0d32a00":"# Show All labels with indices\nclasses = list(train_data.class_indices.keys())\nclasses","caf236af":"# Some Extra Things that can be tried\n# X_train , Y_train = train_images.next()\n# X_test , Y_test = test_images.next()\n# For image classification from one hot encoding labels from train_generator.flow_from_dataframe\n# Categories = os.listdir('..\/input\/food41\/images')\n# Categories\n# print(train_images.class_indices)\n# Getting more info \n# print(train_df['Label'])\n# print(train_df['Filepath'])\n\n#print(train_images.labels)\n\n# One Hot encoding\n# images , labels = train_images.next()\n# print(Categories[np.argmax(labels[0])])\n# plt.imshow(images[0])","f7d67303":"pretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\n# Not training the pretrained layers\npretrained_model.trainable = False","ad872e63":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n#x = tf.keras.layers.Dropout(0.5)(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(101, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\n\n# from tensorflow.keras import Sequential\n# from tensorflow.keras.layers import Dense,Dropout\n# model = Sequential()\n# model.add(pretrained_model)\n# model.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.5))\n# model.add(Dense(101, activation='softmax'))","392c7c8e":"from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n\nopt = Adam(learning_rate=0.001)\nmodel.compile(\n    optimizer=opt,\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss',patience = 1,verbose = 1)\nearly_stop = EarlyStopping(monitor = 'val_loss',patience = 5,verbose = 1,restore_best_weights = True)\n# log = \"logs\/scalars\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ncheckpointer = ModelCheckpoint('model4b.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('model4b.log')\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    epochs=7,\n    callbacks=[early_stop, csv_logger , reduce_lr, checkpointer]\n)","a31d318b":"# How the model performs on the Test Data\nresults = model.evaluate(test_images, verbose=0)\nprint(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))","c2dac126":"# Saving model to different extension\nmodel.save('foodpedia.model')","825a630c":"# If you want to print the model summary\n# print(model.summary())","5e83f32d":"# Plotting the Confusion Matrix and Classification Report to see how our model performs \npredictions = np.argmax(model.predict(test_images), axis=1)\n\ncm = confusion_matrix(test_images.labels, predictions)\nclr = classification_report(test_images.labels, predictions, target_names=test_images.class_indices, zero_division=0)\n\n# confusion matrix\nplt.figure(figsize=(30, 30))\nsns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\nplt.xticks(ticks=np.arange(101) + 0.5, labels=test_images.class_indices, rotation=90)\nplt.yticks(ticks=np.arange(101) + 0.5, labels=test_images.class_indices, rotation=0)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","dcfb03fe":"# Classification Report\nprint(\"Classification Report:\\n----------------------\\n\", clr)","f84c527d":"# Testing Models on single Images (IMP)\nfrom tensorflow.keras.preprocessing import image\nimg_path = '..\/input\/food41\/images\/chocolate_cake\/1001084.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nplt.imshow(img)\nimg_array = image.img_to_array(img)\nimg_batch = np.expand_dims(img_array, axis=0)\nimg_preprocessed = tf.keras.applications.mobilenet_v2.preprocess_input(img_batch)","2a4c287f":"Categories = sorted(os.listdir('..\/input\/food41\/images'))\nCategories","6c90021f":"# Prediciting\nCategories[np.argmax(model.predict([img_preprocessed]))]","54b714b4":"## Additional Models that can be tried to increase accuracy\n1. Google InceptionV3 (recommended)\n2. ResNet50\n3. EfficientNetB0 **\n4. VGG-13\n<hr>","7776b76d":"## Top 1% accuracy of 70% and val_accuracy 0f 58.3%","5945b25f":"# Foodpedia (V1) using MobileNet_V2\n"}}