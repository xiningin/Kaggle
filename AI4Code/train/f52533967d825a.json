{"cell_type":{"417beb27":"code","6e6f6d19":"code","465a8a7b":"code","5a1f29ca":"code","69567f83":"code","2d8c7f50":"code","410d962d":"code","6064e8ac":"code","015762b1":"code","8bcd215a":"code","b891efff":"code","9889cd93":"code","3e6ee565":"code","df3c6658":"code","7e41225b":"code","956b7e06":"code","daccca00":"code","007c0771":"code","a067c3f2":"code","1707e441":"code","2321053f":"code","4c4da727":"code","5c750f74":"code","61d29806":"code","1d51208b":"code","802a92a8":"code","15e4ffe9":"code","ca2eb075":"code","255a5320":"code","3c9b39f8":"code","8723a9b4":"code","fa8cebb1":"code","94b2b67f":"code","1fefe9dc":"code","bfaa5c44":"code","bd033ee2":"code","ee3ffe13":"code","16d01bd9":"code","a3a0ec5e":"code","3f8f0ede":"code","cccf0179":"code","38eeca06":"code","c1780ce7":"code","cdeeb539":"code","d74cd163":"markdown","07291a22":"markdown","c6d29938":"markdown","4da67af6":"markdown","daca5fd5":"markdown","3d3aabfd":"markdown","4abd78eb":"markdown","8fbbdb11":"markdown","c34979bf":"markdown","ccebfc48":"markdown","c453cdcd":"markdown","b98414c1":"markdown","c906d1db":"markdown","439f1180":"markdown","174693b1":"markdown","c8de35b6":"markdown","68dbb446":"markdown","1bcb44e1":"markdown","18f3ceb1":"markdown"},"source":{"417beb27":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.ensemble import AdaBoostClassifier,VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport lightgbm as lgb\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nwarnings.filterwarnings('ignore')","6e6f6d19":"!pip install openpyxl","465a8a7b":"# Read spreadsheet and assign it to swiss_loan\nswiss_loan= pd.read_excel('..\/input\/loan-credit-risk\/ATUCE_Case_study_data_2021.xlsx')","5a1f29ca":"# Create a boolean mask on whether each feature less than 40% missing values.\nmask = swiss_loan.isna().sum() \/ len(swiss_loan) < 0.4\n# Create a reduced dataset by applying the mask\nreduced_df =swiss_loan.loc[:, mask]\n\n# drop ID\nreduced_df.drop('ID', axis=1, inplace=True)\nreduced_df['Pays_corr']=reduced_df['Pays'].str.strip()\nreduced_df['Taux_corr']=reduced_df['Taux'].str.replace('%','').str.strip().str.replace(',','.').str.extract(r'(\\d+.\\d+)')\nreduced_df['Taux_corr'] = np.where(reduced_df['Taux_corr'].isnull(), 0.1, reduced_df['Taux_corr'])\nreduced_df['Taux_corr'] = pd.to_numeric(reduced_df['Taux_corr'], errors='coerce')\n# Specify the boundaries of the bins\nbins = [0.001,5.5,  6.5, 10]\n# Bin labels\nlabels = [ 'Low', 'Medium', 'High']\n# Bin the continuous variable ConvertedSalary using these boundaries\nreduced_df['Taux_corr_binned'] = pd.cut(reduced_df['Taux_corr'], \n                                         bins=bins,labels=labels )\n# Print the first 5 rows of the boundary_binned column\nreduced_df['Montant_corr']=reduced_df['Montant'].str.replace('\u20ac','').str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['Montant_corr'] = np.where(reduced_df['Montant_corr'].isnull(), 0.1, reduced_df['Montant_corr'])\nreduced_df['Montant_corr'] = pd.to_numeric(reduced_df['Montant_corr'], errors='coerce')\nreduced_df['Niveau_risque_corr']=reduced_df['Niveau de risque'].str.rstrip().str.replace('\\s+','')\nEmprunteurs = reduced_df['Emprunteur']\n\n\nEmprunteurs_counts = Emprunteurs.value_counts()\n\n# Create a mask for only categories that occur less than 5 times\nmask = Emprunteurs.isin(Emprunteurs_counts[Emprunteurs_counts<5].index)\n# Label all other categories as Other\nreduced_df['Emprunteur'][mask] = 'Other'\nreduced_df['capital_social_corr']=reduced_df['capital social'].str.replace('\u20ac','').str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['capital_social_corr'] = np.where(reduced_df['capital_social_corr'].isnull(), 0.1, reduced_df['capital_social_corr'])\nreduced_df['capital_social_corr'] = pd.to_numeric(reduced_df['capital_social_corr'], errors='coerce')\n\nreduced_df['Effectifse_corr']=reduced_df['effectifs'].str.rstrip().str.replace('\\s+','')\nreduced_df['Effectifse_corr'][reduced_df['Effectifse_corr'] == '-'] = np.nan\n\nreduced_df['Nombre_mois_p\u00e9riode16_corr']=reduced_df['Nombre de mois de la p\u00e9riode 16'].str.rstrip().str.replace('mois','').str.replace(',','.').str.replace('\\s+','').str.extract(r\"(\\d+\\.\\d+|\\d+)\")\nreduced_df['Nombre_mois_p\u00e9riode16_corr'][reduced_df['Nombre_mois_p\u00e9riode16_corr'] == '-'] = np.nan\nreduced_df['Nombre_mois_p\u00e9riode16_corr'] = pd.to_numeric(reduced_df['Nombre_mois_p\u00e9riode16_corr'], errors='coerce')\n\nreduced_df['Chiffre_Affaires_16_corr']=reduced_df.iloc[:,12].str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['Chiffre_Affaires_16_corr'] = pd.to_numeric(reduced_df['Chiffre_Affaires_16_corr'], errors='coerce')\n\nreduced_df['Total_Bilan_16_corr']= reduced_df['Total Bilan 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Total_Bilan_16_corr']= pd.to_numeric(reduced_df['Total_Bilan_16_corr'], errors='coerce')\n\nreduced_df['Capacit\u00e9_remboursement_FCCR_16_corr']= reduced_df['Capacit\u00e9 de remboursement (FCCR) 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Capacit\u00e9_remboursement_FCCR_16_corr']= pd.to_numeric(reduced_df['Capacit\u00e9_remboursement_FCCR_16_corr'], errors='coerce')\n\n\nreduced_df['Fonds_Propres_16_corr']= reduced_df['Fonds Propres 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Fonds_Propres_16_corr']= pd.to_numeric(reduced_df['Fonds_Propres_16_corr'], errors='coerce')\n\nreduced_df['Fonds_Propres_Total_Bilan_corr']= reduced_df['Fonds Propres \/ Total Bilan 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('%','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Fonds_Propres_Total_Bilan_corr']= pd.to_numeric(reduced_df['Fonds_Propres_Total_Bilan_corr'], errors='coerce')\n\nreduced_df['Dettes_Nettes_EBE_16_corr']= reduced_df['Dettes Nettes \/ EBE(* ann\u00e9es) 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('*','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Dettes_Nettes_EBE_16_corr']= pd.to_numeric(reduced_df['Dettes_Nettes_EBE_16_corr'], errors='coerce')\n\nreduced_df['DettesNettes_Fonds_propres_16_corr']= reduced_df['Dettes Nettes \/ Fonds propres 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('%','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['DettesNettes_Fonds_propres_16_corr']= pd.to_numeric(reduced_df['DettesNettes_Fonds_propres_16_corr'], errors='coerce')\n\n\n# Apply the log normalization function \nreduced_df['Montant_corr_log'] = np.log(reduced_df['Montant_corr'])\n\n# Apply the log normalization function \nreduced_df['Chiffre_Affaires_16_corr_log'] = np.log(reduced_df['Chiffre_Affaires_16_corr'])\n# Apply the log normalization function t\nreduced_df['capital_social_corr_log'] = np.log(reduced_df['capital_social_corr']+1)\n# This function converts given date to age\ndef age(creation):\n    born = int(creation)\n    today = date.today()\n    return today.year - born\n  \nreduced_df['Age'] = reduced_df['ann\u00e9e de cr\u00e9ation'].apply(age)","69567f83":"list_to_keep= [ 'Pays_corr','Mois','Age' ,'Taux_corr', 'Taux_corr_binned', 'Montant_corr_log',\n       'Niveau_risque_corr','Emprunteur', 'capital_social_corr_log', 'Effectifse_corr',\n       'Nombre_mois_p\u00e9riode16_corr', 'Capacit\u00e9_remboursement_FCCR_16_corr','Total_Bilan_16_corr',\n       'Fonds_Propres_16_corr', 'Fonds_Propres_Total_Bilan_corr',\n       'Dettes_Nettes_EBE_16_corr', 'DettesNettes_Fonds_propres_16_corr','Chiffre_Affaires_16_corr_log']\nclean_reduced_df= reduced_df[list_to_keep].copy()\nclean_reduced_df.shape","2d8c7f50":"%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Create a series out of the Country column\nTaux_binned = reduced_df['Taux_corr_binned']\n\n# Get the counts of each category\nTaux_counts = Taux_binned.value_counts()\n\n# Print the count values for each category\nprint(Taux_counts)\nsns.set(style=\"darkgrid\")\nsns.barplot(Taux_counts.index, Taux_counts.values, alpha=0.9)\nplt.title('Frequency Distribution of interst rate')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Interst rate', fontsize=12)\nplt.show()","410d962d":"# Create arrays for the features and the response variable\ncolonne_cible = \"Taux_corr_binned\"\ny= clean_reduced_df['Taux_corr_binned']\nX = clean_reduced_df.drop(['Taux_corr','Taux_corr_binned'], axis=1)","6064e8ac":"np.unique(y)","015762b1":"# select the float columns\nnum_columns = X.select_dtypes(include=['int64','float64']).columns\n# select non-numeric columns\ncat_columns = X.select_dtypes(exclude=['int64','float64']).columns\n\nfill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_Standar_scaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    StandardScaler()\n)\ndata_preprocess = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_Standar_scaler, num_columns)\n)\n\nX_pre =data_preprocess.fit_transform(X)\ny_pre= LabelEncoder().fit_transform(clean_reduced_df['Taux_corr_binned'].astype(str))\n# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_pre, y_pre, test_size=0.1)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","8bcd215a":"np.unique(y_pre)","b891efff":"# Compare the number of features of the resulting DataFrames\nprint(X_pre.shape[1] > X.shape[1])","9889cd93":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(multi_class='multinomial', solver='lbfgs',penalty='l2',C=0.1)\nLR.fit(X_train, y_train)\nLR.score(X_test, y_test)","3e6ee565":"# Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB().fit(X_train, y_train)\nclf.score(X_test, y_test)","df3c6658":"from sklearn.metrics import  accuracy_score\naccuracies={}\n# Create a random forest classifier, fixing the seed to 2\nAda_model = AdaBoostClassifier(random_state=2).fit(X_pre, y_pre)\n# Use it to predict the labels of the test data\nAda_predictions = Ada_model.predict(X_pre)\naccuracies['Ada'] = accuracy_score(y_pre, Ada_predictions)\n\n# Create a random forest classifier, fixing the seed to 2\nrf_model = RandomForestClassifier(random_state=2).fit(\n  X_train, y_train)\n\n# Use it to predict the labels of the test data\nrf_predictions = rf_model.predict(X_test)\n\n# Assess the accuracy of both classifiers\naccuracies['rf'] = accuracy_score(y_test, rf_predictions)","7e41225b":"for x, y in accuracies.items():\n    print(\"model {} have an accuracy {}\".format(x,y))","956b7e06":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\nNN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 3), random_state=1)\nmodels = [\n    RandomForestClassifier(random_state=0),\n    LinearSVC(),\n    GaussianNB(),\n    LogisticRegression(multi_class='multinomial', solver='lbfgs',penalty='l2',C=0.1),\n    NN,\n    XGBClassifier()\n]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\n\nfor model in models:\n    model_name = model.__class__.__name__\n    accuracies = cross_val_score(model,X_train, y_train, scoring='accuracy', cv=CV)\n    for fold_idx, accuracy in enumerate(accuracies):\n        entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\nimport seaborn as sns\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","daccca00":"cv_df.groupby('model_name').accuracy.mean()","007c0771":"clean_reduced_df['Taux_corr_binned'].value_counts().plot(kind='bar', title='Count (target)')","a067c3f2":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='auto')\nX_sm, y_sm = smote.fit_resample(X_pre, y_pre)\n\ndf = pd.DataFrame(X_sm)\ndf['target'] = y_sm\n\ndf['target'].value_counts().plot(kind='bar', title='Count (target)')","1707e441":"X_sm.shape","2321053f":"df['target'].unique()","4c4da727":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.1)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","5c750f74":"from sklearn.model_selection import GridSearchCV\n# Set a range for n_estimators from 10 to 40 in steps of 10\nparam_grid = {'n_estimators': range(10, 50,10)}\n\n# Optimize for a RandomForestClassifier() using GridSearchCV\ngrid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\ngrid.fit(X_train, y_train)\ngrid.best_params_","61d29806":"grid.score(X_test, y_test)","1d51208b":"# Define a grid for n_estimators ranging from 1 to 10\nparam_grid = {'n_estimators': range(1, 11)}\n\n# Optimize for a AdaBoostClassifier() using GridSearchCV\ngrid = GridSearchCV(AdaBoostClassifier(), param_grid, cv=3)\ngrid.fit(X_train, y_train)\ngrid.best_params_","802a92a8":"grid.score(X_test, y_test)","15e4ffe9":" from sklearn.neighbors import KNeighborsClassifier\n# Define a grid for n_neighbors with values 10, 50 and 100\nparam_grid = {'n_neighbors': [10,50,100]}\n\n# Optimize for KNeighborsClassifier() using GridSearchCV\ngrid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3)\ngrid.fit(X_train, y_train)\ngrid.best_params_","ca2eb075":"grid.score(X_test, y_test)","255a5320":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\nNN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 3), random_state=1)\nmodels = [\n    RandomForestClassifier( random_state=0),\n    LinearSVC(),\n    GaussianNB(),\n    LogisticRegression(multi_class='multinomial', solver='lbfgs',penalty='l2',C=0.1),\n    NN,\n    XGBClassifier()\n]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\n\nfor model in models:\n    model_name = model.__class__.__name__\n    accuracies = cross_val_score(model, df.drop('target',axis=1), df['target'], scoring='accuracy', cv=CV)\n    for fold_idx, accuracy in enumerate(accuracies):\n        entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\nimport seaborn as sns\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","3c9b39f8":"cv_df.groupby('model_name').accuracy.mean()","8723a9b4":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.data import iris_data\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.decomposition import PCA\n# Initializing Classifiers\nclf1 = LogisticRegression(random_state=0)\nclf2 = RandomForestClassifier(random_state=0)\nclf3 = SVC(random_state=0, probability=True)\nclf4=XGBClassifier()\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3,clf4],\n                              weights=[1, 2, 1,2], voting='soft')\n\npca = PCA(n_components=2)\nX_reduced=pca.fit_transform(df.drop('target',axis=1).to_numpy())\n# Plotting Decision Regions\n\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(10, 8))\n\nlabels = ['Logistic Regression',\n          'Random Forest',\n          'RBF kernel SVM',\n          'Ensemble']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3,clf4, eclf],\n                         labels,\n                         itertools.product([0, 1],\n                         repeat=2)):\n    clf.fit(X_reduced, df['target'])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X_reduced, y=df['target'].to_numpy(),\n                                clf=clf, legend=2)\n    plt.title(lab)\n\nplt.show()","fa8cebb1":"eclf.fit(X_train, y_train)\neclf.score(X_test, y_test)","94b2b67f":"\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nimport numpy as np\nimport warnings\n\nwarnings.simplefilter('ignore')\n\nRANDOM_SEED = 42\n\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=RANDOM_SEED)\nclf3 = GaussianNB()\nlr = LogisticRegression()\n\nsclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3],\n                            meta_classifier=lr,\n                            random_state=RANDOM_SEED)\nsclf.fit(X_train, y_train)\nsclf.score(X_test, y_test)","1fefe9dc":"from mlxtend.classifier import StackingClassifier\nsclf_with_proba = StackingClassifier(classifiers=[clf1, clf2, clf3],\n                          use_probas=True,\n                          meta_classifier=lr)\nsclf_with_proba.fit(X_train, y_train)\nsclf_with_proba.score(X_test, y_test)","bfaa5c44":"from mlxtend.classifier import StackingClassifier\n\nm = StackingClassifier(\n    classifiers=[\n        LogisticRegression(),\n        XGBClassifier(max_depth=2),\n        MLPClassifier()\n    ],\n    use_probas=True,\n    meta_classifier=LogisticRegression()\n)\n\nm.fit(X_train, y_train)\nm.score(X_test, y_test)","bd033ee2":"from sklearn.ensemble import StackingClassifier \nestimators = [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n    ('svr', make_pipeline(StandardScaler(),\n                           LinearSVC(random_state=42)))]\nSk_Stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nSk_Stacking.fit(X_train, y_train)\n\nSk_Stacking.score(X_test, y_test)","ee3ffe13":"## Deep Learning Approch :\n\ny = y_sm\nX = X_sm\n\n# Print the dimensions of X and y before reshaping\nprint(\"Dimensions of y before reshaping: {}\".format(y.shape))\nprint(\"Dimensions of X before reshaping: {}\".format(X.shape))\n\n#### Create the encoder.\n## encoder = LabelEncoder()\n\n#encoder.fit(y)   # Assume for simplicity all features are categorical.\n\n#target_le = encoder.transform(y)\n\ntarget_oh =tf.keras.utils.to_categorical(y)\n\nprint(\"target_le\",y.shape)\n\nprint(\"target_oh\",target_oh.shape)\n\n#print(LabelEncoder().classes_)\n\n# Print the dimensions of X and y after reshaping\nprint(\"Dimensions of y after reshaping: {}\".format(target_oh.shape))\nprint(\"Dimensions of X after reshaping: {}\".format(X.shape))\n","16d01bd9":"# First define baseline model. Then use it in Keras Classifier for the training\ndef Classification_model():\n    # Create model here\n    model =tf.keras.Sequential()\n    model.add(layers.Dense(15, input_dim = 44, activation = 'relu')) # Rectified Linear Unit Activation Function\n    model.add(layers.Dense(15, activation = 'relu'))\n    model.add(layers.Dense(3, activation = 'softmax')) # Softmax for multi-class classification\n    # Compile model here\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model\nClassification_Model = Classification_model()\nClassification_Model.summary()","a3a0ec5e":"EPOCHS =1000\n# configure early stopping\nes = EarlyStopping(monitor='val_loss',min_delta=0.0000000000001, patience=5)\n#batch_size=1000\n\nhistory = Classification_Model.fit(X,target_oh,batch_size=16,epochs=EPOCHS, validation_split = 0.1, verbose=0 ,callbacks=[es])","3f8f0ede":"Classification_Model.save('Classification_Model.h5') ","cccf0179":"#Let\u2019s see what this looks like when we plot our respective losses:\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","38eeca06":"# evaluate the keras model\nloss, accuracy = Classification_Model.evaluate( X, target_oh, verbose=2)\nprint(\" Accuracy\".format(accuracy))","c1780ce7":"X.shape","cdeeb539":"RFC=RandomForestClassifier()\nRFC.fit(X_sm, y_sm)\nRFC.score(X_sm, y_sm)","d74cd163":"## Stacking 3 :","07291a22":"## Second Model :StackingCVClassifier","c6d29938":"# Preprocess Pipe ","4da67af6":"# Train_Test Split:","daca5fd5":"# Try Muliple Calssifcation Ml :\n## LogisticRegression","3d3aabfd":"**As we see RFC is perfoming well but let's be aware of overfitting \" \nIn our CV = 0.811965811965812**","4abd78eb":"# Define\/Compile the model","8fbbdb11":"# Selected  ML :\nwe find :\n\n    RandomForestClassifier =0.811965811965812\n    Sk_Stacking = 0.8119657\n    estimators = [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n        ('svr', make_pipeline(StandardScaler(),\n                               LinearSVC(random_state=42)))]\n    Sk_Stacking = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n    \n**They have similar accuracy so for simplicity and for performance  we chose RandomForestClassifier** \n## Retrain on all the data without  CV :","c34979bf":"## GaussianNB","ccebfc48":"# Intuition behind Classification \n## Prefer Confidence Intervals to Point Estimates\n\nIt is usually a good idea to get an estimate of confidence in your prediction in addition to producing the prediction itself. For regression analysis this usually takes the form of predicting a range of values that is calibrated to cover the true value 95% of the time or in the case of classification it could be just a matter of producing class probabilities. This becomes more crucial with small data sets as it becomes more likely that certain regions in your feature space are less represented than others. Model averaging as referred to in the previous two points allows us to do that pretty easily in a generic way for regression, classification and density estimation. It is also useful to do that when evaluating your models. Producing confidence intervals on the metrics you are using to compare model performance is likely to save you from jumping to many wrong conclusion","c453cdcd":"# X and y","b98414c1":"# Deep learning Approch :\n## Prepare Data :\n","c906d1db":"# Stacking :\n## Stacking model1 :EnsembleVoteClassifier","439f1180":"# AdaBoostClassifier\/RandomForestClassifier","174693b1":"## NN\/LinearSVC\/XGBClassifier\/","c8de35b6":"## Using Sklearn :\n","68dbb446":"# Balance the dataset with synthetic samples (SMOTE)\nIn addition to being extremely small, our training dataset has the unbalanced target binary variable, which can undermine some models' predictability. We will perform an oversampling, which consists of creating new samples to increase the 0 minority class. For this we will use the SMOTE technique.\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbor\n\n","1bcb44e1":"# Classification Target ","18f3ceb1":"# Using Probabilities as Meta-Features\n\nAlternatively, the class-probabilities of the first-level classifiers can be used to train the meta-classifier (2nd-level classifier) by setting use_probas=True. For example, in a 3-class setting with 2 level-1 classifiers, these classifiers may make the following \"probability\" predictions for 1 training sample:\n\n    classifier 1: [0.2, 0.5, 0.3]\n    classifier 2: [0.3, 0.4, 0.4]\n\nThis results in k features, where k = [n_classes * n_classifiers], by stacking these level-1 probabilities:\n\n    [0.2, 0.5, 0.3, 0.3, 0.4, 0.4]\n"}}