{"cell_type":{"0728bfe9":"code","3c422466":"code","f9cccb67":"code","78c9282e":"code","00c200c2":"code","be56ee35":"code","201a9ec1":"code","389d4d89":"code","95a791a1":"code","a32219d9":"code","52deeb1f":"code","61a96a5c":"code","ee46750b":"code","133b7c44":"code","a38f1be2":"code","26e45b31":"code","309a83cf":"markdown","56833384":"markdown","9a3f9d2e":"markdown","ccc6c2f5":"markdown","1a71a428":"markdown","c4b78703":"markdown","5d74aaf7":"markdown","776d8dee":"markdown","7a93858a":"markdown","9219920c":"markdown","4f0f6b00":"markdown","ccdef148":"markdown","d748693c":"markdown","68ec4a28":"markdown","77f0d67e":"markdown","c2cbb5cb":"markdown"},"source":{"0728bfe9":"# data visualization\nimport matplotlib.pyplot as plt\nplt.rc('figure', figsize=(14, 9))\n\n# data representation and manipulation\nimport pandas as pd\n\n# we'll import several things from various parts of sklearn.\n# sklearn is the library that provides us machine learning models\n# and various utility functions for machine learning\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# load the dataset\ndf = pd.read_csv('\/kaggle\/input\/lemonadesales\/lemonade_clean.csv')\ndf","3c422466":"X = df[['temperature']]\ny = df[['sales']]","f9cccb67":"X_train, X_test, y_train, y_test = train_test_split(X, y)","78c9282e":"# create the model\nlr = LinearRegression()\n# fit the model on the training data\nlr.fit(X_train, y_train)\n\nm = lr.coef_\nb = lr.intercept_\n\nprint('m =', m, 'b =', b)","00c200c2":"print('y = {:.2f}x + {:.2f}'.format(m[0, 0], b[0]))","be56ee35":"temperature = X_train\nactual_sales = y_train\nlr_predicted_sales = lr.predict(X_train)\n\nplt.scatter(temperature, actual_sales, label='Actual')\nplt.scatter(temperature, lr_predicted_sales, label='Predicted')\nplt.xlabel('Temperature')\nplt.ylabel('Sales')\nplt.legend()\nplt.title('Sales: Actual vs Linear Regression Predictions for Training Data')","201a9ec1":"mean_squared_error(actual_sales, lr_predicted_sales)","389d4d89":"knr = KNeighborsRegressor(n_neighbors=2)\nknr.fit(X_train, y_train)\n\nknr_predicted_sales = knr.predict(X_train)","95a791a1":"plt.scatter(temperature, actual_sales, label='Actual')\nplt.scatter(temperature, knr_predicted_sales, label='Predictions')\nplt.legend()\nplt.xlabel('Temperature')\nplt.ylabel('Sales')\nplt.title('Sales: Actual vs K-Neighbors Predictions for Training Data')","a32219d9":"mean_squared_error(actual_sales, knr_predicted_sales)","52deeb1f":"knr_test_predictions = knr.predict(X_test)\nlr_test_predictions = lr.predict(X_test)","61a96a5c":"plt.scatter(X_test, y_test, label='Actual')\nplt.scatter(X_test, lr_test_predictions, label='Predictions')\nplt.legend()\nplt.xlabel('Temperature')\nplt.ylabel('Sales')\nplt.title('Sales: Actual vs Linear Regression Predictions for Test Data')","ee46750b":"plt.scatter(X_test, y_test, label='Actual')\nplt.scatter(X_test, knr_test_predictions, label='Predictions')\nplt.legend()\nplt.xlabel('Temperature')\nplt.ylabel('Sales')\nplt.title('Sales: Actual vs K-Neighbors Predictions for Test Data')","133b7c44":"knr_mse = mean_squared_error(y_test, knr_test_predictions)\nlr_mse = mean_squared_error(y_test, lr_test_predictions)\n\nprint('k-neighbors test mean squared error: ', knr_mse)\nprint('linear regression test mean squared error: ', lr_mse)","a38f1be2":"lr_test_residuals = y_test - lr_test_predictions\nknr_test_residuals = y_test - knr_test_predictions","26e45b31":"fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\nax1.scatter(y_test, lr_test_residuals)\nax1.set(ylabel='Residual ($y - \\hat{y}$)', xlabel='Sales', title='Linear Regression')\n\nax2.scatter(y_test, knr_test_residuals)\nax2.set(title='K-Neighbors', xlabel='Sales')\n\nfig.suptitle('Test Data Set Residuals')","309a83cf":"## Using the Test Data\n\nThe evaluations we have done so far have been on the training data set. We can see how well our model performs on unseen data by evaluating them on the test data set.","56833384":"To evaluate our model's performance, we can take the model's predicted values for the training data, and compare them to the actual values from the training data.\n\nWe'll use a scatter plot to do this.","9a3f9d2e":"The next step is to do the train test split.\n\nWe'll use to training data to create our models and hold back the test data to get an idea of how well our model generalizes.","ccc6c2f5":"# Intro to ML\n\nIn this notebook, we'll run a couple machine learning algorithms!\n\nFirst we'll setup our environment.","1a71a428":"Once we have our environment setup and our data loaded, we'll split our data into our predictors and target variable.\n\nIt's common to see the convention of using uppercase x, `X` to indicate that `X` is a table that holds multiple features, and lowercase y, `y`, as our target variable is a single series.","c4b78703":"We can also look at the *residuals* for a dataset. The residuals are the set of differences between the actual and the predicted value.","5d74aaf7":"Now that we've figured out how our linear regression model performs, let's try a different model!\n\nWe'll now fit a K-Neighbors model. You'll notice that there are many similarities to the code for our linear regression model, however, one key difference is that we specify a **hyperparameter** to our k-neighbors model. The number of neighbors to consider must be set ahead of time, and is an example of a hyperparameter, a number that defines our model's behavior, but is not learned from the training data, rather, it is determined ahead of time.","776d8dee":"Now that we've fit the model, we can evaluate the model's prediction visually, like we did previously.","7a93858a":"In addition to the visualizations above, let's check the mean squared error for the test data.","9219920c":"## Exercises\n\nTweak your models further!\n\n- How does the k-neighbors model's performance change when you set the number of neighbors to 3? to 4? What do you think the tradeoffs are of making this number very large?\n\n- Try out different features. Go back to where we initially split the data into X and y. Change our X variable to be flyers instead of temperature. Make sure you re-run the train-test split as well. How does your model's performance change?\n\n- Like we did for the test data set, you should examine the residuals for the training data.\n\n- Include multiple independent variables.\n\n    All of the examples we have looked at thus far have been with a single X variable, but these concepts and models can generalize to multiple X variables.\n\n    Go back to where we initially split the data into X and y. Change our X variable to be both flyers *and* temperature (`X = df[['temperature', 'flyers']]`).\n    \n    How does this change impact your model's performance?\n    \n    When you include multiple X variables, you won't be able to compare the actual and predicted values visually like we have in the examples. Try making a plot of the actual value on the x-axis and the predicted value on the y-axis. What does this visualization tell us?","4f0f6b00":"## Conclusion\n\nIt looks like our linear regression model ended up performing the best, so at this point, we would select it as our best model.\n\nThere are still further avenues to explore, however. We could try using different features, different model types, or tuning the parameters of our current models.","ccdef148":"Now that we've split up our data, we can begin training our models!\n\nFirst we'll create a linear regression model and fit it with our training data.\n\nFor a linear regression model, we can then find the slope and intercept of the resulting line of best fit.","d748693c":"When we examine the residuals, we are looking for any sort of pattern. For our example above, the residuals look mostly random, so we are satisfied with our model.","68ec4a28":"And we can calculate the mean squared error the same way","77f0d67e":"Like before, we can evaluate the models visually as well:","c2cbb5cb":"To find the mean squared error for our model, we can use the `mean_squared_error` function."}}