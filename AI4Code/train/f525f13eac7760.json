{"cell_type":{"de252fad":"code","997a7f97":"code","ea0eec70":"code","2ba9c489":"code","e1de2273":"code","ced74e09":"code","61ad32c6":"code","7e3a3b4e":"code","8929d60e":"code","8d5aacb2":"code","32e09f91":"code","b02aa967":"code","309776dc":"code","5be1b95f":"code","7ccc696d":"code","a079ec18":"code","437c6d27":"code","58300624":"code","43e56682":"code","8d7e4110":"code","1656f96f":"code","c11e6bdc":"code","2f76b565":"code","b896173d":"code","fa0f146a":"code","565fb156":"code","6a3fe16c":"code","fd7bd6dd":"code","3e16e9fd":"code","75dea9fe":"code","6cf10239":"code","2a2b2675":"code","3ae90bea":"code","9c8a32da":"code","d3069c63":"markdown","ffc127f8":"markdown","2fd88c71":"markdown","803a5641":"markdown","6772f83b":"markdown","5d3ae3c6":"markdown","765366a8":"markdown","c0bfe18f":"markdown","9b6e46af":"markdown","c2881efd":"markdown","4bc08893":"markdown","23b32c61":"markdown","72621137":"markdown","142e05f0":"markdown","90bfe5e1":"markdown","1cd2af04":"markdown","cc4afbe1":"markdown","7edc003d":"markdown","316635d3":"markdown","af5cdbb3":"markdown","9001f45c":"markdown"},"source":{"de252fad":"# necessary installs\n%pip install --upgrade tensorflow-gpu","997a7f97":"# standard library imports\nimport os\n\nfrom typing import List\n\n# third party imports\nimport numpy as np\nimport pandas as pd\nimport spacy\n\nfrom IPython.display import display, Markdown\nfrom sklearn import metrics, set_config\nfrom sklearn.model_selection import train_test_split\n\n# some config settings\npd.set_option(\"display.max_colwidth\", None)\nset_config(display='diagram')\n\n# some helper functions\ndef md(text: str):\n    display(Markdown(text))\n    \n\ndef benchmark(y_test, y_hat):\n    print('_' * 80)\n    print(f\"f1 score: {metrics.f1_score(y_test, y_hat, average='macro'):.3f}\")\n    print(\"classification report:\")\n    print(metrics.classification_report(y_test, y_hat))","ea0eec70":"folder = \"..\/input\/nlp-getting-started\"\n\ntest = pd.read_csv(os.path.join(folder, \"test.csv\"), index_col=\"id\")\ntrain = pd.read_csv(os.path.join(folder, \"train.csv\"), index_col=\"id\")\n\nX = train[\"text\"]\ny = train[\"target\"]\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","2ba9c489":"print(f\"We are using `{len(X_train)}` rows for model training, and `{len(X_valid)}` rows for validation\")","e1de2273":"X_train.head().to_frame()","ced74e09":"from sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\n\nvectorizers = {\n    \"hashing\": HashingVectorizer(stop_words=\"english\", strip_accents=\"unicode\", alternate_sign=False),\n    \"tf-idf\": TfidfVectorizer(stop_words=\"english\", strip_accents=\"unicode\", sublinear_tf=True),\n}\n\npipe = Pipeline([\n    (\"vectorizer\", None),\n    (\"model\", None)\n])","61ad32c6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import RidgeClassifierCV, SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\nfrom sklearn.svm import LinearSVC, SVC\n\nfeature_options = {\n    \"vectorizer\": [vectorizers[\"hashing\"], vectorizers[\"tf-idf\"]]\n}\n\nbaseline_params = [\n    {\n        \"model\": [\n            ComplementNB(),\n        ],\n        **feature_options,\n    },\n    {\n        \"model\": [SGDClassifier()],\n        \"model__penalty\": [\"elasticnet\", \"l2\"],\n        **feature_options,\n    },\n    {\n        \"model\": [LinearSVC()],\n        \"model__penalty\": [\"l1\", \"l2\"],\n        **feature_options\n    }\n]\n\ngrid = GridSearchCV(pipe, param_grid=baseline_params, cv=5, scoring=\"f1\", n_jobs=-1, verbose=3)\n_ = grid.fit(X_train, y_train)","7e3a3b4e":"print(grid.best_params_)\n\nbaseline = grid.best_estimator_\n\nbaseline","8929d60e":"benchmark(y_valid, baseline.predict(X_valid))","8d5aacb2":"failed = (\n    X_valid\n    .to_frame(\"text\")\n    .assign(\n        y_hat=baseline.predict(X_valid),\n        y_true=y_valid\n    )\n)\n\nfalse_negatives = failed.pipe(lambda df: df[df[\"y_hat\"].eq(0) & df[\"y_true\"].eq(1)])\nfalse_positives = failed.pipe(lambda df: df[df[\"y_hat\"].eq(1) & df[\"y_true\"].eq(0)])\n\nfalse_negatives[:20]","32e09f91":"false_positives[:20]","b02aa967":"del false_positives\ndel failed","309776dc":"vectorizer = baseline.get_params()[\"vectorizer\"]\n\ndef highlight_examples(vectorizer, examples: List[str]) -> None:\n    for example in examples:\n        print(\"-\" * 80)\n        print(\"original:\", example)\n        print()\n        print(\"preprocessor:\", vectorizer.build_preprocessor()(example))\n        print()\n        print(\"tokenizer:\", vectorizer.build_tokenizer()(example))\n        print()\n        print(\"complete analyzer:\", vectorizer.build_analyzer()(example))\n\nexample = false_negatives[\"text\"].iloc[5]\n\nhighlight_examples(vectorizer, false_negatives[\"text\"].iloc[:4])","5be1b95f":"del false_negatives","7ccc696d":"import re\n\nfrom spacy.matcher import Matcher\nfrom spacy.pipeline import EntityRuler\nfrom spacy.tokens import Token\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.util import compile_infix_regex, compile_prefix_regex, compile_suffix_regex, filter_spans\n\n\ndef custom_tokenizer(nlp):\n    \"\"\"\n    Custom tokenizer to deal with common gotcha's in tweets.\n    \n    Prefix:\n        - split off the hashtags to start\n    \n    Special cases:\n        - Convert the \"&amp\" artefact into \"&\"\n    \n    Infix splits:\n        - Split tokens if there was a missing sapce between a word and hashtag\n        - Split tokens if there was two words linked by a slash e.g. \"foo\/bar\"\n        \n    Ensure URLs don't get split up.\n    \n    Args:\n        nlp: A spacy pipeline.\n    \n    Returns:\n        A spacy tokenizer.\n    \"\"\"\n    tokenizer = nlp.tokenizer\n    \n    special_cases = {\n        \":)\": [{\"ORTH\": \":)\"}],\n        \"&amp\": [{\"ORTH\": \"&\"}]\n    }\n    \n    for text, change in special_cases.items():\n        tokenizer.add_special_case(text, change)\n    \n    # infixes\n    infixes = nlp.Defaults.infixes + (r\"\"\"[@:;+\/\\-~#*]\"\"\", r\"\"\"&amp\"\"\")\n    infix_re = compile_infix_regex(infixes)\n    tokenizer.infix_finditer = infix_re.finditer\n    \n    # prefixes\n    prefixes = nlp.Defaults.prefixes + (r\"\"\"[\\^\\.]\"\"\",)\n    prefix_re = spacy.util.compile_prefix_regex(prefixes)\n    tokenizer.prefix_search = prefix_re.search\n    \n    # suffixes\n    suffixes = nlp.Defaults.suffixes + (r\"\"\"[\\^\\.]\"\"\",)\n    suffix_re = spacy.util.compile_suffix_regex(suffixes)\n    tokenizer.suffix_search = suffix_re.search\n\n    tokenizer.token_match = re.compile(r\"\"\"https?:\/\/\"\"\").match\n    \n    return tokenizer\n\n\ndef username_labeller(nlp):\n    ruler = EntityRuler(nlp)\n    patterns = [\n        {\"label\": \"USERNAME\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^@\\w+$\"}}]},\n        {\"label\": \"USERNAME\", \"pattern\": [{\"ORTH\": \"@\"}, {\"IS_ASCII\": True}]}\n    ]\n    ruler.add_patterns(patterns)\n    return ruler\n\n\nclass HashtagMerger:\n    \"\"\"\n    Pipeline step to merge hashtags together.\n    \n    Code based on examples here: https:\/\/spacy.io\/usage\/rule-based-matching\n    \"\"\"\n    name = \"hashtag_merger\"\n    \n    def __init__(self, nlp):\n        Token.set_extension(\"is_hashtag\", default=False, force=True)\n        self.matcher = Matcher(nlp.vocab)\n        self.matcher.add(\"HASHTAG\", None, [{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}])\n    \n    def __call__(self, doc) -> \"spacy.tokens.doc.Doc\":\n        # This method is invoked when the component is called on a Doc\n        matches = self.matcher(doc)\n        spans = []  # Collect the matched spans here\n        for match_id, start, end in matches:\n            spans.append(doc[start:end])\n            \n        filtered_spans = filter_spans(spans)\n        with doc.retokenize() as retokenizer:\n            for span in filtered_spans:\n                retokenizer.merge(span)\n                for token in span:\n                    token._.is_hashtag = True  # Mark token as a hashtag\n        return doc\n        \n\ndef create_nlp(model: str = \"en_core_web_sm\"):\n    nlp = spacy.load(model, disable=[\"parser\", \"tagger\", \"ner\"])\n    nlp.tokenizer = custom_tokenizer(nlp)\n    username = username_labeller(nlp)\n    nlp.add_pipe(username)\n    hashtag_merger = HashtagMerger(nlp)\n    nlp.add_pipe(hashtag_merger, last=True)\n    \n    return nlp","a079ec18":"nlp = create_nlp()\ndoc = nlp(\"^oo^ hello-world. :) http:\/\/t.co\/ABC-DEF measures#blah #foo @user &amp friends\/enemies\")\n\npd.DataFrame({\n    \"text\": [token.text for token in doc],\n    \"like_url\": [token.like_url for token in doc],\n    \"entity_type\": [token.ent_type_ for token in doc],\n    \"is_hashtag\": [token._.is_hashtag for token in doc],\n    \"is_ascii\": [token.is_ascii for token in doc],\n    \"is_punct\": [token.is_punct for token in doc]\n})","437c6d27":"from typing import Callable, List\n\ndef is_valid_token(token: spacy.tokens.Token) -> bool:\n    \"\"\"\n    Is not a hashtag, url or username, is ascii and not punctuation.\n    \n    We keep numbers in. It tends to be that models are able to use the useful ones and filter out the noise.\n    \"\"\"\n    return (\n        not token.like_url\n        and not token._.is_hashtag\n        and token.ent_type_ != \"USERNAME\"\n        and not token.is_punct\n        and token.text.strip() != \"\" # drop off any newlines\n        and token.is_ascii # avoiding symbols like: \u0089\u00db\u00aa\n        # we don't filter out stopwords here. we do it as part of the scikit learn text vectorizers\n        # and not token.is_stop\n    )\n\ndef extract_text(\n    tweet: spacy.tokens.Doc,\n    include_text: bool = True,\n    include_hashtags: bool = True,\n    include_handles: bool = False\n) -> str:\n    cleaned_tokens = []\n    for token in tweet:\n        if (token._.is_hashtag and include_hashtags):\n            cleaned_tokens.append(token.text.replace(\"#\", \"\"))\n        elif (token.ent_type_ == \"USERNAME\" and include_handles):\n            cleaned_tokens.append(token.text.replace(\"@\", \"\"))\n        elif (is_valid_token(token) and include_text):\n            cleaned_tokens.append(token.text.lower())\n\n    return \" \".join(cleaned_tokens)\n\ndef clean_tweets(tweets: List[str], **kwargs) -> List[str]:\n    nlp = create_nlp()\n    cleaned_tweets = []\n    \n    for doc in nlp.pipe(tweets, n_process=1):\n        string = extract_text(doc, **kwargs)\n        cleaned_tweets.append(string)\n    return cleaned_tweets","58300624":"from sklearn.base import clone\nfrom sklearn.preprocessing import FunctionTransformer\n\ncleaned_grid = clone(grid)\n\nnew_pipe = Pipeline([\n    (\"clean\", FunctionTransformer(clean_tweets)),\n    (\"vectorizer\", None),\n    (\"model\", None)\n])\n\ncleaning_param_grid = {\n    \"clean__kw_args\": [\n        {\"include_text\": text, \"include_hashtags\": hashtags, \"include_handles\": handles}\n        for text in [True]\n        for hashtags in [True]\n        for handles in [False]\n        # you can try all combinations. For speed I've put in the best combo by default\n    ]\n}\n\nold_param_grid = cleaned_grid.get_params()[\"param_grid\"]\ncleaned_grid.set_params(\n    estimator=new_pipe,\n    param_grid={**{key: [value] for key,value in grid.best_params_.items()}, **cleaning_param_grid}\n)\n\ncleaned_grid.get_params()[\"param_grid\"]","43e56682":"cleaned_grid.fit(X_train, y_train)","8d7e4110":"print(cleaned_grid.best_params_)\n\ncleaned_baseline = cleaned_grid.best_estimator_","1656f96f":"benchmark(y_valid, cleaned_baseline.predict(X_valid))","c11e6bdc":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","2f76b565":"import tensorflow as tf\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom tensorflow.keras.optimizers import Adam\n\n# The maximum length of words we'll allow for the tweets\nMAX_LENGTH = 64\n\n# The bert pre-trained model to use\nPRETRAINED = \"roberta-base\"\n\n\ndef construct_bert(bert_layer, max_length: int = MAX_LENGTH, lr=5e-5, dropout=0.2):\n    attention_mask = tf.keras.Input(shape=(max_length,), dtype=\"int32\", name=\"attention_mask\")\n    input_ids = tf.keras.Input(shape=(max_length,), dtype=\"int32\", name=\"input_ids\")\n\n    output = bert_layer([input_ids, attention_mask])\n    output = output[1]\n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(dropout)(output)\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n    \n    model.compile(Adam(lr=lr), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\n    return model\n\n\ndef bert_encode(tokenizer, texts: list, max_length: int = MAX_LENGTH):\n    encoded = tokenizer.batch_encode_plus(\n        batch_text_or_text_pairs=texts,\n        max_length=max_length,\n        pad_to_max_length=True,\n        return_token_type_ids=False,\n        return_attention_mask=True,\n        return_tensors=\"tf\"\n    )\n    \n    return [encoded[\"input_ids\"], encoded[\"attention_mask\"]]","b896173d":"import optuna\nfrom optuna.integration import TFKerasPruningCallback\n\n# we run for 5 epochs with early stopping enabled\n# most models reach their peak around 2 epochs.\nEPOCHS = 5\n\n# trains models with a range of different hyperparameters\n# return the end score for the validation accuracy\ndef objective(trial: optuna.Trial):\n    lr = trial.suggest_categorical(\"lr\", [1e-5, 2e-5, 3e-5])\n    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 1.0)\n    clean = trial.suggest_categorical(\"clean_tweets\", [True, False])\n\n    monitor = \"val_accuracy\"\n    tokenizer = RobertaTokenizer.from_pretrained(PRETRAINED, do_lower_case=True)\n    \n    \n    if clean:\n        include_hashtags = trial.suggest_categorical(\"include_hashtags\", [True, False])\n        include_handles = trial.suggest_categorical(\"include_handles\", [True, False])\n        \n        tweets = clean_tweets(X_train, include_hashtags=include_hashtags, include_handles=include_handles)\n    else:\n        tweets = X_train\n    \n    encoded_tweets = bert_encode(tokenizer, tweets)\n    \n    model = construct_bert(TFRobertaModel.from_pretrained(PRETRAINED), lr=lr, dropout=dropout_rate)\n    \n    fit_params = {\n        \"batch_size\": batch_size,\n        \"epochs\": EPOCHS,\n        \"callbacks\": [\n            tf.keras.callbacks.EarlyStopping(\n                monitor=monitor,\n                patience=1,\n                restore_best_weights=True\n            ),\n            TFKerasPruningCallback(trial, monitor)\n        ],\n        \"validation_split\": 0.2\n    }\n    \n    history = model.fit(encoded_tweets, y_train.to_numpy(), verbose=2, **fit_params)\n    \n    # since we are doing early stopping we want to report the best accuracy that\n    # was found across the board.\n    # Note, if you adjusted this to be a metric to minimize then you should adjust to `min()`\n    return max(history.history[monitor])","fa0f146a":"# study = optuna.create_study(\n#     direction=\"maximize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=2)\n# )\n\n# study.optimize(objective, n_trials=10, timeout=1800, gc_after_trial=True)\n\n# print(study.best_params)\n# print(study.best_value)\n\n# {'lr': 3e-05, 'batch_size': 32, 'dropout_rate': 0.3432504001345468, 'clean_tweets': False}\n# 0.825944185256958","565fb156":"# params = study.best_params\nparams = {'lr': 3e-05, 'batch_size': 32, 'dropout_rate': 0.3432504001345468, 'clean_tweets': False}","6a3fe16c":"if params[\"clean_tweets\"]:\n    tweets = clean_tweets(X_train, include_hashtags=params[\"include_hashtags\"], include_handles=params[\"include_handles\"])\nelse:\n    tweets = X_train\n\ntokenizer = RobertaTokenizer.from_pretrained(PRETRAINED, do_lower_case=True)\nencoded_tweets = bert_encode(tokenizer, tweets)\n\nmodel = construct_bert(TFRobertaModel.from_pretrained(PRETRAINED), lr=params[\"lr\"], dropout=params[\"dropout_rate\"])\n    \nfit_params = {\n    \"batch_size\": params[\"batch_size\"],\n    \"epochs\": 3,\n    \"callbacks\": [\n        tf.keras.callbacks.EarlyStopping(\n            monitor=\"val_accuracy\",\n            patience=0,\n            restore_best_weights=True\n        )\n    ],\n    \"validation_split\": 0.1\n}\n    \nhistory = model.fit(encoded_tweets, y_train.to_numpy(), **fit_params)","fd7bd6dd":"if params[\"clean_tweets\"]:\n    tweets_valid = clean_tweets(X_valid, include_hashtags=params[\"include_hashtags\"], include_handles=params[\"include_handles\"])\nelse:\n    tweets_valid = X_valid\n    \nencoded_tweets_valid = bert_encode(tokenizer, tweets_valid)\n\npreds = model.predict(encoded_tweets_valid)","3e16e9fd":"benchmark(y_valid, preds > 0.5)","75dea9fe":"model = construct_bert(TFRobertaModel.from_pretrained(PRETRAINED), lr=params[\"lr\"], dropout=params[\"dropout_rate\"])\n\nif params[\"clean_tweets\"]:\n    tweets_all = clean_tweets(X, include_hashtags=params[\"include_hashtags\"], include_handles=params[\"include_handles\"])\nelse:\n    tweets_all = X\n    \nencoded_tweets_all = bert_encode(tokenizer, tweets_all)\n\n\nhistory = model.fit(encoded_tweets_all, y.to_numpy(), **fit_params)","6cf10239":"if params[\"clean_tweets\"]:\n    tweets_test = clean_tweets(test[\"text\"], include_hashtags=params[\"include_hashtags\"], include_handles=params[\"include_handles\"])\nelse:\n    tweets_test = test[\"text\"]\n    \nencoded_tweets_test = bert_encode(tokenizer, tweets_test)","2a2b2675":"target = model.predict(encoded_tweets_test)","3ae90bea":"submission = pd.DataFrame({\"target\": target.reshape(-1).round().astype(int)}, index=test.index)\n\nsubmission.head()","9c8a32da":"submission.to_csv(\"submission.csv\")","d3069c63":"__Takeaways__\n\n- It seems that there's a lot of mislabeled data. I'd say that several of these \"false negatives\" are in fact actual negatives. So there's a limit to how good our model can be.\n- There appears to be a lot of noise in the tweets\n    - twitter handles\n    - hashtags\n    - hashtags that are hard to separate into their component words (when that is even possible)\n    - urls\n    - shortenings\/colloquialisms\n    - mispellings\n    - strange text artefacts (e.g. \u0089\u00db\u00aa)","ffc127f8":"__Preface:__ To get tensorflow working on gpu I found I had to install this before the first import of the tensorflow library.","2fd88c71":"# Using BERT and Sons\n\nOkay, time to bring out the big guns.\nI'm going to pick up the pretrained RoBERTa model `roberta-base` provided in\nthe huggingface transformers model.\n\nFor settings some hyperaparameters I'm going to use [optuna](https:\/\/optuna.org\/).\n\nIf you are running this kernel make sure to set GPU acceleration for your environment!\nYour output for the below cell should show `device_type: \"GPU\"` for one of the devices available.","803a5641":"Below we can see an example of the text processing in action","6772f83b":"We can see from the best results that, the best value is found without any of my text cleaning \ud83d\ude05.\nKnowledge is power. It also helped us to get good values for the dropout and learning rate.\n\n`optuna` comes with some helpful visualizations. Below we can see the relative parameter importances.\nIt suggests that dropping off my text cleaning affected around 50% of the performance \ud83d\ude05\ud83d\ude05.\n\nSo __lesson learned__, the bert transformer models are smart enough to filter out the noisy text tokens,\nso don't try and get in their way.","5d3ae3c6":"To speed up the commit time on the notebook I'm commenting out the below code and\nrunning with the best values that I found.","765366a8":"# Submission\n\nNow we have our final model design, let's re-train it over all available training data.\nThen, we are good to produce our submissions.","c0bfe18f":"# Improved Pre-processing\n\nLet's first process the tweet text before passing them into our baseline model.\nFor this, I used the excellent [spacy](https:\/\/spacy.io\/) library. \n\nHere's what I've tried to do:\n\n1. Split the text off into tokens on many things not just whitespace.\nSince many people concatenate words when they shouldn't have. e.g. `my tweet#example`\n1. Catch url patterns and keeping them all together instead of splitting on all it's forward-slashes.\n1. Pattern matching to find twitter handles and hashtags and re-merging them\n(after splitting them off in step 1).","9b6e46af":"## Seeing where the model falls flat","c2881efd":"optuna.visualization.plot_param_importances(study)","4bc08893":"So a pretty big jump for our local validation, getting a value around `0.84`. Not too shabby.\n\nHopefully this is a good estimate for what our score will be on the leaderboard.\n(On reflection, there's a bit of a dip. Something to investigate and improve on in\na future notebook!)","23b32c61":"# Conclusion\n\nHopefully this was helpful to peak over my shoulder as I tried to move from scikit to \ud83e\udd17 huggingface.\n\nI'll aim to improve on these results and methods, particularly on the hyperparameter improvements\nto get the neural net model performing better and better.","72621137":"Our baseline model gets a value of f1 score `~0.775`.\nTo put this in perspective the max of the public leaderboard (that don't use the leaked labels) is around `0.84`.\nSo, it's _sort of_ close :D\n\n\n\nNote that the lowest value of the difference precision and recall is a low value of recall for our target class.\nThat can be something we can try to improve with more complex models.","142e05f0":"Now that the text is split better and annotated well.\nIt makes it super simple to clean out any noisy tokens (e.g. urls)\n\nThe functions below help to filter out tokens as only those that are:\n\n1. \"valid\" i.e. not a url or text artefact or punctuation\n1. A hashtag\n1. A twitter handle\n\nAll three criteria are able to be switched on or off, to see what is helpful in the modelling.","90bfe5e1":"We can tell that a lot of these failed cases have strange punctuation and links. Let's see what our vectorizer is doing with them.","1cd2af04":"## Result\n\nWe can see that we get around a 0.1 bump in our f1-score up to around `0.787`. This is a pretty good jump from something as simple as cleaning data.\n\nOne suspected downside of the hashtags and user handles is that they are quite unique and so many would be out-of-vocabulary regardless of which data you trained your tokenizer.\nWe're going to move on to creating a smarter model using the \ud83e\udd17 huggingface transformer models.\nA benefit of many of these models is they split up large words into subwords, which means it is more likely to glean out some value for those large concatenated hashtags.","cc4afbe1":"I am only going to use the column that contains the tweets.\nAs that is what I want to focus on.","7edc003d":"Using the hyperparameter values we find above. We retrain our model\non all the training data.","316635d3":"# Disaster Tweets\n\nThis is my first attempt at waking up the genie in the BERTle.\nI wanted to start with a basic NLP task, using classical methods, and then\nup the tempo and use the latest from the \ud83e\udd17 transformers package.\n\nIf you're similar to me, then feel free to come along on this magic carpet ride,\nas we head into this whole new world (from circa 2018 that I'm only just now discovering).","af5cdbb3":"# Baseline Model\n\nWe use scikit-learn with a hashing vectorizer + a range of different models.\nSince these classic models train super quickly, I'm just going to through a whole\nheap of them into a `GridSearchCV` to find the best one as a baseline.","9001f45c":"I'll let the model decide what types of text it wants to have included."}}