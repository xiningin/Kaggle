{"cell_type":{"70f4150d":"code","956403ca":"code","79cfe02b":"code","b8f10ba8":"code","7a1d3954":"code","f44aeb31":"code","aac6eb7f":"code","ffc3f484":"code","55addba0":"code","575821f6":"code","8c02a5e3":"code","6677a7ac":"code","3c3cd316":"code","33c802e5":"code","85cc6f89":"code","e112a6c7":"code","7c5a1f8b":"code","8f7ec2b6":"code","9d695c2e":"code","42a6eea9":"code","f2b3f8f1":"code","8c85c6f4":"code","d731dbcf":"code","362ed886":"code","3ac64a92":"code","2dd3c195":"code","3e666cc1":"code","a95b090e":"code","8c2b6f4a":"code","3c705b1a":"code","05668e22":"code","a0173fac":"code","328ca142":"code","782dfdf3":"code","9aed86c8":"code","7e66ce5d":"code","577bc284":"code","ddf5b692":"code","70bd28b1":"code","3df935a5":"code","6db4157b":"code","78ac6e83":"code","76b61aff":"code","7a8c7ae8":"code","2cf69744":"code","4f119cd1":"code","979a788c":"code","b4cd41db":"markdown","858bf238":"markdown","97d21abf":"markdown","689b9408":"markdown","e1087cc4":"markdown","f2e9b496":"markdown","d985de10":"markdown","4292584b":"markdown","68380e19":"markdown","a14a3487":"markdown","b0bb36f5":"markdown","3a761f34":"markdown","85952003":"markdown","d35698c6":"markdown","3f152371":"markdown","43aaa2d2":"markdown","da5797df":"markdown","788fd96e":"markdown","f3cd3f08":"markdown","8010ef00":"markdown","99a35970":"markdown","a224ebd0":"markdown","91840091":"markdown","f73dac3e":"markdown","33d5a9a8":"markdown","03a56d28":"markdown","2e50b86b":"markdown","248b2a1b":"markdown","41d7b7d9":"markdown","65f9cd71":"markdown","959220d5":"markdown","50a63c15":"markdown","c260ddaf":"markdown"},"source":{"70f4150d":"import os\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport xgboost as xgb","956403ca":"data_dir = '\/kaggle\/input'\nos.listdir(data_dir)","79cfe02b":"sample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\nitems = pd.read_csv(os.path.join(data_dir, 'items.csv'))\nshops = pd.read_csv(os.path.join(data_dir,'shops.csv'))\nsales = pd.read_csv(os.path.join(data_dir, 'sales_train.csv'))\ntest_data = pd.read_csv(os.path.join(data_dir, 'test.csv'))","b8f10ba8":"items.columns, shops.columns, sales.columns, test_data.columns","7a1d3954":"sales.shape, test_data.shape","f44aeb31":"sales.info()","aac6eb7f":"sales.describe()","ffc3f484":"sales.nunique()","55addba0":"x = sales.groupby('date_block_num').agg({'item_cnt_day': 'sum'})\nplt.title('Total sales by date_block_num')\nplt.plot(x.index, x['item_cnt_day'])","575821f6":"plt.title('Total sales by month')\n\nx_1 = x[x.index < 12]\nplt.plot(x_1.index + 1, x_1['item_cnt_day'], color='red', label='2013')\n\nx_2 = x[x.index >= 12]\nx_2 = x_2[x_2.index < 24]\nplt.plot(range(1, len(x_2.index) + 1), x_2['item_cnt_day'], color='green', label='2014')\n\nx_3 = x[x.index >= 24]\nplt.plot(range(1, len(x_3.index) + 1), x_3['item_cnt_day'], color='blue', label='2015')\n\nplt.legend()","8c02a5e3":"# There is only one item more expensive than 100000 (outlier).\nsales[sales['item_price'] > 100000]","6677a7ac":"x_price = sales[sales['item_price'] < 40000] # drop outliers\nx_price = x_price.groupby('item_price').agg({'item_cnt_day': 'sum'})\nx_price = x_price[x_price['item_cnt_day'].values < 20000] # drop outliers\nplt.title('Items sold by price')\nplt.scatter(x_price.index, x_price['item_cnt_day'])\nplt.xlabel('Price')\nplt.ylabel('item_cnt_day')","3c3cd316":"sales = sales[sales['date_block_num'] > 11]","33c802e5":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales[sales['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = sales[sales['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n#turn the grid into pandas dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n#get aggregated values for (shop_id, item_id, month)\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n\n#fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n# #join aggregated data to the grid\nall_data = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n\n#sort the data\nall_data.sort_values(['date_block_num','shop_id','item_id'],inplace=True)","85cc6f89":"all_data.head()","e112a6c7":"all_data.loc[all_data['shop_id'] == 0, 'shop_id'] = 57\nall_data.loc[all_data['shop_id'] == 1, 'shop_id'] = 58\nall_data.loc[all_data['shop_id'] == 11, 'shop_id'] = 10","7c5a1f8b":"lags = [1, 2, 3, 6, 12]\n\nfor lag in lags:\n    lag_col_name = 'target_lag_' + str(lag)\n    shifted = all_data[index_cols + ['target']].copy()\n    shifted.columns = index_cols + [lag_col_name]\n    shifted['date_block_num'] += lag\n    all_data = pd.merge(all_data, shifted, on=index_cols, how='left')\n    all_data[lag_col_name].fillna(0, inplace=True)","8f7ec2b6":"item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')","9d695c2e":"cumsum = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_id')['target'].cumcount()\nall_data['item_target_enc_exp'] = cumsum \/ cumcnt\n\ntarget_mean = all_data['target'].mean()\nall_data['item_target_enc_exp'].fillna(target_mean, inplace=True)","42a6eea9":"cumsum = all_data.groupby('shop_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('shop_id')['target'].cumcount()\nall_data['shop_target_enc_exp'] = cumsum \/ cumcnt\n\ntarget_mean = all_data['target'].mean()\nall_data['shop_target_enc_exp'].fillna(target_mean, inplace=True)","f2b3f8f1":"cumsum = all_data.groupby('item_category_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_category_id')['target'].cumcount()\nall_data['item_category_target_enc_exp'] = cumsum \/ cumcnt\n\ntarget_mean = all_data['target'].mean()\nall_data['item_category_target_enc_exp'].fillna(target_mean, inplace=True)","8c85c6f4":"last_sale_df = []\nfor d in range(1, 35):\n    df = sales[sales.date_block_num < d].groupby(['shop_id', 'item_id'], as_index=False)['date_block_num'].max()\n    df['last_sale_ago'] = d - df.date_block_num\n    df.date_block_num = d\n    last_sale_df.append(df)\nlast_sale_df = pd.concat(last_sale_df)\n\nall_data = pd.merge(all_data, last_sale_df, on=['shop_id', 'item_id', 'date_block_num'], how='left')\nall_data['last_sale_ago'].fillna(0, inplace=True)","d731dbcf":"last_shop_sale_df = []\nfor d in range(1, 35):\n    df = sales[sales.date_block_num < d].groupby('shop_id', as_index=False)['date_block_num'].max()\n    df['last_shop_sale_ago'] = d - df.date_block_num\n    df.date_block_num = d\n    last_shop_sale_df.append(df)\nlast_shop_sale_df = pd.concat(last_shop_sale_df)\n\nall_data = pd.merge(all_data, last_shop_sale_df, on=['shop_id', 'date_block_num'], how='left')\nall_data['last_shop_sale_ago'].fillna(0, inplace=True)","362ed886":"last_item_sale_df = []\nfor d in range(1, 35):\n    df = sales[sales.date_block_num < d].groupby('item_id', as_index=False)['date_block_num'].max()\n    df['last_item_sale_ago'] = d - df.date_block_num\n    df.date_block_num = d\n    last_item_sale_df.append(df)\nlast_item_sale_df = pd.concat(last_item_sale_df)\n\nall_data = pd.merge(all_data, last_item_sale_df, on=['item_id', 'date_block_num'], how='left')\nall_data['last_item_sale_ago'].fillna(0, inplace=True)","3ac64a92":"all_data['year_index'] = all_data['date_block_num'] \/\/ 12\nall_data['month'] = all_data['date_block_num'] % 12 + 1\nall_data = all_data.drop(columns='date_block_num')","2dd3c195":"days = pd.Series([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]) # There is no 0 month\nall_data['days_in_month'] = all_data['month'].map(days)","3e666cc1":"all_data.head()","a95b090e":"year_oh = pd.get_dummies(all_data['year_index'], prefix='year')\nmonth_oh = pd.get_dummies(all_data['month'], prefix='month')\nall_data_oh = all_data.drop(columns=['shop_id', 'item_id', 'item_category_id', 'year_index', 'month', 'target', 'last_sale_ago', 'last_shop_sale_ago', 'last_item_sale_ago'])\nall_data_oh = pd.concat([all_data_oh, year_oh, month_oh], axis=1)\nall_data_oh.head()","8c2b6f4a":"train_b_index = (all_data['year_index'] == 2) & (all_data['month'] == 9)\ntrain_c_index = (all_data['year_index'] == 2) & (all_data['month'] == 10)\ntrain_a_index = ~train_b_index & ~train_c_index\n\nX_train_a = all_data[train_a_index]\ny_train_a = X_train_a['target'].clip(0, 20)\nX_train_a = X_train_a.drop(columns='target')\n\nX_train_b = all_data[train_b_index]\ny_train_b = X_train_b['target'].clip(0, 20)\nX_train_b = X_train_b.drop(columns='target')\n\nX_train_c = all_data[train_c_index]\ny_train_c = X_train_c['target'].clip(0, 20)\nX_train_c = X_train_c.drop(columns='target')","3c705b1a":"X_train_a_oh = all_data_oh[train_a_index]\nX_train_b_oh = all_data_oh[train_b_index]\nX_train_c_oh = all_data_oh[train_c_index]","05668e22":"model_xgb = xgb.XGBRegressor(max_depth=4, learning_rate=0.5, n_jobs=-1)\nmodel_xgb.fit(X_train_a, y_train_a)","a0173fac":"# model_knn = KNeighborsRegressor(n_neighbors=3, n_jobs=-1, leaf_size=500)\n# model_knn.fit(X_train_a_oh.values, y_train_a)","328ca142":"model_mlp = MLPRegressor(hidden_layer_sizes=(100, 100), activation='relu', learning_rate_init=0.01, max_iter=10, shuffle=False, verbose=True)\nmodel_mlp.fit(X_train_a_oh.values, y_train_a)","782dfdf3":"model_rf = RandomForestRegressor(n_estimators=10, criterion='mse', max_depth=None, n_jobs=-1, verbose=1)\nmodel_rf.fit(X_train_a.values, y_train_a)","9aed86c8":"y_pred_1 = model_xgb.predict(X_train_b)\nrmse = math.sqrt(mean_squared_error(y_train_b, y_pred_1))\nR_score = r2_score(y_train_b, y_pred_1)\nprint('XGBoost rmse: ' + str(rmse) + ', R2: ' + str(R_score))","7e66ce5d":"y_pred_2 = model_mlp.predict(X_train_b_oh)\nrmse = math.sqrt(mean_squared_error(y_train_b, y_pred_2))\nR_score = r2_score(y_train_b, y_pred_2)\nprint('MLP rmse: ' + str(rmse) + ', R2: ' + str(R_score))","577bc284":"y_pred_3 = model_rf.predict(X_train_b)\nrmse = math.sqrt(mean_squared_error(y_train_b, y_pred_3))\nR_score = r2_score(y_train_b, y_pred_3)\nprint('Random forest rmse: ' + str(rmse) + ', R2: ' + str(R_score))","ddf5b692":"# y_pred_4 = model_knn.predict(X_train_b_oh)\n# rmse = math.sqrt(mean_squared_error(y_train_b, y_pred_4))\n# R_score = r2_score(y_train_b, y_pred_4)\n# print('k-NN rmse: ' + str(rmse) + ', R2: ' + str(R_score))","70bd28b1":"X_train_b_2 = np.stack([y_pred_1, y_pred_2, y_pred_3], axis=-1)\nmodel = LinearRegression(n_jobs=-1)\nmodel.fit(X_train_b_2, y_train_b)","3df935a5":"y_pred_1 = model_xgb.predict(X_train_c)\ny_pred_2 = model_mlp.predict(X_train_c_oh)\ny_pred_3 = model_rf.predict(X_train_c)\n# y_pred_4 = model_knn.predict(X_train_c_oh)\nX_train_c_2 = np.stack([y_pred_1, y_pred_2, y_pred_3], axis=-1)","6db4157b":"stack_pred = model.predict(X_train_c_2)\nrmse = math.sqrt(mean_squared_error(y_train_c, stack_pred))\nR_score = r2_score(y_train_c, y_pred_1)\nprint('Ensemble rmse: ' + str(rmse) + ', R2: ' + str(R_score))","78ac6e83":"X_train_bc_2 = np.concatenate([X_train_b_2, X_train_c_2], axis=0)\ny_train_bc = np.concatenate([y_train_b, y_train_c], axis=0)","76b61aff":"model.fit(X_train_bc_2, y_train_bc)","7a8c7ae8":"# feature extraction: fix the duplicated shop id\ntest_data.loc[test_data['shop_id'] == 0, 'shop_id'] = 57\ntest_data.loc[test_data['shop_id'] == 1, 'shop_id'] = 58\ntest_data.loc[test_data['shop_id'] == 11, 'shop_id'] = 10\n\n# Generate lag features\ntest_data['date_block_num'] = 34\nlags = [1, 2, 3, 6, 12]\nall_data['date_block_num'] = all_data['year_index'] * 12 + all_data['month'] - 1\n\nfor lag in lags:\n    lag_col_name = 'target_lag_' + str(lag)\n    shifted = all_data[index_cols + ['target']].copy()\n    shifted.columns = index_cols + [lag_col_name]\n    shifted['date_block_num'] += lag\n    test_data = pd.merge(test_data, shifted, on=index_cols, how='left')\n    test_data[lag_col_name].fillna(0, inplace=True)\n\n# Add item category\ntest_data = pd.merge(test_data, item_category_mapping, how='left', on='item_id')\n\n# Add expanding mean encoding for item_id\nitem_id_mean = all_data.groupby('item_id')['target'].mean()\ntest_data['item_target_enc_exp'] = test_data['item_id'].map(item_id_mean)\ntest_data['item_target_enc_exp'].fillna(target_mean, inplace=True)\n\n# Add expanding mean encoding for shop_id\nshop_id_mean = all_data.groupby('shop_id')['target'].mean()\ntest_data['shop_target_enc_exp'] = test_data['shop_id'].map(shop_id_mean)\ntest_data['shop_target_enc_exp'].fillna(target_mean, inplace=True)\n\n# Add expanding mean encoding for item_id\nitem_id_mean = all_data.groupby('item_category_id')['target'].mean()\ntest_data['item_category_target_enc_exp'] = test_data['item_category_id'].map(item_id_mean)\ntest_data['item_category_target_enc_exp'].fillna(target_mean, inplace=True)\n\n# Add last sale ago for shop_id, item_id pairs\ntest_data = pd.merge(test_data, last_sale_df, on=['shop_id', 'item_id', 'date_block_num'], how='left')\ntest_data['last_sale_ago'].fillna(0, inplace=True)\n\n# Add last shop sale\ntest_data = pd.merge(test_data, last_shop_sale_df, on=['shop_id', 'date_block_num'], how='left')\ntest_data['last_shop_sale_ago'].fillna(0, inplace=True)\n\n# Add last item sale\ntest_data = pd.merge(test_data, last_item_sale_df, on=['item_id', 'date_block_num'], how='left')\ntest_data['last_item_sale_ago'].fillna(0, inplace=True)\n\n# Add year index and month\ntest_data['year_index'] = 2\ntest_data['month'] = 11\n\n# Add days in month\ntest_data['days_in_month'] = 30\n\n# Drop id column\ntest_data.drop(columns='ID', inplace=True)\n\n# Drop date_block_num\ntest_data.drop(columns='date_block_num', inplace=True)\n\ntest_data.head()","2cf69744":"test_data_oh = test_data.drop(columns=['shop_id', 'item_id', 'item_category_id', 'year_index', 'month', 'last_sale_ago', 'last_shop_sale_ago', 'last_item_sale_ago'])\nfor year in range(1,3):\n    test_data_oh['year_' + str(year)] = int(year == 2)\nfor month in range(1, 13):\n    test_data_oh['month_' + str(month)] = int(month == 11)\ntest_data_oh.head()","4f119cd1":"X_test_2 = np.stack([model_xgb.predict(test_data), model_mlp.predict(test_data_oh), model_rf.predict(test_data)], axis=-1)\npredictions = model.predict(X_test_2)\n\ndf_pred = pd.DataFrame({'item_cnt_month': predictions})\ndf_pred.to_csv('submission.csv', index_label='ID')","979a788c":"sub = pd.read_csv('submission.csv')\nsub.head()","b4cd41db":"Expanding mean encoding sales by shop id","858bf238":"Train random forest regressor","97d21abf":"# Stacking","689b9408":"Do the split for linear models dataset","e1087cc4":"Train MLP","f2e9b496":"Level 2 model","d985de10":"Train k-NN","4292584b":"Add number of days in a month","68380e19":"# Make predictions","a14a3487":"Create test dataset for linear models","b0bb36f5":"# Train the models on train_a","3a761f34":"One-hot encode year and month for linear models","85952003":"# Train\/validation split by time","d35698c6":"Predict","3f152371":"# Evaluate model on train_c","43aaa2d2":"# Exploratory data analysis","da5797df":"Encode the time as year and month","788fd96e":"Add item category id","f3cd3f08":"Remove first year of sales data","8010ef00":"Generate lag features","99a35970":"Add additional features to the test data","a224ebd0":"# Feature engineering","91840091":"# Read the data","f73dac3e":"Fix the duplicated shop id","33d5a9a8":"# Imports","03a56d28":"# Train level 2 model on train_b + train_c","2e50b86b":"Expanding mean encoding sales by item id","248b2a1b":"Add last item sale","41d7b7d9":"Add last shop sale","65f9cd71":"Add last sale for shop_id, item_id pairs","959220d5":"Train XGBoost","50a63c15":"Expanding mean encoding sales by item category id","c260ddaf":"Aggregate and sort the data"}}