{"cell_type":{"f6641f62":"code","e38f9c70":"code","46040382":"code","f9e64328":"code","5b5490b4":"code","04378c0c":"code","13f54e3c":"code","0f727bea":"code","c2645152":"code","0f2c7e9f":"code","46f56536":"code","be860f9c":"code","de3a86e2":"code","73fac21c":"code","13379ee9":"code","7b6237e3":"code","74b58673":"code","31ada908":"code","3d88b240":"code","6a87391b":"code","8880a891":"code","19b642ff":"code","7c9a1bf1":"code","db6ca689":"code","f4160cae":"code","885b2a94":"code","028e4d0f":"markdown","b053e2b7":"markdown","83abda63":"markdown","547553fa":"markdown","8114e74d":"markdown","96caa646":"markdown","8303c637":"markdown","6e4de121":"markdown","2a1a07f1":"markdown","0f6c7f4a":"markdown","3d3ccbdb":"markdown","a6241769":"markdown","57bfe3f6":"markdown","c0203cd4":"markdown"},"source":{"f6641f62":"import numpy as np\nimport pandas as pd\n\nfrom scipy.stats import chi2_contingency\n\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nSMALL_SIZE = 8\nMEDIUM_SIZE = 10\nBIGGER_SIZE = 12\n\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n\nimport os\n\ndata_path = '..\/input\/mushroom-classification\/mushrooms.csv'\nmushroom_data = pd.read_csv(data_path)","e38f9c70":"mushroom_data.info()","46040382":"mushroom_data.head()","f9e64328":"# Shape of the dataset\nprint(\"We have\", mushroom_data.shape[1], \"features with\", mushroom_data.shape[0], \"observations\")\n\nmushroom_data.describe().T","5b5490b4":"# Number of missing data values\nprint(\"The dataset has\", mushroom_data.isnull().sum().sum(), \"missing points.\")\n\nmushroom_data.isnull().sum()","04378c0c":"# object_columns = [col for col in mushroom_data.columns if mushroom_data[col].dtype==object]\nmushroom_data.dtypes","13f54e3c":"class_data = mushroom_data['class'].value_counts()\npie, ax = plt.subplots(figsize=[10,6])\nlabels = class_data.keys()\nplt.pie(x=class_data, autopct=\"%.1f%%\",explode=[0,0.05], labels=labels, pctdistance=0.5)\nplt.title(\"Edible vs Poisonous Mushrooms in Dataset\", fontsize=14)","0f727bea":"# Copy the original data\nX = mushroom_data.copy()\ny = X.pop('class')\n\n# Remove useless feature\nX = X.drop('veil-type', axis=1)\n\nX = pd.get_dummies(X)\ny = pd.factorize(y)\n\nX.head()","c2645152":"X_temp = mushroom_data.copy().drop('class', axis=1).drop('veil-type', axis=1)\n# Find the p values from the chi2 test for each column\nchi2_scores = []\nchi2_pvalues = []\nfeatures_to_drop = []\nfor i in X_temp.columns:\n    dummies = pd.get_dummies(X_temp[i])\n    for j in dummies:\n        chi_score = chi2_contingency(pd.crosstab(y[0],dummies[j]))[0]\n        p_value = chi2_contingency(pd.crosstab(y[0],dummies[j]))[1]*X_temp[i].nunique()\n        chi2_scores.append(chi_score)\n        chi2_pvalues.append(p_value)\n        if p_value > 0.05:\n            features_to_drop.append(i+'_'+j)\nchi2_scores = pd.Series(chi2_scores, index=X.columns).sort_values(ascending=False)\nchi2_scores","0f2c7e9f":"# Plot the Chi2 scores\nfig, ax = plt.subplots(figsize=(15,25))\n\nsns.barplot(x = chi2_scores, y=chi2_scores.index)\n\nax.set_xlabel('Chi-Squared Scores')\nax.set_ylabel('Features')\nax.set_title('Chi-Squared Scores of Features')","46f56536":"# These are the features that failed to reject the null hypothesis (p-value>0.05)\nfeatures_to_drop","be860f9c":"# Dropping failed features\nX_chi2 = X.drop(features_to_drop, axis=1)\nX_chi2.head()","de3a86e2":"# Mutual Information\nmi_scores = mutual_info_classif(X, y[0])\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\nmi_scores = mi_scores.sort_values(ascending=False)\nmi_scores","73fac21c":"# Plot the Chi2 scores\nfig, ax = plt.subplots(figsize=(15,25))\n\nsns.barplot(x = mi_scores, y=mi_scores.index)\n\nax.set_xlabel('Mutual Information Scores')\nax.set_ylabel('Features')\nax.set_title('MI Score')","13379ee9":"# Lets find redundant and non-relevant features\nduplicates = mi_scores.duplicated()\n\nbad_features = [x for x in duplicates.index if (duplicates[x]==True)]\nbad_features","7b6237e3":"X_mi = X.drop(bad_features, axis=1)\nX_mi.head()","74b58673":"# Common dropped features from Chi2 test and MI test\ncommon_bad_features = set(features_to_drop) & set(bad_features)\ncommon_bad_features","31ada908":"# Separate training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y[0], test_size=0.25, random_state=0)\n\n# Baseline Random Forest with default parameters\nmodel = RandomForestClassifier(random_state=0)\nfit = model.fit(X_train, y_train)\npredictions = fit.predict(X_test)\nprint(classification_report(predictions, y_test))","3d88b240":"# 5 fold CV of default Random Forest model\nscore = cross_val_score(\n    model, X, y[0], cv=5, scoring='accuracy'\n)\nprint('Average accuracy of base model: ', score.mean())","6a87391b":"# Separate training and testing data for the chi2 features\nXchi_train, Xchi_test, y_train, y_test = train_test_split(X_chi2, y[0], test_size=0.25, random_state=0)\n\n# Chi 2 features model\nfit_chi = model.fit(Xchi_train, y_train)\npredictions_chi = fit_chi.predict(Xchi_test)\nprint(classification_report(predictions_chi, y_test))","8880a891":"# 5 fold CV of Chi2 Random Forest model\nscore = cross_val_score(\n    model, X_chi2, y[0], cv=5, scoring='accuracy'\n)\nprint('Average accuracy of chi2 model: ', score.mean())","19b642ff":"# 5 fold CV of MI Random Forest model\nscore = cross_val_score(\n    model, X_mi, y[0], cv=5, scoring='accuracy'\n)\nprint('Average accuracy of MI model: ', score.mean())","7c9a1bf1":"model_50 = RandomForestClassifier(n_estimators=50, random_state=0)\n\n# 5 fold CV of default Random Forest model\nscore = cross_val_score(\n    model_50, X, y[0], cv=5, scoring='accuracy'\n)\nprint('Average accuracy of base model (50 trees):', score.mean())\n\n# 5 fold CV of Chi2 Random Forest model\nscore = cross_val_score(\n    model_50, X_chi2, y[0], cv=5, scoring='accuracy'\n)\nprint('Average accuracy of chi2 model (50 trees):', score.mean())\n\n# 5 fold CV of MI Random Forest model\nscore = cross_val_score(\n    model_50, X_mi, y[0], cv=5, scoring='accuracy'\n)\nprint('Average accuracy of MI model (50 trees):', score.mean())","db6ca689":"n_estimators = [10,20,30,40,50,60,70,80,90,100]\nmean_scores = []\nfor i in n_estimators:\n    model = RandomForestClassifier(n_estimators=i, random_state=0)\n    score = cross_val_score(\n    model, X_mi, y[0], cv=5, scoring='accuracy'\n    )\n    mean_scores.append(score.mean())\n    print('Average accuracy of MI model (',i,'trees):', score.mean())","f4160cae":"mean_scores = pd.Series(mean_scores, index=n_estimators)\n\n# Plot the CV scores\nfig, ax = plt.subplots(figsize=(12,8))\n\nsns.lineplot(x = mean_scores.index, y=mean_scores)\n\nax.set_ylabel('Accuracy')\nax.set_xlabel('# of n_estimators')\nax.set_title('Accuracy of different n_estimators (5 fold CV)')","885b2a94":"# MI model CV with different k-folds\naccuracies = []\nmodel = RandomForestClassifier(n_estimators=50, random_state=0)\nfor i in range(5,55,5):\n    score = cross_val_score(\n    model, X_mi, y[0], cv=i, scoring='accuracy'\n    )\n    accuracies.append(score.mean())\n    \naccuracies = pd.Series(accuracies, index=np.arange(5,55,5))\n\n# Plot the CV scores\nfig, ax = plt.subplots(figsize=(12,8))\n\nsns.lineplot(x = accuracies.index, y=accuracies)\n\nax.set_ylabel('Accuracy')\nax.set_xlabel('# of folds')\nax.set_title('Accuracy vs number of k-folds')","028e4d0f":"# Random Forest - Poisonous Mushroom Classifier\n\nHave you ever picked up wild mushroom and wondered whether it was edible or not? We are given a dataset of purely categorical features and want to create a model to classify whether a mushroom with specific traits is edible or poisonous. \nThe data guide states that there is no simple rule to determine whether a mushroom is poisonous or not. This is why an ML model would be beneficial.\n\n# 0. Import Libraries and Dataset","b053e2b7":"We got an optimal number of estimators of 50. Now compare the accuracy with different k-folds.","83abda63":"Interestingly enough, we got an accuracy of 100% which does not seem very reasonable. I will conduct a 5 fold CV on the default model to get a more robust accuracy.","547553fa":"Even with a random forest with 50 trees, our MI model still comes out on top with the Chi2 on the bottom again.\n\nLets try to optimize the n_estimators parameter such that we are not under nor over fitting the data.","8114e74d":"# 1. Exploratory Data Analysis\n\nWe can start by taking a peak at the dataset and how it is formatted.","96caa646":"### Summary\n\nWe have 8124 observations with 23 features. One of the features will be used for our target variable (class). All our features are object types and are not missing any data points.\n\n### Initial Thoughts\n\nSince we have a relatively small dataset here, I am thinking of one-hot encoding all the features. If we did have missing values, I would experiment with removing rows or imputing and how that would affect the model.\nI also noticed that there a feature that has one value for all 8124 observations (veil-type). I will probably end up dropping that column from the dataset.","8303c637":"# 3. Model Building\n\nThe algorithm we will use for modeling is Random Forest. To increase robustness of the model, I will perform cross validation to get a mean accuracy between 5 folds of training and testing data. Also, all feature selection data sets will be tested, and the performance will be compared.\n\n### Baseline Model","6e4de121":"Again, we have a 100% accuracy but for the chi2 model. Lets try 5-fold CV.","2a1a07f1":"### Summary\n\nOf the dropped features between the two modified input data, we had 4 common features. \n\nOne thing I did not consider for the mutual information test was testing for interaction variables. Just because the features were not correlated to the target variable does not mean they can have some relationship with the other features. ","0f6c7f4a":"### What is the Chi-squared test for feature selection?\n\nThe Chi-squared test for independence is for testing whether two variables are independent from each other. When working with categorical data, the chi-square test is often used to minimize the noise in the input when building the model. \n\n\\begin{gather*}\nH_0 = \\text{The relationship is independent}\\\\\nH_1 = \\text{The relationship is not independent}\n\\end{gather*}\n\nThe p-value we will use as our threshold is 0.05. That means we are looking at a 95% confidence interval. However one thing to note is that we are one-hot encoding the features before running the chi-squared tests. Since we are doing it this way, \nthe p-value that we are going to use to reject the null hypothesis is going to be different depending on the feature's number of unique instances. The p-value we will actually use will be determined by the following formula:\n\n\\begin{gather*}\n\\text{p-value} = \\frac{0.05}{\\text{# of unique indexes}}\n\\end{gather*}","3d3ccbdb":"# Conclusion\n\nBuilding a model based on the features selected from Mutual information resulted in the best performance compared to Chi-squared and default. Also, one thing I realize is that I split the training and testing data after modifying the data. Normally I would split the data first, but since I did not really alter any values (imputation), I do not think it would make a difference.\n\n\n# Question for the Community\n\nI'm under the impression that \"leave one out\" cross validation is preferred for smaller datasets, however I am getting a 100% accuracy on anything past 25 fold CV. Is this a case of overfitting, or is it just the dataset?","a6241769":"### Mutual Information method of Feature Selection\n\nMutual Information measures the non-linear relationship between two variables. We will be calculating a score for mutual information and build a model with the selected features.","57bfe3f6":"With default parameters for the model, we have a slight improvement in accuracy for the MI input data, while a slight decrease in accuracy for the Chi2 input.\n\nLet's try them with 50 trees now in the random forest.","c0203cd4":"# 2. Feature Engineering and Selection\n\nBecause all of our features are categorical along with our target variable, we have two main tests for feature importance\n\n1. Chi-squared test\n2. Mutual Information\n\n### One-Hot Encoding (getting data ready for analysis)"}}