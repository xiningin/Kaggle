{"cell_type":{"c61b961b":"code","aec08998":"code","ac808fbc":"code","beae9258":"code","c81c07ec":"code","d553a919":"code","5e996729":"code","819798a4":"code","1e6fd2f2":"code","ca269b2c":"code","65d64bca":"code","e10d65be":"code","dfce5e79":"code","3d00e7ee":"code","5b5e811a":"code","5d1443fe":"code","a2b31265":"code","3133084c":"code","29a46dc6":"code","9037e65d":"code","2cabc641":"code","cef5779a":"code","54f6eec6":"code","cfacb5ad":"code","930bd627":"code","94109a8f":"code","d1c4c9ff":"code","77bd5f54":"code","3db28e8b":"code","e7678991":"code","a2d7c97c":"code","5096db10":"markdown","73fc11d3":"markdown","e7dfca03":"markdown","a54c0a52":"markdown","5cfea8fd":"markdown","3244a7d3":"markdown","d1d9ec62":"markdown","905330c8":"markdown","c9f291a5":"markdown","31586cd9":"markdown","35cce085":"markdown","25466dc6":"markdown","c60aa636":"markdown","8c38de5d":"markdown","6add64b4":"markdown","a4b5f3fb":"markdown","46edd78d":"markdown","1d9afd79":"markdown","2264652b":"markdown","285ff1a2":"markdown","294add38":"markdown","ef4532fd":"markdown","8845b2ae":"markdown","93095b85":"markdown","fb86fa5c":"markdown","c60307b9":"markdown","aac0197f":"markdown","47e6e76e":"markdown","f800a211":"markdown","1cbbd421":"markdown"},"source":{"c61b961b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras import optimizers\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nprint(os.listdir(\"..\/input\"))","aec08998":"X=np.load(\"..\/input\/sign-language-digits-dataset\/Sign-language-digits-dataset\/X.npy\")\ny=np.load(\"..\/input\/sign-language-digits-dataset\/Sign-language-digits-dataset\/Y.npy\")\nprint(\"The dataset loaded...\")","ac808fbc":"def show_model_history(modelHistory, model_name):\n    history=pd.DataFrame()\n    history[\"Train Loss\"]=modelHistory.history['loss']\n    history[\"Validation Loss\"]=modelHistory.history['val_loss']\n    history[\"Train Accuracy\"]=modelHistory.history['accuracy']\n    history[\"Validation Accuracy\"]=modelHistory.history['val_accuracy']\n    \n    fig, axarr=plt.subplots(nrows=2, ncols=1 ,figsize=(12,8))\n    axarr[0].set_title(\"History of Loss in Train and Validation Datasets\")\n    history[[\"Train Loss\", \"Validation Loss\"]].plot(ax=axarr[0])\n    axarr[1].set_title(\"History of Accuracy in Train and Validation Datasets\")\n    history[[\"Train Accuracy\", \"Validation Accuracy\"]].plot(ax=axarr[1]) \n    plt.suptitle(\" Convulutional Model {} Loss and Accuracy in Train and Validation Datasets\".format(model_name))\n    plt.show()","beae9258":"from keras.callbacks import EarlyStopping\ndef split_dataset(X, y, test_size=0.3, random_state=42):\n    X_conv=X.reshape(X.shape[0], X.shape[1], X.shape[2],1)\n    \n    \n\n    return train_test_split(X_conv,y, stratify=y,test_size=test_size,random_state=random_state)\n\ndef evaluate_conv_model(model, model_name, X, y, epochs=100,\n                        optimizer=optimizers.RMSprop(lr=0.0001), callbacks=None):\n    print(\"[INFO]:Convolutional Model {} created...\".format(model_name))\n    X_train, X_test, y_train, y_test = split_dataset(X, y)\n    \n    \n    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n    print(\"[INFO]:Convolutional Model {} compiled...\".format(model_name))\n    \n    print(\"[INFO]:Convolutional Model {} training....\".format(model_name))\n    earlyStopping = EarlyStopping(monitor = 'val_loss', patience=20, verbose = 1) \n    if callbacks is None:\n        callbacks = [earlyStopping]\n    modelHistory=model.fit(X_train, y_train, \n             validation_data=(X_test, y_test),\n             callbacks=callbacks,\n             epochs=epochs,\n             verbose=0)\n    print(\"[INFO]:Convolutional Model {} trained....\".format(model_name))\n\n    test_scores=model.evaluate(X_test, y_test, verbose=0)\n    train_scores=model.evaluate(X_train, y_train, verbose=0)\n    print(\"[INFO]:Train Accuracy:{:.3f}\".format(train_scores[1]))\n    print(\"[INFO]:Validation Accuracy:{:.3f}\".format(test_scores[1]))\n    \n    show_model_history(modelHistory=modelHistory, model_name=model_name)\n    return model","c81c07ec":"def decode_OneHotEncoding(label):\n    label_new=list()\n    for target in label:\n        label_new.append(np.argmax(target))\n    label=np.array(label_new)\n    \n    return label\ndef correct_mismatches(label):\n    label_map={0:9,1:0, 2:7, 3:6, 4:1, 5:8, 6:4, 7:3, 8:2, 9:5}\n    label_new=list()\n    for s in label:\n        label_new.append(label_map[s])\n    label_new=np.array(label_new)\n    \n    return label_new\n    \ndef show_image_classes(image, label, n=10):\n    label=decode_OneHotEncoding(label)\n    label=correct_mismatches(label)\n    fig, axarr=plt.subplots(nrows=n, ncols=n, figsize=(18, 18))\n    axarr=axarr.flatten()\n    plt_id=0\n    start_index=0\n    for sign in range(10):\n        sign_indexes=np.where(label==sign)[0]\n        for i in range(n):\n\n            image_index=sign_indexes[i]\n            axarr[plt_id].imshow(image[image_index], cmap='gray')\n            axarr[plt_id].set_xticks([])\n            axarr[plt_id].set_yticks([])\n            axarr[plt_id].set_title(\"Sign :{}\".format(sign))\n            plt_id=plt_id+1\n    plt.suptitle(\"{} Sample for Each Classes\".format(n))\n    plt.show()","d553a919":"number_of_pixels=X.shape[1]*X.shape[2]\nnumber_of_classes=y.shape[1]\nprint(20*\"*\", \"SUMMARY of the DATASET\",20*\"*\")\nprint(\"an image size:{}x{}\".format(X.shape[1], X.shape[2]))\nprint(\"number of pixels:\",number_of_pixels)\nprint(\"number of classes:\",number_of_classes)\n\ny_decoded=decode_OneHotEncoding(y.copy())\nsample_per_class=np.unique(y_decoded, return_counts=True)\nprint(\"Number of Samples:{}\".format(X.shape[0]))\nfor sign, number_of_sample in zip(sample_per_class[0], sample_per_class[1]):\n    print(\"  {} sign has {} samples.\".format(sign, number_of_sample))\nprint(65*\"*\")","5e996729":"show_image_classes(image=X, label=y.copy())","819798a4":"def build_conv_model_1():\n    model=Sequential()\n    \n    model.add(layers.Conv2D(64, kernel_size=(3,3),\n                           padding=\"same\",\n                           activation=\"relu\", \n                           input_shape=(64, 64,1)))\n    model.add(layers.MaxPooling2D((2,2)))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(128, activation=\"relu\"))\n    model.add(layers.Dense(number_of_classes, activation=\"softmax\"))\n        \n    return model","1e6fd2f2":"trained_models=dict()\nmodel=build_conv_model_1()\ntrained_model_1=evaluate_conv_model(model=model, model_name=1, X=X, y=y)\n\n#Will be used for serialization\ntrained_models[\"model_1\"]=(trained_model_1,optimizers.RMSprop(lr=0.0001) )","ca269b2c":"def build_conv_model_2():\n    model = Sequential()\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(64, 64, 1)))\n    model.add(layers.MaxPooling2D((2, 2)))\n       \n    model.add(layers.Convolution2D(64, (3, 3), activation='relu', padding=\"same\"))\n    model.add(layers.MaxPooling2D((2, 2)))\n        \n    model.add(layers.Flatten())\n    model.add(layers.Dense(128, activation='relu'))\n    model.add(layers.Dense(10, activation='softmax'))\n      \n    return model","65d64bca":"model=build_conv_model_2()\ntrained_model_2=evaluate_conv_model(model=model, model_name=2, X=X, y=y)\n\n#Will be used for serialization\ntrained_models[\"model_2\"]=(trained_model_2,optimizers.RMSprop(lr=0.0001) )","e10d65be":"def build_conv_model_3():\n    model = Sequential()\n    model.add(layers.Convolution2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n    model.add(layers.MaxPooling2D((2, 2)))\n    \n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    \n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n           \n    model.add(layers.Flatten())\n    model.add(layers.Dense(128, activation='relu'))\n    model.add(layers.Dense(10, activation='softmax'))\n        \n    return model","dfce5e79":"model=build_conv_model_3()\ntrained_model_3=evaluate_conv_model(model=model, model_name=3, X=X, y=y)\n#Will be used for serialization\ntrained_models[\"model_3\"]=(trained_model_3,optimizers.RMSprop(lr=0.0001) )","3d00e7ee":"def build_conv_model_4():\n    model = Sequential()\n    model.add(layers.Convolution2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Dropout(0.25))\n    \n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Dropout(0.25))\n    \n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Dropout(0.25))\n       \n    model.add(layers.Flatten())\n    model.add(layers.Dense(128, activation='relu'))\n    model.add(layers.Dense(10, activation='softmax'))\n\n    return model","5b5e811a":"model=build_conv_model_4()\ntrained_model_4=evaluate_conv_model(model=model, model_name=4, X=X, y=y)\n\n#Will be used for serialization\ntrained_models[\"model_4\"]=(trained_model_4,optimizers.RMSprop(lr=0.0001) )","5d1443fe":"def build_conv_model_5():\n    model = Sequential()\n    model.add(layers.Convolution2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Dropout(0.25))\n    \n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Dropout(0.25))\n    \n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Dropout(0.25))\n    \n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Dropout(0.25))\n       \n    model.add(layers.Flatten())\n    model.add(layers.Dense(128, activation='relu'))\n    model.add(layers.Dense(10, activation='softmax'))\n        \n    return model","a2b31265":"model=build_conv_model_5()\ntrained_model_5=evaluate_conv_model(model=model, model_name=5, X=X, y=y)\n#Will be used for serialization\ntrained_models[\"model_5\"]=(trained_model_5,optimizers.RMSprop(lr=0.0001) )","3133084c":"def build_conv_model_6():\n    model = Sequential()\n    model.add(layers.Convolution2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n    model.add(layers.MaxPooling2D((2, 2)))\n\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(256, activation='relu'))\n    model.add(layers.Dense(10, activation='softmax'))\n        \n    return model","29a46dc6":"model=build_conv_model_6()\ntrained_model_6=evaluate_conv_model(model=model, model_name=6, X=X, y=y)\n#Will be used for serialization\ntrained_models[\"model_6\"]=(trained_model_6,optimizers.RMSprop(lr=0.0001) )","9037e65d":"def build_conv_model_7():\n    model = Sequential()\n    model.add(layers.Convolution2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.BatchNormalization())\n\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.BatchNormalization())\n\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.BatchNormalization())\n\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.BatchNormalization())\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(256, activation='relu'))\n    model.add(layers.Dense(10, activation='softmax'))\n        \n    return model","2cabc641":"model=build_conv_model_7()\ntrained_model_7=evaluate_conv_model(model=model, model_name=7, X=X, y=y)\n#Will be used for serialization\ntrained_models[\"model_7\"]=(trained_model_7,optimizers.RMSprop(lr=0.0001) )","cef5779a":"def build_conv_model_8():\n    model = Sequential()\n    model.add(layers.Convolution2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.25))\n\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.25))\n\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.25))\n\n    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.25))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(256, activation='relu'))\n    model.add(layers.Dense(10, activation='softmax'))\n        \n    return model","54f6eec6":"model=build_conv_model_8()\ntrained_model_8_1=evaluate_conv_model(model=model, model_name=8, X=X, y=y)\n#Will be used for serialization\ntrained_models[\"model_8_1\"]=(trained_model_8_1,optimizers.RMSprop(lr=0.0001) )","cfacb5ad":"model=build_conv_model_8()\noptimizer=optimizers.RMSprop(lr=1e-4)# our default optimizer in evaluate_conv_model function\ntrained_model_8_2=evaluate_conv_model(model=model, model_name=8, X=X, y=y,optimizer=optimizer, epochs=200)\n\n#Will be used for serialization\ntrained_models[\"model_8_2\"]=(trained_model_8_2,optimizer )","930bd627":"model=build_conv_model_8()\noptimizer=optimizers.Adam(lr=0.001)\ntrained_model_8_3=evaluate_conv_model(model=model, model_name=8, X=X, y=y, optimizer=optimizer, epochs=250)\n#Will be used for serialization\ntrained_models[\"model_8_3\"]=(trained_model_8_3,optimizer )","94109a8f":"model=build_conv_model_8()\noptimizer_8_4=optimizers.Adam(lr=0.001)\ntrained_model_8_4=evaluate_conv_model(model=model, model_name=8, X=X, y=y, optimizer=optimizer_8_4, epochs=300)\n#Will be used for serialization\ntrained_models[\"model_8_4\"]=(trained_model_8_4,optimizer )","d1c4c9ff":"from keras.models import model_from_json, model_from_yaml\nclass Save:\n    @classmethod\n    def save(self, model, model_file_name, hdf5_file_name):\n        if \"json\" in model_file_name:\n            model_format=model.to_json()\n        else:\n            model_format=model.to_yaml()\n        with open(model_file_name, \"w\") as file:\n            file.write(model_format)\n        model.save_weights(hdf5_file_name)\nclass Load:\n    @classmethod\n    def load(self, model_file_name, hdf5_file_name):\n        format_file=open(model_file_name)\n        loaded_file=format_file.read()\n        format_file.close()\n        if \"json\" in model_file_name:\n            model=model_from_json(loaded_file)\n        else:\n            model=model_from_yaml(loaded_file)\n        model.load_weights(hdf5_file_name)\n        \n        return model\nclass YAML:\n    def __init__(self):\n        self.yaml_file_name=None\n        self.hdf5_file_name=None\n    \n    def save(self, model, model_name):\n        self.yaml_file_name=model_name+\".yaml\"\n        self.hdf5_file_name=model_name+\"_yaml.hdf5\"\n        Save.save(model,\n                  self.yaml_file_name,\n                  self.hdf5_file_name)        \n        \n        print(\"YAML model and HDF5 weights saved to disk...\")\n        print(\"Model file name:{}\".format(self.yaml_file_name))\n        print(\"Weights file name:{}\".format(self.hdf5_file_name))\n    \n    def load(self):\n              \n        print(\"YAML model and HDF5 loaded from disk...\")\n        return  Load.load(self.yaml_file_name, self.hdf5_file_name)\n        \nclass JSON:\n    def __init__(self):\n        self.json_file_name=None\n        self.hdf5_file_name=None\n        \n    def save(self, model, model_name):\n        self.json_file_name=model_name+\".json\"\n        self.hdf5_file_name=model_name+\"_json.hdf5\"\n        Save.save(model,\n                  self.json_file_name,\n                  self.hdf5_file_name)\n        \n        print(\"JSON model and HDF5 weights saved to disk...\")\n        print(\"Model file name:{}\".format(self.json_file_name))\n        print(\"Weights file name:{}\".format(self.hdf5_file_name))\n        \n    \n    def load(self):\n\n        print(\"JSON model and HDF5 weights loaded from disk...\")\n        return Load.load(self.json_file_name, self.hdf5_file_name)\n        \nclass Serialization():\n    def __init__(self, file_format):\n        assert file_format in [\"json\", \"yaml\"], \"There is no such a serialization format\"\n        self.file_format=file_format\n        if self.file_format==\"json\":\n            self.serialization_type=JSON()\n        else:\n            self.serialization_type=YAML()\n    \n    def save(self, model, model_name=\"model\"):\n        self.serialization_type.save(model, model_name)\n    def load(self):\n        return self.serialization_type.load()","77bd5f54":"def test_serialization(trained_models, format_type):\n    for model_name, model_pack in trained_models.items():\n        model, optimizer=model_pack\n        serialization=Serialization(format_type)\n        serialization.save(model, model_name=model_name)\n\n        loaded_model=serialization.load()\n        X_train, X_test, y_train, y_test=split_dataset(X, y)\n\n\n        #optimizer=optimizers.RMSprop(lr=0.0001)\n        loaded_model.compile(loss=\"categorical_crossentropy\", \n                             optimizer=optimizer,\n                             metrics=[\"accuracy\"])\n\n        train_scores = loaded_model.evaluate(X_train, y_train, verbose=0)\n        test_scores  = loaded_model.evaluate(X_test, y_test, verbose=0)\n        print(\"Train accuracy:{:.3f}\".format(train_scores[1]))\n        print(\"Test accuracy:{:.3f}\".format(test_scores[1]))\n        print()","3db28e8b":"test_serialization(trained_models, format_type=\"json\")","e7678991":"import yaml\nyaml.warnings({'YAMLLoadWarning': False})\ntest_serialization(trained_models, format_type=\"yaml\")","a2d7c97c":"print(\"Created files in Kaggle working directory:\")\nfor file in sorted(os.listdir(\"..\/working\")):\n    if \"ipynb\" in file:\n        continue\n    print(file)","5096db10":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"1.\"><\/a>\n# **1. Summary**\n\nIn this kernel, implementing and performance of Convolutional Neural Network will be demonstrated on Sign Language dataset. CNN networks basically consist of three main parts; Conv, Pooling and Dense layers. Conv layers consist of filters and feature maps. The Pooling layer reduces the feature obtained in the previous Conv layer. The Dense layer is the normal feedforward network layer.\n\nThe summary of the study is explained in the following paragraphs.\n\nAuxiliary functions are defined to reduce code redundancy and confusion in the study. These functions are defined as a function of the same process used in many parts of the study and then  called when it needed.\n\nIn the **first** CNN model, a CNN containing a single Conv layer. The CNN structure is as follows;\n\nConv ==> MaxPooling ==> Dense (relu) ==> Dense (softmax).\n\nObtained train and validation accuracy rates are low and there are problems of overfitting(excessive adaptation) and high variance.\n\nIn the **second** CNN model, two Conv layers were used. The network structure is as follows:\n\nConv ==> MaxPooling ==> Conv ==> MaxPooling ==> Dense (relu) ==> Dense (softmax).\n\nDespite the increased success rates of train and validation of the model, there is still excessive adaption and high variance prolemia.\n\nIn the **third** CNN model, three Conv layers were used. The network structure is as follows:\n\nConv ==> MaxPooling ==> Conv ==> MaxPooling ==> Conv ==> MaxPooling ==> Dense (relu) ==> Dense (softmax).\n\nSince there is little improvement compared to the previous model, a Dropout layer was added to the next model instead of adding a Conv layer.\n\nIn the **fourth** CNN model, three Conv layers and Dropout layers were used. The network structure is as follows:\n\nConv ==> MaxPooling ==> Dropout ==> Conv ==> MaxPooling ==> Dropout ==> Conv ==> MaxPooling ==> Dropout ==> Dense (relu) ==> Dense (softmax).\n\nAlthough the model has reduced overfitting,high variance still exists.\n\nThe **fifth** CNN model has one Conv ==> MaxPooling ==> Dropout block than the fourth CNN model.\n\nConv ==> MaxPooling ==> Dropout ==> Conv ==> MaxPooling ==> Dropout ==> Conv ==> MaxPooling ==> Dropout ==> Conv ==> MaxPooling ==> Dropout ==> Dense ( relu) = Dense (softmax).\n\nOverfitting and high variance were solved in the model, but the train and validation performance of the model decreased.\n\nIn the **sixth** CNN model, the Dropout layer is used between two Dense layers. The network structure is as follows:\n\nConv ==> MaxPooling ==> Conv ==> MaxPooling ==> Conv ==> MaxPooling ==> Conv ==> MaxPooling ==> Dense (relu) ==> Dropout ==> Dense (softmax).\n\nThe success rate of the model increased without excessive fit of the model and high variance.\n\nIn the **seventh** model, BatchNormalization layer is used to improve performance. The network structure is as follows:\n\nConv ==> MaxPooling ==> BatchNormalization ==> Conv ==> MaxPooling ==> BatchNormalization ==> Conv ==> MaxPooling ==> BatchNormalization ==> Conv ==> MaxPooling ==> BatchNormalization ==> Dense ( relu) ==> Dropout ==> Dense (softmax).\n\nThe model performance increased but there was an excessive adaptation problem.\n\nIn the **eighth** model, Dropout layer is used after Conv layers again. The network structure is as follows:\n\nConv ==> MaxPooling ==> BatchNormalization ==> Dropout ==> Conv ==> MaxPooling ==> BatchNormalization ==> Dropout ==> Conv ==> MaxPooling ==> BatchNormalization ==> Dropout ==> Conv = => MaxPooling ==> BatchNormalization ==> Dropout ==> Dense (relu) ==> Dropout ==> Dense (softmax).\n\nOverfitting has been removed and performance has been improved to a little bit.\n\nFinally, other parameters of the CNN model have been adjusted to improve model performance.\n","73fc11d3":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"1.1.\"><\/a>\n**1.1. Helper Function:show_model_history(modelHistory, model_name)**","e7dfca03":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"1.2.\"><\/a>\n**1.2. Helper Function:evaluate_conv_model(model, model_name, X, y)**","a54c0a52":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"2.\"><\/a>\n# **2. About the Dataset**","5cfea8fd":"The dataset consists of images with one-handed display of digits 0 to 9 in sign language. The images are 64X64 in size and gray in color. It was obtained by 218 people making 10 different signs once. There should be a total of 2180 samples, while there are 2062 samples in the data set. This is probably because some unfavorable images have been removed by creator of the dataset..","3244a7d3":"Beside,overfitting  and low robustness problems were resolved, the training and validation performance of the model lifted up.\n\nWe can also fine tunne the number of filters in Conv layers. Filters are the feature detectors. Generally fewer filters are used at the input layer and increasingly more filters used at deeper layers.\n\nFilter size is another parameter we can fine tunne it. The filter size should be as small as possible, but large enough to see features in the input data. It is common to use 3x3 on small images and 5x5 or 7x7 and more on larger image sizes.\n\nBatchNormalization is another layer can be used in CNN. Although the BatchNormalization layer prolongs the training time of deep networks, it has a positive effect on the results. Let's add the BatchNormalization layer to Model 4 and see the results.\n","d1d9ec62":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"3.\"><\/a>\n# **3. Convolutional Model 1**","905330c8":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"9.\"><\/a>\n# **9. Convolutional Model 7**","c9f291a5":"Although the validation success accuracy has increased, the problem of overfitting(high variance) of the model still exists. \n\nLet's try adding a new Conv ==> MaxPool ==> Dropout layer.","31586cd9":"[Go to Content Menu](#0.)\n\n<a class=\"anchor\" id=\"12.5.\"><\/a>\n**15.5. Saving and Loading Model**","35cce085":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"4.\"><\/a>\n# **4. Convolutional Model 2**","25466dc6":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"6.\"><\/a>\n# **6. Convolutional Model 4**","c60aa636":"[Go to Content Menu](#0.)\n\n<a class=\"anchor\" id=\"12.7.\"><\/a>\n**15.7. Test YAML Serialization**","8c38de5d":"When the above graphs are examined, it can be seen that the model has a high training accuracy rate and a lower validation accuracy rate. This means that low bias and high varience. In addition, although the zigzags in the validation chart are reduced, they still exist. It can be assessed that the robustness of validation results is still low.\n\nIn view of the above considerations, it is useful to add a new Conv layer or Dropout layer to avoid overfitted the model. First let's add a new Conv layer.","6add64b4":"[Go to Content Menu](#0.)\n\n<a class=\"anchor\" id=\"12.4.\"><\/a>\n**12.4. Serialization Classes**","a4b5f3fb":"[Go to Content Menu](#0.)\n\n<a class=\"anchor\" id=\"12.\"><\/a>\n# 12. Saving and Loading Deep Learning Model with Serialization\n\nTraingin deep learning model can takes hours or weeks. It is important to know how to save trained model to disk and to load saved model from disk. In this section we will see two different types of serialization for saving and loading of Keras models; JSON and YAML.\n\nBefore serizalization, we will see HDF5 format which is used save and load weihgt of trained model. There are two stage to save and load trained model. First, weights of the model saved and loaded to disk. Then, trained model saved and loaded depend on saved and loaded weights. \n\n<a class=\"achor\" id=\"12.1.\"><\/a>\n**12.1. HDF5: Hierarchal Data Format**\n\nHDF5 allows flexible and easy data storage format to save and load large the weights of trained deep learning model. \n\nHDF5 is not only for storing weights of networks, but it is optimized way of storing extracted features and images. It is necessary to use for traing on large dataset like ImageNet.\n\nKeras model has save_weights() function for saving weights to disk and load_weihgts() function for loading from disk. \n\n<a class=\"anchor\" id=\"12.2.\"><\/a>\n**12.2. JSON(JavaScript Object Notation) Format**\n\nJSON is a simple data-interchange file format, which is easy for human to read and write. It also is for machines to parse and generate.   \n\nJSON built on  structures of, a collection of name\/value pairs and an ordered list of values. These are universel data structures which are supporte by all modern programming languages. It makes sense that JSON is interchangeable with programming languages. \n\nKeras provides two handy function for serialization with JSON; to_json(), model_from_json().\n* to_json(): describe any model in JSON format.\n* model_from_json(): allows to load JSON file. \n\n\n<a class=\"achor\" id=\"12.3.\"><\/a>\n**12.3. YAML(YAML Ain't Markup Language) Format**\n\nYAML is an other human friendly data serialization standard for all programming languages. \n\nKeras provides two handy function for serialization with YAML; to_yaml(), model_from_yaml().\n* to_yaml(): describe any model in YAML format.\n* model_from_yaml(): allows to load YAML file. \n\nWe will do both JSON and YAML serialization in class implementation. ","46edd78d":"As we expect BatchNormalization increase the model performans. But there is overfitting problem in the model. To deal with that we will use Dropout layer in Conv blocks. ","1d9afd79":"When the above graphs are examined, it can be seen that the model has a low training accuracy rate and a lower validation accuracy rate. This means that  high bias and high varience, which is too bad for machine learning model. In addition, the zigzags in the validation graph show that the robustness of validation results is very low.\n\nConsidering the above evaluations, it would be useful to add a new Convolution layer to the model. ","2264652b":"Although the validation accuracy rate has increased, the problem of overfitting of the model still exists. We can assume that adding a new Conv layer is not useful. In addition, although the zigzags in the validation chart are reduced, they still exist. It can be assessed that the robusness of validation results is still low.\n\nLet's try using the Dropout layer, one of the solutions to the problem of overfitting in deep networks.","285ff1a2":"Overfitting  and low robustness problems were resolved, but the training and validation performance of the model was very poor. Let's remove the last Conv ==> MaxPool ==> Dropout layer added to Model 4 and try different things.\n\nWe can fine tunne another parameters to improve model performance. It is better to use Dropout layers between full connected layers and perhaps after pooling layers. We can also increase the number of nodes 128 to 256 in full connected layers.","294add38":"Many thanks for your feedbacks and upvotes ^______^.","ef4532fd":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"1.3.\"><\/a>\n**1.3. Helper Function:show_image_classes(image, label, n=10)**","8845b2ae":"<center><font size=\"5\" color=\"red\">Many thanks for feedback and upvote^______^.<\/font><\/center>","93095b85":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"7.\"><\/a>\n# **7. Convolutional Model 5**","fb86fa5c":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"10.\"><\/a>\n# **10. Convolutional Model 8**","c60307b9":"<a class=\"anchor\" id=\"0.\"><\/a>\n**Contents**\n* [1. Summary](#1.)\n* * [1.1. Helper Function:show_model_history(modelHistory, model_name)](#1.1.)\n* * [1.2. Helper Function:evaluate_conv_model(model, model_name, X, y)](#1.2.)\n* * [1.3. Helper Function:show_image_classes(image, label, n=10)](#1.3.)\n* [2. Naive Model](#2.)\n* [3. Convolutional Model 1](#3.)\n* [4. Convolutional Model 2](#4.)\n* [5. Convolutional Model 3](#5.)\n* [6. Convolutional Model 4](#6.)\n* [7. Convolutional Model 5](#7.)\n* [8. Convolutional Model 6](#8.)\n* [9. Convolutional Model 7](#9.)\n* [10. Convolutional Model 8](#10.)\n* [11. Play with Optimizers](#11.)\n* [12. Saving and Loading Deep Learning Model with Serialization](#12.)\n* * [12.1. HDF5: Hierarchal Data Format](#12.1.)\n* * [12.2. JSON(JavaScript Object Notation) Format](#12.2.)\n* * [12.3. YAML(YAML Ain't Markup Language) Format](#12.3.)\n* * [12.4. Serialization classes](#12.4.)\n* * [12.5. Saving and Loading Model](#12.5.)\n* * [15.7. Test JSON Serialization](#12.6.)\n* * [15.7. Test YAML Serialization](#12.7.)","aac0197f":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"8.\"><\/a>\n# **8. Convolutional Model 6**","47e6e76e":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"5.\"><\/a>\n# **5. Convolutional Model 3**","f800a211":"[Go to Content Menu](#0.)\n\n<a class=\"anchor\" id=\"12.6.\"><\/a>\n**15.6. Test JOSN Serialization**","1cbbd421":"[Go To Content Menu](#0.)\n\n<a class=\"anchor\" id=\"11.\"><\/a>\n# **11. Playing with Optimizers**"}}