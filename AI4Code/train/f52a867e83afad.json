{"cell_type":{"f5bd7234":"code","06b92ad3":"code","9d1b7061":"code","03fa9b7e":"code","5d4f43aa":"code","fa68846a":"code","1c8f4b7a":"code","97a3bc86":"code","18d65f9a":"code","1229470e":"code","18226695":"code","36ca95d9":"code","f6d7f4aa":"code","8703f04b":"code","2bd5513f":"markdown","689b57b1":"markdown","b60052d6":"markdown","0a872e1b":"markdown","6e1d8b10":"markdown","4bd82a84":"markdown","30e70dee":"markdown","ed50e65a":"markdown","60c043e4":"markdown","09c3aa00":"markdown","96ab3b8b":"markdown","f893baf3":"markdown","4eebce37":"markdown","2c5906b6":"markdown","ad7e6995":"markdown","6c847acd":"markdown","28d99edd":"markdown","6e4f28aa":"markdown","5d3d53c3":"markdown","47e8977f":"markdown","5b9c18e5":"markdown","b3ff37b8":"markdown","9cb00f87":"markdown","a50fa224":"markdown","41f5f23c":"markdown","f23b0d47":"markdown","28326903":"markdown","8066b6d9":"markdown","39bb0de1":"markdown"},"source":{"f5bd7234":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt","06b92ad3":"# read the data\ndf = pd.read_csv(\"..\/input\/tvradionewspaperadvertising\/Advertising.csv\", usecols=['TV','Sales'])\n\n# see top 5\ndf.head()","9d1b7061":"# function to get \u03b8\n\ndef get_thetaHat(X,y):\n    # first add x0 = 1 to each instance\n    X_b = np.c_[np.ones((200, 1)), X]\n    # using the inv() from linear algebra module\n    theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n    return theta_best","03fa9b7e":"get_thetaHat(df['TV'],df['Sales'])","5d4f43aa":"print(df.head(3), end='\\n\\n')\n\nprint('predicted: ',7.03259355 + 230.1*0.04753664)\nprint('predicted: ',7.03259355 + 33.5*0.04753664)\nprint('predicted: ',7.03259355 + 17.2*0.04753664)\n\n# close enough, we know that it works","fa68846a":"# Lets plot it...\n\nX = df['TV']\ny = df['Sales']\n\ny_predict = 7.03259355 + X*0.04753664\n\nplt.plot(X, y_predict, \"r-\")\nplt.plot(X, y, \"b.\")\nplt.xlabel('TV')\nplt.ylabel('sales')\nplt.show()","1c8f4b7a":"X_b = np.c_[np.ones((200, 1)), X]\ntheta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\ntheta_best_svd\n\n# same result","97a3bc86":"# creating the data\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\nX_b = np.c_[np.ones((100, 1)), X]\n\neta = 0.1 # learning rate\nn_iterations = 1000\nm = 100\ntheta = np.random.randn(2,1) # random initialization\nfor iteration in range(n_iterations):\n    gradients = 2\/m * X_b.T.dot(X_b.dot(theta) - y)\n    theta = theta - eta * gradients\n    \ntheta","18d65f9a":"from sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\nsgd_reg.fit(X, y.ravel())\nsgd_reg.intercept_, sgd_reg.coef_","1229470e":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\nprint(X[0],X_poly[0])","18226695":"lin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\nlin_reg.intercept_, lin_reg.coef_","36ca95d9":"from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=0.4)   # using random alpha value for now\nmodel.fit(X,y)\nprint(model.intercept_,model.coef_)","f6d7f4aa":"from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.4)    # using random alpha value for now\nmodel.fit(X,y)\nprint(model.intercept_,model.coef_)","8703f04b":"from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet(alpha=0.4)    # using random alpha value for now\nmodel.fit(X,y)\nprint(model.intercept_,model.coef_)","2bd5513f":"There are 3 ways to find the $\\hat\\theta$ value:\n- using the normal equation\n- using the scipy.linalg.lstsq() function\n- using gradient decent ( the one which we use )\n\nYou can really skip to gradient descent if you want, i went through it myself so i mentioned it","689b57b1":"### $R^2$ statistics\n\nThe R-squared statistic provides a measure of fit. It takes the form of a proportion\u2014the proportion of variance\nexplained\u2014and so it always takes on a value between 0 and 1. \nIn simple words, it represents how much of our data is being explained by our model. \nFor example,  $R^2$ statistic = 0.75, it says that our model fits 75 % of the total data set.\nSimilarly, if it is 0, it means none of the data points is being explained and a value of 1 represents 100% data explanation.\nMathematically $R^2$ statistic is calculated as :\n\n$$R^2 = \\frac{TSS - RSS}{TSS}$$\n\nwhere,\n\n$RSS = \\sum (y_i - \\hat y)^2$\n\n    - where $\\hat y$ is y predicted\n    \n$TSS = \\sum (y_i - y^-)^2$\n\n    - where $y^- $ is y mean\n    ","b60052d6":"### Elastic Net\n\nAccording to the Hands-on Machine Learning book, elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso\u2019s regularization terms, and you can control the mix ratio \u03b1. \n\n![elasticNet.PNG](attachment:d452313e-4ece-4e64-9658-f229ba68a892.PNG)\nwhere \u03b1 is the mixing parameter between ridge (\u03b1\u2004=\u20040) and lasso (\u03b1\u2004=\u20041).\n","0a872e1b":"### Stochastic Gradient Descent","6e1d8b10":"### Batch gradient","4bd82a84":"sources:\n- Hands-On Machine Learning with Scikit-Learn and TensorFlow\n- ineuron's course material\n- my brain","30e70dee":"### Adjusted $R^2$ statistics\nAs we increase the number of independent variables in our equation, the R2 increases as well. But that doesn\u2019t mean that the new independent variables have any correlation with the output variable. In other words, even with the addition of new features in our model, it is not necessary that our model will yield better results but R2 value will increase. To rectify this problem, we use Adjusted R2 value which penalises excessive use of such features which do not correlate with the output data.\nLet\u2019s understand this with an example:\n\n$$R^2 adjusted = \\frac{(1-R^2)(N-1)}{N-P-1}$$","ed50e65a":"**There is a lot more to cover and i have the idea of showing a sequence of steps to be taken to approach and Linear regression problem but i'm done for today**","60c043e4":"## Polynomial regression\n\nIn polynomial regression we use the concept of quadratic equation, that is we convert our data from \n\n$y = bx + c$\n\ninto the form\n\n$y = ax^2 + bx + c$\n\nthis above example is for 2 degree polynomial when we only have a single independent feature. we are finding the square of the feature and adding it as another column in the data","09c3aa00":"## Regularization","96ab3b8b":"### But how to use the relationship to predict the output?\n\n\n<font size=\"3\">1. In the case of **simple linear regression** we simply draw a line between the independent and dependent feature that represents that relationship and then for each data point on the X axis the corresponding point on the regression line will be our predicted output in the y axis.\n\nmathematically the predictions made by a regression line can be found using the formula,<\/font>\n\n$$y = \\beta_0 + \\beta_1 x$$\n\nwhere,\n\n- $y$ is the response or the target variable ( predicted value )\n- $x$ is the independent feature\n- $\\beta_1$ is the coefficient of x ( slope : a number that describes both the direction and the steepness of the line )\n- $\\beta_0$ is the intercept ( an intercept is a point on the y-axis, through which the slope of the line passes )","f893baf3":"Ofcourse our predictions won't be spot on, we will have some error so, to understand how well our regression line can predict the data we sum up the squares of errors all in our predicted data and take the mean of it, this is called the mean squared error (mse for short). we also use root mean squared error which is simply the the square root of mse","4eebce37":"### 1. Normal equation","2c5906b6":"### Don't forget to leave a like\/upvote if this was worth your time","ad7e6995":"![top_fig.png](attachment:2747ed8f-40fd-46da-a25e-66d4a03cc888.png)","6c847acd":"# Linear Regression\n\nLinear regression is a linear approach to modelling the relationship between a output feature and one or more features which have a linear relationship with the output feature (also known as dependent and independent features).\n ","28d99edd":"### Assumptions","6e4f28aa":"<font size=\"3\">Now, there are 3 types of gradient descents<\/font>\n\n1. Batch gradient descent\n    - In batch gradient descent we use the full training set in each iteration\n2. Stochastic gradient descent\n    - Stochatic gradient descent picks a random instance in the training set at every step and computes the gradients based only on that single instance.\n3. mini batch gradient\n    - Minibatch GD computes the gradients on small random sets of instances called minibatches","5d3d53c3":"so, $\\theta_0$ is 7.0326 and  $\\theta_1$ is 0.04753664\n\nThese values won't be exact beacuse we will have some noise in the data but we can actually use these to predict the values of sales... lets try","47e8977f":"## Thank you","5b9c18e5":"To calculate the value of $\\hat\\theta$ we will use a simple formula called the normal equation which minimizes the cost function\n\n    To find the value of \u03b8 that minimizes the cost function, there is a closed-form solution\n \u2014in other words, a mathematical equation that gives the result directly. This is called\nthe Normal Equation\n\n$$ \\hat\\theta = (X^T X)^T X^T y $$\n\n","b3ff37b8":"## 2.  np.linalg.lstsq()\n\nThe LinearRegression class in scikit-learn is based on the scipy.linalg.lstsq() function (the\nname stands for \u201cleast squares\u201d) so we can use it as well...","9cb00f87":"### LASSO (Least Absolute Shrinkage and Selection Operator) Regression (L1 Form)\nLASSO regression penalizes the model based on the sum of magnitude of the coefficients. The regularization term is given by\n\n regularization=$ \\lambda *\\sum  |\\beta_j| $\n\nWhere, \u03bb is the shrinkage factor.\nand hence the formula for loss after regularization is:\n\n![L1.PNG](attachment:7c3a835b-ec27-434c-aa93-d5c01b7fb839.PNG)\n","a50fa224":"### Ridge Regression (L2 Form)\nRidge regression penalizes the model based on the sum of squares of magnitude of the coefficients. The regularization term is given by\n\n regularization=$ \\lambda *\\sum  |\\beta_j ^ 2| $\n\nWhere, \u03bb is the shrinkage factor.\n\nand hence the formula for loss after regularization is:\n\n![ridge.PNG](attachment:8c01ae19-f886-41ba-9a30-d76261127ebc.PNG)\n\nThis value of lambda can be anything and should be calculated by cross validation as to what suits the model.\n\nLet\u2019s consider $\\beta_1$ and $\\beta_2$ be coefficients of a linear regression and \u03bb = 1:\n\nFor Lasso, $\\beta_1$ + $\\beta_2$ <= s  \n\nFor Ridge, $\\beta_1^2$ + $\\beta_2^2$  <= s  \n\nWhere s is the maximum value the equations can achieve\n.\nIf we plot both the above equations, we get the following graph:\n\n\n![ridge_vs_lasso.PNG](attachment:b23a19cc-c2c6-4686-a805-94106921db7f.PNG)\nThe red ellipse represents the cost function of the model, whereas the square (left side) represents the Lasso regression and the circle (right side) represents the Ridge regression.\n","41f5f23c":"<font size=\"3\">2. In the case of **multiple linear regression** we also draw a line of sorts but when we have more then one feature then our regression line will be represented in higher dimensions (number_of_features + 1) dimension...\n\nmathematically the predictions made by this regression line can be found using the formula:<\/font>\n\n$$y = \\beta_0  + \\beta_1x_1  + \\beta_2x_2 + .....  + \\beta_nx_n$$\n\nwhere,\n\n- $y$ is the response or the target variable ( predicted value )\n- $x_1,x_2,...,x_n$ are the independent features\n- $\\beta_1,\\beta_2,...,\\beta_n $ are the coefficient of each independent feature ( slope : a number that describes both the direction and the steepness of the line )\n- $\\beta_0$ is the intercept ( an intercept is a point on the y-axis, through which the slope of the line passes )\n\n\nThis can be written in a vectorized form as:\n\n$$y = h_\\theta (x) = \u03b8 \u00b7 x$$\n\nwhere,\n\u2022 \u03b8 is the model\u2019s parameter vector, containing the bias term $\u03b8_0$ and the feature weights $\u03b8_1$ to $\u03b8_n.$\n\n\u2022 x is the instance\u2019s feature vector, containing $x_0$ to $x_n$, with $x_0$ always equal to 1.\n\n\u2022 \u03b8 \u00b7 x is the dot product of the vectors \u03b8 and x, which is of course equal to \n\n$ \u03b8_0 x_0 + \u03b8_1 x_1 + \u03b8_2 x2 + \u22ef + \u03b8_n x_n.$\n\n$ h_\u03b8 $ is the hypothesis function, using the model parameters \u03b8 ","f23b0d47":"Let\u2019s see the underlying assumptions: -\n* The regression model is linear in terms of coefficients and error term.\n* The mean of the residuals is zero.\n* The error terms are not correlated with each other, i.e. given an error value; we cannot predict the next error value.\n* The independent variables(x) are uncorrelated with the residual term, also termed as **exogeneity**. This, in layman term, generalises that in no way should the error term be predicted given the value of independent variables.\n* The error terms have a constant variance, i.e. **homoscedasticity**.\n* No Multicollinearity, i.e. no independent variables should be correlated with each other or affect one another. If there is multicollinearity, the precision of prediction by the OLS model decreases.\n* The error terms are normally distributed.\n","28326903":"## 3. Gradient Descent\n\nThe general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.\n\ngradient descent is faster then the normal equation when working on larger number of columns and is used a lot in a lot of algorithms\n\n**Explanation:**\n\nGradient descent uses a cost function (cost simply means error... like MSE), the cost function tells how far are we from the coeffecients of independent features (slopes) and the intercept\n\nThe cost function is a partial derivative of the error function (eg: mse), it can be given for a single feature as:\n\n$$ \\frac{\\partial}{\\partial \\theta_j} MSE(\\theta) = \\frac{2}{m} \\sum \\limits _{i=1} ^{m}  (\\theta^{T}X^{i} - y^{i})X_j^{i} $$\n\nFor multiple linear regression it can be given as:\n$\\delta$\n\n\nwhen the cost function is calculated we multiply it with the learning rate. The learning rate tells the how fast we want to aim at the desired values and move towards them.\n\n\nwe maybe be calculating this value (cost function X learning rate) for a number of features. all the \"learnt\" values after calculation are subtracted with current slope values inorder to get their new slopes... ( when we start we select the slopes randomly and they after each iteration of the above steps converge to the most optimum values )\n\n$$\\theta^{next} = \\theta^{now} - \\ * \\triangledown_\\theta MSE (\\theta)$$\n\n\nGradient descent is visually seen as a type of a bowl the point where the gradient descent is to have gotten the most optimum values of all slopes and the intercept is called the global minima (bottom most point in gradient descent)\n\n![minima2.png](attachment:fe8ed2b5-182a-44dc-bdc2-169413c79d14.png)\n\nBut as the residual's minima is dependent on two variables m and b, it becomes a _Paraboloid_ and the appropriate m and b are calculated using _*Gradient Descent*_ as shown below:\n![GradientDescent.gif](attachment:fb690184-63d5-4654-a872-a82102036b32.gif)\nPhoto:Google","8066b6d9":"regularization is used to reduce overfitting of regression models.\n\nIt is basically the idea that if we add a extra value to the cost function and there by increasing the cost then the model geenrated will not overfit our training data.\n\nthere are 3 types of regularizations:\n1. Ridge regression\n2. Lasso regression\n3. Elastic Net","39bb0de1":"$$ MSE(X,h_\\theta) = \\frac{1}{m} \\sum \\limits _{i=1} ^{m}  (\\theta^{T}X^{i} - y^{i})^2 $$\n\nOur goal is to find $\\hat\\theta$ which is a vector of $\\theta_0,\\theta_1,...\\theta_n$"}}