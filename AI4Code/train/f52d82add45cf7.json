{"cell_type":{"15a63f93":"code","8db1a853":"code","0101c71d":"code","6cc7a9bb":"code","c7638a08":"code","cf617a33":"code","40a56151":"code","33440bb8":"code","2a0703ea":"code","66e60668":"code","5865bab4":"code","a4e20d55":"code","fe305ec1":"code","ec6fa4d8":"code","74aaaaf9":"code","840c6545":"code","ddfeba05":"code","b7d2c5d3":"code","a07173ae":"code","54c3d0fd":"code","f0ad80fc":"code","6fe36eac":"code","db3719a7":"code","1764e184":"code","f8972214":"code","26cf8b8b":"code","784e4628":"code","38c73038":"code","cfc86923":"code","3d5e743d":"code","1a04d3b5":"code","bb10f5cd":"code","2a08f366":"code","b68ed0e2":"code","d87f9e96":"code","3e81b2f8":"code","f871e3cc":"code","f1e73075":"code","c4bf755d":"code","de3a1e9e":"code","957d8ac8":"code","9bd75647":"code","86dbb54b":"code","e5041685":"code","9a1b641e":"code","64e5e9ef":"code","ebc707b4":"code","3f4e95b1":"code","cfc469f4":"code","4c1c4f27":"code","434b86cf":"code","e522c3d8":"code","f9559769":"code","d429c683":"code","bf6610ee":"code","ff7005e3":"code","4e1f30ef":"code","d29de75d":"markdown","5bca9997":"markdown","a43c145f":"markdown","ca56be35":"markdown","54b5f339":"markdown","85ab0c87":"markdown","8f0a4ef6":"markdown","fb19ca38":"markdown","17783fca":"markdown"},"source":{"15a63f93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.externals import joblib\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8db1a853":"calendar_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsell_prices_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsample_sub_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')","0101c71d":"calendar_df","6cc7a9bb":"calendar_df = calendar_df.drop(['weekday'], axis=1)\ncalendar_df = calendar_df.drop(['date'], axis=1)","c7638a08":"sell_prices_df.head(10)","cf617a33":"train_df.head(10)","40a56151":"sample_sub_df.head()","33440bb8":"print(f'Shape of calendar: {calendar_df.shape}')\nprint(f'Shape of sell prices: {sell_prices_df.shape}')\nprint(f'Shape of validation dataset: {train_df.shape}')\nprint(f'Shape of test dataset: {sample_sub_df.shape}')","2a0703ea":"## Function to reduce the memory usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","66e60668":"calendar_df = reduce_mem_usage(calendar_df)\nsell_prices_df = reduce_mem_usage(sell_prices_df)\ngc.collect()","5865bab4":"calendar_df.isna().sum()","a4e20d55":"sell_prices_df.isna().sum()","fe305ec1":"(train_df.isna().sum() == 0).all()","ec6fa4d8":"calendar_df = calendar_df.fillna('None')\ncalendar_df.isna().sum()","74aaaaf9":"calendar_df.memory_usage()","840c6545":"calendar_df.dtypes","ddfeba05":"for feature in ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']:\n    calendar_df[feature] = calendar_df[feature].astype('category')\n    calendar_df[feature] = calendar_df[feature].cat.codes","b7d2c5d3":"calendar_df.memory_usage()","a07173ae":"TOTAL_TRAINING_DAYS = 1969\nTRAINING_DAYS = 1913","54c3d0fd":"day_dict = {}\nfor i in range(TOTAL_TRAINING_DAYS):\n    day_dict[f'd_{i+1}'] = i + 1\ncalendar_df['d'] = calendar_df['d'].map(day_dict).astype(np.int16)\ncalendar_df","f0ad80fc":"calendar_df.dtypes","6fe36eac":"sell_prices_df.memory_usage()","db3719a7":"for feature in ['store_id', 'item_id']:\n    sell_prices_df[feature] = sell_prices_df[feature].astype('category')\n    sell_prices_df[feature] = sell_prices_df[feature].cat.codes","1764e184":"sell_prices_df.memory_usage()","f8972214":"train_df.memory_usage()","26cf8b8b":"train_df.dtypes","784e4628":"for feature in ['store_id', 'item_id', 'cat_id', 'dept_id', 'state_id']:\n    train_df[feature] = train_df[feature].astype('category')\n    train_df[feature] = train_df[feature].cat.codes","38c73038":"train_df.memory_usage()","cfc86923":"%%time\nfull_train_df = train_df.drop([f'd_{i+1}' for i in range(TRAINING_DAYS)], axis=1)\nfull_train_df = pd.concat([full_train_df]*TRAINING_DAYS, ignore_index=True)\nfull_train_df['sales'] = pd.Series(train_df[[f'd_{i+1}' for i in range(TRAINING_DAYS)]].values.ravel('F'))\n\ndays = [i+1 for i in range(TRAINING_DAYS)] * len(train_df)\ndays.sort()\n\nfull_train_df['d'] = pd.Series(days)\nfull_train_df = reduce_mem_usage(full_train_df)","3d5e743d":"TEST_DAYS = 28","1a04d3b5":"%%time\nfull_test_df = train_df.drop([f'd_{i+1}' for i in range(TRAINING_DAYS)], axis=1)\nfull_test_df = pd.concat([full_test_df]*TEST_DAYS, ignore_index=True)\n\ndays = [i for i in range(1914, 1914+TEST_DAYS)] * len(train_df)\ndays.sort()\n\nfull_test_df['d'] = pd.Series(days)\nfull_test_df = reduce_mem_usage(full_test_df)\nfull_test_df","bb10f5cd":"del days\ndel train_df\ngc.collect()","2a08f366":"%%time\nfull_train_df = full_train_df.merge(calendar_df, how='inner', on='d')\nfull_train_df","b68ed0e2":"test_cal_df = calendar_df[(calendar_df['d'] > 1913) & (calendar_df['d'] <= 1941)]\ntest_cal_df","d87f9e96":"%%time\nfull_test_df = full_test_df.merge(test_cal_df, how='inner', on='d')\nfull_test_df","3e81b2f8":"full_train_df = reduce_mem_usage(full_train_df)\nfull_test_df = reduce_mem_usage(full_test_df)\ngc.collect()","f871e3cc":"%%time\nfull_train_df = full_train_df.merge(sell_prices_df, how='inner', on=['store_id', 'item_id', 'wm_yr_wk'])\nfull_train_df","f1e73075":"%%time\nfull_test_df = full_test_df.merge(sell_prices_df, how='inner', on=['store_id', 'item_id', 'wm_yr_wk'])\nfull_test_df","c4bf755d":"full_train_df = reduce_mem_usage(full_train_df)\nfull_test_df = reduce_mem_usage(full_test_df)\ngc.collect()","de3a1e9e":"del calendar_df\ndel sample_sub_df\ndel day_dict\ngc.collect()","957d8ac8":"full_train_df = full_train_df.drop(['wm_yr_wk', 'd', 'id'], axis=1)\ngc.collect()\nfull_train_df.shape","9bd75647":"full_test_df = full_test_df.drop(['wm_yr_wk', 'd', 'id'], axis=1)\nfull_test_df.shape","86dbb54b":"X_train = full_train_df.drop('sales', axis=1)\nY_train = full_train_df['sales']","e5041685":"del full_train_df\ngc.collect()","9a1b641e":"categoricals = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'wday', 'month', 'year', 'event_name_1', \n               'event_name_2', 'event_type_1', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI']","64e5e9ef":"params = {\n      'num_leaves': 555,\n      'min_child_weight': 0.034,\n      'feature_fraction': 0.379,\n      'bagging_fraction': 0.418,\n      'min_data_in_leaf': 106,\n      'objective': 'regression',\n      'max_depth': -1,\n      'learning_rate': 0.007,\n      \"boosting_type\": \"gbdt\",\n      \"bagging_seed\": 11,\n      \"metric\": 'rmse',\n      \"verbosity\": -1,\n      'reg_alpha': 0.3899,\n      'reg_lambda': 0.648,\n      'random_state': 666,\n    }\nfolds = 5\nseed = 666\n\nkf = StratifiedKFold(n_splits=folds, shuffle=False, random_state=seed)\n\nmodels = []\nfor train_index, val_index in kf.split(X_train, Y_train):\n    x_train = X_train.iloc[train_index]\n    x_val = X_train.iloc[val_index]\n    \n    y_train = Y_train.iloc[train_index]\n    y_val = Y_train.iloc[val_index]\n    \n    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(x_val, y_val, categorical_feature=categoricals)\n    \n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=500,\n                valid_sets=(lgb_train, lgb_eval),\n                early_stopping_rounds=100,\n                verbose_eval = 100)\n    \n    models.append(gbm)","ebc707b4":"# save model\n# joblib.dump(models, 'models.pkl')\n# load model\n# models = joblib.load('\/kaggle\/input\/m5models\/models.pkl')","3f4e95b1":"preds = sum([model.predict(full_test_df) for model in tqdm(models)])\/folds","cfc469f4":"full_test_df['sales'] = preds\nfull_test_df","4c1c4f27":"full_test_df['item_id']","434b86cf":"sample_sub_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\nsample_sub_df","e522c3d8":"daywise_preds = {}\nfor day in range(TEST_DAYS):\n    day_str = f'F{day+1}'\n    for i in range(day, len(full_test_df), 28):\n        if day_str in daywise_preds:\n            daywise_preds[day_str].append(preds[i])\n        else:\n            daywise_preds[day_str] = [preds[i]]\n            \nzeros = [0 for _ in range(30490)]\nfor k, v in daywise_preds.items():\n    daywise_preds[k] = v + zeros","f9559769":"daywise_preds = pd.DataFrame.from_dict(daywise_preds)","d429c683":"daywise_preds","bf6610ee":"daywise_preds['id'] = sample_sub_df['id']\ncols = daywise_preds.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ndaywise_preds = daywise_preds[cols]","ff7005e3":"daywise_preds.to_csv('submission.csv', index=False)\ndaywise_preds","4e1f30ef":"from IPython.display import FileLink\nFileLink('submission.csv')","d29de75d":"Let's convert the categorical variables to `category` datatype to reduce memory usage.","5bca9997":"## Let's analyse Missing values","a43c145f":"`Weekday` is redundent with `wday`. So Let's drop it. Also we have date feature expanded. So let's also drop the `date` feature.","ca56be35":"Since the missing values are for the days when there was no events, we will fill it by string 'None'. Here 'None' will serve as a category instead of missing value.","54b5f339":"Also let's map `d` to integer type to reduce some more memory usage.","85ab0c87":"### Calendar.csv: Contains information about the dates the products are sold.\n* **  date**: The date in a \u201cy-m-d\u201d format.\n* **  wm_yr_wk**: The id of the week the date belongs to.\n* ** weekday**: The type of the day (Saturday, Sunday, \u2026, Friday).\n* ** wday**: The id of the weekday, starting from Saturday.\n* ** month**: The month of the date.\n* ** year**: The year of the date.\n* ** event_name_1**: If the date includes an event, the name of this event.\n* ** event_type_1**: If the date includes an event, the type of this event.\n* ** event_name_2**: If the date includes a second event, the name of this event.\n* ** event_type_2**: If the date includes a second event, the type of this event.\n* ** snap_CA, snap_TX, and snap_WI**: A binary variable (0 or 1) indicating whether the stores of CA,\nTX or WI allow SNAP2 purchases on the examined date. 1 indicates that SNAP purchases are\nallowed.","8f0a4ef6":"### sell_prices.csv: Contains information about the price of the products sold per store and date.\n\nstore_id: The id of the store where the product is sold.\n* ** item_id**: The id of the product.\n* ** wm_yr_wk**: The id of the week.\n* ** sell_price**: The price of the product for the given week\/store. The price is provided per week\n(average across seven days). If not available, this means that the product was not sold during the\nexamined week. Note that although prices are constant at weekly basis, they may change through\ntime (both training and test set). ","fb19ca38":"### sales_train.csv: Contains the historical daily unit sales data per product and store.\n* ** item_id**: The id of the product.\n* ** dept_id**: The id of the department the product belongs to.\n* ** cat_id**: The id of the category the product belongs to.\n* ** store_id**: The id of the store where the product is sold.\n* ** state_id**: The State where the store is located.\n* ** d_1, d_2, \u2026, d_i, \u2026 d_1941**: The number of units sold at day i, starting from 2011-01-29.","17783fca":"## Data Description"}}