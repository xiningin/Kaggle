{"cell_type":{"97457787":"code","75c34f68":"code","4926709d":"code","7dd72134":"code","1ff66b69":"code","1bfdd453":"code","bb854081":"code","191bc270":"code","3b575db6":"code","32ad0765":"code","4ff36818":"code","954b271e":"code","9fa05e67":"code","574ba74e":"code","5f7ab8e1":"code","bafb0475":"code","5862c7da":"code","dd73e7b5":"code","94165d33":"code","4de627f7":"code","b3fded9b":"code","5bd3a2a0":"code","b855db87":"code","cb40191f":"markdown","421b5fb1":"markdown","bde0a403":"markdown","dd1e597e":"markdown","eda22acb":"markdown","cb5ef1ff":"markdown","bab61028":"markdown","479117d5":"markdown","810b3727":"markdown","193074af":"markdown","1f768b79":"markdown","750a1f01":"markdown","d8e3e0dd":"markdown","9ea64b1b":"markdown","55191767":"markdown"},"source":{"97457787":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report \nimport seaborn as sns","75c34f68":"df = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='latin-1')\ndf.head()","4926709d":"df.info()","7dd72134":"df.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"],axis=1,inplace=True)","1ff66b69":"df.head()","1bfdd453":"df.rename(columns={\"v1\":\"Label\",\"v2\":\"Message\"},inplace=True)","bb854081":"df.columns","191bc270":"df.Label.value_counts()","3b575db6":"sns.countplot(x=df.Label)","32ad0765":"df[\"Label\"]=df.Label.map({\"ham\":0,\"spam\":1})","4ff36818":"df.head()","954b271e":"#defining indipendent and dependent variables.\nX=df[\"Message\"]\ny=df[\"Label\"]","9fa05e67":"Count_vec=CountVectorizer()\nX=Count_vec.fit_transform(X) #fit and transform the data","574ba74e":"#train test split\nX_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.33,random_state=42)","5f7ab8e1":"#lets fit our naive bayes classifier\nNB=MultinomialNB()\nNB.fit(X_train,y_train)\nNB.score(X_test,y_test)\ny_pred=NB.predict(X_test)\nprint(classification_report(y_test,y_pred))","bafb0475":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test,y_pred)","5862c7da":"from sklearn.linear_model import LogisticRegression\nLR=LogisticRegression()\nLR.fit(X_train,y_train)\nLR.score(X_test,y_test)\ny_pred_LR=LR.predict(X_test)\nprint(classification_report(y_test,y_pred_LR))\nconfusion_matrix(y_test,y_pred_LR)","dd73e7b5":"from sklearn.neighbors import KNeighborsClassifier\nKnn=KNeighborsClassifier(n_neighbors=1)\nKnn.fit(X_train,y_train)\nKnn.score(X_test,y_test)\ny_pred_knn=Knn.predict(X_test)\nprint(classification_report(y_test,y_pred_knn))\nconfusion_matrix(y_test,y_pred_knn)","94165d33":"from sklearn.ensemble import RandomForestClassifier\nRf=RandomForestClassifier(n_jobs=-1)\nRf.fit(X_train,y_train)\nRf.score(X_test,y_test)\ny_pred_Rf=Rf.predict(X_test)\nprint(classification_report(y_test,y_pred_Rf))\nconfusion_matrix(y_test,y_pred_Rf)","4de627f7":"from sklearn.ensemble import AdaBoostClassifier\nAda=AdaBoostClassifier()\nAda.fit(X_train,y_train)\nAda.score(X_test,y_test)\ny_pred_Ada=Ada.predict(X_test)\nprint(classification_report(y_test,y_pred_Ada))\nconfusion_matrix(y_test,y_pred_Ada)","b3fded9b":"import matplotlib.pyplot as plt\nfrom sklearn.calibration import calibration_curve\n# Plot calibration plots\n\nplt.figure(figsize=(10, 10))\nax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\nax2 = plt.subplot2grid((3, 1), (2, 0))\n\nax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\nfor clf, name in [(LR, 'Logistic'),\n                  (NB, 'Naive Bayes'),\n                  (Ada, 'Adaboost'),\n                  (Rf, 'Random Forest'),\n                  (Knn,\"K Neighbours\")]:\n    clf.fit(X_train, y_train)\n    if hasattr(clf, \"predict_proba\"):\n        prob_pos = clf.predict_proba(X_test)[:, 1]\n    else:  # use decision function\n        prob_pos = clf.decision_function(X_test)\n        prob_pos = \\\n            (prob_pos - prob_pos.min()) \/ (prob_pos.max() - prob_pos.min())\n    fraction_of_positives, mean_predicted_value = \\\n        calibration_curve(y_test, prob_pos, n_bins=10)\n\n    ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n             label=\"%s\" % (name, ))\n\n    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n             histtype=\"step\", lw=2)\n\nax1.set_ylabel(\"Fraction of positives\")\nax1.set_ylim([-0.05, 1.05])\nax1.legend(loc=\"lower right\")\nax1.set_title('Calibration plots  (reliability curve)')\n\nax2.set_xlabel(\"Mean predicted value\")\nax2.set_ylabel(\"Count\")\nax2.legend(loc=\"upper center\", ncol=2)\n\nplt.tight_layout()\nplt.show()","5bd3a2a0":"#from sklearn.externals import joblib\n#joblib.dump(NB,\"Spam_detection_proj.pkl\")","b855db87":"#Spam_detection_proj=open(\"Spam_detection_proj.pkl\",\"rb\")\n#NB=joblib.load(Spam_detection_proj)","cb40191f":"**Adaboost:**","421b5fb1":"**Logistic Regression**","bde0a403":"**Why There is need of Spam Filter:**\n\n**Business Problem:**\n\nWhen you have a business, getting rid of spam is all the more important due to the fact that these can eat up a lot of your inbox space, as well as a lot of your time when you start clearing these out. These emails can also carry malware and viruses that can compromise company security and data. What can you do to stop these from inundating your work email, and by extension, to stop these from compromising your company\u2019s security? You can use spam filters.\n\nSpam filtering is an important tool that your company should use to help keep these unwanted messages from entering your inboxes, and to keep people from clicking on potentially harmful emails. According to studies, more than half of the emails that you get are actually classified as junk or spam. This fact alone shows you that there is a large potential for security issues due to these messages, not to mention the drop in productivity because of the time people will spend on deleting such emails from their inbox.","dd1e597e":"We can see that the Naive Bayes has best at classfiying the Spam and Ham mmessages in the mail .we have also used many other ensembe and classification models but none was better then the Multinomial Naive Bayes.","eda22acb":"**Ensemble Classifier:** **Random Forest**","cb5ef1ff":"**Lets use our machine learning skills and solve the business Problem:**","bab61028":"The above process called \u201cpersist model in a standard format\u201d, that is, models are persisted in a certain format specific to the language in development.","479117d5":"**Deployment: Work In progress**","810b3727":"And we can load and use saved model later like so:","193074af":"**Exploratory Data Analysis**","1f768b79":"Lets try some some other models","750a1f01":"**Modelling**","d8e3e0dd":"And the model will be served in a micro-service that expose endpoints to receive requests from client. This is the next step.","9ea64b1b":"**K Neighbours Classifier**","55191767":"After training the model, it is desirable to have a way to persist the model for future use without having to retrain. To achieve this, we add the following lines to save our model as a .pkl file for the later use."}}