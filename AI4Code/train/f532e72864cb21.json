{"cell_type":{"06e06085":"code","5ac681f2":"code","66d83c1f":"code","6b53722b":"code","1dc7f7b8":"code","d1b3576d":"code","6c369abc":"code","f00f1252":"code","bdb874fa":"code","6780c9e3":"code","ecae393b":"code","425818c5":"code","74fcd7e0":"markdown","ffdc7367":"markdown","f5300abe":"markdown","6d7df8ab":"markdown","67ac0295":"markdown","cb2f1bc1":"markdown","f4ba66ac":"markdown","4b4a351a":"markdown"},"source":{"06e06085":"import numpy as np\nimport pandas as pd\nimport os\nimport collections\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer, WordNetLemmatizer\nimport time\nimport spacy\nimport string","5ac681f2":"metaDataPath = \"\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv\"\nmetaData = pd.read_csv(metaDataPath, header = 0, index_col = 0)\nprint(\"The number of literatures: \" + str(metaData.shape[0]))\nmetaData.head()","66d83c1f":"metaData.isnull().sum()","6b53722b":"sourceDic = collections.defaultdict(int)\nfor s in metaData[\"source_x\"][metaData[\"source_x\"].notnull()]:\n    sourceDic[s] += 1\nsizes = []\nexplode = []\nlabels = []\nfor s in sourceDic:\n    sizes.append(sourceDic[s])\n    explode.append(0)\n    labels.append(s)","1dc7f7b8":"colors = ['gold', 'lightskyblue', 'yellowgreen', 'lightcoral']\nplt.pie(sizes, explode=explode, labels=labels, colors = colors, autopct='%1.1f%%', shadow=True, startangle=140)\nplt.axis('equal')\nplt.show()","d1b3576d":"yearList = []\nfor y in metaData[\"publish_time\"][metaData[\"publish_time\"].notnull()]:\n    yearList.append(int(re.split(' |-', y)[0]))","6c369abc":"sns.distplot(yearList, bins = 50)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Frequency\")","f00f1252":"hasFullText = metaData[\"has_full_text\"][metaData[\"has_full_text\"].notnull()]\nnanCount = metaData.shape[0] - hasFullText.shape[0]\ntrueCount = sum(hasFullText)\nfalseCount = hasFullText.shape[0] - trueCount\nprint(\"The number of literatures with full text: \" + str(trueCount))\nprint(\"The number of literatures without full text: \" + str(falseCount + nanCount))","bdb874fa":"print(\"The number of literatures with abstract: \" + str(sum(metaData[\"abstract\"].notnull())))","6780c9e3":"startTime = time.time()\nabsLength = []\nword2count = {}\nfor abstract in metaData[\"abstract\"][metaData[\"abstract\"].notnull()]:\n    ## Remove web links\n    abstract = re.sub('https?:\/\/\\S+|www\\.\\S+', '', abstract) \n\n    ## Lowercase\n    abstract = abstract.lower()\n    \n    ## Remove punctuation\n    abstract = re.sub('<.*?>+', ' ', abstract)\n    abstract = re.sub('[%s]' % re.escape(string.punctuation), ' ', abstract)\n    \n    ## Tokenize\n    words = word_tokenize(abstract)\n    \n    ## Remove stop words\n    nltk_stop_words = stopwords.words('english')\n    words = [word for word in words if word not in nltk_stop_words]\n    \n    ## Stem\n    stemmer = SnowballStemmer('english')\n    words = [stemmer.stem(word) for word in words]\n    \n    ## Lematize verbs\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n    \n    ## Record length\n    absLength.append(len(words))\n    \n    ## Get word count\n    for word in words:\n        count = word2count.get(word, 0)\n        word2count[word] = count + 1\nprint(\"Time spent: \" + str(round((time.time() - startTime) \/ 60, 3)) + \"min.\")","ecae393b":"sns.distplot(sorted(absLength)[:-20], bins = 50) # There are 20 extremely long abstracts\nplt.xlabel(\"Abstract token count\")\nplt.ylabel(\"Frequency\")\nplt.show()","425818c5":"df_word_count = pd.DataFrame(sorted(word2count.items(), key=lambda x: x[1])[::-1])\nsns.set(rc={'figure.figsize':(12,10)})\nsns.barplot(y = df_word_count[0].values[:50], x = df_word_count[1].values[:50], color='red')","74fcd7e0":"# Has full text distribution","ffdc7367":"There are 20 extremely long abstracts. Excluding them from the following histogram.","f5300abe":"# Literature source distribution","6d7df8ab":"# Nan elements counts in all columns","67ac0295":"# Read-in metadata","cb2f1bc1":"# Abstract distribution","f4ba66ac":"Top 50 mostly frequent tokens","4b4a351a":"# Publish year distribution"}}