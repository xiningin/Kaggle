{"cell_type":{"2f3b5b78":"code","7445dcd0":"code","88f9ad58":"code","e4e9cb21":"code","0d4d1633":"code","05574823":"code","ff4881b2":"code","5b3ec017":"code","cd999fa5":"code","0aa026e9":"code","e772d41e":"code","f2c4c872":"code","b5221127":"code","3a43c924":"code","4b533f6a":"code","7f10b1f3":"code","677ce50c":"code","678dc87b":"code","02799580":"code","cba2e459":"code","5f11d4e6":"code","12f662e8":"code","a7d91036":"code","c5df0d97":"code","85ad1112":"code","12277dae":"code","801acd01":"code","ac98f18e":"code","3485b56a":"code","a302f921":"code","28ab25e0":"code","7dd4a474":"code","69991ef1":"code","81427b31":"code","9ba39920":"code","d040bb48":"code","e06c7605":"code","98fa17d0":"code","a6e4f7c2":"code","1f143f71":"code","956a2a92":"code","8a754e5c":"code","a2f6c0b6":"code","a1c07170":"code","a7b90d18":"code","20d3eed9":"code","db2f3eb0":"code","0ffe3a98":"code","453e7a20":"code","8cefb541":"code","e24f6fce":"code","acc0e13b":"code","5f61e58f":"code","bef3036c":"code","3d6f986f":"code","74333275":"code","c1ff7778":"code","8171243b":"code","51911a28":"code","4f0bb7dd":"code","5ad0a3d3":"code","3f1a33d9":"code","32ff9fe8":"markdown","3c12231e":"markdown","5be21af2":"markdown","cc98eb50":"markdown","0176f977":"markdown","41281c57":"markdown","60241fc8":"markdown","11dfdb95":"markdown","5f9cebd3":"markdown","50822f23":"markdown","6f1d9880":"markdown","30161632":"markdown","ef2be899":"markdown","404287ea":"markdown","768030bd":"markdown","36b10486":"markdown","69e69250":"markdown","b1118cc8":"markdown","0c0b9cfd":"markdown","f830fe3a":"markdown","8ac4061b":"markdown","e6255462":"markdown","c54b98c4":"markdown","3e8483d6":"markdown","807f56d9":"markdown","d6cff966":"markdown","c5a50c50":"markdown","560aeec9":"markdown","1892e4e3":"markdown","af01b7e9":"markdown","a6e65376":"markdown","822a313d":"markdown","0cb48454":"markdown","b6bd6858":"markdown","7d11e05a":"markdown","97486da4":"markdown","575dda92":"markdown","8249c635":"markdown","f5a49143":"markdown","6b6ba5cc":"markdown","ac1d863c":"markdown","59f333aa":"markdown","36d189a2":"markdown","3a03a274":"markdown","4c72b7bb":"markdown","1a6be980":"markdown","b83f64ff":"markdown","8ccdc248":"markdown","5e692cef":"markdown","04cbc056":"markdown","fb08d8d2":"markdown","4b0e3db4":"markdown","855aeaef":"markdown","d074168e":"markdown","e9e11355":"markdown","9c5adf6f":"markdown","7cae1a1b":"markdown","0a27c27e":"markdown","3eca2073":"markdown","cae30ba6":"markdown","86bf2b32":"markdown","1efde589":"markdown","2218a559":"markdown","f3629d4c":"markdown","9e11b63c":"markdown","3d201927":"markdown","e4be2d53":"markdown","87359402":"markdown","853b903f":"markdown","de5224d1":"markdown","3617a2fa":"markdown","2657bba8":"markdown","50eb8bff":"markdown","f9166934":"markdown","ad88b042":"markdown","d9283071":"markdown","e9e39e84":"markdown","0eab53bb":"markdown","5e447774":"markdown","0f046445":"markdown","65547ca8":"markdown","e092d93b":"markdown","bd2c1bf1":"markdown","03887661":"markdown","7c339c31":"markdown"},"source":{"2f3b5b78":"import numpy as np \nimport pandas as pd \nimport os\nimport pickle\n# I took the original data set and used riot's api to get more data about the games.\n# The API has a request limit of 100 requests per 2 min, so I just have a pickle after I made a request for every game\ndf = pd.read_pickle('\/kaggle\/input\/riotapi-pickles\/riotapi_lower_res')\n\npreproc_df = df.copy()\n\ndf.head()","7445dcd0":"df.columns","88f9ad58":"more_games_df = pd.read_pickle('\/kaggle\/input\/riotapi-pickles\/more_games_lower')\n\nmore_games_df.head(10)","e4e9cb21":"# I'm dumb and basically had duplicate columns of these from the original data set and then from using the riot api\ndf.drop(['blue_firstBlood', 'red_firstBlood'], axis=1, inplace=True)\n\ndf['redWins'] = df['blueWins'].apply(lambda x: 1 if x == 0 else 0)\npreproc_df['redWins'] = df['redWins']\n\nmore_games_df.dropna(inplace=True)\nmore_games_df.reset_index(inplace=True)\nmore_games_df.rename(columns={\"redfirstBlood\": 'redFirstBlood', 'bluefirstBlood': 'blueFirstBlood'}, inplace=True)\nmore_games_df['redWins'] = more_games_df['blueWins'].apply(\n    lambda x: 1 if x == 0 else 0\n)\n\n# Copies the order of the original data set (I have obsessive compulsions regarding to order)\nmore_games_df = more_games_df[df.columns.tolist()]\n\n# Combining the two temporarily for part of the preprocessing\ncombined_df = pd.concat([preproc_df, more_games_df], axis=0, ignore_index=True)\ncombined_df.drop(['blue_firstBlood', 'red_firstBlood'], axis=1, inplace=True)\ncombined_df.head()","0d4d1633":"blue_win = df[df['blueWins'] == 1]\nred_win = df[df['blueWins'] == 0]","05574823":"import plotly.graph_objs as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly.offline import init_notebook_mode, iplot, plot\nfrom plotly.subplots import make_subplots\ninit_notebook_mode(connected=True)","ff4881b2":"fig = go.Figure()\n\nblue_loss = df[df['blueWins'] == 0]\n\nfig.add_trace(go.Bar(x=[0], y=list(blue_win['blueWins'].value_counts()), name='Blue', marker_color='#084177', width=0.5))\nfig.add_trace(go.Bar(x=[1], y=list(blue_loss['blueWins'].value_counts()), name='Red',\n                     marker_color=['#d63447'], width=0.5))\n\nfig.update_layout(\n    xaxis=dict(\n        showticklabels=True,\n        tickvals=[0, 1],\n        ticktext=[i for i in ['Blue', 'Red']],\n    ),\n    yaxis_title='Wins',\n    title='Wins From Each Team',\n    height=800,\n    width=800\n)\n\niplot(fig)","5b3ec017":"fig = go.Figure(data=[\n    go.Box(name='Blue Win', y=blue_win['blueWardsPlaced'], boxmean=True),\n    go.Box(name='Blue Loss', y=red_win['blueWardsPlaced'], boxmean=True),\n    go.Box(name='Red Win', y=red_win['redWardsPlaced'], boxmean=True),\n    go.Box(name='Red Loss', y=blue_win['redWardsPlaced'], boxmean=True)\n])\n\nfig.update_layout(\n    title='Wards Placed Distribution',\n    height=800,\n    width=800\n)\n\niplot(fig)","cd999fa5":"fig = go.Figure(data=[\n    go.Histogram(name='Blue Win', x=blue_win['blueWardsDestroyed']),\n    go.Histogram(name='Blue Loss', x=red_win['blueWardsDestroyed']),\n    go.Histogram(name='Red Win', x=red_win['redWardsDestroyed']),\n    go.Histogram(name='Red Loss', x=blue_win['redWardsDestroyed'])\n])\n\nfig.update_layout(\n    title='Wards Destroyed Distribution',\n    height=800,\n    width=800\n)\n\niplot(fig)","0aa026e9":"fig = go.Figure(data=[\n    go.Bar(name='Blue Win', x=[0], y=[np.sum(blue_win['blueFirstBlood'])], width=0.5),\n    go.Bar(name='Blue Loss', x=[1], y=[np.sum(red_win['blueFirstBlood'])], width=0.5),\n    go.Bar(name='Red Win', x=[2], y=[np.sum(red_win['redFirstBlood'])], width=0.5),\n    go.Bar(name='Red Loss', x=[3], y=[np.sum(blue_win['redFirstBlood'])], width=0.5)\n])\n\nfig.update_layout(\n    title='The Importance of First Kills',\n    height=800,\n    width=800,\n    xaxis=dict(\n        tickvals=[i for i in range(4)],\n        ticktext=[i for i in ['Blue Win', 'Blue Loss', 'Red Win', 'Red Loss']],\n        showticklabels=True\n    ),\n)\n\niplot(fig)","e772d41e":"fig = go.Figure(data=[\n    go.Histogram(name='Blue Win', x=blue_win['blueKills']),\n    go.Histogram(name='Blue Loss', x=red_win['blueKills']),\n    go.Histogram(name='Red Win', x=red_win['redKills']),\n    go.Histogram(name='Red Loss', x=blue_win['redKills'])\n])\n\nfig.update_layout(\n    title='Distribution of Team Kills when Winning and Losing',\n    height=800,\n    width=800,\n)\n\niplot(fig)","f2c4c872":"fig = go.Figure(data=[\n    go.Bar(name='Blue Win', x=[0], y=[np.mean(blue_win['blueKills'])], width=0.5),\n    go.Bar(name='Blue Loss', x=[1], y=[np.mean(red_win['blueKills'])], width=0.5),\n    go.Bar(name='Red Win', x=[2], y=[np.mean(red_win['redKills'])], width=0.5),\n    go.Bar(name='Red Loss', x=[3], y=[np.mean(blue_win['redKills'])], width=0.5)\n])\n\nfig.update_layout(\n    title='Average Kills of Teams when Winning and Losing',\n    height=800,\n    width=800,\n    xaxis=dict(\n        tickvals=[i for i in range(4)],\n        ticktext=[i for i in ['Blue Win', 'Blue Loss', 'Red Win', 'Red Loss']],\n        showticklabels=False,\n        title='Team'\n    ),\n)\n\niplot(fig)","b5221127":"fig = go.Figure(data=[\n    go.Bar(name='Blue Win', x=[0], y=[np.mean(blue_win['blueDeaths'])], width=0.5),\n    go.Bar(name='Blue Loss', x=[1], y=[np.mean(red_win['blueDeaths'])], width=0.5),\n    go.Bar(name='Red Win', x=[2], y=[np.mean(red_win['redDeaths'])], width=0.5),\n    go.Bar(name='Red Loss', x=[3], y=[np.mean(blue_win['redDeaths'])], width=0.5)\n])\n\nfig.update_layout(\n    title='Average Deaths of Teams when Winning and Losing',\n    height=800,\n    width=800,\n    xaxis=dict(\n        tickvals=[i for i in range(4)],\n        ticktext=[i for i in ['Blue Win', 'Blue Loss', 'Red Win', 'Red Loss']],\n        showticklabels=False,\n        title='Team'\n    ),\n)\n\niplot(fig)","3a43c924":"fig = go.Figure(data=[\n    go.Bar(name='Blue Win', x=[0], y=[np.mean(blue_win['blueAssists'])], width=0.5),\n    go.Bar(name='Blue Loss', x=[1], y=[np.mean(red_win['blueAssists'])], width=0.5),\n    go.Bar(name='Red Win', x=[2], y=[np.mean(red_win['redAssists'])], width=0.5),\n    go.Bar(name='Red Loss', x=[3], y=[np.mean(blue_win['redAssists'])], width=0.5)\n])\n\nfig.update_layout(\n    title='Average Assists of Teams when Winning and Losing',\n    height=800,\n    width=800,\n    xaxis=dict(\n        tickvals=[i for i in range(4)],\n        ticktext=[i for i in ['Blue Win', 'Blue Loss', 'Red Win', 'Red Loss']],\n        showticklabels=True\n    ),\n)\n\niplot(fig)","4b533f6a":"fig = go.Figure(data=[\n    go.Histogram(name='Blue Win', x=blue_win['blueTowersDestroyed']),\n    go.Histogram(name='Blue Loss', x=red_win['blueTowersDestroyed']),\n    go.Histogram(name='Red Win', x=red_win['redTowersDestroyed']),\n    go.Histogram(name='Red Loss', x=blue_win['redTowersDestroyed'])\n])\n\nfig.update_layout(\n    title='Distribution of Towers Destroyed when Winning and Losing',\n    height=800,\n    width=800,\n)\n\niplot(fig)","7f10b1f3":"fig = go.Figure(data=[\n    go.Bar(name='Blue Win', y=[0], x=[np.sum(blue_win['blueEliteMonsters'])], width=0.5, orientation='h'),\n    go.Bar(name='Blue Loss', y=[1], x=[np.sum(red_win['blueEliteMonsters'])], width=0.5, orientation='h'),\n    go.Bar(name='Red Win', y=[2], x=[np.sum(red_win['redEliteMonsters'])], width=0.5, orientation='h'),\n    go.Bar(name='Red Loss', y=[3], x=[np.sum(blue_win['redEliteMonsters'])], width=0.5, orientation='h')\n])\n\nfig.update_layout(\n    title='Epic Monsters Killed',\n    height=800,\n    width=800,\n    yaxis=dict(\n        tickvals=[i for i in range(4)],\n        ticktext=[i for i in ['Blue Win', 'Blue Loss', 'Red Win', 'Red Loss']],\n        showticklabels=True\n    ),\n)\n\niplot(fig)","677ce50c":"fig = go.Figure(data=[\n    go.Box(name='Blue Win', x=blue_win['blueTotalExperience']),\n    go.Box(name='Blue Loss', x=red_win['blueTotalExperience']),\n    go.Box(name='Red Win', x=red_win['redTotalExperience']),\n    go.Box(name='Red Loss', x=blue_win['redTotalExperience'])\n])\n\nfig.update_layout(\n    title='Total Experience Distrubtion of Champions',\n    height=800,\n    width=800,\n)\n\niplot(fig)","678dc87b":"fig = go.Figure(data=[\n    go.Violin(name='Blue Win', y=blue_win['blueTotalGold'], meanline_visible=True),\n    go.Violin(name='Blue Loss', y=red_win['blueTotalGold'], meanline_visible=True),\n    go.Violin(name='Red Win', y=red_win['redTotalGold'], meanline_visible=True),\n    go.Violin(name='Red Loss', y=blue_win['redTotalGold'], meanline_visible=True),\n])\n\nfig.update_layout(\n    title='Gold on Winning and Losing Teams',\n    height=800,\n    width=800,\n)\n\niplot(fig)","02799580":"fig = go.Figure(data=[\n    go.Box(name='Blue Win', x=blue_win['blueCSPerMin']),\n    go.Box(name='Blue Loss', x=red_win['blueCSPerMin']),\n    go.Box(name='Red Win', x=red_win['redCSPerMin']),\n    go.Box(name='Red Loss', x=blue_win['redCSPerMin'])\n])\n\nfig.update_layout(\n    title='CS Per Min Distribution',\n    height=800,\n    width=800,\n)\n\niplot(fig)","cba2e459":"def get_champions(ids):\n    if not os.path.isfile('\/kaggle\/input\/riotapi-pickles\/champions_lower'):\n        r = requests.get('http:\/\/ddragon.leagueoflegends.com\/cdn\/10.10.3216176\/data\/en_US\/champion.json')\n\n        response = r.json()\n\n        champions_reformatted = {}\n\n        for champion in response['data']:\n            id = response['data'][champion]['key']\n\n            champions_reformatted[int(id)] = 'Wukong' if champion == 'MonkeyKing' else champion\n\n        with open('\/kaggle\/input\/riotapi-pickles\/champions_lower', 'wb') as file:\n            pickle.dump(champions_reformatted, file)\n    else:\n        with open('\/kaggle\/input\/riotapi-pickles\/champions_lower', 'rb') as file:\n            champions_reformatted = pickle.load(file)\n\n    champions = []\n\n    for id in ids:\n\n        champions.append('None' if id == -1 else champions_reformatted[id])\n\n    return champions[0] if len(champions) == 1 else champions\n\ndef format_champs(df, cols, head):\n    champ_dict = {}\n\n    for col in cols:\n        champs = df[col].value_counts()\n\n        champ_names = get_champions(champs.keys())\n        freq = [i for i in champs]\n        counter = 0\n        for name in champ_names:\n\n            champ_dict[name] = champ_dict.setdefault(name, 0) + freq[counter]\n\n            counter += 1\n\n    champ_dict = {k: v for k, v in sorted(champ_dict.items(), key=lambda item: item[1], reverse=True)}\n\n    top_n = dict(list(champ_dict.items())[0:head])\n\n    other = dict(list(champ_dict.items())[head+1:])\n\n    other_total = 0\n    top_n_total = 0\n\n    for key, val in top_n.items():\n        top_n_total += val\n\n    for key, val in other.items():\n        other_total += val\n\n    other_total = np.abs(top_n_total-other_total)\n\n    top_n['Other'] = other_total\n\n    return top_n","5f11d4e6":"bans = ['ban_1', 'ban_2', 'ban_3', 'ban_4', 'ban_5',\n        'ban_6', 'ban_7', 'ban_8', 'ban_9', 'ban_10']\nfigs = []\nfor ban in bans:\n    fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'domain'}, {'type': 'domain'}]],\n                        subplot_titles=('Blue {} {}'.format(ban[0:3], ban[4:]),\n                                        'Red {} {}'.format(ban[0:3], ban[4:]))\n    )\n\n    row = 1\n\n    blue_win_all_ban = blue_win[ban].value_counts()\n    red_win_ban_all_ban = red_win[ban].value_counts()\n\n    blue_win_top_10 = blue_win[ban].value_counts().head(20)\n    red_win_top_10 = red_win[ban].value_counts().head(20)\n\n    blue_win_other = np.abs(np.sum(blue_win_top_10) - np.sum(blue_win_all_ban))\n    red_win_other = np.abs(np.sum(red_win_top_10) - np.sum(red_win_ban_all_ban))\n\n    blue_vals = blue_win_top_10.values\n    red_vals = red_win_top_10.values\n\n    blue_vals = np.append(blue_vals, blue_win_other)\n    red_vals = np.append(red_vals, red_win_other)\n\n    blue_bans = get_champions(list(blue_win_top_10.keys()))\n    red_bans = get_champions(list(red_win_top_10.keys()))\n\n    fig.add_trace(go.Pie(\n        name=ban,\n        labels=blue_bans + ['Other'],\n        values=blue_vals),\n        row=1,\n        col=1\n    )\n\n    fig.add_trace(go.Pie(\n        name=ban,\n        labels=red_bans + ['Other'],\n        values=red_vals),\n        row=1,\n        col=2\n    )\n\n    fig.update_layout(\n        height=600,\n        width=800\n    )\n    \n    figs.append(fig)","12f662e8":"iplot(figs[0])","a7d91036":"iplot(figs[1])","c5df0d97":"iplot(figs[2])","85ad1112":"iplot(figs[3])","12277dae":"iplot(figs[4])","801acd01":"iplot(figs[5])","ac98f18e":"iplot(figs[6])","3485b56a":"iplot(figs[7])","a302f921":"iplot(figs[8])","28ab25e0":"iplot(figs[9])","7dd4a474":"champs = ['blue_champ_1', 'blue_champ_2', 'blue_champ_3', 'blue_champ_4', 'blue_champ_5',\n          'red_champ_1', 'red_champ_2', 'red_champ_3', 'red_champ_4', 'red_champ_5']\n\nchamps_formatted = format_champs(df, champs, 35)\n\nfig = go.Figure(data=[\n    go.Pie(\n        labels=list(champs_formatted.keys()),\n        values=list(champs_formatted.values())\n    )\n])\n\nfig.update_layout(\n    height=900,\n    width=800,\n    title='Most Frequently Selected Champions'\n)\n\niplot(fig)","69991ef1":"blue_champs = champs[0:5]\nred_champs = champs[5:]\n\nblue_win_champs_formatted = format_champs(blue_win, blue_champs, 25)\nblue_lose_champs_formatted = format_champs(red_win, blue_champs, 25)\n\nred_win_champs_formatted = format_champs(red_win, red_champs, 25)\nred_lose_champs_formatted = format_champs(blue_win, red_champs, 25)\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type': 'domain'}, {'type': 'domain'}]],\n                    subplot_titles=('Top 25 Blue Champion Selections (Win)',\n                                    'Top 25 Red Champion Selections (Lose)')\n                    )\n\nfig.add_trace(\n    go.Pie(\n        name='Blue',\n        labels=list(blue_win_champs_formatted.keys()),\n        values=list(blue_win_champs_formatted.values())\n    ),\n    row=1,\n    col=1\n)\n\nfig.add_trace(\n    go.Pie(\n        name='Red',\n        labels=list(red_lose_champs_formatted.keys()),\n        values=list(red_lose_champs_formatted.values())\n    ),\n    row=1,\n    col=2\n)\n\nfig.update_layout(\n    height=800,\n    width=800\n)\n\niplot(fig)","81427b31":"fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'domain'}, {'type': 'domain'}]],\n                    subplot_titles=('Top 25 Red Champion Selections (Win)',\n                                    'Top 25 Blue Champion Selections (Lose)')\n                    )\n\nfig.add_trace(\n    go.Pie(\n        name='Red',\n        labels=list(red_win_champs_formatted.keys()),\n        values=list(red_win_champs_formatted.values())\n    ),\n    row=1,\n    col=1\n)\n\nfig.add_trace(\n    go.Pie(\n        name='Blue',\n        labels=list(blue_lose_champs_formatted.keys()),\n        values=list(blue_lose_champs_formatted.values())\n    ),\n    row=1,\n    col=2\n)\n\nfig.update_layout(\n    height=800,\n    width=800\n)","9ba39920":"blue_win_finhibit = blue_win[blue_win['blue_firstInhibitor'] == 1]\nblue_win_ninhibit = blue_win[blue_win['blue_firstInhibitor'] == 0]\n\nred_win_finhibit = red_win[red_win['red_firstInhibitor'] == 1]\nred_win_ninhibit = red_win[red_win['red_firstInhibitor'] == 0]\n\nfig = go.Figure(data=[\n    go.Pie(\n        labels=['Blue Win with First Inhibitor', 'Blue Win without First Inhibitor'],\n        values=[np.sum(blue_win_finhibit['blueWins']), np.sum(blue_win_ninhibit['blueWins'])]\n    )\n])\n\nfig.update_layout(\n    title='Blue Wins With and Without First Inhibitor',\n    height=800,\n    width=800\n)\n\niplot(fig)","d040bb48":"fig = go.Figure(data=[\n    go.Pie(\n        labels=['Red Win with First Inhibitor', 'Red Win without First Inhibitor'],\n        values=[np.sum(red_win_finhibit['redWins']), np.sum(red_win_ninhibit['redWins'])]\n    )\n])\n\nfig.update_layout(\n    title='Red Wins With and Without First Inhibitor',\n    height=800,\n    width=800\n)\n\niplot(fig)","e06c7605":"fig = go.Figure(data=[\n    go.Bar(name='Blue', x=[0], y=[np.sum(df['blue_firstBaron'])], width=0.5, marker_color='#084177'),\n    go.Bar(name='Red', x=[1], y=[np.sum(df['red_firstBaron'])], width=0.5, marker_color='#d63447')\n])\n\nfig.update_layout(\n    title='First Baron Count',\n    xaxis=dict(\n        tickvals=[i for i in range(2)],\n        ticktext=['Blue', 'Red'],\n        showticklabels=True\n    ),\n    width=800,\n    height=800\n)","98fa17d0":"blue_win_fbaron = blue_win[blue_win['blue_firstBaron'] == 1]\nblue_win_nbaron = blue_win[blue_win['blue_firstBaron'] == 0]\n\nred_win_fbaron = red_win[red_win['red_firstBaron'] == 1]\nred_win_nbaron = red_win[red_win['red_firstBaron'] == 0]\n\nfig = go.Figure(data=[\n    go.Pie(\n        labels=['Blue Win with First Baron', 'Blue Win without First Baron'],\n        values=[np.sum(blue_win_fbaron['blueWins']), np.sum(blue_win_nbaron['blueWins'])]\n    )\n])\n\nfig.update_layout(\n    title='Blue Wins With and Without First Baron',\n    height=800,\n    width=800\n)","a6e4f7c2":"fig = go.Figure(data=[\n    go.Histogram(\n        name='Blue Team Killed by Towers (Won)',\n        x=blue_win['red_towerKills']\n    ),\n    go.Histogram(\n        name='Blue Team Killed by Towers (Lost)',\n        x=red_win['red_towerKills']\n    ),\n    go.Histogram(\n        name='Red Team Killed by Towers (Won)',\n        x=red_win['blue_towerKills']\n    ),\n    go.Histogram(\n        name='Red Team Killed by Towers (Lost)',\n        x=blue_win['blue_towerKills']\n    )\n])\n\nfig.update_layout(\n    title='Distribution of Deaths due to Towers',\n    height=800,\n    width=800\n)\n\niplot(fig)","1f143f71":"import matplotlib.style as style\n\ndf_for_corr = df.copy()\n\nplt.figure(figsize=(20, 20))\n\nstyle.use('seaborn-poster')\n\ndf_for_corr.drop(bans + champs + ['redWins', 'redFirstBlood', 'red_firstInhibitor', 'red_firstBaron', 'red_firstRiftHerald'],  axis=1, inplace=True)\ncorr_df = df_for_corr.corr()\n\nsns.heatmap(corr_df)\nplt.title(\"Correlation Matrix\", fontsize=25)\nplt.tight_layout()","956a2a92":"def evaluate_dist(df, cols):\n    champ_cols = ['blue_champ_1', 'blue_champ_2', 'blue_champ_3', 'blue_champ_4', 'blue_champ_5',\n                  'red_champ_1', 'red_champ_2', 'red_champ_3', 'red_champ_4', 'red_champ_5', 'ban_1',\n                  'ban_2', 'ban_3', 'ban_4', 'ban_5', 'ban_6', 'ban_7', 'ban_8', 'ban_9', 'ban_10']\n    kurtosis_results = dict()\n    skewness = dict()\n\n    for i in cols:\n        if i not in champ_cols:\n            kurtosis_results[i] = kurtosis(df[i])\n            skewness[i] = skew(df[i])\n    \n    kurtosis_results = {k: v for k, v in sorted(kurtosis_results.items(), key=lambda item: item[1])}\n    skewness = {k: v for k, v in sorted(skewness.items(), key=lambda item: item[1])}\n\n    print('Skewness:\\n')\n    [print(\"{}: {}\".format(i, skewness[i])) for i in skewness]\n\n    print('\\nKurtosis:\\n')\n    [print(\"{}: {}\".format(i, kurtosis_results[i])) for i in kurtosis_results]\n\n    print(\"\\n\\n\")","8a754e5c":"from scipy.stats import skew, kurtosis\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom prince import MCA","a2f6c0b6":"numerical_cols = [i for i in df if df[i].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']]\n\nevaluate_dist(preproc_df, numerical_cols)","a1c07170":"cols_to_be_transformed = ['blueWardsDestroyed', 'redWardsDestroyed',\n                          'blueWardsPlaced', 'redWardsPlaced',\n                          'redTowersDestroyed', 'blueTowersDestroyed']\n\nfor col in cols_to_be_transformed:\n    preproc_df[col] = np.log1p(preproc_df[col])\n\nevaluate_dist(preproc_df, cols_to_be_transformed)","a7b90d18":"combined_df.drop(['redFirstBlood', 'red_firstInhibitor', 'red_firstBaron', 'red_firstRiftHerald', 'gameId'], axis=1, inplace=True)\n\ntrain_target = combined_df['blueWins'].iloc[:len(preproc_df)].reset_index(drop=True)\ntest_target = combined_df['blueWins'].iloc[len(preproc_df):].reset_index(drop=True)\n\nchamp_cols = ['blue_champ_1', 'blue_champ_2', 'blue_champ_3', 'blue_champ_4', 'blue_champ_5',\n                  'red_champ_1', 'red_champ_2', 'red_champ_3', 'red_champ_4', 'red_champ_5', 'ban_1',\n                  'ban_2', 'ban_3', 'ban_4', 'ban_5', 'ban_6', 'ban_7', 'ban_8', 'ban_9', 'ban_10']\n\ncombined_df.drop(['blueWins', 'redWins'], axis=1, inplace=True)\n\nfor col in champ_cols:\n    combined_df[col] = get_champions(list(combined_df[col].values))\n\nnumerical_cols = [i for i in combined_df if combined_df[i].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']]\n\n\n\ncols_to_be_transformed = ['blueWardsDestroyed', 'redWardsDestroyed',\n                          'blueWardsPlaced', 'redWardsPlaced',\n                          'redTowersDestroyed', 'blueTowersDestroyed']\n\n# Split the combined dfs back to the original data set and my own test data set\ntrain_df = combined_df.iloc[:len(preproc_df)].reset_index(drop=True)\ntest_df = combined_df.iloc[len(preproc_df):, :].reset_index(drop=True)\n\n\n# Preprocess train data\n\ndf_for_scale = train_df[train_df.columns[~train_df.columns.isin(champ_cols)]]\n\nscaler = RobustScaler()\nscaled_data = scaler.fit_transform(df_for_scale)\npca = PCA(.95)\npcs = pca.fit_transform(scaled_data)\n\npca_df = pd.DataFrame(pcs, columns=['PC_{}'.format(i) for i in range(np.size(pcs, 1))])\n\nchamp_df = train_df[train_df.columns[train_df.columns.isin(champ_cols)]]\nchamp_select_df = champ_df[champ_cols[:10]]\nchamp_ban_df = champ_df[champ_cols[10:]]\n\nmca_ban = MCA(n_components=5)\nmca_select = MCA(n_components=3)\n\nban_mca = mca_ban.fit_transform(champ_ban_df)\nselect_mca = mca_select.fit_transform(champ_select_df)\n\nban_mca.columns = ['MCA_Ban_{}'.format(i) for i in range(np.size(ban_mca, 1))]\nselect_mca.columns = ['MCA_Select_{}'.format(i) for i in range(np.size(select_mca, 1))]\n\ntrain_reduced_df = pd.concat([ban_mca, select_mca, pca_df], axis=1)\n\n# Preprocess Test Data\n\ntest_df_for_scale = test_df[test_df.columns[~test_df.columns.isin(champ_cols)]]\n\nscaled_data = scaler.transform(test_df_for_scale)\n\npcs = pca.transform(scaled_data)\n\ntest_pca_df = pd.DataFrame(pcs, columns=['PC_{}'.format(i) for i in range(np.size(pcs, 1))])\n\nchamp_df = test_df[test_df.columns[test_df.columns.isin(champ_cols)]]\nchamp_select_df = champ_df[champ_cols[:10]]\nchamp_ban_df = champ_df[champ_cols[10:]]\n\nban_mca = mca_ban.fit_transform(champ_ban_df)\nselect_mca = mca_select.fit_transform(champ_select_df)\n\nban_mca.columns = ['MCA_Ban_{}'.format(i) for i in range(np.size(ban_mca, 1))]\nselect_mca.columns = ['MCA_Select_{}'.format(i) for i in range(np.size(select_mca, 1))]\n\ntest_reduced_df = pd.concat([ban_mca, select_mca, test_pca_df], axis=1)","20d3eed9":"train_reduced_df.head()","db2f3eb0":"test_reduced_df.head()","0ffe3a98":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier","453e7a20":"x_train, x_test, y_train, y_test = train_test_split(train_reduced_df, train_target)","8cefb541":"model = LogisticRegression(C=0.5, fit_intercept=True, n_jobs=-1, penalty='l2')\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)","e24f6fce":"accuracy_score(y_test, y_pred)","acc0e13b":"model = XGBClassifier(learning_rate=0.09, n_estimators=500, n_jobs=-1)\n\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)","5f61e58f":"accuracy_score(y_test, y_pred)","bef3036c":"model = LogisticRegression(C=0.5, fit_intercept=True, n_jobs=-1, penalty='l2')\n\nmodel.fit(train_reduced_df, train_target)\n\ny_pred = model.predict(test_reduced_df)\nlogit_matrix = confusion_matrix(test_target, y_pred)","3d6f986f":"accuracy_score(test_target, y_pred)","74333275":"model = XGBClassifier(learning_rate=0.09, n_estimators=500, n_jobs=-1, max_depth=5)\n\nmodel.fit(train_reduced_df, train_target)\ny_pred = model.predict(test_reduced_df)\nxgboost_matrix = confusion_matrix(test_target, y_pred)","c1ff7778":"accuracy_score(test_target, y_pred)","8171243b":"model = RandomForestClassifier(bootstrap=True, \n                               max_depth=5,\n                               max_features='auto',\n                               min_samples_leaf=4, \n                               min_samples_split=5, \n                               n_estimators=500, \n                               oob_score=False)\n\nmodel.fit(train_reduced_df, train_target)\ny_pred = model.predict(test_reduced_df)\nrf_matrix = confusion_matrix(test_target, y_pred)","51911a28":"accuracy_score(test_target, y_pred)","4f0bb7dd":"model = SVC(C=0.5, degree=1, gamma='auto', kernel='rbf')\n\nmodel.fit(train_reduced_df, train_target)\ny_pred = model.predict(test_reduced_df)\nsvm_matrix = confusion_matrix(test_target, y_pred)","5ad0a3d3":"accuracy_score(test_target, y_pred)","3f1a33d9":"model_names = [\"Logistic Regression\", \"XGBoost\", \"Random Forest\", \"SVM\"]\nmodel_results = [logit_matrix, xgboost_matrix, rf_matrix, svm_matrix]\n\nfig, axs = plt.subplots(2,2, figsize=(11, 8))\ngrid_counter = 0\nfor i in range(2):\n    for j in range(2):\n        sns.heatmap(model_results[grid_counter], cmap='Blues', cbar=False, annot=True,\n                    fmt='g', annot_kws={'size': 14}, ax=axs[i][j])\n\n        axs[i][j].title.set_text(model_names[grid_counter])\n\n        grid_counter += 1\nplt.tight_layout()\nplt.show()","32ff9fe8":"## Ban 4","3c12231e":"In the range of 0-2 wards destroyed, there isn't a trend at all regarding the outcome of the game. Once the wards destroyed count exceeds 3, there is a clear trend that destroyed ward count correlates with winning. This is most likely due to the fact that if so many wards are destroyed that quickly, the team is dominating \/ being very aggressive and putting a lot of pressure on the other time.","5be21af2":"# Conclusion\n\nThis data set was pretty fun to explore and I am excited to be able to just test random games at will and see how my models does. That idea and adding more models are on my to-do list.\n\nPlease feel free to to criticize\/give advice on my kernel, as I am looking to improve my skills.\n\n","cc98eb50":"## Ban 7","0176f977":"## Target Variable\nI am hoping that there is an even balance of wins and loses, as I do not like having to fill in values knowing that some are incorrect.","41281c57":"# EDA\nI will look at the features in depth to get an idea of the data that I am working with\n\nFirst I will make a column called 'redWins' so it is easier to manipulate the data. I will then make two dataframes that consist of Red and Blue winning, primarily to see the stats of winning teams and compare them to their loses.","60241fc8":"### Red Winning With and Without First Inhibitor","11dfdb95":"Seems that RF has performed the worst so far. ","5f9cebd3":"Considering that the train test split approach is not as good, I will discontinue it and use my own test data set as the validation data while I continue to experiment with other models","50822f23":"It seems teams usually win more when getting the first baron, but the difference is not as severe as it is with inhibitors.","6f1d9880":"## Preprocessing\n\n* I will use the RobustScaler from SK\n* Perform MCA on Nominal Data\n* PCA on continuous data","30161632":"The distribution of all the variations in this histogram are right skewed slightly.","ef2be899":"Originally, I tested my models by using train test split from sklearn and achieved really high scores and was suspicious that my model was overfitting. To combat that, I decided to grab an additional 5k games from the Riot API using the same methods are the original author of the data set.\n\nThe \"more_games_df\" dataframe represents those extra games.\n\nI will train my model using the entire data set provided by the author, and then test using the games I have queried on my own","404287ea":"### Random Forests Classifier","768030bd":"### Blue Winning","36b10486":"Interesting that when blue loses, Lee Sin is very popular. At the same time, Lee Sin is the third most fequent pick when Red wins.","69e69250":"# Team Deaths\nIf kills are vital to the success of a team, it must mean that survivability is also very important. This means that most likely the team that dies least has the higher chance of winning.","b1118cc8":"## Tower Kills\nSince deaths in League can be detrimental to a team, I would assume that if one team dies a lot to towers that they would most likely lose the match.","0c0b9cfd":"Some features that will be removed regardless of correlation are: redFirstBlood, red_firstInhibitor, red_firstBaron, red_firstRiftHerald, and gameId just because we have the blue counter part to them and gameId is not necessary.\n\nOther than those mentioned, I will keep everything else.\n\nI plan to one hot encode the champion related columns, but this is problematic. There are currently 135 champions in League of Legends, this means that once I finish one hot encoding, I will have ~2025 extra columns added to my original data set. So I plan to try different methods of reducing the dimensionality and comparing their scores.","f830fe3a":"The idea presented by the pie chart makes sense, as usually team composition related games typically have metas between patches or seasons. So it makes sense to see champions that work best together to have pick rates close to one anothers.","8ac4061b":"I suspect the choices to be relatively similar to the champion selection visualization embodying winning and losing teams","e6255462":"# Machine Learning\nI will try using several different models in order to pick the most accurate result.\n\n* The process through which model selection will occur is by getting feature importance, hyperparameter tuning with 5 fold cv, then train test split to see which model performs the best","c54b98c4":"### Support Vector Machine","3e8483d6":"### Applying Log1p Transformations\nSince PCA assumes Gaussian, I will need to normalize skewed data","807f56d9":"Pretty high scores. Possibly overfitting","d6cff966":"## Total Team Kills\nKills in League are very important because it slows down the progression of your opponents build, along with giving you gold to upgrade your champion. For this reason, it is pretty clear that kills will be a very important indicator of whether a team wins or loses","c5a50c50":"In the first 10 min, usually no towers are destroyed. If one is destroyed, usually that team wins.","560aeec9":"## Ban 3","1892e4e3":"On average, about 20 wards a game are placed. It is interesting and mind boggling how some games have up to 276 wards placed... Keep in mind this data is for the first 10 min of a game\n\nIt is clear that the distribution of wards placed are left skewed, which may require a transformation.\n\nHere we can see that there is a slight trend with the amount of wards placed and a team winning. Usually the winning team has a larger amount of wards placed that game.\n\nBoth blue and red have a higher mean of wards placed when winning, but it is not that clear for blue side as it is with red.","af01b7e9":"This bar graph takes the sum of games where a certain team got first blood. It is clear that teams that got a first kill are more likely to win, since it does provide advantages as stated earlier.\n\nBut it is also possible that the team was just overall better and won regardless of the \"first blood advantage\".","a6e65376":"# League of Legends Match Outcome Classifier\nMy visualizations of the features from the dataframe and additional features obtained by using Riot Games MatchV4 API.\nI then experiment with different models to get the best results.\n\nTo see how I interact with the API, the source code can be found here: https:\/\/github.com\/jbofill10\/LoL-MatchOutcome-Predictor","822a313d":"## Ban 10","0cb48454":"## Wards Placed\nHaving more information than your opponent in any game is one of the biggest advantages a team can have over another. Therefore, wards are extremely important for the success of a team as they grant extra vision of the enemy and map.","b6bd6858":"## Ban 9","7d11e05a":"### Logistic Regression","97486da4":"## Towers Destroyed\nTowers prevent the enemy team from attacking the Nexus. The more towers that are destroyed, the easier it becomes to attack the Nexus.\nDue to this idea, teams that win would have destoryed more towers than their opponents.","575dda92":"Same idea as with xp. The more gold you have, the better a build your champion can afford. This will lead into a stronger build, making your champion and team stronger.","8249c635":"## First Inhibitor\nInhibitors are a very important structure in League of Legends because it prevents the opposition from training super minions in the lane with the inhibitor destroys. That having said, the team with a destroyed inhibitor would be put at a major disadvantage. I would suspect that teams win significantly more with a first inhibitor destroyed vs. not having destroyed an inhibitor first.","f5a49143":"### Logistic Regression\n","6b6ba5cc":"Logistic Regression seems to out perform XGBC by 3%, which is interesting.\n\nAgain we also see that overall, scores are dropping with my own test data set, but still remaining pretty accurate.\n","ac1d863c":"## Model Selection\nFor all models, I will run a grid search with cv = 5 and tuned hyperparameters to find the best combination and then predict the values of the test data set to see how the model performed.","59f333aa":"### Blue Winning With and Without First Inhibitor","36d189a2":"# Model Results","3a03a274":"## Reducing Dimensionality\nI began by looking into Logistic PCA, but unfortunately there are no Python packages for that. I would've made one myself had I known Linear Algebra (haven't taken that course yet, but soon I will)\n\nI then looked into MCA, which deals with nominal values in data. I read some papers to grasp an understanding of Multiple Correspondence Analysis and decided this would be my approach to reducing the dimensionality of my data.\n\nI plan to do a grid search on optimal n_components for MCA, but for now I am deciding to do 5 components for both champion selections (red and blue) and ban selections.","4c72b7bb":"I would've liked blue\/redTowers to be a little lower, but it should be OK. The log transformation helped lower the Kurtosis score, but outliers are still very present in towers destroyed.\n\nThis is fine though, as the reason for the outliers is that many times, no towers are destroyed within the first 10 min. That means that if one tower is destroyed, it is significant regarding whether a team wins or no","1a6be980":"Sometimes, games didn't contain ban info -- causing some values to be NaN. I don't know if the author of the data set also experience this issue, but I just decided to drop the rows with NaNs","b83f64ff":"The idea is the same as it is with Blue.\n\nI would suspect that getting the first baron helps to lead to the first inhibitor if not destroyed already, which would help secure the game.","8ccdc248":"It is evident that teams with more assists end up winning more games.","5e692cef":"\n# Acknowledgements\n\n[Multiple Corresdondence Analysis](https:\/\/www.researchgate.net\/profile\/Dominique_Valentin\/publication\/239542271_Multiple_Correspondence_Analysis\/links\/54a979900cf256bf8bb95c95.pdf) by Herv\u00e9 Abdi & Dominique Valentin\n\n[Variable Extractions using Principal Components\nAnalysis and Multiple Correspondence Analysis for\nLarge Number of Mixed Variables Classification\nProblems](https:\/\/s3.amazonaws.com\/academia.edu.documents\/50782331\/GJPAM_Published.pdf?response-content-disposition=inline%3B%20filename%3DVariable_Extractions_using_Principal_Com.pdf&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIATUSBJ6BAG7F36CVX%2F20200530%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200530T070022Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQD3gSoTHQ63nANvvqMflElvp2TbA9R7XSTkFbbPulMQ6QIgQgPJBUDeT5o%2BJsw5Pc7J7jWr4s38dLlCHw%2B1a8XY%2BAMqvQMI%2Fv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgwyNTAzMTg4MTEyMDAiDKwvShduLVGpTOEQUSqRA4XRIgf7eWeKKhYlHQzWolE5Vk1g9dfu73310GIKNjPIPqN5ocYMMnsJJkSPha4Cpbab41m6x3hIxjstzVkMBf8qa65WEE7vJA%2BWtPfntmgNieznvvb8fdEiEPjjwmHlFbbRDVw8WVboG5CvCBV2En1f3Audu1%2BVvOr3x6wfS1uJSQVPjYFP2cXkIr3wIJBDhqjWQZfl36pNHR%2BU2z5pTrn8jgQ0lgkiw%2Bnp01hJqnc3CsteBLcES5J1TgfPkBnBF2VfQECRuYejOkzyhq%2F8fsvP%2BIbEisjuzDiTgHqeWwIs%2BCXlZsiJLR1OK2iEd2hMl0gzUDXxl2yoqmkpv83AUchh%2BtORGjTJE7yA26MSX57aqeGZz%2Fqf1kj4Hj8pW4B7%2FUUcPXhpQ5ryDYKzyYtULzDCRQpif%2FjBUr1bZFj48jeIUY5bppEvyGsF8YJ5YiAS13jRysxz7FPG8XjoQm8%2BR84uYJqLsbNhBb0N3XMrisJ89er2DOg8iuoMH%2FWWBXIsdRjkFymb75xItgLipBEbFZqfMJLFx%2FYFOusBGpR9%2BtvXEK4vPfs95BDFzEu8kGK4pgB1M9PqxJPY9SzATCWo3%2FD3MDRC3OjWwbyZkhtYvehnipLldJwwjXNZAoQ%2BHJ2lwX8G8ZZMVvLkHb1Hp8WKhhOs4qEbaYrzbLgwi0UucN4yRw1Doi6BADo8k4fgZ1n5sVM0mH6mzPNpos%2BSuzm0lrVVd4SEQB1X7%2FEdiagrGzurSiVpMtCynpZzWnlKqWz10AcJS81KhZoLY%2FLbQiPWeutHOSZNvLVRzesqIx6wWoNliRH1yndEJhFC95IAuTT6fGphp6OFVACauf84tYzx5WMMYF0Csg%3D%3D&X-Amz-Signature=1365def7559093df3d23d4dae2e967cefb5f825fc808e02bc5095d18acc89b1d) by Hashibah Hamid, Nazrina Aziz, and Penny Ngu Ai Huong","04cbc056":"## Total Experience\nThe more experience a champion has earned, the stronger the build the champion will have. Therefore teams that win will most likely have more experience.","fb08d8d2":"Quite a difference. The data set is relatively small, but I am certain that if more data was added, the difference would still be large.","4b0e3db4":"### XGBoost Classifier\nThe best cv score was 96%","855aeaef":"## Ban 6","d074168e":"## Ban 8","e9e11355":"## Assists\nThe team with more assists also means that team has more kills which should give them the advantage over their opponents. I expect the team with more assists to win more games","9c5adf6f":"It's interesting to see that Lee Sin is selected more on Red when they lose. I don't think this contributes to Red losing though.","7cae1a1b":"Champions with more xp will be stronger since they can level more abilities and probably would have more gold to have a stronger build. So the teams with more xp are more likely to win","0a27c27e":"### Red Winning","3eca2073":"## Using Custom Data Set that mimics the original","cae30ba6":"SVM scores only a little worse than XGBoost, but ranks 3rd of the 4 models tested.","86bf2b32":"This was basically the distribution I had in mind. As a team suffers more deaths to towers, the less likely they win a game. I think this will also be a strong indicator of winning or losing if data values lie outside of 4-6 tower deaths","1efde589":"### After","2218a559":"### XGBoost Classifier","f3629d4c":"Same result as Blue.\n\nThis means that destroying the inhibitor first is probably a good indicator of whether a team wins or not.","9e11b63c":"## First Baron\nThe Baron is the strongest monster in league due to providing the team that defeats it buffs including increased attack damage, increased ability power, and increases the power of minions. Due to this, I suspect that this will also be a strong indicator of who wins the match.","3d201927":"## CS Per Min\nCreep Score, or CS, is one of the most important aspects of League. Having a good creep score means a reliable and steady amount of income, which is pointed out earlier is very important for winning games. Due to this, I know for sure that the winning teams will have higher CS than the losing teams.","e4be2d53":"## Epic Monsters\nEpic Monsters are important provide high gold\/experience and buffs to the team that defeats them. This in turn provides an advantage for the team. I would expect to see that teams will win more on average if they kill more epic monsters.","87359402":"## Champion Selections\n### Among Winning and Losing Teams","853b903f":"## First Bloods\nFirst bloods are important because it provides gold for the team, which allows wards to be set up early. This will prevent the jungler from ganking as well.","de5224d1":"## Champion Bans\nThese are the 20 most frequent bans when a team wins. Unfortunately the API doesn't give information on who picked first, so when it says \"Blue\" or \"Red\" it just means the team that won. In the legend, the champions go in order of most frequent. For example, Other is the most banned, followed by Kassadin in Ban 1.\n\nKeep in mind that there are 135 champions in league, meaning the other category consists of 115 champions!","3617a2fa":"### Helper Methods to get Champion names\nSince the Riot API only gives championIDs, I needed to make another request to an API to get the mappings of championIds for the current patch. I also have that pickled as well.","2657bba8":"So the model performed slightly worse, but this is a data set more than double the size of the train test split variation. I am slightly leaning towards that the model just scores well and is not overfitting. \n\nThis is where I ask for help though, as I am relatively new and would appreciate the more experienced Kaggle's to have their take on whether they think my approach to achieving these scores is correct and avoids scenarios such as overfitting.","50eb8bff":"Ready to start training!","f9166934":"## Total Gold\nStronger items and upgrades to items can be obtained with gold. So most likely the teams that win will have more gold since they had stronger champions that contributed to winning the game.","ad88b042":"## Wards Destroyed\nSimilar to wards being so important due to the information they can provide to the team, denying that information is also key to winning. Due to this idea, I would suspect that the team with more destroyed wards has a better chance of winning the game.","d9283071":"It seems that people in diamond and master are not fond of Kassadin at all. Generally, it seems that the ban selections are very strict at the top level of play.","e9e39e84":"As suspected, Teams that win tend to die less on average.","0eab53bb":"Pretty good stuff actually. I am glad that they are nearly even.\n\nNow I would like to go through the features.","5e447774":"### Before","0f046445":"## Ban 5","65547ca8":"## Feature Selection","e092d93b":"## Ban 1","bd2c1bf1":"## Ban 2","03887661":"I used the Riot API to get all the champions selected and banned, along with some other information regarding team statistics which I will explain later on. Champion selections are very important, because any team based game at the top level of play has a \"meta\", which means the best style of play \/ team compositions to achieve maximum value within a game. That being said, people who pick the \"meta\" champions are more likely to win.  \n\nThis idea is similar with bans, as some champions are just too strong in the certain patch and are better off banned every game. You will see later on that Kassadin practically permanently banned as of this patch which the data set is from.","7c339c31":"Really high score.. I suspect overfitting, but I haven't looked into it that much yet..\n\nI think I will try to just pull random games from the League database and see how this model does.\n\nI also plan to add more models to experiment some more."}}