{"cell_type":{"33cbbde1":"code","149dad00":"code","cd61361b":"code","b57c81b2":"code","519ff143":"code","a0f2c862":"code","6a56c3f4":"markdown","94401544":"markdown","6ac68ea9":"markdown","c6f6a1e8":"markdown","86b2cf06":"markdown","dd556f2b":"markdown","ae1c0e53":"markdown","eb1c4cab":"markdown"},"source":{"33cbbde1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\n\n# utility for speed\nfrom collections import Counter\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/shakespeare-plays'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","149dad00":"import re\n\n# directory which containes txt files\n#DATA_PATH = \"dataset\" \n\ndef read_data(directory):\n    \"\"\"\n    read text files into one variable and return combined string\n    \"\"\"\n    raw_text = \"\"\n    for entry in os.scandir(directory):\n        if entry.is_file():\n            f = open(entry, encoding=\"utf8\", errors='ignore')\n            raw_text += (f.read())\n            f.close() \n    return raw_text\n\ndef clean_text(text):\n    \"\"\" \n    remove all special sentences\n    remove all non alphanumerica and non space chars\n    \"\"\"\n\n    text = re.sub('<[^>]+>', '', text)\n    text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n    return text\n\n\ndef write_result(text):\n    f = open(\"cleaned.txt\", \"w\")\n    f.write(text)\n    f.close()\n\ntext = read_data(dirname)\ntext = clean_text(text)\nwrite_result(text)","cd61361b":"CLEANED_FILEPATH = \"cleaned.txt\"\n\ndef get_word_freq(text):\n    text = text.lower()\n    word_list = text.split()\n\n    # if Counter allowed, much fatser \n    word_freq = dict(Counter(word_list))\n    \n    # if Counter not allowed, slower \n    # word_freq = {}\n    # unique_words = set(word_list)\n    # for w in unique_words:\n    #     word_freq[w] = word_list.count(w)\n    \n    # reverse sort by freq\n    word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n    return word_freq\n\ndef read_data(filepath):\n    \"\"\" read cleaned text \"\"\"\n    \n    f = open(filepath, \"r\")\n    cleaned_text = (f.read())\n    f.close() \n\n    return cleaned_text\n\ndef write_vocab(word_freq):\n    \"\"\" write word, freq line by line \"\"\"\n\n    f = open(\"vocab.txt\", \"w\")\n    for k, v in word_freq:\n        f.writelines(f\"{k}, {v}\\n\")\n    f.close()\n\n\ndef plot_graph_1(word_freq):\n        df = pd.DataFrame(word_freq[:100], columns=['words', 'freq'])\n        df.plot.bar(x='words', y='freq', figsize=(14,4))\n        plt.show()\n\ndef plot_graph_2(word_freq):\n        df = pd.DataFrame(word_freq, columns=['words', 'freq'])\n        df = df.groupby(['freq']).size().reset_index(name=\"counts\")\n        df = df[df.freq <= 250]\n        df.plot.bar(y='freq', x='counts', fontsize=\"12\", figsize=(14,4))\n        plt.show()\n        \ntext = read_data(CLEANED_FILEPATH)\nword_freq = get_word_freq(text)\nwrite_vocab(word_freq)\nplot_graph_1(word_freq)\nplot_graph_2(word_freq)","b57c81b2":"CLEANED_FILEPATH = \"cleaned.txt\"\nVOCAB_FILEPATH = \"vocab.txt\"\n\n\ndef prob_matrix(cleaned_text, vocab_data):\n    \"\"\"\n    read data and populate matrix\n    \"\"\"\n    cleaned_text = cleaned_text.lower()\n\n    # calculate bigrams\n    bigrams = [x for x in zip(cleaned_text.split()[:-1], cleaned_text.split()[1:])] \n\n    # if counter allow, much faster\n    bigrams_freq = dict(Counter(bigrams))\n\n    # take first 1000 words from vocab\n    vocab_data = vocab_data.split(\"\\n\")[:1000]\n    vocab_dict = {}\n    for line in vocab_data:\n        key, value = line.split(\",\")\n        vocab_dict[key] = int(value)\n    words = vocab_dict.keys()\n    \n    # n by n matrix \n    table = []\n    for word in words:\n        row = []\n        for w in words:\n            key = (word, w)\n\n            # if counter allowed, much faster\n            if (key in bigrams_freq):\n                row.append(bigrams_freq[key])\n            else:\n                row.append(0)\n\n            # if counter not allowed, slower\n            # row.append(bigrams.count((word, w)))\n \n        table.append(row)\n    \n    # convert to prob\n    df = pd.DataFrame(table, columns=words, index=words)\n    df['freq_count'] = vocab_dict.values()\n    df = (df[words] + 1) \/ (df['freq_count'] + len(words))\n    df = df.transpose()\n\n    # return pandas df\n    return df\n\ndef write_df(df):\n    \"\"\" write model to csv \"\"\"\n    df.to_csv(\"model.csv\")\n\n\ncleaned_text = read_data(CLEANED_FILEPATH)\nvocab_data = read_data(VOCAB_FILEPATH)\ndf = prob_matrix(cleaned_text, vocab_data)\nwrite_df(df)","519ff143":"MODEL_FILEPATH = \"model.csv\"\n\ndef load_model(path):\n    \"\"\" load probablity matrix for inference \"\"\"\n\n    df = pd.read_csv(path, index_col=0)\n    return df\n\ndef predict_next(df, stmt):\n    ''' predict net word which highest prob '''\n\n    stmt_words = reversed(stmt.split())\n    for w in stmt_words:\n        if w in df.columns:\n            return df[w].idxmax()\n        else:\n            print(\"..out of vocab, trying previous word\")\n    return \"out of vocab\"\n    \n\ndef take_input():\n    ''' propmt user for input '''\n    sentence = str(input(\"Enter your stmt: \"))\n    sentence = sentence.strip()\n    return sentence","a0f2c862":"df = load_model(MODEL_FILEPATH)\nwhile True:\n    stmt = take_input()\n    print(\"suggested word = \", predict_next(df, stmt))\n    if stmt == \"quit\":\n        break","6a56c3f4":"## Part 4\n1. Load model\n2. Predict next word using bi-gram model\n3. Create input sequence","94401544":"## Part 3\n1. Read cleaned.txt and vocab.txt\n2. Create probability matrix\n3. Write matrix into csv file","6ac68ea9":"## Part 1\n1. Read data\n2. Clean data\n3. Write cleaned data into cleaned.txt file","c6f6a1e8":"# Bi-gram Model using William Shakespeare Plays","86b2cf06":"## Part 2\n1. Read cleaned data\n2. Get word frequency\n3. Write word frequency in vocab.txt\n4. Plot graph of frequenies against first 100 words\n5. Plot number of words that occur n times against word occurance","dd556f2b":"![Language Model](https:\/\/cdn.shortpixel.ai\/client\/q_glossy,ret_img,w_1170\/https:\/\/nlpforhackers.io\/wp-content\/uploads\/2017\/03\/Language-Models.png)","ae1c0e53":"Congratulations you have successfully created bi-gram model without any hihlevel library.\n\n\n> Please **upote** the notebook if you find it useful.\n\n\nThank you.","eb1c4cab":"## **CLI**\n\nInput interface to predict next word"}}