{"cell_type":{"12d2ef40":"code","6cda6f99":"code","9426aad9":"code","f69778e3":"code","ce9365f5":"code","5424bf60":"code","7ec52bc5":"code","4c89397c":"code","f263b3ac":"code","57fc485c":"code","ac7c5b22":"code","5e1da65a":"code","2eb12468":"code","eac9c137":"code","33b01979":"code","fe713069":"code","5b70a404":"code","a9caefe8":"code","8d9c74fa":"code","c788c229":"code","393cf764":"code","1cbbadae":"code","ff31faf2":"code","65a43614":"code","faeaee9a":"code","1c5668ba":"code","2cc311a1":"code","b1fe9f5c":"markdown","e11b3931":"markdown","450ded1b":"markdown","81b9ff89":"markdown","82ad686a":"markdown","a1bbbb51":"markdown","a2b37685":"markdown","33069ff5":"markdown","72fa0068":"markdown","1d4117cc":"markdown","3cc3fba3":"markdown","459bf302":"markdown","6ee9f549":"markdown","264a02c1":"markdown","98211497":"markdown","ba7eef1c":"markdown","1b41771b":"markdown","e6b7ee87":"markdown"},"source":{"12d2ef40":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","6cda6f99":"train_data=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","9426aad9":"train_data.info()","f69778e3":"train_data","ce9365f5":"word_len=[]\nfor i in range(len(train_data)):\n    word_len.append(len(train_data.text.values[i].split(' ')))    ","5424bf60":"plt.figure(figsize=(12,6))\nsns.countplot(word_len)\nplt.xlabel(\"Word lengths:\")\nplt.ylabel('Counts:')\nplt.title('Train Data \\n Max length='+str(max(word_len)))\nplt.show()","7ec52bc5":"plt.figure(figsize=(10,8))\nsns.countplot(train_data.target)\nplt.title('Count for Zeros:'+str(train_data.target.value_counts()[0])+'\\n'+\n         'Count for Ones:'+str(train_data.target.value_counts()[1]))\nplt.show()","4c89397c":"train_data=train_data.drop('keyword',axis=1)\ntrain_data=train_data.drop('location',axis=1)","f263b3ac":"Y_train=train_data.target\nX_train=train_data.text","57fc485c":"Y_train=tf.reshape(Y_train,(-1,1))","ac7c5b22":"Y_train","5e1da65a":"max_words = 100000\nmax_len = 100\n\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","2eb12468":"sequences_matrix[1729]","eac9c137":"model=tf.keras.Sequential()\n\nmodel.add(tf.keras.layers.Input(shape=[max_len]))\nmodel.add(tf.keras.layers.Embedding(max_words,128,input_length=max_len))    \n\nmodel.add(tf.keras.layers.LSTM(200, return_sequences=True))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.LSTM(200,return_sequences=True))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.LSTM(200))\nmodel.add(tf.keras.layers.Dropout(0.5))\n          \nmodel.add(tf.keras.layers.Dense(256))\nmodel.add(tf.keras.layers.Dropout(0.5))\n\nmodel.add(tf.keras.layers.Dense(1,activation='sigmoid')) #output layer","33b01979":"model.summary()","fe713069":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])","5b70a404":"hist=model.fit(sequences_matrix,Y_train,batch_size=64,epochs=30)","a9caefe8":"model.evaluate(sequences_matrix,Y_train)","8d9c74fa":"plt.plot(hist.history['loss'],'g')\nplt.xlabel('Epochs:')\nplt.ylabel('Loss:')\nplt.show()","c788c229":"#accuracy\n\nplt.plot(hist.history['acc'],'r')\nplt.xlabel('Epochs:')\nplt.ylabel('Accuracy:')\nplt.show()","393cf764":"test_data=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","1cbbadae":"X_test=test_data.text","ff31faf2":"tok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_test)\nsequences_test = tok.texts_to_sequences(X_test)\nsequences_matrix_test = sequence.pad_sequences(sequences_test,maxlen=max_len)","65a43614":"pred=model.predict(sequences_matrix_test)","faeaee9a":"pred=(pred>0.5)*1","1c5668ba":"p=pd.DataFrame()\np['id']=test_data['id']\np['target']=pred","2cc311a1":"p.to_csv('.\/Submission_sachin.csv',index=False)","b1fe9f5c":"# Step 1: Import required libraries.","e11b3931":"# Step 6: Metrics and results","450ded1b":"## This step is required because i have used sigmoid activation function at the output layer. ","81b9ff89":"## Tokenizing the train text data","82ad686a":"## Reshaping Y_train so that it's easier to process it when using LSTM.","a1bbbb51":"## Visualizing the number of words in sentences\n","a2b37685":"# Step 2: Read Train data (Descriptive and Exploratory analysis)","33069ff5":"## Define X_train and Y_train data.","72fa0068":"## Train Accuracy","1d4117cc":"# Step 3: Pre-processing","3cc3fba3":"## Visualizing Train loss and accuracy epoch wise","459bf302":"## Visualizing the number of instances in target field","6ee9f549":"# Step 5: Training the model","264a02c1":"# Step 7: Prediction on test data","98211497":"## As we can see there are 2 columns that contain null values (keyword and location), we will drop them since anyways we will only use the text and target columns","ba7eef1c":"## Predict","1b41771b":"# Step 4: Creating RNN Model ","e6b7ee87":"## Submission"}}