{"cell_type":{"1282df53":"code","93bcda20":"code","62c7d232":"code","b99ad8ca":"code","e4764c85":"code","e8fafd3a":"code","1b9c32b2":"code","dd30757d":"code","a1bcbd59":"code","0ce67392":"code","c2169838":"code","bf53eb25":"code","f239feb4":"code","e66b23be":"code","7f848fef":"markdown","3a5e306e":"markdown","20eb6d50":"markdown","ca60847f":"markdown","b5060d8b":"markdown","453775a1":"markdown"},"source":{"1282df53":"import numpy as np\nfrom nltk.corpus import stopwords","93bcda20":"wiki = 'Wikipedia is a free content, multilingual online encyclopedia written and maintained by a community of volunteers through a model of open collaboration, using a wiki-based editing system. Individual contributors, also called editors, are known as '\nwiki","62c7d232":"stop_words = stopwords.words('english')\nstop_words[:5]","b99ad8ca":"stop_words = set(stop_words)\n\nwiki1 = wiki.lower().split()","e4764c85":"wiki_no_stopwords = [word for word in wiki1 if word not in stop_words]\nwiki_no_stopwords","e8fafd3a":"print(' '.join(wiki_no_stopwords))","1b9c32b2":"char_token = [ch for ch in wiki]\nchar_token[:5]","dd30757d":"from nltk.stem import PorterStemmer, LancasterStemmer","a1bcbd59":"porter = PorterStemmer()\nlancaster = LancasterStemmer()\n\n","0ce67392":"hello = [0.61, 0.18, 0.51]\nhi = [0.6, 0.17, 0.5]\napple = [.12, .43, .9]","c2169838":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (16, 12))\n\nax = fig.add_subplot(projection = '3d')\n\nxa = [hello[0], hi[0], apple[0]]\nya = [hello[1], hi[1], apple[1]]\nza = [hello[2], hi[2], apple[2]]\nax.scatter(xa, ya, za, s = 180)\n\n","bf53eb25":"np.dot(np.array(hello), np.array(hi))","f239feb4":"np.dot(np.array(hello), np.array(apple))","e66b23be":"np.dot(np.array(hi), np.array(apple))","7f848fef":"# Remove Stop Words","3a5e306e":"![Screenshot from 2021-10-02 14-42-04.png](attachment:81c24830-4b50-49ae-9107-a38bca467de2.png)","20eb6d50":"# ","ca60847f":"# Stemming","b5060d8b":"# Tokens\n* a token is simply a single unit or aa piece of information that we would feed into a model in NLP","453775a1":"# Lemmatization"}}