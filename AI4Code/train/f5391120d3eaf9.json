{"cell_type":{"37716de2":"code","0e199bae":"code","78d7955e":"code","57aa41dc":"code","60de2d01":"code","b6698718":"code","a956ad5b":"code","5c972deb":"code","13461d40":"code","ebc05f6e":"code","da2844a1":"code","c1bbb501":"code","baf1faae":"code","c6508100":"code","347f9bb6":"code","8265cbeb":"code","faf71199":"code","ccba804e":"code","4e0524de":"code","6be9de1e":"code","c8a50c2e":"code","3edd2ee3":"code","50158261":"code","2f06ff58":"markdown","a9de8562":"markdown","b6e44aa6":"markdown","72d49661":"markdown","c4563adb":"markdown","92a9281a":"markdown","eb0ebd48":"markdown","6d02649e":"markdown","744c299e":"markdown","44e7d294":"markdown","58a4b2af":"markdown","e807e389":"markdown","83ed7d84":"markdown","f4f2cc7e":"markdown","4671a392":"markdown","5d3cdd79":"markdown"},"source":{"37716de2":"# Import packages\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Open dataset\ntrain_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")\ncombined_data = [train_data, test_data]","0e199bae":"# Look at first rows in training set\ntrain_data.head()","78d7955e":"# Look at first rows in test set\ntest_data.head()","57aa41dc":"# Get summary data for training set\ntrain_data.describe()","60de2d01":"# Now look at the test set\ntest_data.describe()","b6698718":"# Look at how many null values there are for the different columns\ntrain_data.info()","a956ad5b":"train_data = train_data.drop(['Cabin'], axis='columns')\ntest_data  = test_data.drop(['Cabin'], axis='columns')\n\ntrain_data.info()\nprint()\ntest_data.info()","5c972deb":"# For simplicity, I will set the variabes with null values to the mean and \n# median of the non-null values.\ntrain_data.Age  = train_data.Age.fillna(train_data.Age.median() )\ntrain_data.Fare = train_data.Fare.fillna(train_data.Fare.mean() )\n\ntest_data.Age  = test_data.Age.fillna(test_data.Age.median() )\ntest_data.Fare = test_data.Fare.fillna(test_data.Fare.mean() )","13461d40":"train_data.info()\nprint()\ntest_data.info()","ebc05f6e":"# There are still 2 null values for the column Embarked.  Since we can't apply a mean\/median \n# for this value, we will put in the most used value in the dataset\ntrain_data.Embarked.value_counts()","da2844a1":"# I will set Embarked = \"S\" where it is null\ntrain_data.Embarked  = train_data.Embarked.fillna(\"S\")","c1bbb501":"# Use Seaborn category plot \ngender_plot = sns.catplot(x=\"Sex\", col=\"Survived\", data=train_data, kind=\"count\", \n              height=4, aspect=.75)\n(gender_plot.set_axis_labels(\"\", \"Count\")\n            .set_xticklabels([\"Men\", \"Women\"]) \n            .set_titles(\"{col_name} {col_var}\") )","baf1faae":"# Look at distribution of survivors by Pclass and gender\nclass_plot = sns.catplot(x=\"Pclass\", col=\"Survived\", hue=\"Sex\", data=train_data,  \n             kind=\"count\", height=4, aspect=.75)","c6508100":"# Look at distribution of survivors by Age, classified into 10 groups\nage_plot = plt.hist(train_data[train_data.Survived == 1].Age, bins=10)","347f9bb6":"# Look at distribution of survivors by number of siblings or spouse\nplcass_plot = plt.hist(train_data[train_data.Survived == 1].SibSp)","8265cbeb":"# Convert the gender to 0 (male) and (female)\ntrain_data.Sex = [0 if i==\"male\" else 1 for i in train_data.Sex]\ntest_data.Sex  = [0 if i==\"male\" else 1 for i in test_data.Sex]\n\n# Drop all of the columns that were not statistically significant\n# (Cabin has already been removed)\nnew_train_x = train_data.drop([\"PassengerId\", \"Survived\", \"Name\", \"Parch\", \"Ticket\", \n                               \"Fare\", \"Embarked\"], axis=1)\n\nnew_train_y = train_data[\"Survived\"]\n\n# Drop the variables from the test data as well\nnew_test_x =  test_data.drop([\"PassengerId\", \"Name\", \"Parch\", \"Ticket\", \n                               \"Fare\", \"Embarked\"], axis=1)","faf71199":"# X is the training set, y is the prediction \ny = new_train_y \nX = new_train_x\n\n# Split the training set in two, with 20% going to test set\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size = .20, random_state = 0)","ccba804e":"# Try Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\n\nreg = LogisticRegression()\nreg.fit(train_x, train_y)\nprint(\"Accuracy - Logistic Regression - train:\", round(reg.score(train_x, train_y), 3),\n      \"test:\", round(reg.score(test_x, test_y), 3) )","4e0524de":"# Try gradient boosting model\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\n\nGBC = GradientBoostingClassifier()\nGBC.fit(train_x, train_y)\n\n# Predicting the test set results\npred_y = GBC.predict(test_x)\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(test_y, pred_y)\n\nprint (\"Accuracy - GradientBoosting - train:\", round(GBC.score(train_x , train_y), 3),\n       \"test:\", round(GBC.score(test_x , test_y), 3) )","6be9de1e":"# Try other algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Random Forest\nRFC = RandomForestClassifier(n_estimators=100)\nRFC.fit(train_x, train_y)\nprint (\"Accuracy - RFC - train:\", round(RFC.score(train_x , train_y), 3),\n       \"test:\", round(RFC.score(test_x , test_y), 3) )","c8a50c2e":"# Support Vector Classification\nsvc = SVC()\nsvc.fit(train_x, train_y)\nprint (\"Accuracy - SVC - train:\", round(svc.score(train_x , train_y), 3),\n       \"test:\", round(svc.score(test_x , test_y), 3) )","3edd2ee3":"# k-nearest neighbors\nknn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(train_x, train_y)\nprint (\"Accuracy - KNN - train:\", round(knn.score(train_x , train_y), 3),\n       \"test:\", round(knn.score(test_x , test_y), 3) )","50158261":"# Create the submission file, with PassengerId and Survival prediction\npsgr_id = test_data[\"PassengerId\"]\n\n# Rerun the model on the entire training set\nGBC = GradientBoostingClassifier()\nGBC.fit(new_train_x, new_train_y)\n\n# Apply the algorithm to the test data file\nprediction = GBC.predict(new_test_x)\n\n# Save the results to a csv file\nsubmission = pd.DataFrame( {\"PassengerId\" : psgr_id, \"Survived\": prediction} )\nsubmission.to_csv(\"submission.csv\", index=False)","2f06ff58":"Now I will split up the training file into train versus test, to evaluate different models.","a9de8562":"There are 418 rows in the test dataset.","b6e44aa6":"All of the plots above confirm possible linearity from the four independent variables, as per the analysis done in Gretl.","72d49661":"This shows again that females were more likely to survive, and that Pclass is also a factor in survival.","c4563adb":"This shows that the majority of the surviving passengers are in the age bracket 20-40.","92a9281a":"Of the 5 models that I tried, GradientBoosting had the highest accuracy rating with 82.7%.  I will submit the results from this model.","eb0ebd48":"The accuracy of this model is 79.9% .","6d02649e":"New to data science, just finished several online courses, and want to put knowledge to practice.\nI know that I'm using a very simplistic approach to dealing with null values, but just want to keep it simple for the first run through.","744c299e":"The datasets are now without any null values.","44e7d294":"This shows that passengers with no sibling\/spouse, or just one sibling\/spouse were more likely to survive.","58a4b2af":"There are 891 rows in the training dataset.","e807e389":"Look at distribution of male versus female, and how many survived.","83ed7d84":"The Cabin columns has more nulls than values, so I shouldn't rely on this for data analysis.  I will drop this column.","f4f2cc7e":"This shows that females were more likely to survive.","4671a392":"To practice what I have learned in courses, I used Gretl to determine which independent variables were more likely to have an influence on the dependent variable (Survival).\n\nI used the Backward eliminination method with multiple logistic regression.  \nThe model iterations showed the following:\n- Model 1 - p-value was highest for Parch, so removed it and reran model\n- Model 2 - p-value was highest for Embarked, so removed it and reran model\n- Model 3 - p-value was highest for Fare, so removed it and reran model  \n- Model 4 - p-value was highest for Ticket, so removed it and reran model\n- Model 5 - all remaining independent variables are showing as significant with this model: \nPclass, Sex, Age, SibSp\nModel 5 shows an accuracy percentage of 80.8%.\n\nWill continue on here with Python, using these remaining 4 variables.","5d3cdd79":"The accuracy of this model is 82.7%, which is better."}}