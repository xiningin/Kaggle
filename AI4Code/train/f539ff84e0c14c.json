{"cell_type":{"b053851b":"code","e6f01b02":"code","1185e548":"code","ab65abdc":"code","e72676da":"code","9ef7f1e3":"code","e7be4c41":"code","cd98b8a6":"code","3e2f9572":"code","d793ffcc":"code","acbb251a":"code","038a8721":"code","b63c948d":"code","794349ce":"code","7582fa4c":"code","cd5135b5":"code","4d3dd7a8":"code","3799bb71":"code","3666bedd":"code","ab8d6f2b":"code","59742897":"code","e1939dba":"code","ade1a0cd":"code","24e57d97":"code","41f993d0":"code","9baa0da2":"code","4898deb1":"code","cda18193":"code","f81ffb7a":"code","9ad1ac79":"code","febd2f45":"code","8397b2fa":"code","65ae26ca":"code","afe21d8d":"code","bf888023":"code","cf1a4b26":"code","93ea2414":"code","1a5a1556":"code","6834208b":"code","bad20fe4":"code","84d30eff":"code","7780c821":"code","fc693404":"code","0bc396d7":"code","3d863659":"code","56aef09a":"code","39a94680":"code","907119ed":"code","d89ad1c4":"code","96f52ba2":"code","63202d05":"code","fdd954bf":"code","9e041a21":"code","7d9e3ab8":"code","9d338e0a":"code","7665628b":"code","edcaaa2f":"code","354c1344":"code","92755fb1":"code","656b4f28":"code","0406e5aa":"code","2c156b35":"code","ef99c192":"code","95b3db00":"code","135fdd1d":"code","7a365d16":"code","a7b6c58a":"code","7c175fce":"code","5aeff238":"code","8a39f685":"code","d9d97714":"code","da506706":"code","9b20ea2b":"code","841bdcc0":"code","9b13adec":"code","8527dcc4":"code","7ecb7afc":"code","8bdfec97":"code","6da102c5":"code","58975975":"code","c3c51689":"code","bfcaee47":"code","011dbade":"code","de13b86a":"code","51434a66":"code","e4576331":"code","440e0c2d":"code","1d17e19d":"code","91a7bcb8":"code","22737416":"code","d5eae983":"code","239cfda5":"code","e8268a62":"code","c12b90c5":"code","8c901dd4":"code","e09beb5b":"code","6538a1f3":"code","1b129103":"code","99521cc2":"code","51fd3614":"code","1e92a0e6":"code","e56859e1":"code","f0ca2135":"code","7a1ce57f":"code","14fc4c87":"code","294d58b5":"code","ae2a614b":"code","7c99a220":"code","fd68714b":"code","8ebf3526":"code","7d857446":"code","e4334e4a":"code","b27dd98e":"code","6e2b4c7e":"code","eb08dcd9":"code","0bb163a3":"code","b919863d":"code","d636f302":"code","c4b3d28a":"code","0e540174":"code","1826c69e":"code","aec8bc88":"code","2cf762d9":"code","ba56de38":"code","996652d7":"code","eef8dd39":"code","793e0ea4":"code","ff9b4308":"code","363d8913":"code","2c700471":"code","29fcc974":"code","3a40c566":"code","bc61b094":"code","e2c39f0b":"code","1083ce31":"code","4f854e51":"code","d38043bf":"code","557e73b7":"code","d1a76c6b":"code","5b3b84a7":"code","b6f4f32d":"code","6974d3cd":"code","e958b093":"code","2552c378":"code","e68296e6":"code","fbaf1b29":"markdown","0c2bdf22":"markdown","abbd39be":"markdown","01392414":"markdown","54bd685e":"markdown","619ed2cd":"markdown","4220a123":"markdown","ef5a91f0":"markdown","ae3bb42b":"markdown","51c68fc2":"markdown","ab856363":"markdown","dc253ecf":"markdown","a819628d":"markdown","46bc36b9":"markdown","d4867089":"markdown","a41787fc":"markdown","21f189a7":"markdown","0606847f":"markdown","7ac2f60e":"markdown","7ccc9ba2":"markdown","9f8649f0":"markdown","f08bbc83":"markdown","df943906":"markdown","15a64fbe":"markdown","c6225f48":"markdown","8579d689":"markdown","b9fc83d9":"markdown","74530744":"markdown","c60eb620":"markdown","ca39cdbd":"markdown","5a6a2ed7":"markdown","bd0e8084":"markdown","c1f31f11":"markdown","8102b8f4":"markdown","d3a91b5f":"markdown","bc7023b3":"markdown","906f4438":"markdown","19ffd3f5":"markdown","6a9bfa6f":"markdown","1716719c":"markdown","c553e7cf":"markdown","7198d29a":"markdown","d41a4691":"markdown","9f237afa":"markdown","4ebac20e":"markdown","27ed26d6":"markdown","018bcd83":"markdown","bb64bc8e":"markdown","b6d8acdf":"markdown","ae4f8319":"markdown","df0f4913":"markdown","6355c768":"markdown","48d59562":"markdown","fd60177c":"markdown","5e9ae6f7":"markdown"},"source":{"b053851b":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","e6f01b02":"#Load the Train set\ntrain_set = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_set=pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain_set.tail(10)","1185e548":"# inspect the structure etc.\nprint(train_set.info(), \"\\n\")\nprint(train_set.shape)","ab65abdc":"train_set['Survived'].value_counts()","e72676da":"sns.countplot(train_set['Survived'])\nplt.show()\nprint('Percent of fraud transaction: ',len(train_set[train_set['Survived']==1])\/len(train_set['Survived'])*100,\"%\")\nprint('Percent of normal transaction: ',len(train_set[train_set['Survived']==0])\/len(train_set['Survived'])*100,\"%\")","9ef7f1e3":"# missing values in Train set df\ntrain_set.isnull().sum()","e7be4c41":"round(100*(test_set.isnull().sum().sort_values(ascending=False)\/len(test_set.index)), 2)","cd98b8a6":"train_set.Age.describe()","3e2f9572":"train_set['Title']=train_set['Name'].map(lambda x: x.split(',')[1].split('.')[0].lstrip())\ntest_set['Title']=test_set['Name'].map(lambda x: x.split(',')[1].split('.')[0].lstrip())\ntrain_set.head()","d793ffcc":"train_set['Title'].value_counts()","acbb251a":"print(train_set.info())","038a8721":"#Check the list of values in title column\ntrain_set.Title.unique()","b63c948d":"# lets sort the remaining other categories in title to various sub category of Mr, Miss, mrs, train_set\ntitle_list=['Mrs', 'Mr', 'Master', 'Miss']\ntrain_set.loc[~train_set['Title'].isin(title_list),['Age','Sex','Title']]","794349ce":"# function to bucket other titles into major 4\ndef fix_title(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col','Sir']:\n        return 'Mr'\n    elif title in ['the Countess', 'Mme','Lady','Dona']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title","7582fa4c":"train_set['Title']=train_set.apply(fix_title, axis=1)\ntrain_set['Title'].value_counts()","cd5135b5":"test_set['Title']=test_set.apply(fix_title, axis=1)\ntest_set['Title'].value_counts()","4d3dd7a8":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4}\ntrain_set['Title'] = train_set['Title'].map(title_mapping)\ntrain_set['Title'] = train_set['Title'].fillna(0)","3799bb71":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4}\ntest_set['Title'] = test_set['Title'].map(title_mapping)\ntest_set['Title'] = test_set['Title'].fillna(0)","3666bedd":"train_set.Age.isnull().sum()","ab8d6f2b":"#Check mean  on title Subclass w.r.t Age\ntrain_set.groupby(['Title'])['Age'].describe()","59742897":"train_set.groupby(['Title'])['Age'].median()","e1939dba":"round(100*(train_set.isnull().sum().sort_values(ascending=False)\/len(train_set.index)), 2)","ade1a0cd":"# Total Nullvalues in Age Column\ntrain_set.Age.isnull().sum()","24e57d97":"title = train_set['Title'].value_counts()\na= dict(title)\nfor keys, values in a.items():\n    print('Value of {} Class with Null Values {other}'.format(keys, other=train_set.loc[(train_set.Title==keys),['Age']].isnull().sum()))","41f993d0":"#Impute Missing values in Age Column\nfor keys, values in a.items():\n    missing_val=train_set.loc[(train_set.Title==keys) & ~(train_set.Age.isnull()),['Age']].median(axis=0, skipna=True).astype('float')\n    train_set.loc[(train_set.Title==keys) & (train_set.Age.isnull()),'Age']=train_set.loc[(train_set.Title==keys) & (train_set.Age.isnull()),'Age'].replace(np.nan,missing_val.median())","9baa0da2":"title = train_set['Title'].value_counts()\nb= dict(title)\nfor keys, values in a.items():\n    print('Value of {} Class with Null Values {other}'.format(keys, other=test_set.loc[(test_set.Title==keys),['Age']].isnull().sum()))","4898deb1":"#Impute Missing values in Age Column\nfor keys, values in b.items():\n    missing_val=test_set.loc[(test_set.Title==keys) & ~(test_set.Age.isnull()),['Age']].median(axis=0, skipna=True).astype('float')\n    test_set.loc[(test_set.Title==keys) & (test_set.Age.isnull()),'Age']=test_set.loc[(test_set.Title==keys) & (test_set.Age.isnull()),'Age'].replace(np.nan,missing_val.median())","cda18193":"# After Imputation on Age Colums verify the null values\ntrain_set.Age.isnull().sum()","f81ffb7a":"test_set.Age.isnull().sum()","9ad1ac79":"test_set['Fare'].fillna(test_set['Fare'].median(), inplace=True)","febd2f45":"# GEt the unique set of Value of Cabin\ntrain_set.Cabin.unique()","8397b2fa":"# Lets see the cabin with passenger class\nclass_cabin=train_set.groupby(['Pclass'])['Cabin'].count()\nclass_cabin","65ae26ca":"# No Pclass\ntrain_set.Pclass.value_counts()","afe21d8d":"cls = train_set['Pclass'].value_counts()\nfor key, value in (dict(cls)).items():\n    print('Value of {} passenger Class with Null Values {other}'.format(key,other=train_set.loc[(train_set.Pclass==key),['Cabin']].isnull().sum()))","bf888023":"train_set.loc[(train_set.Pclass==1) & ~(train_set.Cabin.isnull()),['Cabin']]\n","cf1a4b26":"# Lets have Deck # as separte Columns and null as GNR\ntrain_set['Deck']=pd.Series(train_set.loc[~(train_set.Cabin.isnull()),['Cabin']].values.flatten()).astype('str').str[0]","93ea2414":"deck = pd.Series(train_set['Cabin'].values.flatten().astype('str'))\ndeck1 = []\nfor i in deck:\n    if i != 'nan':\n        deck1.append(i[0])\n    else: \n        deck1.append(i)","1a5a1556":"train_set['Deck']=deck1","6834208b":"train_set.loc[~(train_set.Cabin.isnull()),['Cabin']].values.flatten()","bad20fe4":"train_set['Deck']","84d30eff":"train_set.head(50)","7780c821":"# Lets see the unique value and count of Deck Column\ntrain_set['Deck'].value_counts()","fc693404":"train_set.Deck.unique()","0bc396d7":"train_set.Deck.isnull().sum()","3d863659":"# Replace Nan in Decek to GNR\ntrain_set['Deck']=train_set['Deck'].replace('nan','GNR')","56aef09a":"train_set['Deck'].value_counts()","39a94680":"# Remove Cabin Column\ntrain_set.drop('Cabin',axis=1,inplace=True)\ntest_set.drop('Cabin',axis=1,inplace=True)","907119ed":"train_set.head()","d89ad1c4":"# Now lets check the column with null values\ntrain_set.isnull().sum()","96f52ba2":"# Value of Embarked on various categories\ntrain_set.Embarked.value_counts()","63202d05":"train_set.Embarked.isnull().sum()","fdd954bf":"#Lets impute 2 null records of Embarked with value 'S' as it have max occurance\ntrain_set.loc[(train_set.Embarked.isnull()),'Embarked']=train_set.loc[ (train_set.Embarked.isnull()),'Embarked'].replace(np.nan,'S')\ntrain_set.Embarked.isnull().sum()","9e041a21":"# Check if any null columns are present\ntrain_set.isnull().sum()","7d9e3ab8":"sns.distplot(train_set['Age'])","9d338e0a":"g = sns.FacetGrid(train_set, col='Survived',size=5)\ng.map(plt.hist, 'Age', bins=30)","7665628b":"# pairplot\nsns.pairplot(train_set)\nplt.show()","edcaaa2f":"sns.countplot(x=\"Pclass\", data=train_set)","354c1344":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_set, col='Survived', row='Pclass', size=4, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=30)\ngrid.add_legend();","92755fb1":"sns.distplot(train_set['Fare'])","656b4f28":"sns.boxplot(y=train_set['Fare'])","0406e5aa":"# Checking for Outlier\ntrain_set.Fare.describe(percentiles=[.25, .5, .75, .90, .95, .99])","2c156b35":"grid = sns.FacetGrid(train_set, row='Embarked', col='Survived', size=4, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","ef99c192":"import seaborn as sns\nsns.countplot(x=\"Survived\", data=train_set)","95b3db00":"### Checking the Survival Rate Rate\nsurvival = (sum(train_set['Survived'])\/len(train_set['Survived'].index))*100\nsurvival","135fdd1d":"# Remove name and Passenger Id Column\ntrain_set.drop(['Name'],axis=1,inplace=True)\ntest_set.drop(['Name'],axis=1,inplace=True)","7a365d16":"#Checking the Correlation Matrix\nplt.figure(figsize = (15,10))\nsns.heatmap(train_set.corr(),annot = True)\nplt.show()","a7b6c58a":"#Check the Survival rate by Paasaenger Class\n# print(train_set [['Pclass','Survived']].groupby('Pclass').mean())\na = train_set.groupby(['Pclass','Survived']).agg({'Pclass': 'sum'})\na.groupby(level=0).apply(lambda x: 100 * x \/ float(x.sum()))","7c175fce":"sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train_set)","5aeff238":"sep=\"---------------------------------------------------------------\"\na = train_set.groupby(['Pclass','Sex','Survived']).agg({'Pclass': 'sum'})\na.groupby(level=0).apply(lambda x: 100 * x \/ float(x.sum()))\n# print(round(a,2),'\\n')","8a39f685":"#Check the Survival rate by Paasaenger Class\nsep=\"---------------------------------------------------------------\"\nprint( round(train_set [['Sex','Survived']].groupby(['Sex']).mean()*100,1),'\\n',sep)\nprint(train_set [['Pclass','Sex','Survived']].groupby(['Pclass','Sex']).agg(['count','mean']))","d9d97714":"#tracking the Survival on the basis of Sex and PClass\ng = sns.catplot(x=\"Pclass\", hue=\"Sex\", col=\"Survived\",\n                data=train_set, kind=\"count\",\n                height=4, aspect=.7, size = 7);","da506706":"# check the impact of Embarked Colum on Survival\nprint( round(train_set [['Embarked','Survived']].groupby(['Embarked']).mean()*100,1))","9b20ea2b":"pd.crosstab(train_set['Survived'],train_set['Pclass']).apply(lambda r: (r\/r.sum())*100, axis=1)","841bdcc0":"pd.crosstab(train_set['Survived'],[train_set['Pclass'],train_set['Sex']]).apply(lambda r: (r\/r.sum())*100, axis=1)","9b13adec":"grid = sns.FacetGrid(train_set, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","8527dcc4":"pd.crosstab(train_set['Survived'],[train_set['Embarked'],train_set['Sex']]).apply(lambda r: (r\/r.sum())*100, axis=1)","7ecb7afc":"train_set.loc[(train_set['Parch']==0)&(train_set['SibSp']==0)]","8bdfec97":"sns.countplot(x=\"Parch\", hue=\"Survived\", data=train_set)","6da102c5":"sns.countplot(x=\"SibSp\", hue=\"Survived\", data=train_set)","58975975":"train_set['Family']=train_set['SibSp']+train_set['Parch']+1\ntest_set['Family']=test_set['SibSp']+test_set['Parch']+1\ntrain_set.head()","c3c51689":"train_set[['Family', 'Survived']].groupby(['Family'], as_index=False).mean().sort_values(by='Survived', ascending=False)","bfcaee47":"train_set['IsAlone'] = 0\ntest_set['IsAlone'] = 0\ntrain_set.loc[train_set['Family'] == 1, 'IsAlone'] = 1\ntest_set.loc[test_set['Family'] == 1, 'IsAlone'] = 1\ntrain_set[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","011dbade":"df = train_set.groupby(['Ticket']).size().reset_index(name='count')\nprint(df)","de13b86a":"# New column for Ticket Head Count on teh complete data\n# master=pd.concat([train_set, test_set])\n# master.head()\n# train_set['TicketHeadCount']=train_set['Ticket'].map(master['Ticket'].value_counts())\ntrain_set['TicketHeadCount']=train_set['Ticket'].map(train_set['Ticket'].value_counts())\ntest_set['TicketHeadCount']=test_set['Ticket'].map(test_set['Ticket'].value_counts())\ntrain_set.head()","51434a66":"#Let take fair per Person as per Ticket head Count\ntrain_set['FairPerPerson']=train_set['Fare']\/train_set['TicketHeadCount']\ntest_set['FairPerPerson']=test_set['Fare']\/test_set['TicketHeadCount']\ntrain_set[['FairPerPerson']].describe(percentiles=[.25, .5, .75, .90, .95, .99])","e4576331":"train_set['FairPerPerson'].value_counts()<1","440e0c2d":"# Lets check the distribution\nsns.distplot(train_set['FairPerPerson'])","1d17e19d":"#Check the impact of Fair on chances of Survival\nplt.figure(figsize = (15,10))\nsns.violinplot(x=\"Family\", y=\"FairPerPerson\", hue=\"Survived\",\n                    data=train_set, palette=\"muted\")\nplt.show()","91a7bcb8":"sns.violinplot(x=\"Pclass\", y=\"FairPerPerson\", hue=\"Sex\",\n                    data=train_set, palette=\"muted\")","22737416":"sns.violinplot(x=\"Survived\", y=\"Age\", hue=\"Sex\",\n                    data=train_set, palette=\"muted\")","d5eae983":"plt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.stripplot(x=\"Survived\",y=\"Age\",data=train_set.loc[(train_set['Age']>0.0) & (train_set.Age<15.0)],jitter=True,palette='Set1')\nplt.subplot(2,3,2)\nsns.stripplot(x=\"Survived\",y=\"Age\",data=train_set.loc[(train_set['Age']>15.0) & (train_set.Age<40.0)],jitter=True,palette='Set1')\nplt.subplot(2,3,3)\nsns.stripplot(x=\"Survived\",y=\"Age\",data=train_set.loc[(train_set['Age']>40.0) & (train_set.Age<60.0)],jitter=True,palette='Set1')\nplt.subplot(2,3,4)\nsns.stripplot(x=\"Survived\",y=\"Age\",data=train_set.loc[(train_set['Age']>60.0) & (train_set.Age<80.0)],jitter=True,palette='Set1')\n\nplt.show()","239cfda5":"# Lets check on graph the survival of male aginst Female within age 15-40 years\nsns.stripplot(x=\"Survived\",y=\"Age\",data=train_set.loc[(train_set['Age']>15.0) & (train_set.Age<40.0)],jitter=True,hue='Sex',palette='Set1')","e8268a62":"#tracking th Survival on the basis of Family Size and Sex\ng = sns.catplot(x=\"Family\", hue=\"Sex\", col=\"Survived\",\n                data=train_set, kind=\"count\",\n                height=7, aspect=.7);","c12b90c5":"sns.boxplot(x = 'Pclass', y = 'FairPerPerson',hue='Survived', data = train_set)","8c901dd4":"# Group the Deck by Class\nprint(train_set.groupby([ 'Pclass','Deck'])['Survived'].agg(['count','mean']))","e09beb5b":"# Lets Check the pattern of Deck on Age\nsns.swarmplot(x=\"Deck\",y=\"Age\",hue='Sex',data=train_set,palette=\"Set1\", split=True)","6538a1f3":"sns.swarmplot(x=\"Deck\",y=\"FairPerPerson\",hue='Pclass',data=train_set,palette=\"Set1\", split=True)","1b129103":"plt.figure(figsize=(25, 14))\nsns.catplot(x=\"Deck\", col=\"Survived\",data=train_set, kind=\"count\",height=4, aspect=.7, hue='Pclass')\nplt.show()","99521cc2":"print(train_set.loc[(train_set['FairPerPerson']==0),['Embarked','Ticket','SibSp','Parch','Age','Sex','Family','TicketHeadCount']])","51fd3614":"train_set['FairPerPerson'].describe(percentiles=[.25, .5, .75, .90, .95, .99])","1e92a0e6":"train_set.info()","e56859e1":"quantile_1, quantile_3 = np.percentile(train_set.FairPerPerson, [25, 75])","f0ca2135":"print(quantile_1, quantile_3)","7a1ce57f":"iqr_value = quantile_3 - quantile_1\niqr_value","14fc4c87":"lower_bound_val = quantile_1 - (1.5 * iqr_value)\nupper_bound_val = quantile_3 + (1.5 * iqr_value)\nprint(lower_bound_val, upper_bound_val)","294d58b5":"plt.figure(figsize = (10, 5))\nsns.kdeplot(train_set.FairPerPerson)\nplt.axvline(x=lower_bound_val, color = 'red')\nplt.axvline(x=upper_bound_val, color = 'red')","ae2a614b":"train_set[(train_set.FairPerPerson >= lower_bound_val) & (train_set.FairPerPerson <= upper_bound_val)].info()","7c99a220":"round(100*(train_set[(train_set.FairPerPerson >= lower_bound_val) & (train_set.FairPerPerson <= upper_bound_val)].count()\/len(train_set.index)), 2)","fd68714b":"round(100*(train_set[(train_set.FairPerPerson >= 0) & (train_set.FairPerPerson <= 100)].count()\/len(train_set.index)), 2)","8ebf3526":"train_set_copy=train_set.loc[(train_set.FairPerPerson>0) & (train_set.FairPerPerson<=100)]\ntrain_set_copy.shape","7d857446":"train_set_copy.info()","e4334e4a":"sns.boxplot(x = 'Pclass', y = 'FairPerPerson',hue='Survived', data = train_set_copy)","b27dd98e":"train_set.head()","6e2b4c7e":"test_set.head()","eb08dcd9":"train_set_copy.drop(['Parch','Ticket','Fare','Deck','SibSp','TicketHeadCount'],axis=1,inplace=True)\ntest_set.drop(['Parch','Ticket','Fare','SibSp','TicketHeadCount'],axis=1,inplace=True)\ntrain_set_copy.head()","0bb163a3":"train_set_copy = pd.concat([train_set_copy, pd.get_dummies(train_set_copy['Sex'], drop_first=True)], axis=1)\ntest_set = pd.concat([test_set, pd.get_dummies(test_set['Sex'], drop_first=True)], axis=1)","b919863d":"train_set_copy['Embarked'] = train_set_copy['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntest_set['Embarked'] = test_set['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","d636f302":"#Drop Original Columns\ntrain_set_copy.drop(['Sex'],axis=1,inplace=True)\ntest_set.drop(['Sex'],axis=1,inplace=True)","c4b3d28a":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","0e540174":"X_train = train_set_copy.drop(\"Survived\", axis=1)\ny_train = train_set_copy[\"Survived\"]\nX_test  = test_set.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, y_train.shape, X_test.shape","1826c69e":"scaler = StandardScaler()\n\nX_train[['Age','FairPerPerson']] = scaler.fit_transform(X_train[['Age','FairPerPerson']])\nX_test[['Age','FairPerPerson']] = scaler.fit_transform(X_test[['Age','FairPerPerson']])\n\nX_train.head()","aec8bc88":"### Checking the Survival Rate\nSurvival = (sum(train_set_copy['Survived'])\/len(train_set_copy['Survived'].index))*100\nSurvival","2cf762d9":"plt.figure(figsize = (20,10))\nsns.heatmap(X_train.corr(),annot = True)\nplt.show()","ba56de38":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,accuracy_score","996652d7":"col = [ 'Pclass', 'Age', 'Embarked', 'Title', 'Family',\n       'IsAlone', 'FairPerPerson', 'male']","eef8dd39":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train[col], y_train)\nY_pred = logreg.predict(X_test[col])\nacc_log = round(logreg.score(X_train[col], y_train) * 100, 2)\nacc_log","793e0ea4":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train[col], y_train)\nY_pred = svc.predict(X_test[col])\nacc_svc = round(svc.score(X_train[col], y_train) * 100, 2)\nacc_svc","ff9b4308":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train[col], y_train)\nY_pred = knn.predict(X_test[col])\nacc_knn = round(knn.score(X_train[col], y_train) * 100, 2)\nacc_knn","363d8913":"gaussian = GaussianNB()\ngaussian.fit(X_train[col], y_train)\nY_pred = gaussian.predict(X_test[col])\nacc_gaussian = round(gaussian.score(X_train[col], y_train) * 100, 2)\nacc_gaussian","2c700471":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train[col], y_train)\nY_pred = perceptron.predict(X_test[col])\nacc_perceptron = round(perceptron.score(X_train[col], y_train) * 100, 2)\nacc_perceptron","29fcc974":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train[col], y_train)\nY_pred = linear_svc.predict(X_test[col])\nacc_linear_svc = round(linear_svc.score(X_train[col], y_train) * 100, 2)\nacc_linear_svc","3a40c566":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train[col], y_train)\nY_pred = sgd.predict(X_test[col])\nacc_sgd = round(sgd.score(X_train[col], y_train) * 100, 2)\nacc_sgd","bc61b094":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train[col], y_train)\nY_pred = decision_tree.predict(X_test[col])\nacc_decision_tree = round(decision_tree.score(X_train[col], y_train) * 100, 2)\nacc_decision_tree","e2c39f0b":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train[col], y_train)\nY_pred = random_forest.predict(X_test[col])\nrandom_forest.score(X_train[col], y_train)\nacc_random_forest = round(random_forest.score(X_train[col], y_train) * 100, 2)\nacc_random_forest","1083ce31":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","4f854e51":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train[col], y_train)\nY_pred = random_forest.predict(X_test[col])\nrandom_forest.score(X_train[col], y_train)\nacc_random_forest = round(random_forest.score(X_train[col], y_train) * 100, 2)\nacc_random_forest","d38043bf":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","557e73b7":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train[col],y_train)\n\nada_best = gsadaDTC.best_estimator_\ngsadaDTC.best_score_","d1a76c6b":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train[col],y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","5b3b84a7":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","b6f4f32d":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train[col],y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","6974d3cd":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train[col],y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","e958b093":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train[col], y_train)\nY_pred = votingC.predict(X_test[col])","2552c378":"from xgboost.sklearn import XGBClassifier\nmodel = XGBClassifier(learning_rate=0.001,n_estimators=2500,\n                                max_depth=4, min_child_weight=0,\n                                gamma=0, subsample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=27,\n                                reg_alpha=0.00006)\nmodel.fit(X_train[col], y_train)\nY_pred = model.predict(X_test[col])","e68296e6":"submission = pd.DataFrame({\n        \"PassengerId\": test_set[\"PassengerId\"],\n        \"Survived\": Y_pred\n     })\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)","fbaf1b29":"Highest Survival in Class 1 and lowest in Class 3","0c2bdf22":"Lets make the DECK Nan values as GNR. The null values count is same as Cabin","abbd39be":"###### Now lets check the relationship between targert variable and other dependent variable","01392414":"As % wise the trend is common it is not drawing any impact on Target variable. Less fair or more fair will not impact the cause of Survival realted to family Size kept in between","54bd685e":"This excerice of checking mean, median into subclass is important as it will help in decision on imputing Null values on Age as title give relative information on Age","619ed2cd":"###### Lets Check on Family Size now","4220a123":"# Hyperparameter tunning","ef5a91f0":"In all the clases the survival rate for Female is higher during the evacuation time Female Passenger were the first to be evcuated we can infer from the above.\n\n96% and 92% females were able to survive on the 1st and 2nd class respectively.\n\nThe Survival rate of Male is very less and as 1st is premium class near to the evacuation area o have seen max# of survival in male category. rest keep deminishing","ae3bb42b":"But before the Ticket head Count we need to take test data file for calculating actual no, like how many total passanger were travelling on the same ticket. Then only Fair per person will be the right amount. So for this only combine the test and train data","51c68fc2":" We Can see the Cabin list is not giving much information and every Class of Passanger have some missing values. We can see from the above few Steps that PClass 3 is lower class have highest missing values.\n 1. 1st Class have most popluated have given the Cabin and can say the First letter is the kind of Deck# as we have in Ships for ex A, C, B, T, D, E\n 2. Same as in rest of the class we can relate that if there is Cabin List no is there so particulary assigned then is attached to some deck and for other cases where cabin # is Null we can say it as General\/ unkwnown. Let say it GNR(general).","ab856363":"From the above we can categorize as below\n1. Don, major, Capt,Jonkheer, Rev, Col as MR as Sex=Male and age is also greater that 30.\n2. Countess, MMe can be categorized as Mrs as sex = Female and age is greater than 30\n3. Mlle, Ms as Miss because sex=Female, age<30\n4. Dr will be categorized into Mr and Mrs on the basis of Sex\n","dc253ecf":"The male Deaths are more dense between 15-40 age group","a819628d":"The Deck are randomly distributed over. GNR which we named as unknown have highest distribution.\n\n2 reasons->Either Deck no on others are missing or Deck space were small rooms","46bc36b9":"* From the above we can see female has highest survival rate.\n* survival rate is also depends on passanger class.","d4867089":"Class 3 Passenger are poorly affected with the sink","a41787fc":"Lets first use Name colums to drive the new Title columns which can be helpful in idea of age by grouping them","21f189a7":"###### Defualt Hyperparameters\nLet's first fit a random forest model with default hyperparameters.","0606847f":"**Observations**\n\n* Pclass=3 had most passengers, however most did not survive.\n* Infact passengers in Pclass=2 and Pclass=3 mostly survived.\n* Most passengers in Pclass=1 survived.\n* Pclass varies in terms of Age distribution of passengers.","7ac2f60e":"cabin column is not providing any insights majority GNR class has less survival rate which we imputed.","7ccc9ba2":"Now we can remove the Cabin columns as the information it can be checked and inferred from Deck Column","9f8649f0":"\n1. From the graph the Single Member or solo passanger survival rate is very poor.\n2. Male Solo Passanger number is quite high and mortality rate too in comparision too Female.\n3. Survival Chances become low after family size>4","f08bbc83":"**Observations**\n\n* Infants (Age <=4) had high survival rate.\n* Oldest passengers (Age = 80) survived.\n* Large number of 15-25 year olds did not survive.\n* Most passengers are in 15-35 age range.","df943906":"Few Points:\n1. Age group between 0-10 have the good survival% in both gender.\n2. Female see the more survives than death in any Age Group\n3. After age 60 everybody tends to death.\n4. Male sees more death than survivors between 25-35 Age group","15a64fbe":"Even over the class the Deck are present for each. So nothing surprises here.","c6225f48":"##### Inferential Stats","8579d689":"1. Master Class have Age mean 4.57 and median 3.5\n2. Miss Class have Age mean and median approx 21.0\n3. Mr. Class have Age mean 32 and median 30\n4. Mrs class have Age mean 36 and median 35. \n\nSo Mean and Median are very close by on all the values so lets impute the missing values in Age with Median","b9fc83d9":"1. From this above data it looks like these are Line man and Crew member. \n2. All have the same boarding station and Ticket no on some says\"Line\". \n3. All are male and of approx average age group 25-45 years.\n4. These are solo  travellers no sibiling or parent","74530744":"We can create a new feature for FamilySize which combines Parch and SibSp.","c60eb620":"On Fare Per Person among Survival and non survivers are same . Only 1st class have some outliers","ca39cdbd":"Now Only Cabin have massive chunk \nand Very few in Embarked have the missing values. We need to find pattern in Cabin and for Embarked we can remove and impute from Median","5a6a2ed7":"Now there are no missing values present in the Data. Now Lets do plot some graphs to see distribution of columns and relationship between target variable","bd0e8084":"**Observations**\n\n* Female passengers had much better survival rate than males.\n* Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n* Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports.\n* Ports of embarkation have varying survival rates for Pclass=3 and among male passengers.","c1f31f11":"##### Dropping some columns which we will impact on Survival.\n1. Title is used for imputing Age missing values and things can be drawn out of Age column.\n2. Will drop Parch, Sibsp as we derived Family and IsAlone column.\n3. Will drop Ticket, Deck, Ticket Head Count Column as it won't give any impact.\n4. Will drop Fare columns as we derived FairPerPerson column to use","8102b8f4":"### **Missing Value treatment**","d3a91b5f":"We can Removed Passanger Id and name as they will not of any use in this case","bc7023b3":"Max no of people have per per ticket cost between 0-$50.0 . As min Fair is zero that is not possible either there is a mistake or it can be the staff those will also be boarded for passanger service. We will check in sometime for this","906f4438":"We will choose some Fare range which we set max to leave out after that.\n1. People with Fare 0  we will exclude them.\n2. People with fair greater than 60 we will exclude them\n\nBy doing this we will only loose approve 1% of data and that is on which we can still live with it.","19ffd3f5":"**Observations**\n\n* Higher fare paying passengers had better survival.\n* Port of embarkation correlates with survival rates.","6a9bfa6f":"We can see in Fair column we have Outlier, we will check on this using Scalling later","1716719c":"#### EDA Discovery\n1. Female Gender is highly survived on the ship regardless of any class against male.\n2. People with 1st Class have low casulity rate.\n3. Pclass and Fair are co-linear and will explain the same impact on Targer variable. As Fair increase, class increase and with that your survival chances do increase.\n4. Travelling in a group of 2-4 will increase your chances of getting saved.\n5. Senior Citizen on lower class will not be able to make it out from the casulity.\n6. 25-40 age group will have more chances to get saved.","c553e7cf":"##### As we can see above Cabin and Age have maximum chunk of missing values. So we need to check or get inference for them to imput with some values","7198d29a":"##### 38% of the total have survived which  is good amout of no in target variable.So there is no class imbalance","d41a4691":"Clearly as said earlier the Age Group Section 15-40 have seen more deaths and also major % of travellers\n\nAbove 60+ Age the % of death is very high","9f237afa":"As we can see and multiple people have the same ticket No surprise here as people are travelling with Sibing and Parent. Lets create one more varibale for Ticket Head Count and then we can divide the Total fare per Person","4ebac20e":"## Model Building","27ed26d6":"There is mostly negative correlation between Pclass, Age, Fare rest there is no big Correlation","018bcd83":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.","bb64bc8e":"As expected the difference in Fair per class","b6d8acdf":"There is certainely outlier in Fair and same thing we can see in Fair Per Person","ae4f8319":"## Feature Scaling","df0f4913":"Now only 2 records in Embarked have null values. We can either remove them as very less in no or we can replace with value of Max counts of category ","6355c768":"###### we can see there are unnecessary catogories for Dr, Major and and some title for male as sir and Don and etc. Lets keep four categories only Mr., Miss. , Mrs, train_set","48d59562":"From the Above represtation we can clearly see the Random forest and Decision Model Performace is better among all.","fd60177c":"##### Test-Train Split","5e9ae6f7":"##### For categorical variables with multiple levels, create dummy features (one-hot encoded)\n"}}