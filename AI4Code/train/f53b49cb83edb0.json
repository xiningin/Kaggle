{"cell_type":{"2280b7bd":"code","66e055b0":"code","a87810a3":"code","a59f0919":"code","1146bd75":"code","568a93ed":"code","3c7fb7fc":"code","90ceda18":"code","42fc96d7":"code","d100d294":"code","bf1b42de":"code","3feb064d":"code","81d21d85":"code","0e967cbb":"code","f4702770":"code","15201901":"code","ceca9265":"code","ccace2fa":"code","fa5ed70c":"code","7e6eeb82":"code","715b463f":"code","b7dd1b6d":"markdown","a686d18a":"markdown","9d217498":"markdown","e1030969":"markdown","85caeb5b":"markdown","e34155b1":"markdown","0db27b69":"markdown","bb55bf90":"markdown","4f334ea3":"markdown","9bfcc970":"markdown","f440a06a":"markdown","ceb993a5":"markdown","351f4b07":"markdown","a2dd7504":"markdown","67ca0042":"markdown","fe1cc965":"markdown","e3177081":"markdown","ee904829":"markdown","a24f7df0":"markdown","77b23e2e":"markdown","e38bccca":"markdown","35383175":"markdown","a8cc46e6":"markdown","9234f1a0":"markdown","be02ddd3":"markdown","d8951524":"markdown","afbf05c2":"markdown","42d76506":"markdown","8a057f3d":"markdown","bb583b74":"markdown","deb4492f":"markdown","20dddf34":"markdown","3d69f22c":"markdown","48eba87d":"markdown","9c104ecf":"markdown"},"source":{"2280b7bd":"# regular expression operations\nimport re    \n# string operation \nimport string  \n# shuffle the list\nfrom random import shuffle\n\n# linear algebra\nimport numpy as np \n# data processing\nimport pandas as pd \n\n# NLP library\nimport nltk\n# download twitter dataset\nfrom nltk.corpus import twitter_samples                          \n\n# module for stop words that come with NLTK\nfrom nltk.corpus import stopwords          \n# module for stemming\nfrom nltk.stem import PorterStemmer        \n# module for tokenizing strings\nfrom nltk.tokenize import TweetTokenizer   \n\n# scikit model selection\nfrom sklearn.model_selection import train_test_split\n\n# smart progressor meter\nfrom tqdm import tqdm","66e055b0":"# Download the twitter sample data from NLTK repository\nnltk.download('twitter_samples')","a87810a3":"# read the positive and negative tweets\npos_tweets = twitter_samples.strings('positive_tweets.json')\nneg_tweets = twitter_samples.strings('negative_tweets.json')\nprint(f\"positive sentiment \ud83d\udc4d total samples {len(pos_tweets)} \\nnegative sentiment \ud83d\udc4e total samples {len(neg_tweets)}\")","a59f0919":"# Let's have a look at the data\nno_of_tweets = 3\nprint(f\"Let's take a look at first {no_of_tweets} sample tweets:\\n\")\nprint(\"Example of Positive tweets:\")\nprint('\\n'.join(pos_tweets[:no_of_tweets]))\nprint(\"\\nExample of Negative tweets:\")\nprint('\\n'.join(neg_tweets[:no_of_tweets]))","1146bd75":"# helper class for doing preprocessing\nclass Twitter_Preprocess():\n    \n    def __init__(self):\n        # instantiate tokenizer class\n        self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                                       reduce_len=True)\n        # get the english stopwords \n        self.stopwords_en = stopwords.words('english') \n        # get the english punctuation\n        self.punctuation_en = string.punctuation\n        # Instantiate stemmer object\n        self.stemmer = PorterStemmer() \n        \n    def __remove_unwanted_characters__(self, tweet):\n        \n        # remove retweet style text \"RT\"\n        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n\n        # remove hyperlinks\n        tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n     \n        # remove hashtags\n        tweet = re.sub(r'#', '', tweet)\n        \n        #remove email address\n        tweet = re.sub('\\S+@\\S+', '', tweet)\n        \n        # remove numbers\n        tweet = re.sub(r'\\d+', '', tweet)\n        \n        ## return removed text\n        return tweet\n    \n    def __tokenize_tweet__(self, tweet):        \n        # tokenize tweets\n        return self.tokenizer.tokenize(tweet)\n    \n    def __remove_stopwords__(self, tweet_tokens):\n        # remove stopwords\n        tweets_clean = []\n\n        for word in tweet_tokens:\n            if (word not in self.stopwords_en and  # remove stopwords\n                word not in self.punctuation_en):  # remove punctuation\n                tweets_clean.append(word)\n        return tweets_clean\n    \n    def __text_stemming__(self,tweet_tokens):\n        # store the stemmed word\n        tweets_stem = [] \n\n        for word in tweet_tokens:\n            # stemming word\n            stem_word = self.stemmer.stem(word)  \n            tweets_stem.append(stem_word)\n        return tweets_stem\n    \n    def preprocess(self, tweets):\n        tweets_processed = []\n        for _, tweet in tqdm(enumerate(tweets)):        \n            # apply removing unwated characters and remove style of retweet, URL\n            tweet = self.__remove_unwanted_characters__(tweet)            \n            # apply nltk tokenizer\n            tweet_tokens = self.__tokenize_tweet__(tweet)            \n            # apply stop words removal\n            tweet_clean = self.__remove_stopwords__(tweet_tokens)\n            # apply stemmer \n            tweet_stems = self.__text_stemming__(tweet_clean)\n            tweets_processed.extend([tweet_stems])\n        return tweets_processed","568a93ed":"# initilize the text preprocessor class object\ntwitter_text_processor = Twitter_Preprocess()\n\n# process the positive and negative tweets\nprocessed_pos_tweets = twitter_text_processor.preprocess(pos_tweets)\nprocessed_neg_tweets = twitter_text_processor.preprocess(neg_tweets)","3c7fb7fc":"pos_tweets[:no_of_tweets], processed_pos_tweets[:no_of_tweets]","90ceda18":"# BOW frequency represent the (word, y) and frequency of y class\ndef build_bow_dict(tweets, labels):\n    freq = {}\n    ## create zip of tweets and labels\n    for tweet, label in list(zip(tweets, labels)):\n        for word in tweet:\n            freq[(word, label)] = freq.get((word, label), 0) + 1\n        \n    return freq","42fc96d7":"# create labels of the tweets\n# 1 for positive labels and 0 for negative labels\nlabels = [1 for i in range(len(processed_pos_tweets))]\nlabels.extend([0 for i in range(len(processed_neg_tweets))])\n\n# combine the positive and negative tweets\ntwitter_processed_corpus = processed_pos_tweets + processed_neg_tweets\n\n# build Bog of words frequency \nbow_word_frequency = build_bow_dict(twitter_processed_corpus, labels)","d100d294":"# extract feature for tweet\ndef extract_features(processed_tweet, bow_word_frequency):\n    # feature array\n    features = np.zeros((1,3))\n    # bias term added in the 0th index\n    features[0,0] = 1\n    \n    # iterate processed_tweet\n    for word in processed_tweet:\n        # get the positive frequency of the word\n        features[0,1] = bow_word_frequency.get((word, 1), 0)\n        # get the negative frequency of the word\n        features[0,2] = bow_word_frequency.get((word, 0), 0)\n    \n    return features","bf1b42de":"# shuffle the positive and negative tweets\nshuffle(processed_pos_tweets)\nshuffle(processed_neg_tweets)\n\n# create positive and negative labels\npositive_tweet_label = [1 for i in processed_pos_tweets]\nnegative_tweet_label = [0 for i in processed_neg_tweets]\n\n# create dataframe\ntweet_df = pd.DataFrame(list(zip(twitter_processed_corpus, positive_tweet_label+negative_tweet_label)), columns=[\"processed_tweet\", \"label\"])","3feb064d":"# train and test split\ntrain_X_tweet, test_X_tweet, train_Y, test_Y = train_test_split(tweet_df[\"processed_tweet\"], tweet_df[\"label\"], test_size = 0.20, stratify=tweet_df[\"label\"])\nprint(f\"train_X_tweet {train_X_tweet.shape}, test_X_tweet {test_X_tweet.shape}, train_Y {train_Y.shape}, test_Y {test_Y.shape}\")","81d21d85":"# train X feature dimension\ntrain_X = np.zeros((len(train_X_tweet), 3))\n\nfor index, tweet in enumerate(train_X_tweet):\n    train_X[index, :] = extract_features(tweet, bow_word_frequency)\n\n# test X feature dimension\ntest_X = np.zeros((len(test_X_tweet), 3))\n\nfor index, tweet in enumerate(test_X_tweet):\n    test_X[index, :] = extract_features(tweet, bow_word_frequency)\n\nprint(f\"train_X {train_X.shape}, test_X {test_X.shape}\")","0e967cbb":"train_X[0:5]","f4702770":"def sigmoid(z): \n    \n    # calculate the sigmoid of z\n    h = 1 \/ (1+ np.exp(-z))\n    \n    return h","15201901":"# implementation of gradient descent algorithm  \n\ndef gradientDescent(x, y, theta, alpha, num_iters, c):\n\n    # get the number of samples in the training\n    m = x.shape[0]\n    \n    for i in range(0, num_iters):\n        \n        # find linear regression equation value, X and theta\n        z = np.dot(x, theta)\n        \n        # get the sigmoid of z\n        h = sigmoid(z)\n \n        # calculate the cost function, log loss\n        #J = (-1\/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h)))\n        \n        # let's add L2 regularization\n        # c is L2 regularizer term\n        J = (-1\/m) * ((np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1-h))) + (c * np.sum(theta)))\n        \n        # update the weights theta\n        theta = theta - (alpha \/ m) * np.dot((x.T), (h - y))\n   \n    J = float(J)\n    return J, theta","ceca9265":"# set the seed in numpy\nnp.random.seed(1)\n# Apply gradient descent of logistic regression\n# 0.1 as added L2 regularization term\nJ, theta = gradientDescent(train_X, np.array(train_Y).reshape(-1,1), np.zeros((3, 1)), 1e-7, 1000, 0.1)\nprint(f\"The cost after training is {J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")","ccace2fa":"# predict for the features from learned theata values\ndef predict_tweet(x, theta):\n    \n    # make the prediction for x with learned theta values\n    y_pred = sigmoid(np.dot(x, theta))\n    \n    return y_pred","fa5ed70c":"# predict for the test sample with the learned weights for logistics regression\npredicted_probs = predict_tweet(test_X, theta)\n# assign the probability threshold to class\npredicted_labels = np.where(predicted_probs > 0.5, 1, 0)\n# calculate the accuracy\nprint(f\"Own implementation of logistic regression accuracy is {len(predicted_labels[predicted_labels == np.array(test_Y).reshape(-1,1)]) \/ len(test_Y)*100:.2f}\")","7e6eeb82":"# scikit learn logiticsregression and accuracy score metric\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nclf = LogisticRegression(random_state=42, penalty='l2')\nclf.fit(train_X, np.array(train_Y).reshape(-1,1))\ny_pred = clf.predict(test_X)\ny_pred_probs = clf.predict(test_X)","715b463f":"print(f\"Scikit learn logistic regression accuracy is {accuracy_score(test_Y , y_pred)*100:.2f}\")","b7dd1b6d":"Shuffle the corpus and will split the train and test set.","a686d18a":"Regularization is a technique to solve the problem of overfitting in a machine learning algorithm by penalizing the cost function. There will be an additional penalty term in the cost function. \nThere are two types of regularization techniques:\n\n* Lasso (L1-norm) Regularization\n* Ridge (L2-norm) Regularization \n\n<strong>Lasso Regression (L1)<\/strong> L1-norm loss function is also known as least absolute errors (LAE). $\u03bb*\u2211 |w| $ is a regularization term. It is a product of $\u03bb$ regularization term with an absolute sum of weights. The smaller values indicate stronger regularization.\n\n<strong>Ridge Regression (L2)<\/strong> L2-norm loss function is also known as least squares error (LSE). $\u03bb*\u2211 (w)\u00b2$ is a regularization term. It is a product of $\u03bb$ regularization term with the squared sum of weights. The smaller values indicate stronger regularization.\n\nYou could notice, is that makes a huge difference. Yes, it does well. The main difference is what type of regularization term you are adding in the cost function to minimize the error.\n\n> L2 (Ridge) shrinks all the coefficient by the same proportions but it doesn't eliminate any features, while L1 (Lasso) can shrink some coefficients to zero, and also performs feature selection.\n","9d217498":"### <font color=\"#007bff\"><b>4.5 Regularization<\/b><\/font><br><a id=\"4.5\"><\/a>","e1030969":"The cost function used in logistic regression is:\n$$ -\\frac{1}{M}(ylog(p) - (1-y) log(1-p))  \\tag{3}$$\n\nThis is the <strong>Log loss of binary classification.<\/strong> The average of the log loss across all training samples is calculated in logistic regression, the equation ${3}$ modified for all the training samples as follows:\n\n$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\tag{4}$$\n* $m$ is the number of training examples\n* $y^{(i)}$ is the actual label of the i-th training example.\n* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n\nThe loss function for a single training example is\n$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n\n* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n* When the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0. \n* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss.\n","85caeb5b":"### <font color=\"#007bff\"><b>4.3 Cost function<\/b><\/font><br><a id=\"4.3\"><\/a>","e34155b1":"### <font color=\"#007bff\"><b>2.1 Preprocess the text<\/b><\/font><br><a id=\"2.1\"><\/a>","0db27b69":"Great!!!. The results are pretty much close.\n\nFinally, we implemented the logistic regression on our own and also tried with in-build Scikit learn logistic regression getting similar accuracy. But, this approach of feature extraction is very simple and intuitive.\n\n<font size=+2>\ud83d\ude4f <i>Thanks for reading the kernal. Kindly leave your thoughts or any suggestions in the comments.<\/i><\/font>","bb55bf90":"Lets keep the 80% data for training and 20% data samples for testing","4f334ea3":"<img src=\"https:\/\/miro.medium.com\/max\/568\/1*hlJ4VjDAYT32-1qSgDPinA.png\" title=\"logistic regression intuition\">\n\nThe goal of this kernel is to implement logistic regression from scratch for sentiment analysis using the twitter dataset. We will be mainly focusing on building blocks of logistic regression on our own. This kernel can provide an in-depth understanding of <strong><i>how logistic regression works internally<\/i><\/strong>. This notebook has written in medium article as well. You can find it [here](https:\/\/towardsdatascience.com\/understand-logistic-regression-from-scratch-430aedf5edb9).\n\nGiven a tweet, it will be classified if it has <strong>positive sentiment \ud83d\udc4d or negative sentiment \ud83d\udc4e<\/strong>. It is very useful for beginners and others as well.\n\n<font size=-1><i>Leave your comments or thought about this kernal for improvements. This is my first notebook. I am learning by doing it. Your feedback is highly appreciated to boost my confidence.<\/i><\/font>","9bfcc970":"## <font color=\"#007bff\"><b>6. Test our logistic regression<\/b><\/font><br><a id=\"6\"><\/a>","f440a06a":"### <font color=\"#007bff\"><b>4.2 Sigmoid<\/b><\/font><br><a id=\"4.2\"><\/a>\n\n* The sigmoid function is defined as: \n\n$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{2}$$\n\nIt maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a <strong>probability<\/strong>. ","ceb993a5":"## <font color=\"#007bff\"><b>2. Load the data<\/b><\/font><br><a id=\"2\"><\/a> ","351f4b07":"Take a look at sample train features. \n* The 0th index is a bias term added.\n* 1st index is representing positive word frequency\n* 2nd index is representing negative word frequency","a2dd7504":"##### In the following code will add L2 regularization","67ca0042":"Gradient Descent is a algorithm used for <strong> updating the weights $\\theta$<\/strong> iteratively to minimize the objective function (cost). Why we need to update the weigths iteratively because,\n\n> At initial random weights, model doesn't learn anything much. To imporve the prediction we need to learn the from the data with multiple iteration and tune the random weights accordingly.\n \nThe gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n\n$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n* 'i' is the index across all 'm' training examples.\n* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n\n* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n","fe1cc965":"### <font color=\"#007bff\"><b>3.2. Extracting simple features for our model<\/b><\/font><br><a id=\"3.2\"><\/a>\n\n\n* Given a list of tweets, we will be extracting two features.\n* The first feature is the number of positive words in a tweet.\n* The second feature is the number of negative words in a tweet.\n    \nThis seems to be simple, isn\u2019t it? Perhaps yes. We are not representing our features to the sparse matrix. Will use the simplest features for our analysis.","e3177081":"## <font color=\"#007bff\"><b>3. Extract features from text<\/b><\/font><br><a id=\"3\"><\/a>","ee904829":"### <font color=\"#007bff\"><b>4.4 Gradient Descent<\/b><\/font><br><a id=\"4.4\"><\/a>","a24f7df0":"\n### <font color=\"#007bff\"><b>3.3 Train and Test split<\/b><\/font><br><a id=\"3.3\"><\/a>","77b23e2e":"Now, we have various methods to represent features for our twitter corpus. Some of the basic and powerful techniques are,\n\n    - 1. CountVectorizer \n    - 2. TF-IDF feature\n\n<h4>1. CountVectorizer<\/h4>\nThe count vectorizer indicates the sparse matrix and the value can be the frequency of the word. Each column is a unique token in our corpus.\n\n\n> The sparse matrix dimension would be `no of unique tokens in the corpus * no of sample tweets`. \n\nExample: `\ncorpus = [\n     'This is the first document.',\n     'This document is the second document.',\n     'And this is the third one.',\n     'Is this the first document?',\n ]\n` and the CountVectorizer representation is\n\n`[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n`\n\n<h4>2. TF-IDF (Term Frequency - Inverse Document Frequence)<\/h4>\nTF-IDF statistical measure that evaluates how relevant a word is to a document in a collection of documents. TF-IDF is computed as follows:\n\n$$ tiidf(t, d, D) = tf(t,d)*idf(t,D) $$\n\n<strong>Term Frequency:<\/strong> term frequency $ tf(t,d) $, the simplest choice is to use the frequency of a term (word) in a document.\n<strong>Inverse Document Frequency:<\/strong> $idf(t,D)$ measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the <strong> logarthmic scaled <\/strong> of inverse fraction of the document that contain word. Definition is as per [Wiki](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf).\n","e38bccca":"Now, Let\u2019s see how logistic regression works and gets implemented.\n\nMost of the time, when you hear about logistic regression you may think, it is a regression problem. No, it is not, Logistic regression is a classification problem and it is a non-linear model.\n\n![overview of logistic regression](attachment:image.png)\n\nAs shown in the above picture, there are 4 stages for most of the ML algorithms,\n* Step 1. Initialize the weights\n    - Random weights initialized\n* Step 2. Apply function \n    - Calculate the sigmoid\n* Step 3. Calculate the cost (objective of the algorithm)\n    - Calculate the log-loss for binary classification\n* Step 4. Gradient Descent \n    - Update the weights iteratively till finding the minimum cost\n\nLogistic regression takes a linear regression and applies a <strong>sigmoid<\/strong> to the output of the linear regression. So, It produces the probability of each class and it sums up to 1.\n\n<h6><strong>Regression:<\/strong><\/h6>\nSingle linear regression equation as follows:\n\n$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N \\tag{1}$$\n* Note that the $\\theta$ values are \"weights\" \n* $x_0, x_1, x_2,... x_N$ is input features\n\nYou may think of how complicated equation it is. We need to multiply all the weigths with each feature at `ith` position then sums up all.\n\n> Fortunately, <strong>Linear algebra<\/strong> brings this equation with ease of operation. Yes, It is a matrix `dot` product. You can apply the dot product of features and weights to find the $z$.\n> \n<h6><strong>Logistic regression:<\/strong><\/h6>\nApplying the <strong>sigmoid<\/strong> function to equation $1$ becames <strong>logits<\/strong> function. \n\n$$ h(z) = \\frac{1}{1+\\exp^{-z}} $$\n","35383175":"## <font color=\"#007bff\"><b>4. Implementation of Logistic Regression<\/b><\/font><br><a id=\"4\"><\/a>","a8cc46e6":"It is time to test our logistic regression function on test data that model has not seen before. \n\nPredict whether a tweet is positive or negative. \n\n* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n\n$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$","9234f1a0":"<strong>Version 1.0<\/strong>:  Added Regularization in Logistic Regression and Scikit learn Logistic regression accuracy","be02ddd3":"## <font color=\"#007bff\"><b>1. Objective<\/b><\/font><br><a id=\"1\"><\/a>","d8951524":"## <font color=\"#007bff\"><b>5. Train model<\/b><\/font><br><a id=\"5\"><\/a>","afbf05c2":"Let\u2019s take a look at what output got after preprocessing tweets. It\u2019s good that we were able to process the tweets successfully.","42d76506":"* Given the text, It is very important to represent `features (numeric values)` such a way that we can feed into the model.\n\n\n### <font color=\"#007bff\"><b>3.1 Create a Bag Of Words (BOW) representation<\/b><\/font><br><a id=\"3.1\"><\/a>\n\nBOW represents the word and its frequency for each class. We will create a `dict` for storing the frequency of  `positive` and `negative` classes for each word.Let\u2019s indicate a `positive` tweet is `1` and the `negative` tweet is `0`.\n\nThe `dict` key is a tuple containing the `(word, y)` pair. The word is processed word and `y` indicates the label of the class. The `dict` value represents the frequency of the word for class `y`.\n\nExample:\n    # word good occurs 32 time in the 1.0 (positive) class \n    {(\"good\", 1.0) : 32}\n    \n    # word bad occurs 45 time in the 0 (negative) class \n    {(\"bad\", 0) : 45}","8a057f3d":"## <font color=\"#007bff\"><b>7. Test with Scikit learn logistic regression<\/b><\/font><br><a id=\"7\"><\/a>","bb583b74":"### <font color=\"#007bff\"><b>4.1 Overview<\/b><\/font><br><a id=\"4.1\"><\/a>","deb4492f":"* The `twitter_samples` contains 5,000 positive tweets and 5,000 negative tweets. Total count of 10,000 tweets.  \n* We have the same no of data samples in each class.\n\n* It is a balanced dataset.","20dddf34":"* Tweets may have URLs, numbers, and special characters. Hence, we need to preprocess the text.","3d69f22c":"Preprocessing is one of the important steps in the pipeline. It includes cleaning and removing unnecessary data before building a machine learning model. \n\nPreprocessing steps:\n\n* Tokenizing the string\n   - Convert the tweet into lowercase and split the tweets into tokens(words)\n* Removing stop words and punctuation\n    - Will remove some strings commonly used on the twitter platform like the hashtag, retweet marks, hyperlinks, numbers and email address\n* Stemming\n    - This is the process of converting a word to it's most general form. This helps in reducing the size of our vocabulary. For example, the stemmed word `engag` will have the following different words,    \n        - <strong>engag<\/strong>ement\n        - <strong>engag<\/strong>ed\n        - <strong>engag<\/strong>ing\n\nLet's see how we can implement this.","48eba87d":"## Table of Content\n\n* [1. Objective](#1) \n* [2. Load the data](#2)\n    - [2.1 Preprocess the text](#2.1)\n* [3. Extract features from text](#3)\n    - [3.1 Create a Bag Of Words (BOW) representation](#3.1)\n    - [3.2. Extracting simple features for our model](#3.2)\n    - [3.3 Train and Test split](#3.3)\n* [4. Implementation of Logistic Regression](#4)\n    - [4.1 Overview](#4.1)\n    - [4.2 Sigmoid](#4.2)\n    - [4.3 Cost function](#4.3)\n    - [4.4 Gradient Descent](#4.4)\n    - [4.5 Regularization](#4.5)\n* [5. Train model](#5)\n* [6. Test our logistic regression](#6)\n* [7. Test with Scikit learn logistic regression](#7)\n\n","9c104ecf":" ## <center><i>Learn the algorithm by implementing it on our own.<\/i><\/center>"}}