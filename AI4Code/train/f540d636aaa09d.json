{"cell_type":{"e8c63624":"code","be19e8dd":"code","14a4105d":"code","2f6119fe":"code","e3ab7069":"code","9e0f0f2e":"code","d1868595":"code","a6953435":"code","6ac0b22d":"code","815553dd":"code","c3494762":"code","02359f31":"code","1a286df1":"code","c4636809":"code","44f6871d":"code","9a15780d":"code","d9350c67":"code","e8c1a091":"code","8d066255":"code","98b7e4ee":"code","64669afe":"code","233f5232":"code","ec1b7915":"code","81a3a619":"code","cc34ee2e":"code","8c024520":"code","f6fa1692":"code","37b1f269":"code","e0ba1b6c":"code","90306c01":"code","790f42ae":"code","40792727":"code","20a8eda7":"code","459d499c":"code","c7c76413":"markdown","60024186":"markdown","a944d6cc":"markdown","7953d9a9":"markdown","9d4d40c2":"markdown","631fa4ab":"markdown","256963b5":"markdown","fd9c15b5":"markdown","c82b6213":"markdown","7c3e4de7":"markdown","fed09570":"markdown","15e8282c":"markdown"},"source":{"e8c63624":"import numpy as np\nimport pandas as pd\nimport re\n\ntrain=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","be19e8dd":"train","14a4105d":"test_keywords = test[pd.notnull(test.keyword)].keyword.unique()\ntrain_keywords = train[pd.notnull(train.keyword)].keyword.unique()\ntrain_locations = train[pd.notnull(train.location)].location.unique()\ntest_locations = test[pd.notnull(test.location)].location.unique()","2f6119fe":"diff = set(train_locations) - set(test_locations)\nlist(diff)[:10], len(diff)","e3ab7069":"(test_keywords == train_keywords).all()","9e0f0f2e":"for i in range(100):\n    print(train.iloc[i,3])","d1868595":"train.loc[pd.isnull(train.location), 'location'] = ''\ntest.loc[pd.isnull(test.location), 'location'] = ''","a6953435":"for i in range(train.shape[0]):\n    train.iloc[i,3]=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \", train.iloc[i,3]).split())","6ac0b22d":"for i in range(test.shape[0]):\n    test.iloc[i,3]=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \", test.iloc[i,3]).split())","815553dd":"for i in range(train.shape[0]):\n    train.iloc[i, 2]= ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \", train.iloc[i,2]).split())","c3494762":"for i in range(test.shape[0]):\n    train.iloc[i, 2]= ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \", test.iloc[i,2]).split())","02359f31":"for i in range(10):\n    print(train.iloc[137+i,3])","1a286df1":"for i in range(10):\n    print(train.iloc[137+i,2])","c4636809":"!pip3 install tqdm\n!pip3 install transformers","44f6871d":"import transformers\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')","9a15780d":"import tqdm\n\ndef create_bert_input_features(tokenizer, docs, max_seq_length):\n    \n    all_ids, all_masks = [], []\n    for doc in tqdm.tqdm(docs, desc=\"Converting docs to features\"):\n        tokens = tokenizer.tokenize(doc)\n        if len(tokens) > max_seq_length-2:\n            tokens = tokens[0 : (max_seq_length-2)]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        ids = tokenizer.convert_tokens_to_ids(tokens)\n        masks = [1] * len(ids)\n        # Zero-pad up to the sequence length.\n        while len(ids) < max_seq_length:\n            ids.append(0)\n            masks.append(0)\n        all_ids.append(ids)\n        all_masks.append(masks)\n    encoded = np.array([all_ids, all_masks])\n    return encoded","d9350c67":"from sklearn.model_selection import train_test_split","e8c1a091":"joined = train['location'] + ' ' + train['text'] ","8d066255":"train['location'][101], joined[101]","98b7e4ee":"train_X, val_X, train_Y, val_Y = train_test_split(train['location'] + ' ' + train['text'], train['target'], test_size=0.05, random_state=42)","64669afe":"train_X[:10]","233f5232":"train_X, val_X, train_Y, val_Y = train_X.values, val_X.values, train_Y.values, val_Y.values","ec1b7915":"test_X = test['location'] + ' ' + test['text']\n#test_X = test_X.value","81a3a619":"MAX_SEQ_LENGTH = 500\n\ntrain_features_ids, train_features_masks = create_bert_input_features(tokenizer, train_X, \n                                                                      max_seq_length=MAX_SEQ_LENGTH)\nval_features_ids, val_features_masks = create_bert_input_features(tokenizer, val_X, \n                                                                  max_seq_length=MAX_SEQ_LENGTH)\n#test_features = create_bert_input_features(tokenizer, test_reviews, max_seq_length=MAX_SEQ_LENGTH)\nprint('Train Features:', train_features_ids.shape, train_features_masks.shape)\nprint('Val Features:', val_features_ids.shape, val_features_masks.shape)","cc34ee2e":"test_features_ids, test_features_masks = create_bert_input_features(tokenizer, test_X, \n                                                                    max_seq_length=MAX_SEQ_LENGTH)\nprint('Test Features:', test_features_ids.shape, test_features_masks.shape)","8c024520":"import tensorflow as tf","f6fa1692":"inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_ids\")\ninp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_masks\")\ninputs = [inp_id, inp_mask]\n\nhidden_state = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')(inputs)[0]\npooled_output = hidden_state[:, 0]    \ndense1 = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\ndrop1 = tf.keras.layers.Dropout(0.3)(dense1)\ndense2 = tf.keras.layers.Dense(256, activation='relu')(drop1)\ndrop2 = tf.keras.layers.Dropout(0.3)(dense2)\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)\n\n\nmodel = tf.keras.Model(inputs=inputs, outputs=output)\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-6, \n                                           epsilon=1e-08), \n              loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.AUC()])","37b1f269":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                      patience=2,\n                                      restore_best_weights=True,\n                                      verbose=1)","e0ba1b6c":"model.fit([train_features_ids, \n           train_features_masks], train_Y, \n          validation_data=([val_features_ids, \n                            val_features_masks], val_Y),\n          epochs=10, \n          batch_size=20, \n          shuffle=True,\n          callbacks=[es],\n          verbose=1)","90306c01":"predictions = [1 if pr > 0.5 else 0 \n                   for pr in model.predict([val_features_ids, \n                                            val_features_masks], batch_size=200, verbose=0).ravel()]","790f42ae":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\nprint('null accuracy:', max(sum(val_Y)\/val_Y.shape[0],1-sum(val_Y)\/val_Y.shape[0]))\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(val_Y, predictions)*100))\nprint(\"roc auc:\", roc_auc_score(val_Y, predictions))\nprint(classification_report(val_Y, predictions))\npd.DataFrame(confusion_matrix(val_Y, predictions))","40792727":"test_Y=model.predict([test_features_ids, test_features_masks], batch_size=200, verbose=0)\n\ntest_label=[]\n\nfor i in range(test_Y.shape[0]):\n    if test_Y[i]>=0.5:\n        test_label.append(1)\n    else:\n        test_label.append(0)","20a8eda7":"submission=pd.DataFrame({'id': test['id'], 'target':test_label})\nprint(submission.head(10))\n\nfilename = 'submission_nlp_tweets_bert.csv'\n\nsubmission.to_csv(filename,index=False)","459d499c":"from IPython.display import FileLink\nFileLink('.\/submission_nlp_tweets_bert.csv')","c7c76413":"**create bert input features**","60024186":"**fit and tune model**","a944d6cc":"The author Fanglida Yan has used code from these references in the notebook. <br>\nBERT for classfication: https:\/\/github.com\/dipanjanS\/deep_transfer_learning_nlp_dhs2019\/blob\/master\/notebooks\/6%20-%20Transformers%20-%20DistilBERT.ipynb <br>\nremove @, # and http:\/\/... : https:\/\/stackoverflow.com\/questions\/8376691\/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression <br>\n\n0. feature preprocessing <br>\n1. split train and cross validation sets <br>\n2. create features for BERT<br>\n3. build the model in Keras <br>\n4. model tuning and cross validation <br>\n5. make prediction for test set <br>","7953d9a9":"**read in the data**","9d4d40c2":"**1. remove #, @, punctuations and weblinks** <br>\n**copied from https:\/\/stackoverflow.com\/questions\/8376691\/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression**","631fa4ab":"**tweets look much more clean**","256963b5":"**take a look at the tweets**","fd9c15b5":"**predict the test set**","c82b6213":"**cross validation**","7c3e4de7":"**18. submit**","fed09570":"**define model**","15e8282c":"**seperate train and test**"}}