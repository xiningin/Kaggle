{"cell_type":{"6bb6c036":"code","2ab2f57a":"code","3bbd5355":"code","d3bd59b0":"code","495539df":"code","eef04552":"code","df9dab68":"code","09c44251":"code","9d657aa4":"code","9bfd4b8e":"code","b8646431":"code","e363d480":"code","1a59fa02":"code","cfbf025f":"code","ccb7565d":"code","8078ffc6":"code","e9838ffa":"code","d53bcc2a":"code","89d5216e":"code","e343b228":"code","4eb87cbb":"code","ebd82381":"code","44a34810":"code","73d5ab37":"code","ea8df583":"code","05c5ec95":"code","b40d9391":"code","37cb9cdd":"code","7b6b0d9c":"code","9bdc887e":"code","c8fcc2c6":"code","0d30ed79":"code","1d6de684":"code","e2ba6206":"code","b02f87e1":"code","09302010":"code","54767141":"code","f94a7492":"code","547ddd3c":"code","05acd01e":"code","084acb84":"code","20fd0d38":"code","ceb8f279":"code","fcfd8613":"code","c76f2eb7":"code","55fb32fd":"code","b1251f6c":"code","3829bb96":"code","8dcb9660":"code","810bd857":"code","0747acee":"code","8a6de36b":"code","a5cdb7ca":"code","94b697d3":"code","aaabeab1":"code","aa4a29e4":"code","ec425a4e":"code","0d15e54a":"code","a7e66710":"code","73d27cee":"code","ad03b187":"code","ac6eaad4":"code","2bbb7a8d":"code","c46c058e":"code","81918fa5":"code","b3418e4e":"code","fb0dcdd9":"code","a0cc74d3":"code","013fb3e7":"code","97abc787":"markdown","e35caaeb":"markdown","ee271c3a":"markdown","d43393b5":"markdown","20c63019":"markdown","08326ae9":"markdown","55b99924":"markdown","a22bb707":"markdown","ce3b752d":"markdown","caef45fa":"markdown","976fcc04":"markdown","e0b21a3f":"markdown","36a1d7d2":"markdown","b2aceb9b":"markdown","b553a77c":"markdown","43376db7":"markdown","416d5799":"markdown","ea9f410c":"markdown","cbd4edc7":"markdown","49f3c078":"markdown","890e3784":"markdown","feb91065":"markdown","71d015ea":"markdown","6845ba12":"markdown","b9f888a4":"markdown","5dbecd86":"markdown","908bc7fe":"markdown","fe94026d":"markdown","4ef695c9":"markdown","ffee9278":"markdown","b0eb5bd2":"markdown","a013df29":"markdown","28f45b59":"markdown","0086cac4":"markdown","2db491e5":"markdown","c2d3c94a":"markdown","8aabd7a5":"markdown","7270c53a":"markdown","00fda76e":"markdown","5560c526":"markdown","1aa26ab6":"markdown","c26856dd":"markdown","9a8b4d0a":"markdown","a63a48bb":"markdown","278df9c7":"markdown","3f4b1876":"markdown","d91a765d":"markdown","97e8da69":"markdown","d361ae13":"markdown"},"source":{"6bb6c036":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom math import sqrt\n\nfrom sklearn.preprocessing import StandardScaler, robust_scale, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_curve, auc\nfrom sklearn.preprocessing import LabelEncoder, label_binarize\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers \nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.utils import to_categorical\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2ab2f57a":"df = pd.read_csv(\"..\/input\/fc-bayern-face-recognation\/Bayern.csv\") # read the data\ndf.head()","3bbd5355":"print(f\"The data contains {len(df)} images\")\nprint(f\"Each image is {int(sqrt(df.shape[1] - 1))} * {int(sqrt(df.shape[1]-1))} Pixels\")\nprint(f\"Data contains {len(df.Target.unique())} Classes\")","d3bd59b0":"df.Target.value_counts()","495539df":"plt.figure(figsize=(18,12))\nplt.subplot(2,2,1)\nsns.countplot(x='Target', data=df);\n\nplt.subplot(2,2,2)\ncoman = df.Target[df.Target == 0].count()\nkimmich = df.Target[df.Target == 1].count()\nlewa = df.Target[df.Target == 2].count()\nneuer = df.Target[df.Target == 3].count()\nsane = df.Target[df.Target == 4].count()\nweights = [coman, kimmich, lewa, neuer, sane]\nlabels = ['Coman', 'Kimmich', 'Lewandowski', 'Neuer', 'Sane']\nplt.pie(weights, labels=labels, autopct='%.2f%%', explode=[0.01,0.01,0.01,0.01,0.01])\nmy_circle = plt.Circle( (0,0), 0.4, color='white')\nplt.gcf().gca().add_artist(my_circle)\nplt.legend(bbox_to_anchor=(1, 1))\nplt.show()","eef04552":"X = df.drop(\"Target\", axis=1)\ny = df.Target","df9dab68":"X.shape","09c44251":"y.shape","9d657aa4":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=150)","9bfd4b8e":"print(f\"Number of Training data: {len(X_train)}\")\nprint(f\"Number of Testing data: {len(X_test)}\")","b8646431":"X_train.shape","e363d480":"y_train.shape","1a59fa02":"model_params = {\n    'svm': {\n        'model': SVC(gamma='auto'),\n        'params' : {\n            'C': [100, 10, 1.0, 0.1, 0.001],\n            'kernel': ['rbf','linear', 'poly', 'sigmoid']\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'max_features' : np.arange(1,21),\n            'max_features' : ['sqrt', 'log2'],\n            'n_estimators': [10,100,1000]\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(),\n        'params': {\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n            'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n            'C' : [100,10,1.0,0.1,0.01]\n        }\n    },\n    'KNN': {\n        'model' : KNeighborsClassifier(),\n        'params' : {\n            'n_neighbors' : np.arange(1,22),\n            'weights' : ['uniform', 'distance']\n        }\n    }\n}","cfbf025f":"scores = []\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(X, y)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \ndf = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf","ccb7565d":"def kfolds(model, model_name):\n    model = cross_val_score(model, X,y, cv=10)\n    model_score = np.average(model)\n    print(f\"{model_name} score on cross validation: {model_score * 100}%\")\n\ndef train(model, model_name):\n    model.fit(X_train, y_train)\n    model_train_score = model.score(X_train, y_train)\n    model_test_score = model.score(X_test, y_test)\n    print(f\"{model_name} model score on Training data: {model_train_score * 100}%\\n{model_name} model score on Testing data: {model_test_score * 100}%\")\n","8078ffc6":"def conf_matrix(model):\n    y_pred = model.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(10,8))\n    sns.heatmap(cm, annot=True);\n\ndef class_report(model):\n    y_pred = model.predict(X_test)\n    return classification_report(y_test, y_pred)\n\n\ndef roc(model):\n    y_binarize = label_binarize(y, classes=[0,1,2,3,4])\n    y_binarize.shape\n    n_classes = y_binarize.shape[1]\n    X_train, X_test, y_train, y_test = train_test_split(X,y_binarize, test_size=0.25, random_state=150)\n    classifier = OneVsRestClassifier(\n    model)\n    y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    plt.figure(figsize=(10,7))\n    lw = 2\n    plt.plot(\n        fpr[2],\n        tpr[2],\n        color=\"darkorange\",\n        lw=lw,\n        label=\"ROC curve (area = %0.2f)\" % roc_auc[2],\n    )\n    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver operating characteristic\")\n    plt.legend(loc=\"lower right\")\n    plt.show()","e9838ffa":"svm_model = SVC(kernel='linear', gamma='auto', C=100)","d53bcc2a":"kfolds(svm_model, \"SVM\")","89d5216e":"train(svm_model, \"SVM\")","e343b228":"conf_matrix(svm_model)","4eb87cbb":"print(class_report(svm_model))","ebd82381":"roc(svm_model)","44a34810":"rf_model = RandomForestClassifier(n_estimators=1000, random_state=70, max_features='sqrt')","73d5ab37":"kfolds(rf_model, \"Random Forest\")","ea8df583":"train(rf_model, \"Random Forest\")","05c5ec95":"conf_matrix(rf_model)","b40d9391":"print(class_report(rf_model))","37cb9cdd":"dt_model = DecisionTreeClassifier()","7b6b0d9c":"kfolds(dt_model, \"Decision Tree\")","9bdc887e":"train(dt_model, \"Decision Tree\")","c8fcc2c6":"conf_matrix(dt_model);","0d30ed79":"print(class_report(dt_model))","1d6de684":"bagg_model = BaggingClassifier(n_estimators=150)","e2ba6206":"kfolds(bagg_model, \"Bagging\")","b02f87e1":"train(bagg_model, \"Bagging\")","09302010":"conf_matrix(bagg_model)","54767141":"print(class_report(bagg_model))","f94a7492":"gb_model = GradientBoostingClassifier()","547ddd3c":"kfolds(gb_model, \"Boosting\")","05acd01e":"train(gb_model, \"Boosting\")","084acb84":"conf_matrix(gb_model)","20fd0d38":"print(class_report(gb_model))","ceb8f279":"knn = KNeighborsClassifier(n_neighbors=6, weights='distance')","fcfd8613":"kfolds(knn, \"KNN\")","c76f2eb7":"train(knn, \"KNN\")","55fb32fd":"conf_matrix(knn)","b1251f6c":"print(class_report(knn))","3829bb96":"FEATURES = [col for col in df.columns if col not in ['Target']]","8dcb9660":"X_nn = df[FEATURES].to_numpy()\ny_nn = df['Target'].to_numpy()","810bd857":"X_nn.shape","0747acee":"y_nn.shape","8a6de36b":"X_nn[0].shape","a5cdb7ca":"y_nn","94b697d3":"LE = LabelEncoder()\ny_nn_cat = to_categorical(LE.fit_transform(y_nn))","aaabeab1":"y_nn_cat.shape","aa4a29e4":"X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn,y_nn_cat, test_size=0.25, random_state=100)","ec425a4e":"def load_model(): \n    model = Sequential([\n        Dense(4096, activation ='relu', input_shape = [X.shape[1]]),\n        Dense(2048, activation ='relu'),\n        Dense(1024, activation ='relu'),\n        Dense(512, activation ='relu'),\n        Dense(5, activation='softmax'),\n    ])\n    model.compile(\n        optimizer=  tf.keras.optimizers.Adam(learning_rate = 0.0001),\n        loss='categorical_crossentropy',\n        metrics=['acc'],\n    )\n    return model\n    \nearly_stopping = callbacks.EarlyStopping(\n        patience=10,\n        min_delta=0,\n        monitor='val_loss',\n        restore_best_weights=True,\n        verbose=0,\n        mode='min', \n        baseline=None,\n    )\nplateau = callbacks.ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.2, \n            patience=4, \n            verbose=0,\n            mode='min')\n\n\nnn_model = load_model()\nhistory = nn_model.fit(  X_train_nn , y_train_nn,\n                validation_data = (X_test_nn , y_test_nn),\n                epochs = 1000,\n                batch_size = 10,\n                callbacks = [early_stopping , plateau]\n              )","0d15e54a":"nn_model.evaluate(X_test_nn, y_test_nn)","a7e66710":"plt.figure(figsize=(8,6))\nloss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1,18)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xticks(np.arange(1,20))\nplt.legend()\nplt.show()","73d27cee":"y_predicted = nn_model.predict(X_test_nn)","ad03b187":"y_predicted","ac6eaad4":"y_predicted_labels = [np.argmax(i) for i in y_predicted]","2bbb7a8d":"y_predicted_labels","c46c058e":"y_predicted_labels = np.array(y_predicted_labels)\ny_predicted_labels.shape","81918fa5":"X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn,y_nn, test_size=0.25, random_state=100)","b3418e4e":"y_test_nn.shape","fb0dcdd9":"cm = tf.math.confusion_matrix(labels=y_test_nn, predictions=y_predicted_labels)\ncm","a0cc74d3":"plt.figure(figsize=(9,7))\nsns.heatmap(cm, annot=True)","013fb3e7":"print(classification_report(y_test_nn, y_predicted_labels))","97abc787":"<a id='7.4'><\/a>\n## Decision Tree","e35caaeb":"<a id='7.1'><\/a>\n## SVM Model","ee271c3a":"<a id='7.4.4'><\/a>\n**Classification Report**","d43393b5":"<a id='3'><\/a>\n# Explore the Data","20c63019":"<a id='7.4.1'><\/a>\n**Cross Validation**","08326ae9":"<a id='4'><\/a>\n# Assign Feature and Target Variable","55b99924":"<a id='7.6.4'><\/a>\n**Classification Report**","a22bb707":"<a id='7.1.2'><\/a>\n**Score of the model**","ce3b752d":"<a id='7.6.3'><\/a>\n**Confusion Matrix**","caef45fa":"<a id='7.1.5'><\/a>\n**ROC Curve**","976fcc04":"<a id='7.7'><\/a>\n## KNN","e0b21a3f":"<a id='3.1'><\/a>\n**Information about data**","36a1d7d2":"<a id='7.3.4'><\/a>\n**Classification Report**","b2aceb9b":"<a id=\"7.8.2\"><\/a>\n**Accuracy of ANN**","b553a77c":"<a id='7.5.4'><\/a>\n**Classification Report**","43376db7":"<a id=\"1\"><\/a>\n# Data Description","416d5799":"<a id='7.7.1'><\/a>\n**Cross Validation**","ea9f410c":"<a id='7.1.4'><\/a>\n**Classification Report**","cbd4edc7":"<a id='7.5.1'><\/a>\n**Cross Validation**","49f3c078":"<a d='2'><\/a>\n# Import Necssaries Libraries","890e3784":"<a id='7.3.1'><\/a>\n**Cross Validation**","feb91065":"<a id='7.7.4'><\/a>\n**Classification Report**","71d015ea":"<a id=\"7.8.5\"><\/a>\n**Classification Report**","6845ba12":"<a id='3.2'><\/a>\n**Visualization**","b9f888a4":"<a id='7.1.3'><\/a>\n**Confusion Matrix**","5dbecd86":"<a id='7'><\/a>\n# Modeling","908bc7fe":"<a id='7.5.3'><\/a>\n**Confusion Matrix**","fe94026d":"<a id='7.6.1'><\/a>\n**Cross Validation**","4ef695c9":"***The data contains 5 Bayern Munich players and each player has about 100 random images collected from Google <br>\nso our data have 5 classes:***\n - Kingsley Coman\n - Joshua Kimmich\n - Robert Lewandowski\n - Manuel Neuer\n - Leory Sane<br><br>\n\n***Kingsley Coman Class 0***<br>\n***Joshua Kimmich Class 1***<br>\n***Robert Lewandowski Class 2***<br>\n***Manuel Neuer Class 3***<br>\n***Leory Sane Class 4***<br>","ffee9278":"<a id='7.5'><\/a>\n## Bagging","b0eb5bd2":"<a id=\"7.8.3\"><\/a>\n**Loss Curve**","a013df29":"<a id='7.6.2'><\/a>\n**Accuarcy of the model**","28f45b59":"<a id='7.4.2'><\/a>\n**Accuarcy of the model**","0086cac4":"<a id=\"7.8.1\"><\/a>\n**Build and Train the ANN**","2db491e5":"#### If you want to know more about how the data is collected, Cleaned and Preprocessed You can visit this repo on my github: https:\/\/github.com\/eyadayman12\/FC-Bayern-Munich-Face-Recognation","c2d3c94a":"# Table of Contents\n<a id=\"toc\"><\/a>\n- [1. Data Description](#1)\n- [2. Import Necssaries Libraries](#2)\n- [3. Explore the Data](#3)\n    - [3.1 Information about Data](#3.1)\n    - [3.2 Visualization](#3.2)\n- [4. Assign Features and target Variable](#4)\n- [5. Spliting the data into Training and Testing Data](#5)\n- [6. Hyperparameter Tuning](#6)\n- [7. Machine Learning Modeling](#7)\n    - [7.1 SVM Model](#7.1)\n        - [7.1.1 Cross Validation](#7.1.1)\n        - [7.1.2 Accuarcy of the model](#7.1.2)\n        - [7.1.3 Confusion Matrix](#7.1.3)\n        - [7.1.4 Classification Report](#7.1.4)\n        - [7.1.5 ROC Curve](#7.1.5)\n    - [7.2 Logistic Regression Model](#7.2)\n        - [7.2.1 Cross Validation](#7.2.1)\n        - [7.2.2 Accuarcy of the model](#7.2.2)\n        - [7.2.3 Confusion Matrix](#7.2.3)\n        - [7.2.4 Classification Report](#7.2.4)\n        - [7.2.5 ROC Curve](#7.2.5)\n    - [7.3 Random Forest Model](#7.3)\n        - [7.3.1 Cross Validation](#7.3.1)\n        - [7.3.2 Accuarcy of the model](#7.3.2)\n        - [7.3.3 Confusion Matrix](#7.3.3)\n        - [7.3.4 Classification Report](#7.3.4)\n    - [7.4 Decision Tree Model](#7.4)\n        - [7.4.1 Cross Validation](#7.4.1)\n        - [7.4.2 Accuarcy of the model](#7.4.2)\n        - [7.4.3 Confusion Matrix](#7.4.3)\n        - [7.4.4 Classification Report](#7.4.4)\n    - [7.5 Bagging](#7.5)\n        - [7.5.1 Cross Validation](#7.5.1)\n        - [7.5.2 Accuarcy of the model](#7.5.2)\n        - [7.5.3 Confusion Matrix](#7.5.3)\n        - [7.5.4 Classification Report](#7.5.4)\n    - [7.6 Boosting](#7.6)\n        - [7.6.1 Cross Validation](#7.6.1)\n        - [7.6.2 Accuarcy of the model](#7.6.2)\n        - [7.6.3 Confusion Matrix](#7.6.3)\n        - [7.6.4 Classification Report](#7.6.4)\n    - [7.7 KNN](#7.7)\n        - [7.7.1 Cross Validation](#7.7.1)\n        - [7.7.2 Accuarcy of the model](#7.7.2)\n        - [7.7.3 Confusion Matrix](#7.7.3)\n        - [7.7.4 Classification Report](#7.7.4)\n    - [7.8 ANN](#7.8)\n        - [7.8.1 Build and Train the ANN](#7.8.1)\n        - [7.8.2 Accuarcy of the model](#7.8.2)\n        - [7.8.3 Loss curve](#7.8.3)\n        - [7.8.4 Confusion Matrix](#7.8.4)\n        - [7.8.5 Classification Report](#7.8.5)","8aabd7a5":"<a id='7.3'><\/a>\n## Random Forest","7270c53a":"<a id='7.5.2'><\/a>\n**Accuarcy of the model**","00fda76e":"<a id='7.8'><\/a>\n## ANN","5560c526":"<a id='7.3.3'><\/a>\n**Confusion Matrix**","1aa26ab6":"<a id='7.7.2'><\/a>\n**Accuarcy of the model**","c26856dd":"<a id='7.7.3'><\/a>\n**Confusion Matrix**","9a8b4d0a":"<a id='7.1.1'><\/a>\n**Cross Validation**","a63a48bb":"<a id='7.4.3'><\/a>\n**Confusion Matrix**","278df9c7":"<a id='7.6'><\/a>\n## Boosting","3f4b1876":"<a id='7.3.2'><\/a>\n**Accuarcy of the model**","d91a765d":"<a id='5'><\/a>\n# Splitting the data into Training and Testing Data","97e8da69":"<a id='6'><\/a>\n# Hyperparameter Tuning","d361ae13":"<a id=\"7.8.4\"><\/a>\n**Confusion Matrix**"}}