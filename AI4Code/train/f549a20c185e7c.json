{"cell_type":{"837ba033":"code","915dbbd2":"code","35d71a5f":"code","6f6eb976":"code","372ca5f2":"code","2866de27":"code","7435c1de":"code","624b8ca4":"code","96df7d7d":"code","8753112b":"code","9816437b":"code","8d6776c4":"code","398ded07":"code","fe83afb6":"code","e13faf55":"code","75b91587":"code","23e9f428":"code","19022651":"code","65746e4d":"code","1889e2d3":"code","7469a901":"code","ae04296d":"code","f16ee42b":"code","2b6adaa0":"code","f8f2742e":"code","4337be60":"code","b4760d6e":"code","6196bb07":"code","c62a2d7e":"code","b312c49b":"code","13624e8b":"code","5d96b89b":"code","290ef4a8":"code","10485ade":"code","cb45b39b":"code","f208f513":"code","665a5b16":"code","34c812b2":"code","87708bb8":"code","a17c0d44":"code","6142ce8d":"code","25c3a210":"markdown","b2570d4a":"markdown","0640211c":"markdown","22c6ab25":"markdown","332396ec":"markdown","384ef70d":"markdown","aa47356a":"markdown","d2bc6d3c":"markdown","a1770a50":"markdown","78451abd":"markdown","a9457a0f":"markdown","dac8268e":"markdown","998acc1c":"markdown","e241ecb1":"markdown","53a654e7":"markdown","0afcfca1":"markdown","06f9cc24":"markdown","b6444be8":"markdown","8cb4a7f3":"markdown","cde32cac":"markdown","4f41ecdf":"markdown","70023dc4":"markdown","51394150":"markdown","61ab8240":"markdown","d3a9cf7b":"markdown","0c0ea79c":"markdown","268a629a":"markdown"},"source":{"837ba033":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nfrom textwrap import wrap\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#print(os.listdir(\"..\/input\"))\nimport datetime as dt\nimport matplotlib.colors\n\n# fastai\nfrom fastai.text.all import *\n\n# set dpi fpr better visuals\n\nplt.rcParams['figure.dpi'] = 150\nplt.rcParams[\"font.family\"] = \"monospace\"","915dbbd2":"path = Path('\/kaggle\/input\/')\npath.ls()","35d71a5f":"vaccine_tweets = pd.read_csv(path\/'all-covid19-vaccines-tweets\/vaccination_all_tweets.csv')\ntweets = pd.read_csv(path\/'complete-tweet-sentiment-extraction-data\/tweet_dataset.csv')","6f6eb976":"vaccine_tweets.shape","372ca5f2":"vaccine_tweets.head(1)","2866de27":"tweets.head(3)","7435c1de":"tweets.isnull().sum()","624b8ca4":"# Code via https:\/\/www.kaggle.com\/garyongguanjie\/comments-analysis\ndef de_emojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\n# Code via https:\/\/www.kaggle.com\/pawanbhandarkar\/generate-smarter-word-clouds-with-log-likelihood\ndef tweet_cleaner(df, text_col='text'):\n    df['orig_text'] = df[text_col]\n    # Remove twitter handles\n    df[text_col] = df[text_col].apply(lambda x:re.sub('@[^\\s]+','',x))\n    # Remove URLs\n    df[text_col] = df[text_col].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n    # Remove emojis\n    df[text_col] = df[text_col].apply(de_emojify)\n    # Remove hashtags\n    df[text_col] = df[text_col].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n    return df[df[text_col]!='']","96df7d7d":"# Rename columns for usability \/ aesthetics. The function above refers to 'text' column too, which we can overwrite, but this looks nicer\n# This will also alow us to append the data later\ntweets = tweets[['old_text','new_sentiment']].rename(columns={'old_text':'text','new_sentiment':'sentiment'})\n\n# Add in a column in our df for sentiments - for now this will be a nan value, this is what we will predict!\nvaccine_tweets['sentiment'] = np.nan\n\n# Run our data through the functions we created above\ntweets = tweet_cleaner(tweets)\nvaccine_tweets = tweet_cleaner(vaccine_tweets)","8753112b":"df_lm = tweets[['text', 'sentiment']].append(vaccine_tweets[['text', 'sentiment']])\ndf_clas = df_lm.dropna(subset=['sentiment'])\nprint(len(df_lm), len(df_clas))","9816437b":"df_clas['sentiment'].value_counts()","8d6776c4":"dls_lm = TextDataLoaders.from_df(df_lm,\n                                 text_col='text',\n                                 is_lm=True, \n                                 valid_pct=0.1)","398ded07":"dls_lm.show_batch(max_n=3)","fe83afb6":"learn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3, \n    metrics=[accuracy, Perplexity()]).to_fp16()","e13faf55":"learn.fit_one_cycle(1, 2e-2)","75b91587":"learn.save('1epoch')","23e9f428":"learn.unfreeze()\nlearn.lr_find()","19022651":"learn.fit_one_cycle(5, 1e-4)","65746e4d":"learn.save('model2')\n\nlearn.save_encoder('model_finetuned')","1889e2d3":"Text = \"The vaccine\"\nNumber_of_words = 15     # limit the words in the sentence\nNumber_of_sentences = 2  # how many sentences\/samples\npreds = [learn.predict(Text, Number_of_words, temperature=0.75) # temperature is an element of randomness so we don't get the same predictions\n         for _ in range(Number_of_sentences)]\n\nprint(\"\\n\".join(preds))","7469a901":"dls_clas = DataBlock(\n    blocks = (TextBlock.from_df('text', seq_len=dls_lm.seq_len, vocab=dls_lm.vocab), CategoryBlock),\n    get_x=ColReader('text'),\n    get_y=ColReader('sentiment'),\n    splitter=RandomSplitter()\n).dataloaders(df_clas, bs=128, seq_len=72)","ae04296d":"dls_clas.show_batch(max_n=3)","f16ee42b":"learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n                                metrics=accuracy).to_fp16()","2b6adaa0":"learn = learn.load_encoder('model_finetuned')","f8f2742e":"learn.fit_one_cycle(1, 2e-2)","4337be60":"learn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2))","b4760d6e":"learn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3))","6196bb07":"learn.unfreeze()\nlearn.fit_one_cycle(3, slice(1e-3\/(2.6**4),1e-3))","c62a2d7e":"learn.save('final_classifier')","b312c49b":"predictions_dl = dls_clas.test_dl(vaccine_tweets['text'])\npredictions = learn.get_preds(dl=predictions_dl)","13624e8b":"predictions","5d96b89b":"predictions[0].argmax(dim=-1)","290ef4a8":"vaccine_tweets['sentiment'] = predictions[0].argmax(dim=-1)\n\n# Map results to text \n\nvaccine_tweets['sentiment'] = vaccine_tweets['sentiment'].map({0:'Negative', 1:'Neutral', 2:'Positive'})\n\n# Convert dates\nvaccine_tweets['Date'] = pd.to_datetime(vaccine_tweets['date'], errors='coerce').dt.date\n\n# Save to csv\nvaccine_tweets.to_csv('vaccine_tweets.csv')","10485ade":"background_color = '#f5f8fa'\n\nfig = plt.figure(figsize=(4, 4), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\nvaccine_tweets['Count'] = 1\ntemp = vaccine_tweets.groupby('sentiment')['Count'].sum().sort_values(ascending=False)\n\nax0.bar(temp.index, temp, width=0.5, edgecolor='black',linewidth=0.6, color='#1da1f2')\n\nfor i in temp.index:\n    ax0.annotate(f\"{format(round(temp[i]), ',')}\", xy=(i, temp[i]\/2),color='white', va='center', ha='center', fontweight='light')\n\nax0.grid(axis='y', linestyle='-', alpha=0.4)   \nax0.set_yticks([])\nax0.tick_params(axis=u'both', which=u'both',length=0)\n\nfor s in ['top', 'left', 'right']:\n    ax0.spines[s].set_visible(False)\n    \n\n## For picture \n\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\ndef offset_png(x, y, path, ax, zoom, offset):\n    '''For adding  .png images to the graph.\n    source: https:\/\/stackoverflow.com\/questions\/61971090\/how-can-i-add-images-to-bars-in-axes-matplotlib'''\n    \n    img = plt.imread(path)\n    im = OffsetImage(img, zoom=zoom)\n    im.image.axes = ax\n    x_offset = offset\n    ab = AnnotationBbox(im, (x, y), xybox=(x_offset, 0), frameon=False,\n                        xycoords='data', boxcoords=\"offset points\", pad=0)\n    ax.add_artist(ab)\n    \n# Picture\npath='..\/input\/twitter-icon\/twitter-icon-83.png'\noffset_png(x=2.1, y=33000, path=path, ax=ax0, zoom=0.2, offset=0)\n\n\n# title\nax0.text(0.65,32300,'Tweet Sentiment',fontweight='bold', fontsize=16, zorder=20)\nax0.text(0.66,30000,'COVID-19 Vaccinnations',fontweight='light', fontsize=8, zorder=20)\n\n\n\n\nplt.show()","cb45b39b":"# format date correctly & get month & year\nvaccine_tweets['date'] = pd.to_datetime(vaccine_tweets[\"date\"].dt.strftime('%Y-%m-%d'))\nvaccine_tweets['Year'], vaccine_tweets['Month'], = vaccine_tweets['date'].dt.year, vaccine_tweets['date'].dt.month","f208f513":"temp1 = vaccine_tweets.groupby(['date', 'sentiment'])['Count'].count().reset_index().dropna()\ntemp2 = pd.pivot_table(temp1, index='date',columns='sentiment',values='Count',aggfunc=np.sum, fill_value=0)","665a5b16":"fig = plt.figure(figsize=(8, 3), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\ncolors = ['#9b1b30', '#009473', '#f0c05a']\n\ncolor_num = 0\nfor i in [\"Negative\", \"Positive\", \"Neutral\"]:\n    sns.lineplot(data=temp2[i], x=temp2.index, y=temp2[i], color=colors[color_num], ax=ax0)\n    color_num += 1\n    \nax0.grid(axis='y', linestyle='-', alpha=0.01)   \n#ax0.set_yticks([])\nax0.tick_params(axis=u'both', which=u'both',length=0)\n\nfor s in ['top', 'left', 'right']:\n    ax0.spines[s].set_visible(False)\n    \nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\n\nax0.set_ylabel(\"Tweet Count\",fontsize=8,loc='top', fontfamily='monospace')\nax0.set_xlabel(\" \",fontsize=8,loc='left', fontfamily='arial')\nax0.tick_params(axis = \"both\", which = \"both\", left=False, bottom=False)\nax0.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=7)\nformatter = mdates.ConciseDateFormatter(locator)\nax0.xaxis.set_major_locator(locator)\nax0.xaxis.set_major_formatter(formatter)\n\nax0.text(Xstart,2400,'Tweet Sentiment over time',fontweight='bold', fontsize=16, zorder=20)\n\n\nplt.show()","34c812b2":"astra = (vaccine_tweets[vaccine_tweets['orig_text'].str.lower().str.contains('astra')])\ntemp1 = astra.groupby(['date', 'sentiment'])['Count'].count().reset_index().dropna()\ntemp2 = pd.pivot_table(temp1, index='date',columns='sentiment',values='Count',aggfunc=np.sum, fill_value=0)\n\nfig = plt.figure(figsize=(8, 3), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\ncolors = ['#9b1b30', '#009473', '#f0c05a']\n\n\ncolor_num = 0\nfor i in [\"Negative\", \"Positive\", \"Neutral\"]:\n    sns.lineplot(data=temp2[i], x=temp2.index, y=temp2[i], color=colors[color_num], ax=ax0)\n    color_num += 1\n    \nax0.grid(axis='y', linestyle='-', alpha=0.01)   \n#ax0.set_yticks([])\nax0.tick_params(axis=u'both', which=u'both',length=0)\n\nfor s in ['top', 'left', 'right']:\n    ax0.spines[s].set_visible(False)\n    \nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\n\nax0.set_ylabel(\"Tweet Count\",fontsize=8,loc='top', fontfamily='monospace')\nax0.set_xlabel(\" \",fontsize=8,loc='left', fontfamily='arial')\nax0.tick_params(axis = \"both\", which = \"both\", left=False, bottom=False)\nax0.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=7)\nformatter = mdates.ConciseDateFormatter(locator)\nax0.xaxis.set_major_locator(locator)\nax0.xaxis.set_major_formatter(formatter)\n\nax0.text(Xstart,185,'Tweet Sentiment over time: Oxford\/AstraZeneca',fontweight='bold', fontsize=16, zorder=20)\n\nax0.axvspan(18700, 18708, facecolor='lightgray',alpha=0.5)\n\nax0.axvspan(18722, 18727, facecolor='lightgray',alpha=0.5)\n\n\nplt.annotate('Negative Press Begins', xy=(18700, 95), xytext=(18680, 120),\n             arrowprops=dict(facecolor='steelblue',arrowstyle=\"->\",connectionstyle=\"arc3,rad=.2\",color='black'), fontsize=7,fontfamily='monospace',ha='right', color='black')\n    \n\nplt.show()","87708bb8":"temp2['Neut_%'] = temp2['Neutral'] \/ (temp2['Neutral'] + temp2['Negative'] + temp2['Positive'] )\ntemp2['Neg_%'] = temp2['Negative'] \/ (temp2['Neutral'] + temp2['Negative'] + temp2['Positive'] )\ntemp2['Pos_%'] = temp2['Positive'] \/ (temp2['Neutral'] + temp2['Negative'] + temp2['Positive'] )","a17c0d44":"fig = plt.figure(figsize=(12, 3), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\nNeg_col = '#9b1b30'\nPos_col = '#009473'\nNeut_col = '#f0c05a'\n\ncolor = [Neut_col, Neg_col, Pos_col]\n\nastra_temp = astra.groupby('sentiment')['date'].value_counts().unstack().fillna(0).loc[['Positive','Negative','Neutral']].T\nastra_all = astra_temp.sum(axis=1)\nastra_temp = (astra_temp.T \/ astra_all).cumsum().T\n\nfor i, sents in enumerate(astra['sentiment'].value_counts().index):\n    sentims = astra_temp[sents]\n    ax0.bar(sentims.index, sentims, color=color[i], label=sents)\n    \nfor s in ['top', 'right', 'left']:\n    ax0.spines[s].set_visible(False)\n\nax0.set_yticks([])\n\nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\n\nax0.set_ylabel(\" \",fontsize=8,loc='top', fontfamily='monospace')\nax0.set_xlabel(\" \",fontsize=8,loc='left', fontfamily='arial')\nax0.tick_params(axis = \"both\", which = \"both\", left=False, bottom=False)\nax0.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=7)\nformatter = mdates.ConciseDateFormatter(locator)\nax0.xaxis.set_major_locator(locator)\nax0.xaxis.set_major_formatter(formatter)\n\nax0.text(Xstart+63.5,1.26,'Tweet Sentiment over time: Oxford\/AstraZeneca',fontweight='bold', fontsize=16, zorder=20)\nax0.text(Xstart+63.5,1.125,'February & April 2021',fontweight='light', fontsize=14, zorder=20)\n\nax0.set_xlim(Xstart+63.5,Xend-9.5)\n\nplt.show()","6142ce8d":"fig = plt.figure(figsize=(12, 3), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\n\nNeg_col = '#9b1b30'\nPos_col = '#009473'\n\ncolor = [Neg_col, Pos_col]\n\nneg_pos = astra[(astra['sentiment'] == 'Positive')  | (astra['sentiment'] == 'Negative')]\nastra_temp = neg_pos.groupby('sentiment')['date'].value_counts().unstack().fillna(0).loc[['Positive','Negative']].T\nastra_all = astra_temp.sum(axis=1)\nastra_temp = (astra_temp.T \/ astra_all).cumsum().T\n\nfor i, sents in enumerate(neg_pos['sentiment'].value_counts().index):\n    sentims = astra_temp[sents]\n    ax0.bar(sentims.index, sentims, color=color[i], label=sents)\n    \nfor s in ['top', 'right', 'left']:\n    ax0.spines[s].set_visible(False)\n\nax0.set_yticks([])\n\nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\n\nax0.set_ylabel(\" \",fontsize=8,loc='top', fontfamily='monospace')\nax0.set_xlabel(\" \",fontsize=8,loc='left', fontfamily='arial')\nax0.tick_params(axis = \"both\", which = \"both\", left=False, bottom=False)\nax0.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n\nlocator = mdates.AutoDateLocator(minticks=4, maxticks=7)\nformatter = mdates.ConciseDateFormatter(locator)\nax0.xaxis.set_major_locator(locator)\nax0.xaxis.set_major_formatter(formatter)\n\nax0.text(Xstart+63.5,1.26,'Tweet Sentiment over time: Oxford\/AstraZeneca',fontweight='bold', fontsize=16, zorder=20)\nax0.text(Xstart+63.5,1.125,'February & April 2021: Positive & Negative Only',fontweight='light', fontsize=14, zorder=20)\n\nax0.set_xlim(Xstart+63.5,Xend-9.5)\n\nplt.show()","25c3a210":"# Fine Tuning the Classifier\n\nThe last step is to train with discriminative learning rates and *gradual unfreezing*. In computer vision we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference:","b2570d4a":"Above we see **how the sentiment towards the Oxford\/AstraZeneca jab changed over time.**\n\nInitially, it was consistently neutral, up until Feb 2021 when all sentiments started to increase. \n\nNeutral remained the dominant sentiment, but it could be argued that **Positive & Negative sentiments were fairly equal.**\n\nThen, in mid to late March, came the **negative press** associated with rare side effects. \n\nIt is interesting that **not only did Negative sentiment increase as would be expected, but so too did Neutral and Positive.**\n\nThis again was the case in early April.","0640211c":"# A quick overview of our dataset","22c6ab25":"We now fill the Sentiments column that we made earlier...","332396ec":"# Creating the Language Model","384ef70d":"# Synthetic Text Generation with AI\n\nThings get even more interesting now. \n\nSince our model is attempting to predict the next word of a sentence, we can atually use the model to **generate our own synthetic sentences** or, in this case, tweets.\n\nFeel free to **try it out...**","aa47356a":"# Viewed Differently\n\nWe can also view how the proportions of neutral \/ negative \/ positive tweets changed over time.\n\nFor this, **I'll zoom in to February through April**, so we can see how the negative press affected the sentiment explicitly\n\nPlot inspired by:\n\nhttps:\/\/www.kaggle.com\/subinium\/all-you-need-is-time-series-visualization-20","d2bc6d3c":"This again is a powerful plot. \n\nWe see more clearly now how positive & negative sentiment changed over time\n\n\n# Work in progress...","a1770a50":"# Interpreting the learner\n\nIdeally, we want to select a learning rate where the loss is still decreasing, but not too close to where it begins to increase.\n\nThe plot above appears prerrty horizontal, until a sharp increase at approx. 10^-1.\n\nIt looks as though there might be a slight decrease between 10^-4 and 10^-3 - but it is extremely slight. I will select 10^-4 for my new learning rate, but I don't expect much of an improvement on the model above.","78451abd":"We can pass -2 to 'freeze_to' to freeze all except the last two parameter groups:","a9457a0f":"# Exploration of our results\n\nFirst, let's view how many of each category there are","dac8268e":"# Improving the model\n\nFor the model above, I ran FastAI's default learning rate.\n\nTo try to improve the model, we can seek to **optimize the learning rate**, rather than just taking the default option.","998acc1c":"# **NLP with FastAI**\n\nI cannot reccommend the FastAI course highly enough. It's a great entry point in to Neural Networks and Deep Learning in general. Many of the explanations I provide below are snippets from the course.\n\nYou can find more information here:\n\nhttps:\/\/www.fast.ai\/\n\nand the course here:\n\nhttps:\/\/course.fast.ai\/\n\nI reccommend using Google Collab for the course, rather the a jupyter notebook.\n\n# **Project: Vaccine Sentiment Analysis**\n\n# Project Plan\n\nThe purpose of this notebook is practice using FastAI, to do so, I will perform **vaccine sentiment analysis** \n\n\n**I will be following the FastAI course workbook**, lecture 10, but will apply it the Covid vaccine tweets dataset.\n\n**Many of the explanations I give will be directly taken from the lecture notes.** Please do follow the links above to find out more about the course.\n\nI will also demonstrate how we can **generate our own tweets** using FastAI.\n\nNext, I will visualise if\/how vaccine sentiment has changed over time. For this, I will pay particular interest to Johnson & Johnson and Oxford\/AstraZeneca, as both have had some negative press lately, related to very rare side effects.\n\n\n# References\n\nOther than the FastAI course linked above, a fantastic notebook that follows the same course is:\n\nhttps:\/\/www.kaggle.com\/twhelan\/covid-19-vaccine-sentiment-analysis-with-fastai\/\n\nThis notebook really helped me out and inspired me to make my own version. Check it out if you have the chance.\n","e241ecb1":"# Preparing our data for FastAI","53a654e7":"We convert this ","0afcfca1":"And unfreeze again and run some more","06f9cc24":"# Building the Classifier","b6444be8":"# Now let's fit the model\n\nWe'll fine tune this later, but for now I want to see where we are initially.\n\nThis is often a good idea, as once you have a basline you can ask yourself a number of questions:\n\n * Is this score good enough already?\n\n * Do we have time to train more models?\n\n * Is optimization necessary?\n\nThese are all questions the may or may not provide direction in your specific domain.\n\nA fantastic course relevant to my point above is **\"Structuting Machine Learning Projects\"** by **Andrew Ng** on Coursera.\n\nIt focuses on the practical realities of ML tools & products, optimization, setting a basline, and it is a fantastic resource.\n\nHere's a link:\n\nhttps:\/\/www.coursera.org\/learn\/machine-learning-projects","8cb4a7f3":"And now we'll unfreeze the entire model","cde32cac":"# Oxford\/AstraZeneca\n\nLet's shift focus to the Oxford\/AstraZeneca Jab now, as this has been in the press a lot lately.\n\nWhat can we learn?","4f41ecdf":"This is a valuable plot, because it shows that, regardless of the press coverage, the **predominant sentiment is always neutral**, with only minor changes in positive or negative sentiment.\n\n\n# Let's remove neutrals\n\nTo understand the underlying currents in sentiment, let've remove neutral tweets.\n\nThis way, we'll be able to explicitly view how positive and negative sentiments have changed over time, without the distraction of the majority class of neutral","70023dc4":"Above, I saved the full model, and then saved all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. \n\nThe model not including the final layer is called the **encoder**\n\n**This completes this phase of the text classification process**: fine-tuning the language model. \n\nWe can now use it to fine-tune a classifier for our tweets.\n\nBut first, an **interesting detour...**","51394150":"Our model has an **accuracy of nearly 77%** - pretty good!\n\nWhat does this mean? \n\nIt means that we can **correctly predict tweet sentiment in 77% of cases**\n\nLet's **apply this to our COVID tweets** now...","61ab8240":"# Loading the data","d3a9cf7b":"Let's look at what we've created so far","0c0ea79c":"So, by changing the learning rate **we managed to improve our model** fairly significantly!\n\nThe loss on both the training set & validation set is still decreasing too, so more epochs might have led to an even higher accuracy score.","268a629a":"So there appear to be many more neutral tweets than positive or negative.\n\nWe can now view how the sentiment changed over time"}}