{"cell_type":{"a8cefab6":"code","7667209d":"code","8bc6341c":"code","c8403a6f":"code","26d38796":"code","0a0839cb":"code","46133df2":"code","653ab45c":"code","9eb58d05":"code","ca547faf":"code","67276839":"code","006cc7c1":"code","42055e97":"code","c69f26ac":"code","a9d9b7ba":"code","d6937c20":"code","46c76bc8":"code","2bb42c3f":"code","d9b43e4c":"code","c4760568":"code","772a0fbe":"code","fe51f30a":"code","5fa42344":"code","daf61a88":"code","5c32ac53":"code","6155bfb2":"code","c8a5d63e":"code","1575f85f":"code","6549bcdf":"code","51620e8c":"code","ff521925":"code","e1ea51be":"code","45679823":"code","80479044":"code","92b91a24":"code","93514fc9":"code","ac32f36e":"code","e8c4f39d":"code","eff29076":"code","eb1f06bc":"code","aea06ea1":"code","f8925fa4":"code","bb59afa8":"code","0dee2009":"code","1b0dcd35":"code","ce87417b":"code","43e00f68":"code","621497cb":"code","75ff82ed":"code","72b80d87":"code","edbb43df":"code","e45d9eaa":"code","a3d7c4b7":"code","e5d77076":"code","b07a112c":"code","f90a2ada":"code","fc320b92":"code","57d17d54":"markdown","fe63a55c":"markdown","674edd93":"markdown","bf40c8c0":"markdown","184e9576":"markdown","2d25a410":"markdown","1f9fc6cf":"markdown","01824a3e":"markdown","f8e2e4ad":"markdown","f94800eb":"markdown","42059d10":"markdown","66e66590":"markdown","30ad58ec":"markdown","03ebe029":"markdown","dda1afbd":"markdown","07990d7c":"markdown","6a1e7612":"markdown","646adc73":"markdown","a40483f5":"markdown","efa81563":"markdown","f67a7ad9":"markdown","491d6414":"markdown","10baba9b":"markdown","2dcdaa4d":"markdown","814d6a69":"markdown","5a6b89b0":"markdown","a815c679":"markdown","42f3630a":"markdown","30b9be78":"markdown","deeefaff":"markdown","a216edf3":"markdown","955771bc":"markdown","36e91afa":"markdown","a704b161":"markdown","0d0eeda3":"markdown","eab9926d":"markdown","1c79c4c0":"markdown","b497a2ee":"markdown","c37fa85f":"markdown","50d55c7c":"markdown","fa56f77d":"markdown","9dd08957":"markdown","a3e6cd1a":"markdown","673b8113":"markdown","30cbb083":"markdown","771c088b":"markdown","7e06c0cc":"markdown","5ba77e35":"markdown","32408101":"markdown","2fb6bf1b":"markdown","4898ca9a":"markdown","ae2d2a5b":"markdown","bb5d83fd":"markdown","43f1cc7c":"markdown","866dca31":"markdown","ccb463a9":"markdown","9edc7c7d":"markdown","771a8f14":"markdown","ace67058":"markdown","95d5de89":"markdown","fb065f22":"markdown","aad95b4c":"markdown","824d395b":"markdown","daef4c62":"markdown","cf02de4a":"markdown","8b979127":"markdown","58622c7b":"markdown","81dcfc30":"markdown","33dd20bd":"markdown","e2caeb00":"markdown","daf10816":"markdown","f5727ae3":"markdown","2fa6a210":"markdown","8fa23173":"markdown","2d6e8359":"markdown","9f7ded9d":"markdown","5931a77e":"markdown","108fbf55":"markdown","63da00ca":"markdown","9d83b49a":"markdown","0cd689d1":"markdown","ff833d3b":"markdown","ea7b6602":"markdown","87c6cbc2":"markdown","793860c5":"markdown","21e5e3c0":"markdown","49137843":"markdown","768cb5fa":"markdown","9a9fbbe3":"markdown","250e6574":"markdown","dbc90bbf":"markdown","c7bb1ddb":"markdown","03926eb7":"markdown","47687d1c":"markdown","3c0314d2":"markdown","fbd10af2":"markdown","8a179d02":"markdown","d836a9dc":"markdown","0088c652":"markdown","6be8f581":"markdown","85deb659":"markdown","3b20965b":"markdown","4ee0a4ba":"markdown"},"source":{"a8cefab6":"import pandas as pd\n\n# Data Loading\ndf_stock = pd.read_csv(\"..\/input\/dhaka-stock-exchange-june-2021-stock-info\/StockData.csv\")","7667209d":"class color:\n   gray ='\\033[30m'       \n   greenv = '\\033[32m'\n   yellowl = '\\033[33m'\n   yellow = '\\033[93m'\n   skyblue = '\\033[34m'\n   purple = '\\033[35m'\n   darkcyan = '\\033[36m'\n   white = '\\033[37m'\n   red = '\\033[91m'\n   green = '\\033[92m'\n   blue = '\\033[94m'\n   magenta = '\\033[95m'\n   cyan = '\\033[96m'\n   \n  # Format \n   bold = '\\033[1m'\n   italic = '\\033[3m'\n   underline = '\\033[4m'\n\n  # Text Reset\n   dflt = '\\033[0m'","8bc6341c":"print(df_stock.info())\ndf_stock.head()","c8403a6f":"import numpy as np\nimport matplotlib.pyplot as plt\n\n#  Checking whether input contains NaN, infinity or a value too large for dtype('float64')\nprint(\"Number of \" + color.red+\"NaN\" + color.dflt + \n      \" values in each variable: \\n\",df_stock.isnull().sum())\ndf_stock.isna().sum().plot(kind=\"bar\")\nplt.show()\nprint(\"\\nAll the values of individual variables are \" + color.bold +\"finite:\" \n      + color.dflt+ color.green, any(np.isfinite(df_stock.all())), color.dflt)","26d38796":"missing_prcnt = (df_stock.isna().sum().sort_values(ascending=False) \/\n                       len(df_stock.index))*100\nprint(\"\\nNull value % out of the total data \\n\",missing_prcnt)","0a0839cb":"df_sub = df_stock[df_stock.iloc[:,4:].eq(0).all(axis=1)]\nprint('\\nNumber of '+ color.red +\"rows with zeros in every numeric column \" \n      + color.dflt + color.bold +\"except \" + color.dflt + color.italic \n      + \"Last Price:\"+ color.dflt + color.cyan, len(df_sub.index), color.dflt)\n\nprint('\\nTotal Percentage of '+color.red +\"records with zeros in every numeric column\" \n      + color.dflt + color.bold +\" except \" + color.dflt + color.italic \n      + \"Last Price:\"+ color.dflt + color.cyan, \n      round((len(df_sub.index) \/ len(df_stock.index)*100),2),\"%\\n\" + color.dflt)\ndf_sub","46133df2":"# Print shape of original DataFrame\nprint(\"\\nShape of original DataFrame before preprocessing for rows with zeros in every numeric column except 'Last Price': {}\".format(df_stock.shape))\n\ndf_stock.drop(df_stock[df_stock.iloc[:,4:].eq(0).all(axis=1)].index, \n              inplace=True)\n# # Drop missing values and print shape of new DataFrame\n# df_stock = df_stock.dropna()\n\n# Print shape of new DataFrame\nprint(\"Shape of DataFrame after dropping all rows with zeros in every numeric column except \" \n      + color.italic + \"Last Price\"+ color.dflt + \": {}\".format(df_stock.shape))","653ab45c":"from sklearn.impute import KNNImputer\n\nknn_imp = KNNImputer(n_neighbors=50, weights='uniform')\nX_imp = knn_imp.fit_transform(df_stock.iloc[:,3:])\n\n# KNNImputer(*, missing_values=nan, n_neighbors=5, weights='uniform', \n#            metric='nan_euclidean', copy=True, add_indicator=False)\nX_imp = knn_imp.fit_transform(df_stock.iloc[:,3:])\ndf_X = pd.DataFrame(X_imp,columns=df_stock.columns[3:])","9eb58d05":"from sklearn.preprocessing import StandardScaler\n\n# X = df_X.iloc[:, :5]\n\n# define standard scaler\nscaler = StandardScaler()\n\n# transform data\nX_scl = scaler.fit_transform(X_imp)\ndf_Xscl = pd.DataFrame(X_scl,columns=df_stock.columns[3:])\nprint(color.cyan +color.bold,\n      \"Centered and scaled values of features:\\n\",color.dflt)\ndf_Xscl","ca547faf":"print(color.magenta + color.bold,\n      \"Original values of features:\\n\",color.dflt)\ndf_X","67276839":"print(df_stock.info())\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric_df = df_stock.select_dtypes(include=numerics)\nprint(\"\\nNumber of columns with numeric values:\" + color.green,\n      len(numeric_df.columns),\"\\n\" + color.dflt)\nprint(color.blue + \"\\nSkewness of different features:\\n\" \n      + color.dflt, df_stock.skew(),\"\\n\")\ndf_stock.describe()","006cc7c1":"print(\"Category distribution of DSE, June 2021:\\n\",df_stock[\"Category\"].value_counts(),\"\\n\")\nprint(\"Category proportion of DSE, June 2021:\\n\",df_stock[\"Category\"].value_counts(normalize=True))","42055e97":"print(color.yellowl + \"Sector distribution:\\n\" \n      + color.dflt, df_stock[\"Sector\"].value_counts(),\"\\n\")\n\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nax = sns.countplot(x='Sector', data=df_stock,  order=df_stock.Sector.value_counts().index)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment='right')\nax.set_title('Sectorwise company distribution | DSE, June 2021')\nplt.show()\n\nprint(color.purple + color.bold + \"Sector proportion: \\n\" + color.dflt, \n      df_stock[\"Sector\"].value_counts(normalize=True))\nprint(color.bold + \"\\nTotal Sectors:\" + color.dflt + color.green, \n      len(df_stock.Sector.unique()), color.dflt)\n","c69f26ac":"print(\"Grouped mean of Last Price (BDT):\\n\", \n      df_stock.groupby(\"Category\")[\"Last Price\"].mean(),\"\\n\")\nprint(\"Grouped mean of NAV (mn BDT):\\n\",\n      df_stock.groupby(\"Category\")[\"NAV\"].mean(),\"\\n\")\nprint(\"\\nGrouped mean of Price-to-earnings (P\/E) ratio of the stock:\\n\",\n      df_stock.groupby(\"Category\")[\"P\/E\"].mean(),\"\\n\")\nprint(\"Grouped mean of Earnings Per Share (EPS):\\n\",\n      df_stock.groupby(\"Category\")[\"EPS\"].mean(),\"\\n\")\nex = sns.barplot('Category', 'EPS', data=df_stock, ci=False)\nex.set_ylabel('Earning per Share - EPS')\nex.set_title('Categorywise distribution of EPS)')\nplt.show()\n\nprint(\"\\nCategorywise Grouped mean of Paid up Capital (mn BDT): \\n\",\n      df_stock.groupby(\"Category\")[\"Paid up\"].mean(),\"\\n\")\nsns.barplot('Category', 'Paid up', data=df_stock, ci=False)\nplt.show()\n\nprint(\"\\nSectorwise Grouped mean of Paid up Capital (mn BDT): \\n\",\n      df_stock.groupby(\"Sector\")[\"Paid up\"].mean(),\"\\n\")\ngrp_order = df_stock.groupby('Sector')[\"Paid up\"].agg('mean').sort_values(ascending=False).index\nbx = sns.barplot('Sector', \"Paid up\", data=df_stock, \n                 estimator = np.mean, order=grp_order,ci=False)\nbx.set_xticklabels(bx.get_xticklabels(), \n                   rotation=90, horizontalalignment='right')\nbx.set_ylabel('Paid up Capital (mn BDT)')\nbx.set_title('\\nSectorwise Distribution of grouped mean of Paid up Capital (mn BDT)\\n')","a9d9b7ba":"print(\"Grouped summary of Last Price (BDT):\\n\", df_stock.groupby(\"Category\")[\"Last Price\"].agg([min, max]),\"\\n\")\nprint(\"Grouped summary of NAV (mn BDT):\\n\",df_stock.groupby(\"Category\")[\"NAV\"].agg([min, max]),\"\\n\")\nprint(\"Grouped summary of Price-to-earnings (P\/E) ratio of the stock:\\n\",df_stock.groupby(\"Category\")[\"P\/E\"].agg([min, max]),\"\\n\")\nprint(\"Grouped summary of Earnings Per Share (EPS):\\n\",df_stock.groupby(\"Category\")[\"EPS\"].agg([min, max]),\"\\n\")\nprint(\"Grouped summary of Paid up Capital (mn BDT):\\n\",df_stock.groupby(\"Category\")[\"Paid up\"].agg([min, max]),\"\\n\")","d6937c20":"df_gmv = df_stock.groupby([\"Category\", \"Sector\"])[[\"Last Price\", \"NAV\", \"EPS\", \"Paid up\",\"Pub\",\"Inst\",\"Foreign\"]].mean()\ndf_gmv.sort_values(['Category','NAV',\"Pub\",\"Inst\",\"Foreign\"],ascending=[True,False,False,False,False])","46c76bc8":"df_gmv2 = df_stock.groupby([\"Category\", \"Sector\"])[[\"Pub\",\"Inst\",\"Foreign\"]].mean()\ndf_gmv2.sort_values(['Category',\"Pub\",\"Inst\",\"Foreign\"],\n                    ascending=[True,False,False,False])","2bb42c3f":"df_gmv2.sort_values(['Category',\"Inst\",\"Foreign\",\"Pub\",],ascending=[True,False,False,False])","d9b43e4c":"df_gmv2.sort_values(['Category',\"Foreign\",\"Inst\",\"Pub\"],ascending=[True,False,False,False])","c4760568":"import seaborn as sns\nsns.set_style(\"darkgrid\")\n\n# Computes the pairwise correlation between control variables\ncx = sns.heatmap(df_X.corr(),annot=True,\n            cmap='RdYlGn') \ncx.set_title('\\nFeature Correlation Map\\n')\nplt.show()","772a0fbe":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nlog_data = pd.DataFrame(pt.fit_transform(df_X))\nlog_data.columns= df_X.columns\n\nlx = sns.heatmap(log_data.corr(),annot=True)\nlx.set_title('\\nFeature Correlation Map from Log Transformed Data\\n')\nplt.show()","fe51f30a":"import matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\ny = df_stock['Category'].values\n\n# Encode target labels (y) with value between 0 and n_classes-1 \ny = LabelEncoder().fit_transform(y)\n\nplt.style.use('ggplot')\npd.plotting.scatter_matrix(df_X, c = y, figsize=[13,13], s=15, marker='D')","5fa42344":"sns.distplot(df_stock.Pub, bins=25, kde=False, norm_hist=True)","daf61a88":"sns.histplot(df_X['Last Price'],log_scale=True)","5c32ac53":"plt.figure(figsize=(8,6))\nplt.hist(df_stock.Pub, bins=20, alpha=0.5, label=\"Public\")\nplt.hist(df_stock.Dir, bins=20, alpha=0.5, label=\"Directors\")\nplt.hist(df_stock.Inst, bins=20, alpha=0.5, label=\"Institutes\")\nplt.xlabel(\"Number of Companies\", size=14)\nplt.ylabel(\"Share holding %\", size=14)\nplt.title(\"Distribution of shareholding by stakeholders\")\nplt.legend(loc='upper right')\nplt.savefig(\"overlapping_histograms_with_matplotlib_Python_2.png\")","6155bfb2":"fig, ax =plt.subplots(1,2)\nsns.histplot(data=df_stock, x=\"Paid up\", hue=df_stock.Category, multiple=\"stack\", ax=ax[0])\nsns.histplot(data=df_stock, x=\"P\/E\", hue=df_stock.Category, multiple=\"stack\", ax=ax[1])\nfig.show()\n\n# sns.histplot(data=df_stock, x=\"Paid up\", hue=df_stock.Category, multiple=\"stack\")","c8a5d63e":"sns.histplot(data=df_stock, x=\"Paid up\", hue=df_stock.Category, multiple=\"stack\")\nplt.show()\nprint(\"\\n\")\nsns.histplot(data=df_stock, x=\"P\/E\", hue=df_stock.Category, multiple=\"stack\")","1575f85f":"import matplotlib.pyplot as plt\n\ndf_stock.boxplot('P\/E','Category')\nplt.show()\ndf_stock.boxplot(column=['EPS'],by='Category')\nplt.show()\ndf_stock.boxplot(column=['NAV'],by='Category')\nplt.show()\ndf_stock.boxplot(column=['Paid up'],by='Category')\nplt.show()\ndf_stock.boxplot('Pub','Category')\nplt.show()\ndf_stock.boxplot('Govt','Category')\n\n# df_stock.boxplot(by = 'Category',layout=(4,3), figsize=(20, 25))\n# plt.show()","6549bcdf":"fig, ax =plt.subplots(1,2)\nsns.stripplot(y = df_stock['Pub'], x = df_stock['Category'], ax=ax[0])\nsns.boxplot(x='Category', y=\"Pub\", data=df_stock, ax=ax[1])\nfig.show()","51620e8c":"fig, ax =plt.subplots(1,2)\nsns.stripplot(y = df_stock['Inst'], x = df_stock['Category'], ax=ax[0])\nsns.boxplot(x='Category', y=\"Inst\", data=df_stock, ax=ax[1])\nfig.show()","ff521925":"fig, ax =plt.subplots(1,2)\nsns.stripplot(y = df_stock['Foreign'], x = df_stock['Category'], ax=ax[0])\nsns.boxplot(x='Category', y=\"Foreign\", data=df_stock, ax=ax[1])\nfig.show()\n# df_stock.Foreign[df_stock['Category'=='B']].value_counts()\n# df_stock[df_stock['Category'].isin(['Z','B'])].count().group","e1ea51be":"sns.stripplot(y = df_stock['Pub'], x = df_stock['Category'])\nplt.show()\nprint(\"\\n\")\n\nsns.boxplot(x='Category', y=\"Pub\", data=df_stock)\nplt.show()\nprint(\"\\n\")\n\nsns.stripplot(y = df_stock['Inst'], x = df_stock['Category'])\nplt.show()\nprint(\"\\n\")\n\nsns.boxplot(x='Category', y=\"Inst\", data=df_stock)\nplt.show()\nprint(\"\\n\")\n\nsns.boxplot(x='Category', y=\"Govt\", data=df_stock)\nplt.show()\nprint(\"\\n\")\n# sns.pairplot()\n\nsns.stripplot(y = df_stock['P\/E'], x = df_stock['Category'])\nplt.show()\nprint(\"\\n\")\n\nsns.stripplot(y = df_stock['EPS'], x = df_stock['Category'])\nplt.show()\nprint(\"\\n\")\n\nsns.stripplot(y = df_stock['NAV'], x = df_stock['Category'])\nplt.show()\nprint(\"\\n\")\n\nsns.stripplot(y = df_stock['Paid up'], x = df_stock['Category'])\nplt.show()\nprint(\"\\n\")\n\nsns.boxplot(x='Category', y=\"Paid up\", data=df_stock)\nplt.show()","45679823":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df_Xscl # using scaled values for features\n\ny = df_stock['Category'].values\n\n# Transforming the prediction target (y)\nle = LabelEncoder() # Encode target labels with value between 0 and n_classes-1 \ny = le.fit_transform(y)\n\n# Decoded labels of 'Category' column, the outcome values\nprint(\"Mapped labels of\" + color.green + color.italic,'Category' +color.dflt, \n      \"values: \", color.green, dict(zip(le.classes_,range(len(le.classes_)))),\n      color.dflt)","80479044":"# Import necessary modules using GridSearchCV package for hyperparameter tuning \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y,\ntest_size=0.3, random_state=2)\n\n# Setup the parameters and distributions to sample from: param_grid\nparam_grid = {'n_neighbors': np.arange(1, 30)}\n\n# Instantiate a KNN classifier\nknn = KNeighborsClassifier()\n\n# Instantiate the GridSearchCV object: tree_cv\nknn_cv = GridSearchCV(knn, param_grid, cv=6) # returns a GridSearch object that \n# we can then fit to the data and this fit performs the actual grid search inplace\nknn_cv.fit(X_train, y_train)\nprint(\"Best K value :\" + color.green, knn_cv.best_params_, color.dflt)\nprint(\"Best n_neighbors score computed using 5-fold cross-validation: {}\".format(knn_cv.best_score_))","92b91a24":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train,y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","93514fc9":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Create a k-NN classifier with 5 neighbors as found from hyperparameter tuning\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n\n# Predict the labels of the test data set: y_pred\ny_pred = knn.predict(X_test)\n\n# Check the model accuracy rate\nprint(\"Prediction accuracy of KNN model on\" +color.green +\" test \" + color.dflt + \"data:\"\n      + color.green,round((knn.score(X_test,y_test)*100),2),\"%\",color.dflt)\nprint(\"\\nPrediction accuracy of KNN model on\" +color.cyan +\" training \" + color.dflt +\"data:\"\n      + color.cyan,round((knn.score(X_train,y_train)*100),2),\"%\",color.dflt)\n\n# print('Accuracy score of built KNN classifier:' + color.green, \n#       accuracy_score(y_test, y_pred), \"\\n\" + color.dflt)\n\n# Decoded labels of 'Category' column, the outcome values\nprint(\"Mapped labels of\" + color.green + color.italic,\n      'Category' +color.dflt, \"values: \", color.green, \n      dict(zip(le.classes_,range(len(le.classes_)))), color.dflt)\n\n# Compute and print the confusion matrix and classification report\nprint(color.dflt +  color.underline +\"\\nConfusion Matrix:\\n\" + color.dflt\n      + color.cyan,confusion_matrix(y_test, y_pred),color.dflt) \nprint(color.dflt + color.underline +\"\\nClassification Report:\\n\" \n      + color.dflt + color.skyblue,classification_report(y_test, y_pred))","ac32f36e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n# Create the classifier: logreg\nlogreg = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n\n# Fit the classifier to the training data\nlogreg.fit(X_train, y_train) \n\n# Predict the labels of the test data set: y_pred\ny_pred = logreg.predict(X_test)\n\n# Check the model accuracy rate\nlogreg.score(X_test,y_test)\nprint('Accuracy Score of built Logistic Regression model:' + color.green, \n      accuracy_score(y_test, y_pred), \"\\n\" + color.dflt)\n\n# Decoded labels of 'Category' column, the outcome values\nprint(\"Mapped labels of\" + color.green + color.italic,\n      'Category' +color.dflt, \"values: \", color.green, \n      dict(zip(le.classes_,range(len(le.classes_)))), color.dflt)\n\n# Compute and print the confusion matrix and classification report\nprint(color.dflt +  color.underline +\"\\nConfusion Matrix:\\n\" + color.dflt\n      + color.cyan,confusion_matrix(y_test, y_pred),color.dflt) \nprint(color.dflt + color.underline +\"\\nClassification Report:\\n\" \n      + color.dflt + color.skyblue,classification_report(y_test, y_pred))","e8c4f39d":"# take a general notation\n# X = df_X.iloc[:, :5] # we only take the first five features.\nX_sv = df_Xscl # using scaled values of features \n\n# Split dataset into training and test dataset\nX_train, X_test, y_train, y_test = train_test_split(X_sv, y,\ntest_size=0.3, random_state=1234)\n\n# write function for Support vector machine\nfrom sklearn.svm import SVC\nsvc=SVC()\n\n# model fitting\nsvc.fit(X_train,y_train)\n\n# predict for the labels of the test data set: y_pred\ny_pred = svc.predict(X_test)\n\n# See the accuracy rate\nsvc.score(X_test,y_test)\nprint('Accuracy Score of built SVM model:' + color.green, \n      accuracy_score(y_test, y_pred), \"\\n\" + color.dflt)\n\n# Decoded labels of 'Category' column, the outcome values\nprint(\"Mapped labels of\" + color.green + color.italic,\n      'Category' +color.dflt, \"values: \", color.green, \n      dict(zip(le.classes_,range(len(le.classes_)))), color.dflt)\n\n# Compute and print the confusion matrix and classification report\nprint(color.dflt +  color.underline +\"\\nConfusion Matrix of the SVM model\\n\" \n      + color.dflt + color.cyan,confusion_matrix(y_test, y_pred),color.dflt) \nprint(color.dflt +color.underline +\"\\nClassification Report of the SVM model\\n\" \n      + color.dflt + color.darkcyan,classification_report(y_test, y_pred))","eff29076":"# Model fit using Logistic Regression\n# import numpy as np\n# from sklearn import datasets\n# from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n# from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nX_el = X_imp\n\n# Create training and test split\nX_train, X_test, y_train, y_test = train_test_split(X_el, y,\ntest_size=0.25, random_state=1, stratify=y)\n\n# Pipeline Estimator\npipeline = make_pipeline(StandardScaler(),\nLogisticRegression(random_state=1))\n\n# Fit the model\npipeline.fit(X_train, y_train)\n\n# Model scores on test and training data\nprint('Parallel Ensemble Learning Model test Score: %.3f, ' \n      %pipeline.score(X_test, y_test),\n'Model training Score: %.3f' %pipeline.score(X_train, y_train))","eb1f06bc":"# Model fit using Bagging Classifier\n\n# Pipeline Estimator\npipeline = make_pipeline(StandardScaler(),\nLogisticRegression(random_state=1))\n\n# Instantiate the bagging classifier\nbgclassifier = BaggingClassifier(base_estimator=pipeline,\nn_estimators=100,\nmax_features=10,\nmax_samples=100,\nrandom_state=1, n_jobs=5)\n\n# Fit the bagging classifier\nbgclassifier.fit(X_train, y_train)\n\n# Model scores on test and training data\nprint('Model test Score: %.3f, ' %bgclassifier.score(X_test,y_test),\n      'Model training Score: %.3f' %bgclassifier.score(X_train, y_train))","aea06ea1":"# Base Learners SVM\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\nsvc=SVC(probability=True, kernel='linear')\n\n# Create adaboost classifer object\nabc = AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1)\n\n# Train Adaboost Classifer\nmodel = abc.fit(X_train, y_train)\n#Predict the response for test dataset\ny_pred = model.predict(X_test)\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n\n# print('X_el:', X_el[0,2:5])\n# print('X_imp:', X_imp[0,2:5])\n# print('\\nX:\\n', X.iloc[0,2:5])\n# print('\\ndf_X:\\n', df_X.iloc[0,2:5])\n# print('\\ndf_Xscl:\\n', df_Xscl.iloc[0,2:5])\n# print('\\nX_scl:\\n', X_scl[0,2:5])\n# print('\\nX_sv:\\n',X_sv.iloc[0,2:6])","f8925fa4":"#K-means cluster\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n%matplotlib inline\n\nkm = KMeans(n_clusters=3)\ny_predicted = km.fit_predict(X_scl)\n# y_predicted","bb59afa8":"df_Xscl['cluster']=y_predicted\ndf_Xscl.head()\n\n# See center of the cluster\nkm.cluster_centers_\n\n# Draw a scatter plot\ndf1 = df_Xscl[df_Xscl.cluster==0]\ndf2 = df_Xscl[df_Xscl.cluster==1]\ndf3 = df_Xscl[df_Xscl.cluster==2]\ndf4 = df_Xscl[df_Xscl.cluster==3]\nplt.scatter(df1['NAV'],df1['EPS'],color='green')\nplt.scatter(df2['NAV'],df2['EPS'],color='red')\nplt.scatter(df3['NAV'],df3['EPS'],color='orange')\nplt.scatter(df4['NAV'],df4['EPS'],color='blue')\nplt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple')\nplt.xlabel(\"NAV\")\nplt.ylabel('EPS')\nplt.legend()","0dee2009":"sns.pairplot(X,diag_kind='kde',hue='cluster')\nplt.show()","1b0dcd35":"# Let us check optimal number of clusters-\ncluster_range = range( 1, 15)\ncluster_errors = []\ncluster_sil_scores = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 100,init='k-means++',random_state=0)\n  clusters.fit(X)\n  labels = clusters.labels_                     # capture the cluster lables\n  centroids = clusters.cluster_centers_         # capture the centroids\n  cluster_errors.append( clusters.inertia_ )    # capture the inertia\n# combine the cluster_range and cluster_errors into a dataframe by combining them\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors} )\nclusters_df[0:10]","ce87417b":"# Elbow plot\nfrom sklearn.cluster import KMeans\nsse = []\nk_rng = range(1,10)\nfor k in k_rng:\n  km = KMeans(n_clusters=k)\n  km.fit(X_scl)\n  sse.append(km.inertia_)\nplt.xlabel('K')\nplt.ylabel('Sum of squared error')\nplt.plot(k_rng,sse)","43e00f68":"# from sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm","621497cb":"wcss=[]\nfor i in range(1,11):\n  kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0)\n  kmeans.fit(X_scl)\n  wcss.append(kmeans.inertia_)\n  \nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","75ff82ed":"range_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\n        \"For n_clusters =\",\n        n_clusters,\n        \"The average silhouette_score is :\",\n        silhouette_avg,\n    )\n     \n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # =================\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) \/ n_clusters)\n    ax2.scatter(\n        X.iloc[:, 0], X.iloc[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n    )\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(\n        centers[:, 0],\n        centers[:, 1],\n        marker=\"o\",\n        c=\"white\",\n        alpha=1,\n        s=200,\n        edgecolor=\"k\",\n    )\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle(\n        \"Silhouette analysis for KMeans clustering on given data with n_clusters = %d\"\n        % n_clusters,\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\nplt.show()","72b80d87":"import seaborn as sns\nsns.pairplot(X,diag_kind='kde')\nfrom sklearn.cluster import AgglomerativeClustering\ngroups = AgglomerativeClustering(n_clusters=3,\n                                 affinity='euclidean', linkage='ward')\ngroups.fit_predict(X)\nplt.scatter(X['NAV'] ,X['EPS'], \n            c = groups.labels_, cmap='cool')","edbb43df":"from scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import pdist\nplt.figure(figsize=(18, 16))\nplt.title('Agglomerative Hierarchical Clustering Dendogram')\nplt.xlabel('sample index')\nplt.ylabel('Distance')\nZ = linkage(X, 'ward')\ndendrogram(Z,leaf_rotation=90.0,p=25,color_threshold=10,leaf_font_size=10,truncate_mode='level')\nplt.tight_layout()","e45d9eaa":"# Data table display for Pandas dataframes can be enabled by running:\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\n# Data table display of raw data\ndf_stock\n\n# Disable data table display by running\ndata_table.disable_dataframe_formatter()","a3d7c4b7":"!pwd","e5d77076":"# %cd go_to_your_path\n%cd \/content\/drive\/MyDrive\/Colab Notebooks","b07a112c":"!pwd\n!ls","f90a2ada":"!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended","fc320b92":"!jupyter nbconvert --to html Assignment_2n3_ASDS18_DataMining.ipynb","57d17d54":"Histogram of shareholding by stakeholders","fe63a55c":"### Descriptive Analysis","674edd93":"### About the Data\n\nDSE Stock Company data | June 2021","bf40c8c0":"From the results of the study it can be inferred that of all the classifiers we built probably the KNN model will best serve our purpose. This **KNN** model's **accuracy** was **77.17%** on the test data of the given data set. The closest or second classifier can be chosen as a **Logostic Regression** classifier, because **accuracy** of that model was **74.77%**.\n<br>\n<br>\nHowver, from Confusion Matrix it was found that KNN model correctly classified ALL 68 category 'A' companies to its right category of test data without any wrong classification. But only 1 out of 14 category 'B' companies from test data were correctly assigned to its right category. We can ignore the wrong classifications of 'N' category companies because our training model was never trained with specific information to predict this category comanies. However, our KNN model has rightly classified 2 out of total 5 companies to 'Z' category from test data.\n<br>\n<br>\nLogistic Regression model on the other hand has done quite well in assigning both category 'A' and 'B' companies from test data. Although it made very little mistakes of making wrong prediction of 6 companies out of total 81 companies in labelling 'A' category companies, but along with this it also did a fair job by correctly assigning 6 out of 10 commpanies to category 'B'. So, from overall point of view, Logostic Regression model could also be considred as a good classifier in this case.\n<br>\n<br>\nMoreover, from the results we have found that applying **Bagging methood** from **Parallel Ensemble Learning** has significantly <u> improved the predictoin rate <\/u>. The **accuracy** score of this model was ***82.6%***, significantly higher than a single learner of KNN or Logostic Regresssion model.\n<br>\n<br>\nHowever, our cluster analysis was not that much effective compared to the classification models. We tried both elbow method and Silhouetter method to find the optimum number of clusters. But it was not very disctinct and definitive to come to a concrete solution. In some tests it was suggestive to 2(Silhouetter method), 7 or even 9 (Elbow method and `cluster.inertia_` method of Python package) in one case. Moreover the clustering error was also quite high for different optimum numnber of clusters.   ","184e9576":"Chage the current path to the source `.ipynb` file that needs an output in `.pdf` or other desired format.","2d25a410":"Investors who purchase stocks of different listed companies at Dhaka Stock Exchange (DSE), they mostlly  rely on the category listing policies of DSE authority. DSE categorizes companies on the basis of the regularity of holding AGM (Annual General Meetings) and declaring dividends over a minimum threshold level (10% or more) or on the event of success or failure to declare dividend  in the last English calendar year. Along with these few basic features DSE also consider wheather a company is not in operation continuously for more than six months or whose accumulated loss after adjustment of revenue reserve, if any, exceeds its paid up capital.\n<br>\n<br>\nSo, we can clearly see that DSE's classification rule for categorizing its listed stock copmanies is quite primitive and insufficient in selecting features to give proper prediction about the performance of a company stock. In our consideration this is not an enough guideline for a buyer or investor to help assessing the performance of the company stock before considering the  purchase decision of that stock. A point to be noted that none of these features were ever considered as its business logic for categorizing the listed companies by DSE authority itself.\n<br>\n<br>\nThis study will help investors to better guide with the purchase decision of company stocks as <u> we have attempted building a classification model for the categories of the companany based on **nine (9) new features**<\/u> or dependent variables, namely,  1. `Last price` (Market price of the stock), 2. Net asset value of the stock (`NAV`), 3. Earnings Per Share (`EPS`), 4. Price-to-earnings (`P\/E`) ratio of the stock, 5. `paid up` capital per share, 6. `Dir`,\tpercentage of director holding share, 7. `Pub`,\tpercentage of public holding share, 8. `Inst`, percentage of institute holding share, 8. `Foreign`,\tpercentages of foreign holding share, 9. `Govt`, percentages of government holding share. So, rather than merely relying on category listing of DSE,  investors will be on the upperhand by crosschecking the category prediction of the classification model built by this study with the listed cateogories of DSE to make a smart and safer purchase decision of stocks.\n<br>\n<br>\nMoreover, we will also attempt to identify the stock companies of DSE that have the tendency to change the category. This type of prediction is quite unique in its nature. A true and accurate such prediction will surely help investors to equip with a tool to safeguard their future purchases.\n","1f9fc6cf":"**Selecting the number of clusters with silhouette analysis on KMeans clustering**\n<br>\nSilhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n<br>\n<br>\nSilhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n<br>\n<br>\nIn the following code section the silhouette analysis is used to choose an optimal value for n_clusters. The silhouette plot shows that the `n_clusters` value of **3**, **4**, **5** and **6** are a ***bad*** pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. Silhouette analysis is more suggestive here in deciding **2** as the **appropriate** number for `n_cluster`.","01824a3e":"Summaries by category groups of DSE, June 2021","f8e2e4ad":"# Methodology","f94800eb":"##### <u>Data Set Information<\/u>\n\nThe datasets consist of several stock company predictor (independent) variables and one target (dependent) variable, *Outcome*. Independent variables include the last price, net asset value (NAV) of the stock, Earnings Per Share (EPS), price-to-earnings (P\/E) ratio of the stock, paid up capital per share, and so on.\n<br>\n<br>\nIt contains information of **374 listed companies** from Dhaka Stock Exchange - DSE, Bangladesh. The **outcome** tested was `Category`, 258 tested positive and 500 tested negative. Therefore, there is one target (dependent) variable and 8 attributes. ","42059d10":"Convert the ipnyb notebook to PDF and HTML output for readers and end users","66e66590":"1. SaminaHaque, Murtaza Faruquee, Impact of Fundamental Factors on Stock Price: A Case Based Approach on Pharmaceutical Companies Listed with Dhaka Stock Exchange,*International Journal of Business and Management Invention, ISSN (Online): 2319 \u2013 8028, ISSN (Print): 2319 \u2013 801X, Volume 2 Issue 9*, September 2013 <br><br>\n2. Shiller and RobertJ., *International Exuberance, Princeton University Press*, 2000.<br><br>\n3. Chen J., Hong H. and Stein J.C., Breadth of Ownership and Stock Return, *Journal of Financial Economics, Vol. 66*, 2002, 171-205.<br><br>\n4. Chowdhury R. A. and Chowdhury S.P., Impact of Capital Structure on Firm\u2019s Value: Evidence from Bangladesh, *Business and Economic Horizon, 3 (3)*, 2010, 111-122.<br><br>\n5. Chowdhury A. and Abdullah M.N., Overheated Stock Market: Remidial Measures for Soft Landing, *Journal of Institute of Cost & Management Accounts of Bangladesh, Vol. 5*, 2011, 28.<br><br>\n6. Abdullah M. N., Parvez K. and Khaled M., Is the Stock Market Overvalued: A Study in the Context of Bangladesh? *Asian Business Review, Vol. 1, Issue 1*, 2012, 30-36.<br><br>\n7. Company Listing of Dhaka Stock Exchange | *DSE official website*, 2021","30ad58ec":"#### Optimum number of cluster","03ebe029":"Install packages required for conversion of output to desired formats.","dda1afbd":"First segment of the study (**Assignment #2**) considers the fundamental information of companies\u2019 performance, which includes `Last Price`, earning per share (`EPS`), net asset value (`NVA`), \tPrice-to-earnings (`P\/E`) ratio of the stock, `Paid up` capital, and the % of share holding by various type of stakeholders. For regression analysis significance value has been accepted up to 0.05 (as general statistical standard).\n<br>\n<br>\nIn this segment we have used descriptive statistics that quantitatively describes or summarizes features from a collection of information. We have provied brief descriptive coefficients that have summarized the given data set, which can be either a representation of the entire population or a sample of a population. Descriptive statistics are broken down into measures of central tendency and measures of variability (spread).\n<br>\n<br>\n**Data Visualization**\n- For data visualization We have used `matplotlib` and `Seaborn` a Python visualization library based on matplotlib. \n- We have used boxplot, different frequency distributuion plots using histograms\n\n\n**Classification | Supervised Learning technique of data mining**\n<br>\nFor building the classification models we have used the following algorithms of data mining:\n - **KNN** (k-nearest neighbors algorithm): In statistics, the k-nearest neighbors algorithm is a non-parametric classification method. It is used for classification and regression. \n - **Logistic Regression**: In statistics, the logistic model is used to model the probability of a certain class or event. This can be extended to model several classes of events. \n - **SVM** (Support vector machine): In machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.\n - **Ensemble learning**: A process in which decisions from multiple machine learning models are combined to reduce errors and improve prediction when compared to a single ML model\n  - **Parallel Ensemble Learning** - We have applied the *Bagging methods* of avaraging method type as *Logistic Regression* as our *Base Learner* \n  - **Sequential Ensemple Learning** - We have applied the *Ada Boost* of boosting method type as *SVM* as our *Base Learner* \n<br>\n<br>\nWe have also used ***hyperparameter tuning*** method for deciding best K value of 'n_neighbors' in designing a KNN classifier.\n<br>\n<br>\nSecond segment of the study (**Assignment #3**) compares the current market price for PCI companies to their ideal price. For this a randomly selected normal day\u2019s (not influenced by any extreme factor) market price has been compared with the theoretically calculated ideal price.\n\n**Clustering | Unsupervised Learning technique of data mining**\nFor building the clustering models we have used the following algorithms of data mining:\n * **K-means Clustering**: A method of vector quantization  that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. \n * **Hierarchical clustering**: In data mining and statistics, it is a method of cluster analysis which seeks to build a hierarchy of clusters.. \n * **Determining Optimal Clusters** (Support vector machine): To determine the optimal clusters of our K-means clustering model we have used following two methods:\n   * Elbow method\n   * Silhouette method  ","07990d7c":"### Parallel Ensemble Learning\nBagging methods","6a1e7612":"# Data Analysis and Results","646adc73":"### Logistic Regression classifier","a40483f5":"##### Centering and scaling\nWe have figured out that the ranges of the feature variables vary widely in our working data set. Many machine learning models use some form of distance to inform them so if we have features on far larger scales, they can unduly influence our model. For this reason, we actually want features to be on a similar scale. To achieve this, we do what is called **normalizing** or **scaling** and **centering**.\n<br>\n<br>\nPerformance of a model can improve significantly if the features are scaled when necessary.","efa81563":"***A- Category Companies***\n\nCompanies which are **regular** in holding the **Annual General Meetings** and have declared **dividend** at the rate of **10% or more** in the last English calendar year.\n<br>\n<br>\n***B- Category Companies***\n<br>\nCompanies which are **regular** in holding the **Annual General Meetings** but have **failed** to declare **dividend at least** at the rate of **10%** in the last English calendar year.\n<br>\n<br>\n***G- Category Companies***\n<br>\nG- Category Companies are basically Greenfield companies. The companies, which have **not** yet **started** its **operation** but call subscribers to invest to their Company. They basically **call for capital** in primary market.\n<br>\n<br>\n***N-Category Companies***\n<br>\n**Newly listed** companies except green-field companies which **shall be transferred to other categories** in accordance **with their first dividend** declaration and respective compliance after listing of their shares.\n<br>\n<br>\n***Z-Category Companies***\n<br>\nCompanies which have **failed** to hold the **Annual General Meeting** when due or have **failed** to declare any **dividend** based on annual performance or which are **not in operation continuously for more than six months** or whose **accumulated loss** after adjustment of revenue reserve, if any, **exceeds its paid up capital**.\n","f67a7ad9":"#### Context: \nThis dataset is originally from Dhaka Stock Exchange Ltd. The objective of the dataset is to assign analytical report writing tasks to *Summer 2020* students enrolled in **ASDS18: Data Mining** course in proceedings of the partial fulfillment of the requirements for the **Professional Masters in Applied Statistics and Data Science** (PMASDS) degree. This data set was collected using the Dhaka Stock Exchange API. ","491d6414":"### Categories of Trading Companies in DSE","10baba9b":"#### Distribution Symmetry & Skewness","2dcdaa4d":"The following result shows that percentage (%) of public holding share is highest in ***'A'*** category companies from ***Jute*** sectors. On an average, general investors hold 64.25% of the share stocks of 'A' category companies fromm this sector. <br>\n\nHowever, in case pf ***'B'*** category companies, percentage (%) of public holding share is highest in **Food & Allied** sector.\n","814d6a69":"Percentage of missing values per column","5a6b89b0":"### Sampling\nThe study considers **374 companies** listed with DSE, which comprises **61.11%** **of the total 612 listed companies** with DSE under all sectors. For the purpose of analysis *secondary data* have been considered from June 2021 database of DSE official website. It is worth mentioning here that the 7 companies were taken out of the sample for their missing values corresponding to almost all the numeric variables.","a815c679":"### K-means Clustering","42f3630a":"Check the current working path of Google Colab notebooks","30b9be78":"##### Log transformation to explore feature correlation","deeefaff":"This study is broadly designed to find out the influence of different fundamental information in determining each stock company of DSE in the category of the stock using both classfication and clustering algorithms. Moreover the paper attempts to compares the market price of shares with their ideal price to determine whether these stocks are currently overvalued or undervalued.\n\n","a216edf3":"## c. Clustering","955771bc":"# Appendix","36e91afa":"## Getting Report Output","a704b161":"# Discussion","0d0eeda3":"- Institutes mostly hold lion shares (more than 50% stock) of `Corporate Bond` sector of `'A'` category companies followed by `Mutual Funds` sector of the same cateogory companies.\n- Institutes also hold a significant percentage of stock (more than 25%) in `Travel & Lesiure`, `Bank` and `IT Sector` respectively from `'B'` category companies.\n- Surprisingly institutions are holding a significant percentage of stock (more than 25%) in `Financial Institutions`, `Insurance` and `Jute` sectors respectively all of which are from risky `'Z'` category companies. ","eab9926d":"Categorywise Multiple grouped summaries","1c79c4c0":"#### Frequency Distributions","b497a2ee":"So, from the above result it appears optimal number of cluster(s) could be  k = 9 or 10.","c37fa85f":"#### Source:\n\n\n","50d55c7c":"### Stock Market Overview","fa56f77d":"Grouping by multiple variables","9dd08957":"### Loading the data","a3e6cd1a":"### Support Vector Machines","673b8113":"# Abstract","30cbb083":"### Procedures","771c088b":"##### <u>Attribute Information<\/u>\n\n<center><u>Table<\/u>: Variable descriptions of the stock company <br><br>\n\nVariable | Variable description\n---- | ----\n**CompanyName** | Name of stock company\n**Sector** | Category of stock\n**Category** | Category of company (*target variable*)\n**Last price** | Market price of the stock\n**NAV** | Net asset value (NAV) of the stock\n**EPS** | Earnings Per Share (EPS)\n**p\/e** | Price-to-earnings (P\/E) ratio of the stock\n**Paid up** | Paid-up capital per share\n**Dir** | Percentage of director holding share\n**Pub** | Percentage of public holding share\n**Inst** | Percentage of institute holding share\n**Foreign** | Percentages of foreign holding share\n**Govt** | Percentages of government holding share\n\n    ","7e06c0cc":"From the above plot it is clearly evident that from a range of varying *n_neighbor* values, only ***K=5*** will return best accuracy (in between 76% to 79%) for both training data and test data of the given data set.\n<br>","5ba77e35":"It appears from the above study that we did some misclassification compared to orginal categorization made by Dhaka Stock Exchange (DSE) itself. The misclassification is higher in case of Clustering analysis. But we believe, all of these misclassifications should not be considered as inaccuracy of the built models. This is because the very business logic of our classification of category and DSE authority differs by a wide range. None of the features (or dependent variables) of our study overlap with the features DSE uses as a policy to caegorize each listed company.\n<br>\n<br>\nTherefore, our category outcomes will never completely coincide with the category listing of DSE. We rather consider that our classification models will work as a complementary and additional  tool for the investors of DSE stock market to figure out an alternative way to consider the perfformance of each company from a new and unexplored perspective. \n<br>\n<br>\nMoreover, we could not collect the data on dividends, regularity of holding AGM (Annual General Meetings) and continuity of operations of the company in last six months which, DSE authority uses as decision making factors in categorizing the companies. Incorporating those featues in our study would surely have further improved the accuray and reliability of our models. \n<br>\n<br>\nThis study and its report is not complete and all-inclusive on the said topic. But, the report will definitely pave the way for future researches to work in depth on the topic.","32408101":"#### Exploring feature correlation","2fb6bf1b":"##### Acknowledgements:\n\n**Dr. Md. Rezaul Karim**, *Associate Professor*, Department of Statistics, *Jahangirnagar University*, Dhaka, Bangladesh (2021). Using the Dhaka Stock Exchange API this data set was collected to assign analytical report writing tasks to Summer 2020 students in proceedings of the partial fulfillment of the requirements for the Professional Masters in Applied Statistics and Data Science (PMASDS) degree.","4898ca9a":"Sort by stock holding of `Public`","ae2d2a5b":"**What is the Stock Market?**\n<br>\nIt is a place where shares of pubic listed companies are traded. The stock market refers to the collection of markets and exchanges where regular activities of buying, selling, and issuance of shares of publicly-held companies take place. Such financial activities are conducted through institutionalized formal exchanges or over-the-counter (OTC) marketplaces which operate under a defined set of regulations.\n<br>\n<br>\nWhile both terms - ***stock market*** and ***stock exchange*** - are used interchangeably, the latter term is generally a subset of the former. If one says that s\/he trades in the stock market, it means that s\/he buys and sells shares\/equities on one (or more) of the stock exchange(s) that are part of the overall stock market.\n<br>\n<br>\nA stock exchange facilitates stock brokers to trade company stocks and other securities. A stock may be bought or sold only if it is listed on an exchange. Thus, it is the meeting place of the stock buyers and sellers.\n<br>\n<br>\n**Examples of Stock Markets**\n<br>\nThe first stock market in the world was the London stock exchange. It was started in a coffeehouse, where traders used to meet to exchange shares, in 1773. The first stock exchange in the United States of America was started in Philadelphia in 1790. The Buttonwood agreement, so named because it was signed under a buttonwood tree, marked the beginnings of New York's Wall Street in 1792. \n<br>\n<br>\n<u>Major stock exchanges<\/u>\n\n\nName |\tRegion |\tMarket Capitalization\n---- | ---- | ----\nNew York Stock Exchange\t| United States | USD 25.6 trillion\nNASDAQ\t| United States\t| USD 19.5 trillion\nShanghai Stock Exchange\t| China\t| USD 6.9 trillion\nEuronext Stock Exchange\t| Netherlands\t| $6.8 trillion\n\n<br>\n<br>\nThere can be multiple stock trading venues in a country or a region which allow transactions in stocks and other forms of securities. [The Dhaka Stock Exchange (DSE)](https:\/\/www.dsebd.org\/top_20_share.php) is one of the two stock exchanges of Bangladesh.","bb5d83fd":"A common challenge with k-means is that you must tell it how many clusters you expect. Figuring out how many clusters we need is not obvious from data, thus we will try different clusters numbers and check their silhouette coefficient. The silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). The elbow method can be used to determine the number of clusters as well.\n<br>\n<br>\n**Note**: K-means is sensitive to initializations because those initializations are critical to quality of optima found. Thus, we will use smart initialization called ***k-means++***.","43f1cc7c":"### Visual EDA","866dca31":"### KNN classifier","ccb463a9":"\n\n---\n\n","9edc7c7d":"### Hierarchical Clustering","771a8f14":"Seaborn Boxplot and Stripplot","ace67058":"Sector distribution of DSE, June 2021","95d5de89":"Sort by `Foreign` holding shares","fb065f22":"#### Visualize feature relations <br>\n\nTo get a better understanding of the dataset, we can construct a scatter matrix of each of the features present in the *stock* data.","aad95b4c":"## a. Exploratory data analysis\n","824d395b":"From the above result it appears that out of the entire dataset only the **NAV** (Net asset value of the stock) **column** contains a total of **three** (3) `NaN` values. So before proceeding to further analytical steps we need to process these null values first.","daef4c62":"> ##### URL:\n>> [DSE Listed Companies Database](https:\/\/www.dsebd.org\/company_listing.php)","cf02de4a":"### Sequential Ensemble Learning\nBoosting Methods","8b979127":"While a number of studies were undertaken  to understand the nature of the  relationship between  financial  development  and  economic  growth,  only a  few  included  the role  of stock  markets  in  the  economic  development  process.\n<br>\n<br>\nDifferent authors across the globe have investigated capital market valuation from different perspective. **Glassman and Hassett (1999)** analyzed why even at the verge of market crash the stock prices keep increasing. The study concluded that investors bid up the prices of stocks because of their over optimistic expectation of return on stock investment. The study was conducted on the Dow Jones Industrial Average and it was found that people get irrationally thrilled by any positive news and invest in the market without proper preparation.\n<br>\n<br>\n**Shiller (2000)** summarized the evidence of exuberances of investors as a strong factor for market bubble against rationality of the market. The study identified forecast of future returns as the most commonly evidenced factor. Shiller also identified obvious pricing errors in the sample under study since actual stock prices were more volatile than the present discounted value of actual dividends.\n<br>\n<br>\nThe finding of a number of empirical study identified investor\u2019s over optimism as one of the main factors for stock price overvaluation. **Chen, Hong, and Stein (2002)** analyzed the overvaluation generated by beliefs. The study concluded that the market overvaluation was caused by the investors\u2019 overconfidence.\n<br>\n<br>\nEven though a huge number of empirical studies have been undertaken on behavior of stock price in developed markets, the focus on developing and emerging markets has only begun in recent years.\n<br>\n<br>\n**Chowdhury & Chowdhury (2010)** in their study analyzed the relationship between capital structure and firm value in Bangladesh. The study was based on secondary data of publicly listed companies in Dhaka Stock Exchange (DSE) and Chittagong Stock Exchange (CSE). The study found a strong co-relation among current ratio, operating leverage, EPS, dividend payout ratio or share capital and stock price and concluded that by changing these parameters, a firm may increase its value in the market.\n<br>\n<br>\n**SaminaHaque and Murtaza Faruquee (2013)** in their study of  identifying the influence of various fundamental factors in determining the\nmarket price of shares in Dhaka Stock Exchange (DSE) attempted to find out the co-relation between market price of the stocks and companies\u2019 performance, which includes earning per share (EPS), dividend per share (DPS), return on equity (ROE), return on assets (ROA), and the ratio of fixed asset to total asset (FA\/TA). In the second segment of the study, the market price of stocks under sample has been compared to fundamental or intrinsic price. This paper considers Net Asset Value (NAV) as ideal value of stock. The study depicts that the market price is very insensitive toward fundamentals of companies and current market price is highly overvalued compared to the ideal value of stocks, which reinforces that fact that the impact of unauthorized information has a greater influence in determining the price of stocks in pharmaceuticals and chemical industry in DSE.\n<br>\n<br>\n**Chowudhury & Abdullah (2011)** in their study identified that lack of market regulation, lack of supply of good shares, presence of syndicate, lack of financial knowledge helped the market index to jump over 8000 point during December 2011.\n<br>\n<br>\n**Abdullah et al. (2012)** analyzed whether capital market of Bangladesh was overvalued. The study was based on17 actively traded companies in Dhaka Stock Exchange from 2006 to 2010. The study found that the Stock Market was overvalued and the companies under study were also inefficient in managing costs.","58622c7b":"**Base Learners: SVM**","81dcfc30":"## b. Classification model","33dd20bd":"### History of Dhaka Stock Exchange (DSE)","e2caeb00":"*by*\n<br>\n**Mohammad Ataul Morshed**\n<br>\n<br>\nNovember, 2021","daf10816":"1. Building a classification model to classify each stock company of DSE in the category of the stock\n2. To identify the stock companies that have the tendency to change the category\n3. Writing a scientific report after descriptive analysis of the collected data\n4. Analyzing the fundamental data of the DSE stock market using cluster analysis\n5. To find the optimum number of the cluster and make a list of the members of each cluster\n6. To compare the results of classifier models with the results of clustering methods ","f5727ae3":"##### Silhouetter Method","2fa6a210":"##### Overfitting and underfitting","8fa23173":"From the above result, it can be seen that seven (7) records contain only zero (0) values in all dependent variables except for one variable `Last Price`, therefore these records will not help us predict any meaningful outcome based on a control variable or feature, `Last Price`, which alone has very little influence on the decision of `Category` each stock company belongs to. ","2d6e8359":"Category distribution of DSE, June 2021","9f7ded9d":"##### Dropping inconsequential records","5931a77e":"# Conclusions","108fbf55":"## Raw Data","63da00ca":"#### Hyperparameter tuning","9d83b49a":"# Problem Statement","0cd689d1":"Check whether the current working path changed to the path of source file.","ff833d3b":"Origin: Dhaka Stock Exchange Ltd., 2021.\n\n","ea7b6602":"#### Data Preprocessing","87c6cbc2":"A number of eminent scholars in their study indicated that NAV (Net Asset Value) provides idea about the closest ideal value of a stock. This value indicates the amount on which a shareholder will have right at liquidation of the company. Mostly NAV is applicable for IPO, but in this paper NAV has been taken as a basis for deciding the category of existing stocks due to the misleading result of stock price under CAPM and Dividend discount model based on the available data of DSE.\n<br>\n<br>\nEarnings per share is the amount of profit after tax divided by the total number of shares outstanding. From the perspective of an investor, higher the EPS the better it is, as it indicates the future prospects of the company's business, potential growth opportunities and higher returns for the investors. Therefore price of stock is presumed to have positive relationship with EPS.\n<br>\n<br>\nThe entire analysis of qualitative and quantities factors has been done by `scikit-learn` a free software machine learning library for the `Python` programming language. The report is written using `Google Colab` which allows to combine executable code and rich text in a single document, along with images, HTML, LaTeX and more. ","793860c5":"## Importance of this report","21e5e3c0":"### Research objectives","49137843":"## Literature Review ","768cb5fa":"##### Missing value imputation\nK-Nearest neighbors imputation","9a9fbbe3":"# Introduction","250e6574":"### Statistical methods ","dbc90bbf":"# References","c7bb1ddb":"**Boxplot**\n<br>\n<br>\nA boxplot is a **standardized way of displaying the distribution of data based on a five number summary** (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d). It can also tell us if our data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.","03926eb7":"##### **Elbow Method: Optimal Clusters Size**<br>\nThe location of a bend (knee\/elbow) in the plot is generally considered\nas an indicator of the appropriate number of clusters.\n<br>\n<br>\nThe following code block produces an elbow plot as shown below, which is somewhat difficult to figure out the optimal number of cluster(s). However, we can see a <u>slight elbow<\/u> at ***k = 7***","47687d1c":"**Assignment #1**:\n<br>\nA scientist is interested in exploring the twelve variables of the stock companies found in a dataset collected from Dhaka Stock Exchange (DSE) in June 2021 and building a classification model to classify each stock company in the category of the stock. She\/he also wants to identify the stock companies that have the tendency to change the category. Write a scientific report for that scientist after analyzing this data.\n<br>\n<br>\n**Assignment #2**:\n* Analyze the fundamental data (given in Assignment-2) of the DSE stock market by using cluster analysis.\n* Find the optimum number of the cluster and make a list of the members of each cluster.\n* Add the results with the report of Assignment-2. Comment on your results and compare your results with the results of Assignment-2. \n","3c0314d2":"First incorporated as East Pakistan Stock Exchange Association Ltd on 28 April 1954 and started formal trading in 1956. It was renamed as East Pakistan Stock Exchange Ltd on 23 June 1962. Again renamed as Dacca Stock Exchange Ltd on 13 May 1964. After the liberation war in 1971 the trading was discontinued for five years. In 1976 trading restarted in Bangladesh, on 16 September 1986 DSE was started. The formula for calculating DSE all share price index was changed according to IFC on 1 November 1993. The automated trading was initiated on 10 August 1998 and started on 1 January 2001. A Central Securities Depository System was initiated on 24 January 2004.\n<br>\n<br>\nAs of 16 November 2009, the benchmark index of the Dhaka Stock Exchange (DSE) crossed 4000 points for the first time, setting another new high at 4148 points. In 2010, the index crossed 8500 points and finally crashed in the first quarter of 2011. Millions of investors lost their money and came out onto the street blaming the speculators and regulators for the bubble that finally burst in what became known as the 2011 Bangladesh share market scam. Currently, there are total 22 industrial sectors in DSE which accommodate 578 listed companies.","fbd10af2":"Prepare the environment to create desired readable output (e.g. `.pdf`) for scientists and other target readers. Supported output formats are: \n- HTML\n- LaTeX\n- PDF\n- WebPDF\n- Reveal.js HTML slideshow\n- Markdown\n- Ascii\n- reStructuredText\n- executable script\n- notebook.","8a179d02":"#### Information on Data","d836a9dc":"##### Creating feature and target arrays","0088c652":" - On an average `'Z'` category compnies do not have any Net Asset Value `NAV`. Instead, their average net liability is almost of worth 61 hundred thousand BDT\n - Earnings Per Share, `EPS` is negative for botht the `'B'` and `'Z'` category companies. \n - Average `Paid up` capital of `'Z'` category compnies are **less than half** of that of category **A** companies","6be8f581":"The overall stock market is made up of thousands of investors and traders, who may have differing ideas about the value and performance of a specific stock. Before buying or selling the stock of a company from or in stock market, investors conisder lot of factors along with the 'Category' of the listed company with Dhaka Stock Exchange (DSE). We have therefore, attempted to build a machine learning model to identify the categories of these stock companies based on a set of values of some decisive factors collected from the database of DSE. The present report is divided into two  parts:  in  the  first  section, the  analysis of twelve features related to performance of 373 listed companies in DSE stock market and building a classification model to classify each stock company in the category of the stock based on those control variables. In the second section we have attempted the same goal using cluster analysis methods.","85deb659":"Rows with only zero values in every numeric column except `Last Price` column ","3b20965b":"# Cluster analysis and building a classification model to classify the market category of each DSE stock company","4ee0a4ba":"Sort by `Institute` holding shares"}}