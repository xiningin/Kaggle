{"cell_type":{"6c97be6d":"code","6e3708d5":"code","216cc393":"code","c2025f93":"code","de0192a9":"code","af48356a":"code","17a1c953":"code","69da8c63":"code","b64eec0f":"code","c57c9137":"code","2ce24822":"code","d9b2c991":"code","46ffa094":"code","b8b37aa3":"code","65e87c27":"code","7a7bdfba":"code","609a97a4":"code","213a331c":"code","db46a3ba":"code","f9c393e2":"code","8fccb22c":"code","30894a81":"code","997cbbac":"code","a9a0f2f4":"code","2e2204cc":"markdown","a2f5984e":"markdown","57a4918a":"markdown","3e268c51":"markdown","6a07bd89":"markdown","6484a218":"markdown","5b0d04ae":"markdown","0196104c":"markdown","3733e95a":"markdown","50cf62a2":"markdown","fdb3a607":"markdown"},"source":{"6c97be6d":"import numpy as np\nimport pandas as pd\nimport gc","6e3708d5":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()","216cc393":"(market_train_df, news_train_df) = env.get_training_data()","c2025f93":"### Process market data.\ndef market_process(market_train_df):\n    \n    market_train_df['time'] = market_train_df.time.dt.date\n    market_train_df['bartrend'] = market_train_df['close'] \/ market_train_df['open']\n    market_train_df['average'] = (market_train_df['close'] + market_train_df['open'])\/2\n    market_train_df['pricevolume'] = market_train_df['volume'] * market_train_df['close']\n    \n    # drop nans or not?\n    #market_train_df.dropna(axis=0, inplace=True)\n    market_train_df.drop('assetName', axis=1, inplace=True)\n\n    # Set datatype to float32 to save space\n    float_cols = {c: 'float32' for c in market_train_df.columns if c not in ['assetCode', 'time']}\n    \n    return market_train_df.astype(float_cols)\n\n### process news data.\ndef news_process(news_train_df):\n    \n    news_train_df['time'] = news_train_df.time.dt.date\n    news_train_df['position'] = news_train_df['firstMentionSentence'] \/ news_train_df['sentenceCount']\n    news_train_df['coverage'] = news_train_df['sentimentWordCount'] \/ news_train_df['wordCount']\n    droplist = ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider',\n            'firstMentionSentence','headlineTag','marketCommentary','subjects','audiences',\n            'assetName','noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D',\n            'noveltyCount7D','urgency','sentimentClass']\n    news_train_df.drop(droplist, axis=1, inplace=True)\n    \n    # Remove {} and '' from assetCodes column\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n    return news_train_df\n\n## Unstack assetCodes.\ndef unstack_asset_codes(news_train_df):\n    codes = []\n    indexes = []\n    for i, values in news_train_df['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n    gc.collect()\n    return index_df\n\n## Merge news on index\ndef merge_news_on_index(news_train_df, index_df):\n    news_train_df['news_index'] = news_train_df.index.copy()\n\n    # Merge news on unstacked assets\n    news_unstack_df = index_df.merge(news_train_df, how='left', on='news_index')\n    news_unstack_df.drop(['news_index', 'assetCodes'], axis=1, inplace=True)\n    return news_unstack_df\n\n## Comine multiple news reports for same assets on same day.\ndef group_news(news_frame):\n    \n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'time']).agg(aggregations)\n    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'time']}\n    return gp.astype(float_cols)\n\n### Merge market and news data\ndef merge(market_train_df,news_agg_df):\n    \n    df = market_train_df.merge(news_agg_df, how='left', on=['time','assetCode'])\n    # drop nans or not?\n    #df.dropna(axis=0, inplace=True)\n    \n    del market_train_df, news_agg_df\n    return df\n","de0192a9":"## Market\nmarket_train_df = market_process(market_train_df)\ngc.collect()\n#market_train_df.shape # (4072956, 19) dropnans(3979902, 15)\n\n## News\nnews_train_df = news_process(news_train_df)\nindex_df = unstack_asset_codes(news_train_df)\nnews_unstack_df = merge_news_on_index(news_train_df, index_df) #news_unstack_df.shape #(18821885, 23)\ndel news_train_df, index_df\nnews_agg_df = group_news(news_unstack_df) #news_agg_df.shape #((3839367, 23))\ndel news_unstack_df\n\n## Merge\ndf = merge(market_train_df,news_agg_df) #df.shape # (4072956, 35) dropnans(1121521, 36)\ngc.collect()\ndf.head(3)","af48356a":"df.dropna(axis=0, inplace=True)\n# df.shape #(1121521, __)\n\n# extract useful data.\ndates = df.time\nnum_target = df.returnsOpenNextMktres10.astype('float32')\nbin_target = (df.returnsOpenNextMktres10 >= 0).astype('int8')\nuniverse = df.universe.astype('int8')\n\n# Drop columns that are not features\ndf.drop(['returnsOpenNextMktres10', 'universe', 'assetCode', 'time'], axis=1, inplace=True)\ngc.collect()\ndf.head(3)\n# df.shape #(1121521, 30)","17a1c953":"from sklearn import *\nfrom lightgbm import LGBMClassifier\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix","69da8c63":"train_index, test_index = model_selection.train_test_split(df.index.values, test_size=0.25, \n                                                           random_state = 11)","b64eec0f":"# train_index2, val_index = model_selection.train_test_split(train_index, test_size=0.25, \n#                                                            random_state = 11)","c57c9137":"# def evaluate_model(df, target, train_index, val_index, params):\n#     model = LGBMClassifier(objective ='binary',\n#                            boosting ='gbdt', #dart\n#                            n_jobs = 4,\n#                            max_depth = 8,\n#                            num_iterations = 200,\n#                            learning_rate = 0.05,\n#                            **params)\n#     model.fit(df.loc[train_index],bin_target.loc[train_index])\n#     return metrics.log_loss(target.loc[val_index], \n#                             model.predict_proba(df.loc[val_index]))","2ce24822":"# param_grid = {\n#     'num_leaves': [60, 80],\n#     'n_estimators': [200, 400], #default class*iteration=2*100\n#     'bagging_freq': 5,\n#     'bagging_fraction' : [0.8, 0.9],  # subsample\n#     'feature_fraction' : [0.8, 0.9]  # colsample_bytree\n#     #'reg_alpha': [0.2, 0.6, 0.8],\n#     #'reg_lambda': [0.4, 0.6, 0.8]\n# }\n\n# print('Tuning begins...')\n# best_eval_score = 0\n# for i in range(50):\n#     params = {k: np.random.choice(v) for k, v in param_grid.items()}\n#     score = evaluate_model(df, bin_target, train_index2, val_index, params)\n#     if score < best_eval_score or best_eval_score == 0:\n#         best_eval_score = score\n#         best_params = params\n# print(\"Best evaluation logloss\", best_eval_score)","d9b2c991":"## best parameters found.\nlgb = LGBMClassifier(\n    objective='binary',\n    boosting='gbdt',\n    learning_rate = 0.05,\n    max_depth = 8,\n    num_leaves = 80,\n    n_estimators = 400,\n    bagging_fraction = 0.8,\n    feature_fraction = 0.9)\n    #reg_alpha = 0.2,\n    #reg_lambda = 0.4)","46ffa094":"t = time.time()\nprint('Fitting Up')\nlgb.fit(df.loc[train_index],bin_target.loc[train_index])\nprint('Done')\nprint(f'Done, time = {time.time() - t}')","b8b37aa3":"print(\"lgb accuracy : %f\" % \\\n      accuracy_score(lgb.predict(df.loc[test_index]),\n                     bin_target.loc[test_index]))\nprint(\"lgb AUC : %f\" % \\\n      roc_auc_score(bin_target.loc[test_index].values,\n                    lgb.predict_proba(df.loc[test_index])[:, 1]))","65e87c27":"import matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nimport seaborn as sns\n%matplotlib inline\n\nimport matplotlib as mpl\nmpl.rcParams['axes.titlesize'] = 20\nmpl.rcParams['axes.labelsize'] = 16\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16","7a7bdfba":"#sample5000 = np.random.choice(test_index, size=5000)\n#plt.hist(num_target.loc[sample5000].values.clip(-1,1), bins='auto', alpha=0.3)\nplt.hist(lgb.predict_proba(df.loc[test_index])[:, 1]*2-1, bins='auto', alpha=0.3, color='darkorange')\n#plt.legend(['Ground truth', 'Predicted'])\nplt.xlabel(\"Confidence\")\nplt.ylabel(\"Count\")\nplt.title(\"predicted confidence\")\nplt.show()","609a97a4":"\ncfm = confusion_matrix(y_target=np.array(bin_target.loc[test_index]), \n                       y_predicted=lgb.predict(df.loc[test_index]).tolist())\nfig, ax = plot_confusion_matrix(conf_mat=cfm)\nplt.show()\n","213a331c":"feat_importance = pd.DataFrame()\nfeat_importance[\"feature\"] = df.columns\nfeat_importance[\"value\"] = lgb.feature_importances_\nfeat_importance.sort_values(by='value', ascending=False, inplace=True)\n\nplt.figure(figsize=(8,10))\nax = sns.barplot(y=\"feature\", x=\"value\", data=feat_importance)","db46a3ba":"# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()","f9c393e2":"n_days = 0\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days += 1\n    print(n_days,end=' ')\n    \n    # process market data\n    market_obs_df = market_process(market_obs_df)\n    \n    # process news data\n    news_obs_df = news_process(news_obs_df)\n    index_df = unstack_asset_codes(news_obs_df)\n    news_unstack = merge_news_on_index(news_obs_df, index_df)\n    news_obs_agg = group_news(news_unstack)\n\n    # merge\n    obs_df = merge(market_obs_df,news_obs_agg)\n    del market_obs_df, news_obs_agg, news_obs_df, news_unstack, index_df\n    gc.collect()\n    obs_df = obs_df[obs_df.assetCode.isin(predictions_template_df.assetCode)]\n\n    # Drop cols that are not features\n    feats = [c for c in obs_df.columns if c not in ['assetCode', 'time']]\n\n    #t = time.time()\n    preds = lgb.predict_proba(obs_df[feats])[:, 1] * 2 - 1\n    sub = pd.DataFrame({'assetCode': obs_df['assetCode'], 'confidence': preds})\n    predictions_template_df = predictions_template_df.merge(sub, how='left').drop(\n        'confidenceValue', axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n\n    env.predict(predictions_template_df)\n    del obs_df, predictions_template_df, preds, sub\n    gc.collect()\n","8fccb22c":"env.write_submission_file()","30894a81":"# We've got a submission file!\nimport os\nprint([filename for filename in os.listdir('.') if '.csv' in filename])","997cbbac":"df_competition  = pd.read_csv('submission.csv')\ndf_competition.head(3)","a9a0f2f4":"plt.hist(df_competition.confidenceValue, bins='auto', alpha=0.3, color='green')\n#plt.legend(['Ground truth', 'Predicted'])\nplt.xlabel(\"Confidence\")\nplt.ylabel(\"Count\")\nplt.title(\"predicted confidence for scoring data\")\nplt.show()","2e2204cc":"## Restart the Kernel to run your code again\nIn order to combat cheating, you are only allowed to call `make_env` or iterate through `get_prediction_days` once per Kernel run.  However, while you're iterating on your model it's reasonable to try something out, change the model a bit, and try it again.  Unfortunately, if you try to simply re-run the code, or even refresh the browser page, you'll still be running on the same Kernel execution session you had been running before, and the `twosigmanews` module will still throw errors.  To get around this, you need to explicitly restart your Kernel execution session, which you can do by pressing the Restart button in the Kernel Editor's bottom Console tab:\n![Restart button](https:\/\/i.imgur.com\/hudu8jF.png)","a2f5984e":"## Functions for data processing.","57a4918a":"## Competition results visualization","3e268c51":"Split into train and testing (3:1).","6a07bd89":"## Competition prediction","6484a218":"Confusion matrix.","5b0d04ae":"Tuning parameters using validation set. ","0196104c":"## Prepare for training.","3733e95a":"Prediction confidence distribution.","50cf62a2":"The feature importance in lgb (default) is measured by the numbers of times the feature is used in a model.","fdb3a607":"## Data processing."}}