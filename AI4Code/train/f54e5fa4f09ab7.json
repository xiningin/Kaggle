{"cell_type":{"d3905484":"code","8981b807":"code","161c006f":"code","aff6140c":"code","569b68a5":"code","3b6abfea":"code","3900baf3":"code","f3c875bb":"code","097a8979":"code","71ccb508":"code","bb69853c":"code","351acc2d":"code","209f3eea":"code","a5b2e15a":"code","ed33f909":"code","0ea06097":"code","99bb85fe":"code","c9b7806d":"code","7d326f81":"code","d1ed2d7a":"markdown","835655b9":"markdown","54ffced1":"markdown","c99503e9":"markdown","495691d6":"markdown","496143b5":"markdown","b3afa363":"markdown","99fac641":"markdown","36aa8609":"markdown","fc7728bb":"markdown","c4d6d4d8":"markdown","7a8a902f":"markdown","abcd255f":"markdown","e6c7eb4e":"markdown"},"source":{"d3905484":"import pandas as pd\nimport numpy as np\nimport math\nimport datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nimport random\nfrom string import punctuation\nimport seaborn as sns\n\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\n\n# warnings.filterwarnings('ignore')\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix,classification_report,roc_auc_score\n%matplotlib inline\n\n# Set default plot size\nplt.rcParams[\"figure.figsize\"] = (15,8)","8981b807":"real = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\nfake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\nfake['Authenticity'] = 'Fake'\nreal['Authenticity'] = 'Real'\nnews_data = fake.append(real)\nnews_data.head()","161c006f":"sw = stopwords.words('english')\n\nnew_words=('\u2019','\u201c', '\u201d')\n\nfor i in new_words:\n    sw.append(i)\n\n\n# Convert to lower case\nnews_data['text'] = news_data['text'].str.lower()\n\n# Tokenizing\nnews_data['tokenized_text'] = news_data['text'].apply(word_tokenize)\n\n# Remove stopwords\nnews_data['filtered_text'] = news_data['tokenized_text'].apply(lambda x: [item for item in x if item not in sw])\n\n# Remove punction\nnews_data['filtered_text'] = news_data['filtered_text'].apply(lambda x: [item for item in x if item not in punctuation])\n\n# Check results\nprint(len(news_data['text'].iloc[0]),\n      len(news_data['tokenized_text'].iloc[0]),\n      len(news_data['filtered_text'].iloc[0]))","aff6140c":"news_data.head()","569b68a5":"text = \" \".join(text for text in news_data.text)\n\nwordcloud = WordCloud(background_color=\"white\", max_words=1000,\n                      max_font_size=90, random_state=42).generate(text)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n","3b6abfea":"news_data_fake = news_data[news_data.Authenticity == 'Fake']\nnews_data_real = news_data[news_data.Authenticity == 'Real']\n\nnews_data_fake.head()","3900baf3":"fake = news_data_fake.filtered_text.tolist()\n\nfake_list = []\nfor sublist in fake:\n    for item in sublist:\n        fake_list.append(item)\n\nreal = news_data_real.filtered_text.tolist()\n\nreal_list = []\nfor sublist in real:\n    for item in sublist:\n        real_list.append(item)\n        \nall_words = news_data.filtered_text.tolist()\n\nall_words_list = []\nfor sublist in all_words:\n    for item in sublist:\n        all_words_list.append(item)","f3c875bb":"vocab_fake = nltk.FreqDist(fake_list)\nvocab_real = nltk.FreqDist(real_list)\nvocab_all = nltk.FreqDist(all_words_list)\n\nprint('Fake most common words: ',vocab_fake.most_common(20),\n     'Real most common words: ',vocab_real.most_common(20),\n     'All most common words: ',vocab_real.most_common(20))","097a8979":"common_words_fake = [item[0] for item in vocab_fake.most_common(20)]\nnltk.Text(fake_list[:10000]).dispersion_plot(common_words_fake)\n\ncommon_words_real = [item[0] for item in vocab_real.most_common(20)]\nnltk.Text(real_list[:10000]).dispersion_plot(common_words_real)","71ccb508":"vectorizer = TfidfVectorizer(stop_words=sw,lowercase=True)\ny = news_data.Authenticity\nx = vectorizer.fit_transform(news_data.text)","bb69853c":"print (x.shape)\nprint (y.shape)","351acc2d":"X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.3)\n\nNB_classifier = MultinomialNB()\nNB_classifier.fit(X_train,y_train)","209f3eea":"labels = NB_classifier.predict(X_test)\n\nmat = confusion_matrix(y_test, labels)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label')","a5b2e15a":"roc_auc_score(y_test,NB_classifier.predict_proba(X_test)[:,1])","ed33f909":"print(classification_report(y_test,labels))","0ea06097":"model = LogisticRegression()\nmodel.fit(X_train, y_train)","99bb85fe":"model.score(X_test, y_test)","c9b7806d":"y_model = model.predict(X_test)\n\nmat = confusion_matrix(y_test,y_model)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False) \nplt.xlabel('predicted value')\nplt.ylabel('true value')","7d326f81":"print(classification_report(y_test,y_model))","d1ed2d7a":"Similiarly, the confusion matrix heatmap and the classification report also indicate that logestic regression model fits the data well.\n\nComparing the two classifiers, we can see that the **logestic regression model** actually doing a better job than Naive Bayes in classifying this dataset as it makes more accurate predictions.","835655b9":"Then create a simple wordcloud from the dataset.","54ffced1":"To further analyze the distribution of the most common words, I choose the top most common words from fake and real news dataset and plot the Lexical **Dispersion Plot**. I'm only choosing the first 10000 character from each datasets.\n\nAs we can see, the first plot is the fake news and the second plot is the real news. The word **said** has appeared more frequently and condensed in the real news datasets then the fake news datasets.","c99503e9":"After training the dataset to a Naive Bayes classifier, I also want to see how logestic regression model fits the data. Follwoing the similiar procedures, I train the data using logestic regression.","495691d6":"The data contains the title of the news, the actual content of the news,which will be what I will focus on, the subject of the news and the date of the news. For the convenience of the analysis, I added one more column called **Authenticity** to differentiate fake news from real news. ","496143b5":"## Data Preprocessing and EDA\n\nBefore fit the data into a classifier, the text data needs to be preprocessed and explored.\n\nFirst, we need to tokenize the text, and thenremove the stopwords and punctuation from the text.","b3afa363":"## Classification\n\nAfter exploring the data, we had a better understanding of the dataset. We can start building our classifier to train our datasets.\n\nFor this part, I'm using `TfidfVectorizer` module from `sklearn.feature_extraction.text`, The result is a sparse matrix recording the number of times each word appears and weights the word counts by a measure of how often they appear in the documents. The result can then be fit to a multinomial Naive Bayes classifier.","99fac641":"By printing out `classification_report`, we can see that both fake and real news have a 94% precision.","36aa8609":"The roc auc score shows 98% accuracy, which means it's a very good model.","fc7728bb":"The model score is 0.98, which means logestic regression model also fits pretty well.","c4d6d4d8":"After we train the model, we want to test how accurate the model is. By using the `confusion_matrix`, we can plot a `heatmap` using `seaborn` to see how much records in the testing data the classifier has been successfully predicted.\n\nFrom the plot, we can see that the result is pretty good.","7a8a902f":"Using `most_common()` function from `nltk`, this will print out the most commonly appeared words  in fake, real and all news datasets. As we can see that there are some overlaps in most common words.","abcd255f":"# Fake News Classification\n\n## Introduction\nThe data from this analysis is from kaggle: [Fake and real news dataset](https:\/\/www.kaggle.com\/clmentbisaillon\/fake-and-real-news-dataset), which contains two news data from different sources. One of the data contains real news and another file contains fake news.\n\nThe purpose of this analysis is to analyze the data using **Python NLTK tools** and **Sklearn kit** to perform basic natural language processing and machining learning methods to train the dataset to build a classifier to identify fake news from real ones.\n\nThe methods used in this analysis includes:\n\n- Word tokenizing\n- Stop words and punctuations removal\n- Lexical dispersion plot\n- Naive Bayes Classification\n- logestic Regression\n\n## Loading data\n\nThe analysis starts with loading data and necessary library.","e6c7eb4e":"The I create two separate datasets for fake and real news to examine the most commonly appeared words in each dataset using `FreqDist` function from `nltk`."}}